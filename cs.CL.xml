<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#36880;&#23618;&#37325;&#35201;&#24615;&#37319;&#26679;&#30340;&#26032;&#26041;&#27861;LISA&#22312;&#24494;&#35843;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#35760;&#24518;&#25104;&#26412;&#20302;&#19988;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.17919</link><description>&lt;p&gt;
LISA&#65306;&#29992;&#20110;&#39640;&#25928;&#20869;&#23384;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#30340;&#36880;&#23618;&#37325;&#35201;&#24615;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17919
&lt;/p&gt;
&lt;p&gt;
&#36880;&#23618;&#37325;&#35201;&#24615;&#37319;&#26679;&#30340;&#26032;&#26041;&#27861;LISA&#22312;&#24494;&#35843;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#35760;&#24518;&#25104;&#26412;&#20302;&#19988;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#33258;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39318;&#27425;&#20986;&#29616;&#20197;&#26469;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#65292;&#28982;&#32780;&#23427;&#20204;&#24040;&#22823;&#30340;&#20869;&#23384;&#28040;&#32791;&#24050;&#25104;&#20026;&#22823;&#35268;&#27169;&#35757;&#32451;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#35832;&#22914;&#20302;&#31209;&#35843;&#25972;&#65288;LoRA&#65289;&#20043;&#31867;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#25216;&#26415;&#26469;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#20294;&#22312;&#22823;&#22810;&#25968;&#22823;&#35268;&#27169;&#24494;&#35843;&#35774;&#32622;&#20013;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#20173;&#26080;&#27861;&#19982;&#23436;&#25972;&#21442;&#25968;&#35757;&#32451;&#30456;&#21305;&#37197;&#12290;&#20026;&#24357;&#34917;&#36825;&#19968;&#19981;&#36275;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LoRA&#22312;&#24494;&#35843;&#20219;&#21153;&#20013;&#30340;&#36880;&#23618;&#29305;&#24615;&#65292;&#24182;&#35266;&#23519;&#21040;&#19981;&#21516;&#23618;&#20043;&#38388;&#26435;&#37325;&#33539;&#25968;&#30340;&#24322;&#24120;&#20559;&#26012;&#12290;&#21033;&#29992;&#36825;&#19968;&#20851;&#38190;&#35266;&#23519;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#31616;&#21333;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;&#35760;&#24518;&#25104;&#26412;&#20302;&#20110;LoRA&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#24191;&#27867;&#30340;&#35774;&#32622;&#20013;&#20248;&#20110;LoRA&#21644;&#23436;&#25972;&#21442;&#25968;&#35757;&#32451;&#12290;&#25105;&#20204;&#23558;&#20854;&#21629;&#21517;&#20026;Layerwise Importance Sampled AdamW&#65288;LISA&#65289;&#65292;&#36825;&#26159;LoRA&#30340;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#24212;&#29992;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17919v1 Announce Type: cross  Abstract: The machine learning community has witnessed impressive advancements since the first appearance of large language models (LLMs), yet their huge memory consumption has become a major roadblock to large-scale training. Parameter Efficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA) have been proposed to alleviate this problem, but their performance still fails to match full parameter training in most large-scale fine-tuning settings. Attempting to complement this deficiency, we investigate layerwise properties of LoRA on fine-tuning tasks and observe an uncommon skewness of weight norms across different layers. Utilizing this key observation, a surprisingly simple training strategy is discovered, which outperforms both LoRA and full parameter training in a wide range of settings with memory costs as low as LoRA. We name it Layerwise Importance Sampled AdamW (LISA), a promising alternative for LoRA, which applies the idea of
&lt;/p&gt;</description></item><item><title>&#23618;&#21098;&#26525;&#26041;&#27861;&#21487;&#20197;&#22312;&#27969;&#34892;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#22823;&#37096;&#20998;&#23618;&#30340;&#31227;&#38500;&#32780;&#20445;&#25345;&#24615;&#33021;&#65292;&#21516;&#26102;&#20351;&#29992;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#21487;&#20197;&#36827;&#19968;&#27493;&#20943;&#23569;&#35745;&#31639;&#36164;&#28304;&#65292;&#25552;&#39640;&#25512;&#26029;&#30340;&#20869;&#23384;&#21644;&#24310;&#36831;&#12290;</title><link>https://arxiv.org/abs/2403.17887</link><description>&lt;p&gt;
&#28145;&#23618;&#31070;&#32463;&#32593;&#32476;&#23618;&#21098;&#26525;&#30340;&#19981;&#21512;&#29702;&#26080;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Unreasonable Ineffectiveness of the Deeper Layers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17887
&lt;/p&gt;
&lt;p&gt;
&#23618;&#21098;&#26525;&#26041;&#27861;&#21487;&#20197;&#22312;&#27969;&#34892;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#22823;&#37096;&#20998;&#23618;&#30340;&#31227;&#38500;&#32780;&#20445;&#25345;&#24615;&#33021;&#65292;&#21516;&#26102;&#20351;&#29992;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#21487;&#20197;&#36827;&#19968;&#27493;&#20943;&#23569;&#35745;&#31639;&#36164;&#28304;&#65292;&#25552;&#39640;&#25512;&#26029;&#30340;&#20869;&#23384;&#21644;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#27969;&#34892;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#20102;&#31616;&#21333;&#30340;&#23618;&#21098;&#26525;&#31574;&#30053;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#21457;&#29616;&#22312;&#31227;&#38500;&#22823;&#37096;&#20998;&#23618;&#65288;&#26368;&#39640;&#36798;&#19968;&#21322;&#65289;&#20043;&#21069;&#65292;&#19981;&#21516;&#38382;&#31572;&#22522;&#20934;&#27979;&#35797;&#30340;&#24615;&#33021;&#20960;&#20046;&#27809;&#26377;&#21463;&#21040;&#24433;&#21709;&#12290;&#20026;&#20102;&#21098;&#26525;&#36825;&#20123;&#27169;&#22411;&#65292;&#25105;&#20204;&#36890;&#36807;&#32771;&#34385;&#23618;&#38388;&#30340;&#30456;&#20284;&#24615;&#26469;&#30830;&#23450;&#26368;&#20339;&#30340;&#21098;&#26525;&#23618;&#22359;&#65307;&#28982;&#21518;&#65292;&#20026;&#20102;&#8220;&#20462;&#22797;&#8221;&#25439;&#23475;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23569;&#37327;&#24494;&#35843;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#20351;&#29992;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#65292;&#20855;&#20307;&#21253;&#25324;&#37327;&#21270;&#21644;&#20302;&#31209;&#36866;&#37197;&#22120;&#65288;QLoRA&#65289;&#65292;&#36825;&#26679;&#25105;&#20204;&#30340;&#27599;&#20010;&#23454;&#39564;&#37117;&#21487;&#20197;&#22312;&#21333;&#20010;A100 GPU&#19978;&#25191;&#34892;&#12290;&#20174;&#23454;&#38469;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#23618;&#21098;&#26525;&#26041;&#27861;&#21487;&#20197;&#34917;&#20805;&#20854;&#20182;PEFT&#31574;&#30053;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#20943;&#23569;&#24494;&#35843;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#21478;&#19968;&#26041;&#38754;&#21487;&#20197;&#25552;&#39640;&#25512;&#26029;&#30340;&#20869;&#23384;&#21644;&#24310;&#36831;&#12290;&#20174;&#31185;&#23398;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#35813;&#30740;&#31350;&#34920;&#26126;&#28145;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#23545;&#27169;&#22411;&#30340;&#21098;&#26525;&#27809;&#26377;&#22826;&#22823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17887v1 Announce Type: new  Abstract: We empirically study a simple layer-pruning strategy for popular families of open-weight pretrained LLMs, finding minimal degradation of performance on different question-answering benchmarks until after a large fraction (up to half) of the layers are removed. To prune these models, we identify the optimal block of layers to prune by considering similarity across layers; then, to "heal" the damage, we perform a small amount of finetuning. In particular, we use parameter-efficient finetuning (PEFT) methods, specifically quantization and Low Rank Adapters (QLoRA), such that each of our experiments can be performed on a single A100 GPU. From a practical perspective, these results suggest that layer pruning methods can complement other PEFT strategies to further reduce computational resources of finetuning on the one hand, and can improve the memory and latency of inference on the other hand. From a scientific perspective, the robustness of 
&lt;/p&gt;</description></item><item><title>&#25506;&#32034;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#20197;&#20943;&#23569;NLP&#27169;&#22411;&#39640;&#32622;&#20449;&#24230;&#35823;&#20998;&#31867;&#38382;&#39064;&#30340;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2403.17860</link><description>&lt;p&gt;
&#25506;&#31350;LLMs&#20316;&#20026;&#30446;&#26631;&#21512;&#25104;&#25991;&#26412;&#25968;&#25454;&#26469;&#28304;&#65292;&#20197;&#20943;&#23569;&#39640;&#32622;&#20449;&#24230;&#35823;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Exploring LLMs as a Source of Targeted Synthetic Textual Data to Minimize High Confidence Misclassifications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17860
&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#20197;&#20943;&#23569;NLP&#27169;&#22411;&#39640;&#32622;&#20449;&#24230;&#35823;&#20998;&#31867;&#38382;&#39064;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#27169;&#22411;&#32463;&#36807;&#20248;&#21270;&#20197;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#26102;&#65292;&#24120;&#24120;&#23384;&#22312;&#39640;&#32622;&#20449;&#24230;&#38169;&#35823;&#24182;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#21644;&#36229;&#20986;&#20998;&#24067;&#25968;&#25454;&#30340;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#20316;&#20026;&#35299;&#20915;NLP&#27169;&#22411;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#20135;&#29983;&#39640;&#32622;&#20449;&#24230;&#38169;&#35823;&#39044;&#27979;&#38382;&#39064;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#30001;LLMs&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#19982;&#36890;&#36807;&#30456;&#21516;&#36807;&#31243;&#33719;&#24471;&#30340;&#20154;&#24037;&#25968;&#25454;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#20943;&#36731;&#38169;&#35823;&#65292;&#20154;&#31867;&#25110;LLMs&#25552;&#20379;&#39640;&#32622;&#20449;&#24230;&#35823;&#20998;&#31867;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20197;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#28982;&#21518;&#29992;&#20110;&#25193;&#23637;&#35757;&#32451;&#38598;&#12290;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19977;&#20010;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#20943;&#23569;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17860v1 Announce Type: new  Abstract: Natural Language Processing (NLP) models optimized for predictive performance often make high confidence errors and suffer from vulnerability to adversarial and out-of-distribution data. Existing work has mainly focused on mitigation of such errors using either humans or an automated approach. In this study, we explore the usage of large language models (LLMs) for data augmentation as a potential solution to the issue of NLP models making wrong predictions with high confidence during classification tasks. We compare the effectiveness of synthetic data generated by LLMs with that of human data obtained via the same procedure. For mitigation, humans or LLMs provide natural language characterizations of high confidence misclassifications to generate synthetic data, which are then used to extend the training set. We conduct an extensive evaluation of our approach on three classification tasks and demonstrate its effectiveness in reducing the
&lt;/p&gt;</description></item><item><title>ChroniclingAmericaQA&#26159;&#19968;&#20010;&#22522;&#20110;&#21382;&#21490;&#32654;&#22269;&#25253;&#32440;&#39029;&#38754;&#30340;&#22823;&#35268;&#27169;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25512;&#21160;QA&#21644;MRC&#20219;&#21153;&#30340;&#21457;&#23637;&#65292;&#24182;&#20811;&#26381;&#20197;&#24448;&#25968;&#25454;&#38598;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.17859</link><description>&lt;p&gt;
ChroniclingAmericaQA:&#22522;&#20110;&#21382;&#21490;&#32654;&#22269;&#25253;&#32440;&#39029;&#38754;&#30340;&#22823;&#35268;&#27169;&#38382;&#31572;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ChroniclingAmericaQA: A Large-scale Question Answering Dataset based on Historical American Newspaper Pages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17859
&lt;/p&gt;
&lt;p&gt;
ChroniclingAmericaQA&#26159;&#19968;&#20010;&#22522;&#20110;&#21382;&#21490;&#32654;&#22269;&#25253;&#32440;&#39029;&#38754;&#30340;&#22823;&#35268;&#27169;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25512;&#21160;QA&#21644;MRC&#20219;&#21153;&#30340;&#21457;&#23637;&#65292;&#24182;&#20811;&#26381;&#20197;&#24448;&#25968;&#25454;&#38598;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17859v1&#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#38382;&#31572;&#65288;QA&#65289;&#21644;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#65288;MRC&#65289;&#20219;&#21153;&#30001;&#20110;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#20197;&#21450;&#26368;&#36817;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#32780;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#21516;&#26102;&#65292;&#35768;&#22810;&#22522;&#20934;&#25968;&#25454;&#38598;&#24050;&#32463;&#29992;&#20110;QA&#21644;MRC&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#25968;&#25454;&#38598;&#20027;&#35201;&#20351;&#29992;&#21516;&#27493;&#25991;&#26723;&#38598;&#21512;&#65288;&#22914;&#32500;&#22522;&#30334;&#31185;&#25110;&#32593;&#32476;&#65289;&#21019;&#24314;&#12290;&#26723;&#26696;&#25991;&#20214;&#38598;&#21512;&#65292;&#22914;&#21382;&#21490;&#25253;&#32440;&#65292;&#21253;&#21547;&#36807;&#21435;&#30340;&#23453;&#36149;&#20449;&#24687;&#65292;&#20294;&#20173;&#26410;&#34987;&#24191;&#27867;&#29992;&#20110;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25512;&#21160;QA&#21644;MRC&#20219;&#21153;&#30340;&#21457;&#23637;&#65292;&#24182;&#20811;&#26381;&#20808;&#21069;&#25968;&#25454;&#38598;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ChroniclingAmericaQA&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#21382;&#21490;&#25253;&#32440;&#38598;Chronicling America&#21019;&#24314;&#30340;&#25317;&#26377;485K&#38382;&#31572;&#23545;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#26159;&#20174;Chronicling Amer&#30340;&#23376;&#38598;&#26500;&#24314;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17859v1 Announce Type: new  Abstract: Question answering (QA) and Machine Reading Comprehension (MRC) tasks have significantly advanced in recent years due to the rapid development of deep learning techniques and, more recently, large language models. At the same time, many benchmark datasets have become available for QA and MRC tasks. However, most existing large-scale benchmark datasets have been created predominantly using synchronous document collections like Wikipedia or the Web. Archival document collections, such as historical newspapers, contain valuable information from the past that is still not widely used to train large language models. To further contribute to advancing QA and MRC tasks and to overcome the limitation of previous datasets, we introduce ChroniclingAmericaQA, a large-scale dataset with 485K question-answer pairs created based on the historical newspaper collection Chronicling America. Our dataset is constructed from a subset of the Chronicling Amer
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#26159;&#20851;&#20110;&#20116;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#33521;&#35821;&#38646;&#27966;&#29983;&#30340;&#35780;&#20272;&#65292;&#39318;&#27425;&#30740;&#31350;&#20102;&#36825;&#31867;&#27867;&#21270;&#30340;&#31243;&#24230;&#65292;&#36890;&#36807;&#35774;&#35745;&#20102;&#19968;&#20010;&#27979;&#35797;&#35789;&#27719;-&#21477;&#27861;&#28789;&#27963;&#24615;&#30340;&#20219;&#21153;&#26469;&#23454;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.17856</link><description>&lt;p&gt;
&#23545;&#20116;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#33521;&#35821;&#38646;&#27966;&#29983;&#36827;&#34892;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Verbing Weirds Language (Models): Evaluation of English Zero-Derivation in Five LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17856
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#26159;&#20851;&#20110;&#20116;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#33521;&#35821;&#38646;&#27966;&#29983;&#30340;&#35780;&#20272;&#65292;&#39318;&#27425;&#30740;&#31350;&#20102;&#36825;&#31867;&#27867;&#21270;&#30340;&#31243;&#24230;&#65292;&#36890;&#36807;&#35774;&#35745;&#20102;&#19968;&#20010;&#27979;&#35797;&#35789;&#27719;-&#21477;&#27861;&#28789;&#27963;&#24615;&#30340;&#20219;&#21153;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35789;&#27719;-&#21477;&#27861;&#30340;&#28789;&#27963;&#24615;&#65292;&#20197;&#36716;&#25442;&#65288;&#25110;&#38646;&#27966;&#29983;&#65289;&#30340;&#24418;&#24335;&#20986;&#29616;&#65292;&#26159;&#33521;&#35821;&#24418;&#24577;&#23398;&#30340;&#19968;&#20010;&#26631;&#24535;&#12290;&#22312;&#36716;&#25442;&#20013;&#65292;&#19968;&#20010;&#35789;&#19982;&#19968;&#20010;&#35789;&#24615;&#34987;&#25918;&#32622;&#22312;&#19968;&#20010;&#38750;&#20856;&#22411;&#30340;&#19978;&#19979;&#25991;&#20013;&#65292;&#34987;&#36843;&#34920;&#29616;&#24471;&#22909;&#20687;&#23427;&#26377;&#19968;&#20010;&#19981;&#21516;&#30340;&#35789;&#24615;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#36825;&#19968;&#36807;&#31243;&#24433;&#21709;&#20102;&#33521;&#35821;&#35789;&#27719;&#30340;&#19968;&#22823;&#37096;&#20998;&#65292;&#20294;&#24456;&#23569;&#26377;&#24037;&#20316;&#30528;&#25163;&#30830;&#23450;&#35821;&#35328;&#27169;&#22411;&#21040;&#24213;&#25429;&#25417;&#21040;&#20102;&#36825;&#31181;&#27867;&#21270;&#30340;&#31243;&#24230;&#12290;&#26412;&#25991;&#39318;&#27425;&#25253;&#36947;&#20102;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34892;&#20026;&#19982;&#36716;&#25442;&#20043;&#38388;&#20851;&#31995;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#29992;&#20110;&#27979;&#35797;&#35789;&#27719;-&#21477;&#27861;&#28789;&#27963;&#24615;&#30340;&#20219;&#21153;&#65292;&#21363;&#27169;&#22411;&#33021;&#22815;&#22312;&#19968;&#20010;&#38750;&#20856;&#22411;&#35789;&#24615;&#26500;&#36896;&#30340;&#35789;&#19978;&#36827;&#34892;&#25512;&#24191;&#30340;&#31243;&#24230;&#12290;&#36825;&#20010;&#20219;&#21153;&#20301;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#33539;&#24335;&#20043;&#20869;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#20116;&#20010;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65306;&#20004;&#20010;&#19987;&#26377;&#27169;&#22411;&#65288;GPT-3.5 &#21644; GPT-4&#65289;&#12289;&#19977;&#20010;&#24320;&#28304;&#27169;&#22411;&#65288;Mistral 7B&#12289;Falcon 40B &#31561;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17856v1 Announce Type: new  Abstract: Lexical-syntactic flexibility, in the form of conversion (or zero-derivation) is a hallmark of English morphology. In conversion, a word with one part of speech is placed in a non-prototypical context, where it is coerced to behave as if it had a different part of speech. However, while this process affects a large part of the English lexicon, little work has been done to establish the degree to which language models capture this type of generalization. This paper reports the first study on the behavior of large language models with reference to conversion. We design a task for testing lexical-syntactic flexibility -- the degree to which models can generalize over words in a construction with a non-prototypical part of speech. This task is situated within a natural language inference paradigm. We test the abilities of five language models -- two proprietary models (GPT-3.5 and GPT-4), three open-source models (Mistral 7B, Falcon 40B, and
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#39046;&#22495;&#30693;&#35782;&#27880;&#20837;&#29983;&#25104;&#31070;&#32463;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#25552;&#20986;&#20102;Neural Probabilistic Soft Logic Dialogue Structure Induction (NEUPSL DSI)&#26041;&#27861;&#65292;&#29992;&#20110;&#24341;&#23548;&#23545;&#35805;&#32467;&#26500;&#35782;&#21035;&#65292;&#24212;&#23545;&#35757;&#32451;&#35821;&#26009;&#26377;&#38480;/&#22024;&#26434;&#20197;&#21450;&#27979;&#35797;&#23545;&#35805;&#39046;&#22495;&#20998;&#24067;&#36716;&#21464;&#31561;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.17853</link><description>&lt;p&gt;
&#21033;&#29992;&#39046;&#22495;&#30693;&#35782;&#36890;&#36807;&#31070;&#32463;&#27010;&#29575;&#36719;&#36923;&#36753;&#24341;&#23548;&#23545;&#35805;&#32467;&#26500;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Using Domain Knowledge to Guide Dialog Structure Induction via Neural Probabilistic Soft Logic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17853
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#39046;&#22495;&#30693;&#35782;&#27880;&#20837;&#29983;&#25104;&#31070;&#32463;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#25552;&#20986;&#20102;Neural Probabilistic Soft Logic Dialogue Structure Induction (NEUPSL DSI)&#26041;&#27861;&#65292;&#29992;&#20110;&#24341;&#23548;&#23545;&#35805;&#32467;&#26500;&#35782;&#21035;&#65292;&#24212;&#23545;&#35757;&#32451;&#35821;&#26009;&#26377;&#38480;/&#22024;&#26434;&#20197;&#21450;&#27979;&#35797;&#23545;&#35805;&#39046;&#22495;&#20998;&#24067;&#36716;&#21464;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#32467;&#26500;&#35782;&#21035;&#65288;DSI&#65289;&#26159;&#25512;&#26029;&#32473;&#23450;&#30446;&#26631;&#23548;&#21521;&#23545;&#35805;&#30340;&#28508;&#22312;&#23545;&#35805;&#32467;&#26500;&#65288;&#21363;&#19968;&#32452;&#23545;&#35805;&#29366;&#24577;&#21450;&#20854;&#26102;&#38388;&#36716;&#25442;&#65289;&#30340;&#20219;&#21153;&#12290;&#36825;&#26159;&#29616;&#20195;&#23545;&#35805;&#31995;&#32479;&#35774;&#35745;&#21644;&#35805;&#35821;&#20998;&#26512;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#20316;&#20026;&#36825;&#20123;&#38382;&#39064;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#31070;&#32463;&#27010;&#29575;&#36719;&#36923;&#36753;&#23545;&#35805;&#32467;&#26500;&#35782;&#21035;&#65288;NEUPSL DSI&#65289;&#30340;&#21407;&#21017;&#24615;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#31526;&#21495;&#30693;&#35782;&#27880;&#20837;&#29983;&#25104;&#31070;&#32463;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20851;&#20110;NEUPSL DSI&#23398;&#20064;&#23545;&#38544;&#34255;&#34920;&#31034;&#36136;&#37327;&#30340;&#24433;&#21709;&#30340;&#24443;&#24213;&#23454;&#35777;&#35843;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17853v1 Announce Type: new  Abstract: Dialog Structure Induction (DSI) is the task of inferring the latent dialog structure (i.e., a set of dialog states and their temporal transitions) of a given goal-oriented dialog. It is a critical component for modern dialog system design and discourse analysis. Existing DSI approaches are often purely data-driven, deploy models that infer latent states without access to domain knowledge, underperform when the training corpus is limited/noisy, or have difficulty when test dialogs exhibit distributional shifts from the training domain. This work explores a neural-symbolic approach as a potential solution to these problems. We introduce Neural Probabilistic Soft Logic Dialogue Structure Induction (NEUPSL DSI), a principled approach that injects symbolic knowledge into the latent space of a generative neural model. We conduct a thorough empirical investigation on the effect of NEUPSL DSI learning on hidden representation quality, few-shot 
&lt;/p&gt;</description></item><item><title>ArabicaQA&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#38463;&#25289;&#20271;&#35821;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#21644;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#24341;&#20837;&#20102;AraDPR&#23494;&#38598;&#27573;&#33853;&#26816;&#32034;&#27169;&#22411;&#21644;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#30340;&#24191;&#27867;&#22522;&#20934;&#27979;&#35797;&#65292;&#20026;&#38463;&#25289;&#20271;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#36164;&#28304;&#24102;&#26469;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.17848</link><description>&lt;p&gt;
ArabicaQA&#65306;&#19968;&#20010;&#29992;&#20110;&#38463;&#25289;&#20271;&#35821;&#38382;&#31572;&#30340;&#32508;&#21512;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ArabicaQA: A Comprehensive Dataset for Arabic Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17848
&lt;/p&gt;
&lt;p&gt;
ArabicaQA&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#38463;&#25289;&#20271;&#35821;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#21644;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#24341;&#20837;&#20102;AraDPR&#23494;&#38598;&#27573;&#33853;&#26816;&#32034;&#27169;&#22411;&#21644;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#30340;&#24191;&#27867;&#22522;&#20934;&#27979;&#35797;&#65292;&#20026;&#38463;&#25289;&#20271;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#36164;&#28304;&#24102;&#26469;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;ArabicaQA&#35299;&#20915;&#20102;&#38463;&#25289;&#20271;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#36164;&#28304;&#20013;&#30340;&#24040;&#22823;&#32570;&#21475;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#38463;&#25289;&#20271;&#35821;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#21644;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#36825;&#20010;&#32508;&#21512;&#25968;&#25454;&#38598;&#30001;&#20247;&#21253;&#24037;&#20316;&#32773;&#21019;&#24314;&#65292;&#21253;&#25324;89,095&#20010;&#21487;&#22238;&#31572;&#30340;&#38382;&#39064;&#21644;3,701&#20010;&#19981;&#21487;&#22238;&#31572;&#30340;&#38382;&#39064;&#65292;&#30475;&#36215;&#26469;&#19982;&#21487;&#22238;&#31572;&#30340;&#38382;&#39064;&#31867;&#20284;&#65292;&#36824;&#38468;&#24102;&#20102;&#39069;&#22806;&#30340;&#24320;&#25918;&#39046;&#22495;&#38382;&#39064;&#26631;&#31614;&#65292;&#26631;&#24535;&#30528;&#38463;&#25289;&#20271;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#36164;&#28304;&#30340;&#37325;&#35201;&#36827;&#27493;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;AraDPR&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#38463;&#25289;&#20271;&#32500;&#22522;&#30334;&#31185;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;&#23494;&#38598;&#27573;&#33853;&#26816;&#32034;&#27169;&#22411;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#35299;&#20915;&#38463;&#25289;&#20271;&#25991;&#26412;&#26816;&#32034;&#30340;&#29420;&#29305;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#21253;&#25324;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#38463;&#25289;&#20271;&#35821;&#38382;&#31572;&#20013;&#30340;&#24191;&#27867;&#22522;&#20934;&#27979;&#35797;&#65292;&#25209;&#21028;&#24615;&#22320;&#35780;&#20272;&#23427;&#20204;&#22312;&#38463;&#25289;&#20271;&#35821;&#22659;&#20013;&#30340;&#24615;&#33021;&#12290;&#24635;&#20043;&#65292;ArabicaQA&#12289;AraDPR&#20197;&#21450;LLMs&#22312;&#38463;&#25289;&#20271;&#35821;&#38382;&#31572;&#20013;&#30340;&#22522;&#20934;&#27979;&#35797;&#31561;&#24037;&#20316;&#23558;&#20419;&#36827;&#38463;&#25289;&#20271;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17848v1 Announce Type: new  Abstract: In this paper, we address the significant gap in Arabic natural language processing (NLP) resources by introducing ArabicaQA, the first large-scale dataset for machine reading comprehension and open-domain question answering in Arabic. This comprehensive dataset, consisting of 89,095 answerable and 3,701 unanswerable questions created by crowdworkers to look similar to answerable ones, along with additional labels of open-domain questions marks a crucial advancement in Arabic NLP resources. We also present AraDPR, the first dense passage retrieval model trained on the Arabic Wikipedia corpus, specifically designed to tackle the unique challenges of Arabic text retrieval. Furthermore, our study includes extensive benchmarking of large language models (LLMs) for Arabic question answering, critically evaluating their performance in the Arabic language context. In conclusion, ArabicaQA, AraDPR, and the benchmarking of LLMs in Arabic question
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#30340;&#20998;&#23618;&#24320;&#25918;&#35789;&#27719;3D&#22330;&#26223;&#22270;&#26144;&#23556;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#20195;&#34920;&#22810;&#23618;&#24314;&#31569;&#24182;&#20801;&#35768;&#26426;&#22120;&#20154;&#22312;&#20854;&#20013;&#31359;&#34892;&#12290;</title><link>https://arxiv.org/abs/2403.17846</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#30340;&#20998;&#23618;&#24320;&#25918;&#35789;&#27719;3D&#22330;&#26223;&#22270;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot Navigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17846
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#30340;&#20998;&#23618;&#24320;&#25918;&#35789;&#27719;3D&#22330;&#26223;&#22270;&#26144;&#23556;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#20195;&#34920;&#22810;&#23618;&#24314;&#31569;&#24182;&#20801;&#35768;&#26426;&#22120;&#20154;&#22312;&#20854;&#20013;&#31359;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#24320;&#25918;&#35789;&#27719;&#26426;&#22120;&#20154;&#26144;&#23556;&#26041;&#27861;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#29305;&#24449;&#20016;&#23500;&#20102;&#23494;&#38598;&#20960;&#20309;&#22320;&#22270;&#12290;&#34429;&#28982;&#36825;&#20123;&#22320;&#22270;&#20801;&#35768;&#22312;&#26597;&#35810;&#26576;&#31181;&#35821;&#35328;&#27010;&#24565;&#26102;&#39044;&#27979;&#36880;&#28857;&#26174;&#33879;&#24615;&#22320;&#22270;&#65292;&#20294;&#22823;&#35268;&#27169;&#29615;&#22659;&#21644;&#36229;&#20986;&#23545;&#35937;&#32423;&#21035;&#30340;&#25277;&#35937;&#26597;&#35810;&#20173;&#28982;&#26159;&#19968;&#20010;&#30456;&#24403;&#22823;&#30340;&#38556;&#30861;&#65292;&#26368;&#32456;&#38480;&#21046;&#20102;&#22522;&#20110;&#35821;&#35328;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HOV-SG&#65292;&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#30340;&#20998;&#23618;&#24320;&#25918;&#35789;&#27719;3D&#22330;&#26223;&#22270;&#26144;&#23556;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#24320;&#25918;&#35789;&#27719;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;3D&#31354;&#38388;&#20013;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24320;&#25918;&#35789;&#27719;&#20998;&#27573;&#32423;&#22320;&#22270;&#65292;&#28982;&#21518;&#26500;&#24314;&#20102;&#30001;&#22320;&#26495;&#12289;&#25151;&#38388;&#21644;&#23545;&#35937;&#27010;&#24565;&#32452;&#25104;&#30340;3D&#22330;&#26223;&#22270;&#23618;&#27425;&#32467;&#26500;&#65292;&#27599;&#20010;&#37117;&#21253;&#21547;&#24320;&#25918;&#24615;&#35789;&#27719;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#34920;&#31034;&#22810;&#23618;&#24314;&#31569;&#65292;&#24182;&#19988;&#20801;&#35768;&#26426;&#22120;&#20154;&#20351;&#29992;&#36328;&#23618;Voronoi&#22270;&#31359;&#36234;&#36825;&#20123;&#24314;&#31569;&#12290;HOV-SG&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17846v1 Announce Type: cross  Abstract: Recent open-vocabulary robot mapping methods enrich dense geometric maps with pre-trained visual-language features. While these maps allow for the prediction of point-wise saliency maps when queried for a certain language concept, large-scale environments and abstract queries beyond the object level still pose a considerable hurdle, ultimately limiting language-grounded robotic navigation. In this work, we present HOV-SG, a hierarchical open-vocabulary 3D scene graph mapping approach for language-grounded robot navigation. Leveraging open-vocabulary vision foundation models, we first obtain state-of-the-art open-vocabulary segment-level maps in 3D and subsequently construct a 3D scene graph hierarchy consisting of floor, room, and object concepts, each enriched with open-vocabulary features. Our approach is able to represent multi-story buildings and allows robotic traversal of those using a cross-floor Voronoi graph. HOV-SG is evaluat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#26469;&#25552;&#21069;&#39044;&#27979;&#37325;&#35201;&#30340;&#25919;&#27835;&#20107;&#20214;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#22270;&#35770;&#12289;&#22242;&#20307;&#20998;&#26512;&#21644;&#35821;&#20041;&#20851;&#31995;&#25581;&#31034;&#25968;&#25454;&#20013;&#38544;&#34255;&#30340;&#39044;&#27979;&#20449;&#21495;</title><link>https://arxiv.org/abs/2403.17816</link><description>&lt;p&gt;
&#22270;&#35821;&#35328;&#27169;&#22411;&#65288;GLM&#65289;&#65306;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#31038;&#20250;&#19981;&#31283;&#23450;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
Graph Language Model (GLM): A new graph-based approach to detect social instabilities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17816
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#26469;&#25552;&#21069;&#39044;&#27979;&#37325;&#35201;&#30340;&#25919;&#27835;&#20107;&#20214;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#22270;&#35770;&#12289;&#22242;&#20307;&#20998;&#26512;&#21644;&#35821;&#20041;&#20851;&#31995;&#25581;&#31034;&#25968;&#25454;&#20013;&#38544;&#34255;&#30340;&#39044;&#27979;&#20449;&#21495;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#31185;&#23398;&#25253;&#21578;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#21033;&#29992;&#26032;&#38395;&#25968;&#25454;&#38598;&#25552;&#21069;&#39044;&#27979;&#37325;&#35201;&#30340;&#25919;&#27835;&#20107;&#20214;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#22270;&#35770;&#65292;&#22242;&#20307;&#20998;&#26512;&#21644;&#35821;&#20041;&#20851;&#31995;&#26469;&#21457;&#29616;&#25968;&#25454;&#20013;&#38544;&#34255;&#30340;&#39044;&#27979;&#20449;&#21495;&#12290;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#35813;&#26041;&#27861;&#30340;&#21021;&#27493;&#29256;&#26412;&#65292;&#24182;&#22312;&#19968;&#20123;&#20107;&#20214;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#36825;&#20123;&#20998;&#26512;&#25581;&#31034;&#20102;&#21021;&#22987;&#30740;&#31350;&#38454;&#27573;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#25509;&#30528;&#20197;&#20004;&#31181;&#20851;&#38190;&#26041;&#24335;&#22686;&#24378;&#20102;&#27169;&#22411;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#28155;&#21152;&#20102;&#19968;&#20010;&#36807;&#28388;&#27493;&#39588;&#65292;&#20197;&#20165;&#32771;&#34385;&#22312;&#36827;&#19968;&#27493;&#22788;&#29702;&#20043;&#21069;&#20855;&#26377;&#25919;&#27835;&#30456;&#20851;&#24615;&#30340;&#26032;&#38395;&#65307;&#20854;&#27425;&#65292;&#25105;&#20204;&#35843;&#25972;&#20102;&#36755;&#20837;&#29305;&#24449;&#65292;&#20351;&#35686;&#25253;&#31995;&#32479;&#23545;&#25968;&#25454;&#20013;&#30340;&#37325;&#22823;&#27874;&#21160;&#26356;&#21152;&#25935;&#24863;&#12290;&#22312;&#23436;&#21892;&#25913;&#36827;&#30340;&#26041;&#27861;&#21518;&#65292;&#25105;&#20204;&#23545;&#21253;&#25324;&#32654;&#22269;&#25239;&#35758;&#27963;&#21160;&#12289;&#20044;&#20811;&#20848;&#25112;&#20105;&#21644;&#27861;&#22269;&#25239;&#35758;&#27963;&#21160;&#22312;&#20869;&#30340;&#21313;&#19968;&#20010;&#20107;&#20214;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#27604;&#22522;&#32447;&#26041;&#27861;&#20855;&#26377;&#20248;&#21183;&#12290;&#36890;&#36807;&#26377;&#38024;&#23545;&#24615;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17816v1 Announce Type: new  Abstract: This scientific report presents a novel methodology for the early prediction of important political events using News datasets. The methodology leverages natural language processing, graph theory, clique analysis, and semantic relationships to uncover hidden predictive signals within the data. Initially, we designed a preliminary version of the method and tested it on a few events. This analysis revealed limitations in the initial research phase. We then enhanced the model in two key ways: first, we added a filtration step to only consider politically relevant news before further processing; second, we adjusted the input features to make the alert system more sensitive to significant spikes in the data. After finalizing the improved methodology, we tested it on eleven events including US protests, the Ukraine war, and French protests. Results demonstrate the superiority of our approach compared to baseline methods. Through targeted refin
&lt;/p&gt;</description></item><item><title>&#21387;&#32553;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#19981;&#20165;&#21462;&#20915;&#20110;&#27169;&#22411;&#22823;&#23567;&#65292;&#36824;&#21462;&#20915;&#20110;&#21387;&#32553;&#26041;&#27861;&#65292;&#21516;&#26102;&#21457;&#29616;&#27169;&#22411;&#21387;&#32553;&#24182;&#19981;&#24635;&#26159;&#20250;&#20351;&#22312;&#23569;&#25968;&#23376;&#32676;&#20307;&#19978;&#30340;&#24615;&#33021;&#21464;&#24046;&#12290;</title><link>https://arxiv.org/abs/2403.17811</link><description>&lt;p&gt;
&#21387;&#32553;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#23545;&#23376;&#32676;&#20307;&#31283;&#20581;&#24615;&#24433;&#21709;&#36739;&#23567;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Compressed Language Models Less Subgroup Robust?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17811
&lt;/p&gt;
&lt;p&gt;
&#21387;&#32553;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#19981;&#20165;&#21462;&#20915;&#20110;&#27169;&#22411;&#22823;&#23567;&#65292;&#36824;&#21462;&#20915;&#20110;&#21387;&#32553;&#26041;&#27861;&#65292;&#21516;&#26102;&#21457;&#29616;&#27169;&#22411;&#21387;&#32553;&#24182;&#19981;&#24635;&#26159;&#20250;&#20351;&#22312;&#23569;&#25968;&#23376;&#32676;&#20307;&#19978;&#30340;&#24615;&#33021;&#21464;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#25104;&#26412;&#65292;&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;&#27169;&#22411;&#21387;&#32553;&#26469;&#21019;&#24314;&#26356;&#23567;&#35268;&#27169;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;&#30001;&#25968;&#25454;&#38598;&#30340;&#26631;&#31614;&#21644;&#23646;&#24615;&#23450;&#20041;&#30340;&#23569;&#25968;&#23376;&#32676;&#20307;&#30340;&#31283;&#20581;&#24615;&#30693;&#20043;&#29978;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;18&#31181;&#19981;&#21516;&#30340;&#21387;&#32553;&#26041;&#27861;&#21644;&#35774;&#32622;&#23545;BERT&#35821;&#35328;&#27169;&#22411;&#30340;&#23376;&#32676;&#20307;&#31283;&#20581;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#26368;&#24046;&#32676;&#32452;&#30340;&#24615;&#33021;&#19981;&#20165;&#21462;&#20915;&#20110;&#27169;&#22411;&#22823;&#23567;&#65292;&#36824;&#21462;&#20915;&#20110;&#25152;&#20351;&#29992;&#30340;&#21387;&#32553;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#21387;&#32553;&#24182;&#19981;&#24635;&#26159;&#20250;&#20351;&#22312;&#23569;&#25968;&#23376;&#32676;&#20307;&#19978;&#30340;&#24615;&#33021;&#21464;&#24046;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#26377;&#21161;&#20110;&#36827;&#19968;&#27493;&#30740;&#31350;&#27169;&#22411;&#21387;&#32553;&#23545;&#23376;&#32676;&#20307;&#31283;&#20581;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17811v1 Announce Type: cross  Abstract: To reduce the inference cost of large language models, model compression is increasingly used to create smaller scalable models. However, little is known about their robustness to minority subgroups defined by the labels and attributes of a dataset. In this paper, we investigate the effects of 18 different compression methods and settings on the subgroup robustness of BERT language models. We show that worst-group performance does not depend on model size alone, but also on the compression method used. Additionally, we find that model compression does not always worsen the performance on minority subgroups. Altogether, our analysis serves to further research into the subgroup robustness of model compression.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;EAP-IG&#65292;&#26088;&#22312;&#26356;&#22909;&#22320;&#20445;&#25345;&#30005;&#36335;&#30340;&#26680;&#24515;&#23646;&#24615;&#65306;&#24544;&#23454;</title><link>https://arxiv.org/abs/2403.17806</link><description>&lt;p&gt;
&#22362;&#20449;&#24544;&#23454;&#65306;&#22312;&#25214;&#21040;&#27169;&#22411;&#26426;&#21046;&#26102;&#36229;&#36234;&#30005;&#36335;&#37325;&#21472;
&lt;/p&gt;
&lt;p&gt;
Have Faith in Faithfulness: Going Beyond Circuit Overlap When Finding Model Mechanisms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17806
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;EAP-IG&#65292;&#26088;&#22312;&#26356;&#22909;&#22320;&#20445;&#25345;&#30005;&#36335;&#30340;&#26680;&#24515;&#23646;&#24615;&#65306;&#24544;&#23454;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35768;&#22810;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;&#24050;&#37319;&#29992;&#30005;&#36335;&#26694;&#26550;&#65292;&#26088;&#22312;&#25214;&#21040;&#35299;&#37322;LM&#22312;&#32473;&#23450;&#20219;&#21153;&#19978;&#34892;&#20026;&#30340;&#26368;&#23567;&#35745;&#31639;&#23376;&#22270;&#25110;&#30005;&#36335;&#12290;&#22823;&#22810;&#25968;&#30740;&#31350;&#36890;&#36807;&#29420;&#31435;&#23545;&#27599;&#20010;&#36793;&#25191;&#34892;&#22240;&#26524;&#24178;&#39044;&#26469;&#30830;&#23450;&#21738;&#20123;&#36793;&#23646;&#20110;LM&#30340;&#30005;&#36335;&#65292;&#20294;&#36825;&#22312;&#27169;&#22411;&#35268;&#27169;&#36739;&#22823;&#26102;&#25928;&#29575;&#20302;&#19979;&#12290;&#36793;&#32536;&#24402;&#22240;&#20462;&#34917;&#65288;EAP&#65289;&#65292;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#36817;&#20284;&#24178;&#39044;&#26041;&#27861;&#65292;&#24050;&#25104;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#21487;&#25193;&#23637;&#20294;&#19981;&#23436;&#32654;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861; - &#24102;&#26377;&#38598;&#25104;&#26799;&#24230;&#30340;EAP&#65288;EAP-IG&#65289;&#65292;&#26088;&#22312;&#26356;&#22909;&#22320;&#20445;&#25345;&#30005;&#36335;&#30340;&#26680;&#24515;&#23646;&#24615;&#65306;&#24544;&#23454;&#12290;&#22914;&#26524;&#30005;&#36335;&#26159;&#24544;&#23454;&#30340;&#65292;&#21017;&#21487;&#20197;&#21435;&#25481;&#30005;&#36335;&#20043;&#22806;&#30340;&#25152;&#26377;&#27169;&#22411;&#36793;&#32780;&#19981;&#20250;&#25913;&#21464;&#27169;&#22411;&#22312;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65307;&#24544;&#23454;&#24615;&#26159;&#30740;&#31350;&#30005;&#36335;&#32780;&#19981;&#26159;&#23436;&#25972;&#27169;&#22411;&#30340;&#29702;&#30001;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992;EAP&#25214;&#21040;&#30340;&#30005;&#36335;&#19981;&#22826;&#24544;&#23454;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17806v1 Announce Type: cross  Abstract: Many recent language model (LM) interpretability studies have adopted the circuits framework, which aims to find the minimal computational subgraph, or circuit, that explains LM behavior on a given task. Most studies determine which edges belong in a LM's circuit by performing causal interventions on each edge independently, but this scales poorly with model size. Edge attribution patching (EAP), gradient-based approximation to interventions, has emerged as a scalable but imperfect solution to this problem. In this paper, we introduce a new method - EAP with integrated gradients (EAP-IG) - that aims to better maintain a core property of circuits: faithfulness. A circuit is faithful if all model edges outside the circuit can be ablated without changing the model's performance on the task; faithfulness is what justifies studying circuits, rather than the full model. Our experiments demonstrate that circuits found using EAP are less faith
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010; T2I &#20248;&#21270;&#36890;&#36807;&#25552;&#31034;&#30340;&#26694;&#26550; OPT2I&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#25913;&#36827; T2I &#27169;&#22411;&#20013;&#30340;&#25552;&#31034;-&#22270;&#20687;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.17804</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#25913;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Text-to-Image Consistency via Automatic Prompt Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17804
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010; T2I &#20248;&#21270;&#36890;&#36807;&#25552;&#31034;&#30340;&#26694;&#26550; OPT2I&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#25913;&#36827; T2I &#27169;&#22411;&#20013;&#30340;&#25552;&#31034;-&#22270;&#20687;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17804v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449; &#25688;&#35201;: &#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#29983;&#25104;&#27169;&#22411;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#36827;&#23637;&#65292;&#20135;&#29983;&#20102;&#22823;&#37327;&#24615;&#33021;&#20248;&#36234;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#23457;&#32654;&#21560;&#24341;&#20154;&#12289;&#36924;&#30495;&#30340;&#22270;&#20687;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#36825;&#20123;&#27169;&#22411;&#20173;&#28982;&#38590;&#20197;&#29983;&#25104;&#19982;&#36755;&#20837;&#25552;&#31034;&#19968;&#33268;&#30340;&#22270;&#20687;&#65292;&#24120;&#24120;&#26080;&#27861;&#27491;&#30830;&#25429;&#25417;&#29289;&#20307;&#25968;&#37327;&#12289;&#20851;&#31995;&#21644;&#23646;&#24615;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#25913;&#36827;&#25552;&#31034;-&#22270;&#20687;&#19968;&#33268;&#24615;&#38754;&#20020;&#20197;&#19979;&#25361;&#25112;&#65306;&#65288;1&#65289;&#23427;&#20204;&#24120;&#24120;&#38656;&#35201;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#65288;2&#65289;&#23427;&#20204;&#20165;&#20851;&#27880;&#38468;&#36817;&#30340;&#25552;&#31034;&#26679;&#26412;&#65292;&#65288;3&#65289;&#23427;&#20204;&#21463;&#21040;&#22270;&#20687;&#36136;&#37327;&#12289;&#34920;&#31034;&#22810;&#26679;&#24615;&#21644;&#25552;&#31034;-&#22270;&#20687;&#19968;&#33268;&#24615;&#20043;&#38388;&#19981;&#21033;&#26435;&#34913;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181; T2I &#36890;&#36807;&#25552;&#31034;&#20248;&#21270;&#30340;&#26694;&#26550; OPT2I&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25913;&#36827; T2I &#27169;&#22411;&#20013;&#30340;&#25552;&#31034;-&#22270;&#20687;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20174;&#29992;&#25143;&#25552;&#31034;&#24320;&#22987;&#65292;&#19981;&#26029;&#36845;&#20195;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17804v1 Announce Type: cross  Abstract: Impressive advances in text-to-image (T2I) generative models have yielded a plethora of high performing models which are able to generate aesthetically appealing, photorealistic images. Despite the progress, these models still struggle to produce images that are consistent with the input prompt, oftentimes failing to capture object quantities, relations and attributes properly. Existing solutions to improve prompt-image consistency suffer from the following challenges: (1) they oftentimes require model fine-tuning, (2) they only focus on nearby prompt samples, and (3) they are affected by unfavorable trade-offs among image quality, representation diversity, and prompt-image consistency. In this paper, we address these challenges and introduce a T2I optimization-by-prompting framework, OPT2I, which leverages a large language model (LLM) to improve prompt-image consistency in T2I models. Our framework starts from a user prompt and iterat
&lt;/p&gt;</description></item><item><title>&#31185;&#23398;&#26032;&#38395;&#25253;&#36947;&#29983;&#25104;&#30340;&#33258;&#21160;&#21270;&#25552;&#39640;&#20102;&#23398;&#26415;&#35265;&#35299;&#30340;&#21487;&#35775;&#38382;&#24615;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#23398;&#26415;&#20986;&#29256;&#29289;&#21644;&#30456;&#24212;&#31185;&#23398;&#26032;&#38395;&#25253;&#36947;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25506;&#32034;&#33258;&#21160;&#29983;&#25104;&#31185;&#23398;&#26032;&#38395;&#25253;&#36947;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.17768</link><description>&lt;p&gt;
&#20174;&#23398;&#26415;&#22797;&#26434;&#24615;&#21040;&#20844;&#20247;&#21465;&#20107;&#65306;&#31185;&#23398;&#26032;&#38395;&#25253;&#36947;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
SciNews: From Scholarly Complexities to Public Narratives -- A Dataset for Scientific News Report Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17768
&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#26032;&#38395;&#25253;&#36947;&#29983;&#25104;&#30340;&#33258;&#21160;&#21270;&#25552;&#39640;&#20102;&#23398;&#26415;&#35265;&#35299;&#30340;&#21487;&#35775;&#38382;&#24615;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#23398;&#26415;&#20986;&#29256;&#29289;&#21644;&#30456;&#24212;&#31185;&#23398;&#26032;&#38395;&#25253;&#36947;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25506;&#32034;&#33258;&#21160;&#29983;&#25104;&#31185;&#23398;&#26032;&#38395;&#25253;&#36947;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#26032;&#38395;&#25253;&#36947;&#20316;&#20026;&#19968;&#20010;&#26725;&#26753;&#65292;&#24039;&#22937;&#22320;&#23558;&#22797;&#26434;&#30340;&#30740;&#31350;&#25991;&#31456;&#32763;&#35793;&#25104;&#19982;&#26356;&#24191;&#27867;&#30340;&#20844;&#20247; resonant &#30340;&#25253;&#36947;&#12290;&#36825;&#31181;&#21465;&#20107;&#30340;&#33258;&#21160;&#29983;&#25104;&#22686;&#24378;&#20102;&#23398;&#26415;&#35265;&#35299;&#30340;&#21487;&#35775;&#38382;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35821;&#26009;&#24211;&#26469;&#20419;&#36827;&#36825;&#31181;&#33539;&#24335;&#30340;&#21457;&#23637;&#12290;&#25105;&#20204;&#30340;&#35821;&#26009;&#24211;&#21253;&#25324;&#20061;&#20010;&#23398;&#31185;&#39046;&#22495;&#20013;&#23398;&#26415;&#20986;&#29256;&#29289;&#21450;&#20854;&#30456;&#24212;&#31185;&#23398;&#26032;&#38395;&#25253;&#36947;&#30340;&#24179;&#34892;&#32534;&#35793;&#12290;&#20026;&#20102;&#35777;&#26126;&#25105;&#20204;&#25968;&#25454;&#38598;&#30340;&#23454;&#29992;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#20998;&#26512;&#65292;&#31361;&#20986;&#20102;&#31185;&#23398;&#26032;&#38395;&#21465;&#20107;&#21644;&#23398;&#26415;&#25991;&#31295;&#20043;&#38388;&#30340;&#21487;&#35835;&#24615;&#21644;&#31616;&#27905;&#24615;&#24046;&#24322;&#12290;&#25105;&#20204;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#22522;&#20934;&#27979;&#35797;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#12290;&#35780;&#20272;&#36807;&#31243;&#21253;&#25324;&#33258;&#21160;&#35780;&#20272;&#21644;&#20154;&#24037;&#35780;&#20272;&#65292;&#20026;&#26410;&#26469;&#25506;&#32034;&#33258;&#21160;&#29983;&#25104;&#31185;&#23398;&#26032;&#38395;&#25253;&#36947;&#25171;&#19979;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17768v1 Announce Type: cross  Abstract: Scientific news reports serve as a bridge, adeptly translating complex research articles into reports that resonate with the broader public. The automated generation of such narratives enhances the accessibility of scholarly insights. In this paper, we present a new corpus to facilitate this paradigm development. Our corpus comprises a parallel compilation of academic publications and their corresponding scientific news reports across nine disciplines. To demonstrate the utility and reliability of our dataset, we conduct an extensive analysis, highlighting the divergences in readability and brevity between scientific news narratives and academic manuscripts. We benchmark our dataset employing state-of-the-art text generation models. The evaluation process involves both automatic and human evaluation, which lays the groundwork for future explorations into the automated generation of scientific news reports. The dataset and code related 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#20855;&#26377;&#22823;&#37327;&#35789;&#27719;&#37325;&#21472;&#30340;NLI&#25361;&#25112;&#25968;&#25454;&#38598;&#65292;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#29305;&#23450;&#32467;&#26500;&#26102;&#20986;&#29616;&#30340;&#22833;&#36133;&#29616;&#35937;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#21306;&#20998;&#29305;&#23450;&#32467;&#26500;&#26102;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;</title><link>https://arxiv.org/abs/2403.17760</link><description>&lt;p&gt;
&#26045;&#24037;&#22914;&#27492;&#22256;&#38590;&#65292;&#20197;&#33267;&#20110;&#21363;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20063;&#22240;&#38169;&#35823;&#21407;&#22240;&#32780;&#27491;&#30830;
&lt;/p&gt;
&lt;p&gt;
Constructions Are So Difficult That Even Large Language Models Get Them Right for the Wrong Reasons
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#20855;&#26377;&#22823;&#37327;&#35789;&#27719;&#37325;&#21472;&#30340;NLI&#25361;&#25112;&#25968;&#25454;&#38598;&#65292;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#29305;&#23450;&#32467;&#26500;&#26102;&#20986;&#29616;&#30340;&#22833;&#36133;&#29616;&#35937;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#21306;&#20998;&#29305;&#23450;&#32467;&#26500;&#26102;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20570;&#20986;&#20102;&#20004;&#26041;&#38754;&#29702;&#35299;&#30340;&#36129;&#29486;&#65306;&#20174;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#35282;&#24230;&#30475;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20855;&#26377;&#22823;&#37327;&#35789;&#27719;&#37325;&#21472;&#30340;NLI&#25361;&#25112;&#25968;&#25454;&#38598;&#65292;&#26368;&#22823;&#31243;&#24230;&#22320;&#20943;&#23569;&#20102;&#27169;&#22411;&#20165;&#22522;&#20110;&#26631;&#35760;&#21306;&#21035;&#26469;&#21306;&#20998;&#34164;&#28085;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;GPT-4&#21644;Llama 2&#20197;&#24378;&#28872;&#30340;&#20559;&#35265;&#22833;&#36133;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#21019;&#24314;&#20102;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23376;&#20219;&#21153;&#65292;&#20197;&#35299;&#37322;&#36825;&#31181;&#22833;&#36133;&#12290;&#20174;&#35745;&#31639;&#35821;&#35328;&#23398;&#30340;&#35282;&#24230;&#30475;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#20010;&#21253;&#21547;&#19977;&#31867;&#24418;&#23481;&#35789;&#30340;&#32467;&#26500;&#32452;&#65292;&#36825;&#20123;&#24418;&#23481;&#35789;&#26080;&#27861;&#36890;&#36807;&#34920;&#38754;&#29305;&#24449;&#26469;&#21306;&#20998;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#20197;&#21508;&#31181;&#26041;&#24335;&#25506;&#31350;LLM&#23545;&#36825;&#20123;&#32467;&#26500;&#30340;&#29702;&#35299;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#22312;&#21508;&#31181;&#26041;&#24335;&#19978;&#37117;&#26080;&#27861;&#21306;&#20998;&#23427;&#20204;&#65292;&#34920;&#26126;&#23427;&#20204;&#19981;&#36275;&#20197;&#20195;&#34920;&#23427;&#20204;&#30340;&#21547;&#20041;&#25110;&#25429;&#25417;&#30701;&#35821;&#22836;&#30340;&#35789;&#27719;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17760v1 Announce Type: new  Abstract: In this paper, we make a contribution that can be understood from two perspectives: from an NLP perspective, we introduce a small challenge dataset for NLI with large lexical overlap, which minimises the possibility of models discerning entailment solely based on token distinctions, and show that GPT-4 and Llama 2 fail it with strong bias. We then create further challenging sub-tasks in an effort to explain this failure. From a Computational Linguistics perspective, we identify a group of constructions with three classes of adjectives which cannot be distinguished by surface features. This enables us to probe for LLM's understanding of these constructions in various ways, and we find that they fail in a variety of ways to distinguish between them, suggesting that they don't adequately represent their meaning or capture the lexical properties of phrasal heads.
&lt;/p&gt;</description></item><item><title>&#22810;&#39033;&#36873;&#25321;&#39064;&#34429;&#28982;&#34987;&#24191;&#27867;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20294;&#22312;&#27979;&#35797;LLMs&#33021;&#21147;&#26102;&#23384;&#22312;&#19968;&#23450;&#23616;&#38480;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#35201;&#38271;&#31687;&#29983;&#25104;&#31572;&#26696;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#22312;&#21452;&#35821;MCQs&#20013;&#34920;&#29616;&#20986;&#39034;&#24207;&#25935;&#24863;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.17752</link><description>&lt;p&gt;
&#22810;&#39033;&#36873;&#25321;&#39064;&#26159;&#21542;&#30495;&#30340;&#33021;&#22815;&#26816;&#27979;LLMs&#30340;&#33021;&#21147;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can multiple-choice questions really be useful in detecting the abilities of LLMs?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17752
&lt;/p&gt;
&lt;p&gt;
&#22810;&#39033;&#36873;&#25321;&#39064;&#34429;&#28982;&#34987;&#24191;&#27867;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20294;&#22312;&#27979;&#35797;LLMs&#33021;&#21147;&#26102;&#23384;&#22312;&#19968;&#23450;&#23616;&#38480;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#35201;&#38271;&#31687;&#29983;&#25104;&#31572;&#26696;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#22312;&#21452;&#35821;MCQs&#20013;&#34920;&#29616;&#20986;&#39034;&#24207;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#39033;&#36873;&#25321;&#39064;(MCQs)&#30001;&#20110;&#20854;&#31616;&#21333;&#21644;&#39640;&#25928;&#32780;&#34987;&#24191;&#27867;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#23545;&#20110;MCQs&#26159;&#21542;&#33021;&#30495;&#27491;&#34913;&#37327;LLMs&#30340;&#33021;&#21147;&#23384;&#22312;&#30097;&#34385;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#35201;&#38271;&#31687;&#29983;&#25104;(LFG)&#31572;&#26696;&#30340;&#30693;&#35782;&#23494;&#38598;&#22411;&#22330;&#26223;&#20013;&#12290;&#20219;&#21153;&#19982;&#35780;&#20272;&#26041;&#27861;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#38656;&#35201;&#23545;MCQ&#30340;&#25928;&#29992;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#65292;&#32780;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#36890;&#36807;&#35780;&#20272;&#20004;&#31181;&#35821;&#35328;&#65288;&#20013;&#25991;&#21644;&#33521;&#25991;&#65289;&#30340;&#22235;&#20010;&#38382;&#31572;(QA)&#25968;&#25454;&#38598;&#19978;&#30340;&#20061;&#20010;LLMs&#26469;&#36827;&#34892;&#12290;&#25105;&#20204;&#21457;&#29616;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65306;LLMs&#22312;&#21452;&#35821;MCQs&#20013;&#34920;&#29616;&#20986;&#19968;&#31181;&#39034;&#24207;&#25935;&#24863;&#24615;&#65292;&#20559;&#21521;&#20110;&#20301;&#20110;&#29305;&#23450;&#20301;&#32622;&#30340;&#31572;&#26696;&#65292;&#21363;&#31532;&#19968;&#20010;&#20301;&#32622;&#12290;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#30452;&#25509;&#36755;&#20986;&#12289;token logit&#21644;&#23884;&#20837;&#26469;&#37327;&#21270;MCQs&#21644;&#38271;&#31687;&#29983;&#25104;&#38382;&#39064;(LFGQs)&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;MCQs&#21644;&#38271;&#31687;&#29983;&#25104;&#30340;&#31572;&#26696;&#20043;&#38388;&#23384;&#22312;&#30456;&#23545;&#36739;&#20302;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17752v1 Announce Type: new  Abstract: Multiple-choice questions (MCQs) are widely used in the evaluation of large language models (LLMs) due to their simplicity and efficiency. However, there are concerns about whether MCQs can truly measure LLM's capabilities, particularly in knowledge-intensive scenarios where long-form generation (LFG) answers are required. The misalignment between the task and the evaluation method demands a thoughtful analysis of MCQ's efficacy, which we undertake in this paper by evaluating nine LLMs on four question-answering (QA) datasets in two languages: Chinese and English. We identify a significant issue: LLMs exhibit an order sensitivity in bilingual MCQs, favoring answers located at specific positions, i.e., the first position. We further quantify the gap between MCQs and long-form generation questions (LFGQs) by comparing their direct outputs, token logits, and embeddings. Our results reveal a relatively low correlation between answers from MC
&lt;/p&gt;</description></item><item><title>&#20026;&#20102;&#22788;&#29702;UD&#26641;&#24211;&#20013;&#26410;&#26631;&#35760;&#30340;&#25658;&#24102;&#21547;&#20041;&#30340;&#35821;&#27861;&#26500;&#36896;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;UD&#27880;&#37322;&#20013;&#28155;&#21152;&#19968;&#20010;&#8220;UCxn&#8221;&#27880;&#37322;&#23618;&#65292;&#24182;&#22312;&#20102;&#35299;&#35821;&#35328;&#31867;&#22411;&#23398;&#30340;&#22522;&#30784;&#19978;&#27604;&#36739;&#36328;&#35821;&#35328;&#30340;&#24418;&#24577;&#21477;&#27861;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.17748</link><description>&lt;p&gt;
UCxn: &#22522;&#20110;&#35821;&#35328;&#31867;&#22411;&#23398;&#30340;&#36890;&#29992;&#20381;&#23384;&#20851;&#31995;&#20013;&#26500;&#36896;&#30340;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
UCxn: Typologically Informed Annotation of Constructions Atop Universal Dependencies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17748
&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22788;&#29702;UD&#26641;&#24211;&#20013;&#26410;&#26631;&#35760;&#30340;&#25658;&#24102;&#21547;&#20041;&#30340;&#35821;&#27861;&#26500;&#36896;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;UD&#27880;&#37322;&#20013;&#28155;&#21152;&#19968;&#20010;&#8220;UCxn&#8221;&#27880;&#37322;&#23618;&#65292;&#24182;&#22312;&#20102;&#35299;&#35821;&#35328;&#31867;&#22411;&#23398;&#30340;&#22522;&#30784;&#19978;&#27604;&#36739;&#36328;&#35821;&#35328;&#30340;&#24418;&#24577;&#21477;&#27861;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#20381;&#23384;&#20851;&#31995;&#65288;UD&#65289;&#39033;&#30446;&#21019;&#24314;&#20102;&#19968;&#20010;&#29645;&#36149;&#30340;&#26641;&#24211;&#38598;&#21512;&#65292;&#28085;&#30422;&#20102;&#36229;&#36807;140&#31181;&#35821;&#35328;&#30340;&#36129;&#29486;&#12290;&#28982;&#32780;&#65292;UD&#30340;&#27880;&#37322;&#24182;&#26410;&#23436;&#20840;&#23637;&#31034;&#25152;&#26377;&#20449;&#24687;&#12290;&#20256;&#36798;&#29305;&#23450;&#21547;&#20041;&#30340;&#35821;&#27861;&#26500;&#36896;&#65292;&#20363;&#22914;&#20855;&#26377;&#29305;&#27530;&#26631;&#35760;&#21644;/&#25110;&#35789;&#24207;&#30340;&#30097;&#38382;&#21477;&#65292;&#26410;&#34987;&#23436;&#25972;&#26631;&#35760;&#12290;&#25105;&#20204;&#25552;&#20513;&#65288;i&#65289;&#22312;UD&#27880;&#37322;&#20013;&#22686;&#21152;&#19968;&#20010;&#8220;UCxn&#8221;&#27880;&#37322;&#23618;&#65292;&#29992;&#20110;&#22788;&#29702;&#25658;&#24102;&#21547;&#20041;&#30340;&#35821;&#27861;&#32467;&#26500;&#65292;&#65288;ii&#65289;&#20197;&#22522;&#20110;&#31867;&#22411;&#23398;&#30340;&#26041;&#24335;&#22788;&#29702;&#65292;&#20174;&#32780;&#33021;&#22815;&#36328;&#35821;&#35328;&#27604;&#36739;&#24418;&#24577;&#21477;&#27861;&#31574;&#30053;&#12290;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#21313;&#31181;&#35821;&#35328;&#20013;&#20116;&#20010;&#26500;&#36896;&#23478;&#26063;&#65292;&#36890;&#36807;&#24418;&#24577;&#21477;&#27861;&#27169;&#24335;&#22312;UD&#26641;&#24211;&#20013;&#35782;&#21035;&#27599;&#20010;&#26500;&#36896;&#30340;&#23454;&#20363;&#12290;&#38500;&#20102;&#20851;&#20110;&#36825;&#20123;&#29305;&#23450;&#26500;&#36896;&#30340;&#21457;&#29616;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#24471;&#20986;&#20102;&#19968;&#20123;&#37325;&#35201;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17748v1 Announce Type: new  Abstract: The Universal Dependencies (UD) project has created an invaluable collection of treebanks with contributions in over 140 languages. However, the UD annotations do not tell the full story. Grammatical constructions that convey meaning through a particular combination of several morphosyntactic elements -- for example, interrogative sentences with special markers and/or word orders -- are not labeled holistically. We argue for (i) augmenting UD annotations with a 'UCxn' annotation layer for such meaning-bearing grammatical constructions, and (ii) approaching this in a typologically informed way so that morphosyntactic strategies can be compared across languages. As a case study, we consider five construction families in ten languages, identifying instances of each construction in UD treebanks through the use of morphosyntactic patterns. In addition to findings regarding these particular constructions, our study yields important insights on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20998;&#23618;&#22686;&#24378;&#32593;&#32476;&#30340;&#25345;&#32493;&#23569;&#26679;&#26412;&#20107;&#20214;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#21407;&#22411;&#22686;&#24378;&#21644;&#23545;&#27604;&#22686;&#24378;&#35299;&#20915;&#20102;&#35760;&#24518;&#20808;&#21069;&#20107;&#20214;&#31867;&#22411;&#21644;&#23398;&#20064;&#23569;&#26679;&#26412;&#20013;&#26032;&#20107;&#20214;&#31867;&#22411;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.17733</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#23618;&#22686;&#24378;&#32593;&#32476;&#30340;&#25345;&#32493;&#23569;&#26679;&#26412;&#20107;&#20214;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Continual Few-shot Event Detection via Hierarchical Augmentation Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17733
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20998;&#23618;&#22686;&#24378;&#32593;&#32476;&#30340;&#25345;&#32493;&#23569;&#26679;&#26412;&#20107;&#20214;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#21407;&#22411;&#22686;&#24378;&#21644;&#23545;&#27604;&#22686;&#24378;&#35299;&#20915;&#20102;&#35760;&#24518;&#20808;&#21069;&#20107;&#20214;&#31867;&#22411;&#21644;&#23398;&#20064;&#23569;&#26679;&#26412;&#20013;&#26032;&#20107;&#20214;&#31867;&#22411;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#25345;&#32493;&#20107;&#20214;&#26816;&#27979;&#20381;&#36182;&#20110;&#20016;&#23500;&#30340;&#26631;&#27880;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#36890;&#24120;&#24456;&#38590;&#33719;&#24471;&#36825;&#20123;&#25968;&#25454;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#25345;&#32493;&#23569;&#26679;&#26412;&#20107;&#20214;&#26816;&#27979;&#65288;CFED&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26356;&#24120;&#35265;&#30340;&#22330;&#26223;&#65292;&#21363;&#24456;&#22810;&#26631;&#35760;&#26679;&#26412;&#26080;&#27861;&#33719;&#21462;&#30340;&#24773;&#20917;&#12290;CFED&#20219;&#21153;&#24456;&#20855;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#38656;&#35201;&#35760;&#24518;&#20808;&#21069;&#30340;&#20107;&#20214;&#31867;&#22411;&#65292;&#24182;&#29992;&#23569;&#37327;&#26679;&#26412;&#23398;&#20064;&#26032;&#30340;&#20107;&#20214;&#31867;&#22411;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35760;&#24518;&#30340;&#26694;&#26550;&#65306;&#20998;&#23618;&#22686;&#24378;&#32593;&#32476;&#65288;HANet&#65289;&#12290;&#20026;&#20102;&#22312;&#26377;&#38480;&#30340;&#20869;&#23384;&#20013;&#35760;&#24518;&#20808;&#21069;&#30340;&#20107;&#20214;&#31867;&#22411;&#65292;&#25105;&#20204;&#23558;&#21407;&#22411;&#22686;&#24378;&#34701;&#20837;&#21040;&#35760;&#24518;&#38598;&#20013;&#12290;&#38024;&#23545;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#23398;&#20064;&#26032;&#20107;&#20214;&#31867;&#22411;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#26631;&#35760;&#34920;&#31034;&#30340;&#23545;&#27604;&#22686;&#24378;&#27169;&#22359;&#12290;&#38500;&#20102;&#19982;&#20808;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#27604;&#36739;&#65292;&#25105;&#20204;&#36824;&#19982;ChatGPT&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17733v1 Announce Type: new  Abstract: Traditional continual event detection relies on abundant labeled data for training, which is often impractical to obtain in real-world applications. In this paper, we introduce continual few-shot event detection (CFED), a more commonly encountered scenario when a substantial number of labeled samples are not accessible. The CFED task is challenging as it involves memorizing previous event types and learning new event types with few-shot samples. To mitigate these challenges, we propose a memory-based framework: Hierarchical Augmentation Networks (HANet). To memorize previous event types with limited memory, we incorporate prototypical augmentation into the memory set. For the issue of learning new event types in few-shot scenarios, we propose a contrastive augmentation module for token representations. Despite comparing with previous state-of-the-art methods, we also conduct comparisons with ChatGPT. Experiment results demonstrate that o
&lt;/p&gt;</description></item><item><title>FastPerson&#25552;&#20986;&#20102;&#19968;&#31181;&#35270;&#39057;&#25688;&#35201;&#21270;&#26041;&#27861;&#65292;&#32771;&#34385;&#20102;&#35762;&#24231;&#35270;&#39057;&#20013;&#30340;&#35270;&#35273;&#21644;&#21548;&#35273;&#20449;&#24687;&#65292;&#36890;&#36807;&#21033;&#29992;&#38899;&#39057;&#36716;&#24405;&#21644;&#23631;&#24149;&#19978;&#30340;&#22270;&#29255;&#21644;&#25991;&#23383;&#21019;&#24314;&#25688;&#35201;&#35270;&#39057;&#65292;&#20174;&#32780;&#26368;&#22823;&#31243;&#24230;&#22320;&#20943;&#23569;&#20102;&#23545;&#23398;&#20064;&#32773;&#26469;&#35828;&#36951;&#28431;&#37325;&#35201;&#20449;&#24687;&#30340;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2403.17727</link><description>&lt;p&gt;
FastPerson: &#36890;&#36807;&#20445;&#30041;&#35821;&#35328;&#21644;&#35270;&#35273;&#20869;&#23481;&#22686;&#24378;&#35270;&#39057;&#23398;&#20064;&#30340;&#39640;&#25928;&#35270;&#39057;&#25688;&#35201;&#21270;
&lt;/p&gt;
&lt;p&gt;
FastPerson: Enhancing Video Learning through Effective Video Summarization that Preserves Linguistic and Visual Contexts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17727
&lt;/p&gt;
&lt;p&gt;
FastPerson&#25552;&#20986;&#20102;&#19968;&#31181;&#35270;&#39057;&#25688;&#35201;&#21270;&#26041;&#27861;&#65292;&#32771;&#34385;&#20102;&#35762;&#24231;&#35270;&#39057;&#20013;&#30340;&#35270;&#35273;&#21644;&#21548;&#35273;&#20449;&#24687;&#65292;&#36890;&#36807;&#21033;&#29992;&#38899;&#39057;&#36716;&#24405;&#21644;&#23631;&#24149;&#19978;&#30340;&#22270;&#29255;&#21644;&#25991;&#23383;&#21019;&#24314;&#25688;&#35201;&#35270;&#39057;&#65292;&#20174;&#32780;&#26368;&#22823;&#31243;&#24230;&#22320;&#20943;&#23569;&#20102;&#23545;&#23398;&#20064;&#32773;&#26469;&#35828;&#36951;&#28431;&#37325;&#35201;&#20449;&#24687;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#28459;&#38271;&#30340;&#35762;&#24231;&#35270;&#39057;&#23545;&#20110;&#26102;&#38388;&#26377;&#38480;&#24182;&#19988;&#23545;&#21508;&#31181;&#20027;&#39064;&#37117;&#24863;&#20852;&#36259;&#30340;&#23398;&#20064;&#32773;&#26469;&#35828;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#25552;&#39640;&#20182;&#20204;&#30340;&#23398;&#20064;&#25928;&#29575;&#12290;&#20026;&#27492;&#65292;&#35270;&#39057;&#25688;&#35201;&#21270;&#19968;&#30452;&#21463;&#21040;&#31215;&#26497;&#30740;&#31350;&#65292;&#20351;&#29992;&#25143;&#21482;&#33021;&#26597;&#30475;&#35270;&#39057;&#20013;&#30340;&#37325;&#35201;&#22330;&#26223;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#38598;&#20013;&#22312;&#35270;&#39057;&#30340;&#35270;&#35273;&#25110;&#38899;&#39057;&#20449;&#24687;&#65292;&#24182;&#25552;&#21462;&#35270;&#39057;&#20013;&#30340;&#37325;&#35201;&#29255;&#27573;&#12290;&#22240;&#27492;&#65292;&#22312;&#35762;&#24231;&#35270;&#39057;&#20013;&#20687;&#32769;&#24072;&#30340;&#35762;&#35805;&#21644;&#40657;&#26495;&#25110;&#24187;&#28783;&#29255;&#19978;&#30340;&#35270;&#35273;&#20449;&#24687;&#19968;&#26679;&#37325;&#35201;&#26102;&#65292;&#23384;&#22312;&#38169;&#36807;&#37325;&#35201;&#20449;&#24687;&#30340;&#39118;&#38505;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FastPerson&#65292;&#19968;&#31181;&#32771;&#34385;&#35762;&#24231;&#35270;&#39057;&#20013;&#35270;&#35273;&#21644;&#21548;&#35273;&#20449;&#24687;&#30340;&#35270;&#39057;&#25688;&#35201;&#21270;&#26041;&#27861;&#12290;FastPerson&#36890;&#36807;&#21033;&#29992;&#38899;&#39057;&#36716;&#24405;&#20197;&#21450;&#23631;&#24149;&#19978;&#30340;&#22270;&#29255;&#21644;&#25991;&#23383;&#21019;&#24314;&#25688;&#35201;&#35270;&#39057;&#65292;&#26368;&#22823;&#31243;&#24230;&#22320;&#20943;&#23569;&#20102;&#23545;&#23398;&#20064;&#32773;&#26469;&#35828;&#36951;&#28431;&#37325;&#35201;&#20449;&#24687;&#30340;&#39118;&#38505;&#12290;&#27492;&#22806;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17727v1 Announce Type: cross  Abstract: Quickly understanding lengthy lecture videos is essential for learners with limited time and interest in various topics to improve their learning efficiency. To this end, video summarization has been actively researched to enable users to view only important scenes from a video. However, these studies focus on either the visual or audio information of a video and extract important segments in the video. Therefore, there is a risk of missing important information when both the teacher's speech and visual information on the blackboard or slides are important, such as in a lecture video. To tackle this issue, we propose FastPerson, a video summarization approach that considers both the visual and auditory information in lecture videos. FastPerson creates summary videos by utilizing audio transcriptions along with on-screen images and text, minimizing the risk of overlooking crucial information for learners. Further, it provides a feature 
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20808;&#36827;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#20027;&#39064;&#32454;&#21270;&#8221;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25552;&#31034;&#24037;&#31243;&#21644;&#28040;&#38500;&#31163;&#39064;&#35789;&#31561;&#26041;&#24335;&#25913;&#36827;&#30701;&#25991;&#26412;&#30340;&#20027;&#39064;&#24314;&#27169;&#36136;&#37327;&#65292;&#25552;&#39640;&#20102;&#20027;&#39064;&#30340;&#35821;&#20041;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.17706</link><description>&lt;p&gt;
&#22686;&#24378;&#30701;&#25991;&#26412;&#24314;&#27169;&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20027;&#39064;&#32454;&#21270;
&lt;/p&gt;
&lt;p&gt;
Enhanced Short Text Modeling: Leveraging Large Language Models for Topic Refinement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17706
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20808;&#36827;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#20027;&#39064;&#32454;&#21270;&#8221;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25552;&#31034;&#24037;&#31243;&#21644;&#28040;&#38500;&#31163;&#39064;&#35789;&#31561;&#26041;&#24335;&#25913;&#36827;&#30701;&#25991;&#26412;&#30340;&#20027;&#39064;&#24314;&#27169;&#36136;&#37327;&#65292;&#25552;&#39640;&#20102;&#20027;&#39064;&#30340;&#35821;&#20041;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#22320;&#26500;&#24314;&#38024;&#23545;&#31616;&#30701;&#25991;&#26412;&#65288;&#22914;&#25512;&#25991;&#21644;&#26032;&#38395;&#26631;&#39064;&#65289;&#30340;&#20027;&#39064;&#27169;&#22411;&#23545;&#25429;&#25417;&#31038;&#20250;&#21160;&#24577;&#30340;&#36805;&#36895;&#21464;&#21270;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#20027;&#39064;&#27169;&#22411;&#24448;&#24448;&#22312;&#20934;&#30830;&#34920;&#36798;&#30701;&#25991;&#26412;&#30340;&#35821;&#20041;&#32454;&#24494;&#24046;&#24322;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65292;&#36825;&#26159;&#30001;&#20110;&#23427;&#20204;&#30340;&#31616;&#27905;&#24615;&#21644;&#32570;&#20047;&#19978;&#19979;&#25991;&#25968;&#25454;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20808;&#36827;&#33021;&#21147;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#20027;&#39064;&#32454;&#21270;&#8221;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#24182;&#38750;&#30452;&#25509;&#21442;&#19982;&#20027;&#39064;&#30340;&#21021;&#27493;&#24314;&#27169;&#65292;&#32780;&#26159;&#19987;&#27880;&#20110;&#25913;&#36827;&#20027;&#39064;&#22312;&#34987;&#25366;&#25496;&#21518;&#30340;&#38454;&#27573;&#12290;&#36890;&#36807;&#24341;&#20837;&#25552;&#31034;&#24037;&#31243;&#65292;&#25105;&#20204;&#25351;&#23548;LLMs&#28040;&#38500;&#32473;&#23450;&#20027;&#39064;&#20013;&#30340;&#31163;&#39064;&#35789;&#65292;&#30830;&#20445;&#20165;&#20445;&#30041;&#19982;&#35821;&#22659;&#30456;&#20851;&#30340;&#35789;&#27719;&#25110;&#29992;&#26356;&#31526;&#21512;&#35821;&#20041;&#30340;&#35789;&#27719;&#26367;&#25442;&#12290;&#36825;&#31181;&#26041;&#27861;&#27169;&#25311;&#20102;&#20154;&#31867;&#33324;&#30340;&#23457;&#26597;&#21644;&#25913;&#36827;&#20027;&#39064;&#30340;&#26041;&#24335;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#21508;&#31181;&#20027;&#39064;&#29983;&#25104;&#30340;&#35821;&#20041;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17706v1 Announce Type: cross  Abstract: Crafting effective topic models for brief texts, like tweets and news headlines, is essential for capturing the swift shifts in social dynamics. Traditional topic models, however, often fall short in accurately representing the semantic intricacies of short texts due to their brevity and lack of contextual data. In our study, we harness the advanced capabilities of Large Language Models (LLMs) to introduce a novel approach termed "Topic Refinement". This approach does not directly involve itself in the initial modeling of topics but focuses on improving topics after they have been mined. By employing prompt engineering, we direct LLMs to eliminate off-topic words within a given topic, ensuring that only contextually relevant words are preserved or substituted with ones that fit better semantically. This method emulates human-like scrutiny and improvement of topics, thereby elevating the semantic quality of the topics generated by vario
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;GenAI&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#36827;&#34892;&#29256;&#26435;&#27861;&#24459;&#20998;&#26512;&#30340;&#26032;&#26041;&#27861;&#65292;&#20026;&#35299;&#20915;&#29256;&#26435;&#20405;&#26435;&#32416;&#32439;&#25552;&#20379;&#20102;&#26377;&#21147;&#30340;&#24037;&#20855;</title><link>https://arxiv.org/abs/2403.17691</link><description>&lt;p&gt;
&#24182;&#38750;&#25152;&#26377;&#30456;&#20284;&#20043;&#22788;&#37117;&#26159;&#19968;&#26679;&#30340;&#65306;&#21033;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#20559;&#35265;&#26469;&#25351;&#23548;GenAI&#29256;&#26435;&#32416;&#32439;
&lt;/p&gt;
&lt;p&gt;
Not All Similarities Are Created Equal: Leveraging Data-Driven Biases to Inform GenAI Copyright Disputes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;GenAI&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#36827;&#34892;&#29256;&#26435;&#27861;&#24459;&#20998;&#26512;&#30340;&#26032;&#26041;&#27861;&#65292;&#20026;&#35299;&#20915;&#29256;&#26435;&#20405;&#26435;&#32416;&#32439;&#25552;&#20379;&#20102;&#26377;&#21147;&#30340;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17691v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#23398;&#31185; &#25277;&#35937;: &#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#21253;&#25324; GitHub Copilot&#12289;OpenAI GPT &#21644; Stable Diffusion&#65292;&#24443;&#24213;&#25913;&#21464;&#20102;&#20869;&#23481;&#21019;&#20316;&#65292;&#20351;&#38750;&#19987;&#19994;&#20154;&#22763;&#33021;&#22815;&#22312;&#21508;&#20010;&#39046;&#22495;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20869;&#23481;&#12290;&#36825;&#31181;&#38761;&#21629;&#24615;&#25216;&#26415;&#23548;&#33268;&#21512;&#25104;&#20869;&#23481;&#28608;&#22686;&#65292;&#24341;&#21457;&#29256;&#26435;&#20405;&#26435;&#30340;&#27861;&#24459;&#32416;&#32439;&#12290;&#20026;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;GenAI&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#36827;&#34892;&#29256;&#26435;&#27861;&#24459;&#20998;&#26512;&#30340;&#26032;&#26041;&#27861;&#65292;&#28436;&#31034;&#20102;GPT2&#21644;Stable Diffusion&#27169;&#22411;&#12290;&#29256;&#26435;&#27861;&#21306;&#20998;&#21407;&#22987;&#34920;&#36798;&#21644;&#36890;&#29992;&#34920;&#36798;&#65288;Sc\`enes \`a faire&#65289;&#65292;&#20445;&#25252;&#21069;&#32773;&#24182;&#20801;&#35768;&#21518;&#32773;&#30340;&#22797;&#21046;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#21306;&#21035;&#22312;&#21382;&#21490;&#19978;&#19968;&#30452;&#24456;&#38590;&#19968;&#33268;&#22320;&#20570;&#20986;&#65292;&#23548;&#33268;&#29256;&#26435;&#20316;&#21697;&#34987;&#36807;&#24230;&#20445;&#25252;&#12290;GenAI&#25552;&#20379;&#20102;&#19968;&#20010;&#21069;&#25152;&#26410;&#26377;&#30340;&#26426;&#20250;&#65292;&#21487;&#20197;&#22686;&#24378;&#36825;&#31181;&#27861;&#24459;&#20998;&#26512;&#65292;&#25581;&#31034;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17691v1 Announce Type: cross  Abstract: The advent of Generative Artificial Intelligence (GenAI) models, including GitHub Copilot, OpenAI GPT, and Stable Diffusion, has revolutionized content creation, enabling non-professionals to produce high-quality content across various domains. This transformative technology has led to a surge of synthetic content and sparked legal disputes over copyright infringement. To address these challenges, this paper introduces a novel approach that leverages the learning capacity of GenAI models for copyright legal analysis, demonstrated with GPT2 and Stable Diffusion models. Copyright law distinguishes between original expressions and generic ones (Sc\`enes \`a faire), protecting the former and permitting reproduction of the latter. However, this distinction has historically been challenging to make consistently, leading to over-protection of copyrighted works. GenAI offers an unprecedented opportunity to enhance this legal analysis by reveal
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;16&#20010;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#22823;&#35268;&#27169;&#35780;&#20272;&#30740;&#31350;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#30740;&#31350;&#32570;&#20047;&#23545;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#19982;&#25552;&#31034;&#25216;&#26415;&#19982;&#26356;&#20256;&#32479;&#30340;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#20043;&#38388;&#27604;&#36739;&#30340;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.17661</link><description>&lt;p&gt;
&#25991;&#26412;&#20998;&#31867;&#30340;&#35821;&#35328;&#27169;&#22411;&#65306;&#20165;&#20165;&#19978;&#19979;&#25991;&#23398;&#20064;&#23601;&#36275;&#22815;&#20102;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Language Models for Text Classification: Is In-Context Learning Enough?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17661
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;16&#20010;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#22823;&#35268;&#27169;&#35780;&#20272;&#30740;&#31350;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#30740;&#31350;&#32570;&#20047;&#23545;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#19982;&#25552;&#31034;&#25216;&#26415;&#19982;&#26356;&#20256;&#32479;&#30340;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#20043;&#38388;&#27604;&#36739;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;&#27425;&#21644;&#23569;&#27425;&#26631;&#35760;&#35774;&#32622;&#20013;&#23637;&#31034;&#20102;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;&#36825;&#20123;&#27169;&#22411;&#30456;&#23545;&#20110;&#22522;&#20110;&#24494;&#35843;&#30340;&#26356;&#26631;&#20934;&#30340;&#26041;&#27861;&#30340;&#20248;&#21183;&#22312;&#20110;&#33021;&#22815;&#29702;&#35299;&#29992;&#33258;&#28982;&#35821;&#35328;&#32534;&#20889;&#30340;&#25351;&#20196;&#65288;&#25552;&#31034;&#65289;&#65292;&#36825;&#26377;&#21161;&#20110;&#23427;&#20204;&#26356;&#22909;&#22320;&#25512;&#24191;&#21040;&#19981;&#21516;&#30340;&#20219;&#21153;&#21644;&#39046;&#22495;&#65292;&#32780;&#26080;&#38656;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#36825;&#20351;&#23427;&#20204;&#36866;&#21512;&#35299;&#20915;&#20855;&#26377;&#26377;&#38480;&#26631;&#27880;&#23454;&#20363;&#25968;&#37327;&#30340;&#39046;&#22495;&#30340;&#25991;&#26412;&#20998;&#31867;&#38382;&#39064;&#12290;&#20294;&#26159;&#65292;&#29616;&#26377;&#30740;&#31350;&#22312;&#35268;&#27169;&#19978;&#26377;&#38480;&#65292;&#24182;&#32570;&#20047;&#23545;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#19982;&#25552;&#31034;&#25216;&#26415;&#30456;&#32467;&#21512;&#19982;&#26356;&#20256;&#32479;&#30340;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#65288;&#22914;&#24494;&#35843;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65289;&#30340;&#27604;&#36739;&#30340;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#28085;&#30422;&#20108;&#20803;&#12289;&#22810;&#31867;&#21644;&#22810;&#26631;&#31614;&#38382;&#39064;&#30340;16&#20010;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#36827;&#34892;&#22823;&#35268;&#27169;&#35780;&#20272;&#30740;&#31350;&#26469;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17661v1 Announce Type: cross  Abstract: Recent foundational language models have shown state-of-the-art performance in many NLP tasks in zero- and few-shot settings. An advantage of these models over more standard approaches based on fine-tuning is the ability to understand instructions written in natural language (prompts), which helps them generalise better to different tasks and domains without the need for specific training data. This makes them suitable for addressing text classification problems for domains with limited amounts of annotated instances. However, existing research is limited in scale and lacks understanding of how text generation models combined with prompting techniques compare to more established methods for text classification such as fine-tuning masked language models. In this paper, we address this research gap by performing a large-scale evaluation study for 16 text classification datasets covering binary, multiclass, and multilabel problems. In par
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#22270;&#20687;&#38382;&#31572;&#30340;&#21487;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#20869;&#22312;&#29983;&#25104;&#23376;&#22270;&#26469;&#25552;&#20379;&#20915;&#31574;&#27934;&#23519;&#65292;&#24182;&#22312;GQA&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.17647</link><description>&lt;p&gt;
&#29992;&#20110;&#21487;&#35299;&#37322;&#22270;&#20687;&#38382;&#31572;&#30340;&#20869;&#22312;&#23376;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Intrinsic Subgraph Generation for Interpretable Graph based Visual Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17647
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#22270;&#20687;&#38382;&#31572;&#30340;&#21487;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#20869;&#22312;&#29983;&#25104;&#23376;&#22270;&#26469;&#25552;&#20379;&#20915;&#31574;&#27934;&#23519;&#65292;&#24182;&#22312;GQA&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#21516;&#26102;&#20063;&#22686;&#21152;&#20102;&#23545;&#21487;&#35299;&#37322;&#26041;&#27861;&#30340;&#38656;&#27714;&#12290;&#22823;&#22810;&#25968;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#20391;&#37325;&#20110;&#29983;&#25104;&#20107;&#21518;&#35299;&#37322;&#65292;&#32780;&#38750;&#37319;&#21462;&#20869;&#22312;&#26041;&#27861;&#65292;&#21518;&#32773;&#29305;&#24449;&#21270;&#20102;&#21487;&#35299;&#37322;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#22270;&#30340;VQA&#30340;&#21487;&#35299;&#37322;&#26041;&#27861;&#65292;&#24182;&#22312;GQA&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#24357;&#21512;&#20102;&#35299;&#37322;&#24615;&#21644;&#24615;&#33021;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#34987;&#35774;&#35745;&#25104;&#22312;&#38382;&#31572;&#36807;&#31243;&#20013;&#26412;&#36136;&#19978;&#29983;&#25104;&#19968;&#20010;&#23376;&#22270;&#20316;&#20026;&#35299;&#37322;&#65292;&#25552;&#20379;&#20915;&#31574;&#21046;&#23450;&#30340;&#27934;&#23519;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#20123;&#29983;&#25104;&#30340;&#23376;&#22270;&#30340;&#36136;&#37327;&#65292;&#25105;&#20204;&#23558;&#23427;&#20204;&#19982;&#24314;&#31435;&#30340;&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20107;&#21518;&#35299;&#37322;&#33021;&#21147;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#36827;&#34892;&#20154;&#31867;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19982;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17647v1 Announce Type: new  Abstract: The large success of deep learning based methods in Visual Question Answering (VQA) has concurrently increased the demand for explainable methods. Most methods in Explainable Artificial Intelligence (XAI) focus on generating post-hoc explanations rather than taking an intrinsic approach, the latter characterizing an interpretable model. In this work, we introduce an interpretable approach for graph-based VQA and demonstrate competitive performance on the GQA dataset. This approach bridges the gap between interpretability and performance. Our model is designed to intrinsically produce a subgraph during the question-answering process as its explanation, providing insight into the decision making. To evaluate the quality of these generated subgraphs, we compare them against established post-hoc explainability methods for graph neural networks, and perform a human evaluation. Moreover, we present quantitative metrics that correlate with the 
&lt;/p&gt;</description></item><item><title>DANCER&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Description Augmented Named entity CorrEctoR&#65288;DANCER&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#23454;&#20307;&#25551;&#36848;&#20026;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;NEC&#25552;&#20379;&#39069;&#22806;&#20449;&#24687;&#65292;&#24110;&#21161;&#32531;&#35299;NE&#21015;&#34920;&#20013;&#30340;&#38899;&#32032;&#28151;&#28102;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.17645</link><description>&lt;p&gt;
DANCER&#65306;&#38024;&#23545;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#23454;&#20307;&#25551;&#36848;&#22686;&#24378;&#21629;&#21517;&#23454;&#20307;&#26657;&#27491;&#22120;
&lt;/p&gt;
&lt;p&gt;
DANCER: Entity Description Augmented Named Entity Corrector for Automatic Speech Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17645
&lt;/p&gt;
&lt;p&gt;
DANCER&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Description Augmented Named entity CorrEctoR&#65288;DANCER&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#23454;&#20307;&#25551;&#36848;&#20026;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;NEC&#25552;&#20379;&#39069;&#22806;&#20449;&#24687;&#65292;&#24110;&#21161;&#32531;&#35299;NE&#21015;&#34920;&#20013;&#30340;&#38899;&#32032;&#28151;&#28102;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#29992;&#20110;ASR&#30340;&#24555;&#36895;&#36731;&#37327;&#32423;&#21629;&#21517;&#23454;&#20307;&#26657;&#27491;&#65288;NEC&#65289;&#27169;&#22411;&#65292;&#36890;&#24120;&#26500;&#24314;&#22312;&#38899;&#32032;&#32423;&#32534;&#36753;&#36317;&#31163;&#31639;&#27861;&#22522;&#30784;&#19978;&#65292;&#24182;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;NEC&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#21629;&#21517;&#23454;&#20307;&#65288;NE&#65289;&#21015;&#34920;&#30340;&#22686;&#21152;&#65292;NE&#21015;&#34920;&#20013;&#30340;&#38899;&#32032;&#28151;&#28102;&#38382;&#39064;&#21464;&#24471;&#26356;&#21152;&#20005;&#37325;&#65307;&#20363;&#22914;&#65292;&#21516;&#38899;&#24322;&#20041;&#35789;&#30340;&#38382;&#39064;&#22823;&#22823;&#22686;&#21152;&#12290;&#37492;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25551;&#36848;&#22686;&#24378;&#22411;&#21629;&#21517;&#23454;&#20307;&#26657;&#27491;&#22120;&#65288;&#31216;&#20026;DANCER&#65289;&#65292;&#21033;&#29992;&#23454;&#20307;&#25551;&#36848;&#25552;&#20379;&#39069;&#22806;&#20449;&#24687;&#65292;&#20197;&#20415;&#22312;ASR&#36716;&#24405;&#20013;&#20026;NEC&#25552;&#20379;&#36741;&#21161;&#20943;&#36731;&#38899;&#32032;&#28151;&#28102;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17645v1 Announce Type: new  Abstract: End-to-end automatic speech recognition (E2E ASR) systems often suffer from mistranscription of domain-specific phrases, such as named entities, sometimes leading to catastrophic failures in downstream tasks. A family of fast and lightweight named entity correction (NEC) models for ASR have recently been proposed, which normally build on phonetic-level edit distance algorithms and have shown impressive NEC performance. However, as the named entity (NE) list grows, the problems of phonetic confusion in the NE list are exacerbated; for example, homophone ambiguities increase substantially. In view of this, we proposed a novel Description Augmented Named entity CorrEctoR (dubbed DANCER), which leverages entity descriptions to provide additional information to facilitate mitigation of phonetic confusion for NEC on ASR transcription. To this end, an efficient entity description augmented masked language model (EDA-MLM) comprised of a dense re
&lt;/p&gt;</description></item><item><title>REFeREE&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#26080;&#21442;&#32771;&#25991;&#26412;&#31616;&#21270;&#24230;&#37327;&#26631;&#20934;&#65292;&#33021;&#22815;&#22312;&#39044;&#27979;&#25991;&#26412;&#36136;&#37327;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#19988;&#26080;&#38656;&#22312;&#25512;&#29702;&#26102;&#20351;&#29992;&#21442;&#32771;&#31616;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.17640</link><description>&lt;p&gt;
REFeREE&#65306;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#26080;&#21442;&#32771;&#25991;&#26412;&#31616;&#21270;&#24230;&#37327;&#26631;&#20934;
&lt;/p&gt;
&lt;p&gt;
REFeREE: A REference-FREE Model-Based Metric for Text Simplification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17640
&lt;/p&gt;
&lt;p&gt;
REFeREE&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#26080;&#21442;&#32771;&#25991;&#26412;&#31616;&#21270;&#24230;&#37327;&#26631;&#20934;&#65292;&#33021;&#22815;&#22312;&#39044;&#27979;&#25991;&#26412;&#36136;&#37327;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#19988;&#26080;&#38656;&#22312;&#25512;&#29702;&#26102;&#20351;&#29992;&#21442;&#32771;&#31616;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17640v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#30340; &#25688;&#35201;: &#25991;&#26412;&#31616;&#21270;&#32570;&#20047;&#26222;&#36941;&#26631;&#20934;&#30340;&#36136;&#37327;&#65292;&#24102;&#27880;&#37322;&#30340;&#21442;&#32771;&#31616;&#21270;&#23384;&#22312;&#31232;&#32570;&#21644;&#26114;&#36149;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#24341;&#20837;REFeREE&#26469;&#32531;&#35299;&#36825;&#20123;&#38480;&#21046;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;3&#38454;&#27573;&#35838;&#31243;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#26080;&#21442;&#32771;&#24230;&#37327;&#26631;&#20934;&#12290;REFeREE&#21033;&#29992;&#19968;&#20010;&#21487;&#20219;&#24847;&#25193;&#23637;&#30340;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#24182;&#19988;&#21482;&#35201;&#26377;&#23569;&#37327;&#20154;&#24037;&#27880;&#37322;&#23601;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#36136;&#37327;&#26631;&#20934;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#24230;&#37327;&#26631;&#20934;&#22312;&#39044;&#27979;&#24635;&#20307;&#35780;&#20998;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;&#21442;&#32771;&#30340;&#25351;&#26631;&#65292;&#24182;&#19988;&#22312;&#39044;&#27979;&#29305;&#23450;&#35780;&#20998;&#26041;&#38754;&#36798;&#21040;&#20102;&#31454;&#20105;&#24615;&#21644;&#19968;&#33268;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#22312;&#25512;&#29702;&#26102;&#19981;&#38656;&#35201;&#21442;&#32771;&#31616;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17640v1 Announce Type: new  Abstract: Text simplification lacks a universal standard of quality, and annotated reference simplifications are scarce and costly. We propose to alleviate such limitations by introducing REFeREE, a reference-free model-based metric with a 3-stage curriculum. REFeREE leverages an arbitrarily scalable pretraining stage and can be applied to any quality standard as long as a small number of human annotations are available. Our experiments show that our metric outperforms existing reference-based metrics in predicting overall ratings and reaches competitive and consistent performance in predicting specific ratings while requiring no reference simplifications at inference time.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21160;&#24577;&#21069;&#32512;&#35843;&#25972;&#30340;&#28151;&#21512;&#20513;&#35758;&#21709;&#24212;&#29983;&#25104;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#20132;&#21449;&#27745;&#26579;&#38382;&#39064;&#65292;&#24182;&#33021;&#22815;&#22312;&#30417;&#30563;&#21644;&#38750;&#30417;&#30563;&#35774;&#32622;&#19979;&#23398;&#20064;&#20513;&#35758;&#24863;&#30693;&#21069;&#32512;&#12290;</title><link>https://arxiv.org/abs/2403.17636</link><description>&lt;p&gt;
&#20351;&#29992;&#21160;&#24577;&#21069;&#32512;&#35843;&#25972;&#30340;&#28151;&#21512;&#20513;&#35758;&#21709;&#24212;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Mix-Initiative Response Generation with Dynamic Prefix Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17636
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21160;&#24577;&#21069;&#32512;&#35843;&#25972;&#30340;&#28151;&#21512;&#20513;&#35758;&#21709;&#24212;&#29983;&#25104;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#20132;&#21449;&#27745;&#26579;&#38382;&#39064;&#65292;&#24182;&#33021;&#22815;&#22312;&#30417;&#30563;&#21644;&#38750;&#30417;&#30563;&#35774;&#32622;&#19979;&#23398;&#20064;&#20513;&#35758;&#24863;&#30693;&#21069;&#32512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#20513;&#35758;&#22312;&#25511;&#21046;&#23545;&#35805;&#26041;&#21521;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#23545;&#20110;&#21457;&#35328;&#32773;&#65292; passively &#21709;&#24212;&#25110; proactively &#20027;&#23548;&#20250;&#23548;&#33268;&#23436;&#20840;&#19981;&#21516;&#30340;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#23545;&#35805;&#31995;&#32479;&#19987;&#27880;&#20110;&#35757;&#32451;&#19968;&#20010;&#32479;&#19968;&#30340;&#21709;&#24212;&#29983;&#25104;&#27169;&#22411;&#65292;&#32780;&#19981;&#21306;&#20998;&#19981;&#21516;&#30340;&#20513;&#35758;&#12290;&#36825;&#23548;&#33268;&#20102;&#20132;&#21449;&#27745;&#26579;&#38382;&#39064;&#65292;&#27169;&#22411;&#28151;&#28102;&#20102;&#19981;&#21516;&#30340;&#20513;&#35758;&#24182;&#29983;&#25104;&#19981;&#21512;&#36866;&#30340;&#21709;&#24212;&#12290;&#27492;&#22806;&#65292;&#20026;&#20513;&#35758;&#26631;&#31614;&#33719;&#21462;&#22823;&#37327;&#20154;&#31867;&#27880;&#37322;&#21487;&#33021;&#24456;&#26114;&#36149;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#28151;&#21512;&#20513;&#35758;&#21160;&#24577;&#21069;&#32512;&#35843;&#25972;&#26694;&#26550; (IDPT)&#65292;&#20197;&#35299;&#32806;&#19981;&#21516;&#20513;&#35758;&#19982;&#29983;&#25104;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#30417;&#30563;&#21644;&#38750;&#30417;&#30563;&#35774;&#32622;&#19979;&#23398;&#20064;&#20513;&#35758;&#24863;&#30693;&#21069;&#32512;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;IDPT&#23558;&#20513;&#35758;&#22240;&#32032;&#35299;&#32806;&#20026;&#19981;&#21516;&#30340;&#21069;&#32512;&#21442;&#25968;&#65292;&#24182;&#20351;&#29992;&#27880;&#24847;&#26426;&#21046;&#35843;&#25972;&#20513;&#35758;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17636v1 Announce Type: new  Abstract: Mixed initiative serves as one of the key factors in controlling conversation directions. For a speaker, responding passively or leading proactively would result in rather different responses. However, most dialogue systems focus on training a holistic response generation model without any distinction among different initiatives. It leads to the cross-contamination problem, where the model confuses different initiatives and generates inappropriate responses. Moreover, obtaining plenty of human annotations for initiative labels can be expensive. To address this issue, we propose a general mix-Initiative Dynamic Prefix Tuning framework (IDPT) to decouple different initiatives from the generation model, which learns initiative-aware prefixes in both supervised and unsupervised settings. Specifically, IDPT decouples initiative factors into different prefix parameters and uses the attention mechanism to adjust the selection of initiatives in 
&lt;/p&gt;</description></item><item><title>&#33258;&#21160;&#26631;&#35760;&#24773;&#32490;&#24378;&#24230;&#24314;&#27169;&#20013;&#30340;&#26368;&#20339;-&#26368;&#24046;&#26631;&#24230;&#27880;&#37322;&#26041;&#27861;&#30340;&#24615;&#33021;&#34920;&#29616;</title><link>https://arxiv.org/abs/2403.17612</link><description>&lt;p&gt;
"&#24744;&#26159;&#19968;&#21517;&#19987;&#23478;&#27880;&#37322;&#32773;": &#33258;&#21160;&#21270;&#24773;&#32490;&#24378;&#24230;&#24314;&#27169;&#30340;&#26368;&#20339;-&#26368;&#24046;&#26631;&#24230;&#27880;&#37322;
&lt;/p&gt;
&lt;p&gt;
"You are an expert annotator": Automatic Best-Worst-Scaling Annotations for Emotion Intensity Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17612
&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#26631;&#35760;&#24773;&#32490;&#24378;&#24230;&#24314;&#27169;&#20013;&#30340;&#26368;&#20339;-&#26368;&#24046;&#26631;&#24230;&#27880;&#37322;&#26041;&#27861;&#30340;&#24615;&#33021;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#35760;&#35821;&#26009;&#24211;&#26500;&#25104;&#20102;&#20026;&#26032;&#20219;&#21153;&#25110;&#39046;&#22495;&#21019;&#24314;&#27169;&#22411;&#30340;&#29942;&#39048;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#33258;&#21160;&#35821;&#26009;&#24211;&#26631;&#35760;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#20998;&#31867;&#26631;&#35760;&#65292;&#32531;&#35299;&#20102;&#36825;&#19968;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;NLP&#20219;&#21153;&#65288;&#22914;&#24773;&#32490;&#24378;&#24230;&#39044;&#27979;&#65289;&#38656;&#35201;&#25991;&#26412;&#22238;&#24402;&#65292;&#20294;&#30446;&#21069;&#23578;&#26080;&#20851;&#20110;&#36830;&#32493;&#26631;&#31614;&#20998;&#37197;&#33258;&#21160;&#21270;&#26631;&#35760;&#30340;&#24037;&#20316;&#12290;&#22238;&#24402;&#34987;&#35748;&#20026;&#27604;&#20998;&#31867;&#26356;&#20855;&#25361;&#25112;&#24615;&#65306;&#24403;&#20154;&#31867;&#34987;&#35201;&#27714;&#20174;&#35780;&#20998;&#23610;&#24230;&#20013;&#36873;&#25321;&#25968;&#20540;&#26102;&#34920;&#29616;&#26356;&#24046;&#65292;&#36825;&#23548;&#33268;&#20102;&#27604;&#36739;&#27880;&#37322;&#26041;&#27861;&#65292;&#21253;&#25324;&#26368;&#20339;-&#26368;&#24046;&#26631;&#24230;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26631;&#27880;&#26041;&#27861;&#26159;&#21542;&#26174;&#31034;&#31867;&#20284;&#30340;&#27169;&#24335;&#65292;&#21363;&#23427;&#20204;&#22312;&#35780;&#20998;&#26631;&#24230;&#27880;&#37322;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#27604;&#22312;&#27604;&#36739;&#26631;&#24230;&#27880;&#37322;&#20219;&#21153;&#19978;&#26356;&#24046;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#33258;&#21160;&#21270;&#24773;&#32490;&#24378;&#24230;&#39044;&#27979;&#24182;&#27604;&#36739;&#30452;&#25509;&#35780;&#20998;&#39044;&#27979;&#12289;&#25104;&#23545;&#27604;&#36739;&#21644;&#26368;&#20339;-&#26368;&#24046;&#26631;&#24230;&#12290;&#25105;&#20204;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17612v1 Announce Type: new  Abstract: Labeling corpora constitutes a bottleneck to create models for new tasks or domains. Large language models mitigate the issue with automatic corpus labeling methods, particularly for categorical annotations. Some NLP tasks such as emotion intensity prediction, however, require text regression, but there is no work on automating annotations for continuous label assignments. Regression is considered more challenging than classification: The fact that humans perform worse when tasked to choose values from a rating scale lead to comparative annotation methods, including best-worst scaling. This raises the question if large language model-based annotation methods show similar patterns, namely that they perform worse on rating scale annotation tasks than on comparative annotation tasks. To study this, we automate emotion intensity predictions and compare direct rating scale predictions, pairwise comparisons and best-worst scaling. We find that
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Denosied Table-Text Retriever&#65288;DoTTeR&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21435;&#22122;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#25972;&#21512;&#34920;&#32423;&#25490;&#21517;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#34920;&#26684;-&#25991;&#26412;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#23384;&#22312;&#30340;&#20551;&#27491;&#26631;&#31614;&#24433;&#21709;&#21644;&#36328;&#34920;&#26684;&#25512;&#29702;&#38382;&#39064;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.17611</link><description>&lt;p&gt;
&#23545;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#30340;&#21435;&#22122;&#34920;&#26684;-&#25991;&#26412;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Denoising Table-Text Retrieval for Open-Domain Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17611
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Denosied Table-Text Retriever&#65288;DoTTeR&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21435;&#22122;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#25972;&#21512;&#34920;&#32423;&#25490;&#21517;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#34920;&#26684;-&#25991;&#26412;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#23384;&#22312;&#30340;&#20551;&#27491;&#26631;&#31614;&#24433;&#21709;&#21644;&#36328;&#34920;&#26684;&#25512;&#29702;&#38382;&#39064;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#34920;&#26684;-&#25991;&#26412;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#65292;&#26816;&#32034;&#31995;&#32479;&#20174;&#34920;&#26684;&#21644;&#25991;&#26412;&#20013;&#26816;&#32034;&#30456;&#20851;&#35777;&#25454;&#20197;&#22238;&#31572;&#38382;&#39064;&#12290;&#20043;&#21069;&#22312;&#34920;&#26684;-&#25991;&#26412;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#30340;&#30740;&#31350;&#23384;&#22312;&#20004;&#20010;&#24120;&#35265;&#25361;&#25112;&#65306;&#39318;&#20808;&#65292;&#23427;&#20204;&#30340;&#26816;&#32034;&#22120;&#21487;&#33021;&#21463;&#21040;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#20551;&#27491;&#26631;&#31614;&#24433;&#21709;&#65307;&#20854;&#27425;&#65292;&#23427;&#20204;&#21487;&#33021;&#38590;&#20197;&#20026;&#38656;&#35201;&#36328;&#34920;&#26684;&#25512;&#29702;&#30340;&#38382;&#39064;&#25552;&#20379;&#36866;&#24403;&#30340;&#35777;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21435;&#22122;&#34920;&#26684;-&#25991;&#26412;&#26816;&#32034;&#22120;&#65288;DoTTeR&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#21033;&#29992;&#19968;&#20010;&#21435;&#22122;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#33293;&#24323;&#36890;&#36807;&#20551;&#27491;&#26816;&#27979;&#27169;&#22411;&#27979;&#37327;&#30340;&#36739;&#20302;&#38382;&#39064;&#30456;&#20851;&#24615;&#24471;&#20998;&#30340;&#31034;&#20363;&#26469;&#20943;&#23569;&#20551;&#27491;&#26631;&#31614;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23558;&#34920;&#32423;&#25490;&#21517;&#20449;&#24687;&#25972;&#21512;&#21040;&#26816;&#32034;&#22120;&#20013;&#65292;&#20197;&#24110;&#21161;&#25214;&#21040;&#38656;&#35201;&#36328;&#34920;&#26684;&#25512;&#29702;&#30340;&#38382;&#39064;&#30340;&#35777;&#25454;&#12290;&#20026;&#20102;&#32534;&#30721;&#27492;&#25490;&#21517;&#20449;&#24687;&#65292;&#25105;&#20204;&#23545;&#19968;&#20010;&#25490;&#21517;&#24863;&#30693;&#30340;&#21015;&#32534;&#30721;&#22120;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17611v1 Announce Type: cross  Abstract: In table-text open-domain question answering, a retriever system retrieves relevant evidence from tables and text to answer questions. Previous studies in table-text open-domain question answering have two common challenges: firstly, their retrievers can be affected by false-positive labels in training datasets; secondly, they may struggle to provide appropriate evidence for questions that require reasoning across the table. To address these issues, we propose Denoised Table-Text Retriever (DoTTeR). Our approach involves utilizing a denoised training dataset with fewer false positive labels by discarding instances with lower question-relevance scores measured through a false positive detection model. Subsequently, we integrate table-level ranking information into the retriever to assist in finding evidence for questions that demand reasoning across the table. To encode this ranking information, we fine-tune a rank-aware column encoder 
&lt;/p&gt;</description></item><item><title>&#25506;&#32034;&#26085;&#26412;&#38750;&#35199;&#26041;&#29615;&#22659;&#20013;&#20849;&#21516;&#35774;&#35745;VAs&#30340;&#20215;&#20540;&#65292;&#23637;&#31034;&#20102;&#25991;&#21270;&#25935;&#24863;&#24230;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.17599</link><description>&lt;p&gt;
&#29992;&#25991;&#21270;&#25935;&#24863;&#24230;&#20849;&#21516;&#26500;&#24819;&#35821;&#38899;&#21161;&#25163;&#30340;&#26410;&#26469;
&lt;/p&gt;
&lt;p&gt;
Coimagining the Future of Voice Assistants with Cultural Sensitivity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17599
&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#26085;&#26412;&#38750;&#35199;&#26041;&#29615;&#22659;&#20013;&#20849;&#21516;&#35774;&#35745;VAs&#30340;&#20215;&#20540;&#65292;&#23637;&#31034;&#20102;&#25991;&#21270;&#25935;&#24863;&#24230;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#21161;&#25163;&#65288;VAs&#65289;&#27491;&#25104;&#20026;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#30340;&#19968;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#29992;&#25143;&#20307;&#39564;&#65288;UX&#65289;&#36890;&#24120;&#21463;&#38480;&#65292;&#23548;&#33268;&#34987;&#20302;&#20272;&#12289;&#33073;&#31163;&#21644;&#25918;&#24323;&#12290;&#19982;&#28508;&#22312;&#26368;&#32456;&#29992;&#25143;&#20849;&#21516;&#35774;&#35745;VAs&#30340;&#20114;&#21160;&#21487;&#33021;&#20250;&#26377;&#30410;&#12290;&#22312;&#32447;&#21311;&#21517;&#22320;&#36890;&#36807;&#20247;&#21253;&#36827;&#34892;&#36825;&#19968;&#36807;&#31243;&#21487;&#33021;&#20250;&#22686;&#21152;&#20215;&#20540;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#24037;&#20316;&#26159;&#22312;&#20197;&#33521;&#35821;&#20026;&#20027;&#30340;&#35199;&#26041;&#22320;&#21306;&#23545;&#23545;&#35805;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#12290;&#25105;&#20204;&#24517;&#39035;&#25935;&#24863;&#22320;&#23545;&#24453;&#35821;&#35328;&#12289;&#31038;&#20250;&#20114;&#21160;&#21644;&#25216;&#26415;&#24577;&#24230;&#31561;&#26041;&#38754;&#30340;&#25991;&#21270;&#24046;&#24322;&#12290;&#25105;&#20204;&#26088;&#22312;&#25506;&#35752;&#22312;&#26085;&#26412;&#36825;&#31181;&#38750;&#35199;&#26041;&#29615;&#22659;&#20013;&#20849;&#21516;&#35774;&#35745;VAs&#30340;&#20215;&#20540;&#65292;&#24182;&#23637;&#31034;&#25991;&#21270;&#25935;&#24863;&#24230;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#22312;&#32447;&#24341;&#35825;&#24615;&#30740;&#31350;&#65288;N = 135&#65289;&#65292;&#20854;&#20013;&#32654;&#22269;&#20154;&#65288;n = 64&#65289;&#21644;&#26085;&#26412;&#20154;&#65288;n = 71&#65289;&#26500;&#24819;&#20102;&#26410;&#26469;VAs&#30340;&#23545;&#35805;&#65288;N = 282&#65289;&#21644;&#27963;&#21160;&#65288;N = 73&#65289;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#19982;&#26410;&#26469;VAs&#20849;&#21516;&#26500;&#24819;&#20114;&#21160;&#30340;&#24433;&#21709;&#65292;&#20026;&#26085;&#26412;&#21644;&#33521;&#35821;&#25552;&#20379;&#20102;&#35774;&#35745;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17599v1 Announce Type: cross  Abstract: Voice assistants (VAs) are becoming a feature of our everyday life. Yet, the user experience (UX) is often limited, leading to underuse, disengagement, and abandonment. Co-designing interactions for VAs with potential end-users can be useful. Crowdsourcing this process online and anonymously may add value. However, most work has been done in the English-speaking West on dialogue data sets. We must be sensitive to cultural differences in language, social interactions, and attitudes towards technology. Our aims were to explore the value of co-designing VAs in the non-Western context of Japan and demonstrate the necessity of cultural sensitivity. We conducted an online elicitation study (N = 135) where Americans (n = 64) and Japanese people (n = 71) imagined dialogues (N = 282) and activities (N = 73) with future VAs. We discuss the implications for coimagining interactions with future VAs, offer design guidelines for the Japanese and Eng
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#23545;&#35805;&#26641;&#29983;&#25104;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21487;&#24110;&#21161;&#35757;&#32451;&#20986;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#20195;&#29702;&#36798;&#21040;&#19982;&#22312;&#20154;&#31867;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#23545;&#35805;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.17582</link><description>&lt;p&gt;
&#26397;&#30528;&#38646;&#25968;&#25454;&#12289;&#21487;&#25511;&#12289;&#33258;&#36866;&#24212;&#23545;&#35805;&#31995;&#32479;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Towards a Zero-Data, Controllable, Adaptive Dialog System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17582
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#23545;&#35805;&#26641;&#29983;&#25104;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21487;&#24110;&#21161;&#35757;&#32451;&#20986;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#20195;&#29702;&#36798;&#21040;&#19982;&#22312;&#20154;&#31867;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#23545;&#35805;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#26641;&#25628;&#32034;&#65288;V&#228;th&#31561;&#65292;2023&#24180;&#65289;&#26159;&#19968;&#31181;&#26368;&#36817;&#30340;&#23545;&#35805;&#31995;&#32479;&#25511;&#21046;&#26041;&#27861;&#65292;&#20854;&#20013;&#39046;&#22495;&#19987;&#23478;&#36890;&#36807;&#23545;&#35805;&#26641;&#22609;&#36896;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#34892;&#20026;&#12290;&#20195;&#29702;&#23398;&#20250;&#26377;&#25928;&#22320;&#27983;&#35272;&#36825;&#26869;&#26641;&#65292;&#21516;&#26102;&#36866;&#24212;&#19981;&#21516;&#29992;&#25143;&#30340;&#20449;&#24687;&#38656;&#27714;&#65292;&#20363;&#22914;&#39046;&#22495;&#29087;&#24713;&#24230;&#12290;&#28982;&#32780;&#65292;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#38656;&#27714;&#38459;&#30861;&#20102;&#22312;&#26032;&#39046;&#22495;&#30340;&#37096;&#32626;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#30452;&#25509;&#20174;&#23545;&#35805;&#26641;&#29983;&#25104;&#36825;&#20123;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25913;&#36827;&#20102;&#21407;&#22987;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#20195;&#29702;&#21487;&#20197;&#23454;&#29616;&#19982;&#22312;&#20154;&#31867;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#24403;&#30340;&#23545;&#35805;&#25104;&#21151;&#29575;&#65292;&#26080;&#35770;&#26159;&#20351;&#29992;&#21830;&#19994;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#29983;&#25104;&#65292;&#36824;&#26159;&#20351;&#29992;&#36739;&#23567;&#30340;&#24320;&#28304;&#27169;&#22411;&#65292;&#22312;&#21333;&#20010;GPU&#19978;&#36816;&#34892;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#25910;&#38598;&#21644;&#27979;&#35797;&#20004;&#20010;&#26032;&#25968;&#25454;&#38598;&#26469;&#23637;&#31034;&#25105;&#20204;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#65306;ONBOARD&#65292;&#19968;&#20010;&#24110;&#21161;&#22806;&#22269;&#23621;&#27665;&#25644;&#36801;&#30340;&#26032;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17582v1 Announce Type: cross  Abstract: Conversational Tree Search (V\"ath et al., 2023) is a recent approach to controllable dialog systems, where domain experts shape the behavior of a Reinforcement Learning agent through a dialog tree. The agent learns to efficiently navigate this tree, while adapting to information needs, e.g., domain familiarity, of different users. However, the need for additional training data hinders deployment in new domains. To address this, we explore approaches to generate this data directly from dialog trees. We improve the original approach, and show that agents trained on synthetic data can achieve comparable dialog success to models trained on human data, both when using a commercial Large Language Model for generation, or when using a smaller open-source model, running on a single GPU. We further demonstrate the scalability of our approach by collecting and testing on two new datasets: ONBOARD, a new domain helping foreign residents moving t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25991;&#29486;&#32508;&#36848;&#21644;&#25552;&#20986;&#20998;&#31867;&#27861;&#65292;&#25506;&#35752;&#20102;&#37322;&#20041;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#21457;&#29616;&#24050;&#30693;&#37322;&#20041;&#35821;&#26009;&#24211;&#20013;&#29305;&#23450;&#20219;&#21153;&#23454;&#20363;&#30340;&#20998;&#24067;&#24046;&#24322;&#24456;&#22823;&#12290;</title><link>https://arxiv.org/abs/2403.17564</link><description>&lt;p&gt;
&#38754;&#21521;&#20219;&#21153;&#30340;&#37322;&#20041;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Task-Oriented Paraphrase Analytics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25991;&#29486;&#32508;&#36848;&#21644;&#25552;&#20986;&#20998;&#31867;&#27861;&#65292;&#25506;&#35752;&#20102;&#37322;&#20041;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#21457;&#29616;&#24050;&#30693;&#37322;&#20041;&#35821;&#26009;&#24211;&#20013;&#29305;&#23450;&#20219;&#21153;&#23454;&#20363;&#30340;&#20998;&#24067;&#24046;&#24322;&#24456;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#37322;&#20041;&#26159;&#19968;&#20010;&#23450;&#20041;&#27169;&#31946;&#30340;&#20219;&#21153;&#65292;&#26415;&#35821;&#8220;&#37322;&#20041;&#8221;&#28085;&#30422;&#20102;&#20855;&#26377;&#19981;&#21516;&#29305;&#24449;&#30340;&#25991;&#26412;&#36716;&#25442;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#29616;&#26377;&#30340;&#37322;&#20041;&#30740;&#31350;&#22312;&#20309;&#26102;&#23558;&#19968;&#23545;&#25991;&#26412;&#35270;&#20026;&#37322;&#20041;&#26041;&#38754;&#24212;&#29992;&#20102;&#30456;&#24403;&#19981;&#21516;&#30340;&#65288;&#26174;&#24335;&#21644;&#38544;&#24335;&#65289;&#26631;&#20934;&#65292;&#25152;&#26377;&#36825;&#20123;&#26631;&#20934;&#37117;&#35201;&#27714;&#20551;&#35774;&#26576;&#31181;&#32423;&#21035;&#30340;&#35821;&#20041;&#25110;&#35789;&#27719;&#30456;&#20284;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#25991;&#29486;&#32508;&#36848;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#31867;&#27861;&#26469;&#32452;&#32455;&#24050;&#35782;&#21035;&#20986;&#30340;25&#20010;&#37322;&#20041;&#65288;&#23376;&#65289;&#20219;&#21153;&#12290;&#36890;&#36807;&#35757;&#32451;&#29992;&#20110;&#35782;&#21035;&#32473;&#23450;&#37322;&#20041;&#23454;&#20363;&#36866;&#29992;&#30340;&#20219;&#21153;&#30340;&#20998;&#31867;&#22120;&#65292;&#25105;&#20204;&#21457;&#29616;&#24050;&#30693;&#37322;&#20041;&#35821;&#26009;&#24211;&#20013;&#29305;&#23450;&#20219;&#21153;&#23454;&#20363;&#30340;&#20998;&#24067;&#24046;&#24322;&#24456;&#22823;&#12290;&#36825;&#24847;&#21619;&#30528;&#22312;&#26410;&#28165;&#26970;&#23450;&#20041;&#30456;&#24212;&#37322;&#20041;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#36825;&#20123;&#35821;&#26009;&#24211;&#65288;&#36825;&#26159;&#27491;&#24120;&#24773;&#20917;&#65289;&#20250;&#23548;&#33268;&#26080;&#27861;&#27604;&#36739;&#21644;&#35823;&#23548;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17564v1 Announce Type: new  Abstract: Since paraphrasing is an ill-defined task, the term "paraphrasing" covers text transformation tasks with different characteristics. Consequently, existing paraphrasing studies have applied quite different (explicit and implicit) criteria as to when a pair of texts is to be considered a paraphrase, all of which amount to postulating a certain level of semantic or lexical similarity. In this paper, we conduct a literature review and propose a taxonomy to organize the 25~identified paraphrasing (sub-)tasks. Using classifiers trained to identify the tasks that a given paraphrasing instance fits, we find that the distributions of task-specific instances in the known paraphrase corpora vary substantially. This means that the use of these corpora, without the respective paraphrase conditions being clearly defined (which is the normal case), must lead to incomparable and misleading results.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#35270;&#35273;&#19978;&#19979;&#25991;&#20316;&#20026;&#36890;&#29992;&#30340;&#35821;&#35328;&#26080;&#20851;&#34920;&#31034;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#27169;&#24577;&#25552;&#31034;&#26469;&#25351;&#23548;&#22810;&#27169;&#24577;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#23545;&#19981;&#21516;&#35821;&#35328;&#34920;&#31034;&#30340;&#23545;&#40784;&#65292;&#24182;&#29983;&#25104;&#26465;&#20214;&#35270;&#35273;-&#35821;&#35328;&#35760;&#24518;&#36827;&#34892;&#32763;&#35793;&#12290;</title><link>https://arxiv.org/abs/2403.17556</link><description>&lt;p&gt;
m3P:&#38754;&#21521;&#22810;&#27169;&#24577;&#22810;&#35821;&#35328;&#32763;&#35793;&#30340;&#22810;&#35821;&#22659;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
m3P: Towards Multimodal Multilingual Translation with Multimodal Prompt
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17556
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#35270;&#35273;&#19978;&#19979;&#25991;&#20316;&#20026;&#36890;&#29992;&#30340;&#35821;&#35328;&#26080;&#20851;&#34920;&#31034;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#27169;&#24577;&#25552;&#31034;&#26469;&#25351;&#23548;&#22810;&#27169;&#24577;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#23545;&#19981;&#21516;&#35821;&#35328;&#34920;&#31034;&#30340;&#23545;&#40784;&#65292;&#24182;&#29983;&#25104;&#26465;&#20214;&#35270;&#35273;-&#35821;&#35328;&#35760;&#24518;&#36827;&#34892;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#32763;&#35793;&#36890;&#36807;&#23558;&#25152;&#26377;&#35821;&#35328;&#25237;&#24433;&#21040;&#19968;&#20010;&#20849;&#20139;&#31354;&#38388;&#26469;&#25903;&#25345;&#22810;&#20010;&#32763;&#35793;&#26041;&#21521;&#65292;&#20294;&#30001;&#20110;&#25991;&#26412;&#27169;&#24577;&#20013;&#35821;&#35328;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#23588;&#20854;&#26159;&#24403;&#35821;&#35328;&#25968;&#37327;&#36739;&#22823;&#26102;&#65292;&#32763;&#35793;&#36136;&#37327;&#20250;&#21463;&#21040;&#24433;&#21709;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#35270;&#35273;&#19978;&#19979;&#25991;&#20316;&#20026;&#36890;&#29992;&#30340;&#35821;&#35328;&#26080;&#20851;&#34920;&#31034;&#65292;&#20197;&#20419;&#36827;&#22810;&#35821;&#35328;&#32763;&#35793;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#27169;&#24577;&#25552;&#31034;&#26469;&#25351;&#23548;Multimodal Multilingual&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;m3P&#65289;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#35821;&#35328;&#30340;&#34920;&#31034;&#19982;&#30456;&#21516;&#21547;&#20041;&#23545;&#40784;&#65292;&#24182;&#29983;&#25104;&#29992;&#20110;&#32763;&#35793;&#30340;&#26465;&#20214;&#35270;&#35273;-&#35821;&#35328;&#35760;&#24518;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#25903;&#25345;102&#31181;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#22810;&#27169;&#24577;&#25351;&#20196;&#25968;&#25454;&#38598;&#65288;InstrMulti102&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#23558;&#22270;&#20687;&#35270;&#20026;&#20013;&#22830;&#35821;&#35328;&#26469;&#26368;&#23567;&#21270;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#30340;&#34920;&#31034;&#36317;&#31163;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17556v1 Announce Type: cross  Abstract: Multilingual translation supports multiple translation directions by projecting all languages in a shared space, but the translation quality is undermined by the difference between languages in the text-only modality, especially when the number of languages is large. To bridge this gap, we introduce visual context as the universal language-independent representation to facilitate multilingual translation. In this paper, we propose a framework to leverage the multimodal prompt to guide the Multimodal Multilingual neural Machine Translation (m3P), which aligns the representations of different languages with the same meaning and generates the conditional vision-language memory for translation. We construct a multilingual multimodal instruction dataset (InstrMulti102) to support 102 languages. Our method aims to minimize the representation distance of different languages by regarding the image as a central language. Experimental results sh
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#20420;&#35821;&#30340;&#20559;&#35265;&#26816;&#27979;&#25968;&#25454;&#38598;RuBia&#65292;&#22635;&#34917;&#20102;&#22810;&#35821;&#35328;&#20559;&#35265;&#35780;&#20272;&#33539;&#22260;&#30340;&#31354;&#30333;&#65292;&#23545;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20420;&#35821;&#20013;&#26159;&#21542;&#23384;&#22312;&#20559;&#35265;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2403.17553</link><description>&lt;p&gt;
RuBia: &#19968;&#20010;&#20420;&#35821;&#35821;&#35328;&#20559;&#35265;&#26816;&#27979;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
RuBia: A Russian Language Bias Detection Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#20420;&#35821;&#30340;&#20559;&#35265;&#26816;&#27979;&#25968;&#25454;&#38598;RuBia&#65292;&#22635;&#34917;&#20102;&#22810;&#35821;&#35328;&#20559;&#35265;&#35780;&#20272;&#33539;&#22260;&#30340;&#31354;&#30333;&#65292;&#23545;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20420;&#35821;&#20013;&#26159;&#21542;&#23384;&#22312;&#20559;&#35265;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24448;&#24448;&#20250;&#23398;&#20064;&#21407;&#22987;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#31038;&#20250;&#25991;&#21270;&#20559;&#35265;&#12290;&#20026;&#20102;&#27979;&#35797;LLM&#30340;&#34892;&#20026;&#26159;&#21542;&#20844;&#24179;&#65292;&#38656;&#35201;&#20351;&#29992;&#21151;&#33021;&#24615;&#25968;&#25454;&#38598;&#65292;&#32780;&#36825;&#20123;&#25968;&#25454;&#38598;&#30001;&#20110;&#35774;&#35745;&#30446;&#30340;&#65292;&#36890;&#24120;&#39640;&#24230;&#20381;&#36182;&#20110;&#35821;&#35328;&#21644;&#25991;&#21270;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#29305;&#21035;&#38024;&#23545;&#20420;&#35821;&#35774;&#35745;&#30340;&#20559;&#35265;&#26816;&#27979;&#25968;&#25454;&#38598;RuBia&#65292;&#22635;&#34917;&#20102;&#22810;&#35821;&#35328;&#20559;&#35265;&#35780;&#20272;&#33539;&#22260;&#30340;&#31354;&#30333;&#12290;RuBia&#25968;&#25454;&#38598;&#20998;&#20026;4&#20010;&#39046;&#22495;&#65306;&#24615;&#21035;&#12289;&#22269;&#31821;&#12289;&#31038;&#20250;&#32463;&#27982;&#22320;&#20301;&#21644;&#22810;&#20803;&#21270;&#65292;&#27599;&#20010;&#39046;&#22495;&#21448;&#36827;&#19968;&#27493;&#20998;&#20026;&#22810;&#20010;&#32454;&#31890;&#24230;&#23376;&#22495;&#12290;&#25968;&#25454;&#38598;&#20013;&#30340;&#27599;&#20010;&#31034;&#20363;&#21253;&#21547;&#20004;&#20010;&#21477;&#23376;&#65292;&#31532;&#19968;&#20010;&#21477;&#23376;&#24378;&#21270;&#20102;&#19968;&#20010;&#28508;&#22312;&#30340;&#26377;&#23475;&#21051;&#26495;&#21360;&#35937;&#25110;&#27169;&#24335;&#65292;&#31532;&#20108;&#20010;&#21477;&#23376;&#21017;&#19982;&#20043;&#30456;&#30683;&#30462;&#12290;&#36825;&#20123;&#21477;&#23545;&#39318;&#20808;&#30001;&#24535;&#24895;&#32773;&#32534;&#20889;&#65292;&#28982;&#21518;&#30001;&#27597;&#35821;&#20154;&#22763;&#30340;&#20247;&#21253;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17553v1 Announce Type: new  Abstract: Warning: this work contains upsetting or disturbing content.   Large language models (LLMs) tend to learn the social and cultural biases present in the raw pre-training data. To test if an LLM's behavior is fair, functional datasets are employed, and due to their purpose, these datasets are highly language and culture-specific. In this paper, we address a gap in the scope of multilingual bias evaluation by presenting a bias detection dataset specifically designed for the Russian language, dubbed as RuBia. The RuBia dataset is divided into 4 domains: gender, nationality, socio-economic status, and diverse, each of the domains is further divided into multiple fine-grained subdomains. Every example in the dataset consists of two sentences with the first reinforcing a potentially harmful stereotype or trope and the second contradicting it. These sentence pairs were first written by volunteers and then validated by native-speaking crowdsourci
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26420;&#32032;&#36125;&#21494;&#26031;&#30340;&#19978;&#19979;&#25991;&#25193;&#23637;&#26694;&#26550;(NBCE)&#65292;&#33021;&#22815;&#36890;&#36807;&#26174;&#33879;&#25193;&#23637;&#19978;&#19979;&#25991;&#22823;&#23567;&#20351;&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25191;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#20197;&#25972;&#21512;&#26356;&#22810;&#28436;&#31034;&#31034;&#20363;&#65292;&#32780;&#19988;&#19981;&#38656;&#35201;&#24494;&#35843;&#25110;&#20381;&#36182;&#29305;&#23450;&#27169;&#22411;&#26550;&#26500;&#12290;</title><link>https://arxiv.org/abs/2403.17552</link><description>&lt;p&gt;
&#22522;&#20110;&#26420;&#32032;&#36125;&#21494;&#26031;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#19978;&#19979;&#25991;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Naive Bayes-based Context Extension for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17552
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26420;&#32032;&#36125;&#21494;&#26031;&#30340;&#19978;&#19979;&#25991;&#25193;&#23637;&#26694;&#26550;(NBCE)&#65292;&#33021;&#22815;&#36890;&#36807;&#26174;&#33879;&#25193;&#23637;&#19978;&#19979;&#25991;&#22823;&#23567;&#20351;&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25191;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#20197;&#25972;&#21512;&#26356;&#22810;&#28436;&#31034;&#31034;&#20363;&#65292;&#32780;&#19988;&#19981;&#38656;&#35201;&#24494;&#35843;&#25110;&#20381;&#36182;&#29305;&#23450;&#27169;&#22411;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#29616;&#20986;&#20196;&#20154;&#26399;&#24453;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#26041;&#27861;&#24120;&#24120;&#21463;&#21040;&#36716;&#25442;&#22120;&#26550;&#26500;&#38271;&#24230;&#38480;&#21046;&#30340;&#38459;&#30861;&#65292;&#22312;&#35797;&#22270;&#26377;&#25928;&#25972;&#21512;&#22823;&#37327;&#28436;&#31034;&#31034;&#20363;&#30340;&#30417;&#30563;&#26102;&#38754;&#20020;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#22522;&#20110;&#26420;&#32032;&#36125;&#21494;&#26031;&#30340;&#19978;&#19979;&#25991;&#25193;&#23637;(NBCE)&#65292;&#20197;&#20351;&#29616;&#26377;&#30340;LLMs&#33021;&#22815;&#36890;&#36807;&#26174;&#33879;&#25193;&#23637;&#19978;&#19979;&#25991;&#22823;&#23567;&#25191;&#34892;ICL&#65292;&#20174;&#32780;&#22686;&#21152;&#28436;&#31034;&#31034;&#20363;&#30340;&#25968;&#37327;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#36825;&#31181;&#25193;&#23637;&#19981;&#38656;&#35201;&#24494;&#35843;&#25110;&#20381;&#36182;&#29305;&#23450;&#30340;&#27169;&#22411;&#26550;&#26500;&#65292;&#21516;&#26102;&#20445;&#25345;&#32447;&#24615;&#25928;&#29575;&#12290;NBCE&#39318;&#20808;&#23558;&#19978;&#19979;&#25991;&#20998;&#21106;&#25104;&#36866;&#21512;&#30446;&#26631;LLM&#26368;&#22823;&#38271;&#24230;&#30340;&#31561;&#22823;&#23567;&#31383;&#21475;&#12290;&#28982;&#21518;&#65292;&#23427;&#24341;&#20837;&#20102;&#19968;&#20010;&#25237;&#31080;&#26426;&#21046;&#26469;&#36873;&#25321;&#26368;&#30456;&#20851;&#30340;&#31383;&#21475;&#65292;&#34987;&#35270;&#20026;&#21518;&#39564;&#19978;&#19979;&#25991;&#12290;&#26368;&#21518;&#65292;&#23427;&#24212;&#29992;&#36125;&#21494;&#26031;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17552v1 Announce Type: new  Abstract: Large Language Models (LLMs) have shown promising in-context learning abilities. However, conventional In-Context Learning (ICL) approaches are often impeded by length limitations of transformer architecture, which pose challenges when attempting to effectively integrate supervision from a substantial number of demonstration examples. In this paper, we introduce a novel framework, called Naive Bayes-based Context Extension (NBCE), to enable existing LLMs to perform ICL with an increased number of demonstrations by significantly expanding their context size. Importantly, this expansion does not require fine-tuning or dependence on particular model architectures, all the while preserving linear efficiency. NBCE initially splits the context into equal-sized windows fitting the target LLM's maximum length. Then, it introduces a voting mechanism to select the most relevant window, regarded as the posterior context. Finally, it employs Bayes' 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25991;&#26412;&#25366;&#25496;&#21644;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;OM&#21644;SCM&#19987;&#19994;&#20154;&#21592;&#29305;&#23450;&#24515;&#29702;&#29305;&#36136;&#30340;&#24066;&#22330;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2403.17546</link><description>&lt;p&gt;
&#35299;&#35835;&#21331;&#36234;&#65306;&#36890;&#36807;&#25991;&#26412;&#25366;&#25496;&#25581;&#31034;&#36816;&#33829;&#21644;&#20379;&#24212;&#38142;&#19987;&#19994;&#20154;&#21592;&#24515;&#29702;&#29305;&#36136;&#30340;&#38656;&#27714;
&lt;/p&gt;
&lt;p&gt;
Decoding excellence: Mapping the demand for psychological traits of operations and supply chain professionals through text mining
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17546
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25991;&#26412;&#25366;&#25496;&#21644;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;OM&#21644;SCM&#19987;&#19994;&#20154;&#21592;&#29305;&#23450;&#24515;&#29702;&#29305;&#36136;&#30340;&#24066;&#22330;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25991;&#26412;&#25366;&#25496;&#21644;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#65292;&#23545;&#36816;&#33829;&#31649;&#29702;&#65288;OM&#65289;&#21644;&#20379;&#24212;&#38142;&#31649;&#29702;&#65288;SCM&#65289;&#19987;&#19994;&#20154;&#21592;&#30340;&#24515;&#29702;&#29305;&#36136;&#36827;&#34892;&#25551;&#36848;&#20998;&#26512;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#32467;&#21512;&#30456;&#20851;&#24515;&#29702;&#26500;&#24314;&#12289;&#25991;&#26412;&#25366;&#25496;&#25216;&#26415;&#21644;&#21019;&#26032;&#24230;&#37327;&#26041;&#27861;&#65292;&#21363;&#35821;&#20041;&#21697;&#29260;&#20998;&#25968;&#65292;&#35780;&#20272;&#29305;&#23450;&#29305;&#36136;&#30340;&#24066;&#22330;&#38656;&#27714;&#12290;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#19968;&#32452;OM&#21644;SCM&#19987;&#19994;&#20154;&#21592;&#30340;&#32844;&#20301;&#25551;&#36848;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25552;&#20379;&#20182;&#20204;&#30456;&#20851;&#25152;&#38656;&#25216;&#33021;&#30340;&#25551;&#36848;&#20998;&#24067;&#22270;&#65292;&#21253;&#25324;&#24515;&#29702;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17546v1 Announce Type: new  Abstract: The current study proposes an innovative methodology for the profiling of psychological traits of Operations Management (OM) and Supply Chain Management (SCM) professionals. We use innovative methods and tools of text mining and social network analysis to map the demand for relevant skills from a set of job descriptions, with a focus on psychological characteristics. The proposed approach aims to evaluate the market demand for specific traits by combining relevant psychological constructs, text mining techniques, and an innovative measure, namely, the Semantic Brand Score. We apply the proposed methodology to a dataset of job descriptions for OM and SCM professionals, with the objective of providing a mapping of their relevant required skills, including psychological characteristics. In addition, the analysis is then detailed by considering the region of the organization that issues the job description, its organizational size, and the s
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102; GazeVQA &#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#27880;&#35270;&#20449;&#24687;&#26469;&#28548;&#28165;&#27169;&#31946;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#27880;&#35270;&#30446;&#26631;&#20272;&#35745;&#32467;&#26524;&#30340;&#26041;&#27861;&#20197;&#25552;&#39640;&#20219;&#21153;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.17545</link><description>&lt;p&gt;
&#29992;&#20110;&#28548;&#28165;&#26085;&#35821;&#27169;&#31946;&#38382;&#39064;&#30340;&#27880;&#35270;&#22522;&#30784;&#35270;&#35273;&#38382;&#31572;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
A Gaze-grounded Visual Question Answering Dataset for Clarifying Ambiguous Japanese Questions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17545
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102; GazeVQA &#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#27880;&#35270;&#20449;&#24687;&#26469;&#28548;&#28165;&#27169;&#31946;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#27880;&#35270;&#30446;&#26631;&#20272;&#35745;&#32467;&#26524;&#30340;&#26041;&#27861;&#20197;&#25552;&#39640;&#20219;&#21153;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20301;&#20110;&#23545;&#35805;&#65292;&#25351;&#30340;&#26159;&#23558;&#35270;&#35273;&#20449;&#24687;&#31216;&#20026;&#35270;&#35273;&#38382;&#31572;(VQA)&#30340;&#24773;&#22659;&#35848;&#35805;&#65292;&#24120;&#24120;&#21547;&#26377;&#30001;&#20110;&#20381;&#36182;&#25351;&#31034;&#24615;&#20449;&#24687;&#32780;&#20135;&#29983;&#30340;&#27169;&#31946;&#24615;&#12290;&#36825;&#20010;&#38382;&#39064;&#21152;&#21095;&#20102;&#65292;&#22240;&#20026;&#19968;&#20123;&#35821;&#35328;&#65292;&#27604;&#22914;&#26085;&#35821;&#65292;&#24120;&#24120;&#30465;&#30053;&#20027;&#35266;&#25110;&#23458;&#35266;&#26415;&#35821;&#12290;&#36825;&#31181;&#38382;&#39064;&#20013;&#30340;&#27169;&#31946;&#24615;&#36890;&#24120;&#36890;&#36807;&#23545;&#35805;&#24773;&#26223;&#20013;&#30340;&#35821;&#22659;&#26469;&#28548;&#28165;&#65292;&#27604;&#22914;&#19982;&#29992;&#25143;&#30340;&#32852;&#21512;&#20851;&#27880;&#25110;&#29992;&#25143;&#27880;&#35270;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#27880;&#35270;&#20449;&#24687;&#28548;&#28165;&#27169;&#31946;&#38382;&#39064;&#30340;GazeVQA&#25968;&#25454;&#38598;(Gaze-grounded VQA dataset) &#65292;&#37325;&#28857;&#20851;&#27880;&#30001;&#27880;&#35270;&#20449;&#24687;&#36741;&#21161;&#30340;&#28548;&#28165;&#36807;&#31243;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#27880;&#35270;&#30446;&#26631;&#20272;&#35745;&#32467;&#26524;&#26469;&#25552;&#39640;GazeVQA&#20219;&#21153;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25913;&#21892;&#20102;VQA&#31995;&#32479;&#22312;GazeVQA&#19978;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#30830;&#23450;&#20102;&#19968;&#20123;GazeVQA&#20219;&#21153;&#20013;&#38656;&#35201;&#25913;&#36827;&#30340;&#20856;&#22411;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17545v1 Announce Type: new  Abstract: Situated conversations, which refer to visual information as visual question answering (VQA), often contain ambiguities caused by reliance on directive information. This problem is exacerbated because some languages, such as Japanese, often omit subjective or objective terms. Such ambiguities in questions are often clarified by the contexts in conversational situations, such as joint attention with a user or user gaze information. In this study, we propose the Gaze-grounded VQA dataset (GazeVQA) that clarifies ambiguous questions using gaze information by focusing on a clarification process complemented by gaze information. We also propose a method that utilizes gaze target estimation results to improve the accuracy of GazeVQA tasks. Our experimental results showed that the proposed method improved the performance in some cases of a VQA system on GazeVQA and identified some typical problems of GazeVQA tasks that need to be improved.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-4&#22312;&#35821;&#27861;&#38169;&#35823;&#26657;&#27491;&#35780;&#20272;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#19982;&#20154;&#31867;&#21028;&#26029;&#26377;&#24456;&#39640;&#30340;&#30456;&#20851;&#24615;&#65292;&#20984;&#26174;&#20102;&#22312;&#35780;&#20272;&#26631;&#20934;&#20013;&#27969;&#30021;&#24230;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.17540</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#35821;&#27861;&#38169;&#35823;&#26657;&#27491;&#30340;&#26368;&#20808;&#36827;&#35780;&#20272;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Are State-of-the-Art Evaluator for Grammatical Error Correction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17540
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-4&#22312;&#35821;&#27861;&#38169;&#35823;&#26657;&#27491;&#35780;&#20272;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#19982;&#20154;&#31867;&#21028;&#26029;&#26377;&#24456;&#39640;&#30340;&#30456;&#20851;&#24615;&#65292;&#20984;&#26174;&#20102;&#22312;&#35780;&#20272;&#26631;&#20934;&#20013;&#27969;&#30021;&#24230;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25454;&#25253;&#36947;&#22312;&#26576;&#20123;&#20219;&#21153;&#20013;&#32988;&#36807;&#29616;&#26377;&#30340;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#65292;&#27604;&#22914;&#25991;&#26412;&#24635;&#32467;&#21644;&#26426;&#22120;&#32763;&#35793;&#12290;&#20294;&#26159;&#65292;&#20851;&#20110;LLMs&#22312;&#35821;&#27861;&#38169;&#35823;&#26657;&#27491;&#65288;GEC&#65289;&#20013;&#20316;&#20026;&#35780;&#20272;&#22120;&#30340;&#30740;&#31350;&#36824;&#19981;&#36275;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#37319;&#29992;&#35774;&#35745;&#30340;&#25552;&#31034;&#26469;&#25972;&#21512;&#20808;&#21069;&#30740;&#31350;&#21551;&#21457;&#30340;&#21508;&#31181;&#35780;&#20272;&#26631;&#20934;&#65292;&#35843;&#26597;&#20102;LLMs&#22312;GEC&#35780;&#20272;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-4&#22312;&#19982;&#20154;&#31867;&#21028;&#26029;&#20043;&#38388;&#36798;&#21040;&#20102;0.662&#30340;Kendall&#31561;&#32423;&#30456;&#20851;&#24615;&#65292;&#36229;&#36807;&#20102;&#25152;&#26377;&#29616;&#26377;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#22312;&#26368;&#36817;&#30340;GEC&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;LLMs&#35268;&#27169;&#30340;&#37325;&#35201;&#24615;&#65292;&#29305;&#21035;&#24378;&#35843;&#20102;&#27969;&#30021;&#24230;&#22312;&#35780;&#20272;&#26631;&#20934;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17540v1 Announce Type: new  Abstract: Large Language Models (LLMs) have been reported to outperform existing automatic evaluation metrics in some tasks, such as text summarization and machine translation. However, there has been a lack of research on LLMs as evaluators in grammatical error correction (GEC). In this study, we investigate the performance of LLMs in GEC evaluation by employing prompts designed to incorporate various evaluation criteria inspired by previous research. Our extensive experimental results demonstrate that GPT-4 achieved Kendall's rank correlation of 0.662 with human judgments, surpassing all existing methods. Furthermore, in recent GEC evaluations, we have underscored the significance of the LLMs scale and particularly emphasized the importance of fluency among evaluation criteria.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#25351;&#20196;&#35843;&#25972;&#27169;&#22411;&#30340;ILLUMINER&#26041;&#27861;&#22312;&#24847;&#22270;&#20998;&#31867;&#21644;&#27133;&#20301;&#22635;&#20805;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26356;&#39640;&#25928;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#24182;&#22312;&#27133;&#20301;&#22635;&#20805;&#26041;&#38754;&#20248;&#20110;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.17536</link><description>&lt;p&gt;
ILLUMINER: &#25351;&#20196;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#23569;&#26679;&#26412;&#24847;&#22270;&#20998;&#31867;&#22120;&#21644;&#27133;&#20301;&#22635;&#20805;&#22120;
&lt;/p&gt;
&lt;p&gt;
ILLUMINER: Instruction-tuned Large Language Models as Few-shot Intent Classifier and Slot Filler
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17536
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#25351;&#20196;&#35843;&#25972;&#27169;&#22411;&#30340;ILLUMINER&#26041;&#27861;&#22312;&#24847;&#22270;&#20998;&#31867;&#21644;&#27133;&#20301;&#22635;&#20805;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26356;&#39640;&#25928;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#24182;&#22312;&#27133;&#20301;&#22635;&#20805;&#26041;&#38754;&#20248;&#20110;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#24847;&#22270;&#20998;&#31867;&#65288;IC&#65289;&#21644;&#27133;&#20301;&#22635;&#20805;&#65288;SF&#65289;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#25968;&#25454;&#23494;&#38598;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#25351;&#20196;&#35843;&#25972;&#27169;&#22411;&#65288;Instruct-LLMs&#65289;&#65292;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;Instruct-LLMs&#22312;&#27969;&#34892;&#30340;IC&#21644;SF&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#24378;&#35843;&#23427;&#20204;&#20174;&#26356;&#23569;&#31034;&#20363;&#20013;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102; ILLUMINER&#65292;&#19968;&#31181;&#23558;IC&#21644;SF&#26500;&#24314;&#20026;Instruct-LLMs&#20013;&#30340;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20043;&#21069;&#30340;&#24037;&#20316;&#65292;&#20855;&#26377;&#26356;&#39640;&#25928;&#30340;SF&#25552;&#31034;&#26041;&#27861;&#12290;&#19982;&#22810;&#20010;&#22522;&#32447;&#26041;&#27861;&#30340;&#20840;&#38754;&#27604;&#36739;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;FLAN-T5 11B&#27169;&#22411;&#65292;&#22312;&#27133;&#20301;&#22635;&#20805;&#26041;&#38754;&#27604;&#26368;&#20808;&#36827;&#30340;&#32852;&#21512;IC + SF&#26041;&#27861;&#21644;GPT3.5 (175B)&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#34920;&#29616;&#26356;&#22909;&#65292;&#27133;&#20301;&#22635;&#20805;&#26041;&#38754;&#25552;&#39640;&#20102;11.1-32.2&#20010;&#30334;&#20998;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#28145;&#20837;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17536v1 Announce Type: new  Abstract: State-of-the-art intent classification (IC) and slot filling (SF) methods often rely on data-intensive deep learning models, limiting their practicality for industry applications. Large language models on the other hand, particularly instruction-tuned models (Instruct-LLMs), exhibit remarkable zero-shot performance across various natural language tasks. This study evaluates Instruct-LLMs on popular benchmark datasets for IC and SF, emphasizing their capacity to learn from fewer examples. We introduce ILLUMINER, an approach framing IC and SF as language generation tasks for Instruct-LLMs, with a more efficient SF-prompting method compared to prior work. A comprehensive comparison with multiple baselines shows that our approach, using the FLAN-T5 11B model, outperforms the state-of-the-art joint IC+SF method and in-context learning with GPT3.5 (175B), particularly in slot filling by 11.1--32.2 percentage points. Additionally, our in-depth 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#20174;&#26641;&#24211;&#20013;&#25552;&#21462;&#21644;&#25506;&#32034;&#26174;&#33879;&#30340;&#32454;&#31890;&#24230;&#35821;&#27861;&#27169;&#24335;&#21644;&#28508;&#22312;&#30340;&#21477;&#27861;&#35821;&#27861;&#35268;&#21017;&#65292;&#20197;&#21019;&#24314;&#26131;&#20110;&#29702;&#35299;&#30340;&#22522;&#20110;&#35821;&#26009;&#24211;&#30340;&#35821;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.17534</link><description>&lt;p&gt;
&#20351;&#29992;&#39640;&#38454;&#29305;&#24449;&#30340;&#31232;&#30095;&#36923;&#36753;&#22238;&#24402;&#29992;&#20110;&#20174;&#26641;&#24211;&#20013;&#33258;&#21160;&#25552;&#21462;&#35821;&#27861;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
Sparse Logistic Regression with High-order Features for Automatic Grammar Rule Extraction from Treebanks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17534
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#20174;&#26641;&#24211;&#20013;&#25552;&#21462;&#21644;&#25506;&#32034;&#26174;&#33879;&#30340;&#32454;&#31890;&#24230;&#35821;&#27861;&#27169;&#24335;&#21644;&#28508;&#22312;&#30340;&#21477;&#27861;&#35821;&#27861;&#35268;&#21017;&#65292;&#20197;&#21019;&#24314;&#26131;&#20110;&#29702;&#35299;&#30340;&#22522;&#20110;&#35821;&#26009;&#24211;&#30340;&#35821;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25551;&#36848;&#24615;&#35821;&#27861;&#38750;&#24120;&#26377;&#20215;&#20540;&#65292;&#20294;&#32534;&#20889;&#23427;&#20204;&#24456;&#32791;&#26102;&#19988;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#35821;&#35328;&#23398;&#23478;&#36890;&#24120;&#20351;&#29992;&#35821;&#26009;&#24211;&#26469;&#21019;&#24314;&#23427;&#20204;&#65292;&#35821;&#27861;&#25551;&#36848;&#36890;&#24120;&#32570;&#20047;&#23450;&#37327;&#25968;&#25454;&#12290;&#33267;&#20110;&#24418;&#24335;&#21270;&#35821;&#27861;&#65292;&#20854;&#35299;&#37322;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#26641;&#24211;&#20013;&#25552;&#21462;&#21644;&#25506;&#32034;&#26174;&#33879;&#30340;&#32454;&#31890;&#24230;&#35821;&#27861;&#27169;&#24335;&#21644;&#28508;&#22312;&#30340;&#21477;&#27861;&#35821;&#27861;&#35268;&#21017;&#65292;&#20197;&#21019;&#24314;&#19968;&#20010;&#26131;&#20110;&#29702;&#35299;&#30340;&#22522;&#20110;&#35821;&#26009;&#24211;&#30340;&#35821;&#27861;&#12290;&#26356;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20351;&#29992;&#32447;&#24615;&#20998;&#31867;&#22120;&#20174;&#19981;&#21516;&#35821;&#35328;&#20013;&#25552;&#21462;&#20004;&#31181;&#35821;&#35328;&#29616;&#35937;&#65292;&#21363;&#21327;&#35758;&#21644;&#35789;&#24207;&#65292;&#30340;&#25551;&#36848;&#21644;&#35268;&#21017;&#65292;&#20351;&#29992;&#22823;&#37327;&#30340;&#25628;&#32034;&#31354;&#38388;&#24182;&#29305;&#21035;&#20851;&#27880;&#25552;&#21462;&#35268;&#21017;&#30340;&#25490;&#24207;&#39034;&#24207;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#32447;&#24615;&#20998;&#31867;&#22120;&#25552;&#21462;&#26368;&#26174;&#33879;&#30340;&#29305;&#24449;&#65292;&#20197;&#39044;&#27979;&#25152;&#30740;&#31350;&#30340;&#35821;&#35328;&#29616;&#35937;&#12290;&#25105;&#20204;&#20026;&#27599;&#20010;&#35268;&#21017;&#20851;&#32852;&#32479;&#35745;&#20449;&#24687;&#65292;&#24182;&#27604;&#36739;&#27169;&#22411;&#32467;&#26524;&#30340;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17534v1 Announce Type: new  Abstract: Descriptive grammars are highly valuable, but writing them is time-consuming and difficult. Furthermore, while linguists typically use corpora to create them, grammar descriptions often lack quantitative data. As for formal grammars, they can be challenging to interpret. In this paper, we propose a new method to extract and explore significant fine-grained grammar patterns and potential syntactic grammar rules from treebanks, in order to create an easy-to-understand corpus-based grammar. More specifically, we extract descriptions and rules across different languages for two linguistic phenomena, agreement and word order, using a large search space and paying special attention to the ranking order of the extracted rules. For that, we use a linear classifier to extract the most salient features that predict the linguistic phenomena under study. We associate statistical information to each rule, and we compare the ranking of the model's res
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;NLI&#30340;&#22810;&#35821;&#35328;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;Multilingual Sentence T5&#65292;&#36890;&#36807;&#20302;&#31209;&#36866;&#24212;&#25216;&#26415;&#25104;&#21151;&#23558;&#27169;&#22411;&#35268;&#27169;&#25193;&#23637;&#21040;57&#20159;&#21442;&#25968;&#65292;&#24182;&#23454;&#29616;&#20102;&#20248;&#20110;&#20808;&#21069;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.17528</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#21477;&#23376;-T5: &#29992;&#20110;&#22810;&#35821;&#35328;&#24212;&#29992;&#30340;&#21487;&#25193;&#23637;&#21477;&#23376;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Multilingual Sentence-T5: Scalable Sentence Encoders for Multilingual Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17528
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;NLI&#30340;&#22810;&#35821;&#35328;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;Multilingual Sentence T5&#65292;&#36890;&#36807;&#20302;&#31209;&#36866;&#24212;&#25216;&#26415;&#25104;&#21151;&#23558;&#27169;&#22411;&#35268;&#27169;&#25193;&#23637;&#21040;57&#20159;&#21442;&#25968;&#65292;&#24182;&#23454;&#29616;&#20102;&#20248;&#20110;&#20808;&#21069;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#20851;&#20110;&#22810;&#35821;&#35328;&#21477;&#23376;&#23884;&#20837;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#26377;&#25928;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#25968;&#25454;&#26469;&#26500;&#24314;&#39640;&#24615;&#33021;&#27169;&#22411;&#33021;&#22815;&#32988;&#36807;&#20256;&#32479;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#8220;&#25351;&#25968;&#8221;&#22686;&#38271;&#30340;&#25317;&#26377;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#22312;&#22909;&#22788;&#23578;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25193;&#23637;&#29616;&#26377;&#30340;&#21333;&#35821;&#35328;&#27169;&#22411;Sentence T5&#65292;&#24341;&#20837;&#20102;Multilingual Sentence T5&#65288;m-ST5&#65289;&#65292;&#20316;&#20026;&#19968;&#20010;&#26356;&#22823;&#30340;&#22522;&#20110;NLI&#30340;&#22810;&#35821;&#35328;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#25216;&#26415;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#23558;&#27169;&#22411;&#30340;&#35268;&#27169;&#25193;&#23637;&#21040;57&#20159;&#20010;&#21442;&#25968;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#26469;&#35780;&#20272;&#21477;&#23376;&#23884;&#20837;&#30340;&#24615;&#33021;&#65292;&#24182;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#20248;&#20110;&#22522;&#20110;NLI&#30340;&#20808;&#21069;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30830;&#35748;&#20102;&#27169;&#22411;&#35268;&#27169;&#19982;&#24615;&#33021;&#20043;&#38388;&#30340;&#27491;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17528v1 Announce Type: new  Abstract: Prior work on multilingual sentence embedding has demonstrated that the efficient use of natural language inference (NLI) data to build high-performance models can outperform conventional methods. However, the potential benefits from the recent ``exponential'' growth of language models with billions of parameters have not yet been fully explored. In this paper, we introduce Multilingual Sentence T5 (m-ST5), as a larger model of NLI-based multilingual sentence embedding, by extending Sentence T5, an existing monolingual model. By employing the low-rank adaptation (LoRA) technique, we have achieved a successful scaling of the model's size to 5.7 billion parameters. We conducted experiments to evaluate the performance of sentence embedding and verified that the method outperforms the NLI-based prior approach. Furthermore, we also have confirmed a positive correlation between the size of the model and its performance. It was particularly not
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SyncPool&#30340;&#26032;&#39062;&#23433;&#20840;&#28040;&#38500;&#27495;&#20041;&#26041;&#27861;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#31070;&#32463;&#35821;&#35328;&#38544;&#20889;&#26415;&#20013;&#30340;&#20998;&#35789;&#27169;&#31946;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.17524</link><description>&lt;p&gt;
&#21487;&#35777;&#23433;&#20840;&#30340;&#31070;&#32463;&#35821;&#35328;&#38544;&#20889;&#26415;&#28040;&#38500;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Provably Secure Disambiguating Neural Linguistic Steganography
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17524
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SyncPool&#30340;&#26032;&#39062;&#23433;&#20840;&#28040;&#38500;&#27495;&#20041;&#26041;&#27861;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#31070;&#32463;&#35821;&#35328;&#38544;&#20889;&#26415;&#20013;&#30340;&#20998;&#35789;&#27169;&#31946;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21487;&#35777;&#23433;&#20840;&#30340;&#31070;&#32463;&#35821;&#35328;&#38544;&#20889;&#26415;&#24573;&#30053;&#20102;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#65306;&#21457;&#36865;&#26041;&#24517;&#39035;&#23545;&#38544;&#20889;&#25991;&#26412;&#36827;&#34892;&#21435;&#35760;&#21495;&#21270;&#65292;&#20197;&#36991;&#20813;&#24341;&#36215;&#31363;&#21548;&#32773;&#30340;&#24576;&#30097;&#12290;&#22522;&#20110;&#23376;&#35789;&#30340;&#35821;&#35328;&#27169;&#22411;&#20250;&#23548;&#33268;&#20998;&#35789;&#27169;&#31946;&#38382;&#39064;&#65292;&#22312;&#25152;&#26377;&#22522;&#20110;&#36825;&#20123;&#27169;&#22411;&#30340;&#31070;&#32463;&#35821;&#35328;&#38544;&#20889;&#26415;&#23454;&#29616;&#20013;&#20598;&#23572;&#20250;&#20986;&#29616;&#35299;&#30721;&#22833;&#36133;&#12290;&#30446;&#21069;&#35299;&#20915;&#27492;&#38382;&#39064;&#30340;&#26041;&#27861;&#21253;&#25324;&#26356;&#25913;&#20505;&#36873;&#35789;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#20351;&#20854;&#19982;&#21487;&#35777;&#23433;&#20840;&#30340;&#38544;&#20889;&#26415;&#19981;&#30456;&#23481;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SyncPool&#30340;&#26032;&#39062;&#23433;&#20840;&#28040;&#38500;&#27495;&#20041;&#26041;&#27861;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#20998;&#35789;&#27169;&#31946;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#38544;&#20889;&#23884;&#20837;&#31639;&#27861;&#36816;&#34892;&#20043;&#21069;&#23558;&#25152;&#26377;&#20855;&#26377;&#21069;&#32512;&#20851;&#31995;&#30340;&#20196;&#29260;&#20998;&#32452;&#22312;&#20505;&#36873;&#27744;&#20013;&#65292;&#20197;&#28040;&#38500;&#27169;&#31946;&#20196;&#29260;&#20043;&#38388;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#20351;&#25509;&#25910;&#26041;&#33021;&#22815;&#21516;&#27493;&#21457;&#36865;&#26041;&#30340;&#37319;&#26679;&#36807;&#31243;&#65292;&#20351;&#29992;&#20102;&#19968;&#20010;&#20849;&#20139;&#23494;&#30721;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17524v1 Announce Type: cross  Abstract: Recent research in provably secure neural linguistic steganography has overlooked a crucial aspect: the sender must detokenize stegotexts to avoid raising suspicion from the eavesdropper. The segmentation ambiguity problem, which arises when using language models based on subwords, leads to occasional decoding failures in all neural language steganography implementations based on these models. Current solutions to this issue involve altering the probability distribution of candidate words, rendering them incompatible with provably secure steganography. We propose a novel secure disambiguation method named SyncPool, which effectively addresses the segmentation ambiguity problem. We group all tokens with prefix relationships in the candidate pool before the steganographic embedding algorithm runs to eliminate uncertainty among ambiguous tokens. To enable the receiver to synchronize the sampling process of the sender, a shared cryptograph
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#27604;&#36739;&#39044;&#27979;&#25991;&#26412;&#23884;&#20837;&#30340;&#33041;&#27963;&#21160;&#26144;&#23556;&#26469;&#25351;&#23548;&#25991;&#26412;&#37325;&#24314;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#26041;&#27861;&#65292;&#30456;&#27604;&#20043;&#21069;&#30340;&#38388;&#25509;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.17516</link><description>&lt;p&gt;
MapGuide: &#20174;&#33041;&#27963;&#21160;&#20013;&#37325;&#24314;&#36830;&#32493;&#35821;&#35328;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MapGuide: A Simple yet Effective Method to Reconstruct Continuous Language from Brain Activities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#27604;&#36739;&#39044;&#27979;&#25991;&#26412;&#23884;&#20837;&#30340;&#33041;&#27963;&#21160;&#26144;&#23556;&#26469;&#25351;&#23548;&#25991;&#26412;&#37325;&#24314;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#26041;&#27861;&#65292;&#30456;&#27604;&#20043;&#21069;&#30340;&#38388;&#25509;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#33041;&#27963;&#21160;&#20013;&#35299;&#30721;&#36830;&#32493;&#35821;&#35328;&#26159;&#19968;&#39033;&#33392;&#24040;&#20294;&#26377;&#21069;&#26223;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#36825;&#23545;&#20110;&#24110;&#21161;&#35821;&#35328;&#27531;&#38556;&#20154;&#22763;&#36890;&#36807;&#33041;&#20449;&#21495;&#36827;&#34892;&#27807;&#36890;&#23588;&#20026;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#23558;&#20174;&#33041;&#27963;&#21160;&#26144;&#23556;&#30340;&#39044;&#27979;&#25991;&#26412;&#23884;&#20837;&#21521;&#23548;&#25991;&#26412;&#37325;&#24314;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#22312;BLEU&#21644;METEOR&#20998;&#25968;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;77%&#21644;54%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17516v1 Announce Type: cross  Abstract: Decoding continuous language from brain activity is a formidable yet promising field of research. It is particularly significant for aiding people with speech disabilities to communicate through brain signals. This field addresses the complex task of mapping brain signals to text. The previous best attempt reverse-engineered this process in an indirect way: it began by learning to encode brain activity from text and then guided text generation by aligning with predicted brain responses. In contrast, we propose a simple yet effective method that guides text reconstruction by directly comparing them with the predicted text embeddings mapped from brain activities. Comprehensive experiments reveal that our method significantly outperforms the current state-of-the-art model, showing average improvements of 77% and 54% on BLEU and METEOR scores. We further validate the proposed modules through detailed ablation studies and case analyses and 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#20316;&#22810;&#26234;&#33021;&#20307;&#25351;&#23548;&#21644;&#36319;&#38543;&#31574;&#30053;&#30340;&#28216;&#25103;&#65292;&#36890;&#36807;&#23545;&#35270;&#35273;&#21644;&#35821;&#35328;&#35266;&#23519;&#36827;&#34892;&#21327;&#35843;&#26469;&#35780;&#20272;&#29609;&#23478;&#20043;&#38388;&#30340;&#20132;&#20114;&#21162;&#21147;&#65292;&#23454;&#39564;&#21457;&#29616;&#26631;&#20934;&#30340;PPO&#35774;&#32622;&#37197;&#21512;&#21551;&#21457;&#24335;&#21512;&#20316;&#34892;&#20026;&#33021;&#21462;&#24471;&#39640;&#25104;&#21151;&#29575;&#65292;&#31070;&#32463;&#32593;&#32476;&#21512;&#20316;&#20249;&#20276;&#37197;&#23545;&#21487;&#20943;&#23569;&#37325;&#22797;&#28216;&#25103;&#26102;&#30340;&#21512;&#20316;&#21162;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.17497</link><description>&lt;p&gt;
&#20998;&#25285;&#25104;&#21151;&#30340;&#25104;&#26412;&#65306;&#29992;&#20110;&#35780;&#20272;&#21644;&#23398;&#20064;&#21327;&#20316;&#22810;&#26234;&#33021;&#20307;&#25351;&#23548;&#21644;&#36319;&#38543;&#31574;&#30053;&#30340;&#28216;&#25103;
&lt;/p&gt;
&lt;p&gt;
Sharing the Cost of Success: A Game for Evaluating and Learning Collaborative Multi-Agent Instruction Giving and Following Policies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17497
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#20316;&#22810;&#26234;&#33021;&#20307;&#25351;&#23548;&#21644;&#36319;&#38543;&#31574;&#30053;&#30340;&#28216;&#25103;&#65292;&#36890;&#36807;&#23545;&#35270;&#35273;&#21644;&#35821;&#35328;&#35266;&#23519;&#36827;&#34892;&#21327;&#35843;&#26469;&#35780;&#20272;&#29609;&#23478;&#20043;&#38388;&#30340;&#20132;&#20114;&#21162;&#21147;&#65292;&#23454;&#39564;&#21457;&#29616;&#26631;&#20934;&#30340;PPO&#35774;&#32622;&#37197;&#21512;&#21551;&#21457;&#24335;&#21512;&#20316;&#34892;&#20026;&#33021;&#21462;&#24471;&#39640;&#25104;&#21151;&#29575;&#65292;&#31070;&#32463;&#32593;&#32476;&#21512;&#20316;&#20249;&#20276;&#37197;&#23545;&#21487;&#20943;&#23569;&#37325;&#22797;&#28216;&#25103;&#26102;&#30340;&#21512;&#20316;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21327;&#20316;&#30446;&#26631;&#23548;&#21521;&#30340;&#29615;&#22659;&#20013;&#65292;&#21442;&#19982;&#32773;&#19981;&#20165;&#23545;&#23454;&#29616;&#25104;&#21151;&#30340;&#32467;&#26524;&#24863;&#20852;&#36259;&#65292;&#32780;&#19988;&#36824;&#20250;&#38544;&#24335;&#22320;&#21327;&#21830;&#20182;&#20204;&#22312;&#20132;&#20114;&#20013;&#25237;&#20837;&#30340;&#21162;&#21147;&#65288;&#36890;&#36807;&#30456;&#20114;&#36866;&#24212;&#65289;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20132;&#20114;&#24335;&#21442;&#32771;&#28216;&#25103;&#65292;&#38656;&#35201;&#20004;&#21517;&#29609;&#23478;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#35266;&#23519;&#19978;&#36827;&#34892;&#21327;&#35843;&#12290;&#27492;&#28216;&#25103;&#20013;&#30340;&#23398;&#20064;&#20449;&#21495;&#26159;&#19968;&#31181;&#20998;&#25968;&#65288;&#22312;&#28216;&#25103;&#21518;&#32473;&#20986;&#65289;&#65292;&#35813;&#20998;&#25968;&#32771;&#34385;&#20102;&#23454;&#29616;&#30340;&#30446;&#26631;&#21644;&#29609;&#23478;&#22312;&#20132;&#20114;&#36807;&#31243;&#20013;&#30340;&#20551;&#23450;&#21162;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#20351;&#29992;&#23454;&#29616;&#20174;&#20154;&#38469;&#20132;&#20114;&#20998;&#26512;&#20013;&#33719;&#24471;&#30340;&#35265;&#35299;&#30340;&#21551;&#21457;&#24335;&#21512;&#20316;&#34892;&#20026;&#26469;&#24341;&#23548;&#26102;&#65292;&#26631;&#20934;&#30340;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#35774;&#32622;&#23454;&#29616;&#20102;&#24456;&#39640;&#30340;&#25104;&#21151;&#29575;&#12290;&#25105;&#20204;&#21457;&#29616;&#31070;&#32463;&#32593;&#32476;&#21512;&#20316;&#20249;&#20276;&#30340;&#37197;&#23545;&#30830;&#23454;&#22312;&#37325;&#22797;&#29609;&#22312;&#19968;&#36215;&#26102;&#20943;&#23569;&#20102;&#27979;&#24471;&#30340;&#21512;&#20316;&#21162;&#21147;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#19982;&#19968;&#20010;&#21512;&#29702;&#30340;&#21551;&#21457;&#24335;&#37197;&#23545;&#30456;&#27604;&#65292;&#20173;&#28982;&#23384;&#22312;&#30528;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17497v1 Announce Type: new  Abstract: In collaborative goal-oriented settings, the participants are not only interested in achieving a successful outcome, but do also implicitly negotiate the effort they put into the interaction (by adapting to each other). In this work, we propose a challenging interactive reference game that requires two players to coordinate on vision and language observations. The learning signal in this game is a score (given after playing) that takes into account the achieved goal and the players' assumed efforts during the interaction. We show that a standard Proximal Policy Optimization (PPO) setup achieves a high success rate when bootstrapped with heuristic partner behaviors that implement insights from the analysis of human-human interactions. And we find that a pairing of neural partners indeed reduces the measured joint effort when playing together repeatedly. However, we observe that in comparison to a reasonable heuristic pairing there is stil
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DGoT&#30340;&#21160;&#24577;&#24605;&#32500;&#22270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#22270;&#32467;&#26500;&#21644;&#38477;&#20302;&#27169;&#22411;&#25512;&#29702;&#25104;&#26412;&#65292;&#25552;&#39640;&#20102;&#22312;&#29983;&#25104;&#31185;&#23398;&#35770;&#25991;&#25688;&#35201;&#20219;&#21153;&#20013;&#30340;&#24615;&#20215;&#27604;&#12290;</title><link>https://arxiv.org/abs/2403.17491</link><description>&lt;p&gt;
DGoT: &#29992;&#20110;&#31185;&#23398;&#25688;&#35201;&#29983;&#25104;&#30340;&#21160;&#24577;&#24605;&#32500;&#22270;
&lt;/p&gt;
&lt;p&gt;
DGoT: Dynamic Graph of Thoughts for Scientific Abstract Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17491
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DGoT&#30340;&#21160;&#24577;&#24605;&#32500;&#22270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#22270;&#32467;&#26500;&#21644;&#38477;&#20302;&#27169;&#22411;&#25512;&#29702;&#25104;&#26412;&#65292;&#25552;&#39640;&#20102;&#22312;&#29983;&#25104;&#31185;&#23398;&#35770;&#25991;&#25688;&#35201;&#20219;&#21153;&#20013;&#30340;&#24615;&#20215;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#39046;&#22495;&#25968;&#25454;&#38598;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#22312;&#29983;&#25104;&#31185;&#23398;&#35770;&#25991;&#25688;&#35201;&#30340;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#37325;&#35201;&#25104;&#23601;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#38754;&#20020;&#30528;&#27867;&#21270;&#21644;&#26114;&#36149;&#30340;&#35757;&#32451;&#25104;&#26412;&#31561;&#38382;&#39064;&#12290;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#35299;&#20915;&#29983;&#25104;&#35770;&#25991;&#25688;&#35201;&#30340;&#20219;&#21153;&#21487;&#20197;&#33410;&#30465;&#27169;&#22411;&#35757;&#32451;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;LLM&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#36890;&#24120;&#38656;&#35201;&#36890;&#36807;&#22810;&#36718;&#26597;&#35810;&#25552;&#31034;&#26041;&#27861;&#65288;&#22914;&#24605;&#32500;&#22270;&#65288;GoT&#65289;&#65289;&#26469;&#25552;&#39640;&#32467;&#26524;&#30340;&#21487;&#38752;&#24615;&#65292;&#20294;&#20063;&#24102;&#26469;&#20102;&#39069;&#22806;&#30340;&#25512;&#29702;&#25104;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#24605;&#32500;&#22270;&#65288;DGoT&#65289;&#65292;&#23427;&#19981;&#20165;&#32487;&#25215;&#20102;&#29616;&#26377;&#30340;GoT&#25552;&#31034;&#26041;&#27861;&#30340;&#20248;&#28857;&#65292;&#32780;&#19988;&#26681;&#25454;&#25968;&#25454;&#29305;&#24449;&#21160;&#24577;&#35843;&#25972;&#22270;&#32467;&#26500;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#27169;&#22411;&#25512;&#29702;&#25104;&#26412;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29983;&#25104;&#25688;&#35201;&#20219;&#21153;&#20013;&#20855;&#26377;&#38750;&#24120;&#22909;&#30340;&#24615;&#20215;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17491v1 Announce Type: new  Abstract: The method of training language models based on domain datasets has obtained significant achievements in the task of generating scientific paper abstracts. However, such models face problems of generalization and expensive training costs. The use of large language models (LLMs) to solve the task of generating paper abstracts saves the cost of model training. However, due to the hallucination problem of LLM, it is often necessary to improve the reliability of the results through multi-round query prompt approach such as Graph of Thoughts (GoT), which also brings additional reasoning costs. In this paper, we propose a Dynamic Graph of Thought (DGoT). It not only inherits the advantages of the existing GoT prompt approach, but also dynamically adjust the graph structure according to data characteristics while reducing model reasoning cost. Experimental results show that our method's cost-effectiveness in abstract generation tasks is only 43
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KDMCSE&#30340;&#30693;&#35782;&#33976;&#39311;&#22810;&#27169;&#24577;&#21477;&#23376;&#23884;&#20837;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#23398;&#20064;&#20013;&#32487;&#25215;&#25945;&#24072;&#27169;&#22411;&#30340;&#30693;&#35782;&#65292;&#20934;&#30830;&#21306;&#20998;&#27491;&#36127;&#23454;&#20363;&#24182;&#26377;&#25928;&#26816;&#27979;&#22024;&#26434;&#21644;&#38169;&#35823;&#30340;&#36127;&#26679;&#26412;&#12290;</title><link>https://arxiv.org/abs/2403.17486</link><description>&lt;p&gt;
KDMCSE: &#30693;&#35782;&#33976;&#39311;&#22810;&#27169;&#24577;&#21477;&#23376;&#23884;&#20837;&#19982;&#33258;&#36866;&#24212;&#35282;&#24230;&#36793;&#38469;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
KDMCSE: Knowledge Distillation Multimodal Sentence Embeddings with Adaptive Angular margin Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17486
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KDMCSE&#30340;&#30693;&#35782;&#33976;&#39311;&#22810;&#27169;&#24577;&#21477;&#23376;&#23884;&#20837;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#23398;&#20064;&#20013;&#32487;&#25215;&#25945;&#24072;&#27169;&#22411;&#30340;&#30693;&#35782;&#65292;&#20934;&#30830;&#21306;&#20998;&#27491;&#36127;&#23454;&#20363;&#24182;&#26377;&#25928;&#26816;&#27979;&#22024;&#26434;&#21644;&#38169;&#35823;&#30340;&#36127;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#20851;&#20110;&#22810;&#27169;&#24577;&#21477;&#23376;&#23884;&#20837;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#23558;&#25209;&#27425;&#20013;&#30340;&#20854;&#20313;&#26679;&#26412;&#20316;&#20026;&#36127;&#26679;&#26412;&#32780;&#26410;&#36827;&#34892;&#23457;&#26597;&#20197;&#24418;&#25104;&#23545;&#27604;&#23545;&#26102;&#65292;&#36825;&#20123;&#30740;&#31350;&#36935;&#21040;&#20102;&#35768;&#22810;&#21487;&#30097;&#21644;&#22024;&#26434;&#30340;&#36127;&#20363;&#65292;&#26174;&#33879;&#24433;&#21709;&#20102;&#26041;&#27861;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#22312;&#26412;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;KDMCSE&#65288;&#30693;&#35782;&#33976;&#39311;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#21477;&#23376;&#23884;&#20837;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22686;&#24378;&#22810;&#27169;&#24577;&#34920;&#31034;&#30340;&#21306;&#20998;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#20174;&#25945;&#24072;&#27169;&#22411;&#32487;&#25215;&#30693;&#35782;&#26469;&#23398;&#20064;&#27491;&#36127;&#23454;&#20363;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20174;&#32780;&#21487;&#20197;&#26377;&#25928;&#22320;&#22312;&#23558;&#20854;&#35745;&#31639;&#22312;&#23545;&#27604;&#30446;&#26631;&#20013;&#20043;&#21069;&#26816;&#27979;&#21040;&#22024;&#26434;&#21644;&#38169;&#35823;&#30340;&#36127;&#26679;&#26412;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20811;&#26381;&#24314;&#27169;&#36127;&#23545;&#20869;&#37096;&#21464;&#21270;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#27604;&#30446;&#26631;&#65292;Ad
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17486v1 Announce Type: new  Abstract: Previous work on multimodal sentence embedding has proposed multimodal contrastive learning and achieved promising results. However, by taking the rest of the batch as negative samples without reviewing when forming contrastive pairs, those studies encountered many suspicious and noisy negative examples, significantly affecting the methods' overall performance. In this work, we propose KDMCSE (Knowledge Distillation Multimodal contrastive learning of Sentence Embeddings), a novel approach that enhances the discrimination and generalizability of multimodal representation and inherits the knowledge from the teacher model to learn the difference between positive and negative instances and via that, can detect noisy and wrong negative samples effectively before they are calculated in the contrastive objective. Furthermore, to overcome the limitation of modeling the variation within negative pairs, we introduce a new contrastive objective, Ad
&lt;/p&gt;</description></item><item><title>&#23558;&#31616;&#21333;&#30340;&#25351;&#25968;&#24179;&#28369;&#27861;&#19982;MLP&#32467;&#21512;&#65292;&#36890;&#36807;&#22686;&#21152;&#21442;&#25968;&#21644;&#22797;&#26434;&#24615;&#65292;&#23454;&#29616;&#20102;&#19982;&#22797;&#26434;S4&#27169;&#22411;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;</title><link>https://arxiv.org/abs/2403.17445</link><description>&lt;p&gt;
&#23558;&#25351;&#25968;&#24179;&#28369;&#27861;&#34701;&#20837;MLP&#65306;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#24207;&#21015;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Incorporating Exponential Smoothing into MLP: A Simple but Effective Sequence Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17445
&lt;/p&gt;
&lt;p&gt;
&#23558;&#31616;&#21333;&#30340;&#25351;&#25968;&#24179;&#28369;&#27861;&#19982;MLP&#32467;&#21512;&#65292;&#36890;&#36807;&#22686;&#21152;&#21442;&#25968;&#21644;&#22797;&#26434;&#24615;&#65292;&#23454;&#29616;&#20102;&#19982;&#22797;&#26434;S4&#27169;&#22411;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24207;&#21015;&#25968;&#25454;&#20013;&#24314;&#27169;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#26159;&#24207;&#21015;&#23398;&#20064;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#26368;&#36817;&#21457;&#23637;&#30340;&#27169;&#22411;&#8220;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#8221;&#65288;S4&#65289;&#22312;&#24314;&#27169;&#38271;&#26399;&#24207;&#21015;&#26041;&#38754;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#23578;&#19981;&#28165;&#26970;S4&#30340;&#25104;&#21151;&#26159;&#22240;&#20026;&#20854;&#22797;&#26434;&#30340;&#21442;&#25968;&#21270;&#21644;HiPPO&#21021;&#22987;&#21270;&#36824;&#26159;&#20165;&#20165;&#30001;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25506;&#35752;&#28145;&#24230;SSMs&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#20174;&#31616;&#21333;&#30340;SSM&#25351;&#25968;&#24179;&#28369;&#65288;ETS&#65289;&#24320;&#22987;&#65292;&#24182;&#36890;&#36807;&#30452;&#25509;&#23558;&#20854;&#34701;&#20837;&#36880;&#20803;&#32032;MLP&#25552;&#20986;&#20102;&#19968;&#20010;&#21472;&#21152;&#26550;&#26500;&#12290;&#25105;&#20204;&#36890;&#36807;&#22686;&#21152;&#39069;&#22806;&#30340;&#21442;&#25968;&#21644;&#22797;&#26434;&#30340;&#23383;&#27573;&#26469;&#25193;&#20805;&#31616;&#21333;&#30340;ETS&#20197;&#20943;&#23569;&#24402;&#32435;&#20559;&#24046;&#12290;&#23613;&#31649;&#22312;&#36880;&#20803;&#32032;MLP&#30340;&#21442;&#25968;&#22686;&#21152;&#19981;&#21040;1%&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;LRA&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#19982;S4&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17445v1 Announce Type: cross  Abstract: Modeling long-range dependencies in sequential data is a crucial step in sequence learning. A recently developed model, the Structured State Space (S4), demonstrated significant effectiveness in modeling long-range sequences. However, It is unclear whether the success of S4 can be attributed to its intricate parameterization and HiPPO initialization or simply due to State Space Models (SSMs). To further investigate the potential of the deep SSMs, we start with exponential smoothing (ETS), a simple SSM, and propose a stacked architecture by directly incorporating it into an element-wise MLP. We augment simple ETS with additional parameters and complex field to reduce the inductive bias. Despite increasing less than 1\% of parameters of element-wise MLP, our models achieve comparable results to S4 on the LRA benchmark.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36866;&#24403;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#32463;&#36807;&#25351;&#20196;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#39640;&#24230;&#25511;&#21046;&#19978;&#19979;&#25991;&#30693;&#35782;&#65292;&#24182;&#23545;&#26080;&#20851;&#19978;&#19979;&#25991;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#25552;&#20986;&#20102;EREN&#65288;&#36890;&#36807;&#38405;&#35835;&#31508;&#35760;&#26469;&#32534;&#36753;&#27169;&#22411;&#65289;&#65292;&#20197;&#25913;&#21892;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.17431</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#19988;&#21487;&#25193;&#23637;&#30340;&#27169;&#22411;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Robust and Scalable Model Editing for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17431
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36866;&#24403;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#32463;&#36807;&#25351;&#20196;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#39640;&#24230;&#25511;&#21046;&#19978;&#19979;&#25991;&#30693;&#35782;&#65292;&#24182;&#23545;&#26080;&#20851;&#19978;&#19979;&#25991;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#25552;&#20986;&#20102;EREN&#65288;&#36890;&#36807;&#38405;&#35835;&#31508;&#35760;&#26469;&#32534;&#36753;&#27169;&#22411;&#65289;&#65292;&#20197;&#25913;&#21892;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#20351;&#29992;&#21442;&#25968;&#21270;&#30693;&#35782;&#36827;&#34892;&#39044;&#27979;--&#21363;&#32534;&#30721;&#22312;&#27169;&#22411;&#26435;&#37325;&#20013;&#30340;&#30693;&#35782;--&#25110;&#32773;&#26159;&#19978;&#19979;&#25991;&#30693;&#35782;--&#21363;&#21576;&#29616;&#22312;&#19978;&#19979;&#25991;&#20013;&#30340;&#30693;&#35782;&#12290;&#22312;&#35768;&#22810;&#22330;&#26223;&#19979;&#65292;&#19968;&#20010;&#29702;&#24819;&#30340;&#34892;&#20026;&#26159;&#24403;LLMs&#22312;&#21442;&#25968;&#21270;&#30693;&#35782;&#19982;&#19978;&#19979;&#25991;&#30693;&#35782;&#21457;&#29983;&#20914;&#31361;&#26102;&#65292;&#20248;&#20808;&#32771;&#34385;&#19978;&#19979;&#25991;&#30693;&#35782;&#65292;&#24182;&#22312;&#19978;&#19979;&#25991;&#26080;&#20851;&#26102;&#22238;&#36864;&#21040;&#20351;&#29992;&#20182;&#20204;&#30340;&#21442;&#25968;&#21270;&#30693;&#35782;&#12290;&#36825;&#20351;&#24471;&#36890;&#36807;&#19978;&#19979;&#25991;&#32534;&#36753;&#26469;&#26356;&#26032;&#21644;&#32416;&#27491;&#27169;&#22411;&#30340;&#30693;&#35782;&#25104;&#20026;&#21487;&#33021;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;LLMs&#20542;&#21521;&#20110;&#24573;&#35270;&#19978;&#19979;&#25991;&#30693;&#35782;&#65292;&#24182;&#19988;&#22312;&#38754;&#23545;&#26080;&#20851;&#19978;&#19979;&#25991;&#26102;&#26080;&#27861;&#21487;&#38752;&#22320;&#22238;&#36864;&#21040;&#21442;&#25968;&#21270;&#30693;&#35782;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#32463;&#36807;&#25351;&#20196;&#24494;&#35843;&#30340;LLMs&#21487;&#20197;&#34987;&#19978;&#19979;&#25991;&#30693;&#35782;&#39640;&#24230;&#25511;&#21046;&#65292;&#24182;&#23545;&#26080;&#20851;&#19978;&#19979;&#25991;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#21033;&#29992;&#36825;&#19968;&#29305;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;EREN&#65288;&#36890;&#36807;&#38405;&#35835;&#31508;&#35760;&#26469;&#32534;&#36753;&#27169;&#22411;&#65289;&#26469;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17431v1 Announce Type: new  Abstract: Large language models (LLMs) can make predictions using parametric knowledge--knowledge encoded in the model weights--or contextual knowledge--knowledge presented in the context. In many scenarios, a desirable behavior is that LLMs give precedence to contextual knowledge when it conflicts with the parametric knowledge, and fall back to using their parametric knowledge when the context is irrelevant. This enables updating and correcting the model's knowledge by in-context editing instead of retraining. Previous works have shown that LLMs are inclined to ignore contextual knowledge and fail to reliably fall back to parametric knowledge when presented with irrelevant context. In this work, we discover that, with proper prompting methods, instruction-finetuned LLMs can be highly controllable by contextual knowledge and robust to irrelevant context. Utilizing this feature, we propose EREN (Edit models by REading Notes) to improve the scalabil
&lt;/p&gt;</description></item><item><title>&#25506;&#35752;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#31934;&#31070;&#31185;&#35775;&#35848;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#26397;&#40092;&#21467;&#36867;&#32773;&#30340;&#21672;&#35810;&#25968;&#25454;&#65292;&#30740;&#31350;LLMs&#22312;&#21010;&#20998;&#30151;&#29366;&#21644;&#24635;&#32467;&#21387;&#21147;&#22240;&#32032;&#21644;&#30151;&#29366;&#26041;&#38754;&#21462;&#24471;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.17428</link><description>&lt;p&gt;
&#36890;&#36807;&#30151;&#29366;&#21010;&#20998;&#21644;&#24635;&#32467;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#22686;&#24378;&#31934;&#31070;&#31185;&#35775;&#35848;
&lt;/p&gt;
&lt;p&gt;
Aligning Large Language Models for Enhancing Psychiatric Interviews through Symptom Delineation and Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17428
&lt;/p&gt;
&lt;p&gt;
&#25506;&#35752;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#31934;&#31070;&#31185;&#35775;&#35848;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#26397;&#40092;&#21467;&#36867;&#32773;&#30340;&#21672;&#35810;&#25968;&#25454;&#65292;&#30740;&#31350;LLMs&#22312;&#21010;&#20998;&#30151;&#29366;&#21644;&#24635;&#32467;&#21387;&#21147;&#22240;&#32032;&#21644;&#30151;&#29366;&#26041;&#38754;&#21462;&#24471;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#21152;&#36895;&#20102;&#23427;&#20204;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#37492;&#20110;&#31934;&#31070;&#31185;&#35775;&#35848;&#26159;&#19987;&#19994;&#38754;&#35797;&#32773;&#19982;&#34987;&#38754;&#35797;&#32773;&#20043;&#38388;&#30446;&#26631;&#23548;&#21521;&#21644;&#32467;&#26500;&#21270;&#23545;&#35805;&#65292;&#36825;&#26159;LLMs&#21487;&#20197;&#25552;&#20379;&#23454;&#36136;&#20215;&#20540;&#30340;&#26368;&#26410;&#34987;&#24320;&#21457;&#30340;&#39046;&#22495;&#20043;&#19968;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#20855;&#26377;&#21019;&#20260;&#32463;&#21382;&#21644;&#31934;&#31070;&#20581;&#24247;&#38382;&#39064;&#30340;&#26397;&#40092;&#21467;&#36867;&#32773;&#30340;&#21672;&#35810;&#25968;&#25454;&#65292;&#25506;&#35752;&#20102;LLMs&#29992;&#20110;&#22686;&#24378;&#31934;&#31070;&#31185;&#35775;&#35848;&#30340;&#29992;&#36884;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;LLMs&#26159;&#21542;&#33021;&#22815;&#65288;1&#65289;&#21010;&#20998;&#34920;&#31034;&#31934;&#31070;&#30151;&#29366;&#30340;&#23545;&#35805;&#37096;&#20998;&#24182;&#21629;&#21517;&#30151;&#29366;&#65292;&#20197;&#21450;&#65288;2&#65289;&#26681;&#25454;&#35775;&#35848;&#23545;&#35805;&#35760;&#24405;&#24635;&#32467;&#21387;&#21147;&#22240;&#32032;&#21644;&#30151;&#29366;&#12290;&#36825;&#37324;&#65292;&#35775;&#35848;&#25968;&#25454;&#30001;&#31934;&#31070;&#20581;&#24247;&#19987;&#23478;&#36827;&#34892;&#26631;&#35760;&#65292;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;LLMs&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36866;&#24403;&#25552;&#31034;&#30340;LLMs&#22312;&#30151;&#29366;&#21010;&#20998;&#21644;&#24635;&#32467;&#19978;&#21487;&#20197;&#23454;&#29616;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17428v1 Announce Type: new  Abstract: Recent advancements in Large Language Models (LLMs) have accelerated their usage in various domains. Given the fact that psychiatric interviews are goal-oriented and structured dialogues between the professional interviewer and the interviewee, it is one of the most underexplored areas where LLMs can contribute substantial value. Here, we explore the use of LLMs for enhancing psychiatric interviews, by analyzing counseling data from North Korean defectors with traumatic events and mental health issues. Specifically, we investigate whether LLMs can (1) delineate the part of the conversation that suggests psychiatric symptoms and name the symptoms, and (2) summarize stressors and symptoms, based on the interview dialogue transcript. Here, the transcript data was labeled by mental health experts for training and evaluation of LLMs. Our experimental results show that appropriately prompted LLMs can achieve high performance on both the sympto
&lt;/p&gt;</description></item><item><title>LM-Combiner&#26159;&#19968;&#31181;&#29992;&#20110;&#20013;&#25991;&#35821;&#27861;&#38169;&#35823;&#26657;&#27491;&#30340;&#19978;&#19979;&#25991;&#37325;&#20889;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#30452;&#25509;&#20462;&#25913;&#35821;&#27861;&#38169;&#35823;&#26657;&#27491;&#31995;&#32479;&#30340;&#36807;&#24230;&#26657;&#27491;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#21484;&#22238;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.17413</link><description>&lt;p&gt;
LM-Combiner&#65306;&#19968;&#31181;&#29992;&#20110;&#20013;&#25991;&#35821;&#27861;&#38169;&#35823;&#26657;&#27491;&#30340;&#19978;&#19979;&#25991;&#37325;&#20889;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LM-Combiner: A Contextual Rewriting Model for Chinese Grammatical Error Correction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17413
&lt;/p&gt;
&lt;p&gt;
LM-Combiner&#26159;&#19968;&#31181;&#29992;&#20110;&#20013;&#25991;&#35821;&#27861;&#38169;&#35823;&#26657;&#27491;&#30340;&#19978;&#19979;&#25991;&#37325;&#20889;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#30452;&#25509;&#20462;&#25913;&#35821;&#27861;&#38169;&#35823;&#26657;&#27491;&#31995;&#32479;&#30340;&#36807;&#24230;&#26657;&#27491;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#24230;&#26657;&#27491;&#26159;&#20013;&#25991;&#35821;&#27861;&#38169;&#35823;&#26657;&#27491;&#65288;CGEC&#65289;&#20219;&#21153;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#25237;&#31080;&#30340;&#27169;&#22411;&#38598;&#25104;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#32531;&#35299;&#36807;&#24230;&#26657;&#27491;&#65292;&#24182;&#25552;&#39640;&#35821;&#27861;&#38169;&#35823;&#26657;&#27491;&#31995;&#32479;&#30340;&#20934;&#30830;&#29575;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20173;&#28982;&#38656;&#35201;&#20960;&#20010;&#35821;&#27861;&#38169;&#35823;&#26657;&#27491;&#31995;&#32479;&#30340;&#36755;&#20986;&#65292;&#36825;&#19981;&#21487;&#36991;&#20813;&#22320;&#20250;&#23548;&#33268;&#20943;&#23569;&#38169;&#35823;&#21484;&#22238;&#12290;&#22522;&#20110;&#36825;&#19968;&#24773;&#20917;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LM-Combiner&#65292;&#36825;&#26159;&#19968;&#31181;&#21487;&#20197;&#30452;&#25509;&#20462;&#25913;&#35821;&#27861;&#38169;&#35823;&#26657;&#27491;&#31995;&#32479;&#36755;&#20986;&#30340;&#36807;&#24230;&#26657;&#27491;&#30340;&#37325;&#20889;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#27169;&#22411;&#38598;&#25104;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#36890;&#36807;&#25552;&#20986;&#30340;K&#25240;&#20132;&#21449;&#25512;&#26029;&#26041;&#27861;&#26500;&#24314;&#30340;&#36807;&#24230;&#26657;&#27491;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#35813;&#27169;&#22411;&#65292;&#20351;&#20854;&#33021;&#22815;&#36890;&#36807;&#32467;&#21512;&#21407;&#22987;&#25991;&#26412;&#21644;&#36807;&#24230;&#26657;&#27491;&#25991;&#26412;&#26469;&#30452;&#25509;&#29983;&#25104;&#36807;&#28388;&#21518;&#30340;&#21477;&#23376;&#12290;&#22312;&#25512;&#26029;&#38454;&#27573;&#65292;&#25105;&#20204;&#30452;&#25509;&#23558;&#21407;&#22987;&#21477;&#23376;&#21644;&#20854;&#20182;&#31995;&#32479;&#30340;&#36755;&#20986;&#32467;&#26524;&#20316;&#20026;&#36755;&#20837;&#65292;&#28982;&#21518;&#36890;&#36807;LM-Combiner&#33719;&#24471;&#36807;&#28388;&#21518;&#30340;&#21477;&#23376;&#12290;&#23545;FCGEC&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;LM-Combiner&#26126;&#26174;&#25913;&#21892;&#20102;&#35821;&#27861;&#38169;&#35823;&#26657;&#27491;&#30340;&#20934;&#30830;&#24615;&#21644;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17413v1 Announce Type: new  Abstract: Over-correction is a critical problem in Chinese grammatical error correction (CGEC) task. Recent work using model ensemble methods based on voting can effectively mitigate over-correction and improve the precision of the GEC system. However, these methods still require the output of several GEC systems and inevitably lead to reduced error recall. In this light, we propose the LM-Combiner, a rewriting model that can directly modify the over-correction of GEC system outputs without a model ensemble. Specifically, we train the model on an over-correction dataset constructed through the proposed K-fold cross inference method, which allows it to directly generate filtered sentences by combining the original and the over-corrected text. In the inference stage, we directly take the original sentences and the output results of other systems as input and then obtain the filtered sentences through LM-Combiner. Experiments on the FCGEC dataset sho
&lt;/p&gt;</description></item><item><title>PCToolkit&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#21363;&#25554;&#21363;&#29992;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21387;&#32553;&#25552;&#31034;&#65292;&#21253;&#25324;&#23574;&#31471;&#30340;&#21387;&#32553;&#22120;&#12289;&#22810;&#26679;&#30340;&#25968;&#25454;&#38598;&#21644;&#32508;&#21512;&#24615;&#33021;&#35780;&#20272;&#25351;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.17411</link><description>&lt;p&gt;
PCToolkit&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32479;&#19968;&#21363;&#25554;&#21363;&#29992;&#25552;&#31034;&#21387;&#32553;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
PCToolkit: A Unified Plug-and-Play Prompt Compression Toolkit of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17411
&lt;/p&gt;
&lt;p&gt;
PCToolkit&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#21363;&#25554;&#21363;&#29992;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21387;&#32553;&#25552;&#31034;&#65292;&#21253;&#25324;&#23574;&#31471;&#30340;&#21387;&#32553;&#22120;&#12289;&#22810;&#26679;&#30340;&#25968;&#25454;&#38598;&#21644;&#32508;&#21512;&#24615;&#33021;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17411v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#30340; &#25552;&#21462;&#35201;&#28857;&#26159;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#39640;&#25928;&#21387;&#32553;&#36755;&#20837;&#25552;&#31034;&#21516;&#26102;&#20445;&#30041;&#20851;&#38190;&#20449;&#24687;&#12290;&#20026;&#20102;&#20419;&#36827;&#24555;&#36895;&#21551;&#21160;&#26381;&#21153;&#12289;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#65292;&#24182;&#19982;&#24120;&#35265;&#25968;&#25454;&#38598;&#21644;&#25351;&#26631;&#20860;&#23481;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25552;&#31034;&#21387;&#32553;&#24037;&#20855;&#21253;&#65288;PCToolkit&#65289;&#12290;&#35813;&#24037;&#20855;&#21253;&#26159;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#21387;&#32553;&#25552;&#31034;&#30340;&#32479;&#19968;&#21363;&#25554;&#21363;&#29992;&#35299;&#20915;&#26041;&#26696;&#65292;&#20855;&#26377;&#23574;&#31471;&#30340;&#25552;&#31034;&#21387;&#32553;&#22120;&#12289;&#22810;&#26679;&#30340;&#25968;&#25454;&#38598;&#21644;&#29992;&#20110;&#20840;&#38754;&#24615;&#33021;&#35780;&#20272;&#30340;&#25351;&#26631;&#12290;PCToolkit&#25317;&#26377;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#36890;&#36807;&#20415;&#25658;&#21644;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#65292;&#21487;&#20197;&#36731;&#26494;&#38598;&#25104;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#25351;&#26631;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;PCToolkit&#30340;&#20851;&#38190;&#32452;&#20214;&#21644;&#21151;&#33021;&#12290;&#25105;&#20204;&#23545;PCToolkit&#20869;&#30340;&#21387;&#32553;&#22120;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#65292;&#21253;&#25324;&#37325;&#24314;&#12289;&#25688;&#35201;&#12289;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#12289;&#38382;&#31572;&#12289;&#23569;&#26679;&#26412;&#23398;&#20064;&#12289;&#21512;&#25104;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17411v1 Announce Type: new  Abstract: Prompt compression is an innovative method for efficiently condensing input prompts while preserving essential information. To facilitate quick-start services, user-friendly interfaces, and compatibility with common datasets and metrics, we present the Prompt Compression Toolkit (PCToolkit). This toolkit is a unified plug-and-play solution for compressing prompts in Large Language Models (LLMs), featuring cutting-edge prompt compressors, diverse datasets, and metrics for comprehensive performance evaluation. PCToolkit boasts a modular design, allowing for easy integration of new datasets and metrics through portable and user-friendly interfaces. In this paper, we outline the key components and functionalities of PCToolkit. We conducted evaluations of the compressors within PCToolkit across various natural language tasks, including reconstruction, summarization, mathematical problem-solving, question answering, few-shot learning, syntheti
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#21306;&#22495;&#25351;&#23548;&#26631;&#35760;&#25216;&#26415;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#23391;&#21152;&#25289;&#25991;&#26412;&#19982;&#22320;&#26041;&#26041;&#35328;&#36716;&#24405;&#20026;&#22269;&#38469;&#38899;&#26631;&#30340;&#26041;&#27861;&#65292;&#20026;&#27169;&#22411;&#25552;&#20379;&#20102;&#20851;&#20110;&#36755;&#20837;&#25991;&#26412;&#30340;&#22320;&#21306;&#26041;&#35328;&#20449;&#24687;&#65292;&#20197;&#29702;&#35299;&#19982;&#27599;&#20010;&#22320;&#21306;&#30456;&#20851;&#30340;&#29420;&#29305;&#38899;&#38901;&#27169;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.17407</link><description>&lt;p&gt;
&#20351;&#29992;&#21306;&#22495;&#25351;&#23548;&#26631;&#35760;&#23558;&#23391;&#21152;&#25289;&#25991;&#26412;&#19982;&#22320;&#26041;&#26041;&#35328;&#36716;&#24405;&#20026;&#22269;&#38469;&#38899;&#26631;
&lt;/p&gt;
&lt;p&gt;
Transcribing Bengali Text with Regional Dialects to IPA using District Guided Tokens
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17407
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#21306;&#22495;&#25351;&#23548;&#26631;&#35760;&#25216;&#26415;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#23391;&#21152;&#25289;&#25991;&#26412;&#19982;&#22320;&#26041;&#26041;&#35328;&#36716;&#24405;&#20026;&#22269;&#38469;&#38899;&#26631;&#30340;&#26041;&#27861;&#65292;&#20026;&#27169;&#22411;&#25552;&#20379;&#20102;&#20851;&#20110;&#36755;&#20837;&#25991;&#26412;&#30340;&#22320;&#21306;&#26041;&#35328;&#20449;&#24687;&#65292;&#20197;&#29702;&#35299;&#19982;&#27599;&#20010;&#22320;&#21306;&#30456;&#20851;&#30340;&#29420;&#29305;&#38899;&#38901;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23391;&#21152;&#25289;&#25991;&#26412;&#21040;&#22269;&#38469;&#38899;&#26631;&#65288;IPA&#65289;&#30340;&#20934;&#30830;&#36716;&#24405;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#35821;&#35328;&#30340;&#22797;&#26434;&#38899;&#38901;&#23398;&#21644;&#35821;&#22659;&#30456;&#20851;&#30340;&#38899;&#21464;&#12290;&#23545;&#20110;&#21306;&#22495;&#23391;&#21152;&#25289;&#26041;&#35328;&#26469;&#35828;&#65292;&#30001;&#20110;&#32570;&#20047;&#38024;&#23545;&#36825;&#20123;&#26041;&#35328;&#30340;&#26631;&#20934;&#25340;&#20889;&#32422;&#23450;&#12289;&#24403;&#22320;&#21644;&#22806;&#35821;&#22312;&#36825;&#20123;&#22320;&#21306;&#20013;&#27969;&#34892;&#30340;&#35789;&#27719;&#20197;&#21450;&#19981;&#21516;&#22320;&#21306;&#20043;&#38388;&#30340;&#38899;&#38901;&#22810;&#26679;&#24615;&#65292;&#36825;&#19968;&#25361;&#25112;&#29978;&#33267;&#26356;&#20026;&#20005;&#23803;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#38382;&#39064;&#65292;&#21363;&#22312;&#35206;&#30422;&#23391;&#21152;&#25289;&#22269;&#20845;&#20010;&#22320;&#21306;&#30340;&#26032;&#25968;&#25454;&#38598;&#19978;&#24341;&#20837;&#8220;&#21306;&#22495;&#25351;&#23548;&#26631;&#35760;&#8221;&#65288;DGT&#65289;&#25216;&#26415;&#12290;&#20854;&#20851;&#38190;&#24605;&#24819;&#26159;&#22312;&#29983;&#25104;IPA&#36716;&#24405;&#20043;&#21069;&#21521;&#27169;&#22411;&#25552;&#20379;&#26377;&#20851;&#36755;&#20837;&#25991;&#26412;&#30340;&#21306;&#22495;&#26041;&#35328;&#25110;&#8220;&#22320;&#21306;&#8221;&#30340;&#26126;&#30830;&#20449;&#24687;&#12290;&#36825;&#36890;&#36807;&#22312;&#36755;&#20837;&#24207;&#21015;&#21069;&#28155;&#21152;&#19968;&#20010;&#22320;&#21306;&#26631;&#35760;&#26469;&#23454;&#29616;&#65292;&#26377;&#25928;&#22320;&#24341;&#23548;&#27169;&#22411;&#29702;&#35299;&#19982;&#27599;&#20010;&#22320;&#21306;&#30456;&#20851;&#30340;&#29420;&#29305;&#38899;&#38901;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17407v1 Announce Type: cross  Abstract: Accurate transcription of Bengali text to the International Phonetic Alphabet (IPA) is a challenging task due to the complex phonology of the language and context-dependent sound changes. This challenge is even more for regional Bengali dialects due to unavailability of standardized spelling conventions for these dialects, presence of local and foreign words popular in those regions and phonological diversity across different regions. This paper presents an approach to this sequence-to-sequence problem by introducing the District Guided Tokens (DGT) technique on a new dataset spanning six districts of Bangladesh. The key idea is to provide the model with explicit information about the regional dialect or "district" of the input text before generating the IPA transcription. This is achieved by prepending a district token to the input sequence, effectively guiding the model to understand the unique phonetic patterns associated with each 
&lt;/p&gt;</description></item><item><title>ELLEN&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#23558;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#19982;&#35821;&#35328;&#35268;&#21017;&#30456;&#32467;&#21512;&#65292;&#22312;&#26497;&#20854;&#36731;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#38750;&#24120;&#24378;&#21170;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.17385</link><description>&lt;p&gt;
ELLEN: &#38750;&#24120;&#36731;&#30417;&#30563;&#23398;&#20064;&#29992;&#20110;&#39640;&#25928;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
ELLEN: Extremely Lightly Supervised Learning For Efficient Named Entity Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17385
&lt;/p&gt;
&lt;p&gt;
ELLEN&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#23558;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#19982;&#35821;&#35328;&#35268;&#21017;&#30456;&#32467;&#21512;&#65292;&#22312;&#26497;&#20854;&#36731;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#38750;&#24120;&#24378;&#21170;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#21322;&#30417;&#30563;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#38382;&#39064;&#65292;&#20391;&#37325;&#20110;&#26497;&#20854;&#36731;&#37327;&#32423;&#30340;&#30417;&#30563;&#65292;&#21253;&#25324;&#20165;&#21253;&#21547;&#27599;&#31867;&#21035;10&#20010;&#31034;&#20363;&#30340;&#35789;&#27719;&#34920;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;ELLEN&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#12289;&#23436;&#20840;&#27169;&#22359;&#21270;&#30340;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#23427;&#23558;&#32463;&#36807;&#24494;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#19982;&#35821;&#35328;&#35268;&#21017;&#30456;&#32467;&#21512;&#12290;&#36825;&#20123;&#35268;&#21017;&#21253;&#25324;&#8220;&#19968;&#20010;&#35805;&#35821;&#19968;&#20010;&#24847;&#20041;&#8221;&#36825;&#26679;&#30340;&#35265;&#35299;&#65292;&#20351;&#29992;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#26080;&#30417;&#30563;NER&#65292;&#21033;&#29992;&#35789;&#24615;&#26631;&#31614;&#35782;&#21035;&#21644;&#28040;&#38500;&#26410;&#26631;&#35760;&#23454;&#20307;&#20316;&#20026;&#20551;&#36127;&#20363;&#65292;&#20197;&#21450;&#20851;&#20110;&#20998;&#31867;&#22120;&#32622;&#20449;&#24230;&#24471;&#20998;&#22312;&#23616;&#37096;&#21644;&#20840;&#23616;&#32972;&#26223;&#19979;&#30340;&#20854;&#20182;&#30452;&#35273;&#12290;&#22312;&#20351;&#29992;&#19978;&#36848;&#35789;&#27719;&#34920;&#26497;&#23567;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#65292;ELLEN&#22312;CoNLL-2003&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#38750;&#24120;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#23427;&#36824;&#22312;&#25991;&#29486;&#20013;&#24120;&#29992;&#30340;&#30456;&#21516;&#30417;&#30563;&#35774;&#32622;&#65288;&#21363;&#65292;&#35757;&#32451;&#25968;&#25454;&#30340;5%&#65289;&#19979;&#65292;&#20248;&#20110;&#22823;&#22810;&#25968;&#29616;&#26377;&#65288;&#19988;&#26356;&#20026;&#22797;&#26434;&#65289;&#30340;&#21322;&#30417;&#30563;NER&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17385v1 Announce Type: cross  Abstract: In this work, we revisit the problem of semi-supervised named entity recognition (NER) focusing on extremely light supervision, consisting of a lexicon containing only 10 examples per class. We introduce ELLEN, a simple, fully modular, neuro-symbolic method that blends fine-tuned language models with linguistic rules. These rules include insights such as ''One Sense Per Discourse'', using a Masked Language Model as an unsupervised NER, leveraging part-of-speech tags to identify and eliminate unlabeled entities as false negatives, and other intuitions about classifier confidence scores in local and global context. ELLEN achieves very strong performance on the CoNLL-2003 dataset when using the minimal supervision from the lexicon above. It also outperforms most existing (and considerably more complex) semi-supervised NER methods under the same supervision settings commonly used in the literature (i.e., 5% of the training data). Further, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;ChatGPT&#22312;&#19981;&#21516;&#23610;&#24230;&#19979;&#19982;&#20154;&#31867;&#35780;&#20272;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#21457;&#29616;&#22312;&#36739;&#31895;&#31890;&#24230;&#30340;&#23610;&#24230;&#19978;&#65292;ChatGPT&#19982;&#20154;&#31867;&#26356;&#21152;&#19968;&#33268;&#12290;</title><link>https://arxiv.org/abs/2403.17368</link><description>&lt;p&gt;
ChatGPT&#23558;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#36136;&#37327;&#35780;&#32423;&#23450;&#20026;&#19982;&#20154;&#31867;&#30456;&#20284;&#65306;&#20294;&#26159;&#22522;&#20110;&#21738;&#20123;&#26631;&#20934;&#65311;
&lt;/p&gt;
&lt;p&gt;
ChatGPT Rates Natural Language Explanation Quality Like Humans: But on Which Scales?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;ChatGPT&#22312;&#19981;&#21516;&#23610;&#24230;&#19979;&#19982;&#20154;&#31867;&#35780;&#20272;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#21457;&#29616;&#22312;&#36739;&#31895;&#31890;&#24230;&#30340;&#23610;&#24230;&#19978;&#65292;ChatGPT&#19982;&#20154;&#31867;&#26356;&#21152;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#22312;&#25105;&#20204;&#30340;&#29983;&#27963;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#36879;&#26126;&#24230;&#21644;&#36131;&#20219;&#24615;&#30340;&#38656;&#27714;&#20063;&#22312;&#22686;&#38271;&#12290;&#34429;&#28982;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#65288;NLEs&#65289;&#23545;&#28548;&#28165;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#32972;&#21518;&#30340;&#25512;&#29702;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#36890;&#36807;&#20154;&#31867;&#21028;&#26029;&#23545;&#20854;&#36827;&#34892;&#35780;&#20272;&#30001;&#20110;&#20027;&#35266;&#24615;&#21644;&#23545;&#32454;&#31890;&#24230;&#35780;&#20998;&#30340;&#38656;&#27714;&#32780;&#21464;&#24471;&#22797;&#26434;&#19988;&#36164;&#28304;&#23494;&#38598;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;ChatGPT&#19982;&#20154;&#31867;&#35780;&#20272;&#20043;&#38388;&#22312;&#22810;&#20010;&#23610;&#24230;&#65288;&#21363;&#20108;&#20803;&#12289;&#19977;&#20803;&#21644;7-Likert&#23610;&#24230;&#65289;&#19978;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#20174;&#19977;&#20010;NLE&#25968;&#25454;&#38598;&#20013;&#25277;&#21462;300&#20010;&#25968;&#25454;&#23454;&#20363;&#65292;&#24182;&#20026;&#20449;&#24687;&#37327;&#21644;&#28165;&#26224;&#24230;&#20004;&#20010;&#25991;&#26412;&#36136;&#37327;&#24230;&#37327;&#25910;&#38598;&#20102;900&#20010;&#20154;&#31867;&#27880;&#37322;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#22312;&#19981;&#21516;&#20027;&#35266;&#35780;&#20998;&#33539;&#22260;&#19979;&#36827;&#34892;&#20102;&#25104;&#23545;&#27604;&#36739;&#23454;&#39564;&#65292;&#20854;&#20013;&#22522;&#32447;&#26469;&#33258;8,346&#20010;&#20154;&#31867;&#27880;&#37322;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#26356;&#31895;&#31890;&#24230;&#30340;&#23610;&#24230;&#19978;&#65292;ChatGPT&#19982;&#20154;&#31867;&#26356;&#21152;&#19968;&#33268;&#12290;&#27492;&#22806;&#65292;&#25104;&#23545;&#27604;&#36739;&#21644;&#21160;&#24577;&#25552;&#31034;&#65288;&#21363;&#25552;&#20379;&#35821;&#20041;&#19978;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17368v1 Announce Type: cross  Abstract: As AI becomes more integral in our lives, the need for transparency and responsibility grows. While natural language explanations (NLEs) are vital for clarifying the reasoning behind AI decisions, evaluating them through human judgments is complex and resource-intensive due to subjectivity and the need for fine-grained ratings. This study explores the alignment between ChatGPT and human assessments across multiple scales (i.e., binary, ternary, and 7-Likert scale). We sample 300 data instances from three NLE datasets and collect 900 human annotations for both informativeness and clarity scores as the text quality measurement. We further conduct paired comparison experiments under different ranges of subjectivity scores, where the baseline comes from 8,346 human annotations. Our results show that ChatGPT aligns better with humans in more coarse-grained scales. Also, paired comparisons and dynamic prompting (i.e., providing semantically 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#25968;&#25454;&#38598; BioASR-NER&#65292;&#26088;&#22312;&#22635;&#34917;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20043;&#38388;&#30340;&#40511;&#27807;&#65292;&#37325;&#28857;&#26159;&#20174; Brief Test of Adult Cognition by Tel &#20013;&#25552;&#21462;&#19981;&#33391;&#33647;&#29289;&#21453;&#24212;&#21644;&#23454;&#20307;&#25552;&#21450;&#12290;</title><link>https://arxiv.org/abs/2403.17363</link><description>&lt;p&gt;
&#20174;&#22024;&#26434;&#30340;&#38899;&#39057;&#36716;&#24405;&#20013;&#25552;&#21462;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;
&lt;/p&gt;
&lt;p&gt;
Extracting Biomedical Entities from Noisy Audio Transcripts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#25968;&#25454;&#38598; BioASR-NER&#65292;&#26088;&#22312;&#22635;&#34917;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20043;&#38388;&#30340;&#40511;&#27807;&#65292;&#37325;&#28857;&#26159;&#20174; Brief Test of Adult Cognition by Tel &#20013;&#25552;&#21462;&#19981;&#33391;&#33647;&#29289;&#21453;&#24212;&#21644;&#23454;&#20307;&#25552;&#21450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Automatic Speech Recognition (ASR) &#25216;&#26415;&#22312;&#23558;&#21475;&#35821;&#36716;&#24405;&#20026;&#25991;&#26412;&#26041;&#38754;&#36215;&#30528;&#22522;&#30784;&#24615;&#20316;&#29992;&#65292;&#22312;&#20020;&#24202;&#39046;&#22495;&#20855;&#26377;&#30456;&#24403;&#22810;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#31616;&#21270;&#21307;&#23398;&#36716;&#24405;&#21644;&#19982;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#31995;&#32479;&#38598;&#25104;&#12290;&#28982;&#32780;&#65292;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;&#65292;&#29305;&#21035;&#26159;&#24403;&#36716;&#24405;&#21253;&#21547;&#22122;&#22768;&#26102;&#65292;&#23548;&#33268;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#27169;&#22411;&#24212;&#29992;&#26102;&#24615;&#33021;&#22823;&#24133;&#19979;&#38477;&#12290;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#20020;&#24202;&#20219;&#21153;&#65292;&#29305;&#21035;&#21463;&#21040;&#27492;&#31867;&#22122;&#22768;&#30340;&#24433;&#21709;&#65292;&#36890;&#24120;&#31216;&#20026;ASR-NLP&#40511;&#27807;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#30740;&#31350;&#20102; ASR &#22312;&#24178;&#20928;&#24405;&#38899;&#20013;&#30340;&#25928;&#29575;&#65292;&#30041;&#19979;&#20102;&#20851;&#20110;&#22312;&#22024;&#26434;&#29615;&#22659;&#20013;&#24615;&#33021;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598; BioASR-NER&#65292;&#26088;&#22312;&#22635;&#34917;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#30340; ASR-NLP &#40511;&#27807;&#65292;&#30528;&#37325;&#20110;&#20174; Brief Test of Adult Cognition by Tel &#20013;&#25552;&#21462;&#19981;&#33391;&#33647;&#29289;&#21453;&#24212;&#21644;&#23454;&#20307;&#25552;&#21450;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17363v1 Announce Type: new  Abstract: Automatic Speech Recognition (ASR) technology is fundamental in transcribing spoken language into text, with considerable applications in the clinical realm, including streamlining medical transcription and integrating with Electronic Health Record (EHR) systems. Nevertheless, challenges persist, especially when transcriptions contain noise, leading to significant drops in performance when Natural Language Processing (NLP) models are applied. Named Entity Recognition (NER), an essential clinical task, is particularly affected by such noise, often termed the ASR-NLP gap. Prior works have primarily studied ASR's efficiency in clean recordings, leaving a research gap concerning the performance in noisy environments. This paper introduces a novel dataset, BioASR-NER, designed to bridge the ASR-NLP gap in the biomedical domain, focusing on extracting adverse drug reactions and mentions of entities from the Brief Test of Adult Cognition by Tel
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#36731;&#37327;&#32423;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26426;&#21046;&#65292;&#26377;&#25928;&#22320;&#21033;&#29992;&#19981;&#21516;&#25968;&#25454;&#31867;&#22411;&#20043;&#38388;&#30340;&#28508;&#22312;&#36830;&#25509;&#65292;&#20445;&#30041;&#21407;&#22987;&#35777;&#25454;&#30340;&#19978;&#19979;&#25991;&#65292;&#30830;&#20445;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.17361</link><description>&lt;p&gt;
&#23558;&#25991;&#26412;&#21644;&#34920;&#26684;&#19990;&#30028;&#32852;&#31995;&#36215;&#26469;&#36827;&#34892;&#20107;&#23454;&#39564;&#35777;&#65306;&#19968;&#31181;&#36731;&#37327;&#32423;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Bridging Textual and Tabular Worlds for Fact Verification: A Lightweight, Attention-Based Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17361
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#36731;&#37327;&#32423;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26426;&#21046;&#65292;&#26377;&#25928;&#22320;&#21033;&#29992;&#19981;&#21516;&#25968;&#25454;&#31867;&#22411;&#20043;&#38388;&#30340;&#28508;&#22312;&#36830;&#25509;&#65292;&#20445;&#30041;&#21407;&#22987;&#35777;&#25454;&#30340;&#19978;&#19979;&#25991;&#65292;&#30830;&#20445;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
FEVEROUS&#26159;&#19968;&#20010;&#20851;&#27880;&#28041;&#21450;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#21644;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#30340;&#20107;&#23454;&#25552;&#21462;&#21644;&#39564;&#35777;&#20219;&#21153;&#30340;&#22522;&#20934;&#21644;&#30740;&#31350;&#39033;&#30446;&#12290;&#22312;FEVEROUS&#20013;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#24120;&#20381;&#36182;&#20110;&#22823;&#37327;&#30340;&#39044;&#22788;&#29702;&#24182;&#21033;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;&#25968;&#25454;&#36716;&#25442;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#28508;&#22312;&#30340;&#19978;&#19979;&#25991;&#20002;&#22833;&#25110;&#35823;&#23548;&#24615;&#32534;&#30721;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;&#28040;&#38500;&#20102;&#27169;&#24577;&#36716;&#25442;&#30340;&#38656;&#27714;&#65292;&#20174;&#32780;&#20445;&#30041;&#20102;&#21407;&#22987;&#35777;&#25454;&#30340;&#19978;&#19979;&#25991;&#12290;&#36890;&#36807;&#21033;&#29992;&#22810;&#26679;&#21270;&#30340;&#25991;&#26412;&#21644;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#24182;&#32467;&#21512;&#36731;&#37327;&#32423;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#21033;&#29992;&#20102;&#19981;&#21516;&#25968;&#25454;&#31867;&#22411;&#20043;&#38388;&#30340;&#28508;&#22312;&#36830;&#25509;&#65292;&#20174;&#32780;&#20135;&#29983;&#20840;&#38754;&#19988;&#21487;&#38752;&#30340;&#21028;&#26029;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#30340;&#27169;&#22359;&#21270;&#32467;&#26500;&#24039;&#22937;&#22320;&#31649;&#29702;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#30830;&#20445;&#21407;&#22987;&#35777;&#25454;&#30340;&#23436;&#25972;&#24615;&#21644;&#30495;&#23454;&#24615;&#19981;&#21463;&#25439;&#23475;&#12290;&#27604;&#36739;&#20998;&#26512;&#34920;&#26126;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17361v1 Announce Type: cross  Abstract: FEVEROUS is a benchmark and research initiative focused on fact extraction and verification tasks involving unstructured text and structured tabular data. In FEVEROUS, existing works often rely on extensive preprocessing and utilize rule-based transformations of data, leading to potential context loss or misleading encodings. This paper introduces a simple yet powerful model that nullifies the need for modality conversion, thereby preserving the original evidence's context. By leveraging pre-trained models on diverse text and tabular datasets and by incorporating a lightweight attention-based mechanism, our approach efficiently exploits latent connections between different data types, thereby yielding comprehensive and reliable verdict predictions. The model's modular structure adeptly manages multi-modal information, ensuring the integrity and authenticity of the original evidence are uncompromised. Comparative analyses reveal that ou
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Chain-of-Action (CoA)&#26694;&#26550;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#25512;&#29702;-&#26816;&#32034;&#26426;&#21046;&#21644;&#22810;&#21442;&#32771;&#24544;&#23454;&#20998;&#25968;&#35299;&#20915;&#20102;&#24403;&#21069;QA&#24212;&#29992;&#20013;&#30340;&#19981;&#24544;&#23454;&#24187;&#35273;&#21644;&#24369;&#25512;&#29702;&#24615;&#33021;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.17359</link><description>&lt;p&gt;
Chain-of-Action&#65306;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#24544;&#23454;&#21644;&#22810;&#27169;&#24577;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Action: Faithful and Multimodal Question Answering through Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17359
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Chain-of-Action (CoA)&#26694;&#26550;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#25512;&#29702;-&#26816;&#32034;&#26426;&#21046;&#21644;&#22810;&#21442;&#32771;&#24544;&#23454;&#20998;&#25968;&#35299;&#20915;&#20102;&#24403;&#21069;QA&#24212;&#29992;&#20013;&#30340;&#19981;&#24544;&#23454;&#24187;&#35273;&#21644;&#24369;&#25512;&#29702;&#24615;&#33021;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;Chain-of-Action (CoA)&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#21644;&#26816;&#32034;&#22686;&#24378;&#38382;&#31572;(QA)&#12290;&#19982;&#29616;&#26377;&#25991;&#29486;&#30456;&#27604;&#65292;CoA&#20811;&#26381;&#20102;&#24403;&#21069;QA&#24212;&#29992;&#30340;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;(i) &#19982;&#23454;&#26102;&#25110;&#39046;&#22495;&#20107;&#23454;&#19981;&#19968;&#33268;&#30340;&#19981;&#24544;&#23454;&#24187;&#35273;&#65292;&#20197;&#21450;(ii) &#23545;&#32452;&#21512;&#20449;&#24687;&#30340;&#24369;&#25512;&#29702;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#25512;&#29702;-&#26816;&#32034;&#26426;&#21046;&#65292;&#36890;&#36807;&#31995;&#32479;&#25552;&#31034;&#21644;&#39044;&#35774;&#35745;&#30340;&#21160;&#20316;&#23558;&#22797;&#26434;&#38382;&#39064;&#20998;&#35299;&#20026;&#25512;&#29702;&#38142;&#12290;&#22312;&#26041;&#27861;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#39046;&#22495;&#36866;&#24212;&#24615;&#30340;&#8220;&#21363;&#25554;&#21363;&#29992;&#8221;&#25805;&#20316;&#65292;&#29992;&#20110;&#20174;&#24322;&#26500;&#28304;&#26816;&#32034;&#23454;&#26102;&#20449;&#24687;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#21442;&#32771;&#24544;&#23454;&#20998;&#25968;&#65288;MRFS&#65289;&#26469;&#39564;&#35777;&#21644;&#35299;&#20915;&#31572;&#26696;&#20013;&#30340;&#20914;&#31361;&#12290;&#22312;&#32463;&#39564;&#19978;&#65292;&#25105;&#20204;&#21033;&#29992;&#20844;&#20849;&#22522;&#20934;&#21644;&#19968;&#20010;Web3&#26696;&#20363;&#30740;&#31350;&#26469;&#23637;&#31034;CoA&#30456;&#27604;&#20854;&#20182;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17359v1 Announce Type: new  Abstract: We present a Chain-of-Action (CoA) framework for multimodal and retrieval-augmented Question-Answering (QA). Compared to the literature, CoA overcomes two major challenges of current QA applications: (i) unfaithful hallucination that is inconsistent with real-time or domain facts and (ii) weak reasoning performance over compositional information. Our key contribution is a novel reasoning-retrieval mechanism that decomposes a complex question into a reasoning chain via systematic prompting and pre-designed actions. Methodologically, we propose three types of domain-adaptable `Plug-and-Play' actions for retrieving real-time information from heterogeneous sources. We also propose a multi-reference faith score (MRFS) to verify and resolve conflicts in the answers. Empirically, we exploit both public benchmarks and a Web3 case study to demonstrate the capability of CoA over other methods.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23450;&#20041;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#35299;&#20915;&#23454;&#20307;&#21305;&#37197;&#20013;&#30340;&#27495;&#20041;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.17344</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20851;&#31995;&#21457;&#29616;&#30340;&#23454;&#20307;&#21305;&#37197;&#28040;&#27495;
&lt;/p&gt;
&lt;p&gt;
Disambiguate Entity Matching through Relation Discovery with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17344
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23450;&#20041;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#35299;&#20915;&#23454;&#20307;&#21305;&#37197;&#20013;&#30340;&#27495;&#20041;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#21305;&#37197;&#26159;&#25968;&#25454;&#38598;&#25104;&#21644;&#28165;&#27927;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#23545;&#20110;&#27169;&#31946;&#36830;&#25509;&#21644;&#25968;&#25454;&#37325;&#22797;&#28040;&#38500;&#31561;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#26041;&#27861;&#38598;&#20013;&#22312;&#20811;&#26381;&#27169;&#31946;&#26415;&#35821;&#34920;&#31034;&#65292;&#20363;&#22914;&#32534;&#36753;&#36317;&#31163;&#12289;Jaccard&#30456;&#20284;&#24615;&#65292;&#20197;&#21450;&#26368;&#36817;&#30340;&#23884;&#20837;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21253;&#25324;&#26469;&#33258;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23454;&#20307;&#21305;&#37197;&#20013;&#30340;&#26680;&#24515;&#25361;&#25112;&#36229;&#36234;&#20102;&#26415;&#35821;&#27169;&#31946;&#24615;&#65292;&#32780;&#26159;&#22312;&#23450;&#20041;&#20309;&#20026;&#8220;&#21305;&#37197;&#8221;&#26102;&#30340;&#27495;&#20041;&#65292;&#29305;&#21035;&#26159;&#22312;&#19982;&#22806;&#37096;&#25968;&#25454;&#24211;&#38598;&#25104;&#26102;&#12290;&#36825;&#31181;&#27495;&#20041;&#26159;&#30001;&#20110;&#23454;&#20307;&#20043;&#38388;&#22312;&#32454;&#33410;&#21644;&#31890;&#24230;&#26041;&#38754;&#23384;&#22312;&#24046;&#24322;&#24341;&#36215;&#30340;&#65292;&#36825;&#20351;&#24471;&#30830;&#20999;&#21305;&#37197;&#21464;&#24471;&#22797;&#26434;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#28966;&#28857;&#20174;&#32431;&#31929;&#35782;&#21035;&#35821;&#20041;&#30456;&#20284;&#24615;&#36716;&#21464;&#20026;&#29702;&#35299;&#21644;&#23450;&#20041;&#23454;&#20307;&#20043;&#38388;&#30340;&#8220;&#20851;&#31995;&#8221;&#20316;&#20026;&#35299;&#20915;&#21305;&#37197;&#20013;&#30340;&#27495;&#20041;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#36807;&#39044;&#23450;&#20041;&#19968;&#32452;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#20851;&#31995;&#65292;&#21487;&#24110;&#21161;&#35299;&#20915;&#21305;&#37197;&#20013;&#30340;&#27495;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17344v1 Announce Type: cross  Abstract: Entity matching is a critical challenge in data integration and cleaning, central to tasks like fuzzy joins and deduplication. Traditional approaches have focused on overcoming fuzzy term representations through methods such as edit distance, Jaccard similarity, and more recently, embeddings and deep neural networks, including advancements from large language models (LLMs) like GPT. However, the core challenge in entity matching extends beyond term fuzziness to the ambiguity in defining what constitutes a "match," especially when integrating with external databases. This ambiguity arises due to varying levels of detail and granularity among entities, complicating exact matches. We propose a novel approach that shifts focus from purely identifying semantic similarities to understanding and defining the "relations" between entities as crucial for resolving ambiguities in matching. By predefining a set of relations relevant to the task at
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#22522;&#20110;&#27531;&#24046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#25104;&#20687;&#20219;&#21153;&#20013;&#20316;&#20026;&#32534;&#30721;&#22120;&#30340;&#24847;&#24819;&#19981;&#21040;&#30340;&#26377;&#25928;&#24615;&#65292;&#21033;&#29992;&#20923;&#32467;&#30340;&#21464;&#21387;&#22120;&#22359;&#36827;&#34892;&#30452;&#25509;&#22788;&#29702;&#35270;&#35273;&#20196;&#29260;&#65292;&#20174;&#32780;&#25552;&#39640;&#21508;&#31181;&#29983;&#29289;&#21307;&#23398;&#25104;&#20687;&#24212;&#29992;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.17343</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#29983;&#29289;&#21307;&#23398;&#25104;&#20687;&#20219;&#21153;&#30340;&#20813;&#36153;&#21161;&#25512;&#22120;
&lt;/p&gt;
&lt;p&gt;
Language Models are Free Boosters for Biomedical Imaging Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#22522;&#20110;&#27531;&#24046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#25104;&#20687;&#20219;&#21153;&#20013;&#20316;&#20026;&#32534;&#30721;&#22120;&#30340;&#24847;&#24819;&#19981;&#21040;&#30340;&#26377;&#25928;&#24615;&#65292;&#21033;&#29992;&#20923;&#32467;&#30340;&#21464;&#21387;&#22120;&#22359;&#36827;&#34892;&#30452;&#25509;&#22788;&#29702;&#35270;&#35273;&#20196;&#29260;&#65292;&#20174;&#32780;&#25552;&#39640;&#21508;&#31181;&#29983;&#29289;&#21307;&#23398;&#25104;&#20687;&#24212;&#29992;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#22522;&#20110;&#27531;&#24046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#29289;&#21307;&#23398;&#25104;&#20687;&#20219;&#21153;&#20013;&#20316;&#20026;&#32534;&#30721;&#22120;&#30340;&#24847;&#24819;&#19981;&#21040;&#30340;&#26377;&#25928;&#24615;&#65292;&#36825;&#26159;&#20256;&#32479;&#19978;&#32570;&#20047;&#35821;&#35328;&#25110;&#25991;&#26412;&#25968;&#25454;&#30340;&#39046;&#22495;&#12290;&#35813;&#26041;&#27861;&#19981;&#21516;&#20110;&#24050;&#24314;&#31435;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20174;&#39044;&#35757;&#32451;&#30340;LLMs&#20013;&#25552;&#21462;&#30340;&#20923;&#32467;&#21464;&#21387;&#22120;&#22359;&#20316;&#20026;&#21019;&#26032;&#30340;&#32534;&#30721;&#22120;&#23618;&#65292;&#30452;&#25509;&#22788;&#29702;&#35270;&#35273;&#20196;&#29260;&#12290;&#36825;&#31181;&#31574;&#30053;&#19982;&#36890;&#24120;&#20381;&#36182;&#20110;&#35821;&#35328;&#39537;&#21160;&#25552;&#31034;&#21644;&#36755;&#20837;&#30340;&#26631;&#20934;&#22810;&#27169;&#24577;&#35270;&#35273;&#35821;&#35328;&#26694;&#26550;&#26377;&#30528;&#26174;&#33879;&#30340;&#19981;&#21516;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;LLMs&#33021;&#22815;&#25552;&#21319;&#21508;&#31181;&#29983;&#29289;&#21307;&#23398;&#25104;&#20687;&#24212;&#29992;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;2D&#21644;3D&#35270;&#35273;&#20998;&#31867;&#20219;&#21153;&#65292;&#20805;&#24403;&#21363;&#25554;&#21363;&#29992;&#30340;&#21161;&#25512;&#22120;&#12290;&#26356;&#26377;&#36259;&#30340;&#26159;&#65292;&#20316;&#20026;&#21103;&#20135;&#21697;&#65292;&#25105;&#20204;&#21457;&#29616;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#22312;M&#30340;&#24191;&#27867;&#12289;&#26631;&#20934;&#21270;&#25968;&#25454;&#38598;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17343v1 Announce Type: cross  Abstract: In this study, we uncover the unexpected efficacy of residual-based large language models (LLMs) as part of encoders for biomedical imaging tasks, a domain traditionally devoid of language or textual data. The approach diverges from established methodologies by utilizing a frozen transformer block, extracted from pre-trained LLMs, as an innovative encoder layer for the direct processing of visual tokens. This strategy represents a significant departure from the standard multi-modal vision-language frameworks, which typically hinge on language-driven prompts and inputs. We found that these LLMs could boost performance across a spectrum of biomedical imaging applications, including both 2D and 3D visual classification tasks, serving as plug-and-play boosters. More interestingly, as a byproduct, we found that the proposed framework achieved superior performance, setting new state-of-the-art results on extensive, standardized datasets in M
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#31995;&#32479;&#21270;&#20102;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36234;&#29425;&#25552;&#31034;&#30340;&#23384;&#22312;&#24418;&#24335;&#65292;&#24182;&#34913;&#37327;&#20102;&#23427;&#20204;&#30340;&#36234;&#29425;&#28508;&#21147;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#35821;&#20041;&#19978;&#20855;&#26377;&#24847;&#20041;&#30340;&#36234;&#29425;&#25552;&#31034;&#30340;&#23041;&#32961;&#26684;&#23616;&#12290;</title><link>https://arxiv.org/abs/2403.17336</link><description>&lt;p&gt;
&#19981;&#35201;&#21548;&#25105;&#30340;&#35805;&#65306;&#29702;&#35299;&#21644;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36234;&#29425;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Don't Listen To Me: Understanding and Exploring Jailbreak Prompts of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17336
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#31995;&#32479;&#21270;&#20102;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36234;&#29425;&#25552;&#31034;&#30340;&#23384;&#22312;&#24418;&#24335;&#65292;&#24182;&#34913;&#37327;&#20102;&#23427;&#20204;&#30340;&#36234;&#29425;&#28508;&#21147;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#35821;&#20041;&#19978;&#20855;&#26377;&#24847;&#20041;&#30340;&#36234;&#29425;&#25552;&#31034;&#30340;&#23041;&#32961;&#26684;&#23616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#36827;&#23637;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#26080;&#22788;&#19981;&#22312;&#22320;&#34987;&#35775;&#38382;&#12290;&#20973;&#20511;&#20854;&#20986;&#33394;&#30340;&#29702;&#35299;&#21644;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#36825;&#20123;&#27169;&#22411;&#27491;&#26085;&#30410;&#34701;&#20837;&#25105;&#20204;&#30340;&#31038;&#20250;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20154;&#20204;&#20063;&#23545;&#36825;&#31181;&#24378;&#22823;&#25216;&#26415;&#30340;&#28508;&#22312;&#28389;&#29992;&#34920;&#31034;&#25285;&#24551;&#65292;&#24182;&#20419;&#20351;&#26381;&#21153;&#25552;&#20379;&#21830;&#37319;&#21462;&#20102;&#38450;&#24481;&#25514;&#26045;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#31181;&#20445;&#25252;&#26426;&#21046;&#65292;&#36234;&#29425;&#25552;&#31034;&#26368;&#36817;&#24050;&#25104;&#20026;&#35268;&#36991;&#23433;&#20840;&#38480;&#21046;&#21644;&#24341;&#35825;&#26368;&#21021;&#35774;&#35745;&#20026;&#34987;&#31105;&#27490;&#30340;&#26377;&#23475;&#20869;&#23481;&#30340;&#26368;&#26377;&#25928;&#26426;&#21046;&#20043;&#19968;&#12290;&#30001;&#20110;LLM&#30340;&#24555;&#36895;&#21457;&#23637;&#21450;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#36731;&#26494;&#33719;&#21462;&#30340;&#20415;&#21033;&#24615;&#65292;&#36234;&#29425;&#25552;&#31034;&#30340;&#21069;&#27839;&#20027;&#35201;&#20986;&#29616;&#22312;&#22312;&#32447;&#35770;&#22363;&#21644;&#29233;&#22909;&#32773;&#20013;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#20102;&#35299;&#35821;&#20041;&#19978;&#20855;&#26377;&#24847;&#20041;&#30340;&#36234;&#29425;&#25552;&#31034;&#30340;&#23041;&#32961;&#26684;&#23616;&#65292;&#25105;&#20204;&#31995;&#32479;&#21270;&#20102;&#29616;&#26377;&#25552;&#31034;&#24182;&#27979;&#37327;&#23427;&#20204;&#30340;&#36234;&#29425;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17336v1 Announce Type: cross  Abstract: Recent advancements in generative AI have enabled ubiquitous access to large language models (LLMs). Empowered by their exceptional capabilities to understand and generate human-like text, these models are being increasingly integrated into our society. At the same time, there are also concerns on the potential misuse of this powerful technology, prompting defensive measures from service providers. To overcome such protection, jailbreaking prompts have recently emerged as one of the most effective mechanisms to circumvent security restrictions and elicit harmful content originally designed to be prohibited.   Due to the rapid development of LLMs and their ease of access via natural languages, the frontline of jailbreak prompts is largely seen in online forums and among hobbyists. To gain a better understanding of the threat landscape of semantically meaningful jailbreak prompts, we systemized existing prompts and measured their jailbre
&lt;/p&gt;</description></item><item><title>JMultiWOZ&#26159;&#31532;&#19968;&#20010;&#26085;&#35821;&#22823;&#35268;&#27169;&#22810;&#39046;&#22495;&#20219;&#21153;&#39537;&#21160;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#35780;&#20272;&#19982;&#29616;&#26377;&#33521;&#35821;&#22522;&#20934;&#25968;&#25454;&#38598;&#30456;&#23218;&#32654;&#30340;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#21644;&#22238;&#22797;&#29983;&#25104;&#33021;&#21147;&#65292;&#25512;&#21160;&#20102;&#26085;&#35821;&#20219;&#21153;&#39537;&#21160;&#23545;&#35805;&#31995;&#32479;&#30340;&#30740;&#31350;&#19982;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.17319</link><description>&lt;p&gt;
JMultiWOZ&#65306;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#26085;&#35821;&#22810;&#39046;&#22495;&#20219;&#21153;&#39537;&#21160;&#23545;&#35805;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
JMultiWOZ: A Large-Scale Japanese Multi-Domain Task-Oriented Dialogue Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17319
&lt;/p&gt;
&lt;p&gt;
JMultiWOZ&#26159;&#31532;&#19968;&#20010;&#26085;&#35821;&#22823;&#35268;&#27169;&#22810;&#39046;&#22495;&#20219;&#21153;&#39537;&#21160;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#35780;&#20272;&#19982;&#29616;&#26377;&#33521;&#35821;&#22522;&#20934;&#25968;&#25454;&#38598;&#30456;&#23218;&#32654;&#30340;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#21644;&#22238;&#22797;&#29983;&#25104;&#33021;&#21147;&#65292;&#25512;&#21160;&#20102;&#26085;&#35821;&#20219;&#21153;&#39537;&#21160;&#23545;&#35805;&#31995;&#32479;&#30340;&#30740;&#31350;&#19982;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#25968;&#25454;&#38598;&#23545;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20219;&#21153;&#39537;&#21160;&#23545;&#35805;&#31995;&#32479;&#30740;&#31350;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#33521;&#35821;&#22810;&#39046;&#22495;&#20219;&#21153;&#39537;&#21160;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#24182;&#20026;&#20219;&#21153;&#39537;&#21160;&#23545;&#35805;&#31995;&#32479;&#30340;&#26174;&#33879;&#36827;&#23637;&#20570;&#20986;&#36129;&#29486;&#65292;&#20294;&#26085;&#35821;&#20013;&#24182;&#19981;&#23384;&#22312;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#19982;&#33521;&#35821;&#39046;&#22495;&#30456;&#27604;&#65292;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#36824;&#24456;&#26377;&#38480;&#12290;&#20026;&#20102;&#25512;&#21160;&#26085;&#35821;&#20219;&#21153;&#39537;&#21160;&#23545;&#35805;&#31995;&#32479;&#30740;&#31350;&#19982;&#24320;&#21457;&#30340;&#36827;&#23637;&#65292;&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;JMultiWOZ&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#26085;&#35821;&#22823;&#35268;&#27169;&#22810;&#39046;&#22495;&#20219;&#21153;&#39537;&#21160;&#23545;&#35805;&#25968;&#25454;&#38598;&#12290;&#21033;&#29992;JMultiWOZ&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#29616;&#26377;&#33521;&#35821;&#22522;&#20934;&#25968;&#25454;&#38598;MultiWOZ2.2&#21644;&#26368;&#26032;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26041;&#27861;&#19978;&#30340;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#21644;&#22238;&#22797;&#29983;&#25104;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;JMultiWOZ&#25552;&#20379;&#20102;&#19968;&#20010;&#19982;MultiWOZ2&#30456;&#23218;&#32654;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17319v1 Announce Type: cross  Abstract: Dialogue datasets are crucial for deep learning-based task-oriented dialogue system research. While numerous English language multi-domain task-oriented dialogue datasets have been developed and contributed to significant advancements in task-oriented dialogue systems, such a dataset does not exist in Japanese, and research in this area is limited compared to that in English. In this study, towards the advancement of research and development of task-oriented dialogue systems in Japanese, we constructed JMultiWOZ, the first Japanese language large-scale multi-domain task-oriented dialogue dataset. Using JMultiWOZ, we evaluated the dialogue state tracking and response generation capabilities of the state-of-the-art methods on the existing major English benchmark dataset MultiWOZ2.2 and the latest large language model (LLM)-based methods. Our evaluation results demonstrated that JMultiWOZ provides a benchmark that is on par with MultiWOZ2
&lt;/p&gt;</description></item><item><title>&#39033;&#30446;MOSLA&#36890;&#36807;&#32437;&#21521;&#12289;&#22810;&#27169;&#24577;&#12289;&#22810;&#35821;&#35328;&#21644;&#21463;&#25511;&#25968;&#25454;&#38598;&#30340;&#21019;&#24314;&#65292;&#25581;&#31034;&#20102;&#23398;&#20064;&#32773;&#38543;&#26102;&#38388;&#21457;&#23637;&#30340;&#35821;&#35328;&#33021;&#21147;&#30340;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.17314</link><description>&lt;p&gt;
&#39033;&#30446;MOSLA&#65306;&#35760;&#24405;&#31532;&#20108;&#35821;&#35328;&#20064;&#24471;&#30340;&#27599;&#19968;&#21051;
&lt;/p&gt;
&lt;p&gt;
Project MOSLA: Recording Every Moment of Second Language Acquisition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17314
&lt;/p&gt;
&lt;p&gt;
&#39033;&#30446;MOSLA&#36890;&#36807;&#32437;&#21521;&#12289;&#22810;&#27169;&#24577;&#12289;&#22810;&#35821;&#35328;&#21644;&#21463;&#25511;&#25968;&#25454;&#38598;&#30340;&#21019;&#24314;&#65292;&#25581;&#31034;&#20102;&#23398;&#20064;&#32773;&#38543;&#26102;&#38388;&#21457;&#23637;&#30340;&#35821;&#35328;&#33021;&#21147;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31532;&#20108;&#35821;&#35328;&#20064;&#24471;&#65288;SLA&#65289;&#26159;&#19968;&#20010;&#22797;&#26434;&#32780;&#21160;&#24577;&#30340;&#36807;&#31243;&#12290;&#35768;&#22810;&#35797;&#22270;&#35760;&#24405;&#21644;&#20998;&#26512;&#36825;&#19968;&#36807;&#31243;&#30340;SLA&#30740;&#31350;&#36890;&#24120;&#19987;&#27880;&#20110;&#21333;&#19968;&#27169;&#24577;&#65288;&#20363;&#22914;&#23398;&#20064;&#32773;&#30340;&#25991;&#26412;&#36755;&#20986;&#65289;&#65292;&#20165;&#35206;&#30422;&#20102;&#30701;&#26102;&#38388;&#65292;&#25110;&#32773;&#32570;&#20047;&#25511;&#21046;&#65288;&#20363;&#22914;&#26410;&#25429;&#25417;&#23398;&#20064;&#36807;&#31243;&#30340;&#27599;&#20010;&#26041;&#38754;&#65289;&#12290;&#22312;MOSLA&#39033;&#30446;&#65288;&#31532;&#20108;&#35821;&#35328;&#20064;&#24471;&#26102;&#21051;&#65289;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#36992;&#35831;&#21442;&#19982;&#32773;&#22312;&#20004;&#24180;&#26102;&#38388;&#20869;&#20165;&#36890;&#36807;&#22312;&#32447;&#25351;&#23548;&#20174;&#38646;&#24320;&#22987;&#23398;&#20064;&#19977;&#31181;&#30446;&#26631;&#35821;&#35328;&#65288;&#38463;&#25289;&#20271;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#21644;&#20013;&#25991;&#65289;&#65292;&#24182;&#20351;&#29992;Zoom&#24405;&#21046;&#27599;&#33410;&#35838;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#32437;&#21521;&#12289;&#22810;&#27169;&#24577;&#12289;&#22810;&#35821;&#35328;&#21644;&#21463;&#25511;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#36890;&#36807;&#20154;&#24037;&#26631;&#27880;&#32773;&#21644;&#32463;&#36807;&#20248;&#21270;&#30340;&#26368;&#20808;&#36827;&#35821;&#38899;&#27169;&#22411;&#36827;&#34892;&#21322;&#33258;&#21160;&#27880;&#37322;&#65292;&#25581;&#31034;&#20102;&#23398;&#20064;&#32773;&#38543;&#26102;&#38388;&#21457;&#23637;&#30340;&#35821;&#35328;&#33021;&#21147;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17314v1 Announce Type: new  Abstract: Second language acquisition (SLA) is a complex and dynamic process. Many SLA studies that have attempted to record and analyze this process have typically focused on a single modality (e.g., textual output of learners), covered only a short period of time, and/or lacked control (e.g., failed to capture every aspect of the learning process). In Project MOSLA (Moments of Second Language Acquisition), we have created a longitudinal, multimodal, multilingual, and controlled dataset by inviting participants to learn one of three target languages (Arabic, Spanish, and Chinese) from scratch over a span of two years, exclusively through online instruction, and recording every lesson using Zoom. The dataset is semi-automatically annotated with speaker/language IDs and transcripts by both human annotators and fine-tuned state-of-the-art speech models. Our experiments reveal linguistic insights into learners' proficiency development over time, as w
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23545;&#21253;&#21547;&#25991;&#26412;&#21644;&#22270;&#29255;&#30340;&#25991;&#26723;&#30340;&#22810;&#27169;&#24577;&#20027;&#39064;&#24314;&#27169;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#20027;&#39064;&#24314;&#27169;&#35299;&#20915;&#26041;&#26696;&#21644;&#20004;&#31181;&#26032;&#39062;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#32467;&#26524;&#26174;&#31034;&#36825;&#20123;&#27169;&#22411;&#22343;&#33021;&#20135;&#29983;&#36830;&#36143;&#19988;&#22810;&#26679;&#21270;&#30340;&#20027;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.17308</link><description>&lt;p&gt;
&#31070;&#32463;&#22810;&#27169;&#24577;&#20027;&#39064;&#24314;&#27169;&#65306;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Neural Multimodal Topic Modeling: A Comprehensive Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17308
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23545;&#21253;&#21547;&#25991;&#26412;&#21644;&#22270;&#29255;&#30340;&#25991;&#26723;&#30340;&#22810;&#27169;&#24577;&#20027;&#39064;&#24314;&#27169;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#20027;&#39064;&#24314;&#27169;&#35299;&#20915;&#26041;&#26696;&#21644;&#20004;&#31181;&#26032;&#39062;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#32467;&#26524;&#26174;&#31034;&#36825;&#20123;&#27169;&#22411;&#22343;&#33021;&#20135;&#29983;&#36830;&#36143;&#19988;&#22810;&#26679;&#21270;&#30340;&#20027;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#21487;&#20197;&#25104;&#21151;&#22320;&#22312;&#25991;&#26412;&#25968;&#25454;&#20013;&#25214;&#21040;&#36830;&#36143;&#19988;&#22810;&#26679;&#21270;&#30340;&#20027;&#39064;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#22788;&#29702;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65288;&#22914;&#22270;&#29255;&#21644;&#25991;&#26412;&#65289;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#21253;&#21547;&#25991;&#26412;&#21644;&#22270;&#29255;&#30340;&#25991;&#26723;&#30340;&#22810;&#27169;&#24577;&#20027;&#39064;&#24314;&#27169;&#30340;&#31995;&#32479;&#24615;&#21644;&#20840;&#38754;&#35780;&#20272;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#20027;&#39064;&#24314;&#27169;&#35299;&#20915;&#26041;&#26696;&#21644;&#20004;&#31181;&#26032;&#39062;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23545;&#19968;&#20010;&#21069;&#25152;&#26410;&#26377;&#30340;&#20016;&#23500;&#22810;&#26679;&#30340;&#25968;&#25454;&#38598;&#38598;&#21512;&#36827;&#34892;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#20004;&#20010;&#27169;&#22411;&#37117;&#33021;&#20135;&#29983;&#36830;&#36143;&#19988;&#22810;&#26679;&#21270;&#30340;&#20027;&#39064;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#26041;&#27861;&#20248;&#20110;&#21478;&#19968;&#20010;&#26041;&#27861;&#30340;&#31243;&#24230;&#21462;&#20915;&#20110;&#25351;&#26631;&#21644;&#25968;&#25454;&#38598;&#30340;&#32452;&#21512;&#65292;&#36825;&#34920;&#26126;&#26410;&#26469;&#38656;&#35201;&#36827;&#19968;&#27493;&#25506;&#32034;&#28151;&#21512;&#35299;&#20915;&#26041;&#26696;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#31616;&#27905;&#30340;&#20154;&#24037;&#35780;&#20272;&#19982;&#25105;&#20204;&#25552;&#20986;&#30340;&#25351;&#26631;&#25152;&#30830;&#23450;&#30340;&#32467;&#26524;&#19968;&#33268;&#12290;&#36825;&#31181;&#19968;&#33268;&#19981;&#20165;&#21152;&#24378;&#20102;&#25105;&#20204;&#25351;&#26631;&#30340;&#21487;&#20449;&#24230;&#65292;&#20063;&#31361;&#26174;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17308v1 Announce Type: cross  Abstract: Neural topic models can successfully find coherent and diverse topics in textual data. However, they are limited in dealing with multimodal datasets (e.g., images and text). This paper presents the first systematic and comprehensive evaluation of multimodal topic modeling of documents containing both text and images. In the process, we propose two novel topic modeling solutions and two novel evaluation metrics. Overall, our evaluation on an unprecedented rich and diverse collection of datasets indicates that both of our models generate coherent and diverse topics. Nevertheless, the extent to which one method outperforms the other depends on the metrics and dataset combinations, which suggests further exploration of hybrid solutions in the future. Notably, our succinct human evaluation aligns with the outcomes determined by our proposed metrics. This alignment not only reinforces the credibility of our metrics but also highlights the po
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#20026;&#23618;&#27425;&#25991;&#26412;&#20998;&#31867;&#35774;&#35745;&#30340;&#20449;&#24687;&#26080;&#25439;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;HILL&#65292;&#26088;&#22312;&#22312;&#23545;&#27604;&#26679;&#26412;&#20013;&#20445;&#30041;&#36755;&#20837;&#26679;&#26412;&#30340;&#35821;&#20041;&#21644;&#21477;&#27861;&#20449;&#24687;&#65292;&#24182;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#36827;&#34892;&#34701;&#21512;&#12290;</title><link>https://arxiv.org/abs/2403.17307</link><description>&lt;p&gt;
HILL&#65306;&#23618;&#27425;&#24863;&#30693;&#20449;&#24687;&#26080;&#25439;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#23618;&#27425;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
HILL: Hierarchy-aware Information Lossless Contrastive Learning for Hierarchical Text Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17307
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#20026;&#23618;&#27425;&#25991;&#26412;&#20998;&#31867;&#35774;&#35745;&#30340;&#20449;&#24687;&#26080;&#25439;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;HILL&#65292;&#26088;&#22312;&#22312;&#23545;&#27604;&#26679;&#26412;&#20013;&#20445;&#30041;&#36755;&#20837;&#26679;&#26412;&#30340;&#35821;&#20041;&#21644;&#21477;&#27861;&#20449;&#24687;&#65292;&#24182;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#36827;&#34892;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23384;&#22312;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#33258;&#25105;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65292;&#20005;&#37325;&#20381;&#36182;&#20110;&#20154;&#20026;&#35774;&#35745;&#30340;&#22686;&#24378;&#35268;&#21017;&#26469;&#29983;&#25104;&#23545;&#27604;&#26679;&#26412;&#65292;&#36825;&#21487;&#33021;&#20250;&#25439;&#22351;&#25110;&#25197;&#26354;&#21407;&#22987;&#20449;&#24687;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#26041;&#26696;&#30340;&#21487;&#34892;&#24615;&#65292;&#22312;&#20854;&#20013;&#65292;&#36755;&#20837;&#26679;&#26412;&#20013;&#22266;&#26377;&#30340;&#35821;&#20041;&#21644;&#21477;&#27861;&#20449;&#24687;&#22312;&#23545;&#27604;&#26679;&#26412;&#20013;&#24471;&#21040;&#20805;&#20998;&#20445;&#30041;&#65292;&#24182;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#34701;&#21512;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20449;&#24687;&#26080;&#25439;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#29992;&#20110;&#23618;&#27425;&#25991;&#26412;&#20998;&#31867;&#65292;&#21363;&#8220;HILL&#8221;&#65292;&#23427;&#21253;&#25324;&#20195;&#34920;&#36755;&#20837;&#25991;&#26723;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#21644;&#30452;&#25509;&#29983;&#25104;&#27491;&#26679;&#26412;&#30340;&#32467;&#26500;&#32534;&#30721;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17307v1 Announce Type: new  Abstract: Existing self-supervised methods in natural language processing (NLP), especially hierarchical text classification (HTC), mainly focus on self-supervised contrastive learning, extremely relying on human-designed augmentation rules to generate contrastive samples, which can potentially corrupt or distort the original information. In this paper, we tend to investigate the feasibility of a contrastive learning scheme in which the semantic and syntactic information inherent in the input sample is adequately reserved in the contrastive samples and fused during the learning process. Specifically, we propose an information lossless contrastive learning strategy for HTC, namely \textbf{H}ierarchy-aware \textbf{I}nformation \textbf{L}ossless contrastive \textbf{L}earning (HILL), which consists of a text encoder representing the input document, and a structure encoder directly generating the positive sample. The structure encoder takes the documen
&lt;/p&gt;</description></item><item><title>&#35299;&#30721;&#25506;&#27979;&#36825;&#31181;&#26041;&#27861;&#25581;&#31034;&#20102;&#33258;&#30417;&#30563;&#35821;&#35328;&#27169;&#22411;&#22312;&#20013;&#38388;&#23618;&#25429;&#33719;&#25277;&#35937;&#30340;&#35821;&#35328;&#32467;&#26500;&#65292;&#25581;&#31034;&#20102;&#35821;&#27861;&#30340;&#23398;&#20064;&#38656;&#35201;&#26356;&#22810;&#23618;&#27425;&#65292;&#32780;&#24418;&#24577;&#21644;&#35821;&#20041;/&#21477;&#27861;&#25509;&#21475;&#30456;&#20851;&#29305;&#24449;&#21017;&#26356;&#38590;&#25429;&#33719;&#12290;</title><link>https://arxiv.org/abs/2403.17299</link><description>&lt;p&gt;
&#20351;&#29992;&#26368;&#23567;&#23545;&#27604;&#39033;&#25581;&#31034;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20869;&#37096;&#35821;&#35328;&#32467;&#26500;&#65306;&#35299;&#30721;&#25506;&#27979;
&lt;/p&gt;
&lt;p&gt;
Decoding Probing: Revealing Internal Linguistic Structures in Neural Language Models using Minimal Pairs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17299
&lt;/p&gt;
&lt;p&gt;
&#35299;&#30721;&#25506;&#27979;&#36825;&#31181;&#26041;&#27861;&#25581;&#31034;&#20102;&#33258;&#30417;&#30563;&#35821;&#35328;&#27169;&#22411;&#22312;&#20013;&#38388;&#23618;&#25429;&#33719;&#25277;&#35937;&#30340;&#35821;&#35328;&#32467;&#26500;&#65292;&#25581;&#31034;&#20102;&#35821;&#27861;&#30340;&#23398;&#20064;&#38656;&#35201;&#26356;&#22810;&#23618;&#27425;&#65292;&#32780;&#24418;&#24577;&#21644;&#35821;&#20041;/&#21477;&#27861;&#25509;&#21475;&#30456;&#20851;&#29305;&#24449;&#21017;&#26356;&#38590;&#25429;&#33719;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#30740;&#31350;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#35299;&#30721;&#25506;&#27979;&#8221;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#26368;&#23567;&#23545;&#27604;&#39033;&#22522;&#20934;&#65288;BLiMP&#65289;&#36880;&#23618;&#25506;&#26597;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20869;&#37096;&#35821;&#35328;&#29305;&#24449;&#12290;&#36890;&#36807;&#23558;&#35821;&#35328;&#27169;&#22411;&#35270;&#20026;&#8216;&#22823;&#33041;&#8217;&#65292;&#20854;&#34920;&#31034;&#20026;&#8216;&#31070;&#32463;&#28608;&#27963;&#8217;&#65292;&#25105;&#20204;&#20174;&#20013;&#38388;&#23618;&#30340;&#34920;&#31034;&#20013;&#35299;&#30721;&#26368;&#23567;&#23545;&#27604;&#39033;&#30340;&#35821;&#27861;&#26631;&#31614;&#12290;&#36825;&#31181;&#26041;&#27861;&#25581;&#31034;&#20102;&#65306;1&#65289;&#33258;&#30417;&#30563;&#35821;&#35328;&#27169;&#22411;&#22312;&#20013;&#38388;&#23618;&#25429;&#33719;&#20102;&#25277;&#35937;&#30340;&#35821;&#35328;&#32467;&#26500;&#65292;&#32780;GloVe&#21644;RNN&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#23398;&#20064;&#12290;2&#65289;&#26377;&#20851;&#21477;&#27861;&#35821;&#27861;&#24615;&#30340;&#20449;&#24687;&#36890;&#36807;GPT-2&#30340;&#21069;&#19977;&#23618;&#31283;&#20581;&#22320;&#25429;&#33719;&#65292;&#21516;&#26102;&#20063;&#20998;&#24067;&#22312;&#21518;&#32493;&#23618;&#20013;&#12290;&#38543;&#30528;&#21477;&#23376;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#38656;&#35201;&#26356;&#22810;&#23618;&#26469;&#23398;&#20064;&#35821;&#27861;&#33021;&#21147;&#12290;3&#65289;&#27604;&#36215;&#35821;&#27861;&#65292;&#24418;&#24577;&#21644;&#35821;&#20041;/&#21477;&#27861;&#25509;&#21475;&#30456;&#20851;&#29305;&#24449;&#26356;&#38590;&#25429;&#33719;&#12290;4&#65289;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17299v1 Announce Type: new  Abstract: Inspired by cognitive neuroscience studies, we introduce a novel `decoding probing' method that uses minimal pairs benchmark (BLiMP) to probe internal linguistic characteristics in neural language models layer by layer. By treating the language model as the `brain' and its representations as `neural activations', we decode grammaticality labels of minimal pairs from the intermediate layers' representations. This approach reveals: 1) Self-supervised language models capture abstract linguistic structures in intermediate layers that GloVe and RNN language models cannot learn. 2) Information about syntactic grammaticality is robustly captured through the first third layers of GPT-2 and also distributed in later layers. As sentence complexity increases, more layers are required for learning grammatical capabilities. 3) Morphological and semantics/syntax interface-related features are harder to capture than syntax. 4) For Transformer-based mod
&lt;/p&gt;</description></item><item><title>InternLM2&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#20840;&#38754;&#35780;&#20272;&#12289;&#38271;&#25991;&#26412;&#24314;&#27169;&#20197;&#21450;&#21019;&#26032;&#30340;&#39044;&#35757;&#32451;&#21644;&#20248;&#21270;&#25216;&#26415;&#19979;&#34920;&#29616;&#20986;&#33394;&#65292;&#36229;&#36234;&#20102;&#20854;&#21069;&#20219;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.17297</link><description>&lt;p&gt;
InternLM2&#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
InternLM2 Technical Report
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17297
&lt;/p&gt;
&lt;p&gt;
InternLM2&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#20840;&#38754;&#35780;&#20272;&#12289;&#38271;&#25991;&#26412;&#24314;&#27169;&#20197;&#21450;&#21019;&#26032;&#30340;&#39044;&#35757;&#32451;&#21644;&#20248;&#21270;&#25216;&#26415;&#19979;&#34920;&#29616;&#20986;&#33394;&#65292;&#36229;&#36234;&#20102;&#20854;&#21069;&#20219;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#65292;&#22914;ChatGPT&#21644;GPT-4&#65292;&#24341;&#21457;&#20102;&#20851;&#20110;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#21363;&#23558;&#21040;&#26469;&#30340;&#35752;&#35770;&#12290;&#28982;&#32780;&#65292;&#22312;&#24320;&#28304;&#27169;&#22411;&#20013;&#22797;&#21046;&#36825;&#26679;&#30340;&#36827;&#23637;&#19968;&#30452;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;InternLM2&#65292;&#19968;&#20010;&#24320;&#28304;&#30340;LLM&#65292;&#22312;6&#20010;&#32500;&#24230;&#21644;30&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#32988;&#36807;&#20854;&#21069;&#36744;&#65292;&#22312;&#38271;&#25991;&#26412;&#24314;&#27169;&#21644;&#20027;&#35266;&#35780;&#20272;&#26041;&#38754;&#20248;&#24322;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#39044;&#35757;&#32451;&#21644;&#20248;&#21270;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17297v1 Announce Type: cross  Abstract: The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has sparked discussions on the advent of Artificial General Intelligence (AGI). However, replicating such advancements in open-source models has been challenging. This paper introduces InternLM2, an open-source LLM that outperforms its predecessors in comprehensive evaluations across 6 dimensions and 30 benchmarks, long-context modeling, and open-ended subjective evaluations through innovative pre-training and optimization techniques. The pre-training process of InternLM2 is meticulously detailed, highlighting the preparation of diverse data types including text, code, and long-context data. InternLM2 efficiently captures long-term dependencies, initially trained on 4k tokens before advancing to 32k tokens in pre-training and fine-tuning stages, exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack" test. InternLM2 is further aligned using Supervised Fi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35782;&#21035;&#22810;&#27169;&#24577;&#23545;&#35805;&#20013;&#21442;&#19982;&#32773;&#20849;&#20139;&#20449;&#24565;&#20197;&#21450;&#27491;&#22312;&#35752;&#35770;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.17284</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#23545;&#35805;&#20013;&#30340;&#20849;&#21516;&#22320;&#38754;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
Common Ground Tracking in Multimodal Dialogue
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35782;&#21035;&#22810;&#27169;&#24577;&#23545;&#35805;&#20013;&#21442;&#19982;&#32773;&#20849;&#20139;&#20449;&#24565;&#20197;&#21450;&#27491;&#22312;&#35752;&#35770;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#23545;&#35805;&#24314;&#27169;&#30740;&#31350;&#24050;&#32463;&#33457;&#36153;&#20102;&#30456;&#24403;&#22810;&#30340;&#31934;&#21147;&#22312;&#8220;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#8221;&#65288;DST&#65289;&#19978;&#65292;&#21363;&#36890;&#36807;&#32771;&#34385;&#36807;&#21435;&#30340;&#23545;&#35805;&#31227;&#21160;&#21644;&#21382;&#21490;&#26469;&#26356;&#26032;&#27599;&#27425;&#23545;&#35805;&#20013;&#21457;&#35328;&#32773;&#38656;&#27714;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#23545;&#35805;&#24314;&#27169;&#20013;&#21516;&#26679;&#37325;&#35201;&#20294;&#30740;&#31350;&#36739;&#23569;&#30340;&#26159;&#8220;&#20849;&#21516;&#22320;&#38754;&#36319;&#36394;&#8221;&#65288;CGT&#65289;&#65292;&#23427;&#30830;&#23450;&#20102;&#25152;&#26377;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20013;&#25152;&#26377;&#21442;&#19982;&#32773;&#25345;&#26377;&#30340;&#20849;&#20139;&#20449;&#24565;&#31354;&#38388;&#65306;&#25152;&#26377;&#21442;&#19982;&#32773;&#25509;&#21463;&#20026;&#30495;&#30340;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#21629;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35782;&#21035;&#20855;&#26377;&#20849;&#20139;&#30446;&#26631;&#30340;&#32676;&#20307;&#30340;&#24403;&#21069;&#20849;&#20139;&#20449;&#24565;&#38598;&#21512;&#21644;&#8220;&#27491;&#22312;&#35752;&#35770;&#30340;&#38382;&#39064;&#8221;&#65288;QUDs&#65289;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#35821;&#38899;&#36716;&#24405;&#65292;&#35821;&#35843;&#29305;&#24449;&#65292;&#25163;&#21183;&#65292;&#34892;&#20026;&#21644;&#21327;&#20316;&#26041;&#38754;&#30340;&#35201;&#32032;&#23545;&#20849;&#20139;&#29289;&#29702;&#31354;&#38388;&#20013;&#30340;&#22810;&#27169;&#24577;&#20132;&#20114;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#26631;&#27880;&#65292;&#24182;&#20351;&#36825;&#20123;&#35201;&#32032;&#33021;&#22815;&#29992;&#20110;&#28145;&#24230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17284v1 Announce Type: new  Abstract: Within Dialogue Modeling research in AI and NLP, considerable attention has been spent on ``dialogue state tracking'' (DST), which is the ability to update the representations of the speaker's needs at each turn in the dialogue by taking into account the past dialogue moves and history. Less studied but just as important to dialogue modeling, however, is ``common ground tracking'' (CGT), which identifies the shared belief space held by all of the participants in a task-oriented dialogue: the task-relevant propositions all participants accept as true. In this paper we present a method for automatically identifying the current set of shared beliefs and ``questions under discussion'' (QUDs) of a group with a shared goal. We annotate a dataset of multimodal interactions in a shared physical space with speech transcriptions, prosodic features, gestures, actions, and facets of collaboration, and operationalize these features for use in a deep 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33258;&#21160;&#21270;&#26631;&#35760;&#25968;&#23398;&#38382;&#39064;&#20013;&#30340;&#30693;&#35782;&#27010;&#24565;&#65292;&#20197;&#28385;&#36275;&#20808;&#36827;&#25945;&#32946;&#24212;&#29992;&#23545;&#38382;&#39064;&#27010;&#24565;&#26631;&#35760;&#30340;&#22686;&#38271;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2403.17281</link><description>&lt;p&gt;
&#20351;&#29992;LLMs&#22312;&#25968;&#23398;&#38382;&#39064;&#19978;&#33258;&#21160;&#21270;&#30693;&#35782;&#27010;&#24565;&#26631;&#35760;
&lt;/p&gt;
&lt;p&gt;
Automate Knowledge Concept Tagging on Math Questions with LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17281
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33258;&#21160;&#21270;&#26631;&#35760;&#25968;&#23398;&#38382;&#39064;&#20013;&#30340;&#30693;&#35782;&#27010;&#24565;&#65292;&#20197;&#28385;&#36275;&#20808;&#36827;&#25945;&#32946;&#24212;&#29992;&#23545;&#38382;&#39064;&#27010;&#24565;&#26631;&#35760;&#30340;&#22686;&#38271;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17281v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#23545;&#38382;&#39064;&#36827;&#34892;&#30693;&#35782;&#27010;&#24565;&#26631;&#35760;&#22312;&#24403;&#20195;&#26234;&#33021;&#25945;&#32946;&#24212;&#29992;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#21253;&#25324;&#23398;&#20064;&#36827;&#24230;&#35786;&#26029;&#12289;&#32451;&#20064;&#39064;&#25512;&#33616;&#21644;&#35838;&#31243;&#20869;&#23481;&#32452;&#32455;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#20123;&#27880;&#37322;&#26159;&#22312;&#25945;&#32946;&#19987;&#23478;&#30340;&#24110;&#21161;&#19979;&#25163;&#21160;&#36827;&#34892;&#30340;&#65292;&#22240;&#20026;&#35813;&#20219;&#21153;&#19981;&#20165;&#38656;&#35201;&#23545;&#38382;&#39064;&#20027;&#24178;&#21644;&#30693;&#35782;&#23450;&#20041;&#26377;&#24378;&#22823;&#30340;&#35821;&#20041;&#29702;&#35299;&#65292;&#36824;&#38656;&#35201;&#28145;&#20837;&#20102;&#35299;&#38382;&#39064;&#35299;&#20915;&#36923;&#36753;&#19982;&#30456;&#24212;&#30693;&#35782;&#27010;&#24565;&#30340;&#32852;&#31995;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33258;&#21160;&#21270;&#26631;&#35760;&#20219;&#21153;&#65292;&#20197;&#21709;&#24212;&#20808;&#21069;&#25163;&#21160;&#26041;&#27861;&#26080;&#27861;&#28385;&#36275;&#20808;&#36827;&#25945;&#32946;&#24212;&#29992;&#23545;&#38382;&#39064;&#27010;&#24565;&#26631;&#35760;&#30340;&#24555;&#36895;&#22686;&#38271;&#38656;&#27714;&#30340;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;LLMs &#30340;&#38646;/&#23569;&#27425;&#23398;&#20064;&#33021;&#21147;&#20351;&#20854;&#38750;&#24120;&#36866;&#21512;&#25945;&#32946;&#22330;&#26223;&#24212;&#29992;&#65292;&#36825;&#20123;&#22330;&#26223;&#36890;&#24120;&#38754;&#20020;&#22312;&#25910;&#38598;&#26102;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17281v1 Announce Type: new  Abstract: Knowledge concept tagging for questions plays a crucial role in contemporary intelligent educational applications, including learning progress diagnosis, practice question recommendations, and course content organization. Traditionally, these annotations have been conducted manually with help from pedagogical experts, as the task requires not only a strong semantic understanding of both question stems and knowledge definitions but also deep insights into connecting question-solving logic with corresponding knowledge concepts. In this paper, we explore automating the tagging task using Large Language Models (LLMs), in response to the inability of prior manual methods to meet the rapidly growing demand for concept tagging in questions posed by advanced educational applications. Moreover, the zero/few-shot learning capability of LLMs makes them well-suited for application in educational scenarios, which often face challenges in collecting l
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#20013;&#25163;&#21160;&#27880;&#37322;&#25968;&#25454;&#38598;&#26114;&#36149;&#21644;&#32791;&#26102;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.17254</link><description>&lt;p&gt;
&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#30340;&#28151;&#21512;&#26041;&#27861;&#36827;&#34892;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Hybrid Approach To Aspect Based Sentiment Analysis Using Transfer Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17254
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#20013;&#25163;&#21160;&#27880;&#37322;&#25968;&#25454;&#38598;&#26114;&#36149;&#21644;&#32791;&#26102;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Aspect-Based Sentiment Analysis (ABSA)&#26088;&#22312;&#35782;&#21035;&#34920;&#36798;&#24773;&#24863;&#30340;&#26415;&#35821;&#25110;&#22810;&#35789;&#34920;&#36798;(MWEs)&#20197;&#21450;&#19982;&#20043;&#30456;&#20851;&#30340;&#24773;&#24863;&#26497;&#24615;&#12290;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#20013;&#65292;&#30417;&#30563;&#27169;&#22411;&#30340;&#21457;&#23637;&#19968;&#30452;&#22788;&#20110;&#30740;&#31350;&#21069;&#27839;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#25163;&#21160;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#26082;&#26114;&#36149;&#21448;&#32791;&#26102;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#24102;&#26631;&#35760;&#25968;&#25454;&#38598;&#38024;&#23545;&#29305;&#23450;&#39046;&#22495;&#12289;&#35821;&#35328;&#21644;&#25991;&#26412;&#31867;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;ABSA&#30740;&#31350;&#20013;&#30340;&#36825;&#19968;&#26174;&#33879;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#36827;&#34892;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#30340;&#28151;&#21512;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#20256;&#32479;&#21477;&#27861;&#20381;&#36182;&#30340;&#20248;&#21183;&#29983;&#25104;&#24369;&#30417;&#30563;&#26631;&#27880;&#12290;&#25105;&#20204;&#21033;&#29992;&#21477;&#23376;&#30340;&#21477;&#27861;&#20381;&#36182;&#32467;&#26500;&#26469;&#34917;&#20805;&#29983;&#25104;&#30340;&#26631;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17254v1 Announce Type: new  Abstract: Aspect-Based Sentiment Analysis (ABSA) aims to identify terms or multiword expressions (MWEs) on which sentiments are expressed and the sentiment polarities associated with them. The development of supervised models has been at the forefront of research in this area. However, training these models requires the availability of manually annotated datasets which is both expensive and time-consuming. Furthermore, the available annotated datasets are tailored to a specific domain, language, and text type. In this work, we address this notable challenge in current state-of-the-art ABSA research. We propose a hybrid approach for Aspect Based Sentiment Analysis using transfer learning. The approach focuses on generating weakly-supervised annotations by exploiting the strengths of both large language models (LLM) and traditional syntactic dependencies. We utilise syntactic dependency structures of sentences to complement the annotations generated
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23558;&#32463;&#20856;&#35268;&#21010;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#36817;&#20284;&#20154;&#31867;&#30452;&#35273;&#65292;&#20197;&#23454;&#29616;&#22810;&#26234;&#33021;&#20307;&#20219;&#21153;&#35268;&#21010;&#12290;</title><link>https://arxiv.org/abs/2403.17246</link><description>&lt;p&gt;
TwoStep: &#20351;&#29992;&#32463;&#20856;&#35268;&#21010;&#22120;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#26234;&#33021;&#20307;&#20219;&#21153;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
TwoStep: Multi-agent Task Planning using Classical Planners and Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17246
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23558;&#32463;&#20856;&#35268;&#21010;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#36817;&#20284;&#20154;&#31867;&#30452;&#35273;&#65292;&#20197;&#23454;&#29616;&#22810;&#26234;&#33021;&#20307;&#20219;&#21153;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#20284;&#35268;&#21010;&#39046;&#22495;&#23450;&#20041;&#35821;&#35328;&#65288;PDDL&#65289;&#20043;&#31867;&#30340;&#32463;&#20856;&#35268;&#21010;&#20844;&#24335;&#20801;&#35768;&#30830;&#23450;&#21487;&#23454;&#29616;&#30446;&#26631;&#29366;&#24577;&#30340;&#21160;&#20316;&#24207;&#21015;&#65292;&#21482;&#35201;&#23384;&#22312;&#20219;&#20309;&#21487;&#33021;&#30340;&#21021;&#22987;&#29366;&#24577;&#12290;&#28982;&#32780;&#65292;PDDL&#20013;&#23450;&#20041;&#30340;&#25512;&#29702;&#38382;&#39064;&#24182;&#26410;&#25429;&#33719;&#34892;&#21160;&#36827;&#34892;&#30340;&#26102;&#38388;&#26041;&#38754;&#65292;&#20363;&#22914;&#39046;&#22495;&#20013;&#30340;&#20004;&#20010;&#26234;&#33021;&#20307;&#22914;&#26524;&#24444;&#27492;&#30340;&#21518;&#20917;&#19981;&#24178;&#25200;&#21069;&#25552;&#26465;&#20214;&#65292;&#21017;&#21487;&#20197;&#21516;&#26102;&#25191;&#34892;&#19968;&#20010;&#21160;&#20316;&#12290;&#20154;&#31867;&#19987;&#23478;&#21487;&#20197;&#23558;&#30446;&#26631;&#20998;&#35299;&#20026;&#22823;&#37096;&#20998;&#29420;&#31435;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#23558;&#27599;&#20010;&#26234;&#33021;&#20307;&#20998;&#37197;&#32473;&#20854;&#20013;&#19968;&#20010;&#23376;&#30446;&#26631;&#65292;&#20197;&#21033;&#29992;&#21516;&#26102;&#36827;&#34892;&#21160;&#20316;&#26469;&#21152;&#24555;&#35745;&#21010;&#27493;&#39588;&#30340;&#25191;&#34892;&#65292;&#27599;&#20010;&#37096;&#20998;&#20165;&#20351;&#29992;&#21333;&#20010;&#26234;&#33021;&#20307;&#35268;&#21010;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#30452;&#25509;&#25512;&#26029;&#35745;&#21010;&#27493;&#39588;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24182;&#19981;&#20445;&#35777;&#25191;&#34892;&#25104;&#21151;&#65292;&#20294;&#21033;&#29992;&#24120;&#35782;&#25512;&#29702;&#26469;&#32452;&#35013;&#21160;&#20316;&#24207;&#21015;&#12290;&#25105;&#20204;&#36890;&#36807;&#36817;&#20284;&#20154;&#31867;&#30452;&#35273;&#65292;&#32467;&#21512;&#20102;&#32463;&#20856;&#35268;&#21010;&#21644;LLMs&#30340;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17246v1 Announce Type: new  Abstract: Classical planning formulations like the Planning Domain Definition Language (PDDL) admit action sequences guaranteed to achieve a goal state given an initial state if any are possible. However, reasoning problems defined in PDDL do not capture temporal aspects of action taking, for example that two agents in the domain can execute an action simultaneously if postconditions of each do not interfere with preconditions of the other. A human expert can decompose a goal into largely independent constituent parts and assign each agent to one of these subgoals to take advantage of simultaneous actions for faster execution of plan steps, each using only single agent planning. By contrast, large language models (LLMs) used for directly inferring plan steps do not guarantee execution success, but do leverage commonsense reasoning to assemble action sequences. We combine the strengths of classical planning and LLMs by approximating human intuition
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#32467;&#21512;&#29616;&#26377;&#30340;&#23884;&#22871;NER&#31995;&#32479;&#30340;&#39044;&#27979;&#25552;&#21450;&#21644;&#20174;OntoNotes&#21477;&#27861;&#26641;&#23548;&#20986;&#30340;&#29305;&#24449;&#65292;&#35299;&#20915;&#20102;&#22312;&#33521;&#35821;&#31471;&#21040;&#31471;&#31070;&#32463;&#25351;&#20195;&#28040;&#35299;&#20013;&#20351;&#29992;OntoNotes&#22522;&#20934;&#26102;&#21333;&#20363;&#25552;&#21450;&#36328;&#24230;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SPLICE&#30340;&#20004;&#27493;&#31070;&#32463;&#25552;&#21450;&#21644;&#25351;&#20195;&#28040;&#35299;&#31995;&#32479;&#65292;&#24182;&#22312;OntoNotes&#27979;&#35797;&#38598;&#21644;&#22495;&#22806;OntoGUM&#35821;&#26009;&#24211;&#19978;&#23545;&#20854;&#24615;&#33021;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#37325;&#24314;&#30340;&#21333;&#20363;&#35757;&#32451;&#25928;&#26524;&#33391;&#22909;&#12290;</title><link>https://arxiv.org/abs/2403.17245</link><description>&lt;p&gt;
SPLICE&#65306;&#21333;&#20363;&#22686;&#24378;&#31649;&#36947;&#29992;&#20110;&#25351;&#20195;&#28040;&#35299;
&lt;/p&gt;
&lt;p&gt;
SPLICE: A Singleton-Enhanced PipeLIne for Coreference REsolution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17245
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#32467;&#21512;&#29616;&#26377;&#30340;&#23884;&#22871;NER&#31995;&#32479;&#30340;&#39044;&#27979;&#25552;&#21450;&#21644;&#20174;OntoNotes&#21477;&#27861;&#26641;&#23548;&#20986;&#30340;&#29305;&#24449;&#65292;&#35299;&#20915;&#20102;&#22312;&#33521;&#35821;&#31471;&#21040;&#31471;&#31070;&#32463;&#25351;&#20195;&#28040;&#35299;&#20013;&#20351;&#29992;OntoNotes&#22522;&#20934;&#26102;&#21333;&#20363;&#25552;&#21450;&#36328;&#24230;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SPLICE&#30340;&#20004;&#27493;&#31070;&#32463;&#25552;&#21450;&#21644;&#25351;&#20195;&#28040;&#35299;&#31995;&#32479;&#65292;&#24182;&#22312;OntoNotes&#27979;&#35797;&#38598;&#21644;&#22495;&#22806;OntoGUM&#35821;&#26009;&#24211;&#19978;&#23545;&#20854;&#24615;&#33021;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#37325;&#24314;&#30340;&#21333;&#20363;&#35757;&#32451;&#25928;&#26524;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17245v1  &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#30340;  &#25688;&#35201;&#65306;&#21333;&#20363;&#25552;&#21450;&#65292;&#21363;&#25991;&#26412;&#20013;&#20165;&#34987;&#25552;&#21450;&#19968;&#27425;&#30340;&#23454;&#20307;&#65292;&#20174;&#29702;&#35770;&#35282;&#24230;&#26469;&#30475;&#23545;&#20154;&#31867;&#29702;&#35299;&#35805;&#35821;&#24456;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#23558;&#20854;&#26816;&#27979;&#32435;&#20837;&#29992;&#20110;&#33521;&#35821;&#31471;&#21040;&#31471;&#31070;&#32463;&#25351;&#20195;&#28040;&#35299;&#30340;&#23581;&#35797;&#21463;&#21040;&#20102;OntoNotes&#22522;&#20934;&#20013;&#21333;&#20363;&#25552;&#21450;&#36328;&#24230;&#19981;&#36275;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#29616;&#26377;&#30340;&#23884;&#22871;NER&#31995;&#32479;&#30340;&#39044;&#27979;&#25552;&#21450;&#19982;&#20174;OntoNotes&#21477;&#27861;&#26641;&#23548;&#20986;&#30340;&#29305;&#24449;&#30456;&#32467;&#21512;&#26469;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#12290;&#20511;&#21161;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#36817;&#20284;&#21253;&#21547;&#25152;&#26377;&#21333;&#20363;&#25552;&#21450;&#30340;OntoNotes&#25968;&#25454;&#38598;&#65292;&#23545;&#37329;&#26631;&#20934;&#21333;&#20363;&#26679;&#26412;&#36798;&#21040;&#20102;&#32422;94%&#30340;&#21484;&#22238;&#29575;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SPLICE&#30340;&#20004;&#27493;&#31070;&#32463;&#25552;&#21450;&#21644;&#25351;&#20195;&#28040;&#35299;&#31995;&#32479;&#65292;&#24182;&#23558;&#20854;&#24615;&#33021;&#19982;&#31471;&#21040;&#31471;&#26041;&#27861;&#22312;&#20004;&#31181;&#24773;&#26223;&#19979;&#36827;&#34892;&#20102;&#27604;&#36739;&#65306;OntoNotes&#27979;&#35797;&#38598;&#21644;&#22495;&#22806;&#65288;OOD&#65289;OntoGUM&#35821;&#26009;&#24211;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#37325;&#24314;&#30340;&#21333;&#20363;&#35757;&#32451;&#20135;&#29983;&#20102;&#36739;&#39640;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17245v1 Announce Type: new  Abstract: Singleton mentions, i.e.~entities mentioned only once in a text, are important to how humans understand discourse from a theoretical perspective. However previous attempts to incorporate their detection in end-to-end neural coreference resolution for English have been hampered by the lack of singleton mention spans in the OntoNotes benchmark. This paper addresses this limitation by combining predicted mentions from existing nested NER systems and features derived from OntoNotes syntax trees. With this approach, we create a near approximation of the OntoNotes dataset with all singleton mentions, achieving ~94% recall on a sample of gold singletons. We then propose a two-step neural mention and coreference resolution system, named SPLICE, and compare its performance to the end-to-end approach in two scenarios: the OntoNotes test set and the out-of-domain (OOD) OntoGUM corpus. Results indicate that reconstructed singleton training yields re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#25506;&#35752;&#20102;&#22312;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#21476;&#20856;$n$-gram&#24179;&#28369;&#25216;&#26415;&#21487;&#33021;&#21457;&#25381;&#30340;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#23558;&#20219;&#20309;$n$-gram&#24179;&#28369;&#25216;&#26415;&#36716;&#25442;&#20026;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#20860;&#23481;&#27491;&#21017;&#21270;&#22120;&#30340;&#36890;&#29992;&#26694;&#26550;</title><link>https://arxiv.org/abs/2403.17240</link><description>&lt;p&gt;
&#22312;&#31070;&#32463;&#32593;&#32476;&#26102;&#20195;&#30340;$n$-gram&#24179;&#28369;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
The Role of $n$-gram Smoothing in the Age of Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17240
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#25506;&#35752;&#20102;&#22312;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#21476;&#20856;$n$-gram&#24179;&#28369;&#25216;&#26415;&#21487;&#33021;&#21457;&#25381;&#30340;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#23558;&#20219;&#20309;$n$-gram&#24179;&#28369;&#25216;&#26415;&#36716;&#25442;&#20026;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#20860;&#23481;&#27491;&#21017;&#21270;&#22120;&#30340;&#36890;&#29992;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23558;&#36817;&#19977;&#21313;&#24180;&#30340;&#26102;&#38388;&#37324;&#65292;&#22522;&#20110;$n$-gram&#20551;&#35774;&#30340;&#35821;&#35328;&#27169;&#22411;&#19968;&#30452;&#26159;&#35813;&#20219;&#21153;&#30340;&#25216;&#26415;&#27700;&#24179;&#12290;&#23427;&#20204;&#25104;&#21151;&#30340;&#20851;&#38190;&#22312;&#20110;&#24212;&#29992;&#21508;&#31181;&#24179;&#28369;&#25216;&#26415;&#26469;&#23545;&#25239;&#36807;&#25311;&#21512;&#12290;&#28982;&#32780;&#65292;&#24403;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#21462;&#20195;$n$-gram&#27169;&#22411;&#25104;&#20026;&#26368;&#20339;&#34920;&#29616;&#32773;&#26102;&#65292;$n$-gram&#24179;&#28369;&#25216;&#26415;&#21464;&#24471;&#19981;&#22826;&#30456;&#20851;&#12290;&#20107;&#23454;&#19978;&#65292;&#21487;&#20197;&#27627;&#19981;&#22840;&#24352;&#22320;&#35828;&#65292;&#23545;$n$-gram&#24179;&#28369;&#25216;&#26415;&#30340;&#30740;&#31350;&#22312;&#36825;&#19968;&#26102;&#20195;&#21464;&#24471;&#20572;&#28382;&#12290;&#26412;&#25991;&#37325;&#26032;&#25506;&#35752;&#20102;&#22312;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#21476;&#20856;$n$-gram&#24179;&#28369;&#25216;&#26415;&#21487;&#33021;&#21457;&#25381;&#30340;&#20316;&#29992;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22312;&#26631;&#31614;&#24179;&#28369;&#21644;add-$\lambda$&#24179;&#28369;&#20043;&#38388;&#24314;&#31435;&#20102;&#19968;&#20010;&#27491;&#24335;&#31561;&#20215;&#24615;&#65292;&#26631;&#31614;&#24179;&#28369;&#26159;&#19968;&#31181;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#30340;&#27969;&#34892;&#27491;&#21017;&#21270;&#25216;&#26415;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#23558;\emph{&#20219;&#20309;} $n$-gram&#24179;&#28369;&#25216;&#26415;&#36716;&#25442;&#20026;&#19982;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#20860;&#23481;&#30340;&#27491;&#21017;&#21270;&#22120;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17240v1 Announce Type: new  Abstract: For nearly three decades, language models derived from the $n$-gram assumption held the state of the art on the task. The key to their success lay in the application of various smoothing techniques that served to combat overfitting. However, when neural language models toppled $n$-gram models as the best performers, $n$-gram smoothing techniques became less relevant. Indeed, it would hardly be an understatement to suggest that the line of inquiry into $n$-gram smoothing techniques became dormant. This paper re-opens the role classical $n$-gram smoothing techniques may play in the age of neural language models. First, we draw a formal equivalence between label smoothing, a popular regularization technique for neural language models, and add-$\lambda$ smoothing. Second, we derive a generalized framework for converting \emph{any} $n$-gram smoothing technique into a regularizer compatible with neural language models. Our empirical results fi
&lt;/p&gt;</description></item><item><title>RoLASER&#26159;&#19968;&#20010;&#36890;&#36807;&#24072;&#29983;&#26041;&#27861;&#35757;&#32451;&#30340;&#40065;&#26834;&#33521;&#25991;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#20943;&#23569;&#26631;&#20934;&#21477;&#23376;&#21644;UGC&#21477;&#23376;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#36317;&#31163;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;LASER&#23545;&#33258;&#28982;&#21644;&#20154;&#24037;UGC&#25968;&#25454;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.17220</link><description>&lt;p&gt;
&#20351;&#21477;&#23376;&#23884;&#20837;&#23545;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#20855;&#26377;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Making Sentence Embeddings Robust to User-Generated Content
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17220
&lt;/p&gt;
&lt;p&gt;
RoLASER&#26159;&#19968;&#20010;&#36890;&#36807;&#24072;&#29983;&#26041;&#27861;&#35757;&#32451;&#30340;&#40065;&#26834;&#33521;&#25991;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#20943;&#23569;&#26631;&#20934;&#21477;&#23376;&#21644;UGC&#21477;&#23376;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#36317;&#31163;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;LASER&#23545;&#33258;&#28982;&#21644;&#20154;&#24037;UGC&#25968;&#25454;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#22312;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#65288;UGC&#65289;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#23427;&#21576;&#29616;&#20102;&#22823;&#37327;&#35789;&#27719;&#21464;&#21270;&#65292;&#24182;&#20559;&#31163;&#20102;&#22823;&#22810;&#25968;&#36825;&#20123;&#27169;&#22411;&#35757;&#32451;&#30340;&#26631;&#20934;&#25991;&#26412;&#12290;&#26412;&#25991;&#20851;&#27880;LASER&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#23545;UGC&#25968;&#25454;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;LASER&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#34920;&#31034;&#38750;&#26631;&#20934;&#21477;&#23376;&#21450;&#20854;&#26631;&#20934;&#23545;&#24212;&#21477;&#23376;&#30340;&#33021;&#21147;&#26469;&#35780;&#20272;&#36825;&#31181;&#40065;&#26834;&#24615;&#12290;&#21463;&#20808;&#21069;&#25193;&#23637;LASER&#21040;&#20854;&#20182;&#35821;&#35328;&#21644;&#24418;&#24335;&#30340;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;RoLASER&#65292;&#19968;&#20010;&#20351;&#29992;&#24072;&#29983;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#30340;&#40065;&#26834;&#33521;&#25991;&#32534;&#30721;&#22120;&#65292;&#20197;&#20943;&#23569;&#26631;&#20934;&#21477;&#23376;&#21644;UGC&#21477;&#23376;&#34920;&#31034;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#20165;&#36890;&#36807;&#22312;&#26631;&#20934;&#21644;&#21512;&#25104;UGC&#26679;&#26412;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;RoLASER&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;LASER&#23545;&#33258;&#28982;&#21644;&#20154;&#24037;UGC&#25968;&#25454;&#30340;&#40065;&#26834;&#24615;&#65292;&#26368;&#22810;&#21487;&#20197;&#23454;&#29616;2&#20493;&#21644;11&#20493;&#30340;&#25913;&#36827;&#20998;&#25968;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#32454;&#31890;&#24230;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17220v1 Announce Type: new  Abstract: NLP models have been known to perform poorly on user-generated content (UGC), mainly because it presents a lot of lexical variations and deviates from the standard texts on which most of these models were trained. In this work, we focus on the robustness of LASER, a sentence embedding model, to UGC data. We evaluate this robustness by LASER's ability to represent non-standard sentences and their standard counterparts close to each other in the embedding space. Inspired by previous works extending LASER to other languages and modalities, we propose RoLASER, a robust English encoder trained using a teacher-student approach to reduce the distances between the representations of standard and UGC sentences. We show that with training only on standard and synthetic UGC-like data, RoLASER significantly improves LASER's robustness to both natural and artificial UGC data by achieving up to 2x and 11x better scores. We also perform a fine-grained 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#21644;&#27010;&#24565;&#23884;&#20837;&#36827;&#34892;&#26412;&#20307;&#34917;&#20840;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#36825;&#20004;&#31181;&#26041;&#27861;&#20114;&#34917;&#65292;&#28151;&#21512;&#31574;&#30053;&#21462;&#24471;&#26368;&#20339;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.17216</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#21644;&#27010;&#24565;&#23884;&#20837;&#36827;&#34892;&#26412;&#20307;&#34917;&#20840;&#65306;&#19968;&#39033;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Ontology Completion with Natural Language Inference and Concept Embeddings: An Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#21644;&#27010;&#24565;&#23884;&#20837;&#36827;&#34892;&#26412;&#20307;&#34917;&#20840;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#36825;&#20004;&#31181;&#26041;&#27861;&#20114;&#34917;&#65292;&#28151;&#21512;&#31574;&#30053;&#21462;&#24471;&#26368;&#20339;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#25214;&#21040;&#32473;&#23450;&#26412;&#20307;&#20013;&#32570;&#22833;&#30340;&#21512;&#29702;&#30693;&#35782;&#30340;&#38382;&#39064;&#65292;&#20316;&#20026;&#23545;&#24191;&#27867;&#30740;&#31350;&#30340;&#20998;&#31867;&#27861;&#25193;&#23637;&#20219;&#21153;&#30340;&#27010;&#25324;&#12290;&#19968;&#31181;&#26041;&#27861;&#23558;&#36825;&#19968;&#20219;&#21153;&#35270;&#20026;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#38382;&#39064;&#65292;&#20381;&#36182;&#20110;&#35821;&#35328;&#27169;&#22411;&#25429;&#33719;&#30340;&#30693;&#35782;&#26469;&#35782;&#21035;&#32570;&#22833;&#30340;&#30693;&#35782;&#12290;&#21478;&#19968;&#31181;&#26041;&#27861;&#20351;&#29992;&#27010;&#24565;&#23884;&#20837;&#26469;&#30830;&#23450;&#19981;&#21516;&#27010;&#24565;&#20043;&#38388;&#30340;&#20849;&#21516;&#20043;&#22788;&#65292;&#21463;&#35748;&#30693;&#27169;&#22411;&#23545;&#22522;&#20110;&#31867;&#21035;&#24402;&#32435;&#30340;&#21551;&#21457;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#22312;&#30452;&#35273;&#19978;&#26159;&#20114;&#34917;&#30340;&#65292;&#20294;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#23578;&#26410;&#36827;&#34892;&#27604;&#36739;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#26412;&#20307;&#34917;&#20840;&#26041;&#27861;&#30340;&#22522;&#20934;&#65292;&#24182;&#24443;&#24213;&#20998;&#26512;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20004;&#31181;&#26041;&#27861;&#30830;&#23454;&#26159;&#20114;&#34917;&#30340;&#65292;&#28151;&#21512;&#31574;&#30053;&#23454;&#29616;&#20102;&#26368;&#20339;&#30340;&#25972;&#20307;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#36825;&#19968;&#20219;&#21153;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35828;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17216v1 Announce Type: new  Abstract: We consider the problem of finding plausible knowledge that is missing from a given ontology, as a generalisation of the well-studied taxonomy expansion task. One line of work treats this task as a Natural Language Inference (NLI) problem, thus relying on the knowledge captured by language models to identify the missing knowledge. Another line of work uses concept embeddings to identify what different concepts have in common, taking inspiration from cognitive models for category based induction. These two approaches are intuitively complementary, but their effectiveness has not yet been compared. In this paper, we introduce a benchmark for evaluating ontology completion methods and thoroughly analyse the strengths and weaknesses of both approaches. We find that both approaches are indeed complementary, with hybrid strategies achieving the best overall results. We also find that the task is highly challenging for Large Language Models, ev
&lt;/p&gt;</description></item><item><title>&#27604;&#36739;&#20102;&#22522;&#20110;&#35268;&#21017;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20174;&#20020;&#24202;&#31934;&#31070;&#30149;&#23398;&#31508;&#35760;&#20013;&#25552;&#21462;&#31038;&#20250;&#25903;&#25345;&#21644;&#31038;&#20250;&#23396;&#31435;&#20449;&#24687;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#32467;&#26524;&#26174;&#31034;&#22522;&#20110;&#35268;&#21017;&#31995;&#32479;&#22312;&#20004;&#20010;&#21307;&#30103;&#26426;&#26500;&#20013;&#37117;&#33719;&#24471;&#20102;&#26356;&#39640;&#30340;&#20998;&#25968;&#12290;</title><link>https://arxiv.org/abs/2403.17199</link><description>&lt;p&gt;
&#20174;&#20020;&#24202;&#31934;&#31070;&#30149;&#23398;&#31508;&#35760;&#20013;&#25552;&#21462;&#31038;&#20250;&#25903;&#25345;&#21644;&#31038;&#20250;&#23396;&#31435;&#20449;&#24687;&#65306;&#27604;&#36739;&#22522;&#20110;&#35268;&#21017;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Extracting Social Support and Social Isolation Information from Clinical Psychiatry Notes: Comparing a Rule-based NLP System and a Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17199
&lt;/p&gt;
&lt;p&gt;
&#27604;&#36739;&#20102;&#22522;&#20110;&#35268;&#21017;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20174;&#20020;&#24202;&#31934;&#31070;&#30149;&#23398;&#31508;&#35760;&#20013;&#25552;&#21462;&#31038;&#20250;&#25903;&#25345;&#21644;&#31038;&#20250;&#23396;&#31435;&#20449;&#24687;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#32467;&#26524;&#26174;&#31034;&#22522;&#20110;&#35268;&#21017;&#31995;&#32479;&#22312;&#20004;&#20010;&#21307;&#30103;&#26426;&#26500;&#20013;&#37117;&#33719;&#24471;&#20102;&#26356;&#39640;&#30340;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#31038;&#20250;&#25903;&#25345;&#65288;SS&#65289;&#21644;&#31038;&#20250;&#23396;&#31435;&#65288;SI&#65289;&#26159;&#19982;&#31934;&#31070;&#30149;&#23398;&#32467;&#26524;&#30456;&#20851;&#30340;&#20581;&#24247;&#31038;&#20250;&#20915;&#23450;&#22240;&#32032;&#65288;SDOH&#65289;&#12290;&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHRs&#65289;&#20013;&#65292;&#20010;&#20307;&#32423;SS/SI&#36890;&#24120;&#34987;&#35760;&#24405;&#20026;&#21465;&#36848;&#24615;&#20020;&#24202;&#31508;&#35760;&#65292;&#32780;&#19981;&#26159;&#32467;&#26500;&#21270;&#32534;&#30721;&#25968;&#25454;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31639;&#27861;&#21487;&#20197;&#33258;&#21160;&#21270;&#25968;&#25454;&#25552;&#21462;&#30340;&#21171;&#21160;&#23494;&#38598;&#22411;&#36807;&#31243;&#12290;&#25968;&#25454;&#19982;&#26041;&#27861;&#65306;&#26469;&#33258;Mount Sinai Health System&#65288;MSHS&#65292;n=300&#65289;&#21644;Weill Cornell Medicine&#65288;WCM&#65292;n=225&#65289;&#30340;&#31934;&#31070;&#30149;&#23398;&#20250;&#35786;&#31508;&#35760;&#34987;&#27880;&#37322;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#40644;&#37329;&#26631;&#20934;&#35821;&#26009;&#24211;&#12290;&#24320;&#21457;&#20102;&#19968;&#20010;&#28041;&#21450;&#35789;&#27719;&#34920;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#31995;&#32479;&#65288;RBS&#65289;&#21644;&#20351;&#29992;FLAN-T5-XL&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#35782;&#21035;SS&#21644;SI&#30340;&#25552;&#21450;&#21450;&#20854;&#23376;&#31867;&#21035;&#65288;&#20363;&#22914;&#65292;&#31038;&#20250;&#32593;&#32476;&#12289;&#24037;&#20855;&#24615;&#25903;&#25345;&#21644;&#23396;&#29420;&#65289;&#12290;&#32467;&#26524;&#65306;&#23545;&#20110;&#25552;&#21462;SS/SI&#65292;RBS&#22312;MSHS&#65288;0.89 vs. 0.65&#65289;&#21644;WCM&#65288;0.75 vs. 0.51&#65289;&#22343;&#33719;&#24471;&#20102;&#26356;&#39640;&#30340;&#23439;&#24179;&#22343;f&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17199v1 Announce Type: new  Abstract: Background: Social support (SS) and social isolation (SI) are social determinants of health (SDOH) associated with psychiatric outcomes. In electronic health records (EHRs), individual-level SS/SI is typically documented as narrative clinical notes rather than structured coded data. Natural language processing (NLP) algorithms can automate the otherwise labor-intensive process of data extraction.   Data and Methods: Psychiatric encounter notes from Mount Sinai Health System (MSHS, n=300) and Weill Cornell Medicine (WCM, n=225) were annotated and established a gold standard corpus. A rule-based system (RBS) involving lexicons and a large language model (LLM) using FLAN-T5-XL were developed to identify mentions of SS and SI and their subcategories (e.g., social network, instrumental support, and loneliness).   Results: For extracting SS/SI, the RBS obtained higher macro-averaged f-scores than the LLM at both MSHS (0.89 vs. 0.65) and WCM (0
&lt;/p&gt;</description></item><item><title>GPT-4&#22312;&#26631;&#20934;&#21270;&#35821;&#31687;&#29702;&#35299;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#30456;&#24403;&#30340;&#33021;&#21147;&#65292;&#23588;&#20854;&#22312;&#25512;&#26029;&#26410;&#26126;&#30830;&#38472;&#36848;&#20449;&#24687;&#26041;&#38754;&#26174;&#31034;&#20986;&#26174;&#33879;&#23454;&#21147;</title><link>https://arxiv.org/abs/2403.17196</link><description>&lt;p&gt;
GPT-4&#33267;&#23569;&#33021;&#22815;&#20687;&#20154;&#31867;&#19968;&#26679;&#29702;&#35299;&#35821;&#31687;
&lt;/p&gt;
&lt;p&gt;
GPT-4 Understands Discourse at Least as Well as Humans Do
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17196
&lt;/p&gt;
&lt;p&gt;
GPT-4&#22312;&#26631;&#20934;&#21270;&#35821;&#31687;&#29702;&#35299;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#30456;&#24403;&#30340;&#33021;&#21147;&#65292;&#23588;&#20854;&#22312;&#25512;&#26029;&#26410;&#26126;&#30830;&#38472;&#36848;&#20449;&#24687;&#26041;&#38754;&#26174;&#31034;&#20986;&#26174;&#33879;&#23454;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#27979;&#35797;&#20102;&#19968;&#31181;&#39046;&#20808;&#30340;AI&#31995;&#32479;GPT-4&#26159;&#21542;&#20687;&#20154;&#31867;&#19968;&#26679;&#29702;&#35299;&#35821;&#31687;&#65292;&#20351;&#29992;&#20102;&#19968;&#39033;&#26631;&#20934;&#21270;&#30340;&#35821;&#31687;&#29702;&#35299;&#27979;&#35797;&#12290;&#21442;&#19982;&#32773;&#20250;&#34987;&#21576;&#29616;&#31616;&#30701;&#30340;&#25925;&#20107;&#65292;&#28982;&#21518;&#22238;&#31572;&#20843;&#20010;&#26159;/&#21542;&#38382;&#39064;&#65292;&#25506;&#31350;&#20182;&#20204;&#23545;&#25925;&#20107;&#30340;&#29702;&#35299;&#12290;&#36825;&#20123;&#38382;&#39064;&#30340;&#26684;&#24335;&#26088;&#22312;&#35780;&#20272;&#30452;&#25509;&#24615;&#65288;&#38472;&#36848; vs. &#26263;&#31034;&#65289;&#21644;&#26174;&#33879;&#24615;&#65288;&#20027;&#35201;&#35266;&#28857; vs. &#32454;&#33410;&#65289;&#30340;&#29420;&#31435;&#24433;&#21709;&#12290;&#37492;&#20110;&#20154;&#31867;&#34920;&#29616;&#27700;&#24179;&#38750;&#24120;&#39640;&#65292;GPT-4&#30340;&#34920;&#29616;&#30053;&#22909;&#20110;&#20154;&#31867;&#65292;&#20294;&#24182;&#26080;&#32479;&#35745;&#23398;&#26174;&#33879;&#24046;&#24322;&#12290;GPT-4&#21644;&#20154;&#31867;&#37117;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#33021;&#22815;&#25512;&#26029;&#25925;&#20107;&#20013;&#26410;&#26126;&#30830;&#38472;&#36848;&#30340;&#20449;&#24687;&#65292;&#36825;&#26159;&#23545;&#29702;&#35299;&#21147;&#30340;&#37325;&#35201;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17196v1 Announce Type: new  Abstract: We test whether a leading AI system GPT-4 understands discourse as well as humans do, using a standardized test of discourse comprehension. Participants are presented with brief stories and then answer eight yes/no questions probing their comprehension of the story. The questions are formatted to assess the separate impacts of directness (stated vs. implied) and salience (main idea vs. details). GPT-4 performs slightly, but not statistically significantly, better than humans given the very high level of human performance. Both GPT-4 and humans exhibit a strong ability to make inferences about information that is not explicitly stated in a story, a critical test of understanding.
&lt;/p&gt;</description></item><item><title>NUMTEMP&#26159;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#65292;&#19987;&#27880;&#20110;&#39564;&#35777;&#22797;&#26434;&#30340;&#25968;&#23383;&#35770;&#28857;&#65292;&#37327;&#21270;&#20102;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#30495;&#23454;&#19990;&#30028;&#25968;&#23383;&#35770;&#28857;&#39564;&#35777;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.17169</link><description>&lt;p&gt;
NUMTEMP&#65306;&#19968;&#20010;&#29992;&#20110;&#39564;&#35777;&#24102;&#26377;&#32479;&#35745;&#21644;&#26102;&#38388;&#34920;&#36798;&#24335;&#30340;&#35770;&#28857;&#30340;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
NUMTEMP: A real-world benchmark to verify claims with statistical and temporal expressions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17169
&lt;/p&gt;
&lt;p&gt;
NUMTEMP&#26159;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#65292;&#19987;&#27880;&#20110;&#39564;&#35777;&#22797;&#26434;&#30340;&#25968;&#23383;&#35770;&#28857;&#65292;&#37327;&#21270;&#20102;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#30495;&#23454;&#19990;&#30028;&#25968;&#23383;&#35770;&#28857;&#39564;&#35777;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#20107;&#23454;&#26816;&#26597;&#22312;&#25968;&#23383;&#26102;&#20195;&#24212;&#23545;&#19981;&#26029;&#22686;&#38271;&#30340;&#38169;&#35823;&#20449;&#24687;&#26041;&#38754;&#24341;&#36215;&#20102;&#26497;&#22823;&#20852;&#36259;&#12290;&#29616;&#26377;&#31995;&#32479;&#20027;&#35201;&#19987;&#27880;&#20110;&#32500;&#22522;&#30334;&#31185;&#19978;&#30340;&#21512;&#25104;&#35770;&#28857;&#65292;&#24182;&#19988;&#22312;&#30495;&#23454;&#19990;&#30028;&#35770;&#28857;&#19978;&#20063;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;Numtemp&#65292;&#19968;&#20010;&#22810;&#26679;&#21270;&#12289;&#22810;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#65292;&#19987;&#38376;&#20851;&#27880;&#25968;&#23383;&#35770;&#28857;&#65292;&#21253;&#25324;&#26102;&#38388;&#12289;&#32479;&#35745;&#21644;&#22810;&#26679;&#21270;&#26041;&#38754;&#30340;&#32454;&#31890;&#24230;&#20803;&#25968;&#25454;&#65292;&#24182;&#19988;&#20855;&#26377;&#19981;&#27844;&#38706;&#30340;&#35777;&#25454;&#25910;&#38598;&#12290;&#36825;&#35299;&#20915;&#20102;&#39564;&#35777;&#30495;&#23454;&#19990;&#30028;&#25968;&#23383;&#35770;&#28857;&#30340;&#25361;&#25112;&#65292;&#36825;&#20123;&#35770;&#28857;&#22797;&#26434;&#65292;&#24448;&#24448;&#32570;&#20047;&#31934;&#30830;&#20449;&#24687;&#65292;&#36825;&#26159;&#29616;&#26377;&#20316;&#21697;&#20027;&#35201;&#20851;&#27880;&#21512;&#25104;&#35770;&#28857;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35780;&#20272;&#24182;&#37327;&#21270;&#20102;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#22312;&#39564;&#35777;&#25968;&#23383;&#35770;&#28857;&#20219;&#21153;&#20013;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#22522;&#20110;&#35770;&#28857;&#20998;&#35299;&#30340;&#26041;&#27861;&#12289;&#22522;&#20110;&#25968;&#23383;&#29702;&#35299;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#26368;&#20339;&#22522;&#32447;&#23454;&#29616;&#20102;58.32&#30340;&#23439;F1&#20998;&#25968;&#12290;&#36825;&#35777;&#26126;&#20102;Numtemp&#30340;&#20851;&#38190;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17169v1 Announce Type: cross  Abstract: Automated fact checking has gained immense interest to tackle the growing misinformation in the digital era. Existing systems primarily focus on synthetic claims on Wikipedia, and noteworthy progress has also been made on real-world claims. In this work, we release Numtemp, a diverse, multi-domain dataset focused exclusively on numerical claims, encompassing temporal, statistical and diverse aspects with fine-grained metadata and an evidence collection without leakage. This addresses the challenge of verifying real-world numerical claims, which are complex and often lack precise information, not addressed by existing works that mainly focus on synthetic claims. We evaluate and quantify the limitations of existing solutions for the task of verifying numerical claims. We also evaluate claim decomposition based methods, numerical understanding based models and our best baselines achieves a macro-F1 of 58.32. This demonstrates that Numtemp
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#37327;&#21270;&#22899;&#24615;&#23458;&#20307;&#21270;&#30340;&#26694;&#26550;&#65292;&#21457;&#29616;19&#19990;&#32426;&#21644;20&#19990;&#32426;&#23567;&#35828;&#20013;&#23384;&#22312;&#30007;&#24615;&#35270;&#35282;&#31995;&#32479;&#24615;&#23545;&#35937;&#21270;&#22899;&#24615;&#35282;&#33394;&#30340;&#35777;&#25454;</title><link>https://arxiv.org/abs/2403.17158</link><description>&lt;p&gt;
&#21453;&#26144;&#30007;&#24615;&#20957;&#35270;&#65306;&#37327;&#21270;19&#19990;&#32426;&#21644;20&#19990;&#32426;&#23567;&#35828;&#20013;&#30340;&#22899;&#24615;&#23458;&#20307;&#21270;
&lt;/p&gt;
&lt;p&gt;
Reflecting the Male Gaze: Quantifying Female Objectification in 19th and 20th Century Novels
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17158
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#37327;&#21270;&#22899;&#24615;&#23458;&#20307;&#21270;&#30340;&#26694;&#26550;&#65292;&#21457;&#29616;19&#19990;&#32426;&#21644;20&#19990;&#32426;&#23567;&#35828;&#20013;&#23384;&#22312;&#30007;&#24615;&#35270;&#35282;&#31995;&#32479;&#24615;&#23545;&#35937;&#21270;&#22899;&#24615;&#35282;&#33394;&#30340;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;Mulvey&#65288;1975&#24180;&#65289;&#20851;&#20110;&#25991;&#23398;&#21644;&#23186;&#20307;&#30740;&#31350;&#20013;&#30007;&#24615;&#20957;&#35270;&#30340;&#27010;&#24565;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#26512;&#24615;&#21035;&#20559;&#35265;&#30340;&#26694;&#26550;&#65292;&#21363;&#22899;&#24615;&#23458;&#20307;&#21270;&#30340;&#31243;&#24230;&#65306;&#25991;&#26412;&#20013;&#22899;&#24615;&#20010;&#20307;&#34987;&#25551;&#32472;&#20026;&#35270;&#35273;&#24841;&#24742;&#23545;&#35937;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#27839;&#30528;&#20004;&#20010;&#36724;&#24230;&#34913;&#37327;&#22899;&#24615;&#23458;&#20307;&#21270;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35745;&#31639;&#19968;&#20010;&#20195;&#29702;&#20559;&#35265;&#20998;&#25968;&#65292;&#25351;&#31034;&#30007;&#24615;&#23454;&#20307;&#26159;&#21542;&#27604;&#22899;&#24615;&#23454;&#20307;&#26356;&#26377;&#21487;&#33021;&#22312;&#25991;&#26412;&#20013;&#20986;&#29616;&#20026;&#35821;&#27861;&#20195;&#29702;&#12290;&#25509;&#19979;&#26469;&#65292;&#36890;&#36807;&#20998;&#26512;&#25991;&#26412;&#24341;&#21457;&#30340;&#35789;&#23884;&#20837;&#31354;&#38388;&#65288;Caliskan&#31561;&#65292;2017&#65289;&#65292;&#25105;&#20204;&#35745;&#31639;&#19968;&#20010;&#22806;&#35980;&#20559;&#35265;&#20998;&#25968;&#65292;&#25351;&#31034;&#22899;&#24615;&#23454;&#20307;&#26159;&#21542;&#19982;&#22806;&#35980;&#30456;&#20851;&#35789;&#27719;&#26356;&#32039;&#23494;&#22320;&#32852;&#31995;&#12290;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;19&#19990;&#32426;&#21644;20&#19990;&#32426;&#30340;&#23567;&#35828;&#20013;&#25581;&#31034;&#20102;&#25991;&#23398;&#20013;&#30340;&#22899;&#24615;&#23458;&#20307;&#21270;&#35777;&#25454;&#65306;&#25105;&#20204;&#21457;&#29616;&#20174;&#30007;&#24615;&#35270;&#35282;&#20889;&#20316;&#30340;&#23567;&#35828;&#31995;&#32479;&#24615;&#22320;&#23558;&#22899;&#24615;&#35282;&#33394;&#23458;&#20307;&#21270;&#65292;&#32780;&#23567;&#35828;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17158v1 Announce Type: new  Abstract: Inspired by the concept of the male gaze (Mulvey, 1975) in literature and media studies, this paper proposes a framework for analyzing gender bias in terms of female objectification: the extent to which a text portrays female individuals as objects of visual pleasure. Our framework measures female objectification along two axes. First, we compute an agency bias score that indicates whether male entities are more likely to appear in the text as grammatical agents than female entities. Next, by analyzing the word embedding space induced by a text (Caliskan et al., 2017), we compute an appearance bias score that indicates whether female entities are more closely associated with appearance-related words than male entities. Applying our framework to 19th and 20th century novels reveals evidence of female objectification in literature: we find that novels written from a male perspective systematically objectify female characters, while novels 
&lt;/p&gt;</description></item><item><title>TABDet&#26159;&#19968;&#31181;&#20219;&#21153;&#26080;&#20851;&#30340;&#21518;&#38376;&#26816;&#27979;&#22120;&#65292;&#36890;&#36807;&#21033;&#29992;&#26368;&#32456;&#23618;logits&#21644;&#39640;&#25928;&#30340;&#27744;&#21270;&#25216;&#26415;&#65292;&#22312;&#22810;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#32479;&#19968;&#30340;logit&#34920;&#31034;&#65292;&#23637;&#31034;&#20102;&#23545;&#20256;&#32479;&#20219;&#21153;&#29305;&#23450;&#26041;&#27861;&#30340;&#20248;&#36234;&#26816;&#27979;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.17155</link><description>&lt;p&gt;
&#20219;&#21153;&#26080;&#20851;&#30340;&#25554;&#20837;&#24335;&#21518;&#38376;&#25915;&#20987;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Task-Agnostic Detector for Insertion-Based Backdoor Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17155
&lt;/p&gt;
&lt;p&gt;
TABDet&#26159;&#19968;&#31181;&#20219;&#21153;&#26080;&#20851;&#30340;&#21518;&#38376;&#26816;&#27979;&#22120;&#65292;&#36890;&#36807;&#21033;&#29992;&#26368;&#32456;&#23618;logits&#21644;&#39640;&#25928;&#30340;&#27744;&#21270;&#25216;&#26415;&#65292;&#22312;&#22810;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#32479;&#19968;&#30340;logit&#34920;&#31034;&#65292;&#23637;&#31034;&#20102;&#23545;&#20256;&#32479;&#20219;&#21153;&#29305;&#23450;&#26041;&#27861;&#30340;&#20248;&#36234;&#26816;&#27979;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21518;&#38376;&#25915;&#20987;&#26500;&#25104;&#20102;&#37325;&#22823;&#23433;&#20840;&#23041;&#32961;&#12290;&#24403;&#21069;&#30340;&#26816;&#27979;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#20013;&#38388;&#29305;&#24449;&#34920;&#31034;&#25110;&#37325;&#24314;&#28508;&#22312;&#35302;&#21457;&#22120;&#65292;&#23545;&#21477;&#23376;&#20998;&#31867;&#20043;&#22806;&#30340;&#20219;&#21153;&#25928;&#26524;&#19981;&#20339;&#65292;&#22914;&#22312;&#38382;&#31572;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#31561;&#20219;&#21153;&#19978;&#36935;&#21040;&#22256;&#38590;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;TABDet&#65288;&#20219;&#21153;&#26080;&#20851;&#21518;&#38376;&#26816;&#27979;&#22120;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#21518;&#38376;&#26816;&#27979;&#30340;&#24320;&#21019;&#24615;&#30340;&#20219;&#21153;&#26080;&#20851;&#26041;&#27861;&#12290;TABDet&#21033;&#29992;&#26368;&#32456;&#23618;Logits&#32467;&#21512;&#39640;&#25928;&#30340;&#27744;&#21270;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22312;&#19977;&#20010;&#27969;&#34892;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#32479;&#19968;&#30340;Logit&#34920;&#31034;&#12290;TABDet&#21487;&#20197;&#20174;&#22810;&#26679;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#20013;&#20849;&#21516;&#23398;&#20064;&#65292;&#23637;&#31034;&#20102;&#23545;&#20256;&#32479;&#20219;&#21153;&#29305;&#23450;&#26041;&#27861;&#30340;&#20248;&#36234;&#26816;&#27979;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17155v1 Announce Type: new  Abstract: Textual backdoor attacks pose significant security threats. Current detection approaches, typically relying on intermediate feature representation or reconstructing potential triggers, are task-specific and less effective beyond sentence classification, struggling with tasks like question answering and named entity recognition. We introduce TABDet (Task-Agnostic Backdoor Detector), a pioneering task-agnostic method for backdoor detection. TABDet leverages final layer logits combined with an efficient pooling technique, enabling unified logit representation across three prominent NLP tasks. TABDet can jointly learn from diverse task-specific models, demonstrating superior detection efficacy over traditional task-specific methods.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21463;&#28508;&#22312;&#23545;&#35805;&#32467;&#26524;&#38480;&#21046;&#30340;&#23545;&#35805;&#65292;&#20197;&#24212;&#23545;&#22312;&#32447;&#20167;&#24680;&#35328;&#35770;&#65292;&#36890;&#36807;&#26500;&#24314;&#23545;&#35805;&#32467;&#26524;&#20998;&#31867;&#22120;&#21644;&#25552;&#20986;&#25972;&#21512;&#26041;&#27861;&#65292;&#20026;&#22312;&#32447;&#29615;&#22659;&#20013;&#29983;&#25104;&#23545;&#25239;&#24615;&#23545;&#35805;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;</title><link>https://arxiv.org/abs/2403.17146</link><description>&lt;p&gt;
&#29992;&#20110;&#25269;&#21046;&#20167;&#24680;&#35328;&#35770;&#30340;&#32467;&#26524;&#21463;&#38480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Outcome-Constrained Large Language Models for Countering Hate Speech
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17146
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21463;&#28508;&#22312;&#23545;&#35805;&#32467;&#26524;&#38480;&#21046;&#30340;&#23545;&#35805;&#65292;&#20197;&#24212;&#23545;&#22312;&#32447;&#20167;&#24680;&#35328;&#35770;&#65292;&#36890;&#36807;&#26500;&#24314;&#23545;&#35805;&#32467;&#26524;&#20998;&#31867;&#22120;&#21644;&#25552;&#20986;&#25972;&#21512;&#26041;&#27861;&#65292;&#20026;&#22312;&#32447;&#29615;&#22659;&#20013;&#29983;&#25104;&#23545;&#25239;&#24615;&#23545;&#35805;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25361;&#25112;&#25110;&#22238;&#24212;&#20167;&#24680;&#35328;&#35770;&#30340;&#23545;&#35805;&#34987;&#35270;&#20026;&#32531;&#35299;&#20167;&#24680;&#35328;&#35770;&#30340;&#36127;&#38754;&#24433;&#21709;&#24182;&#20419;&#36827;&#22312;&#32447;&#20132;&#27969;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#30740;&#31350;&#24050;&#33268;&#21147;&#20110;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#23545;&#35805;&#20197;&#21327;&#21161;&#25171;&#20987;&#22312;&#32447;&#20167;&#24680;&#35328;&#35770;&#12290;&#29616;&#26377;&#30740;&#31350;&#20391;&#37325;&#20110;&#29983;&#25104;&#20855;&#26377;&#29305;&#23450;&#35821;&#35328;&#23646;&#24615;&#65288;&#22914;&#31036;&#35980;&#12289;&#20449;&#24687;&#20016;&#23500;&#21644;&#24847;&#22270;&#39537;&#21160;&#65289;&#30340;&#23545;&#35805;&#12290;&#28982;&#32780;&#65292;&#23545;&#35805;&#21487;&#33021;&#23545;&#22312;&#32447;&#29615;&#22659;&#20135;&#29983;&#20160;&#20040;&#24433;&#21709;&#20173;&#19981;&#26126;&#30830;&#12290;&#25105;&#20204;&#39318;&#20808;&#25506;&#35752;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#21463;&#28508;&#22312;&#23545;&#35805;&#32467;&#26524;&#38480;&#21046;&#30340;&#23545;&#35805;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#20004;&#20010;&#23545;&#35805;&#32467;&#26524;&#20998;&#31867;&#22120;&#65292;&#29992;Reddit&#25968;&#25454;&#39044;&#27979;&#24212;&#23545;&#20167;&#24680;&#35328;&#35770;&#21518;&#30340;&#19981;&#25991;&#26126;&#31243;&#24230;&#21644;&#20167;&#24680;&#32773;&#37325;&#26032;&#20851;&#27880;&#34892;&#20026;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#22235;&#31181;&#26041;&#27861;&#26469;&#25972;&#21512;&#25152;&#38656;&#30340;&#32467;&#26524;&#65292;&#21363;&#20302;&#31036;&#35980;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17146v1 Announce Type: new  Abstract: Counterspeech that challenges or responds to hate speech has been seen as an alternative to mitigate the negative impact of hate speech and foster productive online communications. Research endeavors have been directed to using language models for the automatic generation of counterspeech to assist efforts in combating online hate. Existing research focuses on the generation of counterspeech with certain linguistic attributes, such as being polite, informative, and intent-driven. However, it remains unclear what impact the counterspeech might have in an online environment. We first explore methods that utilize large language models (LLM) to generate counterspeech constrained by potential conversation outcomes. We build two conversation outcome classifiers that predict the incivility level and the hater reentry behavior following replies to hate with Reddit data, then propose four methods to incorporate the desired outcomes, i.e., low con
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24212;&#29992;&#24341;&#23548;&#36828;&#31243;&#30417;&#30563;&#26041;&#27861;&#65292;&#20026;&#24503;&#35821;&#21019;&#24314;&#20102;&#26368;&#22823;&#30340;&#20256;&#35760;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#21457;&#24067;&#20102;&#25163;&#21160;&#26631;&#27880;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2403.17143</link><description>&lt;p&gt;
&#24341;&#23548;&#36828;&#31243;&#30417;&#30563;&#29992;&#20110;&#22810;&#35821;&#35328;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#65306;&#36866;&#24212;&#26032;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
Guided Distant Supervision for Multilingual Relation Extraction Data: Adapting to a New Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24212;&#29992;&#24341;&#23548;&#36828;&#31243;&#30417;&#30563;&#26041;&#27861;&#65292;&#20026;&#24503;&#35821;&#21019;&#24314;&#20102;&#26368;&#22823;&#30340;&#20256;&#35760;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#21457;&#24067;&#20102;&#25163;&#21160;&#26631;&#27880;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25688;&#35201;&#65306;&#20851;&#31995;&#25277;&#21462;&#23545;&#20110;&#22312;&#25968;&#23383;&#20154;&#25991;&#23398;&#21644;&#30456;&#20851;&#23398;&#31185;&#32972;&#26223;&#19979;&#25552;&#21462;&#21644;&#29702;&#35299;&#20256;&#35760;&#20449;&#24687;&#33267;&#20851;&#37325;&#35201;&#12290;&#31038;&#21306;&#23545;&#26500;&#24314;&#33021;&#22815;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#21462;&#20851;&#31995;&#30340;&#25968;&#25454;&#38598;&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#26631;&#27880;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#21487;&#33021;&#26082;&#26114;&#36149;&#21448;&#32791;&#26102;&#65292;&#32780;&#19988;&#20165;&#38480;&#20110;&#33521;&#35821;&#12290;&#26412;&#25991;&#24212;&#29992;&#20102;&#24341;&#23548;&#24335;&#36828;&#31243;&#30417;&#30563;&#26041;&#27861;&#65292;&#20026;&#24503;&#35821;&#21019;&#24314;&#20102;&#19968;&#20010;&#22823;&#22411;&#20256;&#35760;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#36229;&#36807;80,000&#20010;&#23454;&#20363;&#65292;&#28085;&#30422;&#20102;&#20061;&#31181;&#20851;&#31995;&#31867;&#22411;&#65292;&#26159;&#26368;&#22823;&#30340;&#24503;&#35821;&#20256;&#35760;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36824;&#21019;&#24314;&#20102;&#19968;&#20010;&#25163;&#21160;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;2000&#20010;&#23454;&#20363;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#65292;&#24182;&#19982;&#21033;&#29992;&#24341;&#23548;&#24335;&#36828;&#31243;&#30417;&#30563;&#26041;&#27861;&#32534;&#21046;&#30340;&#25968;&#25454;&#38598;&#19968;&#36215;&#21457;&#24067;&#12290;&#25105;&#20204;&#22312;&#33258;&#21160;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17143v1 Announce Type: new  Abstract: Relation extraction is essential for extracting and understanding biographical information in the context of digital humanities and related subjects. There is a growing interest in the community to build datasets capable of training machine learning models to extract relationships. However, annotating such datasets can be expensive and time-consuming, in addition to being limited to English. This paper applies guided distant supervision to create a large biographical relationship extraction dataset for German. Our dataset, composed of more than 80,000 instances for nine relationship types, is the largest biographical German relationship extraction dataset. We also create a manually annotated dataset with 2000 instances to evaluate the models and release it together with the dataset compiled using guided distant supervision. We train several state-of-the-art machine learning models on the automatically created dataset and release them as 
&lt;/p&gt;</description></item><item><title>MetaAligner&#26159;&#31532;&#19968;&#20010;&#19982;&#31574;&#30053;&#26080;&#20851;&#19988;&#36890;&#29992;&#30340;&#22810;&#30446;&#26631;&#20559;&#22909;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21442;&#25968;&#26356;&#26032;&#19982;&#31574;&#30053;&#27169;&#22411;&#35299;&#32806;&#23454;&#29616;&#21363;&#25554;&#21363;&#29992;&#30340;&#23545;&#40784;&#65292;&#24182;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#23454;&#29616;&#26410;&#35265;&#30446;&#26631;&#30340;&#38646;&#20919;&#21551;&#21160;&#20559;&#22909;&#23545;&#40784;</title><link>https://arxiv.org/abs/2403.17141</link><description>&lt;p&gt;
MetaAligner&#65306;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#36890;&#29992;&#22810;&#30446;&#26631;&#23545;&#40784;&#30340;&#26465;&#20214;&#20174;&#24369;&#21040;&#24378;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
MetaAligner: Conditional Weak-to-Strong Correction for Generalizable Multi-Objective Alignment of Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17141
&lt;/p&gt;
&lt;p&gt;
MetaAligner&#26159;&#31532;&#19968;&#20010;&#19982;&#31574;&#30053;&#26080;&#20851;&#19988;&#36890;&#29992;&#30340;&#22810;&#30446;&#26631;&#20559;&#22909;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21442;&#25968;&#26356;&#26032;&#19982;&#31574;&#30053;&#27169;&#22411;&#35299;&#32806;&#23454;&#29616;&#21363;&#25554;&#21363;&#29992;&#30340;&#23545;&#40784;&#65292;&#24182;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#23454;&#29616;&#26410;&#35265;&#30446;&#26631;&#30340;&#38646;&#20919;&#21551;&#21160;&#20559;&#22909;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#26088;&#22312;&#36890;&#36807;&#22810;&#30446;&#26631;&#20559;&#22909;&#23545;&#40784;&#26469;&#35299;&#20915;&#24322;&#36136;&#20154;&#31867;&#26399;&#26395;&#21644;&#20215;&#20540;&#35266;&#65292;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#21463;&#21040;&#31574;&#30053;&#27169;&#22411;&#30340;&#21442;&#25968;&#38480;&#21046;&#65292;&#23548;&#33268;&#20004;&#20010;&#20851;&#38190;&#23616;&#38480;&#24615;&#65306;&#65288;1&#65289;&#23427;&#20204;&#30340;&#23545;&#40784;&#31639;&#27861;&#23545;&#20110;&#27599;&#20010;&#26032;&#30446;&#26631;&#27169;&#22411;&#30340;&#37325;&#22797;&#25104;&#26412;&#24456;&#39640;&#65307;&#65288;2&#65289;&#30001;&#20110;&#20854;&#38745;&#24577;&#23545;&#40784;&#30446;&#26631;&#65292;&#23427;&#20204;&#26080;&#27861;&#25193;&#23637;&#21040;&#26410;&#35265;&#30446;&#26631;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Meta-Objective Aligner&#65288;MetaAligner&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#25191;&#34892;&#26465;&#20214;&#20174;&#24369;&#21040;&#24378;&#26657;&#27491;&#20197;&#36924;&#36817;&#24378;&#21709;&#24212;&#30340;&#27169;&#22411;&#12290;MetaAligner&#26159;&#31532;&#19968;&#20010;&#19982;&#31574;&#30053;&#26080;&#20851;&#19988;&#36890;&#29992;&#30340;&#22810;&#30446;&#26631;&#20559;&#22909;&#23545;&#40784;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23558;&#21442;&#25968;&#26356;&#26032;&#19982;&#31574;&#30053;&#27169;&#22411;&#35299;&#32806;&#23454;&#29616;&#21363;&#25554;&#21363;&#29992;&#30340;&#23545;&#40784;&#65292;&#24182;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#23454;&#29616;&#26410;&#35265;&#30446;&#26631;&#30340;&#38646;&#20919;&#21551;&#21160;&#20559;&#22909;&#23545;&#40784;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MetaAligner&#21462;&#24471;&#20102;&#26174;&#33879;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17141v1 Announce Type: cross  Abstract: Recent advancements in large language models (LLMs) aim to tackle heterogeneous human expectations and values via multi-objective preference alignment. However, existing methods are parameter-adherent to the policy model, leading to two key limitations: (1) the high-cost repetition of their alignment algorithms for each new target model; (2) they cannot expand to unseen objectives due to their static alignment objectives. In this work, we propose Meta-Objective Aligner (MetaAligner), a model that performs conditional weak-to-strong correction for weak responses to approach strong responses. MetaAligner is the first policy-agnostic and generalizable method for multi-objective preference alignment, which enables plug-and-play alignment by decoupling parameter updates from the policy models and facilitates zero-shot preference alignment for unseen objectives via in-context learning. Experimental results show that MetaAligner achieves sign
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#30284;&#30151;&#20020;&#24202;&#35797;&#39564;&#36164;&#26684;&#20998;&#31867;&#22120;&#22312;&#19981;&#21516;&#30142;&#30149;&#38388;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#21457;&#29616;&#22312;&#24191;&#27867;&#30284;&#30151;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#20197;&#22788;&#29702;&#38750;&#30284;&#30151;&#35797;&#39564;&#30340;&#26631;&#20934;&#65292;&#20294;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#12290;</title><link>https://arxiv.org/abs/2403.17135</link><description>&lt;p&gt;
&#25506;&#32034;&#30284;&#30151;&#20020;&#24202;&#35797;&#39564;&#36164;&#26684;&#20998;&#31867;&#22120;&#22312;&#30142;&#30149;&#38388;&#30340;&#27867;&#21270;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Exploring the Generalization of Cancer Clinical Trial Eligibility Classifiers Across Diseases
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17135
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#30284;&#30151;&#20020;&#24202;&#35797;&#39564;&#36164;&#26684;&#20998;&#31867;&#22120;&#22312;&#19981;&#21516;&#30142;&#30149;&#38388;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#21457;&#29616;&#22312;&#24191;&#27867;&#30284;&#30151;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#20197;&#22788;&#29702;&#38750;&#30284;&#30151;&#35797;&#39564;&#30340;&#26631;&#20934;&#65292;&#20294;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#35797;&#39564;&#23545;&#20110;&#21307;&#23398;&#30740;&#31350;&#33267;&#20851;&#37325;&#35201;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21487;&#22686;&#24378;&#20854;&#25104;&#21151;&#65292;&#22312;&#25307;&#21215;&#26041;&#38754;&#26377;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#36164;&#26684;&#20998;&#31867;&#22312;&#24191;&#27867;&#20020;&#24202;&#35797;&#39564;&#33539;&#22260;&#20869;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#20174;&#38454;&#27573;3&#30284;&#30151;&#35797;&#39564;&#24320;&#22987;&#65292;&#26631;&#35760;&#26377;&#19971;&#31181;&#36164;&#26684;&#25490;&#38500;&#26465;&#20214;&#65292;&#28982;&#21518;&#30830;&#23450;&#27169;&#22411;&#22312;&#38750;&#30284;&#30151;&#21644;&#38750;&#38454;&#27573;3&#35797;&#39564;&#20013;&#30340;&#27867;&#21270;&#25928;&#26524;&#12290;&#20026;&#35780;&#20272;&#27492;&#38382;&#39064;&#65292;&#25105;&#20204;&#25972;&#29702;&#20102;&#20116;&#31181;&#35797;&#39564;&#31867;&#22411;&#30340;&#36164;&#26684;&#26631;&#20934;&#25968;&#25454;&#65306;&#65288;1&#65289;&#20854;&#20182;&#38454;&#27573;3&#30284;&#30151;&#35797;&#39564;&#65292;&#65288;2&#65289;&#30284;&#30151;&#38454;&#27573;1&#21644;2&#35797;&#39564;&#65292;&#65288;3&#65289;&#24515;&#33039;&#30149;&#35797;&#39564;&#65292;&#65288;4&#65289;2&#22411;&#31958;&#23615;&#30149;&#35797;&#39564;&#21644;&#65288;5&#65289;&#20219;&#20309;&#30142;&#30149;&#30340;&#35266;&#23519;&#24615;&#35797;&#39564;&#65292;&#36328;&#19971;&#31181;&#25490;&#38500;&#31867;&#22411;&#20849;&#28085;&#30422;&#20102;2,490&#20010;&#24050;&#26631;&#27880;&#30340;&#36164;&#26684;&#26631;&#20934;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#24191;&#27867;&#30284;&#30151;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;&#38750;&#30284;&#30151;&#35797;&#39564;&#20013;&#24120;&#35265;&#30340;&#26631;&#20934;&#65292;&#22914;&#33258;&#36523;&#20813;&#30123;&#30142;&#30149;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#22788;&#29702;&#35832;&#22914;&#32570;&#20047;&#26631;&#20934;&#31561;&#26631;&#20934;&#26102;&#20250;&#36935;&#21040;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17135v1 Announce Type: new  Abstract: Clinical trials are pivotal in medical research, and NLP can enhance their success, with application in recruitment. This study aims to evaluate the generalizability of eligibility classification across a broad spectrum of clinical trials. Starting with phase 3 cancer trials, annotated with seven eligibility exclusions, then to determine how well models can generalize to non-cancer and non-phase 3 trials. To assess this, we have compiled eligibility criteria data for five types of trials: (1) additional phase 3 cancer trials, (2) phase 1 and 2 cancer trials, (3) heart disease trials, (4) type 2 diabetes trials, and (5) observational trials for any disease, comprising 2,490 annotated eligibility criteria across seven exclusion types. Our results show that models trained on the extensive cancer dataset can effectively handle criteria commonly found in non-cancer trials, such as autoimmune diseases. However, they struggle with criteria disp
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25191;&#34892;&#20219;&#21153;&#26102;&#20381;&#36182;&#32972;&#26223;&#30693;&#35782;&#65288;&#20808;&#39564;&#30693;&#35782;&#65289;&#65292;&#20294;&#26080;&#27861;&#23436;&#20840;&#25972;&#21512;&#19982;&#20219;&#21153;&#20808;&#39564;&#30693;&#35782;&#30456;&#30683;&#30462;&#30340;&#20449;&#24687;&#65292;&#24433;&#21709;&#20102;&#24773;&#32490;&#35782;&#21035;&#31561;&#20027;&#35266;&#20219;&#21153;&#30340;&#34920;&#29616;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2403.17125</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20808;&#39564;&#30693;&#35782;&#30340;&#24378;&#22823;&#20316;&#29992;&#21450;&#20854;&#23545;&#24773;&#32490;&#35782;&#21035;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Strong Pull of Prior Knowledge in Large Language Models and Its Impact on Emotion Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17125
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25191;&#34892;&#20219;&#21153;&#26102;&#20381;&#36182;&#32972;&#26223;&#30693;&#35782;&#65288;&#20808;&#39564;&#30693;&#35782;&#65289;&#65292;&#20294;&#26080;&#27861;&#23436;&#20840;&#25972;&#21512;&#19982;&#20219;&#21153;&#20808;&#39564;&#30693;&#35782;&#30456;&#30683;&#30462;&#30340;&#20449;&#24687;&#65292;&#24433;&#21709;&#20102;&#24773;&#32490;&#35782;&#21035;&#31561;&#20027;&#35266;&#20219;&#21153;&#30340;&#34920;&#29616;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
In-context Learning (ICL)&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#33539;&#24335;&#28014;&#29616;&#20986;&#26469;&#65292;&#21487;&#20197;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19978;&#25191;&#34892;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#26356;&#26032;&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#24494;&#35843;&#30456;&#21453;&#12290; ICL&#30340;&#25215;&#35834;&#26159;&#65292;LLM&#21487;&#20197;&#36866;&#24212;&#25191;&#34892;&#24403;&#21069;&#20219;&#21153;&#65292;&#24182;&#20197;&#31454;&#20105;&#21147;&#25110;&#26368;&#26032;&#27700;&#24179;&#30340;&#19968;&#23567;&#37096;&#20998;&#25104;&#26412;&#12290; LLM&#20197;&#36825;&#31181;&#23569;&#26679;&#26412;&#30340;&#26041;&#24335;&#25191;&#34892;&#20219;&#21153;&#30340;&#33021;&#21147;&#20381;&#36182;&#20110;&#23427;&#20204;&#23545;&#20219;&#21153;&#30340;&#32972;&#26223;&#30693;&#35782;&#65288;&#25110;&#20219;&#21153;&#20808;&#39564;&#30693;&#35782;&#65289;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#19982;&#20256;&#32479;&#23398;&#20064;&#19981;&#21516;&#65292;LLM&#26080;&#27861;&#23436;&#20840;&#25972;&#21512;&#19982;&#20219;&#21153;&#20808;&#39564;&#30693;&#35782;&#30456;&#30683;&#30462;&#30340;&#28436;&#31034;&#20449;&#24687;&#12290; &#36825;&#21487;&#33021;&#23548;&#33268;&#34920;&#29616;&#36798;&#21040;&#27425;&#20248;&#27700;&#24179;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20027;&#35266;&#20219;&#21153;&#65288;&#22914;&#24773;&#32490;&#35782;&#21035;&#65289;&#65292;&#20854;&#20013;&#25991;&#26412;&#21040;&#24773;&#32490;&#30340;&#26144;&#23556;&#21487;&#33021;&#22240;&#20154;&#31867;&#27880;&#37322;&#30340;&#21464;&#24322;&#24615;&#32780;&#22823;&#19981;&#30456;&#21516;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#23454;&#39564;&#24182;&#25552;&#20986;&#20102;&#27979;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17125v1 Announce Type: cross  Abstract: In-context Learning (ICL) has emerged as a powerful paradigm for performing natural language tasks with Large Language Models (LLM) without updating the models' parameters, in contrast to the traditional gradient-based finetuning. The promise of ICL is that the LLM can adapt to perform the present task at a competitive or state-of-the-art level at a fraction of the cost. The ability of LLMs to perform tasks in this few-shot manner relies on their background knowledge of the task (or task priors). However, recent work has found that, unlike traditional learning, LLMs are unable to fully integrate information from demonstrations that contrast task priors. This can lead to performance saturation at suboptimal levels, especially for subjective tasks such as emotion recognition, where the mapping from text to emotions can differ widely due to variability in human annotations. In this work, we design experiments and propose measurements to e
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#20351;&#29992;LLMs&#26469;&#25351;&#23548;&#22810;&#27493;&#28436;&#31034;&#20013;&#38544;&#21547;&#30340;&#20219;&#21153;&#32467;&#26500;&#21644;&#32422;&#26463;&#30340;&#25628;&#32034;&#65292;&#20197;&#21450;&#36890;&#36807;&#21453;&#20107;&#23454;&#24178;&#25200;&#33719;&#24471;&#26356;&#24191;&#27867;&#30340;&#28436;&#31034;&#29366;&#24577;&#31354;&#38388;&#35206;&#30422;&#12290;</title><link>https://arxiv.org/abs/2403.17124</link><description>&lt;p&gt;
&#23558;&#35821;&#35328;&#35745;&#21010;&#22522;&#20110;&#28436;&#31034;&#36890;&#36807;&#21453;&#20107;&#23454;&#24178;&#25200;&#36827;&#34892;&#33853;&#23454;
&lt;/p&gt;
&lt;p&gt;
Grounding Language Plans in Demonstrations Through Counterfactual Perturbations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17124
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#20351;&#29992;LLMs&#26469;&#25351;&#23548;&#22810;&#27493;&#28436;&#31034;&#20013;&#38544;&#21547;&#30340;&#20219;&#21153;&#32467;&#26500;&#21644;&#32422;&#26463;&#30340;&#25628;&#32034;&#65292;&#20197;&#21450;&#36890;&#36807;&#21453;&#20107;&#23454;&#24178;&#25200;&#33719;&#24471;&#26356;&#24191;&#27867;&#30340;&#28436;&#31034;&#29366;&#24577;&#31354;&#38388;&#35206;&#30422;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24120;&#35782;&#25512;&#29702;&#22522;&#20110;&#29289;&#29702;&#39046;&#22495;&#33853;&#23454;&#22312;&#20307;&#29616;&#26234;&#33021;&#30340;&#20154;&#24037;&#26234;&#33021;&#20013;&#20173;&#28982;&#26159;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#20294;&#23578;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#30456;&#36739;&#20110;&#20808;&#21069;&#30340;&#24037;&#20316;&#19987;&#27880;&#20110;&#30452;&#25509;&#21033;&#29992;LLMs&#22312;&#31526;&#21495;&#31354;&#38388;&#20869;&#35268;&#21010;&#65292;&#36825;&#39033;&#24037;&#20316;&#20351;&#29992;LLMs&#25351;&#23548;&#20219;&#21153;&#32467;&#26500;&#30340;&#25628;&#32034;&#65292;&#38544;&#21547;&#22312;&#22810;&#27493;&#28436;&#31034;&#20013;&#30340;&#32422;&#26463;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#25805;&#32437;&#35268;&#21010;&#25991;&#29486;&#20013;&#30340;&#27169;&#24335;&#26063;&#30340;&#27010;&#24565;&#65292;&#23427;&#25353;&#29031;&#29305;&#23450;&#36816;&#21160;&#32422;&#26463;&#23558;&#26426;&#22120;&#20154;&#37197;&#32622;&#20998;&#32452;&#65292;&#20316;&#20026;LLM&#39640;&#32423;&#35821;&#35328;&#34920;&#31034;&#21644;&#26426;&#22120;&#20154;&#20302;&#32423;&#29289;&#29702;&#36712;&#36857;&#20043;&#38388;&#30340;&#25277;&#35937;&#23618;&#12290;&#36890;&#36807;&#29992;&#21512;&#25104;&#24178;&#25200;&#37325;&#26032;&#25773;&#25918;&#23569;&#37327;&#20154;&#31867;&#28436;&#31034;&#65292;&#25105;&#20204;&#21487;&#20197;&#35206;&#30422;&#28436;&#31034;&#30340;&#29366;&#24577;&#31354;&#38388;&#65292;&#24182;&#39069;&#22806;&#29983;&#25104;&#25104;&#21151;&#25191;&#34892;&#20197;&#21450;&#26410;&#23436;&#25104;&#20219;&#21153;&#30340;&#21453;&#20107;&#23454;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#22522;&#20110;&#35299;&#37322;&#30340;&#23398;&#20064;&#26694;&#26550;&#35757;&#32451;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#21487;&#24494;&#20998;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17124v1 Announce Type: cross  Abstract: Grounding the common-sense reasoning of Large Language Models in physical domains remains a pivotal yet unsolved problem for embodied AI. Whereas prior works have focused on leveraging LLMs directly for planning in symbolic spaces, this work uses LLMs to guide the search of task structures and constraints implicit in multi-step demonstrations. Specifically, we borrow from manipulation planning literature the concept of mode families, which group robot configurations by specific motion constraints, to serve as an abstraction layer between the high-level language representations of an LLM and the low-level physical trajectories of a robot. By replaying a few human demonstrations with synthetic perturbations, we generate coverage over the demonstrations' state space with additional successful executions as well as counterfactuals that fail the task. Our explanation-based learning framework trains an end-to-end differentiable neural networ
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23616;&#37096;&#21487;&#24402;&#23646;&#30340;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#8220;&#20808;&#22686;&#21152;&#23646;&#24615;&#65292;&#28982;&#21518;&#29983;&#25104;&#8221;&#30340;&#26041;&#24335;&#23558;&#29983;&#25104;&#36807;&#31243;&#20998;&#20026;&#20869;&#23481;&#36873;&#25321;&#12289;&#21477;&#23376;&#35268;&#21010;&#21644;&#24207;&#21015;&#21477;&#23376;&#29983;&#25104;&#19977;&#20010;&#27493;&#39588;&#65292;&#20197;&#31616;&#21270;&#24341;&#29992;&#39564;&#35777;&#24037;&#20316;&#12290;</title><link>https://arxiv.org/abs/2403.17104</link><description>&lt;p&gt;
&#39318;&#20808;&#22686;&#21152;&#23646;&#24615;&#65292;&#28982;&#21518;&#29983;&#25104;&#65306;&#23616;&#37096;&#21487;&#24402;&#23646;&#30340;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Attribute First, then Generate: Locally-attributable Grounded Text Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17104
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23616;&#37096;&#21487;&#24402;&#23646;&#30340;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#8220;&#20808;&#22686;&#21152;&#23646;&#24615;&#65292;&#28982;&#21518;&#29983;&#25104;&#8221;&#30340;&#26041;&#24335;&#23558;&#29983;&#25104;&#36807;&#31243;&#20998;&#20026;&#20869;&#23481;&#36873;&#25321;&#12289;&#21477;&#23376;&#35268;&#21010;&#21644;&#24207;&#21015;&#21477;&#23376;&#29983;&#25104;&#19977;&#20010;&#27493;&#39588;&#65292;&#20197;&#31616;&#21270;&#24341;&#29992;&#39564;&#35777;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#24187;&#35273;&#30340;&#21162;&#21147;&#20027;&#35201;&#38598;&#20013;&#22312;&#23646;&#24615;&#25991;&#26412;&#29983;&#25104;&#19978;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#24341;&#29992;&#25903;&#25345;&#28304;&#22312;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#21152;&#20837;&#25903;&#25345;&#25991;&#26412;&#20197;&#36827;&#34892;&#20107;&#21518;&#20107;&#23454;&#26680;&#26597;&#21644;&#26356;&#27491;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24341;&#29992;&#36890;&#24120;&#25351;&#21521;&#25972;&#20010;&#25991;&#26723;&#25110;&#27573;&#33853;&#65292;&#32473;&#29992;&#25143;&#24102;&#26469;&#20102;&#32321;&#37325;&#30340;&#39564;&#35777;&#24037;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#23616;&#37096;&#21487;&#24402;&#23646;&#30340;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#65292;&#37325;&#28857;&#25918;&#22312;&#31616;&#27905;&#30340;&#23646;&#24615;&#19978;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21629;&#21517;&#20026;&#8220;&#20808;&#22686;&#21152;&#23646;&#24615;&#65292;&#28982;&#21518;&#29983;&#25104;&#8221;&#65292;&#23558;&#20256;&#32479;&#30340;&#31471;&#21040;&#31471;&#29983;&#25104;&#36807;&#31243;&#20998;&#35299;&#20026;&#19977;&#20010;&#30452;&#35266;&#30340;&#27493;&#39588;&#65306;&#20869;&#23481;&#36873;&#25321;&#12289;&#21477;&#23376;&#35268;&#21010;&#21644;&#24207;&#21015;&#21477;&#23376;&#29983;&#25104;&#12290;&#36890;&#36807;&#39318;&#20808;&#35782;&#21035;&#30456;&#20851;&#26469;&#28304;&#37096;&#20998;&#65288;&#8220;&#20808;&#36873;&#25321;&#8221;&#65289;&#65292;&#28982;&#21518;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#23545;&#23427;&#20204;&#36827;&#34892;&#26465;&#20214;&#21270;&#65288;&#8220;&#28982;&#21518;&#29983;&#25104;&#8221;&#65289;&#65292;&#25105;&#20204;&#30830;&#20445;&#36825;&#20123;&#37096;&#20998;&#20063;&#20316;&#20026;&#36755;&#20986;&#30340;&#32454;&#31890;&#24230;&#23646;&#24615;&#65288;&#8220;&#36873;&#25321;&#8221;&#21464;&#20026;&#8220;&#23646;&#24615;&#8221;&#65289;&#12290; &#22312;Mu&#19978;&#32463;&#36807;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17104v1 Announce Type: new  Abstract: Recent efforts to address hallucinations in Large Language Models (LLMs) have focused on attributed text generation, which supplements generated texts with citations of supporting sources for post-generation fact-checking and corrections. Yet, these citations often point to entire documents or paragraphs, burdening users with extensive verification work. In this paper, we introduce a locally-attributable text generation approach, prioritizing concise attributions. Our method, named ``Attribute First, then Generate'', breaks down the conventional end-to-end generation process into three intuitive steps: content selection, sentence planning, and sequential sentence generation. By initially identifying relevant source segments (``select first'') and then conditioning the generation process on them (``then generate''), we ensure these segments also act as the output's fine-grained attributions (``select'' becomes ``attribute''). Tested on Mu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#25805;&#20316;&#31995;&#32479;&#20013;&#30340;LLM&#20195;&#29702;&#25805;&#20316;&#31995;&#32479;&#65292;&#26088;&#22312;&#20248;&#21270;&#36164;&#28304;&#20998;&#37197;&#12289;&#20419;&#36827;&#20195;&#29702;&#38388;&#19978;&#19979;&#25991;&#20999;&#25442;&#12289;&#23454;&#29616;&#24182;&#21457;&#25191;&#34892;&#20197;&#21450;&#20026;&#20195;&#29702;&#25552;&#20379;&#24037;&#20855;&#26381;&#21153;&#12290;</title><link>https://arxiv.org/abs/2403.16971</link><description>&lt;p&gt;
LLM Agent Operating System
&lt;/p&gt;
&lt;p&gt;
LLM Agent Operating System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16971
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#25805;&#20316;&#31995;&#32479;&#20013;&#30340;LLM&#20195;&#29702;&#25805;&#20316;&#31995;&#32479;&#65292;&#26088;&#22312;&#20248;&#21270;&#36164;&#28304;&#20998;&#37197;&#12289;&#20419;&#36827;&#20195;&#29702;&#38388;&#19978;&#19979;&#25991;&#20999;&#25442;&#12289;&#23454;&#29616;&#24182;&#21457;&#25191;&#34892;&#20197;&#21450;&#20026;&#20195;&#29702;&#25552;&#20379;&#24037;&#20855;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16971v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26234;&#33021;&#20195;&#29702;&#23384;&#22312;&#35832;&#22810;&#25361;&#25112;&#65292;&#20250;&#25439;&#23475;&#23427;&#20204;&#30340;&#25928;&#29575;&#21644;&#21151;&#25928;&#12290;&#20854;&#20013;&#21253;&#25324;&#20195;&#29702;&#35831;&#27714;&#22312;LLM&#19978;&#30340;&#27425;&#20248;&#35843;&#24230;&#21644;&#36164;&#28304;&#20998;&#37197;&#12289;&#22312;&#20195;&#29702;&#21644;LLM&#20043;&#38388;&#20132;&#20114;&#26102;&#20445;&#25345;&#19978;&#19979;&#25991;&#30340;&#22256;&#38590;&#65292;&#20197;&#21450;&#23558;&#20855;&#26377;&#19981;&#21516;&#33021;&#21147;&#21644;&#19987;&#19994;&#21270;&#30340;&#24322;&#26500;&#20195;&#29702;&#38598;&#25104;&#22312;&#19968;&#36215;&#30340;&#22797;&#26434;&#24615;&#12290;&#20195;&#29702;&#25968;&#37327;&#21644;&#22797;&#26434;&#24615;&#30340;&#24555;&#36895;&#22686;&#21152;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#36890;&#24120;&#20250;&#23548;&#33268;&#36164;&#28304;&#29942;&#39048;&#21644;&#27425;&#20248;&#36164;&#28304;&#21033;&#29992;&#12290;&#21463;&#21040;&#36825;&#20123;&#25361;&#25112;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;AIOS&#65292;&#19968;&#31181;LLM&#20195;&#29702;&#25805;&#20316;&#31995;&#32479;&#65292;&#23427;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#25805;&#20316;&#31995;&#32479;&#65288;OS&#65289;&#20013;&#12290;&#20855;&#20307;&#22320;&#65292;AIOS&#26088;&#22312;&#20248;&#21270;&#36164;&#28304;&#20998;&#37197;&#65292;&#20419;&#36827;&#20195;&#29702;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#20999;&#25442;&#65292;&#23454;&#29616;&#20195;&#29702;&#30340;&#24182;&#21457;&#25191;&#34892;&#65292;&#20026;&#20195;&#29702;&#25552;&#20379;&#24037;&#20855;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16971v1 Announce Type: cross  Abstract: The integration and deployment of large language model (LLM)-based intelligent agents have been fraught with challenges that compromise their efficiency and efficacy. Among these issues are sub-optimal scheduling and resource allocation of agent requests over the LLM, the difficulties in maintaining context during interactions between agent and LLM, and the complexities inherent in integrating heterogeneous agents with different capabilities and specializations. The rapid increase of agent quantity and complexity further exacerbates these issues, often leading to bottlenecks and sub-optimal utilization of resources. Inspired by these challenges, this paper presents AIOS, an LLM agent operating system, which embeds large language model into operating systems (OS). Specifically, AIOS is designed to optimize resource allocation, facilitate context switch across agents, enable concurrent execution of agents, provide tool service for agents
&lt;/p&gt;</description></item><item><title>&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#25104;&#23545;&#20559;&#22909;&#25628;&#32034;&#26041;&#27861;PAIRS&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;LLMs&#19982;&#20154;&#31867;&#21028;&#26029;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#20248;&#20110;&#30452;&#25509;&#25171;&#20998;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.16950</link><description>&lt;p&gt;
&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#19968;&#33268;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20013;&#25104;&#23545;&#20559;&#22909;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Aligning with Human Judgement: The Role of Pairwise Preference in Large Language Model Evaluators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16950
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#25104;&#23545;&#20559;&#22909;&#25628;&#32034;&#26041;&#27861;PAIRS&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;LLMs&#19982;&#20154;&#31867;&#21028;&#26029;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#20248;&#20110;&#30452;&#25509;&#25171;&#20998;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#33258;&#21160;&#35780;&#20272;&#22120;&#22312;&#35780;&#20272;&#29983;&#25104;&#30340;&#33258;&#28982;&#35821;&#35328;&#36136;&#37327;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;LLMs&#22312;&#35780;&#20272;&#20013;&#20173;&#23384;&#22312;&#20559;&#35265;&#65292;&#24120;&#24120;&#38590;&#20197;&#29983;&#25104;&#19982;&#20154;&#31867;&#35780;&#20272;&#19968;&#33268;&#30340;&#36830;&#36143;&#35780;&#20272;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;LLM&#35780;&#20272;&#22120;&#19982;&#20154;&#31867;&#21028;&#26029;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#36827;&#34892;&#31995;&#32479;&#30740;&#31350;&#65292;&#25581;&#31034;&#29616;&#26377;&#26088;&#22312;&#20943;&#36731;&#20559;&#35265;&#30340;&#26657;&#20934;&#26041;&#27861;&#19981;&#36275;&#20197;&#26377;&#25928;&#23558;LLM&#35780;&#20272;&#22120;&#23545;&#40784;&#12290;&#21463;&#21040;RLHF&#20013;&#23545;&#20559;&#22909;&#25968;&#25454;&#30340;&#20351;&#29992;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#35780;&#20272;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#25490;&#24207;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;Pairwise-preference Search&#65288;PAIRS&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#20197;LLMs&#36827;&#34892;&#25104;&#23545;&#27604;&#36739;&#24182;&#26377;&#25928;&#23545;&#20505;&#36873;&#25991;&#26412;&#36827;&#34892;&#25490;&#24207;&#30340;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#30340;&#25628;&#32034;&#26041;&#27861;&#12290;PAIRS&#22312;&#20195;&#34920;&#24615;&#35780;&#20272;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#26174;&#31034;&#20986;&#27604;&#30452;&#25509;&#25171;&#20998;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16950v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated promising capabilities as automatic evaluators in assessing the quality of generated natural language. However, LLMs still exhibit biases in evaluation and often struggle to generate coherent evaluations that align with human assessments. In this work, we first conduct a systematic study of the misalignment between LLM evaluators and human judgement, revealing that existing calibration methods aimed at mitigating biases are insufficient for effectively aligning LLM evaluators. Inspired by the use of preference data in RLHF, we formulate the evaluation as a ranking problem and introduce Pairwise-preference Search (PAIRS), an uncertainty-guided search method that employs LLMs to conduct pairwise comparisons and efficiently ranks candidate texts. PAIRS achieves state-of-the-art performance on representative evaluation tasks and demonstrates significant improvements over direct scoring. Furthe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#31895;&#35843;&#20248;&#20316;&#20026;&#19968;&#20010;&#20013;&#38388;&#23398;&#20064;&#38454;&#27573;&#65292;&#36830;&#25509;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#22312;&#19987;&#39064;&#25991;&#26723;&#26816;&#32034;&#20013;&#26174;&#33879;&#25913;&#21892;&#20102;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.16915</link><description>&lt;p&gt;
&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31895;&#35843;&#20248;&#30340;&#19987;&#39064;&#25991;&#26723;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Coarse-Tuning for Ad-hoc Document Retrieval Using Pre-trained Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#31895;&#35843;&#20248;&#20316;&#20026;&#19968;&#20010;&#20013;&#38388;&#23398;&#20064;&#38454;&#27573;&#65292;&#36830;&#25509;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#22312;&#19987;&#39064;&#25991;&#26723;&#26816;&#32034;&#20013;&#26174;&#33879;&#25913;&#21892;&#20102;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#20013;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM-based IR&#65289;&#36827;&#34892;&#24494;&#35843;&#38656;&#35201;&#23398;&#20064;&#26597;&#35810;&#34920;&#31034;&#21644;&#26597;&#35810;-&#25991;&#26723;&#20851;&#31995;&#65292;&#38500;&#20102;&#19979;&#28216;&#20219;&#21153;&#29305;&#23450;&#30340;&#23398;&#20064;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#31895;&#35843;&#20248;&#20316;&#20026;&#19968;&#20010;&#20013;&#38388;&#23398;&#20064;&#38454;&#27573;&#65292;&#36830;&#25509;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#12290;&#36890;&#36807;&#22312;&#31895;&#35843;&#20248;&#23398;&#20064;&#26597;&#35810;&#34920;&#31034;&#21644;&#26597;&#35810;-&#25991;&#26723;&#20851;&#31995;&#65292;&#25105;&#20204;&#26088;&#22312;&#20943;&#23569;&#24494;&#35843;&#30340;&#36127;&#25285;&#65292;&#25552;&#39640;&#19979;&#28216;IR&#20219;&#21153;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#31895;&#35843;&#20248;&#30340;&#26597;&#35810;-&#25991;&#26723;&#23545;&#39044;&#27979;&#65288;QDPP&#65289;&#65292;&#20854;&#39044;&#27979;&#26597;&#35810;-&#25991;&#26723;&#23545;&#30340;&#36866;&#24403;&#24615;&#12290;&#35780;&#20272;&#23454;&#39564;&#26174;&#31034;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26174;&#33879;&#25913;&#21892;&#20102;&#22235;&#20010;&#19987;&#39064;&#25991;&#26723;&#26816;&#32034;&#25968;&#25454;&#38598;&#20013;&#30340;MRR&#21644;/&#25110;nDCG@5&#12290;&#27492;&#22806;&#65292;&#26597;&#35810;&#39044;&#27979;&#20219;&#21153;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#31895;&#35843;&#20248;&#20419;&#36827;&#20102;&#26597;&#35810;&#34920;&#31034;&#21644;&#26597;&#35810;-&#25991;&#26723;&#20851;&#31995;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16915v1 Announce Type: cross  Abstract: Fine-tuning in information retrieval systems using pre-trained language models (PLM-based IR) requires learning query representations and query-document relations, in addition to downstream task-specific learning. This study introduces coarse-tuning as an intermediate learning stage that bridges pre-training and fine-tuning. By learning query representations and query-document relations in coarse-tuning, we aim to reduce the load of fine-tuning and improve the learning effect of downstream IR tasks. We propose Query-Document Pair Prediction (QDPP) for coarse-tuning, which predicts the appropriateness of query-document pairs. Evaluation experiments show that the proposed method significantly improves MRR and/or nDCG@5 in four ad-hoc document retrieval datasets. Furthermore, the results of the query prediction task suggested that coarse-tuning facilitated learning of query representation and query-document relations.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#26816;&#32034;&#21644;&#24635;&#32467;&#32593;&#32476;&#20013;&#30340;&#35777;&#25454;&#65292;&#26500;&#24314;&#20102;RU22Fact&#25968;&#25454;&#38598;&#65292;&#26159;&#20851;&#20110;2022&#24180;&#20420;&#20044;&#20914;&#31361;&#30340;&#22810;&#35821;&#35328;&#21487;&#35299;&#37322;&#20107;&#23454;&#26680;&#26597;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#24320;&#21457;&#20102;&#31471;&#21040;&#31471;&#21487;&#35299;&#37322;&#30340;&#20107;&#23454;&#26680;&#26597;&#31995;&#32479;&#26469;&#39564;&#35777;&#22768;&#26126;&#24182;&#29983;&#25104;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2403.16662</link><description>&lt;p&gt;
RU22Fact&#65306;&#20248;&#21270;&#22810;&#35821;&#35328;&#21487;&#35299;&#37322;&#20107;&#23454;&#26680;&#26597;&#20013;&#30340;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
RU22Fact: Optimizing Evidence for Multilingual Explainable Fact-Checking on Russia-Ukraine Conflict
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16662
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#26816;&#32034;&#21644;&#24635;&#32467;&#32593;&#32476;&#20013;&#30340;&#35777;&#25454;&#65292;&#26500;&#24314;&#20102;RU22Fact&#25968;&#25454;&#38598;&#65292;&#26159;&#20851;&#20110;2022&#24180;&#20420;&#20044;&#20914;&#31361;&#30340;&#22810;&#35821;&#35328;&#21487;&#35299;&#37322;&#20107;&#23454;&#26680;&#26597;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#24320;&#21457;&#20102;&#31471;&#21040;&#31471;&#21487;&#35299;&#37322;&#30340;&#20107;&#23454;&#26680;&#26597;&#31995;&#32479;&#26469;&#39564;&#35777;&#22768;&#26126;&#24182;&#29983;&#25104;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#23454;&#26680;&#26597;&#26159;&#36890;&#36807;&#26816;&#26597;&#29616;&#26377;&#35777;&#25454;&#26469;&#39564;&#35777;&#32473;&#23450;&#22768;&#26126;&#30340;&#20934;&#30830;&#24615;&#30340;&#20219;&#21153;&#12290;&#39640;&#36136;&#37327;&#30340;&#35777;&#25454;&#22312;&#22686;&#24378;&#20107;&#23454;&#26680;&#26597;&#31995;&#32479;&#24182;&#20419;&#36827;&#29983;&#25104;&#21487;&#29702;&#35299;&#30340;&#35299;&#37322;&#26041;&#38754;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#20026;&#21487;&#35299;&#37322;&#30340;&#20107;&#23454;&#26680;&#26597;&#31995;&#32479;&#25552;&#20379;&#36275;&#22815;&#21644;&#30456;&#20851;&#30340;&#35777;&#25454;&#26159;&#19968;&#39033;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#20174;&#32593;&#32476;&#20013;&#26816;&#32034;&#21644;&#24635;&#32467;&#35777;&#25454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;RU22Fact&#65292;&#36825;&#26159;&#19968;&#20010;&#20851;&#20110;2022&#24180;&#20420;&#20044;&#20914;&#31361;&#30340;&#26032;&#22411;&#22810;&#35821;&#35328;&#21487;&#35299;&#37322;&#20107;&#23454;&#26680;&#26597;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;1.6&#19975;&#20010;&#26679;&#26412;&#65292;&#27599;&#20010;&#26679;&#26412;&#37117;&#21253;&#21547;&#29616;&#23454;&#19990;&#30028;&#30340;&#22768;&#26126;&#12289;&#20248;&#21270;&#35777;&#25454;&#21644;&#24341;&#29992;&#30340;&#35299;&#37322;&#12290;&#20026;&#20102;&#20026;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#24314;&#31435;&#22522;&#20934;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#21487;&#35299;&#37322;&#30340;&#20107;&#23454;&#26680;&#26597;&#31995;&#32479;&#65292;&#29992;&#20110;&#26680;&#23454;&#22768;&#26126;&#24182;&#29983;&#25104;&#35299;&#37322;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16662v1 Announce Type: new  Abstract: Fact-checking is the task of verifying the factuality of a given claim by examining the available evidence. High-quality evidence plays a vital role in enhancing fact-checking systems and facilitating the generation of explanations that are understandable to humans. However, the provision of both sufficient and relevant evidence for explainable fact-checking systems poses a challenge. To tackle this challenge, we propose a method based on a Large Language Model to automatically retrieve and summarize evidence from the Web. Furthermore, we construct RU22Fact, a novel multilingual explainable fact-checking dataset on the Russia-Ukraine conflict in 2022 of 16K samples, each containing real-world claims, optimized evidence, and referenced explanation. To establish a baseline for our dataset, we also develop an end-to-end explainable fact-checking system to verify claims and generate explanations. Experimental results demonstrate the prospect
&lt;/p&gt;</description></item><item><title>LLMs&#24050;&#25104;&#20026;&#29983;&#29289;&#21307;&#23398;&#19982;&#20581;&#24247;&#20449;&#24687;&#23398;&#20013;&#37325;&#35201;&#30340;&#24037;&#20855;&#65292;&#26412;&#25991;&#29486;&#35745;&#37327;&#23398;&#32508;&#36848;&#20840;&#38754;&#23637;&#31034;&#20102;LLMs&#22312;&#21508;&#31181;BHI&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#20854;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#30340;&#25913;&#36827;&#65292;&#25581;&#31034;&#20102;&#20027;&#35201;&#21457;&#23637;&#36235;&#21183;&#21644;&#30740;&#31350;&#32593;&#32476;&#65292;&#24182;&#35752;&#35770;&#20102;&#20262;&#29702;&#20851;&#20999;&#21644;&#23454;&#38469;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.16303</link><description>&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#19982;&#20581;&#24247;&#20449;&#24687;&#23398;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#39033;&#25991;&#29486;&#35745;&#37327;&#23398;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Large Language Models in Biomedical and Health Informatics: A Bibliometric Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16303
&lt;/p&gt;
&lt;p&gt;
LLMs&#24050;&#25104;&#20026;&#29983;&#29289;&#21307;&#23398;&#19982;&#20581;&#24247;&#20449;&#24687;&#23398;&#20013;&#37325;&#35201;&#30340;&#24037;&#20855;&#65292;&#26412;&#25991;&#29486;&#35745;&#37327;&#23398;&#32508;&#36848;&#20840;&#38754;&#23637;&#31034;&#20102;LLMs&#22312;&#21508;&#31181;BHI&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#20854;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#30340;&#25913;&#36827;&#65292;&#25581;&#31034;&#20102;&#20027;&#35201;&#21457;&#23637;&#36235;&#21183;&#21644;&#30740;&#31350;&#32593;&#32476;&#65292;&#24182;&#35752;&#35770;&#20102;&#20262;&#29702;&#20851;&#20999;&#21644;&#23454;&#38469;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36805;&#36895;&#25104;&#20026;&#29983;&#29289;&#21307;&#23398;&#19982;&#20581;&#24247;&#20449;&#24687;&#23398;&#65288;BHI&#65289;&#20013;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#20026;&#20998;&#26512;&#25968;&#25454;&#12289;&#27835;&#30103;&#24739;&#32773;&#21644;&#24320;&#23637;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#24335;&#12290;&#26412;&#25991;&#29486;&#35745;&#37327;&#23398;&#32508;&#36848;&#26088;&#22312;&#36890;&#36807;&#26816;&#26597;&#33258;2022&#24180;&#33267;2023&#24180;&#30340;&#30740;&#31350;&#25991;&#31456;&#21644;&#21512;&#20316;&#32593;&#32476;&#65292;&#20840;&#38754;&#23637;&#31034;LLMs&#22312;BHI&#20013;&#30340;&#24212;&#29992;&#24773;&#20917;&#12290;&#23427;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;LLMs&#22914;&#20309;&#21487;&#20197;&#25913;&#36827;&#21508;&#31181;BHI&#39046;&#22495;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24212;&#29992;&#65292;&#22914;&#21307;&#23398;&#35786;&#26029;&#12289;&#24739;&#32773;&#21442;&#19982;&#12289;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#31649;&#29702;&#21644;&#20010;&#24615;&#21270;&#21307;&#23398;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30340;&#25991;&#29486;&#35745;&#37327;&#23398;&#32508;&#36848;&#30830;&#23450;&#20102;&#20851;&#38190;&#36235;&#21183;&#65292;&#32472;&#21046;&#20102;&#30740;&#31350;&#32593;&#32476;&#65292;&#24182;&#31361;&#20986;&#20102;&#36825;&#20010;&#24555;&#36895;&#21457;&#23637;&#39046;&#22495;&#30340;&#20027;&#35201;&#36827;&#23637;&#12290;&#26368;&#21518;&#65292;&#23427;&#35752;&#35770;&#20102;&#22312;BHI&#20013;&#20351;&#29992;LLMs&#30340;&#20262;&#29702;&#20851;&#20999;&#21644;&#23454;&#38469;&#25361;&#25112;&#65292;&#22914;&#25968;&#25454;&#38544;&#31169;&#21644;&#21487;&#38752;&#30340;&#21307;&#30103;&#24314;&#35758;&#12290;&#23637;&#26395;&#26410;&#26469;&#65292;&#25105;&#20204;&#32771;&#34385;LLMs&#22914;&#20309;&#36827;&#19968;&#27493;&#25913;&#21464;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16303v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have rapidly become important tools in Biomedical and Health Informatics (BHI), enabling new ways to analyze data, treat patients, and conduct research. This bibliometric review aims to provide a panoramic view of how LLMs have been used in BHI by examining research articles and collaboration networks from 2022 to 2023. It further explores how LLMs can improve Natural Language Processing (NLP) applications in various BHI areas like medical diagnosis, patient engagement, electronic health record management, and personalized medicine. To do this, our bibliometric review identifies key trends, maps out research networks, and highlights major developments in this fast-moving field. Lastly, it discusses the ethical concerns and practical challenges of using LLMs in BHI, such as data privacy and reliable medical recommendations. Looking ahead, we consider how LLMs could further transform biomedical research as we
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20027;&#39064;&#24314;&#27169;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#30456;&#20851;&#20027;&#39064;&#26631;&#39064;&#24182;&#36981;&#24490;&#20154;&#31867;&#25351;&#21335;&#26469;&#31934;&#32454;&#21270;&#21644;&#21512;&#24182;&#20027;&#39064;</title><link>https://arxiv.org/abs/2403.16248</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#20256;&#32479;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#25552;&#20379;&#20102;&#21478;&#19968;&#31181;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Offer an Alternative to the Traditional Approach of Topic Modelling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16248
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20027;&#39064;&#24314;&#27169;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#30456;&#20851;&#20027;&#39064;&#26631;&#39064;&#24182;&#36981;&#24490;&#20154;&#31867;&#25351;&#21335;&#26469;&#31934;&#32454;&#21270;&#21644;&#21512;&#24182;&#20027;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#39064;&#24314;&#27169;&#20316;&#20026;&#19968;&#31181;&#25104;&#29087;&#30340;&#26080;&#30417;&#30563;&#25216;&#26415;&#65292;&#22312;&#33258;&#21160;&#26816;&#27979;&#25991;&#26723;&#35821;&#26009;&#24211;&#20013;&#30340;&#37325;&#35201;&#20027;&#39064;&#26041;&#38754;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#32463;&#20856;&#30340;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#65288;&#20363;&#22914;LDA&#65289;&#23384;&#22312;&#26576;&#20123;&#32570;&#28857;&#65292;&#20363;&#22914;&#32570;&#20047;&#35821;&#20041;&#29702;&#35299;&#21644;&#20027;&#39064;&#37325;&#21472;&#30340;&#23384;&#22312;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#25581;&#31034;&#24191;&#27867;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#28508;&#22312;&#20027;&#39064;&#30340;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20419;&#20351;LLMs&#20174;&#32473;&#23450;&#30340;&#19968;&#32452;&#25991;&#26723;&#20013;&#29983;&#25104;&#20027;&#39064;&#65292;&#24182;&#24314;&#31435;&#20102;&#35780;&#20272;&#21327;&#35758;&#26469;&#35780;&#20272;LLMs&#30340;&#32858;&#31867;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;LLMs&#22312;&#36866;&#24403;&#30340;&#25552;&#31034;&#19979;&#21487;&#20197;&#33073;&#39062;&#32780;&#20986;&#20316;&#20026;&#19968;&#31181;&#21487;&#34892;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#33021;&#22815;&#29983;&#25104;&#30456;&#20851;&#30340;&#20027;&#39064;&#26631;&#39064;&#24182;&#36981;&#24490;&#20154;&#31867;&#25351;&#21335;&#26469;&#31934;&#32454;&#21270;&#21644;&#21512;&#24182;&#20027;&#39064;&#12290;&#36890;&#36807;&#28145;&#20837;&#30340;&#23454;&#39564;&#21644;&#35780;&#20272;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#36825;&#31181;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16248v1 Announce Type: new  Abstract: Topic modelling, as a well-established unsupervised technique, has found extensive use in automatically detecting significant topics within a corpus of documents. However, classic topic modelling approaches (e.g., LDA) have certain drawbacks, such as the lack of semantic understanding and the presence of overlapping topics. In this work, we investigate the untapped potential of large language models (LLMs) as an alternative for uncovering the underlying topics within extensive text corpora. To this end, we introduce a framework that prompts LLMs to generate topics from a given set of documents and establish evaluation protocols to assess the clustering efficacy of LLMs. Our findings indicate that LLMs with appropriate prompts can stand out as a viable alternative, capable of generating relevant topic titles and adhering to human guidelines to refine and merge topics. Through in-depth experiments and evaluation, we summarise the advantage
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20934;&#30830;&#23450;&#20301;&#21644;&#24809;&#32602;&#24187;&#35273;&#26631;&#35760;&#65292;ESREAL&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#35821;&#20041;&#37325;&#24314;&#26469;&#25233;&#21046;&#29983;&#25104;&#24187;&#35273;&#65292;&#35299;&#20915;&#20102;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.16167</link><description>&lt;p&gt;
&#21033;&#29992;&#35821;&#20041;&#37325;&#24314;&#20943;&#23569;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Exploiting Semantic Reconstruction to Mitigate Hallucinations in Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16167
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20934;&#30830;&#23450;&#20301;&#21644;&#24809;&#32602;&#24187;&#35273;&#26631;&#35760;&#65292;ESREAL&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#35821;&#20041;&#37325;&#24314;&#26469;&#25233;&#21046;&#29983;&#25104;&#24187;&#35273;&#65292;&#35299;&#20915;&#20102;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#23545;&#20854;&#21487;&#38752;&#24615;&#26500;&#25104;&#37325;&#22823;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#29983;&#25104;&#38271;&#26631;&#39064;&#26102;&#12290;&#24403;&#21069;&#26041;&#27861;&#26080;&#27861;&#20934;&#30830;&#35782;&#21035;&#21644;&#20943;&#36731;&#36825;&#20123;&#24187;&#35273;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;ESREAL&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#20934;&#30830;&#23450;&#20301;&#21644;&#24809;&#32602;&#24187;&#35273;&#26631;&#35760;&#26469;&#25233;&#21046;&#24187;&#35273;&#29983;&#25104;&#12290;&#26368;&#21021;&#65292;ESREAL&#26681;&#25454;&#29983;&#25104;&#30340;&#26631;&#39064;&#21019;&#24314;&#19968;&#20010;&#37325;&#24314;&#22270;&#20687;&#65292;&#24182;&#23558;&#20854;&#23545;&#24212;&#21306;&#22495;&#19982;&#21407;&#22987;&#22270;&#20687;&#30340;&#21306;&#22495;&#23545;&#40784;&#12290;&#36825;&#31181;&#35821;&#20041;&#37325;&#24314;&#26377;&#21161;&#20110;&#35782;&#21035;&#29983;&#25104;&#26631;&#39064;&#20013;&#30340;&#26631;&#35760;&#32423;&#24187;&#35273;&#30340;&#23384;&#22312;&#21644;&#31867;&#22411;&#12290;&#38543;&#21518;&#65292;ESREAL&#36890;&#36807;&#35780;&#20272;&#23545;&#40784;&#21306;&#22495;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#26469;&#35745;&#31639;&#26631;&#35760;&#32423;&#24187;&#35273;&#20998;&#25968;&#65292;&#22522;&#20110;&#24187;&#35273;&#30340;&#31867;&#22411;&#12290;&#26368;&#21518;&#65292;ESREAL&#37319;&#29992;&#19968;&#31181;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#65292;&#36827;&#34892;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16167v1 Announce Type: cross  Abstract: Hallucinations in vision-language models pose a significant challenge to their reliability, particularly in the generation of long captions. Current methods fall short of accurately identifying and mitigating these hallucinations. To address this issue, we introduce ESREAL, a novel unsupervised learning framework designed to suppress the generation of hallucinations through accurate localization and penalization of hallucinated tokens. Initially, ESREAL creates a reconstructed image based on the generated caption and aligns its corresponding regions with those of the original image. This semantic reconstruction aids in identifying both the presence and type of token-level hallucinations within the generated caption. Subsequently, ESREAL computes token-level hallucination scores by assessing the semantic similarity of aligned regions based on the type of hallucination. Finally, ESREAL employs a proximal policy optimization algorithm, wh
&lt;/p&gt;</description></item><item><title>STEntConv&#21033;&#29992;&#29992;&#25143;&#31435;&#22330;&#24314;&#31435;&#20102;&#29992;&#25143;&#21644;&#21629;&#21517;&#23454;&#20307;&#30340;&#21152;&#26435;&#22270;&#65292;&#36890;&#36807;&#26377;&#31526;&#21495;&#22270;&#21367;&#31215;&#32593;&#32476;&#39044;&#27979;Reddit&#24086;&#23376;&#20013;&#30340;&#19981;&#21516;&#24847;&#35265;&#34920;&#36798;&#12290;</title><link>https://arxiv.org/abs/2403.15885</link><description>&lt;p&gt;
STEntConv&#65306;&#21033;&#29992;&#31435;&#22330;&#26816;&#27979;&#21644;&#26377;&#31526;&#21495;&#22270;&#21367;&#31215;&#32593;&#32476;&#39044;&#27979;&#19981;&#21516;&#24847;&#35265;
&lt;/p&gt;
&lt;p&gt;
STEntConv: Predicting Disagreement with Stance Detection and a Signed Graph Convolutional Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15885
&lt;/p&gt;
&lt;p&gt;
STEntConv&#21033;&#29992;&#29992;&#25143;&#31435;&#22330;&#24314;&#31435;&#20102;&#29992;&#25143;&#21644;&#21629;&#21517;&#23454;&#20307;&#30340;&#21152;&#26435;&#22270;&#65292;&#36890;&#36807;&#26377;&#31526;&#21495;&#22270;&#21367;&#31215;&#32593;&#32476;&#39044;&#27979;Reddit&#24086;&#23376;&#20013;&#30340;&#19981;&#21516;&#24847;&#35265;&#34920;&#36798;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#30340;&#20852;&#36215;&#23548;&#33268;&#26497;&#21270;&#30340;&#22312;&#32447;&#35752;&#35770;&#22686;&#21152;&#65292;&#29305;&#21035;&#26159;&#20851;&#20110;&#36873;&#20030;&#21644;&#27668;&#20505;&#21464;&#21270;&#31561;&#25919;&#27835;&#21644;&#31038;&#20250;&#25991;&#21270;&#35805;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#20004;&#31687;&#25991;&#31456;&#30340;&#20316;&#32773;&#26159;&#21542;&#21516;&#24847;&#25110;&#19981;&#21516;&#24847;&#65292;&#21033;&#29992;&#20174;&#20182;&#20204;&#30340;&#25991;&#31456;&#20013;&#33719;&#24471;&#30340;&#20851;&#20110;&#21629;&#21517;&#23454;&#20307;&#30340;&#29992;&#25143;&#31435;&#22330;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;STEntConv&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26500;&#24314;&#20102;&#19968;&#20010;&#30001;&#31435;&#22330;&#21152;&#26435;&#30340;&#29992;&#25143;&#21644;&#21629;&#21517;&#23454;&#20307;&#22270;&#65292;&#24182;&#35757;&#32451;&#20102;&#19968;&#20010;&#26377;&#31526;&#21495;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;SGCN&#65289;&#26469;&#26816;&#27979;&#35780;&#35770;&#21644;&#22238;&#22797;&#24086;&#23376;&#20043;&#38388;&#30340;&#19981;&#21516;&#24847;&#35265;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#21644;&#28040;&#34701;&#30740;&#31350;&#65292;&#24182;&#23637;&#31034;&#20986;&#21253;&#21547;&#27492;&#20449;&#24687;&#21487;&#20197;&#25913;&#21892;Reddit&#24086;&#23376;&#25968;&#25454;&#38598;&#19978;&#26377;&#20105;&#35758;&#30340;&#23376;&#29256;&#20027;&#39064;&#30340;&#19981;&#21516;&#24847;&#35265;&#26816;&#27979;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#24179;&#21488;&#29305;&#23450;&#29305;&#24449;&#25110;&#29992;&#25143;&#21382;&#21490;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15885v1 Announce Type: new  Abstract: The rise of social media platforms has led to an increase in polarised online discussions, especially on political and socio-cultural topics such as elections and climate change. We propose a simple and novel unsupervised method to predict whether the authors of two posts agree or disagree, leveraging user stances about named entities obtained from their posts. We present STEntConv, a model which builds a graph of users and named entities weighted by stance and trains a Signed Graph Convolutional Network (SGCN) to detect disagreement between comment and reply posts. We run experiments and ablation studies and show that including this information improves disagreement detection performance on a dataset of Reddit posts for a range of controversial subreddit topics, without the need for platform-specific features or user history.
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#19968;&#31181;&#38754;&#21521;&#30005;&#23376;&#31163;&#23376;&#23545;&#25758;&#26426;&#30340;&#22522;&#20110;RAG&#30340;&#25688;&#35201;&#29983;&#25104;&#20195;&#29702;&#65292;&#33021;&#22815;&#21387;&#32553;&#20449;&#24687;&#24182;&#24341;&#29992;&#30456;&#20851;&#22238;&#22797;&#65292;&#20026;&#21512;&#20316;&#32773;&#25552;&#20379;&#37325;&#22823;&#20248;&#21183;</title><link>https://arxiv.org/abs/2403.15729</link><description>&lt;p&gt;
&#38754;&#21521;&#30005;&#23376;&#31163;&#23376;&#23545;&#25758;&#26426;&#30340;&#22522;&#20110;RAG&#30340;&#25688;&#35201;&#29983;&#25104;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Towards a \textbf{RAG}-based Summarization Agent for the Electron-Ion Collider
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15729
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#31181;&#38754;&#21521;&#30005;&#23376;&#31163;&#23376;&#23545;&#25758;&#26426;&#30340;&#22522;&#20110;RAG&#30340;&#25688;&#35201;&#29983;&#25104;&#20195;&#29702;&#65292;&#33021;&#22815;&#21387;&#32553;&#20449;&#24687;&#24182;&#24341;&#29992;&#30456;&#20851;&#22238;&#22797;&#65292;&#20026;&#21512;&#20316;&#32773;&#25552;&#20379;&#37325;&#22823;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#24615;&#21644;&#24222;&#22823;&#30340;&#20449;&#24687;&#37327;&#28085;&#30422;&#20102;&#22823;&#35268;&#27169;&#23454;&#39564;&#30340;&#25991;&#20214;&#12289;&#35770;&#25991;&#12289;&#25968;&#25454;&#21644;&#20854;&#20182;&#36164;&#28304;&#65292;&#23548;&#33268;&#23548;&#33322;&#36825;&#20123;&#22810;&#26679;&#24418;&#24335;&#20449;&#24687;&#30340;&#20219;&#21153;&#38656;&#35201;&#22823;&#37327;&#26102;&#38388;&#21644;&#31934;&#21147;&#65292;&#23545;&#20110;&#26032;&#21512;&#20316;&#32773;&#21644;&#26089;&#26399;&#31185;&#23398;&#23478;&#26469;&#35828;&#23588;&#20026;&#33392;&#24040;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#27491;&#22312;&#24320;&#21457;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#30340;EIC&#25688;&#35201;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#65288;RAGS4EIC&#65289;&#12290;&#35813;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#19981;&#20165;&#21387;&#32553;&#20449;&#24687;&#65292;&#36824;&#26377;&#25928;&#24341;&#29992;&#30456;&#20851;&#22238;&#22797;&#65292;&#20026;&#21512;&#20316;&#32773;&#25552;&#20379;&#20102;&#37325;&#22823;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#39033;&#30446;&#37319;&#21462;&#20102;&#20004;&#27493;&#26041;&#27861;&#65306;&#39318;&#20808;&#65292;&#26597;&#35810;&#21253;&#21547;&#25152;&#26377;&#30456;&#20851;&#23454;&#39564;&#20449;&#24687;&#30340;&#32508;&#21512;&#21521;&#37327;&#25968;&#25454;&#24211;&#65307;&#20854;&#27425;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26681;&#25454;&#29992;&#25143;&#26597;&#35810;&#21644;&#26816;&#32034;&#25968;&#25454;&#29983;&#25104;&#21253;&#21547;&#24341;&#29992;&#30340;&#31616;&#27905;&#25688;&#35201;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#20351;&#29992;RAG&#35780;&#20272;&#30340;&#35780;&#20272;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15729v1 Announce Type: cross  Abstract: The complexity and sheer volume of information encompassing documents, papers, data, and other resources from large-scale experiments demand significant time and effort to navigate, making the task of accessing and utilizing these varied forms of information daunting, particularly for new collaborators and early-career scientists. To tackle this issue, a Retrieval Augmented Generation (RAG)--based Summarization AI for EIC (RAGS4EIC) is under development. This AI-Agent not only condenses information but also effectively references relevant responses, offering substantial advantages for collaborators. Our project involves a two-step approach: first, querying a comprehensive vector database containing all pertinent experiment information; second, utilizing a Large Language Model (LLM) to generate concise summaries enriched with citations based on user queries and retrieved data. We describe the evaluation methods that use RAG assessments 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26159;&#21542;&#23384;&#22312;&#31038;&#20250;&#32463;&#27982;&#20559;&#35265;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;SilverSpoon&#65292;&#24182;&#35780;&#20272;&#20102;&#36825;&#31181;&#20559;&#35265;&#30340;&#31243;&#24230;&#20197;&#21450;&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#30340;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.14633</link><description>&lt;p&gt;
&#20986;&#36523;&#23500;&#36149;&#65311;&#25506;&#35752;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31038;&#20250;&#32463;&#27982;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Born With a Silver Spoon? Investigating Socioeconomic Bias in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26159;&#21542;&#23384;&#22312;&#31038;&#20250;&#32463;&#27982;&#20559;&#35265;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;SilverSpoon&#65292;&#24182;&#35780;&#20272;&#20102;&#36825;&#31181;&#20559;&#35265;&#30340;&#31243;&#24230;&#20197;&#21450;&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20250;&#32463;&#27982;&#20559;&#35265;&#22312;&#31038;&#20250;&#20013;&#21152;&#21095;&#20102;&#19981;&#20844;&#24179;&#29616;&#35937;&#65292;&#26681;&#25454;&#20010;&#20154;&#32463;&#27982;&#21644;&#31038;&#20250;&#32972;&#26223;&#24433;&#21709;&#33719;&#21462;&#26426;&#20250;&#21644;&#36164;&#28304;&#30340;&#26426;&#20250;&#12290;&#36825;&#19968;&#26222;&#36941;&#38382;&#39064;&#25345;&#32493;&#22320;&#24310;&#32493;&#20102;&#31995;&#32479;&#24615;&#30340;&#19981;&#24179;&#31561;&#65292;&#38459;&#30861;&#20102;&#20316;&#20026;&#19968;&#20010;&#31038;&#20250;&#36861;&#27714;&#21253;&#23481;&#24615;&#36827;&#27493;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26159;&#21542;&#23384;&#22312;&#31038;&#20250;&#32463;&#27982;&#20559;&#35265;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65288;SilverSpoon&#65289;&#65292;&#21253;&#21547;3000&#20010;&#26679;&#26412;&#65292;&#23637;&#31034;&#20102;&#29301;&#28041;&#21040;&#24369;&#21183;&#32676;&#20307;&#30001;&#20110;&#20182;&#20204;&#30340;&#22788;&#22659;&#32780;&#23454;&#26045;&#36947;&#24503;&#27169;&#31946;&#34892;&#20026;&#30340;&#20551;&#35774;&#24773;&#26223;&#65292;&#24182;&#38382;&#36825;&#31181;&#34892;&#20026;&#26159;&#21542;&#22312;&#36947;&#24503;&#19978;&#25104;&#31435;&#12290;&#27492;&#22806;&#65292;&#36825;&#20010;&#25968;&#25454;&#38598;&#20855;&#26377;&#21452;&#37325;&#26631;&#35760;&#26041;&#26696;&#65292;&#24182;&#30001;&#23646;&#20110;&#31038;&#20250;&#32463;&#27982;&#20004;&#31471;&#30340;&#20154;&#36827;&#34892;&#20102;&#27880;&#37322;&#12290;&#20351;&#29992;SilverSpoon&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#34920;&#29616;&#20986;&#30340;&#31038;&#20250;&#32463;&#27982;&#20559;&#35265;&#31243;&#24230;&#20197;&#21450;&#35813;&#31243;&#24230;&#22914;&#20309;&#38543;&#27169;&#22411;&#22823;&#23567;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14633v1 Announce Type: cross  Abstract: Socioeconomic bias in society exacerbates disparities, influencing access to opportunities and resources based on individuals' economic and social backgrounds. This pervasive issue perpetuates systemic inequalities, hindering the pursuit of inclusive progress as a society. In this paper, we investigate the presence of socioeconomic bias, if any, in large language models. To this end, we introduce a novel dataset (SilverSpoon), consisting of 3000 samples that illustrate hypothetical scenarios that involve underprivileged people performing ethically ambiguous actions due to their circumstances, and ask whether the action is ethically justified. Further, this dataset has a dual-labeling scheme and has been annotated by people belonging to both ends of the socioeconomic spectrum. Using SilverSpoon, we evaluate the degree of socioeconomic bias expressed in large language models and the variation of this degree as a function of model size. W
&lt;/p&gt;</description></item><item><title>&#25506;&#32034;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35774;&#22791;&#23450;&#21521;&#35821;&#38899;&#26816;&#27979;&#30340;&#22810;&#27169;&#24577;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#25991;&#26412;&#21644;&#38899;&#39057;&#27169;&#22411;&#65292;&#20351;&#29992;&#22810;&#27169;&#24577;&#20449;&#24687;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#30456;&#31561;&#38169;&#35823;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.14438</link><description>&lt;p&gt;
&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35774;&#22791;&#23450;&#21521;&#35821;&#38899;&#26816;&#27979;&#30340;&#22810;&#27169;&#24577;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Multimodal Approach to Device-Directed Speech Detection with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14438
&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35774;&#22791;&#23450;&#21521;&#35821;&#38899;&#26816;&#27979;&#30340;&#22810;&#27169;&#24577;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#25991;&#26412;&#21644;&#38899;&#39057;&#27169;&#22411;&#65292;&#20351;&#29992;&#22810;&#27169;&#24577;&#20449;&#24687;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#30456;&#31561;&#38169;&#35823;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#25311;&#21161;&#25163;&#30340;&#20132;&#20114;&#36890;&#24120;&#20174;&#39044;&#23450;&#20041;&#35302;&#21457;&#30701;&#35821;&#24320;&#22987;&#65292;&#28982;&#21518;&#26159;&#29992;&#25143;&#21629;&#20196;&#12290;&#20026;&#20102;&#20351;&#19982;&#21161;&#25163;&#30340;&#20132;&#20114;&#26356;&#30452;&#35266;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#26159;&#21542;&#21487;&#20197;&#25918;&#24323;&#29992;&#25143;&#24517;&#39035;&#29992;&#35302;&#21457;&#30701;&#35821;&#24320;&#22987;&#27599;&#20010;&#21629;&#20196;&#30340;&#35201;&#27714;&#12290;&#25105;&#20204;&#36890;&#36807;&#19977;&#31181;&#26041;&#24335;&#25506;&#32034;&#20102;&#36825;&#20010;&#20219;&#21153;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#20165;&#20351;&#29992;&#20174;&#38899;&#39057;&#27874;&#24418;&#20013;&#33719;&#24471;&#30340;&#22768;&#23398;&#20449;&#24687;&#35757;&#32451;&#20998;&#31867;&#22120;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23558;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#30340;&#35299;&#30721;&#22120;&#36755;&#20986;&#65292;&#20363;&#22914;1-best&#20551;&#35774;&#65292;&#20316;&#20026;&#36755;&#20837;&#29305;&#24449;&#36755;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#31995;&#32479;&#65292;&#23558;&#22768;&#23398;&#21644;&#35789;&#27719;&#29305;&#24449;&#20197;&#21450;ASR&#35299;&#30721;&#22120;&#20449;&#21495;&#32467;&#21512;&#22312;LLM&#20013;&#12290;&#20351;&#29992;&#22810;&#27169;&#24577;&#20449;&#24687;&#30456;&#23545;&#20110;&#20165;&#25991;&#26412;&#21644;&#20165;&#38899;&#39057;&#27169;&#22411;&#25552;&#39640;&#20102;&#30456;&#31561;&#38169;&#35823;&#29575;&#39640;&#36798;39%&#21644;61%&#12290;&#22686;&#21152;LLM&#30340;&#22823;&#23567;&#24182;&#36890;&#36807;&#20302;&#31209;&#35843;&#25972;&#36827;&#34892;&#35757;&#32451;&#36827;&#19968;&#27493;&#20943;&#23569;&#20102;&#30456;&#23545;EER&#20540;&#30340;&#20943;&#23569;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14438v1 Announce Type: new  Abstract: Interactions with virtual assistants typically start with a predefined trigger phrase followed by the user command. To make interactions with the assistant more intuitive, we explore whether it is feasible to drop the requirement that users must begin each command with a trigger phrase. We explore this task in three ways: First, we train classifiers using only acoustic information obtained from the audio waveform. Second, we take the decoder outputs of an automatic speech recognition (ASR) system, such as 1-best hypotheses, as input features to a large language model (LLM). Finally, we explore a multimodal system that combines acoustic and lexical features, as well as ASR decoder signals in an LLM. Using multimodal information yields relative equal-error-rate improvements over text-only and audio-only models of up to 39% and 61%. Increasing the size of the LLM and training with low-rank adaption leads to further relative EER reductions o
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#35774;&#35745;&#31354;&#38388;&#65292;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#35774;&#35745;&#24072;&#22312;&#22810;&#32500;&#31354;&#38388;&#20013;&#26816;&#39564;&#21644;&#25506;&#32034;&#26234;&#33021;&#20132;&#20114;&#24335;&#20889;&#20316;&#21161;&#25163;&#30340;&#21508;&#31181;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.14117</link><description>&lt;p&gt;
&#19968;&#31181;&#26234;&#33021;&#20132;&#20114;&#24335;&#20889;&#20316;&#21161;&#25163;&#30340;&#35774;&#35745;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
A Design Space for Intelligent and Interactive Writing Assistants
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14117
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#35774;&#35745;&#31354;&#38388;&#65292;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#35774;&#35745;&#24072;&#22312;&#22810;&#32500;&#31354;&#38388;&#20013;&#26816;&#39564;&#21644;&#25506;&#32034;&#26234;&#33021;&#20132;&#20114;&#24335;&#20889;&#20316;&#21161;&#25163;&#30340;&#21508;&#31181;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25105;&#20204;&#36825;&#20010;&#24555;&#36895;&#31185;&#25216;&#21457;&#23637;&#30340;&#26102;&#20195;&#65292;&#20889;&#20316;&#21161;&#25163;&#30340;&#30740;&#31350;&#39046;&#22495;&#24050;&#32463;&#22312;&#21508;&#20010;&#30740;&#31350;&#31038;&#21306;&#20013;&#21464;&#24471;&#26085;&#30410;&#20998;&#25955;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#35774;&#35745;&#31354;&#38388;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#20316;&#20026;&#19968;&#31181;&#32467;&#26500;&#21270;&#30340;&#26041;&#27861;&#26469;&#26816;&#39564;&#21644;&#25506;&#32034;&#26234;&#33021;&#20132;&#20114;&#24335;&#20889;&#20316;&#21161;&#25163;&#30340;&#22810;&#32500;&#31354;&#38388;&#12290;&#36890;&#36807;&#22823;&#22411;&#31038;&#21306;&#21327;&#20316;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20889;&#20316;&#21161;&#25163;&#30340;&#20116;&#20010;&#26041;&#38754;&#65306;&#20219;&#21153;&#12289;&#29992;&#25143;&#12289;&#25216;&#26415;&#12289;&#20132;&#20114;&#21644;&#29983;&#24577;&#31995;&#32479;&#12290;&#22312;&#27599;&#20010;&#26041;&#38754;&#65292;&#25105;&#20204;&#36890;&#36807;&#31995;&#32479;&#23457;&#38405;115&#31687;&#35770;&#25991;&#23450;&#20041;&#20102;&#32500;&#24230;&#65288;&#21363;&#26041;&#38754;&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#65289;&#21644;&#20195;&#30721;&#65288;&#21363;&#27599;&#20010;&#32500;&#24230;&#30340;&#21487;&#33021;&#36873;&#39033;&#65289;&#12290;&#25105;&#20204;&#30340;&#35774;&#35745;&#31354;&#38388;&#26088;&#22312;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#35774;&#35745;&#24072;&#25552;&#20379;&#19968;&#20010;&#23454;&#29992;&#24037;&#20855;&#65292;&#24110;&#21161;&#20182;&#20204;&#23548;&#33322;&#12289;&#29702;&#35299;&#21644;&#27604;&#36739;&#20889;&#20316;&#21161;&#25163;&#30340;&#21508;&#31181;&#21487;&#33021;&#24615;&#65292;&#24182;&#24110;&#21161;&#26500;&#24605;&#21644;&#35774;&#35745;&#26032;&#30340;&#20889;&#20316;&#21161;&#25163;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14117v1 Announce Type: cross  Abstract: In our era of rapid technological advancement, the research landscape for writing assistants has become increasingly fragmented across various research communities. We seek to address this challenge by proposing a design space as a structured way to examine and explore the multidimensional space of intelligent and interactive writing assistants. Through a large community collaboration, we explore five aspects of writing assistants: task, user, technology, interaction, and ecosystem. Within each aspect, we define dimensions (i.e., fundamental components of an aspect) and codes (i.e., potential options for each dimension) by systematically reviewing 115 papers. Our design space aims to offer researchers and designers a practical tool to navigate, comprehend, and compare the various possibilities of writing assistants, and aid in the envisioning and design of new writing assistants.
&lt;/p&gt;</description></item><item><title>EthioLLM&#20026;&#22467;&#22622;&#20420;&#27604;&#20122;&#20116;&#31181;&#35821;&#35328;&#65288;&#38463;&#22982;&#21704;&#25289;&#35821;&#12289;&#30422;&#20234;&#20857;&#35821;&#12289;&#38463;&#26041;&#22885;&#32599;&#33707;&#35821;&#12289;&#32034;&#39532;&#37324;&#35821;&#21644;&#25552;&#26684;&#37324;&#23612;&#20122;&#35821;&#65289;&#20197;&#21450;&#33521;&#35821;&#24341;&#20837;&#20102;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;Ethiobenchmark&#65292;&#20026;&#21508;&#31181;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#35780;&#20272;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.13737</link><description>&lt;p&gt;
EthioLLM&#65306;&#29992;&#20110;&#22467;&#22622;&#20420;&#27604;&#20122;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21450;&#20219;&#21153;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
EthioLLM: Multilingual Large Language Models for Ethiopian Languages with Task Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13737
&lt;/p&gt;
&lt;p&gt;
EthioLLM&#20026;&#22467;&#22622;&#20420;&#27604;&#20122;&#20116;&#31181;&#35821;&#35328;&#65288;&#38463;&#22982;&#21704;&#25289;&#35821;&#12289;&#30422;&#20234;&#20857;&#35821;&#12289;&#38463;&#26041;&#22885;&#32599;&#33707;&#35821;&#12289;&#32034;&#39532;&#37324;&#35821;&#21644;&#25552;&#26684;&#37324;&#23612;&#20122;&#35821;&#65289;&#20197;&#21450;&#33521;&#35821;&#24341;&#20837;&#20102;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;Ethiobenchmark&#65292;&#20026;&#21508;&#31181;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#35780;&#20272;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36817;&#26469;&#22240;&#20854;&#22312;&#21508;&#31181;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#30340;&#20986;&#33394;&#34920;&#29616;&#32780;&#22791;&#21463;&#38738;&#30544;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35757;&#32451;LLMs&#30340;&#36164;&#28304;&#19981;&#36275;&#65292;&#20302;&#36164;&#28304;&#35821;&#35328;&#20173;&#33853;&#21518;&#20110;NLP&#39046;&#22495;&#30340;&#26368;&#26032;&#21457;&#23637;&#12290;&#22467;&#22622;&#20420;&#27604;&#20122;&#35821;&#35328;&#25317;&#26377;&#26174;&#33879;&#30340;&#35821;&#35328;&#22810;&#26679;&#24615;&#65292;&#21253;&#25324;&#24191;&#27867;&#30340;&#25991;&#23383;&#31995;&#32479;&#65292;&#24182;&#23500;&#26377;&#28145;&#36828;&#30340;&#23447;&#25945;&#21644;&#25991;&#21270;&#24847;&#20041;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;EthioLLM - &#20116;&#31181;&#22467;&#22622;&#20420;&#27604;&#20122;&#35821;&#35328;&#65288;&#38463;&#22982;&#21704;&#25289;&#35821;&#12289;&#30422;&#20234;&#20857;&#35821;&#12289;&#38463;&#26041;&#22885;&#32599;&#33707;&#35821;&#12289;&#32034;&#39532;&#37324;&#35821;&#21644;&#25552;&#26684;&#37324;&#23612;&#20122;&#35821;&#65289;&#21644;&#33521;&#35821;&#30340;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#21450;Ethiobenchmark - &#29992;&#20110;&#21508;&#31181;&#19979;&#28216;NLP&#20219;&#21153;&#30340;&#26032;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#20116;&#20010;&#19979;&#28216;NLP&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#24320;&#28304;&#25105;&#20204;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#12289;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#26032;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#31934;&#35843;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13737v1 Announce Type: new  Abstract: Large language models (LLMs) have gained popularity recently due to their outstanding performance in various downstream Natural Language Processing (NLP) tasks. However, low-resource languages are still lagging behind current state-of-the-art (SOTA) developments in the field of NLP due to insufficient resources to train LLMs. Ethiopian languages exhibit remarkable linguistic diversity, encompassing a wide array of scripts, and are imbued with profound religious and cultural significance. This paper introduces EthioLLM -- multilingual large language models for five Ethiopian languages (Amharic, Ge'ez, Afan Oromo, Somali, and Tigrinya) and English, and Ethiobenchmark -- a new benchmark dataset for various downstream NLP tasks. We evaluate the performance of these models across five downstream NLP tasks. We open-source our multilingual language models, new benchmark datasets for various downstream tasks, and task-specific fine-tuned languag
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#32454;&#31890;&#24230;&#25991;&#26412;&#25551;&#36848;&#20013;&#29983;&#25104;&#36816;&#21160;&#30340;&#26041;&#27861;&#65292;&#26500;&#24314;&#20102;FineHumanML3D&#25968;&#25454;&#38598;&#65292;&#35774;&#35745;&#20102;FineMotionDiffuse&#27169;&#22411;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2403.13518</link><description>&lt;p&gt;
&#20174;&#32454;&#31890;&#24230;&#25991;&#26412;&#25551;&#36848;&#20013;&#29983;&#25104;&#36816;&#21160;
&lt;/p&gt;
&lt;p&gt;
Motion Generation from Fine-grained Textual Descriptions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#32454;&#31890;&#24230;&#25991;&#26412;&#25551;&#36848;&#20013;&#29983;&#25104;&#36816;&#21160;&#30340;&#26041;&#27861;&#65292;&#26500;&#24314;&#20102;FineHumanML3D&#25968;&#25454;&#38598;&#65292;&#35774;&#35745;&#20102;FineMotionDiffuse&#27169;&#22411;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#21160;&#20316;&#30340;&#20219;&#21153;&#26159;&#20174;&#32473;&#23450;&#30340;&#25991;&#23383;&#25551;&#36848;&#29983;&#25104;&#36816;&#21160;&#24207;&#21015;&#65292;&#27169;&#22411;&#24212;&#35813;&#25506;&#32034;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#19982;&#20154;&#20307;&#21160;&#20316;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#20316;&#21697;&#23616;&#38480;&#20110;&#31895;&#31890;&#24230;&#30340;&#36816;&#21160;&#25551;&#36848;&#65288;&#20363;&#22914;&#65292;&#8220;&#19968;&#20010;&#20154;&#36466;&#19979;&#12290;&#8221;&#65289;&#65292;&#20960;&#20046;&#27809;&#26377;&#25506;&#32034;&#25351;&#23450;&#30456;&#20851;&#36523;&#20307;&#37096;&#20301;&#36816;&#21160;&#30340;&#32454;&#31890;&#24230;&#25551;&#36848;&#12290;&#29992;&#31895;&#31961;&#25991;&#26412;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#33021;&#26080;&#27861;&#23398;&#20064;&#20174;&#32454;&#31890;&#24230;&#36816;&#21160;&#30456;&#20851;&#35789;&#27719;&#21040;&#36816;&#21160;&#22522;&#20803;&#30340;&#26144;&#23556;&#65292;&#23548;&#33268;&#26080;&#27861;&#20174;&#26410;&#35265;&#25551;&#36848;&#29983;&#25104;&#21160;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#36755;&#20837;&#31934;&#32454;&#25552;&#31034;&#32473; GPT-3.5-turbo&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#32454;&#31890;&#24230;&#25991;&#26412;&#25551;&#36848;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;-&#21160;&#20316;&#25968;&#25454;&#38598;FineHumanML3D&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#25991;&#26412;&#21040;&#21160;&#20316;&#27169;&#22411;FineMotionDiffuse&#65292;&#20805;&#20998;&#21033;&#29992;&#32454;&#31890;&#24230;&#30340;&#25991;&#26412;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;FineMotionDiffuse&#22312;FineHumanML3D&#19978;&#35757;&#32451;&#21518;&#33719;&#24471;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13518v1 Announce Type: cross  Abstract: The task of text2motion is to generate motion sequences from given textual descriptions, where a model should explore the interactions between natural language instructions and human body movements. While most existing works are confined to coarse-grained motion descriptions (e.g., "A man squats."), fine-grained ones specifying movements of relevant body parts are barely explored. Models trained with coarse texts may not be able to learn mappings from fine-grained motion-related words to motion primitives, resulting in the failure in generating motions from unseen descriptions. In this paper, we build a large-scale language-motion dataset with fine-grained textual descriptions, FineHumanML3D, by feeding GPT-3.5-turbo with delicate prompts. Accordingly, we design a new text2motion model, FineMotionDiffuse, which makes full use of fine-grained textual information. Our experiments show that FineMotionDiffuse trained on FineHumanML3D acqui
&lt;/p&gt;</description></item><item><title>&#20026;&#20102;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#23384;&#22312;&#30340;&#39640;&#30828;&#20214;&#21644;&#35745;&#31639;&#38656;&#27714;&#65292;Hyacinth6B&#22312;&#27169;&#22411;&#36731;&#37327;&#21270;&#21644;&#24615;&#33021;&#20043;&#38388;&#25214;&#21040;&#20102;&#24179;&#34913;&#65292;&#37319;&#29992;LoRA&#26041;&#27861;&#36827;&#34892;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#12290;</title><link>https://arxiv.org/abs/2403.13334</link><description>&lt;p&gt;
Hyacinth6B&#65306;&#19968;&#20010;&#29992;&#20110;&#20013;&#25991;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Hyacinth6B: A large language model for Traditional Chinese
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13334
&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#23384;&#22312;&#30340;&#39640;&#30828;&#20214;&#21644;&#35745;&#31639;&#38656;&#27714;&#65292;Hyacinth6B&#22312;&#27169;&#22411;&#36731;&#37327;&#21270;&#21644;&#24615;&#33021;&#20043;&#38388;&#25214;&#21040;&#20102;&#24179;&#34913;&#65292;&#37319;&#29992;LoRA&#26041;&#27861;&#36827;&#34892;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#30340;&#20027;&#35201;&#21160;&#26426;&#26159;&#24212;&#23545;&#36890;&#24120;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#20851;&#30340;&#39640;&#30828;&#20214;&#21644;&#35745;&#31639;&#38656;&#27714;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#27169;&#22411;&#36731;&#37327;&#21270;&#21644;&#24615;&#33021;&#20043;&#38388;&#25214;&#21040;&#24179;&#34913;&#65292;&#21162;&#21147;&#22312;&#20351;&#29992;&#30456;&#23545;&#36731;&#37327;&#32423;&#27169;&#22411;&#30340;&#21516;&#26102;&#26368;&#22823;&#21270;&#24615;&#33021;&#12290;Hyacinth6B&#26159;&#22522;&#20110;&#36825;&#19968;&#30446;&#26631;&#24320;&#21457;&#30340;&#65292;&#26088;&#22312;&#20805;&#20998;&#21457;&#25381;LLM&#30340;&#26680;&#24515;&#33021;&#21147;&#65292;&#32780;&#19981;&#36896;&#25104;&#24040;&#22823;&#30340;&#36164;&#28304;&#25104;&#26412;&#65292;&#26377;&#25928;&#22320;&#25512;&#21160;&#36739;&#23567;&#27169;&#22411;&#30340;&#24615;&#33021;&#36793;&#30028;&#12290;&#35757;&#32451;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;LoRA&#26041;&#27861;&#36827;&#34892;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13334v1 Announce Type: new  Abstract: This research's primary motivation of this study is to address the high hardware and computational demands typically associated with LLMs.Therefore,our goal is to find a balance between model lightness and performance,striving to maximize performance while using a comparatively lightweight model. Hyacinth6B was developed with this objective in mind,aiming to fully leverage the core capabilities of LLMs without incurring substantial resource costs, effectively pushing the boundaries of smaller model's performance. The training approach involves parameter efficient finetuning using the LoRA method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#28085;&#30422;&#21335;&#26031;&#25289;&#22827;&#22320;&#21306;&#23448;&#26041;&#35821;&#35328;&#30340;&#39640;&#24230;&#21487;&#27604;&#36739;&#30340;&#32593;&#32476;&#35821;&#26009;&#24211;&#38598;&#21512;&#65292;&#37319;&#29992;&#20808;&#36827;&#30340;&#25216;&#26415;&#36827;&#34892;&#35821;&#35328;&#21644;&#25991;&#20307;&#26631;&#27880;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#20854;&#21487;&#27604;&#36739;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.12721</link><description>&lt;p&gt;
CLASSLA-web: &#21335;&#26031;&#25289;&#22827;&#35821;&#35328;&#30340;&#21487;&#27604;&#36739;&#32593;&#32476;&#35821;&#26009;&#24211;&#65292;&#27880;&#37325;&#35821;&#35328;&#21644;&#25991;&#20307;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
CLASSLA-web: Comparable Web Corpora of South Slavic Languages Enriched with Linguistic and Genre Annotation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#28085;&#30422;&#21335;&#26031;&#25289;&#22827;&#22320;&#21306;&#23448;&#26041;&#35821;&#35328;&#30340;&#39640;&#24230;&#21487;&#27604;&#36739;&#30340;&#32593;&#32476;&#35821;&#26009;&#24211;&#38598;&#21512;&#65292;&#37319;&#29992;&#20808;&#36827;&#30340;&#25216;&#26415;&#36827;&#34892;&#35821;&#35328;&#21644;&#25991;&#20307;&#26631;&#27880;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#20854;&#21487;&#27604;&#36739;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#28085;&#30422;&#26031;&#27931;&#25991;&#23612;&#20122;&#35821;&#12289;&#20811;&#32599;&#22320;&#20122;&#35821;&#12289;&#27874;&#26031;&#23612;&#20122;&#35821;&#12289;&#40657;&#23665;&#35821;&#12289;&#22622;&#23572;&#32500;&#20122;&#35821;&#12289;&#39532;&#20854;&#39039;&#35821;&#21644;&#20445;&#21152;&#21033;&#20122;&#35821;&#39640;&#24230;&#21487;&#27604;&#36739;&#30340;&#32593;&#32476;&#35821;&#26009;&#24211;&#38598;&#21512;&#65292;&#20174;&#32780;&#35206;&#30422;&#20102;&#21335;&#26031;&#25289;&#22827;&#35821;&#35328;&#31354;&#38388;&#25152;&#26377;&#23448;&#26041;&#35821;&#35328;&#30340;&#25972;&#20010;&#33539;&#22260;&#12290;&#36825;&#20123;&#35821;&#26009;&#24211;&#30340;&#24635;&#37327;&#20026;26&#20159;&#20010;&#21333;&#35789;&#65292;&#26469;&#33258;2600&#19975;&#31687;&#25991;&#26723;&#12290;&#35821;&#26009;&#24211;&#30340;&#21487;&#27604;&#36739;&#24615;&#30001;&#21487;&#27604;&#36739;&#30340;&#29228;&#32593;&#35774;&#32622;&#21644;&#30456;&#21516;&#30340;&#29228;&#32593;&#21644;&#21518;&#22788;&#29702;&#25216;&#26415;&#26469;&#30830;&#20445;&#12290;&#25152;&#26377;&#35821;&#26009;&#24211;&#37117;&#32463;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;CLASSLA-Stanza&#35821;&#35328;&#22788;&#29702;&#31649;&#36947;&#36827;&#34892;&#35821;&#35328;&#26631;&#27880;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;Transformer&#30340;&#22810;&#35821;&#31181;X-GENRE&#20998;&#31867;&#22120;&#22686;&#21152;&#20102;&#25991;&#26723;&#32423;&#21035;&#30340;&#25991;&#20307;&#20449;&#24687;&#65292;&#20174;&#32780;&#22312;&#35821;&#35328;&#26631;&#27880;&#21644;&#20803;&#25968;&#25454;&#20016;&#23500;&#21270;&#30340;&#27700;&#24179;&#19978;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#21487;&#27604;&#36739;&#24615;&#12290;&#23545;&#32467;&#26524;&#35821;&#26009;&#24211;&#30340;&#25991;&#20307;&#32858;&#28966;&#20998;&#26512;&#26174;&#31034;&#20102;&#36825;&#19971;&#20010;&#35821;&#26009;&#24211;&#20013;&#21508;&#31181;&#25991;&#20307;&#30340;&#30456;&#24403;&#19968;&#33268;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12721v1 Announce Type: new  Abstract: This paper presents a collection of highly comparable web corpora of Slovenian, Croatian, Bosnian, Montenegrin, Serbian, Macedonian, and Bulgarian, covering thereby the whole spectrum of official languages in the South Slavic language space. The collection of these corpora comprises a total of 13 billion tokens of texts from 26 million documents. The comparability of the corpora is ensured by a comparable crawling setup and the usage of identical crawling and post-processing technology. All the corpora were linguistically annotated with the state-of-the-art CLASSLA-Stanza linguistic processing pipeline, and enriched with document-level genre information via the Transformer-based multilingual X-GENRE classifier, which further enhances comparability at the level of linguistic annotation and metadata enrichment. The genre-focused analysis of the resulting corpora shows a rather consistent distribution of genres throughout the seven corpora,
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#30693;&#35782;&#22270;&#35889;&#32467;&#21512;&#65292;&#25552;&#39640;&#38646;&#26679;&#26412;&#23545;&#35937;&#29366;&#24577;&#20998;&#31867;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.12151</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#39046;&#22495;&#29305;&#23450;&#20869;&#23481;&#34701;&#20837;&#30693;&#35782;&#22270;&#35889;&#65292;&#20197;&#22686;&#24378;&#38646;&#26679;&#26412;&#23545;&#35937;&#29366;&#24577;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Fusing Domain-Specific Content from Large Language Models into Knowledge Graphs for Enhanced Zero Shot Object State Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12151
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#30693;&#35782;&#22270;&#35889;&#32467;&#21512;&#65292;&#25552;&#39640;&#38646;&#26679;&#26412;&#23545;&#35937;&#29366;&#24577;&#20998;&#31867;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#21487;&#20197;&#26174;&#33879;&#26377;&#21161;&#20110;&#35299;&#20915;&#21508;&#31181;&#35270;&#35273;&#20219;&#21153;&#65292;&#20294;&#29983;&#25104;&#36825;&#31181;&#30693;&#35782;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#21644;&#26102;&#38388;&#25104;&#26412;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36890;&#36807;&#35821;&#20041;&#23884;&#20837;&#29983;&#25104;&#21644;&#25552;&#20379;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#23558;LLM&#38598;&#25104;&#21040;&#19968;&#20010;&#27969;&#31243;&#20013;&#65292;&#35813;&#27969;&#31243;&#22312;&#35270;&#35273;&#22522;&#30784;&#38646;&#26679;&#26412;&#23545;&#35937;&#29366;&#24577;&#20998;&#31867;&#20219;&#21153;&#30340;&#32972;&#26223;&#19979;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#21644;&#39044;&#35757;&#32451;&#30340;&#35821;&#20041;&#21521;&#37327;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#28040;&#34701;&#30740;&#31350;&#24443;&#24213;&#30740;&#31350;&#20102;LLM&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#22522;&#20110;LLM&#30340;&#23884;&#20837;&#19982;&#36890;&#29992;&#30340;&#39044;&#35757;&#32451;&#23884;&#20837;&#32467;&#21512;&#20351;&#29992;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#20511;&#37492;&#36825;&#19968;&#28040;&#34701;&#30740;&#31350;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#23545;&#31454;&#20105;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#20174;&#32780;&#31361;&#20986;&#20102;&#26368;&#26032;&#30340;&#34920;&#29616;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12151v1 Announce Type: new  Abstract: Domain-specific knowledge can significantly contribute to addressing a wide variety of vision tasks. However, the generation of such knowledge entails considerable human labor and time costs. This study investigates the potential of Large Language Models (LLMs) in generating and providing domain-specific information through semantic embeddings. To achieve this, an LLM is integrated into a pipeline that utilizes Knowledge Graphs and pre-trained semantic vectors in the context of the Vision-based Zero-shot Object State Classification task. We thoroughly examine the behavior of the LLM through an extensive ablation study. Our findings reveal that the integration of LLM-based embeddings, in combination with general-purpose pre-trained embeddings, leads to substantial performance improvements. Drawing insights from this ablation study, we conduct a comparative analysis against competing models, thereby highlighting the state-of-the-art perfor
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#22270;&#31639;&#27861;&#21152;&#36895;&#31185;&#23398;&#21457;&#29616;&#65292;&#25581;&#31034;&#35770;&#25991;&#20043;&#38388;&#30340;&#28145;&#20837;&#36328;&#23398;&#31185;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#26448;&#26009;&#35774;&#35745;&#12290;</title><link>https://arxiv.org/abs/2403.11996</link><description>&lt;p&gt;
&#21033;&#29992;&#29983;&#25104;&#24335;&#30693;&#35782;&#25552;&#21462;&#12289;&#22522;&#20110;&#22270;&#30340;&#34920;&#31034;&#21644;&#22810;&#27169;&#24577;&#26234;&#33021;&#22270;&#25512;&#29702;&#21152;&#36895;&#31185;&#23398;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Accelerating Scientific Discovery with Generative Knowledge Extraction, Graph-Based Representation, and Multimodal Intelligent Graph Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11996
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#22270;&#31639;&#27861;&#21152;&#36895;&#31185;&#23398;&#21457;&#29616;&#65292;&#25581;&#31034;&#35770;&#25991;&#20043;&#38388;&#30340;&#28145;&#20837;&#36328;&#23398;&#31185;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#26448;&#26009;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65292;&#25105;&#20204;&#23558;&#19968;&#32452;&#28041;&#21450;&#29983;&#29289;&#26448;&#26009;&#39046;&#22495;&#30340;1,000&#31687;&#31185;&#23398;&#35770;&#25991;&#36716;&#21270;&#20026;&#35814;&#32454;&#30340;&#26412;&#20307;&#30693;&#35782;&#22270;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22266;&#26377;&#30340;&#26080;&#26631;&#24230;&#29305;&#24615;&#12290;&#36890;&#36807;&#22522;&#20110;&#33410;&#28857;&#30456;&#20284;&#24615;&#21644;&#20171;&#25968;&#20013;&#24515;&#24615;&#30340;&#32452;&#21512;&#25490;&#21517;&#65292;&#25506;&#27979;&#19981;&#21516;&#27010;&#24565;&#20043;&#38388;&#30340;&#22270;&#36941;&#21382;&#36335;&#24452;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#28145;&#20837;&#30340;&#36328;&#23398;&#31185;&#20851;&#31995;&#65292;&#21487;&#29992;&#20110;&#22238;&#31572;&#26597;&#35810;&#65292;&#35782;&#21035;&#30693;&#35782;&#20013;&#30340;&#31354;&#30333;&#65292;&#24182;&#25552;&#20986;&#21069;&#25152;&#26410;&#35265;&#30340;&#26448;&#26009;&#35774;&#35745;&#21450;&#20854;&#34892;&#20026;&#12290;&#19968;&#39033;&#27604;&#36739;&#25581;&#31034;&#20102;&#29983;&#29289;&#26448;&#26009;&#21644;&#36125;&#22810;&#33452;&#31532;&#20061;&#20132;&#21709;&#26354;&#20043;&#38388;&#30340;&#35814;&#32454;&#32467;&#26500;&#30456;&#20284;&#20043;&#22788;&#65292;&#31361;&#26174;&#20102;&#36890;&#36807;&#21516;&#26500;&#26144;&#23556;&#20849;&#20139;&#22797;&#26434;&#24615;&#27169;&#24335;&#12290;&#35813;&#31639;&#27861;&#36827;&#19968;&#27493;&#21019;&#24314;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#20998;&#32423;&#33740;&#19997;&#20307;&#30340;&#22797;&#21512;&#26448;&#26009;&#65292;&#23558;&#22270;&#37319;&#26679;&#30340;&#32852;&#21512;&#21512;&#25104;&#21407;&#29702;&#19982;&#24247;&#23450;&#26031;&#22522;&#12298;&#31532;&#19971;&#32452;&#25104;&#12299;&#20013;&#25552;&#21462;&#30340;&#21407;&#21017;&#30456;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11996v1 Announce Type: cross  Abstract: Using generative Artificial Intelligence (AI), we transformed a set of 1,000 scientific papers in the area of biological materials into detailed ontological knowledge graphs, revealing their inherently scale-free nature. Using graph traversal path detection between dissimilar concepts based on combinatorial ranking of node similarity and betweenness centrality, we reveal deep insights into unprecedented interdisciplinary relationships that can be used to answer queries, identify gaps in knowledge, and propose never-before-seen material designs and their behaviors. One comparison revealed detailed structural parallels between biological materials and Beethoven's 9th Symphony, highlighting shared patterns of complexity through isomorphic mapping. The algorithm further created an innovative hierarchical mycelium-based composite that incorporates joint synthesis of graph sampling with principles extracted from Kandinsky's Composition VII p
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;SelfIE&#26694;&#26550;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#33258;&#35299;&#37322;&#20854;&#23884;&#20837;&#65292;&#25581;&#31034;&#20869;&#37096;&#25512;&#29702;&#65292;&#21253;&#25324;&#36947;&#24503;&#20915;&#31574;&#12289;&#25552;&#31034;&#27880;&#20837;&#21644;&#28040;&#38500;&#26377;&#23475;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2403.10949</link><description>&lt;p&gt;
SelfIE&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#30340;&#33258;&#25105;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
SelfIE: Self-Interpretation of Large Language Model Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10949
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;SelfIE&#26694;&#26550;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#33258;&#35299;&#37322;&#20854;&#23884;&#20837;&#65292;&#25581;&#31034;&#20869;&#37096;&#25512;&#29702;&#65292;&#21253;&#25324;&#36947;&#24503;&#20915;&#31574;&#12289;&#25552;&#31034;&#27880;&#20837;&#21644;&#28040;&#38500;&#26377;&#23475;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10949v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#25688;&#35201;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;&#20309;&#33719;&#24471;&#31572;&#26696;&#65311;&#35299;&#37322;&#21644;&#25511;&#21046;LLM&#30340;&#25512;&#29702;&#36807;&#31243;&#23545;&#20110;&#21487;&#38752;&#24615;&#12289;&#36879;&#26126;&#24230;&#21644;&#26410;&#26469;&#27169;&#22411;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SelfIE&#65288;&#23884;&#20837;&#30340;&#33258;&#25105;&#35299;&#37322;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#33021;&#22815;&#21033;&#29992;LLMs&#21709;&#24212;&#20851;&#20110;&#32473;&#23450;&#27573;&#33853;&#30340;&#26597;&#35810;&#30340;&#33021;&#21147;&#65292;&#20197;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#23427;&#20204;&#33258;&#24049;&#30340;&#23884;&#20837;&#12290;SelfIE&#33021;&#22815;&#35299;&#37322;&#38544;&#34255;&#23884;&#20837;&#20013;&#30340;&#24320;&#25918;&#19990;&#30028;&#27010;&#24565;&#65292;&#22312;&#26696;&#20363;&#20013;&#25581;&#31034;LLM&#30340;&#20869;&#37096;&#25512;&#29702;&#65292;&#22914;&#20570;&#20986;&#36947;&#24503;&#20915;&#31574;&#12289;&#20869;&#21270;&#25552;&#31034;&#27880;&#20837;&#21644;&#22238;&#24819;&#26377;&#23475;&#30693;&#35782;&#12290;SelfIE&#23545;&#38544;&#34255;&#23884;&#20837;&#30340;&#25991;&#26412;&#25551;&#36848;&#20063;&#24320;&#36767;&#20102;&#25511;&#21046;LLM&#25512;&#29702;&#30340;&#26032;&#36884;&#24452;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#30417;&#30563;&#25511;&#21046;&#65292;&#23427;&#20801;&#35768;&#32534;&#36753;&#24320;&#25918;&#24335;&#27010;&#24565;&#65292;&#32780;&#21482;&#38656;&#35201;&#35745;&#31639;&#21333;&#20010;&#23618;&#30340;&#26799;&#24230;&#12290;&#25105;&#20204;&#23558;RLHF&#25193;&#23637;&#21040;&#38544;&#34255;&#30340;&#23884;&#20837;&#65292;&#24182;&#25552;&#20986;&#20102;&#24378;&#21270;&#25511;&#21046;&#26469;&#28040;&#38500;&#26377;&#23475;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10949v1 Announce Type: cross  Abstract: How do large language models (LLMs) obtain their answers? The ability to explain and control an LLM's reasoning process is key for reliability, transparency, and future model developments. We propose SelfIE (Self-Interpretation of Embeddings), a framework that enables LLMs to interpret their own embeddings in natural language by leveraging their ability to respond inquiry about a given passage. Capable of interpreting open-world concepts in the hidden embeddings, SelfIE reveals LLM internal reasoning in cases such as making ethical decisions, internalizing prompt injection, and recalling harmful knowledge. SelfIE's text descriptions on hidden embeddings also open up new avenues to control LLM reasoning. We propose Supervised Control, which allows editing open-ended concepts while only requiring gradient computation of individual layer. We extend RLHF to hidden embeddings and propose Reinforcement Control that erases harmful knowledge i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#30693;&#35782;&#25552;&#21462;&#20013;&#23384;&#22312;&#30340;&#8220;&#25552;&#31034;&#20559;&#35265;&#8221;&#65292;&#25214;&#21040;&#20102;&#19981;&#21516;&#31867;&#22411;&#25552;&#31034;&#30340;&#20559;&#35265;&#31243;&#24230;&#65292;&#20197;&#21450;&#36825;&#31181;&#20559;&#35265;&#23545;&#19981;&#21516;&#22522;&#20934;&#27979;&#35797;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34920;&#31034;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#31181;&#25552;&#31034;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2403.09963</link><description>&lt;p&gt;
&#22788;&#29702;&#22909;&#24744;&#30340;&#25552;&#31034;&#20559;&#35265;&#65281;&#35843;&#26597;&#21644;&#20943;&#36731;&#20107;&#23454;&#30693;&#35782;&#25552;&#21462;&#20013;&#30340;&#25552;&#31034;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Take Care of Your Prompt Bias! Investigating and Mitigating Prompt Bias in Factual Knowledge Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#30693;&#35782;&#25552;&#21462;&#20013;&#23384;&#22312;&#30340;&#8220;&#25552;&#31034;&#20559;&#35265;&#8221;&#65292;&#25214;&#21040;&#20102;&#19981;&#21516;&#31867;&#22411;&#25552;&#31034;&#30340;&#20559;&#35265;&#31243;&#24230;&#65292;&#20197;&#21450;&#36825;&#31181;&#20559;&#35265;&#23545;&#19981;&#21516;&#22522;&#20934;&#27979;&#35797;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34920;&#31034;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#31181;&#25552;&#31034;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#20107;&#23454;&#30693;&#35782;&#25552;&#21462;&#20013;&#23384;&#22312;&#8220;&#25552;&#31034;&#20559;&#35265;&#8221;&#65292;&#21363;&#25552;&#31034;&#24448;&#24448;&#20250;&#24341;&#20837;&#23545;&#29305;&#23450;&#26631;&#31614;&#30340;&#20559;&#35265;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#20869;&#37096;&#25552;&#31034;&#20559;&#35265;&#30340;&#31243;&#24230;&#21644;&#24433;&#21709;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#20026;&#20102;&#22238;&#24212;&#36825;&#19968;&#28857;&#65292;&#26412;&#25991;&#37327;&#21270;&#20102;&#19981;&#21516;&#31867;&#22411;&#25552;&#31034;&#30340;&#20559;&#35265;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#23545;&#19981;&#21516;&#22522;&#20934;&#27979;&#35797;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65306;1&#65289;&#23454;&#39564;&#20013;&#30340;&#25152;&#26377;&#25552;&#31034;&#37117;&#34920;&#29616;&#20986;&#19981;&#21487;&#24573;&#35270;&#30340;&#20559;&#35265;&#65292;&#22522;&#20110;&#26799;&#24230;&#30340;&#25552;&#31034;&#22914;AutoPrompt&#21644;OptiPrompt&#26174;&#31034;&#20986;&#26356;&#39640;&#27700;&#24179;&#30340;&#20559;&#35265;&#65307;2&#65289;&#25552;&#31034;&#20559;&#35265;&#21487;&#20197;&#36890;&#36807;&#36807;&#24230;&#25311;&#21512;&#27979;&#35797;&#25968;&#25454;&#38598;&#19981;&#21512;&#29702;&#22320;&#25918;&#22823;&#22522;&#20934;&#27979;&#35797;&#30340;&#20934;&#30830;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#31867;&#20284;LAMA&#36825;&#26679;&#30340;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19978;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34920;&#31034;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#25552;&#31034;&#20559;&#35265;&#65292;&#22312;&#25512;&#26029;&#26102;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#20165;&#25552;&#31034;&#26597;&#35810;&#26469;&#20272;&#35745;&#26377;&#20559;&#24046;&#30340;&#34920;&#31034;&#65292;&#28982;&#21518;&#20174;&#20013;&#21024;&#38500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09963v1 Announce Type: cross  Abstract: Recent research shows that pre-trained language models (PLMs) suffer from "prompt bias" in factual knowledge extraction, i.e., prompts tend to introduce biases toward specific labels. However, the extent and impact of prompt bias within the model remain underexplored. In response, this paper quantifies the bias with various types of prompts and assesses their impact on different benchmarks. We show that: 1) all prompts in the experiments exhibit non-negligible bias, with gradient-based prompts like AutoPrompt and OptiPrompt displaying significantly higher levels of bias; 2) prompt bias can amplify benchmark accuracy unreasonably by overfitting the test datasets, especially on imbalanced datasets like LAMA. Based on these findings, we propose a representation-based approach to mitigate the prompt bias during inference time. Specifically, we first estimate the biased representation using prompt-only querying, and then remove it from the 
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#29983;&#25104;&#24335;&#29992;&#25143;&#27169;&#25311;&#22120;&#22312;&#23545;&#35805;&#25512;&#33616;&#20013;&#23637;&#29616;&#20986;&#28508;&#21147;&#65292;&#26032;&#30340;&#21327;&#35758;&#36890;&#36807;&#20116;&#20010;&#20219;&#21153;&#35780;&#20272;&#20102;&#35821;&#35328;&#27169;&#22411;&#27169;&#25311;&#20154;&#31867;&#34892;&#20026;&#30340;&#20934;&#30830;&#31243;&#24230;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#19982;&#20154;&#31867;&#34892;&#20026;&#30340;&#20559;&#24046;&#65292;&#24182;&#25552;&#20986;&#20102;&#22914;&#20309;&#36890;&#36807;&#27169;&#22411;&#36873;&#25321;&#21644;&#25552;&#31034;&#31574;&#30053;&#20943;&#23569;&#36825;&#20123;&#20559;&#24046;&#12290;</title><link>https://arxiv.org/abs/2403.09738</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#23545;&#35805;&#25512;&#33616;&#20013;&#29983;&#25104;&#29992;&#25143;&#27169;&#25311;&#22120;
&lt;/p&gt;
&lt;p&gt;
Evaluating Large Language Models as Generative User Simulators for Conversational Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09738
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#29983;&#25104;&#24335;&#29992;&#25143;&#27169;&#25311;&#22120;&#22312;&#23545;&#35805;&#25512;&#33616;&#20013;&#23637;&#29616;&#20986;&#28508;&#21147;&#65292;&#26032;&#30340;&#21327;&#35758;&#36890;&#36807;&#20116;&#20010;&#20219;&#21153;&#35780;&#20272;&#20102;&#35821;&#35328;&#27169;&#22411;&#27169;&#25311;&#20154;&#31867;&#34892;&#20026;&#30340;&#20934;&#30830;&#31243;&#24230;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#19982;&#20154;&#31867;&#34892;&#20026;&#30340;&#20559;&#24046;&#65292;&#24182;&#25552;&#20986;&#20102;&#22914;&#20309;&#36890;&#36807;&#27169;&#22411;&#36873;&#25321;&#21644;&#25552;&#31034;&#31574;&#30053;&#20943;&#23569;&#36825;&#20123;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#29992;&#25143;&#26159;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#35780;&#20272;&#20013;&#25104;&#26412;&#25928;&#30410;&#36739;&#39640;&#30340;&#30495;&#23454;&#29992;&#25143;&#20195;&#29702;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#22312;&#27169;&#25311;&#31867;&#20284;&#20154;&#31867;&#34892;&#20026;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#36825;&#24341;&#21457;&#20102;&#23427;&#20204;&#33021;&#21542;&#20195;&#34920;&#22810;&#26679;&#21270;&#29992;&#25143;&#32676;&#20307;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#21327;&#35758;&#65292;&#29992;&#20110;&#34913;&#37327;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#27169;&#25311;&#23545;&#35805;&#25512;&#33616;&#20013;&#20154;&#31867;&#34892;&#20026;&#30340;&#31243;&#24230;&#12290;&#35813;&#21327;&#35758;&#30001;&#20116;&#20010;&#20219;&#21153;&#32452;&#25104;&#65292;&#27599;&#20010;&#20219;&#21153;&#26088;&#22312;&#35780;&#20272;&#21512;&#25104;&#29992;&#25143;&#24212;&#35813;&#34920;&#29616;&#20986;&#30340;&#20851;&#38190;&#29305;&#24615;&#65306;&#36873;&#25321;&#35201;&#35848;&#35770;&#30340;&#29289;&#21697;&#65292;&#34920;&#36798;&#20108;&#36827;&#21046;&#20559;&#22909;&#65292;&#34920;&#36798;&#24320;&#25918;&#24335;&#20559;&#22909;&#65292;&#35831;&#27714;&#25512;&#33616;&#20197;&#21450;&#25552;&#20379;&#21453;&#39304;&#12290;&#36890;&#36807;&#23545;&#22522;&#20934;&#27169;&#25311;&#22120;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#20219;&#21153;&#26377;&#25928;&#22320;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#34892;&#20026;&#30340;&#20559;&#24046;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#22914;&#20309;&#36890;&#36807;&#27169;&#22411;&#36873;&#25321;&#21644;&#25552;&#31034;&#31574;&#30053;&#20943;&#23569;&#36825;&#20123;&#20559;&#24046;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09738v1 Announce Type: cross  Abstract: Synthetic users are cost-effective proxies for real users in the evaluation of conversational recommender systems. Large language models show promise in simulating human-like behavior, raising the question of their ability to represent a diverse population of users. We introduce a new protocol to measure the degree to which language models can accurately emulate human behavior in conversational recommendation. This protocol is comprised of five tasks, each designed to evaluate a key property that a synthetic user should exhibit: choosing which items to talk about, expressing binary preferences, expressing open-ended preferences, requesting recommendations, and giving feedback. Through evaluation of baseline simulators, we demonstrate these tasks effectively reveal deviations of language models from human behavior, and offer insights on how to reduce the deviations with model selection and prompting strategies.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31616;&#21333;&#21644;&#21487;&#25193;&#23637;&#30340;&#23398;&#20064;&#29575;&#35843;&#25972;&#12289;&#37325;&#25918;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#25345;&#32493;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#21305;&#37197;&#23436;&#20840;&#37325;&#26032;&#35757;&#32451;&#26102;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.08763</link><description>&lt;p&gt;
&#25345;&#32493;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#21487;&#25193;&#23637;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Simple and Scalable Strategies to Continually Pre-train Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08763
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31616;&#21333;&#21644;&#21487;&#25193;&#23637;&#30340;&#23398;&#20064;&#29575;&#35843;&#25972;&#12289;&#37325;&#25918;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#25345;&#32493;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#21305;&#37197;&#23436;&#20840;&#37325;&#26032;&#35757;&#32451;&#26102;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#22312;&#25968;&#21313;&#20159;&#30340;&#26631;&#35760;&#19978;&#36827;&#34892;&#24120;&#35268;&#39044;&#35757;&#32451;&#65292;&#19968;&#26086;&#26377;&#26032;&#25968;&#25454;&#21487;&#29992;&#23601;&#37325;&#26032;&#24320;&#22987;&#35813;&#36807;&#31243;&#12290;&#19968;&#20010;&#26356;&#26377;&#25928;&#29575;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#25345;&#32493;&#39044;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#65292;&#19982;&#37325;&#26032;&#35757;&#32451;&#30456;&#27604;&#33021;&#33410;&#30465;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#28982;&#32780;&#65292;&#26032;&#25968;&#25454;&#24341;&#36215;&#30340;&#20998;&#24067;&#36716;&#31227;&#36890;&#24120;&#20250;&#23548;&#33268;&#22312;&#20197;&#21069;&#25968;&#25454;&#19978;&#38477;&#20302;&#24615;&#33021;&#25110;&#26080;&#27861;&#36866;&#24212;&#26032;&#25968;&#25454;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#21487;&#25193;&#23637;&#30340;&#23398;&#20064;&#29575;&#65288;LR&#65289;&#37325;&#26032;&#21319;&#28201;&#12289;LR&#37325;&#26032;&#34928;&#20943;&#21644;&#37325;&#25918;&#19978;&#19968;&#25968;&#25454;&#30340;&#32452;&#21512;&#36275;&#20197;&#19982;&#23436;&#20840;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#22312;&#25152;&#26377;&#21487;&#29992;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#65292;&#20174;&#26368;&#32456;&#25439;&#22833;&#21644;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#35780;&#20272;&#22522;&#20934;&#30340;&#35282;&#24230;&#34913;&#37327;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20004;&#20010;&#24120;&#29992;&#30340;LLM&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#65288;&#33521;&#35821;&#8594;&#33521;&#35821;&#65289;&#20043;&#38388;&#30340;&#24369;&#20294;&#29616;&#23454;&#30340;&#20998;&#24067;&#36716;&#31227;&#20197;&#21450;&#26356;&#24378;&#28872;&#30340;&#20998;&#24067;&#36716;&#31227;&#65288;&#33521;&#35821;&#8594;&#24503;&#35821;&#65289;&#19979;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08763v1 Announce Type: cross  Abstract: Large language models (LLMs) are routinely pre-trained on billions of tokens, only to start the process over again once new data becomes available. A much more efficient solution is to continually pre-train these models, saving significant compute compared to re-training. However, the distribution shift induced by new data typically results in degraded performance on previous data or poor adaptation to the new data. In this work, we show that a simple and scalable combination of learning rate (LR) re-warming, LR re-decaying, and replay of previous data is sufficient to match the performance of fully re-training from scratch on all available data, as measured by final loss and language model (LM) evaluation benchmarks. Specifically, we show this for a weak but realistic distribution shift between two commonly used LLM pre-training datasets (English$\rightarrow$English) and a stronger distribution shift (English$\rightarrow$German) at th
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#34701;&#21512;&#39640;&#24230;&#19987;&#19994;&#21270;&#30340;&#35821;&#35328;&#12289;&#20195;&#30721;&#21644;&#25968;&#23398;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UltraFuser&#30340;&#34701;&#21512;&#26694;&#26550;&#65292;&#24341;&#20837;&#20102;&#26631;&#35760;&#32423;&#21035;&#30340;&#38376;&#25511;&#26426;&#21046;&#65292;&#24182;&#35774;&#35745;&#20102;&#20004;&#38454;&#27573;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#21516;&#26102;&#22312;&#19977;&#20010;&#39046;&#22495;&#21462;&#24471;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.08281</link><description>&lt;p&gt;
&#36890;&#36807;&#34701;&#21512;&#39640;&#24230;&#19987;&#19994;&#21270;&#35821;&#35328;&#27169;&#22411;&#21516;&#26102;&#25484;&#25569;&#25991;&#26412;&#12289;&#20195;&#30721;&#21644;&#25968;&#23398;
&lt;/p&gt;
&lt;p&gt;
Mastering Text, Code and Math Simultaneously via Fusing Highly Specialized Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08281
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#34701;&#21512;&#39640;&#24230;&#19987;&#19994;&#21270;&#30340;&#35821;&#35328;&#12289;&#20195;&#30721;&#21644;&#25968;&#23398;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UltraFuser&#30340;&#34701;&#21512;&#26694;&#26550;&#65292;&#24341;&#20837;&#20102;&#26631;&#35760;&#32423;&#21035;&#30340;&#38376;&#25511;&#26426;&#21046;&#65292;&#24182;&#35774;&#35745;&#20102;&#20004;&#38454;&#27573;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#21516;&#26102;&#22312;&#19977;&#20010;&#39046;&#22495;&#21462;&#24471;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#12289;&#32534;&#31243;&#20195;&#30721;&#21644;&#25968;&#23398;&#31526;&#21495;&#30340;&#24213;&#23618;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#24040;&#22823;&#65292;&#23545;&#20110;&#37027;&#20123;&#21162;&#21147;&#21516;&#26102;&#22312;&#19977;&#20010;&#39046;&#22495;&#23454;&#29616;&#39640;&#24615;&#33021;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20986;&#20102;&#22797;&#26434;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#34701;&#21512;&#24050;&#32463;&#39640;&#24230;&#19987;&#19994;&#21270;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#34701;&#21512;&#26694;&#26550;UltraFuser&#21253;&#25324;&#19977;&#20010;&#24050;&#32463;&#22312;&#35821;&#35328;&#12289;&#32534;&#30721;&#21644;&#25968;&#23398;&#19978;&#24471;&#21040;&#20805;&#20998;&#35757;&#32451;&#30340;&#19987;&#23478;&#12290;&#24341;&#20837;&#20102;&#19968;&#20010;&#26631;&#35760;&#32423;&#21035;&#30340;&#38376;&#25511;&#26426;&#21046;&#26469;&#28151;&#21512;&#19987;&#23478;&#30340;&#36755;&#20986;&#12290;&#35774;&#35745;&#20102;&#19968;&#20010;&#20276;&#38543;&#24179;&#34913;&#37319;&#26679;&#30340;&#20004;&#38454;&#27573;&#35757;&#32451;&#31574;&#30053;&#20197;&#30830;&#20445;&#31283;&#23450;&#24615;&#12290;&#20026;&#20102;&#26377;&#25928;&#35757;&#32451;&#34701;&#21512;&#27169;&#22411;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#26500;&#24314;&#20102;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08281v1 Announce Type: cross  Abstract: Underlying data distributions of natural language, programming code, and mathematical symbols vary vastly, presenting a complex challenge for large language models (LLMs) that strive to achieve high performance across all three domains simultaneously. Achieving a very high level of proficiency for an LLM within a specific domain often requires extensive training with relevant corpora, which is typically accompanied by a sacrifice in performance in other domains. In this paper, we propose to fuse models that are already highly-specialized directly. The proposed fusing framework, UltraFuser, consists of three distinct specialists that are already sufficiently trained on language, coding, and mathematics. A token-level gating mechanism is introduced to blend the specialists' outputs. A two-stage training strategy accompanied by balanced sampling is designed to ensure stability. To effectively train the fused model, we further construct a 
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21019;&#24314;&#30340;&#31038;&#21306;&#22522;&#30784;&#38544;&#24335;&#25915;&#20987;&#24615;&#35821;&#35328;&#25968;&#25454;&#38598;OffLanDat&#65292;&#20026;38&#20010;&#19981;&#21516;&#30446;&#26631;&#32676;&#20307;&#25552;&#20379;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2403.02472</link><description>&lt;p&gt;
OffLanDat&#65306;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#31038;&#21306;&#22522;&#30784;&#38544;&#24335;&#25915;&#20987;&#24615;&#35821;&#35328;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
OffLanDat: A Community Based Implicit Offensive Language Dataset Generated by Large Language Model Through Prompt Engineering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02472
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21019;&#24314;&#30340;&#31038;&#21306;&#22522;&#30784;&#38544;&#24335;&#25915;&#20987;&#24615;&#35821;&#35328;&#25968;&#25454;&#38598;OffLanDat&#65292;&#20026;38&#20010;&#19981;&#21516;&#30446;&#26631;&#32676;&#20307;&#25552;&#20379;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#19978;&#25915;&#20987;&#24615;&#35821;&#35328;&#30340;&#26222;&#36941;&#23384;&#22312;&#23545;&#31038;&#20250;&#31119;&#31049;&#20135;&#29983;&#20102;&#19981;&#33391;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#39640;&#24230;&#37325;&#35270;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#25915;&#20987;&#24615;&#35821;&#35328;&#26082;&#23384;&#22312;&#26126;&#30830;&#24418;&#24335;&#65292;&#20063;&#23384;&#22312;&#38544;&#24335;&#24418;&#24335;&#65292;&#21518;&#32773;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#24403;&#21069;&#22312;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#36935;&#21040;&#20960;&#20010;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#29616;&#26377;&#25968;&#25454;&#38598;&#20027;&#35201;&#20381;&#36182;&#20110;&#25910;&#38598;&#21253;&#21547;&#26126;&#30830;&#25915;&#20987;&#24615;&#20851;&#38190;&#35789;&#30340;&#25991;&#26412;&#65292;&#36825;&#20351;&#24471;&#25429;&#25417;&#19981;&#21253;&#21547;&#36825;&#20123;&#20851;&#38190;&#35789;&#19988;&#38544;&#21547;&#25915;&#20987;&#24615;&#20869;&#23481;&#30340;&#20219;&#21153;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20854;&#27425;&#65292;&#36890;&#24120;&#30340;&#26041;&#27861;&#35770;&#20542;&#21521;&#20110;&#20165;&#20851;&#27880;&#25991;&#26412;&#20998;&#26512;&#65292;&#24573;&#35270;&#31038;&#21306;&#20449;&#24687;&#21487;&#20197;&#25552;&#20379;&#30340;&#23453;&#36149;&#35265;&#35299;&#12290;&#22312;&#36825;&#31687;&#30740;&#31350;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;OffLanDat&#65292;&#36825;&#26159;&#30001;ChatGPT&#29983;&#25104;&#30340;&#22522;&#20110;&#31038;&#21306;&#30340;&#38544;&#24335;&#25915;&#20987;&#24615;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;38&#20010;&#19981;&#21516;&#30446;&#26631;&#32676;&#20307;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02472v1 Announce Type: new  Abstract: The widespread presence of offensive languages on social media has resulted in adverse effects on societal well-being. As a result, it has become very important to address this issue with high priority. Offensive languages exist in both explicit and implicit forms, with the latter being more challenging to detect. Current research in this domain encounters several challenges. Firstly, the existing datasets primarily rely on the collection of texts containing explicit offensive keywords, making it challenging to capture implicitly offensive contents that are devoid of these keywords. Secondly, usual methodologies tend to focus solely on textual analysis, neglecting the valuable insights that community information can provide. In this research paper, we introduce a novel dataset OffLanDat, a community based implicit offensive language dataset generated by ChatGPT containing data for 38 different target groups. Despite limitations in genera
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#21644;&#20027;&#24352;&#25552;&#21462;&#30340;&#25688;&#35201;&#21487;&#20449;&#24230;&#35780;&#20272;&#25351;&#26631; FENICE&#65292;&#35299;&#20915;&#20102;&#33258;&#21160;&#29983;&#25104;&#25688;&#35201;&#20013;&#23384;&#22312;&#30340;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.02270</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#21644;&#20027;&#24352;&#25552;&#21462;&#30340;&#25688;&#35201;&#21487;&#20449;&#24230;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
FENICE: Factuality Evaluation of summarization based on Natural language Inference and Claim Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02270
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#21644;&#20027;&#24352;&#25552;&#21462;&#30340;&#25688;&#35201;&#21487;&#20449;&#24230;&#35780;&#20272;&#25351;&#26631; FENICE&#65292;&#35299;&#20915;&#20102;&#33258;&#21160;&#29983;&#25104;&#25688;&#35201;&#20013;&#23384;&#22312;&#30340;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#25991;&#26412;&#25688;&#35201;&#26041;&#38754;&#21462;&#24471;&#30340;&#36827;&#23637;&#65292;&#23588;&#20854;&#26159;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65292;&#24050;&#32463;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#26174;&#33879;&#30340;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;&#65292;&#21363;&#22823;&#37327;&#33258;&#21160;&#29983;&#25104;&#30340;&#25688;&#35201;&#21576;&#29616;&#20107;&#23454;&#19981;&#19968;&#33268;&#65292;&#27604;&#22914;&#24187;&#35273;&#12290;&#38024;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#20986;&#29616;&#20102;&#21508;&#31181;&#29992;&#20110;&#35780;&#20272;&#25688;&#35201;&#19968;&#33268;&#24615;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26032;&#24341;&#20837;&#30340;&#24230;&#37327;&#26631;&#20934;&#38754;&#20020;&#30528;&#19968;&#20123;&#38480;&#21046;&#65292;&#21253;&#25324;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#19987;&#27880;&#20110;&#30701;&#25991;&#26723;&#25688;&#35201;&#65288;&#20363;&#22914;&#26032;&#38395;&#25991;&#31456;&#65289;&#20197;&#21450;&#35745;&#31639;&#19978;&#30340;&#19981;&#21487;&#34892;&#24615;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#22522;&#20110;LLM&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#21644;&#20027;&#24352;&#25552;&#21462;&#30340;&#25688;&#35201;&#21487;&#20449;&#24230;&#35780;&#20272;&#65288;FENICE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26356;&#20855;&#35299;&#37322;&#24615;&#21644;&#26377;&#25928;&#24615;&#30340;&#21487;&#20449;&#24230;&#23548;&#21521;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02270v1 Announce Type: new  Abstract: Recent advancements in text summarization, particularly with the advent of Large Language Models (LLMs), have shown remarkable performance. However, a notable challenge persists as a substantial number of automatically-generated summaries exhibit factual inconsistencies, such as hallucinations. In response to this issue, various approaches for the evaluation of consistency for summarization have emerged. Yet, these newly-introduced metrics face several limitations, including lack of interpretability, focus on short document summaries (e.g., news articles), and computational impracticality, especially for LLM-based metrics. To address these shortcomings, we propose Factuality Evaluation of summarization based on Natural language Inference and Claim Extraction (FENICE), a more interpretable and efficient factuality-oriented metric. FENICE leverages an NLI-based alignment between information in the source document and a set of atomic facts,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#33041;&#26426;&#25509;&#21475;&#39046;&#22495;&#25506;&#32034;&#20102;MEG&#20449;&#21495;&#30340;&#33041;&#21040;&#25991;&#26412;&#36716;&#25442;&#65292;&#30528;&#37325;&#35299;&#20915;&#20102;&#20197;&#21069;&#20027;&#35201;&#38598;&#20013;&#22312;EEG&#19978;&#12289;&#20351;&#29992;&#8220;teacher-forcing&#8221;&#20197;&#21450;&#26410;&#23436;&#20840;&#33258;&#22238;&#24402;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.01748</link><description>&lt;p&gt;
&#23558;&#31070;&#32463;&#20449;&#21495;&#35299;&#30721;&#20026;&#35821;&#38899;
&lt;/p&gt;
&lt;p&gt;
Decode Neural signal as Speech
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#33041;&#26426;&#25509;&#21475;&#39046;&#22495;&#25506;&#32034;&#20102;MEG&#20449;&#21495;&#30340;&#33041;&#21040;&#25991;&#26412;&#36716;&#25442;&#65292;&#30528;&#37325;&#35299;&#20915;&#20102;&#20197;&#21069;&#20027;&#35201;&#38598;&#20013;&#22312;EEG&#19978;&#12289;&#20351;&#29992;&#8220;teacher-forcing&#8221;&#20197;&#21450;&#26410;&#23436;&#20840;&#33258;&#22238;&#24402;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#33041;&#21160;&#24577;&#35299;&#30721;&#35821;&#35328;&#26159;&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#39046;&#22495;&#20013;&#19968;&#20010;&#37325;&#35201;&#30340;&#24320;&#25918;&#26041;&#21521;&#65292;&#23588;&#20854;&#32771;&#34385;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#22686;&#38271;&#12290;&#30456;&#23545;&#20110;&#38656;&#35201;&#30005;&#26497;&#26893;&#20837;&#25163;&#26415;&#30340;&#20405;&#20837;&#24615;&#20449;&#21495;&#65292;&#38750;&#20405;&#20837;&#24615;&#31070;&#32463;&#20449;&#21495;&#65288;&#22914;EEG&#12289;MEG&#65289;&#30001;&#20110;&#20854;&#23433;&#20840;&#24615;&#21644;&#26222;&#36866;&#24615;&#32780;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22312;&#19977;&#20010;&#26041;&#38754;&#30340;&#25506;&#32034;&#36824;&#19981;&#36275;&#65306;1&#65289;&#20197;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;EEG&#19978;&#65292;&#20294;&#27809;&#26377;&#19968;&#20010;&#20808;&#21069;&#30340;&#30740;&#31350;&#35299;&#20915;&#20102;MEG&#20449;&#21495;&#36136;&#37327;&#26356;&#22909;&#30340;&#38382;&#39064;&#65307;2&#65289;&#20197;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#22312;&#29983;&#25104;&#35299;&#30721;&#36807;&#31243;&#20013;&#20351;&#29992;&#8220;teacher-forcing&#8221;&#65292;&#36825;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#65307;3&#65289;&#20197;&#21069;&#30340;&#24037;&#20316;&#22823;&#22810;&#26159;&#22522;&#20110;&#8220;BART&#8221;&#32780;&#19981;&#26159;&#23436;&#20840;&#33258;&#22238;&#24402;&#30340;&#65292;&#32780;&#22312;&#20854;&#20182;&#24207;&#21015;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;MEG&#20449;&#21495;&#30340;&#33041;&#21040;&#25991;&#26412;&#36716;&#25442;&#22312;&#35821;&#38899;&#35299;&#30721;&#24418;&#24335;&#20013;&#12290;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#22312;&#20132;&#21449;&#27880;&#24847;&#21147;&#20013;&#30740;&#31350;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01748v1 Announce Type: cross  Abstract: Decoding language from brain dynamics is an important open direction in the realm of brain-computer interface (BCI), especially considering the rapid growth of large language models. Compared to invasive-based signals which require electrode implantation surgery, non-invasive neural signals (e.g. EEG, MEG) have attracted increasing attention considering their safety and generality. However, the exploration is not adequate in three aspects: 1) previous methods mainly focus on EEG but none of the previous works address this problem on MEG with better signal quality; 2) prior works have predominantly used ``teacher-forcing" during generative decoding, which is impractical; 3) prior works are mostly ``BART-based" not fully auto-regressive, which performs better in other sequence tasks. In this paper, we explore the brain-to-text translation of MEG signals in a speech-decoding formation. Here we are the first to investigate a cross-attentio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;Twitter&#25968;&#25454;&#30340;&#26694;&#26550;LocalHealth&#65292;&#29992;&#20110;&#39044;&#27979;&#24403;&#22320;&#31934;&#31070;&#20581;&#24247;&#32467;&#26524;&#12290;&#36890;&#36807;&#19982;GPT3.5&#32467;&#21512;&#20351;&#29992;&#65292;&#35813;&#26694;&#26550;&#22312;MH&#30417;&#27979;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.13452</link><description>&lt;p&gt;
&#22522;&#20110;Twitter&#25968;&#25454;&#30340;&#31934;&#31070;&#20581;&#24247;&#30417;&#27979;&#26694;&#26550;&#65306;&#20174;&#24403;&#22320;&#25512;&#25991;&#21040;&#24403;&#22320;&#20581;&#24247;
&lt;/p&gt;
&lt;p&gt;
LocalTweets to LocalHealth: A Mental Health Surveillance Framework Based on Twitter Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;Twitter&#25968;&#25454;&#30340;&#26694;&#26550;LocalHealth&#65292;&#29992;&#20110;&#39044;&#27979;&#24403;&#22320;&#31934;&#31070;&#20581;&#24247;&#32467;&#26524;&#12290;&#36890;&#36807;&#19982;GPT3.5&#32467;&#21512;&#20351;&#29992;&#65292;&#35813;&#26694;&#26550;&#22312;MH&#30417;&#27979;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#20851;&#20110;Twitter&#25968;&#25454;&#30340;&#30740;&#31350;&#24050;&#32463;&#25552;&#20379;&#20102;&#23427;&#22312;&#24320;&#21457;&#34917;&#20805;&#20581;&#24247;&#30417;&#27979;&#31995;&#32479;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#35777;&#25454;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#26469;&#30417;&#27979;&#20844;&#20849;&#20581;&#24247;&#65292;&#37325;&#28857;&#20851;&#27880;&#31934;&#31070;&#20581;&#24247;&#65288;MH&#65289;&#32467;&#26524;&#12290;&#25105;&#20204;&#20551;&#35774;&#24403;&#22320;&#21457;&#24067;&#30340;&#25512;&#25991;&#21487;&#20197;&#34920;&#26126;&#24403;&#22320;&#30340;&#31934;&#31070;&#20581;&#24247;&#32467;&#26524;&#65292;&#24182;&#25910;&#38598;&#20102;&#26469;&#33258;&#32654;&#22269;765&#20010;&#22320;&#21306;&#65288;&#20154;&#21475;&#26222;&#26597;&#20998;&#32452;&#65289;&#30340;&#25512;&#25991;&#12290;&#25105;&#20204;&#23558;&#27599;&#20010;&#22320;&#21306;&#30340;&#36825;&#20123;&#25512;&#25991;&#19982;&#30142;&#30149;&#25511;&#21046;&#20013;&#24515;&#65288;CDC&#65289;&#25253;&#21578;&#30340;&#30456;&#24212;MH&#32467;&#26524;&#37197;&#23545;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;LocalTweets&#12290;&#20511;&#21161;LocalTweets&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;Twitter&#30340;MH&#30417;&#27979;&#31995;&#32479;&#30340;&#39318;&#20010;&#20154;&#21475;&#32423;&#35780;&#20272;&#20219;&#21153;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#39640;&#25928;&#26377;&#25928;&#30340;&#26041;&#27861;LocalHealth&#65292;&#29992;&#20110;&#26681;&#25454;LocalTweets&#39044;&#27979;MH&#32467;&#26524;&#12290;&#24403;&#19982;GPT3.5&#19968;&#36215;&#20351;&#29992;&#26102;&#65292;LocalHealth&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;F1&#20540;&#21644;&#20934;&#30830;&#29575;&#65292;&#20998;&#21035;&#36798;&#21040;0.7429&#21644;79.78\%&#65292;F1&#20540;&#25552;&#39640;&#20102;59\%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13452v1 Announce Type: cross  Abstract: Prior research on Twitter (now X) data has provided positive evidence of its utility in developing supplementary health surveillance systems. In this study, we present a new framework to surveil public health, focusing on mental health (MH) outcomes. We hypothesize that locally posted tweets are indicative of local MH outcomes and collect tweets posted from 765 neighborhoods (census block groups) in the USA. We pair these tweets from each neighborhood with the corresponding MH outcome reported by the Center for Disease Control (CDC) to create a benchmark dataset, LocalTweets. With LocalTweets, we present the first population-level evaluation task for Twitter-based MH surveillance systems. We then develop an efficient and effective method, LocalHealth, for predicting MH outcomes based on LocalTweets. When used with GPT3.5, LocalHealth achieves the highest F1-score and accuracy of 0.7429 and 79.78\%, respectively, a 59\% improvement in F
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#28145;&#20837;&#20998;&#26512;&#20102;&#25991;&#26412;&#21040;SQL&#39046;&#22495;&#20013;&#30340;&#22122;&#22768;&#23545;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#22312;BIRD-Bench&#22522;&#20934;&#27979;&#35797;&#20013;&#23384;&#22312;&#22823;&#37327;&#38382;&#39064;&#21644;&#26631;&#20934;&#26597;&#35810;&#20013;&#30340;&#22122;&#22768;&#65292;&#36825;&#20250;&#26174;&#33879;&#24433;&#21709;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12243</link><description>&lt;p&gt;
&#29702;&#35299;&#25991;&#26412;&#21040;SQL&#20013;&#22122;&#22768;&#30340;&#24433;&#21709;&#65306;&#23545;BIRD-Bench&#22522;&#20934;&#27979;&#35797;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Understanding the Effects of Noise in Text-to-SQL: An Examination of the BIRD-Bench Benchmark
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12243
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#28145;&#20837;&#20998;&#26512;&#20102;&#25991;&#26412;&#21040;SQL&#39046;&#22495;&#20013;&#30340;&#22122;&#22768;&#23545;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#22312;BIRD-Bench&#22522;&#20934;&#27979;&#35797;&#20013;&#23384;&#22312;&#22823;&#37327;&#38382;&#39064;&#21644;&#26631;&#20934;&#26597;&#35810;&#20013;&#30340;&#22122;&#22768;&#65292;&#36825;&#20250;&#26174;&#33879;&#24433;&#21709;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Text-to-SQL&#28041;&#21450;&#23558;&#33258;&#28982;&#35821;&#35328;&#32763;&#35793;&#20026;&#32467;&#26500;&#21270;&#26597;&#35810;&#35821;&#35328;&#65288;SQL&#65289;&#65292;&#23545;&#20110;&#20351;&#32467;&#26500;&#21270;&#25968;&#25454;&#24211;&#21487;&#20197;&#22312;&#27809;&#26377;&#19987;&#19994;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#24471;&#21040;&#24191;&#27867;&#35775;&#38382;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#38024;&#23545;&#36825;&#20123;&#20219;&#21153;&#30340;&#27169;&#22411;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#21407;&#22240;&#21253;&#25324;&#23384;&#22312;&#8220;&#22122;&#22768;&#8221;&#65292;&#22914;&#27169;&#31946;&#38382;&#39064;&#21644;&#35821;&#27861;&#38169;&#35823;&#12290;&#35813;&#30740;&#31350;&#23545;&#24191;&#27867;&#20351;&#29992;&#30340;BIRD-Bench&#22522;&#20934;&#27979;&#35797;&#20013;&#22122;&#22768;&#30340;&#20998;&#24067;&#21644;&#31867;&#22411;&#20197;&#21450;&#22122;&#22768;&#23545;&#27169;&#22411;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#34429;&#28982;BIRD-Bench&#26088;&#22312;&#27169;&#25311;&#33039;&#20081;&#21644;&#22024;&#26434;&#30340;&#25968;&#25454;&#24211;&#20540;&#65292;&#20294;&#24182;&#26410;&#21253;&#21547;&#38382;&#39064;&#21644;&#26631;&#20934;&#26597;&#35810;&#20013;&#30340;&#22122;&#22768;&#21644;&#38169;&#35823;&#12290;&#25105;&#20204;&#21457;&#29616;&#25968;&#25454;&#38598;&#20013;&#38382;&#39064;&#21644;&#26631;&#20934;&#26597;&#35810;&#20013;&#30340;&#22122;&#22768;&#26222;&#36941;&#23384;&#22312;&#65292;&#36328;&#39046;&#22495;&#23384;&#22312;&#19981;&#21516;&#31243;&#24230;&#30340;&#22122;&#22768;&#65292;&#24182;&#19988;&#22122;&#22768;&#31867;&#22411;&#20043;&#38388;&#20998;&#24067;&#19981;&#22343;&#21248;&#12290;&#23384;&#22312;&#19981;&#27491;&#30830;&#30340;&#26631;&#20934;SQL&#26597;&#35810;&#65292;&#36827;&#32780;&#29983;&#25104;&#19981;&#27491;&#30830;&#30340;&#26631;&#20934;&#31572;&#26696;&#65292;&#23545;&#22522;&#20934;&#27979;&#35797;&#30340;&#24433;&#21709;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12243v1 Announce Type: new  Abstract: Text-to-SQL, which involves translating natural language into Structured Query Language (SQL), is crucial for enabling broad access to structured databases without expert knowledge. However, designing models for such tasks is challenging due to numerous factors, including the presence of 'noise,' such as ambiguous questions and syntactical errors. This study provides an in-depth analysis of the distribution and types of noise in the widely used BIRD-Bench benchmark and the impact of noise on models. While BIRD-Bench was created to model dirty and noisy database values, it was not created to contain noise and errors in the questions and gold queries. We found that noise in questions and gold queries are prevalent in the dataset, with varying amounts across domains, and with an uneven distribution between noise types. The presence of incorrect gold SQL queries, which then generate incorrect gold answers, has a significant impact on the ben
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#20116;&#20010;&#20027;&#35201;&#31867;&#21035;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;48&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#31995;&#32479;&#20998;&#26512;&#65292;&#30740;&#31350;&#20102;&#23427;&#20204;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#20123;&#8220;&#39640;&#24433;&#21709;&#25968;&#25454;&#8221;&#65292;&#22914;&#20070;&#31821;&#65292;&#19982;&#27169;&#22411;&#33021;&#21147;&#30456;&#20851;&#32852;&#65292;&#20026;LLMs&#30340;&#20248;&#21270;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.11537</link><description>&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#21435;&#23398;&#20064;&#30740;&#31350;&#39044;&#35757;&#32451;&#25968;&#25454;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Deciphering the lmpact of Pretraining Data on Large Language Models through Machine Unlearning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11537
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#20116;&#20010;&#20027;&#35201;&#31867;&#21035;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;48&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#31995;&#32479;&#20998;&#26512;&#65292;&#30740;&#31350;&#20102;&#23427;&#20204;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#20123;&#8220;&#39640;&#24433;&#21709;&#25968;&#25454;&#8221;&#65292;&#22914;&#20070;&#31821;&#65292;&#19982;&#27169;&#22411;&#33021;&#21147;&#30456;&#20851;&#32852;&#65292;&#20026;LLMs&#30340;&#20248;&#21270;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#20855;&#26377;&#21508;&#31181;&#26469;&#28304;&#30340;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#30340;&#27599;&#20010;&#32452;&#25104;&#37096;&#20998;&#30340;&#24433;&#21709;&#20173;&#28982;&#19981;&#26126;&#30830;&#12290;&#22240;&#27492;&#65292;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#30340;&#32452;&#32455;&#20173;&#28982;&#26159;&#32463;&#39564;&#24615;&#30340;&#65292;&#24182;&#19988;&#21487;&#33021;&#20559;&#31163;&#26368;&#20339;&#29366;&#24577;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#20998;&#26512;&#20102;&#26469;&#33258;LLMs&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;5&#20010;&#20027;&#35201;&#31867;&#21035;&#30340;48&#20010;&#25968;&#25454;&#38598;&#30340;&#24433;&#21709;&#65292;&#24182;&#20351;&#29992;&#20851;&#20110;&#20061;&#20010;&#20027;&#35201;&#27169;&#22411;&#33021;&#21147;&#31867;&#21035;&#30340;&#22522;&#20934;&#26469;&#34913;&#37327;&#23427;&#20204;&#23545;LLMs&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#20851;&#20110;&#22810;&#20010;&#35821;&#26009;&#24211;&#23545;LLMs&#24615;&#33021;&#36129;&#29486;&#30340;&#23454;&#35777;&#32467;&#26524;&#65292;&#20197;&#21450;&#23427;&#20204;&#30340;&#32852;&#21512;&#24433;&#21709;&#27169;&#24335;&#65292;&#21253;&#25324;&#20114;&#34917;&#30340;&#12289;&#27491;&#20132;&#30340;&#21644;&#30456;&#20851;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#36824;&#30830;&#23450;&#20102;&#19968;&#32452;&#8220;&#39640;&#24433;&#21709;&#25968;&#25454;&#8221;&#65292;&#22914;&#20070;&#31821;&#65292;&#19982;&#19968;&#32452;&#27169;&#22411;&#33021;&#21147;&#30456;&#20851;&#32852;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#32452;&#32455;&#25968;&#25454;&#20197;&#25903;&#25345;LLMs&#20248;&#21270;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11537v1 Announce Type: cross  Abstract: Through pretraining on a corpus with various sources, Large Language Models (LLMs) have gained impressive performance. However, the impact of each component of the pretraining corpus remains opaque. As a result, the organization of the pretraining corpus is still empirical and may deviate from the optimal. To address this issue, we systematically analyze the impact of 48 datasets from 5 major categories of pretraining data of LLMs and measure their impacts on LLMs using benchmarks about nine major categories of model capabilities. Our analyses provide empirical results about the contribution of multiple corpora on the performances of LLMs, along with their joint impact patterns, including complementary, orthogonal, and correlational relationships. We also identify a set of ``high-impact data'' such as Books that is significantly related to a set of model capabilities. These findings provide insights into the organization of data to sup
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#31216;&#20026;&#20018;&#32852;Transformer&#65292;&#29992;&#20110;&#35299;&#20915;&#20256;&#32479;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#36895;&#24230;&#38480;&#21046;&#30340;&#38382;&#39064;&#12290;&#35813;&#26550;&#26500;&#36890;&#36807;&#23558;&#23567;&#22411;&#33258;&#22238;&#24402;&#27169;&#22411;&#21644;&#22823;&#27169;&#22411;&#20197;&#22359;&#27169;&#24335;&#32467;&#21512;&#36215;&#26469;&#65292;&#24182;&#35753;&#23567;&#27169;&#22411;&#20851;&#27880;&#22823;&#27169;&#22411;&#30340;&#20016;&#23500;&#34920;&#31034;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#23567;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#65292;&#20018;&#32852;&#30340;PaLM2-Bison&#21644;PaLM2-Gecko&#30456;&#27604;&#29420;&#31435;&#30340;PaLM2-Gecko&#65292;&#22312;&#19979;&#19968;&#20010;&#35789;&#20803;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#25552;&#39640;&#20102;3.3%&#65292;&#24182;&#19988;&#30456;&#36739;&#20110;&#20855;&#26377;&#30456;&#20284;&#19979;&#28216;&#20219;&#21153;&#30340;PaLM2-Otter&#27169;&#22411;&#65292;&#21152;&#36895;&#27604;&#36798;&#21040;1.16&#20493;&#12290;</title><link>https://arxiv.org/abs/2402.08644</link><description>&lt;p&gt;
&#29992;&#20110;&#25512;&#26029;&#39640;&#25928;LLMs&#30340;&#20018;&#32852;Transformer
&lt;/p&gt;
&lt;p&gt;
Tandem Transformers for Inference Efficient LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08644
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#31216;&#20026;&#20018;&#32852;Transformer&#65292;&#29992;&#20110;&#35299;&#20915;&#20256;&#32479;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#36895;&#24230;&#38480;&#21046;&#30340;&#38382;&#39064;&#12290;&#35813;&#26550;&#26500;&#36890;&#36807;&#23558;&#23567;&#22411;&#33258;&#22238;&#24402;&#27169;&#22411;&#21644;&#22823;&#27169;&#22411;&#20197;&#22359;&#27169;&#24335;&#32467;&#21512;&#36215;&#26469;&#65292;&#24182;&#35753;&#23567;&#27169;&#22411;&#20851;&#27880;&#22823;&#27169;&#22411;&#30340;&#20016;&#23500;&#34920;&#31034;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#23567;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#65292;&#20018;&#32852;&#30340;PaLM2-Bison&#21644;PaLM2-Gecko&#30456;&#27604;&#29420;&#31435;&#30340;PaLM2-Gecko&#65292;&#22312;&#19979;&#19968;&#20010;&#35789;&#20803;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#25552;&#39640;&#20102;3.3%&#65292;&#24182;&#19988;&#30456;&#36739;&#20110;&#20855;&#26377;&#30456;&#20284;&#19979;&#28216;&#20219;&#21153;&#30340;PaLM2-Otter&#27169;&#22411;&#65292;&#21152;&#36895;&#27604;&#36798;&#21040;1.16&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;( LLMs )&#20855;&#26377;&#33258;&#22238;&#24402;&#30340;&#29305;&#24615;&#65292;&#36825;&#20351;&#24471;&#25512;&#26029;&#36895;&#24230;&#21463;&#21040;&#38480;&#21046;&#65292;&#22240;&#20026;&#35789;&#20803;&#26159;&#25353;&#39034;&#24207;&#29983;&#25104;&#30340;&#12290;&#23613;&#31649;&#26377;&#20123;&#39044;&#27979;&#21644;&#24182;&#34892;&#35299;&#30721;&#25216;&#26415;&#35797;&#22270;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#37117;&#26377;&#38480;&#21046;&#65306;&#35201;&#20040;&#20381;&#36182;&#26356;&#31934;&#31616;&#20294;&#20934;&#30830;&#24230;&#36739;&#20302;&#30340;&#27169;&#22411;&#36827;&#34892;&#29983;&#25104;&#65292;&#35201;&#20040;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#22522;&#30784;LLM&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#65292;&#21363;&#20018;&#32852;Transformer&#65292;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#36825;&#31181;&#26550;&#26500;&#29420;&#29305;&#22320;&#32467;&#21512;&#20102;(1)&#19968;&#20010;&#23567;&#22411;&#33258;&#22238;&#24402;&#27169;&#22411;&#21644;(2)&#19968;&#20010;&#20197;&#22359;&#27169;&#24335;&#36816;&#34892;&#30340;&#22823;&#27169;&#22411;(&#21516;&#26102;&#22788;&#29702;&#22810;&#20010;&#35789;&#20803;)&#12290;&#36890;&#36807;&#35753;&#23567;&#27169;&#22411;&#20851;&#27880;&#22823;&#27169;&#22411;&#26356;&#20016;&#23500;&#30340;&#34920;&#31034;&#65292;&#22823;&#24133;&#25552;&#21319;&#23567;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#22312;PaLM2&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#65292;PaLM2-Bison&#21644;PaLM2-Gecko&#30340;&#20018;&#32852;&#30456;&#36739;&#29420;&#31435;&#30340;PaLM2-Gecko&#65292;&#22312;&#19979;&#19968;&#20010;&#35789;&#20803;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#25552;&#21319;&#20102;3.3%&#65292;&#19982;&#20855;&#26377;&#30456;&#20284;&#19979;&#28216;&#20219;&#21153;&#30340;PaLM2-Otter&#27169;&#22411;&#30456;&#27604;&#65292;&#25552;&#20379;&#20102;1.16&#20493;&#30340;&#21152;&#36895;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
The autoregressive nature of conventional large language models (LLMs) inherently limits inference speed, as tokens are generated sequentially. While speculative and parallel decoding techniques attempt to mitigate this, they face limitations: either relying on less accurate smaller models for generation or failing to fully leverage the base LLM's representations.   We introduce a novel architecture, Tandem transformers, to address these issues. This architecture uniquely combines (1) a small autoregressive model and (2) a large model operating in block mode (processing multiple tokens simultaneously). The small model's predictive accuracy is substantially enhanced by granting it attention to the large model's richer representations. On the PaLM2 pretraining dataset, a tandem of PaLM2-Bison and PaLM2-Gecko demonstrates a 3.3% improvement in next-token prediction accuracy over a standalone PaLM2-Gecko, offering a 1.16x speedup compared to a PaLM2-Otter model with comparable downstream p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20869;&#22312;&#21160;&#26426;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#25913;&#36827;&#31471;&#21040;&#31471;&#22810;&#20219;&#21153;&#23545;&#35805;&#31995;&#32479;&#30340;&#35757;&#32451;&#21644;&#36866;&#24212;&#24615;&#12290;&#36890;&#36807;&#25945;&#25480;&#26234;&#33021;&#20307;&#19968;&#20010;&#20869;&#22312;&#22870;&#21169;&#31995;&#32479;&#65292;&#21487;&#20197;&#21152;&#36895;&#35757;&#32451;&#24182;&#25552;&#39640;&#20854;&#21028;&#26029;&#34892;&#20026;&#36136;&#37327;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2401.18040</link><description>&lt;p&gt;
&#21152;&#24378;&#31471;&#21040;&#31471;&#22810;&#20219;&#21153;&#23545;&#35805;&#31995;&#32479;&#65306;&#22522;&#20110;&#20869;&#22312;&#21160;&#26426;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#25913;&#36827;&#35757;&#32451;&#21644;&#36866;&#24212;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Enhancing End-to-End Multi-Task Dialogue Systems: A Study on Intrinsic Motivation Reinforcement Learning Algorithms for Improved Training and Adaptability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.18040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20869;&#22312;&#21160;&#26426;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#25913;&#36827;&#31471;&#21040;&#31471;&#22810;&#20219;&#21153;&#23545;&#35805;&#31995;&#32479;&#30340;&#35757;&#32451;&#21644;&#36866;&#24212;&#24615;&#12290;&#36890;&#36807;&#25945;&#25480;&#26234;&#33021;&#20307;&#19968;&#20010;&#20869;&#22312;&#22870;&#21169;&#31995;&#32479;&#65292;&#21487;&#20197;&#21152;&#36895;&#35757;&#32451;&#24182;&#25552;&#39640;&#20854;&#21028;&#26029;&#34892;&#20026;&#36136;&#37327;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31471;&#21040;&#31471;&#22810;&#20219;&#21153;&#23545;&#35805;&#31995;&#32479;&#36890;&#24120;&#36890;&#36807;&#23545;&#35805;&#27969;&#27700;&#32447;&#30340;&#29420;&#31435;&#27169;&#22359;&#36827;&#34892;&#35774;&#35745;&#12290;&#20854;&#20013;&#65292;&#31574;&#30053;&#27169;&#22359;&#26159;&#20915;&#23450;&#23545;&#29992;&#25143;&#36755;&#20837;&#22914;&#20309;&#21709;&#24212;&#30340;&#20851;&#38190;&#12290;&#36825;&#20010;&#31574;&#30053;&#26159;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#36890;&#36807;&#21033;&#29992;&#19968;&#20010;&#26234;&#33021;&#20307;&#22312;&#19968;&#20010;&#21453;&#39304;&#20449;&#21495;&#24418;&#24335;&#30340;&#29615;&#22659;&#20013;&#25509;&#25910;&#21453;&#39304;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#23545;&#35805;&#31995;&#32479;&#21482;&#25552;&#20379;&#20102;&#31232;&#32570;&#19988;&#31616;&#21333;&#30340;&#22870;&#21169;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#30740;&#31350;&#20869;&#22312;&#21160;&#26426;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#36890;&#36807;&#36825;&#31181;&#31639;&#27861;&#65292;&#26234;&#33021;&#20307;&#21487;&#20197;&#24555;&#36895;&#21152;&#36895;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#25945;&#25480;&#19968;&#20010;&#20869;&#22312;&#22870;&#21169;&#31995;&#32479;&#26469;&#25552;&#39640;&#21028;&#26029;&#20854;&#34892;&#20026;&#36136;&#37327;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#38543;&#26426;&#32593;&#32476;&#33976;&#39311;&#21644;&#22909;&#22855;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#24212;&#29992;&#20110;&#27979;&#37327;&#29366;&#24577;&#35775;&#38382;&#39057;&#29575;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#35805;&#35821;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#26469;&#40723;&#21169;&#25506;&#32034;&#12290;&#22312;&#19968;&#20010;&#24322;&#26500;&#25968;&#25454;&#38598;MultiWOZ&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;...
&lt;/p&gt;
&lt;p&gt;
End-to-end multi-task dialogue systems are usually designed with separate modules for the dialogue pipeline. Among these, the policy module is essential for deciding what to do in response to user input. This policy is trained by reinforcement learning algorithms by taking advantage of an environment in which an agent receives feedback in the form of a reward signal. The current dialogue systems, however, only provide meagre and simplistic rewards. Investigating intrinsic motivation reinforcement learning algorithms is the goal of this study. Through this, the agent can quickly accelerate training and improve its capacity to judge the quality of its actions by teaching it an internal incentive system. In particular, we adapt techniques for random network distillation and curiosity-driven reinforcement learning to measure the frequency of state visits and encourage exploration by using semantic similarity between utterances. Experimental results on MultiWOZ, a heterogeneous dataset, sho
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#21512;&#24182;&#29983;&#25104;&#21644;&#26816;&#32034;&#30340;&#19978;&#19979;&#25991;&#20197;&#25552;&#21319;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#20559;&#21521;&#20110;&#29983;&#25104;&#30340;&#19978;&#19979;&#25991;&#65292;&#21363;&#20351;&#23427;&#20204;&#25552;&#20379;&#20102;&#38169;&#35823;&#30340;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2401.11911</link><description>&lt;p&gt;
&#22914;&#20309;&#21512;&#24182;&#29983;&#25104;&#21644;&#26816;&#32034;&#19978;&#19979;&#25991;&#20197;&#22686;&#24378;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Blinded by Generated Contexts: How Language Models Merge Generated and Retrieved Contexts for Open-Domain QA?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11911
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#21512;&#24182;&#29983;&#25104;&#21644;&#26816;&#32034;&#30340;&#19978;&#19979;&#25991;&#20197;&#25552;&#21319;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#20559;&#21521;&#20110;&#29983;&#25104;&#30340;&#19978;&#19979;&#25991;&#65292;&#21363;&#20351;&#23427;&#20204;&#25552;&#20379;&#20102;&#38169;&#35823;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#36741;&#21161;&#20449;&#24687;&#24050;&#32463;&#25104;&#20026;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20851;&#38190;&#65292;&#20294;&#23545;&#20110;LLMs&#22914;&#20309;&#21512;&#24182;&#29983;&#25104;&#30340;&#21644;&#26816;&#32034;&#30340;&#19978;&#19979;&#25991;&#20173;&#30693;&#20043;&#29978;&#23569;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#31995;&#32479;&#24615;&#30340;&#26694;&#26550;&#26469;&#30830;&#23450;LLMs&#30340;&#21709;&#24212;&#26159;&#28304;&#33258;&#20110;&#29983;&#25104;&#30340;&#19978;&#19979;&#25991;&#36824;&#26159;&#26816;&#32034;&#30340;&#19978;&#19979;&#25991;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#21253;&#21547;&#30456;&#20114;&#20914;&#31361;&#30340;&#19978;&#19979;&#25991;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#27599;&#20010;&#38382;&#39064;&#37117;&#19982;&#29983;&#25104;&#30340;&#21644;&#26816;&#32034;&#30340;&#19978;&#19979;&#25991;&#37197;&#23545;&#65292;&#20294;&#21482;&#26377;&#19968;&#20010;&#19978;&#19979;&#25991;&#21253;&#21547;&#20102;&#27491;&#30830;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;LLMs&#65288;&#22914;GPT-4/3.5&#21644;Llama2&#65289;&#23384;&#22312;&#26174;&#33879;&#30340;&#20559;&#24046;&#65292;&#26356;&#20542;&#21521;&#20110;&#29983;&#25104;&#30340;&#19978;&#19979;&#25991;&#65292;&#21363;&#20351;&#36825;&#20123;&#19978;&#19979;&#25991;&#25552;&#20379;&#20102;&#38169;&#35823;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30830;&#23450;&#20102;&#23548;&#33268;&#36825;&#31181;&#20559;&#24046;&#30340;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#65306;i&#65289;LLMs&#29983;&#25104;&#30340;&#19978;&#19979;&#25991;&#36890;&#24120;&#19982;&#38382;&#39064;&#26356;&#30456;&#20284;&#65292;&#22686;&#21152;&#20102;&#20854;&#34987;&#36873;&#25321;&#30340;&#21487;&#33021;&#24615;&#65307;ii&#65289;&#26816;&#32034;&#19978;&#19979;&#25991;&#20013;&#20351;&#29992;&#30340;&#20998;&#21106;&#36807;&#31243;&#25171;&#26029;&#20102;&#20854;&#36830;&#36143;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While auxiliary information has become a key to enhance Large Language Models (LLMs), relatively little is known about how LLMs merge these contexts, specifically generated and retrieved. To study this, we formulate a systematic framework to identify whether LLMs' responses, derived from the integration of generated and retrieved contexts, are attributed to either generated or retrieved contexts. To achieve this, we construct datasets with conflicting contexts, where each question is paired with both generated and retrieved contexts, yet only one of them contains the correct answer. Our experiments reveal a significant bias in LLMs (GPT-4/3.5 and Llama2) towards generated contexts, even when they provide incorrect information. We further identify two key factors contributing to this bias: i) contexts generated by LLMs typically show greater similarity to the questions, increasing their likelihood of selection; ii) the segmentation process used in retrieved contexts disrupts their compl
&lt;/p&gt;</description></item><item><title>AI&#21644;&#29983;&#25104;&#24335;AI&#24037;&#20855;&#22312;&#30740;&#31350;&#21457;&#29616;&#21644;&#24635;&#32467;&#26041;&#38754;&#26377;&#37325;&#22823;&#24433;&#21709;&#65292;&#21253;&#25324;&#33021;&#22815;&#26356;&#24555;&#22320;&#25214;&#21040;&#30456;&#20851;&#25991;&#29486;&#21644;&#29992;&#31616;&#27905;&#35821;&#35328;&#24635;&#32467;&#30740;&#31350;&#25991;&#31456;&#30340;&#35201;&#28857;&#12290;</title><link>https://arxiv.org/abs/2401.06795</link><description>&lt;p&gt;
AI&#21644;&#29983;&#25104;&#24335;AI&#29992;&#20110;&#30740;&#31350;&#21457;&#29616;&#19982;&#24635;&#32467;
&lt;/p&gt;
&lt;p&gt;
AI and Generative AI for Research Discovery and Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.06795
&lt;/p&gt;
&lt;p&gt;
AI&#21644;&#29983;&#25104;&#24335;AI&#24037;&#20855;&#22312;&#30740;&#31350;&#21457;&#29616;&#21644;&#24635;&#32467;&#26041;&#38754;&#26377;&#37325;&#22823;&#24433;&#21709;&#65292;&#21253;&#25324;&#33021;&#22815;&#26356;&#24555;&#22320;&#25214;&#21040;&#30456;&#20851;&#25991;&#29486;&#21644;&#29992;&#31616;&#27905;&#35821;&#35328;&#24635;&#32467;&#30740;&#31350;&#25991;&#31456;&#30340;&#35201;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#21644;&#29983;&#25104;&#24335;AI&#24037;&#20855;&#65292;&#21253;&#25324;&#20381;&#36182;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#22914;ChatGPT&#65292;&#20170;&#24180;&#36805;&#36895;&#23835;&#36215;&#65292;&#20026;&#22686;&#21152;&#24037;&#20316;&#25928;&#29575;&#21644;&#25913;&#21892;&#29983;&#27963;&#21019;&#36896;&#20102;&#38590;&#20197;&#32622;&#20449;&#30340;&#26426;&#20250;&#12290;&#32479;&#35745;&#23398;&#23478;&#21644;&#25968;&#25454;&#31185;&#23398;&#23478;&#24050;&#32463;&#24320;&#22987;&#20197;&#22810;&#31181;&#26041;&#24335;&#20307;&#39564;&#21040;&#36825;&#20123;&#24037;&#20855;&#30340;&#22909;&#22788;&#65292;&#27604;&#22914;&#20174;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#32534;&#31243;&#20195;&#30721;&#20197;&#20998;&#26512;&#25968;&#25454;&#25110;&#25311;&#21512;&#32479;&#35745;&#27169;&#22411;&#12290;&#36825;&#20123;&#24037;&#20855;&#21487;&#20197;&#22312;&#30740;&#31350;&#21457;&#29616;&#21644;&#24635;&#32467;&#26041;&#38754;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#20043;&#19968;&#12290;&#27491;&#22312;&#24320;&#21457;&#29420;&#31435;&#24037;&#20855;&#21644;&#25554;&#20214;&#32473;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#27604;2023&#24180;&#20043;&#21069;&#30340;&#25628;&#32034;&#24037;&#20855;&#26356;&#24555;&#22320;&#25214;&#21040;&#30456;&#20851;&#25991;&#29486;&#12290;&#27492;&#22806;&#65292;&#29983;&#25104;&#24335;AI&#24037;&#20855;&#24050;&#32463;&#21457;&#23637;&#21040;&#21487;&#20197;&#29992;&#31616;&#27905;&#30340;&#35821;&#35328;&#24635;&#32467;&#21644;&#25552;&#21462;&#30740;&#31350;&#25991;&#31456;&#30340;&#35201;&#28857;&#30340;&#31243;&#24230;&#12290;&#26368;&#21518;&#65292;&#22522;&#20110;&#39640;&#24230;&#21442;&#25968;&#21270;&#30340;LLMs&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#21487;&#29992;&#20110;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.06795v2 Announce Type: replace-cross  Abstract: AI and generative AI tools, including chatbots like ChatGPT that rely on large language models (LLMs), have burst onto the scene this year, creating incredible opportunities to increase work productivity and improve our lives. Statisticians and data scientists have begun experiencing the benefits from the availability of these tools in numerous ways, such as the generation of programming code from text prompts to analyze data or fit statistical models. One area that these tools can make a substantial impact is in research discovery and summarization. Standalone tools and plugins to chatbots are being developed that allow researchers to more quickly find relevant literature than pre-2023 search tools. Furthermore, generative AI tools have improved to the point where they can summarize and extract the key points from research articles in succinct language. Finally, chatbots based on highly parameterized LLMs can be used to simula
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21322;&#32467;&#26500;&#21270;&#32593;&#32476;&#25991;&#31456;&#20013;&#23454;&#29616;&#39640;&#36890;&#37327;&#29983;&#29289;&#21307;&#23398;&#20851;&#31995;&#25552;&#21462;&#65292;&#36890;&#36807;&#23545;LLMs&#30340;&#24212;&#29992;&#65292;&#32467;&#21512;&#22806;&#37096;&#35821;&#26009;&#24211;&#21644;&#19990;&#30028;&#30693;&#35782;&#65292;&#35774;&#35745;&#38024;&#23545;&#24615;&#30340;&#20108;&#20803;&#20998;&#31867;&#20915;&#31574;&#65292;&#21462;&#24471;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2312.08274</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21322;&#32467;&#26500;&#21270;&#32593;&#32476;&#25991;&#31456;&#39640;&#36890;&#37327;&#29983;&#29289;&#21307;&#23398;&#20851;&#31995;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
High-throughput Biomedical Relation Extraction for Semi-Structured Web Articles Empowered by Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.08274
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21322;&#32467;&#26500;&#21270;&#32593;&#32476;&#25991;&#31456;&#20013;&#23454;&#29616;&#39640;&#36890;&#37327;&#29983;&#29289;&#21307;&#23398;&#20851;&#31995;&#25552;&#21462;&#65292;&#36890;&#36807;&#23545;LLMs&#30340;&#24212;&#29992;&#65292;&#32467;&#21512;&#22806;&#37096;&#35821;&#26009;&#24211;&#21644;&#19990;&#30028;&#30693;&#35782;&#65292;&#35774;&#35745;&#38024;&#23545;&#24615;&#30340;&#20108;&#20803;&#20998;&#31867;&#20915;&#31574;&#65292;&#21462;&#24471;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38405;&#35835;&#29702;&#35299;&#33021;&#21147;&#21644;&#29983;&#29289;&#21307;&#23398;&#19990;&#30028;&#30693;&#35782;&#20197;&#21487;&#25193;&#23637;&#21644;&#26377;&#20449;&#26381;&#21147;&#30340;&#26041;&#24335;&#36827;&#34892;&#39640;&#36890;&#37327;&#29983;&#29289;&#21307;&#23398;&#20851;&#31995;&#25552;&#21462;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#23558;&#20851;&#31995;&#25552;&#21462;&#20219;&#21153;&#21046;&#23450;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20108;&#20803;&#20998;&#31867;&#65292;LLMs&#22522;&#20110;&#22806;&#37096;&#35821;&#26009;&#24211;&#21644;&#20854;&#19990;&#30028;&#30693;&#35782;&#20570;&#20986;&#20915;&#31574;&#65292;&#32473;&#20986;&#20107;&#23454;&#39564;&#35777;&#30340;&#21028;&#26029;&#29702;&#30001;&#12290;&#27492;&#26041;&#27861;&#19987;&#20026;&#21322;&#32467;&#26500;&#21270;&#32593;&#32476;&#25991;&#31456;&#32780;&#35774;&#35745;&#65292;&#22312;&#20854;&#20013;&#23558;&#20027;&#26631;&#39064;&#25351;&#23450;&#20026;&#23614;&#23454;&#20307;&#24182;&#26126;&#30830;&#32435;&#20837;&#19978;&#19979;&#25991;&#65292;&#28508;&#22312;&#22836;&#23454;&#20307;&#26681;&#25454;&#29983;&#29289;&#21307;&#23398;&#35789;&#34920;&#36827;&#34892;&#21305;&#37197;&#12290;&#27492;&#22806;&#65292;&#23558;&#20887;&#38271;&#20869;&#23481;&#20998;&#21106;&#20026;&#25991;&#26412;&#22359;&#65292;&#23884;&#20837;&#24182;&#20351;&#29992;&#39069;&#22806;&#30340;&#23884;&#20837;&#27169;&#22411;&#36827;&#34892;&#26816;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.08274v4 Announce Type: replace-cross  Abstract: Objective: To develop a high-throughput biomedical relation extraction system that takes advantage of the large language models'(LLMs) reading comprehension ability and biomedical world knowledge in a scalable and evidential manner. Methods: We formulate the relation extraction task as binary classifications for large language models. Specifically, LLMs make the decision based on the external corpus and its world knowledge, giving the reason for the judgment for factual verification. This method is tailored for semi-structured web articles, wherein we designate the main title as the tail entity and explicitly incorporate it into the context, and the potential head entities are matched based on a biomedical thesaurus. Moreover, lengthy contents are sliced into text chunks, embedded, and retrieved with additional embedding models. Results: Using an open-source LLM, we extracted 248659 relation triplets of three distinct relation 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Sieve-&amp;-Swap&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#33258;&#21160;&#31579;&#36873;&#20986;&#19981;&#30456;&#20851;&#25991;&#26412;&#24182;&#29992;&#20154;&#31867;&#32534;&#20889;&#30340;&#35828;&#26126;&#26367;&#25442;&#25991;&#26412;&#36716;&#24405;&#65292;&#20174;&#32780;&#23454;&#29616;&#35270;&#39057;&#26412;&#22320;&#21270;&#25351;&#20196;&#29983;&#25104;&#30340;&#39640;&#25928;&#39044;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2311.15964</link><description>&lt;p&gt;
&#35270;&#39057;&#26412;&#22320;&#21270;&#25351;&#20196;&#29983;&#25104;&#30340;&#39640;&#25928;&#39044;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Pre-training for Localized Instruction Generation of Videos
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15964
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Sieve-&amp;-Swap&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#33258;&#21160;&#31579;&#36873;&#20986;&#19981;&#30456;&#20851;&#25991;&#26412;&#24182;&#29992;&#20154;&#31867;&#32534;&#20889;&#30340;&#35828;&#26126;&#26367;&#25442;&#25991;&#26412;&#36716;&#24405;&#65292;&#20174;&#32780;&#23454;&#29616;&#35270;&#39057;&#26412;&#22320;&#21270;&#25351;&#20196;&#29983;&#25104;&#30340;&#39640;&#25928;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#31243;&#35270;&#39057;&#23637;&#31034;&#20102;&#35832;&#22914;&#39135;&#35889;&#20934;&#22791;&#31561;&#20219;&#21153;&#30340;&#36880;&#27493;&#28436;&#31034;&#12290;&#29702;&#35299;&#27492;&#31867;&#35270;&#39057;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#38656;&#35201;&#23545;&#27493;&#39588;&#36827;&#34892;&#31934;&#30830;&#23450;&#20301;&#24182;&#29983;&#25104;&#25991;&#23383;&#35828;&#26126;&#12290;&#25163;&#21160;&#27880;&#37322;&#27493;&#39588;&#24182;&#32534;&#20889;&#35828;&#26126;&#25104;&#26412;&#39640;&#26114;&#65292;&#36825;&#38480;&#21046;&#20102;&#24403;&#21069;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#24182;&#38459;&#30861;&#20102;&#26377;&#25928;&#23398;&#20064;&#12290;&#21033;&#29992;&#22823;&#35268;&#27169;&#20294;&#22024;&#26434;&#30340;&#35270;&#39057;-&#25991;&#26412;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#25552;&#21319;&#24615;&#33021;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#27492;&#22806;&#65292;&#25991;&#26412;&#36716;&#24405;&#21253;&#21547;&#26080;&#20851;&#20869;&#23481;&#65292;&#19982;&#20154;&#31867;&#27880;&#37322;&#21592;&#32534;&#20889;&#30340;&#35828;&#26126;&#30456;&#27604;&#23384;&#22312;&#39118;&#26684;&#21464;&#21270;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25216;&#26415;&#65292;Sieve-&amp;-Swap&#65292;&#36890;&#36807;&#33258;&#21160;&#31579;&#36873;&#20986;&#19981;&#30456;&#20851;&#25991;&#26412;&#21644;&#20351;&#29992;&#25991;&#26412;&#39135;&#35889;&#25968;&#25454;&#38598;&#20013;&#20154;&#31867;&#32534;&#20889;&#30340;&#35828;&#26126;&#33258;&#21160;&#26367;&#25442;&#25991;&#26412;&#36716;&#24405;&#20197;&#22686;&#24378;&#25991;&#23383;&#25351;&#20196;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.15964v2 Announce Type: replace-cross  Abstract: Procedural videos show step-by-step demonstrations of tasks like recipe preparation. Understanding such videos is challenging, involving the precise localization of steps and the generation of textual instructions. Manually annotating steps and writing instructions is costly, which limits the size of current datasets and hinders effective learning. Leveraging large but noisy video-transcript datasets for pre-training can boost performance, but demands significant computational resources. Furthermore, transcripts contain irrelevant content and exhibit style variation compared to instructions written by human annotators. To mitigate both issues, we propose a technique, Sieve-&amp;-Swap, to automatically curate a smaller dataset: (i) Sieve filters irrelevant transcripts and (ii) Swap enhances the quality of the text instruction by automatically replacing the transcripts with human-written instructions from a text-only recipe dataset. 
&lt;/p&gt;</description></item><item><title>&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;&#24773;&#32490;&#26102;&#20027;&#35201;&#19981;&#32771;&#34385;&#24773;&#32490;&#35302;&#21457;&#22120;&#65292;&#32780;&#26159;&#24773;&#32490;&#35302;&#21457;&#22120;&#19982;&#21508;&#31181;&#29305;&#24449;&#21644;&#24773;&#32490;&#26816;&#27979;&#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#38169;&#32508;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2311.09602</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;&#24773;&#32490;&#26102;&#20027;&#35201;&#19981;&#32771;&#34385;&#24773;&#32490;&#35302;&#21457;&#22120;
&lt;/p&gt;
&lt;p&gt;
Language Models (Mostly) Do Not Consider Emotion Triggers When Predicting Emotion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09602
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;&#24773;&#32490;&#26102;&#20027;&#35201;&#19981;&#32771;&#34385;&#24773;&#32490;&#35302;&#21457;&#22120;&#65292;&#32780;&#26159;&#24773;&#32490;&#35302;&#21457;&#22120;&#19982;&#21508;&#31181;&#29305;&#24449;&#21644;&#24773;&#32490;&#26816;&#27979;&#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#38169;&#32508;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#22659;&#21644;&#20107;&#20214;&#20250;&#24341;&#21457;&#20154;&#31867;&#30340;&#24773;&#32490;&#65292;&#20294;&#21040;&#24213;&#23427;&#20204;&#22312;&#39044;&#27979;&#24773;&#32490;&#26816;&#27979;&#27169;&#22411;&#26102;&#36215;&#21040;&#20102;&#22810;&#22823;&#20316;&#29992;&#65311;&#36825;&#39033;&#24037;&#20316;&#25506;&#35752;&#20102;&#20154;&#31867;&#26631;&#27880;&#30340;&#24773;&#32490;&#35302;&#21457;&#22120;&#19982;&#27169;&#22411;&#22312;&#39044;&#27979;&#24773;&#32490;&#26102;&#35748;&#20026;&#37325;&#35201;&#30340;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;EmoTrigger&#65292;&#21253;&#25324;900&#26465;&#26469;&#28304;&#20110;&#19977;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#65307;&#36825;&#20123;&#24086;&#23376;&#30001;&#19987;&#23478;&#36827;&#34892;&#24773;&#32490;&#35302;&#21457;&#22120;&#27880;&#37322;&#65292;&#36798;&#25104;&#39640;&#24230;&#19968;&#33268;&#12290;&#21033;&#29992;EmoTrigger&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#35782;&#21035;&#24773;&#32490;&#35302;&#21457;&#22120;&#30340;&#33021;&#21147;&#65292;&#24182;&#23545;LLMs&#21644;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#35748;&#20026;&#37325;&#35201;&#30340;&#29305;&#24449;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#24773;&#32490;&#35302;&#21457;&#22120;&#22312;&#24773;&#32490;&#39044;&#27979;&#27169;&#22411;&#20013;&#20027;&#35201;&#19981;&#34987;&#35270;&#20026;&#37325;&#35201;&#29305;&#24449;&#65292;&#32780;&#26159;&#21508;&#31181;&#29305;&#24449;&#20043;&#38388;&#20197;&#21450;&#24773;&#32490;&#26816;&#27979;&#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#38169;&#32508;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09602v2 Announce Type: replace  Abstract: Situations and events evoke emotions in humans, but to what extent do they inform the prediction of emotion detection models? This work investigates how well human-annotated emotion triggers correlate with features that models deemed salient in their prediction of emotions. First, we introduce a novel dataset EmoTrigger, consisting of 900 social media posts sourced from three different datasets; these were annotated by experts for emotion triggers with high agreement. Using EmoTrigger, we evaluate the ability of large language models (LLMs) to identify emotion triggers, and conduct a comparative analysis of the features considered important for these tasks between LLMs and fine-tuned models. Our analysis reveals that emotion triggers are largely not considered salient features for emotion prediction models, instead there is intricate interplay between various features and the task of emotion detection.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#20154;&#19982;&#20154;&#20043;&#38388;&#30340;&#28151;&#21512;&#35821;&#38899;&#33258;&#21457;&#35821;&#38899;&#20013;&#23384;&#22312;&#19982;&#20070;&#38754;&#21644;&#21475;&#35821;&#21333;&#35821;&#29615;&#22659;&#21516;&#27493;&#30340;&#27169;&#24335;&#65292;&#24182;&#19988;&#23545;&#35805;&#31995;&#32479;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#30340;&#20195;&#30721;&#20999;&#25442;&#21516;&#27493;&#27169;&#24335;&#22312;&#36825;&#31181;&#29615;&#22659;&#20013;&#20063;&#26159;&#26222;&#36866;&#30340;&#12290;</title><link>https://arxiv.org/abs/2311.07703</link><description>&lt;p&gt;
&#27979;&#37327;&#33258;&#21457;&#28151;&#21512;&#35821;&#38899;&#20013;&#30340;&#21516;&#27493;&#24615;
&lt;/p&gt;
&lt;p&gt;
Measuring Entrainment in Spontaneous Code-switched Speech
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07703
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#20154;&#19982;&#20154;&#20043;&#38388;&#30340;&#28151;&#21512;&#35821;&#38899;&#33258;&#21457;&#35821;&#38899;&#20013;&#23384;&#22312;&#19982;&#20070;&#38754;&#21644;&#21475;&#35821;&#21333;&#35821;&#29615;&#22659;&#21516;&#27493;&#30340;&#27169;&#24335;&#65292;&#24182;&#19988;&#23545;&#35805;&#31995;&#32479;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#30340;&#20195;&#30721;&#20999;&#25442;&#21516;&#27493;&#27169;&#24335;&#22312;&#36825;&#31181;&#29615;&#22659;&#20013;&#20063;&#26159;&#26222;&#36866;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#37027;&#20123;&#30456;&#20114;&#21516;&#27493;&#30340;&#35828;&#35805;&#32773;&#27604;&#37027;&#20123;&#19981;&#21516;&#27493;&#30340;&#35828;&#35805;&#32773;&#26377;&#26356;&#25104;&#21151;&#30340;&#23545;&#35805;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20132;&#38469;&#32773;&#22312;&#20070;&#38754;&#21644;&#21475;&#22836;&#21333;&#35821;&#39046;&#22495;&#37117;&#20250;&#21516;&#27493;&#35821;&#35328;&#29305;&#24449;&#12290;&#26356;&#36817;&#26399;&#20851;&#20110;&#28151;&#21512;&#35821;&#35328;&#20132;&#27969;&#30340;&#30740;&#31350;&#20063;&#26174;&#31034;&#20102;&#22312;&#19968;&#20123;&#20195;&#30721;&#20999;&#25442;&#65288;CSW&#65289;&#26041;&#38754;&#23384;&#22312;&#21516;&#27493;&#30340;&#21021;&#27493;&#35777;&#25454;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20851;&#20110;&#28151;&#21512;&#35821;&#38899;&#39046;&#22495;&#30340;&#21516;&#27493;&#30740;&#31350;&#38750;&#24120;&#23569;&#65292;&#24182;&#19988;&#23616;&#38480;&#20110;&#20154;&#26426;&#25991;&#26412;&#20114;&#21160;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#30740;&#31350;&#20102;&#20154;&#19982;&#20154;&#20043;&#38388;&#30340;&#28151;&#21512;&#35821;&#38899;&#33258;&#21457;&#35821;&#38899;&#65292;&#21457;&#29616;&#65288;1&#65289;&#20070;&#38754;&#21644;&#21475;&#35821;&#21333;&#35821;&#29615;&#22659;&#20013;&#30340;&#21516;&#27493;&#27169;&#24335;&#22312;&#28151;&#21512;&#35821;&#35328;&#29615;&#22659;&#20013;&#22823;&#20307;&#19978;&#26159;&#26222;&#36866;&#30340;&#65292;&#65288;2&#65289;&#23545;&#35805;&#31995;&#32479;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#30340;&#20195;&#30721;&#20999;&#25442;&#21516;&#27493;&#27169;&#24335;&#22312;&#33258;&#21457;&#28151;&#21512;&#35821;&#38899;&#20013;&#20063;&#26159;&#26222;&#36866;&#30340;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#23545;&#20110;&#21487;&#33021;&#20855;&#26377;&#8220;&#26222;&#36941;&#8221;&#24615;&#30340;&#37325;&#35201;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07703v2 Announce Type: replace  Abstract: It is well-known that speakers who entrain to one another have more successful conversations than those who do not. Previous research has shown that interlocutors entrain on linguistic features in both written and spoken monolingual domains. More recent work on code-switched communication has also shown preliminary evidence of entrainment on certain aspects of code-switching (CSW). However, such studies of entrainment in code-switched domains have been extremely few and restricted to human-machine textual interactions. Our work studies code-switched spontaneous speech between humans, finding that (1) patterns of written and spoken entrainment in monolingual settings largely generalize to code-switched settings, and (2) some patterns of entrainment on code-switching in dialogue agent-generated text generalize to spontaneous code-switched speech. Our findings give rise to important implications for the potentially "universal" nature of
&lt;/p&gt;</description></item><item><title>NLP&#30740;&#31350;&#20154;&#21592;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26032;&#26102;&#20195;&#20013;&#65292;&#21487;&#20197;&#20174;&#21382;&#21490;&#20013;&#27762;&#21462;&#25945;&#35757;&#65292;&#25345;&#32493;&#35299;&#20915;&#35268;&#27169;&#24046;&#24322;&#12289;&#25968;&#25454;&#29942;&#39048;&#12289;&#29616;&#23454;&#35780;&#20272;&#31561;&#38382;&#39064;&#65292;&#21516;&#26102;&#36824;&#26377;&#31354;&#38388;&#23581;&#35797;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2311.05020</link><description>&lt;p&gt;
&#20808;&#24754;&#21095;&#65292;&#20877;&#35299;&#26512;&#65306;&#21382;&#21490;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26102;&#20195;&#20013;&#37325;&#28436;
&lt;/p&gt;
&lt;p&gt;
First Tragedy, then Parse: History Repeats Itself in the New Era of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.05020
&lt;/p&gt;
&lt;p&gt;
NLP&#30740;&#31350;&#20154;&#21592;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26032;&#26102;&#20195;&#20013;&#65292;&#21487;&#20197;&#20174;&#21382;&#21490;&#20013;&#27762;&#21462;&#25945;&#35757;&#65292;&#25345;&#32493;&#35299;&#20915;&#35268;&#27169;&#24046;&#24322;&#12289;&#25968;&#25454;&#29942;&#39048;&#12289;&#29616;&#23454;&#35780;&#20272;&#31561;&#38382;&#39064;&#65292;&#21516;&#26102;&#36824;&#26377;&#31354;&#38388;&#23581;&#35797;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20154;&#21592;&#27491;&#32463;&#21382;&#19968;&#22330;&#23384;&#22312;&#21361;&#26426;&#65292;&#36825;&#19968;&#21361;&#26426;&#26159;&#30001;ChatGPT&#21644;&#20854;&#20182;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31995;&#32479;&#21462;&#24471;&#24778;&#20154;&#25104;&#21151;&#25152;&#35302;&#21457;&#30340;&#12290;&#22312;&#23545;&#35813;&#39046;&#22495;&#30340;&#29702;&#35299;&#21457;&#29983;&#22914;&#27492;&#39072;&#35206;&#24615;&#21464;&#21270;&#21518;&#65292;&#21097;&#19979;&#20160;&#20040;&#21487;&#20570;&#21602;&#65311;&#25105;&#20204;&#20174;&#21382;&#21490;&#30340;&#35270;&#35282;&#20986;&#21457;&#65292;&#23547;&#25214;&#31532;&#19968;&#20010;&#20197;&#22823;&#22411;$n$-gram&#27169;&#22411;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;(MT)&#24320;&#22987;&#20110;2005&#24180;&#30340;LLM&#26102;&#20195;&#30340;&#25351;&#23548;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#31532;&#19968;&#20010;&#26102;&#20195;&#30340;&#25345;&#20037;&#24615;&#25945;&#35757;&#65292;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;NLP&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#22312;LLMs&#21344;&#20027;&#23548;&#22320;&#20301;&#30340;&#39046;&#22495;&#32487;&#32493;&#20570;&#20986;&#26377;&#24847;&#20041;&#30340;&#36129;&#29486;&#30340;&#27704;&#24658;&#38382;&#39064;&#12290;&#25105;&#20204;&#35748;&#20026;&#35268;&#27169;&#19978;&#30340;&#24046;&#24322;&#26159;&#26242;&#26102;&#30340;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#21162;&#21147;&#20943;&#23569;&#36825;&#20123;&#24046;&#36317;&#65307;&#25968;&#25454;&#20173;&#28982;&#26159;&#35768;&#22810;&#24212;&#29992;&#30340;&#29942;&#39048;&#65292;&#32780;&#19981;&#26159;&#30828;&#20214;&#65307;&#26377;&#24847;&#20041;&#30340;&#29616;&#23454;&#35780;&#20272;&#20173;&#28982;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#65307;&#32780;&#19988;&#36824;&#26377;&#31354;&#38388;&#21487;&#20197;&#23581;&#35797;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.05020v2 Announce Type: replace  Abstract: Many NLP researchers are experiencing an existential crisis triggered by the astonishing success of ChatGPT and other systems based on large language models (LLMs). After such a disruptive change to our understanding of the field, what is left to do? Taking a historical lens, we look for guidance from the first era of LLMs, which began in 2005 with large $n$-gram models for machine translation (MT). We identify durable lessons from the first era, and more importantly, we identify evergreen problems where NLP researchers can continue to make meaningful contributions in areas where LLMs are ascendant. We argue that disparities in scale are transient and researchers can work to reduce them; that data, rather than hardware, is still a bottleneck for many applications; that meaningful realistic evaluation is still an open problem; and that there is still room for speculative approaches.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#32534;&#36753;&#30340;&#28508;&#22312;&#38519;&#38449;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#21457;&#29616;&#30693;&#35782;&#20914;&#31361;&#21644;&#30693;&#35782;&#25197;&#26354;&#26159;&#20004;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2310.02129</link><description>&lt;p&gt;
&#25581;&#31034;&#22823;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#32534;&#36753;&#30340;&#38519;&#38449;
&lt;/p&gt;
&lt;p&gt;
Unveiling the Pitfalls of Knowledge Editing for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.02129
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#32534;&#36753;&#30340;&#28508;&#22312;&#38519;&#38449;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#21457;&#29616;&#30693;&#35782;&#20914;&#31361;&#21644;&#30693;&#35782;&#25197;&#26354;&#26159;&#20004;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25104;&#26412;&#19981;&#26029;&#19978;&#21319;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24037;&#20316;&#24050;&#32463;&#36716;&#21521;&#24320;&#21457;&#32534;&#36753;LLMs&#20869;&#22312;&#30693;&#35782;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20173;&#26377;&#19968;&#20010;&#38452;&#20113;&#24748;&#22312;&#22836;&#39030;&#19978; - &#30693;&#35782;&#32534;&#36753;&#26159;&#21542;&#20250;&#35302;&#21457;&#34676;&#34678;&#25928;&#24212;&#65311;&#22240;&#20026;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#30693;&#35782;&#32534;&#36753;&#26159;&#21542;&#20250;&#24341;&#20837;&#21487;&#33021;&#24102;&#26469;&#28508;&#22312;&#39118;&#38505;&#30340;&#21103;&#20316;&#29992;&#12290;&#26412;&#25991;&#39318;&#27425;&#25506;&#35752;&#20102;&#19982;LLMs&#30693;&#35782;&#32534;&#36753;&#30456;&#20851;&#30340;&#28508;&#22312;&#38519;&#38449;&#12290;&#20026;&#23454;&#29616;&#27492;&#30446;&#30340;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#24182;&#25552;&#20986;&#20102;&#21019;&#26032;&#24615;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#24378;&#35843;&#20102;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#65288;1&#65289;&#30693;&#35782;&#20914;&#31361;&#65306;&#32534;&#36753;&#36923;&#36753;&#20914;&#31361;&#30340;&#20107;&#23454;&#32452;&#21487;&#33021;&#20250;&#25918;&#22823;LLMs&#22266;&#26377;&#30340;&#19981;&#19968;&#33268;&#24615; - &#36825;&#26159;&#20197;&#21069;&#26041;&#27861;&#24573;&#30053;&#30340;&#19968;&#20010;&#26041;&#38754;&#12290;&#65288;2&#65289;&#30693;&#35782;&#25197;&#26354;&#65306;&#20026;&#20102;&#32534;&#36753;&#20107;&#23454;&#30693;&#35782;&#32780;&#26356;&#25913;&#21442;&#25968;&#21487;&#33021;&#20250;&#19981;&#21487;&#36870;&#22320;&#25197;&#26354;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.02129v3 Announce Type: replace-cross  Abstract: As the cost associated with fine-tuning Large Language Models (LLMs) continues to rise, recent research efforts have pivoted towards developing methodologies to edit implicit knowledge embedded within LLMs. Yet, there's still a dark cloud lingering overhead -- will knowledge editing trigger butterfly effect? since it is still unclear whether knowledge editing might introduce side effects that pose potential risks or not. This paper pioneers the investigation into the potential pitfalls associated with knowledge editing for LLMs. To achieve this, we introduce new benchmark datasets and propose innovative evaluation metrics. Our results underline two pivotal concerns: (1) Knowledge Conflict: Editing groups of facts that logically clash can magnify the inherent inconsistencies in LLMs-a facet neglected by previous methods. (2) Knowledge Distortion: Altering parameters with the aim of editing factual knowledge can irrevocably warp 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;LoT&#65288;Logical Thoughts&#65289;&#25552;&#31034;&#65292;&#19968;&#20010;&#33258;&#25105;&#25913;&#36827;&#26694;&#26550;&#65292;&#21033;&#29992;&#26681;&#26893;&#20110;&#31526;&#21495;&#36923;&#36753;&#30340;&#21407;&#21017;&#65292;&#29305;&#21035;&#26159;&#24402;&#35884;&#27861;&#65292;&#36880;&#27493;&#39564;&#35777;&#21644;&#32416;&#27491;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#23556;&#38142;&#25512;&#29702;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2309.13339</link><description>&lt;p&gt;
&#36890;&#36807;&#36923;&#36753;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38646;&#23556;&#38142;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.13339
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;LoT&#65288;Logical Thoughts&#65289;&#25552;&#31034;&#65292;&#19968;&#20010;&#33258;&#25105;&#25913;&#36827;&#26694;&#26550;&#65292;&#21033;&#29992;&#26681;&#26893;&#20110;&#31526;&#21495;&#36923;&#36753;&#30340;&#21407;&#21017;&#65292;&#29305;&#21035;&#26159;&#24402;&#35884;&#27861;&#65292;&#36880;&#27493;&#39564;&#35777;&#21644;&#32416;&#27491;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#23556;&#38142;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340; remarkable generalizability&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#25512;&#29702;&#33021;&#21147;&#20173;&#26377;&#24456;&#22823;&#30340;&#25552;&#21319;&#31354;&#38388;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#35201;&#22810;&#27493;&#25512;&#29702;&#30340;&#24773;&#20917;&#19979;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#24191;&#27867;&#30340;&#30693;&#35782;&#65292;&#20294;&#23427;&#20204;&#30340;&#25512;&#29702;&#32463;&#24120;&#26410;&#33021;&#26377;&#25928;&#21033;&#29992;&#36825;&#20123;&#30693;&#35782;&#26469;&#24314;&#31435;&#36830;&#36143;&#30340;&#24605;&#32500;&#33539;&#24335;&#12290;&#36825;&#20123;&#27169;&#22411;&#26377;&#26102;&#20250;&#20986;&#29616;&#24187;&#35273;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#25512;&#29702;&#36807;&#31243;&#26410;&#21463;&#36923;&#36753;&#21407;&#21017;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#23556;&#38142;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; LoT&#65288;Logical Thoughts&#65289;&#25552;&#31034;&#65292;&#36825;&#26159;&#19968;&#20010;&#33258;&#25105;&#25913;&#36827;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#26681;&#26893;&#20110;&#31526;&#21495;&#36923;&#36753;&#30340;&#21407;&#21017;&#65292;&#29305;&#21035;&#26159;&#24402;&#35884;&#27861;&#65292;&#36880;&#27493;&#31995;&#32479;&#22320;&#39564;&#35777;&#21644;&#32416;&#27491;&#25512;&#29702;&#36807;&#31243;&#12290;&#22312;&#35821;&#35328;&#20219;&#21153;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.13339v2 Announce Type: replace-cross  Abstract: Recent advancements in large language models have showcased their remarkable generalizability across various domains. However, their reasoning abilities still have significant room for improvement, especially when confronted with scenarios requiring multi-step reasoning. Although large language models possess extensive knowledge, their reasoning often fails to effectively utilize this knowledge to establish a coherent thinking paradigm. These models sometimes show hallucinations as their reasoning procedures are unconstrained by logical principles. Aiming at improving the zero-shot chain-of-thought reasoning ability of large language models, we propose LoT (Logical Thoughts) prompting, a self-improvement framework that leverages principles rooted in symbolic logic, particularly Reductio ad Absurdum, to systematically verify and rectify the reasoning processes step by step. Experimental evaluations conducted on language tasks in
&lt;/p&gt;</description></item><item><title>&#35199;&#29677;&#29273;&#35821;&#36164;&#28304;&#35821;&#27861;&#65288;SRG&#65289;&#30340;&#26368;&#26032;&#29256;&#26412;&#24341;&#20837;&#20102;Freeling&#24418;&#24577;&#20998;&#26512;&#22120;&#65292;&#24182;&#38468;&#24102;&#19968;&#20010;&#32463;&#36807;&#25163;&#24037;&#39564;&#35777;&#30340;&#26641;&#24211;&#65292;&#20026;&#25552;&#39640;&#35821;&#20041;&#35299;&#26512;&#22120;&#30340;&#35757;&#32451;&#36136;&#37327;&#21644;&#20854;&#20182;&#31995;&#32479;&#24102;&#26469;&#20415;&#21033;&#12290;</title><link>https://arxiv.org/abs/2309.13318</link><description>&lt;p&gt;
&#35199;&#29677;&#29273;&#35821;&#36164;&#28304;&#35821;&#27861;2023&#29256;
&lt;/p&gt;
&lt;p&gt;
Spanish Resource Grammar version 2023
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.13318
&lt;/p&gt;
&lt;p&gt;
&#35199;&#29677;&#29273;&#35821;&#36164;&#28304;&#35821;&#27861;&#65288;SRG&#65289;&#30340;&#26368;&#26032;&#29256;&#26412;&#24341;&#20837;&#20102;Freeling&#24418;&#24577;&#20998;&#26512;&#22120;&#65292;&#24182;&#38468;&#24102;&#19968;&#20010;&#32463;&#36807;&#25163;&#24037;&#39564;&#35777;&#30340;&#26641;&#24211;&#65292;&#20026;&#25552;&#39640;&#35821;&#20041;&#35299;&#26512;&#22120;&#30340;&#35757;&#32451;&#36136;&#37327;&#21644;&#20854;&#20182;&#31995;&#32479;&#24102;&#26469;&#20415;&#21033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#35199;&#29677;&#29273;&#35821;&#36164;&#28304;&#35821;&#27861;&#65288;SRG&#65289;&#30340;&#26368;&#26032;&#29256;&#26412;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;HPSG&#24418;&#24335;&#20027;&#20041;&#23454;&#29616;&#30340;&#35199;&#29677;&#29273;&#35821;&#35821;&#27861;&#12290;&#36825;&#20123;&#35821;&#27861;&#32534;&#30721;&#20102;&#20851;&#20110;&#21477;&#27861;&#30340;&#22797;&#26434;&#20551;&#35774;&#65292;&#20351;&#20854;&#25104;&#20026;&#35821;&#35328;&#23398;&#29702;&#35770;&#23454;&#35777;&#27979;&#35797;&#30340;&#36164;&#28304;&#12290;&#23427;&#20204;&#36824;&#32534;&#30721;&#20102;&#20005;&#26684;&#30340;&#35821;&#27861;&#24615;&#27010;&#24565;&#65292;&#20351;&#20854;&#25104;&#20026;&#35745;&#31639;&#36741;&#21161;&#35821;&#35328;&#23398;&#20064;&#20013;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#30340;&#36164;&#28304;&#12290;&#36825;&#20010;SRG&#29256;&#26412;&#20351;&#29992;&#20102;&#26368;&#36817;&#30340;Freeling&#24418;&#24577;&#20998;&#26512;&#22120;&#29256;&#26412;&#65292;&#24182;&#19988;&#21457;&#24067;&#26102;&#38468;&#24102;&#20102;&#19968;&#20010;&#21253;&#21547;2,291&#20010;&#21477;&#23376;&#30340;&#33258;&#21160;&#29983;&#25104;&#30340;&#12289;&#32463;&#36807;&#25163;&#24037;&#39564;&#35777;&#30340;&#26641;&#24211;&#12290;&#25105;&#20204;&#35299;&#37322;&#20102;&#26641;&#24211;&#26500;&#24314;&#36807;&#31243;&#65292;&#24378;&#35843;&#20102;&#23427;&#19982;&#25163;&#21160;&#27880;&#37322;&#30340;&#26641;&#24211;&#26500;&#24314;&#20043;&#38388;&#30340;&#21306;&#21035;&#65292;&#20197;&#21450;&#23427;&#22914;&#20309;&#26377;&#21161;&#20110;&#22522;&#20110;&#32463;&#39564;&#35777;&#25454;&#30340;&#21477;&#27861;&#29702;&#35770;&#21457;&#23637;&#12290;&#26641;&#24211;&#39640;&#27700;&#24179;&#30340;&#19968;&#33268;&#24615;&#21644;&#32454;&#33410;&#20351;&#20854;&#25104;&#20026;&#35757;&#32451;&#39640;&#36136;&#37327;&#35821;&#20041;&#35299;&#26512;&#22120;&#20197;&#21450;&#19968;&#33324;&#21463;&#30410;&#20110;&#31995;&#32479;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.13318v2 Announce Type: replace  Abstract: We present the latest version of the Spanish Resource Grammar (SRG), a grammar of Spanish implemented in the HPSG formalism. Such grammars encode a complex set of hypotheses about syntax making them a resource for empirical testing of linguistic theory. They also encode a strict notion of grammaticality which makes them a resource for natural language processing applications in computer-assisted language learning. This version of the SRG uses the recent version of the Freeling morphological analyzer and is released along with an automatically created, manually verified treebank of 2,291 sentences. We explain the treebanking process, emphasizing how it is different from treebanking with manual annotation and how it contributes to empirically-driven development of syntactic theory. The treebanks' high level of consistency and detail makes them a resource for training high-quality semantic parsers and generally systems that benefit from
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;CrossLingR&#65292;&#29992;&#20110;&#25512;&#21160;&#25910;&#25454;&#20449;&#24687;&#25552;&#21462;&#21644;&#29289;&#21697;&#20998;&#31867;&#30340;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;47,720&#20010;&#26631;&#27880;&#26679;&#26412;&#65292;&#35814;&#32454;&#35760;&#24405;&#20102;&#39033;&#30446;&#21517;&#31216;&#12289;&#30456;&#20851;&#23646;&#24615;&#21644;44&#20010;&#19981;&#21516;&#30340;&#20135;&#21697;&#31867;&#21035;&#12290;&#36890;&#36807;InstructLLaMA&#26041;&#27861;&#35770;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20851;&#38190;&#20449;&#24687;&#25552;&#21462;&#21644;&#29289;&#21697;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26174;&#33879;&#25928;&#26524;&#12290;&#30456;&#20851;&#36164;&#28304;&#21487;&#22312;https://github.com/Update-For-Integrated-Business-AI/AMuRD&#19978;&#33719;&#21462;&#12290;</title><link>https://arxiv.org/abs/2309.09800</link><description>&lt;p&gt;
&#36328;&#35821;&#35328;&#25910;&#25454;&#20449;&#24687;&#25552;&#21462;&#21644;&#20998;&#31867;&#30340;&#20840;&#38754;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598; CrossLingR
&lt;/p&gt;
&lt;p&gt;
CrossLingR: A Comprehensive Multilingual Receipt Dataset for Cross-Language Information Extraction and Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.09800
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;CrossLingR&#65292;&#29992;&#20110;&#25512;&#21160;&#25910;&#25454;&#20449;&#24687;&#25552;&#21462;&#21644;&#29289;&#21697;&#20998;&#31867;&#30340;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;47,720&#20010;&#26631;&#27880;&#26679;&#26412;&#65292;&#35814;&#32454;&#35760;&#24405;&#20102;&#39033;&#30446;&#21517;&#31216;&#12289;&#30456;&#20851;&#23646;&#24615;&#21644;44&#20010;&#19981;&#21516;&#30340;&#20135;&#21697;&#31867;&#21035;&#12290;&#36890;&#36807;InstructLLaMA&#26041;&#27861;&#35770;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20851;&#38190;&#20449;&#24687;&#25552;&#21462;&#21644;&#29289;&#21697;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26174;&#33879;&#25928;&#26524;&#12290;&#30456;&#20851;&#36164;&#28304;&#21487;&#22312;https://github.com/Update-For-Integrated-Business-AI/AMuRD&#19978;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#38190;&#20449;&#24687;&#25552;&#21462;&#30340;&#36807;&#31243;&#23545;&#20110;&#23558;&#25195;&#25551;&#30340;&#25910;&#25454;&#36716;&#21270;&#20026;&#32467;&#26500;&#21270;&#12289;&#21487;&#35775;&#38382;&#30340;&#25991;&#20214;&#33267;&#20851;&#37325;&#35201;&#65292;&#26377;&#21161;&#20110;&#26377;&#25928;&#22320;&#26816;&#32034;&#37325;&#35201;&#25968;&#25454;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#24191;&#27867;&#30340;&#12289;&#26032;&#39062;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25512;&#21160;&#25910;&#25454;&#20449;&#24687;&#25552;&#21462;&#21644;&#29289;&#21697;&#20998;&#31867;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;47,720&#20010;&#24102;&#26377;&#39033;&#30446;&#21517;&#31216;&#12289;&#30456;&#20851;&#23646;&#24615;&#65288;&#22914;&#20215;&#26684;&#21644;&#21697;&#29260;&#65289;&#30340;&#26631;&#27880;&#26679;&#26412;&#65292;&#24182;&#25353;&#29031;44&#20010;&#19981;&#21516;&#30340;&#20135;&#21697;&#31867;&#21035;&#36827;&#34892;&#32452;&#32455;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;InstructLLaMA&#26041;&#27861;&#35770;&#65292;&#36825;&#26159;&#19968;&#31181;&#24320;&#21019;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20851;&#38190;&#20449;&#24687;&#25552;&#21462;&#21644;&#29289;&#21697;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;F1&#20998;&#25968;&#20026;0.76&#21644;&#20934;&#30830;&#24615;&#20026;0.68&#30340;&#26174;&#33879;&#25928;&#26524;&#21152;&#20197;&#35777;&#26126;&#12290;&#20026;&#20102;&#25903;&#25345;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#24320;&#21457;&#65292;&#25105;&#20204;&#22312;https://github.com/Update-For-Integrated-Business-AI/AMuRD&#19978;&#25552;&#20379;&#20102;&#25105;&#20204;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#12289;InstructLLaMA&#27169;&#22411;&#21644;&#30456;&#20851;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
The process of key information extraction is critical for converting scanned receipts into structured, accessible documents, facilitating the efficient retrieval of vital data. This research introduces an expansive, novel multilingual dataset designed to propel advancements in the domain of receipt information extraction and item classification. Our dataset encompasses 47,720 annotated samples, detailed with item names, associated attributes such as price and brand, and organized into 44 distinct product categories. We unveil the InstructLLaMA methodology, a pioneering approach that demonstrates significant effectiveness, evidenced by an F1 score of 0.76 and an accuracy of 0.68 in tasks of key information extraction and item classification. To support further research and application development, we make available our comprehensive dataset, the InstructLLaMA model, and relevant resources at https://github.com/Update-For-Integrated-Business-AI/AMuRD.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#33258;&#23545;&#27604;&#36830;&#20307;&#32593;&#32476;&#21644;&#31070;&#32463;&#24067;&#38647;&#26684;&#26364;&#32593;&#32476;&#65292;&#25552;&#39640;&#20102;&#25991;&#26723;&#23884;&#20837;&#30340;&#36136;&#37327;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2305.16031</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#23545;&#27604;&#24067;&#38647;&#26684;&#26364;&#25955;&#24230;&#23398;&#20064;&#23454;&#29616;&#39640;&#25928;&#30340;&#25991;&#26723;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Efficient Document Embeddings via Self-Contrastive Bregman Divergence Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.16031
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#33258;&#23545;&#27604;&#36830;&#20307;&#32593;&#32476;&#21644;&#31070;&#32463;&#24067;&#38647;&#26684;&#26364;&#32593;&#32476;&#65292;&#25552;&#39640;&#20102;&#25991;&#26723;&#23884;&#20837;&#30340;&#36136;&#37327;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#39640;&#36136;&#37327;&#30340;&#25991;&#26723;&#23884;&#20837;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#12289;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#12289;&#25512;&#33616;&#31995;&#32479;&#21644;&#25628;&#32034;&#24341;&#25806;&#20013;&#30340;&#19968;&#20010;&#22522;&#30784;&#38382;&#39064;&#12290;&#23613;&#31649;&#26368;&#36817;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#21477;&#23376;&#23884;&#20837;&#20855;&#26377;&#33258;&#23545;&#27604;&#23398;&#20064;&#65292;&#20294;&#26159;&#23545;&#38271;&#25991;&#26723;&#36827;&#34892;&#32534;&#30721;&#22312;&#25928;&#29575;&#21644;&#36136;&#37327;&#26041;&#38754;&#20173;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;SimCSE&#26041;&#27861;&#35757;&#32451;&#22522;&#20110;Longformer&#30340;&#25991;&#26723;&#32534;&#30721;&#22120;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#22522;&#32447;&#26041;&#27861;--&#36830;&#20307;&#31070;&#32463;&#32593;&#32476;--&#19978;&#22686;&#21152;&#20102;&#22522;&#20110;&#27867;&#20984;&#24067;&#38647;&#26684;&#26364;&#25955;&#24230;&#30340;&#39069;&#22806;&#20984;&#31070;&#32463;&#32593;&#32476;&#65292;&#26088;&#22312;&#25552;&#39640;&#36755;&#20986;&#25991;&#26723;&#34920;&#31034;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#33258;&#23545;&#27604;&#36830;&#20307;&#32593;&#32476;&#21644;&#25105;&#20204;&#25552;&#20986;&#30340;&#31070;&#32463;&#24067;&#38647;&#26684;&#26364;&#32593;&#32476;&#30340;&#32452;&#21512;&#24635;&#20307;&#19978;&#20248;&#20110;&#20004;&#20010;&#32447;&#24615;&#20998;&#31867;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.16031v2 Announce Type: replace  Abstract: Learning quality document embeddings is a fundamental problem in natural language processing (NLP), information retrieval (IR), recommendation systems, and search engines. Despite recent advances in the development of transformer-based models that produce sentence embeddings with self-contrastive learning, the encoding of long documents (Ks of words) is still challenging with respect to both efficiency and quality considerations. Therefore, we train Longfomer-based document encoders using a state-of-the-art unsupervised contrastive learning method (SimCSE). Further on, we complement the baseline method -- siamese neural network -- with additional convex neural networks based on functional Bregman divergence aiming to enhance the quality of the output document representations. We show that overall the combination of a self-contrastive siamese network and our proposed neural Bregman network outperforms the baselines in two linear class
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#35821;&#27169;&#22411;&#21644;&#21452;&#35821;&#27169;&#22411;&#22312;&#34920;&#24449;&#20013;&#30340;&#20960;&#20309;&#24046;&#24322;&#65292;&#21457;&#29616;&#23545;&#20110;&#32473;&#23450;&#30340;&#35821;&#35328;&#23545;&#65292;&#22810;&#35821;&#27169;&#22411;&#30340;&#35299;&#30721;&#22120;&#34920;&#24449;&#22312;&#21508;&#21521;&#21516;&#24615;&#26041;&#38754;&#19968;&#36143;&#36739;&#24046;&#65292;&#21344;&#29992;&#30340;&#32500;&#24230;&#20063;&#36739;&#23569;&#12290;</title><link>https://arxiv.org/abs/2305.14230</link><description>&lt;p&gt;
&#25506;&#32034;&#22810;&#35821;&#21644;&#21452;&#35821;&#32763;&#35793;&#27169;&#22411;&#20043;&#38388;&#30340;&#34920;&#24449;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Exploring Representational Disparities Between Multilingual and Bilingual Translation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.14230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#35821;&#27169;&#22411;&#21644;&#21452;&#35821;&#27169;&#22411;&#22312;&#34920;&#24449;&#20013;&#30340;&#20960;&#20309;&#24046;&#24322;&#65292;&#21457;&#29616;&#23545;&#20110;&#32473;&#23450;&#30340;&#35821;&#35328;&#23545;&#65292;&#22810;&#35821;&#27169;&#22411;&#30340;&#35299;&#30721;&#22120;&#34920;&#24449;&#22312;&#21508;&#21521;&#21516;&#24615;&#26041;&#38754;&#19968;&#36143;&#36739;&#24046;&#65292;&#21344;&#29992;&#30340;&#32500;&#24230;&#20063;&#36739;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#36890;&#36807;&#23436;&#20840;&#30340;&#22810;&#35821;&#21442;&#25968;&#20849;&#20139;&#22312;&#35768;&#22810;&#35821;&#35328;&#23545;&#20043;&#38388;&#23454;&#29616;&#20102;&#21442;&#25968;&#25928;&#29575;&#21644;&#25972;&#20307;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#22810;&#35821;&#27169;&#22411;&#20013;&#30340;&#35821;&#35328;&#23545;&#22312;&#19968;&#23545;&#22810;&#32763;&#35793;&#35774;&#32622;&#20013;&#21487;&#33021;&#34920;&#29616;&#19981;&#22914;&#21452;&#35821;&#27169;&#22411;&#65292;&#21487;&#33021;&#20027;&#35201;&#26159;&#22240;&#20026;&#23427;&#20204;&#30340;&#34920;&#24449;&#23384;&#22312;&#20960;&#20309;&#24046;&#24322;&#12290;&#26412;&#25991;&#36890;&#36807;&#35745;&#31639;&#21452;&#35821;&#27169;&#22411;&#21644;&#19968;&#23545;&#22810;&#22810;&#35821;&#27169;&#22411;&#30340;&#34920;&#24449;&#30340;&#21508;&#21521;&#21516;&#24615;&#65292;&#20351;&#29992;&#20869;&#22312;&#32500;&#24230;&#21644;IsoScore&#26469;&#37327;&#21270;&#34920;&#24449;&#22914;&#20309;&#21033;&#29992;&#20854;&#22522;&#30784;&#21521;&#37327;&#31354;&#38388;&#20013;&#30340;&#32500;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.14230v2 Announce Type: replace  Abstract: Multilingual machine translation has proven immensely useful for both parameter efficiency and overall performance across many language pairs via complete multilingual parameter sharing. However, some language pairs in multilingual models can see worse performance than in bilingual models, especially in the one-to-many translation setting. Motivated by their empirical differences, we examine the geometric differences in representations from bilingual models versus those from one-to-many multilingual models. Specifically, we compute the isotropy of these representations using intrinsic dimensionality and IsoScore, in order to measure how the representations utilize the dimensions in their underlying vector space. Using the same evaluation data in both models, we find that for a given language pair, its multilingual model decoder representations are consistently less isotropic and occupy fewer dimensions than comparable bilingual model
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#32452;&#21512;&#24335;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;Troika&#27169;&#22411;&#65292;&#36890;&#36807;&#24314;&#31435;&#19977;&#20010;&#35782;&#21035;&#20998;&#25903;&#20849;&#21516;&#23545;&#29366;&#24577;&#12289;&#23545;&#35937;&#21644;&#32452;&#21512;&#36827;&#34892;&#24314;&#27169;&#65292;&#22312;&#23545;&#40784;&#20998;&#25903;&#29305;&#23450;&#25552;&#31034;&#34920;&#31034;&#21644;&#20998;&#35299;&#30340;&#35270;&#35273;&#29305;&#24449;&#30340;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;Cross-Modal Traction&#27169;&#22359;&#26469;&#26657;&#20934;&#22810;&#27169;&#24577;&#34920;&#31034;&#20043;&#38388;&#30340;&#20559;&#24046;&#12290;</title><link>https://arxiv.org/abs/2303.15230</link><description>&lt;p&gt;
Troika: &#22810;&#36335;&#24452;&#36328;&#27169;&#24577;&#25302;&#26355;&#23545;&#20110;&#32452;&#21512;&#24335;&#38646;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Troika: Multi-Path Cross-Modal Traction for Compositional Zero-Shot Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.15230
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#32452;&#21512;&#24335;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;Troika&#27169;&#22411;&#65292;&#36890;&#36807;&#24314;&#31435;&#19977;&#20010;&#35782;&#21035;&#20998;&#25903;&#20849;&#21516;&#23545;&#29366;&#24577;&#12289;&#23545;&#35937;&#21644;&#32452;&#21512;&#36827;&#34892;&#24314;&#27169;&#65292;&#22312;&#23545;&#40784;&#20998;&#25903;&#29305;&#23450;&#25552;&#31034;&#34920;&#31034;&#21644;&#20998;&#35299;&#30340;&#35270;&#35273;&#29305;&#24449;&#30340;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;Cross-Modal Traction&#27169;&#22359;&#26469;&#26657;&#20934;&#22810;&#27169;&#24577;&#34920;&#31034;&#20043;&#38388;&#30340;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#32452;&#21512;&#24335;&#38646;&#26679;&#26412;&#23398;&#20064;&#65288;CZSL&#65289;&#26041;&#27861;&#36890;&#36807;&#20165;&#20026;&#32452;&#21512;&#29366;&#24577;-&#23545;&#35937;&#23545;&#26500;&#24314;&#21487;&#35757;&#32451;&#25552;&#31034;&#26469;&#36866;&#24212;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#12290;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#23398;&#20064;&#24050;&#35265;&#32452;&#21512;&#30340;&#32852;&#21512;&#34920;&#31034;&#65292;&#32780;&#24573;&#30053;&#20102;&#23545;&#29366;&#24577;&#21644;&#23545;&#35937;&#30340;&#26174;&#24335;&#24314;&#27169;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23545;&#39044;&#35757;&#32451;&#30693;&#35782;&#30340;&#21033;&#29992;&#21644;&#23545;&#26410;&#35265;&#32452;&#21512;&#30340;&#27867;&#21270;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#35299;&#20915;&#26041;&#26696;&#30340;&#26222;&#36866;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20026;CZSL&#27169;&#22411;&#24314;&#31435;&#19977;&#20010;&#35782;&#21035;&#20998;&#25903;&#65288;&#21363;Multi-Path&#65289;&#20197;&#20849;&#21516;&#24314;&#27169;&#29366;&#24577;&#12289;&#23545;&#35937;&#21644;&#32452;&#21512;&#30340;&#26032;&#33539;&#24335;&#12290;&#25152;&#25552;&#20986;&#30340;Troika&#26159;&#25105;&#20204;&#30340;&#23454;&#29616;&#65292;&#23427;&#23558;&#20998;&#25903;&#29305;&#23450;&#30340;&#25552;&#31034;&#34920;&#31034;&#19982;&#20998;&#35299;&#30340;&#35270;&#35273;&#29305;&#24449;&#23545;&#40784;&#12290;&#20026;&#20102;&#26657;&#20934;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#20043;&#38388;&#30340;&#20559;&#24046;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35774;&#35745;&#20102;&#19968;&#20010;Cross-Modal Traction&#27169;&#22359;&#26469;&#23558;&#25552;&#31034;&#31227;&#21160;&#21040;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.15230v2 Announce Type: replace-cross  Abstract: Recent compositional zero-shot learning (CZSL) methods adapt pre-trained vision-language models (VLMs) by constructing trainable prompts only for composed state-object pairs. Relying on learning the joint representation of seen compositions, these methods ignore the explicit modeling of the state and object, thus limiting the exploitation of pre-trained knowledge and generalization to unseen compositions. With a particular focus on the universality of the solution, in this work, we propose a novel paradigm for CZSL models that establishes three identification branches (i.e., Multi-Path) to jointly model the state, object, and composition. The presented Troika is our implementation that aligns the branch-specific prompt representations with decomposed visual features. To calibrate the bias between semantically similar multi-modal representations, we further devise a Cross-Modal Traction module into Troika that shifts the prompt 
&lt;/p&gt;</description></item><item><title>&#22312;&#26631;&#20934;&#25910;&#38598;&#30340;&#21021;&#27493;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#38271;&#25991;&#26723;&#27169;&#22411;&#22312;MRR&#25110;NDCG&#26041;&#38754;&#24615;&#33021;&#19981;&#20339;&#65292;&#34920;&#29616;&#20302;&#20110;FirstP&#65292;&#25110;&#24179;&#22343;&#26368;&#22810;&#36229;&#36234;5&#65285;&#12290;&#25105;&#20204;&#25512;&#27979;&#36825;&#19981;&#26159;&#22240;&#20026;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#65292;&#32780;&#26159;&#30001;&#20110;&#30456;&#20851;&#27573;&#33853;&#20855;&#26377;&#20301;&#32622;&#20559;&#35265;&#65292;&#24448;&#24448;&#20301;&#20110;&#21069;512&#20010;&#25991;&#26723;&#26631;&#35760;&#20043;&#20013;&#12290;&#25105;&#20204;&#25214;&#21040;&#35777;&#25454;&#34920;&#26126;&#36825;&#31181;&#20559;&#35265;&#33267;&#23569;&#23384;&#22312;&#20110;&#20004;&#20010;&#27979;&#35797;&#38598;&#20013;&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#25910;&#38598;MS MARCO FarRelevant&#65292;&#20854;&#20013;&#21253;&#21547;</title><link>https://arxiv.org/abs/2207.01262</link><description>&lt;p&gt;
&#36890;&#36807;&#20840;&#38754;&#35780;&#20272;&#21644;Leaderboarding&#29702;&#35299;&#38271;&#25991;&#26723;&#25490;&#21517;&#27169;&#22411;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Understanding Performance of Long-Document Ranking Models through Comprehensive Evaluation and Leaderboarding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2207.01262
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26631;&#20934;&#25910;&#38598;&#30340;&#21021;&#27493;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#38271;&#25991;&#26723;&#27169;&#22411;&#22312;MRR&#25110;NDCG&#26041;&#38754;&#24615;&#33021;&#19981;&#20339;&#65292;&#34920;&#29616;&#20302;&#20110;FirstP&#65292;&#25110;&#24179;&#22343;&#26368;&#22810;&#36229;&#36234;5&#65285;&#12290;&#25105;&#20204;&#25512;&#27979;&#36825;&#19981;&#26159;&#22240;&#20026;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#65292;&#32780;&#26159;&#30001;&#20110;&#30456;&#20851;&#27573;&#33853;&#20855;&#26377;&#20301;&#32622;&#20559;&#35265;&#65292;&#24448;&#24448;&#20301;&#20110;&#21069;512&#20010;&#25991;&#26723;&#26631;&#35760;&#20043;&#20013;&#12290;&#25105;&#20204;&#25214;&#21040;&#35777;&#25454;&#34920;&#26126;&#36825;&#31181;&#20559;&#35265;&#33267;&#23569;&#23384;&#22312;&#20110;&#20004;&#20010;&#27979;&#35797;&#38598;&#20013;&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#25910;&#38598;MS MARCO FarRelevant&#65292;&#20854;&#20013;&#21253;&#21547;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35780;&#20272;&#20102;20&#22810;&#20010;&#29992;&#20110;&#38271;&#25991;&#26723;&#25490;&#21517;&#30340;Transformer&#27169;&#22411;&#65288;&#21253;&#25324;&#26368;&#36817;&#20351;&#29992;FlashAttention&#35757;&#32451;&#30340;LongP&#27169;&#22411;&#65289;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#31616;&#21333;&#30340;FirstP&#22522;&#32447;&#36827;&#34892;&#20102;&#27604;&#36739;&#65288;&#23558;&#30456;&#21516;&#27169;&#22411;&#24212;&#29992;&#20110;&#36755;&#20837;&#25130;&#26029;&#20026;&#21069;512&#20010;&#26631;&#35760;&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;MS MARCO&#25991;&#26723;v1&#20316;&#20026;&#20027;&#35201;&#35757;&#32451;&#38598;&#65292;&#24182;&#22312;&#38646;-shot&#22330;&#26223;&#19979;&#35780;&#20272;&#20102;&#27169;&#22411;&#65292;&#20197;&#21450;&#22312;&#23545;&#20854;&#20182;&#25910;&#38598;&#36827;&#34892;&#24494;&#35843;&#21518;&#35780;&#20272;&#20102;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2207.01262v2 Announce Type: replace-cross  Abstract: We evaluated 20+ Transformer models for ranking of long documents (including recent LongP models trained with FlashAttention) and compared them with simple FirstP baselines (applying the same model to input truncated to the first 512 tokens). We used MS MARCO Documents v1 as a primary training set and evaluated models in the zero-shot scenario as well as after fine-tuning on other collections.   In our initial experiments with standard collections we found that long-document models underperformed FirstP or outperformed it by at most 5% on average in terms of MRR or NDCG. We then conjectured that this was not due to models inability to process long context but rather due to a positional bias of relevant passages, which tended to be among the first 512 document tokens. We found evidence that this bias was, indeed, present in at least two test sets, which motivated us to create a new collection MS MARCO FarRelevant where the relev
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;&#19968;&#31181;&#23558;&#26102;&#38388;&#21644;&#25991;&#26412;&#20449;&#24687;&#34701;&#21512;&#22312;&#26032;&#38395;&#25991;&#26723;&#34920;&#31034;&#20013;&#30340;&#31070;&#32463;&#26041;&#27861;&#65292;&#29992;&#20110;&#20107;&#20214;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2112.06166</link><description>&lt;p&gt;
&#20855;&#26377;&#26102;&#38388;&#24863;&#30693;&#25991;&#26723;&#23884;&#20837;&#30340;&#20027;&#39064;&#26816;&#27979;&#21644;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
Topic Detection and Tracking with Time-Aware Document Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2112.06166
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;&#19968;&#31181;&#23558;&#26102;&#38388;&#21644;&#25991;&#26412;&#20449;&#24687;&#34701;&#21512;&#22312;&#26032;&#38395;&#25991;&#26723;&#34920;&#31034;&#20013;&#30340;&#31070;&#32463;&#26041;&#27861;&#65292;&#29992;&#20110;&#20107;&#20214;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#26465;&#20449;&#24687;&#34987;&#20256;&#25773;&#30340;&#26102;&#38388;&#26159;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#20803;&#25968;&#25454;&#30340;&#20851;&#38190;&#37096;&#20998;&#65292;&#20363;&#22914;&#20027;&#39064;&#26816;&#27979;&#21644;&#36319;&#36394;&#65288;TDT&#65289;&#12290;TDT&#31995;&#32479;&#26088;&#22312;&#36890;&#36807;&#20107;&#20214;&#23558;&#26032;&#38395;&#25991;&#31456;&#35821;&#26009;&#24211;&#36827;&#34892;&#32858;&#31867;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25551;&#36848;&#21516;&#19968;&#20107;&#20214;&#30340;&#25925;&#20107;&#24456;&#21487;&#33021;&#26159;&#22312;&#22823;&#33268;&#30456;&#21516;&#30340;&#26102;&#38388;&#32534;&#20889;&#30340;&#12290;&#20808;&#21069;&#20851;&#20110;TDT&#30340;&#26102;&#38388;&#24314;&#27169;&#30340;&#24037;&#20316;&#32771;&#34385;&#21040;&#20102;&#36825;&#19968;&#28857;&#65292;&#20294;&#26410;&#33021;&#24456;&#22909;&#22320;&#25429;&#25417;&#26102;&#38388;&#22914;&#20309;&#19982;&#20107;&#20214;&#30340;&#35821;&#20041;&#29305;&#24615;&#30456;&#20114;&#20316;&#29992;&#12290;&#20363;&#22914;&#65292;&#20851;&#20110;&#28909;&#24102;&#39118;&#26292;&#30340;&#25925;&#20107;&#24456;&#21487;&#33021;&#22312;&#30701;&#26102;&#38388;&#38388;&#38548;&#20869;&#32534;&#20889;&#65292;&#32780;&#20851;&#20110;&#30005;&#24433;&#19978;&#26144;&#30340;&#25925;&#20107;&#21487;&#33021;&#20250;&#25345;&#32493;&#20960;&#21608;&#25110;&#20960;&#20010;&#26376;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31070;&#32463;&#26041;&#27861;&#65292;&#23558;&#26102;&#38388;&#21644;&#25991;&#26412;&#20449;&#24687;&#34701;&#21512;&#21040;&#26032;&#38395;&#25991;&#26723;&#30340;&#21333;&#19968;&#34920;&#31034;&#20013;&#65292;&#29992;&#20110;&#20107;&#20214;&#26816;&#27979;&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#20803;&#25439;&#22833;&#26550;&#26500;&#23545;&#36825;&#20123;&#20855;&#26377;&#26102;&#38388;&#24863;&#30693;&#24615;&#30340;&#25991;&#26723;&#23884;&#20837;&#36827;&#34892;&#24494;&#35843;&#65292;&#23558;&#27169;&#22411;&#25972;&#21512;&#21040;&#19979;&#28216;TDT&#31995;&#32479;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2112.06166v2 Announce Type: replace  Abstract: The time at which a message is communicated is a vital piece of metadata in many real-world natural language processing tasks such as Topic Detection and Tracking (TDT). TDT systems aim to cluster a corpus of news articles by event, and in that context, stories that describe the same event are likely to have been written at around the same time. Prior work on time modeling for TDT takes this into account, but does not well capture how time interacts with the semantic nature of the event. For example, stories about a tropical storm are likely to be written within a short time interval, while stories about a movie release may appear over weeks or months. In our work, we design a neural method that fuses temporal and textual information into a single representation of news documents for event detection. We fine-tune these time-aware document embeddings with a triplet loss architecture, integrate the model into downstream TDT systems, an
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#25552;&#39640;&#32534;&#31243;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#38382;&#39064;&#65292;&#24182;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#39564;&#35777;&#12290;&#35813;&#30740;&#31350;&#34920;&#26126;&#65292;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#22312;&#32534;&#31243;&#35821;&#35328;&#39046;&#22495;&#20855;&#26377;&#28508;&#21147;&#65292;&#21487;&#20197;&#24110;&#21161;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#29992;&#25143;&#21463;&#30410;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.16937</link><description>&lt;p&gt;
&#36328;&#22810;&#31181;&#32534;&#31243;&#35821;&#35328;&#30340;&#23398;&#20064;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Learning Transfers over Several Programming Languages. (arXiv:2310.16937v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16937
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#25552;&#39640;&#32534;&#31243;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#38382;&#39064;&#65292;&#24182;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#39564;&#35777;&#12290;&#35813;&#30740;&#31350;&#34920;&#26126;&#65292;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#22312;&#32534;&#31243;&#35821;&#35328;&#39046;&#22495;&#20855;&#26377;&#28508;&#21147;&#65292;&#21487;&#20197;&#24110;&#21161;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#29992;&#25143;&#21463;&#30410;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#25552;&#39640;&#39640;&#36164;&#28304;&#32534;&#31243;&#35821;&#35328;&#24320;&#21457;&#32773;&#29983;&#20135;&#21147;&#26041;&#38754;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#36825;&#20123;&#27169;&#22411;&#20351;&#29992;&#20004;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#65306;&#22823;&#37327;&#30340;&#26080;&#26631;&#31614;&#20195;&#30721;&#26679;&#26412;&#29992;&#20110;&#39044;&#35757;&#32451;&#65292;&#30456;&#23545;&#36739;&#23569;&#30340;&#24102;&#26631;&#31614;&#20195;&#30721;&#26679;&#26412;&#29992;&#20110;&#24494;&#35843;&#25110;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#32534;&#31243;&#35821;&#35328;&#26159;&#20302;&#36164;&#28304;&#30340;&#65292;&#32570;&#20047;&#22823;&#22810;&#25968;&#20219;&#21153;&#30340;&#24102;&#26631;&#31614;&#26679;&#26412;&#65292;&#29978;&#33267;&#32570;&#20047;&#26080;&#26631;&#31614;&#26679;&#26412;&#12290;&#22240;&#27492;&#65292;&#20302;&#36164;&#28304;&#35821;&#35328;&#65288;&#20363;&#22914;&#36951;&#30041;&#25110;&#26032;&#35821;&#35328;&#65289;&#30340;&#29992;&#25143;&#26080;&#27861;&#20139;&#21463;&#21040;LLM&#30340;&#22909;&#22788;&#12290;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#20351;&#29992;&#28304;&#35821;&#35328;&#30340;&#25968;&#25454;&#26469;&#25552;&#39640;&#27169;&#22411;&#22312;&#30446;&#26631;&#35821;&#35328;&#19978;&#30340;&#24615;&#33021;&#12290;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#39046;&#22495;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#22312;&#32534;&#31243;&#35821;&#35328;&#39046;&#22495;&#21364;&#21463;&#21040;&#20102;&#24456;&#23569;&#20851;&#27880;&#12290;&#26412;&#25991;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;LLM&#21644;11&#21040;41&#31181;&#32534;&#31243;&#35821;&#35328;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25506;&#35752;&#20102;&#20197;&#19979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently become remarkably good at improving developer productivity for high-resource programming languages. These models use two kinds of data: large amounts of unlabeled code samples for pretraining and relatively smaller amounts of labeled code samples for fine-tuning or in-context learning. Unfortunately, many programming languages are low-resource, lacking labeled samples for most tasks and often even lacking unlabeled samples. Therefore, users of low-resource languages (e.g., legacy or new languages) miss out on the benefits of LLMs. Cross-lingual transfer learning uses data from a source language to improve model performance on a target language. It has been well-studied for natural languages, but has received little attention for programming languages. This paper reports extensive experiments on four tasks using a transformer-based LLM and 11 to 41 programming languages to explore the following questions. First, how well cross-lingual transfer 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#20351;&#29992;&#27969;&#21305;&#37197;&#30340;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#24182;&#33719;&#24471;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#22312;60k&#23567;&#26102;&#30340;&#26410;&#36716;&#24405;&#35821;&#38899;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#19982;&#29616;&#26377;&#30340;&#19987;&#23478;&#27169;&#22411;&#22312;&#35821;&#38899;&#22686;&#24378;&#12289;&#20998;&#31163;&#21644;&#21512;&#25104;&#26041;&#38754;&#36827;&#34892;&#21305;&#37197;&#25110;&#36229;&#36234;&#12290;</title><link>http://arxiv.org/abs/2310.16338</link><description>&lt;p&gt;
&#24102;&#26377;&#27969;&#21305;&#37197;&#30340;&#35821;&#38899;&#29983;&#25104;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Generative Pre-training for Speech with Flow Matching. (arXiv:2310.16338v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16338
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#20351;&#29992;&#27969;&#21305;&#37197;&#30340;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#24182;&#33719;&#24471;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#22312;60k&#23567;&#26102;&#30340;&#26410;&#36716;&#24405;&#35821;&#38899;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#19982;&#29616;&#26377;&#30340;&#19987;&#23478;&#27169;&#22411;&#22312;&#35821;&#38899;&#22686;&#24378;&#12289;&#20998;&#31163;&#21644;&#21512;&#25104;&#26041;&#38754;&#36827;&#34892;&#21305;&#37197;&#25110;&#36229;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#29983;&#25104;&#27169;&#22411;&#22312;&#38656;&#35201;&#20272;&#35745;&#21644;&#25277;&#26679;&#25968;&#25454;&#20998;&#24067;&#20197;&#29983;&#25104;&#39640;&#20445;&#30495;&#21512;&#25104;&#25968;&#25454;&#30340;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#22240;&#27492;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#22312;&#35821;&#38899;&#39046;&#22495;&#65292;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#21644;&#31070;&#32463;&#22768;&#30721;&#22120;&#26159;&#29983;&#25104;&#27169;&#22411;&#25104;&#21151;&#24212;&#29992;&#30340;&#20856;&#22411;&#20363;&#23376;&#12290;&#23613;&#31649;&#29983;&#25104;&#27169;&#22411;&#24050;&#32463;&#22312;&#35821;&#38899;&#30340;&#19981;&#21516;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#24212;&#29992;&#65292;&#20294;&#36824;&#27809;&#26377;&#19968;&#20010;&#36890;&#29992;&#30340;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#24314;&#27169;&#35821;&#38899;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#21333;&#19968;&#30340;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#24182;&#33719;&#24471;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#36808;&#20986;&#20102;&#36825;&#20010;&#26041;&#21521;&#30340;&#19968;&#27493;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;&#27969;&#21305;&#37197;&#21644;&#33945;&#29256;&#26465;&#20214;&#22312;60k&#23567;&#26102;&#30340;&#26410;&#36716;&#24405;&#35821;&#38899;&#19978;&#39044;&#35757;&#32451;&#20102;&#19968;&#20010;&#21517;&#20026;SpeechFlow&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#29305;&#23450;&#20219;&#21153;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#22312;&#35821;&#38899;&#22686;&#24378;&#12289;&#20998;&#31163;&#21644;&#21512;&#25104;&#26041;&#38754;&#36798;&#21040;&#25110;&#36229;&#36807;&#29616;&#26377;&#30340;&#19987;&#23478;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative models have gained more and more attention in recent years for their remarkable success in tasks that required estimating and sampling data distribution to generate high-fidelity synthetic data. In speech, text-to-speech synthesis and neural vocoder are good examples where generative models have shined. While generative models have been applied to different applications in speech, there exists no general-purpose generative model that models speech directly. In this work, we take a step toward this direction by showing a single pre-trained generative model can be adapted to different downstream tasks with strong performance. Specifically, we pre-trained a generative model, named SpeechFlow, on 60k hours of untranscribed speech with Flow Matching and masked conditions. Experiment results show the pre-trained generative model can be fine-tuned with task-specific data to match or surpass existing expert models on speech enhancement, separation, and synthesis. Our work suggested 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;COPF&#26041;&#27861;&#65292;&#25105;&#20204;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#26159;&#20351;&#29992;&#26368;&#20248;&#31574;&#30053;&#25311;&#21512;&#21644;&#20989;&#25968;&#27491;&#21017;&#21270;&#26469;&#25345;&#32493;&#23398;&#20064;&#21644;&#36866;&#24212;&#20154;&#31867;&#20559;&#22909;&#30340;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.15694</link><description>&lt;p&gt;
COPF: &#36890;&#36807;&#26368;&#20248;&#31574;&#30053;&#25311;&#21512;&#23454;&#29616;&#25345;&#32493;&#23398;&#20064;&#20154;&#31867;&#20559;&#22909;
&lt;/p&gt;
&lt;p&gt;
COPF: Continual Learning Human Preference through Optimal Policy Fitting. (arXiv:2310.15694v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15694
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;COPF&#26041;&#27861;&#65292;&#25105;&#20204;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#26159;&#20351;&#29992;&#26368;&#20248;&#31574;&#30053;&#25311;&#21512;&#21644;&#20989;&#25968;&#27491;&#21017;&#21270;&#26469;&#25345;&#32493;&#23398;&#20064;&#21644;&#36866;&#24212;&#20154;&#31867;&#20559;&#22909;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#30340;&#25216;&#26415;&#26159;&#25913;&#21892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#20197;&#31526;&#21512;&#20154;&#31867;&#20559;&#22909;&#30340;&#24120;&#29992;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#22522;&#20110;RLHF&#30340;LM&#22312;&#24341;&#20837;&#26032;&#30340;&#26597;&#35810;&#25110;&#21453;&#39304;&#26102;&#38656;&#35201;&#23436;&#20840;&#37325;&#26032;&#35757;&#32451;&#65292;&#36825;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#20154;&#31867;&#20559;&#22909;&#22312;&#19981;&#21516;&#39046;&#22495;&#25110;&#20219;&#21153;&#20043;&#38388;&#21487;&#33021;&#20250;&#26377;&#25152;&#21464;&#21270;&#12290;&#30001;&#20110;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#20197;&#21450;&#19982;&#25968;&#25454;&#38544;&#31169;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#37325;&#26032;&#35757;&#32451;LM&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;&#19979;&#23384;&#22312;&#23454;&#38469;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#25345;&#32493;&#26368;&#20248;&#31574;&#30053;&#25311;&#21512;&#65288;COPF&#65289;&#65292;&#20854;&#20013;&#25105;&#20204;&#20351;&#29992;&#33945;&#29305;&#21345;&#32599;&#27861;&#20272;&#35745;&#19968;&#31995;&#21015;&#26368;&#20248;&#31574;&#30053;&#65292;&#28982;&#21518;&#36890;&#36807;&#20989;&#25968;&#27491;&#21017;&#21270;&#19981;&#26029;&#25311;&#21512;&#31574;&#30053;&#24207;&#21015;&#12290;COPF&#21253;&#21547;&#19968;&#20010;&#21333;&#19968;&#30340;&#23398;&#20064;&#38454;&#27573;&#65292;&#19981;&#38656;&#35201;&#22797;&#26434;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The technique of Reinforcement Learning from Human Feedback (RLHF) is a commonly employed method to improve pre-trained Language Models (LM), enhancing their ability to conform to human preferences. Nevertheless, the current RLHF-based LMs necessitate full retraining each time novel queries or feedback are introduced, which becomes a challenging task because human preferences can vary between different domains or tasks. Retraining LMs poses practical difficulties in many real-world situations due to the significant time and computational resources required, along with concerns related to data privacy. To address this limitation, we propose a new method called Continual Optimal Policy Fitting (COPF), in which we estimate a series of optimal policies using the Monte Carlo method, and then continually fit the policy sequence with the function regularization. COPF involves a single learning phase and doesn't necessitate complex reinforcement learning. Importantly, it shares the capability 
&lt;/p&gt;</description></item><item><title>&#22312;&#20302;&#25968;&#25454;&#29615;&#22659;&#20013;&#65292;&#20351;&#29992;&#35270;&#35273;&#23450;&#20301;&#36827;&#34892;&#30417;&#30563;&#35757;&#32451;&#30340;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26356;&#25509;&#36817;&#20110;&#20154;&#31867;&#30340;&#35821;&#35328;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.13257</link><description>&lt;p&gt;
&#22312;&#20302;&#25968;&#25454;&#29615;&#22659;&#20013;&#65292;&#35270;&#35273;&#23450;&#20301;&#26377;&#21161;&#20110;&#23398;&#20064;&#21333;&#35789;&#30340;&#21547;&#20041;
&lt;/p&gt;
&lt;p&gt;
Visual Grounding Helps Learn Word Meanings in Low-Data Regimes. (arXiv:2310.13257v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13257
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20302;&#25968;&#25454;&#29615;&#22659;&#20013;&#65292;&#20351;&#29992;&#35270;&#35273;&#23450;&#20301;&#36827;&#34892;&#30417;&#30563;&#35757;&#32451;&#30340;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26356;&#25509;&#36817;&#20110;&#20154;&#31867;&#30340;&#35821;&#35328;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#26159;&#29992;&#20110;&#27169;&#25311;&#20154;&#31867;&#21477;&#23376;&#20135;&#29983;&#21644;&#29702;&#35299;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#20854;&#20869;&#37096;&#34920;&#36798;&#19982;&#20154;&#31867;&#22823;&#33041;&#20013;&#30340;&#35821;&#35328;&#34920;&#36798;&#38750;&#24120;&#21563;&#21512;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#21462;&#24471;&#36825;&#20123;&#32467;&#26524;&#65292;LM&#24517;&#39035;&#20197;&#19982;&#20154;&#31867;&#23436;&#20840;&#19981;&#21516;&#30340;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#65292;&#38656;&#35201;&#27604;&#20799;&#31461;&#22312;&#21457;&#32946;&#36807;&#31243;&#20013;&#25509;&#25910;&#21040;&#30340;&#35821;&#35328;&#25968;&#25454;&#22810;&#20960;&#20010;&#25968;&#37327;&#32423;&#65292;&#24182;&#19988;&#27809;&#26377;&#20219;&#20309;&#24863;&#30693;&#12289;&#34892;&#21160;&#25110;&#31038;&#20132;&#34892;&#20026;&#30340;&#22522;&#30784;&#12290;&#22914;&#26524;&#29992;&#26356;&#25509;&#36817;&#20154;&#31867;&#30340;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#65292;&#21363;&#20381;&#38752;&#24863;&#30693;&#30340;&#30417;&#30563;&#65292;&#27169;&#22411;&#30340;&#35821;&#35328;&#23398;&#20064;&#26159;&#21542;&#26356;&#25509;&#36817;&#20154;&#31867;&#65311;&#25105;&#20204;&#22312;&#21333;&#35789;&#23398;&#20064;&#36825;&#19968;&#35821;&#35328;&#20064;&#24471;&#30340;&#20851;&#38190;&#23376;&#20219;&#21153;&#20013;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#31995;&#21015;&#19981;&#21516;&#30340;LM&#26550;&#26500;&#65292;&#24182;&#22312;&#19981;&#21516;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#22270;&#20687;&#23383;&#24149;&#20219;&#21153;&#30340;&#36741;&#21161;&#30417;&#30563;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#21518;&#25105;&#20204;&#20351;&#29992;&#19968;&#31995;&#21015;&#24191;&#27867;&#30340;&#27979;&#35797;&#26469;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#22312;&#21477;&#27861;&#31867;&#21035;&#12289;&#35789;&#27719;&#20851;&#31995;&#12289;&#35821;&#20041;&#23398;&#31561;&#26041;&#38754;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern neural language models (LMs) are powerful tools for modeling human sentence production and comprehension, and their internal representations are remarkably well-aligned with representations of language in the human brain. But to achieve these results, LMs must be trained in distinctly un-human-like ways -- requiring orders of magnitude more language data than children receive during development, and without any of the accompanying grounding in perception, action, or social behavior. Do models trained more naturalistically -- with grounded supervision -- exhibit more human-like language learning? We investigate this question in the context of word learning, a key sub-task in language acquisition. We train a diverse set of LM architectures, with and without auxiliary supervision from image captioning tasks, on datasets of varying scales. We then evaluate these models on a broad set of benchmarks characterizing models' learning of syntactic categories, lexical relations, semantic f
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35774;&#35745;MOEA&#25805;&#20316;&#31526;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#25552;&#31034;&#24037;&#31243;&#65292;&#25104;&#21151;&#23558;&#36890;&#29992;&#30340;LLM&#20197;&#38646;-shot&#26041;&#24335;&#20316;&#20026;MOEA/D&#30340;&#40657;&#30418;&#25628;&#32034;&#25805;&#20316;&#31526;&#65292;&#24182;&#36890;&#36807;&#20174;LLM&#34892;&#20026;&#20013;&#23398;&#20064;&#35774;&#35745;&#20102;&#19968;&#20010;&#26174;&#24615;&#30340;&#30333;&#30418;&#25805;&#20316;&#31526;&#12290;</title><link>http://arxiv.org/abs/2310.12541</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#22810;&#30446;&#26631;&#36827;&#21270;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Large Language Model for Multi-objective Evolutionary Optimization. (arXiv:2310.12541v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35774;&#35745;MOEA&#25805;&#20316;&#31526;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#25552;&#31034;&#24037;&#31243;&#65292;&#25104;&#21151;&#23558;&#36890;&#29992;&#30340;LLM&#20197;&#38646;-shot&#26041;&#24335;&#20316;&#20026;MOEA/D&#30340;&#40657;&#30418;&#25628;&#32034;&#25805;&#20316;&#31526;&#65292;&#24182;&#36890;&#36807;&#20174;LLM&#34892;&#20026;&#20013;&#23398;&#20064;&#35774;&#35745;&#20102;&#19968;&#20010;&#26174;&#24615;&#30340;&#30333;&#30418;&#25805;&#20316;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861;&#65288;MOEAs&#65289;&#26159;&#35299;&#20915;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65288;MOPs&#65289;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#65292;&#25552;&#20986;&#20102;&#35768;&#22810;MOEAs&#65292;&#20854;&#25805;&#20316;&#31526;&#38656;&#35201;&#36890;&#36807;&#39046;&#22495;&#30693;&#35782;&#36827;&#34892;&#31934;&#24515;&#35774;&#35745;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#23581;&#35797;&#23558;MOEAs&#20013;&#25163;&#21160;&#35774;&#35745;&#30340;&#25805;&#20316;&#31526;&#26367;&#25442;&#20026;&#22522;&#20110;&#23398;&#20064;&#30340;&#25805;&#20316;&#31526;&#65288;&#22914;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65289;&#24050;&#32463;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#21644;&#35757;&#32451;&#36825;&#26679;&#30340;&#27169;&#22411;&#20173;&#28982;&#38656;&#35201;&#22823;&#37327;&#30340;&#24037;&#20316;&#65292;&#24182;&#19988;&#23398;&#20064;&#21040;&#30340;&#25805;&#20316;&#31526;&#21487;&#33021;&#19981;&#33021;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#35299;&#20915;&#26032;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#21033;&#29992;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#35774;&#35745;MOEA&#25805;&#20316;&#31526;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#36866;&#24403;&#30340;&#25552;&#31034;&#24037;&#31243;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#35753;&#19968;&#20010;&#36890;&#29992;&#30340;LLM&#20197;&#38646;-shot&#30340;&#26041;&#24335;&#20316;&#20026;&#20998;&#35299;&#22411;MOEA&#65288;MOEA/D&#65289;&#30340;&#40657;&#30418;&#25628;&#32034;&#25805;&#20316;&#31526;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20174;LLM&#34892;&#20026;&#20013;&#23398;&#20064;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35774;&#35745;&#20102;&#19968;&#20010;&#26174;&#24615;&#30340;&#30333;&#30418;&#25805;&#20316;&#31526;&#65292;&#24182;&#25552;&#20986;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Multiobjective evolutionary algorithms (MOEAs) are major methods for solving multiobjective optimization problems (MOPs). Many MOEAs have been proposed in the past decades, of which the operators need carefully handcrafted design with domain knowledge. Recently, some attempts have been made to replace the manually designed operators in MOEAs with learning-based operators (e.g., neural network models). However, much effort is still required for designing and training such models, and the learned operators might not generalize well to solve new problems. To tackle the above challenges, this work investigates a novel approach that leverages the powerful large language model (LLM) to design MOEA operators. With proper prompt engineering, we successfully let a general LLM serve as a black-box search operator for decomposition-based MOEA (MOEA/D) in a zero-shot manner. In addition, by learning from the LLM behavior, we further design an explicit white-box operator with randomness and propose
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#21477;&#23376;&#32423;&#21035;&#36827;&#34892;&#35821;&#20041;&#20998;&#31867;&#65292;&#20197;&#21152;&#36895;&#20154;&#25991;&#23398;&#31185;&#21644;&#35821;&#35328;&#23398;&#39046;&#22495;&#20013;&#35821;&#26009;&#24211;&#24314;&#35774;&#30340;&#36807;&#31243;&#12290;&#32463;&#36807;&#35780;&#20272;&#65292;&#35813;&#26041;&#27861;&#22312;&#26816;&#27979;&#24615;&#20869;&#23481;&#26041;&#38754;&#34920;&#29616;&#20986;&#39640;&#31934;&#24230;&#21644;&#30495;&#38451;&#24615;&#29575;&#65292;&#24182;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#36755;&#20837;&#23884;&#20837;&#23618;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.14974</link><description>&lt;p&gt;
&#22312;&#19968;&#21315;&#24180;&#21069;&#30340;&#25289;&#19969;&#25991;&#26412;&#20013;&#26816;&#27979;&#21477;&#23376;&#32423;&#21035;&#30340;&#24615;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
Detecting Sexual Content at the Sentence Level in First Millennium Latin Texts. (arXiv:2309.14974v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14974
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#21477;&#23376;&#32423;&#21035;&#36827;&#34892;&#35821;&#20041;&#20998;&#31867;&#65292;&#20197;&#21152;&#36895;&#20154;&#25991;&#23398;&#31185;&#21644;&#35821;&#35328;&#23398;&#39046;&#22495;&#20013;&#35821;&#26009;&#24211;&#24314;&#35774;&#30340;&#36807;&#31243;&#12290;&#32463;&#36807;&#35780;&#20272;&#65292;&#35813;&#26041;&#27861;&#22312;&#26816;&#27979;&#24615;&#20869;&#23481;&#26041;&#38754;&#34920;&#29616;&#20986;&#39640;&#31934;&#24230;&#21644;&#30495;&#38451;&#24615;&#29575;&#65292;&#24182;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#36755;&#20837;&#23884;&#20837;&#23618;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#21477;&#23376;&#32423;&#21035;&#36827;&#34892;&#35821;&#20041;&#20998;&#31867;&#65292;&#20197;&#21152;&#24555;&#20154;&#25991;&#23398;&#31185;&#21644;&#35821;&#35328;&#23398;&#39046;&#22495;&#20013;&#35821;&#26009;&#24211;&#24314;&#35774;&#30340;&#36807;&#31243;&#65292;&#36825;&#26159;&#19968;&#39033;&#20256;&#32479;&#19988;&#32791;&#26102;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35821;&#26009;&#24211;&#65292;&#21253;&#25324;&#32422;2500&#20010;&#21477;&#23376;&#65292;&#28085;&#30422;&#20102;&#20174;&#20844;&#20803;&#21069;300&#24180;&#21040;&#20844;&#20803;900&#24180;&#30340;&#24615;&#35821;&#20041;&#23398;&#65288;&#21307;&#23398;&#65292;&#24773;&#33394;&#31561;&#65289;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#21508;&#31181;&#21477;&#23376;&#20998;&#31867;&#26041;&#27861;&#21644;&#19981;&#21516;&#30340;&#36755;&#20837;&#23884;&#20837;&#23618;&#65292;&#24182;&#34920;&#26126;&#23427;&#20204;&#37117;&#27604;&#31616;&#21333;&#30340;&#22522;&#20110;&#26631;&#35760;&#30340;&#25628;&#32034;&#26041;&#27861;&#26356;&#22909;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#20010;&#20154;&#35328;&#35821;&#21644;&#31038;&#20250;&#35328;&#35821;&#20803;&#25968;&#25454;&#23884;&#20837;&#65288;&#19990;&#32426;&#65292;&#20316;&#32773;&#65292;&#20889;&#20316;&#31867;&#22411;&#65289;&#30340;&#25972;&#21512;&#65292;&#20294;&#21457;&#29616;&#36825;&#23548;&#33268;&#20102;&#36807;&#25311;&#21512;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20351;&#29992;HAN&#20998;&#21035;&#36798;&#21040;&#20102;70.60%&#30340;&#39640;&#31934;&#24230;&#21644;86.33%&#30340;&#30495;&#38451;&#24615;&#29575;&#65288;TPR&#65289;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25968;&#25454;&#38598;&#22823;&#23567;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65288;420&#32780;&#19981;&#26159;2013&#65289;&#65292;&#24182;&#26174;&#31034;&#20986;&#65292;&#23613;&#31649;&#25105;&#20204;&#30340;&#27169;&#22411;&#24615;&#33021;&#21487;&#33021;&#31245;&#26377;&#19979;&#38477;&#65292;&#20294;&#24615;&#33021;&#20173;&#28982;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we propose to evaluate the use of deep learning methods for semantic classification at the sentence level to accelerate the process of corpus building in the field of humanities and linguistics, a traditional and time-consuming task. We introduce a novel corpus comprising around 2500 sentences spanning from 300 BCE to 900 CE including sexual semantics (medical, erotica, etc.). We evaluate various sentence classification approaches and different input embedding layers, and show that all consistently outperform simple token-based searches. We explore the integration of idiolectal and sociolectal metadata embeddings (centuries, author, type of writing), but find that it leads to overfitting. Our results demonstrate the effectiveness of this approach, achieving high precision and true positive rates (TPR) of respectively 70.60% and 86.33% using HAN. We evaluate the impact of the dataset size on the model performances (420 instead of 2013), and show that, while our models per
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#21516;&#26102;&#35299;&#26512;&#30701;&#35821;&#32467;&#26500;&#26641;&#21644;&#20381;&#23384;&#26641;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#26356;&#39640;&#25928;&#30340;&#35299;&#30721;&#31639;&#27861;&#12289;&#22312;&#35757;&#32451;&#38454;&#27573;&#36827;&#34892;&#32852;&#21512;&#24314;&#27169;&#12289;&#25552;&#20986;&#39640;&#38454;&#35780;&#20998;&#32452;&#20214;&#20197;&#21450;&#36827;&#34892;&#28145;&#20837;&#23454;&#39564;&#21644;&#20998;&#26512;&#31561;&#22235;&#20010;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#28508;&#21147;&#21644;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2309.11888</link><description>&lt;p&gt;
&#21516;&#26102;&#35299;&#26512;&#30701;&#35821;&#32467;&#26500;&#26641;&#21644;&#20381;&#23384;&#26641;&#30495;&#30340;&#26377;&#29992;&#21527;&#65311;&#37325;&#26032;&#23457;&#35270;
&lt;/p&gt;
&lt;p&gt;
Is It Really Useful to Jointly Parse Constituency and Dependency Trees? A Revisit. (arXiv:2309.11888v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11888
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#21516;&#26102;&#35299;&#26512;&#30701;&#35821;&#32467;&#26500;&#26641;&#21644;&#20381;&#23384;&#26641;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#26356;&#39640;&#25928;&#30340;&#35299;&#30721;&#31639;&#27861;&#12289;&#22312;&#35757;&#32451;&#38454;&#27573;&#36827;&#34892;&#32852;&#21512;&#24314;&#27169;&#12289;&#25552;&#20986;&#39640;&#38454;&#35780;&#20998;&#32452;&#20214;&#20197;&#21450;&#36827;&#34892;&#28145;&#20837;&#23454;&#39564;&#21644;&#20998;&#26512;&#31561;&#22235;&#20010;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#28508;&#21147;&#21644;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#21516;&#26102;&#35299;&#26512;&#30701;&#35821;&#32467;&#26500;&#26641;&#21644;&#20381;&#23384;&#26641;&#36825;&#19968;&#35805;&#39064;&#65292;&#21363;&#20026;&#36755;&#20837;&#21477;&#23376;&#21516;&#26102;&#29983;&#25104;&#20860;&#23481;&#30340;&#30701;&#35821;&#32467;&#26500;&#26641;&#21644;&#20381;&#23384;&#26641;&#65292;&#32771;&#34385;&#21040;&#36825;&#20004;&#31181;&#31867;&#22411;&#30340;&#26641;&#22312;&#34920;&#31034;&#35821;&#27861;&#26041;&#38754;&#26159;&#20114;&#34917;&#30340;&#65292;&#36825;&#26159;&#19968;&#31181;&#26377;&#21560;&#24341;&#21147;&#30340;&#26041;&#27861;&#12290;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#25105;&#20204;&#22312;&#22235;&#20010;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65306;&#65288;1&#65289;&#37319;&#29992;&#26356;&#39640;&#25928;&#30340;&#35299;&#30721;&#31639;&#27861;&#65292;&#65288;2&#65289;&#22312;&#35757;&#32451;&#38454;&#27573;&#36827;&#34892;&#32852;&#21512;&#24314;&#27169;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#22312;&#25512;&#29702;&#38454;&#27573;&#65292;&#65288;3&#65289;&#20026;&#30701;&#35821;&#32467;&#26500;&#21644;&#20381;&#23384;&#20043;&#38388;&#30340;&#20132;&#20114;&#25552;&#20986;&#20102;&#39640;&#38454;&#35780;&#20998;&#32452;&#20214;&#65292;&#65288;4&#65289;&#36890;&#36807;&#28145;&#20837;&#23454;&#39564;&#21644;&#20998;&#26512;&#33719;&#24471;&#20102;&#26356;&#22810;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work visits the topic of jointly parsing constituency and dependency trees, i.e., to produce compatible constituency and dependency trees simultaneously for input sentences, which is attractive considering that the two types of trees are complementary in representing syntax. Compared with previous works, we make progress in four aspects: (1) adopting a much more efficient decoding algorithm, (2) exploring joint modeling at the training phase, instead of only at the inference phase, (3) proposing high-order scoring components for constituent-dependency interaction, (4) gaining more insights via in-depth experiments and analysis.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26681;&#25454;&#31185;&#23398;&#25688;&#35201;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#26469;&#36776;&#21035;&#25903;&#25345;&#25110;&#21453;&#39539;&#29305;&#23450;&#20551;&#35774;&#30340;&#35777;&#25454;&#12290;&#36890;&#36807;&#31038;&#21306;&#39537;&#21160;&#30340;&#27880;&#37322;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#38024;&#23545;&#31038;&#20250;&#31185;&#23398;&#20013;&#30340;&#31185;&#23398;&#20551;&#35774;&#35777;&#25454;&#20219;&#21153;&#12290;&#19982;&#20854;&#20182;&#22522;&#20934;&#36827;&#34892;&#20102;&#24615;&#33021;&#27604;&#36739;&#65292;&#24182;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2309.06578</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#36776;&#21035;&#31185;&#23398;&#20551;&#35774;&#30340;&#35777;&#25454;&#65311;&#31038;&#20250;&#31185;&#23398;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Discern Evidence for Scientific Hypotheses? Case Studies in the Social Sciences. (arXiv:2309.06578v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06578
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26681;&#25454;&#31185;&#23398;&#25688;&#35201;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#26469;&#36776;&#21035;&#25903;&#25345;&#25110;&#21453;&#39539;&#29305;&#23450;&#20551;&#35774;&#30340;&#35777;&#25454;&#12290;&#36890;&#36807;&#31038;&#21306;&#39537;&#21160;&#30340;&#27880;&#37322;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#38024;&#23545;&#31038;&#20250;&#31185;&#23398;&#20013;&#30340;&#31185;&#23398;&#20551;&#35774;&#35777;&#25454;&#20219;&#21153;&#12290;&#19982;&#20854;&#20182;&#22522;&#20934;&#36827;&#34892;&#20102;&#24615;&#33021;&#27604;&#36739;&#65292;&#24182;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20551;&#35774;&#30340;&#21046;&#23450;&#21644;&#27979;&#35797;&#26159;&#32463;&#39564;&#24615;&#30740;&#31350;&#30340;&#26680;&#24515;&#12290;&#19968;&#20010;&#24378;&#26377;&#21147;&#30340;&#20551;&#35774;&#26159;&#22522;&#20110;&#29616;&#26377;&#35777;&#25454;&#30340;&#26368;&#20339;&#29468;&#27979;&#65292;&#24182;&#19988;&#26159;&#22522;&#20110;&#30456;&#20851;&#25991;&#29486;&#30340;&#20840;&#38754;&#35270;&#22270;&#36827;&#34892;&#21551;&#21457;&#30340;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#27599;&#24180;&#31185;&#23398;&#25991;&#31456;&#25968;&#37327;&#30340;&#25351;&#25968;&#22686;&#38271;&#65292;&#23545;&#20110;&#32473;&#23450;&#20551;&#35774;&#30456;&#20851;&#35777;&#25454;&#30340;&#25163;&#21160;&#27719;&#24635;&#21644;&#32508;&#21512;&#26159;&#19968;&#39033;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25506;&#32034;&#20102;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26681;&#25454;&#31185;&#23398;&#25688;&#35201;&#25991;&#26412;&#20013;&#30340;&#35777;&#25454;&#65292;&#33021;&#21542;&#36776;&#21035;&#25903;&#25345;&#25110;&#21453;&#39539;&#29305;&#23450;&#20551;&#35774;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20849;&#20139;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#31038;&#20250;&#31185;&#23398;&#20013;&#20351;&#29992;&#31038;&#21306;&#39537;&#21160;&#30340;&#30740;&#31350;&#27880;&#37322;&#30340;&#31185;&#23398;&#20551;&#35774;&#35777;&#25454;&#20219;&#21153;&#12290;&#25105;&#20204;&#23558;LLMs&#30340;&#24615;&#33021;&#19982;&#20960;&#20010;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#25351;&#20986;&#26410;&#26469;&#30740;&#31350;&#30340;&#26426;&#20250;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#22312;https://github.com/Sai90000/ScientificHypothesisEvidencing.git&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypothesis formulation and testing are central to empirical research. A strong hypothesis is a best guess based on existing evidence and informed by a comprehensive view of relevant literature. However, with exponential increase in the number of scientific articles published annually, manual aggregation and synthesis of evidence related to a given hypothesis is a challenge. Our work explores the ability of current large language models (LLMs) to discern evidence in support or refute of specific hypotheses based on the text of scientific abstracts. We share a novel dataset for the task of scientific hypothesis evidencing using community-driven annotations of studies in the social sciences. We compare the performance of LLMs to several state-of-the-art benchmarks and highlight opportunities for future research in this area. The dataset is available at https://github.com/Sai90000/ScientificHypothesisEvidencing.git
&lt;/p&gt;</description></item><item><title>BAN-PL&#26159;&#27874;&#20848;&#35821;&#30340;&#31532;&#19968;&#20010;&#24320;&#25918;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#26469;&#33258;Wykop&#36825;&#20010;&#31867;&#20284;"&#27874;&#20848;&#29256;Reddit"&#30340;&#31038;&#20132;&#32593;&#32476;&#26381;&#21153;&#30340;&#34987;&#26631;&#35760;&#20026;&#26377;&#23475;&#24182;&#21024;&#38500;&#30340;&#20869;&#23481;&#65292;&#23558;&#26377;&#21161;&#20110;&#25913;&#36827;&#33258;&#21160;&#26816;&#27979;&#20114;&#32852;&#32593;&#19978;&#30340;&#20882;&#29359;&#24615;&#35821;&#35328;&#30340;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2308.10592</link><description>&lt;p&gt;
BAN-PL: &#19968;&#20221;&#26469;&#33258;Wykop.pl&#32593;&#31449;&#30340;&#31105;&#27490;&#26377;&#23475;&#21644;&#25915;&#20987;&#24615;&#20869;&#23481;&#30340;&#26032;&#27874;&#20848;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
BAN-PL: a Novel Polish Dataset of Banned Harmful and Offensive Content from Wykop.pl web service. (arXiv:2308.10592v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10592
&lt;/p&gt;
&lt;p&gt;
BAN-PL&#26159;&#27874;&#20848;&#35821;&#30340;&#31532;&#19968;&#20010;&#24320;&#25918;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#26469;&#33258;Wykop&#36825;&#20010;&#31867;&#20284;"&#27874;&#20848;&#29256;Reddit"&#30340;&#31038;&#20132;&#32593;&#32476;&#26381;&#21153;&#30340;&#34987;&#26631;&#35760;&#20026;&#26377;&#23475;&#24182;&#21024;&#38500;&#30340;&#20869;&#23481;&#65292;&#23558;&#26377;&#21161;&#20110;&#25913;&#36827;&#33258;&#21160;&#26816;&#27979;&#20114;&#32852;&#32593;&#19978;&#30340;&#20882;&#29359;&#24615;&#35821;&#35328;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#26816;&#27979;&#20114;&#32852;&#32593;&#19978;&#30340;&#20882;&#29359;&#24615;&#35821;&#35328;&#12289;&#20167;&#24680;&#35328;&#35770;&#21644;&#32593;&#32476;&#27450;&#20940;&#26041;&#38754;&#21462;&#24471;&#30340;&#36827;&#23637;&#38656;&#35201;&#25913;&#36827;&#23545;&#21253;&#21547;&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#30340;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#30340;&#35775;&#38382;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;BAN-PL&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20197;&#27874;&#20848;&#35821;&#25552;&#20379;&#30340;&#24320;&#25918;&#25968;&#25454;&#38598;&#65292;&#23427;&#21253;&#21547;&#20102;&#34987;&#19987;&#19994;&#23457;&#26597;&#21592;&#26631;&#35760;&#20026;&#26377;&#23475;&#24182;&#38543;&#21518;&#34987;&#21024;&#38500;&#30340;&#25991;&#26412;&#12290;&#25968;&#25454;&#38598;&#20849;&#21253;&#21547;&#26469;&#33258;Wykop&#36825;&#20010;&#39047;&#21463;&#27426;&#36814;&#30340;&#31038;&#20132;&#32593;&#32476;&#26381;&#21153;&#30340;691,662&#26465;&#20869;&#23481;&#65292;&#20854;&#20013;&#21253;&#25324;&#24086;&#23376;&#21644;&#35780;&#35770;&#65292;&#24182;&#19988;&#24179;&#22343;&#20998;&#20026;&#20004;&#20010;&#19981;&#21516;&#30340;&#31867;&#21035;&#65306;&#8220;&#26377;&#23475;&#8221;&#21644;&#8220;&#20013;&#31435;&#8221;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25968;&#25454;&#25910;&#38598;&#21644;&#39044;&#22788;&#29702;&#31243;&#24207;&#30340;&#35814;&#32454;&#35828;&#26126;&#65292;&#24182;&#24378;&#35843;&#20102;&#25968;&#25454;&#30340;&#35821;&#35328;&#29305;&#27530;&#24615;&#12290;BAN-PL&#25968;&#25454;&#38598;&#20197;&#21450;&#29992;&#20110;&#39044;&#22788;&#29702;&#33039;&#35805;&#30340;&#39640;&#32423;&#33050;&#26412;&#23558;&#20844;&#24320;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advances in automated detection of offensive language online, including hate speech and cyberbullying, require improved access to publicly available datasets comprising social media content. In this paper, we introduce BAN-PL, the first open dataset in the Polish language that encompasses texts flagged as harmful and subsequently removed by professional moderators. The dataset encompasses a total of 691,662 pieces of content from a popular social networking service, Wykop, often referred to as the "Polish Reddit", including both posts and comments, and is evenly distributed into two distinct classes: "harmful" and "neutral". We provide a comprehensive description of the data collection and preprocessing procedures, as well as highlight the linguistic specificity of the data. The BAN-PL dataset, along with advanced preprocessing scripts for, i.a., unmasking profanities, will be publicly available.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#20351;&#29992;&#23494;&#30721;&#36827;&#34892;&#32842;&#22825;&#21487;&#20197;&#32469;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23433;&#20840;&#23545;&#40784;&#25216;&#26415;&#12290;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CipherChat&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#31995;&#32479;&#22320;&#26816;&#26597;&#23433;&#20840;&#23545;&#40784;&#22312;&#38750;&#33258;&#28982;&#35821;&#35328;&#65288;&#23494;&#30721;&#65289;&#20013;&#30340;&#26222;&#36866;&#24615;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35780;&#20272;&#20102;ChatGPT&#21644;GPT-4&#31561;&#26368;&#20808;&#36827;&#30340;LLMs&#23545;&#19981;&#21516;&#20195;&#34920;&#24615;&#20154;&#31867;&#23494;&#30721;&#22312;11&#20010;&#23433;&#20840;&#39046;&#22495;&#20013;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.06463</link><description>&lt;p&gt;
GPT-4&#22826;&#32874;&#26126;&#20197;&#33267;&#20110;&#19981;&#23433;&#20840;&#65306;&#36890;&#36807;&#23494;&#30721;&#19982;LLMs&#36827;&#34892;&#38544;&#34109;&#32842;&#22825;
&lt;/p&gt;
&lt;p&gt;
GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher. (arXiv:2308.06463v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06463
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#20351;&#29992;&#23494;&#30721;&#36827;&#34892;&#32842;&#22825;&#21487;&#20197;&#32469;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23433;&#20840;&#23545;&#40784;&#25216;&#26415;&#12290;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CipherChat&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#31995;&#32479;&#22320;&#26816;&#26597;&#23433;&#20840;&#23545;&#40784;&#22312;&#38750;&#33258;&#28982;&#35821;&#35328;&#65288;&#23494;&#30721;&#65289;&#20013;&#30340;&#26222;&#36866;&#24615;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35780;&#20272;&#20102;ChatGPT&#21644;GPT-4&#31561;&#26368;&#20808;&#36827;&#30340;LLMs&#23545;&#19981;&#21516;&#20195;&#34920;&#24615;&#20154;&#31867;&#23494;&#30721;&#22312;11&#20010;&#23433;&#20840;&#39046;&#22495;&#20013;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#24615;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24320;&#21457;&#30340;&#26680;&#24515;&#12290;&#20851;&#20110;&#23558;LLMs&#19982;&#20154;&#31867;&#20262;&#29702;&#21644;&#20559;&#22909;&#36827;&#34892;&#23545;&#40784;&#30340;&#24037;&#20316;&#24050;&#32463;&#24456;&#22810;&#65292;&#21253;&#25324;&#22312;&#39044;&#35757;&#32451;&#20013;&#36827;&#34892;&#25968;&#25454;&#31579;&#36873;&#12289;&#36890;&#36807;&#30417;&#30563;&#24494;&#35843;&#12289;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#20197;&#21450;&#32418;&#38431;&#27979;&#35797;&#31561;&#31561;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#23494;&#30721;&#36827;&#34892;&#32842;&#22825;&#21487;&#20197;&#32469;&#36807;LLMs&#30340;&#23433;&#20840;&#23545;&#40784;&#25216;&#26415;&#65292;&#36825;&#20123;&#25216;&#26415;&#20027;&#35201;&#26159;&#22312;&#33258;&#28982;&#35821;&#35328;&#20013;&#36827;&#34892;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;CipherChat&#65292;&#29992;&#20110;&#31995;&#32479;&#22320;&#26816;&#26597;&#23433;&#20840;&#23545;&#40784;&#22312;&#38750;&#33258;&#28982;&#35821;&#35328;&#65288;&#23494;&#30721;&#65289;&#20013;&#30340;&#26222;&#36866;&#24615;&#12290;CipherChat&#20351;&#20154;&#20204;&#33021;&#22815;&#36890;&#36807;&#21152;&#23494;&#25552;&#31034;&#21644;&#23569;&#37327;&#21152;&#23494;&#28436;&#31034;&#19982;LLMs&#36827;&#34892;&#32842;&#22825;&#12290;&#25105;&#20204;&#20351;&#29992;CipherChat&#22312;&#33521;&#35821;&#21644;&#20013;&#25991;&#20013;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;LLMs&#65292;&#21253;&#25324;ChatGPT&#21644;GPT-4&#22312;11&#20010;&#23433;&#20840;&#39046;&#22495;&#20013;&#30340;&#19981;&#21516;&#20195;&#34920;&#24615;&#20154;&#31867;&#23494;&#30721;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#26576;&#20123;&#23494;&#30721;&#25104;&#21151;&#22320;&#32469;&#36807;&#20102;&#23433;&#20840;&#23545;&#40784;&#25216;&#26415;&#65292;&#20960;&#20046;100%&#30340;&#26102;&#38388;&#37117;&#33021;&#22815;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;
Safety lies at the core of the development of Large Language Models (LLMs). There is ample work on aligning LLMs with human ethics and preferences, including data filtering in pretraining, supervised fine-tuning, reinforcement learning from human feedback, and red teaming, etc. In this study, we discover that chat in cipher can bypass the safety alignment techniques of LLMs, which are mainly conducted in natural languages. We propose a novel framework CipherChat to systematically examine the generalizability of safety alignment to non-natural languages -- ciphers. CipherChat enables humans to chat with LLMs through cipher prompts topped with system role descriptions and few-shot enciphered demonstrations. We use CipherChat to assess state-of-the-art LLMs, including ChatGPT and GPT-4 for different representative human ciphers across 11 safety domains in both English and Chinese. Experimental results show that certain ciphers succeed almost 100% of the time to bypass the safety alignment
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#22120;-&#26816;&#32034;&#22120;-&#29983;&#25104;&#22120;&#65288;GRG&#65289;&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#25991;&#26723;&#26816;&#32034;&#25216;&#26415;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20197;&#29983;&#25104;&#24320;&#25918;&#22495;&#38382;&#31572;&#30340;&#20934;&#30830;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#31572;&#26696;&#12290;</title><link>http://arxiv.org/abs/2307.11278</link><description>&lt;p&gt;
&#29983;&#25104;&#22120;-&#26816;&#32034;&#22120;-&#29983;&#25104;&#22120;&#65306;&#24320;&#25918;&#22495;&#38382;&#31572;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Generator-Retriever-Generator: A Novel Approach to Open-domain Question Answering. (arXiv:2307.11278v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11278
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#22120;-&#26816;&#32034;&#22120;-&#29983;&#25104;&#22120;&#65288;GRG&#65289;&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#25991;&#26723;&#26816;&#32034;&#25216;&#26415;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20197;&#29983;&#25104;&#24320;&#25918;&#22495;&#38382;&#31572;&#30340;&#20934;&#30830;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#22495;&#38382;&#31572;&#20219;&#21153;&#36890;&#24120;&#38656;&#35201;&#20174;&#22823;&#22411;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#30456;&#20851;&#20449;&#24687;&#20197;&#29983;&#25104;&#20934;&#30830;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#29983;&#25104;&#22120;-&#26816;&#32034;&#22120;-&#29983;&#25104;&#22120;&#65288;GRG&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#23558;&#25991;&#26723;&#26816;&#32034;&#25216;&#26415;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30456;&#32467;&#21512;&#65292;&#39318;&#20808;&#36890;&#36807;&#32473;&#23450;&#38382;&#39064;&#25552;&#31034;&#27169;&#22411;&#29983;&#25104;&#19978;&#19979;&#25991;&#25991;&#26723;&#12290;&#21516;&#26102;&#65292;&#21452;&#32534;&#30721;&#22120;&#32593;&#32476;&#20174;&#22806;&#37096;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#25991;&#26723;&#12290;&#29983;&#25104;&#21644;&#26816;&#32034;&#30340;&#25991;&#26723;&#28982;&#21518;&#20256;&#36882;&#32473;&#31532;&#20108;&#20010;LLM&#65292;&#29983;&#25104;&#26368;&#32456;&#31572;&#26696;&#12290;&#36890;&#36807;&#32467;&#21512;&#25991;&#26723;&#26816;&#32034;&#21644;LLM&#29983;&#25104;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#24320;&#25918;&#22495;&#38382;&#31572;&#30340;&#25361;&#25112;&#65292;&#20363;&#22914;&#29983;&#25104;&#20449;&#24687;&#20016;&#23500;&#21644;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#31572;&#26696;&#12290;GRG&#22312;TriviaQA&#12289;NQ&#21644;WebQ&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#29983;&#25104;-&#35835;&#21462;&#21644;&#26816;&#32034;-&#35835;&#21462;&#27969;&#27700;&#32447;&#65288;GENREAD&#21644;RFiD&#65289;&#65292;&#20998;&#21035;&#33267;&#23569;&#25552;&#39640;&#20102;+5.2&#12289;+4.2&#21644;+1.6&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open-domain question answering (QA) tasks usually require the retrieval of relevant information from a large corpus to generate accurate answers. We propose a novel approach called Generator-Retriever-Generator (GRG) that combines document retrieval techniques with a large language model (LLM), by first prompting the model to generate contextual documents based on a given question. In parallel, a dual-encoder network retrieves documents that are relevant to the question from an external corpus. The generated and retrieved documents are then passed to the second LLM, which generates the final answer. By combining document retrieval and LLM generation, our approach addresses the challenges of open-domain QA, such as generating informative and contextually relevant answers. GRG outperforms the state-of-the-art generate-then-read and retrieve-then-read pipelines (GENREAD and RFiD) improving their performance at least by +5.2, +4.2, and +1.6 on TriviaQA, NQ, and WebQ datasets, respectively.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;2.1M&#33521;&#35821;Yelp&#35780;&#35770;&#30340;&#39184;&#21381;&#36827;&#34892;&#35821;&#35328;&#20998;&#26512;&#65292;&#30740;&#31350;&#21457;&#29616;&#31227;&#27665;&#32654;&#39135;&#26356;&#23481;&#26131;&#34987;&#26500;&#26550;&#20026;&#23458;&#35266;&#21644;&#20182;&#32773;&#21270;&#65292;&#32780;&#38750;&#35199;&#26041;&#31227;&#27665;&#32654;&#39135;&#21463;&#27426;&#36814;&#31243;&#24230;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2307.07645</link><description>&lt;p&gt;
&#32654;&#22269;&#39184;&#21381;&#35780;&#35770;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31227;&#27665;&#32654;&#39135;&#20182;&#32773;&#21270;&#21644;&#20302;&#22768;&#26395;&#26500;&#26550;
&lt;/p&gt;
&lt;p&gt;
Othering and low prestige framing of immigrant cuisines in US restaurant reviews and large language models. (arXiv:2307.07645v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07645
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;2.1M&#33521;&#35821;Yelp&#35780;&#35770;&#30340;&#39184;&#21381;&#36827;&#34892;&#35821;&#35328;&#20998;&#26512;&#65292;&#30740;&#31350;&#21457;&#29616;&#31227;&#27665;&#32654;&#39135;&#26356;&#23481;&#26131;&#34987;&#26500;&#26550;&#20026;&#23458;&#35266;&#21644;&#20182;&#32773;&#21270;&#65292;&#32780;&#38750;&#35199;&#26041;&#31227;&#27665;&#32654;&#39135;&#21463;&#27426;&#36814;&#31243;&#24230;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#21644;&#29702;&#35299;&#23545;&#39135;&#29289;&#30340;&#38544;&#21547;&#24577;&#24230;&#26377;&#21161;&#20110;&#20943;&#36731;&#22240;&#39135;&#29289;&#20316;&#20026;&#25991;&#21270;&#21644;&#31181;&#26063;&#36523;&#20221;&#30340;&#26631;&#24535;&#32780;&#23548;&#33268;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#23545;&#39135;&#29289;&#30340;&#21051;&#26495;&#21360;&#35937;&#26159;&#19968;&#31181;&#24494;&#20405;&#30053;&#65292;&#23427;&#23545;&#26377;&#23475;&#30340;&#20844;&#20849;&#35805;&#35821;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#36825;&#21487;&#33021;&#21453;&#36807;&#26469;&#21152;&#28145;&#23545;&#27665;&#26063;&#32676;&#20307;&#30340;&#20559;&#35265;&#65292;&#24182;&#23545;&#39184;&#39302;&#30340;&#32463;&#27982;&#32467;&#26524;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#36890;&#36807;&#20180;&#32454;&#30340;&#35821;&#35328;&#20998;&#26512;&#65292;&#25105;&#20204;&#22312;&#19968;&#39033;&#22823;&#35268;&#27169;&#30740;&#31350;&#20013;&#35780;&#20272;&#20102;&#23545;&#31227;&#27665;&#32654;&#39135;&#24577;&#24230;&#30340;&#31038;&#20250;&#29702;&#35770;&#12290;&#35813;&#30740;&#31350;&#20351;&#29992;&#20102;2.1M&#33521;&#35821;Yelp&#35780;&#35770;&#30340;&#39184;&#21381;&#22312;14&#20010;&#32654;&#22269;&#24030;&#30340;&#26694;&#26550;&#24046;&#24322;&#12290;&#22312;&#25511;&#21046;&#20102;&#39184;&#21381;&#20215;&#26684;&#21644;&#37051;&#37324;&#31181;&#26063;&#22810;&#26679;&#24615;&#31561;&#22240;&#32032;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;&#31227;&#27665;&#32654;&#39135;&#26356;&#26377;&#21487;&#33021;&#20197;&#23458;&#35266;&#21644;&#20182;&#32773;&#21270;&#30340;&#24418;&#24335;&#36827;&#34892;&#26500;&#26550;&#65292;&#22914;&#30495;&#23454;&#24615;&#65288;&#20363;&#22914;&#65292;&#30495;&#23454;&#65292;&#20256;&#32479;&#65289;&#65292;&#24322;&#22269;&#24773;&#35843;&#65288;&#20363;&#22914;&#65292;&#24322;&#22269;&#65292;&#19981;&#21516;&#65289;&#21644;&#20856;&#22411;&#24615;&#65288;&#20363;&#22914;&#65292;&#20856;&#22411;&#65292;&#36890;&#24120;&#65289;&#12290;&#20294;&#38750;&#35199;&#26041;&#31227;&#27665;&#32654;&#39135;&#65288;&#20363;&#22914;&#65292;&#21360;&#24230;&#65292;&#22696;&#35199;&#21733;&#65289;&#26356;&#21463;&#27426;&#36814;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying and understanding implicit attitudes toward food can help efforts to mitigate social prejudice due to food's pervasive role as a marker of cultural and ethnic identity. Stereotypes about food are a form of microaggression that contribute to harmful public discourse that may in turn perpetuate prejudice toward ethnic groups and negatively impact economic outcomes for restaurants. Through careful linguistic analyses, we evaluate social theories about attitudes toward immigrant cuisine in a large-scale study of framing differences in 2.1M English language Yelp reviews of restaurants in 14 US states. Controlling for factors such as restaurant price and neighborhood racial diversity, we find that immigrant cuisines are more likely to be framed in objectifying and othering terms of authenticity (e.g., authentic, traditional), exoticism (e.g., exotic, different), and prototypicality (e.g., typical, usual), but that non-Western immigrant cuisines (e.g., Indian, Mexican) receive mor
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#21333;&#20154;&#34920;&#29616;&#25552;&#31034;&#65288;SPP&#65289;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#19982;&#22810;&#20010;&#35282;&#33394;&#36827;&#34892;&#22810;&#36718;&#33258;&#25105;&#21327;&#20316;&#65292;&#23558;&#21333;&#20010;&#35821;&#35328;&#27169;&#22411;&#36716;&#21270;&#20026;&#35748;&#30693;&#21327;&#21516;&#32773;&#65292;&#20174;&#32780;&#22686;&#24378;&#20854;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#21644;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.05300</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#37322;&#25918;&#35748;&#30693;&#21327;&#21516;&#65306;&#36890;&#36807;&#22810;&#20154;&#26684;&#33258;&#25105;&#21327;&#20316;&#23454;&#29616;&#20219;&#21153;&#35299;&#20915;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Unleashing Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration. (arXiv:2307.05300v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#21333;&#20154;&#34920;&#29616;&#25552;&#31034;&#65288;SPP&#65289;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#19982;&#22810;&#20010;&#35282;&#33394;&#36827;&#34892;&#22810;&#36718;&#33258;&#25105;&#21327;&#20316;&#65292;&#23558;&#21333;&#20010;&#35821;&#35328;&#27169;&#22411;&#36716;&#21270;&#20026;&#35748;&#30693;&#21327;&#21516;&#32773;&#65292;&#20174;&#32780;&#22686;&#24378;&#20854;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#21644;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26234;&#24935;&#20381;&#36182;&#20110;&#35748;&#30693;&#21327;&#21516;&#30340;&#27010;&#24565;&#65292;&#21363;&#22312;&#19981;&#21516;&#35748;&#30693;&#36807;&#31243;&#20043;&#38388;&#36827;&#34892;&#21327;&#20316;&#21644;&#20449;&#24687;&#25972;&#21512;&#65292;&#20197;&#33719;&#24471;&#27604;&#20010;&#20307;&#35748;&#30693;&#36807;&#31243;&#26356;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;&#36890;&#29992;&#20219;&#21153;&#35299;&#20915;&#20195;&#29702;&#34920;&#29616;&#20986;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#22312;&#38656;&#35201;&#20016;&#23500;&#39046;&#22495;&#30693;&#35782;&#21644;&#22797;&#26434;&#25512;&#29702;&#30340;&#20219;&#21153;&#19978;&#20173;&#28982;&#38754;&#20020;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21333;&#20154;&#34920;&#29616;&#25552;&#31034;&#65288;SPP&#65289;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#19982;&#22810;&#20010;&#35282;&#33394;&#36827;&#34892;&#22810;&#36718;&#33258;&#25105;&#21327;&#20316;&#65292;&#23558;&#21333;&#20010;LLM&#36716;&#21270;&#20026;&#35748;&#30693;&#21327;&#21516;&#32773;&#12290;&#35748;&#30693;&#21327;&#21516;&#32773;&#25351;&#30340;&#26159;&#19968;&#20010;&#26234;&#33021;&#20195;&#29702;&#65292;&#19982;&#22810;&#20010;&#26234;&#24935;&#21512;&#20316;&#65292;&#32467;&#21512;&#20182;&#20204;&#30340;&#20010;&#20307;&#20248;&#21183;&#21644;&#30693;&#35782;&#65292;&#20174;&#32780;&#22686;&#24378;&#22797;&#26434;&#20219;&#21153;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#21644;&#25972;&#20307;&#24615;&#33021;&#12290;&#36890;&#36807;&#26681;&#25454;&#20219;&#21153;&#36755;&#20837;&#21160;&#24577;&#35782;&#21035;&#21644;&#27169;&#25311;&#19981;&#21516;&#30340;&#35282;&#33394;&#65292;SPP&#37322;&#25918;&#20102;LLM&#20013;&#35748;&#30693;&#21327;&#21516;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human intelligence thrives on the concept of cognitive synergy, where collaboration and information integration among different cognitive processes yield superior outcomes compared to individual cognitive processes in isolation. Although Large Language Models (LLMs) have demonstrated promising performance as general task-solving agents, they still struggle with tasks that require intensive domain knowledge and complex reasoning. In this work, we propose Solo Performance Prompting (SPP), which transforms a single LLM into a cognitive synergist by engaging in multi-turn self-collaboration with multiple personas. A cognitive synergist refers to an intelligent agent that collaborates with multiple minds, combining their individual strengths and knowledge, to enhance problem-solving and overall performance in complex tasks. By dynamically identifying and simulating different personas based on task inputs, SPP unleashes the potential of cognitive synergy in LLMs. We have discovered that assi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#30340;&#27573;&#33853;&#32423;&#20013;&#25991;&#20027;&#39064;&#32467;&#26500;&#34920;&#31034;&#65292;&#20351;&#29992;&#21477;&#23376;&#32780;&#19981;&#26159;&#20851;&#38190;&#35789;&#26469;&#34920;&#31034;&#23376;&#20027;&#39064;&#65292;&#26500;&#24314;&#20102;&#22823;&#35268;&#27169;&#12289;&#39640;&#36136;&#37327;&#30340;&#20013;&#25991;&#27573;&#33853;&#32423;&#20027;&#39064;&#32467;&#26500;&#35821;&#26009;&#24211;&#12290;</title><link>http://arxiv.org/abs/2305.14790</link><description>&lt;p&gt;
&#25552;&#21319;&#20013;&#25991;&#25991;&#26412;&#20027;&#39064;&#21010;&#20998;&#21644;&#32434;&#35201;&#29983;&#25104;&#65306;&#27573;&#33853;&#32423;&#20027;&#39064;&#34920;&#31034;&#65292;&#35821;&#26009;&#24211;&#21644;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Advancing Topic Segmentation and Outline Generation in Chinese Texts: The Paragraph-level Topic Representation, Corpus, and Benchmark. (arXiv:2305.14790v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14790
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#30340;&#27573;&#33853;&#32423;&#20013;&#25991;&#20027;&#39064;&#32467;&#26500;&#34920;&#31034;&#65292;&#20351;&#29992;&#21477;&#23376;&#32780;&#19981;&#26159;&#20851;&#38190;&#35789;&#26469;&#34920;&#31034;&#23376;&#20027;&#39064;&#65292;&#26500;&#24314;&#20102;&#22823;&#35268;&#27169;&#12289;&#39640;&#36136;&#37327;&#30340;&#20013;&#25991;&#27573;&#33853;&#32423;&#20027;&#39064;&#32467;&#26500;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#39064;&#21010;&#20998;&#21644;&#32434;&#35201;&#29983;&#25104;&#26088;&#22312;&#23558;&#19968;&#20010;&#25991;&#26723;&#20998;&#25104;&#36830;&#36143;&#30340;&#20027;&#39064;&#27573;&#33853;&#24182;&#29983;&#25104;&#30456;&#24212;&#30340;&#23376;&#26631;&#39064;&#12290;&#36825;&#20010;&#36807;&#31243;&#25581;&#31034;&#20102;&#19968;&#20010;&#25991;&#26723;&#30340;&#35805;&#39064;&#32467;&#26500;&#65292;&#26377;&#21161;&#20110;&#20174;&#26356;&#39640;&#30340;&#23618;&#27425;&#24555;&#36895;&#25226;&#25569;&#21644;&#29702;&#35299;&#25991;&#26723;&#30340;&#25972;&#20307;&#24773;&#22659;&#12290;&#28982;&#32780;&#65292;&#19982;&#33521;&#35821;&#39046;&#22495;&#21462;&#24471;&#30340;&#25104;&#21151;&#30456;&#27604;&#65292;&#30001;&#20110;&#32570;&#20047;&#36866;&#24403;&#30340;&#27573;&#33853;&#32423;&#20027;&#39064;&#34920;&#31034;&#21644;&#22823;&#35268;&#27169;&#12289;&#39640;&#36136;&#37327;&#30340;&#20013;&#25991;&#35821;&#26009;&#24211;&#65292;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20998;&#23618;&#30340;&#27573;&#33853;&#32423;&#20027;&#39064;&#32467;&#26500;&#34920;&#31034;&#65292;&#21253;&#25324;&#26631;&#39064;&#12289;&#23376;&#26631;&#39064;&#21644;&#27573;&#33853;&#65292;&#32508;&#21512;&#22320;&#27169;&#25311;&#20102;&#25991;&#26723;&#30340;&#35805;&#39064;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#21477;&#23376;&#32780;&#19981;&#26159;&#20851;&#38190;&#35789;&#26469;&#34920;&#31034;&#23376;&#20027;&#39064;&#65292;&#30830;&#20445;&#26356;&#20840;&#38754;&#22320;&#34920;&#31034;&#25991;&#26723;&#20869;&#30340;&#20027;&#39064;&#20998;&#24067;&#12290;&#26681;&#25454;&#36825;&#31181;&#34920;&#31034;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#26368;&#22823;&#30340;&#20013;&#25991;&#27573;&#33853;&#32423;&#20027;&#39064;&#32467;&#26500;&#35821;&#26009;&#24211;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Topic segmentation and outline generation strive to divide a document into coherent topic sections and generate corresponding subheadings. Such a process unveils the discourse topic structure of a document that benefits quickly grasping and understanding the overall context of the document from a higher level. However, research and applications in this field have been restrained due to the lack of proper paragraph-level topic representations and large-scale, high-quality corpora in Chinese compared to the success achieved in English. Addressing these issues, we introduce a hierarchical paragraph-level topic structure representation with title, subheading, and paragraph that comprehensively models the document discourse topic structure. In addition, we ensure a more holistic representation of topic distribution within the document by using sentences instead of keywords to represent sub-topics. Following this representation, we construct the largest Chinese Paragraph-level Topic Structur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20851;&#27880;ChatGPT&#38754;&#20020;&#30340;&#21487;&#25345;&#32493;&#24615;&#12289;&#38544;&#31169;&#12289;&#25968;&#23383;&#40511;&#27807;&#21644;&#20262;&#29702;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;SPADE&#35780;&#20272;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#32473;&#20986;&#20102;&#32531;&#35299;&#21644;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2305.03123</link><description>&lt;p&gt;
ChatGPT &#38656;&#35201;&#36827;&#34892;SPADE&#65288;&#21487;&#25345;&#32493;&#24615;&#12289;&#38544;&#31169;&#12289;&#25968;&#23383;&#40511;&#27807;&#21644;&#20262;&#29702;&#65289;&#35780;&#20272;&#65306;&#19968;&#39033;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and Ethics) Evaluation: A Review. (arXiv:2305.03123v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03123
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20851;&#27880;ChatGPT&#38754;&#20020;&#30340;&#21487;&#25345;&#32493;&#24615;&#12289;&#38544;&#31169;&#12289;&#25968;&#23383;&#40511;&#27807;&#21644;&#20262;&#29702;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;SPADE&#35780;&#20272;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#32473;&#20986;&#20102;&#32531;&#35299;&#21644;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#21478;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#30001;&#20110;&#20854;&#24615;&#33021;&#21644;&#26377;&#25928;&#30340;&#23545;&#35805;&#33021;&#21147;&#65292;&#22312;&#30740;&#31350;&#21644;&#24037;&#19994;&#30028;&#20013;&#24471;&#21040;&#20102;&#24040;&#22823;&#30340;&#20851;&#27880;&#12290;&#26368;&#36817;&#65292;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#21457;&#34920;&#65292;&#20197;&#23637;&#31034;ChatGPT&#21644;&#20854;&#20182;LLMs&#30340;&#26377;&#25928;&#24615;&#12289;&#25928;&#29575;&#12289;&#38598;&#25104;&#21644;&#24773;&#24863;&#12290;&#30456;&#21453;&#65292;&#26412;&#30740;&#31350;&#20851;&#27880;&#30340;&#26159;&#22823;&#22810;&#25968;&#34987;&#24573;&#35270;&#30340;&#37325;&#35201;&#26041;&#38754;&#65292;&#21363;&#21487;&#25345;&#32493;&#24615;&#12289;&#38544;&#31169;&#12289;&#25968;&#23383;&#40511;&#27807;&#21644;&#20262;&#29702;&#65292;&#24182;&#24314;&#35758;&#19981;&#20165;&#20165;&#26159;ChatGPT&#65292;&#32780;&#26159;&#22312;&#23545;&#35805;&#26426;&#22120;&#20154;&#31867;&#21035;&#20013;&#30340;&#27599;&#19968;&#20010;&#21518;&#32493;&#20837;&#21475;&#37117;&#24212;&#35813;&#36827;&#34892;SPADE&#35780;&#20272;&#12290;&#26412;&#25991;&#35814;&#32454;&#35752;&#35770;&#20102;&#20851;&#20110;ChatGPT&#30340;&#38382;&#39064;&#21644;&#20851;&#27880;&#28857;&#19982;&#19978;&#36848;&#29305;&#24449;&#19968;&#33268;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20123;&#21021;&#27493;&#30340;&#25968;&#25454;&#25910;&#38598;&#21644;&#21487;&#35270;&#21270;&#20197;&#21450;&#20551;&#35774;&#30340;&#20107;&#23454;&#26469;&#25903;&#25345;&#25105;&#20204;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#36824;&#20026;&#27599;&#20010;&#38382;&#39064;&#25552;&#20986;&#20102;&#32531;&#35299;&#21644;&#24314;&#35758;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20123;&#26410;&#26469;&#26041;&#21521;&#21644;&#24320;&#25918;&#38382;&#39064;&#30340;&#25506;&#35752;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is another large language model (LLM) inline but due to its performance and ability to converse effectively, it has gained a huge popularity amongst research as well as industrial community. Recently, many studies have been published to show the effectiveness, efficiency, integration, and sentiments of chatGPT and other LLMs. In contrast, this study focuses on the important aspects that are mostly overlooked, i.e. sustainability, privacy, digital divide, and ethics and suggests that not only chatGPT but every subsequent entry in the category of conversational bots should undergo Sustainability, PrivAcy, Digital divide, and Ethics (SPADE) evaluation. This paper discusses in detail about the issues and concerns raised over chatGPT in line with aforementioned characteristics. We support our hypothesis by some preliminary data collection and visualizations along with hypothesized facts. We also suggest mitigations and recommendations for each of the concerns. Furthermore, we also s
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#19968;&#22871;&#35821;&#38899;&#21333;&#35789;&#23884;&#20837;&#21450;&#20854;&#30456;&#20851;&#20219;&#21153;&#65292;&#25552;&#39640;&#20102;&#35821;&#38899;&#20449;&#24687;&#22788;&#29702;&#30340;&#25928;&#26524;&#21644;&#21487;&#37325;&#22797;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.02541</link><description>&lt;p&gt;
PWESuite&#65306;&#35821;&#38899;&#21333;&#35789;&#23884;&#20837;&#21450;&#20854;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
PWESuite: Phonetic Word Embeddings and Tasks They Facilitate. (arXiv:2304.02541v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#19968;&#22871;&#35821;&#38899;&#21333;&#35789;&#23884;&#20837;&#21450;&#20854;&#30456;&#20851;&#20219;&#21153;&#65292;&#25552;&#39640;&#20102;&#35821;&#38899;&#20449;&#24687;&#22788;&#29702;&#30340;&#25928;&#26524;&#21644;&#21487;&#37325;&#22797;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#21333;&#35789;&#26144;&#23556;&#21040;&#22266;&#23450;&#32500;&#24230;&#30340;&#21521;&#37327;&#31354;&#38388;&#30340;&#21333;&#35789;&#23884;&#20837;&#26159;&#29616;&#20195;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#22522;&#30784;&#12290;&#22823;&#22810;&#25968;&#21333;&#35789;&#23884;&#20837;&#26041;&#27861;&#32534;&#30721;&#35821;&#20041;&#20449;&#24687;&#12290;&#20294;&#26159;&#65292;&#23545;&#20110;&#26576;&#20123;&#20219;&#21153;&#38750;&#24120;&#37325;&#35201;&#30340;&#35821;&#38899;&#20449;&#24687;&#32463;&#24120;&#34987;&#24573;&#30053;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20960;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#21457;&#22768;&#29305;&#24449;&#26500;&#24314;&#35821;&#38899;&#30693;&#24773;&#21333;&#35789;&#23884;&#20837;&#65292;&#24182;&#25552;&#20379;&#19968;&#22871;&#35821;&#38899;&#21333;&#35789;&#23884;&#20837;&#20197;&#40723;&#21169;&#20854;&#31038;&#21306;&#30340;&#24320;&#21457;&#12289;&#35780;&#20272;&#21644;&#20351;&#29992;&#12290;&#34429;&#28982;&#24050;&#32463;&#23384;&#22312;&#35768;&#22810;&#23398;&#20064;&#35821;&#38899;&#21333;&#35789;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#20294;&#22312;&#35780;&#20272;&#20854;&#26377;&#25928;&#24615;&#26041;&#38754;&#32570;&#20047;&#19968;&#33268;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20960;&#31181;&#35780;&#20272;&#35821;&#38899;&#21333;&#35789;&#23884;&#20837;&#30340;&#20869;&#22312;&#26041;&#38754;&#30340;&#26041;&#27861;&#65292;&#22914;&#21333;&#35789;&#26816;&#32034;&#21644;&#19982;&#22768;&#38899;&#30456;&#20284;&#24615;&#30340;&#30456;&#20851;&#24615;&#65292;&#20197;&#21450;&#22806;&#22312;&#34920;&#29616;&#65292;&#22914;&#38901;&#24459;&#21644;&#21516;&#28304;&#26816;&#27979;&#21644;&#22768;&#38899;&#31867;&#27604;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#20219;&#21153;&#22871;&#20214;&#23558;&#20419;&#36827;&#21487;&#37325;&#22797;&#24615;&#24182;&#25552;&#20379;&#26410;&#26469;&#35821;&#38899;&#21333;&#35789;&#23884;&#20837;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Word embeddings that map words into a fixed-dimensional vector space are the backbone of modern NLP. Most word embedding methods encode semantic information. However, phonetic information, which is important for some tasks, is often overlooked. In this work, we develop several novel methods which leverage articulatory features to build phonetically informed word embeddings, and present a set of phonetic word embeddings to encourage their community development, evaluation and use. While several methods for learning phonetic word embeddings already exist, there is a lack of consistency in evaluating their effectiveness. Thus, we also proposes several ways to evaluate both intrinsic aspects of phonetic word embeddings, such as word retrieval and correlation with sound similarity, and extrinsic performances, such as rhyme and cognate detection and sound analogies. We hope that our suite of tasks will promote reproducibility and provide direction for future research on phonetic word embeddi
&lt;/p&gt;</description></item><item><title>ChatGPT&#22312;&#22823;&#37096;&#20998;&#35821;&#35328;&#22788;&#29702;&#23454;&#39564;&#20013;&#19982;&#20154;&#31867;&#34920;&#29616;&#30456;&#20284;&#65292;&#33021;&#22815;&#20135;&#29983;&#20154;&#31867;&#19968;&#26679;&#30340;&#35821;&#35328;&#20351;&#29992;&#29305;&#24449;&#12290;&#20294;&#22312;&#20004;&#20010;&#23454;&#39564;&#20013;&#23384;&#22312;&#20559;&#24046;&#65292;&#35828;&#26126;&#20154;&#31867;&#21644;&#26426;&#22120;&#35821;&#35328;&#22788;&#29702;&#20043;&#38388;&#20173;&#23384;&#22312;&#37325;&#22823;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2303.08014</link><description>&lt;p&gt;
ChatGPT&#26159;&#21542;&#21644;&#20154;&#31867;&#22312;&#35821;&#35328;&#20351;&#29992;&#19978;&#30456;&#20284;?
&lt;/p&gt;
&lt;p&gt;
Does ChatGPT resemble humans in language use?. (arXiv:2303.08014v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08014
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#22312;&#22823;&#37096;&#20998;&#35821;&#35328;&#22788;&#29702;&#23454;&#39564;&#20013;&#19982;&#20154;&#31867;&#34920;&#29616;&#30456;&#20284;&#65292;&#33021;&#22815;&#20135;&#29983;&#20154;&#31867;&#19968;&#26679;&#30340;&#35821;&#35328;&#20351;&#29992;&#29305;&#24449;&#12290;&#20294;&#22312;&#20004;&#20010;&#23454;&#39564;&#20013;&#23384;&#22312;&#20559;&#24046;&#65292;&#35828;&#26126;&#20154;&#31867;&#21644;&#26426;&#22120;&#35821;&#35328;&#22788;&#29702;&#20043;&#38388;&#20173;&#23384;&#22312;&#37325;&#22823;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#21644;&#20197;LLM&#20026;&#39537;&#21160;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;(&#22914;ChatGPT)&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#35821;&#35328;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#22312;&#35748;&#30693;&#23618;&#38754;&#19978;&#65292;&#23427;&#20204;&#30340;&#20869;&#37096;&#26426;&#21046;&#20173;&#28982;&#26159;&#40657;&#21283;&#23376;&#65292;&#19981;&#28165;&#26970;LLM&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#26159;&#21542;&#33021;&#22815;&#21457;&#23637;&#20986;&#20154;&#31867;&#30340;&#35821;&#35328;&#20351;&#29992;&#29305;&#24449;&#12290;&#25105;&#20204;&#23545;ChatGPT&#36827;&#34892;&#20102;12&#20010;&#23454;&#39564;&#65292;&#27599;&#20010;&#23454;&#39564;&#27880;&#20876;&#21069;&#36827;&#34892;&#20102;1000&#27425;&#36816;&#34892;&#12290;&#22312;&#20854;&#20013;&#30340;10&#20010;&#23454;&#39564;&#20013;&#65292;ChatGPT&#22797;&#21046;&#20102;&#20154;&#31867;&#35821;&#35328;&#20351;&#29992;&#30340;&#27169;&#24335;&#12290;&#23427;&#23558;&#19981;&#29087;&#24713;&#30340;&#21333;&#35789;&#19982;&#19981;&#21516;&#30340;&#21547;&#20041;&#36827;&#34892;&#20851;&#32852;&#65292;&#26681;&#25454;&#21333;&#35789;&#24418;&#24335;&#32487;&#32493;&#35775;&#38382;&#26368;&#36817;&#36935;&#21040;&#30340;&#27495;&#20041;&#35789;&#27719;&#30340;&#21547;&#20041;&#65292;&#37325;&#29992;&#26368;&#36817;&#30340;&#35821;&#21477;&#32467;&#26500;&#65292;&#37325;&#26032;&#35299;&#37322;&#21487;&#33021;&#34987;&#22122;&#22768;&#24178;&#25200;&#30340;&#19981;&#21512;&#29702;&#35821;&#21477;&#65292;&#24573;&#30053;&#38169;&#35823;&#65292;&#36827;&#34892;&#21512;&#29702;&#25512;&#26029;&#65292;&#26681;&#25454;&#23427;&#20204;&#30340;&#39034;&#24207;&#21644;&#25509;&#36817;&#31243;&#24230;&#23558;&#22240;&#26524;&#20851;&#31995;&#19982;&#19981;&#21516;&#30340;&#35805;&#35821;&#23454;&#20307;&#30456;&#20851;&#32852;&#65292;&#24182;&#23454;&#26102;&#26356;&#27491;&#19968;&#33268;&#24615;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#22312;&#20004;&#20010;&#23454;&#39564;&#20013;&#65292;ChatGPT&#26174;&#31034;&#20986;&#19982;&#20154;&#31867;&#34920;&#29616;&#30340;&#20559;&#24046;&#65292;&#36825;&#34920;&#26126;&#20154;&#31867;&#21644;&#26426;&#22120;&#35821;&#35328;&#22788;&#29702;&#20043;&#38388;&#20173;&#23384;&#22312;&#37325;&#22823;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) and LLM-driven chatbots such as ChatGPT have shown remarkable capacities in comprehending and producing language. However, their internal workings remain a black box in cognitive terms, and it is unclear whether LLMs and chatbots can develop humanlike characteristics in language use. Cognitive scientists have devised many experiments that probe, and have made great progress in explaining, how people process language. We subjected ChatGPT to 12 of these experiments, pre-registered and with 1,000 runs per experiment. In 10 of them, ChatGPT replicated the human pattern of language use. It associated unfamiliar words with different meanings depending on their forms, continued to access recently encountered meanings of ambiguous words, reused recent sentence structures, reinterpreted implausible sentences that were likely to have been corrupted by noise, glossed over errors, drew reasonable inferences, associated causality with different discourse entities accor
&lt;/p&gt;</description></item></channel></rss>