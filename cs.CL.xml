<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23398;&#20064;&#25163;&#26426;&#31867;&#22411;&#30340;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#21305;&#37197;&#32858;&#31867;&#20013;&#24515;&#19982;&#25163;&#26426;&#23884;&#20837;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#29983;&#25104;&#20266;&#26631;&#31614;&#21644;&#24341;&#20837;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#33258;&#30417;&#30563;&#34920;&#31034;&#65292;&#23454;&#39564;&#35777;&#26126;&#21305;&#37197;&#32467;&#26524;&#25429;&#25417;&#21040;&#20102;&#25163;&#26426;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#25163;&#26426;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.17558</link><description>&lt;p&gt;
&#23454;&#29616;&#25163;&#26426;&#19982;&#35821;&#38899;&#34920;&#31034;&#30340;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Towards Matching Phones and Speech Representations. (arXiv:2310.17558v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17558
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23398;&#20064;&#25163;&#26426;&#31867;&#22411;&#30340;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#21305;&#37197;&#32858;&#31867;&#20013;&#24515;&#19982;&#25163;&#26426;&#23884;&#20837;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#29983;&#25104;&#20266;&#26631;&#31614;&#21644;&#24341;&#20837;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#33258;&#30417;&#30563;&#34920;&#31034;&#65292;&#23454;&#39564;&#35777;&#26126;&#21305;&#37197;&#32467;&#26524;&#25429;&#25417;&#21040;&#20102;&#25163;&#26426;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#25163;&#26426;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#25163;&#26426;&#31867;&#22411;&#20174;&#25163;&#26426;&#23454;&#20363;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#20294;&#20173;&#28982;&#23578;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#37325;&#26032;&#23457;&#35270;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#25552;&#20986;&#20026;&#23558;&#32858;&#31867;&#20013;&#24515;&#19982;&#25163;&#26426;&#23884;&#20837;&#21305;&#37197;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#20010;&#20851;&#38190;&#23646;&#24615;&#65292;&#23427;&#20204;&#20351;&#21305;&#37197;&#25104;&#20026;&#21487;&#33021;&#65292;&#21363;&#33258;&#30417;&#30563;&#34920;&#31034;&#30340;&#32858;&#31867;&#20013;&#24515;&#26159;&#21542;&#20943;&#23569;&#20102;&#25163;&#26426;&#23454;&#20363;&#30340;&#21464;&#21270;&#24615;&#24182;&#19988;&#26159;&#21542;&#23562;&#37325;&#25163;&#26426;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#21305;&#37197;&#32467;&#26524;&#29983;&#25104;&#20266;&#26631;&#31614;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#25913;&#36827;&#33258;&#30417;&#30563;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#21305;&#37197;&#32467;&#26524;&#25429;&#25417;&#21040;&#20102;&#25163;&#26426;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#23558;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#19982;APC&#21644;CPC&#31561;&#24120;&#35268;&#30340;&#33258;&#30417;&#30563;&#25439;&#22833;&#19968;&#36215;&#35757;&#32451;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#19979;&#28216;&#25163;&#26426;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning phone types from phone instances has been a long-standing problem, while still being open. In this work, we revisit this problem in the context of self-supervised learning, and pose it as the problem of matching cluster centroids to phone embeddings. We study two key properties that enable matching, namely, whether cluster centroids of self-supervised representations reduce the variability of phone instances and respect the relationship among phones. We then use the matching result to produce pseudo-labels and introduce a new loss function for improving self-supervised representations. Our experiments show that the matching result captures the relationship among phones. Training the new loss function jointly with the regular self-supervised losses, such as APC and CPC, significantly improves the downstream phone classification.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#27169;&#22411;&#20013;&#30340;&#20262;&#29702;&#20215;&#20540;&#23545;&#40784;&#38382;&#39064;&#65292;&#35843;&#26597;&#20102;&#29616;&#26377;&#30340;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#20934;&#21017;&#65292;&#24182;&#25552;&#20986;&#20102;&#24314;&#31435;&#32479;&#19968;&#21644;&#26222;&#36941;&#30340;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#26694;&#26550;&#30340;&#37325;&#35201;&#24615;&#12290;&#27492;&#22806;&#65292;&#23545;&#24403;&#21069;&#20027;&#27969;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20262;&#29702;&#20542;&#21521;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#20998;&#26512;&#20102;&#22312;&#23454;&#29616;&#20262;&#29702;&#20215;&#20540;&#23545;&#40784;&#26102;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.17551</link><description>&lt;p&gt;
&#35299;&#35835;&#22823;&#27169;&#22411;&#20013;&#30340;&#20262;&#29702;&#20215;&#20540;&#23545;&#40784;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Unpacking the Ethical Value Alignment in Big Models. (arXiv:2310.17551v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17551
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#27169;&#22411;&#20013;&#30340;&#20262;&#29702;&#20215;&#20540;&#23545;&#40784;&#38382;&#39064;&#65292;&#35843;&#26597;&#20102;&#29616;&#26377;&#30340;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#20934;&#21017;&#65292;&#24182;&#25552;&#20986;&#20102;&#24314;&#31435;&#32479;&#19968;&#21644;&#26222;&#36941;&#30340;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#26694;&#26550;&#30340;&#37325;&#35201;&#24615;&#12290;&#27492;&#22806;&#65292;&#23545;&#24403;&#21069;&#20027;&#27969;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20262;&#29702;&#20542;&#21521;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#20998;&#26512;&#20102;&#22312;&#23454;&#29616;&#20262;&#29702;&#20215;&#20540;&#23545;&#40784;&#26102;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#27169;&#22411;&#26497;&#22823;&#22320;&#25552;&#21319;&#20102;&#20154;&#24037;&#26234;&#33021;&#29702;&#35299;&#12289;&#29983;&#25104;&#21644;&#25805;&#20316;&#20449;&#24687;&#19982;&#20869;&#23481;&#30340;&#33021;&#21147;&#65292;&#24182;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#36825;&#20123;&#27169;&#22411;&#36234;&#26469;&#36234;&#22810;&#22320;&#34701;&#20837;&#26085;&#24120;&#29983;&#27963;&#65292;&#20854;&#22266;&#26377;&#30340;&#20262;&#29702;&#20215;&#20540;&#35266;&#21644;&#28508;&#22312;&#20559;&#35265;&#32473;&#31038;&#20250;&#24102;&#26469;&#20102;&#26410;&#39044;&#26009;&#30340;&#39118;&#38505;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#19982;&#22823;&#22411;&#27169;&#22411;&#26377;&#20851;&#30340;&#39118;&#38505;&#21644;&#25361;&#25112;&#65292;&#35843;&#26597;&#20102;&#29616;&#26377;&#30340;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#20934;&#21017;&#65292;&#24182;&#20998;&#26512;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#25152;&#24341;&#21457;&#30340;&#20262;&#29702;&#24433;&#21709;&#12290;&#20174;&#35268;&#33539;&#20262;&#29702;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#26368;&#36817;&#30340;&#35268;&#33539;&#20934;&#21017;&#36827;&#34892;&#37325;&#26032;&#35780;&#20272;&#30340;&#24314;&#35758;&#65292;&#24378;&#35843;&#20102;&#23398;&#26415;&#30028;&#22312;&#24314;&#31435;&#32479;&#19968;&#21644;&#26222;&#36941;&#30340;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#26694;&#26550;&#26041;&#38754;&#30340;&#21512;&#20316;&#21162;&#21147;&#30340;&#37325;&#35201;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#36947;&#24503;&#22522;&#30784;&#29702;&#35770;&#35843;&#26597;&#20102;&#24403;&#21069;&#20027;&#27969;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36947;&#24503;&#20542;&#21521;&#65292;&#20998;&#26512;&#20102;&#29616;&#26377;&#30340;&#20215;&#20540;&#35266;&#23545;&#40784;&#31639;&#27861;&#65292;&#24182;&#27010;&#36848;&#20102;&#22312;&#23454;&#29616;&#20262;&#29702;&#20215;&#20540;&#23545;&#40784;&#26102;&#36935;&#21040;&#30340;&#29420;&#29305;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Big models have greatly advanced AI's ability to understand, generate, and manipulate information and content, enabling numerous applications. However, as these models become increasingly integrated into everyday life, their inherent ethical values and potential biases pose unforeseen risks to society. This paper provides an overview of the risks and challenges associated with big models, surveys existing AI ethics guidelines, and examines the ethical implications arising from the limitations of these models. Taking a normative ethics perspective, we propose a reassessment of recent normative guidelines, highlighting the importance of collaborative efforts in academia to establish a unified and universal AI ethics framework. Furthermore, we investigate the moral inclinations of current mainstream LLMs using the Moral Foundation theory, analyze existing alignment algorithms, and outline the unique challenges encountered in aligning ethical values within them. To address these challenges
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#24615;&#21035;&#20013;&#31435;&#30340;&#39044;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#65292;&#24182;&#21457;&#29616;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#21518;&#30340;&#20559;&#35265;&#25918;&#22823;&#26159;&#30456;&#20114;&#29420;&#31435;&#30340;&#12290;&#27492;&#22806;&#65292;&#25345;&#32493;&#39044;&#35757;&#32451;&#23545;&#24615;&#21035;&#20013;&#31435;&#25968;&#25454;&#26377;&#21033;&#65292;&#21487;&#20197;&#22312;&#19968;&#20123;&#20219;&#21153;&#20013;&#20419;&#36827;&#20844;&#24179;&#12290;</title><link>http://arxiv.org/abs/2310.17530</link><description>&lt;p&gt;
&#35780;&#20272;&#24615;&#21035;&#20013;&#31435;&#30340;&#39044;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating Bias and Fairness in Gender-Neutral Pretrained Vision-and-Language Models. (arXiv:2310.17530v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#24615;&#21035;&#20013;&#31435;&#30340;&#39044;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#65292;&#24182;&#21457;&#29616;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#21518;&#30340;&#20559;&#35265;&#25918;&#22823;&#26159;&#30456;&#20114;&#29420;&#31435;&#30340;&#12290;&#27492;&#22806;&#65292;&#25345;&#32493;&#39044;&#35757;&#32451;&#23545;&#24615;&#21035;&#20013;&#31435;&#25968;&#25454;&#26377;&#21033;&#65292;&#21487;&#20197;&#22312;&#19968;&#20123;&#20219;&#21153;&#20013;&#20419;&#36827;&#20844;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#30693;&#39044;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20250;&#20445;&#25345;&#29978;&#33267;&#25918;&#22823;&#25968;&#25454;&#20013;&#29616;&#26377;&#30340;&#20559;&#35265;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#19981;&#20844;&#24179;&#30340;&#32467;&#26524;&#65292;&#26368;&#32456;&#24433;&#21709;&#29992;&#25143;&#20307;&#39564;&#12290;&#20026;&#20102;&#30830;&#20445;&#27169;&#22411;&#30340;&#24615;&#33021;&#19981;&#20250;&#23545;&#29305;&#23450;&#32676;&#20307;&#25110;&#20154;&#21475;&#20135;&#29983;&#27495;&#35270;&#24615;&#34892;&#20026;&#65292;&#29702;&#35299;&#36825;&#20123;&#20559;&#35265;&#20559;&#21521;&#30340;&#26426;&#21046;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#24615;&#21035;&#20559;&#35265;&#20316;&#20026;&#26696;&#20363;&#36827;&#34892;&#23450;&#20041;&#12290;&#25105;&#20204;&#37327;&#21270;&#20102;&#19977;&#20010;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#26063;&#32676;&#30340;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#21518;&#30340;&#20559;&#35265;&#25918;&#22823;&#65292;&#24182;&#35843;&#26597;&#20102;&#36825;&#20004;&#20010;&#23398;&#20064;&#38454;&#27573;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#35780;&#20272;&#20102;&#20559;&#35265;&#25918;&#22823;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#21518;&#30340;&#20559;&#35265;&#25918;&#22823;&#26159;&#30456;&#20114;&#29420;&#31435;&#30340;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#24615;&#21035;&#20013;&#31435;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#25345;&#32493;&#39044;&#35757;&#32451;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#36825;&#21487;&#20197;&#20943;&#23569;&#32676;&#20307;&#24046;&#36317;&#65292;&#21363;&#22312;VQAv2&#21644;&#26816;&#32034;&#20219;&#21153;&#20013;&#20419;&#36827;&#20844;&#24179;&#65292;&#32780;&#19981;&#20250;&#26174;&#33879;&#25439;&#23475;&#20219;&#21153;&#25191;&#34892;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained machine learning models are known to perpetuate and even amplify existing biases in data, which can result in unfair outcomes that ultimately impact user experience. Therefore, it is crucial to understand the mechanisms behind those prejudicial biases to ensure that model performance does not result in discriminatory behaviour toward certain groups or populations. In this work, we define gender bias as our case study. We quantify bias amplification in pretraining and after fine-tuning on three families of vision-and-language models. We investigate the connection, if any, between the two learning stages, and evaluate how bias amplification reflects on model performance. Overall, we find that bias amplification in pretraining and after fine-tuning are independent. We then examine the effect of continued pretraining on gender-neutral data, finding that this reduces group disparities, i.e., promotes fairness, on VQAv2 and retrieval tasks without significantly compromising task p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;GPT-4&#22312;&#22810;&#31181;&#35821;&#35328;&#30340;&#21516;&#34892;&#35780;&#23457;&#25991;&#29486;&#21644;&#28784;&#33394;&#25991;&#29486;&#31579;&#36873;&#21644;&#25552;&#21462;&#25968;&#25454;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;GPT-4&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#19978;&#20934;&#30830;&#24230;&#19982;&#20154;&#31867;&#34920;&#29616;&#30456;&#24403;&#65292;&#22312;&#35843;&#25972;&#20102;&#20598;&#28982;&#19968;&#33268;&#24615;&#21644;&#25968;&#25454;&#38598;&#19981;&#24179;&#34913;&#21518;&#65292;&#20854;&#22312;&#25968;&#25454;&#25552;&#21462;&#26041;&#38754;&#34920;&#29616;&#20986;&#20013;&#31561;&#27700;&#24179;&#30340;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.17526</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#21462;&#20195;&#20154;&#31867;&#22312;&#31995;&#32479;&#35780;&#20215;&#36807;&#31243;&#20013;&#30340;&#35282;&#33394;&#65311;&#35780;&#20272;GPT-4&#22312;&#22810;&#31181;&#35821;&#35328;&#30340;&#21516;&#34892;&#35780;&#23457;&#25991;&#29486;&#21644;&#28784;&#33394;&#25991;&#29486;&#31579;&#36873;&#21644;&#25552;&#21462;&#25968;&#25454;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can large language models replace humans in the systematic review process? Evaluating GPT-4's efficacy in screening and extracting data from peer-reviewed and grey literature in multiple languages. (arXiv:2310.17526v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17526
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;GPT-4&#22312;&#22810;&#31181;&#35821;&#35328;&#30340;&#21516;&#34892;&#35780;&#23457;&#25991;&#29486;&#21644;&#28784;&#33394;&#25991;&#29486;&#31579;&#36873;&#21644;&#25552;&#21462;&#25968;&#25454;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;GPT-4&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#19978;&#20934;&#30830;&#24230;&#19982;&#20154;&#31867;&#34920;&#29616;&#30456;&#24403;&#65292;&#22312;&#35843;&#25972;&#20102;&#20598;&#28982;&#19968;&#33268;&#24615;&#21644;&#25968;&#25454;&#38598;&#19981;&#24179;&#34913;&#21518;&#65292;&#20854;&#22312;&#25968;&#25454;&#25552;&#21462;&#26041;&#38754;&#34920;&#29616;&#20986;&#20013;&#31561;&#27700;&#24179;&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31995;&#32479;&#35780;&#20215;&#23545;&#20110;&#25351;&#23548;&#23454;&#36341;&#12289;&#30740;&#31350;&#21644;&#25919;&#31574;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#24120;&#24120;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#26102;&#38388;&#21644;&#20154;&#21147;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21487;&#33021;&#33021;&#22815;&#21152;&#24555;&#21644;&#33258;&#21160;&#21270;&#31995;&#32479;&#35780;&#20215;&#30340;&#36807;&#31243;&#65292;&#20294;&#26159;&#23427;&#20204;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#23578;&#26410;&#32463;&#36807;&#20840;&#38754;&#35780;&#20272;&#65292;&#32780;&#19988;&#36824;&#27809;&#26377;&#30740;&#31350;&#27979;&#35797;&#36807;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;LLM&#8212;&#8212;GPT-4&#12290;&#26412;&#39044;&#27880;&#20876;&#30740;&#31350;&#37319;&#29992;&#8220;&#26080;&#20154;&#21442;&#19982;&#8221;&#30340;&#26041;&#27861;&#35780;&#20272;&#20102;GPT-4&#22312;&#26631;&#39064;/&#25688;&#35201;&#31579;&#36873;&#12289;&#20840;&#25991;&#23457;&#26597;&#21644;&#25968;&#25454;&#25552;&#21462;&#26041;&#38754;&#22312;&#19981;&#21516;&#25991;&#29486;&#31867;&#22411;&#21644;&#35821;&#35328;&#19978;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;GPT-4&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#30340;&#20934;&#30830;&#24230;&#19982;&#20154;&#31867;&#34920;&#29616;&#30456;&#24403;&#65292;&#20294;&#32467;&#26524;&#21463;&#21040;&#20598;&#28982;&#19968;&#33268;&#24615;&#21644;&#25968;&#25454;&#38598;&#19981;&#24179;&#34913;&#30340;&#24433;&#21709;&#12290;&#22312;&#35843;&#25972;&#20102;&#36825;&#20123;&#22240;&#32032;&#21518;&#65292;&#25968;&#25454;&#25552;&#21462;&#26041;&#38754;&#34920;&#29616;&#20986;&#20013;&#31561;&#27700;&#24179;&#30340;&#20934;&#30830;&#24230;&#65292;&#22312;&#20351;&#29992;&#39640;&#21487;&#38752;&#24615;&#25552;&#31034;&#36827;&#34892;&#31579;&#36873;&#30340;&#30740;&#31350;&#20013;&#65292;&#31579;&#36873;&#20840;&#25991;&#25991;&#29486;&#30340;&#34920;&#29616;&#27700;&#24179;&#22312;&#19981;&#21516;&#38454;&#27573;&#21644;&#35821;&#35328;&#19978;&#22343;&#20026;&#26080;&#21040;&#20013;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Systematic reviews are vital for guiding practice, research, and policy, yet they are often slow and labour-intensive. Large language models (LLMs) could offer a way to speed up and automate systematic reviews, but their performance in such tasks has not been comprehensively evaluated against humans, and no study has tested GPT-4, the biggest LLM so far. This pre-registered study evaluates GPT-4's capability in title/abstract screening, full-text review, and data extraction across various literature types and languages using a 'human-out-of-the-loop' approach. Although GPT-4 had accuracy on par with human performance in most tasks, results were skewed by chance agreement and dataset imbalance. After adjusting for these, there was a moderate level of performance for data extraction, and - barring studies that used highly reliable prompts screening performance levelled at none to moderate for different stages and languages. When screening full-text literature using highly reliable prom
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#22312;&#32452;&#21512;&#27867;&#21270;&#39046;&#22495;&#20013;&#29305;&#23450;&#25968;&#25454;&#38598;&#35774;&#35745;&#36873;&#25321;&#23545;&#27169;&#22411;&#33021;&#21147;&#32467;&#35770;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#19981;&#21516;&#25968;&#25454;&#38598;&#23545;&#24314;&#27169;&#26041;&#27861;&#30340;&#25490;&#21517;&#23384;&#22312;&#24046;&#24322;&#65292;&#20154;&#31867;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#39640;&#20110;&#21512;&#25104;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#25968;&#25454;&#38598;&#26159;&#21542;&#26469;&#33258;&#21516;&#19968;&#26469;&#28304;&#26356;&#33021;&#39044;&#27979;&#32467;&#26524;&#30340;&#27169;&#22411;&#25490;&#21517;&#12290;</title><link>http://arxiv.org/abs/2310.17514</link><description>&lt;p&gt;
&#35780;&#20272;&#32467;&#26524;&#30340;&#26377;&#25928;&#24615;&#65306;&#35780;&#20272;&#32452;&#21512;&#24615;&#22522;&#20934;&#30340;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Validity of Evaluation Results: Assessing Concurrence Across Compositionality Benchmarks. (arXiv:2310.17514v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17514
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#22312;&#32452;&#21512;&#27867;&#21270;&#39046;&#22495;&#20013;&#29305;&#23450;&#25968;&#25454;&#38598;&#35774;&#35745;&#36873;&#25321;&#23545;&#27169;&#22411;&#33021;&#21147;&#32467;&#35770;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#19981;&#21516;&#25968;&#25454;&#38598;&#23545;&#24314;&#27169;&#26041;&#27861;&#30340;&#25490;&#21517;&#23384;&#22312;&#24046;&#24322;&#65292;&#20154;&#31867;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#39640;&#20110;&#21512;&#25104;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#25968;&#25454;&#38598;&#26159;&#21542;&#26469;&#33258;&#21516;&#19968;&#26469;&#28304;&#26356;&#33021;&#39044;&#27979;&#32467;&#26524;&#30340;&#27169;&#22411;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#26681;&#25454;&#25552;&#20986;&#30340;&#35768;&#22810;&#25968;&#25454;&#38598;&#65292;NLP&#27169;&#22411;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#27493;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#29305;&#23450;&#25968;&#25454;&#38598;&#35774;&#35745;&#36873;&#25321;&#22914;&#20309;&#24433;&#21709;&#25105;&#20204;&#23545;&#27169;&#22411;&#33021;&#21147;&#30340;&#32467;&#35770;&#20173;&#28982;&#23384;&#22312;&#30097;&#38382;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#32452;&#21512;&#27867;&#21270;&#39046;&#22495;&#20013;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#23545;&#20845;&#31181;&#24314;&#27169;&#26041;&#27861;&#36827;&#34892;&#20102;&#24615;&#33021;&#35780;&#20272;&#65292;&#24182;&#26681;&#25454;&#20843;&#31181;&#32452;&#21512;&#24615;&#20998;&#21106;&#31574;&#30053;&#23558;&#27169;&#22411;&#25353;&#29031;18&#20010;&#32452;&#21512;&#27867;&#21270;&#25286;&#20998;&#36827;&#34892;&#25490;&#21517;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65306;i&#65289;&#23613;&#31649;&#25152;&#26377;&#25968;&#25454;&#38598;&#37117;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;&#32452;&#21512;&#27867;&#21270;&#65292;&#20294;&#23427;&#20204;&#23545;&#24314;&#27169;&#26041;&#27861;&#30340;&#25490;&#21517;&#26377;&#25152;&#19981;&#21516;&#65307;ii&#65289;&#30001;&#20154;&#31867;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#24444;&#27492;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#27604;&#23427;&#20204;&#19982;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#19968;&#33268;&#24615;&#26356;&#39640;&#65292;&#25110;&#32773;&#27604;&#21512;&#25104;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#26356;&#39640;&#65307;iii&#65289;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#25968;&#25454;&#38598;&#26159;&#21542;&#26469;&#33258;&#21516;&#19968;&#26469;&#28304;&#26356;&#33021;&#39044;&#27979;&#32467;&#26524;&#30340;&#27169;&#22411;&#25490;&#21517;&#65292;&#32780;&#19981;&#26159;&#23427;&#20204;&#26159;&#21542;&#20445;&#25345;&#30456;&#21516;&#30340;&#32452;&#21512;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
NLP models have progressed drastically in recent years, according to numerous datasets proposed to evaluate performance. Questions remain, however, about how particular dataset design choices may impact the conclusions we draw about model capabilities. In this work, we investigate this question in the domain of compositional generalization. We examine the performance of six modeling approaches across 4 datasets, split according to 8 compositional splitting strategies, ranking models by 18 compositional generalization splits in total. Our results show that: i) the datasets, although all designed to evaluate compositional generalization, rank modeling approaches differently; ii) datasets generated by humans align better with each other than they with synthetic datasets, or than synthetic datasets among themselves; iii) generally, whether datasets are sampled from the same source is more predictive of the resulting model ranking than whether they maintain the same interpretation of compos
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#23545;&#20110;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#65292;&#24403;LoRA-rank&#8805;&#65288;f&#30340;&#23485;&#24230;&#65289;&#215;&#65288;&#30446;&#26631;&#27169;&#22411;&#30340;&#28145;&#24230;/ f&#30340;&#28145;&#24230;&#65289;&#26102;&#65292;LoRA&#21487;&#20197;&#20351;&#20219;&#20309;&#27169;&#22411;f&#20934;&#30830;&#34920;&#31034;&#20219;&#20309;&#36739;&#23567;&#30340;&#30446;&#26631;&#27169;&#22411;f&#12290;&#23545;&#20110;Transformer&#32593;&#32476;&#65292;&#36890;&#36807;rank-&#65288;&#23884;&#20837;&#22823;&#23567;/ 2&#65289;&#30340;LoRA&#36866;&#37197;&#22120;&#21487;&#20197;&#20351;&#20219;&#20309;&#27169;&#22411;&#36866;&#24212;&#20110;&#30456;&#21516;&#22823;&#23567;&#30340;&#30446;&#26631;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.17513</link><description>&lt;p&gt;
&#12298;&#20302;&#31209;&#36866;&#24212;&#30340;&#34920;&#36798;&#33021;&#21147;&#12299;
&lt;/p&gt;
&lt;p&gt;
The Expressive Power of Low-Rank Adaptation. (arXiv:2310.17513v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#23545;&#20110;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#65292;&#24403;LoRA-rank&#8805;&#65288;f&#30340;&#23485;&#24230;&#65289;&#215;&#65288;&#30446;&#26631;&#27169;&#22411;&#30340;&#28145;&#24230;/ f&#30340;&#28145;&#24230;&#65289;&#26102;&#65292;LoRA&#21487;&#20197;&#20351;&#20219;&#20309;&#27169;&#22411;f&#20934;&#30830;&#34920;&#31034;&#20219;&#20309;&#36739;&#23567;&#30340;&#30446;&#26631;&#27169;&#22411;f&#12290;&#23545;&#20110;Transformer&#32593;&#32476;&#65292;&#36890;&#36807;rank-&#65288;&#23884;&#20837;&#22823;&#23567;/ 2&#65289;&#30340;LoRA&#36866;&#37197;&#22120;&#21487;&#20197;&#20351;&#20219;&#20309;&#27169;&#22411;&#36866;&#24212;&#20110;&#30456;&#21516;&#22823;&#23567;&#30340;&#30446;&#26631;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#26159;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#21033;&#29992;&#30697;&#38453;&#30340;&#20302;&#31209;&#36866;&#24212;&#24615;&#65292;&#22312;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#65289;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#23613;&#31649;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#26159;LoRA&#30340;&#29702;&#35770;&#22522;&#30784;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#23578;&#26410;&#24471;&#21040;&#25506;&#32034;&#12290;&#26412;&#25991;&#36890;&#36807;&#20174;&#29702;&#35770;&#35282;&#24230;&#20998;&#26512;LoRA&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#39318;&#27425;&#23581;&#35797;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#65292;&#22914;&#26524;LoRA-rank&#8805;&#65288;f&#30340;&#23485;&#24230;&#65289;&#215;&#65288;&#30446;&#26631;&#27169;&#22411;&#30340;&#28145;&#24230;/ f&#30340;&#28145;&#24230;&#65289;&#65292;&#21017;LoRA&#21487;&#20197;&#20351;&#20219;&#20309;&#27169;&#22411;f&#20934;&#30830;&#34920;&#31034;&#20219;&#20309;&#36739;&#23567;&#30340;&#30446;&#26631;&#27169;&#22411;f&#12290;&#24403;LoRA-rank&#20302;&#20110;&#38408;&#20540;&#26102;&#65292;&#25105;&#20204;&#36824;&#37327;&#21270;&#20102;&#36924;&#36817;&#35823;&#24046;&#12290;&#23545;&#20110;Transformer&#32593;&#32476;&#65292;&#25105;&#20204;&#35777;&#26126;&#20219;&#20309;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;rank-&#65288;&#23884;&#20837;&#22823;&#23567;/ 2&#65289;&#30340;LoRA&#36866;&#37197;&#22120;&#36866;&#24212;&#20110;&#30456;&#21516;&#22823;&#23567;&#30340;&#30446;&#26631;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-Rank Adaptation (LoRA), a parameter-efficient fine-tuning method that leverages low-rank adaptation of weight matrices, has emerged as a prevalent technique for fine-tuning pre-trained models such as large language models and diffusion models. Despite its huge success in practice, the theoretical underpinnings of LoRA have largely remained unexplored. This paper takes the first step to bridge this gap by theoretically analyzing the expressive power of LoRA. We prove that, for fully connected neural networks, LoRA can adapt any model $f$ to accurately represent any smaller target model $\overline{f}$ if LoRA-rank $\geq(\text{width of }f) \times \frac{\text{depth of }\overline{f}}{\text{depth of }f}$. We also quantify the approximation error when LoRA-rank is lower than the threshold. For Transformer networks, we show any model can be adapted to a target model of the same size with rank-$(\frac{\text{embedding size}}{2})$ LoRA adapters.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#31454;&#20105;&#34892;&#20026;&#12290;&#36890;&#36807;&#24314;&#31435;&#19968;&#20010;&#31454;&#20105;&#29615;&#22659;&#24182;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#31454;&#20105;&#21487;&#20197;&#20419;&#20351;&#26234;&#33021;&#20307;&#36827;&#34892;&#36716;&#21464;&#21644;&#37319;&#21462;&#26032;&#31574;&#30053;&#65292;&#20174;&#32780;&#22312;&#31038;&#20250;&#21644;&#32463;&#27982;&#21457;&#23637;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.17512</link><description>&lt;p&gt;
CompeteAI:&#29702;&#35299;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#20307;&#31454;&#20105;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
CompeteAI: Understanding the Competition Behaviors in Large Language Model-based Agents. (arXiv:2310.17512v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#31454;&#20105;&#34892;&#20026;&#12290;&#36890;&#36807;&#24314;&#31435;&#19968;&#20010;&#31454;&#20105;&#29615;&#22659;&#24182;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#31454;&#20105;&#21487;&#20197;&#20419;&#20351;&#26234;&#33021;&#20307;&#36827;&#34892;&#36716;&#21464;&#21644;&#37319;&#21462;&#26032;&#31574;&#30053;&#65292;&#20174;&#32780;&#22312;&#31038;&#20250;&#21644;&#32463;&#27982;&#21457;&#23637;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#23436;&#25104;&#19981;&#21516;&#20219;&#21153;&#65292;&#22914;&#20010;&#20154;&#21161;&#29702;&#25110;&#20107;&#20214;&#35268;&#21010;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21512;&#20316;&#19982;&#21327;&#20316;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#25506;&#32034;&#21478;&#19968;&#20010;&#37325;&#35201;&#26426;&#21046;&#8212;&#8212;&#31454;&#20105;&#65292;&#23427;&#26159;&#31038;&#20250;&#21644;&#32463;&#27982;&#21457;&#23637;&#30340;&#25512;&#21160;&#21147;&#20043;&#19968;&#12290;&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;LLM&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#31454;&#20105;&#34892;&#20026;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#30740;&#31350;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#31454;&#20105;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;GPT-4&#23454;&#29616;&#20102;&#19968;&#20010;&#23454;&#38469;&#30340;&#31454;&#20105;&#29615;&#22659;&#65292;&#27169;&#25311;&#20102;&#19968;&#20010;&#30001;&#39184;&#39302;&#26234;&#33021;&#20307;&#21644;&#39038;&#23458;&#26234;&#33021;&#20307;&#32452;&#25104;&#30340;&#34394;&#25311;&#22478;&#38215;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#39184;&#39302;&#26234;&#33021;&#20307;&#30456;&#20114;&#31454;&#20105;&#20197;&#21560;&#24341;&#26356;&#22810;&#39038;&#23458;&#65292;&#36825;&#31181;&#31454;&#20105;&#20419;&#20351;&#23427;&#20204;&#36827;&#34892;&#36716;&#21464;&#65292;&#27604;&#22914;&#22521;&#20859;&#26032;&#30340;&#36816;&#33829;&#31574;&#30053;&#12290;&#25105;&#20204;&#23454;&#39564;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#20174;&#31038;&#20250;&#23398;&#20064;&#21040;&#39532;&#22826;&#25928;&#24212;&#31561;&#22810;&#20010;&#26377;&#36259;&#21457;&#29616;&#65292;&#19982;&#29616;&#26377;&#30340;&#31038;&#20250;&#23398;&#21644;&#32463;&#27982;&#23398;&#29702;&#35770;&#30456;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have been widely used as agents to complete different tasks, such as personal assistance or event planning. While most work has focused on cooperation and collaboration between agents, little work explores competition, another important mechanism that fosters the development of society and economy. In this paper, we seek to examine the competition behaviors in LLM-based agents. We first propose a general framework to study the competition between agents. Then, we implement a practical competitive environment using GPT-4 to simulate a virtual town with two types of agents, including restaurant agents and customer agents. Specifically, restaurant agents compete with each other to attract more customers, where the competition fosters them to transform, such as cultivating new operating strategies. The results of our experiments reveal several interesting findings ranging from social learning to Matthew Effect, which aligns well with existing sociological and e
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25913;&#36827;&#20102;&#25105;&#20204;&#22312;Blizzard Challenge 2021&#20013;&#25552;&#20132;&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#19968;&#20010;&#22522;&#20110;&#35268;&#21017;&#30340;&#25991;&#26412;&#21040;&#38899;&#32032;&#22788;&#29702;&#31995;&#32479;&#65292;&#24182;&#19988;&#35774;&#35745;&#20102;&#25968;&#25454;&#22788;&#29702;&#12289;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#26469;&#22788;&#29702;Blizzard Challenge 2023&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#26631;&#35782;&#31526;&#26159;G&#12290;</title><link>http://arxiv.org/abs/2310.17499</link><description>&lt;p&gt;
IMS Toucan&#31995;&#32479;&#29992;&#20110;Blizzard Challenge 2023
&lt;/p&gt;
&lt;p&gt;
The IMS Toucan System for the Blizzard Challenge 2023. (arXiv:2310.17499v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17499
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25913;&#36827;&#20102;&#25105;&#20204;&#22312;Blizzard Challenge 2021&#20013;&#25552;&#20132;&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#19968;&#20010;&#22522;&#20110;&#35268;&#21017;&#30340;&#25991;&#26412;&#21040;&#38899;&#32032;&#22788;&#29702;&#31995;&#32479;&#65292;&#24182;&#19988;&#35774;&#35745;&#20102;&#25968;&#25454;&#22788;&#29702;&#12289;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#26469;&#22788;&#29702;Blizzard Challenge 2023&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#26631;&#35782;&#31526;&#26159;G&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#21442;&#21152;Blizzard Challenge 2023&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#22312;Blizzard Challenge 2021&#20013;&#25552;&#20132;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#19968;&#20010;&#22522;&#20110;&#35268;&#21017;&#30340;&#25991;&#26412;&#21040;&#38899;&#32032;&#22788;&#29702;&#31995;&#32479;&#65292;&#21253;&#25324;&#23545;&#27861;&#35821;&#20013;&#30340;&#21516;&#38899;&#24322;&#24418;&#35789;&#36827;&#34892;&#22522;&#20110;&#35268;&#21017;&#30340;&#28040;&#27495;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;Conformer&#21644;Glow&#30340;&#24555;&#36895;&#39640;&#25928;&#30340;&#38750;&#33258;&#22238;&#24402;&#21512;&#25104;&#26550;&#26500;&#23558;&#38899;&#32032;&#36716;&#25442;&#20026;&#20013;&#38388;&#34920;&#31034; - &#39057;&#35889;&#22270;&#12290;&#19968;&#20010;&#22522;&#20110;GAN&#30340;&#31070;&#32463;&#22768;&#30721;&#22120;&#32467;&#21512;&#20102;&#26368;&#26032;&#30340;&#20808;&#36827;&#26041;&#27861;&#65292;&#23558;&#39057;&#35889;&#22270;&#36716;&#25442;&#20026;&#26368;&#32456;&#30340;&#27874;&#24418;&#12290;&#25105;&#20204;&#31934;&#24515;&#35774;&#35745;&#20102;&#29992;&#20110;&#25361;&#25112;&#25968;&#25454;&#30340;&#25968;&#25454;&#22788;&#29702;&#12289;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#26631;&#35782;&#31526;&#26159;G&#12290;&#25552;&#20379;&#24320;&#28304;&#20195;&#30721;&#21644;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
For our contribution to the Blizzard Challenge 2023, we improved on the system we submitted to the Blizzard Challenge 2021. Our approach entails a rule-based text-to-phoneme processing system that includes rule-based disambiguation of homographs in the French language. It then transforms the phonemes to spectrograms as intermediate representations using a fast and efficient non-autoregressive synthesis architecture based on Conformer and Glow. A GAN based neural vocoder that combines recent state-of-the-art approaches converts the spectrogram to the final wave. We carefully designed the data processing, training, and inference procedures for the challenge data. Our system identifier is G. Open source code and demo are available.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20943;&#23569;&#26080;&#20851;&#25991;&#26723;&#30340;&#24178;&#25200;&#26469;&#25913;&#21892;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#30340;&#38646;&#26679;&#26412;&#38405;&#35835;&#22120;&#30340;&#26041;&#27861;&#12290;&#37319;&#29992;&#20102;&#24178;&#25200;&#24863;&#30693;&#30340;&#31572;&#26696;&#36873;&#25321;(DAS)&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;LLMs&#21463;&#21040;&#24178;&#25200;&#21644;&#36807;&#24230;&#33258;&#20449;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#25104;&#21151;&#22320;&#25913;&#21892;&#20102;&#38646;&#26679;&#26412;&#38405;&#35835;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17490</link><description>&lt;p&gt;
&#25552;&#39640;&#36890;&#36807;&#20943;&#23569;&#26080;&#20851;&#25991;&#26723;&#23545;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#30340;&#38646;&#26679;&#26412;&#38405;&#35835;&#22120;&#30340;&#24178;&#25200;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improving Zero-shot Reader by Reducing Distractions from Irrelevant Documents in Open-Domain Question Answering. (arXiv:2310.17490v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17490
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20943;&#23569;&#26080;&#20851;&#25991;&#26723;&#30340;&#24178;&#25200;&#26469;&#25913;&#21892;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#30340;&#38646;&#26679;&#26412;&#38405;&#35835;&#22120;&#30340;&#26041;&#27861;&#12290;&#37319;&#29992;&#20102;&#24178;&#25200;&#24863;&#30693;&#30340;&#31572;&#26696;&#36873;&#25321;(DAS)&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;LLMs&#21463;&#21040;&#24178;&#25200;&#21644;&#36807;&#24230;&#33258;&#20449;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#25104;&#21151;&#22320;&#25913;&#21892;&#20102;&#38646;&#26679;&#26412;&#38405;&#35835;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20351;&#24471;&#22312;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;(ODQA)&#20013;&#23454;&#29616;&#38646;&#26679;&#26412;&#26041;&#27861;&#25104;&#20026;&#21487;&#33021;&#65292;&#20294;&#26159;&#30001;&#20110;&#38405;&#35835;&#22120;&#30456;&#23545;&#20110;&#26816;&#32034;&#22120;&#30340;&#36827;&#23637;&#26377;&#38480;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#19968;&#31181;&#38646;&#26679;&#26412;&#38405;&#35835;&#22120;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#35299;&#20915;&#35745;&#31639;&#25104;&#26412;&#21644;&#26631;&#27880;&#25968;&#25454;&#38656;&#27714;&#31561;&#25361;&#25112;&#12290;&#25105;&#20204;&#21457;&#29616;LLMs&#30001;&#20110;&#26816;&#32034;&#21040;&#30340;&#26080;&#20851;&#25991;&#26723;&#20197;&#21450;&#20316;&#20026;&#38646;&#26679;&#26412;&#38405;&#35835;&#22120;&#26102;&#29983;&#25104;&#31572;&#26696;&#30340;&#36807;&#24230;&#33258;&#20449;&#32780;&#21463;&#21040;&#24178;&#25200;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#22522;&#20110;&#21542;&#23450;&#30340;&#25351;&#20196;&#21644;&#20998;&#25968;&#35843;&#25972;&#30340;&#24178;&#25200;&#24863;&#30693;&#30340;&#31572;&#26696;&#36873;&#25321;(DAS)&#26041;&#27861;&#65292;&#20197;&#20943;&#36731;&#36825;&#20123;&#25991;&#26723;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#22788;&#29702;&#20102;&#19981;&#21516;&#22330;&#26223;&#19979;&#30340;&#24178;&#25200;&#65292;&#25552;&#39640;&#20102;&#38646;&#26679;&#26412;&#38405;&#35835;&#22120;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#19982;&#38754;&#23545;&#26410;&#35265;&#36807;&#25968;&#25454;&#32780;&#22256;&#38590;&#37325;&#37325;&#30340;&#30417;&#30563;&#24335;&#38405;&#35835;&#22120;&#19981;&#21516;&#65292;&#38646;&#26679;&#26412;&#38405;&#35835;&#22120;&#23637;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#21487;&#36801;&#31227;&#24615;&#65292;&#26080;&#38656;&#20219;&#20309;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) enable zero-shot approaches in open-domain question answering (ODQA), yet with limited advancements as the reader is compared to the retriever. This study aims at the feasibility of a zero-shot reader that addresses the challenges of computational cost and the need for labeled data. We find that LLMs are distracted due to irrelevant documents in the retrieved set and the overconfidence of the generated answers when they are exploited as zero-shot readers. To tackle these problems, we mitigate the impact of such documents via Distraction-aware Answer Selection (DAS) with a negation-based instruction and score adjustment for proper answer selection. Experimental results show that our approach successfully handles distraction across diverse scenarios, enhancing the performance of zero-shot readers. Furthermore, unlike supervised readers struggling with unseen data, zero-shot readers demonstrate outstanding transferability without any training.
&lt;/p&gt;</description></item><item><title>LightLM&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22522;&#20110;Transformer&#30340;&#29983;&#25104;&#25512;&#33616;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#36731;&#37327;&#32423;&#28145;&#31364;Transformer&#26550;&#26500;&#26469;&#23454;&#29616;&#30452;&#25509;&#29983;&#25104;&#25512;&#33616;&#39033;&#12290;</title><link>http://arxiv.org/abs/2310.17488</link><description>&lt;p&gt;
LightLM: &#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22522;&#20110;Transformer&#30340;&#29983;&#25104;&#25512;&#33616;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LightLM: A Lightweight Deep and Narrow Language Model for Generative Recommendation. (arXiv:2310.17488v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17488
&lt;/p&gt;
&lt;p&gt;
LightLM&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22522;&#20110;Transformer&#30340;&#29983;&#25104;&#25512;&#33616;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#36731;&#37327;&#32423;&#28145;&#31364;Transformer&#26550;&#26500;&#26469;&#23454;&#29616;&#30452;&#25509;&#29983;&#25104;&#25512;&#33616;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;LightLM&#65292;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22522;&#20110;Transformer&#30340;&#29983;&#25104;&#25512;&#33616;&#27169;&#22411;&#12290;&#22312;NLP&#21644;&#35270;&#35273;&#31561;&#21508;&#20010;&#20154;&#24037;&#26234;&#33021;&#23376;&#39046;&#22495;&#20013;&#65292;&#22522;&#20110;Transformer&#30340;&#29983;&#25104;&#24314;&#27169;&#24050;&#32463;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#32780;&#29983;&#25104;&#25512;&#33616;&#30001;&#20110;&#20854;&#23545;&#20010;&#24615;&#21270;&#29983;&#25104;&#24314;&#27169;&#30340;&#29420;&#29305;&#38656;&#27714;&#65292;&#20173;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#12290;&#29616;&#26377;&#30340;&#29983;&#25104;&#25512;&#33616;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#38754;&#21521;NLP&#30340;Transformer&#26550;&#26500;&#65292;&#22914;T5&#12289;GPT&#12289;LLaMA&#21644;M6&#65292;&#36825;&#20123;&#27169;&#22411;&#27604;&#36739;&#24222;&#22823;&#65292;&#19988;&#24182;&#27809;&#26377;&#19987;&#38376;&#38024;&#23545;&#25512;&#33616;&#20219;&#21153;&#36827;&#34892;&#35774;&#35745;&#12290;LightLM&#36890;&#36807;&#24341;&#20837;&#36731;&#37327;&#32423;&#28145;&#31364;Transformer&#26550;&#26500;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#26550;&#26500;&#29305;&#21035;&#36866;&#29992;&#20110;&#30452;&#25509;&#29983;&#25104;&#25512;&#33616;&#39033;&#12290;&#36825;&#31181;&#32467;&#26500;&#23545;&#20110;&#30452;&#25509;&#30340;&#29983;&#25104;&#25512;&#33616;&#38750;&#24120;&#21512;&#36866;&#65292;&#22240;&#20026;&#36755;&#20837;&#20027;&#35201;&#30001;&#36866;&#21512;&#27169;&#22411;&#23481;&#37327;&#30340;&#30701;&#26631;&#35760;&#32452;&#25104;&#65292;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#19981;&#38656;&#35201;&#22826;&#23485;&#30340;&#32467;&#26500;&#12290;&#25105;&#20204;&#36824;...
&lt;/p&gt;
&lt;p&gt;
This paper presents LightLM, a lightweight Transformer-based language model for generative recommendation. While Transformer-based generative modeling has gained importance in various AI sub-fields such as NLP and vision, generative recommendation is still in its infancy due to its unique demand on personalized generative modeling. Existing works on generative recommendation often use NLP-oriented Transformer architectures such as T5, GPT, LLaMA and M6, which are heavy-weight and are not specifically designed for recommendation tasks. LightLM tackles the issue by introducing a light-weight deep and narrow Transformer architecture, which is specifically tailored for direct generation of recommendation items. This structure is especially apt for straightforward generative recommendation and stems from the observation that language model does not have to be too wide for this task, as the input predominantly consists of short tokens that are well-suited for the model's capacity. We also sh
&lt;/p&gt;</description></item><item><title>TalTech&#20351;&#29992;&#20102;&#26041;&#35328;&#36866;&#24212;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#24182;&#20998;&#21035;&#22312;&#25552;&#20379;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#39069;&#22806;&#38899;&#39057;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20302;&#36164;&#28304;ASR&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#26368;&#20302;&#30340;&#35789;&#38169;&#35823;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.17448</link><description>&lt;p&gt;
&#12298;&#26041;&#35328;&#36866;&#24212;&#19982;&#25968;&#25454;&#22686;&#24378;&#23545;&#20110;&#36164;&#28304;&#21294;&#20047; ASR &#30340;&#24433;&#21709;&#65306;TalTech &#31995;&#32479;&#22312; MADASR 2023 &#25361;&#25112;&#20013;&#30340;&#34920;&#29616;&#12299;
&lt;/p&gt;
&lt;p&gt;
Dialect Adaptation and Data Augmentation for Low-Resource ASR: TalTech Systems for the MADASR 2023 Challenge. (arXiv:2310.17448v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17448
&lt;/p&gt;
&lt;p&gt;
TalTech&#20351;&#29992;&#20102;&#26041;&#35328;&#36866;&#24212;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#24182;&#20998;&#21035;&#22312;&#25552;&#20379;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#39069;&#22806;&#38899;&#39057;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20302;&#36164;&#28304;ASR&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#26368;&#20302;&#30340;&#35789;&#38169;&#35823;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;TalTech&#24320;&#21457;&#30340;ASRU MADASR 2023&#25361;&#25112;&#20013;&#65292;&#38024;&#23545;&#26041;&#35328;&#20016;&#23500;&#30340;&#21360;&#24230;&#35821;&#35328;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#12290;&#25361;&#25112;&#20027;&#35201;&#20851;&#27880;&#26377;&#38480;&#30340;&#35757;&#32451;&#38899;&#39057;&#21644;&#25991;&#26412;&#25968;&#25454;&#12290;TalTech&#21442;&#19982;&#20102;&#25361;&#25112;&#30340;&#20004;&#20010;&#20219;&#21153;&#65306;&#31532;&#19968;&#20010;&#20219;&#21153;&#21482;&#33021;&#20351;&#29992;&#25552;&#20379;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#31532;&#19977;&#20010;&#20219;&#21153;&#21487;&#20197;&#20351;&#29992;&#39069;&#22806;&#30340;&#38899;&#39057;&#25968;&#25454;&#12290;&#22312;&#36825;&#20004;&#20010;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;wav2vec2.0&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20256;&#32479;&#30340;finetuning&#39044;&#35757;&#32451;&#30340;wav2vec2.0&#27169;&#22411;&#30340;&#36807;&#31243;&#26377;&#20004;&#20010;&#20851;&#38190;&#28857;&#30340;&#24046;&#24322;&#65306;&#39318;&#20808;&#65292;&#36890;&#36807;&#23454;&#26045;&#23545;&#40784;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#26469;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#30340;&#35821;&#35328;&#22810;&#26679;&#24615;&#65307;&#20854;&#27425;&#65292;&#36890;&#36807;&#28145;&#23618;&#21069;&#32512;&#35843;&#25972;&#23558;wav2vec2.0&#27169;&#22411;&#36827;&#34892;&#26041;&#35328;&#36866;&#24212;&#12290;&#22312;&#36825;&#20004;&#20010;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#25913;&#36827;&#20102;&#25552;&#20379;&#30340;&#22522;&#20934;&#32447;&#65292;&#36798;&#21040;&#20102;&#21442;&#19982;&#22242;&#38431;&#20013;&#26368;&#20302;&#30340;&#35789;&#38169;&#35823;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes Tallinn University of Technology (TalTech) systems developed for the ASRU MADASR 2023 Challenge. The challenge focuses on automatic speech recognition of dialect-rich Indian languages with limited training audio and text data. TalTech participated in two tracks of the challenge: Track 1 that allowed using only the provided training data and Track 3 which allowed using additional audio data. In both tracks, we relied on wav2vec2.0 models. Our methodology diverges from the traditional procedure of finetuning pretrained wav2vec2.0 models in two key points: firstly, through the implementation of the aligned data augmentation technique to enhance the linguistic diversity of the training data, and secondly, via the application of deep prefix tuning for dialect adaptation of wav2vec2.0 models. In both tracks, our approach yielded significant improvements over the provided baselines, achieving the lowest word error rates across all participating teams.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21019;&#24314;&#21253;&#21547;&#35268;&#33539;&#35780;&#32423;&#30340;GPT&#29983;&#25104;&#30340;&#33521;&#25991;&#25991;&#26412;&#25968;&#25454;&#38598;&#65292;&#31995;&#32479;&#22320;&#20998;&#26512;&#20102;&#35266;&#23519;&#21040;&#30340;&#24615;&#21035;&#20559;&#35265;&#20027;&#39064;&#30340;&#21464;&#21270;&#21450;&#20854;&#23545;&#20559;&#35265;&#30340;&#24863;&#30693;&#12290;&#36890;&#36807;&#35748;&#35782;&#21040;&#20559;&#35265;&#24517;&#39035;&#20197;&#30456;&#23545;&#30340;&#23610;&#24230;&#26469;&#24863;&#30693;&#65292;&#26412;&#30740;&#31350;&#23545;&#19981;&#21516;&#31243;&#24230;&#20559;&#35265;&#30340;&#25509;&#21463;&#33021;&#21147;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#23545;&#24615;&#21035;&#20559;&#35265;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2310.17428</link><description>&lt;p&gt;
"&#20116;&#21313;&#31181;&#20559;&#35265;": GPT&#29983;&#25104;&#30340;&#33521;&#25991;&#25991;&#26412;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#30340;&#35268;&#33539;&#35780;&#32423;
&lt;/p&gt;
&lt;p&gt;
''Fifty Shades of Bias'': Normative Ratings of Gender Bias in GPT Generated English Text. (arXiv:2310.17428v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17428
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21019;&#24314;&#21253;&#21547;&#35268;&#33539;&#35780;&#32423;&#30340;GPT&#29983;&#25104;&#30340;&#33521;&#25991;&#25991;&#26412;&#25968;&#25454;&#38598;&#65292;&#31995;&#32479;&#22320;&#20998;&#26512;&#20102;&#35266;&#23519;&#21040;&#30340;&#24615;&#21035;&#20559;&#35265;&#20027;&#39064;&#30340;&#21464;&#21270;&#21450;&#20854;&#23545;&#20559;&#35265;&#30340;&#24863;&#30693;&#12290;&#36890;&#36807;&#35748;&#35782;&#21040;&#20559;&#35265;&#24517;&#39035;&#20197;&#30456;&#23545;&#30340;&#23610;&#24230;&#26469;&#24863;&#30693;&#65292;&#26412;&#30740;&#31350;&#23545;&#19981;&#21516;&#31243;&#24230;&#20559;&#35265;&#30340;&#25509;&#21463;&#33021;&#21147;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#23545;&#24615;&#21035;&#20559;&#35265;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#26159;&#31038;&#20250;&#20449;&#20208;&#20307;&#31995;&#34920;&#29616;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#22312;&#36825;&#26679;&#20570;&#30340;&#36807;&#31243;&#20013;&#65292;&#23427;&#20063;&#20351;&#25105;&#20204;&#31038;&#20250;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#20559;&#35265;&#24471;&#20197;&#24310;&#32493;&#12290;&#24615;&#21035;&#20559;&#35265;&#26159;&#25105;&#20204;&#31038;&#20250;&#20013;&#26368;&#26222;&#36941;&#30340;&#20559;&#35265;&#20043;&#19968;&#65292;&#22312;&#22312;&#32447;&#21644;&#31163;&#32447;&#35752;&#35770;&#20013;&#37117;&#33021;&#30475;&#21040;&#12290;&#38543;&#30528;LLMs&#22312;&#25991;&#26412;&#29983;&#25104;&#20013;&#36234;&#26469;&#36234;&#20687;&#20154;&#31867;&#19968;&#26679;&#27969;&#21033;&#65292;&#20102;&#35299;&#36825;&#20123;&#31995;&#32479;&#21487;&#33021;&#20135;&#29983;&#30340;&#20559;&#35265;&#30340;&#32454;&#24494;&#24046;&#21035;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#23558;&#24615;&#21035;&#20559;&#35265;&#35270;&#20026;&#19968;&#20010;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#35782;&#21040;&#20559;&#35265;&#24517;&#39035;&#20197;&#30456;&#23545;&#30340;&#23610;&#24230;&#26469;&#24863;&#30693;&#65307;&#25105;&#20204;&#35843;&#26597;&#20102;&#20559;&#35265;&#30340;&#29983;&#25104;&#21644;&#38543;&#21518;&#30340;&#25163;&#21160;&#26631;&#27880;&#32773;&#23545;&#19981;&#21516;&#31243;&#24230;&#20559;&#35265;&#30340;&#25509;&#21463;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#31532;&#19968;&#20010;&#20855;&#26377;&#35268;&#33539;&#35780;&#32423;&#30340;GPT&#29983;&#25104;&#30340;&#33521;&#25991;&#25991;&#26412;&#25968;&#25454;&#38598;&#12290;&#20351;&#29992;Best--Worst Scaling&#36827;&#34892;&#35780;&#32423;&#33719;&#21462;--&#36825;&#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#27604;&#36739;&#26631;&#27880;&#26694;&#26550;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#20998;&#26512;&#20102;&#35266;&#23519;&#25490;&#21517;&#20013;&#24615;&#21035;&#20559;&#35265;&#20027;&#39064;&#30340;&#21464;&#21270;&#65292;&#24182;&#23637;&#31034;&#20102;&#36523;&#20221;&#22312;&#20854;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language serves as a powerful tool for the manifestation of societal belief systems. In doing so, it also perpetuates the prevalent biases in our society. Gender bias is one of the most pervasive biases in our society and is seen in online and offline discourses. With LLMs increasingly gaining human-like fluency in text generation, gaining a nuanced understanding of the biases these systems can generate is imperative. Prior work often treats gender bias as a binary classification task. However, acknowledging that bias must be perceived at a relative scale; we investigate the generation and consequent receptivity of manual annotators to bias of varying degrees. Specifically, we create the first dataset of GPT-generated English text with normative ratings of gender bias. Ratings were obtained using Best--Worst Scaling -- an efficient comparative annotation framework. Next, we systematically analyze the variation of themes of gender biases in the observed ranking and show that identity-at
&lt;/p&gt;</description></item><item><title>PETA&#36890;&#36807;&#35780;&#20272;&#20122;&#35789;&#20999;&#20998;&#22312;&#34507;&#30333;&#36136;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#20851;&#20110;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#30340;&#32508;&#21512;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#35789;&#27719;&#34920;&#22823;&#23567;&#22312;50&#20197;&#19978;&#33021;&#22815;&#33719;&#24471;&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.17415</link><description>&lt;p&gt;
PETA: &#35780;&#20272;&#20122;&#35789;&#20999;&#20998;&#23545;&#34507;&#30333;&#36136;&#36801;&#31227;&#23398;&#20064;&#22312;&#19979;&#28216;&#24212;&#29992;&#20013;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
PETA: Evaluating the Impact of Protein Transfer Learning with Sub-word Tokenization on Downstream Applications. (arXiv:2310.17415v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17415
&lt;/p&gt;
&lt;p&gt;
PETA&#36890;&#36807;&#35780;&#20272;&#20122;&#35789;&#20999;&#20998;&#22312;&#34507;&#30333;&#36136;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#20851;&#20110;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#30340;&#32508;&#21512;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#35789;&#27719;&#34920;&#22823;&#23567;&#22312;50&#20197;&#19978;&#33021;&#22815;&#33719;&#24471;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#25797;&#38271;&#25429;&#25417;&#21407;&#22987;&#32467;&#26500;&#20013;&#30340;&#36827;&#21270;&#20449;&#24687;&#65292;&#23545;&#34507;&#30333;&#36136;&#24037;&#31243;&#20855;&#26377;&#37325;&#35201;&#23454;&#29992;&#20215;&#20540;&#12290;&#19982;&#33258;&#28982;&#35821;&#35328;&#27169;&#22411;&#30456;&#27604;&#65292;&#34507;&#30333;&#36136;&#27688;&#22522;&#37240;&#24207;&#21015;&#30340;&#25968;&#25454;&#37327;&#36739;&#23567;&#65292;&#32452;&#21512;&#31354;&#38388;&#26377;&#38480;&#12290;&#36873;&#25321;&#21512;&#36866;&#30340;&#35789;&#27719;&#34920;&#22823;&#23567;&#26469;&#20248;&#21270;&#39044;&#35757;&#32451;&#27169;&#22411;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#33258;&#28982;&#35821;&#35328;&#39046;&#22495;&#25317;&#26377;&#22823;&#37327;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#30740;&#31350;&#65292;&#20294;&#30446;&#21069;&#36824;&#32570;&#20047;&#19968;&#20010;&#20840;&#38754;&#35780;&#20272;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#36136;&#37327;&#30340;&#22522;&#20934;&#12290;&#37492;&#20110;&#36825;&#20123;&#25361;&#25112;&#65292;PETA&#20351;&#29992;&#20102;&#19977;&#31181;&#26631;&#35760;&#21270;&#26041;&#27861;&#65292;&#22312;14&#31181;&#19981;&#21516;&#30340;&#35789;&#27719;&#34920;&#22823;&#23567;&#19979;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;&#23427;&#22312;33&#20010;&#19981;&#21516;&#30340;&#19979;&#28216;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#25968;&#21315;&#27425;&#27979;&#35797;&#65292;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#36801;&#31227;&#23398;&#20064;&#33021;&#21147;&#65292;&#24182;&#32467;&#21512;&#20102;&#20004;&#20010;&#20998;&#31867;&#22836;&#21644;&#19977;&#20010;&#38543;&#26426;&#31181;&#23376;&#20197;&#20943;&#36731;&#28508;&#22312;&#20559;&#35265;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#35789;&#27719;&#34920;&#22823;&#23567;&#22312;50&#20197;&#19978;&#22823;&#32422;&#33021;&#22815;&#33719;&#24471;&#26368;&#20339;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large protein language models are adept at capturing the underlying evolutionary information in primary structures, offering significant practical value for protein engineering. Compared to natural language models, protein amino acid sequences have a smaller data volume and a limited combinatorial space. Choosing an appropriate vocabulary size to optimize the pre-trained model is a pivotal issue. Moreover, despite the wealth of benchmarks and studies in the natural language community, there remains a lack of a comprehensive benchmark for systematically evaluating protein language model quality. Given these challenges, PETA trained language models with 14 different vocabulary sizes under three tokenization methods. It conducted thousands of tests on 33 diverse downstream datasets to assess the models' transfer learning capabilities, incorporating two classification heads and three random seeds to mitigate potential biases. Extensive experiments indicate that vocabulary sizes between 50 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;GPT-3.5-turbo&#23545;&#27861;&#24459;&#26696;&#20363;&#20013;&#30340;&#20462;&#36766;&#35282;&#33394;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#31034;&#20363;&#36827;&#34892;&#24341;&#23548;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.17413</link><description>&lt;p&gt;
&#21033;&#29992;GPT-3.5-turbo&#36827;&#34892;&#27861;&#24459;&#26696;&#20363;&#20462;&#36766;&#35282;&#33394;&#39044;&#27979;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Harnessing GPT-3.5-turbo for Rhetorical Role Prediction in Legal Cases. (arXiv:2310.17413v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17413
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;GPT-3.5-turbo&#23545;&#27861;&#24459;&#26696;&#20363;&#20013;&#30340;&#20462;&#36766;&#35282;&#33394;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#31034;&#20363;&#36827;&#34892;&#24341;&#23548;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#25506;&#35752;&#22312;&#27861;&#24459;&#26696;&#20363;&#30340;&#20462;&#36766;&#35282;&#33394;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;&#20351;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#29983;&#25104;&#24335;&#36716;&#25442;&#22120;GPT-3.5-turbo&#30340;&#19968;&#38454;&#24341;&#23548;&#25216;&#26415;&#12290;&#35813;&#20219;&#21153;&#38656;&#35201;&#22788;&#29702;&#25991;&#26412;&#32972;&#26223;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#38646;&#25110;&#23569;&#26679;&#26412;&#12289;&#20219;&#21153;&#35268;&#33539;&#21270;&#19982;&#23450;&#20041;&#12289;&#21400;&#28165;&#26631;&#27880;&#27169;&#31946;&#24615;&#12289;&#25991;&#26412;&#32972;&#26223;&#21644;&#22522;&#20110;&#24120;&#29992;&#25552;&#31034;&#21644;&#20855;&#20307;&#38382;&#39064;&#30340;&#25512;&#29702;&#31561;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#26679;&#26412;&#25968;&#37327;&#12289;&#26631;&#31614;&#23450;&#20041;&#12289;&#25991;&#26412;&#32972;&#26223;&#30340;&#23637;&#31034;&#20197;&#21450;&#38024;&#23545;&#35813;&#32972;&#26223;&#30340;&#20855;&#20307;&#38382;&#39064;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#26377;&#31215;&#26497;&#24433;&#21709;&#12290;&#22312;&#38750;&#31561;&#20215;&#30340;&#27979;&#35797;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20351;&#29992;&#23569;&#37327;&#26469;&#33258;&#30452;&#25509;&#32972;&#26223;&#30340;&#26631;&#35760;&#31034;&#20363;&#36827;&#34892;&#24341;&#23548;&#21487;&#20197;&#27604;&#22522;&#20110;BERT&#32534;&#30721;&#22120;&#30340;&#30417;&#30563;&#35843;&#20248;&#22810;&#20998;&#31867;&#22120;&#65288;&#21152;&#26435;F1&#24471;&#20998;&#20026;72%&#65289;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#20294;&#20173;&#23384;&#22312;&#24046;&#36317;&#65292;&#26377;&#24453;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a comprehensive study of one-stage elicitation techniques for querying a large pre-trained generative transformer (GPT-3.5-turbo) in the rhetorical role prediction task of legal cases. This task is known as requiring textual context to be addressed. Our study explores strategies such as zero-few shots, task specification with definitions and clarification of annotation ambiguities, textual context and reasoning with general prompts and specific questions. We show that the number of examples, the definition of labels, the presentation of the (labelled) textual context and specific questions about this context have a positive influence on the performance of the model. Given non-equivalent test set configurations, we observed that prompting with a few labelled examples from direct context can lead the model to a better performance than a supervised fined-tuned multi-class classifier based on the BERT encoder (weighted F1 score of = 72%). But there is still a gap to reach the pe
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#20351;&#29992;Exo&#32534;&#35793;&#22120;&#29983;&#25104;&#25509;&#36817;&#20110;&#29978;&#33267;&#20248;&#20110;&#25163;&#21160;&#24320;&#21457;&#30340;&#24494;&#20869;&#26680;&#30340;&#27493;&#39588;&#21644;&#26041;&#27861;&#65292;&#24182;&#35299;&#20915;&#20102;&#20026;&#27599;&#20010;&#26032;&#30828;&#20214;&#29983;&#25104;&#19987;&#29992;&#24494;&#20869;&#26680;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.17408</link><description>&lt;p&gt;
&#20351;&#29992;Exo&#35299;&#20915;&#30697;&#38453;&#20056;&#27861;&#24494;&#20869;&#26680;&#29983;&#25104;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Tackling the Matrix Multiplication Micro-kernel Generation with Exo. (arXiv:2310.17408v1 [cs.MS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17408
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#20351;&#29992;Exo&#32534;&#35793;&#22120;&#29983;&#25104;&#25509;&#36817;&#20110;&#29978;&#33267;&#20248;&#20110;&#25163;&#21160;&#24320;&#21457;&#30340;&#24494;&#20869;&#26680;&#30340;&#27493;&#39588;&#21644;&#26041;&#27861;&#65292;&#24182;&#35299;&#20915;&#20102;&#20026;&#27599;&#20010;&#26032;&#30828;&#20214;&#29983;&#25104;&#19987;&#29992;&#24494;&#20869;&#26680;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#21313;&#24180;&#37324;&#65292;&#30697;&#38453;&#20056;&#27861;&#65288;&#25110;GEMM&#65289;&#30340;&#20248;&#21270;&#19968;&#30452;&#26159;&#19968;&#20010;&#38656;&#27714;&#12290;&#36825;&#20010;&#25805;&#20316;&#34987;&#35748;&#20026;&#26159;&#24403;&#21069;&#32447;&#24615;&#20195;&#25968;&#24211;&#65288;&#22914;BLIS&#65292;OpenBLAS&#25110;Intel OneAPI&#65289;&#30340;&#26071;&#33328;&#65292;&#22240;&#20026;&#23427;&#22312;&#21508;&#31181;&#31185;&#23398;&#24212;&#29992;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;GEMM&#36890;&#24120;&#26159;&#25353;&#29031;GotoBLAS&#30340;&#29702;&#24565;&#36827;&#34892;&#23454;&#29616;&#30340;&#65292;&#23427;&#23558;GEMM&#30340;&#25805;&#20316;&#25968;&#36827;&#34892;&#20999;&#21106;&#65292;&#24182;&#20351;&#29992;&#19968;&#31995;&#21015;&#23884;&#22871;&#24490;&#29615;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#36807;&#19968;&#23567;&#22359;&#38754;&#21521;&#30828;&#20214;&#30340;&#39640;&#24615;&#33021;&#20195;&#30721;&#65292;&#21363;&#24494;&#20869;&#26680;&#65292;&#25552;&#21462;&#20307;&#31995;&#32467;&#26500;&#30340;&#26368;&#22823;&#35745;&#31639;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#36843;&#20351;&#24320;&#21457;&#20154;&#21592;&#20026;&#27599;&#20010;&#26032;&#30340;&#30828;&#20214;&#29983;&#25104;&#19968;&#20010;&#19987;&#29992;&#30340;&#24494;&#20869;&#26680;&#65292;&#24182;&#38656;&#35201;&#38750;&#24120;&#22823;&#30340;&#24037;&#20316;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Exo&#32534;&#35793;&#22120;&#29983;&#25104;&#24494;&#20869;&#26680;&#30340;&#36880;&#27493;&#36807;&#31243;&#65292;&#35813;&#36807;&#31243;&#30340;&#24615;&#33021;&#25509;&#36817;&#29978;&#33267;&#36229;&#36807;&#20102;&#25163;&#21160;&#32534;&#20889;&#30340;&#20351;&#29992;&#20869;&#37096;&#20989;&#25968;&#25110;&#27719;&#32534;&#35821;&#35328;&#30340;&#24494;&#20869;&#26680;&#12290;
&lt;/p&gt;
&lt;p&gt;
The optimization of the matrix multiplication (or GEMM) has been a need during the last decades. This operation is considered the flagship of current linear algebra libraries such as BLIS, OpenBLAS, or Intel OneAPI because of its widespread use in a large variety of scientific applications. The GEMM is usually implemented following the GotoBLAS philosophy, which tiles the GEMM operands and uses a series of nested loops for performance improvement. These approaches extract the maximum computational power of the architectures through small pieces of hardware-oriented, high-performance code called micro-kernel. However, this approach forces developers to generate, with a non-negligible effort, a dedicated micro-kernel for each new hardware.  In this work, we present a step-by-step procedure for generating micro-kernels with the Exo compiler that performs close to (or even better than) manually developed microkernels written with intrinsic functions or assembly language. Our solution also 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#24847;&#20041;&#26102;&#30340;&#26465;&#20214;&#12290;&#26368;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#35748;&#20026;&#20256;&#32479;&#30340;&#26426;&#22120;&#29702;&#35299;&#35821;&#35328;&#30340;&#20551;&#35774;&#38656;&#35201;&#20462;&#35746;&#12290;&#36825;&#31687;&#25991;&#31456;&#30528;&#37325;&#24378;&#35843;&#20102;&#26368;&#20808;&#36827;&#30340;LLM&#27169;&#22411;&#19981;&#20165;&#20351;&#29992;&#35821;&#27861;&#65292;&#32780;&#19988;&#20063;&#20351;&#29992;&#35821;&#20041;&#65292;&#24182;&#30830;&#23450;&#23427;&#20204;&#22914;&#20309;&#24314;&#31435;&#35821;&#35328;&#34920;&#36798;&#30340;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.17407</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24847;&#20041;&#21644;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Meaning and understanding in large language models. (arXiv:2310.17407v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17407
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#24847;&#20041;&#26102;&#30340;&#26465;&#20214;&#12290;&#26368;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#35748;&#20026;&#20256;&#32479;&#30340;&#26426;&#22120;&#29702;&#35299;&#35821;&#35328;&#30340;&#20551;&#35774;&#38656;&#35201;&#20462;&#35746;&#12290;&#36825;&#31687;&#25991;&#31456;&#30528;&#37325;&#24378;&#35843;&#20102;&#26368;&#20808;&#36827;&#30340;LLM&#27169;&#22411;&#19981;&#20165;&#20351;&#29992;&#35821;&#27861;&#65292;&#32780;&#19988;&#20063;&#20351;&#29992;&#35821;&#20041;&#65292;&#24182;&#30830;&#23450;&#23427;&#20204;&#22914;&#20309;&#24314;&#31435;&#35821;&#35328;&#34920;&#36798;&#30340;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#33021;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#30340;&#24847;&#20041;&#21527;&#65311;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#26368;&#26032;&#21457;&#23637;&#20351;&#20154;&#20204;&#30456;&#20449;&#20256;&#32479;&#30340;&#20851;&#20110;&#26426;&#22120;&#35821;&#35328;&#29702;&#35299;&#30340;&#21746;&#23398;&#20551;&#35774;&#38656;&#35201;&#20462;&#35746;&#12290;&#26412;&#25991;&#23545;&#26222;&#36941;&#36235;&#21183;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#35780;&#20272;&#65292;&#35748;&#20026;&#26426;&#22120;&#35821;&#35328;&#24615;&#33021;&#20165;&#20165;&#26159;&#35821;&#27861;&#25805;&#20316;&#21644;&#27169;&#25311;&#29702;&#35299;&#65292;&#36825;&#31181;&#29702;&#35299;&#21482;&#26159;&#37096;&#20998;&#30340;&#19988;&#38750;&#24120;&#27973;&#26174;&#65292;&#27809;&#26377;&#36275;&#22815;&#30340;&#19990;&#30028;&#21442;&#32771;&#22522;&#30784;&#12290;&#30446;&#30340;&#26159;&#24378;&#35843;&#23558;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#24402;&#22240;&#20110;&#26368;&#20808;&#36827;&#30340;LLMs&#30340;&#20851;&#38190;&#26465;&#20214;&#65292;&#21487;&#20197;&#21512;&#27861;&#22320;&#35748;&#20026;LLMs&#19981;&#20165;&#20351;&#29992;&#35821;&#27861;&#65292;&#32780;&#19988;&#20351;&#29992;&#35821;&#20041;&#65292;&#23427;&#20204;&#30340;&#29702;&#35299;&#19981;&#26159;&#27169;&#25311;&#32780;&#26159;&#22797;&#21046;&#65292;&#24182;&#30830;&#23450;&#23427;&#20204;&#22914;&#20309;&#24314;&#31435;&#35821;&#35328;&#34920;&#36798;&#30340;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can a machine understand the meanings of natural language? Recent developments in the generative large language models (LLMs) of artificial intelligence have led to the belief that traditional philosophical assumptions about machine understanding of language need to be revised. This article critically evaluates the prevailing tendency to regard machine language performance as mere syntactic manipulation and the simulation of understanding, which is only partial and very shallow, without sufficient referential grounding in the world. The aim is to highlight the conditions crucial to attributing natural language understanding to state-of-the-art LLMs, where it can be legitimately argued that LLMs not only use syntax but also semantics, their understanding not being simulated but duplicated; and determine how they ground the meanings of linguistic expressions.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;ToxicChat&#65292;&#19968;&#20010;&#22522;&#20110;&#23454;&#38469;&#29992;&#25143;&#26597;&#35810;&#26500;&#24314;&#30340;&#26032;&#22411;&#27602;&#24615;&#26816;&#27979;&#22522;&#20934;&#12290;&#35813;&#22522;&#20934;&#25581;&#31034;&#20102;&#24403;&#21069;&#27602;&#24615;&#26816;&#27979;&#27169;&#22411;&#22312;&#23454;&#38469;&#29992;&#25143;-AI&#23545;&#35805;&#20013;&#38754;&#20020;&#30340;&#22256;&#38590;&#65292;&#24378;&#35843;&#20102;&#23454;&#38469;&#23545;&#35805;&#20013;&#23384;&#22312;&#30340;&#29420;&#29305;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.17389</link><description>&lt;p&gt;
ToxicChat: &#25581;&#31034;&#23454;&#38469;&#29992;&#25143;-AI&#23545;&#35805;&#20013;&#30340;&#27602;&#24615;&#26816;&#27979;&#38544;&#34255;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
ToxicChat: Unveiling Hidden Challenges of Toxicity Detection in Real-World User-AI Conversation. (arXiv:2310.17389v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;ToxicChat&#65292;&#19968;&#20010;&#22522;&#20110;&#23454;&#38469;&#29992;&#25143;&#26597;&#35810;&#26500;&#24314;&#30340;&#26032;&#22411;&#27602;&#24615;&#26816;&#27979;&#22522;&#20934;&#12290;&#35813;&#22522;&#20934;&#25581;&#31034;&#20102;&#24403;&#21069;&#27602;&#24615;&#26816;&#27979;&#27169;&#22411;&#22312;&#23454;&#38469;&#29992;&#25143;-AI&#23545;&#35805;&#20013;&#38754;&#20020;&#30340;&#22256;&#38590;&#65292;&#24378;&#35843;&#20102;&#23454;&#38469;&#23545;&#35805;&#20013;&#23384;&#22312;&#30340;&#29420;&#29305;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32842;&#22825;&#26426;&#22120;&#20154;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#22914;&#20170;&#32500;&#25345;&#19968;&#20010;&#38750;&#27602;&#24615;&#30340;&#29992;&#25143;-AI&#20114;&#21160;&#29615;&#22659;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#27602;&#24615;&#26816;&#27979;&#24037;&#20316;&#22823;&#22810;&#22522;&#20110;&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#23548;&#20986;&#30340;&#22522;&#20934;&#65292;&#26410;&#20805;&#20998;&#25506;&#32034;&#23454;&#38469;&#29992;&#25143;-AI&#20114;&#21160;&#20013;&#22266;&#26377;&#30340;&#29420;&#29305;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;ToxicChat&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#24320;&#28304;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#23454;&#38469;&#29992;&#25143;&#26597;&#35810;&#26500;&#24314;&#30340;&#26032;&#22411;&#22522;&#20934;&#12290;&#35813;&#22522;&#20934;&#21253;&#21547;&#20102;&#23545;&#24403;&#21069;&#27602;&#24615;&#26816;&#27979;&#27169;&#22411;&#38590;&#20197;&#35782;&#21035;&#30340;&#20016;&#23500;&#32780;&#24494;&#22937;&#30340;&#29616;&#35937;&#65292;&#30456;&#23545;&#20110;&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#26469;&#35828;&#23384;&#22312;&#26174;&#33879;&#30340;&#39046;&#22495;&#24046;&#24322;&#12290;&#25105;&#20204;&#23545;&#22312;&#29616;&#26377;&#27602;&#24615;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#31995;&#32479;&#35780;&#20272;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#24212;&#29992;&#20110;ToxicChat&#30340;&#36825;&#20010;&#29420;&#29305;&#39046;&#22495;&#26102;&#23384;&#22312;&#32570;&#28857;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#23454;&#38469;&#29992;&#25143;-AI&#23545;&#35805;&#20013;&#27602;&#24615;&#26816;&#27979;&#21487;&#33021;&#34987;&#24573;&#35270;&#30340;&#25361;&#25112;&#12290;&#23558;&#26469;&#65292;ToxicChat&#30340;&#30740;&#31350;&#21487;&#20197;&#25512;&#21160;&#26356;&#22909;&#30340;&#27602;&#24615;&#26816;&#27979;&#26041;&#27861;&#21644;&#24037;&#20855;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite remarkable advances that large language models have achieved in chatbots, maintaining a non-toxic user-AI interactive environment has become increasingly critical nowadays. However, previous efforts in toxicity detection have been mostly based on benchmarks derived from social media content, leaving the unique challenges inherent to real-world user-AI interactions insufficiently explored. In this work, we introduce ToxicChat, a novel benchmark based on real user queries from an open-source chatbot. This benchmark contains the rich, nuanced phenomena that can be tricky for current toxicity detection models to identify, revealing a significant domain difference compared to social media content. Our systematic evaluation of models trained on existing toxicity datasets has shown their shortcomings when applied to this unique domain of ToxicChat. Our work illuminates the potentially overlooked challenges of toxicity detection in real-world user-AI conversations. In the future, Toxic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#29992;&#25143;&#30340;&#33521;&#25991;&#35805;&#35821;&#26144;&#23556;&#20026;&#39046;&#22495;&#29305;&#23450;&#20195;&#30721;&#65292;&#20197;&#25903;&#25345;&#23545;&#35805;&#24335;&#29983;&#25104;&#33258;&#21160;&#39550;&#39542;&#20223;&#30495;&#22330;&#26223;&#65292;&#24182;&#25506;&#32034;&#20102;&#35821;&#35328;&#27169;&#22411;&#25152;&#33021;&#25429;&#25417;&#21040;&#30340;&#19978;&#19979;&#25991;&#25935;&#24863;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17372</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#35805;&#24335;&#29983;&#25104;&#33258;&#21160;&#39550;&#39542;&#20223;&#30495;&#22330;&#26223;
&lt;/p&gt;
&lt;p&gt;
Dialogue-based generation of self-driving simulation scenarios using Large Language Models. (arXiv:2310.17372v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17372
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#29992;&#25143;&#30340;&#33521;&#25991;&#35805;&#35821;&#26144;&#23556;&#20026;&#39046;&#22495;&#29305;&#23450;&#20195;&#30721;&#65292;&#20197;&#25903;&#25345;&#23545;&#35805;&#24335;&#29983;&#25104;&#33258;&#21160;&#39550;&#39542;&#20223;&#30495;&#22330;&#26223;&#65292;&#24182;&#25506;&#32034;&#20102;&#35821;&#35328;&#27169;&#22411;&#25152;&#33021;&#25429;&#25417;&#21040;&#30340;&#19978;&#19979;&#25991;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20223;&#30495;&#26159;&#24320;&#21457;&#21644;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#25511;&#21046;&#22120;&#30340;&#26080;&#20215;&#24037;&#20855;&#12290;&#24403;&#21069;&#30340;&#20223;&#30495;&#26694;&#26550;&#22522;&#20110;&#39640;&#24230;&#19987;&#19994;&#30340;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#65292;&#22240;&#27492;&#33258;&#28982;&#35821;&#35328;&#30028;&#38754;&#23558;&#26497;&#22823;&#22320;&#22686;&#21152;&#21487;&#29992;&#24615;&#12290;&#20294;&#26159;&#65292;&#33521;&#25991;&#31616;&#27905;&#29992;&#35821;&#21644;&#25429;&#25417;&#29992;&#25143;&#24847;&#22270;&#30340;&#21487;&#25191;&#34892;&#20195;&#30721;&#20043;&#38388;&#32463;&#24120;&#23384;&#22312;&#19968;&#23450;&#30340;&#38544;&#21547;&#20551;&#35774;&#24046;&#36317;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#31995;&#32479;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36890;&#36807;&#25903;&#25345;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#20132;&#20114;&#65292;&#29992;&#25143;&#21487;&#20197;&#29992;&#20462;&#27491;&#25110;&#20462;&#25913;&#26469;&#36319;&#36827;&#20043;&#21069;&#30340;&#25351;&#20196;&#65292;&#20197;&#23545;&#36804;&#20170;&#20026;&#27490;&#20174;&#20182;&#20204;&#30340;&#35805;&#35821;&#29983;&#25104;&#30340;&#20223;&#30495;&#20316;&#20986;&#21453;&#24212;&#12290;&#25105;&#20204;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23558;&#29992;&#25143;&#30340;&#33521;&#25991;&#35805;&#35821;&#22312;&#36825;&#31181;&#20114;&#21160;&#20013;&#26144;&#23556;&#21040;&#29305;&#23450;&#39046;&#22495;&#30340;&#20195;&#30721;&#65292;&#22240;&#27492;&#25105;&#20204;&#25506;&#32034;&#20102;LLMs&#33021;&#21542;&#25429;&#25417;&#21040;&#35745;&#31639;&#21457;&#35328;&#32773;&#22312;&#35805;&#35821;&#20013;&#30340;&#39044;&#26399;&#20449;&#24687;&#25152;&#38656;&#30340;&#19978;&#19979;&#25991;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simulation is an invaluable tool for developing and evaluating controllers for self-driving cars. Current simulation frameworks are driven by highly-specialist domain specific languages, and so a natural language interface would greatly enhance usability. But there is often a gap, consisting of tacit assumptions the user is making, between a concise English utterance and the executable code that captures the user's intent. In this paper we describe a system that addresses this issue by supporting an extended multimodal interaction: the user can follow up prior instructions with refinements or revisions, in reaction to the simulations that have been generated from their utterances so far. We use Large Language Models (LLMs) to map the user's English utterances in this interaction into domain-specific code, and so we explore the extent to which LLMs capture the context sensitivity that's necessary for computing the speaker's intended message in discourse.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#30740;&#31350;&#20102;&#25512;&#25991;&#24773;&#32490;&#21160;&#24577;&#21644;&#24515;&#29702;&#20581;&#24247;&#38556;&#30861;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#25512;&#25991;&#24773;&#32490;&#21160;&#24577;&#19982;&#29992;&#25143;&#33258;&#25105;&#25259;&#38706;&#30340;&#35786;&#26029;&#26377;&#20851;&#65292;&#20026;&#24515;&#29702;&#20581;&#24247;&#30340;&#35780;&#20272;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.17369</link><description>&lt;p&gt;
&#35821;&#35328;&#21644;&#24515;&#29702;&#20581;&#24247;&#65306;&#20174;&#25991;&#26412;&#20013;&#27979;&#37327;&#24773;&#32490;&#21160;&#24577;&#20316;&#20026;&#35821;&#35328;&#29983;&#29289;&#31038;&#20250;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language and Mental Health: Measures of Emotion Dynamics from Text as Linguistic Biosocial Markers. (arXiv:2310.17369v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17369
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#30740;&#31350;&#20102;&#25512;&#25991;&#24773;&#32490;&#21160;&#24577;&#21644;&#24515;&#29702;&#20581;&#24247;&#38556;&#30861;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#25512;&#25991;&#24773;&#32490;&#21160;&#24577;&#19982;&#29992;&#25143;&#33258;&#25105;&#25259;&#38706;&#30340;&#35786;&#26029;&#26377;&#20851;&#65292;&#20026;&#24515;&#29702;&#20581;&#24247;&#30340;&#35780;&#20272;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#29702;&#30149;&#29702;&#23398;&#30740;&#31350;&#34920;&#26126;&#65292;&#24773;&#32490;&#38543;&#26102;&#38388;&#30340;&#21464;&#21270;&#27169;&#24335;&#8212;&#8212;&#24773;&#32490;&#21160;&#24577;&#8212;&#8212;&#26159;&#24515;&#29702;&#20581;&#24247;&#30340;&#25351;&#26631;&#12290;&#20256;&#32479;&#19978;&#65292;&#24773;&#32490;&#21464;&#21270;&#30340;&#27169;&#24335;&#26159;&#36890;&#36807;&#24773;&#32490;&#30340;&#33258;&#25105;&#25253;&#21578;&#26469;&#30830;&#23450;&#30340;&#65307;&#28982;&#32780;&#65292;&#24050;&#30693;&#23384;&#22312;&#20934;&#30830;&#24615;&#12289;&#20559;&#35265;&#21644;&#20415;&#21033;&#24615;&#31561;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#36890;&#36807;&#30740;&#31350;&#20010;&#20154;&#26085;&#24120;&#21457;&#35328;&#26469;&#30830;&#23450;&#24773;&#32490;&#21160;&#24577;&#65292;&#35299;&#20915;&#20102;&#35768;&#22810;&#36825;&#20123;&#38382;&#39064;&#65292;&#20294;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#36825;&#20123;&#21457;&#35328;&#24773;&#32490;&#21160;&#24577;&#30340;&#27979;&#37327;&#20540;&#26159;&#21542;&#19982;&#24515;&#29702;&#20581;&#24247;&#35786;&#26029;&#30456;&#20851;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#39318;&#27425;&#30740;&#31350;&#20102;&#25512;&#25991;&#24773;&#32490;&#21160;&#24577;&#19982;&#24515;&#29702;&#20581;&#24247;&#38556;&#30861;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25105;&#20204;&#30740;&#31350;&#30340;&#27599;&#20010;&#24773;&#32490;&#21160;&#24577;&#24230;&#37327;&#20540;&#37117;&#22240;&#29992;&#25143;&#33258;&#25105;&#25259;&#38706;&#30340;&#35786;&#26029;&#32780;&#26377;&#25152;&#21464;&#21270;&#12290;&#20363;&#22914;&#65292;&#24179;&#22343;&#24773;&#32490;&#20215;&#20540;&#36739;&#39640;&#65288;&#21363;&#25991;&#26412;&#36739;&#31215;&#26497;&#65289;&#30340;&#25511;&#21046;&#32452;&#19982;&#24739;&#26377;&#27880;&#24847;&#21147;&#32570;&#38519;&#22810;&#21160;&#38556;&#30861;&#65288;ADHD&#65289;&#12289;&#25233;&#37057;&#30151;&#65288;MDD&#65289;&#21644;&#21019;&#20260;&#21518;&#24212;&#28608;&#38556;&#30861;&#65288;PTSD&#65289;&#30340;&#29992;&#25143;&#30456;&#27604;&#26174;&#33879;&#36739;&#39640;&#12290;&#24773;&#32490;&#20215;&#20540;&#21464;&#24322;&#24615;&#22312;&#25511;&#21046;&#32452;&#20013;&#26174;&#33879;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research in psychopathology has shown that, at an aggregate level, the patterns of emotional change over time -- emotion dynamics -- are indicators of one's mental health. One's patterns of emotion change have traditionally been determined through self-reports of emotions; however, there are known issues with accuracy, bias, and convenience. Recent approaches to determining emotion dynamics from one's everyday utterances, addresses many of these concerns, but it is not yet known whether these measures of utterance emotion dynamics (UED) correlate with mental health diagnoses. Here, for the first time, we study the relationship between tweet emotion dynamics and mental health disorders. We find that each of the UED metrics studied varied by the user's self-disclosed diagnosis. For example: average valence was significantly higher (i.e., more positive text) in the control group compared to users with ADHD, MDD, and PTSD. Valence variability was significantly lower in the control group co
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#39033;&#28041;&#21450;&#20013;&#39184;&#21644;&#33521;&#35821;&#22269;&#23478;&#33756;&#31995;&#20043;&#38388;&#39135;&#35889;&#30340;&#32763;&#35793;&#21644;&#25991;&#21270;&#36866;&#24212;&#30340;&#26032;&#20219;&#21153;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#25968;&#25454;&#38598;&#24182;&#35780;&#20272;&#20102;&#22810;&#31181;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.17353</link><description>&lt;p&gt;
&#39135;&#35889;&#30340;&#25991;&#21270;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Cultural Adaptation of Recipes. (arXiv:2310.17353v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17353
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#39033;&#28041;&#21450;&#20013;&#39184;&#21644;&#33521;&#35821;&#22269;&#23478;&#33756;&#31995;&#20043;&#38388;&#39135;&#35889;&#30340;&#32763;&#35793;&#21644;&#25991;&#21270;&#36866;&#24212;&#30340;&#26032;&#20219;&#21153;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#25968;&#25454;&#38598;&#24182;&#35780;&#20272;&#20102;&#22810;&#31181;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21462;&#24471;&#26174;&#33879;&#36827;&#23637;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#29616;&#22312;&#26377;&#33021;&#21147;&#35299;&#20915;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#23545;&#36328;&#25991;&#21270;&#29615;&#22659;&#26377;&#19968;&#20010;&#32454;&#33268;&#30340;&#29702;&#35299;&#12290;&#19968;&#20010;&#20851;&#38190;&#30340;&#20363;&#23376;&#26159;&#39135;&#35889;&#30340;&#36866;&#24212;&#65292;&#36825;&#36229;&#20986;&#20102;&#31616;&#21333;&#30340;&#32763;&#35793;&#65292;&#36824;&#21253;&#25324;&#23545;&#26576;&#20010;&#29305;&#23450;&#25991;&#21270;&#30340;&#39135;&#26448;&#12289;&#28921;&#39274;&#25216;&#24039;&#21644;&#33203;&#39135;&#20559;&#22909;&#30340;&#25484;&#25569;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#28041;&#21450;&#20013;&#39184;&#21644;&#33521;&#35821;&#22269;&#23478;&#33756;&#31995;&#20043;&#38388;&#39135;&#35889;&#30340;&#32763;&#35793;&#21644;&#25991;&#21270;&#36866;&#24212;&#30340;&#26032;&#20219;&#21153;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#39033;&#35843;&#26597;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;CulturalRecipes&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#33258;&#21160;&#37197;&#23545;&#30340;&#20013;&#25991;&#21644;&#33521;&#25991;&#39135;&#35889;&#26500;&#25104;&#30340;&#29420;&#29305;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#36824;&#36890;&#36807;&#20154;&#24037;&#32534;&#20889;&#21644;&#31934;&#24515;&#31574;&#21010;&#30340;&#27979;&#35797;&#38598;&#36827;&#34892;&#20102;&#20016;&#23500;&#12290;&#22312;&#36825;&#20010;&#22797;&#26434;&#30340;&#36328;&#25991;&#21270;&#39135;&#35889;&#36866;&#24212;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#21508;&#31181;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;GPT-4&#21644;&#20854;&#20182;LLM&#12289;&#20256;&#32479;&#26426;&#22120;&#32763;&#35793;&#21644;&#20449;&#24687;&#26816;&#32034;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#32508;&#21512;&#20998;&#26512;&#21253;&#25324;&#33258;&#21160;&#21270;&#21644;&#20154;&#24037;&#35780;&#20272;&#26041;&#38754;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building upon the considerable advances in Large Language Models (LLMs), we are now equipped to address more sophisticated tasks demanding a nuanced understanding of cross-cultural contexts. A key example is recipe adaptation, which goes beyond simple translation to include a grasp of ingredients, culinary techniques, and dietary preferences specific to a given culture. We introduce a new task involving the translation and cultural adaptation of recipes between Chinese and English-speaking cuisines. To support this investigation, we present CulturalRecipes, a unique dataset comprised of automatically paired recipes written in Mandarin Chinese and English. This dataset is further enriched with a human-written and curated test set. In this intricate task of cross-cultural recipe adaptation, we evaluate the performance of various methods, including GPT-4 and other LLMs, traditional machine translation, and information retrieval techniques. Our comprehensive analysis includes both automati
&lt;/p&gt;</description></item><item><title>ACT-SQL&#26159;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#33258;&#21160;&#29983;&#25104;&#38142;&#24335;&#24605;&#32500;&#30340;&#25991;&#26412;&#21040;SQL&#25216;&#26415;&#65292;&#22312;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#20013;&#35774;&#35745;&#20102;&#31867;&#20284;&#27169;&#24335;&#38142;&#25509;&#30340;&#38142;&#24335;&#24605;&#32500;&#25552;&#31034;&#65292;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#31034;&#20363;&#23454;&#29616;&#20102;&#25104;&#26412;&#33410;&#30465;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;ACT-SQL&#26041;&#27861;&#22312;Spider&#24320;&#21457;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.17342</link><description>&lt;p&gt;
ACT-SQL: &#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#33258;&#21160;&#29983;&#25104;&#38142;&#24335;&#24605;&#32500;&#30340;&#25991;&#26412;&#21040;SQL&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
ACT-SQL: In-Context Learning for Text-to-SQL with Automatically-Generated Chain-of-Thought. (arXiv:2310.17342v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17342
&lt;/p&gt;
&lt;p&gt;
ACT-SQL&#26159;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#33258;&#21160;&#29983;&#25104;&#38142;&#24335;&#24605;&#32500;&#30340;&#25991;&#26412;&#21040;SQL&#25216;&#26415;&#65292;&#22312;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#20013;&#35774;&#35745;&#20102;&#31867;&#20284;&#27169;&#24335;&#38142;&#25509;&#30340;&#38142;&#24335;&#24605;&#32500;&#25552;&#31034;&#65292;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#31034;&#20363;&#23454;&#29616;&#20102;&#25104;&#26412;&#33410;&#30465;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;ACT-SQL&#26041;&#27861;&#22312;Spider&#24320;&#21457;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#21644;&#20219;&#21153;&#20013;&#24050;&#34987;&#35777;&#26126;&#20855;&#26377;&#24456;&#24378;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30740;&#31350;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#20013;&#25552;&#31034;&#35774;&#35745;&#30340;&#38382;&#39064;&#65292;&#24182;&#23581;&#35797;&#25552;&#39640;LLMs&#22312;&#29983;&#25104;SQL&#26597;&#35810;&#26102;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#38500;&#20102;&#20256;&#32479;&#30340;&#23569;&#26679;&#26412;&#19978;&#19979;&#25991;&#23398;&#20064;&#35774;&#32622;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#31867;&#20284;&#20110;&#27169;&#24335;&#38142;&#25509;&#30340;&#26041;&#27861;&#35774;&#35745;&#20102;&#25105;&#20204;&#30340;&#38142;&#24335;&#24605;&#32500;&#65288;CoT&#65289;&#25552;&#31034;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#21517;&#20026;ACT-SQL&#30340;&#26041;&#27861;&#26469;&#33258;&#21160;&#29983;&#25104;&#33258;&#21160;CoT&#31034;&#20363;&#65292;&#22240;&#27492;&#25972;&#20010;&#36807;&#31243;&#19981;&#38656;&#35201;&#25163;&#21160;&#26631;&#35760;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#25104;&#26412;&#33410;&#30465;&#65292;&#22240;&#20026;&#22312;&#29983;&#25104;&#19968;&#20010;SQL&#26597;&#35810;&#26102;&#65292;&#25105;&#20204;&#21482;&#20351;&#29992;LLMs&#30340;API&#35843;&#29992;&#19968;&#27425;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#25193;&#23637;&#21040;&#22810;&#36718;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#30340;&#24615;&#33021;&#21487;&#20197;&#21463;&#30410;&#20110;&#25105;&#20204;&#30340;ACT-SQL&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29616;&#26377;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#20013;&#65292;&#22312;Spider&#24320;&#21457;&#38598;&#19978;&#36798;&#21040;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently Large Language Models (LLMs) have been proven to have strong abilities in various domains and tasks. We study the problem of prompt designing in the text-to-SQL task and attempt to improve the LLMs' reasoning ability when generating SQL queries. Besides the trivial few-shot in-context learning setting, we design our chain-of-thought (CoT) prompt with a similar method to schema linking. We provide a method named ACT-SQL to automatically generate auto-CoT exemplars and thus the whole process doesn't need manual labeling. Our approach is cost-saving since we only use the LLMs' API call once when generating one SQL query. Furthermore, we extend our in-context learning method to the multi-turn text-to-SQL task. The experiment results show that the LLMs' performance can benefit from our ACT-SQL approach. Our approach achieves SOTA performance on the Spider dev set among existing in-context learning approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38463;&#25289;&#20271;&#31934;&#32454;&#21270;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#29616;&#26377;&#30340;&#38463;&#25289;&#20271;&#21629;&#21517;&#23454;&#20307;&#35821;&#26009;&#24211;&#65292;&#23558;&#22320;&#32536;&#25919;&#27835;&#23454;&#20307;&#12289;&#20301;&#32622;&#12289;&#32452;&#32455;&#21644;&#35774;&#26045;&#31561;&#22235;&#31181;&#20027;&#35201;&#23454;&#20307;&#31867;&#22411;&#25193;&#23637;&#20026;31&#20010;&#23376;&#31867;&#22411;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;&#35780;&#27880;&#32773;&#19968;&#33268;&#24615;&#35777;&#26126;&#20102;&#25193;&#23637;&#21518;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.17333</link><description>&lt;p&gt;
&#38463;&#25289;&#20271;&#31934;&#32454;&#21270;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Arabic Fine-Grained Entity Recognition. (arXiv:2310.17333v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38463;&#25289;&#20271;&#31934;&#32454;&#21270;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#29616;&#26377;&#30340;&#38463;&#25289;&#20271;&#21629;&#21517;&#23454;&#20307;&#35821;&#26009;&#24211;&#65292;&#23558;&#22320;&#32536;&#25919;&#27835;&#23454;&#20307;&#12289;&#20301;&#32622;&#12289;&#32452;&#32455;&#21644;&#35774;&#26045;&#31561;&#22235;&#31181;&#20027;&#35201;&#23454;&#20307;&#31867;&#22411;&#25193;&#23637;&#20026;31&#20010;&#23376;&#31867;&#22411;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;&#35780;&#27880;&#32773;&#19968;&#33268;&#24615;&#35777;&#26126;&#20102;&#25193;&#23637;&#21518;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;NER&#31995;&#32479;&#36890;&#24120;&#34987;&#35757;&#32451;&#26469;&#35782;&#21035;&#31895;&#31890;&#24230;&#23454;&#20307;&#65292;&#24182;&#19988;&#23545;&#23558;&#23454;&#20307;&#20998;&#31867;&#20026;&#32454;&#31890;&#24230;&#30340;&#20302;&#23618;&#32423;&#23376;&#31867;&#22411;&#30340;&#20851;&#27880;&#36739;&#23569;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#31934;&#32454;&#21270;&#23454;&#20307;&#25552;&#21319;&#38463;&#25289;&#20271;NER&#12290;&#25105;&#20204;&#36873;&#25321;&#25193;&#23637;Wojood&#65288;&#19968;&#20010;&#24320;&#28304;&#30340;&#23884;&#22871;&#38463;&#25289;&#20271;&#21629;&#21517;&#23454;&#20307;&#35821;&#26009;&#24211;&#65289;&#30340;&#23376;&#31867;&#22411;&#12290;&#23588;&#20854;&#26159;&#65292;Wojood&#20013;&#30340;&#22235;&#31181;&#20027;&#35201;&#23454;&#20307;&#31867;&#22411;&#65292;&#22320;&#32536;&#25919;&#27835;&#23454;&#20307;&#65288;GPE&#65289;&#65292;&#20301;&#32622;&#65288;LOC&#65289;&#65292;&#32452;&#32455;&#65288;ORG&#65289;&#21644;&#35774;&#26045;&#65288;FAC&#65289;&#65292;&#34987;&#25193;&#23637;&#20026;31&#20010;&#23376;&#31867;&#22411;&#12290;&#20026;&#20102;&#20570;&#21040;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#39318;&#20808;&#20462;&#35746;&#20102;Wojood&#23545;GPE&#65292;LOC&#65292;ORG&#21644;FAC&#30340;&#27880;&#37322;&#65292;&#20351;&#20854;&#19982;LDC&#30340;ACE&#25351;&#21335;&#20860;&#23481;&#65292;&#32467;&#26524;&#26377;5614&#22788;&#26356;&#25913;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20154;&#24037;&#27880;&#37322;&#20102;Wojood&#20013;&#25152;&#26377;GPE&#65292;LOC&#65292;ORG&#21644;FAC&#30340;&#25552;&#21450;&#65288;&#32422;44K&#65289;&#65292;&#20351;&#29992;LDC&#30340;ACE&#23376;&#31867;&#22411;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#25193;&#23637;&#29256;&#26412;&#30340;Wojood&#31216;&#20026;WojoodFine&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#27880;&#37322;&#65292;&#25105;&#20204;&#20351;&#29992;Cohen's Kappa&#21644;F1&#24471;&#20998;&#26469;&#27979;&#37327;&#35780;&#27880;&#32773;&#19968;&#33268;&#24615;&#65288;IAA&#65289;&#65292;&#32467;&#26524;&#20998;&#21035;&#20026;0.9861&#21644;0.9889
&lt;/p&gt;
&lt;p&gt;
Traditional NER systems are typically trained to recognize coarse-grained entities, and less attention is given to classifying entities into a hierarchy of fine-grained lower-level subtypes. This article aims to advance Arabic NER with fine-grained entities. We chose to extend Wojood (an open-source Nested Arabic Named Entity Corpus) with subtypes. In particular, four main entity types in Wojood, geopolitical entity (GPE), location (LOC), organization (ORG), and facility (FAC), are extended with 31 subtypes. To do this, we first revised Wojood's annotations of GPE, LOC, ORG, and FAC to be compatible with the LDC's ACE guidelines, which yielded 5, 614 changes. Second, all mentions of GPE, LOC, ORG, and FAC (~44K) in Wojood are manually annotated with the LDC's ACE sub-types. We refer to this extended version of Wojood as WojoodF ine. To evaluate our annotations, we measured the inter-annotator agreement (IAA) using both Cohen's Kappa and F1 score, resulting in 0.9861 and 0.9889, respect
&lt;/p&gt;</description></item><item><title>Nabra&#26159;&#19968;&#20221;&#21253;&#21547;&#21465;&#21033;&#20122;&#38463;&#25289;&#20271;&#26041;&#35328;&#21644;&#24418;&#24577;&#23398;&#27880;&#37322;&#30340;&#35821;&#26009;&#24211;&#65292;&#35206;&#30422;&#20102;&#22810;&#31181;&#21465;&#21033;&#20122;&#26412;&#22303;&#26041;&#35328;&#65292;&#26631;&#27880;&#30340;&#36136;&#37327;&#20248;&#31168;&#65292;&#24320;&#28304;&#21487;&#20844;&#24320;&#33719;&#21462;&#12290;</title><link>http://arxiv.org/abs/2310.17315</link><description>&lt;p&gt;
Nabra: &#20855;&#26377;&#24418;&#24577;&#23398;&#27880;&#37322;&#30340;&#21465;&#21033;&#20122;&#38463;&#25289;&#20271;&#26041;&#35328;
&lt;/p&gt;
&lt;p&gt;
Nabra: Syrian Arabic Dialects with Morphological Annotations. (arXiv:2310.17315v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17315
&lt;/p&gt;
&lt;p&gt;
Nabra&#26159;&#19968;&#20221;&#21253;&#21547;&#21465;&#21033;&#20122;&#38463;&#25289;&#20271;&#26041;&#35328;&#21644;&#24418;&#24577;&#23398;&#27880;&#37322;&#30340;&#35821;&#26009;&#24211;&#65292;&#35206;&#30422;&#20102;&#22810;&#31181;&#21465;&#21033;&#20122;&#26412;&#22303;&#26041;&#35328;&#65292;&#26631;&#27880;&#30340;&#36136;&#37327;&#20248;&#31168;&#65292;&#24320;&#28304;&#21487;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Nabra&#65292;&#36825;&#26159;&#19968;&#20221;&#21253;&#21547;&#21465;&#21033;&#20122;&#38463;&#25289;&#20271;&#26041;&#35328;&#21644;&#24418;&#24577;&#23398;&#27880;&#37322;&#30340;&#35821;&#26009;&#24211;&#12290;&#30001;&#21465;&#21033;&#20122;&#26412;&#22303;&#20154;&#25910;&#38598;&#20102;&#36229;&#36807;6&#21315;&#20010;&#21477;&#23376;&#65292;&#32422;6&#19975;&#20010;&#35789;&#27719;&#65292;&#26469;&#28304;&#21253;&#25324;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#12289;&#30005;&#24433;&#21644;&#36830;&#32493;&#21095;&#30340;&#21095;&#26412;&#12289;&#27468;&#26354;&#27468;&#35789;&#21644;&#24403;&#22320;&#30340;&#35866;&#35821;&#65292;&#29992;&#20110;&#26500;&#24314;Nabra&#12290;Nabra&#28085;&#30422;&#20102;&#21253;&#25324;&#38463;&#21202;&#39047;&#12289;&#22823;&#39532;&#22763;&#38761;&#12289;&#20195;&#23572;&#31062;&#23572;&#12289;&#21704;&#39532;&#12289;&#38669;&#22982;&#26031;&#12289;&#32993;&#20848;&#12289;&#25289;&#22612;&#22522;&#20122;&#12289;&#39532;&#23572;&#19969;&#12289;&#25289;&#21345;&#21644;&#33487;&#38886;&#36798;&#22312;&#20869;&#30340;&#22810;&#31181;&#21465;&#21033;&#20122;&#26412;&#22303;&#26041;&#35328;&#12290;&#20061;&#21517;&#26631;&#27880;&#20154;&#21592;&#23545;&#36825;6&#19975;&#20010;&#26631;&#35760;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#24418;&#24577;&#23398;&#27880;&#37322;&#65292;&#30830;&#20445;&#20102;&#29420;&#29305;&#30340;&#24418;&#24577;&#32032;&#26631;&#27880;&#65292;&#24182;&#36827;&#34892;&#20102;&#26631;&#20934;&#21270;&#30340;&#27880;&#37322;&#12290;&#25353;&#29305;&#24449;&#35745;&#31639;&#30340;F1&#21644;kappa&#19968;&#33268;&#24615;&#24471;&#20998;&#22312;74%&#21040;98%&#20043;&#38388;&#65292;&#26174;&#31034;&#20102;Nabra&#27880;&#37322;&#30340;&#20248;&#36136;&#24615;&#12290;&#25105;&#20204;&#30340;&#35821;&#26009;&#24211;&#26159;&#24320;&#28304;&#30340;&#65292;&#21487;&#20197;&#36890;&#36807;Currasat&#38376;&#25143;&#32593;&#31449;https://sina.birzeit.edu/currasat&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents Nabra, a corpora of Syrian Arabic dialects with morphological annotations. A team of Syrian natives collected more than 6K sentences containing about 60K words from several sources including social media posts, scripts of movies and series, lyrics of songs and local proverbs to build Nabra. Nabra covers several local Syrian dialects including those of Aleppo, Damascus, Deir-ezzur, Hama, Homs, Huran, Latakia, Mardin, Raqqah, and Suwayda. A team of nine annotators annotated the 60K tokens with full morphological annotations across sentence contexts. We trained the annotators to follow methodological annotation guidelines to ensure unique morpheme annotations, and normalized the annotations. F1 and kappa agreement scores ranged between 74% and 98% across features, showing the excellent quality of Nabra annotations. Our corpora are open-source and publicly available as part of the Currasat portal https://sina.birzeit.edu/currasat.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#20154;&#24037;&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#38598;&#25104;&#26550;&#26500;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#21333;&#20010;Transformer&#27169;&#22411;&#65292;&#32780;&#25552;&#20986;&#30340;SciBERT-CNN&#38598;&#25104;&#27169;&#22411;&#22312;ALTA&#20849;&#20139;&#20219;&#21153;2023&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;98.36%&#30340;F1&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2310.17312</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#38598;&#25104;&#26041;&#27861;&#29992;&#20110;&#26816;&#27979;&#20154;&#24037;&#29983;&#25104;&#30340;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
An Ensemble Method Based on the Combination of Transformers with Convolutional Neural Networks to Detect Artificially Generated Text. (arXiv:2310.17312v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17312
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#20154;&#24037;&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#38598;&#25104;&#26550;&#26500;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#21333;&#20010;Transformer&#27169;&#22411;&#65292;&#32780;&#25552;&#20986;&#30340;SciBERT-CNN&#38598;&#25104;&#27169;&#22411;&#22312;ALTA&#20849;&#20139;&#20219;&#21153;2023&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;98.36%&#30340;F1&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#35821;&#35328;&#29983;&#25104;&#24050;&#32463;&#36798;&#21040;&#20102;&#21331;&#36234;&#30340;&#27700;&#24179;&#12290;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20869;&#23481;&#65292;&#22240;&#27492;&#20174;&#20154;&#24037;&#25776;&#20889;&#30340;&#20869;&#23481;&#20013;&#26816;&#27979;&#29983;&#25104;&#25991;&#26412;&#21464;&#25104;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#23613;&#31649;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#25552;&#20379;&#20102;&#35832;&#22810;&#20248;&#21183;&#65292;&#20294;&#26080;&#27861;&#21306;&#20998;&#33258;&#21160;&#29983;&#25104;&#30340;&#25991;&#26412;&#21487;&#33021;&#20250;&#24341;&#21457;&#20851;&#20110;&#30495;&#23454;&#24615;&#30340;&#36947;&#24503;&#39038;&#34385;&#12290;&#22240;&#27492;&#65292;&#35774;&#35745;&#21644;&#24320;&#21457;&#26816;&#27979;&#20154;&#24037;&#20869;&#23481;&#30340;&#26041;&#27861;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#36890;&#36807;&#38598;&#25104;Transformer&#27169;&#22411;&#65288;&#22914;Sci-BERT&#12289;DeBERTa&#21644;XLNet&#65289;&#19982;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#26500;&#24314;&#30340;&#20998;&#31867;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#32771;&#34385;&#30340;&#38598;&#25104;&#26550;&#26500;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#36229;&#36234;&#20102;&#21333;&#20010;Transformer&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#30340;SciBERT-CNN&#38598;&#25104;&#27169;&#22411;&#22312;ALTA&#20849;&#20139;&#20219;&#21153;2023&#25968;&#25454;&#19978;&#24471;&#21040;&#20102;98.36%&#30340;F1&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Thanks to the state-of-the-art Large Language Models (LLMs), language generation has reached outstanding levels. These models are capable of generating high quality content, thus making it a challenging task to detect generated text from human-written content. Despite the advantages provided by Natural Language Generation, the inability to distinguish automatically generated text can raise ethical concerns in terms of authenticity. Consequently, it is important to design and develop methodologies to detect artificial content. In our work, we present some classification models constructed by ensembling transformer models such as Sci-BERT, DeBERTa and XLNet, with Convolutional Neural Networks (CNNs). Our experiments demonstrate that the considered ensemble architectures surpass the performance of the individual transformer models for classification. Furthermore, the proposed SciBERT-CNN ensemble model produced an F1-score of 98.36% on the ALTA shared task 2023 data.
&lt;/p&gt;</description></item><item><title>FormaT5&#26159;&#19968;&#20010;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#26681;&#25454;&#30446;&#26631;&#34920;&#26684;&#21644;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#29983;&#25104;&#25968;&#25454;&#30456;&#20851;&#30340;&#26465;&#20214;&#26684;&#24335;&#35268;&#21017;&#12290;&#20026;&#20102;&#35299;&#20915;&#25551;&#36848;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;FormaT5&#36890;&#36807;&#25918;&#24323;&#30446;&#26631;&#30340;&#26041;&#24335;&#23398;&#20064;&#39044;&#27979;&#21344;&#20301;&#31526;&#12290;</title><link>http://arxiv.org/abs/2310.17306</link><description>&lt;p&gt;
FormaT5: &#20197;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26465;&#20214;&#34920;&#26684;&#26684;&#24335;&#21270;&#30340;&#25277;&#26679;&#21644;&#31034;&#20363;
&lt;/p&gt;
&lt;p&gt;
FormaT5: Abstention and Examples for Conditional Table Formatting with Natural Language. (arXiv:2310.17306v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17306
&lt;/p&gt;
&lt;p&gt;
FormaT5&#26159;&#19968;&#20010;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#26681;&#25454;&#30446;&#26631;&#34920;&#26684;&#21644;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#29983;&#25104;&#25968;&#25454;&#30456;&#20851;&#30340;&#26465;&#20214;&#26684;&#24335;&#35268;&#21017;&#12290;&#20026;&#20102;&#35299;&#20915;&#25551;&#36848;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;FormaT5&#36890;&#36807;&#25918;&#24323;&#30446;&#26631;&#30340;&#26041;&#24335;&#23398;&#20064;&#39044;&#27979;&#21344;&#20301;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#30340;&#26684;&#24335;&#21270;&#26159;&#21487;&#35270;&#21270;&#12289;&#23637;&#31034;&#21644;&#20998;&#26512;&#20013;&#30340;&#37325;&#35201;&#23646;&#24615;&#12290;&#30005;&#23376;&#34920;&#26684;&#36719;&#20214;&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#32534;&#20889;&#25968;&#25454;&#30456;&#20851;&#30340;&#26465;&#20214;&#26684;&#24335;&#35268;&#21017;&#26469;&#33258;&#21160;&#26684;&#24335;&#21270;&#34920;&#26684;&#12290;&#20294;&#23545;&#29992;&#25143;&#26469;&#35828;&#65292;&#32534;&#20889;&#36825;&#26679;&#30340;&#35268;&#21017;&#36890;&#24120;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23427;&#35201;&#27714;&#20182;&#20204;&#29702;&#35299;&#21644;&#23454;&#29616;&#24213;&#23618;&#36923;&#36753;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#27169;&#22411;FormaT5&#65292;&#21487;&#20197;&#26681;&#25454;&#30446;&#26631;&#34920;&#26684;&#21644;&#26399;&#26395;&#30340;&#26684;&#24335;&#36923;&#36753;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#29983;&#25104;&#19968;&#20010;&#26465;&#20214;&#26684;&#24335;&#35268;&#21017;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#29992;&#25143;&#20026;&#36825;&#20123;&#20219;&#21153;&#25552;&#20379;&#30340;&#25551;&#36848;&#36890;&#24120;&#26159;&#19981;&#26126;&#30830;&#25110;&#21547;&#31946;&#30340;&#65292;&#36825;&#20351;&#24471;&#20195;&#30721;&#29983;&#25104;&#31995;&#32479;&#38590;&#20197;&#22312;&#19968;&#27493;&#20013;&#20934;&#30830;&#23398;&#20064;&#21040;&#25152;&#38656;&#30340;&#35268;&#21017;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#35268;&#33539;&#19981;&#36275;&#30340;&#38382;&#39064;&#24182;&#20943;&#23569;&#21442;&#25968;&#38169;&#35823;&#65292;FormaT5&#36890;&#36807;&#25918;&#24323;&#30446;&#26631;&#30340;&#26041;&#24335;&#23398;&#20064;&#39044;&#27979;&#21344;&#20301;&#31526;&#12290;&#36825;&#20123;&#21344;&#20301;&#31526;&#21487;&#20197;&#30001;&#31532;&#20108;&#20010;&#27169;&#22411;&#25110;&#32773;&#24403;&#21487;&#29992;&#30340;&#34892;&#31034;&#20363;&#26102;&#65292;&#30001;&#19968;&#20010;&#22522;&#20110;&#31034;&#20363;&#30340;&#32534;&#31243;&#31995;&#32479;&#22635;&#20805;&#12290;
&lt;/p&gt;
&lt;p&gt;
Formatting is an important property in tables for visualization, presentation, and analysis. Spreadsheet software allows users to automatically format their tables by writing data-dependent conditional formatting (CF) rules. Writing such rules is often challenging for users as it requires them to understand and implement the underlying logic. We present FormaT5, a transformer-based model that can generate a CF rule given the target table and a natural language description of the desired formatting logic. We find that user descriptions for these tasks are often under-specified or ambiguous, making it harder for code generation systems to accurately learn the desired rule in a single step. To tackle this problem of under-specification and minimise argument errors, FormaT5 learns to predict placeholders though an abstention objective. These placeholders can then be filled by a second model or, when examples of rows that should be formatted are available, by a programming-by-example system
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#36924;&#30495;&#21644;&#21160;&#30011;&#24418;&#35937;&#30340;&#20132;&#20114;&#24335;&#23545;&#35805;&#20195;&#29702;&#22312;&#20005;&#32899;&#28216;&#25103;&#20013;&#30340;&#34920;&#29616;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#22522;&#20110;&#35821;&#38899;&#30340;ECAs&#35774;&#35745;&#30340;&#27934;&#23519;&#21644;&#24314;&#35758;&#12290;&#32467;&#26524;&#26174;&#31034;&#36924;&#30495;&#21644;&#21160;&#30011;&#24418;&#35937;&#29256;&#26412;&#37117;&#20855;&#26377;&#24456;&#39640;&#30340;&#21487;&#29992;&#24615;&#65292;&#20294;&#22823;&#22810;&#25968;&#21442;&#19982;&#32773;&#26356;&#21916;&#27426;&#36924;&#30495;&#29256;&#26412;&#12290;</title><link>http://arxiv.org/abs/2310.17300</link><description>&lt;p&gt;
&#27604;&#36739;&#36924;&#30495;&#21644;&#21160;&#30011;&#24418;&#35937;&#30340;&#20132;&#20114;&#24335;&#23545;&#35805;&#20195;&#29702;&#22312;&#20005;&#32899;&#28216;&#25103;&#20013;&#30340;&#29992;&#25143;&#20307;&#39564;&#65306;&#19968;&#39033;&#20851;&#20110;&#29992;&#25143;&#20307;&#39564;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Comparing Photorealistic and Animated Embodied Conversational Agents in Serious Games: An Empirical Study on User Experience. (arXiv:2310.17300v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#36924;&#30495;&#21644;&#21160;&#30011;&#24418;&#35937;&#30340;&#20132;&#20114;&#24335;&#23545;&#35805;&#20195;&#29702;&#22312;&#20005;&#32899;&#28216;&#25103;&#20013;&#30340;&#34920;&#29616;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#22522;&#20110;&#35821;&#38899;&#30340;ECAs&#35774;&#35745;&#30340;&#27934;&#23519;&#21644;&#24314;&#35758;&#12290;&#32467;&#26524;&#26174;&#31034;&#36924;&#30495;&#21644;&#21160;&#30011;&#24418;&#35937;&#29256;&#26412;&#37117;&#20855;&#26377;&#24456;&#39640;&#30340;&#21487;&#29992;&#24615;&#65292;&#20294;&#22823;&#22810;&#25968;&#21442;&#19982;&#32773;&#26356;&#21916;&#27426;&#36924;&#30495;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#20114;&#24335;&#23545;&#35805;&#20195;&#29702;&#65288;ECAs&#65289;&#26159;&#20197;&#20855;&#35937;&#21270;&#35282;&#33394;&#24418;&#24335;&#21576;&#29616;&#30340;&#23545;&#35805;&#29992;&#25143;&#30028;&#38754;&#30340;&#33539;&#20363;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20102;&#20004;&#31181;&#19981;&#21516;&#32423;&#21035;&#30340;&#34920;&#29616;&#36924;&#30495;&#24230;&#65292;&#21363;&#36924;&#30495;&#21644;&#21160;&#30011;&#24418;&#35937;&#12290;&#30740;&#31350;&#26088;&#22312;&#25552;&#20379;&#20851;&#20110;&#22522;&#20110;&#35821;&#38899;&#30340;ECAs&#22312;&#20005;&#32899;&#28216;&#25103;&#29615;&#22659;&#20013;&#30340;&#27934;&#23519;&#21644;&#35774;&#35745;&#24314;&#35758;&#12290;&#30740;&#31350;&#37319;&#29992;&#20102;&#19968;&#20010;&#23436;&#20840;&#32452;&#20869;&#12289;&#20108;&#22240;&#32032;&#35774;&#35745;&#65292;&#20849;&#26377;36&#21517;&#24615;&#21035;&#24179;&#34913;&#30340;&#21442;&#19982;&#32773;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#36924;&#30495;&#21644;&#21160;&#30011;&#24418;&#35937;&#30340;&#29256;&#26412;&#37117;&#34987;&#35748;&#20026;&#20855;&#26377;&#24456;&#39640;&#30340;&#21487;&#29992;&#24615;&#65292;&#20854;&#24179;&#22343;&#24471;&#20998;&#20998;&#21035;&#20026;5.76&#21644;5.71&#12290;&#28982;&#32780;&#65292;69.4&#65285;&#30340;&#21442;&#19982;&#32773;&#34920;&#31034;&#20182;&#20204;&#26356;&#21916;&#27426;&#36924;&#30495;&#29256;&#26412;&#65292;25&#65285;&#30340;&#21442;&#19982;&#32773;&#34920;&#31034;&#20182;&#20204;&#26356;&#21916;&#27426;&#21160;&#30011;&#29256;&#26412;&#65292;&#36824;&#26377;5.6&#65285;&#30340;&#21442;&#19982;&#32773;&#27809;&#26377;&#34920;&#26126;&#20559;&#22909;&#12290;&#36924;&#30495;&#20195;&#29702;&#34987;&#35748;&#20026;&#26356;&#30495;&#23454;&#21644;&#31867;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;
Embodied conversational agents (ECAs) are paradigms of conversational user interfaces in the form of embodied characters. While ECAs offer various manipulable features, this paper focuses on a study conducted to explore two distinct levels of presentation realism. The two agent versions are photorealistic and animated. The study aims to provide insights and design suggestions for speech-enabled ECAs within serious game environments. A within-subjects, two-by-two factorial design was employed for this research with a cohort of 36 participants balanced for gender. The results showed that both the photorealistic and the animated versions were perceived as highly usable, with overall mean scores of 5.76 and 5.71, respectively. However, 69.4 per cent of the participants stated they preferred the photorealistic version, 25 per cent stated they preferred the animated version and 5.6 per cent had no stated preference. The photorealistic agents were perceived as more realistic and human-like, w
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#35328;&#34920;&#31034;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#21516;&#19968;&#27169;&#22411;&#30340;&#19981;&#21516;&#23618;&#32423;&#19978;&#23398;&#20064;&#19981;&#21516;&#32423;&#21035;&#30340;&#25277;&#35937;&#65292;&#24182;&#36890;&#36807;&#38750;&#21442;&#25968;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#23454;&#29616;&#20449;&#24687;&#35770;&#21387;&#32553;&#12290;&#36825;&#31181;&#27169;&#22411;&#20855;&#26377;&#26356;&#39640;&#32423;&#30340;&#25277;&#35937;&#24182;&#19988;&#26356;&#20855;&#35821;&#35328;&#20449;&#24687;&#65292;&#21516;&#26102;&#23545;&#20110;&#25932;&#23545;&#25200;&#21160;&#20855;&#26377;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17284</link><description>&lt;p&gt;
&#20351;&#29992;&#38750;&#21442;&#25968;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#23398;&#20064;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Learning to Abstract with Nonparametric Variational Information Bottleneck. (arXiv:2310.17284v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#35328;&#34920;&#31034;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#21516;&#19968;&#27169;&#22411;&#30340;&#19981;&#21516;&#23618;&#32423;&#19978;&#23398;&#20064;&#19981;&#21516;&#32423;&#21035;&#30340;&#25277;&#35937;&#65292;&#24182;&#36890;&#36807;&#38750;&#21442;&#25968;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#23454;&#29616;&#20449;&#24687;&#35770;&#21387;&#32553;&#12290;&#36825;&#31181;&#27169;&#22411;&#20855;&#26377;&#26356;&#39640;&#32423;&#30340;&#25277;&#35937;&#24182;&#19988;&#26356;&#20855;&#35821;&#35328;&#20449;&#24687;&#65292;&#21516;&#26102;&#23545;&#20110;&#25932;&#23545;&#25200;&#21160;&#20855;&#26377;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23383;&#31526;&#12289;&#23376;&#35789;&#12289;&#35789;&#21644;&#21477;&#23376;&#32423;&#21035;&#19978;&#23398;&#20064;&#34920;&#31034;&#22312;&#29702;&#35299;&#19981;&#21516;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#21644;&#35821;&#35328;&#29616;&#35937;&#26041;&#38754;&#37117;&#26377;&#25152;&#36129;&#29486;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#25991;&#26412;&#23884;&#20837;&#30340;&#25104;&#26412;&#39640;&#26114;&#65292;&#22240;&#20026;&#23427;&#20204;&#26159;&#29305;&#23450;&#20110;&#20998;&#35789;&#30340;&#65292;&#24182;&#19988;&#38656;&#35201;&#35757;&#32451;&#19981;&#21516;&#30340;&#27169;&#22411;&#26469;&#22788;&#29702;&#19981;&#21516;&#30340;&#25277;&#35937;&#32423;&#21035;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#35328;&#34920;&#31034;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#21516;&#19968;&#27169;&#22411;&#30340;&#19981;&#21516;&#23618;&#32423;&#19978;&#23398;&#20064;&#19981;&#21516;&#32423;&#21035;&#30340;&#25277;&#35937;&#12290;&#25105;&#20204;&#22312;&#32534;&#30721;&#22120;&#30340;&#22534;&#21472;Transformer&#33258;&#27880;&#24847;&#23618;&#20013;&#24212;&#29992;&#20102;&#38750;&#21442;&#25968;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#65288;NVIB&#65289;&#65292;&#36890;&#36807;&#35813;&#27169;&#22411;&#40723;&#21169;&#23545;&#34920;&#31034;&#36827;&#34892;&#20449;&#24687;&#35770;&#21387;&#32553;&#12290;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#20869;&#30340;&#23618;&#23545;&#24212;&#30528;&#36234;&#26469;&#36234;&#39640;&#32423;&#30340;&#25277;&#35937;&#32423;&#21035;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#34920;&#31034;&#26356;&#20855;&#35821;&#35328;&#20449;&#24687;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;NVIB&#21387;&#32553;&#23548;&#33268;&#30340;&#27169;&#22411;&#23545;&#20110;&#25932;&#23545;&#25200;&#21160;&#20855;&#26377;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learned representations at the level of characters, sub-words, words and sentences, have each contributed to advances in understanding different NLP tasks and linguistic phenomena. However, learning textual embeddings is costly as they are tokenization specific and require different models to be trained for each level of abstraction. We introduce a novel language representation model which can learn to compress to different levels of abstraction at different layers of the same model. We apply Nonparametric Variational Information Bottleneck (NVIB) to stacked Transformer self-attention layers in the encoder, which encourages an information-theoretic compression of the representations through the model. We find that the layers within the model correspond to increasing levels of abstraction and that their representations are more linguistically informed. Finally, we show that NVIB compression results in a model which is more robust to adversarial perturbations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#36923;&#36753;&#24418;&#24335;&#65288;LF&#65289;&#26469;&#25552;&#39640;&#34920;&#26684;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#20934;&#30830;&#24615;&#65292;&#39318;&#27425;&#23637;&#31034;&#20102;&#29992;&#33258;&#21160;LF&#25913;&#36827;&#31995;&#32479;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;30&#20010;&#30334;&#20998;&#28857;&#65292;&#36824;&#25351;&#20986;&#20102;&#23454;&#29616;&#39640;&#20934;&#30830;&#24615;&#20173;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.17279</link><description>&lt;p&gt;
&#33258;&#21160;&#36923;&#36753;&#24418;&#24335;&#25552;&#39640;&#34920;&#26684;&#21040;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Automatic Logical Forms improve fidelity in Table-to-Text generation. (arXiv:2310.17279v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17279
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#36923;&#36753;&#24418;&#24335;&#65288;LF&#65289;&#26469;&#25552;&#39640;&#34920;&#26684;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#20934;&#30830;&#24615;&#65292;&#39318;&#27425;&#23637;&#31034;&#20102;&#29992;&#33258;&#21160;LF&#25913;&#36827;&#31995;&#32479;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;30&#20010;&#30334;&#20998;&#28857;&#65292;&#36824;&#25351;&#20986;&#20102;&#23454;&#29616;&#39640;&#20934;&#30830;&#24615;&#20173;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#21040;&#25991;&#26412;&#31995;&#32479;&#20174;&#32467;&#26500;&#21270;&#25968;&#25454;&#65288;&#22914;&#34920;&#26684;&#65289;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#38472;&#36848;&#12290;&#34429;&#28982;&#31471;&#21040;&#31471;&#25216;&#26415;&#22312;&#20107;&#23454;&#20934;&#30830;&#24615;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#65292;&#20294;&#20808;&#21069;&#30340;&#30740;&#31350;&#25253;&#21578;&#31216;&#65292;&#22312;&#20351;&#29992;&#25163;&#21160;&#36923;&#36753;&#24418;&#24335;&#65288;LF&#65289;&#34920;&#31034;&#25152;&#36873;&#20869;&#23481;&#21644;&#30446;&#26631;&#25991;&#26412;&#30340;&#35821;&#20041;&#26102;&#65292;&#33719;&#24471;&#20102;&#25552;&#21319;&#12290;&#37492;&#20110;&#25163;&#21160;&#27493;&#39588;&#65292;&#19981;&#28165;&#26970;&#33258;&#21160;LF&#26159;&#21542;&#26377;&#25928;&#65292;&#25110;&#32773;&#25913;&#36827;&#26469;&#33258;&#20869;&#23481;&#36873;&#25321;&#26412;&#36523;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;TlT&#65292;&#32473;&#23450;&#19968;&#20010;&#34920;&#26684;&#21644;&#20869;&#23481;&#36873;&#25321;&#65292;&#39318;&#20808;&#29983;&#25104;LF&#65292;&#28982;&#21518;&#29983;&#25104;&#25991;&#26412;&#38472;&#36848;&#12290;&#25105;&#20204;&#39318;&#27425;&#23637;&#31034;&#20102;&#33258;&#21160;LF&#25552;&#39640;&#36136;&#37327;&#30340;&#25928;&#26524;&#65292;&#19982;&#19981;&#20351;&#29992;LF&#30340;&#31867;&#20284;&#31995;&#32479;&#30456;&#27604;&#65292;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;30&#20010;&#30334;&#20998;&#28857;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#23545;&#20110;&#39640;&#20934;&#30830;&#24615;&#36824;&#38754;&#20020;&#30528;&#19968;&#20123;&#25361;&#25112;&#65292;&#33258;&#21160;&#20869;&#23481;&#36873;&#25321;&#26159;&#39318;&#35201;&#38382;&#39064;&#65292;&#20854;&#27425;&#26159;&#26356;&#22909;&#30340;&#36923;&#36753;&#21040;&#25991;&#26412;&#29983;&#25104;&#65292;&#20197;&#21450;&#36739;&#23569;&#31243;&#24230;&#30340;&#26356;&#22909;&#30340;&#34920;&#26684;&#21040;&#36923;&#36753;&#35299;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Table-to-text systems generate natural language statements from structured data like tables. While end-to-end techniques suffer from low factual correctness (fidelity), a previous study reported gains when using manual logical forms (LF) that represent the selected content and the semantics of the target text. Given the manual step, it was not clear whether automatic LFs would be effective, or whether the improvement came from content selection alone. We present TlT which, given a table and a selection of the content, first produces LFs and then the textual statement. We show for the first time that automatic LFs improve quality, with an increase in fidelity of 30 points over a comparable system not using LFs. Our experiments allow to quantify the remaining challenges for high factual correctness, with automatic selection of content coming first, followed by better Logic-to-Text generation and, to a lesser extent, better Table-to-Logic parsing.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#36755;&#20837;&#21333;&#35789;&#23383;&#31526;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#21363;&#20351;&#21482;&#20351;&#29992;&#21333;&#20010;&#23383;&#31526;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#27169;&#22411;&#22312;&#26631;&#20934;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#20445;&#25345;&#36739;&#39640;&#12290;</title><link>http://arxiv.org/abs/2310.17271</link><description>&lt;p&gt;
&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#20013;&#36755;&#20837;&#21333;&#35789;&#23383;&#31526;&#30340;&#20316;&#29992;&#65306;&#20449;&#24687;&#20002;&#22833;&#22914;&#20309;&#24433;&#21709;&#24615;&#33021;&#65311;
&lt;/p&gt;
&lt;p&gt;
Understanding the Role of Input Token Characters in Language Models: How Does Information Loss Affect Performance?. (arXiv:2310.17271v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17271
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#36755;&#20837;&#21333;&#35789;&#23383;&#31526;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#21363;&#20351;&#21482;&#20351;&#29992;&#21333;&#20010;&#23383;&#31526;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#27169;&#22411;&#22312;&#26631;&#20934;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#20445;&#25345;&#36739;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22914;&#20309;&#23398;&#20064;&#35821;&#35328;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#19968;&#20010;&#24320;&#25918;&#30340;&#25361;&#25112;&#12290;&#26089;&#26399;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#30830;&#23450;&#23427;&#20204;&#26159;&#21542;&#25429;&#25417;&#20102;&#35821;&#20041;&#21644;&#21477;&#27861;&#20449;&#24687;&#65292;&#20197;&#21450;&#25968;&#25454;&#25110;&#39044;&#35757;&#32451;&#30446;&#26631;&#22914;&#20309;&#24433;&#21709;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#20043;&#21069;&#27809;&#26377;&#30740;&#31350;&#19987;&#38376;&#25506;&#35752;&#36755;&#20837;&#21333;&#35789;&#23383;&#31526;&#30340;&#20449;&#24687;&#20002;&#22833;&#23545;PLMs&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#21333;&#20010;&#21333;&#35789;&#23383;&#31526;&#30340;&#23567;&#23376;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#21363;&#20351;&#22312;&#26497;&#31471;&#24773;&#20917;&#19979;&#65292;&#21363;&#27599;&#20010;&#21333;&#35789;&#21482;&#20351;&#29992;&#19968;&#20010;&#23383;&#31526;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#19982;&#23436;&#25972;&#21333;&#35789;&#27169;&#22411;&#30456;&#27604;&#22312;&#26631;&#20934;NLU&#22522;&#20934;&#27979;&#35797;&#21644;&#25506;&#27979;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#20445;&#25345;&#36739;&#39640;&#12290;&#20363;&#22914;&#65292;&#20165;&#20351;&#29992;&#21333;&#35789;&#30340;&#39318;&#20010;&#23383;&#31526;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;SuperGLUE&#20013;&#20445;&#25345;&#20102;&#32422;90&#65285;&#21644;77&#65285;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding how and what pre-trained language models (PLMs) learn about language is an open challenge in natural language processing. Previous work has focused on identifying whether they capture semantic and syntactic information, and how the data or the pre-training objective affects their performance. However, to the best of our knowledge, no previous work has specifically examined how information loss in input token characters affects the performance of PLMs. In this study, we address this gap by pre-training language models using small subsets of characters from individual tokens. Surprisingly, we find that pre-training even under extreme settings, i.e. using only one character of each token, the performance retention in standard NLU benchmarks and probing tasks compared to full-token models is high. For instance, a model pre-trained only on single first characters from tokens achieves performance retention of approximately $90$\% and $77$\% of the full-token model in SuperGLUE 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32852;&#21512;&#23454;&#20307;&#21644;&#20851;&#31995;&#25277;&#21462;&#26041;&#27861;&#65292;&#20351;&#29992;span&#21098;&#26525;&#26426;&#21046;&#20943;&#36731;&#35823;&#24046;&#20256;&#25773;&#38382;&#39064;&#65292;&#36890;&#36807;&#26500;&#24314;&#36229;&#22270;&#36827;&#34892;&#39640;&#38454;&#24314;&#27169;&#65292;&#23454;&#29616;&#22810;&#20010;&#23454;&#20307;&#21644;&#20851;&#31995;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;</title><link>http://arxiv.org/abs/2310.17238</link><description>&lt;p&gt;
&#36890;&#36807;span&#21098;&#26525;&#21644;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#32852;&#21512;&#23454;&#20307;&#21644;&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
Joint Entity and Relation Extraction with Span Pruning and Hypergraph Neural Networks. (arXiv:2310.17238v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32852;&#21512;&#23454;&#20307;&#21644;&#20851;&#31995;&#25277;&#21462;&#26041;&#27861;&#65292;&#20351;&#29992;span&#21098;&#26525;&#26426;&#21046;&#20943;&#36731;&#35823;&#24046;&#20256;&#25773;&#38382;&#39064;&#65292;&#36890;&#36807;&#26500;&#24314;&#36229;&#22270;&#36827;&#34892;&#39640;&#38454;&#24314;&#27169;&#65292;&#23454;&#29616;&#22810;&#20010;&#23454;&#20307;&#21644;&#20851;&#31995;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#21644;&#20851;&#31995;&#25277;&#21462;&#65288;ERE&#65289;&#26159;&#20449;&#24687;&#25552;&#21462;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#26368;&#36817;&#22522;&#20110;&#26631;&#35760;&#30340;&#27969;&#27700;&#32447;&#27169;&#22411;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#35823;&#24046;&#20256;&#25773;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;ERE&#27169;&#22411;&#22312;&#22810;&#20010;&#23454;&#20307;&#21644;&#20851;&#31995;&#20043;&#38388;&#19981;&#32771;&#34385;&#39640;&#38454;&#20132;&#20114;&#65292;&#32780;&#39640;&#38454;&#24314;&#27169;&#21487;&#33021;&#20250;&#26377;&#30410;&#22788;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HGNN&#65289;&#29992;&#20110;ERE&#65292;&#23427;&#26159;&#24314;&#31435;&#22312;PL-marker&#65288;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#26631;&#35760;&#30340;&#27969;&#27700;&#32447;&#27169;&#22411;&#65289;&#20043;&#19978;&#30340;&#12290;&#20026;&#20102;&#20943;&#36731;&#35823;&#24046;&#20256;&#25773;&#65292;&#25105;&#20204;&#20351;&#29992;&#39640;&#21484;&#22238;&#21098;&#26525;&#22120;&#26426;&#21046;&#23558;&#23454;&#20307;&#30340;&#35782;&#21035;&#21644;&#26631;&#27880;&#36127;&#25285;&#20174;NER&#27169;&#22359;&#36716;&#31227;&#21040;&#25105;&#20204;&#27169;&#22411;&#30340;&#32852;&#21512;&#27169;&#22359;&#12290;&#23545;&#20110;&#39640;&#38454;&#24314;&#27169;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#36229;&#22270;&#65292;&#20854;&#20013;&#33410;&#28857;&#26159;&#23454;&#20307;&#65288;&#30001;span&#21098;&#26525;&#22120;&#25552;&#20379;&#65289;&#65292;&#20197;&#21450;&#20854;&#20851;&#31995;&#65292;&#24182;&#19988;&#36229;&#36793;&#32534;&#30721;&#20102;&#20004;&#20010;&#19981;&#21516;&#20851;&#31995;&#20043;&#38388;&#25110;&#20851;&#31995;&#19982;&#20854;&#30456;&#20851;&#30340;&#20027;&#20307;&#21644;&#23486;&#35821;&#23454;&#20307;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#25509;&#30528;&#25105;&#20204;&#36816;&#34892;...
&lt;/p&gt;
&lt;p&gt;
Entity and Relation Extraction (ERE) is an important task in information extraction. Recent marker-based pipeline models achieve state-of-the-art performance, but still suffer from the error propagation issue. Also, most of current ERE models do not take into account higher-order interactions between multiple entities and relations, while higher-order modeling could be beneficial.In this work, we propose HyperGraph neural network for ERE ($\hgnn{}$), which is built upon the PL-marker (a state-of-the-art marker-based pipleline model). To alleviate error propagation,we use a high-recall pruner mechanism to transfer the burden of entity identification and labeling from the NER module to the joint module of our model. For higher-order modeling, we build a hypergraph, where nodes are entities (provided by the span pruner) and relations thereof, and hyperedges encode interactions between two different relations or between a relation and its associated subject and object entities. We then run
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EMMA-X&#30340;&#31867;EM&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#35821;&#35328;&#38750;&#24182;&#34892;&#25968;&#25454;&#36827;&#34892;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#30340;&#36890;&#29992;&#34920;&#31034;&#12290;&#35813;&#31639;&#27861;&#22312;&#19968;&#20010;EM&#26694;&#26550;&#20869;&#32479;&#19968;&#20102;&#36328;&#35821;&#35328;&#34920;&#31034;&#23398;&#20064;&#20219;&#21153;&#21644;&#39069;&#22806;&#30340;&#35821;&#20041;&#20851;&#31995;&#39044;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#20114;&#30456;&#30417;&#30563;&#30452;&#21040;&#25910;&#25947;&#30340;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2310.17233</link><description>&lt;p&gt;
EMMA-X:&#19968;&#31181;&#29992;&#20110;&#36328;&#35821;&#35328;&#34920;&#31034;&#23398;&#20064;&#30340;&#31867;EM&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
EMMA-X: An EM-like Multilingual Pre-training Algorithm for Cross-lingual Representation Learning. (arXiv:2310.17233v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EMMA-X&#30340;&#31867;EM&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#35821;&#35328;&#38750;&#24182;&#34892;&#25968;&#25454;&#36827;&#34892;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#30340;&#36890;&#29992;&#34920;&#31034;&#12290;&#35813;&#31639;&#27861;&#22312;&#19968;&#20010;EM&#26694;&#26550;&#20869;&#32479;&#19968;&#20102;&#36328;&#35821;&#35328;&#34920;&#31034;&#23398;&#20064;&#20219;&#21153;&#21644;&#39069;&#22806;&#30340;&#35821;&#20041;&#20851;&#31995;&#39044;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#20114;&#30456;&#30417;&#30563;&#30452;&#21040;&#25910;&#25947;&#30340;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29702;&#35299;&#22797;&#26434;&#21644;&#29305;&#23450;&#20110;&#25991;&#21270;&#30340;&#21477;&#23376;&#30340;&#21547;&#20041;&#26102;&#65292;&#34920;&#36798;&#25152;&#26377;&#35821;&#35328;&#20849;&#21516;&#30340;&#36890;&#29992;&#35821;&#20041;&#26159;&#26377;&#24110;&#21161;&#30340;&#12290;&#26412;&#25991;&#30340;&#30740;&#31350;&#20027;&#39064;&#26159;&#20351;&#29992;&#28023;&#37327;&#24182;&#34892;&#35821;&#26009;&#24211;&#23398;&#20064;&#36328;&#35821;&#35328;&#30340;&#36890;&#29992;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24182;&#34892;&#25968;&#25454;&#30340;&#31232;&#30095;&#24615;&#21644;&#21294;&#20047;&#24615;&#65292;&#23545;&#20110;&#20219;&#20309;&#20004;&#31181;&#35821;&#35328;&#26469;&#35828;&#65292;&#23398;&#20064;&#30495;&#27491;&#30340;&#8220;&#26222;&#36941;&#24615;&#8221;&#20173;&#28982;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EMMA-X&#30340;&#31867;EM&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#31639;&#27861;&#65292;&#21033;&#29992;&#22810;&#35821;&#35328;&#38750;&#24182;&#34892;&#25968;&#25454;&#26469;&#23398;&#20064;&#36328;&#35821;&#35328;&#30340;&#8220;&#26222;&#36941;&#24615;&#8221;&#12290;EMMA-X&#22312;EM&#26694;&#26550;&#20869;&#23558;&#36328;&#35821;&#35328;&#34920;&#31034;&#23398;&#20064;&#20219;&#21153;&#21644;&#39069;&#22806;&#30340;&#35821;&#20041;&#20851;&#31995;&#39044;&#27979;&#20219;&#21153;&#32479;&#19968;&#36215;&#26469;&#12290;&#39069;&#22806;&#30340;&#35821;&#20041;&#20998;&#31867;&#22120;&#21644;&#36328;&#35821;&#35328;&#21477;&#23376;&#32534;&#30721;&#22120;&#37117;&#36817;&#20284;&#20102;&#20004;&#20010;&#21477;&#23376;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#24182;&#30456;&#20114;&#30417;&#30563;&#30452;&#21040;&#25910;&#25947;&#12290;&#20026;&#20102;&#35780;&#20272;EMMA-X&#65292;&#25105;&#20204;&#22312;&#26032;&#24341;&#20837;&#30340;XRETE&#22522;&#20934;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Expressing universal semantics common to all languages is helpful in understanding the meanings of complex and culture-specific sentences. The research theme underlying this scenario focuses on learning universal representations across languages with the usage of massive parallel corpora. However, due to the sparsity and scarcity of parallel data, there is still a big challenge in learning authentic ``universals'' for any two languages. In this paper, we propose EMMA-X: an EM-like Multilingual pre-training Algorithm, to learn (X)Cross-lingual universals with the aid of excessive multilingual non-parallel data. EMMA-X unifies the cross-lingual representation learning task and an extra semantic relation prediction task within an EM framework. Both the extra semantic classifier and the cross-lingual sentence encoder approximate the semantic relation of two sentences, and supervise each other until convergence. To evaluate EMMA-X, we conduct experiments on XRETE, a newly introduced benchma
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;codebook&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#30340;&#36830;&#32493;&#29305;&#24449;&#37327;&#21270;&#20026;&#31163;&#25955;&#21521;&#37327;&#30721;&#30340;&#24635;&#21644;&#26469;&#23454;&#29616;&#31232;&#30095;&#21644;&#31163;&#25955;&#30340;&#38544;&#34255;&#29366;&#24577;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#36825;&#31181;&#26497;&#31471;&#29942;&#39048;&#26465;&#20214;&#19979;&#36816;&#34892;&#26102;&#24615;&#33021;&#19979;&#38477;&#36866;&#24230;&#65292;&#21516;&#26102;&#36825;&#31181;&#26041;&#27861;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#30452;&#35266;&#30340;&#31070;&#32463;&#32593;&#32476;&#34892;&#20026;&#25511;&#21046;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2310.17230</link><description>&lt;p&gt;
Codebook&#29305;&#24449;&#65306;&#31070;&#32463;&#32593;&#32476;&#30340;&#31232;&#30095;&#21644;&#31163;&#25955;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Codebook Features: Sparse and Discrete Interpretability for Neural Networks. (arXiv:2310.17230v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;codebook&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#30340;&#36830;&#32493;&#29305;&#24449;&#37327;&#21270;&#20026;&#31163;&#25955;&#21521;&#37327;&#30721;&#30340;&#24635;&#21644;&#26469;&#23454;&#29616;&#31232;&#30095;&#21644;&#31163;&#25955;&#30340;&#38544;&#34255;&#29366;&#24577;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#36825;&#31181;&#26497;&#31471;&#29942;&#39048;&#26465;&#20214;&#19979;&#36816;&#34892;&#26102;&#24615;&#33021;&#19979;&#38477;&#36866;&#24230;&#65292;&#21516;&#26102;&#36825;&#31181;&#26041;&#27861;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#30452;&#35266;&#30340;&#31070;&#32463;&#32593;&#32476;&#34892;&#20026;&#25511;&#21046;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#30001;&#20110;&#23427;&#20204;&#30340;&#38544;&#34255;&#29366;&#24577;&#26159;&#23494;&#38598;&#21644;&#36830;&#32493;&#30340;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#23558;&#36830;&#32493;&#29305;&#24449;&#37327;&#21270;&#20026;&#25105;&#20204;&#31216;&#20043;&#20026;codebook&#29305;&#24449;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#20854;&#20855;&#26377;&#31232;&#30095;&#12289;&#31163;&#25955;&#19988;&#26356;&#26131;&#35299;&#37322;&#30340;&#38544;&#34255;&#29366;&#24577;&#12290;&#36890;&#36807;&#22312;&#27599;&#23618;&#24341;&#20837;&#21521;&#37327;&#37327;&#21270;&#29942;&#39048;&#26469;&#24494;&#35843;&#31070;&#32463;&#32593;&#32476;&#65292;&#20135;&#29983;&#30340;codebook&#29305;&#24449;&#30001;&#20174;&#26356;&#22823;&#30340;codebook&#20013;&#36873;&#25321;&#30340;&#23569;&#37327;&#31163;&#25955;&#21521;&#37327;&#30721;&#30340;&#24635;&#21644;&#32452;&#25104;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#22312;&#36825;&#31181;&#26497;&#31471;&#29942;&#39048;&#19979;&#36816;&#34892;&#65292;&#24615;&#33021;&#21482;&#26377;&#36866;&#24230;&#30340;&#19979;&#38477;&#12290;&#36825;&#31181;&#31232;&#30095;&#12289;&#31163;&#25955;&#30340;&#29942;&#39048;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#30452;&#35266;&#30340;&#25511;&#21046;&#31070;&#32463;&#32593;&#32476;&#34892;&#20026;&#30340;&#26041;&#27861;&#65306;&#39318;&#20808;&#65292;&#25214;&#21040;&#22312;&#25152;&#38656;&#34892;&#20026;&#20986;&#29616;&#26102;&#28608;&#27963;&#30340;&#30721;&#65292;&#28982;&#21518;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#28608;&#27963;&#30456;&#21516;&#30340;&#30721;&#20197;&#24341;&#21457;&#35813;&#34892;&#20026;&#12290;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;codebook Transformers&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding neural networks is challenging in part because of the dense, continuous nature of their hidden states. We explore whether we can train neural networks to have hidden states that are sparse, discrete, and more interpretable by quantizing their continuous features into what we call codebook features. Codebook features are produced by finetuning neural networks with vector quantization bottlenecks at each layer, producing a network whose hidden features are the sum of a small number of discrete vector codes chosen from a larger codebook. Surprisingly, we find that neural networks can operate under this extreme bottleneck with only modest degradation in performance. This sparse, discrete bottleneck also provides an intuitive way of controlling neural network behavior: first, find codes that activate when the desired behavior is present, then activate those same codes during generation to elicit that behavior. We validate our approach by training codebook Transformers on sever
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#24212;&#29992;&#21644;&#25913;&#36827;&#30446;&#26631;&#30456;&#20284;&#24230;&#35843;&#25972;&#65288;TST&#65289;&#30340;&#19981;&#21516;&#26041;&#27861;&#65292;&#21253;&#25324;&#20351;&#29992;&#26356;&#22823;&#30340;&#27169;&#22411;&#23884;&#20837;&#12289;&#35757;&#32451;&#19968;&#20010;&#23567;&#27169;&#22411;&#36716;&#25442;&#23884;&#20837;&#20197;&#21305;&#37197;&#20195;&#30721;&#30456;&#20284;&#24230;&#65292;&#24182;&#20171;&#32461;&#20102;&#39640;&#25928;&#36873;&#25321;&#35757;&#32451;&#26679;&#20363;&#21644;&#22522;&#20110;&#25490;&#21517;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.17228</link><description>&lt;p&gt;
TST$^\mathrm{R}$: &#30446;&#26631;&#30456;&#20284;&#24230;&#35843;&#25972;&#36935;&#35265;&#29616;&#23454;&#19990;&#30028;
&lt;/p&gt;
&lt;p&gt;
TST$^\mathrm{R}$: Target Similarity Tuning Meets the Real World. (arXiv:2310.17228v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17228
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#24212;&#29992;&#21644;&#25913;&#36827;&#30446;&#26631;&#30456;&#20284;&#24230;&#35843;&#25972;&#65288;TST&#65289;&#30340;&#19981;&#21516;&#26041;&#27861;&#65292;&#21253;&#25324;&#20351;&#29992;&#26356;&#22823;&#30340;&#27169;&#22411;&#23884;&#20837;&#12289;&#35757;&#32451;&#19968;&#20010;&#23567;&#27169;&#22411;&#36716;&#25442;&#23884;&#20837;&#20197;&#21305;&#37197;&#20195;&#30721;&#30456;&#20284;&#24230;&#65292;&#24182;&#20171;&#32461;&#20102;&#39640;&#25928;&#36873;&#25321;&#35757;&#32451;&#26679;&#20363;&#21644;&#22522;&#20110;&#25490;&#21517;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#30456;&#20284;&#24230;&#35843;&#25972;&#65288;TST&#65289;&#26159;&#19968;&#31181;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#65288;NL&#65289;&#21040;&#20195;&#30721;&#29983;&#25104;&#20013;&#36873;&#25321;&#30456;&#20851;&#20363;&#23376;&#20197;&#25552;&#21319;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#20854;&#30446;&#26631;&#26159;&#20351;&#24471;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#36866;&#24212;&#20004;&#20010;NL&#36755;&#20837;&#30340;&#30456;&#20284;&#24230;&#19982;&#20854;&#30456;&#20851;&#20195;&#30721;&#36755;&#20986;&#30340;&#30456;&#20284;&#24230;&#21305;&#37197;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#24212;&#29992;&#21644;&#25913;&#36827;TST&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#20351;&#29992;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#21477;&#23376;&#36716;&#25442;&#22120;&#26367;&#25442;&#20026;&#26356;&#22823;&#27169;&#22411;&#30340;&#23884;&#20837;&#65292;&#20174;&#32780;&#38477;&#20302;&#23545;&#35821;&#35328;&#20998;&#24067;&#30340;&#25935;&#24863;&#24615;&#65292;&#22686;&#21152;&#20102;&#21512;&#25104;&#31034;&#20363;&#30340;&#28789;&#27963;&#24615;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#23567;&#27169;&#22411;&#23558;&#36825;&#20123;&#23884;&#20837;&#36716;&#25442;&#21040;&#19968;&#20010;&#31354;&#38388;&#20013;&#65292;&#20854;&#20013;&#23884;&#20837;&#30456;&#20284;&#24230;&#21305;&#37197;&#20195;&#30721;&#30456;&#20284;&#24230;&#65292;&#20351;&#24471;&#27169;&#22411;&#20445;&#25345;&#40657;&#31665;&#29366;&#24577;&#65292;&#24182;&#22312;&#25512;&#26029;&#26102;&#21482;&#38656;&#36827;&#34892;&#23569;&#37327;&#30697;&#38453;&#20056;&#27861;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22914;&#20309;&#39640;&#25928;&#22320;&#36873;&#25321;&#36739;&#23569;&#25968;&#37327;&#30340;&#35757;&#32451;&#26679;&#20363;&#26469;&#35757;&#32451;TST&#27169;&#22411;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#25490;&#21517;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Target similarity tuning (TST) is a method of selecting relevant examples in natural language (NL) to code generation through large language models (LLMs) to improve performance. Its goal is to adapt a sentence embedding model to have the similarity between two NL inputs match the similarity between their associated code outputs. In this paper, we propose different methods to apply and improve TST in the real world. First, we replace the sentence transformer with embeddings from a larger model, which reduces sensitivity to the language distribution and thus provides more flexibility in synthetic generation of examples, and we train a tiny model that transforms these embeddings to a space where embedding similarity matches code similarity, which allows the model to remain a black box and only requires a few matrix multiplications at inference time. Second, we how to efficiently select a smaller number of training examples to train the TST model. Third, we introduce a ranking-based evalu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20984;&#20989;&#25968;&#30340;&#35757;&#32451;&#30446;&#26631;&#31867;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#38381;&#21512;&#22411;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#33021;&#22815;&#20351;&#24471;&#27169;&#22411;&#29983;&#25104;&#26356;&#21152;&#21512;&#36866;&#30340;&#21709;&#24212;&#12290;</title><link>http://arxiv.org/abs/2310.17217</link><description>&lt;p&gt;
&#36229;&#36234;MLE: &#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#30340;&#20984;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Beyond MLE: Convex Learning for Text Generation. (arXiv:2310.17217v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20984;&#20989;&#25968;&#30340;&#35757;&#32451;&#30446;&#26631;&#31867;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#38381;&#21512;&#22411;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#33021;&#22815;&#20351;&#24471;&#27169;&#22411;&#29983;&#25104;&#26356;&#21152;&#21512;&#36866;&#30340;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;MLE&#65289;&#26159;&#19968;&#31181;&#32479;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#26368;&#33021;&#35299;&#37322;&#35266;&#27979;&#25968;&#25454;&#30340;&#27010;&#29575;&#20998;&#24067;&#21442;&#25968;&#12290;&#22312;&#25991;&#26412;&#29983;&#25104;&#30340;&#32972;&#26223;&#19979;&#65292;MLE&#32463;&#24120;&#29992;&#20110;&#35757;&#32451;&#29983;&#25104;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#29983;&#25104;&#26032;&#30340;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;MLE&#24182;&#19981;&#24635;&#26159;&#24517;&#35201;&#19988;&#26368;&#20248;&#30340;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#38381;&#21512;&#22411;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#65292;&#22914;&#26426;&#22120;&#32763;&#35793;&#12290;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#65292;&#27169;&#22411;&#30340;&#30446;&#26631;&#26159;&#29983;&#25104;&#26368;&#21512;&#36866;&#30340;&#21709;&#24212;&#65292;&#36825;&#24182;&#19981;&#19968;&#23450;&#38656;&#35201;&#20351;&#29992;MLE&#26469;&#20272;&#35745;&#25972;&#20010;&#25968;&#25454;&#20998;&#24067;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20984;&#20989;&#25968;&#30340;&#26032;&#22411;&#35757;&#32451;&#30446;&#26631;&#31867;&#65292;&#20351;&#24471;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#33021;&#22815;&#38598;&#20013;&#20110;&#39640;&#27010;&#29575;&#30340;&#36755;&#20986;&#65292;&#32780;&#19981;&#38656;&#35201;&#20272;&#35745;&#25972;&#20010;&#25968;&#25454;&#20998;&#24067;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#23558;&#20984;&#20989;&#25968;&#24212;&#29992;&#20110;&#25439;&#22833;&#20989;&#25968;&#26102;&#30340;&#26368;&#20248;&#39044;&#27979;&#20998;&#24067;&#30340;&#29702;&#35770;&#24615;&#36136;&#65292;&#35777;&#26126;&#20102;&#20984;&#20989;&#25968;&#21487;&#20197;&#20351;&#24471;&#26368;&#20248;&#39044;&#27979;&#20998;&#24067;&#21464;&#24471;&#26356;&#21152;&#38160;&#21033;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maximum likelihood estimation (MLE) is a statistical method used to estimate the parameters of a probability distribution that best explain the observed data. In the context of text generation, MLE is often used to train generative language models, which can then be used to generate new text. However, we argue that MLE is not always necessary and optimal, especially for closed-ended text generation tasks like machine translation. In these tasks, the goal of model is to generate the most appropriate response, which does not necessarily require it to estimate the entire data distribution with MLE. To this end, we propose a novel class of training objectives based on convex functions, which enables text generation models to focus on highly probable outputs without having to estimate the entire data distribution. We investigate the theoretical properties of the optimal predicted distribution when applying convex functions to the loss, demonstrating that convex functions can sharpen the opt
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Tsetlin&#26426;&#22120;&#36827;&#34892;&#39640;&#25928;&#25968;&#25454;&#34701;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#30417;&#27979;&#23398;&#20064;&#21040;&#30340;&#36923;&#36753;&#23376;&#21477;&#22312;&#21160;&#24577;&#25968;&#25454;&#20013;&#30340;&#21464;&#21270;&#65292;&#35782;&#21035;&#21644;&#34701;&#21512;&#22122;&#22768;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20986;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.17207</link><description>&lt;p&gt;
&#20351;&#29992;Tsetlin&#26426;&#22120;&#36827;&#34892;&#39640;&#25928;&#25968;&#25454;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Efficient Data Fusion using the Tsetlin Machine. (arXiv:2310.17207v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17207
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Tsetlin&#26426;&#22120;&#36827;&#34892;&#39640;&#25928;&#25968;&#25454;&#34701;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#30417;&#27979;&#23398;&#20064;&#21040;&#30340;&#36923;&#36753;&#23376;&#21477;&#22312;&#21160;&#24577;&#25968;&#25454;&#20013;&#30340;&#21464;&#21270;&#65292;&#35782;&#21035;&#21644;&#34701;&#21512;&#22122;&#22768;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20986;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Tsetlin Machine&#35780;&#20272;&#21644;&#34701;&#21512;&#22024;&#26434;&#21160;&#24577;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#30417;&#27979;Tsetlin Machine&#23398;&#20064;&#21040;&#30340;&#36923;&#36753;&#23376;&#21477;&#22312;&#21160;&#24577;&#25968;&#25454;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#22122;&#22768;&#24773;&#20917;&#19979;&#30340;&#21464;&#21270;&#26469;&#35782;&#21035;&#21644;&#34701;&#21512;&#22122;&#22768;&#12290;&#36890;&#36807;&#38477;&#20302;&#20808;&#21069;&#23398;&#20064;&#21040;&#30340;&#23376;&#21477;&#30340;&#26435;&#37325;&#25110;&#20197;&#26032;&#30340;&#23376;&#21477;&#24418;&#24335;&#21453;&#26144;&#22122;&#22768;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#19968;&#39033;&#20840;&#38754;&#30340;&#23454;&#39564;&#30740;&#31350;&#65292;&#20351;&#29992;&#20102;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65292;&#32467;&#26524;&#26174;&#31034;&#20986;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel way of assessing and fusing noisy dynamic data using a Tsetlin Machine. Our approach consists in monitoring how explanations in form of logical clauses that a TM learns changes with possible noise in dynamic data. This way TM can recognize the noise by lowering weights of previously learned clauses, or reflect it in the form of new clauses. We also perform a comprehensive experimental study using notably different datasets that demonstrated high performance of the proposed approach.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#31034;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#32465;&#23450;ID&#26426;&#21046;&#65292;&#23427;&#21487;&#20197;&#23558;&#23454;&#20307;&#19982;&#23646;&#24615;&#36827;&#34892;&#26377;&#25928;&#22320;&#32465;&#23450;&#12290;&#25105;&#20204;&#36890;&#36807;&#22240;&#26524;&#24178;&#39044;&#23454;&#39564;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#28608;&#27963;&#34920;&#31034;&#32465;&#23450;&#20449;&#24687;&#30340;&#26041;&#24335;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#20013;&#22914;&#20309;&#34920;&#31034;&#31526;&#21495;&#30693;&#35782;&#65292;&#20174;&#32780;&#20026;&#29702;&#35299;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#33324;&#19978;&#19979;&#25991;&#25512;&#29702;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2310.17191</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#23558;&#23454;&#20307;&#32465;&#23450;&#21040;&#19978;&#19979;&#25991;&#20013;?
&lt;/p&gt;
&lt;p&gt;
How do Language Models Bind Entities in Context?. (arXiv:2310.17191v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17191
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#31034;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#32465;&#23450;ID&#26426;&#21046;&#65292;&#23427;&#21487;&#20197;&#23558;&#23454;&#20307;&#19982;&#23646;&#24615;&#36827;&#34892;&#26377;&#25928;&#22320;&#32465;&#23450;&#12290;&#25105;&#20204;&#36890;&#36807;&#22240;&#26524;&#24178;&#39044;&#23454;&#39564;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#28608;&#27963;&#34920;&#31034;&#32465;&#23450;&#20449;&#24687;&#30340;&#26041;&#24335;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#20013;&#22914;&#20309;&#34920;&#31034;&#31526;&#21495;&#30693;&#35782;&#65292;&#20174;&#32780;&#20026;&#29702;&#35299;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#33324;&#19978;&#19979;&#25991;&#25512;&#29702;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#27491;&#30830;&#20351;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#24517;&#39035;&#23558;&#23454;&#20307;&#19982;&#20854;&#23646;&#24615;&#36827;&#34892;&#32465;&#23450;&#12290;&#20363;&#22914;&#65292;&#32473;&#23450;&#25551;&#36848;&#8220;&#32511;&#33394;&#26041;&#22359;&#8221;&#21644;&#8220;&#34013;&#33394;&#22278;&#24418;&#8221;&#30340;&#19978;&#19979;&#25991;&#65292;LMs&#24517;&#39035;&#23558;&#24418;&#29366;&#19982;&#23427;&#20204;&#23545;&#24212;&#30340;&#39068;&#33394;&#36827;&#34892;&#32465;&#23450;&#12290;&#25105;&#20204;&#20998;&#26512;LM&#34920;&#31034;&#24182;&#30830;&#23450;&#32465;&#23450;ID&#26426;&#21046;&#65306;&#36825;&#26159;&#19968;&#31181;&#35299;&#20915;&#32465;&#23450;&#38382;&#39064;&#30340;&#36890;&#29992;&#26426;&#21046;&#65292;&#25105;&#20204;&#22312;Pythia&#21644;LLaMA&#23478;&#26063;&#30340;&#27599;&#20010;&#36275;&#22815;&#22823;&#30340;&#27169;&#22411;&#20013;&#35266;&#23519;&#21040;&#12290;&#36890;&#36807;&#22240;&#26524;&#24178;&#39044;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LMs&#20869;&#37096;&#28608;&#27963;&#36890;&#36807;&#23558;&#32465;&#23450;ID&#21521;&#37327;&#38468;&#21152;&#21040;&#30456;&#24212;&#30340;&#23454;&#20307;&#21644;&#23646;&#24615;&#19978;&#26469;&#34920;&#31034;&#32465;&#23450;&#20449;&#24687;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#32465;&#23450;ID&#21521;&#37327;&#24418;&#25104;&#36830;&#32493;&#30340;&#23376;&#31354;&#38388;&#65292;&#22312;&#36825;&#20010;&#23376;&#31354;&#38388;&#20013;&#65292;&#32465;&#23450;ID&#21521;&#37327;&#20043;&#38388;&#30340;&#36317;&#31163;&#21453;&#26144;&#20102;&#23427;&#20204;&#30340;&#21306;&#21035;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;LMs&#22312;&#19978;&#19979;&#25991;&#20013;&#34920;&#31034;&#31526;&#21495;&#30693;&#35782;&#30340;&#21487;&#35299;&#37322;&#31574;&#30053;&#65292;&#20026;&#29702;&#35299;&#22823;&#35268;&#27169;LMs&#20013;&#30340;&#19968;&#33324;&#19978;&#19979;&#25991;&#25512;&#29702;&#36808;&#20986;&#20102;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
To correctly use in-context information, language models (LMs) must bind entities to their attributes. For example, given a context describing a "green square" and a "blue circle", LMs must bind the shapes to their respective colors. We analyze LM representations and identify the binding ID mechanism: a general mechanism for solving the binding problem, which we observe in every sufficiently large model from the Pythia and LLaMA families. Using causal interventions, we show that LMs' internal activations represent binding information by attaching binding ID vectors to corresponding entities and attributes. We further show that binding ID vectors form a continuous subspace, in which distances between binding ID vectors reflect their discernability. Overall, our results uncover interpretable strategies in LMs for representing symbolic knowledge in-context, providing a step towards understanding general in-context reasoning in large-scale LMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#24037;&#20316;&#36890;&#36807;&#21033;&#29992;&#20004;&#31181;&#35821;&#35328;&#20043;&#38388;&#30340;&#23376;&#32593;&#32476;&#30456;&#20284;&#24615;&#20316;&#20026;&#39044;&#27979;XLT&#20013;&#35821;&#35328;&#20860;&#23481;&#24615;&#30340;&#20195;&#29702;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#26377;&#25928;&#30340;&#27169;&#22411;&#23548;&#21521;&#26041;&#27861;&#65292;&#19981;&#20381;&#36182;&#20110;&#22806;&#37096;&#36164;&#28304;&#65292;&#20165;&#38656;&#35201;&#20505;&#36873;&#35821;&#35328;&#30340;&#36866;&#37327;&#21407;&#22987;&#25991;&#26412;&#12290;</title><link>http://arxiv.org/abs/2310.17166</link><description>&lt;p&gt;
X-SNS: &#36890;&#36807;&#23376;&#32593;&#32476;&#30456;&#20284;&#24615;&#36827;&#34892;&#36328;&#35821;&#35328;&#36801;&#31227;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
X-SNS: Cross-Lingual Transfer Prediction through Sub-Network Similarity. (arXiv:2310.17166v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#24037;&#20316;&#36890;&#36807;&#21033;&#29992;&#20004;&#31181;&#35821;&#35328;&#20043;&#38388;&#30340;&#23376;&#32593;&#32476;&#30456;&#20284;&#24615;&#20316;&#20026;&#39044;&#27979;XLT&#20013;&#35821;&#35328;&#20860;&#23481;&#24615;&#30340;&#20195;&#29702;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#26377;&#25928;&#30340;&#27169;&#22411;&#23548;&#21521;&#26041;&#27861;&#65292;&#19981;&#20381;&#36182;&#20110;&#22806;&#37096;&#36164;&#28304;&#65292;&#20165;&#38656;&#35201;&#20505;&#36873;&#35821;&#35328;&#30340;&#36866;&#37327;&#21407;&#22987;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#35821;&#35328;&#36801;&#31227;&#65288;XLT&#65289;&#26159;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#31181;&#26032;&#20852;&#33021;&#21147;&#65292;&#24403;&#22312;&#26410;&#21253;&#21547;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#35821;&#35328;&#20013;&#35780;&#20272;&#26102;&#65292;&#33021;&#22815;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20445;&#30041;&#20854;&#22312;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#33521;&#35821;&#30001;&#20110;&#20854;&#24191;&#27867;&#20351;&#29992;&#65292;&#36890;&#24120;&#34987;&#35270;&#20026;&#21508;&#31181;&#20219;&#21153;&#20013;&#27169;&#22411;&#36866;&#24212;&#30340;&#20027;&#35201;&#35821;&#35328;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#26681;&#25454;&#29305;&#23450;&#26465;&#20214;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#28304;&#35821;&#35328;&#65292;&#21487;&#20197;&#25918;&#22823;XLT&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#20004;&#31181;&#35821;&#35328;&#20043;&#38388;&#30340;&#23376;&#32593;&#32476;&#30456;&#20284;&#24615;&#21033;&#29992;&#20026;&#22312;XLT&#29615;&#22659;&#20013;&#39044;&#27979;&#35821;&#35328;&#20860;&#23481;&#24615;&#30340;&#20195;&#29702;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#20197;&#27169;&#22411;&#20026;&#23548;&#21521;&#30340;&#65292;&#26356;&#22909;&#22320;&#21453;&#26144;&#20102;&#22522;&#30784;&#27169;&#22411;&#30340;&#20869;&#22312;&#24037;&#20316;&#26041;&#24335;&#12290;&#27492;&#22806;&#65292;&#23427;&#21482;&#38656;&#35201;&#20505;&#36873;&#35821;&#35328;&#30340;&#36866;&#37327;&#21407;&#22987;&#25991;&#26412;&#65292;&#19982;&#22823;&#22810;&#25968;&#20381;&#36182;&#22806;&#37096;&#36164;&#28304;&#30340;&#20197;&#21069;&#30340;&#26041;&#27861;&#36827;&#34892;&#21306;&#20998;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#26356;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-lingual transfer (XLT) is an emergent ability of multilingual language models that preserves their performance on a task to a significant extent when evaluated in languages that were not included in the fine-tuning process. While English, due to its widespread usage, is typically regarded as the primary language for model adaption in various tasks, recent studies have revealed that the efficacy of XLT can be amplified by selecting the most appropriate source languages based on specific conditions. In this work, we propose the utilization of sub-network similarity between two languages as a proxy for predicting the compatibility of the languages in the context of XLT. Our approach is model-oriented, better reflecting the inner workings of foundation models. In addition, it requires only a moderate amount of raw text from candidate languages, distinguishing it from the majority of previous methods that rely on external resources. In experiments, we demonstrate that our method is mo
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25552;&#39640;&#23398;&#26415;&#20889;&#20316;&#36136;&#37327;&#21644;&#25928;&#29575;&#30340;&#21407;&#21017;&#21644;&#26041;&#27861;&#65292;&#21253;&#25324;&#19968;&#20010;&#20154;&#26426;&#21327;&#20316;&#26694;&#26550;&#12289;&#26377;&#25928;&#30340;&#25552;&#31034;&#25216;&#26415;&#21644;&#20004;&#38454;&#27573;&#27169;&#22411;&#65292;&#26088;&#22312;&#23454;&#29616;&#35748;&#30693;&#21368;&#36733;&#21644;&#24819;&#35937;&#21050;&#28608;&#30340;AI&#36741;&#21161;&#20889;&#20316;&#12290;</title><link>http://arxiv.org/abs/2310.17143</link><description>&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#25512;&#21160;&#23398;&#26415;&#20889;&#20316;&#65306;&#26694;&#26550;&#12289;&#25216;&#26415;&#21644;&#27880;&#24847;&#20107;&#39033;
&lt;/p&gt;
&lt;p&gt;
Supercharging academic writing with generative AI: framework, techniques, and caveats. (arXiv:2310.17143v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17143
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25552;&#39640;&#23398;&#26415;&#20889;&#20316;&#36136;&#37327;&#21644;&#25928;&#29575;&#30340;&#21407;&#21017;&#21644;&#26041;&#27861;&#65292;&#21253;&#25324;&#19968;&#20010;&#20154;&#26426;&#21327;&#20316;&#26694;&#26550;&#12289;&#26377;&#25928;&#30340;&#25552;&#31034;&#25216;&#26415;&#21644;&#20004;&#38454;&#27573;&#27169;&#22411;&#65292;&#26088;&#22312;&#23454;&#29616;&#35748;&#30693;&#21368;&#36733;&#21644;&#24819;&#35937;&#21050;&#28608;&#30340;AI&#36741;&#21161;&#20889;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#26415;&#20889;&#20316;&#26159;&#30740;&#31350;&#39033;&#30446;&#20013;&#19981;&#21487;&#25110;&#32570;&#20294;&#36153;&#26102;&#36153;&#21147;&#30340;&#37096;&#20998;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#39640;&#23398;&#26415;&#20889;&#20316;&#36136;&#37327;&#21644;&#25928;&#29575;&#30340;&#21407;&#21017;&#21644;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20154;&#26426;&#21327;&#20316;&#26694;&#26550;&#65292;&#35814;&#32454;&#38416;&#36848;&#20102;AI&#22312;&#20889;&#20316;&#20013;&#30340;&#29702;&#35770;&#22522;&#30784;&#65288;&#20026;&#20160;&#20040;&#65289;&#12289;&#36807;&#31243;&#65288;&#22914;&#20309;&#65289;&#21644;&#24615;&#36136;&#65288;&#20160;&#20040;&#65289;&#12290;&#35813;&#26694;&#26550;&#25351;&#20986;&#20102;&#30701;&#26399;&#21644;&#38271;&#26399;&#21442;&#19982;AI&#20889;&#20316;&#30340;&#21407;&#22240;&#21450;&#20854;&#22522;&#26412;&#26426;&#21046;&#65288;&#22914;&#35748;&#30693;&#21368;&#36733;&#21644;&#24819;&#35937;&#21050;&#28608;&#65289;&#12290;&#23427;&#25581;&#31034;&#20102;AI&#22312;&#25972;&#20010;&#20889;&#20316;&#36807;&#31243;&#20013;&#30340;&#20316;&#29992;&#65292;&#36890;&#36807;&#19968;&#20010;&#20154;&#26426;&#21327;&#20316;&#20889;&#20316;&#30340;&#20004;&#38454;&#27573;&#27169;&#22411;&#21644;&#20889;&#20316;&#36741;&#21161;&#31867;&#22411;&#21644;&#32423;&#21035;&#30340;&#27169;&#22411;&#34920;&#31034;&#20102;AI&#22312;&#20889;&#20316;&#20013;&#30340;&#24110;&#21161;&#26041;&#24335;&#12290;&#22522;&#20110;&#35813;&#26694;&#26550;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#22312;&#20889;&#20316;&#24120;&#35268;&#20013;&#25972;&#21512;AI&#30340;&#26377;&#25928;&#25552;&#31034;&#25216;&#26415;&#65288;&#22823;&#32434;&#12289;&#36215;&#33609;&#21644;&#32534;&#36753;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Academic writing is an indispensable yet laborious part of the research enterprise. This Perspective maps out principles and methods for using generative artificial intelligence (AI), specifically large language models (LLMs), to elevate the quality and efficiency of academic writing. We introduce a human-AI collaborative framework that delineates the rationale (why), process (how), and nature (what) of AI engagement in writing. The framework pinpoints both short-term and long-term reasons for engagement and their underlying mechanisms (e.g., cognitive offloading and imaginative stimulation). It reveals the role of AI throughout the writing process, conceptualized through a two-stage model for human-AI collaborative writing, and the nature of AI assistance in writing, represented through a model of writing-assistance types and levels. Building on this framework, we describe effective prompting techniques for incorporating AI into the writing routine (outlining, drafting, and editing) a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#27169;&#22359;&#21270;&#21644;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#23454;&#38469;&#23545;&#35805;&#31995;&#32479;&#65292;&#36890;&#36807;&#32452;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#12289;&#31526;&#21495;&#21270;&#35268;&#21010;&#22120;&#21644;&#22522;&#20110;&#23454;&#38469;&#30340;&#20195;&#30721;&#25191;&#34892;&#26469;&#35299;&#20915;&#22522;&#20110;&#20219;&#21153;&#30340;&#23545;&#35805;&#20013;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#31995;&#32479;&#22312;&#21327;&#21516;&#21442;&#32771;&#35299;&#26512;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#26174;&#33879;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2310.17140</link><description>&lt;p&gt;
&#31526;&#21495;&#21270;&#35268;&#21010;&#21644;&#20195;&#30721;&#29983;&#25104;&#22312;&#22522;&#20110;&#23454;&#38469;&#23545;&#35805;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Symbolic Planning and Code Generation for Grounded Dialogue. (arXiv:2310.17140v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17140
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#27169;&#22359;&#21270;&#21644;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#23454;&#38469;&#23545;&#35805;&#31995;&#32479;&#65292;&#36890;&#36807;&#32452;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#12289;&#31526;&#21495;&#21270;&#35268;&#21010;&#22120;&#21644;&#22522;&#20110;&#23454;&#38469;&#30340;&#20195;&#30721;&#25191;&#34892;&#26469;&#35299;&#20915;&#22522;&#20110;&#20219;&#21153;&#30340;&#23545;&#35805;&#20013;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#31995;&#32479;&#22312;&#21327;&#21516;&#21442;&#32771;&#35299;&#26512;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#26174;&#33879;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#22788;&#29702;&#21644;&#29983;&#25104;&#25991;&#26412;&#21644;&#20195;&#30721;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#22312;&#22522;&#20110;&#20219;&#21153;&#30340;&#23545;&#35805;&#20013;&#65292;LLMs&#30340;&#36866;&#29992;&#24615;&#26377;&#38480;&#65292;&#22240;&#20026;&#24456;&#38590;&#24341;&#23548;&#20854;&#26397;&#30528;&#20219;&#21153;&#30446;&#26631;&#21069;&#36827;&#65292;&#24182;&#19988;&#26080;&#27861;&#22788;&#29702;&#26032;&#39062;&#30340;&#22522;&#20110;&#23454;&#38469;&#23545;&#35805;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22359;&#21270;&#21644;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#23454;&#38469;&#23545;&#35805;&#31995;&#32479;&#65292;&#36890;&#36807;&#32452;&#21512;LLMs&#21644;&#31526;&#21495;&#21270;&#35268;&#21010;&#22120;&#20197;&#21450;&#22522;&#20110;&#23454;&#38469;&#30340;&#20195;&#30721;&#25191;&#34892;&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#32570;&#28857;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#30001;&#38405;&#35835;&#22120;&#21644;&#35268;&#21010;&#22120;&#32452;&#25104;&#65306;&#38405;&#35835;&#22120;&#21033;&#29992;LLMs&#23558;&#21512;&#20316;&#20249;&#20276;&#30340;&#35805;&#35821;&#36716;&#25442;&#20026;&#21487;&#25191;&#34892;&#30340;&#20195;&#30721;&#65292;&#35843;&#29992;&#25191;&#34892;&#22522;&#20110;&#23454;&#38469;&#30340;&#20989;&#25968;&#12290;&#36716;&#25442;&#21518;&#30340;&#20195;&#30721;&#30340;&#36755;&#20986;&#34987;&#23384;&#20648;&#20197;&#36319;&#36394;&#23545;&#35805;&#29366;&#24577;&#65292;&#32780;&#31526;&#21495;&#21270;&#35268;&#21010;&#22120;&#30830;&#23450;&#19979;&#19968;&#20010;&#36866;&#24403;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#22312;&#35201;&#27714;&#39640;&#30340;OneCommon&#23545;&#35805;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#35813;&#20219;&#21153;&#28041;&#21450;&#35299;&#20915;&#25955;&#28857;&#22270;&#20687;&#30340;&#21327;&#21516;&#21442;&#32771;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#30340;&#22522;&#30784;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#21253;&#25324;&#22312;&#20154;&#24037;&#35780;&#20272;&#20013;&#25913;&#21892;&#20102;&#20219;&#21153;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) excel at processing and generating both text and code. However, LLMs have had limited applicability in grounded task-oriented dialogue as they are difficult to steer toward task objectives and fail to handle novel grounding. We present a modular and interpretable grounded dialogue system that addresses these shortcomings by composing LLMs with a symbolic planner and grounded code execution. Our system consists of a reader and planner: the reader leverages an LLM to convert partner utterances into executable code, calling functions that perform grounding. The translated code's output is stored to track dialogue state, while a symbolic planner determines the next appropriate response. We evaluate our system's performance on the demanding OneCommon dialogue task, involving collaborative reference resolution on abstract images of scattered dots. Our system substantially outperforms the previous state-of-the-art, including improving task success in human evaluat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35270;&#35273;&#38382;&#31572;&#23545;&#30340;&#26041;&#24335;&#23558;&#25506;&#27979;&#20449;&#21495;&#34701;&#20837;&#22810;&#27169;&#24577;&#26426;&#22120;&#32763;&#35793;&#20013;&#65292;&#20197;&#22686;&#24378;&#36328;&#27169;&#24577;&#20132;&#20114;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17133</link><description>&lt;p&gt;
&#36890;&#36807;&#35270;&#35273;&#38382;&#31572;&#23545;&#23558;&#25506;&#27979;&#20449;&#21495;&#34701;&#20837;&#22810;&#27169;&#24577;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Incorporating Probing Signals into Multimodal Machine Translation via Visual Question-Answering Pairs. (arXiv:2310.17133v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17133
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35270;&#35273;&#38382;&#31572;&#23545;&#30340;&#26041;&#24335;&#23558;&#25506;&#27979;&#20449;&#21495;&#34701;&#20837;&#22810;&#27169;&#24577;&#26426;&#22120;&#32763;&#35793;&#20013;&#65292;&#20197;&#22686;&#24378;&#36328;&#27169;&#24577;&#20132;&#20114;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#22810;&#27169;&#24577;&#26426;&#22120;&#32763;&#35793;(MMT)&#30340;&#28145;&#20837;&#30740;&#31350;&#65292;&#26816;&#39564;&#20102;MMT&#31995;&#32479;&#22312;&#25991;&#26412;&#36755;&#20837;&#23436;&#25972;&#26102;&#23545;&#35270;&#35273;&#20449;&#24687;&#30340;&#25935;&#24863;&#24615;&#38477;&#20302;&#30340;&#35748;&#35782;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#29616;&#35937;&#28304;&#20110;&#36328;&#27169;&#24577;&#20132;&#20114;&#19981;&#36275;&#65292;&#32780;&#19981;&#26159;&#22270;&#20687;&#20449;&#24687;&#20887;&#20313;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#20174;&#28304;&#25991;&#26412;&#29983;&#25104;&#24182;&#34892;&#30340;&#35270;&#35273;&#38382;&#31572;(VQA)&#26679;&#24335;&#23545;&#65292;&#20419;&#36827;&#26356;&#24378;&#22823;&#30340;&#36328;&#27169;&#24577;&#20132;&#20114;&#12290;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#65292;&#25105;&#20204;&#26126;&#30830;&#22320;&#23545;MMT&#20013;&#30340;&#25506;&#27979;&#20449;&#21495;&#36827;&#34892;&#24314;&#27169;&#65292;&#23558;&#20854;&#36716;&#21270;&#20026;VQA&#26679;&#24335;&#25968;&#25454;&#65292;&#21019;&#24314;&#20102;Multi30K-VQA&#25968;&#25454;&#38598;&#12290;&#24341;&#20837;&#20102;MMT-VQA&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#25968;&#25454;&#38598;&#20013;&#30340;&#26174;&#24335;&#25506;&#27979;&#20449;&#21495;&#34701;&#20837;MMT&#35757;&#32451;&#36807;&#31243;&#12290;&#22312;&#20004;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#36825;&#31181;&#26032;&#39062;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#21487;&#22312;&#20197;&#19979;&#38142;&#25509;&#33719;&#21462;&#65306;\url{https://github.com/libeineu/MMT-VQA}&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an in-depth study of multimodal machine translation (MMT), examining the prevailing understanding that MMT systems exhibit decreased sensitivity to visual information when text inputs are complete. Instead, we attribute this phenomenon to insufficient cross-modal interaction, rather than image information redundancy. A novel approach is proposed to generate parallel Visual Question-Answering (VQA) style pairs from the source text, fostering more robust cross-modal interaction. Using Large Language Models (LLMs), we explicitly model the probing signal in MMT to convert it into VQA-style data to create the Multi30K-VQA dataset. An MMT-VQA multitask learning framework is introduced to incorporate explicit probing signals from the dataset into the MMT training process. Experimental results on two widely-used benchmarks demonstrate the effectiveness of this novel approach. Our code and data would be available at: \url{https://github.com/libeineu/MMT-VQA}.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#30740;&#31350;&#20219;&#21153;&#8212;&#8212;&#22810;&#27169;&#24577;&#28459;&#30011;&#34917;&#20805;&#65288;M2C&#65289;&#65292;&#26088;&#22312;&#36890;&#36807;&#20849;&#20139;&#35821;&#20041;&#31354;&#38388;&#26469;&#35299;&#20915;&#25163;&#32472;&#28459;&#30011;&#20013;&#36951;&#28431;&#25991;&#23383;&#20869;&#23481;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#20010;&#28085;&#30422;&#20004;&#31181;&#35821;&#35328;&#30340;M2C&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;MCoT&#30340;&#28459;&#30011;&#20105;&#35758;&#26041;&#27861;&#21644;&#19968;&#31181;&#22522;&#20110;&#31934;&#32454;&#35270;&#35273;&#25552;&#31034;&#30340;&#34917;&#20805;&#27169;&#22411;FVP-M$^{2}$&#12290;</title><link>http://arxiv.org/abs/2310.17130</link><description>&lt;p&gt;
M2C&#65306;&#33258;&#21160;&#22810;&#27169;&#24577;&#28459;&#30011;&#34917;&#20805;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
M2C: Towards Automatic Multimodal Manga Complement. (arXiv:2310.17130v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17130
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#30740;&#31350;&#20219;&#21153;&#8212;&#8212;&#22810;&#27169;&#24577;&#28459;&#30011;&#34917;&#20805;&#65288;M2C&#65289;&#65292;&#26088;&#22312;&#36890;&#36807;&#20849;&#20139;&#35821;&#20041;&#31354;&#38388;&#26469;&#35299;&#20915;&#25163;&#32472;&#28459;&#30011;&#20013;&#36951;&#28431;&#25991;&#23383;&#20869;&#23481;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#20010;&#28085;&#30422;&#20004;&#31181;&#35821;&#35328;&#30340;M2C&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;MCoT&#30340;&#28459;&#30011;&#20105;&#35758;&#26041;&#27861;&#21644;&#19968;&#31181;&#22522;&#20110;&#31934;&#32454;&#35270;&#35273;&#25552;&#31034;&#30340;&#34917;&#20805;&#27169;&#22411;FVP-M$^{2}$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#28459;&#30011;&#20998;&#26512;&#26088;&#22312;&#36890;&#36807;&#35270;&#35273;&#21644;&#25991;&#26412;&#29305;&#24449;&#22686;&#24378;&#23545;&#28459;&#30011;&#30340;&#29702;&#35299;&#65292;&#21463;&#21040;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#31038;&#21306;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#24403;&#21069;&#65292;&#22823;&#37096;&#20998;&#28459;&#30011;&#37117;&#26159;&#25163;&#32472;&#30340;&#65292;&#23481;&#26131;&#36935;&#21040;&#38382;&#39064;&#65292;&#22914;&#32570;&#39029;&#12289;&#25991;&#26412;&#27745;&#26579;&#21644;&#32769;&#21270;&#65292;&#20174;&#32780;&#23548;&#33268;&#36951;&#28431;&#20102;&#28459;&#30011;&#25991;&#23383;&#20869;&#23481;&#65292;&#20005;&#37325;&#38459;&#30861;&#20102;&#20154;&#31867;&#30340;&#29702;&#35299;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#22810;&#27169;&#24577;&#28459;&#30011;&#34917;&#20805;&#65288;M2C&#65289;&#20219;&#21153;&#23578;&#26410;&#24471;&#21040;&#30740;&#31350;&#65292;&#35813;&#20219;&#21153;&#26088;&#22312;&#36890;&#36807;&#20026;&#35270;&#35273;&#21644;&#35821;&#35328;&#29702;&#35299;&#25552;&#20379;&#20849;&#20139;&#35821;&#20041;&#31354;&#38388;&#26469;&#22788;&#29702;&#19978;&#36848;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#24314;&#31435;&#19968;&#20010;&#28085;&#30422;&#20004;&#31181;&#35821;&#35328;&#30340;&#26032;M2C&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#28459;&#30011;&#34917;&#20805;&#20219;&#21153;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;MCoT&#30340;&#28459;&#30011;&#20105;&#35758;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#28459;&#30011;&#20013;&#25366;&#25496;&#20107;&#20214;&#30693;&#35782;&#12290;&#28982;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#22522;&#20934;&#26041;&#27861;FVP-M$^{2}$&#65292;&#20351;&#29992;&#31934;&#32454;&#30340;&#35270;&#35273;&#25552;&#31034;&#26469;&#25903;&#25345;&#28459;&#30011;&#34917;&#20805;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal manga analysis focuses on enhancing manga understanding with visual and textual features, which has attracted considerable attention from both natural language processing and computer vision communities. Currently, most comics are hand-drawn and prone to problems such as missing pages, text contamination, and aging, resulting in missing comic text content and seriously hindering human comprehension. In other words, the Multimodal Manga Complement (M2C) task has not been investigated, which aims to handle the aforementioned issues by providing a shared semantic space for vision and language understanding. To this end, we first propose the Multimodal Manga Complement task by establishing a new M2C benchmark dataset covering two languages. First, we design a manga argumentation method called MCoT to mine event knowledge in comics with large language models. Then, an effective baseline FVP-M$^{2}$ using fine-grained visual prompts is proposed to support manga complement. Extensi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20107;&#23454;&#25506;&#27979;&#30340;&#27979;&#35797;&#26102;&#38388;&#25968;&#25454;&#22686;&#24378;&#65288;TTA&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27979;&#35797;&#26102;&#38388;&#33258;&#21160;&#22686;&#21152;&#21644;&#32452;&#35013;&#25552;&#31034;&#65292;&#20943;&#23569;&#20851;&#20110;&#25552;&#31034;&#21464;&#21270;&#30340;&#25935;&#24863;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;TTA&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#27169;&#22411;&#20013;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;TTA&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#65292;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#25552;&#31034;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.17121</link><description>&lt;p&gt;
&#29992;&#20110;&#20107;&#23454;&#25506;&#27979;&#30340;&#27979;&#35797;&#26102;&#38388;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Test-time Augmentation for Factual Probing. (arXiv:2310.17121v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20107;&#23454;&#25506;&#27979;&#30340;&#27979;&#35797;&#26102;&#38388;&#25968;&#25454;&#22686;&#24378;&#65288;TTA&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27979;&#35797;&#26102;&#38388;&#33258;&#21160;&#22686;&#21152;&#21644;&#32452;&#35013;&#25552;&#31034;&#65292;&#20943;&#23569;&#20851;&#20110;&#25552;&#31034;&#21464;&#21270;&#30340;&#25935;&#24863;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;TTA&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#27169;&#22411;&#20013;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;TTA&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#65292;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#25552;&#31034;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#23454;&#25506;&#27979;&#26159;&#19968;&#31181;&#20351;&#29992;&#25552;&#31034;&#26469;&#27979;&#35797;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#8220;&#30693;&#36947;&#8221;&#26576;&#20123;&#19990;&#30028;&#30693;&#35782;&#20107;&#23454;&#30340;&#26041;&#27861;&#12290;&#20107;&#23454;&#25506;&#27979;&#20013;&#30340;&#19968;&#20010;&#38382;&#39064;&#26159;&#65292;&#23545;&#25552;&#31034;&#36827;&#34892;&#24494;&#23567;&#30340;&#26356;&#25913;&#21487;&#33021;&#20250;&#23548;&#33268;&#27169;&#22411;&#36755;&#20986;&#30340;&#24040;&#22823;&#21464;&#21270;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#25991;&#26412;&#25366;&#25496;&#25110;&#24494;&#35843;&#26469;&#20248;&#21270;&#25552;&#31034;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#26159;&#20851;&#31995;&#29305;&#23450;&#30340;&#65292;&#24182;&#19988;&#19981;&#33021;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#20851;&#31995;&#31867;&#22411;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#27979;&#35797;&#26102;&#38388;&#25968;&#25454;&#22686;&#24378;&#65288;TTA&#65289;&#20316;&#20026;&#19968;&#31181;&#20851;&#31995;&#19981;&#21487;&#30693;&#30340;&#26041;&#27861;&#65292;&#22312;&#27979;&#35797;&#26102;&#38388;&#33258;&#21160;&#22686;&#21152;&#21644;&#32452;&#35013;&#25552;&#31034;&#26469;&#20943;&#23569;&#23545;&#25552;&#31034;&#21464;&#21270;&#30340;&#25935;&#24863;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;TTA&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#65292;&#21363;&#20351;&#29992;TTA&#65292;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#26356;&#22909;&#22320;&#21453;&#26144;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#23545;&#20110;&#26576;&#20123;&#27169;&#22411;&#65292;&#39044;&#27979;&#20934;&#30830;&#24615;&#26377;&#25152;&#25552;&#39640;&#65292;&#20294;&#23545;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;TTA&#23548;&#33268;&#20102;&#24615;&#33021;&#19979;&#38477;&#12290;&#38169;&#35823;&#20998;&#26512;&#30830;&#23450;&#20102;&#20135;&#29983;&#39640;&#36136;&#37327;&#25552;&#31034;&#21464;&#21270;&#30340;&#22256;&#38590;&#20316;&#20026;TTA&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Factual probing is a method that uses prompts to test if a language model "knows" certain world knowledge facts. A problem in factual probing is that small changes to the prompt can lead to large changes in model output. Previous work aimed to alleviate this problem by optimizing prompts via text mining or fine-tuning. However, such approaches are relation-specific and do not generalize to unseen relation types. Here, we propose to use test-time augmentation (TTA) as a relation-agnostic method for reducing sensitivity to prompt variations by automatically augmenting and ensembling prompts at test time. Experiments show improved model calibration, i.e., with TTA, model confidence better reflects prediction accuracy. Improvements in prediction accuracy are observed for some models, but for other models, TTA leads to degradation. Error analysis identifies the difficulty of producing high-quality prompt variations as the main challenge for TTA.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#36827;&#34892;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#30446;&#21069;&#20027;&#39064;&#20998;&#27573;&#27169;&#22411;&#22312;&#27492;&#31867;&#25968;&#25454;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#19981;&#36275;&#65292;&#24182;&#25552;&#20986;&#20102;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#30456;&#23545;&#23567;&#35268;&#27169;&#30340;&#30446;&#26631;&#25968;&#25454;&#38598;&#26469;&#25913;&#21892;&#20998;&#27573;&#32467;&#26524;&#30340;&#26041;&#27861;&#12290;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#20351;&#29992;&#22810;&#31181;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#20943;&#36731;&#38750;&#32467;&#26500;&#21270;&#23545;&#35805;&#25968;&#25454;&#38598;&#30340;&#19981;&#24179;&#34913;&#25928;&#24212;&#12290;</title><link>http://arxiv.org/abs/2310.17120</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#23545;&#21322;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#23545;&#35805;&#25968;&#25454;&#36827;&#34892;&#20027;&#39064;&#20998;&#27573;
&lt;/p&gt;
&lt;p&gt;
Topic Segmentation of Semi-Structured and Unstructured Conversational Datasets using Language Models. (arXiv:2310.17120v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17120
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#36827;&#34892;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#30446;&#21069;&#20027;&#39064;&#20998;&#27573;&#27169;&#22411;&#22312;&#27492;&#31867;&#25968;&#25454;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#19981;&#36275;&#65292;&#24182;&#25552;&#20986;&#20102;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#30456;&#23545;&#23567;&#35268;&#27169;&#30340;&#30446;&#26631;&#25968;&#25454;&#38598;&#26469;&#25913;&#21892;&#20998;&#27573;&#32467;&#26524;&#30340;&#26041;&#27861;&#12290;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#20351;&#29992;&#22810;&#31181;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#20943;&#36731;&#38750;&#32467;&#26500;&#21270;&#23545;&#35805;&#25968;&#25454;&#38598;&#30340;&#19981;&#24179;&#34913;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#23558;&#25991;&#26723;&#25110;&#23545;&#35805;&#26681;&#25454;&#20854;&#35821;&#20041;&#32467;&#26500;&#20998;&#35299;&#20026;&#22810;&#20010;&#36830;&#32493;&#29255;&#27573;&#26159;&#19968;&#20010;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#24110;&#21161;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#20851;&#20110;&#20027;&#39064;&#20998;&#27573;&#30340;&#30740;&#31350;&#24448;&#24448;&#38598;&#20013;&#22312;&#32467;&#26500;&#21270;&#25991;&#26412;&#30340;&#20998;&#27573;&#19978;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#20998;&#26512;&#20102;&#26368;&#20808;&#36827;&#30340;&#20027;&#39064;&#20998;&#27573;&#27169;&#22411;&#22312;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#65306;&#65288;a&#65289;&#30446;&#21069;&#22312;&#22823;&#35268;&#27169;&#32467;&#26500;&#21270;&#25991;&#26412;&#35821;&#26009;&#24211;&#65288;&#22914;Wiki-727K&#65289;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#31574;&#30053;&#23545;&#20110;&#22312;&#38750;&#32467;&#26500;&#21270;&#23545;&#35805;&#25968;&#25454;&#19978;&#30340;&#21487;&#20256;&#36882;&#24615;&#24182;&#19981;&#26377;&#24110;&#21161;&#12290;&#65288;b&#65289;&#20174;&#22836;&#24320;&#22987;&#20351;&#29992;&#30456;&#23545;&#23567;&#35268;&#27169;&#30340;&#30446;&#26631;&#38750;&#32467;&#26500;&#21270;&#39046;&#22495;&#25968;&#25454;&#38598;&#35757;&#32451;&#33021;&#26174;&#33879;&#25552;&#39640;&#20998;&#27573;&#32467;&#26524;&#12290;&#25105;&#20204;&#36890;&#36807;&#23581;&#35797;&#22810;&#31181;&#25439;&#22833;&#20989;&#25968;&#26469;&#36827;&#34892;&#25105;&#20204;&#30340;&#20027;&#39064;&#20998;&#27573;&#26041;&#27861;&#30340;&#24378;&#21270;&#27979;&#35797;&#65292;&#20197;&#20943;&#36731;&#38750;&#32467;&#26500;&#21270;&#23545;&#35805;&#25968;&#25454;&#38598;&#30340;&#19981;&#24179;&#34913;&#25928;&#24212;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;Fo
&lt;/p&gt;
&lt;p&gt;
Breaking down a document or a conversation into multiple contiguous segments based on its semantic structure is an important and challenging problem in NLP, which can assist many downstream tasks. However, current works on topic segmentation often focus on segmentation of structured texts. In this paper, we comprehensively analyze the generalization capabilities of state-of-the-art topic segmentation models on unstructured texts. We find that: (a) Current strategies of pre-training on a large corpus of structured text such as Wiki-727K do not help in transferability to unstructured conversational data. (b) Training from scratch with only a relatively small-sized dataset of the target unstructured domain improves the segmentation results by a significant margin. We stress-test our proposed Topic Segmentation approach by experimenting with multiple loss functions, in order to mitigate effects of imbalance in unstructured conversational datasets. Our empirical evaluation indicates that Fo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;FLEEK&#65292;&#19968;&#20010;&#33021;&#33258;&#21160;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#20107;&#23454;&#20027;&#24352;&#24182;&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#28304;&#36827;&#34892;&#35780;&#20272;&#21644;&#20462;&#27491;&#30340;&#24037;&#20855;&#65292;&#20197;&#20943;&#23569;&#25991;&#26412;&#20013;&#30340;&#20107;&#23454;&#38169;&#35823;&#12290;&#21021;&#27493;&#23454;&#35777;&#35780;&#20272;&#26174;&#31034;FLEEK&#20855;&#26377;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.17119</link><description>&lt;p&gt;
FLEEK: &#20174;&#22806;&#37096;&#30693;&#35782;&#20013;&#26816;&#27979;&#21644;&#20462;&#27491;&#20107;&#23454;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;
FLEEK: Factual Error Detection and Correction with Evidence Retrieved from External Knowledge. (arXiv:2310.17119v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;FLEEK&#65292;&#19968;&#20010;&#33021;&#33258;&#21160;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#20107;&#23454;&#20027;&#24352;&#24182;&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#28304;&#36827;&#34892;&#35780;&#20272;&#21644;&#20462;&#27491;&#30340;&#24037;&#20855;&#65292;&#20197;&#20943;&#23569;&#25991;&#26412;&#20013;&#30340;&#20107;&#23454;&#38169;&#35823;&#12290;&#21021;&#27493;&#23454;&#35777;&#35780;&#20272;&#26174;&#31034;FLEEK&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#25991;&#26412;&#20449;&#24687;&#20013;&#30340;&#20107;&#23454;&#38169;&#35823;&#23545;&#20110;&#20570;&#20986;&#26126;&#26234;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#65292;&#26080;&#35770;&#26159;&#30001;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#30340;&#36824;&#26159;&#30001;&#20154;&#31867;&#31574;&#21010;&#30340;&#12290;LLMs&#26080;&#27861;&#23558;&#23427;&#20204;&#30340;&#20027;&#24352;&#24402;&#22240;&#20110;&#22806;&#37096;&#30693;&#35782;&#65292;&#19988;&#26131;&#20110;&#20135;&#29983;&#34394;&#26500;&#65292;&#36825;&#20351;&#24471;&#20381;&#36182;&#23427;&#20204;&#30340;&#22238;&#31572;&#21464;&#24471;&#22256;&#38590;&#12290;&#20154;&#31867;&#22312;&#20889;&#20316;&#36807;&#31243;&#20013;&#20063;&#23481;&#26131;&#20986;&#29616;&#20107;&#23454;&#38169;&#35823;&#12290;&#30001;&#20110;&#25163;&#21160;&#26816;&#27979;&#21644;&#20462;&#27491;&#20107;&#23454;&#38169;&#35823;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#21147;&#25237;&#20837;&#65292;&#22240;&#27492;&#24320;&#21457;&#33258;&#21160;&#21270;&#26041;&#27861;&#21487;&#20197;&#26497;&#22823;&#22320;&#20943;&#23569;&#20154;&#21147;&#24037;&#20316;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;FLEEK&#65292;&#19968;&#20010;&#21407;&#22411;&#24037;&#20855;&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#20107;&#23454;&#20027;&#24352;&#65292;&#20174;&#22806;&#37096;&#30693;&#35782;&#28304;&#25910;&#38598;&#35777;&#25454;&#65292;&#35780;&#20272;&#27599;&#20010;&#20027;&#24352;&#30340;&#30495;&#23454;&#24615;&#65292;&#24182;&#20351;&#29992;&#25910;&#38598;&#30340;&#35777;&#25454;&#25552;&#20986;&#20462;&#27491;&#24314;&#35758;&#12290;&#21021;&#27493;&#30340;&#23454;&#35777;&#35780;&#20272;&#26174;&#31034;FLEEK&#22312;&#20107;&#23454;&#38169;&#35823;&#26816;&#27979;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#65288;77-85&#65285;&#30340;F1&#65289;&#12290;FLEEK&#30340;&#35270;&#39057;&#28436;&#31034;&#21487;&#22312;https://youtu.be/NapJFUlkPdQ&#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting factual errors in textual information, whether generated by large language models (LLM) or curated by humans, is crucial for making informed decisions. LLMs' inability to attribute their claims to external knowledge and their tendency to hallucinate makes it difficult to rely on their responses. Humans, too, are prone to factual errors in their writing. Since manual detection and correction of factual errors is labor-intensive, developing an automatic approach can greatly reduce human effort. We present FLEEK, a prototype tool that automatically extracts factual claims from text, gathers evidence from external knowledge sources, evaluates the factuality of each claim, and suggests revisions for identified errors using the collected evidence. Initial empirical evaluation on fact error detection (77-85\% F1) shows the potential of FLEEK. A video demo of FLEEK can be found at https://youtu.be/NapJFUlkPdQ.
&lt;/p&gt;</description></item><item><title>Transformers&#23398;&#20250;&#20102;&#39640;&#38454;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#36890;&#36807;&#23454;&#29616;&#31867;&#20284;&#20110;&#36845;&#20195;&#29275;&#39039;&#27861;&#30340;&#31639;&#27861;&#65292;&#32780;&#19981;&#26159;&#26799;&#24230;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2310.17086</link><description>&lt;p&gt;
Transformers&#23398;&#20250;&#20102;&#39640;&#38454;&#20248;&#21270;&#26041;&#27861;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#65306;&#19968;&#39033;&#19982;&#32447;&#24615;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Transformers Learn Higher-Order Optimization Methods for In-Context Learning: A Study with Linear Models. (arXiv:2310.17086v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17086
&lt;/p&gt;
&lt;p&gt;
Transformers&#23398;&#20250;&#20102;&#39640;&#38454;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#36890;&#36807;&#23454;&#29616;&#31867;&#20284;&#20110;&#36845;&#20195;&#29275;&#39039;&#27861;&#30340;&#31639;&#27861;&#65292;&#32780;&#19981;&#26159;&#26799;&#24230;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformers&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#26159;&#23427;&#20204;&#26159;&#22914;&#20309;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#20173;&#28982;&#26159;&#19968;&#20010;&#35868;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;Transformers&#21487;&#33021;&#36890;&#36807;&#20869;&#37096;&#36816;&#34892;&#26799;&#24230;&#19979;&#38477;&#65292;&#21363;&#19968;&#38454;&#20248;&#21270;&#26041;&#27861;&#65292;&#26469;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Transformers&#23398;&#20250;&#20102;&#23454;&#29616;&#39640;&#38454;&#20248;&#21270;&#26041;&#27861;&#26469;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#25105;&#20204;&#20197;&#19978;&#19979;&#25991;&#32447;&#24615;&#22238;&#24402;&#20026;&#37325;&#28857;&#65292;&#23637;&#31034;&#20102;Transformers&#23398;&#20250;&#20102;&#23454;&#29616;&#19968;&#20010;&#38750;&#24120;&#31867;&#20284;&#20110;&#36845;&#20195;&#29275;&#39039;&#27861;&#30340;&#31639;&#27861;&#65292;&#32780;&#19981;&#26159;&#26799;&#24230;&#19979;&#38477;&#12290;&#20174;&#23454;&#35777;&#19978;&#26469;&#30475;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36830;&#32493;&#30340;Transformer&#23618;&#30340;&#39044;&#27979;&#19982;&#29275;&#39039;&#27861;&#30340;&#19981;&#21516;&#36845;&#20195;&#38750;&#24120;&#25509;&#36817;&#65292;&#27599;&#20010;&#20013;&#38388;&#23618;&#22823;&#33268;&#35745;&#31639;&#20102;3&#27425;&#36845;&#20195;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#38656;&#35201;&#25351;&#25968;&#32423;&#30340;&#26799;&#24230;&#19979;&#38477;&#27493;&#39588;&#25165;&#33021;&#21305;&#37197;&#39069;&#22806;&#30340;Transformer&#23618;&#65307;&#36825;&#34920;&#26126;Transformers&#20855;&#26377;&#30456;&#24403;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers are remarkably good at in-context learning (ICL) -- learning from demonstrations without parameter updates -- but how they perform ICL remains a mystery. Recent work suggests that Transformers may learn in-context by internally running Gradient Descent, a first-order optimization method. In this paper, we instead demonstrate that Transformers learn to implement higher-order optimization methods to perform ICL. Focusing on in-context linear regression, we show that Transformers learn to implement an algorithm very similar to Iterative Newton's Method, a higher-order optimization method, rather than Gradient Descent. Empirically, we show that predictions from successive Transformer layers closely match different iterations of Newton's Method linearly, with each middle layer roughly computing 3 iterations. In contrast, exponentially more Gradient Descent steps are needed to match an additional Transformers layer; this suggests that Transformers have an comparable rate of conv
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#24418;&#24335;&#21270;&#39640;&#32423;&#25968;&#23398;&#27010;&#24565;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#25209;&#21028;&#24615;&#22320;&#23457;&#26597;&#21644;&#26816;&#26597;&#30740;&#31350;&#35770;&#25991;&#20013;&#25968;&#23398;&#25512;&#29702;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2310.17064</link><description>&lt;p&gt;
math-PVS:&#19968;&#20010;&#23558;&#31185;&#23398;&#20986;&#29256;&#29289;&#26144;&#23556;&#21040;PVS&#29702;&#35770;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
math-PVS: A Large Language Model Framework to Map Scientific Publications to PVS Theories. (arXiv:2310.17064v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17064
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#24418;&#24335;&#21270;&#39640;&#32423;&#25968;&#23398;&#27010;&#24565;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#25209;&#21028;&#24615;&#22320;&#23457;&#26597;&#21644;&#26816;&#26597;&#30740;&#31350;&#35770;&#25991;&#20013;&#25968;&#23398;&#25512;&#29702;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#22312;&#21508;&#31181;&#24212;&#29992;&#39046;&#22495;&#30340;&#24191;&#27867;&#37319;&#29992;&#65292;&#23427;&#22312;&#25968;&#23398;&#21457;&#29616;&#26041;&#38754;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#21487;&#20197;&#24341;&#23548;&#29468;&#24819;&#29983;&#25104;&#65292;&#26500;&#36896;&#21453;&#20363;&#65292;&#21327;&#21161;&#24418;&#24335;&#21270;&#25968;&#23398;&#65292;&#24182;&#21457;&#29616;&#19981;&#21516;&#25968;&#23398;&#39046;&#22495;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#31561;&#31561;&#12290;&#23613;&#31649;&#20043;&#21069;&#30340;&#24037;&#20316;&#21033;&#29992;&#35745;&#31639;&#26426;&#36827;&#34892;&#35814;&#23613;&#30340;&#25968;&#23398;&#35777;&#26126;&#25628;&#32034;&#65292;&#20294;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#36817;&#21162;&#21147;&#33268;&#21147;&#20110;&#23558;&#35745;&#31639;&#24179;&#21488;&#23450;&#20301;&#20026;&#25968;&#23398;&#30740;&#31350;&#36807;&#31243;&#20013;&#30340;&#21512;&#20316;&#36129;&#29486;&#32773;&#12290;&#23613;&#31649;&#30446;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#36923;&#36753;&#21644;&#25968;&#23398;&#20219;&#21153;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#20294;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#23545;&#23558;&#23450;&#29702;&#35777;&#26126;&#31995;&#32479;&#19982;&#22522;&#30784;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#38271;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;LLMs&#22312;&#24418;&#24335;&#21270;&#39640;&#32423;&#25968;&#23398;&#27010;&#24565;&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#22815;&#25209;&#21028;&#24615;&#22320;&#23457;&#26597;&#21644;&#26816;&#26597;&#30740;&#31350;&#35770;&#25991;&#20013;&#25968;&#23398;&#25512;&#29702;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
As artificial intelligence (AI) gains greater adoption in a wide variety of applications, it has immense potential to contribute to mathematical discovery, by guiding conjecture generation, constructing counterexamples, assisting in formalizing mathematics, and discovering connections between different mathematical areas, to name a few.  While prior work has leveraged computers for exhaustive mathematical proof search, recent efforts based on large language models (LLMs) aspire to position computing platforms as co-contributors in the mathematical research process. Despite their current limitations in logic and mathematical tasks, there is growing interest in melding theorem proving systems with foundation models. This work investigates the applicability of LLMs in formalizing advanced mathematical concepts and proposes a framework that can critically review and check mathematical reasoning in research papers. Given the noted reasoning shortcomings of LLMs, our approach synergizes the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#40657;&#30418;&#25511;&#21046;&#26469;&#24341;&#23548;&#20923;&#32467;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PTLM&#65289;&#29983;&#25104;&#26356;&#21152;&#24120;&#35782;&#24615;&#30340;&#25991;&#26412;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2310.17054</link><description>&lt;p&gt;
&#25552;&#21319;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20013;&#30340;&#24120;&#35782;&#33021;&#21147;&#65306;&#21033;&#29992;&#40657;&#30418;&#25511;&#21046;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
BOOST: Harnessing Black-Box Control to Boost Commonsense in LMs' Generation. (arXiv:2310.17054v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#40657;&#30418;&#25511;&#21046;&#26469;&#24341;&#23548;&#20923;&#32467;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PTLM&#65289;&#29983;&#25104;&#26356;&#21152;&#24120;&#35782;&#24615;&#30340;&#25991;&#26412;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;GPT-3&#24050;&#32463;&#23637;&#31034;&#20102;&#29983;&#25104;&#36830;&#36143;&#19988;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#25991;&#26412;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#23427;&#20204;&#30340;&#25104;&#21151;&#20043;&#20013;&#65292;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#65306;&#23427;&#20204;&#29983;&#25104;&#30340;&#36755;&#20986;&#26377;&#26102;&#20173;&#28982;&#32570;&#20047;&#24120;&#35782;&#12290;&#27492;&#22806;&#65292;&#22914;&#26524;&#19981;&#21487;&#34892;&#30340;&#35805;&#65292;&#23558;&#25972;&#20010;LLM&#36827;&#34892;&#24494;&#35843;&#20197;&#33719;&#24471;&#26356;&#21152;&#24120;&#35782;&#24615;&#30340;&#36755;&#20986;&#26159;&#35745;&#31639;&#19978;&#20195;&#20215;&#26114;&#36149;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#23558;&#19968;&#20010;&#20923;&#32467;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PTLM&#65289;&#24341;&#23548;&#21521;&#26356;&#21152;&#24120;&#35782;&#24615;&#30340;&#29983;&#25104;&#65288;&#21363;&#20197;&#26377;&#24847;&#20041;&#30340;&#26041;&#24335;&#20135;&#29983;&#21253;&#21547;&#19968;&#31995;&#21015;&#27010;&#24565;&#30340;&#21512;&#29702;&#36755;&#20986;&#65289;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#20010;&#26080;&#38656;&#21442;&#32771;&#30340;&#35780;&#20272;&#22120;&#65292;&#36890;&#36807;&#23558;&#21477;&#23376;&#19982;&#19968;&#20010;&#21160;&#24577;&#24120;&#35782;&#30693;&#35782;&#24211;&#22312;&#22235;&#20010;&#19981;&#21516;&#20851;&#31995;&#26041;&#38754;&#30456;&#36830;&#26469;&#20026;&#21477;&#23376;&#20998;&#37197;&#19968;&#20010;&#24120;&#35782;&#24471;&#20998;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#35780;&#20998;&#22120;&#20316;&#20026;&#24120;&#35782;&#30693;&#35782;&#30340;&#21442;&#32771;&#65292;&#25193;&#23637;&#20102;&#21517;&#20026;NADO&#30340;&#21487;&#25511;&#29983;&#25104;&#26041;&#27861;&#65292;&#35757;&#32451;&#20102;&#19968;&#20010;&#36741;&#21161;&#22836;&#37096;&#26469;&#24341;&#23548;&#19968;&#20010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) such as GPT-3 have demonstrated a strong capability to generate coherent and contextually relevant text. However, amidst their successes, a crucial issue persists: their generated outputs still lack commonsense at times. Moreover, fine-tuning the entire LLM towards more commonsensical outputs is computationally expensive if not infeasible. In this paper, we present a computation-efficient framework that steers a frozen Pre-Trained Language Model (PTLM) towards more commonsensical generation (i.e., producing a plausible output that incorporates a list of concepts in a meaningful way). Specifically, we first construct a reference-free evaluator that assigns a sentence with a commonsensical score by grounding the sentence to a dynamic commonsense knowledge base from four different relational aspects. We then use the scorer as the oracle for commonsense knowledge, and extend the controllable generation method called NADO to train an auxiliary head that guides a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#34920;&#26126;&#65292;&#23545;&#20110;&#19981;&#21516;&#30340;&#19979;&#28216;&#35821;&#35328;&#20219;&#21153;&#65292;&#21482;&#23545;&#35821;&#35328;&#32534;&#30721;&#22120;&#30340;&#37096;&#20998;&#23618;&#36827;&#34892;&#32454;&#35843;&#21363;&#21487;&#33719;&#24471;&#25509;&#36817;&#29978;&#33267;&#20248;&#20110;&#32454;&#35843;&#25152;&#26377;&#23618;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#39640;&#25928;&#24230;&#37327;&#26041;&#27861;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#21487;&#20197;&#36873;&#25321;&#24615;&#24494;&#35843;&#23548;&#33268;&#24378;&#22823;&#19979;&#28216;&#24615;&#33021;&#30340;&#23618;&#12290;&#30740;&#31350;&#31361;&#20986;&#34920;&#26126;&#65292;&#20219;&#21153;&#29305;&#23450;&#20449;&#24687;&#36890;&#24120;&#23616;&#37096;&#21270;&#22312;&#23569;&#25968;&#23618;&#20869;&#65292;&#21482;&#35843;&#25972;&#36825;&#20123;&#23618;&#23601;&#36275;&#22815;&#20102;&#12290;</title><link>http://arxiv.org/abs/2310.17041</link><description>&lt;p&gt;
&#20851;&#20110;&#35821;&#35328;&#32534;&#30721;&#22120;&#30340;&#25163;&#26415;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
On Surgical Fine-tuning for Language Encoders. (arXiv:2310.17041v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#34920;&#26126;&#65292;&#23545;&#20110;&#19981;&#21516;&#30340;&#19979;&#28216;&#35821;&#35328;&#20219;&#21153;&#65292;&#21482;&#23545;&#35821;&#35328;&#32534;&#30721;&#22120;&#30340;&#37096;&#20998;&#23618;&#36827;&#34892;&#32454;&#35843;&#21363;&#21487;&#33719;&#24471;&#25509;&#36817;&#29978;&#33267;&#20248;&#20110;&#32454;&#35843;&#25152;&#26377;&#23618;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#39640;&#25928;&#24230;&#37327;&#26041;&#27861;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#21487;&#20197;&#36873;&#25321;&#24615;&#24494;&#35843;&#23548;&#33268;&#24378;&#22823;&#19979;&#28216;&#24615;&#33021;&#30340;&#23618;&#12290;&#30740;&#31350;&#31361;&#20986;&#34920;&#26126;&#65292;&#20219;&#21153;&#29305;&#23450;&#20449;&#24687;&#36890;&#24120;&#23616;&#37096;&#21270;&#22312;&#23569;&#25968;&#23618;&#20869;&#65292;&#21482;&#35843;&#25972;&#36825;&#20123;&#23618;&#23601;&#36275;&#22815;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32454;&#35843;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#35821;&#35328;&#32534;&#30721;&#22120;&#30340;&#25152;&#26377;&#23618;&#65288;&#20351;&#29992;&#25152;&#26377;&#21442;&#25968;&#25110;&#20351;&#29992;&#21442;&#25968;&#39640;&#25928;&#30340;&#26041;&#27861;&#65289;&#24448;&#24448;&#26159;&#23558;&#20854;&#36866;&#24212;&#20110;&#26032;&#20219;&#21153;&#30340;&#40664;&#35748;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35777;&#25454;&#65292;&#23545;&#20110;&#19981;&#21516;&#30340;&#19979;&#28216;&#35821;&#35328;&#20219;&#21153;&#65292;&#20165;&#32454;&#35843;&#37096;&#20998;&#23618;&#21363;&#21487;&#33719;&#24471;&#25509;&#36817;&#29978;&#33267;&#20248;&#20110;&#32454;&#35843;&#35821;&#35328;&#32534;&#30721;&#22120;&#30340;&#25152;&#26377;&#23618;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Fisher&#20449;&#24687;&#30697;&#38453;&#30340;&#23545;&#35282;&#32447;&#65288;FIM&#35780;&#20998;&#65289;&#30340;&#39640;&#25928;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#36873;&#25321;&#29992;&#20110;&#36873;&#25321;&#24615;&#24494;&#35843;&#30340;&#20505;&#36873;&#23618;&#12290;&#25105;&#20204;&#22312;GLUE&#21644;SuperGLUE&#20219;&#21153;&#20197;&#21450;&#19981;&#21516;&#30340;&#35821;&#35328;&#32534;&#30721;&#22120;&#19978;&#32463;&#39564;&#24615;&#22320;&#23637;&#31034;&#20102;&#65292;&#36825;&#20010;&#24230;&#37327;&#21487;&#20197;&#26377;&#25928;&#36873;&#25321;&#23548;&#33268;&#24378;&#22823;&#19979;&#28216;&#24615;&#33021;&#30340;&#23618;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#31361;&#20986;&#20102;&#19982;&#32473;&#23450;&#30340;&#19979;&#28216;&#20219;&#21153;&#23545;&#24212;&#30340;&#20219;&#21153;&#29305;&#23450;&#20449;&#24687;&#36890;&#24120;&#23616;&#37096;&#21270;&#22312;&#23569;&#25968;&#23618;&#20869;&#65292;&#21482;&#35843;&#25972;&#36825;&#20123;&#23618;&#23545;&#20110;&#24378;&#22823;&#30340;&#24615;&#33021;&#23601;&#36275;&#22815;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning all the layers of a pre-trained neural language encoder (either using all the parameters or using parameter-efficient methods) is often the de-facto way of adapting it to a new task. We show evidence that for different downstream language tasks, fine-tuning only a subset of layers is sufficient to obtain performance that is close to and often better than fine-tuning all the layers in the language encoder. We propose an efficient metric based on the diagonal of the Fisher information matrix (FIM score), to select the candidate layers for selective fine-tuning. We show, empirically on GLUE and SuperGLUE tasks and across distinct language encoders, that this metric can effectively select layers leading to a strong downstream performance. Our work highlights that task-specific information corresponding to a given downstream task is often localized within a few layers, and tuning only those is sufficient for strong performance. Additionally, we demonstrate the robustness of the 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35299;&#20915;&#20102;&#22312;&#35821;&#38899;&#21161;&#25163;&#20013;&#36890;&#36807;&#22768;&#38899;&#25552;&#31034;&#20026;&#29992;&#25143;&#25552;&#20379;&#21518;&#32493;&#38382;&#39064;&#24314;&#35758;&#30340;&#26032;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015; Transformer &#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22312;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.17034</link><description>&lt;p&gt;
&#36890;&#36807;&#22768;&#38899;&#25552;&#31034;&#20026;&#35821;&#38899;&#21161;&#25163;&#25552;&#20379;&#21518;&#32493;&#38382;&#39064;&#24314;&#35758;
&lt;/p&gt;
&lt;p&gt;
Follow-on Question Suggestion via Voice Hints for Voice Assistants. (arXiv:2310.17034v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17034
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35299;&#20915;&#20102;&#22312;&#35821;&#38899;&#21161;&#25163;&#20013;&#36890;&#36807;&#22768;&#38899;&#25552;&#31034;&#20026;&#29992;&#25143;&#25552;&#20379;&#21518;&#32493;&#38382;&#39064;&#24314;&#35758;&#30340;&#26032;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015; Transformer &#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22312;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22768;&#38899;&#21161;&#25163;&#65288;&#22914;Alexa&#25110;Siri&#65289;&#30340;&#20351;&#29992;&#24050;&#32463;&#24555;&#36895;&#22686;&#38271;&#65292;&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#35821;&#38899;&#25628;&#32034;&#21363;&#26102;&#33719;&#21462;&#20449;&#24687;&#12290;&#26597;&#35810;&#24314;&#35758;&#26159;&#22522;&#20110;&#23631;&#24149;&#30340;&#25628;&#32034;&#20307;&#39564;&#30340;&#26631;&#20934;&#21151;&#33021;&#65292;&#20801;&#35768;&#29992;&#25143;&#25506;&#32034;&#26356;&#22810;&#35805;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#22522;&#20110;&#35821;&#38899;&#30340;&#35774;&#32622;&#20013;&#23454;&#29616;&#36825;&#19968;&#28857;&#24182;&#19981;&#26159;&#31616;&#21333;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#19968;&#39033;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#36890;&#36807;&#31616;&#26126;&#33258;&#28982;&#30340;&#22768;&#38899;&#25552;&#31034;&#24314;&#35758;&#29992;&#25143;&#25552;&#38382;&#21518;&#32493;&#38382;&#39064;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#36825;&#20010;&#20219;&#21153;&#65292;&#23558;&#20854;&#22522;&#20110;&#21477;&#27861;&#29702;&#35770;&#65292;&#24182;&#27010;&#36848;&#20102;&#21475;&#35821;&#25552;&#31034;&#30340;&#35821;&#35328;&#26399;&#26395;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#32447;&#27169;&#22411;&#21644;&#19968;&#31181;&#20351;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015; Transformer &#30340;&#26041;&#27861;&#65292;&#20174;&#19968;&#31995;&#21015;&#38382;&#39064;&#20013;&#29983;&#25104;&#22768;&#38899;&#25552;&#31034;&#12290;&#36890;&#36807;&#19968;&#20010;&#30001;6681&#20010;&#36755;&#20837;&#38382;&#39064;&#21644;&#20154;&#24037;&#32534;&#20889;&#30340;&#25552;&#31034;&#32452;&#25104;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#20351;&#29992;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#21644;&#20154;&#24037;&#35780;&#20272;&#26469;&#35780;&#20272;&#27169;&#22411;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#31616;&#21333;&#22320;&#23558;&#24314;&#35758;&#30340;&#38382;&#39064;&#36830;&#25509;&#36215;&#26469;&#20250;&#20135;&#29983;&#20302;&#36136;&#37327;&#30340;&#22768;&#38899;&#25552;&#31034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#21160;&#26426;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The adoption of voice assistants like Alexa or Siri has grown rapidly, allowing users to instantly access information via voice search. Query suggestion is a standard feature of screen-based search experiences, allowing users to explore additional topics. However, this is not trivial to implement in voice-based settings. To enable this, we tackle the novel task of suggesting questions with compact and natural voice hints to allow users to ask follow-up questions.  We define the task, ground it in syntactic theory and outline linguistic desiderata for spoken hints. We propose baselines and an approach using sequence-to-sequence Transformers to generate spoken hints from a list of questions. Using a new dataset of 6681 input questions and human written hints, we evaluated the models with automatic metrics and human evaluation. Results show that a naive approach of concatenating suggested questions creates poor voice hints. Our approach, which applies a linguistically-motivated pretrainin
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21463;&#25511;&#35299;&#30721;&#65288;CD&#65289;&#30340;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25511;&#21046;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#65292;&#20197;&#36798;&#21040;&#39640;&#22238;&#25253;&#30340;&#32467;&#26524;&#12290;CD&#36890;&#36807;&#21069;&#32512;&#35780;&#20998;&#22120;&#26469;&#24341;&#23548;&#29983;&#25104;&#65292;&#21487;&#20197;&#22312;&#25512;&#29702;&#26102;&#39044;&#27979;&#39044;&#26399;&#22238;&#25253;&#65292;&#24182;&#19988;&#20855;&#26377;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#32780;&#19981;&#22686;&#21152;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17022</link><description>&lt;p&gt;
&#21463;&#25511;&#35299;&#30721;&#26469;&#33258;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Controlled Decoding from Language Models. (arXiv:2310.17022v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17022
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21463;&#25511;&#35299;&#30721;&#65288;CD&#65289;&#30340;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25511;&#21046;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#65292;&#20197;&#36798;&#21040;&#39640;&#22238;&#25253;&#30340;&#32467;&#26524;&#12290;CD&#36890;&#36807;&#21069;&#32512;&#35780;&#20998;&#22120;&#26469;&#24341;&#23548;&#29983;&#25104;&#65292;&#21487;&#20197;&#22312;&#25512;&#29702;&#26102;&#39044;&#27979;&#39044;&#26399;&#22238;&#25253;&#65292;&#24182;&#19988;&#20855;&#26377;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#32780;&#19981;&#22686;&#21152;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#21463;&#25511;&#35299;&#30721;&#65288;CD&#65289;&#65292;&#29992;&#20110;&#25511;&#21046;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#65292;&#20197;&#33719;&#24471;&#39640;&#22238;&#25253;&#30340;&#32467;&#26524;&#12290;CD&#36890;&#36807;&#20540;&#20989;&#25968;&#26469;&#35299;&#20915;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#35813;&#20540;&#20989;&#25968;&#34987;&#31216;&#20026;&#21069;&#32512;&#35780;&#20998;&#22120;&#12290;&#21069;&#32512;&#35780;&#20998;&#22120;&#22312;&#25512;&#29702;&#26102;&#29992;&#20110;&#24341;&#23548;&#29983;&#25104;&#21521;&#26356;&#39640;&#22238;&#25253;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21069;&#32512;&#35780;&#20998;&#22120;&#21487;&#20197;&#20174;&#65288;&#21487;&#33021;&#26159;&#65289;&#31163;&#31574;&#30053;&#25968;&#25454;&#20013;&#35757;&#32451;&#20986;&#26469;&#65292;&#29992;&#20110;&#39044;&#27979;&#20174;&#37096;&#20998;&#35299;&#30721;&#30340;&#21709;&#24212;&#32487;&#32493;&#35299;&#30721;&#26102;&#30340;&#39044;&#26399;&#22238;&#25253;&#12290;&#25105;&#20204;&#22312;Reddit&#23545;&#35805;&#35821;&#26009;&#24211;&#19978;&#32463;&#39564;&#35777;&#26126;&#65292;CD&#20316;&#20026;&#19968;&#31181;&#25511;&#21046;&#26426;&#21046;&#26159;&#26377;&#25928;&#30340;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;CD&#35774;&#35745;&#30340;&#27169;&#22359;&#21270;&#20351;&#20854;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#32780;&#19981;&#20250;&#22686;&#21152;&#20219;&#20309;&#22797;&#26434;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;CD&#21487;&#20197;&#20197;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#22359;&#26041;&#24335;&#22312;&#25512;&#29702;&#26102;&#24212;&#29992;&#65292;&#21516;&#26679;&#26080;&#38656;&#20219;&#20309;&#39069;&#22806;&#30340;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose controlled decoding (CD), a novel off-policy reinforcement learning method to control the autoregressive generation from language models towards high reward outcomes. CD solves an off-policy reinforcement learning problem through a value function for the reward, which we call a prefix scorer. The prefix scorer is used at inference time to steer the generation towards higher reward outcomes. We show that the prefix scorer may be trained on (possibly) off-policy data to predict the expected reward when decoding is continued from a partially decoded response. We empirically demonstrate that CD is effective as a control mechanism on Reddit conversations corpus. We also show that the modularity of the design of CD makes it possible to control for multiple rewards, effectively solving a multi-objective reinforcement learning problem with no additional complexity. Finally, we show that CD can be applied in a novel blockwise fashion at inference-time, again without the need for any 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#25193;&#23637;&#30340;Meta-World&#22522;&#20934;&#65292;&#31216;&#20026;&#8220;&#35821;&#35328;&#19990;&#30028;&#8221;&#65292;&#20801;&#35768;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27169;&#25311;&#26426;&#22120;&#20154;&#29615;&#22659;&#20013;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#21644;&#33050;&#26412;&#25216;&#33021;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#35745;&#21010;&#26465;&#20214;&#34892;&#20026;&#20811;&#38534;&#65288;PCBC&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31471;&#21040;&#31471;&#28436;&#31034;&#23545;&#39640;&#32423;&#35745;&#21010;&#36827;&#34892;&#24494;&#35843;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#65292;PCBC&#22312;&#35821;&#35328;&#19990;&#30028;&#20013;&#33021;&#22815;&#23454;&#29616;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.17019</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#26377;&#26465;&#20214;&#22320;&#32452;&#21512;&#26426;&#22120;&#20154;&#25216;&#33021;
&lt;/p&gt;
&lt;p&gt;
Conditionally Combining Robot Skills using Large Language Models. (arXiv:2310.17019v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17019
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#25193;&#23637;&#30340;Meta-World&#22522;&#20934;&#65292;&#31216;&#20026;&#8220;&#35821;&#35328;&#19990;&#30028;&#8221;&#65292;&#20801;&#35768;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27169;&#25311;&#26426;&#22120;&#20154;&#29615;&#22659;&#20013;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#21644;&#33050;&#26412;&#25216;&#33021;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#35745;&#21010;&#26465;&#20214;&#34892;&#20026;&#20811;&#38534;&#65288;PCBC&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31471;&#21040;&#31471;&#28436;&#31034;&#23545;&#39640;&#32423;&#35745;&#21010;&#36827;&#34892;&#24494;&#35843;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#65292;PCBC&#22312;&#35821;&#35328;&#19990;&#30028;&#20013;&#33021;&#22815;&#23454;&#29616;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32467;&#21512;&#20102;&#20004;&#20010;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Meta-World&#22522;&#20934;&#30340;&#19968;&#20010;&#25193;&#23637;&#65292;&#31216;&#20026;&#8220;&#35821;&#35328;&#19990;&#30028;&#8221;&#65292;&#23427;&#20801;&#35768;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19968;&#20010;&#27169;&#25311;&#26426;&#22120;&#20154;&#29615;&#22659;&#20013;&#20351;&#29992;&#21322;&#32467;&#26500;&#21270;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#21644;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#33050;&#26412;&#25216;&#33021;&#12290;&#36890;&#36807;&#20351;&#29992;&#19982;Meta-World&#30456;&#21516;&#30340;&#20219;&#21153;&#38598;&#65292;&#21487;&#20197;&#36731;&#26494;&#27604;&#36739;&#35821;&#35328;&#19990;&#30028;&#21644;Meta-World&#30340;&#32467;&#26524;&#65292;&#20174;&#32780;&#23545;&#26368;&#36817;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#35745;&#21010;&#26465;&#20214;&#34892;&#20026;&#20811;&#38534;&#65288;PCBC&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#20801;&#35768;&#23545;&#39640;&#32423;&#35745;&#21010;&#30340;&#34892;&#20026;&#36827;&#34892;&#24494;&#35843;&#65292;&#20351;&#29992;&#31471;&#21040;&#31471;&#28436;&#31034;&#12290;&#20351;&#29992;&#35821;&#35328;&#19990;&#30028;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;PCBC&#33021;&#22815;&#22312;&#21508;&#31181;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#21462;&#24471;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#36890;&#24120;&#21482;&#38656;&#19968;&#20010;&#28436;&#31034;&#21363;&#21487;&#23454;&#29616;&#20219;&#21153;&#30340;&#27867;&#21270;&#12290;&#25105;&#20204;&#23558;&#35821;&#35328;&#19990;&#30028;&#20316;&#20026;&#24320;&#28304;&#36719;&#20214;&#25552;&#20379;&#65292;&#32593;&#22336;&#26159;https://...
&lt;/p&gt;
&lt;p&gt;
This paper combines two contributions. First, we introduce an extension of the Meta-World benchmark, which we call "Language-World," which allows a large language model to operate in a simulated robotic environment using semi-structured natural language queries and scripted skills described using natural language. By using the same set of tasks as Meta-World, Language-World results can be easily compared to Meta-World results, allowing for a point of comparison between recent methods using Large Language Models (LLMs) and those using Deep Reinforcement Learning. Second, we introduce a method we call Plan Conditioned Behavioral Cloning (PCBC), that allows finetuning the behavior of high-level plans using end-to-end demonstrations. Using Language-World, we show that PCBC is able to achieve strong performance in a variety of few-shot regimes, often achieving task generalization with as little as a single demonstration. We have made Language-World available as open-source software at https
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;PRISMA&#26694;&#26550;&#32508;&#36848;&#20102;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#21307;&#23398;&#39046;&#22495;&#21457;&#34920;&#30340;534&#31687;&#35770;&#25991;&#65292;&#21457;&#29616;&#20102;136&#31687;&#20851;&#38190;&#35770;&#25991;&#65292;&#28085;&#30422;&#20102;&#24515;&#29702;&#20581;&#24247;&#23545;&#35805;&#20195;&#29702;&#30340;&#22810;&#31181;&#24314;&#27169;&#21644;&#23454;&#39564;&#35774;&#35745;&#25216;&#26415;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2310.17017</link><description>&lt;p&gt;
&#22522;&#20110;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#21307;&#23398;&#35270;&#35282;&#30340;&#24515;&#29702;&#20581;&#24247;&#23545;&#35805;&#20195;&#29702;&#32508;&#21512;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
An Integrative Survey on Mental Health Conversational Agents to Bridge Computer Science and Medical Perspectives. (arXiv:2310.17017v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;PRISMA&#26694;&#26550;&#32508;&#36848;&#20102;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#21307;&#23398;&#39046;&#22495;&#21457;&#34920;&#30340;534&#31687;&#35770;&#25991;&#65292;&#21457;&#29616;&#20102;136&#31687;&#20851;&#38190;&#35770;&#25991;&#65292;&#28085;&#30422;&#20102;&#24515;&#29702;&#20581;&#24247;&#23545;&#35805;&#20195;&#29702;&#30340;&#22810;&#31181;&#24314;&#27169;&#21644;&#23454;&#39564;&#35774;&#35745;&#25216;&#26415;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#29702;&#20581;&#24247;&#23545;&#35805;&#20195;&#29702;&#65288;&#20063;&#31216;&#20026;&#32842;&#22825;&#26426;&#22120;&#20154;&#65289;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#22240;&#20854;&#25552;&#20379;&#23545;&#37027;&#20123;&#38754;&#20020;&#24515;&#29702;&#20581;&#24247;&#25361;&#25112;&#30340;&#20154;&#21487;&#21450;&#24615;&#25903;&#25345;&#30340;&#28508;&#21147;&#12290;&#20043;&#21069;&#23545;&#35813;&#20027;&#39064;&#30340;&#35843;&#26597;&#20027;&#35201;&#32771;&#34385;&#22312;&#35745;&#31639;&#26426;&#31185;&#23398;&#25110;&#21307;&#23398;&#39046;&#22495;&#21457;&#34920;&#30340;&#35770;&#25991;&#65292;&#23548;&#33268;&#29702;&#35299;&#19978;&#30340;&#20998;&#35010;&#65292;&#38459;&#30861;&#20102;&#20004;&#20010;&#39046;&#22495;&#20043;&#38388;&#26377;&#30410;&#30693;&#35782;&#30340;&#20849;&#20139;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#40511;&#27807;&#65292;&#25105;&#20204;&#20351;&#29992;PRISMA&#26694;&#26550;&#36827;&#34892;&#20840;&#38754;&#30340;&#25991;&#29486;&#32508;&#36848;&#65292;&#23457;&#26597;&#20102;534&#31687;&#22312;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#21307;&#23398;&#39046;&#22495;&#21457;&#34920;&#30340;&#35770;&#25991;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#32508;&#36848;&#25581;&#31034;&#20102;136&#31687;&#20851;&#38190;&#35770;&#25991;&#65292;&#28085;&#30422;&#20102;&#24314;&#31435;&#19982;&#24515;&#29702;&#20581;&#24247;&#30456;&#20851;&#30340;&#23545;&#35805;&#20195;&#29702;&#30340;&#22810;&#31181;&#24314;&#27169;&#21644;&#23454;&#39564;&#35774;&#35745;&#25216;&#26415;&#29305;&#24449;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35745;&#31639;&#26426;&#31185;&#23398;&#35770;&#25991;&#20391;&#37325;&#20110;LLM&#25216;&#26415;&#21644;&#20351;&#29992;&#33258;&#21160;&#24230;&#37327;&#35780;&#20272;&#21709;&#24212;&#36136;&#37327;&#65292;&#23545;&#24212;&#29992;&#24212;&#29992;&#20851;&#27880;&#36739;&#23569;&#65292;&#32780;&#21307;&#23398;&#35770;&#25991;&#21017;&#20351;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;&#23545;&#35805;&#20195;&#29702;&#21644;&#32467;&#26524;&#24230;&#37327;&#26469;&#34913;&#37327;&#21442;&#19982;&#32773;&#30340;&#20581;&#24247;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mental health conversational agents (a.k.a. chatbots) are widely studied for their potential to offer accessible support to those experiencing mental health challenges. Previous surveys on the topic primarily consider papers published in either computer science or medicine, leading to a divide in understanding and hindering the sharing of beneficial knowledge between both domains. To bridge this gap, we conduct a comprehensive literature review using the PRISMA framework, reviewing 534 papers published in both computer science and medicine. Our systematic review reveals 136 key papers on building mental health-related conversational agents with diverse characteristics of modeling and experimental design techniques. We find that computer science papers focus on LLM techniques and evaluating response quality using automated metrics with little attention to the application while medical papers use rule-based conversational agents and outcome metrics to measure the health outcomes of parti
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#22312;&#23567;&#35268;&#27169;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19978;&#30340;&#24212;&#29992;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#22686;&#24378;&#21518;&#30340;&#25968;&#25454;&#35757;&#32451;&#20998;&#31867;&#22120;&#27169;&#22411;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24773;&#24863;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.17015</link><description>&lt;p&gt;
&#22312;&#23567;&#35268;&#27169;&#19981;&#24179;&#34913;&#30340;&#25991;&#26412;&#25968;&#25454;&#20013;&#36827;&#34892;&#24773;&#24863;&#26816;&#27979;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation for Emotion Detection in Small Imbalanced Text Data. (arXiv:2310.17015v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17015
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#22312;&#23567;&#35268;&#27169;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19978;&#30340;&#24212;&#29992;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#22686;&#24378;&#21518;&#30340;&#25968;&#25454;&#35757;&#32451;&#20998;&#31867;&#22120;&#27169;&#22411;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24773;&#24863;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#24773;&#24863;&#35782;&#21035;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#20219;&#21153;&#26159;&#35782;&#21035;&#20986;&#35832;&#22914;&#21916;&#24742;&#25110;&#24868;&#24594;&#31561;&#24773;&#24863;&#12290;&#20854;&#20013;&#19968;&#20010;&#25361;&#25112;&#26159;&#21487;&#29992;&#30340;&#26631;&#27880;&#20102;&#24773;&#24863;&#30340;&#25968;&#25454;&#38598;&#30340;&#21294;&#20047;&#12290;&#26576;&#20123;&#29616;&#26377;&#25968;&#25454;&#38598;&#35268;&#27169;&#36739;&#23567;&#65292;&#37319;&#29992;&#19981;&#21516;&#30340;&#24773;&#24863;&#20998;&#31867;&#20307;&#31995;&#65292;&#24182;&#19988;&#24773;&#24863;&#20998;&#24067;&#19981;&#24179;&#34913;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#23545;&#23567;&#35268;&#27169;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#24403;&#21069;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#65288;&#22914;RoBERTa&#65289;&#24615;&#33021;&#19981;&#20339;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#22235;&#31181;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65288;Easy Data Augmentation EDA&#12289;&#22522;&#20110;&#23884;&#20837;&#30340;&#38745;&#24577;&#21644;&#19978;&#19979;&#25991;&#30456;&#20851;&#12289;&#21450;ProtAugment&#65289;&#22312;&#19977;&#20010;&#28304;&#22836;&#21644;&#19981;&#21516;&#35268;&#27169;&#12289;&#24773;&#24863;&#31867;&#21035;&#21450;&#20998;&#24067;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#22686;&#24378;&#21518;&#30340;&#25968;&#25454;&#35757;&#32451;&#20998;&#31867;&#22120;&#27169;&#22411;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#65306;&#30452;&#25509;&#20351;&#29992;&#22686;&#24378;&#21518;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#19982;&#20351;&#29992;&#21407;&#22987;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#23545;&#27604;&#65292;&#24182;&#20998;&#26512;&#20102;&#19981;&#21516;&#24773;&#24863;&#31867;&#21035;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion recognition in text, the task of identifying emotions such as joy or anger, is a challenging problem in NLP with many applications. One of the challenges is the shortage of available datasets that have been annotated with emotions. Certain existing datasets are small, follow different emotion taxonomies and display imbalance in their emotion distribution. In this work, we studied the impact of data augmentation techniques precisely when applied to small imbalanced datasets, for which current state-of-the-art models (such as RoBERTa) under-perform. Specifically, we utilized four data augmentation methods (Easy Data Augmentation EDA, static and contextual Embedding-based, and ProtAugment) on three datasets that come from different sources and vary in size, emotion categories and distributions. Our experimental results show that using the augmented data when training the classifier model leads to significant improvements. Finally, we conducted two case studies: a) directly using t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#21407;&#22411;&#32593;&#32476;&#26041;&#27861;&#25193;&#23637;&#21040;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#23398;&#20064;&#30340;&#21152;&#26435;&#30456;&#20284;&#24230;&#24230;&#37327;&#21644;&#20107;&#21518;&#21487;&#35299;&#37322;&#24615;&#26426;&#21046;&#65292;&#36890;&#36807;&#32858;&#28966;&#20110;&#21477;&#23376;&#23884;&#20837;&#30340;&#37325;&#35201;&#32500;&#24230;&#26469;&#25552;&#39640;&#30456;&#20284;&#24230;&#35745;&#31639;&#65292;&#24182;&#25913;&#21892;&#20102;&#39044;&#27979;&#24615;&#33021;&#21644;&#35299;&#37322;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17010</link><description>&lt;p&gt;
&#36825;&#23601;&#20687;&#37027;&#26679;&#38405;&#35835;&#65306;&#29992;&#20110;&#21487;&#35299;&#37322;&#24615;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
This Reads Like That: Deep Learning for Interpretable Natural Language Processing. (arXiv:2310.17010v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#21407;&#22411;&#32593;&#32476;&#26041;&#27861;&#25193;&#23637;&#21040;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#23398;&#20064;&#30340;&#21152;&#26435;&#30456;&#20284;&#24230;&#24230;&#37327;&#21644;&#20107;&#21518;&#21487;&#35299;&#37322;&#24615;&#26426;&#21046;&#65292;&#36890;&#36807;&#32858;&#28966;&#20110;&#21477;&#23376;&#23884;&#20837;&#30340;&#37325;&#35201;&#32500;&#24230;&#26469;&#25552;&#39640;&#30456;&#20284;&#24230;&#35745;&#31639;&#65292;&#24182;&#25913;&#21892;&#20102;&#39044;&#27979;&#24615;&#33021;&#21644;&#35299;&#37322;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21407;&#22411;&#23398;&#20064;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#24615;&#20915;&#31574;&#65292;&#36890;&#36807;&#23398;&#20064;&#30340;&#21407;&#22411;&#30340;&#30456;&#20284;&#24615;&#26469;&#23545;&#26032;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#12290;&#23613;&#31649;&#23427;&#20027;&#35201;&#24212;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#65292;&#20294;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#22312;&#20043;&#21069;&#30340;&#30740;&#31350;&#22522;&#30784;&#19978;&#65292;&#36827;&#19968;&#27493;&#25506;&#32034;&#20102;&#21407;&#22411;&#32593;&#32476;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#25193;&#23637;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#23398;&#20064;&#30340;&#21152;&#26435;&#30456;&#20284;&#24230;&#24230;&#37327;&#65292;&#36890;&#36807;&#32858;&#28966;&#20110;&#39044;&#35757;&#32451;&#30340;&#21477;&#23376;&#23884;&#20837;&#30340;&#20449;&#24687;&#32500;&#24230;&#26469;&#22686;&#24378;&#30456;&#20284;&#24230;&#35745;&#31639;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20107;&#21518;&#21487;&#35299;&#37322;&#24615;&#26426;&#21046;&#65292;&#20174;&#21407;&#22411;&#21644;&#36755;&#20837;&#21477;&#23376;&#20013;&#25552;&#21462;&#19982;&#39044;&#27979;&#30456;&#20851;&#30340;&#21333;&#35789;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#19981;&#20165;&#25913;&#21892;&#20102;&#22312;AG News&#21644;RT Polarity&#25968;&#25454;&#38598;&#19978;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#32780;&#19988;&#19982;&#22522;&#20110;&#21512;&#29702;&#24615;&#30340;&#36882;&#24402;&#21367;&#31215;&#30456;&#27604;&#65292;&#36824;&#25552;&#39640;&#20102;&#35299;&#37322;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prototype learning, a popular machine learning method designed for inherently interpretable decisions, leverages similarities to learned prototypes for classifying new data. While it is mainly applied in computer vision, in this work, we build upon prior research and further explore the extension of prototypical networks to natural language processing. We introduce a learned weighted similarity measure that enhances the similarity computation by focusing on informative dimensions of pre-trained sentence embeddings. Additionally, we propose a post-hoc explainability mechanism that extracts prediction-relevant words from both the prototype and input sentences. Finally, we empirically demonstrate that our proposed method not only improves predictive performance on the AG News and RT Polarity datasets over a previous prototype-based approach, but also improves the faithfulness of explanations compared to rationale-based recurrent convolutions.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#23553;&#38381;&#39046;&#22495;&#30340;&#25277;&#21462;&#24335;&#38382;&#31572;&#65292;&#24341;&#20837;&#20102;&#26377;&#38024;&#23545;&#24615;&#30340;&#39044;&#35757;&#32451;&#30340;&#27010;&#24565;&#65292;&#24182;&#20351;&#29992;Galactica&#29983;&#25104;&#20102;&#21512;&#25104;&#30340;"&#26377;&#38024;&#23545;&#24615;"&#30340;&#35821;&#26009;&#24211;&#12290;</title><link>http://arxiv.org/abs/2310.16995</link><description>&lt;p&gt;
&#36136;&#37327;&gt;&#25968;&#37327;&#65306;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#23553;&#38381;&#39046;&#22495;&#25277;&#21462;&#24335;&#38382;&#31572;&#30340;&#21512;&#25104;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
Quality &gt; Quantity: Synthetic Corpora from Foundation Models for Closed-Domain Extractive Question Answering. (arXiv:2310.16995v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16995
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#23553;&#38381;&#39046;&#22495;&#30340;&#25277;&#21462;&#24335;&#38382;&#31572;&#65292;&#24341;&#20837;&#20102;&#26377;&#38024;&#23545;&#24615;&#30340;&#39044;&#35757;&#32451;&#30340;&#27010;&#24565;&#65292;&#24182;&#20351;&#29992;Galactica&#29983;&#25104;&#20102;&#21512;&#25104;&#30340;"&#26377;&#38024;&#23545;&#24615;"&#30340;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#36866;&#24212;&#26159;&#23558;&#27169;&#22411;&#22312;&#19968;&#20010;&#39046;&#22495;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#24212;&#29992;&#20110;&#21478;&#19968;&#20010;&#39046;&#22495;&#30340;&#36807;&#31243;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290;&#34429;&#28982;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#19968;&#20010;&#29305;&#23450;&#39046;&#22495;&#30340;&#22522;&#30784;&#27169;&#22411;(FM)&#26159;&#19968;&#20010;&#36873;&#25321;&#65292;&#20294;&#26368;&#36817;&#30340;&#26041;&#27861;&#38598;&#20013;&#22312;&#35843;&#25972;&#39044;&#35757;&#32451;&#30340;FM&#20197;&#28385;&#36275;&#29305;&#23450;&#39046;&#22495;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#21457;&#29616;&#65292;&#26080;&#35770;&#26159;&#21738;&#31181;&#26041;&#27861;&#65292;&#22312;&#30446;&#26631;&#39046;&#22495;&#37117;&#26080;&#27861;&#22987;&#32456;&#36798;&#21040;&#26368;&#20808;&#36827;( SOTA)&#30340;&#32467;&#26524;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#23553;&#38381;&#39046;&#22495;&#20869;&#30340;&#25277;&#21462;&#24335;&#38382;&#31572;&#65292;&#24182;&#24341;&#20837;&#20102;&#26377;&#38024;&#23545;&#24615;&#30340;&#39044;&#35757;&#32451;&#30340;&#27010;&#24565;&#12290;&#36825;&#24847;&#21619;&#30528;&#30830;&#23450;&#21644;&#29983;&#25104;&#30456;&#20851;&#25968;&#25454;&#65292;&#36827;&#19968;&#27493;&#39044;&#35757;&#32451;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#32780;&#19981;&#26159;&#20256;&#32479;&#30340;&#21033;&#29992;&#22312;&#24191;&#27867;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#29305;&#23450;&#39046;&#22495;&#30340;FM&#30340;&#29702;&#24565;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#20351;&#29992;Galactica&#29983;&#25104;&#19982;&#29305;&#23450;&#20889;&#20316;&#39118;&#26684;&#21644;&#20027;&#39064;(&#22914;&#30740;&#31350;&#35770;&#25991;&#21644;&#25918;&#23556;&#23398;&#25253;&#21578;)&#30456;&#19968;&#33268;&#30340;&#21512;&#25104;"&#26377;&#38024;&#23545;&#24615;"&#30340;&#35821;&#26009;&#24211;&#12290;&#36825;&#20010;&#36807;&#31243;&#21487;&#20197;&#30475;&#20316;&#26159;&#19968;&#31181;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Domain adaptation, the process of training a model in one domain and applying it to another, has been extensively explored in machine learning. While training a domain-specific foundation model (FM) from scratch is an option, recent methods have focused on adapting pre-trained FMs for domain-specific tasks. However, our experiments reveal that either approach does not consistently achieve state-of-the-art (SOTA) results in the target domain. In this work, we study extractive question answering within closed domains and introduce the concept of targeted pre-training. This involves determining and generating relevant data to further pre-train our models, as opposed to the conventional philosophy of utilizing domain-specific FMs trained on a wide range of data. Our proposed framework uses Galactica to generate synthetic, ``targeted'' corpora that align with specific writing styles and topics, such as research papers and radiology reports. This process can be viewed as a form of knowledge 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#36827;&#34892;&#20102;&#35782;&#21035;&#65292;&#21457;&#29616;&#22312;&#36739;&#39640;&#28201;&#24230;&#20540;&#19979;&#65292;&#27973;&#23618;&#23398;&#20064;&#31639;&#27861;&#30340;&#26816;&#27979;&#20934;&#30830;&#29575;&#36739;&#20302;&#65292;&#32780;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#20998;&#31867;&#22120;&#30340;&#20934;&#30830;&#29575;&#39640;&#12290;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#25913;&#36827;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#25104;&#21151;&#36530;&#36991;&#22522;&#20110;BERT&#30340;&#25991;&#26412;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2310.16992</link><description>&lt;p&gt;
&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#33021;&#22815;&#34987;&#22810;&#22909;&#22320;&#35782;&#21035;&#20986;&#26469;&#65292;&#33021;&#22815;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26469;&#36991;&#20813;&#35782;&#21035;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
How well can machine-generated texts be identified and can language models be trained to avoid identification?. (arXiv:2310.16992v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#36827;&#34892;&#20102;&#35782;&#21035;&#65292;&#21457;&#29616;&#22312;&#36739;&#39640;&#28201;&#24230;&#20540;&#19979;&#65292;&#27973;&#23618;&#23398;&#20064;&#31639;&#27861;&#30340;&#26816;&#27979;&#20934;&#30830;&#29575;&#36739;&#20302;&#65292;&#32780;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#20998;&#31867;&#22120;&#30340;&#20934;&#30830;&#29575;&#39640;&#12290;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#25913;&#36827;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#25104;&#21151;&#36530;&#36991;&#22522;&#20110;BERT&#30340;&#25991;&#26412;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;GPT-3&#12289;GPT-NeoX&#21644;OPT&#31561;&#29983;&#25104;&#39044;&#35757;&#32451;&#36716;&#25442;&#27169;&#22411;&#30340;&#23835;&#36215;&#65292;&#21306;&#20998;&#20154;&#31867;&#29983;&#25104;&#30340;&#25991;&#26412;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#21464;&#24471;&#24456;&#37325;&#35201;&#12290;&#25105;&#20204;&#23545;&#20116;&#20010;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#31934;&#32454;&#35843;&#25972;&#65292;&#29983;&#25104;&#20102;&#21512;&#25104;&#25512;&#25991;&#65292;&#21457;&#29616;&#27973;&#23618;&#23398;&#20064;&#20998;&#31867;&#31639;&#27861;&#65288;&#22914;&#26420;&#32032;&#36125;&#21494;&#26031;&#65289;&#30340;&#26816;&#27979;&#20934;&#30830;&#29575;&#22312;0.6&#21040;0.8&#20043;&#38388;&#12290;&#24403;&#22312;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#20351;&#29992;&#36739;&#39640;&#30340;&#28201;&#24230;&#20540;&#26102;&#65292;&#27973;&#23618;&#23398;&#20064;&#20998;&#31867;&#22120;&#19982;&#22522;&#20110;&#20154;&#31867;&#30340;&#26816;&#27979;&#26377;&#25152;&#19981;&#21516;&#65292;&#23548;&#33268;&#36739;&#20302;&#30340;&#26816;&#27979;&#29575;&#12290;&#20154;&#31867;&#26356;&#27880;&#37325;&#35821;&#35328;&#30340;&#21487;&#25509;&#21463;&#24615;&#65292;&#22312;&#36739;&#20302;&#30340;&#28201;&#24230;&#20540;&#19979;&#65292;&#35821;&#35328;&#30340;&#21487;&#25509;&#21463;&#24615;&#24448;&#24448;&#36739;&#39640;&#12290;&#30456;&#21453;&#65292;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#20998;&#31867;&#22120;&#30340;&#20934;&#30830;&#29575;&#22312;0.9&#21450;&#20197;&#19978;&#12290;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#25913;&#36827;&#25105;&#20204;&#30340;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#25104;&#21151;&#36530;&#36991;&#22522;&#20110;BERT&#30340;&#20998;&#31867;&#22120;&#65292;&#20854;&#26816;&#27979;&#20934;&#30830;&#29575;&#20026;0.15&#25110;&#26356;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rise of generative pre-trained transformer models such as GPT-3, GPT-NeoX, or OPT, distinguishing human-generated texts from machine-generated ones has become important. We refined five separate language models to generate synthetic tweets, uncovering that shallow learning classification algorithms, like Naive Bayes, achieve detection accuracy between 0.6 and 0.8.  Shallow learning classifiers differ from human-based detection, especially when using higher temperature values during text generation, resulting in a lower detection rate. Humans prioritize linguistic acceptability, which tends to be higher at lower temperature values. In contrast, transformer-based classifiers have an accuracy of 0.9 and above. We found that using a reinforcement learning approach to refine our generative models can successfully evade BERT-based classifiers with a detection accuracy of 0.15 or less.
&lt;/p&gt;</description></item><item><title>STEER&#26159;&#19968;&#20010;&#29992;&#20110;&#35821;&#38899;&#21161;&#25163;&#30340;&#35821;&#20041;&#36716;&#21521;&#25193;&#23637;&#35782;&#21035;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#21551;&#21457;&#24335;&#35268;&#21017;&#36827;&#34892;&#36716;&#21521;&#24847;&#22270;&#39044;&#27979;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.16990</link><description>&lt;p&gt;
STEER: &#35821;&#20041;&#36716;&#21521;&#25193;&#23637;&#35782;&#21035;&#29992;&#20110;&#35821;&#38899;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
STEER: Semantic Turn Extension-Expansion Recognition for Voice Assistants. (arXiv:2310.16990v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16990
&lt;/p&gt;
&lt;p&gt;
STEER&#26159;&#19968;&#20010;&#29992;&#20110;&#35821;&#38899;&#21161;&#25163;&#30340;&#35821;&#20041;&#36716;&#21521;&#25193;&#23637;&#35782;&#21035;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#21551;&#21457;&#24335;&#35268;&#21017;&#36827;&#34892;&#36716;&#21521;&#24847;&#22270;&#39044;&#27979;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35821;&#38899;&#21161;&#25163;&#31995;&#32479;&#30340;&#32972;&#26223;&#19979;&#65292;&#36716;&#21521;&#26159;&#25351;&#29992;&#25143;&#21457;&#20986;&#21518;&#32493;&#21629;&#20196;&#65292;&#35797;&#22270;&#24341;&#23548;&#25110;&#28548;&#28165;&#20043;&#21069;&#30340;&#25351;&#20196;&#30340;&#29616;&#35937;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;STEER&#65292;&#19968;&#20010;&#36716;&#21521;&#26816;&#27979;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#21518;&#32493;&#21629;&#20196;&#26159;&#21542;&#26159;&#29992;&#25143;&#20225;&#22270;&#36716;&#21521;&#20043;&#21069;&#25351;&#20196;&#30340;&#23581;&#35797;&#12290;&#30001;&#20110;&#20919;&#21551;&#21160;&#38382;&#39064;&#65292;&#26500;&#24314;&#29992;&#20110;&#36716;&#21521;&#26696;&#20363;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#21551;&#21457;&#24335;&#35268;&#21017;&#26469;&#37319;&#26679;&#36873;&#25321;&#21152;&#20837;&#20351;&#29992;&#25968;&#25454;&#65292;&#36817;&#20284;&#27491;&#36127;&#26679;&#26412;&#32780;&#26080;&#38656;&#20219;&#20309;&#26631;&#27880;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;&#35782;&#21035;&#36716;&#21521;&#24847;&#22270;&#30340;&#33391;&#22909;&#24615;&#33021;&#65292;&#22312;&#25105;&#20204;&#37319;&#26679;&#30340;&#25968;&#25454;&#19978;&#36229;&#36807;95%&#30340;&#20934;&#30830;&#29575;&#12290;&#27492;&#22806;&#65292;STEER&#32467;&#21512;&#25105;&#20204;&#30340;&#37319;&#26679;&#31574;&#30053;&#65292;&#22312;&#20154;&#24037;&#35780;&#20272;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#65292;&#26377;&#25928;&#22320;&#19982;&#30495;&#23454;&#30340;&#36716;&#21521;&#22330;&#26223;&#30456;&#21305;&#37197;&#12290;&#38500;&#20102;&#20165;&#20381;&#36182;&#29992;&#25143;&#30340;&#36716;&#24405;&#20316;&#20026;&#36755;&#20837;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;STEER+&#65292;&#36825;&#26159;&#27169;&#22411;&#30340;&#22686;&#24378;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the context of a voice assistant system, steering refers to the phenomenon in which a user issues a follow-up command attempting to direct or clarify a previous turn. We propose STEER, a steering detection model that predicts whether a follow-up turn is a user's attempt to steer the previous command. Constructing a training dataset for steering use cases poses challenges due to the cold-start problem. To overcome this, we developed heuristic rules to sample opt-in usage data, approximating positive and negative samples without any annotation. Our experimental results show promising performance in identifying steering intent, with over 95% accuracy on our sampled data. Moreover, STEER, in conjunction with our sampling strategy, aligns effectively with real-world steering scenarios, as evidenced by its strong zero-shot performance on a human-graded evaluation set. In addition to relying solely on user transcripts as input, we introduce STEER+, an enhanced version of the model. STEER+ 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21033;&#29992;&#20154;&#29289;&#20114;&#21160;&#22270;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#25216;&#26415;&#26469;&#25506;&#32034;&#24403;&#20195;&#25991;&#21270;&#23545;&#25991;&#23398;&#20316;&#21697;&#20013;&#31038;&#20250;&#32467;&#26500;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.16968</link><description>&lt;p&gt;
&#20351;&#29992;&#20154;&#29289;&#20114;&#21160;&#22270;&#26469;&#29702;&#35299;&#24403;&#20195;&#25991;&#23398;&#20316;&#21697;&#20013;&#30340;&#31038;&#20250;&#32467;&#26500;&#8212;&#8212;&#23545;&#26377;&#24433;&#21709;&#21147;&#30340;&#23391;&#21152;&#25289;&#20316;&#23478;&#30340;&#21322;&#20010;&#19990;&#32426;&#30340;&#24180;&#20195;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Understanding Social Structures from Contemporary Literary Fiction using Character Interaction Graph -- Half Century Chronology of Influential Bengali Writers. (arXiv:2310.16968v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16968
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21033;&#29992;&#20154;&#29289;&#20114;&#21160;&#22270;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#25216;&#26415;&#26469;&#25506;&#32034;&#24403;&#20195;&#25991;&#21270;&#23545;&#25991;&#23398;&#20316;&#21697;&#20013;&#31038;&#20250;&#32467;&#26500;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20250;&#32467;&#26500;&#21644;&#29616;&#23454;&#19990;&#30028;&#30340;&#20107;&#20214;&#32463;&#24120;&#24433;&#21709;&#24403;&#20195;&#25991;&#23398;&#20316;&#21697;&#12290;&#29616;&#26377;&#30340;&#25991;&#23398;&#20998;&#26512;&#30740;&#31350;&#36890;&#36807;&#23545;&#25925;&#20107;&#36827;&#34892;&#25163;&#21160;&#30340;&#25209;&#21028;&#24615;&#20998;&#26512;&#26469;&#35299;&#37322;&#36825;&#20123;&#29616;&#23454;&#19990;&#30028;&#30340;&#29616;&#35937;&#12290;&#20256;&#32479;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#26041;&#27861;&#65292;&#21253;&#25324;&#24773;&#24863;&#20998;&#26512;&#12289;&#25925;&#20107;&#25688;&#35201;&#21644;&#20027;&#39064;&#24314;&#27169;&#65292;&#22312;&#20998;&#26512;&#21644;&#35782;&#21035;&#34394;&#26500;&#20316;&#21697;&#20013;&#30340;&#30456;&#20284;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#37325;&#35201;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#34394;&#26500;&#20316;&#21697;&#20013;&#22797;&#26434;&#30340;&#20154;&#29289;&#20114;&#21160;&#21160;&#24577;&#38656;&#35201;&#26356;&#21152;&#32454;&#33268;&#20837;&#24494;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#32467;&#21512;&#20102;&#21487;&#35270;&#21270;&#25216;&#26415;&#12290;&#20154;&#29289;&#20114;&#21160;&#22270;&#65288;&#25110;&#32593;&#32476;&#65289;&#25104;&#20026;&#20102;&#20174;&#34394;&#26500;&#39046;&#22495;&#20013;&#25552;&#21462;&#20449;&#24687;&#21644;&#21487;&#35270;&#21270;&#30340;&#39640;&#24230;&#21512;&#36866;&#30340;&#25163;&#27573;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#20855;&#26377;NLP&#29305;&#24449;&#30340;&#20154;&#29289;&#20114;&#21160;&#22270;&#26469;&#25506;&#32034;&#26377;&#20851;&#24403;&#20195;&#25991;&#21270;&#23545;&#25991;&#23398;&#20316;&#21697;&#39046;&#22495;&#30340;&#24433;&#21709;&#30340;&#22810;&#26679;&#24615;&#31038;&#20250;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#28041;&#21450;&#26500;&#24314;...
&lt;/p&gt;
&lt;p&gt;
Social structures and real-world incidents often influence contemporary literary fiction. Existing research in literary fiction analysis explains these real-world phenomena through the manual critical analysis of stories. Conventional Natural Language Processing (NLP) methodologies, including sentiment analysis, narrative summarization, and topic modeling, have demonstrated substantial efficacy in analyzing and identifying similarities within fictional works. However, the intricate dynamics of character interactions within fiction necessitate a more nuanced approach that incorporates visualization techniques. Character interaction graphs (or networks) emerge as a highly suitable means for visualization and information retrieval from the realm of fiction. Therefore, we leverage character interaction graphs with NLP-derived features to explore a diverse spectrum of societal inquiries about contemporary culture's impact on the landscape of literary fiction. Our study involves constructing
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#31070;&#32463;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#36890;&#36807;&#32452;&#21512;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#21644;&#29305;&#27530;&#30340;&#25991;&#26412;&#25209;&#35780;&#23478;&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;&#26469;&#25351;&#23548;&#29983;&#25104;&#36807;&#31243;&#12290;&#26041;&#27861;&#19981;&#38656;&#35201;&#23545;&#27169;&#22411;&#26550;&#26500;&#25110;&#35757;&#32451;&#36807;&#31243;&#36827;&#34892;&#25913;&#21160;&#65292;&#21487;&#19982;&#20219;&#20309;&#22522;&#20110;&#21333;&#35789;&#27010;&#29575;&#36827;&#34892;&#27169;&#22411;&#21644;&#35299;&#30721;&#25805;&#20316;&#30340;&#27169;&#22411;&#32467;&#21512;&#20351;&#29992;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22522;&#20934;&#27979;&#35797;&#19978;&#20248;&#20110;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2310.16964</link><description>&lt;p&gt;
&#25209;&#35780;&#39537;&#21160;&#35299;&#30721;&#20197;&#20943;&#36731;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Critic-Driven Decoding for Mitigating Hallucinations in Data-to-text Generation. (arXiv:2310.16964v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#31070;&#32463;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#36890;&#36807;&#32452;&#21512;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#21644;&#29305;&#27530;&#30340;&#25991;&#26412;&#25209;&#35780;&#23478;&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;&#26469;&#25351;&#23548;&#29983;&#25104;&#36807;&#31243;&#12290;&#26041;&#27861;&#19981;&#38656;&#35201;&#23545;&#27169;&#22411;&#26550;&#26500;&#25110;&#35757;&#32451;&#36807;&#31243;&#36827;&#34892;&#25913;&#21160;&#65292;&#21487;&#19982;&#20219;&#20309;&#22522;&#20110;&#21333;&#35789;&#27010;&#29575;&#36827;&#34892;&#27169;&#22411;&#21644;&#35299;&#30721;&#25805;&#20316;&#30340;&#27169;&#22411;&#32467;&#21512;&#20351;&#29992;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22522;&#20934;&#27979;&#35797;&#19978;&#20248;&#20110;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#22312;&#36755;&#20837;&#20013;&#26080;&#27861;&#24471;&#21040;&#23454;&#38469;&#25903;&#25345;&#30340;&#24187;&#35273;&#26159;&#31070;&#32463;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;&#20013;&#24050;&#30693;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#35768;&#22810;&#26041;&#27861;&#34987;&#25552;&#20986;&#26469;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;&#20462;&#25913;&#27169;&#22411;&#26550;&#26500;&#25110;&#25910;&#38598;&#39069;&#22806;&#30340;&#25968;&#25454;&#65292;&#22240;&#27492;&#19981;&#33021;&#36731;&#26131;&#22320;&#24212;&#29992;&#21040;&#29616;&#26377;&#27169;&#22411;&#20013;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#36890;&#36807;&#32452;&#21512;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#30340;&#27010;&#29575;&#36755;&#20986;&#19982;&#29305;&#27530;&#30340;&#8220;&#25991;&#26412;&#25209;&#35780;&#23478;&#8221;&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;&#26469;&#20943;&#36731;&#24187;&#35273;&#30340;&#26032;&#26041;&#27861;&#65292;&#21518;&#32773;&#36890;&#36807;&#35780;&#20272;&#36755;&#20837;&#25968;&#25454;&#19982;&#21040;&#30446;&#21069;&#20026;&#27490;&#29983;&#25104;&#30340;&#25991;&#26412;&#20043;&#38388;&#30340;&#21305;&#37197;&#26469;&#24341;&#23548;&#29983;&#25104;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#23545;&#22522;&#30784;LM&#30340;&#26550;&#26500;&#25110;&#35757;&#32451;&#36807;&#31243;&#36827;&#34892;&#20219;&#20309;&#25913;&#21160;&#65292;&#22240;&#27492;&#21487;&#20197;&#19982;&#20219;&#20309;&#22522;&#20110;&#21333;&#35789;&#27010;&#29575;&#36827;&#34892;&#27169;&#22411;&#21644;&#35299;&#30721;&#25805;&#20316;&#30340;&#27169;&#22411;&#32467;&#21512;&#20351;&#29992;&#12290;&#25209;&#35780;&#23478;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20351;&#29992;&#22522;&#30784;LM&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#21512;&#25104;&#30340;&#36127;&#38754;&#20363;&#23376;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;WebNLG&#21644;OpenDialKG&#22522;&#20934;&#19978;&#36739;&#22522;&#32447;&#26377;&#25152;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hallucination of text ungrounded in the input is a well-known problem in neural data-to-text generation. Many methods have been proposed to mitigate it, but they typically require altering model architecture or collecting additional data, and thus cannot be easily applied to an existing model. In this paper, we explore a new way to mitigate hallucinations by combining the probabilistic output of a generator language model (LM) with the output of a special "text critic" classifier, which guides the generation by assessing the match between the input data and the text generated so far. Our method does not need any changes to the underlying LM's architecture or training procedure and can thus be combined with any model and decoding operating on word probabilities. The critic does not need any additional training data, using the base LM's training data and synthetic negative examples. Our experimental results show that our method improves over the baseline on the WebNLG and OpenDialKG benc
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#33976;&#39311;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#26041;&#27861;&#65292;&#20351;&#29992;AI&#21453;&#39304;&#25968;&#25454;&#36827;&#34892;&#20248;&#21270;&#65292;&#22312;&#32842;&#22825;&#20219;&#21153;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#24847;&#22270;&#23545;&#40784;&#30340;&#25928;&#26524;&#65292;&#35813;&#26041;&#27861;&#21482;&#38656;&#35201;&#20960;&#20010;&#23567;&#26102;&#30340;&#35757;&#32451;&#26102;&#38388;&#19988;&#26080;&#38656;&#20154;&#24037;&#27880;&#37322;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;7B&#21442;&#25968;&#27169;&#22411;&#19978;&#36229;&#36807;&#20102;&#29616;&#26377;&#26368;&#22909;&#30340;&#24320;&#25918;&#35775;&#38382;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.16944</link><description>&lt;p&gt;
Zephyr: &#30452;&#25509;&#33976;&#39311;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Zephyr: Direct Distillation of LM Alignment. (arXiv:2310.16944v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16944
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#33976;&#39311;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#26041;&#27861;&#65292;&#20351;&#29992;AI&#21453;&#39304;&#25968;&#25454;&#36827;&#34892;&#20248;&#21270;&#65292;&#22312;&#32842;&#22825;&#20219;&#21153;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#24847;&#22270;&#23545;&#40784;&#30340;&#25928;&#26524;&#65292;&#35813;&#26041;&#27861;&#21482;&#38656;&#35201;&#20960;&#20010;&#23567;&#26102;&#30340;&#35757;&#32451;&#26102;&#38388;&#19988;&#26080;&#38656;&#20154;&#24037;&#27880;&#37322;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;7B&#21442;&#25968;&#27169;&#22411;&#19978;&#36229;&#36807;&#20102;&#29616;&#26377;&#26368;&#22909;&#30340;&#24320;&#25918;&#35775;&#38382;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#29983;&#25104;&#19968;&#20010;&#19982;&#29992;&#25143;&#24847;&#22270;&#23545;&#40784;&#30340;&#36739;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#36739;&#22823;&#30340;&#27169;&#22411;&#24212;&#29992;&#33976;&#39311;&#30340;&#30417;&#30563;&#24494;&#35843;&#33021;&#26174;&#33879;&#25552;&#39640;&#20219;&#21153;&#20934;&#30830;&#24615;&#65307;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#27809;&#26377;&#23545;&#40784;&#65292;&#21363;&#23427;&#20204;&#19981;&#33021;&#24456;&#22909;&#22320;&#21709;&#24212;&#33258;&#28982;&#25552;&#31034;&#12290;&#20026;&#20102;&#33976;&#39311;&#36825;&#20010;&#23646;&#24615;&#65292;&#25105;&#20204;&#23581;&#35797;&#20351;&#29992;&#26469;&#33258;AI&#21453;&#39304;&#65288;AIF&#65289;&#30340;&#20559;&#22909;&#25968;&#25454;&#12290;&#20174;&#19968;&#20010;&#30001;&#25945;&#24072;&#27169;&#22411;&#25490;&#21517;&#30340;&#36755;&#20986;&#25968;&#25454;&#38598;&#24320;&#22987;&#65292;&#25105;&#20204;&#24212;&#29992;&#33976;&#39311;&#30340;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;dDPO&#65289;&#26469;&#23398;&#20064;&#19968;&#20010;&#20855;&#26377;&#26174;&#33879;&#25913;&#36827;&#24847;&#22270;&#23545;&#40784;&#30340;&#32842;&#22825;&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#21482;&#38656;&#35201;&#20960;&#20010;&#23567;&#26102;&#30340;&#35757;&#32451;&#26102;&#38388;&#65292;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#26080;&#38656;&#20219;&#20309;&#39069;&#22806;&#30340;&#37319;&#26679;&#12290;&#26368;&#32456;&#32467;&#26524;Zephyr-7B&#22312;7B&#21442;&#25968;&#27169;&#22411;&#30340;&#32842;&#22825;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#26080;&#38656;&#20154;&#24037;&#27880;&#37322;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;MT-Bench&#19978;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;Zephyr-7B&#36229;&#36234;&#20102;&#26368;&#22909;&#30340;&#24320;&#25918;&#35775;&#38382;RLHF&#27169;&#22411;Llama2-Chat-70B&#12290;&#31995;&#32479;&#30340;&#20195;&#30721;&#12289;&#27169;&#22411;&#12289;&#25968;&#25454;&#21644;&#25945;&#31243;&#37117;&#21487;&#20197;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
We aim to produce a smaller language model that is aligned to user intent. Previous research has shown that applying distilled supervised fine-tuning (dSFT) on larger models significantly improves task accuracy; however, these models are unaligned, i.e. they do not respond well to natural prompts. To distill this property, we experiment with the use of preference data from AI Feedback (AIF). Starting from a dataset of outputs ranked by a teacher model, we apply distilled direct preference optimization (dDPO) to learn a chat model with significantly improved intent alignment. The approach requires only a few hours of training without any additional sampling during fine-tuning. The final result, Zephyr-7B, sets the state-of-the-art on chat benchmarks for 7B parameter models, and requires no human annotation. In particular, results on MT-Bench show that Zephyr-7B surpasses Llama2-Chat-70B, the best open-access RLHF-based model. Code, models, data, and tutorials for the system are availabl
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#25552;&#39640;&#32534;&#31243;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#38382;&#39064;&#65292;&#24182;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#39564;&#35777;&#12290;&#35813;&#30740;&#31350;&#34920;&#26126;&#65292;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#22312;&#32534;&#31243;&#35821;&#35328;&#39046;&#22495;&#20855;&#26377;&#28508;&#21147;&#65292;&#21487;&#20197;&#24110;&#21161;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#29992;&#25143;&#21463;&#30410;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.16937</link><description>&lt;p&gt;
&#36328;&#22810;&#31181;&#32534;&#31243;&#35821;&#35328;&#30340;&#23398;&#20064;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Learning Transfers over Several Programming Languages. (arXiv:2310.16937v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16937
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#25552;&#39640;&#32534;&#31243;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#38382;&#39064;&#65292;&#24182;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#39564;&#35777;&#12290;&#35813;&#30740;&#31350;&#34920;&#26126;&#65292;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#22312;&#32534;&#31243;&#35821;&#35328;&#39046;&#22495;&#20855;&#26377;&#28508;&#21147;&#65292;&#21487;&#20197;&#24110;&#21161;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#29992;&#25143;&#21463;&#30410;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#25552;&#39640;&#39640;&#36164;&#28304;&#32534;&#31243;&#35821;&#35328;&#24320;&#21457;&#32773;&#29983;&#20135;&#21147;&#26041;&#38754;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#36825;&#20123;&#27169;&#22411;&#20351;&#29992;&#20004;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#65306;&#22823;&#37327;&#30340;&#26080;&#26631;&#31614;&#20195;&#30721;&#26679;&#26412;&#29992;&#20110;&#39044;&#35757;&#32451;&#65292;&#30456;&#23545;&#36739;&#23569;&#30340;&#24102;&#26631;&#31614;&#20195;&#30721;&#26679;&#26412;&#29992;&#20110;&#24494;&#35843;&#25110;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#32534;&#31243;&#35821;&#35328;&#26159;&#20302;&#36164;&#28304;&#30340;&#65292;&#32570;&#20047;&#22823;&#22810;&#25968;&#20219;&#21153;&#30340;&#24102;&#26631;&#31614;&#26679;&#26412;&#65292;&#29978;&#33267;&#32570;&#20047;&#26080;&#26631;&#31614;&#26679;&#26412;&#12290;&#22240;&#27492;&#65292;&#20302;&#36164;&#28304;&#35821;&#35328;&#65288;&#20363;&#22914;&#36951;&#30041;&#25110;&#26032;&#35821;&#35328;&#65289;&#30340;&#29992;&#25143;&#26080;&#27861;&#20139;&#21463;&#21040;LLM&#30340;&#22909;&#22788;&#12290;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#20351;&#29992;&#28304;&#35821;&#35328;&#30340;&#25968;&#25454;&#26469;&#25552;&#39640;&#27169;&#22411;&#22312;&#30446;&#26631;&#35821;&#35328;&#19978;&#30340;&#24615;&#33021;&#12290;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#39046;&#22495;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#22312;&#32534;&#31243;&#35821;&#35328;&#39046;&#22495;&#21364;&#21463;&#21040;&#20102;&#24456;&#23569;&#20851;&#27880;&#12290;&#26412;&#25991;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;LLM&#21644;11&#21040;41&#31181;&#32534;&#31243;&#35821;&#35328;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25506;&#35752;&#20102;&#20197;&#19979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently become remarkably good at improving developer productivity for high-resource programming languages. These models use two kinds of data: large amounts of unlabeled code samples for pretraining and relatively smaller amounts of labeled code samples for fine-tuning or in-context learning. Unfortunately, many programming languages are low-resource, lacking labeled samples for most tasks and often even lacking unlabeled samples. Therefore, users of low-resource languages (e.g., legacy or new languages) miss out on the benefits of LLMs. Cross-lingual transfer learning uses data from a source language to improve model performance on a target language. It has been well-studied for natural languages, but has received little attention for programming languages. This paper reports extensive experiments on four tasks using a transformer-based LLM and 11 to 41 programming languages to explore the following questions. First, how well cross-lingual transfer 
&lt;/p&gt;</description></item><item><title>CL-MASR&#26159;&#19968;&#20010;&#29992;&#20110;&#30740;&#31350;&#22810;&#35821;&#38899;ASR&#30340;&#25345;&#32493;&#23398;&#20064;&#22522;&#20934;&#65292;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#22810;&#26679;&#21270;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#35299;&#20915;&#20102;&#23398;&#20064;&#26032;&#35821;&#35328;&#26102;&#36951;&#24536;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.16931</link><description>&lt;p&gt;
CL-MASR&#65306;&#19968;&#20010;&#29992;&#20110;&#22810;&#35821;&#38899;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#30340;&#25345;&#32493;&#23398;&#20064;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
CL-MASR: A Continual Learning Benchmark for Multilingual ASR. (arXiv:2310.16931v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16931
&lt;/p&gt;
&lt;p&gt;
CL-MASR&#26159;&#19968;&#20010;&#29992;&#20110;&#30740;&#31350;&#22810;&#35821;&#38899;ASR&#30340;&#25345;&#32493;&#23398;&#20064;&#22522;&#20934;&#65292;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#22810;&#26679;&#21270;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#35299;&#20915;&#20102;&#23398;&#20064;&#26032;&#35821;&#35328;&#26102;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22810;&#35821;&#38899;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#65292;&#22914;Whisper&#65292;&#20351;&#24471;&#33021;&#22815;&#20351;&#29992;&#21333;&#20010;&#27169;&#22411;&#36716;&#24405;&#22810;&#31181;&#35821;&#35328;&#30340;&#38899;&#39057;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;ASR&#27169;&#22411;&#36890;&#24120;&#22312;&#21333;&#20010;&#35821;&#35328;&#25110;&#22810;&#20219;&#21153;&#35774;&#32622;&#19979;&#36827;&#34892;&#35780;&#20272;&#65292;&#24573;&#35270;&#20102;&#25345;&#32493;&#23398;&#20064;&#26032;&#35821;&#35328;&#30340;&#25361;&#25112;&#12290;&#20851;&#20110;&#22914;&#20309;&#28155;&#21152;&#26032;&#35821;&#35328;&#32780;&#19981;&#20002;&#22833;&#20808;&#21069;&#25968;&#25454;&#20013;&#26377;&#20215;&#20540;&#20449;&#24687;&#30340;&#30740;&#31350;&#19981;&#36275;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#25345;&#32493;&#23398;&#20064;&#22522;&#20934;&#20027;&#35201;&#38598;&#20013;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#20219;&#21153;&#19978;&#65292;&#23545;&#20110;&#22810;&#35821;&#38899;ASR&#30340;&#25345;&#32493;&#23398;&#20064;&#36824;&#26410;&#28145;&#20837;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;CL-MASR&#65292;&#19968;&#20010;&#19987;&#20026;&#22312;&#25345;&#32493;&#23398;&#20064;&#29615;&#22659;&#20013;&#30740;&#31350;&#22810;&#35821;&#38899;ASR&#32780;&#35774;&#35745;&#30340;&#22522;&#20934;&#12290;CL-MASR&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#22810;&#26679;&#21270;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#22522;&#20110;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;ASR&#27169;&#22411;&#23454;&#29616;&#65292;&#24182;&#25552;&#20379;&#24120;&#35265;&#30340;&#25351;&#26631;&#26469;&#35780;&#20272;&#23398;&#20064;&#26032;&#35821;&#35328;&#30340;&#25928;&#26524;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern multilingual automatic speech recognition (ASR) systems like Whisper have made it possible to transcribe audio in multiple languages with a single model. However, current state-of-the-art ASR models are typically evaluated on individual languages or in a multi-task setting, overlooking the challenge of continually learning new languages. There is insufficient research on how to add new languages without losing valuable information from previous data. Furthermore, existing continual learning benchmarks focus mostly on vision and language tasks, leaving continual learning for multilingual ASR largely unexplored. To bridge this gap, we propose CL-MASR, a benchmark designed for studying multilingual ASR in a continual learning setting. CL-MASR provides a diverse set of continual learning methods implemented on top of large-scale pretrained ASR models, along with common metrics to assess the effectiveness of learning new languages while addressing the issue of catastrophic forgetting
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20154;&#31867;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#39640;&#39118;&#38505;&#21307;&#30103;&#29615;&#22659;&#20013;&#20351;&#29992;&#36136;&#37327;&#35780;&#20272;&#21453;&#39304;&#26469;&#24110;&#21161;&#21307;&#29983;&#36776;&#21035;&#20309;&#26102;&#20381;&#36182;&#26426;&#22120;&#32763;&#35793;&#36755;&#20986;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36136;&#37327;&#35780;&#20272;&#33021;&#25913;&#21892;&#23545;&#26426;&#22120;&#32763;&#35793;&#30340;&#36866;&#24403;&#20381;&#36182;&#65292;&#20294;&#36861;&#28335;&#32763;&#35793;&#33021;&#24110;&#21161;&#21307;&#29983;&#21457;&#29616;&#26356;&#20005;&#37325;&#30340;&#20020;&#24202;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2310.16924</link><description>&lt;p&gt;
&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#20020;&#24202;&#25439;&#23475;&#34987;&#21307;&#29983;&#21457;&#29616;&#65306;&#36136;&#37327;&#35780;&#20272;&#24110;&#21161;&#20943;&#23569;&#20381;&#36182;&#65292;&#36861;&#28335;&#32763;&#35793;&#35782;&#21035;&#20851;&#38190;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;
Physician Detection of Clinical Harm in Machine Translation: Quality Estimation Aids in Reliance and Backtranslation Identifies Critical Errors. (arXiv:2310.16924v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16924
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20154;&#31867;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#39640;&#39118;&#38505;&#21307;&#30103;&#29615;&#22659;&#20013;&#20351;&#29992;&#36136;&#37327;&#35780;&#20272;&#21453;&#39304;&#26469;&#24110;&#21161;&#21307;&#29983;&#36776;&#21035;&#20309;&#26102;&#20381;&#36182;&#26426;&#22120;&#32763;&#35793;&#36755;&#20986;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36136;&#37327;&#35780;&#20272;&#33021;&#25913;&#21892;&#23545;&#26426;&#22120;&#32763;&#35793;&#30340;&#36866;&#24403;&#20381;&#36182;&#65292;&#20294;&#36861;&#28335;&#32763;&#35793;&#33021;&#24110;&#21161;&#21307;&#29983;&#21457;&#29616;&#26356;&#20005;&#37325;&#30340;&#20020;&#24202;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#32763;&#35793;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#29992;&#25143;&#32570;&#20047;&#25351;&#23548;&#26469;&#20570;&#20986;&#26159;&#21542;&#20381;&#36182;&#20854;&#36755;&#20986;&#30340;&#26126;&#26234;&#20915;&#31574;&#12290;&#36136;&#37327;&#35780;&#20272;&#30740;&#31350;&#30340;&#36827;&#23637;&#25552;&#20379;&#20102;&#33258;&#21160;&#35780;&#20272;&#26426;&#22120;&#32763;&#35793;&#36136;&#37327;&#30340;&#25216;&#26415;&#65292;&#20294;&#36825;&#20123;&#25216;&#26415;&#20027;&#35201;&#22312;&#29305;&#23450;&#30340;&#20351;&#29992;&#29615;&#22659;&#20043;&#22806;&#36890;&#36807;&#19982;&#20154;&#24037;&#21028;&#26029;&#30340;&#27604;&#36739;&#36827;&#34892;&#20307;&#22806;&#35780;&#20272;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;&#39640;&#39118;&#38505;&#21307;&#23398;&#29615;&#22659;&#20013;&#27169;&#25311;&#20915;&#31574;&#36807;&#31243;&#30340;&#20154;&#31867;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#36136;&#37327;&#35780;&#20272;&#21453;&#39304;&#30340;&#23454;&#22320;&#24212;&#29992;&#12290;&#25105;&#20204;&#20351;&#29992;&#24613;&#35786;&#31185;&#20986;&#38498;&#25351;&#31034;&#65292;&#30740;&#31350;&#20102;&#22522;&#20110;&#36136;&#37327;&#35780;&#20272;&#21644;&#36861;&#28335;&#32763;&#35793;&#30340;&#24178;&#39044;&#25514;&#26045;&#22914;&#20309;&#24110;&#21161;&#21307;&#29983;&#20915;&#23450;&#26159;&#21542;&#21521;&#24739;&#32773;&#23637;&#31034;&#26426;&#22120;&#32763;&#35793;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36136;&#37327;&#35780;&#20272;&#25913;&#21892;&#20102;&#23545;&#26426;&#22120;&#32763;&#35793;&#30340;&#36866;&#24403;&#20381;&#36182;&#65292;&#20294;&#36861;&#28335;&#32763;&#35793;&#26377;&#21161;&#20110;&#21307;&#29983;&#21457;&#29616;&#36136;&#37327;&#35780;&#20272;&#24120;&#24120;&#24573;&#30053;&#30340;&#26356;&#20005;&#37325;&#30340;&#20020;&#24202;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
A major challenge in the practical use of Machine Translation (MT) is that users lack guidance to make informed decisions about when to rely on outputs. Progress in quality estimation research provides techniques to automatically assess MT quality, but these techniques have primarily been evaluated in vitro by comparison against human judgments outside of a specific context of use. This paper evaluates quality estimation feedback in vivo with a human study simulating decision-making in high-stakes medical settings. Using Emergency Department discharge instructions, we study how interventions based on quality estimation versus backtranslation assist physicians in deciding whether to show MT outputs to a patient. We find that quality estimation improves appropriate reliance on MT, but backtranslation helps physicians detect more clinically harmful errors that QE alone often misses.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;Transformer&#26550;&#26500;&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#22797;&#26434;NLP&#20219;&#21153;&#23558;&#20854;&#20998;&#35299;&#20026;&#31616;&#21333;&#30340;&#23376;&#20219;&#21153;&#65292;&#24182;&#21033;&#29992;&#22810;&#20010;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#22686;&#24378;&#20219;&#21153;&#30340;&#25511;&#21046;&#33021;&#21147;&#21644;&#24615;&#33021;&#12290;&#22312;&#20943;&#23569;&#24615;&#21035;&#20559;&#35265;&#30340;&#20219;&#21153;&#20013;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#20248;&#20110;&#21333;&#19968;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.16897</link><description>&lt;p&gt;
Divide et Impera: Multi-Transformer Architectures for Complex NLP-Tasks.
&lt;/p&gt;
&lt;p&gt;
Divide et Impera: Multi-Transformer Architectures for Complex NLP-Tasks. (arXiv:2310.16897v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16897
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;Transformer&#26550;&#26500;&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#22797;&#26434;NLP&#20219;&#21153;&#23558;&#20854;&#20998;&#35299;&#20026;&#31616;&#21333;&#30340;&#23376;&#20219;&#21153;&#65292;&#24182;&#21033;&#29992;&#22810;&#20010;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#22686;&#24378;&#20219;&#21153;&#30340;&#25511;&#21046;&#33021;&#21147;&#21644;&#24615;&#33021;&#12290;&#22312;&#20943;&#23569;&#24615;&#21035;&#20559;&#35265;&#30340;&#20219;&#21153;&#20013;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#20248;&#20110;&#21333;&#19968;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;Transformer&#27169;&#22411;&#33021;&#21147;&#30340;&#22686;&#24378;&#65292;&#35299;&#20915;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#30340;&#36947;&#36335;&#24050;&#32463;&#34987;&#38138;&#24320;&#12290;&#25903;&#25345;&#29305;&#23450;&#24212;&#29992;&#38656;&#27714;&#30340;&#20851;&#38190;&#22312;&#20110;&#33021;&#22815;&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#38024;&#23545;&#22797;&#26434;&#20219;&#21153;&#32534;&#21046;&#36866;&#29992;&#30340;&#24494;&#35843;&#25968;&#25454;&#38598;&#26159;&#32321;&#29712;&#30340;&#65292;&#24182;&#19988;&#20250;&#23548;&#33268;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#38480;&#21046;&#20102;&#23545;Transformer&#36755;&#20986;&#30340;&#25511;&#21046;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#22797;&#26434;&#20219;&#21153;&#20998;&#35299;&#20026;&#26356;&#31616;&#21333;&#30340;&#23376;&#20219;&#21153;&#12290;&#22810;&#20010;Transformer&#27169;&#22411;&#20998;&#21035;&#23545;&#27599;&#20010;&#23376;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#25490;&#25104;&#19968;&#34892;&#20197;&#23436;&#25104;&#22797;&#26434;&#20219;&#21153;&#12290;&#36825;&#31616;&#21270;&#20102;&#24494;&#35843;&#25968;&#25454;&#38598;&#30340;&#32534;&#21046;&#65292;&#24182;&#22686;&#21152;&#20102;&#25972;&#20307;&#30340;&#21487;&#25511;&#24615;&#12290;&#20197;&#20943;&#23569;&#24615;&#21035;&#20559;&#35265;&#20316;&#20026;&#22797;&#26434;&#20219;&#21153;&#30340;&#20363;&#23376;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#34920;&#26126;&#23427;&#27604;&#20351;&#29992;&#21333;&#19968;&#27169;&#22411;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing capabilities of transformer models pave the way for solving increasingly complex NLP tasks. A key to supporting application-specific requirements is the ability to fine-tune. However, compiling a fine-tuning dataset tailored to complex tasks is tedious and results in large datasets, limiting the ability to control transformer output. We present an approach in which complex tasks are divided into simpler subtasks. Multiple transformer models are fine-tuned to one subtask each, and lined up to accomplish the complex task. This simplifies the compilation of fine-tuning datasets and increases overall controllability. Using the example of reducing gender bias as a complex task, we demonstrate our approach and show that it performs better than using a single model.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DEFT&#30340;&#25968;&#25454;&#39640;&#25928;&#24494;&#35843;&#26694;&#26550;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#26680;&#24515;&#38598;&#36873;&#25321;&#26469;&#26368;&#23567;&#21270;&#24494;&#35843;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;DEFT&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#19978;&#19982;&#29616;&#26377;&#27169;&#22411;&#30456;&#24403;&#65292;&#24182;&#19988;&#20165;&#20351;&#29992;&#20102;70%&#30340;&#25968;&#25454;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.16776</link><description>&lt;p&gt;
DEFT&#65306;&#36890;&#36807;&#26080;&#30417;&#30563;&#26680;&#24515;&#38598;&#36873;&#25321;&#23454;&#29616;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
DEFT: Data Efficient Fine-Tuning for Large Language Models via Unsupervised Core-Set Selection. (arXiv:2310.16776v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16776
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DEFT&#30340;&#25968;&#25454;&#39640;&#25928;&#24494;&#35843;&#26694;&#26550;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#26680;&#24515;&#38598;&#36873;&#25321;&#26469;&#26368;&#23567;&#21270;&#24494;&#35843;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;DEFT&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#19978;&#19982;&#29616;&#26377;&#27169;&#22411;&#30456;&#24403;&#65292;&#24182;&#19988;&#20165;&#20351;&#29992;&#20102;70%&#30340;&#25968;&#25454;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#36827;&#23637;&#20351;&#24471;&#35768;&#22810;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#21487;&#20197;&#20351;&#29992;&#65307;&#28982;&#32780;&#65292;&#19968;&#20010;&#20173;&#28982;&#23384;&#22312;&#30340;&#38382;&#39064;&#26159;&#24494;&#35843;PLMs&#20197;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#31350;&#31455;&#38656;&#35201;&#22810;&#23569;&#25968;&#25454;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DEFT&#65292;&#19968;&#31181;&#25968;&#25454;&#39640;&#25928;&#30340;&#24494;&#35843;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#26080;&#30417;&#30563;&#30340;&#26680;&#24515;&#38598;&#36873;&#25321;&#26469;&#26368;&#23567;&#21270;&#24494;&#35843;PLMs&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#12290;&#25105;&#20204;&#22312;&#25991;&#26412;&#32534;&#36753;LM&#30340;&#32972;&#26223;&#19979;&#23637;&#31034;&#20102;DEFT&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19982;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#32534;&#36753;&#27169;&#22411;CoEDIT&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#32467;&#26524;&#34920;&#26126;&#65292;DEFT&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#19978;&#19982;CoEDIT&#19968;&#26679;&#65292;&#32780;&#20351;&#29992;&#30340;&#25968;&#25454;&#37327;&#35201;&#23569;&#32422;70%&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances have led to the availability of many pre-trained language models (PLMs); however, a question that remains is how much data is truly needed to fine-tune PLMs for downstream tasks? In this work, we introduce DEFT, a data-efficient fine-tuning framework that leverages unsupervised core-set selection to minimize the amount of data needed to fine-tune PLMs for downstream tasks. We demonstrate the efficacy of our DEFT framework in the context of text-editing LMs, and compare to the state-of-the art text-editing model, CoEDIT. Our quantitative and qualitative results demonstrate that DEFT models are just as accurate as CoEDIT while being finetuned on ~70% less data.
&lt;/p&gt;</description></item><item><title>SkyMath&#26159;&#19968;&#20010;13&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#27604;&#36739;&#24494;&#35843;&#65292;&#23427;&#22312;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#31168;&#65292;&#36229;&#36807;&#20102;&#21516;&#31561;&#35268;&#27169;&#30340;&#25152;&#26377;&#24320;&#28304;&#27169;&#22411;&#65292;&#24182;&#21462;&#24471;&#20102;&#26032;&#30340;SOTA&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.16713</link><description>&lt;p&gt;
SkyMath&#65306;&#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
SkyMath: Technical Report. (arXiv:2310.16713v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16713
&lt;/p&gt;
&lt;p&gt;
SkyMath&#26159;&#19968;&#20010;13&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#27604;&#36739;&#24494;&#35843;&#65292;&#23427;&#22312;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#31168;&#65292;&#36229;&#36807;&#20102;&#21516;&#31561;&#35268;&#27169;&#30340;&#25152;&#26377;&#24320;&#28304;&#27169;&#22411;&#65292;&#24182;&#21462;&#24471;&#20102;&#26032;&#30340;SOTA&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#35299;&#20915;&#38382;&#39064;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#21253;&#25324;&#25968;&#23398;&#25512;&#29702;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;130&#20159;&#21442;&#25968;&#30340;&#25968;&#23398;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;SkyMath&#12290;&#36890;&#36807;&#33258;&#27604;&#36739;&#24494;&#35843;&#65292;&#25105;&#20204;&#26174;&#33879;&#25552;&#21319;&#20102;Skywork-13B-Base&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;&#22312;GSM8K&#19978;&#65292;SkyMath&#36229;&#36807;&#20102;&#25152;&#26377;&#24050;&#30693;&#30340;&#24320;&#28304;&#27169;&#22411;&#65292;&#24182;&#24314;&#31435;&#20102;&#26032;&#30340;SOTA&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown great potential to solve varieties of natural language processing (NLP) tasks, including mathematical reasoning. In this work, we present SkyMath, a large language model for mathematics with 13 billion parameters. By applying self-compare fine-tuning, we have enhanced mathematical reasoning abilities of Skywork-13B-Base remarkably. On GSM8K, SkyMath outperforms all known open-source models of similar size and has established a new SOTA performance.
&lt;/p&gt;</description></item><item><title>DDCoT&#26159;&#19968;&#31181;&#32844;&#36131;&#20998;&#26126;&#30340;&#24605;&#36335;&#38142;&#21050;&#28608;&#26041;&#27861;&#65292;&#36890;&#36807;&#36127;&#31354;&#38388;&#21050;&#28608;&#20445;&#25345;&#25209;&#21028;&#24577;&#24230;&#65292;&#24182;&#22312;&#22810;&#27169;&#24577;&#25512;&#29702;&#20013;&#32467;&#21512;&#20102;&#20851;&#38190;&#27934;&#35265;&#8220;&#20445;&#25345;&#25209;&#21028;&#24615;&#24605;&#32771;&#8221;&#21644;&#8220;&#35753;&#27599;&#20010;&#20154;&#21457;&#25381;&#33258;&#24049;&#30340;&#20316;&#29992;&#8221;&#12290;</title><link>http://arxiv.org/abs/2310.16436</link><description>&lt;p&gt;
DDCoT: &#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32844;&#36131;&#20998;&#26126;&#30340;&#24605;&#36335;&#38142;&#21050;&#28608;
&lt;/p&gt;
&lt;p&gt;
DDCoT: Duty-Distinct Chain-of-Thought Prompting for Multimodal Reasoning in Language Models. (arXiv:2310.16436v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16436
&lt;/p&gt;
&lt;p&gt;
DDCoT&#26159;&#19968;&#31181;&#32844;&#36131;&#20998;&#26126;&#30340;&#24605;&#36335;&#38142;&#21050;&#28608;&#26041;&#27861;&#65292;&#36890;&#36807;&#36127;&#31354;&#38388;&#21050;&#28608;&#20445;&#25345;&#25209;&#21028;&#24577;&#24230;&#65292;&#24182;&#22312;&#22810;&#27169;&#24577;&#25512;&#29702;&#20013;&#32467;&#21512;&#20102;&#20851;&#38190;&#27934;&#35265;&#8220;&#20445;&#25345;&#25209;&#21028;&#24615;&#24605;&#32771;&#8221;&#21644;&#8220;&#35753;&#27599;&#20010;&#20154;&#21457;&#25381;&#33258;&#24049;&#30340;&#20316;&#29992;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#19968;&#20010;&#38271;&#36828;&#30446;&#26631;&#26159;&#23454;&#29616;&#31867;&#20284;&#20154;&#31867;&#30340;&#22797;&#26434;&#22810;&#27169;&#24577;&#25512;&#29702;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35821;&#35328;&#27169;&#24577;&#19978;&#21033;&#29992;&#24605;&#36335;&#38142;&#65288;CoT&#65289;&#27169;&#25311;&#20154;&#31867;&#24605;&#32500;&#65292;&#22312;&#22810;&#27493;&#25512;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#36827;&#23637;&#36716;&#31227;&#21040;&#22810;&#27169;&#24577;&#29615;&#22659;&#20013;&#24341;&#20837;&#20102;&#26356;&#39640;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#23545;&#21171;&#21160;&#23494;&#38598;&#22411;&#27880;&#37322;&#30340;&#38656;&#27714;&#20197;&#21450;&#28789;&#27963;&#24615;&#12289;&#19968;&#33324;&#21270;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#22312;&#22810;&#27169;&#24577;&#29615;&#22659;&#20013;&#21796;&#36215;CoT&#25512;&#29702;&#65292;&#26412;&#30740;&#31350;&#39318;&#20808;&#23545;&#22810;&#27169;&#24577;&#29615;&#22659;&#20013;&#30340;&#36825;&#20123;&#25361;&#25112;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#20851;&#38190;&#27934;&#35265;&#65306;&#8220;&#20445;&#25345;&#25209;&#21028;&#24615;&#24605;&#32771;&#8221;&#21644;&#8220;&#35753;&#27599;&#20010;&#20154;&#21457;&#25381;&#33258;&#24049;&#30340;&#20316;&#29992;&#8221;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;DDCoT&#21050;&#28608;&#26041;&#27861;&#65292;&#36890;&#36807;&#36127;&#31354;&#38388;&#21050;&#28608;&#20445;&#25345;&#25209;&#21028;&#24577;&#24230;&#65292;&#24182;&#36890;&#36807;&#39318;&#20808;&#21010;&#20998;&#36131;&#20219;&#26126;&#30830;&#22269;&#22810;&#27169;&#24577;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
A long-standing goal of AI systems is to perform complex multimodal reasoning like humans. Recently, large language models (LLMs) have made remarkable strides in such multi-step reasoning on the language modality solely by leveraging the chain of thought (CoT) to mimic human thinking. However, the transfer of these advancements to multimodal contexts introduces heightened challenges, including but not limited to the impractical need for labor-intensive annotation and the limitations in terms of flexibility, generalizability, and explainability. To evoke CoT reasoning in multimodality, this work first conducts an in-depth analysis of these challenges posed by multimodality and presents two key insights: "keeping critical thinking" and "letting everyone do their jobs" in multimodal CoT reasoning. Furthermore, this study proposes a novel DDCoT prompting that maintains a critical attitude through negative-space prompting and incorporates multimodality into reasoning by first dividing the r
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#20999;&#32447;&#26680;&#30340;&#29702;&#35770;&#26041;&#27861;&#65292;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#29305;&#24449;&#25552;&#21462;&#26426;&#21046;&#12290;&#30740;&#31350;&#21457;&#29616;&#22312;&#26799;&#24230;&#19979;&#38477;&#36807;&#31243;&#20013;&#27169;&#22411;&#22914;&#20309;&#21033;&#29992;&#32479;&#35745;&#29305;&#24449;&#65292;&#24182;&#25581;&#31034;&#20102;&#28608;&#27963;&#20989;&#25968;&#36873;&#25321;&#23545;&#29305;&#24449;&#25552;&#21462;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.16350</link><description>&lt;p&gt;
&#25581;&#31034;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#29305;&#24449;&#25552;&#21462;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Unraveling Feature Extraction Mechanisms in Neural Networks. (arXiv:2310.16350v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16350
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#20999;&#32447;&#26680;&#30340;&#29702;&#35770;&#26041;&#27861;&#65292;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#29305;&#24449;&#25552;&#21462;&#26426;&#21046;&#12290;&#30740;&#31350;&#21457;&#29616;&#22312;&#26799;&#24230;&#19979;&#38477;&#36807;&#31243;&#20013;&#27169;&#22411;&#22914;&#20309;&#21033;&#29992;&#32479;&#35745;&#29305;&#24449;&#65292;&#24182;&#25581;&#31034;&#20102;&#28608;&#27963;&#20989;&#25968;&#36873;&#25321;&#23545;&#29305;&#24449;&#25552;&#21462;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#25429;&#25417;&#31934;&#30830;&#30693;&#35782;&#26041;&#38754;&#30340;&#22522;&#26412;&#26426;&#21046;&#19968;&#30452;&#26159;&#25345;&#32493;&#30740;&#31350;&#30340;&#23545;&#35937;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#20999;&#32447;&#26680;&#65288;NTK&#65289;&#30340;&#29702;&#35770;&#26041;&#27861;&#65292;&#29992;&#20110;&#30740;&#31350;&#36825;&#20123;&#26426;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#32771;&#34385;&#26080;&#38480;&#32593;&#32476;&#23485;&#24230;&#65292;&#25105;&#20204;&#29468;&#27979;&#30446;&#26631;&#27169;&#22411;&#30340;&#23398;&#20064;&#21160;&#24577;&#21487;&#33021;&#30452;&#35266;&#22320;&#25581;&#31034;&#20854;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#33719;&#21462;&#30340;&#29305;&#24449;&#65292;&#36827;&#19968;&#27493;&#21152;&#28145;&#25105;&#20204;&#23545;&#20854;&#20869;&#37096;&#26426;&#21046;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#20960;&#20010;&#22522;&#26412;&#27169;&#22411;&#65292;&#24182;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#26799;&#24230;&#19979;&#38477;&#36807;&#31243;&#20013;&#22914;&#20309;&#21033;&#29992;&#32479;&#35745;&#29305;&#24449;&#12289;&#20197;&#21450;&#23427;&#20204;&#22914;&#20309;&#34701;&#20837;&#26368;&#32456;&#30340;&#20915;&#31574;&#20013;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#28608;&#27963;&#20989;&#25968;&#30340;&#36873;&#25321;&#20250;&#24433;&#21709;&#29305;&#24449;&#25552;&#21462;&#12290;&#20363;&#22914;&#65292;&#20351;&#29992;ReLU&#28608;&#27963;&#20989;&#25968;&#21487;&#33021;&#20250;&#24341;&#20837;&#29305;&#24449;&#20559;&#24046;&#65292;&#36825;&#21487;&#20197;&#35299;&#37322;&#20026;&#20160;&#20040;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#36873;&#25321;&#20102;&#26367;&#20195;&#20989;&#25968;&#26469;&#20195;&#26367;&#23427;&#12290;
&lt;/p&gt;
&lt;p&gt;
The underlying mechanism of neural networks in capturing precise knowledge has been the subject of consistent research efforts. In this work, we propose a theoretical approach based on Neural Tangent Kernels (NTKs) to investigate such mechanisms. Specifically, considering the infinite network width, we hypothesize the learning dynamics of target models may intuitively unravel the features they acquire from training data, deepening our insights into their internal mechanisms. We apply our approach to several fundamental models and reveal how these models leverage statistical features during gradient descent and how they are integrated into final decisions. We also discovered that the choice of activation function can affect feature extraction. For instance, the use of the \textit{ReLU} activation function could potentially introduce a bias in features, providing a plausible explanation for its replacement with alternative functions in recent pre-trained language models. Additionally, we
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#23398;&#26415;&#21644;&#24037;&#19994;&#39046;&#22495;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;LLMs&#30340;&#30693;&#35782;&#32534;&#36753;&#38382;&#39064;&#65292;&#24378;&#35843;&#20102;&#38656;&#35201;&#24320;&#21457;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#25216;&#26415;&#26469;&#26356;&#26032;&#39044;&#35757;&#32451;LLMs&#20197;&#32435;&#20837;&#26032;&#30693;&#35782;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16218</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Knowledge Editing for Large Language Models: A Survey. (arXiv:2310.16218v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16218
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#23398;&#26415;&#21644;&#24037;&#19994;&#39046;&#22495;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;LLMs&#30340;&#30693;&#35782;&#32534;&#36753;&#38382;&#39064;&#65292;&#24378;&#35843;&#20102;&#38656;&#35201;&#24320;&#21457;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#25216;&#26415;&#26469;&#26356;&#26032;&#39044;&#35757;&#32451;LLMs&#20197;&#32435;&#20837;&#26032;&#30693;&#35782;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36817;&#26399;&#20197;&#20854;&#20986;&#33394;&#30340;&#29702;&#35299;&#12289;&#20998;&#26512;&#21644;&#29983;&#25104;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#26681;&#25454;&#20854;&#24191;&#21338;&#30340;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#25913;&#21464;&#20102;&#23398;&#26415;&#21644;&#24037;&#19994;&#39046;&#22495;&#30340;&#26684;&#23616;&#12290;&#28982;&#32780;&#65292;LLMs&#30340;&#19968;&#20010;&#20027;&#35201;&#32570;&#28857;&#26159;&#23427;&#20204;&#22312;&#39044;&#35757;&#32451;&#26102;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#65292;&#22240;&#20026;&#20854;&#21442;&#25968;&#25968;&#37327;&#21069;&#25152;&#26410;&#26377;&#12290;&#24403;&#38656;&#35201;&#39057;&#32321;&#24341;&#20837;&#26032;&#30693;&#35782;&#21040;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#26102;&#65292;&#36825;&#20010;&#32570;&#28857;&#26356;&#21152;&#26174;&#33879;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#25216;&#26415;&#26469;&#26356;&#26032;&#39044;&#35757;&#32451;LLMs&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#20256;&#32479;&#26041;&#27861;&#26159;&#36890;&#36807;&#30452;&#25509;&#24494;&#35843;&#23558;&#26032;&#30693;&#35782;&#32534;&#30721;&#21040;&#39044;&#35757;&#32451;LLMs&#20013;&#12290;&#28982;&#32780;&#65292;&#31616;&#21333;&#22320;&#37325;&#26032;&#35757;&#32451;LLMs&#21487;&#33021;&#35745;&#31639;&#36164;&#28304;&#23494;&#38598;&#65292;&#24182;&#19988;&#23384;&#22312;&#23558;&#19982;&#27169;&#22411;&#26356;&#26032;&#26080;&#20851;&#30340;&#26377;&#20215;&#20540;&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#36864;&#21270;&#30340;&#39118;&#38505;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#30693;&#35782;&#30340;&#27169;&#22411;&#32534;&#36753;(KME)&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#26088;&#22312;&#31934;&#30830;&#20462;&#25913;LLMs&#20197;&#32435;&#20837;&#29305;&#23450;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently transformed both the academic and industrial landscapes due to their remarkable capacity to understand, analyze, and generate texts based on their vast knowledge and reasoning ability. Nevertheless, one major drawback of LLMs is their substantial computational cost for pre-training due to their unprecedented amounts of parameters. The disadvantage is exacerbated when new knowledge frequently needs to be introduced into the pre-trained model. Therefore, it is imperative to develop effective and efficient techniques to update pre-trained LLMs. Traditional methods encode new knowledge in pre-trained LLMs through direct fine-tuning. However, naively re-training LLMs can be computationally intensive and risks degenerating valuable pre-trained knowledge irrelevant to the update in the model. Recently, Knowledge-based Model Editing (KME) has attracted increasing attention, which aims to precisely modify the LLMs to incorporate specific knowledge, wit
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;COPF&#26041;&#27861;&#65292;&#25105;&#20204;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#26159;&#20351;&#29992;&#26368;&#20248;&#31574;&#30053;&#25311;&#21512;&#21644;&#20989;&#25968;&#27491;&#21017;&#21270;&#26469;&#25345;&#32493;&#23398;&#20064;&#21644;&#36866;&#24212;&#20154;&#31867;&#20559;&#22909;&#30340;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.15694</link><description>&lt;p&gt;
COPF: &#36890;&#36807;&#26368;&#20248;&#31574;&#30053;&#25311;&#21512;&#23454;&#29616;&#25345;&#32493;&#23398;&#20064;&#20154;&#31867;&#20559;&#22909;
&lt;/p&gt;
&lt;p&gt;
COPF: Continual Learning Human Preference through Optimal Policy Fitting. (arXiv:2310.15694v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15694
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;COPF&#26041;&#27861;&#65292;&#25105;&#20204;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#26159;&#20351;&#29992;&#26368;&#20248;&#31574;&#30053;&#25311;&#21512;&#21644;&#20989;&#25968;&#27491;&#21017;&#21270;&#26469;&#25345;&#32493;&#23398;&#20064;&#21644;&#36866;&#24212;&#20154;&#31867;&#20559;&#22909;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#30340;&#25216;&#26415;&#26159;&#25913;&#21892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#20197;&#31526;&#21512;&#20154;&#31867;&#20559;&#22909;&#30340;&#24120;&#29992;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#22522;&#20110;RLHF&#30340;LM&#22312;&#24341;&#20837;&#26032;&#30340;&#26597;&#35810;&#25110;&#21453;&#39304;&#26102;&#38656;&#35201;&#23436;&#20840;&#37325;&#26032;&#35757;&#32451;&#65292;&#36825;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#20154;&#31867;&#20559;&#22909;&#22312;&#19981;&#21516;&#39046;&#22495;&#25110;&#20219;&#21153;&#20043;&#38388;&#21487;&#33021;&#20250;&#26377;&#25152;&#21464;&#21270;&#12290;&#30001;&#20110;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#20197;&#21450;&#19982;&#25968;&#25454;&#38544;&#31169;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#37325;&#26032;&#35757;&#32451;LM&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;&#19979;&#23384;&#22312;&#23454;&#38469;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#25345;&#32493;&#26368;&#20248;&#31574;&#30053;&#25311;&#21512;&#65288;COPF&#65289;&#65292;&#20854;&#20013;&#25105;&#20204;&#20351;&#29992;&#33945;&#29305;&#21345;&#32599;&#27861;&#20272;&#35745;&#19968;&#31995;&#21015;&#26368;&#20248;&#31574;&#30053;&#65292;&#28982;&#21518;&#36890;&#36807;&#20989;&#25968;&#27491;&#21017;&#21270;&#19981;&#26029;&#25311;&#21512;&#31574;&#30053;&#24207;&#21015;&#12290;COPF&#21253;&#21547;&#19968;&#20010;&#21333;&#19968;&#30340;&#23398;&#20064;&#38454;&#27573;&#65292;&#19981;&#38656;&#35201;&#22797;&#26434;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The technique of Reinforcement Learning from Human Feedback (RLHF) is a commonly employed method to improve pre-trained Language Models (LM), enhancing their ability to conform to human preferences. Nevertheless, the current RLHF-based LMs necessitate full retraining each time novel queries or feedback are introduced, which becomes a challenging task because human preferences can vary between different domains or tasks. Retraining LMs poses practical difficulties in many real-world situations due to the significant time and computational resources required, along with concerns related to data privacy. To address this limitation, we propose a new method called Continual Optimal Policy Fitting (COPF), in which we estimate a series of optimal policies using the Monte Carlo method, and then continually fit the policy sequence with the function regularization. COPF involves a single learning phase and doesn't necessitate complex reinforcement learning. Importantly, it shares the capability 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;ChatGPT&#22312;&#22235;&#31181;&#35821;&#35328;&#19978;&#30340;&#24418;&#24577;&#23398;&#33021;&#21147;&#36827;&#34892;&#20005;&#26684;&#20998;&#26512;&#65292;&#21457;&#29616;&#23427;&#22312;&#33521;&#35821;&#19978;&#30340;&#34920;&#29616;&#29305;&#21035;&#19981;&#29702;&#24819;&#65292;&#36828;&#36828;&#36798;&#19981;&#21040;&#19987;&#38376;&#26500;&#24314;&#31995;&#32479;&#30340;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2310.15113</link><description>&lt;p&gt;
ChatGPT&#20044;&#26684;&#30340;&#32570;&#38519;&#32479;&#35745;&#65306;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24418;&#24577;&#23398;&#33021;&#21147;&#36827;&#34892;&#30340;&#36328;&#35821;&#35328;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Counting the Bugs in ChatGPT's Wugs: A Multilingual Investigation into the Morphological Capabilities of a Large Language Model. (arXiv:2310.15113v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15113
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;ChatGPT&#22312;&#22235;&#31181;&#35821;&#35328;&#19978;&#30340;&#24418;&#24577;&#23398;&#33021;&#21147;&#36827;&#34892;&#20005;&#26684;&#20998;&#26512;&#65292;&#21457;&#29616;&#23427;&#22312;&#33521;&#35821;&#19978;&#30340;&#34920;&#29616;&#29305;&#21035;&#19981;&#29702;&#24819;&#65292;&#36828;&#36828;&#36798;&#19981;&#21040;&#19987;&#38376;&#26500;&#24314;&#31995;&#32479;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36817;&#26399;&#22312;&#35821;&#35328;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#36827;&#23637;&#65292;&#19982;&#20154;&#31867;&#35821;&#35328;&#33021;&#21147;&#30456;&#25552;&#24182;&#35770;&#12290;&#28982;&#32780;&#65292;&#23545;&#26368;&#26032;&#19968;&#20195;LLMs&#30340;&#35821;&#35328;&#33021;&#21147;&#36827;&#34892;&#31995;&#32479;&#35843;&#26597;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#65292;&#32780;&#19988;&#36825;&#20123;&#30740;&#31350;&#24573;&#35270;&#20102;&#20154;&#31867;&#27867;&#21270;&#30340;&#26174;&#33879;&#33021;&#21147;&#65292;&#21482;&#20851;&#27880;&#33521;&#35821;&#65292;&#24182;&#19988;&#24573;&#35270;&#20102;&#35821;&#35328;&#30340;&#20854;&#20182;&#26680;&#24515;&#33021;&#21147;&#65292;&#22914;&#24418;&#24577;&#23398;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#22235;&#31181;&#35821;&#35328;&#65288;&#20855;&#20307;&#26469;&#35828;&#26159;&#33521;&#35821;&#65292;&#24503;&#35821;&#65292;&#27888;&#31859;&#23572;&#35821;&#21644;&#22303;&#32819;&#20854;&#35821;&#65289;&#19978;&#36827;&#34892;&#31532;&#19968;&#27425;&#20005;&#26684;&#30340;ChatGPT&#24418;&#24577;&#23398;&#33021;&#21147;&#20998;&#26512;&#26469;&#22635;&#34917;&#36825;&#20123;&#31354;&#30333;&#12290;&#25105;&#20204;&#20351;&#29992;Berko&#65288;1958&#65289;&#30340;&#20044;&#26684;&#27979;&#35797;&#29256;&#26412;&#23545;ChatGPT&#36827;&#34892;&#23454;&#39564;&#65292;&#20351;&#29992;&#20102;&#38024;&#23545;&#22235;&#31181;&#35821;&#35328;&#30340;&#26032;&#39062;&#12289;&#26080;&#27745;&#26579;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#21457;&#29616;ChatGPT&#30340;&#34920;&#29616;&#36828;&#36828;&#19981;&#21450;&#19987;&#38376;&#26500;&#24314;&#30340;&#31995;&#32479;&#65292;&#23588;&#20854;&#26159;&#22312;&#33521;&#35821;&#26041;&#38754;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#25353;&#24418;&#24577;&#23398;&#33021;&#21147;&#34913;&#37327;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently reached an impressive level of linguistic capability, prompting comparisons with human language skills. However, there have been relatively few systematic inquiries into the linguistic capabilities of the latest generation of LLMs, and those studies that do exist (i) ignore the remarkable ability of humans to generalize, (ii) focus only on English, and (iii) investigate syntax or semantics and overlook other capabilities that lie at the heart of human language, like morphology. Here, we close these gaps by conducting the first rigorous analysis of the morphological capabilities of ChatGPT in four typologically varied languages (specifically, English, German, Tamil, and Turkish). We apply a version of Berko's (1958) wug test to ChatGPT, using novel, uncontaminated datasets for the four examined languages. We find that ChatGPT massively underperforms purpose-built systems, particularly in English. Overall, our results -- through the lens of morp
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#29992;&#25143;&#29983;&#25104;&#25968;&#25454;&#38598;&#20013;&#31579;&#36873;&#24182;&#27880;&#37322;&#20102;10K&#20010;YouTube&#19978;&#30340;&#26377;&#36259;&#22810;&#27169;&#24577;&#35270;&#39057;&#65292;&#20511;&#21161;GPT-3.5&#39564;&#35777;&#20102;&#35821;&#35328;&#21644;&#35270;&#35273;&#20803;&#32032;&#23545;&#24189;&#40664;&#30340;&#36129;&#29486;&#12290;&#27492;&#22806;&#65292;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#38646;-shot&#35270;&#39057;&#21040;&#25991;&#26412;&#25552;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#35270;&#39057;&#24189;&#40664;&#30340;&#29702;&#35299;&#12290;&#36825;&#20010;&#30740;&#31350;&#22635;&#34917;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#23545;&#22810;&#39046;&#22495;&#22810;&#27169;&#24577;&#24189;&#40664;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2310.14159</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#22066;&#31505;YouTube&#30701;&#35270;&#39057;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Language Models Laugh at YouTube Short-form Videos?. (arXiv:2310.14159v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14159
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#29992;&#25143;&#29983;&#25104;&#25968;&#25454;&#38598;&#20013;&#31579;&#36873;&#24182;&#27880;&#37322;&#20102;10K&#20010;YouTube&#19978;&#30340;&#26377;&#36259;&#22810;&#27169;&#24577;&#35270;&#39057;&#65292;&#20511;&#21161;GPT-3.5&#39564;&#35777;&#20102;&#35821;&#35328;&#21644;&#35270;&#35273;&#20803;&#32032;&#23545;&#24189;&#40664;&#30340;&#36129;&#29486;&#12290;&#27492;&#22806;&#65292;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#38646;-shot&#35270;&#39057;&#21040;&#25991;&#26412;&#25552;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#35270;&#39057;&#24189;&#40664;&#30340;&#29702;&#35299;&#12290;&#36825;&#20010;&#30740;&#31350;&#22635;&#34917;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#23545;&#22810;&#39046;&#22495;&#22810;&#27169;&#24577;&#24189;&#40664;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31038;&#20132;&#32593;&#32476;&#19978;&#30701;&#35270;&#39057;&#30340;&#27969;&#34892;&#65292;&#35201;&#27714;AI&#27169;&#22411;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#20123;&#35270;&#39057;&#20197;&#19982;&#20154;&#31867;&#36827;&#34892;&#26356;&#22909;&#30340;&#20132;&#27969;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#30340;&#35270;&#39057;&#24189;&#40664;&#25968;&#25454;&#38598;&#20027;&#35201;&#38024;&#23545;&#29305;&#23450;&#39046;&#22495;&#65292;&#22914;&#28436;&#35762;&#25110;&#24773;&#26223;&#21916;&#21095;&#65292;&#24182;&#19988;&#22823;&#22810;&#20851;&#27880;&#35821;&#35328;&#32447;&#32034;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#26469;&#33258;YouTube&#30340;10K&#20010;&#22810;&#27169;&#24577;&#26377;&#36259;&#35270;&#39057;&#30340;&#29992;&#25143;&#29983;&#25104;&#25968;&#25454;&#38598;&#65292;&#31216;&#20026;ExFunTube&#12290;&#20351;&#29992;&#22522;&#20110;GPT-3.5&#30340;&#35270;&#39057;&#36807;&#28388;&#27969;&#31243;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#35821;&#35328;&#21644;&#35270;&#35273;&#20803;&#32032;&#23545;&#24189;&#40664;&#30340;&#36129;&#29486;&#12290;&#22312;&#36807;&#28388;&#21518;&#65292;&#25105;&#20204;&#20026;&#27599;&#20010;&#35270;&#39057;&#30340;&#26377;&#36259;&#26102;&#21051;&#21152;&#19978;&#20102;&#26102;&#38388;&#25139;&#21644;&#25991;&#26412;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;ExFunTube&#22312;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#29420;&#29305;&#20043;&#22788;&#22312;&#20110;&#65292;&#25105;&#20204;&#30340;&#35270;&#39057;&#28085;&#30422;&#20102;&#21508;&#31181;&#31867;&#22411;&#24189;&#40664;&#30340;&#24191;&#27867;&#39046;&#22495;&#65292;&#38656;&#35201;&#23545;&#20869;&#23481;&#36827;&#34892;&#22810;&#27169;&#24577;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#38646;-shot&#35270;&#39057;&#21040;&#25991;&#26412;&#25552;&#31034;&#65292;&#20197;&#26368;&#22823;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#35270;&#39057;&#24189;&#40664;&#30340;&#29702;&#35299;&#12290;&#20351;&#29992;&#33258;&#21160;&#35780;&#20998;&#12289;&#21407;&#29702;&#36136;&#37327;&#23454;&#39564;&#20197;&#21450;&#20154;&#31867;&#35780;&#20215;&#26041;&#27861;&#36827;&#34892;&#19977;&#31181;&#19981;&#21516;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
As short-form funny videos on social networks are gaining popularity, it becomes demanding for AI models to understand them for better communication with humans. Unfortunately, previous video humor datasets target specific domains, such as speeches or sitcoms, and mostly focus on verbal cues. We curate a user-generated dataset of 10K multimodal funny videos from YouTube, called ExFunTube. Using a video filtering pipeline with GPT-3.5, we verify both verbal and visual elements contributing to humor. After filtering, we annotate each video with timestamps and text explanations for funny moments. Our ExFunTube is unique over existing datasets in that our videos cover a wide range of domains with various types of humor that necessitate a multimodal understanding of the content. Also, we develop a zero-shot video-to-text prompting to maximize video humor understanding of large language models (LLMs). With three different evaluation methods using automatic scores, rationale quality experimen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23450;&#21046;&#21442;&#32771;&#25991;&#29486;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22312;&#21516;&#26102;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#37325;&#36848;&#30495;&#23454;&#21442;&#32771;&#25991;&#29486;&#65292;&#23450;&#21046;&#21442;&#32771;&#25991;&#29486;&#21487;&#20197;&#22312;&#19981;&#21516;&#24310;&#36831;&#19979;&#35757;&#32451;SiMT&#27169;&#22411;&#65292;&#36991;&#20813;&#20102;&#24378;&#21046;&#39044;&#27979;&#24182;&#20445;&#25345;&#39640;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.13588</link><description>&lt;p&gt;
&#20351;&#29992;&#23450;&#21046;&#30340;&#21442;&#32771;&#25991;&#29486;&#30340;&#21516;&#26102;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Simultaneous Machine Translation with Tailored Reference. (arXiv:2310.13588v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23450;&#21046;&#21442;&#32771;&#25991;&#29486;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22312;&#21516;&#26102;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#37325;&#36848;&#30495;&#23454;&#21442;&#32771;&#25991;&#29486;&#65292;&#23450;&#21046;&#21442;&#32771;&#25991;&#29486;&#21487;&#20197;&#22312;&#19981;&#21516;&#24310;&#36831;&#19979;&#35757;&#32451;SiMT&#27169;&#22411;&#65292;&#36991;&#20813;&#20102;&#24378;&#21046;&#39044;&#27979;&#24182;&#20445;&#25345;&#39640;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#26102;&#26426;&#22120;&#32763;&#35793;&#65288;SiMT&#65289;&#22312;&#35835;&#21462;&#25972;&#20010;&#28304;&#21477;&#23376;&#30340;&#21516;&#26102;&#29983;&#25104;&#32763;&#35793;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;SiMT&#27169;&#22411;&#36890;&#24120;&#20351;&#29992;&#30456;&#21516;&#30340;&#21442;&#32771;&#25991;&#29486;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#24573;&#30053;&#20102;&#22312;&#19981;&#21516;&#24310;&#36831;&#19979;&#21487;&#29992;&#30340;&#28304;&#20449;&#24687;&#30340;&#25968;&#37327;&#19981;&#21516;&#12290;&#20351;&#29992;&#20302;&#24310;&#36831;&#19979;&#30340;&#30495;&#23454;&#21442;&#32771;&#25991;&#29486;&#21487;&#33021;&#24341;&#20837;&#24378;&#21046;&#39044;&#27979;&#65292;&#32780;&#20351;&#29992;&#19982;&#28304;&#35789;&#39034;&#24207;&#19968;&#33268;&#30340;&#21442;&#32771;&#25991;&#29486;&#22312;&#39640;&#24310;&#36831;&#19979;&#20250;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#22240;&#27492;&#65292;&#20851;&#38190;&#26159;&#20351;&#29992;&#36866;&#24403;&#30340;&#21442;&#32771;&#25991;&#29486;&#26469;&#35757;&#32451;SiMT&#27169;&#22411;&#65292;&#26082;&#36991;&#20813;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24378;&#21046;&#39044;&#27979;&#65292;&#21448;&#20445;&#25345;&#39640;&#36136;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#36848;&#30495;&#23454;&#21442;&#32771;&#25991;&#29486;&#65292;&#20026;&#19981;&#21516;&#24310;&#36831;&#36827;&#34892;&#35757;&#32451;&#30340;SiMT&#27169;&#22411;&#25552;&#20379;&#23450;&#21046;&#30340;&#21442;&#32771;&#25991;&#29486;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#30001;&#24378;&#21270;&#23398;&#20064;&#24341;&#21457;&#30340;&#25913;&#32534;&#22120;&#26469;&#20462;&#25913;&#30495;&#23454;&#21442;&#32771;&#25991;&#29486;&#65292;&#20197;&#24471;&#21040;&#23450;&#21046;&#30340;&#21442;&#32771;&#25991;&#29486;&#12290;SiMT&#27169;&#22411;&#26159;&#20351;&#29992;&#23450;&#21046;&#30340;&#21442;&#32771;&#25991;&#29486;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19982;&#23450;&#21046;&#22120;&#20849;&#21516;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simultaneous machine translation (SiMT) generates translation while reading the whole source sentence. However, existing SiMT models are typically trained using the same reference disregarding the varying amounts of available source information at different latency. Training the model with ground-truth at low latency may introduce forced anticipations, whereas utilizing reference consistent with the source word order at high latency results in performance degradation. Consequently, it is crucial to train the SiMT model with appropriate reference that avoids forced anticipations during training while maintaining high quality. In this paper, we propose a novel method that provides tailored reference for the SiMT models trained at different latency by rephrasing the ground-truth. Specifically, we introduce the tailor, induced by reinforcement learning, to modify ground-truth to the tailored reference. The SiMT model is trained with the tailored reference and jointly optimized with the tai
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#21307;&#23398;&#39046;&#22495;&#25991;&#26412;&#31616;&#21270;&#21487;&#35835;&#24615;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#19968;&#31181;&#26032;&#30340;&#38750;&#20856;&#22411;&#25439;&#22833;&#21644;&#19968;&#31181;&#20248;&#21270;&#31616;&#21333;&#24615;&#30340;&#37325;&#26032;&#25490;&#24207;&#35299;&#30721;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.11191</link><description>&lt;p&gt;
&#21307;&#23398;&#25991;&#26412;&#31616;&#21270;&#65306;&#36890;&#36807;&#38750;&#20856;&#22411;&#35757;&#32451;&#21644;&#37325;&#26032;&#25490;&#24207;&#30340;Beam Search&#35299;&#30721;&#20248;&#21270;&#21487;&#35835;&#24615;
&lt;/p&gt;
&lt;p&gt;
Medical Text Simplification: Optimizing for Readability with Unlikelihood Training and Reranked Beam Search Decoding. (arXiv:2310.11191v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11191
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#21307;&#23398;&#39046;&#22495;&#25991;&#26412;&#31616;&#21270;&#21487;&#35835;&#24615;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#19968;&#31181;&#26032;&#30340;&#38750;&#20856;&#22411;&#25439;&#22833;&#21644;&#19968;&#31181;&#20248;&#21270;&#31616;&#21333;&#24615;&#30340;&#37325;&#26032;&#25490;&#24207;&#35299;&#30721;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#31616;&#21270;&#20316;&#20026;&#20154;&#24037;&#26234;&#33021;&#22312;&#19987;&#19994;&#39046;&#22495;&#65288;&#22914;&#21307;&#23398;&#65289;&#20013;&#24357;&#21512;&#27807;&#36890;&#24046;&#36317;&#30340;&#36234;&#26469;&#36234;&#26377;&#29992;&#30340;&#24212;&#29992;&#65292;&#24050;&#36880;&#28176;&#23853;&#38706;&#22836;&#35282;&#12290;&#28982;&#32780;&#65292;&#21307;&#23398;&#31616;&#21270;&#26041;&#27861;&#26377;&#26102;&#20250;&#23548;&#33268;&#29983;&#25104;&#30340;&#25991;&#26412;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#19979;&#38477;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#21307;&#23398;&#39046;&#22495;&#25991;&#26412;&#31616;&#21270;&#21487;&#35835;&#24615;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#20856;&#22411;&#21507;&#20111;&#25439;&#22833;&#24178;&#22270;&#29255;&#21050;&#28608;&#29983;&#25104;&#26356;&#31616;&#21333;&#30340;&#26415;&#35821;&#65292;&#20197;&#21450;&#19968;&#31181;&#20248;&#21270;&#31616;&#21333;&#24615;&#30340;&#37325;&#26032;&#25490;&#24207;&#30340;Beam Search&#35299;&#30721;&#26041;&#27861;&#65292;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#21487;&#35835;&#24615;&#25351;&#26631;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#21457;&#29616;&#20026;&#25913;&#36827;&#21307;&#23398;&#39046;&#22495;&#30340;&#25991;&#26412;&#31616;&#21270;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text simplification has emerged as an increasingly useful application of AI for bridging the communication gap in specialized fields such as medicine, where the lexicon is often dominated by technical jargon and complex constructs. Despite notable progress, methods in medical simplification sometimes result in the generated text having lower quality and diversity. In this work, we explore ways to further improve the readability of text simplification in the medical domain. We propose (1) a new unlikelihood loss that encourages generation of simpler terms and (2) a reranked beam search decoding method that optimizes for simplicity, which achieve better performance on readability metrics on three datasets. This study's findings offer promising avenues for improving text simplification in the medical field.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;MathVista&#65292;&#36825;&#26159;&#19968;&#20010;&#35780;&#20272;&#35270;&#35273;&#22330;&#26223;&#20013;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#36890;&#36807;&#23545;12&#20010;&#33879;&#21517;&#30340;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#30340;&#23450;&#37327;&#35780;&#20272;&#65292;&#21457;&#29616;&#26368;&#22909;&#30340;GPT-4V&#27169;&#22411;&#30456;&#23545;&#20110;&#31532;&#20108;&#21517;&#30340;Bard&#27169;&#22411;&#22312;&#20934;&#30830;&#29575;&#19978;&#25552;&#21319;&#20102;15.1%&#12290;</title><link>http://arxiv.org/abs/2310.02255</link><description>&lt;p&gt;
MathVista: &#29992;GPT-4V&#12289;Bard&#21644;&#20854;&#20182;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#35780;&#20272;&#35270;&#35273;&#22330;&#26223;&#20013;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
MathVista: Evaluating Math Reasoning in Visual Contexts with GPT-4V, Bard, and Other Large Multimodal Models. (arXiv:2310.02255v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;MathVista&#65292;&#36825;&#26159;&#19968;&#20010;&#35780;&#20272;&#35270;&#35273;&#22330;&#26223;&#20013;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#36890;&#36807;&#23545;12&#20010;&#33879;&#21517;&#30340;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#30340;&#23450;&#37327;&#35780;&#20272;&#65292;&#21457;&#29616;&#26368;&#22909;&#30340;GPT-4V&#27169;&#22411;&#30456;&#23545;&#20110;&#31532;&#20108;&#21517;&#30340;Bard&#27169;&#22411;&#22312;&#20934;&#30830;&#29575;&#19978;&#25552;&#21319;&#20102;15.1%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#22312;&#35768;&#22810;&#20219;&#21153;&#21644;&#39046;&#22495;&#20013;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#35270;&#35273;&#29615;&#22659;&#20013;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#23578;&#26410;&#24471;&#21040;&#31995;&#32479;&#30740;&#31350;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MathVista&#65292;&#36825;&#26159;&#19968;&#20010;&#32508;&#21512;&#20102;&#19981;&#21516;&#25968;&#23398;&#21644;&#35270;&#35273;&#20219;&#21153;&#30340;&#25361;&#25112;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#23427;&#21253;&#21547;&#20102;6141&#20010;&#20363;&#23376;&#65292;&#20854;&#20013;&#26377;28&#20010;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#21644;3&#20010;&#26032;&#21019;&#24314;&#30340;&#25968;&#25454;&#38598;&#65288;&#21363;IQTest&#12289;FunctionQA&#21644;PaperQA&#65289;&#12290;&#23436;&#25104;&#36825;&#20123;&#20219;&#21153;&#38656;&#35201;&#31934;&#32454;&#30340;&#12289;&#28145;&#20837;&#30340;&#35270;&#35273;&#29702;&#35299;&#21644;&#32452;&#21512;&#25512;&#29702;&#65292;&#36825;&#20123;&#37117;&#26159;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#30784;&#27169;&#22411;&#25152;&#38754;&#20020;&#30340;&#22256;&#38590;&#12290;&#36890;&#36807;MathVista&#65292;&#25105;&#20204;&#23545;12&#20010;&#33879;&#21517;&#30340;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23450;&#37327;&#35780;&#20272;&#12290;&#34920;&#29616;&#26368;&#22909;&#30340;GPT-4V&#27169;&#22411;&#30340;&#25972;&#20307;&#20934;&#30830;&#29575;&#20026;49.9%&#65292;&#26126;&#26174;&#20248;&#20110;&#31532;&#20108;&#21517;&#30340;Bard&#27169;&#22411;&#65292;&#30456;&#24046;15.1%&#12290;&#25105;&#20204;&#30340;&#28145;&#20837;&#20998;&#26512;&#25581;&#31034;&#20102;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) and Large Multimodal Models (LMMs) exhibit impressive problem-solving skills in many tasks and domains, but their ability in mathematical reasoning in visual contexts has not been systematically studied. To bridge this gap, we present MathVista, a benchmark designed to combine challenges from diverse mathematical and visual tasks. It consists of 6,141 examples, derived from 28 existing multimodal datasets involving mathematics and 3 newly created datasets (i.e., IQTest, FunctionQA, and PaperQA). Completing these tasks requires fine-grained, deep visual understanding and compositional reasoning, which all state-of-the-art foundation models find challenging. With MathVista, we have conducted a comprehensive, quantitative evaluation of 12 prominent foundation models. The best-performing GPT-4V model achieves an overall accuracy of 49.9%, substantially outperforming Bard, the second-best performer, by 15.1%. Our in-depth analysis reveals that the superiority of
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#22810;&#35821;&#35328;&#25991;&#26412;&#21040;&#25991;&#26412;&#21464;&#25442;&#22120;&#27169;&#22411;&#19978;&#24494;&#35843;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#33021;&#22815;&#33258;&#21160;&#22312;&#22810;&#35821;&#35328;&#20013;&#24635;&#32467;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#26377;&#21161;&#20110;&#25552;&#39640;&#26410;&#26469;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#65292;&#19988;&#33021;&#22815;&#24212;&#29992;&#20110;&#19981;&#21516;&#26063;&#35028;&#32972;&#26223;&#30340;&#24739;&#32773;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2310.00100</link><description>&lt;p&gt;
&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#22810;&#35821;&#35328;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;--&#25688;&#35201;&#26159;&#20320;&#38656;&#35201;&#30340;&#19968;&#20999;&#65281;
&lt;/p&gt;
&lt;p&gt;
Multilingual Natural Language ProcessingModel for Radiology Reports -- The Summary is all you need!. (arXiv:2310.00100v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00100
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#22810;&#35821;&#35328;&#25991;&#26412;&#21040;&#25991;&#26412;&#21464;&#25442;&#22120;&#27169;&#22411;&#19978;&#24494;&#35843;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#33021;&#22815;&#33258;&#21160;&#22312;&#22810;&#35821;&#35328;&#20013;&#24635;&#32467;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#26377;&#21161;&#20110;&#25552;&#39640;&#26410;&#26469;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#65292;&#19988;&#33021;&#22815;&#24212;&#29992;&#20110;&#19981;&#21516;&#26063;&#35028;&#32972;&#26223;&#30340;&#24739;&#32773;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#21360;&#35937;&#37096;&#20998;&#24635;&#32467;&#20102;&#37325;&#35201;&#30340;&#25918;&#23556;&#23398;&#21457;&#29616;&#65292;&#24182;&#22312;&#21521;&#21307;&#29983;&#20256;&#36798;&#36825;&#20123;&#21457;&#29616;&#26102;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#25918;&#23556;&#31185;&#21307;&#29983;&#26469;&#35828;&#65292;&#20934;&#22791;&#36825;&#20123;&#25688;&#35201;&#26082;&#32791;&#26102;&#21448;&#23481;&#26131;&#20986;&#38169;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#29992;&#20110;&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#33021;&#22815;&#22312;&#22810;&#31181;&#35821;&#35328;&#20013;&#24635;&#32467;&#36825;&#20123;&#25253;&#21578;&#30340;&#27169;&#22411;&#12290;&#36825;&#26679;&#30340;&#27169;&#22411;&#21487;&#20197;&#26497;&#22823;&#22320;&#25913;&#36827;&#26410;&#26469;&#30340;&#30740;&#31350;&#21644;&#34701;&#21512;&#26469;&#33258;&#19981;&#21516;&#26063;&#35028;&#32972;&#26223;&#30340;&#24739;&#32773;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;&#22522;&#20110;&#22810;&#35821;&#35328;&#25991;&#26412;&#21040;&#25991;&#26412;&#21464;&#25442;&#22120;&#30340;&#27169;&#22411;&#19978;&#24494;&#35843;&#65292;&#33258;&#21160;&#21270;&#22320;&#29983;&#25104;&#20102;&#19981;&#21516;&#35821;&#35328;&#30340;&#25918;&#23556;&#23398;&#21360;&#35937;&#65292;&#20197;&#24635;&#32467;&#33521;&#35821;&#12289;&#33889;&#33796;&#29273;&#35821;&#21644;&#24503;&#35821;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#30340;&#21457;&#29616;&#12290;&#22312;&#19968;&#39033;&#30450;&#27979;&#20013;&#65292;&#20004;&#20301;&#26377;&#25191;&#19994;&#36164;&#26684;&#30340;&#25918;&#23556;&#31185;&#21307;&#29983;&#34920;&#31034;&#65292;&#23545;&#20110;&#33267;&#23569;70%&#30340;&#31995;&#32479;&#29983;&#25104;&#30340;&#25688;&#35201;&#65292;&#20854;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
The impression section of a radiology report summarizes important radiology findings and plays a critical role in communicating these findings to physicians. However, the preparation of these summaries is time-consuming and error-prone for radiologists. Recently, numerous models for radiology report summarization have been developed. Nevertheless, there is currently no model that can summarize these reports in multiple languages. Such a model could greatly improve future research and the development of Deep Learning models that incorporate data from patients with different ethnic backgrounds. In this study, the generation of radiology impressions in different languages was automated by fine-tuning a model, publicly available, based on a multilingual text-to-text Transformer to summarize findings available in English, Portuguese, and German radiology reports. In a blind test, two board-certified radiologists indicated that for at least 70% of the system-generated summaries, the quality 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65288;SSTM&#65289;&#65292;&#36890;&#36807;&#23545;ChatGPT&#30340;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#21644;&#29992;&#25143;&#26597;&#35810;&#36827;&#34892;&#23454;&#39564;&#65292;&#25104;&#21151;&#25552;&#21462;&#20986;&#20102;&#20855;&#26377;&#26356;&#39640;&#36136;&#37327;&#12289;&#26356;&#22909;&#21487;&#35299;&#37322;&#24615;&#21644;&#22810;&#26679;&#24615;&#30340;&#20844;&#20247;&#20851;&#20999;&#12290;</title><link>http://arxiv.org/abs/2309.01522</link><description>&lt;p&gt;
ChatGPT&#30340;&#20844;&#20247;&#20851;&#20999;&#26159;&#20160;&#20040;&#65311;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#21578;&#35785;&#20320;&#12290;
&lt;/p&gt;
&lt;p&gt;
What are Public Concerns about ChatGPT? A Novel Self-Supervised Neural Topic Model Tells You. (arXiv:2309.01522v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65288;SSTM&#65289;&#65292;&#36890;&#36807;&#23545;ChatGPT&#30340;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#21644;&#29992;&#25143;&#26597;&#35810;&#36827;&#34892;&#23454;&#39564;&#65292;&#25104;&#21151;&#25552;&#21462;&#20986;&#20102;&#20855;&#26377;&#26356;&#39640;&#36136;&#37327;&#12289;&#26356;&#22909;&#21487;&#35299;&#37322;&#24615;&#21644;&#22810;&#26679;&#24615;&#30340;&#20844;&#20247;&#20851;&#20999;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#21457;&#24067;&#30340;&#20154;&#24037;&#26234;&#33021;&#23545;&#35805;&#20195;&#29702;ChatGPT&#22312;&#23398;&#26415;&#30028;&#21644;&#29616;&#23454;&#29983;&#27963;&#20013;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#35768;&#22810;&#26089;&#26399;&#30340;ChatGPT&#29992;&#25143;&#28909;&#20999;&#22320;&#25506;&#32034;&#20854;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#31038;&#20132;&#23186;&#20307;&#20998;&#20139;&#20182;&#20204;&#23545;&#23427;&#30340;&#24847;&#35265;&#12290;&#29992;&#25143;&#26597;&#35810;&#21644;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#37117;&#34920;&#36798;&#20102;&#23545;&#36825;&#20010;&#20808;&#36827;&#23545;&#35805;&#31995;&#32479;&#30340;&#20844;&#20247;&#20851;&#20999;&#12290;&#20026;&#20102;&#25366;&#25496;&#20851;&#20110;ChatGPT&#30340;&#20844;&#20247;&#20851;&#20999;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65288;SSTM&#65289;&#65292;&#35813;&#27169;&#22411;&#23558;&#20027;&#39064;&#24314;&#27169;&#24418;&#24335;&#21270;&#20026;&#19968;&#31181;&#34920;&#31034;&#23398;&#20064;&#36807;&#31243;&#12290;&#36890;&#36807;&#23545;&#20851;&#20110;ChatGPT&#30340;Twitter&#24086;&#23376;&#21644;ChatGPT&#29992;&#25143;&#30340;&#26597;&#35810;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#25552;&#21462;&#20855;&#26377;&#26356;&#39640;&#36136;&#37327;&#12289;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#22810;&#26679;&#24615;&#30340;&#20844;&#20247;&#20851;&#20999;&#65292;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recently released artificial intelligence conversational agent, ChatGPT, has gained significant attention in academia and real life. A multitude of early ChatGPT users eagerly explore its capabilities and share their opinions on it via social media. Both user queries and social media posts express public concerns regarding this advanced dialogue system. To mine public concerns about ChatGPT, a novel Self-Supervised neural Topic Model (SSTM), which formalizes topic modeling as a representation learning procedure, is proposed in this paper. Extensive experiments have been conducted on Twitter posts about ChatGPT and queries asked by ChatGPT users. And experimental results demonstrate that the proposed approach could extract higher quality public concerns with improved interpretability and diversity, surpassing the performance of state-of-the-art approaches.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22823;&#22411;&#20869;&#23481;&#21644;&#34892;&#20026;&#27169;&#22411;&#26469;&#29702;&#35299;&#12289;&#27169;&#25311;&#21644;&#20248;&#21270;&#20869;&#23481;&#21644;&#34892;&#20026;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34429;&#28982;&#22312;&#20219;&#21153;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#36824;&#26080;&#27861;&#35299;&#20915;&#39044;&#27979;&#21644;&#20248;&#21270;&#36890;&#20449;&#20197;&#23454;&#29616;&#26399;&#26395;&#25509;&#25910;&#32773;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;&#20854;&#20013;&#30340;&#19968;&#20010;&#21407;&#22240;&#21487;&#33021;&#26159;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#32570;&#23569;"&#34892;&#20026;&#26631;&#35760;"&#12290;</title><link>http://arxiv.org/abs/2309.00359</link><description>&lt;p&gt;
&#22823;&#22411;&#20869;&#23481;&#21644;&#34892;&#20026;&#27169;&#22411;&#29992;&#20110;&#29702;&#35299;&#12289;&#27169;&#25311;&#21644;&#20248;&#21270;&#20869;&#23481;&#21644;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Large Content And Behavior Models To Understand, Simulate, And Optimize Content And Behavior. (arXiv:2309.00359v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00359
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22823;&#22411;&#20869;&#23481;&#21644;&#34892;&#20026;&#27169;&#22411;&#26469;&#29702;&#35299;&#12289;&#27169;&#25311;&#21644;&#20248;&#21270;&#20869;&#23481;&#21644;&#34892;&#20026;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34429;&#28982;&#22312;&#20219;&#21153;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#36824;&#26080;&#27861;&#35299;&#20915;&#39044;&#27979;&#21644;&#20248;&#21270;&#36890;&#20449;&#20197;&#23454;&#29616;&#26399;&#26395;&#25509;&#25910;&#32773;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;&#20854;&#20013;&#30340;&#19968;&#20010;&#21407;&#22240;&#21487;&#33021;&#26159;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#32570;&#23569;"&#34892;&#20026;&#26631;&#35760;"&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39321;&#20892;&#22312;&#24341;&#20837;&#20449;&#24687;&#29702;&#35770;&#30340;&#32463;&#20856;&#35770;&#25991;&#20013;&#23558;&#36890;&#20449;&#20998;&#20026;&#19977;&#20010;&#23618;&#27425;&#65306;&#25216;&#26415;&#23618;&#12289;&#35821;&#20041;&#23618;&#21644;&#25928;&#26524;&#23618;&#12290;&#25216;&#26415;&#23618;&#20851;&#27880;&#30340;&#26159;&#20934;&#30830;&#37325;&#26500;&#20256;&#36755;&#30340;&#31526;&#21495;&#65292;&#32780;&#35821;&#20041;&#23618;&#21644;&#25928;&#26524;&#23618;&#21017;&#28041;&#21450;&#25512;&#26029;&#20986;&#30340;&#24847;&#20041;&#21450;&#20854;&#23545;&#25509;&#25910;&#32773;&#30340;&#24433;&#21709;&#12290;&#24471;&#30410;&#20110;&#30005;&#20449;&#25216;&#26415;&#65292;&#31532;&#19968;&#23618;&#38382;&#39064;&#24050;&#32463;&#21462;&#24471;&#20102;&#36739;&#22823;&#30340;&#36827;&#27493;&#65292;&#22914;&#20114;&#32852;&#32593;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#31532;&#20108;&#20010;&#30446;&#26631;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#31532;&#19977;&#23618;&#20173;&#28982;&#22522;&#26412;&#19978;&#26410;&#34987;&#35302;&#21450;&#12290;&#31532;&#19977;&#20010;&#38382;&#39064;&#28041;&#21450;&#39044;&#27979;&#21644;&#20248;&#21270;&#36890;&#20449;&#20197;&#23454;&#29616;&#26399;&#26395;&#30340;&#25509;&#25910;&#32773;&#34892;&#20026;&#12290;LLM&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#20102;&#24191;&#27867;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20294;&#26080;&#27861;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#34920;&#29616;&#19981;&#20339;&#30340;&#21407;&#22240;&#20043;&#19968;&#21487;&#33021;&#26159;LLM&#30340;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#32570;&#23569;"&#34892;&#20026;&#26631;&#35760;"&#12290;&#34892;&#20026;&#26631;&#35760;&#23450;&#20041;&#20102;&#22312;&#19968;&#27425;&#36890;&#20449;&#20013;&#30340;&#25509;&#25910;&#32773;&#34892;&#20026;&#65292;&#22914;&#20998;&#20139;&#12289;&#28857;&#36190;&#12289;&#28857;&#20987;&#12289;&#36141;&#20080;&#12289;&#36716;&#25512;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Shannon, in his seminal paper introducing information theory, divided the communication into three levels: technical, semantic, and effectivenss. While the technical level is concerned with accurate reconstruction of transmitted symbols, the semantic and effectiveness levels deal with the inferred meaning and its effect on the receiver. Thanks to telecommunications, the first level problem has produced great advances like the internet. Large Language Models (LLMs) make some progress towards the second goal, but the third level still remains largely untouched. The third problem deals with predicting and optimizing communication for desired receiver behavior. LLMs, while showing wide generalization capabilities across a wide range of tasks, are unable to solve for this. One reason for the underperformance could be a lack of "behavior tokens" in LLMs' training corpora. Behavior tokens define receiver behavior over a communication, such as shares, likes, clicks, purchases, retweets, etc. W
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36328;&#35821;&#35328;&#21644;&#26041;&#35328;&#20013;&#30340;&#20146;&#23646;&#20851;&#31995;&#35789;&#27719;&#30340;&#35789;&#27719;&#22810;&#26679;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#20016;&#23500;&#35745;&#31639;&#35789;&#27719;&#36164;&#28304;&#12290;&#36890;&#36807;&#22823;&#35268;&#27169;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#20379;&#27983;&#35272;&#21644;&#19979;&#36733;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#25193;&#23637;&#20102;&#23545;&#20146;&#23646;&#20851;&#31995;&#26415;&#35821;&#30340;&#35821;&#35328;&#23398;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#22312;&#35821;&#35328;&#21644;&#25991;&#21270;&#19978;&#30456;&#20114;&#25509;&#36817;&#30340;&#31038;&#21306;&#20013;&#22810;&#26679;&#24615;&#30340;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.13056</link><description>&lt;p&gt;
&#36328;&#35821;&#35328;&#21644;&#26041;&#35328;&#20013;&#30340;&#20146;&#23646;&#20851;&#31995;&#35789;&#27719;&#30340;&#35789;&#27719;&#22810;&#26679;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Lexical Diversity in Kinship Across Languages and Dialects. (arXiv:2308.13056v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13056
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36328;&#35821;&#35328;&#21644;&#26041;&#35328;&#20013;&#30340;&#20146;&#23646;&#20851;&#31995;&#35789;&#27719;&#30340;&#35789;&#27719;&#22810;&#26679;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#20016;&#23500;&#35745;&#31639;&#35789;&#27719;&#36164;&#28304;&#12290;&#36890;&#36807;&#22823;&#35268;&#27169;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#20379;&#27983;&#35272;&#21644;&#19979;&#36733;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#25193;&#23637;&#20102;&#23545;&#20146;&#23646;&#20851;&#31995;&#26415;&#35821;&#30340;&#35821;&#35328;&#23398;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#22312;&#35821;&#35328;&#21644;&#25991;&#21270;&#19978;&#30456;&#20114;&#25509;&#36817;&#30340;&#31038;&#21306;&#20013;&#22810;&#26679;&#24615;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#30693;&#35821;&#35328;&#20197;&#22810;&#26679;&#30340;&#26041;&#24335;&#25551;&#36848;&#19990;&#30028;&#12290;&#22312;&#35789;&#27719;&#20013;&#65292;&#22810;&#26679;&#24615;&#24191;&#27867;&#23384;&#22312;&#65292;&#22914;&#35789;&#27719;&#31354;&#32570;&#21644;&#26080;&#27861;&#32763;&#35793;&#31561;&#29616;&#35937;&#12290;&#28982;&#32780;&#65292;&#22312;&#35745;&#31639;&#36164;&#28304;&#20013;&#65292;&#22914;&#22810;&#35821;&#31181;&#35789;&#27719;&#25968;&#25454;&#24211;&#20013;&#65292;&#22810;&#26679;&#24615;&#24456;&#23569;&#24471;&#21040;&#34920;&#31034;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#19982;&#35821;&#35328;&#22810;&#26679;&#24615;&#30456;&#20851;&#30340;&#20869;&#23481;&#65292;&#20016;&#23500;&#35745;&#31639;&#35789;&#27719;&#36164;&#28304;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23545;&#20146;&#23646;&#20851;&#31995;&#26415;&#35821;&#36827;&#34892;&#20004;&#20010;&#22823;&#35268;&#27169;&#26696;&#20363;&#30740;&#31350;&#36827;&#34892;&#39564;&#35777;&#65292;&#36825;&#26159;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#22312;&#35821;&#35328;&#21644;&#25991;&#21270;&#20013;&#20855;&#26377;&#22810;&#26679;&#24615;&#30340;&#39046;&#22495;&#65306;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#28041;&#21450;&#19971;&#31181;&#38463;&#25289;&#20271;&#26041;&#35328;&#65292;&#32780;&#21478;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#28041;&#21450;&#19977;&#31181;&#21360;&#24230;&#23612;&#35199;&#20122;&#35821;&#35328;&#12290;&#25105;&#20204;&#25152;&#24471;&#21040;&#30340;&#32467;&#26524;&#20197;&#21487;&#27983;&#35272;&#21644;&#21487;&#19979;&#36733;&#30340;&#35745;&#31639;&#36164;&#28304;&#30340;&#24418;&#24335;&#25552;&#20379;&#65292;&#25193;&#23637;&#20102;&#20808;&#21069;&#23545;&#20146;&#23646;&#20851;&#31995;&#26415;&#35821;&#30340;&#35821;&#35328;&#23398;&#30740;&#31350;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#22312;&#35821;&#35328;&#21644;&#25991;&#21270;&#19978;&#30456;&#20114;&#25509;&#36817;&#30340;&#31038;&#21306;&#20013;&#22810;&#26679;&#24615;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Languages are known to describe the world in diverse ways. Across lexicons, diversity is pervasive, appearing through phenomena such as lexical gaps and untranslatability. However, in computational resources, such as multilingual lexical databases, diversity is hardly ever represented. In this paper, we introduce a method to enrich computational lexicons with content relating to linguistic diversity. The method is verified through two large-scale case studies on kinship terminology, a domain known to be diverse across languages and cultures: one case study deals with seven Arabic dialects, while the other one with three Indonesian languages. Our results, made available as browseable and downloadable computational resources, extend prior linguistics research on kinship terminology, and provide insight into the extent of diversity even within linguistically and culturally close communities.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#30693;&#35782;&#22686;&#24378;&#30340;&#20004;&#38454;&#27573;&#29983;&#25104;&#26694;&#26550;&#65288;KTGF&#65289;&#29992;&#20110;&#21307;&#23398;&#23545;&#35805;&#20449;&#24687;&#25552;&#21462;&#12290;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#29983;&#25104;&#65292;&#20998;&#21035;&#29983;&#25104;&#21307;&#23398;&#23545;&#35805;&#20013;&#30340;&#26415;&#35821;&#21644;&#27599;&#20010;&#26415;&#35821;&#30340;&#29366;&#24577;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#24314;&#27169;&#26415;&#35821;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2307.16200</link><description>&lt;p&gt;
&#19968;&#20010;&#30693;&#35782;&#22686;&#24378;&#30340;&#20004;&#38454;&#27573;&#29983;&#25104;&#26694;&#26550;&#29992;&#20110;&#21307;&#23398;&#23545;&#35805;&#20449;&#24687;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
A Knowledge-enhanced Two-stage Generative Framework for Medical Dialogue Information Extraction. (arXiv:2307.16200v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#30693;&#35782;&#22686;&#24378;&#30340;&#20004;&#38454;&#27573;&#29983;&#25104;&#26694;&#26550;&#65288;KTGF&#65289;&#29992;&#20110;&#21307;&#23398;&#23545;&#35805;&#20449;&#24687;&#25552;&#21462;&#12290;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#29983;&#25104;&#65292;&#20998;&#21035;&#29983;&#25104;&#21307;&#23398;&#23545;&#35805;&#20013;&#30340;&#26415;&#35821;&#21644;&#27599;&#20010;&#26415;&#35821;&#30340;&#29366;&#24577;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#24314;&#27169;&#26415;&#35821;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#21307;&#23398;&#23545;&#35805;&#20013;&#30340;&#26415;&#35821;-&#29366;&#24577;&#23545;&#25552;&#21462;&#65288;MD-TSPE&#65289;&#65292;&#36825;&#22312;&#35786;&#26029;&#23545;&#35805;&#31995;&#32479;&#21644;&#30005;&#23376;&#21307;&#30103;&#35760;&#24405;&#65288;EMR&#65289;&#30340;&#33258;&#21160;&#25220;&#20889;&#20013;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;MD-TSPE&#30340;&#30740;&#31350;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#29305;&#21035;&#26159;&#22312;&#29983;&#25104;&#26041;&#27861;&#21462;&#24471;&#26174;&#33879;&#36827;&#23637;&#20043;&#21518;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#29983;&#25104;&#26041;&#27861;&#22312;&#19968;&#38454;&#27573;&#36755;&#20986;&#25972;&#20010;&#30001;&#26415;&#35821;-&#29366;&#24577;&#23545;&#32452;&#25104;&#30340;&#24207;&#21015;&#26102;&#24573;&#30053;&#20102;&#38598;&#25104;&#20808;&#21069;&#30693;&#35782;&#30340;&#38656;&#27714;&#65292;&#36825;&#38656;&#35201;&#26356;&#28145;&#20837;&#30340;&#29702;&#35299;&#26469;&#24314;&#27169;&#26415;&#35821;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#25512;&#26029;&#27599;&#20010;&#26415;&#35821;&#30340;&#29366;&#24577;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#30693;&#35782;&#22686;&#24378;&#30340;&#20004;&#38454;&#27573;&#29983;&#25104;&#26694;&#26550;&#65288;KTGF&#65289;&#26469;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#12290;&#36890;&#36807;&#20351;&#29992;&#20219;&#21153;&#29305;&#23450;&#30340;&#25552;&#31034;&#65292;&#25105;&#20204;&#37319;&#29992;&#21333;&#19968;&#27169;&#22411;&#20197;&#32479;&#19968;&#30340;&#29983;&#25104;&#24418;&#24335;&#23436;&#25104;MD-TSPE&#30340;&#20004;&#20010;&#38454;&#27573;&#65306;&#39318;&#20808;&#29983;&#25104;&#25152;&#26377;&#30340;&#26415;&#35821;&#65292;&#28982;&#21518;&#29983;&#25104;&#27599;&#20010;&#29983;&#25104;&#30340;&#26415;&#35821;&#30340;&#29366;&#24577;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#26415;&#35821;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper focuses on term-status pair extraction from medical dialogues (MD-TSPE), which is essential in diagnosis dialogue systems and the automatic scribe of electronic medical records (EMRs). In the past few years, works on MD-TSPE have attracted increasing research attention, especially after the remarkable progress made by generative methods. However, these generative methods output a whole sequence consisting of term-status pairs in one stage and ignore integrating prior knowledge, which demands a deeper understanding to model the relationship between terms and infer the status of each term. This paper presents a knowledge-enhanced two-stage generative framework (KTGF) to address the above challenges. Using task-specific prompts, we employ a single model to complete the MD-TSPE through two phases in a unified generative form: we generate all terms the first and then generate the status of each generated term. In this way, the relationship between terms can be learned more effect
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#31579;&#36873;&#31574;&#30053;AlpaGasus&#65292;&#36890;&#36807;&#20351;&#29992;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#36807;&#28388;&#25481;&#20302;&#36136;&#37327;&#25968;&#25454;&#65292;&#23427;&#22312;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#27604;&#21407;&#22987;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#24555;&#30340;&#35757;&#32451;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.08701</link><description>&lt;p&gt;
AlpaGasus: &#29992;&#26356;&#23569;&#25968;&#25454;&#35757;&#32451;&#26356;&#22909;&#30340;&#32650;&#39548;
&lt;/p&gt;
&lt;p&gt;
AlpaGasus: Training A Better Alpaca with Fewer Data. (arXiv:2307.08701v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08701
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#31579;&#36873;&#31574;&#30053;AlpaGasus&#65292;&#36890;&#36807;&#20351;&#29992;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#36807;&#28388;&#25481;&#20302;&#36136;&#37327;&#25968;&#25454;&#65292;&#23427;&#22312;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#27604;&#21407;&#22987;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#24555;&#30340;&#35757;&#32451;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#22312;&#26377;&#30417;&#30563;&#30340;&#25351;&#20196;/&#22238;&#22797;&#25968;&#25454;&#19978;&#36827;&#34892;&#25351;&#20196;&#24494;&#35843;&#65288;IFT&#65289;&#26469;&#22686;&#24378;&#20854;&#36981;&#24490;&#25351;&#20196;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#24191;&#27867;&#20351;&#29992;&#30340;IFT&#25968;&#25454;&#38598;&#65288;&#20363;&#22914;&#65306;Alpaca&#30340;52k&#25968;&#25454;&#65289;&#20986;&#20046;&#24847;&#26009;&#22320;&#21253;&#21547;&#35768;&#22810;&#20855;&#26377;&#19981;&#27491;&#30830;&#25110;&#19981;&#30456;&#20851;&#22238;&#22797;&#30340;&#20302;&#36136;&#37327;&#23454;&#20363;&#65292;&#36825;&#20123;&#23454;&#20363;&#20250;&#35823;&#23548;&#21644;&#23545;IFT&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25968;&#25454;&#36873;&#25321;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#20351;&#29992;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;&#65306;ChatGPT&#65289;&#33258;&#21160;&#35782;&#21035;&#24182;&#36807;&#28388;&#25481;&#20302;&#36136;&#37327;&#25968;&#25454;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AlpaGasus&#65292;&#23427;&#20165;&#22312;&#20174;52k Alpaca&#25968;&#25454;&#20013;&#36807;&#28388;&#24471;&#21040;&#30340;9k&#39640;&#36136;&#37327;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;AlpaGasus&#22312;&#22810;&#20010;&#27979;&#35797;&#25968;&#25454;&#38598;&#21644;&#20154;&#24037;&#35780;&#20272;&#20013;&#22343;&#26174;&#33879;&#20248;&#20110;&#21407;&#22987;&#30340;Alpaca&#65292;&#30001;GPT-4&#36827;&#34892;&#35780;&#20272;&#12290;&#20854;13B&#21464;&#31181;&#22312;&#27979;&#35797;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#19982;&#20854;&#25945;&#24072;&#27169;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#21363;&#29983;&#25104;52k&#25968;&#25454;&#30340;Text-Davinci-003&#65289;&#30340;&#24615;&#33021;&#21305;&#37197;&#29575;&#36229;&#36807;90&#65285;&#12290;&#23427;&#36824;&#25552;&#20379;&#20102;5.7&#20493;&#26356;&#24555;&#30340;&#35757;&#32451;&#36895;&#24230;&#65292;&#23558;7B&#21464;&#31181;&#30340;&#35757;&#32451;&#26102;&#38388;&#20174;80&#20998;&#38047;&#20943;&#23569;&#21040;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Large language models~(LLMs) strengthen instruction-following capability through instruction-finetuning (IFT) on supervised instruction/response data. However, widely used IFT datasets (e.g., Alpaca's 52k data) surprisingly contain many low-quality instances with incorrect or irrelevant responses, which are misleading and detrimental to IFT. In this paper, we propose a simple and effective data selection strategy that automatically identifies and filters out low-quality data using a strong LLM (e.g., ChatGPT). To this end, we introduce AlpaGasus, which is finetuned on only 9k high-quality data filtered from the 52k Alpaca data. AlpaGasus significantly outperforms the original Alpaca as evaluated by GPT-4 on multiple test sets and the controlled human evaluation. Its 13B variant matches $&gt;90\%$ performance of its teacher LLM (i.e., Text-Davinci-003 generating the 52k data) on test tasks. It also provides 5.7x faster training, reducing the training time for a 7B variant from 80 minutes (
&lt;/p&gt;</description></item><item><title>&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#24207;&#21015;&#27169;&#24335;&#23436;&#25104;&#33021;&#21147;&#65292;&#21363;&#20351;&#22312;&#38543;&#26426;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#20445;&#25345;&#19968;&#23450;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#38382;&#39064;&#65292;&#22914;&#21160;&#24577;&#29366;&#24577;&#24207;&#21015;&#39044;&#27979;&#21644;&#33258;&#20027;&#36335;&#24452;&#35268;&#21010;&#12290;</title><link>http://arxiv.org/abs/2307.04721</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36890;&#29992;&#27169;&#24335;&#26426;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as General Pattern Machines. (arXiv:2307.04721v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04721
&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#24207;&#21015;&#27169;&#24335;&#23436;&#25104;&#33021;&#21147;&#65292;&#21363;&#20351;&#22312;&#38543;&#26426;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#20445;&#25345;&#19968;&#23450;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#38382;&#39064;&#65292;&#22914;&#21160;&#24577;&#29366;&#24577;&#24207;&#21015;&#39044;&#27979;&#21644;&#33258;&#20027;&#36335;&#24452;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35266;&#23519;&#21040;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#33258;&#21160;&#23436;&#25104;&#22797;&#26434;&#30340;&#20196;&#29260;&#24207;&#21015;&#65292;&#20174;&#30001;&#27010;&#29575;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;&#65288;PCFG&#65289;&#38543;&#26426;&#29983;&#25104;&#30340;&#20219;&#24847;&#24207;&#21015;&#65292;&#21040;&#22312;Abstraction and Reasoning Corpus&#65288;ARC&#65289;&#20013;&#21457;&#29616;&#30340;&#26356;&#20016;&#23500;&#30340;&#31354;&#38388;&#27169;&#24335;&#65292;&#20197;ASCII&#33402;&#26415;&#30340;&#24418;&#24335;&#25552;&#31034;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#21363;&#20351;&#20351;&#29992;&#20174;&#35789;&#27719;&#34920;&#20013;&#38543;&#26426;&#25277;&#26679;&#30340;&#20196;&#29260;&#34920;&#31034;&#24207;&#21015;&#65292;&#27169;&#24335;&#23436;&#25104;&#33021;&#21147;&#20063;&#21487;&#20197;&#37096;&#20998;&#20445;&#30041;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#27809;&#26377;&#20219;&#20309;&#39069;&#22806;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;LLMs&#21487;&#20197;&#20316;&#20026;&#36890;&#29992;&#24207;&#21015;&#27169;&#22411;&#22120;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#39537;&#21160;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20123;&#38646;&#26679;&#26412;&#33021;&#21147;&#22914;&#20309;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#38382;&#39064;&#65292;&#20174;&#23545;&#34920;&#31034;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#29366;&#24577;&#30340;&#25968;&#23383;&#24207;&#21015;&#36827;&#34892;&#22806;&#25512;&#65292;&#20197;&#23436;&#25104;&#31616;&#21333;&#30340;&#36816;&#21160;&#65292;&#21040;&#20197;&#22870;&#21169;&#26465;&#20214;&#36712;&#36857;&#30340;&#26368;&#23567;&#21040;&#26368;&#22823;&#25552;&#31034;&#26041;&#24335;&#65292;&#33021;&#22815;&#21457;&#29616;&#21644;&#34920;&#31034;&#38381;&#29615;&#31574;&#30053;&#65288;&#20363;&#22914;&#65292;&#31283;&#23450;&#30340;&#25511;&#21046;&#31995;&#32479;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We observe that pre-trained large language models (LLMs) are capable of autoregressively completing complex token sequences -- from arbitrary ones procedurally generated by probabilistic context-free grammars (PCFG), to more rich spatial patterns found in the Abstraction and Reasoning Corpus (ARC), a general AI benchmark, prompted in the style of ASCII art. Surprisingly, pattern completion proficiency can be partially retained even when the sequences are expressed using tokens randomly sampled from the vocabulary. These results suggest that without any additional training, LLMs can serve as general sequence modelers, driven by in-context learning. In this work, we investigate how these zero-shot capabilities may be applied to problems in robotics -- from extrapolating sequences of numbers that represent states over time to complete simple motions, to least-to-most prompting of reward-conditioned trajectories that can discover and represent closed-loop policies (e.g., a stabilizing cont
&lt;/p&gt;</description></item><item><title>CARE-MI&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#20013;&#22269;&#23381;&#23156;&#25252;&#29702;&#39046;&#22495;LLM&#34394;&#20551;&#20449;&#24687;&#30340;&#22522;&#20934;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#24182;&#25552;&#20379;&#20102;&#26500;&#24314;&#38271;&#31687;&#29983;&#25104;&#35780;&#20272;&#22522;&#20934;&#30340;&#21019;&#26032;&#33539;&#24335;&#12290;</title><link>http://arxiv.org/abs/2307.01458</link><description>&lt;p&gt;
CARE-MI: &#20013;&#22269;&#23381;&#23156;&#25252;&#29702;&#39046;&#22495;&#30340;&#34394;&#20551;&#20449;&#24687;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
CARE-MI: Chinese Benchmark for Misinformation Evaluation in Maternity and Infant Care. (arXiv:2307.01458v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01458
&lt;/p&gt;
&lt;p&gt;
CARE-MI&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#20013;&#22269;&#23381;&#23156;&#25252;&#29702;&#39046;&#22495;LLM&#34394;&#20551;&#20449;&#24687;&#30340;&#22522;&#20934;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#24182;&#25552;&#20379;&#20102;&#26500;&#24314;&#38271;&#31687;&#29983;&#25104;&#35780;&#20272;&#22522;&#20934;&#30340;&#21019;&#26032;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#36827;&#23637;&#23548;&#33268;&#20102;&#23558;LLM&#24212;&#29992;&#20110;&#29616;&#23454;&#22330;&#26223;&#30340;&#26032;&#36235;&#21183;&#12290;&#23613;&#31649;&#26368;&#26032;&#30340;LLM&#22312;&#19982;&#20154;&#31867;&#20114;&#21160;&#26102;&#20196;&#20154;&#24778;&#21497;&#22320;&#27969;&#21033;&#65292;&#20294;&#23427;&#20204;&#22312;&#29983;&#25104;&#38169;&#35823;&#20107;&#23454;&#38472;&#36848;&#26102;&#20250;&#24847;&#22806;&#20135;&#29983;&#34394;&#20551;&#20449;&#24687;&#38382;&#39064;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#26377;&#23475;&#21518;&#26524;&#65292;&#23588;&#20854;&#26159;&#22312;&#25935;&#24863;&#29615;&#22659;&#19979;&#65292;&#27604;&#22914;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#24456;&#23569;&#26377;&#30740;&#31350;&#20851;&#27880;&#35780;&#20272;LLM&#38271;&#31687;&#29983;&#25104;&#20013;&#30340;&#34394;&#20551;&#20449;&#24687;&#65292;&#23588;&#20854;&#26159;&#38024;&#23545;&#30693;&#35782;&#23494;&#38598;&#22411;&#20027;&#39064;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;LLM&#22312;&#19981;&#21516;&#35821;&#35328;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#34394;&#20551;&#20449;&#24687;&#35780;&#20272;&#20027;&#35201;&#22312;&#33521;&#35821;&#20013;&#36827;&#34892;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20934;&#65292;CARE-MI&#65292;&#29992;&#20110;&#35780;&#20272;LLM&#34394;&#20551;&#20449;&#24687;&#22312;&#65306;1&#65289;&#19968;&#20010;&#25935;&#24863;&#20027;&#39064;&#65292;&#20855;&#20307;&#26159;&#23381;&#23156;&#25252;&#29702;&#39046;&#22495;&#65307;&#21644;2&#65289;&#19968;&#31181;&#38750;&#33521;&#35821;&#35821;&#35328;&#65292;&#21363;&#20013;&#25991;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#33539;&#24335;&#65292;&#29992;&#20110;&#26500;&#24314;&#38271;&#31687;&#29983;&#25104;&#35780;&#20272;&#22522;&#20934;&#65292;&#21487;&#20197;
&lt;/p&gt;
&lt;p&gt;
The recent advances in NLP, have led to a new trend of applying LLMs to real-world scenarios. While the latest LLMs are astonishingly fluent when interacting with humans, they suffer from the misinformation problem by unintentionally generating factually false statements. This can lead to harmful consequences, especially when produced within sensitive contexts, such as healthcare. Yet few previous works have focused on evaluating misinformation in the long-form generation of LLMs, especially for knowledge-intensive topics. Moreover, although LLMs have been shown to perform well in different languages, misinformation evaluation has been mostly conducted in English. To this end, we present a benchmark, CARE-MI, for evaluating LLM misinformation in: 1) a sensitive topic, specifically the maternity and infant care domain; and 2) a language other than English, namely Chinese. Most importantly, we provide an innovative paradigm for building long-form generation evaluation benchmarks that can
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;DocumentNet&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;Web&#19978;&#25910;&#38598;&#22823;&#35268;&#27169;&#21644;&#24369;&#26631;&#27880;&#30340;&#25968;&#25454;&#65292;&#24357;&#21512;&#20102;&#25991;&#26723;&#39044;&#35757;&#32451;&#20013;&#30340;&#25968;&#25454;&#24046;&#36317;&#65292;&#24182;&#22312;&#21508;&#31867;VDER&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2306.08937</link><description>&lt;p&gt;
DocumentNet: &#22312;&#25991;&#26723;&#39044;&#35757;&#32451;&#20013;&#24357;&#21512;&#25968;&#25454;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
DocumentNet: Bridging the Data Gap in Document Pre-Training. (arXiv:2306.08937v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08937
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;DocumentNet&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;Web&#19978;&#25910;&#38598;&#22823;&#35268;&#27169;&#21644;&#24369;&#26631;&#27880;&#30340;&#25968;&#25454;&#65292;&#24357;&#21512;&#20102;&#25991;&#26723;&#39044;&#35757;&#32451;&#20013;&#30340;&#25968;&#25454;&#24046;&#36317;&#65292;&#24182;&#22312;&#21508;&#31867;VDER&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25991;&#26723;&#29702;&#35299;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#23500;&#26377;&#35270;&#35273;&#20803;&#32032;&#30340;&#25991;&#26723;&#23454;&#20307;&#26816;&#32034;&#65288;VDER&#65289;&#65292;&#30001;&#20110;&#22312;&#20225;&#19994;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#21463;&#21040;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20005;&#26684;&#30340;&#38544;&#31169;&#32422;&#26463;&#21644;&#39640;&#26114;&#30340;&#26631;&#27880;&#25104;&#26412;&#65292;&#36825;&#20123;&#20219;&#21153;&#30340;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38750;&#24120;&#26377;&#38480;&#12290;&#26356;&#31967;&#31957;&#30340;&#26159;&#65292;&#26469;&#33258;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#19981;&#37325;&#21472;&#23454;&#20307;&#31354;&#38388;&#22952;&#30861;&#20102;&#25991;&#26723;&#31867;&#22411;&#20043;&#38388;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;Web&#25910;&#38598;&#22823;&#35268;&#27169;&#21644;&#24369;&#26631;&#27880;&#30340;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#20197;&#21033;&#20110;VDER&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#25152;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#21517;&#20026;DocumentNet&#65292;&#19981;&#20381;&#36182;&#20110;&#29305;&#23450;&#30340;&#25991;&#26723;&#31867;&#22411;&#25110;&#23454;&#20307;&#38598;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#25152;&#26377;&#30340;VDER&#20219;&#21153;&#12290;&#30446;&#21069;&#30340;DocumentNet&#21253;&#21547;&#20102;30M&#20010;&#25991;&#26723;&#65292;&#28085;&#30422;&#20102;&#36817;400&#20010;&#25991;&#26723;&#31867;&#22411;&#65292;&#32452;&#32455;&#25104;&#20102;&#19968;&#20010;&#22235;&#32423;&#26412;&#20307;&#32467;&#26500;&#12290;&#22312;&#19968;&#31995;&#21015;&#24191;&#27867;&#37319;&#29992;&#30340;VDER&#20219;&#21153;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#24403;&#23558;DocumentNet&#32435;&#20837;&#39044;&#35757;&#32451;&#36807;&#31243;&#26102;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document understanding tasks, in particular, Visually-rich Document Entity Retrieval (VDER), have gained significant attention in recent years thanks to their broad applications in enterprise AI. However, publicly available data have been scarce for these tasks due to strict privacy constraints and high annotation costs. To make things worse, the non-overlapping entity spaces from different datasets hinder the knowledge transfer between document types. In this paper, we propose a method to collect massive-scale and weakly labeled data from the web to benefit the training of VDER models. The collected dataset, named DocumentNet, does not depend on specific document types or entity sets, making it universally applicable to all VDER tasks. The current DocumentNet consists of 30M documents spanning nearly 400 document types organized in a four-level ontology. Experiments on a set of broadly adopted VDER tasks show significant improvements when DocumentNet is incorporated into the pre-train
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#21327;&#35758;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#39046;&#22495;&#22806;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#20010;&#22522;&#20934;&#22871;&#20214;&#65292;&#20316;&#32773;&#20204;&#21457;&#29616;OOD&#19982;ID&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#24182;&#19981;&#24635;&#26159;&#19968;&#33268;&#30340;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;LLMs&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#26174;&#33879;&#25552;&#39640;OOD&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04618</link><description>&lt;p&gt;
&#37325;&#28201;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#39046;&#22495;&#22806;&#40065;&#26834;&#24615;: &#22522;&#20934;&#65292;&#20998;&#26512;&#21644;LLMs&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Revisiting Out-of-distribution Robustness in NLP: Benchmark, Analysis, and LLMs Evaluations. (arXiv:2306.04618v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#21327;&#35758;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#39046;&#22495;&#22806;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#20010;&#22522;&#20934;&#22871;&#20214;&#65292;&#20316;&#32773;&#20204;&#21457;&#29616;OOD&#19982;ID&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#24182;&#19981;&#24635;&#26159;&#19968;&#33268;&#30340;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;LLMs&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#26174;&#33879;&#25552;&#39640;OOD&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#39046;&#22495;&#20013;&#39046;&#22495;&#22806;&#40065;&#26834;&#24615;(OOD)&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#21457;&#29616;&#20197;&#24448;&#30740;&#31350;&#20013;&#30340;&#20998;&#24067;&#36716;&#31227;&#35774;&#32622;&#26222;&#36941;&#32570;&#20047;&#36275;&#22815;&#30340;&#25361;&#25112;&#65292;&#38480;&#21046;&#20102;&#23545;OOD&#40065;&#26834;&#24615;&#30340;&#20934;&#30830;&#35780;&#20272;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#26500;&#24314;&#26041;&#26696;&#65292;&#30830;&#20445;&#20102;&#26126;&#30830;&#30340;&#21306;&#20998;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20998;&#24067;&#36716;&#31227;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;BOSS&#65292;&#19968;&#20010;&#28085;&#30422;5&#20010;&#20219;&#21153;&#21644;20&#20010;&#25968;&#25454;&#38598;&#30340;&#29992;&#20110;&#35780;&#20272;OOT&#40065;&#26834;&#24615;&#30340;&#22522;&#20934;&#22871;&#20214;&#12290;&#22522;&#20110;BOSS&#65292;&#25105;&#20204;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#20197;&#20998;&#26512;&#21644;&#35780;&#20272;OOD&#40065;&#26834;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#39321;&#33609;&#24494;&#35843;&#30340;ID&#21644;OOD&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#19977;&#31181;&#20856;&#22411;&#31867;&#22411;&#25581;&#31034;&#20102;&#20869;&#22312;&#30340;&#23398;&#20064;&#26426;&#21046;&#65292;&#21487;&#33021;&#26377;&#21161;&#20110;&#39044;&#27979;OOD&#40065;&#26834;&#24615;&#65292;&#24182;&#19982;ID&#25968;&#25454;&#38598;&#19978;&#30340;&#36827;&#23637;&#30456;&#20851;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;BOSS&#19978;&#35780;&#20272;&#20102;5&#31181;&#32463;&#20856;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#30340;OOD&#24615;&#33021;&#24182;&#19981;&#24635;&#26159;&#19982;ID&#24615;&#33021;&#19968;&#33268;&#65292;&#36825;&#34920;&#26126;&#20102;&#29305;&#21035;&#35780;&#20272;OOD&#40065;&#26834;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LLMs&#65288;&#28508;&#22312;&#35821;&#35328;&#27169;&#22411;&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#26174;&#33879;&#25552;&#39640;OOD&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper reexamines the research on out-of-distribution (OOD) robustness in the field of NLP. We find that the distribution shift settings in previous studies commonly lack adequate challenges, hindering the accurate evaluation of OOD robustness. To address these issues, we propose a benchmark construction protocol that ensures clear differentiation and challenging distribution shifts. Then we introduce BOSS, a Benchmark suite for Out-of-distribution robustneSS evaluation covering 5 tasks and 20 datasets. Based on BOSS, we conduct a series of experiments on pre-trained language models for analysis and evaluation of OOD robustness. First, for vanilla fine-tuning, we examine the relationship between in-distribution (ID) and OOD performance. We identify three typical types that unveil the inner learning mechanism, which could potentially facilitate the forecasting of OOD robustness, correlating with the advancements on ID datasets. Then, we evaluate 5 classic methods on BOSS and find th
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20154;&#21592;&#30740;&#31350;&#20102;&#22312;&#25968;&#25454;&#21463;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#32553;&#25918;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#26368;&#20248;&#24615;&#30340;&#32553;&#25918;&#23450;&#24459;&#65292;&#32771;&#34385;&#21040;&#37325;&#22797;&#20196;&#29260;&#21644;&#36807;&#37327;&#21442;&#25968;&#30340;&#20215;&#20540;&#36882;&#20943;&#12290;</title><link>http://arxiv.org/abs/2305.16264</link><description>&lt;p&gt;
&#32553;&#25918;&#25968;&#25454;&#21463;&#38480;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Scaling Data-Constrained Language Models. (arXiv:2305.16264v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16264
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#30740;&#31350;&#20102;&#22312;&#25968;&#25454;&#21463;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#32553;&#25918;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#26368;&#20248;&#24615;&#30340;&#32553;&#25918;&#23450;&#24459;&#65292;&#32771;&#34385;&#21040;&#37325;&#22797;&#20196;&#29260;&#21644;&#36807;&#37327;&#21442;&#25968;&#30340;&#20215;&#20540;&#36882;&#20943;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22312;&#25193;&#23637;&#35821;&#35328;&#27169;&#22411;&#30340;&#36235;&#21183;&#28041;&#21450;&#22686;&#21152;&#21442;&#25968;&#35745;&#25968;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#12290;&#25512;&#26029;&#36825;&#20010;&#36235;&#21183;&#34920;&#26126;&#65292;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#21487;&#33021;&#24456;&#24555;&#23601;&#20250;&#21463;&#21040;&#20114;&#32852;&#32593;&#19978;&#21487;&#29992;&#25991;&#26412;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;&#20986;&#20110;&#27492;&#38480;&#21046;&#30340;&#21160;&#26426;&#65292;&#25105;&#20204;&#30740;&#31350;&#22312;&#25968;&#25454;&#21463;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#32553;&#25918;&#35821;&#35328;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36816;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#65292;&#21464;&#21270;&#25968;&#25454;&#37325;&#22797;&#31243;&#24230;&#21644;&#35745;&#31639;&#39044;&#31639;&#65292;&#33539;&#22260;&#36798;&#21040;&#20102;9000&#20159;&#20010;&#35757;&#32451;&#20196;&#29260;&#21644;9&#20159;&#21442;&#25968;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#26377;&#38480;&#30340;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#39640;&#36798;4&#27425;&#37325;&#22797;&#25968;&#25454;&#30340;&#35757;&#32451;&#19982;&#20351;&#29992;&#21807;&#19968;&#25968;&#25454;&#30456;&#27604;&#23545;&#25439;&#22833;&#30340;&#36129;&#29486;&#24494;&#19981;&#36275;&#36947;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#26356;&#22810;&#30340;&#37325;&#22797;&#25968;&#25454;&#65292;&#28155;&#21152;&#35745;&#31639;&#30340;&#20215;&#20540;&#26368;&#32456;&#20250;&#34928;&#20943;&#20026;&#38646;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#32463;&#39564;&#35777;&#20102;&#19968;&#20010;&#35745;&#31639;&#26368;&#20248;&#24615;&#30340;&#32553;&#25918;&#23450;&#24459;&#65292;&#32771;&#34385;&#21040;&#37325;&#22797;&#20196;&#29260;&#21644;&#36807;&#37327;&#21442;&#25968;&#30340;&#20215;&#20540;&#36882;&#20943;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23581;&#35797;&#20102;&#32531;&#35299;&#25968;&#25454;&#31232;&#32570;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current trend of scaling language models involves increasing both parameter count and training dataset size. Extrapolating this trend suggests that training dataset size may soon be limited by the amount of text data available on the internet. Motivated by this limit, we investigate scaling language models in data-constrained regimes. Specifically, we run a large set of experiments varying the extent of data repetition and compute budget, ranging up to 900 billion training tokens and 9 billion parameter models. We find that with constrained data for a fixed compute budget, training with up to 4 epochs of repeated data yields negligible changes to loss compared to having unique data. However, with more repetition, the value of adding compute eventually decays to zero. We propose and empirically validate a scaling law for compute optimality that accounts for the decreasing value of repeated tokens and excess parameters. Finally, we experiment with approaches mitigating data scarcity,
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20379;&#20102;22&#31181;&#21360;&#24230;&#23466;&#27861;&#20013;&#21015;&#20986;&#30340;&#25152;&#26377;21&#31181;&#26412;&#22303;&#25991;&#23383;&#21644;&#32599;&#39532;&#23383;&#27597;&#30340;&#20844;&#24320;&#35821;&#35328;&#37492;&#21035;&#65288;LID&#65289;&#25968;&#25454;&#21644;&#27169;&#22411;&#12290;IndicLID&#26159;&#19978;&#36848;&#35821;&#35328;&#30340;&#26412;&#22303;&#21644;&#32599;&#39532;&#21270;&#33050;&#26412;&#30340;&#35821;&#35328;&#37492;&#21035;&#22120;&#65292;&#36824;&#25552;&#20986;&#20102;&#35299;&#20915;&#32599;&#39532;&#21270;&#25991;&#26412;&#30340;LID&#38382;&#39064;&#30340;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.15814</link><description>&lt;p&gt;
Bhasha-Abhijnaanam&#65306;22&#31181;&#21360;&#24230;&#25991;&#23383;&#21644;&#32599;&#39532;&#25340;&#38899;&#35821;&#35328;&#37492;&#21035;&#12290; (arXiv&#65306;2305.15814v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
Bhasha-Abhijnaanam: Native-script and romanized Language Identification for 22 Indic languages. (arXiv:2305.15814v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15814
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20379;&#20102;22&#31181;&#21360;&#24230;&#23466;&#27861;&#20013;&#21015;&#20986;&#30340;&#25152;&#26377;21&#31181;&#26412;&#22303;&#25991;&#23383;&#21644;&#32599;&#39532;&#23383;&#27597;&#30340;&#20844;&#24320;&#35821;&#35328;&#37492;&#21035;&#65288;LID&#65289;&#25968;&#25454;&#21644;&#27169;&#22411;&#12290;IndicLID&#26159;&#19978;&#36848;&#35821;&#35328;&#30340;&#26412;&#22303;&#21644;&#32599;&#39532;&#21270;&#33050;&#26412;&#30340;&#35821;&#35328;&#37492;&#21035;&#22120;&#65292;&#36824;&#25552;&#20986;&#20102;&#35299;&#20915;&#32599;&#39532;&#21270;&#25991;&#26412;&#30340;LID&#38382;&#39064;&#30340;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;22&#20010;&#21360;&#24230;&#23466;&#27861;&#20013;&#21015;&#20986;&#30340;&#25152;&#26377;21&#31181;&#26412;&#22303;&#25991;&#23383;&#21644;&#32599;&#39532;&#23383;&#27597;&#30340;&#20844;&#24320;&#35821;&#35328;&#37492;&#21035;&#65288;LID&#65289;&#25968;&#25454;&#21644;&#27169;&#22411;&#12290;&#19982;&#29616;&#26377;&#30340;LID&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;Bhasha-Abhijnaanam&#22312;&#26412;&#22303;&#25991;&#23383;&#25991;&#26412;&#30340;&#35821;&#35328;&#28085;&#30422;&#33539;&#22260;&#26041;&#38754;&#26356;&#20026;&#24191;&#27867;&#65292;&#24182;&#20855;&#26377;&#31454;&#20105;&#21147;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;IndicLID&#26159;&#19978;&#36848;&#35821;&#35328;&#30340;&#26412;&#22303;&#21644;&#32599;&#39532;&#21270;&#33050;&#26412;&#30340;&#35821;&#35328;&#37492;&#21035;&#22120;&#12290;&#23545;&#20110;&#32599;&#39532;&#21270;&#25991;&#26412;&#30340;LID&#65292;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#21644;&#24403;&#35821;&#35328;&#30456;&#20284;&#26102;&#65292;&#20302;LID&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#31616;&#21333;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#22312;&#20219;&#20309;&#35821;&#35328;&#20013;&#65292;&#32599;&#39532;&#21270;&#25991;&#26412;&#30340;&#30740;&#31350;&#37117;&#24456;&#26377;&#38480;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#23545;&#38656;&#35201;&#32599;&#39532;&#21270;&#35821;&#35328;&#37492;&#21035;&#30340;&#20854;&#20182;&#35821;&#35328;&#20063;&#20855;&#26377;&#21442;&#32771;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
We create publicly available language identification (LID) datasets and models in all 22 Indian languages listed in the Indian constitution in both native-script and romanized text. First, we create Bhasha-Abhijnaanam, a language identification test set for native-script as well as romanized text which spans all 22 Indic languages. We also train IndicLID, a language identifier for all the above-mentioned languages in both native and romanized script. For native-script text, it has better language coverage than existing LIDs and is competitive or better than other LIDs. IndicLID is the first LID for romanized text in Indian languages. Two major challenges for romanized text LID are the lack of training data and low-LID performance when languages are similar. We provide simple and effective solutions to these problems. In general, there has been limited work on romanized text in any language, and our findings are relevant to other languages that need romanized language identification. Ou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#23545;&#27604;&#38405;&#35835;&#27169;&#22411;&#65288;Cream&#65289;&#30340;&#26032;&#22411;&#31070;&#32463;&#26550;&#26500;&#65292;&#26088;&#22312;&#36890;&#36807;&#25429;&#25417;&#22797;&#26434;&#32454;&#33410;&#65292;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35821;&#35328;-&#22270;&#20687;&#29702;&#35299;&#33021;&#21147;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35270;&#35273;&#21644;&#36741;&#21161;&#32534;&#30721;&#22120;&#21450;&#23545;&#27604;&#29305;&#24449;&#23545;&#40784;&#25216;&#26415;&#23454;&#29616;&#20102;&#23545;&#35270;&#35273;&#23450;&#20301;&#19978;&#19979;&#25991;&#20013;&#35821;&#35328;&#20449;&#24687;&#30340;&#26356;&#26377;&#25928;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.15080</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#38405;&#35835;&#27169;&#22411;&#21644;&#20923;&#32467;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35270;&#35273;&#23450;&#20301;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Visually-Situated Natural Language Understanding with Contrastive Reading Model and Frozen Large Language Models. (arXiv:2305.15080v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15080
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#23545;&#27604;&#38405;&#35835;&#27169;&#22411;&#65288;Cream&#65289;&#30340;&#26032;&#22411;&#31070;&#32463;&#26550;&#26500;&#65292;&#26088;&#22312;&#36890;&#36807;&#25429;&#25417;&#22797;&#26434;&#32454;&#33410;&#65292;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35821;&#35328;-&#22270;&#20687;&#29702;&#35299;&#33021;&#21147;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35270;&#35273;&#21644;&#36741;&#21161;&#32534;&#30721;&#22120;&#21450;&#23545;&#27604;&#29305;&#24449;&#23545;&#40784;&#25216;&#26415;&#23454;&#29616;&#20102;&#23545;&#35270;&#35273;&#23450;&#20301;&#19978;&#19979;&#25991;&#20013;&#35821;&#35328;&#20449;&#24687;&#30340;&#26356;&#26377;&#25928;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#21050;&#28608;&#20102;&#23545;&#23558;&#20854;&#24212;&#29992;&#25193;&#23637;&#21040;&#35270;&#35273;&#39046;&#22495;&#30340;&#30740;&#31350;&#28526;&#27969;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#22312;&#29983;&#25104;&#25277;&#35937;&#22270;&#20687;&#26631;&#39064;&#21644;&#20419;&#36827;&#33258;&#28982;&#23545;&#35805;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#25991;&#26412;&#20016;&#23500;&#30340;&#22270;&#20687;&#19978;&#30340;&#34920;&#29616;&#20173;&#38656;&#35201;&#25913;&#36827;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#23545;&#27604;&#38405;&#35835;&#27169;&#22411;&#65288;Cream&#65289;&#30340;&#26032;&#22411;&#31070;&#32463;&#26550;&#26500;&#65292;&#26088;&#22312;&#36890;&#36807;&#25429;&#25417;&#29616;&#26377;&#26041;&#27861;&#24120;&#24120;&#24573;&#35270;&#30340;&#22797;&#26434;&#32454;&#33410;&#65292;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35821;&#35328;-&#22270;&#20687;&#29702;&#35299;&#33021;&#21147;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;Cream&#32467;&#21512;&#20102;&#35270;&#35273;&#21644;&#36741;&#21161;&#32534;&#30721;&#22120;&#65292;&#24182;&#20511;&#21161;&#23545;&#27604;&#29305;&#24449;&#23545;&#40784;&#25216;&#26415;&#26469;&#23454;&#29616;&#23545;&#22270;&#20687;&#20013;&#35270;&#35273;&#23450;&#20301;&#19978;&#19979;&#25991;&#20013;&#35821;&#35328;&#20449;&#24687;&#30340;&#26356;&#26377;&#25928;&#29702;&#35299;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24357;&#21512;&#20102;&#35270;&#35273;&#19982;&#35821;&#35328;&#29702;&#35299;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20026;&#26356;&#22797;&#26434;&#30340;&#25991;&#26723;&#26234;&#33021;&#21161;&#25163;&#30340;&#24320;&#21457;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#36890;&#36807;&#23545;&#19981;&#21516;&#35270;&#35273;&#23450;&#20301;&#19978;&#19979;&#25991;&#20013;&#30340;&#20005;&#26684;&#35780;&#20272;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in Large Language Models (LLMs) have stimulated a surge of research aimed at extending their applications to the visual domain. While these models exhibit promise in generating abstract image captions and facilitating natural conversations, their performance on text-rich images still requires improvement. In this paper, we introduce Contrastive Reading Model (Cream), a novel neural architecture designed to enhance the language-image understanding capability of LLMs by capturing intricate details that are often overlooked in existing methods. Cream combines vision and auxiliary encoders, fortified by a contrastive feature alignment technique, to achieve a more effective comprehension of language information in visually situated contexts within the images. Our approach bridges the gap between vision and language understanding, paving the way for the development of more sophisticated Document Intelligence Assistants. Through rigorous evaluations across diverse visually-sit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AutoPlan&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#25351;&#23548;&#20195;&#29702;&#23436;&#25104;&#22797;&#26434;&#30340;&#20132;&#20114;&#24335;&#20915;&#31574;&#20219;&#21153;&#12290;&#36890;&#36807;&#23558;LLM&#25552;&#31034;&#19982;&#20219;&#21153;&#35299;&#20915;&#35745;&#21010;&#30456;&#32467;&#21512;&#24182;&#36827;&#34892;&#20248;&#21270;&#65292;AutoPlan&#22312;&#27809;&#26377;&#19978;&#19979;&#25991;&#28436;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;ALFWorld&#19978;&#23454;&#29616;&#20102;&#19982;&#20154;&#31867;&#25776;&#20889;&#30340;&#28436;&#31034;&#22522;&#32447;&#30456;&#24403;&#30340;&#25104;&#21151;&#29575;&#65292;&#24182;&#22312;HotpotQA&#19978;&#36229;&#36807;&#20102;8%&#12290;</title><link>http://arxiv.org/abs/2305.15064</link><description>&lt;p&gt;
&#33258;&#21160;&#35268;&#21010;&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20132;&#20114;&#20915;&#31574;&#20219;&#21153;&#30340;&#33258;&#21160;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
AutoPlan: Automatic Planning of Interactive Decision-Making Tasks With Large Language Models. (arXiv:2305.15064v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15064
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AutoPlan&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#25351;&#23548;&#20195;&#29702;&#23436;&#25104;&#22797;&#26434;&#30340;&#20132;&#20114;&#24335;&#20915;&#31574;&#20219;&#21153;&#12290;&#36890;&#36807;&#23558;LLM&#25552;&#31034;&#19982;&#20219;&#21153;&#35299;&#20915;&#35745;&#21010;&#30456;&#32467;&#21512;&#24182;&#36827;&#34892;&#20248;&#21270;&#65292;AutoPlan&#22312;&#27809;&#26377;&#19978;&#19979;&#25991;&#28436;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;ALFWorld&#19978;&#23454;&#29616;&#20102;&#19982;&#20154;&#31867;&#25776;&#20889;&#30340;&#28436;&#31034;&#22522;&#32447;&#30456;&#24403;&#30340;&#25104;&#21151;&#29575;&#65292;&#24182;&#22312;HotpotQA&#19978;&#36229;&#36807;&#20102;8%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23545;&#20110;&#22312;&#29616;&#23454;&#29615;&#22659;&#20013;&#36827;&#34892;&#20915;&#31574;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;LLM&#20013;&#39044;&#35757;&#32451;&#30693;&#35782;&#19982;&#23454;&#38469;&#29615;&#22659;&#35268;&#21017;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#65292;LLM&#22312;&#22797;&#26434;&#30340;&#20915;&#31574;&#20219;&#21153;&#20013;&#32463;&#24120;&#22833;&#36133;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#26114;&#36149;&#30340;&#26799;&#24230;&#35745;&#31639;&#65292;&#35201;&#20040;&#38656;&#35201;&#32791;&#26102;&#30340;&#19978;&#19979;&#25991;&#28436;&#31034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AutoPlan&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24341;&#23548;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#23436;&#25104;&#20132;&#20114;&#24335;&#20915;&#31574;&#20219;&#21153;&#12290;AutoPlan&#36890;&#36807;&#36845;&#20195;&#30340;&#32463;&#39564;&#25910;&#38598;&#21644;&#21453;&#24605;&#65292;&#23558;LLM&#25552;&#31034;&#19982;&#20219;&#21153;&#35299;&#20915;&#35745;&#21010;&#30456;&#32467;&#21512;&#65292;&#24182;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#34429;&#28982;&#26410;&#20351;&#29992;&#19978;&#19979;&#25991;&#28436;&#31034;&#65292;&#20294;AutoPlan&#22312;ALFWorld&#19978;&#23454;&#29616;&#20102;&#19982;&#20154;&#31867;&#25776;&#20889;&#30340;&#28436;&#31034;&#22522;&#32447;&#30456;&#24403;&#30340;&#25104;&#21151;&#29575;&#65292;&#29978;&#33267;&#22312;HotpotQA&#19978;&#36229;&#36807;&#20102;8%&#12290;&#20195;&#30721;&#21487;&#20197;&#22312;https://github.com/owaski/AutoPlan&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent large language models (LLMs) are promising for making decisions in grounded environments. However, LLMs frequently fail in complex decision-making tasks due to the misalignment between the pre-trained knowledge in LLMs and the actual rules in the environment. Existing methods require either costly gradient computation or lengthy in-context demonstrations. In this paper, we propose AutoPlan, an approach to guide LLM-based agents to accomplish interactive decision-making tasks. AutoPlan augments the LLM prompt with a task-solving plan and optimizes it through iterative experience collection and reflection. Our experiments show that AutoPlan, though using no in-context demonstrations, achieves success rates on par with the baselines using human-written demonstrations on ALFWorld and even outperforms them by 8% on HotpotQA. The code is available at https://github.com/owaski/AutoPlan.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#24120;&#35782;&#21028;&#26029;&#26159;&#21542;&#19982;Transformer&#27169;&#22411;&#20013;&#30340;&#21487;&#32534;&#36753;&#21442;&#25968;&#23384;&#22312;&#22240;&#26524;&#20851;&#32852;&#65292;&#24182;&#36890;&#36807;&#25913;&#36827;&#32534;&#36753;&#31639;&#27861;&#21644;&#23618;&#36873;&#25321;&#31574;&#30053;&#65292;&#22312;&#24120;&#35782;&#39046;&#22495;&#20013;&#25552;&#21319;&#20102;&#32534;&#36753;&#25928;&#26524;&#65292;&#20351;&#24471;&#32463;&#36807;&#32534;&#36753;&#30340;GPT-2&#27169;&#22411;&#22312;&#21508;&#27979;&#35797;&#38598;&#19978;&#30340;F1&#20998;&#25968;&#30456;&#36739;&#20110;&#26368;&#20339;&#24494;&#35843;&#22522;&#32447;&#25552;&#39640;&#20102;10.97%&#21644;10.73%&#12290;</title><link>http://arxiv.org/abs/2305.14956</link><description>&lt;p&gt;
&#22312;Transformer&#27169;&#22411;&#20013;&#32534;&#36753;&#24120;&#35782;
&lt;/p&gt;
&lt;p&gt;
Editing Common Sense in Transformers. (arXiv:2305.14956v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14956
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#24120;&#35782;&#21028;&#26029;&#26159;&#21542;&#19982;Transformer&#27169;&#22411;&#20013;&#30340;&#21487;&#32534;&#36753;&#21442;&#25968;&#23384;&#22312;&#22240;&#26524;&#20851;&#32852;&#65292;&#24182;&#36890;&#36807;&#25913;&#36827;&#32534;&#36753;&#31639;&#27861;&#21644;&#23618;&#36873;&#25321;&#31574;&#30053;&#65292;&#22312;&#24120;&#35782;&#39046;&#22495;&#20013;&#25552;&#21319;&#20102;&#32534;&#36753;&#25928;&#26524;&#65292;&#20351;&#24471;&#32463;&#36807;&#32534;&#36753;&#30340;GPT-2&#27169;&#22411;&#22312;&#21508;&#27979;&#35797;&#38598;&#19978;&#30340;F1&#20998;&#25968;&#30456;&#36739;&#20110;&#26368;&#20339;&#24494;&#35843;&#22522;&#32447;&#25552;&#39640;&#20102;10.97%&#21644;10.73%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;Transformer&#27169;&#22411;&#20013;&#30452;&#25509;&#32534;&#36753;&#27169;&#22411;&#21442;&#25968;&#20351;&#24471;&#22312;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#26356;&#26032;&#40657;&#30418;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32534;&#36753;&#26041;&#27861;&#20165;&#22312;&#20851;&#20110;&#30334;&#31185;&#30693;&#35782;&#19988;&#21482;&#26377;&#19968;&#20010;&#27491;&#30830;&#31572;&#26696;&#30340;&#38472;&#36848;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23578;&#26410;&#23545;&#22810;&#20010;&#27491;&#30830;&#31572;&#26696;&#30340;&#24120;&#35782;&#30693;&#35782;&#36827;&#34892;&#30740;&#31350;&#65292;&#20363;&#22914;&#65292;&#19968;&#20010;&#33529;&#26524;&#21487;&#20197;&#26159;&#32511;&#33394;&#25110;&#32418;&#33394;&#20294;&#19981;&#33021;&#26159;&#36879;&#26126;&#30340;&#65292;&#36825;&#23545;&#20110;&#25552;&#39640;Transformer&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#21644;&#23454;&#29992;&#24615;&#21516;&#26679;&#37325;&#35201;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#24120;&#35782;&#21028;&#26029;&#26159;&#21542;&#19982;Transformer&#27169;&#22411;&#20013;&#30340;&#21487;&#32534;&#36753;&#21442;&#25968;&#23384;&#22312;&#22240;&#26524;&#20851;&#32852;&#65292;&#24182;&#32473;&#20986;&#20102;&#32943;&#23450;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#21457;&#29616;&#30452;&#25509;&#24212;&#29992;MEMIT&#32534;&#36753;&#31639;&#27861;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#65292;&#24182;&#36890;&#36807;&#25913;&#21464;&#32534;&#36753;&#26631;&#35760;&#21644;&#25913;&#36827;&#23618;&#36873;&#25321;&#31574;&#30053;&#65292;&#21363;$MEMIT_{CSK}$&#65292;&#25913;&#36827;&#20102;&#23545;&#24120;&#35782;&#39046;&#22495;&#30340;&#32534;&#36753;&#25928;&#26524;&#12290;&#20351;&#29992;$MEMIT_{CSK}$&#32534;&#36753;&#36807;&#30340;GPT-2 Large&#21644;XL&#27169;&#22411;&#22312;PEP3k&#21644;20Q&#27979;&#35797;&#38598;&#19978;&#30340;F1&#20998;&#25968;&#30456;&#36739;&#20110;&#26368;&#20339;&#24494;&#35843;&#22522;&#32447;&#25552;&#39640;&#20102;10.97%&#21644;10.73%&#12290;
&lt;/p&gt;
&lt;p&gt;
Editing model parameters directly in Transformers makes updating black-box models possible without re-training (Meng et al., 2023). However, these editing methods have only been evaluated on statements about encyclopedic knowledge with a single correct answer. Commonsense knowledge with multiple correct answers, e.g., an apple can be green or red but not transparent, has not been studied but is as essential for enhancing transformers' reliability and usefulness. In this paper, we investigate whether commonsense judgments are causally associated with localized, editable parameters in Transformers, and we provide an affirmative answer. We find that directly applying the MEMIT editing algorithm results in sub-par performance and improve it for the commonsense domain by varying edit tokens and improving the layer selection strategy, i.e., $MEMIT_{CSK}$. GPT-2 Large and XL models edited using $MEMIT_{CSK}$ outperform best-fine-tuned baselines by 10.97% and 10.73% F1 scores on PEP3k and 20Q 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35206;&#30422;&#29575;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#31034;&#20363;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;BERTScore-Recall&#24230;&#37327;&#26041;&#27861;&#36873;&#25321;&#26356;&#22909;&#30340;&#31034;&#20363;&#26469;&#23637;&#31034;&#27979;&#35797;&#36755;&#20837;&#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#21516;&#26102;&#36824;&#36890;&#36807;&#25193;&#23637;&#25104;&#38598;&#21512;&#32423;&#21035;&#24230;&#37327;&#26041;&#27861;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#35206;&#30422;&#29575;&#34920;&#29616;&#12290;&#23454;&#39564;&#35777;&#26126;BSR&#26159;&#19978;&#19979;&#25991;&#31034;&#20363;&#36873;&#25321;&#20013;&#20248;&#36234;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#19988;&#23545;&#20110;&#32452;&#21512;&#20219;&#21153;&#65292;&#20351;&#29992;Set-BSR&#36827;&#34892;&#38598;&#21512;&#36873;&#25321;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.14907</link><description>&lt;p&gt;
&#22522;&#20110;&#35206;&#30422;&#29575;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#31034;&#20363;&#36873;&#25321;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Coverage-based Example Selection for In-Context Learning. (arXiv:2305.14907v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35206;&#30422;&#29575;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#31034;&#20363;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;BERTScore-Recall&#24230;&#37327;&#26041;&#27861;&#36873;&#25321;&#26356;&#22909;&#30340;&#31034;&#20363;&#26469;&#23637;&#31034;&#27979;&#35797;&#36755;&#20837;&#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#21516;&#26102;&#36824;&#36890;&#36807;&#25193;&#23637;&#25104;&#38598;&#21512;&#32423;&#21035;&#24230;&#37327;&#26041;&#27861;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#35206;&#30422;&#29575;&#34920;&#29616;&#12290;&#23454;&#39564;&#35777;&#26126;BSR&#26159;&#19978;&#19979;&#25991;&#31034;&#20363;&#36873;&#25321;&#20013;&#20248;&#36234;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#19988;&#23545;&#20110;&#32452;&#21512;&#20219;&#21153;&#65292;&#20351;&#29992;Set-BSR&#36827;&#34892;&#38598;&#21512;&#36873;&#25321;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#22312;&#19968;&#20123;&#20219;&#21153;&#31034;&#20363;&#19978;&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#20174;&#32780;&#23454;&#29616;&#26032;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#36825;&#35201;&#27714;&#36825;&#20123;&#31034;&#20363;&#23545;&#27979;&#35797;&#23454;&#20363;&#20855;&#26377;&#20449;&#24687;&#37327;&#12290;&#26631;&#20934;&#30340;&#26041;&#27861;&#26159;&#29420;&#31435;&#22320;&#23545;&#26368;&#30456;&#20284;&#30340;&#31034;&#20363;&#36827;&#34892;&#25490;&#21517;&#21644;&#36873;&#25321;&#65292;&#36825;&#26679;&#36873;&#25321;&#20986;&#30340;&#31034;&#20363;&#20250;&#37325;&#22797;&#19988;&#36951;&#28431;&#37325;&#35201;&#20449;&#24687;&#12290;&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;BERTScore-Recall&#65288;BSR&#65289;&#36873;&#25321;&#20102;&#26356;&#22909;&#30340;&#31034;&#20363;&#65292;&#36825;&#20123;&#31034;&#20363;&#23637;&#31034;&#20102;&#27979;&#35797;&#36755;&#20837;&#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#22914;&#25512;&#29702;&#27169;&#24335;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25193;&#23637;&#20102;BSR&#21644;&#35768;&#22810;&#26631;&#20934;&#24230;&#37327;&#26041;&#27861;&#65292;&#23558;&#20854;&#36716;&#21270;&#20026;&#26131;&#20110;&#20248;&#21270;&#30340;&#38598;&#21512;&#32423;&#21035;&#24230;&#37327;&#26041;&#27861;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#35206;&#30422;&#36825;&#20123;&#20851;&#38190;&#26041;&#38754;&#12290;&#22312;&#28085;&#30422;6&#20010;&#20219;&#21153;&#30340;15&#20010;&#25968;&#25454;&#38598;&#21644;7&#20010;&#19981;&#21516;&#30340;LLM&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#65288;1&#65289;BSR&#22312;&#19978;&#19979;&#25991;&#31034;&#20363;&#36873;&#25321;&#26041;&#38754;&#26159;&#20248;&#36234;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#65288;2&#65289;&#23545;&#20110;&#32452;&#21512;&#20219;&#21153;&#65292;&#20351;&#29992;Set-BSR&#36827;&#34892;&#38598;&#21512;&#36873;&#25321;&#30340;&#24615;&#33021;&#20248;&#20110;&#29420;&#31435;&#25490;&#21517;&#65292;&#24179;&#22343;&#25552;&#39640;17&#20010;&#30334;&#20998;&#28857;&#65292;&#24182;&#19988;&#23613;&#31649;&#26080;&#38656;&#35757;&#32451;&#20294;&#36229;&#36807;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning (ICL), the ability of large language models to perform novel tasks by conditioning on a prompt with a few task examples, requires these examples to be informative about the test instance. The standard approach of independently ranking and selecting the most similar examples selects redundant examples while omitting important information. In this work, we show that BERTScore-Recall (BSR) selects better examples that demonstrate more of the salient aspects, e.g. reasoning patterns, of the test input. We further extend BSR and many standard metrics to easily optimizable set-level metrics, giving still better coverage of those salient aspects. On 15 datasets spanning 6 tasks and with 7 diverse LLMs, we show that (1) BSR is the superior metric for in-context example selection across the board, and (2) for compositional tasks, set selection using Set-BSR outperforms independent ranking by up to 17 points on average and, despite being training-free, surpasses methods that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26080;&#26631;&#31614;&#27979;&#35797;&#25968;&#25454;&#19979;&#20272;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;LLM&#32622;&#20449;&#24230;&#20998;&#25968;&#35757;&#32451;&#20803;&#27169;&#22411;&#26469;&#25191;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#20934;&#30830;&#24615;&#20272;&#35745;&#20219;&#21153;&#65292;&#24182;&#22312;&#30456;&#20851;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#30340;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.14802</link><description>&lt;p&gt;
&#26080;&#26631;&#31614;&#27979;&#35797;&#25968;&#25454;&#19979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#30340;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Estimating Large Language Model Capabilities without Labeled Test Data. (arXiv:2305.14802v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26080;&#26631;&#31614;&#27979;&#35797;&#25968;&#25454;&#19979;&#20272;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;LLM&#32622;&#20449;&#24230;&#20998;&#25968;&#35757;&#32451;&#20803;&#27169;&#22411;&#26469;&#25191;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#20934;&#30830;&#24615;&#20272;&#35745;&#20219;&#21153;&#65292;&#24182;&#22312;&#30456;&#20851;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#24778;&#20154;&#30340;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#20013;&#21482;&#38656;&#35201;&#20960;&#20010;&#20363;&#23376;&#21363;&#21487;&#25191;&#34892;&#30340;&#33021;&#21147;&#65292;&#20294;ICL&#30340;&#25104;&#21151;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#21464;&#21270;&#24456;&#22823;&#12290;&#22240;&#27492;&#65292;&#24555;&#36895;&#30830;&#23450;ICL&#26159;&#21542;&#36866;&#29992;&#20110;&#26032;&#20219;&#21153;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#30452;&#25509;&#35780;&#20272;ICL&#30340;&#20934;&#30830;&#24615;&#21487;&#33021;&#22312;&#27979;&#35797;&#25968;&#25454;&#26114;&#36149;&#30340;&#24773;&#20917;&#19979;&#21464;&#24471;&#26114;&#36149;&#65292;&#32780;ICL&#26368;&#26377;&#21560;&#24341;&#21147;&#30340;&#27491;&#26159;&#36825;&#20123;&#24773;&#20917;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;ICL&#20934;&#30830;&#24615;&#20272;&#35745;&#20219;&#21153;&#65292;&#21363;&#22312;&#20165;&#32473;&#23450;&#35813;&#20219;&#21153;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#39044;&#27979;LLM&#22312;&#26032;&#20219;&#21153;&#19978;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#26102;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#25191;&#34892;ICL&#20934;&#30830;&#24615;&#20272;&#35745;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;LLM&#32622;&#20449;&#24230;&#20998;&#25968;&#20316;&#20026;&#29305;&#24449;&#26469;&#35757;&#32451;&#20803;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#35206;&#30422;4&#20010;LLM&#21644;3&#20010;&#20219;&#21153;&#38598;&#21512;&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#19978;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20960;&#31181;&#24378;&#30340;&#20934;&#30830;&#24615;&#20272;&#35745;&#22522;&#32447;&#36827;&#34892;&#27604;&#36739;&#12290;&#24179;&#22343;&#32780;&#35328;&#65292;&#20803;&#27169;&#22411;&#20248;&#20110;&#25152;&#26377;&#22522;&#32447;&#65292;&#24182;&#23454;&#29616;&#19982;&#30452;&#25509;&#35780;&#20272;&#30456;&#21516;&#30340;&#20272;&#35745;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have exhibited an impressive ability to perform in-context learning (ICL) from only a few examples, but the success of ICL varies widely from task to task. Thus, it is important to quickly determine whether ICL is applicable to a new task, but directly evaluating ICL accuracy can be expensive in situations where test data is expensive to annotate -- the exact situations where ICL is most appealing. In this paper, we propose the task of ICL accuracy estimation, in which we predict the accuracy of an LLM when doing in-context learning on a new task given only unlabeled data for that task. To perform ICL accuracy estimation, we propose a method that trains a meta-model using LLM confidence scores as features. We compare our method to several strong accuracy estimation baselines on a new benchmark that covers 4 LLMs and 3 task collections. On average, the meta-model improves over all baselines and achieves the same estimation performance as directly evaluating 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;Bradley-Terry-Luce&#27169;&#22411;&#23545;OpenAI&#20844;&#24067;&#30340;&#20154;&#31867;&#20559;&#22909;&#21028;&#26029;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#20154;&#31867;&#20559;&#22909;&#21028;&#26029;&#20013;&#25152;&#34164;&#21547;&#30340;&#22266;&#26377;&#20559;&#22909;&#65292;&#25552;&#20986;&#20102;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#30340;&#31574;&#30053;&#65292;&#24182;&#20026;&#26500;&#24314;&#24179;&#34913;&#30340;&#20154;&#31867;&#20559;&#22909;&#21028;&#26029;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#27934;&#35265;&#12290;</title><link>http://arxiv.org/abs/2305.14702</link><description>&lt;p&gt;
&#36890;&#36807;GPT-4&#20998;&#26512;&#24433;&#21709;&#20154;&#31867;&#20559;&#22909;&#21028;&#26029;&#30340;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
Analyzing Influential Factors in Human Preference Judgments via GPT-4. (arXiv:2305.14702v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;Bradley-Terry-Luce&#27169;&#22411;&#23545;OpenAI&#20844;&#24067;&#30340;&#20154;&#31867;&#20559;&#22909;&#21028;&#26029;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#20154;&#31867;&#20559;&#22909;&#21028;&#26029;&#20013;&#25152;&#34164;&#21547;&#30340;&#22266;&#26377;&#20559;&#22909;&#65292;&#25552;&#20986;&#20102;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#30340;&#31574;&#30053;&#65292;&#24182;&#20026;&#26500;&#24314;&#24179;&#34913;&#30340;&#20154;&#31867;&#20559;&#22909;&#21028;&#26029;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20559;&#22909;&#21028;&#26029;&#22312;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#31526;&#21512;&#20154;&#31867;&#20559;&#22909;&#30340;&#36755;&#20986;&#21644;&#35780;&#20272;&#33258;&#21160;&#25688;&#35201;&#24230;&#37327;&#26041;&#38754;&#20855;&#26377;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36825;&#20123;&#20559;&#22909;&#21028;&#26029;&#30340;&#20849;&#21516;&#24433;&#21709;&#21644;&#22240;&#32032;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#31561;&#38382;&#39064;&#65292;&#30446;&#21069;&#30340;&#30740;&#31350;&#20173;&#36739;&#20026;&#26377;&#38480;&#12290;&#26412;&#25991;&#21033;&#29992;Bradley-Terry-Luce&#27169;&#22411;&#23545;OpenAI&#20844;&#24067;&#30340;&#20154;&#31867;&#20559;&#22909;&#21028;&#26029;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#35782;&#21035;&#20102;&#21487;&#33021;&#24433;&#21709;&#20154;&#31867;&#20559;&#22909;&#21028;&#26029;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#20154;&#31867;&#20559;&#22909;&#21028;&#26029;&#20013;&#25152;&#34164;&#21547;&#30340;&#22266;&#26377;&#20559;&#22909;&#65292;&#24182;&#25552;&#20986;&#20102;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#30340;&#31574;&#30053;&#65292;&#26368;&#21518;&#23545;&#20110;&#22914;&#20309;&#26500;&#24314;&#24179;&#34913;&#30340;&#20154;&#31867;&#20559;&#22909;&#21028;&#26029;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pairwise human judgments are pivotal in guiding large language models (LLMs) to generate outputs that align with human preferences. They are also often used in summarization evaluation, complementing existing automatic metrics. Despite their significance, however, there has been limited research probing these pairwise human judgments. The collective impact and respective weights of factors such as informativeness, coherence, fluency, and factual consistency remain elusive. The impact of hidden factors on the final judgment is also unclear. In this paper, we conduct an in-depth examination of a dataset of pairwise human judgments released by OpenAI. Utilizing the Bradley-Terry-Luce model, we identify key factors that could potentially influence human judgments. Our research uncovers the inherent preferences embedded in human judgments and suggests strategies to boost sample efficiency. Finally, we provide insights on the construction of balanced datasets for human judgment evaluations, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#29992;&#25143;&#22312;&#35780;&#20272;&#27169;&#22411;&#39044;&#27979;&#26102;&#32570;&#20047;&#36275;&#22815;&#20449;&#24687;&#26102;&#19982;QA&#31995;&#32479;&#30340;&#20132;&#20114;&#26041;&#24335;&#65292;&#24182;&#21457;&#29616;&#21363;&#20351;&#32570;&#20047;&#36825;&#20123;&#20449;&#24687;&#65292;&#29992;&#25143;&#20173;&#28982;&#36807;&#24230;&#20381;&#36182;&#20110;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#25552;&#20379;&#30456;&#20851;&#32972;&#26223;&#20449;&#24687;&#26377;&#21161;&#20110;&#20943;&#23569;&#23545;&#38169;&#35823;&#39044;&#27979;&#30340;&#20381;&#36182;&#12290;</title><link>http://arxiv.org/abs/2305.14331</link><description>&lt;p&gt;
&#29992;&#25143;&#23545;&#20110;QA&#31995;&#32479;&#30340;&#32972;&#26223;&#20449;&#24687;&#20381;&#36182;&#30340;&#24433;&#21709;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
What Else Do I Need to Know? The Effect of Background Information on Users' Reliance on QA Systems. (arXiv:2305.14331v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#29992;&#25143;&#22312;&#35780;&#20272;&#27169;&#22411;&#39044;&#27979;&#26102;&#32570;&#20047;&#36275;&#22815;&#20449;&#24687;&#26102;&#19982;QA&#31995;&#32479;&#30340;&#20132;&#20114;&#26041;&#24335;&#65292;&#24182;&#21457;&#29616;&#21363;&#20351;&#32570;&#20047;&#36825;&#20123;&#20449;&#24687;&#65292;&#29992;&#25143;&#20173;&#28982;&#36807;&#24230;&#20381;&#36182;&#20110;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#25552;&#20379;&#30456;&#20851;&#32972;&#26223;&#20449;&#24687;&#26377;&#21161;&#20110;&#20943;&#23569;&#23545;&#38169;&#35823;&#39044;&#27979;&#30340;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
NLP&#31995;&#32479;&#22312;&#36890;&#36807;&#26816;&#32034;&#30456;&#20851;&#19978;&#19979;&#25991;&#26469;&#22238;&#31572;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#27169;&#22411;&#36234;&#26469;&#36234;&#22823;&#65292;&#20165;&#38480;&#20110;&#26816;&#32034;&#21040;&#30340;&#19978;&#19979;&#25991;&#26469;&#38480;&#21046;&#27169;&#22411;&#30340;&#30693;&#35782;&#25110;&#25512;&#29702;&#26159;&#19981;&#21487;&#33021;&#30340;&#19988;&#36890;&#24120;&#20063;&#19981;&#21487;&#21462;&#30340;&#12290;&#36825;&#23548;&#33268;&#20102;&#27169;&#22411;&#20174;&#20013;&#25552;&#21462;&#31572;&#26696;&#30340;&#20449;&#24687;&#19982;&#29992;&#25143;&#29992;&#26469;&#35780;&#20272;&#27169;&#22411;&#39044;&#27979;&#31572;&#26696;&#30340;&#20449;&#24687;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32570;&#20047;&#36275;&#22815;&#20449;&#24687;&#26469;&#35780;&#20272;&#39044;&#27979;&#26102;&#29992;&#25143;&#22914;&#20309;&#19982;QA&#31995;&#32479;&#20132;&#20114;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35810;&#38382;&#26159;&#21542;&#28155;&#21152;&#24517;&#35201;&#30340;&#32972;&#26223;&#20449;&#24687;&#26377;&#21161;&#20110;&#20943;&#23569;&#29992;&#25143;&#23545;&#39044;&#27979;&#30340;&#36807;&#24230;&#20381;&#36182;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#21363;&#20351;&#22312;&#32570;&#20047;&#36275;&#22815;&#20449;&#24687;&#26469;&#35780;&#20272;&#27169;&#22411;&#27491;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#29992;&#25143;&#20173;&#28982;&#20381;&#36182;&#20110;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#25552;&#20379;&#30456;&#20851;&#32972;&#26223;&#20449;&#24687;&#26377;&#21161;&#20110;&#29992;&#25143;&#26356;&#22909;&#22320;&#21457;&#29616;&#27169;&#22411;&#38169;&#35823;&#65292;&#20943;&#23569;&#23545;&#19981;&#27491;&#30830;&#39044;&#27979;&#30340;&#20381;&#36182;&#12290;&#32780;&#32972;&#26223;&#20449;&#24687;&#30340;&#28155;&#21152;&#20063;&#21487;&#33021;&#22686;&#21152;&#29992;&#25143;&#23545;&#27169;&#22411;&#30340;&#36807;&#24230;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;
NLP systems have shown impressive performance at answering questions by retrieving relevant context. However, with the increasingly large models, it is impossible and often undesirable to constrain models' knowledge or reasoning to only the retrieved context. This leads to a mismatch between the information that the models access to derive the answer and the information that is available to the user to assess the model predicted answer. In this work, we study how users interact with QA systems in the absence of sufficient information to assess their predictions. Further, we ask whether adding the requisite background helps mitigate users' over-reliance on predictions. Our study reveals that users rely on model predictions even in the absence of sufficient information needed to assess the model's correctness. Providing the relevant background, however, helps users better catch model errors, reducing over-reliance on incorrect predictions. On the flip side, background information also in
&lt;/p&gt;</description></item><item><title>Dynosaur&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#22686;&#38271;&#27169;&#24335;&#65292;&#29992;&#20110;&#33258;&#21160;&#25972;&#29702;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#12290;&#36890;&#36807;&#21033;&#29992;&#29616;&#26377;&#30340;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;Dynosaur&#33021;&#22815;&#20197;&#36739;&#20302;&#30340;API&#25104;&#26412;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2305.14327</link><description>&lt;p&gt;
Dynosaur: &#19968;&#31181;&#29992;&#20110;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#25972;&#29702;&#30340;&#21160;&#24577;&#22686;&#38271;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Dynosaur: A Dynamic Growth Paradigm for Instruction-Tuning Data Curation. (arXiv:2305.14327v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14327
&lt;/p&gt;
&lt;p&gt;
Dynosaur&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#22686;&#38271;&#27169;&#24335;&#65292;&#29992;&#20110;&#33258;&#21160;&#25972;&#29702;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#12290;&#36890;&#36807;&#21033;&#29992;&#29616;&#26377;&#30340;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;Dynosaur&#33021;&#22815;&#20197;&#36739;&#20302;&#30340;API&#25104;&#26412;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#20248;&#24050;&#32463;&#25104;&#20026;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29702;&#35299;&#25351;&#20196;&#21644;&#29983;&#25104;&#36866;&#24403;&#22238;&#24212;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#25163;&#21160;&#27880;&#37322;&#65292;&#35201;&#20040;&#20351;&#29992;LLM&#65288;&#22914;GPT&#31995;&#21015;&#65289;&#29983;&#25104;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#32463;&#24120;&#24573;&#35270;&#23558;&#25351;&#20196;&#19982;&#29616;&#26377;&#30340;&#27880;&#37322;&#25968;&#25454;&#38598;&#20851;&#32852;&#36215;&#26469;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Dynosaur&#65292;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#25972;&#29702;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#30340;&#21160;&#24577;&#22686;&#38271;&#27169;&#24335;&#12290;&#22522;&#20110;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#20803;&#25968;&#25454;&#65292;&#25105;&#20204;&#20351;&#29992;LLM&#33258;&#21160;&#26500;&#24314;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#65292;&#36890;&#36807;&#35782;&#21035;&#30456;&#20851;&#30340;&#25968;&#25454;&#23383;&#27573;&#24182;&#29983;&#25104;&#36866;&#24403;&#30340;&#25351;&#20196;&#12290;&#36890;&#36807;&#21033;&#29992;&#29616;&#26377;&#30340;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;Dynosaur&#20855;&#26377;&#20197;&#19979;&#20960;&#20010;&#20248;&#28857;&#65306;1&#65289;&#20943;&#23569;&#20102;&#29983;&#25104;&#25351;&#20196;&#30340;API&#25104;&#26412;&#65288;&#20363;&#22914;&#65292;&#36890;&#36807;&#35843;&#29992;GPT-3.5-turbo&#29983;&#25104;80&#19975;&#20010;&#25351;&#20196;&#35843;&#20248;&#26679;&#26412;&#30340;&#25104;&#26412;&#20302;&#20110;12&#32654;&#20803;&#65289;&#65307;2&#65289;&#20026;&#25351;&#20196;&#35843;&#20248;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#65288;&#20363;&#22914;&#65292;&#34920;&#29616;&#20248;&#20110;Alpaca&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction tuning has emerged to enhance the capabilities of large language models (LLMs) to comprehend instructions and generate appropriate responses. Existing methods either manually annotate or employ LLM (e.g., GPT-series) to generate data for instruction tuning. However, they often overlook associating instructions with existing annotated datasets. In this paper, we propose Dynosaur, a dynamic growth paradigm for the automatic curation of instruction-tuning data. Based on the metadata of existing datasets, we use LLMs to automatically construct instruction-tuning data by identifying relevant data fields and generating appropriate instructions.  By leveraging the existing annotated datasets, Dynosaur offers several advantages: 1) it reduces the API cost for generating instructions (e.g., it costs less than $12 USD by calling GPT-3.5-turbo for generating 800K instruction tuning samples; 2) it provides high-quality data for instruction tuning (e.g., it performs better than Alpaca a
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20840;&#23616;&#32467;&#26500;&#30693;&#35782;&#30340;&#36830;&#32493;&#36845;&#20195;&#30340;&#26041;&#24335;&#21435;&#25429;&#33719;&#23454;&#20307;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#25552;&#39640;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#20013;&#20851;&#31995;&#25277;&#21462;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13850</link><description>&lt;p&gt;
&#32467;&#21512;&#20840;&#23616;&#32467;&#26500;&#30693;&#35782;&#30340;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#20851;&#31995;&#25277;&#21462;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Global Structure Knowledge-Guided Relation Extraction Method for Visually-Rich Document. (arXiv:2305.13850v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13850
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20840;&#23616;&#32467;&#26500;&#30693;&#35782;&#30340;&#36830;&#32493;&#36845;&#20195;&#30340;&#26041;&#24335;&#21435;&#25429;&#33719;&#23454;&#20307;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#25552;&#39640;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#20013;&#20851;&#31995;&#25277;&#21462;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#20851;&#31995;&#25552;&#21462;&#65288;VRE&#65289;&#26088;&#22312;&#20174;&#35270;&#35273;&#20016;&#23500;&#30340;&#25991;&#26723;&#20013;&#25552;&#21462;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#22522;&#20110;&#23454;&#20307;&#29305;&#24449;&#21333;&#29420;&#39044;&#27979;&#27599;&#23545;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20294;&#24573;&#30053;&#20102;&#20840;&#23616;&#32467;&#26500;&#20449;&#24687;&#65292;&#21363;&#23454;&#20307;&#23545;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#32570;&#20047;&#20840;&#23616;&#32467;&#26500;&#20449;&#24687;&#21487;&#33021;&#20351;&#27169;&#22411;&#38590;&#20197;&#23398;&#20064;&#38271;&#31243;&#20851;&#31995;&#65292;&#24182;&#23481;&#26131;&#20135;&#29983;&#20914;&#31361;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;GOSE&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20197;&#36845;&#20195;&#30340;&#26041;&#24335;&#25429;&#33719;&#23454;&#20307;&#23545;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#32473;&#23450;&#25991;&#26723;&#30340;&#25195;&#25551;&#22270;&#20687;&#65292;GOSE&#39318;&#20808;&#23545;&#23454;&#20307;&#23545;&#29983;&#25104;&#21021;&#27493;&#30340;&#20851;&#31995;&#39044;&#27979;&#12290;&#31532;&#20108;&#65292;&#22312;&#20808;&#21069;&#36845;&#20195;&#30340;&#39044;&#27979;&#32467;&#26524;&#22522;&#30784;&#19978;&#65292;GOSE&#21033;&#29992;&#20840;&#23616;&#32467;&#26500;&#30693;&#35782;&#36827;&#19968;&#27493;&#25972;&#21512;&#23454;&#20307;&#34920;&#31034;&#12290;&#36825;&#31181;&#8220;&#29983;&#25104;-&#25429;&#33719;-&#25972;&#21512;&#8221;&#27169;&#24335;&#34987;&#22810;&#27425;&#25191;&#34892;&#65292;&#20197;&#20415;&#23454;&#20307;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#33021;&#22815;&#34987;&#24456;&#22909;&#22320;&#25429;&#33719;&#21644;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual relation extraction (VRE) aims to extract relations between entities from visuallyrich documents. Existing methods usually predict relations for each entity pair independently based on entity features but ignore the global structure information, i.e., dependencies between entity pairs. The absence of global structure information may make the model struggle to learn long-range relations and easily predict conflicted results. To alleviate such limitations, we propose a GlObal Structure knowledgeguided relation Extraction (GOSE) framework, which captures dependencies between entity pairs in an iterative manner. Given a scanned image of the document, GOSE firstly generates preliminary relation predictions on entity pairs. Secondly, it mines global structure knowledge based on prediction results of the previous iteration and further incorporates global structure knowledge into entity representations. This "generate-capture-incorporate" schema is performed multiple times so that entit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;mFACT&#65292;&#21487;&#20197;&#22312;&#38750;&#33521;&#35821;&#25688;&#35201;&#20013;&#35780;&#20272;&#20854;&#24544;&#23454;&#24615;&#12290;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#21152;&#26435;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#36328;&#35821;&#35328;&#36716;&#31227;&#20943;&#23569;&#25688;&#35201;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.13632</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#25688;&#35201;&#20013;&#30340;&#24187;&#35273;&#26816;&#27979;&#21644;&#32531;&#35299;
&lt;/p&gt;
&lt;p&gt;
Detecting and Mitigating Hallucinations in Multilingual Summarisation. (arXiv:2305.13632v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13632
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;mFACT&#65292;&#21487;&#20197;&#22312;&#38750;&#33521;&#35821;&#25688;&#35201;&#20013;&#35780;&#20272;&#20854;&#24544;&#23454;&#24615;&#12290;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#21152;&#26435;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#36328;&#35821;&#35328;&#36716;&#31227;&#20943;&#23569;&#25688;&#35201;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24187;&#35273;&#23545;&#20110;&#25277;&#35937;&#25688;&#35201;&#30340;&#31070;&#32463;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#34429;&#28982;&#33258;&#21160;&#20135;&#29983;&#30340;&#25688;&#35201;&#21487;&#33021;&#27969;&#30021;&#65292;&#20294;&#36890;&#24120;&#32570;&#20047;&#23545;&#21407;&#22987;&#25991;&#26723;&#30340;&#24544;&#23454;&#24615;&#12290;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#65292;&#22914;&#36328;&#35821;&#35328;&#36716;&#31227;&#65292;&#36825;&#20010;&#38382;&#39064;&#21464;&#24471;&#26356;&#21152;&#31361;&#20986;&#12290;&#30001;&#20110;&#29616;&#26377;&#30340;&#24544;&#23454;&#24615;&#27979;&#37327;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#20110;&#33521;&#35821;&#65292;&#22240;&#27492;&#22312;&#36328;&#35821;&#35328;&#29615;&#22659;&#20013;&#29978;&#33267;&#34913;&#37327;&#36825;&#31181;&#29616;&#35937;&#30340;&#31243;&#24230;&#20063;&#24456;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20316;&#32773;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;mFACT&#65292;&#36890;&#36807;&#20174;&#22810;&#20010;&#33521;&#35821;&#30340;&#24544;&#23454;&#24615;&#27979;&#37327;&#32467;&#26524;&#20013;&#20511;&#37492;&#32763;&#35793;&#22522;&#30784;&#30693;&#35782;&#20026;&#38750;&#33521;&#35821;&#25688;&#35201;&#35780;&#20272;&#20854;&#24544;&#23454;&#24615;&#12290;&#28982;&#21518;&#65292;&#20182;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#36890;&#36807;&#36328;&#35821;&#35328;&#36716;&#31227;&#20943;&#23569;&#24187;&#35273;&#65292;&#35813;&#26041;&#27861;&#23558;&#27599;&#20010;&#35757;&#32451;&#26679;&#26412;&#30340;&#25439;&#22833;&#20056;&#20197;&#20854;&#24544;&#23454;&#24615;&#24471;&#20998;&#12290;&#36890;&#36807;&#22810;&#31181;&#35821;&#35328;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#20316;&#32773;&#35777;&#26126;&#20102;mFACT&#26159;&#26368;&#36866;&#21512;&#26816;&#27979;&#24187;&#35273;&#30340;&#24230;&#37327;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#20182;&#20204;&#21457;&#29616;&#20182;&#20204;&#30340;&#25552;&#20986;&#30340;&#21152;&#26435;&#26041;&#27861;&#21487;&#20197;&#32531;&#35299;&#24187;&#35273;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hallucinations pose a significant challenge to the reliability of neural models for abstractive summarisation. While automatically generated summaries may be fluent, they often lack faithfulness to the original document. This issue becomes even more pronounced in low-resource settings, such as cross-lingual transfer. With the existing faithful metrics focusing on English, even measuring the extent of this phenomenon in cross-lingual settings is hard. To address this, we first develop a novel metric, mFACT, evaluating the faithfulness of non-English summaries, leveraging translation-based transfer from multiple English faithfulness metrics. We then propose a simple but effective method to reduce hallucinations with a cross-lingual transfer, which weighs the loss of each training example by its faithfulness score. Through extensive experiments in multiple languages, we demonstrate that mFACT is the metric that is most suited to detect hallucinations. Moreover, we find that our proposed l
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#33258;&#21160;&#20107;&#23454;&#26680;&#26597;&#30340;&#26694;&#26550;&#65292;&#24182;&#21253;&#25324;&#20102;&#29420;&#29305;&#30340;&#23376;&#20219;&#21153;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#25991;&#26412;&#65292;&#22270;&#20687;&#65292;&#38899;&#39057;&#21644;&#35270;&#39057;&#36825;&#22235;&#31181;&#27169;&#24577;&#30340;&#29616;&#23454;&#24212;&#29992;&#12290;&#32426;&#24405;&#20102;&#30456;&#20851;&#30340;&#22522;&#20934;&#27169;&#22411;&#65292;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#23616;&#38480;&#24615;&#21644;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2305.13507</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#33258;&#21160;&#20107;&#23454;&#26680;&#26597;&#65306;&#19968;&#20221;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Multimodal Automated Fact-Checking: A Survey. (arXiv:2305.13507v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#33258;&#21160;&#20107;&#23454;&#26680;&#26597;&#30340;&#26694;&#26550;&#65292;&#24182;&#21253;&#25324;&#20102;&#29420;&#29305;&#30340;&#23376;&#20219;&#21153;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#25991;&#26412;&#65292;&#22270;&#20687;&#65292;&#38899;&#39057;&#21644;&#35270;&#39057;&#36825;&#22235;&#31181;&#27169;&#24577;&#30340;&#29616;&#23454;&#24212;&#29992;&#12290;&#32426;&#24405;&#20102;&#30456;&#20851;&#30340;&#22522;&#20934;&#27169;&#22411;&#65292;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#23616;&#38480;&#24615;&#21644;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38169;&#35823;&#20449;&#24687;&#65292;&#21363;&#20107;&#23454;&#19978;&#19981;&#27491;&#30830;&#30340;&#20449;&#24687;&#65292;&#36890;&#24120;&#20197;&#22810;&#31181;&#24418;&#24335;&#20256;&#36798;&#65292;&#20363;&#22914;&#24102;&#26377;&#26631;&#39064;&#30340;&#22270;&#20687;&#12290; &#23427;&#34987;&#20154;&#20204;&#35270;&#20026;&#26356;&#21487;&#20449;&#65292;&#27604;&#20854;&#20165;&#38480;&#20110;&#25991;&#26412;&#30340;&#23545;&#24212;&#29289;&#25193;&#25955;&#36895;&#24230;&#26356;&#24555;&#65292;&#33539;&#22260;&#26356;&#24191;&#12290; &#23613;&#31649;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#28041;&#21450;&#33258;&#21160;&#20107;&#23454;&#26680;&#26597;&#65288;AFC&#65289;&#65292;&#20294;&#20197;&#24448;&#30340;&#35843;&#26597;&#20027;&#35201;&#38598;&#20013;&#22312;&#25991;&#26412;&#35823;&#23548;&#26041;&#38754;&#12290; &#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#25324;&#22810;&#27169;&#24577;&#35823;&#23548;&#29420;&#29305;&#23376;&#20219;&#21153;&#22312;&#20869;&#30340;AFC&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#19978;&#35752;&#35770;&#20102;&#19981;&#21516;&#31038;&#21306;&#25152;&#21457;&#23637;&#30340;&#30456;&#20851;&#26415;&#35821;&#12290; &#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#29616;&#23454;&#19990;&#30028;&#20107;&#23454;&#26680;&#26597;&#20013;&#23384;&#22312;&#30340;&#22235;&#31181;&#27169;&#24577;&#65306;&#25991;&#26412;&#65292;&#22270;&#20687;&#65292;&#38899;&#39057;&#21644;&#35270;&#39057;&#12290; &#25105;&#20204;&#35843;&#26597;&#20102;&#22522;&#20934;&#21644;&#27169;&#22411;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#23616;&#38480;&#24615;&#21644;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Misinformation, i.e. factually incorrect information, is often conveyed in multiple modalities, e.g. an image accompanied by a caption. It is perceived as more credible by humans, and spreads faster and wider than its text-only counterparts. While an increasing body of research investigates automated fact-checking (AFC), previous surveys mostly focus on textual misinformation. In this survey, we conceptualise a framework for AFC including subtasks unique to multimodal misinformation. Furthermore, we discuss related terminological developed in different communities in the context of our framework. We focus on four modalities prevalent in real-world fact-checking: text, image, audio, and video. We survey benchmarks and models, and discuss limitations and promising directions for future research.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CLASS&#30340;&#35774;&#35745;&#26694;&#26550;&#65292;&#29992;&#20110;&#26500;&#24314;&#22522;&#20110;&#23398;&#20064;&#31185;&#23398;&#21407;&#29702;&#30340;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#65288;ITS&#65289;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#25552;&#20379;&#20851;&#38190;&#33021;&#21147;&#20351;ITS&#33021;&#22815;&#25552;&#20379;&#36880;&#27493;&#25351;&#23548;&#21644;&#20419;&#36827;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#65292;&#20174;&#32780;&#25552;&#39640;&#23398;&#29983;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.13272</link><description>&lt;p&gt;
CLASS&#65306;&#22522;&#20110;&#23398;&#20064;&#31185;&#23398;&#21407;&#29702;&#26500;&#24314;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#30340;&#35774;&#35745;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CLASS: A Design Framework for building Intelligent Tutoring Systems based on Learning Science principles. (arXiv:2305.13272v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13272
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CLASS&#30340;&#35774;&#35745;&#26694;&#26550;&#65292;&#29992;&#20110;&#26500;&#24314;&#22522;&#20110;&#23398;&#20064;&#31185;&#23398;&#21407;&#29702;&#30340;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#65288;ITS&#65289;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#25552;&#20379;&#20851;&#38190;&#33021;&#21147;&#20351;ITS&#33021;&#22815;&#25552;&#20379;&#36880;&#27493;&#25351;&#23548;&#21644;&#20419;&#36827;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#65292;&#20174;&#32780;&#25552;&#39640;&#23398;&#29983;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Conversational Learning with Analytical Step-by-Step Strategies&#65288;CLASS&#65289;&#30340;&#35774;&#35745;&#26694;&#26550;&#65292;&#29992;&#20110;&#26500;&#24314;&#30001;&#39640;&#24615;&#33021;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39537;&#21160;&#30340;&#20808;&#36827;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#65288;ITS&#65289;&#12290;CLASS&#26694;&#26550;&#36171;&#20104;ITS&#20004;&#20010;&#20851;&#38190;&#33021;&#21147;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#19968;&#20010;&#31934;&#24515;&#31574;&#21010;&#30340;&#33050;&#25163;&#26550;&#25968;&#25454;&#38598;&#65292;CLASS&#20026;ITS&#25552;&#20379;&#20102;&#24517;&#35201;&#30340;&#38382;&#39064;&#35299;&#20915;&#31574;&#30053;&#65292;&#20351;&#20854;&#33021;&#22815;&#20026;&#23398;&#29983;&#25552;&#20379;&#31867;&#20284;&#23548;&#24072;&#30340;&#36880;&#27493;&#25351;&#23548;&#12290;&#20854;&#27425;&#65292;&#36890;&#36807;&#20351;&#29992;&#21160;&#24577;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;CLASS&#24110;&#21161;ITS&#20419;&#36827;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#65292;&#20419;&#36827;&#26377;&#36259;&#30340;&#23398;&#29983;-&#23548;&#24072;&#23545;&#35805;&#12290;CLASS&#26694;&#26550;&#36824;&#25552;&#20379;&#20102;&#23545;ITS&#20869;&#37096;&#20915;&#31574;&#36807;&#31243;&#30340;&#23453;&#36149;&#27934;&#23519;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#29992;&#25143;&#21453;&#39304;&#30340;&#26080;&#32541;&#38598;&#25104;&#65292;&#23454;&#29616;&#25345;&#32493;&#30340;&#23436;&#21892;&#21644;&#25913;&#36827;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#19968;&#20010;&#21517;&#20026;SPOCK&#30340;&#27010;&#24565;&#39564;&#35777;ITS&#65292;&#23427;&#26159;&#20351;&#29992;CLASS&#26694;&#26550;&#24182;&#19987;&#27880;&#20110;&#22823;&#23398;&#21021;&#32423;&#29983;&#29289;&#23398;&#20869;&#23481;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a design framework called Conversational Learning with Analytical Step-by-Step Strategies (CLASS) for building advanced Intelligent Tutoring Systems (ITS) powered by high-performance Large Language Models (LLMs). The CLASS framework empowers ITS with two key capabilities. First, through a carefully curated scaffolding dataset, CLASS equips ITS with essential problem-solving strategies, enabling it to provide tutor-like, step-by-step guidance to students. Second, by using a dynamic conversational dataset, CLASS assists ITS in facilitating natural language interactions, fostering engaging student-tutor conversations. The CLASS framework also provides valuable insights into ITS' internal decision-making process which allows seamless integration of user feedback, thus enabling continuous refinement and improvement. We also present a proof-of-concept ITS, referred to as SPOCK, which is trained using the CLASS framework with a focus on introductory college-level biology content. A
&lt;/p&gt;</description></item><item><title>ExplainCPE&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31616;&#20307;&#20013;&#25991;&#21307;&#23398;&#22522;&#20934;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#33021;&#21147;&#38382;&#39064;&#12290;&#35813;&#22522;&#20934;&#20998;&#26512;&#20102;ChatGPT&#21644;GPT-4&#30340;&#38169;&#35823;&#65292;&#24182;&#25351;&#20986;&#20102;&#24403;&#21069;&#27169;&#22411;&#22312;&#29702;&#35299;&#25991;&#26412;&#21644;&#35745;&#31639;&#25512;&#29702;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.12945</link><description>&lt;p&gt;
&#35299;&#37322;CPE&#65306;&#20013;&#22269;&#25191;&#19994;&#33647;&#24072;&#32771;&#35797;&#30340;&#33258;&#30001;&#25991;&#26412;&#35299;&#37322;&#22522;&#20934;&#12290;&#65288;arXiv:2305.12945v2 [cs.CL] &#26356;&#26032;&#65289;
&lt;/p&gt;
&lt;p&gt;
ExplainCPE: A Free-text Explanation Benchmark of Chinese Pharmacist Examination. (arXiv:2305.12945v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12945
&lt;/p&gt;
&lt;p&gt;
ExplainCPE&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31616;&#20307;&#20013;&#25991;&#21307;&#23398;&#22522;&#20934;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#33021;&#21147;&#38382;&#39064;&#12290;&#35813;&#22522;&#20934;&#20998;&#26512;&#20102;ChatGPT&#21644;GPT-4&#30340;&#38169;&#35823;&#65292;&#24182;&#25351;&#20986;&#20102;&#24403;&#21069;&#27169;&#22411;&#22312;&#29702;&#35299;&#25991;&#26412;&#21644;&#35745;&#31639;&#25512;&#29702;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;ChatGPT&#21644;GPT-4&#24341;&#39046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#32773;&#27491;&#22312;&#30740;&#31350;&#23427;&#20204;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#20294;&#26159;&#65292;&#23545;LLMs&#30340;&#21487;&#35299;&#37322;&#24615;&#33021;&#21147;&#65292;&#20063;&#23601;&#26159;&#22312;&#32473;&#20986;&#31572;&#26696;&#21518;&#29983;&#25104;&#21407;&#22240;&#30340;&#33021;&#21147;&#65292;&#36824;&#38656;&#35201;&#36827;&#34892;&#26356;&#22810;&#30340;&#30740;&#31350;&#12290;&#29616;&#26377;&#30340;&#35299;&#37322;&#25968;&#25454;&#38598;&#20027;&#35201;&#26159;&#33521;&#35821;&#36890;&#29992;&#30693;&#35782;&#38382;&#39064;&#65292;&#23548;&#33268;&#20027;&#39064;&#21644;&#35821;&#35328;&#22810;&#26679;&#24615;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;&#35821;&#35328;&#20559;&#35265;&#21644;&#29983;&#25104;&#21512;&#29702;&#38382;&#31572;&#25968;&#25454;&#38598;&#20013;&#32570;&#20047;&#21307;&#30103;&#36164;&#28304;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ExplainCPE&#65288;&#36229;&#36807;7k&#20010;&#23454;&#20363;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31616;&#20307;&#20013;&#25991;&#21307;&#23398;&#22522;&#20934;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;ChatGPT&#21644;GPT-4&#30340;&#38169;&#35823;&#65292;&#25351;&#20986;&#20102;&#24403;&#21069;LLMs&#22312;&#29702;&#35299;&#25991;&#26412;&#21644;&#35745;&#31639;&#25512;&#29702;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#22312;&#23454;&#39564;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#19981;&#21516;&#30340;LLMs&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#26377;&#19981;&#21516;&#30340;&#20559;&#22909;&#12290;ExplainCPE&#25552;&#20986;&#20102;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#20294;&#23427;&#23545;&#36827;&#19968;&#27493;&#30740;&#31350;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
As ChatGPT and GPT-4 spearhead the development of Large Language Models (LLMs), more researchers are investigating their performance across various tasks. But more research needs to be done on the interpretability capabilities of LLMs, that is, the ability to generate reasons after an answer has been given. Existing explanation datasets are mostly English-language general knowledge questions, which leads to insufficient thematic and linguistic diversity. To address the language bias and lack of medical resources in generating rationales QA datasets, we present ExplainCPE (over 7k instances), a challenging medical benchmark in Simplified Chinese. We analyzed the errors of ChatGPT and GPT-4, pointing out the limitations of current LLMs in understanding text and computational reasoning. During the experiment, we also found that different LLMs have different preferences for in-context learning. ExplainCPE presents a significant challenge, but its potential for further investigation is prom
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#35821;&#38899;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#30340;&#36890;&#29992;&#21387;&#32553;&#31574;&#30053;&#65292;&#36890;&#36807;&#37325;&#29992;&#27880;&#24847;&#21147;&#26144;&#23556;&#21644;&#33976;&#39311;&#23631;&#34109;&#26469;&#25552;&#39640;&#23398;&#29983;&#27169;&#22411;&#30340;&#35821;&#38899;&#34920;&#31034;&#36136;&#37327;&#65292;&#23454;&#29616;&#20102;&#36739;&#20302;&#30340;&#38169;&#35823;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.11685</link><description>&lt;p&gt;
&#22238;&#25910;&#21644;&#31934;&#39311;&#65306;&#24102;&#26377;&#27880;&#24847;&#21147;&#26144;&#23556;&#37325;&#29992;&#21644;&#33976;&#39311;&#23631;&#34109;&#30340;&#22522;&#20110;Transformer&#30340;&#35821;&#38899;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#30340;&#36890;&#29992;&#21387;&#32553;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Recycle-and-Distill: Universal Compression Strategy for Transformer-based Speech SSL Models with Attention Map Reusing and Masking Distillation. (arXiv:2305.11685v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11685
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#35821;&#38899;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#30340;&#36890;&#29992;&#21387;&#32553;&#31574;&#30053;&#65292;&#36890;&#36807;&#37325;&#29992;&#27880;&#24847;&#21147;&#26144;&#23556;&#21644;&#33976;&#39311;&#23631;&#34109;&#26469;&#25552;&#39640;&#23398;&#29983;&#27169;&#22411;&#30340;&#35821;&#38899;&#34920;&#31034;&#36136;&#37327;&#65292;&#23454;&#29616;&#20102;&#36739;&#20302;&#30340;&#38169;&#35823;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#35821;&#38899;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#22312;&#21508;&#31181;&#35821;&#38899;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#35821;&#38899; SSL &#27169;&#22411;&#20013;&#24222;&#22823;&#30340;&#21442;&#25968;&#25968;&#37327;&#38656;&#35201;&#21387;&#32553;&#25104;&#26356;&#32039;&#20945;&#30340;&#27169;&#22411;&#65292;&#20197;&#20415;&#22312;&#23398;&#26415;&#30028;&#25110;&#23567;&#20844;&#21496;&#20013;&#26356;&#24191;&#27867;&#22320;&#20351;&#29992;&#12290;&#26412;&#30740;&#31350;&#24314;&#35758;&#37325;&#29992;Transformer&#23618;&#20043;&#38388;&#30340;&#27880;&#24847;&#21147;&#26144;&#23556;&#65292;&#22240;&#27492;&#21487;&#20197;&#21024;&#38500;&#38190;&#21644;&#26597;&#35810;&#21442;&#25968;&#65292;&#21516;&#26102;&#20445;&#30041;&#23618;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33976;&#39311;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#23398;&#29983;&#27169;&#22411;&#30340;&#35821;&#38899;&#34920;&#31034;&#36136;&#37327;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#33976;&#39311;&#25439;&#22833;&#65292;&#21033;&#29992;&#36974;&#32617;&#21644;&#26410;&#36974;&#32617;&#30340;&#35821;&#38899;&#24103;&#65292;&#20805;&#20998;&#21033;&#29992;&#25945;&#24072;&#27169;&#22411;&#30340;&#39640;&#36136;&#37327;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#36890;&#29992;&#21387;&#32553;&#31574;&#30053;&#20135;&#29983;&#30340;&#23398;&#29983;&#27169;&#22411;&#22312;SUPERB&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;7.72%&#30340;&#38899;&#32032;&#35823;&#24046;&#29575;&#65288;PER&#65289;&#21644;9.96%&#30340;&#21333;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based speech self-supervised learning (SSL) models, such as HuBERT, show surprising performance in various speech processing tasks. However, huge number of parameters in speech SSL models necessitate the compression to a more compact model for wider usage in academia or small companies. In this study, we suggest to reuse attention maps across the Transformer layers, so as to remove key and query parameters while retaining the number of layers. Furthermore, we propose a novel masking distillation strategy to improve the student model's speech representation quality. We extend the distillation loss to utilize both masked and unmasked speech frames to fully leverage the teacher model's high-quality representation. Our universal compression strategy yields the student model that achieves phoneme error rate (PER) of 7.72% and word error rate (WER) of 9.96% on the SUPERB benchmark.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26159;&#23545;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#29289;&#20307;&#24187;&#35273;&#38382;&#39064;&#36827;&#34892;&#30340;&#31532;&#19968;&#39033;&#31995;&#32479;&#30740;&#31350;&#65292;&#36890;&#36807;&#30740;&#31350;&#21457;&#29616;&#35270;&#35273;&#25351;&#20196;&#21487;&#33021;&#24433;&#21709;&#24187;&#35273;&#65292;&#25552;&#20986;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#25104;&#21151;&#35299;&#20915;&#20102;&#29616;&#26377;&#35780;&#20272;&#26041;&#27861;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2305.10355</link><description>&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#29289;&#20307;&#24187;&#35273;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Evaluating Object Hallucination in Large Vision-Language Models. (arXiv:2305.10355v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26159;&#23545;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#29289;&#20307;&#24187;&#35273;&#38382;&#39064;&#36827;&#34892;&#30340;&#31532;&#19968;&#39033;&#31995;&#32479;&#30740;&#31350;&#65292;&#36890;&#36807;&#30740;&#31350;&#21457;&#29616;&#35270;&#35273;&#25351;&#20196;&#21487;&#33021;&#24433;&#21709;&#24187;&#35273;&#65292;&#25552;&#20986;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#25104;&#21151;&#35299;&#20915;&#20102;&#29616;&#26377;&#35780;&#20272;&#26041;&#27861;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#25496;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22240;&#20026;&#20854;&#20986;&#33394;&#30340;&#35821;&#35328;&#33021;&#21147;&#36817;&#26469;&#24050;&#32463;&#24320;&#22987;&#30740;&#31350;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(LVLM)&#65292;&#24182;&#23558;&#24378;&#22823;&#30340;LLM&#38598;&#25104;&#20110;LVLM&#20013;&#65292;&#20197;&#25552;&#39640;LVLM&#22312;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#34429;&#28982;LVLM&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#27493;&#65292;&#20294;&#26159;&#26412;&#30740;&#31350;&#21457;&#29616;LVLM&#23384;&#22312;&#38271;&#24230;&#24187;&#35273;&#38382;&#39064;&#65292;&#21363;&#23427;&#20204;&#20542;&#21521;&#20110;&#29983;&#25104;&#19982;&#30446;&#26631;&#22270;&#20687;&#19981;&#19968;&#33268;&#30340;&#29289;&#20307;&#25551;&#36848;&#12290;&#20026;&#20102;&#35843;&#26597;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#24320;&#23637;&#20102;&#31532;&#19968;&#39033;&#31995;&#32479;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;LVLM&#20013;&#30340;&#29289;&#20307;&#24187;&#35273;&#12290;&#25105;&#20204;&#23545;&#20960;&#20010;&#20195;&#34920;&#24615;&#30340;LVLM&#36827;&#34892;&#20102;&#35780;&#20272;&#23454;&#39564;&#65292;&#24182;&#34920;&#26126;&#23427;&#20204;&#22823;&#22810;&#25968;&#37117;&#23384;&#22312;&#20005;&#37325;&#30340;&#29289;&#20307;&#24187;&#35273;&#38382;&#39064;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#35270;&#35273;&#25351;&#20196;&#21487;&#33021;&#20250;&#24433;&#21709;&#24187;&#35273;&#65292;&#24182;&#21457;&#29616;&#22312;&#35270;&#35273;&#25351;&#20196;&#20013;&#32463;&#24120;&#20986;&#29616;&#25110;&#19982;&#22270;&#20687;&#20013;&#30340;&#29289;&#20307;&#20849;&#29616;&#30340;&#29289;&#20307;&#65292;&#26356;&#23481;&#26131;&#34987;LVLM&#20135;&#29983;&#24187;&#35273;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#35780;&#20272;&#26041;&#27861;&#21487;&#33021;&#20250;&#21463;&#21040;&#36755;&#20837;&#25351;&#20196;&#30340;&#24433;&#21709;&#65292;&#19981;&#33021;&#36275;&#20197;&#35782;&#21035;&#29289;&#20307;&#24187;&#35273;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35780;&#20272;&#29289;&#20307;&#24187;&#35273;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#25351;&#26631;&#19981;&#20165;&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#29289;&#20307;&#24187;&#35273;&#38382;&#39064;&#65292;&#36824;&#21487;&#20197;&#25552;&#20379;&#26377;&#20851;&#24187;&#35273;&#38382;&#39064;&#20986;&#29616;&#20301;&#32622;&#21644;&#22914;&#20309;&#32531;&#35299;&#23427;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by the superior language abilities of large language models (LLM), large vision-language models (LVLM) have been recently explored by integrating powerful LLMs for improving the performance on complex multimodal tasks. Despite the promising progress on LVLMs, we find that LVLMs suffer from the hallucination problem, i.e. they tend to generate objects that are inconsistent with the target images in the descriptions. To investigate it, this work presents the first systematic study on object hallucination of LVLMs. We conduct the evaluation experiments on several representative LVLMs, and show that they mostly suffer from severe object hallucination issue. We further discuss that the visual instructions may influence the hallucination, and find that: objects that frequently occur in the visual instructions or co-occur with the image objects, are obviously prone to be hallucinated by LVLMs. Besides, we find that existing evaluation methods might be affected by the input instructio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#27169;&#22411;&#65292;&#33021;&#22815;&#25506;&#32034;&#28145;&#24230;&#31070;&#32463;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#25512;&#29702;&#20013;&#30340;&#38750;&#23545;&#31216;&#39640;&#38454;&#29305;&#24449;&#20132;&#20114;&#12290;&#22312;&#20004;&#20010;&#24773;&#24863;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#35782;&#21035;&#24433;&#21709;&#29305;&#24449;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#29305;&#24449;&#20132;&#20114;&#24402;&#22240;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.07224</link><description>&lt;p&gt;
&#38754;&#21521;&#27169;&#22411;&#39044;&#27979;&#35299;&#37322;&#30340;&#38750;&#23545;&#31216;&#29305;&#24449;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
Asymmetric feature interaction for interpreting model predictions. (arXiv:2305.07224v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#27169;&#22411;&#65292;&#33021;&#22815;&#25506;&#32034;&#28145;&#24230;&#31070;&#32463;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#25512;&#29702;&#20013;&#30340;&#38750;&#23545;&#31216;&#39640;&#38454;&#29305;&#24449;&#20132;&#20114;&#12290;&#22312;&#20004;&#20010;&#24773;&#24863;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#35782;&#21035;&#24433;&#21709;&#29305;&#24449;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#29305;&#24449;&#20132;&#20114;&#24402;&#22240;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#27169;&#25311;&#19978;&#19979;&#25991;&#20043;&#38388;&#30340;&#22797;&#26434;&#20132;&#20114;&#65292;&#24182;&#22312;&#19968;&#31995;&#21015;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#26524;&#12290;&#20808;&#21069;&#26377;&#20851;&#29305;&#24449;&#20132;&#20114;&#24402;&#22240;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#23545;&#31216;&#20132;&#20114;&#30340;&#30740;&#31350;&#19978;&#65292;&#23427;&#21482;&#33021;&#35299;&#37322;&#21333;&#20010;&#35789;&#27719;&#32452;&#21512;&#21518;&#23545;&#27169;&#22411;&#39044;&#27979;&#30340;&#38468;&#21152;&#24433;&#21709;&#65292;&#32780;&#26080;&#27861;&#25429;&#25417;&#23548;&#33268;&#27169;&#22411;&#39044;&#27979;&#30340;&#38750;&#23545;&#31216;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#23545;&#31216;&#29305;&#24449;&#20132;&#20114;&#35299;&#37322;&#27169;&#22411;&#65292;&#26088;&#22312;&#25506;&#32034;&#28145;&#24230;&#31070;&#32463;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#25512;&#29702;&#20013;&#30340;&#38750;&#23545;&#31216;&#39640;&#38454;&#29305;&#24449;&#20132;&#20114;&#12290;&#36890;&#36807;&#34920;&#31034;&#25105;&#20204;&#30340;&#35299;&#37322;&#20026;&#19968;&#20010;&#26377;&#21521;&#20132;&#20114;&#22270;&#65292;&#25105;&#20204;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#22270;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#33021;&#22815;&#21457;&#29616;&#38750;&#23545;&#31216;&#29305;&#24449;&#20132;&#20114;&#20316;&#29992;&#12290;&#22312;&#20004;&#20010;&#24773;&#24863;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#35782;&#21035;&#24433;&#21709;&#29305;&#24449;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#29305;&#24449;&#20132;&#20114;&#24402;&#22240;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In natural language processing (NLP), deep neural networks (DNNs) could model complex interactions between context and have achieved impressive results on a range of NLP tasks. Prior works on feature interaction attribution mainly focus on studying symmetric interaction that only explains the additional influence of a set of words in combination, which fails to capture asymmetric influence that contributes to model prediction. In this work, we propose an asymmetric feature interaction attribution explanation model that aims to explore asymmetric higher-order feature interactions in the inference of deep neural NLP models. By representing our explanation with an directed interaction graph, we experimentally demonstrate interpretability of the graph to discover asymmetric feature interactions. Experimental results on two sentiment classification datasets show the superiority of our model against the state-of-the-art feature interaction attribution methods in identifying influential featu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#31890;&#24230;&#36229;&#22270;&#20852;&#36259;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21382;&#21490;&#23545;&#35805;&#25968;&#25454;&#20016;&#23500;&#24403;&#21069;&#23545;&#35805;&#30340;&#19978;&#19979;&#25991;&#65292;&#20174;&#19981;&#21516;&#35282;&#24230;&#25429;&#25417;&#29992;&#25143;&#20852;&#36259;&#12290;&#37319;&#29992;&#36229;&#22270;&#32467;&#26500;&#34920;&#31034;&#22797;&#26434;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#24314;&#27169;&#29992;&#25143;&#30340;&#21382;&#21490;&#23545;&#35805;&#20250;&#35805;&#65292;&#25429;&#25417;&#31895;&#31890;&#24230;&#30340;&#20250;&#35805;&#32423;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2305.04798</link><description>&lt;p&gt;
&#22810;&#31890;&#24230;&#36229;&#22270;&#20852;&#36259;&#24314;&#27169;&#23545;&#35805;&#24335;&#25512;&#33616;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multi-grained Hypergraph Interest Modeling for Conversational Recommendation. (arXiv:2305.04798v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#31890;&#24230;&#36229;&#22270;&#20852;&#36259;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21382;&#21490;&#23545;&#35805;&#25968;&#25454;&#20016;&#23500;&#24403;&#21069;&#23545;&#35805;&#30340;&#19978;&#19979;&#25991;&#65292;&#20174;&#19981;&#21516;&#35282;&#24230;&#25429;&#25417;&#29992;&#25143;&#20852;&#36259;&#12290;&#37319;&#29992;&#36229;&#22270;&#32467;&#26500;&#34920;&#31034;&#22797;&#26434;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#24314;&#27169;&#29992;&#25143;&#30340;&#21382;&#21490;&#23545;&#35805;&#20250;&#35805;&#65292;&#25429;&#25417;&#31895;&#31890;&#24230;&#30340;&#20250;&#35805;&#32423;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#30340;&#22810;&#36718;&#23545;&#35805;&#19982;&#29992;&#25143;&#36827;&#34892;&#20132;&#20114;&#65292;&#26088;&#22312;&#20026;&#29992;&#25143;&#30340;&#21363;&#26102;&#20449;&#24687;&#38656;&#27714;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#25512;&#33616;&#12290;&#23613;&#31649;&#24050;&#32463;&#20570;&#20986;&#20102;&#24456;&#22810;&#26377;&#25928;&#30340;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#65292;&#20294;&#22823;&#22810;&#25968;&#20173;&#28982;&#38598;&#20013;&#22312;&#24403;&#21069;&#23545;&#35805;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#19978;&#65292;&#36890;&#24120;&#20250;&#36935;&#21040;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#32771;&#34385;&#21033;&#29992;&#21382;&#21490;&#23545;&#35805;&#25968;&#25454;&#26469;&#20016;&#23500;&#24403;&#21069;&#23545;&#35805;&#30340;&#26377;&#38480;&#19978;&#19979;&#25991;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#31890;&#24230;&#36229;&#22270;&#20852;&#36259;&#24314;&#27169;&#26041;&#27861;&#65292;&#20197;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#25429;&#25417;&#22797;&#26434;&#21382;&#21490;&#25968;&#25454;&#19979;&#30340;&#29992;&#25143;&#20852;&#36259;&#12290;&#20316;&#20026;&#26680;&#24515;&#24605;&#24819;&#65292;&#25105;&#20204;&#20351;&#29992;&#36229;&#22270;&#26469;&#34920;&#31034;&#21382;&#21490;&#23545;&#35805;&#20013;&#22797;&#26434;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#36229;&#22270;&#32467;&#26500;&#26469;&#24314;&#27169;&#29992;&#25143;&#30340;&#21382;&#21490;&#23545;&#35805;&#20250;&#35805;&#65292;&#24182;&#24418;&#25104;&#19968;&#20010;&#22522;&#20110;&#20250;&#35805;&#30340;&#36229;&#22270;&#65292;&#35813;&#36229;&#22270;&#25429;&#25417;&#20102;&#31895;&#31890;&#24230;&#30340;&#20250;&#35805;&#32423;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational recommender system (CRS) interacts with users through multi-turn dialogues in natural language, which aims to provide high-quality recommendations for user's instant information need. Although great efforts have been made to develop effective CRS, most of them still focus on the contextual information from the current dialogue, usually suffering from the data scarcity issue. Therefore, we consider leveraging historical dialogue data to enrich the limited contexts of the current dialogue session.  In this paper, we propose a novel multi-grained hypergraph interest modeling approach to capture user interest beneath intricate historical data from different perspectives. As the core idea, we employ hypergraph to represent complicated semantic relations underlying historical dialogues. In our approach, we first employ the hypergraph structure to model users' historical dialogue sessions and form a session-based hypergraph, which captures coarse-grained, session-level relation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20020;&#24202;&#35797;&#39564;&#25253;&#21578;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#26816;&#32034;&#25903;&#25345;&#20107;&#23454;&#26469;&#30830;&#23450;&#33258;&#28982;&#35821;&#35328;&#38472;&#36848;&#21644;CTR&#20043;&#38388;&#30340;&#25512;&#29702;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2305.03598</link><description>&lt;p&gt;
NLI4CT&#65306;&#38754;&#21521;&#20020;&#24202;&#35797;&#39564;&#25253;&#21578;&#30340;&#22810;&#35777;&#25454;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
NLI4CT: Multi-Evidence Natural Language Inference for Clinical Trial Reports. (arXiv:2305.03598v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20020;&#24202;&#35797;&#39564;&#25253;&#21578;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#26816;&#32034;&#25903;&#25345;&#20107;&#23454;&#26469;&#30830;&#23450;&#33258;&#28982;&#35821;&#35328;&#38472;&#36848;&#21644;CTR&#20043;&#38388;&#30340;&#25512;&#29702;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#35299;&#37322;&#21644;&#26816;&#32034;&#29992;&#20110;&#25903;&#25345;&#20020;&#24202;&#20915;&#31574;&#30340;&#21307;&#23398;&#35777;&#25454;&#65311;&#22810;&#24180;&#26469;&#65292;&#31215;&#32047;&#19979;&#26469;&#30340;&#20020;&#24202;&#35797;&#39564;&#25253;&#21578;&#21253;&#21547;&#20102;&#21457;&#23637;&#20010;&#24615;&#21270;&#21307;&#23398;&#25152;&#24517;&#38656;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#25214;&#21040;&#26368;&#20339;&#30340;&#23454;&#39564;&#27835;&#30103;&#35777;&#25454;&#65292;&#25163;&#21160;&#26816;&#26597;&#36229;&#36807;400,000&#20010;&#20020;&#24202;&#35797;&#39564;&#25253;&#21578;&#26159;&#23454;&#38469;&#19978;&#19981;&#21487;&#34892;&#30340;&#12290;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20801;&#35768;&#21487;&#25193;&#23637;&#35745;&#31639;&#25991;&#26412;&#34164;&#21547;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;NLI&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#35821;&#26009;&#24211;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#20043;&#21069;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#26080;&#27861;&#25429;&#25417;CTR&#25512;&#29702;&#30340;&#20840;&#37096;&#22797;&#26434;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36164;&#28304;&#65292;&#20197;&#25512;&#36827;&#20851;&#20110;CTR&#25512;&#29702;&#30340;NLI&#30740;&#31350;&#12290;&#35813;&#36164;&#28304;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#20219;&#21153;&#12290;&#39318;&#20808;&#65292;&#30830;&#23450;&#33258;&#28982;&#35821;&#35328;&#38472;&#36848;&#21644;CTR&#20043;&#38388;&#30340;&#25512;&#29702;&#20851;&#31995;&#12290;&#20854;&#27425;&#65292;&#26816;&#32034;&#25903;&#25345;&#20107;&#23454;&#20197;&#35777;&#26126;&#39044;&#27979;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;NLI4CT&#65292;&#19968;&#20010;&#22522;&#20110;CTR&#30340;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
How can we interpret and retrieve medical evidence to support clinical decisions? Clinical trial reports (CTR) amassed over the years contain indispensable information for the development of personalized medicine. However, it is practically infeasible to manually inspect over 400,000+ clinical trial reports in order to find the best evidence for experimental treatments. Natural Language Inference (NLI) offers a potential solution to this problem, by allowing the scalable computation of textual entailment. However, existing NLI models perform poorly on biomedical corpora, and previously published datasets fail to capture the full complexity of inference over CTRs. In this work, we present a novel resource to advance research on NLI for reasoning on CTRs. The resource includes two main tasks. Firstly, to determine the inference relation between a natural language statement, and a CTR. Secondly, to retrieve supporting facts to justify the predicted relation. We provide NLI4CT, a corpus of
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#35821;&#35328;&#25552;&#31034;&#19979;&#30340;&#22870;&#21169;&#26102;&#38388;&#20559;&#22909;&#65292;&#24182;&#21457;&#29616;GPT&#22312;&#20855;&#26377;&#36739;&#24369;&#26410;&#26469;&#26102;&#24577;&#30340;&#35821;&#35328;&#19979;&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#32784;&#24515;&#65292;&#36825;&#19982;&#20351;&#29992;&#35813;&#35821;&#35328;&#30340;&#20154;&#31867;&#30340;&#20559;&#22909;&#30456;&#20284;&#12290;</title><link>http://arxiv.org/abs/2305.02531</link><description>&lt;p&gt;
&#35821;&#35328;&#12289;&#26102;&#38388;&#20559;&#22909;&#21644;&#28040;&#36153;&#34892;&#20026;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
Language, Time Preferences, and Consumer Behavior: Evidence from Large Language Models. (arXiv:2305.02531v1 [econ.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02531
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#35821;&#35328;&#25552;&#31034;&#19979;&#30340;&#22870;&#21169;&#26102;&#38388;&#20559;&#22909;&#65292;&#24182;&#21457;&#29616;GPT&#22312;&#20855;&#26377;&#36739;&#24369;&#26410;&#26469;&#26102;&#24577;&#30340;&#35821;&#35328;&#19979;&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#32784;&#24515;&#65292;&#36825;&#19982;&#20351;&#29992;&#35813;&#35821;&#35328;&#30340;&#20154;&#31867;&#30340;&#20559;&#22909;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#23545;&#25105;&#20204;&#23545;&#26102;&#38388;&#21644;&#22870;&#21169;&#30340;&#24863;&#30693;&#26377;&#24456;&#22823;&#30340;&#24433;&#21709;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#24403;&#20197;&#19981;&#21516;&#30340;&#35821;&#35328;&#35810;&#38382;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#23427;&#20204;&#26159;&#21542;&#26174;&#31034;&#20986;&#19981;&#21516;&#30340;&#22870;&#21169;&#26102;&#38388;&#20559;&#22909;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#36873;&#25321;&#26159;&#21542;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#36873;&#25321;&#12290;&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;GPT-3.5&#65288;&#20197;&#19979;&#31616;&#31216;GPT&#65289;&#22312;&#22810;&#31181;&#35821;&#35328;&#25552;&#31034;&#19979;&#30340;&#21709;&#24212;&#65292;&#25506;&#32034;&#20102;&#36739;&#23567;&#12289;&#36739;&#26089;&#30340;&#22870;&#21169;&#21644;&#36739;&#22823;&#12289;&#36739;&#26202;&#30340;&#22870;&#21169;&#20043;&#38388;&#30340;&#20559;&#22909;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#24403;&#20197;&#35821;&#20041;&#21547;&#20041;&#36739;&#24369;&#30340;&#26410;&#26469;&#26102;&#24577;&#21442;&#32771;&#65288;FTR&#65289;&#65292;&#22914;&#24503;&#35821;&#21644;&#27721;&#35821;&#65292;&#20026;&#25552;&#31034;&#35821;&#26102;&#65292;GPT&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#32784;&#24515;&#65292;&#30456;&#27604;&#33521;&#35821;&#21644;&#27861;&#35821;&#31561;&#20855;&#26377;&#24378;&#22823;FTR&#30340;&#35821;&#35328;&#12290;&#36825;&#20123;&#21457;&#29616;&#19982;&#29616;&#26377;&#25991;&#29486;&#19968;&#33268;&#65292;&#24182;&#34920;&#26126;&#20102;GPT&#30340;&#36873;&#25321;&#19982;&#36825;&#20123;&#35821;&#35328;&#30340;&#20351;&#29992;&#32773;&#30340;&#20559;&#22909;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#28982;&#32780;&#65292;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#36739;&#26089;&#25110;&#36739;&#26202;&#22870;&#21169;&#30340;&#20559;&#22909;&#24182;&#27809;&#26377;&#38543;&#30528;&#22870;&#21169;&#24046;&#24322;&#31995;&#32479;&#22320;&#25913;&#21464;&#65292;&#36825;&#34920;&#26126;&#20102;&#19968;&#31181;&#35789;&#20856;&#24207;&#20248;&#20808;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language has a strong influence on our perceptions of time and rewards. This raises the question of whether large language models, when asked in different languages, show different preferences for rewards over time and if their choices are similar to those of humans. In this study, we analyze the responses of GPT-3.5 (hereafter referred to as GPT) to prompts in multiple languages, exploring preferences between smaller, sooner rewards and larger, later rewards. Our results show that GPT displays greater patience when prompted in languages with weak future tense references (FTR), such as German and Mandarin, compared to languages with strong FTR, like English and French. These findings are consistent with existing literature and suggest a correlation between GPT's choices and the preferences of speakers of these languages. However, further analysis reveals that the preference for earlier or later rewards does not systematically change with reward gaps, indicating a lexicographic preferen
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#25105;&#35780;&#20272;&#24341;&#23548;&#35299;&#30721;&#25552;&#39640;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#32463;&#36807;&#26657;&#20934;&#30340;&#33258;&#21160;&#26631;&#20934;&#25506;&#32034;&#25512;&#29702;&#25628;&#32034;&#31354;&#38388;&#65292;&#20351;&#25628;&#32034;&#33021;&#22815;&#20135;&#29983;&#26356;&#39640;&#36136;&#37327;&#30340;&#26368;&#32456;&#39044;&#27979;&#32467;&#26524;&#65307;&#20351;&#29992;&#33258;&#25105;&#35780;&#20272;&#24341;&#23548;&#30340;&#38543;&#26426;&#26463;&#25628;&#32034;&#22312;&#20135;&#29983;&#25512;&#29702;&#38142;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#20043;&#38388;&#24179;&#34913;&#26435;&#34913;&#65292;&#36866;&#24212;&#22810;&#25968;&#25237;&#31080;&#65292;&#24182;&#19988;&#21487;&#20197;&#20934;&#30830;&#21028;&#26029;&#36923;&#36753;&#38169;&#35823;&#65292;&#25552;&#39640;&#19968;&#33268;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.00633</link><description>&lt;p&gt;
&#20998;&#35299;&#22686;&#24378;&#25512;&#29702;&#30340;&#33258;&#25105;&#35780;&#20272;&#24341;&#23548;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Decomposition Enhances Reasoning via Self-Evaluation Guided Decoding. (arXiv:2305.00633v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#25105;&#35780;&#20272;&#24341;&#23548;&#35299;&#30721;&#25552;&#39640;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#32463;&#36807;&#26657;&#20934;&#30340;&#33258;&#21160;&#26631;&#20934;&#25506;&#32034;&#25512;&#29702;&#25628;&#32034;&#31354;&#38388;&#65292;&#20351;&#25628;&#32034;&#33021;&#22815;&#20135;&#29983;&#26356;&#39640;&#36136;&#37327;&#30340;&#26368;&#32456;&#39044;&#27979;&#32467;&#26524;&#65307;&#20351;&#29992;&#33258;&#25105;&#35780;&#20272;&#24341;&#23548;&#30340;&#38543;&#26426;&#26463;&#25628;&#32034;&#22312;&#20135;&#29983;&#25512;&#29702;&#38142;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#20043;&#38388;&#24179;&#34913;&#26435;&#34913;&#65292;&#36866;&#24212;&#22810;&#25968;&#25237;&#31080;&#65292;&#24182;&#19988;&#21487;&#20197;&#20934;&#30830;&#21028;&#26029;&#36923;&#36753;&#38169;&#35823;&#65292;&#25552;&#39640;&#19968;&#33268;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#26463;&#25628;&#32034;&#32467;&#21512;&#33258;&#25105;&#35780;&#20272;&#24341;&#23548;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#32463;&#36807;&#26657;&#20934;&#30340;&#33258;&#21160;&#26631;&#20934;&#25506;&#32034;&#25512;&#29702;&#25628;&#32034;&#31354;&#38388;&#12290;&#36825;&#20351;&#24471;&#26377;&#25928;&#25628;&#32034;&#33021;&#22815;&#20135;&#29983;&#26356;&#39640;&#36136;&#37327;&#30340;&#26368;&#32456;&#39044;&#27979;&#32467;&#26524;&#12290;&#20351;&#29992;&#33258;&#25105;&#35780;&#20272;&#24341;&#23548;&#30340;&#38543;&#26426;&#26463;&#25628;&#32034;&#65292;&#25105;&#20204;&#22312;&#20135;&#29983;&#25512;&#29702;&#38142;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#20043;&#38388;&#24179;&#34913;&#26435;&#34913;&#65292;&#20174;&#32780;&#33021;&#22815;&#36866;&#24212;&#22810;&#25968;&#25237;&#31080;&#65292;&#24182;&#22312;GSM8K&#12289;AQUA&#21644;StrategyQA&#22522;&#20934;&#27979;&#35797;&#20013;&#20197;&#23569;&#37327;&#31034;&#20363;&#20934;&#30830;&#24615;&#20998;&#21035;&#36229;&#36234;&#23545;&#24212;&#30340;Codex-backboned&#22522;&#32447;$6.34\%$&#12289;$9.56\%$&#21644;$5.46\%$&#12290;&#23545;&#25105;&#20204;&#30340;&#20998;&#35299;&#24335;&#25512;&#29702;&#20998;&#26512;&#21457;&#29616;&#65292;&#23427;&#21487;&#20197;&#25351;&#20986;&#36923;&#36753;&#38169;&#35823;&#24182;&#23548;&#33268;&#26356;&#39640;&#30340;&#19968;&#33268;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an effective prompting approach that integrates self-evaluation guidance through stochastic beam search. Our approach explores the reasoning search space using a well-calibrated automatic criterion. This enables an efficient search to produce higher-quality final predictions. With the self-evaluation guided stochastic beam search, we also balance the quality--diversity trade-off in the generation of reasoning chains. This allows our approach to adapt well with majority voting and surpass the corresponding Codex-backboned baselines by $6.34\%$, $9.56\%$, and $5.46\%$ on the GSM8K, AQUA, and StrategyQA benchmarks, respectively, in few-shot accuracy. Analysis of our decompositional reasoning finds it pinpoints logic failures and leads to higher consistency and robustness.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;ChatGPT&#20316;&#20026;&#20851;&#38190;&#35789;&#29983;&#25104;&#22120;&#36827;&#34892;&#20102;&#21021;&#27493;&#30740;&#31350;&#65292;&#21457;&#29616;&#20854;&#22312;&#21508;&#20010;&#26041;&#38754;&#30340;&#24615;&#33021;&#34920;&#29616;&#33391;&#22909;&#65292;&#29305;&#21035;&#26159;&#22312;&#22810;&#39046;&#22495;&#20851;&#38190;&#35789;&#29983;&#25104;&#26041;&#38754;&#12290;ChatGPT&#20173;&#38754;&#20020;&#29983;&#25104;&#32570;&#22833;&#20851;&#38190;&#35789;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2303.13001</link><description>&lt;p&gt;
ChatGPT&#26159;&#19968;&#27454;&#22909;&#30340;&#20851;&#38190;&#35789;&#29983;&#25104;&#22120;&#21527;&#65311;&#21021;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Is ChatGPT A Good Keyphrase Generator? A Preliminary Study. (arXiv:2303.13001v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;ChatGPT&#20316;&#20026;&#20851;&#38190;&#35789;&#29983;&#25104;&#22120;&#36827;&#34892;&#20102;&#21021;&#27493;&#30740;&#31350;&#65292;&#21457;&#29616;&#20854;&#22312;&#21508;&#20010;&#26041;&#38754;&#30340;&#24615;&#33021;&#34920;&#29616;&#33391;&#22909;&#65292;&#29305;&#21035;&#26159;&#22312;&#22810;&#39046;&#22495;&#20851;&#38190;&#35789;&#29983;&#25104;&#26041;&#38754;&#12290;ChatGPT&#20173;&#38754;&#20020;&#29983;&#25104;&#32570;&#22833;&#20851;&#38190;&#35789;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#30340;&#20986;&#29616;&#24341;&#36215;&#20102;&#35745;&#31639;&#35821;&#35328;&#23398;&#30028;&#30340;&#37325;&#35270;&#12290;&#20026;&#20102;&#23637;&#31034;&#20854;&#20316;&#20026;&#20851;&#38190;&#35789;&#29983;&#25104;&#22120;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#23545;ChatGPT&#36827;&#34892;&#20102;&#21021;&#27493;&#35780;&#20272;&#20197;&#29992;&#20110;&#20851;&#38190;&#35789;&#29983;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20854;&#22312;&#21508;&#20010;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#20851;&#38190;&#35789;&#29983;&#25104;&#25552;&#31034;&#65292;&#20851;&#38190;&#35789;&#29983;&#25104;&#22810;&#26679;&#24615;&#65292;&#22810;&#39046;&#22495;&#20851;&#38190;&#35789;&#29983;&#25104;&#21644;&#38271;&#25991;&#26412;&#29702;&#35299;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#22522;&#20110;&#20845;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#37319;&#29992;OpenAI&#24314;&#35758;&#30340;&#25552;&#31034;&#65292;&#24182;&#23558;&#20854;&#25193;&#23637;&#20026;&#20845;&#20010;&#20505;&#36873;&#25552;&#31034;&#12290;&#25105;&#20204;&#21457;&#29616;ChatGPT&#22312;&#25152;&#26377;&#20845;&#20010;&#20505;&#36873;&#25552;&#31034;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#35266;&#23519;&#21040;&#20102;&#36731;&#24494;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;ChatGPT&#26377;&#24456;&#22823;&#30340;&#20851;&#38190;&#35789;&#29983;&#25104;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;ChatGPT&#22312;&#29983;&#25104;&#32570;&#22833;&#20851;&#38190;&#35789;&#26041;&#38754;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;&#26368;&#21518;&#65292;&#22312;&#26368;&#21518;&#19968;&#33410;&#20013;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20123;&#38480;&#21046;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of ChatGPT has recently garnered significant attention from the computational linguistics community. To demonstrate its capabilities as a keyphrase generator, we conduct a preliminary evaluation of ChatGPT for the keyphrase generation task. We evaluate its performance in various aspects, including keyphrase generation prompts, keyphrase generation diversity, multi-domain keyphrase generation, and long document understanding. Our evaluation is based on six benchmark datasets, and we adopt the prompt suggested by OpenAI while extending it to six candidate prompts. We find that ChatGPT performs exceptionally well on all six candidate prompts, with minor performance differences observed across the datasets. Based on our findings, we conclude that ChatGPT has great potential for keyphrase generation. Moreover, we discover that ChatGPT still faces challenges when it comes to generating absent keyphrases. Meanwhile, in the final section, we also present some limitations and futu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#27979;&#35797;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#20013;&#22522;&#20110;&#31867;&#21035;&#30340;&#24863;&#30693;&#22522;&#30784;&#30340;&#26694;&#26550;&#65292;&#24182;&#27604;&#36739;&#20102;&#22522;&#20110;&#21407;&#22411;&#21644;&#26679;&#26412;&#30340;&#34920;&#31034;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.04053</link><description>&lt;p&gt;
&#25551;&#36848;&#19968;&#20010;Aucklet&#65306;&#29983;&#25104;&#22522;&#20110;&#24863;&#30693;&#30340;&#31867;&#21035;&#25551;&#36848;
&lt;/p&gt;
&lt;p&gt;
Describe me an Aucklet: Generating Grounded Perceptual Category Descriptions. (arXiv:2303.04053v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#27979;&#35797;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#20013;&#22522;&#20110;&#31867;&#21035;&#30340;&#24863;&#30693;&#22522;&#30784;&#30340;&#26694;&#26550;&#65292;&#24182;&#27604;&#36739;&#20102;&#22522;&#20110;&#21407;&#22411;&#21644;&#26679;&#26412;&#30340;&#34920;&#31034;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#28436;&#35762;&#32773;&#21487;&#20197;&#29983;&#25104;&#20174;&#23454;&#20363;&#32423;&#21035;&#25277;&#35937;&#20986;&#26469;&#30340;&#24863;&#30693;&#27010;&#24565;&#25551;&#36848;&#12290;&#27492;&#22806;&#65292;&#20854;&#20182;&#28436;&#35762;&#32773;&#21487;&#20197;&#21033;&#29992;&#36825;&#20123;&#25551;&#36848;&#26469;&#23398;&#20064;&#36825;&#20123;&#27010;&#24565;&#30340;&#20020;&#26102;&#34920;&#31034;&#12290;&#22312;&#35821;&#35328;&#21644;&#35270;&#35273;&#39046;&#22495;&#65292;&#23398;&#20064;&#21644;&#20351;&#29992;&#25277;&#35937;&#24863;&#30693;&#27010;&#24565;&#30340;&#38382;&#39064;&#36824;&#19981;&#22815;&#30740;&#31350;&#12290;&#36825;&#20010;&#38382;&#39064;&#23545;&#22810;&#27169;&#24577;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;&#39046;&#22495;&#20063;&#38750;&#24120;&#30456;&#20851;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#27979;&#35797;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#20013;&#22522;&#20110;&#31867;&#21035;&#30340;&#24863;&#30693;&#22522;&#30784;&#30340;&#26694;&#26550;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#29983;&#25104;&#21644;&#35299;&#37322;&#35270;&#35273;&#31867;&#21035;&#30340;&#25551;&#36848;&#12290;&#25105;&#20204;&#29992;&#35299;&#37322;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#24615;&#33021;&#26469;&#34913;&#37327;&#20004;&#20010;&#27169;&#22411;&#30340;&#20132;&#27969;&#25104;&#21151;&#24230;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#26159;&#24863;&#30693;&#22522;&#30784;&#30340;&#19968;&#20010;&#25351;&#26631;&#12290;&#21033;&#29992;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#22522;&#20110;&#21407;&#22411;&#21644;&#26679;&#26412;&#30340;&#34920;&#31034;&#30340;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20132;&#27969;&#25104;&#21151;&#25581;&#31034;&#20102;&#24615;&#33021;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human speakers can generate descriptions of perceptual concepts, abstracted from the instance-level. Moreover, such descriptions can be used by other speakers to learn provisional representations of those concepts. Learning and using abstract perceptual concepts is under-investigated in the language-and-vision field. The problem is also highly relevant to the field of representation learning in multi-modal NLP. In this paper, we introduce a framework for testing category-level perceptual grounding in multi-modal language models. In particular, we train separate neural networks to generate and interpret descriptions of visual categories. We measure the communicative success of the two models with the zero-shot classification performance of the interpretation model, which we argue is an indicator of perceptual grounding. Using this framework, we compare the performance of prototype- and exemplar-based representations. Finally, we show that communicative success exposes performance issues
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#38405;&#35835;&#24182;&#22870;&#21169;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#38405;&#35835;Atari&#28216;&#25103;&#24320;&#21457;&#32773;&#21457;&#24067;&#30340;&#25351;&#23548;&#25163;&#20876;&#65292;&#20197;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;Atari&#28216;&#25103;&#20013;&#30340;&#25928;&#29575;&#12290;&#35813;&#26694;&#26550;&#21253;&#21547;&#19968;&#20010;QA&#25552;&#21462;&#27169;&#22359;&#21644;&#19968;&#20010;&#25512;&#29702;&#27169;&#22359;&#65292;&#33021;&#22815;&#20174;&#25351;&#23548;&#25163;&#20876;&#20013;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#65292;&#24182;&#35780;&#20272;&#29289;&#20307;&#19982;&#26234;&#33021;&#20307;&#30340;&#20132;&#20114;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.04449</link><description>&lt;p&gt;
&#38405;&#35835;&#24182;&#33719;&#24471;&#22238;&#25253;&#65306;&#22312;&#19982;&#25351;&#23548;&#25163;&#20876;&#30340;&#24110;&#21161;&#19979;&#23398;&#20064;&#29609;Atari&#28216;&#25103;
&lt;/p&gt;
&lt;p&gt;
Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals. (arXiv:2302.04449v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04449
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#38405;&#35835;&#24182;&#22870;&#21169;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#38405;&#35835;Atari&#28216;&#25103;&#24320;&#21457;&#32773;&#21457;&#24067;&#30340;&#25351;&#23548;&#25163;&#20876;&#65292;&#20197;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;Atari&#28216;&#25103;&#20013;&#30340;&#25928;&#29575;&#12290;&#35813;&#26694;&#26550;&#21253;&#21547;&#19968;&#20010;QA&#25552;&#21462;&#27169;&#22359;&#21644;&#19968;&#20010;&#25512;&#29702;&#27169;&#22359;&#65292;&#33021;&#22815;&#20174;&#25351;&#23548;&#25163;&#20876;&#20013;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#65292;&#24182;&#35780;&#20272;&#29289;&#20307;&#19982;&#26234;&#33021;&#20307;&#30340;&#20132;&#20114;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#39640;&#26679;&#26412;&#22797;&#26434;&#24615;&#19968;&#30452;&#26159;&#24378;&#21270;&#23398;&#20064;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#23398;&#20064;&#25191;&#34892;&#20219;&#21153;&#30340;&#26041;&#24335;&#19981;&#20165;&#20165;&#26159;&#36890;&#36807;&#20132;&#20114;&#25110;&#28436;&#31034;&#65292;&#36824;&#21253;&#25324;&#38405;&#35835;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#25991;&#26723;&#65292;&#20363;&#22914;&#25351;&#23548;&#25163;&#20876;&#12290;&#25351;&#23548;&#25163;&#20876;&#21644;&#32500;&#22522;&#39029;&#38754;&#26159;&#26368;&#20016;&#23500;&#30340;&#25968;&#25454;&#20043;&#19968;&#65292;&#23427;&#20204;&#21487;&#20197;&#25552;&#20379;&#26377;&#20851;&#23453;&#36149;&#29305;&#24449;&#12289;&#31574;&#30053;&#12289;&#20219;&#21153;&#29305;&#23450;&#30340;&#29615;&#22659;&#21160;&#24577;&#21644;&#22870;&#21169;&#32467;&#26500;&#30340;&#20449;&#24687;&#65292;&#22240;&#27492;&#25105;&#20204;&#20551;&#35774;&#21033;&#29992;&#20154;&#20889;&#30340;&#25351;&#23548;&#25163;&#20876;&#26469;&#24110;&#21161;&#23398;&#20064;&#29305;&#23450;&#20219;&#21153;&#30340;&#31574;&#30053;&#23558;&#23548;&#33268;&#26356;&#39640;&#25928;&#21644;&#26356;&#20248;&#31168;&#30340;&#26234;&#33021;&#20307;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#38405;&#35835;&#24182;&#22870;&#21169;&#30340;&#26694;&#26550;&#12290;&#38405;&#35835;&#24182;&#22870;&#21169;&#36890;&#36807;&#38405;&#35835;Atari&#28216;&#25103;&#24320;&#21457;&#32773;&#21457;&#24067;&#30340;&#25351;&#23548;&#25163;&#20876;&#26469;&#21152;&#36895;RL&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#19968;&#20010;QA&#25552;&#21462;&#27169;&#22359;&#65292;&#29992;&#20110;&#25552;&#21462;&#21644;&#24635;&#32467;&#25351;&#23548;&#25163;&#20876;&#20013;&#30340;&#30456;&#20851;&#20449;&#24687;&#65292;&#20197;&#21450;&#19968;&#20010;&#25512;&#29702;&#27169;&#22359;&#65292;&#26681;&#25454;&#25351;&#23548;&#25163;&#20876;&#20013;&#30340;&#20449;&#24687;&#35780;&#20272;&#29289;&#20307;-&#26234;&#33021;&#20307;&#30340;&#20132;&#20114;&#25928;&#26524;&#12290;&#19968;&#20010;&#36741;&#21161;&#30340;&#21453;&#39304;&#26426;&#21046;&#21487;&#20197;&#25552;&#39640;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
High sample complexity has long been a challenge for RL. On the other hand, humans learn to perform tasks not only from interaction or demonstrations, but also by reading unstructured text documents, e.g., instruction manuals. Instruction manuals and wiki pages are among the most abundant data that could inform agents of valuable features and policies or task-specific environmental dynamics and reward structures. Therefore, we hypothesize that the ability to utilize human-written instruction manuals to assist learning policies for specific tasks should lead to a more efficient and better-performing agent. We propose the Read and Reward framework. Read and Reward speeds up RL algorithms on Atari games by reading manuals released by the Atari game developers. Our framework consists of a QA Extraction module that extracts and summarizes relevant information from the manual and a Reasoning module that evaluates object-agent interactions based on information from the manual. An auxiliary re
&lt;/p&gt;</description></item><item><title>ZipLM&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20219;&#20309;&#32473;&#23450;&#30340;&#25512;&#29702;&#29615;&#22659;&#20013;&#23454;&#29616;&#19982;&#30446;&#26631;&#36816;&#34892;&#36895;&#24230;&#30456;&#21305;&#37197;&#30340;&#26368;&#20808;&#36827;&#21387;&#32553;&#27169;&#22411;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;ZipLM&#22312;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#21462;&#24471;&#20102;&#26368;&#20339;&#30340;&#26435;&#34913;&#65292;&#24182;&#19988;&#20197;&#26356;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.04089</link><description>&lt;p&gt;
ZipLM: &#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#24863;&#30693;&#32467;&#26500;&#21270;&#21098;&#26525;
&lt;/p&gt;
&lt;p&gt;
ZipLM: Inference-Aware Structured Pruning of Language Models. (arXiv:2302.04089v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04089
&lt;/p&gt;
&lt;p&gt;
ZipLM&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20219;&#20309;&#32473;&#23450;&#30340;&#25512;&#29702;&#29615;&#22659;&#20013;&#23454;&#29616;&#19982;&#30446;&#26631;&#36816;&#34892;&#36895;&#24230;&#30456;&#21305;&#37197;&#30340;&#26368;&#20808;&#36827;&#21387;&#32553;&#27169;&#22411;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;ZipLM&#22312;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#21462;&#24471;&#20102;&#26368;&#20339;&#30340;&#26435;&#34913;&#65292;&#24182;&#19988;&#20197;&#26356;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#31361;&#30772;&#24615;&#24615;&#33021;&#32473;&#35745;&#31639;&#21644;&#37096;&#32626;&#25104;&#26412;&#24102;&#26469;&#20102;&#37325;&#22823;&#36127;&#25285;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;ZipLM&#30340;&#26032;&#22411;&#32467;&#26500;&#21270;&#21387;&#32553;&#26041;&#27861;&#65292;&#21521;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#36808;&#36827;&#12290;ZipLM&#22312;&#36798;&#21040;&#19968;&#32452;&#30446;&#26631;&#25512;&#29702;&#26102;&#36895;&#24230;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#20219;&#20309;&#32473;&#23450;&#30340;&#25512;&#29702;&#29615;&#22659;&#20013;&#21305;&#37197;&#19968;&#32452;&#30446;&#26631;&#36816;&#34892;&#26102;&#21152;&#36895;&#24230;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#32473;&#23450;&#27169;&#22411;&#12289;&#25968;&#25454;&#38598;&#12289;&#25512;&#29702;&#29615;&#22659;&#20197;&#21450;&#19968;&#32452;&#21152;&#36895;&#24230;&#30446;&#26631;&#65292;ZipLM&#36845;&#20195;&#22320;&#35782;&#21035;&#24182;&#21024;&#38500;&#25439;&#22833;&#26102;&#38271;&#26435;&#34913;&#26368;&#24046;&#30340;&#32452;&#20214;&#12290;&#19982;&#20808;&#21069;&#19987;&#38376;&#29992;&#20110;&#21518;&#35757;&#32451;/&#19968;&#27425;&#24615;&#25110;&#36880;&#28176;&#21387;&#32553;&#35774;&#32622;&#65292;&#24182;&#19988;&#20165;&#36866;&#29992;&#20110;&#29305;&#23450;&#30340;&#27169;&#22411;&#23478;&#26063;&#65288;&#22914;BERT&#65288;&#32534;&#30721;&#22120;&#65289;&#25110;GPT&#65288;&#35299;&#30721;&#22120;&#65289;&#65289;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;ZipLM&#22312;&#25152;&#26377;&#36825;&#20123;&#35774;&#32622;&#20013;&#29983;&#25104;&#20102;&#26368;&#20808;&#36827;&#30340;&#21387;&#32553;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#19982;&#20808;&#21069;&#30340;&#33976;&#39311;&#21644;&#20462;&#21098;&#26041;&#27861;&#30456;&#27604;&#65292;ZipLM&#20197;&#26356;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The breakthrough performance of large language models (LLMs) comes with major computational footprints and high deployment costs. In this paper, we progress towards resolving this problem by proposing a novel structured compression approach for LLMs, called ZipLM. ZipLM achieves state-of-the-art accuracy-vs-speedup, while matching a set of desired target runtime speedups in any given inference environment. Specifically, given a model, a dataset, an inference environment, as well as a set of speedup targets, ZipLM iteratively identifies and removes components with the worst loss-runtime trade-off. Unlike prior methods that specialize in either the post-training/one-shot or the gradual compression setting, and only for specific families of models such as BERT (encoder) or GPT (decoder), ZipLM produces state-of-the-art compressed models across all these settings. Furthermore, ZipLM achieves superior results for a fraction of the computational cost relative to prior distillation and prunin
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25317;&#26377;&#20016;&#23500;&#30340;&#20107;&#20214;&#30693;&#35782;&#65292;&#20960;&#20046;&#24635;&#26159;&#23558;&#21487;&#33021;&#20107;&#20214;&#30340;&#25551;&#36848;&#27604;&#19981;&#21487;&#33021;&#20107;&#20214;&#30340;&#25551;&#36848;&#36171;&#20104;&#26356;&#39640;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.01488</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20107;&#20214;&#30693;&#35782;&#65306;&#19981;&#21487;&#33021;&#24615;&#21644;&#19981;&#22826;&#21487;&#33021;&#24615;&#20043;&#38388;&#30340;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Event knowledge in large language models: the gap between the impossible and the unlikely. (arXiv:2212.01488v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01488
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25317;&#26377;&#20016;&#23500;&#30340;&#20107;&#20214;&#30693;&#35782;&#65292;&#20960;&#20046;&#24635;&#26159;&#23558;&#21487;&#33021;&#20107;&#20214;&#30340;&#25551;&#36848;&#27604;&#19981;&#21487;&#33021;&#20107;&#20214;&#30340;&#25551;&#36848;&#36171;&#20104;&#26356;&#39640;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#35821;&#26009;&#24211;&#20013;&#30340;&#35789;&#20849;&#29616;&#27169;&#24335;&#21253;&#21547;&#30528;&#24847;&#24819;&#19981;&#21040;&#30340;&#27010;&#24565;&#30693;&#35782;&#12290;&#36890;&#36807;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26469;&#39044;&#27979;&#19978;&#19979;&#25991;&#20013;&#30340;&#35789;&#35821;&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#21033;&#29992;&#36825;&#20123;&#27169;&#24335;&#65292;&#22312;&#38656;&#35201;&#19990;&#30028;&#30693;&#35782;&#30340;&#21508;&#31181;&#35821;&#20041;&#20219;&#21153;&#19978;&#21462;&#24471;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#20851;&#20110;LLMs&#30340;&#35821;&#20041;&#33021;&#21147;&#30340;&#37325;&#35201;&#20294;&#40092;&#20026;&#30740;&#31350;&#30340;&#38382;&#39064;&#26159;&#23427;&#20204;&#26159;&#21542;&#33719;&#24471;&#20102;&#24120;&#35265;&#20107;&#20214;&#30340;&#19968;&#33324;&#21270;&#30693;&#35782;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#20116;&#20010;&#39044;&#35757;&#32451;&#30340;LLMs&#65288;&#20174;2018&#24180;&#30340;BERT&#21040;2023&#24180;&#30340;MPT&#65289;&#26159;&#21542;&#27604;&#21516;&#19968;&#20107;&#20214;&#30340;&#19981;&#22826;&#21487;&#33021;&#30340;&#29256;&#26412;&#26356;&#21487;&#33021;&#22320;&#20998;&#37197;&#32473;&#21512;&#29702;&#30340;&#20195;&#29702;-&#24739;&#32773;&#30456;&#20114;&#20316;&#29992;&#12290;&#20351;&#29992;&#19977;&#20010;&#31934;&#24515;&#31574;&#21010;&#30340;&#26368;&#23567;&#21477;&#23545;&#38598;&#21512;&#65288;&#24635;&#25968;n=1,215&#65289;&#65292;&#25105;&#20204;&#21457;&#29616;&#39044;&#35757;&#32451;&#30340;LLMs&#25317;&#26377;&#30456;&#24403;&#22823;&#30340;&#20107;&#20214;&#30693;&#35782;&#65292;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#20998;&#24067;&#24335;&#35821;&#35328;&#27169;&#22411;&#12290;&#29305;&#21035;&#26159;&#65292;&#23427;&#20204;&#20960;&#20046;&#24635;&#26159;&#23558;&#21487;&#33021;&#20107;&#20214;&#19982;&#19981;&#21487;&#33021;&#20107;&#20214;&#30456;&#27604;&#36171;&#20104;&#26356;&#39640;&#30340;&#21487;&#33021;&#24615;&#65288;&#25945;&#24072;&#20080;&#20102;&#31508;&#35760;&#26412;&#30005;&#33041;&#30456;&#23545;&#20110;&#31508;&#35760;&#26412;&#30005;&#33041;&#20080;&#20102;&#25945;&#24072;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Word co-occurrence patterns in language corpora contain a surprising amount of conceptual knowledge. Large language models (LLMs), trained to predict words in context, leverage these patterns to achieve impressive performance on diverse semantic tasks requiring world knowledge. An important but understudied question about LLMs' semantic abilities is whether they acquire generalized knowledge of common events. Here, we test whether five pre-trained LLMs (from 2018's BERT to 2023's MPT) assign higher likelihood to plausible descriptions of agent-patient interactions than to minimally different implausible versions of the same event. Using three curated sets of minimal sentence pairs (total n=1,215), we found that pre-trained LLMs possess substantial event knowledge, outperforming other distributional language models. In particular, they almost always assign higher likelihood to possible vs. impossible events (The teacher bought the laptop vs. The laptop bought the teacher). However, LLMs
&lt;/p&gt;</description></item><item><title>Aksharantar&#26159;&#26368;&#22823;&#30340;&#20844;&#24320;&#21487;&#29992;&#30340;&#21360;&#24230;&#35821;&#35328;&#36716;&#20889;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;2600&#19975;&#20010;&#36716;&#20889;&#23545;&#65292;&#28085;&#30422;21&#31181;&#21360;&#24230;&#35821;&#35328;&#21644;12&#31181;&#25991;&#23383;&#12290;&#21516;&#26102;&#65292;&#23427;&#20063;&#25552;&#20379;&#20102;&#19968;&#20010;&#27979;&#35797;&#38598;&#65292;&#29992;&#20110;&#23545;&#36716;&#20889;&#27169;&#22411;&#36827;&#34892;&#32454;&#31890;&#24230;&#20998;&#26512;&#12290;&#21033;&#29992;&#35813;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#36716;&#20889;&#27169;&#22411;IndicXlit&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2205.03018</link><description>&lt;p&gt;
Aksharantar: &#24320;&#25918;&#30340;&#21360;&#24230;&#35821;&#35328;&#36716;&#20889;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#65292;&#38754;&#21521;&#26410;&#26469;&#30340;&#21313;&#20159;&#29992;&#25143;
&lt;/p&gt;
&lt;p&gt;
Aksharantar: Open Indic-language Transliteration datasets and models for the Next Billion Users. (arXiv:2205.03018v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.03018
&lt;/p&gt;
&lt;p&gt;
Aksharantar&#26159;&#26368;&#22823;&#30340;&#20844;&#24320;&#21487;&#29992;&#30340;&#21360;&#24230;&#35821;&#35328;&#36716;&#20889;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;2600&#19975;&#20010;&#36716;&#20889;&#23545;&#65292;&#28085;&#30422;21&#31181;&#21360;&#24230;&#35821;&#35328;&#21644;12&#31181;&#25991;&#23383;&#12290;&#21516;&#26102;&#65292;&#23427;&#20063;&#25552;&#20379;&#20102;&#19968;&#20010;&#27979;&#35797;&#38598;&#65292;&#29992;&#20110;&#23545;&#36716;&#20889;&#27169;&#22411;&#36827;&#34892;&#32454;&#31890;&#24230;&#20998;&#26512;&#12290;&#21033;&#29992;&#35813;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#36716;&#20889;&#27169;&#22411;IndicXlit&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21360;&#24230;&#35821;&#35328;&#29615;&#22659;&#20013;&#65292;&#36716;&#20889;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#20351;&#29992;&#20102;&#22810;&#31181;&#25991;&#23383;&#65292;&#24182;&#24191;&#27867;&#20351;&#29992;&#32599;&#39532;&#21270;&#36755;&#20837;&#12290;&#28982;&#32780;&#65292;&#20844;&#24320;&#21487;&#29992;&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#25968;&#25454;&#38598;&#24456;&#23569;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;Aksharantar&#65292;&#36825;&#26159;&#26368;&#22823;&#30340;&#20844;&#24320;&#21487;&#29992;&#30340;&#21360;&#24230;&#35821;&#35328;&#36716;&#20889;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20174;&#21333;&#35821;&#21644;&#24179;&#34892;&#35821;&#26009;&#24211;&#20013;&#25366;&#25496;&#65292;&#24182;&#25910;&#38598;&#20154;&#24037;&#27880;&#37322;&#25968;&#25454;&#32780;&#21019;&#24314;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#26469;&#33258;&#19977;&#20010;&#35821;&#35328;&#23478;&#26063;&#12289;&#20351;&#29992;12&#31181;&#25991;&#23383;&#30340;21&#31181;&#21360;&#24230;&#35821;&#35328;&#30340;2600&#19975;&#20010;&#36716;&#20889;&#23545;&#12290;Aksharantar&#27604;&#29616;&#26377;&#25968;&#25454;&#38598;&#22823;21&#20493;&#65292;&#20063;&#26159;7&#31181;&#35821;&#35328;&#21644;1&#20010;&#35821;&#35328;&#23478;&#26063;&#20013;&#39318;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;Aksharantar&#27979;&#35797;&#38598;&#65292;&#21253;&#21547;103k&#20010;&#21333;&#35789;&#23545;&#65292;&#28085;&#30422;19&#31181;&#35821;&#35328;&#65292;&#21487;&#20197;&#23545;&#26412;&#22320;&#35789;&#28304;&#35789;&#12289;&#22806;&#26469;&#35789;&#12289;&#39057;&#32321;&#35789;&#21644;&#32597;&#35265;&#35789;&#30340;&#36716;&#20889;&#27169;&#22411;&#36827;&#34892;&#32454;&#31890;&#24230;&#20998;&#26512;&#12290;&#21033;&#29992;&#35757;&#32451;&#38598;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;IndicXlit&#65292;&#36825;&#26159;&#19968;&#20010;&#25552;&#39640;&#20102;&#24615;&#33021;&#30340;&#22810;&#35821;&#35328;&#36716;&#20889;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transliteration is very important in the Indian language context due to the usage of multiple scripts and the widespread use of romanized inputs. However, few training and evaluation sets are publicly available. We introduce Aksharantar, the largest publicly available transliteration dataset for Indian languages created by mining from monolingual and parallel corpora, as well as collecting data from human annotators. The dataset contains 26 million transliteration pairs for 21 Indic languages from 3 language families using 12 scripts. Aksharantar is 21 times larger than existing datasets and is the first publicly available dataset for 7 languages and 1 language family. We also introduce the Aksharantar testset comprising 103k word pairs spanning 19 languages that enables a fine-grained analysis of transliteration models on native origin words, foreign words, frequent words, and rare words. Using the training set, we trained IndicXlit, a multilingual transliteration model that improves 
&lt;/p&gt;</description></item><item><title>pysentimiento&#26159;&#19968;&#20010;&#22810;&#35821;&#35328;&#30340;Python&#24037;&#20855;&#21253;&#65292;&#29992;&#20110;&#35266;&#28857;&#25366;&#25496;&#21644;&#31038;&#20132;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#25552;&#20379;&#20102;&#26131;&#20110;&#20351;&#29992;&#30340;&#24211;&#21644;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#21033;&#29992;&#36825;&#20123;&#25216;&#26415;&#36827;&#34892;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2106.09462</link><description>&lt;p&gt;
pysentimiento: &#19968;&#20010;&#29992;&#20110;&#35266;&#28857;&#25366;&#25496;&#21644;&#31038;&#20132;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;Python&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
pysentimiento: A Python Toolkit for Opinion Mining and Social NLP tasks. (arXiv:2106.09462v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.09462
&lt;/p&gt;
&lt;p&gt;
pysentimiento&#26159;&#19968;&#20010;&#22810;&#35821;&#35328;&#30340;Python&#24037;&#20855;&#21253;&#65292;&#29992;&#20110;&#35266;&#28857;&#25366;&#25496;&#21644;&#31038;&#20132;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#25552;&#20379;&#20102;&#26131;&#20110;&#20351;&#29992;&#30340;&#24211;&#21644;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#21033;&#29992;&#36825;&#20123;&#25216;&#26415;&#36827;&#34892;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20174;&#29992;&#25143;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#25552;&#21462;&#35266;&#28857;&#21644;&#20449;&#24687;&#24341;&#36215;&#20102;&#24456;&#22823;&#30340;&#20852;&#36259;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#31038;&#20132;&#23186;&#20307;&#20013;&#20869;&#23481;&#30340;&#21069;&#25152;&#26410;&#26377;&#30340;&#25968;&#37327;&#12290;&#28982;&#32780;&#65292;&#31038;&#20250;&#30740;&#31350;&#20154;&#21592;&#22312;&#37319;&#29992;&#26368;&#20808;&#36827;&#30340;&#24037;&#20855;&#36827;&#34892;&#36825;&#20123;&#20219;&#21153;&#26102;&#20250;&#36935;&#21040;&#19968;&#20123;&#38382;&#39064;&#65292;&#22240;&#20026;&#36825;&#20123;&#24037;&#20855;&#36890;&#24120;&#33853;&#21518;&#20110;&#21830;&#19994;API&#65292;&#19981;&#36866;&#29992;&#20110;&#38500;&#33521;&#35821;&#20197;&#22806;&#30340;&#20854;&#20182;&#35821;&#35328;&#65292;&#25110;&#32773;&#23545;&#38750;&#19987;&#23478;&#26469;&#35828;&#38750;&#24120;&#22797;&#26434;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;pysentimiento&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#22810;&#35821;&#35328;Python&#24037;&#20855;&#21253;&#65292;&#19987;&#20026;&#35266;&#28857;&#25366;&#25496;&#21644;&#20854;&#20182;&#31038;&#20132;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#32780;&#35774;&#35745;&#12290;&#36825;&#20010;&#24320;&#28304;&#24211;&#25552;&#20379;&#20102;&#26131;&#20110;&#20351;&#29992;&#30340;Python&#24211;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#29992;&#20110;&#35199;&#29677;&#29273;&#35821;&#12289;&#33521;&#35821;&#12289;&#24847;&#22823;&#21033;&#35821;&#21644;&#33889;&#33796;&#29273;&#35821;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#21487;&#20197;&#35753;&#30740;&#31350;&#20154;&#21592;&#21033;&#29992;&#36825;&#20123;&#25216;&#26415;&#12290;&#25105;&#20204;&#23545;&#20960;&#31181;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#12289;&#35821;&#35328;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21253;&#25324;&#23545;&#32467;&#26524;&#20844;&#24179;&#24615;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the extraction of opinions and information from user-generated text has attracted a lot of interest, largely due to the unprecedented volume of content in Social Media. However, social researchers face some issues in adopting cutting-edge tools for these tasks, as they are usually behind commercial APIs, unavailable for other languages than English, or very complex to use for non-experts. To address these issues, we present pysentimiento, a comprehensive multilingual Python toolkit designed for opinion mining and other Social NLP tasks. This open-source library brings state-of-the-art models for Spanish, English, Italian, and Portuguese in an easy-to-use Python library, allowing researchers to leverage these techniques. We present a comprehensive assessment of performance for several pre-trained language models across a variety of tasks, languages, and datasets, including an evaluation of fairness in the results.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;&#30340;&#20998;&#31867;&#65292;&#37325;&#28857;&#20851;&#27880;&#20855;&#26377;&#21152;&#27861;&#32422;&#26463;&#21644;&#20854;&#20182;&#25216;&#26415;&#26465;&#20214;&#30340;&#38382;&#39064;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#19982;&#23384;&#22312;&#24615;&#23454;&#25968;&#29702;&#35770;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2106.02397</link><description>&lt;p&gt;
&#20851;&#20110;&#36830;&#32493;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;&#30340;&#20998;&#31867;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Classifying Continuous Constraint Satisfaction Problems. (arXiv:2106.02397v5 [cs.CC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.02397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;&#30340;&#20998;&#31867;&#65292;&#37325;&#28857;&#20851;&#27880;&#20855;&#26377;&#21152;&#27861;&#32422;&#26463;&#21644;&#20854;&#20182;&#25216;&#26415;&#26465;&#20214;&#30340;&#38382;&#39064;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#19982;&#23384;&#22312;&#24615;&#23454;&#25968;&#29702;&#35770;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;&#65288;CCSP&#65289;&#26159;&#20855;&#26377;&#21306;&#38388;&#22495; $U \subset \mathbb{R}$ &#30340;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;&#65288;CSP&#65289;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#23545;&#28385;&#36275;&#23384;&#22312;&#24615;&#23454;&#25968;&#29702;&#35770;&#65288;ER-complete&#65289;&#30340;CCSP&#36827;&#34892;&#20998;&#31867;&#12290;&#20026;&#20102;&#23450;&#20041;&#36825;&#20010;&#31867;&#21035;&#65292;&#25105;&#20204;&#39318;&#20808;&#32771;&#34385;&#38382;&#39064;ETR&#65292;&#23427;&#20063;&#20195;&#34920;&#23384;&#22312;&#24615;&#23454;&#25968;&#29702;&#35770;&#12290;&#22312;&#36825;&#20010;&#38382;&#39064;&#30340;&#23454;&#20363;&#20013;&#65292;&#25105;&#20204;&#34987;&#32473;&#23450;&#19968;&#20010;&#24418;&#22914; $\exists x_1, \ldots, x_n \in \mathbb{R} : \Phi(x_1, \ldots, x_n)$ &#30340;&#21477;&#23376;&#65292;&#20854;&#20013; $\Phi$ &#26159;&#19968;&#20010;&#30001;&#31526;&#21495; $\{0, 1, +, \cdot, \geq, &gt;, \wedge, \vee, \neg\}$ &#26500;&#25104;&#30340;&#33391;&#22909;&#24418;&#24335;&#30340;&#38750;&#37327;&#21270;&#20844;&#24335;&#65292;&#30446;&#26631;&#26159;&#26816;&#26597;&#36825;&#20010;&#21477;&#23376;&#26159;&#21542;&#20026;&#30495;&#12290;&#29616;&#22312;&#65292;&#31867;&#21035;ER&#26159;&#25152;&#26377;&#33021;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#24402;&#32422;&#21040;ETR&#30340;&#38382;&#39064;&#30340;&#38598;&#21512;&#12290;&#24050;&#30693; NP $\subseteq$ ER $\subseteq$ PSPACE&#12290;&#25105;&#20204;&#23558;&#27880;&#24847;&#21147;&#38598;&#20013;&#22312;&#20855;&#26377;&#21152;&#27861;&#32422;&#26463;&#65288;$x + y = z$&#65289;&#21644;&#20854;&#20182;&#19968;&#20123;&#28201;&#21644;&#25216;&#26415;&#26465;&#20214;&#30340;CCSP&#19978;&#12290;&#20197;&#21069;&#24050;&#32463;&#35777;&#26126;&#20102;&#19968;&#20123;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
A continuous constraint satisfaction problem (CCSP) is a constraint satisfaction problem (CSP) with an interval domain $U \subset \mathbb{R}$. We engage in a systematic study to classify CCSPs that are complete of the Existential Theory of the Reals, i.e., ER-complete. To define this class, we first consider the problem ETR, which also stands for Existential Theory of the Reals. In an instance of this problem we are given some sentence of the form $\exists x_1, \ldots, x_n \in \mathbb{R} : \Phi(x_1, \ldots, x_n)$, where $\Phi$ is a well-formed quantifier-free formula consisting of the symbols $\{0, 1, +, \cdot, \geq, &gt;, \wedge, \vee, \neg\}$, the goal is to check whether this sentence is true. Now the class ER is the family of all problems that admit a polynomial-time many-one reduction to ETR. It is known that NP $\subseteq$ ER $\subseteq$ PSPACE.  We restrict our attention on CCSPs with addition constraints ($x + y = z$) and some other mild technical condition. Previously, it was sho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#20998;&#24067;&#35821;&#20041;&#30740;&#31350;&#25506;&#31350;&#21453;&#35789;&#34892;&#20026;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#31639;&#27861;&#29992;&#20110;&#26816;&#27979;&#21453;&#35789;&#12290;&#22312;&#23567;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;39%&#30340;&#20934;&#30830;&#29575;&#65292;&#23637;&#31034;&#20102;&#35813;&#39046;&#22495;&#20173;&#26377;&#24453;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/1901.05066</link><description>&lt;p&gt;
&#20351;&#29992;&#20998;&#24067;&#35821;&#20041;&#30740;&#31350;&#21453;&#35789;&#34892;&#20026;&#30340;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
Investigating Antigram Behaviour using Distributional Semantics. (arXiv:1901.05066v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1901.05066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#20998;&#24067;&#35821;&#20041;&#30740;&#31350;&#25506;&#31350;&#21453;&#35789;&#34892;&#20026;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#31639;&#27861;&#29992;&#20110;&#26816;&#27979;&#21453;&#35789;&#12290;&#22312;&#23567;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;39%&#30340;&#20934;&#30830;&#29575;&#65292;&#23637;&#31034;&#20102;&#35813;&#39046;&#22495;&#20173;&#26377;&#24453;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#35821;&#35328;&#23398;&#39046;&#22495;&#19981;&#26029;&#20026;&#30740;&#31350;&#25552;&#20986;&#26032;&#30340;&#25361;&#25112;&#21644;&#20027;&#39064;&#65292;&#26080;&#35770;&#26159;&#20998;&#26512;&#35789;&#35821;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#29992;&#27861;&#36824;&#26159;&#35782;&#21035;&#30475;&#20284;&#19981;&#30456;&#20851;&#30340;&#35789;&#35821;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#22238;&#25991;&#21644;&#21453;&#35789;&#20316;&#20026;&#20855;&#26377;&#29420;&#29305;&#23646;&#24615;&#30340;&#35789;&#35821;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#20351;&#29992;GloVe&#23884;&#20837;&#26469;&#29983;&#25104;&#32473;&#23450;&#35789;&#35821;&#30340;&#22238;&#25991;&#65292;&#24182;&#30830;&#23450;&#29983;&#25104;&#30340;&#22238;&#25991;&#20043;&#38388;&#26159;&#21542;&#23384;&#22312;&#21453;&#35789;&#65288;&#35821;&#20041;&#19978;&#30456;&#21453;&#30340;&#22238;&#25991;&#65289;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#31616;&#21333;&#20294;&#21487;&#35299;&#37322;&#30340;&#31639;&#27861;&#26469;&#26816;&#27979;&#21453;&#35789;&#12290;&#22312;&#20165;&#26377;12&#20010;&#21453;&#35789;&#30340;&#23567;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;39&#65285;&#65292;&#36825;&#34920;&#26126;&#22312;&#36825;&#20010;&#39046;&#22495;&#36824;&#26377;&#24456;&#22810;&#24037;&#20316;&#35201;&#20570;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of computational linguistics constantly presents new challenges and topics for research. Whether it be analyzing word usage changes over time or identifying relationships between pairs of seemingly unrelated words. To this point, we identify Anagrams and Antigrams as words possessing such unique properties. The presented work is an exploration into generating anagrams from a given word and determining whether there exists antigram (semantically opposite anagrams) relationships between the pairs of generated anagrams using GloVe embeddings. We propose a rudimentary, yet interpretable, rule-based algorithm for detecting antigrams. On a small dataset of just 12 antigrams, our approach yielded an accuracy of 39\% which shows that there is much work left to be done in this space.
&lt;/p&gt;</description></item></channel></rss>