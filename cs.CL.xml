<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#30740;&#31350;&#25506;&#35752;&#35748;&#30693;&#22810;&#26679;&#24615;&#21450;&#20854;&#23545;&#32676;&#20307;&#20915;&#31574;&#25104;&#21151;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#20998;&#26512;500&#20010;&#22242;&#38431;&#35752;&#35770;&#30340;&#23545;&#35805;&#35760;&#24405;&#65292;&#25105;&#20204;&#21457;&#29616;&#36739;&#22823;&#30340;&#35748;&#30693;&#22810;&#26679;&#24615;&#19982;&#26356;&#25104;&#21151;&#30340;&#32676;&#20307;&#20915;&#31574;&#30456;&#20851;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01427</link><description>&lt;p&gt;
&#22810;&#26679;&#24615;&#23545;&#32676;&#20307;&#20915;&#31574;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The effect of diversity on group decision-making
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01427
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#35748;&#30693;&#22810;&#26679;&#24615;&#21450;&#20854;&#23545;&#32676;&#20307;&#20915;&#31574;&#25104;&#21151;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#20998;&#26512;500&#20010;&#22242;&#38431;&#35752;&#35770;&#30340;&#23545;&#35805;&#35760;&#24405;&#65292;&#25105;&#20204;&#21457;&#29616;&#36739;&#22823;&#30340;&#35748;&#30693;&#22810;&#26679;&#24615;&#19982;&#26356;&#25104;&#21151;&#30340;&#32676;&#20307;&#20915;&#31574;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#35748;&#30693;&#22810;&#26679;&#24615;&#30340;&#19981;&#21516;&#26041;&#38754;&#20197;&#21450;&#20854;&#23545;&#32676;&#20307;&#21327;&#21830;&#25104;&#21151;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;500&#20010;&#23567;&#22411;&#22312;&#32447;&#22242;&#38431;&#35752;&#35770;&#29926;&#32034;&#21345;&#29255;&#36873;&#25321;&#20219;&#21153;&#30340;&#23545;&#35805;&#35760;&#24405;&#36827;&#34892;&#35780;&#20272; - DeliData&#35821;&#26009;&#24211;&#12290;&#21033;&#29992;&#35813;&#35821;&#26009;&#24211;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23450;&#37327;&#20998;&#26512;&#65292;&#35780;&#20272;&#20102;&#35748;&#30693;&#22810;&#26679;&#24615;&#30340;&#19977;&#31181;&#19981;&#21516;&#24230;&#37327;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#32676;&#20307;&#35268;&#27169;&#20316;&#20026;&#22810;&#26679;&#24615;&#30340;&#20195;&#29702;&#24230;&#37327;&#30340;&#24433;&#21709;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#21021;&#22987;&#24819;&#27861;&#27744;&#30340;&#22823;&#23567;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#35752;&#35770;&#30340;&#35299;&#20915;&#26041;&#26696;&#12289;&#35752;&#35770;&#27169;&#24335;&#20197;&#21450;&#35848;&#35805;&#25506;&#31350;&#22914;&#20309;&#25913;&#21892;&#36825;&#20123;&#29305;&#24449;&#26469;&#30740;&#31350;&#35752;&#35770;&#20869;&#23481;&#12290;&#23613;&#31649;&#32676;&#20307;&#22240;&#20026;&#21152;&#37325;&#20559;&#35265;&#32780;&#21517;&#22768;&#19981;&#20339;&#65292;&#25105;&#20204;&#34920;&#26126;&#23567;&#22242;&#38431;&#21487;&#20197;&#36890;&#36807;&#23545;&#35805;&#20811;&#26381;&#30452;&#35273;&#20559;&#35265;&#65292;&#25552;&#39640;&#20010;&#20307;&#20915;&#31574;&#33021;&#21147;&#12290;&#22312;&#22823;&#26679;&#26412;&#21644;&#19981;&#21516;&#25805;&#20316;&#20013;&#65292;&#25105;&#20204;&#22987;&#32456;&#21457;&#29616;&#35748;&#30693;&#22810;&#26679;&#24615;&#19982;&#32676;&#20307;&#21327;&#21830;&#30340;&#25104;&#21151;&#24615;&#26377;&#20851;
&lt;/p&gt;
&lt;p&gt;
We explore different aspects of cognitive diversity and its effect on the success of group deliberation. To evaluate this, we use 500 dialogues from small, online groups discussing the Wason Card Selection task - the DeliData corpus. Leveraging the corpus, we perform quantitative analysis evaluating three different measures of cognitive diversity. First, we analyse the effect of group size as a proxy measure for diversity. Second, we evaluate the effect of the size of the initial idea pool. Finally, we look into the content of the discussion by analysing discussed solutions, discussion patterns, and how conversational probing can improve those characteristics.   Despite the reputation of groups for compounding bias, we show that small groups can, through dialogue, overcome intuitive biases and improve individual decision-making. Across a large sample and different operationalisations, we consistently find that greater cognitive diversity is associated with more successful group deliber
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#39318;&#27425;&#23581;&#35797;&#32763;&#35793;&#22270;&#20687;&#20197;&#20351;&#20854;&#20855;&#26377;&#25991;&#21270;&#30456;&#20851;&#24615;&#65292;&#26500;&#24314;&#20102;&#35780;&#20272;&#25968;&#25454;&#38598;&#24182;&#36827;&#34892;&#20102;&#22810;&#26041;&#38754;&#30340;&#20154;&#31867;&#35780;&#20272;&#65292;&#21457;&#29616;&#30446;&#21069;&#22270;&#20687;&#32534;&#36753;&#27169;&#22411;&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2404.01247</link><description>&lt;p&gt;
&#22270;&#29255;&#34429;&#28982;&#20195;&#34920;&#21315;&#35328;&#19975;&#35821;&#65292;&#20294;&#27599;&#20010;&#20154;&#37117;&#33021;&#21548;&#25026;&#21527;&#65311;&#20851;&#20110;&#32763;&#35793;&#20855;&#26377;&#25991;&#21270;&#30456;&#20851;&#24615;&#30340;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
An image speaks a thousand words, but can everyone listen? On translating images for cultural relevance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01247
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#39318;&#27425;&#23581;&#35797;&#32763;&#35793;&#22270;&#20687;&#20197;&#20351;&#20854;&#20855;&#26377;&#25991;&#21270;&#30456;&#20851;&#24615;&#65292;&#26500;&#24314;&#20102;&#35780;&#20272;&#25968;&#25454;&#38598;&#24182;&#36827;&#34892;&#20102;&#22810;&#26041;&#38754;&#30340;&#20154;&#31867;&#35780;&#20272;&#65292;&#21457;&#29616;&#30446;&#21069;&#22270;&#20687;&#32534;&#36753;&#27169;&#22411;&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22810;&#23186;&#20307;&#20869;&#23481;&#30340;&#20852;&#36215;&#65292;&#20154;&#31867;&#32763;&#35793;&#36234;&#26469;&#36234;&#22810;&#22320;&#20851;&#27880;&#20110;&#25991;&#21270;&#36866;&#24212;&#65292;&#19981;&#20165;&#38480;&#20110;&#25991;&#23383;&#65292;&#36824;&#21253;&#25324;&#22270;&#29255;&#31561;&#20854;&#20182;&#24418;&#24335;&#65292;&#20197;&#20256;&#36798;&#30456;&#21516;&#30340;&#21547;&#20041;&#12290;&#34429;&#28982;&#26377;&#20960;&#31181;&#24212;&#29992;&#23558;&#21463;&#30410;&#20110;&#36825;&#19968;&#28857;&#65292;&#20294;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#20173;&#28982;&#23616;&#38480;&#20110;&#22788;&#29702;&#35821;&#35328;&#30340;&#21475;&#22836;&#21644;&#25991;&#23383;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36808;&#20986;&#20102;&#32763;&#35793;&#22270;&#20687;&#20197;&#20351;&#20854;&#20855;&#26377;&#25991;&#21270;&#30456;&#20851;&#24615;&#30340;&#31532;&#19968;&#27493;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19977;&#20010;&#21253;&#21547;&#26368;&#20808;&#36827;&#29983;&#25104;&#27169;&#22411;&#30340;&#27969;&#27700;&#32447;&#26469;&#23436;&#25104;&#36825;&#39033;&#20219;&#21153;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#30001;&#20004;&#37096;&#20998;&#32452;&#25104;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#65306;i&#65289;&#27010;&#24565;&#65306;&#21253;&#25324;600&#24352;&#36328;&#25991;&#21270;&#36830;&#36143;&#30340;&#22270;&#20687;&#65292;&#27599;&#24352;&#22270;&#20687;&#19987;&#27880;&#20110;&#21333;&#20010;&#27010;&#24565;&#65292;ii&#65289;&#24212;&#29992;&#65306;&#21253;&#25324;&#20174;&#23454;&#38469;&#24212;&#29992;&#20013;&#31579;&#36873;&#20986;&#30340;100&#24352;&#22270;&#20687;&#12290;&#25105;&#20204;&#23545;&#32763;&#35793;&#21518;&#30340;&#22270;&#20687;&#36827;&#34892;&#20102;&#22810;&#26041;&#38754;&#30340;&#20154;&#31867;&#35780;&#20272;&#65292;&#20197;&#35780;&#20272;&#20854;&#25991;&#21270;&#30456;&#20851;&#24615;&#21644;&#21547;&#20041;&#20445;&#30041;&#12290;&#25105;&#20204;&#21457;&#29616;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#22270;&#20687;&#32534;&#36753;&#27169;&#22411;&#22312;&#36825;&#39033;&#20219;&#21153;&#19978;&#22833;&#36133;&#20102;&#65292;&#20294;&#21487;&#20197;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01247v1 Announce Type: new  Abstract: Given the rise of multimedia content, human translators increasingly focus on culturally adapting not only words but also other modalities such as images to convey the same meaning. While several applications stand to benefit from this, machine translation systems remain confined to dealing with language in speech and text. In this work, we take a first step towards translating images to make them culturally relevant. First, we build three pipelines comprising state-of-the-art generative models to do the task. Next, we build a two-part evaluation dataset: i) concept: comprising 600 images that are cross-culturally coherent, focusing on a single concept per image, and ii) application: comprising 100 images curated from real-world applications. We conduct a multi-faceted human evaluation of translated images to assess for cultural relevance and meaning preservation. We find that as of today, image-editing models fail at this task, but can 
&lt;/p&gt;</description></item><item><title>&#20513;&#23548;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#24418;&#24335;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#31995;&#32479;&#35780;&#20272;&#29702;&#24615;&#21270;&#35299;&#37322;&#65292;&#38024;&#23545;&#33258;&#21160;&#20107;&#23454;&#39564;&#35777;&#20219;&#21153;&#20013;&#19981;&#21516;&#32467;&#26500;&#30340;&#35299;&#37322;&#25552;&#20379;&#20102;&#19968;&#22871;&#35780;&#20272;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.20322</link><description>&lt;p&gt;
&#22312;&#33258;&#21160;&#20107;&#23454;&#39564;&#35777;&#20013;&#35780;&#20272;&#35299;&#37322;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Towards a Framework for Evaluating Explanations in Automated Fact Verification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20322
&lt;/p&gt;
&lt;p&gt;
&#20513;&#23548;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#24418;&#24335;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#31995;&#32479;&#35780;&#20272;&#29702;&#24615;&#21270;&#35299;&#37322;&#65292;&#38024;&#23545;&#33258;&#21160;&#20107;&#23454;&#39564;&#35777;&#20219;&#21153;&#20013;&#19981;&#21516;&#32467;&#26500;&#30340;&#35299;&#37322;&#25552;&#20379;&#20102;&#19968;&#22871;&#35780;&#20272;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#28145;&#24230;&#31070;&#32463;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#20063;&#22240;&#27492;&#21464;&#24471;&#19981;&#36879;&#26126;&#65292;&#35299;&#37322;&#23427;&#20204;&#30340;&#24517;&#35201;&#24615;&#21464;&#24471;&#26356;&#20026;&#37325;&#35201;&#12290;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#23545;&#29702;&#24615;&#21270;&#35299;&#37322;&#20135;&#29983;&#20102;&#20852;&#36259;&#65292;&#20197;&#25552;&#20379;&#23545;&#39044;&#27979;&#30340;&#31616;&#30701;&#21644;&#36830;&#36143;&#30340;&#29702;&#30001;&#12290;&#22312;&#36825;&#31687;&#31435;&#22330;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20513;&#23548;&#19968;&#20010;&#24418;&#24335;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#20851;&#20110;&#29702;&#24615;&#21270;&#35299;&#37322;&#30340;&#20851;&#38190;&#27010;&#24565;&#21644;&#23646;&#24615;&#65292;&#20197;&#25903;&#25345;&#31995;&#32479;&#22320;&#35780;&#20272;&#23427;&#20204;&#12290;&#25105;&#20204;&#36824;&#27010;&#36848;&#20102;&#19968;&#20010;&#36825;&#26679;&#30340;&#24418;&#24335;&#21270;&#26694;&#26550;&#65292;&#23450;&#21046;&#20110;&#36234;&#26469;&#36234;&#22797;&#26434;&#32467;&#26500;&#30340;&#29702;&#24615;&#21270;&#35299;&#37322;&#65292;&#20174;&#33258;&#30001;&#24418;&#24335;&#30340;&#35299;&#37322;&#21040;&#28436;&#32462;&#24615;&#35299;&#37322;&#65292;&#20877;&#21040;&#20855;&#26377;&#26368;&#20016;&#23500;&#32467;&#26500;&#30340;&#35770;&#35777;&#24615;&#35299;&#37322;&#12290;&#25105;&#20204;&#32858;&#28966;&#20110;&#33258;&#21160;&#20107;&#23454;&#39564;&#35777;&#20219;&#21153;&#65292;&#25552;&#20379;&#20102;&#20351;&#29992;&#25105;&#20204;&#30340;&#24418;&#24335;&#21270;&#35780;&#20272;&#35299;&#37322;&#30340;&#31034;&#20363;&#21450;&#20854;&#23454;&#29992;&#24615;&#65292;&#38024;&#23545;&#19981;&#21516;&#32467;&#26500;&#20570;&#20986;&#20102;&#23450;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20322v1 Announce Type: new  Abstract: As deep neural models in NLP become more complex, and as a consequence opaque, the necessity to interpret them becomes greater. A burgeoning interest has emerged in rationalizing explanations to provide short and coherent justifications for predictions. In this position paper, we advocate for a formal framework for key concepts and properties about rationalizing explanations to support their evaluation systematically. We also outline one such formal framework, tailored to rationalizing explanations of increasingly complex structures, from free-form explanations to deductive explanations, to argumentative explanations (with the richest structure). Focusing on the automated fact verification task, we provide illustrations of the use and usefulness of our formalization for evaluating explanations, tailored to their varying structures.
&lt;/p&gt;</description></item><item><title>ToXCL&#26159;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#26088;&#22312;&#26816;&#27979;&#21644;&#35299;&#37322;&#38544;&#24615;&#27602;&#24615;&#35328;&#35770;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#27169;&#22411;&#22312;&#26816;&#27979;&#21644;&#35299;&#37322;&#20219;&#21153;&#20013;&#21487;&#33021;&#36935;&#21040;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.16685</link><description>&lt;p&gt;
ToXCL&#65306;&#27602;&#24615;&#35328;&#35770;&#26816;&#27979;&#21644;&#35299;&#37322;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
ToXCL: A Unified Framework for Toxic Speech Detection and Explanation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16685
&lt;/p&gt;
&lt;p&gt;
ToXCL&#26159;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#26088;&#22312;&#26816;&#27979;&#21644;&#35299;&#37322;&#38544;&#24615;&#27602;&#24615;&#35328;&#35770;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#27169;&#22411;&#22312;&#26816;&#27979;&#21644;&#35299;&#37322;&#20219;&#21153;&#20013;&#21487;&#33021;&#36935;&#21040;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#19978;&#27602;&#24615;&#35328;&#35770;&#30340;&#34067;&#24310;&#26159;&#19968;&#20010;&#20196;&#20154;&#20851;&#27880;&#30340;&#38382;&#39064;&#65292;&#23545;&#20154;&#32676;&#26500;&#25104;&#23041;&#32961;&#12290;&#26126;&#26174;&#30340;&#27602;&#24615;&#35328;&#35770;&#21253;&#21547;&#20882;&#29359;&#24615;&#35789;&#27719;&#20449;&#21495;&#65292;&#38544;&#24615;&#30340;&#35328;&#35770;&#21017;&#21253;&#21547;&#32534;&#30721;&#25110;&#38388;&#25509;&#35821;&#35328;&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#19981;&#20165;&#38656;&#35201;&#26816;&#27979;&#38544;&#24615;&#27602;&#24615;&#35328;&#35770;&#65292;&#36824;&#38656;&#35201;&#35299;&#37322;&#20854;&#27602;&#24615;&#12290;&#36825;&#23548;&#33268;&#20102;&#23545;&#26377;&#25928;&#26816;&#27979;&#21644;&#35299;&#37322;&#38544;&#24615;&#27602;&#24615;&#35328;&#35770;&#30340;&#32479;&#19968;&#26694;&#26550;&#30340;&#29420;&#29305;&#38656;&#27714;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#23558;&#27602;&#24615;&#35328;&#35770;&#30340;&#26816;&#27979;&#21644;&#35299;&#37322;&#20219;&#21153;&#21046;&#23450;&#20026;&#25991;&#26412;&#29983;&#25104;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#36825;&#31181;&#31574;&#30053;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#33021;&#23481;&#26131;&#21463;&#21040;&#21518;&#32493;&#38169;&#35823;&#20256;&#25773;&#38382;&#39064;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#27169;&#22411;&#30340;&#26816;&#27979;&#32467;&#26524;&#36828;&#20302;&#20110;&#37027;&#20123;&#20165;&#19987;&#27880;&#20110;&#26816;&#27979;&#20219;&#21153;&#30340;&#27169;&#22411;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#20123;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;ToXCL&#65292;&#19968;&#20010;&#29992;&#20110;&#26816;&#27979;&#21644;&#35299;&#37322;&#38544;&#24615;&#27602;&#24615;&#35328;&#35770;&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16685v1 Announce Type: new  Abstract: The proliferation of online toxic speech is a pertinent problem posing threats to demographic groups. While explicit toxic speech contains offensive lexical signals, implicit one consists of coded or indirect language. Therefore, it is crucial for models not only to detect implicit toxic speech but also to explain its toxicity. This draws a unique need for unified frameworks that can effectively detect and explain implicit toxic speech. Prior works mainly formulated the task of toxic speech detection and explanation as a text generation problem. Nonetheless, models trained using this strategy can be prone to suffer from the consequent error propagation problem. Moreover, our experiments reveal that the detection results of such models are much lower than those that focus only on the detection task. To bridge these gaps, we introduce ToXCL, a unified framework for the detection and explanation of implicit toxic speech. Our model consists 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23545;25&#31181;&#20302;&#36164;&#28304;&#35821;&#35328;&#21644;7&#31181;&#30456;&#23545;&#36739;&#39640;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;&#24773;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#21450;&#20854;&#36328;&#35821;&#35328;&#21464;&#20307;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#20351;&#29992;LLMs&#36827;&#34892;ICL&#30340;&#26377;&#25928;&#24615;&#65292;&#25552;&#20986;&#20102;&#26367;&#20195;&#26041;&#27861;&#26597;&#35810;&#23545;&#40784;&#65292;&#24182;&#20026;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;ICL&#25552;&#20379;&#20102;&#23453;&#36149;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.16512</link><description>&lt;p&gt;
LLMs&#26159;&#23569;&#26679;&#26412;&#24773;&#22659;&#20302;&#36164;&#28304;&#35821;&#35328;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
LLMs Are Few-Shot In-Context Low-Resource Language Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16512
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23545;25&#31181;&#20302;&#36164;&#28304;&#35821;&#35328;&#21644;7&#31181;&#30456;&#23545;&#36739;&#39640;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;&#24773;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#21450;&#20854;&#36328;&#35821;&#35328;&#21464;&#20307;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#20351;&#29992;LLMs&#36827;&#34892;ICL&#30340;&#26377;&#25928;&#24615;&#65292;&#25552;&#20986;&#20102;&#26367;&#20195;&#26041;&#27861;&#26597;&#35810;&#23545;&#40784;&#65292;&#24182;&#20026;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;ICL&#25552;&#20379;&#20102;&#23453;&#36149;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24773;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#30340;&#25903;&#25345;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#21033;&#29992;&#30701;&#26102;&#30340;&#24773;&#22659;&#20449;&#24687;&#25191;&#34892;&#21508;&#31181;&#20219;&#21153;&#65292;&#36825;&#20026;&#32553;&#23567;&#39640;&#36164;&#28304;&#35821;&#35328;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#20043;&#38388;&#30340;&#24046;&#36317;&#25552;&#20379;&#20102;&#37325;&#35201;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#21482;&#26377;&#23569;&#25968;&#30740;&#31350;&#25506;&#35752;&#20102;&#38024;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;ICL&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#38598;&#20013;&#22312;&#30456;&#23545;&#39640;&#36164;&#28304;&#30340;&#35821;&#35328;&#65292;&#27604;&#22914;&#27861;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;25&#31181;&#20302;&#36164;&#28304;&#35821;&#35328;&#21644;7&#31181;&#30456;&#23545;&#36739;&#39640;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;ICL&#21450;&#20854;&#36328;&#35821;&#35328;&#21464;&#20307;&#65288;X-ICL&#65289;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#19981;&#20165;&#35780;&#20272;&#20102;LLMs&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#20351;&#29992;ICL&#30340;&#26377;&#25928;&#24615;&#65292;&#36824;&#21457;&#29616;&#20102;&#24773;&#22659;&#26631;&#31614;&#23545;&#40784;&#30340;&#32570;&#38519;&#65292;&#24182;&#24341;&#20837;&#20102;&#26356;&#26377;&#25928;&#30340;&#26367;&#20195;&#26041;&#27861;&#65306;&#26597;&#35810;&#23545;&#40784;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20026;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;ICL&#30340;&#21508;&#20010;&#26041;&#38754;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24635;&#32467;&#20102;&#23569;&#26679;&#26412;&#24773;&#22659;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16512v1 Announce Type: cross  Abstract: In-context learning (ICL) empowers large language models (LLMs) to perform diverse tasks in underrepresented languages using only short in-context information, offering a crucial avenue for narrowing the gap between high-resource and low-resource languages. Nonetheless, there is only a handful of works explored ICL for low-resource languages with most of them focusing on relatively high-resource languages, such as French and Spanish. In this work, we extensively study ICL and its cross-lingual variation (X-ICL) on 25 low-resource and 7 relatively higher-resource languages. Our study not only assesses the effectiveness of ICL with LLMs in low-resource languages but also identifies the shortcomings of in-context label alignment, and introduces a more effective alternative: query alignment. Moreover, we provide valuable insights into various facets of ICL for low-resource languages. Our study concludes the significance of few-shot in-cont
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21435;&#27602;&#21270;&#65292;&#22312;&#26500;&#24314;&#20102;SafeEdit&#22522;&#20934;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861; DINM&#65292;&#21487;&#20197;&#36890;&#36807;&#23569;&#37327;&#35843;&#25972;&#27493;&#39588;&#20943;&#23569;&#27169;&#22411;&#30340;&#27602;&#24615;&#65292;&#21516;&#26102;&#23545;&#21508;&#31181;&#21435;&#27602;&#26041;&#27861;&#30340;&#20869;&#37096;&#26426;&#21046;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.14472</link><description>&lt;p&gt;
&#36890;&#36807;&#30693;&#35782;&#32534;&#36753;&#23454;&#29616;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21435;&#27602;&#21270;
&lt;/p&gt;
&lt;p&gt;
Detoxifying Large Language Models via Knowledge Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21435;&#27602;&#21270;&#65292;&#22312;&#26500;&#24314;&#20102;SafeEdit&#22522;&#20934;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861; DINM&#65292;&#21487;&#20197;&#36890;&#36807;&#23569;&#37327;&#35843;&#25972;&#27493;&#39588;&#20943;&#23569;&#27169;&#22411;&#30340;&#27602;&#24615;&#65292;&#21516;&#26102;&#23545;&#21508;&#31181;&#21435;&#27602;&#26041;&#27861;&#30340;&#20869;&#37096;&#26426;&#21046;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#26469;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#21435;&#27602;&#21270;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;SafeEdit&#30340;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#20061;&#31181;&#19981;&#23433;&#20840;&#31867;&#21035;&#65292;&#20855;&#26377;&#21508;&#31181;&#24378;&#22823;&#30340;&#25915;&#20987;&#25552;&#31034;&#65292;&#24182;&#37197;&#22791;&#20102;&#20840;&#38754;&#30340;&#24230;&#37327;&#26631;&#20934;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#19982;&#20043;&#21069;&#30340;&#22522;&#20934;&#32447;&#65292;&#32467;&#26524;&#34920;&#26126;&#30693;&#35782;&#32534;&#36753;&#26377;&#28508;&#21147;&#22312;&#23545;LLMs&#36827;&#34892;&#21435;&#27602;&#21270;&#26102;&#65292;&#22312;&#23545;&#19968;&#33324;&#24615;&#33021;&#30340;&#24433;&#21709;&#30456;&#23545;&#26377;&#38480;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#22522;&#20934;&#32447;&#65292;&#31216;&#20026;&#36890;&#36807;&#26415;&#20013;&#31070;&#32463;&#30417;&#27979;&#21435;&#27602;&#21270;&#65288;DINM&#65289;&#65292;&#36890;&#36807;&#20165;&#19968;&#27425;&#23454;&#20363;&#30340;&#23569;&#37327;&#35843;&#25972;&#27493;&#39588;&#20943;&#23569;LLMs&#30340;&#27602;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23545;&#21508;&#31181;&#21435;&#27602;&#26041;&#27861;&#30340;&#20869;&#37096;&#26426;&#21046;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#34920;&#26126;&#20808;&#21069;&#30340;&#26041;&#27861;&#22914;SFT&#21644;DPO&#21487;&#33021;&#20165;&#25233;&#21046;&#26377;&#27602;&#21442;&#25968;&#30340;&#28608;&#27963;&#65292;&#32780;DINM&#21017;&#20943;&#36731;&#26377;&#27602;&#21442;&#25968;&#30340;&#27602;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14472v1 Announce Type: cross  Abstract: This paper investigates using knowledge editing techniques to detoxify Large Language Models (LLMs). We construct a benchmark, SafeEdit, which covers nine unsafe categories with various powerful attack prompts and equips comprehensive metrics for systematic evaluation. We conduct experiments to compare knowledge editing approaches with previous baselines, indicating that knowledge editing has the potential to efficiently detoxify LLMs with limited impact on general performance. Then, we propose a simple yet effective baseline, dubbed Detoxifying with Intraoperative Neural Monitoring (DINM), to diminish the toxicity of LLMs within a few tuning steps via only one instance. We further provide an in-depth analysis of the internal mechanism for various detoxify approaches, demonstrating that previous methods like SFT and DPO may merely suppress the activations of toxic parameters, while DINM mitigates the toxicity of the toxic parameters to
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#28151;&#21512;&#25991;&#26412;&#20013;&#21477;&#23376;&#32423;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#21106;&#30340;&#20004;&#27493;&#39588;&#27969;&#31243;&#26469;&#26816;&#27979;&#21508;&#27573;&#33853;&#30340;&#19968;&#33268;&#20316;&#32773;&#21477;&#23376;&#12290;</title><link>https://arxiv.org/abs/2403.03506</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#19982;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#28151;&#21512;&#25991;&#26412;&#20013;&#30340;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Detecting AI-Generated Text within Human-AI Collaborative Hybrid Texts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#28151;&#21512;&#25991;&#26412;&#20013;&#21477;&#23376;&#32423;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#21106;&#30340;&#20004;&#27493;&#39588;&#27969;&#31243;&#26469;&#26816;&#27979;&#21508;&#27573;&#33853;&#30340;&#19968;&#33268;&#20316;&#32773;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#28151;&#21512;&#25991;&#26412;&#20013;&#21477;&#23376;&#32423;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#20851;&#20110;&#28151;&#21512;&#25991;&#26412;&#20013;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#30740;&#31350;&#36890;&#24120;&#20381;&#36182;&#20110;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#36890;&#24120;&#28041;&#21450;&#24102;&#26377;&#26377;&#38480;&#36793;&#30028;&#30340;&#28151;&#21512;&#25991;&#26412;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#26816;&#27979;&#28151;&#21512;&#25991;&#26412;&#20013;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#30740;&#31350;&#24212;&#35206;&#30422;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#29983;&#25104;&#30340;&#19981;&#21516;&#31867;&#22411;&#28151;&#21512;&#25991;&#26412;&#65292;&#20197;&#26356;&#22909;&#22320;&#25351;&#23548;&#23454;&#38469;&#24212;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21033;&#29992;&#20102;CoAuthor&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#36890;&#36807;&#20154;&#31867;&#20316;&#32773;&#21644;&#26234;&#33021;&#20889;&#20316;&#31995;&#32479;&#20043;&#38388;&#30340;&#21327;&#20316;&#29983;&#25104;&#30340;&#22810;&#36718;&#20132;&#20114;&#20013;&#20135;&#29983;&#30340;&#22810;&#26679;&#21270;&#12289;&#30495;&#23454;&#30340;&#28151;&#21512;&#25991;&#26412;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#20004;&#27493;&#20998;&#21106;&#20026;&#22522;&#30784;&#30340;&#27969;&#31243;&#65306;(i)&#26816;&#27979;&#32473;&#23450;&#28151;&#21512;&#25991;&#26412;&#20013;&#30340;&#21508;&#20010;&#27573;&#33853;&#65292;&#20854;&#20013;&#27599;&#20010;&#27573;&#33853;&#21253;&#21547;&#19968;&#33268;&#20316;&#32773;&#30340;&#21477;&#23376;&#65292;&#20197;&#21450;(ii)&#20998;&#31867;&#27599;&#20010;&#30830;&#23450;&#27573;&#33853;&#30340;&#20316;&#32773;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03506v1 Announce Type: cross  Abstract: This study explores the challenge of sentence-level AI-generated text detection within human-AI collaborative hybrid texts. Existing studies of AI-generated text detection for hybrid texts often rely on synthetic datasets. These typically involve hybrid texts with a limited number of boundaries. We contend that studies of detecting AI-generated content within hybrid texts should cover different types of hybrid texts generated in realistic settings to better inform real-world applications. Therefore, our study utilizes the CoAuthor dataset, which includes diverse, realistic hybrid texts generated through the collaboration between human writers and an intelligent writing system in multi-turn interactions. We adopt a two-step, segmentation-based pipeline: (i) detect segments within a given hybrid text where each segment contains sentences of consistent authorship, and (ii) classify the authorship of each identified segment. Our empirical 
&lt;/p&gt;</description></item><item><title>&#37117;&#24066;GPT&#26088;&#22312;&#24314;&#31435;&#19968;&#20010;&#20855;&#26377;&#24378;&#22823;&#27867;&#21270;&#33021;&#21147;&#30340;&#26102;&#31354;&#27169;&#22411;&#65292;&#20511;&#37492;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25104;&#23601;&#12290;</title><link>https://arxiv.org/abs/2403.00813</link><description>&lt;p&gt;
UrbanGPT: &#26102;&#31354;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
UrbanGPT: Spatio-Temporal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00813
&lt;/p&gt;
&lt;p&gt;
&#37117;&#24066;GPT&#26088;&#22312;&#24314;&#31435;&#19968;&#20010;&#20855;&#26377;&#24378;&#22823;&#27867;&#21270;&#33021;&#21147;&#30340;&#26102;&#31354;&#27169;&#22411;&#65292;&#20511;&#37492;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25104;&#23601;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37117;&#24066;GPT&#26088;&#22312;&#39044;&#27979;&#24182;&#27934;&#23519;&#22478;&#24066;&#29615;&#22659;&#22312;&#26102;&#38388;&#21644;&#31354;&#38388;&#19978;&#19981;&#26029;&#21464;&#21270;&#30340;&#21160;&#24577;&#12290;&#20854;&#30446;&#30340;&#26159;&#39044;&#27979;&#37117;&#24066;&#29983;&#27963;&#21508;&#20010;&#26041;&#38754;&#30340;&#26410;&#26469;&#27169;&#24335;&#12289;&#36235;&#21183;&#21644;&#20107;&#20214;&#65292;&#21253;&#25324;&#20132;&#36890;&#12289;&#20154;&#21475;&#27969;&#21160;&#21644;&#29359;&#32618;&#29575;&#31561;&#12290;&#23613;&#31649;&#24050;&#32463;&#20184;&#20986;&#20102;&#22823;&#37327;&#21162;&#21147;&#24320;&#21457;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#20197;&#20934;&#30830;&#39044;&#27979;&#26102;&#31354;&#25968;&#25454;&#65292;&#20294;&#38656;&#27880;&#24847;&#21040;&#24456;&#22810;&#26041;&#27861;&#22312;&#29983;&#25104;&#31934;&#30830;&#30340;&#26102;&#31354;&#34920;&#31034;&#26102;&#20005;&#37325;&#20381;&#36182;&#20110;&#26377;&#36275;&#22815;&#26631;&#35760;&#30340;&#25968;&#25454;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22312;&#23454;&#38469;&#37117;&#24066;&#24863;&#30693;&#22330;&#26223;&#20013;&#65292;&#25968;&#25454;&#31232;&#32570;&#26159;&#19968;&#20010;&#26222;&#36941;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#24314;&#31435;&#19968;&#20010;&#20855;&#26377;&#24378;&#22823;&#27867;&#21270;&#33021;&#21147;&#30340;&#26102;&#31354;&#27169;&#22411;&#36328;&#36234;&#22810;&#26679;&#26102;&#31354;&#23398;&#20064;&#22330;&#26223;&#26159;&#24517;&#35201;&#30340;&#12290;&#21463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#21331;&#36234;&#25104;&#23601;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00813v1 Announce Type: cross  Abstract: Spatio-temporal prediction aims to forecast and gain insights into the ever-changing dynamics of urban environments across both time and space. Its purpose is to anticipate future patterns, trends, and events in diverse facets of urban life, including transportation, population movement, and crime rates. Although numerous efforts have been dedicated to developing neural network techniques for accurate predictions on spatio-temporal data, it is important to note that many of these methods heavily depend on having sufficient labeled data to generate precise spatio-temporal representations. Unfortunately, the issue of data scarcity is pervasive in practical urban sensing scenarios. Consequently, it becomes necessary to build a spatio-temporal model with strong generalization capabilities across diverse spatio-temporal learning scenarios. Taking inspiration from the remarkable achievements of large language models (LLMs), our objective is 
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#27169;&#22411;&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#12289;&#22810;&#20219;&#21153;&#12289;&#22810;&#25945;&#24072;&#30340;&#32852;&#21512;&#32454;&#31890;&#24230;&#30693;&#35782;&#33976;&#39311;&#27169;&#22411;&#65292;&#36890;&#36807;&#24494;&#22937;&#21327;&#20316;&#20196;&#29260;&#21644;&#23454;&#20307;&#34920;&#31034;&#65292;&#22788;&#29702;&#22797;&#26434;&#30340;&#34920;&#21333;&#25991;&#26723;&#65292;&#24341;&#20837;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#25913;&#36827;&#30693;&#35782;&#33976;&#39311;&#36807;&#31243;&#65292;&#22312;&#22788;&#29702;&#35270;&#35273;&#22797;&#26434;&#34920;&#21333;&#25991;&#26723;&#30340;&#32467;&#26500;&#21644;&#20869;&#23481;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2402.17983</link><description>&lt;p&gt;
M3-VRD: &#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#22810;&#25945;&#24072;&#35270;&#35273;&#20016;&#23500;&#34920;&#21333;&#25991;&#26723;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
M3-VRD: Multimodal Multi-task Multi-teacher Visually-Rich Form Document Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17983
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#27169;&#22411;&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#12289;&#22810;&#20219;&#21153;&#12289;&#22810;&#25945;&#24072;&#30340;&#32852;&#21512;&#32454;&#31890;&#24230;&#30693;&#35782;&#33976;&#39311;&#27169;&#22411;&#65292;&#36890;&#36807;&#24494;&#22937;&#21327;&#20316;&#20196;&#29260;&#21644;&#23454;&#20307;&#34920;&#31034;&#65292;&#22788;&#29702;&#22797;&#26434;&#30340;&#34920;&#21333;&#25991;&#26723;&#65292;&#24341;&#20837;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#25913;&#36827;&#30693;&#35782;&#33976;&#39311;&#36807;&#31243;&#65292;&#22312;&#22788;&#29702;&#35270;&#35273;&#22797;&#26434;&#34920;&#21333;&#25991;&#26723;&#30340;&#32467;&#26500;&#21644;&#20869;&#23481;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31361;&#30772;&#24615;&#30340;&#22810;&#27169;&#24577;&#12289;&#22810;&#20219;&#21153;&#12289;&#22810;&#25945;&#24072;&#32852;&#21512;&#32454;&#31890;&#24230;&#30693;&#35782;&#33976;&#39311;&#27169;&#22411;&#65292;&#29992;&#20110;&#35270;&#35273;&#20016;&#23500;&#30340;&#34920;&#21333;&#25991;&#26723;&#29702;&#35299;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#36890;&#36807;&#20419;&#36827;&#20196;&#29260;&#21644;&#23454;&#20307;&#34920;&#31034;&#20043;&#38388;&#30340;&#24494;&#22937;&#30456;&#20851;&#24615;&#26469;&#21033;&#29992;&#32454;&#31890;&#24230;&#21644;&#31895;&#31890;&#24230;&#32423;&#21035;&#30340;&#35265;&#35299;&#65292;&#35299;&#20915;&#34920;&#21333;&#25991;&#26723;&#22266;&#26377;&#30340;&#22797;&#26434;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#30340;&#36328;&#32454;&#31890;&#24230;&#21644;&#36328;&#31895;&#31890;&#24230;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#36827;&#19968;&#27493;&#25913;&#36827;&#22810;&#25945;&#24072;&#30693;&#35782;&#33976;&#39311;&#20256;&#36882;&#36807;&#31243;&#65292;&#21576;&#29616;&#20998;&#24067;&#24046;&#36317;&#21644;&#23545;&#34920;&#21333;&#25991;&#26723;&#30340;&#32479;&#19968;&#29702;&#35299;&#12290;&#36890;&#36807;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;&#34920;&#21333;&#25991;&#26723;&#29702;&#35299;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22987;&#32456;&#34920;&#29616;&#20986;&#33394;&#22320;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#22788;&#29702;&#22797;&#26434;&#35270;&#35273;&#34920;&#21333;&#25991;&#26723;&#30340;&#22797;&#26434;&#32467;&#26500;&#21644;&#20869;&#23481;&#26041;&#38754;&#30340;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17983v1 Announce Type: new  Abstract: This paper presents a groundbreaking multimodal, multi-task, multi-teacher joint-grained knowledge distillation model for visually-rich form document understanding. The model is designed to leverage insights from both fine-grained and coarse-grained levels by facilitating a nuanced correlation between token and entity representations, addressing the complexities inherent in form documents. Additionally, we introduce new inter-grained and cross-grained loss functions to further refine diverse multi-teacher knowledge distillation transfer process, presenting distribution gaps and a harmonised understanding of form documents. Through a comprehensive evaluation across publicly available form document understanding datasets, our proposed model consistently outperforms existing baselines, showcasing its efficacy in handling the intricate structures and content of visually complex form documents.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;HiGPT&#27169;&#22411;&#65292;&#33268;&#21147;&#20110;&#35299;&#20915;&#24322;&#36136;&#22270;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#27867;&#21270;&#38480;&#21046;&#21644;&#20998;&#24067;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.16024</link><description>&lt;p&gt;
HiGPT&#65306;&#24322;&#36136;&#22270;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
HiGPT: Heterogeneous Graph Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16024
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;HiGPT&#27169;&#22411;&#65292;&#33268;&#21147;&#20110;&#35299;&#20915;&#24322;&#36136;&#22270;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#27867;&#21270;&#38480;&#21046;&#21644;&#20998;&#24067;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#22270;&#23398;&#20064;&#26088;&#22312;&#25429;&#25417;&#24322;&#26500;&#22270;&#20013;&#23454;&#20307;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#21644;&#22810;&#26679;&#21270;&#20851;&#31995;&#35821;&#20041;&#65292;&#20197;&#33719;&#24471;&#33410;&#28857;&#21644;&#36793;&#30340;&#26377;&#24847;&#20041;&#34920;&#31034;&#12290;&#26368;&#36817;&#22312;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HGNNs&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#32771;&#34385;&#20851;&#31995;&#30340;&#24322;&#36136;&#24615;&#24182;&#20351;&#29992;&#19987;&#38376;&#30340;&#28040;&#24687;&#20989;&#25968;&#21644;&#32858;&#21512;&#35268;&#21017;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24322;&#26500;&#22270;&#23398;&#20064;&#26694;&#26550;&#22312;&#27867;&#21270;&#21040;&#19981;&#21516;&#30340;&#24322;&#26500;&#22270;&#25968;&#25454;&#38598;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#12290;&#22823;&#22810;&#25968;&#36825;&#20123;&#26694;&#26550;&#37117;&#36981;&#24490;&#21516;&#19968;&#25968;&#25454;&#38598;&#19978;&#30340;&#8220;&#39044;&#35757;&#32451;&#8221;&#21644;&#8220;&#24494;&#35843;&#8221;&#33539;&#24335;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#36866;&#24212;&#26032;&#30340;&#21644;&#30475;&#19981;&#35265;&#30340;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#36825;&#24341;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#8220;&#25105;&#20204;&#26159;&#21542;&#33021;&#22815;&#23558;&#24322;&#36136;&#22270;&#27169;&#22411;&#27867;&#21270;&#20026;&#36866;&#24212;&#20855;&#26377;&#33410;&#28857;&#20196;&#29260;&#38598;&#21644;&#20851;&#31995;&#31867;&#22411;&#24322;&#36136;&#24615;&#20998;&#24067;&#21464;&#21270;&#30340;&#19981;&#21516;&#19979;&#28216;&#23398;&#20064;&#20219;&#21153;&#65311;&#8221;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;p
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16024v1 Announce Type: new  Abstract: Heterogeneous graph learning aims to capture complex relationships and diverse relational semantics among entities in a heterogeneous graph to obtain meaningful representations for nodes and edges. Recent advancements in heterogeneous graph neural networks (HGNNs) have achieved state-of-the-art performance by considering relation heterogeneity and using specialized message functions and aggregation rules. However, existing frameworks for heterogeneous graph learning have limitations in generalizing across diverse heterogeneous graph datasets. Most of these frameworks follow the "pre-train" and "fine-tune" paradigm on the same dataset, which restricts their capacity to adapt to new and unseen data. This raises the question: "Can we generalize heterogeneous graph models to be well-adapted to diverse downstream learning tasks with distribution shifts in both node token sets and relation type heterogeneity?'' To tackle those challenges, we p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;API-BLEND&#65292;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#21644;&#31995;&#32479;&#27979;&#35797;&#24037;&#20855;&#22686;&#24378;&#22411;LLMs&#30340;&#22823;&#22411;&#35821;&#26009;&#24211;&#65292;&#26088;&#22312;&#35299;&#20915;&#33719;&#21462;&#28041;&#21450;&#35843;&#29992;&#24037;&#20855;/API&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#24182;&#27169;&#25311;&#30495;&#23454;&#22330;&#26223;&#30340;API&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.15491</link><description>&lt;p&gt;
API-BLEND&#65306;&#29992;&#20110;&#35757;&#32451;&#21644;&#22522;&#20934;&#27979;&#35797;API LLM&#30340;&#32508;&#21512;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
API-BLEND: A Comprehensive Corpora for Training and Benchmarking API LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15491
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;API-BLEND&#65292;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#21644;&#31995;&#32479;&#27979;&#35797;&#24037;&#20855;&#22686;&#24378;&#22411;LLMs&#30340;&#22823;&#22411;&#35821;&#26009;&#24211;&#65292;&#26088;&#22312;&#35299;&#20915;&#33719;&#21462;&#28041;&#21450;&#35843;&#29992;&#24037;&#20855;/API&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#24182;&#27169;&#25311;&#30495;&#23454;&#22330;&#26223;&#30340;API&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26377;&#25928;&#20351;&#29992;&#24037;&#20855;&#21644;&#22806;&#37096;&#24212;&#29992;&#31243;&#24207;&#32534;&#31243;&#25509;&#21475;&#65288;APIs&#65289;&#26469;&#35268;&#21010;&#21644;&#23436;&#25104;&#20219;&#21153;&#30340;&#38656;&#27714;&#26085;&#30410;&#22686;&#38271;&#65292;&#23545;&#21487;&#20197;&#33719;&#21462;&#28041;&#21450;&#35843;&#29992;&#24037;&#20855;/API&#30340;&#36275;&#22815;&#25968;&#37327;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#30340;&#26041;&#27861;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#26412;&#25991;&#20851;&#27880;&#35782;&#21035;&#12289;&#25972;&#29702;&#21644;&#36716;&#21270;&#29616;&#26377;&#25968;&#25454;&#38598;&#65292;&#24182;&#20171;&#32461;API-BLEND&#65292;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#21644;&#31995;&#32479;&#27979;&#35797;&#24037;&#20855;&#22686;&#24378;&#22411;LLMs&#30340;&#22823;&#22411;&#35821;&#26009;&#24211;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#27169;&#25311;&#28041;&#21450;API&#20219;&#21153;&#30340;&#30495;&#23454;&#22330;&#26223;&#65292;&#22914;API/&#24037;&#20855;&#26816;&#27979;&#12289;&#27133;&#22635;&#20805;&#20197;&#21450;&#26816;&#27979;&#21040;&#30340;API&#30340;&#25490;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15491v1 Announce Type: cross  Abstract: There is a growing need for Large Language Models (LLMs) to effectively use tools and external Application Programming Interfaces (APIs) to plan and complete tasks. As such, there is tremendous interest in methods that can acquire sufficient quantities of train and test data that involve calls to tools / APIs. Two lines of research have emerged as the predominant strategies for addressing this challenge. The first has focused on synthetic data generation techniques, while the second has involved curating task-adjacent datasets which can be transformed into API / Tool-based tasks. In this paper, we focus on the task of identifying, curating, and transforming existing datasets and, in turn, introduce API-BLEND, a large corpora for training and systematic testing of tool-augmented LLMs. The datasets mimic real-world scenarios involving API-tasks such as API / tool detection, slot filling, and sequencing of the detected APIs. We demonstrat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Smart Parallel Auto-Correct Decoding (SPACE)&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#21322;&#33258;&#22238;&#24402;&#25512;&#29702;&#21644;&#29468;&#27979;&#35299;&#30721;&#65292;&#23454;&#29616;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#21152;&#36895;&#21644;&#24182;&#34892;&#29983;&#25104;&#39564;&#35777;&#20196;&#29260;&#30340;&#21151;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.11809</link><description>&lt;p&gt;
&#26234;&#33021;&#24182;&#34892;&#33258;&#21160;&#32416;&#38169;&#35299;&#30721;&#65306;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Generation Meets Verification: Accelerating Large Language Model Inference with Smart Parallel Auto-Correct Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11809
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Smart Parallel Auto-Correct Decoding (SPACE)&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#21322;&#33258;&#22238;&#24402;&#25512;&#29702;&#21644;&#29468;&#27979;&#35299;&#30721;&#65292;&#23454;&#29616;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#21152;&#36895;&#21644;&#24182;&#34892;&#29983;&#25104;&#39564;&#35777;&#20196;&#29260;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#21152;&#36895;&#25317;&#26377;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#26234;&#33021;&#24182;&#34892;&#33258;&#21160;&#32416;&#38169;&#35299;&#30721;&#8221;&#65288;SPACE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#23454;&#29616;LLMs&#30340;&#26080;&#25439;&#21152;&#36895;&#12290;&#36890;&#36807;&#38598;&#25104;&#21322;&#33258;&#22238;&#24402;&#25512;&#29702;&#21644;&#29468;&#27979;&#35299;&#30721;&#33021;&#21147;&#65292;SPACE&#29420;&#29305;&#22320;&#20351;&#33258;&#22238;&#24402;LLMs&#33021;&#22815;&#24182;&#34892;&#29983;&#25104;&#21644;&#39564;&#35777;&#20196;&#29260;&#12290;&#36825;&#26159;&#36890;&#36807;&#19987;&#38376;&#30340;&#21322;&#33258;&#22238;&#24402;&#30417;&#30563;&#24494;&#35843;&#36807;&#31243;&#23454;&#29616;&#30340;&#65292;&#35813;&#36807;&#31243;&#20351;&#29616;&#26377;LLMs&#20855;&#26377;&#21516;&#26102;&#39044;&#27979;&#22810;&#20010;&#20196;&#29260;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#19968;&#31181;&#33258;&#21160;&#32416;&#38169;&#35299;&#30721;&#31639;&#27861;&#20419;&#36827;&#20102;&#21333;&#20010;&#27169;&#22411;&#35843;&#29992;&#20869;&#20196;&#29260;&#24207;&#21015;&#30340;&#21516;&#26102;&#29983;&#25104;&#21644;&#39564;&#35777;&#12290;&#36890;&#36807;&#22312;&#19968;&#31995;&#21015;LLMs&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#65292;SPACE&#22312;HumanEval-X&#19978;&#34920;&#29616;&#20986;2.7&#20493;&#33267;4.0&#20493;&#30340;&#25512;&#29702;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11809v1 Announce Type: cross  Abstract: This research aims to accelerate the inference speed of large language models (LLMs) with billions of parameters. We propose \textbf{S}mart \textbf{P}arallel \textbf{A}uto-\textbf{C}orrect d\textbf{E}coding (SPACE), an innovative approach designed for achieving lossless acceleration of LLMs. By integrating semi-autoregressive inference and speculative decoding capabilities, SPACE uniquely enables autoregressive LLMs to parallelize token generation and verification. This is realized through a specialized semi-autoregressive supervised fine-tuning process that equips existing LLMs with the ability to simultaneously predict multiple tokens. Additionally, an auto-correct decoding algorithm facilitates the simultaneous generation and verification of token sequences within a single model invocation. Through extensive experiments on a range of LLMs, SPACE has demonstrated inference speedup ranging from 2.7x-4.0x on HumanEval-X while maintaini
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#36890;&#29992;&#25552;&#31034;&#20248;&#21270;&#22120;&#65292;&#29992;&#20110;&#22312;&#40657;&#30418;&#22330;&#26223;&#20013;&#23433;&#20840;&#29983;&#25104;&#25991;&#26412;&#21040;&#22270;&#20687;&#65292;&#36890;&#36807;&#26500;&#24314;&#27602;&#32032;-&#28165;&#27905;&#25552;&#31034;&#23545;&#25968;&#25454;&#38598;&#65292;&#35774;&#35745;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807; Proximal Policy Optimization &#35757;&#32451;&#20248;&#21270;&#22120;&#65292;&#25104;&#21151;&#38477;&#20302;&#21508;&#31181; T2I &#27169;&#22411;&#29983;&#25104;&#19981;&#23433;&#20840;&#20869;&#23481;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10882</link><description>&lt;p&gt;
&#36890;&#29992;&#25552;&#31034;&#20248;&#21270;&#22120;&#29992;&#20110;&#23433;&#20840;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Universal Prompt Optimizer for Safe Text-to-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10882
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#36890;&#29992;&#25552;&#31034;&#20248;&#21270;&#22120;&#65292;&#29992;&#20110;&#22312;&#40657;&#30418;&#22330;&#26223;&#20013;&#23433;&#20840;&#29983;&#25104;&#25991;&#26412;&#21040;&#22270;&#20687;&#65292;&#36890;&#36807;&#26500;&#24314;&#27602;&#32032;-&#28165;&#27905;&#25552;&#31034;&#23545;&#25968;&#25454;&#38598;&#65292;&#35774;&#35745;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807; Proximal Policy Optimization &#35757;&#32451;&#20248;&#21270;&#22120;&#65292;&#25104;&#21151;&#38477;&#20302;&#21508;&#31181; T2I &#27169;&#22411;&#29983;&#25104;&#19981;&#23433;&#20840;&#20869;&#23481;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#27169;&#22411;&#22312;&#26681;&#25454;&#25991;&#23383;&#25552;&#31034;&#29983;&#25104;&#22270;&#20687;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#19981;&#23433;&#20840;&#36755;&#20837;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#29983;&#25104;&#19981;&#23433;&#20840;&#20869;&#23481;&#65292;&#22914;&#33394;&#24773;&#12289;&#39578;&#25200;&#21644;&#38750;&#27861;&#27963;&#21160;&#22270;&#20687;&#12290;&#22522;&#20110;&#22270;&#20687;&#26816;&#26597;&#22120;&#12289;&#27169;&#22411;&#24494;&#35843;&#21644;&#23884;&#20837;&#24335;&#38459;&#27490;&#30340;&#29616;&#26377;&#30740;&#31350;&#22312;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#19981;&#21487;&#34892;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#40657;&#30418;&#22330;&#26223;&#20013;&#23433;&#20840; T2I &#29983;&#25104;&#30340;&#36890;&#29992;&#25552;&#31034;&#20248;&#21270;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10882v1 Announce Type: cross  Abstract: Text-to-Image (T2I) models have shown great performance in generating images based on textual prompts. However, these models are vulnerable to unsafe input to generate unsafe content like sexual, harassment and illegal-activity images. Existing studies based on image checker, model fine-tuning and embedding blocking are impractical in real-world applications. Hence, \textit{we propose the first universal prompt optimizer for safe T2I generation in black-box scenario}. We first construct a dataset consisting of toxic-clean prompt pairs by GPT-3.5 Turbo. To guide the optimizer to have the ability of converting toxic prompt to clean prompt while preserving semantic information, we design a novel reward function measuring toxicity and text alignment of generated images and train the optimizer through Proximal Policy Optimization. Experiments show that our approach can effectively reduce the likelihood of various T2I models in generating in
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#38271;&#25991;&#29983;&#25104;&#20013;&#30340;&#20107;&#23454;&#24615;&#38382;&#39064;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#27573;&#33853;&#26102;&#21487;&#33021;&#20250;&#30001;&#20110;&#23454;&#20307;&#27169;&#31946;&#32780;&#23558;&#21487;&#39564;&#35777;&#30340;&#20107;&#23454;&#32452;&#21512;&#25104;&#38750;&#20107;&#23454;&#30340;&#27573;&#33853;&#12290;&#29616;&#26377;&#30340;&#20107;&#23454;&#20934;&#30830;&#24230;&#35780;&#20272;&#26041;&#27861;&#26080;&#27861;&#27491;&#30830;&#35780;&#20272;&#36825;&#20123;&#38750;&#20107;&#23454;&#27573;&#33853;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#24230;&#37327;&#25351;&#26631;&#26469;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.05629</link><description>&lt;p&gt;
&#21512;&#24182;&#20107;&#23454;&#65292;&#22609;&#36896;&#35884;&#35823;&#65306;&#35780;&#20272;&#38271;&#25991;&#29983;&#25104;&#20013;&#32858;&#21512;&#20107;&#23454;&#24615;&#20027;&#24352;&#30340;&#30683;&#30462;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
Merging Facts, Crafting Fallacies: Evaluating the Contradictory Nature of Aggregated Factual Claims in Long-Form Generations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05629
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#38271;&#25991;&#29983;&#25104;&#20013;&#30340;&#20107;&#23454;&#24615;&#38382;&#39064;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#27573;&#33853;&#26102;&#21487;&#33021;&#20250;&#30001;&#20110;&#23454;&#20307;&#27169;&#31946;&#32780;&#23558;&#21487;&#39564;&#35777;&#30340;&#20107;&#23454;&#32452;&#21512;&#25104;&#38750;&#20107;&#23454;&#30340;&#27573;&#33853;&#12290;&#29616;&#26377;&#30340;&#20107;&#23454;&#20934;&#30830;&#24230;&#35780;&#20272;&#26041;&#27861;&#26080;&#27861;&#27491;&#30830;&#35780;&#20272;&#36825;&#20123;&#38750;&#20107;&#23454;&#27573;&#33853;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#24230;&#37327;&#25351;&#26631;&#26469;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20135;&#29983;&#30340;&#38271;&#25991;&#29983;&#25104;&#29289;&#21253;&#21547;&#20102;&#19968;&#31995;&#21015;&#20107;&#23454;&#21644;&#38750;&#20107;&#23454;&#30340;&#20027;&#24352;&#65292;&#36825;&#20351;&#24471;&#35780;&#20272;&#20107;&#23454;&#24615;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#20102;&#20197;&#26356;&#31934;&#32454;&#30340;&#26041;&#24335;&#35780;&#20272;&#38271;&#25991;&#29983;&#25104;&#29289;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#23558;&#38271;&#25991;&#29983;&#25104;&#29289;&#20998;&#35299;&#20026;&#22810;&#20010;&#21487;&#39564;&#35777;&#30340;&#20107;&#23454;&#24182;&#29420;&#31435;&#39564;&#35777;&#36825;&#20123;&#20107;&#23454;&#12290;&#29983;&#25104;&#29289;&#30340;&#20107;&#23454;&#24615;&#26159;&#25152;&#26377;&#20107;&#23454;&#20013;&#21487;&#39564;&#35777;&#20107;&#23454;&#30340;&#27604;&#20363;&#12290;&#36825;&#20123;&#26041;&#27861;&#20551;&#35774;&#32467;&#21512;&#20102;&#20107;&#23454;&#20027;&#24352;&#24418;&#25104;&#20102;&#19968;&#20010;&#20107;&#23454;&#24615;&#27573;&#33853;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#36825;&#19968;&#20551;&#35774;&#21487;&#33021;&#22240;&#20026;&#23454;&#20307;&#27169;&#31946;&#32780;&#34987;&#36829;&#21453;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LLMs&#21487;&#20197;&#29983;&#25104;&#21253;&#21547;&#21487;&#39564;&#35777;&#20107;&#23454;&#30340;&#27573;&#33853;&#65292;&#20294;&#30001;&#20110;&#23454;&#20307;&#27169;&#31946;&#65292;&#36825;&#20123;&#20107;&#23454;&#34987;&#32467;&#21512;&#24418;&#25104;&#20102;&#19968;&#20010;&#38750;&#20107;&#23454;&#30340;&#27573;&#33853;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#29616;&#26377;&#30340;&#20107;&#23454;&#20934;&#30830;&#24230;&#24230;&#37327;&#25351;&#26631;&#65292;&#21253;&#25324;FActScore&#21644;&#24341;&#29992;&#22238;&#39038;&#65292;&#26080;&#27861;&#27491;&#30830;&#35780;&#20272;&#36825;&#20123;&#38750;&#20107;&#23454;&#27573;&#33853;&#30340;&#20107;&#23454;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22686;&#24378;&#24230;&#37327;&#25351;&#26631;&#65292;D-FActScore&#65292;&#20316;&#20026;&#19968;&#20010;&#20855;&#20307;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Long-form generations from large language models (LLMs) contains a mix of factual and non-factual claims, making evaluating factuality difficult. To evaluate factual precision of long-form generations in a more fine-grained way, prior works propose to decompose long-form generations into multiple verifiable facts and verify those facts independently. The factuality of the generation is the proportion of verifiable facts among all the facts. Such methods assume that combining factual claims forms a factual paragraph. This paper shows that the assumption can be violated due to entity ambiguity. We show that LLMs can generate paragraphs that contain verifiable facts, but the facts are combined to form a non-factual paragraph due to entity ambiguity. We further reveal that existing factual precision metrics, including FActScore and citation recall, cannot properly evaluate the factuality of these non-factual paragraphs. To address this, we introduce an enhanced metric, D-FActScore, specifi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#19977;&#20010;&#26631;&#31614;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20559;&#35265;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#36896;&#20195;&#34920;&#19981;&#21516;&#31867;&#22411;&#20559;&#35265;&#30340;&#35780;&#20272;&#25968;&#25454;&#32452;&#65292;&#24182;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#26356;&#22909;&#22320;&#21306;&#20998;&#26377;&#20559;&#35265;&#30340;&#12289;&#19981;&#27491;&#30830;&#30340;&#25512;&#29702;&#21644;&#38750;&#26377;&#20559;&#35265;&#30340;&#19981;&#27491;&#30830;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2309.09697</link><description>&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20013;&#35780;&#20272;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#32771;&#34385;&#25152;&#26377;&#26631;&#31614;
&lt;/p&gt;
&lt;p&gt;
Evaluating Gender Bias of Pre-trained Language Models in Natural Language Inference by Considering All Labels
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.09697
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#19977;&#20010;&#26631;&#31614;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20559;&#35265;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#36896;&#20195;&#34920;&#19981;&#21516;&#31867;&#22411;&#20559;&#35265;&#30340;&#35780;&#20272;&#25968;&#25454;&#32452;&#65292;&#24182;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#26356;&#22909;&#22320;&#21306;&#20998;&#26377;&#20559;&#35265;&#30340;&#12289;&#19981;&#27491;&#30830;&#30340;&#25512;&#29702;&#21644;&#38750;&#26377;&#20559;&#35265;&#30340;&#19981;&#27491;&#30830;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#31181;&#35821;&#35328;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#20013;&#21457;&#29616;&#20102;&#27495;&#35270;&#24615;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#20013;&#65292;&#29616;&#26377;&#30340;&#20559;&#35265;&#35780;&#20272;&#26041;&#27861;&#19987;&#27880;&#20110;&#19977;&#20010;&#26631;&#31614;&#20013;&#30340;&#19968;&#20010;&#29305;&#23450;&#26631;&#31614;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#20363;&#22914;&#20013;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35780;&#20272;&#26041;&#27861;&#21487;&#33021;&#19981;&#20934;&#30830;&#65292;&#22240;&#20026;&#29420;&#29305;&#30340;&#20559;&#35265;&#25512;&#29702;&#19982;&#29420;&#29305;&#30340;&#39044;&#27979;&#26631;&#31614;&#30456;&#20851;&#32852;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;NLI&#20219;&#21153;&#30340;&#19977;&#20010;&#26631;&#31614;&#30340;PLMs&#20559;&#35265;&#35780;&#20272;&#26041;&#27861;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19977;&#20010;&#20195;&#34920;&#19981;&#21516;&#31867;&#22411;&#20559;&#35265;&#30340;&#35780;&#20272;&#25968;&#25454;&#32452;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22522;&#20110;&#27599;&#20010;&#25968;&#25454;&#32452;&#30340;&#30456;&#24212;&#26631;&#31614;&#36755;&#20986;&#23450;&#20041;&#20102;&#19968;&#31181;&#20559;&#35265;&#24230;&#37327;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#20110;NLI&#20559;&#35265;&#24230;&#37327;&#30340;&#20803;&#35780;&#20272;&#25216;&#26415;&#65292;&#24182;&#29992;&#23427;&#26469;&#30830;&#35748;&#25105;&#20204;&#30340;&#20559;&#35265;&#24230;&#37327;&#21487;&#20197;&#26356;&#22909;&#22320;&#21306;&#20998;&#26377;&#20559;&#35265;&#30340;&#65292;&#19981;&#27491;&#30830;&#30340;&#25512;&#29702;&#19982;&#38750;&#20559;&#35265;&#30340;&#19981;&#27491;&#30830;&#25512;&#29702;&#65292;&#32988;&#36807;&#22522;&#32447;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;m
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.09697v2 Announce Type: replace  Abstract: Discriminatory gender biases have been found in Pre-trained Language Models (PLMs) for multiple languages. In Natural Language Inference (NLI), existing bias evaluation methods have focused on the prediction results of a specific label out of three labels, such as neutral. However, such evaluation methods can be inaccurate since unique biased inferences are associated with unique prediction labels. Addressing this limitation, we propose a bias evaluation method for PLMs that considers all the three labels of NLI task. We create three evaluation data groups that represent different types of biases. Then, we define a bias measure based on the corresponding label output of each data group. In the experiments, we introduce a meta-evaluation technique for NLI bias measures and use it to confirm that our bias measure can distinguish biased, incorrect inferences from non-biased incorrect inferences better than the baseline, resulting in a m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#27010;&#24565;&#24863;&#30693;&#27880;&#24847;&#21147;&#30340;&#30693;&#35782;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#19981;&#33391;&#33647;&#29289;&#20107;&#20214;&#26816;&#27979;&#65292;&#36890;&#36807;&#23558;&#21307;&#23398;&#30693;&#35782;&#21644;&#25991;&#26412;&#22270;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#19981;&#21516;&#31867;&#22411;&#33410;&#28857;&#30340;&#29305;&#24449;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2301.10451</link><description>&lt;p&gt;
&#20855;&#26377;&#27010;&#24565;&#24863;&#30693;&#27880;&#24847;&#21147;&#30340;&#30693;&#35782;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#19981;&#33391;&#33647;&#29289;&#20107;&#20214;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Knowledge-augmented Graph Neural Networks with Concept-aware Attention for Adverse Drug Event Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.10451
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#27010;&#24565;&#24863;&#30693;&#27880;&#24847;&#21147;&#30340;&#30693;&#35782;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#19981;&#33391;&#33647;&#29289;&#20107;&#20214;&#26816;&#27979;&#65292;&#36890;&#36807;&#23558;&#21307;&#23398;&#30693;&#35782;&#21644;&#25991;&#26412;&#22270;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#19981;&#21516;&#31867;&#22411;&#33410;&#28857;&#30340;&#29305;&#24449;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#33391;&#33647;&#29289;&#20107;&#20214;&#65288;ADEs&#65289;&#26159;&#33647;&#29289;&#23433;&#20840;&#30340;&#37325;&#35201;&#26041;&#38754;&#12290;&#21508;&#31181;&#25991;&#26412;&#65292;&#22914;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#12289;&#33647;&#29289;&#35780;&#35770;&#12289;&#31038;&#20132;&#23186;&#20307;&#21644;&#21307;&#30103;&#35770;&#22363;&#19978;&#30340;&#29992;&#25143;&#24086;&#23376;&#65292;&#21253;&#21547;&#22823;&#37327;&#20851;&#20110;ADEs&#30340;&#20449;&#24687;&#12290;&#36817;&#26399;&#30340;&#30740;&#31350;&#24212;&#29992;&#20102;&#22522;&#20110;&#35789;&#23884;&#20837;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#26469;&#33258;&#21160;&#21270;&#20174;&#25991;&#26412;&#20013;&#26816;&#27979;ADEs&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#26410;&#25506;&#35752;&#22914;&#20309;&#23558;&#33647;&#29289;&#21644;&#19981;&#33391;&#21453;&#24212;&#30340;&#26174;&#24335;&#21307;&#23398;&#30693;&#35782;&#25110;&#30456;&#24212;&#30340;&#29305;&#24449;&#23398;&#20064;&#32435;&#20837;&#21040;&#20854;&#20013;&#12290;&#26412;&#25991;&#37319;&#29992;&#25551;&#36848;&#25991;&#26723;&#12289;&#21333;&#35789;&#21644;&#27010;&#24565;&#20043;&#38388;&#20851;&#31995;&#30340;&#24322;&#36136;&#25991;&#26412;&#22270;&#65292;&#24182;&#36890;&#36807;&#32479;&#19968;&#21307;&#23398;&#35821;&#35328;&#31995;&#32479;&#20013;&#30340;&#21307;&#23398;&#30693;&#35782;&#22686;&#24378;&#23427;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#24565;&#24863;&#30693;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#20026;&#22270;&#20013;&#19981;&#21516;&#31867;&#22411;&#30340;&#33410;&#28857;&#19981;&#21516;&#22320;&#23398;&#20064;&#29305;&#24449;&#12290;&#25105;&#20204;&#36824;&#21033;&#29992;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#21270;&#23884;&#20837;&#21644;&#21367;&#31215;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#36827;&#34892;&#26377;&#25928;&#30340;&#29305;&#24449;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.10451v2 Announce Type: replace  Abstract: Adverse drug events (ADEs) are an important aspect of drug safety. Various texts such as biomedical literature, drug reviews, and user posts on social media and medical forums contain a wealth of information about ADEs. Recent studies have applied word embedding and deep learning -based natural language processing to automate ADE detection from text. However, they did not explore incorporating explicit medical knowledge about drugs and adverse reactions or the corresponding feature learning. This paper adopts the heterogenous text graph which describes relationships between documents, words and concepts, augments it with medical knowledge from the Unified Medical Language System, and proposes a concept-aware attention mechanism which learns features differently for the different types of nodes in the graph. We further utilize contextualized embeddings from pretrained language models and convolutional graph neural networks for effecti
&lt;/p&gt;</description></item><item><title>MoSECroT&#26159;&#19968;&#20010;&#32467;&#21512;&#38745;&#24577;&#35789;&#21521;&#37327;&#30340;&#27169;&#22411;&#25340;&#25509;&#26694;&#26550;&#65292;&#29992;&#20110;&#36328;&#35821;&#35328;&#38646;&#26679;&#20363;&#36801;&#31227;&#12290;&#23427;&#21033;&#29992;&#30456;&#23545;&#34920;&#31034;&#26500;&#24314;&#20102;&#28304;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#30446;&#26631;&#35821;&#35328;&#38745;&#24577;&#35789;&#21521;&#37327;&#30340;&#20849;&#20139;&#31354;&#38388;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#36890;&#36807;&#31616;&#21333;&#20132;&#25442;&#23884;&#20837;&#20174;&#28304;&#35821;&#35328;&#35757;&#32451;&#25968;&#25454;&#20013;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#30446;&#26631;&#35821;&#35328;&#19978;&#36827;&#34892;&#38646;&#26679;&#20363;&#36801;&#31227;&#12290;</title><link>http://arxiv.org/abs/2401.04821</link><description>&lt;p&gt;
MoSECroT: &#20351;&#29992;&#38745;&#24577;&#35789;&#21521;&#37327;&#36827;&#34892;&#27169;&#22411;&#25340;&#25509;&#23454;&#29616;&#36328;&#35821;&#35328;&#38646;&#26679;&#20363;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
MoSECroT: Model Stitching with Static Word Embeddings for Crosslingual Zero-shot Transfer. (arXiv:2401.04821v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04821
&lt;/p&gt;
&lt;p&gt;
MoSECroT&#26159;&#19968;&#20010;&#32467;&#21512;&#38745;&#24577;&#35789;&#21521;&#37327;&#30340;&#27169;&#22411;&#25340;&#25509;&#26694;&#26550;&#65292;&#29992;&#20110;&#36328;&#35821;&#35328;&#38646;&#26679;&#20363;&#36801;&#31227;&#12290;&#23427;&#21033;&#29992;&#30456;&#23545;&#34920;&#31034;&#26500;&#24314;&#20102;&#28304;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#30446;&#26631;&#35821;&#35328;&#38745;&#24577;&#35789;&#21521;&#37327;&#30340;&#20849;&#20139;&#31354;&#38388;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#36890;&#36807;&#31616;&#21333;&#20132;&#25442;&#23884;&#20837;&#20174;&#28304;&#35821;&#35328;&#35757;&#32451;&#25968;&#25454;&#20013;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#30446;&#26631;&#35821;&#35328;&#19978;&#36827;&#34892;&#38646;&#26679;&#20363;&#36801;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#38656;&#35201;&#22823;&#37327;&#36164;&#28304;&#65292;&#32780;&#36825;&#20123;&#36164;&#28304;&#20960;&#20046;&#21482;&#26377;&#39640;&#36164;&#28304;&#35821;&#35328;&#25165;&#33021;&#33719;&#24471;&#12290;&#30456;&#21453;&#65292;&#38745;&#24577;&#35789;&#21521;&#37327;&#30340;&#35757;&#32451;&#26356;&#23481;&#26131;&#65292;&#21487;&#20197;&#26356;&#33410;&#30465;&#35745;&#31639;&#36164;&#28304;&#21644;&#25968;&#25454;&#37327;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;MoSECroT&#65288;Model Stitching with Static Word Embeddings for Crosslingual Zero-shot Transfer&#65289;&#27169;&#22411;&#25340;&#25509;&#19982;&#38745;&#24577;&#35789;&#21521;&#37327;&#32467;&#21512;&#30340;&#26032;&#39062;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#23384;&#22312;&#38745;&#24577;&#35789;&#21521;&#37327;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21033;&#29992;&#30456;&#23545;&#34920;&#31034;&#26500;&#24314;&#28304;&#35821;&#35328;PLM&#23884;&#20837;&#21644;&#30446;&#26631;&#35821;&#35328;&#38745;&#24577;&#35789;&#21521;&#37327;&#20043;&#38388;&#30340;&#20849;&#20139;&#31354;&#38388;&#30340;&#26694;&#26550;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#28304;&#35821;&#35328;&#35757;&#32451;&#25968;&#25454;&#35757;&#32451;PLM&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#22320;&#20132;&#25442;&#23884;&#20837;&#23436;&#25104;&#20174;&#28304;&#35821;&#35328;&#21040;&#30446;&#26631;&#35821;&#35328;&#30340;&#38646;&#26679;&#20363;&#36801;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based pre-trained language models (PLMs) have achieved remarkable performance in various natural language processing (NLP) tasks. However, pre-training such models can take considerable resources that are almost only available to high-resource languages. On the contrary, static word embeddings are easier to train in terms of computing resources and the amount of data required. In this paper, we introduce MoSECroT Model Stitching with Static Word Embeddings for Crosslingual Zero-shot Transfer), a novel and challenging task that is especially relevant to low-resource languages for which static word embeddings are available. To tackle the task, we present the first framework that leverages relative representations to construct a common space for the embeddings of a source language PLM and the static word embeddings of a target language. In this way, we can train the PLM on source-language training data and perform zero-shot transfer to the target language by simply swapping th
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#23545;&#34920;&#26684;&#25968;&#25454;&#26597;&#35810;&#21644;&#21487;&#35270;&#21270;&#30340;&#33258;&#28982;&#35821;&#35328;&#30028;&#38754;&#36827;&#34892;&#20102;&#20840;&#38754;&#27010;&#36848;&#65292;&#20171;&#32461;&#20102;&#35821;&#20041;&#35299;&#26512;&#31561;&#20851;&#38190;&#25216;&#26415;&#65292;&#24182;&#28145;&#20837;&#25506;&#35752;&#20102;Text-to-SQL&#21644;Text-to-Vis&#38382;&#39064;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2310.17894</link><description>&lt;p&gt;
&#23545;&#34920;&#26684;&#25968;&#25454;&#26597;&#35810;&#21644;&#21487;&#35270;&#21270;&#30340;&#33258;&#28982;&#35821;&#35328;&#30028;&#38754;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Natural Language Interfaces for Tabular Data Querying and Visualization: A Survey. (arXiv:2310.17894v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17894
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#23545;&#34920;&#26684;&#25968;&#25454;&#26597;&#35810;&#21644;&#21487;&#35270;&#21270;&#30340;&#33258;&#28982;&#35821;&#35328;&#30028;&#38754;&#36827;&#34892;&#20102;&#20840;&#38754;&#27010;&#36848;&#65292;&#20171;&#32461;&#20102;&#35821;&#20041;&#35299;&#26512;&#31561;&#20851;&#38190;&#25216;&#26415;&#65292;&#24182;&#28145;&#20837;&#25506;&#35752;&#20102;Text-to-SQL&#21644;Text-to-Vis&#38382;&#39064;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#20986;&#29616;&#24443;&#24213;&#25913;&#21464;&#20102;&#29992;&#25143;&#19982;&#34920;&#26684;&#25968;&#25454;&#30340;&#20132;&#20114;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#20174;&#20256;&#32479;&#30340;&#26597;&#35810;&#35821;&#35328;&#21644;&#25163;&#21160;&#32472;&#22270;&#36716;&#21521;&#26356;&#30452;&#35266;&#12289;&#22522;&#20110;&#35821;&#35328;&#30340;&#30028;&#38754;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;ChatGPT&#21450;&#20854;&#21518;&#32487;&#32773;&#36827;&#19968;&#27493;&#25512;&#21160;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;&#26412;&#35843;&#26597;&#25552;&#20379;&#20102;&#20851;&#20110;&#34920;&#26684;&#25968;&#25454;&#26597;&#35810;&#21644;&#21487;&#35270;&#21270;&#30340;&#33258;&#28982;&#35821;&#35328;&#30028;&#38754;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#36825;&#20123;&#30028;&#38754;&#20801;&#35768;&#29992;&#25143;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#19982;&#25968;&#25454;&#36827;&#34892;&#20132;&#20114;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#36825;&#20123;&#30028;&#38754;&#30340;&#22522;&#26412;&#27010;&#24565;&#21644;&#25216;&#26415;&#65292;&#29305;&#21035;&#24378;&#35843;&#35821;&#20041;&#35299;&#26512;&#65292;&#36825;&#26159;&#23454;&#29616;&#20174;&#33258;&#28982;&#35821;&#35328;&#21040;SQL&#26597;&#35810;&#25110;&#25968;&#25454;&#21487;&#35270;&#21270;&#21629;&#20196;&#36716;&#21270;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#28982;&#21518;&#20174;&#25968;&#25454;&#38598;&#12289;&#26041;&#27861;&#35770;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#31995;&#32479;&#35774;&#35745;&#30340;&#35282;&#24230;&#28145;&#20837;&#25506;&#35752;&#20102;Text-to-SQL&#21644;Text-to-Vis&#38382;&#39064;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of natural language processing has revolutionized the way users interact with tabular data, enabling a shift from traditional query languages and manual plotting to more intuitive, language-based interfaces. The rise of large language models (LLMs) such as ChatGPT and its successors has further advanced this field, opening new avenues for natural language processing techniques. This survey presents a comprehensive overview of natural language interfaces for tabular data querying and visualization, which allow users to interact with data using natural language queries. We introduce the fundamental concepts and techniques underlying these interfaces with a particular emphasis on semantic parsing, the key technology facilitating the translation from natural language to SQL queries or data visualization commands. We then delve into the recent advancements in Text-to-SQL and Text-to-Vis problems from the perspectives of datasets, methodologies, metrics, and system designs. Thi
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20219;&#21153;&#32423;MoE&#22312;&#30452;&#25509;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#39640;&#24615;&#33021;&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#37197;&#32622;&#65292;&#36890;&#36807;&#36825;&#20123;&#37197;&#32622;&#65292;&#20219;&#21153;&#32423;MoE&#30340;&#30452;&#25509;NMT&#31995;&#32479;&#22312;&#22823;&#37327;&#20302;&#36164;&#28304;&#35821;&#35328;&#23545;&#19978;&#20248;&#20110;&#21452;&#35821;&#21644;&#22522;&#20110;&#20013;&#38388;&#35821;&#35328;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.12236</link><description>&lt;p&gt;
&#20351;&#29992;&#20219;&#21153;&#32423;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#30340;&#30452;&#25509;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Direct Neural Machine Translation with Task-level Mixture of Experts models. (arXiv:2310.12236v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12236
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20219;&#21153;&#32423;MoE&#22312;&#30452;&#25509;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#39640;&#24615;&#33021;&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#37197;&#32622;&#65292;&#36890;&#36807;&#36825;&#20123;&#37197;&#32622;&#65292;&#20219;&#21153;&#32423;MoE&#30340;&#30452;&#25509;NMT&#31995;&#32479;&#22312;&#22823;&#37327;&#20302;&#36164;&#28304;&#35821;&#35328;&#23545;&#19978;&#20248;&#20110;&#21452;&#35821;&#21644;&#22522;&#20110;&#20013;&#38388;&#35821;&#35328;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30452;&#25509;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;Direct NMT&#65289;&#26159;&#19968;&#31181;&#22312;&#20004;&#31181;&#38750;&#33521;&#35821;&#35821;&#35328;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#30340;NMT&#31995;&#32479;&#12290;&#30452;&#25509;NMT&#31995;&#32479;&#36890;&#24120;&#38754;&#20020;&#30001;&#20110;&#38750;&#33521;&#35821;&#35821;&#35328;&#23545;&#20043;&#38388;&#24179;&#34892;&#25968;&#25454;&#31232;&#32570;&#23548;&#33268;&#30340;&#38480;&#21046;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#20363;&#22914;&#22810;&#35821;NMT&#21644;&#22522;&#20110;&#20013;&#38388;&#35821;&#35328;&#65288;&#36890;&#36807;&#33521;&#35821;&#36827;&#34892;&#32763;&#35793;&#30340;NMT&#65289;&#30340;NMT&#12290;&#20219;&#21153;&#32423;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#65288;Task-level MoE&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#25512;&#29702;&#39640;&#25928;&#21464;&#20307;&#65292;&#23545;&#35768;&#22810;&#35821;&#35328;&#23545;&#23637;&#29616;&#20102;&#26377;&#21069;&#26223;&#30340;NMT&#24615;&#33021;&#12290;&#22312;&#20219;&#21153;&#32423;MoE&#20013;&#65292;&#19981;&#21516;&#30340;&#35821;&#35328;&#20998;&#32452;&#21487;&#20197;&#20351;&#29992;&#19981;&#21516;&#30340;&#36335;&#30001;&#31574;&#30053;&#26469;&#20248;&#21270;&#36328;&#35821;&#35328;&#23398;&#20064;&#21644;&#25512;&#29702;&#36895;&#24230;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20219;&#21153;&#32423;MoE&#22312;&#30452;&#25509;NMT&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#39640;&#24615;&#33021;&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#37197;&#32622;&#65292;&#36890;&#36807;&#36825;&#20123;&#20219;&#21153;&#32423;MoE&#22522;&#30784;&#30340;&#30452;&#25509;NMT&#31995;&#32479;&#22312;&#22823;&#37327;&#20302;&#36164;&#28304;&#35821;&#35328;&#23545;&#19978;&#20248;&#20110;&#21452;&#35821;&#21644;&#22522;&#20110;&#20013;&#38388;&#35821;&#35328;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Direct neural machine translation (direct NMT) is a type of NMT system that translates text between two non-English languages. Direct NMT systems often face limitations due to the scarcity of parallel data between non-English language pairs. Several approaches have been proposed to address this limitation, such as multilingual NMT and pivot NMT (translation between two languages via English). Task-level Mixture of expert models (Task-level MoE), an inference-efficient variation of Transformer-based models, has shown promising NMT performance for a large number of language pairs. In Task-level MoE, different language groups can use different routing strategies to optimize cross-lingual learning and inference speed. In this work, we examine Task-level MoE's applicability in direct NMT and propose a series of high-performing training and evaluation configurations, through which Task-level MoE-based direct NMT systems outperform bilingual and pivot-based models for a large number of low an
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#20154;&#31867;&#35760;&#24518;&#26426;&#21046;&#21551;&#21457;&#30340;&#31070;&#32463;&#27169;&#22359;&#65292;&#27169;&#25311;&#20154;&#31867;&#21644;&#26426;&#22120;&#22914;&#20309;&#23545;&#24403;&#21069;&#36755;&#20837;&#36827;&#34892;&#20851;&#32852;&#25512;&#29702;&#21644;&#38382;&#31572;&#65292;&#24182;&#23558;&#20854;&#19982;&#36807;&#21435;&#30340;&#35760;&#24518;&#32467;&#21512;&#36215;&#26469;&#12290;&#36890;&#36807;&#24863;&#30693;&#12289;&#35760;&#24518;&#21644;&#25512;&#29702;&#32452;&#20214;&#65292;&#35813;&#27169;&#22359;&#23454;&#29616;&#20102;&#24863;&#30693;&#26356;&#26032;&#12289;&#35760;&#24518;&#34701;&#21512;&#21644;&#20449;&#24687;&#26816;&#32034;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.09297</link><description>&lt;p&gt;
&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#35748;&#30693;&#65306;&#21463;&#20154;&#31867;&#35760;&#24518;&#26426;&#21046;&#21551;&#21457;&#30340;&#25512;&#29702;&#31070;&#32463;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
Understanding AI Cognition: A Neural Module for Inference Inspired by Human Memory Mechanisms. (arXiv:2310.09297v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09297
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#20154;&#31867;&#35760;&#24518;&#26426;&#21046;&#21551;&#21457;&#30340;&#31070;&#32463;&#27169;&#22359;&#65292;&#27169;&#25311;&#20154;&#31867;&#21644;&#26426;&#22120;&#22914;&#20309;&#23545;&#24403;&#21069;&#36755;&#20837;&#36827;&#34892;&#20851;&#32852;&#25512;&#29702;&#21644;&#38382;&#31572;&#65292;&#24182;&#23558;&#20854;&#19982;&#36807;&#21435;&#30340;&#35760;&#24518;&#32467;&#21512;&#36215;&#26469;&#12290;&#36890;&#36807;&#24863;&#30693;&#12289;&#35760;&#24518;&#21644;&#25512;&#29702;&#32452;&#20214;&#65292;&#35813;&#27169;&#22359;&#23454;&#29616;&#20102;&#24863;&#30693;&#26356;&#26032;&#12289;&#35760;&#24518;&#34701;&#21512;&#21644;&#20449;&#24687;&#26816;&#32034;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21644;&#26426;&#22120;&#22914;&#20309;&#23558;&#24403;&#21069;&#30340;&#36755;&#20837;&#19982;&#36807;&#21435;&#30340;&#35760;&#24518;&#32467;&#21512;&#36215;&#26469;&#65292;&#36827;&#34892;&#20851;&#32852;&#25512;&#29702;&#21644;&#38382;&#31572;&#65292;&#24182;&#23558;&#24863;&#30693;&#21040;&#30340;&#20449;&#24687;&#32622;&#20110;&#19978;&#19979;&#25991;&#20013;&#65292;&#36825;&#26159;&#35748;&#30693;&#31185;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35868;&#39064;&#12290;&#21463;&#21040;&#20154;&#33041;&#35760;&#24518;&#31995;&#32479;&#21644;&#35748;&#30693;&#32467;&#26500;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#24863;&#30693;&#12289;&#35760;&#24518;&#21644;&#25512;&#29702;&#32452;&#20214;&#30340;PMI&#26694;&#26550;&#12290;&#29305;&#21035;&#22320;&#65292;&#35760;&#24518;&#27169;&#22359;&#21253;&#25324;&#24037;&#20316;&#35760;&#24518;&#21644;&#38271;&#26399;&#35760;&#24518;&#65292;&#20854;&#20013;&#21518;&#32773;&#20855;&#26377;&#26356;&#39640;&#38454;&#30340;&#32467;&#26500;&#26469;&#20445;&#30041;&#26356;&#22810;&#30340;&#32047;&#31215;&#30693;&#35782;&#21644;&#32463;&#39564;&#12290;&#36890;&#36807;&#21487;&#21306;&#20998;&#30340;&#31454;&#20105;&#20889;&#20837;&#35775;&#38382;&#65292;&#24403;&#21069;&#30340;&#24863;&#30693;&#26356;&#26032;&#24037;&#20316;&#35760;&#24518;&#65292;&#20043;&#21518;&#36890;&#36807;&#22806;&#31215;&#20851;&#32852;&#19982;&#38271;&#26399;&#35760;&#24518;&#34701;&#21512;&#65292;&#36991;&#20813;&#20869;&#23384;&#28322;&#20986;&#24182;&#26368;&#23567;&#21270;&#20449;&#24687;&#20914;&#31361;&#12290;&#22312;&#25512;&#29702;&#27169;&#22359;&#20013;&#65292;&#30456;&#20851;&#20449;&#24687;&#20174;&#20004;&#20010;&#21333;&#29420;&#30340;&#35760;&#24518;&#28304;&#26816;&#32034;&#24182;&#32467;&#21512;&#65292;&#20197;&#33719;&#24471;&#26356;&#20840;&#38754;&#21644;&#31934;&#30830;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
How humans and machines make sense of current inputs for relation reasoning and question-answering while putting the perceived information into context of our past memories, has been a challenging conundrum in cognitive science and artificial intelligence. Inspired by human brain's memory system and cognitive architectures, we propose a PMI framework that consists of perception, memory and inference components. Notably, the memory module comprises working and long-term memory, with the latter endowed with a higher-order structure to retain more accumulated knowledge and experiences. Through a differentiable competitive write access, current perceptions update working memory, which is later merged with long-term memory via outer product associations, averting memory overflow and minimizing information conflicts. In the inference module, relevant information is retrieved from two separate memory origins and associatively integrated to attain a more comprehensive and precise interpretatio
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#19981;&#21464;&#27700;&#21360;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#21478;&#19968;&#20010;&#23884;&#20837;&#24335;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25152;&#26377;&#21069;&#38754;token&#30340;&#35821;&#20041;&#23884;&#20837;&#65292;&#28982;&#21518;&#21033;&#29992;&#35757;&#32451;&#24471;&#21040;&#30340;&#27700;&#21360;&#27169;&#22411;&#23558;&#36825;&#20123;&#35821;&#20041;&#23884;&#20837;&#36716;&#25442;&#20026;&#27700;&#21360;logits&#12290;&#35813;&#26041;&#27861;&#26082;&#20855;&#26377;&#25915;&#20987;&#40065;&#26834;&#24615;&#21448;&#20855;&#26377;&#23433;&#20840;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.06356</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#19981;&#21464;&#40065;&#26834;&#27700;&#21360;
&lt;/p&gt;
&lt;p&gt;
A Semantic Invariant Robust Watermark for Large Language Models. (arXiv:2310.06356v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06356
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#19981;&#21464;&#27700;&#21360;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#21478;&#19968;&#20010;&#23884;&#20837;&#24335;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25152;&#26377;&#21069;&#38754;token&#30340;&#35821;&#20041;&#23884;&#20837;&#65292;&#28982;&#21518;&#21033;&#29992;&#35757;&#32451;&#24471;&#21040;&#30340;&#27700;&#21360;&#27169;&#22411;&#23558;&#36825;&#20123;&#35821;&#20041;&#23884;&#20837;&#36716;&#25442;&#20026;&#27700;&#21360;logits&#12290;&#35813;&#26041;&#27861;&#26082;&#20855;&#26377;&#25915;&#20987;&#40065;&#26834;&#24615;&#21448;&#20855;&#26377;&#23433;&#20840;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27700;&#21360;&#31639;&#27861;&#22312;&#26816;&#27979;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#26041;&#38754;&#21462;&#24471;&#20102;&#26497;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#31639;&#27861;&#22312;&#25915;&#20987;&#40065;&#26834;&#24615;&#21644;&#23433;&#20840;&#40065;&#26834;&#24615;&#20043;&#38388;&#23384;&#22312;&#19968;&#20010;&#26435;&#34913;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;LLM&#36827;&#34892;&#35821;&#20041;&#19981;&#21464;&#27700;&#21360;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26082;&#20855;&#26377;&#25915;&#20987;&#40065;&#26834;&#24615;&#21448;&#20855;&#26377;&#23433;&#20840;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#21478;&#19968;&#20010;&#23884;&#20837;&#24335;LLM&#29983;&#25104;&#25152;&#26377;&#21069;&#38754;token&#30340;&#35821;&#20041;&#23884;&#20837;&#65292;&#28982;&#21518;&#36890;&#36807;&#35757;&#32451;&#24471;&#21040;&#30340;&#27700;&#21360;&#27169;&#22411;&#23558;&#36825;&#20123;&#35821;&#20041;&#23884;&#20837;&#36716;&#25442;&#20026;&#27700;&#21360;logits&#12290;
&lt;/p&gt;
&lt;p&gt;
Watermark algorithms for large language models (LLMs) have achieved extremely high accuracy in detecting text generated by LLMs. Such algorithms typically involve adding extra watermark logits to the LLM's logits at each generation step. However, prior algorithms face a trade-off between attack robustness and security robustness. This is because the watermark logits for a token are determined by a certain number of preceding tokens; a small number leads to low security robustness, while a large number results in insufficient attack robustness. In this work, we propose a semantic invariant watermarking method for LLMs that provides both attack robustness and security robustness. The watermark logits in our work are determined by the semantics of all preceding tokens. Specifically, we utilize another embedding LLM to generate semantic embeddings for all preceding tokens, and then these semantic embeddings are transformed into the watermark logits through our trained watermark model. Subs
&lt;/p&gt;</description></item><item><title>&#35780;&#22996;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#35780;&#20272;&#26694;&#26550;&#21644;&#24037;&#20855;&#21253;&#65292;&#26088;&#22312;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#20013;&#19981;&#21516;&#31995;&#32479;&#21644;&#25351;&#26631;&#30340;&#35780;&#20272;&#25361;&#25112;&#65292;&#24182;&#25913;&#36827;&#24230;&#37327;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2310.02040</link><description>&lt;p&gt;
&#35780;&#22996;&#65306;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
Jury: A Comprehensive Evaluation Toolkit. (arXiv:2310.02040v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02040
&lt;/p&gt;
&lt;p&gt;
&#35780;&#22996;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#35780;&#20272;&#26694;&#26550;&#21644;&#24037;&#20855;&#21253;&#65292;&#26088;&#22312;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#20013;&#19981;&#21516;&#31995;&#32479;&#21644;&#25351;&#26631;&#30340;&#35780;&#20272;&#25361;&#25112;&#65292;&#24182;&#25913;&#36827;&#24230;&#37327;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#25198;&#28436;&#30528;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#65292;&#20316;&#20026;&#20219;&#20309;&#22522;&#20110;&#39044;&#27979;&#30340;&#31995;&#32479;&#30340;&#22522;&#26412;&#27169;&#22359;&#12290;&#28982;&#32780;&#65292;&#22823;&#37327;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#21644;&#21508;&#31181;&#25351;&#26631;&#30340;&#21457;&#23637;&#23548;&#33268;&#20102;&#20351;&#29992;&#19981;&#21516;&#25351;&#26631;&#35780;&#20272;&#19981;&#21516;&#31995;&#32479;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#35780;&#22996;&#30340;&#24037;&#20855;&#21253;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35780;&#20272;&#26694;&#26550;&#21644;&#26631;&#20934;&#21270;&#32467;&#26500;&#65292;&#20197;&#36328;&#19981;&#21516;&#20219;&#21153;&#21644;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#12290;&#35780;&#22996;&#30340;&#30446;&#26631;&#26159;&#26631;&#20934;&#21270;&#21644;&#25913;&#36827;&#25152;&#26377;&#31995;&#32479;&#30340;&#24230;&#37327;&#35780;&#20272;&#65292;&#24182;&#24110;&#21161;&#31038;&#21306;&#20811;&#26381;&#35780;&#20272;&#20013;&#30340;&#25361;&#25112;&#12290;&#33258;&#35780;&#22996;&#30340;&#24320;&#28304;&#21457;&#24067;&#20197;&#26469;&#65292;&#24050;&#32463;&#21560;&#24341;&#20102;&#24191;&#22823;&#29992;&#25143;&#65292;&#21487;&#22312;https://github.com/obss/jury &#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluation plays a critical role in deep learning as a fundamental block of any prediction-based system. However, the vast number of Natural Language Processing (NLP) tasks and the development of various metrics have led to challenges in evaluating different systems with different metrics. To address these challenges, we introduce jury, a toolkit that provides a unified evaluation framework with standardized structures for performing evaluation across different tasks and metrics. The objective of jury is to standardize and improve metric evaluation for all systems and aid the community in overcoming the challenges in evaluation. Since its open-source release, jury has reached a wide audience and is available at https://github.com/obss/jury.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Auto-UI&#30340;&#22810;&#27169;&#24577;&#21160;&#20316;&#38142;&#26426;&#22120;&#20154;&#65292;&#36890;&#36807;&#30452;&#25509;&#19982;&#30028;&#38754;&#20132;&#20114;&#65292;&#36991;&#20813;&#20102;&#29615;&#22659;&#35299;&#26512;&#25110;&#20381;&#36182;&#20110;&#24212;&#29992;&#31243;&#24207;API&#30340;&#38656;&#35201;&#65292;&#24182;&#24341;&#20837;&#20102;&#21160;&#20316;&#38142;&#25216;&#26415;&#26469;&#24110;&#21161;&#27169;&#22411;&#36827;&#34892;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2309.11436</link><description>&lt;p&gt;
&#20320;&#20165;&#20851;&#27880;&#23631;&#24149;&#65306;&#22810;&#27169;&#24577;&#21160;&#20316;&#38142;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
You Only Look at Screens: Multimodal Chain-of-Action Agents. (arXiv:2309.11436v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Auto-UI&#30340;&#22810;&#27169;&#24577;&#21160;&#20316;&#38142;&#26426;&#22120;&#20154;&#65292;&#36890;&#36807;&#30452;&#25509;&#19982;&#30028;&#38754;&#20132;&#20114;&#65292;&#36991;&#20813;&#20102;&#29615;&#22659;&#35299;&#26512;&#25110;&#20381;&#36182;&#20110;&#24212;&#29992;&#31243;&#24207;API&#30340;&#38656;&#35201;&#65292;&#24182;&#24341;&#20837;&#20102;&#21160;&#20316;&#38142;&#25216;&#26415;&#26469;&#24110;&#21161;&#27169;&#22411;&#36827;&#34892;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#29992;&#25143;&#30028;&#38754;&#65288;UI&#65289;&#26426;&#22120;&#20154;&#26088;&#22312;&#36890;&#36807;&#19982;&#29992;&#25143;&#30028;&#38754;&#36827;&#34892;&#20132;&#20114;&#65292;&#23454;&#29616;&#20219;&#21153;&#33258;&#21160;&#21270;&#65292;&#26080;&#38656;&#25163;&#21160;&#24178;&#39044;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#65292;&#20197;&#22312;&#22810;&#26679;&#29615;&#22659;&#20013;&#26377;&#25928;&#21442;&#19982;&#12290;&#20026;&#20102;&#31526;&#21512;LLM&#30340;&#36755;&#20837;-&#36755;&#20986;&#35201;&#27714;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#27801;&#30418;&#29615;&#22659;&#20013;&#24320;&#21457;&#65292;&#20381;&#36182;&#20110;&#22806;&#37096;&#24037;&#20855;&#21644;&#24212;&#29992;&#31243;&#24207;&#29305;&#23450;&#30340;API&#23558;&#29615;&#22659;&#35299;&#26512;&#20026;&#25991;&#26412;&#20803;&#32032;&#65292;&#24182;&#35299;&#37322;&#39044;&#27979;&#30340;&#21160;&#20316;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#26041;&#27861;&#24120;&#24120;&#21463;&#21040;&#25512;&#29702;&#25928;&#29575;&#20302;&#21644;&#38169;&#35823;&#20256;&#25773;&#39118;&#38505;&#30340;&#22256;&#25200;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Auto-UI&#65292;&#19968;&#31181;&#22810;&#27169;&#24577;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#30452;&#25509;&#19982;&#30028;&#38754;&#20132;&#20114;&#65292;&#36991;&#20813;&#20102;&#23545;&#29615;&#22659;&#35299;&#26512;&#25110;&#20381;&#36182;&#20110;&#24212;&#29992;&#31243;&#24207;&#30456;&#20851;&#30340;API&#30340;&#38656;&#27714;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#20316;&#38142;&#25216;&#26415;&#65292;&#21033;&#29992;&#19968;&#31995;&#21015;&#20013;&#38388;&#20808;&#21069;&#21160;&#20316;&#21382;&#21490;&#21644;&#26410;&#26469;&#21160;&#20316;&#35745;&#21010;&#65292;&#20197;&#24110;&#21161;&#27169;&#22411;&#36827;&#34892;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous user interface (UI) agents aim to facilitate task automation by interacting with the user interface without manual intervention. Recent studies have investigated eliciting the capabilities of large language models (LLMs) for effective engagement in diverse environments. To align with the input-output requirement of LLMs, existing approaches are developed under a sandbox setting where they rely on external tools and application-specific APIs to parse the environment into textual elements and interpret the predicted actions. Consequently, those approaches often grapple with inference inefficiency and error propagation risks. To mitigate the challenges, we introduce Auto-UI, a multimodal solution that directly interacts with the interface, bypassing the need for environment parsing or reliance on application-dependent APIs. Moreover, we propose a chain-of-action technique -leveraging a series of intermediate previous action histories and future action plans -- to help the age
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#32773;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;K-pop&#27468;&#35789;&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#25581;&#31034;&#20102;K-pop&#27468;&#35789;&#32763;&#35793;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#31070;&#32463;&#27468;&#35789;&#32763;&#35793;&#27169;&#22411;&#65292;&#24378;&#35843;&#20102;&#19987;&#29992;&#25968;&#25454;&#38598;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.11093</link><description>&lt;p&gt;
K-pop&#27468;&#35789;&#32763;&#35793;&#65306;&#25968;&#25454;&#38598;&#12289;&#20998;&#26512;&#19982;&#31070;&#32463;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
K-pop Lyric Translation: Dataset, Analysis, and Neural-Modelling. (arXiv:2309.11093v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11093
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32773;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;K-pop&#27468;&#35789;&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#25581;&#31034;&#20102;K-pop&#27468;&#35789;&#32763;&#35793;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#31070;&#32463;&#27468;&#35789;&#32763;&#35793;&#27169;&#22411;&#65292;&#24378;&#35843;&#20102;&#19987;&#29992;&#25968;&#25454;&#38598;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27468;&#35789;&#32763;&#35793;&#20316;&#20026;&#19968;&#20010;&#30740;&#31350;&#20102;&#19968;&#20010;&#19990;&#32426;&#30340;&#39046;&#22495;&#65292;&#22914;&#20170;&#21560;&#24341;&#30528;&#35745;&#31639;&#35821;&#35328;&#23398;&#30740;&#31350;&#32773;&#30340;&#27880;&#24847;&#12290;&#25105;&#20204;&#22312;&#20197;&#24448;&#30740;&#31350;&#20013;&#21457;&#29616;&#20102;&#20004;&#20010;&#38480;&#21046;&#12290;&#39318;&#20808;&#65292;&#22312;&#27468;&#35789;&#32763;&#35793;&#30740;&#31350;&#20013;&#65292;&#23613;&#31649;K-pop&#38750;&#24120;&#21463;&#27426;&#36814;&#65292;&#20294;&#20027;&#35201;&#20851;&#27880;&#30340;&#26159;&#35199;&#26041;&#27969;&#27966;&#21644;&#35821;&#35328;&#65292;&#27809;&#26377;&#30740;&#31350;&#38598;&#20013;&#22312;K-pop&#19978;&#12290;&#20854;&#27425;&#65292;&#27468;&#35789;&#32763;&#35793;&#39046;&#22495;&#32570;&#20047;&#21487;&#20844;&#24320;&#33719;&#24471;&#30340;&#25968;&#25454;&#38598;&#65307;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#30446;&#21069;&#23578;&#26080;&#27492;&#31867;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#25299;&#23485;&#27468;&#35789;&#32763;&#35793;&#30740;&#31350;&#30340;&#27969;&#27966;&#21644;&#35821;&#35328;&#33539;&#22260;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#21809;&#27468;&#35789;&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#32422;89%&#20026;K-pop&#27468;&#35789;&#12290;&#35813;&#25968;&#25454;&#38598;&#36890;&#36807;&#36880;&#34892;&#21644;&#36880;&#33410;&#23545;&#40784;&#20102;&#38889;&#35821;&#21644;&#33521;&#35821;&#27468;&#35789;&#12290;&#25105;&#20204;&#21033;&#29992;&#35813;&#25968;&#25454;&#38598;&#25581;&#31034;&#20102;K-pop&#27468;&#35789;&#32763;&#35793;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#19982;&#20854;&#20182;&#24191;&#27867;&#30740;&#31350;&#30340;&#27969;&#27966;&#21306;&#20998;&#24320;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#31070;&#32463;&#27468;&#35789;&#32763;&#35793;&#27169;&#22411;&#65292;&#20174;&#32780;&#24378;&#35843;&#20102;&#19987;&#29992;&#25968;&#25454;&#38598;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lyric translation, a field studied for over a century, is now attracting computational linguistics researchers. We identified two limitations in previous studies. Firstly, lyric translation studies have predominantly focused on Western genres and languages, with no previous study centering on K-pop despite its popularity. Second, the field of lyric translation suffers from a lack of publicly available datasets; to the best of our knowledge, no such dataset exists. To broaden the scope of genres and languages in lyric translation studies, we introduce a novel singable lyric translation dataset, approximately 89\% of which consists of K-pop song lyrics. This dataset aligns Korean and English lyrics line-by-line and section-by-section. We leveraged this dataset to unveil unique characteristics of K-pop lyric translation, distinguishing it from other extensively studied genres, and to construct a neural lyric translation model, thereby underscoring the importance of a dedicated dataset for
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#33258;&#25105;&#25512;&#27979;&#35299;&#30721;&#30340;Draft &amp; Verify&#26041;&#27861;&#33021;&#22815;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#21516;&#26102;&#20445;&#25345;&#36755;&#20986;&#36136;&#37327;&#65292;&#24182;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#21644;&#20869;&#23384;&#21344;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.08168</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#25105;&#25512;&#27979;&#35299;&#30721;&#23454;&#29616;&#26080;&#25439;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21152;&#36895;&#30340;Draft &amp; Verify&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Draft &amp; Verify: Lossless Large Language Model Acceleration via Self-Speculative Decoding. (arXiv:2309.08168v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08168
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#33258;&#25105;&#25512;&#27979;&#35299;&#30721;&#30340;Draft &amp; Verify&#26041;&#27861;&#33021;&#22815;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#21516;&#26102;&#20445;&#25345;&#36755;&#20986;&#36136;&#37327;&#65292;&#24182;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#21644;&#20869;&#23384;&#21344;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25512;&#29702;&#26041;&#26696;&#65292;&#31216;&#20026;&#33258;&#25105;&#25512;&#27979;&#35299;&#30721;&#65292;&#29992;&#20110;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#32780;&#26080;&#38656;&#36741;&#21161;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#36807;&#31243;&#26469;&#23454;&#29616;&#65306;&#36215;&#33609;&#21644;&#39564;&#35777;&#12290;&#36215;&#33609;&#38454;&#27573;&#20197;&#31245;&#20302;&#36136;&#37327;&#20294;&#26356;&#24555;&#30340;&#36895;&#24230;&#29983;&#25104;&#33609;&#26696;&#26631;&#35760;&#65292;&#36825;&#26159;&#36890;&#36807;&#22312;&#36215;&#33609;&#36807;&#31243;&#20013;&#26377;&#36873;&#25321;&#22320;&#36339;&#36807;&#26576;&#20123;&#20013;&#38388;&#23618;&#26469;&#23454;&#29616;&#30340;&#12290;&#38543;&#21518;&#65292;&#39564;&#35777;&#38454;&#27573;&#20351;&#29992;&#21407;&#22987;LLM&#22312;&#19968;&#27425;&#21069;&#21521;&#20256;&#36882;&#20013;&#39564;&#35777;&#37027;&#20123;&#36215;&#33609;&#20135;&#29983;&#30340;&#36755;&#20986;&#26631;&#35760;&#12290;&#36825;&#20010;&#36807;&#31243;&#30830;&#20445;&#26368;&#32456;&#36755;&#20986;&#19982;&#26410;&#20462;&#25913;&#30340;LLM&#20135;&#29983;&#30340;&#36755;&#20986;&#23436;&#20840;&#30456;&#21516;&#65292;&#20174;&#32780;&#30830;&#20445;&#36755;&#20986;&#36136;&#37327;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#21644;&#39069;&#22806;&#30340;&#20869;&#23384;&#21344;&#29992;&#65292;&#21487;&#20197;&#20316;&#20026;&#19968;&#31181;&#21363;&#25554;&#21363;&#29992;&#19988;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#25512;&#29702;&#21152;&#36895;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel inference scheme, self-speculative decoding, for accelerating Large Language Models (LLMs) without the need for an auxiliary model. This approach is characterized by a two-stage process: drafting and verification. The drafting stage generates draft tokens at a slightly lower quality but more quickly, which is achieved by selectively skipping certain intermediate layers during drafting Subsequently, the verification stage employs the original LLM to validate those draft output tokens in one forward pass. This process ensures the final output remains identical to that produced by the unaltered LLM, thereby maintaining output quality. The proposed method requires no additional neural network training and no extra memory footprint, making it a plug-and-play and cost-effective solution for inference acceleration. Benchmarks with LLaMA-2 and its fine-tuned models demonstrated a speedup up to 1.73$\times$.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#37325;&#26032;&#35782;&#21035;&#21311;&#21517;&#20010;&#20154;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#27169;&#22411;&#22823;&#23567;&#12289;&#36755;&#20837;&#38271;&#24230;&#21644;&#25351;&#20196;&#35843;&#25972;&#26159;&#26368;&#37325;&#35201;&#30340;&#20915;&#23450;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2308.11103</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20877;&#35782;&#21035;&#33021;&#21147;&#65306;&#21311;&#21517;&#38754;&#20020;&#39118;&#38505;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Anonymity at Risk? Assessing Re-Identification Capabilities of Large Language Models. (arXiv:2308.11103v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#37325;&#26032;&#35782;&#21035;&#21311;&#21517;&#20010;&#20154;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#27169;&#22411;&#22823;&#23567;&#12289;&#36755;&#20837;&#38271;&#24230;&#21644;&#25351;&#20196;&#35843;&#25972;&#26159;&#26368;&#37325;&#35201;&#30340;&#20915;&#23450;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27431;&#30431;&#21644;&#29790;&#22763;&#65292;&#27861;&#38498;&#35009;&#20915;&#20013;&#33258;&#28982;&#20154;&#21644;&#27861;&#20154;&#30340;&#21311;&#21517;&#24615;&#26159;&#38544;&#31169;&#20445;&#25252;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65292;&#23545;&#20110;&#21311;&#21517;&#20154;&#21592;&#30340;&#22823;&#35268;&#27169;&#20877;&#35782;&#21035;&#30340;&#25285;&#24551;&#26085;&#30410;&#22686;&#38271;&#12290;&#26681;&#25454;&#29790;&#22763;&#32852;&#37030;&#26368;&#39640;&#27861;&#38498;&#30340;&#35201;&#27714;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#29790;&#22763;&#32852;&#37030;&#26368;&#39640;&#27861;&#38498;&#30340;&#23454;&#38469;&#27861;&#24459;&#25968;&#25454;&#26500;&#24314;&#20102;&#19968;&#20010;&#27010;&#24565;&#39564;&#35777;&#65292;&#26469;&#25506;&#35752;LLMs&#37325;&#26032;&#35782;&#21035;&#27861;&#38498;&#35009;&#20915;&#20013;&#20010;&#20154;&#30340;&#28508;&#21147;&#12290;&#22312;&#26368;&#21021;&#30340;&#23454;&#39564;&#20043;&#21518;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#32463;&#36807;&#21311;&#21517;&#21270;&#22788;&#29702;&#30340;&#32500;&#22522;&#30334;&#31185;&#25968;&#25454;&#38598;&#65292;&#20316;&#20026;&#19968;&#20010;&#26356;&#20005;&#26684;&#30340;&#27979;&#35797;&#22330;&#22320;&#26469;&#36827;&#19968;&#27493;&#30740;&#31350;&#30740;&#31350;&#32467;&#26524;&#12290;&#36890;&#36807;&#24341;&#20837;&#24182;&#24212;&#29992;&#25991;&#26412;&#20013;&#20877;&#35782;&#21035;&#20154;&#21592;&#30340;&#26032;&#20219;&#21153;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#26032;&#30340;&#24615;&#33021;&#34913;&#37327;&#25351;&#26631;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#20998;&#26512;&#20102;&#24433;&#21709;&#25104;&#21151;&#20877;&#35782;&#21035;&#30340;&#22240;&#32032;&#65292;&#30830;&#23450;&#27169;&#22411;&#22823;&#23567;&#12289;&#36755;&#20837;&#38271;&#24230;&#21644;&#25351;&#20196;&#35843;&#25972;&#26159;&#26368;&#37325;&#35201;&#30340;&#20915;&#23450;&#22240;&#32032;&#20043;&#19968;&#12290;&#23613;&#31649;&#22312;&#21311;&#21517;&#21270;&#22788;&#29702;&#21518;&#65292;LLMs&#22312;&#37325;&#26032;&#35782;&#21035;&#19978;&#30340;&#25104;&#21151;&#29575;&#24456;&#39640;&#65292;&#20294;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#20173;&#28982;&#23384;&#22312;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anonymity of both natural and legal persons in court rulings is a critical aspect of privacy protection in the European Union and Switzerland. With the advent of LLMs, concerns about large-scale re-identification of anonymized persons are growing. In accordance with the Federal Supreme Court of Switzerland, we explore the potential of LLMs to re-identify individuals in court rulings by constructing a proof-of-concept using actual legal data from the Swiss federal supreme court. Following the initial experiment, we constructed an anonymized Wikipedia dataset as a more rigorous testing ground to further investigate the findings. With the introduction and application of the new task of re-identifying people in texts, we also introduce new metrics to measure performance. We systematically analyze the factors that influence successful re-identifications, identifying model size, input length, and instruction tuning among the most critical determinants. Despite high re-identification rates on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#39033;&#36873;&#25321;&#32422;&#40065;&#24052;&#35821;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;YORC&#65292;&#36890;&#36807;&#36328;&#35821;&#35328;&#36716;&#31227;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20351;&#29992;&#65292;&#25552;&#20379;&#20102;&#35813;&#25968;&#25454;&#38598;&#30340;&#22522;&#20934;&#32467;&#26524;&#21644;&#26356;&#39640;&#23618;&#27425;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.09768</link><description>&lt;p&gt;
YORC&#65306;&#32422;&#40065;&#24052;&#35821;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
YORC: Yoruba Reading Comprehension dataset. (arXiv:2308.09768v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09768
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#39033;&#36873;&#25321;&#32422;&#40065;&#24052;&#35821;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;YORC&#65292;&#36890;&#36807;&#36328;&#35821;&#35328;&#36716;&#31227;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20351;&#29992;&#65292;&#25552;&#20379;&#20102;&#35813;&#25968;&#25454;&#38598;&#30340;&#22522;&#20934;&#32467;&#26524;&#21644;&#26356;&#39640;&#23618;&#27425;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;YORC&#65306;&#19968;&#20010;&#22522;&#20110;&#32422;&#40065;&#24052;&#35821;&#39640;&#20013;&#38405;&#35835;&#29702;&#35299;&#32771;&#35797;&#30340;&#26032;&#30340;&#22810;&#39033;&#36873;&#25321;&#32422;&#40065;&#24052;&#35821;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;&#24050;&#35757;&#32451;&#30340;&#20165;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#36890;&#36807;&#36328;&#35821;&#35328;&#36716;&#31227;&#26469;&#25552;&#20379;&#22522;&#20934;&#32467;&#26524;&#65292;&#21516;&#26102;&#36824;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-4&#25552;&#20379;&#20102;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we create YORC: a new multi-choice Yoruba Reading Comprehension dataset that is based on Yoruba high-school reading comprehension examination. We provide baseline results by performing cross-lingual transfer using existing English RACE dataset based on a pre-trained encoder-only model. Additionally, we provide results by prompting large language models (LLMs) like GPT-4.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#23884;&#20837;&#27169;&#22411;&#30340;&#38598;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#22522;&#22240;&#31361;&#21464;&#22312;&#30284;&#30151;&#20013;&#30340;&#20998;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#20934;&#30830;&#29575;&#12289;&#31934;&#30830;&#29575;&#12289;&#21484;&#22238;&#29575;&#31561;&#25351;&#26631;&#19978;&#20248;&#20110;&#20854;&#20182;&#20256;&#32479;&#21644;&#26368;&#26032;&#30340;&#36716;&#25442;&#22120;&#27169;&#22411;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#39640;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.14361</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;LSTM&#12289;BiLSTM&#12289;CNN&#12289;GRU&#21644;GloVe&#30340;&#28151;&#21512;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#29992;&#20110;&#22522;&#22240;&#31361;&#21464;&#22312;&#30284;&#30151;&#20013;&#30340;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
A Hybrid Machine Learning Model for Classifying Gene Mutations in Cancer using LSTM, BiLSTM, CNN, GRU, and GloVe. (arXiv:2307.14361v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#23884;&#20837;&#27169;&#22411;&#30340;&#38598;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#22522;&#22240;&#31361;&#21464;&#22312;&#30284;&#30151;&#20013;&#30340;&#20998;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#20934;&#30830;&#29575;&#12289;&#31934;&#30830;&#29575;&#12289;&#21484;&#22238;&#29575;&#31561;&#25351;&#26631;&#19978;&#20248;&#20110;&#20854;&#20182;&#20256;&#32479;&#21644;&#26368;&#26032;&#30340;&#36716;&#25442;&#22120;&#27169;&#22411;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#39640;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#27169;&#22411;&#65292;&#23558;LSTM&#12289;BiLSTM&#12289;CNN&#12289;GRU&#21644;GloVe&#32467;&#21512;&#36215;&#26469;&#65292;&#29992;&#20110;&#22312;Kaggle&#30340;&#8220;&#20010;&#24615;&#21270;&#21307;&#23398;&#65306;&#37325;&#26032;&#23450;&#20041;&#30284;&#30151;&#27835;&#30103;&#8221;&#25968;&#25454;&#38598;&#20013;&#23545;&#22522;&#22240;&#31361;&#21464;&#36827;&#34892;&#20998;&#31867;&#12290;&#36890;&#36807;&#19982;BERT&#12289;Electra&#12289;Roberta&#12289;XLNet&#12289;Distilbert&#20197;&#21450;&#23427;&#20204;&#30340;LSTM&#38598;&#25104;&#31561;&#30693;&#21517;&#36716;&#25442;&#22120;&#36827;&#34892;&#27604;&#36739;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20934;&#30830;&#29575;&#12289;&#31934;&#30830;&#29575;&#12289;&#21484;&#22238;&#29575;&#12289;F1&#20998;&#25968;&#21644;&#22343;&#26041;&#35823;&#24046;&#26041;&#38754;&#37117;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#23427;&#36824;&#38656;&#35201;&#36739;&#23569;&#30340;&#35757;&#32451;&#26102;&#38388;&#65292;&#23454;&#29616;&#20102;&#24615;&#33021;&#21644;&#25928;&#29575;&#30340;&#23436;&#32654;&#32467;&#21512;&#12290;&#35813;&#30740;&#31350;&#35777;&#26126;&#20102;&#38598;&#25104;&#27169;&#22411;&#22312;&#22522;&#22240;&#31361;&#21464;&#20998;&#31867;&#31561;&#22256;&#38590;&#20219;&#21153;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents an ensemble model combining LSTM, BiLSTM, CNN, GRU, and GloVe to classify gene mutations using Kaggle's Personalized Medicine: Redefining Cancer Treatment dataset. The results were compared against well-known transformers like as BERT, Electra, Roberta, XLNet, Distilbert, and their LSTM ensembles. Our model outperformed all other models in terms of accuracy, precision, recall, F1 score, and Mean Squared Error. Surprisingly, it also needed less training time, resulting in a perfect combination of performance and efficiency. This study demonstrates the utility of ensemble models for difficult tasks such as gene mutation classification.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#31934;&#24515;&#21046;&#20316;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#24341;&#23548;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;LLMs&#25805;&#20316;&#20219;&#21153;&#29305;&#23450;&#23646;&#24615;&#24182;&#37325;&#26500;&#26032;&#30340;&#21477;&#23376;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#26631;&#31614;&#20132;&#25442;&#25968;&#25454;&#30340;&#29983;&#25104;&#65292;&#19982;&#20854;&#20182;&#22522;&#20110;LLMs&#30340;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#32467;&#26524;&#36824;&#26174;&#31034;&#20102;&#36890;&#36807;LLM&#24341;&#23548;&#23398;&#20064;&#30340;&#28508;&#21147;&#65292;&#21363;&#20351;&#22312;&#26356;&#23569;&#30340;&#30417;&#30563;&#24773;&#20917;&#19979;&#20063;&#33021;&#21462;&#24471;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.07099</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;LLM&#30340;&#23646;&#24615;&#25805;&#20316;&#29983;&#25104;&#39640;&#25928;&#35757;&#32451;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Generating Efficient Training Data via LLM-based Attribute Manipulation. (arXiv:2307.07099v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#31934;&#24515;&#21046;&#20316;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#24341;&#23548;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;LLMs&#25805;&#20316;&#20219;&#21153;&#29305;&#23450;&#23646;&#24615;&#24182;&#37325;&#26500;&#26032;&#30340;&#21477;&#23376;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#26631;&#31614;&#20132;&#25442;&#25968;&#25454;&#30340;&#29983;&#25104;&#65292;&#19982;&#20854;&#20182;&#22522;&#20110;LLMs&#30340;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#32467;&#26524;&#36824;&#26174;&#31034;&#20102;&#36890;&#36807;LLM&#24341;&#23548;&#23398;&#20064;&#30340;&#28508;&#21147;&#65292;&#21363;&#20351;&#22312;&#26356;&#23569;&#30340;&#30417;&#30563;&#24773;&#20917;&#19979;&#20063;&#33021;&#21462;&#24471;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#38142;&#24335;&#24605;&#32500;&#23646;&#24615;&#25805;&#20316;&#65288;CoTAM&#65289;&#65292;&#36890;&#36807;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#31934;&#24515;&#21046;&#20316;&#30340;&#25968;&#25454;&#26469;&#24341;&#23548;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#20027;&#35201;&#24605;&#24819;&#26159;&#20165;&#23545;&#20219;&#21153;&#30446;&#26631;&#23646;&#24615;&#36827;&#34892;&#26356;&#25913;&#24182;&#21019;&#24314;&#25968;&#25454;&#12290;&#21463;&#21040;&#38754;&#37096;&#23646;&#24615;&#25805;&#20316;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;LLMs&#26469;&#25805;&#20316;&#20219;&#21153;&#29305;&#23450;&#23646;&#24615;&#24182;&#20197;&#21463;&#25511;&#30340;&#26041;&#24335;&#37325;&#26500;&#26032;&#30340;&#21477;&#23376;&#65292;&#20174;&#32780;&#29983;&#25104;&#26631;&#31614;&#20132;&#25442;&#25968;&#25454;&#12290;&#25105;&#20204;&#37319;&#29992;&#38142;&#24335;&#24605;&#32500;&#20998;&#35299;&#21644;&#37325;&#26500;&#26469;&#36866;&#24212;LLMs&#65292;&#32780;&#19981;&#26159;&#20256;&#32479;&#30340;&#28508;&#22312;&#34920;&#31034;&#25511;&#21046;&#26041;&#27861;&#12290;&#22312;&#25991;&#26412;&#20998;&#31867;&#21644;&#20854;&#20182;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;CoTAM&#30456;&#23545;&#20110;&#20854;&#20182;&#20855;&#26377;&#30456;&#21516;&#25968;&#37327;&#35757;&#32451;&#26679;&#26412;&#30340;&#22522;&#20110;LLMs&#30340;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;&#20998;&#26512;&#32467;&#26524;&#21487;&#35270;&#21270;&#20102;CoTAM&#30340;&#23646;&#24615;&#25805;&#20316;&#25928;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#26356;&#23569;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#36890;&#36807;LLM&#24341;&#23548;&#23398;&#20064;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel method, Chain-of-Thoughts Attribute Manipulation (CoTAM), to guide few-shot learning by carefully crafted data from Large Language Models (LLMs). The main idea is to create data with changes only in the attribute targeted by the task. Inspired by facial attribute manipulation, our approach generates label-switched data by leveraging LLMs to manipulate task-specific attributes and reconstruct new sentences in a controlled manner. Instead of conventional latent representation controlling, we implement chain-of-thoughts decomposition and reconstruction to adapt the procedure to LLMs. Extensive results on text classification and other tasks verify the advantage of CoTAM over other LLM-based text generation methods with the same number of training examples. Analysis visualizes the attribute manipulation effectiveness of CoTAM and presents the potential of LLM-guided learning with even less supervision.
&lt;/p&gt;</description></item><item><title>Statler&#26159;&#19968;&#20010;&#20026;LLMs&#36171;&#20104;&#20102;&#26126;&#30830;&#30340;&#12289;&#32500;&#25345;&#29366;&#24577;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#35299;&#20915;&#24403;&#20195;LLMs&#22312;&#38271;&#26102;&#38388;&#33539;&#22260;&#20869;&#25512;&#29702;&#30340;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2306.17840</link><description>&lt;p&gt;
Statler&#65306;&#29992;&#20110;&#20855;&#36523;&#25512;&#29702;&#30340;&#20445;&#25345;&#29366;&#24577;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Statler: State-Maintaining Language Models for Embodied Reasoning. (arXiv:2306.17840v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17840
&lt;/p&gt;
&lt;p&gt;
Statler&#26159;&#19968;&#20010;&#20026;LLMs&#36171;&#20104;&#20102;&#26126;&#30830;&#30340;&#12289;&#32500;&#25345;&#29366;&#24577;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#35299;&#20915;&#24403;&#20195;LLMs&#22312;&#38271;&#26102;&#38388;&#33539;&#22260;&#20869;&#25512;&#29702;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20026;&#26426;&#22120;&#20154;&#25191;&#34892;&#22797;&#26434;&#30340;&#26426;&#22120;&#20154;&#25512;&#29702;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#24403;&#20195;LLMs&#30340;&#26377;&#38480;&#19978;&#19979;&#25991;&#31383;&#21475;&#20351;&#24471;&#22312;&#38271;&#26102;&#38388;&#33539;&#22260;&#20869;&#36827;&#34892;&#25512;&#29702;&#21464;&#24471;&#22256;&#38590;&#12290;&#20855;&#36523;&#20219;&#21153;&#65288;&#20363;&#22914;&#25105;&#20204;&#26399;&#26395;&#19968;&#20010;&#23478;&#24237;&#26426;&#22120;&#20154;&#25191;&#34892;&#30340;&#20219;&#21153;&#65289;&#36890;&#24120;&#38656;&#35201;&#35268;&#21010;&#32773;&#32771;&#34385;&#24456;&#20037;&#20043;&#21069;&#33719;&#24471;&#30340;&#20449;&#24687;&#65288;&#20363;&#22914;&#65292;&#26426;&#22120;&#20154;&#22312;&#29615;&#22659;&#20013;&#36935;&#21040;&#30340;&#35768;&#22810;&#23545;&#35937;&#30340;&#23646;&#24615;&#65289;&#12290;&#36890;&#36807;LLM&#30340;&#38544;&#21547;&#20869;&#37096;&#34920;&#31034;&#26469;&#25429;&#33719;&#19990;&#30028;&#29366;&#24577;&#30340;&#23581;&#35797;&#20250;&#22240;&#20026;&#26426;&#22120;&#20154;&#25805;&#20316;&#21382;&#21490;&#20013;&#21487;&#29992;&#30340;&#19982;&#20219;&#21153;&#21644;&#29615;&#22659;&#30456;&#20851;&#30340;&#20449;&#24687;&#26377;&#38480;&#32780;&#21464;&#24471;&#22797;&#26434;&#65292;&#32780;&#20381;&#36182;&#36890;&#36807;&#25552;&#31034;&#21521;LLM&#20256;&#36882;&#20449;&#24687;&#30340;&#26041;&#27861;&#21017;&#21463;&#20854;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Statler&#65292;&#19968;&#20010;&#20026;LLMs&#36171;&#20104;&#20102;&#26126;&#30830;&#30340;&#12289;&#20316;&#20026;&#8220;&#35760;&#24518;&#8221;&#30340;&#19990;&#30028;&#29366;&#24577;&#34920;&#31034;&#30340;&#26694;&#26550;&#65292;&#36825;&#31181;&#35760;&#24518;&#38543;&#26102;&#38388;&#20445;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) provide a promising tool that enable robots to perform complex robot reasoning tasks. However, the limited context window of contemporary LLMs makes reasoning over long time horizons difficult. Embodied tasks such as those that one might expect a household robot to perform typically require that the planner consider information acquired a long time ago (e.g., properties of the many objects that the robot previously encountered in the environment). Attempts to capture the world state using an LLM's implicit internal representation is complicated by the paucity of task- and environment-relevant information available in a robot's action history, while methods that rely on the ability to convey information via the prompt to the LLM are subject to its limited context window. In this paper, we propose Statler, a framework that endows LLMs with an explicit representation of the world state as a form of ``memory'' that is maintained over time. Integral to Statler i
&lt;/p&gt;</description></item><item><title>MultiLegalPile&#26159;&#19968;&#20010;689GB&#30340;&#22810;&#35821;&#35328;&#27861;&#24459;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547;&#26469;&#33258;17&#20010;&#21496;&#27861;&#31649;&#36758;&#21306;&#30340;24&#31181;&#35821;&#35328;&#30340;&#19981;&#21516;&#27861;&#24459;&#25968;&#25454;&#28304;&#65292;&#20801;&#35768;&#22312;&#20844;&#24179;&#20351;&#29992;&#19979;&#38024;&#23545;&#39044;&#35757;&#32451;NLP&#27169;&#22411;&#12290;&#35813;&#35821;&#26009;&#24211;&#20026;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#25552;&#20379;&#20102;&#26032;&#30340;&#26368;&#20339;&#34920;&#29616;&#65292;&#24182;&#22312;LexGLUE&#19978;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2306.02069</link><description>&lt;p&gt;
MultiLegalPile&#65306;689GB&#30340;&#22810;&#35821;&#35328;&#27861;&#24459;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
MultiLegalPile: A 689GB Multilingual Legal Corpus. (arXiv:2306.02069v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02069
&lt;/p&gt;
&lt;p&gt;
MultiLegalPile&#26159;&#19968;&#20010;689GB&#30340;&#22810;&#35821;&#35328;&#27861;&#24459;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547;&#26469;&#33258;17&#20010;&#21496;&#27861;&#31649;&#36758;&#21306;&#30340;24&#31181;&#35821;&#35328;&#30340;&#19981;&#21516;&#27861;&#24459;&#25968;&#25454;&#28304;&#65292;&#20801;&#35768;&#22312;&#20844;&#24179;&#20351;&#29992;&#19979;&#38024;&#23545;&#39044;&#35757;&#32451;NLP&#27169;&#22411;&#12290;&#35813;&#35821;&#26009;&#24211;&#20026;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#25552;&#20379;&#20102;&#26032;&#30340;&#26368;&#20339;&#34920;&#29616;&#65292;&#24182;&#22312;LexGLUE&#19978;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#23545;&#20110;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20026;&#27490;&#65292;&#19987;&#19994;&#39046;&#22495;&#65288;&#22914;&#27861;&#24459;&#65289;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#24456;&#23569;&#65292;&#32780;&#19988;&#32463;&#24120;&#20165;&#38480;&#20110;&#33521;&#35821;&#12290;&#25105;&#20204;&#25972;&#29702;&#24182;&#21457;&#24067;&#20102;MultiLegalPile&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#26469;&#33258;17&#20010;&#21496;&#27861;&#31649;&#36758;&#21306;&#30340;24&#31181;&#35821;&#35328;&#30340;689GB&#35821;&#26009;&#24211;&#12290;MultiLegalPile&#35821;&#26009;&#24211;&#21253;&#25324;&#21508;&#31181;&#35768;&#21487;&#35777;&#30340;&#19981;&#21516;&#27861;&#24459;&#25968;&#25454;&#28304;&#65292;&#20801;&#35768;&#22312;&#20844;&#24179;&#20351;&#29992;&#19979;&#38024;&#23545;&#39044;&#35757;&#32451;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#27169;&#22411;&#65292;&#23545;&#20110;Eurlex Resources&#21644;Legal mC4&#23376;&#38598;&#25317;&#26377;&#26356;&#23485;&#26494;&#30340;&#35768;&#21487;&#35777;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20004;&#20010;RoBERTa&#27169;&#22411;&#21644;&#19968;&#20010;&#22810;&#35821;&#35328;Longformer&#30340;&#39044;&#35757;&#32451;&#65292;&#24182;&#20998;&#21035;&#22312;&#27599;&#31181;&#29305;&#23450;&#35821;&#35328;&#23376;&#38598;&#19978;&#36827;&#34892;&#20102;24&#20010;&#21333;&#35821;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#65292;&#24182;&#22312;LEXTREME&#19978;&#23545;&#23427;&#20204;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;LexGLUE&#19978;&#23545;&#33521;&#35821;&#21644;&#22810;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#22312;LEXTREME&#19978;&#21019;&#36896;&#20102;&#26032;&#30340;&#26368;&#20339;&#34920;&#29616;(SotA)&#65292;&#33521;&#35821;&#27169;&#22411;&#21017;&#22312;LexGLUE&#19978;&#34920;&#29616;&#26368;&#20339;&#12290;&#25105;&#20204;&#23558;&#25968;&#25454;&#38598;&#12289;&#35757;&#32451;&#27169;&#22411;&#21644;&#20195;&#30721;&#20840;&#37096;&#37322;&#25918;&#22312;&#26368;&#24320;&#25918;&#30340;&#35768;&#21487;&#35777;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large, high-quality datasets are crucial for training Large Language Models (LLMs). However, so far, there are few datasets available for specialized critical domains such as law and the available ones are often only for the English language. We curate and release MultiLegalPile, a 689GB corpus in 24 languages from 17 jurisdictions. The MultiLegalPile corpus, which includes diverse legal data sources with varying licenses, allows for pretraining NLP models under fair use, with more permissive licenses for the Eurlex Resources and Legal mC4 subsets. We pretrain two RoBERTa models and one Longformer multilingually, and 24 monolingual models on each of the language-specific subsets and evaluate them on LEXTREME. Additionally, we evaluate the English and multilingual models on LexGLUE. Our multilingual models set a new SotA on LEXTREME and our English models on LexGLUE. We release the dataset, the trained models, and all of the code under the most open possible licenses.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#24212;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20449;&#24230;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#30740;&#31350;&#40657;&#30418;&#27169;&#22411;&#20013;&#32622;&#20449;&#24230;&#19982;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#36873;&#25321;&#24615;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2305.19187</link><description>&lt;p&gt;
&#29983;&#25104;&#21487;&#20449;&#30340;&#25991;&#26412;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Generating with Confidence: Uncertainty Quantification for Black-box Large Language Models. (arXiv:2305.19187v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19187
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#24212;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20449;&#24230;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#30740;&#31350;&#40657;&#30418;&#27169;&#22411;&#20013;&#32622;&#20449;&#24230;&#19982;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#36873;&#25321;&#24615;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#19987;&#38376;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#20010;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#33021;&#21147;&#65292;&#20294;&#26159;&#35780;&#20272;LLMs&#29983;&#25104;&#30340;&#32467;&#26524;&#30340;&#21487;&#20449;&#24230;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#20851;&#20110;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#30740;&#31350;&#20063;&#36739;&#23569;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#25991;&#29486;&#36890;&#24120;&#20551;&#23450;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#30333;&#30418;&#35775;&#38382;&#65292;&#36825;&#35201;&#20040;&#26159;&#30001;&#20110;&#26368;&#26032;&#30340;LLMs&#30340;&#23553;&#38381;&#28304;&#20195;&#30721;&#30340;&#24615;&#36136;&#65292;&#35201;&#20040;&#26159;&#30001;&#20110;&#35745;&#31639;&#38480;&#21046;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#40657;&#30418;LLMs&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#21306;&#20998;&#20102;&#20004;&#31181;&#23494;&#20999;&#30456;&#20851;&#30340;&#27010;&#24565;: &#21482;&#19982;&#36755;&#20837;&#26377;&#20851;&#30340;&#8220;&#19981;&#30830;&#23450;&#24615;&#8221;&#21644;&#36824;&#19982;&#29983;&#25104;&#30340;&#22238;&#22797;&#26377;&#20851;&#30340;&#8220;&#32622;&#20449;&#24230;&#8221;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#24182;&#27604;&#36739;&#20102;&#20960;&#20010;&#32622;&#20449;&#24230;/&#19981;&#30830;&#23450;&#24230;&#25351;&#26631;&#65292;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#8220;&#36873;&#25321;&#24615;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#8221;&#65292;&#20854;&#20013;&#19981;&#21487;&#38752;&#30340;&#32467;&#26524;&#21487;&#20197;&#34987;&#24573;&#30053;&#25110;&#32773;&#31227;&#20132;&#32473;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) specializing in natural language generation (NLG) have recently started exhibiting promising capabilities across a variety of domains. However, gauging the trustworthiness of responses generated by LLMs remains an open challenge, with limited research on uncertainty quantification for NLG. Furthermore, existing literature typically assumes white-box access to language models, which is becoming unrealistic either due to the closed-source nature of the latest LLMs or due to computational constraints. In this work, we investigate uncertainty quantification in NLG for $\textit{black-box}$ LLMs. We first differentiate two closely-related notions: $\textit{uncertainty}$, which depends only on the input, and $\textit{confidence}$, which additionally depends on the generated response. We then propose and compare several confidence/uncertainty metrics, applying them to $\textit{selective NLG}$, where unreliable results could either be ignored or yielded for further 
&lt;/p&gt;</description></item><item><title>COKE&#26159;&#19968;&#20010;&#26426;&#22120;&#24515;&#26234;&#29702;&#35770;&#30340;&#35748;&#30693;&#30693;&#35782;&#22270;&#35889;&#65292;&#23558;ToM&#24418;&#24335;&#21270;&#20026;&#19968;&#32452;&#32463;&#25163;&#21160;&#39564;&#35777;&#30340;&#35748;&#30693;&#38142;&#65292;&#21487;&#20197;&#24110;&#21161;AI&#31995;&#32479;&#22312;&#31038;&#20132;&#26234;&#33021;&#31561;&#20219;&#21153;&#19978;&#20855;&#22791;&#25512;&#29702;&#20154;&#31867;&#24515;&#26234;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.05390</link><description>&lt;p&gt;
COKE&#65306;&#26426;&#22120;&#24515;&#26234;&#29702;&#35770;&#30340;&#35748;&#30693;&#30693;&#35782;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
COKE: A Cognitive Knowledge Graph for Machine Theory of Mind. (arXiv:2305.05390v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05390
&lt;/p&gt;
&lt;p&gt;
COKE&#26159;&#19968;&#20010;&#26426;&#22120;&#24515;&#26234;&#29702;&#35770;&#30340;&#35748;&#30693;&#30693;&#35782;&#22270;&#35889;&#65292;&#23558;ToM&#24418;&#24335;&#21270;&#20026;&#19968;&#32452;&#32463;&#25163;&#21160;&#39564;&#35777;&#30340;&#35748;&#30693;&#38142;&#65292;&#21487;&#20197;&#24110;&#21161;AI&#31995;&#32479;&#22312;&#31038;&#20132;&#26234;&#33021;&#31561;&#20219;&#21153;&#19978;&#20855;&#22791;&#25512;&#29702;&#20154;&#31867;&#24515;&#26234;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#26234;&#29702;&#35770;&#65288;ToM&#65289;&#26159;&#25351;&#20154;&#31867;&#29702;&#35299;&#21644;&#25512;&#26029;&#20182;&#20154;&#27442;&#26395;&#12289;&#20449;&#24565;&#21644;&#24847;&#22270;&#30340;&#33021;&#21147;&#12290;&#33719;&#21462;ToM&#23545;&#20154;&#31867;&#30340;&#31038;&#20250;&#35748;&#30693;&#21644;&#20154;&#38469;&#20851;&#31995;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#23613;&#31649;ToM&#23545;&#20110;&#31038;&#20132;&#26234;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#29616;&#20195;AI&#21644;NLP&#31995;&#32479;&#20173;&#28982;&#32570;&#20047;&#35813;&#33021;&#21147;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#35775;&#38382;&#35757;&#32451;&#35821;&#26009;&#24211;&#20043;&#19979;&#30340;&#20154;&#31867;&#24515;&#26234;&#29366;&#24577;&#21644;&#35748;&#30693;&#36807;&#31243;&#12290;&#20026;&#20102;&#36171;&#20104;AI&#31995;&#32479;ToM&#33021;&#21147;&#65292;&#32553;&#23567;&#23427;&#20204;&#19982;&#20154;&#31867;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;COKE&#65306;&#31532;&#19968;&#20010;&#26426;&#22120;&#24515;&#26234;&#29702;&#35770;&#30340;&#35748;&#30693;&#30693;&#35782;&#22270;&#35889;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;COKE&#23558;ToM&#24418;&#24335;&#21270;&#20026;&#19968;&#32452;45k+&#32463;&#25163;&#21160;&#39564;&#35777;&#30340;&#35748;&#30693;&#38142;&#65292;&#36825;&#20123;&#38142;&#25551;&#32472;&#20102;&#20154;&#31867;&#22312;&#29305;&#23450;&#31038;&#20132;&#29615;&#22659;&#19979;&#30340;&#24515;&#29702;&#27963;&#21160;&#21644;&#38543;&#21518;&#30340;&#34892;&#20026;/&#24773;&#24863;&#21453;&#24212;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#19968;&#27493;&#25512;&#24191;&#20102;COKE&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#35748;&#30693;&#29983;&#25104;&#27169;&#22411;COKE+&#12290;&#22312;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#26041;&#38754;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;COKE&#23545;&#20110;&#21508;&#31181;&#19982;ToM&#30456;&#20851;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#31038;&#20132;&#24120;&#35782;&#25512;&#29702;&#12289;&#24773;&#24863;&#35782;&#21035;&#21644;&#21487;&#35299;&#37322;&#23545;&#35805;&#65292;&#20855;&#26377;&#26377;&#25928;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Theory of mind (ToM) refers to humans' ability to understand and infer the desires, beliefs, and intentions of others. The acquisition of ToM plays a key role in humans' social cognition and interpersonal relations. Though indispensable for social intelligence, ToM is still lacking for modern AI and NLP systems since they cannot access the human mental state and cognitive process beneath the training corpus. To empower AI systems with the ToM ability and narrow the gap between them and humans, in this paper, we propose COKE: the first cognitive knowledge graph for machine theory of mind. Specifically, COKE formalizes ToM as a collection of 45k+ manually verified cognitive chains that characterize human mental activities and subsequent behavioral/affective responses when facing specific social circumstances. Beyond that, we further generalize COKE using pre-trained language models and build a powerful cognitive generation model COKE+. Experimental results in both automatic and human eva
&lt;/p&gt;</description></item></channel></rss>