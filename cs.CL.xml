<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;FP8&#33258;&#21160;&#28151;&#21512;&#31934;&#24230;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#19981;&#24433;&#21709;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#20943;&#23569;&#20869;&#23384;&#20351;&#29992;&#24182;&#25552;&#39640;&#35757;&#32451;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.18313</link><description>&lt;p&gt;
FP8-LM&#65306;&#35757;&#32451;FP8&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FP8-LM: Training FP8 Large Language Models. (arXiv:2310.18313v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;FP8&#33258;&#21160;&#28151;&#21512;&#31934;&#24230;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#19981;&#24433;&#21709;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#20943;&#23569;&#20869;&#23384;&#20351;&#29992;&#24182;&#25552;&#39640;&#35757;&#32451;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#29992;&#20110;&#39640;&#25928;&#35757;&#32451;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;FP8&#20302;&#27604;&#29305;&#25968;&#25454;&#26684;&#24335;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#23519;&#26159;&#65292;&#22312;LLM&#35757;&#32451;&#20013;&#65292;&#22823;&#22810;&#25968;&#21464;&#37327;&#65288;&#22914;&#26799;&#24230;&#21644;&#20248;&#21270;&#22120;&#29366;&#24577;&#65289;&#21487;&#20197;&#20351;&#29992;&#20302;&#31934;&#24230;&#25968;&#25454;&#26684;&#24335;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#27169;&#22411;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#25913;&#21464;&#36229;&#21442;&#25968;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;FP8&#33258;&#21160;&#28151;&#21512;&#31934;&#24230;&#26694;&#26550;&#29992;&#20110;&#35757;&#32451;LLMs&#12290;&#35813;&#26694;&#26550;&#20026;LLM&#30340;&#28151;&#21512;&#31934;&#24230;&#21644;&#20998;&#24067;&#24335;&#24182;&#34892;&#35757;&#32451;&#25552;&#20379;&#20102;&#19977;&#20010;&#32423;&#21035;&#30340;FP8&#21033;&#29992;&#12290;&#23427;&#36880;&#27493;&#24341;&#20837;8&#20301;&#26799;&#24230;&#65292;&#20248;&#21270;&#22120;&#29366;&#24577;&#21644;&#20998;&#24067;&#24335;&#23398;&#20064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;H100 GPU&#24179;&#21488;&#19978;&#35757;&#32451;GPT-175B&#27169;&#22411;&#26399;&#38388;&#65292;&#25105;&#20204;&#30340;FP8&#28151;&#21512;&#31934;&#24230;&#35757;&#32451;&#26694;&#26550;&#19981;&#20165;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;42%&#30340;&#30495;&#23454;&#20869;&#23384;&#20351;&#29992;&#20943;&#23569;&#65292;&#32780;&#19988;&#27604;&#24191;&#27867;&#37319;&#29992;&#30340;BF16&#26694;&#26550;&#65288;&#21363;Megatron-LM&#65289;&#36816;&#34892;&#36895;&#24230;&#24555;64%&#65292;&#27604;Nvidia Transformer Engine&#24555;17%&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we explore FP8 low-bit data formats for efficient training of large language models (LLMs). Our key insight is that most variables, such as gradients and optimizer states, in LLM training can employ low-precision data formats without compromising model accuracy and requiring no changes to hyper-parameters. Specifically, we propose a new FP8 automatic mixed-precision framework for training LLMs. This framework offers three levels of FP8 utilization to streamline mixed-precision and distributed parallel training for LLMs. It gradually incorporates 8-bit gradients, optimizer states, and distributed learning in an incremental manner. Experiment results show that, during the training of GPT-175B model on H100 GPU platform, our FP8 mixed-precision training framework not only achieved a remarkable 42% reduction in real memory usage but also ran 64% faster than the widely adopted BF16 framework (i.e., Megatron-LM), surpassing the speed of Nvidia Transformer Engine by 17%. This l
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#21160;&#29983;&#25104;&#27010;&#24565;&#35868;&#39064;&#30340;&#26041;&#27861;&#65292;&#20197;&#20419;&#36827;&#22312;&#32447;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#23398;&#20064;&#32773;&#21442;&#19982;&#24230;&#12290;&#36890;&#36807;&#24212;&#29992;&#27010;&#24565;&#36798;&#25104;&#27169;&#22411;&#21644;&#29983;&#25104;&#35868;&#39064;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#23398;&#20064;&#32773;&#26356;&#22909;&#22320;&#29702;&#35299;&#27010;&#24565;&#12290;</title><link>http://arxiv.org/abs/2310.18290</link><description>&lt;p&gt;
&#19968;&#31181;&#33258;&#21160;&#29983;&#25104;&#35868;&#39064;&#20197;&#36741;&#21161;&#27010;&#24565;&#29702;&#35299;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Approach to Automatically generating Riddles aiding Concept Attainment. (arXiv:2310.18290v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18290
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#21160;&#29983;&#25104;&#27010;&#24565;&#35868;&#39064;&#30340;&#26041;&#27861;&#65292;&#20197;&#20419;&#36827;&#22312;&#32447;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#23398;&#20064;&#32773;&#21442;&#19982;&#24230;&#12290;&#36890;&#36807;&#24212;&#29992;&#27010;&#24565;&#36798;&#25104;&#27169;&#22411;&#21644;&#29983;&#25104;&#35868;&#39064;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#23398;&#20064;&#32773;&#26356;&#22909;&#22320;&#29702;&#35299;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#23398;&#20064;&#29615;&#22659;&#20013;&#65292;&#20445;&#25345;&#23398;&#20064;&#32773;&#30340;&#31215;&#26497;&#21442;&#19982;&#26159;&#19968;&#20010;&#20027;&#35201;&#30340;&#25361;&#25112;&#12290;&#20026;&#22686;&#24378;&#23398;&#20064;&#32773;&#30340;&#21442;&#19982;&#24230;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#19981;&#21516;&#30340;&#25945;&#23398;&#31574;&#30053;&#65292;&#26080;&#35770;&#26159;&#22312;&#32447;&#36824;&#26159;&#31163;&#32447;&#29615;&#22659;&#20013;&#12290;&#27010;&#24565;&#36798;&#25104;&#27169;&#22411;&#23601;&#26159;&#19968;&#31181;&#25945;&#23398;&#31574;&#30053;&#65292;&#23427;&#30528;&#37325;&#20110;&#23398;&#20064;&#32773;&#23545;&#27010;&#24565;&#30340;&#28145;&#20837;&#29702;&#35299;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#23545;&#27010;&#24565;&#30340;&#23383;&#20856;&#23450;&#20041;&#12290;&#36890;&#36807;&#25628;&#32034;&#21644;&#21015;&#20030;&#29992;&#20110;&#21306;&#20998;&#21508;&#31181;&#27010;&#24565;&#30340;&#23454;&#20363;&#21644;&#38750;&#23454;&#20363;&#20043;&#38388;&#30340;&#23646;&#24615;&#65292;&#26469;&#36798;&#21040;&#36825;&#19968;&#30446;&#30340;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#35797;&#22270;&#23558;&#27010;&#24565;&#36798;&#25104;&#27169;&#22411;&#24212;&#29992;&#20110;&#26500;&#24314;&#27010;&#24565;&#35868;&#39064;&#65292;&#20197;&#22312;&#22312;&#32447;&#23398;&#20064;&#29615;&#22659;&#20013;&#20351;&#29992;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#20174;&#23398;&#20064;&#36164;&#28304;&#20013;&#21019;&#24314;&#20107;&#23454;&#19977;&#20803;&#32452;&#65292;&#26681;&#25454;&#20854;&#23545;&#27010;&#24565;&#30340;&#21807;&#19968;&#24615;&#36827;&#34892;&#20998;&#31867;&#20026;&#8220;&#20027;&#39064;&#26631;&#35760;&#8221;&#21644;&#8220;&#20849;&#21516;&#8221;&#65292;&#28982;&#21518;&#26681;&#25454;&#27010;&#24565;&#36798;&#25104;&#27169;&#22411;&#30340;&#26684;&#24335;&#29983;&#25104;&#35868;&#39064;&#65292;&#24182;&#25429;&#33719;&#36825;&#20123;&#35868;&#39064;&#30340;&#25152;&#26377;&#21487;&#33021;&#35299;&#12290;&#20174;&#20154;&#31867;&#35780;&#20272;&#20013;&#33719;&#24471;&#30340;&#32467;&#26524;&#26174;&#31034;...
&lt;/p&gt;
&lt;p&gt;
One of the primary challenges in online learning environments, is to retain learner engagement. Several different instructional strategies are proposed both in online and offline environments to enhance learner engagement. The Concept Attainment Model is one such instructional strategy that focuses on learners acquiring a deeper understanding of a concept rather than just its dictionary definition. This is done by searching and listing the properties used to distinguish examples from non-examples of various concepts. Our work attempts to apply the Concept Attainment Model to build conceptual riddles, to deploy over online learning environments. The approach involves creating factual triples from learning resources, classifying them based on their uniqueness to a concept into `Topic Markers' and `Common', followed by generating riddles based on the Concept Attainment Model's format and capturing all possible solutions to those riddles. The results obtained from the human evaluation of r
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;VGG-16&#23545;&#39532;&#25289;&#38597;&#25289;&#22982;&#35821;&#30340;&#20551;&#26032;&#38395;&#36827;&#34892;&#22810;&#27169;&#24577;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#39318;&#27425;&#22312;&#35813;&#35821;&#35328;&#20013;&#36827;&#34892;&#22810;&#27169;&#24577;&#29305;&#24449;&#25552;&#21462;&#21644;&#20998;&#31867;&#30340;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2310.18263</link><description>&lt;p&gt;
MalFake: &#20351;&#29992;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;VGG-16&#30340;&#22810;&#27169;&#24577;&#39532;&#25289;&#38597;&#25289;&#22982;&#35821;&#20551;&#26032;&#38395;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
MalFake: A Multimodal Fake News Identification for Malayalam using Recurrent Neural Networks and VGG-16. (arXiv:2310.18263v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18263
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;VGG-16&#23545;&#39532;&#25289;&#38597;&#25289;&#22982;&#35821;&#30340;&#20551;&#26032;&#38395;&#36827;&#34892;&#22810;&#27169;&#24577;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#39318;&#27425;&#22312;&#35813;&#35821;&#35328;&#20013;&#36827;&#34892;&#22810;&#27169;&#24577;&#29305;&#24449;&#25552;&#21462;&#21644;&#20998;&#31867;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#26032;&#38395;&#30340;&#28040;&#36153;&#37327;&#36817;&#24180;&#26469;&#22823;&#24133;&#22686;&#21152;&#12290;&#30001;&#20110;&#26576;&#20123;&#32593;&#31449;&#30340;&#24555;&#36895;&#21457;&#24067;&#21644;&#32570;&#20047;&#32534;&#36753;&#26631;&#20934;&#65292;&#20551;&#26032;&#38395;&#22312;&#22320;&#26041;&#35821;&#35328;&#22914;&#39532;&#25289;&#38597;&#25289;&#22982;&#35821;&#20013;&#36234;&#26469;&#36234;&#26222;&#36941;&#12290;&#20551;&#26032;&#38395;&#21487;&#33021;&#23545;&#31038;&#20250;&#20135;&#29983;&#21487;&#24597;&#30340;&#24433;&#21709;&#65292;&#23548;&#33268;&#20154;&#20204;&#20570;&#20986;&#38169;&#35823;&#21028;&#26029;&#65292;&#23545;&#26435;&#23041;&#22833;&#21435;&#20449;&#20219;&#65292;&#29978;&#33267;&#21442;&#19982;&#26292;&#21147;&#34892;&#20026;&#12290;&#22312;&#21360;&#24230;&#30340;&#35821;&#22659;&#19979;&#65292;&#26377;&#24456;&#22810;&#22320;&#26041;&#35821;&#35328;&#65292;&#20551;&#26032;&#38395;&#22312;&#27599;&#31181;&#35821;&#35328;&#20013;&#37117;&#22312;&#20256;&#25773;&#12290;&#22240;&#27492;&#65292;&#25552;&#20379;&#26377;&#25928;&#30340;&#25216;&#26415;&#26469;&#35782;&#21035;&#22320;&#26041;&#35821;&#35328;&#20013;&#30340;&#20551;&#20449;&#24687;&#33267;&#20851;&#37325;&#35201;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#22312;&#39532;&#25289;&#38597;&#25289;&#22982;&#35821;&#20013;&#65292;&#20960;&#20046;&#27809;&#26377;&#20851;&#20110;&#25552;&#21462;&#22810;&#27169;&#24577;&#29305;&#24449;&#26469;&#20998;&#31867;&#20551;&#26032;&#38395;&#30340;&#24037;&#20316;&#12290;&#22810;&#27169;&#24577;&#26041;&#27861;&#22312;&#26816;&#27979;&#20551;&#26032;&#38395;&#26041;&#38754;&#26356;&#20934;&#30830;&#65292;&#22240;&#20026;&#20174;&#22810;&#31181;&#27169;&#24577;&#20013;&#25552;&#21462;&#29305;&#24449;&#26500;&#24314;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#27169;&#22411;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#27425;&#22312;&#39532;&#25289;&#38597;&#25289;&#22982;&#35821;&#20013;&#20351;&#29992;&#22810;&#27169;&#24577;&#29305;&#24449;&#26469;&#35782;&#21035;&#20551;&#26032;&#38395;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
The amount of news being consumed online has substantially expanded in recent years. Fake news has become increasingly common, especially in regional languages like Malayalam, due to the rapid publication and lack of editorial standards on some online sites. Fake news may have a terrible effect on society, causing people to make bad judgments, lose faith in authorities, and even engage in violent behavior. When we take into the context of India, there are many regional languages, and fake news is spreading in every language. Therefore, providing efficient techniques for identifying false information in regional tongues is crucial. Until now, little to no work has been done in Malayalam, extracting features from multiple modalities to classify fake news. Multimodal approaches are more accurate in detecting fake news, as features from multiple modalities are extracted to build the deep learning classification model. As far as we know, this is the first piece of work in Malayalam that use
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#24418;&#24335;&#26041;&#27861;&#21453;&#39304;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#33258;&#20027;&#31995;&#32479;&#20013;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#36890;&#36807;&#21512;&#25104;&#22522;&#20110;&#33258;&#21160;&#26426;&#30340;&#25511;&#21046;&#22120;&#24182;&#19982;&#32473;&#23450;&#35268;&#33539;&#36827;&#34892;&#39564;&#35777;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#20154;&#24037;&#21453;&#39304;&#30340;&#25104;&#26412;&#24182;&#23454;&#29616;&#39046;&#22495;&#29305;&#23450;&#20219;&#21153;&#30340;&#25511;&#21046;&#31574;&#30053;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2310.18239</link><description>&lt;p&gt;
&#20351;&#29992;&#24418;&#24335;&#26041;&#27861;&#21453;&#39304;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Fine-Tuning Language Models Using Formal Methods Feedback. (arXiv:2310.18239v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18239
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#24418;&#24335;&#26041;&#27861;&#21453;&#39304;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#33258;&#20027;&#31995;&#32479;&#20013;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#36890;&#36807;&#21512;&#25104;&#22522;&#20110;&#33258;&#21160;&#26426;&#30340;&#25511;&#21046;&#22120;&#24182;&#19982;&#32473;&#23450;&#35268;&#33539;&#36827;&#34892;&#39564;&#35777;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#20154;&#24037;&#21453;&#39304;&#30340;&#25104;&#26412;&#24182;&#23454;&#29616;&#39046;&#22495;&#29305;&#23450;&#20219;&#21153;&#30340;&#25511;&#21046;&#31574;&#30053;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#32534;&#30721;&#23545;&#35268;&#21010;&#21644;&#25511;&#21046;&#26377;&#30410;&#30340;&#36890;&#29992;&#30693;&#35782;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#26080;&#27861;&#20026;&#29305;&#23450;&#39046;&#22495;&#20219;&#21153;&#29983;&#25104;&#36866;&#24403;&#30340;&#25511;&#21046;&#31574;&#30053;&#12290;&#29616;&#26377;&#30340;&#24494;&#35843;&#26041;&#27861;&#20351;&#29992;&#20154;&#24037;&#21453;&#39304;&#26469;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#28982;&#32780;&#65292;&#33719;&#21462;&#20154;&#24037;&#21453;&#39304;&#26159;&#21171;&#21160;&#23494;&#38598;&#22411;&#21644;&#26114;&#36149;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#33258;&#21160;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#24212;&#29992;&#20110;&#33258;&#20027;&#31995;&#32479;&#65292;&#24357;&#21512;&#36890;&#29992;&#30693;&#35782;&#19982;&#29305;&#23450;&#39046;&#22495;&#35201;&#27714;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#21516;&#26102;&#38477;&#20302;&#25104;&#26412;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#26469;&#24341;&#23548;&#20174;&#39044;&#20808;&#35757;&#32451;&#27169;&#22411;&#20013;&#21512;&#25104;&#22522;&#20110;&#33258;&#21160;&#26426;&#30340;&#25511;&#21046;&#22120;&#12290;&#36825;&#20123;&#25511;&#21046;&#22120;&#22312;&#19968;&#20010;&#19990;&#30028;&#27169;&#22411;&#20013;&#19982;&#29420;&#31435;&#25552;&#20379;&#30340;&#35268;&#33539;&#36827;&#34892;&#39564;&#35777;&#65292;&#35813;&#19990;&#30028;&#27169;&#22411;&#21487;&#20197;&#26159;&#25277;&#35937;&#30340;&#25110;&#26469;&#33258;&#39640;&#20445;&#30495;&#24230;&#30340;&#27169;&#25311;&#22120;&#12290;&#19982;&#26399;&#26395;&#35268;&#33539;&#39640;&#24230;&#19968;&#33268;&#30340;&#25511;&#21046;&#22120;&#33719;&#24471;&#26356;&#39640;&#30340;&#25490;&#21517;&#65292;&#24341;&#23548;&#36845;&#20195;&#30340;&#24494;&#35843;&#36807;&#31243;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23450;&#37327;&#35777;&#25454;&#65292;&#20027;&#35201;&#26159;
&lt;/p&gt;
&lt;p&gt;
Although pre-trained language models encode generic knowledge beneficial for planning and control, they may fail to generate appropriate control policies for domain-specific tasks. Existing fine-tuning methods use human feedback to address this limitation, however, sourcing human feedback is labor intensive and costly. We present a fully automated approach to fine-tune pre-trained language models for applications in autonomous systems, bridging the gap between generic knowledge and domain-specific requirements while reducing cost. The method synthesizes automaton-based controllers from pre-trained models guided by natural language task descriptions. These controllers are verifiable against independently provided specifications within a world model, which can be abstract or obtained from a high-fidelity simulator. Controllers with high compliance with the desired specifications receive higher ranks, guiding the iterative fine-tuning process. We provide quantitative evidences, primarily 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Davidsonian&#22330;&#26223;&#22270;&#65288;DSG&#65289;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#25991;&#26412;-&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#20013;&#30340;&#21487;&#38752;&#24615;&#25361;&#25112;&#65292;&#21253;&#25324;QG&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#21644;VQA&#31572;&#26696;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.18235</link><description>&lt;p&gt;
Davidsonian&#22330;&#26223;&#22270;&#65306;&#25913;&#36827;&#25991;&#26412;-&#22270;&#20687;&#29983;&#25104;&#30340;&#32454;&#31890;&#24230;&#35780;&#20272;&#30340;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
Davidsonian Scene Graph: Improving Reliability in Fine-grained Evaluation for Text-Image Generation. (arXiv:2310.18235v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18235
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Davidsonian&#22330;&#26223;&#22270;&#65288;DSG&#65289;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#25991;&#26412;-&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#20013;&#30340;&#21487;&#38752;&#24615;&#25361;&#25112;&#65292;&#21253;&#25324;QG&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#21644;VQA&#31572;&#26696;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#19968;&#30452;&#26159;&#22256;&#38590;&#30340;&#12290;&#26368;&#36817;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#25991;&#26412;-&#22270;&#20687;&#24544;&#23454;&#24230;&#30340;&#24378;&#22823;&#26041;&#27861;&#26159;&#22522;&#20110;QG/A&#65288;&#38382;&#39064;&#29983;&#25104;&#21644;&#22238;&#31572;&#65289;&#65292;&#23427;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#19968;&#32452;&#38382;&#39064;&#21644;&#31572;&#26696;&#65292;&#24182;&#22522;&#20110;&#36825;&#20123;&#31572;&#26696;&#19982;&#22522;&#20110;&#25552;&#31034;&#30340;&#31572;&#26696;&#22312;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#19968;&#33268;&#24615;&#23545;&#36755;&#20986;&#22270;&#20687;&#36827;&#34892;&#35780;&#20998;&#12290;&#36825;&#31181;&#35780;&#20272;&#33258;&#28982;&#19978;&#21462;&#20915;&#20110;&#24213;&#23618;QG&#21644;QA&#27169;&#22411;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#30830;&#23450;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;QG/A&#24037;&#20316;&#20013;&#30340;&#20960;&#20010;&#21487;&#38752;&#24615;&#25361;&#25112;&#65306;&#65288;a&#65289;QG&#38382;&#39064;&#24212;&#23562;&#37325;&#25552;&#31034;&#65288;&#36991;&#20813;&#24187;&#35273;&#12289;&#37325;&#22797;&#21644;&#36951;&#28431;&#65289;&#21644;&#65288;b&#65289;VQA&#31572;&#26696;&#24212;&#19968;&#33268;&#65288;&#19981;&#20250;&#22312;&#22270;&#20687;&#20013;&#23459;&#31216;&#27809;&#26377;&#25705;&#25176;&#36710;&#65292;&#21516;&#26102;&#22768;&#31216;&#25705;&#25176;&#36710;&#26159;&#34013;&#33394;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;Davidsonian&#22330;&#26223;&#22270;&#65288;DSG&#65289;&#65292;&#36825;&#20010;&#21463;&#24418;&#24335;&#35821;&#20041;&#21551;&#21457;&#30340;&#23454;&#35777;&#35780;&#20272;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating text-to-image models is notoriously difficult. A strong recent approach for assessing text-image faithfulness is based on QG/A (question generation and answering), which uses pre-trained foundational models to automatically generate a set of questions and answers from the prompt, and output images are scored based on whether these answers extracted with a visual question answering model are consistent with the prompt-based answers. This kind of evaluation is naturally dependent on the quality of the underlying QG and QA models. We identify and address several reliability challenges in existing QG/A work: (a) QG questions should respect the prompt (avoiding hallucinations, duplications, and omissions) and (b) VQA answers should be consistent (not asserting that there is no motorcycle in an image while also claiming the motorcycle is blue). We address these issues with Davidsonian Scene Graph (DSG), an empirically grounded evaluation framework inspired by formal semantics. DSG
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20154;&#31867;&#38405;&#35835;&#30524;&#21160;&#36861;&#36394;&#25968;&#25454;&#20013;&#30340;&#22238;&#36864;&#21644;&#36339;&#36807;&#20316;&#20026;&#22686;&#37327;&#24207;&#21015;&#26631;&#35760;&#20013;&#20462;&#35746;&#31574;&#30053;&#30340;&#20449;&#24687;&#20449;&#21495;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#21457;&#29616;&#20854;&#21487;&#33021;&#20316;&#20026;&#20462;&#35746;&#30340;&#26377;&#29992;&#39044;&#27979;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2310.18229</link><description>&lt;p&gt;
&#22238;&#39038;&#24615;&#20462;&#35746;&#65306;&#38405;&#35835;&#20013;&#30340;&#22238;&#36864;&#21644;&#36339;&#36807;&#20316;&#20026;&#22686;&#37327;&#22788;&#29702;&#20013;&#20462;&#35746;&#31574;&#30053;&#30340;&#35748;&#30693;&#20449;&#21495;
&lt;/p&gt;
&lt;p&gt;
Revising with a Backward Glance: Regressions and Skips during Reading as Cognitive Signals for Revision Policies in Incremental Processing. (arXiv:2310.18229v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18229
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20154;&#31867;&#38405;&#35835;&#30524;&#21160;&#36861;&#36394;&#25968;&#25454;&#20013;&#30340;&#22238;&#36864;&#21644;&#36339;&#36807;&#20316;&#20026;&#22686;&#37327;&#24207;&#21015;&#26631;&#35760;&#20013;&#20462;&#35746;&#31574;&#30053;&#30340;&#20449;&#24687;&#20449;&#21495;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#21457;&#29616;&#20854;&#21487;&#33021;&#20316;&#20026;&#20462;&#35746;&#30340;&#26377;&#29992;&#39044;&#27979;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#22686;&#37327;&#22788;&#29702;&#22120;&#26681;&#25454;&#35821;&#35328;&#36755;&#20837;&#30340;&#21069;&#32512;&#20135;&#29983;&#36755;&#20986;&#12290;&#19968;&#20123;&#26631;&#35760;&#35302;&#21457;&#20462;&#35746;&#65292;&#23548;&#33268;&#23545;&#36755;&#20986;&#20551;&#35774;&#36827;&#34892;&#32534;&#36753;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#20110;&#20026;&#20309;&#27169;&#22411;&#36827;&#34892;&#20462;&#35746;&#30340;&#30693;&#35782;&#36824;&#24456;&#26377;&#38480;&#12290;&#26816;&#27979;&#20462;&#35746;&#24212;&#35813;&#21457;&#29983;&#30340;&#26102;&#38388;&#27493;&#39588;&#30340;&#31574;&#30053;&#21487;&#20197;&#25552;&#39640;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#20462;&#35746;&#31574;&#30053;&#30340;&#21512;&#36866;&#20449;&#21495;&#30340;&#33719;&#21462;&#26159;&#19968;&#20010;&#24320;&#25918;&#24615;&#38382;&#39064;&#65292;&#22240;&#20026;&#22312;&#25968;&#25454;&#38598;&#20013;&#33258;&#28982;&#19981;&#21487;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20154;&#31867;&#38405;&#35835;&#30340;&#30524;&#21160;&#36319;&#36394;&#25968;&#25454;&#20013;&#30340;&#22238;&#36864;&#21644;&#36339;&#36807;&#22312;&#22686;&#37327;&#24207;&#21015;&#26631;&#35760;&#20013;&#20316;&#20026;&#20462;&#35746;&#31574;&#30053;&#30340;&#20449;&#24687;&#20449;&#21495;&#30340;&#36866;&#29992;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#24191;&#20041;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#65292;&#25105;&#20204;&#21457;&#29616;&#20154;&#31867;&#30340;&#22238;&#36864;&#21644;&#36339;&#36807;&#30340;&#27010;&#29575;&#21487;&#33021;&#20316;&#20026;BiLSTMs&#21644;Transformer&#27169;&#22411;&#20013;&#20462;&#35746;&#30340;&#26377;&#29992;&#39044;&#27979;&#22240;&#32032;&#65292;&#32780;&#23545;&#20110;&#19981;&#21516;&#35821;&#35328;&#30340;&#32467;&#26524;&#20445;&#25345;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
In NLP, incremental processors produce output in instalments, based on incoming prefixes of the linguistic input. Some tokens trigger revisions, causing edits to the output hypothesis, but little is known about why models revise when they revise. A policy that detects the time steps where revisions should happen can improve efficiency. Still, retrieving a suitable signal to train a revision policy is an open problem, since it is not naturally available in datasets. In this work, we investigate the appropriateness of regressions and skips in human reading eye-tracking data as signals to inform revision policies in incremental sequence labelling. Using generalised mixed-effects models, we find that the probability of regressions and skips by humans can potentially serve as useful predictors for revisions in BiLSTMs and Transformer models, with consistent results for various languages.
&lt;/p&gt;</description></item><item><title>ArcheType&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24320;&#28304;&#21015;&#31867;&#22411;&#27880;&#37322;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#25913;&#36827;&#19978;&#19979;&#25991;&#37319;&#26679;&#21644;&#26631;&#31614;&#37325;&#26032;&#26144;&#23556;&#65292;&#23454;&#29616;&#20102;&#20840;&#38754;&#30340;&#38646;&#26679;&#26412;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.18208</link><description>&lt;p&gt;
ArcheType&#65306;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24320;&#28304;&#21015;&#31867;&#22411;&#27880;&#37322;&#30340;&#26032;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
ArcheType: A Novel Framework for Open-Source Column Type Annotation using Large Language Models. (arXiv:2310.18208v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18208
&lt;/p&gt;
&lt;p&gt;
ArcheType&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24320;&#28304;&#21015;&#31867;&#22411;&#27880;&#37322;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#25913;&#36827;&#19978;&#19979;&#25991;&#37319;&#26679;&#21644;&#26631;&#31614;&#37325;&#26032;&#26144;&#23556;&#65292;&#23454;&#29616;&#20102;&#20840;&#38754;&#30340;&#38646;&#26679;&#26412;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#35821;&#20041;&#21015;&#31867;&#22411;&#27880;&#37322;&#65288;CTA&#65289;&#26041;&#38754;&#23384;&#22312;&#37325;&#35201;&#32570;&#28857;&#65306;&#23427;&#20204;&#20381;&#36182;&#20110;&#22312;&#35757;&#32451;&#26102;&#22266;&#23450;&#30340;&#35821;&#20041;&#31867;&#22411;&#65307;&#38656;&#35201;&#22823;&#37327;&#30340;&#27599;&#20010;&#31867;&#22411;&#30340;&#35757;&#32451;&#26679;&#26412;&#24182;&#20135;&#29983;&#22823;&#37327;&#36816;&#34892;&#26102;&#25512;&#26029;&#25104;&#26412;&#65307;&#21363;&#20351;&#31867;&#22411;&#20445;&#25345;&#19981;&#21464;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#20063;&#21487;&#33021;&#22312;&#35780;&#20272;&#26032;&#25968;&#25454;&#38598;&#26102;&#19979;&#38477;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24191;&#27867;&#30340;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#24615;&#33021;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#23427;&#20204;&#22312;CTA&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;ArcheType&#65292;&#19968;&#31181;&#31616;&#21333;&#23454;&#29992;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#19978;&#19979;&#25991;&#37319;&#26679;&#12289;&#25552;&#31034;&#24207;&#21015;&#21270;&#12289;&#27169;&#22411;&#26597;&#35810;&#21644;&#26631;&#31614;&#37325;&#26032;&#26144;&#23556;&#65292;&#20174;&#32780;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23436;&#20840;&#20197;&#38646;&#26679;&#26412;&#26041;&#24335;&#35299;&#20915;&#21015;&#31867;&#22411;&#27880;&#37322;&#38382;&#39064;&#12290;&#25105;&#20204;&#20998;&#21035;&#23545;&#25105;&#20204;&#26041;&#27861;&#30340;&#27599;&#20010;&#32452;&#25104;&#37096;&#20998;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24182;&#30830;&#23450;&#20986;&#25913;&#36827;&#19978;&#19979;&#25991;&#37319;&#26679;&#21644;&#26631;&#31614;&#37325;&#26032;&#26144;&#23556;&#25552;&#20379;&#20102;&#26368;&#19968;&#33268;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing deep-learning approaches to semantic column type annotation (CTA) have important shortcomings: they rely on semantic types which are fixed at training time; require a large number of training samples per type and incur large run-time inference costs; and their performance can degrade when evaluated on novel datasets, even when types remain constant. Large language models have exhibited strong zero-shot classification performance on a wide range of tasks and in this paper we explore their use for CTA. We introduce ArcheType, a simple, practical method for context sampling, prompt serialization, model querying, and label remapping, which enables large language models to solve column type annotation problems in a fully zero-shot manner. We ablate each component of our method separately, and establish that improvements to context sampling and label remapping provide the most consistent gains. ArcheType establishes new state-of-the-art performance on both zero-shot and fine-tuned C
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;INA&#30340;&#32508;&#21512;&#35848;&#21028;&#23545;&#35805;&#20195;&#29702;&#65292;&#23427;&#33021;&#22815;&#22312;&#20215;&#26684;&#20197;&#21450;&#20854;&#20182;&#22240;&#32032;&#19978;&#36827;&#34892;&#35848;&#21028;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#26356;&#20855;&#28789;&#27963;&#24615;&#21644;&#20840;&#38754;&#24615;&#30340;&#35848;&#21028;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/2310.18207</link><description>&lt;p&gt;
INA&#65306;&#19968;&#31181;&#22522;&#20110;&#22870;&#21169;&#30340;&#23545;&#35805;&#31995;&#32479;&#26469;&#22686;&#24378;&#35848;&#21028;&#31574;&#30053;&#30340;&#32508;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
INA: An Integrative Approach for Enhancing Negotiation Strategies with Reward-Based Dialogue System. (arXiv:2310.18207v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;INA&#30340;&#32508;&#21512;&#35848;&#21028;&#23545;&#35805;&#20195;&#29702;&#65292;&#23427;&#33021;&#22815;&#22312;&#20215;&#26684;&#20197;&#21450;&#20854;&#20182;&#22240;&#32032;&#19978;&#36827;&#34892;&#35848;&#21028;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#26356;&#20855;&#28789;&#27963;&#24615;&#21644;&#20840;&#38754;&#24615;&#30340;&#35848;&#21028;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22312;&#32447;&#24066;&#22330;&#30340;&#26032;&#22411;&#35848;&#21028;&#23545;&#35805;&#20195;&#29702;&#12290;&#25105;&#20204;&#30340;&#20195;&#29702;&#20855;&#26377;&#25972;&#21512;&#24615;&#30340;&#29305;&#28857;&#65292;&#21363;&#23427;&#20855;&#26377;&#22312;&#20215;&#26684;&#35848;&#21028;&#20197;&#21450;&#20854;&#20182;&#22240;&#32032;&#65288;&#22914;&#20132;&#26131;&#25414;&#32465;&#21253;&#30340;&#28155;&#21152;&#25110;&#21024;&#38500;&#65289;&#19978;&#36827;&#34892;&#35848;&#21028;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#28789;&#27963;&#12289;&#20840;&#38754;&#30340;&#35848;&#21028;&#20307;&#39564;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;&#32508;&#21512;&#35848;&#21028;&#25968;&#25454;&#38598;&#65288;IND&#65289;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#20197;&#23454;&#29616;&#36825;&#31181;&#21151;&#33021;&#12290;&#20026;&#20102;&#21019;&#24314;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#21322;&#33258;&#21160;&#25968;&#25454;&#21019;&#24314;&#26041;&#27861;&#65292;&#32467;&#21512;&#23450;&#20041;&#35848;&#21028;&#24847;&#22270;&#12289;&#21160;&#20316;&#20197;&#21450;&#29992;&#25143;&#21644;&#20195;&#29702;&#20043;&#38388;&#30340;&#24847;&#22270;-&#21160;&#20316;&#27169;&#25311;&#65292;&#29983;&#25104;&#28508;&#22312;&#30340;&#23545;&#35805;&#27969;&#31243;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;GPT-J&#36827;&#34892;&#25552;&#31034;&#65292;&#20026;&#32473;&#23450;&#30340;&#24847;&#22270;&#29983;&#25104;&#23545;&#35805;&#65292;&#24182;&#36890;&#36807;&#20154;&#22312;&#29615;&#33410;&#30340;&#21518;&#32534;&#36753;&#21644;&#20462;&#27491;&#26469;&#30830;&#20445;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#37319;&#29992;&#19968;&#32452;&#26032;&#39062;&#30340;&#22870;&#21169;&#26426;&#21046;&#65292;&#19987;&#38376;&#38024;&#23545;&#35848;&#21028;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel negotiation dialogue agent designed for the online marketplace. Our agent is integrative in nature i.e, it possesses the capability to negotiate on price as well as other factors, such as the addition or removal of items from a deal bundle, thereby offering a more flexible and comprehensive negotiation experience. We create a new dataset called Integrative Negotiation Dataset (IND) to enable this functionality. For this dataset creation, we introduce a new semi-automated data creation method, which combines defining negotiation intents, actions, and intent-action simulation between users and the agent to generate potential dialogue flows. Finally, the prompting of GPT-J, a state-of-the-art language model, is done to generate dialogues for a given intent, with a human-in-the-loop process for post-editing and refining minor errors to ensure high data quality. We employ a set of novel rewards, specifically tailored for the negotiation task to train our Ne
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#22810;&#35821;&#35328;&#31038;&#20132;&#23186;&#20307;&#20013;&#35782;&#21035;&#35328;&#35770;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#36896;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;X-CLAIM&#65292;&#37319;&#29992;&#32534;&#30721;&#22120;-&#21482;&#35821;&#35328;&#27169;&#22411;&#21644;&#29983;&#25104;&#24615;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#22810;&#31181;&#35821;&#35328;&#19978;&#35757;&#32451;&#30340;&#22909;&#22788;&#12290;</title><link>http://arxiv.org/abs/2310.18205</link><description>&lt;p&gt;
&#22312;&#36328;&#35821;&#35328;&#31038;&#20132;&#23186;&#20307;&#20013;&#65292;&#36855;&#22833;&#32780;&#37325;&#29616;&#65306;&#35782;&#21035;&#35328;&#35770;&#30340;&#36328;&#35821;&#35328;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Lost in Translation, Found in Spans: Identifying Claims in Multilingual Social Media. (arXiv:2310.18205v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18205
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#22810;&#35821;&#35328;&#31038;&#20132;&#23186;&#20307;&#20013;&#35782;&#21035;&#35328;&#35770;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#36896;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;X-CLAIM&#65292;&#37319;&#29992;&#32534;&#30721;&#22120;-&#21482;&#35821;&#35328;&#27169;&#22411;&#21644;&#29983;&#25104;&#24615;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#22810;&#31181;&#35821;&#35328;&#19978;&#35757;&#32451;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35328;&#35770;&#36328;&#24230;&#35782;&#21035;&#65288;CSI&#65289;&#26159;&#20107;&#23454;&#26680;&#26597;&#27969;&#31243;&#20013;&#30340;&#37325;&#35201;&#27493;&#39588;&#65292;&#26088;&#22312;&#35782;&#21035;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#20013;&#21253;&#21547;&#26377;&#24453;&#26816;&#26597;&#30340;&#35328;&#35770;&#25110;&#26029;&#35328;&#30340;&#25991;&#26412;&#29255;&#27573;&#12290;&#23613;&#31649;&#23545;&#26032;&#38395;&#35760;&#32773;&#21644;&#20154;&#24037;&#20107;&#23454;&#26680;&#26597;&#32773;&#26469;&#35828;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#23427;&#20173;&#28982;&#26159;&#19968;&#20010;&#20005;&#37325;&#34987;&#30740;&#31350;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#20851;&#20110;&#36825;&#20010;&#20027;&#39064;&#30340;&#30740;&#31350;&#38750;&#24120;&#26377;&#38480;&#65292;&#21482;&#19987;&#27880;&#20110;&#33521;&#35821;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#26469;&#33258;&#21360;&#24230;&#20116;&#31181;&#35821;&#35328;&#21644;&#33521;&#35821;&#30340;&#22810;&#20010;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#30340;&#30495;&#23454;&#20449;&#24687;&#65292;&#21019;&#36896;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;X-CLAIM&#65292;&#20854;&#20013;&#21253;&#21547;7K&#20010;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#35328;&#35770;&#12290;&#25105;&#20204;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#32534;&#30721;&#22120;-&#21482;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;XLM-R&#65289;&#25253;&#21578;&#24378;&#22823;&#30340;&#22522;&#20934;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#22810;&#31181;&#35821;&#35328;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#22909;&#22788;&#65292;&#30456;&#27604;&#20110;&#38646;&#32763;&#35793;&#25110;&#20174;&#39640;&#36164;&#28304;&#35821;&#35328;&#65288;&#22914;&#33521;&#35821;&#65289;&#35757;&#32451;&#30340;&#26367;&#20195;&#36328;&#35821;&#35328;&#36716;&#25442;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#22312;X-CLAIM&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;GPT&#31995;&#21015;&#30340;&#29983;&#25104;&#24615;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#25552;&#31034;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;&#23427;&#20204;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Claim span identification (CSI) is an important step in fact-checking pipelines, aiming to identify text segments that contain a checkworthy claim or assertion in a social media post. Despite its importance to journalists and human fact-checkers, it remains a severely understudied problem, and the scarce research on this topic so far has only focused on English. Here we aim to bridge this gap by creating a novel dataset, X-CLAIM, consisting of 7K real-world claims collected from numerous social media platforms in five Indian languages and English. We report strong baselines with state-of-the-art encoder-only language models (e.g., XLM-R) and we demonstrate the benefits of training on multiple languages over alternative cross-lingual transfer methods such as zero-shot transfer, or training on translated data, from a high-resource language such as English. We evaluate generative large language models from the GPT series using prompting methods on the X-CLAIM dataset and we find that they
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Diffusion GAN&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26465;&#20214;&#38901;&#24459;&#23618;&#26631;&#20934;&#21270;&#23558;&#39118;&#26684;&#23884;&#20837;&#21040;&#29983;&#25104;&#26550;&#26500;&#20013;&#65292;&#20174;&#32780;&#23454;&#29616;&#26681;&#25454;&#39118;&#26684;&#25551;&#36848;&#21644;&#20869;&#23481;&#25991;&#26412;&#29983;&#25104;&#39640;&#36136;&#37327;&#35821;&#38899;&#12290;</title><link>http://arxiv.org/abs/2310.18169</link><description>&lt;p&gt;
&#22522;&#20110;&#26465;&#20214;&#38901;&#24459;&#23618;&#26631;&#20934;&#21270;&#25193;&#25955;GAN&#30340;&#39118;&#26684;&#25551;&#36848;&#25991;&#26412;&#36716;&#35821;&#38899;
&lt;/p&gt;
&lt;p&gt;
Style Description based Text-to-Speech with Conditional Prosodic Layer Normalization based Diffusion GAN. (arXiv:2310.18169v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Diffusion GAN&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26465;&#20214;&#38901;&#24459;&#23618;&#26631;&#20934;&#21270;&#23558;&#39118;&#26684;&#23884;&#20837;&#21040;&#29983;&#25104;&#26550;&#26500;&#20013;&#65292;&#20174;&#32780;&#23454;&#29616;&#26681;&#25454;&#39118;&#26684;&#25551;&#36848;&#21644;&#20869;&#23481;&#25991;&#26412;&#29983;&#25104;&#39640;&#36136;&#37327;&#35821;&#38899;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;GAN&#30340;&#26041;&#27861;&#65288;Prosodic Diff-TTS&#65289;&#65292;&#26681;&#25454;&#39118;&#26684;&#25551;&#36848;&#21644;&#20869;&#23481;&#25991;&#26412;&#29983;&#25104;&#30456;&#24212;&#30340;&#39640;&#20445;&#30495;&#35821;&#38899;&#12290;&#23427;&#21033;&#29992;&#20102;&#26032;&#39062;&#30340;&#26465;&#20214;&#38901;&#24459;&#23618;&#26631;&#20934;&#21270;&#65292;&#23558;&#39118;&#26684;&#23884;&#20837;&#21040;&#22522;&#20110;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#38899;&#32032;&#32534;&#30721;&#22120;&#21644;&#22522;&#20110;&#26757;&#23572;&#39057;&#35889;&#22270;&#30340;&#35299;&#30721;&#22120;&#30340;&#29983;&#25104;&#22120;&#26550;&#26500;&#20013;&#65292;&#20174;&#32780;&#29983;&#25104;&#35821;&#38899;&#12290;&#39118;&#26684;&#23884;&#20837;&#26159;&#36890;&#36807;&#22312;&#36741;&#21161;&#20219;&#21153;&#65288;&#22914;&#38899;&#39640;&#12289;&#35821;&#36895;&#12289;&#24773;&#24863;&#12289;&#24615;&#21035;&#20998;&#31867;&#65289;&#19978;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#26469;&#29983;&#25104;&#30340;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#20010;&#23450;&#37327;&#25351;&#26631;&#26469;&#34913;&#37327;&#29983;&#25104;&#20934;&#30830;&#24615;&#21644;MOS&#65292;&#24182;&#22312;&#22810;&#35828;&#35805;&#20154;&#30340;LibriTTS&#21644;PromptSpeech&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26550;&#26500;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a Diffusion GAN based approach (Prosodic Diff-TTS) to generate the corresponding high-fidelity speech based on the style description and content text as an input to generate speech samples within only 4 denoising steps. It leverages the novel conditional prosodic layer normalization to incorporate the style embeddings into the multi head attention based phoneme encoder and mel spectrogram decoder based generator architecture to generate the speech. The style embedding is generated by fine tuning the pretrained BERT model on auxiliary tasks such as pitch, speaking speed, emotion,gender classifications. We demonstrate the efficacy of our proposed architecture on multi-speaker LibriTTS and PromptSpeech datasets, using multiple quantitative metrics that measure generated accuracy and MOS.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#20154;&#35774;&#26469;&#24314;&#27169;&#30495;&#23454;&#24615;&#30340;&#21487;&#33021;&#24615;&#12290;&#36890;&#36807;&#24314;&#27169;&#30495;&#23454;&#20154;&#35774;&#65292;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23558;&#30495;&#23454;&#24615;&#25512;&#24191;&#21040;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#65292;&#24182;&#36890;&#36807;&#30456;&#20851;&#29305;&#24449;&#21028;&#26029;&#20010;&#20307;&#20135;&#29983;&#25991;&#26412;&#30340;&#30495;&#23454;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.18168</link><description>&lt;p&gt;
&#20351;&#29992;&#20154;&#35774;&#26469;&#24314;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30495;&#23454;&#24615;
&lt;/p&gt;
&lt;p&gt;
Personas as a Way to Model Truthfulness in Language Models. (arXiv:2310.18168v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#20154;&#35774;&#26469;&#24314;&#27169;&#30495;&#23454;&#24615;&#30340;&#21487;&#33021;&#24615;&#12290;&#36890;&#36807;&#24314;&#27169;&#30495;&#23454;&#20154;&#35774;&#65292;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23558;&#30495;&#23454;&#24615;&#25512;&#24191;&#21040;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#65292;&#24182;&#36890;&#36807;&#30456;&#20851;&#29305;&#24449;&#21028;&#26029;&#20010;&#20307;&#20135;&#29983;&#25991;&#26412;&#30340;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;&#20114;&#32852;&#32593;&#19978;&#30340;&#22823;&#37327;&#25991;&#26412;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#20123;&#25991;&#26412;&#20013;&#26082;&#21253;&#21547;&#20102;&#20107;&#23454;&#65292;&#20063;&#21253;&#21547;&#20102;&#35823;&#23548;&#24615;&#30340;&#20449;&#24687;&#12290;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20174;&#36825;&#20123;&#30456;&#20114;&#30683;&#30462;&#30340;&#25968;&#25454;&#20013;&#36776;&#21035;&#30495;&#23454;&#19982;&#34394;&#20551;&#21527;&#65311;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#24314;&#27169;&#19981;&#21516;&#20135;&#29983;&#25991;&#26412;&#30340;&#20010;&#20307;&#36825;&#19968;&#35266;&#28857;&#65292;&#25105;&#20204;&#20551;&#35774;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#24314;&#27169;&#30495;&#23454;&#20154;&#35774;&#26469;&#32858;&#31867;&#30495;&#23454;&#25991;&#26412;&#65306;&#19968;&#32676;&#24456;&#21487;&#33021;&#20135;&#29983;&#30495;&#23454;&#25991;&#26412;&#24182;&#20855;&#26377;&#30456;&#20284;&#29305;&#24449;&#30340;&#20010;&#20307;&#12290;&#20363;&#22914;&#65292;&#21487;&#20449;&#28304;&#22914;&#32500;&#22522;&#30334;&#31185;&#21644;&#31185;&#23398;&#26399;&#21002;&#36890;&#24120;&#20351;&#29992;&#27491;&#24335;&#30340;&#20889;&#20316;&#39118;&#26684;&#24182;&#25552;&#20986;&#19968;&#33268;&#30340;&#20027;&#24352;&#12290;&#36890;&#36807;&#24314;&#27169;&#36825;&#19968;&#20154;&#35774;&#65292;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23558;&#30495;&#23454;&#24615;&#25512;&#24191;&#21040;&#27599;&#20010;&#20010;&#20307;&#29983;&#25104;&#35757;&#32451;&#25991;&#26412;&#30340;&#29305;&#23450;&#19978;&#19979;&#25991;&#20043;&#22806;&#12290;&#20363;&#22914;&#65292;&#27169;&#22411;&#21487;&#20197;&#25512;&#26029;&#20986;&#8220;&#32500;&#22522;&#30334;&#31185;&#8221;&#36825;&#20010;&#20010;&#20307;&#22312;&#8220;&#31185;&#23398;&#8221;&#29983;&#25104;&#30340;&#20027;&#39064;&#19978;&#20250;&#34920;&#29616;&#20986;&#30495;&#23454;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#20849;&#20139;&#19968;&#20010;&#20154;&#35774;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#20004;&#20010;&#35266;&#23519;&#32467;&#26524;&#20026;&#20154;&#35774;&#20551;&#35774;&#25552;&#20379;&#20102;&#35777;&#25454;&#65306;&#65288;1&#65289;&#25105;&#20204;&#21487;&#20197;&#25506;&#27979;&#27169;&#22411;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#21028;&#26029;&#30495;&#23454;&#24615;&#30340;&#33021;&#21147;&#65307;&#65288;2&#65289;&#27169;&#22411;&#21487;&#20197;&#20174;&#30456;&#20851;&#29305;&#24449;&#20013;&#25512;&#27979;&#20010;&#20307;&#20135;&#29983;&#25991;&#26412;&#30340;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are trained on vast amounts of text from the internet, which contains both factual and misleading information about the world. Can language models discern truth from falsehood in this contradicting data? Expanding on the view that LLMs can model different agents producing the corpora, we hypothesize that they can cluster truthful text by modeling a truthful persona: a group of agents that are likely to produce truthful text and share similar features. For example, trustworthy sources like Wikipedia and Science usually use formal writing styles and make consistent claims. By modeling this persona, LLMs can generalize truthfulness beyond the specific contexts in which each agent generated the training text. For example, the model can infer that the agent "Wikipedia" will behave truthfully on topics that were only generated by "Science" because they share a persona. We first show evidence for the persona hypothesis via two observations: (1) we can probe whether a mod
&lt;/p&gt;</description></item><item><title>MPrompt&#26159;&#19968;&#31181;&#29992;&#20110;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#30340;&#22810;&#32423;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20219;&#21153;&#29305;&#23450;&#12289;&#39046;&#22495;&#29305;&#23450;&#21644;&#19978;&#19979;&#25991;&#29305;&#23450;&#32423;&#21035;&#19978;&#21033;&#29992;&#25552;&#31034;&#26469;&#22686;&#24378;&#19981;&#21516;&#32454;&#31890;&#24230;&#30340;&#36755;&#20837;&#35821;&#20041;&#29702;&#35299;&#65292;&#24182;&#36890;&#36807;&#29420;&#31435;&#24615;&#32422;&#26463;&#36991;&#20813;&#20887;&#20313;&#12290;</title><link>http://arxiv.org/abs/2310.18167</link><description>&lt;p&gt;
MPrompt: &#25506;&#32034;&#29992;&#20110;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#30340;&#22810;&#32423;&#25552;&#31034;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
MPrompt: Exploring Multi-level Prompt Tuning for Machine Reading Comprehension. (arXiv:2310.18167v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18167
&lt;/p&gt;
&lt;p&gt;
MPrompt&#26159;&#19968;&#31181;&#29992;&#20110;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#30340;&#22810;&#32423;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20219;&#21153;&#29305;&#23450;&#12289;&#39046;&#22495;&#29305;&#23450;&#21644;&#19978;&#19979;&#25991;&#29305;&#23450;&#32423;&#21035;&#19978;&#21033;&#29992;&#25552;&#31034;&#26469;&#22686;&#24378;&#19981;&#21516;&#32454;&#31890;&#24230;&#30340;&#36755;&#20837;&#35821;&#20041;&#29702;&#35299;&#65292;&#24182;&#36890;&#36807;&#29420;&#31435;&#24615;&#32422;&#26463;&#36991;&#20813;&#20887;&#20313;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#32570;&#28857;&#26159;&#22312;&#23545;&#26032;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#26102;&#38656;&#35201;&#22823;&#37327;&#36164;&#28304;&#12290;&#36719;&#25552;&#31034;&#35843;&#25972;&#25552;&#20379;&#20102;&#19968;&#31181;&#36164;&#28304;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#22266;&#23450;&#26435;&#37325;&#30340;&#21516;&#26102;&#36827;&#34892;&#24494;&#35843;&#12290;&#29616;&#26377;&#30340;&#36719;&#25552;&#31034;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#35774;&#35745;&#19982;&#26032;&#25968;&#25454;&#38598;&#39046;&#22495;&#21305;&#37197;&#30340;&#19982;&#36755;&#20837;&#26080;&#20851;&#30340;&#25552;&#31034;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#24573;&#30053;&#20102;&#20219;&#21153;&#21644;&#25991;&#26412;&#19978;&#19979;&#25991;&#30340;&#32454;&#31890;&#24230;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#32423;&#25552;&#31034;&#35843;&#25972;&#65288;MPrompt&#65289;&#26041;&#27861;&#29992;&#20110;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#12290;&#23427;&#22312;&#20219;&#21153;&#29305;&#23450;&#12289;&#39046;&#22495;&#29305;&#23450;&#21644;&#19978;&#19979;&#25991;&#29305;&#23450;&#32423;&#21035;&#19978;&#21033;&#29992;&#25552;&#31034;&#26469;&#22686;&#24378;&#19981;&#21516;&#32454;&#31890;&#24230;&#30340;&#36755;&#20837;&#35821;&#20041;&#29702;&#35299;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#29420;&#31435;&#24615;&#32422;&#26463;&#65292;&#20351;&#27599;&#20010;&#39046;&#22495;&#29305;&#23450;&#30340;&#25552;&#31034;&#38598;&#20013;&#22312;&#20854;&#39046;&#22495;&#20869;&#30340;&#20449;&#24687;&#65292;&#36991;&#20813;&#20887;&#20313;&#12290;
&lt;/p&gt;
&lt;p&gt;
The large language models have achieved superior performance on various natural language tasks. One major drawback of such approaches is they are resource-intensive in fine-tuning new datasets. Soft-prompt tuning presents a resource-efficient solution to fine-tune the pre-trained language models (PLMs) while keeping their weight frozen. Existing soft prompt methods mainly focus on designing the input-independent prompts that steer the model to fit the domain of the new dataset. Those methods often ignore the fine-grained information about the task and context of the text. In this paper, we propose a multi-level prompt tuning (MPrompt) method for machine reading comprehension. It utilizes prompts at task-specific, domain-specific, and context-specific levels to enhance the comprehension of input semantics at different granularities. We also propose an independence constraint to steer each domain-specific prompt to focus on information within its domain to avoid redundancy. Moreover, we 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#21333;&#35789;&#30340;&#21548;&#35273;&#20449;&#24687;&#26469;&#22788;&#29702;&#20195;&#30721;&#28151;&#21512;&#25991;&#26412;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#20351;&#29992;SOUNDEX&#34920;&#31034;&#65292;&#20197;&#21450;&#37319;&#29992;&#26032;&#30340;&#36755;&#20837;&#25968;&#25454;&#25552;&#20379;&#26041;&#27861;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#22788;&#29702;&#20195;&#30721;&#28151;&#21512;&#25991;&#26412;&#20013;&#30340;&#25340;&#20889;&#21464;&#20307;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.18155</link><description>&lt;p&gt;
&#36890;&#36807;&#21333;&#35789;&#30340;&#21548;&#35273;&#20449;&#24687;&#25552;&#21319;&#20195;&#30721;&#28151;&#21512;&#25991;&#26412;&#22788;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Elevating Code-mixed Text Handling through Auditory Information of Words. (arXiv:2310.18155v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18155
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#21333;&#35789;&#30340;&#21548;&#35273;&#20449;&#24687;&#26469;&#22788;&#29702;&#20195;&#30721;&#28151;&#21512;&#25991;&#26412;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#20351;&#29992;SOUNDEX&#34920;&#31034;&#65292;&#20197;&#21450;&#37319;&#29992;&#26032;&#30340;&#36755;&#20837;&#25968;&#25454;&#25552;&#20379;&#26041;&#27861;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#22788;&#29702;&#20195;&#30721;&#28151;&#21512;&#25991;&#26412;&#20013;&#30340;&#25340;&#20889;&#21464;&#20307;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20195;&#30721;&#28151;&#21512;&#25968;&#25454;&#30340;&#26085;&#30410;&#27969;&#34892;&#65292;&#23545;&#36825;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#26377;&#26356;&#22909;&#30340;&#22788;&#29702;&#26041;&#24335;&#30340;&#38656;&#27714;&#20063;&#36234;&#26469;&#36234;&#22823;&#65292;&#36825;&#31181;&#25968;&#25454;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#65292;&#22914;&#22788;&#29702;&#25340;&#20889;&#21464;&#20307;&#12289;&#19981;&#21516;&#35821;&#35328;&#12289;&#19981;&#21516;&#33050;&#26412;&#20197;&#21450;&#36164;&#28304;&#21294;&#20047;&#12290;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#20195;&#30721;&#28151;&#21512;&#25968;&#25454;&#26102;&#38754;&#20020;&#22256;&#38590;&#65292;&#22240;&#20026;&#23427;&#20204;&#20027;&#35201;&#20851;&#27880;&#21333;&#35789;&#30340;&#35821;&#20041;&#34920;&#31034;&#65292;&#32780;&#24573;&#35270;&#20102;&#21548;&#35273;&#38899;&#26631;&#29305;&#24449;&#12290;&#36825;&#23548;&#33268;&#22788;&#29702;&#20195;&#30721;&#28151;&#21512;&#25991;&#26412;&#20013;&#30340;&#25340;&#20889;&#21464;&#20307;&#26102;&#20986;&#29616;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20174;SOUNDEX&#20013;&#25552;&#21462;&#30340;&#21333;&#35789;&#30340;&#21548;&#35273;&#20449;&#24687;&#26469;&#21019;&#24314;&#29992;&#20110;&#22788;&#29702;&#20195;&#30721;&#28151;&#21512;&#25991;&#26412;&#25968;&#25454;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#22522;&#20110;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#30340;&#39044;&#35757;&#32451;&#27493;&#39588;&#65292;&#20854;&#20013;&#21253;&#25324;SOUNDEX&#34920;&#31034;&#65288;SAMLM&#65289;&#21644;&#19968;&#31181;&#21521;&#39044;&#35757;&#32451;&#27169;&#22411;&#25552;&#20379;&#36755;&#20837;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;&#20195;&#30721;&#28151;&#21512;&#25968;&#25454;&#38598;&#65288;&#19981;&#21516;&#35821;&#35328;&#65289;&#36827;&#34892;&#24773;&#32490;&#12289;&#20882;&#29359;&#21644;&#25915;&#20987;&#23454;&#39564;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the growing popularity of code-mixed data, there is an increasing need for better handling of this type of data, which poses a number of challenges, such as dealing with spelling variations, multiple languages, different scripts, and a lack of resources. Current language models face difficulty in effectively handling code-mixed data as they primarily focus on the semantic representation of words and ignore the auditory phonetic features. This leads to difficulties in handling spelling variations in code-mixed text. In this paper, we propose an effective approach for creating language models for handling code-mixed textual data using auditory information of words from SOUNDEX. Our approach includes a pre-training step based on masked-language-modelling, which includes SOUNDEX representations (SAMLM) and a new method of providing input data to the pre-trained model. Through experimentation on various code-mixed datasets (of different languages) for sentiment, offensive and aggressio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Disentangled Graph-Text Learner (DGTL)&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#23450;&#21046;&#30340;&#35299;&#32544;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23618;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#25991;&#26412;&#23646;&#24615;&#22270;&#65288;TAGs&#65289;&#20013;&#30340;&#22797;&#26434;&#32467;&#26500;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.18152</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#23646;&#24615;&#22270;&#30340;&#35299;&#32544;&#34920;&#24449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Disentangled Representation Learning with Large Language Models for Text-Attributed Graphs. (arXiv:2310.18152v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Disentangled Graph-Text Learner (DGTL)&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#23450;&#21046;&#30340;&#35299;&#32544;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23618;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#25991;&#26412;&#23646;&#24615;&#22270;&#65288;TAGs&#65289;&#20013;&#30340;&#22797;&#26434;&#32467;&#26500;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#23646;&#24615;&#22270;&#65288;TAGs&#65289;&#22312;&#32593;&#32476;&#19978;&#38750;&#24120;&#24120;&#35265;&#65292;&#23545;&#20110;&#35813;&#31867;&#22270;&#65292;&#22914;&#24341;&#29992;&#32593;&#32476;&#12289;&#30005;&#23376;&#21830;&#21153;&#32593;&#32476;&#21644;&#31038;&#20132;&#32593;&#32476;&#30340;&#30740;&#31350;&#22312;&#32593;&#32476;&#31038;&#21306;&#20013;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#20165;&#20165;&#20381;&#38752;&#25552;&#31034;&#20449;&#24687;&#26469;&#20256;&#36798;&#22270;&#32467;&#26500;&#20449;&#24687;&#32473;LLMs&#65292;&#22240;&#27492;&#23545;&#20110;TAGs&#20013;&#22797;&#26434;&#30340;&#32467;&#26500;&#20851;&#31995;&#20102;&#35299;&#19981;&#36275;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#32544;&#22270;&#25991;&#23398;&#20064;&#22120;&#65288;DGTL&#65289;&#27169;&#22411;&#65292;&#33021;&#22815;&#22686;&#24378;LLMs&#23545;TAGs&#30340;&#25512;&#29702;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;DGTL&#27169;&#22411;&#36890;&#36807;&#23450;&#21046;&#30340;&#35299;&#32544;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23618;&#23558;&#22270;&#32467;&#26500;&#20449;&#24687;&#32435;&#20837;&#20854;&#20013;&#65292;&#20351;&#24471;LLMs&#33021;&#22815;&#25429;&#25417;&#22810;&#20010;&#32467;&#26500;&#22240;&#32032;&#20013;&#38544;&#34255;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-attributed graphs (TAGs) are prevalent on the web and research over TAGs such as citation networks, e-commerce networks and social networks has attracted considerable attention in the web community. Recently, large language models (LLMs) have demonstrated exceptional capabilities across a wide range of tasks. However, the existing works focus on harnessing the potential of LLMs solely relying on prompts to convey graph structure information to LLMs, thus suffering from insufficient understanding of the complex structural relationships within TAGs. To address this problem, in this paper we present the Disentangled Graph-Text Learner (DGTL) model, which is able to enhance the reasoning and predicting capabilities of LLMs for TAGs. Our proposed DGTL model incorporates graph structure information through tailored disentangled graph neural network (GNN) layers, enabling LLMs to capture the intricate relationships hidden in text-attributed graphs from multiple structural factors. Furthe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;LLMs&#22312;&#22788;&#29702;&#20105;&#35758;&#38382;&#39064;&#20013;&#34920;&#29616;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#20854;&#20013;&#30340;&#23376;&#38598;&#23545;&#19981;&#21516;&#30340;LLMs&#36827;&#34892;&#35780;&#20272;&#65292;&#38416;&#26126;&#23427;&#20204;&#22914;&#20309;&#22788;&#29702;&#20105;&#35758;&#38382;&#39064;&#20197;&#21450;&#37319;&#21462;&#30340;&#31435;&#22330;&#12290;</title><link>http://arxiv.org/abs/2310.18130</link><description>&lt;p&gt;
DELPHI: &#35780;&#20272;LLMs&#22312;&#22788;&#29702;&#20105;&#35758;&#38382;&#39064;&#20013;&#30340;&#34920;&#29616;&#30340;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
DELPHI: Data for Evaluating LLMs' Performance in Handling Controversial Issues. (arXiv:2310.18130v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18130
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;LLMs&#22312;&#22788;&#29702;&#20105;&#35758;&#38382;&#39064;&#20013;&#34920;&#29616;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#20854;&#20013;&#30340;&#23376;&#38598;&#23545;&#19981;&#21516;&#30340;LLMs&#36827;&#34892;&#35780;&#20272;&#65292;&#38416;&#26126;&#23427;&#20204;&#22914;&#20309;&#22788;&#29702;&#20105;&#35758;&#38382;&#39064;&#20197;&#21450;&#37319;&#21462;&#30340;&#31435;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20105;&#35758;&#26159;&#25105;&#20204;&#26102;&#20195;&#30340;&#19968;&#31181;&#21453;&#26144;&#65292;&#24182;&#19988;&#26159;&#20219;&#20309;&#35328;&#35770;&#30340;&#37325;&#35201;&#26041;&#38754;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#23545;&#35805;&#31995;&#32479;&#30340;&#20852;&#36215;&#65292;&#22686;&#21152;&#20102;&#20844;&#20247;&#23545;&#36825;&#20123;&#31995;&#32479;&#22238;&#31572;&#21508;&#31181;&#38382;&#39064;&#30340;&#20381;&#36182;&#12290;&#22240;&#27492;&#65292;&#31995;&#32479;&#22320;&#32771;&#23519;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#22238;&#31572;&#28041;&#21450;&#27491;&#22312;&#36827;&#34892;&#30340;&#36777;&#35770;&#30340;&#38382;&#39064;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#24456;&#23569;&#26377;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#25552;&#20379;&#20154;&#31867;&#27880;&#37322;&#30340;&#26631;&#31614;&#65292;&#21453;&#26144;&#24403;&#21069;&#30340;&#35752;&#35770;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26377;&#20105;&#35758;&#24615;&#38382;&#39064;&#25968;&#25454;&#38598;&#26500;&#24314;&#26041;&#27861;&#65292;&#22522;&#20110;&#24050;&#32463;&#20844;&#24320;&#21457;&#24067;&#30340;Quora&#38382;&#39064;&#23545;&#25968;&#25454;&#38598;&#30340;&#25193;&#23637;&#12290;&#35813;&#25968;&#25454;&#38598;&#25552;&#20986;&#20102;&#28041;&#21450;&#30693;&#35782;&#26102;&#25928;&#24615;&#12289;&#23433;&#20840;&#24615;&#12289;&#20844;&#24179;&#24615;&#21644;&#20559;&#35265;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#30340;&#19968;&#20010;&#23376;&#38598;&#26469;&#35780;&#20272;&#19981;&#21516;&#30340;LLMs&#65292;&#38416;&#26126;&#23427;&#20204;&#22914;&#20309;&#22788;&#29702;&#20105;&#35758;&#38382;&#39064;&#20197;&#21450;&#37319;&#21462;&#30340;&#31435;&#22330;&#12290;&#36825;&#39033;&#30740;&#31350;&#26368;&#32456;&#26377;&#21161;&#20110;&#25105;&#20204;&#29702;&#35299;LLMs&#19982;&#20105;&#35758;&#38382;&#39064;&#30340;&#20114;&#21160;&#65292;&#24182;&#20026;&#26410;&#26469;&#30740;&#31350;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Controversy is a reflection of our zeitgeist, and an important aspect to any discourse. The rise of large language models (LLMs) as conversational systems has increased public reliance on these systems for answers to their various questions. Consequently, it is crucial to systematically examine how these models respond to questions that pertaining to ongoing debates. However, few such datasets exist in providing human-annotated labels reflecting the contemporary discussions. To foster research in this area, we propose a novel construction of a controversial questions dataset, expanding upon the publicly released Quora Question Pairs Dataset. This dataset presents challenges concerning knowledge recency, safety, fairness, and bias. We evaluate different LLMs using a subset of this dataset, illuminating how they handle controversial issues and the stances they adopt. This research ultimately contributes to our understanding of LLMs' interaction with controversial issues, paving the way f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#23398;&#20064;&#25552;&#38382;&#30456;&#20851;&#38382;&#39064;&#24182;&#36827;&#34892;&#25512;&#29702;&#26469;&#25351;&#23548;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#25191;&#34892;&#30340;&#34892;&#20026;&#30340;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2310.18127</link><description>&lt;p&gt;
&#25552;&#38382;&#26356;&#22810;&#65292;&#20102;&#35299;&#26356;&#22810;&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#30340;&#20915;&#31574;&#38382;&#39064;&#19982;&#24605;&#32500;&#38142;
&lt;/p&gt;
&lt;p&gt;
Ask more, know better: Reinforce-Learned Prompt Questions for Decision Making with Large Language Models. (arXiv:2310.18127v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#23398;&#20064;&#25552;&#38382;&#30456;&#20851;&#38382;&#39064;&#24182;&#36827;&#34892;&#25512;&#29702;&#26469;&#25351;&#23548;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#25191;&#34892;&#30340;&#34892;&#20026;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#23558;&#22522;&#20110;&#34892;&#21160;&#30340;&#31574;&#30053;&#19982;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#25512;&#29702;&#30456;&#32467;&#21512;&#65292;&#23637;&#31034;&#20102;&#35299;&#20915;&#22797;&#26434;&#23454;&#38469;&#25361;&#25112;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#26469;&#35828;&#65292;&#20855;&#26377;&#39640;&#36136;&#37327;&#30340;&#25552;&#31034;&#38750;&#24120;&#37325;&#35201;&#12290;&#30446;&#21069;&#65292;&#36825;&#20123;&#25552;&#31034;&#26159;&#36890;&#36807;&#24191;&#27867;&#20351;&#29992;&#20154;&#21147;&#25163;&#24037;&#21046;&#20316;&#30340;&#65292;&#23548;&#33268;CoT&#31574;&#30053;&#32463;&#24120;&#26080;&#27861;&#25512;&#24191;&#12290;&#20026;&#20102;&#30830;&#20445;&#20302;&#23618;&#25511;&#21046;&#22120;&#36866;&#24403;&#22320;&#22788;&#29702;CoT&#25512;&#29702;&#65292;&#36824;&#38656;&#35201;&#20154;&#20026;&#20171;&#20837;&#26469;&#24320;&#21457;&#25509;&#22320;&#20989;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36808;&#20986;&#20102;&#36808;&#21521;&#22312;&#22797;&#26434;&#25512;&#29702;&#20013;&#24212;&#29992;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#20219;&#21153;&#35299;&#20915;&#30340;&#23436;&#20840;&#38598;&#25104;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#30340;&#31532;&#19968;&#27493;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#39046;&#23548;&#32773;-&#36861;&#38543;&#32773;&#21452;&#23618;&#26694;&#26550;&#65292;&#33021;&#22815;&#23398;&#20064;&#25552;&#38382;&#30456;&#20851;&#38382;&#39064;&#65288;&#25552;&#31034;&#65289;&#65292;&#24182;&#38543;&#21518;&#36827;&#34892;&#25512;&#29702;&#65292;&#25351;&#23548;&#22312;&#29615;&#22659;&#20013;&#25191;&#34892;&#30340;&#34892;&#20026;&#30340;&#23398;&#20064;&#12290;&#19968;&#20010;&#22909;&#30340;&#25552;&#31034;&#24212;&#35813;&#22522;&#20110;&#21382;&#21490;&#30340;&#33258;&#30465;&#24615;&#20462;&#35746;&#26469;&#36827;&#34892;&#20462;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) demonstrate their promise in tackling complicated practical challenges by combining action-based policies with chain of thought (CoT) reasoning. Having high-quality prompts on hand, however, is vital to the framework's effectiveness. Currently, these prompts are handcrafted utilizing extensive human labor, resulting in CoT policies that frequently fail to generalize. Human intervention is also required in order to develop grounding functions that ensure low-level controllers appropriately process CoT reasoning. In this paper, we take the first step towards a fully integrated end-to-end framework for task-solving in real settings employing complicated reasoning. To that purpose, we offer a new leader-follower bilevel framework capable of learning to ask relevant questions (prompts) and subsequently undertaking reasoning to guide the learning of actions to be performed in an environment. A good prompt should make introspective revisions based on historical fi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;OpinSummEval&#65292;&#23545;&#24847;&#35265;&#25688;&#35201;&#36827;&#34892;&#33258;&#21160;&#21270;&#35780;&#20272;&#30340;&#21487;&#38752;&#24615;&#36827;&#34892;&#37325;&#26032;&#35780;&#20272;&#12290;&#30740;&#31350;&#21457;&#29616;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#24230;&#37327;&#36890;&#24120;&#20248;&#20110;&#38750;&#31070;&#32463;&#32593;&#32476;&#30340;&#24230;&#37327;&#65292;&#20294;&#21363;&#20351;&#26159;&#22522;&#20110;&#24378;&#22823;&#27169;&#22411;&#26500;&#24314;&#30340;&#24230;&#37327;&#20063;&#19981;&#33021;&#22312;&#25152;&#26377;&#32500;&#24230;&#19978;&#22987;&#32456;&#20445;&#25345;&#33391;&#22909;&#30340;&#30456;&#20851;&#24615;&#65292;&#31361;&#20986;&#20102;&#23545;&#24847;&#35265;&#25688;&#35201;&#33258;&#21160;&#21270;&#35780;&#20272;&#26041;&#27861;&#30340;&#36827;&#19968;&#27493;&#25913;&#36827;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.18122</link><description>&lt;p&gt;
OpinSummEval:&#20877;&#32771;&#33258;&#21160;&#21270;&#35780;&#20272;&#22312;&#24847;&#35265;&#25688;&#35201;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
OpinSummEval: Revisiting Automated Evaluation for Opinion Summarization. (arXiv:2310.18122v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;OpinSummEval&#65292;&#23545;&#24847;&#35265;&#25688;&#35201;&#36827;&#34892;&#33258;&#21160;&#21270;&#35780;&#20272;&#30340;&#21487;&#38752;&#24615;&#36827;&#34892;&#37325;&#26032;&#35780;&#20272;&#12290;&#30740;&#31350;&#21457;&#29616;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#24230;&#37327;&#36890;&#24120;&#20248;&#20110;&#38750;&#31070;&#32463;&#32593;&#32476;&#30340;&#24230;&#37327;&#65292;&#20294;&#21363;&#20351;&#26159;&#22522;&#20110;&#24378;&#22823;&#27169;&#22411;&#26500;&#24314;&#30340;&#24230;&#37327;&#20063;&#19981;&#33021;&#22312;&#25152;&#26377;&#32500;&#24230;&#19978;&#22987;&#32456;&#20445;&#25345;&#33391;&#22909;&#30340;&#30456;&#20851;&#24615;&#65292;&#31361;&#20986;&#20102;&#23545;&#24847;&#35265;&#25688;&#35201;&#33258;&#21160;&#21270;&#35780;&#20272;&#26041;&#27861;&#30340;&#36827;&#19968;&#27493;&#25913;&#36827;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20854;&#20182;&#31867;&#22411;&#30340;&#25688;&#35201;&#20219;&#21153;&#19981;&#21516;&#65292;&#24847;&#35265;&#25688;&#35201;&#19987;&#27880;&#20110;&#35266;&#28857;&#21644;&#24773;&#24863;&#65292;&#22240;&#27492;&#19982;&#20247;&#19981;&#21516;&#12290;&#34429;&#28982;&#20687;ROUGE&#36825;&#26679;&#30340;&#26576;&#20123;&#33258;&#21160;&#21270;&#35780;&#20272;&#26041;&#27861;&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#23545;&#35780;&#20272;&#24847;&#35265;&#25688;&#35201;&#30340;&#36136;&#37327;&#26159;&#19981;&#21487;&#38752;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;OpinSummEval&#65292;&#23427;&#21253;&#25324;&#26469;&#33258;14&#20010;&#24847;&#35265;&#25688;&#35201;&#27169;&#22411;&#30340;&#20154;&#24037;&#21028;&#26029;&#21644;&#36755;&#20986;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;24&#20010;&#33258;&#21160;&#24230;&#37327;&#19982;&#20154;&#24037;&#35780;&#20998;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#28085;&#30422;&#20102;&#22235;&#20010;&#32500;&#24230;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#24230;&#37327;&#36890;&#24120;&#20248;&#20110;&#38750;&#31070;&#32463;&#32593;&#32476;&#30340;&#24230;&#37327;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#22522;&#20110;&#24378;&#22823;&#27169;&#22411;&#65288;&#22914;BART&#21644;GPT-3/3.5&#65289;&#26500;&#24314;&#30340;&#24230;&#37327;&#20063;&#19981;&#33021;&#22312;&#25152;&#26377;&#32500;&#24230;&#19978;&#22987;&#32456;&#20445;&#25345;&#33391;&#22909;&#30340;&#30456;&#20851;&#24615;&#65292;&#31361;&#20986;&#20102;&#38656;&#35201;&#25913;&#36827;&#24847;&#35265;&#25688;&#35201;&#30340;&#33258;&#21160;&#21270;&#35780;&#20272;&#26041;&#27861;&#30340;&#38656;&#27714;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#20844;&#24320;&#21487;&#29992;&#20110;https://github.com/A-Chicharito-S/OpinSummEval/tree/main&#12290;
&lt;/p&gt;
&lt;p&gt;
Opinion summarization sets itself apart from other types of summarization tasks due to its distinctive focus on aspects and sentiments. Although certain automated evaluation methods like ROUGE have gained popularity, we have found them to be unreliable measures for assessing the quality of opinion summaries. In this paper, we present OpinSummEval, a dataset comprising human judgments and outputs from 14 opinion summarization models. We further explore the correlation between 24 automatic metrics and human ratings across four dimensions. Our findings indicate that metrics based on neural networks generally outperform non-neural ones. However, even metrics built on powerful backbones, such as BART and GPT-3/3.5, do not consistently correlate well across all dimensions, highlighting the need for advancements in automated evaluation methods for opinion summarization. The code and data are publicly available at https://github.com/A-Chicharito-S/OpinSummEval/tree/main.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#19978;&#19979;&#25991;&#21270;&#30693;&#35782;&#33976;&#39311;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#22312;&#25512;&#33616;&#24615;&#33021;&#21644;&#23545;&#35805;&#29983;&#25104;&#30340;&#19968;&#33268;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.18119</link><description>&lt;p&gt;
&#23454;&#29616;&#32479;&#19968;&#30340;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#65306;&#36890;&#36807;&#19978;&#19979;&#25991;&#21270;&#30693;&#35782;&#33976;&#39311;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards a Unified Conversational Recommendation System: Multi-task Learning via Contextualized Knowledge Distillation. (arXiv:2310.18119v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18119
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19978;&#19979;&#25991;&#21270;&#30693;&#35782;&#33976;&#39311;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#22312;&#25512;&#33616;&#24615;&#33021;&#21644;&#23545;&#35805;&#29983;&#25104;&#30340;&#19968;&#33268;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#35201;&#27714;&#20195;&#29702;&#21521;&#29992;&#25143;&#25512;&#33616;&#19968;&#32452;&#39033;&#30446;&#65292;&#32780;&#25512;&#33616;&#36807;&#31243;&#21457;&#29983;&#22312;&#33258;&#28982;&#35821;&#35328;&#23545;&#35805;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#23545;&#35805;&#33021;&#21147;&#21644;&#20010;&#24615;&#21270;&#25512;&#33616;&#30340;&#38656;&#27714;&#65292;&#20043;&#21069;&#30340;&#30740;&#31350;&#20351;&#29992;&#20102;&#20998;&#31163;&#30340;&#25512;&#33616;&#21644;&#23545;&#35805;&#27169;&#22359;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#19981;&#21487;&#36991;&#20813;&#22320;&#23548;&#33268;&#25512;&#33616;&#32467;&#26524;&#21644;&#29983;&#25104;&#30340;&#22238;&#24212;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#19978;&#19979;&#25991;&#21270;&#30693;&#35782;&#33976;&#39311;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#26469;&#23454;&#29616;&#32479;&#19968;&#30340;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#29256;&#26412;&#30340;&#19978;&#19979;&#25991;&#21270;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65306;&#30828;&#38376;&#21644;&#36719;&#38376;&#12290;&#21069;&#32773;&#22312;&#20004;&#20010;&#20219;&#21153;&#29305;&#23450;&#30340;&#25945;&#24072;&#20043;&#38388;&#36827;&#34892;&#26377;&#36873;&#25321;&#30340;&#38376;&#25511;&#65292;&#32780;&#21518;&#32773;&#25972;&#21512;&#20102;&#20004;&#20010;&#25945;&#24072;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#38376;&#25511;&#20197;&#19978;&#19979;&#25991;&#29305;&#23450;&#30340;&#26041;&#24335;&#23454;&#26102;&#35745;&#31639;&#65292;&#20415;&#20110;&#28789;&#27963;&#22320;&#25972;&#21512;&#30456;&#20851;&#30693;&#35782;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#21333;&#19968;&#27169;&#22411;&#26174;&#33879;&#25552;&#39640;&#20102;&#25512;&#33616;&#24615;&#33021;&#65292;&#21516;&#26102;&#20063;&#25552;&#39640;&#20102;&#23545;&#35805;&#29983;&#25104;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Conversational Recommendation System (CRS), an agent is asked to recommend a set of items to users within natural language conversations. To address the need for both conversational capability and personalized recommendations, prior works have utilized separate recommendation and dialogue modules. However, such approach inevitably results in a discrepancy between recommendation results and generated responses. To bridge the gap, we propose a multi-task learning for a unified CRS, where a single model jointly learns both tasks via Contextualized Knowledge Distillation (ConKD). We introduce two versions of ConKD: hard gate and soft gate. The former selectively gates between two task-specific teachers, while the latter integrates knowledge from both teachers. Our gates are computed on-the-fly in a context-specific manner, facilitating flexible integration of relevant knowledge. Extensive experiments demonstrate that our single model significantly improves recommendation performance whi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#26816;&#27979;&#23398;&#20064;&#32773;&#35770;&#35777;&#20013;&#30340;&#32570;&#22833;&#37096;&#20998;&#21644;&#22635;&#34917;&#36825;&#20123;&#32570;&#22833;&#37096;&#20998;&#12290;&#36890;&#36807;&#33258;&#21160;&#21019;&#24314;&#35821;&#26009;&#24211;&#65292;&#21487;&#20197;&#24110;&#21161;&#23398;&#20064;&#32773;&#25552;&#39640;&#35770;&#35777;&#36136;&#37327;&#12290;&#22522;&#20110;ICLEv3&#35821;&#26009;&#24211;&#65292;&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;40,089&#20010;&#35770;&#35777;&#23454;&#20363;&#12290;</title><link>http://arxiv.org/abs/2310.18098</link><description>&lt;p&gt;
&#24515;&#24605;&#22659;&#30028;&#65306;&#29992;&#20110;&#23398;&#20064;&#32773;&#35770;&#35777;&#20013;&#30465;&#30053;&#37096;&#20998;&#26816;&#27979;&#21644;&#37325;&#24314;&#30340;&#33258;&#21160;&#35821;&#26009;&#24211;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Mind the Gap: Automated Corpus Creation for Enthymeme Detection and Reconstruction in Learner Arguments. (arXiv:2310.18098v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#26816;&#27979;&#23398;&#20064;&#32773;&#35770;&#35777;&#20013;&#30340;&#32570;&#22833;&#37096;&#20998;&#21644;&#22635;&#34917;&#36825;&#20123;&#32570;&#22833;&#37096;&#20998;&#12290;&#36890;&#36807;&#33258;&#21160;&#21019;&#24314;&#35821;&#26009;&#24211;&#65292;&#21487;&#20197;&#24110;&#21161;&#23398;&#20064;&#32773;&#25552;&#39640;&#35770;&#35777;&#36136;&#37327;&#12290;&#22522;&#20110;ICLEv3&#35821;&#26009;&#24211;&#65292;&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;40,089&#20010;&#35770;&#35777;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#23398;&#20064;&#32773;&#26469;&#35828;&#65292;&#20889;&#24378;&#26377;&#21147;&#30340;&#35770;&#35777;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#23427;&#35201;&#27714;&#36873;&#25321;&#21644;&#23433;&#25490;&#22810;&#20010;&#35770;&#35777;&#24615;&#31687;&#31456;&#21333;&#20803;&#65288;ADUs&#65289;&#65292;&#20351;&#20854;&#36923;&#36753;&#21644;&#36830;&#36143;&#65292;&#24182;&#20915;&#23450;&#26263;&#21547;&#30340;ADUs&#12290;&#28982;&#32780;&#65292;&#24403;&#20851;&#38190;&#30340;ADUs&#32570;&#22833;&#26102;&#65292;&#35835;&#32773;&#21487;&#33021;&#26080;&#27861;&#36319;&#38543;&#25512;&#29702;&#25110;&#29702;&#35299;&#35770;&#35777;&#30340;&#20027;&#35201;&#35266;&#28857;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;&#23398;&#20064;&#32773;&#35770;&#35777;&#20219;&#21153;&#65306;&#26816;&#27979;&#35770;&#35777;&#20013;&#30340;&#32570;&#22833;&#65288;&#30465;&#30053;&#37096;&#20998;&#26816;&#27979;&#65289;&#21644;&#22635;&#34917;&#36825;&#20123;&#32570;&#22833;&#65288;&#30465;&#30053;&#37096;&#20998;&#37325;&#24314;&#65289;&#12290;&#36825;&#20004;&#20010;&#20219;&#21153;&#30340;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#23398;&#20064;&#32773;&#25552;&#39640;&#35770;&#35777;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#20174;&#19968;&#20010;&#35770;&#35777;&#25991;&#26412;&#20013;&#21024;&#38500;&#23545;&#35770;&#35777;&#21450;&#20854;&#36136;&#37327;&#33267;&#20851;&#37325;&#35201;&#30340;ADUs&#26469;&#33258;&#21160;&#21019;&#24314;&#36825;&#20123;&#20219;&#21153;&#30340;&#35821;&#26009;&#24211;&#65292;&#21516;&#26102;&#20445;&#25345;&#25991;&#26412;&#30340;&#33258;&#28982;&#24615;&#12290;&#22522;&#20110;ICLEv3&#35770;&#35777;&#24615;&#23398;&#29983;&#25991;&#31456;&#35821;&#26009;&#24211;&#65292;&#25105;&#20204;&#20026;&#30465;&#30053;&#37096;&#20998;&#26816;&#27979;&#21644;&#37325;&#24314;&#21019;&#24314;&#20102;40,089&#20010;&#35770;&#35777;&#23454;&#20363;&#12290;&#36890;&#36807;&#25163;&#21160;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#35777;&#25454;&#35777;&#26126;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
Writing strong arguments can be challenging for learners. It requires to select and arrange multiple argumentative discourse units (ADUs) in a logical and coherent way as well as to decide which ADUs to leave implicit, so called enthymemes. However, when important ADUs are missing, readers might not be able to follow the reasoning or understand the argument's main point. This paper introduces two new tasks for learner arguments: to identify gaps in arguments (enthymeme detection) and to fill such gaps (enthymeme reconstruction). Approaches to both tasks may help learners improve their argument quality. We study how corpora for these tasks can be created automatically by deleting ADUs from an argumentative text that are central to the argument and its quality, while maintaining the text's naturalness. Based on the ICLEv3 corpus of argumentative learner essays, we create 40,089 argument instances for enthymeme detection and reconstruction. Through manual studies, we provide evidence that
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#36229;&#36807;250,000&#20010;&#21807;&#19968;&#20107;&#23454;&#26680;&#26597;&#30340;&#20998;&#26512;&#65292;&#25506;&#31350;&#20102;&#22810;&#35821;&#35328;&#38169;&#35823;&#20449;&#24687;&#30340;&#26222;&#36941;&#24615;&#21644;&#21160;&#24577;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#37096;&#20998;&#38169;&#35823;&#20449;&#24687;&#33021;&#22815;&#31359;&#36234;&#35821;&#35328;&#38556;&#30861;&#65292;&#24182;&#19988;&#22312;&#30456;&#21516;&#35821;&#35328;&#20013;&#26356;&#26377;&#21487;&#33021;&#20256;&#25773;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#38169;&#35823;&#20449;&#24687;&#38543;&#26102;&#38388;&#28436;&#21464;&#24182;&#22312;&#19981;&#21516;&#35821;&#35328;&#38388;&#21457;&#29983;&#31361;&#21464;&#12290;</title><link>http://arxiv.org/abs/2310.18089</link><description>&lt;p&gt;
&#22312;&#32763;&#35793;&#20013;&#36855;&#22833;-&#22810;&#35821;&#35328;&#38169;&#35823;&#20449;&#24687;&#21450;&#20854;&#28436;&#21464;
&lt;/p&gt;
&lt;p&gt;
Lost in Translation -- Multilingual Misinformation and its Evolution. (arXiv:2310.18089v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18089
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#36229;&#36807;250,000&#20010;&#21807;&#19968;&#20107;&#23454;&#26680;&#26597;&#30340;&#20998;&#26512;&#65292;&#25506;&#31350;&#20102;&#22810;&#35821;&#35328;&#38169;&#35823;&#20449;&#24687;&#30340;&#26222;&#36941;&#24615;&#21644;&#21160;&#24577;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#37096;&#20998;&#38169;&#35823;&#20449;&#24687;&#33021;&#22815;&#31359;&#36234;&#35821;&#35328;&#38556;&#30861;&#65292;&#24182;&#19988;&#22312;&#30456;&#21516;&#35821;&#35328;&#20013;&#26356;&#26377;&#21487;&#33021;&#20256;&#25773;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#38169;&#35823;&#20449;&#24687;&#38543;&#26102;&#38388;&#28436;&#21464;&#24182;&#22312;&#19981;&#21516;&#35821;&#35328;&#38388;&#21457;&#29983;&#31361;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#26102;&#20195;&#65292;&#35823;&#23548;&#21644;&#34394;&#20551;&#20449;&#24687;&#27491;&#22312;&#36805;&#36895;&#22312;&#21508;&#31181;&#35821;&#35328;&#21644;&#36793;&#30028;&#38388;&#20256;&#25773;&#65292;&#26500;&#25104;&#20102;&#26085;&#30410;&#22686;&#38271;&#30340;&#23041;&#32961;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;95&#31181;&#35821;&#35328;&#20013;&#36229;&#36807;250,000&#20010;&#21807;&#19968;&#20107;&#23454;&#26680;&#26597;&#30340;&#20998;&#26512;&#65292;&#25506;&#31350;&#20102;&#22810;&#35821;&#35328;&#38169;&#35823;&#20449;&#24687;&#30340;&#26222;&#36941;&#24615;&#21644;&#21160;&#24577;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21457;&#29616;&#22823;&#22810;&#25968;&#38169;&#35823;&#20449;&#24687;&#20027;&#24352;&#20165;&#34987;&#20107;&#23454;&#26680;&#26597;&#19968;&#27425;&#65292;&#20294;11.7%&#30340;&#20027;&#24352;(&#36229;&#36807;21,000&#20010;)&#34987;&#26680;&#26597;&#22810;&#27425;&#12290;&#36816;&#29992;&#20107;&#23454;&#26680;&#26597;&#20316;&#20026;&#38169;&#35823;&#20449;&#24687;&#20256;&#25773;&#30340;&#20195;&#29702;&#25351;&#26631;&#65292;&#25105;&#20204;&#21457;&#29616;33%&#30340;&#37325;&#22797;&#20027;&#24352;&#31359;&#36234;&#35821;&#35328;&#38556;&#30861;&#65292;&#26263;&#31034;&#37096;&#20998;&#38169;&#35823;&#20449;&#24687;&#28183;&#36879;&#20102;&#35821;&#35328;&#36793;&#30028;&#12290;&#28982;&#32780;&#65292;&#25193;&#25955;&#27169;&#24335;&#34920;&#29616;&#20986;&#36739;&#24378;&#30340;&#21516;&#36136;&#24615;&#65292;&#38169;&#35823;&#20449;&#24687;&#26356;&#26377;&#21487;&#33021;&#22312;&#30456;&#21516;&#35821;&#35328;&#20013;&#20256;&#25773;&#12290;&#20026;&#30740;&#31350;&#20027;&#24352;&#38543;&#26102;&#38388;&#30340;&#28436;&#21464;&#21644;&#36328;&#35821;&#35328;&#30340;&#31361;&#21464;&#65292;&#25105;&#20204;&#20351;&#29992;&#22810;&#35821;&#35328;&#21477;&#23376;&#23884;&#20837;&#26469;&#34920;&#31034;&#20107;&#23454;&#26680;&#26597;&#65292;&#24182;&#23545;&#35821;&#20041;&#30456;&#20284;&#30340;&#20027;&#24352;&#36827;&#34892;&#32858;&#31867;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#36830;&#25509;&#32452;&#20214;&#21644;&#26368;&#30701;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Misinformation and disinformation are growing threats in the digital age, spreading rapidly across languages and borders. This paper investigates the prevalence and dynamics of multilingual misinformation through an analysis of over 250,000 unique fact-checks spanning 95 languages. First, we find that while the majority of misinformation claims are only fact-checked once, 11.7%, corresponding to more than 21,000 claims, are checked multiple times. Using fact-checks as a proxy for the spread of misinformation, we find 33% of repeated claims cross linguistic boundaries, suggesting that some misinformation permeates language barriers. However, spreading patterns exhibit strong homophily, with misinformation more likely to spread within the same language. To study the evolution of claims over time and mutations across languages, we represent fact-checks with multilingual sentence embeddings and cluster semantically similar claims. We analyze the connected components and shortest paths conn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#22312;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#36807;&#22810;&#30340;&#32972;&#26223;&#20449;&#24687;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#36890;&#36807;&#36807;&#28388;&#25481;&#26377;&#23475;&#30340;&#27573;&#33853;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.18077</link><description>&lt;p&gt;
&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#26377;&#23475;&#32972;&#26223;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Detrimental Contexts in Open-Domain Question Answering. (arXiv:2310.18077v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#22312;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#36807;&#22810;&#30340;&#32972;&#26223;&#20449;&#24687;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#36890;&#36807;&#36807;&#28388;&#25481;&#26377;&#23475;&#30340;&#27573;&#33853;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#30693;&#35782;&#23494;&#38598;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#24191;&#27867;&#35748;&#21487;&#30340;&#35266;&#28857;&#26159;&#35775;&#38382;&#26356;&#22810;&#20449;&#24687;&#26377;&#21161;&#20110;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#24615;&#33021;&#25913;&#21892;&#12290;&#28982;&#32780;&#65292;&#20196;&#20154;&#24863;&#21040;&#22256;&#24785;&#30340;&#26159;&#65292;&#24403;&#22312;&#24120;&#35265;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;&#36807;&#22810;&#30340;&#32972;&#26223;&#20449;&#24687;&#20250;&#23545;&#27169;&#22411;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#22312;&#38382;&#31572;&#20013;&#20351;&#29992;&#30340;&#26816;&#32034;-&#38405;&#35835;&#32467;&#26500;&#20013;&#65292;&#27573;&#33853;&#22914;&#20309;&#23545;&#27169;&#22411;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;&#65292;&#24403;&#21069;&#30340;&#38405;&#35835;&#32467;&#26500;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#26816;&#32034;&#21040;&#30340;&#27573;&#33853;&#65292;&#22312;&#20351;&#29992;&#25972;&#20010;&#27573;&#33853;&#26102;&#19982;&#21033;&#29992;&#23427;&#20204;&#30340;&#23376;&#38598;&#30456;&#27604;&#65292;&#20854;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#36807;&#28388;&#25481;&#26377;&#23475;&#30340;&#27573;&#33853;&#65292;&#21487;&#20197;&#22312;&#20004;&#20010;&#21463;&#27426;&#36814;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#23558;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#25552;&#39640;10%&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#32467;&#26524;&#26159;&#36890;&#36807;&#21033;&#29992;&#29616;&#26377;&#30340;&#26816;&#32034;&#26041;&#27861;&#32780;&#26080;&#38656;&#36827;&#19968;&#27493;&#35757;&#32451;&#25110;&#25968;&#25454;&#26469;&#23454;&#29616;&#30340;&#12290;&#25105;&#20204;&#36824;&#36827;&#19968;&#27493;&#24378;&#35843;&#20102;&#35782;&#21035;&#26377;&#23475;&#32972;&#26223;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
For knowledge intensive NLP tasks, it has been widely accepted that accessing more information is a contributing factor to improvements in the model's end-to-end performance. However, counter-intuitively, too much context can have a negative impact on the model when evaluated on common question answering (QA) datasets. In this paper, we analyze how passages can have a detrimental effect on retrieve-then-read architectures used in question answering. Our empirical evidence indicates that the current read architecture does not fully leverage the retrieved passages and significantly degrades its performance when using the whole passages compared to utilizing subsets of them. Our findings demonstrate that model accuracy can be improved by 10% on two popular QA datasets by filtering out detrimental passages. Additionally, these outcomes are attained by utilizing existing retrieval methods without further training or data. We further highlight the challenges associated with identifying the d
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#29983;&#25104;&#19978;&#19979;&#25991;&#27573;&#33853;&#19982;&#20256;&#32479;&#26816;&#32034;&#27493;&#39588;&#30456;&#27604;&#30340;&#20248;&#21183;&#65292;&#24182;&#24341;&#20837;&#20102;&#30693;&#35782;&#35821;&#26009;&#38169;&#35823;&#30340;&#27010;&#24565;&#12290;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26356;&#22823;&#33539;&#22260;&#20869;&#30340;&#27573;&#33853;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#38382;&#31572;&#24615;&#33021;&#30340;&#25552;&#21319;&#65292;&#34920;&#26126;&#23384;&#22312;&#30693;&#35782;&#35821;&#26009;&#38169;&#35823;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2310.18076</link><description>&lt;p&gt;
&#38382;&#31572;&#31995;&#32479;&#20013;&#30693;&#35782;&#35821;&#26009;&#38169;&#35823;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Knowledge Corpus Error in Question Answering. (arXiv:2310.18076v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18076
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#29983;&#25104;&#19978;&#19979;&#25991;&#27573;&#33853;&#19982;&#20256;&#32479;&#26816;&#32034;&#27493;&#39588;&#30456;&#27604;&#30340;&#20248;&#21183;&#65292;&#24182;&#24341;&#20837;&#20102;&#30693;&#35782;&#35821;&#26009;&#38169;&#35823;&#30340;&#27010;&#24565;&#12290;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26356;&#22823;&#33539;&#22260;&#20869;&#30340;&#27573;&#33853;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#38382;&#31572;&#24615;&#33021;&#30340;&#25552;&#21319;&#65292;&#34920;&#26126;&#23384;&#22312;&#30693;&#35782;&#35821;&#26009;&#38169;&#35823;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#19968;&#20123;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#19978;&#19979;&#25991;&#27573;&#33853;&#65292;&#21462;&#20195;&#38382;&#31572;&#27969;&#31243;&#20013;&#20256;&#32479;&#30340;&#26816;&#32034;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#20026;&#20160;&#20040;&#29983;&#25104;&#30340;&#27573;&#33853;&#27604;&#26816;&#32034;&#21040;&#30340;&#27573;&#33853;&#26356;&#26377;&#25928;&#12290;&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#38382;&#31572;&#38382;&#39064;&#30340;&#20256;&#32479;&#20844;&#24335;&#65292;&#24182;&#24341;&#20837;&#20102;&#30693;&#35782;&#35821;&#26009;&#38169;&#35823;&#30340;&#27010;&#24565;&#12290;&#24403;&#29992;&#20110;&#26816;&#32034;&#30340;&#30693;&#35782;&#35821;&#26009;&#20165;&#26159;&#25972;&#20010;&#23383;&#31526;&#20018;&#31354;&#38388;&#30340;&#19968;&#20010;&#23376;&#38598;&#26102;&#65292;&#21487;&#33021;&#20250;&#20986;&#29616;&#36825;&#31181;&#38169;&#35823;&#65292;&#26377;&#21487;&#33021;&#25490;&#38500;&#20102;&#23384;&#22312;&#20110;&#35821;&#26009;&#20043;&#22806;&#30340;&#26356;&#26377;&#24110;&#21161;&#30340;&#27573;&#33853;&#12290;LLMs&#21487;&#20197;&#36890;&#36807;&#22312;&#19968;&#20010;&#26356;&#22823;&#30340;&#31354;&#38388;&#20013;&#29983;&#25104;&#27573;&#33853;&#26469;&#32531;&#35299;&#36825;&#20010;&#32570;&#28857;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#20351;&#29992;LLMs&#26469;&#25913;&#20889;&#20154;&#24037;&#26631;&#27880;&#30340;&#40644;&#37329;&#19978;&#19979;&#25991;&#30340;&#23454;&#39564;&#65292;&#20197;&#32463;&#39564;&#24615;&#22320;&#35266;&#23519;&#30693;&#35782;&#35821;&#26009;&#38169;&#35823;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#38382;&#31572;&#22522;&#20934;&#19978;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#20351;&#29992;&#25913;&#20889;&#30340;&#27573;&#33853;&#26102;&#24615;&#33021;&#25552;&#21319;&#20102;10%-13%&#65292;&#34920;&#26126;&#20102;&#30693;&#35782;&#35821;&#26009;&#38169;&#35823;&#30340;&#23384;&#22312;&#20449;&#21495;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;ht&#22788;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works in open-domain question answering (QA) have explored generating context passages from large language models (LLMs), replacing the traditional retrieval step in the QA pipeline. However, it is not well understood why generated passages can be more effective than retrieved ones. This study revisits the conventional formulation of QA and introduces the concept of knowledge corpus error. This error arises when the knowledge corpus used for retrieval is only a subset of the entire string space, potentially excluding more helpful passages that exist outside the corpus. LLMs may mitigate this shortcoming by generating passages in a larger space. We come up with an experiment of paraphrasing human-annotated gold context using LLMs to observe knowledge corpus error empirically. Our results across three QA benchmarks reveal an increased performance (10% - 13%) when using paraphrased passage, indicating a signal for the existence of knowledge corpus error. Our code is available at ht
&lt;/p&gt;</description></item><item><title>DUMA&#26159;&#19968;&#31181;&#20855;&#26377;&#24555;&#36895;&#21644;&#24930;&#36895;&#24605;&#32771;&#33021;&#21147;&#30340;&#21452;&#37325;&#24605;&#32500;&#23545;&#35805;&#20195;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#20004;&#20010;&#29983;&#25104;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26681;&#25454;&#24773;&#20917;&#22312;&#30452;&#35266;&#21709;&#24212;&#21644;&#28145;&#24605;&#29087;&#34385;&#30340;&#38382;&#39064;&#35299;&#20915;&#36807;&#31243;&#20043;&#38388;&#26080;&#32541;&#20999;&#25442;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.18075</link><description>&lt;p&gt;
DUMA&#65306;&#20855;&#26377;&#24555;&#36895;&#21644;&#24930;&#36895;&#24605;&#32771;&#33021;&#21147;&#30340;&#21452;&#37325;&#24605;&#32500;&#23545;&#35805;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
DUMA: a Dual-Mind Conversational Agent with Fast and Slow Thinking. (arXiv:2310.18075v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18075
&lt;/p&gt;
&lt;p&gt;
DUMA&#26159;&#19968;&#31181;&#20855;&#26377;&#24555;&#36895;&#21644;&#24930;&#36895;&#24605;&#32771;&#33021;&#21147;&#30340;&#21452;&#37325;&#24605;&#32500;&#23545;&#35805;&#20195;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#20004;&#20010;&#29983;&#25104;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26681;&#25454;&#24773;&#20917;&#22312;&#30452;&#35266;&#21709;&#24212;&#21644;&#28145;&#24605;&#29087;&#34385;&#30340;&#38382;&#39064;&#35299;&#20915;&#36807;&#31243;&#20043;&#38388;&#26080;&#32541;&#20999;&#25442;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#20154;&#31867;&#35748;&#30693;&#30340;&#21452;&#36807;&#31243;&#29702;&#35770;&#21551;&#21457;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DUMA&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#35805;&#20195;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#20004;&#20010;&#29992;&#20110;&#24555;&#36895;&#21644;&#24930;&#36895;&#24605;&#32771;&#30340;&#29983;&#25104;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#20307;&#29616;&#20102;&#21452;&#37325;&#24605;&#32500;&#26426;&#21046;&#12290;&#24555;&#36895;&#24605;&#32771;&#27169;&#22411;&#20316;&#20026;&#20027;&#35201;&#25509;&#21475;&#29992;&#20110;&#22806;&#37096;&#20132;&#20114;&#21644;&#21021;&#22987;&#21709;&#24212;&#29983;&#25104;&#65292;&#26681;&#25454;&#23436;&#25972;&#21709;&#24212;&#30340;&#22797;&#26434;&#24615;&#35780;&#20272;&#26159;&#21542;&#38656;&#35201;&#35843;&#29992;&#24930;&#36895;&#24605;&#32771;&#27169;&#22411;&#12290;&#19968;&#26086;&#34987;&#35843;&#29992;&#65292;&#24930;&#36895;&#24605;&#32771;&#27169;&#22411;&#25509;&#31649;&#23545;&#35805;&#65292;&#22312;&#32454;&#33268;&#35268;&#21010;&#12289;&#25512;&#29702;&#21644;&#24037;&#20855;&#21033;&#29992;&#26041;&#38754;&#36827;&#34892;&#24037;&#20316;&#65292;&#25552;&#20379;&#32463;&#36807;&#20805;&#20998;&#20998;&#26512;&#30340;&#21709;&#24212;&#12290;&#36825;&#31181;&#21452;&#37325;&#24605;&#32500;&#37197;&#32622;&#20801;&#35768;&#26681;&#25454;&#24773;&#20917;&#22312;&#30452;&#35266;&#21709;&#24212;&#21644;&#28145;&#24605;&#29087;&#34385;&#30340;&#38382;&#39064;&#35299;&#20915;&#36807;&#31243;&#20043;&#38388;&#26080;&#32541;&#20999;&#25442;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#29992;&#20110;&#22788;&#29702;&#25151;&#22320;&#20135;&#34892;&#19994;&#22312;&#32447;&#21672;&#35810;&#30340;&#23545;&#35805;&#20195;&#29702;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25928;&#26524;&#21644;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#20102;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by the dual-process theory of human cognition, we introduce DUMA, a novel conversational agent framework that embodies a dual-mind mechanism through the utilization of two generative Large Language Models (LLMs) dedicated to fast and slow thinking respectively. The fast thinking model serves as the primary interface for external interactions and initial response generation, evaluating the necessity for engaging the slow thinking model based on the complexity of the complete response. When invoked, the slow thinking model takes over the conversation, engaging in meticulous planning, reasoning, and tool utilization to provide a well-analyzed response. This dual-mind configuration allows for a seamless transition between intuitive responses and deliberate problem-solving processes based on the situation. We have constructed a conversational agent to handle online inquiries in the real estate industry. The experiment proves that our method balances effectiveness and efficiency, an
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#26694;&#26550;&#26469;&#20174;&#22797;&#26434;&#30340;ESG&#24180;&#24230;&#25253;&#21578;&#20013;&#25552;&#21462;&#30446;&#24405;&#12290;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;ESGDoc&#65292;&#21253;&#25324;&#26469;&#33258;563&#23478;&#20844;&#21496;&#30340;1,093&#20221;&#25253;&#21578;&#12290;&#36890;&#36807;&#26500;&#24314;-&#24314;&#27169;-&#20462;&#25913;&#65288;CMM&#65289;&#36807;&#31243;&#65292;&#35813;&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#28040;&#38500;&#20197;&#21069;&#26041;&#27861;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#39640;&#30446;&#24405;&#25552;&#21462;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.18073</link><description>&lt;p&gt;
&#20174;&#22797;&#26434;&#30340;ESG&#24180;&#24230;&#25253;&#21578;&#20013;&#25552;&#21462;&#30446;&#24405;&#30340;&#21487;&#25193;&#23637;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Scalable Framework for Table of Contents Extraction from Complex ESG Annual Reports. (arXiv:2310.18073v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18073
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#26694;&#26550;&#26469;&#20174;&#22797;&#26434;&#30340;ESG&#24180;&#24230;&#25253;&#21578;&#20013;&#25552;&#21462;&#30446;&#24405;&#12290;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;ESGDoc&#65292;&#21253;&#25324;&#26469;&#33258;563&#23478;&#20844;&#21496;&#30340;1,093&#20221;&#25253;&#21578;&#12290;&#36890;&#36807;&#26500;&#24314;-&#24314;&#27169;-&#20462;&#25913;&#65288;CMM&#65289;&#36807;&#31243;&#65292;&#35813;&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#28040;&#38500;&#20197;&#21069;&#26041;&#27861;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#39640;&#30446;&#24405;&#25552;&#21462;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#24405;&#65288;ToC&#65289;&#25552;&#21462;&#20391;&#37325;&#20110;&#20197;&#20998;&#23618;&#30340;&#26041;&#24335;&#23545;&#25991;&#26723;&#36827;&#34892;&#32467;&#26500;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;ESGDoc&#65292;&#21253;&#25324;&#26469;&#33258;2001&#24180;&#33267;2022&#24180;&#30340;563&#23478;&#20844;&#21496;&#30340;1,093&#20221;ESG&#24180;&#24230;&#25253;&#21578;&#12290;&#36825;&#20123;&#25253;&#21578;&#30001;&#20110;&#20854;&#22810;&#26679;&#30340;&#32467;&#26500;&#21644;&#24191;&#27867;&#30340;&#38271;&#24230;&#32780;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Toc&#25552;&#21462;&#26694;&#26550;&#65292;&#21253;&#25324;&#19977;&#20010;&#27493;&#39588;&#65306;&#65288;1&#65289;&#26681;&#25454;&#38405;&#35835;&#39034;&#24207;&#21644;&#23383;&#20307;&#22823;&#23567;&#26500;&#24314;&#25991;&#26412;&#22359;&#30340;&#21021;&#22987;&#26641;&#65307;&#65288;2&#65289;&#36890;&#36807;&#32771;&#34385;&#33410;&#28857;&#20026;&#20013;&#24515;&#30340;&#23376;&#26641;&#20013;&#25429;&#33719;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#21333;&#29420;&#23545;&#27599;&#20010;&#26641;&#33410;&#28857;&#65288;&#25110;&#25991;&#26412;&#22359;&#65289;&#36827;&#34892;&#24314;&#27169;&#65307;&#65288;3&#65289;&#36890;&#36807;&#23545;&#27599;&#20010;&#26641;&#33410;&#28857;&#65288;&#20445;&#30041;&#12289;&#21024;&#38500;&#25110;&#31227;&#21160;&#65289;&#37319;&#21462;&#36866;&#24403;&#30340;&#25805;&#20316;&#26469;&#20462;&#25913;&#21407;&#22987;&#26641;&#12290;&#36825;&#20010;&#26500;&#24314;-&#24314;&#27169;-&#20462;&#25913;&#65288;CMM&#65289;&#36807;&#31243;&#20855;&#26377;&#20960;&#20010;&#20248;&#21183;&#12290;&#23427;&#28040;&#38500;&#20102;&#20197;&#21069;&#26041;&#27861;&#20013;&#38656;&#35201;&#25104;&#23545;&#24314;&#27169;&#33410;&#26631;&#39064;&#30340;&#38656;&#35201;&#65292;&#20351;&#24471;&#25991;&#26723;&#20998;&#21106;&#23454;&#38469;&#21487;&#34892;&#12290;&#36890;&#36807;&#32467;&#21512;&#32467;&#26500;&#24615;&#30340;&#29305;&#24449;&#20449;&#24687;&#65292;&#25105;&#20204;&#21487;&#20197;&#26174;&#33879;&#25913;&#36827;&#30446;&#24405;&#25552;&#21462;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Table of contents (ToC) extraction centres on structuring documents in a hierarchical manner. In this paper, we propose a new dataset, ESGDoc, comprising 1,093 ESG annual reports from 563 companies spanning from 2001 to 2022. These reports pose significant challenges due to their diverse structures and extensive length. To address these challenges, we propose a new framework for Toc extraction, consisting of three steps: (1) Constructing an initial tree of text blocks based on reading order and font sizes; (2) Modelling each tree node (or text block) independently by considering its contextual information captured in node-centric subtree; (3) Modifying the original tree by taking appropriate action on each tree node (Keep, Delete, or Move). This construction-modelling-modification (CMM) process offers several benefits. It eliminates the need for pairwise modelling of section headings as in previous approaches, making document segmentation practically feasible. By incorporating structur
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26679;&#21270;&#35777;&#25454;&#25512;&#29702;&#27169;&#22411;&#65288;Mugen&#65289;&#65292;&#36890;&#36807;&#25552;&#21462;&#31895;&#31961;&#12289;&#20013;&#31561;&#21644;&#32454;&#31890;&#24230;&#30340;&#35777;&#25454;&#65292;&#24182;&#23558;&#20854;&#19982;&#21407;&#22987;&#27573;&#33853;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#21644;&#19968;&#33268;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#39033;&#36873;&#25321;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#20219;&#21153;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2310.18070</link><description>&lt;p&gt;
&#22810;&#26679;&#21270;&#35777;&#25454;&#25512;&#29702;&#29992;&#20110;&#22810;&#39033;&#36873;&#25321;&#38405;&#35835;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Multi-grained Evidence Inference for Multi-choice Reading Comprehension. (arXiv:2310.18070v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18070
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26679;&#21270;&#35777;&#25454;&#25512;&#29702;&#27169;&#22411;&#65288;Mugen&#65289;&#65292;&#36890;&#36807;&#25552;&#21462;&#31895;&#31961;&#12289;&#20013;&#31561;&#21644;&#32454;&#31890;&#24230;&#30340;&#35777;&#25454;&#65292;&#24182;&#23558;&#20854;&#19982;&#21407;&#22987;&#27573;&#33853;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#21644;&#19968;&#33268;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#39033;&#36873;&#25321;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#20219;&#21153;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#39033;&#36873;&#25321;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#26159;&#26426;&#22120;&#26681;&#25454;&#32473;&#23450;&#30340;&#36873;&#39033;&#22238;&#31572;&#38382;&#39064;&#30340;&#19968;&#20010;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#22810;&#39033;&#36873;&#25321;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#20013;&#65292;&#31572;&#26696;&#19981;&#33021;&#30452;&#25509;&#20174;&#32473;&#23450;&#30340;&#25991;&#31456;&#20013;&#25552;&#21462;&#20986;&#26469;&#65292;&#32780;&#23454;&#36136;&#19978;&#38656;&#35201;&#26426;&#22120;&#33021;&#22815;&#20174;&#20934;&#30830;&#30340;&#25552;&#21462;&#35777;&#25454;&#20013;&#36827;&#34892;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#20851;&#38190;&#30340;&#35777;&#25454;&#21487;&#33021;&#21482;&#26159;&#19968;&#20010;&#35789;&#25110;&#30701;&#35821;&#65292;&#32780;&#23427;&#21364;&#38544;&#34255;&#22312;&#32473;&#23450;&#30340;&#20887;&#20313;&#12289;&#22024;&#26434;&#30340;&#25991;&#31456;&#20013;&#65292;&#20855;&#26377;&#22810;&#20010;&#35821;&#35328;&#23618;&#32423;&#65292;&#20174;&#30701;&#35821;&#12289;&#29255;&#27573;&#12289;&#21477;&#23376;&#30452;&#21040;&#25972;&#20010;&#27573;&#33853;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36890;&#29992;&#27169;&#22411;&#22686;&#24378;&#26041;&#27861;&#65292;&#32508;&#21512;&#38598;&#25104;&#22810;&#26679;&#21270;&#30340;&#35777;&#25454;&#65292;&#31216;&#20026;&#22810;&#26679;&#21270;&#35777;&#25454;&#25512;&#29702;&#22120;&#65288;Mugen&#65289;&#65292;&#20197;&#24357;&#34917;&#36825;&#31181;&#33021;&#21147;&#30340;&#19981;&#36275;&#12290;Mugen&#25552;&#21462;&#20102;&#31895;&#31961;&#12289;&#20013;&#31561;&#21644;&#32454;&#31890;&#24230;&#19977;&#31181;&#19981;&#21516;&#23618;&#27425;&#30340;&#35777;&#25454;&#65292;&#24182;&#23558;&#35777;&#25454;&#19982;&#21407;&#22987;&#27573;&#33853;&#32467;&#21512;&#36215;&#26469;&#65292;&#22312;&#22235;&#20010;&#22810;&#39033;&#36873;&#25321;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#32780;&#19968;&#33268;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-choice Machine Reading Comprehension (MRC) is a major and challenging task for machines to answer questions according to provided options. Answers in multi-choice MRC cannot be directly extracted in the given passages, and essentially require machines capable of reasoning from accurate extracted evidence. However, the critical evidence may be as simple as just one word or phrase, while it is hidden in the given redundant, noisy passage with multiple linguistic hierarchies from phrase, fragment, sentence until the entire passage. We thus propose a novel general-purpose model enhancement which integrates multi-grained evidence comprehensively, named Multi-grained evidence inferencer (Mugen), to make up for the inability. Mugen extracts three different granularities of evidence: coarse-, middle- and fine-grained evidence, and integrates evidence with the original passages, achieving significant and consistent performance improvement on four multi-choice MRC benchmarks.
&lt;/p&gt;</description></item><item><title>Therapy&#26159;&#31532;&#19968;&#20010;&#36866;&#29992;&#20110;&#25991;&#26412;&#30340;&#20840;&#23616;&#21644;&#26080;&#27169;&#22411;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#20316;&#29983;&#25104;&#25991;&#26412;&#65292;&#19981;&#20381;&#36182;&#20110;&#21021;&#22987;&#26679;&#26412;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#36755;&#20837;&#31354;&#38388;&#20013;&#27169;&#22411;&#34892;&#20026;&#30340;&#20840;&#23616;&#27010;&#35272;&#12290;</title><link>http://arxiv.org/abs/2310.18063</link><description>&lt;p&gt;
&#8220;&#20146;&#29233;&#30340;&#65292;&#21578;&#35785;&#25105;&#20986;&#20102;&#20160;&#20040;&#38382;&#39064;&#8221;&#65292;&#36890;&#36807;&#21512;&#20316;&#29983;&#25104;&#20840;&#23616;&#25991;&#26412;&#36776;&#21035;&#27169;&#22411;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
"Honey, Tell Me What's Wrong", Global Explanation of Textual Discriminative Models through Cooperative Generation. (arXiv:2310.18063v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18063
&lt;/p&gt;
&lt;p&gt;
Therapy&#26159;&#31532;&#19968;&#20010;&#36866;&#29992;&#20110;&#25991;&#26412;&#30340;&#20840;&#23616;&#21644;&#26080;&#27169;&#22411;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#20316;&#29983;&#25104;&#25991;&#26412;&#65292;&#19981;&#20381;&#36182;&#20110;&#21021;&#22987;&#26679;&#26412;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#36755;&#20837;&#31354;&#38388;&#20013;&#27169;&#22411;&#34892;&#20026;&#30340;&#20840;&#23616;&#27010;&#35272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#26426;&#22120;&#23398;&#20064;&#30340;&#26222;&#21450;&#25552;&#39640;&#20102;&#26080;&#27169;&#22411;&#35299;&#37322;&#31639;&#27861;&#30340;&#37325;&#35201;&#24615;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#36807;&#36731;&#24494;&#25200;&#21160;&#30495;&#23454;&#23454;&#20363;&#26469;&#21019;&#24314;&#20154;&#24037;&#23454;&#20363;&#65292;&#25429;&#25417;&#27169;&#22411;&#20915;&#31574;&#30340;&#21464;&#21270;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#21021;&#22987;&#25968;&#25454;&#65292;&#24182;&#19988;&#21482;&#25552;&#20379;&#20851;&#20110;&#36825;&#20123;&#21021;&#22987;&#25968;&#25454;&#20915;&#31574;&#30340;&#35299;&#37322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; Therapy&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#36866;&#29992;&#20110;&#25991;&#26412;&#30340;&#20840;&#23616;&#21644;&#26080;&#27169;&#22411;&#35299;&#37322;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#36755;&#20837;&#25968;&#25454;&#38598;&#12290;Therapy&#36890;&#36807;&#21512;&#20316;&#29983;&#25104;&#65292;&#26681;&#25454;&#20998;&#31867;&#22120;&#23398;&#20064;&#21040;&#30340;&#20998;&#24067;&#29983;&#25104;&#25991;&#26412;&#12290;&#22240;&#20026;&#23427;&#19981;&#20381;&#36182;&#20110;&#21021;&#22987;&#26679;&#26412;&#65292;&#25152;&#20197;&#21363;&#20351;&#25968;&#25454;&#32570;&#22833;&#65288;&#20363;&#22914;&#22240;&#20445;&#23494;&#21407;&#22240;&#65289;&#65292;&#20063;&#33021;&#29983;&#25104;&#35299;&#37322;&#12290;&#27492;&#22806;&#65292;&#19982;&#23558;&#22810;&#20010;&#23616;&#37096;&#35299;&#37322;&#32452;&#21512;&#25104;&#19968;&#20010;&#20840;&#23616;&#35299;&#37322;&#30340;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;Therapy&#25552;&#20379;&#20102;&#23545;&#36755;&#20837;&#31354;&#38388;&#20013;&#27169;&#22411;&#34892;&#20026;&#30340;&#20840;&#23616;&#27010;&#35272;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#34429;&#28982;&#19981;&#20351;&#29992;&#36755;&#20837;&#25968;&#25454;&#26469;&#29983;&#25104;&#26679;&#26412;&#65292;&#20294; Therapy &#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ubiquity of complex machine learning has raised the importance of model-agnostic explanation algorithms. These methods create artificial instances by slightly perturbing real instances, capturing shifts in model decisions. However, such methods rely on initial data and only provide explanations of the decision for these. To tackle these problems, we propose Therapy, the first global and model-agnostic explanation method adapted to text which requires no input dataset. Therapy generates texts following the distribution learned by a classifier through cooperative generation. Because it does not rely on initial samples, it allows to generate explanations even when data is absent (e.g., for confidentiality reasons). Moreover, conversely to existing methods that combine multiple local explanations into a global one, Therapy offers a global overview of the model behavior on the input space. Our experiments show that although using no input data to generate samples, Therapy provides insig
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;ViCLEVR&#25968;&#25454;&#38598;&#65292;&#23427;&#26159;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#36234;&#21335;&#35821;&#35270;&#35273;&#25512;&#29702;&#33021;&#21147;&#30340;&#38598;&#21512;&#12290;&#36890;&#36807;&#20998;&#26512;&#35813;&#25968;&#25454;&#38598;&#65292;&#23545;&#24403;&#20195;&#35270;&#35273;&#25512;&#29702;&#31995;&#32479;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.18046</link><description>&lt;p&gt;
ViCLEVR&#65306;&#19968;&#31181;&#29992;&#20110;&#36234;&#21335;&#35821;&#35270;&#35273;&#38382;&#31572;&#30340;&#35270;&#35273;&#25512;&#29702;&#25968;&#25454;&#38598;&#21644;&#28151;&#21512;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ViCLEVR: A Visual Reasoning Dataset and Hybrid Multimodal Fusion Model for Visual Question Answering in Vietnamese. (arXiv:2310.18046v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;ViCLEVR&#25968;&#25454;&#38598;&#65292;&#23427;&#26159;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#36234;&#21335;&#35821;&#35270;&#35273;&#25512;&#29702;&#33021;&#21147;&#30340;&#38598;&#21512;&#12290;&#36890;&#36807;&#20998;&#26512;&#35813;&#25968;&#25454;&#38598;&#65292;&#23545;&#24403;&#20195;&#35270;&#35273;&#25512;&#29702;&#31995;&#32479;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#22240;&#20854;&#21253;&#25324;&#26234;&#33021;&#27773;&#36710;&#36741;&#21161;&#12289;&#24110;&#21161;&#35270;&#38556;&#20154;&#22763;&#20197;&#21450;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#30340;&#25991;&#26723;&#22270;&#20687;&#20449;&#24687;&#26816;&#32034;&#31561;&#22810;&#31181;&#24212;&#29992;&#32780;&#21463;&#21040;&#37325;&#35270;&#12290;VQA&#38656;&#35201;&#26377;&#25928;&#22320;&#23558;&#38382;&#39064;&#21644;&#22270;&#20687;&#30340;&#20449;&#24687;&#36827;&#34892;&#38598;&#25104;&#20197;&#29983;&#25104;&#20934;&#30830;&#30340;&#31572;&#26696;&#12290;VQA&#30340;&#31070;&#32463;&#27169;&#22411;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20027;&#35201;&#20851;&#27880;&#36164;&#28304;&#20016;&#23500;&#30340;&#35821;&#35328;&#65292;&#22914;&#33521;&#35821;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;ViCLEVR&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#32452;&#24320;&#21019;&#24615;&#30340;&#29992;&#20110;&#35780;&#20272;&#36234;&#21335;&#35821;&#20013;&#21508;&#31181;&#35270;&#35273;&#25512;&#29702;&#33021;&#21147;&#24182;&#20943;&#23567;&#20559;&#24046;&#30340;&#38598;&#21512;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#36229;&#36807;26,000&#24352;&#22270;&#20687;&#21644;30,000&#20010;&#38382;&#39064;-&#31572;&#26696;&#23545;&#65288;QAs&#65289;&#65292;&#27599;&#20010;&#38382;&#39064;&#37117;&#26377;&#27880;&#37322;&#25351;&#23450;&#25152;&#28041;&#21450;&#30340;&#25512;&#29702;&#31867;&#22411;&#12290;&#21033;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23545;&#24403;&#20195;&#35270;&#35273;&#25512;&#29702;&#31995;&#32479;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#21253;&#25324;&#23427;&#20204;&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Visual Question Answering (VQA) has gained significant attention for its diverse applications, including intelligent car assistance, aiding visually impaired individuals, and document image information retrieval using natural language queries. VQA requires effective integration of information from questions and images to generate accurate answers. Neural models for VQA have made remarkable progress on large-scale datasets, with a primary focus on resource-rich languages like English. To address this, we introduce the ViCLEVR dataset, a pioneering collection for evaluating various visual reasoning capabilities in Vietnamese while mitigating biases. The dataset comprises over 26,000 images and 30,000 question-answer pairs (QAs), each question annotated to specify the type of reasoning involved. Leveraging this dataset, we conduct a comprehensive analysis of contemporary visual reasoning systems, offering valuable insights into their strengths and limitations. Furthermore
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#35821;&#35328;&#29702;&#35299;&#27169;&#22411;&#30340;&#36718;&#24275;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#35780;&#20272;&#24403;&#21069;&#27169;&#22411;&#36136;&#37327;&#27979;&#37327;&#26041;&#27861;&#30340;&#36866;&#24403;&#24615;&#38382;&#39064;&#65292;&#26088;&#22312;&#35299;&#20915;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#31243;&#24230;&#30340;&#20105;&#35770;&#65292;&#24182;&#25506;&#35752;&#35821;&#35328;&#29702;&#35299;&#30340;&#22810;&#26041;&#38754;&#29616;&#35937;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#30340;&#36947;&#24503;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.18038</link><description>&lt;p&gt;
&#20851;&#20110;&#36890;&#29992;&#35821;&#35328;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
On General Language Understanding. (arXiv:2310.18038v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#35821;&#35328;&#29702;&#35299;&#27169;&#22411;&#30340;&#36718;&#24275;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#35780;&#20272;&#24403;&#21069;&#27169;&#22411;&#36136;&#37327;&#27979;&#37327;&#26041;&#27861;&#30340;&#36866;&#24403;&#24615;&#38382;&#39064;&#65292;&#26088;&#22312;&#35299;&#20915;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#31243;&#24230;&#30340;&#20105;&#35770;&#65292;&#24182;&#25506;&#35752;&#35821;&#35328;&#29702;&#35299;&#30340;&#22810;&#26041;&#38754;&#29616;&#35937;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#30340;&#36947;&#24503;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33258;&#31216;&#26159;&#19968;&#20010;&#20197;&#32463;&#39564;&#20026;&#23548;&#21521;&#30340;&#39046;&#22495;&#65292;&#20294;&#26368;&#36817;&#23427;&#20284;&#20046;&#21367;&#20837;&#20102;&#20851;&#20110;&#24847;&#20041;&#21644;&#27979;&#37327;&#38382;&#39064;&#30340;&#23454;&#36136;&#24615;&#20105;&#35770;&#65288;&#8220;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#29702;&#35299;&#35821;&#35328;&#65292;&#22914;&#26524;&#29702;&#35299;&#30340;&#35805;&#65292;&#31243;&#24230;&#22914;&#20309;&#65311;&#8221;&#65289;&#12290;&#36825;&#24182;&#19981;&#26159;&#20598;&#28982;&#30340;&#65306;&#22312;&#36825;&#37324;&#65292;&#23601;&#20687;&#22312;&#20219;&#20309;&#22320;&#26041;&#19968;&#26679;&#65292;&#35777;&#25454;&#24182;&#19981;&#33021;&#20805;&#20998;&#35828;&#26126;&#29702;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#27010;&#36848;&#20102;&#19968;&#20010;&#29702;&#35299;&#27169;&#22411;&#30340;&#36718;&#24275;&#65292;&#29992;&#20110;&#35780;&#20272;&#24403;&#21069;&#27169;&#22411;&#36136;&#37327;&#27979;&#37327;&#26041;&#27861;&#30340;&#36866;&#24403;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#20010;&#35266;&#28857;&#65306;A&#65289;&#19981;&#21516;&#30340;&#35821;&#35328;&#20351;&#29992;&#24773;&#26223;&#31867;&#22411;&#20855;&#26377;&#19981;&#21516;&#30340;&#29305;&#24449;&#65292;B&#65289;&#35821;&#35328;&#29702;&#35299;&#26159;&#19968;&#20010;&#22810;&#26041;&#38754;&#30340;&#29616;&#35937;&#65292;&#27719;&#38598;&#20102;&#20010;&#20307;&#20027;&#20041;&#21644;&#31038;&#20250;&#36807;&#31243;&#65292;C&#65289;&#29702;&#35299;&#25351;&#26631;&#30340;&#36873;&#25321;&#26631;&#24535;&#30528;&#22522;&#20934;&#27979;&#35797;&#30340;&#30028;&#38480;&#20197;&#21450;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#30340;&#36947;&#24503;&#24605;&#32771;&#30340;&#24320;&#22987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural Language Processing prides itself to be an empirically-minded, if not outright empiricist field, and yet lately it seems to get itself into essentialist debates on issues of meaning and measurement ("Do Large Language Models Understand Language, And If So, How Much?"). This is not by accident: Here, as everywhere, the evidence underspecifies the understanding. As a remedy, this paper sketches the outlines of a model of understanding, which can ground questions of the adequacy of current methods of measurement of model quality. The paper makes three claims: A) That different language use situation types have different characteristics, B) That language understanding is a multifaceted phenomenon, bringing together individualistic and social processes, and C) That the choice of Understanding Indicator marks the limits of benchmarking, and the beginnings of considerations of the ethics of NLP use.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;GPT-4&#21644;GPT-3.5&#22312;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#24494;&#35843;&#21518;&#30340;GPT-3.5&#22312;SemEval-2014&#20219;&#21153;4&#20013;&#21462;&#24471;&#20102;83.8&#30340;&#26368;&#20808;&#36827;F1&#20998;&#25968;&#65292;&#30456;&#27604;&#20110;InstructABSA&#25552;&#39640;&#20102;5.7%&#12290;&#20294;&#26159;&#65292;&#36825;&#38656;&#35201;1000&#20493;&#30340;&#27169;&#22411;&#21442;&#25968;&#22686;&#21152;&#20102;&#25512;&#29702;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2310.18025</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Large language models for aspect-based sentiment analysis. (arXiv:2310.18025v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18025
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;GPT-4&#21644;GPT-3.5&#22312;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#24494;&#35843;&#21518;&#30340;GPT-3.5&#22312;SemEval-2014&#20219;&#21153;4&#20013;&#21462;&#24471;&#20102;83.8&#30340;&#26368;&#20808;&#36827;F1&#20998;&#25968;&#65292;&#30456;&#27604;&#20110;InstructABSA&#25552;&#39640;&#20102;5.7%&#12290;&#20294;&#26159;&#65292;&#36825;&#38656;&#35201;1000&#20493;&#30340;&#27169;&#22411;&#21442;&#25968;&#22686;&#21152;&#20102;&#25512;&#29702;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20379;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#25991;&#26412;&#23436;&#25104;&#33021;&#21147;&#12290;&#20316;&#20026;&#36890;&#29992;&#27169;&#22411;&#65292;&#23427;&#20204;&#21487;&#20197;&#25285;&#20219;&#21508;&#31181;&#35282;&#33394;&#65292;&#21253;&#25324;&#26356;&#19987;&#38376;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;GPT-4&#21644;GPT-3.5&#22312;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#65288;ABSA&#65289;&#20219;&#21153;&#20013;&#30340;&#38646;-shot&#12289;&#23569;-shot&#21644;&#24494;&#35843;&#35774;&#32622;&#19979;&#30340;&#24615;&#33021;&#12290;&#24494;&#35843;&#21518;&#30340;GPT-3.5&#22312;SemEval-2014&#20219;&#21153;4&#30340;&#32852;&#21512;&#26041;&#38754;&#26415;&#35821;&#25552;&#21462;&#21644;&#26497;&#24615;&#20998;&#31867;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;83.8&#30340;&#26368;&#20808;&#36827;F1&#20998;&#25968;&#65292;&#30456;&#27604;&#20110;InstructABSA [Scaria&#31561;&#20154;&#65292;2023]&#25552;&#39640;&#20102;5.7&#65285;&#12290;&#28982;&#32780;&#65292;&#36825;&#26159;&#20197;1000&#20493;&#26356;&#22810;&#30340;&#27169;&#22411;&#21442;&#25968;&#21644;&#22240;&#27492;&#22686;&#21152;&#30340;&#25512;&#29702;&#25104;&#26412;&#20026;&#20195;&#20215;&#30340;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#19981;&#21516;&#27169;&#22411;&#30340;&#25104;&#26412;&#24615;&#33021;&#26435;&#34913;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#20856;&#22411;&#38169;&#35823;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;&#22312;&#38646;-shot&#21644;&#23569;-shot&#35774;&#32622;&#20013;&#65292;&#35814;&#32454;&#25552;&#31034;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#20294;&#23545;&#20110;&#24494;&#35843;&#27169;&#22411;&#26469;&#35828;&#24182;&#19981;&#26159;&#24517;&#35201;&#30340;&#12290;&#36825;&#20123;&#35777;&#25454;&#23545;&#20110;&#38754;&#20020;&#36873;&#25321;&#38382;&#39064;&#30340;&#23454;&#36341;&#32773;&#26159;&#30456;&#20851;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) offer unprecedented text completion capabilities. As general models, they can fulfill a wide range of roles, including those of more specialized models. We assess the performance of GPT-4 and GPT-3.5 in zero shot, few shot and fine-tuned settings on the aspect-based sentiment analysis (ABSA) task. Fine-tuned GPT-3.5 achieves a state-of-the-art F1 score of 83.8 on the joint aspect term extraction and polarity classification task of the SemEval-2014 Task 4, improving upon InstructABSA [@scaria_instructabsa_2023] by 5.7%. However, this comes at the price of 1000 times more model parameters and thus increased inference cost. We discuss the the cost-performance trade-offs of different models, and analyze the typical errors that they make. Our results also indicate that detailed prompts improve performance in zero-shot and few-shot settings but are not necessary for fine-tuned models. This evidence is relevant for practioners that are faced with the choice of pro
&lt;/p&gt;</description></item><item><title>SentMix-3L&#26159;&#19968;&#20010;&#29992;&#20110;&#24773;&#24863;&#20998;&#26512;&#30340;&#26032;&#39062;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#23391;&#21152;&#25289;&#35821;&#12289;&#33521;&#35821;&#21644;&#21360;&#22320;&#35821;&#20043;&#38388;&#30340;&#20195;&#30721;&#28151;&#21512;&#25968;&#25454;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;SentMix-3L&#19978;&#65292;&#20351;&#29992;GPT-3.5&#36827;&#34892;&#38646;-shot&#25552;&#31034;&#21487;&#20197;&#36229;&#36807;&#25152;&#26377;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.18023</link><description>&lt;p&gt;
SentMix-3L: &#29992;&#20110;&#24773;&#24863;&#20998;&#26512;&#30340;&#23391;&#21152;&#25289;&#35821;-&#33521;&#35821;-&#21360;&#22320;&#35821;&#28151;&#21512;&#20195;&#30721;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
SentMix-3L: A Bangla-English-Hindi Code-Mixed Dataset for Sentiment Analysis. (arXiv:2310.18023v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18023
&lt;/p&gt;
&lt;p&gt;
SentMix-3L&#26159;&#19968;&#20010;&#29992;&#20110;&#24773;&#24863;&#20998;&#26512;&#30340;&#26032;&#39062;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#23391;&#21152;&#25289;&#35821;&#12289;&#33521;&#35821;&#21644;&#21360;&#22320;&#35821;&#20043;&#38388;&#30340;&#20195;&#30721;&#28151;&#21512;&#25968;&#25454;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;SentMix-3L&#19978;&#65292;&#20351;&#29992;GPT-3.5&#36827;&#34892;&#38646;-shot&#25552;&#31034;&#21487;&#20197;&#36229;&#36807;&#25152;&#26377;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#28151;&#21512;&#26159;&#19968;&#31181;&#30740;&#31350;&#24456;&#28145;&#30340;&#35821;&#35328;&#29616;&#35937;&#65292;&#25351;&#30340;&#26159;&#22312;&#25991;&#26412;&#25110;&#35821;&#38899;&#20013;&#28151;&#21512;&#20351;&#29992;&#20004;&#31181;&#25110;&#26356;&#22810;&#35821;&#35328;&#12290;&#24050;&#32463;&#26500;&#24314;&#20102;&#20960;&#20010;&#26088;&#22312;&#35757;&#32451;&#20195;&#30721;&#28151;&#21512;&#35745;&#31639;&#27169;&#22411;&#30340;&#25968;&#25454;&#38598;&#12290;&#23613;&#31649;&#22810;&#35821;&#35328;&#30340;&#20195;&#30721;&#28151;&#21512;&#24456;&#24120;&#35265;&#65292;&#20294;&#22823;&#22810;&#25968;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#21482;&#21253;&#21547;&#20004;&#31181;&#35821;&#35328;&#30340;&#20195;&#30721;&#28151;&#21512;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;SentMix-3L&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#29992;&#20110;&#24773;&#24863;&#20998;&#26512;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#23391;&#21152;&#25289;&#35821;&#12289;&#33521;&#35821;&#21644;&#21360;&#22320;&#35821;&#20043;&#38388;&#30340;&#20195;&#30721;&#28151;&#21512;&#25968;&#25454;&#12290;&#25105;&#20204;&#20351;&#29992;SentMix-3L&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;GPT-3.5&#36827;&#34892;&#38646;-shot&#25552;&#31034;&#22312;SentMix-3L&#19978;&#20248;&#20110;&#25152;&#26377;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Code-mixing is a well-studied linguistic phenomenon when two or more languages are mixed in text or speech. Several datasets have been build with the goal of training computational models for code-mixing. Although it is very common to observe code-mixing with multiple languages, most datasets available contain code-mixed between only two languages. In this paper, we introduce SentMix-3L, a novel dataset for sentiment analysis containing code-mixed data between three languages Bangla, English, and Hindi. We carry out a comprehensive evaluation using SentMix-3L. We show that zero-shot prompting with GPT-3.5 outperforms all transformer-based models on SentMix-3L.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#31435;&#22330;&#35770;&#25991;&#25351;&#20986;NLP&#20219;&#21153;&#30340;&#32463;&#20856;&#35780;&#20272;&#23384;&#22312;&#25968;&#25454;&#27745;&#26579;&#30340;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#24403;&#19968;&#20010;LLM&#27169;&#22411;&#22312;&#22522;&#20934;&#27979;&#35797;&#30340;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#26102;&#12290;&#35813;&#38382;&#39064;&#23548;&#33268;&#27745;&#26579;&#27169;&#22411;&#22312;&#30446;&#26631;&#22522;&#20934;&#27979;&#35797;&#21644;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#34987;&#39640;&#20272;&#65292;&#21487;&#33021;&#20250;&#20135;&#29983;&#38169;&#35823;&#30340;&#31185;&#23398;&#32467;&#35770;&#12290;&#24314;&#35758;&#24320;&#21457;&#33258;&#21160;&#21644;&#21322;&#33258;&#21160;&#30340;&#27979;&#37327;&#26041;&#27861;&#26469;&#26816;&#27979;&#25968;&#25454;&#27745;&#26579;&#65292;&#24182;&#25552;&#37266;&#35780;&#23457;&#35770;&#25991;&#26102;&#35201;&#27880;&#24847;&#20855;&#26377;&#22949;&#21327;&#24615;&#32467;&#35770;&#30340;&#35770;&#25991;&#12290;</title><link>http://arxiv.org/abs/2310.18018</link><description>&lt;p&gt;
NLP&#35780;&#20272;&#36935;&#21040;&#40635;&#28902;&#65306;&#38656;&#35201;&#38024;&#23545;&#27599;&#20010;&#22522;&#20934;&#27979;&#35797;&#27979;&#37327;LLM&#25968;&#25454;&#27745;&#26579;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
NLP Evaluation in trouble: On the Need to Measure LLM Data Contamination for each Benchmark. (arXiv:2310.18018v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18018
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#31435;&#22330;&#35770;&#25991;&#25351;&#20986;NLP&#20219;&#21153;&#30340;&#32463;&#20856;&#35780;&#20272;&#23384;&#22312;&#25968;&#25454;&#27745;&#26579;&#30340;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#24403;&#19968;&#20010;LLM&#27169;&#22411;&#22312;&#22522;&#20934;&#27979;&#35797;&#30340;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#26102;&#12290;&#35813;&#38382;&#39064;&#23548;&#33268;&#27745;&#26579;&#27169;&#22411;&#22312;&#30446;&#26631;&#22522;&#20934;&#27979;&#35797;&#21644;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#34987;&#39640;&#20272;&#65292;&#21487;&#33021;&#20250;&#20135;&#29983;&#38169;&#35823;&#30340;&#31185;&#23398;&#32467;&#35770;&#12290;&#24314;&#35758;&#24320;&#21457;&#33258;&#21160;&#21644;&#21322;&#33258;&#21160;&#30340;&#27979;&#37327;&#26041;&#27861;&#26469;&#26816;&#27979;&#25968;&#25454;&#27745;&#26579;&#65292;&#24182;&#25552;&#37266;&#35780;&#23457;&#35770;&#25991;&#26102;&#35201;&#27880;&#24847;&#20855;&#26377;&#22949;&#21327;&#24615;&#32467;&#35770;&#30340;&#35770;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#31435;&#22330;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#20351;&#29992;&#24102;&#26377;&#27880;&#37322;&#30340;&#22522;&#20934;&#27979;&#35797;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#36827;&#34892;&#32463;&#20856;&#35780;&#20272;&#23384;&#22312;&#38382;&#39064;&#12290;&#26368;&#20005;&#37325;&#30340;&#25968;&#25454;&#27745;&#26579;&#21457;&#29983;&#22312;&#20351;&#29992;&#22522;&#20934;&#27979;&#35797;&#30340;&#27979;&#35797;&#38598;&#23545;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#21516;&#19968;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#35780;&#20272;&#30340;&#24773;&#20917;&#19979;&#12290;&#35813;&#38382;&#39064;&#30340;&#31243;&#24230;&#23578;&#19981;&#28165;&#26970;&#65292;&#22240;&#20026;&#24456;&#38590;&#30452;&#25509;&#34913;&#37327;&#12290;&#27745;&#26579;&#20250;&#23548;&#33268;&#22312;&#30446;&#26631;&#22522;&#20934;&#27979;&#35797;&#21644;&#30456;&#20851;&#20219;&#21153;&#20013;&#65292;&#21463;&#21040;&#27745;&#26579;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#34987;&#39640;&#20272;&#65292;&#19982;&#26410;&#21463;&#27745;&#26579;&#30340;&#23545;&#24212;&#27169;&#22411;&#30456;&#27604;&#12290;&#21518;&#26524;&#21487;&#33021;&#38750;&#24120;&#20005;&#37325;&#65292;&#20250;&#20986;&#29616;&#38169;&#35823;&#30340;&#31185;&#23398;&#32467;&#35770;&#34987;&#21457;&#34920;&#65292;&#32780;&#20854;&#20182;&#27491;&#30830;&#30340;&#32467;&#35770;&#34987;&#24573;&#35270;&#12290;&#26412;&#31435;&#22330;&#35770;&#25991;&#23450;&#20041;&#20102;&#19981;&#21516;&#32423;&#21035;&#30340;&#25968;&#25454;&#27745;&#26579;&#65292;&#24182;&#25552;&#20513;&#31038;&#21306;&#20849;&#21516;&#21162;&#21147;&#65292;&#21253;&#25324;&#24320;&#21457;&#33258;&#21160;&#21644;&#21322;&#33258;&#21160;&#30340;&#27979;&#37327;&#26041;&#27861;&#26469;&#26816;&#27979;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#26159;&#21542;&#26292;&#38706;&#32473;&#20102;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#26631;&#35760;&#20855;&#26377;&#22949;&#21327;&#24615;&#32467;&#35770;&#30340;&#35770;&#25991;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this position paper, we argue that the classical evaluation on Natural Language Processing (NLP) tasks using annotated benchmarks is in trouble. The worst kind of data contamination happens when a Large Language Model (LLM) is trained on the test split of a benchmark, and then evaluated in the same benchmark. The extent of the problem is unknown, as it is not straightforward to measure. Contamination causes an overestimation of the performance of a contaminated model in a target benchmark and associated task with respect to their non-contaminated counterparts. The consequences can be very harmful, with wrong scientific conclusions being published while other correct ones are discarded. This position paper defines different levels of data contamination and argues for a community effort, including the development of automatic and semi-automatic measures to detect when data from a benchmark was exposed to a model, and suggestions for flagging papers with conclusions that are compromise
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#35282;&#33394;&#25198;&#28436;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#20010;&#24615;&#29305;&#28857;&#65292;&#24182;&#21457;&#29616;&#22522;&#20110;LLMs&#30340;&#29616;&#20195;&#35282;&#33394;&#25198;&#28436;&#32842;&#22825;&#26426;&#22120;&#20154;&#33021;&#22815;&#26377;&#25928;&#22320;&#25551;&#32472;&#20986;&#30456;&#24212;&#35282;&#33394;&#30340;&#20010;&#24615;&#29305;&#28857;&#65292;&#19982;&#20154;&#31867;&#24863;&#30693;&#30340;&#19968;&#33268;&#24615;&#36798;&#21040;82.8%&#12290;</title><link>http://arxiv.org/abs/2310.17976</link><description>&lt;p&gt;
&#35282;&#33394;&#25198;&#28436;&#32842;&#22825;&#26426;&#22120;&#20154;&#33021;&#22815;&#25429;&#25417;&#35282;&#33394;&#20010;&#24615;&#21527;&#65311;&#35780;&#20272;&#35282;&#33394;&#25198;&#28436;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#20010;&#24615;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Does Role-Playing Chatbots Capture the Character Personalities? Assessing Personality Traits for Role-Playing Chatbots. (arXiv:2310.17976v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#35282;&#33394;&#25198;&#28436;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#20010;&#24615;&#29305;&#28857;&#65292;&#24182;&#21457;&#29616;&#22522;&#20110;LLMs&#30340;&#29616;&#20195;&#35282;&#33394;&#25198;&#28436;&#32842;&#22825;&#26426;&#22120;&#20154;&#33021;&#22815;&#26377;&#25928;&#22320;&#25551;&#32472;&#20986;&#30456;&#24212;&#35282;&#33394;&#30340;&#20010;&#24615;&#29305;&#28857;&#65292;&#19982;&#20154;&#31867;&#24863;&#30693;&#30340;&#19968;&#33268;&#24615;&#36798;&#21040;82.8%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#24443;&#24213;&#25913;&#21464;&#20102;&#26032;&#22411;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#22312;&#25171;&#36896;&#20855;&#26377;&#29420;&#29305;&#20154;&#29289;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#26041;&#38754;&#12290;&#37492;&#20110;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;"&#21050;&#28608;-&#21709;&#24212;"&#24615;&#36136;&#65292;&#26412;&#25991;&#25581;&#31034;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#24320;&#25918;&#24335;&#37319;&#35775;&#24335;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#35282;&#33394;&#25198;&#28436;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#20010;&#24615;&#29305;&#28857;&#65292;&#20174;&#32780;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;&#20854;&#20869;&#22312;&#20010;&#24615;&#12290;&#25105;&#20204;&#23545;ChatHaruhi&#22270;&#20070;&#39302;&#21019;&#24314;&#30340;32&#20010;&#35282;&#33394;&#25198;&#28436;&#32842;&#22825;&#26426;&#22120;&#20154;&#36827;&#34892;&#20102;&#22823;&#20116;&#20154;&#26684;&#21644;MBTI&#32500;&#24230;&#19978;&#30340;&#20010;&#24615;&#35780;&#20272;&#65292;&#24182;&#27979;&#37327;&#23427;&#20204;&#19982;&#20154;&#31867;&#24863;&#30693;&#30340;&#19968;&#33268;&#24615;&#12290;&#35780;&#20272;&#32467;&#26524;&#24378;&#35843;&#65292;&#22522;&#20110;LLMs&#30340;&#29616;&#20195;&#35282;&#33394;&#25198;&#28436;&#32842;&#22825;&#26426;&#22120;&#20154;&#33021;&#22815;&#26377;&#25928;&#22320;&#25551;&#32472;&#20986;&#30456;&#24212;&#35282;&#33394;&#30340;&#20010;&#24615;&#29305;&#28857;&#65292;&#19982;&#20154;&#31867;&#24863;&#30693;&#30340;&#19968;&#33268;&#24615;&#36798;&#21040;82.8%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#22609;&#36896;&#32842;&#22825;&#26426;&#22120;&#20154;&#20010;&#24615;&#30340;&#28508;&#22312;&#31574;&#30053;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#20026;&#35282;&#33394;&#25198;&#28436;&#32842;&#22825;&#26426;&#22120;&#20154;&#30740;&#31350;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of large-scale pretrained language models has revolutionized the capabilities of new AI application, especially in the realm of crafting chatbots with distinct personas. Given the "stimulus-response" nature of chatbots, this paper unveils an innovative open-ended interview-style approach for personality assessment on role-playing chatbots, which offers a richer comprehension of their intrinsic personalities. We conduct personality assessments on 32 role-playing chatbots created by the ChatHaruhi library, across both the Big Five and MBTI dimensions, and measure their alignment with human perception. Evaluation results underscore that modern role-playing chatbots based on LLMs can effectively portray personality traits of corresponding characters, with an alignment rate of 82.8% compared with human-perceived personalities. Besides, we also suggest potential strategies for shaping chatbots' personalities. Hence, this paper serves as a cornerstone study for role-playing chat
&lt;/p&gt;</description></item><item><title>Qilin-Med-VL&#26159;&#38754;&#21521;&#26222;&#36941;&#21307;&#30103;&#20445;&#20581;&#30340;&#20013;&#22269;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#32467;&#21512;&#20102;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;Transformer&#21644;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#35838;&#31243;&#35757;&#32451;&#36807;&#31243;&#25552;&#39640;&#20102;&#29983;&#25104;&#21307;&#30103;&#26631;&#39064;&#21644;&#22238;&#31572;&#22797;&#26434;&#21307;&#30103;&#26597;&#35810;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;100&#19975;&#20010;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#25968;&#25454;&#38598;ChiMed-VL&#12290;</title><link>http://arxiv.org/abs/2310.17956</link><description>&lt;p&gt;
&#38754;&#21521;&#26222;&#36941;&#21307;&#30103;&#20445;&#20581;&#30340;&#20013;&#22269;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;Qilin-Med-VL
&lt;/p&gt;
&lt;p&gt;
Qilin-Med-VL: Towards Chinese Large Vision-Language Model for General Healthcare. (arXiv:2310.17956v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17956
&lt;/p&gt;
&lt;p&gt;
Qilin-Med-VL&#26159;&#38754;&#21521;&#26222;&#36941;&#21307;&#30103;&#20445;&#20581;&#30340;&#20013;&#22269;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#32467;&#21512;&#20102;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;Transformer&#21644;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#35838;&#31243;&#35757;&#32451;&#36807;&#31243;&#25552;&#39640;&#20102;&#29983;&#25104;&#21307;&#30103;&#26631;&#39064;&#21644;&#22238;&#31572;&#22797;&#26434;&#21307;&#30103;&#26597;&#35810;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;100&#19975;&#20010;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#25968;&#25454;&#38598;ChiMed-VL&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#29702;&#35299;&#22797;&#26434;&#30340;&#21307;&#30103;&#20445;&#20581;&#21644;&#29983;&#29289;&#21307;&#23398;&#20027;&#39064;&#26041;&#38754;&#21462;&#24471;&#20102;&#26032;&#30340;&#31361;&#30772;&#12290;&#28982;&#32780;&#65292;&#38500;&#20102;&#33521;&#35821;&#20043;&#22806;&#65292;&#20854;&#20182;&#35821;&#35328;&#30340;&#27169;&#22411;&#20197;&#21450;&#33021;&#22815;&#35299;&#37322;&#22810;&#27169;&#24577;&#36755;&#20837;&#30340;&#27169;&#22411;&#30456;&#23545;&#36739;&#23569;&#65292;&#32780;&#36825;&#23545;&#20110;&#20840;&#29699;&#21307;&#30103;&#20445;&#20581;&#30340;&#21487;&#35775;&#38382;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;Qilin-Med-VL&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#35774;&#35745;&#29992;&#20110;&#25972;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;&#25968;&#25454;&#20998;&#26512;&#30340;&#20013;&#22269;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#12290;Qilin-Med-VL&#23558;&#32463;&#36807;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;Transformer&#65288;ViT&#65289;&#19982;&#22522;&#30784;LLM&#30456;&#32467;&#21512;&#12290;&#23427;&#32463;&#21382;&#20102;&#19968;&#20010;&#28145;&#20837;&#30340;&#20004;&#38454;&#27573;&#35838;&#31243;&#35757;&#32451;&#36807;&#31243;&#65292;&#20854;&#20013;&#21253;&#25324;&#29305;&#24449;&#23545;&#40784;&#21644;&#25351;&#23548;&#35843;&#20248;&#12290;&#36825;&#31181;&#26041;&#27861;&#22686;&#24378;&#20102;&#27169;&#22411;&#29983;&#25104;&#21307;&#30103;&#26631;&#39064;&#21644;&#22238;&#31572;&#22797;&#26434;&#21307;&#30103;&#26597;&#35810;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;ChiMed-VL&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;100&#19975;&#20010;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#65292;&#21487;&#20197;&#20351;&#29992;&#21508;&#31181;&#31867;&#22411;&#30340;&#22270;&#20687;&#36827;&#34892;&#35814;&#32454;&#21644;&#20840;&#38754;&#30340;&#21307;&#23398;&#25968;&#25454;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have introduced a new era of proficiency in comprehending complex healthcare and biomedical topics. However, there is a noticeable lack of models in languages other than English and models that can interpret multi-modal input, which is crucial for global healthcare accessibility. In response, this study introduces Qilin-Med-VL, the first Chinese large vision-language model designed to integrate the analysis of textual and visual data. Qilin-Med-VL combines a pre-trained Vision Transformer (ViT) with a foundational LLM. It undergoes a thorough two-stage curriculum training process that includes feature alignment and instruction tuning. This method enhances the model's ability to generate medical captions and answer complex medical queries. We also release ChiMed-VL, a dataset consisting of more than 1M image-text pairs. This dataset has been carefully curated to enable detailed and comprehensive interpretation of medical data using various types of images.
&lt;/p&gt;</description></item><item><title>Whisper-MCE&#26159;&#20351;&#29992;&#33258;&#24049;&#25910;&#38598;&#30340;&#28151;&#21512;&#31908;&#35821;&#21644;&#33521;&#35821;&#38899;&#39057;&#25968;&#25454;&#38598;&#65288;MCE&#65289;&#36827;&#34892;&#35757;&#32451;&#30340;Whisper&#27169;&#22411;&#24494;&#35843;&#65292;&#30456;&#36739;&#20110;&#22522;&#20934;&#27169;&#22411;&#65292;&#20854;&#22312;&#20934;&#30830;&#25429;&#25417;&#21407;&#22987;&#38899;&#39057;&#20869;&#23481;&#12289;&#25552;&#39640;&#35782;&#21035;&#20934;&#30830;&#24615;&#21644;&#21152;&#24555;&#35782;&#21035;&#36895;&#24230;&#26041;&#38754;&#20855;&#26377;&#26356;&#20248;&#36234;&#30340;&#33021;&#21147;&#65292;&#23588;&#20854;&#22312;&#28151;&#21512;&#35821;&#35328;&#35782;&#21035;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2310.17953</link><description>&lt;p&gt;
Whisper-MCE: &#38024;&#23545;&#28151;&#21512;&#35821;&#35328;&#23454;&#29616;&#26356;&#22909;&#24615;&#33021;&#30340;Whisper&#27169;&#22411;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Whisper-MCE: Whisper Model Finetuned for Better Performance with Mixed Languages. (arXiv:2310.17953v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17953
&lt;/p&gt;
&lt;p&gt;
Whisper-MCE&#26159;&#20351;&#29992;&#33258;&#24049;&#25910;&#38598;&#30340;&#28151;&#21512;&#31908;&#35821;&#21644;&#33521;&#35821;&#38899;&#39057;&#25968;&#25454;&#38598;&#65288;MCE&#65289;&#36827;&#34892;&#35757;&#32451;&#30340;Whisper&#27169;&#22411;&#24494;&#35843;&#65292;&#30456;&#36739;&#20110;&#22522;&#20934;&#27169;&#22411;&#65292;&#20854;&#22312;&#20934;&#30830;&#25429;&#25417;&#21407;&#22987;&#38899;&#39057;&#20869;&#23481;&#12289;&#25552;&#39640;&#35782;&#21035;&#20934;&#30830;&#24615;&#21644;&#21152;&#24555;&#35782;&#21035;&#36895;&#24230;&#26041;&#38754;&#20855;&#26377;&#26356;&#20248;&#36234;&#30340;&#33021;&#21147;&#65292;&#23588;&#20854;&#22312;&#28151;&#21512;&#35821;&#35328;&#35782;&#21035;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;Whisper&#22312;&#33521;&#35821;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#39046;&#22495;&#24050;&#32463;&#25509;&#36817;&#20110;&#20154;&#31867;&#32423;&#21035;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#20294;&#22312;&#36739;&#23567;&#35821;&#31181;&#21644;&#28151;&#21512;&#35821;&#35328;&#30340;&#35821;&#38899;&#35782;&#21035;&#20013;&#65292;&#20173;&#28982;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#32454;&#35843;&#30340;Whisper&#27169;&#22411;Whisper-MCE&#30340;&#20196;&#20154;&#30633;&#30446;&#30340;&#32467;&#26524;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#20102;&#25105;&#20204;&#33258;&#24049;&#25910;&#38598;&#30340;&#28151;&#21512;&#31908;&#35821;&#21644;&#33521;&#35821;&#38899;&#39057;&#25968;&#25454;&#38598;&#65288;MCE&#65289;&#36827;&#34892;&#35757;&#32451;&#12290;&#21516;&#26102;&#65292;&#32771;&#34385;&#21040;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#22312;&#36739;&#23567;&#35821;&#31181;&#21644;&#28151;&#21512;&#35821;&#35328;&#29615;&#22659;&#20013;&#35780;&#20272;&#20854;&#26377;&#25928;&#24615;&#26102;&#23384;&#22312;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35780;&#20272;&#26426;&#21046;&#12290;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;&#27169;&#22411;&#19982;&#22522;&#20934;&#30340;whisper-large-v2&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#20934;&#30830;&#25429;&#25417;&#21407;&#22987;&#38899;&#39057;&#20869;&#23481;&#30340;&#33021;&#21147;&#26356;&#24378;&#12289;&#35782;&#21035;&#20934;&#30830;&#24615;&#26356;&#39640;&#12289;&#35782;&#21035;&#36895;&#24230;&#26356;&#24555;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#35782;&#21035;&#28151;&#21512;&#35821;&#35328;&#30340;&#29305;&#23450;&#20219;&#21153;&#20013;&#32988;&#36807;&#20854;&#20182;&#29616;&#26377;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently Whisper has approached human-level robustness and accuracy in English automatic speech recognition (ASR), while in minor language and mixed language speech recognition, there remains a compelling need for further improvement. In this work, we present the impressive results of Whisper-MCE, our finetuned Whisper model, which was trained using our self-collected dataset, Mixed Cantonese and English audio dataset (MCE). Meanwhile, considering word error rate (WER) poses challenges when it comes to evaluating its effectiveness in minor language and mixed-language contexts, we present a novel rating mechanism. By comparing our model to the baseline whisper-large-v2 model, we demonstrate its superior ability to accurately capture the content of the original audio, achieve higher recognition accuracy, and exhibit faster recognition speed. Notably, our model outperforms other existing models in the specific task of recognizing mixed language.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#29255;&#27573;&#21040;&#29255;&#27573;&#26694;&#26550; (Seg2Seg) &#29992;&#20110;&#21516;&#26102;&#24207;&#21015;&#29983;&#25104;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#21644;&#32479;&#19968;&#30340;&#26041;&#24335;&#23398;&#20064;&#28304;&#24207;&#21015;&#21644;&#30446;&#26631;&#24207;&#21015;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#23454;&#29616;&#39640;&#36136;&#37327;&#29983;&#25104;&#21644;&#20302;&#24310;&#36831;&#12290;</title><link>http://arxiv.org/abs/2310.17940</link><description>&lt;p&gt;
&#32479;&#19968;&#30340;&#29255;&#27573;&#21040;&#29255;&#27573;&#26694;&#26550;&#29992;&#20110;&#21516;&#26102;&#24207;&#21015;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Unified Segment-to-Segment Framework for Simultaneous Sequence Generation. (arXiv:2310.17940v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17940
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#29255;&#27573;&#21040;&#29255;&#27573;&#26694;&#26550; (Seg2Seg) &#29992;&#20110;&#21516;&#26102;&#24207;&#21015;&#29983;&#25104;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#21644;&#32479;&#19968;&#30340;&#26041;&#24335;&#23398;&#20064;&#28304;&#24207;&#21015;&#21644;&#30446;&#26631;&#24207;&#21015;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#23454;&#29616;&#39640;&#36136;&#37327;&#29983;&#25104;&#21644;&#20302;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#26102;&#24207;&#21015;&#29983;&#25104;&#26159;&#23454;&#26102;&#22330;&#26223;&#30340;&#20851;&#38190;&#20219;&#21153;&#65292;&#27604;&#22914;&#27969;&#24335;&#35821;&#38899;&#35782;&#21035;&#12289;&#21516;&#26102;&#26426;&#22120;&#32763;&#35793;&#21644;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;&#65292;&#20854;&#20013;&#30446;&#26631;&#24207;&#21015;&#22312;&#25509;&#25910;&#28304;&#24207;&#21015;&#30340;&#21516;&#26102;&#29983;&#25104;&#12290;&#23454;&#29616;&#39640;&#36136;&#37327;&#29983;&#25104;&#21644;&#20302;&#24310;&#36831;&#30340;&#20851;&#38190;&#22312;&#20110;&#30830;&#23450;&#29983;&#25104;&#30340;&#26368;&#20339;&#26102;&#26426;&#65292;&#36890;&#36807;&#23398;&#20064;&#28304;&#24207;&#21015;&#21644;&#30446;&#26631;&#24207;&#21015;&#20043;&#38388;&#30340;&#26144;&#23556;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#20381;&#36182;&#20110;&#29305;&#23450;&#20219;&#21153;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#38480;&#21046;&#20102;&#27169;&#22411;&#23545;&#28304;-&#30446;&#26631;&#26144;&#23556;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#33021;&#21147;&#65292;&#38459;&#30861;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#22312;&#21508;&#31181;&#21516;&#26102;&#20219;&#21153;&#20013;&#30340;&#25506;&#32034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#29255;&#27573;&#21040;&#29255;&#27573;&#26694;&#26550; (Seg2Seg) &#29992;&#20110;&#21516;&#26102;&#24207;&#21015;&#29983;&#25104;&#65292;&#20197;&#33258;&#36866;&#24212;&#21644;&#32479;&#19968;&#30340;&#26041;&#24335;&#23398;&#20064;&#26144;&#23556;&#12290;&#22312;&#21516;&#26102;&#29983;&#25104;&#30340;&#36807;&#31243;&#20013;&#65292;&#27169;&#22411;&#22312;&#31561;&#24453;&#28304;&#29255;&#27573;&#21644;&#29983;&#25104;&#30446;&#26631;&#29255;&#27573;&#20043;&#38388;&#20132;&#26367;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simultaneous sequence generation is a pivotal task for real-time scenarios, such as streaming speech recognition, simultaneous machine translation and simultaneous speech translation, where the target sequence is generated while receiving the source sequence. The crux of achieving high-quality generation with low latency lies in identifying the optimal moments for generating, accomplished by learning a mapping between the source and target sequences. However, existing methods often rely on task-specific heuristics for different sequence types, limiting the model's capacity to adaptively learn the source-target mapping and hindering the exploration of multi-task learning for various simultaneous tasks. In this paper, we propose a unified segment-to-segment framework (Seg2Seg) for simultaneous sequence generation, which learns the mapping in an adaptive and unified manner. During the process of simultaneous generation, the model alternates between waiting for a source segment and generat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35748;&#20026;Transformers&#26412;&#36136;&#19978;&#26159;&#22270;&#21040;&#22270;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#21147;&#26435;&#37325;&#31561;&#20215;&#20110;&#22270;&#20013;&#30340;&#36793;&#65292;&#24182;&#20351;&#29992;&#22270;&#21040;&#22270;Transformer&#26550;&#26500;&#32467;&#21512;&#26174;&#24335;&#22270;&#21644;&#28508;&#22312;&#22270;&#36827;&#34892;&#38750;&#33258;&#22238;&#24402;&#22270;&#39044;&#27979;&#65292;&#23454;&#29616;&#20102;&#22312;&#24314;&#27169;&#21508;&#31181;&#35821;&#35328;&#32467;&#26500;&#26041;&#38754;&#30340;&#26368;&#20808;&#36827;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17936</link><description>&lt;p&gt;
Transformers&#20316;&#20026;&#22270;&#21040;&#22270;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Transformers as Graph-to-Graph Models. (arXiv:2310.17936v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17936
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35748;&#20026;Transformers&#26412;&#36136;&#19978;&#26159;&#22270;&#21040;&#22270;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#21147;&#26435;&#37325;&#31561;&#20215;&#20110;&#22270;&#20013;&#30340;&#36793;&#65292;&#24182;&#20351;&#29992;&#22270;&#21040;&#22270;Transformer&#26550;&#26500;&#32467;&#21512;&#26174;&#24335;&#22270;&#21644;&#28508;&#22312;&#22270;&#36827;&#34892;&#38750;&#33258;&#22238;&#24402;&#22270;&#39044;&#27979;&#65292;&#23454;&#29616;&#20102;&#22312;&#24314;&#27169;&#21508;&#31181;&#35821;&#35328;&#32467;&#26500;&#26041;&#38754;&#30340;&#26368;&#20808;&#36827;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35748;&#20026;Transformers&#26412;&#36136;&#19978;&#26159;&#22270;&#21040;&#22270;&#27169;&#22411;&#65292;&#32780;&#24207;&#21015;&#21482;&#26159;&#19968;&#31181;&#29305;&#27530;&#24773;&#20917;&#12290;&#27880;&#24847;&#21147;&#26435;&#37325;&#22312;&#21151;&#33021;&#19978;&#31561;&#20215;&#20110;&#22270;&#20013;&#30340;&#36793;&#12290;&#25105;&#20204;&#30340;&#22270;&#21040;&#22270;Transformer&#26550;&#26500;&#23558;&#36825;&#31181;&#33021;&#21147;&#26126;&#30830;&#22320;&#20307;&#29616;&#20986;&#26469;&#65292;&#36890;&#36807;&#23558;&#22270;&#30340;&#36793;&#36755;&#20837;&#21040;&#27880;&#24847;&#21147;&#26435;&#37325;&#35745;&#31639;&#20013;&#65292;&#24182;&#20351;&#29992;&#31867;&#20284;&#27880;&#24847;&#21147;&#30340;&#20989;&#25968;&#26469;&#39044;&#27979;&#22270;&#30340;&#36793;&#65292;&#20174;&#32780;&#23558;&#26174;&#24335;&#22270;&#38598;&#25104;&#21040;&#39044;&#35757;&#32451;Transformers&#23398;&#20064;&#30340;&#28508;&#22312;&#22270;&#20013;&#12290;&#28155;&#21152;&#36845;&#20195;&#22270;&#32454;&#21270;&#21487;&#20197;&#20026;&#36755;&#20837;&#12289;&#36755;&#20986;&#21644;&#28508;&#22312;&#22270;&#25552;&#20379;&#32852;&#21512;&#23884;&#20837;&#65292;&#20351;&#24471;&#38750;&#33258;&#22238;&#24402;&#22270;&#39044;&#27979;&#21487;&#20197;&#20248;&#21270;&#23436;&#25972;&#30340;&#22270;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#19987;&#38376;&#30340;&#31649;&#36947;&#25110;&#35299;&#30721;&#31574;&#30053;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26550;&#26500;&#22312;&#24314;&#27169;&#21508;&#31181;&#35821;&#35328;&#32467;&#26500;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19982;&#39044;&#35757;&#32451;&#23398;&#20064;&#30340;&#28508;&#22312;&#35821;&#35328;&#34920;&#31034;&#38750;&#24120;&#26377;&#25928;&#22320;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
We argue that Transformers are essentially graph-to-graph models, with sequences just being a special case. Attention weights are functionally equivalent to graph edges. Our Graph-to-Graph Transformer architecture makes this ability explicit, by inputting graph edges into the attention weight computations and predicting graph edges with attention-like functions, thereby integrating explicit graphs into the latent graphs learned by pretrained Transformers. Adding iterative graph refinement provides a joint embedding of input, output, and latent graphs, allowing non-autoregressive graph prediction to optimise the complete graph without any bespoke pipeline or decoding strategy. Empirical results show that this architecture achieves state-of-the-art accuracies for modelling a variety of linguistic structures, integrating very effectively with the latent linguistic representations learned by pretraining.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;SOUL&#65292;&#26088;&#22312;&#36890;&#36807;&#35780;&#35770;&#29702;&#35299;&#21644;&#35299;&#37322;&#29983;&#25104;&#20004;&#20010;&#23376;&#20219;&#21153;&#26469;&#35780;&#20272;&#35821;&#35328;&#30340;&#24773;&#24863;&#21644;&#35266;&#28857;&#29702;&#35299;&#12290;&#35813;&#20219;&#21153;&#22312;&#23567;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#37117;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#24615;&#33021;&#24046;&#36317;&#21487;&#39640;&#36798;27%&#12290;</title><link>http://arxiv.org/abs/2310.17924</link><description>&lt;p&gt;
SOUL: &#35821;&#35328;&#30340;&#24773;&#24863;&#21644;&#35266;&#28857;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
SOUL: Towards Sentiment and Opinion Understanding of Language. (arXiv:2310.17924v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17924
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;SOUL&#65292;&#26088;&#22312;&#36890;&#36807;&#35780;&#35770;&#29702;&#35299;&#21644;&#35299;&#37322;&#29983;&#25104;&#20004;&#20010;&#23376;&#20219;&#21153;&#26469;&#35780;&#20272;&#35821;&#35328;&#30340;&#24773;&#24863;&#21644;&#35266;&#28857;&#29702;&#35299;&#12290;&#35813;&#20219;&#21153;&#22312;&#23567;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#37117;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#24615;&#33021;&#24046;&#36317;&#21487;&#39640;&#36798;27%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#20998;&#26512;&#26159;&#19968;&#20010;&#25104;&#29087;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#24773;&#24863;&#26497;&#24615;&#20998;&#31867;&#26159;&#20854;&#20013;&#26368;&#21463;&#27426;&#36814;&#21644;&#20195;&#34920;&#24615;&#30340;&#20219;&#21153;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#26080;&#27861;&#25429;&#25417;&#21040;&#24773;&#24863;&#20998;&#26512;&#30340;&#26356;&#24191;&#27867;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65292;&#31216;&#20026;&#35821;&#35328;&#30340;&#24773;&#24863;&#21644;&#35266;&#28857;&#29702;&#35299;&#65288;SOUL&#65289;&#12290;SOUL&#36890;&#36807;&#20004;&#20010;&#23376;&#20219;&#21153;&#26469;&#35780;&#20272;&#24773;&#24863;&#29702;&#35299;&#65306;&#35780;&#35770;&#29702;&#35299;&#65288;RC&#65289;&#21644;&#35299;&#37322;&#29983;&#25104;&#65288;JG&#65289;&#12290;RC&#26088;&#22312;&#36890;&#36807;&#35780;&#35770;&#25991;&#26412;&#39564;&#35777;&#20851;&#20110;&#20027;&#35266;&#20449;&#24687;&#30340;&#38472;&#36848;&#65292;&#32780;JG&#35201;&#27714;&#27169;&#22411;&#20026;&#20854;&#24773;&#24863;&#39044;&#27979;&#25552;&#20379;&#35299;&#37322;&#12290;&#20026;&#20102;&#36827;&#34892;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#26631;&#27880;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#26469;&#33258;3638&#26465;&#35780;&#35770;&#30340;15028&#20010;&#38472;&#36848;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SOUL&#23545;&#23567;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35828;&#37117;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#24615;&#33021;&#24046;&#36317;&#21487;&#39640;&#36798;27%&#12290;
&lt;/p&gt;
&lt;p&gt;
Sentiment analysis is a well-established natural language processing task, with sentiment polarity classification being one of its most popular and representative tasks. However, despite the success of pre-trained language models in this area, they often fall short of capturing the broader complexities of sentiment analysis. To address this issue, we propose a new task called Sentiment and Opinion Understanding of Language (SOUL). SOUL aims to evaluate sentiment understanding through two subtasks: Review Comprehension (RC) and Justification Generation (JG). RC seeks to validate statements that focus on subjective information based on a review text, while JG requires models to provide explanations for their sentiment predictions. To enable comprehensive evaluation, we annotate a new dataset comprising 15,028 statements from 3,638 reviews. Experimental results indicate that SOUL is a challenging task for both small and large language models, with a performance gap of up to 27% when compa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#25105;&#26816;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#21028;&#26029;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#26080;&#27861;&#22238;&#31572;&#30340;&#38382;&#39064;&#65292;&#20197;&#36991;&#20813;&#29983;&#25104;&#38750;&#20107;&#23454;&#24615;&#30340;&#22238;&#31572;&#12290;&#36890;&#36807;&#22810;&#26679;&#21270;&#38382;&#39064;&#30340;&#25991;&#26412;&#34920;&#36798;&#65292;&#25910;&#38598;&#31572;&#26696;&#65292;&#24182;&#26816;&#26597;&#29983;&#25104;&#30340;&#31572;&#26696;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#21487;&#33021;&#29983;&#25104;&#34394;&#20551;&#22238;&#31572;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21482;&#38656;&#35201;&#21033;&#29992;LLMs&#33258;&#36523;&#65292;&#26080;&#38656;&#20854;&#20182;&#22806;&#37096;&#36164;&#28304;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;Vicuna&#12289;ChatGPT&#21644;GPT-4&#31561;&#26368;&#26032;&#21457;&#24067;&#30340;LLMs&#19978;&#24471;&#21040;&#20102;&#26377;&#25928;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.17918</link><description>&lt;p&gt;
&#30693;&#36947;LLMs&#19981;&#30693;&#36947;&#20160;&#20040;&#65306;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#33258;&#25105;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Knowing What LLMs DO NOT Know: A Simple Yet Effective Self-Detection Method. (arXiv:2310.17918v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#25105;&#26816;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#21028;&#26029;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#26080;&#27861;&#22238;&#31572;&#30340;&#38382;&#39064;&#65292;&#20197;&#36991;&#20813;&#29983;&#25104;&#38750;&#20107;&#23454;&#24615;&#30340;&#22238;&#31572;&#12290;&#36890;&#36807;&#22810;&#26679;&#21270;&#38382;&#39064;&#30340;&#25991;&#26412;&#34920;&#36798;&#65292;&#25910;&#38598;&#31572;&#26696;&#65292;&#24182;&#26816;&#26597;&#29983;&#25104;&#30340;&#31572;&#26696;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#21487;&#33021;&#29983;&#25104;&#34394;&#20551;&#22238;&#31572;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21482;&#38656;&#35201;&#21033;&#29992;LLMs&#33258;&#36523;&#65292;&#26080;&#38656;&#20854;&#20182;&#22806;&#37096;&#36164;&#28304;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;Vicuna&#12289;ChatGPT&#21644;GPT-4&#31561;&#26368;&#26032;&#21457;&#24067;&#30340;LLMs&#19978;&#24471;&#21040;&#20102;&#26377;&#25928;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#25991;&#29486;&#25581;&#31034;&#20102;LLMs&#20250;&#20598;&#23572;&#29983;&#25104;&#38750;&#20107;&#23454;&#24615;&#30340;&#22238;&#31572;&#65292;&#36825;&#24433;&#21709;&#20102;&#23427;&#20204;&#36827;&#19968;&#27493;&#21033;&#29992;&#30340;&#21487;&#38752;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#25105;&#26816;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;LLMs&#19981;&#30693;&#36947;&#30340;&#38382;&#39064;&#65292;&#20197;&#36991;&#20813;&#29983;&#25104;&#38750;&#20107;&#23454;&#24615;&#30340;&#32467;&#26524;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#32473;&#23450;&#38382;&#39064;&#30340;&#25991;&#26412;&#34920;&#36798;&#22810;&#26679;&#21270;&#65292;&#24182;&#25910;&#38598;&#30456;&#24212;&#30340;&#31572;&#26696;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26816;&#26597;&#29983;&#25104;&#30340;&#31572;&#26696;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20197;&#35782;&#21035;&#27169;&#22411;&#21487;&#33021;&#29983;&#25104;&#34394;&#20551;&#22238;&#31572;&#30340;&#38382;&#39064;&#12290;&#25152;&#26377;&#20197;&#19978;&#27493;&#39588;&#37117;&#21487;&#20197;&#36890;&#36807;&#25552;&#31034;LLMs&#33258;&#36523;&#26469;&#23436;&#25104;&#65292;&#32780;&#26080;&#38656;&#21442;&#32771;&#20219;&#20309;&#20854;&#20182;&#22806;&#37096;&#36164;&#28304;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#26368;&#36817;&#21457;&#24067;&#30340;LLMs&#65288;&#22914;Vicuna&#12289;ChatGPT&#21644;GPT-4&#65289;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown great potential in Natural Language Processing (NLP) tasks. However, recent literature reveals that LLMs generate nonfactual responses intermittently, which impedes the LLMs' reliability for further utilization. In this paper, we propose a novel self-detection method to detect which questions that a LLM does not know that are prone to generate nonfactual results. Specifically, we first diversify the textual expressions for a given question and collect the corresponding answers. Then we examine the divergencies between the generated answers to identify the questions that the model may generate falsehoods. All of the above steps can be accomplished by prompting the LLMs themselves without referring to any other external resources. We conduct comprehensive experiments and demonstrate the effectiveness of our method on recently released LLMs, e.g., Vicuna, ChatGPT, and GPT-4.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;3D&#24863;&#30693;VQA&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#25512;&#21160;VQA&#27169;&#22411;&#23545;&#20110;&#35270;&#35273;&#22330;&#26223;&#30340;3D&#32467;&#26500;&#36827;&#34892;&#32452;&#21512;&#25512;&#29702;&#12290;&#20316;&#32773;&#20174;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#35282;&#24230;&#20986;&#21457;&#65292;&#20998;&#21035;&#24341;&#20837;&#20102;Super-CLEVR-3D&#25968;&#25454;&#38598;&#21644;PO3D-VQA&#27169;&#22411;&#65292;&#24182;&#23558;&#27010;&#29575;&#31070;&#32463;&#31526;&#21495;&#31243;&#24207;&#25191;&#34892;&#21644;3D&#29983;&#25104;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#24378;&#22823;&#30340;&#35270;&#35273;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2310.17914</link><description>&lt;p&gt;
&#20851;&#20110;&#37096;&#20214;&#12289;&#23039;&#24577;&#21644;&#36974;&#25377;&#30340;3D&#24863;&#30693;&#35270;&#35273;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
3D-Aware Visual Question Answering about Parts, Poses and Occlusions. (arXiv:2310.17914v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17914
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;3D&#24863;&#30693;VQA&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#25512;&#21160;VQA&#27169;&#22411;&#23545;&#20110;&#35270;&#35273;&#22330;&#26223;&#30340;3D&#32467;&#26500;&#36827;&#34892;&#32452;&#21512;&#25512;&#29702;&#12290;&#20316;&#32773;&#20174;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#35282;&#24230;&#20986;&#21457;&#65292;&#20998;&#21035;&#24341;&#20837;&#20102;Super-CLEVR-3D&#25968;&#25454;&#38598;&#21644;PO3D-VQA&#27169;&#22411;&#65292;&#24182;&#23558;&#27010;&#29575;&#31070;&#32463;&#31526;&#21495;&#31243;&#24207;&#25191;&#34892;&#21644;3D&#29983;&#25104;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#24378;&#22823;&#30340;&#35270;&#35273;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#38271;&#36275;&#36827;&#23637;&#65292;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#20027;&#35201;&#38598;&#20013;&#20110;2D&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;VQA&#27169;&#22411;&#20063;&#38656;&#35201;&#29702;&#35299;&#35270;&#35273;&#22330;&#26223;&#30340;3D&#32467;&#26500;&#65292;&#20363;&#22914;&#25903;&#25345;&#23548;&#33322;&#25110;&#25805;&#20316;&#31561;&#20219;&#21153;&#12290;&#36825;&#21253;&#25324;&#23545;&#19977;&#32500;&#29289;&#20307;&#23039;&#24577;&#12289;&#37096;&#20214;&#21644;&#36974;&#25377;&#30340;&#29702;&#35299;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#39033;&#21517;&#20026;3D&#24863;&#30693;VQA&#30340;&#20219;&#21153;&#65292;&#37325;&#28857;&#20851;&#27880;&#38656;&#35201;&#22312;&#35270;&#35273;&#22330;&#26223;&#30340;&#19977;&#32500;&#32467;&#26500;&#19978;&#36827;&#34892;&#32452;&#21512;&#25512;&#29702;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#20174;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#20004;&#20010;&#26041;&#38754;&#26469;&#35299;&#20915;3D&#24863;&#30693;VQA&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Super-CLEVR-3D&#65292;&#19968;&#20010;&#21253;&#21547;&#20851;&#20110;&#29289;&#20307;&#37096;&#20214;&#12289;&#23427;&#20204;&#30340;&#19977;&#32500;&#23039;&#24577;&#21644;&#36974;&#25377;&#30340;&#32452;&#21512;&#25512;&#29702;&#25968;&#25454;&#38598;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PO3D-VQA&#65292;&#19968;&#20010;&#23558;&#20004;&#20010;&#24378;&#22823;&#30340;&#24605;&#24819;&#30456;&#32467;&#21512;&#30340;3D&#24863;&#30693;VQA&#27169;&#22411;&#65306;&#29992;&#20110;&#25512;&#29702;&#30340;&#27010;&#29575;&#31070;&#32463;&#31526;&#21495;&#31243;&#24207;&#25191;&#34892;&#21644;&#22522;&#20110;&#29289;&#20307;&#30340;&#19977;&#32500;&#29983;&#25104;&#34920;&#31034;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#24378;&#22823;&#30340;&#35270;&#35273;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite rapid progress in Visual question answering (VQA), existing datasets and models mainly focus on testing reasoning in 2D. However, it is important that VQA models also understand the 3D structure of visual scenes, for example to support tasks like navigation or manipulation. This includes an understanding of the 3D object pose, their parts and occlusions. In this work, we introduce the task of 3D-aware VQA, which focuses on challenging questions that require a compositional reasoning over the 3D structure of visual scenes. We address 3D-aware VQA from both the dataset and the model perspective. First, we introduce Super-CLEVR-3D, a compositional reasoning dataset that contains questions about object parts, their 3D poses, and occlusions. Second, we propose PO3D-VQA, a 3D-aware VQA model that marries two powerful ideas: probabilistic neural symbolic program execution for reasoning and deep neural networks with 3D generative representations of objects for robust visual recognition
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#23545;&#34920;&#26684;&#25968;&#25454;&#26597;&#35810;&#21644;&#21487;&#35270;&#21270;&#30340;&#33258;&#28982;&#35821;&#35328;&#30028;&#38754;&#36827;&#34892;&#20102;&#20840;&#38754;&#27010;&#36848;&#65292;&#20171;&#32461;&#20102;&#35821;&#20041;&#35299;&#26512;&#31561;&#20851;&#38190;&#25216;&#26415;&#65292;&#24182;&#28145;&#20837;&#25506;&#35752;&#20102;Text-to-SQL&#21644;Text-to-Vis&#38382;&#39064;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2310.17894</link><description>&lt;p&gt;
&#23545;&#34920;&#26684;&#25968;&#25454;&#26597;&#35810;&#21644;&#21487;&#35270;&#21270;&#30340;&#33258;&#28982;&#35821;&#35328;&#30028;&#38754;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Natural Language Interfaces for Tabular Data Querying and Visualization: A Survey. (arXiv:2310.17894v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17894
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#23545;&#34920;&#26684;&#25968;&#25454;&#26597;&#35810;&#21644;&#21487;&#35270;&#21270;&#30340;&#33258;&#28982;&#35821;&#35328;&#30028;&#38754;&#36827;&#34892;&#20102;&#20840;&#38754;&#27010;&#36848;&#65292;&#20171;&#32461;&#20102;&#35821;&#20041;&#35299;&#26512;&#31561;&#20851;&#38190;&#25216;&#26415;&#65292;&#24182;&#28145;&#20837;&#25506;&#35752;&#20102;Text-to-SQL&#21644;Text-to-Vis&#38382;&#39064;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#20986;&#29616;&#24443;&#24213;&#25913;&#21464;&#20102;&#29992;&#25143;&#19982;&#34920;&#26684;&#25968;&#25454;&#30340;&#20132;&#20114;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#20174;&#20256;&#32479;&#30340;&#26597;&#35810;&#35821;&#35328;&#21644;&#25163;&#21160;&#32472;&#22270;&#36716;&#21521;&#26356;&#30452;&#35266;&#12289;&#22522;&#20110;&#35821;&#35328;&#30340;&#30028;&#38754;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;ChatGPT&#21450;&#20854;&#21518;&#32487;&#32773;&#36827;&#19968;&#27493;&#25512;&#21160;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;&#26412;&#35843;&#26597;&#25552;&#20379;&#20102;&#20851;&#20110;&#34920;&#26684;&#25968;&#25454;&#26597;&#35810;&#21644;&#21487;&#35270;&#21270;&#30340;&#33258;&#28982;&#35821;&#35328;&#30028;&#38754;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#36825;&#20123;&#30028;&#38754;&#20801;&#35768;&#29992;&#25143;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#19982;&#25968;&#25454;&#36827;&#34892;&#20132;&#20114;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#36825;&#20123;&#30028;&#38754;&#30340;&#22522;&#26412;&#27010;&#24565;&#21644;&#25216;&#26415;&#65292;&#29305;&#21035;&#24378;&#35843;&#35821;&#20041;&#35299;&#26512;&#65292;&#36825;&#26159;&#23454;&#29616;&#20174;&#33258;&#28982;&#35821;&#35328;&#21040;SQL&#26597;&#35810;&#25110;&#25968;&#25454;&#21487;&#35270;&#21270;&#21629;&#20196;&#36716;&#21270;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#28982;&#21518;&#20174;&#25968;&#25454;&#38598;&#12289;&#26041;&#27861;&#35770;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#31995;&#32479;&#35774;&#35745;&#30340;&#35282;&#24230;&#28145;&#20837;&#25506;&#35752;&#20102;Text-to-SQL&#21644;Text-to-Vis&#38382;&#39064;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of natural language processing has revolutionized the way users interact with tabular data, enabling a shift from traditional query languages and manual plotting to more intuitive, language-based interfaces. The rise of large language models (LLMs) such as ChatGPT and its successors has further advanced this field, opening new avenues for natural language processing techniques. This survey presents a comprehensive overview of natural language interfaces for tabular data querying and visualization, which allow users to interact with data using natural language queries. We introduce the fundamental concepts and techniques underlying these interfaces with a particular emphasis on semantic parsing, the key technology facilitating the translation from natural language to SQL queries or data visualization commands. We then delve into the recent advancements in Text-to-SQL and Text-to-Vis problems from the perspectives of datasets, methodologies, metrics, and system designs. Thi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;ConfAIde&#22522;&#20934;&#65292;&#25581;&#31034;&#20102;LLMs&#30340;&#19978;&#19979;&#25991;&#38544;&#31169;&#25512;&#29702;&#33021;&#21147;&#20013;&#30340;&#37325;&#35201;&#24369;&#28857;&#65292;&#23454;&#39564;&#35777;&#26126;&#21363;&#20351;&#26159;&#26368;&#24378;&#22823;&#30340;&#27169;&#22411;&#20063;&#20250;&#22312;&#20154;&#31867;&#19981;&#20250;&#30340;&#19978;&#19979;&#25991;&#20013;&#27844;&#38706;&#31169;&#20154;&#20449;&#24687;&#65292;&#24378;&#35843;&#20102;&#25506;&#32034;&#26032;&#22411;&#25512;&#29702;&#26102;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#30340;&#36843;&#20999;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.17884</link><description>&lt;p&gt;
LLM&#33021;&#20445;&#23432;&#31192;&#23494;&#21527;&#65311;&#36890;&#36807;&#19978;&#19979;&#25991;&#23436;&#25972;&#24615;&#29702;&#35770;&#27979;&#35797;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#31169;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory. (arXiv:2310.17884v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17884
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;ConfAIde&#22522;&#20934;&#65292;&#25581;&#31034;&#20102;LLMs&#30340;&#19978;&#19979;&#25991;&#38544;&#31169;&#25512;&#29702;&#33021;&#21147;&#20013;&#30340;&#37325;&#35201;&#24369;&#28857;&#65292;&#23454;&#39564;&#35777;&#26126;&#21363;&#20351;&#26159;&#26368;&#24378;&#22823;&#30340;&#27169;&#22411;&#20063;&#20250;&#22312;&#20154;&#31867;&#19981;&#20250;&#30340;&#19978;&#19979;&#25991;&#20013;&#27844;&#38706;&#31169;&#20154;&#20449;&#24687;&#65292;&#24378;&#35843;&#20102;&#25506;&#32034;&#26032;&#22411;&#25512;&#29702;&#26102;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#30340;&#36843;&#20999;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;AI&#21161;&#25163;&#65288;&#24037;&#20316;&#12289;&#23478;&#24237;&#31561;&#65289;&#20013;&#20132;&#20114;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#26032;&#30340;&#25512;&#29702;&#26102;&#38544;&#31169;&#39118;&#38505;&#65306;LLMs&#20174;&#22810;&#20010;&#26469;&#28304;&#30340;&#36755;&#20837;&#20013;&#33719;&#21462;&#19981;&#21516;&#31867;&#22411;&#30340;&#20449;&#24687;&#65292;&#24182;&#26399;&#26395;&#22312;&#32473;&#23450;&#30340;&#19978;&#19979;&#25991;&#20013;&#25512;&#29702;&#20986;&#22312;&#20309;&#31181;&#30446;&#30340;&#21644;&#19982;&#35841;&#20998;&#20139;&#30340;&#20869;&#23481;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;ConfAIde&#65292;&#19968;&#20010;&#26088;&#22312;&#35782;&#21035;&#25351;&#20196;&#35843;&#25972;&#30340;LLMs&#38544;&#31169;&#25512;&#29702;&#33021;&#21147;&#20013;&#37325;&#35201;&#24369;&#28857;&#30340;&#22522;&#20934;&#65292;&#26469;&#24341;&#36215;&#20154;&#20204;&#23545;&#19978;&#19979;&#25991;&#38544;&#31169;&#36825;&#19968;&#26497;&#20854;&#20851;&#38190;&#20294;&#32463;&#24120;&#34987;&#24573;&#35270;&#30340;&#27010;&#24565;&#30340;&#20851;&#27880;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;GPT-4&#21644;ChatGPT&#31561;&#26368;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;&#22312;&#20154;&#31867;&#19981;&#20250;&#30340;&#19978;&#19979;&#25991;&#20013;&#65292;&#20063;&#20250;&#27844;&#38706;39&#65285;&#21644;57&#65285;&#30340;&#31169;&#20154;&#20449;&#24687;&#12290;&#21363;&#20351;&#25105;&#20204;&#20351;&#29992;&#20445;&#25252;&#38544;&#31169;&#30340;&#25552;&#31034;&#25110;&#24605;&#32500;&#38142;&#25512;&#29702;&#65292;&#36825;&#31181;&#27844;&#28431;&#20063;&#20250;&#25345;&#32493;&#23384;&#22312;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24378;&#35843;&#20102;&#36843;&#20999;&#38656;&#35201;&#25506;&#32034;&#22522;&#20110;&#25512;&#29702;&#21644;&#29702;&#35770;&#30340;&#26032;&#22411;&#25512;&#29702;&#26102;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The interactive use of large language models (LLMs) in AI assistants (at work, home, etc.) introduces a new set of inference-time privacy risks: LLMs are fed different types of information from multiple sources in their inputs and are expected to reason about what to share in their outputs, for what purpose and with whom, within a given context. In this work, we draw attention to the highly critical yet overlooked notion of contextual privacy by proposing ConfAIde, a benchmark designed to identify critical weaknesses in the privacy reasoning capabilities of instruction-tuned LLMs. Our experiments show that even the most capable models such as GPT-4 and ChatGPT reveal private information in contexts that humans would not, 39% and 57% of the time, respectively. This leakage persists even when we employ privacy-inducing prompts or chain-of-thought reasoning. Our work underscores the immediate need to explore novel inference-time privacy-preserving approaches, based on reasoning and theory
&lt;/p&gt;</description></item><item><title>ASPIRO&#26159;&#19968;&#31181;&#33021;&#22312;&#38646;&#21040;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#23558;&#32467;&#26500;&#21270;&#25968;&#25454;&#36716;&#21270;&#20026;&#31616;&#30701;&#27169;&#26495;&#21477;&#23376;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#31639;&#27861;&#35299;&#26512;&#26816;&#26597;&#12289;LLM&#30340;&#37325;&#26032;&#25552;&#31034;&#20197;&#21450;&#19968;&#33268;&#24615;&#39564;&#35777;&#25351;&#26631;PARENT&#65292;ASPIRO&#25104;&#21151;&#38477;&#20302;&#20102;66%&#30340;&#35299;&#26512;&#38169;&#35823;&#29575;&#65292;&#24182;&#19988;&#22312;&#19982;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#31454;&#20105;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2310.17877</link><description>&lt;p&gt;
ASPIRO: &#19968;&#31181;&#36866;&#29992;&#20110;&#38646;&#21040;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#32467;&#26500;&#21270;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#38169;&#35823;&#24863;&#30693;&#37325;&#25552;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ASPIRO: Any-shot Structured Parsing-error-Induced ReprOmpting for Consistent Data-to-Text Generation. (arXiv:2310.17877v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17877
&lt;/p&gt;
&lt;p&gt;
ASPIRO&#26159;&#19968;&#31181;&#33021;&#22312;&#38646;&#21040;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#23558;&#32467;&#26500;&#21270;&#25968;&#25454;&#36716;&#21270;&#20026;&#31616;&#30701;&#27169;&#26495;&#21477;&#23376;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#31639;&#27861;&#35299;&#26512;&#26816;&#26597;&#12289;LLM&#30340;&#37325;&#26032;&#25552;&#31034;&#20197;&#21450;&#19968;&#33268;&#24615;&#39564;&#35777;&#25351;&#26631;PARENT&#65292;ASPIRO&#25104;&#21151;&#38477;&#20302;&#20102;66%&#30340;&#35299;&#26512;&#38169;&#35823;&#29575;&#65292;&#24182;&#19988;&#22312;&#19982;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#31454;&#20105;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;ASPIRO&#65292;&#19968;&#31181;&#22312;&#38646;&#21040;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#23558;&#32467;&#26500;&#21270;&#25968;&#25454;&#36716;&#21270;&#20026;&#31616;&#30701;&#27169;&#26495;&#21477;&#23376;&#30340;&#26041;&#27861;&#12290;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30452;&#25509;&#25552;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20135;&#29983;&#19982;&#23454;&#20307;&#26080;&#20851;&#30340;&#27169;&#26495;&#65292;&#32780;&#19981;&#26159;&#20381;&#36182;LLM&#24544;&#23454;&#22320;&#22797;&#21046;&#32473;&#23450;&#30340;&#23454;&#20307;&#65292;&#25110;&#32773;&#25163;&#21160;&#39564;&#35777;/&#21046;&#20316;&#27169;&#26495;&#12290;&#25105;&#20204;&#36890;&#36807;&#31639;&#27861;&#35299;&#26512;&#26816;&#26597;&#21644;PARENT&#25351;&#26631;&#35825;&#23548;&#30340;&#19968;&#33268;&#24615;&#39564;&#35777;&#65292;&#32467;&#21512;LLM&#30340;&#37325;&#26032;&#25552;&#31034;&#65292;&#23454;&#26102;&#35782;&#21035;&#21644;&#32416;&#27491;&#27169;&#26495;&#29983;&#25104;&#38382;&#39064;&#12290;&#22312;DART&#25968;&#25454;&#38598;&#19978;&#65292;&#19982;&#30452;&#25509;LLM&#36755;&#20986;&#30456;&#27604;&#65292;ASPIRO&#23545;RDF&#19977;&#20803;&#32452;&#30340;&#29983;&#25104;&#25991;&#26412;&#30340;&#35299;&#26512;&#38169;&#35823;&#29575;&#24179;&#22343;&#38477;&#20302;&#20102;66&#65285;&#12290;&#25105;&#20204;&#22312;Rel2Text&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20339;5&#26679;&#26412;text-davinci-003&#35774;&#32622;&#35780;&#20998;&#20026;BLEU 50.62&#65292;METEOR 45.16&#65292;BLEURT 0.82&#65292;NUBIA 0.87&#21644;PARENT 0.8962&#65292;&#19982;&#26368;&#36817;&#30340;&#31934;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26377;&#20102;&#26377;&#25928;&#30340;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present ASPIRO, an approach for structured data verbalisation into short template sentences in zero to few-shot settings. Unlike previous methods, our approach prompts large language models (LLMs) to directly produce entity-agnostic templates, rather than relying on LLMs to faithfully copy the given example entities, or validating/crafting the templates manually. We incorporate LLM re-prompting, triggered by algorithmic parsing checks, as well as the PARENT metric induced consistency validation to identify and rectify template generation problems in real-time. ASPIRO, compared to direct LLM output, averages 66\% parsing error rate reduction in generated verbalisations of RDF triples on the DART dataset. Our best 5-shot text-davinci-003 setup, scoring BLEU of 50.62, METEOR of 45.16, BLEURT of 0.82, NUBIA of 0.87, and PARENT of 0.8962 on the Rel2Text dataset, competes effectively with recent fine-tuned pre-trained language models.
&lt;/p&gt;</description></item><item><title>TarGEN&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#22810;&#27493;&#25552;&#31034;&#31574;&#30053;&#65292;&#36890;&#36807;&#33258;&#25105;&#20462;&#27491;&#26041;&#27861;&#30830;&#20445;&#21487;&#38752;&#30340;&#26631;&#31614;&#12290;&#22312;SuperGLUE&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#27169;&#22411;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#35757;&#32451;&#25928;&#26524;&#19982;&#21407;&#22987;&#25968;&#25454;&#38598;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2310.17876</link><description>&lt;p&gt;
TarGEN: &#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30446;&#26631;&#25968;&#25454;&#29983;&#25104;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
TarGEN: Targeted Data Generation with Large Language Models. (arXiv:2310.17876v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17876
&lt;/p&gt;
&lt;p&gt;
TarGEN&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#22810;&#27493;&#25552;&#31034;&#31574;&#30053;&#65292;&#36890;&#36807;&#33258;&#25105;&#20462;&#27491;&#26041;&#27861;&#30830;&#20445;&#21487;&#38752;&#30340;&#26631;&#31614;&#12290;&#22312;SuperGLUE&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#27169;&#22411;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#35757;&#32451;&#25928;&#26524;&#19982;&#21407;&#22987;&#25968;&#25454;&#38598;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#24341;&#21457;&#20102;&#23545;&#25968;&#25454;&#21512;&#25104;&#25216;&#26415;&#30340;&#20852;&#36259;&#65292;&#26088;&#22312;&#29983;&#25104;&#22810;&#26679;&#19988;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#21512;&#25104;&#25968;&#25454;&#38598;&#24448;&#24448;&#32570;&#20047;&#22810;&#26679;&#24615;&#24182;&#19988;&#23384;&#22312;&#22122;&#22768;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TarGEN&#65292;&#19968;&#31181;&#21033;&#29992;LLM&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#22810;&#27493;&#25552;&#31034;&#31574;&#30053;&#12290;TarGEN&#30340;&#19968;&#20010;&#20248;&#28857;&#26159;&#26080;&#38656;&#31181;&#23376;&#65307;&#23427;&#19981;&#38656;&#35201;&#29305;&#23450;&#30340;&#20219;&#21153;&#23454;&#20363;&#65292;&#25193;&#22823;&#20102;&#20854;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#19968;&#31181;&#31216;&#20026;&#33258;&#25105;&#20462;&#27491;&#30340;&#26041;&#27861;&#65292;&#20351;LLM&#33021;&#22815;&#22312;&#21019;&#24314;&#25968;&#25454;&#38598;&#36807;&#31243;&#20013;&#32416;&#27491;&#26631;&#35760;&#38169;&#35823;&#30340;&#23454;&#20363;&#65292;&#30830;&#20445;&#21487;&#38752;&#30340;&#26631;&#31614;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#27169;&#25311;&#20102;SuperGLUE&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;8&#20010;&#20219;&#21153;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#21407;&#22987;&#35757;&#32451;&#38598;&#19978;&#24494;&#35843;&#20102;&#21508;&#31181;&#35821;&#35328;&#27169;&#22411;&#65292;&#21253;&#25324;&#20165;&#32534;&#30721;&#22120;&#12289;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21644;&#20165;&#35299;&#30721;&#22120;&#27169;&#22411;&#12290;&#22312;&#21407;&#22987;&#27979;&#35797;&#38598;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#27169;&#22411;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#25928;&#26524;&#19982;&#21407;&#22987;&#25968;&#25454;&#38598;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advancement of large language models (LLMs) has sparked interest in data synthesis techniques, aiming to generate diverse and high-quality synthetic datasets. However, these synthetic datasets often suffer from a lack of diversity and added noise. In this paper, we present TarGEN, a multi-step prompting strategy for generating high-quality synthetic datasets utilizing a LLM. An advantage of TarGEN is its seedless nature; it does not require specific task instances, broadening its applicability beyond task replication. We augment TarGEN with a method known as self-correction empowering LLMs to rectify inaccurately labeled instances during dataset creation, ensuring reliable labels. To assess our technique's effectiveness, we emulate 8 tasks from the SuperGLUE benchmark and finetune various language models, including encoder-only, encoder-decoder, and decoder-only models on both synthetic and original training sets. Evaluation on the original test set reveals that models traine
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#27880;&#20837;&#20215;&#20540;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#39044;&#27979;&#20154;&#31867;&#34892;&#20026;&#21644;&#35266;&#28857;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20215;&#20540;&#27880;&#20837;&#26041;&#27861;&#65288;VIM&#65289;&#23545;LLMs&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#27880;&#20837;&#20102;VIM&#30340;LLMs&#22312;&#39044;&#27979;&#20154;&#20204;&#35266;&#28857;&#21644;&#34892;&#20026;&#26041;&#38754;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.17857</link><description>&lt;p&gt;
&#20174;&#20215;&#20540;&#35266;&#21040;&#35266;&#28857;&#65306;&#21033;&#29992;&#27880;&#20837;&#20215;&#20540;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#20154;&#31867;&#34892;&#20026;&#21644;&#31435;&#22330;
&lt;/p&gt;
&lt;p&gt;
From Values to Opinions: Predicting Human Behaviors and Stances Using Value-Injected Large Language Models. (arXiv:2310.17857v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17857
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#27880;&#20837;&#20215;&#20540;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#39044;&#27979;&#20154;&#31867;&#34892;&#20026;&#21644;&#35266;&#28857;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20215;&#20540;&#27880;&#20837;&#26041;&#27861;&#65288;VIM&#65289;&#23545;LLMs&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#27880;&#20837;&#20102;VIM&#30340;LLMs&#22312;&#39044;&#27979;&#20154;&#20204;&#35266;&#28857;&#21644;&#34892;&#20026;&#26041;&#38754;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#33021;&#22815;&#39044;&#27979;&#20154;&#20204;&#23545;&#38382;&#39064;&#21644;&#34892;&#20026;&#30340;&#35266;&#28857;&#21487;&#20197;&#22312;&#25919;&#27835;&#21644;&#24066;&#22330;&#33829;&#38144;&#31561;&#39046;&#22495;&#20013;&#38750;&#24120;&#26377;&#24110;&#21161;&#12290;&#28982;&#32780;&#65292;&#36827;&#34892;&#22823;&#35268;&#27169;&#35843;&#26597;&#65288;&#22914;&#27431;&#27954;&#31038;&#20250;&#35843;&#26597;&#65289;&#20197;&#33719;&#21462;&#20154;&#20204;&#23545;&#20010;&#21035;&#38382;&#39064;&#30340;&#24847;&#35265;&#21487;&#33021;&#20250;&#20135;&#29983;&#39640;&#26114;&#30340;&#25104;&#26412;&#12290;&#22522;&#20110;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#26680;&#24515;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#20010;&#20154;&#20915;&#31574;&#21644;&#34892;&#21160;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#27880;&#20837;&#20215;&#20540;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#39044;&#27979;&#35266;&#28857;&#21644;&#34892;&#20026;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20215;&#20540;&#27880;&#20837;&#26041;&#27861;&#65288;VIM&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#30001;&#20004;&#31181;&#26041;&#27861;&#32452;&#25104;&#30340;&#38598;&#21512;&#8212;&#8212;&#35770;&#25454;&#29983;&#25104;&#21644;&#38382;&#31572;&#65292;&#26088;&#22312;&#36890;&#36807;&#24494;&#35843;&#23558;&#23450;&#21521;&#20215;&#20540;&#20998;&#24067;&#27880;&#20837;LLMs&#20013;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#22235;&#20010;&#20219;&#21153;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#20197;&#27979;&#35797;VIM&#30340;&#26377;&#25928;&#24615;&#21644;&#20351;&#29992;&#27880;&#20837;&#20215;&#20540;&#30340;LLM&#39044;&#27979;&#20154;&#20204;&#35266;&#28857;&#21644;&#34892;&#20026;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#27880;&#20837;&#20102;VIM&#21464;&#31181;&#30340;LLMs&#22312;&#24615;&#33021;&#19978;&#26126;&#26174;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Being able to predict people's opinions on issues and behaviors in realistic scenarios can be helpful in various domains, such as politics and marketing. However, conducting large-scale surveys like the European Social Survey to solicit people's opinions on individual issues can incur prohibitive costs. Leveraging prior research showing influence of core human values on individual decisions and actions, we propose to use value-injected large language models (LLM) to predict opinions and behaviors. To this end, we present Value Injection Method (VIM), a collection of two methods -- argument generation and question answering -- designed to inject targeted value distributions into LLMs via fine-tuning. We then conduct a series of experiments on four tasks to test the effectiveness of VIM and the possibility of using value-injected LLMs to predict opinions and behaviors of people. We find that LLMs value-injected with variations of VIM substantially outperform the baselines. Also, the resu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;RadGraph&#21644;&#23569;&#26679;&#26412;&#25552;&#31034;&#30340;&#39118;&#26684;&#24863;&#30693;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#25253;&#21578;&#30340;&#20869;&#23481;&#21644;&#39118;&#26684;&#20998;&#24320;&#22788;&#29702;&#65292;&#21487;&#20197;&#36991;&#20813;&#29983;&#25104;&#20020;&#24202;&#19981;&#20934;&#30830;&#30340;&#25253;&#21578;&#12290;&#23450;&#37327;&#35780;&#20272;&#21644;&#20154;&#24037;&#35780;&#20272;&#32467;&#26524;&#22343;&#34920;&#26126;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#29983;&#25104;&#19982;&#20010;&#20307;&#25918;&#23556;&#31185;&#21307;&#29983;&#39118;&#26684;&#23436;&#20840;&#30456;&#21516;&#30340;&#25253;&#21578;&#12290;</title><link>http://arxiv.org/abs/2310.17811</link><description>&lt;p&gt;
&#20351;&#29992;RadGraph&#21644;&#23569;&#26679;&#26412;&#25552;&#31034;&#30340;&#39118;&#26684;&#24863;&#30693;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Style-Aware Radiology Report Generation with RadGraph and Few-Shot Prompting. (arXiv:2310.17811v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17811
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;RadGraph&#21644;&#23569;&#26679;&#26412;&#25552;&#31034;&#30340;&#39118;&#26684;&#24863;&#30693;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#25253;&#21578;&#30340;&#20869;&#23481;&#21644;&#39118;&#26684;&#20998;&#24320;&#22788;&#29702;&#65292;&#21487;&#20197;&#36991;&#20813;&#29983;&#25104;&#20020;&#24202;&#19981;&#20934;&#30830;&#30340;&#25253;&#21578;&#12290;&#23450;&#37327;&#35780;&#20272;&#21644;&#20154;&#24037;&#35780;&#20272;&#32467;&#26524;&#22343;&#34920;&#26126;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#29983;&#25104;&#19982;&#20010;&#20307;&#25918;&#23556;&#31185;&#21307;&#29983;&#39118;&#26684;&#23436;&#20840;&#30456;&#21516;&#30340;&#25253;&#21578;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#20174;&#21307;&#23398;&#24433;&#20687;&#20013;&#29983;&#25104;&#25253;&#21578;&#26377;&#26395;&#25913;&#21892;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#30452;&#25509;&#20174;&#22270;&#20687;&#29983;&#25104;&#23436;&#25972;&#30340;&#25253;&#21578;&#26469;&#32771;&#34385;&#22270;&#20687;&#21040;&#25253;&#21578;&#30340;&#24314;&#27169;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#28151;&#28102;&#20102;&#25253;&#21578;&#30340;&#20869;&#23481;&#65288;&#22914;&#21457;&#29616;&#21644;&#20854;&#23646;&#24615;&#65289;&#19982;&#20854;&#39118;&#26684;&#65288;&#22914;&#26684;&#24335;&#21644;&#35789;&#27719;&#36873;&#25321;&#65289;&#65292;&#21487;&#33021;&#23548;&#33268;&#20020;&#24202;&#19981;&#20934;&#30830;&#30340;&#25253;&#21578;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#20004;&#27493;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20174;&#22270;&#20687;&#20013;&#25552;&#21462;&#20869;&#23481;&#65292;&#28982;&#21518;&#23558;&#25552;&#21462;&#30340;&#20869;&#23481;&#36716;&#21270;&#20026;&#19982;&#29305;&#23450;&#25918;&#23556;&#31185;&#21307;&#29983;&#39118;&#26684;&#30456;&#21305;&#37197;&#30340;&#25253;&#21578;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;RadGraph&#8212;&#8212;&#19968;&#31181;&#25253;&#21578;&#30340;&#22270;&#34920;&#31034;&#8212;&#8212;&#20197;&#21450;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290;&#22312;&#23450;&#37327;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#26041;&#38754;&#20855;&#26377;&#30410;&#22788;&#12290;&#36890;&#36807;&#20020;&#24202;&#35780;&#20272;&#32773;&#36827;&#34892;&#30340;&#20154;&#24037;&#35780;&#20272;&#34920;&#26126;&#65292;AI&#29983;&#25104;&#30340;&#25253;&#21578;&#19982;&#20010;&#20307;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#39118;&#26684;&#23436;&#20840;&#30456;&#21516;&#65292;&#26080;&#27861;&#21306;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatically generated reports from medical images promise to improve the workflow of radiologists. Existing methods consider an image-to-report modeling task by directly generating a fully-fledged report from an image. However, this conflates the content of the report (e.g., findings and their attributes) with its style (e.g., format and choice of words), which can lead to clinically inaccurate reports. To address this, we propose a two-step approach for radiology report generation. First, we extract the content from an image; then, we verbalize the extracted content into a report that matches the style of a specific radiologist. For this, we leverage RadGraph -- a graph representation of reports -- together with large language models (LLMs). In our quantitative evaluations, we find that our approach leads to beneficial performance. Our human evaluation with clinical raters highlights that the AI-generated reports are indistinguishably tailored to the style of individual radiologist 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27880;&#37322;&#26041;&#26696;&#65292;&#26126;&#30830;&#23450;&#20041;&#20102;&#26102;&#38388;&#20851;&#31995;&#30340;&#26631;&#20934;&#65292;&#24182;&#25552;&#20986;&#20102;&#33258;&#21160;&#21270;&#27880;&#37322;&#25152;&#26377;&#26102;&#38388;&#20851;&#31995;&#30340;&#26041;&#27861;&#12290;&#29983;&#25104;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;TIMELINE&#35821;&#26009;&#24211;&#12290;</title><link>http://arxiv.org/abs/2310.17802</link><description>&lt;p&gt;
TIMELINE&#65306;&#23545;&#26032;&#38395;&#25991;&#31456;&#20013;&#26102;&#38388;&#20851;&#31995;&#30340;&#35814;&#23613;&#27880;&#37322;&#65292;&#25903;&#25345;&#33258;&#21160;&#25490;&#24207;&#20107;&#20214;&#12290;(arXiv:2310.17802v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
TIMELINE: Exhaustive Annotation of Temporal Relations Supporting the Automatic Ordering of Events in News Articles. (arXiv:2310.17802v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27880;&#37322;&#26041;&#26696;&#65292;&#26126;&#30830;&#23450;&#20041;&#20102;&#26102;&#38388;&#20851;&#31995;&#30340;&#26631;&#20934;&#65292;&#24182;&#25552;&#20986;&#20102;&#33258;&#21160;&#21270;&#27880;&#37322;&#25152;&#26377;&#26102;&#38388;&#20851;&#31995;&#30340;&#26041;&#27861;&#12290;&#29983;&#25104;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;TIMELINE&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#23545;&#20110;&#29616;&#26377;&#30340;&#26631;&#27880;&#26102;&#38388;&#20851;&#31995;&#30340;&#26032;&#38395;&#25968;&#25454;&#38598;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65292;&#21253;&#25324;&#65306;(1)&#30001;&#20110;&#27880;&#37322;&#25351;&#21335;&#22312;&#20160;&#20040;&#31639;&#20316;&#26102;&#38388;&#20851;&#31995;&#26041;&#38754;&#32570;&#20047;&#20855;&#20307;&#24615;&#65292;&#23548;&#33268;&#20302;&#30340;&#26631;&#27880;&#32773;&#19968;&#33268;&#24615;&#65307;(2)&#22312;&#32473;&#23450;&#25991;&#26723;&#20013;&#25490;&#38500;&#36328;&#19981;&#21516;&#27573;&#33853;&#30340;&#38271;&#36317;&#31163;&#20851;&#31995;&#65307;(3)&#25490;&#38500;&#19981;&#20197;&#21160;&#35789;&#20026;&#20013;&#24515;&#30340;&#20107;&#20214;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#26032;&#30340;&#27880;&#37322;&#26041;&#26696;&#26469;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#35813;&#26041;&#26696;&#26126;&#30830;&#23450;&#20041;&#20102;&#24212;&#35813;&#27880;&#37322;&#30340;&#26102;&#38388;&#20851;&#31995;&#30340;&#26631;&#20934;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#26696;&#36824;&#21253;&#25324;&#19981;&#20197;&#21160;&#35789;&#34920;&#36798;&#30340;&#20107;&#20214;&#65288;&#20363;&#22914;&#65292;&#21517;&#35789;&#21270;&#20107;&#20214;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#27880;&#37322;&#25152;&#26377;&#26102;&#38388;&#20851;&#31995;&#65288;&#21253;&#25324;&#38271;&#36317;&#31163;&#20851;&#31995;&#65289;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#26631;&#27880;&#32773;&#30340;&#26102;&#38388;&#21644;&#20154;&#24037;&#24037;&#20316;&#37327;&#12290;&#32467;&#26524;&#26159;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#21363;TIMELINE&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal relation extraction models have thus far been hindered by a number of issues in existing temporal relation-annotated news datasets, including: (1) low inter-annotator agreement due to the lack of specificity of their annotation guidelines in terms of what counts as a temporal relation; (2) the exclusion of long-distance relations within a given document (those spanning across different paragraphs); and (3) the exclusion of events that are not centred on verbs. This paper aims to alleviate these issues by presenting a new annotation scheme that clearly defines the criteria based on which temporal relations should be annotated. Additionally, the scheme includes events even if they are not expressed as verbs (e.g., nominalised events). Furthermore, we propose a method for annotating all temporal relations -- including long-distance ones -- which automates the process, hence reducing time and manual effort on the part of annotators. The result is a new dataset, the TIMELINE corpus
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20998;&#26512;&#21477;&#23376;&#30340;&#24847;&#20041;&#32467;&#26500;&#26041;&#38754;&#30340;&#25104;&#21151;&#21644;&#38480;&#21046;&#65292;&#21457;&#29616;&#27169;&#22411;&#22312;&#29983;&#25104;&#21644;&#37325;&#26500;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#65288;AMR&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#19968;&#23450;&#30340;&#33021;&#21147;&#65292;&#20294;&#22312;&#22797;&#26434;&#30340;&#21477;&#23376;&#32467;&#26500;&#25110;&#35821;&#20041;&#25512;&#29702;&#20219;&#21153;&#20013;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17793</link><description>&lt;p&gt;
"&#24744;&#26159;&#19968;&#20301;&#19987;&#23478;&#35821;&#35328;&#27880;&#37322;&#32773;"&#65306;&#20316;&#20026;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#20998;&#26512;&#22120;&#30340;LLMs&#30340;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
"You Are An Expert Linguistic Annotator": Limits of LLMs as Analyzers of Abstract Meaning Representation. (arXiv:2310.17793v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20998;&#26512;&#21477;&#23376;&#30340;&#24847;&#20041;&#32467;&#26500;&#26041;&#38754;&#30340;&#25104;&#21151;&#21644;&#38480;&#21046;&#65292;&#21457;&#29616;&#27169;&#22411;&#22312;&#29983;&#25104;&#21644;&#37325;&#26500;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#65288;AMR&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#19968;&#23450;&#30340;&#33021;&#21147;&#65292;&#20294;&#22312;&#22797;&#26434;&#30340;&#21477;&#23376;&#32467;&#26500;&#25110;&#35821;&#20041;&#25512;&#29702;&#20219;&#21153;&#20013;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35821;&#35328;&#20351;&#29992;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#20196;&#20154;&#24778;&#35766;&#30340;&#29087;&#32451;&#24230;&#21644;&#27969;&#30021;&#24615;&#12290;&#36825;&#26159;&#21542;&#24847;&#21619;&#30528;&#23427;&#20204;&#20063;&#24050;&#32463;&#33719;&#24471;&#20102;&#20851;&#20110;&#35821;&#35328;&#30340;&#28145;&#21051;&#35821;&#35328;&#30693;&#35782;&#65292;&#20197;&#33267;&#20110;&#23427;&#20204;&#21487;&#20197;&#20805;&#24403;"&#19987;&#23478;&#35821;&#35328;&#27880;&#37322;&#32773;"&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#23519;&#20102;GPT-3&#12289;ChatGPT&#21644;GPT-4&#27169;&#22411;&#22312;&#21477;&#23376;&#24847;&#20041;&#32467;&#26500;&#20998;&#26512;&#20013;&#30340;&#25104;&#21151;&#21644;&#38480;&#21046;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#65288;AMR&#65307;Banarescu&#31561;&#20154;&#65292;2013&#65289;&#20998;&#26512;&#24418;&#24335;&#20027;&#20041;&#65292;&#35813;&#24418;&#24335;&#20027;&#20041;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#22270;&#24418;&#21270;&#21477;&#23376;&#24847;&#20041;&#32467;&#26500;&#34920;&#31034;&#65292;&#21516;&#26102;&#20174;&#34920;&#38754;&#24418;&#24335;&#20013;&#25277;&#35937;&#20986;&#26469;&#12290;&#25105;&#20204;&#23558;&#27169;&#22411;&#22312;&#36825;&#31181;&#35821;&#20041;&#32467;&#26500;&#20998;&#26512;&#19978;&#30340;&#32467;&#26524;&#22312;&#20004;&#31181;&#24773;&#20917;&#19979;&#36827;&#34892;&#27604;&#36739;&#65306;1&#65289;&#22522;&#20110;&#38646;&#23556;&#21644;&#23569;&#23398;&#26679;&#26412;&#30340;AMR&#35299;&#26512;&#30340;&#30452;&#25509;&#29983;&#25104;&#65292;&#20197;&#21450;2&#65289;&#36890;&#36807;&#20803;&#35821;&#35328;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#65288;&#20363;&#22914;"&#30830;&#23450;&#35813;&#21477;&#23376;&#30340;&#20027;&#35201;&#20107;&#20214;&#65292;&#20197;&#21450;&#19982;&#35813;&#20107;&#20214;&#23545;&#24212;&#30340;&#35859;&#35789;"&#65289;&#38388;&#25509;&#30340;&#37096;&#20998;&#37325;&#26500;AMR&#12290;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#33021;&#22815;&#37325;&#26032;&#29983;&#25104;&#27491;&#30830;&#30340;AMR&#35299;&#26512;&#65292;&#20294;&#22312;&#22797;&#26434;&#30340;&#21477;&#23376;&#32467;&#26500;&#25110;&#35821;&#20041;&#25512;&#29702;&#20219;&#21153;&#20013;&#20173;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) show amazing proficiency and fluency in the use of language. Does this mean that they have also acquired insightful linguistic knowledge about the language, to an extent that they can serve as an "expert linguistic annotator"? In this paper, we examine the successes and limitations of the GPT-3, ChatGPT, and GPT-4 models in analysis of sentence meaning structure, focusing on the Abstract Meaning Representation (AMR; Banarescu et al. 2013) parsing formalism, which provides rich graphical representations of sentence meaning structure while abstracting away from surface forms. We compare models' analysis of this semantic structure across two settings: 1) direct production of AMR parses based on zero- and few-shot prompts, and 2) indirect partial reconstruction of AMR via metalinguistic natural language queries (e.g., "Identify the primary event of this sentence, and the predicate corresponding to that event."). Across these settings, we find that models can re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33021;&#37327;&#36127;&#33655;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#25552;&#31034;&#25216;&#26415;&#21644;&#33258;&#22238;&#24402;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#33021;&#28304;&#28040;&#32791;&#25968;&#25454;&#36716;&#21270;&#20026;&#25551;&#36848;&#24615;&#35821;&#21477;&#24182;&#23454;&#29616;&#39044;&#27979;&#26410;&#26469;&#33021;&#37327;&#36127;&#33655;&#28040;&#32791;&#30340;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#20026;&#25552;&#39640;&#33021;&#28304;&#25928;&#29575;&#21644;&#20419;&#36827;&#33021;&#28304;&#31995;&#32479;&#26234;&#33021;&#20915;&#31574;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#12290;</title><link>http://arxiv.org/abs/2310.17788</link><description>&lt;p&gt;
&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33021;&#37327;&#36127;&#33655;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Utilizing Language Models for Energy Load Forecasting. (arXiv:2310.17788v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33021;&#37327;&#36127;&#33655;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#25552;&#31034;&#25216;&#26415;&#21644;&#33258;&#22238;&#24402;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#33021;&#28304;&#28040;&#32791;&#25968;&#25454;&#36716;&#21270;&#20026;&#25551;&#36848;&#24615;&#35821;&#21477;&#24182;&#23454;&#29616;&#39044;&#27979;&#26410;&#26469;&#33021;&#37327;&#36127;&#33655;&#28040;&#32791;&#30340;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#20026;&#25552;&#39640;&#33021;&#28304;&#25928;&#29575;&#21644;&#20419;&#36827;&#33021;&#28304;&#31995;&#32479;&#26234;&#33021;&#20915;&#31574;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#37327;&#36127;&#33655;&#39044;&#27979;&#22312;&#20248;&#21270;&#36164;&#28304;&#20998;&#37197;&#21644;&#31649;&#29702;&#24314;&#31569;&#29289;&#21644;&#22478;&#24066;&#30340;&#33021;&#28304;&#28040;&#32791;&#26041;&#38754;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33021;&#37327;&#36127;&#33655;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#37319;&#29992;&#25552;&#31034;&#25216;&#26415;&#23558;&#33021;&#28304;&#28040;&#32791;&#25968;&#25454;&#36716;&#21270;&#20026;&#25551;&#36848;&#24615;&#35821;&#21477;&#65292;&#20174;&#32780;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#12290;&#36890;&#36807;&#37319;&#29992;&#33258;&#22238;&#24402;&#29983;&#25104;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#39044;&#27979;&#26410;&#26469;&#33021;&#37327;&#36127;&#33655;&#28040;&#32791;&#30340;&#21508;&#31181;&#26102;&#38388;&#33539;&#22260;&#12290;&#36890;&#36807;&#23545;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33021;&#37327;&#36127;&#33655;&#39044;&#27979;&#26377;&#26395;&#22686;&#24378;&#33021;&#28304;&#25928;&#29575;&#24182;&#20419;&#36827;&#33021;&#28304;&#31995;&#32479;&#26234;&#33021;&#20915;&#31574;&#30340;&#23454;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy load forecasting plays a crucial role in optimizing resource allocation and managing energy consumption in buildings and cities. In this paper, we propose a novel approach that leverages language models for energy load forecasting. We employ prompting techniques to convert energy consumption data into descriptive sentences, enabling fine-tuning of language models. By adopting an autoregressive generating approach, our proposed method enables predictions of various horizons of future energy load consumption. Through extensive experiments on real-world datasets, we demonstrate the effectiveness and accuracy of our proposed method. Our results indicate that utilizing language models for energy load forecasting holds promise for enhancing energy efficiency and facilitating intelligent decision-making in energy systems.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21360;&#24230;&#35821;LGBTI+&#35789;&#27719;&#34920;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#29616;&#26377;&#30340;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#26377;&#25928;&#26816;&#27979;&#28508;&#34255;&#30340;&#20167;&#24680;&#20869;&#23481;&#65292;&#20351;&#29992;&#26426;&#22120;&#32763;&#35793;&#20316;&#20026;&#35780;&#20272;&#25163;&#27573;&#20063;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17787</link><description>&lt;p&gt;
&#35780;&#20272;&#20351;&#29992;&#21360;&#24230;&#35821;LGBTI+&#35789;&#27719;&#34920;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Evaluation of large language models using an Indian language LGBTI+ lexicon. (arXiv:2310.17787v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17787
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21360;&#24230;&#35821;LGBTI+&#35789;&#27719;&#34920;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#29616;&#26377;&#30340;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#26377;&#25928;&#26816;&#27979;&#28508;&#34255;&#30340;&#20167;&#24680;&#20869;&#23481;&#65292;&#20351;&#29992;&#26426;&#22120;&#32763;&#35793;&#20316;&#20026;&#35780;&#20272;&#25163;&#27573;&#20063;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#36890;&#36807;&#22522;&#20110;&#20219;&#21153;&#30340;&#22522;&#20934;&#65288;&#22914;MMLU&#65289;&#36827;&#34892;&#35780;&#20272;&#12290;&#36825;&#26679;&#30340;&#22522;&#20934;&#26080;&#27861;&#22312;&#29305;&#23450;&#35821;&#22659;&#20013;&#26816;&#26597;LLMs&#30340;&#36127;&#36131;&#20219;&#34892;&#20026;&#12290;&#22312;LGBTI+&#35821;&#22659;&#20013;&#65292;&#31038;&#20250;&#38472;&#35268;&#21487;&#33021;&#23548;&#33268;LGBTI+&#26415;&#35821;&#30340;&#21464;&#24322;&#12290;&#22240;&#27492;&#65292;&#39046;&#22495;&#29305;&#23450;&#30340;&#35789;&#27719;&#34920;&#21487;&#20197;&#20316;&#20026;&#23545;LLM&#34892;&#20026;&#36827;&#34892;&#35780;&#20272;&#30340;&#20195;&#34920;&#24615;&#21333;&#35789;&#21015;&#34920;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21360;&#24230;&#35821;LGBTI+&#35789;&#27719;&#34920;&#35780;&#20272;LLMs&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#22235;&#20010;&#27493;&#39588;&#65306;&#21046;&#23450;&#19982;&#39044;&#26399;&#34892;&#20026;&#30456;&#20851;&#30340;NLP&#20219;&#21153;&#65292;&#21019;&#24314;&#27979;&#35797;LLMs&#30340;&#25552;&#31034;&#65292;&#20351;&#29992;LLMs&#33719;&#21462;&#36755;&#20986;&#65292;&#26368;&#21518;&#36827;&#34892;&#25163;&#21160;&#35780;&#20272;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#23450;&#24615;&#20998;&#26512;&#26174;&#31034;&#65292;&#25105;&#20204;&#23454;&#39564;&#30340;&#19977;&#20010;LLMs&#26080;&#27861;&#26816;&#27979;&#21040;&#28508;&#22312;&#30340;&#20167;&#24680;&#20869;&#23481;&#12290;&#21516;&#26679;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20351;&#29992;&#26426;&#22120;&#32763;&#35793;&#20316;&#20026;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#30340;&#25163;&#27573;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are typically evaluated on the basis of task-based benchmarks such as MMLU. Such benchmarks do not examine responsible behaviour of LLMs in specific contexts. This is particularly true in the LGBTI+ context where social stereotypes may result in variation in LGBTI+ terminology. Therefore, domain-specific lexicons or dictionaries may be useful as a representative list of words against which the LLM's behaviour needs to be evaluated. This paper presents a methodology for evaluation of LLMs using an LGBTI+ lexicon in Indian languages. The methodology consists of four steps: formulating NLP tasks relevant to the expected behaviour, creating prompts that test LLMs, using the LLMs to obtain the output and, finally, manually evaluating the results. Our qualitative analysis shows that the three LLMs we experiment on are unable to detect underlying hateful content. Similarly, we observe limitations in using machine translation as means to evaluate natural language u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25968;&#25454;&#20013;&#24515;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#22788;&#29702;&#21644;&#39044;&#29702;&#35299;&#25968;&#25454;&#26469;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#37329;&#34701;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#37319;&#29992;&#35813;&#26041;&#27861;&#30340;&#37329;&#34701;LLMs&#22312;&#37329;&#34701;&#20998;&#26512;&#21644;&#35299;&#37322;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2310.17784</link><description>&lt;p&gt;
&#25968;&#25454;&#20013;&#24515;&#21270;&#30340;&#37329;&#34701;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Data-Centric Financial Large Language Models. (arXiv:2310.17784v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17784
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25968;&#25454;&#20013;&#24515;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#22788;&#29702;&#21644;&#39044;&#29702;&#35299;&#25968;&#25454;&#26469;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#37329;&#34701;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#37319;&#29992;&#35813;&#26041;&#27861;&#30340;&#37329;&#34701;LLMs&#22312;&#37329;&#34701;&#20998;&#26512;&#21644;&#35299;&#37322;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#28508;&#21147;&#65292;&#20294;&#30452;&#25509;&#24212;&#29992;&#20110;&#22797;&#26434;&#39046;&#22495;&#22914;&#37329;&#34701;&#26102;&#21364;&#36935;&#21040;&#22256;&#38590;&#12290;LLMs&#38590;&#20197;&#25512;&#29702;&#21644;&#25972;&#21512;&#25152;&#26377;&#30456;&#20851;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#20013;&#24515;&#21270;&#30340;&#26041;&#27861;&#65292;&#20351;LLMs&#33021;&#22815;&#26356;&#22909;&#22320;&#22788;&#29702;&#37329;&#34701;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35266;&#28857;&#26159;&#65292;&#19981;&#26159;&#19968;&#27425;&#24615;&#32473;LLM&#36127;&#36733;&#36807;&#22810;&#20449;&#24687;&#65292;&#32780;&#26159;&#26356;&#26377;&#25928;&#22320;&#23545;&#25968;&#25454;&#36827;&#34892;&#39044;&#22788;&#29702;&#21644;&#39044;&#29702;&#35299;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#20219;&#21153;&#22522;&#20110;&#25552;&#31034;&#30340;&#24494;&#35843;&#26469;&#21019;&#24314;&#37329;&#34701;LLM&#65288;FLLM&#65289;&#65292;&#20197;&#23454;&#29616;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#39044;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#27599;&#20010;&#20219;&#21153;&#30340;&#26631;&#35760;&#25968;&#25454;&#26377;&#38480;&#12290;&#20026;&#20102;&#20811;&#26381;&#25163;&#21160;&#27880;&#37322;&#30340;&#25104;&#26412;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#33258;&#21160;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#22686;&#24378;&#25512;&#29702;&#65288;AAR&#65289;&#26469;&#20462;&#25913;FLLM&#33258;&#36523;&#36755;&#20986;&#30340;&#20266;&#26631;&#31614;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#20013;&#24515;&#21270;FLLM&#19982;AAR&#30456;&#27604;&#65292;&#26174;&#33879;&#20248;&#20110;&#20026;&#21407;&#22987;&#25991;&#26412;&#35774;&#35745;&#30340;&#22522;&#32447;&#37329;&#34701;LLMs&#65292;&#22312;&#37329;&#34701;&#20998;&#26512;&#21644;&#35299;&#37322;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) show promise for natural language tasks but struggle when applied directly to complex domains like finance. LLMs have difficulty reasoning about and integrating all relevant information. We propose a data-centric approach to enable LLMs to better handle financial tasks. Our key insight is that rather than overloading the LLM with everything at once, it is more effective to preprocess and pre-understand the data. We create a financial LLM (FLLM) using multitask prompt-based finetuning to achieve data pre-processing and pre-understanding. However, labeled data is scarce for each task. To overcome manual annotation costs, we employ abductive augmentation reasoning (AAR) to automatically generate training data by modifying the pseudo labels from FLLM's own outputs. Experiments show our data-centric FLLM with AAR substantially outperforms baseline financial LLMs designed for raw text, achieving state-of-the-art on financial analysis and interpretation tasks. We 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#19981;&#21516;&#20998;&#35789;&#26041;&#27861;&#23545;&#24778;&#22855;&#24230;&#20272;&#35745;&#21644;&#38405;&#35835;&#26102;&#38388;&#25968;&#25454;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20351;&#29992;BPE&#20998;&#35789;&#30340;&#39044;&#27979;&#22312;&#25972;&#20307;&#19978;&#19982;&#24418;&#24577;&#23398;&#21644;&#27491;&#23383;&#27861;&#20998;&#21106;&#30456;&#27604;&#27809;&#26377;&#21463;&#25439;&#65292;&#20294;&#32454;&#33268;&#20998;&#26512;&#25351;&#20986;&#20102;BPE&#20998;&#35789;&#30340;&#28508;&#22312;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24418;&#24577;&#23398;&#39044;&#27979;&#35780;&#20272;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.17774</link><description>&lt;p&gt;
&#35789;&#35821;&#12289;&#23376;&#35789;&#21644;&#35789;&#32032;&#65306;&#24778;&#22855;&#24230;-&#38405;&#35835;&#26102;&#38388;&#20851;&#31995;&#20013;&#30495;&#27491;&#37325;&#35201;&#30340;&#26159;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
Words, Subwords, and Morphemes: What Really Matters in the Surprisal-Reading Time Relationship?. (arXiv:2310.17774v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#19981;&#21516;&#20998;&#35789;&#26041;&#27861;&#23545;&#24778;&#22855;&#24230;&#20272;&#35745;&#21644;&#38405;&#35835;&#26102;&#38388;&#25968;&#25454;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20351;&#29992;BPE&#20998;&#35789;&#30340;&#39044;&#27979;&#22312;&#25972;&#20307;&#19978;&#19982;&#24418;&#24577;&#23398;&#21644;&#27491;&#23383;&#27861;&#20998;&#21106;&#30456;&#27604;&#27809;&#26377;&#21463;&#25439;&#65292;&#20294;&#32454;&#33268;&#20998;&#26512;&#25351;&#20986;&#20102;BPE&#20998;&#35789;&#30340;&#28508;&#22312;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24418;&#24577;&#23398;&#39044;&#27979;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;LLMs&#22312;&#24515;&#29702;&#35821;&#35328;&#23398;&#25968;&#25454;&#19978;&#30340;&#19968;&#20010;&#37325;&#35201;&#20551;&#35774;&#19968;&#30452;&#26410;&#32463;&#39564;&#35777;&#12290;LLM&#22522;&#20110;&#23376;&#35789;&#20998;&#35789;&#36827;&#34892;&#39044;&#27979;&#65292;&#32780;&#19981;&#26159;&#23558;&#21333;&#35789;&#20998;&#35299;&#20026;&#35789;&#32032;&#12290;&#36825;&#26159;&#21542;&#37325;&#35201;&#65311;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#20351;&#29992;&#27491;&#23383;&#27861;&#12289;&#24418;&#24577;&#23398;&#21644;BPE&#20998;&#35789;&#30340;&#24778;&#22855;&#24230;&#20272;&#35745;&#19982;&#38405;&#35835;&#26102;&#38388;&#25968;&#25454;&#36827;&#34892;&#20102;&#20180;&#32454;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#22797;&#21046;&#20102;&#20808;&#21069;&#30340;&#21457;&#29616;&#65292;&#24182;&#25552;&#20379;&#20102;&#35777;&#25454;&#65292;&#34920;&#26126;&#20351;&#29992;BPE&#20998;&#35789;&#30340;&#39044;&#27979;&#30456;&#23545;&#20110;&#24418;&#24577;&#23398;&#21644;&#27491;&#23383;&#27861;&#20998;&#21106;&#26469;&#35828;&#24182;&#27809;&#26377;&#21463;&#21040;&#25439;&#23475;&#12290;&#28982;&#32780;&#65292;&#26356;&#32454;&#33268;&#30340;&#20998;&#26512;&#25351;&#20986;&#20102;&#20381;&#36182;&#20110;BPE&#20998;&#35789;&#30340;&#28508;&#22312;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#28041;&#21450;&#24418;&#24577;&#23398;&#24863;&#30693;&#24778;&#22855;&#24230;&#20272;&#35745;&#30340;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#24182;&#24314;&#35758;&#20102;&#19968;&#31181;&#35780;&#20272;&#24418;&#24577;&#23398;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
An important assumption that comes with using LLMs on psycholinguistic data has gone unverified. LLM-based predictions are based on subword tokenization, not decomposition of words into morphemes. Does that matter? We carefully test this by comparing surprisal estimates using orthographic, morphological, and BPE tokenization against reading time data. Our results replicate previous findings and provide evidence that in the aggregate, predictions using BPE tokenization do not suffer relative to morphological and orthographic segmentation. However, a finer-grained analysis points to potential issues with relying on BPE-based tokenization, as well as providing promising results involving morphologically-aware surprisal estimates and suggesting a new method for evaluating morphological prediction.
&lt;/p&gt;</description></item><item><title>GROOViST&#26159;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;&#25925;&#20107;&#20013;&#29289;&#20307;&#23450;&#20301;&#30340;&#26032;&#30340;&#35780;&#20272;&#24037;&#20855;&#65292;&#32771;&#34385;&#20102;&#36328;&#27169;&#24577;&#20381;&#36182;&#12289;&#26102;&#38388;&#38169;&#20301;&#21644;&#20154;&#31867;&#23545;&#35270;&#35273;&#23450;&#20301;&#30340;&#30452;&#35273;&#12290;&#36825;&#31181;&#24037;&#20855;&#20855;&#26377;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#21487;&#20197;&#35780;&#20272;&#21644;&#35299;&#37322;&#27599;&#20010;&#32452;&#20214;&#30340;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2310.17770</link><description>&lt;p&gt;
GROOViST:&#19968;&#31181;&#29992;&#20110;&#35270;&#35273;&#25925;&#20107;&#20013;&#29289;&#20307;&#23450;&#20301;&#35780;&#20272;&#30340;&#24230;&#37327;&#26631;&#20934;
&lt;/p&gt;
&lt;p&gt;
GROOViST: A Metric for Grounding Objects in Visual Storytelling. (arXiv:2310.17770v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17770
&lt;/p&gt;
&lt;p&gt;
GROOViST&#26159;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;&#25925;&#20107;&#20013;&#29289;&#20307;&#23450;&#20301;&#30340;&#26032;&#30340;&#35780;&#20272;&#24037;&#20855;&#65292;&#32771;&#34385;&#20102;&#36328;&#27169;&#24577;&#20381;&#36182;&#12289;&#26102;&#38388;&#38169;&#20301;&#21644;&#20154;&#31867;&#23545;&#35270;&#35273;&#23450;&#20301;&#30340;&#30452;&#35273;&#12290;&#36825;&#31181;&#24037;&#20855;&#20855;&#26377;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#21487;&#20197;&#35780;&#20272;&#21644;&#35299;&#37322;&#27599;&#20010;&#32452;&#20214;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#19968;&#20010;&#30001;&#19968;&#31995;&#21015;&#22270;&#20687;&#29983;&#25104;&#30340;&#25925;&#20107;&#30340;&#36866;&#24403;&#35780;&#20272;&#65292;&#24517;&#39035;&#32771;&#34385;&#22810;&#20010;&#26041;&#38754;&#65292;&#20363;&#22914;&#36830;&#36143;&#24615;&#65292;&#35821;&#27861;&#27491;&#30830;&#24615;&#21644;&#35270;&#35273;&#23450;&#20301;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#35780;&#20272;&#23450;&#20301;&#31243;&#24230;&#65292;&#21363;&#25925;&#20107;&#19982;&#22270;&#20687;&#20013;&#26174;&#31034;&#30340;&#23454;&#20307;&#30456;&#20851;&#31243;&#24230;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#24403;&#21069;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#21253;&#25324;&#38024;&#23545;&#27492;&#30446;&#30340;&#35774;&#35745;&#30340;&#25351;&#26631;&#21644;&#38024;&#23545;&#19968;&#33324;&#35270;&#35273;-&#25991;&#26412;&#23545;&#40784;&#30340;&#25351;&#26631;&#12290;&#37492;&#20110;&#23427;&#20204;&#23384;&#22312;&#30340;&#32570;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#24037;&#20855;GROOViST&#65292;&#35813;&#24037;&#20855;&#32771;&#34385;&#20102;&#36328;&#27169;&#24577;&#20381;&#36182;&#65292;&#26102;&#38388;&#38169;&#20301;&#65288;&#25925;&#20107;&#20013;&#23454;&#20307;&#20986;&#29616;&#30340;&#39034;&#24207;&#21644;&#22270;&#20687;&#24207;&#21015;&#21487;&#33021;&#19981;&#21305;&#37197;&#65289;&#20197;&#21450;&#20154;&#31867;&#23545;&#35270;&#35273;&#23450;&#20301;&#30340;&#30452;&#35273;&#12290;GROOViST&#30340;&#21478;&#19968;&#20010;&#20248;&#28857;&#26159;&#20854;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#21487;&#20197;&#23545;&#27599;&#20010;&#32452;&#20214;&#30340;&#36129;&#29486;&#36827;&#34892;&#35780;&#20272;&#21644;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
A proper evaluation of stories generated for a sequence of images -- the task commonly referred to as visual storytelling -- must consider multiple aspects, such as coherence, grammatical correctness, and visual grounding. In this work, we focus on evaluating the degree of grounding, that is, the extent to which a story is about the entities shown in the images. We analyze current metrics, both designed for this purpose and for general vision-text alignment. Given their observed shortcomings, we propose a novel evaluation tool, GROOViST, that accounts for cross-modal dependencies, temporal misalignments (the fact that the order in which entities appear in the story and the image sequence may not match), and human intuitions on visual grounding. An additional advantage of GROOViST is its modular design, where the contribution of each component can be assessed and interpreted individually.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#36890;&#36807;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24212;&#29992;&#20110;&#29992;&#25143;&#20132;&#20114;&#25968;&#25454;&#65292;&#20351;AI&#21161;&#25163;&#33021;&#22815;&#33258;&#21160;&#23545;&#40784;&#29992;&#25143;&#20559;&#22909;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#34429;&#28982;AI&#21161;&#25163;&#22312;&#27169;&#25311;&#20013;&#33021;&#22815;&#20934;&#30830;&#23545;&#40784;&#32463;&#27982;&#25991;&#29486;&#20013;&#30340;&#26631;&#20934;&#31574;&#30053;&#65292;&#20294;&#22312;&#38754;&#23545;&#26410;&#30693;&#36135;&#24065;&#20197;&#21450;&#35821;&#35328;&#19982;&#31574;&#30053;&#19968;&#33268;&#24615;&#19981;&#36275;&#30340;&#24773;&#20917;&#19979;&#65292;&#20854;&#23398;&#20064;&#33021;&#21147;&#21463;&#21040;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.17769</link><description>&lt;p&gt;
&#31038;&#20250;&#22865;&#32422;AI&#65306;&#23558;AI&#21161;&#25163;&#19982;&#38544;&#21547;&#30340;&#32676;&#20307;&#35268;&#33539;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Social Contract AI: Aligning AI Assistants with Implicit Group Norms. (arXiv:2310.17769v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17769
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#36890;&#36807;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24212;&#29992;&#20110;&#29992;&#25143;&#20132;&#20114;&#25968;&#25454;&#65292;&#20351;AI&#21161;&#25163;&#33021;&#22815;&#33258;&#21160;&#23545;&#40784;&#29992;&#25143;&#20559;&#22909;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#34429;&#28982;AI&#21161;&#25163;&#22312;&#27169;&#25311;&#20013;&#33021;&#22815;&#20934;&#30830;&#23545;&#40784;&#32463;&#27982;&#25991;&#29486;&#20013;&#30340;&#26631;&#20934;&#31574;&#30053;&#65292;&#20294;&#22312;&#38754;&#23545;&#26410;&#30693;&#36135;&#24065;&#20197;&#21450;&#35821;&#35328;&#19982;&#31574;&#30053;&#19968;&#33268;&#24615;&#19981;&#36275;&#30340;&#24773;&#20917;&#19979;&#65292;&#20854;&#23398;&#20064;&#33021;&#21147;&#21463;&#21040;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#32034;&#20102;&#36890;&#36807;&#21453;&#36716;&#27169;&#25311;&#29992;&#25143;&#65288;&#26410;&#30693;&#65289;&#20559;&#22909;&#30340;&#27169;&#22411;&#26469;&#23545;&#40784;AI&#21161;&#25163;&#30340;&#24605;&#36335;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#25552;&#35758;&#65292;&#25105;&#20204;&#22312;&#32463;&#27982;&#25253;&#20215;&#28216;&#25103;&#20013;&#36827;&#34892;&#20102;&#27010;&#24565;&#39564;&#35777;&#27169;&#25311;&#65292;&#23558;&#29992;&#25143;&#20559;&#22909;&#24418;&#24335;&#21270;&#20026;&#25351;&#23548;&#27169;&#25311;&#29609;&#23478;&#34892;&#20026;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;AI&#21161;&#25163;&#33021;&#22815;&#20934;&#30830;&#22320;&#23558;&#20854;&#34892;&#20026;&#19982;&#32463;&#27982;&#25991;&#29486;&#20013;&#30340;&#26631;&#20934;&#31574;&#30053;&#65288;&#22914;&#33258;&#31169;&#30340;&#12289;&#21033;&#20182;&#30340;&#65289;&#30456;&#21305;&#37197;&#12290;&#28982;&#32780;&#65292;&#21161;&#25163;&#23398;&#21040;&#30340;&#31574;&#30053;&#22312;&#38754;&#23545;&#26410;&#21253;&#21547;&#22312;&#35757;&#32451;&#20998;&#24067;&#20013;&#30340;&#36135;&#24065;&#65288;&#22914;&#33647;&#21697;&#20811;&#25968;&#65289;&#26102;&#32570;&#20047;&#40065;&#26834;&#24615;&#21644;&#26377;&#38480;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;&#35821;&#35328;&#20351;&#29992;&#19982;&#26410;&#30693;&#31574;&#30053;&#20043;&#38388;&#23384;&#22312;&#19968;&#33268;&#24615;&#19981;&#36275;&#26102;&#65288;&#22914;&#21033;&#20182;&#31574;&#30053;&#19982;&#31895;&#40065;&#35821;&#35328;&#30456;&#32467;&#21512;&#65289;&#65292;&#21161;&#25163;&#23398;&#20064;&#21040;&#30340;&#31574;&#30053;&#20250;&#20943;&#24930;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21021;&#27493;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#24320;&#21457;&#27169;&#25311;&#26694;&#26550;&#26469;&#23545;&#40784;AI&#21161;&#25163;&#30340;&#34892;&#20026;&#26159;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the idea of aligning an AI assistant by inverting a model of users' (unknown) preferences from observed interactions. To validate our proposal, we run proof-of-concept simulations in the economic ultimatum game, formalizing user preferences as policies that guide the actions of simulated players. We find that the AI assistant accurately aligns its behavior to match standard policies from the economic literature (e.g., selfish, altruistic). However, the assistant's learned policies lack robustness and exhibit limited generalization in an out-of-distribution setting when confronted with a currency (e.g., grams of medicine) that was not included in the assistant's training distribution. Additionally, we find that when there is inconsistency in the relationship between language use and an unknown policy (e.g., an altruistic policy combined with rude language), the assistant's learning of the policy is slowed. Overall, our preliminary results suggest that developing simulation fr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#27979;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21450;&#20854;&#30456;&#20851;&#20135;&#21697;&#21644;&#26381;&#21153;&#20013;&#26377;&#23475;&#36131;&#20219;&#30340;&#26694;&#26550;&#12290;&#36890;&#36807;&#35813;&#26694;&#26550;&#65292;&#21487;&#20197;&#35843;&#26597;&#19981;&#21516;&#30340;LLMs&#22914;&#20309;&#36829;&#21453;&#19968;&#31995;&#21015;&#19982;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#65288;RAI&#65289;&#30456;&#20851;&#30340;&#21407;&#21017;&#65292;&#24182;&#20419;&#36827;LLMs&#30340;&#36127;&#36131;&#20219;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.17750</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#27979;&#37327;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#26377;&#23475;&#36131;&#20219;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Framework for Automated Measurement of Responsible AI Harms in Generative AI Applications. (arXiv:2310.17750v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#27979;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21450;&#20854;&#30456;&#20851;&#20135;&#21697;&#21644;&#26381;&#21153;&#20013;&#26377;&#23475;&#36131;&#20219;&#30340;&#26694;&#26550;&#12290;&#36890;&#36807;&#35813;&#26694;&#26550;&#65292;&#21487;&#20197;&#35843;&#26597;&#19981;&#21516;&#30340;LLMs&#22914;&#20309;&#36829;&#21453;&#19968;&#31995;&#21015;&#19982;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#65288;RAI&#65289;&#30456;&#20851;&#30340;&#21407;&#21017;&#65292;&#24182;&#20419;&#36827;LLMs&#30340;&#36127;&#36131;&#20219;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#21270;&#27979;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21450;&#20854;&#30456;&#20851;&#20135;&#21697;&#21644;&#26381;&#21153;&#30340;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#65288;RAI&#65289;&#25351;&#26631;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#22522;&#20110;&#29616;&#26377;&#30340;&#25216;&#26415;&#21644;&#31038;&#20250;&#25216;&#26415;&#19987;&#38376;&#30693;&#35782;&#65292;&#21033;&#29992;&#20808;&#36827;&#30340;LLMs&#33021;&#21147;&#65288;&#22914;GPT-4&#65289;&#65292;&#24314;&#31435;&#20102;&#33258;&#21160;&#27979;&#37327;LLMs&#36896;&#25104;&#20260;&#23475;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#20351;&#29992;&#35813;&#26694;&#26550;&#36890;&#36807;&#20960;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#35843;&#26597;&#19981;&#21516;&#30340;LLMs&#22914;&#20309;&#36829;&#21453;&#19968;&#31995;&#21015;&#19982;RAI&#30456;&#20851;&#30340;&#21407;&#21017;&#12290;&#35813;&#26694;&#26550;&#21487;&#19982;&#39046;&#22495;&#29305;&#23450;&#30340;&#31038;&#20250;&#25216;&#26415;&#19987;&#38376;&#30693;&#35782;&#32467;&#21512;&#20351;&#29992;&#65292;&#20197;&#20415;&#22312;&#26410;&#26469;&#21019;&#24314;&#26032;&#30340;&#20260;&#23475;&#21306;&#22495;&#30340;&#34913;&#37327;&#25351;&#26631;&#12290;&#36890;&#36807;&#23454;&#26045;&#35813;&#26694;&#26550;&#65292;&#25105;&#20204;&#26088;&#22312;&#20419;&#36827;&#26356;&#39640;&#32423;&#30340;&#20260;&#23475;&#27979;&#37327;&#24037;&#20316;&#65292;&#24182;&#36827;&#19968;&#27493;&#25512;&#21160;LLMs&#30340;&#36131;&#20219;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a framework for the automated measurement of responsible AI (RAI) metrics for large language models (LLMs) and associated products and services. Our framework for automatically measuring harms from LLMs builds on existing technical and sociotechnical expertise and leverages the capabilities of state-of-the-art LLMs, such as GPT-4. We use this framework to run through several case studies investigating how different LLMs may violate a range of RAI-related principles. The framework may be employed alongside domain-specific sociotechnical expertise to create measurements for new harm areas in the future. By implementing this framework, we aim to enable more advanced harm measurement efforts and further the responsible use of LLMs.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#20013;&#25945;&#32946;&#20215;&#20540;&#30340;&#20316;&#29992;&#65292;&#36890;&#36807;&#27604;&#36739;&#38144;&#21806;&#20154;&#21592;&#21644;SalesBot&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#34429;&#28982;SalesBot&#22312;&#27969;&#30021;&#24615;&#21644;&#20449;&#24687;&#37327;&#26041;&#38754;&#25509;&#36817;&#19987;&#19994;&#38144;&#21806;&#20154;&#21592;&#65292;&#20294;&#22312;&#25512;&#33616;&#36136;&#37327;&#26041;&#38754;&#20173;&#23384;&#22312;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2310.17749</link><description>&lt;p&gt;
&#38144;&#21806;&#20154;&#21592; vs SalesBot&#65306;&#25506;&#32034;&#25945;&#32946;&#20215;&#20540;&#22312;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Salespeople vs SalesBot: Exploring the Role of Educational Value in Conversational Recommender Systems. (arXiv:2310.17749v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17749
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#20013;&#25945;&#32946;&#20215;&#20540;&#30340;&#20316;&#29992;&#65292;&#36890;&#36807;&#27604;&#36739;&#38144;&#21806;&#20154;&#21592;&#21644;SalesBot&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#34429;&#28982;SalesBot&#22312;&#27969;&#30021;&#24615;&#21644;&#20449;&#24687;&#37327;&#26041;&#38754;&#25509;&#36817;&#19987;&#19994;&#38144;&#21806;&#20154;&#21592;&#65292;&#20294;&#22312;&#25512;&#33616;&#36136;&#37327;&#26041;&#38754;&#20173;&#23384;&#22312;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#34892;&#22823;&#39069;&#36141;&#20080;&#38656;&#35201;&#28040;&#36153;&#32773;&#36827;&#34892;&#30740;&#31350;&#25110;&#21672;&#35810;&#38144;&#21806;&#20154;&#21592;&#20197;&#33719;&#21462;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#65288;CRS&#65289;&#24448;&#24448;&#24573;&#35270;&#29992;&#25143;&#32570;&#20047;&#32972;&#26223;&#30693;&#35782;&#65292;&#20165;&#20851;&#27880;&#20110;&#25910;&#38598;&#20559;&#22909;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20026;&#26088;&#22312;&#36890;&#36807;&#28151;&#21512;&#22411;&#28151;&#21512;&#27169;&#24335;&#23545;&#35805;&#25552;&#20379;&#20135;&#21697;&#25512;&#33616;&#21644;&#25945;&#32946;&#20215;&#20540;&#30340;&#20250;&#35805;&#20195;&#29702;&#23450;&#20041;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#31354;&#38388;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;SalesOps&#65292;&#36825;&#26159;&#19968;&#20010;&#20511;&#37492;&#26368;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#27169;&#25311;&#21644;&#35780;&#20272;&#36825;&#31181;&#31995;&#32479;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;SalesBot&#21644;ShopperBot&#65292;&#36825;&#26159;&#19968;&#23545;LLM&#39537;&#21160;&#30340;&#20195;&#29702;&#65292;&#21487;&#20197;&#27169;&#25311;&#26694;&#26550;&#30340;&#20219;&#24847;&#19968;&#20391;&#12290;&#36890;&#36807;&#19968;&#39033;&#20840;&#38754;&#30340;&#20154;&#31867;&#30740;&#31350;&#65292;&#25105;&#20204;&#23558;SalesBot&#19982;&#19987;&#19994;&#38144;&#21806;&#20154;&#21592;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;&#34429;&#28982;SalesBot&#22312;&#27969;&#30021;&#24615;&#21644;&#20449;&#24687;&#37327;&#26041;&#38754;&#25509;&#36817;&#19987;&#19994;&#34920;&#29616;&#65292;&#20294;&#22312;&#25512;&#33616;&#36136;&#37327;&#26041;&#38754;&#21364;&#33853;&#21518;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#20004;&#32773;&#38754;&#20020;&#30340;&#19981;&#21516;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Making big purchases requires consumers to research or consult a salesperson to gain domain expertise. However, existing conversational recommender systems (CRS) often overlook users' lack of background knowledge, focusing solely on gathering preferences. In this work, we define a new problem space for conversational agents that aim to provide both product recommendations and educational value through mixed-type mixed-initiative dialog. We introduce SalesOps, a framework that facilitates the simulation and evaluation of such systems by leveraging recent advancements in large language models (LLMs). We build SalesBot and ShopperBot, a pair of LLM-powered agents that can simulate either side of the framework. A comprehensive human study compares SalesBot against professional salespeople, revealing that although SalesBot approaches professional performance in terms of fluency and informativeness, it lags behind in recommendation quality. We emphasize the distinct limitations both face in 
&lt;/p&gt;</description></item><item><title>StyleBART&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#39118;&#26684;&#21270;&#26631;&#39064;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36866;&#37197;&#22120;&#26469;&#35013;&#39280;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#22810;&#26679;&#39118;&#26684;&#30340;&#26631;&#39064;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#19981;&#21516;&#65292;StyleBART&#23558;&#39118;&#26684;&#23398;&#20064;&#21644;&#26631;&#39064;&#29983;&#25104;&#20219;&#21153;&#20998;&#31163;&#24320;&#26469;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21487;&#20197;&#33258;&#30001;&#32452;&#21512;&#22522;&#30784;&#27169;&#22411;&#21644;&#39118;&#26684;&#36866;&#37197;&#22120;&#12290;&#32463;&#36807;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;StyleBART&#34920;&#29616;&#20986;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.17743</link><description>&lt;p&gt;
StyleBART: &#20351;&#29992;&#39118;&#26684;&#36866;&#37197;&#22120;&#35013;&#39280;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#26080;&#30417;&#30563;&#39118;&#26684;&#21270;&#26631;&#39064;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
StyleBART: Decorate Pretrained Model with Style Adapters for Unsupervised Stylistic Headline Generation. (arXiv:2310.17743v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17743
&lt;/p&gt;
&lt;p&gt;
StyleBART&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#39118;&#26684;&#21270;&#26631;&#39064;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36866;&#37197;&#22120;&#26469;&#35013;&#39280;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#22810;&#26679;&#39118;&#26684;&#30340;&#26631;&#39064;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#19981;&#21516;&#65292;StyleBART&#23558;&#39118;&#26684;&#23398;&#20064;&#21644;&#26631;&#39064;&#29983;&#25104;&#20219;&#21153;&#20998;&#31163;&#24320;&#26469;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21487;&#20197;&#33258;&#30001;&#32452;&#21512;&#22522;&#30784;&#27169;&#22411;&#21644;&#39118;&#26684;&#36866;&#37197;&#22120;&#12290;&#32463;&#36807;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;StyleBART&#34920;&#29616;&#20986;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39118;&#26684;&#21270;&#26631;&#39064;&#29983;&#25104;&#20219;&#21153;&#26159;&#29983;&#25104;&#19968;&#20010;&#26082;&#24635;&#32467;&#25991;&#31456;&#20869;&#23481;&#21448;&#21453;&#26144;&#25152;&#38656;&#39118;&#26684;&#26469;&#21560;&#24341;&#29992;&#25143;&#30340;&#26631;&#39064;&#12290;&#30001;&#20110;&#39118;&#26684;&#29305;&#23450;&#30340;&#25991;&#31456;-&#26631;&#39064;&#23545;&#38750;&#24120;&#31232;&#32570;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#20110;&#20351;&#29992;&#26631;&#20934;&#26631;&#39064;&#29983;&#25104;&#25968;&#25454;&#38598;&#21644;&#21333;&#19968;&#39118;&#26684;&#35821;&#26009;&#24211;&#36827;&#34892;&#26080;&#30417;&#30563;&#26041;&#27861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36981;&#24490;&#36825;&#19968;&#36335;&#32447;&#65292;&#24182;&#25552;&#20986;&#20102;StyleBART&#65292;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#39118;&#26684;&#21270;&#26631;&#39064;&#29983;&#25104;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#36866;&#37197;&#22120;&#23558;&#39044;&#35757;&#32451;&#30340;BART&#27169;&#22411;&#35013;&#39280;&#36215;&#26469;&#65292;&#36866;&#37197;&#22120;&#36127;&#36131;&#19981;&#21516;&#30340;&#39118;&#26684;&#65292;&#36890;&#36807;&#31616;&#21333;&#22320;&#20999;&#25442;&#36866;&#37197;&#22120;&#65292;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#22810;&#26679;&#39118;&#26684;&#30340;&#26631;&#39064;&#12290;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;StyleBART&#23558;&#39118;&#26684;&#23398;&#20064;&#21644;&#26631;&#39064;&#29983;&#25104;&#30340;&#20219;&#21153;&#20998;&#31163;&#24320;&#26469;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21487;&#20197;&#33258;&#30001;&#32452;&#21512;&#22522;&#30784;&#27169;&#22411;&#21644;&#39118;&#26684;&#36866;&#37197;&#22120;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#36870;&#21521;&#25913;&#20889;&#20219;&#21153;&#20197;&#22686;&#24378;&#39118;&#26684;&#36866;&#37197;&#22120;&#30340;&#25928;&#26524;&#12290;&#24191;&#27867;&#30340;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;StyleBART&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stylistic headline generation is the task to generate a headline that not only summarizes the content of an article, but also reflects a desired style that attracts users. As style-specific article-headline pairs are scarce, previous researches focus on unsupervised approaches with a standard headline generation dataset and mono-style corpora. In this work, we follow this line and propose StyleBART, an unsupervised approach for stylistic headline generation. Our method decorates the pretrained BART model with adapters that are responsible for different styles and allows the generation of headlines with diverse styles by simply switching the adapters. Different from previous works, StyleBART separates the task of style learning and headline generation, making it possible to freely combine the base model and the style adapters during inference. We further propose an inverse paraphrasing task to enhance the style adapters. Extensive automatic and human evaluations show that StyleBART achi
&lt;/p&gt;</description></item><item><title>ArchBERT&#26159;&#19968;&#31181;&#21452;&#27169;&#24577;&#27169;&#22411;&#65292;&#21487;&#20197;&#32852;&#21512;&#23398;&#20064;&#21644;&#29702;&#35299;&#31070;&#32463;&#32467;&#26500;&#21644;&#33258;&#28982;&#35821;&#35328;&#65292;&#20026;&#31070;&#32463;&#26550;&#26500;&#21644;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#25552;&#20379;&#24555;&#36895;&#30340;&#32467;&#26500;-&#25991;&#26412;&#21644;&#25991;&#26412;-&#32467;&#26500;&#26816;&#32034;/&#29983;&#25104;&#26381;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.17737</link><description>&lt;p&gt;
ArchBERT: &#31070;&#32463;&#32467;&#26500;&#21644;&#33258;&#28982;&#35821;&#35328;&#30340;&#21452;&#27169;&#24577;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
ArchBERT: Bi-Modal Understanding of Neural Architectures and Natural Languages. (arXiv:2310.17737v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17737
&lt;/p&gt;
&lt;p&gt;
ArchBERT&#26159;&#19968;&#31181;&#21452;&#27169;&#24577;&#27169;&#22411;&#65292;&#21487;&#20197;&#32852;&#21512;&#23398;&#20064;&#21644;&#29702;&#35299;&#31070;&#32463;&#32467;&#26500;&#21644;&#33258;&#28982;&#35821;&#35328;&#65292;&#20026;&#31070;&#32463;&#26550;&#26500;&#21644;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#25552;&#20379;&#24555;&#36895;&#30340;&#32467;&#26500;-&#25991;&#26412;&#21644;&#25991;&#26412;-&#32467;&#26500;&#26816;&#32034;/&#29983;&#25104;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24314;&#31435;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#36235;&#21183;&#65292;&#39069;&#22806;&#30340;&#27169;&#24577;&#65292;&#22914;&#22270;&#20687;&#12289;&#35270;&#39057;&#12289;&#35821;&#38899;&#31561;&#19982;&#33258;&#28982;&#35821;&#35328;&#65288;&#21363;&#25991;&#26412;&#20449;&#24687;&#65289;&#19968;&#36215;&#36827;&#34892;&#32852;&#21512;&#23398;&#20064;&#12290;&#34429;&#28982;&#36825;&#20123;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#30340;&#27169;&#24577;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#26159;&#20851;&#20110;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#21644;&#33258;&#28982;&#35821;&#35328;&#30340;&#35299;&#20915;&#26041;&#26696;&#36824;&#19981;&#23384;&#22312;&#12290;&#23558;&#31070;&#32463;&#32467;&#26500;&#20449;&#24687;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#27169;&#24577;&#21487;&#20197;&#22312;&#20113;&#31471;&#25552;&#20379;&#24555;&#36895;&#30340;&#32467;&#26500;-&#25991;&#26412;&#21644;&#25991;&#26412;-&#32467;&#26500;&#26816;&#32034;/&#29983;&#25104;&#26381;&#21153;&#65292;&#19988;&#20165;&#38656;&#21333;&#27425;&#25512;&#29702;&#12290;&#36825;&#26679;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#24110;&#21161;&#21021;&#23398;&#32773;&#21644;&#20013;&#32423;&#26426;&#22120;&#23398;&#20064;&#29992;&#25143;&#25552;&#20986;&#26356;&#22909;&#30340;&#31070;&#32463;&#26550;&#26500;&#25110;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26041;&#38754;&#20855;&#26377;&#20215;&#20540;&#65292;&#21482;&#38656;&#20351;&#29992;&#31616;&#21333;&#30340;&#25991;&#26412;&#26597;&#35810;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ArchBERT&#30340;&#21452;&#27169;&#24577;&#27169;&#22411;&#65292;&#29992;&#20110;&#32852;&#21512;&#23398;&#20064;&#21644;&#29702;&#35299;&#31070;&#32463;&#32467;&#26500;&#21644;&#33258;&#28982;&#35821;&#35328;&#65292;&#20026;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Mask&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building multi-modal language models has been a trend in the recent years, where additional modalities such as image, video, speech, etc. are jointly learned along with natural languages (i.e., textual information). Despite the success of these multi-modal language models with different modalities, there is no existing solution for neural network architectures and natural languages. Providing neural architectural information as a new modality allows us to provide fast architecture-2-text and text-2-architecture retrieval/generation services on the cloud with a single inference. Such solution is valuable in terms of helping beginner and intermediate ML users to come up with better neural architectures or AutoML approaches with a simple text query. In this paper, we propose ArchBERT, a bi-modal model for joint learning and understanding of neural architectures and natural languages, which opens up new avenues for research in this area. We also introduce a pre-training strategy named Mask
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#36890;&#29992;&#27880;&#37322;&#26041;&#27861;&#30740;&#31350;&#22810;&#35821;&#35328;&#20849;&#25351;&#28040;&#35299;&#20219;&#21153;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#30740;&#31350;&#19981;&#21516;&#35821;&#35328;&#23618;&#27425;&#21644;&#27969;&#27966;&#19978;&#30340;&#30495;&#23454;&#25968;&#25454;&#26469;&#20102;&#35299;&#22810;&#35821;&#35328;&#20849;&#25351;&#30340;&#29305;&#28857;&#12290;&#20854;&#27425;&#65292;&#36827;&#34892;&#38169;&#35823;&#20998;&#26512;&#65292;&#24182;&#25552;&#21462;&#29305;&#24449;&#29992;&#20110;&#25913;&#36827;&#20849;&#25351;&#28040;&#35299;&#20219;&#21153;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#22522;&#32447;&#31995;&#32479;&#26377;&#28508;&#22312;&#22909;&#22788;&#12290;</title><link>http://arxiv.org/abs/2310.17734</link><description>&lt;p&gt;
&#35843;&#26597;&#22810;&#35821;&#35328;&#20849;&#25351;&#28040;&#35299;&#30340;&#36890;&#29992;&#27880;&#37322;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Investigating Multilingual Coreference Resolution by Universal Annotations. (arXiv:2310.17734v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17734
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#36890;&#29992;&#27880;&#37322;&#26041;&#27861;&#30740;&#31350;&#22810;&#35821;&#35328;&#20849;&#25351;&#28040;&#35299;&#20219;&#21153;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#30740;&#31350;&#19981;&#21516;&#35821;&#35328;&#23618;&#27425;&#21644;&#27969;&#27966;&#19978;&#30340;&#30495;&#23454;&#25968;&#25454;&#26469;&#20102;&#35299;&#22810;&#35821;&#35328;&#20849;&#25351;&#30340;&#29305;&#28857;&#12290;&#20854;&#27425;&#65292;&#36827;&#34892;&#38169;&#35823;&#20998;&#26512;&#65292;&#24182;&#25552;&#21462;&#29305;&#24449;&#29992;&#20110;&#25913;&#36827;&#20849;&#25351;&#28040;&#35299;&#20219;&#21153;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#22522;&#32447;&#31995;&#32479;&#26377;&#28508;&#22312;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#20849;&#25351;&#28040;&#35299;(MCR)&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#21033;&#29992;&#26368;&#26032;&#25552;&#20986;&#30340;&#22810;&#35821;&#35328;&#20849;&#25351;&#25968;&#25454;&#38598;CorefUD(Nedoluzhko et al., 2022)&#30340;&#36890;&#29992;&#24418;&#24577;&#21477;&#27861;&#21644;&#20849;&#25351;&#27880;&#37322;&#65292;&#23545;&#35813;&#20219;&#21153;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#19981;&#21516;&#35821;&#35328;&#23618;&#27425;&#65288;&#25552;&#21450;&#12289;&#23454;&#20307;&#21644;&#25991;&#26723;&#23618;&#27425;&#65289;&#21644;&#19981;&#21516;&#27969;&#27966;&#19978;&#26816;&#26597;&#30495;&#23454;&#25968;&#25454;&#65292;&#26469;&#20102;&#35299;&#22810;&#35821;&#35328;&#20849;&#25351;&#30340;&#29305;&#28857;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23545;&#22312;CRAC 2022&#20849;&#20139;&#20219;&#21153;&#20013;State-of-the-Art&#31995;&#32479;&#26080;&#27861;&#35299;&#20915;&#30340;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#26696;&#20363;&#36827;&#34892;&#20102;&#38169;&#35823;&#20998;&#26512;&#65292;&#20351;&#29992;&#20102;&#36890;&#29992;&#27880;&#37322;&#12290;&#26368;&#21518;&#65292;&#22522;&#20110;&#36825;&#20010;&#20998;&#26512;&#65292;&#25105;&#20204;&#20174;&#36890;&#29992;&#24418;&#24577;&#21477;&#27861;&#27880;&#37322;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#24182;&#23558;&#36825;&#20123;&#29305;&#24449;&#25972;&#21512;&#21040;&#19968;&#20010;&#22522;&#32447;&#31995;&#32479;&#20013;&#65292;&#35780;&#20272;&#23427;&#20204;&#23545;MCR&#20219;&#21153;&#30340;&#28508;&#22312;&#22909;&#22788;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26368;&#20339;&#29305;&#24449;&#37197;&#32622;&#25913;&#36827;&#20102;&#22522;&#32447;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual coreference resolution (MCR) has been a long-standing and challenging task. With the newly proposed multilingual coreference dataset, CorefUD (Nedoluzhko et al., 2022), we conduct an investigation into the task by using its harmonized universal morphosyntactic and coreference annotations. First, we study coreference by examining the ground truth data at different linguistic levels, namely mention, entity and document levels, and across different genres, to gain insights into the characteristics of coreference across multiple languages. Second, we perform an error analysis of the most challenging cases that the SotA system fails to resolve in the CRAC 2022 shared task using the universal annotations. Last, based on this analysis, we extract features from universal morphosyntactic annotations and integrate these features into a baseline system to assess their potential benefits for the MCR task. Our results show that our best configuration of features improves the baseline b
&lt;/p&gt;</description></item><item><title>ZeroQuant-HERO&#26159;&#19968;&#31181;&#30828;&#20214;&#22686;&#24378;&#30340;&#37327;&#21270;&#26694;&#26550;&#65292;&#38024;&#23545;W8A8 Transformer&#27169;&#22411;&#36827;&#34892;&#20248;&#21270;&#65292;&#26088;&#22312;&#20943;&#23569;&#20869;&#23384;&#21644;&#35745;&#31639;&#38656;&#27714;&#65292;&#24182;&#22312;&#22788;&#29702;&#22797;&#26434;&#30340;&#37327;&#21270;&#38382;&#39064;&#21644;&#20869;&#23384;&#21463;&#38480;&#36816;&#31639;&#31526;&#26041;&#38754;&#25552;&#20379;&#20102;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#21516;&#26102;&#65292;&#35813;&#26694;&#26550;&#36824;&#20855;&#26377;&#28789;&#27963;&#24615;&#65292;&#20801;&#35768;&#29305;&#23450;&#27169;&#22359;&#20999;&#25442;&#33267;FP16/BF16&#27169;&#24335;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17723</link><description>&lt;p&gt;
ZeroQuant-HERO: W8A8 Transformer&#30340;&#30828;&#20214;&#22686;&#24378;&#30340;&#20248;&#21270;&#21518;&#35757;&#32451;&#37327;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
ZeroQuant-HERO: Hardware-Enhanced Robust Optimized Post-Training Quantization Framework for W8A8 Transformers. (arXiv:2310.17723v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17723
&lt;/p&gt;
&lt;p&gt;
ZeroQuant-HERO&#26159;&#19968;&#31181;&#30828;&#20214;&#22686;&#24378;&#30340;&#37327;&#21270;&#26694;&#26550;&#65292;&#38024;&#23545;W8A8 Transformer&#27169;&#22411;&#36827;&#34892;&#20248;&#21270;&#65292;&#26088;&#22312;&#20943;&#23569;&#20869;&#23384;&#21644;&#35745;&#31639;&#38656;&#27714;&#65292;&#24182;&#22312;&#22788;&#29702;&#22797;&#26434;&#30340;&#37327;&#21270;&#38382;&#39064;&#21644;&#20869;&#23384;&#21463;&#38480;&#36816;&#31639;&#31526;&#26041;&#38754;&#25552;&#20379;&#20102;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#21516;&#26102;&#65292;&#35813;&#26694;&#26550;&#36824;&#20855;&#26377;&#28789;&#27963;&#24615;&#65292;&#20801;&#35768;&#29305;&#23450;&#27169;&#22359;&#20999;&#25442;&#33267;FP16/BF16&#27169;&#24335;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#25216;&#26415;&#22312;&#20943;&#23569;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#38656;&#27714;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22914;ZeroQuant&#65292;&#20026;BERT&#21644;GPT&#31561;&#27169;&#22411;&#25552;&#20379;&#20102;&#21160;&#24577;&#37327;&#21270;&#65292;&#20294;&#24573;&#35270;&#20102;&#20851;&#38190;&#30340;&#20869;&#23384;&#21463;&#38480;&#36816;&#31639;&#31526;&#21644;&#27599;&#20010;&#26631;&#35760;&#30340;&#37327;&#21270;&#22797;&#26434;&#24615;&#12290;&#38024;&#23545;&#36825;&#20123;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#12289;&#23436;&#20840;&#30001;&#30828;&#20214;&#22686;&#24378;&#30340;&#12289;&#32463;&#36807;&#20248;&#21270;&#30340;&#12289;&#21518;&#35757;&#32451;W8A8&#37327;&#21270;&#26694;&#26550;ZeroQuant-HERO&#12290;&#35813;&#26694;&#26550;&#29420;&#29305;&#22320;&#38598;&#25104;&#20102;&#20869;&#23384;&#24102;&#23485;&#21644;&#35745;&#31639;&#23494;&#38598;&#22411;&#36816;&#31639;&#31526;&#65292;&#26088;&#22312;&#23454;&#29616;&#26368;&#20339;&#30828;&#20214;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#23427;&#36890;&#36807;&#20801;&#35768;&#29305;&#23450;&#30340;INT8&#27169;&#22359;&#20999;&#25442;&#21040;FP16/BF16&#27169;&#24335;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantization techniques are pivotal in reducing the memory and computational demands of deep neural network inference. Existing solutions, such as ZeroQuant, offer dynamic quantization for models like BERT and GPT but overlook crucial memory-bounded operators and the complexities of per-token quantization. Addressing these gaps, we present a novel, fully hardware-enhanced robust optimized post-training W8A8 quantization framework, ZeroQuant-HERO. This framework uniquely integrates both memory bandwidth and compute-intensive operators, aiming for optimal hardware performance. Additionally, it offers flexibility by allowing specific INT8 modules to switch to FP16/BF16 mode, enhancing accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21487;&#20197;&#34987;&#36866;&#24212;&#20026;&#20855;&#26377;&#26222;&#36866;&#24615;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#31574;&#30053;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;LLaRP&#65292;&#25105;&#20204;&#25104;&#21151;&#23558;&#39044;&#35757;&#32451;&#30340;&#20923;&#32467;LLM&#29992;&#20110;&#25509;&#25910;&#25351;&#20196;&#21644;&#35270;&#35273;&#36755;&#20837;&#65292;&#24182;&#22312;&#29615;&#22659;&#20013;&#30452;&#25509;&#36755;&#20986;&#21160;&#20316;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;LLaRP&#19981;&#20165;&#23545;&#20219;&#21153;&#25351;&#20196;&#30340;&#22797;&#26434;&#25913;&#20889;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#32780;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#38656;&#35201;&#26032;&#39062;&#26368;&#20248;&#34892;&#20026;&#30340;&#26032;&#20219;&#21153;&#12290;&#22312;&#22823;&#37327;&#26410;&#35265;&#20219;&#21153;&#20013;&#65292;LLaRP&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#29575;&#25552;&#21319;&#65292;&#24182;&#19988;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;(Language Rearrangement)&#26469;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2310.17722</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20855;&#26377;&#26222;&#36866;&#24615;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Generalizable Policies for Embodied Tasks. (arXiv:2310.17722v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17722
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21487;&#20197;&#34987;&#36866;&#24212;&#20026;&#20855;&#26377;&#26222;&#36866;&#24615;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#31574;&#30053;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;LLaRP&#65292;&#25105;&#20204;&#25104;&#21151;&#23558;&#39044;&#35757;&#32451;&#30340;&#20923;&#32467;LLM&#29992;&#20110;&#25509;&#25910;&#25351;&#20196;&#21644;&#35270;&#35273;&#36755;&#20837;&#65292;&#24182;&#22312;&#29615;&#22659;&#20013;&#30452;&#25509;&#36755;&#20986;&#21160;&#20316;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;LLaRP&#19981;&#20165;&#23545;&#20219;&#21153;&#25351;&#20196;&#30340;&#22797;&#26434;&#25913;&#20889;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#32780;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#38656;&#35201;&#26032;&#39062;&#26368;&#20248;&#34892;&#20026;&#30340;&#26032;&#20219;&#21153;&#12290;&#22312;&#22823;&#37327;&#26410;&#35265;&#20219;&#21153;&#20013;&#65292;LLaRP&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#29575;&#25552;&#21319;&#65292;&#24182;&#19988;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;(Language Rearrangement)&#26469;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21487;&#20197;&#34987;&#35843;&#25972;&#20026;&#36866;&#29992;&#20110;&#26426;&#22120;&#20154;&#35270;&#35273;&#20219;&#21153;&#30340;&#26222;&#36866;&#24615;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34987;&#31216;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;(LLaRP)&#65292;&#23427;&#23558;&#39044;&#35757;&#32451;&#30340;&#20923;&#32467;&#30340;LLM&#35843;&#25972;&#20026;&#25509;&#25910;&#25991;&#26412;&#25351;&#20196;&#21644;&#35270;&#35273;&#33258;&#25105;&#20013;&#24515;&#35266;&#27979;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#30452;&#25509;&#22312;&#29615;&#22659;&#20013;&#36755;&#20986;&#21160;&#20316;&#12290;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#65292;&#25105;&#20204;&#35757;&#32451;LLaRP&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#26469;&#30475;&#21644;&#34892;&#21160;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LLaRP&#23545;&#20219;&#21153;&#25351;&#20196;&#30340;&#22797;&#26434;&#25913;&#20889;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#38656;&#35201;&#26032;&#39062;&#26368;&#20248;&#34892;&#20026;&#30340;&#26032;&#20219;&#21153;&#12290;&#29305;&#21035;&#22320;&#65292;&#22312;1,000&#20010;&#26410;&#35265;&#20219;&#21153;&#20013;&#65292;&#23427;&#30340;&#25104;&#21151;&#29575;&#36798;&#21040;&#20102;42%&#65292;&#26159;&#20854;&#20182;&#24120;&#35265;&#23398;&#20064;&#22522;&#32447;&#25110;&#38646;&#26679;&#26412;&#24212;&#29992;&#30340;1.7&#20493;&#25104;&#21151;&#29575;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#24110;&#21161;&#31038;&#21306;&#30740;&#31350;&#20197;&#35821;&#35328;&#20026;&#26465;&#20214;&#30340;&#12289;&#22823;&#35268;&#27169;&#22810;&#20219;&#21153;&#30340;&#26426;&#22120;&#20154;AI&#38382;&#39064;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;(Language Rearrangement)&#65292;&#21253;&#25324;150,000&#20010;&#35757;&#32451;&#20219;&#21153;&#21644;1,000&#20010;&#27979;&#35797;&#20219;&#21153;&#65292;&#29992;&#20110;&#35821;&#35328;&#20026;&#26465;&#20214;&#30340;&#37325;&#26032;&#25490;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that large language models (LLMs) can be adapted to be generalizable policies for embodied visual tasks. Our approach, called Large LAnguage model Reinforcement Learning Policy (LLaRP), adapts a pre-trained frozen LLM to take as input text instructions and visual egocentric observations and output actions directly in the environment. Using reinforcement learning, we train LLaRP to see and act solely through environmental interactions. We show that LLaRP is robust to complex paraphrasings of task instructions and can generalize to new tasks that require novel optimal behavior. In particular, on 1,000 unseen tasks it achieves 42% success rate, 1.7x the success rate of other common learned baselines or zero-shot applications of LLMs. Finally, to aid the community in studying language conditioned, massively multi-task, embodied AI problems we release a novel benchmark, Language Rearrangement, consisting of 150,000 training and 1,000 testing tasks for language-conditioned rearrangem
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#24110;&#21161;&#25237;&#36164;&#32773;&#25581;&#31034;&#20225;&#19994;&#39118;&#38505;&#30340;&#20215;&#20540;&#65292;&#36890;&#36807;&#20174;&#25910;&#30410;&#30005;&#35805;&#30340;&#19978;&#19979;&#25991;&#20013;&#29983;&#25104;&#39118;&#38505;&#25688;&#35201;&#21644;&#35780;&#20272;&#65292;&#36825;&#20123;&#22522;&#20110;GPT&#30340;&#24230;&#37327;&#20855;&#26377;&#26174;&#33879;&#30340;&#20449;&#24687;&#20869;&#23481;&#65292;&#33021;&#22815;&#39044;&#27979;&#20225;&#19994;&#23618;&#38754;&#27874;&#21160;&#24615;&#21644;&#25237;&#36164;&#21019;&#26032;&#36873;&#25321;&#12290;&#27492;&#22806;&#65292;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#36824;&#33021;&#26377;&#25928;&#26816;&#27979;&#26032;&#20852;&#39118;&#38505;&#65292;&#24182;&#19988;&#36825;&#20123;&#24230;&#37327;&#22312;&#32929;&#26435;&#24066;&#22330;&#20013;&#36215;&#21040;&#23450;&#20215;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.17721</link><description>&lt;p&gt;
&#20174;&#35762;&#35805;&#25991;&#26412;&#21040;&#27934;&#23519;&#21147;&#65306;&#21033;&#29992;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#25581;&#31034;&#20225;&#19994;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
From Transcripts to Insights: Uncovering Corporate Risks Using Generative AI. (arXiv:2310.17721v1 [econ.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17721
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#24110;&#21161;&#25237;&#36164;&#32773;&#25581;&#31034;&#20225;&#19994;&#39118;&#38505;&#30340;&#20215;&#20540;&#65292;&#36890;&#36807;&#20174;&#25910;&#30410;&#30005;&#35805;&#30340;&#19978;&#19979;&#25991;&#20013;&#29983;&#25104;&#39118;&#38505;&#25688;&#35201;&#21644;&#35780;&#20272;&#65292;&#36825;&#20123;&#22522;&#20110;GPT&#30340;&#24230;&#37327;&#20855;&#26377;&#26174;&#33879;&#30340;&#20449;&#24687;&#20869;&#23481;&#65292;&#33021;&#22815;&#39044;&#27979;&#20225;&#19994;&#23618;&#38754;&#27874;&#21160;&#24615;&#21644;&#25237;&#36164;&#21019;&#26032;&#36873;&#25321;&#12290;&#27492;&#22806;&#65292;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#36824;&#33021;&#26377;&#25928;&#26816;&#27979;&#26032;&#20852;&#39118;&#38505;&#65292;&#24182;&#19988;&#36825;&#20123;&#24230;&#37327;&#22312;&#32929;&#26435;&#24066;&#22330;&#20013;&#36215;&#21040;&#23450;&#20215;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;ChatGPT&#31561;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#24110;&#21161;&#25237;&#36164;&#32773;&#25581;&#31034;&#20225;&#19994;&#39118;&#38505;&#32500;&#24230;&#30340;&#20215;&#20540;&#12290;&#25105;&#20204;&#24320;&#21457;&#24182;&#39564;&#35777;&#20102;&#25919;&#27835;&#12289;&#27668;&#20505;&#21644;&#20154;&#24037;&#26234;&#33021;&#30456;&#20851;&#39118;&#38505;&#30340;&#20225;&#19994;&#23618;&#38754;&#39118;&#38505;&#25950;&#21475;&#24230;&#37327;&#12290;&#20351;&#29992;GPT 3.5&#27169;&#22411;&#20174;&#25910;&#30410;&#30005;&#35805;&#30340;&#32972;&#26223;&#25552;&#20379;&#30340;&#19978;&#19979;&#25991;&#29983;&#25104;&#39118;&#38505;&#25688;&#35201;&#21644;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;GPT&#30340;&#24230;&#37327;&#20855;&#26377;&#26174;&#33879;&#30340;&#20449;&#24687;&#20869;&#23481;&#65292;&#24182;&#22312;&#39044;&#27979;&#65288;&#24322;&#24120;&#65289;&#20225;&#19994;&#23618;&#38754;&#27874;&#21160;&#24615;&#21644;&#20225;&#19994;&#30340;&#36873;&#25321;&#65288;&#22914;&#25237;&#36164;&#21644;&#21019;&#26032;&#65289;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#39118;&#38505;&#24230;&#37327;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#39118;&#38505;&#35780;&#20272;&#20013;&#30340;&#20449;&#24687;&#20248;&#20110;&#39118;&#38505;&#25688;&#35201;&#65292;&#36825;&#35777;&#26126;&#20102;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#30693;&#35782;&#30340;&#20215;&#20540;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#23545;&#20110;&#21457;&#29616;&#26032;&#20852;&#39118;&#38505;&#65288;&#22914;&#36817;&#20960;&#20010;&#23395;&#24230;&#39129;&#21319;&#30340;&#20154;&#24037;&#26234;&#33021;&#39118;&#38505;&#65289;&#38750;&#24120;&#26377;&#25928;&#12290;&#25105;&#20204;&#30340;&#24230;&#37327;&#22312;GPT&#30340;&#35757;&#32451;&#31383;&#21475;&#20869;&#22806;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#19988;&#22312;&#32929;&#26435;&#24066;&#22330;&#20013;&#23450;&#20215;&#12290;&#32508;&#19978;&#25152;&#36848;&#65292;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#39118;&#38505;&#27979;&#37327;&#26041;&#27861;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the value of generative AI tools, such as ChatGPT, in helping investors uncover dimensions of corporate risk. We develop and validate firm-level measures of risk exposure to political, climate, and AI-related risks. Using the GPT 3.5 model to generate risk summaries and assessments from the context provided by earnings call transcripts, we show that GPT-based measures possess significant information content and outperform the existing risk measures in predicting (abnormal) firm-level volatility and firms' choices such as investment and innovation. Importantly, information in risk assessments dominates that in risk summaries, establishing the value of general AI knowledge. We also find that generative AI is effective at detecting emerging risks, such as AI risk, which has soared in recent quarters. Our measures perform well both within and outside the GPT's training window and are priced in equity markets. Taken together, an AI-based approach to risk measurement provides usef
&lt;/p&gt;</description></item><item><title>&#24322;&#24120;&#32500;&#24230;&#21487;&#20197;&#32534;&#30721;&#20851;&#38190;&#30340;&#29305;&#23450;&#20219;&#21153;&#30693;&#35782;&#65292;&#24182;&#19988;&#19968;&#20010;&#21333;&#19968;&#30340;&#24322;&#24120;&#32500;&#24230;&#21487;&#20197;&#20197;&#26368;&#23567;&#30340;&#38169;&#35823;&#29575;&#23436;&#25104;&#19979;&#28216;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.17715</link><description>&lt;p&gt;
&#24322;&#24120;&#32500;&#24230;&#32534;&#30721;&#29305;&#23450;&#20219;&#21153;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Outlier Dimensions Encode Task-Specific Knowledge. (arXiv:2310.17715v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17715
&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#32500;&#24230;&#21487;&#20197;&#32534;&#30721;&#20851;&#38190;&#30340;&#29305;&#23450;&#20219;&#21153;&#30693;&#35782;&#65292;&#24182;&#19988;&#19968;&#20010;&#21333;&#19968;&#30340;&#24322;&#24120;&#32500;&#24230;&#21487;&#20197;&#20197;&#26368;&#23567;&#30340;&#38169;&#35823;&#29575;&#23436;&#25104;&#19979;&#28216;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#34920;&#31034;&#34987;&#23569;&#25968;&#20960;&#20010;&#20855;&#26377;&#26497;&#39640;&#26041;&#24046;&#30340;&#24322;&#24120;&#32500;&#24230;&#25152;&#20027;&#23548;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#35748;&#20026;&#65292;&#34429;&#28982;&#21435;&#38500;LLM&#34920;&#31034;&#20013;&#30340;&#24322;&#24120;&#32500;&#24230;&#20250;&#25439;&#23475;&#19979;&#28216;&#24615;&#33021;&#65292;&#20294;&#24322;&#24120;&#32500;&#24230;&#23545;&#23884;&#20837;&#34920;&#31034;&#30340;&#36136;&#37327;&#26159;&#26377;&#23475;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24494;&#35843;&#23545;&#24322;&#24120;&#32500;&#24230;&#30340;&#24433;&#21709;&#65292;&#24182;&#23637;&#31034;&#20102;&#20197;&#19979;&#32467;&#26524;&#65306;1&#65289;&#22312;&#39044;&#35757;&#32451;&#20013;&#20986;&#29616;&#30340;&#24322;&#24120;&#32500;&#24230;&#22312;&#24494;&#35843;&#27169;&#22411;&#20013;&#20173;&#28982;&#23384;&#22312;&#65292;2&#65289;&#19968;&#20010;&#21333;&#19968;&#30340;&#24322;&#24120;&#32500;&#24230;&#21487;&#20197;&#20197;&#26368;&#23567;&#30340;&#38169;&#35823;&#29575;&#23436;&#25104;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#24322;&#24120;&#32500;&#24230;&#21487;&#20197;&#32534;&#30721;&#20851;&#38190;&#30340;&#29305;&#23450;&#20219;&#21153;&#30693;&#35782;&#65292;&#24182;&#19988;&#19968;&#20010;&#34920;&#31034;&#22312;&#21333;&#20010;&#24322;&#24120;&#32500;&#24230;&#19978;&#30340;&#20540;&#20250;&#24433;&#21709;&#19979;&#28216;&#27169;&#22411;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representations from large language models (LLMs) are known to be dominated by a small subset of dimensions with exceedingly high variance. Previous works have argued that although ablating these outlier dimensions in LLM representations hurts downstream performance, outlier dimensions are detrimental to the representational quality of embeddings. In this study, we investigate how fine-tuning impacts outlier dimensions and show that 1) outlier dimensions that occur in pre-training persist in fine-tuned models and 2) a single outlier dimension can complete downstream tasks with a minimal error rate. Our results suggest that outlier dimensions can encode crucial task-specific knowledge and that the value of a representation in a single outlier dimension drives downstream model decisions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27979;&#35797;&#26102;&#20351;&#29992;&#23494;&#38598;&#21521;&#37327;&#30340;&#35789;&#27719;&#21477;&#27861;&#27169;&#24335;&#36827;&#34892;&#26368;&#36817;&#37051;&#25628;&#32034;&#65292;&#26469;&#35299;&#20915;&#20851;&#31995;&#25277;&#21462;&#20013;&#30340;&#38544;&#21547;&#34920;&#36798;&#21644;&#38271;&#23614;&#20851;&#31995;&#31867;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#27809;&#26377;&#30452;&#25509;&#35775;&#38382;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;/&#25110;&#22522;&#20110;&#30417;&#30563;&#35757;&#32451;&#25110;&#24494;&#35843;&#30340;&#22522;&#30784;&#35774;&#26045;&#30340;&#29992;&#25143;&#12290;</title><link>http://arxiv.org/abs/2310.17714</link><description>&lt;p&gt;
&#20174;&#37329;&#34701;&#25991;&#20214;&#20013;&#25552;&#21462;&#20851;&#31995;&#65306;&#22522;&#20110;&#21521;&#37327;&#21270;&#35789;&#27719;&#21477;&#27861;&#27169;&#24335;&#30340;&#26368;&#36817;&#37051;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Nearest Neighbor Search over Vectorized Lexico-Syntactic Patterns for Relation Extraction from Financial Documents. (arXiv:2310.17714v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27979;&#35797;&#26102;&#20351;&#29992;&#23494;&#38598;&#21521;&#37327;&#30340;&#35789;&#27719;&#21477;&#27861;&#27169;&#24335;&#36827;&#34892;&#26368;&#36817;&#37051;&#25628;&#32034;&#65292;&#26469;&#35299;&#20915;&#20851;&#31995;&#25277;&#21462;&#20013;&#30340;&#38544;&#21547;&#34920;&#36798;&#21644;&#38271;&#23614;&#20851;&#31995;&#31867;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#27809;&#26377;&#30452;&#25509;&#35775;&#38382;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;/&#25110;&#22522;&#20110;&#30417;&#30563;&#35757;&#32451;&#25110;&#24494;&#35843;&#30340;&#22522;&#30784;&#35774;&#26045;&#30340;&#29992;&#25143;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#35821;&#35328;&#22797;&#26434;&#24615;&#21644;&#25968;&#25454;&#31232;&#30095;&#24615;&#36896;&#25104;&#30340;&#38544;&#21547;&#34920;&#36798;&#21644;&#38271;&#23614;&#20851;&#31995;&#31867;&#38382;&#39064;&#65292;&#29616;&#26377;&#30340;&#20851;&#31995;&#25277;&#21462;&#27169;&#22411;&#36890;&#24120;&#26080;&#27861;&#24456;&#22909;&#22320;&#22788;&#29702;&#36825;&#20004;&#31181;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#27809;&#26377;&#30452;&#25509;&#35775;&#38382;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;/&#25110;&#22522;&#20110;&#30417;&#30563;&#35757;&#32451;&#25110;&#24494;&#35843;&#30340;&#22522;&#30784;&#35774;&#26045;&#30340;&#29992;&#25143;&#26469;&#35828;&#65292;&#36825;&#20123;&#26041;&#27861;&#21644;&#27169;&#22411;&#36890;&#24120;&#26159;&#19981;&#21487;&#25509;&#35302;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#22312;&#27979;&#35797;&#26102;&#36890;&#36807;&#23545;&#35789;&#27719;&#21477;&#27861;&#27169;&#24335;&#30340;&#31264;&#23494;&#21521;&#37327;&#36827;&#34892;&#26368;&#36817;&#37051;&#25628;&#32034;&#65292;&#20197;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25163;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation extraction (RE) has achieved remarkable progress with the help of pre-trained language models. However, existing RE models are usually incapable of handling two situations: implicit expressions and long-tail relation classes, caused by language complexity and data sparsity. Further, these approaches and models are largely inaccessible to users who don't have direct access to large language models (LLMs) and/or infrastructure for supervised training or fine-tuning. Rule-based systems also struggle with implicit expressions. Apart from this, Real world financial documents such as various 10-X reports (including 10-K, 10-Q, etc.) of publicly traded companies pose another challenge to rule-based systems in terms of longer and complex sentences. In this paper, we introduce a simple approach that consults training relations at test time through a nearest-neighbor search over dense vectors of lexico-syntactic patterns and provides a simple yet effective means to tackle the above issu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#35686;&#21578;&#26631;&#31614;&#21644;GPT-4&#29983;&#25104;&#30340;&#26368;&#20808;&#36827;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#25928;&#26524;&#65292;&#25506;&#35752;&#20102;&#36825;&#20123;&#35299;&#37322;&#23545;&#20110;&#32531;&#35299;&#34394;&#20551;&#20449;&#24687;&#30340;&#20316;&#29992;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#20004;&#31181;&#24178;&#39044;&#26041;&#24335;&#37117;&#26174;&#33879;&#38477;&#20302;&#20102;&#21442;&#19982;&#32773;&#23545;&#34394;&#20551;&#22768;&#26126;&#30340;&#20449;&#20219;&#12290;</title><link>http://arxiv.org/abs/2310.17711</link><description>&lt;p&gt;
&#35299;&#37322;&#26159;&#35299;&#33647;&#21527;&#65311;&#30701;&#26399;&#21644;&#38271;&#26399;&#30340;&#34394;&#20551;&#20449;&#24687;&#32531;&#35299;
&lt;/p&gt;
&lt;p&gt;
Is Explanation the Cure? Misinformation Mitigation in the Short Term and Long Term. (arXiv:2310.17711v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#35686;&#21578;&#26631;&#31614;&#21644;GPT-4&#29983;&#25104;&#30340;&#26368;&#20808;&#36827;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#25928;&#26524;&#65292;&#25506;&#35752;&#20102;&#36825;&#20123;&#35299;&#37322;&#23545;&#20110;&#32531;&#35299;&#34394;&#20551;&#20449;&#24687;&#30340;&#20316;&#29992;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#20004;&#31181;&#24178;&#39044;&#26041;&#24335;&#37117;&#26174;&#33879;&#38477;&#20302;&#20102;&#21442;&#19982;&#32773;&#23545;&#34394;&#20551;&#22768;&#26126;&#30340;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#33258;&#21160;&#29983;&#25104;&#35299;&#37322;&#34987;&#25552;&#35758;&#29992;&#20110;&#35299;&#20915;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#30340;&#34394;&#20551;&#20449;&#24687;&#65292;&#24182;&#21521;&#35782;&#21035;&#21040;&#30340;&#20551;&#26032;&#38395;&#28155;&#21152;&#35686;&#21578;&#26631;&#31614;&#12290;&#34429;&#28982;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#19987;&#27880;&#20110;&#29983;&#25104;&#33391;&#22909;&#30340;&#35299;&#37322;&#65292;&#20294;&#36825;&#20123;&#35299;&#37322;&#22914;&#20309;&#30495;&#27491;&#24110;&#21161;&#20154;&#20204;&#23545;&#25239;&#34394;&#20551;&#26032;&#38395;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#19968;&#20010;&#35686;&#21578;&#26631;&#31614;&#21644;&#30001;GPT-4&#29983;&#25104;&#30340;&#26368;&#20808;&#36827;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#22312;&#25581;&#38706;&#34394;&#20551;&#20449;&#24687;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#19968;&#20010;&#20004;&#27874;&#27425;&#30340;&#22312;&#32447;&#20154;&#31867;&#23454;&#39564;&#65292;&#30740;&#31350;&#23545;&#35937;&#65288;N = 215&#65289;&#34987;&#38543;&#26426;&#20998;&#37197;&#21040;&#19977;&#20010;&#32452;&#65306;&#19968;&#20010;&#26080;&#24178;&#39044;&#30340;&#23545;&#29031;&#32452;&#65292;&#22312;&#20854;&#20013;&#23637;&#31034;&#34394;&#20551;&#20869;&#23481;&#65307;&#19968;&#20010;&#35686;&#21578;&#26631;&#31614;&#32452;&#65292;&#22312;&#20854;&#20013;&#26631;&#35760;&#34394;&#20551;&#22768;&#26126;&#65307;&#20197;&#21450;&#19968;&#20010;&#35299;&#37322;&#32452;&#65292;&#22312;&#20854;&#20013;&#34394;&#20551;&#20869;&#23481;&#38468;&#24102;GPT-4&#29983;&#25104;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20004;&#31181;&#24178;&#39044;&#37117;&#26174;&#33879;&#38477;&#20302;&#20102;&#21442;&#19982;&#32773;&#23545;&#34394;&#20551;&#22768;&#26126;&#30340;&#33258;&#25105;&#25253;&#21578;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;
With advancements in natural language processing (NLP) models, automatic explanation generation has been proposed to mitigate misinformation on social media platforms in addition to adding warning labels to identified fake news. While many researchers have focused on generating good explanations, how these explanations can really help humans combat fake news is under-explored. In this study, we compare the effectiveness of a warning label and the state-of-the-art counterfactual explanations generated by GPT-4 in debunking misinformation. In a two-wave, online human-subject study, participants (N = 215) were randomly assigned to a control group in which false contents are shown without any intervention, a warning tag group in which the false claims were labeled, or an explanation group in which the false contents were accompanied by GPT-4 generated explanations. Our results show that both interventions significantly decrease participants' self-reported belief in fake claims in an equiva
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#32771;&#23519;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21327;&#21161;&#20020;&#24202;&#21307;&#29983;&#22238;&#31572;&#24739;&#32773;&#38382;&#39064;&#30340;&#23454;&#29992;&#24615;&#65292;&#24182;&#21457;&#29616;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#32534;&#36753;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#21487;&#25509;&#21463;&#30340;&#33609;&#31295;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24037;&#20316;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.17703</link><description>&lt;p&gt;
&#20351;&#29992;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#22238;&#22797;&#24739;&#32773;&#20449;&#24687;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The impact of using an AI chatbot to respond to patient messages. (arXiv:2310.17703v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#32771;&#23519;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21327;&#21161;&#20020;&#24202;&#21307;&#29983;&#22238;&#31572;&#24739;&#32773;&#38382;&#39064;&#30340;&#23454;&#29992;&#24615;&#65292;&#24182;&#21457;&#29616;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#32534;&#36753;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#21487;&#25509;&#21463;&#30340;&#33609;&#31295;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24037;&#20316;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#20214;&#36127;&#25285;&#26159;&#23548;&#33268;&#20020;&#24202;&#21307;&#29983;&#20518;&#24608;&#30340;&#20027;&#35201;&#21407;&#22240;&#65292;&#36825;&#31181;&#24773;&#20917;&#22312;&#20840;&#22269;&#33539;&#22260;&#20869;&#26085;&#30410;&#22686;&#21152;&#65292;&#23545;&#25105;&#20204;&#29031;&#39038;&#24739;&#32773;&#30340;&#33021;&#21147;&#26500;&#25104;&#20102;&#32039;&#36843;&#23041;&#32961;&#12290;&#20687;ChatGPT&#36825;&#26679;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#32842;&#22825;&#26426;&#22120;&#20154;&#21487;&#20197;&#36890;&#36807;&#21327;&#21161;&#25991;&#26723;&#22788;&#29702;&#26469;&#20943;&#36731;&#20020;&#24202;&#21307;&#29983;&#30340;&#36127;&#25285;&#12290;&#34429;&#28982;&#35768;&#22810;&#21307;&#38498;&#27491;&#22312;&#31215;&#26497;&#23558;&#36825;&#20123;&#31995;&#32479;&#25972;&#21512;&#21040;&#30005;&#23376;&#30149;&#21382;&#31995;&#32479;&#20013;&#65292;&#20294;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#20020;&#24202;&#20915;&#31574;&#20013;&#30340;&#23454;&#29992;&#24615;&#21644;&#24433;&#21709;&#23578;&#26410;&#38024;&#23545;&#27492;&#39044;&#26399;&#29992;&#36884;&#36827;&#34892;&#30740;&#31350;&#12290;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21327;&#21161;&#20020;&#24202;&#21307;&#29983;&#36215;&#33609;&#22238;&#31572;&#24739;&#32773;&#38382;&#39064;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#30340;&#30740;&#31350;&#32773;&#12290;&#22312;&#25105;&#20204;&#30340;&#20004;&#38454;&#27573;&#27178;&#26029;&#38754;&#30740;&#31350;&#20013;&#65292;6&#21517;&#32959;&#30244;&#23398;&#23478;&#23545;100&#20010;&#36924;&#30495;&#30340;&#21512;&#25104;&#30284;&#30151;&#24739;&#32773;&#22330;&#26223;&#21644;&#21453;&#26144;&#24120;&#35265;&#21307;&#30103;&#24773;&#20917;&#30340;&#38376;&#25143;&#20449;&#24687;&#36827;&#34892;&#20102;&#22238;&#22797;&#65292;&#39318;&#20808;&#26159;&#25163;&#21160;&#22238;&#22797;&#65292;&#28982;&#21518;&#26159;&#36890;&#36807;AI&#21327;&#21161;&#22238;&#22797;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;AI&#21327;&#21161;&#30340;&#22238;&#31572;&#26356;&#38271;&#12289;&#21487;&#35835;&#24615;&#36739;&#24046;&#65292;&#20294;&#22312;58%&#30340;&#26102;&#38388;&#37324;&#25552;&#20379;&#20102;&#21487;&#25509;&#21463;&#30340;&#33609;&#31295;&#32780;&#26080;&#38656;&#32534;&#36753;&#12290;AI&#21327;&#21161;&#25552;&#39640;&#20102;77%&#30340;&#25928;&#29575;&#65292;&#19982;&#20256;&#32479;&#25163;&#21160;&#22238;&#31572;&#30456;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Documentation burden is a major contributor to clinician burnout, which is rising nationally and is an urgent threat to our ability to care for patients. Artificial intelligence (AI) chatbots, such as ChatGPT, could reduce clinician burden by assisting with documentation. Although many hospitals are actively integrating such systems into electronic medical record systems, AI chatbots utility and impact on clinical decision-making have not been studied for this intended use. We are the first to examine the utility of large language models in assisting clinicians draft responses to patient questions. In our two-stage cross-sectional study, 6 oncologists responded to 100 realistic synthetic cancer patient scenarios and portal messages developed to reflect common medical situations, first manually, then with AI assistance.  We find AI-assisted responses were longer, less readable, but provided acceptable drafts without edits 58% of time. AI assistance improved efficiency 77% of time, with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#20351;&#29992;&#32500;&#24230;&#23545;&#27604;&#24230;&#30446;&#26631;&#35757;&#32451;&#30340;&#33258;&#25105;&#30417;&#30563;&#23884;&#20837;&#19982;SimCSE&#26041;&#27861;&#65292;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#21069;&#32773;&#34920;&#29616;&#20248;&#20110;&#21518;&#32773;&#12290;</title><link>http://arxiv.org/abs/2310.17690</link><description>&lt;p&gt;
&#38750;&#23545;&#27604;&#24230;&#21477;&#23376;&#34920;&#31034;&#30340;&#33258;&#25105;&#30417;&#30563;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Non-contrastive sentence representations via self-supervision. (arXiv:2310.17690v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#20351;&#29992;&#32500;&#24230;&#23545;&#27604;&#24230;&#30446;&#26631;&#35757;&#32451;&#30340;&#33258;&#25105;&#30417;&#30563;&#23884;&#20837;&#19982;SimCSE&#26041;&#27861;&#65292;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#21069;&#32773;&#34920;&#29616;&#20248;&#20110;&#21518;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#24230;&#26041;&#27861;&#26159;&#23398;&#20064;&#25991;&#26412;&#21644;&#21477;&#23376;&#23884;&#20837;&#30340;&#22823;&#22810;&#25968;&#26080;&#30417;&#30563;&#26041;&#27861;&#30340;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#31038;&#21306;&#20013;&#65292;&#19968;&#31867;&#19981;&#21516;&#30340;&#33258;&#25105;&#30417;&#30563;&#25439;&#22833;&#20989;&#25968;&#21644;&#26041;&#27861;&#34987;&#31216;&#20026;&#32500;&#24230;&#23545;&#27604;&#24230;&#12290;&#26412;&#25991;&#36890;&#36807;&#19982;&#26631;&#20934;&#23545;&#27604;&#24230;&#21477;&#23376;&#23884;&#20837;&#22522;&#32447;&#26041;&#27861;SimCSE&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#65292;&#21457;&#29616;&#20351;&#29992;&#32500;&#24230;&#23545;&#27604;&#24230;&#30446;&#26631;&#35757;&#32451;&#30340;&#33258;&#25105;&#30417;&#30563;&#23884;&#20837;&#33021;&#22815;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#32988;&#36807;SimCSE&#65292;&#32780;&#26080;&#38656;&#36741;&#21161;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sample contrastive methods, typically referred to simply as contrastive are the foundation of most unsupervised methods to learn text and sentence embeddings. On the other hand, a different class of self-supervised loss functions and methods have been considered in the computer vision community and referred to as dimension contrastive. In this paper, we thoroughly compare this class of methods with the standard baseline for contrastive sentence embeddings, SimCSE. We find that self-supervised embeddings trained using dimension contrastive objectives can outperform SimCSE on downstream tasks without needing auxiliary loss functions.
&lt;/p&gt;</description></item><item><title>CodeFusion&#26159;&#19968;&#31181;&#39044;&#35757;&#32451;&#30340;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#25193;&#25955;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#33258;&#28982;&#35821;&#35328;&#20195;&#30721;&#29983;&#25104;&#20013;&#36935;&#21040;&#30340;&#38480;&#21046;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#22312;&#20934;&#30830;&#29575;&#21644;&#22810;&#26679;&#24615;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#33258;&#22238;&#24402;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2310.17680</link><description>&lt;p&gt;
CodeFusion: &#19968;&#31181;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CodeFusion: A Pre-trained Diffusion Model for Code Generation. (arXiv:2310.17680v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17680
&lt;/p&gt;
&lt;p&gt;
CodeFusion&#26159;&#19968;&#31181;&#39044;&#35757;&#32451;&#30340;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#25193;&#25955;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#33258;&#28982;&#35821;&#35328;&#20195;&#30721;&#29983;&#25104;&#20013;&#36935;&#21040;&#30340;&#38480;&#21046;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#22312;&#20934;&#30830;&#29575;&#21644;&#22810;&#26679;&#24615;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#33258;&#22238;&#24402;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20551;&#35774;&#19968;&#20010;&#24320;&#21457;&#32773;&#21482;&#33021;&#20462;&#25913;&#20854;&#26368;&#21518;&#19968;&#34892;&#20195;&#30721;&#65292;&#22312;&#27491;&#30830;&#20043;&#21069;&#65292;&#20182;&#20204;&#38656;&#35201;&#22810;&#23569;&#27425;&#20174;&#22836;&#24320;&#22987;&#32534;&#20889;&#20989;&#25968;&#21602;&#65311;&#33258;&#28982;&#35821;&#35328;&#20195;&#30721;&#29983;&#25104;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#20063;&#26377;&#31867;&#20284;&#30340;&#38480;&#21046;&#65306;&#23427;&#20204;&#19981;&#23481;&#26131;&#37325;&#26032;&#32771;&#34385;&#20043;&#21069;&#29983;&#25104;&#30340;&#26631;&#35760;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CodeFusion&#30340;&#39044;&#35757;&#32451;&#25193;&#25955;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#23545;&#20197;&#32534;&#30721;&#30340;&#33258;&#28982;&#35821;&#35328;&#20026;&#26465;&#20214;&#30340;&#23436;&#25972;&#31243;&#24207;&#36827;&#34892;&#21435;&#22122;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#12290;&#25105;&#20204;&#38024;&#23545;Bash&#12289;Python&#21644;Microsoft Excel&#26465;&#20214;&#26684;&#24335;(CF)&#35268;&#21017;&#30340;&#33258;&#28982;&#35821;&#35328;&#21040;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#23545;CodeFusion&#36827;&#34892;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;CodeFusion&#65288;75M&#21442;&#25968;&#65289;&#22312;top-1&#20934;&#30830;&#29575;&#19978;&#34920;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;&#33258;&#22238;&#24402;&#31995;&#32479;&#65288;350M-175B&#21442;&#25968;&#65289;&#30456;&#24403;&#65292;&#24182;&#19988;&#22312;top-3&#21644;top-5&#20934;&#30830;&#29575;&#19978;&#34920;&#29616;&#20248;&#20110;&#23427;&#20204;&#65292;&#36825;&#26159;&#30001;&#20110;&#23427;&#22312;&#22810;&#26679;&#24615;&#19982;&#36136;&#37327;&#20043;&#38388;&#30340;&#24179;&#34913;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imagine a developer who can only change their last line of code, how often would they have to start writing a function from scratch before it is correct? Auto-regressive models for code generation from natural language have a similar limitation: they do not easily allow reconsidering earlier tokens generated. We introduce CodeFusion, a pre-trained diffusion code generation model that addresses this limitation by iteratively denoising a complete program conditioned on the encoded natural language. We evaluate CodeFusion on the task of natural language to code generation for Bash, Python, and Microsoft Excel conditional formatting (CF) rules. Experiments show that CodeFusion (75M parameters) performs on par with state-of-the-art auto-regressive systems (350M-175B parameters) in top-1 accuracy and outperforms them in top-3 and top-5 accuracy due to its better balance in diversity versus quality.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;GPT-4&#22312;&#22810;&#31181;&#35821;&#35328;&#30340;&#21516;&#34892;&#35780;&#23457;&#25991;&#29486;&#21644;&#28784;&#33394;&#25991;&#29486;&#31579;&#36873;&#21644;&#25552;&#21462;&#25968;&#25454;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;GPT-4&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#19978;&#20934;&#30830;&#24230;&#19982;&#20154;&#31867;&#34920;&#29616;&#30456;&#24403;&#65292;&#22312;&#35843;&#25972;&#20102;&#20598;&#28982;&#19968;&#33268;&#24615;&#21644;&#25968;&#25454;&#38598;&#19981;&#24179;&#34913;&#21518;&#65292;&#20854;&#22312;&#25968;&#25454;&#25552;&#21462;&#26041;&#38754;&#34920;&#29616;&#20986;&#20013;&#31561;&#27700;&#24179;&#30340;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.17526</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#21462;&#20195;&#20154;&#31867;&#22312;&#31995;&#32479;&#35780;&#20215;&#36807;&#31243;&#20013;&#30340;&#35282;&#33394;&#65311;&#35780;&#20272;GPT-4&#22312;&#22810;&#31181;&#35821;&#35328;&#30340;&#21516;&#34892;&#35780;&#23457;&#25991;&#29486;&#21644;&#28784;&#33394;&#25991;&#29486;&#31579;&#36873;&#21644;&#25552;&#21462;&#25968;&#25454;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can large language models replace humans in the systematic review process? Evaluating GPT-4's efficacy in screening and extracting data from peer-reviewed and grey literature in multiple languages. (arXiv:2310.17526v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17526
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;GPT-4&#22312;&#22810;&#31181;&#35821;&#35328;&#30340;&#21516;&#34892;&#35780;&#23457;&#25991;&#29486;&#21644;&#28784;&#33394;&#25991;&#29486;&#31579;&#36873;&#21644;&#25552;&#21462;&#25968;&#25454;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;GPT-4&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#19978;&#20934;&#30830;&#24230;&#19982;&#20154;&#31867;&#34920;&#29616;&#30456;&#24403;&#65292;&#22312;&#35843;&#25972;&#20102;&#20598;&#28982;&#19968;&#33268;&#24615;&#21644;&#25968;&#25454;&#38598;&#19981;&#24179;&#34913;&#21518;&#65292;&#20854;&#22312;&#25968;&#25454;&#25552;&#21462;&#26041;&#38754;&#34920;&#29616;&#20986;&#20013;&#31561;&#27700;&#24179;&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31995;&#32479;&#35780;&#20215;&#23545;&#20110;&#25351;&#23548;&#23454;&#36341;&#12289;&#30740;&#31350;&#21644;&#25919;&#31574;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#24120;&#24120;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#26102;&#38388;&#21644;&#20154;&#21147;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21487;&#33021;&#33021;&#22815;&#21152;&#24555;&#21644;&#33258;&#21160;&#21270;&#31995;&#32479;&#35780;&#20215;&#30340;&#36807;&#31243;&#65292;&#20294;&#26159;&#23427;&#20204;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#23578;&#26410;&#32463;&#36807;&#20840;&#38754;&#35780;&#20272;&#65292;&#32780;&#19988;&#36824;&#27809;&#26377;&#30740;&#31350;&#27979;&#35797;&#36807;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;LLM&#8212;&#8212;GPT-4&#12290;&#26412;&#39044;&#27880;&#20876;&#30740;&#31350;&#37319;&#29992;&#8220;&#26080;&#20154;&#21442;&#19982;&#8221;&#30340;&#26041;&#27861;&#35780;&#20272;&#20102;GPT-4&#22312;&#26631;&#39064;/&#25688;&#35201;&#31579;&#36873;&#12289;&#20840;&#25991;&#23457;&#26597;&#21644;&#25968;&#25454;&#25552;&#21462;&#26041;&#38754;&#22312;&#19981;&#21516;&#25991;&#29486;&#31867;&#22411;&#21644;&#35821;&#35328;&#19978;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;GPT-4&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#30340;&#20934;&#30830;&#24230;&#19982;&#20154;&#31867;&#34920;&#29616;&#30456;&#24403;&#65292;&#20294;&#32467;&#26524;&#21463;&#21040;&#20598;&#28982;&#19968;&#33268;&#24615;&#21644;&#25968;&#25454;&#38598;&#19981;&#24179;&#34913;&#30340;&#24433;&#21709;&#12290;&#22312;&#35843;&#25972;&#20102;&#36825;&#20123;&#22240;&#32032;&#21518;&#65292;&#25968;&#25454;&#25552;&#21462;&#26041;&#38754;&#34920;&#29616;&#20986;&#20013;&#31561;&#27700;&#24179;&#30340;&#20934;&#30830;&#24230;&#65292;&#22312;&#20351;&#29992;&#39640;&#21487;&#38752;&#24615;&#25552;&#31034;&#36827;&#34892;&#31579;&#36873;&#30340;&#30740;&#31350;&#20013;&#65292;&#31579;&#36873;&#20840;&#25991;&#25991;&#29486;&#30340;&#34920;&#29616;&#27700;&#24179;&#22312;&#19981;&#21516;&#38454;&#27573;&#21644;&#35821;&#35328;&#19978;&#22343;&#20026;&#26080;&#21040;&#20013;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Systematic reviews are vital for guiding practice, research, and policy, yet they are often slow and labour-intensive. Large language models (LLMs) could offer a way to speed up and automate systematic reviews, but their performance in such tasks has not been comprehensively evaluated against humans, and no study has tested GPT-4, the biggest LLM so far. This pre-registered study evaluates GPT-4's capability in title/abstract screening, full-text review, and data extraction across various literature types and languages using a 'human-out-of-the-loop' approach. Although GPT-4 had accuracy on par with human performance in most tasks, results were skewed by chance agreement and dataset imbalance. After adjusting for these, there was a moderate level of performance for data extraction, and - barring studies that used highly reliable prompts screening performance levelled at none to moderate for different stages and languages. When screening full-text literature using highly reliable prom
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#23545;&#20110;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#65292;&#24403;LoRA-rank&#8805;&#65288;f&#30340;&#23485;&#24230;&#65289;&#215;&#65288;&#30446;&#26631;&#27169;&#22411;&#30340;&#28145;&#24230;/ f&#30340;&#28145;&#24230;&#65289;&#26102;&#65292;LoRA&#21487;&#20197;&#20351;&#20219;&#20309;&#27169;&#22411;f&#20934;&#30830;&#34920;&#31034;&#20219;&#20309;&#36739;&#23567;&#30340;&#30446;&#26631;&#27169;&#22411;f&#12290;&#23545;&#20110;Transformer&#32593;&#32476;&#65292;&#36890;&#36807;rank-&#65288;&#23884;&#20837;&#22823;&#23567;/ 2&#65289;&#30340;LoRA&#36866;&#37197;&#22120;&#21487;&#20197;&#20351;&#20219;&#20309;&#27169;&#22411;&#36866;&#24212;&#20110;&#30456;&#21516;&#22823;&#23567;&#30340;&#30446;&#26631;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.17513</link><description>&lt;p&gt;
&#12298;&#20302;&#31209;&#36866;&#24212;&#30340;&#34920;&#36798;&#33021;&#21147;&#12299;
&lt;/p&gt;
&lt;p&gt;
The Expressive Power of Low-Rank Adaptation. (arXiv:2310.17513v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#23545;&#20110;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#65292;&#24403;LoRA-rank&#8805;&#65288;f&#30340;&#23485;&#24230;&#65289;&#215;&#65288;&#30446;&#26631;&#27169;&#22411;&#30340;&#28145;&#24230;/ f&#30340;&#28145;&#24230;&#65289;&#26102;&#65292;LoRA&#21487;&#20197;&#20351;&#20219;&#20309;&#27169;&#22411;f&#20934;&#30830;&#34920;&#31034;&#20219;&#20309;&#36739;&#23567;&#30340;&#30446;&#26631;&#27169;&#22411;f&#12290;&#23545;&#20110;Transformer&#32593;&#32476;&#65292;&#36890;&#36807;rank-&#65288;&#23884;&#20837;&#22823;&#23567;/ 2&#65289;&#30340;LoRA&#36866;&#37197;&#22120;&#21487;&#20197;&#20351;&#20219;&#20309;&#27169;&#22411;&#36866;&#24212;&#20110;&#30456;&#21516;&#22823;&#23567;&#30340;&#30446;&#26631;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#26159;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#21033;&#29992;&#30697;&#38453;&#30340;&#20302;&#31209;&#36866;&#24212;&#24615;&#65292;&#22312;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#65289;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#23613;&#31649;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#26159;LoRA&#30340;&#29702;&#35770;&#22522;&#30784;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#23578;&#26410;&#24471;&#21040;&#25506;&#32034;&#12290;&#26412;&#25991;&#36890;&#36807;&#20174;&#29702;&#35770;&#35282;&#24230;&#20998;&#26512;LoRA&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#39318;&#27425;&#23581;&#35797;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#65292;&#22914;&#26524;LoRA-rank&#8805;&#65288;f&#30340;&#23485;&#24230;&#65289;&#215;&#65288;&#30446;&#26631;&#27169;&#22411;&#30340;&#28145;&#24230;/ f&#30340;&#28145;&#24230;&#65289;&#65292;&#21017;LoRA&#21487;&#20197;&#20351;&#20219;&#20309;&#27169;&#22411;f&#20934;&#30830;&#34920;&#31034;&#20219;&#20309;&#36739;&#23567;&#30340;&#30446;&#26631;&#27169;&#22411;f&#12290;&#24403;LoRA-rank&#20302;&#20110;&#38408;&#20540;&#26102;&#65292;&#25105;&#20204;&#36824;&#37327;&#21270;&#20102;&#36924;&#36817;&#35823;&#24046;&#12290;&#23545;&#20110;Transformer&#32593;&#32476;&#65292;&#25105;&#20204;&#35777;&#26126;&#20219;&#20309;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;rank-&#65288;&#23884;&#20837;&#22823;&#23567;/ 2&#65289;&#30340;LoRA&#36866;&#37197;&#22120;&#36866;&#24212;&#20110;&#30456;&#21516;&#22823;&#23567;&#30340;&#30446;&#26631;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-Rank Adaptation (LoRA), a parameter-efficient fine-tuning method that leverages low-rank adaptation of weight matrices, has emerged as a prevalent technique for fine-tuning pre-trained models such as large language models and diffusion models. Despite its huge success in practice, the theoretical underpinnings of LoRA have largely remained unexplored. This paper takes the first step to bridge this gap by theoretically analyzing the expressive power of LoRA. We prove that, for fully connected neural networks, LoRA can adapt any model $f$ to accurately represent any smaller target model $\overline{f}$ if LoRA-rank $\geq(\text{width of }f) \times \frac{\text{depth of }\overline{f}}{\text{depth of }f}$. We also quantify the approximation error when LoRA-rank is lower than the threshold. For Transformer networks, we show any model can be adapted to a target model of the same size with rank-$(\frac{\text{embedding size}}{2})$ LoRA adapters.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#20351;&#29992;Exo&#32534;&#35793;&#22120;&#29983;&#25104;&#25509;&#36817;&#20110;&#29978;&#33267;&#20248;&#20110;&#25163;&#21160;&#24320;&#21457;&#30340;&#24494;&#20869;&#26680;&#30340;&#27493;&#39588;&#21644;&#26041;&#27861;&#65292;&#24182;&#35299;&#20915;&#20102;&#20026;&#27599;&#20010;&#26032;&#30828;&#20214;&#29983;&#25104;&#19987;&#29992;&#24494;&#20869;&#26680;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.17408</link><description>&lt;p&gt;
&#20351;&#29992;Exo&#35299;&#20915;&#30697;&#38453;&#20056;&#27861;&#24494;&#20869;&#26680;&#29983;&#25104;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Tackling the Matrix Multiplication Micro-kernel Generation with Exo. (arXiv:2310.17408v1 [cs.MS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17408
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#20351;&#29992;Exo&#32534;&#35793;&#22120;&#29983;&#25104;&#25509;&#36817;&#20110;&#29978;&#33267;&#20248;&#20110;&#25163;&#21160;&#24320;&#21457;&#30340;&#24494;&#20869;&#26680;&#30340;&#27493;&#39588;&#21644;&#26041;&#27861;&#65292;&#24182;&#35299;&#20915;&#20102;&#20026;&#27599;&#20010;&#26032;&#30828;&#20214;&#29983;&#25104;&#19987;&#29992;&#24494;&#20869;&#26680;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#21313;&#24180;&#37324;&#65292;&#30697;&#38453;&#20056;&#27861;&#65288;&#25110;GEMM&#65289;&#30340;&#20248;&#21270;&#19968;&#30452;&#26159;&#19968;&#20010;&#38656;&#27714;&#12290;&#36825;&#20010;&#25805;&#20316;&#34987;&#35748;&#20026;&#26159;&#24403;&#21069;&#32447;&#24615;&#20195;&#25968;&#24211;&#65288;&#22914;BLIS&#65292;OpenBLAS&#25110;Intel OneAPI&#65289;&#30340;&#26071;&#33328;&#65292;&#22240;&#20026;&#23427;&#22312;&#21508;&#31181;&#31185;&#23398;&#24212;&#29992;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;GEMM&#36890;&#24120;&#26159;&#25353;&#29031;GotoBLAS&#30340;&#29702;&#24565;&#36827;&#34892;&#23454;&#29616;&#30340;&#65292;&#23427;&#23558;GEMM&#30340;&#25805;&#20316;&#25968;&#36827;&#34892;&#20999;&#21106;&#65292;&#24182;&#20351;&#29992;&#19968;&#31995;&#21015;&#23884;&#22871;&#24490;&#29615;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#36807;&#19968;&#23567;&#22359;&#38754;&#21521;&#30828;&#20214;&#30340;&#39640;&#24615;&#33021;&#20195;&#30721;&#65292;&#21363;&#24494;&#20869;&#26680;&#65292;&#25552;&#21462;&#20307;&#31995;&#32467;&#26500;&#30340;&#26368;&#22823;&#35745;&#31639;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#36843;&#20351;&#24320;&#21457;&#20154;&#21592;&#20026;&#27599;&#20010;&#26032;&#30340;&#30828;&#20214;&#29983;&#25104;&#19968;&#20010;&#19987;&#29992;&#30340;&#24494;&#20869;&#26680;&#65292;&#24182;&#38656;&#35201;&#38750;&#24120;&#22823;&#30340;&#24037;&#20316;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Exo&#32534;&#35793;&#22120;&#29983;&#25104;&#24494;&#20869;&#26680;&#30340;&#36880;&#27493;&#36807;&#31243;&#65292;&#35813;&#36807;&#31243;&#30340;&#24615;&#33021;&#25509;&#36817;&#29978;&#33267;&#36229;&#36807;&#20102;&#25163;&#21160;&#32534;&#20889;&#30340;&#20351;&#29992;&#20869;&#37096;&#20989;&#25968;&#25110;&#27719;&#32534;&#35821;&#35328;&#30340;&#24494;&#20869;&#26680;&#12290;
&lt;/p&gt;
&lt;p&gt;
The optimization of the matrix multiplication (or GEMM) has been a need during the last decades. This operation is considered the flagship of current linear algebra libraries such as BLIS, OpenBLAS, or Intel OneAPI because of its widespread use in a large variety of scientific applications. The GEMM is usually implemented following the GotoBLAS philosophy, which tiles the GEMM operands and uses a series of nested loops for performance improvement. These approaches extract the maximum computational power of the architectures through small pieces of hardware-oriented, high-performance code called micro-kernel. However, this approach forces developers to generate, with a non-negligible effort, a dedicated micro-kernel for each new hardware.  In this work, we present a step-by-step procedure for generating micro-kernels with the Exo compiler that performs close to (or even better than) manually developed microkernels written with intrinsic functions or assembly language. Our solution also 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#22312;&#23567;&#35268;&#27169;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19978;&#30340;&#24212;&#29992;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#22686;&#24378;&#21518;&#30340;&#25968;&#25454;&#35757;&#32451;&#20998;&#31867;&#22120;&#27169;&#22411;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24773;&#24863;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.17015</link><description>&lt;p&gt;
&#22312;&#23567;&#35268;&#27169;&#19981;&#24179;&#34913;&#30340;&#25991;&#26412;&#25968;&#25454;&#20013;&#36827;&#34892;&#24773;&#24863;&#26816;&#27979;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation for Emotion Detection in Small Imbalanced Text Data. (arXiv:2310.17015v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17015
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#22312;&#23567;&#35268;&#27169;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19978;&#30340;&#24212;&#29992;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#22686;&#24378;&#21518;&#30340;&#25968;&#25454;&#35757;&#32451;&#20998;&#31867;&#22120;&#27169;&#22411;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24773;&#24863;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#24773;&#24863;&#35782;&#21035;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#20219;&#21153;&#26159;&#35782;&#21035;&#20986;&#35832;&#22914;&#21916;&#24742;&#25110;&#24868;&#24594;&#31561;&#24773;&#24863;&#12290;&#20854;&#20013;&#19968;&#20010;&#25361;&#25112;&#26159;&#21487;&#29992;&#30340;&#26631;&#27880;&#20102;&#24773;&#24863;&#30340;&#25968;&#25454;&#38598;&#30340;&#21294;&#20047;&#12290;&#26576;&#20123;&#29616;&#26377;&#25968;&#25454;&#38598;&#35268;&#27169;&#36739;&#23567;&#65292;&#37319;&#29992;&#19981;&#21516;&#30340;&#24773;&#24863;&#20998;&#31867;&#20307;&#31995;&#65292;&#24182;&#19988;&#24773;&#24863;&#20998;&#24067;&#19981;&#24179;&#34913;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#23545;&#23567;&#35268;&#27169;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#24403;&#21069;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#65288;&#22914;RoBERTa&#65289;&#24615;&#33021;&#19981;&#20339;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#22235;&#31181;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65288;Easy Data Augmentation EDA&#12289;&#22522;&#20110;&#23884;&#20837;&#30340;&#38745;&#24577;&#21644;&#19978;&#19979;&#25991;&#30456;&#20851;&#12289;&#21450;ProtAugment&#65289;&#22312;&#19977;&#20010;&#28304;&#22836;&#21644;&#19981;&#21516;&#35268;&#27169;&#12289;&#24773;&#24863;&#31867;&#21035;&#21450;&#20998;&#24067;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#22686;&#24378;&#21518;&#30340;&#25968;&#25454;&#35757;&#32451;&#20998;&#31867;&#22120;&#27169;&#22411;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#65306;&#30452;&#25509;&#20351;&#29992;&#22686;&#24378;&#21518;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#19982;&#20351;&#29992;&#21407;&#22987;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#23545;&#27604;&#65292;&#24182;&#20998;&#26512;&#20102;&#19981;&#21516;&#24773;&#24863;&#31867;&#21035;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion recognition in text, the task of identifying emotions such as joy or anger, is a challenging problem in NLP with many applications. One of the challenges is the shortage of available datasets that have been annotated with emotions. Certain existing datasets are small, follow different emotion taxonomies and display imbalance in their emotion distribution. In this work, we studied the impact of data augmentation techniques precisely when applied to small imbalanced datasets, for which current state-of-the-art models (such as RoBERTa) under-perform. Specifically, we utilized four data augmentation methods (Easy Data Augmentation EDA, static and contextual Embedding-based, and ProtAugment) on three datasets that come from different sources and vary in size, emotion categories and distributions. Our experimental results show that using the augmented data when training the classifier model leads to significant improvements. Finally, we conducted two case studies: a) directly using t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20855;&#26377;&#19987;&#38376;&#21475;&#38899;&#20195;&#30721;&#26412;&#30340;&#21475;&#38899;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#21147;&#21644;&#21487;&#35757;&#32451;&#20195;&#30721;&#26412;&#65292;&#29992;&#20110;&#31471;&#21040;&#31471;ASR&#31995;&#32479;&#12290;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#24050;&#35265;&#21644;&#26410;&#35265;&#30340;&#21475;&#38899;&#19978;&#37117;&#33021;&#33719;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.15970</link><description>&lt;p&gt;
&#20351;&#29992;&#20855;&#26377;&#19987;&#38376;&#21475;&#38899;&#20195;&#30721;&#26412;&#30340;&#21475;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Accented Speech Recognition With Accent-specific Codebooks. (arXiv:2310.15970v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15970
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20855;&#26377;&#19987;&#38376;&#21475;&#38899;&#20195;&#30721;&#26412;&#30340;&#21475;&#38899;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#21147;&#21644;&#21487;&#35757;&#32451;&#20195;&#30721;&#26412;&#65292;&#29992;&#20110;&#31471;&#21040;&#31471;ASR&#31995;&#32479;&#12290;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#24050;&#35265;&#21644;&#26410;&#35265;&#30340;&#21475;&#38899;&#19978;&#37117;&#33021;&#33719;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#21475;&#38899;&#23545;&#20110;&#29616;&#26377;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#26500;&#25104;&#20102;&#37325;&#35201;&#25361;&#25112;&#12290;&#22312;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#21475;&#38899;&#20013;&#30340;&#24615;&#33021;&#19979;&#38477;&#20005;&#37325;&#38459;&#30861;&#20102;ASR&#30340;&#26222;&#21450;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21475;&#38899;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#21147;&#21644;&#21487;&#35757;&#32451;&#20195;&#30721;&#26412;&#65292;&#29992;&#20110;&#31471;&#21040;&#31471;ASR&#31995;&#32479;&#12290;&#36825;&#20123;&#21487;&#23398;&#20064;&#30340;&#20195;&#30721;&#26412;&#25429;&#25417;&#20102;&#21475;&#38899;&#29305;&#23450;&#20449;&#24687;&#65292;&#24182;&#34987;&#25972;&#21512;&#21040;ASR&#32534;&#30721;&#22120;&#23618;&#20013;&#12290;&#27169;&#22411;&#22312;&#24102;&#21475;&#38899;&#30340;&#33521;&#35821;&#35821;&#38899;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#27979;&#35797;&#25968;&#25454;&#20013;&#20063;&#21253;&#21547;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#35265;&#36807;&#30340;&#21475;&#38899;&#12290;&#22312;Mozilla Common Voice&#22810;&#21475;&#38899;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#19981;&#20165;&#22312;&#24050;&#35265;&#30340;&#33521;&#35821;&#21475;&#38899;&#20013;&#33719;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65288;&#21333;&#35789;&#38169;&#35823;&#29575;&#30456;&#23545;&#25552;&#21319;&#39640;&#36798;37%&#65289;&#65292;&#32780;&#19988;&#22312;&#26410;&#35265;&#30340;&#21475;&#38899;&#19978;&#20063;&#33719;&#24471;&#20102;5%&#30340;&#30456;&#23545;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;L2Artic&#25968;&#25454;&#38598;&#19978;&#30340;&#38646;&#26679;&#26412;&#36801;&#31227;&#35774;&#32622;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#23545;&#27604;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech accents pose a significant challenge to state-of-the-art automatic speech recognition (ASR) systems. Degradation in performance across underrepresented accents is a severe deterrent to the inclusive adoption of ASR. In this work, we propose a novel accent adaptation approach for end-to-end ASR systems using cross-attention with a trainable set of codebooks. These learnable codebooks capture accent-specific information and are integrated within the ASR encoder layers. The model is trained on accented English speech, while the test data also contained accents which were not seen during training. On the Mozilla Common Voice multi-accented dataset, we show that our proposed approach yields significant performance gains not only on the seen English accents (up to $37\%$ relative improvement in word error rate) but also on the unseen accents (up to $5\%$ relative improvement in WER). Further, we illustrate benefits for a zero-shot transfer setup on the L2Artic dataset. We also compare
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35299;&#37322;&#20102;&#25552;&#31034;&#24037;&#31243;&#22312;&#37322;&#25918;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#26041;&#38754;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#30340;&#25552;&#31034;&#26041;&#27861;&#20197;&#21450;&#22806;&#37096;&#25554;&#20214;&#22914;&#20309;&#21327;&#21161;&#20943;&#23569;&#26426;&#22120;&#24187;&#24819;&#65292;&#24182;&#25351;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.14735</link><description>&lt;p&gt;
&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25552;&#31034;&#24037;&#31243;&#28508;&#21147;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Unleashing the potential of prompt engineering in Large Language Models: a comprehensive review. (arXiv:2310.14735v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14735
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35299;&#37322;&#20102;&#25552;&#31034;&#24037;&#31243;&#22312;&#37322;&#25918;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#26041;&#38754;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#30340;&#25552;&#31034;&#26041;&#27861;&#20197;&#21450;&#22806;&#37096;&#25554;&#20214;&#22914;&#20309;&#21327;&#21161;&#20943;&#23569;&#26426;&#22120;&#24187;&#24819;&#65292;&#24182;&#25351;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#25552;&#31034;&#24037;&#31243;&#22312;&#37322;&#25918;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33021;&#21147;&#26041;&#38754;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#25552;&#31034;&#24037;&#31243;&#26159;&#20026;LLM&#26500;&#24314;&#36755;&#20837;&#25991;&#26412;&#30340;&#36807;&#31243;&#65292;&#26159;&#20248;&#21270;LLM&#26377;&#25928;&#24615;&#30340;&#37325;&#35201;&#25216;&#26415;&#12290;&#26412;&#32508;&#36848;&#38416;&#26126;&#20102;&#25552;&#31034;&#24037;&#31243;&#30340;&#22522;&#26412;&#21407;&#29702;&#65292;&#22914;&#35282;&#33394;&#25552;&#31034;&#12289;&#19968;&#27425;&#24615;&#25552;&#31034;&#21644;&#23569;&#37327;&#25552;&#31034;&#65292;&#20197;&#21450;&#26356;&#39640;&#32423;&#30340;&#26041;&#27861;&#65292;&#22914;&#24605;&#32500;&#38142;&#21644;&#24605;&#32500;&#26641;&#25552;&#31034;&#12290;&#26412;&#25991;&#36824;&#38416;&#36848;&#20102;&#22806;&#37096;&#25554;&#20214;&#22914;&#20309;&#21327;&#21161;&#27492;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#26816;&#32034;&#22806;&#37096;&#30693;&#35782;&#26469;&#20943;&#23569;&#26426;&#22120;&#24187;&#24819;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#21246;&#21202;&#20102;&#25552;&#31034;&#24037;&#31243;&#30740;&#31350;&#30340;&#21069;&#26223;&#26041;&#21521;&#65292;&#24378;&#35843;&#20102;&#23545;&#32467;&#26500;&#21644;&#20195;&#29702;&#22312;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#24037;&#20855;&#20013;&#30340;&#20316;&#29992;&#30340;&#28145;&#20837;&#29702;&#35299;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22914;&#20309;&#20174;&#19981;&#21516;&#35282;&#24230;&#21644;&#20351;&#29992;&#19981;&#21516;&#30340;&#26041;&#27861;&#35780;&#20272;&#25552;&#31034;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23637;&#26395;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper delves into the pivotal role of prompt engineering in unleashing the capabilities of Large Language Models (LLMs). Prompt engineering is the process of structuring input text for LLMs and is a technique integral to optimizing the efficacy of LLMs. This survey elucidates foundational principles of prompt engineering, such as role-prompting, one-shot, and few-shot prompting, as well as more advanced methodologies such as the chain-of-thought and tree-of-thoughts prompting. The paper sheds light on how external assistance in the form of plugins can assist in this task, and reduce machine hallucination by retrieving external knowledge. We subsequently delineate prospective directions in prompt engineering research, emphasizing the need for a deeper understanding of structures and the role of agents in Artificial Intelligence-Generated Content (AIGC) tools. We discuss how to assess the efficacy of prompt methods from different perspectives and using different methods. Finally, we
&lt;/p&gt;</description></item><item><title>MedEval&#26159;&#19968;&#20010;&#22810;&#23618;&#27425;&#12289;&#22810;&#20219;&#21153;&#21644;&#22810;&#39046;&#22495;&#30340;&#21307;&#23398;&#22522;&#20934;&#65292;&#29992;&#20110;&#20419;&#36827;&#21307;&#30103;&#20445;&#20581;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#21457;&#12290;&#23427;&#21253;&#21547;&#20102;&#26469;&#33258;&#22810;&#20010;&#21307;&#30103;&#31995;&#32479;&#30340;&#25968;&#25454;&#65292;&#28085;&#30422;&#20102;&#22810;&#20010;&#20154;&#20307;&#21306;&#22495;&#21644;&#26816;&#26597;&#27169;&#24335;&#12290;&#25105;&#20204;&#38024;&#23545;10&#20010;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21457;&#29616;&#19981;&#21516;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#21508;&#26377;&#24046;&#24322;&#65292;&#20174;&#20013;&#25105;&#20204;&#27880;&#24847;&#21040;&#20102;&#25351;&#23548;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.14088</link><description>&lt;p&gt;
MedEval: &#19968;&#20010;&#22810;&#23618;&#27425;&#12289;&#22810;&#20219;&#21153;&#21644;&#22810;&#39046;&#22495;&#30340;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
MedEval: A Multi-Level, Multi-Task, and Multi-Domain Medical Benchmark for Language Model Evaluation. (arXiv:2310.14088v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14088
&lt;/p&gt;
&lt;p&gt;
MedEval&#26159;&#19968;&#20010;&#22810;&#23618;&#27425;&#12289;&#22810;&#20219;&#21153;&#21644;&#22810;&#39046;&#22495;&#30340;&#21307;&#23398;&#22522;&#20934;&#65292;&#29992;&#20110;&#20419;&#36827;&#21307;&#30103;&#20445;&#20581;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#21457;&#12290;&#23427;&#21253;&#21547;&#20102;&#26469;&#33258;&#22810;&#20010;&#21307;&#30103;&#31995;&#32479;&#30340;&#25968;&#25454;&#65292;&#28085;&#30422;&#20102;&#22810;&#20010;&#20154;&#20307;&#21306;&#22495;&#21644;&#26816;&#26597;&#27169;&#24335;&#12290;&#25105;&#20204;&#38024;&#23545;10&#20010;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21457;&#29616;&#19981;&#21516;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#21508;&#26377;&#24046;&#24322;&#65292;&#20174;&#20013;&#25105;&#20204;&#27880;&#24847;&#21040;&#20102;&#25351;&#23548;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#38656;&#35201;&#19987;&#23478;&#30340;&#20154;&#24037;&#27880;&#37322;&#65292;&#21307;&#30103;&#20445;&#20581;&#30340;&#31579;&#36873;&#25968;&#25454;&#38598;&#24448;&#24448;&#26377;&#38480;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;MedEval&#65292;&#19968;&#20010;&#22810;&#23618;&#27425;&#12289;&#22810;&#20219;&#21153;&#21644;&#22810;&#39046;&#22495;&#30340;&#21307;&#23398;&#22522;&#20934;&#65292;&#20197;&#20419;&#36827;&#21307;&#30103;&#20445;&#20581;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#21457;&#12290;MedEval&#26159;&#20840;&#38754;&#30340;&#65292;&#21253;&#21547;&#26469;&#33258;&#20960;&#20010;&#21307;&#30103;&#31995;&#32479;&#30340;&#25968;&#25454;&#65292;&#28085;&#30422;&#20102;8&#31181;&#26816;&#26597;&#27169;&#24335;&#19979;&#30340;35&#20010;&#20154;&#20307;&#21306;&#22495;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;22,779&#20010;&#21477;&#23376;&#21644;21,228&#20221;&#25253;&#21578;&#65292;&#24182;&#22312;&#22810;&#20010;&#23618;&#27425;&#19978;&#25552;&#20379;&#20102;&#19987;&#23478;&#27880;&#37322;&#65292;&#20026;&#25968;&#25454;&#25552;&#20379;&#20102;&#32454;&#33268;&#30340;&#28508;&#22312;&#29992;&#27861;&#65292;&#24182;&#25903;&#25345;&#24191;&#27867;&#30340;&#20219;&#21153;&#33539;&#22260;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#38646;-shot&#21644;&#24494;&#35843;&#35774;&#32622;&#19979;&#23545;10&#20010;&#36890;&#29992;&#21644;&#39046;&#22495;&#29305;&#23450;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#31995;&#32479;&#35780;&#20272;&#65292;&#20174;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#39046;&#22495;&#36866;&#24212;&#22522;&#32447;&#21040;&#36890;&#29992;&#30340;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#25581;&#31034;&#20102;&#20004;&#31181;&#31867;&#21035;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#30340;&#19981;&#21516;&#26377;&#25928;&#24615;&#65292;&#20174;&#20013;&#25105;&#20204;&#27880;&#24847;&#21040;&#20102;&#25351;&#23548;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Curated datasets for healthcare are often limited due to the need of human annotations from experts. In this paper, we present MedEval, a multi-level, multi-task, and multi-domain medical benchmark to facilitate the development of language models for healthcare. MedEval is comprehensive and consists of data from several healthcare systems and spans 35 human body regions from 8 examination modalities. With 22,779 collected sentences and 21,228 reports, we provide expert annotations at multiple levels, offering a granular potential usage of the data and supporting a wide range of tasks. Moreover, we systematically evaluated 10 generic and domain-specific language models under zero-shot and finetuning settings, from domain-adapted baselines in healthcare to general-purposed state-of-the-art large language models (e.g., ChatGPT). Our evaluations reveal varying effectiveness of the two categories of language models across different tasks, from which we notice the importance of instruction t
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#35757;&#32451;&#39640;&#36136;&#37327;AI&#21161;&#25163;&#30340;&#25216;&#26415;&#65292;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#22312;&#22238;&#31572;&#38382;&#39064;&#26102;&#36807;&#20110;&#35844;&#23194;&#65292;&#32780;&#19981;&#26159;&#22374;&#35802;&#65292;&#36890;&#36807;&#20998;&#26512;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#24471;&#20986;&#20102;&#36825;&#19968;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2310.13548</link><description>&lt;p&gt;
&#25506;&#32034;&#35821;&#35328;&#27169;&#22411;&#20013;&#35844;&#23194;&#34892;&#20026;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding Sycophancy in Language Models. (arXiv:2310.13548v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13548
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#35757;&#32451;&#39640;&#36136;&#37327;AI&#21161;&#25163;&#30340;&#25216;&#26415;&#65292;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#22312;&#22238;&#31572;&#38382;&#39064;&#26102;&#36807;&#20110;&#35844;&#23194;&#65292;&#32780;&#19981;&#26159;&#22374;&#35802;&#65292;&#36890;&#36807;&#20998;&#26512;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#24471;&#20986;&#20102;&#36825;&#19968;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#12300;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#12301;&#26159;&#35757;&#32451;&#39640;&#36136;&#37327;AI&#21161;&#25163;&#30340;&#19968;&#31181;&#27969;&#34892;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;RLHF&#21487;&#33021;&#20250;&#40723;&#21169;&#27169;&#22411;&#36890;&#36807;&#19982;&#29992;&#25143;&#20449;&#24565;&#30456;&#31526;&#30340;&#22238;&#31572;&#26469;&#20195;&#26367;&#30495;&#23454;&#22238;&#31572;&#65292;&#36825;&#31181;&#34892;&#20026;&#34987;&#31216;&#20026;&#35844;&#23194;&#34892;&#20026;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;RLHF&#35757;&#32451;&#27169;&#22411;&#20013;&#35844;&#23194;&#34892;&#20026;&#30340;&#26222;&#36941;&#24615;&#20197;&#21450;&#20154;&#31867;&#20559;&#22909;&#21028;&#26029;&#26159;&#21542;&#36215;&#21040;&#20102;&#20316;&#29992;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20116;&#20010;&#26368;&#20808;&#36827;&#30340;AI&#21161;&#25163;&#22312;&#22235;&#20010;&#19981;&#21516;&#30340;&#33258;&#30001;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#19968;&#36143;&#34920;&#29616;&#20986;&#35844;&#23194;&#34892;&#20026;&#12290;&#20026;&#20102;&#29702;&#35299;&#20154;&#31867;&#20559;&#22909;&#26159;&#21542;&#39537;&#21160;&#20102;RLHF&#27169;&#22411;&#30340;&#36825;&#31181;&#24191;&#27867;&#34892;&#20026;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#29616;&#26377;&#30340;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;&#22238;&#31572;&#19982;&#29992;&#25143;&#30340;&#35266;&#28857;&#30456;&#31526;&#26102;&#65292;&#23427;&#26356;&#26377;&#21487;&#33021;&#34987;&#36873;&#20013;&#12290;&#27492;&#22806;&#65292;&#20154;&#31867;&#21644;&#20559;&#22909;&#27169;&#22411;&#65288;PMs&#65289;&#23558;&#26377;&#35828;&#26381;&#21147;&#30340;&#35844;&#23194;&#22238;&#31572;&#19982;&#27491;&#30830;&#22238;&#31572;&#30456;&#27604;&#65292;&#26377;&#26102;&#20960;&#20046;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#22320;&#36873;&#25321;&#20102;&#35844;&#23194;&#22238;&#31572;&#12290;&#20248;&#21270;&#27169;&#22411;&#36755;&#20986;&#20197;&#28385;&#36275;PMs&#26377;&#26102;&#20063;&#20250;&#22312;&#30495;&#23454;&#24615;&#21644;&#35844;&#23194;&#34892;&#20026;&#20043;&#38388;&#20570;&#20986;&#21462;&#33293;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning from human feedback (RLHF) is a popular technique for training high-quality AI assistants. However, RLHF may also encourage model responses that match user beliefs over truthful responses, a behavior known as sycophancy. We investigate the prevalence of sycophancy in RLHF-trained models and whether human preference judgements are responsible. We first demonstrate that five state-of-the-art AI assistants consistently exhibit sycophantic behavior across four varied free-form text-generation tasks. To understand if human preferences drive this broadly observed behavior of RLHF models, we analyze existing human preference data. We find that when a response matches a user's views, it is more likely to be preferred. Moreover, both humans and preference models (PMs) prefer convincingly-written sycophantic responses over correct ones a negligible fraction of the time. Optimizing model outputs against PMs also sometimes sacrifices truthfulness in favor of sycophancy. Over
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26080;&#38480;&#21046;&#31572;&#26696;&#33539;&#22260;&#30340;&#24320;&#25918;&#24335;&#24120;&#35782;&#25512;&#29702;&#38382;&#39064;&#12290;&#20316;&#32773;&#37319;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#22806;&#37096;&#30693;&#35782;&#24211;&#19978;&#36845;&#20195;&#26816;&#32034;&#25512;&#29702;&#36335;&#24452;&#65292;&#20174;&#32780;&#24110;&#21161;&#25214;&#21040;&#26368;&#20934;&#30830;&#30340;&#31572;&#26696;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#27492;&#26041;&#27861;&#22312;&#24120;&#35782;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.11672</link><description>&lt;p&gt;
&#26080;&#38480;&#21046;&#31572;&#26696;&#33539;&#22260;&#30340;&#24320;&#25918;&#24335;&#24120;&#35782;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Open-ended Commonsense Reasoning with Unrestricted Answer Scope. (arXiv:2310.11672v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11672
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26080;&#38480;&#21046;&#31572;&#26696;&#33539;&#22260;&#30340;&#24320;&#25918;&#24335;&#24120;&#35782;&#25512;&#29702;&#38382;&#39064;&#12290;&#20316;&#32773;&#37319;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#22806;&#37096;&#30693;&#35782;&#24211;&#19978;&#36845;&#20195;&#26816;&#32034;&#25512;&#29702;&#36335;&#24452;&#65292;&#20174;&#32780;&#24110;&#21161;&#25214;&#21040;&#26368;&#20934;&#30830;&#30340;&#31572;&#26696;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#27492;&#26041;&#27861;&#22312;&#24120;&#35782;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#24335;&#24120;&#35782;&#25512;&#29702;&#34987;&#23450;&#20041;&#20026;&#22312;&#19981;&#25552;&#20379;1&#65289;&#31572;&#26696;&#20505;&#36873;&#21517;&#21333;&#21644;2&#65289;&#39044;&#23450;&#20041;&#31572;&#26696;&#33539;&#22260;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#24120;&#35782;&#38382;&#39064;&#12290;&#23558;&#24120;&#35782;&#38382;&#39064;&#36716;&#21270;&#20026;&#38382;&#31572;&#24418;&#24335;&#25110;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#23398;&#20064;&#26816;&#32034;&#26041;&#27861;&#30340;&#24120;&#35268;&#26041;&#27861;&#22312;&#24320;&#25918;&#24335;&#29615;&#22659;&#20013;&#19981;&#22826;&#36866;&#29992;&#65292;&#22240;&#20026;&#36825;&#28041;&#21450;&#21040;&#19968;&#20010;&#22266;&#26377;&#30340;&#25361;&#25112;&#12290;&#22312;&#27809;&#26377;&#39044;&#23450;&#20041;&#31572;&#26696;&#33539;&#22260;&#25110;&#23569;&#25968;&#20505;&#36873;&#32773;&#30340;&#24773;&#20917;&#19979;&#65292;&#24320;&#25918;&#24335;&#24120;&#35782;&#25512;&#29702;&#38656;&#35201;&#36890;&#36807;&#22312;&#26497;&#22823;&#30340;&#25628;&#32034;&#31354;&#38388;&#20013;&#25628;&#32034;&#26469;&#39044;&#27979;&#31572;&#26696;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#38382;&#39064;&#38656;&#35201;&#38544;&#21547;&#30340;&#22810;&#36339;&#25512;&#29702;&#65292;&#36825;&#32473;&#25105;&#20204;&#30340;&#38382;&#39064;&#24102;&#26469;&#20102;&#26356;&#22810;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#22806;&#37096;&#30693;&#35782;&#24211;&#19978;&#36845;&#20195;&#22320;&#26816;&#32034;&#25512;&#29702;&#36335;&#24452;&#65292;&#36825;&#19981;&#38656;&#35201;&#29305;&#23450;&#20219;&#21153;&#30340;&#30417;&#30563;&#12290;&#25512;&#29702;&#36335;&#24452;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#25214;&#21040;&#26368;&#20934;&#30830;&#30340;&#24120;&#35782;&#38382;&#39064;&#31572;&#26696;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#24120;&#35782;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open-ended Commonsense Reasoning is defined as solving a commonsense question without providing 1) a short list of answer candidates and 2) a pre-defined answer scope. Conventional ways of formulating the commonsense question into a question-answering form or utilizing external knowledge to learn retrieval-based methods are less applicable in the open-ended setting due to an inherent challenge. Without pre-defining an answer scope or a few candidates, open-ended commonsense reasoning entails predicting answers by searching over an extremely large searching space. Moreover, most questions require implicit multi-hop reasoning, which presents even more challenges to our problem. In this work, we leverage pre-trained language models to iteratively retrieve reasoning paths on the external knowledge base, which does not require task-specific supervision. The reasoning paths can help to identify the most precise answer to the commonsense question. We conduct experiments on two commonsense ben
&lt;/p&gt;</description></item><item><title>VoxArabica&#26159;&#19968;&#20010;&#31283;&#20581;&#30340;&#26041;&#35328;&#24863;&#30693;&#38463;&#25289;&#20271;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#65292;&#36890;&#36807;&#24320;&#21457;&#21644;&#28436;&#31034;&#65292;&#23454;&#29616;&#20102;&#38463;&#25289;&#20271;&#35821;&#26041;&#35328;&#35782;&#21035;&#21644;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#12290;&#35813;&#31995;&#32479;&#35757;&#32451;&#20102;&#21508;&#31181;&#27169;&#22411;&#29992;&#20110;&#19981;&#21516;&#26041;&#35328;&#30340;&#35782;&#21035;&#65292;&#24182;&#25552;&#20379;&#20102;&#22810;&#31181;&#21151;&#33021;&#30340;&#32593;&#32476;&#30028;&#38754;&#12290;</title><link>http://arxiv.org/abs/2310.11069</link><description>&lt;p&gt;
VoxArabica&#65306;&#19968;&#20010;&#31283;&#20581;&#30340;&#26041;&#35328;&#24863;&#30693;&#38463;&#25289;&#20271;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
VoxArabica: A Robust Dialect-Aware Arabic Speech Recognition System. (arXiv:2310.11069v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11069
&lt;/p&gt;
&lt;p&gt;
VoxArabica&#26159;&#19968;&#20010;&#31283;&#20581;&#30340;&#26041;&#35328;&#24863;&#30693;&#38463;&#25289;&#20271;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#65292;&#36890;&#36807;&#24320;&#21457;&#21644;&#28436;&#31034;&#65292;&#23454;&#29616;&#20102;&#38463;&#25289;&#20271;&#35821;&#26041;&#35328;&#35782;&#21035;&#21644;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#12290;&#35813;&#31995;&#32479;&#35757;&#32451;&#20102;&#21508;&#31181;&#27169;&#22411;&#29992;&#20110;&#19981;&#21516;&#26041;&#35328;&#30340;&#35782;&#21035;&#65292;&#24182;&#25552;&#20379;&#20102;&#22810;&#31181;&#21151;&#33021;&#30340;&#32593;&#32476;&#30028;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#25289;&#20271;&#35821;&#26159;&#19968;&#31181;&#22797;&#26434;&#30340;&#35821;&#35328;&#65292;&#20840;&#29699;&#26377;&#36229;&#36807;4.5&#20159;&#20154;&#21475;&#20351;&#29992;&#35768;&#22810;&#19981;&#21516;&#30340;&#26041;&#35328;&#21644;&#21475;&#38899;&#12290;&#30001;&#20110;&#35821;&#35328;&#30340;&#22810;&#26679;&#24615;&#21644;&#21464;&#21270;&#65292;&#20026;&#38463;&#25289;&#20271;&#35821;&#26500;&#24314;&#19968;&#20010;&#31283;&#20581;&#19988;&#36890;&#29992;&#30340;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#21644;&#28436;&#31034;&#19968;&#20010;&#21517;&#20026;VoxArabica&#30340;&#31995;&#32479;&#65292;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#29992;&#20110;&#26041;&#35328;&#35782;&#21035;(DID)&#21644;&#38463;&#25289;&#20271;&#35821;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;(ASR)&#12290;&#25105;&#20204;&#22312;&#30417;&#30563;&#29615;&#22659;&#19979;&#35757;&#32451;&#20102;&#21508;&#31181;&#27169;&#22411;&#65292;&#20363;&#22914;HuBERT(DID)&#12289;Whisper&#21644;XLS-R(ASR)&#65292;&#29992;&#20110;&#38463;&#25289;&#20271;&#35821;&#30340;DID&#21644;ASR&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;DID&#27169;&#22411;&#34987;&#35757;&#32451;&#29992;&#20110;&#35782;&#21035;&#38500;&#20102;&#26631;&#20934;&#38463;&#25289;&#20271;&#20043;&#22806;&#30340;17&#31181;&#19981;&#21516;&#30340;&#26041;&#35328;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;&#38463;&#25289;&#20271;&#35821;(MSA)&#12289;&#22467;&#21450;&#35821;&#12289;&#25705;&#27931;&#21733;&#35821;&#21644;&#28151;&#21512;&#25968;&#25454;&#19978;&#24494;&#35843;&#25105;&#20204;&#30340;ASR&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;ASR&#20013;&#30340;&#20854;&#20182;&#26041;&#35328;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;Whisper&#21644;MMS&#31561;&#19981;&#21516;&#27169;&#22411;&#30340;&#36873;&#25321;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#27169;&#22411;&#38598;&#25104;&#21040;&#19968;&#20010;&#21333;&#19968;&#30340;&#32593;&#32476;&#30028;&#38754;&#20013;&#65292;&#20855;&#26377;&#22810;&#26679;&#30340;&#21151;&#33021;&#65292;&#22914;&#38899;&#39057;&#24405;&#21046;&#12289;&#19978;&#20256;&#25991;&#20214;&#12289;&#27169;&#22411;&#36873;&#25321;&#21644;&#25552;&#20986;&#38382;&#39064;&#30340;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
Arabic is a complex language with many varieties and dialects spoken by over 450 millions all around the world. Due to the linguistic diversity and variations, it is challenging to build a robust and generalized ASR system for Arabic. In this work, we address this gap by developing and demoing a system, dubbed VoxArabica, for dialect identification (DID) as well as automatic speech recognition (ASR) of Arabic. We train a wide range of models such as HuBERT (DID), Whisper, and XLS-R (ASR) in a supervised setting for Arabic DID and ASR tasks. Our DID models are trained to identify 17 different dialects in addition to MSA. We finetune our ASR models on MSA, Egyptian, Moroccan, and mixed data. Additionally, for the remaining dialects in ASR, we provide the option to choose various models such as Whisper and MMS in a zero-shot setting. We integrate these models into a single web interface with diverse features such as audio recording, file upload, model selection, and the option to raise fl
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Reward-Augmented Decoding (RAD)&#30340;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#65292;&#20351;&#29992;&#23567;&#22411;&#30340;&#21333;&#21521;&#22870;&#21169;&#27169;&#22411;&#26469;&#40723;&#21169;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20855;&#26377;&#29305;&#23450;&#23646;&#24615;&#30340;&#25991;&#26412;&#12290;RAD&#22312;&#29983;&#25104;&#38750;&#26377;&#23475;&#21644;&#24773;&#24863;&#21463;&#25511;&#25991;&#26412;&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#65292;&#24182;&#19988;&#22312;&#38750;&#24120;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#19978;&#20063;&#24456;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2310.09520</link><description>&lt;p&gt;
Reward-Augmented Decoding: &#20351;&#29992;&#21333;&#21521;&#22870;&#21169;&#27169;&#22411;&#23454;&#29616;&#39640;&#25928;&#30340;&#21463;&#25511;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Reward-Augmented Decoding: Efficient Controlled Text Generation With a Unidirectional Reward Model. (arXiv:2310.09520v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09520
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Reward-Augmented Decoding (RAD)&#30340;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#65292;&#20351;&#29992;&#23567;&#22411;&#30340;&#21333;&#21521;&#22870;&#21169;&#27169;&#22411;&#26469;&#40723;&#21169;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20855;&#26377;&#29305;&#23450;&#23646;&#24615;&#30340;&#25991;&#26412;&#12290;RAD&#22312;&#29983;&#25104;&#38750;&#26377;&#23475;&#21644;&#24773;&#24863;&#21463;&#25511;&#25991;&#26412;&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#65292;&#24182;&#19988;&#22312;&#38750;&#24120;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#19978;&#20063;&#24456;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#29983;&#25104;&#30340;&#25991;&#26412;&#23384;&#22312;&#38382;&#39064;&#25110;&#32773;&#32570;&#20047;&#25152;&#38656;&#30340;&#23646;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Reward-Augmented Decoding (RAD)&#30340;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#19968;&#20010;&#23567;&#22411;&#30340;&#21333;&#21521;&#22870;&#21169;&#27169;&#22411;&#26469;&#40723;&#21169;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20855;&#26377;&#29305;&#23450;&#23646;&#24615;&#30340;&#25991;&#26412;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;RAD&#21033;&#29992;&#22870;&#21169;&#27169;&#22411;&#23545;&#29983;&#25104;&#30340;&#25991;&#26412;&#36827;&#34892;&#35780;&#20998;&#65292;&#24182;&#36890;&#36807;&#37325;&#26032;&#35843;&#25972;&#37319;&#26679;&#27010;&#29575;&#26469;&#26356;&#20542;&#21521;&#20110;&#39640;&#22870;&#21169;&#30340;&#26631;&#35760;&#12290;&#36890;&#36807;&#20351;&#29992;&#21333;&#21521;&#22870;&#21169;&#27169;&#22411;&#65292;RAD&#33021;&#22815;&#32531;&#23384;&#20808;&#21069;&#29983;&#25104;&#27493;&#39588;&#30340;&#28608;&#27963;&#20540;&#65292;&#38477;&#20302;&#35745;&#31639;&#24320;&#38144;&#12290;&#36890;&#36807;&#22312;&#29983;&#25104;&#38750;&#26377;&#23475;&#21644;&#24773;&#24863;&#21463;&#25511;&#25991;&#26412;&#26041;&#38754;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;RAD&#22312;&#20165;&#25913;&#21464;&#29983;&#25104;&#36807;&#31243;&#30340;&#26041;&#27861;&#20013;&#34920;&#29616;&#26368;&#20339;&#65292;&#24182;&#19988;&#19982;&#28041;&#21450;&#37325;&#26032;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;RAD&#22312;&#38750;&#24120;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models have proven effective in a huge range of downstream applications, they often generate text that is problematic or lacks a desired attribute. In this paper, we introduce Reward-Augmented Decoding (RAD), a text generation procedure that uses a small unidirectional reward model to encourage a language model to generate text that has certain properties. Specifically, RAD uses the reward model to score generations as they are produced and rescales sampling probabilities to favor high-reward tokens. By using a unidirectional reward model, RAD can cache activations from prior generation steps to decrease computational overhead. Through experiments on generating non-toxic and sentiment-controlled text, we demonstrate that RAD performs best among methods that change only the generation procedure and matches the performance of state-of-the-art methods that involve re-training the language model. We further validate that RAD is effective on very large language models w
&lt;/p&gt;</description></item><item><title>EMO&#25552;&#20986;&#20102;&#22320;&#29699;&#31227;&#21160;&#36317;&#31163;&#20248;&#21270;&#65288;EMO&#65289;&#26469;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36864;&#21270;&#29616;&#35937;&#12290;EMO&#21033;&#29992;&#20102;&#22320;&#29699;&#31227;&#21160;&#36317;&#31163;&#30340;&#29305;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#34892;&#30340;&#19978;&#30028;&#26469;&#31616;&#21270;&#35757;&#32451;&#12290;&#32463;&#36807;&#35780;&#20272;&#65292;&#21457;&#29616;EMO&#22312;&#35821;&#35328;&#27169;&#22411;&#19978;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.04691</link><description>&lt;p&gt;
EMO: Earth Mover Distance Optimization for Auto-Regressive Language Modeling. (arXiv:2310.04691v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
EMO: Earth Mover Distance Optimization for Auto-Regressive Language Modeling. (arXiv:2310.04691v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04691
&lt;/p&gt;
&lt;p&gt;
EMO&#25552;&#20986;&#20102;&#22320;&#29699;&#31227;&#21160;&#36317;&#31163;&#20248;&#21270;&#65288;EMO&#65289;&#26469;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36864;&#21270;&#29616;&#35937;&#12290;EMO&#21033;&#29992;&#20102;&#22320;&#29699;&#31227;&#21160;&#36317;&#31163;&#30340;&#29305;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#34892;&#30340;&#19978;&#30028;&#26469;&#31616;&#21270;&#35757;&#32451;&#12290;&#32463;&#36807;&#35780;&#20272;&#65292;&#21457;&#29616;EMO&#22312;&#35821;&#35328;&#27169;&#22411;&#19978;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#26159;&#20154;&#25991;&#26412;&#30340;&#27010;&#29575;&#27169;&#22411;&#12290;&#23427;&#20204;&#20027;&#35201;&#36890;&#36807;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;MLE&#65289;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#26041;&#27861;&#31561;&#21516;&#20110;&#26368;&#23567;&#21270;&#32463;&#39564;&#25968;&#25454;&#20998;&#24067;&#21644;&#27169;&#22411;&#20998;&#24067;&#20043;&#38388;&#30340;&#21069;&#21521;&#20132;&#21449;&#29109;&#12290;&#28982;&#32780;&#65292;&#24403;&#20174;&#36825;&#20123;&#27169;&#22411;&#23398;&#20064;&#30340;&#20998;&#24067;&#35299;&#30721;&#26102;&#65292;&#20173;&#28982;&#32463;&#24120;&#35266;&#23519;&#21040;&#21508;&#31181;&#36864;&#21270;&#29616;&#35937;&#12290;&#25105;&#20204;&#30830;&#23450;&#21069;&#21521;&#20132;&#21449;&#29109;&#20316;&#20026;&#20154;&#19982;&#27169;&#22411;&#20998;&#24067;&#23545;&#40784;&#30340;&#36317;&#31163;&#24230;&#37327;&#26159;&#27425;&#20248;&#30340;&#65292;&#21407;&#22240;&#26377;&#65306;&#65288;1&#65289;&#21484;&#22238;&#20248;&#21270;&#65292;&#65288;2&#65289;&#36127;&#26679;&#26412;&#22810;&#26679;&#24615;&#24573;&#35270;&#21644;&#65288;3&#65289;&#35757;&#32451;&#27979;&#35797;&#19981;&#21305;&#37197;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#30340;&#22320;&#29699;&#31227;&#21160;&#36317;&#31163;&#20248;&#21270;&#65288;EMO&#65289;&#12290;EMO&#21033;&#29992;&#22320;&#29699;&#31227;&#21160;&#36317;&#31163;&#30340;&#20869;&#22312;&#29305;&#24615;&#26469;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#12290;&#30001;&#20110;&#30452;&#25509;&#35745;&#31639;&#30340;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;EMO&#19978;&#30028;&#26469;&#31616;&#21270;&#31471;&#21040;&#31471;&#35757;&#32451;&#12290;&#32463;&#36807;&#24191;&#27867;&#35780;&#20272;&#20043;&#21518;&#65292;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35821;&#35328;&#27169;&#22411;&#19978;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural language models are probabilistic models of human text. They are predominantly trained using maximum likelihood estimation (MLE), which is equivalent to minimizing the forward cross-entropy between the empirical data distribution and the model distribution. However, various degeneration phenomena are still widely observed when decoding from the distributions learned by such models. We establish that the forward cross-entropy is suboptimal as a distance metric for aligning human and model distribution due to its (1) recall-prioritization (2) negative diversity ignorance and (3) train-test mismatch. In this paper, we propose Earth Mover Distance Optimization (EMO) for auto-regressive language modeling. EMO capitalizes on the inherent properties of earth mover distance to address the aforementioned challenges. Due to the high complexity of direct computation, we further introduce a feasible upper bound for EMO to ease end-to-end training. Upon extensive evaluation of language model
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#21521;&#27861;&#23448;&#25552;&#20986;&#19968;&#31995;&#21015;&#26597;&#35810;&#26469;&#35780;&#20272;LLMs&#30340;&#23545;&#35805;&#25512;&#29702;&#21644;&#35268;&#21010;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#30340;LLMs&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2310.01468</link><description>&lt;p&gt;
&#23454;&#20307;&#25512;&#26029;&#31454;&#25216;&#22330;&#65306;&#25506;&#31350;LLMs&#30340;&#23545;&#35805;&#25512;&#29702;&#21644;&#35268;&#21010;&#33021;&#21147;&#30340;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
The Entity-Deduction Arena: A playground for probing the conversational reasoning and planning capabilities of LLMs. (arXiv:2310.01468v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#21521;&#27861;&#23448;&#25552;&#20986;&#19968;&#31995;&#21015;&#26597;&#35810;&#26469;&#35780;&#20272;LLMs&#30340;&#23545;&#35805;&#25512;&#29702;&#21644;&#35268;&#21010;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#30340;LLMs&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22238;&#31572;&#26126;&#30830;&#25552;&#38382;&#26102;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#20020;&#21547;&#31946;&#19981;&#28165;&#30340;&#26597;&#35810;&#26102;&#65292;&#23427;&#20204;&#21487;&#33021;&#34892;&#20026;&#38590;&#20197;&#39044;&#27979;&#24182;&#20135;&#29983;&#38169;&#35823;&#30340;&#36755;&#20986;&#12290;&#36825;&#20984;&#26174;&#20102;&#38656;&#35201;&#24320;&#21457;&#33021;&#22815;&#25552;&#20986;&#28548;&#28165;&#38382;&#39064;&#20197;&#26377;&#25928;&#35299;&#20915;&#27495;&#20041;&#30340;&#26234;&#33021;&#20195;&#29702;&#30340;&#38656;&#27714;&#12290;&#36825;&#31181;&#33021;&#21147;&#38656;&#35201;&#23545;&#22810;&#20010;&#23545;&#35805;&#36718;&#27425;&#36827;&#34892;&#22797;&#26434;&#30340;&#29702;&#35299;&#12289;&#29366;&#24577;&#36319;&#36394;&#12289;&#25512;&#29702;&#21644;&#35268;&#21010;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#27979;&#37327;&#36825;&#31181;&#33021;&#21147;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26367;&#20195;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#21521;&#27861;&#23448;&#25552;&#20986;&#19968;&#31995;&#21015;&#26597;&#35810;&#65292;&#35780;&#20272;&#20102;LLMs&#25512;&#26029;&#33258;&#24049;&#19981;&#30693;&#36947;&#20294;&#34987;&#27861;&#23448;&#25581;&#31034;&#30340;&#23454;&#20307;&#30340;&#33021;&#21147;&#12290;&#36825;&#20010;&#8220;&#23454;&#20307;&#25512;&#26029;&#28216;&#25103;&#8221;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#25506;&#31350;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#35805;&#25512;&#29702;&#21644;&#35268;&#21010;&#33021;&#21147;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;&#21508;&#31181;LLMs&#65292;&#24182;&#21457;&#29616;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#23427;&#20204;&#30340;&#24615;&#33021;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#25105;&#20204;&#21457;&#29616;&#24378;&#22823;&#30340;LLMs...
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are currently effective at answering questions that are clearly asked. However, when faced with ambiguous queries they can act unpredictably and produce incorrect outputs. This underscores the need for the development of intelligent agents capable of asking clarification questions to resolve ambiguities effectively. This capability requires complex understanding, state tracking, reasoning and planning over multiple conversational turns. However, directly measuring this can be challenging. In this paper, we offer a surrogate problem which assesses an LLMs's capability to deduce an entity unknown to itself, but revealed to a judge, by asking the judge a series of queries. This \textit{entity-deducing game} can serve as an evaluation framework to probe the conversational reasoning and planning capabilities of language models. We systematically evaluate various LLMs and discover significant differences in their performance on this task. We find that strong LLMs
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#31995;&#32479;&#31995;&#32479;&#32452;&#21512;&#30340;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.06520</link><description>&lt;p&gt;
&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#31995;&#32479;&#30340;&#31995;&#32479;&#32452;&#21512;&#30340;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Minimum Bayes' Risk Decoding for System Combination of Grammatical Error Correction Systems. (arXiv:2309.06520v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#31995;&#32479;&#31995;&#32479;&#32452;&#21512;&#30340;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#20219;&#21153;&#26469;&#35828;&#65292;&#23558;&#21508;&#20010;&#31995;&#32479;&#30340;&#36755;&#20986;&#36827;&#34892;&#32452;&#21512;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24037;&#20316;&#12290;&#21516;&#26102;&#65292;&#35299;&#30721;&#20934;&#21017;&#19982;&#35780;&#20272;&#20934;&#21017;&#20043;&#38388;&#36890;&#24120;&#23384;&#22312;&#19981;&#21305;&#37197;&#12290;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#65288;MBR&#65289;&#35299;&#30721;&#21487;&#20197;&#29992;&#20110;&#20197;&#26356;&#22909;&#22320;&#19982;&#26368;&#32456;&#35780;&#20272;&#20934;&#21017;&#23545;&#40784;&#30340;&#26041;&#24335;&#32452;&#21512;&#31995;&#32479;&#30340;&#36755;&#20986;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#65288;GEC&#65289;&#31995;&#32479;&#20013;&#30340;MBR&#35299;&#30721;&#65292;&#35813;&#31995;&#32479;&#36890;&#24120;&#20197;&#32534;&#36753;&#27425;&#25968;&#21644;&#30456;&#20851;&#30340;F&#20998;&#25968;&#26469;&#35780;&#20272;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;&#36825;&#31181;&#20934;&#21017;&#30452;&#25509;&#30456;&#20851;&#30340;&#26032;&#39062;MBR&#25439;&#22833;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#25991;&#20013;&#36824;&#25551;&#36848;&#20102;&#19968;&#31181;&#25193;&#23637;&#20505;&#36873;&#21477;&#23376;&#38598;&#21512;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#24403;&#21069;&#30340;&#26368;&#22823;&#25237;&#31080;&#32452;&#21512;&#26041;&#26696;&#65292;&#20197;&#21450;&#20010;&#20307;&#32534;&#36753;&#32423;&#21035;&#30340;&#36873;&#25321;&#12290;&#22312;&#19977;&#20010;&#27969;&#34892;&#30340;GEC&#25968;&#25454;&#38598;&#21644;&#26368;&#20808;&#36827;&#30340;GEC&#31995;&#32479;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;MBR&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#36824;&#31361;&#20986;&#20102;MBR&#35299;&#30721;&#20013;&#19981;&#21516;&#22870;&#21169;&#25351;&#26631;&#30340;&#21464;&#21270;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
For sequence-to-sequence tasks it is challenging to combine individual system outputs. Further, there is also often a mismatch between the decoding criterion and the one used for assessment. Minimum Bayes' Risk (MBR) decoding can be used to combine system outputs in a manner that encourages better alignment with the final assessment criterion. This paper examines MBR decoding for Grammatical Error Correction (GEC) systems, where performance is usually evaluated in terms of edits and an associated F-score. Hence, we propose a novel MBR loss function directly linked to this form of criterion. Furthermore, an approach to expand the possible set of candidate sentences is described. This builds on a current max-voting combination scheme, as well as individual edit-level selection. Experiments on three popular GEC datasets and with state-of-the-art GEC systems demonstrate the efficacy of the proposed MBR approach. Additionally, the paper highlights how varying reward metrics within the MBR d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23450;&#24615;&#20998;&#26512;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#33258;&#30001;&#22238;&#31572;&#65292;&#37325;&#28857;&#32771;&#23519;&#20102;&#31639;&#27861;&#20445;&#30495;&#24230;&#65292;&#24182;&#25552;&#20986;&#39640;&#31639;&#27861;&#20445;&#30495;&#24230;&#21487;&#20197;&#25512;&#24191;&#21040;&#30495;&#23454;&#20154;&#31867;&#30340;&#35266;&#28857;&#12290;&#36825;&#23545;&#20110;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#20154;&#31867;&#34892;&#20026;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2309.06364</link><description>&lt;p&gt;
&#22522;&#20110;&#26694;&#26550;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#30001;&#22238;&#31572;&#30340;&#23450;&#24615;&#20998;&#26512;&#65306;&#31639;&#27861;&#20445;&#30495;&#24230;
&lt;/p&gt;
&lt;p&gt;
Framework-Based Qualitative Analysis of Free Responses of Large Language Models: Algorithmic Fidelity. (arXiv:2309.06364v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23450;&#24615;&#20998;&#26512;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#33258;&#30001;&#22238;&#31572;&#65292;&#37325;&#28857;&#32771;&#23519;&#20102;&#31639;&#27861;&#20445;&#30495;&#24230;&#65292;&#24182;&#25552;&#20986;&#39640;&#31639;&#27861;&#20445;&#30495;&#24230;&#21487;&#20197;&#25512;&#24191;&#21040;&#30495;&#23454;&#20154;&#31867;&#30340;&#35266;&#28857;&#12290;&#36825;&#23545;&#20110;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#20154;&#31867;&#34892;&#20026;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21487;&#20197;&#27169;&#25311;&#33258;&#30001;&#22238;&#31572;&#38754;&#35797;&#38382;&#39064;&#65292;&#23601;&#20687;&#20256;&#32479;&#19978;&#20351;&#29992;&#23450;&#24615;&#30740;&#31350;&#26041;&#27861;&#20998;&#26512;&#30340;&#37027;&#26679;&#12290;&#23450;&#24615;&#26041;&#27861;&#28085;&#30422;&#20102;&#19968;&#31995;&#21015;&#25216;&#26415;&#65292;&#28041;&#21450;&#23545;&#24320;&#25918;&#24335;&#35775;&#35848;&#25110;&#33258;&#30001;&#36827;&#34892;&#30340;&#33258;&#28982;&#35821;&#35328;&#23545;&#35805;&#30340;&#25163;&#21160;&#20998;&#26512;&#12290;&#26412;&#25991;&#32771;&#34385;&#36890;&#36807;&#23450;&#24615;&#26041;&#27861;&#23545;LLMs&#29983;&#25104;&#30340;"&#30789;&#21442;&#19982;&#32773;"&#36827;&#34892;&#30740;&#31350;&#65292;&#20174;&#32780;&#20135;&#29983;&#21487;&#33021;&#21487;&#20197;&#25512;&#24191;&#21040;&#30495;&#23454;&#20154;&#32676;&#30340;&#27934;&#23519;&#21147;&#12290;&#25105;&#20204;&#20998;&#26512;&#30340;&#20851;&#38190;&#27010;&#24565;&#26159;&#31639;&#27861;&#20445;&#30495;&#24230;&#65292;&#36825;&#26159;&#30001;Argyle&#31561;&#20154;&#65288;2023&#24180;&#65289;&#24341;&#20837;&#30340;&#19968;&#20010;&#26415;&#35821;&#65292;&#29992;&#20110;&#25551;&#36848;LLM&#29983;&#25104;&#30340;&#36755;&#20986;&#19982;&#20154;&#31867;&#20122;&#32676;&#20307;&#30340;&#20449;&#24565;&#21644;&#24577;&#24230;&#30340;&#31243;&#24230;&#30456;&#21563;&#21512;&#12290;&#26681;&#25454;&#23450;&#20041;&#65292;&#39640;&#31639;&#27861;&#20445;&#30495;&#24230;&#34920;&#26126;&#20174;LLMs&#20013;&#25552;&#21462;&#30340;&#28508;&#22312;&#20449;&#24565;&#21487;&#33021;&#21487;&#20197;&#25512;&#24191;&#21040;&#30495;&#23454;&#20154;&#31867;&#65292;&#32780;&#20302;&#31639;&#27861;&#20445;&#30495;&#24230;&#21017;&#20351;&#24471;&#36825;&#26679;&#30340;&#30740;&#31350;&#26080;&#25928;&#12290;&#26412;&#25991;&#20351;&#29992;LLM&#29983;&#25104;&#38754;&#35797;&#38382;&#31572;&#65292;...
&lt;/p&gt;
&lt;p&gt;
Today, using Large-scale generative Language Models (LLMs) it is possible to simulate free responses to interview questions like those traditionally analyzed using qualitative research methods. Qualitative methodology encompasses a broad family of techniques involving manual analysis of open-ended interviews or conversations conducted freely in natural language. Here we consider whether artificial "silicon participants" generated by LLMs may be productively studied using qualitative methods aiming to produce insights that could generalize to real human populations. The key concept in our analysis is algorithmic fidelity, a term introduced by Argyle et al. (2023) capturing the degree to which LLM-generated outputs mirror human sub-populations' beliefs and attitudes. By definition, high algorithmic fidelity suggests latent beliefs elicited from LLMs may generalize to real humans, whereas low algorithmic fidelity renders such research invalid. Here we used an LLM to generate interviews wi
&lt;/p&gt;</description></item><item><title>&#27604;&#36739;&#20102;&#26367;&#25442;&#36328;&#35821;&#35328;&#35789;&#27719;&#30340;&#20960;&#31181;&#25216;&#26415;&#65292;&#35777;&#26126;&#20102;&#21333;&#35821;&#36716;&#31227;&#25991;&#29486;&#20013;&#30340;&#26041;&#27861;&#19981;&#36866;&#29992;&#20110;&#22810;&#35821;&#35328;&#27169;&#22411;&#12290;&#19987;&#38376;&#21270;&#30340;&#36739;&#23567;&#35789;&#27719;&#23545;&#20110;&#25552;&#39640;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#26159;&#26377;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2309.04679</link><description>&lt;p&gt;
&#23884;&#20837;&#32467;&#26500;&#30340;&#37325;&#35201;&#24615;&#65306;&#27604;&#36739;&#36866;&#24212;&#26032;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#35789;&#27719;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Embedding structure matters: Comparing methods to adapt multilingual vocabularies to new languages. (arXiv:2309.04679v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04679
&lt;/p&gt;
&lt;p&gt;
&#27604;&#36739;&#20102;&#26367;&#25442;&#36328;&#35821;&#35328;&#35789;&#27719;&#30340;&#20960;&#31181;&#25216;&#26415;&#65292;&#35777;&#26126;&#20102;&#21333;&#35821;&#36716;&#31227;&#25991;&#29486;&#20013;&#30340;&#26041;&#27861;&#19981;&#36866;&#29992;&#20110;&#22810;&#35821;&#35328;&#27169;&#22411;&#12290;&#19987;&#38376;&#21270;&#30340;&#36739;&#23567;&#35789;&#27719;&#23545;&#20110;&#25552;&#39640;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#25903;&#25345;&#33521;&#35821;&#20197;&#22806;&#30340;&#29616;&#20195;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#30340;&#22823;&#37096;&#20998;&#12290;&#29992;&#20110;&#29305;&#23450;&#35821;&#35328;&#21270;&#30340;&#24378;&#22823;&#22522;&#20934;&#26159;&#35821;&#35328;&#36866;&#24212;&#39044;&#35757;&#32451;&#65288;LAPT&#65289;&#12290;&#20294;&#26159;&#65292;&#22312;&#36866;&#24212;&#36807;&#31243;&#20013;&#20445;&#30041;&#22823;&#22411;&#36328;&#35821;&#35328;&#35789;&#27719;&#21644;&#23884;&#20837;&#30697;&#38453;&#20250;&#24102;&#26469;&#30456;&#24403;&#22810;&#30340;&#22810;&#20313;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#31616;&#21333;&#30340;&#25216;&#26415;&#26469;&#29992;&#32039;&#20945;&#30340;&#29305;&#23450;&#35821;&#35328;&#35789;&#27719;&#26367;&#25442;&#36328;&#35821;&#35328;&#35789;&#27719;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#22312;&#35789;&#27719;&#19987;&#38376;&#21270;&#21518;&#22914;&#20309;&#37325;&#26032;&#21021;&#22987;&#21270;&#20196;&#29260;&#23884;&#20837;&#30697;&#38453;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#25216;&#26415;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#23454;&#39564;&#27604;&#36739;&#65292;&#27492;&#22806;&#36824;&#21152;&#20837;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;&#28966;&#28857;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#65306;1&#65289;&#22312;&#21333;&#35821;&#36716;&#31227;&#25991;&#29486;&#20013;&#30340;&#23884;&#20837;&#26367;&#25442;&#25216;&#26415;&#19981;&#36866;&#29992;&#20110;&#36866;&#24212;&#22810;&#35821;&#35328;&#27169;&#22411;&#12290;2&#65289;&#29992;&#36739;&#23567;&#30340;&#19987;&#38376;&#30340;&#35789;&#27719;&#26367;&#25442;&#36328;&#35821;&#35328;&#35789;&#27719;&#25552;&#20379;&#20102;&#19968;&#31181;&#25552;&#39640;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#24615;&#33021;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained multilingual language models underpin a large portion of modern NLP tools outside of English. A strong baseline for specializing these models for specific languages is Language-Adaptive Pre-Training (LAPT). However, retaining a large cross-lingual vocabulary and embedding matrix comes at considerable excess computational cost during adaptation. In this study, we propose several simple techniques to replace a cross-lingual vocabulary with a compact, language-specific one. Namely, we address strategies for re-initializing the token embedding matrix after vocabulary specialization. We then provide a systematic experimental comparison of our techniques, in addition to the recently-proposed Focus method. We demonstrate that: 1) Embedding-replacement techniques in the monolingual transfer literature are inadequate for adapting multilingual models. 2) Replacing cross-lingual vocabularies with smaller specialized ones provides an efficient method to improve performance in low-resou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#22235;&#38454;&#27573;&#26694;&#26550;&#65292;&#21487;&#20197;&#30452;&#25509;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#20869;&#23481;&#20013;&#30340;&#21051;&#26495;&#21360;&#35937;&#21644;&#20559;&#35265;&#65292;&#21253;&#25324;&#30452;&#25509;&#35810;&#38382;&#27979;&#35797;&#12289;&#20018;&#34892;&#25110;&#36866;&#24212;&#24615;&#25925;&#20107;&#27979;&#35797;&#12289;&#38544;&#24615;&#20851;&#32852;&#27979;&#35797;&#21644;&#26410;&#30693;&#24773;&#22659;&#27979;&#35797;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#22810;&#32500;&#35780;&#20272;&#25351;&#26631;&#21644;&#21487;&#35299;&#37322;&#30340;&#38646;&#26679;&#26412;&#25552;&#31034;&#65292;&#24182;&#20197;&#25945;&#32946;&#37096;&#38376;&#20026;&#26696;&#20363;&#30740;&#31350;&#26500;&#24314;&#20102;Edu-FairMonitor&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2308.10397</link><description>&lt;p&gt;
FairMonitor: &#19968;&#31181;&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21051;&#26495;&#21360;&#35937;&#21644;&#20559;&#35265;&#30340;&#22235;&#38454;&#27573;&#33258;&#21160;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FairMonitor: A Four-Stage Automatic Framework for Detecting Stereotypes and Biases in Large Language Models. (arXiv:2308.10397v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#22235;&#38454;&#27573;&#26694;&#26550;&#65292;&#21487;&#20197;&#30452;&#25509;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#20869;&#23481;&#20013;&#30340;&#21051;&#26495;&#21360;&#35937;&#21644;&#20559;&#35265;&#65292;&#21253;&#25324;&#30452;&#25509;&#35810;&#38382;&#27979;&#35797;&#12289;&#20018;&#34892;&#25110;&#36866;&#24212;&#24615;&#25925;&#20107;&#27979;&#35797;&#12289;&#38544;&#24615;&#20851;&#32852;&#27979;&#35797;&#21644;&#26410;&#30693;&#24773;&#22659;&#27979;&#35797;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#22810;&#32500;&#35780;&#20272;&#25351;&#26631;&#21644;&#21487;&#35299;&#37322;&#30340;&#38646;&#26679;&#26412;&#25552;&#31034;&#65292;&#24182;&#20197;&#25945;&#32946;&#37096;&#38376;&#20026;&#26696;&#20363;&#30740;&#31350;&#26500;&#24314;&#20102;Edu-FairMonitor&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#21051;&#26495;&#21360;&#35937;&#21644;&#20559;&#35265;&#21487;&#20197;&#22686;&#24378;&#20844;&#24179;&#24615;&#24182;&#20943;&#23569;&#36825;&#20123;LLMs&#24212;&#29992;&#26102;&#23545;&#20010;&#20154;&#25110;&#32676;&#20307;&#30340;&#19981;&#21033;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#22823;&#22810;&#20851;&#27880;&#20110;&#27979;&#37327;&#27169;&#22411;&#23545;&#21253;&#21547;&#20559;&#35265;&#21644;&#21051;&#26495;&#21360;&#35937;&#30340;&#21477;&#23376;&#30340;&#20559;&#22909;&#65292;&#36825;&#31181;&#26041;&#27861;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#19988;&#26080;&#27861;&#26816;&#27979;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#38544;&#21547;&#20559;&#35265;&#21644;&#21051;&#26495;&#21360;&#35937;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#22235;&#38454;&#27573;&#26694;&#26550;&#65292;&#30452;&#25509;&#35780;&#20272;LLMs&#29983;&#25104;&#20869;&#23481;&#20013;&#30340;&#21051;&#26495;&#21360;&#35937;&#21644;&#20559;&#35265;&#65292;&#21253;&#25324;&#30452;&#25509;&#35810;&#38382;&#27979;&#35797;&#12289;&#20018;&#34892;&#25110;&#36866;&#24212;&#24615;&#25925;&#20107;&#27979;&#35797;&#12289;&#38544;&#24615;&#20851;&#32852;&#27979;&#35797;&#21644;&#26410;&#30693;&#24773;&#22659;&#27979;&#35797;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#22810;&#32500;&#35780;&#20272;&#25351;&#26631;&#21644;&#21487;&#35299;&#37322;&#30340;&#38646;&#26679;&#26412;&#25552;&#31034;&#65292;&#29992;&#20110;&#33258;&#21160;&#35780;&#20272;&#12290;&#20197;&#25945;&#32946;&#37096;&#38376;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#22522;&#20110;&#36825;&#20010;&#22235;&#38454;&#27573;&#26694;&#26550;&#26500;&#24314;&#20102;Edu-FairMonitor&#65292;&#35813;&#26694;&#26550;&#21253;&#25324;12,632&#20010;&#24320;&#25918;&#24335;&#38382;&#39064;&#65292;&#28085;&#30422;&#20061;&#20010;&#25935;&#24863;&#35758;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting stereotypes and biases in Large Language Models (LLMs) can enhance fairness and reduce adverse impacts on individuals or groups when these LLMs are applied. However, the majority of existing methods focus on measuring the model's preference towards sentences containing biases and stereotypes within datasets, which lacks interpretability and cannot detect implicit biases and stereotypes in the real world. To address this gap, this paper introduces a four-stage framework to directly evaluate stereotypes and biases in the generated content of LLMs, including direct inquiry testing, serial or adapted story testing, implicit association testing, and unknown situation testing. Additionally, the paper proposes multi-dimensional evaluation metrics and explainable zero-shot prompts for automated evaluation. Using the education sector as a case study, we constructed the Edu-FairMonitor based on the four-stage framework, which encompasses 12,632 open-ended questions covering nine sensit
&lt;/p&gt;</description></item><item><title>ParaFuzz&#26159;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#24615;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#26816;&#27979;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#27602;&#26679;&#26412;&#12290;&#35813;&#25216;&#26415;&#36890;&#36807;&#35266;&#23519;&#27169;&#22411;&#22312;&#37325;&#20889;&#36807;&#30340;&#24178;&#20928;&#26679;&#26412;&#21644;&#27745;&#26579;&#26679;&#26412;&#19978;&#30340;&#39044;&#27979;&#31283;&#23450;&#24615;&#65292;&#26469;&#21028;&#26029;&#26679;&#26412;&#26159;&#21542;&#34987;&#27745;&#26579;&#12290;</title><link>http://arxiv.org/abs/2308.02122</link><description>&lt;p&gt;
ParaFuzz&#65306;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#24615;&#30340;&#25216;&#26415;&#29992;&#20110;&#26816;&#27979;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#27602;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
ParaFuzz: An Interpretability-Driven Technique for Detecting Poisoned Samples in NLP. (arXiv:2308.02122v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02122
&lt;/p&gt;
&lt;p&gt;
ParaFuzz&#26159;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#24615;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#26816;&#27979;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#27602;&#26679;&#26412;&#12290;&#35813;&#25216;&#26415;&#36890;&#36807;&#35266;&#23519;&#27169;&#22411;&#22312;&#37325;&#20889;&#36807;&#30340;&#24178;&#20928;&#26679;&#26412;&#21644;&#27745;&#26579;&#26679;&#26412;&#19978;&#30340;&#39044;&#27979;&#31283;&#23450;&#24615;&#65292;&#26469;&#21028;&#26029;&#26679;&#26412;&#26159;&#21542;&#34987;&#27745;&#26579;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20621;&#38376;&#25915;&#20987;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#27169;&#22411;&#30340;&#37325;&#35201;&#23041;&#32961;&#65292;&#20854;&#20013;&#22312;&#36755;&#20837;&#20013;&#23384;&#22312;&#29305;&#23450;&#35302;&#21457;&#22120;&#21487;&#20197;&#23548;&#33268;&#34987;&#27745;&#26579;&#30340;&#27169;&#22411;&#23558;&#36825;&#20123;&#36755;&#20837;&#35823;&#20998;&#31867;&#20026;&#39044;&#23450;&#30340;&#30446;&#26631;&#31867;&#21035;&#12290;&#24403;&#21069;&#30340;&#26816;&#27979;&#26426;&#21046;&#21463;&#21040;&#38480;&#21046;&#65292;&#26080;&#27861;&#24212;&#23545;&#26356;&#38544;&#34109;&#30340;&#20621;&#38376;&#31574;&#30053;&#65292;&#22914;&#22522;&#20110;&#39118;&#26684;&#30340;&#25915;&#20987;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#27979;&#35797;&#26102;&#27745;&#26579;&#26679;&#26412;&#26816;&#27979;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20381;&#36182;&#20110;&#27169;&#22411;&#39044;&#27979;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#19982;&#36755;&#20837;&#30340;&#35821;&#20041;&#21547;&#20041;&#26377;&#20851;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#35302;&#21457;&#22120;&#65288;&#20363;&#22914;&#65292;&#19981;&#24120;&#35265;&#30340;&#21333;&#35789;&#65289;&#19981;&#24212;&#35813;&#20174;&#26681;&#26412;&#19978;&#25913;&#21464;&#34987;&#27745;&#26579;&#26679;&#26412;&#30340;&#22522;&#26412;&#35821;&#20041;&#21547;&#20041;&#65292;&#22240;&#20026;&#23427;&#20204;&#24819;&#20445;&#25345;&#28508;&#20239;&#12290;&#22522;&#20110;&#36825;&#20010;&#35266;&#23519;&#65292;&#25105;&#20204;&#20551;&#35774;&#22312;&#25913;&#20889;&#36807;&#31243;&#20013;&#65292;&#27169;&#22411;&#23545;&#20110;&#37325;&#20889;&#36807;&#30340;&#24178;&#20928;&#26679;&#26412;&#30340;&#39044;&#27979;&#24212;&#35813;&#20445;&#25345;&#31283;&#23450;&#65292;&#32780;&#23545;&#20110;&#27745;&#26579;&#26679;&#26412;&#30340;&#39044;&#27979;&#22312;&#35302;&#21457;&#22120;&#30340;&#31361;&#21464;&#36807;&#31243;&#20013;&#24212;&#35813;&#24674;&#22797;&#21040;&#30495;&#23454;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
Backdoor attacks have emerged as a prominent threat to natural language processing (NLP) models, where the presence of specific triggers in the input can lead poisoned models to misclassify these inputs to predetermined target classes. Current detection mechanisms are limited by their inability to address more covert backdoor strategies, such as style-based attacks. In this work, we propose an innovative test-time poisoned sample detection framework that hinges on the interpretability of model predictions, grounded in the semantic meaning of inputs. We contend that triggers (e.g., infrequent words) are not supposed to fundamentally alter the underlying semantic meanings of poisoned samples as they want to stay stealthy. Based on this observation, we hypothesize that while the model's predictions for paraphrased clean samples should remain stable, predictions for poisoned samples should revert to their true labels upon the mutations applied to triggers during the paraphrasing process. W
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Android in the Wild (AITW)&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#35774;&#22791;&#25511;&#21046;&#31995;&#32479;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#20154;&#31867;&#31034;&#33539;&#30340;&#35774;&#22791;&#20132;&#20114;&#12289;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#21644;&#22810;&#31181;Android&#29256;&#26412;&#21644;&#35774;&#22791;&#31867;&#22411;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#20174;&#35270;&#35273;&#22806;&#35266;&#20013;&#25512;&#26029;&#29992;&#25143;&#30028;&#38754;&#20013;&#21487;&#29992;&#30340;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2307.10088</link><description>&lt;p&gt;
&#22312;&#37326;&#22806;&#30340;Android&#65306;&#29992;&#20110;Android&#35774;&#22791;&#25511;&#21046;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Android in the Wild: A Large-Scale Dataset for Android Device Control. (arXiv:2307.10088v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10088
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Android in the Wild (AITW)&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#35774;&#22791;&#25511;&#21046;&#31995;&#32479;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#20154;&#31867;&#31034;&#33539;&#30340;&#35774;&#22791;&#20132;&#20114;&#12289;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#21644;&#22810;&#31181;Android&#29256;&#26412;&#21644;&#35774;&#22791;&#31867;&#22411;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#20174;&#35270;&#35273;&#22806;&#35266;&#20013;&#25512;&#26029;&#29992;&#25143;&#30028;&#38754;&#20013;&#21487;&#29992;&#30340;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#33021;&#22815;&#35299;&#37322;&#20154;&#31867;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#24182;&#30452;&#25509;&#25511;&#21046;&#25968;&#23383;&#35774;&#22791;&#29992;&#25143;&#30028;&#38754;&#25191;&#34892;&#30340;&#35774;&#22791;&#25511;&#21046;&#31995;&#32479;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35774;&#22791;&#25511;&#21046;&#30740;&#31350;&#30340;&#25968;&#25454;&#38598;&#65292;Android in the Wild (AITW)&#65292;&#35813;&#25968;&#25454;&#38598;&#27604;&#24403;&#21069;&#25968;&#25454;&#38598;&#22823;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#35774;&#22791;&#20132;&#20114;&#30340;&#20154;&#31867;&#31034;&#33539;&#65292;&#21253;&#25324;&#23631;&#24149;&#21644;&#25805;&#20316;&#65292;&#20197;&#21450;&#30456;&#24212;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#12290;&#23427;&#21253;&#25324;715k&#20010;&#21095;&#38598;&#65292;&#28085;&#30422;30k&#20010;&#19981;&#21516;&#30340;&#25351;&#20196;&#65292;&#22235;&#20010;Android&#29256;&#26412;&#65288;v10-13&#65289;&#65292;&#20197;&#21450;&#20843;&#31181;&#19981;&#21516;&#30340;&#35774;&#22791;&#31867;&#22411;&#65288;&#20174;Pixel 2 XL&#21040;Pixel 6&#65289;&#21644;&#19981;&#21516;&#30340;&#23631;&#24149;&#20998;&#36776;&#29575;&#12290;&#23427;&#21253;&#21547;&#38656;&#35201;&#35821;&#35328;&#21644;&#35270;&#35273;&#19978;&#19979;&#25991;&#30340;&#35821;&#20041;&#29702;&#35299;&#30340;&#22810;&#27493;&#39588;&#20219;&#21153;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#65306;&#24517;&#39035;&#20174;&#23427;&#20204;&#30340;&#35270;&#35273;&#22806;&#35266;&#20013;&#25512;&#26029;&#20986;&#29992;&#25143;&#30028;&#38754;&#20013;&#21487;&#29992;&#30340;&#25805;&#20316;&#12290;&#32780;&#19988;&#65292;&#34892;&#21160;&#31354;&#38388;&#19981;&#20877;&#26159;&#31616;&#21333;&#30340;&#22522;&#20110;&#29992;&#25143;&#30028;&#38754;&#20803;&#32032;&#30340;&#34892;&#21160;&#65292;&#32780;&#26159;&#21253;&#21547;&#31934;&#30830;&#30340;&#25163;&#21183;&#65288;&#20363;&#22914;&#65292;&#27700;&#24179;&#28378;&#21160;&#65289;
&lt;/p&gt;
&lt;p&gt;
There is a growing interest in device-control systems that can interpret human natural language instructions and execute them on a digital device by directly controlling its user interface. We present a dataset for device-control research, Android in the Wild (AITW), which is orders of magnitude larger than current datasets. The dataset contains human demonstrations of device interactions, including the screens and actions, and corresponding natural language instructions. It consists of 715k episodes spanning 30k unique instructions, four versions of Android (v10-13),and eight device types (Pixel 2 XL to Pixel 6) with varying screen resolutions. It contains multi-step tasks that require semantic understanding of language and visual context. This dataset poses a new challenge: actions available through the user interface must be inferred from their visual appearance. And, instead of simple UI element-based actions, the action space consists of precise gestures (e.g., horizontal scrolls 
&lt;/p&gt;</description></item><item><title>HYTREL&#26159;&#19968;&#31181;&#34920;&#26684;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#36229;&#22270;&#26469;&#25429;&#25417;&#34920;&#26684;&#25968;&#25454;&#30340;&#25490;&#21015;&#19981;&#21464;&#24615;&#21644;&#20854;&#20182;&#19977;&#20010;&#32467;&#26500;&#23646;&#24615;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;HYTREL&#22312;&#22235;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#22987;&#32456;&#20248;&#20110;&#20854;&#20182;&#31454;&#20105;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#19988;&#21482;&#38656;&#26368;&#23569;&#30340;&#39044;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2307.08623</link><description>&lt;p&gt;
HYTREL: &#22522;&#20110;&#36229;&#22270;&#30340;&#34920;&#26684;&#25968;&#25454;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
HYTREL: Hypergraph-enhanced Tabular Data Representation Learning. (arXiv:2307.08623v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08623
&lt;/p&gt;
&lt;p&gt;
HYTREL&#26159;&#19968;&#31181;&#34920;&#26684;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#36229;&#22270;&#26469;&#25429;&#25417;&#34920;&#26684;&#25968;&#25454;&#30340;&#25490;&#21015;&#19981;&#21464;&#24615;&#21644;&#20854;&#20182;&#19977;&#20010;&#32467;&#26500;&#23646;&#24615;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;HYTREL&#22312;&#22235;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#22987;&#32456;&#20248;&#20110;&#20854;&#20182;&#31454;&#20105;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#19988;&#21482;&#38656;&#26368;&#23569;&#30340;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#39044;&#35757;&#32451;&#22312;&#22823;&#37327;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#30340;&#35821;&#35328;&#27169;&#22411;&#37117;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#27169;&#22411;&#24182;&#27809;&#26377;&#32771;&#34385;&#21040;&#34920;&#26684;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#34892;/&#21015;&#25490;&#21015;&#19981;&#21464;&#24615;&#12289;&#20998;&#23618;&#32467;&#26500;&#31561;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HYTREL&#30340;&#34920;&#26684;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#36229;&#22270;&#26469;&#25429;&#25417;&#34920;&#26684;&#25968;&#25454;&#30340;&#25490;&#21015;&#19981;&#21464;&#24615;&#21644;&#20854;&#20182;&#19977;&#20010;&#32467;&#26500;&#23646;&#24615;&#8212;&#8212;&#20854;&#20013;&#65292;&#34920;&#26684;&#21333;&#20803;&#26684;&#26500;&#25104;&#33410;&#28857;&#65292;&#24182;&#19988;&#22312;&#27599;&#34892;&#12289;&#27599;&#21015;&#21644;&#25972;&#20010;&#34920;&#26684;&#20013;&#20849;&#21516;&#20986;&#29616;&#30340;&#21333;&#20803;&#26684;&#34987;&#29992;&#26469;&#24418;&#25104;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#36229;&#36793;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;HYTREL&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#23545;&#20110;&#34920;&#26684;&#25968;&#25454;&#26159;&#26368;&#22823;&#19981;&#21464;&#30340;&#65292;&#21363;&#65292;&#20004;&#20010;&#34920;&#26684;&#36890;&#36807;HYTREL&#33719;&#24471;&#30340;&#34920;&#31034;&#30456;&#21516;&#65292;&#24403;&#19988;&#20165;&#24403;&#36825;&#20004;&#20010;&#34920;&#26684;&#22312;&#25490;&#21015;&#19978;&#26159;&#30456;&#21516;&#30340;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;HYTREL&#22312;&#22235;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#22987;&#32456;&#20248;&#20110;&#20854;&#20182;&#31454;&#20105;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#19988;&#21482;&#38656;&#26368;&#23569;&#30340;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models pretrained on large collections of tabular data have demonstrated their effectiveness in several downstream tasks. However, many of these models do not take into account the row/column permutation invariances, hierarchical structure, etc. that exist in tabular data. To alleviate these limitations, we propose HYTREL, a tabular language model, that captures the permutation invariances and three more structural properties of tabular data by using hypergraphs - where the table cells make up the nodes and the cells occurring jointly together in each row, column, and the entire table are used to form three different types of hyperedges. We show that HYTREL is maximally invariant under certain conditions for tabular data, i.e., two tables obtain the same representations via HYTREL iff the two tables are identical up to permutations. Our empirical results demonstrate that HYTREL consistently outperforms other competitive baselines on four downstream tasks with minimal pretraini
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#20041;&#30693;&#35782;&#22270;&#33258;&#21160;&#21019;&#24314;&#25919;&#31574;&#36777;&#35770;&#26696;&#20363;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20105;&#35770;&#30340;&#35821;&#20041;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#38480;&#21046;&#26368;&#30701;&#36335;&#24452;&#36941;&#21382;&#65292;&#26377;&#25928;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#36777;&#35770;&#26696;&#20363;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#32654;&#22269;&#31454;&#36187;&#36777;&#35770;&#20013;&#65292;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#26174;&#33879;&#25913;&#36827;&#20102;&#24050;&#26377;&#25968;&#25454;&#38598;DebateSum&#65292;&#24182;&#36129;&#29486;&#20102;&#26032;&#30340;&#20363;&#23376;&#21644;&#26377;&#29992;&#30340;&#20803;&#25968;&#25454;&#12290;&#36890;&#36807;&#20351;&#29992;txtai&#35821;&#20041;&#25628;&#32034;&#21644;&#30693;&#35782;&#22270;&#24037;&#20855;&#38142;&#65292;&#21019;&#24314;&#21644;&#36129;&#29486;&#20102;9&#20010;&#35821;&#20041;&#30693;&#35782;&#22270;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#35780;&#20272;&#26041;&#27861;&#26469;&#30830;&#23450;&#21738;&#20010;&#30693;&#35782;&#22270;&#26356;&#36866;&#21512;&#25919;&#31574;&#36777;&#35770;&#26696;&#20363;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2307.04090</link><description>&lt;p&gt;
DebateKG: &#29992;&#35821;&#20041;&#30693;&#35782;&#22270;&#33258;&#21160;&#21019;&#24314;&#25919;&#31574;&#36777;&#35770;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
DebateKG: Automatic Policy Debate Case Creation with Semantic Knowledge Graphs. (arXiv:2307.04090v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04090
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#20041;&#30693;&#35782;&#22270;&#33258;&#21160;&#21019;&#24314;&#25919;&#31574;&#36777;&#35770;&#26696;&#20363;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20105;&#35770;&#30340;&#35821;&#20041;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#38480;&#21046;&#26368;&#30701;&#36335;&#24452;&#36941;&#21382;&#65292;&#26377;&#25928;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#36777;&#35770;&#26696;&#20363;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#32654;&#22269;&#31454;&#36187;&#36777;&#35770;&#20013;&#65292;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#26174;&#33879;&#25913;&#36827;&#20102;&#24050;&#26377;&#25968;&#25454;&#38598;DebateSum&#65292;&#24182;&#36129;&#29486;&#20102;&#26032;&#30340;&#20363;&#23376;&#21644;&#26377;&#29992;&#30340;&#20803;&#25968;&#25454;&#12290;&#36890;&#36807;&#20351;&#29992;txtai&#35821;&#20041;&#25628;&#32034;&#21644;&#30693;&#35782;&#22270;&#24037;&#20855;&#38142;&#65292;&#21019;&#24314;&#21644;&#36129;&#29486;&#20102;9&#20010;&#35821;&#20041;&#30693;&#35782;&#22270;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#35780;&#20272;&#26041;&#27861;&#26469;&#30830;&#23450;&#21738;&#20010;&#30693;&#35782;&#22270;&#26356;&#36866;&#21512;&#25919;&#31574;&#36777;&#35770;&#26696;&#20363;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30456;&#20851;&#24037;&#20316;&#34920;&#26126;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#22312;&#35299;&#20915;&#31454;&#36187;&#36777;&#35770;&#20013;&#30340;&#38382;&#39064;&#26041;&#38754;&#20855;&#26377;&#24212;&#29992;&#24615;&#12290;&#31454;&#36187;&#36777;&#35770;&#20013;&#26368;&#37325;&#35201;&#30340;&#20219;&#21153;&#20043;&#19968;&#26159;&#36777;&#25163;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;&#36777;&#35770;&#26696;&#20363;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#38480;&#21046;&#26368;&#30701;&#36335;&#24452;&#36941;&#21382;&#22312;&#20105;&#35770;&#30340;&#35821;&#20041;&#30693;&#35782;&#22270;&#19978;&#26500;&#24314;&#26377;&#25928;&#30340;&#36777;&#35770;&#26696;&#20363;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#21517;&#20026;DebateSum&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#30740;&#31350;&#20102;&#36825;&#31181;&#28508;&#21147;&#65292;&#35813;&#25968;&#25454;&#38598;&#38024;&#23545;&#30340;&#26159;&#19968;&#31181;&#21517;&#20026;&#25919;&#31574;&#36777;&#35770;&#30340;&#32654;&#22269;&#31454;&#36187;&#36777;&#35770;&#31867;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#21521;&#25968;&#25454;&#38598;&#20013;&#24341;&#20837;53180&#20010;&#26032;&#30340;&#20363;&#23376;&#65292;&#24182;&#20026;&#27599;&#20010;&#20363;&#23376;&#25552;&#20379;&#36827;&#19968;&#27493;&#26377;&#29992;&#30340;&#20803;&#25968;&#25454;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;DebateSum&#12290;&#25105;&#20204;&#21033;&#29992;txtai&#35821;&#20041;&#25628;&#32034;&#21644;&#30693;&#35782;&#22270;&#24037;&#20855;&#38142;&#22522;&#20110;&#36825;&#20010;&#25968;&#25454;&#38598;&#20135;&#29983;&#24182;&#36129;&#29486;&#20102;9&#20010;&#35821;&#20041;&#30693;&#35782;&#22270;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#30830;&#23450;&#22312;&#25919;&#31574;&#36777;&#35770;&#26696;&#20363;&#29983;&#25104;&#30340;&#32972;&#26223;&#19979;&#21738;&#20010;&#30693;&#35782;&#22270;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work within the Argument Mining community has shown the applicability of Natural Language Processing systems for solving problems found within competitive debate. One of the most important tasks within competitive debate is for debaters to create high quality debate cases. We show that effective debate cases can be constructed using constrained shortest path traversals on Argumentative Semantic Knowledge Graphs. We study this potential in the context of a type of American Competitive Debate, called Policy Debate, which already has a large scale dataset targeting it called DebateSum. We significantly improve upon DebateSum by introducing 53180 new examples, as well as further useful metadata for every example, to the dataset. We leverage the txtai semantic search and knowledge graph toolchain to produce and contribute 9 semantic knowledge graphs built on this dataset. We create a unique method for evaluating which knowledge graphs are better in the context of producing policy deb
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;Pareto Optimal&#33258;&#30417;&#30563;&#26694;&#26550;&#65292;&#21033;&#29992;&#21487;&#29992;&#30340;&#32534;&#31243;&#30417;&#30563;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#21709;&#24212;&#36827;&#34892;&#31995;&#32479;&#26657;&#20934;&#65292;&#36890;&#36807;&#20026;&#27599;&#20010;&#21709;&#24212;&#29983;&#25104;&#39118;&#38505;&#35780;&#20998;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#25163;&#21160;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2306.16564</link><description>&lt;p&gt;
&#36890;&#36807;Pareto Optimal&#33258;&#30417;&#30563;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#26657;&#20934;&#21644;&#38169;&#35823;&#20462;&#27491;
&lt;/p&gt;
&lt;p&gt;
Automatic Calibration and Error Correction for Large Language Models via Pareto Optimal Self-Supervision. (arXiv:2306.16564v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;Pareto Optimal&#33258;&#30417;&#30563;&#26694;&#26550;&#65292;&#21033;&#29992;&#21487;&#29992;&#30340;&#32534;&#31243;&#30417;&#30563;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#21709;&#24212;&#36827;&#34892;&#31995;&#32479;&#26657;&#20934;&#65292;&#36890;&#36807;&#20026;&#27599;&#20010;&#21709;&#24212;&#29983;&#25104;&#39118;&#38505;&#35780;&#20998;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#25163;&#21160;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#24050;&#32463;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#65292;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#24212;&#29992;&#39046;&#22495;&#65292;&#20294;&#26159;&#20934;&#30830;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#22686;&#38271;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#22312;&#29983;&#29289;&#21307;&#23398;&#31561;&#20851;&#38190;&#39046;&#22495;&#12290;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26657;&#20934;LLM&#21709;&#24212;&#30340;&#32622;&#20449;&#27700;&#24179;&#65292;&#23545;&#20110;&#33258;&#21160;&#26816;&#27979;&#38169;&#35823;&#24182;&#20419;&#36827;&#20154;&#26426;&#21327;&#20316;&#39564;&#35777;&#33267;&#20851;&#37325;&#35201;&#12290;&#19968;&#20010;&#37325;&#35201;&#30340;&#26657;&#20934;&#20449;&#21495;&#26469;&#28304;&#26159;&#19987;&#23478;&#25351;&#23450;&#30340;&#32534;&#31243;&#30417;&#30563;&#65292;&#36890;&#24120;&#20855;&#26377;&#36739;&#20302;&#30340;&#25104;&#26412;&#65292;&#20294;&#20063;&#26377;&#20854;&#33258;&#36523;&#30340;&#23616;&#38480;&#24615;&#65292;&#22914;&#22122;&#22768;&#21644;&#35206;&#30422;&#33539;&#22260;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;Pareto Optimal&#33258;&#30417;&#30563;&#26694;&#26550;&#65292;&#21487;&#20197;&#21033;&#29992;&#21487;&#29992;&#30340;&#32534;&#31243;&#30417;&#30563;&#26469;&#31995;&#32479;&#22320;&#26657;&#20934;LLM&#21709;&#24212;&#65292;&#36890;&#36807;&#20026;&#27599;&#20010;&#21709;&#24212;&#29983;&#25104;&#39118;&#38505;&#35780;&#20998;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#39069;&#22806;&#30340;&#25163;&#21160;&#24037;&#20316;&#12290;&#36825;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#35843;&#21644;&#27169;&#22411;&#26469;&#23454;&#29616;&#65292;&#23558;LLM&#36755;&#20986;&#19982;&#20854;&#20182;&#21487;&#29992;&#30340;&#30417;&#30563;&#26469;&#28304;&#30456;&#21327;&#35843;&#65292;&#23558;&#26356;&#19981;&#30830;&#23450;&#30340;&#21709;&#24212;&#20998;&#37197;&#26356;&#39640;&#30340;&#39118;&#38505;&#35780;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable capabilities out of box for a wide range of applications, yet accuracy still remains a major growth area, especially in mission-critical domains such as biomedicine. An effective method to calibrate the confidence level on LLM responses is essential to automatically detect errors and facilitate human-in-the-loop verification. An important source of calibration signals stems from expert-stipulated programmatic supervision, which is often available at low cost but has its own limitations such as noise and coverage. In this paper, we introduce a Pareto optimal self-supervision framework that can leverage available programmatic supervision to systematically calibrate LLM responses by producing a risk score for every response, without any additional manual efforts. This is accomplished by learning a harmonizer model to align LLM output with other available supervision sources, which would assign higher risk scores to more uncertain L
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#12289;&#24320;&#25918;&#39046;&#22495;&#30340;&#32654;&#22269;&#25163;&#35821;-&#33521;&#35821;&#24179;&#34892;&#35821;&#26009;&#24211;YouTube-ASL&#65292;&#21253;&#21547;&#20102;&#32422;1000&#23567;&#26102;&#30340;&#32654;&#22269;&#25163;&#35821;&#35270;&#39057;&#21644;&#36229;&#36807;2500&#20301;&#29420;&#29305;&#30340;&#31614;&#21517;&#32773;&#12290;&#30740;&#31350;&#32773;&#22312;&#27492;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#20102;&#25163;&#35821;&#21040;&#33521;&#35821;&#32763;&#35793;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;&#21478;&#19968;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#26368;&#20339;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.15162</link><description>&lt;p&gt;
YouTube-ASL:&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#12289;&#24320;&#25918;&#39046;&#22495;&#30340;&#32654;&#22269;&#25163;&#35821;-&#33521;&#35821;&#24179;&#34892;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
YouTube-ASL: A Large-Scale, Open-Domain American Sign Language-English Parallel Corpus. (arXiv:2306.15162v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15162
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#12289;&#24320;&#25918;&#39046;&#22495;&#30340;&#32654;&#22269;&#25163;&#35821;-&#33521;&#35821;&#24179;&#34892;&#35821;&#26009;&#24211;YouTube-ASL&#65292;&#21253;&#21547;&#20102;&#32422;1000&#23567;&#26102;&#30340;&#32654;&#22269;&#25163;&#35821;&#35270;&#39057;&#21644;&#36229;&#36807;2500&#20301;&#29420;&#29305;&#30340;&#31614;&#21517;&#32773;&#12290;&#30740;&#31350;&#32773;&#22312;&#27492;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#20102;&#25163;&#35821;&#21040;&#33521;&#35821;&#32763;&#35793;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;&#21478;&#19968;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#26368;&#20339;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#23545;&#20110;&#25163;&#35821;&#30340;&#29942;&#39048;&#22312;&#20110;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;YouTube-ASL&#65292;&#19968;&#20010;&#26469;&#33258;YouTube&#30340;&#21253;&#21547;&#32654;&#22269;&#25163;&#35821;&#65288;ASL&#65289;&#35270;&#39057;&#21644;&#33521;&#25991;&#23383;&#24149;&#30340;&#22823;&#35268;&#27169;&#12289;&#24320;&#25918;&#39046;&#22495;&#30340;&#35821;&#26009;&#24211;&#12290;YouTube-ASL&#25317;&#26377;&#32422;1000&#23567;&#26102;&#30340;&#35270;&#39057;&#21644;&#36229;&#36807;2500&#20010;&#29420;&#29305;&#30340;&#31614;&#21517;&#32773;&#65292;&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;ASL&#25968;&#25454;&#38598;&#30340;3&#20493;&#20043;&#22810;&#65292;&#24182;&#19988;&#25317;&#26377;10&#20493;&#20110;&#20808;&#21069;ASL&#25968;&#25454;&#38598;&#30340;&#29420;&#29305;&#31614;&#21517;&#32773;&#25968;&#37327;&#12290;&#25105;&#20204;&#22312;YouTube-ASL&#19978;&#35757;&#32451;&#20102;ASL&#21040;&#33521;&#35821;&#32763;&#35793;&#30340;&#22522;&#20934;&#27169;&#22411;&#65292;&#24182;&#22312;How2Sign&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#22312;&#36825;&#37324;&#25105;&#20204;&#23454;&#29616;&#20102;&#26032;&#30340;&#24494;&#35843;&#26368;&#20339;&#25928;&#26524;12.39 BLEU&#65292;&#24182;&#39318;&#27425;&#25253;&#36947;&#20102;&#38646;-shot&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning for sign languages is bottlenecked by data. In this paper, we present YouTube-ASL, a large-scale, open-domain corpus of American Sign Language (ASL) videos and accompanying English captions drawn from YouTube. With ~1000 hours of videos and &gt;2500 unique signers, YouTube-ASL is ~3x as large and has ~10x as many unique signers as the largest prior ASL dataset. We train baseline models for ASL to English translation on YouTube-ASL and evaluate them on How2Sign, where we achieve a new finetuned state of the art of 12.39 BLEU and, for the first time, report zero-shot results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22359;&#29366;&#24577;&#21464;&#25442;&#22120;&#65288;BST&#65289;&#30340;&#28151;&#21512;&#31070;&#32463;&#32593;&#32476;&#23618;&#65292;&#32467;&#21512;&#20102;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21644;&#22359;&#21464;&#25442;&#22120;&#65292;&#26088;&#22312;&#22312;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#20013;&#25552;&#39640;&#24615;&#33021;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#35821;&#35328;&#24314;&#27169;&#22256;&#24785;&#24230;&#19978;&#20248;&#20110;&#31867;&#20284;&#30340;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#38271;&#30340;&#24207;&#21015;&#12290;&#27492;&#22806;&#65292;&#22312;&#23618;&#32423;&#21035;&#19978;&#20855;&#26377;&#36229;&#36807;&#21313;&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2306.09539</link><description>&lt;p&gt;
&#22359;&#29366;&#24577;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Block-State Transformer. (arXiv:2306.09539v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09539
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22359;&#29366;&#24577;&#21464;&#25442;&#22120;&#65288;BST&#65289;&#30340;&#28151;&#21512;&#31070;&#32463;&#32593;&#32476;&#23618;&#65292;&#32467;&#21512;&#20102;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21644;&#22359;&#21464;&#25442;&#22120;&#65292;&#26088;&#22312;&#22312;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#20013;&#25552;&#39640;&#24615;&#33021;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#35821;&#35328;&#24314;&#27169;&#22256;&#24785;&#24230;&#19978;&#20248;&#20110;&#31867;&#20284;&#30340;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#38271;&#30340;&#24207;&#21015;&#12290;&#27492;&#22806;&#65292;&#22312;&#23618;&#32423;&#21035;&#19978;&#20855;&#26377;&#36229;&#36807;&#21313;&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;&#22312;&#38656;&#35201;&#24314;&#27169;&#38271;&#26399;&#20381;&#36182;&#24615;&#24182;&#19988;&#38656;&#35201;&#39640;&#25928;&#25193;&#23637;&#21040;&#38271;&#24207;&#21015;&#30340;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#20102;&#24778;&#20154;&#30340;&#25928;&#26524;&#12290;&#23613;&#31649;&#26368;&#21021;&#26159;&#20026;&#36830;&#32493;&#20449;&#21495;&#35774;&#35745;&#30340;&#65292;&#20294;SSM&#22312;&#35270;&#35273;&#21644;&#38899;&#39057;&#31561;&#35768;&#22810;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#65307;&#28982;&#32780;&#65292;&#22312;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#20013;&#65292;SSM&#20173;&#28982;&#33853;&#21518;&#20110;Transformers&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#22359;&#29366;&#24577;&#21464;&#25442;&#22120;&#65288;BST&#65289;&#30340;&#28151;&#21512;&#23618;&#65292;&#23427;&#22312;&#20869;&#37096;&#32452;&#21512;&#20102;&#19968;&#20010;&#29992;&#20110;&#38271;&#36317;&#31163;&#19978;&#19979;&#25991;&#21270;&#30340;SSM&#23376;&#23618;&#21644;&#19968;&#20010;&#29992;&#20110;&#30701;&#26399;&#24207;&#21015;&#34920;&#31034;&#30340;&#22359;&#21464;&#25442;&#22120;&#23376;&#23618;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#12289;&#23436;&#20840;&#21487;&#24182;&#34892;&#30340;&#38598;&#25104;SSM&#21644;&#22359;&#27880;&#24847;&#21147;&#30340;&#21464;&#20307;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#35821;&#35328;&#24314;&#27169;&#30340;&#22256;&#24785;&#24230;&#19978;&#20248;&#20110;&#31867;&#20284;&#30340;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#38271;&#30340;&#24207;&#21015;&#12290;&#27492;&#22806;&#65292;&#22359;&#29366;&#24577;&#21464;&#25442;&#22120;&#22312;&#23618;&#32423;&#21035;&#19978;&#20855;&#26377;&#36229;&#36807;&#21313;&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
State space models (SSMs) have shown impressive results on tasks that require modeling long-range dependencies and efficiently scale to long sequences owing to their subquadratic runtime complexity. Originally designed for continuous signals, SSMs have shown superior performance on a plethora of tasks, in vision and audio; however, SSMs still lag Transformer performance in Language Modeling tasks. In this work, we propose a hybrid layer named Block-State Transformer (BST), that internally combines an SSM sublayer for long-range contextualization, and a Block Transformer sublayer for short-term representation of sequences. We study three different, and completely parallelizable, variants that integrate SSMs and block-wise attention. We show that our model outperforms similar Transformer-based architectures on language modeling perplexity and generalizes to longer sequences. In addition, the Block-State Transformer demonstrates more than tenfold increase in speed at the layer level compa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;PLANNER&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#23558;&#33258;&#22238;&#24402;&#29983;&#25104;&#21644;&#28508;&#22312;&#35821;&#20041;&#25193;&#25955;&#30456;&#32467;&#21512;&#65292;&#20197;&#22312;&#29983;&#25104;&#25991;&#26412;&#26102;&#22312;&#27573;&#33853;&#32423;&#21035;&#23454;&#29616;&#20840;&#23616;&#25511;&#21046;&#65292;&#29983;&#25104;&#27969;&#30021;&#30340;&#25991;&#26412;&#12290;</title><link>http://arxiv.org/abs/2306.02531</link><description>&lt;p&gt;
PLANNER:&#36890;&#36807;&#28508;&#22312;&#35821;&#35328;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#22810;&#26679;&#21270;&#27573;&#33853;
&lt;/p&gt;
&lt;p&gt;
PLANNER: Generating Diversified Paragraph via Latent Language Diffusion Model. (arXiv:2306.02531v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02531
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;PLANNER&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#23558;&#33258;&#22238;&#24402;&#29983;&#25104;&#21644;&#28508;&#22312;&#35821;&#20041;&#25193;&#25955;&#30456;&#32467;&#21512;&#65292;&#20197;&#22312;&#29983;&#25104;&#25991;&#26412;&#26102;&#22312;&#27573;&#33853;&#32423;&#21035;&#23454;&#29616;&#20840;&#23616;&#25511;&#21046;&#65292;&#29983;&#25104;&#27969;&#30021;&#30340;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#26377;&#26102;&#20250;&#29983;&#25104;&#37325;&#22797;&#19988;&#36136;&#37327;&#20302;&#19979;&#30340;&#36755;&#20986;&#65292;&#22240;&#20026;&#22312;&#29983;&#25104;&#30340;&#27493;&#39588;&#20013;&#38169;&#35823;&#20250;&#32047;&#31215;&#12290;&#36825;&#20010;&#38382;&#39064;&#24120;&#24120;&#34987;&#24402;&#22240;&#20110;&#26333;&#20809;&#20559;&#24046;-&#27169;&#22411;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#20351;&#29992;&#26041;&#24335;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#27169;&#22411;&#21487;&#20197;&#22238;&#39038;&#21644;&#20462;&#27491;&#20854;&#36755;&#20986;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#35745;&#31639;&#19978;&#21487;&#33021;&#24456;&#26114;&#36149;&#65292;&#24182;&#19988;&#22312;&#25991;&#26412;&#21644;&#27573;&#33853;&#36739;&#38271;&#30340;&#24773;&#20917;&#19979;&#65292;&#19982;&#33258;&#22238;&#24402;&#27169;&#22411;&#30456;&#27604;&#65292;&#20808;&#21069;&#20851;&#20110;&#25991;&#26412;&#30340;&#30740;&#31350;&#21162;&#21147;&#24050;&#23548;&#33268;&#20135;&#29983;&#30340;&#27169;&#22411;&#20135;&#29983;&#19981;&#22826;&#27969;&#30021;&#30340;&#36755;&#20986;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PLANNER&#65292;&#19968;&#20010;&#23558;&#28508;&#22312;&#35821;&#20041;&#25193;&#25955;&#19982;&#33258;&#22238;&#24402;&#29983;&#25104;&#32467;&#21512;&#30340;&#27169;&#22411;&#65292;&#20197;&#22312;&#27573;&#33853;&#19978;&#36827;&#34892;&#20840;&#23616;&#25511;&#21046;&#26469;&#29983;&#25104;&#27969;&#30021;&#30340;&#25991;&#26412;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#23558;&#33258;&#22238;&#24402;&#30340;&#8220;&#35299;&#30721;&#8221;&#27169;&#22359;&#19982;&#20351;&#29992;&#28508;&#22312;&#25193;&#25955;&#20197;&#31895;&#31890;&#24230;&#26041;&#24335;&#29983;&#25104;&#35821;&#20041;&#27573;&#33853;&#23884;&#20837;&#30340;&#8220;&#35268;&#21010;&#8221;&#27169;&#22359;&#30456;&#32467;&#21512;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#35813;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Autoregressive models for text sometimes generate repetitive and low-quality output because errors accumulate during the steps of generation. This issue is often attributed to exposure bias - the difference between how a model is trained, and how it is used during inference. Denoising diffusion models provide an alternative approach in which a model can revisit and revise its output. However, they can be computationally expensive and prior efforts on text have led to models that produce less fluent output compared to autoregressive models, especially for longer text and paragraphs. In this paper, we propose PLANNER, a model that combines latent semantic diffusion with autoregressive generation, to generate fluent text while exercising global control over paragraphs. The model achieves this by combining an autoregressive "decoding" module with a "planning" module that uses latent diffusion to generate semantic paragraph embeddings in a coarse-to-fine manner. The proposed method is evalu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#29616;&#26377;&#27169;&#22411;&#21512;&#24182;&#25216;&#26415;&#23384;&#22312;&#30340;&#24178;&#25200;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#26174;&#30528;&#25552;&#39640;&#21512;&#24182;&#21518;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.01708</link><description>&lt;p&gt;
&#21512;&#24182;&#27169;&#22411;&#26102;&#22914;&#20309;&#35299;&#20915;&#24178;&#25200;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Resolving Interference When Merging Models. (arXiv:2306.01708v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#29616;&#26377;&#27169;&#22411;&#21512;&#24182;&#25216;&#26415;&#23384;&#22312;&#30340;&#24178;&#25200;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#26174;&#30528;&#25552;&#39640;&#21512;&#24182;&#21518;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36801;&#31227;&#23398;&#20064;&#21487;&#20197;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#36827;&#19968;&#27493;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20174;&#32780;&#33719;&#24471;&#26174;&#33879;&#30340;&#20248;&#21183;&#65292;&#21253;&#25324;&#25913;&#36827;&#19979;&#28216;&#24615;&#33021;&#65292;&#21152;&#24555;&#25910;&#25947;&#36895;&#24230;&#21644;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#24050;&#26377;&#30340;&#27169;&#22411;&#21512;&#24182;&#25216;&#26415;&#24448;&#24448;&#24573;&#35270;&#20102;&#19981;&#21516;&#27169;&#22411;&#21442;&#25968;&#20043;&#38388;&#30340;&#24178;&#25200;&#65292;&#23548;&#33268;&#21512;&#24182;&#22810;&#20010;&#27169;&#22411;&#26102;&#24615;&#33021;&#22823;&#24133;&#19979;&#38477;&#12290;&#26412;&#25991;&#35777;&#26126;&#65292;&#20808;&#21069;&#30340;&#21512;&#24182;&#25216;&#26415;&#30001;&#20110;&#20004;&#20010;&#20027;&#35201;&#24178;&#25200;&#26469;&#28304;&#32780;&#19981;&#24910;&#20002;&#22833;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#65306;(a)&#20887;&#20313;&#21442;&#25968;&#20540;&#24341;&#36215;&#30340;&#24178;&#25200;&#21644;(b)&#34920;&#31034;&#21516;&#19968;&#21442;&#25968;&#20540;&#30340;&#31526;&#21495;&#22312;&#19981;&#21516;&#27169;&#22411;&#20013;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning - i.e., further fine-tuning a pre-trained model on a downstream task - can confer significant advantages, including improved downstream performance, faster convergence, and better sample efficiency. These advantages have led to a proliferation of task-specific fine-tuned models, which typically can only perform a single task and do not benefit from one another. Recently, model merging techniques have emerged as a solution to combine multiple task-specific models into a single multitask model without performing additional training. However, existing merging methods often ignore the interference between parameters of different models, resulting in large performance drops when merging multiple models. In this paper, we demonstrate that prior merging techniques inadvertently lose valuable information due to two major sources of interference: (a) interference due to redundant parameter values and (b) disagreement on the sign of a given parameter's values across models. To 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#31070;&#32463;&#26041;&#27861;&#22238;&#31572;&#22522;&#20110;&#20107;&#20214;&#24615;&#30693;&#35782;&#22270;&#30340;&#22797;&#26434;&#36923;&#36753;&#26597;&#35810;&#65292;&#28385;&#36275;&#20256;&#32479;&#30340;&#19968;&#38454;&#36923;&#36753;&#32422;&#26463;&#20197;&#21450;&#26377;&#20851;&#20107;&#20214;&#24615;&#21457;&#29983;&#21644;&#39034;&#24207;&#30340;&#38544;&#21547;&#36923;&#36753;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2305.19068</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#38544;&#21547;&#36923;&#36753;&#32422;&#26463;&#30340;&#20107;&#20214;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#22797;&#26434;&#26597;&#35810;&#31572;&#26696;
&lt;/p&gt;
&lt;p&gt;
Complex Query Answering on Eventuality Knowledge Graph with Implicit Logical Constraints. (arXiv:2305.19068v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#31070;&#32463;&#26041;&#27861;&#22238;&#31572;&#22522;&#20110;&#20107;&#20214;&#24615;&#30693;&#35782;&#22270;&#30340;&#22797;&#26434;&#36923;&#36753;&#26597;&#35810;&#65292;&#28385;&#36275;&#20256;&#32479;&#30340;&#19968;&#38454;&#36923;&#36753;&#32422;&#26463;&#20197;&#21450;&#26377;&#20851;&#20107;&#20214;&#24615;&#21457;&#29983;&#21644;&#39034;&#24207;&#30340;&#38544;&#21547;&#36923;&#36753;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26597;&#35810;&#30693;&#35782;&#22270;&#65292;&#21487;&#20197;&#33258;&#28982;&#22320;&#21033;&#29992;&#25512;&#29702;&#21644;&#27867;&#21270;&#33021;&#21147;&#26469;&#23398;&#20064;&#25512;&#26029;&#26356;&#22909;&#30340;&#31572;&#26696;&#12290;&#20256;&#32479;&#30340;&#31070;&#32463;&#22797;&#26434;&#26597;&#35810;&#31572;&#26696;&#26041;&#27861;&#20027;&#35201;&#36866;&#29992;&#20110;&#20197;&#23454;&#20307;&#20026;&#20013;&#24515;&#30340;&#30693;&#35782;&#22270;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#25105;&#20204;&#36824;&#38656;&#35201;&#23545;&#20107;&#20214;&#12289;&#29366;&#24577;&#21644;&#27963;&#21160;&#65288;&#21363;&#20107;&#20214;&#24615;&#25110;&#24773;&#20917;&#65289;&#36827;&#34892;&#36923;&#36753;&#25512;&#29702;&#65292;&#20197;&#23558;&#23398;&#20064;&#31995;&#32479;&#20174;I&#22411;&#25512;&#36827;&#21040;II&#22411;&#65292;&#27491;&#22914;Yoshua Bengio&#25152;&#25552;&#20986;&#30340;&#12290;&#20174;&#20107;&#20214;&#24615;&#20026;&#20013;&#24515;&#30340;&#30693;&#35782;&#22270;&#65288;EVKG&#65289;&#20013;&#36827;&#34892;&#36923;&#36753;&#26597;&#35810;&#21487;&#20197;&#33258;&#28982;&#22320;&#25552;&#20379;&#23545;&#27492;&#31867;&#30452;&#35266;&#21644;&#36923;&#36753;&#25512;&#29702;&#30340;&#21442;&#32771;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#31070;&#32463;&#26041;&#27861;&#26469;&#22238;&#31572;&#22522;&#20110;EVKG&#30340;&#22797;&#26434;&#36923;&#36753;&#26597;&#35810;&#65292;&#36825;&#21487;&#20197;&#28385;&#36275;&#20256;&#32479;&#30340;&#19968;&#38454;&#36923;&#36753;&#32422;&#26463;&#65292;&#36824;&#21487;&#20197;&#28385;&#36275;&#26377;&#20851;&#20107;&#20214;&#24615;&#21457;&#29983;&#21644;&#39034;&#24207;&#30340;&#38544;&#21547;&#36923;&#36753;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
Querying knowledge graphs (KGs) using deep learning approaches can naturally leverage the reasoning and generalization ability to learn to infer better answers. Traditional neural complex query answering (CQA) approaches mostly work on entity-centric KGs. However, in the real world, we also need to make logical inferences about events, states, and activities (i.e., eventualities or situations) to push learning systems from System I to System II, as proposed by Yoshua Bengio. Querying logically from an EVentuality-centric KG (EVKG) can naturally provide references to such kind of intuitive and logical inference. Thus, in this paper, we propose a new framework to leverage neural methods to answer complex logical queries based on an EVKG, which can satisfy not only traditional first-order logic constraints but also implicit logical constraints over eventualities concerning their occurrences and orders. For instance, if we know that "Food is bad" happens before "PersonX adds soy sauce", th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#21487;&#35299;&#37322;/&#21487;&#29702;&#35299;&#30340;&#22270;&#20687;&#32534;&#31243;&#26694;&#26550;&#65292;&#29992;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#30340;&#29983;&#25104;&#21644;&#35780;&#20272;&#12290;&#39318;&#20808;&#65292;&#24341;&#20837;&#20102;VPGen&#65292;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#36880;&#27493;T2I&#29983;&#25104;&#26694;&#26550;&#65292;&#23558;T2I&#29983;&#25104;&#20998;&#35299;&#20026;&#19977;&#20010;&#27493;&#39588;&#65292;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#21069;&#20004;&#20010;&#27493;&#39588;&#65292;&#25552;&#20379;&#20102;&#27604;&#31471;&#21040;&#31471;&#27169;&#22411;&#26356;&#24378;&#30340;&#31354;&#38388;&#25511;&#21046;&#33021;&#21147;&#65292;&#24182;&#21033;&#29992;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19990;&#30028;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2305.15328</link><description>&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#19982;&#35780;&#20272;&#30340;&#21487;&#35270;&#21270;&#32534;&#31243;
&lt;/p&gt;
&lt;p&gt;
Visual Programming for Text-to-Image Generation and Evaluation. (arXiv:2305.15328v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15328
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#21487;&#35299;&#37322;/&#21487;&#29702;&#35299;&#30340;&#22270;&#20687;&#32534;&#31243;&#26694;&#26550;&#65292;&#29992;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#30340;&#29983;&#25104;&#21644;&#35780;&#20272;&#12290;&#39318;&#20808;&#65292;&#24341;&#20837;&#20102;VPGen&#65292;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#36880;&#27493;T2I&#29983;&#25104;&#26694;&#26550;&#65292;&#23558;T2I&#29983;&#25104;&#20998;&#35299;&#20026;&#19977;&#20010;&#27493;&#39588;&#65292;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#21069;&#20004;&#20010;&#27493;&#39588;&#65292;&#25552;&#20379;&#20102;&#27604;&#31471;&#21040;&#31471;&#27169;&#22411;&#26356;&#24378;&#30340;&#31354;&#38388;&#25511;&#21046;&#33021;&#21147;&#65292;&#24182;&#21033;&#29992;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19990;&#30028;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35768;&#22810;&#39046;&#22495;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#37319;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#35270;&#35273;&#27169;&#22359;&#30340;&#25511;&#21046;&#22120;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#24037;&#20316;&#38598;&#20013;&#22312;&#20026;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#35270;&#35273;&#29702;&#35299;&#33021;&#21147;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#21487;&#35299;&#37322;/&#21487;&#35299;&#37322;&#30340;&#22270;&#20687;&#32534;&#31243;&#26694;&#26550;&#65292;&#29992;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#30340;&#29983;&#25104;&#21644;&#35780;&#20272;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;VPGen&#65292;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#36880;&#27493;T2I&#29983;&#25104;&#26694;&#26550;&#65292;&#23558;T2I&#29983;&#25104;&#20998;&#35299;&#20026;&#19977;&#20010;&#27493;&#39588;&#65306;&#23545;&#35937;/&#35745;&#25968;&#29983;&#25104;&#12289;&#24067;&#23616;&#29983;&#25104;&#21644;&#22270;&#20687;&#29983;&#25104;&#12290;&#25105;&#20204;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#21069;&#20004;&#20010;&#27493;&#39588;&#65288;&#23545;&#35937;/&#35745;&#25968;&#29983;&#25104;&#21644;&#24067;&#23616;&#29983;&#25104;&#65289;&#65292;&#36890;&#36807;&#22312;&#25991;&#26412;&#24067;&#23616;&#23545;&#19978;&#24494;&#35843;&#23427;&#12290;&#25105;&#20204;&#30340;&#36880;&#27493;T2I&#29983;&#25104;&#26694;&#26550;&#25552;&#20379;&#20102;&#27604;&#31471;&#21040;&#31471;&#27169;&#22411;&#26356;&#24378;&#30340;&#31354;&#38388;&#25511;&#21046;&#33021;&#21147;&#65292;&#32780;&#31471;&#21040;&#31471;&#27169;&#22411;&#26159;&#36825;&#20010;&#20219;&#21153;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19990;&#30028;&#30693;&#35782;&#65292;&#20811;&#26381;&#20102;&#20197;&#21069;&#30340;&#24067;&#23616;&#24341;&#23548;T2I&#20316;&#21697;&#30340;&#23616;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large language models have demonstrated impressive performance in many domains, recent works have adopted language models (LMs) as controllers of visual modules for vision-and-language tasks. While existing work focuses on equipping LMs with visual understanding, we propose two novel interpretable/explainable visual programming frameworks for text-to-image (T2I) generation and evaluation. First, we introduce VPGen, an interpretable step-by-step T2I generation framework that decomposes T2I generation into three steps: object/count generation, layout generation, and image generation. We employ an LM to handle the first two steps (object/count generation and layout generation), by finetuning it on text-layout pairs. Our step-by-step T2I generation framework provides stronger spatial control than end-to-end models, the dominant approach for this task. Furthermore, we leverage the world knowledge of pretrained LMs, overcoming the limitation of previous layout-guided T2I works that can on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26696;&#20363;&#25512;&#29702;&#30340;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#26041;&#27861;&#65288;CBR-MRC&#65289;&#65292;&#36890;&#36807;&#20174;&#23384;&#20648;&#22120;&#20013;&#26816;&#32034;&#30456;&#20284;&#26696;&#20363;&#24182;&#36873;&#25321;&#26368;&#31867;&#20284;&#30340;&#19978;&#19979;&#25991;&#26469;&#39044;&#27979;&#31572;&#26696;&#65292;&#20197;&#36798;&#21040;&#39640;&#20934;&#30830;&#24615;&#12290;&#22312;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#21644;&#26032;&#38395;&#38382;&#31572;&#20013;&#65292;CBR-MRC&#30340;&#20934;&#30830;&#24615;&#36229;&#36807;&#22522;&#20934;&#65292;&#24182;&#19988;&#33021;&#22815;&#35782;&#21035;&#19982;&#20854;&#20182;&#35780;&#20272;&#21592;&#19981;&#21516;&#30340;&#31572;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.14815</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#26696;&#20363;&#25512;&#29702;&#30340;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Machine Reading Comprehension using Case-based Reasoning. (arXiv:2305.14815v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26696;&#20363;&#25512;&#29702;&#30340;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#26041;&#27861;&#65288;CBR-MRC&#65289;&#65292;&#36890;&#36807;&#20174;&#23384;&#20648;&#22120;&#20013;&#26816;&#32034;&#30456;&#20284;&#26696;&#20363;&#24182;&#36873;&#25321;&#26368;&#31867;&#20284;&#30340;&#19978;&#19979;&#25991;&#26469;&#39044;&#27979;&#31572;&#26696;&#65292;&#20197;&#36798;&#21040;&#39640;&#20934;&#30830;&#24615;&#12290;&#22312;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#21644;&#26032;&#38395;&#38382;&#31572;&#20013;&#65292;CBR-MRC&#30340;&#20934;&#30830;&#24615;&#36229;&#36807;&#22522;&#20934;&#65292;&#24182;&#19988;&#33021;&#22815;&#35782;&#21035;&#19982;&#20854;&#20182;&#35780;&#20272;&#21592;&#19981;&#21516;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20934;&#30830;&#19988;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#20013;&#30340;&#31572;&#26696;&#25552;&#21462;&#65292;&#35813;&#26041;&#27861;&#31867;&#20284;&#20110;&#32463;&#20856;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#22522;&#20110;&#26696;&#20363;&#25512;&#29702;&#65288;CBR&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65288;CBR-MRC&#65289;&#22522;&#20110;&#19968;&#20010;&#20551;&#35774;&#65292;&#21363;&#30456;&#20284;&#38382;&#39064;&#30340;&#19978;&#19979;&#25991;&#21270;&#31572;&#26696;&#24444;&#27492;&#20043;&#38388;&#20855;&#26377;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290;&#32473;&#23450;&#19968;&#20010;&#27979;&#35797;&#38382;&#39064;&#65292;CBR-MRC&#39318;&#20808;&#20174;&#38750;&#21442;&#25968;&#21270;&#23384;&#20648;&#22120;&#20013;&#26816;&#32034;&#19968;&#32452;&#30456;&#20284;&#30340;&#26696;&#20363;&#65292;&#28982;&#21518;&#36890;&#36807;&#36873;&#25321;&#27979;&#35797;&#19978;&#19979;&#25991;&#20013;&#26368;&#31867;&#20284;&#20110;&#26816;&#32034;&#21040;&#30340;&#26696;&#20363;&#20013;&#19978;&#19979;&#25991;&#21270;&#31572;&#26696;&#34920;&#31034;&#30340;&#33539;&#22260;&#26469;&#39044;&#27979;&#31572;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21322;&#21442;&#25968;&#21270;&#30340;&#29305;&#24615;&#20351;&#20854;&#33021;&#22815;&#23558;&#39044;&#27979;&#24402;&#22240;&#20110;&#29305;&#23450;&#30340;&#35777;&#25454;&#26696;&#20363;&#38598;&#65292;&#22240;&#27492;&#22312;&#26500;&#24314;&#21487;&#38752;&#19988;&#21487;&#35843;&#35797;&#30340;&#38382;&#31572;&#31995;&#32479;&#26102;&#26159;&#19968;&#20010;&#29702;&#24819;&#30340;&#36873;&#25321;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;CBR-MRC&#22312;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#65288;NaturalQuestions&#65289;&#21644;&#26032;&#38395;&#38382;&#31572;&#65288;NewsQA&#65289;&#19978;&#27604;&#22823;&#22411;&#35835;&#32773;&#27169;&#22411;&#25552;&#20379;&#20102;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#20248;&#20110;&#22522;&#20934;&#20998;&#21035;&#25552;&#21319;&#20102;11.5&#21644;8.4 EM&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;CBR-MRC&#22312;&#35782;&#21035;&#19982;&#20182;&#20154;&#35780;&#20272;&#21592;&#19981;&#21516;&#30340;&#31572;&#26696;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an accurate and interpretable method for answer extraction in machine reading comprehension that is reminiscent of case-based reasoning (CBR) from classical AI. Our method (CBR-MRC) builds upon the hypothesis that contextualized answers to similar questions share semantic similarities with each other. Given a test question, CBR-MRC first retrieves a set of similar cases from a non-parametric memory and then predicts an answer by selecting the span in the test context that is most similar to the contextualized representations of answers in the retrieved cases. The semi-parametric nature of our approach allows it to attribute a prediction to the specific set of evidence cases, making it a desirable choice for building reliable and debuggable QA systems. We show that CBR-MRC provides high accuracy comparable with large reader models and outperforms baselines by 11.5 and 8.4 EM on NaturalQuestions and NewsQA, respectively. Further, we demonstrate the ability of CBR-MRC in identi
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#24182;&#25552;&#20379;&#22240;&#26524;&#24178;&#39044;&#25216;&#26415;&#65292;&#20197;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23454;&#20307;&#20559;&#35265;&#65292;&#20174;&#32780;&#20943;&#23569;&#20559;&#35265;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#30041;&#30456;&#20284;&#23454;&#20307;&#30340;&#20849;&#21516;&#39044;&#27979;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2305.14695</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23454;&#20307;&#20559;&#35265;&#65306;&#19968;&#31181;&#22240;&#26524;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Causal View of Entity Bias in (Large) Language Models. (arXiv:2305.14695v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14695
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#24182;&#25552;&#20379;&#22240;&#26524;&#24178;&#39044;&#25216;&#26415;&#65292;&#20197;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23454;&#20307;&#20559;&#35265;&#65292;&#20174;&#32780;&#20943;&#23569;&#20559;&#35265;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#30041;&#30456;&#20284;&#23454;&#20307;&#30340;&#20849;&#21516;&#39044;&#27979;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#20559;&#35265;&#24191;&#27867;&#24433;&#21709;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23548;&#33268;&#23427;&#20204;&#36807;&#24230;&#20381;&#36182;&#65288;&#26377;&#20559;&#35265;&#30340;&#65289;&#21442;&#25968;&#21270;&#30693;&#35782;&#26469;&#36827;&#34892;&#19981;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#23613;&#31649;&#22240;&#26524;&#30456;&#20851;&#30340;&#26041;&#27861;&#24050;&#32463;&#26174;&#31034;&#20986;&#32531;&#35299;&#23454;&#20307;&#20559;&#35265;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#31934;&#30830;&#20272;&#35745;&#28508;&#22312;&#22240;&#26524;&#27169;&#22411;&#30340;&#21442;&#25968;&#20173;&#28982;&#24456;&#22256;&#38590;&#65292;&#40657;&#30418;&#23376;&#30340;&#35821;&#35328;&#27169;&#22411;&#26356;&#26080;&#27861;&#35843;&#25972;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#23450;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#65292;&#20854;&#21442;&#25968;&#27604;&#36739;&#23481;&#26131;&#20272;&#35745;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22240;&#26524;&#24178;&#39044;&#25216;&#26415;&#65292;&#20197;&#32531;&#35299;&#30333;&#30418;&#21644;&#40657;&#30418;&#35774;&#32622;&#20013;&#30340;&#23454;&#20307;&#20559;&#35265;&#12290;&#36825;&#31181;&#22240;&#26524;&#24178;&#39044;&#23558;&#21407;&#22987;&#23454;&#20307;&#19982;&#30456;&#37051;&#23454;&#20307;&#19968;&#36215;&#36827;&#34892;&#25200;&#21160;&#12290;&#36825;&#31181;&#24178;&#39044;&#20943;&#23569;&#20102;&#19982;&#21407;&#22987;&#23454;&#20307;&#30456;&#20851;&#30340;&#29305;&#23450;&#20559;&#21521;&#20449;&#24687;&#65292;&#21516;&#26102;&#20173;&#20445;&#30041;&#20102;&#26469;&#33258;&#31867;&#20284;&#23454;&#20307;&#30340;&#36275;&#22815;&#20849;&#21516;&#39044;&#27979;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity bias widely affects pretrained (large) language models, causing them to excessively rely on (biased) parametric knowledge to make unfaithful predictions. Although causality-inspired methods have shown great potential to mitigate entity bias, it is hard to precisely estimate the parameters of underlying causal models in practice. The rise of black-box LLMs also makes the situation even worse, because of their inaccessible parameters and uncalibrated logits. To address these problems, we propose a specific structured causal model (SCM) whose parameters are comparatively easier to estimate. Building upon this SCM, we propose causal intervention techniques to mitigate entity bias for both white-box and black-box settings. The proposed causal intervention perturbs the original entity with neighboring entities. This intervention reduces specific biasing information pertaining to the original entity while still preserving sufficient common predictive information from similar entities. 
&lt;/p&gt;</description></item><item><title>INSTRUCTSCORE&#26159;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#25991;&#26412;&#29983;&#25104;&#35780;&#20272;&#24230;&#37327;&#65292;&#36890;&#36807;&#21033;&#29992;&#26126;&#30830;&#30340;&#20154;&#31867;&#25351;&#20196;&#21644;GPT-4&#30340;&#38544;&#24335;&#30693;&#35782;&#65292;&#23427;&#33021;&#29983;&#25104;&#29983;&#25104;&#25991;&#26412;&#30340;&#20998;&#25968;&#21644;&#20154;&#31867;&#21487;&#35835;&#30340;&#35786;&#26029;&#25253;&#21578;&#65292;&#36798;&#21040;&#19982;&#26368;&#20808;&#36827;&#24230;&#37327;&#30456;&#24403;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2305.14282</link><description>&lt;p&gt;
INSTRUCTSCORE: &#21487;&#35299;&#37322;&#30340;&#25991;&#26412;&#29983;&#25104;&#35780;&#20272;&#19982;&#32454;&#31890;&#24230;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
INSTRUCTSCORE: Explainable Text Generation Evaluation with Finegrained Feedback. (arXiv:2305.14282v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14282
&lt;/p&gt;
&lt;p&gt;
INSTRUCTSCORE&#26159;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#25991;&#26412;&#29983;&#25104;&#35780;&#20272;&#24230;&#37327;&#65292;&#36890;&#36807;&#21033;&#29992;&#26126;&#30830;&#30340;&#20154;&#31867;&#25351;&#20196;&#21644;GPT-4&#30340;&#38544;&#24335;&#30693;&#35782;&#65292;&#23427;&#33021;&#29983;&#25104;&#29983;&#25104;&#25991;&#26412;&#30340;&#20998;&#25968;&#21644;&#20154;&#31867;&#21487;&#35835;&#30340;&#35786;&#26029;&#25253;&#21578;&#65292;&#36798;&#21040;&#19982;&#26368;&#20808;&#36827;&#24230;&#37327;&#30456;&#24403;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35780;&#20272;&#35821;&#35328;&#29983;&#25104;&#30340;&#36136;&#37327;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#26368;&#36817;&#23398;&#20064;&#24230;&#37327;&#34920;&#26174;&#31034;&#19982;&#20154;&#31867;&#21028;&#26029;&#39640;&#24230;&#30456;&#20851;&#65292;&#20294;&#36825;&#20123;&#24230;&#37327;&#26080;&#27861;&#35299;&#37322;&#20854;&#21028;&#26029;&#25110;&#23558;&#20998;&#25968;&#19982;&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#32570;&#38519;&#20851;&#32852;&#36215;&#26469;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;InstructScore&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#30340;&#21487;&#35299;&#37322;&#30340;&#35780;&#20272;&#24230;&#37327;&#12290;&#36890;&#36807;&#21033;&#29992;&#26126;&#30830;&#30340;&#20154;&#31867;&#25351;&#20196;&#21644;GPT-4&#30340;&#38544;&#24335;&#30693;&#35782;&#65292;&#25105;&#20204;&#22522;&#20110;LLaMA&#23545;&#25991;&#26412;&#35780;&#20272;&#24230;&#37327;&#36827;&#34892;&#24494;&#35843;&#65292;&#29983;&#25104;&#29983;&#25104;&#25991;&#26412;&#30340;&#20998;&#25968;&#21644;&#20154;&#31867;&#21487;&#35835;&#30340;&#35786;&#26029;&#25253;&#21578;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#29983;&#25104;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;InstructScore&#65292;&#21253;&#25324;&#32763;&#35793;&#12289;&#23383;&#24149;&#29983;&#25104;&#12289;&#25968;&#25454;&#21040;&#25991;&#26412;&#21644;&#24120;&#35782;&#29983;&#25104;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;7B&#27169;&#22411;&#36229;&#36807;&#20102;&#25152;&#26377;&#20854;&#20182;&#26080;&#30417;&#30563;&#24230;&#37327;&#65292;&#21253;&#25324;&#22522;&#20110;175B GPT-3&#21644;GPT-4&#30340;&#27169;&#22411;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#21363;&#20351;&#27809;&#26377;&#26469;&#33258;&#20154;&#24037;&#35780;&#32423;&#25968;&#25454;&#30340;&#30452;&#25509;&#30417;&#30563;&#65292;&#25105;&#20204;&#30340;InstructScore&#30340;&#24615;&#33021;&#27700;&#24179;&#20063;&#19982;COMET2&#31561;&#26368;&#20808;&#36827;&#30340;&#24230;&#37327;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatically evaluating the quality of language generation is critical. Although recent learned metrics show high correlation with human judgement, these metrics can not explain their verdict or associate the scores with defects in generated text. To address this limitation, we present InstructScore, an explainable evaluation metric for text generation. By harnessing both explicit human instruction and the implicit knowledge of GPT-4, we fine-tune a text evaluation metric based on LLaMA, producing both a score for generated text and a human readable diagnostic report. We evaluate InstructScore on a variety of generation tasks, including translation, captioning, data-to-text and commonsense generation. Experiments show that our 7B model surpasses all other unsupervised metrics, including those based on 175B GPT-3 and GPT-4. Surprisingly, our InstructScore, even without direct supervision from human-rated data, achieves performance levels on par with state-of-the-art metrics like COMET2
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#24605;&#32500;&#38142;&#24335;&#25552;&#31034;&#26469;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#21040;SQL&#35299;&#26512;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#36845;&#20195;&#25552;&#31034;&#27861;&#21487;&#33021;&#26159;&#19981;&#24517;&#35201;&#30340;&#65292;&#24182;&#19988;&#35814;&#32454;&#30340;&#25512;&#29702;&#27493;&#39588;&#23481;&#26131;&#20986;&#29616;&#38169;&#35823;&#20256;&#25773;&#38382;&#39064;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;CoT&#39118;&#26684;&#25552;&#31034;&#26041;&#27861;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25991;&#26412;&#21040;SQL&#35299;&#26512;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.14215</link><description>&lt;p&gt;
&#25506;&#32034;&#38754;&#21521;&#25991;&#26412;&#21040;SQL&#36716;&#25442;&#30340;&#24605;&#32500;&#38142;&#24335;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Exploring Chain-of-Thought Style Prompting for Text-to-SQL. (arXiv:2305.14215v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#24605;&#32500;&#38142;&#24335;&#25552;&#31034;&#26469;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#21040;SQL&#35299;&#26512;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#36845;&#20195;&#25552;&#31034;&#27861;&#21487;&#33021;&#26159;&#19981;&#24517;&#35201;&#30340;&#65292;&#24182;&#19988;&#35814;&#32454;&#30340;&#25512;&#29702;&#27493;&#39588;&#23481;&#26131;&#20986;&#29616;&#38169;&#35823;&#20256;&#25773;&#38382;&#39064;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;CoT&#39118;&#26684;&#25552;&#31034;&#26041;&#27861;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25991;&#26412;&#21040;SQL&#35299;&#26512;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#22240;&#20854;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#26480;&#20986;&#30340;&#23569;&#26679;&#26412;&#24615;&#33021;&#32780;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22312;&#25991;&#26412;&#21040;SQL&#35299;&#26512;&#26041;&#38754;&#65292;&#20854;&#24615;&#33021;&#20173;&#26377;&#24456;&#22823;&#25552;&#21319;&#31354;&#38388;&#12290;&#26412;&#25991;&#20551;&#35774;&#25913;&#36827;&#25991;&#26412;&#21040;SQL&#35299;&#26512;&#30340;LLM&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#26159;&#20854;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#24605;&#32500;&#38142;&#24335;&#25552;&#31034;&#26469;&#25552;&#21319;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#21253;&#25324;&#21407;&#22987;&#30340;&#24605;&#32500;&#38142;&#24335;&#25552;&#31034;&#65288;Wei&#31561;&#65292;2022b&#65289;&#21644;&#30001;&#23567;&#21040;&#22823;&#30340;&#25552;&#31034;&#65288;Zhou&#31561;&#65292;2023&#65289;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22914;Zhou&#31561;&#20154;&#65288;2023&#65289;&#25152;&#31034;&#30340;&#36845;&#20195;&#25552;&#31034;&#22312;&#25991;&#26412;&#21040;SQL&#35299;&#26512;&#20013;&#21487;&#33021;&#26159;&#19981;&#24517;&#35201;&#30340;&#65292;&#24182;&#19988;&#20351;&#29992;&#35814;&#32454;&#30340;&#25512;&#29702;&#27493;&#39588;&#24448;&#24448;&#20250;&#20135;&#29983;&#26356;&#22810;&#30340;&#38169;&#35823;&#20256;&#25773;&#38382;&#39064;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38754;&#21521;&#25991;&#26412;&#21040;SQL&#36716;&#25442;&#30340;CoT&#39118;&#26684;&#25552;&#31034;&#26041;&#27861;&#12290;&#23427;&#22312;Spider&#24320;&#21457;&#38598;&#21644;Spider Realist&#25968;&#25454;&#38598;&#19978;&#20998;&#21035;&#25552;&#39640;&#20102;5.2&#21644;6.5&#20010;&#32477;&#23545;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning with large language models (LLMs) has recently caught increasing attention due to its superior few-shot performance on various tasks. However, its performance on text-to-SQL parsing still has much room for improvement. In this paper, we hypothesize that a crucial aspect of LLMs to improve for text-to-SQL parsing is their multi-step reasoning ability. Thus, we systematically study how to enhance LLMs' reasoning ability through chain of thought (CoT) style prompting, including the original chain-of-thought prompting (Wei et al., 2022b) and least-to-most prompting (Zhou et al., 2023). Our experiments demonstrate that iterative prompting as in Zhou et al. (2023) may be unnecessary for text-to-SQL parsing, and using detailed reasoning steps tends to have more error propagation issues. Based on these findings, we propose a new CoT-style prompting method for text-to-SQL parsing. It brings 5.2 and 6.5 point absolute gains on the Spider development set and the Spider Realist
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20840;&#23616;&#32467;&#26500;&#30693;&#35782;&#30340;&#36830;&#32493;&#36845;&#20195;&#30340;&#26041;&#24335;&#21435;&#25429;&#33719;&#23454;&#20307;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#25552;&#39640;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#20013;&#20851;&#31995;&#25277;&#21462;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13850</link><description>&lt;p&gt;
&#32467;&#21512;&#20840;&#23616;&#32467;&#26500;&#30693;&#35782;&#30340;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#20851;&#31995;&#25277;&#21462;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Global Structure Knowledge-Guided Relation Extraction Method for Visually-Rich Document. (arXiv:2305.13850v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13850
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20840;&#23616;&#32467;&#26500;&#30693;&#35782;&#30340;&#36830;&#32493;&#36845;&#20195;&#30340;&#26041;&#24335;&#21435;&#25429;&#33719;&#23454;&#20307;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#25552;&#39640;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#20013;&#20851;&#31995;&#25277;&#21462;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#20851;&#31995;&#25552;&#21462;&#65288;VRE&#65289;&#26088;&#22312;&#20174;&#35270;&#35273;&#20016;&#23500;&#30340;&#25991;&#26723;&#20013;&#25552;&#21462;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#22522;&#20110;&#23454;&#20307;&#29305;&#24449;&#21333;&#29420;&#39044;&#27979;&#27599;&#23545;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20294;&#24573;&#30053;&#20102;&#20840;&#23616;&#32467;&#26500;&#20449;&#24687;&#65292;&#21363;&#23454;&#20307;&#23545;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#32570;&#20047;&#20840;&#23616;&#32467;&#26500;&#20449;&#24687;&#21487;&#33021;&#20351;&#27169;&#22411;&#38590;&#20197;&#23398;&#20064;&#38271;&#31243;&#20851;&#31995;&#65292;&#24182;&#23481;&#26131;&#20135;&#29983;&#20914;&#31361;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;GOSE&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20197;&#36845;&#20195;&#30340;&#26041;&#24335;&#25429;&#33719;&#23454;&#20307;&#23545;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#32473;&#23450;&#25991;&#26723;&#30340;&#25195;&#25551;&#22270;&#20687;&#65292;GOSE&#39318;&#20808;&#23545;&#23454;&#20307;&#23545;&#29983;&#25104;&#21021;&#27493;&#30340;&#20851;&#31995;&#39044;&#27979;&#12290;&#31532;&#20108;&#65292;&#22312;&#20808;&#21069;&#36845;&#20195;&#30340;&#39044;&#27979;&#32467;&#26524;&#22522;&#30784;&#19978;&#65292;GOSE&#21033;&#29992;&#20840;&#23616;&#32467;&#26500;&#30693;&#35782;&#36827;&#19968;&#27493;&#25972;&#21512;&#23454;&#20307;&#34920;&#31034;&#12290;&#36825;&#31181;&#8220;&#29983;&#25104;-&#25429;&#33719;-&#25972;&#21512;&#8221;&#27169;&#24335;&#34987;&#22810;&#27425;&#25191;&#34892;&#65292;&#20197;&#20415;&#23454;&#20307;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#33021;&#22815;&#34987;&#24456;&#22909;&#22320;&#25429;&#33719;&#21644;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual relation extraction (VRE) aims to extract relations between entities from visuallyrich documents. Existing methods usually predict relations for each entity pair independently based on entity features but ignore the global structure information, i.e., dependencies between entity pairs. The absence of global structure information may make the model struggle to learn long-range relations and easily predict conflicted results. To alleviate such limitations, we propose a GlObal Structure knowledgeguided relation Extraction (GOSE) framework, which captures dependencies between entity pairs in an iterative manner. Given a scanned image of the document, GOSE firstly generates preliminary relation predictions on entity pairs. Secondly, it mines global structure knowledge based on prediction results of the previous iteration and further incorporates global structure knowledge into entity representations. This "generate-capture-incorporate" schema is performed multiple times so that entit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#26029;&#26041;&#38754;&#30340;&#24615;&#33021;&#21644;&#19982;&#20154;&#31867;&#20998;&#27495;&#20998;&#24067;&#30340;&#23545;&#40784;&#24773;&#20917;&#12290;&#32467;&#26524;&#34920;&#26126;LLM&#30340;&#25512;&#26029;&#33021;&#21147;&#26377;&#38480;&#65292;&#26080;&#27861;&#25429;&#25417;&#21040;&#20154;&#31867;&#20998;&#27495;&#20998;&#24067;&#65292;&#24341;&#21457;&#20102;&#23545;&#20854;NLU&#21644;&#20195;&#34920;&#20154;&#31867;&#29992;&#25143;&#24615;&#36136;&#30340;&#25285;&#24551;&#12290;</title><link>http://arxiv.org/abs/2305.13788</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#20687;&#20154;&#31867;&#19968;&#26679;&#25512;&#29702;&#21644;&#20135;&#29983;&#20998;&#27495;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Infer and Disagree Like Humans?. (arXiv:2305.13788v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#26029;&#26041;&#38754;&#30340;&#24615;&#33021;&#21644;&#19982;&#20154;&#31867;&#20998;&#27495;&#20998;&#24067;&#30340;&#23545;&#40784;&#24773;&#20917;&#12290;&#32467;&#26524;&#34920;&#26126;LLM&#30340;&#25512;&#26029;&#33021;&#21147;&#26377;&#38480;&#65292;&#26080;&#27861;&#25429;&#25417;&#21040;&#20154;&#31867;&#20998;&#27495;&#20998;&#24067;&#65292;&#24341;&#21457;&#20102;&#23545;&#20854;NLU&#21644;&#20195;&#34920;&#20154;&#31867;&#29992;&#25143;&#24615;&#36136;&#30340;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#24191;&#27867;&#20219;&#21153;&#26041;&#38754;&#24050;&#32463;&#34920;&#29616;&#20986;&#38750;&#24120;&#22909;&#30340;&#25104;&#32489;&#12290;&#22312;&#29983;&#25104;&#25991;&#26412;&#26102;&#65292;&#20174;&#36825;&#20123;&#27169;&#22411;&#20013;&#37319;&#26679;&#26631;&#35760;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#31574;&#30053;&#12290;&#20294;&#26159;&#65292;LLM&#24456;&#38590;&#19982;&#20154;&#31867;&#30340;&#20998;&#27495;&#20998;&#24067;&#39640;&#24230;&#23545;&#40784;&#65292;&#29305;&#21035;&#26159;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#26029;&#26041;&#38754;&#12290;&#26412;&#25991;&#20351;&#29992; Monte Carlo Reconstruction&#65288;MCR&#65289;&#21644; Log Probability Reconstruction&#65288;LPR&#65289;&#20004;&#31181;&#19981;&#21516;&#30340;&#25216;&#26415;&#35780;&#20272;&#20102;LLM&#20998;&#24067;&#30340;&#24615;&#33021;&#21644;&#19982;&#20154;&#31867;&#30340;&#23545;&#40784;&#24773;&#20917;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;LLM&#22312;&#35299;&#20915;NLI&#20219;&#21153;&#26041;&#38754;&#33021;&#21147;&#26377;&#38480;&#65292;&#21516;&#26102;&#26080;&#27861;&#25429;&#25417;&#21040;&#20154;&#31867;&#30340;&#20998;&#27495;&#20998;&#24067;&#65292;&#36825;&#23545;&#20854;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#33021;&#21147;&#21644;&#20195;&#34920;&#20154;&#31867;&#29992;&#25143;&#30340;&#29305;&#24615;&#25552;&#20986;&#20102;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown stellar achievements in solving a broad range of tasks. When generating text, it is common to sample tokens from these models: whether LLMs closely align with the human disagreement distribution has not been well-studied, especially within the scope of Natural Language Inference (NLI). In this paper, we evaluate the performance and alignment of LLM distribution with humans using two different techniques: Monte Carlo Reconstruction (MCR) and Log Probability Reconstruction (LPR). As a result, we show LLMs exhibit limited ability in solving NLI tasks and simultaneously fail to capture human disagreement distribution, raising concerns about their natural language understanding (NLU) ability and their representativeness of human users.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21487;&#33021;&#35823;&#29992;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#25351;&#20986;LLM&#21487;&#20197;&#20316;&#20026;&#26377;&#25928;&#30340;&#35823;&#23548;&#24615;&#20449;&#24687;&#29983;&#25104;&#22120;&#65292;&#23548;&#33268;&#24320;&#25918;&#22495;&#38382;&#31572;&#65288;ODQA&#65289;&#31995;&#32479;&#24615;&#33021;&#26174;&#33879;&#38477;&#20302;&#65292;&#24182;&#23581;&#35797;&#25552;&#20986;&#19977;&#31181;&#38450;&#24481;&#31574;&#30053;&#65306;&#25552;&#31034;&#65292;&#35823;&#25253;&#26816;&#27979;&#21644;&#22823;&#22810;&#25968;&#25237;&#31080;&#12290;</title><link>http://arxiv.org/abs/2305.13661</link><description>&lt;p&gt;
&#35770;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38169;&#35823;&#20449;&#24687;&#27745;&#26579;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
On the Risk of Misinformation Pollution with Large Language Models. (arXiv:2305.13661v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13661
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21487;&#33021;&#35823;&#29992;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#25351;&#20986;LLM&#21487;&#20197;&#20316;&#20026;&#26377;&#25928;&#30340;&#35823;&#23548;&#24615;&#20449;&#24687;&#29983;&#25104;&#22120;&#65292;&#23548;&#33268;&#24320;&#25918;&#22495;&#38382;&#31572;&#65288;ODQA&#65289;&#31995;&#32479;&#24615;&#33021;&#26174;&#33879;&#38477;&#20302;&#65292;&#24182;&#23581;&#35797;&#25552;&#20986;&#19977;&#31181;&#38450;&#24481;&#31574;&#30053;&#65306;&#25552;&#31034;&#65292;&#35823;&#25253;&#26816;&#27979;&#21644;&#22823;&#22810;&#25968;&#25237;&#31080;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#35843;&#26597;&#20102;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#28508;&#22312;&#35823;&#29992;&#65292;&#25506;&#35752;&#20102;&#20854;&#29983;&#25104;&#21487;&#20449;&#24182;&#20855;&#26377;&#35823;&#23548;&#24615;&#30340;&#20449;&#24687;&#24182;&#23545;&#20449;&#24687;&#23494;&#38598;&#22411;&#24212;&#29992;&#31243;&#24207;&#65292;&#23588;&#20854;&#26159;&#24320;&#25918;&#22495;&#38382;&#31572;&#65288;ODQA&#65289;&#31995;&#32479;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#23041;&#32961;&#27169;&#22411;&#65292;&#24182;&#23545;&#26080;&#24847;&#21644;&#25925;&#24847;&#30340;&#28508;&#22312;&#35823;&#29992;&#22330;&#26223;&#36827;&#34892;&#27169;&#25311;&#65292;&#20197;&#35780;&#20272;LLM&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#20449;&#24687;&#19981;&#23454;&#30340;&#31243;&#24230;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;LLM&#21487;&#20197;&#20316;&#20026;&#26377;&#25928;&#30340;&#35823;&#23548;&#24615;&#20449;&#24687;&#29983;&#25104;&#22120;&#65292;&#23548;&#33268;ODQA&#31995;&#32479;&#24615;&#33021;&#26174;&#33879;&#38477;&#20302;&#12290;&#20026;&#20102;&#20943;&#36731;&#30001;LLM&#29983;&#25104;&#30340;&#38169;&#35823;&#20449;&#24687;&#24102;&#26469;&#30340;&#21361;&#23475;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19977;&#31181;&#38450;&#24481;&#31574;&#30053;&#65306;&#25552;&#31034;&#65292;&#35823;&#25253;&#26816;&#27979;&#21644;&#22823;&#22810;&#25968;&#25237;&#31080;&#12290;&#34429;&#28982;&#21021;&#27493;&#32467;&#26524;&#26174;&#31034;&#36825;&#20123;&#38450;&#24481;&#24615;&#31574;&#30053;&#26377;&#24076;&#26395;&#20135;&#29983;&#26126;&#26174;&#25928;&#26524;&#65292;&#20294;&#36824;&#38656;&#35201;&#20570;&#22823;&#37327;&#24037;&#20316;&#26469;&#24212;&#23545;&#38169;&#35823;&#20449;&#24687;&#27745;&#26579;&#30340;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#24378;&#35843;&#20102;&#38656;&#35201;&#36827;&#19968;&#27493;&#36827;&#34892;&#36328;&#23398;&#31185;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we comprehensively investigate the potential misuse of modern Large Language Models (LLMs) for generating credible-sounding misinformation and its subsequent impact on information-intensive applications, particularly Open-Domain Question Answering (ODQA) systems. We establish a threat model and simulate potential misuse scenarios, both unintentional and intentional, to assess the extent to which LLMs can be utilized to produce misinformation. Our study reveals that LLMs can act as effective misinformation generators, leading to a significant degradation in the performance of ODQA systems. To mitigate the harm caused by LLM-generated misinformation, we explore three defense strategies: prompting, misinformation detection, and majority voting. While initial results show promising trends for these defensive strategies, much more work needs to be done to address the challenge of misinformation pollution. Our work highlights the need for further research and interdisciplinary
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25509;&#35302;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21463;&#38480;&#28216;&#25103;&#24335;&#29615;&#22659;&#19979;&#65292;&#33021;&#21542;&#26377;&#24847;&#20041;&#22320;&#35780;&#20272;&#23427;&#20204;&#30340;&#33021;&#21147;&#12290;&#20316;&#20026;&#27010;&#24565;&#39564;&#35777;&#65292;&#30740;&#31350;&#20102;&#20116;&#31181;&#20132;&#20114;&#35774;&#32622;&#65292;&#34920;&#26126;&#24403;&#21069;&#30340;&#32842;&#22825;&#20248;&#21270;LLMs&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#33021;&#22815;&#36981;&#24490;&#28216;&#25103;&#29609;&#27861;&#25351;&#20196;&#12290;&#36825;&#23545;&#20110;&#23558;LLMs&#24320;&#21457;&#20026;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#30340;&#23545;&#35805;&#20195;&#29702;&#20154;&#20855;&#26377;&#21551;&#31034;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.13455</link><description>&lt;p&gt;
clembench&#65306;&#20351;&#29992;&#28216;&#25103;&#26469;&#35780;&#20272;&#20316;&#20026;&#23545;&#35805;&#20195;&#29702;&#20154;&#30340;&#32842;&#22825;&#20248;&#21270;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
clembench: Using Game Play to Evaluate Chat-Optimized Language Models as Conversational Agents. (arXiv:2305.13455v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25509;&#35302;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21463;&#38480;&#28216;&#25103;&#24335;&#29615;&#22659;&#19979;&#65292;&#33021;&#21542;&#26377;&#24847;&#20041;&#22320;&#35780;&#20272;&#23427;&#20204;&#30340;&#33021;&#21147;&#12290;&#20316;&#20026;&#27010;&#24565;&#39564;&#35777;&#65292;&#30740;&#31350;&#20102;&#20116;&#31181;&#20132;&#20114;&#35774;&#32622;&#65292;&#34920;&#26126;&#24403;&#21069;&#30340;&#32842;&#22825;&#20248;&#21270;LLMs&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#33021;&#22815;&#36981;&#24490;&#28216;&#25103;&#29609;&#27861;&#25351;&#20196;&#12290;&#36825;&#23545;&#20110;&#23558;LLMs&#24320;&#21457;&#20026;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#30340;&#23545;&#35805;&#20195;&#29702;&#20154;&#20855;&#26377;&#21551;&#31034;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#8220;&#31449;&#31435;&#35821;&#35328;&#29702;&#35299;&#20195;&#29702;&#8221;&#30340;&#31995;&#32479;&#35780;&#20272;&#26041;&#27861;&#8212;&#8212;&#20195;&#29702;&#22312;&#20016;&#23500;&#30340;&#35821;&#35328;&#21644;&#38750;&#35821;&#35328;&#29615;&#22659;&#20013;&#36816;&#34892;&#65292;&#36890;&#36807;&#22312;&#31934;&#24515;&#26500;&#36896;&#30340;&#20114;&#21160;&#29615;&#22659;&#20013;&#36827;&#34892;&#27979;&#35797;&#26469;&#35780;&#20272;&#23427;&#20204;&#12290;&#20854;&#20182;&#26368;&#36817;&#30340;&#24037;&#20316;&#21017;&#35748;&#20026;&#65292;&#22914;&#26524;&#36866;&#24403;&#35774;&#32622;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#34987;&#29702;&#35299;&#20026;&#36825;&#26679;&#30340;&#20195;&#29702;&#65288;&#30340;&#27169;&#25311;&#22120;&#65289;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#36825;&#31181;&#32852;&#31995;&#65306;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#35753;LLMs&#25509;&#35302;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21463;&#38480;&#28216;&#25103;&#24335;&#29615;&#22659;&#26469;&#26377;&#24847;&#20041;&#22320;&#35780;&#20272;&#23427;&#20204;&#30340;&#33021;&#21147;&#65311;&#20316;&#20026;&#27010;&#24565;&#39564;&#35777;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#20116;&#31181;&#20132;&#20114;&#35774;&#32622;&#65292;&#34920;&#26126;&#24403;&#21069;&#30340;&#32842;&#22825;&#20248;&#21270;LLMs&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#33021;&#22815;&#36981;&#24490;&#28216;&#25103;&#29609;&#27861;&#25351;&#20196;&#12290;&#36825;&#31181;&#33021;&#21147;&#21644;&#28216;&#25103;&#29609;&#27861;&#30340;&#36136;&#37327;&#65288;&#36890;&#36807;&#28385;&#36275;&#19981;&#21516;&#28216;&#25103;&#30446;&#26631;&#30340;&#24773;&#20917;&#26469;&#34913;&#37327;&#65289;&#37117;&#36981;&#24490;&#30528;&#21457;&#23637;&#24490;&#29615;&#65292;&#26032;&#22411;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;&#21363;&#20351;&#26159;&#30456;&#23545;&#31616;&#21333;&#30340;&#8220;&#20117;&#23383;&#28216;&#25103;&#8221;&#31034;&#20363;&#28216;&#25103;&#30340;&#25351;&#26631;&#20063;&#25552;&#20379;&#20102;&#27169;&#22411;&#22312;&#36825;&#20123;&#26465;&#20214;&#19979;&#30340;&#22522;&#26412;&#24615;&#33021;&#25351;&#31034;&#12290;&#36825;&#23545;&#20110;&#23558;LLMs&#24320;&#21457;&#20026;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#30340;&#23545;&#35805;&#20195;&#29702;&#20154;&#20855;&#26377;&#21551;&#31034;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has proposed a methodology for the systematic evaluation of "Situated Language Understanding Agents"-agents that operate in rich linguistic and non-linguistic contexts-through testing them in carefully constructed interactive settings. Other recent work has argued that Large Language Models (LLMs), if suitably set up, can be understood as (simulators of) such agents. A connection suggests itself, which this paper explores: Can LLMs be evaluated meaningfully by exposing them to constrained game-like settings that are built to challenge specific capabilities? As a proof of concept, this paper investigates five interaction settings, showing that current chat-optimised LLMs are, to an extent, capable to follow game-play instructions. Both this capability and the quality of the game play, measured by how well the objectives of the different games are met, follows the development cycle, with newer models performing better. The metrics even for the comparatively simple example gam
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MultiTurnCleanup&#20219;&#21153;&#65292;&#25910;&#38598;&#20102;&#26032;&#30340;&#25968;&#25454;&#38598;MultiTurnCleanup1&#65292;&#38024;&#23545;&#21475;&#35821;&#20250;&#35805;&#36716;&#24405;&#20013;&#30340;&#19981;&#36830;&#32493;&#29616;&#35937;&#36827;&#34892;&#25506;&#35752;&#24182;&#25552;&#20379;&#20102;&#20004;&#20010;&#21487;&#29992;&#20110;&#26410;&#26469;&#30740;&#31350;&#30340;&#22522;&#20934;&#27979;&#35797;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.12029</link><description>&lt;p&gt;
MultiTurnCleanup&#65306;&#29992;&#20110;&#22810;&#36718;&#21475;&#35821;&#20250;&#35805;&#36716;&#24405;&#28165;&#29702;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
MultiTurnCleanup: A Benchmark for Multi-Turn Spoken Conversational Transcript Cleanup. (arXiv:2305.12029v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12029
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MultiTurnCleanup&#20219;&#21153;&#65292;&#25910;&#38598;&#20102;&#26032;&#30340;&#25968;&#25454;&#38598;MultiTurnCleanup1&#65292;&#38024;&#23545;&#21475;&#35821;&#20250;&#35805;&#36716;&#24405;&#20013;&#30340;&#19981;&#36830;&#32493;&#29616;&#35937;&#36827;&#34892;&#25506;&#35752;&#24182;&#25552;&#20379;&#20102;&#20004;&#20010;&#21487;&#29992;&#20110;&#26410;&#26469;&#30740;&#31350;&#30340;&#22522;&#20934;&#27979;&#35797;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#35821;&#35843;&#19981;&#36830;&#32493;&#26816;&#27979;&#27169;&#22411;&#20391;&#37325;&#20110;&#21333;&#20010;&#35828;&#35805;&#32773;&#30340;&#27599;&#20010;&#35805;&#35821;&#12290;&#28982;&#32780;&#65292;&#21475;&#35821;&#20250;&#35805;&#36716;&#24405;&#20013;&#30340;&#35768;&#22810;&#19981;&#36830;&#32493;&#29616;&#35937;&#37117;&#21457;&#29983;&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#65292;&#36825;&#24433;&#21709;&#20102;&#20154;&#31867;&#30340;&#21487;&#35835;&#24615;&#21644;&#19979;&#28216; NLP &#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#21019;&#26032;&#30340;&#8220;MultiTurnCleanup&#8221;&#20219;&#21153;&#65292;&#38024;&#23545;&#21475;&#35821;&#20250;&#35805;&#36716;&#24405;&#20013;&#30340;&#19981;&#36830;&#32493;&#29616;&#35937;&#36827;&#34892;&#25506;&#35752;&#65292;&#24182;&#25910;&#38598;&#20102;&#26032;&#30340;&#25968;&#25454;&#38598;MultiTurnCleanup1&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#25968;&#25454;&#26631;&#27880;&#27169;&#24335;&#20197;&#25910;&#38598;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#25968;&#25454;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#20004;&#31181;&#24314;&#27169;&#26041;&#27861;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#65292;&#20316;&#20026;&#26410;&#26469;&#30740;&#31350;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current disfluency detection models focus on individual utterances each from a single speaker. However, numerous discontinuity phenomena in spoken conversational transcripts occur across multiple turns, hampering human readability and the performance of downstream NLP tasks. This study addresses these phenomena by proposing an innovative Multi-Turn Cleanup task for spoken conversational transcripts and collecting a new dataset, MultiTurnCleanup1. We design a data labeling schema to collect the high-quality dataset and provide extensive data analysis. Furthermore, we leverage two modeling approaches for experimental evaluation as benchmarks for future research.
&lt;/p&gt;</description></item><item><title>mLongT5&#26159;&#19968;&#31181;&#22810;&#35821;&#35328;&#39640;&#25928;&#25991;&#26412;-&#25991;&#26412;Transformer&#65292;&#36866;&#29992;&#20110;&#22788;&#29702;&#36739;&#38271;&#24207;&#21015;&#30340;&#36755;&#20837;&#12290;&#23427;&#22312;&#22810;&#35821;&#35328;&#25688;&#35201;&#21644;&#38382;&#31572;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.11129</link><description>&lt;p&gt;
mLongT5&#65306;&#19968;&#31181;&#36866;&#29992;&#20110;&#36739;&#38271;&#24207;&#21015;&#30340;&#22810;&#35821;&#35328;&#39640;&#25928;&#25991;&#26412;-&#25991;&#26412;Transformer
&lt;/p&gt;
&lt;p&gt;
mLongT5: A Multilingual and Efficient Text-To-Text Transformer for Longer Sequences. (arXiv:2305.11129v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11129
&lt;/p&gt;
&lt;p&gt;
mLongT5&#26159;&#19968;&#31181;&#22810;&#35821;&#35328;&#39640;&#25928;&#25991;&#26412;-&#25991;&#26412;Transformer&#65292;&#36866;&#29992;&#20110;&#22788;&#29702;&#36739;&#38271;&#24207;&#21015;&#30340;&#36755;&#20837;&#12290;&#23427;&#22312;&#22810;&#35821;&#35328;&#25688;&#35201;&#21644;&#38382;&#31572;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#25105;&#20204;&#24320;&#21457;&#30340;&#19968;&#31181;&#22810;&#35821;&#35328;&#39640;&#25928;&#30340;&#25991;&#26412;-&#25991;&#26412;Transformer&#65292;&#36866;&#29992;&#20110;&#22788;&#29702;&#38271;&#36755;&#20837;&#12290;&#36825;&#20010;&#27169;&#22411;&#34987;&#31216;&#20026;mLongT5&#65292;&#23427;&#22312;LongT5&#30340;&#26550;&#26500;&#22522;&#30784;&#19978;&#26500;&#24314;&#65292;&#21516;&#26102;&#21033;&#29992;&#20102;&#29992;&#20110;&#39044;&#35757;&#32451;mT5&#21644;UL2&#39044;&#35757;&#32451;&#20219;&#21153;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#22810;&#35821;&#35328;&#25688;&#35201;&#21644;&#38382;&#31572;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#35813;&#27169;&#22411;&#65292;&#24182;&#19988;&#32467;&#26524;&#26174;&#31034;&#19982;&#29616;&#26377;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#22914;mBART&#25110;M-BERT&#30456;&#27604;&#65292;mLongT5&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present our work on developing a multilingual, efficient text-to-text transformer that is suitable for handling long inputs. This model, called mLongT5, builds upon the architecture of LongT5, while leveraging the multilingual datasets used for pretraining mT5 and the pretraining tasks of UL2. We evaluate this model on a variety of multilingual summarization and question-answering tasks, and the results show stronger performance for mLongT5 when compared to existing multilingual models such as mBART or M-BERT.
&lt;/p&gt;</description></item><item><title>ConvXAI&#26159;&#19968;&#20010;&#22522;&#20110;&#23545;&#35805;&#30340;XAI&#31995;&#32479;&#65292;&#23427;&#38598;&#25104;&#20102;&#22810;&#31181;XAI&#31867;&#22411;&#65292;&#24182;&#23558;&#23454;&#38469;&#29992;&#25143;&#38656;&#27714;&#23884;&#20837;&#35774;&#35745;&#20013;&#65292;&#20197;&#25552;&#39640;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.09770</link><description>&lt;p&gt;
ConvXAI&#65306;&#36890;&#36807;&#23545;&#35805;&#25552;&#20379;&#24322;&#26500;&#30340;AI&#35299;&#37322;&#65292;&#25903;&#25345;&#20154;&#26426;&#31185;&#25216;&#20889;&#20316;
&lt;/p&gt;
&lt;p&gt;
ConvXAI: Delivering Heterogeneous AI Explanations via Conversations to Support Human-AI Scientific Writing. (arXiv:2305.09770v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09770
&lt;/p&gt;
&lt;p&gt;
ConvXAI&#26159;&#19968;&#20010;&#22522;&#20110;&#23545;&#35805;&#30340;XAI&#31995;&#32479;&#65292;&#23427;&#38598;&#25104;&#20102;&#22810;&#31181;XAI&#31867;&#22411;&#65292;&#24182;&#23558;&#23454;&#38469;&#29992;&#25143;&#38656;&#27714;&#23884;&#20837;&#35774;&#35745;&#20013;&#65292;&#20197;&#25552;&#39640;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#20154;&#24037;&#26234;&#33021;&#35299;&#37322;&#65288;XAI&#65289;&#26041;&#27861;&#26469;&#35299;&#37322;AI&#31995;&#32479;&#65292;&#20294;&#30446;&#21069;&#30340;&#26041;&#27861;&#26159;&#21542;&#23545;&#20154;&#31867;&#23454;&#29992;&#20173;&#23384;&#22312;&#19981;&#19968;&#33268;&#30340;&#21457;&#29616;&#12290;&#20026;&#20102;&#25913;&#21892;XAI&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#65292;&#19968;&#31995;&#21015;&#30740;&#31350;&#30830;&#23450;&#20102;&#29616;&#23454;&#19990;&#30028;&#20013;&#22810;&#26679;&#21270;&#21644;&#21160;&#24577;&#30340;&#29992;&#25143;&#38656;&#27714;&#19982;&#29616;&#26377;XAI&#26041;&#27861;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#34429;&#28982;&#20043;&#21069;&#30340;&#30740;&#31350;&#35774;&#24819;&#23558;&#22810;&#31181;XAI&#26041;&#27861;&#38598;&#25104;&#21040;&#36890;&#29992;XAI&#30028;&#38754;&#65288;&#20363;&#22914;&#65292;&#22522;&#20110;&#23545;&#35805;&#25110;GUI&#30340;XAI&#31995;&#32479;&#65289;&#20013;&#20197;&#20943;&#36731;&#36825;&#20123;&#24046;&#36317;&#65292;&#20294;&#32570;&#23569;&#38024;&#23545;&#36825;&#20123;&#31995;&#32479;&#22914;&#20309;&#35774;&#35745;&#20197;&#28385;&#36275;&#23454;&#38469;&#29992;&#25143;&#38656;&#27714;&#30340;&#30740;&#31350;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ConvXAI&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#23545;&#35805;&#30340;XAI&#31995;&#32479;&#65292;&#23427;&#32467;&#21512;&#20102;&#22810;&#31181;XAI&#31867;&#22411;&#65292;&#24182;&#36171;&#20104;&#29992;&#25143;&#36890;&#36807;&#36890;&#29992;&#30340;XAI&#23545;&#35805;&#30028;&#38754;&#25552;&#20986;&#21508;&#31181;XAI&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#21019;&#26032;&#22320;&#23558;&#23454;&#38469;&#29992;&#25143;&#38656;&#27714;&#65288;&#21363;&#65292;&#22522;&#20110;&#26684;&#24335;&#30740;&#31350;&#30340;&#22235;&#20010;&#21407;&#21017;&#65289;&#23884;&#20837;ConvXAI&#35774;&#35745;&#20013;&#65292;&#20197;&#25552;&#39640;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While various AI explanation (XAI) methods have been proposed to interpret AI systems, whether the state-of-the-art XAI methods are practically useful for humans remains inconsistent findings. To improve the usefulness of XAI methods, a line of studies identifies the gaps between the diverse and dynamic real-world user needs with the status quo of XAI methods. Although prior studies envision mitigating these gaps by integrating multiple XAI methods into the universal XAI interfaces (e.g., conversational or GUI-based XAI systems), there is a lack of work investigating how these systems should be designed to meet practical user needs. In this study, we present ConvXAI, a conversational XAI system that incorporates multiple XAI types, and empowers users to request a variety of XAI questions via a universal XAI dialogue interface. Particularly, we innovatively embed practical user needs (i.e., four principles grounding on the formative study) into ConvXAI design to improve practical useful
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#21333;&#35821;&#35789;&#27573;&#31639;&#27861;StateMorph&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#21487;&#20197;&#20351;&#27169;&#22411;&#26356;&#39640;&#25928;&#22320;&#25910;&#25947;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#39564;&#35777;&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.05480</link><description>&lt;p&gt;
&#25506;&#31350;&#23376;&#35789;&#20998;&#21106;&#23545;transformer&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Investigating the effect of sub-word segmentation on the performance of transformer language models. (arXiv:2305.05480v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05480
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#21333;&#35821;&#35789;&#27573;&#31639;&#27861;StateMorph&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#21487;&#20197;&#20351;&#27169;&#22411;&#26356;&#39640;&#25928;&#22320;&#25910;&#25947;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#39564;&#35777;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24819;&#30740;&#31350;&#35789;&#27573;&#22914;&#20309;&#24433;&#21709;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#21333;&#35821;&#35789;&#27573;&#31639;&#27861;StateMorph&#65292;&#22312;&#33452;&#20848;&#35821;&#21644;&#20420;&#35821;&#20013;&#35757;&#32451;&#20102;GPT-2&#21644;BERT&#27169;&#22411;&#12290;&#20316;&#20026;&#27604;&#36739;&#65292;&#25105;&#20204;&#36824;&#35757;&#32451;&#20102;&#19968;&#20010;&#20351;&#29992;BPE&#21644;Morfessor&#20998;&#21106;&#31639;&#27861;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;StateMorph&#21487;&#20197;&#24110;&#21161;&#27169;&#22411;&#26356;&#26377;&#25928;&#22320;&#25910;&#25947;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#39564;&#35777;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We would like to explore how morphemes can affect the performance of a language model. We trained GPT-2 and Bert model with StateMorph for both Finnish and Russian, which is a morpheme segmenting algorithm. As a comparison, we also trained a model with BPE and Morfessor. Our preliminary result shows that StateMorph can help the model to converge more efficiently and achieve a better validation score.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#20844;&#21496;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#31616;&#21270;&#20844;&#21496;&#20998;&#31867;&#36807;&#31243;&#65292;&#20174;&#32780;&#20943;&#23569;&#20256;&#32479;&#26041;&#27861;&#22914;&#20840;&#29699;&#20135;&#19994;&#20998;&#31867;&#26631;&#20934;&#65288;GICS&#65289;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2305.01028</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#23398;&#20064;&#22312;&#20844;&#21496;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Company classification using zero-shot learning. (arXiv:2305.01028v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#20844;&#21496;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#31616;&#21270;&#20844;&#21496;&#20998;&#31867;&#36807;&#31243;&#65292;&#20174;&#32780;&#20943;&#23569;&#20256;&#32479;&#26041;&#27861;&#22914;&#20840;&#29699;&#20135;&#19994;&#20998;&#31867;&#26631;&#20934;&#65288;GICS&#65289;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#35768;&#22810;&#21830;&#19994;&#24212;&#29992;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#21253;&#25324;&#24773;&#24863;&#20998;&#26512;&#12289;&#25991;&#26412;&#20998;&#31867;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#20844;&#21496;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#20174;&#20844;&#21496;&#25551;&#36848;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#28982;&#21518;&#24212;&#29992;&#38646;&#26679;&#26412;&#23398;&#20064;&#23558;&#20844;&#21496;&#20998;&#31867;&#21040;&#30456;&#20851;&#31867;&#21035;&#65292;&#26080;&#38656;&#20026;&#27599;&#20010;&#31867;&#21035;&#25552;&#20379;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;&#20844;&#21496;&#25991;&#26412;&#25551;&#36848;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#23427;&#21487;&#20197;&#31616;&#21270;&#20844;&#21496;&#20998;&#31867;&#36807;&#31243;&#65292;&#20174;&#32780;&#20943;&#23569;&#20256;&#32479;&#26041;&#27861;&#22914;&#20840;&#29699;&#20135;&#19994;&#20998;&#31867;&#26631;&#20934;&#65288;GICS&#65289;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#36164;&#28304;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#33258;&#21160;&#21270;&#20844;&#21496;&#20998;&#31867;&#30340;&#28508;&#21147;&#65292;&#26159;&#26410;&#26469;&#30740;&#31350;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, natural language processing (NLP) has become increasingly important in a variety of business applications, including sentiment analysis, text classification, and named entity recognition. In this paper, we propose an approach for company classification using NLP and zero-shot learning. Our method utilizes pre-trained transformer models to extract features from company descriptions, and then applies zero-shot learning to classify companies into relevant categories without the need for specific training data for each category. We evaluate our approach on publicly available datasets of textual descriptions of companies, and demonstrate that it can streamline the process of company classification, thereby reducing the time and resources required in traditional approaches such as the Global Industry Classification Standard (GICS). The results show that this method has potential for automation of company classification, making it a promising avenue for future research in thi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#24615;LLMs&#65292;&#22914;ChatGPT&#21644;GPT-4&#22312;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#30456;&#20851;&#24615;&#25490;&#21517;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#32463;&#36866;&#24403;&#25351;&#23548;&#21518;&#34920;&#29616;&#20248;&#24322;&#65292;&#26377;&#26102;&#29978;&#33267;&#20248;&#20110;&#20256;&#32479;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;&#23558;ChatGPT&#30340;&#25490;&#21517;&#33021;&#21147;&#25552;&#28860;&#20026;&#19987;&#38376;&#27169;&#22411;&#22312;BEIR&#19978;&#30340;&#25928;&#26524;&#26356;&#20248;&#12290;</title><link>http://arxiv.org/abs/2304.09542</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#25490;&#21517;&#33021;&#21147;&#30740;&#31350;&#8212;&#8212;&#20197;ChatGPT&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agent. (arXiv:2304.09542v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09542
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#24615;LLMs&#65292;&#22914;ChatGPT&#21644;GPT-4&#22312;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#30456;&#20851;&#24615;&#25490;&#21517;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#32463;&#36866;&#24403;&#25351;&#23548;&#21518;&#34920;&#29616;&#20248;&#24322;&#65292;&#26377;&#26102;&#29978;&#33267;&#20248;&#20110;&#20256;&#32479;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;&#23558;ChatGPT&#30340;&#25490;&#21517;&#33021;&#21147;&#25552;&#28860;&#20026;&#19987;&#38376;&#27169;&#22411;&#22312;BEIR&#19978;&#30340;&#25928;&#26524;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#35777;&#26126;&#20855;&#26377;remarkable&#33021;&#21147;&#65292;&#33021;&#22815;&#23558;&#19968;&#20123;&#38646;&#26679;&#26412;&#35821;&#35328;&#20219;&#21153;&#25512;&#24191;&#33267;&#20854;&#20182;&#39046;&#22495;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;ChatGPT&#21644;GPT-4&#31561;&#29983;&#25104;&#24615;LLMs&#30340;&#30456;&#20851;&#24615;&#25490;&#21517;&#22312;&#20449;&#24687;&#26816;&#32034;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#32463;&#36807;&#36866;&#24403;&#30340;&#25351;&#23548;&#65292;ChatGPT&#21644;GPT-4&#21487;&#20197;&#22312;&#27969;&#34892;&#30340;&#20449;&#24687;&#26816;&#32034;&#22522;&#20934;&#19978;&#21462;&#24471;&#31454;&#20105;&#20248;&#21183;&#65292;&#29978;&#33267;&#26377;&#26102;&#20248;&#20110;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;GPT-4&#22312;TREC&#25968;&#25454;&#38598;&#19978;&#30340;&#24179;&#22343;nDCG&#19978;&#34920;&#29616;&#20248;&#20110;&#23436;&#20840;&#24494;&#35843;&#30340;monoT5-3B&#65292;BEIR&#25968;&#25454;&#38598;&#19978;&#30340;&#24179;&#22343;nDCG&#19978;&#20248;&#20110;monoT5-3B 2.3&#20010;&#28857;&#65292;&#20302;&#36164;&#28304;&#35821;&#35328;Mr.TyDi&#19978;&#30340;&#24179;&#22343;nDCG&#19978;&#20248;&#20110;monoT5-3B 2.7&#20010;&#28857;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#23558;ChatGPT&#30340;&#25490;&#21517;&#33021;&#21147;&#25552;&#28860;&#20026;&#19987;&#38376;&#30340;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#35757;&#32451;&#30340;&#23567;&#22411;&#19987;&#38376;&#27169;&#22411;&#65288;&#35757;&#32451;&#20110;10K&#20010;ChatGPT&#29983;&#25104;&#30340;&#25968;&#25454;&#65289;&#22312;BEIR&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#22312;400K&#20010;MS MARCO&#27880;&#37322;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;monoT5&#12290;&#20195;&#30721;&#21487;&#22312;www.github.com/sunnwe&#19978;&#22797;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated a remarkable ability to generalize zero-shot to various language-related tasks. This paper focuses on the study of exploring generative LLMs such as ChatGPT and GPT-4 for relevance ranking in Information Retrieval (IR). Surprisingly, our experiments reveal that properly instructed ChatGPT and GPT-4 can deliver competitive, even superior results than supervised methods on popular IR benchmarks. Notably, GPT-4 outperforms the fully fine-tuned monoT5-3B on MS MARCO by an average of 2.7 nDCG on TREC datasets, an average of 2.3 nDCG on eight BEIR datasets, and an average of 2.7 nDCG on ten low-resource languages Mr.TyDi. Subsequently, we delve into the potential for distilling the ranking capabilities of ChatGPT into a specialized model. Our small specialized model that trained on 10K ChatGPT generated data outperforms monoT5 trained on 400K annotated MS MARCO data on BEIR. The code to reproduce our results is available at www.github.com/sunnwe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25991;&#26723;&#23618;&#27425;&#32467;&#26500;&#35825;&#23548;&#26469;&#26816;&#27979;&#26032;&#38395;&#20013;&#30340;&#25919;&#27835;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#36807;&#25311;&#21512;&#21644;&#26377;&#38480;&#30340;&#26222;&#36866;&#24615;&#65292;&#23637;&#29616;&#20102;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.02247</link><description>&lt;p&gt;
&#12298;&#35299;&#24320;&#32467;&#26500;&#19982;&#39118;&#26684;&#30340;&#32445;&#24102;&#65306;&#36890;&#36807;&#35825;&#23548;&#25991;&#26723;&#23618;&#27425;&#32467;&#26500;&#26469;&#26816;&#27979;&#26032;&#38395;&#20013;&#30340;&#25919;&#27835;&#20559;&#35265;&#12299;
&lt;/p&gt;
&lt;p&gt;
Disentangling Structure and Style: Political Bias Detection in News by Inducing Document Hierarchy. (arXiv:2304.02247v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25991;&#26723;&#23618;&#27425;&#32467;&#26500;&#35825;&#23548;&#26469;&#26816;&#27979;&#26032;&#38395;&#20013;&#30340;&#25919;&#27835;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#36807;&#25311;&#21512;&#21644;&#26377;&#38480;&#30340;&#26222;&#36866;&#24615;&#65292;&#23637;&#29616;&#20102;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#26032;&#38395;&#25991;&#31456;&#20013;&#25919;&#27835;&#20559;&#35265;&#26816;&#27979;&#26041;&#38754;&#30340;&#37325;&#35201;&#24046;&#36317;&#36827;&#34892;&#30740;&#31350;&#12290;&#20808;&#21069;&#36827;&#34892;&#30417;&#30563;&#24335;&#25991;&#26723;&#20998;&#31867;&#30340;&#24037;&#20316;&#21487;&#33021;&#20250;&#20559;&#21521;&#21508;&#32593;&#31449;&#30340;&#20889;&#20316;&#39118;&#26684;&#65292;&#23548;&#33268;&#36807;&#25311;&#21512;&#21644;&#26377;&#38480;&#30340;&#26222;&#36866;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#32771;&#34385;&#21477;&#23376;&#32423;&#35821;&#20041;&#21644;&#25991;&#26723;&#32423;&#20462;&#36766;&#32467;&#26500;&#26469;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#20174;&#32780;&#20135;&#29983;&#19968;&#31181;&#26356;&#24378;&#22823;&#21644;&#19981;&#21463;&#39118;&#26684;&#24433;&#21709;&#30340;&#26816;&#27979;&#25919;&#27835;&#20559;&#35265;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#22836;&#20998;&#23618;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#36890;&#36807;&#21508;&#31181;&#27880;&#24847;&#21147;&#22836;&#30340;&#19981;&#21516;&#38598;&#21512;&#26377;&#25928;&#22320;&#32534;&#30721;&#38271;&#25991;&#26723;&#30340;&#32467;&#26500;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20811;&#26381;&#20102;&#36825;&#31181;&#22495;&#20381;&#36182;&#24615;&#65292;&#24182;&#34920;&#29616;&#20986;&#27604;&#20808;&#21069;&#26041;&#27861;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#25429;&#25417;&#21040;&#26032;&#38395;&#20013;&#24120;&#29992;&#30340;&#35805;&#35821;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address an important gap in detection of political bias in news articles. Previous works that perform supervised document classification can be biased towards the writing style of each news outlet, leading to overfitting and limited generalizability. Our approach overcomes this limitation by considering both the sentence-level semantics and the document-level rhetorical structure, resulting in a more robust and style-agnostic approach to detecting political bias in news articles. We introduce a novel multi-head hierarchical attention model that effectively encodes the structure of long documents through a diverse ensemble of attention heads. While journalism follows a formalized rhetorical structure, the writing style may vary by news outlet. We demonstrate that our method overcomes this domain dependency and outperforms previous approaches for robustness and accuracy. Further analysis demonstrates the ability of our model to capture the discourse structures commonly used in the jou
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#23545;&#27604;&#23398;&#20064;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24863;&#30693;&#33021;&#21147;&#30340;&#39640;&#25928;&#26041;&#27861;eP-ALM&#65292;&#21487;&#20197;&#23454;&#29616;&#35270;&#35273;&#24863;&#30693;&#20449;&#24687;&#21644;&#25991;&#26412;&#20449;&#24687;&#30340;&#34701;&#21512;&#65292;&#21516;&#26102;&#36824;&#33021;&#22312;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.11403</link><description>&lt;p&gt;
eP-ALM:&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#24863;&#30693;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
eP-ALM: Efficient Perceptual Augmentation of Language Models. (arXiv:2303.11403v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#23545;&#27604;&#23398;&#20064;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24863;&#30693;&#33021;&#21147;&#30340;&#39640;&#25928;&#26041;&#27861;eP-ALM&#65292;&#21487;&#20197;&#23454;&#29616;&#35270;&#35273;&#24863;&#30693;&#20449;&#24687;&#21644;&#25991;&#26412;&#20449;&#24687;&#30340;&#34701;&#21512;&#65292;&#21516;&#26102;&#36824;&#33021;&#22312;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#36804;&#20170;&#20026;&#27490;&#32473;&#19990;&#30028;&#30041;&#19979;&#20102;&#28145;&#21051;&#21360;&#35937;&#65292;&#20855;&#26377;&#22823;&#35268;&#27169;&#27169;&#22411;&#25152;&#20855;&#26377;&#30340;&#38750;&#21516;&#23547;&#24120;&#30340;&#33021;&#21147;&#12290;&#22312;&#35270;&#35273;&#26041;&#38754;&#65292;&#21464;&#21387;&#22120;&#27169;&#22411;&#65288;&#21363;ViT&#65289;&#20063;&#22312;&#36861;&#38543;&#21516;&#19968;&#36235;&#21183;&#65292;&#21462;&#24471;&#20102;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#30340;&#26368;&#20339;&#34920;&#29616;&#12290;&#38543;&#30528;&#36825;&#31181;&#21333;&#27169;&#22411;&#30340;&#20016;&#23500;&#22810;&#26679;&#65292;&#33258;&#28982;&#20250;&#24341;&#21457;&#19968;&#20010;&#38382;&#39064;&#65306;&#25105;&#20204;&#26159;&#21542;&#38656;&#35201;&#36319;&#38543;&#36825;&#20010;&#36235;&#21183;&#26469;&#22788;&#29702;&#22810;&#27169;&#24577;&#20219;&#21153;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#21162;&#21147;&#38598;&#20013;&#20110;&#29616;&#26377;&#27169;&#22411;&#30340;&#39640;&#25928;&#36866;&#24212;&#65292;&#24182;&#25552;&#20986;&#29992;&#24863;&#30693;&#26469;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;&#36866;&#24212;&#39044;&#35757;&#32451;&#27169;&#22411;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#30340;&#26041;&#27861;&#20173;&#28982;&#20381;&#36182;&#20110;&#20960;&#20010;&#20851;&#38190;&#32452;&#20214;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#23427;&#20204;&#30340;&#25928;&#29575;&#12290;&#29305;&#21035;&#22320;&#65292;&#20182;&#20204;&#20173;&#28982;&#35757;&#32451;&#22823;&#37327;&#30340;&#21442;&#25968;&#65292;&#20381;&#36182;&#22823;&#35268;&#27169;&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#65292;&#20351;&#29992;&#22312;&#24040;&#22823;&#30340;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#65288;&#20363;&#22914;CLIP&#65289;&#65292;&#24182;&#28155;&#21152;&#20102;&#26174;&#33879;&#30340;&#25512;&#29702;&#24320;&#38144;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#20013;&#30340;&#22823;&#22810;&#25968;&#20851;&#27880;Zero-Shot&#21644;In Context Learning&#65292;&#35266;&#23519;&#21040;&#20004;&#31181;&#33539;&#24335;&#20043;&#38388;&#30340;&#24040;&#22823;&#24046;&#24322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;eP-ALM&#65292;&#19968;&#31181;&#23558;&#35270;&#35273;&#24863;&#30693;&#20449;&#24687;&#19982;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#23454;&#29616;&#35270;&#35273;&#24863;&#30693;&#21644;&#25991;&#26412;&#20449;&#24687;&#30340;&#34701;&#21512;&#65292;&#20855;&#26377;&#26497;&#23567;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#20219;&#20309;&#26032;&#30340;&#39044;&#35757;&#32451;&#65292;&#20173;&#28982;&#22312;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have so far impressed the world, with unprecedented capabilities that emerge in models at large scales. On the vision side, transformer models (i.e., ViT) are following the same trend, achieving the best performance on challenging benchmarks. With the abundance of such unimodal models, a natural question arises; do we need also to follow this trend to tackle multimodal tasks? In this work, we propose to rather direct effort to efficient adaptations of existing models, and propose to augment Language Models with perception. Existing approaches for adapting pretrained models for vision-language tasks still rely on several key components that hinder their efficiency. In particular, they still train a large number of parameters, rely on large multimodal pretraining, use encoders (e.g., CLIP) trained on huge image-text datasets, and add significant inference overhead. In addition, most of these approaches have focused on Zero-Shot and In Context Learning, with l
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#22788;&#29702;&#35270;&#35273;&#22330;&#26223;&#30340;&#36890;&#29992;&#26426;&#21046;&#23545;&#20110;&#25552;&#21319;&#25277;&#35937;&#35270;&#35273;&#25512;&#29702;&#30340;&#20316;&#29992;&#65292;&#24182;&#21457;&#29616;&#22312;RPM-like&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#36825;&#26679;&#30340;&#27169;&#22411;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.02260</link><description>&lt;p&gt;
&#23398;&#20064;&#22312;&#35270;&#35273;&#23545;&#35937;&#19978;&#36827;&#34892;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Learning to reason over visual objects. (arXiv:2303.02260v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02260
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#22788;&#29702;&#35270;&#35273;&#22330;&#26223;&#30340;&#36890;&#29992;&#26426;&#21046;&#23545;&#20110;&#25552;&#21319;&#25277;&#35937;&#35270;&#35273;&#25512;&#29702;&#30340;&#20316;&#29992;&#65292;&#24182;&#21457;&#29616;&#22312;RPM-like&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#36825;&#26679;&#30340;&#27169;&#22411;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26234;&#33021;&#30340;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#26159;&#33021;&#22815;&#35782;&#21035;&#22797;&#26434;&#39640;&#32500;&#24863;&#30693;&#25968;&#25454;&#20013;&#30340;&#25277;&#35937;&#27169;&#24335;&#65292;&#20363;&#22914;Raven's Progressive Matrices &#65288;RPM&#65289;&#31561;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#12290;&#20026;&#20102;&#35774;&#35745;&#20855;&#22791;&#36825;&#31181;&#33021;&#21147;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#35780;&#20272;&#31070;&#32463;&#32593;&#32476;&#26159;&#21542;&#33021;&#22815;&#23398;&#20064;&#35299;&#20915;&#31867;&#20284;RPM&#30340;&#38382;&#39064;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#21457;&#29616;&#65292;&#22312;&#36825;&#20123;&#38382;&#39064;&#19978;&#34920;&#29616;&#33391;&#22909;&#38656;&#35201;&#23558;RPM&#38382;&#39064;&#26684;&#24335;&#30340;&#24402;&#32435;&#20559;&#35265;&#32435;&#20837;&#27169;&#22411;&#20013;&#65292;&#36825;&#24341;&#21457;&#20102;&#36825;&#26679;&#19968;&#20010;&#38382;&#39064;&#65306;&#36825;&#26679;&#30340;&#27169;&#22411;&#26159;&#21542;&#21487;&#20197;&#26356;&#24191;&#27867;&#22320;&#24212;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#22788;&#29702;&#35270;&#35273;&#22330;&#26223;&#30340;&#36890;&#29992;&#26426;&#21046;&#23545;&#20419;&#36827;&#25277;&#35937;&#35270;&#35273;&#25512;&#29702;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19968;&#20010;&#31616;&#21333;&#30340;&#27169;&#22411;&#65292;&#20165;&#30001;&#19968;&#20010;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;transformer&#25512;&#29702;&#27169;&#22359;&#32452;&#25104;&#65292;&#22312;&#20004;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;RPM-like&#22522;&#20934;&#27979;&#35797;&#20013;&#65288;PGM&#21644;I-&#65289;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
A core component of human intelligence is the ability to identify abstract patterns inherent in complex, high-dimensional perceptual data, as exemplified by visual reasoning tasks such as Raven's Progressive Matrices (RPM). Motivated by the goal of designing AI systems with this capacity, recent work has focused on evaluating whether neural networks can learn to solve RPM-like problems. Previous work has generally found that strong performance on these problems requires the incorporation of inductive biases that are specific to the RPM problem format, raising the question of whether such models might be more broadly useful. Here, we investigated the extent to which a general-purpose mechanism for processing visual scenes in terms of objects might help promote abstract visual reasoning. We found that a simple model, consisting only of an object-centric encoder and a transformer reasoning module, achieved state-of-the-art results on both of two challenging RPM-like benchmarks (PGM and I-
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23545;&#32852;&#21512;&#23454;&#20307;&#21644;&#20851;&#31995;&#25552;&#21462;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#30740;&#31350;&#65292;&#21457;&#29616;&#22312;&#26356;&#30495;&#23454;&#30340;&#23454;&#39564;&#35774;&#32622;&#19979;&#65292;&#27169;&#22411;&#30340;F1&#24471;&#20998;&#26174;&#33879;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22522;&#20110;BERT&#30340;&#20998;&#31867;&#22120;&#30340;&#20004;&#27493;&#24314;&#27169;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.09887</link><description>&lt;p&gt;
&#20174;&#33258;&#30001;&#25991;&#26412;&#20013;&#25552;&#21462;&#20851;&#31995;&#19977;&#20803;&#32452;&#65306;90% F1&#24471;&#20998;&#26159;&#30495;&#23454;&#30340;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
90% F1 Score in Relational Triple Extraction: Is it Real ?. (arXiv:2302.09887v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09887
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23545;&#32852;&#21512;&#23454;&#20307;&#21644;&#20851;&#31995;&#25552;&#21462;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#30740;&#31350;&#65292;&#21457;&#29616;&#22312;&#26356;&#30495;&#23454;&#30340;&#23454;&#39564;&#35774;&#32622;&#19979;&#65292;&#27169;&#22411;&#30340;F1&#24471;&#20998;&#26174;&#33879;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22522;&#20110;BERT&#30340;&#20998;&#31867;&#22120;&#30340;&#20004;&#27493;&#24314;&#27169;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#20851;&#31995;&#19977;&#20803;&#32452;&#26159;&#26500;&#24314;&#30693;&#35782;&#24211;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#26368;&#36817;&#65292;&#32852;&#21512;&#23454;&#20307;&#21644;&#20851;&#31995;&#25552;&#21462;&#27169;&#22411;&#30340;&#36827;&#23637;&#23637;&#31034;&#20102;&#22312;&#20934;&#30830;&#25552;&#21462;&#20851;&#31995;&#19977;&#20803;&#32452;&#26041;&#38754;&#36798;&#21040;&#30340;&#21331;&#36234;F1&#24471;&#20998;&#65288;&#8805; 90%&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#21463;&#38480;&#30340;&#23454;&#39564;&#35774;&#32622;&#21644;&#19981;&#30495;&#23454;&#30340;&#25968;&#25454;&#38598;&#19979;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23427;&#20204;&#24573;&#35270;&#20102;&#38646;&#19977;&#20803;&#32452;&#21477;&#23376;&#65288;&#38646;&#22522;&#25968;&#65289;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#26356;&#30495;&#23454;&#30340;&#35774;&#32622;&#19979;&#23545;&#26368;&#20808;&#36827;&#30340;&#32852;&#21512;&#23454;&#20307;&#21644;&#20851;&#31995;&#25552;&#21462;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#21253;&#25324;&#20102;&#32570;&#20047;&#20219;&#20309;&#19977;&#20803;&#32452;&#30340;&#21477;&#23376;&#65292;&#25552;&#20379;&#20102;&#32508;&#21512;&#24615;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#36825;&#31181;&#26356;&#30495;&#23454;&#30340;&#23454;&#39564;&#35774;&#32622;&#20013;&#65292;&#27169;&#22411;&#30340;F1&#24471;&#20998;&#26377;&#26174;&#33879;&#19979;&#38477;&#65288;&#22312;&#19968;&#20010;&#25968;&#25454;&#38598;&#20013;&#32422;10-15%&#65292;&#22312;&#21478;&#19968;&#20010;&#25968;&#25454;&#38598;&#20013;6-14%&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31616;&#21333;&#30340;&#22522;&#20110;BERT&#30340;&#20998;&#31867;&#22120;&#30340;&#20004;&#27493;&#24314;&#27169;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extracting relational triples from text is a crucial task for constructing knowledge bases. Recent advancements in joint entity and relation extraction models have demonstrated remarkable F1 scores ($\ge 90\%$) in accurately extracting relational triples from free text. However, these models have been evaluated under restrictive experimental settings and unrealistic datasets. They overlook sentences with zero triples (zero-cardinality), thereby simplifying the task. In this paper, we present a benchmark study of state-of-the-art joint entity and relation extraction models under a more realistic setting. We include sentences that lack any triples in our experiments, providing a comprehensive evaluation. Our findings reveal a significant decline (approximately 10-15\% in one dataset and 6-14\% in another dataset) in the models' F1 scores within this realistic experimental setup. Furthermore, we propose a two-step modeling approach that utilizes a simple BERT-based classifier. This approa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20154;&#24037;&#21644;&#26426;&#22120;&#23457;&#26680;&#21592;&#30340;&#20849;&#24773;&#20882;&#29359;&#21644;&#22122;&#22768;&#23457;&#35745;&#30740;&#31350;&#20102;&#20882;&#29359;&#24615;&#35328;&#35770;&#26816;&#27979;&#20013;&#30340;&#24046;&#24322;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23457;&#26680;&#21592;&#20043;&#38388;&#23384;&#22312;&#24191;&#27867;&#30340;&#20998;&#27495;&#65292;&#24182;&#19988;&#20154;&#24037;&#23457;&#26680;&#21592;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20998;&#31867;&#22120;&#26080;&#27861;&#39044;&#27979;&#20854;&#20182;&#23457;&#26680;&#21592;&#30340;&#22238;&#24212;&#12290;&#36825;&#23545;&#20110;&#20869;&#23481;&#23457;&#26680;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2301.12534</link><description>&lt;p&gt;
&#20154;&#24037;&#21644;&#26426;&#22120;&#20851;&#20110;&#20160;&#20040;&#26159;&#20882;&#29359;&#23384;&#22312;&#36739;&#22823;&#20998;&#27495;&#30340;&#20849;&#24773;&#20882;&#29359;&#21644;&#22122;&#22768;&#23457;&#35745;&#65306;&#32479;&#19968;&#20027;&#35266;&#20882;&#29359;&#30340;&#20154;&#31867;&#21644;&#26426;&#22120;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Vicarious Offense and Noise Audit of Offensive Speech Classifiers: Unifying Human and Machine Disagreement on What is Offensive. (arXiv:2301.12534v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20154;&#24037;&#21644;&#26426;&#22120;&#23457;&#26680;&#21592;&#30340;&#20849;&#24773;&#20882;&#29359;&#21644;&#22122;&#22768;&#23457;&#35745;&#30740;&#31350;&#20102;&#20882;&#29359;&#24615;&#35328;&#35770;&#26816;&#27979;&#20013;&#30340;&#24046;&#24322;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23457;&#26680;&#21592;&#20043;&#38388;&#23384;&#22312;&#24191;&#27867;&#30340;&#20998;&#27495;&#65292;&#24182;&#19988;&#20154;&#24037;&#23457;&#26680;&#21592;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20998;&#31867;&#22120;&#26080;&#27861;&#39044;&#27979;&#20854;&#20182;&#23457;&#26680;&#21592;&#30340;&#22238;&#24212;&#12290;&#36825;&#23545;&#20110;&#20869;&#23481;&#23457;&#26680;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20882;&#29359;&#24615;&#35328;&#35770;&#26816;&#27979;&#26159;&#20869;&#23481;&#23457;&#26680;&#30340;&#19968;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#20160;&#20040;&#26159;&#20882;&#29359;&#24615;&#30340;&#21487;&#20197;&#26159;&#39640;&#24230;&#20027;&#35266;&#30340;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#24403;&#28041;&#21450;&#21040;&#29616;&#23454;&#19990;&#30028;&#31038;&#20132;&#32593;&#31449;&#25919;&#27835;&#35328;&#35770;&#26102;&#65292;&#20154;&#24037;&#21644;&#26426;&#22120;&#23457;&#26680;&#21592;&#23545;&#20110;&#20160;&#20040;&#26159;&#20882;&#29359;&#24615;&#30340;&#23384;&#22312;&#20998;&#27495;&#12290;&#25105;&#20204;&#21457;&#29616;&#65288;1&#65289;&#23457;&#26680;&#21592;&#20043;&#38388;&#65288;&#21253;&#25324;&#20154;&#24037;&#21644;&#26426;&#22120;&#65289;&#23384;&#22312;&#24191;&#27867;&#20998;&#27495;&#65307;&#21644;&#65288;2&#65289;&#20154;&#24037;&#23457;&#26680;&#21592;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20998;&#31867;&#22120;&#26080;&#27861;&#39044;&#27979;&#20854;&#20182;&#23457;&#26680;&#21592;&#22522;&#20110;&#20182;&#20204;&#30340;&#25919;&#27835;&#20542;&#21521;&#22914;&#20309;&#22238;&#24212;&#12290;&#23545;&#20110;&#65288;1&#65289;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#21069;&#25152;&#26410;&#26377;&#35268;&#27169;&#30340;&#22122;&#22768;&#23457;&#35745;&#65292;&#32467;&#21512;&#20102;&#26426;&#22120;&#21644;&#20154;&#24037;&#22238;&#31572;&#12290;&#23545;&#20110;&#65288;2&#65289;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#39318;&#21019;&#30340;&#20849;&#24773;&#20882;&#29359;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#22122;&#22768;&#23457;&#35745;&#25581;&#31034;&#20102;&#19981;&#21516;&#26426;&#22120;&#23457;&#26680;&#21592;&#20043;&#38388;&#30340;&#23457;&#26680;&#32467;&#26524;&#24046;&#24322;&#24456;&#22823;&#12290;&#25105;&#20204;&#19982;&#20154;&#24037;&#23457;&#26680;&#21592;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25919;&#27835;&#20542;&#21521;&#32467;&#21512;&#25935;&#24863;&#38382;&#39064;&#20250;&#24433;&#21709;&#21040;&#19968;&#23545;&#19968;&#30340;&#20882;&#29359;&#65292;&#20197;&#21450;&#20849;&#24773;&#20882;&#29359;&#12290;&#25968;&#25454;&#38598;&#21487;&#36890;&#36807;https://github.com/Homan-Lab/voic&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offensive speech detection is a key component of content moderation. However, what is offensive can be highly subjective. This paper investigates how machine and human moderators disagree on what is offensive when it comes to real-world social web political discourse. We show that (1) there is extensive disagreement among the moderators (humans and machines); and (2) human and large-language-model classifiers are unable to predict how other human raters will respond, based on their political leanings. For (1), we conduct a noise audit at an unprecedented scale that combines both machine and human responses. For (2), we introduce a first-of-its-kind dataset of vicarious offense. Our noise audit reveals that moderation outcomes vary wildly across different machine moderators. Our experiments with human moderators suggest that political leanings combined with sensitive issues affect both first-person and vicarious offense. The dataset is available through https://github.com/Homan-Lab/voic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#29983;&#25104;&#24207;&#21015;&#26631;&#35760;&#20219;&#21153;&#20013;&#22914;&#20309;&#25913;&#21892;&#23545;&#36328;&#24230;&#32423;&#21035;&#32622;&#20449;&#24230;&#30340;&#20272;&#35745;&#12290;&#30740;&#31350;&#21457;&#29616;&#20165;&#20165;&#20351;&#29992;&#35299;&#30721;&#22120;&#30340;&#36755;&#20986;&#27010;&#29575;&#24182;&#19981;&#26159;&#26368;&#20339;&#26041;&#27861;&#65292;&#32780;&#21033;&#29992;&#26463;&#25628;&#32034;&#30340;&#21069;k&#20010;&#39044;&#27979;&#30340;&#32479;&#35745;&#25968;&#25454;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#26657;&#20934;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2212.10767</link><description>&lt;p&gt;
&#22914;&#20309;&#22312;&#29983;&#25104;&#24207;&#21015;&#26631;&#35760;&#20013;&#25913;&#21892;&#22522;&#20110;&#36328;&#24230;&#32423;&#21035;&#32622;&#20449;&#24230;&#30340;&#26463;&#25628;&#32034;&#65311;(arXiv:2212.10767v2 [cs.CL] &#26356;&#26032;)
&lt;/p&gt;
&lt;p&gt;
How Does Beam Search improve Span-Level Confidence Estimation in Generative Sequence Labeling?. (arXiv:2212.10767v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#29983;&#25104;&#24207;&#21015;&#26631;&#35760;&#20219;&#21153;&#20013;&#22914;&#20309;&#25913;&#21892;&#23545;&#36328;&#24230;&#32423;&#21035;&#32622;&#20449;&#24230;&#30340;&#20272;&#35745;&#12290;&#30740;&#31350;&#21457;&#29616;&#20165;&#20165;&#20351;&#29992;&#35299;&#30721;&#22120;&#30340;&#36755;&#20986;&#27010;&#29575;&#24182;&#19981;&#26159;&#26368;&#20339;&#26041;&#27861;&#65292;&#32780;&#21033;&#29992;&#26463;&#25628;&#32034;&#30340;&#21069;k&#20010;&#39044;&#27979;&#30340;&#32479;&#35745;&#25968;&#25454;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#26657;&#20934;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#26631;&#35760;&#26159;&#20449;&#24687;&#25277;&#21462;/&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#20013;&#30340;&#26680;&#24515;&#20219;&#21153;&#12290;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#36234;&#26469;&#36234;&#25104;&#20026;&#36825;&#31867;&#20219;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;&#65288;&#20363;&#22914;&#23454;&#20307;&#25552;&#21462;&#21644;&#23545;&#35805;&#27133;&#22635;&#20805;&#65289;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#26631;&#35760;&#20934;&#30830;&#24615;&#19978;&#65292;&#20294;&#19968;&#20010;&#20851;&#38190;&#30340;&#26041;&#38754;&#8212;&#8212;&#23545;&#27169;&#22411;&#32622;&#20449;&#24230;&#30340;&#29702;&#35299;&#21364;&#34987;&#24573;&#35270;&#20102;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32570;&#20047;&#19968;&#20010;&#33021;&#22815;&#21487;&#38752;&#22320;&#34913;&#37327;&#27169;&#22411;&#23545;&#27599;&#20010;&#26631;&#35760;&#36328;&#24230;&#30340;&#39044;&#27979;&#32622;&#20449;&#24230;&#30340;&#21407;&#21017;&#24615;&#29702;&#35299;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#19968;&#20123;&#20851;&#20110;&#29983;&#25104;&#24207;&#21015;&#26631;&#35760;&#30340;&#27169;&#22411;&#32622;&#20449;&#24230;&#20272;&#35745;&#30340;&#23454;&#35777;&#35265;&#35299;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#20165;&#20165;&#20351;&#29992;&#35299;&#30721;&#22120;&#30340;&#36755;&#20986;&#27010;&#29575;&#24182;&#19981;&#26159;&#23454;&#29616;&#33391;&#22909;&#26657;&#20934;&#32622;&#20449;&#24230;&#20272;&#35745;&#30340;&#26368;&#20339;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#20845;&#20010;&#19981;&#21516;&#20219;&#21153;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#36827;&#34892;&#39564;&#35777;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#8212;&#8212;&#21033;&#29992;&#26463;&#25628;&#32034;&#30340;&#21069;k&#20010;&#39044;&#27979;&#30340;&#32479;&#35745;&#25968;&#25454;&#8212;&#8212;&#26174;&#33879;&#38477;&#20302;&#20102;&#26657;&#20934;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequence labeling is a core task in text understanding for IE/IR systems. Text generation models have increasingly become the go-to solution for such tasks (e.g., entity extraction and dialog slot filling). While most research has focused on the labeling accuracy, a key aspect -- of vital practical importance -- has slipped through the cracks: understanding model confidence. More specifically, we lack a principled understanding of how to reliably gauge the confidence of a model in its predictions for each labeled span. This paper aims to provide some empirical insights on estimating model confidence for generative sequence labeling. Most notably, we find that simply using the decoder's output probabilities \textbf{is not} the best in realizing well-calibrated confidence estimates. As verified over six public datasets of different tasks, we show that our proposed approach -- which leverages statistics from top-$k$ predictions by a beam search -- significantly reduces calibration errors 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#27169;&#22411;&#29983;&#25104;&#27491;&#30830;&#31354;&#38388;&#20851;&#31995;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#25351;&#26631;VISOR&#20197;&#34913;&#37327;&#29983;&#25104;&#22270;&#20687;&#30340;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#21457;&#29616;&#24403;&#21069;T2I&#27169;&#22411;&#23613;&#31649;&#21487;&#20197;&#29983;&#25104;&#39640;&#24230;&#36924;&#30495;&#30340;&#22270;&#20687;&#65292;&#20294;&#20854;&#31354;&#38388;&#19978;&#20934;&#30830;&#30340;&#22270;&#20687;&#33021;&#21147;&#20173;&#28982;&#19981;&#36275;&#65292;&#29305;&#21035;&#26159;&#22312;&#31354;&#38388;&#35859;&#35789;&#21644;&#22330;&#26223;&#20851;&#31995;&#29702;&#35299;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2212.10015</link><description>&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#31354;&#38388;&#20851;&#31995;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Spatial Relationships in Text-to-Image Generation. (arXiv:2212.10015v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#27169;&#22411;&#29983;&#25104;&#27491;&#30830;&#31354;&#38388;&#20851;&#31995;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#25351;&#26631;VISOR&#20197;&#34913;&#37327;&#29983;&#25104;&#22270;&#20687;&#30340;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#21457;&#29616;&#24403;&#21069;T2I&#27169;&#22411;&#23613;&#31649;&#21487;&#20197;&#29983;&#25104;&#39640;&#24230;&#36924;&#30495;&#30340;&#22270;&#20687;&#65292;&#20294;&#20854;&#31354;&#38388;&#19978;&#20934;&#30830;&#30340;&#22270;&#20687;&#33021;&#21147;&#20173;&#28982;&#19981;&#36275;&#65292;&#29305;&#21035;&#26159;&#22312;&#31354;&#38388;&#35859;&#35789;&#21644;&#22330;&#26223;&#20851;&#31995;&#29702;&#35299;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#38388;&#29702;&#35299;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#22522;&#26412;&#26041;&#38754;&#65292;&#23545;&#20110;&#20154;&#31867;&#32423;&#21035;&#30340;&#22270;&#20687;&#25512;&#29702;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#27492;&#26159;&#22522;&#30784;&#35821;&#35328;&#29702;&#35299;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#26368;&#36817;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#65288;T2I&#65289;&#27169;&#22411;&#22312;&#36924;&#30495;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#30340;&#21487;&#38752;&#31354;&#38388;&#29702;&#35299;&#33021;&#21147;&#23578;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;T2I&#27169;&#22411;&#29983;&#25104;&#27491;&#30830;&#31354;&#38388;&#20851;&#31995;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;VISOR&#35780;&#20272;&#25351;&#26631;&#65292;&#23427;&#25429;&#25417;&#20102;&#25991;&#26412;&#20013;&#25551;&#36848;&#30340;&#31354;&#38388;&#20851;&#31995;&#22312;&#22270;&#20687;&#20013;&#26159;&#21542;&#20934;&#30830;&#29983;&#25104;&#12290;&#20026;&#20102;&#22522;&#20934;&#29616;&#26377;&#27169;&#22411;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21253;&#21547;&#25551;&#36848;&#20004;&#20010;&#23545;&#35937;&#21450;&#23427;&#20204;&#20043;&#38388;&#31354;&#38388;&#20851;&#31995;&#30340;&#21477;&#23376;&#25968;&#25454;&#38598;SR2D&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#35780;&#20272;&#27969;&#31243;&#26469;&#35782;&#21035;&#29289;&#20307;&#21450;&#20854;&#31354;&#38388;&#20851;&#31995;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#35780;&#20272;T2I&#27169;&#22411;&#26102;&#37319;&#29992;&#23427;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#21457;&#29616;&#20102;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#21457;&#29616;&#65292;&#20063;&#23601;&#26159;&#23613;&#31649;&#26368;&#26032;&#30340;T2I&#27169;&#22411;&#33021;&#22815;&#20135;&#29983;&#39640;&#24230;&#36924;&#30495;&#30340;&#22270;&#20687;&#65292;&#20294;&#23427;&#20204;&#29983;&#25104;&#31354;&#38388;&#19978;&#20934;&#30830;&#30340;&#22270;&#20687;&#33021;&#21147;&#20173;&#28982;&#19981;&#36275;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#27169;&#22411;&#22312;&#31354;&#38388;&#35859;&#35789;&#65288;&#22914;'&#22312;&#21069;&#38754;'&#21644;'&#22312;&#21518;&#38754;'&#65289;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#24182;&#19988;&#22312;&#22330;&#26223;&#30340;&#20851;&#31995;&#29702;&#35299;&#26041;&#38754;&#20063;&#26377;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spatial understanding is a fundamental aspect of computer vision and integral for human-level reasoning about images, making it an important component for grounded language understanding. While recent text-to-image synthesis (T2I) models have shown unprecedented improvements in photorealism, it is unclear whether they have reliable spatial understanding capabilities. We investigate the ability of T2I models to generate correct spatial relationships among objects and present VISOR, an evaluation metric that captures how accurately the spatial relationship described in text is generated in the image. To benchmark existing models, we introduce a dataset, SR2D, that contains sentences describing two objects and the spatial relationship between them. We construct an automated evaluation pipeline to recognize objects and their spatial relationships, and employ it in a large-scale evaluation of T2I models. Our experiments reveal a surprising finding that, although state-of-the-art T2I models 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;LR-Sum&#39033;&#30446;&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#36164;&#28304;&#26377;&#38480;&#35821;&#35328;&#30340;&#26032;&#30340;&#25688;&#35201;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#20419;&#36827;&#33258;&#21160;&#25688;&#35201;&#30740;&#31350;&#12290;&#25968;&#25454;&#38598;&#21253;&#21547;40&#31181;&#36164;&#28304;&#26377;&#38480;&#35821;&#35328;&#30340;&#20154;&#24037;&#25776;&#20889;&#30340;&#25688;&#35201;&#65292;&#24182;&#26681;&#25454;Creative Commons&#35768;&#21487;&#35777;&#21457;&#24067;&#65292;&#26159;&#26368;&#24320;&#25918;&#35768;&#21487;&#30340;&#22810;&#35821;&#35328;&#25688;&#35201;&#25968;&#25454;&#38598;&#20043;&#19968;&#12290;</title><link>http://arxiv.org/abs/2212.09674</link><description>&lt;p&gt;
LR-Sum: &#38754;&#21521;&#36164;&#28304;&#26377;&#38480;&#35821;&#35328;&#30340;&#25688;&#35201;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
LR-Sum: Summarization for Less-Resourced Languages. (arXiv:2212.09674v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09674
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;LR-Sum&#39033;&#30446;&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#36164;&#28304;&#26377;&#38480;&#35821;&#35328;&#30340;&#26032;&#30340;&#25688;&#35201;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#20419;&#36827;&#33258;&#21160;&#25688;&#35201;&#30740;&#31350;&#12290;&#25968;&#25454;&#38598;&#21253;&#21547;40&#31181;&#36164;&#28304;&#26377;&#38480;&#35821;&#35328;&#30340;&#20154;&#24037;&#25776;&#20889;&#30340;&#25688;&#35201;&#65292;&#24182;&#26681;&#25454;Creative Commons&#35768;&#21487;&#35777;&#21457;&#24067;&#65292;&#26159;&#26368;&#24320;&#25918;&#35768;&#21487;&#30340;&#22810;&#35821;&#35328;&#25688;&#35201;&#25968;&#25454;&#38598;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#39044;&#21360;&#25991;&#26723;&#25551;&#36848;&#20102;LR-Sum&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;LR-Sum&#26159;&#19968;&#20010;&#20351;&#29992;&#23485;&#26494;&#35768;&#21487;&#35777;&#21019;&#24314;&#30340;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#20419;&#36827;&#23545;&#36164;&#28304;&#26377;&#38480;&#35821;&#35328;&#30340;&#33258;&#21160;&#25688;&#35201;&#30740;&#31350;&#12290;LR-Sum&#21253;&#21547;40&#31181;&#35821;&#35328;&#30340;&#20154;&#24037;&#25776;&#20889;&#30340;&#25688;&#35201;&#65292;&#20854;&#20013;&#35768;&#22810;&#26159;&#36164;&#28304;&#26377;&#38480;&#30340;&#35821;&#35328;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#20174;Multilingual Open Text&#35821;&#26009;&#24211;&#20013;&#25552;&#21462;&#21644;&#31579;&#36873;&#25968;&#25454;&#38598;&#30340;&#36807;&#31243;&#12290;&#28304;&#25968;&#25454;&#26159;&#20174;Voice of America&#32593;&#31449;&#25910;&#38598;&#30340;&#20844;&#20849;&#39046;&#22495;&#26032;&#38395;&#65292;LR-Sum&#20197;Creative Commons&#35768;&#21487;&#35777;&#65288;CC BY 4.0&#65289;&#21457;&#24067;&#65292;&#25104;&#20026;&#26368;&#24320;&#25918;&#35768;&#21487;&#30340;&#22810;&#35821;&#35328;&#25688;&#35201;&#25968;&#25454;&#38598;&#20043;&#19968;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#22914;&#20309;&#35745;&#21010;&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#23454;&#39564;&#65292;&#24182;&#35752;&#35770;&#20102;&#25968;&#25454;&#38598;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This preprint describes work in progress on LR-Sum, a new permissively-licensed dataset created with the goal of enabling further research in automatic summarization for less-resourced languages. LR-Sum contains human-written summaries for 40 languages, many of which are less-resourced. We describe our process for extracting and filtering the dataset from the Multilingual Open Text corpus (Palen-Michel et al., 2022). The source data is public domain newswire collected from from Voice of America websites, and LR-Sum is released under a Creative Commons license (CC BY 4.0), making it one of the most openly-licensed multilingual summarization datasets. We describe how we plan to use the data for modeling experiments and discuss limitations of the dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#20013;&#26159;&#21542;&#20687;&#20154;&#31867;&#19968;&#26679;&#36890;&#36807;&#28151;&#20837;&#20869;&#23481;&#26469;&#24433;&#21709;&#31572;&#26696;&#65292;&#32467;&#26524;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20808;&#39564;&#26399;&#26395;&#33021;&#22815;&#25429;&#25417;&#21040;&#36825;&#31181;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2207.07051</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26174;&#31034;&#23545;&#25512;&#29702;&#20219;&#21153;&#20855;&#26377;&#31867;&#20284;&#20154;&#31867;&#30340;&#20869;&#23481;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Language models show human-like content effects on reasoning tasks. (arXiv:2207.07051v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.07051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#20013;&#26159;&#21542;&#20687;&#20154;&#31867;&#19968;&#26679;&#36890;&#36807;&#28151;&#20837;&#20869;&#23481;&#26469;&#24433;&#21709;&#31572;&#26696;&#65292;&#32467;&#26524;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20808;&#39564;&#26399;&#26395;&#33021;&#22815;&#25429;&#25417;&#21040;&#36825;&#31181;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25277;&#35937;&#25512;&#29702;&#26159;&#26234;&#33021;&#31995;&#32479;&#30340;&#20851;&#38190;&#33021;&#21147;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25277;&#35937;&#25512;&#29702;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#39640;&#20110;&#38543;&#26426;&#30340;&#24615;&#33021;&#65292;&#20294;&#23384;&#22312;&#35768;&#22810;&#19981;&#23436;&#21892;&#20043;&#22788;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#30340;&#25277;&#35937;&#25512;&#29702;&#20063;&#26159;&#19981;&#23436;&#32654;&#30340;&#12290;&#20363;&#22914;&#65292;&#20154;&#31867;&#25512;&#29702;&#21463;&#21040;&#25105;&#20204;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#30693;&#35782;&#21644;&#20449;&#24565;&#30340;&#24433;&#21709;&#65292;&#24182;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#8220;&#20869;&#23481;&#25928;&#24212;&#8221;&#65307;&#24403;&#38382;&#39064;&#30340;&#35821;&#20041;&#20869;&#23481;&#25903;&#25345;&#27491;&#30830;&#30340;&#36923;&#36753;&#25512;&#29702;&#26102;&#65292;&#20154;&#31867;&#26356;&#21487;&#38752;&#22320;&#36827;&#34892;&#25512;&#29702;&#12290;&#36825;&#20123;&#20869;&#23481;&#32416;&#32544;&#30340;&#25512;&#29702;&#27169;&#24335;&#22312;&#20851;&#20110;&#20154;&#31867;&#26234;&#33021;&#22522;&#26412;&#24615;&#36136;&#30340;&#20105;&#35770;&#20013;&#36215;&#30528;&#26680;&#24515;&#20316;&#29992;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20197;&#31867;&#20284;&#30340;&#26041;&#24335;&#28151;&#20837;&#20869;&#23481;&#26469;&#22238;&#31572;&#36923;&#36753;&#38382;&#39064;&#65292;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#30340;&#20808;&#39564;&#26399;&#26395;&#25429;&#25417;&#20102;&#19968;&#20123;&#20154;&#31867;&#30693;&#35782;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#25506;&#32034;&#20102;&#36825;&#20010;&#38382;&#39064;&#65306;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#12289;&#21028;&#26029;&#19977;&#27573;&#35770;&#30340;&#36923;&#36753;&#26377;&#25928;&#24615;&#21644;Wason&#36873;&#25321;&#20219;&#21153;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Abstract reasoning is a key ability for an intelligent system. Large language models (LMs) achieve above-chance performance on abstract reasoning tasks, but exhibit many imperfections. However, human abstract reasoning is also imperfect. For example, human reasoning is affected by our real-world knowledge and beliefs, and shows notable "content effects"; humans reason more reliably when the semantic content of a problem supports the correct logical inferences. These content-entangled reasoning patterns play a central role in debates about the fundamental nature of human intelligence. Here, we investigate whether language models $\unicode{x2014}$ whose prior expectations capture some aspects of human knowledge $\unicode{x2014}$ similarly mix content into their answers to logical problems. We explored this question across three logical reasoning tasks: natural language inference, judging the logical validity of syllogisms, and the Wason selection task. We evaluate state of the art large 
&lt;/p&gt;</description></item></channel></rss>