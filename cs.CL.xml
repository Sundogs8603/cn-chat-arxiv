<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;SAM&#20010;&#24615;&#21270;&#26041;&#27861;PerSAM&#65292;&#21482;&#38656;&#35201;&#19968;&#24352;&#24102;&#26377;&#21442;&#32771;&#25513;&#27169;&#30340;&#21333;&#24352;&#22270;&#20687;&#21363;&#21487;&#23450;&#20301;&#21644;&#20998;&#21106;&#30446;&#26631;&#27010;&#24565;&#65292;&#36824;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;&#19968;&#27425;&#24615;&#24494;&#35843;&#21464;&#20307;PerSAM-F&#65292;&#26088;&#22312;&#35299;&#20915;&#25513;&#27169;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.03048</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#19968;&#27425;&#24615;&#20998;&#21106;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Personalize Segment Anything Model with One Shot. (arXiv:2305.03048v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;SAM&#20010;&#24615;&#21270;&#26041;&#27861;PerSAM&#65292;&#21482;&#38656;&#35201;&#19968;&#24352;&#24102;&#26377;&#21442;&#32771;&#25513;&#27169;&#30340;&#21333;&#24352;&#22270;&#20687;&#21363;&#21487;&#23450;&#20301;&#21644;&#20998;&#21106;&#30446;&#26631;&#27010;&#24565;&#65292;&#36824;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;&#19968;&#27425;&#24615;&#24494;&#35843;&#21464;&#20307;PerSAM-F&#65292;&#26088;&#22312;&#35299;&#20915;&#25513;&#27169;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#25968;&#25454;&#39044;&#35757;&#32451;&#30340;&#25512;&#21160;&#19979;&#65292;&#20998;&#21106;&#20219;&#20309;&#29289;&#20307;&#27169;&#22411;&#65288;SAM&#65289;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#20010;&#24378;&#22823;&#19988;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#38761;&#26032;&#20102;&#20998;&#21106;&#27169;&#22411;&#39046;&#22495;&#12290;&#23613;&#31649;SAM&#38750;&#24120;&#36890;&#29992;&#65292;&#20294;&#33258;&#21160;&#20026;&#29305;&#23450;&#35270;&#35273;&#27010;&#24565;&#23450;&#21046;SAM&#32780;&#19981;&#38656;&#35201;&#25163;&#21160;&#25552;&#31034;&#65292;&#22914;&#22312;&#19981;&#21516;&#22270;&#20687;&#20013;&#33258;&#21160;&#20998;&#21106;&#20320;&#30340;&#23456;&#29289;&#29399;&#31561;&#65292; &#36824;&#26410;&#28145;&#20837;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;SAM&#20010;&#24615;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;PerSAM&#12290;&#21482;&#38656;&#35201;&#19968;&#24352;&#24102;&#26377;&#21442;&#32771;&#25513;&#27169;&#30340;&#21333;&#24352;&#22270;&#20687;&#65292;PerSAM&#39318;&#20808;&#36890;&#36807;&#20301;&#32622;&#20808;&#39564;&#23450;&#20301;&#30446;&#26631;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#19977;&#31181;&#25216;&#26415;&#26469;&#22312;&#20854;&#20182;&#22270;&#20687;&#25110;&#35270;&#39057;&#20013;&#20998;&#21106;&#23427;&#65306;&#30446;&#26631;&#24341;&#23548;&#27880;&#24847;&#21147;&#65292;&#30446;&#26631;&#35821;&#20041;&#25552;&#31034;&#21644;&#32423;&#32852;&#21518;&#22788;&#29702;&#12290;&#36825;&#26679;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#36866;&#24212;&#20102;SAM&#30340;&#31169;&#20154;&#20351;&#29992;&#32780;&#26080;&#38656;&#20219;&#20309;&#35757;&#32451;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#32531;&#35299;&#25513;&#27169;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#19968;&#27425;&#24615;&#24494;&#35843;&#21464;&#20307;&#65292;&#21363;PerSAM-F&#12290;&#20923;&#32467;&#25972;&#20010;SAM&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#21487;&#23398;&#20064;&#26435;&#37325;&#29992;&#20110;&#22810;&#23610;&#24230;&#25513;&#27169;&#65292;&#20165;&#35757;&#32451;2&#20010;&#21442;&#25968;&#21363;&#21487;&#12290;
&lt;/p&gt;
&lt;p&gt;
Driven by large-data pre-training, Segment Anything Model (SAM) has been demonstrated as a powerful and promptable framework, revolutionizing the segmentation models. Despite the generality, customizing SAM for specific visual concepts without man-powered prompting is under explored, e.g., automatically segmenting your pet dog in different images. In this paper, we propose a training-free Personalization approach for SAM, termed as PerSAM. Given only a single image with a reference mask, PerSAM first localizes the target concept by a location prior, and segments it within other images or videos via three techniques: target-guided attention, target-semantic prompting, and cascaded post-refinement. In this way, we effectively adapt SAM for private use without any training. To further alleviate the mask ambiguity, we present an efficient one-shot fine-tuning variant, PerSAM-F. Freezing the entire SAM, we introduce two learnable weights for multi-scale masks, only training 2 parameters wit
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;SELF-ALIGN&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;&#21407;&#21017;&#30340;&#25512;&#29702;&#21644;LLMs&#30340;&#29983;&#25104;&#33021;&#21147;&#20197;&#26368;&#23569;&#30340;&#20154;&#31867;&#30417;&#30563;&#23454;&#29616;AI&#20195;&#29702;&#30340;&#33258;&#25105;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2305.03047</link><description>&lt;p&gt;
&#21407;&#21017;&#39537;&#21160;&#33258;&#25105;&#23545;&#40784;&#30340;&#26368;&#23567;&#20154;&#21147;&#30417;&#30563;&#30340;&#35821;&#35328;&#27169;&#22411;&#20174;&#38646;&#24320;&#22987;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision. (arXiv:2305.03047v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03047
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;SELF-ALIGN&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;&#21407;&#21017;&#30340;&#25512;&#29702;&#21644;LLMs&#30340;&#29983;&#25104;&#33021;&#21147;&#20197;&#26368;&#23569;&#30340;&#20154;&#31867;&#30417;&#30563;&#23454;&#29616;AI&#20195;&#29702;&#30340;&#33258;&#25105;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;AI&#21161;&#25163;&#20195;&#29702;&#65292;&#22914;ChatGPT&#65292;&#20027;&#35201;&#20381;&#36182;&#20110;&#30417;&#30563;&#24494;&#35843;&#21644;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26469;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#19982;&#20154;&#31867;&#24847;&#22270;&#65292;&#30830;&#20445;&#23427;&#20204;&#26159;&#26377;&#29992;&#30340;&#12289;&#36947;&#24503;&#30340;&#12289;&#21487;&#38752;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20381;&#36182;&#24615;&#21487;&#33021;&#20250;&#26497;&#22823;&#22320;&#38480;&#21046;AI&#21161;&#25163;&#20195;&#29702;&#30340;&#30495;&#27491;&#28508;&#21147;&#65292;&#22240;&#20026;&#33719;&#24471;&#20154;&#31867;&#30417;&#30563;&#30340;&#25104;&#26412;&#24456;&#39640;&#65292;&#30456;&#20851;&#38382;&#39064;&#26377;&#36136;&#37327;&#12289;&#21487;&#38752;&#24615;&#12289;&#22810;&#26679;&#24615;&#12289;&#33258;&#19968;&#33268;&#24615;&#21644;&#19981;&#33391;&#20559;&#35265;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861; SELF-ALIGN&#65292;&#23427;&#32467;&#21512;&#20102;&#22522;&#20110;&#21407;&#21017;&#30340;&#25512;&#29702;&#21644;LLMs&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#20197;&#26368;&#23569;&#30340;&#20154;&#31867;&#30417;&#30563;&#23454;&#29616;AI&#20195;&#29702;&#30340;&#33258;&#25105;&#23545;&#40784;&#12290;&#26041;&#27861;&#21253;&#25324;&#22235;&#20010;&#38454;&#27573;&#65306;&#31532;&#19968;&#65292;&#25105;&#20204;&#20351;&#29992;LLM&#29983;&#25104;&#21512;&#25104;&#25552;&#31034;&#65292;&#20351;&#29992;&#20027;&#39064;&#24341;&#23548;&#26041;&#27861;&#22686;&#21152;&#25552;&#31034;&#22810;&#26679;&#24615;&#65307;&#31532;&#20108;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#23567;&#32452;&#20154;&#24037;&#32534;&#20889;&#30340;AI&#27169;&#22411;&#21407;&#21017;&#65292;&#24182;&#25351;&#23548;AI&#27169;&#22411;&#36981;&#24490;&#65307;
&lt;/p&gt;
&lt;p&gt;
Recent AI-assistant agents, such as ChatGPT, predominantly rely on supervised fine-tuning (SFT) with human annotations and reinforcement learning from human feedback (RLHF) to align the output of large language models (LLMs) with human intentions, ensuring they are helpful, ethical, and reliable. However, this dependence can significantly constrain the true potential of AI-assistant agents due to the high cost of obtaining human supervision and the related issues on quality, reliability, diversity, self-consistency, and undesirable biases. To address these challenges, we propose a novel approach called SELF-ALIGN, which combines principle-driven reasoning and the generative power of LLMs for the self-alignment of AI agents with minimal human supervision. Our approach encompasses four stages: first, we use an LLM to generate synthetic prompts, and a topic-guided method to augment the prompt diversity; second, we use a small set of human-written principles for AI models to follow, and gu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#20010;&#38543;&#26426;BPE&#21464;&#20307;&#65292;&#22312;&#32763;&#35793;&#21040;&#24418;&#24577;&#20016;&#23500;&#30340;&#35821;&#35328;&#26102;&#65292;&#38543;&#26426;&#36873;&#25321;&#21512;&#24182;&#25805;&#20316;&#23545;&#19979;&#28216;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#24433;&#21709;&#24456;&#23567;&#12290;&#26631;&#20934;BPE&#34429;&#28982;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#23384;&#22312;&#30528;&#26377;&#36259;&#30340;&#28508;&#22312;&#21464;&#21270;&#23431;&#23449;&#20540;&#24471;&#25506;&#31350;&#12290;</title><link>http://arxiv.org/abs/2305.03029</link><description>&lt;p&gt;
&#38543;&#26426;&#36873;&#25321;BPE&#21512;&#24182;&#25805;&#20316;&#20250;&#24102;&#26469;&#20160;&#20040;&#21464;&#21270;&#65311;&#19981;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
What changes when you randomly choose BPE merge operations? Not much. (arXiv:2305.03029v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03029
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#20010;&#38543;&#26426;BPE&#21464;&#20307;&#65292;&#22312;&#32763;&#35793;&#21040;&#24418;&#24577;&#20016;&#23500;&#30340;&#35821;&#35328;&#26102;&#65292;&#38543;&#26426;&#36873;&#25321;&#21512;&#24182;&#25805;&#20316;&#23545;&#19979;&#28216;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#24433;&#21709;&#24456;&#23567;&#12290;&#26631;&#20934;BPE&#34429;&#28982;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#23384;&#22312;&#30528;&#26377;&#36259;&#30340;&#28508;&#22312;&#21464;&#21270;&#23431;&#23449;&#20540;&#24471;&#25506;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19977;&#20010;&#31616;&#21333;&#30340;&#38543;&#26426;BPE&#21464;&#20307;&#65292;&#24182;&#25506;&#35752;&#20102;&#38543;&#26426;&#36873;&#25321;&#21512;&#24182;&#25805;&#20316;&#26159;&#21542;&#20250;&#23545;&#19979;&#28216;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20135;&#29983;&#23454;&#36136;&#24615;&#24433;&#21709;&#12290;&#25105;&#20204;&#30528;&#37325;&#32763;&#35793;&#21040;&#24418;&#24577;&#20016;&#23500;&#30340;&#35821;&#35328;&#65292;&#20551;&#35774;&#36825;&#20010;&#20219;&#21153;&#21487;&#33021;&#23545;&#36873;&#25321;&#23376;&#35789;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#25935;&#24863;&#24615;&#12290;&#20351;&#29992;&#36125;&#21494;&#26031;&#32447;&#24615;&#27169;&#22411;&#36827;&#34892;&#20998;&#26512;&#34920;&#26126;&#65292;&#20854;&#20013;&#20004;&#20010;&#21464;&#20307;&#19982;&#26631;&#20934;BPE&#30456;&#27604;&#34920;&#29616;&#20960;&#20046;&#26080;&#27861;&#21306;&#20998;&#65292;&#32780;&#21478;&#19968;&#20010;&#21464;&#20307;&#30340;&#24615;&#33021;&#19979;&#38477;&#31243;&#24230;&#27604;&#25105;&#20204;&#39044;&#26399;&#30340;&#35201;&#23567;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#23613;&#31649;&#26631;&#20934;BPE&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#23384;&#22312;&#19968;&#20010;&#26377;&#36259;&#30340;&#28508;&#22312;&#21464;&#21270;&#23431;&#23449;&#20540;&#24471;&#25506;&#31350;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;&#20197;&#19979;&#32593;&#22336;&#25214;&#21040;&#65306;https://github.com/bltlab/random-bpe&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce three simple randomized variants of byte pair encoding (BPE) and explore whether randomizing the selection of merge operations substantially affects a downstream machine translation task. We focus on translation into morphologically rich languages, hypothesizing that this task may show sensitivity to the method of choosing subwords. Analysis using a Bayesian linear model indicates that two of the variants perform nearly indistinguishably compared to standard BPE while the other degrades performance less than we anticipated. We conclude that although standard BPE is widely used, there exists an interesting universe of potential variations on it worth investigating. Our code is available at: https://github.com/bltlab/random-bpe.
&lt;/p&gt;</description></item><item><title>&#35813;&#39033;&#30446;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#25351;&#20196;&#35843;&#25972;&#26469;&#25552;&#21319;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25506;&#35752;&#20102;&#35757;&#32451;&#25968;&#25454;&#30340;&#22240;&#32032;&#23545;&#25351;&#20196;&#35843;&#25972;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#37327;&#21270;&#20998;&#26512;&#26469;&#20026;&#32842;&#22825;&#27169;&#22411;&#30340;&#25345;&#32493;&#21457;&#23637;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#27934;&#23519;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.03025</link><description>&lt;p&gt;
Panda LLM&#65306;&#35757;&#32451;&#25968;&#25454;&#21644;&#35780;&#20272;&#38024;&#23545;&#24320;&#28304;&#27721;&#35821;&#25351;&#20196;&#36319;&#38543;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Panda LLM: Training Data and Evaluation for Open-Sourced Chinese Instruction-Following Large Language Models. (arXiv:2305.03025v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03025
&lt;/p&gt;
&lt;p&gt;
&#35813;&#39033;&#30446;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#25351;&#20196;&#35843;&#25972;&#26469;&#25552;&#21319;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25506;&#35752;&#20102;&#35757;&#32451;&#25968;&#25454;&#30340;&#22240;&#32032;&#23545;&#25351;&#20196;&#35843;&#25972;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#37327;&#21270;&#20998;&#26512;&#26469;&#20026;&#32842;&#22825;&#27169;&#22411;&#30340;&#25345;&#32493;&#21457;&#23637;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#27934;&#23519;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#39033;&#30446;&#30528;&#37325;&#20110;&#36890;&#36807;&#25351;&#20196;&#35843;&#25972;&#26469;&#22686;&#24378;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#23545;&#20854;&#24615;&#33021;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#21508;&#31181;&#35757;&#32451;&#25968;&#25454;&#22240;&#32032;&#65292;&#22914;&#25968;&#37327;&#12289;&#36136;&#37327;&#21644;&#35821;&#35328;&#20998;&#24067;&#65292;&#23545;&#22312;&#20844;&#24320;&#39640;&#36136;&#37327;&#25351;&#20196;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#20013;&#33521;&#21452;&#35821;&#25351;&#20196;&#35843;&#25972;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#23450;&#37327;&#20998;&#26512;&#26469;&#34917;&#20805;&#35780;&#20272;&#65292;&#20026;&#24320;&#28304;&#32842;&#22825;&#27169;&#22411;&#30340;&#25345;&#32493;&#21457;&#23637;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#27934;&#23519;&#21147;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#12289;&#25968;&#25454;&#21644;&#20195;&#30721;&#37117;&#26159;&#20844;&#24320;&#30340;&#65292;&#20379;&#20854;&#20182;&#20154;&#20351;&#29992;&#21644;&#24314;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;
This project focuses on enhancing open-source large language models through instruction-tuning and providing comprehensive evaluations of their performance. We explore how various training data factors, such as quantity, quality, and linguistic distribution, influence the performance of instruction-tuned models trained on publicly accessible high-quality instruction datasets for both English and Chinese languages. Our goal is to supplement evaluation with quantitative analyses, providing valuable insights for the continued advancement of open-source chat models. Our model, data, and code are publicly available for others to use and build upon.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#24335;&#23884;&#20837;&#36870;&#21521;&#25915;&#20987;&#26041;&#27861;&#65292;&#20351;&#29992;&#35813;&#26041;&#27861;&#21487;&#20197;&#21482;&#22522;&#20110;&#21477;&#23376;&#23884;&#20837;&#26469;&#37325;&#26500;&#36755;&#20837;&#24207;&#21015;&#65292;&#20174;&#32780;&#23454;&#29616;&#20449;&#24687;&#27844;&#38706;&#30340;&#25915;&#20987;&#12290;&#26412;&#26041;&#27861;&#22312;&#20998;&#31867;&#25351;&#26631;&#19978;&#20248;&#20110;&#20808;&#21069;&#30340;&#23884;&#20837;&#36870;&#21521;&#25915;&#20987;&#65292;&#29983;&#25104;&#30340;&#21477;&#23376;&#36830;&#36143;&#19988;&#19978;&#19979;&#25991;&#30456;&#20851;&#12290;&#21516;&#26102;&#65292;&#35813;&#30740;&#31350;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#38382;&#39064;&#25552;&#20986;&#20102;&#35686;&#37266;&#65292;&#24182;&#21628;&#21505;&#36827;&#19968;&#27493;&#30740;&#31350;&#24320;&#21457;&#38024;&#23545;&#27492;&#31867;&#25915;&#20987;&#30340;&#38450;&#24481;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.03010</link><description>&lt;p&gt;
&#21477;&#23376;&#23884;&#20837;&#27844;&#38706;&#30340;&#20449;&#24687;&#27604;&#24744;&#24819;&#35937;&#30340;&#35201;&#22810;&#65306;&#29983;&#25104;&#24335;&#23884;&#20837;&#36870;&#21521;&#25915;&#20987;&#29992;&#20110;&#24674;&#22797;&#25972;&#20010;&#21477;&#23376;
&lt;/p&gt;
&lt;p&gt;
Sentence Embedding Leaks More Information than You Expect: Generative Embedding Inversion Attack to Recover the Whole Sentence. (arXiv:2305.03010v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#24335;&#23884;&#20837;&#36870;&#21521;&#25915;&#20987;&#26041;&#27861;&#65292;&#20351;&#29992;&#35813;&#26041;&#27861;&#21487;&#20197;&#21482;&#22522;&#20110;&#21477;&#23376;&#23884;&#20837;&#26469;&#37325;&#26500;&#36755;&#20837;&#24207;&#21015;&#65292;&#20174;&#32780;&#23454;&#29616;&#20449;&#24687;&#27844;&#38706;&#30340;&#25915;&#20987;&#12290;&#26412;&#26041;&#27861;&#22312;&#20998;&#31867;&#25351;&#26631;&#19978;&#20248;&#20110;&#20808;&#21069;&#30340;&#23884;&#20837;&#36870;&#21521;&#25915;&#20987;&#65292;&#29983;&#25104;&#30340;&#21477;&#23376;&#36830;&#36143;&#19988;&#19978;&#19979;&#25991;&#30456;&#20851;&#12290;&#21516;&#26102;&#65292;&#35813;&#30740;&#31350;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#38382;&#39064;&#25552;&#20986;&#20102;&#35686;&#37266;&#65292;&#24182;&#21628;&#21505;&#36827;&#19968;&#27493;&#30740;&#31350;&#24320;&#21457;&#38024;&#23545;&#27492;&#31867;&#25915;&#20987;&#30340;&#38450;&#24481;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21477;&#23376;&#32423;&#21035;&#30340;&#34920;&#31034;&#23545;&#20110;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#37117;&#26377;&#30410;&#22788;&#12290;&#20154;&#20204;&#26222;&#36941;&#35748;&#20026;&#21521;&#37327;&#34920;&#31034;&#21487;&#20197;&#25429;&#25417;&#21040;&#20016;&#23500;&#30340;&#35821;&#35328;&#23646;&#24615;&#12290;&#30446;&#21069;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LMs)&#22312;&#21477;&#23376;&#23884;&#20837;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#26368;&#26032;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20174;LMs&#20013;&#33719;&#24471;&#30340;&#21521;&#37327;&#34920;&#31034;&#21487;&#33021;&#20250;&#23548;&#33268;&#20449;&#24687;&#27844;&#38706;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#20449;&#24687;&#27844;&#38706;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#24335;&#23884;&#20837;&#36870;&#21521;&#25915;&#20987;(GEIA)&#65292;&#26088;&#22312;&#20165;&#22522;&#20110;&#20854;&#21477;&#23376;&#23884;&#20837;&#26469;&#37325;&#26500;&#36755;&#20837;&#24207;&#21015;&#12290;&#37492;&#20110;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#40657;&#30418;&#35775;&#38382;&#65292;&#25105;&#20204;&#23558;&#21477;&#23376;&#23884;&#20837;&#35270;&#20026;&#21021;&#22987;&#26631;&#35760;&#30340;&#34920;&#31034;&#65292;&#24182;&#35757;&#32451;&#25110;&#24494;&#35843;&#19968;&#20010;&#24378;&#22823;&#30340;&#35299;&#30721;&#22120;&#27169;&#22411;&#30452;&#25509;&#35299;&#30721;&#25972;&#20010;&#24207;&#21015;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#25105;&#20204;&#30340;&#29983;&#25104;&#36870;&#21521;&#25915;&#20987;&#22312;&#20998;&#31867;&#25351;&#26631;&#19978;&#20248;&#20110;&#20808;&#21069;&#30340;&#23884;&#20837;&#36870;&#21521;&#25915;&#20987;&#65292;&#24182;&#29983;&#25104;&#36830;&#36143;&#19988;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#21477;&#23376;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#21066;&#20943;&#30740;&#31350;&#65292;&#20197;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#25915;&#20987;&#26159;&#26377;&#25928;&#19988;&#31283;&#20581;&#30340;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#31361;&#26174;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#38544;&#31169;&#21644;&#23433;&#20840;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#21628;&#21505;&#36827;&#19968;&#27493;&#30740;&#31350;&#24320;&#21457;&#38024;&#23545;&#27492;&#31867;&#25915;&#20987;&#30340;&#38450;&#24481;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sentence-level representations are beneficial for various natural language processing tasks. It is commonly believed that vector representations can capture rich linguistic properties. Currently, large language models (LMs) achieve state-of-the-art performance on sentence embedding. However, some recent works suggest that vector representations from LMs can cause information leakage. In this work, we further investigate the information leakage issue and propose a generative embedding inversion attack (GEIA) that aims to reconstruct input sequences based only on their sentence embeddings. Given the black-box access to a language model, we treat sentence embeddings as initial tokens' representations and train or fine-tune a powerful decoder model to decode the whole sequences directly. We conduct extensive experiments to demonstrate that our generative inversion attack outperforms previous embedding inversion attacks in classification metrics and generates coherent and contextually simil
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;NatCS&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#39046;&#22495;&#21475;&#22836;&#23458;&#25143;&#26381;&#21153;&#23545;&#35805;&#38598;&#12290;&#30456;&#23545;&#20110;&#20808;&#21069;&#30340;&#25968;&#25454;&#38598;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#29616;&#35937;&#65292;NatCS&#26356;&#33021;&#20195;&#34920;&#30495;&#23454;&#20154;&#38469;&#23545;&#35805;&#65292;&#24182;&#25552;&#20379;&#27604;&#29616;&#26377;&#25968;&#25454;&#38598;&#26356;&#20026;&#29616;&#23454;&#30340;&#22522;&#20934;&#20379;&#38754;&#21521;&#33258;&#28982;&#23458;&#25143;&#25903;&#25345;&#23545;&#35805;&#31995;&#32479;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.03007</link><description>&lt;p&gt;
NatCS: &#24341;&#21457;&#33258;&#28982;&#23458;&#26381;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
NatCS: Eliciting Natural Customer Support Dialogues. (arXiv:2305.03007v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03007
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;NatCS&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#39046;&#22495;&#21475;&#22836;&#23458;&#25143;&#26381;&#21153;&#23545;&#35805;&#38598;&#12290;&#30456;&#23545;&#20110;&#20808;&#21069;&#30340;&#25968;&#25454;&#38598;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#29616;&#35937;&#65292;NatCS&#26356;&#33021;&#20195;&#34920;&#30495;&#23454;&#20154;&#38469;&#23545;&#35805;&#65292;&#24182;&#25552;&#20379;&#27604;&#29616;&#26377;&#25968;&#25454;&#38598;&#26356;&#20026;&#29616;&#23454;&#30340;&#22522;&#20934;&#20379;&#38754;&#21521;&#33258;&#28982;&#23458;&#25143;&#25903;&#25345;&#23545;&#35805;&#31995;&#32479;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36234;&#26469;&#36234;&#22810;&#30340;&#24212;&#29992;&#31243;&#24207;&#22522;&#20110;&#33258;&#28982;&#30340;&#23458;&#25143;&#25903;&#25345;&#23545;&#35805;&#65292;&#20294;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#26497;&#23569;&#21453;&#26144;&#20986;&#36825;&#20123;&#29615;&#22659;&#20013;&#23545;&#35805;&#30340;&#39044;&#26399;&#29305;&#24449;&#12290;&#29616;&#26377;&#30340;&#38754;&#21521;&#20219;&#21153;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#20027;&#35201;&#25910;&#38598;&#20026;&#22522;&#20934;&#23545;&#35805;&#31995;&#32479;&#65292;&#20027;&#35201;&#26159;&#22312;&#20154;&#19982;&#26426;&#22120;&#20154;&#20043;&#38388;&#30340;&#20070;&#38754;&#23545;&#35805;&#29615;&#22659;&#20013;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#24182;&#19981;&#20195;&#34920;&#30495;&#23454;&#30340;&#23458;&#25143;&#25903;&#25345;&#23545;&#35805;&#65292;&#20063;&#19981;&#33021;&#25552;&#20379;&#24212;&#29992;&#20110;&#33258;&#28982;&#25968;&#25454;&#30340;&#31995;&#32479;&#30340;&#29616;&#23454;&#22522;&#20934;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;NatCS&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#39046;&#22495;&#30340;&#21475;&#35821;&#23458;&#25143;&#26381;&#21153;&#23545;&#35805;&#38598;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#25105;&#20204;&#37319;&#38598;&#20154;&#24037;&#21512;&#25104;&#23545;&#35805;&#30340;&#36807;&#31243;&#65292;&#36825;&#20123;&#23545;&#35805;&#26159;&#22522;&#20110;&#35266;&#23519;&#21040;&#30340;&#30495;&#23454;&#23545;&#35805;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#29616;&#35937;&#12290;&#19982;&#20808;&#21069;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;&#25105;&#20204;&#37319;&#29992;&#36825;&#31181;&#26041;&#27861;&#25910;&#38598;&#30340;&#23545;&#35805;&#22312;&#22810;&#20010;&#25351;&#26631;&#19978;&#26356;&#33021;&#20195;&#34920;&#30495;&#23454;&#30340;&#20154;&#38469;&#23545;&#35805;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;NatCS&#30340;&#28508;&#22312;&#29992;&#36884;&#65292;&#21253;&#25324;...
&lt;/p&gt;
&lt;p&gt;
Despite growing interest in applications based on natural customer support conversations, there exist remarkably few publicly available datasets that reflect the expected characteristics of conversations in these settings. Existing task-oriented dialogue datasets, which were collected to benchmark dialogue systems mainly in written human-to-bot settings, are not representative of real customer support conversations and do not provide realistic benchmarks for systems that are applied to natural data. To address this gap, we introduce NatCS, a multi-domain collection of spoken customer service conversations. We describe our process for collecting synthetic conversations between customers and agents based on natural language phenomena observed in real conversations. Compared to previous dialogue datasets, the conversations collected with our approach are more representative of real human-to-human conversations along multiple metrics. Finally, we demonstrate potential uses of NatCS, includ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#38170;&#28857;&#36873;&#25321;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#36739;&#23567;&#30340;&#35745;&#31639;&#25104;&#26412;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#19982;&#38543;&#26426;&#25277;&#26679;&#38170;&#28857;&#30456;&#24403;&#25110;&#32773;&#26356;&#22909;&#30340;k-NN&#21484;&#22238;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.02996</link><description>&lt;p&gt;
&#24102;&#26377;&#20132;&#21449;&#32534;&#30721;&#22120;&#30340;CUR k-NN&#25628;&#32034;&#30340;&#33258;&#36866;&#24212;&#38170;&#23450;&#39033;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Adaptive Selection of Anchor Items for CUR-based k-NN search with Cross-Encoders. (arXiv:2305.02996v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#38170;&#28857;&#36873;&#25321;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#36739;&#23567;&#30340;&#35745;&#31639;&#25104;&#26412;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#19982;&#38543;&#26426;&#25277;&#26679;&#38170;&#28857;&#30456;&#24403;&#25110;&#32773;&#26356;&#22909;&#30340;k-NN&#21484;&#22238;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#38170;&#28857;&#36873;&#25321;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;ANNCUR&#27169;&#22411;&#20013;&#39640;&#21069;k&#39033;&#30340;&#36924;&#36817;&#35823;&#24046;&#21644;&#21484;&#22238;&#29575;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20445;&#25345;&#36739;&#23567;&#30340;&#35745;&#31639;&#25104;&#26412;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#19982;&#38543;&#26426;&#25277;&#26679;&#38170;&#28857;&#30456;&#24403;&#25110;&#32773;&#26356;&#22909;&#30340;k-NN&#21484;&#22238;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-encoder models, which jointly encode and score a query-item pair, are typically prohibitively expensive for k-nearest neighbor search. Consequently, k-NN search is performed not with a cross-encoder, but with a heuristic retrieve (e.g., using BM25 or dual-encoder) and re-rank approach. Recent work proposes ANNCUR (Yadav et al., 2022) which uses CUR matrix factorization to produce an embedding space for efficient vector-based search that directly approximates the cross-encoder without the need for dual-encoders. ANNCUR defines this shared query-item embedding space by scoring the test query against anchor items which are sampled uniformly at random. While this minimizes average approximation error over all items, unsuitably high approximation error on top-k items remains and leads to poor recall of top-k (and especially top-1) items. Increasing the number of anchor items is a straightforward way of improving the approximation error and hence k-NN recall of ANNCUR but at the cost o
&lt;/p&gt;</description></item><item><title>&#22312;&#19981;&#21516;&#25968;&#25454;&#23376;&#32676;&#20307;&#38388;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20869;&#37096;&#20934;&#30830;&#24615;&#21644;&#22806;&#37096;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26159;&#38750;&#32447;&#24615;&#30340;&#65292;&#21576;&#29616;&#20986;&#8220;&#26376;&#20142;&#24418;&#8221;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.02995</link><description>&lt;p&gt;
&#20851;&#20110;&#25968;&#25454;&#23376;&#32676;&#20307;&#38388;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#30340;&#38750;&#32447;&#24615;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the nonlinear correlation of ML performance between data subpopulations. (arXiv:2305.02995v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02995
&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#21516;&#25968;&#25454;&#23376;&#32676;&#20307;&#38388;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20869;&#37096;&#20934;&#30830;&#24615;&#21644;&#22806;&#37096;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26159;&#38750;&#32447;&#24615;&#30340;&#65292;&#21576;&#29616;&#20986;&#8220;&#26376;&#20142;&#24418;&#8221;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#19979;&#30340;&#24615;&#33021;&#23545;&#20110;&#21487;&#38752;&#30340;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#26368;&#26032;&#30340;&#32463;&#39564;&#30740;&#31350;&#35748;&#20026;&#35757;&#32451;&#25968;&#25454;&#20869;&#37096;&#30340;&#20934;&#30830;&#24615;&#21644;&#26032;&#25968;&#25454;&#22806;&#37096;&#30340;&#20934;&#30830;&#24615;&#20043;&#38388;&#23384;&#22312;&#36817;&#20046;&#23436;&#32654;&#30340;&#32447;&#24615;&#30456;&#20851;&#24615;&#65292;&#20294;&#25105;&#20204;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#35757;&#32451;&#26102;&#26399;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#23454;&#39564;&#21644;&#20998;&#26512;&#65292;&#21457;&#29616;&#22312;&#23376;&#32676;&#20307;&#36716;&#31227;&#19979;&#65292;&#20869;&#37096;&#20934;&#30830;&#24615;&#21644;&#22806;&#37096;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26356;&#20026;&#24494;&#22937;&#65292;&#24182;&#19988;&#22312;&#19978;&#21319;&#38454;&#27573;&#23384;&#22312;&#8220;&#26376;&#20142;&#24418;&#8221;&#30340;&#30456;&#20851;&#24615;&#65288;&#25243;&#29289;&#32447;&#19978;&#21319;&#26354;&#32447;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the performance of machine learning (ML) models across diverse data distributions is critically important for reliable applications. Despite recent empirical studies positing a near-perfect linear correlation between in-distribution (ID) and out-of-distribution (OOD) accuracies, we empirically demonstrate that this correlation is more nuanced under subpopulation shifts. Through rigorous experimentation and analysis across a variety of datasets, models, and training epochs, we demonstrate that OOD performance often has a nonlinear correlation with ID performance in subpopulation shifts. Our findings, which contrast previous studies that have posited a linear correlation in model performance during distribution shifts, reveal a "moon shape" correlation (parabolic uptrend curve) between the test performance on the majority subpopulation and the minority subpopulation. This non-trivial nonlinear correlation holds across model architectures, hyperparameters, training durations
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;SemEval 2023&#30340;&#20219;&#21153;&#19971;&#65292;&#26088;&#22312;&#36827;&#34892;&#20020;&#24202;&#35797;&#39564;&#25968;&#25454;&#30340;&#22810;&#35777;&#25454;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65292;&#35813;&#20219;&#21153;&#38590;&#24230;&#36739;&#22823;&#65292;&#35777;&#25454;&#36873;&#25321;&#20219;&#21153;&#30456;&#23545;&#20110;&#34164;&#21547;&#20219;&#21153;&#34920;&#29616;&#26356;&#20339;&#12290;</title><link>http://arxiv.org/abs/2305.02993</link><description>&lt;p&gt;
SemEval-2023&#20219;&#21153;7: &#20020;&#24202;&#35797;&#39564;&#25968;&#25454;&#30340;&#22810;&#35777;&#25454;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
SemEval-2023 Task 7: Multi-Evidence Natural Language Inference for Clinical Trial Data. (arXiv:2305.02993v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02993
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;SemEval 2023&#30340;&#20219;&#21153;&#19971;&#65292;&#26088;&#22312;&#36827;&#34892;&#20020;&#24202;&#35797;&#39564;&#25968;&#25454;&#30340;&#22810;&#35777;&#25454;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65292;&#35813;&#20219;&#21153;&#38590;&#24230;&#36739;&#22823;&#65292;&#35777;&#25454;&#36873;&#25321;&#20219;&#21153;&#30456;&#23545;&#20110;&#34164;&#21547;&#20219;&#21153;&#34920;&#29616;&#26356;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#20171;&#32461;SemEval 2023&#20219;&#21153;7&#30340;&#32467;&#26524;&#65292;&#35813;&#20219;&#21153;&#20027;&#35201;&#28041;&#21450;&#20020;&#24202;&#35797;&#39564;&#25968;&#25454;&#20013;&#30340;&#22810;&#35777;&#25454;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI4CT&#65289;&#65292;&#30001;&#20004;&#20010;&#23376;&#20219;&#21153;&#32452;&#25104;&#65306;&#19968;&#20010;&#26159;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#20219;&#21153;&#65292;&#21478;&#19968;&#20010;&#26159;&#35777;&#25454;&#36873;&#25321;&#20219;&#21153;&#12290;&#36825;&#20004;&#20010;&#20219;&#21153;&#38656;&#35201;&#36827;&#34892;&#21307;&#23398;&#21644;&#25968;&#23383;&#25512;&#29702;&#65292;&#36825;&#23545;&#20110;&#24320;&#21457;&#33021;&#22815;&#36827;&#34892;&#22823;&#35268;&#27169;&#21307;&#30103;&#35777;&#25454;&#35299;&#37322;&#21644;&#26816;&#32034;&#12289;&#25552;&#20379;&#20010;&#24615;&#21270;&#22522;&#20110;&#35777;&#25454;&#30340;&#20445;&#20581;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#31532;1&#20010;&#23376;&#20219;&#21153;&#8220;&#34164;&#21547;&#20219;&#21153;&#8221;&#25910;&#21040;&#20102;&#26469;&#33258;40&#20301;&#21442;&#36187;&#32773;&#30340;643&#20221;&#25552;&#20132;&#65292;&#31532;2&#20010;&#23376;&#20219;&#21153;&#8220;&#35777;&#25454;&#36873;&#25321;&#20219;&#21153;&#8221;&#25910;&#21040;&#20102;&#26469;&#33258;23&#20301;&#21442;&#36187;&#32773;&#30340;364&#20221;&#25552;&#20132;&#12290;&#36825;&#20004;&#20010;&#20219;&#21153;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22823;&#37096;&#20998;&#25552;&#20132;&#30340;&#31995;&#32479;&#22312;&#34164;&#21547;&#20219;&#21153;&#19978;&#26410;&#33021;&#26126;&#26174;&#20248;&#20110;&#22823;&#22810;&#25968;&#31867;&#22522;&#32447;&#65292;&#32780;&#25105;&#20204;&#35266;&#23519;&#21040;&#35777;&#25454;&#36873;&#25321;&#20219;&#21153;&#30340;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#34164;&#21547;&#20219;&#21153;&#12290;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#20250;&#23548;&#33268;&#27169;&#22411;&#22312;&#27979;&#35797;&#38598;&#19978;&#34920;&#29616;&#26356;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes the results of SemEval 2023 task 7 -- Multi-Evidence Natural Language Inference for Clinical Trial Data (NLI4CT) -- consisting of 2 tasks, a Natural Language Inference (NLI) task, and an evidence selection task on clinical trial data. The proposed challenges require multi-hop biomedical and numerical reasoning, which are of significant importance to the development of systems capable of large-scale interpretation and retrieval of medical evidence, to provide personalized evidence-based care.  Task 1, the entailment task, received 643 submissions from 40 participants, and Task 2, the evidence selection task, received 364 submissions from 23 participants. The tasks are challenging, with the majority of submitted systems failing to significantly outperform the majority class baseline on the entailment task, and we observe significantly better performance on the evidence selection task than on the entailment task. Increasing the number of model parameters leads to a di
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32852;&#21512;CTC&#25439;&#22833;&#21644;&#39044;&#35757;&#32451;&#22768;&#23398;&#32534;&#30721;&#22120;&#30340;&#22522;&#20110;&#31471;&#21040;&#31471;&#30340;&#21475;&#35821;&#29702;&#35299;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#36229;&#36234;SOTA&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.02937</link><description>&lt;p&gt;
&#20351;&#29992;&#32852;&#21512;CTC&#25439;&#22833;&#21644;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#22768;&#23398;&#32534;&#30721;&#22120;&#30340;&#31471;&#21040;&#31471;&#21475;&#35821;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
End-to-end spoken language understanding using joint CTC loss and self-supervised, pretrained acoustic encoders. (arXiv:2305.02937v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02937
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32852;&#21512;CTC&#25439;&#22833;&#21644;&#39044;&#35757;&#32451;&#22768;&#23398;&#32534;&#30721;&#22120;&#30340;&#22522;&#20110;&#31471;&#21040;&#31471;&#30340;&#21475;&#35821;&#29702;&#35299;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#36229;&#36234;SOTA&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21475;&#35821;&#29702;&#35299;&#20219;&#21153;&#20013;&#65292;&#30001;&#20110;&#32570;&#20047;&#25991;&#26412;&#20449;&#24687;&#65292;&#30452;&#25509;&#20174;&#22768;&#38899;&#20449;&#21495;&#20013;&#25552;&#21462;&#35821;&#20041;&#24847;&#20041;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#27969;&#34892;&#30340;&#31471;&#21040;&#31471;&#65288;E2E&#65289;&#21475;&#35821;&#29702;&#35299;&#27169;&#22411;&#21033;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#25552;&#21462;&#25991;&#26412;&#23884;&#20837;&#20316;&#20026;&#36755;&#20837;&#26469;&#25512;&#26029;&#35821;&#20041;&#65292;&#20294;&#26159;&#36825;&#38656;&#35201;&#26114;&#36149;&#30340;&#33258;&#22238;&#24402;&#35299;&#30721;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#33258;&#30417;&#30563;&#22768;&#23398;&#32534;&#30721;&#22120;&#65292;Fine-tuned Connectionist Temporal Classification&#65288;CTC&#65289;&#26469;&#25552;&#21462;&#25991;&#26412;&#23884;&#20837;&#65292;&#24182;&#20351;&#29992;&#32852;&#21512;CTC&#21644;SLU&#25439;&#22833;&#36827;&#34892;&#35805;&#35821;&#32423;&#21475;&#35821;&#29702;&#35299;&#20219;&#21153;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;DSTC2&#25968;&#25454;&#38598;&#19978;&#23545;&#35805;&#34892;&#20026;&#20998;&#31867;&#27169;&#22411;&#21462;&#24471;&#20102;4&#65285;&#30340;&#32477;&#23545;&#25913;&#36827;&#65292;&#24182;&#22312;SLURP&#25968;&#25454;&#38598;&#19978;&#36229;&#36234;&#20102;SOTA SLU&#27169;&#22411;1.3&#65285;&#30340;&#32477;&#23545;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is challenging to extract semantic meanings directly from audio signals in spoken language understanding (SLU), due to the lack of textual information. Popular end-to-end (E2E) SLU models utilize sequence-to-sequence automatic speech recognition (ASR) models to extract textual embeddings as input to infer semantics, which, however, require computationally expensive auto-regressive decoding. In this work, we leverage self-supervised acoustic encoders fine-tuned with Connectionist Temporal Classification (CTC) to extract textual embeddings and use joint CTC and SLU losses for utterance-level SLU tasks. Experiments show that our model achieves 4% absolute improvement over the the state-of-the-art (SOTA) dialogue act classification model on the DSTC2 dataset and 1.3% absolute improvement over the SOTA SLU model on the SLURP dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31995;&#21015;&#38646;&#29031;&#39038;&#25552;&#31034;&#22312;&#20845;&#20010;&#26368;&#26032;&#21457;&#24067;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#38382;&#39064;&#22238;&#31572;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#33258;&#21160;&#25552;&#31034;&#21457;&#29616;&#30340;CoT&#25552;&#31034;&#21487;&#22312;&#26032;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#22312;&#24212;&#29992;&#20110;GPT-4&#27169;&#22411;&#26102;&#21462;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.02897</link><description>&lt;p&gt;
&#33258;&#21160;&#21457;&#29616;&#30340;&#24605;&#32500;&#38142;&#25552;&#31034;&#21487;&#20197;&#25512;&#24191;&#21040;&#26032;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
An automatically discovered chain-of-thought prompt generalizes to novel models and datasets. (arXiv:2305.02897v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02897
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31995;&#21015;&#38646;&#29031;&#39038;&#25552;&#31034;&#22312;&#20845;&#20010;&#26368;&#26032;&#21457;&#24067;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#38382;&#39064;&#22238;&#31572;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#33258;&#21160;&#25552;&#31034;&#21457;&#29616;&#30340;CoT&#25552;&#31034;&#21487;&#22312;&#26032;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#22312;&#24212;&#29992;&#20110;GPT-4&#27169;&#22411;&#26102;&#21462;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#20852;&#30340;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#25512;&#29702;&#33021;&#21147;&#26377;&#26395;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20808;&#21069;&#27169;&#22411;&#25152;&#21046;&#23450;&#30340;&#25552;&#31034;&#31574;&#30053;&#22914;&#20309;&#36866;&#29992;&#20110;&#26032;&#27169;&#22411;&#21644;&#19981;&#21516;&#25968;&#25454;&#38598;&#20173;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#36825;&#39033;&#23567;&#22411;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#19968;&#31995;&#21015;&#38646;&#29031;&#39038;&#25552;&#31034;&#65288;zero-shot prompts&#65289;&#30340;&#24615;&#33021;&#65292;&#20197;&#35825;&#23548;CoT&#25512;&#29702;&#65292;&#22312;6&#20010;&#26368;&#26032;&#21457;&#24067;&#30340;LLM&#65288;davinci-002&#65292;davinci-003&#65292;GPT-3.5-turbo&#65292;GPT-4&#65292;Flan-T5-xxl&#21644;Cohere command-xlarge&#65289;&#19978;&#19982;&#21253;&#25324;&#31185;&#23398;&#21644;&#21307;&#23398;&#39046;&#22495;&#30340;&#20845;&#20010;&#38382;&#31572;&#25968;&#25454;&#38598;&#28151;&#21512;&#22312;&#19968;&#36215;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#33258;&#21160;&#25552;&#31034;&#21457;&#29616;&#30340;CoT&#25552;&#31034;&#22312;&#23454;&#39564;&#26465;&#20214;&#19979;&#34920;&#29616;&#20986;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#24212;&#29992;&#20110;&#26368;&#20808;&#36827;&#30340;GPT-4&#27169;&#22411;&#26102;&#20135;&#29983;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emergent chain-of-thought (CoT) reasoning capabilities promise to improve performance and explainability of large language models (LLMs). However, uncertainties remain about how prompting strategies formulated for previous model generations generalize to new model generations and different datasets. In this small-scale study we compare the performance of a range of zero-shot prompts for inducing CoT reasoning across six recently released LLMs (davinci-002, davinci-003, GPT-3.5-turbo, GPT-4, Flan-T5-xxl and Cohere command-xlarge) on a mixture of six question-answering datasets, including datasets from scientific and medical domains. We find that a CoT prompt that was previously discovered through automated prompt discovery shows robust performance across experimental conditions and produces best results when applied to the state-of-the-art model GPT-4.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#25513;&#30721;&#32467;&#26500;&#25104;&#38271;&#65288;MSG&#65289;&#65292;&#21487;&#20197;&#21152;&#36895;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#65292;&#20854;&#20013;&#21253;&#25324;&#20840;&#32500;&#24230;&#25104;&#38271;&#36827;&#31243;&#21644;&#29420;&#31435;&#20110;&#26032;&#26435;&#37325;&#21021;&#22987;&#21270;&#30340;&#20989;&#25968;&#20005;&#26684;&#20445;&#30041;&#25104;&#38271;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2305.02869</link><description>&lt;p&gt;
&#36890;&#36807;&#25513;&#30721;&#32467;&#26500;&#25104;&#38271;&#23454;&#29616;2&#20493;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
2x Faster Language Model Pre-training via Masked Structural Growth. (arXiv:2305.02869v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#25513;&#30721;&#32467;&#26500;&#25104;&#38271;&#65288;MSG&#65289;&#65292;&#21487;&#20197;&#21152;&#36895;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#65292;&#20854;&#20013;&#21253;&#25324;&#20840;&#32500;&#24230;&#25104;&#38271;&#36827;&#31243;&#21644;&#29420;&#31435;&#20110;&#26032;&#26435;&#37325;&#21021;&#22987;&#21270;&#30340;&#20989;&#25968;&#20005;&#26684;&#20445;&#30041;&#25104;&#38271;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20013;&#65292;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#20174;&#23567;&#22411;Transformer&#32467;&#26500;&#36880;&#27493;&#25193;&#23637;&#21040;&#22823;&#22411;&#32467;&#26500;&#65292;&#21152;&#24555;&#39044;&#35757;&#32451;&#36827;&#31243;&#12290;&#36825;&#31181;&#28176;&#36827;&#24335;&#25104;&#38271;&#30340;&#20027;&#35201;&#30740;&#31350;&#38382;&#39064;&#26377;&#20004;&#20010;&#65292;&#21363;&#25104;&#38271;&#36827;&#31243;&#21644;&#25104;&#38271;&#25805;&#20316;&#12290;&#23545;&#20110;&#25104;&#38271;&#36827;&#31243;&#65292;&#29616;&#26377;&#30740;&#31350;&#24050;&#32463;&#25506;&#32034;&#20102;&#28145;&#24230;&#21644;&#21069;&#39304;&#23618;&#30340;&#22810;&#38454;&#27573;&#25193;&#23637;&#65292;&#20294;&#27599;&#20010;&#32500;&#24230;&#23545;&#36827;&#31243;&#25928;&#29575;&#30340;&#24433;&#21709;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#32780;&#23545;&#20110;&#25104;&#38271;&#25805;&#20316;&#65292;&#29616;&#26377;&#30740;&#31350;&#20381;&#36182;&#20110;&#26032;&#26435;&#37325;&#30340;&#21021;&#22987;&#21270;&#26469;&#32487;&#25215;&#21407;&#26377;&#30340;&#30693;&#35782;&#65292;&#21482;&#23454;&#29616;&#20102;&#38750;&#20005;&#26684;&#30340;&#20989;&#25968;&#20445;&#30041;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#36827;&#19968;&#27493;&#30340;&#35757;&#32451;&#21160;&#24577;&#20248;&#21270;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#25513;&#30721;&#32467;&#26500;&#25104;&#38271;&#65288;MSG&#65289;&#65292;&#20854;&#20013;&#21253;&#25324;&#28041;&#21450;&#25152;&#26377;&#21487;&#33021;&#32500;&#24230;&#30340;&#25104;&#38271;&#36827;&#31243;&#21644;&#29420;&#31435;&#20110;&#26032;&#26435;&#37325;&#21021;&#22987;&#21270;&#30340;&#20989;&#25968;&#20005;&#26684;&#20445;&#30041;&#25104;&#38271;&#25805;&#20316;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;MSG&#21487;&#26174;&#33879;&#21152;&#36895;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Acceleration of large language model pre-training is a critical issue in present NLP research. In this paper, we focus on speeding up pre-training by progressively growing from a small Transformer structure to a large one. There are two main research problems related to progressive growth: growth schedule and growth operator. For growth schedule, existing work has explored multi-stage expansion of depth and feedforward layers. However, the impact of each dimension on the schedule's efficiency is still an open question. For growth operator, existing work relies on the initialization of new weights to inherit knowledge, and achieve only non-strict function preservation, limiting further optimization of training dynamics. To address these issues, we propose Masked Structural Growth (MSG), including growth schedules involving all possible dimensions and strictly function-preserving growth operators that is independent of the initialization of new weights. Experiments show that MSG is signi
&lt;/p&gt;</description></item><item><title>CausalAPM&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;NLU&#25991;&#26412;&#35299;&#32544;&#26694;&#26550;&#65292;&#23427;&#23558;&#25991;&#23383;&#21644;&#35821;&#20041;&#20449;&#24687;&#25237;&#23556;&#21040;&#29420;&#31435;&#30340;&#29305;&#24449;&#23376;&#31354;&#38388;&#20013;&#65292;&#24182;&#38480;&#21046;&#20102;&#25991;&#23383;&#20449;&#24687;&#22312;&#21518;&#32493;&#39044;&#27979;&#20013;&#30340;&#21442;&#19982;&#31243;&#24230;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#26377;&#25928;&#22320;&#24212;&#23545;&#25968;&#25454;&#38598;&#20559;&#32622;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#25512;&#24191;&#24615;&#33021;&#32780;&#19981;&#25439;&#22833;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2305.02865</link><description>&lt;p&gt;
CausalAPM: &#29992;&#20110;NLU&#21435;&#20559;&#32622;&#30340;&#36890;&#29992;&#25991;&#26412;&#35299;&#32544;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CausalAPM: Generalizable Literal Disentanglement for NLU Debiasing. (arXiv:2305.02865v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02865
&lt;/p&gt;
&lt;p&gt;
CausalAPM&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;NLU&#25991;&#26412;&#35299;&#32544;&#26694;&#26550;&#65292;&#23427;&#23558;&#25991;&#23383;&#21644;&#35821;&#20041;&#20449;&#24687;&#25237;&#23556;&#21040;&#29420;&#31435;&#30340;&#29305;&#24449;&#23376;&#31354;&#38388;&#20013;&#65292;&#24182;&#38480;&#21046;&#20102;&#25991;&#23383;&#20449;&#24687;&#22312;&#21518;&#32493;&#39044;&#27979;&#20013;&#30340;&#21442;&#19982;&#31243;&#24230;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#26377;&#25928;&#22320;&#24212;&#23545;&#25968;&#25454;&#38598;&#20559;&#32622;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#25512;&#24191;&#24615;&#33021;&#32780;&#19981;&#25439;&#22833;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#20559;&#32622;&#38382;&#39064;&#36234;&#26469;&#36234;&#24341;&#36215;&#20154;&#20204;&#23545;NLU&#27169;&#22411;&#25512;&#24191;&#33021;&#21147;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#20174;&#22240;&#26524;&#25512;&#26029;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;&#25968;&#25454;&#38598;&#20559;&#32622;&#38382;&#39064;&#30340;&#21407;&#22240;&#65292;&#25552;&#20986;&#20102;CausalAPM&#65292;&#19968;&#31181;&#36890;&#29992;&#30340;&#25991;&#26412;&#35299;&#32544;&#26694;&#26550;&#26469;&#35299;&#20915;&#20559;&#32622;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#23558;&#25991;&#23383;&#21644;&#35821;&#20041;&#20449;&#24687;&#25237;&#23556;&#21040;&#29420;&#31435;&#30340;&#29305;&#24449;&#23376;&#31354;&#38388;&#20013;&#65292;&#24182;&#38480;&#21046;&#20102;&#25991;&#23383;&#20449;&#24687;&#22312;&#21518;&#32493;&#39044;&#27979;&#20013;&#30340;&#21442;&#19982;&#31243;&#24230;&#12290;&#22312;&#19977;&#20010;NLP&#22522;&#20934;&#27979;&#35797;&#20013;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#26174;&#33879;&#25552;&#39640;&#20102;OOD&#30340;&#25512;&#24191;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;ID&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dataset bias, i.e., the over-reliance on dataset-specific literal heuristics, is getting increasing attention for its detrimental effect on the generalization ability of NLU models. Existing works focus on eliminating dataset bias by down-weighting problematic data in the training process, which induce the omission of valid feature information while mitigating bias. In this work, We analyze the causes of dataset bias from the perspective of causal inference and propose CausalAPM, a generalizable literal disentangling framework to ameliorate the bias problem from feature granularity. The proposed approach projects literal and semantic information into independent feature subspaces, and constrains the involvement of literal information in subsequent predictions. Extensive experiments on three NLP benchmarks (MNLI, FEVER, and QQP) demonstrate that our proposed framework significantly improves the OOD generalization performance while maintaining ID performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#19977;&#27493;&#39046;&#22495;&#28151;&#28102;&#26041;&#27861;&#65292;&#21253;&#25324;&#22522;&#20110;&#39057;&#29575;&#21644;&#27880;&#24847;&#21147;&#35268;&#33539;&#30340;&#36974;&#30422;&#12289;&#36974;&#30422;&#39046;&#22495;&#29305;&#23450;&#25552;&#31034;&#20449;&#24687;&#65292;&#20197;&#21450;&#25581;&#31034;&#39046;&#22495;&#36890;&#29992;&#19978;&#19979;&#25991;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#39046;&#22495;&#36716;&#31227;&#26041;&#38754;&#26377;&#36739;&#22909;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.02858</link><description>&lt;p&gt;
ReMask&#65306;&#19968;&#31181;&#38024;&#23545;&#39046;&#22495;&#21453;&#20107;&#23454;&#29983;&#25104;&#30340;&#31283;&#20581;&#20449;&#24687;&#36974;&#30422;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ReMask: A Robust Information-Masking Approach for Domain Counterfactual Generation. (arXiv:2305.02858v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#19977;&#27493;&#39046;&#22495;&#28151;&#28102;&#26041;&#27861;&#65292;&#21253;&#25324;&#22522;&#20110;&#39057;&#29575;&#21644;&#27880;&#24847;&#21147;&#35268;&#33539;&#30340;&#36974;&#30422;&#12289;&#36974;&#30422;&#39046;&#22495;&#29305;&#23450;&#25552;&#31034;&#20449;&#24687;&#65292;&#20197;&#21450;&#25581;&#31034;&#39046;&#22495;&#36890;&#29992;&#19978;&#19979;&#25991;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#39046;&#22495;&#36716;&#31227;&#26041;&#38754;&#26377;&#36739;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#20559;&#31227;&#26159;NLP&#20013;&#30340;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#27492;&#35768;&#22810;&#26041;&#27861;&#37319;&#29992;&#23398;&#20064;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#26469;&#20943;&#36731;&#25512;&#29702;&#38454;&#27573;&#30340;&#39046;&#22495;&#20559;&#31227;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#26080;&#27861;&#21033;&#29992;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#39046;&#22495;&#29305;&#23450;&#32454;&#24494;&#24046;&#21035;&#12290;&#20026;&#36991;&#20813;&#36825;&#31181;&#32570;&#28857;&#65292;&#39046;&#22495;&#21453;&#20107;&#23454;&#29983;&#25104;&#26088;&#22312;&#23558;&#28304;&#22495;&#20013;&#30340;&#25991;&#26412;&#36716;&#25442;&#20026;&#32473;&#23450;&#30340;&#30446;&#26631;&#22495;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#30340;&#26377;&#38480;&#24615;&#65292;&#36825;&#31181;&#22522;&#20110;&#39057;&#29575;&#30340;&#26041;&#27861;&#32463;&#24120;&#20250;&#38169;&#36807;&#19968;&#20123;&#26377;&#25928;&#21644;&#34394;&#20551;&#30340;&#39046;&#22495;&#26631;&#35760;&#20851;&#32852;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#19977;&#27493;&#39046;&#22495;&#28151;&#28102;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#22522;&#20110;&#39057;&#29575;&#21644;&#27880;&#24847;&#21147;&#35268;&#33539;&#30340;&#36974;&#30422;&#12289;&#36974;&#30422;&#39046;&#22495;&#29305;&#23450;&#25552;&#31034;&#20449;&#24687;&#65292;&#20197;&#21450;&#25581;&#31034;&#39046;&#22495;&#36890;&#29992;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#23454;&#35777;&#34920;&#26126;&#65292;&#26469;&#33258;&#25105;&#20204;&#36974;&#30422;&#25991;&#26412;&#30340;&#21453;&#20107;&#23454;&#26679;&#26412;&#22312;12&#20010;&#39046;&#22495;&#24773;&#24863;&#20998;&#31867;&#35774;&#32622;&#20013;&#26377;10&#20010;&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#39046;&#22495;&#36716;&#31227;&#65292;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#25216;&#26415;&#24179;&#22343;&#25552;&#39640;&#20102;2%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain shift is a big challenge in NLP, thus, many approaches resort to learning domain-invariant features to mitigate the inference phase domain shift. Such methods, however, fail to leverage the domain-specific nuances relevant to the task at hand. To avoid such drawbacks, domain counterfactual generation aims to transform a text from the source domain to a given target domain. However, due to the limited availability of data, such frequency-based methods often miss and lead to some valid and spurious domain-token associations. Hence, we employ a three-step domain obfuscation approach that involves frequency and attention norm-based masking, to mask domain-specific cues, and unmasking to regain the domain generic context. Our experiments empirically show that the counterfactual samples sourced from our masked text lead to improved domain transfer on 10 out of 12 domain sentiment classification settings, with an average of 2% accuracy improvement over the state-of-the-art for unsuperv
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#20171;&#32461;&#20102;&#21517;&#20026;DASC&#30340;&#21487;&#25511;&#29983;&#25104;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#23646;&#24615;&#35821;&#20041;&#31354;&#38388;&#30340;&#21152;&#26435;&#35299;&#30721;&#26469;&#23454;&#29616;&#22810;&#23646;&#24615;&#29983;&#25104;&#65292;&#24182;&#33021;&#22815;&#22312;&#22810;&#20010;&#26041;&#38754;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#25511;&#21046;&#31934;&#24230;&#21644;&#39640;&#36136;&#37327;&#30340;&#29983;&#25104;&#21709;&#24212;&#12290;</title><link>http://arxiv.org/abs/2305.02820</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#20041;&#31354;&#38388;&#30340;&#22810;&#23646;&#24615;&#21487;&#25511;&#23545;&#35805;&#29983;&#25104;&#20013;&#30340;&#21152;&#26435;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Semantic Space Grounded Weighted Decoding for Multi-Attribute Controllable Dialogue Generation. (arXiv:2305.02820v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02820
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#20171;&#32461;&#20102;&#21517;&#20026;DASC&#30340;&#21487;&#25511;&#29983;&#25104;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#23646;&#24615;&#35821;&#20041;&#31354;&#38388;&#30340;&#21152;&#26435;&#35299;&#30721;&#26469;&#23454;&#29616;&#22810;&#23646;&#24615;&#29983;&#25104;&#65292;&#24182;&#33021;&#22815;&#22312;&#22810;&#20010;&#26041;&#38754;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#25511;&#21046;&#31934;&#24230;&#21644;&#39640;&#36136;&#37327;&#30340;&#29983;&#25104;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25511;&#21046;&#32842;&#22825;&#26426;&#22120;&#20154;&#29983;&#25104;&#20855;&#26377;&#20010;&#24615;&#12289;&#24773;&#24863;&#21644;&#23545;&#35805;&#34892;&#20026;&#31561;&#22810;&#20010;&#23646;&#24615;&#30340;&#35805;&#35821;&#26159;&#19968;&#20010;&#23454;&#38469;&#26377;&#29992;&#20294;&#40092;&#26377;&#30740;&#31350;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#25511;&#29983;&#25104;&#26694;&#26550;DASC&#65292;&#23427;&#36890;&#36807;&#21152;&#26435;&#35299;&#30721;&#33539;&#24335;&#20855;&#26377;&#24378;&#22823;&#30340;&#21487;&#25511;&#24615;&#65292;&#21516;&#26102;&#22312;&#23646;&#24615;&#35821;&#20041;&#31354;&#38388;&#30340;&#22522;&#30784;&#19978;&#25552;&#39640;&#20102;&#29983;&#25104;&#36136;&#37327;&#12290;&#28982;&#21518;&#65292;&#22810;&#23646;&#24615;&#29983;&#25104;&#36890;&#36807;&#22810;&#20010;&#23646;&#24615;&#23884;&#20837;&#30340;&#25554;&#20540;&#30452;&#35266;&#22320;&#23454;&#29616;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;DASC&#22312;&#19977;&#20010;&#26041;&#38754;&#21487;&#25511;&#29983;&#25104;&#20219;&#21153;&#20013;&#21487;&#20197;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#25511;&#21046;&#31934;&#24230;&#65292;&#21516;&#26102;&#20135;&#29983;&#26377;&#36259;&#32780;&#21512;&#29702;&#30340;&#21709;&#24212;&#65292;&#21363;&#20351;&#22312;&#20998;&#24067;&#40065;&#26834;&#24615;&#27979;&#35797;&#20013;&#20063;&#26159;&#22914;&#27492;&#12290;&#23646;&#24615;&#35821;&#20041;&#31354;&#38388;&#20013;&#23398;&#20064;&#21040;&#30340;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#30340;&#21487;&#35270;&#21270;&#20063;&#25903;&#25345;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Controlling chatbot utterance generation with multiple attributes such as personalities, emotions and dialogue acts is a practically useful but under-studied problem. We propose a novel controllable generation framework called DASC that possesses strong controllability with weighted decoding paradigm, while improving generation quality with the grounding in an attribute semantics space. Generation with multiple attributes is then intuitively implemented with an interpolation of multiple attribute embeddings. Experiments show that DASC can achieve state-of-the-art control accuracy in 3-aspect controllable generation tasks while also producing interesting and reasonably sensible responses, even if in an out-of-distribution robustness test. Visualization of the meaningful representations learned in the attribute semantic space also supports its effectiveness.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;VAEs&#21644;Transformers&#26500;&#24314;&#20004;&#31181;&#20855;&#26377;&#24402;&#32435;&#20559;&#24046;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#30340;&#35299;&#37322;&#24615;&#21644;&#25968;&#25454;&#25928;&#29575;&#65292;&#33021;&#22815;&#23558;&#28508;&#22312;&#34920;&#31034;&#20013;&#30340;&#20449;&#24687;&#20998;&#31163;&#20026;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#25552;&#20379;&#20102;&#30452;&#35266;&#19988;&#21487;&#35299;&#37322;&#30340;&#34920;&#31034;&#24418;&#24335;&#65292;&#20855;&#26377;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.02810</link><description>&lt;p&gt;
&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#21477;&#23376;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Interpretable Sentence Representation with Variational Autoencoders and Attention. (arXiv:2305.02810v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02810
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;VAEs&#21644;Transformers&#26500;&#24314;&#20004;&#31181;&#20855;&#26377;&#24402;&#32435;&#20559;&#24046;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#30340;&#35299;&#37322;&#24615;&#21644;&#25968;&#25454;&#25928;&#29575;&#65292;&#33021;&#22815;&#23558;&#28508;&#22312;&#34920;&#31034;&#20013;&#30340;&#20449;&#24687;&#20998;&#31163;&#20026;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#25552;&#20379;&#20102;&#30452;&#35266;&#19988;&#21487;&#35299;&#37322;&#30340;&#34920;&#31034;&#24418;&#24335;&#65292;&#20855;&#26377;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#26088;&#22312;&#22686;&#24378;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26368;&#36817;&#19968;&#20123;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#30340;&#35299;&#37322;&#24615;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#32570;&#20047;&#27880;&#37322;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#30740;&#31350;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36873;&#25321;&#21033;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#23558;&#35266;&#23519;&#32467;&#26524;&#19982;&#38544;&#34255;&#30340;&#29983;&#25104;&#22240;&#32032;&#32852;&#31995;&#36215;&#26469;&#26041;&#38754;&#24456;&#26377;&#25928;&#65292;&#24182;&#19988;&#22312;&#25968;&#25454;&#25928;&#29575;&#23398;&#20064;&#21644;&#21487;&#35299;&#37322;&#34920;&#31034;&#23398;&#20064;&#26041;&#38754;&#20063;&#24456;&#26377;&#25928;&#12290;&#25105;&#20204;&#39318;&#20808;&#21024;&#38500;&#21322;&#30417;&#30563;VAEs&#36816;&#34892;&#26041;&#26696;&#20013;&#30340;&#19981;&#24517;&#35201;&#32452;&#20214;&#65292;&#20351;&#24471;&#23427;&#20204;&#26356;&#24555;&#36895;&#12289;&#26356;&#23567;&#12289;&#26356;&#26131;&#20110;&#35774;&#35745;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;VAEs&#21644;Transformer&#26500;&#24314;&#20102;&#20004;&#20010;&#20855;&#26377;&#24402;&#32435;&#20559;&#24046;&#30340;&#27169;&#22411;&#65292;&#23558;&#28508;&#22312;&#34920;&#31034;&#20013;&#30340;&#20449;&#24687;&#20998;&#31163;&#25104;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#65292;&#32780;&#19981;&#38656;&#35201;&#27880;&#37322;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#25552;&#20379;&#20102;&#30452;&#35266;&#19988;&#21487;&#35299;&#37322;&#30340;&#34920;&#31034;&#24418;&#24335;&#65292;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#19979;&#28216;&#20219;&#21153;&#38750;&#24120;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this thesis, we develop methods to enhance the interpretability of recent representation learning techniques in natural language processing (NLP) while accounting for the unavailability of annotated data. We choose to leverage Variational Autoencoders (VAEs) due to their efficiency in relating observations to latent generative factors and their effectiveness in data-efficient learning and interpretable representation learning. As a first contribution, we identify and remove unnecessary components in the functioning scheme of semi-supervised VAEs making them faster, smaller and easier to design. Our second and main contribution is to use VAEs and Transformers to build two models with inductive bias to separate information in latent representations into understandable concepts without annotated data. The first model, Attention-Driven VAE (ADVAE), is able to separately represent and control information about syntactic roles in sentences. The second model, QKVAE, uses separate latent va
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24037;&#19994;&#30028;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20013;&#30340;&#23384;&#22312;&#21644;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#22312;&#36807;&#21435;&#20116;&#24180;&#20013;&#65292;&#24037;&#19994;&#30028;&#30340;&#23384;&#22312;&#19982;&#24433;&#21709;&#21576;&#29616;&#24613;&#21095;&#22686;&#38271;&#65292;&#19968;&#20123;&#20844;&#21496;&#21344;&#25454;&#20102;&#22823;&#37096;&#20998;&#20986;&#29256;&#29289;&#65292;&#24182;&#21521;&#23398;&#26415;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#36164;&#37329;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2305.02797</link><description>&lt;p&gt;
&#25151;&#38388;&#37324;&#30340;&#22823;&#35937;&#65306;&#20998;&#26512;&#22823;&#22411;&#31185;&#25216;&#20844;&#21496;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20013;&#30340;&#23384;&#22312;
&lt;/p&gt;
&lt;p&gt;
The Elephant in the Room: Analyzing the Presence of Big Tech in Natural Language Processing Research. (arXiv:2305.02797v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02797
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24037;&#19994;&#30028;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20013;&#30340;&#23384;&#22312;&#21644;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#22312;&#36807;&#21435;&#20116;&#24180;&#20013;&#65292;&#24037;&#19994;&#30028;&#30340;&#23384;&#22312;&#19982;&#24433;&#21709;&#21576;&#29616;&#24613;&#21095;&#22686;&#38271;&#65292;&#19968;&#20123;&#20844;&#21496;&#21344;&#25454;&#20102;&#22823;&#37096;&#20998;&#20986;&#29256;&#29289;&#65292;&#24182;&#21521;&#23398;&#26415;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#36164;&#37329;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21019;&#36896;&#20102;&#26032;&#30340;&#21830;&#19994;&#26426;&#20250;&#65292;&#24182;&#19988;&#20351;&#24471;NLP&#30740;&#31350;&#23545;&#20135;&#19994;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;&#20316;&#20026;NLP&#39046;&#22495;&#30340;&#22823;&#29609;&#23478;&#20043;&#19968;&#65292;&#36830;&#21516;&#25919;&#24220;&#21644;&#22823;&#23398;&#19968;&#36215;&#65292;&#36319;&#36394;&#20135;&#19994;&#23545;&#30740;&#31350;&#30340;&#24433;&#21709;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#33268;&#21147;&#20110;&#37327;&#21270;&#21644;&#34920;&#24449;&#24037;&#19994;&#30028;&#22312;NLP&#31038;&#21306;&#20013;&#30340;&#23384;&#22312;&#12290;&#20351;&#29992;&#20855;&#26377;78,187&#31687;NLP&#20986;&#29256;&#29289;&#21644;701&#20010;NLP&#20316;&#32773;&#31616;&#21382;&#30340;&#20840;&#38754;&#20803;&#25968;&#25454;&#35821;&#26009;&#24211;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#33258;&#19978;&#19990;&#32426;90&#24180;&#20195;&#20197;&#26469;&#35813;&#39046;&#22495;&#20013;&#30340;&#24037;&#19994;&#23384;&#22312;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;NLP&#20316;&#32773;&#20013;&#30340;&#24037;&#19994;&#23384;&#22312;&#22312;&#36807;&#21435;&#20116;&#24180;&#20013;&#24613;&#21095;&#22686;&#38271;&#65288;&#20174;2017&#24180;&#21040;2022&#24180;&#30340;&#22686;&#38271;&#29575;&#20026;180&#65285;&#65289;&#12290;&#19968;&#20123;&#20844;&#21496;&#21344;&#25454;&#20102;&#22823;&#37096;&#20998;&#20986;&#29256;&#29289;&#65292;&#24182;&#36890;&#36807;&#25320;&#27454;&#21644;&#23454;&#20064;&#20026;&#23398;&#26415;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#36164;&#37329;&#25903;&#25345;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24037;&#19994;&#30028;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#30340;&#23384;&#22312;&#21644;&#24433;&#21709;&#26159;&#26174;&#33879;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in deep learning methods for natural language processing (NLP) have created new business opportunities and made NLP research critical for industry development. As one of the big players in the field of NLP, together with governments and universities, it is important to track the influence of industry on research. In this study, we seek to quantify and characterize industry presence in the NLP community over time. Using a corpus with comprehensive metadata of 78,187 NLP publications and 701 resumes of NLP publication authors, we explore the industry presence in the field since the early 90s. We find that industry presence among NLP authors has been steady before a steep increase over the past five years (180% growth from 2017 to 2022). A few companies account for most of the publications and provide funding to academic researchers through grants and internships. Our study shows that the presence and impact of the industry on natural language processing research are signi
&lt;/p&gt;</description></item><item><title>BranchNorm&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#37325;&#26032;&#35843;&#25972;Transformer&#30340;&#38750;&#27531;&#24046;&#20998;&#25903;&#65292;&#29702;&#35770;&#19978;&#31283;&#23450;&#20102;&#35757;&#32451;&#65292;&#24182;&#22312;&#38543;&#21518;&#30340;&#35757;&#32451;&#38454;&#27573;&#20013;&#20419;&#36827;&#20102;&#26356;&#22909;&#30340;&#25910;&#25947;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;BranchNorm&#22312;&#35757;&#32451;&#31283;&#23450;&#24615;&#21644;&#25910;&#25947;&#24615;&#33021;&#20043;&#38388;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.02790</link><description>&lt;p&gt;
BranchNorm: &#40065;&#26834;&#22320;&#25193;&#23637;&#26497;&#28145;&#30340;Transformer
&lt;/p&gt;
&lt;p&gt;
BranchNorm: Robustly Scaling Extremely Deep Transformers. (arXiv:2305.02790v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02790
&lt;/p&gt;
&lt;p&gt;
BranchNorm&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#37325;&#26032;&#35843;&#25972;Transformer&#30340;&#38750;&#27531;&#24046;&#20998;&#25903;&#65292;&#29702;&#35770;&#19978;&#31283;&#23450;&#20102;&#35757;&#32451;&#65292;&#24182;&#22312;&#38543;&#21518;&#30340;&#35757;&#32451;&#38454;&#27573;&#20013;&#20419;&#36827;&#20102;&#26356;&#22909;&#30340;&#25910;&#25947;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;BranchNorm&#22312;&#35757;&#32451;&#31283;&#23450;&#24615;&#21644;&#25910;&#25947;&#24615;&#33021;&#20043;&#38388;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;DeepNorm&#23558;Transformer&#25193;&#23637;&#21040;&#26497;&#28145;&#65288;&#21363;1000&#23618;&#65289;&#65292;&#23637;&#31034;&#20102;&#28145;&#24230;&#25193;&#23637;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#31283;&#23450;&#28145;&#24230;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;DeepNorm&#35797;&#22270;&#23558;&#27169;&#22411;&#26356;&#26032;&#32422;&#26463;&#20026;&#19968;&#20010;&#24658;&#23450;&#20540;&#12290;&#23613;&#31649;&#24212;&#29992;&#36825;&#31181;&#32422;&#26463;&#21487;&#20197;&#20351;&#27169;&#22411;&#22312;&#26089;&#26399;&#35757;&#32451;&#38454;&#27573;&#21463;&#30410;&#65292;&#20294;&#21487;&#33021;&#23548;&#33268;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#27169;&#22411;&#35757;&#32451;&#19981;&#36275;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BranchNorm&#65292;&#23427;&#26681;&#25454;&#35757;&#32451;&#26399;&#38388;&#21160;&#24577;&#37325;&#26032;&#35843;&#25972;Transformer&#30340;&#38750;&#27531;&#24046;&#20998;&#25903;&#12290;BranchNorm&#19981;&#20165;&#22312;&#26089;&#26399;&#38454;&#27573;&#29702;&#35770;&#19978;&#31283;&#23450;&#20102;&#35757;&#32451;&#65292;&#32780;&#19988;&#22312;&#38543;&#21518;&#30340;&#35757;&#32451;&#38454;&#27573;&#20013;&#20419;&#36827;&#20102;&#26356;&#22909;&#30340;&#25910;&#25947;&#12290;&#22810;&#20010;&#32763;&#35793;&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;BranchNorm&#22312;&#35757;&#32451;&#31283;&#23450;&#24615;&#21644;&#25910;&#25947;&#24615;&#33021;&#20043;&#38388;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, DeepNorm scales Transformers into extremely deep (i.e., 1000 layers) and reveals the promising potential of deep scaling. To stabilize the training of deep models, DeepNorm (Wang et al., 2022) attempts to constrain the model update to a constant value. Although applying such a constraint can benefit the early stage of model training, it may lead to undertrained models during the whole training procedure. In this paper, we propose BranchNorm, which dynamically rescales the non-residual branch of Transformer in accordance with the training period. BranchNorm not only theoretically stabilizes the training with smooth gradient norms at the early stage, but also encourages better convergence in the subsequent training stage. Experiment results on multiple translation tasks demonstrate that BranchNorm achieves a better trade-off between training stability and converge performance.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Ansible Wisdom&#30340;&#33258;&#28982;&#35821;&#35328;&#36716;Ansible-YAML&#20195;&#30721;&#30340;&#24037;&#20855;&#65292;&#21487;&#33258;&#21160;&#21270;&#29983;&#25104;Ansible&#33050;&#26412;&#65292;&#25552;&#39640;IT&#33258;&#21160;&#21270;&#29983;&#20135;&#21147;&#65292;&#24182;&#30456;&#27604;&#29616;&#26377;&#25216;&#26415;&#36798;&#21040;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2305.02783</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#24687;&#25216;&#26415;&#20219;&#21153;&#20013;&#33258;&#21160;&#29983;&#25104;YAML&#20195;&#30721;
&lt;/p&gt;
&lt;p&gt;
Automated Code generation for Information Technology Tasks in YAML through Large Language Models. (arXiv:2305.02783v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02783
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Ansible Wisdom&#30340;&#33258;&#28982;&#35821;&#35328;&#36716;Ansible-YAML&#20195;&#30721;&#30340;&#24037;&#20855;&#65292;&#21487;&#33258;&#21160;&#21270;&#29983;&#25104;Ansible&#33050;&#26412;&#65292;&#25552;&#39640;IT&#33258;&#21160;&#21270;&#29983;&#20135;&#21147;&#65292;&#24182;&#30456;&#27604;&#29616;&#26377;&#25216;&#26415;&#36798;&#21040;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#30340;&#19981;&#26029;&#25552;&#21319;&#65292;&#22312;&#36890;&#29992;&#32534;&#31243;&#35821;&#35328;&#26041;&#38754;&#30340;&#21463;&#30410;&#26368;&#22823;&#65292;&#32780;&#38024;&#23545;IT&#33258;&#21160;&#21270;&#31561;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#30340;&#30740;&#31350;&#36739;&#23569;&#12290;&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;Ansible-YAML&#30340;&#29983;&#25104;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Ansible Wisdom&#30340;&#33258;&#28982;&#35821;&#35328;&#36716;Ansible-YAML&#20195;&#30721;&#30340;&#24037;&#20855;&#65292;&#26088;&#22312;&#25552;&#39640;IT&#33258;&#21160;&#21270;&#29983;&#20135;&#21147;&#12290;&#30740;&#31350;&#37319;&#29992;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#26032;&#30340;&#21253;&#21547;Ansible-YAML&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#25193;&#23637;&#35757;&#32451;&#12290;&#21516;&#26102;&#65292;&#36824;&#24320;&#21457;&#20102;&#20004;&#20010;&#29992;&#20110;&#25429;&#25417;&#27492;&#39046;&#22495;&#29305;&#24449;&#30340;YAML&#21644;Ansible&#24615;&#33021;&#25351;&#26631;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;Ansible Wisdom&#21487;&#20197;&#31934;&#30830;&#22320;&#20174;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#20013;&#29983;&#25104;Ansible&#33050;&#26412;&#65292;&#24182;&#19988;&#20854;&#24615;&#33021;&#21487;&#19982;&#29616;&#26377;&#25216;&#26415;&#30340;&#29366;&#24577;&#30456;&#23218;&#32654;&#25110;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent improvement in code generation capabilities due to the use of large language models has mainly benefited general purpose programming languages. Domain specific languages, such as the ones used for IT Automation, have received far less attention, despite involving many active developers and being an essential component of modern cloud platforms. This work focuses on the generation of Ansible-YAML, a widely used markup language for IT Automation. We present Ansible Wisdom, a natural-language to Ansible-YAML code generation tool, aimed at improving IT automation productivity. Ansible Wisdom is a transformer-based model, extended by training with a new dataset containing Ansible-YAML. We also develop two novel performance metrics for YAML and Ansible to capture the specific characteristics of this domain. Results show that Ansible Wisdom can accurately generate Ansible script from natural language prompts with performance comparable or better than existing state of the art code 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#32479;&#19968;&#27169;&#22411;&#23398;&#20064;&#65292;&#21487;&#20197;&#21516;&#26102;&#36866;&#29992;&#20110;&#32763;&#35793;&#21508;&#31181;&#20219;&#21153;&#25968;&#25454;&#65292;&#24182;&#23454;&#29616;&#26234;&#33021;&#25353;&#38656;&#32763;&#35793;&#65292;&#30456;&#23545;&#29616;&#26377;&#30340;&#29305;&#23450;&#25968;&#25454;&#38598;&#27169;&#22411;&#33021;&#22815;&#24471;&#21040;&#26126;&#26174;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.02777</link><description>&lt;p&gt;
&#21508;&#31181;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#32479;&#19968;&#27169;&#22411;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unified Model Learning for Various Neural Machine Translation. (arXiv:2305.02777v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02777
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#32479;&#19968;&#27169;&#22411;&#23398;&#20064;&#65292;&#21487;&#20197;&#21516;&#26102;&#36866;&#29992;&#20110;&#32763;&#35793;&#21508;&#31181;&#20219;&#21153;&#25968;&#25454;&#65292;&#24182;&#23454;&#29616;&#26234;&#33021;&#25353;&#38656;&#32763;&#35793;&#65292;&#30456;&#23545;&#29616;&#26377;&#30340;&#29305;&#23450;&#25968;&#25454;&#38598;&#27169;&#22411;&#33021;&#22815;&#24471;&#21040;&#26126;&#26174;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;(NMT)&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#26681;&#25454;&#26469;&#33258;&#19981;&#21516;&#20219;&#21153;(&#20363;&#22914;&#65292;&#25991;&#26723;&#32763;&#35793;&#21644;&#32842;&#22825;&#32763;&#35793;)&#30340;&#25968;&#25454;&#24320;&#21457;&#29305;&#23450;&#20110;&#25968;&#25454;&#38598;&#30340;&#27169;&#22411;&#12290;&#34429;&#28982;&#29305;&#23450;&#20110;&#25968;&#25454;&#38598;&#30340;&#27169;&#22411;&#24050;&#32463;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#24615;&#33021;&#65292;&#20294;&#27599;&#20010;&#25968;&#25454;&#38598;&#38656;&#35201;&#35774;&#35745;&#12289;&#35757;&#32451;&#21644;&#23384;&#20648;&#19968;&#20010;&#27169;&#22411;&#65292;&#36825;&#24456;&#40635;&#28902;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23558;&#36825;&#20123;&#32763;&#35793;&#20219;&#21153;&#32479;&#19968;&#21040;&#26356;&#26222;&#36941;&#30340;&#35774;&#32622;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#8220;&#22810;&#25165;&#22810;&#33402;&#8221;&#30340;&#27169;&#22411;&#65292;&#21363;&#36866;&#29992;&#20110;&#19981;&#21516;&#20219;&#21153;&#25968;&#25454;&#30340;&#32479;&#19968;&#27169;&#22411;&#23398;&#20064;(NMT)&#65292;&#21487;&#20197;&#21516;&#26102;&#22312;&#22810;&#31181;&#29615;&#22659;&#19979;&#36827;&#34892;&#33391;&#22909;&#30340;&#32763;&#35793;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#21487;&#20197;&#23613;&#21487;&#33021;&#22810;&#22320;&#25193;&#23637;&#12290;&#36890;&#36807;&#32479;&#19968;&#23398;&#20064;&#65292;UMLNMT&#33021;&#22815;&#36328;&#22810;&#20010;&#20219;&#21153;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#65292;&#23454;&#29616;&#26234;&#33021;&#25353;&#38656;&#32763;&#35793;&#12290;&#22312;&#19971;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#32763;&#35793;&#20219;&#21153;&#65292;&#21253;&#25324;&#21477;&#23376;&#32763;&#35793;&#12289;&#25991;&#26723;&#32763;&#35793;&#21644;&#32842;&#22825;&#32763;&#35793;&#20013;&#65292;&#25105;&#20204;&#30340;UMLNMT&#30456;&#23545;&#20110;&#29305;&#23450;&#25968;&#25454;&#38598;&#27169;&#22411;&#34920;&#29616;&#20986;&#20102;&#26126;&#26174;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing neural machine translation (NMT) studies mainly focus on developing dataset-specific models based on data from different tasks (e.g., document translation and chat translation). Although the dataset-specific models have achieved impressive performance, it is cumbersome as each dataset demands a model to be designed, trained, and stored. In this work, we aim to unify these translation tasks into a more general setting. Specifically, we propose a ``versatile'' model, i.e., the Unified Model Learning for NMT (UMLNMT) that works with data from different tasks, and can translate well in multiple settings simultaneously, and theoretically it can be as many as possible. Through unified learning, UMLNMT is able to jointly train across multiple tasks, implementing intelligent on-demand translation. On seven widely-used translation tasks, including sentence translation, document translation, and chat translation, our UMLNMT results in substantial improvements over dataset-specific model
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20420;&#20044;&#25112;&#20105;&#26399;&#38388;&#20044;&#20811;&#20848;&#20154;&#22312; Twitter &#19978;&#30340;&#35821;&#35328;&#20351;&#29992;&#65292;&#21457;&#29616;&#22312;&#25112;&#20105;&#29190;&#21457;&#21069;&#24050;&#32463;&#20986;&#29616;&#20174;&#20420;&#35821;&#21521;&#20044;&#20811;&#20848;&#35821;&#36716;&#21464;&#30340;&#36235;&#21183;&#65292;&#32780;&#25112;&#20105;&#29190;&#21457;&#21518;&#36825;&#31181;&#36235;&#21183;&#21152;&#36895;&#20102;&#65292;&#24182;&#19988;&#35768;&#22810;&#20351;&#29992;&#20420;&#35821;&#30340;&#29992;&#25143;&#22312;&#25112;&#20105;&#26399;&#38388;&#36716;&#21464;&#25104;&#20351;&#29992;&#20044;&#20811;&#20848;&#35821;&#12290;</title><link>http://arxiv.org/abs/2305.02770</link><description>&lt;p&gt;
&#35821;&#35328;&#36873;&#25321;&#30340;&#25919;&#27835;&#65306;&#20420;&#20044;&#25112;&#20105;&#22914;&#20309;&#24433;&#21709;&#20044;&#20811;&#20848;&#20154;&#22312; Twitter &#19978;&#30340;&#35821;&#35328;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Politics of Language Choice: How the Russian-Ukrainian War Influences Ukrainians' Language Use on Twitter. (arXiv:2305.02770v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20420;&#20044;&#25112;&#20105;&#26399;&#38388;&#20044;&#20811;&#20848;&#20154;&#22312; Twitter &#19978;&#30340;&#35821;&#35328;&#20351;&#29992;&#65292;&#21457;&#29616;&#22312;&#25112;&#20105;&#29190;&#21457;&#21069;&#24050;&#32463;&#20986;&#29616;&#20174;&#20420;&#35821;&#21521;&#20044;&#20811;&#20848;&#35821;&#36716;&#21464;&#30340;&#36235;&#21183;&#65292;&#32780;&#25112;&#20105;&#29190;&#21457;&#21518;&#36825;&#31181;&#36235;&#21183;&#21152;&#36895;&#20102;&#65292;&#24182;&#19988;&#35768;&#22810;&#20351;&#29992;&#20420;&#35821;&#30340;&#29992;&#25143;&#22312;&#25112;&#20105;&#26399;&#38388;&#36716;&#21464;&#25104;&#20351;&#29992;&#20044;&#20811;&#20848;&#35821;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#20351;&#29992;&#22825;&#29983;&#26159;&#25919;&#27835;&#30340;&#65292;&#24182;&#32463;&#24120;&#29992;&#20316;&#25991;&#21270;&#36523;&#20221;&#30340;&#36733;&#20307;&#65292;&#21516;&#26102;&#20063;&#26159;&#22269;&#23478;&#24314;&#35774;&#30340;&#22522;&#30784;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20420;&#20044;&#25112;&#20105;&#26399;&#38388;&#65288;2020&#24180;1&#26376;&#33267;2022&#24180;10&#26376;&#65289;&#65292;&#22522;&#20110;&#36229;&#36807;62,000&#20301;&#29992;&#25143;&#21457;&#24067;&#30340;400&#19975;&#26465;&#22320;&#29702;&#26631;&#35760;&#25512;&#25991;&#20013;&#65292;&#20044;&#20811;&#20848;&#20844;&#27665;&#30340;&#35821;&#35328;&#36873;&#25321;&#21644;&#25512;&#25991;&#27963;&#21160;&#12290;&#20351;&#29992;&#32479;&#35745;&#27169;&#22411;&#65292;&#21306;&#20998;&#20102;Twitter&#19978;&#29992;&#25143;&#30340;&#27969;&#20837;&#27969;&#20986;&#25152;&#24341;&#36215;&#30340;&#26679;&#26412;&#25928;&#24212;&#21644;&#29992;&#25143;&#34892;&#20026;&#21464;&#21270;&#25152;&#24341;&#36215;&#30340;&#34892;&#20026;&#25928;&#24212;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#25112;&#20105;&#29190;&#21457;&#20043;&#21069;&#24050;&#32463;&#26377;&#19968;&#20010;&#31283;&#23450;&#30340;&#20174;&#20420;&#35821;&#21521;&#20044;&#20811;&#20848;&#35821;&#30340;&#36716;&#21464;&#65292;&#32780;&#36825;&#19968;&#36807;&#31243;&#22312;&#25112;&#20105;&#29190;&#21457;&#21518;&#36805;&#36895;&#21152;&#36895;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#21464;&#21270;&#20027;&#35201;&#24402;&#22240;&#20110;&#29992;&#25143;&#34892;&#20026;&#30340;&#25913;&#21464;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#35768;&#22810;&#20351;&#29992;&#20420;&#35821;&#30340;&#29992;&#25143;&#22312;&#25112;&#20105;&#26399;&#38388;&#20250;&#36716;&#21464;&#25104;&#20351;&#29992;&#20044;&#20811;&#20848;&#35821;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of language is innately political and often a vehicle of cultural identity as well as the basis for nation building. Here, we examine language choice and tweeting activity of Ukrainian citizens based on more than 4 million geo-tagged tweets from over 62,000 users before and during the Russian-Ukrainian War, from January 2020 to October 2022. Using statistical models, we disentangle sample effects, arising from the in- and outflux of users on Twitter, from behavioural effects, arising from behavioural changes of the users. We observe a steady shift from the Russian language towards the Ukrainian language already before the war, which drastically speeds up with its outbreak. We attribute these shifts in large part to users' behavioural changes. Notably, we find that many Russian-tweeting users perform a hard-switch to Ukrainian as a result of the war.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VendorLink&#30340;NLP&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#35782;&#21035;&#21644;&#38142;&#25509;&#26263;&#32593;&#24066;&#22330;&#19978;&#30340;&#20379;&#24212;&#21830;&#34987;&#36801;&#31227;&#21644;&#28508;&#22312;&#21035;&#21517;&#65292;&#20943;&#23569;&#38750;&#27861;&#24066;&#22330;&#30340;&#21311;&#21517;&#24615;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2305.02763</link><description>&lt;p&gt;
VendorLink&#65306;&#19968;&#31181;NLP&#26041;&#27861;&#29992;&#20110;&#35782;&#21035;&#21644;&#38142;&#25509;&#26263;&#32593;&#24066;&#22330;&#19978;&#30340;&#20379;&#24212;&#21830;&#34987;&#36801;&#31227;&#21644;&#28508;&#22312;&#21035;&#21517;
&lt;/p&gt;
&lt;p&gt;
VendorLink: An NLP approach for Identifying &amp; Linking Vendor Migrants &amp; Potential Aliases on Darknet Markets. (arXiv:2305.02763v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02763
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VendorLink&#30340;NLP&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#35782;&#21035;&#21644;&#38142;&#25509;&#26263;&#32593;&#24066;&#22330;&#19978;&#30340;&#20379;&#24212;&#21830;&#34987;&#36801;&#31227;&#21644;&#28508;&#22312;&#21035;&#21517;&#65292;&#20943;&#23569;&#38750;&#27861;&#24066;&#22330;&#30340;&#21311;&#21517;&#24615;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26263;&#32593;&#19978;&#30340;&#21311;&#21517;&#24615;&#20351;&#24471;&#20379;&#24212;&#21830;&#21487;&#20197;&#20351;&#29992;&#22810;&#20010;&#20379;&#24212;&#21830;&#21035;&#21517;&#25110;&#39057;&#32321;&#36801;&#31227;&#24066;&#22330;&#32780;&#19981;&#34987;&#21457;&#29616;&#12290;&#22240;&#27492;&#65292;&#22312;&#26263;&#32593;&#19978;&#21457;&#29616;&#38750;&#27861;&#24066;&#22330;&#21450;&#20854;&#32852;&#31995;&#20154;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#35782;&#21035;&#38750;&#27861;&#24066;&#22330;&#21644;&#20379;&#24212;&#21830;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;VendorLink&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;NLP&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#26597;&#20889;&#20316;&#27169;&#24335;&#26469;&#39564;&#35777;&#12289;&#35782;&#21035;&#21644;&#38142;&#25509;&#19971;&#20010;&#20844;&#20849;&#26263;&#32593;&#24066;&#22330;&#19978;&#30340;&#21807;&#19968;&#20379;&#24212;&#21830;&#24080;&#25143;&#12290;&#19982;&#29616;&#26377;&#25991;&#29486;&#19981;&#21516;&#65292;VendorLink&#21033;&#29992;&#26377;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#30340;&#20248;&#21183;&#26469;&#25191;&#34892;&#23553;&#38381;&#38598;&#20379;&#24212;&#21830;&#39564;&#35777;&#12289;&#24320;&#25918;&#38598;&#20379;&#24212;&#21830;&#35782;&#21035;&#21644;&#20302;&#36164;&#28304;&#24066;&#22330;&#36866;&#24212;&#20219;&#21153;&#12290;&#36890;&#36807;VendorLink&#65292;&#25105;&#20204;&#22312;Alphabay-Dreams-Silk&#25968;&#25454;&#38598;&#20013;&#25581;&#31034;&#20102;15&#20010;&#31227;&#27665;&#21644;71&#20010;&#28508;&#22312;&#21035;&#21517;&#65292;&#22312;Valhalla-Berlusconi&#25968;&#25454;&#38598;&#20013;&#25581;&#31034;&#20102;17&#20010;&#31227;&#27665;&#21644;3&#20010;&#28508;&#22312;&#21035;&#21517;&#65292;&#22312;Traderoute-Agora&#25968;&#25454;&#38598;&#20013;&#25581;&#31034;&#20102;75&#20010;&#31227;&#27665;&#21644;10&#20010;&#28508;&#22312;&#21035;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
The anonymity on the Darknet allows vendors to stay undetected by using multiple vendor aliases or frequently migrating between markets. Consequently, illegal markets and their connections are challenging to uncover on the Darknet. To identify relationships between illegal markets and their vendors, we propose VendorLink, an NLP-based approach that examines writing patterns to verify, identify, and link unique vendor accounts across text advertisements (ads) on seven public Darknet markets. In contrast to existing literature, VendorLink utilizes the strength of supervised pre-training to perform closed-set vendor verification, open-set vendor identification, and low-resource market adaption tasks. Through VendorLink, we uncover (i) 15 migrants and 71 potential aliases in the Alphabay-Dreams-Silk dataset, (ii) 17 migrants and 3 potential aliases in the Valhalla-Berlusconi dataset, and (iii) 75 migrants and 10 potential aliases in the Traderoute-Agora dataset. Altogether, our approach ca
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#20840;&#38754;&#27010;&#36848;&#20102;&#19981;&#21516;&#31867;&#22411;&#23545;&#35805;&#20013;&#23545;&#35805;&#20195;&#29702;&#20027;&#21160;&#24615;&#30340;&#31361;&#20986;&#38382;&#39064;&#21644;&#20808;&#36827;&#35774;&#35745;&#65292;&#35752;&#35770;&#20102;&#31526;&#21512;&#23454;&#38469;&#24212;&#29992;&#38656;&#27714;&#20294;&#38656;&#35201;&#26410;&#26469;&#26356;&#22823;&#30740;&#31350;&#37325;&#28857;&#30340;&#25361;&#25112;&#65292;&#28608;&#21457;&#26356;&#22810;&#30340;&#20250;&#35805; AI &#36827;&#23637;&#21040;&#19979;&#19968;&#32423;&#21035;&#12290;</title><link>http://arxiv.org/abs/2305.02750</link><description>&lt;p&gt;
&#20027;&#21160;&#23545;&#35805;&#31995;&#32479;&#32508;&#36848;&#65306;&#38382;&#39064;&#12289;&#26041;&#27861;&#21644;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
A Survey on Proactive Dialogue Systems: Problems, Methods, and Prospects. (arXiv:2305.02750v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#20840;&#38754;&#27010;&#36848;&#20102;&#19981;&#21516;&#31867;&#22411;&#23545;&#35805;&#20013;&#23545;&#35805;&#20195;&#29702;&#20027;&#21160;&#24615;&#30340;&#31361;&#20986;&#38382;&#39064;&#21644;&#20808;&#36827;&#35774;&#35745;&#65292;&#35752;&#35770;&#20102;&#31526;&#21512;&#23454;&#38469;&#24212;&#29992;&#38656;&#27714;&#20294;&#38656;&#35201;&#26410;&#26469;&#26356;&#22823;&#30740;&#31350;&#37325;&#28857;&#30340;&#25361;&#25112;&#65292;&#28608;&#21457;&#26356;&#22810;&#30340;&#20250;&#35805; AI &#36827;&#23637;&#21040;&#19979;&#19968;&#32423;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23545;&#35805;&#31995;&#32479;&#19982;&#24191;&#27867;&#30340;&#29616;&#23454;&#19990;&#30028;&#23545;&#35805;&#24212;&#29992;&#30456;&#20851;&#65292;&#20351;&#23545;&#35805;&#20195;&#29702;&#33021;&#22815;&#24341;&#23548;&#23545;&#35805;&#26041;&#21521;&#65292;&#20197;&#23454;&#29616;&#39044;&#23450;&#20041;&#30340;&#30446;&#26631;&#25110;&#28385;&#36275;&#31995;&#32479;&#26041;&#38754;&#30340;&#29305;&#23450;&#30446;&#26631;&#12290;&#23427;&#36890;&#36807;&#20808;&#36827;&#25216;&#26415;&#36171;&#33021;&#20197;&#36827;&#23637;&#21040;&#38656;&#35201;&#25112;&#30053;&#24615;&#21644;&#28608;&#21169;&#24615;&#20132;&#20114;&#30340;&#26356;&#22797;&#26434;&#20219;&#21153;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#27010;&#36848;&#20102;&#19981;&#21516;&#31867;&#22411;&#23545;&#35805;&#20013;&#23545;&#35805;&#20195;&#29702;&#20027;&#21160;&#24615;&#30340;&#31361;&#20986;&#38382;&#39064;&#21644;&#20808;&#36827;&#35774;&#35745;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#31526;&#21512;&#23454;&#38469;&#24212;&#29992;&#38656;&#27714;&#20294;&#38656;&#35201;&#26410;&#26469;&#26356;&#22823;&#30740;&#31350;&#37325;&#28857;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#31687;&#20027;&#21160;&#23545;&#35805;&#31995;&#32479;&#30340;&#31532;&#19968;&#31687;&#32508;&#36848;&#21487;&#20197;&#20026;&#31038;&#21306;&#25552;&#20379;&#24555;&#36895;&#35775;&#38382;&#21644;&#25972;&#20307;&#22270;&#29255;&#65292;&#28608;&#21457;&#26356;&#22810;&#30340;&#20250;&#35805; AI &#36827;&#23637;&#21040;&#19979;&#19968;&#32423;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Proactive dialogue systems, related to a wide range of real-world conversational applications, equip the conversational agent with the capability of leading the conversation direction towards achieving pre-defined targets or fulfilling certain goals from the system side. It is empowered by advanced techniques to progress to more complicated tasks that require strategical and motivational interactions. In this survey, we provide a comprehensive overview of the prominent problems and advanced designs for conversational agent's proactivity in different types of dialogues. Furthermore, we discuss challenges that meet the real-world application needs but require a greater research focus in the future. We hope that this first survey of proactive dialogue systems can provide the community with a quick access and an overall picture to this practical problem, and stimulate more progresses on conversational AI to the next level.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#23545;&#35805;&#25968;&#25454;&#36827;&#34892;&#26080;&#30417;&#30563;&#23545;&#35805;&#20027;&#39064;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37051;&#36817;&#35805;&#35821;&#21305;&#37197;&#21644;&#20266;&#20998;&#21106;&#23398;&#20064;&#20027;&#39064;&#24863;&#30693;&#30340;&#35805;&#35821;&#34920;&#31034;&#65292;&#23454;&#39564;&#26174;&#31034;&#20854;&#26126;&#26174;&#20248;&#20110;&#24378;&#22522;&#20934;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.02747</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#23545;&#35805;&#20027;&#39064;&#20998;&#21106;&#21450;&#35805;&#35821;&#34920;&#31034;&#20013;&#20027;&#39064;&#24863;&#30693;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Dialogue Topic Segmentation with Topic-aware Utterance Representation. (arXiv:2305.02747v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02747
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#23545;&#35805;&#25968;&#25454;&#36827;&#34892;&#26080;&#30417;&#30563;&#23545;&#35805;&#20027;&#39064;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37051;&#36817;&#35805;&#35821;&#21305;&#37197;&#21644;&#20266;&#20998;&#21106;&#23398;&#20064;&#20027;&#39064;&#24863;&#30693;&#30340;&#35805;&#35821;&#34920;&#31034;&#65292;&#23454;&#39564;&#26174;&#31034;&#20854;&#26126;&#26174;&#20248;&#20110;&#24378;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#20027;&#39064;&#20998;&#21106;&#22312;&#21508;&#31181;&#23545;&#35805;&#24314;&#27169;&#20219;&#21153;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#29616;&#26377;&#30340;DTS&#26041;&#27861;&#35201;&#20040;&#20851;&#27880;&#35821;&#20041;&#30456;&#20284;&#24615;&#65292;&#35201;&#20040;&#20851;&#27880;&#23545;&#35805;&#36830;&#36143;&#24615;&#26469;&#35780;&#20272;&#20027;&#39064;&#30456;&#20284;&#24615;&#20197;&#36827;&#34892;&#26080;&#30417;&#30563;&#23545;&#35805;&#20998;&#21106;&#12290;&#20294;&#20027;&#39064;&#30456;&#20284;&#24615;&#19981;&#33021;&#36890;&#36807;&#35821;&#20041;&#30456;&#20284;&#24615;&#25110;&#23545;&#35805;&#36830;&#36143;&#24615;&#30340;&#26041;&#24335;&#26469;&#23436;&#20840;&#35782;&#21035;&#12290;&#27492;&#22806;&#65292;&#26410;&#26631;&#35760;&#30340;&#23545;&#35805;&#25968;&#25454;&#20013;&#21253;&#21547;&#26377;&#29992;&#30340;&#35805;&#35821;&#20851;&#31995;&#25552;&#31034;&#65292;&#20294;&#36825;&#20123;&#25552;&#31034;&#26410;&#34987;&#20805;&#20998;&#21033;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;DTS&#26694;&#26550;&#65292;&#36890;&#36807;&#37051;&#36817;&#35805;&#35821;&#21305;&#37197;&#21644;&#20266;&#20998;&#21106;&#20174;&#26410;&#26631;&#35760;&#30340;&#23545;&#35805;&#25968;&#25454;&#20013;&#23398;&#20064;&#20027;&#39064;&#24863;&#30693;&#30340;&#35805;&#35821;&#34920;&#31034;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65288;DialSeg711&#21644;Doc2Dial&#65289;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#20248;&#20110;&#24378;&#22522;&#20934;&#26041;&#27861;&#12290;&#20026;&#20102;&#21487;&#22797;&#29616;&#24615;&#65292;&#25105;&#20204;&#22312;https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/dial-start&#19978;&#25552;&#20379;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialogue Topic Segmentation (DTS) plays an essential role in a variety of dialogue modeling tasks. Previous DTS methods either focus on semantic similarity or dialogue coherence to assess topic similarity for unsupervised dialogue segmentation. However, the topic similarity cannot be fully identified via semantic similarity or dialogue coherence. In addition, the unlabeled dialogue data, which contains useful clues of utterance relationships, remains underexploited. In this paper, we propose a novel unsupervised DTS framework, which learns topic-aware utterance representations from unlabeled dialogue data through neighboring utterance matching and pseudo-segmentation. Extensive experiments on two benchmark datasets (i.e., DialSeg711 and Doc2Dial) demonstrate that our method significantly outperforms the strong baseline methods. For reproducibility, we provide our code and data at:https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/dial-start.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#27493;&#26356;&#26032;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550; (AURL) &#26469;&#35757;&#32451;&#38754;&#21521;&#20219;&#21153;&#22411;&#23545;&#35805;&#31995;&#32479;&#65292;&#35299;&#20915;&#20102;&#19981;&#21516;&#27169;&#22359;&#20114;&#30456;&#24433;&#21709;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#35838;&#31243;&#23398;&#20064;&#26469;&#35299;&#20915;&#25968;&#25454;&#20998;&#24067;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#22810;&#20010;&#29992;&#25143;&#27169;&#22411;&#26469;&#22686;&#21152;&#23545;&#35805;&#30340;&#22810;&#26679;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;31.37%&#30340;&#23545;&#35805;&#25104;&#21151;&#29575;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.02718</link><description>&lt;p&gt;
&#38754;&#21521;&#20219;&#21153;&#22411;&#23545;&#35805;&#31995;&#32479;&#30340;&#24322;&#27493;&#26356;&#26032;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
An Asynchronous Updating Reinforcement Learning Framework for Task-oriented Dialog System. (arXiv:2305.02718v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02718
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#27493;&#26356;&#26032;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550; (AURL) &#26469;&#35757;&#32451;&#38754;&#21521;&#20219;&#21153;&#22411;&#23545;&#35805;&#31995;&#32479;&#65292;&#35299;&#20915;&#20102;&#19981;&#21516;&#27169;&#22359;&#20114;&#30456;&#24433;&#21709;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#35838;&#31243;&#23398;&#20064;&#26469;&#35299;&#20915;&#25968;&#25454;&#20998;&#24067;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#22810;&#20010;&#29992;&#25143;&#27169;&#22411;&#26469;&#22686;&#21152;&#23545;&#35805;&#30340;&#22810;&#26679;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;31.37%&#30340;&#23545;&#35805;&#25104;&#21151;&#29575;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#23545;&#35805;&#31995;&#32479;&#20013;&#24050;&#32463;&#24212;&#29992;&#20102;&#24378;&#21270;&#23398;&#20064;&#26469;&#36827;&#34892;&#35757;&#32451;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#23558;&#23545;&#35805;&#31995;&#32479;&#20998;&#25104;&#22810;&#20010;&#27169;&#22359;&#65292;&#21253;&#25324;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394; (DST) &#21644;&#23545;&#35805;&#31574;&#30053; (DP)&#65292;&#24182;&#21516;&#26102;&#35757;&#32451;&#36825;&#20123;&#27169;&#22359;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#27169;&#22359;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20250;&#30456;&#20114;&#24433;&#21709;&#12290;&#26469;&#33258; DST &#30340;&#35823;&#24046;&#21487;&#33021;&#20250;&#35823;&#23548;&#23545;&#35805;&#31574;&#30053;&#65292;&#31995;&#32479;&#25805;&#20316;&#20063;&#20250;&#32473; DST &#27169;&#22359;&#24102;&#26469;&#39069;&#22806;&#30340;&#22256;&#38590;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#27493;&#26356;&#26032;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550; (AURL)&#65292;&#22312;&#21512;&#20316;&#35774;&#32622;&#19979;&#24322;&#27493;&#22320;&#26356;&#26032; DST &#27169;&#22359;&#21644; DP &#27169;&#22359;&#12290;&#27492;&#22806;&#65292;&#37319;&#29992;&#35838;&#31243;&#23398;&#20064;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#37319;&#26679;&#36807;&#31243;&#20013;&#30340;&#25968;&#25454;&#20998;&#24067;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#22810;&#20010;&#29992;&#25143;&#27169;&#22411;&#26469;&#22686;&#21152;&#23545;&#35805;&#30340;&#22810;&#26679;&#24615;&#12290;&#20844;&#20849; SSD-PHONE &#25968;&#25454;&#38598;&#19978;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23545;&#35805;&#25104;&#21151;&#29575;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;31.37%&#25913;&#36827;&#12290;&#20195;&#30721;&#21487;&#22312; https://github.com/thu-coai/AURL-Dialog-System &#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning has been applied to train the dialog systems in many works. Previous approaches divide the dialog system into multiple modules including DST (dialog state tracking) and DP (dialog policy), and train these modules simultaneously. However, different modules influence each other during training. The errors from DST might misguide the dialog policy, and the system action brings extra difficulties for the DST module. To alleviate this problem, we propose Asynchronous Updating Reinforcement Learning framework (AURL) that updates the DST module and the DP module asynchronously under a cooperative setting. Furthermore, curriculum learning is implemented to address the problem of unbalanced data distribution during reinforcement learning sampling, and multiple user models are introduced to increase the dialog diversity. Results on the public SSD-PHONE dataset show that our method achieves a compelling result with a 31.37% improvement on the dialog success rate. The code i
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25581;&#31034;&#20102;&#22823;&#25968;&#24615;&#36136;&#23545;&#20110;&#35299;&#37322;&#40784;&#26222;&#22827;&#23450;&#24459;&#30340;&#24433;&#21709;&#65292;&#25351;&#20986;&#40784;&#26222;&#22827;&#23450;&#24459;&#22122;&#38899;&#26159;&#36825;&#31181;&#24433;&#21709;&#30340;&#20363;&#23376;&#65292;&#24182;&#20998;&#26512;&#20102;&#26435;&#21147;&#20998;&#24067;&#21644;&#31867;&#20284;&#20998;&#24067;&#22312;&#31181;&#32676;&#26159;&#26377;&#38480;&#30340;&#12289;&#25490;&#21517;&#21644;&#20803;&#32032;&#35745;&#25968;&#26159;&#33258;&#28982;&#25968;&#30340;&#24773;&#20917;&#19979;&#30340;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.02687</link><description>&lt;p&gt;
&#22823;&#25968;&#25454;&#21644;&#22823;&#25968;&#30340;&#35299;&#37322;&#65306;&#35299;&#35835;&#40784;&#26222;&#22827;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
Big Data and Large Numbers. Interpreting Zipf's Law. (arXiv:2305.02687v1 [physics.soc-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02687
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25581;&#31034;&#20102;&#22823;&#25968;&#24615;&#36136;&#23545;&#20110;&#35299;&#37322;&#40784;&#26222;&#22827;&#23450;&#24459;&#30340;&#24433;&#21709;&#65292;&#25351;&#20986;&#40784;&#26222;&#22827;&#23450;&#24459;&#22122;&#38899;&#26159;&#36825;&#31181;&#24433;&#21709;&#30340;&#20363;&#23376;&#65292;&#24182;&#20998;&#26512;&#20102;&#26435;&#21147;&#20998;&#24067;&#21644;&#31867;&#20284;&#20998;&#24067;&#22312;&#31181;&#32676;&#26159;&#26377;&#38480;&#30340;&#12289;&#25490;&#21517;&#21644;&#20803;&#32032;&#35745;&#25968;&#26159;&#33258;&#28982;&#25968;&#30340;&#24773;&#20917;&#19979;&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20123;&#22823;&#25968;&#25454;&#39046;&#22495;&#30340;&#23454;&#35777;&#20107;&#23454;&#23646;&#20110;&#22823;&#25968;&#24615;&#36136;&#30340;&#24433;&#21709;&#12290;&#40784;&#26222;&#22827;&#23450;&#24459;&#22122;&#38899;&#23601;&#26159;&#36825;&#31181;&#29616;&#35937;&#30340;&#20363;&#23376;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#26435;&#21147;&#20998;&#24067;&#21644;&#31867;&#20284;&#20998;&#24067;&#30340;&#20960;&#20010;&#29305;&#24615;&#65292;&#36825;&#20123;&#20998;&#24067;&#21457;&#29983;&#22312;&#31181;&#32676;&#26159;&#26377;&#38480;&#30340;&#12289;&#25490;&#21517;&#21644;&#20803;&#32032;&#35745;&#25968;&#26159;&#33258;&#28982;&#25968;&#30340;&#24773;&#20917;&#19979;&#12290;&#35752;&#35770;&#20102;&#36825;&#20123;&#29305;&#24615;&#23545;&#35299;&#37322;&#40784;&#26222;&#22827;&#23450;&#24459;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
It turns out that some empirical facts in Big Data are the effects of properties of large numbers. Zipf's law noise is an example of such an artefact. We expose several properties of the power law distributions and of similar distribution that occur when the population is finite and the rank and counts of elements in the population are natural numbers. Consequences in the interpretation of Zipf's law are discussed.
&lt;/p&gt;</description></item><item><title>&#30456;&#37051;&#21333;&#35789;&#21487;&#20197;&#24433;&#21709;&#35299;&#37322;&#32773;&#23545;&#21333;&#35789;&#37325;&#35201;&#24615;&#30340;&#29702;&#35299;&#65292;&#24212;&#35813;&#32771;&#34385;&#25991;&#26412;&#20013;&#20854;&#20182;&#22240;&#32032;&#30340;&#24433;&#21709;&#26469;&#26367;&#20195;&#21333;&#35789;&#32423;&#21035;&#30340;&#26174;&#33879;&#24615;&#35299;&#37322;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.02679</link><description>&lt;p&gt;
&#30456;&#37051;&#21333;&#35789;&#24433;&#21709;&#20154;&#31867;&#23545;&#26174;&#33879;&#24615;&#35299;&#37322;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Neighboring Words Affect Human Interpretation of Saliency Explanations. (arXiv:2305.02679v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02679
&lt;/p&gt;
&lt;p&gt;
&#30456;&#37051;&#21333;&#35789;&#21487;&#20197;&#24433;&#21709;&#35299;&#37322;&#32773;&#23545;&#21333;&#35789;&#37325;&#35201;&#24615;&#30340;&#29702;&#35299;&#65292;&#24212;&#35813;&#32771;&#34385;&#25991;&#26412;&#20013;&#20854;&#20182;&#22240;&#32032;&#30340;&#24433;&#21709;&#26469;&#26367;&#20195;&#21333;&#35789;&#32423;&#21035;&#30340;&#26174;&#33879;&#24615;&#35299;&#37322;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#35789;&#32423;&#21035;&#30340;&#26174;&#33879;&#24615;&#35299;&#37322;&#65288;&#8220;&#21333;&#35789;&#28909;&#22270;&#8221;&#65289;&#32463;&#24120;&#29992;&#20110;&#22312;&#22522;&#20110;&#25991;&#26412;&#30340;&#27169;&#22411;&#20013;&#20256;&#36798;&#29305;&#24449;&#24402;&#22240;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#35832;&#22914;&#21333;&#35789;&#38271;&#24230;&#31561;&#34920;&#38754;&#22240;&#32032;&#21487;&#33021;&#20250;&#25197;&#26354;&#20256;&#36798;&#30340;&#26174;&#33879;&#24615;&#35780;&#20998;&#30340;&#20154;&#31867;&#35299;&#35835;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#29992;&#25143;&#30740;&#31350;&#65292;&#20197;&#30740;&#31350;&#19968;&#20010;&#21333;&#35789;&#30340;&#30456;&#37051;&#21333;&#35789;&#30340;&#26631;&#35760;&#22914;&#20309;&#24433;&#21709;&#35299;&#37322;&#32773;&#23545;&#26174;&#33879;&#24615;&#35299;&#37322;&#20013;&#21333;&#35789;&#30340;&#37325;&#35201;&#24615;&#30340;&#24863;&#30693;&#12290;&#25105;&#20204;&#21457;&#29616;&#30456;&#37051;&#21333;&#35789;&#23545;&#21333;&#35789;&#30340;&#37325;&#35201;&#24615;&#35780;&#20998;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#24433;&#21709;&#22522;&#20110;&#30456;&#37051;&#26041;&#21521;&#65288;&#24038;&#20391;&#19982;&#21491;&#20391;&#65289;&#20197;&#21450;&#30701;&#35821;&#21644;&#25645;&#37197;&#30340;&#20808;&#39564;&#35821;&#35328;&#21644;&#35745;&#31639;&#24230;&#37327;&#20540;&#65288;&#19982;&#19981;&#30456;&#20851;&#30340;&#30456;&#37051;&#21333;&#35789;&#65289;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#35753;&#20154;&#36136;&#30097;&#22312;&#25991;&#26412;&#20026;&#22522;&#30784;&#30340;&#26174;&#33879;&#24615;&#35299;&#37322;&#26159;&#21542;&#24212;&#35813;&#32487;&#32493;&#22312;&#21333;&#35789;&#32423;&#21035;&#19978;&#36827;&#34892;&#20256;&#36798;&#65292;&#24182;&#20026;&#26367;&#20195;&#26174;&#33879;&#24615;&#35299;&#37322;&#26041;&#27861;&#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Word-level saliency explanations ("heat maps over words") are often used to communicate feature-attribution in text-based models. Recent studies found that superficial factors such as word length can distort human interpretation of the communicated saliency scores. We conduct a user study to investigate how the marking of a word's neighboring words affect the explainee's perception of the word's importance in the context of a saliency explanation. We find that neighboring words have significant effects on the word's importance rating. Concretely, we identify that the influence changes based on neighboring direction (left vs. right) and a-priori linguistic and computational measures of phrases and collocations (vs. unrelated neighboring words). Our results question whether text-based saliency explanations should be continued to be communicated at word level, and inform future research on alternative saliency explanation methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#35821;&#35328;&#29305;&#24322;Transformer&#23618;&#65288;LSLs&#65289;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#22686;&#21152;&#27169;&#22411;&#23481;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#27491;&#21521;&#20256;&#36882;&#20013;&#20351;&#29992;&#30340;&#35745;&#31639;&#37327;&#21644;&#21442;&#25968;&#25968;&#37327;&#19981;&#21464;&#65292;&#20174;&#32780;&#25552;&#39640;&#22810;&#35821;&#26426;&#22120;&#32763;&#35793;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.02665</link><description>&lt;p&gt;
&#23398;&#20064;&#22810;&#35821;&#26426;&#22120;&#32763;&#35793;&#30340;&#35821;&#35328;&#29305;&#24322;&#23618;
&lt;/p&gt;
&lt;p&gt;
Learning Language-Specific Layers for Multilingual Machine Translation. (arXiv:2305.02665v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02665
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#35821;&#35328;&#29305;&#24322;Transformer&#23618;&#65288;LSLs&#65289;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#22686;&#21152;&#27169;&#22411;&#23481;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#27491;&#21521;&#20256;&#36882;&#20013;&#20351;&#29992;&#30340;&#35745;&#31639;&#37327;&#21644;&#21442;&#25968;&#25968;&#37327;&#19981;&#21464;&#65292;&#20174;&#32780;&#25552;&#39640;&#22810;&#35821;&#26426;&#22120;&#32763;&#35793;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#26426;&#22120;&#32763;&#35793;&#21487;&#20197;&#25552;&#39640;&#38750;&#33521;&#35821;&#35821;&#35328;&#20043;&#38388;&#30340;&#32763;&#35793;&#36136;&#37327;&#65292;&#22312;&#35768;&#22810;&#26041;&#38754;&#37117;&#26377;&#20248;&#21183;&#65292;&#20363;&#22914;&#26356;&#20302;&#30340;&#24310;&#36831;&#65288;&#26080;&#38656;&#32763;&#35793;&#20004;&#27425;&#65289;&#21644;&#20943;&#23569;&#38169;&#35823;&#32423;&#32852;&#65288;&#20363;&#22914;&#65292;&#22312;&#36890;&#36807;&#33521;&#35821;&#36827;&#34892;&#32763;&#35793;&#26102;&#36991;&#20813;&#20002;&#22833;&#24615;&#21035;&#21644;&#31036;&#35980;&#31561;&#20449;&#24687;&#65289;&#12290;&#20294;&#26159;&#65292;&#28155;&#21152;&#26356;&#22810;&#35821;&#35328;&#20250;&#20943;&#23569;&#27599;&#31181;&#35821;&#35328;&#30340;&#27169;&#22411;&#23481;&#37327;&#65292;&#36890;&#24120;&#36890;&#36807;&#22686;&#21152;&#24635;&#20307;&#27169;&#22411;&#22823;&#23567;&#26469;&#25269;&#28040;&#65292;&#20174;&#32780;&#20351;&#35757;&#32451;&#26356;&#21152;&#22256;&#38590;&#65292;&#25512;&#29702;&#36895;&#24230;&#21464;&#24930;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#35821;&#35328;&#29305;&#24322;Transformer&#23618;&#65288;LSLs&#65289;&#65292;&#23427;&#20204;&#20351;&#25105;&#20204;&#33021;&#22815;&#22686;&#21152;&#27169;&#22411;&#23481;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#27491;&#21521;&#20256;&#36882;&#20013;&#20351;&#29992;&#30340;&#35745;&#31639;&#37327;&#21644;&#21442;&#25968;&#25968;&#37327;&#19981;&#21464;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#35753;&#32534;&#30721;&#22120;&#30340;&#26576;&#20123;&#23618;&#20026;&#28304;&#35821;&#35328;&#25110;&#30446;&#26631;&#35821;&#35328;&#29305;&#24322;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#20313;&#23618;&#20849;&#20139;&#12290;&#25105;&#20204;&#20351;&#29992;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#21551;&#21457;&#24335;&#26041;&#27861;&#30740;&#31350;&#20102;&#25918;&#32622;&#36825;&#20123;&#23618;&#30340;&#26368;&#20339;&#26041;&#27861;&#65292;&#24182;&#23454;&#29616;&#20102;1.3 chrF&#65288;1.5 spBLE&#65289;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual Machine Translation promises to improve translation quality between non-English languages. This is advantageous for several reasons, namely lower latency (no need to translate twice), and reduced error cascades (e.g., avoiding losing gender and formality information when translating through English). On the downside, adding more languages reduces model capacity per language, which is usually countered by increasing the overall model size, making training harder and inference slower. In this work, we introduce Language-Specific Transformer Layers (LSLs), which allow us to increase model capacity, while keeping the amount of computation and the number of parameters used in the forward pass constant. The key idea is to have some layers of the encoder be source or target language-specific, while keeping the remaining layers shared. We study the best way to place these layers using a neural architecture search inspired approach, and achieve an improvement of 1.3 chrF (1.5 spBLE
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#26497;&#24230;&#24369;&#30340;&#30417;&#30563;&#26041;&#27861;&#65292;&#21482;&#20381;&#36182;&#20110;&#31867;&#21035;&#21517;&#31216;&#32780;&#19981;&#26159;&#27880;&#37322;&#25968;&#25454;&#20013;&#30340;&#31867;&#21035;&#31034;&#20363;&#65292;&#35299;&#20915;&#24403;&#21069;&#20167;&#24680;&#35328;&#35770;&#35782;&#21035;&#30340;&#30740;&#31350;&#23384;&#22312;&#30340;&#25968;&#25454;&#21019;&#24314;&#31574;&#30053;&#19981;&#31995;&#32479;&#21644;&#19981;&#21516;&#27880;&#37322;&#26041;&#26696;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.02637</link><description>&lt;p&gt;
&#38754;&#21521;&#36328;&#25968;&#25454;&#38598;&#30340;&#24369;&#30417;&#30563;&#20167;&#24680;&#35328;&#35770;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Towards Weakly-Supervised Hate Speech Classification Across Datasets. (arXiv:2305.02637v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02637
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#26497;&#24230;&#24369;&#30340;&#30417;&#30563;&#26041;&#27861;&#65292;&#21482;&#20381;&#36182;&#20110;&#31867;&#21035;&#21517;&#31216;&#32780;&#19981;&#26159;&#27880;&#37322;&#25968;&#25454;&#20013;&#30340;&#31867;&#21035;&#31034;&#20363;&#65292;&#35299;&#20915;&#24403;&#21069;&#20167;&#24680;&#35328;&#35770;&#35782;&#21035;&#30340;&#30740;&#31350;&#23384;&#22312;&#30340;&#25968;&#25454;&#21019;&#24314;&#31574;&#30053;&#19981;&#31995;&#32479;&#21644;&#19981;&#21516;&#27880;&#37322;&#26041;&#26696;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#22810;&#20301;&#23398;&#32773;&#25351;&#20986;&#30340;&#37027;&#26679;&#65292;&#24403;&#21069;&#38024;&#23545;&#20167;&#24680;&#35328;&#35770;&#65288;HS&#65289;&#35782;&#21035;&#30340;&#30740;&#31350;&#29305;&#28857;&#26159;&#19981;&#31995;&#32479;&#30340;&#25968;&#25454;&#21019;&#24314;&#31574;&#30053;&#21644;&#19981;&#21516;&#30340;&#27880;&#37322;&#26041;&#26696;&#12290;&#22240;&#27492;&#65292;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#24448;&#24448;&#23545;&#23427;&#20204;&#26410;&#34987;&#35757;&#32451;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#27867;&#21270;&#24615;&#33021;&#24046;&#65292;&#24182;&#19988;&#19981;&#21516;HS&#20998;&#31867;&#27861;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#25152;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#26080;&#27861;&#27604;&#36739;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26497;&#24230;&#24369;&#30340;&#30417;&#30563;&#26041;&#27861;&#65292;&#21482;&#20381;&#36182;&#20110;&#31867;&#21035;&#21517;&#31216;&#32780;&#19981;&#26159;&#27880;&#37322;&#25968;&#25454;&#20013;&#30340;&#31867;&#21035;&#31034;&#20363;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#20869;&#21644;&#36328;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;HS&#20998;&#31867;&#27169;&#22411;&#36890;&#29992;&#24615;&#36739;&#24046;&#30340;&#21407;&#22240;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
As pointed out by several scholars, current research on hate speech (HS) recognition is characterized by unsystematic data creation strategies and diverging annotation schemata. Subsequently, supervised-learning models tend to generalize poorly to datasets they were not trained on, and the performance of the models trained on datasets labeled using different HS taxonomies cannot be compared. To ease this problem, we propose applying extremely weak supervision that only relies on the class name rather than on class samples from the annotated data. We demonstrate the effectiveness of a state-of-the-art weakly-supervised text classification model in various in-dataset and cross-dataset settings. Furthermore, we conduct an in-depth quantitative and qualitative analysis of the source of poor generalizability of HS classification models.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#31526;&#21512;&#35821;&#35328;&#27169;&#22411;&#30340;&#26680;&#24515;&#25277;&#26679;&#65292;&#24182;&#37319;&#29992;&#31526;&#21512;&#24615;&#39044;&#27979;&#36827;&#34892;&#26657;&#20934;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;OPT&#27169;&#22411;&#36807;&#20110;&#33258;&#20449;&#65292;&#24182;&#19988;&#26657;&#20934;&#26174;&#31034;&#20986;&#20013;&#24230;&#30340;&#36870;&#27604;&#20363;&#32553;&#25918;&#19982;&#27169;&#22411;&#22823;&#23567;&#12290;</title><link>http://arxiv.org/abs/2305.02633</link><description>&lt;p&gt;
&#31526;&#21512;&#35821;&#35328;&#27169;&#22411;&#30340;&#26680;&#24515;&#25277;&#26679;
&lt;/p&gt;
&lt;p&gt;
Conformal Nucleus Sampling. (arXiv:2305.02633v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#31526;&#21512;&#35821;&#35328;&#27169;&#22411;&#30340;&#26680;&#24515;&#25277;&#26679;&#65292;&#24182;&#37319;&#29992;&#31526;&#21512;&#24615;&#39044;&#27979;&#36827;&#34892;&#26657;&#20934;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;OPT&#27169;&#22411;&#36807;&#20110;&#33258;&#20449;&#65292;&#24182;&#19988;&#26657;&#20934;&#26174;&#31034;&#20986;&#20013;&#24230;&#30340;&#36870;&#27604;&#20363;&#32553;&#25918;&#19982;&#27169;&#22411;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#30340;&#36807;&#31243;&#26159;&#22522;&#20110;&#20381;&#27425;&#25277;&#26679;&#19979;&#19968;&#20010;&#21333;&#35789;&#12290;&#22522;&#20110;&#26680;&#24515;&#65288;top-p&#65289;&#25277;&#26679;&#30340;&#35299;&#30721;&#36807;&#31243;&#20250;&#20174;&#26368;&#23567;&#21487;&#33021;&#30340;&#21333;&#35789;&#38598;&#20013;&#36873;&#25321;&#65292;&#36825;&#20123;&#21333;&#35789;&#30340;&#32047;&#35745;&#27010;&#29575;&#36229;&#36807;&#27010;&#29575;p&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22312;&#21508;&#31181;&#35821;&#35328;&#29615;&#22659;&#19979;&#65292;top-p&#38598;&#26159;&#21542;&#30495;&#27491;&#19982;&#20854;&#27010;&#29575;&#21547;&#20041;&#23545;&#40784;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#31526;&#21512;&#24615;&#39044;&#27979;&#65292;&#36825;&#26159;&#19968;&#31181;&#26657;&#20934;&#31243;&#24207;&#65292;&#26681;&#25454;&#25152;&#38656;&#30340;&#32622;&#20449;&#27700;&#24179;&#65292;&#19987;&#27880;&#20110;&#26500;&#24314;&#26368;&#23567;&#39044;&#27979;&#38598;&#65292;&#20197;&#26657;&#20934;&#21442;&#25968;p&#20316;&#20026;&#19979;&#19968;&#20010;&#21333;&#35789;&#20998;&#24067;&#29109;&#30340;&#20989;&#25968;&#12290;&#25105;&#20204;&#21457;&#29616;OPT&#27169;&#22411;&#36807;&#20110;&#33258;&#20449;&#65292;&#26657;&#20934;&#26174;&#31034;&#20986;&#20013;&#24230;&#30340;&#36870;&#27604;&#20363;&#32553;&#25918;&#19982;&#27169;&#22411;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models generate text based on successively sampling the next word. A decoding procedure based on nucleus (top-$p$) sampling chooses from the smallest possible set of words whose cumulative probability exceeds the probability $p$. In this work, we assess whether a top-$p$ set is indeed aligned with its probabilistic meaning in various linguistic contexts. We employ conformal prediction, a calibration procedure that focuses on the construction of minimal prediction sets according to a desired confidence level, to calibrate the parameter $p$ as a function of the entropy of the next word distribution. We find that OPT models are overconfident, and that calibration shows a moderate inverse scaling with model size.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#25311;&#35821;&#35328;&#29305;&#24449;&#30340;&#36890;&#20449;&#21327;&#35758;&#65292;&#29992;&#20110;&#20998;&#26512;&#20010;&#20307;&#21644;&#20849;&#20139;&#25277;&#35937;&#30340;&#24418;&#25104;&#21450;&#20854;&#23545;&#20219;&#21153;&#34920;&#29616;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#20248;&#21270;&#20449;&#24687;&#20869;&#23481;&#20197;&#26368;&#22823;&#21270;&#23398;&#29983;&#22870;&#21169;&#25913;&#21892;&#20102;&#20449;&#24687;&#32534;&#30721;&#65292;&#25552;&#39640;&#20102;&#23398;&#20064;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.02632</link><description>&lt;p&gt;
&#19968;&#31181;&#31038;&#20132;&#23398;&#20064;&#20195;&#29702;&#20013;&#35821;&#35328;&#20986;&#29616;&#19982;&#20998;&#26512;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A framework for the emergence and analysis of language in social learning agents. (arXiv:2305.02632v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02632
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#25311;&#35821;&#35328;&#29305;&#24449;&#30340;&#36890;&#20449;&#21327;&#35758;&#65292;&#29992;&#20110;&#20998;&#26512;&#20010;&#20307;&#21644;&#20849;&#20139;&#25277;&#35937;&#30340;&#24418;&#25104;&#21450;&#20854;&#23545;&#20219;&#21153;&#34920;&#29616;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#20248;&#21270;&#20449;&#24687;&#20869;&#23481;&#20197;&#26368;&#22823;&#21270;&#23398;&#29983;&#22870;&#21169;&#25913;&#21892;&#20102;&#20449;&#24687;&#32534;&#30721;&#65292;&#25552;&#39640;&#20102;&#23398;&#20064;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#36234;&#26469;&#36234;&#34987;&#29992;&#20316;&#30740;&#31350;&#27169;&#22411;&#65292;&#20294;&#23427;&#20204;&#30340;&#26222;&#36866;&#24615;&#21644;&#34920;&#24449;&#19981;&#21464;&#24615;&#20173;&#23384;&#22312;&#38382;&#39064;&#12290;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#22312;&#31038;&#20132;&#32422;&#26463;&#19979;&#28436;&#21270;&#24418;&#25104;&#21487;&#20256;&#36798;&#30340;&#34920;&#24449;&#65292;&#23637;&#31034;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21512;&#20316;&#20195;&#29702;&#20043;&#38388;&#30340;&#36890;&#20449;&#21327;&#35758;&#65292;&#29992;&#20110;&#20998;&#26512;&#20010;&#20307;&#21644;&#20849;&#20139;&#25277;&#35937;&#30340;&#24418;&#25104;&#21450;&#20854;&#23545;&#20219;&#21153;&#34920;&#29616;&#30340;&#24433;&#21709;&#12290;&#35813;&#36890;&#20449;&#21327;&#35758;&#26088;&#22312;&#36890;&#36807;&#20302;&#32500;&#34920;&#31034;&#32534;&#30721;&#39640;&#32500;&#20449;&#24687;&#65292;&#27169;&#25311;&#35821;&#35328;&#29305;&#24449;&#12290;&#20351;&#29992;&#32593;&#26684;&#19990;&#30028;&#36855;&#23467;&#21644;&#24378;&#21270;&#23398;&#20064;&#65292;&#25945;&#24072;ANNs&#21521;&#23398;&#29983;ANN&#20256;&#36882;&#21387;&#32553;&#28040;&#24687;&#65292;&#20197;&#20415;&#26356;&#22909;&#22320;&#23436;&#25104;&#20219;&#21153;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#23398;&#29983;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#30446;&#26631;&#21457;&#29616;&#29575;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#20219;&#21153;&#19990;&#30028;&#20013;&#25512;&#24191;&#20102;&#30446;&#26631;&#20301;&#32622;&#12290;&#36827;&#19968;&#27493;&#20248;&#21270;&#20449;&#24687;&#20869;&#23481;&#20197;&#26368;&#22823;&#21270;&#23398;&#29983;&#22870;&#21169;&#25913;&#21892;&#20102;&#20449;&#24687;&#32534;&#30721;&#65292;&#34920;&#26126;&#31934;&#30830;&#30340;&#34920;&#24449;&#21487;&#20197;&#22312;&#36890;&#20449;&#21327;&#35758;&#20013;&#24471;&#21040;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial neural networks (ANNs) are increasingly used as research models, but questions remain about their generalizability and representational invariance. Biological neural networks under social constraints evolved to enable communicable representations, demonstrating generalization capabilities. This study proposes a communication protocol between cooperative agents to analyze the formation of individual and shared abstractions and their impact on task performance. This communication protocol aims to mimic language features by encoding high-dimensional information through low-dimensional representation. Using grid-world mazes and reinforcement learning, teacher ANNs pass a compressed message to a student ANN for better task completion. Through this, the student achieves a higher goal-finding rate and generalizes the goal location across task worlds. Further optimizing message content to maximize student reward improves information encoding, suggesting that an accurate representati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20250;&#35805;&#24773;&#24863;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65288;CACD&#65289;&#65292;&#24182;&#36890;&#36807;&#35774;&#35745;&#20844;&#20849;&#39592;&#26550;&#21644;&#29983;&#25104;&#26367;&#20195;&#38544;&#21547;&#21407;&#22240;&#35299;&#20915;&#20102;&#22240;&#26524;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#38544;&#21547;&#21407;&#22240;&#30340;&#19981;&#21487;&#35266;&#23519;&#24615;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#21464;&#38271;&#20250;&#35805;&#20013;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2305.02615</link><description>&lt;p&gt;
&#20250;&#35805;&#20013;&#30340;&#24773;&#24863;&#25512;&#29702;&#65306;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Affective Reasoning at Utterance Level in Conversations: A Causal Discovery Approach. (arXiv:2305.02615v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02615
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20250;&#35805;&#24773;&#24863;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65288;CACD&#65289;&#65292;&#24182;&#36890;&#36807;&#35774;&#35745;&#20844;&#20849;&#39592;&#26550;&#21644;&#29983;&#25104;&#26367;&#20195;&#38544;&#21547;&#21407;&#22240;&#35299;&#20915;&#20102;&#22240;&#26524;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#38544;&#21547;&#21407;&#22240;&#30340;&#19981;&#21487;&#35266;&#23519;&#24615;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#21464;&#38271;&#20250;&#35805;&#20013;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#25512;&#29702;&#20219;&#21153;&#26159;&#21253;&#25324;&#20250;&#35805;&#20013;&#30340;&#24773;&#24863;&#35782;&#21035;&#12289;&#24773;&#24863;-&#21407;&#22240;&#23545;&#25277;&#21462;&#21644;&#24773;&#24863;-&#21407;&#22240;&#36328;&#24230;&#35782;&#21035;&#22312;&#20869;&#30340;&#19968;&#32452;&#26032;&#20852;&#30340;&#22522;&#20110;&#24773;&#24863;&#30340;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#20551;&#35774;&#34920;&#38754;&#20851;&#31995;&#26102;&#24573;&#30053;&#20102;&#22522;&#26412;&#30340;&#22240;&#26524;&#27169;&#22411;&#65292;&#22240;&#20026;&#39592;&#26550;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#38544;&#21547;&#21407;&#22240;&#30340;&#19981;&#21487;&#35266;&#23519;&#24615;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#19978;&#36848;&#20004;&#20010;&#38382;&#39064;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#20250;&#35805;&#24773;&#24863;&#22240;&#26524;&#21457;&#29616;&#65288;CACD&#65289;&#26041;&#27861;&#12290;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#35774;&#35745;&#20844;&#20849;&#39592;&#26550;&#21644;&#29983;&#25104;&#26367;&#20195;&#38544;&#21547;&#21407;&#22240;&#26469;&#21457;&#29616;&#20250;&#35805;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;CACD&#21253;&#21547;&#20004;&#20010;&#27493;&#39588;&#65306;&#65288;i&#65289;&#20026;&#21464;&#38271;&#20250;&#35805;&#20013;&#30340;&#25152;&#26377;&#35805;&#35821;&#24314;&#31435;&#19968;&#20010;&#20013;&#24515;&#21270;&#30340;&#21333;&#19968;&#22270;&#33410;&#28857;&#22240;&#26524;&#39592;&#26550;&#65307;&#65288;ii&#65289;&#22240;&#26524;&#33258;&#32534;&#30721;&#22120;&#65288;CAE&#65289;&#36890;&#36807;&#29983;&#25104;&#38544;&#21547;&#21407;&#22240;&#21644;&#24050;&#30693;&#26174;&#24335;&#21407;&#22240;&#26469;&#20462;&#27491;&#39592;&#26550;&#65292;&#20174;&#32780;&#20135;&#29983;&#22240;&#26524;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The affective reasoning task is a set of emerging affect-based tasks in conversation, including Emotion Recognition in Conversation (ERC),Emotion-Cause Pair Extraction (ECPE), and Emotion-Cause Span Recognition (ECSR). Existing methods make various assumptions on the apparent relationship while neglecting the essential causal model due to the nonuniqueness of skeletons and unobservability of implicit causes. This paper settled down the above two problems and further proposed Conversational Affective Causal Discovery (CACD). It is a novel causal discovery method showing how to discover causal relationships in a conversation via designing a common skeleton and generating a substitute for implicit causes. CACD contains two steps: (i) building a common centering one graph node causal skeleton for all utterances in variable-length conversations; (ii) Causal Auto-Encoder (CAE) correcting the skeleton to yield causal representation through generated implicit causes and known explicit causes. 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;SemEval-2023&#20302;&#36164;&#28304;&#38750;&#27954;&#35821;&#35328;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#29992;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#21462;&#24471;&#20102;&#31532;&#19977;&#21517;&#25104;&#32489;&#65292;&#24182;&#31361;&#26174;&#20102;&#24320;&#21457;&#38024;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#26377;&#25928;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.02607</link><description>&lt;p&gt;
SemEval-2023&#20219;&#21153;12&#20013;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#25991;&#26412;&#20998;&#31867;&#65306;&#36890;&#36807;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
DN at SemEval-2023 Task 12: Low-Resource Language Text Classification via Multilingual Pretrained Language Model Fine-tuning. (arXiv:2305.02607v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02607
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;SemEval-2023&#20302;&#36164;&#28304;&#38750;&#27954;&#35821;&#35328;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#29992;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#21462;&#24471;&#20102;&#31532;&#19977;&#21517;&#25104;&#32489;&#65292;&#24182;&#31361;&#26174;&#20102;&#24320;&#21457;&#38024;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#26377;&#25928;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#24773;&#24863;&#20998;&#26512;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#21464;&#24471;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#29992;&#20110;&#24773;&#24863;&#20998;&#26512;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#37117;&#26159;&#38024;&#23545;&#39640;&#36164;&#28304;&#35821;&#35328;&#65292;&#27604;&#22914;&#33521;&#35821;&#21644;&#20013;&#25991;&#24320;&#21457;&#30340;&#65292;&#32780;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#29305;&#21035;&#26159;&#38750;&#27954;&#35821;&#35328;&#65292;&#24448;&#24448;&#40092;&#26377;&#30740;&#31350;&#12290;AfriSenti-SemEval 2023&#20849;&#20139;&#20219;&#21153;12&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#36890;&#36807;&#22312;&#20302;&#36164;&#28304;&#30340;&#38750;&#27954;&#35821;&#35328;&#19978;&#35780;&#20272;&#24773;&#24863;&#20998;&#26512;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#25105;&#20204;&#38024;&#23545;&#20849;&#20139;&#20219;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19981;&#21516;&#30340;&#22810;&#35821;&#35328;XLM-R&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#22312;&#38750;&#27954;&#26041;&#35328;&#19978;&#37325;&#26032;&#35757;&#32451;&#24182;&#22312;&#30446;&#26631;&#35821;&#35328;&#19978;&#24494;&#35843;&#30340;&#20998;&#31867;&#22836;&#35757;&#32451;&#20102;&#36825;&#20123;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#22242;&#38431;&#22312;Subtask B&#65292;Track 16: Multilingual&#20013;&#21462;&#24471;&#20102;&#31532;&#19977;&#22909;&#30340;&#25104;&#32489;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#34429;&#28982;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22810;&#35821;&#35328;&#25968;&#25454;&#19978;&#34920;&#29616;&#20986;&#20102;&#30456;&#23545;&#19981;&#38169;&#30340;&#32467;&#26524;&#65292;&#20294;&#22312;&#26576;&#20123;&#35821;&#35328;&#19978;&#34920;&#29616;&#36739;&#24046;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#24378;&#35843;&#20102;&#24320;&#21457;&#26356;&#22810;&#38024;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#26377;&#25928;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, sentiment analysis has gained significant importance in natural language processing. However, most existing models and datasets for sentiment analysis are developed for high-resource languages, such as English and Chinese, leaving low-resource languages, particularly African languages, largely unexplored. The AfriSenti-SemEval 2023 Shared Task 12 aims to fill this gap by evaluating sentiment analysis models on low-resource African languages. In this paper, we present our solution to the shared task, where we employed different multilingual XLM-R models with classification head trained on various data, including those retrained in African dialects and fine-tuned on target languages. Our team achieved the third-best results in Subtask B, Track 16: Multilingual, demonstrating the effectiveness of our approach. While our model showed relatively good results on multilingual data, it performed poorly in some languages. Our findings highlight the importance of developing more
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#20851;&#38190;&#35789;&#39044;&#27979;&#30340;&#20195;&#34920;&#24615;&#30740;&#31350;&#65292;&#20998;&#26512;&#20102;&#20027;&#35201;&#27169;&#22411;&#12289;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20851;&#38190;&#35789;&#39044;&#27979;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#39564;&#27604;&#36739;&#20195;&#34920;&#24615;&#27169;&#22411;&#65292;&#20026;&#28145;&#20837;&#20998;&#26512;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#25552;&#20379;&#20102;&#21487;&#27604;&#24615;&#30340;&#25968;&#25454;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2305.02579</link><description>&lt;p&gt;
&#20174;&#32479;&#35745;&#26041;&#27861;&#21040;&#28145;&#24230;&#23398;&#20064;&#65292;&#33258;&#21160;&#20851;&#38190;&#35789;&#39044;&#27979;&#65306;&#19968;&#27425;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
From Statistical Methods to Deep Learning, Automatic Keyphrase Prediction: A Survey. (arXiv:2305.02579v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#20851;&#38190;&#35789;&#39044;&#27979;&#30340;&#20195;&#34920;&#24615;&#30740;&#31350;&#65292;&#20998;&#26512;&#20102;&#20027;&#35201;&#27169;&#22411;&#12289;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20851;&#38190;&#35789;&#39044;&#27979;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#39564;&#27604;&#36739;&#20195;&#34920;&#24615;&#27169;&#22411;&#65292;&#20026;&#28145;&#20837;&#20998;&#26512;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#25552;&#20379;&#20102;&#21487;&#27604;&#24615;&#30340;&#25968;&#25454;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#38190;&#35789;&#39044;&#27979;&#26088;&#22312;&#29983;&#25104;&#39640;&#24230;&#24635;&#32467;&#32473;&#23450;&#25991;&#26723;&#30340;&#30701;&#35821;&#65288;&#20851;&#38190;&#35789;&#65289;&#12290;&#36817;&#24180;&#26469;&#65292;&#30740;&#31350;&#20154;&#21592;&#20174;&#19981;&#21516;&#30340;&#35270;&#35282;&#23545;&#36825;&#20010;&#20219;&#21153;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#20174;&#20027;&#35201;&#27169;&#22411;&#12289;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#30340;&#35282;&#24230;&#20840;&#38754;&#24635;&#32467;&#20102;&#20195;&#34920;&#24615;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#22810;&#36798;167&#31687;&#20808;&#21069;&#30340;&#20316;&#21697;&#65292;&#27604;&#20197;&#21069;&#30340;&#35843;&#26597;&#23454;&#29616;&#20102;&#26356;&#22823;&#33539;&#22260;&#30340;&#35206;&#30422;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#39640;&#24230;&#20851;&#27880;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20851;&#38190;&#35789;&#39044;&#27979;&#65292;&#22312;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20960;&#32452;&#23454;&#39564;&#65292;&#20180;&#32454;&#27604;&#36739;&#20102;&#20195;&#34920;&#24615;&#27169;&#22411;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;&#30456;&#21516;&#24120;&#29992;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#26469;&#27604;&#36739;&#36825;&#20123;&#27169;&#22411;&#30340;&#23581;&#35797;&#65292;&#26377;&#21161;&#20110;&#28145;&#20837;&#20998;&#26512;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26410;&#26469;&#35813;&#20219;&#21153;&#30340;&#21487;&#33021;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Keyphrase prediction aims to generate phrases (keyphrases) that highly summarizes a given document. Recently, researchers have conducted in-depth studies on this task from various perspectives. In this paper, we comprehensively summarize representative studies from the perspectives of dominant models, datasets and evaluation metrics. Our work analyzes up to 167 previous works, achieving greater coverage of this task than previous surveys. Particularly, we focus highly on deep learning-based keyphrase prediction, which attracts increasing attention of this task in recent years. Afterwards, we conduct several groups of experiments to carefully compare representative models. To the best of our knowledge, our work is the first attempt to compare these models using the identical commonly-used datasets and evaluation metric, facilitating in-depth analyses of their disadvantages and advantages. Finally, we discuss the possible research directions of this task in the future.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DupMAE&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#21033;&#29992;&#20004;&#20010;&#33258;&#21160;&#32534;&#30721;&#20219;&#21153;&#26469;&#25552;&#39640;&#35821;&#20041;&#34920;&#31034;&#36136;&#37327;&#65292;&#25193;&#23637;&#20102;&#24403;&#21069;&#26041;&#27861;&#65292;&#20351;&#25152;&#26377;&#19978;&#19979;&#25991;&#23884;&#20837;&#37117;&#21487;&#20197;&#29992;&#20110;&#32852;&#21512;&#39044;&#35757;&#32451;&#26816;&#32034;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.02564</link><description>&lt;p&gt;
RetroMAE-2&#65306;&#29992;&#20110;&#39044;&#35757;&#32451;&#26816;&#32034;&#23548;&#21521;&#35821;&#35328;&#27169;&#22411;&#30340;&#21452;&#24037;&#25513;&#30721;&#33258;&#21160;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
RetroMAE-2: Duplex Masked Auto-Encoder For Pre-Training Retrieval-Oriented Language Models. (arXiv:2305.02564v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DupMAE&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#21033;&#29992;&#20004;&#20010;&#33258;&#21160;&#32534;&#30721;&#20219;&#21153;&#26469;&#25552;&#39640;&#35821;&#20041;&#34920;&#31034;&#36136;&#37327;&#65292;&#25193;&#23637;&#20102;&#24403;&#21069;&#26041;&#27861;&#65292;&#20351;&#25152;&#26377;&#19978;&#19979;&#25991;&#23884;&#20837;&#37117;&#21487;&#20197;&#29992;&#20110;&#32852;&#21512;&#39044;&#35757;&#32451;&#26816;&#32034;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#26356;&#22909;&#22320;&#25903;&#25345;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#65292;&#20363;&#22914;&#32593;&#32476;&#25628;&#32034;&#21644;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#65292;&#20154;&#20204;&#27491;&#22312;&#21162;&#21147;&#24320;&#21457;&#26816;&#32034;&#23548;&#21521;&#35821;&#35328;&#27169;&#22411;&#65292;&#20363;&#22914;RetroMAE&#21644;&#35768;&#22810;&#20854;&#20182;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#31216;&#20026;DupMAE&#65292;&#26088;&#22312;&#25552;&#39640;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#25152;&#26377;&#19978;&#19979;&#25991;&#23884;&#20837;&#30340;&#35821;&#20041;&#34920;&#31034;&#36136;&#37327;&#12290;&#23427;&#21033;&#29992;&#20102;&#20004;&#20010;&#20114;&#34917;&#30340;&#33258;&#21160;&#32534;&#30721;&#20219;&#21153;&#65306;&#19968;&#20010;&#22522;&#20110;[CLS]&#23884;&#20837;&#37325;&#24314;&#36755;&#20837;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
To better support information retrieval tasks such as web search and open-domain question answering, growing effort is made to develop retrieval-oriented language models, e.g., RetroMAE and many others. Most of the existing works focus on improving the semantic representation capability for the contextualized embedding of the [CLS] token. However, recent study shows that the ordinary tokens besides [CLS] may provide extra information, which help to produce a better representation effect. As such, it's necessary to extend the current methods where all contextualized embeddings can be jointly pre-trained for the retrieval tasks. In this work, we propose a novel pre-training method called Duplex Masked Auto-Encoder, a.k.a. DupMAE. It is designed to improve the quality of semantic representation where all contextualized embeddings of the pre-trained model can be leveraged. It takes advantage of two complementary auto-encoding tasks: one reconstructs the input sentence on top of the [CLS] e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#22810;&#31181;&#22522;&#20110;&#32479;&#35745;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#31561;&#26041;&#27861;&#26469;&#26377;&#25928;&#22320;&#20998;&#26512;&#39321;&#28207;&#30340;&#27861;&#24459;&#21028;&#20915;&#65292;&#24182;&#20174;&#20013;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#20215;&#26684;&#39640;&#21644;&#36164;&#28304;&#32570;&#20047;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.02558</link><description>&lt;p&gt;
&#20197;&#35745;&#31639;&#35821;&#35328;&#23398;&#35270;&#35282;&#20998;&#26512;&#39321;&#28207;&#30340;&#27861;&#24459;&#21028;&#20915;
&lt;/p&gt;
&lt;p&gt;
Analyzing Hong Kong's Legal Judgments from a Computational Linguistics point-of-view. (arXiv:2305.02558v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02558
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#22810;&#31181;&#22522;&#20110;&#32479;&#35745;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#31561;&#26041;&#27861;&#26469;&#26377;&#25928;&#22320;&#20998;&#26512;&#39321;&#28207;&#30340;&#27861;&#24459;&#21028;&#20915;&#65292;&#24182;&#20174;&#20013;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#20215;&#26684;&#39640;&#21644;&#36164;&#28304;&#32570;&#20047;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#35745;&#31639;&#35821;&#35328;&#23398;&#20174;&#27861;&#24459;&#21028;&#20915;&#20013;&#25552;&#21462;&#26377;&#29992;&#20449;&#24687;&#26159;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#26089;&#26399;&#25552;&#20986;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;&#30446;&#21069;&#65292;&#23384;&#22312;&#22810;&#20010;&#21830;&#19994;&#20379;&#24212;&#21830;&#33258;&#21160;&#21270;&#25191;&#34892;&#36825;&#20123;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22312;&#20998;&#26512;&#39321;&#28207;&#27861;&#38498;&#31995;&#32479;&#30340;&#21028;&#20915;&#26102;&#65292;&#23384;&#22312;&#20215;&#26684;&#36807;&#39640;&#21644;&#32570;&#20047;&#36164;&#28304;&#30340;&#20851;&#38190;&#29942;&#39048;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#20960;&#31181;&#22522;&#20110;&#32479;&#35745;&#23398;&#12289;&#26426;&#22120;&#23398;&#20064;&#12289;&#28145;&#24230;&#23398;&#20064;&#21644;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20197;&#26377;&#25928;&#22320;&#20998;&#26512;&#39321;&#28207;&#27861;&#38498;&#31995;&#32479;&#30340;&#27861;&#24459;&#21028;&#20915;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21253;&#25324;&#65306;&#65288;1&#65289;&#24341;&#25991;&#32593;&#32476;&#22270;&#29983;&#25104;&#65292;&#65288;2&#65289;PageRank&#31639;&#27861;&#65292;&#65288;3&#65289;&#20851;&#38190;&#35789;&#20998;&#26512;&#21644;&#25688;&#35201;&#65292;&#65288;4&#65289;&#24773;&#24863;&#26497;&#24615;&#65292;&#20197;&#21450;&#65288;5&#65289;&#27573;&#33853;&#20998;&#31867;&#65292;&#20197;&#20415;&#33021;&#22815;&#25552;&#21462;&#21333;&#20010;&#21028;&#20915;&#20197;&#21450;&#32676;&#20307;&#21028;&#20915;&#30340;&#20851;&#38190;&#35265;&#35299;&#12290;&#36825;&#23558;&#20351;&#23545;&#39321;&#28207;&#21028;&#20915;&#30340;&#25972;&#20307;&#20998;&#26512;&#21464;&#24471;&#19981;&#37027;&#20040;&#32321;&#29712;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analysis and extraction of useful information from legal judgments using computational linguistics was one of the earliest problems posed in the domain of information retrieval. Presently, several commercial vendors exist who automate such tasks. However, a crucial bottleneck arises in the form of exorbitant pricing and lack of resources available in analysis of judgements mete out by Hong Kong's Legal System. This paper attempts to bridge this gap by providing several statistical, machine learning, deep learning and zero-shot learning based methods to effectively analyze legal judgments from Hong Kong's Court System. The methods proposed consists of: (1) Citation Network Graph Generation, (2) PageRank Algorithm, (3) Keyword Analysis and Summarization, (4) Sentiment Polarity, and (5) Paragrah Classification, in order to be able to extract key insights from individual as well a group of judgments together. This would make the overall analysis of judgments in Hong Kong less tedious and m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#33945;&#29305;&#21345;&#27931;&#35268;&#21010;&#30340;FAME&#38382;&#31572;&#31995;&#32479;&#65292;&#36890;&#36807;&#32452;&#32455;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#20316;&#20026;&#32467;&#26500;&#21270;&#34164;&#21547;&#26641;&#26469;&#24544;&#23454;&#22238;&#31572;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#33945;&#29305;&#21345;&#27931;&#35268;&#21010;&#31639;&#27861;&#65292;FAME&#22312;&#20445;&#25345;&#24544;&#23454;&#25512;&#29702;&#30340;&#21516;&#26102;&#65292;&#22312;&#39046;&#22495;&#36890;&#29992;&#30340;&#38382;&#31572;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.02556</link><description>&lt;p&gt;
&#22522;&#20110;&#33945;&#29305;&#21345;&#27931;&#35268;&#21010;&#30340;&#24544;&#23454;&#38382;&#31572;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Faithful Question Answering with Monte-Carlo Planning. (arXiv:2305.02556v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02556
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#33945;&#29305;&#21345;&#27931;&#35268;&#21010;&#30340;FAME&#38382;&#31572;&#31995;&#32479;&#65292;&#36890;&#36807;&#32452;&#32455;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#20316;&#20026;&#32467;&#26500;&#21270;&#34164;&#21547;&#26641;&#26469;&#24544;&#23454;&#22238;&#31572;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#33945;&#29305;&#21345;&#27931;&#35268;&#21010;&#31639;&#27861;&#65292;FAME&#22312;&#20445;&#25345;&#24544;&#23454;&#25512;&#29702;&#30340;&#21516;&#26102;&#65292;&#22312;&#39046;&#22495;&#36890;&#29992;&#30340;&#38382;&#31572;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38382;&#31572;&#26041;&#38754;&#30340;&#34920;&#29616;&#20196;&#20154;&#30633;&#30446;&#65292;&#20294;&#22914;&#20309;&#23637;&#29616;&#27169;&#22411;&#24544;&#23454;&#36981;&#24490;&#30340;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#20173;&#20855;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;FAME&#65288;&#22522;&#20110;&#33945;&#29305;&#21345;&#27931;&#35268;&#21010;&#30340;&#24544;&#23454;&#38382;&#31572;&#31995;&#32479;&#65289;&#65292;&#20197;&#23545;&#24544;&#23454;&#30340;&#25512;&#29702;&#27493;&#39588;&#36827;&#34892;&#38382;&#31572;&#12290;&#25512;&#29702;&#27493;&#39588;&#34987;&#32452;&#32455;&#20026;&#32467;&#26500;&#21270;&#34164;&#21547;&#26641;&#65292;&#23427;&#23637;&#31034;&#20102;&#21069;&#25552;&#22914;&#20309;&#34987;&#29992;&#20110;&#20135;&#29983;&#35777;&#26126;&#22238;&#31572;&#27491;&#30830;&#24615;&#30340;&#20013;&#38388;&#32467;&#35770;&#12290;&#25105;&#20204;&#23558;&#20219;&#21153;&#23450;&#20041;&#20026;&#31163;&#25955;&#20915;&#31574;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25512;&#29702;&#29615;&#22659;&#21644;&#25511;&#21046;&#22120;&#30340;&#20132;&#20114;&#26469;&#35299;&#20915;&#23427;&#12290;&#29615;&#22659;&#26159;&#27169;&#22359;&#21270;&#30340;&#65292;&#24182;&#21253;&#21547;&#20960;&#20010;&#22522;&#26412;&#30340;&#20219;&#21153;&#23548;&#21521;&#27169;&#22359;&#65292;&#32780;&#25511;&#21046;&#22120;&#21017;&#25552;&#20986;&#25805;&#20316;&#26469;&#32452;&#35013;&#36825;&#20123;&#27169;&#22359;&#12290;&#30001;&#20110;&#25628;&#32034;&#31354;&#38388;&#21487;&#33021;&#24456;&#22823;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#33945;&#29305;&#21345;&#27931;&#35268;&#21010;&#31639;&#27861;&#26469;&#36827;&#34892;&#21521;&#21069;&#25628;&#32034;&#65292;&#24182;&#36873;&#25321;&#26368;&#32456;&#23558;&#23548;&#33268;&#39640;&#36136;&#37327;&#27493;&#39588;&#30340;&#25805;&#20316;&#12290;FAME&#22312;DROP&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#36825;&#26159;&#19968;&#20010;&#39046;&#22495;&#36890;&#29992;&#30340;&#38382;&#31572;&#22522;&#20934;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#24544;&#23454;&#30340;&#25512;&#29702;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although large language models demonstrate remarkable question-answering performances, revealing the intermediate reasoning steps that the models faithfully follow remains challenging. In this paper, we propose FAME (FAithful question answering with MontE-carlo planning) to answer questions based on faithful reasoning steps. The reasoning steps are organized as a structured entailment tree, which shows how premises are used to produce intermediate conclusions that can prove the correctness of the answer. We formulate the task as a discrete decision-making problem and solve it through the interaction of a reasoning environment and a controller. The environment is modular and contains several basic task-oriented modules, while the controller proposes actions to assemble the modules. Since the search space could be large, we introduce a Monte-Carlo planning algorithm to do a look-ahead search and select actions that will eventually lead to high-quality steps. FAME achieves state-of-the-ar
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#34920;&#26684;&#25991;&#26723;&#20449;&#24687;&#25552;&#21462;&#30340;&#22810;&#27169;&#24577;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65288;FormNetV2&#65289;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#32479;&#19968;&#25152;&#26377;&#27169;&#24577;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#21040;&#19968;&#20010;&#25439;&#22833;&#20013;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20339;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.02549</link><description>&lt;p&gt;
FormNetV2&#65306;&#29992;&#20110;&#34920;&#26684;&#25991;&#26723;&#20449;&#24687;&#25552;&#21462;&#30340;&#22810;&#27169;&#24577;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FormNetV2: Multimodal Graph Contrastive Learning for Form Document Information Extraction. (arXiv:2305.02549v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02549
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#34920;&#26684;&#25991;&#26723;&#20449;&#24687;&#25552;&#21462;&#30340;&#22810;&#27169;&#24577;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65288;FormNetV2&#65289;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#32479;&#19968;&#25152;&#26377;&#27169;&#24577;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#21040;&#19968;&#20010;&#25439;&#22833;&#20013;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20339;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#25216;&#26415;&#30340;&#20986;&#29616;&#23548;&#33268;&#20102;&#22810;&#27169;&#24577;&#23398;&#20064;&#22312;&#34920;&#26684;&#25991;&#26723;&#29702;&#35299;&#20013;&#30340;&#28608;&#22686;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25193;&#23637;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#21040;&#20854;&#20182;&#27169;&#24577;&#30340;&#26041;&#27861;&#38656;&#35201;&#20180;&#32454;&#30340;&#22810;&#20219;&#21153;&#35843;&#25972;&#12289;&#22797;&#26434;&#30340;&#37325;&#26500;&#30446;&#26631;&#35774;&#35745;&#25110;&#39069;&#22806;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#12290;&#22312;FormNetV2&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#38598;&#20013;&#30340;&#22810;&#27169;&#24577;&#22270;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#32479;&#19968;&#25152;&#26377;&#27169;&#24577;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#21040;&#19968;&#20010;&#25439;&#22833;&#20013;&#12290;&#22270;&#23545;&#27604;&#30446;&#26631;&#26368;&#22823;&#21270;&#22810;&#27169;&#24577;&#34920;&#31034;&#30340;&#19968;&#33268;&#24615;&#65292;&#20026;&#25152;&#26377;&#27169;&#24577;&#25552;&#20379;&#33258;&#28982;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#32780;&#19981;&#38656;&#35201;&#29305;&#27530;&#30340;&#23450;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#36830;&#25509;&#22270;&#36793;&#32536;&#30340;&#19968;&#23545;&#26631;&#35760;&#30340;&#36793;&#26694;&#20869;&#25552;&#21462;&#22270;&#20687;&#29305;&#24449;&#65292;&#25429;&#25417;&#26356;&#26377;&#38024;&#23545;&#24615;&#30340;&#35270;&#35273;&#32447;&#32034;&#65292;&#32780;&#26080;&#38656;&#21152;&#36733;&#32463;&#36807;&#22797;&#26434;&#21644;&#21333;&#29420;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#23884;&#20837;&#22120;&#12290;FormNetV2&#22312;FUNSD&#12289;CORD&#12289;SROIE&#21644;Payment&#22522;&#20934;&#27979;&#35797;&#20013;&#30830;&#31435;&#20102;&#26368;&#26032;&#30340;&#26368;&#20339;&#34920;&#29616;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advent of self-supervised pre-training techniques has led to a surge in the use of multimodal learning in form document understanding. However, existing approaches that extend the mask language modeling to other modalities require careful multi-task tuning, complex reconstruction target designs, or additional pre-training data. In FormNetV2, we introduce a centralized multimodal graph contrastive learning strategy to unify self-supervised pre-training for all modalities in one loss. The graph contrastive objective maximizes the agreement of multimodal representations, providing a natural interplay for all modalities without special customization. In addition, we extract image features within the bounding box that joins a pair of tokens connected by a graph edge, capturing more targeted visual cues without loading a sophisticated and separately pre-trained image embedder. FormNetV2 establishes new state-of-the-art performance on FUNSD, CORD, SROIE and Payment benchmarks with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#22522;&#20110;LLMs&#27169;&#25311;&#20195;&#29702;&#30340;&#34892;&#20026;&#65292;&#31216;&#20043;&#20026;LLM Personas&#65292;&#22312;&#20998;&#37197;&#22823;&#20116;&#20154;&#26684;&#31867;&#22411;&#21644;&#24615;&#21035;&#35282;&#33394;&#26102;&#26159;&#21542;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#19968;&#33268;&#24615;&#30340;&#20010;&#24615;&#21270;&#29305;&#36136;&#30340;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2305.02547</link><description>&lt;p&gt;
PersonaLLM: &#25506;&#31350;GPT-3.5&#34920;&#36798;&#20010;&#24615;&#29305;&#24449;&#21644;&#24615;&#21035;&#24046;&#24322;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
PersonaLLM: Investigating the Ability of GPT-3.5 to Express Personality Traits and Gender Differences. (arXiv:2305.02547v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02547
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#22522;&#20110;LLMs&#27169;&#25311;&#20195;&#29702;&#30340;&#34892;&#20026;&#65292;&#31216;&#20043;&#20026;LLM Personas&#65292;&#22312;&#20998;&#37197;&#22823;&#20116;&#20154;&#26684;&#31867;&#22411;&#21644;&#24615;&#21035;&#35282;&#33394;&#26102;&#26159;&#21542;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#19968;&#33268;&#24615;&#30340;&#20010;&#24615;&#21270;&#29305;&#36136;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#20010;&#34892;&#19994;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#35774;&#35745;&#20013;&#26377;&#35768;&#22810;&#29992;&#36884;&#65292;&#24182;&#19988;&#30740;&#31350;&#34920;&#26126;&#20010;&#24615;&#21270;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#28385;&#36275;&#19981;&#21516;&#20154;&#26684;&#29305;&#24449;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#35780;&#20272;&#20010;&#24615;&#21270;LLM&#30340;&#34892;&#20026;&#26159;&#21542;&#33021;&#22815;&#20934;&#30830;&#12289;&#19968;&#33268;&#22320;&#21453;&#26144;&#26576;&#20123;&#20154;&#26684;&#29305;&#24449;&#12290;&#25105;&#20204;&#32771;&#34385;&#30740;&#31350;&#22522;&#20110;LLM&#30340;&#27169;&#25311;&#20195;&#29702;&#30340;&#34892;&#20026;&#65292;&#31216;&#20043;&#20026;LLM personas&#65292;&#24182;&#20351;&#29992;GPT-3.5&#65288;text-davinci-003&#65289;&#36827;&#34892;&#26696;&#20363;&#30740;&#31350;&#65292;&#20197;&#30740;&#31350;LLM&#22312;&#20998;&#37197;&#22823;&#20116;&#20154;&#26684;&#31867;&#22411;&#21644;&#24615;&#21035;&#35282;&#33394;&#26102;&#26159;&#21542;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#19968;&#33268;&#24615;&#30340;&#20010;&#24615;&#21270;&#29305;&#36136;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;320&#20010;LLM personas&#65288;&#27599;&#31181;&#22823;&#20116;&#20154;&#26684;&#31867;&#22411;&#26377;5&#20010;&#22899;&#24615;&#21644;5&#20010;&#30007;&#24615;&#65289;&#65292;&#24182;&#25552;&#31034;&#20182;&#20204;&#23436;&#25104;&#32463;&#20856;&#30340;44&#39033;&#22823;&#20116;&#20154;&#26684;&#38382;&#21367;&#65288;BFI&#65289;&#65292;&#28982;&#21518;&#25776;&#20889;&#19968;&#20010;&#20851;&#20110;&#20182;&#20204;&#31461;&#24180;&#30340;800&#23383;&#25925;&#20107;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;LLM personas&#30340;&#33258;&#25105;&#25253;&#21578;&#30340;BFI&#20998;&#25968;&#19982;&#20182;&#20204;&#20998;&#37197;&#30340;&#20154;&#26684;&#31867;&#22411;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the many use cases for large language models (LLMs) in the design of chatbots in various industries and the research showing the importance of personalizing chatbots to cater to different personality traits, little work has been done to evaluate whether the behaviors of personalized LLMs can reflect certain personality traits accurately and consistently. We consider studying the behavior of LLM-based simulated agents which refer to as LLM personas and present a case study with GPT-3.5 (text-davinci-003) to investigate whether LLMs can generate content with consistent, personalized traits when assigned Big Five personality types and gender roles. We created 320 LLM personas (5 females and 5 males for each of the 32 Big Five personality types) and prompted them to complete the classic 44-item Big Five Inventory (BFI) and then write an 800-word story about their childhood. Results showed that LLM personas' self-reported BFI scores are consistent with their assigned personality typ
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#35821;&#35328;&#25552;&#31034;&#19979;&#30340;&#22870;&#21169;&#26102;&#38388;&#20559;&#22909;&#65292;&#24182;&#21457;&#29616;GPT&#22312;&#20855;&#26377;&#36739;&#24369;&#26410;&#26469;&#26102;&#24577;&#30340;&#35821;&#35328;&#19979;&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#32784;&#24515;&#65292;&#36825;&#19982;&#20351;&#29992;&#35813;&#35821;&#35328;&#30340;&#20154;&#31867;&#30340;&#20559;&#22909;&#30456;&#20284;&#12290;</title><link>http://arxiv.org/abs/2305.02531</link><description>&lt;p&gt;
&#35821;&#35328;&#12289;&#26102;&#38388;&#20559;&#22909;&#21644;&#28040;&#36153;&#34892;&#20026;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
Language, Time Preferences, and Consumer Behavior: Evidence from Large Language Models. (arXiv:2305.02531v1 [econ.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02531
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#35821;&#35328;&#25552;&#31034;&#19979;&#30340;&#22870;&#21169;&#26102;&#38388;&#20559;&#22909;&#65292;&#24182;&#21457;&#29616;GPT&#22312;&#20855;&#26377;&#36739;&#24369;&#26410;&#26469;&#26102;&#24577;&#30340;&#35821;&#35328;&#19979;&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#32784;&#24515;&#65292;&#36825;&#19982;&#20351;&#29992;&#35813;&#35821;&#35328;&#30340;&#20154;&#31867;&#30340;&#20559;&#22909;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#23545;&#25105;&#20204;&#23545;&#26102;&#38388;&#21644;&#22870;&#21169;&#30340;&#24863;&#30693;&#26377;&#24456;&#22823;&#30340;&#24433;&#21709;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#24403;&#20197;&#19981;&#21516;&#30340;&#35821;&#35328;&#35810;&#38382;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#23427;&#20204;&#26159;&#21542;&#26174;&#31034;&#20986;&#19981;&#21516;&#30340;&#22870;&#21169;&#26102;&#38388;&#20559;&#22909;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#36873;&#25321;&#26159;&#21542;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#36873;&#25321;&#12290;&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;GPT-3.5&#65288;&#20197;&#19979;&#31616;&#31216;GPT&#65289;&#22312;&#22810;&#31181;&#35821;&#35328;&#25552;&#31034;&#19979;&#30340;&#21709;&#24212;&#65292;&#25506;&#32034;&#20102;&#36739;&#23567;&#12289;&#36739;&#26089;&#30340;&#22870;&#21169;&#21644;&#36739;&#22823;&#12289;&#36739;&#26202;&#30340;&#22870;&#21169;&#20043;&#38388;&#30340;&#20559;&#22909;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#24403;&#20197;&#35821;&#20041;&#21547;&#20041;&#36739;&#24369;&#30340;&#26410;&#26469;&#26102;&#24577;&#21442;&#32771;&#65288;FTR&#65289;&#65292;&#22914;&#24503;&#35821;&#21644;&#27721;&#35821;&#65292;&#20026;&#25552;&#31034;&#35821;&#26102;&#65292;GPT&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#32784;&#24515;&#65292;&#30456;&#27604;&#33521;&#35821;&#21644;&#27861;&#35821;&#31561;&#20855;&#26377;&#24378;&#22823;FTR&#30340;&#35821;&#35328;&#12290;&#36825;&#20123;&#21457;&#29616;&#19982;&#29616;&#26377;&#25991;&#29486;&#19968;&#33268;&#65292;&#24182;&#34920;&#26126;&#20102;GPT&#30340;&#36873;&#25321;&#19982;&#36825;&#20123;&#35821;&#35328;&#30340;&#20351;&#29992;&#32773;&#30340;&#20559;&#22909;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#28982;&#32780;&#65292;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#36739;&#26089;&#25110;&#36739;&#26202;&#22870;&#21169;&#30340;&#20559;&#22909;&#24182;&#27809;&#26377;&#38543;&#30528;&#22870;&#21169;&#24046;&#24322;&#31995;&#32479;&#22320;&#25913;&#21464;&#65292;&#36825;&#34920;&#26126;&#20102;&#19968;&#31181;&#35789;&#20856;&#24207;&#20248;&#20808;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language has a strong influence on our perceptions of time and rewards. This raises the question of whether large language models, when asked in different languages, show different preferences for rewards over time and if their choices are similar to those of humans. In this study, we analyze the responses of GPT-3.5 (hereafter referred to as GPT) to prompts in multiple languages, exploring preferences between smaller, sooner rewards and larger, later rewards. Our results show that GPT displays greater patience when prompted in languages with weak future tense references (FTR), such as German and Mandarin, compared to languages with strong FTR, like English and French. These findings are consistent with existing literature and suggest a correlation between GPT's choices and the preferences of speakers of these languages. However, further analysis reveals that the preference for earlier or later rewards does not systematically change with reward gaps, indicating a lexicographic preferen
&lt;/p&gt;</description></item><item><title>ANetQA&#26159;&#19968;&#20010;&#26032;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#27979;&#35797;&#65292;&#25903;&#25345;&#23545;&#26410;&#21098;&#36753;&#35270;&#39057;&#36827;&#34892;&#31934;&#32454;&#30340;&#32452;&#21512;&#25512;&#29702;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#19981;&#21516;&#65292;ANetQA&#30340;&#38382;&#39064;&#31867;&#22411;&#38656;&#35201;&#31934;&#32454;&#30340;&#32452;&#25104;&#24335;&#25512;&#29702;&#65292;&#24182;&#35206;&#30422;&#22810;&#26679;&#21270;&#30340;&#20449;&#24687;&#31867;&#22411;&#65292;&#21487;&#20197;&#26356;&#21152;&#20840;&#38754;&#22320;&#35780;&#20272;VideoQA&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.02519</link><description>&lt;p&gt;
ANetQA&#65306;&#19968;&#20010;&#29992;&#20110;&#26410;&#21098;&#36753;&#35270;&#39057;&#31934;&#32454;&#32452;&#21512;&#25512;&#29702;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
ANetQA: A Large-scale Benchmark for Fine-grained Compositional Reasoning over Untrimmed Videos. (arXiv:2305.02519v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02519
&lt;/p&gt;
&lt;p&gt;
ANetQA&#26159;&#19968;&#20010;&#26032;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#27979;&#35797;&#65292;&#25903;&#25345;&#23545;&#26410;&#21098;&#36753;&#35270;&#39057;&#36827;&#34892;&#31934;&#32454;&#30340;&#32452;&#21512;&#25512;&#29702;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#19981;&#21516;&#65292;ANetQA&#30340;&#38382;&#39064;&#31867;&#22411;&#38656;&#35201;&#31934;&#32454;&#30340;&#32452;&#25104;&#24335;&#25512;&#29702;&#65292;&#24182;&#35206;&#30422;&#22810;&#26679;&#21270;&#30340;&#20449;&#24687;&#31867;&#22411;&#65292;&#21487;&#20197;&#26356;&#21152;&#20840;&#38754;&#22320;&#35780;&#20272;VideoQA&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31435;&#22522;&#20934;&#27979;&#35797;&#26469;&#31995;&#32479;&#22320;&#20998;&#26512;&#35270;&#39057;&#38382;&#31572;&#65288;VideoQA&#65289;&#27169;&#22411;&#30340;&#19981;&#21516;&#33021;&#21147;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#36890;&#24120;&#20351;&#29992;&#38750;&#32452;&#25104;&#24418;&#31616;&#21333;&#38382;&#39064;&#24182;&#36973;&#21463;&#35821;&#35328;&#20559;&#35265;&#65292;&#20351;&#24471;&#28145;&#20837;&#35786;&#26029;&#27169;&#22411;&#24369;&#28857;&#21464;&#24471;&#22256;&#38590;&#12290;&#26368;&#36817;&#30340;&#22522;&#20934;&#27979;&#35797;AGQA&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#33539;&#20363;&#65292;&#23427;&#21487;&#20197;&#20174;&#39044;&#27880;&#37322;&#22330;&#26223;&#22270;&#20013;&#33258;&#21160;&#29983;&#25104;QA&#23545;&#65292;&#20351;&#20854;&#33021;&#22815;&#20197;&#39063;&#31890;&#21270;&#30340;&#25511;&#21046;&#24230;&#37327;&#22810;&#26679;&#21270;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#30340;&#38382;&#39064;&#22312;&#20110;&#26080;&#27861;&#29702;&#35299;&#35270;&#39057;&#20013;&#32454;&#31890;&#24230;&#30340;&#35821;&#20041;&#65292;&#22240;&#20026;&#36825;&#26679;&#30340;&#20449;&#24687;&#22312;&#20854;&#22330;&#26223;&#22270;&#20013;&#27809;&#26377;&#21576;&#29616;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ANetQA&#65292;&#36825;&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#22522;&#20934;&#27979;&#35797;&#65292;&#25903;&#25345;&#20174;ActivityNet&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26410;&#21098;&#36753;&#35270;&#39057;&#20013;&#36827;&#34892;&#31934;&#32454;&#30340;&#32452;&#21512;&#25512;&#29702;&#12290;&#19982;AGQA&#31867;&#20284;&#65292;ANetQA&#20013;&#30340;QA&#23545;&#26159;&#20174;&#24050;&#27880;&#37322;&#30340;&#35270;&#39057;&#22330;&#26223;&#22270;&#20013;&#33258;&#21160;&#29983;&#25104;&#30340;&#12290;ANetQA &#30340;&#32454;&#31890;&#24230;&#29305;&#24615;&#20307;&#29616;&#22312;&#20197;&#19979;&#20960;&#20010;&#26041;&#38754;&#65306;&#65288;i&#65289;&#26410;&#21098;&#36753;&#30340;&#35270;&#39057;&#65292;&#21253;&#21547;&#22797;&#26434;&#21160;&#20316;&#20197;&#21450;&#22810;&#20010;&#21160;&#20316;&#24207;&#21015;&#65307;&#65288;ii&#65289;&#35206;&#30422;&#22810;&#26679;&#21270;&#30340;&#20449;&#24687;&#31867;&#22411;&#65292;&#21253;&#25324;&#23545;&#35937;&#12289;&#22330;&#26223;&#12289;&#23646;&#24615;&#31561;&#65307;&#65288;iii&#65289;&#20016;&#23500;&#30340;&#38382;&#39064;&#31867;&#22411;&#65292;&#38656;&#35201;&#31934;&#32454;&#30340;&#32452;&#25104;&#24335;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building benchmarks to systemically analyze different capabilities of video question answering (VideoQA) models is challenging yet crucial. Existing benchmarks often use non-compositional simple questions and suffer from language biases, making it difficult to diagnose model weaknesses incisively. A recent benchmark AGQA poses a promising paradigm to generate QA pairs automatically from pre-annotated scene graphs, enabling it to measure diverse reasoning abilities with granular control. However, its questions have limitations in reasoning about the fine-grained semantics in videos as such information is absent in its scene graphs. To this end, we present ANetQA, a large-scale benchmark that supports fine-grained compositional reasoning over the challenging untrimmed videos from ActivityNet. Similar to AGQA, the QA pairs in ANetQA are automatically generated from annotated video scene graphs. The fine-grained properties of ANetQA are reflected in the following: (i) untrimmed videos with
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;USTC-NELSLIP&#22242;&#38431;&#20026;SemEval-2023&#20219;&#21153;2&#24320;&#21457;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;SCDAG&#8221;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#35821;&#35328;&#22797;&#26434;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26500;&#24314;&#36766;&#20070;&#21644;&#36866;&#24212;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#31034;&#26469;&#25552;&#39640;&#35782;&#21035;&#31934;&#24230;&#65292;&#22312;&#21360;&#22320;&#35821;&#36187;&#36947;&#19978;&#25490;&#21517;&#31532;&#19968;&#12290;</title><link>http://arxiv.org/abs/2305.02517</link><description>&lt;p&gt;
USTC-NELSLIP&#22312;SemEval-2023&#20219;&#21153;2&#20013;&#30340;&#24212;&#29992;&#65306;&#38024;&#23545;&#22810;&#35821;&#35328;&#22797;&#26434;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#36827;&#34892;&#32479;&#35745;&#26500;&#24314;&#21644;&#21452;&#37325;&#36866;&#24212;&#24615;&#36766;&#20070;&#30340;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
USTC-NELSLIP at SemEval-2023 Task 2: Statistical Construction and Dual Adaptation of Gazetteer for Multilingual Complex NER. (arXiv:2305.02517v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02517
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;USTC-NELSLIP&#22242;&#38431;&#20026;SemEval-2023&#20219;&#21153;2&#24320;&#21457;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;SCDAG&#8221;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#35821;&#35328;&#22797;&#26434;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26500;&#24314;&#36766;&#20070;&#21644;&#36866;&#24212;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#31034;&#26469;&#25552;&#39640;&#35782;&#21035;&#31934;&#24230;&#65292;&#22312;&#21360;&#22320;&#35821;&#36187;&#36947;&#19978;&#25490;&#21517;&#31532;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;USTC-NELSLIP&#22242;&#38431;&#20026;SemEval-2023 &#20219;&#21153;2&#22810;&#35821;&#35328;&#22797;&#26434;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;MultiCoNER II&#65289;&#24320;&#21457;&#30340;&#31995;&#32479;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#32479;&#35745;&#26500;&#24314;&#21644;&#21452;&#37325;&#36866;&#24212;&#24615;&#36766;&#20070;&#65288;SCDAG&#65289;&#8221;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#35821;&#35328;&#22797;&#26434;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;&#35813;&#26041;&#27861;&#39318;&#20808;&#21033;&#29992;&#22522;&#20110;&#32479;&#35745;&#30340;&#26041;&#27861;&#26500;&#24314;&#36766;&#20070;&#12290;&#20854;&#27425;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#21477;&#23376;&#32423;&#21644;&#23454;&#20307;&#32423;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#65292;&#36866;&#24212;&#36766;&#20070;&#32593;&#32476;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#31034;&#12290;&#26368;&#21518;&#65292;&#36825;&#20004;&#20010;&#32593;&#32476;&#34987;&#38598;&#25104;&#29992;&#20110;&#30417;&#30563;&#24335;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#35757;&#32451;&#12290;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#20351;&#29992;Wikidata&#26500;&#24314;&#30340;&#36766;&#20070;&#19978;&#30340;XLM-R&#65292;&#34920;&#29616;&#20986;&#36328;&#19981;&#21516;&#35821;&#35328;&#30340;&#24456;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290; &#23454;&#39564;&#32467;&#26524;&#21644;&#35814;&#32454;&#20998;&#26512;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#35813;&#20219;&#21153;&#20013;&#65292;&#23448;&#26041;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#21360;&#22320;&#35821;&#36187;&#36947;&#19978;&#25490;&#21517;&#31532;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes the system developed by the USTC-NELSLIP team for SemEval-2023 Task 2 Multilingual Complex Named Entity Recognition (MultiCoNER II). A method named Statistical Construction and Dual Adaptation of Gazetteer (SCDAG) is proposed for Multilingual Complex NER. The method first utilizes a statistics-based approach to construct a gazetteer. Secondly, the representations of gazetteer networks and language models are adapted by minimizing the KL divergence between them at both the sentence-level and entity-level. Finally, these two networks are then integrated for supervised named entity recognition (NER) training. The proposed method is applied to XLM-R with a gazetteer built from Wikidata, and shows great generalization ability across different tracks. Experimental results and detailed analysis verify the effectiveness of the proposed method. The official results show that our system ranked 1st on one track (Hindi) in this task.
&lt;/p&gt;</description></item><item><title>AutoML-GPT &#26159;&#19968;&#31181;&#22522;&#20110; GPT &#30340;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21160;&#24577;&#22320;&#21033;&#29992;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#33258;&#21160;&#21270;&#35757;&#32451;&#31649;&#36947;&#65292;&#33410;&#32422;&#20102;&#36873;&#25321;&#27169;&#22411;&#26550;&#26500;&#12289;&#20248;&#21270;&#31639;&#27861;&#21644;&#35843;&#25972;&#36229;&#21442;&#25968;&#30340;&#20154;&#21147;&#21644;&#26102;&#38388;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2305.02499</link><description>&lt;p&gt;
AutoML-GPT: &#22522;&#20110; GPT &#30340;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
AutoML-GPT: Automatic Machine Learning with GPT. (arXiv:2305.02499v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02499
&lt;/p&gt;
&lt;p&gt;
AutoML-GPT &#26159;&#19968;&#31181;&#22522;&#20110; GPT &#30340;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21160;&#24577;&#22320;&#21033;&#29992;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#33258;&#21160;&#21270;&#35757;&#32451;&#31649;&#36947;&#65292;&#33410;&#32422;&#20102;&#36873;&#25321;&#27169;&#22411;&#26550;&#26500;&#12289;&#20248;&#21270;&#31639;&#27861;&#21644;&#35843;&#25972;&#36229;&#21442;&#25968;&#30340;&#20154;&#21147;&#21644;&#26102;&#38388;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI &#20219;&#21153;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#39046;&#22495;&#21644;&#39046;&#22495;&#12290;&#34429;&#28982;&#20026;&#29305;&#23450;&#20219;&#21153;&#21644;&#24212;&#29992;&#31243;&#24207;&#35774;&#35745;&#20102;&#20247;&#22810; AI &#27169;&#22411;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#21147;&#25237;&#20837;&#26469;&#26597;&#25214;&#27491;&#30830;&#30340;&#27169;&#22411;&#26550;&#26500;&#12289;&#20248;&#21270;&#31639;&#27861;&#21644;&#36229;&#21442;&#25968;&#12290;&#26368;&#36817;&#65292;&#20687; ChatGPT &#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#22312;&#25512;&#29702;&#12289;&#29702;&#35299;&#21644;&#20132;&#20114;&#30340;&#21508;&#20010;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24320;&#21457;&#38754;&#21521;&#20219;&#21153;&#30340;&#25552;&#31034;&#24182;&#33258;&#21160;&#21033;&#29992; LLM &#33258;&#21160;&#21270;&#35757;&#32451;&#31649;&#36947;&#30340;&#24819;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#27010;&#24565;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102; AutoML-GPT&#65292;&#23427;&#37319;&#29992; GPT &#20316;&#20026;&#36830;&#25509;&#22810;&#31181; AI &#27169;&#22411;&#30340;&#26725;&#26753;&#65292;&#24182;&#21160;&#24577;&#22320;&#20351;&#29992;&#20248;&#21270;&#36229;&#21442;&#25968;&#35757;&#32451;&#27169;&#22411;&#12290;AutoML-GPT &#20174;&#27169;&#22411;&#21644;&#25968;&#25454;&#21345;&#20013;&#21160;&#24577;&#33719;&#21462;&#29992;&#25143;&#35831;&#27714;&#65292;&#24182;&#32452;&#25104;&#30456;&#24212;&#30340;&#25552;&#31034;&#27573;&#33853;&#12290;&#26368;&#32456;&#65292;&#36890;&#36807;&#36825;&#20010;&#25552;&#31034;&#27573;&#33853;&#65292;AutoML-GPT &#23558;&#33258;&#21160;&#20174;&#25968;&#25454;&#22788;&#29702;&#21040;&#27169;&#22411;&#26550;&#26500;&#12289;&#36229;&#21442;&#25968;&#35843;&#25972;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI tasks encompass a wide range of domains and fields. While numerous AI models have been designed for specific tasks and applications, they often require considerable human efforts in finding the right model architecture, optimization algorithm, and hyperparameters. Recent advances in large language models (LLMs) like ChatGPT show remarkable capabilities in various aspects of reasoning, comprehension, and interaction. Consequently, we propose developing task-oriented prompts and automatically utilizing LLMs to automate the training pipeline. To implement this concept, we present the AutoML-GPT, which employs GPT as the bridge to diverse AI models and dynamically trains models with optimized hyperparameters. AutoML-GPT dynamically takes user requests from the model and data cards and composes the corresponding prompt paragraph. Ultimately, with this prompt paragraph, AutoML-GPT will automatically conduct the experiments from data processing to model architecture, hyperparameter tuning,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#20010;&#20195;&#29702;&#30340;&#29983;&#25104;&#26041;&#26696;&#65292;&#21253;&#25324;&#29983;&#25104;&#22120;&#12289;&#36741;&#23548;&#21592;&#21644;&#32534;&#36753;&#22120;&#65292;&#20197;&#22686;&#24378;&#29983;&#25104;&#36755;&#20986;&#30340;&#33258;&#23450;&#20041;&#12290;&#22312;&#20004;&#20010;&#25688;&#35201;&#24635;&#32467;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#26356;&#22909;&#30340;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2305.02483</link><description>&lt;p&gt;
ChatGPT&#24341;&#23548;&#30340;&#32534;&#36753;&#36741;&#21161;&#24037;&#20855;&#29992;&#20110;&#25688;&#35201;&#27719;&#24635;&#33258;&#23450;&#20041;
&lt;/p&gt;
&lt;p&gt;
ChatGPT-steered Editing Instructor for Customization of Abstractive Summarization. (arXiv:2305.02483v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02483
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#20010;&#20195;&#29702;&#30340;&#29983;&#25104;&#26041;&#26696;&#65292;&#21253;&#25324;&#29983;&#25104;&#22120;&#12289;&#36741;&#23548;&#21592;&#21644;&#32534;&#36753;&#22120;&#65292;&#20197;&#22686;&#24378;&#29983;&#25104;&#36755;&#20986;&#30340;&#33258;&#23450;&#20041;&#12290;&#22312;&#20004;&#20010;&#25688;&#35201;&#24635;&#32467;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#26356;&#22909;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#30340;&#29983;&#25104;&#36136;&#37327;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#65292;&#20294;&#26681;&#25454;&#29305;&#23450;&#29992;&#25143;&#38656;&#27714;&#35843;&#25972;&#20854;&#36755;&#20986;&#20173;&#28982;&#26159;&#19968;&#39033;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#20010;&#20195;&#29702;&#30340;&#29983;&#25104;&#26041;&#26696;&#8212;&#8212;&#29983;&#25104;&#22120;&#12289;&#36741;&#23548;&#21592;&#21644;&#32534;&#36753;&#22120;&#65292;&#20197;&#22686;&#24378;&#29983;&#25104;&#36755;&#20986;&#30340;&#33258;&#23450;&#20041;&#12290;&#29983;&#25104;&#22120;&#20135;&#29983;&#21021;&#22987;&#36755;&#20986;&#65292;&#38024;&#23545;&#29992;&#25143;&#38656;&#27714;&#30340;&#36741;&#23548;&#21592;&#20135;&#29983;&#32534;&#36753;&#25351;&#23548;&#65292;&#32780;&#32534;&#36753;&#22120;&#20135;&#29983;&#31526;&#21512;&#29992;&#25143;&#20559;&#22909;&#30340;&#20462;&#35746;&#36755;&#20986;&#12290;&#26080;&#27861;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;ChatGPT&#65289;&#26082;&#20805;&#24403;&#29983;&#25104;&#22120;&#21448;&#20805;&#24403;&#32534;&#36753;&#22120;&#65292;&#32780;&#36739;&#23567;&#30340;&#27169;&#22411;&#21017;&#20805;&#24403;&#29992;&#25143;&#29305;&#23450;&#30340;&#36741;&#23548;&#21592;&#65292;&#24341;&#23548;&#29983;&#25104;&#36807;&#31243;&#26397;&#21521;&#29992;&#25143;&#38656;&#27714;&#30340;&#26041;&#21521;&#21457;&#23637;&#12290;&#36741;&#23548;&#21592;&#20351;&#29992;&#32534;&#36753;&#32773;&#24341;&#23548;&#30340;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#22521;&#35757;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#32534;&#36753;&#27169;&#22411;&#30340;&#21453;&#39304;&#26469;&#20248;&#21270;&#25351;&#23548;&#29983;&#25104;&#12290;&#22312;&#20004;&#20010;&#25688;&#35201;&#24635;&#32467;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#26356;&#22909;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tailoring outputs of large language models, such as ChatGPT, to specific user needs remains a challenge despite their impressive generation quality. In this paper, we propose a tri-agent generation pipeline consisting of a generator, an instructor, and an editor to enhance the customization of generated outputs. The generator produces an initial output, the user-specific instructor generates editing instructions, and the editor generates a revised output aligned with user preferences. The inference-only large language model (ChatGPT) serves as both the generator and the editor, while a smaller model acts as the user-specific instructor to guide the generation process toward user needs. The instructor is trained using editor-steered reinforcement learning, leveraging feedback from the large-scale editor model to optimize instruction generation. Experimental results on two abstractive summarization datasets demonstrate the effectiveness of our approach in generating outputs that better f
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#28023;&#20107;&#39046;&#22495;&#30340;&#27010;&#29575;&#30693;&#35782;&#22270;&#35889;&#30340;&#33258;&#21160;&#26500;&#24314;&#65292;&#20197;&#21033;&#29992;&#21547;&#26377;&#20016;&#23500;&#20449;&#24687;&#30340;&#26410;&#32467;&#26500;&#21270;&#36719;&#25968;&#25454;&#65292;&#24182;&#35299;&#20915;&#20102;&#36719;&#25968;&#25454;&#25552;&#21462;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.02471</link><description>&lt;p&gt;
&#38754;&#21521;&#28023;&#20107;&#39046;&#22495;&#27010;&#29575;&#30693;&#35782;&#22270;&#35889;&#30340;&#33258;&#21160;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Toward the Automated Construction of Probabilistic Knowledge Graphs for the Maritime Domain. (arXiv:2305.02471v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02471
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#28023;&#20107;&#39046;&#22495;&#30340;&#27010;&#29575;&#30693;&#35782;&#22270;&#35889;&#30340;&#33258;&#21160;&#26500;&#24314;&#65292;&#20197;&#21033;&#29992;&#21547;&#26377;&#20016;&#23500;&#20449;&#24687;&#30340;&#26410;&#32467;&#26500;&#21270;&#36719;&#25968;&#25454;&#65292;&#24182;&#35299;&#20915;&#20102;&#36719;&#25968;&#25454;&#25552;&#21462;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22269;&#38469;&#28023;&#19978;&#29359;&#32618;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#24182;&#24120;&#19982;&#26356;&#24191;&#27867;&#30340;&#29359;&#32618;&#32593;&#32476;&#26377;&#20851;&#12290;&#20165;&#34701;&#21512;&#19982;&#29289;&#29702;&#31227;&#21160;&#30456;&#20851;&#30340;&#25968;&#25454;&#65288;&#21363;&#30001;&#29289;&#29702;&#20256;&#24863;&#22120;&#25110;&#30828;&#25968;&#25454;&#29983;&#25104;&#30340;&#25968;&#25454;&#65289;&#26469;&#26816;&#27979;&#28023;&#19978;&#23041;&#32961;&#26159;&#19981;&#22815;&#30340;&#12290;&#36825;&#23548;&#33268;&#20102;&#30740;&#31350;&#21644;&#24320;&#21457;&#24037;&#20316;&#65292;&#26088;&#22312;&#23558;&#30828;&#25968;&#25454;&#19982;&#20854;&#20182;&#31867;&#22411;&#30340;&#25968;&#25454;&#65288;&#29305;&#21035;&#26159;&#20154;&#24037;&#29983;&#25104;&#30340;&#36719;&#25968;&#25454;&#65289;&#30456;&#32467;&#21512;&#12290;&#29616;&#26377;&#30740;&#31350;&#36890;&#24120;&#20551;&#35774;&#36755;&#20837;&#30340;&#36719;&#25968;&#25454;&#20197;&#32467;&#26500;&#21270;&#26684;&#24335;&#25552;&#20379;&#65292;&#25110;&#32773;&#38598;&#20013;&#20110;&#25552;&#21462;&#26576;&#20123;&#30456;&#20851;&#23454;&#20307;&#25110;&#27010;&#24565;&#20197;&#37197;&#21512;&#25110;&#27880;&#37322;&#30828;&#25968;&#25454;&#12290;&#23545;&#20110;&#20174;&#26410;&#32467;&#26500;&#21270;&#26684;&#24335;&#65288;&#22914;&#24773;&#25253;&#25253;&#21578;&#21644;&#26032;&#38395;&#25991;&#31456;&#65289;&#20013;&#25552;&#21462;&#19982;&#24863;&#20852;&#36259;&#24773;&#20917;&#38544;&#21547;&#30456;&#20851;&#30340;&#20016;&#23500;&#20449;&#24687;&#65292;&#20851;&#27880;&#24230;&#35201;&#23569;&#24471;&#22810;&#12290;&#20026;&#20102;&#21033;&#29992;&#36825;&#20123;&#26469;&#28304;&#20013;&#28508;&#22312;&#26377;&#29992;&#21644;&#20016;&#23500;&#30340;&#20449;&#24687;&#65292;&#38656;&#35201;&#25552;&#21462;&#19981;&#20165;&#30456;&#20851;&#30340;&#23454;&#20307;&#21644;&#27010;&#24565;&#65292;&#36824;&#38656;&#35201;&#25552;&#21462;&#38544;&#21547;&#20110;&#22823;&#37327;&#36719;&#25968;&#25454;&#20013;&#30340;&#30456;&#20851;&#24773;&#22659;&#30340;&#20016;&#23500;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
International maritime crime is becoming increasingly sophisticated, often associated with wider criminal networks. Detecting maritime threats by means of fusing data purely related to physical movement (i.e., those generated by physical sensors, or hard data) is not sufficient. This has led to research and development efforts aimed at combining hard data with other types of data (especially human-generated or soft data). Existing work often assumes that input soft data is available in a structured format, or is focused on extracting certain relevant entities or concepts to accompany or annotate hard data. Much less attention has been given to extracting the rich knowledge about the situations of interest implicitly embedded in the large amount of soft data existing in unstructured formats (such as intelligence reports and news articles). In order to exploit the potentially useful and rich information from such sources, it is necessary to extract not only the relevant entities and conc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#32593;&#32476;&#30340;&#22266;&#23450;&#23618;&#21518;&#28155;&#21152;&#23569;&#37327;&#21442;&#25968;&#30340;&#20219;&#21153;&#20248;&#21270;&#36866;&#37197;&#22120;&#26469;&#29420;&#31435;&#22320;&#23398;&#20064;&#27599;&#20010;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#25552;&#39640;DST&#21644;NLG&#27169;&#22359;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.02468</link><description>&lt;p&gt;
&#38754;&#21521;&#20219;&#21153;&#30340;&#31471;&#21040;&#31471;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#20219;&#21153;&#20248;&#21270;&#36866;&#37197;&#22120;
&lt;/p&gt;
&lt;p&gt;
Task-Optimized Adapters for an End-to-End Task-Oriented Dialogue System. (arXiv:2305.02468v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#32593;&#32476;&#30340;&#22266;&#23450;&#23618;&#21518;&#28155;&#21152;&#23569;&#37327;&#21442;&#25968;&#30340;&#20219;&#21153;&#20248;&#21270;&#36866;&#37197;&#22120;&#26469;&#29420;&#31435;&#22320;&#23398;&#20064;&#27599;&#20010;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#25552;&#39640;DST&#21644;NLG&#27169;&#22359;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#26088;&#22312;&#36890;&#36807;&#36319;&#36394;&#23545;&#35805;&#29366;&#24577;&#21644;&#29983;&#25104;&#36866;&#24403;&#30340;&#21709;&#24212;&#26469;&#25191;&#34892;&#29305;&#23450;&#20219;&#21153;&#65292;&#24110;&#21161;&#29992;&#25143;&#23454;&#29616;&#23450;&#20041;&#30340;&#30446;&#26631;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#39044;&#35757;&#32451;&#30340;&#31471;&#21040;&#31471;&#23545;&#35805;&#27169;&#22411;&#22312;&#23545;&#35805;&#31995;&#32479;&#20013;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20849;&#20139;&#30456;&#21516;&#30340;&#21442;&#25968;&#20197;&#35757;&#32451;&#23545;&#35805;&#31995;&#32479;&#30340;&#20219;&#21153;(NLU&#65292;DST&#65292;NLG)&#65292;&#22240;&#27492;&#27599;&#20010;&#20219;&#21153;&#30340;&#35843;&#35797;&#37117;&#24456;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#30456;&#36739;&#20110;PLM&#65292;&#23558;&#22823;&#37327;&#21442;&#25968;&#24494;&#35843;&#26469;&#21019;&#24314;&#38754;&#21521;&#20219;&#21153;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#38656;&#35201;&#22823;&#37327;&#30340;&#21162;&#21147;&#65292;&#36825;&#20351;&#24471;&#38750;&#19987;&#23478;&#38590;&#20197;&#22788;&#29702;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25171;&#31639;&#35757;&#32451;&#30456;&#23545;&#36731;&#37327;&#32423;&#21644;&#24555;&#36895;&#30340;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20219;&#21153;&#20248;&#21270;&#36866;&#37197;&#22120;&#30340;&#31471;&#21040;&#31471;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#65292;&#27599;&#20010;&#20219;&#21153;&#29420;&#31435;&#23398;&#20064;&#65292;&#22312;&#39044;&#35757;&#32451;&#32593;&#32476;&#30340;&#22266;&#23450;&#23618;&#20043;&#21518;&#20165;&#28155;&#21152;&#23569;&#37327;&#21442;&#25968;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#25552;&#39640;&#20102;DST&#21644;NLG&#27169;&#22359;&#30340;&#24615;&#33021;&#65292;&#20811;&#26381;&#20102;&#23398;&#20064;&#26354;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Task-Oriented Dialogue (TOD) systems are designed to carry out specific tasks by tracking dialogue states and generating appropriate responses to help users achieve defined goals. Recently, end-to-end dialogue models pre-trained based on large datasets have shown promising performance in the conversational system. However, they share the same parameters to train tasks of the dialogue system (NLU, DST, NLG), so debugging each task is challenging. Also, they require a lot of effort to fine-tune large parameters to create a task-oriented chatbot, making it difficult for non-experts to handle. Therefore, we intend to train relatively lightweight and fast models compared to PLM. In this paper, we propose an End-to-end TOD system with Task-Optimized Adapters which learn independently per task, adding only small number of parameters after fixed layers of pre-trained network. We also enhance the performance of the DST and NLG modules through reinforcement learning, overcoming the learning curv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#24110;&#21161;&#20154;&#20204;&#37325;&#26500;&#36127;&#38754;&#24605;&#24819;&#65292;&#23450;&#20041;&#20102;&#19971;&#20010;&#29992;&#20110;&#37325;&#26500;&#24605;&#24819;&#30340;&#35821;&#35328;&#23646;&#24615;&#26694;&#26550;&#65292;&#24182;&#24320;&#21457;&#20102;&#33258;&#21160;&#21270;&#25351;&#26631;&#12290;&#23454;&#39564;&#21457;&#29616;&#20351;&#29992;&#27169;&#22411;&#36741;&#21161;&#30340;&#21442;&#19982;&#32773;&#26356;&#26377;&#21487;&#33021;&#20135;&#29983;&#31526;&#21512;&#19971;&#20010;&#35821;&#35328;&#23646;&#24615;&#30340;&#37325;&#26500;&#24605;&#24819;&#65292;&#34920;&#26126;&#35813;&#27169;&#22411;&#22312;&#24110;&#21161;&#20154;&#20204;&#37325;&#26500;&#36127;&#38754;&#24605;&#24819;&#19978;&#26159;&#25104;&#21151;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.02466</link><description>&lt;p&gt;
&#36890;&#36807;&#20154;&#31867;&#35821;&#35328;&#27169;&#22411;&#20132;&#20114;&#30340;&#36127;&#38754;&#24605;&#32500;&#35748;&#30693;&#37325;&#26500;
&lt;/p&gt;
&lt;p&gt;
Cognitive Reframing of Negative Thoughts through Human-Language Model Interaction. (arXiv:2305.02466v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#24110;&#21161;&#20154;&#20204;&#37325;&#26500;&#36127;&#38754;&#24605;&#24819;&#65292;&#23450;&#20041;&#20102;&#19971;&#20010;&#29992;&#20110;&#37325;&#26500;&#24605;&#24819;&#30340;&#35821;&#35328;&#23646;&#24615;&#26694;&#26550;&#65292;&#24182;&#24320;&#21457;&#20102;&#33258;&#21160;&#21270;&#25351;&#26631;&#12290;&#23454;&#39564;&#21457;&#29616;&#20351;&#29992;&#27169;&#22411;&#36741;&#21161;&#30340;&#21442;&#19982;&#32773;&#26356;&#26377;&#21487;&#33021;&#20135;&#29983;&#31526;&#21512;&#19971;&#20010;&#35821;&#35328;&#23646;&#24615;&#30340;&#37325;&#26500;&#24605;&#24819;&#65292;&#34920;&#26126;&#35813;&#27169;&#22411;&#22312;&#24110;&#21161;&#20154;&#20204;&#37325;&#26500;&#36127;&#38754;&#24605;&#24819;&#19978;&#26159;&#25104;&#21151;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20811;&#26381;&#36127;&#38754;&#24605;&#24819;&#30340;&#19968;&#31181;&#26377;&#25928;&#30103;&#27861;&#26159;&#29992;&#26356;&#26377;&#24076;&#26395;&#30340;&#8220;&#37325;&#26500;&#24605;&#24819;&#8221;&#21462;&#32780;&#20195;&#20043;&#12290;&#34429;&#28982;&#27835;&#30103;&#21487;&#20197;&#24110;&#21161;&#20154;&#20204;&#32451;&#20064;&#21644;&#23398;&#20064;&#35748;&#30693;&#37325;&#26500;&#36127;&#38754;&#24605;&#24819;&#65292;&#20294;&#20020;&#24202;&#21307;&#29983;&#30701;&#32570;&#21644;&#24515;&#29702;&#20581;&#24247;&#30340;&#27745;&#21517;&#21270;&#36890;&#24120;&#20250;&#38480;&#21046;&#20154;&#20204;&#25509;&#21463;&#27835;&#30103;&#12290;&#26412;&#25991;&#36890;&#36807;&#20154;&#31867;&#20013;&#24515;&#30340;&#26041;&#27861;&#30740;&#31350;&#22914;&#20309;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#24110;&#21161;&#20154;&#20204;&#37325;&#26500;&#36127;&#38754;&#24605;&#24819;&#12290;&#25105;&#20204;&#22522;&#20110;&#24515;&#29702;&#23398;&#25991;&#29486;&#65292;&#23450;&#20041;&#20102;&#19971;&#20010;&#21487;&#20197;&#29992;&#26469;&#37325;&#26500;&#24605;&#24819;&#30340;&#35821;&#35328;&#23646;&#24615;&#26694;&#26550;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#33258;&#21160;&#21270;&#25351;&#26631;&#26469;&#34913;&#37327;&#36825;&#20123;&#23646;&#24615;&#65292;&#24182;&#36890;&#36807;&#24515;&#29702;&#20581;&#24247;&#20174;&#19994;&#32773;&#30340;&#19987;&#23478;&#21028;&#26029;&#36827;&#34892;&#39564;&#35777;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#20174;&#20174;&#19994;&#32773;&#25910;&#38598;&#30340; 600 &#31181;&#24773;&#22659;&#12289;&#24605;&#24819;&#21644;&#37325;&#26500;&#25968;&#25454;&#38598;&#65292;&#24182;&#29992;&#23427;&#26469;&#35757;&#32451;&#19968;&#20010;&#26816;&#32034;&#22686;&#24378;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#29983;&#25104;&#37325;&#26032;&#26500;&#24605;&#21644;&#25511;&#21046;&#23427;&#20204;&#30340;&#35821;&#35328;&#23646;&#24615;&#12290;&#20026;&#20102;&#35843;&#26597;&#8220;&#39640;&#36136;&#37327;&#8221;&#37325;&#26500;&#30340;&#26500;&#25104;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#30001;IRB&#25209;&#20934;&#30340;&#29992;&#25143;&#30740;&#31350;&#65292;&#23558;&#21442;&#19982;&#32773;&#20998;&#37197;&#21040;&#8220;&#27169;&#22411;&#36741;&#21161;&#37325;&#26500;&#8221;&#25110;&#25511;&#21046;&#26465;&#20214;&#12290;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#27169;&#22411;&#36741;&#21161;&#30340;&#21442;&#19982;&#32773;&#26356;&#26377;&#21487;&#33021;&#20135;&#29983;&#31526;&#21512;&#19971;&#20010;&#35821;&#35328;&#23646;&#24615;&#30340;&#37325;&#26500;&#24605;&#24819;&#65292;&#34920;&#26126;&#25105;&#20204;&#30340;&#27169;&#22411;&#25104;&#21151;&#22320;&#24110;&#21161;&#20154;&#20204;&#37325;&#26500;&#36127;&#38754;&#24605;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
A proven therapeutic technique to overcome negative thoughts is to replace them with a more hopeful "reframed thought." Although therapy can help people practice and learn this Cognitive Reframing of Negative Thoughts, clinician shortages and mental health stigma commonly limit people's access to therapy. In this paper, we conduct a human-centered study of how language models may assist people in reframing negative thoughts. Based on psychology literature, we define a framework of seven linguistic attributes that can be used to reframe a thought. We develop automated metrics to measure these attributes and validate them with expert judgements from mental health practitioners. We collect a dataset of 600 situations, thoughts and reframes from practitioners and use it to train a retrieval-enhanced in-context learning model that effectively generates reframed thoughts and controls their linguistic attributes. To investigate what constitutes a "high-quality" reframe, we conduct an IRB-appr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#24182;&#25506;&#31350;&#20102;&#22522;&#20110;&#36716;&#31227;&#21644;&#20027;&#21160;&#23398;&#20064;&#30340;&#31232;&#26377;&#31867;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#21033;&#29992;&#22312;&#23494;&#20999;&#30456;&#20851;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#21644;&#35780;&#20272;&#33719;&#21462;&#31574;&#30053;&#26469;&#35299;&#20915;&#20849;&#25391;&#26816;&#27979;&#30340;&#31232;&#26377;&#31867;&#38382;&#39064;&#65292;&#24182;&#19988;&#21457;&#29616;&#20102;&#19968;&#31181;&#21517;&#20026;PRC&#30340;&#26377;&#25928;&#30340;&#31574;&#30053;&#26469;&#25351;&#23548;&#27880;&#37322;&#12290;</title><link>http://arxiv.org/abs/2305.02459</link><description>&lt;p&gt;
&#36716;&#31227;&#23398;&#20064;&#19982;&#20027;&#21160;&#23398;&#20064;&#29992;&#20110;&#20849;&#40483;&#26816;&#27979;&#65306;&#35299;&#20915;&#31232;&#26377;&#31867;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge. (arXiv:2305.02459v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#24182;&#25506;&#31350;&#20102;&#22522;&#20110;&#36716;&#31227;&#21644;&#20027;&#21160;&#23398;&#20064;&#30340;&#31232;&#26377;&#31867;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#21033;&#29992;&#22312;&#23494;&#20999;&#30456;&#20851;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#21644;&#35780;&#20272;&#33719;&#21462;&#31574;&#30053;&#26469;&#35299;&#20915;&#20849;&#25391;&#26816;&#27979;&#30340;&#31232;&#26377;&#31867;&#38382;&#39064;&#65292;&#24182;&#19988;&#21457;&#29616;&#20102;&#19968;&#31181;&#21517;&#20026;PRC&#30340;&#26377;&#25928;&#30340;&#31574;&#30053;&#26469;&#25351;&#23548;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#31995;&#32479;&#20351;&#24471;&#20351;&#29992;&#26356;&#23569;&#30340;&#35757;&#32451;&#26679;&#20363;&#33021;&#22815;&#24471;&#21040;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#23545;&#20110;&#31232;&#26377;&#31867;&#20219;&#21153;&#65288;&#21363;&#31867;&#21035;&#26631;&#31614;&#38750;&#24120;&#23569;&#35265;&#30340;&#24773;&#20917;&#65292;&#20363;&#22914;&lt;5%&#30340;&#26679;&#26412;&#65289;&#65292;&#25968;&#25454;&#37319;&#38598;&#38556;&#30861;&#20173;&#28982;&#23384;&#22312;&#12290;&#20027;&#21160;&#23398;&#20064;&#19968;&#33324;&#34987;&#25552;&#20986;&#29992;&#20110;&#32531;&#35299;&#36825;&#31181;&#25361;&#25112;&#65292;&#20294;&#36873;&#25321;&#31574;&#30053;&#65292;&#21363;&#36873;&#25321;&#31232;&#26377;&#31867;&#31034;&#20363;&#30340;&#26631;&#20934;&#65292;&#23578;&#26410;&#24471;&#21040;&#31995;&#32479;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#21464;&#21387;&#22120;&#21487;&#20197;&#23454;&#29616;&#36845;&#20195;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#36716;&#31227;&#21644;&#20027;&#21160;&#23398;&#20064;&#35299;&#20915;&#20102;&#36890;&#36807;&#21033;&#29992;&#22312;&#23494;&#20999;&#30456;&#20851;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#21644;&#35780;&#20272;&#33719;&#21462;&#31574;&#30053;&#26469;&#35299;&#20915;&#20849;&#25391;&#26816;&#27979;&#30340;&#31232;&#26377;&#31867;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31181;&#25552;&#20986;&#30340;&#31232;&#26377;&#31867;&#27010;&#29575;&#65288;PRC&#65289;&#26041;&#27861;&#12290;&#25105;&#20204;&#38024;&#23545;&#29305;&#23450;&#30340;&#31232;&#26377;&#31867;&#38382;&#39064;&#65288;&#20174;&#31038;&#20132;&#23186;&#20307;&#20013;&#25910;&#38598;&#35748;&#30693;&#20849;&#25391;&#30340;&#35821;&#35328;&#26679;&#26412;&#65289;&#36827;&#34892;&#20102;&#36825;&#20123;&#23454;&#39564;&#12290;&#25105;&#20204;&#21457;&#29616;PRC&#26159;&#25351;&#23548;&#27880;&#37322;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#31574;&#30053;&#65292;&#26368;&#32456;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#19988;&#36716;&#31227;&#23398;&#20064;&#21487;&#20197;&#22312;&#31232;&#32570;&#25968;&#25454;&#24773;&#20917;&#19979;&#25552;&#20379;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
While transformer-based systems have enabled greater accuracies with fewer training examples, data acquisition obstacles still persist for rare-class tasks -- when the class label is very infrequent (e.g. &lt; 5% of samples). Active learning has in general been proposed to alleviate such challenges, but choice of selection strategy, the criteria by which rare-class examples are chosen, has not been systematically evaluated. Further, transformers enable iterative transfer-learning approaches. We propose and investigate transfer- and active learning solutions to the rare class problem of dissonance detection through utilizing models trained on closely related tasks and the evaluation of acquisition strategies, including a proposed probability-of-rare-class (PRC) approach. We perform these experiments for a specific rare class problem: collecting language samples of cognitive dissonance from social media. We find that PRC is a simple and effective strategy to guide annotations and ultimately
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#20351;&#29992;&#19977;&#31181;&#19981;&#21516;&#30340;&#25991;&#26412;&#34920;&#31034;&#26041;&#24335;&#21644;&#19977;&#20010;&#19981;&#21516;&#30340;&#32858;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#22522;&#20110;&#21333;&#35789;&#39057;&#29575;&#30340;&#24191;&#20041;Jensen-Shannon&#20998;&#27495;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#26368;&#20339;&#26041;&#27861;&#30340;&#36873;&#25321;&#26368;&#32456;&#21462;&#20915;&#20110;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.02457</link><description>&lt;p&gt;
&#37327;&#21270;&#25991;&#26412;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Quantifying the Dissimilarity of Texts. (arXiv:2305.02457v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#20351;&#29992;&#19977;&#31181;&#19981;&#21516;&#30340;&#25991;&#26412;&#34920;&#31034;&#26041;&#24335;&#21644;&#19977;&#20010;&#19981;&#21516;&#30340;&#32858;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#22522;&#20110;&#21333;&#35789;&#39057;&#29575;&#30340;&#24191;&#20041;Jensen-Shannon&#20998;&#27495;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#26368;&#20339;&#26041;&#27861;&#30340;&#36873;&#25321;&#26368;&#32456;&#21462;&#20915;&#20110;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#20004;&#20010;&#25991;&#26412;&#30340;&#24046;&#24322;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#37325;&#35201;&#26041;&#38754;&#65292;&#21253;&#25324;&#35821;&#20041;&#20449;&#24687;&#26816;&#32034;&#12289;&#20027;&#39064;&#20998;&#31867;&#21644;&#25991;&#26723;&#32858;&#31867;&#31561;&#12290;&#26412;&#25991;&#27604;&#36739;&#20102;&#20351;&#29992;&#19977;&#31181;&#19981;&#21516;&#30340;&#25991;&#26412;&#34920;&#31034;&#26041;&#24335;&#65288;&#35789;&#27719;&#12289;&#35789;&#39057;&#20998;&#24067;&#21644;&#21521;&#37327;&#23884;&#20837;&#65289;&#21644;&#19977;&#20010;&#31616;&#21333;&#20219;&#21153;&#65288;&#25353;&#20316;&#32773;&#12289;&#20027;&#39064;&#21644;&#26102;&#38388;&#27573;&#23545;&#25991;&#26412;&#36827;&#34892;&#32858;&#31867;&#65289;&#30340;&#19981;&#21516;&#24046;&#24322;&#24230;&#37327;&#30340;&#23646;&#24615;&#21644;&#24615;&#33021;&#12290;&#36890;&#36807;&#20351;&#29992;Project Gutenberg&#25968;&#25454;&#24211;, &#25105;&#20204;&#21457;&#29616;&#22522;&#20110;&#21333;&#35789;&#39057;&#29575;&#30340;&#24191;&#20041;Jensen-Shannon&#20998;&#27495;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#22522;&#20110;&#21521;&#37327;&#23884;&#20837;&#34920;&#31034;&#30340;D&#23548;&#33268;&#23567;&#25991;&#26412;&#24615;&#33021;&#26356;&#24378;&#65292;&#32780;&#26368;&#20339;&#26041;&#27861;&#30340;&#36873;&#25321;&#26368;&#32456;&#21462;&#20915;&#20110;&#20219;&#21153;&#12290;&#25105;&#20204;&#36824;&#22312;&#29702;&#35770;&#19978;&#21644;&#25968;&#20540;&#19978;&#30740;&#31350;&#20102;&#20004;&#20010;&#25991;&#26412;&#38271;&#24230;&#22240;&#23376;h&#21464;&#21270;&#26102;&#65292;&#19981;&#21516;D&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Quantifying the dissimilarity of two texts is an important aspect of a number of natural language processing tasks, including semantic information retrieval, topic classification, and document clustering. In this paper, we compared the properties and performance of different dissimilarity measures $D$ using three different representations of texts -- vocabularies, word frequency distributions, and vector embeddings -- and three simple tasks -- clustering texts by author, subject, and time period. Using the Project Gutenberg database, we found that the generalised Jensen--Shannon divergence applied to word frequencies performed strongly across all tasks, that $D$'s based on vector embedding representations led to stronger performance for smaller texts, and that the optimal choice of approach was ultimately task-dependent. We also investigated, both analytically and numerically, the behaviour of the different $D$'s when the two texts varied in length by a factor $h$. We demonstrated that
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26816;&#32034;&#22686;&#24378;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;Selfmem&#65292;&#36890;&#36807;&#36845;&#20195;&#29983;&#25104;&#33258;&#25105;&#35760;&#24518;&#27744;&#24182;&#37319;&#29992;&#35760;&#24518;&#36873;&#25321;&#22120;&#65292;&#20351;&#26816;&#32034;&#26356;&#21152;&#33258;&#36866;&#24212;&#65292;&#25552;&#39640;&#20102;&#25991;&#26412;&#29983;&#25104;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.02437</link><description>&lt;p&gt;
&#36816;&#29992;&#33258;&#25105;&#35760;&#24518;&#30340;&#26816;&#32034;&#22686;&#24378;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Lift Yourself Up: Retrieval-augmented Text Generation with Self Memory. (arXiv:2305.02437v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26816;&#32034;&#22686;&#24378;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;Selfmem&#65292;&#36890;&#36807;&#36845;&#20195;&#29983;&#25104;&#33258;&#25105;&#35760;&#24518;&#27744;&#24182;&#37319;&#29992;&#35760;&#24518;&#36873;&#25321;&#22120;&#65292;&#20351;&#26816;&#32034;&#26356;&#21152;&#33258;&#36866;&#24212;&#65292;&#25552;&#39640;&#20102;&#25991;&#26412;&#29983;&#25104;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#36739;&#20110;&#20256;&#32479;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#65292;&#26816;&#32034;&#22686;&#24378;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#33021;&#22815;&#30452;&#25509;&#36845;&#20195;&#20154;&#31867;&#32534;&#20889;&#30340;&#21442;&#32771;&#24211;&#65292;&#24182;&#20174;&#20013;&#26816;&#32034;&#20986;&#30456;&#24212;&#30340;&#20449;&#24687;&#65292;&#20197;&#29983;&#25104;&#26356;&#20248;&#36136;&#30340;&#25991;&#26412;&#12290;&#20294;&#24403;&#21069;&#25991;&#29486;&#23384;&#22312;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#26816;&#32034;&#21040;&#30340;&#35760;&#24518;&#26469;&#33258;&#20110;&#22266;&#23450;&#30340;&#35821;&#26009;&#24211;&#65292;&#20854;&#36136;&#37327;&#23384;&#22312;&#19968;&#23450;&#23616;&#38480;&#24615;&#65292;&#21487;&#33021;&#20250;&#38480;&#21046;&#35760;&#24518;&#22686;&#24378;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;Selfmem&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#36845;&#20195;&#22320;&#37319;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#22120;&#33258;&#36523;&#20197;&#29983;&#25104;&#26080;&#38480;&#21046;&#30340;&#33258;&#25105;&#35760;&#24518;&#27744;&#65292;&#24182;&#20351;&#29992;&#35760;&#24518;&#36873;&#25321;&#22120;&#20026;&#19979;&#19968;&#36718;&#29983;&#25104;&#36873;&#25321;&#19968;&#20010;&#29983;&#25104;&#30340;&#35760;&#24518;&#12290;&#30456;&#32467;&#21512;&#65292;&#36825;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#25552;&#20986;&#20102;&#36816;&#29992;&#33258;&#25105;&#35760;&#24518;&#30340;&#26816;&#32034;&#22686;&#24378;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
With direct access to human-written reference as memory, retrieval-augmented generation has achieved much progress in a wide range of text generation tasks. Since better memory would typically prompt better generation~(we define this as primal problem), previous works mainly focus on how to retrieve better memory. However, one fundamental limitation exists for current literature: the memory is retrieved from a fixed corpus and is bounded by the quality of the corpus. Due to the finite retrieval space, bounded memory would greatly limit the potential of the memory-augmented generation model. In this paper, by exploring the duality of the primal problem: better generation also prompts better memory, we propose a framework called Selfmem, which iteratively adopts a retrieval-augmented generator itself to generate an unbounded memory pool and uses a memory selector to pick one generated memory for the next generation round. By combining the primal and dual problem, a retrieval-augmented ge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;Bert&#21644;ParsBERT&#22312;&#20998;&#26512;&#27874;&#26031;&#24191;&#21578;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#21457;&#29616;ParsBERT&#22312;&#39044;&#27979;&#24191;&#21578;&#21457;&#24067;&#30334;&#20998;&#27604;&#26041;&#38754;&#30340;&#24615;&#33021;&#20248;&#20110;mBERT&#12290;</title><link>http://arxiv.org/abs/2305.02426</link><description>&lt;p&gt;
&#23545;Bert&#21644;ParsBERT&#22312;&#20998;&#26512;&#27874;&#26031;&#24191;&#21578;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
evaluating bert and parsbert for analyzing persian advertisement data. (arXiv:2305.02426v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;Bert&#21644;ParsBERT&#22312;&#20998;&#26512;&#27874;&#26031;&#24191;&#21578;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#21457;&#29616;ParsBERT&#22312;&#39044;&#27979;&#24191;&#21578;&#21457;&#24067;&#30334;&#20998;&#27604;&#26041;&#38754;&#30340;&#24615;&#33021;&#20248;&#20110;mBERT&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#20114;&#32852;&#32593;&#23545;&#29616;&#20195;&#20132;&#26131;&#30340;&#24433;&#21709;&#21450;&#30001;&#27492;&#20135;&#29983;&#30340;&#25968;&#25454;&#23545;&#26426;&#26500;&#25913;&#36827;&#24066;&#22330;&#33829;&#38144;&#30340;&#37325;&#35201;&#24615;&#12290;&#25991;&#31456;&#20197;Divar&#20316;&#20026;&#31034;&#20363;&#65292;&#22312;&#20234;&#26391;&#36827;&#34892;&#22312;&#32447;&#36141;&#20080;&#21644;&#38144;&#21806;&#20135;&#21697;&#19982;&#26381;&#21153;&#30340;&#24066;&#22330;&#65292;&#20030;&#21150;&#20102;&#19968;&#20010;&#39044;&#27979;&#27773;&#36710;&#38144;&#21806;&#24191;&#21578;&#21457;&#24067;&#30334;&#20998;&#27604;&#30340;&#31454;&#36187;&#12290;&#30001;&#20110;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#27874;&#26031;&#25991;&#26412;&#25968;&#25454;&#65292;&#20316;&#32773;&#20351;&#29992;Hazm&#24211;&#21644;&#20004;&#20010;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;mBERT&#21644;ParsBERT&#26469;&#20998;&#26512;&#23427;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#27604;&#36739;mBERT&#21644;ParsBERT&#22312;Divar&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;&#20316;&#32773;&#25552;&#20379;&#20102;&#26377;&#20851;&#25968;&#25454;&#25366;&#25496;&#12289;&#27874;&#26031;&#35821;&#35328;&#21644;&#20004;&#31181;&#35821;&#35328;&#27169;&#22411;&#30340;&#32972;&#26223;&#65292;&#26816;&#26597;&#25968;&#25454;&#38598;&#30340;&#32452;&#25104;&#21644;&#32479;&#35745;&#29305;&#24449;&#65292;&#24182;&#25552;&#20379;&#20102;&#20182;&#20204;&#23545;&#20004;&#31181;&#26041;&#27861;&#30340;&#24494;&#35843;&#21644;&#35757;&#32451;&#37197;&#32622;&#30340;&#35814;&#32454;&#35828;&#26126;&#12290;&#26368;&#32456;&#65292;&#20182;&#20204;&#35780;&#20272;&#21644;&#27604;&#36739;&#20102;mBERT&#21644;ParsBERT&#22312;&#39044;&#27979;&#27773;&#36710;&#38144;&#21806;&#24191;&#21578;&#21457;&#24067;&#30334;&#20998;&#27604;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#32467;&#26524;&#26174;&#31034;ParsBERT&#22312;&#20934;&#30830;&#24230;&#21644;f1&#20998;&#25968;&#26041;&#38754;&#22343;&#20248;&#20110;mBERT&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper discusses the impact of the Internet on modern trading and the importance of data generated from these transactions for organizations to improve their marketing efforts. The paper uses the example of Divar, an online marketplace for buying and selling products and services in Iran, and presents a competition to predict the percentage of a car sales ad that would be published on the Divar website. Since the dataset provides a rich source of Persian text data, the authors use the Hazm library, a Python library designed for processing Persian text, and two state-of-the-art language models, mBERT and ParsBERT, to analyze it. The paper's primary objective is to compare the performance of mBERT and ParsBERT on the Divar dataset. The authors provide some background on data mining, Persian language, and the two language models, examine the dataset's composition and statistical features, and provide details on their fine-tuning and training configurations for both approaches. They pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#23545;&#21518;&#38376;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#21457;&#29616;&#21482;&#27880;&#20837;0.2&#65285;&#30340;&#26679;&#26412;&#21363;&#21487;&#35753;&#27169;&#22411;&#29983;&#25104;&#25351;&#23450;&#30340;&#20851;&#38190;&#35789;&#21644;&#25972;&#20010;&#21477;&#23376;&#65292;&#21033;&#29992;&#23383;&#33410;&#23545;&#32534;&#30721;&#21019;&#24314;&#22810;&#20010;&#26032;&#35302;&#21457;&#22120;&#23545;&#21518;&#38376;&#26816;&#27979;&#24102;&#26469;&#20102;&#26032;&#25361;&#25112;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#21487;&#20197;&#36798;&#21040;90&#65285;&#20197;&#19978;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.02424</link><description>&lt;p&gt;
&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#30340;&#21518;&#38376;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Backdoor Learning on Sequence to Sequence Models. (arXiv:2305.02424v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#23545;&#21518;&#38376;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#21457;&#29616;&#21482;&#27880;&#20837;0.2&#65285;&#30340;&#26679;&#26412;&#21363;&#21487;&#35753;&#27169;&#22411;&#29983;&#25104;&#25351;&#23450;&#30340;&#20851;&#38190;&#35789;&#21644;&#25972;&#20010;&#21477;&#23376;&#65292;&#21033;&#29992;&#23383;&#33410;&#23545;&#32534;&#30721;&#21019;&#24314;&#22810;&#20010;&#26032;&#35302;&#21457;&#22120;&#23545;&#21518;&#38376;&#26816;&#27979;&#24102;&#26469;&#20102;&#26032;&#25361;&#25112;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#21487;&#20197;&#36798;&#21040;90&#65285;&#20197;&#19978;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#38376;&#25915;&#20987;&#24050;&#25104;&#20026;&#26500;&#24314;&#21487;&#20449;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#26032;&#20852;&#30740;&#31350;&#39046;&#22495;&#12290;&#34429;&#28982;&#24456;&#22810;&#30740;&#31350;&#24050;&#32463;&#30740;&#31350;&#20102;&#23545;&#22270;&#20687;&#25110;&#25991;&#26412;&#20998;&#31867;&#30340;&#21518;&#38376;&#25915;&#20987;&#30340;&#38544;&#34255;&#21361;&#38505;&#65292;&#20294;&#23545;&#20110;&#36755;&#20986;&#31354;&#38388;&#20026;&#26080;&#38480;&#21644;&#31163;&#25955;&#30340;&#27169;&#22411;&#22312;&#21518;&#38376;&#25915;&#20987;&#19979;&#30340;&#40065;&#26834;&#24615;&#21364;&#20102;&#35299;&#26377;&#38480;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#20010;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#21363;&#27979;&#35797;&#24207;&#21015;&#21040;&#24207;&#21015;(seq2seq)&#27169;&#22411;&#26159;&#21542;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#21482;&#27880;&#20837;0.2&#65285;&#30340;&#26679;&#26412;&#21363;&#21487;&#20351;seq2seq&#27169;&#22411;&#29983;&#25104;&#25351;&#23450;&#30340;&#20851;&#38190;&#35789;&#65292;&#29978;&#33267;&#25972;&#20010;&#21477;&#23376;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#23383;&#33410;&#23545;&#32534;&#30721;(Byte Pair Encoding, BPE)&#26469;&#21019;&#24314;&#22810;&#20010;&#26032;&#35302;&#21457;&#22120;&#65292;&#36825;&#32473;&#21518;&#38376;&#26816;&#27979;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#36825;&#20123;&#21518;&#38376;&#26159;&#19981;&#22266;&#23450;&#30340;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#26426;&#22120;&#32763;&#35793;&#21644;&#25991;&#26412;&#25688;&#35201;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#21487;&#20197;&#36798;&#21040;90&#65285;&#20197;&#19978;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Backdoor learning has become an emerging research area towards building a trustworthy machine learning system. While a lot of works have studied the hidden danger of backdoor attacks in image or text classification, there is a limited understanding of the model's robustness on backdoor attacks when the output space is infinite and discrete. In this paper, we study a much more challenging problem of testing whether sequence-to-sequence (seq2seq) models are vulnerable to backdoor attacks. Specifically, we find by only injecting 0.2\% samples of the dataset, we can cause the seq2seq model to generate the designated keyword and even the whole sentence. Furthermore, we utilize Byte Pair Encoding (BPE) to create multiple new triggers, which brings new challenges to backdoor detection since these backdoors are not static. Extensive experiments on machine translation and text summarization have been conducted to show our proposed methods could achieve over 90\% attack success rate on multiple 
&lt;/p&gt;</description></item><item><title>PTP&#31639;&#27861;&#24341;&#20837;&#22522;&#20110;&#25200;&#21160;&#30340;&#27491;&#21017;&#21270;&#22120;&#26469;&#24179;&#28369;loss&#22270;&#20687;&#65292;&#25552;&#21319;prompt tuning&#24615;&#33021;&#21644;&#31283;&#23450;&#24615;&#65292;&#22312;&#22235;&#20010;&#27979;&#35797;&#25968;&#25454;&#38598;&#20013;&#33719;&#24471;&#20102;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.02423</link><description>&lt;p&gt;
PTP&#65306;&#21033;&#29992;&#22522;&#20110;&#25200;&#21160;&#30340;&#27491;&#21017;&#21270;&#22120;&#25552;&#21319;Prompt Tuning&#30340;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
PTP: Boosting Stability and Performance of Prompt Tuning with Perturbation-Based Regularizer. (arXiv:2305.02423v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02423
&lt;/p&gt;
&lt;p&gt;
PTP&#31639;&#27861;&#24341;&#20837;&#22522;&#20110;&#25200;&#21160;&#30340;&#27491;&#21017;&#21270;&#22120;&#26469;&#24179;&#28369;loss&#22270;&#20687;&#65292;&#25552;&#21319;prompt tuning&#24615;&#33021;&#21644;&#31283;&#23450;&#24615;&#65292;&#22312;&#22235;&#20010;&#27979;&#35797;&#25968;&#25454;&#38598;&#20013;&#33719;&#24471;&#20102;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#19978;&#65292;&#20351;&#29992;prompt tuning&#27604;&#24494;&#35843;&#26041;&#27861;&#26356;&#33021;&#21457;&#25381;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21147;&#37327;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;prompt tuning&#26041;&#27861;&#23384;&#22312;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#22240;&#20026;&#19981;&#21516;&#38543;&#26426;&#31181;&#23376;&#19979;&#30340;&#20998;&#25968;&#26041;&#24046;&#30456;&#24403;&#22823;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#35843;&#26597;&#24182;&#21457;&#29616;&#65292;&#26222;&#36890;&#30340;prompt tuning&#30340;&#25439;&#22833;&#20989;&#25968;&#22270;&#20687;&#22312;&#21487;&#35270;&#21270;&#26102;&#21576;&#23789;&#22721;&#29366;&#65292;&#36755;&#20837;&#25968;&#25454;&#30340;&#24494;&#23567;&#21464;&#21270;&#21487;&#20197;&#23548;&#33268;&#25439;&#22833;&#20989;&#25968;&#22270;&#20687;&#30340;&#21095;&#28872;&#27874;&#21160;&#12290;&#36825;&#26159;&#23548;&#33268;prompt tuning&#19981;&#31283;&#23450;&#24615;&#30340;&#19968;&#20010;&#37325;&#35201;&#22240;&#32032;&#12290;&#22522;&#20110;&#36825;&#20010;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#23558;&#24179;&#28369;&#25439;&#22833;&#20989;&#25968;&#22270;&#20687;&#30340;&#22522;&#20110;&#25200;&#21160;&#30340;&#27491;&#21017;&#21270;&#22120;&#24341;&#20837;&#21040;prompt tuning&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PTP&#30340;&#26032;&#31639;&#27861;&#65292;&#23427;&#19981;&#20165;&#21487;&#20197;&#26174;&#33879;&#20943;&#36731;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#65292;&#36824;&#21487;&#20197;&#25552;&#39640;prompt tuning&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;&#22522;&#20110;&#25200;&#21160;&#30340;&#27491;&#21017;&#21270;&#22120;&#65292;&#24182;&#22312;&#22235;&#20010;&#21463;&#27426;&#36814;&#30340;NLU&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PTP&#22312;&#36229;&#32423;GLUE&#21644;GLUE&#19978;&#20998;&#21035;&#33719;&#24471;&#20102;&#39640;&#36798;3.9&#65285;&#21644;2.0&#65285;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#21487;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;prompt tuning&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;PTP&#36824;&#21487;&#20197;&#25552;&#39640;prompt tuning&#30340;&#40065;&#26834;&#24615;&#65292;&#20351;&#22810;&#27425;&#36816;&#34892;&#33719;&#24471;&#30340;&#24615;&#33021;&#26631;&#20934;&#20559;&#24046;&#26356;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies show that prompt tuning can better leverage the power of large language models than fine-tuning on downstream natural language understanding tasks. However, the existing prompt tuning methods have training instability issues, as the variance of scores under different random seeds is quite large. To address this critical problem, we first investigate and find that the loss landscape of vanilla prompt tuning is precipitous when it is visualized, where a slight change of input data can cause a big fluctuation in the loss landscape. This is an essential factor that leads to the instability of prompt tuning. Based on this observation, we introduce perturbation-based regularizers, which can smooth the loss landscape, into prompt tuning. We propose a new algorithm, called Prompt Tuning with Perturbation-based regularizer~(PTP), which can not only alleviate training instability dramatically but also boost the performance of prompt tuning. We design two kinds of perturbation-base
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Plan&#65292;Eliminate&#65292;&#21644;Track&#65288;PET&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24110;&#21161;&#26234;&#33021;&#20307;&#31616;&#21270;&#25511;&#21046;&#20219;&#21153;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;LLM&#30452;&#25509;&#20316;&#20026;&#26234;&#33021;&#20307;&#25152;&#38754;&#20020;&#30340;&#19968;&#20123;&#38480;&#21046;&#21644;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.02412</link><description>&lt;p&gt;
&#35745;&#21010;&#12289;&#28040;&#38500;&#21644;&#36319;&#36394;&#8212;&#8212;&#35821;&#35328;&#27169;&#22411;&#26159;&#20855;&#22791;&#20307;&#39564;&#30340;&#26234;&#33021;&#20307;&#30340;&#33391;&#24072;&#30410;&#21451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Plan, Eliminate, and Track -- Language Models are Good Teachers for Embodied Agents. (arXiv:2305.02412v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02412
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Plan&#65292;Eliminate&#65292;&#21644;Track&#65288;PET&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24110;&#21161;&#26234;&#33021;&#20307;&#31616;&#21270;&#25511;&#21046;&#20219;&#21153;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;LLM&#30452;&#25509;&#20316;&#20026;&#26234;&#33021;&#20307;&#25152;&#38754;&#20020;&#30340;&#19968;&#20123;&#38480;&#21046;&#21644;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21487;&#20197;&#25429;&#25417;&#21040;&#20851;&#20110;&#19990;&#30028;&#30340;&#31243;&#24207;&#21270;&#30693;&#35782;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21033;&#29992;LLM&#20135;&#29983;&#30340;&#25277;&#35937;&#35745;&#21010;&#26469;&#31616;&#21270;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25511;&#21046;&#20219;&#21153;&#65292;&#36890;&#36807;&#21160;&#20316;&#25171;&#20998;&#25110;&#21160;&#20316;&#24314;&#27169;&#65288;&#24494;&#35843;&#65289;&#26469;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#21464;&#21387;&#22120;&#26550;&#26500;&#32487;&#25215;&#20102;&#20960;&#20010;&#38480;&#21046;&#65292;&#20351;&#24471;LLM&#38590;&#20197;&#30452;&#25509;&#20316;&#20026;&#26234;&#33021;&#20307;&#65306;&#20363;&#22914;&#26377;&#38480;&#30340;&#36755;&#20837;&#38271;&#24230;&#65292;&#24494;&#35843;&#30340;&#25928;&#29575;&#65292;&#39044;&#35757;&#32451;&#30340;&#20559;&#35265;&#20197;&#21450;&#19982;&#38750;&#25991;&#26412;&#29615;&#22659;&#30340;&#19981;&#20860;&#23481;&#24615;&#12290;&#20026;&#20102;&#19982;&#20302;&#32423;&#21035;&#21487;&#35757;&#32451;&#30340;&#25191;&#34892;&#22120;&#20445;&#25345;&#20860;&#23481;&#24615;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;LLMs&#20013;&#30340;&#30693;&#35782;&#26469;&#31616;&#21270;&#25511;&#21046;&#38382;&#39064;&#65292;&#32780;&#19981;&#26159;&#35299;&#20915;&#38382;&#39064;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;Plan&#65292;Eliminate&#21644;Track&#65288;PET&#65289;&#26694;&#26550;&#12290;&#35745;&#21010;&#27169;&#22359;&#23558;&#20219;&#21153;&#25551;&#36848;&#36716;&#21270;&#20026;&#39640;&#23618;&#27425;&#23376;&#20219;&#21153;&#30340;&#21015;&#34920;&#12290;&#28040;&#38500;&#27169;&#22359;&#20174;&#24403;&#21069;&#23376;&#20219;&#21153;&#30340;&#35266;&#23519;&#20013;&#23631;&#34109;&#19981;&#30456;&#20851;&#30340;&#23545;&#35937;&#21644;&#23481;&#22120;&#12290;&#26368;&#21518;&#65292;&#36319;&#36394;&#27169;&#22359;&#30830;&#23450;&#26234;&#33021;&#20307;&#26159;&#21542;&#24050;&#32463;&#23454;&#29616;&#20102;&#24403;&#21069;&#23376;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained large language models (LLMs) capture procedural knowledge about the world. Recent work has leveraged LLM's ability to generate abstract plans to simplify challenging control tasks, either by action scoring, or action modeling (fine-tuning). However, the transformer architecture inherits several constraints that make it difficult for the LLM to directly serve as the agent: e.g. limited input lengths, fine-tuning inefficiency, bias from pre-training, and incompatibility with non-text environments. To maintain compatibility with a low-level trainable actor, we propose to instead use the knowledge in LLMs to simplify the control problem, rather than solving it. We propose the Plan, Eliminate, and Track (PET) framework. The Plan module translates a task description into a list of high-level sub-tasks. The Eliminate module masks out irrelevant objects and receptacles from the observation for the current sub-task. Finally, the Track module determines whether the agent has accompli
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24402;&#22240;&#30340;&#31649;&#36947;AttDef&#65292;&#29992;&#20110;&#38450;&#24481;&#20004;&#31181;&#25554;&#20837;&#24335;&#27745;&#26579;&#25915;&#20987;BadNL&#21644;InSent&#65292;&#35813;&#31649;&#36947;&#21487;&#20197;&#25104;&#21151;&#32531;&#35299;&#25554;&#20837;&#24335;&#25991;&#26412;&#21518;&#38376;&#25915;&#20987;&#24182;&#22312;&#22235;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;56.59%&#33267;79.97%&#21644;15.25%&#33267;48.34%&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.02394</link><description>&lt;p&gt;
&#22522;&#20110;&#24402;&#22240;&#30340;&#38450;&#24481;&#25554;&#20837;&#24335;&#25991;&#26412;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Defending against Insertion-based Textual Backdoor Attacks via Attribution. (arXiv:2305.02394v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02394
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24402;&#22240;&#30340;&#31649;&#36947;AttDef&#65292;&#29992;&#20110;&#38450;&#24481;&#20004;&#31181;&#25554;&#20837;&#24335;&#27745;&#26579;&#25915;&#20987;BadNL&#21644;InSent&#65292;&#35813;&#31649;&#36947;&#21487;&#20197;&#25104;&#21151;&#32531;&#35299;&#25554;&#20837;&#24335;&#25991;&#26412;&#21518;&#38376;&#25915;&#20987;&#24182;&#22312;&#22235;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;56.59%&#33267;79.97%&#21644;15.25%&#33267;48.34%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21518;&#38376;&#25915;&#20987;&#26159;&#19968;&#31181;&#26032;&#22411;&#25915;&#20987;&#27169;&#24335;&#65292;&#24050;&#34987;&#35777;&#26126;&#22312;&#35757;&#32451;&#26399;&#38388;&#21521;&#27169;&#22411;&#28155;&#21152;&#21518;&#38376;&#26159;&#26377;&#25928;&#30340;&#12290;&#38450;&#24481;&#27492;&#31867;&#21518;&#38376;&#25915;&#20987;&#24050;&#21464;&#24471;&#32039;&#36843;&#21644;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AttDef&#30340;&#39640;&#25928;&#24402;&#22240;&#31649;&#36947;&#65292;&#29992;&#20110;&#38450;&#24481;&#20004;&#31181;&#25554;&#20837;&#24335;&#27745;&#26579;&#25915;&#20987;BadNL&#21644;InSent&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#20855;&#26377;&#36739;&#22823;&#24402;&#22240;&#20998;&#25968;&#30340;&#20196;&#29260;&#35270;&#20026;&#28508;&#22312;&#35302;&#21457;&#22120;&#65292;&#22240;&#20026;&#36739;&#22823;&#30340;&#24402;&#22240;&#35789;&#23545;&#20110;&#38169;&#35823;&#39044;&#27979;&#32467;&#26524;&#20570;&#20986;&#36739;&#22823;&#36129;&#29486;&#65292;&#22240;&#27492;&#26356;&#26377;&#21487;&#33021;&#26159;&#27745;&#26579;&#35302;&#21457;&#22120;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#21033;&#29992;&#22806;&#37096;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26469;&#21306;&#20998;&#36755;&#20837;&#26159;&#21542;&#34987;&#27745;&#26579;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#20004;&#31181;&#24120;&#35265;&#30340;&#25915;&#20987;&#22330;&#26223;&#65288;&#27745;&#26579;&#35757;&#32451;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#65289;&#20013;&#20855;&#26377;&#36275;&#22815;&#30340;&#27867;&#21270;&#24615;&#65292;&#36825;&#19968;&#28857;&#25345;&#32493;&#25913;&#21892;&#20102;&#20043;&#21069;&#30340;&#26041;&#27861;&#12290;&#20363;&#22914;&#65292;AttDef&#22312;&#22235;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21487;&#20197;&#25104;&#21151;&#32531;&#35299;&#20004;&#31181;&#25915;&#20987;&#65292;&#24179;&#22343;&#20934;&#30830;&#29575;&#20026;79.97%&#65288;&#25552;&#39640;&#20102;56.59%&#65289;&#21644;48.34%&#65288;&#25552;&#39640;&#20102;15.25%&#65289;&#65292;&#35777;&#26126;&#20102;&#23427;&#22312;&#38450;&#24481;&#25554;&#20837;&#24335;&#25991;&#26412;&#21518;&#38376;&#25915;&#20987;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Textual backdoor attack, as a novel attack model, has been shown to be effective in adding a backdoor to the model during training. Defending against such backdoor attacks has become urgent and important. In this paper, we propose AttDef, an efficient attribution-based pipeline to defend against two insertion-based poisoning attacks, BadNL and InSent. Specifically, we regard the tokens with larger attribution scores as potential triggers since larger attribution words contribute more to the false prediction results and therefore are more likely to be poison triggers. Additionally, we further utilize an external pre-trained language model to distinguish whether input is poisoned or not. We show that our proposed method can generalize sufficiently well in two common attack scenarios (poisoning training data and testing data), which consistently improves previous methods. For instance, AttDef can successfully mitigate both attacks with an average accuracy of 79.97% (56.59% up) and 48.34% 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#27169;&#22411;&#36924;&#36817;CKY&#31639;&#27861;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#26799;&#24230;&#39044;&#27979;&#35299;&#26512;&#30340;&#26041;&#27861;&#65292;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#31454;&#20105;&#21147;&#26356;&#22909;&#65292;&#21516;&#26102;&#36895;&#24230;&#26356;&#24555;&#12290;&#22312;&#38543;&#26426;PCFG&#19979;&#35299;&#26512;&#26102;&#65292;&#24615;&#33021;&#19979;&#38477;&#65292;&#20294;&#21152;&#20837;&#39069;&#22806;&#30340;&#24402;&#32435;&#20559;&#24046;&#26159;&#26377;&#24110;&#21161;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.02386</link><description>&lt;p&gt;
&#29992;Transformer&#36924;&#36817;CKY&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Approximating CKY with Transformers. (arXiv:2305.02386v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#27169;&#22411;&#36924;&#36817;CKY&#31639;&#27861;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#26799;&#24230;&#39044;&#27979;&#35299;&#26512;&#30340;&#26041;&#27861;&#65292;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#31454;&#20105;&#21147;&#26356;&#22909;&#65292;&#21516;&#26102;&#36895;&#24230;&#26356;&#24555;&#12290;&#22312;&#38543;&#26426;PCFG&#19979;&#35299;&#26512;&#26102;&#65292;&#24615;&#33021;&#19979;&#38477;&#65292;&#20294;&#21152;&#20837;&#39069;&#22806;&#30340;&#24402;&#32435;&#20559;&#24046;&#26159;&#26377;&#24110;&#21161;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#27169;&#22411;&#36924;&#36817;CKY&#31639;&#27861;&#30340;&#33021;&#21147;&#65292;&#30452;&#25509;&#39044;&#27979;&#21477;&#23376;&#30340;&#35299;&#26512;&#65292;&#36991;&#20813;&#20102;CKY&#31639;&#27861;&#23545;&#21477;&#23376;&#38271;&#24230;&#30340;&#19977;&#27425;&#20381;&#36182;&#12290;&#22312;&#26631;&#20934;&#30340;&#32452;&#25104;&#21477;&#20998;&#26512;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#27604;&#20351;&#29992;CKY&#30340;&#21487;&#27604;&#20998;&#26512;&#22120;&#21462;&#24471;&#20102;&#31454;&#20105;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#36895;&#24230;&#26356;&#24555;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#22312;&#38543;&#26426;PCFG&#19979;&#36827;&#34892;&#35299;&#26512;&#30340;&#21487;&#34892;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#35821;&#27861;&#21464;&#24471;&#26356;&#21152;&#27169;&#31946;&#30340;&#24773;&#20917;&#19979;&#65292;&#24615;&#33021;&#19979;&#38477;&#65292;&#36825;&#34920;&#26126;Transformer&#27809;&#26377;&#23436;&#20840;&#25429;&#25417;&#21040;CKY&#35745;&#31639;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#20063;&#21457;&#29616;&#65292;&#32467;&#21512;&#39069;&#22806;&#30340;&#24402;&#32435;&#20559;&#24046;&#26159;&#26377;&#24110;&#21161;&#30340;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#30456;&#23545;&#20110;&#22270;&#34920;&#34920;&#31034;&#30340;&#26799;&#24230;&#26469;&#39044;&#27979;&#35299;&#26512;&#65292;&#31867;&#27604;&#20110;CKY&#31639;&#27861;&#19982;&#22270;&#34920;&#30456;&#20851;&#30340;&#19968;&#20010;&#20998;&#21306;&#20989;&#25968;&#21464;&#20307;&#30340;&#23376;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the ability of transformer models to approximate the CKY algorithm, using them to directly predict a parse and thus avoid the CKY algorithm's cubic dependence on sentence length. We find that on standard constituency parsing benchmarks this approach achieves competitive or better performance than comparable parsers that make use of CKY, while being faster. We also evaluate the viability of this approach for parsing under random PCFGs. Here we find that performance declines as the grammar becomes more ambiguous, suggesting that the transformer is not fully capturing the CKY computation. However, we also find that incorporating additional inductive bias is helpful, and we propose a novel approach that makes use of gradients with respect to chart representations in predicting the parse, in analogy with the CKY algorithm being the subgradient of a partition function variant with respect to the chart.
&lt;/p&gt;</description></item><item><title>PeaCoK&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#35282;&#33394;&#24120;&#35782;&#30693;&#35782;&#22270;&#65292;&#21253;&#21547;&#32422;10&#19975;&#20010;&#32463;&#36807;&#20154;&#31867;&#39564;&#35777;&#30340;&#35282;&#33394;&#20107;&#23454;&#12290;&#35813;&#30693;&#35782;&#22270;&#23637;&#29616;&#20102;&#22312;&#20197;&#21069;&#30340;&#20154;&#31867;&#20132;&#20114;&#34892;&#20026;&#30740;&#31350;&#20013;&#30830;&#23450;&#30340;&#20116;&#20010;&#35282;&#33394;&#30693;&#35782;&#32500;&#24230;&#65292;&#24182;&#21306;&#20998;&#20102;&#24120;&#35782;&#21644;&#24773;&#24863;&#23618;&#38754;&#12290;</title><link>http://arxiv.org/abs/2305.02364</link><description>&lt;p&gt;
PeaCoK: &#32500;&#25345;&#19968;&#33268;&#24182;&#24341;&#20154;&#20837;&#32988;&#30340;&#21465;&#36848;&#25152;&#38656;&#30340;&#35282;&#33394;&#24120;&#35782;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
PeaCoK: Persona Commonsense Knowledge for Consistent and Engaging Narratives. (arXiv:2305.02364v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02364
&lt;/p&gt;
&lt;p&gt;
PeaCoK&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#35282;&#33394;&#24120;&#35782;&#30693;&#35782;&#22270;&#65292;&#21253;&#21547;&#32422;10&#19975;&#20010;&#32463;&#36807;&#20154;&#31867;&#39564;&#35777;&#30340;&#35282;&#33394;&#20107;&#23454;&#12290;&#35813;&#30693;&#35782;&#22270;&#23637;&#29616;&#20102;&#22312;&#20197;&#21069;&#30340;&#20154;&#31867;&#20132;&#20114;&#34892;&#20026;&#30740;&#31350;&#20013;&#30830;&#23450;&#30340;&#20116;&#20010;&#35282;&#33394;&#30693;&#35782;&#32500;&#24230;&#65292;&#24182;&#21306;&#20998;&#20102;&#24120;&#35782;&#21644;&#24773;&#24863;&#23618;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32500;&#25345;&#19968;&#33268;&#19988;&#24341;&#20154;&#20837;&#32988;&#30340;&#21465;&#36848;&#38656;&#35201;&#23545;&#35805;&#25110;&#25925;&#20107;&#20195;&#29702;&#20154;&#29702;&#35299;&#35828;&#35805;&#32773;&#25110;&#21548;&#20247;&#30340;&#35282;&#33394;&#22914;&#20309;&#19982;&#21465;&#36848;&#30456;&#20851;&#32852;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36825;&#20123;&#20195;&#29702;&#20154;&#24517;&#39035;&#25512;&#26029;&#20182;&#20204;&#21548;&#20247;&#30340;&#35282;&#33394;&#65292;&#20197;&#20135;&#29983;&#31526;&#21512;&#20182;&#20204;&#20852;&#36259;&#30340;&#38472;&#36848;&#12290;&#20182;&#20204;&#36824;&#24517;&#39035;&#23398;&#20064;&#22312;&#25972;&#20010;&#21465;&#36848;&#20013;&#20445;&#25345;&#19968;&#33268;&#30340;&#35828;&#35805;&#32773;&#35282;&#33394;&#65292;&#20197;&#20415;&#20182;&#20204;&#30340;&#23545;&#31561;&#26041;&#24863;&#21040;&#21442;&#19982;&#20854;&#20013;&#24182;&#19988;&#26159;&#19968;&#27573;&#36924;&#30495;&#30340;&#23545;&#35805;&#25110;&#25925;&#20107;&#12290;&#28982;&#32780;&#65292;&#35282;&#33394;&#26159;&#22810;&#26679;&#21270;&#21644;&#22797;&#26434;&#30340;&#65306;&#23427;&#20204;&#21253;&#21547;&#22823;&#37327;&#20016;&#23500;&#30340;&#30456;&#20114;&#20851;&#32852;&#30340;&#19990;&#30028;&#30693;&#35782;&#65292;&#36825;&#23545;&#20110;&#24378;&#22823;&#30340;&#19968;&#33324;&#21465;&#36848;&#31995;&#32479;&#32780;&#35328;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65288;&#20363;&#22914;&#65292;&#27468;&#25163;&#25797;&#38271;&#21809;&#27468;&#65292;&#24182;&#21487;&#33021;&#26366;&#21442;&#21152;&#36807;&#38899;&#20048;&#23398;&#38498;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#35268;&#27169;&#35282;&#33394;&#24120;&#35782;&#30693;&#35782;&#22270;PeaCoK&#65292;&#20854;&#20013;&#21253;&#21547;&#32422;10&#19975;&#20010;&#32463;&#36807;&#20154;&#31867;&#39564;&#35777;&#30340;&#35282;&#33394;&#20107;&#23454;&#12290;&#25105;&#20204;&#30340;&#30693;&#35782;&#22270;&#34920;&#29616;&#20102;&#22312;&#20197;&#21069;&#30340;&#20154;&#31867;&#20132;&#20114;&#34892;&#20026;&#30740;&#31350;&#20013;&#30830;&#23450;&#30340;&#20116;&#20010;&#35282;&#33394;&#30693;&#35782;&#32500;&#24230;&#65292;&#24182;&#21306;&#20998;&#20102;&#24120;&#35782;&#21644;&#24773;&#24863;&#23618;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sustaining coherent and engaging narratives requires dialogue or storytelling agents to understand how the personas of speakers or listeners ground the narrative. Specifically, these agents must infer personas of their listeners to produce statements that cater to their interests. They must also learn to maintain consistent speaker personas for themselves throughout the narrative, so that their counterparts feel involved in a realistic conversation or story.  However, personas are diverse and complex: they entail large quantities of rich interconnected world knowledge that is challenging to robustly represent in general narrative systems (e.g., a singer is good at singing, and may have attended conservatoire). In this work, we construct a new large-scale persona commonsense knowledge graph, PeaCoK, containing ~100K human-validated persona facts. Our knowledge graph schematizes five dimensions of persona knowledge identified in previous studies of human interactive behaviours, and disti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36861;&#36394;&#23454;&#20307;&#29366;&#24577;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#32463;&#36807;&#22823;&#37327;&#20195;&#30721;&#39044;&#35757;&#32451;&#30340;GPT-3.5&#27169;&#22411;&#34920;&#29616;&#26368;&#22909;&#65292;&#21363;&#20351;&#35757;&#32451;&#21644;&#35780;&#20272;&#20013;&#20960;&#20046;&#27809;&#26377;&#35789;&#27719;&#37325;&#21472;&#30340;&#24773;&#20917;&#19979;&#65292;&#20173;&#28982;&#21487;&#20197;&#33719;&#24471;&#19981;&#38169;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.02363</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23454;&#20307;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
Entity Tracking in Language Models. (arXiv:2305.02363v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36861;&#36394;&#23454;&#20307;&#29366;&#24577;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#32463;&#36807;&#22823;&#37327;&#20195;&#30721;&#39044;&#35757;&#32451;&#30340;GPT-3.5&#27169;&#22411;&#34920;&#29616;&#26368;&#22909;&#65292;&#21363;&#20351;&#35757;&#32451;&#21644;&#35780;&#20272;&#20013;&#20960;&#20046;&#27809;&#26377;&#35789;&#27719;&#37325;&#21472;&#30340;&#24773;&#20917;&#19979;&#65292;&#20173;&#28982;&#21487;&#20197;&#33719;&#24471;&#19981;&#38169;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36861;&#36394;&#25805;&#20316;&#23545;&#35937;&#30340;&#29366;&#24577;&#24182;&#36319;&#36394;&#23427;&#20204;&#38543;&#25991;&#26412;&#25110;&#23545;&#35805;&#30340;&#23637;&#24320;&#32780;&#21457;&#29983;&#30340;&#20851;&#31995;&#21464;&#21270;&#26159;&#29702;&#35299;&#35805;&#35821;&#30340;&#20851;&#38190;&#21069;&#25552;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36861;&#36394;&#35805;&#35821;&#23454;&#20307;&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#24456;&#23569;&#30340;&#31995;&#32479;&#35843;&#26597;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#39033;&#20219;&#21153;&#65292;&#20197;&#25506;&#31350;&#35821;&#35328;&#27169;&#22411;&#22312;&#32473;&#23450;&#21021;&#22987;&#29366;&#24577;&#30340;&#33521;&#25991;&#25551;&#36848;&#21644;&#19968;&#31995;&#21015;&#29366;&#24577;&#26356;&#25913;&#25805;&#20316;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#25512;&#26029;&#20986;&#23454;&#20307;&#30340;&#26368;&#32456;&#29366;&#24577;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Keeping track of how states and relations of entities change as a text or dialog unfolds is a key prerequisite to discourse understanding. Despite this fact, there have been few systematic investigations into the ability of large language models (LLMs) to track discourse entities. In this work, we present a task to probe to what extent a language model can infer the final state of an entity given an English description of the initial state and a series of state-changing operations. We use this task to first investigate whether Flan-T5, GPT-3 and GPT-3.5 can track the state of entities, and find that only GPT-3.5 models, which have been pretrained on large amounts of code, exhibit this ability. We then investigate whether smaller models pretrained primarily on text can learn to track entities, through finetuning T5 on several training/evaluation splits. While performance degrades for more complex splits, we find that even for splits with almost no lexical overlap between training and ev
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#22312;&#20302;&#31471;&#30828;&#20214;&#19978;&#20351;&#29992;&#22266;&#23450;&#35821;&#35328;&#27169;&#22411;&#26469;&#35757;&#32451;&#25991;&#26412;&#20998;&#31867;&#32593;&#32476;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#21457;&#29616;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#19981;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#22312;&#26356;&#24555;&#30340;&#35757;&#32451;&#20013;&#20135;&#29983;&#31454;&#20105;&#24615;&#30340;&#25928;&#26524;&#65292;&#20165;&#38656;&#35201;&#21407;&#20808;&#20869;&#23384;&#30340;&#22235;&#20998;&#20043;&#19968;&#21363;&#21487;&#12290;</title><link>http://arxiv.org/abs/2305.02350</link><description>&lt;p&gt;
&#22312;&#20302;&#31471;&#30828;&#20214;&#19978;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Using Language Models on Low-end Hardware. (arXiv:2305.02350v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02350
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#22312;&#20302;&#31471;&#30828;&#20214;&#19978;&#20351;&#29992;&#22266;&#23450;&#35821;&#35328;&#27169;&#22411;&#26469;&#35757;&#32451;&#25991;&#26412;&#20998;&#31867;&#32593;&#32476;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#21457;&#29616;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#19981;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#22312;&#26356;&#24555;&#30340;&#35757;&#32451;&#20013;&#20135;&#29983;&#31454;&#20105;&#24615;&#30340;&#25928;&#26524;&#65292;&#20165;&#38656;&#35201;&#21407;&#20808;&#20869;&#23384;&#30340;&#22235;&#20998;&#20043;&#19968;&#21363;&#21487;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22312;&#20302;&#31471;&#30828;&#20214;&#19978;&#20351;&#29992;&#22266;&#23450;&#35821;&#35328;&#27169;&#22411;&#26469;&#35757;&#32451;&#25991;&#26412;&#20998;&#31867;&#32593;&#32476;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;CNN&#26550;&#26500;&#30456;&#32467;&#21512;&#65292;&#24182;&#32452;&#25104;&#20102;&#21253;&#25324;&#21333;&#26631;&#31614;&#21644;&#22810;&#26631;&#31614;&#20998;&#31867;&#30340;&#35805;&#39064;&#12289;&#24773;&#24863;&#21644;&#39118;&#26684;&#30340;8&#32452;&#25968;&#25454;&#38598;&#30340;&#32508;&#21512;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#35266;&#23519;&#24635;&#32467;&#25104;&#19968;&#20010;&#26435;&#34913;&#21015;&#34920;&#65292;&#24182;&#24471;&#20986;&#32467;&#35770;&#65292;&#21363;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#19981;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#22312;&#26356;&#24555;&#30340;&#35757;&#32451;&#20013;&#20135;&#29983;&#31454;&#20105;&#24615;&#30340;&#25928;&#26524;&#65292;&#20165;&#38656;&#35201;&#21407;&#20808;&#20869;&#23384;&#30340;&#22235;&#20998;&#20043;&#19968;&#21363;&#21487;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper evaluates the viability of using fixed language models for training text classification networks on low-end hardware. We combine language models with a CNN architecture and put together a comprehensive benchmark with 8 datasets covering single-label and multi-label classification of topic, sentiment, and genre. Our observations are distilled into a list of trade-offs, concluding that there are scenarios, where not fine-tuning a language model yields competitive effectiveness at faster training, requiring only a quarter of the memory compared to fine-tuning.
&lt;/p&gt;</description></item><item><title>&#20026;&#20102;&#39564;&#35777;&#25991;&#26412;&#20998;&#32452;&#30340;&#20551;&#35774;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#35745;&#25991;&#26412;&#25506;&#32034;&#30340;&#27969;&#31243;&#65292;&#24182;&#22312;&#22307;&#32463;&#30340;&#21069;&#20004;&#21367;&#20070;&#20013;&#24212;&#29992;&#27492;&#27969;&#31243;&#65292;&#25104;&#21151;&#22320;&#35782;&#21035;&#24182;&#25506;&#32034;&#20102;&#21496;&#31085;&#27966;&#21035;&#21644;&#38750;&#21496;&#31085;&#27966;&#21035;&#20043;&#38388;&#30340;&#32479;&#35745;&#26126;&#26174;&#30340;&#25991;&#20307;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.02170</link><description>&lt;p&gt;
&#19968;&#31181;&#25991;&#26412;&#20998;&#32452;&#30340;&#32479;&#35745;&#25506;&#32034;&#65306;&#12298;&#21019;&#19990;&#35760;&#12299;&#21644;&#12298;&#20986;&#22467;&#21450;&#35760;&#12299;&#20013;&#21496;&#31085;&#27966;&#21035;&#30340;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
A Statistical Exploration of Text Partition Into Constituents: The Case of the Priestly Source in the Books of Genesis and Exodus. (arXiv:2305.02170v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02170
&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#39564;&#35777;&#25991;&#26412;&#20998;&#32452;&#30340;&#20551;&#35774;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#35745;&#25991;&#26412;&#25506;&#32034;&#30340;&#27969;&#31243;&#65292;&#24182;&#22312;&#22307;&#32463;&#30340;&#21069;&#20004;&#21367;&#20070;&#20013;&#24212;&#29992;&#27492;&#27969;&#31243;&#65292;&#25104;&#21151;&#22320;&#35782;&#21035;&#24182;&#25506;&#32034;&#20102;&#21496;&#31085;&#27966;&#21035;&#21644;&#38750;&#21496;&#31085;&#27966;&#21035;&#20043;&#38388;&#30340;&#32479;&#35745;&#26126;&#26174;&#30340;&#25991;&#20307;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#35745;&#25991;&#26412;&#25506;&#32034;&#30340;&#27969;&#31243;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#20307;&#23398;&#30340;&#35299;&#37322;&#65292;&#24182;&#23545;&#25991;&#26412;&#30340;&#20551;&#35774;&#20998;&#32452;&#36827;&#34892;&#20102;&#32479;&#35745;&#39564;&#35777;&#12290;&#32473;&#23450;&#25991;&#26412;&#30340;&#21442;&#25968;&#21270;&#65292;&#25105;&#20204;&#30340;&#27969;&#31243;&#65306;&#65288;1&#65289;&#26816;&#27979;&#25991;&#23398;&#29305;&#24449;&#65292;&#20197;&#20135;&#29983;&#20551;&#35774;&#20998;&#32452;&#21644;&#26080;&#30417;&#30563;&#20998;&#32452;&#20043;&#38388;&#30340;&#26368;&#20339;&#37325;&#21472;&#65292;&#65288;2&#65289;&#25191;&#34892;&#20551;&#35774;&#26816;&#39564;&#20998;&#26512;&#65292;&#37327;&#21270;&#26368;&#20339;&#37325;&#21472;&#30340;&#32479;&#35745;&#26174;&#33879;&#24615;&#65292;&#21516;&#26102;&#20445;&#30041;&#26356;&#21487;&#33021;&#34987;&#20998;&#32452;&#30340;&#25991;&#26412;&#21333;&#20301;&#20043;&#38388;&#30340;&#38544;&#24335;&#30456;&#20851;&#24615;&#65292;&#20197;&#21450;&#65288;3&#65289;&#25552;&#21462;&#21644;&#37327;&#21270;&#23545;&#20998;&#31867;&#26368;&#36127;&#36131;&#30340;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#20272;&#35745;&#23427;&#20204;&#30340;&#32479;&#35745;&#31283;&#23450;&#24615;&#21644;&#32858;&#31867;-wise&#20016;&#24230;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#27969;&#31243;&#24212;&#29992;&#20110;&#22307;&#32463;&#20013;&#30340;&#21069;&#20004;&#21367;&#20070;&#65292;&#22307;&#32463;&#23398;&#32773;&#20204;&#35748;&#20026;&#65292;&#20854;&#20013;&#19968;&#31181;&#25991;&#20307;&#25104;&#20998;&#29305;&#21035;&#31361;&#20986;&#65292;&#21363;&#21496;&#31085;&#27966;&#21035;&#12290;&#25105;&#20204;&#30830;&#23450;&#24182;&#25506;&#32034;&#20102;&#21496;&#31085;&#27966;&#21035;&#21644;&#38750;&#21496;&#31085;&#27966;&#21035;&#20043;&#38388;&#30340;&#32479;&#35745;&#26126;&#26174;&#30340;&#25991;&#20307;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a pipeline for a statistical textual exploration, offering a stylometry-based explanation and statistical validation of a hypothesized partition of a text. Given a parameterization of the text, our pipeline: (1) detects literary features yielding the optimal overlap between the hypothesized and unsupervised partitions, (2) performs a hypothesis-testing analysis to quantify the statistical significance of the optimal overlap, while conserving implicit correlations between units of text that are more likely to be grouped, and (3) extracts and quantifies the importance of features most responsible for the classification, estimates their statistical stability and cluster-wise abundance.  We apply our pipeline to the first two books in the Bible, where one stylistic component stands out in the eyes of biblical scholars, namely, the Priestly component. We identify and explore statistically significant stylistic differences between the Priestly and non-Priestly components.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; Doc2SoarGraph &#26694;&#26550;&#65292;&#21033;&#29992;&#35821;&#20041;&#23548;&#21521;&#20998;&#23618;&#22270;&#32467;&#26500;&#20013;&#20803;&#32032;&#20043;&#38388;&#30340;&#24046;&#24322;&#21644;&#30456;&#20851;&#24615;&#65292;&#22312;&#23500;&#21547;&#35270;&#35273;&#34920;&#26684;&#25991;&#26412;&#30340;TAT-DQA&#38382;&#39064;&#19979;&#23454;&#29616;&#20102;&#31163;&#25955;&#25512;&#29702;&#65292;&#34920;&#29616;&#20986;&#20102;&#26368;&#20339;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.01938</link><description>&lt;p&gt;
Doc2SoarGraph&#65306;&#22522;&#20110;&#35821;&#20041;&#23548;&#21521;&#20998;&#23618;&#22270;&#30340;&#23500;&#21547;&#35270;&#35273;&#34920;&#26684;&#25991;&#26723;&#30340;&#31163;&#25955;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Doc2SoarGraph: Discrete Reasoning over Visually-Rich Table-Text Documents with Semantic-Oriented Hierarchical Graphs. (arXiv:2305.01938v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; Doc2SoarGraph &#26694;&#26550;&#65292;&#21033;&#29992;&#35821;&#20041;&#23548;&#21521;&#20998;&#23618;&#22270;&#32467;&#26500;&#20013;&#20803;&#32032;&#20043;&#38388;&#30340;&#24046;&#24322;&#21644;&#30456;&#20851;&#24615;&#65292;&#22312;&#23500;&#21547;&#35270;&#35273;&#34920;&#26684;&#25991;&#26412;&#30340;TAT-DQA&#38382;&#39064;&#19979;&#23454;&#29616;&#20102;&#31163;&#25955;&#25512;&#29702;&#65292;&#34920;&#29616;&#20986;&#20102;&#26368;&#20339;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#20004;&#24180;&#26469;&#65292;&#23545;&#20110;&#34920;&#26684;&#25991;&#26412;&#25991;&#26723;&#65288;&#20363;&#22914;&#36130;&#21153;&#25253;&#21578;&#65289;&#30340;&#31163;&#25955;&#25512;&#29702;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#22823;&#22810;&#36890;&#36807;&#25163;&#21160;&#36873;&#25321;&#21644;&#36716;&#25442;&#25991;&#26723;&#39029;&#38754;&#21040;&#32467;&#26500;&#21270;&#30340;&#34920;&#26684;&#21644;&#27573;&#33853;&#26469;&#31616;&#21270;&#36825;&#19968;&#25361;&#25112;&#65292;&#20174;&#32780;&#38459;&#30861;&#20854;&#23454;&#38469;&#24212;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#31350;&#20102;&#19968;&#31181;&#26356;&#20026;&#29616;&#23454;&#30340;&#38382;&#39064;&#35774;&#32622;&#65292;&#21363;&#20197; TAT-DQA &#30340;&#24418;&#24335;&#22238;&#31572;&#23500;&#21547;&#35270;&#35273;&#34920;&#26684;&#25991;&#26412;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340; Doc2SoarGraph &#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#35821;&#20041;&#23548;&#21521;&#20998;&#23618;&#22270;&#32467;&#26500;&#20013;&#19981;&#21516;&#20803;&#32032;&#20043;&#38388;&#30340;&#24046;&#24322;&#21644;&#30456;&#20851;&#24615;&#65292;&#25552;&#39640;&#20102;&#20854;&#31163;&#25955;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#23545; TAT-DQA &#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#27979;&#35797;&#38598;&#19978;&#30340;&#31934;&#30830;&#21305;&#37197;&#65288;EM&#65289;&#21644; F1 &#24471;&#20998;&#26041;&#38754;&#20998;&#21035;&#27604;&#26368;&#20339;&#22522;&#32447;&#27169;&#22411;&#20998;&#21035;&#25552;&#39640;&#20102; 17.73% &#21644; 16.91%&#65292;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discrete reasoning over table-text documents (e.g., financial reports) gains increasing attention in recent two years. Existing works mostly simplify this challenge by manually selecting and transforming document pages to structured tables and paragraphs, hindering their practical application. In this work, we explore a more realistic problem setting in the form of TAT-DQA, i.e. to answer the question over a visually-rich table-text document. Specifically, we propose a novel Doc2SoarGraph framework with enhanced discrete reasoning capability by harnessing the differences and correlations among different elements (e.g., quantities, dates) of the given question and document with Semantic-oriented hierarchical Graph structures. We conduct extensive experiments on TAT-DQA dataset, and the results show that our proposed framework outperforms the best baseline model by 17.73% and 16.91% in terms of Exact Match (EM) and F1 score respectively on the test set, achieving the new state-of-the-art
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21453;&#39304;&#25913;&#36827;&#21477;&#23376;&#23884;&#20837;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#25552;&#39640;&#23545;&#27604;&#23398;&#20064;&#26679;&#26412;&#23545;&#30340;&#36136;&#37327;&#65292;&#24182;&#32467;&#21512;&#20154;&#31867;&#21453;&#39304;&#26469;&#25552;&#20379;&#26356;&#22909;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;</title><link>http://arxiv.org/abs/2305.01918</link><description>&lt;p&gt;
&#25913;&#36827;&#21477;&#23376;&#23884;&#20837;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Improving Contrastive Learning of Sentence Embeddings from AI Feedback. (arXiv:2305.01918v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21453;&#39304;&#25913;&#36827;&#21477;&#23376;&#23884;&#20837;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#25552;&#39640;&#23545;&#27604;&#23398;&#20064;&#26679;&#26412;&#23545;&#30340;&#36136;&#37327;&#65292;&#24182;&#32467;&#21512;&#20154;&#31867;&#21453;&#39304;&#26469;&#25552;&#20379;&#26356;&#22909;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#21477;&#23376;&#23884;&#20837;&#23398;&#20064;&#20013;&#30340;&#27969;&#34892;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#33258;&#28982;&#35821;&#35328;&#30340;&#31163;&#25955;&#24615;&#20351;&#24471;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#29983;&#25104;&#30340;&#27491;&#36127;&#26679;&#26412;&#23545;&#30340;&#36136;&#37327;&#38590;&#20197;&#20445;&#35777;&#12290;&#34429;&#28982;&#26377;&#30417;&#30563;&#30340;&#23545;&#27604;&#23398;&#20064;&#21487;&#20197;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#26631;&#31614;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#26679;&#26412;&#23545;&#65292;&#20294;&#20173;&#32570;&#20047;&#32454;&#31890;&#24230;&#30340;&#35757;&#32451;&#20449;&#21495;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;AI&#21453;&#39304;&#26469;&#25913;&#36827;&#21477;&#23376;&#23884;&#20837;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#65288;CLAIF&#65289;&#65292;&#21033;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411; (LLMs) &#30340;AI&#21453;&#39304;&#26500;&#24314;&#24102;&#26377;&#32454;&#31890;&#24230;&#26679;&#26412;&#30456;&#20284;&#24230;&#20998;&#25968;&#30340;&#26679;&#26412;&#23545;&#65292;&#20197;&#25913;&#36827;&#23545;&#27604;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32467;&#21512;&#20154;&#24037;&#21453;&#39304;&#21644;AI&#21453;&#39304;&#20026;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#21477;&#23376;&#23884;&#20837;&#25552;&#20379;&#26356;&#22909;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning has become a popular approach in natural language processing, particularly for the learning of sentence embeddings. However, the discrete nature of natural language makes it difficult to ensure the quality of positive and negative sample pairs generated through data augmentation methods. Although supervised contrastive learning can produce more accurate sample pairs with human feedback labels, it still lacks fine-grained training signals. In this paper, we propose to improve \textbf{C}ontrastive \textbf{L}earning of sentence embeddings from \textbf{AI} \textbf{F}eedback \textbf{(CLAIF)}. Our method utilizes AI feedback from large pre-trained language models (LLMs) to construct sample pairs with fine-grained sample similarity scores to improve contrastive learning. Besides, we combine human feedback and AI feedback to provide better supervision signals for supervised contrastive learning of sentence embeddings. Experimental results show that our method achieves stat
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#24863;&#30693;&#30340;&#30693;&#35782;&#24341;&#23548;&#25552;&#31034;&#26041;&#27861;&#65292;&#23558;&#20854;&#20316;&#20026;&#24178;&#39044;&#22120;&#35013;&#22791;&#21040;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21477;&#23376;&#25552;&#21462;&#22120;&#20013;&#65292;&#20197;&#32531;&#35299;&#27010;&#24565;&#20559;&#24046;&#12290;&#22312;&#20195;&#34920;&#24615;&#30340;&#22810;&#35821;&#35328;KG&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.01876</link><description>&lt;p&gt;
&#22522;&#20110;&#22240;&#26524;&#24863;&#30693;&#30340;&#30693;&#35782;&#24341;&#23548;&#21477;&#23376;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Causality-aware Concept Extraction based on Knowledge-guided Prompting. (arXiv:2305.01876v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01876
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#24863;&#30693;&#30340;&#30693;&#35782;&#24341;&#23548;&#25552;&#31034;&#26041;&#27861;&#65292;&#23558;&#20854;&#20316;&#20026;&#24178;&#39044;&#22120;&#35013;&#22791;&#21040;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21477;&#23376;&#25552;&#21462;&#22120;&#20013;&#65292;&#20197;&#32531;&#35299;&#27010;&#24565;&#20559;&#24046;&#12290;&#22312;&#20195;&#34920;&#24615;&#30340;&#22810;&#35821;&#35328;KG&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#26377;&#21161;&#20110;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65292;&#20294;&#29616;&#26377;&#30340;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#20013;&#36828;&#26410;&#23436;&#21892;&#12290;&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#24050;&#34987;&#24191;&#27867;&#29992;&#20110;&#22522;&#20110;&#25991;&#26412;&#30340;&#27010;&#24565;&#25552;&#21462;&#65288;CE&#65289;&#12290;&#28982;&#32780;&#65292;PLM&#24448;&#24448;&#20174;&#22823;&#37327;&#35821;&#26009;&#24211;&#30340;&#20849;&#29616;&#20851;&#32852;&#20013;&#36827;&#34892;&#39044;&#35757;&#32451;&#30693;&#35782;&#25366;&#25496;&#65292;&#32780;&#38750;Token&#20043;&#38388;&#30340;&#30495;&#23454;&#22240;&#26524;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#39044;&#35757;&#32451;&#30693;&#35782;&#28151;&#28102;&#20102;PLM&#65292;&#23548;&#33268;&#25552;&#21462;&#22522;&#20110;&#34394;&#20551;&#20849;&#29616;&#30456;&#20851;&#24615;&#30340;&#26377;&#20559;&#27010;&#24565;&#65292;&#19981;&#21487;&#36991;&#20813;&#22320;&#23548;&#33268;&#20302;&#31934;&#24230;&#12290;&#26412;&#25991;&#36890;&#36807;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#24341;&#23548;&#25552;&#31034;&#26041;&#27861;&#65292;&#23558;&#20854;&#20316;&#20026;&#24178;&#39044;&#22120;&#35013;&#22791;&#21040;&#22522;&#20110;PLM&#30340;&#25552;&#21462;&#22120;&#20013;&#65292;&#20197;&#20943;&#36731;&#27010;&#24565;&#20559;&#24046;&#12290;&#25552;&#31034;&#37319;&#29992;&#29616;&#26377;KG&#20013;&#30340;&#32473;&#23450;&#23454;&#20307;&#20027;&#39064;&#26469;&#32531;&#35299;&#23454;&#20307;&#21644;&#26377;&#20559;&#27010;&#24565;&#20043;&#38388;&#30340;&#34394;&#20551;&#20849;&#29616;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#22312;&#20195;&#34920;&#24615;&#30340;&#22810;&#35821;&#35328;KG&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#25552;&#31034;&#26174;&#33879;&#25913;&#36827;&#20102;&#25552;&#21462;&#24615;&#33021;&#65292;&#24182;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concepts benefit natural language understanding but are far from complete in existing knowledge graphs (KGs). Recently, pre-trained language models (PLMs) have been widely used in text-based concept extraction (CE). However, PLMs tend to mine the co-occurrence associations from massive corpus as pre-trained knowledge rather than the real causal effect between tokens.As a result, the pre-trained knowledge confounds PLMs to extract biased concepts based on spurious co-occurrence correlations, inevitably resulting in low precision. In this paper, through the lens of a Structural Causal Model (SCM), we propose equipping the PLM-based extractor with a knowledge-guided prompt as an intervention to alleviate concept bias. The prompt adopts the topic of the given entity from the existing knowledge in KGs to mitigate the spurious co-occurrence correlations between entities and biased concepts. Our extensive experiments on representative multilingual KG datasets justify that our proposed prompt 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#31070;&#32463;&#25490;&#24207;&#27169;&#22411;&#30340;&#19981;&#26131;&#34987;&#26816;&#27979;&#21040;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#26694;&#26550;&#65292;&#31216;&#20026;&#8220;&#20960;&#20046;&#19981;&#21487;&#23519;&#35273;&#25991;&#26723;&#25805;&#20316;&#8221;&#65288;IDEM&#65289;&#12290;IDEM&#20351;&#29992;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36830;&#32467;&#21477;&#65292;&#26080;&#27861;&#24341;&#20837;&#26131;&#20110;&#26816;&#27979;&#30340;&#38169;&#35823;&#65292;&#24182;&#19988;&#20351;&#29992;&#21333;&#29420;&#30340;&#20301;&#32622;&#21512;&#24182;&#31574;&#30053;&#26469;&#24179;&#34913;&#25200;&#21160;&#25991;&#26412;&#30340;&#30456;&#20851;&#24615;&#21644;&#36830;&#36143;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;IDEM&#21487;&#20197;&#22312;&#20445;&#25345;&#39640;&#20154;&#31867;&#35780;&#20272;&#24471;&#20998;&#30340;&#21516;&#26102;&#20248;&#20110;&#24378;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2305.01860</link><description>&lt;p&gt;
&#38024;&#23545;&#31070;&#32463;&#25490;&#24207;&#27169;&#22411;&#30340;&#20960;&#20046;&#19981;&#21487;&#23519;&#35273;&#30340;&#25991;&#26723;&#31713;&#25913;
&lt;/p&gt;
&lt;p&gt;
Towards Imperceptible Document Manipulations against Neural Ranking Models. (arXiv:2305.01860v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01860
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#31070;&#32463;&#25490;&#24207;&#27169;&#22411;&#30340;&#19981;&#26131;&#34987;&#26816;&#27979;&#21040;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#26694;&#26550;&#65292;&#31216;&#20026;&#8220;&#20960;&#20046;&#19981;&#21487;&#23519;&#35273;&#25991;&#26723;&#25805;&#20316;&#8221;&#65288;IDEM&#65289;&#12290;IDEM&#20351;&#29992;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36830;&#32467;&#21477;&#65292;&#26080;&#27861;&#24341;&#20837;&#26131;&#20110;&#26816;&#27979;&#30340;&#38169;&#35823;&#65292;&#24182;&#19988;&#20351;&#29992;&#21333;&#29420;&#30340;&#20301;&#32622;&#21512;&#24182;&#31574;&#30053;&#26469;&#24179;&#34913;&#25200;&#21160;&#25991;&#26412;&#30340;&#30456;&#20851;&#24615;&#21644;&#36830;&#36143;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;IDEM&#21487;&#20197;&#22312;&#20445;&#25345;&#39640;&#20154;&#31867;&#35780;&#20272;&#24471;&#20998;&#30340;&#21516;&#26102;&#20248;&#20110;&#24378;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#25915;&#20987;&#24050;&#32463;&#24320;&#22987;&#24212;&#29992;&#20110;&#21457;&#29616;&#31070;&#32463;&#25490;&#24207;&#27169;&#22411;&#65288;NRMs&#65289;&#20013;&#30340;&#28508;&#22312;&#28431;&#27934;&#65292;&#20294;&#26159;&#24403;&#21069;&#25915;&#20987;&#26041;&#27861;&#24120;&#24120;&#20250;&#24341;&#20837;&#35821;&#27861;&#38169;&#35823;&#65292;&#26080;&#24847;&#20041;&#30340;&#34920;&#36798;&#65292;&#25110;&#19981;&#36830;&#36143;&#30340;&#25991;&#26412;&#29255;&#27573;&#65292;&#36825;&#20123;&#37117;&#24456;&#23481;&#26131;&#34987;&#26816;&#27979;&#21040;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#20351;&#29992;&#19982;&#30495;&#23454;&#30340;NRM&#30456;&#20284;&#30340;&#27169;&#25311;NRM&#26469;&#20445;&#35777;&#25915;&#20987;&#25928;&#26524;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#22312;&#23454;&#36341;&#20013;&#38590;&#20197;&#20351;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;&#8220;&#20960;&#20046;&#19981;&#21487;&#23519;&#35273;&#25991;&#26723;&#25805;&#20316;&#8221;&#65288;IDEM&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#23545;&#31639;&#27861;&#21644;&#20154;&#31867;&#26469;&#35828;&#37117;&#19981;&#22826;&#26126;&#26174;&#30340;&#23545;&#25239;&#25991;&#26723;&#12290;IDEM&#25351;&#31034;&#19968;&#20010;&#32463;&#36807;&#33391;&#22909;&#24314;&#31435;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;BART&#65289;&#29983;&#25104;&#36830;&#25509;&#21477;&#65292;&#32780;&#19981;&#20250;&#24341;&#20837;&#26131;&#20110;&#26816;&#27979;&#30340;&#38169;&#35823;&#65292;&#24182;&#37319;&#29992;&#21333;&#29420;&#30340;&#36880;&#20301;&#32622;&#21512;&#24182;&#31574;&#30053;&#26469;&#24179;&#34913;&#25200;&#21160;&#25991;&#26412;&#30340;&#30456;&#20851;&#24615;&#21644;&#36830;&#36143;&#24615;&#12290;&#22312;&#27969;&#34892;&#30340;MS MARCO&#22522;&#20934;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;IDEM&#21487;&#20197;&#22312;&#20445;&#25345;&#39640;&#20154;&#31867;&#35780;&#20272;&#24471;&#20998;&#30340;&#21516;&#26102;&#65292;&#20248;&#20110;&#24378;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks have gained traction in order to identify potential vulnerabilities in neural ranking models (NRMs), but current attack methods often introduce grammatical errors, nonsensical expressions, or incoherent text fragments, which can be easily detected. Additionally, current methods rely heavily on the use of a well-imitated surrogate NRM to guarantee the attack effect, which makes them difficult to use in practice. To address these issues, we propose a framework called Imperceptible DocumEnt Manipulation (IDEM) to produce adversarial documents that are less noticeable to both algorithms and humans. IDEM instructs a well-established generative language model, such as BART, to generate connection sentences without introducing easy-to-detect errors, and employs a separate position-wise merging strategy to balance relevance and coherence of the perturbed text. Experimental results on the popular MS MARCO benchmark demonstrate that IDEM can outperform strong baselines while 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;KB-BINDER&#26694;&#26550;&#65292;&#36890;&#36807;&#23569;&#37327;&#30340;&#19978;&#19979;&#25991;&#28436;&#31034;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#30693;&#35782;&#24211;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#30340;&#32972;&#26223;&#23398;&#20064;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;KBQA&#38382;&#39064;&#30340;&#21487;&#35299;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.01750</link><description>&lt;p&gt;
&#22522;&#20110;&#23569;&#26679;&#26412;&#32972;&#26223;&#23398;&#20064;&#30340;&#30693;&#35782;&#24211;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Few-shot In-context Learning for Knowledge Base Question Answering. (arXiv:2305.01750v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01750
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;KB-BINDER&#26694;&#26550;&#65292;&#36890;&#36807;&#23569;&#37327;&#30340;&#19978;&#19979;&#25991;&#28436;&#31034;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#30693;&#35782;&#24211;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#30340;&#32972;&#26223;&#23398;&#20064;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;KBQA&#38382;&#39064;&#30340;&#21487;&#35299;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#24211;&#38382;&#31572;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#38590;&#20197;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#38656;&#35201;&#24212;&#23545;&#21508;&#31181;&#21487;&#33021;&#30340;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#19981;&#21516;&#30693;&#35782;&#24211;&#26550;&#26500;&#39033;&#20043;&#38388;&#30340;&#24322;&#26500;&#24615;&#36890;&#24120;&#38656;&#35201;&#38024;&#23545;&#19981;&#21516;&#30340;&#30693;&#35782;&#24211;&#38382;&#31572;&#65288;KBQA&#65289;&#25968;&#25454;&#38598;&#36827;&#34892;&#19987;&#38376;&#30340;&#35757;&#32451;&#12290;&#20026;&#20102;&#22788;&#29702;&#22810;&#31181;KBQA&#25968;&#25454;&#38598;&#19978;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;KB-BINDER&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#36827;&#34892;&#23569;&#37327;&#26679;&#26412;&#30340;&#32972;&#26223;&#23398;&#20064;&#65292;&#24182;&#23558;&#19981;&#21516;&#30340;KBQA&#25968;&#25454;&#38598;&#32479;&#19968;&#12290;&#39318;&#20808;&#65292;KB-BINDER&#21033;&#29992;&#20687;Codex&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#27169;&#20223;&#23569;&#37327;&#28436;&#31034;&#26469;&#29983;&#25104;&#29305;&#23450;&#38382;&#39064;&#30340;&#36923;&#36753;&#24418;&#24335;&#20316;&#20026;&#33609;&#31295;&#12290;&#20854;&#27425;&#65292;KB-BINDER&#22522;&#20110;&#30693;&#35782;&#24211;&#26469;&#32465;&#23450;&#29983;&#25104;&#30340;&#33609;&#31295;&#33267;&#21487;&#25191;&#34892;&#24418;&#24335;&#65292;&#36890;&#36807;BM25&#20998;&#25968;&#21305;&#37197;&#12290;&#22312;&#22235;&#20010;&#20844;&#24320;&#30340;&#24322;&#26500;KBQA&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;KB-BINDER&#21487;&#20197;&#22312;&#23569;&#37327;&#19978;&#19979;&#25991;&#28436;&#31034;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Question answering over knowledge bases is considered a difficult problem due to the challenge of generalizing to a wide variety of possible natural language questions. Additionally, the heterogeneity of knowledge base schema items between different knowledge bases often necessitates specialized training for different knowledge base question-answering (KBQA) datasets. To handle questions over diverse KBQA datasets with a unified training-free framework, we propose KB-BINDER, which for the first time enables few-shot in-context learning over KBQA tasks. Firstly, KB-BINDER leverages large language models like Codex to generate logical forms as the draft for a specific question by imitating a few demonstrations. Secondly, KB-BINDER grounds on the knowledge base to bind the generated draft to an executable one with BM25 score matching. The experimental results on four public heterogeneous KBQA datasets show that KB-BINDER can achieve a strong performance with only a few in-context demonstr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;SHAP&#26694;&#26550;&#21644;&#35270;&#35273;&#20808;&#39564;&#30693;&#35782;&#29983;&#25104;&#20840;&#38754;&#12289;&#26377;&#24847;&#20041;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#12289;&#26356;&#39640;&#30340;&#35299;&#37322;&#34920;&#29616;&#21147;&#65292;&#24182;&#21487;&#20197;&#25512;&#24191;&#21040;&#20854;&#20182;&#27169;&#22411;&#19978;&#12290;</title><link>http://arxiv.org/abs/2304.14986</link><description>&lt;p&gt;
&#21033;&#29992;&#35821;&#20041;&#35270;&#35273;&#20808;&#39564;&#35299;&#37322;&#35270;&#35273;&#21644;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Interpreting Vision and Language Generative Models with Semantic Visual Priors. (arXiv:2304.14986v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;SHAP&#26694;&#26550;&#21644;&#35270;&#35273;&#20808;&#39564;&#30693;&#35782;&#29983;&#25104;&#20840;&#38754;&#12289;&#26377;&#24847;&#20041;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#12289;&#26356;&#39640;&#30340;&#35299;&#37322;&#34920;&#29616;&#21147;&#65292;&#24182;&#21487;&#20197;&#25512;&#24191;&#21040;&#20854;&#20182;&#27169;&#22411;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24212;&#29992;&#20110;&#22270;&#20687;&#21040;&#25991;&#26412;&#27169;&#22411;&#26102;&#65292;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#36890;&#24120;&#25552;&#20379;&#36880;&#20010;&#26631;&#35760;&#30340;&#35299;&#37322;&#65292;&#21363;&#20026;&#25152;&#29983;&#25104;&#30340;&#24207;&#21015;&#20013;&#30340;&#27599;&#20010;&#26631;&#35760;&#35745;&#31639;&#35270;&#35273;&#35299;&#37322;&#12290;&#36825;&#20123;&#35299;&#37322;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#26080;&#27861;&#20840;&#38754;&#35299;&#37322;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#26576;&#31181;&#36817;&#20284;&#26041;&#27861;&#65292;&#26368;&#32456;&#20250;&#23548;&#33268;&#35823;&#23548;&#24615;&#30340;&#35299;&#37322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;SHAP&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20801;&#35768;&#21033;&#29992;&#36755;&#20986;&#24207;&#21015;&#30340;&#21547;&#20041;&#34920;&#31034;&#29983;&#25104;&#20840;&#38754;&#12289;&#26377;&#24847;&#20041;&#30340;&#35299;&#37322;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#21033;&#29992;&#35270;&#35273;&#20027;&#24178;&#32593;&#32476;&#20013;&#30340;&#35821;&#20041;&#20808;&#39564;&#30693;&#35782;&#65292;&#25105;&#20204;&#25552;&#21462;&#20102;&#20219;&#24847;&#25968;&#37327;&#30340;&#29305;&#24449;&#65292;&#24182;&#33021;&#22815;&#22312;&#22823;&#35268;&#27169;&#27169;&#22411;&#19978;&#39640;&#25928;&#35745;&#31639;Shapley&#20540;&#65292;&#21516;&#26102;&#29983;&#25104;&#39640;&#24230;&#26126;&#30830;&#30340;&#35270;&#35273;&#35299;&#37322;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26356;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#29983;&#25104;&#35821;&#20041;&#19978;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#35299;&#37322;&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#20854;&#20182;&#27169;&#22411;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
When applied to Image-to-text models, interpretability methods often provide token-by-token explanations namely, they compute a visual explanation for each token of the generated sequence. Those explanations are expensive to compute and unable to comprehensively explain the model's output. Therefore, these models often require some sort of approximation that eventually leads to misleading explanations. We develop a framework based on SHAP, that allows for generating comprehensive, meaningful explanations leveraging the meaning representation of the output sequence as a whole. Moreover, by exploiting semantic priors in the visual backbone, we extract an arbitrary number of features that allows the efficient computation of Shapley values on large-scale models, generating at the same time highly meaningful visual explanations. We demonstrate that our method generates semantically more expressive explanations than traditional methods at a lower compute cost and that it can be generalized o
&lt;/p&gt;</description></item><item><title>DataComp&#26159;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;&#35757;&#32451;&#38598;&#26469;&#35299;&#20915;&#25968;&#25454;&#38598;&#22312;&#26426;&#22120;&#23398;&#20064;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#32570;&#38519;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#35268;&#27169;&#35774;&#35745;&#30340;&#23454;&#39564;&#27979;&#35797;&#24179;&#21488;&#65292;&#20351;&#29992;12.8B&#20010;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#26032;&#20505;&#36873;&#27744;&#65292;&#35753;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#36807;&#28388;&#25216;&#26415;&#25110;&#31574;&#21010;&#26032;&#30340;&#25968;&#25454;&#28304;&#24182;&#35780;&#20272;&#23427;&#20204;&#30340;&#26032;&#25968;&#25454;&#38598;&#26469;&#36827;&#34892;&#21019;&#26032;&#12290;</title><link>http://arxiv.org/abs/2304.14108</link><description>&lt;p&gt;
DataComp&#65306;&#23547;&#25214;&#19979;&#19968;&#20195;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DataComp: In search of the next generation of multimodal datasets. (arXiv:2304.14108v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14108
&lt;/p&gt;
&lt;p&gt;
DataComp&#26159;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;&#35757;&#32451;&#38598;&#26469;&#35299;&#20915;&#25968;&#25454;&#38598;&#22312;&#26426;&#22120;&#23398;&#20064;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#32570;&#38519;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#35268;&#27169;&#35774;&#35745;&#30340;&#23454;&#39564;&#27979;&#35797;&#24179;&#21488;&#65292;&#20351;&#29992;12.8B&#20010;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#26032;&#20505;&#36873;&#27744;&#65292;&#35753;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#36807;&#28388;&#25216;&#26415;&#25110;&#31574;&#21010;&#26032;&#30340;&#25968;&#25454;&#28304;&#24182;&#35780;&#20272;&#23427;&#20204;&#30340;&#26032;&#25968;&#25454;&#38598;&#26469;&#36827;&#34892;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#22312;&#36817;&#26399;&#30340;&#31361;&#30772;&#20013;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#65292;&#27604;&#22914;CLIP&#12289;Stable Diffusion&#21644;GPT-4&#31561;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#25968;&#25454;&#38598;&#24456;&#23569;&#24471;&#21040;&#19982;&#27169;&#22411;&#26550;&#26500;&#25110;&#35757;&#32451;&#31639;&#27861;&#21516;&#31561;&#30340;&#30740;&#31350;&#20851;&#27880;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#22312;&#26426;&#22120;&#23398;&#20064;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#32570;&#38519;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DataComp&#65292;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#35757;&#32451;&#20195;&#30721;&#26159;&#22266;&#23450;&#30340;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;&#35757;&#32451;&#38598;&#26469;&#36827;&#34892;&#21019;&#26032;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;Common Crawl&#30340;&#26032;&#20505;&#36873;&#27744;&#65292;&#20854;&#20013;&#21253;&#21547;12.8B&#20010;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#25968;&#25454;&#38598;&#23454;&#39564;&#27979;&#35797;&#24179;&#21488;&#12290;&#21442;&#21152;&#25105;&#20204;&#22522;&#20934;&#27979;&#35797;&#30340;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#35774;&#35745;&#26032;&#30340;&#36807;&#28388;&#25216;&#26415;&#25110;&#31574;&#21010;&#26032;&#30340;&#25968;&#25454;&#28304;&#65292;&#24182;&#36890;&#36807;&#36816;&#34892;&#25105;&#20204;&#26631;&#20934;&#21270;&#30340;CLIP&#35757;&#32451;&#20195;&#30721;&#24182;&#22312;38&#20010;&#19979;&#28216;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#27979;&#35797;&#26469;&#35780;&#20272;&#20182;&#20204;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#21253;&#21547;&#22810;&#20010;&#35268;&#27169;&#65292;&#22235;&#20010;&#20505;&#36873;&#27744;&#22823;&#23567;&#21644;&#30456;&#24212;&#30340;&#35745;&#31639;&#39044;&#31639;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#28085;&#30422;&#20102;&#20174;12.8M&#21040;12.8B&#20010;&#26679;&#26412;&#12290;&#36825;&#31181;&#22810;&#35268;&#27169;&#35774;&#35745;&#26377;&#21161;&#20110;&#30740;&#31350;&#35268;&#27169;&#36235;&#21183;&#65292;&#24182;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#26356;&#22810;&#30340;&#36873;&#25321;&#20313;&#22320;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large multimodal datasets have been instrumental in recent breakthroughs such as CLIP, Stable Diffusion, and GPT-4. At the same time, datasets rarely receive the same research attention as model architectures or training algorithms. To address this shortcoming in the machine learning ecosystem, we introduce DataComp, a benchmark where the training code is fixed and researchers innovate by proposing new training sets. We provide a testbed for dataset experiments centered around a new candidate pool of 12.8B image-text pairs from Common Crawl. Participants in our benchmark design new filtering techniques or curate new data sources and then evaluate their new dataset by running our standardized CLIP training code and testing on 38 downstream test sets. Our benchmark consists of multiple scales, with four candidate pool sizes and associated compute budgets ranging from 12.8M to 12.8B samples seen during training. This multi-scale design facilitates the study of scaling trends and makes the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#39064;&#23884;&#20837;&#26041;&#27861;&#21644;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#26080;&#30417;&#30563;&#22312;&#32447;&#25925;&#20107;&#21457;&#29616;&#26694;&#26550;USTORY&#65292;&#21487;&#20197;&#21160;&#24577;&#34920;&#31034;&#25991;&#31456;&#21644;&#25925;&#20107;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#20849;&#20139;&#30340;&#26102;&#38388;&#20027;&#39064;&#21644;&#26032;&#39062;&#24615;&#65292;&#20197;&#24110;&#21161;&#20154;&#20204;&#28040;&#21270;&#22823;&#37327;&#30340;&#26032;&#38395;&#27969;&#12290;</title><link>http://arxiv.org/abs/2304.04099</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#25193;&#23637;&#30340;&#20027;&#39064;&#23884;&#20837;&#20174;&#36830;&#32493;&#26032;&#38395;&#27969;&#20013;&#26080;&#30417;&#30563;&#22320;&#21457;&#29616;&#25925;&#20107;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Story Discovery from Continuous News Streams via Scalable Thematic Embedding. (arXiv:2304.04099v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#39064;&#23884;&#20837;&#26041;&#27861;&#21644;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#26080;&#30417;&#30563;&#22312;&#32447;&#25925;&#20107;&#21457;&#29616;&#26694;&#26550;USTORY&#65292;&#21487;&#20197;&#21160;&#24577;&#34920;&#31034;&#25991;&#31456;&#21644;&#25925;&#20107;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#20849;&#20139;&#30340;&#26102;&#38388;&#20027;&#39064;&#21644;&#26032;&#39062;&#24615;&#65292;&#20197;&#24110;&#21161;&#20154;&#20204;&#28040;&#21270;&#22823;&#37327;&#30340;&#26032;&#38395;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#22320;&#21457;&#29616;&#23454;&#26102;&#30456;&#20851;&#26032;&#38395;&#25991;&#31456;&#25925;&#20107;&#65292;&#26377;&#21161;&#20110;&#20154;&#20204;&#22312;&#19981;&#38656;&#35201;&#26114;&#36149;&#20154;&#24037;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#28040;&#21270;&#22823;&#37327;&#30340;&#26032;&#38395;&#27969;&#12290;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#22312;&#32447;&#25925;&#20107;&#21457;&#29616;&#30740;&#31350;&#30340;&#26222;&#36941;&#26041;&#27861;&#26159;&#29992;&#31526;&#21495;&#25110;&#22522;&#20110;&#22270;&#30340;&#23884;&#20837;&#26469;&#34920;&#31034;&#26032;&#38395;&#25991;&#31456;&#65292;&#24182;&#23558;&#23427;&#20204;&#36880;&#27493;&#32858;&#31867;&#25104;&#25925;&#20107;&#12290;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#26395;&#36827;&#19968;&#27493;&#25913;&#21892;&#23884;&#20837;&#65292;&#20294;&#26159;&#36890;&#36807;&#26080;&#24046;&#21035;&#22320;&#32534;&#30721;&#25991;&#31456;&#20013;&#30340;&#25152;&#26377;&#20449;&#24687;&#26469;&#30452;&#25509;&#37319;&#29992;&#36825;&#20123;&#27169;&#22411;&#26080;&#27861;&#26377;&#25928;&#22788;&#29702;&#23500;&#21547;&#25991;&#26412;&#19988;&#19981;&#26029;&#21457;&#23637;&#30340;&#26032;&#38395;&#27969;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#39064;&#23884;&#20837;&#26041;&#27861;&#65292;&#20351;&#29992;&#29616;&#25104;&#30340;&#39044;&#35757;&#32451;&#21477;&#23376;&#32534;&#30721;&#22120;&#26469;&#21160;&#24577;&#34920;&#31034;&#25991;&#31456;&#21644;&#25925;&#20107;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#20849;&#20139;&#30340;&#26102;&#38388;&#20027;&#39064;&#12290;&#20026;&#20102;&#23454;&#29616;&#26080;&#30417;&#30563;&#22312;&#32447;&#25925;&#20107;&#21457;&#29616;&#30340;&#24819;&#27861;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#26694;&#26550;USTORY&#65292;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#25216;&#26415;&#65292;&#21363;&#20027;&#39064;&#21644;&#26102;&#38388;&#24863;&#30693;&#30340;&#21160;&#24577;&#23884;&#20837;&#21644;&#26032;&#39062;&#24615;&#24863;&#30693;&#30340;&#33258;&#36866;&#24212;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised discovery of stories with correlated news articles in real-time helps people digest massive news streams without expensive human annotations. A common approach of the existing studies for unsupervised online story discovery is to represent news articles with symbolic- or graph-based embedding and incrementally cluster them into stories. Recent large language models are expected to improve the embedding further, but a straightforward adoption of the models by indiscriminately encoding all information in articles is ineffective to deal with text-rich and evolving news streams. In this work, we propose a novel thematic embedding with an off-the-shelf pretrained sentence encoder to dynamically represent articles and stories by considering their shared temporal themes. To realize the idea for unsupervised online story discovery, a scalable framework USTORY is introduced with two main techniques, theme- and time-aware dynamic embedding and novelty-aware adaptive clustering, fuel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#21382;&#31243;&#20197;&#21450;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#65292;&#24182;&#24378;&#35843;&#27169;&#22411;&#25193;&#23637;&#23558;&#24102;&#26469;&#24615;&#33021;&#25913;&#36827;&#21644;&#29305;&#27530;&#33021;&#21147;&#30340;&#21457;&#25496;&#12290;</title><link>http://arxiv.org/abs/2303.18223</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Large Language Models. (arXiv:2303.18223v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#21382;&#31243;&#20197;&#21450;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#65292;&#24182;&#24378;&#35843;&#27169;&#22411;&#25193;&#23637;&#23558;&#24102;&#26469;&#24615;&#33021;&#25913;&#36827;&#21644;&#29305;&#27530;&#33021;&#21147;&#30340;&#21457;&#25496;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#26412;&#36136;&#19978;&#26159;&#19968;&#20010;&#30001;&#35821;&#27861;&#35268;&#21017;&#25511;&#21046;&#30340;&#22797;&#26434;&#31934;&#32454;&#30340;&#20154;&#31867;&#34920;&#36798;&#31995;&#32479;&#65292;&#23545;&#20110;&#24320;&#21457;&#29702;&#35299;&#21644;&#25484;&#25569;&#35821;&#35328;&#30340;&#33021;&#21147;&#30340;AI&#31639;&#27861;&#26469;&#35828;&#26159;&#19968;&#39033;&#37325;&#22823;&#25361;&#25112;&#12290;&#20316;&#20026;&#20027;&#35201;&#26041;&#27861;&#20043;&#19968;&#65292;&#35821;&#35328;&#24314;&#27169;&#22312;&#36807;&#21435;&#20108;&#21313;&#24180;&#37324;&#24191;&#27867;&#30740;&#31350;&#29992;&#20110;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#65292;&#20174;&#32479;&#35745;&#35821;&#35328;&#27169;&#22411;&#28436;&#21270;&#20026;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#12290;&#26368;&#36817;&#65292;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;&#22312;&#35299;&#20915;&#21508;&#31181;NLP&#20219;&#21153;&#26041;&#38754;&#26174;&#31034;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#30001;&#20110;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#27169;&#22411;&#32553;&#25918;&#21487;&#20197;&#23548;&#33268;&#24615;&#33021;&#25913;&#36827;&#65292;&#20182;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#22686;&#21152;&#27169;&#22411;&#35268;&#27169;&#26469;&#30740;&#31350;&#32553;&#25918;&#25928;&#24212;&#65292;&#26377;&#36259;&#30340;&#26159;&#65292;&#24403;&#21442;&#25968;&#35268;&#27169;&#36229;&#36807;&#19968;&#23450;&#27700;&#24179;&#26102;&#65292;&#36825;&#20123;&#25193;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#19981;&#20165;&#21487;&#20197;&#23454;&#29616;&#26174;&#30528;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#32780;&#19988;&#36824;&#26174;&#31034;&#20986;&#19968;&#20123;&#23567;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25152;&#27809;&#26377;&#30340;&#29305;&#27530;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale langu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#36807;&#31243;&#20013;&#20551;&#23450;&#26377;&#36741;&#21161;&#25968;&#25454;&#30340;&#35757;&#32451;&#33539;&#24335;FLAD&#65292;&#24182;&#38024;&#23545;&#33258;&#21160;&#28151;&#21512;&#36741;&#21161;&#21644;&#30446;&#26631;&#25968;&#25454;&#30340;&#26041;&#27861;&#23616;&#38480;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#35745;&#31639;&#22797;&#26434;&#24230;&#29420;&#31435;&#20110;&#36741;&#21161;&#25968;&#25454;&#38598;&#25968;&#37327;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;FLAD&#21644;&#36825;&#20004;&#31181;&#31639;&#27861;&#30340;&#27604;&#36739;&#65292;&#21487;&#20197;&#21457;&#29616;&#36825;&#20004;&#31181;&#31639;&#27861;&#30340;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2302.00674</link><description>&lt;p&gt;
&#25506;&#32034;&#21644;&#21033;&#29992;&#36741;&#21161;&#25968;&#25454;&#26469;&#25913;&#21892;&#23567;&#26679;&#26412;&#27867;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Improving Few-Shot Generalization by Exploring and Exploiting Auxiliary Data. (arXiv:2302.00674v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#36807;&#31243;&#20013;&#20551;&#23450;&#26377;&#36741;&#21161;&#25968;&#25454;&#30340;&#35757;&#32451;&#33539;&#24335;FLAD&#65292;&#24182;&#38024;&#23545;&#33258;&#21160;&#28151;&#21512;&#36741;&#21161;&#21644;&#30446;&#26631;&#25968;&#25454;&#30340;&#26041;&#27861;&#23616;&#38480;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#35745;&#31639;&#22797;&#26434;&#24230;&#29420;&#31435;&#20110;&#36741;&#21161;&#25968;&#25454;&#38598;&#25968;&#37327;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;FLAD&#21644;&#36825;&#20004;&#31181;&#31639;&#27861;&#30340;&#27604;&#36739;&#65292;&#21487;&#20197;&#21457;&#29616;&#36825;&#20004;&#31181;&#31639;&#27861;&#30340;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23567;&#26679;&#26412;&#23398;&#20064;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#37117;&#26377;&#20215;&#20540;&#65292;&#20294;&#23398;&#20064;&#19968;&#20010;&#36890;&#29992;&#30340;&#27169;&#22411;&#19988;&#19981;&#36807;&#24230;&#25311;&#21512;&#23569;&#25968;&#26631;&#35760;&#25968;&#25454;&#28857;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#20851;&#27880;&#36741;&#21161;&#25968;&#25454;&#30340;&#23567;&#26679;&#26412;&#23398;&#20064;&#65288;FLAD&#65289;&#65292;&#19968;&#31181;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#36807;&#31243;&#20013;&#20551;&#23450;&#26377;&#36741;&#21161;&#25968;&#25454;&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#20197;&#26399;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#25552;&#20986;&#20102;&#33258;&#21160;&#28151;&#21512;&#36741;&#21161;&#21644;&#30446;&#26631;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38543;&#36741;&#21161;&#25968;&#25454;&#38598;&#30340;&#25968;&#37327;&#21576;&#32447;&#24615;&#65288;&#25110;&#26356;&#24046;&#65289;&#32553;&#25918;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#29992;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;FLAD&#19982;&#22312;&#22810;&#33218;&#32769;&#34382;&#26426;&#35774;&#32622;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#25506;&#32034;&#19982;&#21033;&#29992;&#22256;&#22659;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#25512;&#23548;&#20986;&#31639;&#27861;&#65292;&#20854;&#35745;&#31639;&#22797;&#26434;&#24230;&#29420;&#31435;&#20110;&#36741;&#21161;&#25968;&#25454;&#38598;&#30340;&#25968;&#37327;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#25193;&#23637;&#21040;&#27604;&#20808;&#21069;&#26041;&#27861;&#22810;100&#20493;&#30340;&#36741;&#21161;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#8212;&#8212;EXP3-FLAD&#21644;UCB1-FLAD&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#20808;&#21069;&#21482;&#36827;&#34892;&#25506;&#32034;&#25110;&#21033;&#29992;&#30340;FLAD&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#21457;&#29616;&#36825;&#20123;&#31639;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot learning is valuable in many real-world applications, but learning a generalizable model without overfitting to the few labeled datapoints is challenging. In this work, we focus on Few-shot Learning with Auxiliary Data (FLAD), a training paradigm that assumes access to auxiliary data during few-shot learning in hopes of improving generalization. Previous works have proposed automated methods for mixing auxiliary and target data, but these methods typically scale linearly (or worse) with the number of auxiliary datasets, limiting their practicality. In this work we relate FLAD to the explore-exploit dilemma that is central to the multi-armed bandit setting and derive algorithms whose computational complexity is independent of the number of auxiliary datasets, allowing us to scale to 100x more auxiliary datasets than prior methods. We propose two algorithms -- EXP3-FLAD and UCB1-FLAD -- and compare them with prior FLAD methods that either explore or exploit, finding that the com
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#34987;&#35270;&#20026;&#38544;&#24335;&#30340;&#20027;&#39064;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#20174;&#27880;&#37322;&#25968;&#25454;&#20013;&#36873;&#25321;&#26368;&#20339;&#31034;&#33539;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2301.11916</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#34987;&#35270;&#20026;&#38544;&#21547;&#30340;&#20027;&#39064;&#27169;&#22411;&#65306;&#35299;&#37322;&#21644;&#23547;&#25214;&#22909;&#30340;&#31034;&#33539;&#20197;&#23454;&#29616;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Are Implicitly Topic Models: Explaining and Finding Good Demonstrations for In-Context Learning. (arXiv:2301.11916v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#34987;&#35270;&#20026;&#38544;&#24335;&#30340;&#20027;&#39064;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#20174;&#27880;&#37322;&#25968;&#25454;&#20013;&#36873;&#25321;&#26368;&#20339;&#31034;&#33539;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#20102;&#22312;&#25512;&#29702;&#26102;&#23454;&#29616;&#23569;&#37327;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#30340;&#26174;&#33879;&#25928;&#29575;&#65292;&#34987;&#31216;&#20026;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290; &#28982;&#32780;&#65292;&#29616;&#26377;&#25991;&#29486;&#24378;&#35843;&#36825;&#31181;&#33021;&#21147;&#23545;&#23569;&#37327;&#26679;&#26412;&#31034;&#33539;&#30340;&#36873;&#25321;&#24456;&#25935;&#24863;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#36125;&#21494;&#26031;&#35270;&#35282;&#30740;&#31350;&#19978;&#19979;&#25991;&#23398;&#20064;&#29616;&#35937;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35270;&#20026;&#20174;&#31034;&#33539;&#20013;&#38544;&#21547;&#22320;&#25512;&#26029;&#20986;&#30456;&#20851;&#20449;&#24687;&#30340;&#20027;&#39064;&#27169;&#22411;&#12290;&#22312;&#27492;&#21069;&#25552;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#19968;&#32452;&#27880;&#37322;&#25968;&#25454;&#20013;&#36873;&#25321;&#26368;&#20339;&#31034;&#33539;&#65292;&#24182;&#35777;&#26126;&#30456;&#23545;&#20110;&#38543;&#26426;&#36873;&#25321;&#22522;&#32447;&#30340;&#24179;&#22343;&#20540;&#65292;&#22312;&#20843;&#20010;&#19981;&#21516;&#30340;&#30495;&#23454;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#24179;&#22343;&#27599;&#20010; GPT2 &#21644; GPT3 &#27169;&#22411;&#26377;&#26174;&#30528;&#30340; 12.5% &#30340;&#25552;&#21319;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#21457;&#29616;&#25903;&#25345;&#25105;&#20204;&#30340;&#20551;&#35774;&#65292;&#21363;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#34987;&#35270;&#20026;&#38544;&#21547;&#30340;&#20027;&#39064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, pre-trained large language models have demonstrated remarkable efficiency in achieving an inference-time few-shot learning capability known as in-context learning. However, existing literature has highlighted the sensitivity of this capability to the selection of few-shot demonstrations. The underlying mechanisms by which this capability arises from regular language model pretraining objectives remain poorly understood. In this study, we aim to examine the in-context learning phenomenon through a Bayesian lens, viewing large language models as topic models that implicitly infer task-related information from demonstrations. On this premise, we propose an algorithm for selecting optimal demonstrations from a set of annotated data and demonstrate a significant 12.5% improvement relative to the random selection baseline, averaged over eight GPT2 and GPT3 models on eight different real-world text classification datasets. Our empirical findings support our hypothesis that la
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SeqDiffuSeq&#30340;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#24207;&#21015;&#29983;&#25104;&#65292;&#37319;&#29992;&#20102;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;Transformer&#26550;&#26500;&#21644;&#33258;&#36866;&#24212;&#22122;&#22768;&#35843;&#24230;&#25216;&#26415;&#65292;&#26088;&#22312;&#25506;&#32034;&#25193;&#25955;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2212.10325</link><description>&lt;p&gt;
SeqDiffuSeq: &#19968;&#31181;&#20351;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;Transformer&#30340;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#24207;&#21015;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
SeqDiffuSeq: Text Diffusion Model with Encoder-Decoder Transformers for Sequence-to-Sequence Generation. (arXiv:2212.10325v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SeqDiffuSeq&#30340;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#24207;&#21015;&#29983;&#25104;&#65292;&#37319;&#29992;&#20102;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;Transformer&#26550;&#26500;&#21644;&#33258;&#36866;&#24212;&#22122;&#22768;&#35843;&#24230;&#25216;&#26415;&#65292;&#26088;&#22312;&#25506;&#32034;&#25193;&#25955;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#24314;&#27169;&#33539;&#24335;&#65292;&#22312;&#22270;&#20687;&#12289;&#38899;&#39057;&#21644;&#35270;&#39057;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#32771;&#34385;&#21040;&#25991;&#26412;&#30340;&#31163;&#25955;&#20998;&#31867;&#24615;&#36136;&#65292;&#23558;&#36830;&#32493;&#25193;&#25955;&#27169;&#22411;&#25193;&#23637;&#21040;&#33258;&#28982;&#35821;&#35328;&#24182;&#19981;&#26159;&#24494;&#19981;&#36275;&#36947;&#30340;&#65292;&#32780;&#19988;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#30740;&#31350;&#36739;&#23569;&#12290;&#24207;&#21015;&#29983;&#25104;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#35805;&#39064;&#20043;&#19968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#25193;&#25955;&#27169;&#22411;&#24212;&#29992;&#20110;&#24207;&#21015;&#29983;&#25104;&#65292;&#25506;&#32034;&#25193;&#25955;&#27169;&#22411;&#30340;&#20248;&#36234;&#29983;&#25104;&#24615;&#33021;&#33021;&#21542;&#36716;&#31227;&#21040;&#33258;&#28982;&#35821;&#35328;&#39046;&#22495;&#12290;&#25105;&#20204;&#25552;&#20986;SeqDiffuSeq&#65292;&#19968;&#31181;&#29992;&#20110;&#24207;&#21015;&#29983;&#25104;&#30340;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#12290;SeqDiffuSeq&#20351;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;Transformer&#26550;&#26500;&#26469;&#24314;&#27169;&#21435;&#22122;&#20989;&#25968;&#12290;&#20026;&#20102;&#25552;&#39640;&#29983;&#25104;&#36136;&#37327;&#65292;SeqDiffuSeq&#32467;&#21512;&#20102;&#33258;&#25105;&#35843;&#33410;&#25216;&#26415;&#21644;&#19968;&#20010;&#26032;&#25552;&#20986;&#30340;&#33258;&#36866;&#24212;&#22122;&#22768;&#35843;&#24230;&#25216;&#26415;&#12290;&#33258;&#36866;&#24212;&#22122;&#22768;&#35843;&#24230;&#20855;&#26377;&#22343;&#21248;&#21435;&#22122;&#30340;&#22256;&#38590;
&lt;/p&gt;
&lt;p&gt;
Diffusion model, a new generative modelling paradigm, has achieved great success in image, audio, and video generation. However, considering the discrete categorical nature of text, it is not trivial to extend continuous diffusion models to natural language, and text diffusion models are less studied. Sequence-to-sequence text generation is one of the essential natural language processing topics. In this work, we apply diffusion models to approach sequence-to-sequence text generation, and explore whether the superiority generation performance of diffusion model can transfer to natural language domain. We propose SeqDiffuSeq, a text diffusion model for sequence-to-sequence generation. SeqDiffuSeq uses an encoder-decoder Transformers architecture to model denoising function. In order to improve generation quality, SeqDiffuSeq combines the self-conditioning technique and a newly proposed adaptive noise schedule technique. The adaptive noise schedule has the difficulty of denoising evenly 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#32508;&#21512;&#35843;&#26597;&#12290;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#25552;&#20379;&#31995;&#32479;&#36164;&#28304;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;</title><link>http://arxiv.org/abs/2212.09597</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Reasoning with Language Model Prompting: A Survey. (arXiv:2212.09597v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#32508;&#21512;&#35843;&#26597;&#12290;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#25552;&#20379;&#31995;&#32479;&#36164;&#28304;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;&#20316;&#20026;&#22797;&#26434;&#38382;&#39064;&#35299;&#20915;&#30340;&#37325;&#35201;&#33021;&#21147;&#65292;&#21487;&#20197;&#20026;&#21307;&#30103;&#35786;&#26029;&#12289;&#35848;&#21028;&#31561;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#21518;&#31471;&#25903;&#25345;&#12290;&#26412;&#25991;&#23545;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#36827;&#34892;&#20102;&#32508;&#21512;&#35843;&#26597;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#30740;&#31350;&#25104;&#26524;&#30340;&#27604;&#36739;&#21644;&#24635;&#32467;&#65292;&#24182;&#25552;&#20379;&#20102;&#31995;&#32479;&#36164;&#28304;&#20197;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#31361;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#36164;&#28304;&#21487;&#22312; https://github.com/zjunlp/Prompt4ReasoningPapers &#19978;&#33719;&#21462;&#65288;&#23450;&#26399;&#26356;&#26032;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning, as an essential ability for complex problem-solving, can provide back-end support for various real-world applications, such as medical diagnosis, negotiation, etc. This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting. We introduce research works with comparisons and summaries and provide systematic resources to help beginners. We also discuss the potential reasons for emerging such reasoning abilities and highlight future research directions. Resources are available at https://github.com/zjunlp/Prompt4ReasoningPapers (updated periodically).
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#31350;&#20102;&#36890;&#36807;&#24120;&#35782;&#33976;&#39311;&#31639;&#27861;&#24378;&#21270;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#25361;&#25112;&#22823;&#22411;&#27169;&#22411;&#30340;&#24120;&#35782;&#33719;&#21462;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20381;&#36182;&#35268;&#27169;&#30340;&#23398;&#20064;&#31639;&#27861;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2212.09246</link><description>&lt;p&gt;
I2D2: &#22522;&#20110;NeuroLogic&#21644;&#33258;&#25105;&#27169;&#20223;&#30340;&#24402;&#32435;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
I2D2: Inductive Knowledge Distillation with NeuroLogic and Self-Imitation. (arXiv:2212.09246v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#31350;&#20102;&#36890;&#36807;&#24120;&#35782;&#33976;&#39311;&#31639;&#27861;&#24378;&#21270;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#25361;&#25112;&#22823;&#22411;&#27169;&#22411;&#30340;&#24120;&#35782;&#33719;&#21462;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20381;&#36182;&#35268;&#27169;&#30340;&#23398;&#20064;&#31639;&#27861;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#35268;&#27169;&#26041;&#38754;&#19981;&#26029;&#24378;&#21270;&#65292;&#20294;&#20173;&#32570;&#20047;&#22362;&#23454;&#30340;&#24120;&#35782;&#21151;&#33021;&#12290;&#28982;&#32780;&#65292;&#35268;&#27169;&#20284;&#20046;&#26159;&#21046;&#32988;&#27861;&#23453;&#65307;&#27605;&#31455;&#65292;&#26368;&#22823;&#30340;&#27169;&#22411;&#20284;&#20046;&#24050;&#32463;&#33719;&#24471;&#20102;&#26368;&#22810;&#30340;&#24120;&#35782;&#21151;&#33021;&#12290;&#36825;&#31687;&#35770;&#25991;&#25506;&#31350;&#20102;&#20284;&#20046;&#19981;&#21487;&#33021;&#23454;&#29616;&#30340;&#21305;&#37197;&#65306;&#22914;&#26524;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT-2&#65289;&#36890;&#36807;&#26032;&#39062;&#30340;&#24120;&#35782;&#33976;&#39311;&#31639;&#27861;&#24471;&#21040;&#21160;&#21147;&#65292;&#23427;&#20204;&#26159;&#21542;&#33021;&#36194;&#36807;&#27604;&#23427;&#20204;&#22823;&#25968;&#20010;&#25968;&#37327;&#32423;&#24182;&#19988;&#26356;&#20248;&#31168;&#30340;&#27169;&#22411;&#65288;&#22914;GPT-3&#65289;&#65311;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;&#20851;&#38190;&#26234;&#21147;&#38382;&#39064;&#26159;&#65292;&#26159;&#21542;&#21487;&#33021;&#35774;&#35745;&#19968;&#31181;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#24182;&#19981;&#21463;&#21040;&#35268;&#27169;&#30340;&#22909;&#22788;&#65292;&#32780;&#21364;&#26377;&#31454;&#20105;&#21147;&#30340;&#24120;&#35782;&#33719;&#21462;&#27700;&#24179;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24120;&#35782;&#30693;&#35782;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#37325;&#28857;&#20851;&#27880;&#29983;&#25104;&#36890;&#29992;&#35821;&#21477;&#30340;&#20219;&#21153;&#65292;&#21363;&#20851;&#20110;&#26085;&#24120;&#27010;&#24565;&#30340;&#24120;&#35782;&#20107;&#23454;&#38472;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models, despite their rapid advancements powered by scale, still fall short of robust commonsense capabilities. And yet, scale appears to be the winning recipe; after all, the largest models seem to have acquired the largest amount of commonsense capabilities. Or is it?  In this paper, we investigate the possibility of a seemingly impossible match: can smaller language models with dismal commonsense capabilities (i.e., GPT-2), ever win over models that are orders of magnitude larger and better (i.e., GPT-3), if the smaller models are powered with novel commonsense distillation algorithms? The key intellectual question we ask here is whether it is possible, if at all, to design a learning algorithm that does not benefit from scale, yet leads to a competitive level of commonsense acquisition. In this work, we study the generative models of commonsense knowledge, focusing on the task of generating generics, statements of commonsense facts about everyday concepts, e.g.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#24635;&#32467;&#20026;&#23548;&#21521;&#30340;&#22810;&#27169;&#24577;&#35270;&#35273;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#36741;&#21161;&#20219;&#21153;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#25277;&#35937;&#24635;&#32467;&#30340;&#36136;&#37327;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22810;&#31181;&#35821;&#35328; &#19978;&#37117;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.07672</link><description>&lt;p&gt;
&#20197;&#24635;&#32467;&#20026;&#23548;&#21521;&#30340;&#22810;&#27169;&#24577;&#35270;&#35273;&#24314;&#27169;&#29992;&#20110;&#25277;&#35937;&#24635;&#32467;
&lt;/p&gt;
&lt;p&gt;
Summary-Oriented Vision Modeling for Multimodal Abstractive Summarization. (arXiv:2212.07672v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07672
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#24635;&#32467;&#20026;&#23548;&#21521;&#30340;&#22810;&#27169;&#24577;&#35270;&#35273;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#36741;&#21161;&#20219;&#21153;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#25277;&#35937;&#24635;&#32467;&#30340;&#36136;&#37327;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22810;&#31181;&#35821;&#35328; &#19978;&#37117;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#25277;&#35937;&#24635;&#32467;(MAS)&#26088;&#22312;&#32473;&#23450;&#22810;&#27169;&#24577;&#25968;&#25454;(&#25991;&#26412;&#21644;&#22270;&#20687;)&#20135;&#29983;&#31616;&#27905;&#25688;&#35201;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#22914;&#20309;&#26377;&#25928;&#20351;&#29992;&#25991;&#31456;&#35270;&#35273;&#29305;&#24449;&#30340;&#35282;&#24230;&#65292;&#24050;&#32463;&#22312;&#39640;&#36164;&#28304;&#30340;&#33521;&#25991;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#21151;&#12290;&#20294;&#26159;&#65292;&#23545;&#20174;&#25688;&#35201;&#35270;&#35273;&#29305;&#24449;&#30340;&#35282;&#24230;&#26469;&#30475;&#30340;&#23569;&#20851;&#27880;&#65292;&#36825;&#21487;&#33021;&#20250;&#38480;&#21046;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#20302;&#21644;&#38646;&#36164;&#28304;&#22330;&#26223;&#19979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#24635;&#32467;&#23548;&#21521;&#30340;&#35270;&#35273;&#29305;&#24449;&#26469;&#25552;&#39640;&#24635;&#32467;&#36136;&#37327;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#20010;&#36741;&#21161;&#20219;&#21153;&#65292;&#21253;&#25324;&#35270;&#35273;&#21040;&#24635;&#32467;&#20219;&#21153;&#21644;&#25513;&#30721;&#22270;&#20687;&#24314;&#27169;&#20219;&#21153;&#12290;&#19982;&#20027;&#35201;&#25688;&#35201;&#20219;&#21153;&#19968;&#36215;&#65292;&#36890;&#36807;&#25152;&#26377;&#36825;&#20123;&#20219;&#21153;&#30340;&#35757;&#32451;&#30446;&#26631;&#23545;MAS&#27169;&#22411;&#36827;&#34892;&#20248;&#21270;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;MAS&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#25429;&#33719;&#24635;&#32467;&#23548;&#21521;&#30340;&#35270;&#35273;&#29305;&#24449;&#26469;&#24471;&#21040;&#26356;&#20934;&#30830;&#30340;&#24635;&#32467;&#12290; &#22312;44&#31181;&#35821;&#35328;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#21253;&#25324;&#20013;...
&lt;/p&gt;
&lt;p&gt;
Multimodal abstractive summarization (MAS) aims to produce a concise summary given the multimodal data (text and vision). Existing studies mainly focus on how to effectively use the visual features from the perspective of an article, having achieved impressive success on the high-resource English dataset. However, less attention has been paid to the visual features from the perspective of the summary, which may limit the model performance, especially in the low- and zero-resource scenarios. In this paper, we propose to improve the summary quality through summary-oriented visual features. To this end, we devise two auxiliary tasks including vision to summary task and masked image modeling task. Together with the main summarization task, we optimize the MAS model via the training objectives of all these tasks. By these means, the MAS model can be enhanced by capturing the summary-oriented visual features, thereby yielding more accurate summaries. Experiments on 44 languages, covering mid
&lt;/p&gt;</description></item><item><title>xTrimoABFold&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#25239;&#20307;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#25239;&#20307;&#32467;&#26500;&#39044;&#27979;&#26041;&#27861;&#65292;&#26080;&#38656;&#22810;&#24207;&#21015;&#27604;&#23545;&#65292;&#26377;&#26395;&#20419;&#36827;&#39640;&#36890;&#37327;&#33647;&#29289;&#35774;&#35745;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2212.00735</link><description>&lt;p&gt;
xTrimoABFold&#65306;&#26080;&#22810;&#24207;&#21015;&#27604;&#23545;&#30340;&#26032;&#22411;&#25239;&#20307;&#32467;&#26500;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
xTrimoABFold: De novo Antibody Structure Prediction without MSA. (arXiv:2212.00735v2 [q-bio.QM] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00735
&lt;/p&gt;
&lt;p&gt;
xTrimoABFold&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#25239;&#20307;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#25239;&#20307;&#32467;&#26500;&#39044;&#27979;&#26041;&#27861;&#65292;&#26080;&#38656;&#22810;&#24207;&#21015;&#27604;&#23545;&#65292;&#26377;&#26395;&#20419;&#36827;&#39640;&#36890;&#37327;&#33647;&#29289;&#35774;&#35745;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25239;&#20307;&#24037;&#31243;&#39046;&#22495;&#65292;&#35774;&#35745;&#19968;&#20010;&#26032;&#22411;&#25239;&#20307;&#20197;&#27491;&#30830;&#22320;&#32467;&#21512;&#29305;&#23450;&#25239;&#21407;&#30340;&#34920;&#20301;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#20102;&#35299;&#25239;&#20307;&#32467;&#26500;&#21644;&#20854;&#34920;&#20301;&#21487;&#20197;&#20419;&#36827;&#23545;&#20854;&#21151;&#33021;&#30340;&#26426;&#21046;&#29702;&#35299;&#12290;&#22240;&#27492;&#65292;&#20174;&#20854;&#24207;&#21015;&#39044;&#27979;&#25239;&#20307;&#32467;&#26500;&#19968;&#30452;&#26159;&#19968;&#39033;&#39640;&#24230;&#26377;&#20215;&#20540;&#30340;&#20219;&#21153;&#65292;&#32780;AlphaFold2&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#34507;&#30333;&#36136;&#24207;&#21015;&#39044;&#27979;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#23545;&#20110;&#25239;&#20307;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#25239;&#20307;&#30340;&#20114;&#34917;&#20915;&#23450;&#21306;&#65288;CDRs&#65289;&#65292;&#20854;&#39044;&#27979;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#26377;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of antibody engineering, an essential task is to design a novel antibody whose paratopes bind to a specific antigen with correct epitopes. Understanding antibody structure and its paratope can facilitate a mechanistic understanding of its function. Therefore, antibody structure prediction from its sequence alone has always been a highly valuable problem for de novo antibody design. AlphaFold2, a breakthrough in the field of structural biology, provides a solution to predict protein structure based on protein sequences and computationally expensive coevolutionary multiple sequence alignments (MSAs). However, the computational efficiency and undesirable prediction accuracy of antibodies, especially on the complementarity-determining regions (CDRs) of antibodies limit their applications in the industrially high-throughput drug design. To learn an informative representation of antibodies, we employed a deep antibody language model (ALM) on curated sequences from the observed a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#20316;&#25512;&#29702;&#35825;&#23548;&#30340;&#35821;&#35328;&#27169;&#22411;&#8212;&#8212;CoRe&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35299;&#20915;&#25968;&#23398;&#24212;&#29992;&#39064;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;CoRe &#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.16257</link><description>&lt;p&gt;
&#36890;&#36807;&#21512;&#20316;&#25512;&#29702;&#35825;&#23548;&#30340;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#25968;&#23398;&#24212;&#29992;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving Math Word Problems via Cooperative Reasoning induced Language Models. (arXiv:2210.16257v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16257
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#20316;&#25512;&#29702;&#35825;&#23548;&#30340;&#35821;&#35328;&#27169;&#22411;&#8212;&#8212;CoRe&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35299;&#20915;&#25968;&#23398;&#24212;&#29992;&#39064;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;CoRe &#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411; (PLMs) &#20026;&#38656;&#35201;&#39640;&#27700;&#24179;&#26234;&#33021;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#65288;&#22914;&#25968;&#23398;&#24212;&#29992;&#39064;&#65289;&#24102;&#26469;&#20102;&#26032;&#30340;&#26426;&#36935;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#24212;&#29992;&#29616;&#26377;&#30340; PLMs &#21040;&#25968;&#23398;&#24212;&#29992;&#39064;&#19978;&#20250;&#22833;&#36133;&#65292;&#22240;&#20026;&#20854;&#29983;&#25104;&#30340;&#36807;&#31243;&#32570;&#20047;&#36275;&#22815;&#30340;&#30417;&#30563;&#65292;&#32570;&#20047;&#20687;&#20154;&#31867;&#19968;&#26679;&#30340;&#24555;&#36895;&#36866;&#24212;&#24615;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#20154;&#31867;&#30340;&#25512;&#29702;&#36807;&#31243;&#26377;&#19968;&#20010;&#21452;&#37325;&#25512;&#29702;&#26694;&#26550;&#65292;&#21253;&#25324;&#19968;&#20010;&#21363;&#26102;&#21453;&#24212;&#31995;&#32479; (system 1) &#21644;&#19968;&#20010;&#31934;&#32454;&#25512;&#29702;&#31995;&#32479; (system 2)&#65292;&#25972;&#20010;&#25512;&#29702;&#36807;&#31243;&#30001;&#23427;&#20204;&#30340;&#20132;&#20114;&#20915;&#23450;&#12290;&#36825;&#21551;&#21457;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21512;&#20316;&#25512;&#29702;&#35825;&#23548;&#30340; PLM &#27169;&#22411;&#65292;&#31216;&#20026; Cooperative Reasoning (CoRe)&#65292;&#24471;&#21040;&#20102;&#19968;&#20010;&#20687;&#20154;&#31867;&#25512;&#29702;&#32467;&#26500;&#30340;&#26550;&#26500;&#65292;&#20854;&#20013; system 1 &#20316;&#20026;&#29983;&#25104;&#22120;&#65292;system 2 &#20316;&#20026;&#39564;&#35777;&#22120;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#29983;&#25104;&#22120;&#36127;&#36131;&#20135;&#29983;&#25512;&#29702;&#36335;&#24452;&#65292;&#39564;&#35777;&#22120;&#29992;&#20110;&#30417;&#30563;&#35780;&#20272;&#20197;&#33719;&#21462;&#21487;&#38752;&#30340;&#21453;&#39304;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CoRe &#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale pre-trained language models (PLMs) bring new opportunities to challenging problems, especially those that need high-level intelligence, such as the math word problem (MWPs). However, directly applying existing PLMs to MWPs can fail as the generation process lacks sufficient supervision and thus lacks fast adaptivity as humans. We notice that human reasoning has a dual reasoning framework that consists of an immediate reaction system (system 1) and a delicate reasoning system (system 2), where the entire reasoning is determined by their interaction. This inspires us to develop a cooperative reasoning-induced PLM for solving MWPs, called Cooperative Reasoning (CoRe), resulting in a human-like reasoning architecture with system 1 as the generator and system 2 as the verifier. In our approach, the generator is responsible for generating reasoning paths, and the verifiers are used to supervise the evaluation in order to obtain reliable feedback for the generator. We evaluate our
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#32463;&#36807;&#26102;&#38388;&#35843;&#25972;&#30340;&#33521;&#35821;&#27169;&#22411;&#24615;&#33021;&#24182;&#19981;&#20250;&#38543;&#26102;&#38388;&#25913;&#21892;&#65292;&#32780;&#27809;&#26377;&#32463;&#36807;&#26102;&#38388;&#35843;&#25972;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#23454;&#38469;&#19978;&#26356;&#21152;&#26377;&#25928;&#21644;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2210.07365</link><description>&lt;p&gt;
&#20197;&#29615;&#22659;&#25104;&#26412;&#20026;&#20195;&#20215;&#36827;&#34892;&#25345;&#32493;&#35757;&#32451;&#26159;&#21542;&#20540;&#24471;&#65311;&#23545;&#20110;&#26102;&#38388;&#36866;&#24212;&#24615;&#30340;&#35777;&#25454;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;
Is It Worth the (Environmental) Cost? Limited Evidence for Temporal Adaptation via Continuous Training. (arXiv:2210.07365v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#32463;&#36807;&#26102;&#38388;&#35843;&#25972;&#30340;&#33521;&#35821;&#27169;&#22411;&#24615;&#33021;&#24182;&#19981;&#20250;&#38543;&#26102;&#38388;&#25913;&#21892;&#65292;&#32780;&#27809;&#26377;&#32463;&#36807;&#26102;&#38388;&#35843;&#25972;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#23454;&#38469;&#19978;&#26356;&#21152;&#26377;&#25928;&#21644;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#19981;&#26029;&#21464;&#21270;&#21644;&#28436;&#21464;&#65292;&#20174;&#32780;&#23548;&#33268;&#35821;&#35328;&#27169;&#22411;&#24456;&#24555;&#36807;&#26102;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24212;&#35813;&#36890;&#36807;&#26032;&#25968;&#25454;&#19981;&#26029;&#26356;&#26032;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#20351;&#20854;&#26292;&#38706;&#20110;&#26032;&#30340;&#20107;&#20214;&#21644;&#20107;&#23454;&#20013;&#12290;&#28982;&#32780;&#65292;&#36825;&#38656;&#35201;&#39069;&#22806;&#30340;&#35745;&#31639;&#65292;&#20063;&#23601;&#24847;&#21619;&#30528;&#26032;&#30340;&#30899;&#25490;&#25918;&#12290;&#26377;&#27809;&#26377;&#21487;&#34913;&#37327;&#30340;&#25928;&#30410;&#21487;&#20197;&#35777;&#26126;&#36825;&#20010;&#25104;&#26412;&#26159;&#21512;&#29702;&#30340;&#21602;&#65311;&#26412;&#25991;&#23547;&#25214;&#23454;&#35777;&#35777;&#25454;&#26469;&#25903;&#25345;&#25345;&#32493;&#35757;&#32451;&#12290;&#25105;&#20204;&#37325;&#29616;&#20102;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#25193;&#23637;&#20102;&#23427;&#20204;&#65292;&#21253;&#25324;&#39069;&#22806;&#30340;&#26102;&#38388;&#27573;&#12289;&#27169;&#22411;&#21644;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#38024;&#23545;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#30340;&#33521;&#35821;&#27169;&#22411;&#32463;&#36807;&#26102;&#38388;&#35843;&#25972;&#21518;&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#19981;&#20250;&#38543;&#26102;&#38388;&#25913;&#21892;&#12290;&#27809;&#26377;&#32463;&#36807;&#26102;&#38388;&#35843;&#25972;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#23454;&#38469;&#19978;&#26356;&#21152;&#26377;&#25928;&#21644;&#39640;&#25928;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#20063;&#27880;&#24847;&#21040;&#32570;&#20047;&#21512;&#36866;&#30340;&#26102;&#38388;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#20419;&#20351;&#20154;&#20204;&#23545;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#36866;&#24212;&#35821;&#35328;&#27169;&#22411;&#22312;&#21487;&#25345;&#32493;&#24615;&#26041;&#38754;&#36827;&#34892;&#25209;&#21028;&#24615;&#30340;&#21453;&#24605;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language is constantly changing and evolving, leaving language models to become quickly outdated. Consequently, we should continuously update our models with new data to expose them to new events and facts. However, that requires additional computing, which means new carbon emissions. Do any measurable benefits justify this cost? This paper looks for empirical evidence to support continuous training. We reproduce existing benchmarks and extend them to include additional time periods, models, and tasks. Our results show that the downstream task performance of temporally adapted English models for social media data do not improve over time. Pretrained models without temporal adaptation are actually significantly more effective and efficient. However, we also note a lack of suitable temporal benchmarks. Our findings invite a critical reflection on when and how to temporally adapt language models, accounting for sustainability.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#23569;&#26679;&#26412;&#22686;&#37327;&#20107;&#20214;&#26816;&#27979;&#20219;&#21153;&#65292;&#38024;&#23545;&#23398;&#20064;&#26816;&#27979;&#24102;&#26377;&#26497;&#23569;&#26631;&#31614;&#25968;&#25454;&#30340;&#26032;&#20107;&#20214;&#31867;&#21035;&#24182;&#20445;&#30041;&#26816;&#27979;&#26087;&#31867;&#21035;&#33021;&#21147;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#22871;&#30001;&#32858;&#31867;&#27169;&#22359;&#21644;&#30693;&#35782;&#33976;&#39311;&#27169;&#22359;&#32452;&#25104;&#30340;&#35299;&#20915;&#26694;&#26550;&#65292;&#24182;&#22312; IFSED &#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2209.01979</link><description>&lt;p&gt;
&#23569;&#26679;&#26412;&#22686;&#37327;&#20107;&#20214;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Few-shot Incremental Event Detection. (arXiv:2209.01979v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.01979
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23569;&#26679;&#26412;&#22686;&#37327;&#20107;&#20214;&#26816;&#27979;&#20219;&#21153;&#65292;&#38024;&#23545;&#23398;&#20064;&#26816;&#27979;&#24102;&#26377;&#26497;&#23569;&#26631;&#31614;&#25968;&#25454;&#30340;&#26032;&#20107;&#20214;&#31867;&#21035;&#24182;&#20445;&#30041;&#26816;&#27979;&#26087;&#31867;&#21035;&#33021;&#21147;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#22871;&#30001;&#32858;&#31867;&#27169;&#22359;&#21644;&#30693;&#35782;&#33976;&#39311;&#27169;&#22359;&#32452;&#25104;&#30340;&#35299;&#20915;&#26694;&#26550;&#65292;&#24182;&#22312; IFSED &#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#26816;&#27979;&#20219;&#21153;&#21487;&#20197;&#20174;&#25991;&#26412;&#20013;&#24555;&#36895;&#26816;&#27979;&#20107;&#20214;&#65292;&#24182;&#20026;&#19979;&#28216;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#25552;&#20379;&#26377;&#21147;&#30340;&#25903;&#25345;&#12290;&#22823;&#22810;&#25968;&#36825;&#26679;&#30340;&#26041;&#27861;&#21482;&#33021;&#26816;&#27979;&#19968;&#32452;&#22266;&#23450;&#30340;&#39044;&#23450;&#20041;&#20107;&#20214;&#31867;&#21035;&#12290;&#25193;&#23637;&#23427;&#20204;&#20197;&#26816;&#27979;&#26032;&#31867;&#21035;&#65292;&#32780;&#19981;&#22833;&#21435;&#26816;&#27979;&#26087;&#31867;&#21035;&#30340;&#33021;&#21147;&#65292;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#25104;&#26412;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#22686;&#37327;&#23398;&#20064;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#26159;&#38656;&#35201;&#22823;&#37327;&#26032;&#31867;&#21035;&#30340;&#26631;&#35760;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#26032;&#20107;&#20214;&#31867;&#21035;&#32570;&#20047;&#39640;&#36136;&#37327;&#30340;&#26631;&#35760;&#25968;&#25454;&#65292;&#23548;&#33268;&#24456;&#38590;&#33719;&#24471;&#36275;&#22815;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#38024;&#23545;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#21363;"&#23569;&#26679;&#26412;&#22686;&#37327;&#20107;&#20214;&#26816;&#27979;"&#65292;&#26088;&#22312;&#23398;&#20064;&#20165;&#26377;&#23569;&#37327;&#25968;&#25454;&#30340;&#26032;&#20107;&#20214;&#31867;&#21035;&#30340;&#26816;&#27979;&#65292;&#21516;&#26102;&#23613;&#21487;&#33021;&#22320;&#20445;&#30041;&#26816;&#27979;&#26087;&#31867;&#21035;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22522;&#20110;FewEvent&#21019;&#24314;&#20102;IFSED&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;IFSED-K&#21644;IFSED-C&#20197;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30001;&#32858;&#31867;&#27169;&#22359;&#21644;&#30693;&#35782;&#33976;&#39311;&#27169;&#22359;&#32452;&#25104;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#35782;&#21035;&#26032;&#20107;&#20214;&#23454;&#20363;&#21644;&#30693;&#35782;&#20256;&#36755;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#23569;&#26679;&#26412;&#22686;&#37327;&#20107;&#20214;&#26816;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Event detection tasks can enable the quick detection of events from texts and provide powerful support for downstream natural language processing tasks. Most such methods can only detect a fixed set of predefined event classes. To extend them to detect a new class without losing the ability to detect old classes requires costly retraining of the model from scratch. Incremental learning can effectively solve this problem, but it requires abundant data of new classes. In practice, however, the lack of high-quality labeled data of new event classes makes it difficult to obtain enough data for model training. To address the above mentioned issues, we define a new task, few-shot incremental event detection, which focuses on learning to detect a new event class with limited data, while retaining the ability to detect old classes to the extent possible. We created a benchmark dataset IFSED for the few-shot incremental event detection task based on FewEvent and propose two benchmarks, IFSED-K 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22359;&#21270;&#20559;&#24046;&#32531;&#35299;&#26041;&#27861;&#65292;&#22312;&#25512;&#29702;&#26102;&#38388;&#25353;&#38656;&#38598;&#25104;&#21040;&#26680;&#24515;&#27169;&#22411;&#20013;&#30340;&#29420;&#31435;&#21435;&#20559;&#32622;&#23376;&#32593;&#32476;&#65292;&#22312;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#24180;&#40836;&#31561;&#21463;&#20445;&#25252;&#23646;&#24615;&#30340;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#35813;&#26041;&#27861;&#22312;&#32531;&#35299;&#20559;&#24046;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#65292;&#24182;&#19988;&#22312;&#31934;&#24230;&#21644;&#28789;&#27963;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2205.15171</link><description>&lt;p&gt;
&#24102;&#26377;&#23646;&#24615;&#21024;&#38500;&#23376;&#32593;&#32476;&#30340;&#27169;&#22359;&#21270;&#21644;&#25353;&#38656;&#20559;&#24046;&#32531;&#35299;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Modular and On-demand Bias Mitigation with Attribute-Removal Subnetworks. (arXiv:2205.15171v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.15171
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22359;&#21270;&#20559;&#24046;&#32531;&#35299;&#26041;&#27861;&#65292;&#22312;&#25512;&#29702;&#26102;&#38388;&#25353;&#38656;&#38598;&#25104;&#21040;&#26680;&#24515;&#27169;&#22411;&#20013;&#30340;&#29420;&#31435;&#21435;&#20559;&#32622;&#23376;&#32593;&#32476;&#65292;&#22312;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#24180;&#40836;&#31561;&#21463;&#20445;&#25252;&#23646;&#24615;&#30340;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#35813;&#26041;&#27861;&#22312;&#32531;&#35299;&#20559;&#24046;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#65292;&#24182;&#19988;&#22312;&#31934;&#24230;&#21644;&#28789;&#27963;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20250;&#20559;&#35265;&#21453;&#26144;&#22312;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21450;&#20854;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24494;&#35843;&#29256;&#26412;&#20013;&#12290;&#24120;&#35265;&#30340;&#22788;&#29702;&#20559;&#24046;&#30340;&#26041;&#27861;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#20248;&#21270;&#26631;&#20934;&#65292;&#24182;&#26356;&#26032;&#27169;&#22411;&#20197;&#36798;&#21040;&#26032;&#30340;&#21435;&#20559;&#32622;&#29366;&#24577;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#26368;&#32456;&#29992;&#25143;&#21644;&#20174;&#19994;&#20154;&#21592;&#21487;&#33021;&#26356;&#21916;&#27426;&#20999;&#25442;&#22238;&#21407;&#22987;&#27169;&#22411;&#65292;&#25110;&#20165;&#23545;&#29305;&#23450;&#23376;&#38598;&#30340;&#20445;&#25252;&#23646;&#24615;&#24212;&#29992;&#21435;&#20559;&#32622;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22359;&#21270;&#20559;&#24046;&#32531;&#35299;&#26041;&#27861;&#65292;&#21253;&#25324;&#29420;&#31435;&#39640;&#24230;&#31232;&#30095;&#30340;&#21435;&#20559;&#32622;&#23376;&#32593;&#32476;&#65292;&#20854;&#20013;&#27599;&#20010;&#21435;&#20559;&#32622;&#27169;&#22359;&#21487;&#20197;&#22312;&#25512;&#29702;&#26102;&#38388;&#25353;&#38656;&#38598;&#25104;&#21040;&#26680;&#24515;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20511;&#37492;&#20102;&#8220;diff&#8221;&#21098;&#26525;&#30340;&#27010;&#24565;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#21512;&#20110;&#21508;&#31181;&#34920;&#31034;&#20998;&#31163;&#20248;&#21270;&#30340;&#26032;&#22411;&#35757;&#32451;&#26041;&#24335;&#12290;&#25105;&#20204;&#22312;&#20855;&#26377;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#24180;&#40836;&#31561;&#21463;&#20445;&#25252;&#23646;&#24615;&#30340;&#19977;&#20010;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22359;&#21270;&#26041;&#27861;&#22312;&#32531;&#35299;&#20559;&#24046;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#65292;&#24182;&#19988;&#22312;&#31934;&#24230;&#21644;&#28789;&#27963;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Societal biases are reflected in large pre-trained language models and their fine-tuned versions on downstream tasks. Common in-processing bias mitigation approaches, such as adversarial training and mutual information removal, introduce additional optimization criteria, and update the model to reach a new debiased state. However, in practice, end-users and practitioners might prefer to switch back to the original model, or apply debiasing only on a specific subset of protected attributes. To enable this, we propose a novel modular bias mitigation approach, consisting of stand-alone highly sparse debiasing subnetworks, where each debiasing module can be integrated into the core model on-demand at inference time. Our approach draws from the concept of \emph{diff} pruning, and proposes a novel training regime adaptable to various representation disentanglement optimizations. We conduct experiments on three classification tasks with gender, race, and age as protected attributes. The resul
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21483;&#20570;MiniDisc&#30340;&#26368;&#23567;&#33976;&#39311;&#35745;&#21010;&#65292;&#21487;&#20197;&#22312;&#26368;&#23569;&#19968;&#27425;&#23581;&#35797;&#20013;&#35843;&#24230;&#26368;&#20248;&#30340;&#25945;&#24072;&#21161;&#25163;&#65292;&#29992;&#20110;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;&#12290;</title><link>http://arxiv.org/abs/2205.14570</link><description>&lt;p&gt;
MiniDisc: &#26368;&#23567;&#33976;&#39311;&#35745;&#21010;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
MiniDisc: Minimal Distillation Schedule for Language Model Compression. (arXiv:2205.14570v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21483;&#20570;MiniDisc&#30340;&#26368;&#23567;&#33976;&#39311;&#35745;&#21010;&#65292;&#21487;&#20197;&#22312;&#26368;&#23569;&#19968;&#27425;&#23581;&#35797;&#20013;&#35843;&#24230;&#26368;&#20248;&#30340;&#25945;&#24072;&#21161;&#25163;&#65292;&#29992;&#20110;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#25945;&#24072;&#27169;&#22411;&#21644;&#23398;&#29983;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#36739;&#22823;&#30340;&#23481;&#37327;&#24046;&#36317;&#26102;&#65292;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;&#30340;&#25928;&#26524;&#19981;&#20339;&#65292;&#24341;&#20837;&#20102;&#25945;&#24072;&#21161;&#25163;&#36741;&#21161;&#33976;&#39311;&#26469;&#24357;&#34917;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#25945;&#24072;&#21161;&#25163;&#30340;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#23581;&#35797;&#25165;&#33021;&#35843;&#24230;&#20986;&#26368;&#20248;&#30340;&#25945;&#24072;&#21161;&#25163;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#23567;&#33976;&#39311;&#35745;&#21010;&#65288;MiniDisc&#65289;&#65292;&#21487;&#20197;&#22312;&#26368;&#23569;&#19968;&#27425;&#23581;&#35797;&#20013;&#35843;&#24230;&#26368;&#20248;&#30340;&#25945;&#24072;&#21161;&#25163;&#12290;MiniDisc&#26159;&#22522;&#20110;&#25945;&#24072;&#21161;&#25163;&#30340;&#35268;&#27169;-&#24615;&#33021;&#30340;&#26435;&#34913;&#26469;&#24230;&#37327;&#25945;&#24072;&#21161;&#25163;&#30340;&#26368;&#20248;&#24615;&#65292;&#24182;&#21487;&#20197;&#22312;&#19981;&#23545;&#23398;&#29983;&#36827;&#34892;&#23454;&#39564;&#30340;&#24773;&#20917;&#19979;&#35843;&#24230;&#26368;&#20248;&#30340;&#25945;&#24072;&#21161;&#25163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have uncovered that language model distillation is less effective when facing a large capacity gap between the teacher and the student, and introduced teacher assistant-based distillation to bridge the gap. As a connection, the scale and the performance of the teacher assistant is of vital importance to bring the knowledge from the teacher to the student. However, existing teacher assistant-based methods require maximally many trials before scheduling an optimal teacher assistant. To this end, we propose a minimal distillation schedule (MiniDisc) for scheduling the optimal teacher assistant in minimally one trial. In particular, motivated by the finding that the performance of the student is positively correlated to the scale-performance tradeoff of the teacher assistant, MiniDisc is designed with a $\lambda$-tradeoff to measure the optimality of the teacher assistant without trial distillation to the student. MiniDisc then can schedule the optimal teacher assistant with
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#22914;&#20309;&#23558;&#25991;&#26412;&#25968;&#25454;&#19982;&#26102;&#38388;&#30693;&#35782;&#23884;&#20837;&#30456;&#32467;&#21512;&#20197;&#21152;&#24378;&#26102;&#38388;&#30693;&#35782;&#23884;&#20837;&#34920;&#24449;&#30340;&#36136;&#37327;&#65292;&#25552;&#20986;&#20102;ECOLA&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#26102;&#38388;&#22240;&#32032;&#65292;&#24182;&#23558;&#25991;&#26412;&#20449;&#24687;&#27880;&#20837;&#21040;&#26102;&#38388;&#30693;&#35782;&#23884;&#20837;&#20013;&#12290;</title><link>http://arxiv.org/abs/2203.09590</link><description>&lt;p&gt;
ECOLA: &#20351;&#29992;&#19978;&#19979;&#25991;&#21270;&#30340;&#35821;&#35328;&#34920;&#31034;&#22686;&#24378;&#26102;&#38388;&#30693;&#35782;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
ECOLA: Enhanced Temporal Knowledge Embeddings with Contextualized Language Representations. (arXiv:2203.09590v5 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.09590
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#22914;&#20309;&#23558;&#25991;&#26412;&#25968;&#25454;&#19982;&#26102;&#38388;&#30693;&#35782;&#23884;&#20837;&#30456;&#32467;&#21512;&#20197;&#21152;&#24378;&#26102;&#38388;&#30693;&#35782;&#23884;&#20837;&#34920;&#24449;&#30340;&#36136;&#37327;&#65292;&#25552;&#20986;&#20102;ECOLA&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#26102;&#38388;&#22240;&#32032;&#65292;&#24182;&#23558;&#25991;&#26412;&#20449;&#24687;&#27880;&#20837;&#21040;&#26102;&#38388;&#30693;&#35782;&#23884;&#20837;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#30693;&#35782;&#23884;&#20837;&#27169;&#22411;&#19981;&#33021;&#20805;&#20998;&#21033;&#29992;&#20016;&#23500;&#30340;&#25991;&#26412;&#20449;&#24687;&#65292;&#22240;&#27492;&#24050;&#32463;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#30740;&#31350;&#20197;&#21033;&#29992;&#25991;&#26412;&#26469;&#22686;&#24378;&#30693;&#35782;&#23884;&#20837;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22686;&#24378;&#26041;&#27861;&#26080;&#27861;&#24212;&#29992;&#20110;&#21253;&#21547;&#26102;&#38388;&#20381;&#36182;&#20107;&#20214;&#30693;&#35782;&#21644;&#22797;&#26434;&#26102;&#38388;&#21160;&#24577;&#30340;&#26102;&#38388;&#30693;&#35782;&#22270;&#65288;tKG&#65289;&#12290; &#29305;&#21035;&#26159;&#65292;&#29616;&#26377;&#30340;&#22686;&#24378;&#26041;&#27861;&#36890;&#24120;&#20551;&#23450;&#30693;&#35782;&#23884;&#20837;&#26159;&#29420;&#31435;&#20110;&#26102;&#38388;&#30340;&#12290;&#30456;&#21453;&#65292;&#22312;tKG&#27169;&#22411;&#20013;&#65292;&#23454;&#20307;&#23884;&#20837;&#36890;&#24120;&#20250;&#19981;&#26029;&#28436;&#21270;&#65292;&#36825;&#23601;&#25552;&#20986;&#20102;&#23558;&#26102;&#38388;&#30456;&#20851;&#25991;&#26412;&#19982;&#23454;&#20307;&#23545;&#40784;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#30740;&#31350;&#22914;&#20309;&#23558;&#25991;&#26412;&#25968;&#25454;&#19982;&#26102;&#38388;&#30693;&#35782;&#23884;&#20837;&#30456;&#32467;&#21512;&#26469;&#22686;&#24378;&#26102;&#38388;&#30693;&#35782;&#23884;&#20837;&#12290;&#20316;&#20026;&#36825;&#39033;&#20219;&#21153;&#30340;&#19968;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#19978;&#19979;&#25991;&#21270;&#30340;&#35821;&#35328;&#34920;&#31034;&#22686;&#24378;&#26102;&#38388;&#30693;&#35782;&#23884;&#20837;&#65288;ECOLA&#65289;&#65292;&#23427;&#32771;&#34385;&#20102;&#26102;&#38388;&#22240;&#32032;&#65292;&#24182;&#23558;&#25991;&#26412;&#20449;&#24687;&#27880;&#20837;&#21040;&#26102;&#38388;&#30693;&#35782;&#23884;&#20837;&#20013;&#12290;&#20026;&#20102;&#35780;&#20272;ECOLA&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19977;&#20010;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Since conventional knowledge embedding models cannot take full advantage of the abundant textual information, there have been extensive research efforts in enhancing knowledge embedding using texts. However, existing enhancement approaches cannot apply to temporal knowledge graphs (tKGs), which contain time-dependent event knowledge with complex temporal dynamics. Specifically, existing enhancement approaches often assume knowledge embedding is time-independent. In contrast, the entity embedding in tKG models usually evolves, which poses the challenge of aligning temporally relevant texts with entities. To this end, we propose to study enhancing temporal knowledge embedding with textual data in this paper. As an approach to this task, we propose Enhanced Temporal Knowledge Embeddings with Contextualized Language Representations (ECOLA), which takes the temporal aspect into account and injects textual information into temporal knowledge embedding. To evaluate ECOLA, we introduce three n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#22024;&#26434;&#30340;&#20013;&#38388;&#35268;&#27169;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#36827;&#34892;&#30340;&#39318;&#20010;&#22823;&#20110;100&#20010;&#21477;&#23376;&#25968;&#25454;&#38598;&#30340;NLP&#23454;&#39564;&#32467;&#26524;&#65292;&#25104;&#21151;&#22320;&#35757;&#32451;&#20102;&#35299;&#20915;&#31616;&#21333;&#21477;&#23376;&#20998;&#31867;&#20219;&#21153;&#30340;NLP&#27169;&#22411;&#65292;&#35777;&#26126;&#20102;&#32452;&#21512;&#27169;&#22411;&#30340;&#21547;&#20041;&#19982;&#37327;&#23376;&#29702;&#35770;&#20855;&#26377;&#24418;&#24335;&#30456;&#20284;&#24615;&#12290;</title><link>http://arxiv.org/abs/2102.12846</link><description>&lt;p&gt;
&#23454;&#36341;&#20013;&#30340;QNLP&#65306;&#22312;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#36816;&#34892;&#32452;&#21512;&#27169;&#22411;&#30340;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
QNLP in Practice: Running Compositional Models of Meaning on a Quantum Computer. (arXiv:2102.12846v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.12846
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#22024;&#26434;&#30340;&#20013;&#38388;&#35268;&#27169;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#36827;&#34892;&#30340;&#39318;&#20010;&#22823;&#20110;100&#20010;&#21477;&#23376;&#25968;&#25454;&#38598;&#30340;NLP&#23454;&#39564;&#32467;&#26524;&#65292;&#25104;&#21151;&#22320;&#35757;&#32451;&#20102;&#35299;&#20915;&#31616;&#21333;&#21477;&#23376;&#20998;&#31867;&#20219;&#21153;&#30340;NLP&#27169;&#22411;&#65292;&#35777;&#26126;&#20102;&#32452;&#21512;&#27169;&#22411;&#30340;&#21547;&#20041;&#19982;&#37327;&#23376;&#29702;&#35770;&#20855;&#26377;&#24418;&#24335;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;QNLP&#65289;&#28041;&#21450;&#35774;&#35745;&#21644;&#23454;&#29616;&#26088;&#22312;&#22312;&#37327;&#23376;&#30828;&#20214;&#19978;&#36816;&#34892;&#30340;NLP&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22312;&#22024;&#26434;&#30340;&#20013;&#38388;&#35268;&#27169;&#37327;&#23376;&#65288;NISQ&#65289;&#35745;&#31639;&#26426;&#19978;&#36827;&#34892;&#30340;&#39318;&#20010;&#22823;&#20110;100&#20010;&#21477;&#23376;&#25968;&#25454;&#38598;&#30340;NLP&#23454;&#39564;&#32467;&#26524;&#12290;&#21033;&#29992;&#30001;Coecke&#12289;Sadrzadeh&#21644;Clark&#65288;2010&#65289;&#25552;&#20986;&#30340;&#21547;&#20041;&#32452;&#21512;&#27169;&#22411;&#19982;&#37327;&#23376;&#29702;&#35770;&#30340;&#24418;&#24335;&#30456;&#20284;&#24615;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#20855;&#26377;&#33258;&#28982;&#26144;&#23556;&#21040;&#37327;&#23376;&#30005;&#36335;&#30340;&#21477;&#23376;&#34920;&#31034;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#34920;&#31034;&#26469;&#23454;&#29616;&#24182;&#25104;&#21151;&#35757;&#32451;&#22312;&#37327;&#23376;&#30828;&#20214;&#19978;&#35299;&#20915;&#31616;&#21333;&#21477;&#23376;&#20998;&#31867;&#20219;&#21153;&#30340;NLP&#27169;&#22411;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#37327;&#23376;&#27169;&#25311;&#65292;&#27604;&#36739;&#20102;Coecke&#31561;&#20154;&#30340;&#35821;&#27861;&#25935;&#24863;&#27169;&#22411;&#19982;&#20351;&#29992;&#36739;&#23569;&#25110;&#26080;&#35821;&#27861;&#30340;&#20004;&#20010;&#22522;&#32447;&#65292;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#8220;&#35789;&#34955;&#8221;&#27169;&#22411;&#30340;&#37327;&#23376;&#27169;&#25311;&#65292;&#20854;&#20013;&#26681;&#26412;&#19981;&#32771;&#34385;&#35821;&#27861;&#65292;&#20197;&#21450;&#21333;&#35789;&#24207;&#21015;&#27169;&#22411;&#30340;&#37327;&#23376;&#27169;&#25311;&#65292;&#20165;&#23562;&#37325;&#21333;&#35789;&#39034;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum Natural Language Processing (QNLP) deals with the design and implementation of NLP models intended to be run on quantum hardware. In this paper, we present results on the first NLP experiments conducted on Noisy Intermediate-Scale Quantum (NISQ) computers for datasets of size greater than 100 sentences. Exploiting the formal similarity of the compositional model of meaning by Coecke, Sadrzadeh and Clark (2010) with quantum theory, we create representations for sentences that have a natural mapping to quantum circuits. We use these representations to implement and successfully train NLP models that solve simple sentence classification tasks on quantum hardware. We conduct quantum simulations that compare the syntax-sensitive model of Coecke et al. with two baselines that use less or no syntax; specifically, we implement the quantum analogues of a "bag-of-words" model, where syntax is not taken into account at all, and of a word-sequence model, where only word order is respected.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#25991;&#26723;&#26816;&#32034;&#27169;&#22411;Simplified TinyBERT&#65292;&#23427;&#22312;&#25552;&#20379;15&#20493;&#36895;&#24230;&#25552;&#21319;&#30340;&#24773;&#20917;&#19979;&#27604;BERT-Base&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2009.07531</link><description>&lt;p&gt;
&#31616;&#21270;&#29256;TinyBERT: &#29992;&#20110;&#25991;&#26723;&#26816;&#32034;&#30340;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Simplified TinyBERT: Knowledge Distillation for Document Retrieval. (arXiv:2009.07531v2 [cs.IR] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2009.07531
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#25991;&#26723;&#26816;&#32034;&#27169;&#22411;Simplified TinyBERT&#65292;&#23427;&#22312;&#25552;&#20379;15&#20493;&#36895;&#24230;&#25552;&#21319;&#30340;&#24773;&#20917;&#19979;&#27604;BERT-Base&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21033;&#29992;BERT&#27169;&#22411;&#36827;&#34892;&#25991;&#26723;&#25490;&#24207;&#21313;&#20998;&#26377;&#25928;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#38480;&#21046;&#20102;&#20854;&#20351;&#29992;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#39318;&#20808;&#22312;&#25991;&#26723;&#25490;&#24207;&#20219;&#21153;&#19978;&#23454;&#35777;&#30740;&#31350;&#20102;&#20004;&#20010;&#30693;&#35782;&#33976;&#39311;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#22312;&#26368;&#36817;&#25552;&#20986;&#30340;TinyBERT&#27169;&#22411;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#31616;&#21270;&#26041;&#26696;&#12290;&#20004;&#20010;&#19981;&#21516;&#24182;&#19988;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20934;&#27979;&#35797;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#20855;&#26377;&#25152;&#25552;&#20986;&#31616;&#21270;&#26041;&#26696;&#30340;Simplified TinyBERT&#19981;&#20165;&#25552;&#21319;&#20102;TinyBERT&#65292;&#32780;&#19988;&#22312;&#25552;&#20379;15&#20493;&#36895;&#24230;&#25552;&#21319;&#30340;&#24773;&#20917;&#19979;&#20063;&#26126;&#26174;&#20248;&#20110;BERT-Base&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the effectiveness of utilizing the BERT model for document ranking, the high computational cost of such approaches limits their uses. To this end, this paper first empirically investigates the effectiveness of two knowledge distillation models on the document ranking task. In addition, on top of the recently proposed TinyBERT model, two simplifications are proposed. Evaluations on two different and widely-used benchmarks demonstrate that Simplified TinyBERT with the proposed simplifications not only boosts TinyBERT, but also significantly outperforms BERT-Base when providing 15$\times$ speedup.
&lt;/p&gt;</description></item></channel></rss>