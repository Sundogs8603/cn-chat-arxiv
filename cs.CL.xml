<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;ChatGPT&#25903;&#25345;&#31185;&#23398;&#24037;&#20316;&#27969;&#26102;&#30340;&#25928;&#29575;&#12290;&#36890;&#36807;&#22312;&#20004;&#20010;&#31185;&#23398;&#39046;&#22495;&#36827;&#34892;&#19977;&#20010;&#29992;&#25143;&#30740;&#31350;&#65292;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;LLMs&#33021;&#22815;&#39640;&#25928;&#35299;&#37322;&#24037;&#20316;&#27969;&#65292;&#20294;&#22312;&#32452;&#20214;&#20132;&#25442;&#21644;&#26377;&#30446;&#30340;&#24037;&#20316;&#26041;&#38754;&#30340;&#24615;&#33021;&#36739;&#20302;&#12290;</title><link>http://arxiv.org/abs/2311.01825</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25327;&#25937;&#34892;&#21160;&#65306;&#20351;&#29992;ChatGPT&#20943;&#23569;&#31185;&#23398;&#24037;&#20316;&#27969;&#24320;&#21457;&#30340;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
Large Language Models to the Rescue: Reducing the Complexity in Scientific Workflow Development Using ChatGPT. (arXiv:2311.01825v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01825
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;ChatGPT&#25903;&#25345;&#31185;&#23398;&#24037;&#20316;&#27969;&#26102;&#30340;&#25928;&#29575;&#12290;&#36890;&#36807;&#22312;&#20004;&#20010;&#31185;&#23398;&#39046;&#22495;&#36827;&#34892;&#19977;&#20010;&#29992;&#25143;&#30740;&#31350;&#65292;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;LLMs&#33021;&#22815;&#39640;&#25928;&#35299;&#37322;&#24037;&#20316;&#27969;&#65292;&#20294;&#22312;&#32452;&#20214;&#20132;&#25442;&#21644;&#26377;&#30446;&#30340;&#24037;&#20316;&#26041;&#38754;&#30340;&#24615;&#33021;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#24037;&#20316;&#27969;&#31995;&#32479;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#34920;&#36798;&#21644;&#25191;&#34892;&#22797;&#26434;&#30340;&#25968;&#25454;&#20998;&#26512;&#27969;&#31243;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#36807;&#22312;&#22823;&#22411;&#35745;&#31639;&#38598;&#32676;&#19978;&#30340;&#33258;&#21160;&#24182;&#34892;&#21270;&#25552;&#20379;&#20102;&#21487;&#22797;&#21046;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#30340;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#28041;&#21450;&#35768;&#22810;&#40657;&#30418;&#24037;&#20855;&#21644;&#24517;&#35201;&#30340;&#28145;&#23618;&#22522;&#30784;&#35774;&#26045;&#26632;&#65292;&#23454;&#29616;&#24037;&#20316;&#27969;&#38750;&#24120;&#22256;&#38590;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#29992;&#25143;&#25903;&#25345;&#24037;&#20855;&#24456;&#23569;&#65292;&#24182;&#19988;&#21487;&#29992;&#31034;&#20363;&#30340;&#25968;&#37327;&#36828;&#36828;&#20302;&#20110;&#20256;&#32479;&#32534;&#31243;&#35821;&#35328;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#29305;&#21035;&#26159;ChatGPT&#65292;&#22312;&#22788;&#29702;&#31185;&#23398;&#24037;&#20316;&#27969;&#26102;&#25903;&#25345;&#29992;&#25143;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#31185;&#23398;&#39046;&#22495;&#36827;&#34892;&#20102;&#19977;&#20010;&#29992;&#25143;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;ChatGPT&#30340;&#29702;&#35299;&#12289;&#36866;&#24212;&#21644;&#25193;&#23637;&#24037;&#20316;&#27969;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#21487;&#20197;&#26377;&#25928;&#35299;&#37322;&#24037;&#20316;&#27969;&#65292;&#20294;&#22312;&#20132;&#25442;&#32452;&#20214;&#25110;&#26377;&#30446;&#30340;&#30340;&#24037;&#20316;&#26041;&#38754;&#24615;&#33021;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scientific workflow systems are increasingly popular for expressing and executing complex data analysis pipelines over large datasets, as they offer reproducibility, dependability, and scalability of analyses by automatic parallelization on large compute clusters. However, implementing workflows is difficult due to the involvement of many black-box tools and the deep infrastructure stack necessary for their execution. Simultaneously, user-supporting tools are rare, and the number of available examples is much lower than in classical programming languages. To address these challenges, we investigate the efficiency of Large Language Models (LLMs), specifically ChatGPT, to support users when dealing with scientific workflows. We performed three user studies in two scientific domains to evaluate ChatGPT for comprehending, adapting, and extending workflows. Our results indicate that LLMs efficiently interpret workflows but achieve lower performance for exchanging components or purposeful wo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#35777;&#25454;&#30340;&#31435;&#22330;&#25277;&#21462;&#32593;&#32476;&#65288;SEN&#65289;&#26469;&#26816;&#27979;&#19978;&#19979;&#25991;&#38169;&#35823;&#30340;&#35823;&#23548;&#20449;&#24687;&#12290;&#36890;&#36807;&#32771;&#34385;&#19981;&#21516;&#35777;&#25454;&#30340;&#31435;&#22330;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#20934;&#30830;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#20849;&#29616;&#20851;&#31995;&#30340;&#25903;&#25345;-&#21453;&#39539;&#20998;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20844;&#20849;&#22823;&#35268;&#27169;&#25968;&#25454;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01766</link><description>&lt;p&gt;
&#25903;&#25345;&#36824;&#26159;&#21453;&#39539;&#65306;&#20998;&#26512;&#35777;&#25454;&#31435;&#22330;&#20197;&#26816;&#27979;&#19978;&#19979;&#25991;&#38169;&#35823;&#30340;&#35823;&#23548;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Support or Refute: Analyzing the Stance of Evidence to Detect Out-of-Context Mis- and Disinformation. (arXiv:2311.01766v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#35777;&#25454;&#30340;&#31435;&#22330;&#25277;&#21462;&#32593;&#32476;&#65288;SEN&#65289;&#26469;&#26816;&#27979;&#19978;&#19979;&#25991;&#38169;&#35823;&#30340;&#35823;&#23548;&#20449;&#24687;&#12290;&#36890;&#36807;&#32771;&#34385;&#19981;&#21516;&#35777;&#25454;&#30340;&#31435;&#22330;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#20934;&#30830;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#20849;&#29616;&#20851;&#31995;&#30340;&#25903;&#25345;-&#21453;&#39539;&#20998;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20844;&#20849;&#22823;&#35268;&#27169;&#25968;&#25454;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#35823;&#23548;&#20449;&#24687;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#22269;&#23478;&#32423;&#30340;&#31038;&#20250;&#38382;&#39064;&#65292;&#26159;&#21508;&#31181;&#22312;&#32447;&#20260;&#23475;&#30340;&#20027;&#35201;&#26469;&#28304;&#20043;&#19968;&#12290;&#20854;&#20013;&#19968;&#31181;&#24120;&#35265;&#30340;&#35823;&#23548;&#20449;&#24687;&#24418;&#24335;&#26159;&#19978;&#19979;&#25991;&#38169;&#35823;&#65288;OOC&#65289;&#20449;&#24687;&#65292;&#20854;&#20013;&#19981;&#21516;&#30340;&#20449;&#24687;&#34987;&#38169;&#35823;&#22320;&#20851;&#32852;&#36215;&#26469;&#65292;&#20363;&#22914;&#30495;&#23454;&#22270;&#20687;&#19982;&#34394;&#20551;&#30340;&#25991;&#26412;&#26631;&#39064;&#25110;&#35823;&#23548;&#24615;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#23613;&#31649;&#19968;&#20123;&#30740;&#31350;&#35797;&#22270;&#36890;&#36807;&#22806;&#37096;&#35777;&#25454;&#26469;&#25269;&#24481;&#19978;&#19979;&#25991;&#38169;&#35823;&#30340;&#35823;&#23548;&#20449;&#24687;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#24573;&#35270;&#20102;&#19981;&#21516;&#31435;&#22330;&#30340;&#19981;&#21516;&#35777;&#25454;&#30340;&#20316;&#29992;&#12290;&#21463;&#21040;&#35777;&#25454;&#31435;&#22330;&#20195;&#34920;&#19981;&#21516;&#26816;&#27979;&#32467;&#26524;&#30340;&#20559;&#35265;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#32479;&#19968;&#26694;&#26550;&#20013;&#25552;&#21462;&#22810;&#27169;&#24577;&#35777;&#25454;&#30340;&#31435;&#22330;&#30340;&#31435;&#22330;&#25277;&#21462;&#32593;&#32476;&#65288;SEN&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#22522;&#20110;&#21629;&#21517;&#23454;&#20307;&#30340;&#20849;&#29616;&#20851;&#31995;&#35745;&#31639;&#30340;&#25903;&#25345;-&#21453;&#39539;&#20998;&#25968;&#21040;&#25991;&#26412;SEN&#20013;&#12290;&#23545;&#20844;&#20849;&#22823;&#35268;&#27169;&#25968;&#25454;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mis- and disinformation online have become a major societal problem as major sources of online harms of different kinds. One common form of mis- and disinformation is out-of-context (OOC) information, where different pieces of information are falsely associated, e.g., a real image combined with a false textual caption or a misleading textual description. Although some past studies have attempted to defend against OOC mis- and disinformation through external evidence, they tend to disregard the role of different pieces of evidence with different stances. Motivated by the intuition that the stance of evidence represents a bias towards different detection results, we propose a stance extraction network (SEN) that can extract the stances of different pieces of multi-modal evidence in a unified framework. Moreover, we introduce a support-refutation score calculated based on the co-occurrence relations of named entities into the textual SEN. Extensive experiments on a public large-scale data
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;PsyCoT&#30340;&#26032;&#39062;&#20010;&#24615;&#26816;&#27979;&#26041;&#27861;&#65292;&#23558;&#24515;&#29702;&#38382;&#21367;&#20316;&#20026;&#24605;&#32500;&#38142;&#26465;&#36827;&#34892;&#20010;&#24615;&#35782;&#21035;&#65292;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#22686;&#24378;&#23545;&#20010;&#24615;&#30340;&#21512;&#29702;&#25512;&#26029;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.20256</link><description>&lt;p&gt;
PsyCoT: &#23558;&#24515;&#29702;&#38382;&#21367;&#20316;&#20026;&#24378;&#22823;&#30340;&#24605;&#32500;&#38142;&#26465;&#29992;&#20110;&#20010;&#24615;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
PsyCoT: Psychological Questionnaire as Powerful Chain-of-Thought for Personality Detection. (arXiv:2310.20256v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20256
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;PsyCoT&#30340;&#26032;&#39062;&#20010;&#24615;&#26816;&#27979;&#26041;&#27861;&#65292;&#23558;&#24515;&#29702;&#38382;&#21367;&#20316;&#20026;&#24605;&#32500;&#38142;&#26465;&#36827;&#34892;&#20010;&#24615;&#35782;&#21035;&#65292;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#22686;&#24378;&#23545;&#20010;&#24615;&#30340;&#21512;&#29702;&#25512;&#26029;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#65292;&#22914;ChatGPT&#65292;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#20013;remarkable&#30340;&#38646;-shot&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;LLM&#22312;&#20010;&#24615;&#26816;&#27979;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#21363;&#36890;&#36807;&#20889;&#20316;&#25991;&#26412;&#26469;&#35782;&#21035;&#20010;&#20307;&#30340;&#20010;&#24615;&#65292;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#21463;&#21040;&#24515;&#29702;&#38382;&#21367;&#30340;&#21551;&#21457;&#65292;&#24515;&#29702;&#38382;&#21367;&#30001;&#24515;&#29702;&#23398;&#23478;&#31934;&#24515;&#35774;&#35745;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#26377;&#38024;&#23545;&#24615;&#30340;&#39033;&#30446;&#35780;&#20272;&#20010;&#20307;&#30340;&#20010;&#24615;&#29305;&#24449;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#39033;&#30446;&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#32452;&#33391;&#22909;&#32467;&#26500;&#21270;&#30340;&#24605;&#32500;&#38142;&#26465;&#36807;&#31243;&#12290;&#36890;&#36807;&#25972;&#21512;&#36825;&#20123;&#36807;&#31243;&#65292;LLM&#21487;&#20197;&#22686;&#24378;&#20854;&#20174;&#25991;&#26412;&#36755;&#20837;&#20013;&#23545;&#20010;&#24615;&#30340;&#21512;&#29702;&#25512;&#26029;&#33021;&#21147;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20010;&#24615;&#26816;&#27979;&#26041;&#27861;&#65292;&#31216;&#20026;PsyCoT&#65292;&#23427;&#27169;&#20223;&#20010;&#20307;&#20197;&#22810;&#36718;&#23545;&#35805;&#26041;&#24335;&#23436;&#25104;&#24515;&#29702;&#38382;&#21367;&#30340;&#26041;&#24335;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;LLM&#20316;&#20026;&#19968;&#20010;&#22312;&#25991;&#26412;&#22788;&#29702;&#26041;&#21521;&#19978;&#20855;&#26377;&#19987;&#38271;&#30340;AI&#21161;&#25163;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in large language models (LLMs), such as ChatGPT, have showcased remarkable zero-shot performance across various NLP tasks. However, the potential of LLMs in personality detection, which involves identifying an individual's personality from their written texts, remains largely unexplored. Drawing inspiration from Psychological Questionnaires, which are carefully designed by psychologists to evaluate individual personality traits through a series of targeted items, we argue that these items can be regarded as a collection of well-structured chain-of-thought (CoT) processes. By incorporating these processes, LLMs can enhance their capabilities to make more reasonable inferences on personality from textual input. In light of this, we propose a novel personality detection method, called PsyCoT, which mimics the way individuals complete psychological questionnaires in a multi-turn dialogue manner. In particular, we employ a LLM as an AI assistant with a specialization in tex
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#27979;&#35797;&#20102;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#35821;&#31181;&#24815;&#29992;&#35821;&#22659;&#20013;&#29983;&#25104;&#24310;&#32493;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#27169;&#22411;&#22312;&#23383;&#38754;&#21644;&#24815;&#29992;&#19978;&#19979;&#25991;&#20013;&#30340;&#34920;&#29616;&#30456;&#20284;&#65292;&#24182;&#19988;&#22312;&#20004;&#31181;&#35821;&#35328;&#20013;&#22343;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.20195</link><description>&lt;p&gt;
&#22312;&#22810;&#35821;&#31181;&#24815;&#29992;&#35821;&#22659;&#19979;&#29983;&#25104;&#24310;&#32493;
&lt;/p&gt;
&lt;p&gt;
Generating Continuations in Multilingual Idiomatic Contexts. (arXiv:2310.20195v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20195
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#27979;&#35797;&#20102;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#35821;&#31181;&#24815;&#29992;&#35821;&#22659;&#20013;&#29983;&#25104;&#24310;&#32493;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#27169;&#22411;&#22312;&#23383;&#38754;&#21644;&#24815;&#29992;&#19978;&#19979;&#25991;&#20013;&#30340;&#34920;&#29616;&#30456;&#20284;&#65292;&#24182;&#19988;&#22312;&#20004;&#31181;&#35821;&#35328;&#20013;&#22343;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#24815;&#29992;&#25110;&#23383;&#38754;&#22810;&#35789;&#34920;&#36798;&#26159;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#20309;&#35821;&#35328;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#20026;&#21253;&#21547;&#24815;&#29992;&#65288;&#25110;&#23383;&#38754;&#65289;&#34920;&#36798;&#30340;&#21465;&#36848;&#29983;&#25104;&#20855;&#26377;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#24310;&#32493;&#30340;&#20219;&#21153;&#21487;&#20197;&#35753;&#25105;&#20204;&#27979;&#35797;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#29702;&#35299;&#38750;&#32452;&#21512;&#24615;&#27604;&#21947;&#25991;&#26412;&#30340;&#32420;&#32454;&#35821;&#35328;&#33021;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#31181;&#19981;&#21516;&#35821;&#35328;&#65288;&#33521;&#35821;&#21644;&#33889;&#33796;&#29273;&#35821;&#65289;&#30340;&#25968;&#25454;&#38598;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;&#35757;&#32451;&#35774;&#32622;&#19979;&#65288;&#38646;&#26679;&#26412;&#12289;&#23569;&#26679;&#26412;&#21644;&#24494;&#35843;&#65289;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#22312;&#29983;&#25104;&#23383;&#38754;&#19978;&#19979;&#25991;&#30340;&#24310;&#32493;&#26102;&#30053;&#20248;&#20110;&#24815;&#29992;&#19978;&#19979;&#25991;&#65292;&#20294;&#24046;&#36317;&#24456;&#23567;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#20013;&#30740;&#31350;&#30340;&#27169;&#22411;&#22312;&#20004;&#31181;&#35821;&#35328;&#20013;&#34920;&#29616;&#20986;&#21516;&#26679;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#34920;&#26126;&#29983;&#25104;&#27169;&#22411;&#22312;&#25191;&#34892;&#27492;&#20219;&#21153;&#26102;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to process idiomatic or literal multiword expressions is a crucial aspect of understanding and generating any language. The task of generating contextually relevant continuations for narratives containing idiomatic (or literal) expressions can allow us to test the ability of generative language models (LMs) in understanding nuanced language containing non-compositional figurative text. We conduct a series of experiments using datasets in two distinct languages (English and Portuguese) under three different training settings (zero-shot, few-shot, and fine-tuned). Our results suggest that the models are only slightly better at generating continuations for literal contexts than idiomatic contexts, with exceedingly small margins. Furthermore, the models studied in this work perform equally well across both languages, indicating the robustness of generative models in performing this task.
&lt;/p&gt;</description></item><item><title>BioInstruct&#26159;&#19968;&#20010;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#38024;&#23545;&#24615;&#25351;&#20196;&#25968;&#25454;&#38598;BioInstruct&#65292;&#36890;&#36807;GPT-4&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31934;&#35843;&#65292;&#20248;&#21270;&#20102;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.19975</link><description>&lt;p&gt;
BioInstruct:&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25351;&#20196;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
BioInstruct: Instruction Tuning of Large Language Models for Biomedical Natural Language Processing. (arXiv:2310.19975v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19975
&lt;/p&gt;
&lt;p&gt;
BioInstruct&#26159;&#19968;&#20010;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#38024;&#23545;&#24615;&#25351;&#20196;&#25968;&#25454;&#38598;BioInstruct&#65292;&#36890;&#36807;GPT-4&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31934;&#35843;&#65292;&#20248;&#21270;&#20102;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#22312;&#22823;&#37327;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#36827;&#34892;&#29305;&#23450;&#39046;&#22495;&#30340;&#25351;&#20196;&#35843;&#25972;&#65292;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#21482;&#21457;&#34920;&#20102;&#24456;&#23569;&#30340;&#25351;&#20196;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;BioInstruct&#65292;&#36825;&#26159;&#19968;&#20010;&#23450;&#21046;&#30340;&#20219;&#21153;&#29305;&#23450;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#36229;&#36807;25,000&#20010;&#31034;&#20363;&#12290;&#36890;&#36807;&#20351;&#29992;&#19977;&#20010;&#20154;&#24037;&#31579;&#36873;&#30340;&#25351;&#20196;&#26679;&#26412;&#65292;&#20197;GPT-4&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25552;&#31034;&#65292;&#31934;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#26088;&#22312;&#20248;&#21270;&#20854;&#22312;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;LLaMA LLMs (1&amp;2,7B&amp;13B)&#36827;&#34892;&#20102;&#25351;&#20196;&#35843;&#25972;&#65292;&#24182;&#22312;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;&#20449;&#24687;&#25552;&#21462;&#12289;&#38382;&#31572;&#21644;&#25991;&#26412;&#29983;&#25104;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#25351;&#20196;&#22914;&#20309;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#36129;&#29486;&#65292;&#20351;&#29992;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) has achieved a great success in many natural language processing (NLP) tasks. This is achieved by pretraining of LLMs on vast amount of data and then instruction tuning to specific domains. However, only a few instructions in the biomedical domain have been published. To address this issue, we introduce BioInstruct, a customized task-specific instruction dataset containing more than 25,000 examples. This dataset was generated attractively by prompting a GPT-4 language model with a three-seed-sample of 80 human-curated instructions. By fine-tuning LLMs using the BioInstruct dataset, we aim to optimize the LLM's performance in biomedical natural language processing (BioNLP). We conducted instruction tuning on the LLaMA LLMs (1\&amp;2, 7B\&amp;13B) and evaluated them on BioNLP applications, including information extraction, question answering, and text generation. We also evaluated how instructions contributed to model performance using multi-tasking learning principl
&lt;/p&gt;</description></item><item><title>ArcheType&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24320;&#28304;&#21015;&#31867;&#22411;&#27880;&#37322;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#25913;&#36827;&#19978;&#19979;&#25991;&#37319;&#26679;&#21644;&#26631;&#31614;&#37325;&#26032;&#26144;&#23556;&#65292;&#23454;&#29616;&#20102;&#20840;&#38754;&#30340;&#38646;&#26679;&#26412;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.18208</link><description>&lt;p&gt;
ArcheType&#65306;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24320;&#28304;&#21015;&#31867;&#22411;&#27880;&#37322;&#30340;&#26032;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
ArcheType: A Novel Framework for Open-Source Column Type Annotation using Large Language Models. (arXiv:2310.18208v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18208
&lt;/p&gt;
&lt;p&gt;
ArcheType&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24320;&#28304;&#21015;&#31867;&#22411;&#27880;&#37322;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#25913;&#36827;&#19978;&#19979;&#25991;&#37319;&#26679;&#21644;&#26631;&#31614;&#37325;&#26032;&#26144;&#23556;&#65292;&#23454;&#29616;&#20102;&#20840;&#38754;&#30340;&#38646;&#26679;&#26412;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#35821;&#20041;&#21015;&#31867;&#22411;&#27880;&#37322;&#65288;CTA&#65289;&#26041;&#38754;&#23384;&#22312;&#37325;&#35201;&#32570;&#28857;&#65306;&#23427;&#20204;&#20381;&#36182;&#20110;&#22312;&#35757;&#32451;&#26102;&#22266;&#23450;&#30340;&#35821;&#20041;&#31867;&#22411;&#65307;&#38656;&#35201;&#22823;&#37327;&#30340;&#27599;&#20010;&#31867;&#22411;&#30340;&#35757;&#32451;&#26679;&#26412;&#24182;&#20135;&#29983;&#22823;&#37327;&#36816;&#34892;&#26102;&#25512;&#26029;&#25104;&#26412;&#65307;&#21363;&#20351;&#31867;&#22411;&#20445;&#25345;&#19981;&#21464;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#20063;&#21487;&#33021;&#22312;&#35780;&#20272;&#26032;&#25968;&#25454;&#38598;&#26102;&#19979;&#38477;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24191;&#27867;&#30340;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#24615;&#33021;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#23427;&#20204;&#22312;CTA&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;ArcheType&#65292;&#19968;&#31181;&#31616;&#21333;&#23454;&#29992;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#19978;&#19979;&#25991;&#37319;&#26679;&#12289;&#25552;&#31034;&#24207;&#21015;&#21270;&#12289;&#27169;&#22411;&#26597;&#35810;&#21644;&#26631;&#31614;&#37325;&#26032;&#26144;&#23556;&#65292;&#20174;&#32780;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23436;&#20840;&#20197;&#38646;&#26679;&#26412;&#26041;&#24335;&#35299;&#20915;&#21015;&#31867;&#22411;&#27880;&#37322;&#38382;&#39064;&#12290;&#25105;&#20204;&#20998;&#21035;&#23545;&#25105;&#20204;&#26041;&#27861;&#30340;&#27599;&#20010;&#32452;&#25104;&#37096;&#20998;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24182;&#30830;&#23450;&#20986;&#25913;&#36827;&#19978;&#19979;&#25991;&#37319;&#26679;&#21644;&#26631;&#31614;&#37325;&#26032;&#26144;&#23556;&#25552;&#20379;&#20102;&#26368;&#19968;&#33268;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing deep-learning approaches to semantic column type annotation (CTA) have important shortcomings: they rely on semantic types which are fixed at training time; require a large number of training samples per type and incur large run-time inference costs; and their performance can degrade when evaluated on novel datasets, even when types remain constant. Large language models have exhibited strong zero-shot classification performance on a wide range of tasks and in this paper we explore their use for CTA. We introduce ArcheType, a simple, practical method for context sampling, prompt serialization, model querying, and label remapping, which enables large language models to solve column type annotation problems in a fully zero-shot manner. We ablate each component of our method separately, and establish that improvements to context sampling and label remapping provide the most consistent gains. ArcheType establishes new state-of-the-art performance on both zero-shot and fine-tuned C
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Disentangled Graph-Text Learner (DGTL)&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#23450;&#21046;&#30340;&#35299;&#32544;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23618;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#25991;&#26412;&#23646;&#24615;&#22270;&#65288;TAGs&#65289;&#20013;&#30340;&#22797;&#26434;&#32467;&#26500;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.18152</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#23646;&#24615;&#22270;&#30340;&#35299;&#32544;&#34920;&#24449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Disentangled Representation Learning with Large Language Models for Text-Attributed Graphs. (arXiv:2310.18152v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Disentangled Graph-Text Learner (DGTL)&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#23450;&#21046;&#30340;&#35299;&#32544;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23618;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#25991;&#26412;&#23646;&#24615;&#22270;&#65288;TAGs&#65289;&#20013;&#30340;&#22797;&#26434;&#32467;&#26500;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#23646;&#24615;&#22270;&#65288;TAGs&#65289;&#22312;&#32593;&#32476;&#19978;&#38750;&#24120;&#24120;&#35265;&#65292;&#23545;&#20110;&#35813;&#31867;&#22270;&#65292;&#22914;&#24341;&#29992;&#32593;&#32476;&#12289;&#30005;&#23376;&#21830;&#21153;&#32593;&#32476;&#21644;&#31038;&#20132;&#32593;&#32476;&#30340;&#30740;&#31350;&#22312;&#32593;&#32476;&#31038;&#21306;&#20013;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#20165;&#20165;&#20381;&#38752;&#25552;&#31034;&#20449;&#24687;&#26469;&#20256;&#36798;&#22270;&#32467;&#26500;&#20449;&#24687;&#32473;LLMs&#65292;&#22240;&#27492;&#23545;&#20110;TAGs&#20013;&#22797;&#26434;&#30340;&#32467;&#26500;&#20851;&#31995;&#20102;&#35299;&#19981;&#36275;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#32544;&#22270;&#25991;&#23398;&#20064;&#22120;&#65288;DGTL&#65289;&#27169;&#22411;&#65292;&#33021;&#22815;&#22686;&#24378;LLMs&#23545;TAGs&#30340;&#25512;&#29702;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;DGTL&#27169;&#22411;&#36890;&#36807;&#23450;&#21046;&#30340;&#35299;&#32544;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23618;&#23558;&#22270;&#32467;&#26500;&#20449;&#24687;&#32435;&#20837;&#20854;&#20013;&#65292;&#20351;&#24471;LLMs&#33021;&#22815;&#25429;&#25417;&#22810;&#20010;&#32467;&#26500;&#22240;&#32032;&#20013;&#38544;&#34255;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-attributed graphs (TAGs) are prevalent on the web and research over TAGs such as citation networks, e-commerce networks and social networks has attracted considerable attention in the web community. Recently, large language models (LLMs) have demonstrated exceptional capabilities across a wide range of tasks. However, the existing works focus on harnessing the potential of LLMs solely relying on prompts to convey graph structure information to LLMs, thus suffering from insufficient understanding of the complex structural relationships within TAGs. To address this problem, in this paper we present the Disentangled Graph-Text Learner (DGTL) model, which is able to enhance the reasoning and predicting capabilities of LLMs for TAGs. Our proposed DGTL model incorporates graph structure information through tailored disentangled graph neural network (GNN) layers, enabling LLMs to capture the intricate relationships hidden in text-attributed graphs from multiple structural factors. Furthe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#30740;&#31350;&#20102;&#25512;&#25991;&#24773;&#32490;&#21160;&#24577;&#21644;&#24515;&#29702;&#20581;&#24247;&#38556;&#30861;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#25512;&#25991;&#24773;&#32490;&#21160;&#24577;&#19982;&#29992;&#25143;&#33258;&#25105;&#25259;&#38706;&#30340;&#35786;&#26029;&#26377;&#20851;&#65292;&#20026;&#24515;&#29702;&#20581;&#24247;&#30340;&#35780;&#20272;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.17369</link><description>&lt;p&gt;
&#35821;&#35328;&#21644;&#24515;&#29702;&#20581;&#24247;&#65306;&#20174;&#25991;&#26412;&#20013;&#27979;&#37327;&#24773;&#32490;&#21160;&#24577;&#20316;&#20026;&#35821;&#35328;&#29983;&#29289;&#31038;&#20250;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language and Mental Health: Measures of Emotion Dynamics from Text as Linguistic Biosocial Markers. (arXiv:2310.17369v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17369
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#30740;&#31350;&#20102;&#25512;&#25991;&#24773;&#32490;&#21160;&#24577;&#21644;&#24515;&#29702;&#20581;&#24247;&#38556;&#30861;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#25512;&#25991;&#24773;&#32490;&#21160;&#24577;&#19982;&#29992;&#25143;&#33258;&#25105;&#25259;&#38706;&#30340;&#35786;&#26029;&#26377;&#20851;&#65292;&#20026;&#24515;&#29702;&#20581;&#24247;&#30340;&#35780;&#20272;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#29702;&#30149;&#29702;&#23398;&#30740;&#31350;&#34920;&#26126;&#65292;&#24773;&#32490;&#38543;&#26102;&#38388;&#30340;&#21464;&#21270;&#27169;&#24335;&#8212;&#8212;&#24773;&#32490;&#21160;&#24577;&#8212;&#8212;&#26159;&#24515;&#29702;&#20581;&#24247;&#30340;&#25351;&#26631;&#12290;&#20256;&#32479;&#19978;&#65292;&#24773;&#32490;&#21464;&#21270;&#30340;&#27169;&#24335;&#26159;&#36890;&#36807;&#24773;&#32490;&#30340;&#33258;&#25105;&#25253;&#21578;&#26469;&#30830;&#23450;&#30340;&#65307;&#28982;&#32780;&#65292;&#24050;&#30693;&#23384;&#22312;&#20934;&#30830;&#24615;&#12289;&#20559;&#35265;&#21644;&#20415;&#21033;&#24615;&#31561;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#36890;&#36807;&#30740;&#31350;&#20010;&#20154;&#26085;&#24120;&#21457;&#35328;&#26469;&#30830;&#23450;&#24773;&#32490;&#21160;&#24577;&#65292;&#35299;&#20915;&#20102;&#35768;&#22810;&#36825;&#20123;&#38382;&#39064;&#65292;&#20294;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#36825;&#20123;&#21457;&#35328;&#24773;&#32490;&#21160;&#24577;&#30340;&#27979;&#37327;&#20540;&#26159;&#21542;&#19982;&#24515;&#29702;&#20581;&#24247;&#35786;&#26029;&#30456;&#20851;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#39318;&#27425;&#30740;&#31350;&#20102;&#25512;&#25991;&#24773;&#32490;&#21160;&#24577;&#19982;&#24515;&#29702;&#20581;&#24247;&#38556;&#30861;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25105;&#20204;&#30740;&#31350;&#30340;&#27599;&#20010;&#24773;&#32490;&#21160;&#24577;&#24230;&#37327;&#20540;&#37117;&#22240;&#29992;&#25143;&#33258;&#25105;&#25259;&#38706;&#30340;&#35786;&#26029;&#32780;&#26377;&#25152;&#21464;&#21270;&#12290;&#20363;&#22914;&#65292;&#24179;&#22343;&#24773;&#32490;&#20215;&#20540;&#36739;&#39640;&#65288;&#21363;&#25991;&#26412;&#36739;&#31215;&#26497;&#65289;&#30340;&#25511;&#21046;&#32452;&#19982;&#24739;&#26377;&#27880;&#24847;&#21147;&#32570;&#38519;&#22810;&#21160;&#38556;&#30861;&#65288;ADHD&#65289;&#12289;&#25233;&#37057;&#30151;&#65288;MDD&#65289;&#21644;&#21019;&#20260;&#21518;&#24212;&#28608;&#38556;&#30861;&#65288;PTSD&#65289;&#30340;&#29992;&#25143;&#30456;&#27604;&#26174;&#33879;&#36739;&#39640;&#12290;&#24773;&#32490;&#20215;&#20540;&#21464;&#24322;&#24615;&#22312;&#25511;&#21046;&#32452;&#20013;&#26174;&#33879;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research in psychopathology has shown that, at an aggregate level, the patterns of emotional change over time -- emotion dynamics -- are indicators of one's mental health. One's patterns of emotion change have traditionally been determined through self-reports of emotions; however, there are known issues with accuracy, bias, and convenience. Recent approaches to determining emotion dynamics from one's everyday utterances, addresses many of these concerns, but it is not yet known whether these measures of utterance emotion dynamics (UED) correlate with mental health diagnoses. Here, for the first time, we study the relationship between tweet emotion dynamics and mental health disorders. We find that each of the UED metrics studied varied by the user's self-disclosed diagnosis. For example: average valence was significantly higher (i.e., more positive text) in the control group compared to users with ADHD, MDD, and PTSD. Valence variability was significantly lower in the control group co
&lt;/p&gt;</description></item><item><title>GlotLID-M&#26159;&#19968;&#20010;&#28385;&#36275;&#24191;&#27867;&#35206;&#30422;&#12289;&#21487;&#38752;&#24615;&#21644;&#25928;&#29575;&#35201;&#27714;&#30340;&#35821;&#35328;&#35782;&#21035;&#27169;&#22411;&#65292;&#20855;&#26377;1665&#20010;&#21487;&#35782;&#21035;&#35821;&#35328;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#23427;&#35299;&#20915;&#20102;&#20302;&#36164;&#28304;LID&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#26377;&#26395;&#25552;&#39640;&#25968;&#25454;&#38598;&#36136;&#37327;&#21644;&#22686;&#24378;&#35775;&#38382;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.16248</link><description>&lt;p&gt;
GlotLID: &#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#35821;&#35328;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
GlotLID: Language Identification for Low-Resource Languages. (arXiv:2310.16248v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16248
&lt;/p&gt;
&lt;p&gt;
GlotLID-M&#26159;&#19968;&#20010;&#28385;&#36275;&#24191;&#27867;&#35206;&#30422;&#12289;&#21487;&#38752;&#24615;&#21644;&#25928;&#29575;&#35201;&#27714;&#30340;&#35821;&#35328;&#35782;&#21035;&#27169;&#22411;&#65292;&#20855;&#26377;1665&#20010;&#21487;&#35782;&#21035;&#35821;&#35328;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#23427;&#35299;&#20915;&#20102;&#20302;&#36164;&#28304;LID&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#26377;&#26395;&#25552;&#39640;&#25968;&#25454;&#38598;&#36136;&#37327;&#21644;&#22686;&#24378;&#35775;&#38382;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#26377;&#20960;&#31687;&#35770;&#25991;&#21457;&#34920;&#20102;&#38024;&#23545;&#32422;300&#31181;&#39640;&#36164;&#28304;&#21644;&#20013;&#36164;&#28304;&#35821;&#35328;&#30340;&#35821;&#35328;&#35782;&#21035;&#65288;LID&#65289;&#30340;&#33391;&#22909;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27809;&#26377;&#21487;&#29992;&#30340;LID&#28385;&#36275;&#20197;&#19979;&#35201;&#27714;&#65306;&#65288;i&#65289;&#28085;&#30422;&#24191;&#27867;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#65288;ii&#65289;&#32463;&#36807;&#20005;&#26684;&#35780;&#20272;&#19988;&#21487;&#38752;&#65292;&#65288;iii&#65289;&#39640;&#25928;&#26131;&#29992;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;GlotLID-M&#65292;&#19968;&#20010;&#28385;&#36275;&#24191;&#27867;&#35206;&#30422;&#12289;&#21487;&#38752;&#24615;&#21644;&#25928;&#29575;&#35201;&#27714;&#30340;LID&#27169;&#22411;&#12290;&#23427;&#21487;&#20197;&#35782;&#21035;1665&#31181;&#35821;&#35328;&#65292;&#22312;&#35206;&#30422;&#33539;&#22260;&#19978;&#30456;&#27604;&#20043;&#21069;&#30340;&#24037;&#20316;&#26377;&#20102;&#22823;&#24133;&#22686;&#21152;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;GlotLID-M&#22312;&#24179;&#34913;F1&#20998;&#25968;&#21644;&#20551;&#38451;&#24615;&#29575;&#65288;FPR&#65289;&#26041;&#38754;&#20248;&#20110;&#22235;&#20010;&#22522;&#20934;&#27169;&#22411;&#65288;CLD3&#65292;FT176&#65292;OpenLID&#21644;NLLB&#65289;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20302;&#36164;&#28304;LID&#38754;&#20020;&#30340;&#29420;&#29305;&#25361;&#25112;&#65306;&#19981;&#27491;&#30830;&#30340;&#35821;&#26009;&#24211;&#20803;&#25968;&#25454;&#65292;&#26469;&#33258;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#27844;&#28431;&#65292;&#38590;&#20197;&#21306;&#20998;&#23494;&#20999;&#30456;&#20851;&#30340;&#35821;&#35328;&#65292;&#22788;&#29702;&#23439;&#35821;&#35328;&#19982;&#26041;&#35328;&#65292;&#20197;&#21450;&#19968;&#33324;&#30340;&#22122;&#22768;&#25968;&#25454;&#12290;&#25105;&#20204;&#24076;&#26395;&#23558;GlotLID-M&#38598;&#25104;&#21040;&#25968;&#25454;&#38598;&#21019;&#24314;&#27969;&#31243;&#20013;&#65292;&#20197;&#25552;&#39640;&#36136;&#37327;&#21644;&#22686;&#24378;&#35775;&#38382;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several recent papers have published good solutions for language identification (LID) for about 300 high-resource and medium-resource languages. However, there is no LID available that (i) covers a wide range of low-resource languages, (ii) is rigorously evaluated and reliable and (iii) efficient and easy to use. Here, we publish GlotLID-M, an LID model that satisfies the desiderata of wide coverage, reliability and efficiency. It identifies 1665 languages, a large increase in coverage compared to prior work. In our experiments, GlotLID-M outperforms four baselines (CLD3, FT176, OpenLID and NLLB) when balancing F1 and false positive rate (FPR). We analyze the unique challenges that low-resource LID poses: incorrect corpus metadata, leakage from high-resource languages, difficulty separating closely related languages, handling of macrolanguage vs varieties and in general noisy data. We hope that integrating GlotLID-M into dataset creation pipelines will improve quality and enhance acces
&lt;/p&gt;</description></item><item><title>TRAMS&#26159;&#19968;&#31181;&#35757;&#32451;&#20813;&#36153;&#30340;&#38271;&#31243;&#35821;&#35328;&#24314;&#27169;&#35760;&#24518;&#36873;&#25321;&#31574;&#30053;&#65292;&#23427;&#33021;&#22815;&#25552;&#39640;Transformer&#26550;&#26500;&#22312;&#38271;&#31243;&#35821;&#35328;&#24314;&#27169;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#25110;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2310.15494</link><description>&lt;p&gt;
TRAMS:&#35757;&#32451;&#20813;&#36153;&#30340;&#38271;&#31243;&#35821;&#35328;&#24314;&#27169;&#35760;&#24518;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
TRAMS: Training-free Memory Selection for Long-range Language Modeling. (arXiv:2310.15494v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15494
&lt;/p&gt;
&lt;p&gt;
TRAMS&#26159;&#19968;&#31181;&#35757;&#32451;&#20813;&#36153;&#30340;&#38271;&#31243;&#35821;&#35328;&#24314;&#27169;&#35760;&#24518;&#36873;&#25321;&#31574;&#30053;&#65292;&#23427;&#33021;&#22815;&#25552;&#39640;Transformer&#26550;&#26500;&#22312;&#38271;&#31243;&#35821;&#35328;&#24314;&#27169;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#25110;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26550;&#26500;&#23545;&#20110;&#20247;&#22810;AI&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#22312;&#38271;&#31243;&#35821;&#35328;&#24314;&#27169;&#26041;&#38754;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;&#23613;&#31649;&#24050;&#32463;&#35774;&#35745;&#20102;&#20960;&#31181;&#29305;&#23450;&#30340;Transformer&#26550;&#26500;&#26469;&#35299;&#20915;&#38271;&#31243;&#20381;&#36182;&#30340;&#38382;&#39064;&#65292;&#20294;&#29616;&#26377;&#30340;&#26041;&#27861;&#22914;Transformer-XL&#23384;&#22312;&#22823;&#37327;&#26080;&#25928;&#35760;&#24518;&#30340;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21363;&#25554;&#21363;&#29992;&#30340;&#31574;&#30053;&#65292;&#31216;&#20026;TRAining-free Memory Selection&#65288;TRAMS&#65289;&#65292;&#23427;&#26681;&#25454;&#19968;&#20010;&#31616;&#21333;&#30340;&#25351;&#26631;&#36873;&#25321;&#21442;&#19982;&#27880;&#24847;&#21147;&#35745;&#31639;&#30340;&#26631;&#35760;&#12290;&#35813;&#31574;&#30053;&#20801;&#35768;&#25105;&#20204;&#20445;&#30041;&#19982;&#24403;&#21069;&#26597;&#35810;&#20855;&#26377;&#39640;&#20851;&#27880;&#20998;&#25968;&#21487;&#33021;&#24615;&#30340;&#26631;&#35760;&#65292;&#24182;&#24573;&#30053;&#20854;&#20182;&#26631;&#35760;&#12290;&#25105;&#20204;&#22312;&#21333;&#35789;&#32423;&#22522;&#20934;&#65288;WikiText-103&#65289;&#21644;&#23383;&#31526;&#32423;&#22522;&#20934;&#65288;enwik8&#65289;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#19981;&#36827;&#34892;&#39069;&#22806;&#35757;&#32451;&#25110;&#28155;&#21152;&#39069;&#22806;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Transformer architecture is crucial for numerous AI models, but it still faces challenges in long-range language modeling. Though several specific transformer architectures have been designed to tackle issues of long-range dependencies, existing methods like Transformer-XL are plagued by a high percentage of ineffective memories. In this study, we present a plug-and-play strategy, known as TRAining-free Memory Selection (TRAMS), that selects tokens participating in attention calculation based on one simple metric. This strategy allows us to keep tokens that are likely to have a high attention score with the current queries and ignore the other ones. We have tested our approach on the word-level benchmark (WikiText-103) and the character-level benchmark (enwik8), and the results indicate an improvement without having additional training or adding additional parameters.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;REIGN&#65292;&#36890;&#36807;&#29983;&#25104;&#35757;&#32451;&#38382;&#39064;&#30340;&#25913;&#20889;&#65292;&#24182;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#25351;&#23548;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#65292;&#22686;&#21152;&#27169;&#22411;&#23545;&#34920;&#38754;&#24418;&#24335;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#22312;&#19981;&#21516;&#30340;&#22522;&#20934;&#19978;&#36827;&#34892;&#38646;-shot&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.13505</link><description>&lt;p&gt;
&#20855;&#26377;&#24378;&#21270;&#25913;&#20889;&#29983;&#25104;&#30340;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#30340;&#40065;&#26834;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Robust Training for Conversational Question Answering Models with Reinforced Reformulation Generation. (arXiv:2310.13505v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13505
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;REIGN&#65292;&#36890;&#36807;&#29983;&#25104;&#35757;&#32451;&#38382;&#39064;&#30340;&#25913;&#20889;&#65292;&#24182;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#25351;&#23548;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#65292;&#22686;&#21152;&#27169;&#22411;&#23545;&#34920;&#38754;&#24418;&#24335;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#22312;&#19981;&#21516;&#30340;&#22522;&#20934;&#19978;&#36827;&#34892;&#38646;-shot&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#19978;&#30340;&#23545;&#35805;&#38382;&#31572;&#65288;ConvQA&#65289;&#27169;&#22411;&#36890;&#24120;&#22312;&#40644;&#37329;QA&#23545;&#30340;&#22522;&#20934;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;&#36825;&#24847;&#21619;&#30528;&#35757;&#32451;&#20165;&#38480;&#20110;&#22312;&#30456;&#24212;&#25968;&#25454;&#38598;&#20013;&#35265;&#21040;&#30340;&#34920;&#38754;&#24418;&#24335;&#65292;&#35780;&#20272;&#20165;&#38024;&#23545;&#19968;&#23567;&#37096;&#20998;&#38382;&#39064;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#25552;&#20986;&#30340;&#26694;&#26550;REIGN&#65292;&#25105;&#20204;&#37319;&#21462;&#20102;&#20960;&#20010;&#27493;&#39588;&#26469;&#35299;&#20915;&#36825;&#20010;&#21463;&#38480;&#30340;&#23398;&#20064;&#35774;&#32622;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#29983;&#25104;&#35757;&#32451;&#38382;&#39064;&#30340;&#25913;&#20889;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#23545;&#34920;&#38754;&#24418;&#24335;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#12290;&#36825;&#26159;&#19968;&#20010;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#36825;&#20123;&#38382;&#39064;&#30340;&#19981;&#23436;&#25972;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23558;ConvQA&#27169;&#22411;&#24341;&#23548;&#21040;&#26356;&#39640;&#30340;&#24615;&#33021;&#65292;&#21482;&#25552;&#20379;&#37027;&#20123;&#26377;&#21161;&#20110;&#25552;&#39640;&#22238;&#31572;&#36136;&#37327;&#30340;&#25913;&#20889;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#19968;&#20010;&#22522;&#20934;&#19978;&#35757;&#32451;&#20027;&#35201;&#27169;&#22411;&#32452;&#20214;&#24182;&#23558;&#20854;&#38646;-shot&#24212;&#29992;&#20110;&#21478;&#19968;&#20010;&#30340;&#21487;&#34892;&#24615;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#20005;&#26684;&#35780;&#20272;&#65292;&#25105;&#20204;&#20351;&#29992;&#21644;&#37325;&#26032;&#37197;&#32622;&#21021;&#22987;&#30340;&#25913;&#20889;&#12289;&#27979;&#35797;&#35821;&#26009;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models for conversational question answering (ConvQA) over knowledge graphs (KGs) are usually trained and tested on benchmarks of gold QA pairs. This implies that training is limited to surface forms seen in the respective datasets, and evaluation is on a small set of held-out questions. Through our proposed framework REIGN, we take several steps to remedy this restricted learning setup. First, we systematically generate reformulations of training questions to increase robustness of models to surface form variations. This is a particularly challenging problem, given the incomplete nature of such questions. Second, we guide ConvQA models towards higher performance by feeding it only those reformulations that help improve their answering quality, using deep reinforcement learning. Third, we demonstrate the viability of training major model components on one benchmark and applying them zero-shot to another. Finally, for a rigorous evaluation of robustness for trained models, we use and re
&lt;/p&gt;</description></item><item><title>VIBE&#26159;&#19968;&#31181;&#35299;&#20915;Twitter&#20998;&#31867;&#20013;&#35821;&#35328;&#29305;&#24449;&#28436;&#21464;&#38382;&#39064;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24314;&#27169;&#28508;&#22312;&#20027;&#39064;&#28436;&#21464;&#20197;&#36866;&#24212;&#21160;&#24577;&#29615;&#22659;&#65292;&#24182;&#19988;&#22312;&#22823;&#35268;&#27169;Twitter&#23454;&#39564;&#20013;&#23637;&#29616;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.10191</link><description>&lt;p&gt;
VIBE&#65306;Twitter&#20998;&#31867;&#30340;&#20027;&#39064;&#39537;&#21160;&#26102;&#38388;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
VIBE: Topic-Driven Temporal Adaptation for Twitter Classification. (arXiv:2310.10191v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10191
&lt;/p&gt;
&lt;p&gt;
VIBE&#26159;&#19968;&#31181;&#35299;&#20915;Twitter&#20998;&#31867;&#20013;&#35821;&#35328;&#29305;&#24449;&#28436;&#21464;&#38382;&#39064;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24314;&#27169;&#28508;&#22312;&#20027;&#39064;&#28436;&#21464;&#20197;&#36866;&#24212;&#21160;&#24577;&#29615;&#22659;&#65292;&#24182;&#19988;&#22312;&#22823;&#35268;&#27169;Twitter&#23454;&#39564;&#20013;&#23637;&#29616;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#29305;&#24449;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#31038;&#20132;&#23186;&#20307;&#20013;&#19981;&#26029;&#21464;&#21270;&#65292;&#23548;&#33268;&#25991;&#26412;&#20998;&#31867;&#22312;&#21160;&#24577;&#29615;&#22659;&#19979;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26102;&#38388;&#33258;&#36866;&#24212;&#65292;&#21363;&#22312;&#36807;&#21435;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#26410;&#26469;&#36827;&#34892;&#27979;&#35797;&#12290;&#20808;&#21069;&#30340;&#22823;&#37096;&#20998;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#32487;&#32493;&#39044;&#35757;&#32451;&#25110;&#30693;&#35782;&#26356;&#26032;&#19978;&#65292;&#36825;&#21487;&#33021;&#20250;&#24433;&#21709;&#23427;&#20204;&#22312;&#22122;&#22768;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36890;&#36807;&#24314;&#27169;&#28508;&#22312;&#20027;&#39064;&#28436;&#21464;&#26469;&#21453;&#26144;&#29305;&#24449;&#21464;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;VIBE&#65306;Evolutions&#30340;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#20004;&#20010;&#20449;&#24687;&#29942;&#39048;(Bottleneck)&#27491;&#21017;&#21270;&#22120;&#26469;&#21306;&#20998;&#36807;&#21435;&#21644;&#26410;&#26469;&#30340;&#20027;&#39064;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#21306;&#20998;&#30340;&#20027;&#39064;&#36890;&#36807;&#26102;&#38388;&#25139;&#21644;&#31867;&#21035;&#26631;&#31614;&#39044;&#27979;&#36827;&#34892;&#22810;&#20219;&#21153;&#35757;&#32451;&#65292;&#20316;&#20026;&#33258;&#36866;&#24212;&#29305;&#24449;&#12290;&#22312;&#33258;&#36866;&#24212;&#23398;&#20064;&#36807;&#31243;&#20013;&#65292;VIBE&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#26102;&#38388;&#20043;&#21518;&#21019;&#24314;&#30340;&#22312;&#32447;&#27969;&#31243;&#20013;&#26816;&#32034;&#21040;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#12290;&#22312;&#19977;&#20010;&#20998;&#31867;&#20219;&#21153;&#30340;&#22823;&#35268;&#27169;Twitter&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language features are evolving in real-world social media, resulting in the deteriorating performance of text classification in dynamics. To address this challenge, we study temporal adaptation, where models trained on past data are tested in the future. Most prior work focused on continued pretraining or knowledge updating, which may compromise their performance on noisy social media data. To tackle this issue, we reflect feature change via modeling latent topic evolution and propose a novel model, VIBE: Variational Information Bottleneck for Evolutions. Concretely, we first employ two Information Bottleneck (IB) regularizers to distinguish past and future topics. Then, the distinguished topics work as adaptive features via multi-task training with timestamp and class label prediction. In adaptive learning, VIBE utilizes retrieved unlabeled data from online streams created posterior to training data time. Substantial Twitter experiments on three classification tasks show that our mode
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#24212;&#29992;&#8220;&#19987;&#23478;&#30340;&#20056;&#31215;&#8221;&#65288;PoE&#65289;&#25216;&#26415;&#26469;&#20943;&#36731;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38271;&#24230;&#20559;&#24046;&#38382;&#39064;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#20027;&#35201;&#30340;&#19987;&#23478;&#20851;&#27880;&#29702;&#35299;&#20154;&#31867;&#24847;&#22270;&#65292;&#32780;&#20559;&#35265;&#19987;&#23478;&#21017;&#33268;&#21147;&#20110;&#35782;&#21035;&#21644;&#25429;&#25417;&#38271;&#24230;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2310.05199</link><description>&lt;p&gt;
&#23485;&#26494;&#30340;&#22068;&#21767;&#20250;&#20351;&#33337;&#27785;&#27809;&#65306;&#20943;&#36731;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38271;&#24230;&#20559;&#24046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Loose lips sink ships: Mitigating Length Bias in Reinforcement Learning from Human Feedback. (arXiv:2310.05199v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05199
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#24212;&#29992;&#8220;&#19987;&#23478;&#30340;&#20056;&#31215;&#8221;&#65288;PoE&#65289;&#25216;&#26415;&#26469;&#20943;&#36731;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38271;&#24230;&#20559;&#24046;&#38382;&#39064;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#20027;&#35201;&#30340;&#19987;&#23478;&#20851;&#27880;&#29702;&#35299;&#20154;&#31867;&#24847;&#22270;&#65292;&#32780;&#20559;&#35265;&#19987;&#23478;&#21017;&#33268;&#21147;&#20110;&#35782;&#21035;&#21644;&#25429;&#25417;&#38271;&#24230;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#26159;&#37325;&#35201;&#30340;&#26725;&#26753;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#21644;&#31038;&#20250;&#20215;&#20540;&#35266;&#23545;&#40784;&#12290;&#36825;&#31181;&#23545;&#40784;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#31867;&#21453;&#39304;&#35821;&#26009;&#24211;&#26469;&#23398;&#20064;&#22870;&#21169;&#27169;&#22411;&#65292;&#28982;&#21518;&#29992;&#20110;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#22870;&#21169;&#27169;&#22411;&#24120;&#24120;&#20250;&#25214;&#21040;&#32469;&#36807;&#39044;&#26399;&#30446;&#26631;&#30340;&#25463;&#24452;&#65292;&#38169;&#35823;&#22320;&#20551;&#35774;&#20154;&#31867;&#26356;&#21916;&#27426;&#36739;&#38271;&#30340;&#22238;&#31572;&#12290;&#38271;&#24230;&#20559;&#24046;&#30340;&#20986;&#29616;&#24120;&#24120;&#20250;&#23548;&#33268;&#27169;&#22411;&#20542;&#21521;&#20110;&#36739;&#38271;&#30340;&#36755;&#20986;&#65292;&#20294;&#24182;&#19981;&#24847;&#21619;&#30528;&#36825;&#20123;&#36755;&#20986;&#20013;&#26377;&#26356;&#22810;&#26377;&#29992;&#30340;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24212;&#29992;&#20102;&#8220;&#19987;&#23478;&#30340;&#20056;&#31215;&#8221;&#65288;PoE&#65289;&#25216;&#26415;&#26469;&#23558;&#22870;&#21169;&#24314;&#27169;&#19982;&#24207;&#21015;&#38271;&#24230;&#30340;&#24433;&#21709;&#20998;&#31163;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#20027;&#35201;&#30340;&#19987;&#23478;&#20851;&#27880;&#29702;&#35299;&#20154;&#31867;&#24847;&#22270;&#65292;&#32780;&#20559;&#35265;&#19987;&#23478;&#21017;&#33268;&#21147;&#20110;&#35782;&#21035;&#21644;&#25429;&#25417;&#38271;&#24230;&#20559;&#24046;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378;&#20559;&#35265;&#30340;&#23398;&#20064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25200;&#21160;&#36827;&#20837;&#20559;&#24046;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning from human feedback serves as a crucial bridge, aligning large language models with human and societal values. This alignment requires a vast corpus of human feedback to learn a reward model, which is subsequently used to finetune language models. However, we have identified that the reward model often finds shortcuts to bypass its intended objectives, misleadingly assuming that humans prefer longer responses. The emergence of length bias often induces the model to favor longer outputs, yet it doesn't equate to an increase in helpful information within these outputs. In this paper, we propose an innovative solution, applying the Product-of-Experts (PoE) technique to separate reward modeling from the influence of sequence length. In our framework, the main expert concentrates on understanding human intents, while the biased expert targets the identification and capture of length bias. To further enhance the learning of bias, we introduce perturbations into the bia
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#26816;&#32034;&#22686;&#24378;&#22411;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#25552;&#21319;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#30340;&#25928;&#26524;&#65292;&#24182;&#35299;&#20915;&#20102;&#20256;&#32479;&#27169;&#22411;&#22312;&#21442;&#25968;&#35268;&#27169;&#21644;&#35757;&#32451;&#25968;&#25454;&#33539;&#22260;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.04027</link><description>&lt;p&gt;
&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#21319;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Enhancing Financial Sentiment Analysis via Retrieval Augmented Large Language Models. (arXiv:2310.04027v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#26816;&#32034;&#22686;&#24378;&#22411;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#25552;&#21319;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#30340;&#25928;&#26524;&#65292;&#24182;&#35299;&#20915;&#20102;&#20256;&#32479;&#27169;&#22411;&#22312;&#21442;&#25968;&#35268;&#27169;&#21644;&#35757;&#32451;&#25968;&#25454;&#33539;&#22260;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#23545;&#20110;&#20272;&#20540;&#21644;&#25237;&#36164;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#21463;&#20854;&#21442;&#25968;&#35268;&#27169;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#33539;&#22260;&#30340;&#38480;&#21046;&#65292;&#20854;&#27867;&#21270;&#33021;&#21147;&#21644;&#22312;&#35813;&#39046;&#22495;&#30340;&#26377;&#25928;&#24615;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#26368;&#36817;&#65292;&#20197;&#24191;&#27867;&#35821;&#26009;&#24211;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30001;&#20110;&#20854;&#20196;&#20154;&#31216;&#36190;&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#65292;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#23558;LLMs&#24212;&#29992;&#20110;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#23384;&#22312;&#25361;&#25112;&#65306;LLMs&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#19982;&#24773;&#24863;&#26631;&#31614;&#39044;&#27979;&#20043;&#38388;&#30340;&#24046;&#24322;&#21487;&#33021;&#20250; compromise&#20854;&#39044;&#27979;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#37329;&#34701;&#26032;&#38395;&#30340;&#31616;&#27905;&#24615;&#65292;&#24120;&#24120;&#32570;&#20047;&#36275;&#22815;&#30340;&#19978;&#19979;&#25991;&#65292;&#20063;&#21487;&#33021;&#20250;&#26174;&#33879;&#38477;&#20302;LLMs&#30340;&#24773;&#24863;&#20998;&#26512;&#21487;&#38752;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#30340;&#26816;&#32034;&#22686;&#24378;&#22411;LLMs&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Financial sentiment analysis is critical for valuation and investment decision-making. Traditional NLP models, however, are limited by their parameter size and the scope of their training datasets, which hampers their generalization capabilities and effectiveness in this field. Recently, Large Language Models (LLMs) pre-trained on extensive corpora have demonstrated superior performance across various NLP tasks due to their commendable zero-shot abilities. Yet, directly applying LLMs to financial sentiment analysis presents challenges: The discrepancy between the pre-training objective of LLMs and predicting the sentiment label can compromise their predictive performance. Furthermore, the succinct nature of financial news, often devoid of sufficient context, can significantly diminish the reliability of LLMs' sentiment analysis. To address these challenges, we introduce a retrieval-augmented LLMs framework for financial sentiment analysis. This framework includes an instruction-tuned L
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#24494;&#35843;&#26041;&#27861;RA-DIT&#65292;&#36890;&#36807;&#20026;&#20219;&#20309;&#35821;&#35328;&#27169;&#22411;&#28155;&#21152;&#26816;&#32034;&#33021;&#21147;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#20998;&#20026;&#20004;&#20010;&#27493;&#39588;&#65306;&#19968;&#26159;&#26356;&#26032;&#35821;&#35328;&#27169;&#22411;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#26816;&#32034;&#21040;&#30340;&#20449;&#24687;&#65292;&#20108;&#26159;&#26356;&#26032;&#26816;&#32034;&#22120;&#20197;&#36820;&#22238;&#26356;&#30456;&#20851;&#30340;&#32467;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#27599;&#20010;&#27493;&#39588;&#37117;&#33021;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#21516;&#26102;&#20351;&#29992;&#20004;&#20010;&#27493;&#39588;&#21487;&#20197;&#33719;&#24471;&#39069;&#22806;&#30340;&#25910;&#30410;&#12290;</title><link>http://arxiv.org/abs/2310.01352</link><description>&lt;p&gt;
RA-DIT: &#26816;&#32034;&#22686;&#24378;&#30340;&#21452;&#37325;&#25351;&#20196;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
RA-DIT: Retrieval-Augmented Dual Instruction Tuning. (arXiv:2310.01352v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01352
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#24494;&#35843;&#26041;&#27861;RA-DIT&#65292;&#36890;&#36807;&#20026;&#20219;&#20309;&#35821;&#35328;&#27169;&#22411;&#28155;&#21152;&#26816;&#32034;&#33021;&#21147;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#20998;&#20026;&#20004;&#20010;&#27493;&#39588;&#65306;&#19968;&#26159;&#26356;&#26032;&#35821;&#35328;&#27169;&#22411;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#26816;&#32034;&#21040;&#30340;&#20449;&#24687;&#65292;&#20108;&#26159;&#26356;&#26032;&#26816;&#32034;&#22120;&#20197;&#36820;&#22238;&#26356;&#30456;&#20851;&#30340;&#32467;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#27599;&#20010;&#27493;&#39588;&#37117;&#33021;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#21516;&#26102;&#20351;&#29992;&#20004;&#20010;&#27493;&#39588;&#21487;&#20197;&#33719;&#24471;&#39069;&#22806;&#30340;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65288;RALMs&#65289;&#36890;&#36807;&#35775;&#38382;&#22806;&#37096;&#25968;&#25454;&#23384;&#20648;&#20013;&#30340;&#38271;&#23614;&#21644;&#26368;&#26032;&#30693;&#35782;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#20294;&#26500;&#24314;&#36215;&#26469;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#26114;&#36149;&#30340;&#26816;&#32034;&#29305;&#23450;&#20462;&#25913;&#26469;&#36827;&#34892;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#65292;&#35201;&#20040;&#20351;&#29992;&#20107;&#21518;&#38598;&#25104;&#25968;&#25454;&#23384;&#20648;&#30340;&#26041;&#27861;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#29702;&#24819;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#24494;&#35843;&#26041;&#27861;&#8212;&#8212;&#26816;&#32034;&#22686;&#24378;&#30340;&#21452;&#37325;&#25351;&#20196;&#35843;&#20248;&#65288;RA-DIT&#65289;&#65292;&#36890;&#36807;&#20026;&#20219;&#20309;&#35821;&#35328;&#27169;&#22411;&#28155;&#21152;&#26816;&#32034;&#33021;&#21147;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20998;&#20026;&#20004;&#20010;&#19981;&#21516;&#30340;&#24494;&#35843;&#27493;&#39588;&#65306;&#65288;1&#65289;&#19968;&#20010;&#26356;&#26032;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#26816;&#32034;&#21040;&#30340;&#20449;&#24687;&#65292;&#65288;2&#65289;&#21478;&#19968;&#20010;&#26356;&#26032;&#26816;&#32034;&#22120;&#20197;&#36820;&#22238;&#26356;&#30456;&#20851;&#30340;&#32467;&#26524;&#65292;&#31526;&#21512;&#35821;&#35328;&#27169;&#22411;&#30340;&#20559;&#22909;&#12290;&#36890;&#36807;&#22312;&#38656;&#35201;&#30693;&#35782;&#21033;&#29992;&#21644;&#19978;&#19979;&#25991;&#24847;&#35782;&#30340;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#27599;&#20010;&#38454;&#27573;&#37117;&#33021;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#19988;&#21516;&#26102;&#20351;&#29992;&#20004;&#20010;&#38454;&#27573;&#21487;&#20197;&#33719;&#24471;&#39069;&#22806;&#30340;&#25910;&#30410;&#12290;&#25105;&#20204;&#30340;&#26368;&#20339;&#27169;&#22411;&#26159;RA-DIT 65B&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrieval-augmented language models (RALMs) improve performance by accessing long-tail and up-to-date knowledge from external data stores, but are challenging to build. Existing approaches require either expensive retrieval-specific modifications to LM pre-training or use post-hoc integration of the data store that leads to suboptimal performance. We introduce Retrieval-Augmented Dual Instruction Tuning (RA-DIT), a lightweight fine-tuning methodology that provides a third option by retrofitting any LLM with retrieval capabilities. Our approach operates in two distinct fine-tuning steps: (1) one updates a pre-trained LM to better use retrieved information, while (2) the other updates the retriever to return more relevant results, as preferred by the LM. By fine-tuning over tasks that require both knowledge utilization and contextual awareness, we demonstrate that each stage yields significant performance improvements, and using both leads to additional gains. Our best model, RA-DIT 65B,
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26500;&#24314;&#26085;&#35821;&#25351;&#20196;&#25968;&#25454;&#38598;&#24182;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#65292;&#39564;&#35777;&#20102;&#26085;&#35821;&#25351;&#20196;&#25968;&#25454;&#38598;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#34920;&#26126;&#36890;&#36807;&#25351;&#20196;&#35843;&#25972;&#21487;&#20197;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.03412</link><description>&lt;p&gt;
&#20174;&#22522;&#30784;&#21040;&#23545;&#35805;&#24335;&#65306;&#26085;&#35821;&#25351;&#20196;&#25968;&#25454;&#38598;&#21644;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
From Base to Conversational: Japanese Instruction Dataset and Tuning Large Language Models. (arXiv:2309.03412v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03412
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26500;&#24314;&#26085;&#35821;&#25351;&#20196;&#25968;&#25454;&#38598;&#24182;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#65292;&#39564;&#35777;&#20102;&#26085;&#35821;&#25351;&#20196;&#25968;&#25454;&#38598;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#34920;&#26126;&#36890;&#36807;&#25351;&#20196;&#35843;&#25972;&#21487;&#20197;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#21464;&#24471;&#20132;&#20114;&#24615;&#26469;&#35828;&#65292;&#25351;&#20196;&#35843;&#25972;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#23613;&#31649;&#23384;&#22312;&#35768;&#22810;&#33521;&#25991;&#30340;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#20294;&#20854;&#20182;&#35821;&#35328;&#32570;&#20047;&#26126;&#26174;&#12290;&#32780;&#19988;&#65292;&#23427;&#20204;&#22312;&#38750;&#33521;&#35821;&#35821;&#35328;&#20013;&#30340;&#25928;&#26524;&#23578;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#39564;&#35777;&#12290;&#25105;&#20204;&#36890;&#36807;&#25193;&#23637;&#21644;&#31579;&#36873;&#29616;&#26377;&#25968;&#25454;&#38598;&#26500;&#24314;&#20102;&#19968;&#20010;&#26085;&#35821;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#24182;&#23558;&#35813;&#25968;&#25454;&#38598;&#24212;&#29992;&#20110;&#19968;&#20010;&#26085;&#35821;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#25351;&#20196;&#25968;&#25454;&#38598;&#23545;&#26085;&#35821;&#21644;&#33521;&#35821;&#29616;&#26377;&#27169;&#22411;&#36827;&#34892;&#20102;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#35843;&#25972;&#12290;&#25105;&#20204;&#20174;&#25968;&#37327;&#21644;&#36136;&#37327;&#20004;&#20010;&#35282;&#24230;&#35780;&#20272;&#20102;&#36825;&#20123;&#27169;&#22411;&#12290;&#32467;&#26524;&#30830;&#35748;&#20102;&#26085;&#35821;&#25351;&#20196;&#25968;&#25454;&#38598;&#30340;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#30456;&#23545;&#36739;&#23567;&#30340;LLMs&#65292;&#36890;&#36807;&#25351;&#20196;&#35843;&#25972;&#20063;&#33021;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#25351;&#20196;&#25968;&#25454;&#38598;&#12289;&#35843;&#25972;&#27169;&#22411;&#21644;&#23454;&#29616;&#22343;&#21487;&#22312;&#32593;&#19978;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction tuning is essential for large language models (LLMs) to become interactive. While many instruction tuning datasets exist in English, there is a noticeable lack in other languages. Also, their effectiveness has not been well verified in non-English languages. We construct a Japanese instruction dataset by expanding and filtering existing datasets and apply the dataset to a Japanese pre-trained base model. We performed Low-Rank Adaptation (LoRA) tuning on both Japanese and English existing models using our instruction dataset. We evaluated these models from both quantitative and qualitative perspectives. As a result, the effectiveness of Japanese instruction datasets is confirmed. The results also indicate that even with relatively small LLMs, performances in downstream tasks would be improved through instruction tuning. Our instruction dataset, tuned models, and implementation are publicly available online.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27604;&#36739;&#35805;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#39532;&#20811;&#30333;&#24422;&#30149;&#30740;&#31350;&#20013;&#23384;&#22312;&#30683;&#30462;&#32467;&#26524;&#30340;&#25253;&#21578;&#12290;&#36890;&#36807;&#23545;&#27604;&#19981;&#21516;&#35805;&#39064;&#19982;&#26174;&#33879;&#32467;&#26524;&#30340;&#30456;&#20851;&#24615;&#65292;&#25214;&#21040;&#20102;&#19982;&#40644;&#26001;&#21464;&#24615;&#30740;&#31350;&#20013;&#26174;&#33879;&#32467;&#26524;&#25253;&#21578;&#30456;&#20851;&#30340;&#20843;&#31181;&#21270;&#21512;&#29289;&#12290;</title><link>http://arxiv.org/abs/2309.00312</link><description>&lt;p&gt;
&#29992;&#20110;&#39532;&#20811;&#30333;&#24422;&#30149;&#30740;&#31350;&#30340;&#19981;&#21516;&#25253;&#21578;&#32467;&#26524;&#30340;&#27604;&#36739;&#35805;&#39064;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Comparative Topic Modeling for Determinants of Divergent Report Results Applied to Macular Degeneration Studies. (arXiv:2309.00312v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27604;&#36739;&#35805;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#39532;&#20811;&#30333;&#24422;&#30149;&#30740;&#31350;&#20013;&#23384;&#22312;&#30683;&#30462;&#32467;&#26524;&#30340;&#25253;&#21578;&#12290;&#36890;&#36807;&#23545;&#27604;&#19981;&#21516;&#35805;&#39064;&#19982;&#26174;&#33879;&#32467;&#26524;&#30340;&#30456;&#20851;&#24615;&#65292;&#25214;&#21040;&#20102;&#19982;&#40644;&#26001;&#21464;&#24615;&#30740;&#31350;&#20013;&#26174;&#33879;&#32467;&#26524;&#25253;&#21578;&#30456;&#20851;&#30340;&#20843;&#31181;&#21270;&#21512;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35805;&#39064;&#24314;&#27169;&#21644;&#25991;&#26412;&#25366;&#25496;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#23376;&#38598;&#65292;&#36866;&#29992;&#20110;&#36827;&#34892;&#20803;&#20998;&#26512;&#21644;&#31995;&#32479;&#23457;&#26597;&#12290;&#23545;&#20110;&#35777;&#25454;&#32508;&#36848;&#65292;&#19978;&#36848;NLP&#26041;&#27861;&#36890;&#24120;&#29992;&#20110;&#29305;&#23450;&#20027;&#39064;&#30340;&#25991;&#29486;&#25628;&#32034;&#25110;&#20174;&#25253;&#21578;&#20013;&#25552;&#21462;&#20540;&#20197;&#33258;&#21160;&#21270;SR&#21644;MA&#30340;&#20851;&#38190;&#38454;&#27573;&#12290;&#30456;&#21453;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27604;&#36739;&#35805;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#21516;&#19968;&#24191;&#20041;&#30740;&#31350;&#38382;&#39064;&#19978;&#23384;&#22312;&#30683;&#30462;&#32467;&#26524;&#30340;&#25253;&#21578;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#30446;&#26631;&#26159;&#36890;&#36807;&#26681;&#25454;&#20854;&#27604;&#20363;&#21457;&#29983;&#21644;&#22312;&#26174;&#33879;&#32467;&#26524;&#25253;&#21578;&#20013;&#30340;&#19968;&#33268;&#24615;&#20998;&#24067;&#23545;&#20854;&#36827;&#34892;&#25490;&#21517;&#65292;&#25214;&#21040;&#19982;&#24863;&#20852;&#36259;&#30340;&#32467;&#26524;&#26174;&#33879;&#30456;&#20851;&#30340;&#35805;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#28041;&#21450;&#34917;&#20805;&#33829;&#20859;&#21270;&#21512;&#29289;&#26159;&#21542;&#26174;&#33879;&#26377;&#30410;&#20110;&#40644;&#26001;&#21464;&#24615;(MD)&#30340;&#24191;&#27867;&#33539;&#22260;&#30340;&#30740;&#31350;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#30830;&#23450;&#20102;&#20843;&#31181;&#21270;&#21512;&#29289;&#19982;&#26174;&#33879;&#32467;&#26524;&#25253;&#21578;&#30340;&#29305;&#23450;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Topic modeling and text mining are subsets of Natural Language Processing with relevance for conducting meta-analysis (MA) and systematic review (SR). For evidence synthesis, the above NLP methods are conventionally used for topic-specific literature searches or extracting values from reports to automate essential phases of SR and MA. Instead, this work proposes a comparative topic modeling approach to analyze reports of contradictory results on the same general research question. Specifically, the objective is to find topics exhibiting distinct associations with significant results for an outcome of interest by ranking them according to their proportional occurrence and consistency of distribution across reports of significant results. The proposed method was tested on broad-scope studies addressing whether supplemental nutritional compounds significantly benefit macular degeneration (MD). Eight compounds were identified as having a particular association with reports of significant r
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#30340;&#12289;&#23454;&#20307;&#32423;&#30340;&#23450;&#20041;&#26469;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35760;&#24518;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#25552;&#21462;&#25935;&#24863;&#23454;&#20307;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#35774;&#32622;&#19979;&#37325;&#26500;&#25935;&#24863;&#23454;&#20307;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.15727</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#37327;&#21270;&#21644;&#20998;&#26512;&#23454;&#20307;&#32423;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
Quantifying and Analyzing Entity-level Memorization in Large Language Models. (arXiv:2308.15727v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15727
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#30340;&#12289;&#23454;&#20307;&#32423;&#30340;&#23450;&#20041;&#26469;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35760;&#24518;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#25552;&#21462;&#25935;&#24863;&#23454;&#20307;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#35774;&#32622;&#19979;&#37325;&#26500;&#25935;&#24863;&#23454;&#20307;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#35777;&#26126;&#33021;&#22815;&#35760;&#24518;&#20854;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#29305;&#23450;&#35774;&#35745;&#30340;&#25552;&#31034;&#25552;&#21462;&#20986;&#26469;&#12290;&#38543;&#30528;&#25968;&#25454;&#38598;&#35268;&#27169;&#30340;&#19981;&#26029;&#22686;&#38271;&#65292;&#30001;&#35760;&#24518;&#24341;&#36215;&#30340;&#38544;&#31169;&#39118;&#38505;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#37327;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#35760;&#24518;&#33021;&#21147;&#26377;&#21161;&#20110;&#35780;&#20272;&#28508;&#22312;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#20851;&#20110;&#37327;&#21270;&#35760;&#24518;&#30340;&#30740;&#31350;&#38656;&#35201;&#35775;&#38382;&#31934;&#30830;&#30340;&#21407;&#22987;&#25968;&#25454;&#25110;&#20135;&#29983;&#30456;&#24403;&#22823;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#36825;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#35828;&#24456;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#30340;&#12289;&#23454;&#20307;&#32423;&#30340;&#23450;&#20041;&#65292;&#29992;&#20110;&#20197;&#26356;&#25509;&#36817;&#23454;&#38469;&#22330;&#26223;&#30340;&#26465;&#20214;&#21644;&#24230;&#37327;&#26041;&#24335;&#26469;&#37327;&#21270;&#35760;&#24518;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#20013;&#39640;&#25928;&#25552;&#21462;&#25935;&#24863;&#23454;&#20307;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22522;&#20110;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#35774;&#32622;&#19979;&#30340;&#37325;&#26500;&#25935;&#24863;&#23454;&#20307;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have been proven capable of memorizing their training data, which can be extracted through specifically designed prompts. As the scale of datasets continues to grow, privacy risks arising from memorization have attracted increasing attention. Quantifying language model memorization helps evaluate potential privacy risks. However, prior works on quantifying memorization require access to the precise original data or incur substantial computational overhead, making it difficult for applications in real-world language models. To this end, we propose a fine-grained, entity-level definition to quantify memorization with conditions and metrics closer to real-world scenarios. In addition, we also present an approach for efficiently extracting sensitive entities from autoregressive language models. We conduct extensive experiments based on the proposed, probing language models' ability to reconstruct sensitive entities under different settings. We find that languag
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;DS4DH&#22312;#SMM4H 2023&#20013;&#24320;&#21457;&#30340;&#19981;&#33391;&#33647;&#29289;&#20107;&#20214;&#35268;&#33539;&#21270;&#31995;&#32479;&#30340;&#24615;&#33021;&#35780;&#20272;&#65292;&#35813;&#31995;&#32479;&#21033;&#29992;&#21477;&#23376;&#36716;&#25442;&#21644;&#20498;&#25968;&#25490;&#21517;&#34701;&#21512;&#36827;&#34892;&#38646;&#26679;&#26412;&#35268;&#33539;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#20849;&#20139;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#21487;&#26377;&#25928;&#24212;&#29992;&#20110;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#25366;&#25496;&#20013;&#30340;&#19981;&#33391;&#33647;&#29289;&#20107;&#20214;&#35268;&#33539;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.12877</link><description>&lt;p&gt;
DS4DH&#22312;#SMM4H 2023&#19978;&#65306;&#20351;&#29992;&#21477;&#23376;&#36716;&#25442;&#21644;&#20498;&#25968;&#25490;&#21517;&#34701;&#21512;&#36827;&#34892;&#38646;&#26679;&#26412;&#19981;&#33391;&#33647;&#29289;&#20107;&#20214;&#35268;&#33539;&#21270;
&lt;/p&gt;
&lt;p&gt;
DS4DH at #SMM4H 2023: Zero-Shot Adverse Drug Events Normalization using Sentence Transformers and Reciprocal-Rank Fusion. (arXiv:2308.12877v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DS4DH&#22312;#SMM4H 2023&#20013;&#24320;&#21457;&#30340;&#19981;&#33391;&#33647;&#29289;&#20107;&#20214;&#35268;&#33539;&#21270;&#31995;&#32479;&#30340;&#24615;&#33021;&#35780;&#20272;&#65292;&#35813;&#31995;&#32479;&#21033;&#29992;&#21477;&#23376;&#36716;&#25442;&#21644;&#20498;&#25968;&#25490;&#21517;&#34701;&#21512;&#36827;&#34892;&#38646;&#26679;&#26412;&#35268;&#33539;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#20849;&#20139;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#21487;&#26377;&#25928;&#24212;&#29992;&#20110;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#25366;&#25496;&#20013;&#30340;&#19981;&#33391;&#33647;&#29289;&#20107;&#20214;&#35268;&#33539;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27010;&#36848;&#20102;&#30001;&#25968;&#25454;&#31185;&#23398;&#19982;&#25968;&#23383;&#20581;&#24247;&#22242;&#38431;&#24320;&#21457;&#30340;&#29992;&#20110;&#31038;&#20132;&#23186;&#20307;&#25366;&#25496;&#20581;&#24247;&#24212;&#29992;2023&#20849;&#20139;&#20219;&#21153;5&#30340;&#19981;&#33391;&#33647;&#29289;&#20107;&#20214;&#35268;&#33539;&#21270;&#31995;&#32479;&#30340;&#24615;&#33021;&#35780;&#20272;&#12290;&#20849;&#20139;&#20219;&#21153;5&#26088;&#22312;&#23558;Twitter&#20013;&#30340;&#19981;&#33391;&#33647;&#29289;&#20107;&#20214;&#25552;&#21450;&#26631;&#20934;&#21270;&#20026;&#21307;&#30103;&#27861;&#35268;&#27963;&#21160;&#26415;&#35821;&#23383;&#20856;&#20013;&#30340;&#26631;&#20934;&#27010;&#24565;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#37319;&#29992;&#20004;&#38454;&#27573;&#26041;&#27861;&#65306;BERT&#24494;&#35843;&#23454;&#20307;&#35782;&#21035;&#65292;&#28982;&#21518;&#20351;&#29992;&#21477;&#23376;&#36716;&#25442;&#21644;&#20498;&#25968;&#25490;&#21517;&#34701;&#21512;&#36827;&#34892;&#38646;&#26679;&#26412;&#35268;&#33539;&#21270;&#12290;&#35813;&#26041;&#27861;&#30340;&#31934;&#30830;&#24230;&#20026;44.9%&#65292;&#21484;&#22238;&#29575;&#20026;40.5%&#65292;F1&#20998;&#25968;&#20026;42.6%&#12290;&#23427;&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#20849;&#20139;&#20219;&#21153;5&#20013;&#20301;&#25968;&#34920;&#29616;10%&#65292;&#24182;&#22312;&#25152;&#26377;&#21442;&#19982;&#32773;&#20013;&#23637;&#31034;&#20102;&#26368;&#39640;&#24615;&#33021;&#12290;&#36825;&#20123;&#32467;&#26524;&#35777;&#23454;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#22312;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#25366;&#25496;&#39046;&#22495;&#20013;&#36827;&#34892;&#19981;&#33391;&#33647;&#29289;&#20107;&#20214;&#35268;&#33539;&#21270;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper outlines the performance evaluation of a system for adverse drug event normalization, developed by the Data Science for Digital Health group for the Social Media Mining for Health Applications 2023 shared task 5. Shared task 5 targeted the normalization of adverse drug event mentions in Twitter to standard concepts from the Medical Dictionary for Regulatory Activities terminology. Our system hinges on a two-stage approach: BERT fine-tuning for entity recognition, followed by zero-shot normalization using sentence transformers and reciprocal-rank fusion. The approach yielded a precision of 44.9%, recall of 40.5%, and an F1-score of 42.6%. It outperformed the median performance in shared task 5 by 10% and demonstrated the highest performance among all participants. These results substantiate the effectiveness of our approach and its potential application for adverse drug event normalization in the realm of social media text mining.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#21152;&#23494;&#36135;&#24065;&#35777;&#21048;&#26696;&#20214;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#33021;&#22815;&#20934;&#30830;&#21028;&#26029;&#36829;&#27861;&#34892;&#20026;&#65292;&#24182;&#27604;&#36739;&#20102;&#30001;LLM&#21644;&#24459;&#24072;&#25776;&#20889;&#30340;&#25237;&#35785;&#20070;&#23545;&#38506;&#23457;&#22242;&#20915;&#31574;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#30446;&#21069;&#30340;LLMs&#22312;&#27861;&#24459;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#36739;&#24369;&#65292;&#20294;&#38543;&#30528;&#26410;&#26469;&#27169;&#22411;&#30340;&#25913;&#36827;&#65292;&#20854;&#28508;&#21147;&#26377;&#26395;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2308.06032</link><description>&lt;p&gt;
&#21152;&#23494;&#36135;&#24065;&#35777;&#21048;&#26696;&#20214;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;ChatGPT&#33021;&#21542;&#21462;&#20195;&#24459;&#24072;&#65311;
&lt;/p&gt;
&lt;p&gt;
Large Language Models in Cryptocurrency Securities Cases: Can ChatGPT Replace Lawyers?. (arXiv:2308.06032v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06032
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#21152;&#23494;&#36135;&#24065;&#35777;&#21048;&#26696;&#20214;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#33021;&#22815;&#20934;&#30830;&#21028;&#26029;&#36829;&#27861;&#34892;&#20026;&#65292;&#24182;&#27604;&#36739;&#20102;&#30001;LLM&#21644;&#24459;&#24072;&#25776;&#20889;&#30340;&#25237;&#35785;&#20070;&#23545;&#38506;&#23457;&#22242;&#20915;&#31574;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#30446;&#21069;&#30340;LLMs&#22312;&#27861;&#24459;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#36739;&#24369;&#65292;&#20294;&#38543;&#30528;&#26410;&#26469;&#27169;&#22411;&#30340;&#25913;&#36827;&#65292;&#20854;&#28508;&#21147;&#26377;&#26395;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#22686;&#24378;&#23545;&#27861;&#24459;&#31995;&#32479;&#30340;&#35775;&#38382;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#23427;&#20204;&#22312;&#36827;&#34892;&#27861;&#24459;&#20219;&#21153;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#30340;&#23454;&#35777;&#30740;&#31350;&#38750;&#24120;&#26377;&#38480;&#12290;&#25105;&#20204;&#30740;&#31350;&#28041;&#21450;&#21152;&#23494;&#36135;&#24065;&#30340;&#35777;&#21048;&#26696;&#20214;&#65292;&#20316;&#20026;AI&#21487;&#20197;&#25903;&#25345;&#27861;&#24459;&#36807;&#31243;&#30340;&#20247;&#22810;&#24773;&#22659;&#20043;&#19968;&#65292;&#30740;&#31350;LLMs&#30340;&#27861;&#24459;&#25512;&#29702;&#21644;&#36215;&#33609;&#33021;&#21147;&#12290;&#25105;&#20204;&#26816;&#26597;&#20197;&#19979;&#20004;&#20010;&#26041;&#38754;&#65306;a&#65289;LLM&#33021;&#21542;&#20934;&#30830;&#30830;&#23450;&#20107;&#23454;&#27169;&#24335;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#36829;&#27861;&#34892;&#20026;&#65292;b&#65289;&#22522;&#20110;LLM&#21644;&#24459;&#24072;&#25776;&#20889;&#30340;&#25237;&#35785;&#20070;&#65292;&#38506;&#23457;&#22242;&#30340;&#20915;&#31574;&#26159;&#21542;&#26377;&#25152;&#24046;&#24322;&#12290;&#25105;&#20204;&#23558;&#30495;&#23454;&#26696;&#20363;&#20013;&#30340;&#20107;&#23454;&#27169;&#24335;&#36755;&#20837;GPT-3.5&#65292;&#24182;&#35780;&#20272;&#20854;&#30830;&#23450;&#27491;&#30830;&#28508;&#22312;&#36829;&#27861;&#34892;&#20026;&#24182;&#25490;&#38500;&#34394;&#20551;&#36829;&#27861;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35831;&#27169;&#25311;&#38506;&#23457;&#21592;&#35780;&#20272;LLM&#21644;&#24459;&#24072;&#25776;&#20889;&#30340;&#25237;&#35785;&#20070;&#12290;GPT-3.5&#30340;&#27861;&#24459;&#25512;&#29702;&#33021;&#21147;&#36739;&#24369;&#65292;&#20294;&#25105;&#20204;&#39044;&#26399;&#26410;&#26469;&#27169;&#22411;&#30340;&#25913;&#36827;&#65292;&#29305;&#21035;&#26159;&#32771;&#34385;&#21040;&#23427;&#24314;&#35758;&#30340;&#36829;&#27861;&#34892;&#20026;&#24448;&#24448;&#26159;&#27491;&#30830;&#30340;&#65288;&#23427;&#20165;&#20165;&#36807;&#20110;&#20445;&#23432;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) could enhance access to the legal system. However, empirical research on their effectiveness in conducting legal tasks is scant. We study securities cases involving cryptocurrencies as one of numerous contexts where AI could support the legal process, studying LLMs' legal reasoning and drafting capabilities. We examine whether a) an LLM can accurately determine which laws are potentially being violated from a fact pattern, and b) whether there is a difference in juror decision-making based on complaints written by a lawyer compared to an LLM. We feed fact patterns from real-life cases to GPT-3.5 and evaluate its ability to determine correct potential violations from the scenario and exclude spurious violations. Second, we had mock jurors assess complaints written by the LLM and lawyers. GPT-3.5's legal reasoning skills proved weak, though we expect improvement in future models, particularly given the violations it suggested tended to be correct (it merely m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;Fine-Tuned&#30340;OpenAI LLM&#36827;&#34892;&#32763;&#35793;&#36136;&#37327;&#20272;&#35745;&#30340;&#33021;&#21147;&#65292;&#23454;&#39564;&#35777;&#26126;&#21487;&#20197;&#36890;&#36807;Fine-Tuned&#30340;ChatGPT&#26469;&#39044;&#27979;&#26426;&#22120;&#32763;&#35793;&#30340;&#36136;&#37327;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2308.00158</link><description>&lt;p&gt;
&#20351;&#29992;Fine-Tuned&#30340;OpenAI LLM&#39044;&#27979;&#26426;&#22120;&#32763;&#35793;&#36755;&#20986;&#20013;&#30340;&#23436;&#32654;&#36136;&#37327;&#27573;&#33853;&#65306;&#26159;&#21542;&#21487;&#20197;&#20174;&#21382;&#21490;&#25968;&#25454;&#20013;&#25429;&#25417;&#32534;&#36753;&#36317;&#31163;&#27169;&#24335;&#65311;
&lt;/p&gt;
&lt;p&gt;
Predicting Perfect Quality Segments in MT Output with Fine-Tuned OpenAI LLM: Is it possible to capture editing distance patterns from historical data?. (arXiv:2308.00158v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;Fine-Tuned&#30340;OpenAI LLM&#36827;&#34892;&#32763;&#35793;&#36136;&#37327;&#20272;&#35745;&#30340;&#33021;&#21147;&#65292;&#23454;&#39564;&#35777;&#26126;&#21487;&#20197;&#36890;&#36807;Fine-Tuned&#30340;ChatGPT&#26469;&#39044;&#27979;&#26426;&#22120;&#32763;&#35793;&#30340;&#36136;&#37327;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32763;&#35793;&#36136;&#37327;&#20272;&#35745;&#65288;TQE&#65289;&#26159;&#23558;&#36755;&#20986;&#32763;&#35793;&#37096;&#32626;&#21040;&#20351;&#29992;&#20013;&#20043;&#21069;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290; TQE&#23545;&#20110;&#35780;&#20272;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#21644;&#20154;&#24037;&#32763;&#35793;&#65288;HT&#65289;&#30340;&#36136;&#37327;&#20063;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#32780;&#19981;&#38656;&#35201;&#26597;&#30475;&#21442;&#32771;&#32763;&#35793;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26816;&#26597;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#21487;&#20197;&#20026;TQE&#20219;&#21153;&#21644;&#23427;&#20204;&#30340;&#33021;&#21147;&#36827;&#34892;Fine-Tune&#12290;&#25105;&#20204;&#20197;ChatGPT&#20026;&#20363;&#65292;&#23558;TQE&#35270;&#20026;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#12290;&#20351;&#29992;&#33521;&#24847;&#21644;&#33521;&#24503;&#35757;&#32451;&#35821;&#26009;&#24211;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36890;&#36807;ChatGPT&#30340;API Fine-Tuned&#21487;&#20197;&#22312;&#39044;&#27979;&#32763;&#35793;&#36136;&#37327;&#26041;&#38754;&#33719;&#24471;&#30456;&#23545;&#36739;&#39640;&#30340;&#24471;&#20998;&#65292;&#21363;&#26159;&#21542;&#38656;&#35201;&#32534;&#36753;&#32763;&#35793;&#65292;&#20294;&#32943;&#23450;&#26377;&#25913;&#36827;&#20934;&#30830;&#24615;&#30340;&#31354;&#38388;&#12290;&#33521;&#24847;&#21452;&#35821;&#25688;&#35201;&#21487;&#22312;&#35770;&#25991;&#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Translation Quality Estimation (TQE) is an important step before deploying the output translation into usage. TQE is also critical in assessing machine translation (MT) and human translation (HT) quality without seeing the reference translations. In this work, we examine if the state-of-the-art large language models (LLMs) can be fine-tuned for the TQE task and their capability. We take ChatGPT as one example and approach TQE as a binary classification task. Using English-Italian and English-German training corpus, our experimental results show that fine-tuned ChatGPT via its API can achieve a relatively high score on predicting translation quality, i.e. if the translation needs to be edited, but there is definitely space to improve the accuracy. English-Italiano bilingual Abstract is available in the paper.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#35745;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#35821;&#35328;&#27169;&#22411;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#19982;&#22522;&#26412;&#35821;&#35328;&#20219;&#21153;&#30340;&#33021;&#21147;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#22797;&#26434;&#25216;&#33021;&#20135;&#29983;&#30340;&#26426;&#21046;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#25193;&#23637;&#23450;&#24459;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#33021;&#22815;&#39640;&#25928;&#23398;&#20064;&#65292;&#24182;&#34920;&#29616;&#20986;&#36829;&#21453;&#36890;&#24120;&#27867;&#21270;&#29702;&#35770;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.15936</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20013;&#22797;&#26434;&#25216;&#33021;&#20135;&#29983;&#30340;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
A Theory for Emergence of Complex Skills in Language Models. (arXiv:2307.15936v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15936
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#35745;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#35821;&#35328;&#27169;&#22411;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#19982;&#22522;&#26412;&#35821;&#35328;&#20219;&#21153;&#30340;&#33021;&#21147;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#22797;&#26434;&#25216;&#33021;&#20135;&#29983;&#30340;&#26426;&#21046;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#25193;&#23637;&#23450;&#24459;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#33021;&#22815;&#39640;&#25928;&#23398;&#20064;&#65292;&#24182;&#34920;&#29616;&#20986;&#36829;&#21453;&#36890;&#24120;&#27867;&#21270;&#29702;&#35770;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#38598;&#21512;&#21644;&#35757;&#32451;&#35821;&#26009;&#24211;&#25193;&#22823;&#26102;&#65292;&#26032;&#30340;&#25216;&#33021;&#23558;&#22312; AI &#20135;&#21697;&#20013;&#20986;&#29616;&#30340;&#20027;&#35201;&#39537;&#21160;&#22240;&#32032;&#12290;&#36825;&#31181;&#29616;&#35937;&#23578;&#19981;&#20026;&#20154;&#25152;&#29702;&#35299;&#65292;&#24182;&#19988;&#36890;&#36807;&#23545;&#22522;&#20110;&#26799;&#24230;&#35757;&#32451;&#30340;&#25968;&#23398;&#20998;&#26512;&#25552;&#20379;&#26426;&#26800;&#35299;&#37322;&#20284;&#20046;&#24456;&#22256;&#38590;&#12290;&#26412;&#25991;&#37319;&#29992;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#33879;&#21517;&#30340;&#65288;&#21644;&#32463;&#39564;&#24615;&#30340;&#65289;LLM&#25193;&#23637;&#23450;&#24459;&#21644;&#31616;&#21333;&#30340;&#32479;&#35745;&#26694;&#26550;&#26469;&#20998;&#26512;&#20986;&#29616;&#12290;&#36129;&#29486;&#21253;&#25324;&#65306;&#65288;a&#65289;&#19968;&#20010;&#32479;&#35745;&#26694;&#26550;&#23558;LLM&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#19982;&#35821;&#35328;&#20219;&#21153;&#22522;&#26412;&#25216;&#33021;&#30340;&#33021;&#21147;&#30456;&#20851;&#32852;&#12290;&#65288;b&#65289;&#25968;&#23398;&#20998;&#26512;&#34920;&#26126;&#65292;&#25193;&#23637;&#23450;&#24459;&#24847;&#21619;&#30528;&#24378;&#28872;&#30340;&#24402;&#32435;&#20559;&#35265;&#65292;&#20351;&#39044;&#35757;&#32451;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#24471;&#38750;&#24120;&#39640;&#25928;&#12290;&#25105;&#20204;&#38750;&#27491;&#24335;&#22320;&#31216;&#20043;&#20026;&#8220;&#24377;&#24339;&#27867;&#21270;&#8221;&#65292;&#22240;&#20026;&#34920;&#38754;&#19978;&#30475;&#65292;&#23427;&#20284;&#20046;&#25552;&#20379;&#20102;&#22312;&#25216;&#33021;&#27700;&#24179;&#19978;&#36829;&#21453;&#36890;&#24120;&#27867;&#21270;&#29702;&#35770;&#30340;&#33021;&#21147;&#12290;&#65288;c&#65289;&#24377;&#24339;&#27867;&#21270;&#30340;&#19968;&#20010;&#20851;&#38190;&#20363;&#23376;&#65292;&#21363;&#22312;&#25191;&#34892;&#20219;&#21153;&#26102;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
A major driver of AI products today is the fact that new skills emerge in language models when their parameter set and training corpora are scaled up. This phenomenon is poorly understood, and a mechanistic explanation via mathematical analysis of gradient-based training seems difficult. The current paper takes a different approach, analysing emergence using the famous (and empirical) Scaling Laws of LLMs and a simple statistical framework. Contributions include: (a) A statistical framework that relates cross-entropy loss of LLMs to competence on the basic skills that underlie language tasks. (b) Mathematical analysis showing that the Scaling Laws imply a strong form of inductive bias that allows the pre-trained model to learn very efficiently. We informally call this {\em slingshot generalization} since naively viewed it appears to give competence levels at skills that violate usual generalization theory. (c) A key example of slingshot generalization, that competence at executing task
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#39318;&#27425;&#23558;&#27597;&#35821;&#35782;&#21035;&#24212;&#29992;&#20110;&#22303;&#32819;&#20854;&#35821;,&#36890;&#36807;&#20998;&#26512;&#20316;&#32773;&#19981;&#21516;&#35821;&#35328;&#30340;&#20889;&#20316;&#26469;&#39044;&#27979;&#20316;&#32773;&#30340;&#27597;&#35821;&#12290;&#30740;&#31350;&#20351;&#29992;&#20102;&#22303;&#32819;&#20854;&#23398;&#20064;&#32773;&#35821;&#26009;&#24211;&#21644;&#19977;&#20010;&#21477;&#27861;&#29305;&#24449;&#26469;&#23637;&#31034;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.14850</link><description>&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#39318;&#27425;&#23558;&#27597;&#35821;&#35782;&#21035;&#65288;Native Language Identification&#65292;NLI&#65289;&#24212;&#29992;&#20110;&#22303;&#32819;&#20854;&#35821;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Turkish Native Language Identification. (arXiv:2307.14850v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14850
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#39318;&#27425;&#23558;&#27597;&#35821;&#35782;&#21035;&#24212;&#29992;&#20110;&#22303;&#32819;&#20854;&#35821;,&#36890;&#36807;&#20998;&#26512;&#20316;&#32773;&#19981;&#21516;&#35821;&#35328;&#30340;&#20889;&#20316;&#26469;&#39044;&#27979;&#20316;&#32773;&#30340;&#27597;&#35821;&#12290;&#30740;&#31350;&#20351;&#29992;&#20102;&#22303;&#32819;&#20854;&#23398;&#20064;&#32773;&#35821;&#26009;&#24211;&#21644;&#19977;&#20010;&#21477;&#27861;&#29305;&#24449;&#26469;&#23637;&#31034;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23558;&#27597;&#35821;&#35782;&#21035;&#65288;NLI&#65289;&#24212;&#29992;&#20110;&#22303;&#32819;&#20854;&#35821;&#12290;NLI &#26159;&#36890;&#36807;&#20998;&#26512;&#20316;&#32773;&#19981;&#21516;&#35821;&#35328;&#30340;&#20889;&#20316;&#26469;&#39044;&#27979;&#20316;&#32773;&#30340;&#27597;&#35821;&#12290;&#23613;&#31649;&#22823;&#22810;&#25968;NLI&#30740;&#31350;&#37117;&#20391;&#37325;&#20110;&#33521;&#35821;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#23558;&#20854;&#33539;&#22260;&#25193;&#23637;&#21040;&#22303;&#32819;&#20854;&#35821;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#26368;&#36817;&#26500;&#24314;&#30340;&#22303;&#32819;&#20854;&#23398;&#20064;&#32773;&#35821;&#26009;&#24211;&#65292;&#24182;&#32467;&#21512;&#20102;&#19977;&#20010;&#21477;&#27861;&#29305;&#24449;&#65288;CFG &#20135;&#29983;&#35268;&#21017;&#65292;&#35789;&#24615;n-gram&#21644;&#20989;&#25968;&#35789;&#65289;&#19982;L2&#25991;&#26412;&#65292;&#20197;&#23637;&#31034;&#23427;&#20204;&#22312;&#35813;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present the first application of Native Language Identification (NLI) for the Turkish language. NLI involves predicting the writer's first language by analysing their writing in different languages. While most NLI research has focused on English, our study extends its scope to Turkish. We used the recently constructed Turkish Learner Corpus and employed a combination of three syntactic features (CFG production rules, part-of-speech n-grams and function words) with L2 texts to demonstrate their effectiveness in this task.
&lt;/p&gt;</description></item><item><title>EmotionPrompt&#26159;&#19968;&#20010;&#22522;&#20110;&#24515;&#29702;&#23398;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24773;&#24863;&#21050;&#28608;&#34701;&#20837;&#21040;&#25552;&#31034;&#20013;&#65292;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#39033;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21516;&#26102;&#25913;&#21892;&#20102;&#20854;&#30495;&#23454;&#24615;&#21644;&#20449;&#24687;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.11760</link><description>&lt;p&gt;
EmotionPrompt: &#36890;&#36807;&#24773;&#24863;&#21050;&#28608;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#38190;&#24515;&#29702;&#23398;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
EmotionPrompt: Leveraging Psychology for Large Language Models Enhancement via Emotional Stimulus. (arXiv:2307.11760v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11760
&lt;/p&gt;
&lt;p&gt;
EmotionPrompt&#26159;&#19968;&#20010;&#22522;&#20110;&#24515;&#29702;&#23398;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24773;&#24863;&#21050;&#28608;&#34701;&#20837;&#21040;&#25552;&#31034;&#20013;&#65292;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#39033;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21516;&#26102;&#25913;&#21892;&#20102;&#20854;&#30495;&#23454;&#24615;&#21644;&#20449;&#24687;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25512;&#29702;&#12289;&#35821;&#35328;&#29702;&#35299;&#21644;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#31561;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#24182;&#34987;&#35270;&#20026;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;LLMs&#23545;&#25552;&#31034;&#30340;&#25935;&#24863;&#24615;&#20173;&#28982;&#26159;&#20854;&#26085;&#24120;&#24212;&#29992;&#30340;&#20027;&#35201;&#29942;&#39048;&#12290;&#26412;&#25991;&#20174;&#24515;&#29702;&#23398;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#25552;&#20986;&#20102;EmotionPrompt&#26469;&#25506;&#32034;&#24773;&#24863;&#26234;&#33021;&#20197;&#25552;&#21319;LLMs&#30340;&#24615;&#33021;&#12290;EmotionPrompt&#22522;&#20110;&#19968;&#20010;&#38750;&#24120;&#31616;&#21333;&#26126;&#20102;&#30340;&#21407;&#21017;&#65306;&#23558;&#24773;&#24863;&#21050;&#28608;&#34701;&#20837;&#21040;&#25552;&#31034;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#30456;&#21516;&#30340;&#21333;&#19968;&#25552;&#31034;&#27169;&#26495;&#19978;&#65292;&#19982;&#21407;&#22987;&#30340;&#38646;&#26679;&#26412;&#25552;&#31034;&#21644;Zero-shot-CoT&#30456;&#27604;&#65292;&#22312;8&#20010;&#20219;&#21153;&#19978;&#37117;&#26174;&#33879;&#20248;&#20110;&#22810;&#31181;&#27169;&#22411;&#65306;ChatGPT&#12289;Vicuna-13b&#12289;Bloom&#21644;T5&#12290;&#27492;&#22806;&#65292;&#35266;&#23519;&#21040;EmotionPrompt&#33021;&#22815;&#25552;&#39640;&#30495;&#23454;&#24615;&#21644;&#20449;&#24687;&#37327;&#12290;&#25105;&#20204;&#30456;&#20449;EmotionPrompt&#20026;&#25506;&#32034;&#36328;&#23398;&#31185;&#30693;&#35782;&#24320;&#36767;&#20102;&#19968;&#26465;&#26032;&#30340;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have achieved significant performance in many fields such as reasoning, language understanding, and math problem-solving, and are regarded as a crucial step to artificial general intelligence (AGI). However, the sensitivity of LLMs to prompts remains a major bottleneck for their daily adoption. In this paper, we take inspiration from psychology and propose EmotionPrompt to explore emotional intelligence to enhance the performance of LLMs. EmotionPrompt operates on a remarkably straightforward principle: the incorporation of emotional stimulus into prompts. Experimental results demonstrate that our \method, using the same single prompt templates, significantly outperforms original zero-shot prompt and Zero-shot-CoT on 8 tasks with diverse models: ChatGPT, Vicuna-13b, Bloom, and T5. Further, EmotionPrompt was observed to improve both truthfulness and informativeness. We believe that EmotionPrompt heralds a novel avenue for exploring interdisciplinary knowledg
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#31579;&#36873;&#31574;&#30053;AlpaGasus&#65292;&#36890;&#36807;&#20351;&#29992;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#36807;&#28388;&#25481;&#20302;&#36136;&#37327;&#25968;&#25454;&#65292;&#23427;&#22312;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#27604;&#21407;&#22987;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#24555;&#30340;&#35757;&#32451;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.08701</link><description>&lt;p&gt;
AlpaGasus: &#29992;&#26356;&#23569;&#25968;&#25454;&#35757;&#32451;&#26356;&#22909;&#30340;&#32650;&#39548;
&lt;/p&gt;
&lt;p&gt;
AlpaGasus: Training A Better Alpaca with Fewer Data. (arXiv:2307.08701v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08701
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#31579;&#36873;&#31574;&#30053;AlpaGasus&#65292;&#36890;&#36807;&#20351;&#29992;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#36807;&#28388;&#25481;&#20302;&#36136;&#37327;&#25968;&#25454;&#65292;&#23427;&#22312;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#27604;&#21407;&#22987;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#24555;&#30340;&#35757;&#32451;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#22312;&#26377;&#30417;&#30563;&#30340;&#25351;&#20196;/&#22238;&#22797;&#25968;&#25454;&#19978;&#36827;&#34892;&#25351;&#20196;&#24494;&#35843;&#65288;IFT&#65289;&#26469;&#22686;&#24378;&#20854;&#36981;&#24490;&#25351;&#20196;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#24191;&#27867;&#20351;&#29992;&#30340;IFT&#25968;&#25454;&#38598;&#65288;&#20363;&#22914;&#65306;Alpaca&#30340;52k&#25968;&#25454;&#65289;&#20986;&#20046;&#24847;&#26009;&#22320;&#21253;&#21547;&#35768;&#22810;&#20855;&#26377;&#19981;&#27491;&#30830;&#25110;&#19981;&#30456;&#20851;&#22238;&#22797;&#30340;&#20302;&#36136;&#37327;&#23454;&#20363;&#65292;&#36825;&#20123;&#23454;&#20363;&#20250;&#35823;&#23548;&#21644;&#23545;IFT&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25968;&#25454;&#36873;&#25321;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#20351;&#29992;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;&#65306;ChatGPT&#65289;&#33258;&#21160;&#35782;&#21035;&#24182;&#36807;&#28388;&#25481;&#20302;&#36136;&#37327;&#25968;&#25454;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AlpaGasus&#65292;&#23427;&#20165;&#22312;&#20174;52k Alpaca&#25968;&#25454;&#20013;&#36807;&#28388;&#24471;&#21040;&#30340;9k&#39640;&#36136;&#37327;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;AlpaGasus&#22312;&#22810;&#20010;&#27979;&#35797;&#25968;&#25454;&#38598;&#21644;&#20154;&#24037;&#35780;&#20272;&#20013;&#22343;&#26174;&#33879;&#20248;&#20110;&#21407;&#22987;&#30340;Alpaca&#65292;&#30001;GPT-4&#36827;&#34892;&#35780;&#20272;&#12290;&#20854;13B&#21464;&#31181;&#22312;&#27979;&#35797;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#19982;&#20854;&#25945;&#24072;&#27169;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#21363;&#29983;&#25104;52k&#25968;&#25454;&#30340;Text-Davinci-003&#65289;&#30340;&#24615;&#33021;&#21305;&#37197;&#29575;&#36229;&#36807;90&#65285;&#12290;&#23427;&#36824;&#25552;&#20379;&#20102;5.7&#20493;&#26356;&#24555;&#30340;&#35757;&#32451;&#36895;&#24230;&#65292;&#23558;7B&#21464;&#31181;&#30340;&#35757;&#32451;&#26102;&#38388;&#20174;80&#20998;&#38047;&#20943;&#23569;&#21040;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Large language models~(LLMs) strengthen instruction-following capability through instruction-finetuning (IFT) on supervised instruction/response data. However, widely used IFT datasets (e.g., Alpaca's 52k data) surprisingly contain many low-quality instances with incorrect or irrelevant responses, which are misleading and detrimental to IFT. In this paper, we propose a simple and effective data selection strategy that automatically identifies and filters out low-quality data using a strong LLM (e.g., ChatGPT). To this end, we introduce AlpaGasus, which is finetuned on only 9k high-quality data filtered from the 52k Alpaca data. AlpaGasus significantly outperforms the original Alpaca as evaluated by GPT-4 on multiple test sets and the controlled human evaluation. Its 13B variant matches $&gt;90\%$ performance of its teacher LLM (i.e., Text-Davinci-003 generating the 52k data) on test tasks. It also provides 5.7x faster training, reducing the training time for a 7B variant from 80 minutes (
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23558;&#25991;&#26412;&#21644;&#35270;&#35273;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#30340;&#20869;&#23481;&#12290;&#26368;&#26377;&#25928;&#30340;&#27169;&#22411;&#26159;RoBERTa&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#21644;MaxViT&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#34701;&#21512;&#27169;&#22411;&#65292;&#20934;&#30830;&#29575;&#21644;F1&#20998;&#25968;&#20998;&#21035;&#36798;&#21040;95.9%&#21644;0.959&#12290;</title><link>http://arxiv.org/abs/2307.06775</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#22411;&#30340;&#19982;&#24179;&#21488;&#26080;&#20851;&#30340;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#35782;&#21035;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
A Novel Site-Agnostic Multimodal Deep Learning Model to Identify Pro-Eating Disorder Content on Social Media. (arXiv:2307.06775v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23558;&#25991;&#26412;&#21644;&#35270;&#35273;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#30340;&#20869;&#23481;&#12290;&#26368;&#26377;&#25928;&#30340;&#27169;&#22411;&#26159;RoBERTa&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#21644;MaxViT&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#34701;&#21512;&#27169;&#22411;&#65292;&#20934;&#30830;&#29575;&#21644;F1&#20998;&#25968;&#20998;&#21035;&#36798;&#21040;95.9%&#21644;0.959&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#39278;&#39135;&#32010;&#20081;&#30340;&#35786;&#26029;&#21644;&#19982;&#20043;&#30456;&#20851;&#30340;&#27515;&#20129;&#25968;&#37327;&#22823;&#24133;&#22686;&#21152;&#65292;&#23588;&#20854;&#26159;&#22312;&#26032;&#20896;&#30123;&#24773;&#26399;&#38388;&#12290;&#36825;&#31181;&#24040;&#22823;&#22686;&#38271;&#37096;&#20998;&#26469;&#28304;&#20110;&#30123;&#24773;&#30340;&#21387;&#21147;&#65292;&#20294;&#20063;&#19982;&#31038;&#20132;&#23186;&#20307;&#30340;&#26292;&#38706;&#22686;&#21152;&#26377;&#20851;&#65292;&#31038;&#20132;&#23186;&#20307;&#19978;&#20805;&#26021;&#30528;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#30340;&#20869;&#23481;&#12290;&#36825;&#20123;&#20869;&#23481;&#21487;&#20197;&#35825;&#21457;&#35266;&#30475;&#32773;&#30340;&#39278;&#39135;&#32010;&#20081;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#21019;&#24314;&#19968;&#20010;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#33021;&#22815;&#22522;&#20110;&#35270;&#35273;&#21644;&#25991;&#26412;&#25968;&#25454;&#30340;&#32452;&#21512;&#21028;&#26029;&#32473;&#23450;&#30340;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#26159;&#21542;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#12290;&#20174;Twitter&#25910;&#38598;&#20102;&#19968;&#20010;&#24102;&#26377;&#26631;&#31614;&#30340;&#25512;&#25991;&#25968;&#25454;&#38598;&#65292;&#23545;&#20854;&#36827;&#34892;&#20102;&#21313;&#20108;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;&#26681;&#25454;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#26368;&#26377;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26159;RoBERTa&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#21644;MaxViT&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22411;&#65292;&#20934;&#30830;&#29575;&#21644;F1&#20998;&#25968;&#20998;&#21035;&#36798;&#21040;95.9%&#21644;0.959&#12290;RoBERTa&#21644;MaxViT&#34701;&#21512;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the last decade, there has been a vast increase in eating disorder diagnoses and eating disorder-attributed deaths, reaching their zenith during the Covid-19 pandemic. This immense growth derived in part from the stressors of the pandemic but also from increased exposure to social media, which is rife with content that promotes eating disorders. Such content can induce eating disorders in viewers. This study aimed to create a multimodal deep learning model capable of determining whether a given social media post promotes eating disorders based on a combination of visual and textual data. A labeled dataset of Tweets was collected from Twitter, upon which twelve deep learning models were trained and tested. Based on model performance, the most effective deep learning model was the multimodal fusion of the RoBERTa natural language processing model and the MaxViT image classification model, attaining accuracy and F1 scores of 95.9% and 0.959 respectively. The RoBERTa and MaxViT fusion
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#31232;&#30095;&#27169;&#22359;&#28608;&#27963; (SMA) &#26426;&#21046;&#65292;&#29992;&#20110;&#39640;&#25928;&#30340;&#24207;&#21015;&#24314;&#27169;&#12290;&#36825;&#31181;&#26426;&#21046;&#21487;&#20197;&#21160;&#24577;&#22320;&#31232;&#30095;&#28608;&#27963;&#24207;&#21015;&#20803;&#32032;&#30340;&#23376;&#27169;&#22359;&#65292;&#20943;&#23569;&#35745;&#31639;&#21644;&#20869;&#23384;&#28040;&#32791;&#12290;</title><link>http://arxiv.org/abs/2306.11197</link><description>&lt;p&gt;
&#31232;&#30095;&#27169;&#22359;&#28608;&#27963;&#29992;&#20110;&#39640;&#25928;&#30340;&#24207;&#21015;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Sparse Modular Activation for Efficient Sequence Modeling. (arXiv:2306.11197v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11197
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#31232;&#30095;&#27169;&#22359;&#28608;&#27963; (SMA) &#26426;&#21046;&#65292;&#29992;&#20110;&#39640;&#25928;&#30340;&#24207;&#21015;&#24314;&#27169;&#12290;&#36825;&#31181;&#26426;&#21046;&#21487;&#20197;&#21160;&#24577;&#22320;&#31232;&#30095;&#28608;&#27963;&#24207;&#21015;&#20803;&#32032;&#30340;&#23376;&#27169;&#22359;&#65292;&#20943;&#23569;&#35745;&#31639;&#21644;&#20869;&#23384;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411; (SSM) &#22312;&#21508;&#31181;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#24456;&#24378;&#30340;&#24615;&#33021;&#65292;&#22240;&#20026;&#23427;&#20204;&#26377;&#25928;&#22320;&#32534;&#30721;&#20102;&#24490;&#29615;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#22312;&#26356;&#32508;&#21512;&#30340;&#20219;&#21153;&#20013;&#65292;&#22914;&#35821;&#35328;&#24314;&#27169;&#21644;&#26426;&#22120;&#32763;&#35793;&#20013;&#65292;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#20173;&#28982;&#20248;&#20110;SSM&#12290;&#21516;&#26102;&#20351;&#29992;SSM&#21644;&#33258;&#27880;&#24847;&#21147;&#30340;&#28151;&#21512;&#27169;&#22411;&#36890;&#24120;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#65292;&#20294;&#24403;&#21069;&#26041;&#27861;&#23558;&#27880;&#24847;&#21147;&#27169;&#22359;&#38745;&#24577;&#19988;&#22343;&#21248;&#22320;&#24212;&#29992;&#20110;&#36755;&#20837;&#24207;&#21015;&#20013;&#30340;&#25152;&#26377;&#20803;&#32032;&#65292;&#23548;&#33268;&#20102;&#36136;&#37327;&#21644;&#25928;&#29575;&#20043;&#38388;&#30340;&#27425;&#20248;&#26435;&#34913;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31232;&#30095;&#27169;&#22359;&#28608;&#27963; (SMA)&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#29992;&#26426;&#21046;&#65292;&#20351;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#20197;&#21487;&#24494;&#20998;&#30340;&#26041;&#24335;&#31232;&#30095;&#22320;&#21160;&#24577;&#28608;&#27963;&#24207;&#21015;&#20803;&#32032;&#30340;&#23376;&#27169;&#22359;&#12290;&#36890;&#36807;&#20801;&#35768;&#27599;&#20010;&#20803;&#32032;&#36339;&#36807;&#38750;&#28608;&#27963;&#30340;&#23376;&#27169;&#22359;&#65292;SMA&#21487;&#20197;&#22312;&#24207;&#21015;&#24314;&#27169;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#38454;&#27573;&#38477;&#20302;&#35745;&#31639;&#21644;&#20869;&#23384;&#28040;&#32791;&#12290;&#20316;&#20026;SMA&#30340;&#19968;&#20010;&#29305;&#23450;&#23454;&#20363;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linear State Space Models (SSMs) have demonstrated strong performance in a variety of sequence modeling tasks due to their efficient encoding of the recurrent structure. However, in more comprehensive tasks like language modeling and machine translation, self-attention-based models still outperform SSMs. Hybrid models employing both SSM and self-attention generally show promising performance, but current approaches apply attention modules statically and uniformly to all elements in the input sequences, leading to sub-optimal quality-efficiency trade-offs. In this work, we introduce Sparse Modular Activation (SMA), a general mechanism enabling neural networks to sparsely and dynamically activate sub-modules for sequence elements in a differentiable manner. Through allowing each element to skip non-activated sub-modules, SMA reduces computation and memory consumption at both training and inference stages of sequence modeling. As a specific instantiation of SMA, we design a novel neural a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#33021;&#37327;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;EBM&#26694;&#26550;&#65292;&#36890;&#36807;&#26356;&#26032;&#21644;&#36716;&#31227;&#19978;&#19979;&#25991;&#21521;&#37327;&#65292;&#38544;&#24335;&#26368;&#23567;&#21270;&#33021;&#37327;&#20989;&#25968;&#30340;&#23884;&#22871;&#23618;&#27425;&#65292;&#20248;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#20041;&#23545;&#40784;&#38382;&#39064;&#65292;&#23454;&#29616;&#38646;&#26679;&#26412;&#32452;&#21512;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2306.09869</link><description>&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#33021;&#37327;&#20132;&#21449;&#27880;&#24847;&#21147;&#29992;&#20110;&#36125;&#21494;&#26031;&#19978;&#19979;&#25991;&#26356;&#26032;
&lt;/p&gt;
&lt;p&gt;
Energy-Based Cross Attention for Bayesian Context Update in Text-to-Image Diffusion Models. (arXiv:2306.09869v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#33021;&#37327;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;EBM&#26694;&#26550;&#65292;&#36890;&#36807;&#26356;&#26032;&#21644;&#36716;&#31227;&#19978;&#19979;&#25991;&#21521;&#37327;&#65292;&#38544;&#24335;&#26368;&#23567;&#21270;&#33021;&#37327;&#20989;&#25968;&#30340;&#23884;&#22871;&#23618;&#27425;&#65292;&#20248;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#20041;&#23545;&#40784;&#38382;&#39064;&#65292;&#23454;&#29616;&#38646;&#26679;&#26412;&#32452;&#21512;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#29983;&#25104;&#30340;&#22270;&#20687;&#26377;&#26102;&#26080;&#27861;&#25429;&#25417;&#21040;&#25991;&#26412;&#25552;&#31034;&#30340;&#39044;&#26399;&#35821;&#20041;&#20869;&#23481;&#65292;&#36825;&#31181;&#29616;&#35937;&#36890;&#24120;&#34987;&#31216;&#20026;&#35821;&#20041;&#38169;&#20301;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65288;EBM&#65289;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;&#21435;&#22122;&#33258;&#32534;&#30721;&#22120;&#30340;&#27599;&#20010;&#20132;&#21449;&#27880;&#24847;&#21147;&#23618;&#20013;&#21046;&#23450;&#28508;&#22312;&#22270;&#20687;&#34920;&#31034;&#21644;&#25991;&#26412;&#23884;&#20837;&#30340;EBM&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#33719;&#24471;&#19978;&#19979;&#25991;&#21521;&#37327;&#30340;&#23545;&#25968;&#21518;&#39564;&#26799;&#24230;&#65292;&#21487;&#20197;&#26356;&#26032;&#21644;&#36716;&#31227;&#21040;&#21518;&#32493;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#23618;&#65292;&#20174;&#32780;&#38544;&#24335;&#22320;&#26368;&#23567;&#21270;&#23884;&#22871;&#23618;&#27425;&#30340;&#33021;&#37327;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#28508;&#22312;EBMs&#36824;&#20801;&#35768;&#38646;&#26679;&#26412;&#32452;&#21512;&#29983;&#25104;&#65292;&#21363;&#36890;&#36807;&#19981;&#21516;&#19978;&#19979;&#25991;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#36755;&#20986;&#30340;&#32447;&#24615;&#32452;&#21512;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;&#21508;&#31181;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#24182;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#25991;&#26412;&#25552;&#31034;&#21644;&#29983;&#25104;&#22270;&#20687;&#20043;&#38388;&#30340;&#35821;&#20041;&#38169;&#20301;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the remarkable performance of text-to-image diffusion models in image generation tasks, recent studies have raised the issue that generated images sometimes cannot capture the intended semantic contents of the text prompts, which phenomenon is often called semantic misalignment. To address this, here we present a novel energy-based model (EBM) framework. Specifically, we first formulate EBMs of latent image representations and text embeddings in each cross-attention layer of the denoising autoencoder. Then, we obtain the gradient of the log posterior of context vectors, which can be updated and transferred to the subsequent cross-attention layer, thereby implicitly minimizing a nested hierarchy of energy functions. Our latent EBMs further allow zero-shot compositional generation as a linear combination of cross-attention outputs from different contexts. Using extensive experiments, we demonstrate that the proposed method is highly effective in handling various image generation 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#32771;&#23448;&#65292;&#21487;&#20197;&#26080;&#21442;&#32771;&#26041;&#24335;&#35780;&#20272;&#31572;&#26696;&#12290;&#36825;&#20010;&#26694;&#26550;&#35299;&#20915;&#20102;&#36807;&#21435;&#22522;&#20934;&#27979;&#35797;&#27969;&#31243;&#20013;&#30340;&#27979;&#35797;&#27844;&#28431;&#21644;&#35780;&#20272;&#33258;&#21160;&#21270;&#38382;&#39064;&#65292;&#24182;&#20801;&#35768;&#26131;&#20110;&#25193;&#23637;&#65292;&#21487;&#20197;&#37319;&#29992;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#32771;&#23448;&#12290;</title><link>http://arxiv.org/abs/2306.04181</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#8220;&#32771;&#23448;&#8221;&#23545;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Foundation Models with Language-Model-as-an-Examiner. (arXiv:2306.04181v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#32771;&#23448;&#65292;&#21487;&#20197;&#26080;&#21442;&#32771;&#26041;&#24335;&#35780;&#20272;&#31572;&#26696;&#12290;&#36825;&#20010;&#26694;&#26550;&#35299;&#20915;&#20102;&#36807;&#21435;&#22522;&#20934;&#27979;&#35797;&#27969;&#31243;&#20013;&#30340;&#27979;&#35797;&#27844;&#28431;&#21644;&#35780;&#20272;&#33258;&#21160;&#21270;&#38382;&#39064;&#65292;&#24182;&#20801;&#35768;&#26131;&#20110;&#25193;&#23637;&#65292;&#21487;&#20197;&#37319;&#29992;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#32771;&#23448;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#24314;&#31435;&#20102;&#35768;&#22810;&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;&#22522;&#30784;&#27169;&#22411;&#22312;&#24320;&#25918;&#24335;&#38382;&#31572;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#23427;&#26159;&#27979;&#35797;&#27169;&#22411;&#29702;&#35299;&#21644;&#29983;&#25104;&#35821;&#35328;&#30340;&#33021;&#21147;&#30340;&#20840;&#38754;&#27979;&#35797;&#12290;&#22823;&#22810;&#25968;&#24037;&#20316;&#38598;&#20013;&#22312;&#25552;&#20986;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#28982;&#32780;&#65292;&#25105;&#20204;&#22312;&#20043;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#27969;&#31243;&#20013;&#30475;&#21040;&#20102;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#65292;&#21363;&#27979;&#35797;&#27844;&#28431;&#21644;&#35780;&#20272;&#33258;&#21160;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#32771;&#23448;&#65288;LMAE&#65289;&#65292;&#20854;&#20013;LM&#20316;&#20026;&#30693;&#35782;&#28170;&#21338;&#30340;&#32771;&#23448;&#65292;&#26681;&#25454;&#20854;&#30693;&#35782;&#21046;&#23450;&#38382;&#39064;&#24182;&#20197;&#26080;&#21442;&#32771;&#26041;&#24335;&#35780;&#20272;&#31572;&#26696;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20801;&#35768;&#26131;&#20110;&#25193;&#23637;&#65292;&#22240;&#20026;&#21487;&#20197;&#37319;&#29992;&#21508;&#31181;LM&#20316;&#20026;&#32771;&#23448;&#65292;&#24182;&#19988;&#21487;&#20197;&#19981;&#26029;&#26356;&#26032;&#38382;&#39064;&#65292;&#32473;&#20104;&#26356;&#22810;&#26679;&#21270;&#30340;&#35302;&#21457;&#20027;&#39064;&#12290;&#20026;&#20102;&#26356;&#20840;&#38754;&#21644;&#20844;&#27491;&#22320;&#35780;&#20272;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19977;&#20010;&#31574;&#30053;&#65306;&#65288;1&#65289;&#25105;&#20204;&#25351;&#31034;LM&#32771;&#23448;&#22312;&#35768;&#22810;&#39046;&#22495;&#29983;&#25104;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous benchmarks have been established to assess the performance of foundation models on open-ended question answering, which serves as a comprehensive test of a model's ability to understand and generate language in a manner similar to humans. Most of these works focus on proposing new datasets, however, we see two main issues within previous benchmarking pipelines, namely testing leakage and evaluation automation. In this paper, we propose a novel benchmarking framework, Language-Model-as-an-Examiner, where the LM serves as a knowledgeable examiner that formulates questions based on its knowledge and evaluates responses in a reference-free manner. Our framework allows for effortless extensibility as various LMs can be adopted as the examiner, and the questions can be constantly updated given more diverse trigger topics. For a more comprehensive and equitable evaluation, we devise three strategies: (1) We instruct the LM examiner to generate questions across a multitude of domains 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32467;&#21512;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21644;&#21508;&#35821;&#35328;&#23618;&#38754;&#19978;&#30340;&#29305;&#24449;&#65292;&#24320;&#21457;&#20986;&#19968;&#31181;&#20808;&#36827;&#30340;&#22303;&#32819;&#20854;&#25991;&#26412;&#21487;&#35835;&#24615;&#24037;&#20855;&#65292;&#21457;&#29616;&#20102;&#24433;&#21709;&#22303;&#32819;&#20854;&#25991;&#26412;&#21487;&#35835;&#24615;&#30340;&#20851;&#38190;&#35821;&#35328;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2306.03774</link><description>&lt;p&gt;
&#20351;&#29992;&#28151;&#21512;&#35821;&#35328;&#29305;&#24449;&#25552;&#39640;&#22303;&#32819;&#20854;&#25991;&#26412;&#21487;&#35835;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Exploring Hybrid Linguistic Features for Turkish Text Readability. (arXiv:2306.03774v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32467;&#21512;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21644;&#21508;&#35821;&#35328;&#23618;&#38754;&#19978;&#30340;&#29305;&#24449;&#65292;&#24320;&#21457;&#20986;&#19968;&#31181;&#20808;&#36827;&#30340;&#22303;&#32819;&#20854;&#25991;&#26412;&#21487;&#35835;&#24615;&#24037;&#20855;&#65292;&#21457;&#29616;&#20102;&#24433;&#21709;&#22303;&#32819;&#20854;&#25991;&#26412;&#21487;&#35835;&#24615;&#30340;&#20851;&#38190;&#35821;&#35328;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#23545;&#22303;&#32819;&#20854;&#25991;&#26412;&#30340;&#33258;&#21160;&#21487;&#35835;&#24615;&#35780;&#20272;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#12290;&#25105;&#20204;&#32467;&#21512;&#20102;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21644;&#35789;&#27719;&#12289;&#24418;&#24577;&#21477;&#27861;&#12289;&#35821;&#27861;&#21644;&#35805;&#35821;&#27700;&#24179;&#30340;&#35821;&#35328;&#29305;&#24449;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#20808;&#36827;&#30340;&#21487;&#35835;&#24615;&#24037;&#20855;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20256;&#32479;&#21487;&#35835;&#24615;&#20844;&#24335;&#19982;&#29616;&#20195;&#33258;&#21160;&#26041;&#27861;&#30340;&#25928;&#26524;&#65292;&#24182;&#30830;&#23450;&#20102;&#24433;&#21709;&#22303;&#32819;&#20854;&#25991;&#26412;&#21487;&#35835;&#24615;&#30340;&#20851;&#38190;&#35821;&#35328;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the first comprehensive study on automatic readability assessment of Turkish texts. We combine state-of-the-art neural network models with linguistic features at lexical, morphosyntactic, syntactic and discourse levels to develop an advanced readability tool. We evaluate the effectiveness of traditional readability formulas compared to modern automated methods and identify key linguistic features that determine the readability of Turkish texts.
&lt;/p&gt;</description></item><item><title>SciLit &#26159;&#19968;&#20010;&#33021;&#22815;&#33258;&#21160;&#26816;&#32034;&#12289;&#25688;&#35201;&#21644;&#24341;&#29992;&#30456;&#20851;&#35770;&#25991;&#30340;&#24179;&#21488;&#65292;&#23427;&#21487;&#20197;&#20174;&#25968;&#30334;&#19975;&#31687;&#25991;&#29486;&#20013;&#39640;&#25928;&#22320;&#25512;&#33616;&#35770;&#25991;&#65292;&#24182;&#25552;&#20379;&#20855;&#26377;&#19978;&#19979;&#25991;&#20851;&#32852;&#30340;&#24341;&#29992;&#21477;&#23376;&#12290;</title><link>http://arxiv.org/abs/2306.03535</link><description>&lt;p&gt;
SciLit: &#19968;&#31181;&#32852;&#21512;&#31185;&#23398;&#25991;&#29486;&#21457;&#29616;&#12289;&#25688;&#35201;&#21644;&#24341;&#25991;&#29983;&#25104;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
SciLit: A Platform for Joint Scientific Literature Discovery, Summarization and Citation Generation. (arXiv:2306.03535v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03535
&lt;/p&gt;
&lt;p&gt;
SciLit &#26159;&#19968;&#20010;&#33021;&#22815;&#33258;&#21160;&#26816;&#32034;&#12289;&#25688;&#35201;&#21644;&#24341;&#29992;&#30456;&#20851;&#35770;&#25991;&#30340;&#24179;&#21488;&#65292;&#23427;&#21487;&#20197;&#20174;&#25968;&#30334;&#19975;&#31687;&#25991;&#29486;&#20013;&#39640;&#25928;&#22320;&#25512;&#33616;&#35770;&#25991;&#65292;&#24182;&#25552;&#20379;&#20855;&#26377;&#19978;&#19979;&#25991;&#20851;&#32852;&#30340;&#24341;&#29992;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#20889;&#20316;&#28041;&#21450;&#26816;&#32034;&#12289;&#24635;&#32467;&#21644;&#24341;&#29992;&#30456;&#20851;&#35770;&#25991;&#65292;&#36825;&#22312;&#22823;&#35268;&#27169;&#21644;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#20013;&#21487;&#33021;&#26159;&#32791;&#26102;&#30340;&#36807;&#31243;&#12290;&#36890;&#36807;&#23558;&#36825;&#20123;&#36807;&#31243;&#30456;&#20114;&#25805;&#20316;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702; (NLP) &#25552;&#20379;&#20102;&#21019;&#24314;&#31471;&#21040;&#31471;&#36741;&#21161;&#20889;&#20316;&#24037;&#20855;&#30340;&#26426;&#20250;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SciLit&#65292;&#36825;&#26159;&#19968;&#20010;&#27969;&#27700;&#32447;&#65292;&#21487;&#20197;&#33258;&#21160;&#25512;&#33616;&#30456;&#20851;&#35770;&#25991;&#65292;&#25552;&#21462;&#20142;&#28857;&#65292;&#24182;&#24314;&#35758;&#19968;&#20010;&#24341;&#29992;&#21477;&#23376;&#20316;&#20026;&#35770;&#25991;&#30340;&#24341;&#29992;&#65292;&#32771;&#34385;&#21040;&#29992;&#25143;&#25552;&#20379;&#30340;&#19978;&#19979;&#25991;&#21644;&#20851;&#38190;&#35789;&#12290;SciLit&#21487;&#20197;&#39640;&#25928;&#22320;&#20174;&#25968;&#30334;&#19975;&#31687;&#35770;&#25991;&#30340;&#22823;&#22411;&#25968;&#25454;&#24211;&#20013;&#25512;&#33616;&#35770;&#25991;&#65292;&#20351;&#29992;&#20004;&#38454;&#27573;&#30340;&#39044;&#21462;&#21644;&#37325;&#26032;&#25490;&#21517;&#25991;&#29486;&#25628;&#32034;&#31995;&#32479;&#65292;&#28789;&#27963;&#22788;&#29702;&#35770;&#25991;&#25968;&#25454;&#24211;&#30340;&#28155;&#21152;&#21644;&#21024;&#38500;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26041;&#20415;&#30340;&#29992;&#25143;&#30028;&#38754;&#65292;&#26174;&#31034;&#25512;&#33616;&#30340;&#35770;&#25991;&#20316;&#20026;&#25688;&#35201;&#65292;&#24182;&#25552;&#20379;&#19982;&#25552;&#20379;&#30340;&#19978;&#19979;&#25991;&#23545;&#40784;&#24182;&#25552;&#21040;&#25152;&#36873;&#20851;&#38190;&#35789;&#30340;&#25688;&#35201;&#24341;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scientific writing involves retrieving, summarizing, and citing relevant papers, which can be time-consuming processes in large and rapidly evolving fields. By making these processes inter-operable, natural language processing (NLP) provides opportunities for creating end-to-end assistive writing tools. We propose SciLit, a pipeline that automatically recommends relevant papers, extracts highlights, and suggests a reference sentence as a citation of a paper, taking into consideration the user-provided context and keywords. SciLit efficiently recommends papers from large databases of hundreds of millions of papers using a two-stage pre-fetching and re-ranking literature search system that flexibly deals with addition and removal of a paper database. We provide a convenient user interface that displays the recommended papers as extractive summaries and that offers abstractively-generated citing sentences which are aligned with the provided context and which mention the chosen keyword(s).
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#23545;&#33258;&#21160;&#29983;&#25104;&#30340;&#24773;&#24863;&#26354;&#32447;&#36827;&#34892;&#20102;&#31995;&#32479;&#21644;&#23450;&#37327;&#30340;&#35780;&#20272;&#65292;&#24182;&#27604;&#36739;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#35789;&#20856;&#26041;&#27861;&#20004;&#31181;&#29983;&#25104;&#24773;&#24863;&#26354;&#32447;&#30340;&#26041;&#24335;&#12290;&#36890;&#36807;&#22312;&#19981;&#21516;&#35821;&#35328;&#30340;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#34429;&#28982;&#35789;&#20856;&#26041;&#27861;&#22312;&#23454;&#20363;&#32423;&#24773;&#24863;&#20998;&#31867;&#26041;&#38754;&#34920;&#29616;&#36739;&#24046;&#65292;&#20294;&#22312;&#32858;&#21512;&#20449;&#24687;&#26102;&#29983;&#25104;&#24773;&#24863;&#26354;&#32447;&#30340;&#20934;&#30830;&#24615;&#38750;&#24120;&#39640;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36890;&#36807;&#33258;&#21160;&#32763;&#35793;&#33521;&#35821;&#24773;&#24863;&#35789;&#20856;&#65292;&#21487;&#20197;&#22312;&#36164;&#28304;&#36739;&#23569;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#24773;&#24863;&#26354;&#32447;&#12290;</title><link>http://arxiv.org/abs/2306.02213</link><description>&lt;p&gt;
&#36328;&#35821;&#35328;&#35780;&#20272;&#24773;&#24863;&#26354;&#32447;&#65306;&#24357;&#21512;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#20840;&#29699;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Evaluating Emotion Arcs Across Languages: Bridging the Global Divide in Sentiment Analysis. (arXiv:2306.02213v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#23545;&#33258;&#21160;&#29983;&#25104;&#30340;&#24773;&#24863;&#26354;&#32447;&#36827;&#34892;&#20102;&#31995;&#32479;&#21644;&#23450;&#37327;&#30340;&#35780;&#20272;&#65292;&#24182;&#27604;&#36739;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#35789;&#20856;&#26041;&#27861;&#20004;&#31181;&#29983;&#25104;&#24773;&#24863;&#26354;&#32447;&#30340;&#26041;&#24335;&#12290;&#36890;&#36807;&#22312;&#19981;&#21516;&#35821;&#35328;&#30340;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#34429;&#28982;&#35789;&#20856;&#26041;&#27861;&#22312;&#23454;&#20363;&#32423;&#24773;&#24863;&#20998;&#31867;&#26041;&#38754;&#34920;&#29616;&#36739;&#24046;&#65292;&#20294;&#22312;&#32858;&#21512;&#20449;&#24687;&#26102;&#29983;&#25104;&#24773;&#24863;&#26354;&#32447;&#30340;&#20934;&#30830;&#24615;&#38750;&#24120;&#39640;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36890;&#36807;&#33258;&#21160;&#32763;&#35793;&#33521;&#35821;&#24773;&#24863;&#35789;&#20856;&#65292;&#21487;&#20197;&#22312;&#36164;&#28304;&#36739;&#23569;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#24773;&#24863;&#26354;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#26354;&#32447;&#25429;&#25417;&#20102;&#19968;&#20010;&#20154;&#65288;&#25110;&#19968;&#20010;&#32676;&#20307;&#65289;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#24773;&#24863;&#29366;&#24577;&#12290;&#23427;&#20204;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#24037;&#19994;&#21644;&#30740;&#31350;&#39046;&#22495;&#65307;&#28982;&#32780;&#65292;&#23545;&#20110;&#33258;&#21160;&#29983;&#25104;&#30340;&#24773;&#24863;&#26354;&#32447;&#30340;&#35780;&#20272;&#24037;&#20316;&#24456;&#23569;&#12290;&#36825;&#26159;&#22240;&#20026;&#24314;&#31435;&#30495;&#23454;&#65288;&#40644;&#37329;&#65289;&#24773;&#24863;&#26354;&#32447;&#30340;&#22256;&#38590;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#39318;&#27425;&#23545;&#33258;&#21160;&#29983;&#25104;&#30340;&#24773;&#24863;&#26354;&#32447;&#36827;&#34892;&#20102;&#31995;&#32479;&#21644;&#23450;&#37327;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#36824;&#27604;&#36739;&#20102;&#20004;&#31181;&#24120;&#35265;&#30340;&#24773;&#24863;&#26354;&#32447;&#29983;&#25104;&#26041;&#27861;&#65306;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#21644;&#20165;&#35789;&#20856;&#65288;LexO&#65289;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;9&#31181;&#35821;&#35328;&#30340;18&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#34920;&#26126;&#23613;&#31649;&#22312;&#23454;&#20363;&#32423;&#24773;&#24863;&#20998;&#31867;&#26041;&#38754;&#34920;&#29616;&#24046;&#65292;&#20294;LexO&#26041;&#27861;&#22312;&#20174;&#25968;&#30334;&#20010;&#23454;&#20363;&#20013;&#32858;&#21512;&#20449;&#24687;&#26102;&#29983;&#25104;&#24773;&#24863;&#26354;&#32447;&#30340;&#20934;&#30830;&#24615;&#38750;&#24120;&#39640;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23545;&#20845;&#31181;&#38750;&#27954;&#22303;&#33879;&#35821;&#35328;&#20197;&#21450;&#38463;&#25289;&#20271;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#33521;&#35821;&#24773;&#24863;&#35789;&#20856;&#30340;&#33258;&#21160;&#32763;&#35793;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#24773;&#24863;&#26354;&#32447;&#65292;&#32780;&#36164;&#28304;&#24320;&#38144;&#30456;&#23545;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion arcs capture how an individual (or a population) feels over time. They are widely used in industry and research; however, there is little work on evaluating the automatically generated arcs. This is because of the difficulty of establishing the true (gold) emotion arc. Our work, for the first time, systematically and quantitatively evaluates automatically generated emotion arcs. We also compare two common ways of generating emotion arcs: Machine-Learning (ML) models and Lexicon-Only (LexO) methods. By running experiments on 18 diverse datasets in 9 languages, we show that despite being markedly poor at instance level emotion classification, LexO methods are highly accurate at generating emotion arcs when aggregating information from hundreds of instances. We also show, through experiments on six indigenous African languages, as well as Arabic, and Spanish, that automatic translations of English emotion lexicons can be used to generate high-quality emotion arcs in less-resource 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#29702;&#35770;&#23618;&#38754;&#25506;&#31350;&#20102;&#24102;&#26377;&#8220;&#24605;&#32500;&#38142;&#8221;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#22522;&#26412;&#25968;&#23398;&#21644;&#20915;&#31574;&#38382;&#39064;&#20013;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#33258;&#22238;&#24402;Transformer&#22823;&#23567;&#24658;&#23450;&#21363;&#21487;&#35299;&#20915;&#20219;&#21153;&#65292;&#25581;&#31034;&#20102;&#8220;&#24605;&#32500;&#38142;&#8221;&#25552;&#31034;&#30340;&#32972;&#21518;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.15408</link><description>&lt;p&gt;
&#20174;&#29702;&#35770;&#35282;&#24230;&#25581;&#31034;&#8220;&#24605;&#32500;&#38142;&#8221;&#32972;&#21518;&#30340;&#22885;&#31192;
&lt;/p&gt;
&lt;p&gt;
Towards Revealing the Mystery behind Chain of Thought: a Theoretical Perspective. (arXiv:2305.15408v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15408
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#29702;&#35770;&#23618;&#38754;&#25506;&#31350;&#20102;&#24102;&#26377;&#8220;&#24605;&#32500;&#38142;&#8221;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#22522;&#26412;&#25968;&#23398;&#21644;&#20915;&#31574;&#38382;&#39064;&#20013;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#33258;&#22238;&#24402;Transformer&#22823;&#23567;&#24658;&#23450;&#21363;&#21487;&#35299;&#20915;&#20219;&#21153;&#65292;&#25581;&#31034;&#20102;&#8220;&#24605;&#32500;&#38142;&#8221;&#25552;&#31034;&#30340;&#32972;&#21518;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;"&#24605;&#32500;&#38142;"&#25552;&#31034;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#28041;&#21450;&#25968;&#23398;&#25110;&#25512;&#29702;&#30340;&#22797;&#26434;&#20219;&#21153;&#20013;&#12290;&#23613;&#31649;&#33719;&#24471;&#20102;&#24040;&#22823;&#30340;&#23454;&#35777;&#25104;&#21151;&#65292;&#20294;&#8220;&#24605;&#32500;&#38142;&#8221;&#32972;&#21518;&#30340;&#26426;&#21046;&#20197;&#21450;&#23427;&#22914;&#20309;&#37322;&#25918;LLMs&#30340;&#28508;&#21147;&#20173;&#28982;&#26159;&#31070;&#31192;&#30340;&#12290;&#26412;&#25991;&#39318;&#27425;&#20174;&#29702;&#35770;&#19978;&#22238;&#31572;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#24102;&#26377;&#8220;&#24605;&#32500;&#38142;&#8221;&#22312;&#35299;&#20915;&#22522;&#26412;&#25968;&#23398;&#21644;&#20915;&#31574;&#38382;&#39064;&#20013;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#32473;&#20986;&#19968;&#20010;&#19981;&#21487;&#33021;&#30340;&#32467;&#26524;&#65292;&#34920;&#26126;&#20219;&#20309;&#26377;&#38480;&#28145;&#24230;&#30340;Transformer&#37117;&#19981;&#33021;&#30452;&#25509;&#36755;&#20986;&#27491;&#30830;&#30340;&#22522;&#26412;&#31639;&#26415;/&#26041;&#31243;&#20219;&#21153;&#30340;&#31572;&#26696;&#65292;&#38500;&#38750;&#27169;&#22411;&#22823;&#23567;&#38543;&#30528;&#36755;&#20837;&#38271;&#24230;&#30340;&#22686;&#21152;&#21576;&#36229;&#22810;&#39033;&#24335;&#22686;&#38271;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#36890;&#36807;&#26500;&#36896;&#35777;&#26126;&#65292;&#22823;&#23567;&#24658;&#23450;&#30340;&#33258;&#22238;&#24402;Transformer&#36275;&#20197;&#36890;&#36807;&#20351;&#29992;&#24120;&#29992;&#30340;&#25968;&#23398;&#35821;&#35328;&#24418;&#24335;&#29983;&#25104;&#8220;&#24605;&#32500;&#38142;&#8221;&#25512;&#23548;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have discovered that Chain-of-Thought prompting (CoT) can dramatically improve the performance of Large Language Models (LLMs), particularly when dealing with complex tasks involving mathematics or reasoning. Despite the enormous empirical success, the underlying mechanisms behind CoT and how it unlocks the potential of LLMs remain elusive. In this paper, we take a first step towards theoretically answering these questions. Specifically, we examine the capacity of LLMs with CoT in solving fundamental mathematical and decision-making problems. We start by giving an impossibility result showing that any bounded-depth Transformer cannot directly output correct answers for basic arithmetic/equation tasks unless the model size grows super-polynomially with respect to the input length. In contrast, we then prove by construction that autoregressive Transformers of a constant size suffice to solve both tasks by generating CoT derivations using a commonly-used math language forma
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;OOD&#26696;&#20363;&#27979;&#35797;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26222;&#36941;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#26032;&#30340;&#25512;&#29702;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#25506;&#31350;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#25512;&#24191;&#21040;&#26356;&#22797;&#26434;&#30340;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2305.15269</link><description>&lt;p&gt;
&#20351;&#29992;OOD&#26696;&#20363;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26222;&#36941;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Testing the General Deductive Reasoning Capacity of Large Language Models Using OOD Examples. (arXiv:2305.15269v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15269
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;OOD&#26696;&#20363;&#27979;&#35797;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26222;&#36941;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#26032;&#30340;&#25512;&#29702;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#25506;&#31350;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#25512;&#24191;&#21040;&#26356;&#22797;&#26434;&#30340;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#35777;&#26126;&#31354;&#38388;&#30340;&#24222;&#22823;&#65292;&#20219;&#20309;&#20855;&#26377;&#26222;&#36941;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#30340;&#27169;&#22411;&#24517;&#39035;&#33021;&#22815;&#25512;&#29702;&#26356;&#22797;&#26434;&#30340;&#35777;&#26126;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#32473;&#23450;&#25512;&#29702;&#38142;&#26465;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#26576;&#20123;&#25277;&#35937;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20027;&#35201;&#26159;&#22312;&#20351;&#29992;&#33707;&#24503;&#26031;&#22374;&#26031;&#25110;&#29305;&#23450;&#22823;&#23567;&#30340;&#35777;&#26126;&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#24182;&#19988;&#19982;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#20998;&#24067;&#30456;&#21516;&#12290;&#20026;&#20102;&#34913;&#37327;LLM&#30340;&#26222;&#36941;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#24191;&#27867;&#30340;&#28436;&#32462;&#35268;&#21017;&#65292;&#24182;&#27979;&#37327;&#23427;&#20204;&#25512;&#29702;&#26356;&#22797;&#26434;&#35777;&#26126;&#30340;&#33021;&#21147;&#65292;&#26041;&#27861;&#21253;&#25324;&#28145;&#24230;&#27867;&#21270;&#12289;&#23485;&#24230;&#27867;&#21270;&#21644;&#32452;&#21512;&#27867;&#21270;&#12290;&#20026;&#20102;&#20415;&#20110;&#31995;&#32479;&#30340;&#25506;&#32034;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#21512;&#25104;&#21644;&#21487;&#32534;&#31243;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#23545;&#28436;&#32462;&#35268;&#21017;&#21644;&#35777;&#26126;&#22797;&#26434;&#24615;&#36827;&#34892;&#25511;&#21046;&#12290;&#25105;&#20204;&#23545;&#22235;&#20010;&#20855;&#26377;&#19981;&#21516;&#22823;&#23567;&#21644;&#35757;&#32451;&#30446;&#26631;&#30340;LLMs&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#23427;&#20204;&#33021;&#22815;&#25512;&#24191;&#21040;&#22797;&#26434;&#30340;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the intractably large size of the space of proofs, any model that is capable of general deductive reasoning must generalize to proofs of greater complexity. Recent studies have shown that large language models (LLMs) possess some abstract deductive reasoning ability given chain-of-thought prompts. However, they have primarily been tested on proofs using modus ponens or of a specific size, and from the same distribution as the in-context examples. To measure the general deductive reasoning ability of LLMs, we test on a broad set of deduction rules and measure their ability to generalize to more complex proofs from simpler demonstrations from multiple angles: depth-, width-, and compositional generalization. To facilitate systematic exploration, we construct a new synthetic and programmable reasoning dataset that enables control over deduction rules and proof complexity. Our experiments on four LLMs of various sizes and training objectives show that they are able to generalize to c
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#20026;&#33258;&#21160;&#21387;&#32553;&#22120;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#23558;&#38271;&#31687;&#25991;&#26412;&#21387;&#32553;&#25104;&#32039;&#20945;&#30340;&#25688;&#35201;&#21521;&#37327;&#65292;&#25552;&#39640;&#19978;&#19979;&#25991;&#30340;&#21033;&#29992;&#25928;&#29575;&#21644;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#21516;&#26102;&#36890;&#36807;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#31934;&#24230;&#24182;&#38477;&#20302;&#25512;&#26029;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2305.14788</link><description>&lt;p&gt;
&#23558;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#20026;&#33258;&#21160;&#21387;&#32553;&#22120;&#20197;&#25552;&#39640;&#27169;&#22411;&#19978;&#19979;&#25991;&#30340;&#21033;&#29992;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Adapting Language Models to Compress Contexts. (arXiv:2305.14788v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#20026;&#33258;&#21160;&#21387;&#32553;&#22120;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#23558;&#38271;&#31687;&#25991;&#26412;&#21387;&#32553;&#25104;&#32039;&#20945;&#30340;&#25688;&#35201;&#21521;&#37327;&#65292;&#25552;&#39640;&#19978;&#19979;&#25991;&#30340;&#21033;&#29992;&#25928;&#29575;&#21644;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#21516;&#26102;&#36890;&#36807;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#31934;&#24230;&#24182;&#38477;&#20302;&#25512;&#26029;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#26159;&#21151;&#33021;&#24378;&#22823;&#19988;&#24191;&#27867;&#24212;&#29992;&#30340;&#24037;&#20855;&#65292;&#20294;&#20854;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#21644;&#39640;&#35745;&#31639;&#25104;&#26412;&#32422;&#26463;&#20102;&#20854;&#23454;&#29992;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#20026;&#33258;&#21160;&#21387;&#32553;&#22120;&#65292;&#33021;&#22815;&#23558;&#38271;&#31687;&#25991;&#26412;&#21387;&#32553;&#25104;&#32039;&#20945;&#30340;&#25688;&#35201;&#21521;&#37327;&#65292;&#20174;&#32780;&#25552;&#39640;&#19978;&#19979;&#25991;&#30340;&#21033;&#29992;&#25928;&#29575;&#21644;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;&#21516;&#26102;&#65292;&#25688;&#35201;&#21521;&#37327;&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#20316;&#20026;&#36719;&#25552;&#31034;&#34987;&#27169;&#22411;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based language models (LMs) are powerful and widely-applicable tools, but their usefulness is constrained by a finite context window and the expensive computational cost of processing long text documents. We propose to adapt pre-trained LMs into AutoCompressors. These models are capable of compressing long contexts into compact summary vectors, which are then accessible to the model as soft prompts. Summary vectors are trained with an unsupervised objective, whereby long documents are processed in segments and summary vectors from all previous segments are used in language modeling. We fine-tune OPT models on sequences of up to 30,720 tokens and show that AutoCompressors can utilize long contexts to improve perplexity. We evaluate AutoCompressors on in-context learning by compressing task demonstrations. We find that summary vectors are good substitutes for plain-text demonstrations, increasing accuracy while reducing inference cost. Finally, we explore the benefits of pre-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;InterSent&#26694;&#26550;&#65292;&#25506;&#32034;&#23558;&#19981;&#21516;&#30340;&#32452;&#21512;&#23646;&#24615;&#32435;&#20837;&#21477;&#23376;&#23884;&#20837;&#31354;&#38388;&#20013;&#20351;&#23884;&#20837;&#21464;&#25442;&#21464;&#20026;&#32452;&#21512;&#21477;&#23376;&#25805;&#20316;&#65292;&#23454;&#29616;&#20102;&#21487;&#35299;&#37322;&#30340;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2305.14599</link><description>&lt;p&gt;
&#36830;&#32493;&#21644;&#31163;&#25955;&#31354;&#38388;&#30340;&#26725;&#26753;: &#36890;&#36807;&#32452;&#21512;&#25805;&#20316;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Bridging Continuous and Discrete Spaces: Interpretable Sentence Representation Learning via Compositional Operations. (arXiv:2305.14599v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;InterSent&#26694;&#26550;&#65292;&#25506;&#32034;&#23558;&#19981;&#21516;&#30340;&#32452;&#21512;&#23646;&#24615;&#32435;&#20837;&#21477;&#23376;&#23884;&#20837;&#31354;&#38388;&#20013;&#20351;&#23884;&#20837;&#21464;&#25442;&#21464;&#20026;&#32452;&#21512;&#21477;&#23376;&#25805;&#20316;&#65292;&#23454;&#29616;&#20102;&#21487;&#35299;&#37322;&#30340;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#23558;&#21477;&#23376;&#32534;&#30721;&#20026;&#21521;&#37327;&#34920;&#31034;&#65292;&#20197;&#25429;&#25417;&#35832;&#22914;&#21477;&#23376;&#35821;&#20041;&#30456;&#20284;&#24615;&#20043;&#31867;&#30340;&#26377;&#29992;&#23646;&#24615;&#12290;&#28982;&#32780;&#65292;&#38500;&#20102;&#30456;&#20284;&#24615;&#22806;&#65292;&#21477;&#23376;&#35821;&#20041;&#36824;&#21487;&#20197;&#36890;&#36807;&#32452;&#21512;&#25805;&#20316;&#65288;&#22914;&#21477;&#23376;&#34701;&#21512;&#25110;&#24046;&#24322;&#65289;&#36827;&#34892;&#35299;&#37322;&#12290;&#19981;&#28165;&#26970;&#26159;&#21542;&#21487;&#20197;&#30452;&#25509;&#23558;&#21477;&#23376;&#30340;&#32452;&#21512;&#35821;&#20041;&#21453;&#26144;&#20026;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#32452;&#21512;&#25805;&#20316;&#12290;&#20026;&#20102;&#26356;&#26377;&#25928;&#22320;&#36830;&#25509;&#36830;&#32493;&#23884;&#20837;&#21644;&#31163;&#25955;&#25991;&#26412;&#31354;&#38388;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#23558;&#21508;&#31181;&#32452;&#21512;&#24615;&#36136;&#21512;&#24182;&#21040;&#21477;&#23376;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#21487;&#34892;&#24615;&#65292;&#20174;&#32780;&#20801;&#35768;&#25105;&#20204;&#23558;&#23884;&#20837;&#21464;&#25442;&#35299;&#37322;&#20026;&#32452;&#21512;&#21477;&#23376;&#25805;&#20316;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;InterSent&#65292;&#19968;&#31181;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#21477;&#23376;&#23884;&#20837;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#23427;&#25903;&#25345;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#32452;&#21512;&#21477;&#23376;&#25805;&#20316;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#21270;&#25805;&#20316;&#22120;&#32593;&#32476;&#21644;&#29942;&#39048;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#20197;&#20135;&#29983;&#26377;&#24847;&#20041;&#30340;&#29983;&#25104;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional sentence embedding models encode sentences into vector representations to capture useful properties such as the semantic similarity between sentences. However, in addition to similarity, sentence semantics can also be interpreted via compositional operations such as sentence fusion or difference. It is unclear whether the compositional semantics of sentences can be directly reflected as compositional operations in the embedding space. To more effectively bridge the continuous embedding and discrete text spaces, we explore the plausibility of incorporating various compositional properties into the sentence embedding space that allows us to interpret embedding transformations as compositional sentence operations. We propose InterSent, an end-to-end framework for learning interpretable sentence embeddings that supports compositional sentence operations in the embedding space. Our method optimizes operator networks and a bottleneck encoder-decoder model to produce meaningful an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;FOCUS&#65292;&#22312;&#22810;&#35821;&#35328;&#28304;&#27169;&#22411;&#35774;&#32622;&#19979;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#37325;&#21472;&#26631;&#35760;&#32452;&#21512;&#26377;&#25928;&#22320;&#21021;&#22987;&#21270;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#26435;&#37325;&#65292;&#25552;&#39640;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#36866;&#24212;&#26032;&#35821;&#35328;&#26102;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.14481</link><description>&lt;p&gt;
FOCUS&#65306;&#22522;&#20110;&#21333;&#35821;&#35328;&#30340;&#39044;&#35757;&#32451;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#25928;&#23884;&#20837;&#21021;&#22987;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FOCUS: Effective Embedding Initialization for Specializing Pretrained Multilingual Models on a Single Language. (arXiv:2305.14481v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14481
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FOCUS&#65292;&#22312;&#22810;&#35821;&#35328;&#28304;&#27169;&#22411;&#35774;&#32622;&#19979;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#37325;&#21472;&#26631;&#35760;&#32452;&#21512;&#26377;&#25928;&#22320;&#21021;&#22987;&#21270;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#26435;&#37325;&#65292;&#25552;&#39640;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#36866;&#24212;&#26032;&#35821;&#35328;&#26102;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#35821;&#35328;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#12290;&#20351;&#29992;&#22312;&#39640;&#36164;&#28304;&#35821;&#35328;&#19978;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#26435;&#37325;&#20316;&#20026;&#28201;&#21551;&#21160;&#65292;&#21487;&#20197;&#20943;&#23569;&#27492;&#38656;&#27714;&#12290;&#20026;&#20102;&#36866;&#24212;&#26032;&#35821;&#35328;&#65292;&#38656;&#35201;&#23545;&#39044;&#35757;&#32451;&#30340;&#35789;&#27719;&#34920;&#21644;&#23884;&#20837;&#36827;&#34892;&#35843;&#25972;&#12290;&#22312;&#20197;&#21069;&#30340;&#24037;&#20316;&#20013;&#65292;&#38024;&#23545;&#36866;&#24212;&#21518;&#30340;&#35789;&#27719;&#34920;&#30340;&#23884;&#20837;&#21021;&#22987;&#21270;&#22823;&#22810;&#32858;&#28966;&#20110;&#21333;&#35821;&#35328;&#28304;&#27169;&#22411;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#28304;&#27169;&#22411;&#35774;&#32622;&#65292;&#24182;&#25552;&#20986;&#20102;FOCUS-&#24555;&#36895;&#37325;&#21472;&#26631;&#35760;&#32452;&#21512;&#20351;&#29992;Sparsemax&#30340;&#26032;&#22411;&#23884;&#20837;&#21021;&#22987;&#21270;&#26041;&#27861;&#65292;&#24403;&#36866;&#24212;XLM-R&#26102;&#65292;&#23427;&#30340;&#34920;&#29616;&#20248;&#20110;&#20197;&#21069;&#30340;&#24037;&#20316;&#12290;FOCUS&#23558;&#26032;&#22686;&#30340;&#26631;&#35760;&#34920;&#31034;&#20026;&#39044;&#35757;&#32451;&#21644;&#26032;&#35789;&#27719;&#34920;&#20043;&#38388;&#30340;&#37325;&#21472;&#26631;&#35760;&#32452;&#21512;&#12290;&#36825;&#20123;&#37325;&#21472;&#26631;&#35760;&#26159;&#22522;&#20110;&#36741;&#21161;&#26631;&#35760;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#36827;&#34892;&#36873;&#25321;&#30340;&#12290;&#25105;&#20204;&#23454;&#29616;&#30340;FOCUS&#20844;&#24320;&#22312;GitHub&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Using model weights pretrained on a high-resource language as a warm start can reduce the need for data and compute to obtain high-quality language models in low-resource languages. To accommodate the new language, the pretrained vocabulary and embeddings need to be adapted. Previous work on embedding initialization for such adapted vocabularies has mostly focused on monolingual source models. In this paper, we investigate the multilingual source model setting and propose FOCUS - Fast Overlapping Token Combinations Using Sparsemax, a novel embedding initialization method that outperforms previous work when adapting XLM-R. FOCUS represents newly added tokens as combinations of tokens in the overlap of the pretrained and new vocabularies. The overlapping tokens are selected based on semantic similarity in an auxiliary token embedding space. Our implementation of FOCUS is publicly available on GitHub.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24037;&#20855;&#22686;&#24378;&#30340;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;&#26694;&#26550; ChatCoT&#65292;&#29992;&#20110;&#22522;&#20110;&#32842;&#22825;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#32842;&#22825;&#30340;&#26041;&#24335;&#23454;&#29616;&#22810;&#36718;&#25512;&#29702;&#65292;&#24039;&#22937;&#22320;&#32467;&#21512;&#20102;&#24605;&#32500;&#38142;&#36319;&#36394;&#21644;&#24037;&#20855;&#25805;&#20316;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.14323</link><description>&lt;p&gt;
ChatCoT: &#22522;&#20110;&#24037;&#20855;&#22686;&#24378;&#30340;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;&#22312;&#22522;&#20110;&#32842;&#22825;&#22823;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on Chat-based Large Language Models. (arXiv:2305.14323v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14323
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24037;&#20855;&#22686;&#24378;&#30340;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;&#26694;&#26550; ChatCoT&#65292;&#29992;&#20110;&#22522;&#20110;&#32842;&#22825;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#32842;&#22825;&#30340;&#26041;&#24335;&#23454;&#29616;&#22810;&#36718;&#25512;&#29702;&#65292;&#24039;&#22937;&#22320;&#32467;&#21512;&#20102;&#24605;&#32500;&#38142;&#36319;&#36394;&#21644;&#24037;&#20855;&#25805;&#20316;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#35780;&#20272;&#22522;&#20934;&#20013;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#34920;&#29616;&#65292;&#20294;&#23427;&#20204;&#22312;&#38656;&#35201;&#29305;&#23450;&#30693;&#35782;&#21644;&#22810;&#27425;&#25512;&#29702;&#30340;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#12290;&#20026;&#20102;&#25552;&#39640;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24037;&#20855;&#22686;&#24378;&#30340;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;&#26694;&#26550; ChatCoT&#65292;&#29992;&#20110;&#22522;&#20110;&#32842;&#22825;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#22312; ChatCoT &#20013;&#65292;&#25105;&#20204;&#23558;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;&#24314;&#27169;&#20026;&#22810;&#36718;&#23545;&#35805;&#65292;&#36890;&#36807;&#32842;&#22825;&#30340;&#26041;&#24335;&#26356;&#33258;&#28982;&#22320;&#21033;&#29992;&#24037;&#20855;&#12290;&#22312;&#27599;&#20010;&#36718;&#27425;&#20013;&#65292;LLM &#33021;&#22815;&#20132;&#20114;&#24037;&#20855;&#25110;&#36827;&#34892;&#25512;&#29702;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#21033;&#29992;&#22522;&#20110;&#32842;&#22825;&#30340; LLM &#30340;&#22810;&#36718;&#23545;&#35805;&#33021;&#21147;&#65292;&#24182;&#20197;&#32479;&#19968;&#30340;&#26041;&#24335;&#38598;&#25104;&#24605;&#32500;&#38142;&#36319;&#36394;&#21644;&#24037;&#20855;&#25805;&#20316;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#24037;&#20855;&#12289;&#20219;&#21153;&#21644;&#25512;&#29702;&#26684;&#24335;&#21021;&#22987;&#21270;&#23545;&#35805;&#30340;&#26089;&#26399;&#36718;&#27425;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36845;&#20195;&#30340;&#24037;&#20855;&#22686;&#24378;&#25512;&#29702;&#27493;&#39588;&#26469;&#36880;&#27493;&#36827;&#34892;&#24037;&#20855;&#22686;&#24378;&#25512;&#29702;&#12290;&#23454;&#39564;&#32467;&#26524;&#22312;&#20004;&#20010;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102; ChatCoT &#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although large language models (LLMs) have achieved excellent performance in a variety of evaluation benchmarks, they still struggle in complex reasoning tasks which require specific knowledge and multi-hop reasoning. To improve the reasoning abilities, we propose \textbf{ChatCoT}, a tool-augmented chain-of-thought reasoning framework for chat-based LLMs. In ChatCoT, we model the chain-of-thought~(CoT) reasoning as multi-turn conversations, to utilize tools in a more natural way through chatting. At each turn, LLMs can either interact with tools or perform the reasoning. Our approach can effectively leverage the multi-turn conversation ability of chat-based LLMs, and integrate the thought chain following and tools manipulation in a unified way. Specially, we initialize the early turns of the conversation by the tools, tasks and reasoning format, and propose an iterative \emph{tool-augmented reasoning} step to perform step-by-step tool-augmented reasoning. The experiment results on two 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521; VideoCOT&#65292;&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#29983;&#25104;&#33021;&#21147;&#65292;&#20197;&#22686;&#24378;&#35270;&#39057;&#25512;&#29702;&#65292;&#21516;&#26102;&#20943;&#23569;&#22788;&#29702;&#25968;&#30334;&#25110;&#25968;&#21315;&#24103;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#22312;VIP&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#22522;&#20110;&#21508;&#31181;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#23637;&#31034;&#20102;&#20351;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;VideoCOT&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.13903</link><description>&lt;p&gt;
&#35753;&#25105;&#20204;&#36880;&#24103;&#24605;&#32771;&#65306;&#20351;&#29992;&#35270;&#39057;&#25554;&#24103;&#21644;&#39044;&#27979;&#35780;&#20272;&#35270;&#39057;&#24605;&#32500;&#38142;
&lt;/p&gt;
&lt;p&gt;
Let's Think Frame by Frame: Evaluating Video Chain of Thought with Video Infilling and Prediction. (arXiv:2305.13903v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13903
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521; VideoCOT&#65292;&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#29983;&#25104;&#33021;&#21147;&#65292;&#20197;&#22686;&#24378;&#35270;&#39057;&#25512;&#29702;&#65292;&#21516;&#26102;&#20943;&#23569;&#22788;&#29702;&#25968;&#30334;&#25110;&#25968;&#21315;&#24103;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#22312;VIP&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#22522;&#20110;&#21508;&#31181;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#23637;&#31034;&#20102;&#20351;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;VideoCOT&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;2023&#24180;&#26500;&#25104;&#20102;&#25152;&#26377;&#20114;&#32852;&#32593;&#27969;&#37327;&#30340;65&#65285;&#65292;&#20294;&#35270;&#39057;&#20869;&#23481;&#22312;&#29983;&#25104;AI&#30740;&#31350;&#20013;&#21364;&#34987;&#20302;&#20272;&#20102;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#36234;&#26469;&#36234;&#22810;&#22320;&#19982;&#35270;&#35273;&#27169;&#24577;&#34701;&#21512;&#12290;&#23558;&#35270;&#39057;&#19982;LLM&#25972;&#21512;&#26159;&#19979;&#19968;&#27493;&#33258;&#28982;&#30340;&#21457;&#23637;&#26041;&#21521;&#65292;&#37027;&#20040;&#36825;&#20010;&#40511;&#27807;&#22914;&#20309;&#34987;&#22635;&#34917;&#65311;&#20026;&#20102;&#25512;&#36827;&#35270;&#39057;&#25512;&#29702;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#21363;&#22522;&#20110;&#35270;&#39057;&#20851;&#38190;&#24103;&#30340;VideoCOT&#65292;&#23427;&#21033;&#29992;&#20102;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#29983;&#25104;&#33021;&#21147;&#65292;&#20197;&#22686;&#24378;&#35270;&#39057;&#25512;&#29702;&#65292;&#21516;&#26102;&#20943;&#23569;&#22788;&#29702;&#25968;&#30334;&#25110;&#25968;&#21315;&#24103;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;VIP&#65292;&#19968;&#31181;&#21487;&#20197;&#29992;&#26469;&#35780;&#20272;VideoCOT&#30340;&#25512;&#26029;&#26102;&#38388;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;1&#65289;&#21508;&#31181;&#24102;&#26377;&#20851;&#38190;&#24103;&#30340;&#30495;&#23454;&#29983;&#27963;&#35270;&#39057;&#20197;&#21450;&#30456;&#24212;&#30340;&#38750;&#32467;&#26500;&#21270;&#21644;&#32467;&#26500;&#21270;&#22330;&#26223;&#25551;&#36848;&#65292;2&#65289;&#20004;&#20010;&#26032;&#30340;&#35270;&#39057;&#25512;&#29702;&#20219;&#21153;&#65306;&#35270;&#39057;&#25554;&#24103;&#21644;&#22330;&#26223;&#39044;&#27979;&#12290;&#25105;&#20204;&#22312;VIP&#19978;&#23545;&#21508;&#31181;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#23637;&#31034;&#20102;&#20351;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;VideoCOT&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite constituting 65% of all internet traffic in 2023, video content is underrepresented in generative AI research. Meanwhile, recent large language models (LLMs) have become increasingly integrated with capabilities in the visual modality. Integrating video with LLMs is a natural next step, so how can this gap be bridged? To advance video reasoning, we propose a new research direction of VideoCOT on video keyframes, which leverages the multimodal generative abilities of vision-language models to enhance video reasoning while reducing the computational complexity of processing hundreds or thousands of frames. We introduce VIP, an inference-time dataset that can be used to evaluate VideoCOT, containing 1) a variety of real-life videos with keyframes and corresponding unstructured and structured scene descriptions, and 2) two new video reasoning tasks: video infilling and scene prediction. We benchmark various vision-language models on VIP, demonstrating the potential to use vision-la
&lt;/p&gt;</description></item><item><title>C-Eval&#26159;&#31532;&#19968;&#20010;&#19987;&#20026;&#20013;&#25991;&#22522;&#30784;&#27169;&#22411;&#35780;&#20272;&#32780;&#35774;&#35745;&#30340;&#20840;&#38754;&#22871;&#20214;&#65292;&#28085;&#30422;52&#20010;&#19981;&#21516;&#23398;&#31185;&#30340;&#22810;&#32423;&#21035;&#36873;&#25321;&#39064;&#21644;&#25361;&#25112;&#24615;&#31185;&#30446;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#21482;&#26377;GPT-4&#33021;&#22815;&#36798;&#21040;&#36229;&#36807;60&#65285;&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#65292;&#36824;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2305.08322</link><description>&lt;p&gt;
C-Eval: &#29992;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#22810;&#32423;&#22810;&#23398;&#31185;&#20013;&#25991;&#35780;&#20272;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models. (arXiv:2305.08322v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08322
&lt;/p&gt;
&lt;p&gt;
C-Eval&#26159;&#31532;&#19968;&#20010;&#19987;&#20026;&#20013;&#25991;&#22522;&#30784;&#27169;&#22411;&#35780;&#20272;&#32780;&#35774;&#35745;&#30340;&#20840;&#38754;&#22871;&#20214;&#65292;&#28085;&#30422;52&#20010;&#19981;&#21516;&#23398;&#31185;&#30340;&#22810;&#32423;&#21035;&#36873;&#25321;&#39064;&#21644;&#25361;&#25112;&#24615;&#31185;&#30446;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#21482;&#26377;GPT-4&#33021;&#22815;&#36798;&#21040;&#36229;&#36807;60&#65285;&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#65292;&#36824;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#36843;&#20999;&#38656;&#35201;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22522;&#20934;&#26469;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#30340;&#39640;&#32423;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;C-Eval&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19987;&#20026;&#20013;&#25991;&#35821;&#22659;&#19979;&#22522;&#30784;&#27169;&#22411;&#35780;&#20272;&#32780;&#35774;&#35745;&#30340;&#20840;&#38754;&#35780;&#20272;&#22871;&#20214;&#12290;C-Eval&#21253;&#21547;&#22235;&#20010;&#38590;&#24230;&#32423;&#21035;&#30340;&#36873;&#25321;&#39064;&#65306;&#21021;&#20013;&#12289;&#39640;&#20013;&#12289;&#22823;&#23398;&#21644;&#19987;&#19994;&#27700;&#24179;&#12290;&#36825;&#20123;&#39064;&#30446;&#28085;&#30422;52&#20010;&#19981;&#21516;&#30340;&#23398;&#31185;&#65292;&#21253;&#25324;&#20154;&#25991;&#12289;&#31185;&#23398;&#21644;&#24037;&#31243;&#23398;&#31185;&#12290;C-Eval&#36824;&#37197;&#22791;&#20102;C-Eval Hard&#65292;&#36825;&#26159;C-Eval&#20013;&#19968;&#20123;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#31185;&#30446;&#65292;&#38656;&#35201;&#39640;&#32423;&#25512;&#29702;&#33021;&#21147;&#25165;&#33021;&#35299;&#20915;&#12290;&#25105;&#20204;&#23545;&#21253;&#25324;&#33521;&#25991;&#21644;&#20013;&#25991;&#27169;&#22411;&#22312;&#20869;&#30340;&#26368;&#20808;&#36827;&#30340;LLM&#22312;C-Eval&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21482;&#26377;GPT-4&#21487;&#20197;&#23454;&#29616;&#36229;&#36807;60&#65285;&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#65292;&#36825;&#34920;&#26126;&#24403;&#21069;&#30340;LLM&#20173;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;&#25105;&#20204;&#26399;&#26395;C-Eval&#23558;&#26377;&#21161;&#20110;&#20998;&#26512;&#37325;&#35201;&#30340;&#20248;&#21183;&#21644;&#30701;&#26495;&#12290;
&lt;/p&gt;
&lt;p&gt;
New NLP benchmarks are urgently needed to align with the rapid development of large language models (LLMs). We present C-Eval, the first comprehensive Chinese evaluation suite designed to assess advanced knowledge and reasoning abilities of foundation models in a Chinese context. C-Eval comprises multiple-choice questions across four difficulty levels: middle school, high school, college, and professional. The questions span 52 diverse disciplines, ranging from humanities to science and engineering. C-Eval is accompanied by C-Eval Hard, a subset of very challenging subjects in C-Eval that requires advanced reasoning abilities to solve. We conduct a comprehensive evaluation of the most advanced LLMs on C-Eval, including both English- and Chinese-oriented models. Results indicate that only GPT-4 could achieve an average accuracy of over 60%, suggesting that there is still significant room for improvement for current LLMs. We anticipate C-Eval will help analyze important strengths and sho
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#20171;&#32461;&#20102;&#21517;&#20026;DASC&#30340;&#21487;&#25511;&#29983;&#25104;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#23646;&#24615;&#35821;&#20041;&#31354;&#38388;&#30340;&#21152;&#26435;&#35299;&#30721;&#26469;&#23454;&#29616;&#22810;&#23646;&#24615;&#29983;&#25104;&#65292;&#24182;&#33021;&#22815;&#22312;&#22810;&#20010;&#26041;&#38754;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#25511;&#21046;&#31934;&#24230;&#21644;&#39640;&#36136;&#37327;&#30340;&#29983;&#25104;&#21709;&#24212;&#12290;</title><link>http://arxiv.org/abs/2305.02820</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#20041;&#31354;&#38388;&#30340;&#22810;&#23646;&#24615;&#21487;&#25511;&#23545;&#35805;&#29983;&#25104;&#20013;&#30340;&#21152;&#26435;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Semantic Space Grounded Weighted Decoding for Multi-Attribute Controllable Dialogue Generation. (arXiv:2305.02820v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02820
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#20171;&#32461;&#20102;&#21517;&#20026;DASC&#30340;&#21487;&#25511;&#29983;&#25104;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#23646;&#24615;&#35821;&#20041;&#31354;&#38388;&#30340;&#21152;&#26435;&#35299;&#30721;&#26469;&#23454;&#29616;&#22810;&#23646;&#24615;&#29983;&#25104;&#65292;&#24182;&#33021;&#22815;&#22312;&#22810;&#20010;&#26041;&#38754;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#25511;&#21046;&#31934;&#24230;&#21644;&#39640;&#36136;&#37327;&#30340;&#29983;&#25104;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25511;&#21046;&#32842;&#22825;&#26426;&#22120;&#20154;&#29983;&#25104;&#20855;&#26377;&#20010;&#24615;&#12289;&#24773;&#24863;&#21644;&#23545;&#35805;&#34892;&#20026;&#31561;&#22810;&#20010;&#23646;&#24615;&#30340;&#35805;&#35821;&#26159;&#19968;&#20010;&#23454;&#38469;&#26377;&#29992;&#20294;&#40092;&#26377;&#30740;&#31350;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#25511;&#29983;&#25104;&#26694;&#26550;DASC&#65292;&#23427;&#36890;&#36807;&#21152;&#26435;&#35299;&#30721;&#33539;&#24335;&#20855;&#26377;&#24378;&#22823;&#30340;&#21487;&#25511;&#24615;&#65292;&#21516;&#26102;&#22312;&#23646;&#24615;&#35821;&#20041;&#31354;&#38388;&#30340;&#22522;&#30784;&#19978;&#25552;&#39640;&#20102;&#29983;&#25104;&#36136;&#37327;&#12290;&#28982;&#21518;&#65292;&#22810;&#23646;&#24615;&#29983;&#25104;&#36890;&#36807;&#22810;&#20010;&#23646;&#24615;&#23884;&#20837;&#30340;&#25554;&#20540;&#30452;&#35266;&#22320;&#23454;&#29616;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;DASC&#22312;&#19977;&#20010;&#26041;&#38754;&#21487;&#25511;&#29983;&#25104;&#20219;&#21153;&#20013;&#21487;&#20197;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#25511;&#21046;&#31934;&#24230;&#65292;&#21516;&#26102;&#20135;&#29983;&#26377;&#36259;&#32780;&#21512;&#29702;&#30340;&#21709;&#24212;&#65292;&#21363;&#20351;&#22312;&#20998;&#24067;&#40065;&#26834;&#24615;&#27979;&#35797;&#20013;&#20063;&#26159;&#22914;&#27492;&#12290;&#23646;&#24615;&#35821;&#20041;&#31354;&#38388;&#20013;&#23398;&#20064;&#21040;&#30340;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#30340;&#21487;&#35270;&#21270;&#20063;&#25903;&#25345;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Controlling chatbot utterance generation with multiple attributes such as personalities, emotions and dialogue acts is a practically useful but under-studied problem. We propose a novel controllable generation framework called DASC that possesses strong controllability with weighted decoding paradigm, while improving generation quality with the grounding in an attribute semantics space. Generation with multiple attributes is then intuitively implemented with an interpolation of multiple attribute embeddings. Experiments show that DASC can achieve state-of-the-art control accuracy in 3-aspect controllable generation tasks while also producing interesting and reasonably sensible responses, even if in an out-of-distribution robustness test. Visualization of the meaningful representations learned in the attribute semantic space also supports its effectiveness.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#27169;&#22411;&#36924;&#36817;CKY&#31639;&#27861;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#26799;&#24230;&#39044;&#27979;&#35299;&#26512;&#30340;&#26041;&#27861;&#65292;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#31454;&#20105;&#21147;&#26356;&#22909;&#65292;&#21516;&#26102;&#36895;&#24230;&#26356;&#24555;&#12290;&#22312;&#38543;&#26426;PCFG&#19979;&#35299;&#26512;&#26102;&#65292;&#24615;&#33021;&#19979;&#38477;&#65292;&#20294;&#21152;&#20837;&#39069;&#22806;&#30340;&#24402;&#32435;&#20559;&#24046;&#26159;&#26377;&#24110;&#21161;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.02386</link><description>&lt;p&gt;
&#29992;Transformer&#36924;&#36817;CKY&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Approximating CKY with Transformers. (arXiv:2305.02386v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#27169;&#22411;&#36924;&#36817;CKY&#31639;&#27861;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#26799;&#24230;&#39044;&#27979;&#35299;&#26512;&#30340;&#26041;&#27861;&#65292;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#31454;&#20105;&#21147;&#26356;&#22909;&#65292;&#21516;&#26102;&#36895;&#24230;&#26356;&#24555;&#12290;&#22312;&#38543;&#26426;PCFG&#19979;&#35299;&#26512;&#26102;&#65292;&#24615;&#33021;&#19979;&#38477;&#65292;&#20294;&#21152;&#20837;&#39069;&#22806;&#30340;&#24402;&#32435;&#20559;&#24046;&#26159;&#26377;&#24110;&#21161;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#27169;&#22411;&#36924;&#36817;CKY&#31639;&#27861;&#30340;&#33021;&#21147;&#65292;&#30452;&#25509;&#39044;&#27979;&#21477;&#23376;&#30340;&#35299;&#26512;&#65292;&#36991;&#20813;&#20102;CKY&#31639;&#27861;&#23545;&#21477;&#23376;&#38271;&#24230;&#30340;&#19977;&#27425;&#20381;&#36182;&#12290;&#22312;&#26631;&#20934;&#30340;&#32452;&#25104;&#21477;&#20998;&#26512;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#27604;&#20351;&#29992;CKY&#30340;&#21487;&#27604;&#20998;&#26512;&#22120;&#21462;&#24471;&#20102;&#31454;&#20105;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#36895;&#24230;&#26356;&#24555;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#22312;&#38543;&#26426;PCFG&#19979;&#36827;&#34892;&#35299;&#26512;&#30340;&#21487;&#34892;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#35821;&#27861;&#21464;&#24471;&#26356;&#21152;&#27169;&#31946;&#30340;&#24773;&#20917;&#19979;&#65292;&#24615;&#33021;&#19979;&#38477;&#65292;&#36825;&#34920;&#26126;Transformer&#27809;&#26377;&#23436;&#20840;&#25429;&#25417;&#21040;CKY&#35745;&#31639;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#20063;&#21457;&#29616;&#65292;&#32467;&#21512;&#39069;&#22806;&#30340;&#24402;&#32435;&#20559;&#24046;&#26159;&#26377;&#24110;&#21161;&#30340;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#30456;&#23545;&#20110;&#22270;&#34920;&#34920;&#31034;&#30340;&#26799;&#24230;&#26469;&#39044;&#27979;&#35299;&#26512;&#65292;&#31867;&#27604;&#20110;CKY&#31639;&#27861;&#19982;&#22270;&#34920;&#30456;&#20851;&#30340;&#19968;&#20010;&#20998;&#21306;&#20989;&#25968;&#21464;&#20307;&#30340;&#23376;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the ability of transformer models to approximate the CKY algorithm, using them to directly predict a parse and thus avoid the CKY algorithm's cubic dependence on sentence length. We find that on standard constituency parsing benchmarks this approach achieves competitive or better performance than comparable parsers that make use of CKY, while being faster. We also evaluate the viability of this approach for parsing under random PCFGs. Here we find that performance declines as the grammar becomes more ambiguous, suggesting that the transformer is not fully capturing the CKY computation. However, we also find that incorporating additional inductive bias is helpful, and we propose a novel approach that makes use of gradients with respect to chart representations in predicting the parse, in analogy with the CKY algorithm being the subgradient of a partition function variant with respect to the chart.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#31995;&#32479;&#35770;&#36807;&#31243;&#20998;&#26512;&#65288;STPA&#65289;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#37319;&#29992;ChatGPT&#23545;&#33258;&#21160;&#32039;&#24613;&#21046;&#21160;&#65288;AEB&#65289;&#31995;&#32479;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#37325;&#22797;&#21452;&#24037;&#20132;&#20114;&#26041;&#27861;&#26159;&#26368;&#26377;&#25928;&#30340;&#65292;&#24182;&#26174;&#30528;&#25552;&#39640;&#20102;STPA&#30340;&#36136;&#37327;&#12290;&#26412;&#30740;&#31350;&#35777;&#26126;&#65292;LLMs&#21487;&#20197;&#24212;&#29992;&#20110;&#23433;&#20840;&#20998;&#26512;&#65292;&#24182;&#20026;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2304.01246</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#23433;&#20840;&#20998;&#26512;&#65306;&#32842;&#22825;GPT&#22312;STPA&#26696;&#20363;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Safety Analysis in the Era of Large Language Models: A Case Study of STPA using ChatGPT. (arXiv:2304.01246v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#31995;&#32479;&#35770;&#36807;&#31243;&#20998;&#26512;&#65288;STPA&#65289;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#37319;&#29992;ChatGPT&#23545;&#33258;&#21160;&#32039;&#24613;&#21046;&#21160;&#65288;AEB&#65289;&#31995;&#32479;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#37325;&#22797;&#21452;&#24037;&#20132;&#20114;&#26041;&#27861;&#26159;&#26368;&#26377;&#25928;&#30340;&#65292;&#24182;&#26174;&#30528;&#25552;&#39640;&#20102;STPA&#30340;&#36136;&#37327;&#12290;&#26412;&#30740;&#31350;&#35777;&#26126;&#65292;LLMs&#21487;&#20197;&#24212;&#29992;&#20110;&#23433;&#20840;&#20998;&#26512;&#65292;&#24182;&#20026;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#21644;BERT&#65292;&#30001;&#20110;&#20854;&#20855;&#26377;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#23545;&#35805;&#65292;&#22312;&#35768;&#22810;&#30693;&#35782;&#39046;&#22495;&#20013;&#20855;&#26377;&#35814;&#32454;&#21644;&#26126;&#30830;&#30340;&#31572;&#26696;&#65292;&#27491;&#22312;&#24341;&#39046;&#19968;&#22330;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#28909;&#28526;&#12290;&#34429;&#28982;LLMs&#27491;&#22312;&#36805;&#36895;&#24212;&#29992;&#20110;&#35768;&#22810;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#39046;&#22495;&#65292;&#20294;&#25105;&#20204;&#23545;&#20197;&#19979;&#38382;&#39064;&#24863;&#20852;&#36259;&#65306;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#30340;&#23433;&#20840;&#20998;&#26512;&#26159;&#21542;&#21487;&#20197;&#21033;&#29992;LLMs&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;ChatGPT&#23545;&#33258;&#21160;&#32039;&#24613;&#21046;&#21160;&#65288;AEB&#65289;&#31995;&#32479;&#30340;&#31995;&#32479;&#35770;&#36807;&#31243;&#20998;&#26512;&#65288;STPA&#65289;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#12290;STPA&#26159;&#26368;&#26222;&#36941;&#30340;&#21361;&#38505;&#20998;&#26512;&#25216;&#26415;&#20043;&#19968;&#65292;&#20294;&#23427;&#23384;&#22312;&#35832;&#22810;&#23616;&#38480;&#24615;&#65292;&#20363;&#22914;&#39640;&#22797;&#26434;&#24615;&#21644;&#20027;&#35266;&#24615;&#65292;&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;ChatGPT&#30340;&#24212;&#29992;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#32771;&#34385;&#20854;&#19982;&#20154;&#31867;&#19987;&#23478;&#30340;&#20132;&#20114;&#65292;&#30740;&#31350;&#20102;&#19977;&#31181;&#23558;ChatGPT&#32435;&#20837;STPA&#20013;&#30340;&#26041;&#27861;&#65306;&#19968;&#27425;&#24615;&#21333;&#24037;&#20132;&#20114;&#12289;&#37325;&#22797;&#21333;&#24037;&#20132;&#20114;&#21644;&#37325;&#22797;&#21452;&#24037;&#20132;&#20114;&#12290;&#27604;&#36739;&#32467;&#26524;&#34920;&#26126;&#65306;&#65288;i&#65289;&#22312;&#27809;&#26377;&#20154;&#31867;&#19987;&#23478;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;ChatGPT&#19981;&#33021;&#20026;STPA&#25552;&#20379;&#36275;&#22815;&#30340;&#20449;&#24687;&#65307;&#65288;ii&#65289;&#19968;&#27425;&#24615;&#21333;&#24037;&#20132;&#20114;&#23545;STPA&#26377;&#24110;&#21161;&#65292;&#20294;&#19981;&#22914;&#37325;&#22797;&#20132;&#20114;&#26377;&#25928;&#65307;&#65288;iii&#65289;&#37325;&#22797;&#21452;&#24037;&#20132;&#20114;&#19968;&#33268;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#26174;&#30528;&#25552;&#39640;&#20102;STPA&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;LLMs&#21487;&#20197;&#24212;&#29992;&#20110;&#23433;&#20840;&#20998;&#26512;&#65292;&#24182;&#20026;AEB&#20197;&#22806;&#30340;&#20854;&#20182;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), such as ChatGPT and BERT, are leading a new AI heatwave due to its human-like conversations with detailed and articulate answers across many domains of knowledge. While LLMs are being quickly applied to many AI application domains, we are interested in the following question: Can safety analysis for safety-critical systems make use of LLMs? To answer, we conduct a case study of Systems Theoretic Process Analysis (STPA) on Automatic Emergency Brake (AEB) systems using ChatGPT. STPA, one of the most prevalent techniques for hazard analysis, is known to have limitations such as high complexity and subjectivity, which this paper aims to explore the use of ChatGPT to address. Specifically, three ways of incorporating ChatGPT into STPA are investigated by considering its interaction with human experts: one-off simplex interaction, recurring simplex interaction, and recurring duplex interaction. Comparative results reveal that: (i) using ChatGPT without human exp
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;YouTube&#35780;&#35770;&#20013;&#30340;&#25705;&#27931;&#21733;&#26041;&#35328;&#36827;&#34892;&#24773;&#24863;&#20998;&#31867;&#65292;&#37319;&#29992;&#22810;&#31181;&#25991;&#26412;&#39044;&#22788;&#29702;&#21644;&#25968;&#25454;&#34920;&#31034;&#25216;&#26415;&#23545;&#25991;&#26412;&#36827;&#34892;&#20998;&#26512;&#65292;&#30740;&#31350;&#35813;&#26041;&#35328;&#30340;&#24847;&#35265;&#21644;&#24773;&#24863;&#34920;&#36798;&#12290;</title><link>http://arxiv.org/abs/2303.15987</link><description>&lt;p&gt;
&#20851;&#20110;&#25705;&#27931;&#21733;&#26041;&#35328;&#25991;&#26412;&#24773;&#24863;&#20998;&#31867;&#30340;&#23454;&#39564;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Experimental Study on Sentiment Classification of Moroccan dialect texts in the web. (arXiv:2303.15987v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;YouTube&#35780;&#35770;&#20013;&#30340;&#25705;&#27931;&#21733;&#26041;&#35328;&#36827;&#34892;&#24773;&#24863;&#20998;&#31867;&#65292;&#37319;&#29992;&#22810;&#31181;&#25991;&#26412;&#39044;&#22788;&#29702;&#21644;&#25968;&#25454;&#34920;&#31034;&#25216;&#26415;&#23545;&#25991;&#26412;&#36827;&#34892;&#20998;&#26512;&#65292;&#30740;&#31350;&#35813;&#26041;&#35328;&#30340;&#24847;&#35265;&#21644;&#24773;&#24863;&#34920;&#36798;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31038;&#20132;&#23186;&#20307;&#32593;&#31449;&#30340;&#36805;&#36895;&#22686;&#38271;&#65292;&#33258;&#21160;&#33719;&#21462;&#29992;&#25143;&#21453;&#39304;&#25104;&#20026;&#35780;&#20272;&#20854;&#22312;&#32447;&#36235;&#21183;&#21644;&#34892;&#20026;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#23613;&#31649;&#20449;&#24687;&#22823;&#37327;&#21487;&#29992;&#65292;&#38463;&#25289;&#20271;&#20351;&#29992;&#32773;&#25968;&#37327;&#22686;&#21152;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#22788;&#29702;&#38463;&#25289;&#20271;&#26041;&#35328;&#12290;&#26412;&#25991;&#26088;&#22312;&#20934;&#30830;&#30740;&#31350;&#22312;YouTube&#35780;&#35770;&#20013;&#34920;&#36798;&#30340;&#30495;&#23454;&#25705;&#27931;&#21733;&#26041;&#35328;&#25991;&#26412;&#30340;&#35266;&#28857;&#21644;&#24773;&#24863;&#65292;&#20351;&#29992;&#19968;&#20123;&#20247;&#25152;&#21608;&#30693;&#19988;&#24120;&#29992;&#30340;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#36827;&#34892;&#12290;&#36890;&#36807;&#37319;&#29992;&#35768;&#22810;&#25991;&#26412;&#39044;&#22788;&#29702;&#21644;&#25968;&#25454;&#34920;&#31034;&#25216;&#26415;&#65292;&#25105;&#20204;&#26088;&#22312;&#27604;&#36739;&#25105;&#20204;&#20351;&#29992;&#26368;&#24120;&#29992;&#30340;&#30417;&#30563;&#20998;&#31867;&#22120;&#36827;&#34892;&#20998;&#31867;&#32467;&#26524;&#65306;K&#26368;&#36817;&#37051;&#65288;KNN&#65289;&#12289;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#12289;&#26420;&#32032;&#36125;&#21494;&#26031;&#65288;NB&#65289;&#21644;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#20998;&#31867;&#22120;&#65292;&#36825;&#20123;&#37117;&#26159;&#22522;&#20110;&#25105;&#20204;&#25910;&#38598;&#21644;&#25163;&#21160;&#27880;&#37322;&#30340;YouTube&#25705;&#27931;&#21733;&#26041;&#35328;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid growth of the use of social media websites, obtaining the users' feedback automatically became a crucial task to evaluate their tendencies and behaviors online. Despite this great availability of information, and the increasing number of Arabic users only few research has managed to treat Arabic dialects. The purpose of this paper is to study the opinion and emotion expressed in real Moroccan texts precisely in the YouTube comments using some well-known and commonly used methods for sentiment analysis. In this paper, we present our work of Moroccan dialect comments classification using Machine Learning (ML) models and based on our collected and manually annotated YouTube Moroccan dialect dataset. By employing many text preprocessing and data representation techniques we aim to compare our classification results utilizing the most commonly used supervised classifiers: k-nearest neighbors (KNN), Support Vector Machine (SVM), Naive Bayes (NB), and deep learning (DL) classif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#23545;&#20165;&#25991;&#26412;&#20219;&#21153;&#30340;&#34920;&#29616;&#26159;&#21542;&#26377;&#25552;&#39640;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#22871;&#35270;&#35273;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#65292;&#35777;&#26126;&#20102;&#22810;&#27169;&#24577;&#35757;&#32451;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#22312;&#35270;&#35273;&#25512;&#29702;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.12513</link><description>&lt;p&gt;
BERT&#26159;&#21542;&#30450;&#30446;&#65311;&#25506;&#32034;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#23545;&#35270;&#35273;&#35821;&#35328;&#29702;&#35299;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Is BERT Blind? Exploring the Effect of Vision-and-Language Pretraining on Visual Language Understanding. (arXiv:2303.12513v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#23545;&#20165;&#25991;&#26412;&#20219;&#21153;&#30340;&#34920;&#29616;&#26159;&#21542;&#26377;&#25552;&#39640;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#22871;&#35270;&#35273;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#65292;&#35777;&#26126;&#20102;&#22810;&#27169;&#24577;&#35757;&#32451;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#22312;&#35270;&#35273;&#25512;&#29702;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#20154;&#20351;&#29992;&#35270;&#35273;&#24819;&#35937;&#26469;&#29702;&#35299;&#21644;&#25512;&#29702;&#35821;&#35328;&#65292;&#20294;&#26159;&#20687;BERT&#36825;&#26679;&#30340;&#27169;&#22411;&#20351;&#29992;&#22312;&#20165;&#21253;&#25324;&#25991;&#26412;&#30340;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#33719;&#21462;&#30340;&#30693;&#35782;&#26469;&#25512;&#29702;&#35821;&#35328;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#26159;&#21542;&#21487;&#20197;&#25552;&#39640;&#22312;&#28041;&#21450;&#38544;&#21547;&#35270;&#35273;&#25512;&#29702;&#30340;&#20165;&#25991;&#26412;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#37325;&#28857;&#26159;&#38646;&#26679;&#26412;&#25506;&#27979;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#22871;&#29992;&#20110;&#25506;&#27979;&#25991;&#26412;&#32534;&#30721;&#22120;&#27169;&#22411;&#35270;&#35273;&#25512;&#29702;&#33021;&#21147;&#30340;&#35270;&#35273;&#35821;&#35328;&#29702;&#35299;&#65288;VLU&#65289;&#20219;&#21153;&#65292;&#20197;&#21450;&#21508;&#31181;&#38750;&#35270;&#35273;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#20219;&#21153;&#29992;&#20110;&#27604;&#36739;&#12290;&#25105;&#20204;&#36824;&#36129;&#29486;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#38646;&#26679;&#26412;&#30693;&#35782;&#25506;&#27979;&#26041;&#27861;&#65292;Stroop probing&#65292;&#29992;&#20110;&#23558;&#20687;CLIP&#36825;&#26679;&#30340;&#27169;&#22411;&#24212;&#29992;&#20110;&#20165;&#25991;&#26412;&#20219;&#21153;&#65292;&#32780;&#19981;&#38656;&#35201;&#20687;BERT&#27169;&#22411;&#30340;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#22836;&#37027;&#26679;&#30340;&#39044;&#27979;&#22836;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;SOTA&#22810;&#27169;&#24577;&#35757;&#32451;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#22312;VLU&#20219;&#21153;&#19978;&#20248;&#20110;&#21333;&#27169;&#24577;&#35757;&#32451;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#65292;&#20294;&#22312;NLU&#20219;&#21153;&#19978;&#19981;&#21450;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most humans use visual imagination to understand and reason about language, but models such as BERT reason about language using knowledge acquired during text-only pretraining. In this work, we investigate whether vision-and-language pretraining can improve performance on text-only tasks that involve implicit visual reasoning, focusing primarily on zero-shot probing methods. We propose a suite of visual language understanding (VLU) tasks for probing the visual reasoning abilities of text encoder models, as well as various non-visual natural language understanding (NLU) tasks for comparison. We also contribute a novel zero-shot knowledge probing method, Stroop probing, for applying models such as CLIP to text-only tasks without needing a prediction head such as the masked language modelling head of models like BERT. We show that SOTA multimodally trained text encoders outperform unimodally trained text encoders on the VLU tasks while being underperformed by them on the NLU tasks, lendin
&lt;/p&gt;</description></item><item><title>xCodeEval&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#22810;&#20219;&#21153;&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#12289;&#20462;&#22797;&#12289;&#32763;&#35793;&#21644;&#26816;&#32034;&#20195;&#30721;&#30340;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#20102;&#20197;&#24448;&#20165;&#20851;&#27880;&#29305;&#23450;&#20219;&#21153;&#21644;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.03004</link><description>&lt;p&gt;
xCodeEval&#65306;&#19968;&#20010;&#29992;&#20110;&#20195;&#30721;&#29702;&#35299;&#12289;&#29983;&#25104;&#12289;&#32763;&#35793;&#21644;&#26816;&#32034;&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#22810;&#20219;&#21153;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
xCodeEval: A Large Scale Multilingual Multitask Benchmark for Code Understanding, Generation, Translation and Retrieval. (arXiv:2303.03004v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03004
&lt;/p&gt;
&lt;p&gt;
xCodeEval&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#22810;&#20219;&#21153;&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#12289;&#20462;&#22797;&#12289;&#32763;&#35793;&#21644;&#26816;&#32034;&#20195;&#30721;&#30340;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#20102;&#20197;&#24448;&#20165;&#20851;&#27880;&#29305;&#23450;&#20219;&#21153;&#21644;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#38382;&#39064;&#30340;&#33021;&#21147;&#26159;&#26234;&#33021;&#30340;&#26631;&#24535;&#65292;&#24182;&#19988;&#19968;&#30452;&#26159; AI &#30340;&#30446;&#26631;&#12290;&#33021;&#22815;&#21019;&#24314;&#20316;&#20026;&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#30340;&#31243;&#24207;&#30340; AI &#31995;&#32479;&#65292;&#25110;&#32773;&#21327;&#21161;&#24320;&#21457;&#20154;&#21592;&#32534;&#20889;&#31243;&#24207;&#65292;&#37117;&#21487;&#20197;&#25552;&#39640;&#29983;&#20135;&#29575;&#24182;&#20351;&#32534;&#31243;&#26356;&#26131;&#20110;&#35775;&#38382;&#12290;&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20174;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#29983;&#25104;&#26032;&#20195;&#30721;&#12289;&#20462;&#22797;&#26377;&#38382;&#39064;&#30340;&#20195;&#30721;&#12289;&#22312;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#36827;&#34892;&#20195;&#30721;&#32763;&#35793;&#20197;&#21450;&#26816;&#32034;&#30456;&#20851;&#20195;&#30721;&#29255;&#27573;&#26041;&#38754;&#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#35780;&#20272;&#36890;&#24120;&#26159;&#20998;&#25955;&#22312;&#20165;&#19968;&#20010;&#25110;&#20004;&#20010;&#29305;&#23450;&#20219;&#21153;&#19978;&#65292;&#22312;&#23569;&#25968;&#35821;&#35328;&#12289;&#22312;&#37096;&#20998;&#31890;&#24230;&#27700;&#24179;&#65288;&#20363;&#22914;&#20989;&#25968;&#32423;&#21035;&#65289;&#19978;&#36827;&#34892;&#65292;&#24182;&#19988;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#32570;&#20047;&#36866;&#24403;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#26356;&#20026;&#20196;&#20154;&#25285;&#24551;&#30340;&#26159;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#35780;&#20272;&#26159;&#20197;&#20165;&#20165;&#35789;&#27719;&#37325;&#21472;&#20026;&#22522;&#30784;&#65292;&#32780;&#19981;&#26159;&#23454;&#38469;&#25191;&#34892;&#65292;&#32780;&#20004;&#27573;&#20195;&#30721;&#27573;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#65288;&#25110;&#31561;&#25928;&#24615;&#65289;&#20165;&#21462;&#20915;&#20110;&#23427;&#20204;&#30340;&#8220;&#25191;&#34892;&#30456;&#20284;&#24615;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to solve problems is a hallmark of intelligence and has been an enduring goal in AI. AI systems that can create programs as solutions to problems or assist developers in writing programs can increase productivity and make programming more accessible. Recently, pre-trained large language models have shown impressive abilities in generating new codes from natural language descriptions, repairing buggy codes, translating codes between languages, and retrieving relevant code segments. However, the evaluation of these models has often been performed in a scattered way on only one or two specific tasks, in a few languages, at a partial granularity (e.g., function) level and in many cases without proper training data. Even more concerning is that in most cases the evaluation of generated codes has been done in terms of mere lexical overlap rather than actual execution whereas semantic similarity (or equivalence) of two code segments depends only on their ``execution similarity'', 
&lt;/p&gt;</description></item><item><title>CiteBench&#26159;&#19968;&#20010;&#31185;&#23398;&#24341;&#25991;&#25991;&#26412;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35299;&#20915;&#30740;&#31350;&#21152;&#36895;&#23548;&#33268;&#30340;&#35299;&#35835;&#21644;&#24635;&#32467;&#20808;&#21069;&#24037;&#20316;&#30340;&#22256;&#38590;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#21487;&#20197;&#36827;&#34892;&#26631;&#20934;&#21270;&#35780;&#20272;&#65292;&#30740;&#31350;&#19981;&#21516;&#20219;&#21153;&#35774;&#35745;&#21644;&#39046;&#22495;&#30340;&#24341;&#25991;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#12290;&#23545;&#22810;&#20010;&#22522;&#32447;&#27169;&#22411;&#30340;&#22823;&#37327;&#27979;&#35797;&#21457;&#29616;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2212.09577</link><description>&lt;p&gt;
CiteBench&#65306;&#31185;&#23398;&#24341;&#25991;&#25991;&#26412;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
CiteBench: A benchmark for Scientific Citation Text Generation. (arXiv:2212.09577v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09577
&lt;/p&gt;
&lt;p&gt;
CiteBench&#26159;&#19968;&#20010;&#31185;&#23398;&#24341;&#25991;&#25991;&#26412;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35299;&#20915;&#30740;&#31350;&#21152;&#36895;&#23548;&#33268;&#30340;&#35299;&#35835;&#21644;&#24635;&#32467;&#20808;&#21069;&#24037;&#20316;&#30340;&#22256;&#38590;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#21487;&#20197;&#36827;&#34892;&#26631;&#20934;&#21270;&#35780;&#20272;&#65292;&#30740;&#31350;&#19981;&#21516;&#20219;&#21153;&#35774;&#35745;&#21644;&#39046;&#22495;&#30340;&#24341;&#25991;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#12290;&#23545;&#22810;&#20010;&#22522;&#32447;&#27169;&#22411;&#30340;&#22823;&#37327;&#27979;&#35797;&#21457;&#29616;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#36890;&#36807;&#36880;&#27493;&#24314;&#31435;&#22312;&#31185;&#23398;&#20986;&#29256;&#29289;&#20013;&#35760;&#24405;&#30340;&#20808;&#21069;&#30693;&#35782;&#20307;&#31995;&#30340;&#22522;&#30784;&#19978;&#25552;&#39640;&#12290;&#35768;&#22810;&#39046;&#22495;&#30340;&#30740;&#31350;&#21152;&#36895;&#20351;&#24471;&#36319;&#19978;&#26368;&#26032;&#21457;&#23637;&#24182;&#24635;&#32467;&#19981;&#26029;&#22686;&#38271;&#30340;&#20808;&#21069;&#24037;&#20316;&#30340;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24341;&#25991;&#25991;&#26412;&#29983;&#25104;&#30340;&#20219;&#21153;&#26088;&#22312;&#22312;&#32473;&#23450;&#38656;&#35201;&#24341;&#29992;&#30340;&#35770;&#25991;&#21644;&#24341;&#29992;&#35770;&#25991;&#30340;&#24773;&#22659;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#20934;&#30830;&#30340;&#25991;&#26412;&#25688;&#35201;&#12290;&#29616;&#26377;&#30340;&#24341;&#25991;&#25991;&#26412;&#29983;&#25104;&#30740;&#31350;&#22522;&#20110;&#24191;&#27867;&#20998;&#27495;&#30340;&#20219;&#21153;&#23450;&#20041;&#65292;&#36825;&#20351;&#24471;&#31995;&#32479;&#22320;&#30740;&#31350;&#36825;&#20010;&#20219;&#21153;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CiteBench&#65306;&#19968;&#20010;&#24341;&#25991;&#25991;&#26412;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#65292;&#23427;&#32479;&#19968;&#20102;&#22810;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65292;&#20351;&#24471;&#21487;&#20197;&#23545;&#20219;&#21153;&#35774;&#35745;&#21644;&#39046;&#22495;&#20013;&#30340;&#24341;&#25991;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#26631;&#20934;&#21270;&#35780;&#20272;&#12290;&#20351;&#29992;&#36825;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#22810;&#20010;&#24378;&#22522;&#20934;&#30340;&#24615;&#33021;&#65292;&#27979;&#35797;&#20102;&#23427;&#20204;&#22312;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#21487;&#36716;&#31227;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#20219;&#21153;&#30340;&#26032;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Science progresses by incrementally building upon the prior body of knowledge documented in scientific publications. The acceleration of research across many fields makes it hard to stay up-to-date with the recent developments and to summarize the ever-growing body of prior work. To target this issue, the task of citation text generation aims to produce accurate textual summaries given a set of papers-to-cite and the citing paper context. Existing studies in citation text generation are based upon widely diverging task definitions, which makes it hard to study this task systematically. To address this challenge, we propose CiteBench: a benchmark for citation text generation that unifies multiple diverse datasets and enables standardized evaluation of citation text generation models across task designs and domains. Using the new benchmark, we investigate the performance of multiple strong baselines, test their transferability between the datasets, and deliver new insights into the task 
&lt;/p&gt;</description></item><item><title>ProtoryNet&#26159;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#36712;&#36857;&#30340;&#21487;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#36890;&#36807;&#25429;&#25417;&#26102;&#38388;&#27169;&#24335;&#21644;&#21407;&#22411;&#30340;&#36817;&#20284;&#31243;&#24230;&#26469;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#65292;&#24182;&#23454;&#29616;&#20102;&#30452;&#35266;&#21644;&#32454;&#33268;&#30340;&#25512;&#29702;&#36807;&#31243;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2007.01777</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#24207;&#21015;&#20998;&#31867;&#36890;&#36807;&#21407;&#22411;&#36712;&#36857;
&lt;/p&gt;
&lt;p&gt;
Interpretable Sequence Classification Via Prototype Trajectory. (arXiv:2007.01777v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2007.01777
&lt;/p&gt;
&lt;p&gt;
ProtoryNet&#26159;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#36712;&#36857;&#30340;&#21487;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#36890;&#36807;&#25429;&#25417;&#26102;&#38388;&#27169;&#24335;&#21644;&#21407;&#22411;&#30340;&#36817;&#20284;&#31243;&#24230;&#26469;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#65292;&#24182;&#23454;&#29616;&#20102;&#30452;&#35266;&#21644;&#32454;&#33268;&#30340;&#25512;&#29702;&#36807;&#31243;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;&#30340;&#21487;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#31216;&#20026;ProtoryNet&#65292;&#23427;&#22522;&#20110;&#21407;&#22411;&#36712;&#36857;&#30340;&#26032;&#27010;&#24565;&#12290;&#21463;&#29616;&#20195;&#35821;&#35328;&#23398;&#20013;&#30340;&#21407;&#22411;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;ProtoryNet&#36890;&#36807;&#20026;&#25991;&#26412;&#24207;&#21015;&#20013;&#30340;&#27599;&#20010;&#21477;&#23376;&#25214;&#21040;&#26368;&#30456;&#20284;&#30340;&#21407;&#22411;&#65292;&#24182;&#23558;&#27599;&#20010;&#21477;&#23376;&#19982;&#30456;&#24212;&#30340;&#27963;&#21160;&#21407;&#22411;&#30340;&#25509;&#36817;&#31243;&#24230;&#36755;&#20837;&#21040;RNN&#20027;&#24178;&#20013;&#36827;&#34892;&#39044;&#27979;&#12290;&#28982;&#21518;&#65292;RNN&#20027;&#24178;&#25429;&#25417;&#21040;&#21407;&#22411;&#30340;&#26102;&#38388;&#27169;&#24335;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#21407;&#22411;&#36712;&#36857;&#12290;&#21407;&#22411;&#36712;&#36857;&#33021;&#22815;&#30452;&#35266;&#32780;&#32454;&#33268;&#22320;&#35299;&#37322;RNN&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#20998;&#26512;&#25991;&#26412;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#21407;&#22411;&#20462;&#21098;&#36807;&#31243;&#65292;&#20197;&#20943;&#23569;&#27169;&#22411;&#20351;&#29992;&#30340;&#21407;&#22411;&#24635;&#25968;&#65292;&#20197;&#25552;&#39640;&#35299;&#37322;&#24615;&#12290;&#22312;&#22810;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;ProtoryNet&#27604;&#22522;&#32447;&#30340;&#22522;&#20110;&#21407;&#22411;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26356;&#20934;&#30830;&#65292;&#24182;&#20943;&#23569;&#20102;&#19982;&#29616;&#26377;&#27169;&#22411;&#30456;&#27604;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel interpretable deep neural network for text classification, called ProtoryNet, based on a new concept of prototype trajectories. Motivated by the prototype theory in modern linguistics, ProtoryNet makes a prediction by finding the most similar prototype for each sentence in a text sequence and feeding an RNN backbone with the proximity of each sentence to the corresponding active prototype. The RNN backbone then captures the temporal pattern of the prototypes, which we refer to as prototype trajectories. Prototype trajectories enable intuitive and fine-grained interpretation of the reasoning process of the RNN model, in resemblance to how humans analyze texts. We also design a prototype pruning procedure to reduce the total number of prototypes used by the model for better interpretability. Experiments on multiple public data sets show that ProtoryNet is more accurate than the baseline prototype-based deep neural net and reduces the performance gap compared to state-o
&lt;/p&gt;</description></item></channel></rss>