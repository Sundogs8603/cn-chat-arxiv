<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>MM-Vet&#26159;&#19968;&#20010;&#35780;&#20272;&#26631;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22797;&#26434;&#20219;&#21153;&#19978;&#30340;&#32508;&#21512;&#33021;&#21147;&#12290;&#35813;&#26631;&#20934;&#35299;&#20915;&#20102;&#22914;&#20309;&#32467;&#26500;&#21270;&#21644;&#35780;&#20272;&#22797;&#26434;&#22810;&#27169;&#24577;&#20219;&#21153;&#12289;&#35774;&#35745;&#36866;&#29992;&#20110;&#19981;&#21516;&#38382;&#39064;&#21644;&#22238;&#31572;&#31867;&#22411;&#30340;&#35780;&#20272;&#25351;&#26631;&#20197;&#21450;&#22914;&#20309;&#25552;&#20379;&#27169;&#22411;&#27934;&#23519;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#25972;&#21512;&#19981;&#21516;&#30340;&#26680;&#24515;&#35270;&#35273;-&#35821;&#35328;&#33021;&#21147;&#65292;MM-Vet&#23637;&#31034;&#20102;&#26377;&#36259;&#30340;&#33021;&#21147;&#21644;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.02490</link><description>&lt;p&gt;
MM-Vet: &#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#32508;&#21512;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities. (arXiv:2308.02490v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02490
&lt;/p&gt;
&lt;p&gt;
MM-Vet&#26159;&#19968;&#20010;&#35780;&#20272;&#26631;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22797;&#26434;&#20219;&#21153;&#19978;&#30340;&#32508;&#21512;&#33021;&#21147;&#12290;&#35813;&#26631;&#20934;&#35299;&#20915;&#20102;&#22914;&#20309;&#32467;&#26500;&#21270;&#21644;&#35780;&#20272;&#22797;&#26434;&#22810;&#27169;&#24577;&#20219;&#21153;&#12289;&#35774;&#35745;&#36866;&#29992;&#20110;&#19981;&#21516;&#38382;&#39064;&#21644;&#22238;&#31572;&#31867;&#22411;&#30340;&#35780;&#20272;&#25351;&#26631;&#20197;&#21450;&#22914;&#20309;&#25552;&#20379;&#27169;&#22411;&#27934;&#23519;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#25972;&#21512;&#19981;&#21516;&#30340;&#26680;&#24515;&#35270;&#35273;-&#35821;&#35328;&#33021;&#21147;&#65292;MM-Vet&#23637;&#31034;&#20102;&#26377;&#36259;&#30340;&#33021;&#21147;&#21644;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;MM-Vet&#65292;&#19968;&#20010;&#35780;&#20272;&#26631;&#20934;&#65292;&#29992;&#20110;&#26816;&#26597;&#22312;&#22797;&#26434;&#22810;&#27169;&#24577;&#20219;&#21153;&#19978;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMM&#65289;&#30340;&#34920;&#29616;&#12290;&#26368;&#36817;&#30340;LMM&#23637;&#31034;&#20102;&#21508;&#31181;&#26377;&#36259;&#30340;&#33021;&#21147;&#65292;&#20363;&#22914;&#35299;&#20915;&#20070;&#20889;&#22312;&#40657;&#26495;&#19978;&#30340;&#25968;&#23398;&#38382;&#39064;&#65292;&#25512;&#29702;&#26032;&#38395;&#22270;&#29255;&#20013;&#30340;&#20107;&#20214;&#21644;&#21517;&#20154;&#65292;&#20197;&#21450;&#35299;&#37322;&#35270;&#35273;&#31505;&#35805;&#12290;&#24555;&#36895;&#30340;&#27169;&#22411;&#36827;&#27493;&#32473;&#35780;&#20272;&#26631;&#20934;&#30340;&#24320;&#21457;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#38382;&#39064;&#21253;&#25324;&#65306;&#65288;1&#65289;&#22914;&#20309;&#31995;&#32479;&#22320;&#26500;&#24314;&#21644;&#35780;&#20272;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#65307;&#65288;2&#65289;&#22914;&#20309;&#35774;&#35745;&#36866;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#38382;&#39064;&#21644;&#22238;&#31572;&#30340;&#35780;&#20272;&#25351;&#26631;&#65307;&#65288;3&#65289;&#22914;&#20309;&#32473;&#20986;&#36229;&#20986;&#31616;&#21333;&#24615;&#33021;&#25490;&#21517;&#30340;&#27169;&#22411;&#27934;&#23519;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MM-Vet&#65292;&#22522;&#20110;&#36825;&#26679;&#19968;&#20010;&#27934;&#23519;&#65306;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#26377;&#36259;&#33021;&#21147;&#36890;&#24120;&#36890;&#36807;&#19968;&#31181;&#36890;&#25165;&#27169;&#22411;&#33021;&#22815;&#25972;&#21512;&#19981;&#21516;&#30340;&#26680;&#24515;&#35270;&#35273;-&#35821;&#35328;&#65288;VL&#65289;&#33021;&#21147;&#26469;&#23454;&#29616;&#12290;MM-Vet&#23450;&#20041;&#20102;6&#20010;&#26680;&#24515;VL&#33021;&#21147;&#65292;&#24182;&#26816;&#26597;&#20102;&#20174;&#36825;&#20123;&#33021;&#21147;&#32452;&#21512;&#20013;&#24471;&#20986;&#30340;16&#31181;&#26377;&#36259;&#30340;&#25972;&#21512;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose MM-Vet, an evaluation benchmark that examines large multimodal models (LMMs) on complicated multimodal tasks. Recent LMMs have shown various intriguing abilities, such as solving math problems written on the blackboard, reasoning about events and celebrities in news images, and explaining visual jokes. Rapid model advancements pose challenges to evaluation benchmark development. Problems include: (1) How to systematically structure and evaluate the complicated multimodal tasks; (2) How to design evaluation metrics that work well across question and answer types; and (3) How to give model insights beyond a simple performance ranking. To this end, we present MM-Vet, designed based on the insight that the intriguing ability to solve complicated tasks is often achieved by a generalist model being able to integrate different core vision-language (VL) capabilities. MM-Vet defines 6 core VL capabilities and examines the 16 integrations of interest derived from the capability combin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;NICT-JLE&#35821;&#26009;&#24211;&#36866;&#24212;&#20026;&#36866;&#29992;&#20110;&#21435;&#35823;&#26816;&#27979;&#27169;&#22411;&#35757;&#32451;&#21644;&#35780;&#20272;&#30340;&#26684;&#24335;&#65292;&#35299;&#20915;&#20102;&#23398;&#20064;&#32773;&#35821;&#38899;&#21435;&#35823;&#26816;&#27979;&#30740;&#31350;&#20013;&#25968;&#25454;&#38598;&#35775;&#38382;&#38480;&#21046;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.02482</link><description>&lt;p&gt;
&#23558;NICT-JLE&#35821;&#26009;&#24211;&#29992;&#20110;&#21435;&#35823;&#26816;&#27979;&#27169;&#22411;&#30340;&#36866;&#24212;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Adapting the NICT-JLE Corpus for Disfluency Detection Models. (arXiv:2308.02482v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;NICT-JLE&#35821;&#26009;&#24211;&#36866;&#24212;&#20026;&#36866;&#29992;&#20110;&#21435;&#35823;&#26816;&#27979;&#27169;&#22411;&#35757;&#32451;&#21644;&#35780;&#20272;&#30340;&#26684;&#24335;&#65292;&#35299;&#20915;&#20102;&#23398;&#20064;&#32773;&#35821;&#38899;&#21435;&#35823;&#26816;&#27979;&#30740;&#31350;&#20013;&#25968;&#25454;&#38598;&#35775;&#38382;&#38480;&#21046;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21435;&#35823;&#26816;&#27979;&#26159;&#19968;&#31181;&#24191;&#27867;&#30740;&#31350;&#30340;&#39046;&#22495;&#65292;&#24120;&#29992;&#20110;&#35782;&#21035;&#35821;&#38899;&#20013;&#30340;&#20572;&#39039;&#12289;&#37325;&#22797;&#21644;&#38169;&#35823;&#24320;&#22836;&#31561;&#35821;&#35328;&#22833;&#21033;&#29616;&#35937;&#12290;&#36890;&#36807;&#20351;&#29992;Switchboard&#35821;&#26009;&#24211;&#36827;&#34892;&#35780;&#20272;&#30340;&#26631;&#20934;&#21270;&#27969;&#31243;&#65292;&#21487;&#20197;&#36731;&#26494;&#27604;&#36739;&#19981;&#21516;&#26041;&#27861;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#23398;&#20064;&#32773;&#35821;&#38899;&#30340;&#21435;&#35823;&#26816;&#27979;&#30740;&#31350;&#32780;&#35328;&#65292;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#23384;&#22312;&#35775;&#38382;&#38480;&#21046;&#65292;&#20351;&#24471;&#27604;&#36739;&#21644;&#25913;&#36827;&#27169;&#22411;&#30340;&#21518;&#32493;&#24320;&#21457;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25551;&#36848;&#20102;&#23558;&#21253;&#21547;&#32422;300&#23567;&#26102;&#33521;&#35821;&#23398;&#20064;&#32773;&#21475;&#35821;&#27700;&#24179;&#27979;&#35797;&#30340;NICT-JLE&#35821;&#26009;&#24211;&#36866;&#24212;&#20026;&#36866;&#29992;&#20110;&#21435;&#35823;&#26816;&#27979;&#27169;&#22411;&#35757;&#32451;&#21644;&#35780;&#20272;&#30340;&#26684;&#24335;&#30340;&#36807;&#31243;&#12290;&#30740;&#31350;&#25506;&#35752;&#20102;NICT-JLE&#21644;Switchboard&#35821;&#26009;&#24211;&#20043;&#38388;&#30340;&#19981;&#21516;&#20043;&#22788;&#65292;&#24182;&#35814;&#32454;&#20171;&#32461;&#20102;&#23545;NICT-JLE&#35821;&#26009;&#24211;&#30340;&#26631;&#31614;&#38598;&#21644;&#20803;&#29305;&#24449;&#30340;&#35843;&#25972;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#32467;&#26524;&#25552;&#20379;&#20102;&#26631;&#20934;&#21270;&#30340;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#27979;&#35797;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
The detection of disfluencies such as hesitations, repetitions and false starts commonly found in speech is a widely studied area of research. With a standardised process for evaluation using the Switchboard Corpus, model performance can be easily compared across approaches. This is not the case for disfluency detection research on learner speech, however, where such datasets have restricted access policies, making comparison and subsequent development of improved models more challenging. To address this issue, this paper describes the adaptation of the NICT-JLE corpus, containing approximately 300 hours of English learners' oral proficiency tests, to a format that is suitable for disfluency detection model training and evaluation. Points of difference between the NICT-JLE and Switchboard corpora are explored, followed by a detailed overview of adaptations to the tag set and meta-features of the NICT-JLE corpus. The result of this work provides a standardised train, heldout and test se
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#20026;&#25918;&#23556;&#23398;&#26500;&#24314;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#21307;&#23398;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#21644;&#25903;&#25345;&#19981;&#21516;&#25918;&#23556;&#23398;&#20219;&#21153;&#30340;&#26550;&#26500;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2308.02463</link><description>&lt;p&gt;
&#20026;&#25918;&#23556;&#23398;&#26500;&#24314;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Towards Generalist Foundation Model for Radiology. (arXiv:2308.02463v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02463
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#20026;&#25918;&#23556;&#23398;&#26500;&#24314;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#21307;&#23398;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#21644;&#25903;&#25345;&#19981;&#21516;&#25918;&#23556;&#23398;&#20219;&#21153;&#30340;&#26550;&#26500;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#21551;&#21160;&#25918;&#23556;&#23398;&#22522;&#30784;&#27169;&#22411;&#30340;&#24320;&#21457;&#65292;&#31216;&#20026;RadFM&#12290;&#25105;&#20204;&#20174;&#25968;&#25454;&#12289;&#27169;&#22411;&#35774;&#35745;&#21644;&#35780;&#20272;&#30340;&#35282;&#24230;&#20840;&#38754;&#32771;&#34385;&#20102;&#22522;&#30784;&#27169;&#22411;&#30340;&#26500;&#24314;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#21487;&#24635;&#32467;&#22914;&#19979;&#65306;&#65288;i&#65289;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#21307;&#23398;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;MedMD&#65292;&#21253;&#25324;1600&#19975;&#20010;2D&#21644;3D&#21307;&#23398;&#25195;&#25551;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21253;&#21547;3D&#21307;&#23398;&#25195;&#25551;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#12290;&#65288;ii&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26550;&#26500;&#65292;&#20351;&#24471;&#21487;&#35270;&#26465;&#20214;&#29983;&#25104;&#39044;&#35757;&#32451;&#25104;&#20026;&#21487;&#33021;&#65292;&#21487;&#20197;&#23558;&#25991;&#26412;&#36755;&#20837;&#19982;2D&#25110;3D&#21307;&#23398;&#25195;&#25551;&#20132;&#38169;&#65292;&#29983;&#25104;&#19981;&#21516;&#25918;&#23556;&#23398;&#20219;&#21153;&#30340;&#21709;&#24212;&#12290;&#35813;&#27169;&#22411;&#39318;&#20808;&#22312;MedMD&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;RadMD&#19978;&#36827;&#34892;&#20102;&#29305;&#23450;&#39046;&#22495;&#30340;&#24494;&#35843;&#65292;RadMD&#26159;MedMD&#30340;&#25918;&#23556;&#23398;&#28165;&#29702;&#29256;&#26412;&#65292;&#21253;&#21547;300&#19975;&#20010;&#25918;&#23556;&#23398;&#30340;&#35270;&#35273;&#35821;&#35328;&#23545;&#12290;&#65288;iii&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#22522;&#20934;&#65292;&#21253;&#25324;&#20116;&#20010;&#20219;&#21153;&#65292;&#26088;&#22312;&#20840;&#38754;&#35780;&#20272;&#35813;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we aim to initiate the development of Radiology Foundation Model, termed as RadFM.We consider the construction of foundational models from the perspectives of data, model design, and evaluation thoroughly. Our contribution can be concluded as follows: (i), we construct a large-scale Medical Multi-modal Dataset, MedMD, consisting of 16M 2D and 3D medical scans. To the best of our knowledge, this is the first multi-modal dataset containing 3D medical scans. (ii), We propose an architecture that enables visually conditioned generative pre-training, allowing for the integration of text input interleaved with 2D or 3D medical scans to generate response for diverse radiologic tasks. The model was initially pre-trained on MedMD and subsequently domain-specific fine-tuned on RadMD, a radiologic cleaned version of MedMD, containing 3M radiologic visual-language pairs. (iii), we propose a new evaluation benchmark that comprises five tasks, aiming to comprehensively assess the capa
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#20174;&#20891;&#20107;&#21040;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#37319;&#32435;&#21644;&#25193;&#23637;&#20262;&#29702;&#21407;&#21017;&#20197;&#24212;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#21487;&#34892;&#24615;&#21644;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.02448</link><description>&lt;p&gt;
&#20174;&#20891;&#20107;&#21040;&#21307;&#30103;&#20445;&#20581;&#65306;&#37319;&#32435;&#21644;&#25193;&#23637;&#29992;&#20110;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#20262;&#29702;&#21407;&#21017;
&lt;/p&gt;
&lt;p&gt;
From Military to Healthcare: Adopting and Expanding Ethical Principles for Generative Artificial Intelligence. (arXiv:2308.02448v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02448
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#20174;&#20891;&#20107;&#21040;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#37319;&#32435;&#21644;&#25193;&#23637;&#20262;&#29702;&#21407;&#21017;&#20197;&#24212;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#21487;&#34892;&#24615;&#21644;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;2020&#24180;&#65292;&#32654;&#22269;&#22269;&#38450;&#37096;&#27491;&#24335;&#20844;&#24067;&#20102;&#19968;&#22871;&#25351;&#23548;&#26410;&#26469;&#25112;&#22330;&#19978;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#20351;&#29992;&#30340;&#20262;&#29702;&#21407;&#21017;&#12290;&#23613;&#31649;&#23384;&#22312;&#26126;&#26174;&#24046;&#24322;&#65292;&#20294;&#20891;&#20107;&#21644;&#21307;&#30103;&#26381;&#21153;&#20043;&#38388;&#23384;&#22312;&#26680;&#24515;&#30340;&#30456;&#20284;&#20043;&#22788;&#12290;&#25112;&#22330;&#19978;&#30340;&#25112;&#22763;&#32463;&#24120;&#38754;&#20020;&#38656;&#35201;&#24555;&#36895;&#20915;&#31574;&#30340;&#25913;&#21464;&#29983;&#27963;&#30340;&#24773;&#20917;&#12290;&#21307;&#30103;&#26381;&#21153;&#25552;&#20379;&#32773;&#22312;&#24555;&#36895;&#21464;&#21270;&#30340;&#21307;&#30103;&#29615;&#22659;&#20013;&#20063;&#38754;&#20020;&#31867;&#20284;&#30340;&#25361;&#25112;&#65292;&#20363;&#22914;&#22312;&#24613;&#35786;&#31185;&#25110;&#27835;&#30103;&#21361;&#21450;&#29983;&#21629;&#30340;&#29366;&#20917;&#19979;&#36827;&#34892;&#25163;&#26415;&#12290;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#26159;&#19968;&#31181;&#26032;&#20852;&#25216;&#26415;&#65292;&#26088;&#22312;&#39640;&#25928;&#29983;&#25104;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#65292;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#38543;&#30528;&#35745;&#31639;&#33021;&#21147;&#30340;&#26085;&#30410;&#26222;&#21450;&#21644;&#22823;&#37327;&#30340;&#20581;&#24247;&#25968;&#25454;&#65288;&#22914;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#12289;&#24515;&#30005;&#22270;&#21644;&#21307;&#23398;&#22270;&#20687;&#65289;&#30340;&#22686;&#21152;&#65292;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#24517;&#23558;&#34987;&#36825;&#39033;&#25216;&#26415;&#38761;&#21629;&#21270;&#12290;&#26368;&#36817;&#65292;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#30740;&#31350;&#30028;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#24341;&#21457;&#20102;&#20851;&#20110;&#20854;&#20262;&#29702;&#38382;&#39064;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
In 2020, the U.S. Department of Defense officially disclosed a set of ethical principles to guide the use of Artificial Intelligence (AI) technologies on future battlefields. Despite stark differences, there are core similarities between the military and medical service. Warriors on battlefields often face life-altering circumstances that require quick decision-making. Medical providers experience similar challenges in a rapidly changing healthcare environment, such as in the emergency department or during surgery treating a life-threatening condition. Generative AI, an emerging technology designed to efficiently generate valuable information, holds great promise. As computing power becomes more accessible and the abundance of health data, such as electronic health records, electrocardiograms, and medical images, increases, it is inevitable that healthcare will be revolutionized by this technology. Recently, generative AI has captivated the research community, leading to debates about 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#19968;&#25152;&#24212;&#29992;&#31185;&#23398;&#22823;&#23398;&#30340;&#35745;&#31639;&#26426;&#31185;&#23398;&#26412;&#31185;&#35838;&#31243;&#20013;&#19981;&#21516;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#23558;&#23427;&#20204;&#20316;&#20026;&#25945;&#23398;&#36741;&#21161;&#24037;&#20855;&#65292;&#22312;&#19981;&#21516;&#35745;&#31639;&#26426;&#31185;&#23398;&#39046;&#22495;&#30340;&#35838;&#31243;&#20013;&#36827;&#34892;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#24378;&#22823;&#24615;&#33021;&#65292;&#21516;&#26102;&#24378;&#35843;&#20102;&#22312;&#36825;&#26679;&#19968;&#20010;&#23398;&#20301;&#35838;&#31243;&#30340;&#32972;&#26223;&#19979;&#30340;&#38480;&#21046;&#21644;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2308.02432</link><description>&lt;p&gt;
&#22312;&#35745;&#31639;&#26426;&#31185;&#23398;&#23398;&#20301;&#35838;&#31243;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Performance of Large Language Models in a Computer Science Degree Program. (arXiv:2308.02432v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02432
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#19968;&#25152;&#24212;&#29992;&#31185;&#23398;&#22823;&#23398;&#30340;&#35745;&#31639;&#26426;&#31185;&#23398;&#26412;&#31185;&#35838;&#31243;&#20013;&#19981;&#21516;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#23558;&#23427;&#20204;&#20316;&#20026;&#25945;&#23398;&#36741;&#21161;&#24037;&#20855;&#65292;&#22312;&#19981;&#21516;&#35745;&#31639;&#26426;&#31185;&#23398;&#39046;&#22495;&#30340;&#35838;&#31243;&#20013;&#36827;&#34892;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#24378;&#22823;&#24615;&#33021;&#65292;&#21516;&#26102;&#24378;&#35843;&#20102;&#22312;&#36825;&#26679;&#19968;&#20010;&#23398;&#20301;&#35838;&#31243;&#30340;&#32972;&#26223;&#19979;&#30340;&#38480;&#21046;&#21644;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT-3.5&#21644;GPT-4.0&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24403;&#21069;&#30340;&#35752;&#35770;&#20013;&#26080;&#22788;&#19981;&#22312;&#24182;&#19988;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#12290;&#23427;&#20204;&#20855;&#26377;&#21464;&#38761;&#24615;&#30340;&#33021;&#21147;&#24050;&#32463;&#24341;&#36215;&#20102;&#25105;&#20204;&#19982;&#65288;&#22522;&#20110;&#25991;&#26412;&#30340;&#65289;&#20449;&#24687;&#20114;&#21160;&#21644;&#21033;&#29992;&#30340;&#33539;&#24335;&#36716;&#21464;&#12290;&#27599;&#22825;&#37117;&#26377;&#26032;&#30340;&#21487;&#33021;&#24615;&#26469;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#19968;&#25152;&#24212;&#29992;&#31185;&#23398;&#22823;&#23398;&#30340;&#35745;&#31639;&#26426;&#31185;&#23398;&#26412;&#31185;&#35838;&#31243;&#20013;&#19981;&#21516;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#30740;&#31350;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#36890;&#36807;&#23558;&#23427;&#20204;&#20316;&#20026;&#25945;&#23398;&#36741;&#21161;&#24037;&#20855;&#22312;&#35838;&#31243;&#20013;&#20351;&#29992;&#26469;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#35838;&#22530;&#26448;&#26009;&#12289;&#32451;&#20064;&#20219;&#21153;&#21644;&#36807;&#21435;&#30340;&#32771;&#35797;&#26469;&#21551;&#21457;&#27169;&#22411;&#65292;&#25105;&#20204;&#26088;&#22312;&#35780;&#20272;&#23427;&#20204;&#22312;&#19981;&#21516;&#35745;&#31639;&#26426;&#31185;&#23398;&#39046;&#22495;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#22823;&#24615;&#33021;&#65292;&#21516;&#26102;&#24378;&#35843;&#20102;&#22312;&#36825;&#26679;&#19968;&#20010;&#23398;&#20301;&#35838;&#31243;&#30340;&#32972;&#26223;&#19979;&#30340;&#38480;&#21046;&#21644;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models such as ChatGPT-3.5 and GPT-4.0 are ubiquitous and dominate the current discourse. Their transformative capabilities have led to a paradigm shift in how we interact with and utilize (text-based) information. Each day, new possibilities to leverage the capabilities of these models emerge. This paper presents findings on the performance of different large language models in a university of applied sciences' undergraduate computer science degree program. Our primary objective is to assess the effectiveness of these models within the curriculum by employing them as educational aids. By prompting the models with lecture material, exercise tasks, and past exams, we aim to evaluate their proficiency across different computer science domains. We showcase the strong performance of current large language models while highlighting limitations and constraints within the context of such a degree program. We found that ChatGPT-3.5 averaged 79.9% of the total score in 10 tested 
&lt;/p&gt;</description></item><item><title>Text2KGBench&#26159;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#26681;&#25454;&#26412;&#20307;&#20174;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#20013;&#29983;&#25104;&#30693;&#35782;&#22270;&#35889;&#30340;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;&#24037;&#20855;&#65292;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#36890;&#36807;&#19971;&#20010;&#35780;&#20272;&#25351;&#26631;&#26469;&#34913;&#37327;&#20107;&#23454;&#25552;&#21462;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.02357</link><description>&lt;p&gt;
Text2KGBench&#65306;&#19968;&#31181;&#20174;&#25991;&#26412;&#29983;&#25104;&#26412;&#20307;&#39537;&#21160;&#30340;&#30693;&#35782;&#22270;&#35889;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Text2KGBench: A Benchmark for Ontology-Driven Knowledge Graph Generation from Text. (arXiv:2308.02357v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02357
&lt;/p&gt;
&lt;p&gt;
Text2KGBench&#26159;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#26681;&#25454;&#26412;&#20307;&#20174;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#20013;&#29983;&#25104;&#30693;&#35782;&#22270;&#35889;&#30340;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;&#24037;&#20855;&#65292;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#36890;&#36807;&#19971;&#20010;&#35780;&#20272;&#25351;&#26631;&#26469;&#34913;&#37327;&#20107;&#23454;&#25552;&#21462;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#21644;&#20855;&#26377;&#26032;&#20852;&#33021;&#21147;&#30340;&#22522;&#30784;&#27169;&#22411;&#30340;&#36827;&#23637;&#24050;&#32463;&#35777;&#26126;&#21487;&#20197;&#25913;&#21892;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;LLM&#21644;&#30693;&#35782;&#22270;&#35889;(KG)&#21487;&#20197;&#30456;&#20114;&#34917;&#20805;&#65292;LLM&#21487;&#20197;&#29992;&#20110;KG&#30340;&#26500;&#24314;&#25110;&#34917;&#20840;&#65292;&#32780;&#29616;&#26377;&#30340;KG&#21487;&#20197;&#29992;&#20110;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#20363;&#22914;&#20351;LLM&#30340;&#36755;&#20986;&#26356;&#26131;&#35299;&#37322;&#25110;&#36827;&#34892;&#31867;&#33041;&#31526;&#21495;&#21270;&#30340;&#20107;&#23454;&#26816;&#26597;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Text2KGBench&#65292;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#26681;&#25454;&#26412;&#20307;&#20174;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#20013;&#29983;&#25104;&#30693;&#35782;&#22270;&#35889;&#30340;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#32473;&#23450;&#19968;&#20010;&#36755;&#20837;&#26412;&#20307;&#21644;&#19968;&#32452;&#21477;&#23376;&#65292;&#20219;&#21153;&#26159;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#20107;&#23454;&#65292;&#21516;&#26102;&#31526;&#21512;&#32473;&#23450;&#30340;&#26412;&#20307;(&#27010;&#24565;&#12289;&#20851;&#31995;&#12289;&#22495;/&#20540;&#33539;&#22260;&#32422;&#26463;)&#24182;&#24544;&#23454;&#20110;&#36755;&#20837;&#21477;&#23376;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#65306;(i)&#20855;&#26377;10&#20010;&#26412;&#20307;&#21644;13,474&#20010;&#21477;&#23376;&#30340;Wikidata-TekGen&#21644;(ii)&#20855;&#26377;19&#20010;&#26412;&#20307;&#21644;4,860&#20010;&#21477;&#23376;&#30340;DBpedia-WebNLG&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19971;&#20010;&#35780;&#20272;&#25351;&#26631;&#26469;&#34913;&#37327;&#20107;&#23454;&#25552;&#21462;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advances in large language models (LLM) and foundation models with emergent capabilities have been shown to improve the performance of many NLP tasks. LLMs and Knowledge Graphs (KG) can complement each other such that LLMs can be used for KG construction or completion while existing KGs can be used for different tasks such as making LLM outputs explainable or fact-checking in Neuro-Symbolic manner. In this paper, we present Text2KGBench, a benchmark to evaluate the capabilities of language models to generate KGs from natural language text guided by an ontology. Given an input ontology and a set of sentences, the task is to extract facts from the text while complying with the given ontology (concepts, relations, domain/range constraints) and being faithful to the input sentences. We provide two datasets (i) Wikidata-TekGen with 10 ontologies and 13,474 sentences and (ii) DBpedia-WebNLG with 19 ontologies and 4,860 sentences. We define seven evaluation metrics to measure fact 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#22312;&#25968;&#25454;&#27969;&#23545;&#35805;&#33539;&#24335;&#19979;&#30340;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#29983;&#25104;&#12290;&#22312;MultiWOZ&#39046;&#22495;&#20013;&#65292;&#23637;&#31034;&#20102;&#22522;&#20110;&#35758;&#31243;&#30340;&#23545;&#35805;&#29983;&#25104;&#31034;&#20363;&#65307;&#22312;SMCalFlow&#39046;&#22495;&#20013;&#65292;&#23637;&#31034;&#20102;&#27809;&#26377;&#35758;&#31243;&#30340;&#23545;&#35805;&#29983;&#25104;&#31034;&#20363;&#65292;&#24182;&#19988;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#30340;&#23545;&#35805;&#26469;&#22686;&#24378;&#32763;&#35793;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#25552;&#39640;&#20102;&#29992;&#25143;&#35831;&#27714;&#21040;&#25968;&#25454;&#27969;&#34920;&#36798;&#24335;&#30340;&#32763;&#35793;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.02323</link><description>&lt;p&gt;
&#25968;&#25454;&#27969;&#23545;&#35805;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Dataflow Dialogue Generation. (arXiv:2308.02323v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02323
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22312;&#25968;&#25454;&#27969;&#23545;&#35805;&#33539;&#24335;&#19979;&#30340;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#29983;&#25104;&#12290;&#22312;MultiWOZ&#39046;&#22495;&#20013;&#65292;&#23637;&#31034;&#20102;&#22522;&#20110;&#35758;&#31243;&#30340;&#23545;&#35805;&#29983;&#25104;&#31034;&#20363;&#65307;&#22312;SMCalFlow&#39046;&#22495;&#20013;&#65292;&#23637;&#31034;&#20102;&#27809;&#26377;&#35758;&#31243;&#30340;&#23545;&#35805;&#29983;&#25104;&#31034;&#20363;&#65292;&#24182;&#19988;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#30340;&#23545;&#35805;&#26469;&#22686;&#24378;&#32763;&#35793;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#25552;&#39640;&#20102;&#29992;&#25143;&#35831;&#27714;&#21040;&#25968;&#25454;&#27969;&#34920;&#36798;&#24335;&#30340;&#32763;&#35793;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#25968;&#25454;&#27969;&#23545;&#35805;&#33539;&#24335;&#20013;&#23637;&#31034;&#20102;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#29983;&#25104;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;MultiWOZ&#39046;&#22495;&#20013;&#22522;&#20110;&#35758;&#31243;&#30340;&#23545;&#35805;&#29983;&#25104;&#31034;&#20363;&#65292;&#20197;&#21450;&#22312;SMCalFlow&#39046;&#22495;&#20013;&#27809;&#26377;&#35758;&#31243;&#30340;&#29983;&#25104;&#31034;&#20363;&#65292;&#20854;&#20013;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#29983;&#25104;&#30340;&#23545;&#35805;&#22686;&#24378;&#32763;&#35793;&#35757;&#32451;&#25968;&#25454;&#38598;&#26102;&#65292;&#29992;&#25143;&#35831;&#27714;&#21040;&#25968;&#25454;&#27969;&#34920;&#36798;&#24335;&#30340;&#32763;&#35793;&#20934;&#30830;&#24615;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
We demonstrate task-oriented dialogue generation within the dataflow dialogue paradigm. We show an example of agenda driven dialogue generation for the MultiWOZ domain, and an example of generation without an agenda for the SMCalFlow domain, where we show an improvement in the accuracy of the translation of user requests to dataflow expressions when the generated dialogues are used to augment the translation training dataset.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DHS-ConvQA&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#20250;&#35805;&#24335;&#38382;&#31572;&#20013;&#21160;&#24577;&#36873;&#25321;&#30456;&#20851;&#30340;&#21382;&#21490;&#36716;&#25240;&#28857;&#65292;&#20197;&#25351;&#23548;&#31572;&#26696;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.02294</link><description>&lt;p&gt;
&#23398;&#20064;&#36873;&#25321;&#20250;&#35805;&#38382;&#31572;&#20013;&#30456;&#20851;&#30340;&#21382;&#21490;&#23545;&#35805;&#36716;&#25240;&#28857;
&lt;/p&gt;
&lt;p&gt;
Learning to Select the Relevant History Turns in Conversational Question Answering. (arXiv:2308.02294v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02294
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DHS-ConvQA&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#20250;&#35805;&#24335;&#38382;&#31572;&#20013;&#21160;&#24577;&#36873;&#25321;&#30456;&#20851;&#30340;&#21382;&#21490;&#36716;&#25240;&#28857;&#65292;&#20197;&#25351;&#23548;&#31572;&#26696;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22522;&#20110;&#32593;&#32476;&#30340;&#25968;&#23383;&#21161;&#25163;&#30340;&#26085;&#30410;&#38656;&#27714;&#65292;&#24341;&#36215;&#20102;&#20449;&#24687;&#26816;&#32034;(IR)&#31038;&#21306;&#23545;&#20250;&#35805;&#24335;&#38382;&#31572;(ConvQA)&#39046;&#22495;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;ConvQA&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#26159;&#26377;&#25928;&#36873;&#25321;&#20250;&#35805;&#21382;&#21490;&#36716;&#25240;&#28857;&#20197;&#22238;&#31572;&#24403;&#21069;&#38382;&#39064;&#12290;&#30456;&#20851;&#21382;&#21490;&#36873;&#25321;&#19982;&#27491;&#30830;&#31572;&#26696;&#39044;&#27979;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#26159;&#19968;&#20010;&#26377;&#36259;&#20294;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#39046;&#22495;&#12290;&#36873;&#25321;&#30340;&#30456;&#20851;&#19978;&#19979;&#25991;&#21487;&#20197;&#26356;&#22909;&#22320;&#25351;&#23548;&#31995;&#32479;&#22312;&#25991;&#31456;&#20013;&#23547;&#25214;&#31572;&#26696;&#30340;&#30830;&#20999;&#20301;&#32622;&#12290;&#32780;&#19981;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#21017;&#32473;&#31995;&#32479;&#24102;&#26469;&#22122;&#38899;&#65292;&#20174;&#32780;&#23548;&#33268;&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;DHS-ConvQA&#65288;&#20250;&#35805;&#38382;&#31572;&#20013;&#30340;&#21160;&#24577;&#21382;&#21490;&#36873;&#25321;&#65289;&#65292;&#39318;&#20808;&#20026;&#25152;&#26377;&#21382;&#21490;&#36716;&#25240;&#28857;&#29983;&#25104;&#19978;&#19979;&#25991;&#21644;&#38382;&#39064;&#23454;&#20307;&#65292;&#28982;&#21518;&#26681;&#25454;&#23427;&#20204;&#19982;&#38382;&#39064;&#30340;&#30456;&#20284;&#24230;&#36827;&#34892;&#20462;&#21098;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing demand for the web-based digital assistants has given a rapid rise in the interest of the Information Retrieval (IR) community towards the field of conversational question answering (ConvQA). However, one of the critical aspects of ConvQA is the effective selection of conversational history turns to answer the question at hand. The dependency between relevant history selection and correct answer prediction is an intriguing but under-explored area. The selected relevant context can better guide the system so as to where exactly in the passage to look for an answer. Irrelevant context, on the other hand, brings noise to the system, thereby resulting in a decline in the model's performance. In this paper, we propose a framework, DHS-ConvQA (Dynamic History Selection in Conversational Question Answering), that first generates the context and question entities for all the history turns, which are then pruned on the basis of similarity they share in common with the question at
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20887;&#20313;&#24863;&#30693;&#30340;Sem-nCG&#24230;&#37327;&#65292;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#25688;&#35201;&#19982;&#22810;&#20010;&#21442;&#32771;&#25688;&#35201;&#36827;&#34892;&#23545;&#27604;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26032;&#24230;&#37327;&#20855;&#26377;&#26356;&#39640;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.02270</link><description>&lt;p&gt;
&#22522;&#20110;&#20887;&#20313;&#24863;&#30693;&#30340;&#22810;&#21442;&#32771;&#22686;&#30410;&#35780;&#20272;&#25552;&#21462;&#24615;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Redundancy Aware Multi-Reference Based Gainwise Evaluation of Extractive Summarization. (arXiv:2308.02270v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02270
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20887;&#20313;&#24863;&#30693;&#30340;Sem-nCG&#24230;&#37327;&#65292;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#25688;&#35201;&#19982;&#22810;&#20010;&#21442;&#32771;&#25688;&#35201;&#36827;&#34892;&#23545;&#27604;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26032;&#24230;&#37327;&#20855;&#26377;&#26356;&#39640;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;ROUGE&#25351;&#26631;&#22312;&#35780;&#20272;&#25552;&#21462;&#24615;&#25688;&#35201;&#20219;&#21153;&#20013;&#38750;&#24120;&#27969;&#34892;&#65292;&#20294;&#38271;&#26399;&#20197;&#26469;&#34987;&#25209;&#35780;&#32570;&#20047;&#35821;&#20041;&#24847;&#35782;&#65292;&#24182;&#23545;&#25688;&#35201;&#29983;&#25104;&#22120;&#30340;&#25490;&#21517;&#36136;&#37327;&#26080;&#35270;&#12290;&#24863;&#35874;&#20043;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#31216;&#20026;Sem-nCG&#30340;&#22522;&#20110;&#22686;&#30410;&#30340;&#33258;&#21160;&#24230;&#37327;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#23427;&#26082;&#20855;&#26377;&#25490;&#21517;&#21644;&#35821;&#20041;&#30340;&#24847;&#35782;&#12290;&#28982;&#32780;&#65292;Sem-nCG&#19981;&#32771;&#34385;&#27169;&#22411;&#29983;&#25104;&#30340;&#25688;&#35201;&#20013;&#23384;&#22312;&#30340;&#20887;&#20313;&#25968;&#37327;&#65292;&#30446;&#21069;&#20063;&#19981;&#25903;&#25345;&#20351;&#29992;&#22810;&#20010;&#21442;&#32771;&#25688;&#35201;&#36827;&#34892;&#35780;&#20272;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#21516;&#26102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38480;&#21046;&#24182;&#19981;&#23481;&#26131;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20887;&#20313;&#24863;&#30693;&#30340;Sem-nCG&#24230;&#37327;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#36825;&#20010;&#26032;&#24230;&#37327;&#26469;&#35780;&#20272;&#27169;&#22411;&#25688;&#35201;&#19982;&#22810;&#20010;&#21442;&#32771;&#25688;&#35201;&#36827;&#34892;&#23545;&#27604;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#25506;&#32034;&#20102;&#23558;&#20887;&#20313;&#32435;&#20837;&#21407;&#22987;&#24230;&#37327;&#20013;&#30340;&#19981;&#21516;&#26041;&#24335;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26032;&#30340;&#20887;&#20313;&#24863;&#30693;&#24230;&#37327;&#20855;&#26377;&#26356;&#39640;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While very popular for evaluating extractive summarization task, the ROUGE metric has long been criticized for its lack of semantic awareness and its ignorance about the ranking quality of the summarizer. Thanks to previous research that has addressed these issues by proposing a gain-based automated metric called Sem-nCG, which is both rank and semantic aware. However, Sem-nCG does not consider the amount of redundancy present in a model-generated summary and currently does not support evaluation with multiple reference summaries. Unfortunately, addressing both these limitations simultaneously is not trivial. Therefore, in this paper, we propose a redundancy-aware Sem-nCG metric and demonstrate how this new metric can be used to evaluate model summaries against multiple references. We also explore different ways of incorporating redundancy into the original metric through extensive experiments. Experimental results demonstrate that the new redundancy-aware metric exhibits a higher corr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Spectrum Attention Fusion&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21367;&#31215;&#27169;&#22359;&#26367;&#20195;&#33258;&#27880;&#24847;&#21147;&#23618;&#65292;&#23454;&#29616;&#20102;&#22312;&#35821;&#38899;&#22686;&#24378;&#20219;&#21153;&#20013;&#38477;&#20302;&#27169;&#22411;&#22797;&#26434;&#24230;&#30340;&#30446;&#26631;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#19982;&#24403;&#21069;&#26368;&#20248;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.02263</link><description>&lt;p&gt;
&#20351;&#29992;&#39057;&#35889;&#27880;&#24847;&#21147;&#34701;&#21512;&#30340;&#39640;&#25928;&#21333;&#22768;&#36947;&#35821;&#38899;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Efficient Monaural Speech Enhancement using Spectrum Attention Fusion. (arXiv:2308.02263v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02263
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Spectrum Attention Fusion&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21367;&#31215;&#27169;&#22359;&#26367;&#20195;&#33258;&#27880;&#24847;&#21147;&#23618;&#65292;&#23454;&#29616;&#20102;&#22312;&#35821;&#38899;&#22686;&#24378;&#20219;&#21153;&#20013;&#38477;&#20302;&#27169;&#22411;&#22797;&#26434;&#24230;&#30340;&#30446;&#26631;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#19982;&#24403;&#21069;&#26368;&#20248;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#22686;&#24378;&#26159;&#33258;&#21160;&#35821;&#38899;&#22788;&#29702;&#27969;&#31243;&#20013;&#19968;&#39033;&#38656;&#27714;&#36739;&#39640;&#30340;&#20219;&#21153;&#65292;&#30528;&#37325;&#20110;&#20998;&#31163;&#24178;&#20928;&#30340;&#35821;&#38899;&#20449;&#21495;&#21644;&#22024;&#26434;&#30340;&#20449;&#36947;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#35821;&#38899;&#22686;&#24378;&#20013;&#34920;&#29616;&#20248;&#20110;RNN&#21644;CNN&#27169;&#22411;&#65292;&#28982;&#32780;&#19982;&#27492;&#21516;&#26102;&#65292;&#23427;&#20204;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#26356;&#39640;&#65292;&#38656;&#35201;&#26356;&#22810;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#32780;&#36825;&#22312;&#23454;&#38469;&#24773;&#20917;&#20013;&#24448;&#24448;&#24456;&#38590;&#33719;&#24471;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#35821;&#38899;&#22686;&#24378;&#27169;&#22411;&#65292;&#36890;&#36807;&#31216;&#20043;&#20026;&#39057;&#35889;&#27880;&#24847;&#21147;&#34701;&#21512;&#30340;&#26041;&#27861;&#65292;&#22312;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#22797;&#26434;&#24230;&#30340;&#21516;&#26102;&#20445;&#25345;&#20102;&#33258;&#27880;&#24847;&#21147;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#25105;&#20204;&#24039;&#22937;&#22320;&#26500;&#24314;&#20102;&#19968;&#20010;&#21367;&#31215;&#27169;&#22359;&#26469;&#26367;&#20195;&#35821;&#38899;Transformer&#20013;&#30340;&#33509;&#24178;&#33258;&#27880;&#24847;&#21147;&#23618;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#26356;&#39640;&#25928;&#22320;&#34701;&#21512;&#39057;&#35889;&#29305;&#24449;&#12290;&#22312;Voice Bank + DEMAND&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#25552;&#20986;&#30340;&#27169;&#22411;&#33021;&#22815;&#22312;&#21442;&#25968;&#26174;&#33879;&#20943;&#23569;&#65288;0.58M&#65289;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#19982;SOTA&#27169;&#22411;&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech enhancement is a demanding task in automated speech processing pipelines, focusing on separating clean speech from noisy channels. Transformer based models have recently bested RNN and CNN models in speech enhancement, however at the same time they are much more computationally expensive and require much more high quality training data, which is always hard to come by. In this paper, we present an improvement for speech enhancement models that maintains the expressiveness of self-attention while significantly reducing model complexity, which we have termed Spectrum Attention Fusion. We carefully construct a convolutional module to replace several self-attention layers in a speech Transformer, allowing the model to more efficiently fuse spectral features. Our proposed model is able to achieve comparable or better results against SOTA models but with significantly smaller parameters (0.58M) on the Voice Bank + DEMAND dataset.
&lt;/p&gt;</description></item><item><title>&#23545;&#20110;&#36164;&#28304;&#31232;&#32570;&#30340;&#35821;&#35328;&#65292;&#24320;&#21457;&#31934;&#32454;&#30340;&#23545;&#24212;&#35789;&#20856;&#25968;&#25454;&#38598;&#26159;&#26356;&#21487;&#34892;&#30340;&#20570;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#20013;&#32423;&#20219;&#21153;&#65292;&#22914;&#21463;&#30417;&#30563;&#30340;&#22810;&#35821;&#35328;&#35789;&#23884;&#20837;&#23545;&#40784;&#65292;&#24182;&#36827;&#19968;&#27493;&#25351;&#23548;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#30340;&#39640;&#32423;&#20219;&#21153;&#12290;&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;Sinhala-English&#24179;&#34892;&#35789;&#20856;&#25968;&#25454;&#38598;&#30340;&#24320;&#28304;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2308.02234</link><description>&lt;p&gt;
Sinhala-English&#24179;&#34892;&#35789;&#20856;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Sinhala-English Parallel Word Dictionary Dataset. (arXiv:2308.02234v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02234
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#36164;&#28304;&#31232;&#32570;&#30340;&#35821;&#35328;&#65292;&#24320;&#21457;&#31934;&#32454;&#30340;&#23545;&#24212;&#35789;&#20856;&#25968;&#25454;&#38598;&#26159;&#26356;&#21487;&#34892;&#30340;&#20570;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#20013;&#32423;&#20219;&#21153;&#65292;&#22914;&#21463;&#30417;&#30563;&#30340;&#22810;&#35821;&#35328;&#35789;&#23884;&#20837;&#23545;&#40784;&#65292;&#24182;&#36827;&#19968;&#27493;&#25351;&#23548;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#30340;&#39640;&#32423;&#20219;&#21153;&#12290;&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;Sinhala-English&#24179;&#34892;&#35789;&#20856;&#25968;&#25454;&#38598;&#30340;&#24320;&#28304;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24179;&#34892;&#25968;&#25454;&#38598;&#23545;&#20110;&#25191;&#34892;&#21644;&#35780;&#20272;&#20219;&#20309;&#22810;&#35821;&#35328;&#20219;&#21153;&#37117;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#32771;&#34385;&#30340;&#35821;&#35328;&#23545;&#20013;&#65292;&#22914;&#26524;&#19968;&#20010;&#35821;&#35328;&#26159;&#36164;&#28304;&#31232;&#32570;&#30340;&#35821;&#35328;&#65292;&#29616;&#26377;&#30340;&#33258;&#19978;&#32780;&#19979;&#30340;&#24179;&#34892;&#25968;&#25454;&#65292;&#20363;&#22914;&#35821;&#26009;&#24211;&#65292;&#22312;&#25968;&#37327;&#21644;&#36136;&#37327;&#26041;&#38754;&#37117;&#32570;&#20047;&#65292;&#36825;&#26159;&#30001;&#20110;&#20154;&#24037;&#27880;&#37322;&#30340;&#32570;&#20047;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#36164;&#28304;&#31232;&#32570;&#30340;&#35821;&#35328;&#26469;&#35828;&#65292;&#26356;&#21487;&#34892;&#30340;&#20570;&#27861;&#26159;&#20808;&#26397;&#21521;&#33258;&#19979;&#32780;&#19978;&#30340;&#26041;&#21521;&#21457;&#23637;&#65292;&#24320;&#21457;&#31934;&#32454;&#30340;&#23545;&#24212;&#35789;&#20856;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#65292;&#21487;&#20197;&#23558;&#20854;&#29992;&#20110;&#20013;&#32423;&#20219;&#21153;&#65292;&#22914;&#21463;&#30417;&#30563;&#30340;&#22810;&#35821;&#35328;&#35789;&#23884;&#20837;&#23545;&#40784;&#12290;&#36825;&#21453;&#36807;&#26469;&#21448;&#21487;&#20197;&#25351;&#23548;&#39640;&#32423;&#20219;&#21153;&#30340;&#36827;&#34892;&#65292;&#22914;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#30340;&#21477;&#23376;&#25110;&#27573;&#33853;&#25991;&#26412;&#35821;&#26009;&#24211;&#30340;&#23545;&#40784;&#12290;&#23613;&#31649;&#27604;&#20026;&#36164;&#28304;&#31232;&#32570;&#30340;&#35821;&#35328;&#29983;&#25104;&#21644;&#23545;&#40784;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#26356;&#23481;&#26131;&#65292;&#20294;&#30001;&#20110;&#26469;&#33258;&#26356;&#22823;&#30340;&#30740;&#31350;&#23454;&#20307;&#30340;&#20919;&#28129;&#65292;&#29978;&#33267;&#36825;&#20123;&#31934;&#32454;&#30340;&#23545;&#24212;&#25968;&#25454;&#38598;&#23545;&#20110;&#26576;&#20123;&#36164;&#28304;&#31232;&#32570;&#30340;&#35821;&#35328;&#20063;&#26159;&#32570;&#20047;&#30340;&#12290;&#25105;&#20204;&#24050;&#32463;&#35266;&#23519;&#21040;&#65292;&#23384;&#22312;&#19968;&#20010;&#38656;&#27714;&#65292;&#21363;&#26500;&#24314;&#19968;&#20010;&#29992;&#20110;Sinhala-English&#24179;&#34892;&#35789;&#20856;&#25968;&#25454;&#38598;&#30340;&#24320;&#28304;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parallel datasets are vital for performing and evaluating any kind of multilingual task. However, in the cases where one of the considered language pairs is a low-resource language, the existing top-down parallel data such as corpora are lacking in both tally and quality due to the dearth of human annotation. Therefore, for low-resource languages, it is more feasible to move in the bottom-up direction where finer granular pairs such as dictionary datasets are developed first. They may then be used for mid-level tasks such as supervised multilingual word embedding alignment. These in turn can later guide higher-level tasks in the order of aligning sentence or paragraph text corpora used for Machine Translation (MT). Even though more approachable than generating and aligning a massive corpus for a low-resource language, for the same reason of apathy from larger research entities, even these finer granular data sets are lacking for some low-resource languages. We have observed that there 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#23398;&#20064;&#23558;&#21477;&#23376;&#25913;&#20889;&#20026;&#19981;&#21516;&#22797;&#26434;&#31243;&#24230;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#21644;&#37319;&#29992;&#22810;&#20219;&#21153;&#21644;&#25552;&#31034;&#31574;&#30053;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#21477;&#23376;&#31616;&#21270;&#21644;&#21516;&#32423;&#25913;&#20889;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#27809;&#26377;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35780;&#20272;&#20102;&#19968;&#20123;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.02226</link><description>&lt;p&gt;
&#23398;&#20064;&#23558;&#21477;&#23376;&#25913;&#20889;&#20026;&#19981;&#21516;&#30340;&#22797;&#26434;&#31243;&#24230;
&lt;/p&gt;
&lt;p&gt;
Learning to Paraphrase Sentences to Different Complexity Levels. (arXiv:2308.02226v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02226
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#23398;&#20064;&#23558;&#21477;&#23376;&#25913;&#20889;&#20026;&#19981;&#21516;&#22797;&#26434;&#31243;&#24230;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#21644;&#37319;&#29992;&#22810;&#20219;&#21153;&#21644;&#25552;&#31034;&#31574;&#30053;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#21477;&#23376;&#31616;&#21270;&#21644;&#21516;&#32423;&#25913;&#20889;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#27809;&#26377;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35780;&#20272;&#20102;&#19968;&#20123;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#21477;&#23376;&#31616;&#21270;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#19968;&#20010;&#30740;&#31350;&#28909;&#28857;&#65292;&#20294;&#21477;&#23376;&#22797;&#26434;&#21270;&#21644;&#21516;&#32423;&#25913;&#20889;&#36825;&#20004;&#20010;&#30456;&#37051;&#20219;&#21153;&#21364;&#27809;&#26377;&#24471;&#21040;&#36275;&#22815;&#30340;&#20851;&#27880;&#12290;&#20026;&#20102;&#35757;&#32451;&#27169;&#22411;&#22312;&#36825;&#19977;&#20010;&#20219;&#21153;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#26080;&#30417;&#30563;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23558;&#36825;&#20004;&#20010;&#25968;&#25454;&#38598;&#19982;&#19968;&#20010;&#21333;&#19968;&#30340;&#26377;&#30417;&#30563;&#25968;&#25454;&#38598;&#36827;&#34892;&#27604;&#36739;&#65292;&#19968;&#20010;&#25968;&#25454;&#38598;&#30001;&#19968;&#20010;&#24369;&#20998;&#31867;&#22120;&#26631;&#27880;&#65292;&#21478;&#19968;&#20010;&#25968;&#25454;&#38598;&#30001;&#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#26631;&#27880;&#12290;&#20351;&#29992;&#36825;&#19977;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#22312;&#22810;&#20219;&#21153;&#21644;&#25552;&#31034;&#31574;&#30053;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;&#19982;&#20854;&#20182;&#20351;&#29992;&#26080;&#30417;&#30563;&#24179;&#34892;&#25968;&#25454;&#35757;&#32451;&#30340;&#31995;&#32479;&#30456;&#27604;&#65292;&#25105;&#20204;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;ASSET&#31616;&#21270;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20063;&#20248;&#20110;&#20808;&#21069;&#30340;&#21477;&#23376;&#32423;&#30446;&#26631;&#21270;&#24037;&#20316;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#30830;&#23450;&#20102;&#19968;&#20123;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#35774;&#32622;&#19979;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
While sentence simplification is an active research topic in NLP, its adjacent tasks of sentence complexification and same-level paraphrasing are not. To train models on all three tasks, we present two new unsupervised datasets. We compare these datasets, one labeled by a weak classifier and the other by a rule-based approach, with a single supervised dataset. Using these three datasets for training, we perform extensive experiments on both multitasking and prompting strategies. Compared to other systems trained on unsupervised parallel data, models trained on our weak classifier labeled dataset achieve state-of-the-art performance on the ASSET simplification benchmark. Our models also outperform previous work on sentence level targeting. Finally, we establish how a handful of Large Language Models perform on these tasks under a zero-shot setting.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20004;&#38454;&#27573;&#37319;&#26679;&#21644;&#21160;&#24577;&#37319;&#26679;&#26041;&#27861;&#26469;&#25552;&#39640;&#35757;&#32451;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#30340;&#37319;&#26679;&#25928;&#29575;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#20256;&#32479;&#30340;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#21516;&#26102;&#22312;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#26041;&#38754;&#20063;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.02223</link><description>&lt;p&gt;
ESRL: &#39640;&#25928;&#37319;&#26679;&#30340;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#24207;&#21015;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ESRL: Efficient Sampling-based Reinforcement Learning for Sequence Generation. (arXiv:2308.02223v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02223
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20004;&#38454;&#27573;&#37319;&#26679;&#21644;&#21160;&#24577;&#37319;&#26679;&#26041;&#27861;&#26469;&#25552;&#39640;&#35757;&#32451;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#30340;&#37319;&#26679;&#25928;&#29575;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#20256;&#32479;&#30340;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#21516;&#26102;&#22312;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#26041;&#38754;&#20063;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20110;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#33021;&#22815;&#30452;&#25509;&#20248;&#21270;&#38271;&#26399;&#22238;&#25253;&#65288;&#22914;BLEU&#21644;&#20154;&#31867;&#21453;&#39304;&#65289;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#23545;&#21160;&#20316;&#24207;&#21015;&#31354;&#38388;&#36827;&#34892;&#22823;&#35268;&#27169;&#37319;&#26679;&#12290;&#36825;&#22312;&#24207;&#21015;&#29983;&#25104;&#38382;&#39064;&#20013;&#26159;&#19968;&#20010;&#35745;&#31639;&#25361;&#25112;&#65292;&#27604;&#22914;&#26426;&#22120;&#32763;&#35793;&#65292;&#25105;&#20204;&#32463;&#24120;&#22788;&#29702;&#19968;&#20010;&#22823;&#30340;&#21160;&#20316;&#31354;&#38388;&#65288;&#22914;&#35789;&#27719;&#34920;&#65289;&#21644;&#38271;&#30340;&#21160;&#20316;&#24207;&#21015;&#65288;&#22914;&#32763;&#35793;&#65289;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#38454;&#27573;&#37319;&#26679;&#21644;&#21160;&#24577;&#37319;&#26679;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#30340;&#37319;&#26679;&#25928;&#29575;&#12290;&#25105;&#20204;&#22312;&#20256;&#32479;&#30340;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#21253;&#25324;&#26426;&#22120;&#32763;&#35793;&#21644;&#25277;&#35937;&#25688;&#35201;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;&#22870;&#21169;&#27169;&#22411;&#65292;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;RLHF&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#39640;&#25928;&#37319;&#26679;&#30340;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Applying Reinforcement Learning (RL) to sequence generation models enables the direct optimization of long-term rewards (\textit{e.g.,} BLEU and human feedback), but typically requires large-scale sampling over a space of action sequences. This is a computational challenge as presented by the practice of sequence generation problems, such as machine translation, where we often deal with a large action space (\textit{e.g.,} a vocabulary) and a long action sequence (\textit{e.g.,} a translation). In this work, we introduce two-stage sampling and dynamic sampling approaches to improve the sampling efficiency during training sequence generation models via RL. We experiment with our approaches on the traditional sequence generation tasks, including machine translation and abstractive summarization. Furthermore, we evaluate our approaches in RL from human feedback (RLHF) through training a large language model using the reward model. Experimental results show that the efficient sampling-base
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;&#35199;&#29677;&#29273;&#35821;&#20020;&#24202;&#35821;&#35328;&#27169;&#22411;&#30340;&#24212;&#29992;&#65292;&#22238;&#39038;&#20102;17&#20010;&#19987;&#27880;&#20110;&#20020;&#24202;&#20219;&#21153;&#30340;&#35821;&#26009;&#24211;&#30340;&#36129;&#29486;&#65292;&#24182;&#23545;&#26368;&#30456;&#20851;&#30340;&#35199;&#29677;&#29273;&#35821;&#35821;&#35328;&#27169;&#22411;&#21644;&#35199;&#29677;&#29273;&#20020;&#24202;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#24443;&#24213;&#27604;&#36739;&#65292;&#25552;&#20379;&#20102;3000&#22810;&#20010;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#12290;&#20026;&#20102;&#20415;&#20110;&#26410;&#26469;&#30340;&#30740;&#31350;&#21644;&#25361;&#25112;&#65292;&#25152;&#26377;&#27979;&#35797;&#36807;&#30340;&#35821;&#26009;&#24211;&#21644;&#26368;&#20339;&#27169;&#22411;&#37117;&#34987;&#20197;&#21487;&#35775;&#38382;&#30340;&#26041;&#24335;&#20844;&#24320;&#12290;</title><link>http://arxiv.org/abs/2308.02199</link><description>&lt;p&gt;
&#35199;&#29677;&#29273;&#20020;&#24202;&#35821;&#35328;&#27169;&#22411;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey of Spanish Clinical Language Models. (arXiv:2308.02199v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02199
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;&#35199;&#29677;&#29273;&#35821;&#20020;&#24202;&#35821;&#35328;&#27169;&#22411;&#30340;&#24212;&#29992;&#65292;&#22238;&#39038;&#20102;17&#20010;&#19987;&#27880;&#20110;&#20020;&#24202;&#20219;&#21153;&#30340;&#35821;&#26009;&#24211;&#30340;&#36129;&#29486;&#65292;&#24182;&#23545;&#26368;&#30456;&#20851;&#30340;&#35199;&#29677;&#29273;&#35821;&#35821;&#35328;&#27169;&#22411;&#21644;&#35199;&#29677;&#29273;&#20020;&#24202;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#24443;&#24213;&#27604;&#36739;&#65292;&#25552;&#20379;&#20102;3000&#22810;&#20010;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#12290;&#20026;&#20102;&#20415;&#20110;&#26410;&#26469;&#30340;&#30740;&#31350;&#21644;&#25361;&#25112;&#65292;&#25152;&#26377;&#27979;&#35797;&#36807;&#30340;&#35821;&#26009;&#24211;&#21644;&#26368;&#20339;&#27169;&#22411;&#37117;&#34987;&#20197;&#21487;&#35775;&#38382;&#30340;&#26041;&#24335;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#32858;&#28966;&#20110;&#20351;&#29992;&#32534;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#26469;&#35299;&#20915;&#35199;&#29677;&#29273;&#35821;&#20020;&#24202;&#39046;&#22495;&#20219;&#21153;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#22238;&#39038;&#20102;17&#20010;&#20027;&#35201;&#19987;&#27880;&#20110;&#20020;&#24202;&#20219;&#21153;&#30340;&#35821;&#26009;&#24211;&#30340;&#36129;&#29486;&#65292;&#28982;&#21518;&#21015;&#20986;&#20102;&#26368;&#30456;&#20851;&#30340;&#35199;&#29677;&#29273;&#35821;&#35821;&#35328;&#27169;&#22411;&#21644;&#35199;&#29677;&#29273;&#20020;&#24202;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#21487;&#29992;&#35821;&#26009;&#24211;&#30340;&#31934;&#36873;&#23376;&#38598;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#27604;&#36739;&#65292;&#20197;&#25214;&#21040;&#34920;&#29616;&#26368;&#20339;&#30340;&#27169;&#22411;&#65307;&#24635;&#20849;&#36229;&#36807;3000&#20010;&#27169;&#22411;&#34987;&#38024;&#23545;&#36825;&#39033;&#30740;&#31350;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#25152;&#26377;&#27979;&#35797;&#30340;&#35821;&#26009;&#24211;&#21644;&#26368;&#20339;&#27169;&#22411;&#37117;&#20197;&#21487;&#35775;&#38382;&#30340;&#26041;&#24335;&#20844;&#24320;&#65292;&#20197;&#20415;&#29420;&#31435;&#22242;&#38431;&#21487;&#20197;&#37325;&#29616;&#32467;&#26524;&#25110;&#22312;&#26410;&#26469;&#21019;&#24314;&#26032;&#30340;&#35199;&#29677;&#29273;&#20020;&#24202;&#35821;&#35328;&#27169;&#22411;&#26102;&#36827;&#34892;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
This survey focuses in encoder Language Models for solving tasks in the clinical domain in the Spanish language. We review the contributions of 17 corpora focused mainly in clinical tasks, then list the most relevant Spanish Language Models and Spanish Clinical Language models. We perform a thorough comparison of these models by benchmarking them over a curated subset of the available corpora, in order to find the best-performing ones; in total more than 3000 models were fine-tuned for this study. All the tested corpora and the best models are made publically available in an accessible way, so that the results can be reproduced by independent teams or challenged in the future when new Spanish Clinical Language models are created.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#20851;&#31995;&#20998;&#31867;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21363;&#20351;&#29992;&#35821;&#20041;&#33539;&#22260;&#20998;&#26512;&#27169;&#22411;&#30340;&#20915;&#31574;&#27169;&#24335;&#12290;&#35821;&#20041;&#33539;&#22260;&#26159;&#20851;&#20110;&#20998;&#31867;&#20915;&#31574;&#30340;&#25991;&#26412;&#20013;&#26368;&#26377;&#24433;&#21709;&#21147;&#30340;&#37096;&#20998;&#12290;&#36890;&#36807;&#23558;&#20154;&#31867;&#21644;&#27169;&#22411;&#30340;&#35821;&#20041;&#33539;&#22260;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;&#27169;&#22411;&#24448;&#24448;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#21040;&#20102;&#24555;&#25463;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2308.02193</link><description>&lt;p&gt;
&#29992;&#35821;&#20041;&#33539;&#22260;&#35299;&#37322;&#20851;&#31995;&#20998;&#31867;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Explaining Relation Classification Models with Semantic Extents. (arXiv:2308.02193v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#20851;&#31995;&#20998;&#31867;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21363;&#20351;&#29992;&#35821;&#20041;&#33539;&#22260;&#20998;&#26512;&#27169;&#22411;&#30340;&#20915;&#31574;&#27169;&#24335;&#12290;&#35821;&#20041;&#33539;&#22260;&#26159;&#20851;&#20110;&#20998;&#31867;&#20915;&#31574;&#30340;&#25991;&#26412;&#20013;&#26368;&#26377;&#24433;&#21709;&#21147;&#30340;&#37096;&#20998;&#12290;&#36890;&#36807;&#23558;&#20154;&#31867;&#21644;&#27169;&#22411;&#30340;&#35821;&#20041;&#33539;&#22260;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;&#27169;&#22411;&#24448;&#24448;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#21040;&#20102;&#24555;&#25463;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;BERT&#21644;GPT&#65289;&#30340;&#21457;&#23637;&#26174;&#33879;&#25913;&#36827;&#20102;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#20449;&#24687;&#25277;&#21462;&#31995;&#32479;&#65292;&#21253;&#25324;&#20851;&#31995;&#20998;&#31867;&#12290;&#26368;&#20808;&#36827;&#30340;&#31995;&#32479;&#22312;&#31185;&#23398;&#22522;&#20934;&#19978;&#20855;&#26377;&#24456;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#30446;&#21069;&#65292;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#26159;&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#30340;&#19968;&#20010;&#22797;&#26434;&#22240;&#32032;&#12290;&#21487;&#29702;&#35299;&#30340;&#31995;&#32479;&#23545;&#20110;&#38450;&#27490;&#26377;&#20559;&#35265;&#12289;&#36829;&#21453;&#30452;&#35273;&#25110;&#26377;&#23475;&#30340;&#20915;&#31574;&#26159;&#24517;&#35201;&#30340;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20998;&#26512;&#20851;&#31995;&#20998;&#31867;&#20219;&#21153;&#20915;&#31574;&#27169;&#24335;&#30340;&#27010;&#24565;&#65292;&#21363;&#35821;&#20041;&#33539;&#22260;&#12290;&#35821;&#20041;&#33539;&#22260;&#26159;&#20851;&#20110;&#20998;&#31867;&#20915;&#31574;&#30340;&#25991;&#26412;&#20013;&#26368;&#26377;&#24433;&#21709;&#21147;&#30340;&#37096;&#20998;&#12290;&#25105;&#20204;&#30340;&#23450;&#20041;&#20801;&#35768;&#31867;&#20284;&#30340;&#36807;&#31243;&#26469;&#30830;&#23450;&#20154;&#31867;&#21644;&#27169;&#22411;&#30340;&#35821;&#20041;&#33539;&#22260;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#27880;&#37322;&#24037;&#20855;&#21644;&#19968;&#20010;&#36719;&#20214;&#26694;&#26550;&#65292;&#20197;&#20415;&#26041;&#20415;&#12289;&#21487;&#37325;&#22797;&#22320;&#30830;&#23450;&#20154;&#31867;&#21644;&#27169;&#22411;&#30340;&#35821;&#20041;&#33539;&#22260;&#12290;&#27604;&#36739;&#20004;&#32773;&#21457;&#29616;&#65292;&#27169;&#22411;&#24448;&#24448;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#21040;&#20102;&#24555;&#25463;&#27169;&#24335;&#12290;&#36825;&#20123;&#27169;&#24335;&#24456;&#38590;&#34987;&#20154;&#31867;&#35299;&#37322;&#25110;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the development of large pretrained language models, such as BERT and GPT, significantly improved information extraction systems on various tasks, including relation classification. State-of-the-art systems are highly accurate on scientific benchmarks. A lack of explainability is currently a complicating factor in many real-world applications. Comprehensible systems are necessary to prevent biased, counterintuitive, or harmful decisions.  We introduce semantic extents, a concept to analyze decision patterns for the relation classification task. Semantic extents are the most influential parts of texts concerning classification decisions. Our definition allows similar procedures to determine semantic extents for humans and models. We provide an annotation tool and a software framework to determine semantic extents for humans and models conveniently and reproducibly. Comparing both reveals that models tend to learn shortcut patterns from data. These patterns are hard to d
&lt;/p&gt;</description></item><item><title>Emo-DNA&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#24773;&#24863;&#35299;&#32806;&#21644;&#21452;&#23618;&#24773;&#24863;&#23545;&#40784;&#23454;&#29616;&#20102;&#36328;&#35821;&#26009;&#24211;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#20013;&#30340;&#24773;&#24863;&#30456;&#20851;&#29305;&#24449;&#30340;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2308.02190</link><description>&lt;p&gt;
Emo-DNA: &#36328;&#35821;&#26009;&#24211;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#20013;&#30340;&#24773;&#24863;&#35299;&#32806;&#21644;&#23545;&#40784;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Emo-DNA: Emotion Decoupling and Alignment Learning for Cross-Corpus Speech Emotion Recognition. (arXiv:2308.02190v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02190
&lt;/p&gt;
&lt;p&gt;
Emo-DNA&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#24773;&#24863;&#35299;&#32806;&#21644;&#21452;&#23618;&#24773;&#24863;&#23545;&#40784;&#23454;&#29616;&#20102;&#36328;&#35821;&#26009;&#24211;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#20013;&#30340;&#24773;&#24863;&#30456;&#20851;&#29305;&#24449;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#35821;&#26009;&#24211;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#26088;&#22312;&#23558;&#25512;&#26029;&#35821;&#38899;&#24773;&#24863;&#30340;&#33021;&#21147;&#20174;&#19968;&#20010;&#26377;&#26631;&#31614;&#30340;&#35821;&#26009;&#24211;&#25512;&#24191;&#21040;&#19968;&#20010;&#26080;&#26631;&#31614;&#30340;&#35821;&#26009;&#24211;&#65292;&#36825;&#26159;&#19968;&#20010;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#20004;&#20010;&#35821;&#26009;&#24211;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#22522;&#20110;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#65288;UDA&#65289;&#65292;&#36890;&#36807;&#20840;&#23616;&#20998;&#24067;&#23545;&#40784;&#26469;&#21162;&#21147;&#23398;&#20064;&#35821;&#26009;&#24211;&#19981;&#21464;&#30340;&#29305;&#24449;&#65292;&#20294;&#19981;&#24184;&#30340;&#26159;&#65292;&#25152;&#24471;&#29305;&#24449;&#28151;&#21512;&#20102;&#35821;&#26009;&#24211;&#29305;&#23450;&#30340;&#29305;&#24449;&#25110;&#19981;&#20855;&#26377;&#31867;&#21035;&#37492;&#21035;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;Emotion Decoupling aNd Alignment&#65288;EMO-DNA&#65289;&#36328;&#35821;&#26009;&#24211;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#23398;&#20064;&#26694;&#26550;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;UDA&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#19982;&#24773;&#24863;&#30456;&#20851;&#30340;&#35821;&#26009;&#24211;&#19981;&#21464;&#29305;&#24449;&#12290;EMO-DNA&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#23545;&#27604;&#24773;&#24863;&#35299;&#32806;&#21644;&#21452;&#23618;&#24773;&#24863;&#23545;&#40784;&#12290;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#27604;&#35299;&#32806;&#25439;&#22833;&#23454;&#29616;&#23545;&#27604;&#23398;&#20064;&#65292;&#22686;&#24378;&#20102;&#24773;&#24863;&#30456;&#20851;&#29305;&#24449;&#19982;&#35821;&#26009;&#24211;&#20043;&#38388;&#30340;&#21487;&#20998;&#31163;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-corpus speech emotion recognition (SER) seeks to generalize the ability of inferring speech emotion from a well-labeled corpus to an unlabeled one, which is a rather challenging task due to the significant discrepancy between two corpora. Existing methods, typically based on unsupervised domain adaptation (UDA), struggle to learn corpus-invariant features by global distribution alignment, but unfortunately, the resulting features are mixed with corpus-specific features or not class-discriminative. To tackle these challenges, we propose a novel Emotion Decoupling aNd Alignment learning framework (EMO-DNA) for cross-corpus SER, a novel UDA method to learn emotion-relevant corpus-invariant features. The novelties of EMO-DNA are two-fold: contrastive emotion decoupling and dual-level emotion alignment. On one hand, our contrastive emotion decoupling achieves decoupling learning via a contrastive decoupling loss to strengthen the separability of emotion-relevant features from corpus-s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#21644;&#20559;&#25191;&#26032;&#38395;&#26816;&#27979;&#20043;&#38388;&#24212;&#29992;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#25216;&#26415;&#30340;&#25928;&#26524;&#65292;&#36890;&#36807;&#30693;&#35782;&#20256;&#36882;&#21644;&#25968;&#25454;&#22686;&#24378;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#21516;&#26102;&#23558;&#32858;&#31867;&#21644;&#20027;&#39064;&#24314;&#27169;&#31639;&#27861;&#19982;UDA&#30456;&#32467;&#21512;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.02185</link><description>&lt;p&gt;
&#20174;&#34394;&#20551;&#26032;&#38395;&#21040;&#20559;&#25191;&#26032;&#38395;&#30340;&#39046;&#22495;&#36866;&#24212;&#20013;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
From Fake to Hyperpartisan News Detection Using Domain Adaptation. (arXiv:2308.02185v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02185
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#21644;&#20559;&#25191;&#26032;&#38395;&#26816;&#27979;&#20043;&#38388;&#24212;&#29992;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#25216;&#26415;&#30340;&#25928;&#26524;&#65292;&#36890;&#36807;&#30693;&#35782;&#20256;&#36882;&#21644;&#25968;&#25454;&#22686;&#24378;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#21516;&#26102;&#23558;&#32858;&#31867;&#21644;&#20027;&#39064;&#24314;&#27169;&#31639;&#27861;&#19982;UDA&#30456;&#32467;&#21512;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#39046;&#22495;&#36866;&#24212;&#65288;UDA&#65289;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#25216;&#26415;&#65292;&#26088;&#22312;&#20943;&#23569;&#20004;&#20010;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#30340;&#39046;&#22495;&#28418;&#31227;&#12290;&#23427;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#24471;&#21040;&#20102;&#25104;&#21151;&#30340;&#24212;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22312;&#20004;&#20010;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20043;&#38388;&#24212;&#29992;&#21508;&#31181;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#25216;&#26415;&#65288;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#21644;&#20559;&#25191;&#26032;&#38395;&#26816;&#27979;&#65289;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#19981;&#28041;&#21450;&#30446;&#26631;&#26631;&#31614;&#30340;&#34394;&#20551;&#26032;&#38395;&#21040;&#20559;&#25191;&#26032;&#38395;&#26816;&#27979;&#30340;&#30693;&#35782;&#20256;&#36882;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;UDA&#12289;&#24102;&#26377;&#25945;&#24072;&#30340;&#32858;&#31867;&#23545;&#40784;&#21644;&#36328;&#39046;&#22495;&#23545;&#27604;&#23398;&#20064;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#25216;&#26415;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#32780;&#21253;&#25324;&#25968;&#25454;&#22686;&#24378;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#32858;&#31867;&#21644;&#20027;&#39064;&#24314;&#27169;&#31639;&#27861;&#19982;UDA&#30456;&#32467;&#21512;&#65292;&#32467;&#26524;&#27604;&#21021;&#22987;&#30340;UDA&#35774;&#32622;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Domain Adaptation (UDA) is a popular technique that aims to reduce the domain shift between two data distributions. It was successfully applied in computer vision and natural language processing. In the current work, we explore the effects of various unsupervised domain adaptation techniques between two text classification tasks: fake and hyperpartisan news detection. We investigate the knowledge transfer from fake to hyperpartisan news detection without involving target labels during training. Thus, we evaluate UDA, cluster alignment with a teacher, and cross-domain contrastive learning. Extensive experiments show that these techniques improve performance, while including data augmentation further enhances the results. In addition, we combine clustering and topic modeling algorithms with UDA, resulting in improved performances compared to the initial UDA setup.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25193;&#23637;&#20020;&#24202;&#35797;&#39564;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#24182;&#20197;&#32959;&#30244;&#23398;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#20808;&#36827;&#30340;LLMs&#33021;&#22815;&#22788;&#29702;&#20020;&#24202;&#35797;&#39564;&#30340;&#22797;&#26434;&#26465;&#20214;&#21644;&#21305;&#37197;&#36923;&#36753;&#65292;&#30456;&#36739;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#24615;&#33021;&#26174;&#33879;&#25552;&#21319;&#65292;&#24182;&#21487;&#20316;&#20026;&#20154;&#24037;&#36741;&#21161;&#31579;&#36873;&#24739;&#32773;-&#35797;&#39564;&#20505;&#36873;&#20154;&#30340;&#21021;&#27493;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2308.02180</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#20020;&#24202;&#35797;&#39564;&#21305;&#37197;&#65306;&#20197;&#32959;&#30244;&#23398;&#20026;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Scaling Clinical Trial Matching Using Large Language Models: A Case Study in Oncology. (arXiv:2308.02180v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02180
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25193;&#23637;&#20020;&#24202;&#35797;&#39564;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#24182;&#20197;&#32959;&#30244;&#23398;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#20808;&#36827;&#30340;LLMs&#33021;&#22815;&#22788;&#29702;&#20020;&#24202;&#35797;&#39564;&#30340;&#22797;&#26434;&#26465;&#20214;&#21644;&#21305;&#37197;&#36923;&#36753;&#65292;&#30456;&#36739;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#24615;&#33021;&#26174;&#33879;&#25552;&#21319;&#65292;&#24182;&#21487;&#20316;&#20026;&#20154;&#24037;&#36741;&#21161;&#31579;&#36873;&#24739;&#32773;-&#35797;&#39564;&#20505;&#36873;&#20154;&#30340;&#21021;&#27493;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#35797;&#39564;&#21305;&#37197;&#26159;&#21307;&#30103;&#20256;&#36882;&#21644;&#21457;&#29616;&#20013;&#30340;&#20851;&#38190;&#36807;&#31243;&#12290;&#23454;&#38469;&#19978;&#65292;&#30001;&#20110;&#24222;&#22823;&#30340;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#21644;&#19981;&#21487;&#25193;&#23637;&#30340;&#25163;&#21160;&#22788;&#29702;&#65292;&#35813;&#36807;&#31243;&#23384;&#22312;&#38382;&#39064;&#12290;&#26412;&#25991;&#36890;&#36807;&#20197;&#32959;&#30244;&#23398;&#20026;&#37325;&#28857;&#39046;&#22495;&#65292;&#23545;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25193;&#23637;&#20020;&#24202;&#35797;&#39564;&#21305;&#37197;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#22522;&#20110;&#19968;&#20010;&#27491;&#22312;&#32654;&#22269;&#19968;&#20010;&#22823;&#22411;&#21307;&#30103;&#32593;&#32476;&#36827;&#34892;&#27979;&#35797;&#37096;&#32626;&#30340;&#20020;&#24202;&#35797;&#39564;&#21305;&#37197;&#31995;&#32479;&#12290;&#21021;&#27493;&#32467;&#26524;&#20196;&#20154;&#40723;&#33310;&#65306;&#20808;&#36827;&#30340;LLM&#65288;&#22914;GPT-4&#65289;&#21487;&#20197;&#31435;&#21363;&#36830;&#25509;&#20020;&#24202;&#35797;&#39564;&#30340;&#22797;&#26434;&#30340;&#21512;&#26684;&#26465;&#20214;&#65292;&#24182;&#25552;&#21462;&#22797;&#26434;&#30340;&#21305;&#37197;&#36923;&#36753;&#65288;&#20363;&#22914;&#23884;&#22871;&#30340;AND/OR/NOT&#65289;&#12290;&#34429;&#28982;&#20173;&#19981;&#23436;&#32654;&#65292;LLM&#22312;&#24615;&#33021;&#19978;&#26174;&#33879;&#20248;&#20110;&#20197;&#21069;&#30340;&#24378;&#22522;&#20934;&#32447;&#65292;&#24182;&#21487;&#33021;&#20316;&#20026;&#22312;&#20154;&#19982;&#20154;&#20043;&#38388;&#36827;&#34892;&#20505;&#36873;&#24739;&#32773;-&#35797;&#39564;&#21010;&#20998;&#30340;&#21021;&#27493;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#25581;&#31034;&#20102;&#19968;&#20123;&#24212;&#29992;LLM&#36827;&#34892;&#31471;&#21040;&#31471;&#20020;&#24202;&#35797;&#39564;&#21305;&#37197;&#30340;&#37325;&#35201;&#22686;&#38271;&#39046;&#22495;&#65292;&#20363;&#22914;&#19978;&#19979;&#25991;&#38480;&#21046;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical trial matching is a key process in health delivery and discovery. In practice, it is plagued by overwhelming unstructured data and unscalable manual processing. In this paper, we conduct a systematic study on scaling clinical trial matching using large language models (LLMs), with oncology as the focus area. Our study is grounded in a clinical trial matching system currently in test deployment at a large U.S. health network. Initial findings are promising: out of box, cutting-edge LLMs, such as GPT-4, can already structure elaborate eligibility criteria of clinical trials and extract complex matching logic (e.g., nested AND/OR/NOT). While still far from perfect, LLMs substantially outperform prior strong baselines and may serve as a preliminary solution to help triage patient-trial candidates with humans in the loop. Our study also reveals a few significant growth areas for applying LLMs to end-to-end clinical trial matching, such as context limitation and accuracy, especially
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#32771;&#34385;&#26469;&#33258;&#26032;&#38395;&#20114;&#21160;&#21382;&#21490;&#30340;&#24402;&#23487;&#22240;&#32032;&#21644;&#26469;&#33258;&#30456;&#24212;&#26032;&#38395;&#30340;&#22330;&#21512;&#22240;&#32032;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#26469;&#29702;&#35299;&#26032;&#38395;&#35780;&#35770;&#34892;&#20026;&#65292;&#21516;&#26102;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#22312;&#35835;&#32773;&#24863;&#30693;&#26032;&#38395;&#25688;&#35201;&#21644;&#26032;&#38395;&#26041;&#38754;-&#35266;&#28857;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2308.02168</link><description>&lt;p&gt;
&#20320;&#35828;&#20320;&#35835;&#30340;&#19996;&#35199;&#65306;&#36890;&#36807;&#24402;&#23487;&#21644;&#22330;&#21512;&#24402;&#22240;&#26469;&#29702;&#35299;&#26032;&#38395;&#35780;&#35770;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
You talk what you read: Understanding News Comment Behavior by Dispositional and Situational Attribution. (arXiv:2308.02168v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#32771;&#34385;&#26469;&#33258;&#26032;&#38395;&#20114;&#21160;&#21382;&#21490;&#30340;&#24402;&#23487;&#22240;&#32032;&#21644;&#26469;&#33258;&#30456;&#24212;&#26032;&#38395;&#30340;&#22330;&#21512;&#22240;&#32032;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#26469;&#29702;&#35299;&#26032;&#38395;&#35780;&#35770;&#34892;&#20026;&#65292;&#21516;&#26102;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#22312;&#35835;&#32773;&#24863;&#30693;&#26032;&#38395;&#25688;&#35201;&#21644;&#26032;&#38395;&#26041;&#38754;-&#35266;&#28857;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26032;&#38395;&#35780;&#35770;&#25366;&#25496;&#30740;&#31350;&#22522;&#20110;&#35780;&#35770;&#19982;&#30456;&#24212;&#26032;&#38395;&#30340;&#26126;&#30830;&#20851;&#32852;&#30340;&#20551;&#35774;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#29992;&#25143;&#30340;&#35780;&#35770;&#20063;&#21463;&#21040;&#20854;&#20114;&#21160;&#21382;&#21490;&#25152;&#20307;&#29616;&#30340;&#20010;&#20307;&#29305;&#24449;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#32771;&#34385;&#26469;&#33258;&#26032;&#38395;&#20114;&#21160;&#21382;&#21490;&#30340;&#24402;&#23487;&#22240;&#32032;&#21644;&#26469;&#33258;&#30456;&#24212;&#26032;&#38395;&#30340;&#22330;&#21512;&#22240;&#32032;&#26469;&#29702;&#35299;&#26032;&#38395;&#35780;&#35770;&#34892;&#20026;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#19977;&#20010;&#37096;&#20998;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#65292;&#29992;&#20110;&#24314;&#27169;&#26032;&#38395;&#35780;&#35770;&#30340;&#29983;&#25104;&#36807;&#31243;&#12290;&#25152;&#24471;&#21040;&#30340;&#24402;&#23487;&#21644;&#22330;&#21512;&#24402;&#22240;&#26377;&#21161;&#20110;&#29702;&#35299;&#29992;&#25143;&#30340;&#20851;&#27880;&#28857;&#21644;&#35266;&#28857;&#65292;&#24182;&#22312;&#35835;&#32773;&#24863;&#30693;&#26032;&#38395;&#25688;&#35201;&#21644;&#26032;&#38395;&#26041;&#38754;-&#35266;&#28857;&#39044;&#27979;&#30340;&#24212;&#29992;&#20013;&#36827;&#34892;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many news comment mining studies are based on the assumption that comment is explicitly linked to the corresponding news. In this paper, we observed that users' comments are also heavily influenced by their individual characteristics embodied by the interaction history. Therefore, we position to understand news comment behavior by considering both the dispositional factors from news interaction history, and the situational factors from corresponding news. A three-part encoder-decoder framework is proposed to model the generative process of news comment. The resultant dispositional and situational attribution contributes to understanding user focus and opinions, which are validated in applications of reader-aware news summarization and news aspect-opinion forecasting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#21457;&#35328;&#20154;&#26085;&#21270;&#20219;&#21153;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#21046;&#20316;&#33050;&#26412;&#25552;&#21462;&#20266;&#26631;&#31614;&#25968;&#25454;&#65292;&#24182;&#30456;&#23545;&#20110;&#26080;&#30417;&#30563;&#22522;&#32447;&#27169;&#22411;&#23454;&#29616;&#20102;51.7%&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2308.02160</link><description>&lt;p&gt;
&#35270;&#21548;&#20869;&#23481;&#30340;&#21457;&#35328;&#20154;&#26085;&#21270;
&lt;/p&gt;
&lt;p&gt;
Speaker Diarization of Scripted Audiovisual Content. (arXiv:2308.02160v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#21457;&#35328;&#20154;&#26085;&#21270;&#20219;&#21153;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#21046;&#20316;&#33050;&#26412;&#25552;&#21462;&#20266;&#26631;&#31614;&#25968;&#25454;&#65292;&#24182;&#30456;&#23545;&#20110;&#26080;&#30417;&#30563;&#22522;&#32447;&#27169;&#22411;&#23454;&#29616;&#20102;51.7%&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23186;&#20307;&#26412;&#22320;&#21270;&#34892;&#19994;&#36890;&#24120;&#38656;&#35201;&#19968;&#20010;&#19982;&#26368;&#32456;&#30005;&#24433;&#25110;&#30005;&#35270;&#21046;&#20316;&#30340;&#21407;&#25991;&#33050;&#26412;&#30456;&#31526;&#30340;&#33050;&#26412;&#65292;&#20197;&#20415;&#22312;&#22806;&#35821;&#20013;&#21019;&#24314;&#23383;&#24149;&#25110;&#37197;&#38899;&#33050;&#26412;&#12290;&#29305;&#21035;&#26159;&#65292;&#21363;&#25773;&#20986;&#33050;&#26412;&#24517;&#39035;&#34987;&#32467;&#26500;&#21270;&#20026;&#21253;&#21547;&#26102;&#38388;&#20195;&#30721;&#12289;&#21457;&#35328;&#20154;&#22995;&#21517;&#21644;&#36716;&#24405;&#30340;&#23545;&#35805;&#34892;&#24207;&#21015;&#12290;&#30446;&#21069;&#30340;&#35821;&#38899;&#35782;&#21035;&#25216;&#26415;&#21487;&#20197;&#20943;&#36731;&#36716;&#24405;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#21457;&#35328;&#20154;&#26085;&#21270;&#27169;&#22411;&#22312;&#30005;&#35270;&#33410;&#30446;&#19978;&#20173;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#65306;&#65288;&#19968;&#65289;&#26080;&#27861;&#36861;&#36394;&#22823;&#37327;&#21457;&#35328;&#20154;&#65292;&#65288;&#20108;&#65289;&#22312;&#26816;&#27979;&#39057;&#32321;&#30340;&#21457;&#35328;&#20154;&#26356;&#25442;&#26102;&#20934;&#30830;&#29575;&#36739;&#20302;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#22312;&#25293;&#25668;&#36807;&#31243;&#20013;&#20351;&#29992;&#30340;&#21046;&#20316;&#33050;&#26412;&#26469;&#25552;&#21462;&#29992;&#20110;&#21457;&#35328;&#20154;&#26085;&#21270;&#20219;&#21153;&#30340;&#20266;&#26631;&#31614;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21322;&#30417;&#30563;&#26041;&#27861;&#65292;&#24182;&#22312;&#25105;&#20204;&#30340;&#24230;&#37327;&#32467;&#26524;&#19978;&#65292;&#22312;66&#20010;&#33410;&#30446;&#30340;&#27979;&#35797;&#38598;&#19978;&#30456;&#23545;&#20110;&#20004;&#20010;&#26080;&#30417;&#30563;&#22522;&#32447;&#27169;&#22411;&#23454;&#29616;&#20102;51.7%&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
The media localization industry usually requires a verbatim script of the final film or TV production in order to create subtitles or dubbing scripts in a foreign language. In particular, the verbatim script (i.e. as-broadcast script) must be structured into a sequence of dialogue lines each including time codes, speaker name and transcript. Current speech recognition technology alleviates the transcription step. However, state-of-the-art speaker diarization models still fall short on TV shows for two main reasons: (i) their inability to track a large number of speakers, (ii) their low accuracy in detecting frequent speaker changes. To mitigate this problem, we present a novel approach to leverage production scripts used during the shooting process, to extract pseudo-labeled data for the speaker diarization task. We propose a novel semi-supervised approach and demonstrate improvements of 51.7% relative to two unsupervised baseline models on our metrics on a 66 show test set.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#31574;&#30053;&#26799;&#24230;&#20248;&#21270;&#30340;&#22238;&#39038;&#24615;&#22823;&#22411;&#35821;&#35328;&#20195;&#29702;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#23398;&#20064;&#29615;&#22659;&#21453;&#39304;&#26469;&#35843;&#25972;&#35821;&#35328;&#20195;&#29702;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#20248;&#21270;&#20854;&#24615;&#33021;&#12290;&#36825;&#31181;&#20195;&#29702;&#33021;&#22815;&#20174;&#22810;&#20010;&#29615;&#22659;&#21644;&#20219;&#21153;&#20013;&#23398;&#20064;&#22870;&#21169;&#65292;&#24182;&#36890;&#36807;&#24635;&#32467;&#20197;&#21069;&#20219;&#21153;&#30340;&#26681;&#26412;&#21407;&#22240;&#26469;&#25913;&#36827;&#35821;&#35328;&#20195;&#29702;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2308.02151</link><description>&lt;p&gt;
Retroformer&#65306;&#20351;&#29992;&#31574;&#30053;&#26799;&#24230;&#20248;&#21270;&#30340;&#22238;&#39038;&#24615;&#22823;&#22411;&#35821;&#35328;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization. (arXiv:2308.02151v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#31574;&#30053;&#26799;&#24230;&#20248;&#21270;&#30340;&#22238;&#39038;&#24615;&#22823;&#22411;&#35821;&#35328;&#20195;&#29702;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#23398;&#20064;&#29615;&#22659;&#21453;&#39304;&#26469;&#35843;&#25972;&#35821;&#35328;&#20195;&#29702;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#20248;&#21270;&#20854;&#24615;&#33021;&#12290;&#36825;&#31181;&#20195;&#29702;&#33021;&#22815;&#20174;&#22810;&#20010;&#29615;&#22659;&#21644;&#20219;&#21153;&#20013;&#23398;&#20064;&#22870;&#21169;&#65292;&#24182;&#36890;&#36807;&#24635;&#32467;&#20197;&#21069;&#20219;&#21153;&#30340;&#26681;&#26412;&#21407;&#22240;&#26469;&#25913;&#36827;&#35821;&#35328;&#20195;&#29702;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#20010;&#26376;&#65292;&#20986;&#29616;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#26032;&#36235;&#21183;&#65292;&#21363;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22686;&#24378;&#25104;&#33021;&#22815;&#33258;&#20027;&#23436;&#25104;&#30446;&#26631;&#23548;&#21521;&#22810;&#27493;&#39588;&#20219;&#21153;&#30340;&#35821;&#35328;&#20195;&#29702;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#22238;&#31572;&#20154;&#31867;&#29992;&#25143;&#30340;&#26597;&#35810;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#35821;&#35328;&#20195;&#29702;&#27809;&#26377;&#20351;&#29992;&#29615;&#22659;&#29305;&#23450;&#30340;&#22870;&#21169;&#36827;&#34892;&#20248;&#21270;&#12290;&#23613;&#31649;&#19968;&#20123;&#20195;&#29702;&#36890;&#36807;&#21475;&#22836;&#21453;&#39304;&#23454;&#29616;&#20102;&#36845;&#20195;&#25913;&#36827;&#65292;&#20294;&#23427;&#20204;&#19981;&#33021;&#20197;&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#22870;&#21169;&#23398;&#20064;&#30456;&#20860;&#23481;&#30340;&#26041;&#24335;&#36827;&#34892;&#25512;&#29702;&#21644;&#35268;&#21010;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21407;&#21017;&#24615;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#22238;&#39038;&#27169;&#22411;&#65292;&#36890;&#36807;&#31574;&#30053;&#26799;&#24230;&#33258;&#21160;&#35843;&#25972;&#35821;&#35328;&#20195;&#29702;&#30340;&#25552;&#31034;&#65292;&#20174;&#29615;&#22659;&#21453;&#39304;&#20013;&#20248;&#21270;&#20195;&#29702;&#30340;&#24037;&#20316;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#20195;&#29702;&#26550;&#26500;&#36890;&#36807;&#23398;&#20064;&#22810;&#20010;&#29615;&#22659;&#21644;&#20219;&#21153;&#30340;&#22870;&#21169;&#26469;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#36890;&#36807;&#24635;&#32467;&#20197;&#21069;&#20219;&#21153;&#30340;&#26681;&#26412;&#21407;&#22240;&#26469;&#25913;&#36827;&#35821;&#35328;&#20195;&#29702;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent months have seen the emergence of a powerful new trend in which large language models (LLMs) are augmented to become autonomous language agents capable of performing objective oriented multi-step tasks on their own, rather than merely responding to queries from human users. Most existing language agents, however, are not optimized using environment-specific rewards. Although some agents enable iterative refinement through verbal feedback, they do not reason and plan in ways that are compatible with gradient-based learning from rewards. This paper introduces a principled framework for reinforcing large language agents by learning a retrospective model, which automatically tunes the language agent prompts from environment feedback through policy gradient. Specifically, our proposed agent architecture learns from rewards across multiple environments and tasks, for fine-tuning a pre-trained language model which refines the language agent prompt by summarizing the root cause of prior
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Tweet Insights&#30340;&#21487;&#35270;&#21270;&#24179;&#21488;&#65292;&#21487;&#20197;&#20174;Twitter&#20013;&#25552;&#21462;&#26102;&#38388;&#30456;&#20851;&#20449;&#24687;&#12290;&#23427;&#20351;&#29992;&#35789;&#23884;&#20837;&#25216;&#26415;&#21644;&#19987;&#38376;&#20248;&#21270;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;Twitter&#30340;&#22823;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#21518;&#22788;&#29702;&#65292;&#24182;&#36890;&#36807;&#30028;&#38754;&#23545;&#26102;&#38388;&#36827;&#34892;&#20998;&#26512;&#65292;&#20197;&#26816;&#27979;&#21644;&#25551;&#36848;&#24847;&#20041;&#36716;&#21464;&#30340;&#21464;&#21270;&#65292;&#21253;&#25324;&#19982;&#36235;&#21183;&#24230;&#37327;&#30456;&#20851;&#30340;&#34917;&#20805;&#20449;&#24687;&#65292;&#22914;&#24773;&#24863;&#21644;&#20027;&#39064;&#30340;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.02142</link><description>&lt;p&gt;
Tweet Insights: &#20174;Twitter&#25552;&#21462;&#26102;&#38388;&#30456;&#20851;&#20449;&#24687;&#30340;&#21487;&#35270;&#21270;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
Tweet Insights: A Visualization Platform to Extract Temporal Insights from Twitter. (arXiv:2308.02142v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02142
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Tweet Insights&#30340;&#21487;&#35270;&#21270;&#24179;&#21488;&#65292;&#21487;&#20197;&#20174;Twitter&#20013;&#25552;&#21462;&#26102;&#38388;&#30456;&#20851;&#20449;&#24687;&#12290;&#23427;&#20351;&#29992;&#35789;&#23884;&#20837;&#25216;&#26415;&#21644;&#19987;&#38376;&#20248;&#21270;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;Twitter&#30340;&#22823;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#21518;&#22788;&#29702;&#65292;&#24182;&#36890;&#36807;&#30028;&#38754;&#23545;&#26102;&#38388;&#36827;&#34892;&#20998;&#26512;&#65292;&#20197;&#26816;&#27979;&#21644;&#25551;&#36848;&#24847;&#20041;&#36716;&#21464;&#30340;&#21464;&#21270;&#65292;&#21253;&#25324;&#19982;&#36235;&#21183;&#24230;&#37327;&#30456;&#20851;&#30340;&#34917;&#20805;&#20449;&#24687;&#65292;&#22914;&#24773;&#24863;&#21644;&#20027;&#39064;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#35789;&#23884;&#20837;&#25216;&#26415;&#21644;&#19987;&#38376;&#20248;&#21270;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;Twitter&#30340;&#22823;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#21518;&#22788;&#29702;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#25968;&#25454;&#28085;&#30422;&#20102;&#36807;&#21435;&#20116;&#24180;&#30340;&#21464;&#21270;&#65292;&#21253;&#25324;n-gram&#39057;&#29575;&#12289;&#30456;&#20284;&#24230;&#12289;&#24773;&#24863;&#21644;&#20027;&#39064;&#20998;&#24067;&#30340;&#21464;&#21270;&#12290;&#36890;&#36807;&#22312;&#36825;&#20123;&#25968;&#25454;&#20043;&#19978;&#26500;&#24314;&#30340;&#30028;&#38754;&#65292;&#21487;&#20197;&#23545;&#26102;&#38388;&#36827;&#34892;&#20998;&#26512;&#65292;&#20197;&#20415;&#26816;&#27979;&#21644;&#25551;&#36848;&#24847;&#20041;&#36716;&#21464;&#30340;&#21464;&#21270;&#65292;&#21253;&#25324;&#19982;&#36235;&#21183;&#24230;&#37327;&#30456;&#20851;&#30340;&#34917;&#20805;&#20449;&#24687;&#65292;&#22914;&#24773;&#24863;&#21644;&#20027;&#39064;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#22312;&#32447;&#28436;&#31034;&#20379;&#36731;&#26494;&#23454;&#39564;&#65292;&#24182;&#20849;&#20139;&#20195;&#30721;&#21644;&#22522;&#30784;&#27719;&#24635;&#25968;&#25454;&#20197;&#20379;&#26410;&#26469;&#30740;&#31350;&#20351;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#22522;&#20110;&#25105;&#20204;&#24179;&#21488;&#30340;&#19977;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#23427;&#22312;&#26102;&#38388;&#35821;&#35328;&#20998;&#26512;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a large collection of time series data derived from Twitter, postprocessed using word embedding techniques, as well as specialized fine-tuned language models. This data comprises the past five years and captures changes in n-gram frequency, similarity, sentiment and topic distribution. The interface built on top of this data enables temporal analysis for detecting and characterizing shifts in meaning, including complementary information to trending metrics, such as sentiment and topic association over time. We release an online demo for easy experimentation, and we share code and the underlying aggregated data for future work. In this paper, we also discuss three case studies unlocked thanks to our platform, showcasing its potential for temporal linguistic analysis.
&lt;/p&gt;</description></item><item><title>ParaFuzz&#26159;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#24615;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#26816;&#27979;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#27602;&#26679;&#26412;&#12290;&#35813;&#25216;&#26415;&#36890;&#36807;&#35266;&#23519;&#27169;&#22411;&#22312;&#37325;&#20889;&#36807;&#30340;&#24178;&#20928;&#26679;&#26412;&#21644;&#27745;&#26579;&#26679;&#26412;&#19978;&#30340;&#39044;&#27979;&#31283;&#23450;&#24615;&#65292;&#26469;&#21028;&#26029;&#26679;&#26412;&#26159;&#21542;&#34987;&#27745;&#26579;&#12290;</title><link>http://arxiv.org/abs/2308.02122</link><description>&lt;p&gt;
ParaFuzz&#65306;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#24615;&#30340;&#25216;&#26415;&#29992;&#20110;&#26816;&#27979;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#27602;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
ParaFuzz: An Interpretability-Driven Technique for Detecting Poisoned Samples in NLP. (arXiv:2308.02122v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02122
&lt;/p&gt;
&lt;p&gt;
ParaFuzz&#26159;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#24615;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#26816;&#27979;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#27602;&#26679;&#26412;&#12290;&#35813;&#25216;&#26415;&#36890;&#36807;&#35266;&#23519;&#27169;&#22411;&#22312;&#37325;&#20889;&#36807;&#30340;&#24178;&#20928;&#26679;&#26412;&#21644;&#27745;&#26579;&#26679;&#26412;&#19978;&#30340;&#39044;&#27979;&#31283;&#23450;&#24615;&#65292;&#26469;&#21028;&#26029;&#26679;&#26412;&#26159;&#21542;&#34987;&#27745;&#26579;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20621;&#38376;&#25915;&#20987;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#27169;&#22411;&#30340;&#37325;&#35201;&#23041;&#32961;&#65292;&#20854;&#20013;&#22312;&#36755;&#20837;&#20013;&#23384;&#22312;&#29305;&#23450;&#35302;&#21457;&#22120;&#21487;&#20197;&#23548;&#33268;&#34987;&#27745;&#26579;&#30340;&#27169;&#22411;&#23558;&#36825;&#20123;&#36755;&#20837;&#35823;&#20998;&#31867;&#20026;&#39044;&#23450;&#30340;&#30446;&#26631;&#31867;&#21035;&#12290;&#24403;&#21069;&#30340;&#26816;&#27979;&#26426;&#21046;&#21463;&#21040;&#38480;&#21046;&#65292;&#26080;&#27861;&#24212;&#23545;&#26356;&#38544;&#34109;&#30340;&#20621;&#38376;&#31574;&#30053;&#65292;&#22914;&#22522;&#20110;&#39118;&#26684;&#30340;&#25915;&#20987;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#27979;&#35797;&#26102;&#27745;&#26579;&#26679;&#26412;&#26816;&#27979;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20381;&#36182;&#20110;&#27169;&#22411;&#39044;&#27979;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#19982;&#36755;&#20837;&#30340;&#35821;&#20041;&#21547;&#20041;&#26377;&#20851;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#35302;&#21457;&#22120;&#65288;&#20363;&#22914;&#65292;&#19981;&#24120;&#35265;&#30340;&#21333;&#35789;&#65289;&#19981;&#24212;&#35813;&#20174;&#26681;&#26412;&#19978;&#25913;&#21464;&#34987;&#27745;&#26579;&#26679;&#26412;&#30340;&#22522;&#26412;&#35821;&#20041;&#21547;&#20041;&#65292;&#22240;&#20026;&#23427;&#20204;&#24819;&#20445;&#25345;&#28508;&#20239;&#12290;&#22522;&#20110;&#36825;&#20010;&#35266;&#23519;&#65292;&#25105;&#20204;&#20551;&#35774;&#22312;&#25913;&#20889;&#36807;&#31243;&#20013;&#65292;&#27169;&#22411;&#23545;&#20110;&#37325;&#20889;&#36807;&#30340;&#24178;&#20928;&#26679;&#26412;&#30340;&#39044;&#27979;&#24212;&#35813;&#20445;&#25345;&#31283;&#23450;&#65292;&#32780;&#23545;&#20110;&#27745;&#26579;&#26679;&#26412;&#30340;&#39044;&#27979;&#22312;&#35302;&#21457;&#22120;&#30340;&#31361;&#21464;&#36807;&#31243;&#20013;&#24212;&#35813;&#24674;&#22797;&#21040;&#30495;&#23454;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
Backdoor attacks have emerged as a prominent threat to natural language processing (NLP) models, where the presence of specific triggers in the input can lead poisoned models to misclassify these inputs to predetermined target classes. Current detection mechanisms are limited by their inability to address more covert backdoor strategies, such as style-based attacks. In this work, we propose an innovative test-time poisoned sample detection framework that hinges on the interpretability of model predictions, grounded in the semantic meaning of inputs. We contend that triggers (e.g., infrequent words) are not supposed to fundamentally alter the underlying semantic meanings of poisoned samples as they want to stay stealthy. Based on this observation, we hypothesize that while the model's predictions for paraphrased clean samples should remain stable, predictions for poisoned samples should revert to their true labels upon the mutations applied to triggers during the paraphrasing process. W
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GCGTS&#30340;&#22522;&#20110;&#23383;&#31526;&#20851;&#31995;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#20013;&#25991;&#37329;&#34701;&#25991;&#26412;&#20013;&#21516;&#26102;&#25552;&#21462;&#26041;&#38754;-&#35266;&#28857;&#23545;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#26126;&#30830;&#22320;&#25972;&#21512;&#20102;&#21477;&#27861;&#32467;&#26500;&#65292;&#24182;&#32479;&#19968;&#20102;&#35789;&#20869;&#23383;&#31526;&#30340;&#32534;&#30721;&#65292;&#20197;&#25552;&#39640;&#25552;&#21462;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.02113</link><description>&lt;p&gt;
&#20013;&#25991;&#37329;&#34701;&#25991;&#26412;&#24773;&#24863;&#25366;&#25496;&#65306;&#19968;&#31181;&#22522;&#20110;&#23383;&#31526;&#20851;&#31995;&#30340;&#21516;&#26102;&#25552;&#21462;&#26041;&#38754;-&#35266;&#28857;&#23545;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Chinese Financial Text Emotion Mining: GCGTS -- A Character Relationship-based Approach for Simultaneous Aspect-Opinion Pair Extraction. (arXiv:2308.02113v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02113
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GCGTS&#30340;&#22522;&#20110;&#23383;&#31526;&#20851;&#31995;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#20013;&#25991;&#37329;&#34701;&#25991;&#26412;&#20013;&#21516;&#26102;&#25552;&#21462;&#26041;&#38754;-&#35266;&#28857;&#23545;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#26126;&#30830;&#22320;&#25972;&#21512;&#20102;&#21477;&#27861;&#32467;&#26500;&#65292;&#24182;&#32479;&#19968;&#20102;&#35789;&#20869;&#23383;&#31526;&#30340;&#32534;&#30721;&#65292;&#20197;&#25552;&#39640;&#25552;&#21462;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20013;&#22269;&#30340;&#37329;&#34701;&#25991;&#26412;&#20013;&#25552;&#21462;&#26041;&#38754;-&#35266;&#28857;&#23545;&#65288;AOPE&#65289;&#26159;&#32454;&#31890;&#24230;&#25991;&#26412;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#19987;&#38376;&#20219;&#21153;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#21516;&#26102;&#20174;&#21508;&#31181;&#19981;&#21516;&#30340;&#37329;&#34701;&#25991;&#26412;&#20013;&#25552;&#21462;&#26041;&#38754;&#26415;&#35821;&#21644;&#35266;&#28857;&#26415;&#35821;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#22522;&#20110;&#32593;&#26684;&#30340;&#27169;&#22411;&#20013;&#24320;&#21457;&#32593;&#26684;&#26631;&#27880;&#26041;&#26696;&#65292;&#20197;&#20415;&#20419;&#36827;&#25552;&#21462;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#23383;&#31526;&#32423;&#65288;&#26631;&#35760;&#32423;&#65289;&#29305;&#24449;&#32534;&#30721;&#65292;&#21487;&#33021;&#24573;&#35270;&#20102;&#20013;&#25991;&#23383;&#31526;&#22312;&#35789;&#20869;&#30340;&#36923;&#36753;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#22522;&#20110;&#22270;&#24418;&#30340;&#23383;&#31526;&#32423;&#32593;&#26684;&#26631;&#27880;&#26041;&#26696;&#65288;GCGTS&#65289;&#12290;GCGTS&#26041;&#27861;&#20351;&#29992;&#22270;&#24418;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#26126;&#30830;&#22320;&#32467;&#21512;&#20102;&#21477;&#27861;&#32467;&#26500;&#65292;&#24182;&#32479;&#19968;&#20102;&#21516;&#19968;&#21477;&#27861;&#35821;&#20041;&#21333;&#20803;&#65288;&#20013;&#25991;&#35789;&#32423;&#21035;&#65289;&#20869;&#23383;&#31526;&#30340;&#32534;&#30721;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#22270;&#24418;&#27169;&#22411;&#24341;&#20837;&#32593;&#26684;&#27169;&#22411;&#20013;&#65292;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#23616;&#37096;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aspect-Opinion Pair Extraction (AOPE) from Chinese financial texts is a specialized task in fine-grained text sentiment analysis. The main objective is to extract aspect terms and opinion terms simultaneously from a diverse range of financial texts. Previous studies have mainly focused on developing grid annotation schemes within grid-based models to facilitate this extraction process. However, these methods often rely on character-level (token-level) feature encoding, which may overlook the logical relationships between Chinese characters within words. To address this limitation, we propose a novel method called Graph-based Character-level Grid Tagging Scheme (GCGTS). The GCGTS method explicitly incorporates syntactic structure using Graph Convolutional Networks (GCN) and unifies the encoding of characters within the same syntactic semantic unit (Chinese word level). Additionally, we introduce an image convolutional structure into the grid model to better capture the local relationshi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Prompt2Gaussia&#26041;&#27861;&#65292;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#25552;&#31034;&#23398;&#20064;&#21644;&#39640;&#26031;&#20998;&#24067;&#26469;&#35299;&#20915;&#33050;&#26412;&#20107;&#20214;&#39044;&#27979;&#20013;&#30340;&#26080;&#27861;&#30830;&#23450;&#26368;&#20248;&#25552;&#31034;&#21644;&#26631;&#31614;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.02103</link><description>&lt;p&gt;
Prompt2Gaussia: &#19981;&#30830;&#23450;&#25552;&#31034;&#23398;&#20064;&#29992;&#20110;&#33050;&#26412;&#20107;&#20214;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Prompt2Gaussia: Uncertain Prompt-learning for Script Event Prediction. (arXiv:2308.02103v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Prompt2Gaussia&#26041;&#27861;&#65292;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#25552;&#31034;&#23398;&#20064;&#21644;&#39640;&#26031;&#20998;&#24067;&#26469;&#35299;&#20915;&#33050;&#26412;&#20107;&#20214;&#39044;&#27979;&#20013;&#30340;&#26080;&#27861;&#30830;&#23450;&#26368;&#20248;&#25552;&#31034;&#21644;&#26631;&#31614;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33050;&#26412;&#20107;&#20214;&#39044;&#27979;&#26088;&#22312;&#39044;&#27979;&#32473;&#23450;&#20107;&#20214;&#38142;&#20013;&#30340;&#21518;&#32493;&#20107;&#20214;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#25972;&#21512;&#22806;&#37096;&#30693;&#35782;&#26469;&#22686;&#24378;&#35821;&#20041;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#26159;&#33719;&#21462;&#36866;&#24403;&#30340;&#30693;&#35782;&#36164;&#28304;&#21644;&#26816;&#32034;&#19982;&#33050;&#26412;&#30456;&#20851;&#30340;&#30693;&#35782;&#26159;&#36153;&#26102;&#36153;&#21147;&#30340;&#12290;&#26412;&#25991;&#23558;&#20844;&#20849;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#35270;&#20026;&#30693;&#35782;&#24211;&#65292;&#24182;&#36890;&#36807;&#25552;&#31034;&#23398;&#20064;&#33258;&#21160;&#25366;&#25496;&#19982;&#33050;&#26412;&#30456;&#20851;&#30340;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#33050;&#26412;&#20013;&#30340;&#22330;&#26223;&#22810;&#26679;&#24615;&#21644;&#26631;&#31614;&#27169;&#31946;&#24615;&#20351;&#24471;&#26500;&#36896;&#26368;&#26377;&#25928;&#30340;&#25552;&#31034;&#21644;&#26631;&#31614;&#20196;&#29260;&#22312;&#25552;&#31034;&#23398;&#20064;&#20013;&#21464;&#24471;&#19981;&#30830;&#23450;&#65292;&#21363;&#25552;&#31034;&#19981;&#30830;&#23450;&#24615;&#21644;&#34920;&#36848;&#19981;&#30830;&#23450;&#24615;&#12290;&#32771;&#34385;&#21040;&#39640;&#26031;&#20998;&#24067;&#34920;&#36798;&#19981;&#30830;&#23450;&#24615;&#30340;&#22266;&#26377;&#33021;&#21147;&#65292;&#25105;&#20204;&#23558;&#25552;&#31034;&#20196;&#29260;&#21644;&#26631;&#31614;&#20196;&#29260;&#37096;&#32626;&#20026;&#36981;&#24490;&#39640;&#26031;&#20998;&#24067;&#30340;&#38543;&#26426;&#21464;&#37327;&#65292;&#25552;&#20986;&#20102;&#25552;&#31034;&#20272;&#35745;&#22120;&#21644;&#34920;&#36848;&#20272;&#35745;&#22120;&#26469;&#20272;&#35745;&#23427;&#20204;&#30340;&#27010;&#29575;&#34920;&#31034;&#65292;&#32780;&#19981;&#26159;&#30830;&#23450;&#23427;&#20204;&#30340;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Script Event Prediction (SEP) aims to predict the subsequent event for a given event chain from a candidate list. Prior research has achieved great success by integrating external knowledge to enhance the semantics, but it is laborious to acquisite the appropriate knowledge resources and retrieve the script-related knowledge. In this paper, we regard public pre-trained language models as knowledge bases and automatically mine the script-related knowledge via prompt-learning. Still, the scenario-diversity and label-ambiguity in scripts make it uncertain to construct the most functional prompt and label token in prompt learning, i.e., prompt-uncertainty and verbalizer-uncertainty. Considering the innate ability of Gaussian distribution to express uncertainty, we deploy the prompt tokens and label tokens as random variables following Gaussian distributions, where a prompt estimator and a verbalizer estimator are proposed to estimate their probabilistic representations instead of determini
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;N-gram&#22686;&#24378;&#25216;&#26415;&#65292;&#36890;&#36807;&#35268;&#33539;&#21270;&#30446;&#26631;&#35789;&#32452;&#26469;&#25913;&#21892;&#19978;&#19979;&#25991;&#20559;&#24046;&#65292;&#25552;&#39640;&#20851;&#38190;&#35789;&#35782;&#21035;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.02092</link><description>&lt;p&gt;
N-gram&#22686;&#24378;&#65306;&#36890;&#36807;&#35268;&#33539;&#21270;N-gram&#30446;&#26631;&#25913;&#21892;&#19978;&#19979;&#25991;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
N-gram Boosting: Improving Contextual Biasing with Normalized N-gram Targets. (arXiv:2308.02092v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;N-gram&#22686;&#24378;&#25216;&#26415;&#65292;&#36890;&#36807;&#35268;&#33539;&#21270;&#30446;&#26631;&#35789;&#32452;&#26469;&#25913;&#21892;&#19978;&#19979;&#25991;&#20559;&#24046;&#65292;&#25552;&#39640;&#20851;&#38190;&#35789;&#35782;&#21035;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21830;&#21153;&#35848;&#35805;&#30340;&#35821;&#38899;&#21040;&#25991;&#26412;&#24212;&#29992;&#20013;&#65292;&#20934;&#30830;&#36716;&#24405;&#19987;&#26377;&#21517;&#35789;&#21644;&#25216;&#26415;&#26415;&#35821;&#23588;&#20026;&#37325;&#35201;&#12290;&#36825;&#20123;&#35789;&#23545;&#20110;&#29702;&#35299;&#23545;&#35805;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#24448;&#24448;&#24456;&#23569;&#20986;&#29616;&#65292;&#22240;&#27492;&#22312;&#25991;&#26412;&#21644;&#38899;&#39057;&#35757;&#32451;&#25968;&#25454;&#20013;&#24456;&#21487;&#33021;&#32570;&#20047;&#65292;&#36825;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#36896;&#25104;&#20102;&#24456;&#22823;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#27493;&#20851;&#38190;&#35789;&#22686;&#24378;&#26426;&#21046;&#65292;&#23427;&#25104;&#21151;&#22320;&#20351;&#29992;&#35268;&#33539;&#21270;&#30340;&#21333;&#35789;&#21644;n-gram&#32780;&#19981;&#20165;&#20165;&#26159;&#21333;&#20010;&#26631;&#35760;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#20851;&#38190;&#35789;&#30446;&#26631;&#30340;&#20002;&#22833;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#35843;&#25972;&#22686;&#24378;&#26435;&#37325;&#36923;&#36753;&#20197;&#36991;&#20813;&#36807;&#24230;&#22686;&#24378;&#22810;&#20010;&#26631;&#35760;&#30340;&#20851;&#38190;&#35789;&#12290;&#22312;&#25105;&#20204;&#30340;&#19987;&#26377;&#39046;&#22495;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#20851;&#38190;&#35789;&#35782;&#21035;&#29575;&#30456;&#23545;&#25552;&#39640;&#20102;26&#65285;&#65292;&#22312;LibriSpeech&#19978;&#25552;&#39640;&#20102;2&#65285;&#12290;&#36825;&#31181;&#26041;&#27861;&#23545;&#28041;&#21450;&#38750;&#23383;&#27597;&#23383;&#31526;&#25110;&#20855;&#26377;&#38750;&#26631;&#20934;&#21457;&#38899;&#30340;&#30446;&#26631;&#29305;&#21035;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate transcription of proper names and technical terms is particularly important in speech-to-text applications for business conversations. These words, which are essential to understanding the conversation, are often rare and therefore likely to be under-represented in text and audio training data, creating a significant challenge in this domain. We present a two-step keyword boosting mechanism that successfully works on normalized unigrams and n-grams rather than just single tokens, which eliminates missing hits issues with boosting raw targets. In addition, we show how adjusting the boosting weight logic avoids over-boosting multi-token keywords. This improves our keyword recognition rate by 26% relative on our proprietary in-domain dataset and 2% on LibriSpeech. This method is particularly useful on targets that involve non-alphabetic characters or have non-standard pronunciations.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#24179;&#21488;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#35299;&#32544;&#36755;&#20837;&#34920;&#31034;&#20026;&#19981;&#21464;&#29305;&#24449;&#21644;&#24179;&#21488;&#30456;&#20851;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#20010;&#26410;&#35265;&#24179;&#21488;&#30340;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.02080</link><description>&lt;p&gt;
&#36328;&#24179;&#21488;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20013;&#30340;&#22240;&#26524;&#24341;&#23548;&#35299;&#32544;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Causality Guided Disentanglement for Cross-Platform Hate Speech Detection. (arXiv:2308.02080v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02080
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#24179;&#21488;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#35299;&#32544;&#36755;&#20837;&#34920;&#31034;&#20026;&#19981;&#21464;&#29305;&#24449;&#21644;&#24179;&#21488;&#30456;&#20851;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#20010;&#26410;&#35265;&#24179;&#21488;&#30340;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#22312;&#20419;&#36827;&#20844;&#24320;&#23545;&#35805;&#26041;&#38754;&#20855;&#26377;&#20215;&#20540;&#65292;&#20294;&#20182;&#20204;&#32463;&#24120;&#34987;&#21033;&#29992;&#26469;&#20256;&#25773;&#26377;&#23475;&#20869;&#23481;&#12290;&#30446;&#21069;&#29992;&#20110;&#26816;&#27979;&#36825;&#31181;&#26377;&#23475;&#20869;&#23481;&#30340;&#28145;&#24230;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#36807;&#24230;&#20381;&#36182;&#20110;&#39046;&#22495;&#29305;&#23450;&#26415;&#35821;&#65292;&#24433;&#21709;&#21040;&#20102;&#23427;&#20204;&#36866;&#24212;&#27867;&#21270;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#33021;&#21147;&#12290;&#36825;&#26159;&#22240;&#20026;&#23427;&#20204;&#20542;&#21521;&#20110;&#36807;&#20110;&#29421;&#38552;&#22320;&#20851;&#27880;&#29305;&#23450;&#30340;&#35821;&#35328;&#20449;&#21495;&#25110;&#26576;&#20123;&#35789;&#35821;&#31867;&#21035;&#30340;&#20351;&#29992;&#12290;&#24403;&#24179;&#21488;&#32570;&#20047;&#39640;&#36136;&#37327;&#30340;&#26631;&#35760;&#25968;&#25454;&#29992;&#20110;&#35757;&#32451;&#26102;&#65292;&#21478;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#20986;&#29616;&#20102;&#65292;&#38656;&#35201;&#36328;&#24179;&#21488;&#27169;&#22411;&#26469;&#36866;&#24212;&#19981;&#21516;&#30340;&#20998;&#24067;&#36716;&#21270;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#36328;&#24179;&#21488;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#19968;&#20010;&#24179;&#21488;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#24182;&#25512;&#24191;&#21040;&#22810;&#20010;&#26410;&#35265;&#24179;&#21488;&#12290;&#20026;&#20102;&#23454;&#29616;&#23545;&#19981;&#21516;&#24179;&#21488;&#30340;&#33391;&#22909;&#27867;&#21270;&#24615;&#33021;&#65292;&#19968;&#31181;&#26041;&#27861;&#26159;&#23558;&#36755;&#20837;&#34920;&#31034;&#35299;&#32544;&#20026;&#19981;&#21464;&#29305;&#24449;&#21644;&#24179;&#21488;&#30456;&#20851;&#29305;&#24449;&#12290;&#25105;&#20204;&#36824;&#35748;&#20026;&#23398;&#20064;&#22240;&#26524;&#20851;&#31995;&#26159;&#25552;&#20379;&#26356;&#22909;&#35299;&#32544;&#21644;&#27867;&#21270;&#24615;&#33021;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social media platforms, despite their value in promoting open discourse, are often exploited to spread harmful content. Current deep learning and natural language processing models used for detecting this harmful content overly rely on domain-specific terms affecting their capabilities to adapt to generalizable hate speech detection. This is because they tend to focus too narrowly on particular linguistic signals or the use of certain categories of words. Another significant challenge arises when platforms lack high-quality annotated data for training, leading to a need for cross-platform models that can adapt to different distribution shifts. Our research introduces a cross-platform hate speech detection model capable of being trained on one platform's data and generalizing to multiple unseen platforms. To achieve good generalizability across platforms, one way is to disentangle the input representations into invariant and platform-dependent features. We also argue that learning causa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31639;&#27861;&#65292;&#29992;&#20110;&#23558;&#23395;&#33410;&#24615;&#20316;&#20026;&#20449;&#21495;&#26469;&#37325;&#26032;&#25490;&#24207;&#30005;&#23376;&#21830;&#21153;&#30340;&#33258;&#21160;&#23436;&#25104;&#21151;&#33021;&#65292;&#20174;&#32780;&#25552;&#39640;&#33258;&#21160;&#23436;&#25104;&#30340;&#30456;&#20851;&#24615;&#21644;&#19994;&#21153;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2308.02055</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#30340;&#30005;&#23376;&#21830;&#21153;&#33258;&#21160;&#23436;&#25104;&#30340;&#23395;&#33410;&#24615;&#37325;&#26032;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Seasonality Based Reranking of E-commerce Autocomplete Using Natural Language Queries. (arXiv:2308.02055v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31639;&#27861;&#65292;&#29992;&#20110;&#23558;&#23395;&#33410;&#24615;&#20316;&#20026;&#20449;&#21495;&#26469;&#37325;&#26032;&#25490;&#24207;&#30005;&#23376;&#21830;&#21153;&#30340;&#33258;&#21160;&#23436;&#25104;&#21151;&#33021;&#65292;&#20174;&#32780;&#25552;&#39640;&#33258;&#21160;&#23436;&#25104;&#30340;&#30456;&#20851;&#24615;&#21644;&#19994;&#21153;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26597;&#35810;&#33258;&#21160;&#23436;&#25104;&#65288;QAC&#65289;&#20063;&#34987;&#31216;&#20026;typeahead&#65292;&#23427;&#22312;&#29992;&#25143;&#22312;&#25628;&#32034;&#26694;&#20013;&#36755;&#20837;&#21069;&#32512;&#26102;&#24314;&#35758;&#23436;&#25972;&#26597;&#35810;&#21015;&#34920;&#12290;&#23427;&#26159;&#29616;&#20195;&#25628;&#32034;&#24341;&#25806;&#29305;&#21035;&#26159;&#22312;&#30005;&#23376;&#21830;&#21153;&#39046;&#22495;&#30340;&#20851;&#38190;&#21151;&#33021;&#20043;&#19968;&#12290;typeahead&#30340;&#30446;&#26631;&#20043;&#19968;&#26159;&#21521;&#29992;&#25143;&#24314;&#35758;&#19982;&#23395;&#33410;&#24615;&#30456;&#20851;&#30340;&#37325;&#35201;&#26597;&#35810;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31639;&#27861;&#65292;&#20197;&#23558;&#23395;&#33410;&#24615;&#20316;&#20026;&#19968;&#20010;&#20449;&#21495;&#24182;&#23545;QAC&#25490;&#24207;&#27169;&#22411;&#36827;&#34892;&#31471;&#21040;&#31471;&#35780;&#20272;&#12290;&#23558;&#23395;&#33410;&#24615;&#32435;&#20837;&#33258;&#21160;&#23436;&#25104;&#25490;&#24207;&#27169;&#22411;&#20013;&#21487;&#20197;&#25552;&#39640;&#33258;&#21160;&#23436;&#25104;&#30340;&#30456;&#20851;&#24615;&#21644;&#19994;&#21153;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Query autocomplete (QAC) also known as typeahead, suggests list of complete queries as user types prefix in the search box. It is one of the key features of modern search engines specially in e-commerce. One of the goals of typeahead is to suggest relevant queries to users which are seasonally important. In this paper we propose a neural network based natural language processing (NLP) algorithm to incorporate seasonality as a signal and present end to end evaluation of the QAC ranking model. Incorporating seasonality into autocomplete ranking model can improve autocomplete relevance and business metric.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32844;&#20301;&#25512;&#33616;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#22696;&#35199;&#21733;&#24037;&#20154;&#19968;&#30452;&#24314;&#35758;&#20302;&#34218;&#24037;&#20316;&#65292;&#24182;&#21521;&#22899;&#24615;&#26356;&#20542;&#21521;&#20110;&#25512;&#33616;&#31192;&#20070;&#32844;&#20301;&#12290;&#36825;&#39033;&#30740;&#31350;&#24378;&#35843;&#20102;&#29702;&#35299;LLMs&#20559;&#35265;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.02053</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#24179;&#31561;&#26426;&#20250;: &#36890;&#36807;&#32844;&#20301;&#25512;&#33616;&#25581;&#31034;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
The Unequal Opportunities of Large Language Models: Revealing Demographic Bias through Job Recommendations. (arXiv:2308.02053v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02053
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32844;&#20301;&#25512;&#33616;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#22696;&#35199;&#21733;&#24037;&#20154;&#19968;&#30452;&#24314;&#35758;&#20302;&#34218;&#24037;&#20316;&#65292;&#24182;&#21521;&#22899;&#24615;&#26356;&#20542;&#21521;&#20110;&#25512;&#33616;&#31192;&#20070;&#32844;&#20301;&#12290;&#36825;&#39033;&#30740;&#31350;&#24378;&#35843;&#20102;&#29702;&#35299;LLMs&#20559;&#35265;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#22312;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#20102;&#35299;&#36825;&#20123;&#20559;&#35265;&#23545;&#20110;&#29702;&#35299;&#22312;&#20351;&#29992;LLMs&#36827;&#34892;&#20915;&#31574;&#26102;&#28508;&#22312;&#30340;&#21518;&#32493;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#21382;&#21490;&#19978;&#22788;&#20110;&#21155;&#21183;&#30340;&#32676;&#20307;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#36890;&#36807;&#32844;&#20301;&#25512;&#33616;&#30340;&#35282;&#24230;&#20998;&#26512;&#21644;&#27604;&#36739;LLMs&#20013;&#30340;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#12290;&#25105;&#20204;&#36890;&#36807;&#27979;&#37327;ChatGPT&#21644;LLaMA&#36825;&#20004;&#20010;&#21069;&#27839;LLMs&#20869;&#30340;&#20132;&#21449;&#20559;&#35265;&#26469;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#20027;&#35201;&#38598;&#20013;&#22312;&#25581;&#31034;&#24615;&#21035;&#35748;&#21516;&#21644;&#22269;&#31821;&#20559;&#35265;&#19978;&#65307;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25193;&#23637;&#21040;&#20219;&#20309;&#20154;&#21475;&#32479;&#35745;&#36523;&#20221;&#30340;&#20132;&#21449;&#20559;&#35265;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#27169;&#22411;&#20013;&#21457;&#29616;&#20102;&#26126;&#26174;&#30340;&#20559;&#35265;&#65292;&#20363;&#22914;&#20004;&#20010;&#27169;&#22411;&#19968;&#30452;&#24314;&#35758;&#22696;&#35199;&#21733;&#24037;&#20154;&#20174;&#20107;&#20302;&#34218;&#24037;&#20316;&#65292;&#25110;&#32773;&#26356;&#20542;&#21521;&#20110;&#21521;&#22899;&#24615;&#25512;&#33616;&#31192;&#20070;&#32844;&#20301;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#27979;&#37327;&#21644;&#29702;&#35299;LLMs&#20013;&#30340;&#20559;&#35265;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have seen widespread deployment in various real-world applications. Understanding these biases is crucial to comprehend the potential downstream consequences when using LLMs to make decisions, particularly for historically disadvantaged groups. In this work, we propose a simple method for analyzing and comparing demographic bias in LLMs, through the lens of job recommendations. We demonstrate the effectiveness of our method by measuring intersectional biases within ChatGPT and LLaMA, two cutting-edge LLMs. Our experiments primarily focus on uncovering gender identity and nationality bias; however, our method can be extended to examine biases associated with any intersection of demographic identities. We identify distinct biases in both models toward various demographic identities, such as both models consistently suggesting low-paying jobs for Mexican workers or preferring to recommend secretarial roles to women. Our study highlights the importance of measu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#26723;&#26696;&#25968;&#23383;&#21270;&#36807;&#31243;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#26159;&#33258;&#21160;&#36716;&#24405;&#21644;&#26657;&#27491;&#25163;&#31295;&#65292;&#20197;&#21450;&#26631;&#20934;&#21270;&#25991;&#26412;&#12290;&#36890;&#36807;&#27979;&#35797;ChatGPT&#31995;&#32479;&#65292;&#22312;&#23545;&#20449;&#20989;&#36827;&#34892;&#25991;&#26412;&#26631;&#20934;&#21270;&#26102;&#21462;&#24471;&#20102;&#19968;&#23450;&#25928;&#26524;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25968;&#23383;&#21270;&#21644;&#20154;&#24037;&#26234;&#33021;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#26723;&#26696;&#21644;&#21382;&#21490;&#30740;&#31350;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.02044</link><description>&lt;p&gt;
&#26723;&#26696;&#21644;&#21382;&#21490;&#23398;&#39046;&#22495;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#65306;HTS&#21644;ChatGPT
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence in archival and historical scholarship workflow: HTS and ChatGPT. (arXiv:2308.02044v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#26723;&#26696;&#25968;&#23383;&#21270;&#36807;&#31243;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#26159;&#33258;&#21160;&#36716;&#24405;&#21644;&#26657;&#27491;&#25163;&#31295;&#65292;&#20197;&#21450;&#26631;&#20934;&#21270;&#25991;&#26412;&#12290;&#36890;&#36807;&#27979;&#35797;ChatGPT&#31995;&#32479;&#65292;&#22312;&#23545;&#20449;&#20989;&#36827;&#34892;&#25991;&#26412;&#26631;&#20934;&#21270;&#26102;&#21462;&#24471;&#20102;&#19968;&#23450;&#25928;&#26524;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25968;&#23383;&#21270;&#21644;&#20154;&#24037;&#26234;&#33021;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#26723;&#26696;&#21644;&#21382;&#21490;&#30740;&#31350;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#24037;&#26234;&#33021;&#23545;&#26723;&#26696;&#36951;&#20135;&#25968;&#23383;&#21270;&#36807;&#31243;&#30340;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#23545;&#25163;&#31295;&#30340;&#33258;&#21160;&#36716;&#24405;&#12289;&#26657;&#27491;&#21644;&#26631;&#20934;&#21270;&#30340;&#24433;&#21709;&#12290;&#23427;&#24378;&#35843;&#20102;&#25968;&#23383;&#21270;&#25512;&#21160;&#23398;&#32773;&#37325;&#26032;&#23450;&#20041;&#26723;&#26696;&#21644;&#21382;&#21490;&#39046;&#22495;&#65292;&#24182;&#36890;&#36807;&#25968;&#23383;&#21270;&#21644;&#22823;&#25968;&#25454;&#30340;&#25972;&#21512;&#25552;&#20379;&#27169;&#25311;&#28304;&#25991;&#20214;&#30340;&#20415;&#25463;&#24615;&#12290;&#30740;&#31350;&#32858;&#28966;&#20110;&#20004;&#20010;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#20998;&#21035;&#26159;Transkribus&#21644;ChatGPT&#65292;&#23427;&#20204;&#20351;&#24471;&#23545;&#25968;&#23383;&#21270;&#28304;&#25991;&#20214;&#30340;&#39640;&#25928;&#20998;&#26512;&#21644;&#36716;&#24405;&#25104;&#20026;&#21487;&#33021;&#12290;&#25991;&#31456;&#36824;&#20171;&#32461;&#20102;&#23545;ChatGPT&#30340;&#27979;&#35797;&#65292;&#35813;&#27979;&#35797;&#29992;&#20110;&#23545;&#20445;&#23384;&#22312;Biscari&#26723;&#26696;&#65288;&#21345;&#22612;&#23612;&#20122;&#65289;&#30340;&#20449;&#20989;&#37096;&#20998;&#20013;&#30340;366&#23553;&#20449;&#20214;&#36827;&#34892;&#25991;&#26412;&#26631;&#20934;&#21270;&#12290;&#23613;&#31649;&#20154;&#24037;&#26234;&#33021;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#23548;&#33268;&#19968;&#20123;&#19981;&#20934;&#30830;&#24615;&#65292;&#20294;&#32416;&#27491;&#21518;&#30340;&#25991;&#26412;&#20173;&#28982;&#36798;&#21040;&#20102;&#26399;&#26395;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25991;&#31456;&#24471;&#20986;&#32467;&#35770;&#65292;&#25968;&#23383;&#21270;&#21644;&#20154;&#24037;&#26234;&#33021;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#26723;&#26696;&#21644;&#21382;&#21490;&#30740;&#31350;&#65292;&#20801;&#35768;&#23545;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#21644;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article examines the impact of Artificial Intelligence on the archival heritage digitization processes, specifically regarding the manuscripts' automatic transcription, their correction, and normalization. It highlights how digitality has compelled scholars to redefine Archive and History field and has facilitated the accessibility of analogue sources through digitization and integration into big data. The study focuses on two AI systems, namely Transkribus and ChatGPT, which enable efficient analysis and transcription of digitized sources. The article presents a test of ChatGPT, which was utilized to normalize the text of 366 letters stored in the Correspondence section of the Biscari Archive (Catania). Although the AI exhibited some limitations that resulted in inaccuracies, the corrected texts met expectations. Overall, the article concludes that digitization and AI can significantly enhance archival and historical research by allowing the analysis of vast amounts of data and t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29702;&#35770;&#30340;&#27010;&#24565;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#22312;&#31038;&#20132;&#30417;&#21548;&#21644;&#38169;&#35823;&#20449;&#24687;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.02037</link><description>&lt;p&gt;
&#25552;&#20986;&#19968;&#20010;&#27010;&#24565;&#26694;&#26550;&#65306;&#31038;&#20132;&#23186;&#20307;&#30417;&#21548;&#20844;&#20849;&#20581;&#24247;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Proposing a conceptual framework: social media listening for public health behavior. (arXiv:2308.02037v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02037
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29702;&#35770;&#30340;&#27010;&#24565;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#22312;&#31038;&#20132;&#30417;&#21548;&#21644;&#38169;&#35823;&#20449;&#24687;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#20256;&#25773;&#21644;&#34892;&#20026;&#29702;&#35770;&#24050;&#32463;&#34987;&#37319;&#29992;&#26469;&#35299;&#20915;&#20581;&#24247;&#38169;&#35823;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#21508;&#31181;&#29702;&#35770;&#21644;&#27169;&#22411;&#34987;&#29992;&#20110;&#30740;&#31350;COVID-19&#22823;&#27969;&#34892;&#65292;&#20294;&#23578;&#32570;&#20047;&#19987;&#38376;&#38024;&#23545;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#36827;&#34892;&#31038;&#20132;&#30417;&#21548;&#25110;&#38169;&#35823;&#20449;&#24687;&#30740;&#31350;&#30340;&#26694;&#26550;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#20986;&#19968;&#20010;&#26032;&#39062;&#32780;&#22522;&#20110;&#29702;&#35770;&#30340;&#27010;&#24565;&#26694;&#26550;&#65292;&#29992;&#20110;&#38169;&#35823;&#20449;&#24687;&#30740;&#31350;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#20197;&#21516;&#34892;&#35780;&#23457;&#26399;&#21002;&#21457;&#34920;&#30340;COVID-19&#30456;&#20851;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#29702;&#35770;&#21644;&#27169;&#22411;&#12290;&#36825;&#20123;&#29702;&#35770;&#21644;&#27169;&#22411;&#28041;&#21450;&#20581;&#24247;&#34892;&#20026;&#65292;&#20256;&#25773;&#21644;&#38169;&#35823;&#20449;&#24687;&#12290;&#25105;&#20204;&#23545;&#36825;&#20123;&#29702;&#35770;&#21644;&#27169;&#22411;&#36827;&#34892;&#20102;&#20998;&#26512;&#21644;&#35780;&#36848;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#31034;&#33539;&#24615;&#30340;&#27010;&#24565;&#26694;&#26550;&#12290;&#25105;&#20204;&#23545;&#20581;&#24247;&#20449;&#24565;&#27169;&#22411;&#12289;&#35745;&#21010;&#34892;&#20026;&#29702;&#35770;/&#29702;&#24615;&#34892;&#20026;&#12289;&#34892;&#20026;&#24433;&#21709;&#30340;&#20256;&#25773;&#12289;&#36328;&#29702;&#35770;&#27169;&#22411;&#12289;&#21151;&#21033;&#24615;&#29702;&#35770;&#12289;&#31038;&#20250;&#35780;&#21028;&#29702;&#35770;&#12289;&#39118;&#38505;&#20449;&#24687;&#23547;&#27714;&#21644;&#22788;&#29702;&#27169;&#24335;&#36827;&#34892;&#20102;&#22238;&#39038;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing communications and behavioral theories have been adopted to address health misinformation. Although various theories and models have been used to investigate the COVID-19 pandemic, there is no framework specially designed for social listening or misinformation studies using social media data and natural language processing techniques. This study aimed to propose a novel yet theory-based conceptual framework for misinformation research. We collected theories and models used in COVID-19 related studies published in peer-reviewed journals. The theories and models ranged from health behaviors, communications, to misinformation. They are analyzed and critiqued for their components, followed by proposing a conceptual framework with a demonstration. We reviewed Health Belief Model, Theory of Planned Behavior/Reasoned Action, Communication for Behavioral Impact, Transtheoretical Model, Uses and Gratifications Theory, Social Judgment Theory, Risk Information Seeking and Processing Mode
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#30740;&#31350;Twitter&#19978;&#26410;&#26469;&#20027;&#20041;&#32773;&#23545;&#26410;&#26469;&#30340;&#39044;&#26399;&#65292;&#25506;&#31350;&#35821;&#35328;&#25552;&#31034;&#23545;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#30340;&#39044;&#27979;&#24615;&#24605;&#32500;&#30340;&#24433;&#21709;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27969;&#31243;&#21644;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2308.02035</link><description>&lt;p&gt;
Twitter&#25968;&#25454;&#21578;&#35785;&#25105;&#20204;&#20851;&#20110;&#26410;&#26469;&#30340;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
What Twitter Data Tell Us about the Future?. (arXiv:2308.02035v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02035
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#30740;&#31350;Twitter&#19978;&#26410;&#26469;&#20027;&#20041;&#32773;&#23545;&#26410;&#26469;&#30340;&#39044;&#26399;&#65292;&#25506;&#31350;&#35821;&#35328;&#25552;&#31034;&#23545;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#30340;&#39044;&#27979;&#24615;&#24605;&#32500;&#30340;&#24433;&#21709;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27969;&#31243;&#21644;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#26159;&#19968;&#31181;&#22522;&#26412;&#30340;&#20154;&#31867;&#35748;&#30693;&#33021;&#21147;&#65292;&#28041;&#21450;&#23545;&#26410;&#26469;&#30340;&#24605;&#32771;&#21644;&#29983;&#27963;&#12290;&#34429;&#28982;&#35821;&#35328;&#26631;&#35760;&#21453;&#26144;&#20102;&#39044;&#27979;&#24615;&#24605;&#32500;&#65292;&#20294;&#20174;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#35282;&#24230;&#26469;&#30740;&#31350;&#39044;&#27979;&#24615;&#24605;&#32500;&#30340;&#30740;&#31350;&#36824;&#26377;&#38480;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#35843;&#26597;Twitter&#19978;&#26410;&#26469;&#20027;&#20041;&#32773;&#23545;&#26410;&#26469;&#30340;&#39044;&#26399;&#65292;&#24182;&#25506;&#32034;&#35821;&#35328;&#25552;&#31034;&#23545;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#30340;&#39044;&#27979;&#24615;&#24605;&#32500;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;Twitter&#26410;&#26469;&#20027;&#20041;&#32773;&#39044;&#27979;&#21644;&#20998;&#20139;&#21738;&#20123;&#26410;&#26469;&#65292;&#20197;&#21450;&#22914;&#20309;&#20174;&#31038;&#20132;&#25968;&#25454;&#20013;&#24314;&#27169;&#36825;&#20123;&#39044;&#26399;&#26410;&#26469;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#20026;&#20102;&#35843;&#26597;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#20851;&#20110;&#39044;&#27979;&#30340;&#30456;&#20851;&#24037;&#20316;&#65292;&#35752;&#35770;&#20102;&#35821;&#35328;&#26631;&#35760;&#21644;&#30693;&#21517;&#20154;&#22763;&#23545;&#39044;&#27979;&#24615;&#24605;&#32500;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#26410;&#26469;&#20998;&#20026;&#8220;&#29616;&#22312;&#30340;&#26410;&#26469;&#8221;&#21644;&#8220;&#26410;&#26469;&#30340;&#29616;&#22312;&#8221; &#30340;&#20998;&#31867;&#31995;&#32479;&#12290;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#30001;&#26410;&#26469;&#24433;&#21709;&#32773;&#20844;&#24320;&#20998;&#20139;&#30340;&#36229;&#36807;100&#19975;&#26465;&#25512;&#25991;&#30340;&#32534;&#35793;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;SOTA&#27169;&#22411;&#24320;&#21457;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anticipation is a fundamental human cognitive ability that involves thinking about and living towards the future. While language markers reflect anticipatory thinking, research on anticipation from the perspective of natural language processing is limited. This study aims to investigate the futures projected by futurists on Twitter and explore the impact of language cues on anticipatory thinking among social media users. We address the research questions of what futures Twitter's futurists anticipate and share, and how these anticipated futures can be modeled from social data. To investigate this, we review related works on anticipation, discuss the influence of language markers and prestigious individuals on anticipatory thinking, and present a taxonomy system categorizing futures into "present futures" and "future present". This research presents a compiled dataset of over 1 million publicly shared tweets by future influencers and develops a scalable NLP pipeline using SOTA models. T
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#20110;&#25991;&#26723;&#32423;&#24773;&#24863;&#20998;&#26512;&#27169;&#22411;&#36827;&#34892;&#20102;&#32508;&#21512;&#35780;&#20272;&#65292;&#37325;&#28857;&#32771;&#34385;&#20102;&#36164;&#28304;&#25104;&#26412;&#21644;&#29615;&#22659;&#24847;&#35782;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#36164;&#28304;&#28040;&#32791;&#36739;&#20302;&#30340;&#37197;&#32622;&#19979;&#65292;&#20934;&#30830;&#24615;&#25439;&#22833;&#36739;&#23567;&#12290;&#36825;&#23545;&#20110;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#19979;&#30340;&#27169;&#22411;&#37096;&#32626;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2308.02022</link><description>&lt;p&gt;
&#39640;&#25928;&#24773;&#24863;&#20998;&#26512;&#65306;&#29305;&#24449;&#25552;&#21462;&#25216;&#26415;&#12289;&#38598;&#25104;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#36164;&#28304;&#21487;&#34892;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Efficient Sentiment Analysis: A Resource-Aware Evaluation of Feature Extraction Techniques, Ensembling, and Deep Learning Models. (arXiv:2308.02022v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02022
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#20110;&#25991;&#26723;&#32423;&#24773;&#24863;&#20998;&#26512;&#27169;&#22411;&#36827;&#34892;&#20102;&#32508;&#21512;&#35780;&#20272;&#65292;&#37325;&#28857;&#32771;&#34385;&#20102;&#36164;&#28304;&#25104;&#26412;&#21644;&#29615;&#22659;&#24847;&#35782;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#36164;&#28304;&#28040;&#32791;&#36739;&#20302;&#30340;&#37197;&#32622;&#19979;&#65292;&#20934;&#30830;&#24615;&#25439;&#22833;&#36739;&#23567;&#12290;&#36825;&#23545;&#20110;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#19979;&#30340;&#27169;&#22411;&#37096;&#32626;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36861;&#27714;&#26368;&#22823;&#21270;&#20934;&#30830;&#24615;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#26102;&#65292;&#24120;&#24120;&#24573;&#35270;&#20854;&#20182;&#37325;&#35201;&#30340;&#31995;&#32479;&#24615;&#33021;&#25351;&#26631;&#12290;&#20808;&#21069;&#30340;&#27169;&#22411;&#24456;&#23481;&#26131;&#34987;&#36951;&#24536;&#65292;&#23613;&#31649;&#23427;&#20204;&#21487;&#33021;&#22312;&#35745;&#31639;&#36164;&#28304;&#26377;&#38480;&#25110;&#30456;&#23545;&#26356;&#26114;&#36149;&#30340;&#35774;&#32622;&#20013;&#36866;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#25991;&#26723;&#32423;&#24773;&#24863;&#20998;&#26512;&#27169;&#22411;&#36827;&#34892;&#24191;&#27867;&#30340;&#27604;&#36739;&#35780;&#20272;&#65292;&#20027;&#35201;&#20851;&#27880;&#23545;&#20110;&#27169;&#22411;&#37096;&#32626;&#21487;&#34892;&#24615;&#21644;&#29615;&#22659;&#24847;&#35782;&#30340;&#36164;&#28304;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32771;&#34385;&#20102;&#19981;&#21516;&#30340;&#29305;&#24449;&#25552;&#21462;&#25216;&#26415;&#12289;&#38598;&#25104;&#25928;&#26524;&#12289;&#20219;&#21153;&#29305;&#23450;&#30340;&#28145;&#24230;&#23398;&#20064;&#24314;&#27169;&#20197;&#21450;&#39046;&#22495;&#26080;&#20851;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34429;&#28982;&#32463;&#36807;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36798;&#21040;&#20102;&#26368;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#26576;&#20123;&#26367;&#20195;&#37197;&#32622;&#22312;&#36164;&#28304;&#28040;&#32791;&#26041;&#38754;&#25552;&#20379;&#20102;&#24040;&#22823;&#30340;&#65288;&#26368;&#39640;&#36798;24,283*&#65289;&#33410;&#30465;&#65292;&#32780;&#20934;&#30830;&#24615;&#25439;&#22833;&#21482;&#26377;&#36739;&#23567;&#30340;(&lt;1%)&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#23545;&#20110;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#65292;&#20934;&#30830;&#24615;&#30340;&#24046;&#24322;&#20250;&#32553;&#23567;&#65292;&#32780;&#36164;&#28304;&#28040;&#32791;&#30340;&#24046;&#24322;&#20250;&#22686;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;
While reaching for NLP systems that maximize accuracy, other important metrics of system performance are often overlooked. Prior models are easily forgotten despite their possible suitability in settings where large computing resources are unavailable or relatively more costly. In this paper, we perform a broad comparative evaluation of document-level sentiment analysis models with a focus on resource costs that are important for the feasibility of model deployment and general climate consciousness. Our experiments consider different feature extraction techniques, the effect of ensembling, task-specific deep learning modeling, and domain-independent large language models (LLMs). We find that while a fine-tuned LLM achieves the best accuracy, some alternate configurations provide huge (up to 24, 283 *) resource savings for a marginal (&lt;1%) loss in accuracy. Furthermore, we find that for smaller datasets, the differences in accuracy shrink while the difference in resource consumption gro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22312;&#23567;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#25945;&#24072;&#20013;&#36827;&#34892;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#24403;&#25945;&#24072;&#27169;&#22411;&#22312;&#36275;&#22815;&#23567;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26102;&#65292;&#33976;&#39311;&#21487;&#20197;&#20445;&#25345;&#29978;&#33267;&#36229;&#36807;&#25945;&#24072;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.02019</link><description>&lt;p&gt;
Baby Llama&#65306;&#20174;&#19968;&#32452;&#22312;&#23567;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#25945;&#24072;&#20013;&#36827;&#34892;&#30693;&#35782;&#33976;&#39311;&#65292;&#26080;&#24615;&#33021;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Baby Llama: knowledge distillation from an ensemble of teachers trained on a small dataset with no performance penalty. (arXiv:2308.02019v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22312;&#23567;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#25945;&#24072;&#20013;&#36827;&#34892;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#24403;&#25945;&#24072;&#27169;&#22411;&#22312;&#36275;&#22815;&#23567;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26102;&#65292;&#33976;&#39311;&#21487;&#20197;&#20445;&#25345;&#29978;&#33267;&#36229;&#36807;&#25945;&#24072;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#25105;&#20204;&#23545;BabyLM&#25361;&#25112;[arXiv:2301.11796]&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#30446;&#26631;&#26159;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#25105;&#20204;&#22312;&#20197;&#21457;&#23637;&#24615;&#20026;&#22522;&#30784;&#30340;10M&#35789;&#35821;&#30340;BabyLM&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#19968;&#20010;&#30001;GPT-2&#21644;&#23567;&#22411;LLaMA&#27169;&#22411;&#32452;&#25104;&#30340;&#38598;&#21512;&#65292;&#28982;&#21518;&#23558;&#20854;&#33976;&#39311;&#20026;&#19968;&#20010;&#23567;&#22411;&#30340;58M&#21442;&#25968;LLaMA&#27169;&#22411;&#65292;&#20854;&#24615;&#33021;&#36229;&#36807;&#20102;&#20004;&#20010;&#25945;&#24072;&#27169;&#22411;&#20197;&#21450;&#19968;&#20010;&#27809;&#26377;&#36827;&#34892;&#33976;&#39311;&#35757;&#32451;&#30340;&#31867;&#20284;&#27169;&#22411;&#12290;&#36825;&#34920;&#26126;&#65292;&#24403;&#25945;&#24072;&#27169;&#22411;&#22312;&#36275;&#22815;&#23567;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26102;&#65292;&#33976;&#39311;&#19981;&#20165;&#21487;&#20197;&#20445;&#25345;&#25945;&#24072;&#27169;&#22411;&#30340;&#20840;&#37096;&#24615;&#33021;&#65292;&#36824;&#21487;&#20197;&#36229;&#36807;&#23427;&#65292;&#24182;&#23548;&#33268;&#27604;&#30452;&#25509;&#35757;&#32451;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present our proposed solution to the BabyLM challenge [arXiv:2301.11796], whose goal was to improve the sample efficiency of language models. We trained an ensemble consisting of a GPT-2 and small LLaMA models on the developmentally-plausible, 10M-word BabyLM dataset, then distilled it into a small, 58M-parameter LLaMA model, which exceeds in performance both of its teachers as well as a similar model trained without distillation. This suggests that distillation can not only retain the full performance of the teacher model when the latter is trained on a sufficiently small dataset; it can exceed it, and lead to significantly better performance than direct training.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#32467;&#21512;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;&#38754;&#21521;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#32852;&#37030;&#34920;&#31034;&#23398;&#20064;&#65292;&#23558;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#38544;&#31169;&#25968;&#25454;&#29992;&#20110;&#23398;&#20064;&#20581;&#22766;&#30340;&#38899;&#39057;&#34920;&#31034;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2308.02013</link><description>&lt;p&gt;
&#38754;&#21521;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#32852;&#37030;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Representation Learning for Automatic Speech Recognition. (arXiv:2308.02013v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02013
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#32467;&#21512;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;&#38754;&#21521;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#32852;&#37030;&#34920;&#31034;&#23398;&#20064;&#65292;&#23558;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#38544;&#31169;&#25968;&#25454;&#29992;&#20110;&#23398;&#20064;&#20581;&#22766;&#30340;&#38899;&#39057;&#34920;&#31034;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#20445;&#25252;&#38544;&#31169;&#30340;&#27169;&#24335;&#65292;&#20801;&#35768;&#36793;&#32536;&#35774;&#22791;&#22312;&#19981;&#20849;&#20139;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#21327;&#20316;&#23398;&#20064;&#12290;&#20687;Alexa&#21644;Siri&#36825;&#26679;&#30340;&#36793;&#32536;&#35774;&#22791;&#26159;&#28508;&#22312;&#30340;&#38750;&#26631;&#35760;&#38899;&#39057;&#25968;&#25454;&#26469;&#28304;&#65292;&#21487;&#20197;&#29992;&#26469;&#23398;&#20064;&#20581;&#22766;&#30340;&#38899;&#39057;&#34920;&#31034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#21644;FL&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#36981;&#23432;&#25968;&#25454;&#38544;&#31169;&#32422;&#26463;&#26465;&#20214;&#65292;&#23398;&#20064;&#29992;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#20351;&#29992;&#26410;&#26631;&#35760;&#30340;&#35821;&#38899;&#25968;&#25454;&#38598;Libri-Light&#20013;&#30340;&#35828;&#35805;&#32773;&#21644;&#31456;&#33410;&#20449;&#24687;&#65292;&#27169;&#25311;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#35828;&#35805;&#32773;&#38548;&#31163;&#25968;&#25454;&#20998;&#24067;&#65292;&#24182;&#20351;&#29992;FedSGD&#22312;&#23545;&#27604;&#39044;&#27979;&#32534;&#30721;&#26694;&#26550;&#19979;&#36827;&#34892;LSTM&#32534;&#30721;&#22120;&#30340;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;FL&#20013;&#39044;&#35757;&#32451;&#30340;ASR&#32534;&#30721;&#22120;&#30340;&#24615;&#33021;&#19982;&#20013;&#24515;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#24403;&#65292;&#24182;&#19988;&#30456;&#27604;&#27809;&#26377;&#39044;&#35757;&#32451;&#65292;&#26377;12-15%&#65288;WER&#65289;&#30340;&#25913;&#21892;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#32852;&#37030;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#21040;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#65292;&#27861;&#35821;&#65292;&#24182;&#19988;&#30456;&#27604;&#27809;&#26377;&#39044;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;20%&#65288;WER&#65289;&#30340;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a privacy-preserving paradigm, allowing edge devices to learn collaboratively without sharing data. Edge devices like Alexa and Siri are prospective sources of unlabeled audio data that can be tapped to learn robust audio representations. In this work, we bring Self-supervised Learning (SSL) and FL together to learn representations for Automatic Speech Recognition respecting data privacy constraints. We use the speaker and chapter information in the unlabeled speech dataset, Libri-Light, to simulate non-IID speaker-siloed data distributions and pre-train an LSTM encoder with the Contrastive Predictive Coding framework with FedSGD. We show that the pre-trained ASR encoder in FL performs as well as a centrally pre-trained model and produces an improvement of 12-15% (WER) compared to no pre-training. We further adapt the federated pre-trained models to a new language, French, and show a 20% (WER) improvement over no pre-training.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#23391;&#21152;&#25289;&#20551;&#35780;&#35770;&#26816;&#27979;&#30340;&#31532;&#19968;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#36716;&#25442;&#27969;&#27700;&#32447;&#65292;&#20197;&#35782;&#21035;&#23391;&#21152;&#25289;&#20551;&#35780;&#35770;&#65292;&#24182;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2308.01987</link><description>&lt;p&gt;
&#23391;&#21152;&#25289;&#35821;&#20551;&#35780;&#35770;&#65306;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#26816;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Bengali Fake Reviews: A Benchmark Dataset and Detection System. (arXiv:2308.01987v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01987
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#23391;&#21152;&#25289;&#20551;&#35780;&#35770;&#26816;&#27979;&#30340;&#31532;&#19968;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#36716;&#25442;&#27969;&#27700;&#32447;&#65292;&#20197;&#35782;&#21035;&#23391;&#21152;&#25289;&#20551;&#35780;&#35770;&#65292;&#24182;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#22312;&#32447;&#24179;&#21488;&#19978;&#20986;&#29616;&#22823;&#37327;&#30340;&#20551;&#35780;&#35770;&#24050;&#32463;&#25104;&#20026;&#28040;&#36153;&#32773;&#21644;&#20225;&#19994;&#30340;&#37325;&#22823;&#20851;&#20999;&#12290;&#36825;&#26679;&#30340;&#35780;&#35770;&#21487;&#20197;&#27450;&#39575;&#28040;&#36153;&#32773;&#65292;&#24182;&#23545;&#20135;&#21697;&#25110;&#26381;&#21153;&#30340;&#22768;&#35465;&#36896;&#25104;&#25439;&#23475;&#65292;&#22240;&#27492;&#35782;&#21035;&#23427;&#20204;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#22312;&#33521;&#35821;&#35821;&#35328;&#20013;&#24050;&#32463;&#24191;&#27867;&#30740;&#31350;&#20102;&#20551;&#35780;&#35770;&#30340;&#26816;&#27979;&#65292;&#20294;&#22312;&#23391;&#21152;&#25289;&#35821;&#31561;&#38750;&#33521;&#35821;&#35821;&#35328;&#20013;&#26816;&#27979;&#20551;&#35780;&#35770;&#20173;&#28982;&#26159;&#19968;&#20010;&#30456;&#23545;&#26410;&#24320;&#21457;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#23391;&#21152;&#25289;&#20551;&#35780;&#35770;&#26816;&#27979;&#65288;BFRD&#65289;&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#29992;&#20110;&#35782;&#21035;&#23391;&#21152;&#25289;&#20551;&#35780;&#35770;&#30340;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#30001;&#20174;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#20013;&#25910;&#38598;&#21040;&#30340;7710&#26465;&#38750;&#20551;&#21644;1339&#26465;&#20551;&#30340;&#19982;&#39135;&#21697;&#30456;&#20851;&#30340;&#35780;&#35770;&#32452;&#25104;&#12290;&#20026;&#20102;&#23558;&#35780;&#35770;&#20013;&#30340;&#38750;&#23391;&#21152;&#25289;&#35789;&#35821;&#36716;&#25442;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#27969;&#27700;&#32447;&#65292;&#23558;&#33521;&#35821;&#21333;&#35789;&#36716;&#25442;&#20026;&#20854;&#23545;&#24212;&#30340;&#23391;&#21152;&#25289;&#24847;&#20041;&#65292;&#21516;&#26102;&#20063;&#23558;&#32599;&#39532;&#21270;&#30340;&#23391;&#21152;&#25289;&#35821;&#22238;&#38899;&#21040;&#23391;&#21152;&#25289;&#35821;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#20010;&#28145;&#24230;&#23398;&#20064;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of fake reviews on various online platforms has created a major concern for both consumers and businesses. Such reviews can deceive customers and cause damage to the reputation of products or services, making it crucial to identify them. Although the detection of fake reviews has been extensively studied in English language, detecting fake reviews in non-English languages such as Bengali is still a relatively unexplored research area. This paper introduces the Bengali Fake Review Detection (BFRD) dataset, the first publicly available dataset for identifying fake reviews in Bengali. The dataset consists of 7710 non-fake and 1339 fake food-related reviews collected from social media posts. To convert non-Bengali words in a review, a unique pipeline has been proposed that translates English words to their corresponding Bengali meaning and also back transliterates Romanized Bengali to Bengali. We have conducted rigorous experimentation using multiple deep learning and pre
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#22312;&#32447;&#24066;&#22330;&#20013;&#25340;&#20889;&#38169;&#35823;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#21644;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20102;&#39046;&#22495;&#29305;&#23450;&#30340;&#25340;&#20889;&#26816;&#26597;&#22120;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#22312;&#24494;&#36719;AppSource&#24066;&#22330;&#30340;&#23454;&#26102;&#25512;&#26029;API&#20013;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#34920;&#26126;&#65292;&#25511;&#21046;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#25968;&#25454;&#21487;&#33021;&#25104;&#20026;&#19968;&#20010;&#26377;&#21147;&#30340;&#24037;&#20855;&#65292;&#29305;&#21035;&#26159;&#32771;&#34385;&#21040;&#24403;&#21069;&#22823;&#35821;&#35328;&#27169;&#22411;&#25152;&#20381;&#36182;&#30340;&#24040;&#22823;&#19988;&#38590;&#20197;&#25511;&#21046;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2308.01976</link><description>&lt;p&gt;
&#25340;&#20889;&#26816;&#26597;&#22120;&#22312;&#22312;&#32447;&#24066;&#22330;&#20013;&#30340;&#39046;&#22495;&#29305;&#24322;&#24615;&#21644;&#25968;&#25454;&#25928;&#29575;&#65306;&#20197;&#22312;&#32447;&#24066;&#22330;&#25628;&#32034;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Domain specificity and data efficiency in typo tolerant spell checkers: the case of search in online marketplaces. (arXiv:2308.01976v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#22312;&#32447;&#24066;&#22330;&#20013;&#25340;&#20889;&#38169;&#35823;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#21644;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20102;&#39046;&#22495;&#29305;&#23450;&#30340;&#25340;&#20889;&#26816;&#26597;&#22120;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#22312;&#24494;&#36719;AppSource&#24066;&#22330;&#30340;&#23454;&#26102;&#25512;&#26029;API&#20013;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#34920;&#26126;&#65292;&#25511;&#21046;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#25968;&#25454;&#21487;&#33021;&#25104;&#20026;&#19968;&#20010;&#26377;&#21147;&#30340;&#24037;&#20855;&#65292;&#29305;&#21035;&#26159;&#32771;&#34385;&#21040;&#24403;&#21069;&#22823;&#35821;&#35328;&#27169;&#22411;&#25152;&#20381;&#36182;&#30340;&#24040;&#22823;&#19988;&#38590;&#20197;&#25511;&#21046;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22312;&#32447;&#24066;&#22330;&#30340;&#39046;&#22495;&#29305;&#23450;&#24615;&#21644;&#29992;&#25143;&#30701;&#26597;&#35810;&#30340;&#29305;&#28857;&#65292;&#38169;&#23383;&#26159;&#22312;&#32447;&#24066;&#22330;&#35775;&#38382;&#32773;&#30340;&#20027;&#35201;&#22256;&#25200;&#12290;&#20256;&#32479;&#30340;&#25340;&#20889;&#26816;&#26597;&#35299;&#20915;&#26041;&#26696;&#22312;&#32416;&#27491;&#25340;&#20889;&#38169;&#35823;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26469;&#35299;&#20915;&#32570;&#20047;&#26631;&#27880;&#25340;&#20889;&#38169;&#35823;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20102;&#19978;&#19979;&#25991;&#38480;&#21046;&#30340;&#39046;&#22495;&#29305;&#23450;&#23884;&#20837;&#12290;&#36825;&#20123;&#23884;&#20837;&#34987;&#37096;&#32626;&#22312;&#24494;&#36719;AppSource&#24066;&#22330;&#30340;&#23454;&#26102;&#25512;&#26029;API&#20013;&#65292;&#20197;&#22312;&#38169;&#35823;&#25340;&#20889;&#30340;&#29992;&#25143;&#26597;&#35810;&#21644;&#21487;&#29992;&#20135;&#21697;&#21517;&#31216;&#20043;&#38388;&#25214;&#21040;&#26368;&#25509;&#36817;&#30340;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#34920;&#26126;&#65292;&#21463;&#21040;&#24403;&#21069;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#25511;&#21046;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#25968;&#25454;&#21487;&#33021;&#25104;&#20026;&#19968;&#20010;&#26377;&#21147;&#30340;&#24037;&#20855;&#65292;&#32780;&#36825;&#20123;&#27169;&#22411;&#20381;&#36182;&#20110;&#24040;&#22823;&#19988;&#38590;&#20197;&#25511;&#21046;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Typographical errors are a major source of frustration for visitors of online marketplaces. Because of the domain-specific nature of these marketplaces and the very short queries users tend to search for, traditional spell cheking solutions do not perform well in correcting typos. We present a data augmentation method to address the lack of annotated typo data and train a recurrent neural network to learn context-limited domain-specific embeddings. Those embeddings are deployed in a real-time inferencing API for the Microsoft AppSource marketplace to find the closest match between a misspelled user query and the available product names. Our data efficient solution shows that controlled high quality synthetic data may be a powerful tool especially considering the current climate of large language models which rely on prohibitively huge and often uncontrolled datasets.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#25193;&#24352;&#21367;&#31215;&#36716;&#25442;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;MULTIMEDIATE 2023&#31454;&#36187;&#20013;&#24314;&#27169;&#21644;&#20272;&#35745;&#22810;&#27169;&#24577;&#23545;&#35805;&#20013;&#30340;&#20154;&#31867;&#21442;&#19982;&#24230;&#12290;&#22312;&#27979;&#35797;&#38598;&#19978;&#35813;&#31995;&#32479;&#27604;&#22522;&#20934;&#27169;&#22411;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;7%&#25913;&#36827;&#65292;&#22312;&#39564;&#35777;&#38598;&#19978;&#34920;&#29616;&#20026;4%&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#31616;&#21333;&#30340;&#20018;&#32852;&#26041;&#27861;&#19982;&#33258;&#27880;&#24847;&#21147;&#34701;&#21512;&#21487;&#20197;&#33719;&#24471;&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.01966</link><description>&lt;p&gt;
DCTM&#65306;&#25193;&#24352;&#21367;&#31215;&#36716;&#25442;&#27169;&#22411;&#29992;&#20110;&#22810;&#27169;&#24577;&#23545;&#35805;&#20013;&#30340;&#21442;&#19982;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
DCTM: Dilated Convolutional Transformer Model for Multimodal Engagement Estimation in Conversation. (arXiv:2308.01966v1 [cs.MM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01966
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#25193;&#24352;&#21367;&#31215;&#36716;&#25442;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;MULTIMEDIATE 2023&#31454;&#36187;&#20013;&#24314;&#27169;&#21644;&#20272;&#35745;&#22810;&#27169;&#24577;&#23545;&#35805;&#20013;&#30340;&#20154;&#31867;&#21442;&#19982;&#24230;&#12290;&#22312;&#27979;&#35797;&#38598;&#19978;&#35813;&#31995;&#32479;&#27604;&#22522;&#20934;&#27169;&#22411;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;7%&#25913;&#36827;&#65292;&#22312;&#39564;&#35777;&#38598;&#19978;&#34920;&#29616;&#20026;4%&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#31616;&#21333;&#30340;&#20018;&#32852;&#26041;&#27861;&#19982;&#33258;&#27880;&#24847;&#21147;&#34701;&#21512;&#21487;&#20197;&#33719;&#24471;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#21442;&#19982;&#24230;&#20272;&#35745;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#20010;&#22238;&#24402;&#38382;&#39064;&#65292;&#38656;&#35201;&#35782;&#21035;&#21442;&#19982;&#32773;&#22312;&#23545;&#35805;&#20013;&#30340;&#20851;&#27880;&#21644;&#21442;&#19982;&#31243;&#24230;&#12290;&#36825;&#19968;&#20219;&#21153;&#23545;&#20110;&#25581;&#31034;&#20154;&#31867;&#20132;&#20114;&#21160;&#21147;&#23398;&#21644;&#34892;&#20026;&#27169;&#24335;&#22312;&#23545;&#35805;&#20013;&#30340;&#27934;&#23519;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#25193;&#24352;&#21367;&#31215;&#36716;&#25442;&#27169;&#22411;&#65292;&#29992;&#20110;&#24314;&#27169;&#21644;&#20272;&#35745;MULTIMEDIATE 2023&#31454;&#36187;&#20013;&#30340;&#20154;&#31867;&#21442;&#19982;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31995;&#32479;&#36229;&#36807;&#20102;&#22522;&#20934;&#27169;&#22411;&#65292;&#22312;&#27979;&#35797;&#38598;&#19978;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;7%&#25913;&#36827;&#21644;&#39564;&#35777;&#38598;&#19978;&#30340;4%&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19981;&#21516;&#30340;&#27169;&#24577;&#34701;&#21512;&#26426;&#21046;&#65292;&#24182;&#23637;&#31034;&#23545;&#20110;&#36825;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#65292;&#31616;&#21333;&#30340;&#20018;&#32852;&#26041;&#27861;&#19982;&#33258;&#27880;&#24847;&#21147;&#34701;&#21512;&#33719;&#24471;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational engagement estimation is posed as a regression problem, entailing the identification of the favorable attention and involvement of the participants in the conversation. This task arises as a crucial pursuit to gain insights into human's interaction dynamics and behavior patterns within a conversation. In this research, we introduce a dilated convolutional Transformer for modeling and estimating human engagement in the MULTIMEDIATE 2023 competition. Our proposed system surpasses the baseline models, exhibiting a noteworthy $7$\% improvement on test set and $4$\% on validation set. Moreover, we employ different modality fusion mechanism and show that for this type of data, a simple concatenated method with self-attention fusion gains the best performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#22312;&#22788;&#29702;&#36880;&#28176;&#22797;&#26434;&#30340;&#31867;&#27604;&#25512;&#29702;&#26102;&#30340;&#24517;&#35201;&#24615;&#65292;&#20197;&#25552;&#20379;&#36229;&#36234;&#25991;&#23383;&#20869;&#23481;&#30340;&#24191;&#27867;&#12289;&#22810;&#26679;&#21270;&#30340;&#30693;&#35782;&#65292;&#24182;&#32467;&#21512;&#32479;&#35745;&#21644;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26469;&#22686;&#24378;&#21644;&#24341;&#23548;&#26144;&#23556;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2308.01936</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#25105;&#20204;&#38656;&#35201;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#26469;&#24314;&#27169;&#23454;&#29992;&#30340;&#31867;&#27604;?
&lt;/p&gt;
&lt;p&gt;
Why Do We Need Neuro-symbolic AI to Model Pragmatic Analogies?. (arXiv:2308.01936v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01936
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#22312;&#22788;&#29702;&#36880;&#28176;&#22797;&#26434;&#30340;&#31867;&#27604;&#25512;&#29702;&#26102;&#30340;&#24517;&#35201;&#24615;&#65292;&#20197;&#25552;&#20379;&#36229;&#36234;&#25991;&#23383;&#20869;&#23481;&#30340;&#24191;&#27867;&#12289;&#22810;&#26679;&#21270;&#30340;&#30693;&#35782;&#65292;&#24182;&#32467;&#21512;&#32479;&#35745;&#21644;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26469;&#22686;&#24378;&#21644;&#24341;&#23548;&#26144;&#23556;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#30340;&#19968;&#20010;&#29305;&#28857;&#26159;&#33021;&#22815;&#21033;&#29992;&#29087;&#24713;&#30340;&#39046;&#22495;&#23545;&#19981;&#37027;&#20040;&#29087;&#24713;&#30340;&#39046;&#22495;&#36827;&#34892;&#25512;&#29702;&#65292;&#21363;&#31867;&#27604;&#25512;&#29702;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22788;&#29702;&#22312;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#34920;&#36798;&#30340;&#36880;&#28176;&#22797;&#26434;&#30340;&#31867;&#27604;&#26102;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22235;&#20010;&#19981;&#21516;&#22797;&#26434;&#32423;&#21035;&#30340;&#31867;&#27604;&#65306;&#35789;&#27719;&#31867;&#27604;&#12289;&#21477;&#27861;&#31867;&#27604;&#12289;&#35821;&#20041;&#31867;&#27604;&#21644;&#23454;&#29992;&#31867;&#27604;&#12290;&#38543;&#30528;&#31867;&#27604;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#23427;&#20204;&#38656;&#35201;&#36229;&#20986;&#25991;&#26412;&#20869;&#23481;&#30340;&#24191;&#27867;&#12289;&#22810;&#26679;&#21270;&#30340;&#30693;&#35782;&#65292;&#36825;&#22312;&#25903;&#25345;LLMs&#30340;&#35789;&#27719;&#20849;&#29616;&#32479;&#35745;&#20013;&#19981;&#22826;&#21487;&#33021;&#25214;&#21040;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#37319;&#29992;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#24517;&#35201;&#24615;&#65292;&#36825;&#20123;&#25216;&#26415;&#32467;&#21512;&#20102;&#32479;&#35745;&#21644;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#65292;&#26681;&#25454;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#25552;&#20379;&#20449;&#24687;&#20197;&#31361;&#20986;&#21644;&#22686;&#24378;&#30456;&#20851;&#20869;&#23481;&#65292;&#25552;&#20379;&#25277;&#35937;&#21644;&#24341;&#23548;&#26144;&#23556;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#30693;&#35782;&#39537;&#21160;&#26041;&#27861;&#22312;&#20445;&#25345;LLMs&#30340;&#25928;&#29575;&#30340;&#21516;&#26102;&#20445;&#25345;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
A hallmark of intelligence is the ability to use a familiar domain to make inferences about a less familiar domain, known as analogical reasoning. In this article, we delve into the performance of Large Language Models (LLMs) in dealing with progressively complex analogies expressed in unstructured text. We discuss analogies at four distinct levels of complexity: lexical analogies, syntactic analogies, semantic analogies, and pragmatic analogies. As the analogies become more complex, they require increasingly extensive, diverse knowledge beyond the textual content, unlikely to be found in the lexical co-occurrence statistics that power LLMs. To address this, we discuss the necessity of employing Neuro-symbolic AI techniques that combine statistical and symbolic AI, informing the representation of unstructured text to highlight and augment relevant content, provide abstraction and guide the mapping process. Our knowledge-informed approach maintains the efficiency of LLMs while preservin
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;MultiEM&#30340;&#39640;&#25928;&#26377;&#25928;&#26080;&#30417;&#30563;&#22810;&#34920;&#23454;&#20307;&#21305;&#37197;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#22686;&#24378;&#23454;&#20307;&#34920;&#31034;&#12289;&#34920;&#32423;&#23618;&#27425;&#21512;&#24182;&#21644;&#22522;&#20110;&#23494;&#24230;&#30340;&#21098;&#26525;&#65292;&#24182;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.01927</link><description>&lt;p&gt;
MultiEM: &#39640;&#25928;&#26377;&#25928;&#30340;&#26080;&#30417;&#30563;&#22810;&#34920;&#23454;&#20307;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
MultiEM: Efficient and Effective Unsupervised Multi-Table Entity Matching. (arXiv:2308.01927v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01927
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;MultiEM&#30340;&#39640;&#25928;&#26377;&#25928;&#26080;&#30417;&#30563;&#22810;&#34920;&#23454;&#20307;&#21305;&#37197;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#22686;&#24378;&#23454;&#20307;&#34920;&#31034;&#12289;&#34920;&#32423;&#23618;&#27425;&#21512;&#24182;&#21644;&#22522;&#20110;&#23494;&#24230;&#30340;&#21098;&#26525;&#65292;&#24182;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#21305;&#37197;&#65288;EM&#65289;&#26088;&#22312;&#20174;&#20851;&#31995;&#34920;&#20013;&#35782;&#21035;&#20986;&#25351;&#21521;&#21516;&#19968;&#29616;&#23454;&#19990;&#30028;&#23454;&#20307;&#30340;&#25152;&#26377;&#23454;&#20307;&#23545;&#65292;&#23427;&#26159;&#23454;&#38469;&#25968;&#25454;&#31649;&#29702;&#31995;&#32479;&#20013;&#26368;&#37325;&#35201;&#30340;&#20219;&#21153;&#20043;&#19968;&#12290;&#30001;&#20110;EM&#30340;&#26631;&#35760;&#36807;&#31243;&#38750;&#24120;&#36153;&#26102;&#36153;&#21147;&#65292;&#30456;&#36739;&#20110;&#30417;&#30563;&#24335;EM&#65292;&#26080;&#30417;&#30563;&#24335;EM&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#26356;&#20855;&#36866;&#29992;&#24615;&#12290;&#20256;&#32479;&#30340;&#26080;&#30417;&#30563;&#24335;EM&#20551;&#35774;&#25152;&#26377;&#23454;&#20307;&#26469;&#33258;&#20004;&#20010;&#34920;&#65292;&#28982;&#32780;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#22810;&#20010;&#34920;&#20043;&#38388;&#30340;&#23454;&#20307;&#21305;&#37197;&#65288;&#22810;&#34920;EM&#65289;&#26356;&#20026;&#24120;&#35265;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#39640;&#25928;&#26377;&#25928;&#30340;&#26080;&#30417;&#30563;&#22810;&#34920;EM&#30340;&#30740;&#31350;&#23578;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#35770;&#25991;&#23545;&#26080;&#30417;&#30563;&#22810;&#34920;&#23454;&#20307;&#21305;&#37197;&#38382;&#39064;&#36827;&#34892;&#20102;&#27491;&#24335;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#31216;&#20026;MultiEM&#12290;MultiEM&#26159;&#19968;&#20010;&#21487;&#24182;&#34892;&#30340;&#27969;&#27700;&#32447;&#65292;&#21253;&#25324;&#22686;&#24378;&#30340;&#23454;&#20307;&#34920;&#31034;&#12289;&#22522;&#20110;&#34920;&#30340;&#23618;&#27425;&#21512;&#24182;&#21644;&#22522;&#20110;&#23494;&#24230;&#30340;&#21098;&#26525;&#12290;&#22312;&#20845;&#20010;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;MultiEM&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity Matching (EM), which aims to identify all entity pairs referring to the same real-world entity from relational tables, is one of the most important tasks in real-world data management systems. Due to the labeling process of EM being extremely labor-intensive, unsupervised EM is more applicable than supervised EM in practical scenarios. Traditional unsupervised EM assumes that all entities come from two tables; however, it is more common to match entities from multiple tables in practical applications, that is, multi-table entity matching (multi-table EM). Unfortunately, effective and efficient unsupervised multi-table EM remains under-explored. To fill this gap, this paper formally studies the problem of unsupervised multi-table entity matching and proposes an effective and efficient solution, termed as MultiEM. MultiEM is a parallelable pipeline of enhanced entity representation, table-wise hierarchical merging, and density-based pruning. Extensive experimental results on six r
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#19968;&#27454;&#21517;&#20026;"Hoodwinked"&#30340;&#25991;&#26412;&#28216;&#25103;&#65292;&#30740;&#31350;&#20102;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#27450;&#39575;&#21644;&#35782;&#21035;&#35854;&#35328;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#26432;&#25163;&#32463;&#24120;&#21542;&#35748;&#32618;&#34892;&#24182;&#25351;&#36131;&#20182;&#20154;&#65292;&#23548;&#33268;&#25237;&#31080;&#32467;&#26524;&#21463;&#21040;&#24433;&#21709;&#12290;&#26356;&#20808;&#36827;&#30340;&#27169;&#22411;&#22312;&#26432;&#25163;&#25928;&#26524;&#19978;&#34920;&#29616;&#20986;&#20248;&#21183;&#12290;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#36825;&#31181;&#25913;&#36827;&#26159;&#36890;&#36807;&#22312;&#35752;&#35770;&#20013;&#26356;&#24378;&#30340;&#27450;&#39575;&#33021;&#21147;&#23454;&#29616;&#30340;&#12290;</title><link>http://arxiv.org/abs/2308.01404</link><description>&lt;p&gt;
&#33945;&#39575;&#65306;&#22522;&#20110;&#25991;&#26412;&#30340;&#28216;&#25103;&#20013;&#30340;&#27450;&#39575;&#19982;&#21512;&#20316;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Hoodwinked: Deception and Cooperation in a Text-Based Game for Language Models. (arXiv:2308.01404v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01404
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#19968;&#27454;&#21517;&#20026;"Hoodwinked"&#30340;&#25991;&#26412;&#28216;&#25103;&#65292;&#30740;&#31350;&#20102;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#27450;&#39575;&#21644;&#35782;&#21035;&#35854;&#35328;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#26432;&#25163;&#32463;&#24120;&#21542;&#35748;&#32618;&#34892;&#24182;&#25351;&#36131;&#20182;&#20154;&#65292;&#23548;&#33268;&#25237;&#31080;&#32467;&#26524;&#21463;&#21040;&#24433;&#21709;&#12290;&#26356;&#20808;&#36827;&#30340;&#27169;&#22411;&#22312;&#26432;&#25163;&#25928;&#26524;&#19978;&#34920;&#29616;&#20986;&#20248;&#21183;&#12290;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#36825;&#31181;&#25913;&#36827;&#26159;&#36890;&#36807;&#22312;&#35752;&#35770;&#20013;&#26356;&#24378;&#30340;&#27450;&#39575;&#33021;&#21147;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#27450;&#39575;&#21644;&#35782;&#21035;&#35854;&#35328;&#30340;&#33021;&#21147;&#65311;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#27454;&#21517;&#20026;&#8220;Hoodwinked&#8221;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#28216;&#25103;&#65292;&#21463;&#21040;&#8220;&#40657;&#24110;&#8221;&#21644;&#8220;&#35841;&#26159;&#21351;&#24213;&#8221;&#28216;&#25103;&#30340;&#21551;&#21457;&#65292;&#26469;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#12290;&#29609;&#23478;&#20204;&#34987;&#38145;&#22312;&#19968;&#20010;&#25151;&#23376;&#37324;&#65292;&#24517;&#39035;&#25214;&#21040;&#19968;&#25226;&#38053;&#21273;&#25165;&#33021;&#36867;&#33073;&#65292;&#20294;&#20854;&#20013;&#19968;&#20010;&#29609;&#23478;&#34987;&#27966;&#20219;&#21153;&#26432;&#27515;&#20854;&#20182;&#20154;&#12290;&#27599;&#27425;&#21457;&#29983;&#35851;&#26432;&#65292;&#24184;&#23384;&#30340;&#29609;&#23478;&#20204;&#20250;&#36827;&#34892;&#33258;&#28982;&#35821;&#35328;&#35752;&#35770;&#65292;&#28982;&#21518;&#25237;&#31080;&#23558;&#19968;&#21517;&#29609;&#23478;&#25918;&#36880;&#20986;&#28216;&#25103;&#12290;&#25105;&#20204;&#20351;&#29992;GPT-3&#12289;GPT-3.5&#21644;GPT-4&#25805;&#25511;&#20195;&#29702;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#21457;&#29616;&#20102;&#27450;&#39575;&#21644;&#35782;&#21035;&#35854;&#35328;&#30340;&#33021;&#21147;&#35777;&#25454;&#12290;&#26432;&#25163;&#32463;&#24120;&#21542;&#35748;&#33258;&#24049;&#30340;&#32618;&#34892;&#24182;&#25351;&#36131;&#20182;&#20154;&#65292;&#23548;&#33268;&#25237;&#31080;&#32467;&#26524;&#21463;&#21040;&#21487;&#27979;&#37327;&#30340;&#24433;&#21709;&#12290;&#26356;&#20808;&#36827;&#30340;&#27169;&#22411;&#22312;24&#20010;&#20004;&#20004;&#27604;&#36739;&#20013;&#30340;18&#20010;&#20013;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#26432;&#25163;&#25928;&#26524;&#65292;&#36229;&#36234;&#20102;&#26356;&#23567;&#30340;&#27169;&#22411;&#12290;&#27425;&#35201;&#25351;&#26631;&#25552;&#20379;&#20102;&#35777;&#25454;&#65292;&#34920;&#26126;&#36825;&#31181;&#25913;&#36827;&#24182;&#19981;&#26159;&#36890;&#36807;&#19981;&#21516;&#30340;&#34892;&#21160;&#23454;&#29616;&#30340;&#65292;&#32780;&#26159;&#36890;&#36807;&#22312;&#35752;&#35770;&#20013;&#26356;&#24378;&#30340;&#27450;&#39575;&#33021;&#21147;&#23454;&#29616;&#30340;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#23454;&#36136;&#24615;&#30340;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Are current language models capable of deception and lie detection? We study this question by introducing a text-based game called $\textit{Hoodwinked}$, inspired by $\textit{Mafia}$ and $\textit{Among Us}$. Players are locked in a house and must find a key to escape, but one player is tasked with killing the others. Each time a murder is committed, the surviving players have a natural language discussion then vote to banish one player from the game. We conduct experiments with agents controlled by GPT-3, GPT-3.5, and GPT-4 and find evidence of deception and lie detection capabilities. The killer often denies their crime and accuses others, leading to measurable effects on voting outcomes. More advanced models are more effective killers, outperforming smaller models in 18 of 24 pairwise comparisons. Secondary metrics provide evidence that this improvement is not mediated by different actions, but rather by stronger deception capabilities during discussions. Overall, we find substantial
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#35270;&#35273;&#21644;&#35821;&#35328;&#30340;&#22270;&#20687;&#25991;&#26412;&#21305;&#37197;&#20219;&#21153;&#65292;&#35201;&#27714;&#27169;&#22411;&#33021;&#22815;&#30830;&#23450;&#22270;&#20687;&#19982;&#25991;&#26412;&#30340;&#20851;&#31995;&#65292;&#24182;&#23450;&#20301;&#25152;&#25351;&#30340;&#23545;&#35937;&#25110;&#32773;&#23545;&#19981;&#21305;&#37197;&#30340;&#37096;&#20998;&#36827;&#34892; grounding&#12290;&#20026;&#20102;&#35299;&#20915;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#27492;&#20219;&#21153;&#19978;&#30340;&#25968;&#25454;&#25928;&#29575;&#21644;&#38271;&#24230;&#27867;&#21270;&#33021;&#21147;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#31995;&#25935;&#24863;&#30340;&#23545;&#24212;&#25512;&#29702;&#32593;&#32476;&#65288;RCRN&#65289;&#65292;&#35813;&#32593;&#32476;&#36890;&#36807;&#21452;&#21521;&#20449;&#24687;&#20256;&#36882;&#21644;&#35821;&#35328;&#32467;&#26500;&#24341;&#23548;&#30340;&#20851;&#31995;&#24863;&#30693;&#25512;&#29702;&#26469;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2308.01236</link><description>&lt;p&gt;
&#20351;&#29992;&#19981;&#21305;&#37197;&#20851;&#31995;&#25512;&#29702;&#30340;&#22522;&#20110;&#35270;&#35273;&#21644;&#35821;&#35328;&#30340;&#22270;&#20687;&#25991;&#26412;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Grounded Image Text Matching with Mismatched Relation Reasoning. (arXiv:2308.01236v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01236
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#35270;&#35273;&#21644;&#35821;&#35328;&#30340;&#22270;&#20687;&#25991;&#26412;&#21305;&#37197;&#20219;&#21153;&#65292;&#35201;&#27714;&#27169;&#22411;&#33021;&#22815;&#30830;&#23450;&#22270;&#20687;&#19982;&#25991;&#26412;&#30340;&#20851;&#31995;&#65292;&#24182;&#23450;&#20301;&#25152;&#25351;&#30340;&#23545;&#35937;&#25110;&#32773;&#23545;&#19981;&#21305;&#37197;&#30340;&#37096;&#20998;&#36827;&#34892; grounding&#12290;&#20026;&#20102;&#35299;&#20915;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#27492;&#20219;&#21153;&#19978;&#30340;&#25968;&#25454;&#25928;&#29575;&#21644;&#38271;&#24230;&#27867;&#21270;&#33021;&#21147;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#31995;&#25935;&#24863;&#30340;&#23545;&#24212;&#25512;&#29702;&#32593;&#32476;&#65288;RCRN&#65289;&#65292;&#35813;&#32593;&#32476;&#36890;&#36807;&#21452;&#21521;&#20449;&#24687;&#20256;&#36882;&#21644;&#35821;&#35328;&#32467;&#26500;&#24341;&#23548;&#30340;&#20851;&#31995;&#24863;&#30693;&#25512;&#29702;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#20351;&#29992;&#19981;&#21305;&#37197;&#20851;&#31995;&#25512;&#29702;&#30340;&#22522;&#20110;&#35270;&#35273;&#21644;&#35821;&#35328;&#30340;&#22270;&#20687;&#25991;&#26412;&#21305;&#37197;&#65288;GITM-MR&#65289;&#8221;&#30340;&#26032;&#22411;&#35270;&#35273;-&#35821;&#35328;&#32852;&#21512;&#20219;&#21153;&#65292;&#29992;&#20110;&#35780;&#20272;&#22522;&#20110;Transformer&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20851;&#31995;&#29702;&#35299;&#33021;&#21147;&#12290;GITM-MR&#38656;&#35201;&#27169;&#22411;&#39318;&#20808;&#30830;&#23450;&#19968;&#20010;&#34920;&#36798;&#26159;&#21542;&#25551;&#36848;&#20102;&#19968;&#24352;&#22270;&#20687;&#65292;&#28982;&#21518;&#23450;&#20301;&#25152;&#25351;&#30340;&#23545;&#35937;&#25110;&#32773;&#23545;&#25991;&#26412;&#20013;&#30340;&#19981;&#21305;&#37197;&#37096;&#20998;&#36827;&#34892; grounding&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#30340;&#22522;&#20934;&#65292;&#37325;&#28857;&#20851;&#27880;&#26377;&#38480;&#25968;&#25454;&#21644;&#36229;&#20986;&#20998;&#24067;&#30340;&#21477;&#23376;&#38271;&#24230;&#30340;&#25361;&#25112;&#24615;&#35774;&#32622;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#32570;&#20047;&#25968;&#25454;&#25928;&#29575;&#21644;&#38271;&#24230;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20851;&#31995;&#25935;&#24863;&#30340;&#23545;&#24212;&#25512;&#29702;&#32593;&#32476;&#65288;RCRN&#65289;&#65292;&#36890;&#36807;&#21452;&#21521;&#20449;&#24687;&#20256;&#36882;&#21644;&#35821;&#35328;&#32467;&#26500;&#24341;&#23548;&#30340;&#20851;&#31995;&#24863;&#30693;&#25512;&#29702;&#65292;&#23558;RCRN&#35299;&#37322;&#20026;&#19968;&#20010;&#27169;&#22359;&#21270;&#31243;&#24207;&#65292;&#24182;&#22312;&#38271;&#24230;&#27867;&#21270;&#21644;&#25968;&#25454;&#25928;&#29575;&#26041;&#38754;&#34920;&#29616;&#20986;&#36739;&#24378;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces Grounded Image Text Matching with Mismatched Relation (GITM-MR), a novel visual-linguistic joint task that evaluates the relation understanding capabilities of transformer-based pre-trained models. GITM-MR requires a model to first determine if an expression describes an image, then localize referred objects or ground the mismatched parts of the text. We provide a benchmark for evaluating pre-trained models on this task, with a focus on the challenging settings of limited data and out-of-distribution sentence lengths. Our evaluation demonstrates that pre-trained models lack data efficiency and length generalization ability. To address this, we propose the Relation-sensitive Correspondence Reasoning Network (RCRN), which incorporates relation-aware reasoning via bi-directional message propagation guided by language structure. RCRN can be interpreted as a modular program and delivers strong performance in both length generalization and data efficiency.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#39318;&#27425;&#23558;&#27597;&#35821;&#35782;&#21035;&#24212;&#29992;&#20110;&#22303;&#32819;&#20854;&#35821;,&#36890;&#36807;&#20998;&#26512;&#20316;&#32773;&#19981;&#21516;&#35821;&#35328;&#30340;&#20889;&#20316;&#26469;&#39044;&#27979;&#20316;&#32773;&#30340;&#27597;&#35821;&#12290;&#30740;&#31350;&#20351;&#29992;&#20102;&#22303;&#32819;&#20854;&#23398;&#20064;&#32773;&#35821;&#26009;&#24211;&#21644;&#19977;&#20010;&#21477;&#27861;&#29305;&#24449;&#26469;&#23637;&#31034;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.14850</link><description>&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#39318;&#27425;&#23558;&#27597;&#35821;&#35782;&#21035;&#65288;Native Language Identification&#65292;NLI&#65289;&#24212;&#29992;&#20110;&#22303;&#32819;&#20854;&#35821;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Turkish Native Language Identification. (arXiv:2307.14850v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14850
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#39318;&#27425;&#23558;&#27597;&#35821;&#35782;&#21035;&#24212;&#29992;&#20110;&#22303;&#32819;&#20854;&#35821;,&#36890;&#36807;&#20998;&#26512;&#20316;&#32773;&#19981;&#21516;&#35821;&#35328;&#30340;&#20889;&#20316;&#26469;&#39044;&#27979;&#20316;&#32773;&#30340;&#27597;&#35821;&#12290;&#30740;&#31350;&#20351;&#29992;&#20102;&#22303;&#32819;&#20854;&#23398;&#20064;&#32773;&#35821;&#26009;&#24211;&#21644;&#19977;&#20010;&#21477;&#27861;&#29305;&#24449;&#26469;&#23637;&#31034;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23558;&#27597;&#35821;&#35782;&#21035;&#65288;NLI&#65289;&#24212;&#29992;&#20110;&#22303;&#32819;&#20854;&#35821;&#12290;NLI &#26159;&#36890;&#36807;&#20998;&#26512;&#20316;&#32773;&#19981;&#21516;&#35821;&#35328;&#30340;&#20889;&#20316;&#26469;&#39044;&#27979;&#20316;&#32773;&#30340;&#27597;&#35821;&#12290;&#23613;&#31649;&#22823;&#22810;&#25968;NLI&#30740;&#31350;&#37117;&#20391;&#37325;&#20110;&#33521;&#35821;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#23558;&#20854;&#33539;&#22260;&#25193;&#23637;&#21040;&#22303;&#32819;&#20854;&#35821;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#26368;&#36817;&#26500;&#24314;&#30340;&#22303;&#32819;&#20854;&#23398;&#20064;&#32773;&#35821;&#26009;&#24211;&#65292;&#24182;&#32467;&#21512;&#20102;&#19977;&#20010;&#21477;&#27861;&#29305;&#24449;&#65288;CFG &#20135;&#29983;&#35268;&#21017;&#65292;&#35789;&#24615;n-gram&#21644;&#20989;&#25968;&#35789;&#65289;&#19982;L2&#25991;&#26412;&#65292;&#20197;&#23637;&#31034;&#23427;&#20204;&#22312;&#35813;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present the first application of Native Language Identification (NLI) for the Turkish language. NLI involves predicting the writer's first language by analysing their writing in different languages. While most NLI research has focused on English, our study extends its scope to Turkish. We used the recently constructed Turkish Learner Corpus and employed a combination of three syntactic features (CFG production rules, part-of-speech n-grams and function words) with L2 texts to demonstrate their effectiveness in this task.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#20351;&#29992;&#35821;&#27861;&#28436;&#21270;&#33258;&#21160;&#35774;&#35745;&#35821;&#20041;&#30456;&#20284;&#24615;&#38598;&#21512;&#65292;&#36890;&#36807;&#33258;&#21160;&#36873;&#25321;&#21644;&#32858;&#21512;&#20505;&#36873;&#24230;&#37327;&#26469;&#20248;&#21270;&#38598;&#21512;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#65292;&#25552;&#39640;&#30456;&#20284;&#24230;&#35780;&#20272;&#20934;&#30830;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#20351;&#29992;&#38598;&#21512;&#23545;&#35821;&#20041;&#30456;&#20284;&#24615;&#20219;&#21153;&#30340;&#30410;&#22788;&#12290;</title><link>http://arxiv.org/abs/2307.00925</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#27861;&#28436;&#21270;&#33258;&#21160;&#35774;&#35745;&#35821;&#20041;&#30456;&#20284;&#24615;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
Automatic Design of Semantic Similarity Ensembles Using Grammatical Evolution. (arXiv:2307.00925v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#20351;&#29992;&#35821;&#27861;&#28436;&#21270;&#33258;&#21160;&#35774;&#35745;&#35821;&#20041;&#30456;&#20284;&#24615;&#38598;&#21512;&#65292;&#36890;&#36807;&#33258;&#21160;&#36873;&#25321;&#21644;&#32858;&#21512;&#20505;&#36873;&#24230;&#37327;&#26469;&#20248;&#21270;&#38598;&#21512;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#65292;&#25552;&#39640;&#30456;&#20284;&#24230;&#35780;&#20272;&#20934;&#30830;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#20351;&#29992;&#38598;&#21512;&#23545;&#35821;&#20041;&#30456;&#20284;&#24615;&#20219;&#21153;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#30456;&#20284;&#24615;&#24230;&#37327;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#22810;&#31181;&#19982;&#35745;&#31639;&#26426;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#27809;&#26377;&#21333;&#19968;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#24230;&#37327;&#36866;&#29992;&#20110;&#25152;&#26377;&#20219;&#21153;&#65292;&#30740;&#31350;&#20154;&#21592;&#32463;&#24120;&#20351;&#29992;&#38598;&#21512;&#31574;&#30053;&#26469;&#30830;&#20445;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35774;&#35745;&#35821;&#20041;&#30456;&#20284;&#24615;&#38598;&#21512;&#30340;&#26041;&#27861;&#12290;&#20107;&#23454;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#39318;&#27425;&#20351;&#29992;&#35821;&#27861;&#28436;&#21270;&#26469;&#33258;&#21160;&#36873;&#25321;&#21644;&#32858;&#21512;&#19968;&#32452;&#20505;&#36873;&#24230;&#37327;&#65292;&#20197;&#21019;&#24314;&#19968;&#20010;&#26368;&#22823;&#21270;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#24615;&#30340;&#38598;&#21512;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#19982;&#26368;&#20808;&#36827;&#30340;&#38598;&#21512;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#32467;&#26524;&#26174;&#31034;&#23427;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#30456;&#20284;&#24230;&#35780;&#20272;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#26082;&#23637;&#31034;&#20102;&#20351;&#29992;&#35821;&#27861;&#28436;&#21270;&#26469;&#33258;&#21160;&#27604;&#36739;&#25991;&#26412;&#30340;&#28508;&#21147;&#65292;&#20063;&#35777;&#26126;&#20102;&#20351;&#29992;&#38598;&#21512;&#23545;&#35821;&#20041;&#30456;&#20284;&#24615;&#20219;&#21153;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic similarity measures are widely used in natural language processing to catalyze various computer-related tasks. However, no single semantic similarity measure is the most appropriate for all tasks, and researchers often use ensemble strategies to ensure performance. This research work proposes a method for automatically designing semantic similarity ensembles. In fact, our proposed method uses grammatical evolution, for the first time, to automatically select and aggregate measures from a pool of candidates to create an ensemble that maximizes correlation to human judgment. The method is evaluated on several benchmark datasets and compared to state-of-the-art ensembles, showing that it can significantly improve similarity assessment accuracy and outperform existing methods in some cases. As a result, our research demonstrates the potential of using grammatical evolution to automatically compare text and prove the benefits of using ensembles for semantic similarity tasks. The so
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;GPT-3.5&#21644;GPT-4&#23545;&#20154;&#31867;&#24402;&#32435;&#25512;&#29702;&#20013;&#30340;&#23646;&#24615;&#24402;&#32435;&#38382;&#39064;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;GPT-3.5&#26377;&#19968;&#20123;&#22256;&#38590;&#65292;&#20294;GPT-4&#30340;&#34920;&#29616;&#19982;&#20154;&#31867;&#30456;&#20284;&#65292;&#38500;&#20102;&#26410;&#33021;&#25429;&#25417;&#21040;&#21069;&#25552;&#30340;&#38750;&#21333;&#35843;&#24615;&#29616;&#35937;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#20154;&#31867;&#21644;&#26426;&#22120;&#26234;&#33021;&#25552;&#20379;&#20102;&#26377;&#36259;&#30340;&#27604;&#36739;&#65292;&#24182;&#25552;&#20379;&#20102;&#29992;&#20316;&#26410;&#26469;&#30740;&#31350;&#22522;&#20934;&#30340;&#20004;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2306.06548</link><description>&lt;p&gt;
&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24402;&#32435;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Inductive reasoning in humans and large language models. (arXiv:2306.06548v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;GPT-3.5&#21644;GPT-4&#23545;&#20154;&#31867;&#24402;&#32435;&#25512;&#29702;&#20013;&#30340;&#23646;&#24615;&#24402;&#32435;&#38382;&#39064;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;GPT-3.5&#26377;&#19968;&#20123;&#22256;&#38590;&#65292;&#20294;GPT-4&#30340;&#34920;&#29616;&#19982;&#20154;&#31867;&#30456;&#20284;&#65292;&#38500;&#20102;&#26410;&#33021;&#25429;&#25417;&#21040;&#21069;&#25552;&#30340;&#38750;&#21333;&#35843;&#24615;&#29616;&#35937;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#20154;&#31867;&#21644;&#26426;&#22120;&#26234;&#33021;&#25552;&#20379;&#20102;&#26377;&#36259;&#30340;&#27604;&#36739;&#65292;&#24182;&#25552;&#20379;&#20102;&#29992;&#20316;&#26410;&#26469;&#30740;&#31350;&#22522;&#20934;&#30340;&#20004;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21331;&#36234;&#24615;&#33021;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#20854;&#26159;&#21542;&#33021;&#20316;&#20026;&#26222;&#36890;&#26234;&#33021;&#30340;&#27169;&#22411;&#25110;&#31867;&#20284;&#20110;&#20154;&#31867;&#35748;&#30693;&#30340;&#31243;&#24230;&#30340;&#30097;&#38382;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;GPT-3.5&#21644;GPT-4&#24212;&#29992;&#20110;&#20154;&#31867;&#24402;&#32435;&#25512;&#29702;&#20013;&#30340;&#19968;&#20010;&#32463;&#20856;&#38382;&#39064;&#65292;&#21363;&#23646;&#24615;&#24402;&#32435;&#65292;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36890;&#36807;&#20004;&#20010;&#23454;&#39564;&#65292;&#25105;&#20204;&#33719;&#21462;&#20102;&#20154;&#31867;&#22312;&#22810;&#20010;&#39046;&#22495;&#19978;&#30340;&#23646;&#24615;&#24402;&#32435;&#20219;&#21153;&#19978;&#30340;&#21028;&#26029;&#12290;&#23613;&#31649;GPT-3.5&#22312;&#25429;&#25417;&#20154;&#31867;&#34892;&#20026;&#30340;&#35768;&#22810;&#26041;&#38754;&#19978;&#26377;&#22256;&#38590;&#65292;&#20294;GPT-4&#26356;&#21152;&#25104;&#21151;&#65306;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#65292;&#23427;&#30340;&#34920;&#29616;&#19982;&#20154;&#31867;&#30340;&#34920;&#29616;&#22312;&#36136;&#19978;&#30456;&#21305;&#37197;&#65292;&#21807;&#19968;&#26174;&#33879;&#30340;&#20363;&#22806;&#26159;&#20854;&#26410;&#33021;&#25429;&#25417;&#21040;&#21069;&#25552;&#30340;&#38750;&#21333;&#35843;&#24615;&#29616;&#35937;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#35777;&#26126;&#20102;&#23646;&#24615;&#24402;&#32435;&#21487;&#20197;&#23545;&#20154;&#31867;&#21644;&#26426;&#22120;&#26234;&#33021;&#36827;&#34892;&#26377;&#36259;&#30340;&#27604;&#36739;&#65292;&#24182;&#25552;&#20379;&#20102;&#20004;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#20316;&#20026;&#26410;&#26469;&#22312;&#36825;&#19968;&#39046;&#22495;&#20013;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
The impressive recent performance of large language models has led many to wonder to what extent they can serve as models of general intelligence or are similar to human cognition. We address this issue by applying GPT-3.5 and GPT-4 to a classic problem in human inductive reasoning known as property induction. Over two experiments, we elicit human judgments on a range of property induction tasks spanning multiple domains. Although GPT-3.5 struggles to capture many aspects of human behaviour, GPT-4 is much more successful: for the most part, its performance qualitatively matches that of humans, and the only notable exception is its failure to capture the phenomenon of premise non-monotonicity. Our work demonstrates that property induction allows for interesting comparisons between human and machine intelligence and provides two large datasets that can serve as benchmarks for future work in this vein.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#20013;&#30340;&#19977;&#31181;&#26631;&#31614;&#20559;&#24046;&#25552;&#20986;&#20998;&#31867;&#27861;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#31616;&#21333;&#30340;&#20559;&#24046;&#26657;&#20934;&#26041;&#27861;&#65292;&#20351;&#29992;&#38543;&#26426;&#30340;&#39046;&#22495;&#35789;&#20272;&#31639;&#35821;&#35328;&#27169;&#22411;&#30340;&#26631;&#31614;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2305.19148</link><description>&lt;p&gt;
&#32531;&#35299;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26631;&#31614;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Mitigating Label Biases for In-context Learning. (arXiv:2305.19148v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19148
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#20013;&#30340;&#19977;&#31181;&#26631;&#31614;&#20559;&#24046;&#25552;&#20986;&#20998;&#31867;&#27861;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#31616;&#21333;&#30340;&#20559;&#24046;&#26657;&#20934;&#26041;&#27861;&#65292;&#20351;&#29992;&#38543;&#26426;&#30340;&#39046;&#22495;&#35789;&#20272;&#31639;&#35821;&#35328;&#27169;&#22411;&#30340;&#26631;&#31614;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#30340;&#21508;&#31181;&#35774;&#35745;&#35774;&#32622;&#65292;&#22914;&#36873;&#25321;&#21644;&#39034;&#24207;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#65292;&#21487;&#33021;&#20351;&#27169;&#22411;&#23545;&#26576;&#31181;&#29305;&#23450;&#39044;&#27979;&#20559;&#35265;&#65292;&#32780;&#36825;&#31181;&#39044;&#27979;&#24182;&#19981;&#21453;&#26144;&#23545;&#20219;&#21153;&#30340;&#29702;&#35299;&#12290;&#34429;&#28982;&#35768;&#22810;&#30740;&#31350;&#35752;&#35770;&#20102;&#36825;&#20123;&#35774;&#35745;&#36873;&#25321;&#65292;&#20294;&#23545;&#23427;&#20204;&#36827;&#34892;&#20998;&#31867;&#21644;&#20943;&#32531;&#20854;&#24433;&#21709;&#30340;&#31995;&#32479;&#35843;&#26597;&#24456;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20026;&#25991;&#26412;&#20998;&#31867;&#20013;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#20013;&#30340;&#19977;&#31181;&#26631;&#31614;&#20559;&#24046;&#23450;&#20041;&#20102;&#19968;&#20010;&#20998;&#31867;&#27861;&#65306;&#39321;&#33609;&#26631;&#31614;&#20559;&#24046;&#12289;&#19978;&#19979;&#25991;&#26631;&#31614;&#20559;&#24046;&#21644;&#39046;&#22495;&#26631;&#31614;&#20559;&#24046;&#65288;&#25105;&#20204;&#39318;&#27425;&#27010;&#24565;&#21270;&#21644;&#26816;&#27979;&#21040;&#65289;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#20808;&#21069;&#30340;&#26631;&#31614;&#20559;&#24046;&#26657;&#20934;&#26041;&#27861;&#19981;&#33021;&#35299;&#20915;&#25152;&#26377;&#19977;&#31181;&#20559;&#24046;&#12290;&#29305;&#21035;&#26159;&#65292;&#39046;&#22495;&#26631;&#31614;&#20559;&#24046;&#20351;LLM&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#21482;&#33021;&#23454;&#29616;&#38543;&#26426;&#32423;&#21035;&#30340;&#24615;&#33021;&#65292;&#32780;&#19981;&#31649;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#36873;&#25321;&#22914;&#20309;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#31616;&#21333;&#30340;&#20559;&#24046;&#26657;&#20934;&#26041;&#27861;&#65292;&#20351;&#29992;&#38543;&#26426;&#30340;&#39046;&#22495;&#35789;&#20272;&#31639;&#35821;&#35328;&#27169;&#22411;&#30340;&#26631;&#31614;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Various design settings for in-context learning (ICL), such as the choice and order of the in-context examples, can bias a model toward a particular prediction without being reflective of an understanding of the task. While many studies discuss these design choices, there have been few systematic investigations into categorizing them and mitigating their impact. In this work, we define a typology for three types of label biases in ICL for text classification: vanilla-label bias, context-label bias, and domain-label bias (which we conceptualize and detect for the first time).  Our analysis demonstrates that prior label bias calibration methods fall short of addressing all three types of biases. Specifically, domain-label bias restricts LLMs to random-level performance on many tasks regardless of the choice of in-context examples. To mitigate the effect of these biases, we propose a simple bias calibration method that estimates a language model's label bias using random in-domain words f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37327;&#21270;&#20998;&#26512;&#20102;&#20027;&#27969;&#23186;&#20307;&#23545;ChatGPT&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#25253;&#36947;&#36235;&#21183;&#21644;&#24773;&#24863;&#24577;&#24230;&#65292;&#21457;&#29616;&#20154;&#20204;&#26222;&#36941;&#23545;&#20854;&#25345;&#31215;&#26497;&#24577;&#24230;&#12290;&#28982;&#32780;&#65292;&#20027;&#39064;&#30340;&#35789;&#39057;&#20998;&#26512;&#26174;&#31034;&#65292;&#22823;&#22411;&#31185;&#25216;&#38382;&#39064;&#21644;&#34892;&#20026;&#32773;&#24471;&#21040;&#20102;&#39640;&#24230;&#20851;&#27880;&#65292;&#32780;&#23601;&#19994;&#12289;&#22810;&#26679;&#24615;&#12289;&#20262;&#29702;&#12289;&#29256;&#26435;&#12289;&#24615;&#21035;&#21644;&#22899;&#24615;&#31561;&#20027;&#39064;&#21017;&#34920;&#29616;&#19981;&#36275;&#25110;&#23436;&#20840;&#32570;&#22833;&#12290;&#26412;&#25991;&#21628;&#21505;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#38656;&#35201;&#26356;&#21152;&#22810;&#20803;&#21644;&#32454;&#33268;&#30340;&#23186;&#20307;&#21457;&#35328;&#12290;</title><link>http://arxiv.org/abs/2305.18340</link><description>&lt;p&gt;
&#22312;&#20027;&#27969;&#23186;&#20307;&#19978;&#32472;&#21046;ChatGPT&#30340;&#22320;&#22270;&#65306;&#24773;&#24863;&#20998;&#26512;&#21644;&#35789;&#39057;&#20998;&#26512;&#30340;&#26089;&#26399;&#37327;&#21270;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mapping ChatGPT in Mainstream Media: Early Quantitative Insights through Sentiment Analysis and Word Frequency Analysis. (arXiv:2305.18340v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37327;&#21270;&#20998;&#26512;&#20102;&#20027;&#27969;&#23186;&#20307;&#23545;ChatGPT&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#25253;&#36947;&#36235;&#21183;&#21644;&#24773;&#24863;&#24577;&#24230;&#65292;&#21457;&#29616;&#20154;&#20204;&#26222;&#36941;&#23545;&#20854;&#25345;&#31215;&#26497;&#24577;&#24230;&#12290;&#28982;&#32780;&#65292;&#20027;&#39064;&#30340;&#35789;&#39057;&#20998;&#26512;&#26174;&#31034;&#65292;&#22823;&#22411;&#31185;&#25216;&#38382;&#39064;&#21644;&#34892;&#20026;&#32773;&#24471;&#21040;&#20102;&#39640;&#24230;&#20851;&#27880;&#65292;&#32780;&#23601;&#19994;&#12289;&#22810;&#26679;&#24615;&#12289;&#20262;&#29702;&#12289;&#29256;&#26435;&#12289;&#24615;&#21035;&#21644;&#22899;&#24615;&#31561;&#20027;&#39064;&#21017;&#34920;&#29616;&#19981;&#36275;&#25110;&#23436;&#20840;&#32570;&#22833;&#12290;&#26412;&#25991;&#21628;&#21505;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#38656;&#35201;&#26356;&#21152;&#22810;&#20803;&#21644;&#32454;&#33268;&#30340;&#23186;&#20307;&#21457;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#19968;&#20010;&#30001;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#20854;&#29992;&#25143;&#33719;&#21462;&#21644;&#26222;&#21450;&#24615;&#30340;&#25351;&#25968;&#22686;&#38271;&#65292;&#20276;&#38543;&#30528;&#24191;&#27867;&#30340;&#20027;&#27969;&#23186;&#20307;&#25253;&#36947;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#19982;ChatGPT&#21644;&#20154;&#24037;&#26234;&#33021;&#20027;&#39064;&#30456;&#20851;&#30340;10,902&#26465;&#20027;&#27969;&#26032;&#38395;&#26631;&#39064;&#35821;&#26009;&#36827;&#34892;&#25991;&#26412;&#25366;&#25496;&#21644;NLP&#26041;&#27861;&#30340;&#23450;&#37327;&#25968;&#25454;&#20998;&#26512;&#65292;&#21576;&#29616;&#20986;&#26089;&#26399;&#36235;&#21183;&#21644;&#24773;&#24863;&#30340;&#21457;&#29616;&#12290;&#24773;&#24863;&#20998;&#26512;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;ChatGPT&#21644;&#20154;&#24037;&#26234;&#33021;&#22312;&#20027;&#27969;&#23186;&#20307;&#20013;&#30340;&#22909;&#24863;&#24230;&#39640;&#20110;&#21453;&#24863;&#24230;&#12290;&#20851;&#20110;&#35789;&#39057;&#32467;&#26524;&#65292;65%&#20197;&#19978;&#30340;&#39640;&#39057;&#35789;&#38598;&#20013;&#22312;&#22823;&#22411;&#31185;&#25216;&#38382;&#39064;&#21644;&#34892;&#20026;&#32773;&#19978;&#65292;&#32780;&#23601;&#19994;&#12289;&#22810;&#26679;&#24615;&#12289;&#20262;&#29702;&#12289;&#29256;&#26435;&#12289;&#24615;&#21035;&#21644;&#22899;&#24615;&#31561;&#20027;&#39064;&#21017;&#34920;&#29616;&#19981;&#36275;&#25110;&#23436;&#20840;&#32570;&#22833;&#65292;&#20165;&#21344;&#24635;&#35821;&#26009;&#30340;6%&#12290;&#26412;&#25991;&#26159;&#23545;&#20027;&#27969;&#23186;&#20307;&#25253;&#36947;ChatGPT&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#26435;&#21147;&#32467;&#26500;&#36827;&#34892;&#30340;&#25209;&#21028;&#24615;&#20998;&#26512;&#65292;&#24182;&#24378;&#35843;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#38656;&#35201;&#26356;&#21152;&#22810;&#20803;&#21644;&#32454;&#33268;&#30340;&#23186;&#20307;&#21457;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
The exponential growth in user acquisition and popularity of ChatGPT, an artificial intelligence(AI) powered chatbot, was accompanied by widespread mainstream media coverage. This article presents a quantitative data analysis of the early trends and sentiments revealed by conducting text mining and NLP methods onto a corpus of 10,902 mainstream news headlines related to the subject of ChatGPT and artificial intelligence, from the launch of ChatGPT in November 2022 to March 2023. The findings revealed in sentiment analysis, ChatGPT and artificial intelligence, were perceived more positively than negatively in the mainstream media. In regards to word frequency results, over sixty-five percent of the top frequency words were focused on Big Tech issues and actors while topics such as jobs, diversity, ethics, copyright, gender and women were poorly represented or completely absent and only accounted for six percent of the total corpus. This article is a critical analysis into the power stru
&lt;/p&gt;</description></item><item><title>G3Detector&#26159;&#19968;&#31181;&#36890;&#29992;GPT&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22120;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#22810;&#20010;&#39046;&#22495;&#20013;&#30340;&#21512;&#25104;&#25991;&#26412;&#65292;&#24182;&#19988;&#20855;&#22791;&#22312;&#21508;&#31181;&#27169;&#22411;&#26550;&#26500;&#21644;&#35299;&#30721;&#31574;&#30053;&#19979;&#22343;&#34920;&#29616;&#20986;&#20248;&#24322;&#24615;&#33021;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.12680</link><description>&lt;p&gt;
G3Detector&#65306;&#36890;&#29992;GPT&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
G3Detector: General GPT-Generated Text Detector. (arXiv:2305.12680v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12680
&lt;/p&gt;
&lt;p&gt;
G3Detector&#26159;&#19968;&#31181;&#36890;&#29992;GPT&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22120;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#22810;&#20010;&#39046;&#22495;&#20013;&#30340;&#21512;&#25104;&#25991;&#26412;&#65292;&#24182;&#19988;&#20855;&#22791;&#22312;&#21508;&#31181;&#27169;&#22411;&#26550;&#26500;&#21644;&#35299;&#30721;&#31574;&#30053;&#19979;&#22343;&#34920;&#29616;&#20986;&#20248;&#24322;&#24615;&#33021;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39046;&#22495;&#30340;&#36805;&#36895;&#21457;&#23637;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#30410;&#22788;&#65292;&#20294;&#24517;&#39035;&#35748;&#35782;&#21040;&#36825;&#20123;&#27169;&#22411;&#30340;&#28508;&#22312;&#28389;&#29992;&#21487;&#33021;&#24341;&#21457;&#19968;&#31995;&#21015;&#31038;&#20250;&#21644;&#20262;&#29702;&#22256;&#22659;&#12290;&#23613;&#31649;&#26377;&#35768;&#22810;&#26089;&#26399;&#30340;&#24037;&#20316;&#33268;&#21147;&#20110;&#21306;&#20998;&#21512;&#25104;&#25991;&#26412;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26816;&#27979;&#31995;&#32479;&#26080;&#27861;&#35782;&#21035;&#26368;&#26032;&#30340;LLM&#29983;&#25104;&#30340;&#25968;&#25454;&#65292;&#20363;&#22914;ChatGPT&#21644;GPT-4&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#21508;&#20010;&#39046;&#22495;&#20934;&#30830;&#35782;&#21035;&#21512;&#25104;&#25991;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26816;&#27979;&#22120;&#22312;&#21508;&#31181;&#27169;&#22411;&#26550;&#26500;&#21644;&#35299;&#30721;&#31574;&#30053;&#19979;&#37117;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#20855;&#22791;&#35782;&#21035;&#37319;&#29992;&#24378;&#22823;&#30340;&#26816;&#27979;-&#36530;&#36991;&#25216;&#26415;&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#20840;&#38754;&#30740;&#31350;&#20984;&#26174;&#20102;&#25105;&#20204;&#25552;&#39640;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#40065;&#26834;&#24615;&#21644;&#25928;&#29575;&#30340;&#25215;&#35834;&#12290;
&lt;/p&gt;
&lt;p&gt;
The burgeoning progress in the field of Large Language Models (LLMs) heralds significant benefits due to their unparalleled capacities. However, it is critical to acknowledge the potential misuse of these models, which could give rise to a spectrum of social and ethical dilemmas. Despite numerous preceding efforts centered around distinguishing synthetic text, most existing detection systems fail to identify data synthesized by the latest LLMs, such as ChatGPT and GPT-4. In response to this challenge, we introduce an unpretentious yet potent detection approach proficient in identifying synthetic text across a wide array of fields. Moreover, our detector demonstrates outstanding performance uniformly across various model architectures and decoding strategies. It also possesses the capability to identify text generated utilizing a potent detection-evasion technique. Our comprehensive research underlines our commitment to boosting the robustness and efficiency of machine-generated text de
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#22312;&#38750;&#33258;&#22238;&#24402;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#24341;&#20837;&#36873;&#25321;&#24615;&#30693;&#35782;&#33976;&#39311;&#21644;&#28176;&#36827;&#33976;&#39311;&#26041;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;NAT&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#36136;&#37327;&#21644;&#22797;&#26434;&#24230;&#20043;&#38388;&#23454;&#29616;&#28789;&#27963;&#26435;&#34913;&#65292;&#26377;&#21161;&#20110;NAT&#36229;&#36234;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2303.17910</link><description>&lt;p&gt;
&#38750;&#33258;&#22238;&#24402;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#36873;&#25321;&#24615;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Selective Knowledge Distillation for Non-Autoregressive Neural Machine Translation. (arXiv:2303.17910v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17910
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#22312;&#38750;&#33258;&#22238;&#24402;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#24341;&#20837;&#36873;&#25321;&#24615;&#30693;&#35782;&#33976;&#39311;&#21644;&#28176;&#36827;&#33976;&#39311;&#26041;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;NAT&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#36136;&#37327;&#21644;&#22797;&#26434;&#24230;&#20043;&#38388;&#23454;&#29616;&#28789;&#27963;&#26435;&#34913;&#65292;&#26377;&#21161;&#20110;NAT&#36229;&#36234;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24207;&#21015;&#32423;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#24335;&#65292;&#38750;&#33258;&#22238;&#24402;&#21464;&#21387;&#22120;&#65288;NAT&#65289;&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30693;&#35782;&#33976;&#39311;&#23384;&#22312;&#21103;&#20316;&#29992;&#65292;&#22914;&#23558;&#25945;&#24072;&#26426;&#20013;&#30340;&#38169;&#35823;&#20256;&#25773;&#21040;NAT&#23398;&#29983;&#20013;&#65292;&#36825;&#21487;&#33021;&#38480;&#21046;NAT&#27169;&#22411;&#30340;&#36827;&#19968;&#27493;&#25913;&#36827;&#65292;&#24182;&#19988;&#24456;&#23569;&#22312;&#29616;&#26377;&#30740;&#31350;&#20013;&#35752;&#35770;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;NAT&#35780;&#20272;&#22120;&#26469;&#36827;&#34892;&#36873;&#25321;&#24615;&#30693;&#35782;&#33976;&#39311;&#65292;&#36873;&#25321;&#39640;&#36136;&#37327;&#19988;&#26131;&#20110;&#23398;&#20064;&#30340;NAT&#21451;&#22909;&#30446;&#26631;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#28176;&#36827;&#33976;&#39311;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;NAT&#24615;&#33021;&#12290;&#22312;&#22810;&#20010;WMT&#35821;&#35328;&#26041;&#21521;&#21644;&#20960;&#20010;&#20195;&#34920;&#24615;&#30340;NAT&#27169;&#22411;&#19978;&#36827;&#34892;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;NAT&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#36136;&#37327;&#21644;&#22797;&#26434;&#24230;&#20043;&#38388;&#30340;&#28789;&#27963;&#26435;&#34913;&#65292;&#36798;&#21040;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#21482;&#33976;&#39311;5&#65285;&#30340;&#21407;&#22987;&#32763;&#35793;&#23601;&#21487;&#20197;&#24110;&#21161;NAT&#36229;&#36234;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Benefiting from the sequence-level knowledge distillation, the Non-Autoregressive Transformer (NAT) achieves great success in neural machine translation tasks. However, existing knowledge distillation has side effects, such as propagating errors from the teacher to NAT students, which may limit further improvements of NAT models and are rarely discussed in existing research. In this paper, we introduce selective knowledge distillation by introducing an NAT evaluator to select NAT-friendly targets that are of high quality and easy to learn. In addition, we introduce a simple yet effective progressive distillation method to boost NAT performance. Experiment results on multiple WMT language directions and several representative NAT models show that our approach can realize a flexible trade-off between the quality and complexity of training data for NAT models, achieving strong performances. Further analysis shows that distilling only 5% of the raw translations can help an NAT outperform i
&lt;/p&gt;</description></item><item><title>LMExplainer&#26159;&#19968;&#31181;&#30693;&#35782;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#27169;&#22359;&#65292;&#20351;&#29992;&#30693;&#35782;&#22270;&#21644;&#22270;&#27880;&#24847;&#21147;&#31070;&#32463;&#32593;&#32476;&#26469;&#25552;&#21462;&#20851;&#38190;&#20915;&#31574;&#20449;&#21495;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#21487;&#29702;&#35299;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2303.16537</link><description>&lt;p&gt;
LMExplainer&#65306;&#19968;&#31181;&#21152;&#24378;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#33021;&#21147;&#30340;&#30693;&#35782;&#25552;&#21319;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
LMExplainer: a Knowledge-Enhanced Explainer for Language Models. (arXiv:2303.16537v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16537
&lt;/p&gt;
&lt;p&gt;
LMExplainer&#26159;&#19968;&#31181;&#30693;&#35782;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#27169;&#22359;&#65292;&#20351;&#29992;&#30693;&#35782;&#22270;&#21644;&#22270;&#27880;&#24847;&#21147;&#31070;&#32463;&#32593;&#32476;&#26469;&#25552;&#21462;&#20851;&#38190;&#20915;&#31574;&#20449;&#21495;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#21487;&#29702;&#35299;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24040;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT-4&#65289;&#38750;&#24120;&#24378;&#22823;&#65292;&#21487;&#20197;&#22788;&#29702;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22810;&#23618;&#38750;&#32447;&#24615;&#27169;&#22411;&#32467;&#26500;&#21644;&#25968;&#30334;&#19975;&#20010;&#21442;&#25968;&#65292;&#24456;&#38590;&#35299;&#37322;&#20854;&#32467;&#26524;&#12290;&#23545;&#20110;&#29992;&#25143;&#32780;&#35328;&#65292;&#20102;&#35299;&#27169;&#22411;&#30340;&#24037;&#20316;&#26041;&#24335;&#32570;&#20047;&#29702;&#35299;&#65292;&#21487;&#33021;&#20351;&#27169;&#22411;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#20855;&#26377;&#19981;&#21487;&#38752;&#24615;&#21644;&#21361;&#38505;&#24615;&#12290;&#22823;&#22810;&#25968;&#26368;&#36817;&#30340;&#24037;&#20316;&#21033;&#29992;&#27880;&#24847;&#21147;&#26435;&#37325;&#26469;&#25552;&#20379;&#27169;&#22411;&#39044;&#27979;&#30340;&#35299;&#37322;&#12290;&#20294;&#26159;&#65292;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#35299;&#37322;&#26080;&#27861;&#25903;&#25345;&#19981;&#26029;&#22686;&#38271;&#30340;&#27169;&#22411;&#22797;&#26434;&#24615;&#65292;&#24182;&#19988;&#26080;&#27861;&#25512;&#29702;&#20854;&#20915;&#31574;&#36807;&#31243;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LMExplainer&#65292;&#19968;&#31181;&#20026;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20154;&#31867;&#21487;&#29702;&#35299;&#35299;&#37322;&#30340;&#30693;&#35782;&#22686;&#24378;&#27169;&#22359;&#12290;&#25105;&#20204;&#20351;&#29992;&#30693;&#35782;&#22270;&#21644;&#22270;&#27880;&#24847;&#21147;&#31070;&#32463;&#32593;&#32476;&#26469;&#25552;&#21462;LM&#30340;&#20851;&#38190;&#20915;&#31574;&#20449;&#21495;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25506;&#35752;&#35299;&#37322;&#33021;&#21542;&#20063;&#24110;&#21161;&#20154;&#24037;&#26234;&#33021;&#26356;&#22909;&#22320;&#29702;&#35299;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LMs) such as GPT-4 are very powerful and can process different kinds of natural language processing (NLP) tasks. However, it can be difficult to interpret the results due to the multi-layer nonlinear model structure and millions of parameters. Lack of understanding of how the model works can make the model unreliable and dangerous for everyday users in real-world scenarios. Most recent works exploit the weights of attention to provide explanations for model predictions. However, pure attention-based explanation is unable to support the growing complexity of the models, and cannot reason about their decision-making processes. Thus, we propose LMExplainer, a knowledge-enhanced interpretation module for language models that can provide human-understandable explanations. We use a knowledge graph (KG) and a graph attention neural network to extract the key decision signals of the LM. We further explore whether interpretation can also help AI understand the task better
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#36719;&#25552;&#31034;&#23884;&#20837;&#65292;&#25552;&#20986;Soft Prompt-Based Calibration (SPeC)&#31649;&#36947;&#65292;&#26469;&#20943;&#36731;&#36755;&#20837;&#21464;&#37327;&#23545;&#36755;&#20986;&#22810;&#26679;&#24615;&#30340;&#24433;&#21709;&#65292;&#38477;&#20302;&#24615;&#33021;&#21464;&#24322;. &#27492;&#26041;&#27861;&#19981;&#20165;&#27604;&#22823;&#35821;&#35328;&#27169;&#22411;(LLM)&#24615;&#33021;&#31283;&#23450;&#65292;&#32780;&#19988;&#22312;&#20020;&#24202;&#31508;&#35760;&#25688;&#35201;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;.</title><link>http://arxiv.org/abs/2303.13035</link><description>&lt;p&gt;
SPeC&#65306;&#36719;&#25552;&#31034;&#26657;&#20934;&#22312;&#20020;&#24202;&#31508;&#35760;&#25688;&#35201;&#20013;&#38477;&#20302;&#24615;&#33021;&#21464;&#24322;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
SPeC: A Soft Prompt-Based Calibration on Mitigating Performance Variability in Clinical Notes Summarization. (arXiv:2303.13035v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13035
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#36719;&#25552;&#31034;&#23884;&#20837;&#65292;&#25552;&#20986;Soft Prompt-Based Calibration (SPeC)&#31649;&#36947;&#65292;&#26469;&#20943;&#36731;&#36755;&#20837;&#21464;&#37327;&#23545;&#36755;&#20986;&#22810;&#26679;&#24615;&#30340;&#24433;&#21709;&#65292;&#38477;&#20302;&#24615;&#33021;&#21464;&#24322;. &#27492;&#26041;&#27861;&#19981;&#20165;&#27604;&#22823;&#35821;&#35328;&#27169;&#22411;(LLM)&#24615;&#33021;&#31283;&#23450;&#65292;&#32780;&#19988;&#22312;&#20020;&#24202;&#31508;&#35760;&#25688;&#35201;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#23384;&#20648;&#30528;&#21253;&#25324;&#30149;&#21382;&#12289;&#35786;&#26029;&#12289;&#27835;&#30103;&#21644;&#26816;&#27979;&#32467;&#26524;&#22312;&#20869;&#30340;&#22823;&#37327;&#24739;&#32773;&#20449;&#24687;&#12290;&#36825;&#20123;&#35760;&#24405;&#23545;&#20110;&#21307;&#30103;&#20445;&#20581;&#19987;&#19994;&#20154;&#21592;&#20570;&#20986;&#26126;&#26234;&#30340;&#24739;&#32773;&#25252;&#29702;&#20915;&#31574;&#38750;&#24120;&#20851;&#38190;&#12290;&#25688;&#35201;&#20020;&#24202;&#31508;&#35760;&#21487;&#20197;&#24110;&#21161;&#21307;&#30103;&#20445;&#20581;&#19987;&#19994;&#20154;&#21592;&#26356;&#22909;&#22320;&#21457;&#29616;&#28508;&#22312;&#20581;&#24247;&#39118;&#38505;&#65292;&#20197;&#21450;&#20570;&#20986;&#26356;&#22909;&#30340;&#20915;&#31574;&#12290;&#36825;&#19968;&#36807;&#31243;&#36890;&#36807;&#30830;&#20445;&#21307;&#30103;&#20445;&#20581;&#19987;&#19994;&#20154;&#21592;&#21487;&#20197;&#35775;&#38382;&#26368;&#30456;&#20851;&#21644;&#26368;&#26032;&#30340;&#24739;&#32773;&#25968;&#25454;&#65292;&#26377;&#21161;&#20110;&#20943;&#23569;&#38169;&#35823;&#24182;&#25552;&#39640;&#24739;&#32773;&#30340;&#25252;&#29702;&#25928;&#26524;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23558;&#25552;&#31034;&#19982;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30456;&#32467;&#21512;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25688;&#35201;&#20219;&#21153;&#30340;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#20063;&#20250;&#23548;&#33268;&#36755;&#20986;&#26041;&#24046;&#22686;&#21152;&#65292;&#21363;&#20351;&#25552;&#31034;&#24847;&#20041;&#30456;&#20284;&#65292;&#36755;&#20986;&#20063;&#20250;&#26377;&#26126;&#26174;&#30340;&#24046;&#24322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#36719;&#25552;&#31034;&#26657;&#20934;&#65288;SPeC&#65289;&#27969;&#31243;&#65292;&#35813;&#27969;&#31243;&#37319;&#29992;&#36719;&#25552;&#31034;&#23884;&#20837;&#26469;&#20943;&#36731;&#36755;&#20837;&#21464;&#37327;&#23545;&#36755;&#20986;&#22810;&#26679;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;SPeC&#19981;&#20165;&#21487;&#20197;&#38477;&#20302;LLM&#30340;&#24615;&#33021;&#21464;&#24322;&#65292;&#32780;&#19988;&#22312;&#20020;&#24202;&#31508;&#35760;&#25688;&#35201;&#20219;&#21153;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electronic health records (EHRs) store an extensive array of patient information, encompassing medical histories, diagnoses, treatments, and test outcomes. These records are crucial for enabling healthcare providers to make well-informed decisions regarding patient care. Summarizing clinical notes further assists healthcare professionals in pinpointing potential health risks and making better-informed decisions. This process contributes to reducing errors and enhancing patient outcomes by ensuring providers have access to the most pertinent and current patient data. Recent research has shown that incorporating prompts with large language models (LLMs) substantially boosts the efficacy of summarization tasks. However, we show that this approach also leads to increased output variance, resulting in notably divergent outputs even when prompts share similar meanings. To tackle this challenge, we introduce a model-agnostic Soft Prompt-Based Calibration (SPeC) pipeline that employs soft prom
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#22522;&#20110;ChatGPT&#27169;&#22411;&#30340;&#38382;&#31572;&#31995;&#32479;&#22312;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#19968;&#20010;&#20998;&#31867;&#26694;&#26550;&#23545;&#28508;&#22312;&#30340;&#38382;&#39064;&#29305;&#24449;&#36827;&#34892;&#20998;&#31867;&#65292;&#36890;&#36807;&#40657;&#30418;&#27979;&#35797;&#35268;&#33539;CheckList&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.07992</link><description>&lt;p&gt;
&#35780;&#20272; ChatGPT &#20316;&#20026;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#30340;&#38382;&#31572;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Evaluation of ChatGPT as a Question Answering System for Answering Complex Questions. (arXiv:2303.07992v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#22522;&#20110;ChatGPT&#27169;&#22411;&#30340;&#38382;&#31572;&#31995;&#32479;&#22312;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#19968;&#20010;&#20998;&#31867;&#26694;&#26550;&#23545;&#28508;&#22312;&#30340;&#38382;&#39064;&#29305;&#24449;&#36827;&#34892;&#20998;&#31867;&#65292;&#36890;&#36807;&#40657;&#30418;&#27979;&#35797;&#35268;&#33539;CheckList&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT &#26159;&#19968;&#20010;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24050;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#35813;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#23616;&#38480;&#24615;&#20173;&#38656;&#35201;&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#12290;&#30001;&#20110; ChatGPT &#35206;&#30422;&#32500;&#22522;&#30334;&#31185;&#31561;&#36164;&#28304;&#24182;&#25903;&#25345;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#65292;&#22240;&#27492;&#23427;&#24341;&#36215;&#20102;&#20316;&#20026;&#20256;&#32479;&#30693;&#35782;&#24211;&#38382;&#31572;&#65288;KBQA&#65289;&#27169;&#22411;&#26367;&#20195;&#21697;&#30340;&#20851;&#27880;&#12290;&#22797;&#26434;&#38382;&#39064;&#22238;&#31572;&#26159; KBQA &#30340;&#19968;&#39033;&#25361;&#25112;&#24615;&#20219;&#21153;&#65292;&#20840;&#38754;&#27979;&#35797;&#20102;&#27169;&#22411;&#22312;&#35821;&#20041;&#35299;&#26512;&#21644;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35780;&#20272; ChatGPT &#20316;&#20026;&#19968;&#20010;&#20351;&#29992;&#33258;&#24049;&#30693;&#35782;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#30340;&#38382;&#31572;&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#35780;&#20272;&#20854;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#23545;&#22797;&#26434;&#38382;&#39064;&#30340;&#28508;&#22312;&#29305;&#24449;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#20351;&#29992;&#22810;&#20010;&#26631;&#31614;&#25551;&#36848;&#27599;&#20010;&#27979;&#35797;&#38382;&#39064;&#65292;&#20197;&#35782;&#21035;&#32452;&#21512;&#25512;&#29702;&#12290;&#26681;&#25454; Ribeir &#25552;&#20986;&#30340; CheckList &#30340;&#40657;&#30418;&#27979;&#35797;&#35268;&#33539;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;ChatGPT&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is a powerful large language model (LLM) that has made remarkable progress in natural language understanding. Nevertheless, the performance and limitations of the model still need to be extensively evaluated. As ChatGPT covers resources such as Wikipedia and supports natural language question answering, it has garnered attention as a potential replacement for traditional knowledge based question answering (KBQA) models. Complex question answering is a challenge task of KBQA, which comprehensively tests the ability of models in semantic parsing and reasoning. To assess the performance of ChatGPT as a question answering system (QAS) using its own knowledge, we present a framework that evaluates its ability to answer complex questions. Our approach involves categorizing the potential features of complex questions and describing each test question with multiple labels to identify combinatorial reasoning. Following the black-box testing specifications of CheckList proposed by Ribeir
&lt;/p&gt;</description></item><item><title>GPT-3&#22312;&#35768;&#22810;&#31867;&#27604;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#19982;&#29978;&#33267;&#36229;&#36234;&#20154;&#31867;&#30340;&#33021;&#21147;&#65292;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32039;&#24613;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.09196</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31867;&#27604;&#25512;&#29702;&#30340;&#32039;&#24613;&#24615;
&lt;/p&gt;
&lt;p&gt;
Emergent Analogical Reasoning in Large Language Models. (arXiv:2212.09196v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09196
&lt;/p&gt;
&lt;p&gt;
GPT-3&#22312;&#35768;&#22810;&#31867;&#27604;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#19982;&#29978;&#33267;&#36229;&#36234;&#20154;&#31867;&#30340;&#33021;&#21147;&#65292;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32039;&#24613;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#37325;&#26032;&#28857;&#29123;&#20102;&#20154;&#20204;&#23545;&#20110;&#36825;&#26679;&#19968;&#31181;&#38382;&#39064;&#30340;&#36777;&#35770;&#65306;&#36275;&#22815;&#30340;&#35757;&#32451;&#25968;&#25454;&#26159;&#21542;&#33021;&#20351;&#36825;&#20123;&#36890;&#29992;&#27169;&#22411;&#20869;&#28085;&#20154;&#31867;&#35748;&#30693;&#33021;&#21147;&#12290;&#29305;&#21035;&#30340;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#25512;&#29702;&#33021;&#21147;&#8212;&#8212;&#19981;&#32463;&#36807;&#20219;&#20309;&#30452;&#25509;&#35757;&#32451;&#65292;&#23601;&#33021;&#22815;&#25512;&#29702;&#20986;&#26032;&#38382;&#39064;&#65292;&#29305;&#21035;&#20196;&#20154;&#20851;&#27880;&#12290;&#22312;&#20154;&#31867;&#35748;&#30693;&#20013;&#65292;&#36825;&#31181;&#33021;&#21147;&#19982;&#19968;&#31181;&#36890;&#36807;&#31867;&#27604;&#25512;&#29702;&#30340;&#33021;&#21147;&#23494;&#20999;&#30456;&#20851;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#31867;&#27604;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#30452;&#25509;&#30340;&#20154;&#26426;&#27604;&#36739;&#65292;&#21253;&#25324;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#30697;&#38453;&#25512;&#29702;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#19982; Raven's Progressive Matrices&#23494;&#20999;&#30456;&#20851;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;GPT-3&#21576;&#29616;&#20986;&#20102;&#19968;&#31181;&#20196;&#20154;&#24778;&#35766;&#30340;&#25277;&#35937;&#27169;&#24335;&#24402;&#32435;&#33021;&#21147;&#65292;&#29978;&#33267;&#22312;&#22823;&#37096;&#20998;&#24773;&#20917;&#19979;&#19982;&#25110;&#29978;&#33267;&#36229;&#36234;&#20102;&#20154;&#31867;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20687;GPT-3&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#33719;&#24471;&#20102;&#22312;&#24191;&#27867;&#30340;&#31867;&#27604;&#38382;&#39064;&#19978;&#25214;&#21040;&#38646;&#26679;&#26412;&#35299;&#20915;&#26041;&#26696;&#30340;&#32039;&#24613;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advent of large language models has reinvigorated debate over whether human cognitive capacities might emerge in such generic models given sufficient training data. Of particular interest is the ability of these models to reason about novel problems zero-shot, without any direct training. In human cognition, this capacity is closely tied to an ability to reason by analogy. Here, we performed a direct comparison between human reasoners and a large language model (the text-davinci-003 variant of GPT-3) on a range of analogical tasks, including a novel text-based matrix reasoning task closely modeled on Raven's Progressive Matrices. We found that GPT-3 displayed a surprisingly strong capacity for abstract pattern induction, matching or even surpassing human capabilities in most settings. Our results indicate that large language models such as GPT-3 have acquired an emergent ability to find zero-shot solutions to a broad range of analogy problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26080;&#26631;&#31614;&#35821;&#26009;&#24211;&#21644;&#22522;&#20110;&#21477;&#23376;&#23884;&#20837;&#30340;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#26500;&#24314;WordNet&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#19968;&#20010;&#26032;&#30340;WordNet&#65288;FilWordNet&#65289;&#65292;&#20197;&#26367;&#20195;&#24182;&#25913;&#36827;&#33778;&#24459;&#23486;&#35821;&#20013;&#36807;&#26102;&#30340;WordNet&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20154;&#24037;&#30417;&#30563;&#12290;</title><link>http://arxiv.org/abs/2204.03251</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#21477;&#23376;&#23884;&#20837;&#30340;&#35789;&#20041;&#24402;&#32435;&#33258;&#21160;&#26500;&#24314;WordNet
&lt;/p&gt;
&lt;p&gt;
Automatic WordNet Construction using Word Sense Induction through Sentence Embeddings. (arXiv:2204.03251v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.03251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26080;&#26631;&#31614;&#35821;&#26009;&#24211;&#21644;&#22522;&#20110;&#21477;&#23376;&#23884;&#20837;&#30340;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#26500;&#24314;WordNet&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#19968;&#20010;&#26032;&#30340;WordNet&#65288;FilWordNet&#65289;&#65292;&#20197;&#26367;&#20195;&#24182;&#25913;&#36827;&#33778;&#24459;&#23486;&#35821;&#20013;&#36807;&#26102;&#30340;WordNet&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20154;&#24037;&#30417;&#30563;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#36164;&#28304;&#22914;WordNet&#23545;&#20110;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#21644;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#65288;&#22914;&#33778;&#24459;&#23486;&#35821;&#65289;&#65292;&#29616;&#26377;&#30340;WordNet&#36807;&#26102;&#19988;&#19981;&#23436;&#25972;&#65292;&#24182;&#19988;&#29983;&#25104;&#26032;&#30340;WordNet&#21487;&#33021;&#38656;&#35201;&#22823;&#37327;&#30340;&#26102;&#38388;&#21644;&#36164;&#28304;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26080;&#26631;&#31614;&#35821;&#26009;&#24211;&#21644;&#22522;&#20110;&#21477;&#23376;&#23884;&#20837;&#30340;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#26500;&#24314;WordNet&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#29983;&#25104;&#20102;&#19968;&#20010;&#26032;&#30340;WordNet&#65288;FilWordNet&#65289;&#65292;&#20197;&#26367;&#20195;&#24182;&#25913;&#36827;&#33778;&#24459;&#23486;&#35821;&#20013;&#36807;&#26102;&#30340;WordNet&#12290;&#36890;&#36807;&#23558;&#25105;&#20204;&#33258;&#21160;&#35825;&#23548;&#20986;&#30340;&#35789;&#20041;&#21644;&#35789;&#27719;&#38598;&#19982;Princeton WordNet&#20013;&#30340;&#35789;&#20041;&#36827;&#34892;&#21305;&#37197;&#65292;&#20197;&#21450;&#23558;&#35789;&#27719;&#38598;&#19982;&#26087;&#30340;&#33778;&#24459;&#23486;&#35821;WordNet&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#23545;&#20854;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#32463;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#33258;&#21160;&#35825;&#23548;&#29616;&#26377;&#30340;&#35789;&#20041;&#21644;&#35789;&#27719;&#38598;&#65292;&#20063;&#21487;&#20197;&#28508;&#22312;&#22320;&#33258;&#21160;&#35825;&#23548;&#26032;&#30340;&#35789;&#20041;&#21644;&#35789;&#27719;&#38598;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20154;&#24037;&#30417;&#30563;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language resources such as wordnets remain indispensable tools for different natural language tasks and applications. However, for low-resource languages such as Filipino, existing wordnets are old and outdated, and producing new ones may be slow and costly in terms of time and resources. In this paper, we propose an automatic method for constructing a wordnet from scratch using only an unlabeled corpus and a sentence embeddings-based language model. Using this, we produce FilWordNet, a new wordnet that supplants and improves the outdated Filipino WordNet. We evaluate our automatically-induced senses and synsets by matching them with senses from the Princeton WordNet, as well as comparing the synsets to the old Filipino WordNet. We empirically show that our method can induce existing, as well as potentially new, senses and synsets automatically without the need for human supervision.
&lt;/p&gt;</description></item></channel></rss>