<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#23558;&#24418;&#24577;&#32032;&#20998;&#21106;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#24207;&#21015;&#21040;&#24207;&#21015;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#22810;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20986;&#20248;&#24322;&#24615;&#33021;&#65292;&#25581;&#31034;&#20102;&#39640;&#36164;&#28304;&#35821;&#35328;&#29615;&#22659;&#19979;&#30340;&#21487;&#27604;&#25928;&#21147;&#65292;&#20197;&#21450;&#20302;&#36164;&#28304;&#35821;&#35328;&#22330;&#26223;&#19979;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15436</link><description>&lt;p&gt;
&#21033;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#36827;&#34892;&#21477;&#23376;&#32423;&#24418;&#24577;&#32032;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Using Contextual Information for Sentence-level Morpheme Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15436
&lt;/p&gt;
&lt;p&gt;
&#23558;&#24418;&#24577;&#32032;&#20998;&#21106;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#24207;&#21015;&#21040;&#24207;&#21015;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#22810;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20986;&#20248;&#24322;&#24615;&#33021;&#65292;&#25581;&#31034;&#20102;&#39640;&#36164;&#28304;&#35821;&#35328;&#29615;&#22659;&#19979;&#30340;&#21487;&#27604;&#25928;&#21147;&#65292;&#20197;&#21450;&#20302;&#36164;&#28304;&#35821;&#35328;&#22330;&#26223;&#19979;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#24418;&#24577;&#32032;&#20998;&#21106;&#30340;&#21457;&#23637;&#20027;&#35201;&#24378;&#35843;&#21333;&#35789;&#32423;&#21035;&#30340;&#20998;&#21106;&#65292;&#36890;&#24120;&#24573;&#35270;&#20102;&#21477;&#23376;&#20869;&#30340;&#19978;&#19979;&#25991;&#30456;&#20851;&#24615;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#24418;&#24577;&#32032;&#20998;&#21106;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#19968;&#20010;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#38382;&#39064;&#65292;&#23558;&#25972;&#20010;&#21477;&#23376;&#20316;&#20026;&#36755;&#20837;&#65292;&#32780;&#19981;&#26159;&#23396;&#31435;&#22320;&#22788;&#29702;&#21333;&#20010;&#21333;&#35789;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22810;&#35821;&#35328;&#27169;&#22411;&#19982;&#21333;&#35821;&#27169;&#22411;&#30456;&#27604;&#22987;&#32456;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;&#34429;&#28982;&#25105;&#20204;&#30340;&#27169;&#22411;&#27809;&#26377;&#36229;&#36234;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#20294;&#22312;&#39640;&#36164;&#28304;&#35821;&#35328;&#20013;&#23637;&#29616;&#20986;&#21487;&#27604;&#36739;&#30340;&#26377;&#25928;&#24615;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#22330;&#26223;&#20013;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15436v1 Announce Type: new  Abstract: Recent advancements in morpheme segmentation primarily emphasize word-level segmentation, often neglecting the contextual relevance within the sentence. In this study, we redefine the morpheme segmentation task as a sequence-to-sequence problem, treating the entire sentence as input rather than isolating individual words. Our findings reveal that the multilingual model consistently exhibits superior performance compared to monolingual counterparts. While our model did not surpass the performance of the current state-of-the-art, it demonstrated comparable efficacy with high-resource languages while revealing limitations in low-resource language scenarios.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24320;&#21457;&#21452;&#35821;&#32763;&#35793;&#27169;&#22411;&#65292;&#25506;&#32034;&#21488;&#28286;&#31119;&#24314;&#35805;&#21644;&#32321;&#20307;&#20013;&#25991;/&#33521;&#25991;&#20043;&#38388;&#30340;&#32763;&#35793;&#65292;&#24341;&#20837;&#38480;&#21046;&#21333;&#35821;&#35821;&#26009;&#24211;&#24182;&#23558;&#25152;&#26377;&#21488;&#28286;&#31119;&#24314;&#35805;&#25991;&#23383;&#31995;&#32479;&#35268;&#33539;&#20026;&#31119;&#24314;&#35805;&#27721;&#23383;&#65292;&#20174;&#32780;&#25552;&#39640;&#32763;&#35793;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.12024</link><description>&lt;p&gt;
&#25506;&#32034;&#21644;&#35268;&#33539;&#22235;&#31181;&#25991;&#23383;&#31995;&#32479;&#65292;&#22686;&#24378;&#31119;&#24314;&#35805;&#30340;&#21452;&#35821;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Enhancing Hokkien Dual Translation by Exploring and Standardizing of Four Writing Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12024
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24320;&#21457;&#21452;&#35821;&#32763;&#35793;&#27169;&#22411;&#65292;&#25506;&#32034;&#21488;&#28286;&#31119;&#24314;&#35805;&#21644;&#32321;&#20307;&#20013;&#25991;/&#33521;&#25991;&#20043;&#38388;&#30340;&#32763;&#35793;&#65292;&#24341;&#20837;&#38480;&#21046;&#21333;&#35821;&#35821;&#26009;&#24211;&#24182;&#23558;&#25152;&#26377;&#21488;&#28286;&#31119;&#24314;&#35805;&#25991;&#23383;&#31995;&#32479;&#35268;&#33539;&#20026;&#31119;&#24314;&#35805;&#27721;&#23383;&#65292;&#20174;&#32780;&#25552;&#39640;&#32763;&#35793;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12024v1 &#20844;&#21578;&#31867;&#22411;: &#26032; &#25552;&#35201;: &#26426;&#22120;&#32763;&#35793;&#20027;&#35201;&#38598;&#20013;&#22312;&#39640;&#36164;&#28304;&#35821;&#35328;&#65288;HRLs&#65289;&#65292;&#32780;&#20302;&#36164;&#28304;&#35821;&#35328;&#65288;LRLs&#65289;&#22914;&#21488;&#28286;&#31119;&#24314;&#35805;&#30456;&#23545;&#26410;&#34987;&#28145;&#20837;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#24320;&#21457;&#21488;&#28286;&#31119;&#24314;&#35805;&#19982;&#32321;&#20307;&#20013;&#25991;&#21644;&#33521;&#25991;&#20043;&#38388;&#30340;&#21452;&#35821;&#32763;&#35793;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#19968;&#24046;&#36317;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#32321;&#20307;&#20013;&#25991;&#30340;&#39044;&#35757;&#32451;LLaMA2-7B&#27169;&#22411;&#26469;&#21033;&#29992;&#21488;&#28286;&#31119;&#24314;&#35805;&#27721;&#23383;&#19982;&#32321;&#20307;&#20013;&#25991;&#20043;&#38388;&#30340;&#25340;&#38899;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#30340;&#20840;&#38754;&#23454;&#39564;&#28041;&#21450;&#21488;&#28286;&#31119;&#24314;&#35805;&#21508;&#31181;&#25991;&#23383;&#31995;&#32479;&#20043;&#38388;&#30340;&#32763;&#35793;&#20219;&#21153;&#65292;&#20197;&#21450;&#21488;&#28286;&#31119;&#24314;&#35805;&#19982;&#20854;&#20182;&#39640;&#36164;&#28304;&#35821;&#35328;&#20043;&#38388;&#30340;&#32763;&#35793;&#12290;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#26377;&#38480;&#30340;&#21333;&#35821;&#35821;&#26009;&#24211;&#36824;&#36827;&#19968;&#27493;&#25913;&#21892;&#20102;&#27169;&#22411;&#23545;&#21488;&#28286;&#31119;&#24314;&#35805;&#30340;&#33021;&#21147;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#25105;&#20204;&#30340;&#32763;&#35793;&#27169;&#22411;&#23558;&#25152;&#26377;&#21488;&#28286;&#31119;&#24314;&#35805;&#25991;&#23383;&#31995;&#32479;&#35268;&#33539;&#20026;&#31119;&#24314;&#35805;&#27721;&#23383;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12024v1 Announce Type: new  Abstract: Machine translation focuses mainly on high-resource languages (HRLs), while low-resource languages (LRLs) like Taiwanese Hokkien are relatively under-explored. This study aims to address this gap by developing a dual translation model between Taiwanese Hokkien and both Traditional Mandarin Chinese and English. We employ a pre-trained LLaMA2-7B model specialized in Traditional Mandarin Chinese to leverage the orthographic similarities between Taiwanese Hokkien Han and Traditional Mandarin Chinese. Our comprehensive experiments involve translation tasks across various writing systems of Taiwanese Hokkien and between Taiwanese Hokkien and other HRLs. We find that the use of a limited monolingual corpus also further improve the model's Taiwanese Hokkien capabilities. We then utilize our translation model to standardize all Taiwanese Hokkien writing systems into Hokkien Han, resulting in further performance improvements. Additionally, we intr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#21098;&#26525;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#27169;&#25311;&#27599;&#20010;&#23376;&#32467;&#26500;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#26681;&#25454;&#22810;&#23618;&#32467;&#26500;&#30340;&#32467;&#26524;&#33258;&#36866;&#24212;&#22320;&#34701;&#21512;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#30340;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2403.10799</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#36866;&#24212;&#20272;&#35745;&#34701;&#21512;&#39640;&#25928;&#21098;&#26525;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Efficient Pruning of Large Language Model with Adaptive Estimation Fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10799
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#21098;&#26525;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#27169;&#25311;&#27599;&#20010;&#23376;&#32467;&#26500;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#26681;&#25454;&#22810;&#23618;&#32467;&#26500;&#30340;&#32467;&#26524;&#33258;&#36866;&#24212;&#22320;&#34701;&#21512;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#25104;&#20026;&#35768;&#22810;&#29983;&#25104;&#24615;&#19979;&#28216;&#20219;&#21153;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#36825;&#23548;&#33268;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#39640;&#25928;&#37096;&#32626;&#23427;&#20204;&#25104;&#20026;&#19981;&#21487;&#36991;&#20813;&#30340;&#36235;&#21183;&#21644;&#37325;&#22823;&#25361;&#25112;&#12290;&#32467;&#26500;&#21270;&#21098;&#26525;&#26159;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#24191;&#27867;&#24212;&#29992;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#22788;&#29702;&#22810;&#20010;&#35299;&#30721;&#22120;&#23618;&#30340;&#22797;&#26434;&#32467;&#26500;&#26102;&#65292;&#36890;&#24120;&#30340;&#26041;&#27861;&#24448;&#24448;&#37319;&#29992;&#24120;&#35265;&#30340;&#20272;&#35745;&#26041;&#27861;&#36827;&#34892;&#21098;&#26525;&#12290;&#36825;&#20123;&#26041;&#27861;&#23548;&#33268;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#31934;&#24230;&#19979;&#38477;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#33258;&#36866;&#24212;&#22320;&#27169;&#25311;&#27599;&#20010;&#23376;&#32467;&#26500;&#30340;&#37325;&#35201;&#24615;&#12290;&#21516;&#26102;&#65292;&#23427;&#21487;&#20197;&#22522;&#20110;&#22797;&#26434;&#21644;&#22810;&#23618;&#32467;&#26500;&#30340;&#32467;&#26524;&#65292;&#33258;&#36866;&#24212;&#22320;&#34701;&#21512;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#30340;&#20272;&#35745;&#12290;&#25105;&#20204;&#35774;&#35745;&#30340;&#25152;&#26377;&#26041;&#38754;&#37117;&#26080;&#32541;&#38598;&#25104;&#21040;&#31471;&#21040;&#31471;&#30340;&#21098;&#26525;&#26694;&#26550;&#20013;&#12290;&#19982;&#20027;&#27969;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10799v1 Announce Type: cross  Abstract: Large language models (LLMs) have become crucial for many generative downstream tasks, leading to an inevitable trend and significant challenge to deploy them efficiently on resource-constrained devices. Structured pruning is a widely used method to address this challenge. However, when dealing with the complex structure of the multiple decoder layers, general methods often employ common estimation approaches for pruning. These approaches lead to a decline in accuracy for specific downstream tasks. In this paper, we introduce a simple yet efficient method that adaptively models the importance of each substructure. Meanwhile, it can adaptively fuse coarse-grained and finegrained estimations based on the results from complex and multilayer structures. All aspects of our design seamlessly integrate into the endto-end pruning framework. Our experimental results, compared with state-of-the-art methods on mainstream datasets, demonstrate ave
&lt;/p&gt;</description></item><item><title>VidProM&#26159;&#19968;&#20010;&#21253;&#21547;167&#19975;&#20010;&#29420;&#29305;&#25991;&#26412;&#21040;&#35270;&#39057;&#25552;&#31034;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#23545;&#20110;&#25991;&#26412;&#21040;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#24102;&#26469;&#20102;&#26032;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#25581;&#31034;&#20102;&#30495;&#23454;&#29992;&#25143;&#25552;&#31034;&#23545;&#35270;&#39057;&#29983;&#25104;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.06098</link><description>&lt;p&gt;
VidProM&#65306;&#19968;&#20010;&#30334;&#19975;&#35268;&#27169;&#30340;&#30495;&#23454;&#21363;&#26102;&#22270;&#24211;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25991;&#26412;&#21040;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
VidProM: A Million-scale Real Prompt-Gallery Dataset for Text-to-Video Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06098
&lt;/p&gt;
&lt;p&gt;
VidProM&#26159;&#19968;&#20010;&#21253;&#21547;167&#19975;&#20010;&#29420;&#29305;&#25991;&#26412;&#21040;&#35270;&#39057;&#25552;&#31034;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#23545;&#20110;&#25991;&#26412;&#21040;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#24102;&#26469;&#20102;&#26032;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#25581;&#31034;&#20102;&#30495;&#23454;&#29992;&#25143;&#25552;&#31034;&#23545;&#35270;&#39057;&#29983;&#25104;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Sora&#30340;&#21040;&#26469;&#26631;&#24535;&#30528;&#25991;&#26412;&#21040;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#26102;&#20195;&#30340;&#21040;&#26469;&#65292;&#24102;&#26469;&#20102;&#35270;&#39057;&#29983;&#25104;&#21644;&#28508;&#22312;&#24212;&#29992;&#26041;&#38754;&#30340;&#26174;&#33879;&#36827;&#27493;&#12290;&#28982;&#32780;&#65292;Sora&#20197;&#21450;&#20854;&#20182;&#25991;&#26412;&#21040;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#39640;&#24230;&#20381;&#36182;&#25552;&#31034;&#65292;&#20294;&#30446;&#21069;&#23578;&#27809;&#26377;&#20844;&#24320;&#21487;&#29992;&#30340;&#21253;&#21547;&#25991;&#26412;&#21040;&#35270;&#39057;&#25552;&#31034;&#30740;&#31350;&#30340;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;VidProM&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#30001;167&#19975;&#20010;&#26469;&#33258;&#30495;&#23454;&#29992;&#25143;&#30340;&#29420;&#29305;&#25991;&#26412;&#21040;&#35270;&#39057;&#25552;&#31034;&#32452;&#25104;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#30001;&#22235;&#31181;&#26368;&#20808;&#36827;&#30340;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;669&#19975;&#20010;&#35270;&#39057;&#20197;&#21450;&#19968;&#20123;&#30456;&#20851;&#25968;&#25454;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#36825;&#19968;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#31574;&#23637;&#36807;&#31243;&#65292;&#36825;&#26159;&#19968;&#20010;&#32791;&#26102;&#19988;&#26114;&#36149;&#30340;&#36807;&#31243;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;VidProM&#19982;DiffusionDB&#20043;&#38388;&#30340;&#21306;&#21035;&#65292;&#21518;&#32773;&#26159;&#19968;&#20010;&#29992;&#20110;&#22270;&#20687;&#29983;&#25104;&#30340;&#22823;&#35268;&#27169;&#25552;&#31034;&#22270;&#24211;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#23545;&#36825;&#20123;&#25552;&#31034;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#20010;&#19987;&#38376;&#30340;&#26032;&#25552;&#31034;&#25968;&#25454;&#38598;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06098v1 Announce Type: cross  Abstract: The arrival of Sora marks a new era for text-to-video diffusion models, bringing significant advancements in video generation and potential applications. However, Sora, as well as other text-to-video diffusion models, highly relies on the prompts, and there is no publicly available dataset featuring a study of text-to-video prompts. In this paper, we introduce VidProM, the first large-scale dataset comprising 1.67 million unique text-to-video prompts from real users. Additionally, the dataset includes 6.69 million videos generated by four state-of-the-art diffusion models and some related data. We initially demonstrate the curation of this large-scale dataset, which is a time-consuming and costly process. Subsequently, we show how the proposed VidProM differs from DiffusionDB, a large-scale prompt-gallery dataset for image generation. Based on the analysis of these prompts, we identify the necessity for a new prompt dataset specificall
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;LaGTran&#26694;&#26550;&#65292;&#21033;&#29992;&#25991;&#26412;&#25551;&#36848;&#26469;&#24341;&#23548;&#30693;&#35782;&#36716;&#31227;&#65292;&#22312;&#22788;&#29702;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.05535</link><description>&lt;p&gt;
&#35762;&#36848;&#65292;&#32780;&#19981;&#26159;&#23637;&#31034;&#65281;&#65306;&#35821;&#35328;&#25351;&#23548;&#26377;&#21161;&#20110;&#22312;&#22270;&#20687;&#21644;&#35270;&#39057;&#39046;&#22495;&#20043;&#38388;&#36827;&#34892;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Tell, Don't Show!: Language Guidance Eases Transfer Across Domains in Images and Videos
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05535
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;LaGTran&#26694;&#26550;&#65292;&#21033;&#29992;&#25991;&#26412;&#25551;&#36848;&#26469;&#24341;&#23548;&#30693;&#35782;&#36716;&#31227;&#65292;&#22312;&#22788;&#29702;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;LaGTran&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#21363;&#21487;&#33719;&#24471;&#25110;&#26131;&#20110;&#33719;&#21462;&#30340;&#25991;&#26412;&#25551;&#36848;&#65292;&#24341;&#23548;&#20174;&#24102;&#26631;&#31614;&#30340;&#28304;&#25968;&#25454;&#21040;&#20855;&#26377;&#22495;&#20559;&#31227;&#30340;&#26080;&#26631;&#31614;&#30446;&#26631;&#25968;&#25454;&#30340;&#40065;&#26834;&#24615;&#30693;&#35782;&#36716;&#31227;&#12290;&#21463;&#21040;&#25105;&#20204;&#35266;&#23519;&#21040;&#26356;&#23500;&#35821;&#20041;&#30340;&#25991;&#26412;&#27169;&#24577;&#20855;&#26377;&#26356;&#26377;&#21033;&#30340;&#36716;&#31227;&#29305;&#24615;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#36716;&#31227;&#26426;&#21046;&#65292;&#20351;&#29992;&#28304;&#35757;&#32451;&#30340;&#25991;&#26412;&#20998;&#31867;&#22120;&#22312;&#30446;&#26631;&#25991;&#26412;&#25551;&#36848;&#19978;&#29983;&#25104;&#39044;&#27979;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#39044;&#27979;&#20316;&#20026;&#30456;&#24212;&#22270;&#20687;&#30340;&#30417;&#30563;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20197;&#35821;&#35328;&#25351;&#23548;&#20026;&#39537;&#21160;&#65292;&#20986;&#22855;&#22320;&#31616;&#21333;&#26131;&#34892;&#65292;&#21364;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#22914;GeoNet&#21644;DomainNet&#19978;&#26174;&#33879;&#20248;&#20110;&#20197;&#24448;&#25152;&#26377;&#26041;&#27861;&#65292;&#39564;&#35777;&#20102;&#20854;&#26497;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05535v1 Announce Type: cross  Abstract: We introduce LaGTran, a novel framework that utilizes readily available or easily acquired text descriptions to guide robust transfer of discriminative knowledge from labeled source to unlabeled target data with domain shifts. While unsupervised adaptation methods have been established to address this problem, they show limitations in handling challenging domain shifts due to their exclusive operation within the pixel-space. Motivated by our observation that semantically richer text modality has more favorable transfer properties, we devise a transfer mechanism to use a source-trained text-classifier to generate predictions on the target text descriptions, and utilize these predictions as supervision for the corresponding images. Our approach driven by language guidance is surprisingly easy and simple, yet significantly outperforms all prior approaches on challenging datasets like GeoNet and DomainNet, validating its extreme effectiven
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;ParallelPARC&#27969;&#27700;&#32447;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22797;&#26434;&#27573;&#33853;&#31867;&#27604;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#21508;&#31181;&#31867;&#27604;&#31867;&#22411;&#65292;&#24182;&#23637;&#31034;&#20986;&#20154;&#31867;&#22312;&#31867;&#27604;&#35782;&#21035;&#20013;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.01139</link><description>&lt;p&gt;
ParallelPARC: &#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#31867;&#27604;&#30340;&#21487;&#25193;&#23637;&#27969;&#27700;&#32447;
&lt;/p&gt;
&lt;p&gt;
ParallelPARC: A Scalable Pipeline for Generating Natural-Language Analogies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01139
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;ParallelPARC&#27969;&#27700;&#32447;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22797;&#26434;&#27573;&#33853;&#31867;&#27604;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#21508;&#31181;&#31867;&#27604;&#31867;&#22411;&#65292;&#24182;&#23637;&#31034;&#20986;&#20154;&#31867;&#22312;&#31867;&#27604;&#35782;&#21035;&#20013;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Analogy-making&#23545;&#20110;&#20154;&#31867;&#35748;&#30693;&#33267;&#20851;&#37325;&#35201;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#36866;&#24212;&#26032;&#39062;&#24773;&#22659;--&#36825;&#26159;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20173;&#28982;&#32570;&#20047;&#30340;&#33021;&#21147;&#12290;&#22823;&#22810;&#25968;&#31867;&#27604;&#25968;&#25454;&#38598;&#20170;&#22825;&#20851;&#27880;&#31616;&#21333;&#30340;&#31867;&#27604;&#65288;&#20363;&#22914;&#65292;&#35789;&#31867;&#27604;&#65289;&#65307;&#21253;&#21547;&#22797;&#26434;&#31867;&#22411;&#31867;&#27604;&#30340;&#25968;&#25454;&#38598;&#36890;&#24120;&#26159;&#25163;&#24037;&#31574;&#21010;&#30340;&#65292;&#24182;&#19988;&#38750;&#24120;&#23567;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#38480;&#21046;&#20102;&#35745;&#31639;&#31867;&#27604;&#30340;&#36827;&#23637;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#25968;&#25454;&#29983;&#25104;&#27969;&#27700;&#32447;&#65292;ParallelPARC&#65288;Parallel Paragraph Creator&#65289;&#65292;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#21019;&#24314;&#22522;&#20110;&#27573;&#33853;&#30340;&#22797;&#26434;&#31867;&#27604;&#65292;&#20197;&#21450;&#31616;&#21333;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24178;&#25200;&#39033;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27969;&#27700;&#32447;&#65292;&#24182;&#21019;&#24314;&#20102;ProPara-Logy&#65292;&#19968;&#20010;&#20851;&#20110;&#31185;&#23398;&#36807;&#31243;&#38388;&#31867;&#27604;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#30001;&#20154;&#31867;&#39564;&#35777;&#36807;&#30340;&#37329;&#26631;&#20934;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19968;&#20010;&#33258;&#21160;&#29983;&#25104;&#30340;&#38134;&#26631;&#20934;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#22312;&#20108;&#36827;&#21046;&#21644;&#22810;&#36873;&#29615;&#22659;&#20013;&#27979;&#35797;&#20102;LLMs&#21644;&#20154;&#31867;&#23545;&#31867;&#27604;&#30340;&#35782;&#21035;&#65292;&#21457;&#29616;&#20154;&#31867;&#32988;&#36807;&#26368;&#20339;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01139v1 Announce Type: cross  Abstract: Analogy-making is central to human cognition, allowing us to adapt to novel situations -- an ability that current AI systems still lack. Most analogy datasets today focus on simple analogies (e.g., word analogies); datasets including complex types of analogies are typically manually curated and very small. We believe that this holds back progress in computational analogy. In this work, we design a data generation pipeline, ParallelPARC (Parallel Paragraph Creator) leveraging state-of-the-art Large Language Models (LLMs) to create complex, paragraph-based analogies, as well as distractors, both simple and challenging. We demonstrate our pipeline and create ProPara-Logy, a dataset of analogies between scientific processes. We publish a gold-set, validated by humans, and a silver-set, generated automatically. We test LLMs' and humans' analogy recognition in binary and multiple-choice settings, and found that humans outperform the best mod
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#31181;&#29992;&#20110;Llama 2 Chat 7B&#25110;&#26356;&#22823;&#27169;&#22411;&#30340;&#33609;&#26696;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#21152;&#36895;&#25512;&#29702;&#65292;&#20165;&#21344;&#21407;&#22987;&#22823;&#23567;&#30340;1.64&#65285;&#12290;</title><link>https://arxiv.org/abs/2403.00858</link><description>&lt;p&gt;
&#30452;&#25509;&#19982;Chat-Fine-Tuned LLMs&#30340;&#33609;&#26696;&#27169;&#22411;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Direct Alignment of Draft Model for Speculative Decoding with Chat-Fine-Tuned LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00858
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#31181;&#29992;&#20110;Llama 2 Chat 7B&#25110;&#26356;&#22823;&#27169;&#22411;&#30340;&#33609;&#26696;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#21152;&#36895;&#25512;&#29702;&#65292;&#20165;&#21344;&#21407;&#22987;&#22823;&#23567;&#30340;1.64&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#29983;&#25104;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30001;&#20110;&#20854;&#33258;&#22238;&#24402;&#26412;&#36136;&#12289;&#24040;&#22823;&#30340;&#21442;&#25968;&#25968;&#37327;&#21644;&#26377;&#38480;&#30340;&#20869;&#23384;&#24102;&#23485;&#32780;&#34987;&#35748;&#20026;&#26159;&#20869;&#23384;&#23494;&#38598;&#22411;&#65292;&#36890;&#24120;&#23548;&#33268;&#20302;&#20196;&#29260;&#36895;&#29575;&#12290;&#29468;&#27979;&#35299;&#30721;&#24050;&#34987;&#25552;&#20986;&#20316;&#20026;LLM&#25512;&#29702;&#21152;&#36895;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#20195;&#24320;&#28304;LLM&#31995;&#21015;&#20013;&#65292;&#20363;&#22914;Llama 2 7B&#65292;&#30001;&#20110;&#33609;&#26696;&#27169;&#22411;&#36890;&#24120;&#19981;&#21487;&#29992;&#65292;&#22240;&#27492;&#38656;&#35201;&#35757;&#32451;&#39640;&#36136;&#37327;&#30340;&#33609;&#26696;&#27169;&#22411;&#20197;&#36890;&#36807;&#29468;&#27979;&#35299;&#30721;&#23454;&#29616;&#25512;&#29702;&#21152;&#36895;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#33609;&#26696;&#27169;&#22411;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#30452;&#25509;&#19982;Chat-capable&#30446;&#26631;&#27169;&#22411;&#23545;&#40784;&#12290;&#36890;&#36807;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#35757;&#32451;&#20986;Llama 2 Chat Drafter 115M&#65292;&#36825;&#26159;&#19968;&#20010;&#36866;&#29992;&#20110;Llama 2 Chat 7B&#25110;&#26356;&#22823;&#27169;&#22411;&#30340;&#33609;&#26696;&#27169;&#22411;&#65292;&#20165;&#21344;&#21407;&#22987;&#22823;&#23567;&#30340;1.64&#65285;&#12290;&#25105;&#20204;&#30340;&#35757;&#32451;&#26694;&#26550;&#20165;&#21253;&#25324;&#39044;&#35757;&#32451;&#12289;&#33976;&#39311;&#25968;&#25454;&#38598;&#29983;&#25104;&#21644;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#36827;&#34892;&#24494;&#35843;&#65292;&#27809;&#26377;&#39069;&#22806;&#30340;&#23545;&#40784;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00858v1 Announce Type: cross  Abstract: Text generation with Large Language Models (LLMs) is known to be memory bound due to the combination of their auto-regressive nature, huge parameter counts, and limited memory bandwidths, often resulting in low token rates. Speculative decoding has been proposed as a solution for LLM inference acceleration. However, since draft models are often unavailable in the modern open-source LLM families, e.g., for Llama 2 7B, training a high-quality draft model is required to enable inference acceleration via speculative decoding. In this paper, we propose a simple draft model training framework for direct alignment to chat-capable target models. With the proposed framework, we train Llama 2 Chat Drafter 115M, a draft model for Llama 2 Chat 7B or larger, with only 1.64\% of the original size. Our training framework only consists of pretraining, distillation dataset generation, and finetuning with knowledge distillation, with no additional align
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;VSP-LLM&#26694;&#26550;&#65292;&#29992;&#20110;&#26368;&#22823;&#21270;&#19978;&#19979;&#25991;&#24314;&#27169;&#33021;&#21147;&#65292;&#23454;&#29616;&#35270;&#35273;&#35821;&#38899;&#35782;&#21035;&#21644;&#32763;&#35793;&#30340;&#22810;&#20219;&#21153;&#25191;&#34892;&#12290;</title><link>https://arxiv.org/abs/2402.15151</link><description>&lt;p&gt;
&#35270;&#35273;&#35821;&#38899;&#36935;&#35265;&#35821;&#35328;&#65306;VSP-LLM&#26694;&#26550;&#29992;&#20110;&#39640;&#25928;&#21644;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#35270;&#35273;&#35821;&#38899;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15151
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;VSP-LLM&#26694;&#26550;&#65292;&#29992;&#20110;&#26368;&#22823;&#21270;&#19978;&#19979;&#25991;&#24314;&#27169;&#33021;&#21147;&#65292;&#23454;&#29616;&#35270;&#35273;&#35821;&#38899;&#35782;&#21035;&#21644;&#32763;&#35793;&#30340;&#22810;&#20219;&#21153;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35270;&#35273;&#35821;&#38899;&#22788;&#29702;&#20013;&#65292;&#30001;&#20110;&#21767;&#37096;&#36816;&#21160;&#30340;&#27169;&#31946;&#24615;&#36136;&#65292;&#19978;&#19979;&#25991;&#24314;&#27169;&#33021;&#21147;&#26159;&#26368;&#37325;&#35201;&#30340;&#35201;&#27714;&#20043;&#19968;&#12290;&#20363;&#22914;&#65292;&#21516;&#38899;&#24322;&#20041;&#35789;&#65292;&#21363;&#20855;&#26377;&#30456;&#21516;&#21767;&#37096;&#36816;&#21160;&#20294;&#20135;&#29983;&#19981;&#21516;&#22768;&#38899;&#30340;&#21333;&#35789;&#65292;&#21487;&#20197;&#36890;&#36807;&#32771;&#34385;&#19978;&#19979;&#25991;&#26469;&#21306;&#20998;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#38598;&#25104;LLM&#30340;&#35270;&#35273;&#35821;&#38899;&#22788;&#29702;&#65288;VSP-LLM&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;LLM&#30340;&#24378;&#22823;&#33021;&#21147;&#26469;&#26368;&#22823;&#21270;&#19978;&#19979;&#25991;&#24314;&#27169;&#33021;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;VSP-LLM&#26088;&#22312;&#25191;&#34892;&#35270;&#35273;&#35821;&#38899;&#35782;&#21035;&#21644;&#32763;&#35793;&#30340;&#22810;&#20219;&#21153;&#65292;&#20854;&#20013;&#32473;&#23450;&#30340;&#25351;&#20196;&#25511;&#21046;&#20219;&#21153;&#31867;&#22411;&#12290;&#36890;&#36807;&#21033;&#29992;&#33258;&#30417;&#30563;&#35270;&#35273;&#35821;&#38899;&#27169;&#22411;&#65292;&#23558;&#36755;&#20837;&#35270;&#39057;&#26144;&#23556;&#21040;LLM&#30340;&#36755;&#20837;&#28508;&#22312;&#31354;&#38388;&#12290;&#38024;&#23545;&#36755;&#20837;&#24103;&#23384;&#22312;&#20887;&#20313;&#20449;&#24687;&#30340;&#20107;&#23454;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21435;&#37325;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#35270;&#35273;&#35821;&#38899;&#21333;&#20803;&#20943;&#23569;&#23884;&#20837;&#30340;&#35270;&#35273;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15151v1 Announce Type: cross  Abstract: In visual speech processing, context modeling capability is one of the most important requirements due to the ambiguous nature of lip movements. For example, homophenes, words that share identical lip movements but produce different sounds, can be distinguished by considering the context. In this paper, we propose a novel framework, namely Visual Speech Processing incorporated with LLMs (VSP-LLM), to maximize the context modeling ability by bringing the overwhelming power of LLMs. Specifically, VSP-LLM is designed to perform multi-tasks of visual speech recognition and translation, where the given instructions control the type of task. The input video is mapped to the input latent space of a LLM by employing a self-supervised visual speech model. Focused on the fact that there is redundant information in input frames, we propose a novel deduplication method that reduces the embedded visual features by employing visual speech units. Thr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#21313;&#20010;&#19981;&#21516;&#35821;&#35328;&#23478;&#26063;&#30340;&#22810;&#26679;&#25968;&#25454;&#38598;&#36827;&#34892;&#30740;&#31350;&#65292;&#39318;&#27425;&#22312;&#31995;&#32479;&#21457;&#32946;&#37325;&#24314;&#20013;&#27604;&#36739;&#20102;&#22522;&#20110;&#22768;&#38899;&#21644;&#22522;&#20110;&#21516;&#28304;&#30340;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22522;&#20110;&#35789;&#27719;&#21516;&#28304;&#30340;&#37325;&#24314;&#35889;&#31995;&#19982;&#30495;&#23454;&#35889;&#31995;&#24179;&#22343;&#26356;&#25509;&#36817;&#65292;&#25552;&#39640;&#20102;&#32422;&#19977;&#20998;&#20043;&#19968;&#12290;</title><link>https://arxiv.org/abs/2402.02807</link><description>&lt;p&gt;
&#22768;&#38899;&#23545;&#20110;&#31995;&#32479;&#21457;&#32946;&#37325;&#24314;&#21487;&#38752;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Sounds Sound for Phylogenetic Reconstruction?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#21313;&#20010;&#19981;&#21516;&#35821;&#35328;&#23478;&#26063;&#30340;&#22810;&#26679;&#25968;&#25454;&#38598;&#36827;&#34892;&#30740;&#31350;&#65292;&#39318;&#27425;&#22312;&#31995;&#32479;&#21457;&#32946;&#37325;&#24314;&#20013;&#27604;&#36739;&#20102;&#22522;&#20110;&#22768;&#38899;&#21644;&#22522;&#20110;&#21516;&#28304;&#30340;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22522;&#20110;&#35789;&#27719;&#21516;&#28304;&#30340;&#37325;&#24314;&#35889;&#31995;&#19982;&#30495;&#23454;&#35889;&#31995;&#24179;&#22343;&#26356;&#25509;&#36817;&#65292;&#25552;&#39640;&#20102;&#32422;&#19977;&#20998;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20256;&#32479;&#30340;&#35821;&#35328;&#36827;&#21270;&#30740;&#31350;&#20013;&#65292;&#23398;&#32773;&#20204;&#36890;&#24120;&#24378;&#35843;&#22768;&#38899;&#35268;&#24459;&#21644;&#23545;&#24212;&#20851;&#31995;&#23545;&#20110;&#35821;&#35328;&#23478;&#26063;&#35889;&#31995;&#25512;&#26029;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#35745;&#31639;&#26041;&#27861;&#24448;&#24448;&#27809;&#26377;&#20805;&#20998;&#32771;&#34385;&#21040;&#36825;&#19968;&#28508;&#21147;&#12290;&#22823;&#22810;&#25968;&#35745;&#31639;&#26041;&#27861;&#20173;&#28982;&#20381;&#36182;&#20110;&#35789;&#27719;&#21516;&#28304;&#20316;&#20026;&#35821;&#35328;&#23398;&#31995;&#32479;&#21457;&#32946;&#37325;&#24314;&#30340;&#20027;&#35201;&#25968;&#25454;&#26469;&#28304;&#65292;&#23613;&#31649;&#20063;&#26377;&#19968;&#20123;&#30740;&#31350;&#20013;&#30340;&#20316;&#32773;&#36190;&#36175;&#27604;&#36739;&#22768;&#38899;&#24207;&#21015;&#30340;&#22909;&#22788;&#12290;&#22522;&#20110;&#21313;&#20010;&#26469;&#33258;&#19981;&#21516;&#35821;&#35328;&#23478;&#26063;&#30340;&#22810;&#26679;&#25968;&#25454;&#38598;&#21644;&#29616;&#20195;&#33258;&#21160;&#21516;&#28304;&#21644;&#22768;&#38899;&#23545;&#24212;&#26816;&#27979;&#26041;&#27861;&#65292;&#25105;&#20204;&#39318;&#27425;&#27979;&#35797;&#20102;&#22522;&#20110;&#22768;&#38899;&#21644;&#22522;&#20110;&#21516;&#28304;&#30340;&#26041;&#27861;&#22312;&#31995;&#32479;&#21457;&#32946;&#37325;&#24314;&#20013;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#35789;&#27719;&#21516;&#28304;&#37325;&#24314;&#30340;&#35889;&#31995;&#22312;&#24191;&#20041;&#22235;&#20803;&#32452;&#36317;&#31163;&#19978;&#19982;&#30495;&#23454;&#35889;&#31995;&#24179;&#22343;&#26356;&#25509;&#36817;&#65292;&#25552;&#21319;&#20102;&#32422;&#19977;&#20998;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
In traditional studies on language evolution, scholars often emphasize the importance of sound laws and sound correspondences for phylogenetic inference of language family trees. However, to date, computational approaches have typically not taken this potential into account. Most computational studies still rely on lexical cognates as major data source for phylogenetic reconstruction in linguistics, although there do exist a few studies in which authors praise the benefits of comparing words at the level of sound sequences. Building on (a) ten diverse datasets from different language families, and (b) state-of-the-art methods for automated cognate and sound correspondence detection, we test, for the first time, the performance of sound-based versus cognate-based approaches to phylogenetic reconstruction. Our results show that phylogenies reconstructed from lexical cognates are topologically closer, by approximately one third with respect to the generalized quartet distance on average, 
&lt;/p&gt;</description></item><item><title>ChatGPT&#22312;&#36873;&#25321;&#31572;&#26696;&#26102;&#34920;&#29616;&#20986;&#21021;&#21360;&#35937;&#25928;&#24212;&#65292;&#21363;&#26356;&#20542;&#21521;&#20110;&#36873;&#25321;&#25552;&#31034;&#20013;&#36739;&#26089;&#20301;&#32622;&#30340;&#26631;&#31614;&#20316;&#20026;&#31572;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.13206</link><description>&lt;p&gt;
ChatGPT&#30340;&#21021;&#21360;&#35937;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Primacy Effect of ChatGPT. (arXiv:2310.13206v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13206
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#22312;&#36873;&#25321;&#31572;&#26696;&#26102;&#34920;&#29616;&#20986;&#21021;&#21360;&#35937;&#25928;&#24212;&#65292;&#21363;&#26356;&#20542;&#21521;&#20110;&#36873;&#25321;&#25552;&#31034;&#20013;&#36739;&#26089;&#20301;&#32622;&#30340;&#26631;&#31614;&#20316;&#20026;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;ChatGPT&#19978;&#36827;&#34892;&#23454;&#39564;&#21644;&#20998;&#26512;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;ChatGPT&#30340;&#21021;&#21360;&#35937;&#25928;&#24212;&#65292;&#21363;&#36873;&#25321;&#36739;&#26089;&#20301;&#32622;&#30340;&#26631;&#31614;&#20316;&#20026;&#31572;&#26696;&#30340;&#20542;&#21521;&#12290;&#25105;&#20204;&#21457;&#29616;&#65306;i&#65289;ChatGPT&#30340;&#20915;&#31574;&#23545;&#25552;&#31034;&#20013;&#26631;&#31614;&#30340;&#39034;&#24207;&#25935;&#24863;&#65307;ii&#65289;ChatGPT&#26356;&#20542;&#21521;&#20110;&#36873;&#25321;&#36739;&#26089;&#20301;&#32622;&#30340;&#26631;&#31614;&#20316;&#20026;&#31572;&#26696;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#23454;&#39564;&#21644;&#20998;&#26512;&#33021;&#20026;&#26500;&#24314;&#26356;&#21487;&#38752;&#30340;&#22522;&#20110;ChatGPT&#30340;&#35299;&#20915;&#26041;&#26696;&#25552;&#20379;&#39069;&#22806;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction-tuned large language models (LLMs), such as ChatGPT, have led to promising zero-shot performance in discriminative natural language understanding (NLU) tasks. This involves querying the LLM using a prompt containing the question, and the candidate labels to choose from. The question-answering capabilities of ChatGPT arise from its pre-training on large amounts of human-written text, as well as its subsequent fine-tuning on human preferences, which motivates us to ask: Does ChatGPT also inherits humans' cognitive biases? In this paper, we study the primacy effect of ChatGPT: the tendency of selecting the labels at earlier positions as the answer. We have two main findings: i) ChatGPT's decision is sensitive to the order of labels in the prompt; ii) ChatGPT has a clearly higher chance to select the labels at earlier positions as the answer. We hope that our experiments and analyses provide additional insights into building more reliable ChatGPT-based solutions. We release the
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411; (VLMs) &#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#26469;&#36991;&#20813;&#35775;&#38382;&#27169;&#22411;&#21442;&#25968;&#65292;&#37319;&#29992;&#32842;&#22825;&#24335;&#30340;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#40657;&#30418;&#20248;&#21270;&#22120;&#65292;&#22312;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#36798;&#21040;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.05950</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#40657;&#30418;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Language Models as Black-Box Optimizers for Vision-Language Models. (arXiv:2309.05950v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05950
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411; (VLMs) &#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#26469;&#36991;&#20813;&#35775;&#38382;&#27169;&#22411;&#21442;&#25968;&#65292;&#37319;&#29992;&#32842;&#22825;&#24335;&#30340;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#40657;&#30418;&#20248;&#21270;&#22120;&#65292;&#22312;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#36798;&#21040;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#22312;&#22823;&#35268;&#27169;&#32593;&#32476;&#25968;&#25454;&#38598;&#19978;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411; (VLMs) &#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#35270;&#35273;&#21644;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#30446;&#21069;&#65292;VLMs &#30340;&#24494;&#35843;&#26041;&#27861;&#20027;&#35201;&#22312;&#30333;&#30418;&#29615;&#22659;&#20013;&#25805;&#20316;&#65292;&#38656;&#35201;&#35775;&#38382;&#27169;&#22411;&#21442;&#25968;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810; VLMs &#20381;&#36182;&#20110;&#19987;&#26377;&#25968;&#25454;&#19988;&#19981;&#24320;&#28304;&#65292;&#38480;&#21046;&#20102;&#20351;&#29992;&#30333;&#30418;&#26041;&#27861;&#36827;&#34892;&#24494;&#35843;&#12290;&#37492;&#20110;&#20687; ChatGPT &#36825;&#26679;&#30340;&#21463;&#27426;&#36814;&#31169;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#20173;&#28982;&#25552;&#20379;&#22522;&#20110;&#35821;&#35328;&#30340;&#29992;&#25143;&#30028;&#38754;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#24320;&#21457;&#19968;&#31181;&#26032;&#30340; VLMs &#24494;&#35843;&#26041;&#27861;&#65292;&#20174;&#32780;&#36991;&#20813;&#35775;&#38382;&#27169;&#22411;&#21442;&#25968;&#12289;&#29305;&#24449;&#23884;&#20837;&#25110;&#36755;&#20986; logits &#30340;&#38656;&#35201;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22522;&#20110;&#32842;&#22825;&#30340; LLMs &#20316;&#20026;&#40657;&#30418;&#20248;&#21270;&#22120;&#65292;&#20197;&#22312;&#20351;&#29992; CLIP &#36827;&#34892;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#30340;&#31034;&#20363;&#20219;&#21153;&#20013;&#23547;&#25214;&#26368;&#20339;&#25991;&#26412;&#25552;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#33258;&#21160;"&#29228;&#23665;"&#31243;&#24207;&#65292;&#23427;&#33021;&#25910;&#25947;&#21040;&#26377;&#25928;&#30340;&#25552;&#31034;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-language models (VLMs) pre-trained on web-scale datasets have demonstrated remarkable capabilities across a variety of vision and multimodal tasks. Currently, fine-tuning methods for VLMs mainly operate in a white-box setting, requiring access to model parameters for backpropagation. However, many VLMs rely on proprietary data and are not open-source, which restricts the use of white-box approaches for fine-tuning. Given that popular private large language models (LLMs) like ChatGPT still offer a language-based user interface, we aim to develop a novel fine-tuning approach for VLMs through natural language prompts, thereby avoiding the need to access model parameters, feature embeddings, or output logits. In this setup, we propose employing chat-based LLMs as black-box optimizers to search for the best text prompt on the illustrative task of few-shot image classification using CLIP. Specifically, we adopt an automatic "hill-climbing" procedure that converges on an effective prom
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#21360;&#24230;&#30340;&#27861;&#24459;&#39046;&#22495;&#20986;&#21457;&#65292;&#36890;&#36807;&#23545;&#22312;&#21360;&#22320;&#35821;&#27861;&#24459;&#25991;&#26723;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#20445;&#37322;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#31639;&#27861;&#20559;&#35265;&#20256;&#36882;&#36827;&#34892;&#20102;&#21021;&#27493;&#35843;&#26597;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20915;&#31574;&#26641;&#27169;&#22411;&#22312;&#19982;&#21360;&#24230;&#25945;&#24466;&#21644;&#31302;&#26031;&#26519;&#30456;&#20851;&#30340;&#36755;&#20837;&#29305;&#24449;&#19978;&#20855;&#26377;0.237&#30340;&#25972;&#20307;&#20844;&#24179;&#24615;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2303.07247</link><description>&lt;p&gt;
&#21360;&#24230;&#27861;&#24459;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#26159;&#21542;&#20844;&#24179;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Models Trained on Indian Legal Data Fair?. (arXiv:2303.07247v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#21360;&#24230;&#30340;&#27861;&#24459;&#39046;&#22495;&#20986;&#21457;&#65292;&#36890;&#36807;&#23545;&#22312;&#21360;&#22320;&#35821;&#27861;&#24459;&#25991;&#26723;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#20445;&#37322;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#31639;&#27861;&#20559;&#35265;&#20256;&#36882;&#36827;&#34892;&#20102;&#21021;&#27493;&#35843;&#26597;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20915;&#31574;&#26641;&#27169;&#22411;&#22312;&#19982;&#21360;&#24230;&#25945;&#24466;&#21644;&#31302;&#26031;&#26519;&#30456;&#20851;&#30340;&#36755;&#20837;&#29305;&#24449;&#19978;&#20855;&#26377;0.237&#30340;&#25972;&#20307;&#20844;&#24179;&#24615;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#36827;&#23637;&#19982;&#24212;&#29992;&#22312;&#22810;&#20010;&#39046;&#22495;&#65288;&#22914;&#27861;&#24459;&#12289;&#21307;&#30103;&#21644;&#24515;&#29702;&#20581;&#24247;&#65289;&#21462;&#24471;&#20102;&#24456;&#22823;&#25104;&#21151;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;&#21028;&#20915;&#39044;&#27979;&#65289;&#29992;&#20110;&#27861;&#24459;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#25429;&#25417;&#21040;&#20102;&#31038;&#20250;&#20559;&#35265;&#12290;&#34429;&#28982;NLP&#39046;&#22495;&#30340;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#24050;&#32463;&#24471;&#21040;&#30740;&#31350;&#65292;&#20294;&#22823;&#22810;&#25968;&#30740;&#31350;&#20027;&#35201;&#23450;&#20301;&#22312;&#35199;&#26041;&#32972;&#26223;&#19979;&#12290;&#26412;&#25991;&#20174;&#21360;&#24230;&#30340;&#27861;&#24459;&#39046;&#22495;&#20986;&#21457;&#65292;&#23545;&#20844;&#24179;&#24615;&#36827;&#34892;&#20102;&#21021;&#27493;&#35843;&#26597;&#12290;&#25105;&#20204;&#37325;&#28857;&#30740;&#31350;&#20102;&#22312;&#21360;&#22320;&#35821;&#27861;&#24459;&#25991;&#26723;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#20445;&#37322;&#39044;&#27979;&#20219;&#21153;&#20013;&#20256;&#36882;&#23398;&#20064;&#21040;&#30340;&#31639;&#27861;&#20559;&#35265;&#12290;&#25105;&#20204;&#20351;&#29992;&#32676;&#20307;&#24179;&#31561;&#35780;&#20272;&#20844;&#24179;&#24615;&#24046;&#36317;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20010;&#20915;&#31574;&#26641;&#27169;&#22411;&#22312;&#20445;&#37322;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;&#22312;&#19982;&#21360;&#24230;&#25945;&#24466;&#21644;&#31302;&#26031;&#26519;&#30456;&#20851;&#30340;&#36755;&#20837;&#29305;&#24449;&#19978;&#20855;&#26377;0.237&#30340;&#25972;&#20307;&#20844;&#24179;&#24615;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#23545;&#21360;&#24230;&#27861;&#24459;&#39046;&#22495;&#30340;&#20844;&#24179;&#24615;&#30740;&#31350;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances and applications of language technology and artificial intelligence have enabled much success across multiple domains like law, medical and mental health. AI-based Language Models, like Judgement Prediction, have recently been proposed for the legal sector. However, these models are strife with encoded social biases picked up from the training data. While bias and fairness have been studied across NLP, most studies primarily locate themselves within a Western context. In this work, we present an initial investigation of fairness from the Indian perspective in the legal domain. We highlight the propagation of learnt algorithmic biases in the bail prediction task for models trained on Hindi legal documents. We evaluate the fairness gap using demographic parity and show that a decision tree model trained for the bail prediction task has an overall fairness disparity of 0.237 between input features associated with Hindus and Muslims. Additionally, we highlight the need for 
&lt;/p&gt;</description></item></channel></rss>