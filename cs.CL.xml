<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24212;&#29992;&#32467;&#26500;&#21270;&#35748;&#30693;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#30340;&#28508;&#22312;&#22909;&#22788;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#24230;&#32467;&#26500;&#21270;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;&#38543;&#26426;&#21270;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.09518</link><description>&lt;p&gt;
&#20154;&#31867;&#35838;&#31243;&#25351;&#23548;&#19979;&#30340;&#25351;&#20196;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Instruction Tuning with Human Curriculum. (arXiv:2310.09518v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24212;&#29992;&#32467;&#26500;&#21270;&#35748;&#30693;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#30340;&#28508;&#22312;&#22909;&#22788;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#24230;&#32467;&#26500;&#21270;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;&#38543;&#26426;&#21270;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#30340;&#20027;&#27969;&#33539;&#24335;&#26159;&#38543;&#26426;&#27927;&#29260;&#35757;&#32451;&#26368;&#22823;&#22810;&#26679;&#21270;&#25351;&#20196;-&#21709;&#24212;&#23545;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#24403;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;ChatGPT&#21644;GPT-4&#20013;&#24212;&#29992;&#32467;&#26500;&#21270;&#35748;&#30693;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#30340;&#28508;&#22312;&#22909;&#22788;&#12290;&#19982;&#20197;&#24448;&#20256;&#32479;&#30340;&#38543;&#26426;&#21270;&#25351;&#20196;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#24230;&#32467;&#26500;&#21270;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#27169;&#25311;&#20102;&#20154;&#31867;&#25945;&#32946;&#30340;&#28176;&#36827;&#24615;&#21644;&#26377;&#32452;&#32455;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#25968;&#25454;&#38598;&#19982;&#25945;&#32946;&#26694;&#26550;&#23545;&#40784;&#26469;&#31574;&#21010;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#65292;&#20026;&#27599;&#20010;&#26679;&#26412;&#21253;&#25324;&#20027;&#39064;&#21644;&#35748;&#30693;&#20005;&#35880;&#31243;&#24230;&#31561;&#20803;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#20174;&#20013;&#23398;&#21040;&#30740;&#31350;&#29983;&#38454;&#27573;&#30340;&#20840;&#38754;&#32454;&#31890;&#24230;&#20027;&#39064;&#65292;&#27599;&#20010;&#20027;&#39064;&#37117;&#26377;&#21508;&#31181;&#38382;&#39064;&#65292;&#20197;&#21033;&#29992;&#24067;&#40065;&#22982;&#30340;&#35748;&#30693;&#20998;&#32423;&#27861;&#25552;&#39640;&#27010;&#24565;&#28145;&#24230;&#65292;&#35813;&#20998;&#32423;&#27861;&#29992;&#20110;&#21306;&#20998;&#27599;&#20010;&#27010;&#24565;&#30340;&#19981;&#21516;&#20154;&#31867;&#35748;&#30693;&#27700;&#24179;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#35748;&#30693;&#23398;&#20064;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;&#38543;&#26426;&#21270;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dominant paradigm for instruction tuning is the random-shuffled training of maximally diverse instruction-response pairs. This paper explores the potential benefits of applying a structured cognitive learning approach to instruction tuning in contemporary large language models like ChatGPT and GPT-4. Unlike the previous conventional randomized instruction dataset, we propose a highly structured synthetic dataset that mimics the progressive and organized nature of human education. We curate our dataset by aligning it with educational frameworks, incorporating meta information including its topic and cognitive rigor level for each sample. Our dataset covers comprehensive fine-grained topics spanning diverse educational stages (from middle school to graduate school) with various questions for each topic to enhance conceptual depth using Bloom's taxonomy-a classification framework distinguishing various levels of human cognition for each concept. The results demonstrate that this cogni
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27880;&#37325;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;AMLP&#65289;&#30340;&#38750;&#33258;&#22238;&#24402;&#29983;&#25104;&#27169;&#22411;&#65292;&#20855;&#26377;&#32447;&#24615;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#65292;&#24182;&#36890;&#36807;&#26679;&#26412;&#24863;&#30693;&#30340;&#33258;&#36866;&#24212;&#25237;&#24433;&#23454;&#29616;&#20102;&#24207;&#21015;&#20013;&#26631;&#35760;&#20043;&#38388;&#30340;&#36890;&#20449;&#12290;</title><link>http://arxiv.org/abs/2310.09512</link><description>&lt;p&gt;
&#27880;&#37325;&#22810;&#23618;&#24863;&#30693;&#26426;&#30340;&#38750;&#33258;&#22238;&#24402;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Attentive Multi-Layer Perceptron for Non-autoregressive Generation. (arXiv:2310.09512v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27880;&#37325;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;AMLP&#65289;&#30340;&#38750;&#33258;&#22238;&#24402;&#29983;&#25104;&#27169;&#22411;&#65292;&#20855;&#26377;&#32447;&#24615;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#65292;&#24182;&#36890;&#36807;&#26679;&#26412;&#24863;&#30693;&#30340;&#33258;&#36866;&#24212;&#25237;&#24433;&#23454;&#29616;&#20102;&#24207;&#21015;&#20013;&#26631;&#35760;&#20043;&#38388;&#30340;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#22238;&#24402;&#65288;AR&#65289;&#29983;&#25104;&#22240;&#20854;&#39640;&#25928;&#24615;&#32780;&#20960;&#20046;&#20027;&#23472;&#20102;&#24207;&#21015;&#29983;&#25104;&#12290;&#26368;&#36817;&#65292;&#38750;&#33258;&#22238;&#24402;&#65288;NAR&#65289;&#29983;&#25104;&#22240;&#20854;&#25928;&#29575;&#21644;&#26085;&#30410;&#22686;&#38271;&#30340;&#21151;&#25928;&#32780;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#20854;&#25928;&#29575;&#20173;&#28982;&#34987;&#24207;&#21015;&#38271;&#24230;&#30340;&#20108;&#27425;&#22797;&#26434;&#24230;&#38480;&#21046;&#65292;&#36825;&#23545;&#20110;&#38271;&#24207;&#21015;&#29983;&#25104;&#30340;&#25193;&#23637;&#26469;&#35828;&#26159;&#19981;&#21487;&#25509;&#21463;&#30340;&#65292;&#32780;&#19988;&#24456;&#23569;&#26377;&#24037;&#20316;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;MLP&#21464;&#20307;&#65292;&#21363;&#27880;&#37325;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;AMLP&#65289;&#65292;&#20197;&#20135;&#29983;&#20855;&#26377;&#32447;&#24615;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;AMLP&#19982;&#32463;&#20856;&#30340;MLP&#19981;&#21516;&#65292;&#23427;&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#35745;&#31639;&#36755;&#20837;&#20013;&#30340;&#33258;&#36866;&#24212;&#25237;&#24433;&#12290;&#26679;&#26412;&#24863;&#30693;&#30340;&#33258;&#36866;&#24212;&#25237;&#24433;&#20351;&#24471;&#24207;&#21015;&#20013;&#30340;&#26631;&#35760;&#20043;&#38388;&#21487;&#20197;&#36827;&#34892;&#36890;&#20449;&#65292;&#24182;&#27169;&#25311;&#26597;&#35810;&#21644;&#38190;&#31354;&#38388;&#20043;&#38388;&#30340;&#24230;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;AMLP&#19982;&#27969;&#34892;&#30340;NAR&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#24471;&#21040;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;NAR-AMLP&#20307;&#31995;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autoregressive~(AR) generation almost dominates sequence generation for its efficacy. Recently, non-autoregressive~(NAR) generation gains increasing popularity for its efficiency and growing efficacy. However, its efficiency is still bottlenecked by quadratic complexity in sequence lengths, which is prohibitive for scaling to long sequence generation and few works have been done to mitigate this problem. In this paper, we propose a novel MLP variant, \textbf{A}ttentive \textbf{M}ulti-\textbf{L}ayer \textbf{P}erceptron~(AMLP), to produce a generation model with linear time and space complexity. Different from classic MLP with static and learnable projection matrices, AMLP leverages adaptive projections computed from inputs in an attentive mode. The sample-aware adaptive projections enable communications among tokens in a sequence, and model the measurement between the query and key space. Furthermore, we marry AMLP with popular NAR models, deriving a highly efficient NAR-AMLP architectu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20381;&#36182;&#24615;&#22522;&#20110;&#23884;&#22871;&#22797;&#21512;&#31867;&#22411;&#35782;&#21035;&#65288;DepNeCTI&#65289;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#26805;&#35821;&#20013;&#22810;&#32452;&#20998;&#22797;&#21512;&#30340;&#38544;&#21547;&#32467;&#26500;&#35782;&#21035;&#38382;&#39064;&#12290;&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#65292;&#24182;&#25506;&#32034;&#20102;&#22810;&#31181;&#38382;&#39064;&#24418;&#24335;&#30340;&#26377;&#25928;&#24615;&#65292;&#35813;&#26694;&#26550;&#23454;&#29616;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.09501</link><description>&lt;p&gt;
DepNeCTI: &#20381;&#36182;&#24615;&#22522;&#20110;&#23884;&#22871;&#22797;&#21512;&#31867;&#22411;&#35782;&#21035;&#30340;&#26805;&#35821;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
DepNeCTI: Dependency-based Nested Compound Type Identification for Sanskrit. (arXiv:2310.09501v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09501
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20381;&#36182;&#24615;&#22522;&#20110;&#23884;&#22871;&#22797;&#21512;&#31867;&#22411;&#35782;&#21035;&#65288;DepNeCTI&#65289;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#26805;&#35821;&#20013;&#22810;&#32452;&#20998;&#22797;&#21512;&#30340;&#38544;&#21547;&#32467;&#26500;&#35782;&#21035;&#38382;&#39064;&#12290;&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#65292;&#24182;&#25506;&#32034;&#20102;&#22810;&#31181;&#38382;&#39064;&#24418;&#24335;&#30340;&#26377;&#25928;&#24615;&#65292;&#35813;&#26694;&#26550;&#23454;&#29616;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#32452;&#20998;&#30340;&#22797;&#21512;&#26159;&#26805;&#35821;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#29616;&#35937;&#65292;&#29702;&#35299;&#22797;&#21512;&#35821;&#30340;&#32452;&#20998;&#30340;&#38544;&#21547;&#32467;&#26500;&#23545;&#20110;&#35299;&#35835;&#20854;&#21547;&#20041;&#33267;&#20851;&#37325;&#35201;&#12290;&#20043;&#21069;&#30340;&#26805;&#35821;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#20108;&#20803;&#22797;&#21512;&#65292;&#24573;&#35270;&#20102;&#22810;&#32452;&#20998;&#22797;&#21512;&#30340;&#24773;&#20917;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#23884;&#22871;&#22797;&#21512;&#31867;&#22411;&#35782;&#21035;&#65288;NeCTI&#65289;&#36825;&#19968;&#26032;&#39062;&#20219;&#21153;&#65292;&#26088;&#22312;&#35782;&#21035;&#22810;&#32452;&#20998;&#22797;&#21512;&#30340;&#23884;&#22871;&#29255;&#27573;&#65292;&#24182;&#35299;&#30721;&#23427;&#20204;&#20043;&#38388;&#30340;&#38544;&#21547;&#35821;&#20041;&#20851;&#31995;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#35789;&#27719;&#35821;&#20041;&#39046;&#22495;&#20013;&#39318;&#27425;&#25552;&#20986;&#36825;&#19968;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21253;&#25324;&#22495;&#22806;&#25968;&#25454;&#38598;&#22312;&#20869;&#30340;&#20004;&#20010;&#26032;&#27880;&#37322;&#25968;&#25454;&#38598;&#26469;&#36827;&#34892;&#36825;&#39033;&#20219;&#21153;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#25506;&#32034;&#23884;&#22871;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#32452;&#25104;&#20998;&#26512;&#21644;&#24207;&#21015;&#21040;&#24207;&#21015;&#31561;&#26631;&#20934;&#38382;&#39064;&#30340;&#26377;&#25928;&#24615;&#65292;&#36827;&#34892;&#20102;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DepNeCTI&#30340;&#26032;&#22411;&#26694;&#26550;&#65306;&#22522;&#20110;&#20381;&#36182;&#20851;&#31995;&#30340;&#23884;&#22871;&#22797;&#21512;&#31867;&#22411;&#35782;&#21035;&#22120;&#65292;&#20854;&#24615;&#33021;&#36229;&#36234;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-component compounding is a prevalent phenomenon in Sanskrit, and understanding the implicit structure of a compound's components is crucial for deciphering its meaning. Earlier approaches in Sanskrit have focused on binary compounds and neglected the multi-component compound setting. This work introduces the novel task of nested compound type identification (NeCTI), which aims to identify nested spans of a multi-component compound and decode the implicit semantic relations between them. To the best of our knowledge, this is the first attempt in the field of lexical semantics to propose this task.  We present 2 newly annotated datasets including an out-of-domain dataset for this task. We also benchmark these datasets by exploring the efficacy of the standard problem formulations such as nested named entity recognition, constituency parsing and seq2seq, etc. We present a novel framework named DepNeCTI: Dependency-based Nested Compound Type Identifier that surpasses the performance 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25935;&#24863;&#24230;&#24863;&#30693;&#28151;&#21512;&#31232;&#30095;&#21270;&#21098;&#26525;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21098;&#26525;&#33267;&#33267;&#23569;50&#65285;&#30340;&#31232;&#30095;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#31232;&#30095;&#24615;&#27700;&#24179;&#21644;&#20943;&#23569;&#21098;&#26525;&#24341;&#36215;&#30340;&#35823;&#24046;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#36824;&#19982;&#37327;&#21270;&#20860;&#23481;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#21387;&#32553;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.09499</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#27425;&#25935;&#24863;&#24230;&#24863;&#30693;&#28151;&#21512;&#31232;&#30095;&#21270;&#21098;&#26525;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
One-Shot Sensitivity-Aware Mixed Sparsity Pruning for Large Language Models. (arXiv:2310.09499v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09499
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25935;&#24863;&#24230;&#24863;&#30693;&#28151;&#21512;&#31232;&#30095;&#21270;&#21098;&#26525;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21098;&#26525;&#33267;&#33267;&#23569;50&#65285;&#30340;&#31232;&#30095;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#31232;&#30095;&#24615;&#27700;&#24179;&#21644;&#20943;&#23569;&#21098;&#26525;&#24341;&#36215;&#30340;&#35823;&#24046;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#36824;&#19982;&#37327;&#21270;&#20860;&#23481;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#21387;&#32553;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPT&#65289;&#31995;&#21015;&#20013;&#30340;&#21508;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39640;&#25512;&#29702;&#24310;&#36831;&#65292;&#24040;&#22823;&#30340;&#27169;&#22411;&#22823;&#23567;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#37327;&#21270;&#12289;&#21098;&#26525;&#21644;&#20854;&#20182;&#26041;&#27861;&#25552;&#39640;LLMs&#30340;&#25928;&#29575;&#25104;&#20026;LLM&#30740;&#31350;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Hessian&#25935;&#24863;&#24230;&#24863;&#30693;&#28151;&#21512;&#31232;&#30095;&#21270;&#21098;&#26525;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;LLMs&#21098;&#26525;&#33267;&#33267;&#23569;50%&#30340;&#31232;&#30095;&#24615;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;&#23427;&#26681;&#25454;&#25935;&#24863;&#24230;&#33258;&#36866;&#24212;&#22320;&#20998;&#37197;&#31232;&#30095;&#24615;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#38477;&#20302;&#21098;&#26525;&#24341;&#36215;&#30340;&#35823;&#24046;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#31232;&#30095;&#24615;&#27700;&#24179;&#12290;&#24403;&#31232;&#30095;&#24230;&#38750;&#24120;&#39640;&#26102;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#20248;&#21183;&#26356;&#21152;&#26126;&#26174;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#37327;&#21270;&#20860;&#23481;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#21387;&#32553;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
Various Large Language Models(LLMs) from the Generative Pretrained Transformer~(GPT) family have achieved outstanding performances in a wide range of text generation tasks. However, the enormous model sizes have hindered their practical use in real-world applications due to high inference latency. Therefore, improving the efficiencies of LLMs through quantization, pruning, and other means has been a key issue in LLM studies. In this work, we propose a method based on Hessian sensitivity-aware mixed sparsity pruning to prune LLMs to at least 50\% sparsity without the need of any retraining. It allocates sparsity adaptively based on sensitivity, allowing us to reduce pruning-induced error while maintaining the overall sparsity level. The advantages of the proposed method exhibit even more when the sparsity is extremely high. Furthermore, our method is compatible with quantization, enabling further compression of LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;&#24102;&#26377;&#31934;&#31070;&#20998;&#35010;&#30151;&#21644;&#23396;&#29420;&#30151;&#29305;&#24449;&#20197;&#21450;&#24418;&#24335;&#24605;&#32500;&#38556;&#30861;&#30340;&#35821;&#35328;&#29305;&#24449;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#35780;&#20998;&#26469;&#30740;&#31350;&#20854;&#30456;&#20851;&#24615;&#65292;&#24182;&#25506;&#35752;&#20102;&#24341;&#20986;FTD&#30151;&#29366;&#30340;&#26368;&#36866;&#21512;&#20219;&#21153;&#21644;&#35328;&#35821;&#38271;&#24230;&#26159;&#21542;&#23545;FTD&#30151;&#29366;&#20135;&#29983;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.09494</link><description>&lt;p&gt;
&#24102;&#26377;&#31934;&#31070;&#20998;&#35010;&#30151;&#21644;&#23396;&#29420;&#30151;&#29305;&#24449;&#20197;&#21450;&#24418;&#24335;&#24605;&#32500;&#38556;&#30861;&#30340;&#35821;&#35328;&#29305;&#24449;&#30340;&#35745;&#31639;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Computational analyses of linguistic features with schizophrenic and autistic traits along with formal thought disorders. (arXiv:2310.09494v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;&#24102;&#26377;&#31934;&#31070;&#20998;&#35010;&#30151;&#21644;&#23396;&#29420;&#30151;&#29305;&#24449;&#20197;&#21450;&#24418;&#24335;&#24605;&#32500;&#38556;&#30861;&#30340;&#35821;&#35328;&#29305;&#24449;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#35780;&#20998;&#26469;&#30740;&#31350;&#20854;&#30456;&#20851;&#24615;&#65292;&#24182;&#25506;&#35752;&#20102;&#24341;&#20986;FTD&#30151;&#29366;&#30340;&#26368;&#36866;&#21512;&#20219;&#21153;&#21644;&#35328;&#35821;&#38271;&#24230;&#26159;&#21542;&#23545;FTD&#30151;&#29366;&#20135;&#29983;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24418;&#24335;&#24605;&#32500;&#38556;&#30861;(FTD)&#26159;&#19968;&#32452;&#24433;&#21709;&#35821;&#35328;&#21644;&#24605;&#32500;&#30340;&#35748;&#30693;&#30151;&#29366;&#65292;&#21487;&#20197;&#36890;&#36807;&#35821;&#35328;&#35266;&#23519;&#21040;&#12290;FTD&#22312;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;(ASD)&#21644;&#31934;&#31070;&#20998;&#35010;&#30151;&#31561;&#21457;&#32946;&#25110;&#31934;&#31070;&#38556;&#30861;&#20013;&#37117;&#26377;&#21457;&#29616;&#65292;&#20197;&#21450;&#30456;&#20851;&#30340;&#31934;&#31070;&#20998;&#35010;&#22411;&#20154;&#26684;&#38556;&#30861;(SPD)&#12290;&#26412;&#25991;&#36890;&#36807;&#20174;&#19968;&#33324;&#20154;&#32676;&#20013;&#30340;&#20247;&#21253;&#26381;&#21153;&#25910;&#38598;&#20102;&#19968;&#20010;&#24102;&#26377;&#19982;ASD&#21644;SPD&#30456;&#20851;&#30340;&#35780;&#20998;&#26631;&#31614;&#30340;&#26085;&#25991;&#38899;&#39057;&#25253;&#21578;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;&#31038;&#20250;&#21453;&#24212;&#24615;&#37327;&#34920;&#31532;&#20108;&#29256;(SRS2)&#21644;&#31934;&#31070;&#20998;&#35010;&#22411;&#20154;&#26684;&#38382;&#21367;(SPQ)&#26469;&#27979;&#37327;&#35821;&#35328;&#29305;&#24449;&#65292;&#21253;&#25324;SPQ&#20013;&#22855;&#24322;&#35328;&#35821;&#23376;&#37327;&#34920;&#26469;&#37327;&#21270;FTD&#30151;&#29366;&#12290;&#25105;&#20204;&#36890;&#36807;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#35780;&#20998;&#39044;&#27979;&#26469;&#30740;&#31350;&#20197;&#19979;&#22235;&#20010;&#30740;&#31350;&#38382;&#39064;&#65306;(RQ1)&#31934;&#31070;&#20998;&#35010;&#22411;&#21644;&#23396;&#29420;&#30151;&#27979;&#37327;&#26377;&#20309;&#30456;&#20851;&#24615;&#65311;(RQ2)&#21738;&#31181;&#20219;&#21153;&#26368;&#36866;&#21512;&#24341;&#20986;FTD&#30151;&#29366;&#65311;(RQ3)&#35328;&#35821;&#38271;&#24230;&#26159;&#21542;&#20250;&#24433;&#21709;&#35825;&#21457;FTD&#30151;&#29366;&#65311;
&lt;/p&gt;
&lt;p&gt;
[See full abstract in the pdf] Formal Thought Disorder (FTD), which is a group of symptoms in cognition that affects language and thought, can be observed through language. FTD is seen across such developmental or psychiatric disorders as Autism Spectrum Disorder (ASD) or Schizophrenia, and its related Schizotypal Personality Disorder (SPD). This paper collected a Japanese audio-report dataset with score labels related to ASD and SPD through a crowd-sourcing service from the general population. We measured language characteristics with the 2nd edition of the Social Responsiveness Scale (SRS2) and the Schizotypal Personality Questionnaire (SPQ), including an odd speech subscale from SPQ to quantify the FTD symptoms. We investigated the following four research questions through machine-learning-based score predictions: (RQ1) How are schizotypal and autistic measures correlated? (RQ2) What is the most suitable task to elicit FTD symptoms? (RQ3) Does the length of speech affect the elicita
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21457;&#29616;&#27599;&#20010;&#20219;&#21153;&#30340;&#23376;&#32593;&#32476;&#21644;&#36719;&#25513;&#34109;&#26426;&#21046;&#26469;&#20811;&#26381;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#20419;&#36827;&#30693;&#35782;&#20256;&#36882;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#20219;&#21153;&#30340;&#24773;&#20917;&#19979;&#37117;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2310.09436</link><description>&lt;p&gt;
&#28151;&#21512;&#20219;&#21153;&#30340;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#23376;&#32593;&#32476;&#21457;&#29616;&#21644;&#36719;&#25513;&#34109;
&lt;/p&gt;
&lt;p&gt;
Sub-network Discovery and Soft-masking for Continual Learning of Mixed Tasks. (arXiv:2310.09436v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09436
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21457;&#29616;&#27599;&#20010;&#20219;&#21153;&#30340;&#23376;&#32593;&#32476;&#21644;&#36719;&#25513;&#34109;&#26426;&#21046;&#26469;&#20811;&#26381;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#20419;&#36827;&#30693;&#35782;&#20256;&#36882;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#20219;&#21153;&#30340;&#24773;&#20917;&#19979;&#37117;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#26377;&#20004;&#20010;&#20027;&#35201;&#30446;&#26631;: &#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#20419;&#36827;&#30693;&#35782;&#20256;&#36882;&#12290;&#29616;&#26377;&#25991;&#29486;&#20027;&#35201;&#20851;&#27880;&#20110;&#20811;&#26381;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#19968;&#20123;&#24037;&#20316;&#20063;&#38024;&#23545;&#20219;&#21153;&#30456;&#20284;&#26102;&#30340;&#30693;&#35782;&#20256;&#36882;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#21482;&#26377;&#19968;&#31181;&#26041;&#27861;&#25552;&#20986;&#20102;&#36830;&#32493;&#23398;&#20064;&#28151;&#21512;&#20219;&#21153;&#24207;&#21015;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#20173;&#28982;&#23384;&#22312;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;/&#25110;&#26377;&#38480;&#30340;&#30693;&#35782;&#20256;&#36882;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#26469;&#23454;&#29616;&#20004;&#32773;&#12290;&#23427;&#36890;&#36807;&#21457;&#29616;&#27599;&#20010;&#20219;&#21153;&#30340;&#23376;&#32593;&#32476;&#26469;&#20811;&#26381;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#36719;&#25513;&#34109;&#26426;&#21046;&#26469;&#20445;&#30041;&#20808;&#21069;&#30340;&#30693;&#35782;&#65292;&#24182;&#20351;&#26032;&#20219;&#21153;&#33021;&#22815;&#20511;&#21161;&#36807;&#21435;&#30340;&#30693;&#35782;&#23454;&#29616;&#30693;&#35782;&#20256;&#36882;&#12290;&#20351;&#29992;&#20998;&#31867;&#12289;&#29983;&#25104;&#12289;&#20449;&#24687;&#25552;&#21462;&#21450;&#20854;&#28151;&#21512; (&#21363;&#24322;&#26500;&#20219;&#21153;) &#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;&#24378;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning (CL) has two main objectives: preventing catastrophic forgetting (CF) and encouraging knowledge transfer (KT). The existing literature mainly focused on overcoming CF. Some work has also been done on KT when the tasks are similar. To our knowledge, only one method has been proposed to learn a sequence of mixed tasks. However, these techniques still suffer from CF and/or limited KT. This paper proposes a new CL method to achieve both. It overcomes CF by isolating the knowledge of each task via discovering a subnetwork for it. A soft-masking mechanism is also proposed to preserve the previous knowledge and to enable the new task to leverage the past knowledge to achieve KT. Experiments using classification, generation, information extraction, and their mixture (i.e., heterogeneous tasks) show that the proposed method consistently outperforms strong baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20851;&#38190;&#35789;&#39537;&#21160;&#30340;&#21477;&#23376;&#36873;&#25321;&#31574;&#30053;&#26469;&#22686;&#24378;&#22522;&#20110;BERT&#30340;&#35270;&#35273;&#38382;&#31572;&#12290;&#36890;&#36807;&#21033;&#29992;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#25216;&#26415;&#24494;&#35843;BERT&#27169;&#22411;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#24102;&#26377;&#25935;&#24863;&#20851;&#38190;&#35789;&#30340;&#21477;&#23376;&#65292;&#26412;&#26041;&#27861;&#33021;&#22815;&#22312;&#25991;&#26723;&#20013;&#35782;&#21035;&#20986;&#22238;&#31572;&#38382;&#39064;&#30340;&#30456;&#20851;&#20803;&#32032;&#65292;&#24182;&#22312;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#20013;&#21462;&#24471;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.09432</link><description>&lt;p&gt;
&#36890;&#36807;&#20851;&#38190;&#35789;&#39537;&#21160;&#30340;&#21477;&#23376;&#36873;&#25321;&#22686;&#24378;&#22522;&#20110;BERT&#30340;&#35270;&#35273;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Enhancing BERT-Based Visual Question Answering through Keyword-Driven Sentence Selection. (arXiv:2310.09432v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09432
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20851;&#38190;&#35789;&#39537;&#21160;&#30340;&#21477;&#23376;&#36873;&#25321;&#31574;&#30053;&#26469;&#22686;&#24378;&#22522;&#20110;BERT&#30340;&#35270;&#35273;&#38382;&#31572;&#12290;&#36890;&#36807;&#21033;&#29992;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#25216;&#26415;&#24494;&#35843;BERT&#27169;&#22411;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#24102;&#26377;&#25935;&#24863;&#20851;&#38190;&#35789;&#30340;&#21477;&#23376;&#65292;&#26412;&#26041;&#27861;&#33021;&#22815;&#22312;&#25991;&#26723;&#20013;&#35782;&#21035;&#20986;&#22238;&#31572;&#38382;&#39064;&#30340;&#30456;&#20851;&#20803;&#32032;&#65292;&#24182;&#22312;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#20013;&#21462;&#24471;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25991;&#26723;&#30340;&#35270;&#35273;&#38382;&#31572;&#31454;&#36187;&#35299;&#20915;&#20102;&#22312;&#22810;&#39029;&#25991;&#26723;&#20013;&#33258;&#21160;&#26816;&#27979;&#20803;&#32032;&#20043;&#38388;&#30340;&#29238;&#23376;&#20851;&#31995;&#30340;&#38382;&#39064;&#12290;&#30446;&#26631;&#26159;&#35782;&#21035;&#22238;&#31572;&#33258;&#28982;&#35821;&#35328;&#25552;&#20986;&#30340;&#29305;&#23450;&#38382;&#39064;&#30340;&#25991;&#26723;&#20803;&#32032;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;PoliTo&#38024;&#23545;&#27492;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#25506;&#32034;&#20102;&#19968;&#31181;&#20165;&#20351;&#29992;&#25991;&#26412;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#19968;&#31181;&#29305;&#21046;&#30340;&#25277;&#26679;&#31574;&#30053;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#25216;&#26415;&#26469;&#24494;&#35843;BERT&#27169;&#22411;&#65292;&#37325;&#28857;&#20851;&#27880;&#21253;&#21547;&#25935;&#24863;&#20851;&#38190;&#35789;&#19988;&#19982;&#38382;&#39064;&#20013;&#20986;&#29616;&#30340;&#20851;&#38190;&#35789;&#30456;&#21516;&#30340;&#21477;&#23376;&#65292;&#20363;&#22914;&#23545;&#34920;&#26684;&#25110;&#22270;&#20687;&#30340;&#24341;&#29992;&#12290;&#30001;&#20110;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#33021;&#22815;&#19982;&#22522;&#32447;&#30456;&#27604;&#21462;&#24471;&#36739;&#39640;&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#23545;&#27492;&#20219;&#21153;&#30340;&#31215;&#26497;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Document-based Visual Question Answering competition addresses the automatic detection of parent-child relationships between elements in multi-page documents. The goal is to identify the document elements that answer a specific question posed in natural language. This paper describes the PoliTo's approach to addressing this task, in particular, our best solution explores a text-only approach, leveraging an ad hoc sampling strategy. Specifically, our approach leverages the Masked Language Modeling technique to fine-tune a BERT model, focusing on sentences containing sensitive keywords that also occur in the questions, such as references to tables or images. Thanks to the effectiveness of this approach, we are able to achieve high performance compared to baselines, demonstrating how our solution contributes positively to this task.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#20998;&#24067;&#24335;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#22788;&#29702;&#25105;&#20204;&#26032;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#26102;&#37117;&#23384;&#22312;&#22256;&#38590;&#65292;&#23613;&#31649;&#23427;&#20204;&#22312;&#20854;&#20182;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#36825;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#30340;&#27867;&#21270;&#21644;&#40065;&#26834;&#24615;&#20173;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2310.09430</link><description>&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#20998;&#24067;&#24335;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#31995;&#32479;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Systematic Evaluation of Large Language Models on Out-of-Distribution Logical Reasoning Tasks. (arXiv:2310.09430v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09430
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#20998;&#24067;&#24335;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#22788;&#29702;&#25105;&#20204;&#26032;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#26102;&#37117;&#23384;&#22312;&#22256;&#38590;&#65292;&#23613;&#31649;&#23427;&#20204;&#22312;&#20854;&#20182;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#36825;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#30340;&#27867;&#21270;&#21644;&#40065;&#26834;&#24615;&#20173;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;GPT-3.5&#21644;GPT-4&#65292;&#24050;&#32463;&#23558;&#20154;&#24037;&#31995;&#32479;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#25552;&#21319;&#21040;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#30340;&#27867;&#21270;&#21644;&#40065;&#26834;&#24615;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#35780;&#20272;&#12290;&#20026;&#20102;&#25506;&#32034;&#36825;&#31181;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#26032;&#30340;&#36923;&#36753;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#20998;&#21035;&#21517;&#20026;"ReClor-plus"&#12289;"LogiQA-plus"&#21644;"LogiQAv2-plus"&#65292;&#27599;&#20010;&#25968;&#25454;&#38598;&#37117;&#21253;&#21547;&#19977;&#20010;&#23376;&#38598;&#65306;&#31532;&#19968;&#20010;&#26159;&#36873;&#39033;&#38543;&#26426;&#25171;&#20081;&#65292;&#31532;&#20108;&#20010;&#26159;&#23558;&#27491;&#30830;&#36873;&#39033;&#26367;&#25442;&#20026;"&#27809;&#26377;&#20854;&#20182;&#36873;&#39033;&#26159;&#27491;&#30830;&#30340;"&#65292;&#31532;&#19977;&#20010;&#26159;&#21069;&#20004;&#20010;&#23376;&#38598;&#30340;&#32452;&#21512;&#12290;&#25105;&#20204;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20351;&#29992;&#20102;&#37492;&#21035;&#21644;&#29983;&#25104;&#22411;&#30340;LLMs&#65292;&#24182;&#34920;&#26126;&#36825;&#20123;&#31616;&#21333;&#30340;&#25216;&#24039;&#26497;&#22823;&#22320;&#38459;&#30861;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#22312;&#21407;&#22987;&#30340;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#25152;&#26377;&#27169;&#22411;&#37117;&#24456;&#38590;&#22238;&#31572;&#25105;&#20204;&#26032;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#25200;&#21160;&#24341;&#20837;&#20219;&#21153;&#21464;&#21270;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), such as GPT-3.5 and GPT-4, have greatly advanced the performance of artificial systems on various natural language processing tasks to human-like levels. However, their generalisation and robustness to perform logical reasoning remain under-evaluated. To probe this ability, we propose three new logical reasoning datasets named "ReClor-plus", "LogiQA-plus" and "LogiQAv2-plus", each featuring three subsets: the first with randomly shuffled options, the second with the correct choices replaced by "none of the other options are correct", and a combination of the previous two subsets. We carry out experiments on these datasets with both discriminative and generative LLMs and show that these simple tricks greatly hinder the performance of the language models. Despite their superior performance on the original publicly available datasets, we find that all models struggle to answer our newly constructed datasets. We show that introducing task variations by perturb
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;SALM&#65292;&#23427;&#26159;&#19968;&#31181;&#20855;&#26377;&#22810;&#20219;&#21153;&#21644;&#35821;&#22659;&#23398;&#20064;&#33021;&#21147;&#30340;&#35821;&#38899;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#22312;&#35821;&#38899;&#35782;&#21035;&#21644;&#32763;&#35793;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#19982;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#29616;&#20102;&#38646;-shot&#35821;&#22659;&#23398;&#20064;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#35821;&#38899;&#30417;&#30563;&#30340;&#35821;&#22659;&#35757;&#32451;&#26041;&#27861;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#35821;&#38899;&#21040;&#25991;&#26412;&#27169;&#22411;&#30340;&#35821;&#22659;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.09424</link><description>&lt;p&gt;
SALM:&#20855;&#26377;&#35821;&#22659;&#23398;&#20064;&#33021;&#21147;&#30340;&#35821;&#38899;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#21644;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
SALM: Speech-augmented Language Model with In-context Learning for Speech Recognition and Translation. (arXiv:2310.09424v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09424
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;SALM&#65292;&#23427;&#26159;&#19968;&#31181;&#20855;&#26377;&#22810;&#20219;&#21153;&#21644;&#35821;&#22659;&#23398;&#20064;&#33021;&#21147;&#30340;&#35821;&#38899;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#22312;&#35821;&#38899;&#35782;&#21035;&#21644;&#32763;&#35793;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#19982;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#29616;&#20102;&#38646;-shot&#35821;&#22659;&#23398;&#20064;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#35821;&#38899;&#30417;&#30563;&#30340;&#35821;&#22659;&#35757;&#32451;&#26041;&#27861;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#35821;&#38899;&#21040;&#25991;&#26412;&#27169;&#22411;&#30340;&#35821;&#22659;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20855;&#26377;&#22810;&#20219;&#21153;&#21644;&#35821;&#22659;&#23398;&#20064;&#33021;&#21147;&#30340;&#35821;&#38899;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65288;SALM&#65289;&#12290;SALM&#30001;&#19968;&#20010;&#20923;&#32467;&#30340;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#12289;&#19968;&#20010;&#38899;&#39057;&#32534;&#30721;&#22120;&#12289;&#19968;&#20010;&#27169;&#24577;&#36866;&#37197;&#22120;&#27169;&#22359;&#21644;&#36866;&#24212;&#35821;&#38899;&#36755;&#20837;&#21644;&#30456;&#20851;&#20219;&#21153;&#25351;&#20196;&#30340;LoRA&#23618;&#32452;&#25104;&#12290;&#32479;&#19968;&#30340;SALM&#19981;&#20165;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#21644;&#35821;&#38899;&#32763;&#35793;&#65288;AST&#65289;&#30340;&#20219;&#21153;&#29305;&#23450;Conformer&#22522;&#20934;&#19978;&#36798;&#21040;&#20102;&#19982;&#20043;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#65292;&#36824;&#23637;&#29616;&#20102;&#38646;-shot&#35821;&#22659;&#23398;&#20064;&#33021;&#21147;&#65292;&#36890;&#36807;ASR&#21644;AST&#30340;&#20851;&#38190;&#35789;&#25552;&#21319;&#20219;&#21153;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#35821;&#38899;&#30417;&#30563;&#30340;&#35821;&#22659;&#35757;&#32451;&#8221;&#26469;&#24357;&#21512;LLM&#35757;&#32451;&#21644;&#19979;&#28216;&#35821;&#38899;&#20219;&#21153;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#35821;&#38899;&#21040;&#25991;&#26412;&#27169;&#22411;&#30340;&#35821;&#22659;&#23398;&#20064;&#33021;&#21147;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#36890;&#36807;NeMo&#24037;&#20855;&#21253;&#24320;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel Speech Augmented Language Model (SALM) with {\em multitask} and {\em in-context} learning capabilities. SALM comprises a frozen text LLM, a audio encoder, a modality adapter module, and LoRA layers to accommodate speech input and associated task instructions. The unified SALM not only achieves performance on par with task-specific Conformer baselines for Automatic Speech Recognition (ASR) and Speech Translation (AST), but also exhibits zero-shot in-context learning capabilities, demonstrated through keyword-boosting task for ASR and AST. Moreover, {\em speech supervised in-context training} is proposed to bridge the gap between LLM training and downstream speech tasks, which further boosts the in-context learning ability of speech-to-text models. Proposed model is open-sourced via NeMo toolkit.
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#23398;&#20064;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#65292;&#24182;&#19988;&#22312;&#25991;&#26412;&#25688;&#35201;&#39046;&#22495;&#20063;&#34920;&#29616;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.09411</link><description>&lt;p&gt;
&#29992;&#28145;&#24230;&#23398;&#20064;&#35843;&#26597;&#25991;&#26412;&#25688;&#35201;&#30340;&#29616;&#29366;&#65306;&#19968;&#39033;&#32508;&#36848;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Surveying the Landscape of Text Summarization with Deep Learning: A Comprehensive Review. (arXiv:2310.09411v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09411
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#65292;&#24182;&#19988;&#22312;&#25991;&#26412;&#25688;&#35201;&#39046;&#22495;&#20063;&#34920;&#29616;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#36890;&#36807;&#20351;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#35821;&#35328;&#25968;&#25454;&#30340;&#22797;&#26434;&#34920;&#31034;&#65292;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#65292;&#20174;&#32780;&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#30340;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#19982;&#20256;&#32479;&#30340;NLP&#26041;&#27861;&#30456;&#21453;&#65292;&#28145;&#24230;&#23398;&#20064;NLP&#27169;&#22411;&#36890;&#24120;&#20351;&#29992;&#22823;&#37327;&#25968;&#25454;&#26469;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#20854;&#33021;&#22815;&#23398;&#20064;&#35821;&#35328;&#25968;&#25454;&#20013;&#30340;&#27169;&#24335;&#21644;&#20851;&#31995;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#23398;&#20064;&#35821;&#35328;&#25968;&#25454;&#30340;&#23618;&#27425;&#34920;&#31034;&#65292;&#22788;&#29702;&#21487;&#21464;&#38271;&#24230;&#30340;&#36755;&#20837;&#24207;&#21015;&#65292;&#24182;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#36825;&#20351;&#23427;&#20204;&#38750;&#24120;&#36866;&#21512;NLP&#24212;&#29992;&#12290;&#30001;&#20110;&#25991;&#26412;&#25968;&#25454;&#30340;&#25351;&#25968;&#22686;&#38271;&#21644;&#23545;&#31616;&#27905;&#12289;&#36830;&#36143;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#25688;&#35201;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#21152;&#65292;&#25991;&#26412;&#25688;&#35201;&#24050;&#25104;&#20026;NLP&#39046;&#22495;&#30340;&#19968;&#20010;&#20851;&#38190;&#30740;&#31350;&#39046;&#22495;&#12290;&#23558;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20110;&#25991;&#26412;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
In recent years, deep learning has revolutionized natural language processing (NLP) by enabling the development of models that can learn complex representations of language data, leading to significant improvements in performance across a wide range of NLP tasks. Deep learning models for NLP typically use large amounts of data to train deep neural networks, allowing them to learn the patterns and relationships in language data. This is in contrast to traditional NLP approaches, which rely on hand-engineered features and rules to perform NLP tasks. The ability of deep neural networks to learn hierarchical representations of language data, handle variable-length input sequences, and perform well on large datasets makes them well-suited for NLP applications. Driven by the exponential growth of textual data and the increasing demand for condensed, coherent, and informative summaries, text summarization has been a critical research area in the field of NLP. Applying deep learning to text su
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23450;&#37327;&#26041;&#27861;&#35780;&#20272;&#32654;&#22269;&#35799;&#27468;&#30340;&#39118;&#26684;&#65292;&#24182;&#36890;&#36807;&#21487;&#35270;&#21270;&#23637;&#31034;&#35799;&#38598;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20998;&#26512;&#27491;&#23383;&#27861;&#12289;&#21477;&#27861;&#21644;&#38899;&#20301;&#29305;&#24449;&#65292;&#20174;&#35799;&#27468;&#30340;&#22810;&#23618;&#28508;&#22312;&#32467;&#26500;&#20013;&#21457;&#29616;&#20840;&#38754;&#30340;&#39118;&#26684;&#20449;&#24687;&#12290;&#19982;&#20256;&#32479;&#30340;&#35789;&#39057;&#29305;&#24449;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#26356;&#22909;&#22320;&#21051;&#30011;&#20102;&#35799;&#27468;&#39118;&#26684;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#23398;&#26415;&#30740;&#31350;&#12289;&#35799;&#27468;&#30340;&#20010;&#20154;&#22238;&#24212;&#30740;&#31350;&#20197;&#21450;&#26681;&#25454;&#35835;&#32773;&#21916;&#22909;&#36827;&#34892;&#25512;&#33616;&#31561;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2310.09357</link><description>&lt;p&gt;
&#19968;&#31181;&#35780;&#20272;&#32654;&#22269;&#35799;&#27468;&#39118;&#26684;&#30340;&#35745;&#31639;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Computational Approach to Style in American Poetry. (arXiv:2310.09357v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09357
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23450;&#37327;&#26041;&#27861;&#35780;&#20272;&#32654;&#22269;&#35799;&#27468;&#30340;&#39118;&#26684;&#65292;&#24182;&#36890;&#36807;&#21487;&#35270;&#21270;&#23637;&#31034;&#35799;&#38598;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20998;&#26512;&#27491;&#23383;&#27861;&#12289;&#21477;&#27861;&#21644;&#38899;&#20301;&#29305;&#24449;&#65292;&#20174;&#35799;&#27468;&#30340;&#22810;&#23618;&#28508;&#22312;&#32467;&#26500;&#20013;&#21457;&#29616;&#20840;&#38754;&#30340;&#39118;&#26684;&#20449;&#24687;&#12290;&#19982;&#20256;&#32479;&#30340;&#35789;&#39057;&#29305;&#24449;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#26356;&#22909;&#22320;&#21051;&#30011;&#20102;&#35799;&#27468;&#39118;&#26684;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#23398;&#26415;&#30740;&#31350;&#12289;&#35799;&#27468;&#30340;&#20010;&#20154;&#22238;&#24212;&#30740;&#31350;&#20197;&#21450;&#26681;&#25454;&#35835;&#32773;&#21916;&#22909;&#36827;&#34892;&#25512;&#33616;&#31561;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#23450;&#37327;&#26041;&#27861;&#26469;&#35780;&#20272;&#32654;&#22269;&#35799;&#27468;&#30340;&#39118;&#26684;&#65292;&#24182;&#21487;&#35270;&#21270;&#35799;&#38598;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#23450;&#24615;&#35799;&#27468;&#25209;&#35780;&#24110;&#21161;&#25105;&#20204;&#24320;&#21457;&#20102;&#20998;&#26512;&#21508;&#31181;&#27491;&#23383;&#27861;&#12289;&#21477;&#27861;&#21644;&#38899;&#20301;&#29305;&#24449;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#36825;&#20123;&#29305;&#24449;&#29992;&#20110;&#20174;&#35799;&#27468;&#30340;&#22810;&#23618;&#28508;&#22312;&#32467;&#26500;&#20013;&#21457;&#29616;&#20840;&#38754;&#30340;&#39118;&#26684;&#20449;&#24687;&#65292;&#24182;&#35745;&#31639;&#35799;&#27468;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#21487;&#35270;&#21270;&#25552;&#20379;&#20102;&#23545;&#20998;&#26512;&#32452;&#20214;&#30340;&#20415;&#25463;&#35775;&#38382;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#35799;&#27468;&#38598;&#21512;&#19978;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#26174;&#31034;&#20986;&#23427;&#27604;&#20256;&#32479;&#30340;&#35789;&#39057;&#29305;&#24449;&#22312;&#20856;&#22411;&#25991;&#26412;&#20998;&#26512;&#31639;&#27861;&#20013;&#26356;&#22909;&#22320;&#21051;&#30011;&#20102;&#35799;&#27468;&#39118;&#26684;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23398;&#26415;&#30740;&#31350;&#12289;&#23545;&#35799;&#27468;&#30340;&#30452;&#35266;&#20010;&#20154;&#22238;&#24212;&#30340;&#30740;&#31350;&#20197;&#21450;&#26681;&#25454;&#35835;&#32773;&#21916;&#22909;&#36827;&#34892;&#25512;&#33616;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a quantitative method to assess the style of American poems and to visualize a collection of poems in relation to one another. Qualitative poetry criticism helped guide our development of metrics that analyze various orthographic, syntactic, and phonemic features. These features are used to discover comprehensive stylistic information from a poem's multi-layered latent structure, and to compute distances between poems in this space. Visualizations provide ready access to the analytical components. We demonstrate our method on several collections of poetry, showing that it better delineates poetry style than the traditional word-occurrence features that are used in typical text analysis algorithms. Our method has potential applications to academic research of texts, to research of the intuitive personal response to poetry, and to making recommendations to readers based on their favorite poems.
&lt;/p&gt;</description></item><item><title>&#22312;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#20013;&#65292;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#23545;&#20110;&#25552;&#39640;&#24615;&#33021;&#27604;&#38646;-shot&#30340;&#38646;&#35757;&#32451;&#25968;&#25454;&#26041;&#24335;&#26356;&#21152;&#26377;&#25928;&#65292;&#24182;&#19988;&#19982;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25152;&#26377;&#24773;&#26223;&#19979;&#37117;&#34920;&#29616;&#20986;&#36739;&#22823;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.09350</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#29992;&#20110;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Domain Adaption for Neural Information Retrieval. (arXiv:2310.09350v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09350
&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#20013;&#65292;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#23545;&#20110;&#25552;&#39640;&#24615;&#33021;&#27604;&#38646;-shot&#30340;&#38646;&#35757;&#32451;&#25968;&#25454;&#26041;&#24335;&#26356;&#21152;&#26377;&#25928;&#65292;&#24182;&#19988;&#19982;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25152;&#26377;&#24773;&#26223;&#19979;&#37117;&#34920;&#29616;&#20986;&#36739;&#22823;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#38656;&#35201;&#26114;&#36149;&#30340;&#23545;&#30446;&#26631;&#39046;&#22495;&#36827;&#34892;&#27880;&#37322;&#30340;&#25968;&#25454;&#25165;&#33021;&#20445;&#25345;&#31454;&#20105;&#21147;&#12290;&#25552;&#20986;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25110;&#22522;&#20110;&#35268;&#21017;&#30340;&#23383;&#31526;&#20018;&#25805;&#20316;&#36827;&#34892;&#26597;&#35810;&#29983;&#25104;&#30340;&#21512;&#25104;&#27880;&#37322;&#20316;&#20026;&#26367;&#20195;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#30340;&#30456;&#23545;&#20248;&#21183;&#23578;&#26410;&#36827;&#34892;&#20998;&#26512;&#12290;&#26412;&#25991;&#20351;&#29992;&#30456;&#21516;&#30340;&#31070;&#32463;IR&#26550;&#26500;&#30452;&#25509;&#27604;&#36739;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;BEIR&#22522;&#20934;&#27979;&#35797;&#65292;&#35813;&#27979;&#35797;&#21253;&#25324;&#26469;&#33258;&#22810;&#20010;&#39046;&#22495;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#27809;&#26377;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#25506;&#32034;&#20102;&#20004;&#31181;&#24773;&#26223;&#65306;&#38646;-shot&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#30417;&#30563;&#31995;&#32479;&#22312;&#22823;&#35268;&#27169;&#30340;&#31867;&#20284;&#39046;&#22495;&#25968;&#25454;&#38598;&#65288;MS-MARCO&#65289;&#19978;&#36827;&#34892;&#35757;&#32451;; &#21644;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#38500;&#20102;MS-MARCO&#65292;&#31995;&#32479;&#36824;&#22312;&#30446;&#26631;&#39046;&#22495;&#30340;&#21512;&#25104;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25152;&#26377;&#24773;&#26223;&#20013;&#37117;&#26126;&#26174;&#20248;&#20110;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#19982;&#20197;&#38646;-shot&#30340;&#26041;&#24335;&#24212;&#29992;&#30417;&#30563;IR&#31995;&#32479;&#30456;&#27604;&#65292;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural information retrieval requires costly annotated data for each target domain to be competitive. Synthetic annotation by query generation using Large Language Models or rule-based string manipulation has been proposed as an alternative, but their relative merits have not been analysed. In this paper, we compare both methods head-to-head using the same neural IR architecture. We focus on the BEIR benchmark, which includes test datasets from several domains with no training data, and explore two scenarios: zero-shot, where the supervised system is trained in a large out-of-domain dataset (MS-MARCO); and unsupervised domain adaptation, where, in addition to MS-MARCO, the system is fine-tuned in synthetic data from the target domain. Our results indicate that Large Language Models outperform rule-based methods in all scenarios by a large margin, and, more importantly, that unsupervised domain adaptation is effective compared to applying a supervised IR system in a zero-shot fashion. I
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#35805;&#24605;&#36335;&#25552;&#28860;&#30340;&#30693;&#35782;&#25552;&#28860;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25945;&#24072;&#65292;&#24182;&#36890;&#36807;&#23545;&#40784;&#36807;&#28388;&#22120;&#36873;&#25321;&#24615;&#22320;&#25552;&#28860;&#19968;&#33268;&#21644;&#26377;&#29992;&#30340;&#29702;&#30001;&#12290;&#21516;&#26102;&#65292;&#20171;&#32461;&#20102;&#19968;&#20010;&#21487;&#38752;&#30340;&#23545;&#35805;&#24605;&#36335;&#25512;&#29702;&#22120;&#65292;&#29992;&#20110;&#29983;&#25104;&#21709;&#24212;&#12290;</title><link>http://arxiv.org/abs/2310.09343</link><description>&lt;p&gt;
&#23545;&#24120;&#35782;&#24863;&#30693;&#23545;&#35805;&#20195;&#29702;&#30340;&#23545;&#35805;&#24605;&#36335;&#25552;&#28860;
&lt;/p&gt;
&lt;p&gt;
Dialogue Chain-of-Thought Distillation for Commonsense-aware Conversational Agents. (arXiv:2310.09343v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#35805;&#24605;&#36335;&#25552;&#28860;&#30340;&#30693;&#35782;&#25552;&#28860;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25945;&#24072;&#65292;&#24182;&#36890;&#36807;&#23545;&#40784;&#36807;&#28388;&#22120;&#36873;&#25321;&#24615;&#22320;&#25552;&#28860;&#19968;&#33268;&#21644;&#26377;&#29992;&#30340;&#29702;&#30001;&#12290;&#21516;&#26102;&#65292;&#20171;&#32461;&#20102;&#19968;&#20010;&#21487;&#38752;&#30340;&#23545;&#35805;&#24605;&#36335;&#25512;&#29702;&#22120;&#65292;&#29992;&#20110;&#29983;&#25104;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#20154;&#31867;&#21270;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#38656;&#35201;&#20351;&#29992;&#24120;&#35782;&#25512;&#29702;&#26469;&#26377;&#25928;&#22320;&#29702;&#35299;&#21644;&#22238;&#24212;&#23545;&#35805;&#20013;&#30340;&#38544;&#21547;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#36825;&#26679;&#30340;&#36830;&#36143;&#24615;&#21644;&#20449;&#24687;&#21547;&#37327;&#26159;&#19968;&#20010;&#38750;&#24120;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#21363;&#20351;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22312;&#19968;&#20010;&#21333;&#36339;&#20869;&#35782;&#21035;&#21644;&#32858;&#21512;&#20851;&#38190;&#35777;&#25454;&#30340;&#20219;&#21153;&#20063;&#26159;&#20855;&#26377;&#30456;&#24403;&#22823;&#25361;&#25112;&#24615;&#30340;&#12290;&#36825;&#20010;&#22797;&#26434;&#24615;&#30340;&#21407;&#22240;&#26159;&#36825;&#26679;&#30340;&#35777;&#25454;&#20998;&#25955;&#22312;&#23545;&#35805;&#30340;&#22810;&#20010;&#36718;&#27425;&#20013;&#65292;&#22240;&#27492;&#38656;&#35201;&#22312;&#22810;&#20010;&#36339;&#20013;&#36827;&#34892;&#25972;&#21512;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#28966;&#28857;&#26159;&#20419;&#36827;&#23545;&#35805;&#19978;&#30340;&#22810;&#36339;&#25512;&#29702;&#65292;&#21363;&#23545;&#35805;&#24605;&#36335;&#65288;CoT&#65289;&#25512;&#29702;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30693;&#35782;&#25552;&#28860;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#40784;&#36807;&#28388;&#22120;&#36873;&#25321;&#24615;&#22320;&#25552;&#28860;&#19968;&#33268;&#21644;&#26377;&#29992;&#30340;&#29702;&#30001;&#65292;&#21033;&#29992;LLMs&#20316;&#20026;&#19981;&#21487;&#38752;&#30340;&#25945;&#24072;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;DOCTOR&#65292;&#19968;&#20010;&#25552;&#20379;&#21487;&#38752;&#30340;CoT&#29702;&#30001;&#20197;&#36827;&#34892;&#21709;&#24212;&#29983;&#25104;&#30340;&#23545;&#35805;&#24605;&#36335;&#25512;&#29702;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human-like chatbots necessitate the use of commonsense reasoning in order to effectively comprehend and respond to implicit information present within conversations. Achieving such coherence and informativeness in responses, however, is a non-trivial task. Even for large language models (LLMs), the task of identifying and aggregating key evidence within a single hop presents a substantial challenge. This complexity arises because such evidence is scattered across multiple turns in a conversation, thus necessitating integration over multiple hops. Hence, our focus is to facilitate such multi-hop reasoning over a dialogue context, namely dialogue chain-of-thought (CoT) reasoning. To this end, we propose a knowledge distillation framework that leverages LLMs as unreliable teachers and selectively distills consistent and helpful rationales via alignment filters. We further present DOCTOR, a DialOgue Chain-of-ThOught Reasoner that provides reliable CoT rationales for response generation. We
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;LLM&#29983;&#25104;&#32467;&#26524;&#36827;&#34892;&#37325;&#26032;&#25490;&#21517;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27491;&#30830;&#19981;&#21464;&#37327;&#30340;&#25490;&#21517;&#65292;&#20174;&#32780;&#20943;&#23569;&#31243;&#24207;&#39564;&#35777;&#30340;&#35843;&#29992;&#27425;&#25968;&#12290;</title><link>http://arxiv.org/abs/2310.09342</link><description>&lt;p&gt;
&#20026;&#31243;&#24207;&#39564;&#35777;&#23545;LLM&#29983;&#25104;&#30340;&#24490;&#29615;&#19981;&#21464;&#24335;&#36827;&#34892;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
Ranking LLM-Generated Loop Invariants for Program Verification. (arXiv:2310.09342v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;LLM&#29983;&#25104;&#32467;&#26524;&#36827;&#34892;&#37325;&#26032;&#25490;&#21517;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27491;&#30830;&#19981;&#21464;&#37327;&#30340;&#25490;&#21517;&#65292;&#20174;&#32780;&#20943;&#23569;&#31243;&#24207;&#39564;&#35777;&#30340;&#35843;&#29992;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#24402;&#32435;&#24490;&#29615;&#19981;&#21464;&#37327;&#26159;&#33258;&#21160;&#21270;&#31243;&#24207;&#39564;&#35777;&#30340;&#22522;&#30784;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;gpt-3.5&#25110;gpt-4&#65289;&#33021;&#22815;&#22312;0-shot&#29615;&#22659;&#19979;&#20026;&#19968;&#31867;&#31243;&#24207;&#21512;&#25104;&#24490;&#29615;&#19981;&#21464;&#37327;&#65292;&#20294;&#38656;&#35201;&#22810;&#20010;&#26679;&#26412;&#25165;&#33021;&#29983;&#25104;&#27491;&#30830;&#30340;&#19981;&#21464;&#37327;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#22823;&#37327;&#35843;&#29992;&#31243;&#24207;&#39564;&#35777;&#22120;&#26469;&#24314;&#31435;&#19981;&#21464;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;LLM&#29983;&#25104;&#32467;&#26524;&#36827;&#34892;&#37325;&#26032;&#25490;&#21517;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#25490;&#21517;&#22120;&#65292;&#21487;&#20197;&#26681;&#25454;&#38382;&#39064;&#23450;&#20041;&#21306;&#20998;&#27491;&#30830;&#30340;&#24402;&#32435;&#19981;&#21464;&#37327;&#21644;&#38169;&#35823;&#30340;&#23581;&#35797;&#12290;&#35813;&#25490;&#21517;&#22120;&#32463;&#36807;&#23545;&#27604;&#25490;&#21517;&#20248;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#37325;&#26032;&#25490;&#21517;&#26426;&#21046;&#26174;&#33879;&#25552;&#39640;&#20102;&#27491;&#30830;&#19981;&#21464;&#37327;&#22312;&#29983;&#25104;&#30340;&#20505;&#36873;&#39033;&#20013;&#30340;&#25490;&#21517;&#65292;&#20174;&#32780;&#22823;&#24133;&#20943;&#23569;&#20102;&#23545;&#39564;&#35777;&#22120;&#30340;&#35843;&#29992;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthesizing inductive loop invariants is fundamental to automating program verification. In this work, we observe that Large Language Models (such as gpt-3.5 or gpt-4) are capable of synthesizing loop invariants for a class of programs in a 0-shot setting, yet require several samples to generate the correct invariants. This can lead to a large number of calls to a program verifier to establish an invariant. To address this issue, we propose a {\it re-ranking} approach for the generated results of LLMs. We have designed a ranker that can distinguish between correct inductive invariants and incorrect attempts based on the problem definition. The ranker is optimized as a contrastive ranker. Experimental results demonstrate that this re-ranking mechanism significantly improves the ranking of correct invariants among the generated candidates, leading to a notable reduction in the number of calls to a verifier.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#20154;&#31867;&#35760;&#24518;&#26426;&#21046;&#21551;&#21457;&#30340;&#31070;&#32463;&#27169;&#22359;&#65292;&#27169;&#25311;&#20154;&#31867;&#21644;&#26426;&#22120;&#22914;&#20309;&#23545;&#24403;&#21069;&#36755;&#20837;&#36827;&#34892;&#20851;&#32852;&#25512;&#29702;&#21644;&#38382;&#31572;&#65292;&#24182;&#23558;&#20854;&#19982;&#36807;&#21435;&#30340;&#35760;&#24518;&#32467;&#21512;&#36215;&#26469;&#12290;&#36890;&#36807;&#24863;&#30693;&#12289;&#35760;&#24518;&#21644;&#25512;&#29702;&#32452;&#20214;&#65292;&#35813;&#27169;&#22359;&#23454;&#29616;&#20102;&#24863;&#30693;&#26356;&#26032;&#12289;&#35760;&#24518;&#34701;&#21512;&#21644;&#20449;&#24687;&#26816;&#32034;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.09297</link><description>&lt;p&gt;
&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#35748;&#30693;&#65306;&#21463;&#20154;&#31867;&#35760;&#24518;&#26426;&#21046;&#21551;&#21457;&#30340;&#25512;&#29702;&#31070;&#32463;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
Understanding AI Cognition: A Neural Module for Inference Inspired by Human Memory Mechanisms. (arXiv:2310.09297v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09297
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#20154;&#31867;&#35760;&#24518;&#26426;&#21046;&#21551;&#21457;&#30340;&#31070;&#32463;&#27169;&#22359;&#65292;&#27169;&#25311;&#20154;&#31867;&#21644;&#26426;&#22120;&#22914;&#20309;&#23545;&#24403;&#21069;&#36755;&#20837;&#36827;&#34892;&#20851;&#32852;&#25512;&#29702;&#21644;&#38382;&#31572;&#65292;&#24182;&#23558;&#20854;&#19982;&#36807;&#21435;&#30340;&#35760;&#24518;&#32467;&#21512;&#36215;&#26469;&#12290;&#36890;&#36807;&#24863;&#30693;&#12289;&#35760;&#24518;&#21644;&#25512;&#29702;&#32452;&#20214;&#65292;&#35813;&#27169;&#22359;&#23454;&#29616;&#20102;&#24863;&#30693;&#26356;&#26032;&#12289;&#35760;&#24518;&#34701;&#21512;&#21644;&#20449;&#24687;&#26816;&#32034;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21644;&#26426;&#22120;&#22914;&#20309;&#23558;&#24403;&#21069;&#30340;&#36755;&#20837;&#19982;&#36807;&#21435;&#30340;&#35760;&#24518;&#32467;&#21512;&#36215;&#26469;&#65292;&#36827;&#34892;&#20851;&#32852;&#25512;&#29702;&#21644;&#38382;&#31572;&#65292;&#24182;&#23558;&#24863;&#30693;&#21040;&#30340;&#20449;&#24687;&#32622;&#20110;&#19978;&#19979;&#25991;&#20013;&#65292;&#36825;&#26159;&#35748;&#30693;&#31185;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35868;&#39064;&#12290;&#21463;&#21040;&#20154;&#33041;&#35760;&#24518;&#31995;&#32479;&#21644;&#35748;&#30693;&#32467;&#26500;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#24863;&#30693;&#12289;&#35760;&#24518;&#21644;&#25512;&#29702;&#32452;&#20214;&#30340;PMI&#26694;&#26550;&#12290;&#29305;&#21035;&#22320;&#65292;&#35760;&#24518;&#27169;&#22359;&#21253;&#25324;&#24037;&#20316;&#35760;&#24518;&#21644;&#38271;&#26399;&#35760;&#24518;&#65292;&#20854;&#20013;&#21518;&#32773;&#20855;&#26377;&#26356;&#39640;&#38454;&#30340;&#32467;&#26500;&#26469;&#20445;&#30041;&#26356;&#22810;&#30340;&#32047;&#31215;&#30693;&#35782;&#21644;&#32463;&#39564;&#12290;&#36890;&#36807;&#21487;&#21306;&#20998;&#30340;&#31454;&#20105;&#20889;&#20837;&#35775;&#38382;&#65292;&#24403;&#21069;&#30340;&#24863;&#30693;&#26356;&#26032;&#24037;&#20316;&#35760;&#24518;&#65292;&#20043;&#21518;&#36890;&#36807;&#22806;&#31215;&#20851;&#32852;&#19982;&#38271;&#26399;&#35760;&#24518;&#34701;&#21512;&#65292;&#36991;&#20813;&#20869;&#23384;&#28322;&#20986;&#24182;&#26368;&#23567;&#21270;&#20449;&#24687;&#20914;&#31361;&#12290;&#22312;&#25512;&#29702;&#27169;&#22359;&#20013;&#65292;&#30456;&#20851;&#20449;&#24687;&#20174;&#20004;&#20010;&#21333;&#29420;&#30340;&#35760;&#24518;&#28304;&#26816;&#32034;&#24182;&#32467;&#21512;&#65292;&#20197;&#33719;&#24471;&#26356;&#20840;&#38754;&#21644;&#31934;&#30830;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
How humans and machines make sense of current inputs for relation reasoning and question-answering while putting the perceived information into context of our past memories, has been a challenging conundrum in cognitive science and artificial intelligence. Inspired by human brain's memory system and cognitive architectures, we propose a PMI framework that consists of perception, memory and inference components. Notably, the memory module comprises working and long-term memory, with the latter endowed with a higher-order structure to retain more accumulated knowledge and experiences. Through a differentiable competitive write access, current perceptions update working memory, which is later merged with long-term memory via outer product associations, averting memory overflow and minimizing information conflicts. In the inference module, relevant information is retrieved from two separate memory origins and associatively integrated to attain a more comprehensive and precise interpretatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#23545;&#35270;&#35273;&#25968;&#25454;&#31867;&#22411;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#21457;&#29616;&#34429;&#28982;&#22312;&#26576;&#20123;&#26679;&#24335;&#21270;&#25968;&#25454;&#31867;&#22411;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#22522;&#26412;&#25805;&#20316;&#24341;&#36215;&#30340;&#31616;&#21333;&#25968;&#25454;&#31867;&#22411;&#19978;&#34920;&#29616;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2310.08577</link><description>&lt;p&gt;
&#35270;&#35273;&#25968;&#25454;&#31867;&#22411;&#29702;&#35299;&#24182;&#38750;&#28304;&#33258;&#25193;&#23637;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Visual Data-Type Understanding does not emerge from Scaling Vision-Language Models. (arXiv:2310.08577v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08577
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#23545;&#35270;&#35273;&#25968;&#25454;&#31867;&#22411;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#21457;&#29616;&#34429;&#28982;&#22312;&#26576;&#20123;&#26679;&#24335;&#21270;&#25968;&#25454;&#31867;&#22411;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#22522;&#26412;&#25805;&#20316;&#24341;&#36215;&#30340;&#31616;&#21333;&#25968;&#25454;&#31867;&#22411;&#19978;&#34920;&#29616;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#30340;&#21457;&#23637;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#35270;&#35273;&#35821;&#20041;&#20869;&#23481;&#35782;&#21035;&#25928;&#26524;&#65292;&#21253;&#25324;&#20986;&#33394;&#30340;&#22797;&#21512;&#22270;&#20687;&#29702;&#35299;&#23454;&#20363;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#35270;&#35273;&#25968;&#25454;&#31867;&#22411;&#35782;&#21035;&#65292;&#36825;&#26159;&#19968;&#39033;&#22522;&#26412;&#30340;&#24863;&#30693;&#25216;&#33021;&#65292;&#23545;&#25968;&#25454;&#25972;&#29702;&#65288;&#20363;&#22914;&#20174;&#22823;&#22411;&#25968;&#25454;&#38598;&#20013;&#21435;&#38500;&#22122;&#22768;&#25968;&#25454;&#65292;&#39046;&#22495;&#29305;&#23450;&#30340;&#26816;&#32034;&#65289;&#21644;&#33258;&#20027;&#35270;&#35273;&#65288;&#20363;&#22914;&#21306;&#20998;&#19981;&#21516;&#30340;&#22825;&#27668;&#21464;&#21270;&#21644;&#30456;&#26426;&#38236;&#22836;&#27745;&#26579;&#65289;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#32463;&#36807;27&#31181;&#35270;&#35273;&#25968;&#25454;&#31867;&#22411;&#30340;&#21160;&#29289;&#22270;&#20687;&#30340;&#20462;&#25913;&#65292;&#28085;&#30422;&#20102;&#22235;&#20010;&#24191;&#27867;&#30340;&#31867;&#21035;&#12290;&#23545;39&#20010;&#21442;&#25968;&#33539;&#22260;&#20174;100M&#21040;80B&#30340;VLMs&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#38646;&#26679;&#26412;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#20102;&#19968;&#20010;&#32454;&#33268;&#30340;&#24615;&#33021;&#26223;&#35266;&#12290;&#34429;&#28982;VLMs&#22312;&#35782;&#21035;&#26576;&#20123;&#26679;&#24335;&#21270;&#30340;&#25968;&#25454;&#31867;&#22411;&#65288;&#20363;&#22914;&#21345;&#36890;&#21644;&#33609;&#22270;&#65289;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#22522;&#26412;&#25805;&#20316;&#65288;&#20363;&#22914;&#22270;&#20687;&#26059;&#36716;&#25110;&#28155;&#21152;&#22122;&#22768;&#65289;&#24341;&#36215;&#30340;&#31616;&#21333;&#25968;&#25454;&#31867;&#22411;&#19978;&#34920;&#29616;&#20986;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in the development of vision-language models (VLMs) are yielding remarkable success in recognizing visual semantic content, including impressive instances of compositional image understanding. Here, we introduce the novel task of Visual Data-Type Identification, a basic perceptual skill with implications for data curation (e.g., noisy data-removal from large datasets, domain-specific retrieval) and autonomous vision (e.g., distinguishing changing weather conditions from camera lens staining). We develop two datasets consisting of animal images altered across a diverse set of 27 visual data-types, spanning four broad categories. An extensive zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a nuanced performance landscape. While VLMs are reasonably good at identifying certain stylistic \textit{data-types}, such as cartoons and sketches, they struggle with simpler data-types arising from basic manipulations like image rotations or additive noise.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#30693;&#35782;&#22686;&#24378;&#21644;&#23545;&#40784;&#20004;&#31181;&#26041;&#27861;&#26469;&#25552;&#39640;&#22522;&#20110;&#30693;&#35782;&#30340;&#23545;&#35805;&#31995;&#32479;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#65292;&#20197;&#35299;&#20915;&#29983;&#25104;&#19982;&#25552;&#20379;&#30340;&#30693;&#35782;&#28304;&#20107;&#23454;&#19981;&#19968;&#33268;&#30340;&#22238;&#22797;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.08372</link><description>&lt;p&gt;
&#36890;&#36807;&#30693;&#35782;&#22686;&#24378;&#21644;&#23545;&#40784;&#26469;&#25552;&#21319;&#22522;&#20110;&#30693;&#35782;&#30340;&#23545;&#35805;&#31995;&#32479;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Factual Consistency for Knowledge-Grounded Dialogue Systems via Knowledge Enhancement and Alignment. (arXiv:2310.08372v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08372
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#30693;&#35782;&#22686;&#24378;&#21644;&#23545;&#40784;&#20004;&#31181;&#26041;&#27861;&#26469;&#25552;&#39640;&#22522;&#20110;&#30693;&#35782;&#30340;&#23545;&#35805;&#31995;&#32479;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#65292;&#20197;&#35299;&#20915;&#29983;&#25104;&#19982;&#25552;&#20379;&#30340;&#30693;&#35782;&#28304;&#20107;&#23454;&#19981;&#19968;&#33268;&#30340;&#22238;&#22797;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22522;&#20110;&#30693;&#35782;&#30340;&#23545;&#35805;&#31995;&#32479;&#23481;&#26131;&#29983;&#25104;&#19982;&#25552;&#20379;&#30340;&#30693;&#35782;&#28304;&#20107;&#23454;&#19981;&#19968;&#33268;&#30340;&#22238;&#22797;&#12290;&#22312;&#36825;&#31181;&#19981;&#19968;&#33268;&#30340;&#22238;&#22797;&#20013;&#65292;&#23545;&#35805;&#27169;&#22411;&#26080;&#27861;&#20934;&#30830;&#34920;&#36798;&#20854;&#20381;&#36182;&#30340;&#22806;&#37096;&#30693;&#35782;&#12290;&#21463;&#20808;&#21069;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;&#35813;&#24037;&#20316;&#21457;&#29616;&#21464;&#21387;&#22120;&#20013;&#30340;&#21069;&#39304;&#32593;&#32476;&#65288;FFNs&#65289;&#36127;&#36131;&#20107;&#23454;&#30693;&#35782;&#30340;&#34920;&#36798;&#65292;&#22240;&#27492;&#25105;&#20204;&#36890;&#36807;&#30693;&#35782;&#22686;&#24378;&#21644;&#23545;&#40784;&#20004;&#31181;&#26041;&#27861;&#65292;&#23545;FFNs&#30340;&#20107;&#23454;&#34920;&#36798;&#33021;&#21147;&#36827;&#34892;&#20102;&#39640;&#25928;&#25913;&#36827;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;K-Dial&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21464;&#21387;&#22120;&#20013;&#30340;&#25193;&#23637;FFNs&#20197;&#22686;&#24378;&#29305;&#23450;&#27169;&#24335;&#30340;&#30693;&#35782;&#23545;&#35805;&#36755;&#20837;&#30340;&#20107;&#23454;&#30693;&#35782;&#34920;&#36798;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#23545;FFNs&#22312;&#22238;&#22797;&#20013;&#30340;&#34920;&#36798;&#36827;&#34892;&#20102;&#35843;&#25972;&#65292;&#20197;&#20351;&#20854;&#19982;&#20107;&#23454;&#19968;&#33268;&#30340;&#26368;&#20248;&#30693;&#35782;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained language models (PLMs) based knowledge-grounded dialogue systems are prone to generate responses that are factually inconsistent with the provided knowledge source. In such inconsistent responses, the dialogue models fail to accurately express the external knowledge they rely upon. Inspired by previous work which identified that feed-forward networks (FFNs) within Transformers are responsible for factual knowledge expressions, we investigate two methods to efficiently improve the factual expression capability {of FFNs} by knowledge enhancement and alignment respectively. We first propose \textsc{K-Dial}, which {explicitly} introduces {extended FFNs in Transformers to enhance factual knowledge expressions} given the specific patterns of knowledge-grounded dialogue inputs. Additionally, we apply the reinforcement learning for factual consistency (RLFC) method to implicitly adjust FFNs' expressions in responses by aligning with gold knowledge for the factual consistency prefere
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#26631;&#35760;&#31526;&#21495;&#30340;&#33258;&#22238;&#24402;Transformer&#30340;&#19978;&#19979;&#25991;&#21387;&#32553;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#25351;&#23450;&#33539;&#22260;&#20869;&#30340;&#20013;&#38388;&#28608;&#27963;&#36880;&#27493;&#21387;&#32553;&#20026;&#32039;&#20945;&#24418;&#24335;&#65292;&#20174;&#32780;&#20943;&#23569;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#35821;&#35328;&#24314;&#27169;&#21644;&#25991;&#26723;&#29983;&#25104;&#26041;&#38754;&#65292;&#35813;&#26041;&#27861;&#30456;&#27604;&#31232;&#30095;&#27880;&#24847;&#21147;&#22522;&#32447;&#20855;&#26377;&#26356;&#22909;&#30340;&#27969;&#30021;&#24230;&#12289;N-gram&#21305;&#37197;&#21644;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08152</link><description>&lt;p&gt;
&#24102;&#26377;&#26631;&#35760;&#31526;&#21495;&#30340;&#33258;&#22238;&#24402;Transformer&#30340;&#19978;&#19979;&#25991;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Context Compression for Auto-regressive Transformers with Sentinel Tokens. (arXiv:2310.08152v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#26631;&#35760;&#31526;&#21495;&#30340;&#33258;&#22238;&#24402;Transformer&#30340;&#19978;&#19979;&#25991;&#21387;&#32553;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#25351;&#23450;&#33539;&#22260;&#20869;&#30340;&#20013;&#38388;&#28608;&#27963;&#36880;&#27493;&#21387;&#32553;&#20026;&#32039;&#20945;&#24418;&#24335;&#65292;&#20174;&#32780;&#20943;&#23569;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#35821;&#35328;&#24314;&#27169;&#21644;&#25991;&#26723;&#29983;&#25104;&#26041;&#38754;&#65292;&#35813;&#26041;&#27861;&#30456;&#27604;&#31232;&#30095;&#27880;&#24847;&#21147;&#22522;&#32447;&#20855;&#26377;&#26356;&#22909;&#30340;&#27969;&#30021;&#24230;&#12289;N-gram&#21305;&#37197;&#21644;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#27169;&#22359;&#30340;&#20108;&#27425;&#22797;&#26434;&#24615;&#20351;&#20854;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#36880;&#28176;&#25104;&#20026;&#22522;&#20110;Transformer&#30340;LLM&#30340;&#20027;&#35201;&#35745;&#31639;&#37096;&#20998;&#12290;&#27492;&#22806;&#65292;&#22788;&#29702;&#38271;&#36755;&#20837;&#26102;&#20135;&#29983;&#30340;&#36807;&#22810;&#30340;&#38190;&#20540;&#32531;&#23384;&#20063;&#20250;&#22312;&#20869;&#23384;&#21344;&#29992;&#21644;&#25512;&#29702;&#24310;&#36831;&#26041;&#38754;&#24102;&#26469;&#20005;&#37325;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21363;&#25554;&#21363;&#29992;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#23558;&#25351;&#23450;&#33539;&#22260;&#20869;&#30340;&#20013;&#38388;&#28608;&#27963;&#36880;&#27493;&#21387;&#32553;&#20026;&#32039;&#20945;&#24418;&#24335;&#65292;&#20174;&#32780;&#22312;&#22788;&#29702;&#21518;&#32493;&#19978;&#19979;&#25991;&#26102;&#20943;&#23569;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#39046;&#22495;&#20869;&#35821;&#35328;&#24314;&#27169;&#21644;&#38646;&#26679;&#26412;&#24320;&#25918;&#25991;&#26723;&#29983;&#25104;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27969;&#30021;&#24230;&#12289;N-gram&#21305;&#37197;&#21644;&#35821;&#20041;&#30456;&#20284;&#24615;&#26041;&#38754;&#20248;&#20110;&#31232;&#30095;&#27880;&#24847;&#21147;&#22522;&#32447;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20840;&#38754;&#35780;&#20272;&#20102;&#19978;&#19979;&#25991;&#21387;&#32553;&#23545;&#31995;&#32479;&#25913;&#36827;&#30340;&#30410;&#22788;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/DRSY/KV_Compression&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
The quadratic complexity of the attention module makes it gradually become the bulk of compute in Transformer-based LLMs during generation. Moreover, the excessive key-value cache that arises when dealing with long inputs also brings severe issues on memory footprint and inference latency. In this work, we propose a plug-and-play approach that is able to incrementally compress the intermediate activation of a specified span of tokens into compact ones, thereby reducing both memory and computational cost when processing subsequent context. Experiments on both in-domain language modeling and zero-shot open-ended document generation demonstrate the advantage of our approach over sparse attention baselines in terms of fluency, n-gram matching, and semantic similarity. At last, we comprehensively profile the benefit of context compression on improving the system throughout. Code is available at https://github.com/DRSY/KV_Compression.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;Promptor&#65292;&#19968;&#20010;&#29992;&#20110;&#26234;&#33021;&#25991;&#26412;&#36755;&#20837;&#25216;&#26415;&#30340;&#23545;&#35805;&#24335;&#33258;&#20027;&#25552;&#31034;&#29983;&#25104;&#20195;&#29702;&#12290;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#21487;&#20197;&#20811;&#26381;&#25968;&#25454;&#25910;&#38598;&#21644;&#27169;&#22411;&#24494;&#35843;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#36890;&#36807;&#20197;GPT-3.5&#20026;&#20363;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#20165;&#36890;&#36807;&#25552;&#31034;&#21363;&#21487;&#36229;&#36807;GPT-2&#25903;&#25345;&#30340;&#31995;&#32479;&#65292;&#24182;&#19988;&#21487;&#19982;&#32463;&#36807;&#31934;&#35843;&#30340;GPT-3.5&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;</title><link>http://arxiv.org/abs/2310.08101</link><description>&lt;p&gt;
Promptor:&#19968;&#31181;&#29992;&#20110;&#26234;&#33021;&#25991;&#26412;&#36755;&#20837;&#25216;&#26415;&#30340;&#23545;&#35805;&#24335;&#33258;&#20027;&#25552;&#31034;&#29983;&#25104;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Promptor: A Conversational and Autonomous Prompt Generation Agent for Intelligent Text Entry Techniques. (arXiv:2310.08101v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;Promptor&#65292;&#19968;&#20010;&#29992;&#20110;&#26234;&#33021;&#25991;&#26412;&#36755;&#20837;&#25216;&#26415;&#30340;&#23545;&#35805;&#24335;&#33258;&#20027;&#25552;&#31034;&#29983;&#25104;&#20195;&#29702;&#12290;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#21487;&#20197;&#20811;&#26381;&#25968;&#25454;&#25910;&#38598;&#21644;&#27169;&#22411;&#24494;&#35843;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#36890;&#36807;&#20197;GPT-3.5&#20026;&#20363;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#20165;&#36890;&#36807;&#25552;&#31034;&#21363;&#21487;&#36229;&#36807;GPT-2&#25903;&#25345;&#30340;&#31995;&#32479;&#65292;&#24182;&#19988;&#21487;&#19982;&#32463;&#36807;&#31934;&#35843;&#30340;GPT-3.5&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25105;&#20204;&#26085;&#24120;&#25968;&#23383;&#20132;&#20114;&#20013;&#65292;&#25991;&#26412;&#36755;&#20837;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#20351;&#25991;&#26412;&#36755;&#20837;&#26356;&#26377;&#25928;&#12289;&#39640;&#25928;&#21644;&#27969;&#30021;&#65292;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#35768;&#22810;&#26234;&#33021;&#21151;&#33021;&#65292;&#21253;&#25324;&#21477;&#23376;&#39044;&#27979;&#21644;&#29992;&#25143;&#20010;&#24615;&#21270;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#36825;&#20123;&#39640;&#32423;&#21151;&#33021;&#30340;&#24120;&#35268;&#65292;&#25968;&#25454;&#25910;&#38598;&#21644;&#27169;&#22411;&#24494;&#35843;&#30340;&#24517;&#35201;&#24615;&#20063;&#22686;&#21152;&#20102;&#12290;&#21033;&#29992;GPT-3.5&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#21487;&#20197;&#20943;&#36731;&#36825;&#20123;&#25361;&#25112;&#12290;&#36825;&#19968;&#29420;&#29305;&#30340;&#29305;&#24615;&#20801;&#35768;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#25552;&#31034;&#33719;&#24471;&#26032;&#30340;&#25216;&#33021;&#65292;&#28040;&#38500;&#20102;&#25968;&#25454;&#25910;&#38598;&#21644;&#24494;&#35843;&#30340;&#38656;&#35201;&#12290;&#22240;&#27492;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#21508;&#31181;&#25991;&#26412;&#39044;&#27979;&#25216;&#26415;&#12290;&#25105;&#20204;&#26368;&#21021;&#23637;&#31034;&#20102;&#20165;&#36890;&#36807;&#25552;&#31034;GPT-3.5&#21363;&#21487;&#36229;&#36807;GPT-2&#25903;&#25345;&#30340;&#31995;&#32479;&#65292;&#24182;&#19988;&#19982;&#32463;&#36807;&#31934;&#35843;&#30340;GPT-3.5&#27169;&#22411;&#30456;&#24403;&#65292;&#22312;&#21518;&#20004;&#31181;&#26041;&#27861;&#38656;&#35201;&#26114;&#36149;&#30340;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text entry is an essential task in our day-to-day digital interactions. Numerous intelligent features have been developed to streamline this process, making text entry more effective, efficient, and fluid. These improvements include sentence prediction and user personalization. However, as deep learning-based language models become the norm for these advanced features, the necessity for data collection and model fine-tuning increases. These challenges can be mitigated by harnessing the in-context learning capability of large language models such as GPT-3.5. This unique feature allows the language model to acquire new skills through prompts, eliminating the need for data collection and fine-tuning. Consequently, large language models can learn various text prediction techniques. We initially showed that, for a sentence prediction task, merely prompting GPT-3.5 surpassed a GPT-2 backed system and is comparable with a fine-tuned GPT-3.5 model, with the latter two methods requiring costly 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#22522;&#20110;&#24605;&#32500;&#38142;&#30340;Transformer&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#36890;&#36807;&#20801;&#35768;&#20351;&#29992;&#20013;&#38388;&#29983;&#25104;&#30340;&#26041;&#24335;&#25552;&#39640;&#20102;Transformer&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#32447;&#24615;&#25968;&#37327;&#30340;&#35299;&#30721;&#27493;&#39588;&#22312;&#26631;&#20934;&#35745;&#31639;&#22797;&#26434;&#24230;&#19979;&#22686;&#21152;&#20102;&#26126;&#26174;&#30340;&#26032;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.07923</link><description>&lt;p&gt;
&#22522;&#20110;&#24605;&#32500;&#38142;&#30340;Transformer&#30340;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
The Expresssive Power of Transformers with Chain of Thought. (arXiv:2310.07923v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#22522;&#20110;&#24605;&#32500;&#38142;&#30340;Transformer&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#36890;&#36807;&#20801;&#35768;&#20351;&#29992;&#20013;&#38388;&#29983;&#25104;&#30340;&#26041;&#24335;&#25552;&#39640;&#20102;Transformer&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#32447;&#24615;&#25968;&#37327;&#30340;&#35299;&#30721;&#27493;&#39588;&#22312;&#26631;&#20934;&#35745;&#31639;&#22797;&#26434;&#24230;&#19979;&#22686;&#21152;&#20102;&#26126;&#26174;&#30340;&#26032;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#29702;&#35770;&#30740;&#31350;&#21457;&#29616;&#20102;&#19968;&#20123;&#20986;&#20154;&#24847;&#26009;&#22320;&#31616;&#21333;&#30340;&#25512;&#29702;&#38382;&#39064;&#65292;&#20363;&#22914;&#26816;&#26597;&#22270;&#20013;&#26159;&#21542;&#23384;&#22312;&#36830;&#25509;&#30340;&#20004;&#20010;&#33410;&#28857;&#65292;&#25110;&#27169;&#25311;&#26377;&#38480;&#29366;&#24577;&#26426;&#65292;&#36825;&#20123;&#38382;&#39064;&#34987;&#35777;&#26126;&#26080;&#27861;&#30001;&#31435;&#21363;&#35835;&#21462;&#36755;&#20837;&#21518;&#22238;&#31572;&#30340;&#26631;&#20934;Transformer&#35299;&#20915;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#36890;&#36807;&#20801;&#35768;Transformer&#20351;&#29992;&#8220;&#24605;&#32500;&#38142;&#8221;&#25110;&#8220;&#33609;&#31295;&#32440;&#8221;&#65292;&#21363;&#22312;&#22238;&#31572;&#20043;&#21069;&#29983;&#25104;&#24182;&#20381;&#36182;&#19968;&#31995;&#21015;&#20013;&#38388;token&#65292;&#21487;&#20197;&#25913;&#21892;&#20854;&#25512;&#29702;&#33021;&#21147;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#38382;&#65306;&#36825;&#31181;&#20013;&#38388;&#29983;&#25104;&#26159;&#21542;&#20174;&#26681;&#26412;&#19978;&#25193;&#23637;&#20102;&#20165;&#26377;&#35299;&#30721;&#22120;&#30340;Transformer&#30340;&#35745;&#31639;&#33021;&#21147;&#65311;&#25105;&#20204;&#34920;&#26126;&#31572;&#26696;&#26159;&#32943;&#23450;&#30340;&#65292;&#20294;&#22686;&#21152;&#30340;&#31243;&#24230;&#20851;&#38190;&#21462;&#20915;&#20110;&#20013;&#38388;&#29983;&#25104;&#30340;&#25968;&#37327;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21457;&#29616;&#30456;&#23545;&#20110;&#36755;&#20837;&#38271;&#24230;&#26469;&#35828;&#65292;&#20855;&#26377;&#23545;&#25968;&#32423;&#35299;&#30721;&#27493;&#39588;&#30340;Transformer&#35299;&#30721;&#22120;&#20165;&#30053;&#24494;&#25512;&#21160;&#20102;&#26631;&#20934;Transformer&#30340;&#26497;&#38480;&#65292;&#32780;&#32447;&#24615;&#25968;&#37327;&#30340;&#35299;&#30721;&#27493;&#39588;&#21017;&#22686;&#21152;&#20102;&#26126;&#26174;&#30340;&#26032;&#33021;&#21147;&#65288;&#22312;&#26631;&#20934;&#35745;&#31639;&#22797;&#26434;&#24230;&#19979;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent theoretical work has identified surprisingly simple reasoning problems, such as checking if two nodes in a graph are connected or simulating finite-state machines, that are provably unsolvable by standard transformers that answer immediately after reading their input. However, in practice, transformers' reasoning can be improved by allowing them to use a "chain of thought" or "scratchpad", i.e., generate and condition on a sequence of intermediate tokens before answering. Motivated by this, we ask: Does such intermediate generation fundamentally extend the computational power of a decoder-only transformer? We show that the answer is yes, but the amount of increase depends crucially on the amount of intermediate generation. For instance, we find that transformer decoders with a logarithmic number of decoding steps (w.r.t. the input length) push the limits of standard transformers only slightly, while a linear number of decoding steps adds a clear new ability (under standard compl
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35782;&#21035;&#21477;&#23376;&#31867;&#27604;&#30340;&#33021;&#21147;&#19982;&#20854;&#32534;&#30721;&#21477;&#27861;&#21644;&#35821;&#20041;&#32467;&#26500;&#33021;&#21147;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.07818</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#31867;&#27604;&#35782;&#21035;&#19982;&#21477;&#23376;&#32467;&#26500;&#32534;&#30721;&#20043;&#38388;&#30340;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Exploring the Relationship between Analogy Identification and Sentence Structure Encoding in Large Language Models. (arXiv:2310.07818v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07818
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35782;&#21035;&#21477;&#23376;&#31867;&#27604;&#30340;&#33021;&#21147;&#19982;&#20854;&#32534;&#30721;&#21477;&#27861;&#21644;&#35821;&#20041;&#32467;&#26500;&#33021;&#21147;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#31867;&#27604;&#22312;&#20154;&#31867;&#35748;&#30693;&#21644;&#35821;&#35328;&#33021;&#21147;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#37324;&#65292;&#23545;&#20110;&#8220;A&#23545;B&#23601;&#20687;C&#23545;D&#8221;&#36825;&#31181;&#24418;&#24335;&#30340;&#35789;&#35821;&#31867;&#27604;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#28041;&#21450;&#26356;&#38271;&#25991;&#26412;&#30340;&#31867;&#27604;&#65292;&#22914;&#21477;&#23376;&#21644;&#21477;&#23376;&#38598;&#21512;&#65292;&#20256;&#36798;&#31867;&#27604;&#24847;&#20041;&#30340;&#38382;&#39064;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20852;&#36259;&#12290;&#24403;&#21069;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#31038;&#21306;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35782;&#21035;&#27492;&#31867;&#31867;&#27604;&#30340;&#33021;&#21147;&#65292;&#20294;&#36825;&#20123;&#33021;&#21147;&#32972;&#21518;&#30340;&#21407;&#22240;&#38656;&#35201;&#36827;&#19968;&#27493;&#25506;&#31350;&#12290;&#27492;&#22806;&#65292;LLMs&#22312;&#20854;&#23884;&#20837;&#20013;&#32534;&#30721;&#35821;&#35328;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#32467;&#26500;&#30340;&#33021;&#21147;&#65292;&#36817;&#24180;&#26469;&#24471;&#21040;&#20102;&#26174;&#33879;&#20851;&#27880;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#20010;LLMs&#35782;&#21035;&#21477;&#23376;&#31867;&#27604;&#30340;&#33021;&#21147;&#19982;&#20854;&#32534;&#30721;&#21477;&#27861;&#21644;&#35821;&#20041;&#32467;&#26500;&#30340;&#33021;&#21147;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36890;&#36807;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#30340;&#31867;&#27604;&#35782;&#21035;&#33021;&#21147;&#19982;&#20854;&#32534;&#30721;&#33021;&#21147;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying analogies plays a pivotal role in human cognition and language proficiency. In the last decade, there has been extensive research on word analogies in the form of ``A is to B as C is to D.'' However, there is a growing interest in analogies that involve longer text, such as sentences and collections of sentences, which convey analogous meanings. While the current NLP research community evaluates the ability of Large Language Models (LLMs) to identify such analogies, the underlying reasons behind these abilities warrant deeper investigation. Furthermore, the capability of LLMs to encode both syntactic and semantic structures of language within their embeddings has garnered significant attention with the surge in their utilization. In this work, we examine the relationship between the abilities of multiple LLMs to identify sentence analogies, and their capacity to encode syntactic and semantic structures. Through our analysis, we find that analogy identification ability of LL
&lt;/p&gt;</description></item><item><title>LLM4Vis&#26159;&#19968;&#31181;&#20351;&#29992;ChatGPT&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#21487;&#35270;&#21270;&#25512;&#33616;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#24120;&#23569;&#30340;&#28436;&#31034;&#31034;&#20363;&#26469;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#30340;&#35299;&#37322;&#65292;&#24182;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#21644;&#32570;&#20047;&#33258;&#28982;&#35299;&#37322;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07652</link><description>&lt;p&gt;
LLM4Vis: &#20351;&#29992;ChatGPT&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#21487;&#35270;&#21270;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
LLM4Vis: Explainable Visualization Recommendation using ChatGPT. (arXiv:2310.07652v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07652
&lt;/p&gt;
&lt;p&gt;
LLM4Vis&#26159;&#19968;&#31181;&#20351;&#29992;ChatGPT&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#21487;&#35270;&#21270;&#25512;&#33616;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#24120;&#23569;&#30340;&#28436;&#31034;&#31034;&#20363;&#26469;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#30340;&#35299;&#37322;&#65292;&#24182;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#21644;&#32570;&#20047;&#33258;&#28982;&#35299;&#37322;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#21487;&#35270;&#21270;&#26159;&#22312;&#21508;&#20010;&#39046;&#22495;&#25506;&#32034;&#21644;&#20256;&#36798;&#27934;&#23519;&#21147;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#20026;&#20102;&#33258;&#21160;&#21270;&#25968;&#25454;&#38598;&#30340;&#21487;&#35270;&#21270;&#36873;&#25321;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21487;&#35270;&#21270;&#25512;&#33616;&#30340;&#20219;&#21153;&#12290;&#20026;&#27492;&#30446;&#30340;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#38598;-&#21487;&#35270;&#21270;&#23545;&#30340;&#35757;&#32451;&#35821;&#26009;&#24211;&#65292;&#24182;&#19988;&#32570;&#20047;&#33258;&#28982;&#35299;&#37322;&#20854;&#32467;&#26524;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LLM4Vis&#65292;&#19968;&#31181;&#22522;&#20110;ChatGPT&#30340;&#26032;&#22411;&#25552;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#36827;&#34892;&#21487;&#35270;&#21270;&#25512;&#33616;&#24182;&#36820;&#22238;&#31867;&#20284;&#20154;&#31867;&#30340;&#35299;&#37322;&#65292;&#21482;&#20351;&#29992;&#38750;&#24120;&#23569;&#30340;&#31034;&#20363;&#28436;&#31034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#29305;&#24449;&#25551;&#36848;&#65292;&#31034;&#20363;&#28436;&#31034;&#36873;&#25321;&#65292;&#35299;&#37322;&#29983;&#25104;&#65292;&#31034;&#20363;&#28436;&#31034;&#26500;&#24314;&#21644;&#25512;&#29702;&#27493;&#39588;&#12290;&#20026;&#20102;&#33719;&#24471;&#20855;&#26377;&#39640;&#36136;&#37327;&#35299;&#37322;&#30340;&#31034;&#20363;&#28436;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#37322;&#29983;&#25104;&#24341;&#23548;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#21069;&#19968;&#20195;&#21644;&#27169;&#26495;&#26469;&#36845;&#20195;&#22320;&#25913;&#36827;&#29983;&#25104;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data visualization is a powerful tool for exploring and communicating insights in various domains. To automate visualization choice for datasets, a task known as visualization recommendation has been proposed. Various machine-learning-based approaches have been developed for this purpose, but they often require a large corpus of dataset-visualization pairs for training and lack natural explanations for their results. To address this research gap, we propose LLM4Vis, a novel ChatGPT-based prompting approach to perform visualization recommendation and return human-like explanations using very few demonstration examples. Our approach involves feature description, demonstration example selection, explanation generation, demonstration example construction, and inference steps. To obtain demonstration examples with high-quality explanations, we propose a new explanation generation bootstrapping to iteratively refine generated explanations by considering the previous generation and template-b
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LLM-TSE&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#29992;&#25143;&#38190;&#20837;&#30340;&#25991;&#26412;&#36755;&#20837;&#20013;&#25552;&#21462;&#35821;&#20041;&#32447;&#32034;&#65292;&#20197;&#22686;&#24378;&#30446;&#26631;&#35828;&#35805;&#20154;&#25552;&#21462;(TSE)&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#21644;&#21487;&#25511;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.07284</link><description>&lt;p&gt;
&#25171;&#23383;&#20542;&#21548;&#40481;&#23614;&#37202;&#20250;&#65306;&#25991;&#26412;&#24341;&#23548;&#30340;&#30446;&#26631;&#35828;&#35805;&#20154;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Typing to Listen at the Cocktail Party: Text-Guided Target Speaker Extraction. (arXiv:2310.07284v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07284
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LLM-TSE&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#29992;&#25143;&#38190;&#20837;&#30340;&#25991;&#26412;&#36755;&#20837;&#20013;&#25552;&#21462;&#35821;&#20041;&#32447;&#32034;&#65292;&#20197;&#22686;&#24378;&#30446;&#26631;&#35828;&#35805;&#20154;&#25552;&#21462;(TSE)&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#21644;&#21487;&#25511;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#25317;&#26377;&#19968;&#31181;&#22312;&#22797;&#26434;&#30340;&#22768;&#23398;&#29615;&#22659;&#20013;&#26377;&#36873;&#25321;&#24615;&#22320;&#19987;&#27880;&#20110;&#24863;&#20852;&#36259;&#30340;&#22768;&#38899;&#28304;&#30340;&#38750;&#20961;&#33021;&#21147;&#65292;&#36890;&#24120;&#31216;&#20026;&#40481;&#23614;&#37202;&#20250;&#22330;&#26223;&#12290;&#20026;&#20102;&#22312;&#26426;&#22120;&#20013;&#22797;&#21046;&#36825;&#31181;&#24341;&#20154;&#27880;&#30446;&#30340;&#21548;&#35273;&#27880;&#24847;&#33021;&#21147;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#30446;&#26631;&#35828;&#35805;&#20154;&#25552;&#21462;(TSE)&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#21033;&#29992;&#30446;&#26631;&#35828;&#35805;&#20154;&#30340;&#39044;&#20808;&#27880;&#20876;&#32447;&#32034;&#26469;&#25552;&#21462;&#24863;&#20852;&#36259;&#30340;&#22768;&#28304;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#24773;&#26223;&#20013;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#21463;&#21040;&#20102;&#39044;&#20808;&#27880;&#20876;&#32447;&#32034;&#30340;&#21487;&#33021;&#21464;&#21270;&#29978;&#33267;&#32570;&#22833;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#23558;&#33258;&#28982;&#35821;&#35328;&#25972;&#21512;&#21040;&#29616;&#26377;TSE&#27169;&#22411;&#20013;&#20197;&#22686;&#24378;&#20854;&#28789;&#27963;&#24615;&#21644;&#21487;&#25511;&#24615;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LLM-TSE&#30340;&#27169;&#22411;&#65292;&#20854;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20174;&#29992;&#25143;&#30340;&#38190;&#20837;&#25991;&#26412;&#36755;&#20837;&#20013;&#25552;&#21462;&#26377;&#29992;&#30340;&#35821;&#20041;&#32447;&#32034;&#65292;&#36825;&#20123;&#32447;&#32034;&#21487;&#20197;&#34917;&#20805;&#39044;&#20808;&#27880;&#20876;&#30340;&#32447;&#32034;&#25110;&#29420;&#31435;&#24037;&#20316;&#20197;&#25511;&#21046;TSE&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans possess an extraordinary ability to selectively focus on the sound source of interest amidst complex acoustic environments, commonly referred to as cocktail party scenarios. In an attempt to replicate this remarkable auditory attention capability in machines, target speaker extraction (TSE) models have been developed. These models leverage the pre-registered cues of the target speaker to extract the sound source of interest. However, the effectiveness of these models is hindered in real-world scenarios due to the potential variation or even absence of pre-registered cues. To address this limitation, this study investigates the integration of natural language to enhance the flexibility and controllability of existing TSE models. Specifically, we propose a model named LLM-TSE, wherein a large language model (LLM) to extract useful semantic cues from the user's typed text input, which can complement the pre-registered cues or work independently to control the TSE process. Our exper
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DiffuSeq-v2&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#31163;&#25955;&#21644;&#36830;&#32493;&#25991;&#26412;&#31354;&#38388;&#36830;&#25509;&#36215;&#26469;&#23454;&#29616;&#20102;Seq2Seq&#25193;&#25955;&#27169;&#22411;&#30340;&#21152;&#36895;&#12290;&#22312;&#35757;&#32451;&#20013;&#24341;&#20837;&#36719;&#21560;&#25910;&#24577;&#65292;&#25552;&#39640;&#31163;&#25955;&#31361;&#21464;&#37325;&#26500;&#33021;&#21147;&#65307;&#22312;&#37319;&#26679;&#38454;&#27573;&#20351;&#29992;ODE&#27714;&#35299;&#22120;&#21152;&#24555;&#37319;&#26679;&#36895;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35757;&#32451;&#25910;&#25947;&#36895;&#24230;&#25552;&#39640;4&#20493;&#65292;&#29983;&#25104;&#26679;&#26412;&#36895;&#24230;&#25552;&#39640;800&#20493;&#65292;&#26356;&#36866;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.05793</link><description>&lt;p&gt;
DiffuSeq-v2&#65306;&#23558;&#31163;&#25955;&#21644;&#36830;&#32493;&#25991;&#26412;&#31354;&#38388;&#36830;&#25509;&#36215;&#26469;&#20197;&#21152;&#36895;Seq2Seq&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DiffuSeq-v2: Bridging Discrete and Continuous Text Spaces for Accelerated Seq2Seq Diffusion Models. (arXiv:2310.05793v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DiffuSeq-v2&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#31163;&#25955;&#21644;&#36830;&#32493;&#25991;&#26412;&#31354;&#38388;&#36830;&#25509;&#36215;&#26469;&#23454;&#29616;&#20102;Seq2Seq&#25193;&#25955;&#27169;&#22411;&#30340;&#21152;&#36895;&#12290;&#22312;&#35757;&#32451;&#20013;&#24341;&#20837;&#36719;&#21560;&#25910;&#24577;&#65292;&#25552;&#39640;&#31163;&#25955;&#31361;&#21464;&#37325;&#26500;&#33021;&#21147;&#65307;&#22312;&#37319;&#26679;&#38454;&#27573;&#20351;&#29992;ODE&#27714;&#35299;&#22120;&#21152;&#24555;&#37319;&#26679;&#36895;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35757;&#32451;&#25910;&#25947;&#36895;&#24230;&#25552;&#39640;4&#20493;&#65292;&#29983;&#25104;&#26679;&#26412;&#36895;&#24230;&#25552;&#39640;800&#20493;&#65292;&#26356;&#36866;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#24207;&#21015;&#26041;&#38754;&#26159;&#24456;&#26377;&#28508;&#21147;&#30340;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#26159;&#21033;&#29992;&#36830;&#32493;&#30340;&#25193;&#25955;&#31354;&#38388;&#34920;&#31034;&#31163;&#25955;&#25991;&#26412;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20135;&#29983;&#20102;&#22823;&#37327;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#23548;&#33268;&#37319;&#26679;&#36895;&#24230;&#21464;&#24930;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#36719;&#21560;&#25910;&#24577;&#65292;&#24110;&#21161;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#22522;&#20110;&#24213;&#23618;&#39640;&#26031;&#31354;&#38388;&#30340;&#31163;&#25955;&#31361;&#21464;&#37325;&#26500;&#65292;&#20174;&#32780;&#22686;&#24378;&#20854;&#24674;&#22797;&#26465;&#20214;&#20449;&#21495;&#30340;&#33021;&#21147;&#12290;&#22312;&#37319;&#26679;&#38454;&#27573;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#26368;&#20808;&#36827;&#30340;&#36830;&#32493;&#31354;&#38388;ODE&#27714;&#35299;&#22120;&#26469;&#21152;&#24555;&#37319;&#26679;&#36807;&#31243;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#23558;&#35757;&#32451;&#25910;&#25947;&#36895;&#24230;&#25552;&#39640;&#20102;4&#20493;&#65292;&#24182;&#20197;800&#20493;&#30340;&#36895;&#24230;&#29983;&#25104;&#30456;&#36817;&#36136;&#37327;&#30340;&#26679;&#26412;&#65292;&#20351;&#20854;&#26356;&#25509;&#36817;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have gained prominence in generating high-quality sequences of text. Nevertheless, current approaches predominantly represent discrete text within a continuous diffusion space, which incurs substantial computational overhead during training and results in slower sampling speeds. In this paper, we introduce a soft absorbing state that facilitates the diffusion model in learning to reconstruct discrete mutations based on the underlying Gaussian space, thereby enhancing its capacity to recover conditional signals. During the sampling phase, we employ state-of-the-art ODE solvers within the continuous space to expedite the sampling process. Comprehensive experimental evaluations reveal that our proposed method effectively accelerates the training convergence by 4x and generates samples of similar quality 800x faster, rendering it significantly closer to practical application. \footnote{The code is released at \url{https://github.com/Shark-NLP/DiffuSeq}
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LaSyn&#30340;&#25991;&#26412;&#25968;&#25454;&#21033;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#25991;&#26412;&#25968;&#25454;&#36716;&#25442;&#20026;&#20013;&#38388;&#28508;&#21464;&#34920;&#31034;&#26469;&#22686;&#24378;&#31471;&#21040;&#31471;&#35821;&#38899;&#22788;&#29702;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#30340;&#35821;&#38899;&#35782;&#21035;&#21644;&#21475;&#35821;&#29702;&#35299;&#20219;&#21153;&#20013;&#65292;LaSyn&#30456;&#23545;&#35789;&#38169;&#35823;&#29575;&#20943;&#23569;&#20102;22.3%&#65292;&#32477;&#23545;&#24847;&#22270;&#20998;&#31867;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;4.1%&#12290;</title><link>http://arxiv.org/abs/2310.05374</link><description>&lt;p&gt;
&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#25991;&#26412;&#25968;&#25454;&#21512;&#25104;&#25552;&#39640;&#31471;&#21040;&#31471;&#35821;&#38899;&#22788;&#29702;&#30340;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Improving End-to-End Speech Processing by Efficient Text Data Utilization with Latent Synthesis. (arXiv:2310.05374v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05374
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LaSyn&#30340;&#25991;&#26412;&#25968;&#25454;&#21033;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#25991;&#26412;&#25968;&#25454;&#36716;&#25442;&#20026;&#20013;&#38388;&#28508;&#21464;&#34920;&#31034;&#26469;&#22686;&#24378;&#31471;&#21040;&#31471;&#35821;&#38899;&#22788;&#29702;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#30340;&#35821;&#38899;&#35782;&#21035;&#21644;&#21475;&#35821;&#29702;&#35299;&#20219;&#21153;&#20013;&#65292;LaSyn&#30456;&#23545;&#35789;&#38169;&#35823;&#29575;&#20943;&#23569;&#20102;22.3%&#65292;&#32477;&#23545;&#24847;&#22270;&#20998;&#31867;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;4.1%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#20013;&#24515;&#30340;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#65292;&#22521;&#35757;&#39640;&#24615;&#33021;&#30340;&#31471;&#21040;&#31471;&#35821;&#38899;&#22788;&#29702;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#30340;&#35821;&#38899;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#19982;&#25991;&#26412;&#25968;&#25454;&#30456;&#27604;&#65292;&#26631;&#35760;&#30340;&#35821;&#38899;&#25968;&#25454;&#36890;&#24120;&#26356;&#21152;&#31232;&#32570;&#21644;&#26114;&#36149;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LaSyn&#30340;&#26377;&#25928;&#30340;&#25991;&#26412;&#25968;&#25454;&#21033;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#31471;&#21040;&#31471;&#35821;&#38899;&#22788;&#29702;&#27169;&#22411;&#12290;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#28508;&#21464;&#21512;&#25104;&#22120;&#23558;&#25991;&#26412;&#25968;&#25454;&#36716;&#25442;&#20026;&#39044;&#35757;&#32451;&#35821;&#38899;&#27169;&#22411;&#30340;&#20013;&#38388;&#28508;&#21464;&#34920;&#31034;&#12290;&#36825;&#20123;&#20266;&#22768;&#23398;&#34920;&#31034;&#29992;&#20110;&#22686;&#24378;&#27169;&#22411;&#35757;&#32451;&#30340;&#22768;&#23398;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#20302;&#36164;&#28304;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#21644;&#21475;&#35821;&#29702;&#35299;&#65288;SLU&#65289;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;LaSyn&#12290;&#23545;&#20110;ASR&#65292;LaSyn&#25913;&#36827;&#20102;&#22312;LibriSpeech train-clean-100&#19978;&#35757;&#32451;&#30340;E2E&#22522;&#32447;&#65292;&#22312;&#19981;&#21516;&#30340;&#27979;&#35797;&#38598;&#19978;&#30456;&#23545;&#35789;&#38169;&#35823;&#29575;&#20943;&#23569;&#20102;22.3%&#12290;&#23545;&#20110;SLU&#65292;LaSyn&#25913;&#36827;&#20102;&#25105;&#20204;&#30340;E2E&#22522;&#32447;&#65292;&#32477;&#23545;&#24847;&#22270;&#20998;&#31867;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;4.1%&#12290;
&lt;/p&gt;
&lt;p&gt;
Training a high performance end-to-end speech (E2E) processing model requires an enormous amount of labeled speech data, especially in the era of data-centric artificial intelligence. However, labeled speech data are usually scarcer and more expensive for collection, compared to textual data. We propose Latent Synthesis (LaSyn), an efficient textual data utilization framework for E2E speech processing models. We train a latent synthesizer to convert textual data into an intermediate latent representation of a pre-trained speech model. These pseudo acoustic representations of textual data augment acoustic data for model training. We evaluate LaSyn on low-resource automatic speech recognition (ASR) and spoken language understanding (SLU) tasks. For ASR, LaSyn improves an E2E baseline trained on LibriSpeech train-clean-100, with relative word error rate reductions over 22.3% on different test sets. For SLU, LaSyn improves our E2E baseline by absolute 4.1% for intent classification accurac
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25968;&#25454;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;&#22686;&#24378;&#30740;&#31350;&#25552;&#26696;&#26469;&#35299;&#20915;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#23384;&#22312;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#19979;&#28216;&#20027;&#39064;&#27169;&#22411;&#23545;&#25552;&#26696;&#25152;&#23646;&#23398;&#31185;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#35299;&#20915;AI&#36741;&#21161;&#35780;&#23457;&#21592;&#20998;&#37197;&#31995;&#32479;&#20013;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.05318</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;LLM&#30340;&#25968;&#25454;&#22686;&#24378;&#35299;&#20915;&#23618;&#32423;&#23398;&#31185;&#20027;&#39064;&#25512;&#26029;&#20013;&#30340;&#19981;&#24179;&#34913;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Resolving the Imbalance Issue in Hierarchical Disciplinary Topic Inference via LLM-based Data Augmentation. (arXiv:2310.05318v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05318
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25968;&#25454;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;&#22686;&#24378;&#30740;&#31350;&#25552;&#26696;&#26469;&#35299;&#20915;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#23384;&#22312;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#19979;&#28216;&#20027;&#39064;&#27169;&#22411;&#23545;&#25552;&#26696;&#25152;&#23646;&#23398;&#31185;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#35299;&#20915;AI&#36741;&#21161;&#35780;&#23457;&#21592;&#20998;&#37197;&#31995;&#32479;&#20013;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#65292;&#20026;&#35299;&#20915;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#24050;&#25104;&#20026;&#20851;&#38190;&#24615;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#36164;&#37329;&#30003;&#35831;&#36807;&#31243;&#20013;&#25552;&#20132;&#30340;&#30740;&#31350;&#25552;&#26696;&#20013;&#23384;&#22312;&#36825;&#31181;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#36825;&#31181;&#19981;&#24179;&#34913;&#30001;&#23398;&#31185;&#30340;&#21463;&#27426;&#36814;&#31243;&#24230;&#25110;&#36328;&#23398;&#31185;&#30740;&#31350;&#30340;&#20986;&#29616;&#24341;&#36215;&#65292;&#20005;&#37325;&#24433;&#21709;&#20102;&#33021;&#22815;&#25512;&#26029;&#20986;&#36825;&#20123;&#25552;&#26696;&#25152;&#23646;&#23398;&#31185;&#30340;&#19979;&#28216;&#20027;&#39064;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#25968;&#25454;&#23618;&#38754;&#19978;&#65292;&#30001;&#19987;&#23478;&#21644;&#31185;&#23398;&#23478;&#25776;&#20889;&#30340;&#25552;&#26696;&#26412;&#36136;&#19978;&#26159;&#22797;&#26434;&#30340;&#25216;&#26415;&#25991;&#26412;&#65292;&#20805;&#26021;&#30528;&#22797;&#26434;&#30340;&#26415;&#35821;&#65292;&#22240;&#27492;&#22686;&#24378;&#36825;&#31181;&#19987;&#19994;&#25991;&#26412;&#25968;&#25454;&#24102;&#26469;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#22312;&#31995;&#32479;&#23618;&#38754;&#19978;&#65292;&#36825;&#21453;&#36807;&#26469;&#25439;&#23475;&#20102;AI&#36741;&#21161;&#35780;&#23457;&#21592;&#20998;&#37197;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#65292;&#20174;&#32780;&#24341;&#21457;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#20851;&#27880;&#28857;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;Llama V1&#65289;&#20316;&#20026;&#25968;&#25454;&#29983;&#25104;&#22120;&#26469;&#22686;&#24378;&#20998;&#31867;&#22312;&#22797;&#26434;&#23398;&#31185;&#20998;&#31867;&#20013;&#30340;&#30740;&#31350;&#25552;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
In addressing the imbalanced issue of data within the realm of Natural Language Processing, text data augmentation methods have emerged as pivotal solutions. This data imbalance is prevalent in the research proposals submitted during the funding application process. Such imbalances, resulting from the varying popularity of disciplines or the emergence of interdisciplinary studies, significantly impede the precision of downstream topic models that deduce the affiliated disciplines of these proposals. At the data level, proposals penned by experts and scientists are inherently complex technological texts, replete with intricate terminologies, which augmenting such specialized text data poses unique challenges. At the system level, this, in turn, compromises the fairness of AI-assisted reviewer assignment systems, which raises a spotlight on solving this issue. This study leverages large language models (Llama V1) as data generators to augment research proposals categorized within intrica
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#33258;&#36866;&#24212;&#20998;&#35789;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#20998;&#35789;&#36807;&#31243;&#26469;&#22686;&#24378;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#20013;&#30340;&#38271;&#25991;&#26412;&#29983;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20943;&#23569;&#26631;&#35760;&#25968;&#37327;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;&#29983;&#25104;&#24615;&#33021;&#65292;&#24182;&#19988;&#21487;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.05317</link><description>&lt;p&gt;
&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#20013;&#36890;&#36807;&#20219;&#21153;&#33258;&#36866;&#24212;&#20998;&#35789;&#26469;&#22686;&#24378;&#38271;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Enhancing Long-form Text Generation in Mental Health with Task-adaptive Tokenization. (arXiv:2310.05317v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05317
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#33258;&#36866;&#24212;&#20998;&#35789;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#20998;&#35789;&#36807;&#31243;&#26469;&#22686;&#24378;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#20013;&#30340;&#38271;&#25991;&#26412;&#29983;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20943;&#23569;&#26631;&#35760;&#25968;&#37327;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;&#29983;&#25104;&#24615;&#33021;&#65292;&#24182;&#19988;&#21487;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20219;&#21153;&#33258;&#36866;&#24212;&#20998;&#35789;&#20316;&#20026;&#19968;&#31181;&#26041;&#24335;&#65292;&#23558;&#29983;&#25104;&#27969;&#27700;&#32447;&#36866;&#24212;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#29305;&#23450;&#35201;&#27714;&#65292;&#24182;&#22686;&#24378;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#30340;&#38271;&#25991;&#26412;&#29983;&#25104;&#12290;&#21463;&#35748;&#30693;&#31185;&#23398;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#20219;&#21153;&#33258;&#36866;&#24212;&#20998;&#35789;&#22120;&#20174;&#22810;&#20010;&#32467;&#26524;&#20013;&#37319;&#26679;&#21487;&#21464;&#30340;&#20998;&#27573;&#65292;&#37319;&#26679;&#27010;&#29575;&#22522;&#20110;&#20219;&#21153;&#29305;&#23450;&#30340;&#25968;&#25454;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26500;&#24314;&#19987;&#29992;&#35789;&#27719;&#30340;&#31574;&#30053;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#35789;&#27719;&#21512;&#24182;&#21327;&#35758;&#65292;&#21487;&#20197;&#23558;&#20219;&#21153;&#29305;&#23450;&#30340;&#26631;&#35760;&#25972;&#21512;&#21040;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20998;&#35789;&#27493;&#39588;&#20013;&#12290;&#36890;&#36807;&#23545;&#20013;&#33521;&#25991;&#24515;&#29702;&#38382;&#31572;&#20219;&#21153;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#20219;&#21153;&#33258;&#36866;&#24212;&#20998;&#35789;&#26041;&#27861;&#22312;&#20351;&#29992;&#26356;&#23569;&#30340;&#26631;&#35760;&#30340;&#24773;&#20917;&#19979;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#29983;&#25104;&#24615;&#33021;&#25552;&#21319;&#65292;&#26368;&#39640;&#21487;&#36798;60%&#12290;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#20998;&#35789;&#26041;&#27861;&#19982;&#38750;&#24120;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#33021;&#22815;&#24471;&#21040;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose task-adaptive tokenization as a way to adapt the generation pipeline to the specifics of a downstream task and enhance long-form generation in mental health. Inspired by insights from cognitive science, our task-adaptive tokenizer samples variable segmentations from multiple outcomes, with sampling probabilities optimized based on task-specific data. We introduce a strategy for building a specialized vocabulary and introduce a vocabulary merging protocol that allows for the integration of task-specific tokens into the pre-trained model's tokenization step. Through extensive experiments on psychological question-answering tasks in both Chinese and English, we find that our task-adaptive tokenization approach brings a significant improvement in generation performance while using up to 60% fewer tokens. Preliminary experiments point to promising results when using our tokenization approach with very large language models.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#27425;&#24863;&#30693;&#32852;&#21512;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65288;HJCL&#65289;&#65292;&#29992;&#20110;&#23618;&#27425;&#21270;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#23454;&#20363;&#32423;&#21644;&#26631;&#31614;&#32423;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#65292;&#20197;&#21450;&#31934;&#24515;&#26500;&#24314;&#25209;&#27425;&#26469;&#22788;&#29702;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#65292;&#35299;&#20915;&#20102;&#22312;HMTC&#20013;&#20351;&#29992;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.05128</link><description>&lt;p&gt;
&#23454;&#20363;&#21644;&#26631;&#31614;: &#38024;&#23545;&#23618;&#27425;&#21270;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#30340;&#23618;&#27425;&#24863;&#30693;&#32852;&#21512;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Instances and Labels: Hierarchy-aware Joint Supervised Contrastive Learning for Hierarchical Multi-Label Text Classification. (arXiv:2310.05128v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05128
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#27425;&#24863;&#30693;&#32852;&#21512;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65288;HJCL&#65289;&#65292;&#29992;&#20110;&#23618;&#27425;&#21270;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#23454;&#20363;&#32423;&#21644;&#26631;&#31614;&#32423;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#65292;&#20197;&#21450;&#31934;&#24515;&#26500;&#24314;&#25209;&#27425;&#26469;&#22788;&#29702;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#65292;&#35299;&#20915;&#20102;&#22312;HMTC&#20013;&#20351;&#29992;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23618;&#27425;&#21270;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#65288;HMTC&#65289;&#26088;&#22312;&#21033;&#29992;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#36827;&#34892;&#22810;&#26631;&#31614;&#20998;&#31867;&#12290;&#36817;&#26399;&#20851;&#20110;HMTC&#30340;&#26041;&#27861;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#22312;&#29983;&#25104;&#30340;&#26679;&#26412;&#19978;&#20197;&#21322;&#30417;&#30563;&#30340;&#26041;&#24335;&#23558;&#25991;&#26412;&#21644;&#26631;&#31614;&#23884;&#20837;&#25509;&#36817;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#23545;&#36755;&#20986;&#31354;&#38388;&#26045;&#21152;&#36807;&#24230;&#32422;&#26463;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#26679;&#26412;&#30340;&#29983;&#25104;&#24448;&#24448;&#24341;&#20837;&#22122;&#22768;&#65292;&#22240;&#20026;&#23427;&#24573;&#30053;&#20102;&#21516;&#19968;&#25209;&#27425;&#20013;&#30456;&#20284;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#26041;&#27861;&#26159;&#20351;&#29992;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65292;&#20294;&#30001;&#20110;&#20854;&#22797;&#26434;&#30340;&#32467;&#26500;&#21270;&#26631;&#31614;&#65292;&#36825;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;$\textbf{HJCL}$&#30340;&#23618;&#27425;&#24863;&#30693;&#32852;&#21512;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22635;&#34917;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#21644;HMTC&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#23454;&#20363;&#32423;&#21644;&#26631;&#31614;&#32423;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#65292;&#24182;&#20180;&#32454;&#26500;&#36896;&#25209;&#27425;&#26469;&#28385;&#36275;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hierarchical multi-label text classification (HMTC) aims at utilizing a label hierarchy in multi-label classification. Recent approaches to HMTC deal with the problem of imposing an over-constrained premise on the output space by using contrastive learning on generated samples in a semi-supervised manner to bring text and label embeddings closer. However, the generation of samples tends to introduce noise as it ignores the correlation between similar samples in the same batch. One solution to this issue is supervised contrastive learning, but it remains an underexplored topic in HMTC due to its complex structured labels. To overcome this challenge, we propose $\textbf{HJCL}$, a $\textbf{H}$ierarchy-aware $\textbf{J}$oint Supervised $\textbf{C}$ontrastive $\textbf{L}$earning method that bridges the gap between supervised contrastive learning and HMTC. Specifically, we employ both instance-wise and label-wise contrastive learning techniques and carefully construct batches to fulfill the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#20013;&#38388;&#23618;&#21464;&#25442;&#30340;&#24179;&#28369;&#24615;&#26469;&#26816;&#27979;&#24102;&#22806;&#25968;&#25454;&#30340;&#26041;&#27861;(BLOOD),&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#27809;&#26377;&#35757;&#32451;&#25968;&#25454;&#35775;&#38382;&#26435;&#38480;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#22312;Transformer&#32593;&#32476;&#19978;&#30340;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.02832</link><description>&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#23618;&#38388;&#21464;&#25442;&#30340;&#24179;&#28369;&#24615;&#36827;&#34892;&#24102;&#22806;&#20998;&#24067;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Out-of-Distribution Detection by Leveraging Between-Layer Transformation Smoothness. (arXiv:2310.02832v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02832
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#20013;&#38388;&#23618;&#21464;&#25442;&#30340;&#24179;&#28369;&#24615;&#26469;&#26816;&#27979;&#24102;&#22806;&#25968;&#25454;&#30340;&#26041;&#27861;(BLOOD),&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#27809;&#26377;&#35757;&#32451;&#25968;&#25454;&#35775;&#38382;&#26435;&#38480;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#22312;Transformer&#32593;&#32476;&#19978;&#30340;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#24102;&#22806;&#20998;&#24067;&#26816;&#27979;&#23545;&#20110;&#21487;&#38752;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#22823;&#22810;&#25968;&#24403;&#21069;&#26041;&#27861;&#30001;&#20110;&#38656;&#35201;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#25110;&#32773;&#24178;&#39044;&#35757;&#32451;&#32780;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21463;&#21040;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32593;&#32476;&#20013;&#38388;&#23618;&#30340;&#21464;&#25442;&#24179;&#28369;&#24615;&#26469;&#26816;&#27979;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24102;&#22806;&#25968;&#25454;&#65288;BLOOD&#65289;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#27809;&#26377;&#35757;&#32451;&#25968;&#25454;&#35775;&#38382;&#26435;&#38480;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;BLOOD&#21033;&#29992;&#20869;&#20998;&#24067;&#65288;ID&#65289;&#25968;&#25454;&#30340;&#23618;&#38388;&#34920;&#31034;&#21464;&#25442;&#30456;&#36739;&#20110;&#24102;&#22806;&#25968;&#25454;&#30340;&#21464;&#25442;&#26356;&#24179;&#28369;&#30340;&#20542;&#21521;&#65292;&#36825;&#20063;&#26159;&#25105;&#20204;&#22312;Transformer&#32593;&#32476;&#20013;&#32463;&#39564;&#35777;&#26126;&#30340;&#19968;&#20010;&#29305;&#24615;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;BLOOD&#19982;Transformer&#32593;&#32476;&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;&#36164;&#28304;&#38656;&#27714;&#30456;&#24403;&#30340;&#26041;&#27861;&#19978;&#24615;&#33021;&#26356;&#22909;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#36824;&#34920;&#26126;&#65292;&#24403;&#23398;&#20064;&#26356;&#31616;&#21333;&#30340;&#20219;&#21153;&#26102;&#65292;&#24102;&#22806;&#25968;&#25454;&#30340;&#21464;&#25442;&#20250;&#20445;&#25345;&#20854;&#21407;&#22987;&#30340;&#38160;&#24230;&#65292;&#32780;&#38160;&#24230;&#20250;&#38543;&#30528;&#20219;&#21153;&#30340;&#22686;&#21152;&#32780;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective OOD detection is crucial for reliable machine learning models, yet most current methods are limited in practical use due to requirements like access to training data or intervention in training. We present a novel method for detecting OOD data in deep neural networks based on transformation smoothness between intermediate layers of a network (BLOOD), which is applicable to pre-trained models without access to training data. BLOOD utilizes the tendency of between-layer representation transformations of in-distribution (ID) data to be smoother than the corresponding transformations of OOD data, a property that we also demonstrate empirically for Transformer networks. We evaluate BLOOD on several text classification tasks with Transformer networks and demonstrate that it outperforms methods with comparable resource requirements. Our analysis also suggests that when learning simpler tasks, OOD data transformations maintain their original sharpness, whereas sharpness increases wit
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#29702;&#30340;&#20855;&#36523;&#20915;&#31574;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#22522;&#20934;PCA-EVAL&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#20195;&#29702;&#21327;&#20316;&#26694;&#26550;HOLMES&#65292;&#20197;&#25552;&#39640;&#20915;&#31574;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;GPT4-Vision&#27169;&#22411;&#22312;&#31471;&#21040;&#31471;&#30340;&#20855;&#36523;&#20915;&#31574;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2310.02071</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#31471;&#21040;&#31471;&#30340;&#20855;&#36523;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond. (arXiv:2310.02071v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#29702;&#30340;&#20855;&#36523;&#20915;&#31574;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#22522;&#20934;PCA-EVAL&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#20195;&#29702;&#21327;&#20316;&#26694;&#26550;HOLMES&#65292;&#20197;&#25552;&#39640;&#20915;&#31574;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;GPT4-Vision&#27169;&#22411;&#22312;&#31471;&#21040;&#31471;&#30340;&#20855;&#36523;&#20915;&#31574;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#22312;&#25913;&#36827;&#20195;&#29702;&#30340;&#20855;&#36523;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#28508;&#21147;&#12290;&#23613;&#31649;&#30001;&#20110;&#20854;&#20808;&#36827;&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#24191;&#27867;&#30340;&#19990;&#30028;&#30693;&#35782;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#20687;GPT4-Vision&#36825;&#26679;&#30340;MLLM&#25552;&#20379;&#20102;&#22686;&#24378;&#30340;&#35270;&#35273;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#26368;&#20808;&#36827;&#30340;MLLMs&#33021;&#21542;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#22788;&#29702;&#20855;&#36523;&#20915;&#31574;&#65292;&#24182;&#19988;LLMs&#21644;MLLMs&#20043;&#38388;&#30340;&#21327;&#20316;&#26159;&#21542;&#33021;&#22686;&#24378;&#20915;&#31574;&#33021;&#21147;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;PCA-EVAL&#30340;&#26032;&#22522;&#20934;&#65292;&#35813;&#22522;&#20934;&#20174;&#24863;&#30693;&#12289;&#35748;&#30693;&#21644;&#34892;&#21160;&#30340;&#35282;&#24230;&#35780;&#20272;&#20855;&#36523;&#20915;&#31574;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HOLMES&#65292;&#19968;&#20010;&#22810;&#20195;&#29702;&#21327;&#20316;&#26694;&#26550;&#65292;&#20801;&#35768;LLMs&#21033;&#29992;MLLMs&#21644;APIs&#33719;&#21462;&#22810;&#27169;&#24577;&#20449;&#24687;&#20197;&#36827;&#34892;&#26126;&#26234;&#30340;&#20915;&#31574;&#12290;&#25105;&#20204;&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#19978;&#27604;&#36739;&#20102;&#31471;&#21040;&#31471;&#30340;&#20855;&#36523;&#20915;&#31574;&#21644;HOLMES&#65292;&#24182;&#21457;&#29616;GPT4-Vision&#27169;&#22411;&#30340;&#24615;&#33021;&#26368;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we explore the potential of Multimodal Large Language Models (MLLMs) in improving embodied decision-making processes for agents. While Large Language Models (LLMs) have been widely used due to their advanced reasoning skills and vast world knowledge, MLLMs like GPT4-Vision offer enhanced visual understanding and reasoning capabilities. We investigate whether state-of-the-art MLLMs can handle embodied decision-making in an end-to-end manner and whether collaborations between LLMs and MLLMs can enhance decision-making. To address these questions, we introduce a new benchmark called PCA-EVAL, which evaluates embodied decision-making from the perspectives of Perception, Cognition, and Action. Additionally, we propose HOLMES, a multi-agent cooperation framework that allows LLMs to leverage MLLMs and APIs to gather multimodal information for informed decision-making. We compare end-to-end embodied decision-making and HOLMES on our benchmark and find that the GPT4-Vision model 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#34701;&#21512;&#23545;&#35937;&#32423;&#22810;&#27169;&#24577;LLM&#26550;&#26500;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21521;&#37327;&#21270;&#30340;&#25968;&#23383;&#27169;&#24577;&#19982;&#39044;&#35757;&#32451;&#30340;LLM&#30456;&#32467;&#21512;&#26469;&#25552;&#39640;&#33258;&#21160;&#39550;&#39542;&#20013;&#23545;&#19978;&#19979;&#25991;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#21516;&#26102;&#36824;&#20351;&#29992;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;LLM&#39537;&#21160;&#31243;&#24207;&#22312;&#35299;&#37322;&#39550;&#39542;&#24773;&#22659;&#12289;&#22238;&#31572;&#38382;&#39064;&#21644;&#20915;&#31574;&#26041;&#38754;&#30340;&#25928;&#26524;&#20248;&#20110;&#20256;&#32479;&#30340;&#34892;&#20026;&#20811;&#38534;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.01957</link><description>&lt;p&gt;
&#20351;&#29992;LLM&#36827;&#34892;&#39550;&#39542;&#65306;&#34701;&#21512;&#23545;&#35937;&#32423;&#21521;&#37327;&#27169;&#24577;&#20197;&#35299;&#37322;&#33258;&#21160;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
Driving with LLMs: Fusing Object-Level Vector Modality for Explainable Autonomous Driving. (arXiv:2310.01957v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01957
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#34701;&#21512;&#23545;&#35937;&#32423;&#22810;&#27169;&#24577;LLM&#26550;&#26500;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21521;&#37327;&#21270;&#30340;&#25968;&#23383;&#27169;&#24577;&#19982;&#39044;&#35757;&#32451;&#30340;LLM&#30456;&#32467;&#21512;&#26469;&#25552;&#39640;&#33258;&#21160;&#39550;&#39542;&#20013;&#23545;&#19978;&#19979;&#25991;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#21516;&#26102;&#36824;&#20351;&#29992;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;LLM&#39537;&#21160;&#31243;&#24207;&#22312;&#35299;&#37322;&#39550;&#39542;&#24773;&#22659;&#12289;&#22238;&#31572;&#38382;&#39064;&#21644;&#20915;&#31574;&#26041;&#38754;&#30340;&#25928;&#26524;&#20248;&#20110;&#20256;&#32479;&#30340;&#34892;&#20026;&#20811;&#38534;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#27867;&#21270;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#23545;&#35937;&#32423;&#22810;&#27169;&#24577;LLM&#26550;&#26500;&#65292;&#23558;&#21521;&#37327;&#21270;&#30340;&#25968;&#23383;&#27169;&#24577;&#19982;&#39044;&#35757;&#32451;&#30340;LLM&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#39550;&#39542;&#24773;&#22659;&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#26469;&#33258;10k&#20010;&#39550;&#39542;&#24773;&#22659;&#30340;160k&#20010;&#38382;&#31572;&#23545;&#65292;&#36825;&#20123;&#38382;&#31572;&#23545;&#19982;&#30001;RL&#20195;&#29702;&#25910;&#38598;&#30340;&#39640;&#36136;&#37327;&#25511;&#21046;&#21629;&#20196;&#21644;&#30001;&#25945;&#24072;LLM&#65288;GPT-3.5&#65289;&#29983;&#25104;&#30340;&#38382;&#39064;&#31572;&#26696;&#23545;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#20351;&#29992;&#21521;&#37327;&#23383;&#24149;&#35821;&#35328;&#25968;&#25454;&#26469;&#23545;&#40784;&#25968;&#23383;&#21521;&#37327;&#27169;&#24577;&#21644;&#38745;&#24577;LLM&#34920;&#31034;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#39550;&#39542;&#38382;&#31572;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;LLM&#39537;&#21160;&#31243;&#24207;&#22312;&#35299;&#37322;&#39550;&#39542;&#24773;&#22659;&#12289;&#22238;&#31572;&#38382;&#39064;&#21644;&#20915;&#31574;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#31361;&#26174;&#20102;&#22522;&#20110;LLM&#30340;&#39550;&#39542;&#34892;&#20026;&#29983;&#25104;&#19982;&#20256;&#32479;&#34892;&#20026;&#20811;&#38534;&#30456;&#27604;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25105;&#20204;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown promise in the autonomous driving sector, particularly in generalization and interpretability. We introduce a unique object-level multimodal LLM architecture that merges vectorized numeric modalities with a pre-trained LLM to improve context understanding in driving situations. We also present a new dataset of 160k QA pairs derived from 10k driving scenarios, paired with high quality control commands collected with RL agent and question answer pairs generated by teacher LLM (GPT-3.5). A distinct pretraining strategy is devised to align numeric vector modalities with static LLM representations using vector captioning language data. We also introduce an evaluation metric for Driving QA and demonstrate our LLM-driver's proficiency in interpreting driving scenarios, answering questions, and decision-making. Our findings highlight the potential of LLM-based driving action generation in comparison to traditional behavioral cloning. We make our benchmar
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#25552;&#31034;(IDP)&#30340;&#26426;&#21046;&#65292;&#23427;&#21487;&#20197;&#20316;&#20026;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#36866;&#24212;&#24037;&#20855;&#65292;&#20462;&#22797;&#21387;&#32553;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#19968;&#20123;&#23454;&#38469;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2310.00867</link><description>&lt;p&gt;
(&#21160;&#24577;)&#25552;&#31034;&#21487;&#33021;&#26159;&#20462;&#22797;&#21387;&#32553;LLMs&#25152;&#38656;&#30340;&#20840;&#37096;&#12290;(arXiv:2310.00867v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
(Dynamic) Prompting might be all you need to repair Compressed LLMs. (arXiv:2310.00867v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00867
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#25552;&#31034;(IDP)&#30340;&#26426;&#21046;&#65292;&#23427;&#21487;&#20197;&#20316;&#20026;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#36866;&#24212;&#24037;&#20855;&#65292;&#20462;&#22797;&#21387;&#32553;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#19968;&#20123;&#23454;&#38469;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#26377;&#30528;&#37325;&#22823;&#30340;&#21464;&#38761;&#65292;&#20294;&#21516;&#26102;&#20063;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#35745;&#31639;&#38656;&#27714;&#65292;&#24378;&#35843;&#20102;&#39640;&#25928;&#12289;&#26080;&#38656;&#35757;&#32451;&#30340;&#21387;&#32553;&#30340;&#38656;&#27714;&#12290;&#23613;&#31649;&#38024;&#23545;&#26368;&#22823;&#30340;LLMs&#22312;&#26080;&#38656;&#35757;&#32451;&#30340;&#21387;&#32553;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20294;&#25105;&#20204;&#20351;&#29992;LLaMA-7B&#21644;OPT-6.7b&#36827;&#34892;&#30340;&#27979;&#35797;&#26174;&#31034;&#65292;&#22312;&#19968;&#20123;&#23454;&#38469;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#23384;&#22312;&#26174;&#33879;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#23545;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#21387;&#32553;&#21518;&#37325;&#26032;&#35757;&#32451;&#30340;&#26435;&#34913;&#30340;&#35843;&#26597;&#34920;&#26126;&#65292;&#25552;&#31034;&#39537;&#21160;&#30340;&#24674;&#22797;&#20316;&#20026;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#36866;&#24212;&#24037;&#20855;&#20855;&#26377;&#28508;&#22312;&#30340;&#21069;&#26223;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#23616;&#38480;&#22312;&#22256;&#24785;&#24230;&#35780;&#20272;&#21644;&#31616;&#21333;&#20219;&#21153;&#19978;&#65292;&#23545;&#25552;&#31034;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#36890;&#29992;&#24615;&#27809;&#26377;&#32473;&#20986;&#26126;&#30830;&#30340;&#20449;&#24515;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#31181;&#20851;&#38190;&#26041;&#27861;&#35299;&#20915;&#20102;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;LLM&#21387;&#32553;&#20013;&#22825;&#30495;&#25552;&#31034;&#30340;&#33030;&#24369;&#24615;&#65292;&#21363;&#36807;&#24230;&#20381;&#36182;&#21333;&#19968;&#36755;&#20837;&#30340;&#25552;&#31034;&#12290;&#20316;&#20026;&#22238;&#24212;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25512;&#29702;&#26102;&#21160;&#24577;&#25552;&#31034;(IDP)&#30340;&#26426;&#21046;&#65292;&#23427;&#21487;&#20197;&#33258;&#20027;&#36873;&#25321;&#26368;&#20339;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), while transformative for NLP, come with significant computational demands, underlining the need for efficient, training-free compression. Notably, despite the marked improvement in training-free compression for the largest of LLMs, our tests using LLaMA-7B and OPT-6.7b highlight a significant performance drop in several realistic downstream tasks. Investigation into the trade-off between resource-intensive post-compression re-training highlights the prospect of prompt-driven recovery as a lightweight adaption tool. However, existing studies, confined mainly to perplexity evaluations and simple tasks, fail to offer unequivocal confidence in the scalability and generalizability of prompting. We tackle this uncertainty in two key ways. First, we uncover the vulnerability of naive prompts in LLM compression as an over-reliance on a singular prompt per input. In response, we propose inference-time dynamic prompting (IDP), a mechanism that autonomously chooses f
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Contextualized Bi-Directional Dual Transformer (CBDT) Classifier&#30340;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#25991;&#26412;&#20013;&#30340;&#20559;&#35265;&#26816;&#27979;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#20102;&#20004;&#20010;Transformer&#32593;&#32476;&#65292;&#33021;&#22815;&#20934;&#30830;&#21306;&#20998;&#26377;&#20559;&#35265;&#21644;&#20013;&#31435;&#30340;&#38472;&#36848;&#65292;&#24182;&#25214;&#20986;&#20855;&#20307;&#30340;&#26377;&#20559;&#35265;&#35789;&#27719;&#12290;CBDT&#27169;&#22411;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#26041;&#27861;&#65292;&#24615;&#33021;&#25552;&#21319;&#20102;2-4&#65285;&#65292;&#21516;&#26102;&#36824;&#20026;&#22312;&#19981;&#21516;&#35821;&#35328;&#21644;&#25991;&#21270;&#29615;&#22659;&#20013;&#24212;&#29992;&#35813;&#27169;&#22411;&#25171;&#24320;&#20102;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.00347</link><description>&lt;p&gt;
&#35299;&#38145;&#20559;&#35265;&#26816;&#27979;&#65306;&#21033;&#29992;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#36827;&#34892;&#20869;&#23481;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Unlocking Bias Detection: Leveraging Transformer-Based Models for Content Analysis. (arXiv:2310.00347v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00347
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Contextualized Bi-Directional Dual Transformer (CBDT) Classifier&#30340;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#25991;&#26412;&#20013;&#30340;&#20559;&#35265;&#26816;&#27979;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#20102;&#20004;&#20010;Transformer&#32593;&#32476;&#65292;&#33021;&#22815;&#20934;&#30830;&#21306;&#20998;&#26377;&#20559;&#35265;&#21644;&#20013;&#31435;&#30340;&#38472;&#36848;&#65292;&#24182;&#25214;&#20986;&#20855;&#20307;&#30340;&#26377;&#20559;&#35265;&#35789;&#27719;&#12290;CBDT&#27169;&#22411;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#26041;&#27861;&#65292;&#24615;&#33021;&#25552;&#21319;&#20102;2-4&#65285;&#65292;&#21516;&#26102;&#36824;&#20026;&#22312;&#19981;&#21516;&#35821;&#35328;&#21644;&#25991;&#21270;&#29615;&#22659;&#20013;&#24212;&#29992;&#35813;&#27169;&#22411;&#25171;&#24320;&#20102;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20559;&#35265;&#23545;&#20110;&#24378;&#21270;&#36127;&#38754;&#21051;&#26495;&#21360;&#35937;&#12289;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#21644;&#24433;&#21709;&#20915;&#31574;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#22240;&#27492;&#22312;&#25991;&#26412;&#20013;&#36827;&#34892;&#20559;&#35265;&#26816;&#27979;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#24448;&#24448;&#22312;&#36229;&#20986;&#20854;&#35757;&#32451;&#38598;&#33539;&#22260;&#26102;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Contextualized Bi-Directional Dual Transformer&#65288;CBDT&#65289;&#20998;&#31867;&#22120;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#21033;&#29992;&#20102;&#20004;&#20010;&#21327;&#21516;&#24037;&#20316;&#30340;Transformer&#32593;&#32476;&#65306;Context Transformer&#21644;Entity Transformer&#65292;&#26088;&#22312;&#22686;&#24378;&#20559;&#35265;&#26816;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#20934;&#22791;&#36981;&#24490;FAIR&#21407;&#21017;&#65292;&#30830;&#20445;&#25968;&#25454;&#20351;&#29992;&#20855;&#26377;&#36947;&#24503;&#24615;&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;&#25968;&#25454;&#38598;&#36827;&#34892;&#20005;&#26684;&#27979;&#35797;&#65292;CBDT&#23637;&#31034;&#20102;&#20854;&#22312;&#21306;&#20998;&#26377;&#20559;&#35265;&#19982;&#20013;&#31435;&#38472;&#36848;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#25351;&#20986;&#20855;&#20307;&#30340;&#26377;&#20559;&#35265;&#35789;&#27719;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#22312;&#22522;&#20934;&#24615;&#33021;&#19978;&#23454;&#29616;&#20102;2-4&#65285;&#30340;&#25552;&#21319;&#12290;&#36825;&#20026;&#23558;CBDT&#27169;&#22411;&#22312;&#19981;&#21516;&#30340;&#35821;&#35328;&#21644;&#25991;&#21270;&#29615;&#22659;&#20013;&#36827;&#34892;&#36866;&#24212;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bias detection in text is imperative due to its role in reinforcing negative stereotypes, disseminating misinformation, and influencing decisions. Current language models often fall short in generalizing beyond their training sets. In response, we introduce the Contextualized Bi-Directional Dual Transformer (CBDT) Classifier. This novel architecture utilizes two synergistic transformer networks: the Context Transformer and the Entity Transformer, aiming for enhanced bias detection. Our dataset preparation follows the FAIR principles, ensuring ethical data usage. Through rigorous testing on various datasets, CBDT showcases its ability in distinguishing biased from neutral statements, while also pinpointing exact biased lexemes. Our approach outperforms existing methods, achieving a 2-4\% increase over benchmark performances. This opens avenues for adapting the CBDT model across diverse linguistic and cultural landscapes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#24320;&#28304;&#22522;&#20934;&#27979;&#35797;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#38169;&#35823;&#20462;&#27491;&#65292;&#23454;&#29616;&#20102;&#19982;&#20154;&#31867;&#27700;&#24179;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2309.15701</link><description>&lt;p&gt;
HyPoradise&#65306;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#24335;&#35821;&#38899;&#35782;&#21035;&#30340;&#24320;&#25918;&#22522;&#20934;&#32447;
&lt;/p&gt;
&lt;p&gt;
HyPoradise: An Open Baseline for Generative Speech Recognition with Large Language Models. (arXiv:2309.15701v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#24320;&#28304;&#22522;&#20934;&#27979;&#35797;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#38169;&#35823;&#20462;&#27491;&#65292;&#23454;&#29616;&#20102;&#19982;&#20154;&#31867;&#27700;&#24179;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36827;&#23637;&#20351;&#24471;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#22312;&#20960;&#20010;&#20844;&#24320;&#30340;&#24178;&#20928;&#35821;&#38899;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#20154;&#31867;&#27700;&#24179;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#22312;&#38754;&#23545;&#36870;&#22659;&#26102;&#20063;&#20250;&#20986;&#29616;&#24615;&#33021;&#19979;&#38477;&#65292;&#22240;&#20026;&#33391;&#22909;&#35757;&#32451;&#30340;&#22768;&#23398;&#27169;&#22411;&#23545;&#20110;&#35821;&#38899;&#39046;&#22495;&#30340;&#21464;&#24322;&#24615;&#24456;&#25935;&#24863;&#65292;&#22914;&#32972;&#26223;&#22122;&#22768;&#12290;&#21463;&#21040;&#36825;&#19968;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#24320;&#28304;&#22522;&#20934;&#27979;&#35797;&#65292;&#21033;&#29992;&#22806;&#37096;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#36827;&#34892;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#38169;&#35823;&#20462;&#27491;&#65292;&#20854;&#20013;N&#26368;&#20339;&#35299;&#30721;&#20551;&#35774;&#20026;&#30495;&#23454;&#36716;&#24405;&#39044;&#27979;&#25552;&#20379;&#20102;&#26377;&#20449;&#24687;&#37327;&#30340;&#20803;&#32032;&#12290;&#36825;&#31181;&#26041;&#27861;&#19982;&#20256;&#32479;&#30340;&#35821;&#35328;&#27169;&#22411;&#37325;&#35780;&#20998;&#31574;&#30053;&#19981;&#21516;&#65292;&#21518;&#32773;&#21482;&#33021;&#36873;&#25321;&#19968;&#20010;&#20505;&#36873;&#20551;&#35774;&#20316;&#20026;&#26368;&#32456;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advancements in deep neural networks have allowed automatic speech recognition (ASR) systems to attain human parity on several publicly available clean speech datasets. However, even state-of-the-art ASR systems experience performance degradation when confronted with adverse conditions, as a well-trained acoustic model is sensitive to variations in the speech domain, e.g., background noise. Intuitively, humans address this issue by relying on their linguistic knowledge: the meaning of ambiguous spoken terms is usually inferred from contextual cues thereby reducing the dependency on the auditory system. Inspired by this observation, we introduce the first open-source benchmark to utilize external large language models (LLMs) for ASR error correction, where N-best decoding hypotheses provide informative elements for true transcription prediction. This approach is a paradigm shift from the traditional language model rescoring strategy that can only select one candidate hypothesis as the o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#20840;&#38754;&#35843;&#26597;&#20102;&#24605;&#32500;&#38142;&#25512;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#28085;&#30422;&#20102;&#26500;&#24314;&#12289;&#32467;&#26500;&#21464;&#20307;&#21644;&#22686;&#24378;&#25216;&#26415;&#31561;&#26041;&#27861;&#20998;&#31867;&#65292;&#20197;&#21450;&#35268;&#21010;&#12289;&#24037;&#20855;&#20351;&#29992;&#21644;&#25552;&#28860;&#31561;&#21069;&#27839;&#24212;&#29992;&#12290;&#21516;&#26102;&#35752;&#35770;&#20102;&#25361;&#25112;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;&#36825;&#20221;&#35843;&#26597;&#25253;&#21578;&#23545;&#20110;&#22312;&#24605;&#32500;&#38142;&#25512;&#29702;&#39046;&#22495;&#23547;&#27714;&#21019;&#26032;&#30340;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#26159;&#19968;&#20010;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2309.15402</link><description>&lt;p&gt;
&#20851;&#20110;&#24605;&#32500;&#38142;&#25512;&#29702;&#65306;&#36827;&#23637;&#12289;&#21069;&#27839;&#21644;&#26410;&#26469;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future. (arXiv:2309.15402v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15402
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#20840;&#38754;&#35843;&#26597;&#20102;&#24605;&#32500;&#38142;&#25512;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#28085;&#30422;&#20102;&#26500;&#24314;&#12289;&#32467;&#26500;&#21464;&#20307;&#21644;&#22686;&#24378;&#25216;&#26415;&#31561;&#26041;&#27861;&#20998;&#31867;&#65292;&#20197;&#21450;&#35268;&#21010;&#12289;&#24037;&#20855;&#20351;&#29992;&#21644;&#25552;&#28860;&#31561;&#21069;&#27839;&#24212;&#29992;&#12290;&#21516;&#26102;&#35752;&#35770;&#20102;&#25361;&#25112;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;&#36825;&#20221;&#35843;&#26597;&#25253;&#21578;&#23545;&#20110;&#22312;&#24605;&#32500;&#38142;&#25512;&#29702;&#39046;&#22495;&#23547;&#27714;&#21019;&#26032;&#30340;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#26159;&#19968;&#20010;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24605;&#32500;&#38142;&#25512;&#29702;&#26159;&#20154;&#31867;&#26234;&#33021;&#30340;&#22522;&#26412;&#35748;&#30693;&#36807;&#31243;&#65292;&#22312;&#20154;&#24037;&#26234;&#33021;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20173;&#32570;&#20047;&#19968;&#20221;&#20840;&#38754;&#30340;&#35843;&#26597;&#25253;&#21578;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36808;&#20986;&#20102;&#31532;&#19968;&#27493;&#65292;&#20180;&#32454;&#24191;&#27867;&#22320;&#27010;&#36848;&#20102;&#36825;&#20010;&#30740;&#31350;&#39046;&#22495;&#12290;&#25105;&#20204;&#29992;&#8220;X-of-Thought&#8221;&#26469;&#25351;&#20195;&#24191;&#20041;&#19978;&#30340;&#24605;&#32500;&#38142;&#25512;&#29702;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26681;&#25454;&#26041;&#27861;&#30340;&#20998;&#31867;&#20307;&#31995;&#23545;&#24403;&#21069;&#30340;&#30740;&#31350;&#36827;&#34892;&#20102;&#31995;&#32479;&#32452;&#32455;&#65292;&#21253;&#25324;&#24605;&#32500;&#38142;&#30340;&#26500;&#24314;&#12289;&#32467;&#26500;&#21464;&#20307;&#21644;&#22686;&#24378;&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#24605;&#32500;&#38142;&#22312;&#35268;&#21010;&#12289;&#24037;&#20855;&#20351;&#29992;&#21644;&#25552;&#28860;&#31561;&#39046;&#22495;&#30340;&#21069;&#27839;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#19968;&#20123;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#26041;&#21521;&#65292;&#21253;&#25324;&#24544;&#23454;&#24230;&#12289;&#22810;&#27169;&#24577;&#21644;&#29702;&#35770;&#31561;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#20221;&#35843;&#26597;&#25253;&#21578;&#33021;&#25104;&#20026;&#23547;&#27714;&#22312;&#24605;&#32500;&#38142;&#25512;&#29702;&#39046;&#22495;&#21019;&#26032;&#30340;&#30740;&#31350;&#20154;&#21592;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chain-of-thought reasoning, a cognitive process fundamental to human intelligence, has garnered significant attention in the realm of artificial intelligence and natural language processing. However, there still remains a lack of a comprehensive survey for this arena. To this end, we take the first step and present a thorough survey of this research field carefully and widely. We use X-of-Thought to refer to Chain-of-Thought in a broad sense. In detail, we systematically organize the current research according to the taxonomies of methods, including XoT construction, XoT structure variants, and enhanced XoT. Additionally, we describe XoT with frontier applications, covering planning, tool use, and distillation. Furthermore, we address challenges and discuss some future directions, including faithfulness, multi-modal, and theory. We hope this survey serves as a valuable resource for researchers seeking to innovate within the domain of chain-of-thought reasoning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#21360;&#24230;&#27861;&#24459;&#38382;&#31572;&#31995;&#32479;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;AILQA&#31995;&#32479;&#33021;&#22815;&#33258;&#21160;&#35299;&#26512;&#29992;&#25143;&#30340;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#24182;&#29983;&#25104;&#39640;&#24230;&#20934;&#30830;&#30340;&#21709;&#24212;&#12290;</title><link>http://arxiv.org/abs/2309.14735</link><description>&lt;p&gt;
&#22522;&#20110;&#19981;&#21516;&#26816;&#32034;&#21644;&#38382;&#31572;&#27169;&#22411;&#30340;&#21360;&#24230;&#27861;&#24459;&#38382;&#31572;&#31995;&#32479;&#30340;&#20154;&#24037;&#26234;&#33021;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Comparative Analysis of Artificial Intelligence for Indian Legal Question Answering (AILQA) Using Different Retrieval and QA Models. (arXiv:2309.14735v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#21360;&#24230;&#27861;&#24459;&#38382;&#31572;&#31995;&#32479;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;AILQA&#31995;&#32479;&#33021;&#22815;&#33258;&#21160;&#35299;&#26512;&#29992;&#25143;&#30340;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#24182;&#29983;&#25104;&#39640;&#24230;&#20934;&#30830;&#30340;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27861;&#24459;&#38382;&#31572;&#65288;QA&#65289;&#31995;&#32479;&#26377;&#28508;&#21147;&#25913;&#21464;&#27861;&#24459;&#19987;&#19994;&#20154;&#22763;&#19982;&#26696;&#20363;&#25991;&#20214;&#30340;&#20114;&#21160;&#26041;&#24335;&#12290;&#26412;&#25991;&#23545;&#29616;&#26377;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#20197;&#35780;&#20272;&#20854;&#22312;&#21360;&#24230;&#27861;&#24459;&#20307;&#31995;&#19979;&#22238;&#31572;&#27861;&#24459;&#38382;&#39064;&#30340;&#25928;&#29992;&#65292;&#29305;&#21035;&#20851;&#27880;&#21360;&#24230;&#27861;&#24459;&#38382;&#31572;&#65288;AILQA&#65289;&#24182;&#30740;&#31350;&#20102;&#24403;&#21069;&#21487;&#29992;&#30340;&#19981;&#21516;&#26816;&#32034;&#21644;QA&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#21033;&#29992;OpenAI GPT&#27169;&#22411;&#20316;&#20026;&#22522;&#20934;&#65292;&#32467;&#21512;&#26597;&#35810;&#25552;&#31034;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#29616;&#26377;&#30340;AILQA&#31995;&#32479;&#33021;&#22815;&#33258;&#21160;&#35299;&#26512;&#29992;&#25143;&#30340;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#24182;&#29983;&#25104;&#39640;&#24230;&#20934;&#30830;&#30340;&#21709;&#24212;&#12290;&#26412;&#30740;&#31350;&#29305;&#21035;&#20851;&#27880;&#21360;&#24230;&#21009;&#20107;&#21496;&#27861;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#35813;&#39046;&#22495;&#30001;&#20110;&#22797;&#26434;&#24615;&#21644;&#36164;&#28304;&#38480;&#21046;&#32780;&#38754;&#20020;&#19968;&#31995;&#21015;&#25361;&#25112;&#12290;&#20026;&#20102;&#20005;&#26684;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#32463;&#39564;&#35777;&#35780;&#20272;&#19982;&#20174;&#23454;&#36341;&#20013;&#33719;&#24471;&#30340;&#21453;&#39304;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Legal question-answering (QA) systems have the potential to revolutionize the way legal professionals interact with case law documents. This paper conducts a comparative analysis of existing artificial intelligence models for their utility in answering legal questions within the Indian legal system, specifically focusing on Indian Legal Question Answering (AILQA) and our study investigates the efficacy of different retrieval and QA algorithms currently available. Utilizing the OpenAI GPT model as a benchmark, along with query prompts, our investigation shows that existing AILQA systems can automatically interpret natural language queries from users and generate highly accurate responses. This research is particularly focused on applications within the Indian criminal justice domain, which has its own set of challenges due to its complexity and resource constraints. In order to rigorously assess the performance of these models, empirical evaluations are complemented by feedback from pra
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#12290;&#38024;&#23545;&#35299;&#37322;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#30740;&#31350;&#21457;&#29616;ChatGPT&#33021;&#22815;&#29983;&#25104;&#25509;&#36817;&#20154;&#31867;&#35299;&#37322;&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;LLMs&#22312;&#38646; shot/few-shot &#26041;&#24335;&#19979;&#30340;&#20998;&#31867;&#24615;&#33021;&#20173;&#19981;&#29702;&#24819;&#12290;&#20026;&#20102;&#35299;&#20915;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#21644;&#24320;&#28304;LLMs&#30340;&#38382;&#39064;&#65292;&#30740;&#31350;&#24314;&#31435;&#20102;&#31532;&#19968;&#20010;&#22810;&#20219;&#21153;&#21644;&#22810;&#28304;&#30340;&#35299;&#37322;&#24615;&#24515;&#29702;&#20581;&#24247;&#25351;&#23548;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2309.13567</link><description>&lt;p&gt;
MentaLLaMA&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
MentaLLaMA: Interpretable Mental Health Analysis on Social Media with Large Language Models. (arXiv:2309.13567v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13567
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#12290;&#38024;&#23545;&#35299;&#37322;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#30740;&#31350;&#21457;&#29616;ChatGPT&#33021;&#22815;&#29983;&#25104;&#25509;&#36817;&#20154;&#31867;&#35299;&#37322;&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;LLMs&#22312;&#38646; shot/few-shot &#26041;&#24335;&#19979;&#30340;&#20998;&#31867;&#24615;&#33021;&#20173;&#19981;&#29702;&#24819;&#12290;&#20026;&#20102;&#35299;&#20915;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#21644;&#24320;&#28304;LLMs&#30340;&#38382;&#39064;&#65292;&#30740;&#31350;&#24314;&#31435;&#20102;&#31532;&#19968;&#20010;&#22810;&#20219;&#21153;&#21644;&#22810;&#28304;&#30340;&#35299;&#37322;&#24615;&#24515;&#29702;&#20581;&#24247;&#25351;&#23548;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#32593;&#32476;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#27491;&#22312;&#25104;&#20026;&#33258;&#21160;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#30340;&#20016;&#23500;&#25968;&#25454;&#28304;&#12290;&#30001;&#20110;&#20256;&#32479;&#30340;&#21028;&#21035;&#26041;&#27861;&#23384;&#22312;&#35299;&#37322;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#26368;&#36817;&#24320;&#22987;&#25506;&#32034;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31038;&#20132;&#23186;&#20307;&#19978;&#21487;&#35299;&#37322;&#30340;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#65292;&#26088;&#22312;&#25552;&#20379;&#35814;&#32454;&#30340;&#35299;&#37322;&#21644;&#39044;&#27979;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#33021;&#22815;&#29983;&#25104;&#25509;&#36817;&#20154;&#31867;&#35299;&#37322;&#30340;&#27491;&#30830;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;LLMs&#22312;&#38646; shot/few-shot &#26041;&#24335;&#19979;&#20173;&#28982;&#23454;&#29616;&#20102;&#19981;&#20196;&#20154;&#28385;&#24847;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;&#39046;&#22495;&#29305;&#23450;&#30340;&#24494;&#35843;&#26159;&#19968;&#20010;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#38754;&#20020;&#20004;&#20010;&#25361;&#25112;&#65306;1&#65289;&#32570;&#20047;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;2&#65289;&#27809;&#26377;&#21457;&#24067;&#29992;&#20110;&#21487;&#35299;&#37322;&#30340;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#30340;&#24320;&#28304; LLMs &#20197;&#38477;&#20302;&#24494;&#35843;&#25104;&#26412;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#26500;&#24314;&#20102;&#31532;&#19968;&#20010;&#22810;&#20219;&#21153;&#21644;&#22810;&#28304;&#21487;&#35299;&#37322;&#30340;&#24515;&#29702;&#20581;&#24247;&#25351;&#23548; (IMHI) &#25968;&#25454;&#38598;&#65292;&#21253;&#21547;105K&#20010;&#25968;&#25454;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of web technology, social media texts are becoming a rich source for automatic mental health analysis. As traditional discriminative methods bear the problem of low interpretability, the recent large language models have been explored for interpretable mental health analysis on social media, which aims to provide detailed explanations along with predictions. The results show that ChatGPT can generate approaching-human explanations for its correct classifications. However, LLMs still achieve unsatisfactory classification performance in a zero-shot/few-shot manner. Domain-specific finetuning is an effective solution, but faces 2 challenges: 1) lack of high-quality training data. 2) no open-source LLMs for interpretable mental health analysis were released to lower the finetuning cost. To alleviate these problems, we build the first multi-task and multi-source interpretable mental health instruction (IMHI) dataset on social media, with 105K data samples. The raw socia
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#21512;&#20998;&#26512;&#20102;&#22312;&#20154;&#25991;&#23398;&#31185;&#21644;&#32771;&#21476;&#23398;&#39046;&#22495;&#20013;&#20845;&#20010;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#23398;&#26415;&#20889;&#20316;&#26041;&#38754;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#37325;&#26032;&#32452;&#21512;&#29616;&#26377;&#30693;&#35782;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#20135;&#29983;&#21407;&#21019;&#31185;&#23398;&#20869;&#23481;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.08636</link><description>&lt;p&gt;
ChatGPT v Bard v Bing v Claude 2 v Aria v human-expert. AI&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#31185;&#23398;&#20889;&#20316;&#26041;&#38754;&#34920;&#29616;&#22914;&#20309;&#65311;&#65288;&#31532;23&#23395;&#31532;3&#23395;&#65289;&#12290;&#65288;arXiv:2309.08636v1 [cs.CL]&#65289;
&lt;/p&gt;
&lt;p&gt;
ChatGPT v Bard v Bing v Claude 2 v Aria v human-expert. How good are AI chatbots at scientific writing? (ver. 23Q3). (arXiv:2309.08636v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08636
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#21512;&#20998;&#26512;&#20102;&#22312;&#20154;&#25991;&#23398;&#31185;&#21644;&#32771;&#21476;&#23398;&#39046;&#22495;&#20013;&#20845;&#20010;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#23398;&#26415;&#20889;&#20316;&#26041;&#38754;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#37325;&#26032;&#32452;&#21512;&#29616;&#26377;&#30693;&#35782;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#20135;&#29983;&#21407;&#21019;&#31185;&#23398;&#20869;&#23481;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21382;&#21490;&#19978;&#65292;&#29087;&#32451;&#30340;&#20889;&#20316;&#34987;&#35748;&#20026;&#26159;&#20154;&#31867;&#36827;&#27493;&#30340;&#20851;&#38190;&#65292;&#21019;&#36896;&#24615;&#34920;&#36798;&#34987;&#35270;&#20026;&#20154;&#31867;&#25104;&#23601;&#30340;&#26631;&#24535;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#24335;AI&#30340;&#26368;&#26032;&#36827;&#23637;&#26631;&#24535;&#30528;&#36825;&#19968;&#21465;&#20107;&#30340;&#19968;&#20010;&#36716;&#25240;&#28857;&#65292;&#21253;&#25324;&#22312;&#31185;&#23398;&#20889;&#20316;&#26041;&#38754;&#12290;&#26412;&#25991;&#20840;&#38754;&#20998;&#26512;&#20102;&#20845;&#20010;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#20154;&#25991;&#23398;&#31185;&#21644;&#32771;&#21476;&#23398;&#26041;&#38754;&#23398;&#26415;&#20889;&#20316;&#20013;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;&#26041;&#27861;&#22522;&#20110;&#30001;&#20154;&#31867;&#19987;&#23478;&#23545;AI&#29983;&#25104;&#20869;&#23481;&#36827;&#34892;&#23450;&#37327;&#20934;&#30830;&#24615;&#21644;&#23450;&#24615;&#31934;&#30830;&#24615;&#26631;&#35760;&#12290;&#23450;&#37327;&#20934;&#30830;&#24615;&#35780;&#20272;&#20102;&#20107;&#23454;&#30340;&#27491;&#30830;&#24615;&#65292;&#32780;&#23450;&#24615;&#31934;&#30830;&#24615;&#35780;&#20272;&#20102;&#31185;&#23398;&#36129;&#29486;&#12290;&#34429;&#28982;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#29305;&#21035;&#26159;ChatGPT-4&#65292;&#22312;&#37325;&#26032;&#32452;&#21512;&#29616;&#26377;&#30693;&#35782;&#26041;&#38754;&#34920;&#29616;&#20986;&#29087;&#32451;&#24615;&#65292;&#20294;&#22312;&#29983;&#25104;&#21407;&#21019;&#31185;&#23398;&#20869;&#23481;&#26041;&#38754;&#22833;&#36133;&#20102;&#12290;&#39034;&#20415;&#25552;&#19968;&#19979;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#36824;&#26174;&#31034;&#65292;&#38543;&#30528;ChatGPT-4&#65292;&#35821;&#35328;&#27169;&#22411;&#22823;&#23567;&#24050;&#32463;&#20572;&#28382;&#19981;&#21069;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#24378;&#35843;&#20102;&#22797;&#26434;&#19988;&#21453;&#22797;&#26080;&#24120;&#30340;&#29983;&#25104;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Historically, proficient writing was deemed essential for human advancement, with creative expression viewed as one of the hallmarks of human achievement. However, recent advances in generative AI have marked an inflection point in this narrative, including for scientific writing. This article provides a comprehensive analysis of the capabilities and limitations of six AI chatbots in scholarly writing in the humanities and archaeology. The methodology was based on tagging AI generated content for quantitative accuracy and qualitative precision by human experts. Quantitative accuracy assessed the factual correctness, while qualitative precision gauged the scientific contribution. While the AI chatbots, especially ChatGPT-4, demonstrated proficiency in recombining existing knowledge, they failed in generating original scientific content. As a side note, our results also suggest that with ChatGPT-4 the size of the LLMs has plateaued. Furthermore, the paper underscores the intricate and re
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21160;&#24577;&#28201;&#24230;&#37319;&#26679;&#30340;AdapT&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20195;&#30721;&#29983;&#25104;&#30340;&#26032;&#30340;&#35299;&#30721;&#31574;&#30053;&#65292;&#36890;&#36807;&#35843;&#25972;&#28201;&#24230;&#31995;&#25968;&#26469;&#35299;&#20915;&#38590;&#20197;&#39044;&#27979;&#30340;&#20195;&#30721;&#26631;&#35760;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.02772</link><description>&lt;p&gt;
&#36890;&#36807;&#21160;&#24577;&#28201;&#24230;&#37319;&#26679;&#25913;&#36827;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Improving Code Generation by Dynamic Temperature Sampling. (arXiv:2309.02772v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02772
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21160;&#24577;&#28201;&#24230;&#37319;&#26679;&#30340;AdapT&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20195;&#30721;&#29983;&#25104;&#30340;&#26032;&#30340;&#35299;&#30721;&#31574;&#30053;&#65292;&#36890;&#36807;&#35843;&#25972;&#28201;&#24230;&#31995;&#25968;&#26469;&#35299;&#20915;&#38590;&#20197;&#39044;&#27979;&#30340;&#20195;&#30721;&#26631;&#35760;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35299;&#30721;&#31574;&#30053;&#26159;&#38024;&#23545;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#35774;&#35745;&#30340;&#65292;&#24573;&#35270;&#20102;&#33258;&#28982;&#35821;&#35328;&#21644;&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#30001;&#20110;&#36825;&#20010;&#30095;&#24573;&#65292;&#22914;&#20309;&#35774;&#35745;&#26356;&#22909;&#30340;&#20195;&#30721;&#29983;&#25104;&#35299;&#30721;&#31574;&#30053;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31532;&#19968;&#27425;&#31995;&#32479;&#30740;&#31350;&#65292;&#25506;&#32034;&#20102;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#30340;&#35299;&#30721;&#31574;&#30053;&#12290;&#36890;&#36807;&#23545;&#20195;&#30721;&#26631;&#35760;&#20002;&#22833;&#20998;&#24067;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#20195;&#30721;&#26631;&#35760;&#21487;&#20197;&#20998;&#20026;&#20004;&#31867;&#65306;&#38590;&#20197;&#39044;&#27979;&#30340;&#25361;&#25112;&#24615;&#26631;&#35760;&#21644;&#26131;&#20110;&#25512;&#26029;&#30340;&#33258;&#20449;&#26631;&#35760;&#12290;&#20854;&#20013;&#65292;&#25361;&#25112;&#24615;&#26631;&#35760;&#20027;&#35201;&#20986;&#29616;&#22312;&#20195;&#30721;&#22359;&#30340;&#24320;&#22836;&#12290;&#21463;&#21040;&#19978;&#36848;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65306;&#33258;&#36866;&#24212;&#28201;&#24230;&#65288;AdapT&#65289;&#37319;&#26679;&#65292;&#23427;&#22312;&#35299;&#30721;&#19981;&#21516;&#30340;&#26631;&#35760;&#26102;&#21160;&#24577;&#35843;&#25972;&#28201;&#24230;&#31995;&#25968;&#12290;&#25105;&#20204;&#22312;&#37319;&#26679;&#25361;&#25112;&#24615;&#26631;&#35760;&#26102;&#24212;&#29992;&#36739;&#22823;&#30340;&#28201;&#24230;&#20540;&#12290;&#21516;&#26102;&#65292;&#22312;&#37319;&#26679;&#33258;&#20449;&#26631;&#35760;&#26102;&#24212;&#29992;&#36739;&#23567;&#30340;&#28201;&#24230;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Large Language Models (LLMs) have shown impressive results in code generation. However, existing decoding strategies are designed for Natural Language (NL) generation, overlooking the differences between NL and programming languages (PL). Due to this oversight, a better decoding strategy for code generation remains an open question. In this paper, we conduct the first systematic study to explore a decoding strategy specialized in code generation. With an analysis of loss distributions of code tokens, we find that code tokens can be divided into two categories: challenging tokens that are difficult to predict and confident tokens that can be easily inferred. Among them, the challenging tokens mainly appear at the beginning of a code block. Inspired by the above findings, we propose a simple yet effective method: Adaptive Temperature (AdapT) sampling, which dynamically adjusts the temperature coefficient when decoding different tokens. We apply a larger temperature when samplin
&lt;/p&gt;</description></item><item><title>LMSanitator&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#21644;&#28040;&#38500;Transformer&#27169;&#22411;&#20013;&#30340;&#20219;&#21153;&#19981;&#21487;&#30693;&#21518;&#38376;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;LMSanitator&#36890;&#36807;&#36870;&#36716;&#39044;&#23450;&#20041;&#30340;&#25915;&#20987;&#21521;&#37327;&#32780;&#19981;&#26159;&#35302;&#21457;&#22120;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#25910;&#25947;&#24615;&#33021;&#21644;&#21518;&#38376;&#26816;&#27979;&#31934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.13904</link><description>&lt;p&gt;
LMSanitator: &#38024;&#23545;&#20219;&#21153;&#19981;&#21487;&#30693;&#21518;&#38376;&#30340;Prompt-Tuning&#38450;&#24481;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
LMSanitator: Defending Prompt-Tuning Against Task-Agnostic Backdoors. (arXiv:2308.13904v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13904
&lt;/p&gt;
&lt;p&gt;
LMSanitator&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#21644;&#28040;&#38500;Transformer&#27169;&#22411;&#20013;&#30340;&#20219;&#21153;&#19981;&#21487;&#30693;&#21518;&#38376;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;LMSanitator&#36890;&#36807;&#36870;&#36716;&#39044;&#23450;&#20041;&#30340;&#25915;&#20987;&#21521;&#37327;&#32780;&#19981;&#26159;&#35302;&#21457;&#22120;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#25910;&#25947;&#24615;&#33021;&#21644;&#21518;&#38376;&#26816;&#27979;&#31934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Prompt-Tuning&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#24341;&#20154;&#27880;&#30446;&#30340;&#33539;&#24335;&#65292;&#29992;&#20110;&#37096;&#32626;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#24378;&#22823;&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#21644;&#39640;&#25928;&#30340;&#22810;&#20219;&#21153;&#26381;&#21153;&#33021;&#21147;&#12290;&#23613;&#31649;&#23427;&#34987;&#24191;&#27867;&#37319;&#29992;&#65292;&#25105;&#20204;&#23454;&#35777;&#34920;&#26126;&#65292;Prompt-Tuning&#23481;&#26131;&#21463;&#21040;&#20219;&#21153;&#19981;&#21487;&#30693;&#21518;&#38376;&#30340;&#25915;&#20987;&#65292;&#36825;&#20123;&#21518;&#38376;&#23384;&#22312;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#65292;&#21487;&#20197;&#24433;&#21709;&#20219;&#24847;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#30446;&#21069;&#30340;&#21518;&#38376;&#26816;&#27979;&#26041;&#27861;&#26080;&#27861;&#38450;&#24481;&#20219;&#21153;&#19981;&#21487;&#30693;&#21518;&#38376;&#65292;&#22240;&#20026;&#23427;&#20204;&#24456;&#38590;&#22312;&#36870;&#36716;&#21518;&#38376;&#35302;&#21457;&#22120;&#26041;&#38754;&#25910;&#25947;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LMSanitator&#65292;&#19968;&#31181;&#22312;Transformer&#27169;&#22411;&#19978;&#26816;&#27979;&#21644;&#21435;&#38500;&#20219;&#21153;&#19981;&#21487;&#30693;&#21518;&#38376;&#30340;&#26032;&#26041;&#27861;&#12290;LMSanitator&#19981;&#30452;&#25509;&#36870;&#36716;&#35302;&#21457;&#22120;&#65292;&#32780;&#26159;&#36870;&#36716;&#39044;&#23450;&#20041;&#30340;&#25915;&#20987;&#21521;&#37327;&#65288;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#36755;&#20837;&#23884;&#20837;&#35302;&#21457;&#22120;&#26102;&#30340;&#36755;&#20986;&#65289;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#25910;&#25947;&#24615;&#33021;&#21644;&#21518;&#38376;&#26816;&#27979;&#31934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt-tuning has emerged as an attractive paradigm for deploying large-scale language models due to its strong downstream task performance and efficient multitask serving ability. Despite its wide adoption, we empirically show that prompt-tuning is vulnerable to downstream task-agnostic backdoors, which reside in the pretrained models and can affect arbitrary downstream tasks. The state-of-the-art backdoor detection approaches cannot defend against task-agnostic backdoors since they hardly converge in reversing the backdoor triggers. To address this issue, we propose LMSanitator, a novel approach for detecting and removing task-agnostic backdoors on Transformer models. Instead of directly inversing the triggers, LMSanitator aims to inverse the predefined attack vectors (pretrained models' output when the input is embedded with triggers) of the task-agnostic backdoors, which achieves much better convergence performance and backdoor detection accuracy. LMSanitator further leverages prom
&lt;/p&gt;</description></item><item><title>RaLLe&#26159;&#19968;&#20010;&#24320;&#28304;&#26694;&#26550;&#65292;&#29992;&#20110;&#24320;&#21457;&#21644;&#35780;&#20272;&#26816;&#32034;&#22686;&#24378;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;R-LLMs&#65289;&#65292;&#20197;&#25552;&#39640;&#30693;&#35782;&#23494;&#38598;&#22411;&#29983;&#25104;&#20219;&#21153;&#30340;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.10633</link><description>&lt;p&gt;
RaLLe&#65306;&#29992;&#20110;&#24320;&#21457;&#21644;&#35780;&#20272;&#26816;&#32034;&#22686;&#24378;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
RaLLe: A Framework for Developing and Evaluating Retrieval-Augmented Large Language Models. (arXiv:2308.10633v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10633
&lt;/p&gt;
&lt;p&gt;
RaLLe&#26159;&#19968;&#20010;&#24320;&#28304;&#26694;&#26550;&#65292;&#29992;&#20110;&#24320;&#21457;&#21644;&#35780;&#20272;&#26816;&#32034;&#22686;&#24378;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;R-LLMs&#65289;&#65292;&#20197;&#25552;&#39640;&#30693;&#35782;&#23494;&#38598;&#22411;&#29983;&#25104;&#20219;&#21153;&#30340;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;R-LLMs&#65289;&#23558;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#20107;&#23454;&#38382;&#31572;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#29992;&#20110;&#26500;&#24314;R-LLMs&#30340;&#24211;&#25552;&#20379;&#20102;&#39640;&#32423;&#25277;&#35937;&#65292;&#20294;&#22312;&#35780;&#20272;&#21644;&#20248;&#21270;&#29305;&#23450;&#25512;&#29702;&#36807;&#31243;&#65288;&#22914;&#26816;&#32034;&#21644;&#29983;&#25104;&#65289;&#20013;&#30340;&#25552;&#31034;&#26102;&#32570;&#20047;&#36275;&#22815;&#30340;&#36879;&#26126;&#24230;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RaLLe&#65292;&#19968;&#20010;&#24320;&#28304;&#26694;&#26550;&#65292;&#26088;&#22312;&#20419;&#36827;R-LLMs&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#30340;&#24320;&#21457;&#12289;&#35780;&#20272;&#21644;&#20248;&#21270;&#12290;&#20351;&#29992;RaLLe&#65292;&#24320;&#21457;&#20154;&#21592;&#21487;&#20197;&#36731;&#26494;&#24320;&#21457;&#21644;&#35780;&#20272;R-LLMs&#65292;&#25913;&#36827;&#25163;&#24037;&#25552;&#31034;&#65292;&#35780;&#20272;&#20010;&#21035;&#25512;&#29702;&#36807;&#31243;&#65292;&#24182;&#23450;&#37327;&#22320;&#23458;&#35266;&#34913;&#37327;&#25972;&#20307;&#31995;&#32479;&#24615;&#33021;&#12290;&#36890;&#36807;&#21033;&#29992;&#36825;&#20123;&#21151;&#33021;&#65292;&#24320;&#21457;&#20154;&#21592;&#21487;&#20197;&#25552;&#39640;R-LLMs&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrieval-augmented large language models (R-LLMs) combine pre-trained large language models (LLMs) with information retrieval systems to improve the accuracy of factual question-answering. However, current libraries for building R-LLMs provide high-level abstractions without sufficient transparency for evaluating and optimizing prompts within specific inference processes such as retrieval and generation. To address this gap, we present RaLLe, an open-source framework designed to facilitate the development, evaluation, and optimization of R-LLMs for knowledge-intensive tasks. With RaLLe, developers can easily develop and evaluate R-LLMs, improving hand-crafted prompts, assessing individual inference processes, and objectively measuring overall system performance quantitatively. By leveraging these features, developers can enhance the performance and accuracy of their R-LLMs in knowledge-intensive generation tasks. We open-source our code at https://github.com/yhoshi3/RaLLe.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#21487;&#38752;&#24615;&#21644;&#40065;&#26834;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#22312;&#30495;&#23454;&#30340;&#36719;&#20214;&#24320;&#21457;&#20013;&#21487;&#25191;&#34892;&#30340;&#20195;&#30721;&#24182;&#19981;&#33021;&#20445;&#35777;&#21487;&#38752;&#21644;&#40065;&#26834;&#65292;&#28389;&#29992;API&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#38382;&#39064;&#12290;&#36825;&#23545;&#21021;&#32423;&#24320;&#21457;&#32773;&#26469;&#35828;&#23588;&#20854;&#21361;&#38505;&#65292;&#22240;&#20026;&#20182;&#20204;&#24456;&#38590;&#23519;&#35273;&#21040;&#20195;&#30721;&#20013;&#30340;API&#28389;&#29992;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.10335</link><description>&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#30721;&#29983;&#25104;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#38752;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study on Robustness and Reliability of Large Language Model Code Generation. (arXiv:2308.10335v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#21487;&#38752;&#24615;&#21644;&#40065;&#26834;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#22312;&#30495;&#23454;&#30340;&#36719;&#20214;&#24320;&#21457;&#20013;&#21487;&#25191;&#34892;&#30340;&#20195;&#30721;&#24182;&#19981;&#33021;&#20445;&#35777;&#21487;&#38752;&#21644;&#40065;&#26834;&#65292;&#28389;&#29992;API&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#38382;&#39064;&#12290;&#36825;&#23545;&#21021;&#32423;&#24320;&#21457;&#32773;&#26469;&#35828;&#23588;&#20854;&#21361;&#38505;&#65292;&#22240;&#20026;&#20182;&#20204;&#24456;&#38590;&#23519;&#35273;&#21040;&#20195;&#30721;&#20013;&#30340;API&#28389;&#29992;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#21644;&#29983;&#25104;&#32534;&#31243;&#20195;&#30721;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#38750;&#20961;&#33021;&#21147;&#12290;&#24403;&#36935;&#21040;&#32534;&#30721;&#38382;&#39064;&#26102;&#65292;&#36719;&#20214;&#24037;&#31243;&#24072;&#24120;&#24120;&#20250;&#21672;&#35810;LLMs&#12290;&#23613;&#31649;&#24050;&#32463;&#20570;&#20986;&#20102;&#19968;&#20123;&#21162;&#21147;&#26469;&#36991;&#20813;&#35821;&#27861;&#38169;&#35823;&#24182;&#20351;&#20195;&#30721;&#19982;&#39044;&#26399;&#30340;&#35821;&#20041;&#23545;&#40784;&#65292;&#20294;LLMs&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#21487;&#38752;&#24615;&#21644;&#40065;&#26834;&#24615;&#23578;&#26410;&#34987;&#28145;&#20837;&#30740;&#31350;&#12290;&#22312;&#30495;&#23454;&#30340;&#36719;&#20214;&#24320;&#21457;&#29615;&#22659;&#20013;&#65292;&#21487;&#25191;&#34892;&#30340;&#20195;&#30721;&#24182;&#19981;&#31561;&#21516;&#20110;&#21487;&#38752;&#21644;&#40065;&#26834;&#30340;&#20195;&#30721;&#12290;&#22312;&#29983;&#25104;&#30340;&#20195;&#30721;&#20013;&#28389;&#29992;API&#21487;&#33021;&#20250;&#23548;&#33268;&#20005;&#37325;&#30340;&#38382;&#39064;&#65292;&#22914;&#36164;&#28304;&#27844;&#28431;&#12289;&#31243;&#24207;&#23849;&#28291;&#12290;&#26356;&#31967;&#31957;&#30340;&#26159;&#65292;LLM&#20195;&#30721;&#29983;&#25104;&#26381;&#21153;&#30340;&#29992;&#25143;&#23454;&#38469;&#19978;&#26159;&#26368;&#23481;&#26131;&#21463;&#21040;&#36825;&#20123;&#30475;&#20284;&#27491;&#30830;&#30340;&#20195;&#30721;&#24433;&#21709;&#30340;&#24320;&#21457;&#32773;&#8212;&#8212;&#20182;&#20204;&#36890;&#24120;&#26159;&#19981;&#29087;&#24713;LLMs&#20026;&#20182;&#20204;&#29983;&#25104;&#20195;&#30721;&#30340;API&#30340;&#21021;&#32423;&#24320;&#21457;&#32773;&#12290;&#22240;&#27492;&#65292;&#20182;&#20204;&#24456;&#38590;&#23519;&#35273;&#21040;API&#30340;&#28389;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the large language models (LLMs) have shown extraordinary ability in understanding natural language and generating programming code. It has been a common practice of software engineers to consult LLMs when encountering coding questions. Although efforts have been made to avoid syntax errors and align the code with the intended semantics, the reliability and robustness of the code generationfrom LLMs have not yet been thoroughly studied. The executable code is not equivalent to the reliable and robust code, especially in the context of real-world software development. The misuse of APIs in the generated code could lead to severe problem, such as resource leaks, program crashes. To make things worse, the users of LLM code generation services are actually the developers that are most vulnerable to these code that seems right -- They are always novice developers that are not familiar with the APIs that LLMs generate code for them. Therefore, they could hardly tell the misuse in t
&lt;/p&gt;</description></item><item><title>SummHelper&#26159;&#19968;&#20010;&#21327;&#20316;&#24335;&#20154;&#26426;&#25688;&#35201;&#29983;&#25104;&#24037;&#20855;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#30340;&#36741;&#21161;&#36807;&#31243;&#65292;&#29992;&#25143;&#21487;&#20197;&#36873;&#25321;&#21644;&#20462;&#25913;&#25991;&#26412;&#20869;&#23481;&#65292;&#24182;&#29983;&#25104;&#19968;&#20221;&#36830;&#36143;&#30340;&#25688;&#35201;&#12290;&#29992;&#25143;&#30740;&#31350;&#26174;&#31034;&#35813;&#24212;&#29992;&#31243;&#24207;&#22312;&#33258;&#21160;&#21270;&#24341;&#23548;&#21644;&#20010;&#20154;&#36755;&#20837;&#20043;&#38388;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2308.08363</link><description>&lt;p&gt;
SummHelper&#65306;&#21327;&#20316;&#24335;&#20154;&#26426;&#25688;&#35201;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
SummHelper: Collaborative Human-Computer Summarization. (arXiv:2308.08363v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08363
&lt;/p&gt;
&lt;p&gt;
SummHelper&#26159;&#19968;&#20010;&#21327;&#20316;&#24335;&#20154;&#26426;&#25688;&#35201;&#29983;&#25104;&#24037;&#20855;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#30340;&#36741;&#21161;&#36807;&#31243;&#65292;&#29992;&#25143;&#21487;&#20197;&#36873;&#25321;&#21644;&#20462;&#25913;&#25991;&#26412;&#20869;&#23481;&#65292;&#24182;&#29983;&#25104;&#19968;&#20221;&#36830;&#36143;&#30340;&#25688;&#35201;&#12290;&#29992;&#25143;&#30740;&#31350;&#26174;&#31034;&#35813;&#24212;&#29992;&#31243;&#24207;&#22312;&#33258;&#21160;&#21270;&#24341;&#23548;&#21644;&#20010;&#20154;&#36755;&#20837;&#20043;&#38388;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#25991;&#26412;&#25688;&#35201;&#26041;&#27861;&#20027;&#35201;&#26159;&#33258;&#21160;&#21270;&#30340;&#65292;&#23545;&#20154;&#31867;&#24178;&#39044;&#21644;&#25511;&#21046;&#30340;&#31354;&#38388;&#26377;&#38480;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;SummHelper&#65292;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#25688;&#35201;&#36741;&#21161;&#31995;&#32479;&#65292;&#26088;&#22312;&#20419;&#36827;&#20154;&#26426;&#21327;&#20316;&#12290;&#21021;&#22987;&#38454;&#27573;&#28041;&#21450;&#20869;&#23481;&#36873;&#25321;&#65292;&#31995;&#32479;&#25512;&#33616;&#28508;&#22312;&#20869;&#23481;&#65292;&#20801;&#35768;&#29992;&#25143;&#25509;&#21463;&#12289;&#20462;&#25913;&#25110;&#24341;&#20837;&#20854;&#20182;&#36873;&#25321;&#12290;&#38543;&#21518;&#30340;&#20869;&#23481;&#25972;&#21512;&#38454;&#27573;&#65292;SummHelper&#20174;&#36825;&#20123;&#36873;&#25321;&#20013;&#29983;&#25104;&#19968;&#20010;&#36830;&#36143;&#30340;&#25688;&#35201;&#65292;&#29992;&#25143;&#21487;&#20197;&#21033;&#29992;&#25688;&#35201;&#19982;&#28304;&#25991;&#26412;&#20043;&#38388;&#30340;&#21487;&#35270;&#26144;&#23556;&#36827;&#34892;&#20248;&#21270;&#12290;&#23567;&#35268;&#27169;&#29992;&#25143;&#30740;&#31350;&#26174;&#31034;&#25105;&#20204;&#30340;&#24212;&#29992;&#31243;&#24207;&#30340;&#26377;&#25928;&#24615;&#65292;&#21442;&#19982;&#32773;&#29305;&#21035;&#36190;&#36175;&#33258;&#21160;&#24341;&#23548;&#21644;&#20010;&#20154;&#36755;&#20837;&#26426;&#20250;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current approaches for text summarization are predominantly automatic, with rather limited space for human intervention and control over the process. In this paper, we introduce SummHelper, a 2-phase summarization assistant designed to foster human-machine collaboration. The initial phase involves content selection, where the system recommends potential content, allowing users to accept, modify, or introduce additional selections. The subsequent phase, content consolidation, involves SummHelper generating a coherent summary from these selections, which users can then refine using visual mappings between the summary and the source text. Small-scale user studies reveal the effectiveness of our application, with participants being especially appreciative of the balance between automated guidance and opportunities for personal input.
&lt;/p&gt;</description></item><item><title>Thresh&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#12289;&#21487;&#23450;&#21046;&#30340;&#21644;&#21487;&#37096;&#32626;&#30340;&#32454;&#31890;&#24230;&#25991;&#26412;&#35780;&#20272;&#24179;&#21488;&#65292;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;YAML&#37197;&#32622;&#25991;&#20214;&#65292;&#29992;&#25143;&#21487;&#20197;&#24555;&#36895;&#26500;&#24314;&#21644;&#27979;&#35797;&#20219;&#20309;&#26694;&#26550;&#30340;&#27880;&#37322;&#30028;&#38754;&#65292;&#24182;&#25552;&#20379;&#19968;&#20010;&#31038;&#21306;&#20013;&#24515;&#26469;&#20419;&#36827;&#21327;&#20316;&#21644;&#20849;&#20139;&#12290;</title><link>http://arxiv.org/abs/2308.06953</link><description>&lt;p&gt;
Thresh&#65306;&#19968;&#20010;&#32479;&#19968;&#30340;&#12289;&#21487;&#23450;&#21046;&#30340;&#21644;&#21487;&#37096;&#32626;&#30340;&#32454;&#31890;&#24230;&#25991;&#26412;&#35780;&#20272;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
Thresh: A Unified, Customizable and Deployable Platform for Fine-Grained Text Evaluation. (arXiv:2308.06953v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06953
&lt;/p&gt;
&lt;p&gt;
Thresh&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#12289;&#21487;&#23450;&#21046;&#30340;&#21644;&#21487;&#37096;&#32626;&#30340;&#32454;&#31890;&#24230;&#25991;&#26412;&#35780;&#20272;&#24179;&#21488;&#65292;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;YAML&#37197;&#32622;&#25991;&#20214;&#65292;&#29992;&#25143;&#21487;&#20197;&#24555;&#36895;&#26500;&#24314;&#21644;&#27979;&#35797;&#20219;&#20309;&#26694;&#26550;&#30340;&#27880;&#37322;&#30028;&#38754;&#65292;&#24182;&#25552;&#20379;&#19968;&#20010;&#31038;&#21306;&#20013;&#24515;&#26469;&#20419;&#36827;&#21327;&#20316;&#21644;&#20849;&#20139;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32454;&#31890;&#24230;&#30340;&#12289;&#36328;&#24230;&#32423;&#21035;&#30340;&#20154;&#24037;&#35780;&#20272;&#24050;&#32463;&#25104;&#20026;&#35780;&#20215;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#30340;&#21487;&#38752;&#21644;&#31283;&#20581;&#30340;&#26041;&#27861;&#65292;&#22914;&#25688;&#35201;&#12289;&#31616;&#21270;&#12289;&#26426;&#22120;&#32763;&#35793;&#21644;&#26032;&#38395;&#29983;&#25104;&#65292;&#24182;&#19988;&#24471;&#21040;&#30340;&#26631;&#27880;&#23545;&#35757;&#32451;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#21644;&#25913;&#21892;&#35821;&#35328;&#27169;&#22411;&#38750;&#24120;&#26377;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20026;&#36825;&#20123;&#35780;&#20272;&#26694;&#26550;&#23454;&#26045;&#30340;&#26631;&#27880;&#24037;&#20855;&#32570;&#20047;&#36866;&#24212;&#19981;&#21516;&#39046;&#22495;&#25110;&#35821;&#35328;&#30340;&#33021;&#21147;&#65292;&#25110;&#32773;&#26681;&#25454;&#29992;&#25143;&#38656;&#27714;&#20462;&#25913;&#26631;&#27880;&#35774;&#32622;&#12290;&#32570;&#20047;&#32479;&#19968;&#30340;&#26631;&#27880;&#25968;&#25454;&#26684;&#24335;&#38459;&#30861;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Thresh&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#12289;&#21487;&#23450;&#21046;&#30340;&#21644;&#21487;&#37096;&#32626;&#30340;&#32454;&#31890;&#24230;&#35780;&#20272;&#24179;&#21488;&#12290;&#21482;&#38656;&#21019;&#24314;&#19968;&#20010;YAML&#37197;&#32622;&#25991;&#20214;&#65292;&#29992;&#25143;&#21487;&#20197;&#22312;&#20960;&#20998;&#38047;&#20869;&#26500;&#24314;&#21644;&#27979;&#35797;&#20219;&#20309;&#26694;&#26550;&#30340;&#27880;&#37322;&#30028;&#38754; - &#25152;&#26377;&#36825;&#20123;&#37117;&#22312;&#19968;&#20010;web&#27983;&#35272;&#22120;&#31383;&#21475;&#20013;&#12290;&#20026;&#20102;&#20419;&#36827;&#21327;&#20316;&#21644;&#20849;&#20139;&#65292;Thresh&#25552;&#20379;&#20102;&#19968;&#20010;&#31038;&#21306;&#20013;&#24515;&#65292;&#36825;&#20010;&#20013;&#24515;&#25176;&#31649;&#20102;&#19968;&#31995;&#21015;&#32454;&#31890;&#24230;&#35780;&#20272;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-grained, span-level human evaluation has emerged as a reliable and robust method for evaluating text generation tasks such as summarization, simplification, machine translation and news generation, and the derived annotations have been useful for training automatic metrics and improving language models. However, existing annotation tools implemented for these evaluation frameworks lack the adaptability to be extended to different domains or languages, or modify annotation settings according to user needs. And the absence of a unified annotated data format inhibits the research in multi-task learning. In this paper, we introduce Thresh, a unified, customizable and deployable platform for fine-grained evaluation. By simply creating a YAML configuration file, users can build and test an annotation interface for any framework within minutes -- all in one web browser window. To facilitate collaboration and sharing, Thresh provides a community hub that hosts a collection of fine-grained
&lt;/p&gt;</description></item><item><title>CLEVA&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#30340;&#29992;&#25143;&#21451;&#22909;&#24179;&#21488;&#65292;&#36890;&#36807;&#26631;&#20934;&#21270;&#24037;&#20316;&#27969;&#31243;&#12289;&#31454;&#20105;&#25490;&#34892;&#27036;&#21644;&#20943;&#23569;&#27745;&#26579;&#30340;&#31574;&#30053;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#36731;&#26494;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2308.04813</link><description>&lt;p&gt;
CLEVA&#65306;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
CLEVA: Chinese Language Models EVAluation Platform. (arXiv:2308.04813v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04813
&lt;/p&gt;
&lt;p&gt;
CLEVA&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#30340;&#29992;&#25143;&#21451;&#22909;&#24179;&#21488;&#65292;&#36890;&#36807;&#26631;&#20934;&#21270;&#24037;&#20316;&#27969;&#31243;&#12289;&#31454;&#20105;&#25490;&#34892;&#27036;&#21644;&#20943;&#23569;&#27745;&#26579;&#30340;&#31574;&#30053;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#36731;&#26494;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20013;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#19981;&#26029;&#20986;&#29616;&#65292;&#22914;&#20309;&#35780;&#20272;&#27169;&#22411;&#30340;&#33021;&#21147;&#24050;&#25104;&#20026;&#19968;&#20010;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#24403;&#21069;&#35780;&#20272;&#20013;&#25991;LLMs&#38754;&#20020;&#30528;&#32570;&#20047;&#20840;&#38754;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#30340;&#22522;&#20934;&#12289;&#38750;&#26631;&#20934;&#21270;&#21644;&#26080;&#27861;&#27604;&#36739;&#30340;&#25552;&#31034;&#36807;&#31243;&#65292;&#20197;&#21450;&#26222;&#36941;&#23384;&#22312;&#30340;&#27745;&#26579;&#39118;&#38505;&#31561;&#20027;&#35201;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;CLEVA&#65292;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;&#24179;&#21488;&#65292;&#29992;&#20110;&#20840;&#38754;&#35780;&#20272;&#20013;&#25991;LLMs&#12290;&#25105;&#20204;&#30340;&#24179;&#21488;&#37319;&#29992;&#26631;&#20934;&#21270;&#24037;&#20316;&#27969;&#31243;&#65292;&#23450;&#26399;&#26356;&#26032;&#31454;&#20105;&#25490;&#34892;&#27036;&#65292;&#35780;&#20272;LLMs&#22312;&#21508;&#20010;&#32500;&#24230;&#19978;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#20943;&#23569;&#27745;&#26579;&#65292;CLEVA&#31934;&#36873;&#20102;&#22823;&#37327;&#26032;&#25968;&#25454;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#37319;&#26679;&#31574;&#30053;&#65292;&#20445;&#35777;&#27599;&#20010;&#25490;&#34892;&#27036;&#36718;&#27425;&#37117;&#26377;&#29420;&#29305;&#30340;&#23376;&#38598;&#12290;&#29992;&#25143;&#21482;&#38656;&#28857;&#20987;&#20960;&#19979;&#40736;&#26631;&#24182;&#20351;&#29992;&#27169;&#22411;API&#21363;&#21487;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#26080;&#38656;&#32534;&#20889;&#22823;&#37327;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the continuous emergence of Chinese Large Language Models (LLMs), how to evaluate a model's capabilities has become an increasingly significant issue. The absence of a comprehensive Chinese benchmark that thoroughly assesses a model's performance, the unstandardized and incomparable prompting procedure, and the prevalent risk of contamination pose major challenges in the current evaluation of Chinese LLMs. We present CLEVA, a user-friendly platform crafted to holistically evaluate Chinese LLMs. Our platform employs a standardized workflow to assess LLMs' performance across various dimensions, regularly updating a competitive leaderboard. To alleviate contamination, CLEVA curates a significant proportion of new data and develops a sampling strategy that guarantees a unique subset for each leaderboard round. Empowered by an easy-to-use interface that requires just a few mouse clicks and a model API, users can conduct a thorough evaluation with minimal coding. Large-scale experiments
&lt;/p&gt;</description></item><item><title>SynJax&#26159;&#19968;&#20010;&#38024;&#23545;JAX&#30340;&#32467;&#26500;&#21270;&#27010;&#29575;&#20998;&#24067;&#24211;&#65292;&#36890;&#36807;&#25552;&#20379;&#39640;&#25928;&#30340;&#21521;&#37327;&#21270;&#23454;&#29616;&#35299;&#20915;&#20102;&#23545;&#20110;&#32467;&#26500;&#21270;&#23545;&#35937;&#30340;&#38590;&#20197;&#23454;&#29616;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.03291</link><description>&lt;p&gt;
SynJax: JAX&#30340;&#32467;&#26500;&#21270;&#27010;&#29575;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
SynJax: Structured Probability Distributions for JAX. (arXiv:2308.03291v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03291
&lt;/p&gt;
&lt;p&gt;
SynJax&#26159;&#19968;&#20010;&#38024;&#23545;JAX&#30340;&#32467;&#26500;&#21270;&#27010;&#29575;&#20998;&#24067;&#24211;&#65292;&#36890;&#36807;&#25552;&#20379;&#39640;&#25928;&#30340;&#21521;&#37327;&#21270;&#23454;&#29616;&#35299;&#20915;&#20102;&#23545;&#20110;&#32467;&#26500;&#21270;&#23545;&#35937;&#30340;&#38590;&#20197;&#23454;&#29616;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#36719;&#20214;&#24211;&#30340;&#21457;&#23637;&#20351;&#24471;&#35813;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#23427;&#20351;&#29992;&#25143;&#33021;&#22815;&#19987;&#27880;&#20110;&#24314;&#27169;&#65292;&#21516;&#26102;&#35753;&#24211;&#26469;&#22788;&#29702;&#38024;&#23545;&#29616;&#20195;&#30828;&#20214;&#21152;&#36895;&#22120;&#36827;&#34892;&#20248;&#21270;&#25191;&#34892;&#30340;&#32321;&#29712;&#21644;&#32791;&#26102;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20165;&#23545;&#29305;&#23450;&#31867;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26377;&#30410;&#65292;&#20363;&#22914;Transformer&#65292;&#20854;&#22522;&#26412;&#25805;&#20316;&#26131;&#20110;&#26144;&#23556;&#21040;&#21521;&#37327;&#21270;&#35745;&#31639;&#12290;&#32780;&#23545;&#20110;&#26174;&#24335;&#32771;&#34385;&#32467;&#26500;&#21270;&#23545;&#35937;&#65288;&#22914;&#26641;&#21644;&#20998;&#21106;&#65289;&#30340;&#27169;&#22411;&#65292;&#24182;&#27809;&#26377;&#21516;&#26679;&#30340;&#21463;&#30410;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#23450;&#21046;&#30340;&#38590;&#20197;&#20197;&#21521;&#37327;&#21270;&#24418;&#24335;&#23454;&#29616;&#30340;&#31639;&#27861;&#12290;SynJax&#36890;&#36807;&#25552;&#20379;&#29992;&#20110;&#32467;&#26500;&#21270;&#20998;&#24067;&#30340;&#25512;&#29702;&#31639;&#27861;&#30340;&#39640;&#25928;&#21521;&#37327;&#21270;&#23454;&#29616;&#26469;&#30452;&#25509;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21253;&#25324;&#23545;&#40784;&#12289;&#26631;&#35760;&#12289;&#20998;&#21106;&#12289;&#32452;&#25104;&#26641;&#21644;&#29983;&#25104;&#26641;&#30340;&#22788;&#29702;&#12290;&#20351;&#29992;SynJax&#65292;&#25105;&#20204;&#21487;&#20197;&#26500;&#24314;&#22823;&#35268;&#27169;&#30340;&#21487;&#24494;&#20998;&#27169;&#22411;&#65292;&#26174;&#24335;&#22320;&#23545;&#25968;&#25454;&#30340;&#32467;&#26500;&#36827;&#34892;&#24314;&#27169;&#12290;&#20195;&#30721;&#21487;&#22312;https://g&#20013;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of deep learning software libraries enabled significant progress in the field by allowing users to focus on modeling, while letting the library to take care of the tedious and time-consuming task of optimizing execution for modern hardware accelerators. However, this has benefited only particular types of deep learning models, such as Transformers, whose primitives map easily to the vectorized computation. The models that explicitly account for structured objects, such as trees and segmentations, did not benefit equally because they require custom algorithms that are difficult to implement in a vectorized form.  SynJax directly addresses this problem by providing an efficient vectorized implementation of inference algorithms for structured distributions covering alignment, tagging, segmentation, constituency trees and spanning trees. With SynJax we can build large-scale differentiable models that explicitly model structure in the data. The code is available at https://g
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#24052;&#35199;&#20840;&#22269;&#31185;&#23398;&#31454;&#36187;&#36807;&#21435;20&#24180;&#30340;&#25968;&#25454;&#65292;&#21457;&#29616;&#23398;&#29983;&#20204;&#22312;&#31185;&#23398;&#25506;&#31350;&#20013;&#25506;&#32034;&#20102;&#24191;&#27867;&#30340;&#20027;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#25903;&#25345;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#25945;&#23398;&#35774;&#35745;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.02962</link><description>&lt;p&gt;
&#31185;&#23398;&#21644;&#24037;&#31243;&#26159;&#20026;&#20102;&#20160;&#20040;&#65311;&#23545;&#31185;&#23398;&#31454;&#36187;&#20013;&#23398;&#29983;&#39033;&#30446;&#30340;&#22823;&#35268;&#27169;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Science and engineering for what? A large-scale analysis of students' projects in science fairs. (arXiv:2308.02962v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02962
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#24052;&#35199;&#20840;&#22269;&#31185;&#23398;&#31454;&#36187;&#36807;&#21435;20&#24180;&#30340;&#25968;&#25454;&#65292;&#21457;&#29616;&#23398;&#29983;&#20204;&#22312;&#31185;&#23398;&#25506;&#31350;&#20013;&#25506;&#32034;&#20102;&#24191;&#27867;&#30340;&#20027;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#25903;&#25345;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#25945;&#23398;&#35774;&#35745;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#21644;&#24037;&#31243;&#31454;&#36187;&#20026;K-12&#23398;&#29983;&#25552;&#20379;&#20102;&#21442;&#19982;&#30495;&#23454;STEM&#23454;&#36341;&#30340;&#26426;&#20250;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#20027;&#39064;&#24314;&#27169;&#25216;&#26415;&#20998;&#26512;&#20102;&#24052;&#35199;&#20840;&#22269;&#31185;&#23398;&#31454;&#36187;&#36807;&#21435;20&#24180;&#20013;&#36229;&#36807;5000&#20010;&#39033;&#30446;&#30340;&#25968;&#25454;&#65292;&#20197;&#35782;&#21035;&#25512;&#21160;&#23398;&#29983;&#25506;&#31350;&#21644;&#35774;&#35745;&#30340;&#20027;&#35201;&#20027;&#39064;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21457;&#29616;&#65292;&#23398;&#29983;&#20204;&#25506;&#32034;&#20102;&#24191;&#27867;&#30340;&#20027;&#39064;&#65292;&#24182;&#19988;&#38543;&#26102;&#38388;&#12289;&#22320;&#21306;&#21644;&#23398;&#26657;&#29615;&#22659;&#30340;&#21464;&#21270;&#32780;&#20135;&#29983;&#26174;&#33879;&#24046;&#24322;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#32467;&#26524;&#21644;&#25552;&#20986;&#30340;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#25903;&#25345;&#36827;&#19968;&#27493;&#30340;&#31185;&#23398;&#31454;&#36187;&#30740;&#31350;&#65292;&#36824;&#21487;&#20197;&#20026;&#19981;&#21516;&#29615;&#22659;&#20013;&#25903;&#25345;&#23398;&#29983;&#24320;&#23637;&#24320;&#25918;&#24615;&#25506;&#31350;&#27963;&#21160;&#30340;&#25945;&#23398;&#21644;&#35774;&#35745;&#25552;&#20379;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Science and Engineering fairs offer K-12 students opportunities to engage with authentic STEM practices. Particularly, students are given the chance to experience authentic and open inquiry processes, by defining which themes, questions and approaches will guide their scientific endeavors. In this study, we analyzed data from over 5,000 projects presented at a nationwide science fair in Brazil over the past 20 years using topic modeling to identify the main topics that have driven students' inquiry and design. Our analysis identified a broad range of topics being explored, with significant variations over time, region, and school setting. We argue those results and proposed methodology can not only support further research in the context of science fairs, but also inform instruction and design of contexts-specific resources to support students in open inquiry experiences in different settings.
&lt;/p&gt;</description></item><item><title>Med-HALT&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#21644;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#20943;&#23569;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#21307;&#30103;&#39046;&#22495;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21253;&#25324;&#22810;&#31181;&#21019;&#26032;&#30340;&#27979;&#35797;&#27169;&#24335;&#65292;&#24182;&#35780;&#20272;&#20102;&#39046;&#20808;&#30340;LLMs&#22312;&#24615;&#33021;&#19978;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2307.15343</link><description>&lt;p&gt;
Med-HALT:&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#21307;&#30103;&#39046;&#22495;&#24187;&#35273;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Med-HALT: Medical Domain Hallucination Test for Large Language Models. (arXiv:2307.15343v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15343
&lt;/p&gt;
&lt;p&gt;
Med-HALT&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#21644;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#20943;&#23569;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#21307;&#30103;&#39046;&#22495;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21253;&#25324;&#22810;&#31181;&#21019;&#26032;&#30340;&#27979;&#35797;&#27169;&#24335;&#65292;&#24182;&#35780;&#20272;&#20102;&#39046;&#20808;&#30340;LLMs&#22312;&#24615;&#33021;&#19978;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35770;&#25991;&#20851;&#27880;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#24187;&#35273;&#38382;&#39064;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#30103;&#39046;&#22495;&#30340;&#32972;&#26223;&#19979;&#12290;&#24187;&#35273;&#25351;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#20102;&#21512;&#29702;&#20294;&#26410;&#32463;&#39564;&#35777;&#25110;&#38169;&#35823;&#30340;&#20449;&#24687;&#65292;&#36825;&#21487;&#33021;&#23545;&#21307;&#30103;&#24212;&#29992;&#20135;&#29983;&#20005;&#37325;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#21644;&#25968;&#25454;&#38598;&#65292;Med-HALT&#65288;&#21307;&#30103;&#39046;&#22495;&#24187;&#35273;&#27979;&#35797;&#65289;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;&#21644;&#20943;&#23569;&#24187;&#35273;&#12290;Med-HALT&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#20803;&#21270;&#30340;&#36328;&#22269;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#26469;&#33258;&#19981;&#21516;&#22269;&#23478;&#30340;&#21307;&#30103;&#26816;&#26597;&#65292;&#21253;&#25324;&#22810;&#31181;&#21019;&#26032;&#30340;&#27979;&#35797;&#27169;&#24335;&#12290;Med-HALT&#21253;&#25324;&#20004;&#31867;&#27979;&#35797;&#65306;&#25512;&#29702;&#21644;&#22522;&#20110;&#35760;&#24518;&#30340;&#24187;&#35273;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;LLMs&#30340;&#38382;&#39064;&#35299;&#20915;&#21644;&#20449;&#24687;&#26816;&#32034;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35780;&#20272;&#20102;&#25991;&#26412;Davinci&#65292;GPT-3.5&#65292;LlaMa-2&#65292;MPT&#21644;Falcon&#31561;&#39046;&#20808;&#30340;LLMs&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#24615;&#33021;&#19978;&#30340;&#26174;&#33879;&#24046;&#24322;&#12290;&#36825;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#26377;&#20851;&#25968;&#25454;&#38598;&#30340;&#35814;&#32454;&#35265;&#35299;&#65292;&#20419;&#36827;&#20102;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research paper focuses on the challenges posed by hallucinations in large language models (LLMs), particularly in the context of the medical domain. Hallucination, wherein these models generate plausible yet unverified or incorrect information, can have serious consequences in healthcare applications. We propose a new benchmark and dataset, Med-HALT (Medical Domain Hallucination Test), designed specifically to evaluate and reduce hallucinations. Med-HALT provides a diverse multinational dataset derived from medical examinations across various countries and includes multiple innovative testing modalities. Med-HALT includes two categories of tests reasoning and memory-based hallucination tests, designed to assess LLMs's problem-solving and information retrieval abilities.  Our study evaluated leading LLMs, including Text Davinci, GPT-3.5, LlaMa-2, MPT, and Falcon, revealing significant differences in their performance. The paper provides detailed insights into the dataset, promoting
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;CamChoice&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#22810;&#39033;&#36873;&#25321;&#29702;&#35299;&#38382;&#39064;&#21644;&#30495;&#23454;&#20505;&#36873;&#31572;&#26696;&#36873;&#39033;&#20998;&#24067;&#65292;&#20026;&#20505;&#36873;&#20154;&#20998;&#24067;&#21305;&#37197;&#20219;&#21153;&#25552;&#20379;&#20102;&#33258;&#21160;&#35780;&#20272;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2306.13047</link><description>&lt;p&gt;
CamChoice&#65306;&#19968;&#20221;&#21253;&#21547;&#22810;&#39033;&#36873;&#25321;&#39064;&#21644;&#20505;&#36873;&#31572;&#26696;&#20998;&#24067;&#30340;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
CamChoice: A Corpus of Multiple Choice Questions and Candidate Response Distributions. (arXiv:2306.13047v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13047
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;CamChoice&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#22810;&#39033;&#36873;&#25321;&#29702;&#35299;&#38382;&#39064;&#21644;&#30495;&#23454;&#20505;&#36873;&#31572;&#26696;&#36873;&#39033;&#20998;&#24067;&#65292;&#20026;&#20505;&#36873;&#20154;&#20998;&#24067;&#21305;&#37197;&#20219;&#21153;&#25552;&#20379;&#20102;&#33258;&#21160;&#35780;&#20272;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#39033;&#36873;&#25321;&#39064;&#26159;&#29992;&#20110;&#34913;&#37327;&#20505;&#36873;&#20154;&#22312;&#21508;&#31181;&#39046;&#22495;&#21644;&#20219;&#21153;&#20013;&#33021;&#21147;&#30340;&#26222;&#36941;&#35780;&#20272;&#24418;&#24335;&#12290;&#25552;&#20986;&#30340;&#38382;&#39064;&#30340;&#36136;&#37327;&#23545;&#20110;&#27979;&#35797;&#35774;&#35745;&#20154;&#21592;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#27492;&#26032;&#25552;&#20986;&#30340;&#38382;&#39064;&#22312;&#37096;&#32626;&#21040;&#23454;&#38469;&#32771;&#35797;&#20043;&#21069;&#38656;&#35201;&#32463;&#36807;&#20960;&#20010;&#39044;&#27979;&#35797;&#35780;&#20272;&#38454;&#27573;&#12290;&#30446;&#21069;&#65292;&#36825;&#20010;&#36807;&#31243;&#26159;&#30456;&#24403;&#25163;&#21160;&#21270;&#30340;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#38382;&#39064;&#24320;&#21457;&#21608;&#26399;&#30340;&#26102;&#38388;&#28382;&#21518;&#12290;&#33258;&#21160;&#21270;&#27492;&#36807;&#31243;&#23558;&#22823;&#22823;&#25552;&#39640;&#25928;&#29575;&#65292;&#28982;&#32780;&#30446;&#21069;&#30340;&#25968;&#25454;&#38598;&#19981;&#21253;&#21547;&#36275;&#22815;&#30340;&#39044;&#27979;&#35797;&#20998;&#26512;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CamChoice&#65306;&#19968;&#20221;&#21253;&#21547;&#19981;&#21516;&#30446;&#26631;&#32423;&#21035;&#38382;&#39064;&#21644;&#30495;&#23454;&#20505;&#36873;&#31572;&#26696;&#36873;&#39033;&#20998;&#24067;&#30340;&#22810;&#39033;&#36873;&#25321;&#29702;&#35299;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20505;&#36873;&#20154;&#20998;&#24067;&#21305;&#37197;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#20960;&#31181;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;RACE++&#19978;&#35757;&#32451;&#30340;&#33258;&#21160;&#31995;&#32479;&#21487;&#20197;&#23454;&#29616;&#35813;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiple Choice examinations are a ubiquitous form of assessment that is used to measure the ability of candidates across various domains and tasks. Maintaining the quality of proposed questions is of great importance to test designers, and therefore newly proposed questions go through several pre-test evaluation stages before they can be deployed into real-world exams. This process is currently quite manual, which can lead to time lags in the question development cycle. Automating this process would lead to a large improvement in efficiency, however, current datasets do not contain sufficient pre-test analysis information. In this paper, we introduce CamChoice; a multiple-choice comprehension dataset with questions at different target levels, where questions have the true candidate selected options distributions. We introduce the task of candidate distribution matching, propose several evaluation metrics for the task, and demonstrate that automatic systems trained on RACE++ can be lev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#28857;&#20987;&#35825;&#39575;&#26816;&#27979;&#19978;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;LLM&#26080;&#27861;&#21462;&#24471;&#26368;&#20339;&#32467;&#26524;&#19988;&#19981;&#33021;&#20165;&#36890;&#36807;&#26631;&#39064;&#23454;&#29616;&#28385;&#24847;&#30340;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2306.09597</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28857;&#20987;&#35825;&#39575;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Clickbait Detection via Large Language Models. (arXiv:2306.09597v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#28857;&#20987;&#35825;&#39575;&#26816;&#27979;&#19978;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;LLM&#26080;&#27861;&#21462;&#24471;&#26368;&#20339;&#32467;&#26524;&#19988;&#19981;&#33021;&#20165;&#36890;&#36807;&#26631;&#39064;&#23454;&#29616;&#28385;&#24847;&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#20987;&#35825;&#39575;&#65288;Clickbait&#65289;&#20250;&#36890;&#36807;&#19968;&#20123;&#20196;&#20154;&#24778;&#35766;&#29978;&#33267;&#24341;&#20154;&#20837;&#32988;&#30340;&#26631;&#39064;&#26469;&#35825;&#23548;&#29992;&#25143;&#36827;&#34892;&#28857;&#20987;&#65292;&#20960;&#20046;&#28183;&#36879;&#21040;&#25152;&#26377;&#22312;&#32447;&#20869;&#23481;&#21457;&#24067;&#32773;&#65292;&#22914;&#26032;&#38395;&#38376;&#25143;&#21644;&#31038;&#20132;&#23186;&#20307;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM)&#24050;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#24182;&#22312;&#19968;&#31995;&#21015;NLP&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#20294;&#26159;&#65292;LLM&#26159;&#21542;&#21487;&#20197;&#20316;&#20026;&#39640;&#36136;&#37327;&#30340;&#28857;&#20987;&#35825;&#39575;&#26816;&#27979;&#31995;&#32479;&#36824;&#19981;&#20026;&#20154;&#25152;&#30693;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;LLM&#22312;&#22810;&#20010;&#33521;&#25991;&#21644;&#20013;&#25991;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#23569;&#26679;&#26412;&#22330;&#26223;&#19979;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#21644;&#24494;&#35843;PLM&#26041;&#27861;&#30456;&#27604;&#65292;LLM&#26080;&#27861;&#36798;&#21040;&#26368;&#20339;&#32467;&#26524;&#12290;&#19982;&#20154;&#31867;&#30452;&#35273;&#19981;&#21516;&#65292;&#23454;&#39564;&#34920;&#26126;LLM&#19981;&#33021;&#20165;&#36890;&#36807;&#26631;&#39064;&#23454;&#29616;&#28385;&#24847;&#30340;&#28857;&#20987;&#35825;&#39575;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clickbait, which aims to induce users with some surprising and even thrilling headlines for increasing click-through rates, permeates almost all online content publishers, such as news portals and social media. Recently, Large Language Models (LLMs) have emerged as a powerful instrument and achieved tremendous success in a serious of NLP downstream tasks. However, it is not yet known whether LLMs can be served as a high-quality clickbait detection system. In this paper, we analyze the performance of LLMs in the few-shot scenarios on a number of English and Chinese benchmark datasets. Experimental results show that LLMs cannot achieve the best results compared to the state-of-the-art deep and fine-tuning PLMs methods. Different from the human intuition, the experiments demonstrated that LLMs cannot make satisfied clickbait detection just by the headlines.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20351;&#29992;&#24378;&#22823;&#30340;LLM&#20316;&#20026;&#35780;&#21028;&#21592;&#65292;&#36890;&#36807;&#24341;&#20837;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#30830;&#35748;LLM&#35780;&#21028;&#21592;&#21644;&#20154;&#31867;&#20559;&#22909;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#21487;&#35843;&#25972;&#32842;&#22825;&#21161;&#25163;&#30340;&#27169;&#22411;&#26550;&#26500;&#21644;&#24494;&#35843;&#26041;&#27861;&#26469;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.05685</link><description>&lt;p&gt;
&#29992;MT-Bench&#21644;Chatbot Arena&#35780;&#20272;&#20197;LLM&#20026;&#22522;&#30784;&#30340;&#32842;&#22825;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. (arXiv:2306.05685v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05685
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20351;&#29992;&#24378;&#22823;&#30340;LLM&#20316;&#20026;&#35780;&#21028;&#21592;&#65292;&#36890;&#36807;&#24341;&#20837;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#30830;&#35748;LLM&#35780;&#21028;&#21592;&#21644;&#20154;&#31867;&#20559;&#22909;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#21487;&#35843;&#25972;&#32842;&#22825;&#21161;&#25163;&#30340;&#27169;&#22411;&#26550;&#26500;&#21644;&#24494;&#35843;&#26041;&#27861;&#26469;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#32842;&#22825;&#21161;&#25163;&#20250;&#38754;&#20020;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#24191;&#27867;&#30340;&#21151;&#33021;&#65292;&#32780;&#29616;&#26377;&#30340;&#22522;&#20934;&#26080;&#27861;&#34913;&#37327;&#20154;&#31867;&#20559;&#22909;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20351;&#29992;&#24378;&#22823;&#30340;LLM&#20316;&#20026;&#35780;&#21028;&#21592;&#65292;&#22312;&#26356;&#21152;&#24320;&#25918;&#30340;&#38382;&#39064;&#19978;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;LLM&#20316;&#20026;&#35780;&#21028;&#21592;&#30340;&#20351;&#29992;&#21644;&#23616;&#38480;&#24615;&#65292;&#22914;&#20301;&#32622;&#21644;&#20887;&#20313;&#20559;&#35265;&#20197;&#21450;&#26377;&#38480;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#35299;&#20915;&#26041;&#26696;&#26469;&#36801;&#31227;&#20854;&#20013;&#19968;&#20123;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#65288;&#19968;&#20010;&#22810;&#36718;&#38382;&#31572;&#38598;&#21644;&#19968;&#20010;&#20247;&#21253;&#31454;&#25216;&#24179;&#21488;&#65289;&#26469;&#30830;&#35748;LLM&#35780;&#21028;&#21592;&#21644;&#20154;&#31867;&#20559;&#22909;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#20687;GPT-4&#36825;&#26679;&#30340;&#24378;&#22823;LLM&#35780;&#21028;&#21592;&#21487;&#20197;&#24456;&#22909;&#22320;&#21305;&#37197;&#21463;&#25511;&#21644;&#20247;&#21253;&#20154;&#31867;&#20559;&#22909;&#65292;&#36798;&#21040;&#20102;80&#65285;&#20197;&#19978;&#30340;&#19968;&#33268;&#24615;&#65292;&#19982;&#20154;&#31867;&#19968;&#33268;&#24615;&#27700;&#24179;&#30456;&#21516;&#12290;&#22240;&#27492;&#65292;LLM&#20316;&#20026;&#35780;&#21028;&#21592;&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#19988;&#21487;&#35299;&#37322;&#30340;&#36924;&#36817;&#20154;&#31867;&#20559;&#22909;&#30340;&#26041;&#24335;&#65292;&#32780;&#36825;&#20123;&#20559;&#22909;&#26159;&#38750;&#24120;&#26114;&#36149;&#33719;&#21462;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;LLM&#20316;&#20026;&#35780;&#21028;&#21592;&#65292;&#21487;&#20197;&#36890;&#36807;&#35843;&#25972;&#32842;&#22825;&#21161;&#25163;&#30340;&#27169;&#22411;&#26550;&#26500;&#21644;&#24494;&#35843;&#26041;&#27861;&#26469;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, such as position and verbosity biases and limited reasoning ability, and propose solutions to migrate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80\% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24212;&#29992;&#20110;&#29305;&#23450;&#39046;&#22495;&#65292;&#24182;&#22522;&#20110;&#35813;&#26694;&#26550;&#26500;&#24314;&#20102;&#24459;&#24072;LLaMA&#65292;&#19968;&#31181;&#38024;&#23545;&#27861;&#24459;&#39046;&#22495;&#30340;LLM&#12290;&#36890;&#36807;&#22312;&#25345;&#32493;&#35757;&#32451;&#38454;&#27573;&#27880;&#20837;&#39046;&#22495;&#30693;&#35782;&#21644;&#35774;&#35745;&#26377;&#30417;&#30563;&#24494;&#35843;&#20219;&#21153;&#26469;&#25945;&#25480;&#19987;&#19994;&#25216;&#33021;&#65292;&#20197;&#35299;&#20915;&#27169;&#22411;&#22312;&#29305;&#23450;&#39046;&#22495;&#20013;&#36935;&#21040;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#28155;&#21152;&#26816;&#32034;&#27169;&#22359;&#24182;&#22312;&#29983;&#25104;&#20043;&#21069;&#25552;&#21462;&#30456;&#20851;&#27861;&#24459;&#25991;&#31456;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#24187;&#35937;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.15062</link><description>&lt;p&gt;
&#24459;&#24072;LLaMA&#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
Lawyer LLaMA Technical Report. (arXiv:2305.15062v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15062
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24212;&#29992;&#20110;&#29305;&#23450;&#39046;&#22495;&#65292;&#24182;&#22522;&#20110;&#35813;&#26694;&#26550;&#26500;&#24314;&#20102;&#24459;&#24072;LLaMA&#65292;&#19968;&#31181;&#38024;&#23545;&#27861;&#24459;&#39046;&#22495;&#30340;LLM&#12290;&#36890;&#36807;&#22312;&#25345;&#32493;&#35757;&#32451;&#38454;&#27573;&#27880;&#20837;&#39046;&#22495;&#30693;&#35782;&#21644;&#35774;&#35745;&#26377;&#30417;&#30563;&#24494;&#35843;&#20219;&#21153;&#26469;&#25945;&#25480;&#19987;&#19994;&#25216;&#33021;&#65292;&#20197;&#35299;&#20915;&#27169;&#22411;&#22312;&#29305;&#23450;&#39046;&#22495;&#20013;&#36935;&#21040;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#28155;&#21152;&#26816;&#32034;&#27169;&#22359;&#24182;&#22312;&#29983;&#25104;&#20043;&#21069;&#25552;&#21462;&#30456;&#20851;&#27861;&#24459;&#25991;&#31456;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#24187;&#35937;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#65292;&#22914;LLaMA&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24403;&#37096;&#32626;&#21040;&#29305;&#23450;&#39046;&#22495;&#65292;&#22914;&#27861;&#24459;&#25110;&#21307;&#23398;&#26102;&#65292;&#27169;&#22411;&#20173;&#28982;&#38754;&#20020;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#19981;&#36275;&#21644;&#19981;&#36275;&#20197;&#35299;&#20915;&#19982;&#39046;&#22495;&#30456;&#20851;&#38382;&#39064;&#30340;&#33021;&#21147;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;LLMs&#36866;&#24212;&#29305;&#23450;&#39046;&#22495;&#65292;&#24182;&#22522;&#20110;&#35813;&#26694;&#26550;&#26500;&#24314;&#20102;&#24459;&#24072;LLaMA&#65292;&#19968;&#31181;&#22522;&#20110;&#27861;&#24459;&#39046;&#22495;&#30340;LLM&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#25345;&#32493;&#35757;&#32451;&#38454;&#27573;&#27880;&#20837;&#39046;&#22495;&#30693;&#35782;&#65292;&#24182;&#25945;&#25480;&#27169;&#22411;&#20351;&#29992;&#32463;&#36807;&#36866;&#24403;&#35774;&#35745;&#30340;&#26377;&#30417;&#30563;&#24494;&#35843;&#20219;&#21153;&#26469;&#23398;&#20064;&#19987;&#19994;&#25216;&#33021;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#35299;&#20915;&#27169;&#22411;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#24187;&#35937;&#38382;&#39064;&#65292;&#25105;&#20204;&#28155;&#21152;&#20102;&#19968;&#20010;&#26816;&#32034;&#27169;&#22359;&#65292;&#22312;&#27169;&#22411;&#22238;&#31572;&#20219;&#20309;&#26597;&#35810;&#20043;&#21069;&#25552;&#21462;&#30456;&#20851;&#30340;&#27861;&#24459;&#25991;&#31456;&#12290;&#22312;&#23398;&#20064;&#39046;&#22495;&#29305;&#23450;&#25216;&#33021;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#19987;&#23478;&#30340;&#32463;&#39564;&#27604;&#20174;ChatGPT&#20013;&#25552;&#28860;&#30340;&#32463;&#39564;&#26356;&#26377;&#29992;&#65292;ChatGPT&#21547;&#26377;&#25968;&#30334;&#20010;&#20363;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), like LLaMA, have exhibited remarkable performance across various tasks. Nevertheless, when deployed to specific domains such as law or medicine, the models still confront the challenge of a deficiency in domain-specific knowledge and an inadequate capability to leverage that knowledge to resolve domain-related problems. In this paper, we propose a new framework to adapt LLMs to specific domains and build Lawyer LLaMA, a legal domain LLM, based on this framework. Specifically, we inject domain knowledge during the continual training stage and teach the model to learn professional skills using properly designed supervised fine-tuning tasks. Moreover, to alleviate the hallucination problem during the model's generation, we add a retrieval module and extract relevant legal articles before the model answers any queries. When learning domain-specific skills, we find that experts' experience is much more useful than experiences distilled from ChatGPT, where hundr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#24615;&#31034;&#33539;&#25915;&#20987;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#26041;&#27861;advICL&#65292;&#36890;&#36807;&#25913;&#21464;&#31034;&#33539;&#32780;&#19981;&#25913;&#21464;&#36755;&#20837;&#26469;&#35823;&#23548;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#38543;&#30528;&#31034;&#33539;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;&#38477;&#20302;&#12290;</title><link>http://arxiv.org/abs/2305.14950</link><description>&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#24615;&#31034;&#33539;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Adversarial Demonstration Attacks on Large Language Models. (arXiv:2305.14950v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14950
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#24615;&#31034;&#33539;&#25915;&#20987;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#26041;&#27861;advICL&#65292;&#36890;&#36807;&#25913;&#21464;&#31034;&#33539;&#32780;&#19981;&#25913;&#21464;&#36755;&#20837;&#26469;&#35823;&#23548;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#38543;&#30528;&#31034;&#33539;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26356;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65292;&#22914;ChatGPT&#21644;GPT-4&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;-&#26631;&#31614;&#23545;&#20316;&#20026;&#39044;&#20808;&#26465;&#20214;&#25552;&#31034;&#65292;&#24050;&#32463;&#22312;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#29305;&#23450;&#20219;&#21153;&#26041;&#38754;&#33719;&#24471;&#26174;&#33879;&#30340;&#37325;&#35201;&#24615;&#12290;&#34429;&#28982;&#24341;&#20837;&#31034;&#33539;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;LLMs&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20063;&#21487;&#33021;&#24341;&#20837;&#26032;&#30340;&#23433;&#20840;&#38382;&#39064;&#65306;&#25915;&#20987;&#32773;&#21487;&#20197;&#20165;&#20165;&#25805;&#32437;&#31034;&#33539;&#32780;&#19981;&#25913;&#21464;&#36755;&#20837;&#26469;&#36827;&#34892;&#25915;&#20987;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#23545;&#25239;&#24615;&#30340;&#35282;&#24230;&#35843;&#26597;&#20102;ICL&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#37325;&#28857;&#20851;&#27880;&#31034;&#33539;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;advICL&#30340;&#26032;&#22411;&#25915;&#20987;&#26041;&#27861;&#65292;&#26088;&#22312;&#20165;&#20165;&#25913;&#21464;&#31034;&#33539;&#32780;&#19981;&#25913;&#21464;&#36755;&#20837;&#20197;&#35823;&#23548;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#38543;&#30528;&#31034;&#33539;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;&#23558;&#20250;&#38477;&#20302;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#31034;&#33539;&#30340;&#22266;&#26377;&#29305;&#24615;&#26159;&#21487;&#20197;&#34987;&#20351;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the emergence of more powerful large language models (LLMs), such as ChatGPT and GPT-4, in-context learning (ICL) has gained significant prominence in leveraging these models for specific tasks by utilizing data-label pairs as precondition prompts. While incorporating demonstrations can greatly enhance the performance of LLMs across various tasks, it may introduce a new security concern: attackers can manipulate only the demonstrations without changing the input to perform an attack. In this paper, we investigate the security concern of ICL from an adversarial perspective, focusing on the impact of demonstrations. We propose a novel attack method named advICL, which aims to manipulate only the demonstration without changing the input to mislead the models. Our results demonstrate that as the number of demonstrations increases, the robustness of in-context learning would decrease. Additionally, we also identify the intrinsic property of the demonstrations is that they can be used (
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#24403;&#20195;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#29983;&#25104;&#30005;&#36335;&#35774;&#35745;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#22522;&#20934;&#35780;&#20272;&#27169;&#22411;&#23545;&#30005;&#23376;&#20803;&#20214;&#30340;&#30693;&#35782;&#21644;&#22312;Arduino&#29983;&#24577;&#31995;&#32479;&#20013;&#35774;&#35745;&#24494;&#25511;&#21046;&#22120;&#30005;&#36335;&#21644;&#20195;&#30721;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20845;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35774;&#35745;&#21161;&#25163;&#22312;&#20013;&#31561;&#22797;&#26434;&#35774;&#22791;&#19978;&#30340;&#24212;&#29992;&#12290;&#35813;&#30740;&#31350;&#20026;&#25913;&#36827;&#22797;&#26434;&#30005;&#36335;&#35774;&#35745;&#21644;&#23454;&#38469;&#24212;&#29992;&#24615;&#25552;&#20379;&#20102;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.14874</link><description>&lt;p&gt;
&#20174;&#25991;&#26412;&#21040;&#30005;&#36335;&#65306;&#20174;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#29983;&#25104;&#21151;&#33021;&#40784;&#22791;&#30340;&#30005;&#23376;&#35774;&#22791;
&lt;/p&gt;
&lt;p&gt;
From Words to Wires: Generating Functioning Electronic Devices from Natural Language Descriptions. (arXiv:2305.14874v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#24403;&#20195;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#29983;&#25104;&#30005;&#36335;&#35774;&#35745;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#22522;&#20934;&#35780;&#20272;&#27169;&#22411;&#23545;&#30005;&#23376;&#20803;&#20214;&#30340;&#30693;&#35782;&#21644;&#22312;Arduino&#29983;&#24577;&#31995;&#32479;&#20013;&#35774;&#35745;&#24494;&#25511;&#21046;&#22120;&#30005;&#36335;&#21644;&#20195;&#30721;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20845;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35774;&#35745;&#21161;&#25163;&#22312;&#20013;&#31561;&#22797;&#26434;&#35774;&#22791;&#19978;&#30340;&#24212;&#29992;&#12290;&#35813;&#30740;&#31350;&#20026;&#25913;&#36827;&#22797;&#26434;&#30005;&#36335;&#35774;&#35745;&#21644;&#23454;&#38469;&#24212;&#29992;&#24615;&#25552;&#20379;&#20102;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#20195;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#20808;&#21069;&#26410;&#30693;&#30340;&#25216;&#33021;&#65292;&#21363;&#33021;&#22815;&#20174;&#39640;&#32423;&#25991;&#26412;&#25551;&#36848;&#20013;&#36827;&#34892;&#30005;&#36335;&#35774;&#35745;&#65292;&#31867;&#20284;&#20110;&#20195;&#30721;&#29983;&#25104;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#22522;&#20934;&#65306;Pins100&#65292;&#35780;&#20272;&#27169;&#22411;&#23545;&#30005;&#23376;&#20803;&#20214;&#30340;&#30693;&#35782;&#65307;Micro25&#65292;&#35780;&#20272;&#27169;&#22411;&#22312;Arduino&#29983;&#24577;&#31995;&#32479;&#20013;&#35774;&#35745;&#24120;&#35265;&#30340;&#24494;&#25511;&#21046;&#22120;&#30005;&#36335;&#21644;&#20195;&#30721;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#36755;&#20837;&#12289;&#36755;&#20986;&#12289;&#20256;&#24863;&#22120;&#12289;&#30005;&#26426;&#12289;&#21327;&#35758;&#21644;&#36923;&#36753;&#12290;&#22312;&#29983;&#25104;&#23436;&#25972;&#35774;&#22791;&#26041;&#38754;&#65292;&#20687;GPT-4&#21644;Claude-V1&#36825;&#26679;&#30340;&#27169;&#22411;&#22312;Pass@1&#26041;&#38754;&#36798;&#21040;&#20102;60%&#21040;96%&#30340;&#20934;&#30830;&#29575;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20845;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35774;&#35745;&#21161;&#25163;&#26469;&#35774;&#35745;&#20013;&#31561;&#22797;&#26434;&#30340;&#35774;&#22791;&#65292;&#20363;&#22914;&#36752;&#23556;&#20379;&#30005;&#30340;&#38543;&#26426;&#25968;&#21457;&#29983;&#22120;&#12289;&#34920;&#24773;&#31526;&#21495;&#38190;&#30424;&#12289;&#21487;&#35265;&#20809;&#35889;&#20202;&#21644;&#20960;&#20010;&#36741;&#21161;&#35774;&#22791;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#23545;&#24615;&#33021;&#36827;&#34892;&#20102;&#23450;&#24615;&#20998;&#26512;&#65292;&#27010;&#36848;&#20102;&#35780;&#20272;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#22797;&#26434;&#30005;&#36335;&#35774;&#35745;&#21644;&#23454;&#38469;&#24212;&#29992;&#24615;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we show that contemporary language models have a previously unknown skill -- the capacity for electronic circuit design from high-level textual descriptions, akin to code generation. We introduce two benchmarks: Pins100, assessing model knowledge of electrical components, and Micro25, evaluating a model's capability to design common microcontroller circuits and code in the Arduino ecosystem that involve input, output, sensors, motors, protocols, and logic -- with models such as GPT-4 and Claude-V1 achieving between 60% to 96% Pass@1 on generating full devices. We include six case studies of using language models as a design assistant for moderately complex devices, such as a radiation-powered random number generator, an emoji keyboard, a visible spectrometer, and several assistive devices, while offering a qualitative analysis performance, outlining evaluation challenges, and suggesting areas of development to improve complex circuit design and practical utility. With thi
&lt;/p&gt;</description></item><item><title>ComSL&#26159;&#19968;&#31181;&#22797;&#21512;&#35821;&#38899;-&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#36328;&#27169;&#24577;&#23398;&#20064;&#32435;&#20837;&#36801;&#31227;&#23398;&#20064;&#24182;&#20197;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#22312;&#31471;&#21040;&#31471;&#35821;&#38899;&#21040;&#25991;&#26412;&#32763;&#35793;&#20219;&#21153;&#20013;&#30340;&#26032;&#30340;&#26368;&#39640;BLEU&#20998;&#25968;&#12290; (31.5 on CoVoST2 evaluation set&#65292;21&#31181;&#35821;&#35328;)</title><link>http://arxiv.org/abs/2305.14838</link><description>&lt;p&gt;
ComSL:&#19968;&#31181;&#29992;&#20110;&#31471;&#21040;&#31471;&#35821;&#38899;&#21040;&#25991;&#26412;&#32763;&#35793;&#30340;&#22797;&#21512;&#35821;&#38899;-&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ComSL: A Composite Speech-Language Model for End-to-End Speech-to-Text Translation. (arXiv:2305.14838v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14838
&lt;/p&gt;
&lt;p&gt;
ComSL&#26159;&#19968;&#31181;&#22797;&#21512;&#35821;&#38899;-&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#36328;&#27169;&#24577;&#23398;&#20064;&#32435;&#20837;&#36801;&#31227;&#23398;&#20064;&#24182;&#20197;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#22312;&#31471;&#21040;&#31471;&#35821;&#38899;&#21040;&#25991;&#26412;&#32763;&#35793;&#20219;&#21153;&#20013;&#30340;&#26032;&#30340;&#26368;&#39640;BLEU&#20998;&#25968;&#12290; (31.5 on CoVoST2 evaluation set&#65292;21&#31181;&#35821;&#35328;)
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#21644;GPU&#28040;&#32791;&#37327;&#22823;&#20197;&#21450;&#35821;&#38899;&#21644;&#35821;&#35328;&#20043;&#38388;&#30340;&#27169;&#24577;&#24046;&#24322;&#65292;&#32852;&#21512;&#35821;&#38899;-&#35821;&#35328;&#35757;&#32451;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ComSL&#65292;&#23427;&#26159;&#24314;&#31435;&#22312;&#20844;&#20849;&#39044;&#35757;&#32451;&#30340;&#20165;&#35821;&#38899;&#21644;&#20165;&#35821;&#35328;&#27169;&#22411;&#30340;&#22797;&#21512;&#26550;&#26500;&#20043;&#19978;&#30340;&#35821;&#38899;-&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#19988;&#36890;&#36807;&#20248;&#21270;&#25968;&#25454;&#25928;&#29575;&#65292;&#38024;&#23545;&#21475;&#35821;&#20219;&#21153;&#36827;&#34892;&#20102;&#30456;&#24212;&#30340;&#35757;&#32451;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#36328;&#27169;&#24577;&#23398;&#20064;&#32435;&#20837;&#36801;&#31227;&#23398;&#20064;&#24182;&#20197;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#26041;&#24335;&#21516;&#26102;&#36827;&#34892;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#31471;&#21040;&#31471;&#35821;&#38899;&#21040;&#25991;&#26412;&#32763;&#35793;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#65292;&#22312;21&#31181;&#35821;&#35328;&#30340;&#22810;&#35821;&#31181;&#35821;&#38899;&#21040;&#33521;&#25991;&#25991;&#26412;&#32763;&#35793;&#20219;&#21153;&#19978;&#65292;&#20197;&#20844;&#24320;&#30340;CoVoST2&#35780;&#20272;&#38598;&#19978;&#30340;&#24179;&#22343;BLEU&#20998;&#25968;&#36798;&#21040;&#20102;31.5&#65292;&#36798;&#21040;&#20102;&#26032;&#30340;&#26368;&#39640;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Joint speech-language training is challenging due to the large demand for training data and GPU consumption, as well as the modality gap between speech and language. We present ComSL, a speech-language model built atop a composite architecture of public pretrained speech-only and language-only models and optimized data-efficiently for spoken language tasks. Particularly, we propose to incorporate cross-modality learning into transfer learning and conduct them simultaneously for downstream tasks in a multi-task learning manner. Our approach has demonstrated effectiveness in end-to-end speech-to-text translation tasks, achieving a new state-of-the-art average BLEU score of 31.5 on the multilingual speech to English text translation task for 21 languages, as measured on the public CoVoST2 evaluation set.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#29983;&#29289;&#21307;&#23398;&#35686;&#25253;&#26032;&#38395;&#25968;&#25454;&#38598;&#65288;BAND&#65289;&#65292;&#20854;&#21253;&#25324;&#20102;1,508&#20010;&#26679;&#26412;&#21644;30&#20010;&#19982;&#27969;&#34892;&#30149;&#23398;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#25552;&#20379;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#26356;&#22909;&#30340;&#20869;&#23481;&#20266;&#35013;&#33021;&#21147;&#21644;&#37325;&#35201;&#20449;&#24687;&#25512;&#26029;&#33021;&#21147;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#20026;&#30142;&#30149;&#30417;&#27979;&#21644;&#27969;&#34892;&#30149;&#23398;&#20998;&#26512;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#27934;&#23519;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.14480</link><description>&lt;p&gt;
BAND: &#29983;&#29289;&#21307;&#23398;&#35686;&#25253;&#26032;&#38395;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
BAND: Biomedical Alert News Dataset. (arXiv:2305.14480v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14480
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#29983;&#29289;&#21307;&#23398;&#35686;&#25253;&#26032;&#38395;&#25968;&#25454;&#38598;&#65288;BAND&#65289;&#65292;&#20854;&#21253;&#25324;&#20102;1,508&#20010;&#26679;&#26412;&#21644;30&#20010;&#19982;&#27969;&#34892;&#30149;&#23398;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#25552;&#20379;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#26356;&#22909;&#30340;&#20869;&#23481;&#20266;&#35013;&#33021;&#21147;&#21644;&#37325;&#35201;&#20449;&#24687;&#25512;&#26029;&#33021;&#21147;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#20026;&#30142;&#30149;&#30417;&#27979;&#21644;&#27969;&#34892;&#30149;&#23398;&#20998;&#26512;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#27934;&#23519;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#26579;&#24615;&#30142;&#30149;&#30340;&#29190;&#21457;&#23545;&#20154;&#31867;&#20581;&#24247;&#21644;&#31119;&#21033;&#26500;&#25104;&#37325;&#22823;&#23041;&#32961;&#12290;&#20026;&#20102;&#25913;&#21892;&#30142;&#30149;&#30417;&#27979;&#21644;&#20102;&#35299;&#30142;&#30149;&#20256;&#25773;&#24773;&#20917;&#65292;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#20960;&#20010;&#30417;&#27979;&#31995;&#32479;&#26469;&#30417;&#35270;&#27599;&#26085;&#26032;&#38395;&#35686;&#25253;&#21644;&#31038;&#20132;&#23186;&#20307;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#32463;&#36807;&#33391;&#22909;&#27880;&#37322;&#30340;&#25253;&#21578;&#25968;&#25454;&#65292;&#29616;&#26377;&#31995;&#32479;&#22312;&#19982;&#30456;&#24212;&#25552;&#37266;&#25110;&#26032;&#38395;&#30340;&#27969;&#34892;&#30149;&#23398;&#20998;&#26512;&#26041;&#38754;&#32570;&#20047;&#20005;&#35880;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#29983;&#29289;&#21307;&#23398;&#35686;&#25253;&#26032;&#38395;&#25968;&#25454;&#38598;&#65288;BAND&#65289;&#65292;&#21253;&#25324;&#26469;&#33258;&#29616;&#26377;&#25253;&#21578;&#26032;&#38395;&#25991;&#31456;&#12289;&#20844;&#24320;&#30005;&#23376;&#37038;&#20214;&#21644;&#25552;&#37266;&#30340;1,508&#20010;&#26679;&#26412;&#20197;&#21450;30&#20010;&#19982;&#27969;&#34892;&#30149;&#23398;&#30456;&#20851;&#30340;&#38382;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#38656;&#35201;&#27169;&#22411;&#19987;&#23478;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20174;&#32780;&#20026;&#30142;&#30149;&#29190;&#21457;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#27934;&#23519;&#21147;&#12290;BAND&#25968;&#25454;&#38598;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#25552;&#20379;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#26356;&#22909;&#30340;&#20869;&#23481;&#20266;&#35013;&#33021;&#21147;&#21644;&#37325;&#35201;&#20449;&#24687;&#25512;&#26029;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20960;&#20010;&#22522;&#20934;&#20219;&#21153;&#65292;&#21253;&#25324;&#21629;&#21517;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Infectious disease outbreaks continue to pose a significant threat to human health and well-being. To improve disease surveillance and understanding of disease spread, several surveillance systems have been developed to monitor daily news alerts and social media. However, existing systems lack thorough epidemiological analysis in relation to corresponding alerts or news, largely due to the scarcity of well-annotated reports data. To address this gap, we introduce the Biomedical Alert News Dataset (BAND), which includes 1,508 samples from existing reported news articles, open emails, and alerts, as well as 30 epidemiology-related questions. These questions necessitate the model's expert reasoning abilities, thereby offering valuable insights into the outbreak of the disease. The BAND dataset brings new challenges to the NLP world, requiring better disguise capability of the content and the ability to infer important information. We provide several benchmark tasks, including Named Entity
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#32858;&#21512;&#22836;&#27880;&#24847;&#21147;(Grouped Head Attention)&#65292;&#20351;&#29992;&#33258;&#30417;&#30563;&#32452;&#32422;&#26463;&#36827;&#34892;&#35757;&#32451;&#65292;&#20026;&#27880;&#24847;&#21147;&#22836;&#36827;&#34892;&#20998;&#32452;&#65292;&#20854;&#20013;&#27599;&#20010;&#32452;&#19987;&#27880;&#20110;&#19968;&#20010;&#37325;&#35201;&#32780;&#29420;&#29305;&#30340;&#29305;&#24449;&#23376;&#38598;&#12290;&#27492;&#26041;&#27861;&#21487;&#20197;&#32531;&#35299;MHA&#30340;&#20887;&#20313;&#24615;&#21644;&#36807;&#24230;&#21442;&#25968;&#21270;&#38382;&#39064;&#65292;&#24182;&#23548;&#33268;&#26356;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;MHA&#65292;&#36827;&#32780;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.14380</link><description>&lt;p&gt;
&#23547;&#25214;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#25903;&#26609;
&lt;/p&gt;
&lt;p&gt;
Finding the Pillars of Strength for Multi-Head Attention. (arXiv:2305.14380v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#32858;&#21512;&#22836;&#27880;&#24847;&#21147;(Grouped Head Attention)&#65292;&#20351;&#29992;&#33258;&#30417;&#30563;&#32452;&#32422;&#26463;&#36827;&#34892;&#35757;&#32451;&#65292;&#20026;&#27880;&#24847;&#21147;&#22836;&#36827;&#34892;&#20998;&#32452;&#65292;&#20854;&#20013;&#27599;&#20010;&#32452;&#19987;&#27880;&#20110;&#19968;&#20010;&#37325;&#35201;&#32780;&#29420;&#29305;&#30340;&#29305;&#24449;&#23376;&#38598;&#12290;&#27492;&#26041;&#27861;&#21487;&#20197;&#32531;&#35299;MHA&#30340;&#20887;&#20313;&#24615;&#21644;&#36807;&#24230;&#21442;&#25968;&#21270;&#38382;&#39064;&#65292;&#24182;&#23548;&#33268;&#26356;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;MHA&#65292;&#36827;&#32780;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#22810;&#22836;&#27880;&#24847;&#21147;(Multi-Head Attention, MHA)&#30340;&#19968;&#20123;&#38382;&#39064;&#65292;&#20363;&#22914;&#20887;&#20313;&#24615;&#21644;&#36807;&#24230;&#21442;&#25968;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MHA&#30340;&#22836;&#26368;&#21021;&#35774;&#35745;&#20026;&#20174;&#19981;&#21516;&#30340;&#34920;&#24449;&#23376;&#31354;&#38388;&#20013;&#20851;&#27880;&#20449;&#24687;&#65292;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#21457;&#29616;&#19968;&#20123;&#27880;&#24847;&#21147;&#22836;&#21487;&#33021;&#23398;&#20064;&#31867;&#20284;&#30340;&#29305;&#24449;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#20462;&#21098;&#26469;&#25552;&#39640;&#25928;&#29575;&#32780;&#19981;&#20250;&#25439;&#23475;&#24615;&#33021;&#12290;&#21463;&#26368;&#23567;&#20887;&#20313;&#29305;&#24449;&#36873;&#25321;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20551;&#35774;&#32858;&#28966;&#20110;&#26368;&#20855;&#20195;&#34920;&#24615;&#21644;&#29420;&#29305;&#24615;&#30340;&#29305;&#24449;&#65292;&#24182;&#26368;&#23567;&#21270;&#36164;&#28304;&#28040;&#32791;&#65292;&#21487;&#20197;&#32531;&#35299;&#19978;&#36848;&#38382;&#39064;&#65292;&#24182;&#23548;&#33268;&#26356;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;MHA&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32858;&#21512;&#22836;&#27880;&#24847;&#21147;(Grouped Head Attention)&#65292;&#20351;&#29992;&#33258;&#30417;&#30563;&#32452;&#32422;&#26463;&#36827;&#34892;&#35757;&#32451;&#65292;&#20026;&#27880;&#24847;&#21147;&#22836;&#36827;&#34892;&#20998;&#32452;&#65292;&#20854;&#20013;&#27599;&#20010;&#32452;&#19987;&#27880;&#20110;&#19968;&#20010;&#37325;&#35201;&#32780;&#29420;&#29305;&#30340;&#29305;&#24449;&#23376;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#25237;&#31080;&#20445;&#30041;&#31243;&#24207;(Voting-to-Stay)&#65292;&#20197;&#21024;&#38500;&#20887;&#20313;&#22836;&#65292;&#20174;&#32780;&#23454;&#29616;&#20855;&#26377;&#26356;&#36731;&#37327;&#32423;&#26435;&#37325;&#30340;&#36716;&#25442;&#22120;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19977;&#20010;&#30693;&#21517;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#19988;&#25105;&#20204;&#30340;&#28040;&#34701;&#30740;&#31350;&#25552;&#20379;&#20102;&#25903;&#25345;&#24615;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have revealed some issues of Multi-Head Attention (MHA), e.g., redundancy and over-parameterization. Specifically, the heads of MHA were originally designed to attend to information from different representation subspaces, whereas prior studies found that some attention heads likely learn similar features and can be pruned without harming performance. Inspired by the minimum-redundancy feature selection, we assume that focusing on the most representative and distinctive features with minimum resources can mitigate the above issues and lead to more effective and efficient MHAs. In particular, we propose Grouped Head Attention, trained with a self-supervised group constraint that group attention heads, where each group focuses on an essential but distinctive feature subset. We additionally propose a Voting-to-Stay procedure to remove redundant heads, thus achieving a transformer with lighter weights. Moreover, our method achieves significant performance gains on three well
&lt;/p&gt;</description></item><item><title>ZeroSCROLLS&#26159;&#19968;&#20010;&#29992;&#20110;&#38271;&#25991;&#26412;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#38646;Shot&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#20845;&#20010;&#20219;&#21153;&#21644;&#22235;&#20010;&#25968;&#25454;&#38598;&#65292;&#33021;&#22815;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#24403;&#21069;&#65292;GPT-4&#30340;&#24179;&#22343;&#24471;&#20998;&#26368;&#39640;&#65292;&#20294;&#22312;&#32858;&#21512;&#20219;&#21153;&#31561;&#22810;&#20010;&#25361;&#25112;&#19978;&#65292;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2305.14196</link><description>&lt;p&gt;
ZeroSCROLLS&#65306;&#19968;&#20010;&#29992;&#20110;&#38271;&#25991;&#26412;&#29702;&#35299;&#30340;&#38646;Shot&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
ZeroSCROLLS: A Zero-Shot Benchmark for Long Text Understanding. (arXiv:2305.14196v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14196
&lt;/p&gt;
&lt;p&gt;
ZeroSCROLLS&#26159;&#19968;&#20010;&#29992;&#20110;&#38271;&#25991;&#26412;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#38646;Shot&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#20845;&#20010;&#20219;&#21153;&#21644;&#22235;&#20010;&#25968;&#25454;&#38598;&#65292;&#33021;&#22815;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#24403;&#21069;&#65292;GPT-4&#30340;&#24179;&#22343;&#24471;&#20998;&#26368;&#39640;&#65292;&#20294;&#22312;&#32858;&#21512;&#20219;&#21153;&#31561;&#22810;&#20010;&#25361;&#25112;&#19978;&#65292;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102; ZeroSCROLLS&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#38271;&#25991;&#26412;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#38646;Shot&#22522;&#20934;&#27979;&#35797;&#65292;&#20165;&#21253;&#21547;&#27979;&#35797;&#38598;&#32780;&#27809;&#26377;&#35757;&#32451;&#25110;&#24320;&#21457;&#25968;&#25454;&#12290;&#25105;&#20204;&#20174;SCROLLS&#22522;&#20934;&#27979;&#35797;&#20013;&#36866;&#24212;&#20102;&#20845;&#20010;&#20219;&#21153;&#65292;&#24182;&#28155;&#21152;&#20102;&#22235;&#20010;&#26032;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#20004;&#20010;&#26032;&#30340;&#20449;&#24687;&#34701;&#21512;&#20219;&#21153;&#65292;&#20363;&#22914;&#32858;&#21512;&#27491;&#38754;&#35780;&#20215;&#30340;&#30334;&#20998;&#27604;&#12290;&#20351;&#29992;ZeroSCROLLS&#65292;&#25105;&#20204;&#23545;&#24320;&#28304;&#21644;&#38381;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;Claude&#20248;&#20110;ChatGPT&#65292;&#24182;&#19988;GPT-4&#33719;&#24471;&#20102;&#26368;&#39640;&#30340;&#24179;&#22343;&#20998;&#25968;&#12290;&#28982;&#32780;&#65292;&#22312;ZeroSCROLLS&#30340;&#22810;&#20010;&#24320;&#25918;&#25361;&#25112;&#26041;&#38754;&#65288;&#20363;&#22914;&#65292;&#32858;&#21512;&#20219;&#21153;&#65289;&#65292;&#36824;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#65292;&#22240;&#20026;&#27169;&#22411;&#24456;&#38590;&#36890;&#36807;&#26420;&#32032;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#30001;&#20110;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#36824;&#22312;&#19981;&#26029;&#26356;&#26032;&#65292;&#25105;&#20204;&#36992;&#35831;&#30740;&#31350;&#20154;&#21592;&#22312;&#23454;&#26102;&#30340;ZeroSCROLLS&#25490;&#34892;&#27036;&#19978;&#35780;&#20272;&#20182;&#20204;&#30340;&#24819;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce ZeroSCROLLS, a zero-shot benchmark for natural language understanding over long texts, which contains only test sets, without training or development data. We adapt six tasks from the SCROLLS benchmark, and add four new datasets, including two novel information fusing tasks, such as aggregating the percentage of positive reviews. Using ZeroSCROLLS, we conduct a comprehensive evaluation of both open-source and closed large language models, finding that Claude outperforms ChatGPT, and that GPT-4 achieves the highest average score. However, there is still room for improvement on multiple open challenges in ZeroSCROLLS, such as aggregation tasks, where models struggle to pass the naive baseline. As the state of the art is a moving target, we invite researchers to evaluate their ideas on the live ZeroSCROLLS leaderboard
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24605;&#32500;&#38142;&#26465;&#24494;&#35843;&#65288;CoT fine-tuning&#65289;&#26469;&#25552;&#39640;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;CoT Collection&#25968;&#25454;&#38598;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#36880;&#27493;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26410;&#35265;&#20219;&#21153;&#21644;4&#20010;&#29305;&#23450;&#39046;&#22495;&#20219;&#21153;&#19978;&#65292;&#36890;&#36807;CoT fine-tuning&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#27169;&#22411;&#30340;&#20934;&#30830;&#24230;&#21644;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.14045</link><description>&lt;p&gt;
CoT Collection: &#36890;&#36807;&#24605;&#32500;&#38142;&#26465;&#24494;&#35843;&#26469;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning. (arXiv:2305.14045v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24605;&#32500;&#38142;&#26465;&#24494;&#35843;&#65288;CoT fine-tuning&#65289;&#26469;&#25552;&#39640;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;CoT Collection&#25968;&#25454;&#38598;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#36880;&#27493;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26410;&#35265;&#20219;&#21153;&#21644;4&#20010;&#29305;&#23450;&#39046;&#22495;&#20219;&#21153;&#19978;&#65292;&#36890;&#36807;CoT fine-tuning&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#27169;&#22411;&#30340;&#20934;&#30830;&#24230;&#21644;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#30693;&#23545;&#20110;&#23567;&#20110;100&#20159;&#21442;&#25968;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#35299;&#20915;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#26102;&#65292;&#20854;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;&#33021;&#21147;&#19981;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#24605;&#32500;&#38142;&#26465;&#30340;&#32622;&#20449;&#24230;&#35843;&#25972;&#26469;&#36171;&#20104;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#36880;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#35843;&#25972;&#25351;&#20196;&#25968;&#25454;&#38598;&#8212;&#8212;CoT Collection&#65292;&#35813;&#25968;&#25454;&#38598;&#36890;&#36807;&#22686;&#21152;184&#19975;&#20010;&#32622;&#20449;&#24230;&#27880;&#37322;&#21040;1060&#20010;&#20219;&#21153;&#30340;&#29616;&#26377;Flan Collection&#65288;&#21253;&#21547;9&#20010;&#24605;&#32500;&#38142;&#26465;&#20219;&#21153;&#65289;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#26410;&#35265;&#20219;&#21153;&#19978;&#20351;&#29992;CoT Collection&#23545;Flan-T5&#65288;3B&#21644;11B&#65289;&#36827;&#34892;&#24605;&#32500;&#38142;&#26465;&#24494;&#35843;&#65292;&#20351;&#24471;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22312;&#24605;&#32500;&#38142;&#26465;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;&#22312;BIG-Bench-Hard&#65288;BBH&#65289;&#22522;&#20934;&#27979;&#35797;&#19978;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#38646;&#26679;&#26412;&#20219;&#21153;&#20934;&#30830;&#24230;&#26041;&#38754;&#30340;&#24179;&#22343;&#25552;&#21319;&#65306;+4.34%&#65288;Flan-T5 3B&#65289;&#21644;+2.60%&#65288;Flan-T5 11B&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#20351;&#29992;CoT Collection&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#20351;&#35821;&#35328;&#27169;&#22411;&#22312;4&#20010;&#29305;&#23450;&#39046;&#22495;&#20219;&#21153;&#19978;&#20855;&#26377;&#26356;&#24378;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) with less than 100B parameters are known to perform poorly on chain-of-thought (CoT) reasoning in contrast to large LMs when solving unseen tasks. In this work, we aim to equip smaller LMs with the step-by-step reasoning capability by instruction tuning with CoT rationales. In order to achieve this goal, we first introduce a new instruction-tuning dataset called the CoT Collection, which augments the existing Flan Collection (including only 9 CoT tasks) with additional 1.84 million rationales across 1,060 tasks. We show that CoT fine-tuning Flan-T5 (3B &amp; 11B) with CoT Collection enables smaller LMs to have better CoT capabilities on unseen tasks. On the BIG-Bench-Hard (BBH) benchmark, we report an average improvement of +4.34% (Flan-T5 3B) and +2.60% (Flan-T5 11B), in terms of zero-shot task accuracy. Furthermore, we show that instruction tuning with CoT Collection allows LMs to possess stronger few-shot learning capabilities on 4 domain-specific tasks, resulting 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#35805;&#31995;&#32479;&#65292;&#38024;&#23545;&#28548;&#28165;&#12289;&#30446;&#26631;&#23548;&#21521;&#21644;&#38750;&#21327;&#20316;&#23545;&#35805;&#65292;&#25552;&#20986;&#20102;Proactive Chain-of-Thought&#25552;&#31034;&#26041;&#26696;&#65292;&#20197;&#22686;&#24378;&#31995;&#32479;&#30340;&#20027;&#21160;&#24615;&#33021;&#21147;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#32463;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.13626</link><description>&lt;p&gt;
&#28608;&#21169;&#21644;&#35780;&#20272;&#29992;&#20110;&#20027;&#21160;&#23545;&#35805;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#28548;&#28165;&#12289;&#30446;&#26631;&#23548;&#21521;&#21644;&#38750;&#21327;&#20316;&#24615;
&lt;/p&gt;
&lt;p&gt;
Prompting and Evaluating Large Language Models for Proactive Dialogues: Clarification, Target-guided, and Non-collaboration. (arXiv:2305.13626v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#35805;&#31995;&#32479;&#65292;&#38024;&#23545;&#28548;&#28165;&#12289;&#30446;&#26631;&#23548;&#21521;&#21644;&#38750;&#21327;&#20316;&#23545;&#35805;&#65292;&#25552;&#20986;&#20102;Proactive Chain-of-Thought&#25552;&#31034;&#26041;&#26696;&#65292;&#20197;&#22686;&#24378;&#31995;&#32479;&#30340;&#20027;&#21160;&#24615;&#33021;&#21147;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#32463;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23545;&#35805;&#31995;&#32479;&#65292;&#22914;ChatGPT&#65292;&#22312;&#19978;&#19979;&#25991;&#29702;&#35299;&#21644;&#21709;&#24212;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#24322;&#24120;&#30340;&#29087;&#32451;&#31243;&#24230;&#65292;&#20294;&#26159;&#65292;&#23613;&#31649;&#23427;&#20204;&#20855;&#26377;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#20173;&#28982;&#23384;&#22312;&#38480;&#21046;&#65292;&#20363;&#22914;&#23545;&#27169;&#31946;&#26597;&#35810;&#25552;&#20379;&#38543;&#26426;&#29468;&#27979;&#31572;&#26696;&#25110;&#26080;&#27861;&#25298;&#32477;&#29992;&#25143;&#30340;&#35831;&#27714;&#65292;&#36825;&#20123;&#37117;&#34987;&#35748;&#20026;&#26159;&#23545;&#35805;&#20195;&#29702;&#30340;&#20027;&#21160;&#24615;&#26041;&#38754;&#12290;&#36825;&#24341;&#21457;&#20102;LLM&#22522;&#20110;&#23545;&#35805;&#31995;&#32479;&#26159;&#21542;&#33021;&#22815;&#22788;&#29702;&#20027;&#21160;&#23545;&#35805;&#38382;&#39064;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#22522;&#20110;LLM&#30340;&#23545;&#35805;&#31995;&#32479;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#20998;&#26512;&#65292;&#20855;&#20307;&#20851;&#27880;&#20027;&#21160;&#23545;&#35805;&#31995;&#32479;&#30340;&#19977;&#20010;&#26041;&#38754;&#65306;&#28548;&#28165;&#12289;&#30446;&#26631;&#23548;&#21521;&#21644;&#38750;&#21327;&#20316;&#23545;&#35805;&#12290;&#20026;&#20102;&#35302;&#21457;LLM&#30340;&#20027;&#21160;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Proactive Chain-of-Thought&#25552;&#31034;&#26041;&#26696;&#65292;&#23427;&#36890;&#36807;&#23545;&#25551;&#36848;&#24615;&#25512;&#29702;&#38142;&#30340;&#30446;&#26631;&#35268;&#21010;&#33021;&#21147;&#22686;&#24378;&#20102;LLM&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#23454;&#35777;&#32467;&#26524;&#20197;&#20419;&#36827;&#26410;&#26469;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational systems based on Large Language Models (LLMs), such as ChatGPT, show exceptional proficiency in context understanding and response generation. However, despite their impressive capabilities, they still possess limitations, such as providing randomly-guessed answers to ambiguous queries or failing to refuse users' requests, both of which are considered aspects of a conversational agent's proactivity. This raises the question of whether LLM-based conversational systems are equipped to handle proactive dialogue problems. In this work, we conduct a comprehensive analysis of LLM-based conversational systems, specifically focusing on three aspects of proactive dialogue systems: clarification, target-guided, and non-collaborative dialogues. To trigger the proactivity of LLMs, we propose the Proactive Chain-of-Thought prompting scheme, which augments LLMs with the goal planning capability over descriptive reasoning chains. Empirical findings are discussed to promote future studi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21069;&#32512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#24102;&#26377;&#28304;&#20219;&#21153;&#30340;&#22266;&#23450;&#25991;&#26412;&#34920;&#31034;&#12290;&#29420;&#31435;&#22320;&#23398;&#20064;&#27599;&#20010;&#28304;&#20219;&#21153;&#30340;&#20219;&#21153;&#29305;&#23450;&#21069;&#32512;&#65292;&#24182;&#23558;&#23427;&#20204;&#32452;&#21512;&#25104;&#26368;&#32456;&#34920;&#31034;&#65292;&#20197;&#35299;&#20915;&#22914;&#20309;&#23398;&#20064;&#26131;&#20110;&#26356;&#26032;&#12289;&#36866;&#29992;&#24191;&#27867;&#30340;&#36890;&#29992;&#25991;&#26412;&#34920;&#31034;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.13499</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#36866;&#24212;&#20219;&#21153;&#29305;&#23450;&#21069;&#32512;&#23398;&#20064;&#26131;&#20110;&#26356;&#26032;&#30340;&#36890;&#29992;&#25991;&#26412;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Easily Updated General Purpose Text Representations with Adaptable Task-Specific Prefixes. (arXiv:2305.13499v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13499
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21069;&#32512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#24102;&#26377;&#28304;&#20219;&#21153;&#30340;&#22266;&#23450;&#25991;&#26412;&#34920;&#31034;&#12290;&#29420;&#31435;&#22320;&#23398;&#20064;&#27599;&#20010;&#28304;&#20219;&#21153;&#30340;&#20219;&#21153;&#29305;&#23450;&#21069;&#32512;&#65292;&#24182;&#23558;&#23427;&#20204;&#32452;&#21512;&#25104;&#26368;&#32456;&#34920;&#31034;&#65292;&#20197;&#35299;&#20915;&#22914;&#20309;&#23398;&#20064;&#26131;&#20110;&#26356;&#26032;&#12289;&#36866;&#29992;&#24191;&#27867;&#30340;&#36890;&#29992;&#25991;&#26412;&#34920;&#31034;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#38656;&#35201;&#20174;&#30456;&#21516;&#30340;&#25991;&#26412;&#20013;&#36827;&#34892;&#22810;&#27425;&#39044;&#27979;&#12290;&#38024;&#23545;&#27599;&#20010;&#19979;&#28216;&#20219;&#21153;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20250;&#22312;&#25512;&#26029;&#26102;&#24102;&#26469;&#35745;&#31639;&#36127;&#25285;&#65292;&#22240;&#20026;&#38656;&#35201;&#22810;&#27425;&#27491;&#21521;&#20256;&#36882;&#12290;&#20026;&#20102;&#25674;&#38144;&#35745;&#31639;&#25104;&#26412;&#65292;&#20923;&#32467;&#35821;&#35328;&#27169;&#22411;&#24182;&#22522;&#20110;&#22266;&#23450;&#25991;&#26412;&#34920;&#31034;&#20026;&#19979;&#28216;&#20219;&#21153;&#24314;&#31435;&#36731;&#37327;&#32423;&#27169;&#22411;&#26159;&#24120;&#35265;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22240;&#27492;&#65292;&#22914;&#20309;&#23398;&#20064;&#19968;&#31181;&#22266;&#23450;&#20294;&#36890;&#29992;&#30340;&#25991;&#26412;&#34920;&#31034;&#65292;&#20197;&#20415;&#22312;&#26410;&#30693;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#25104;&#20026;&#19968;&#39033;&#25361;&#25112;&#12290;&#36807;&#21435;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#20197;&#22810;&#20219;&#21153;&#30340;&#26041;&#24335;&#23545;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#21487;&#20197;&#25552;&#39640;&#34920;&#31034;&#30340;&#36890;&#29992;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21069;&#32512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#24102;&#26377;&#28304;&#20219;&#21153;&#30340;&#22266;&#23450;&#25991;&#26412;&#34920;&#31034;&#12290;&#25105;&#20204;&#29420;&#31435;&#22320;&#23398;&#20064;&#27599;&#20010;&#28304;&#20219;&#21153;&#30340;&#20219;&#21153;&#29305;&#23450;&#21069;&#32512;&#65292;&#24182;&#23558;&#23427;&#20204;&#32452;&#21512;&#25104;&#26368;&#32456;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;...
&lt;/p&gt;
&lt;p&gt;
Many real-world applications require making multiple predictions from the same text. Fine-tuning a large pre-trained language model for each downstream task causes computational burdens in the inference time due to several times of forward passes. To amortize the computational cost, freezing the language model and building lightweight models for downstream tasks based on fixed text representations are common solutions. Accordingly, how to learn fixed but general text representations that can generalize well to unseen downstream tasks becomes a challenge. Previous works have shown that the generalizability of representations can be improved by fine-tuning the pre-trained language model with some source tasks in a multi-tasking way. In this work, we propose a prefix-based method to learn the fixed text representations with source tasks. We learn a task-specific prefix for each source task independently and combine them to get the final representations. Our experimental results show that 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Lion&#30340;&#23545;&#25239;&#24615;&#33976;&#39311;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#20174;&#22797;&#26434;&#30340;&#31169;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21521;&#32039;&#20945;&#30340;&#24320;&#28304;&#27169;&#22411;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#36845;&#20195;&#22320;&#25552;&#39640;&#23398;&#29983;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#21033;&#29992;&#25945;&#24072;&#27169;&#22411;&#30340;&#36866;&#24212;&#24615;&#26469;&#29983;&#25104;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#25351;&#20196;&#65292;&#24182;&#36890;&#36807;&#27169;&#20223;&#12289;&#36776;&#21035;&#21644;&#29983;&#25104;&#30340;&#19977;&#38454;&#27573;&#23545;&#25239;&#24490;&#29615;&#26469;&#20419;&#36827;&#30693;&#35782;&#30340;&#36801;&#31227;&#12290;</title><link>http://arxiv.org/abs/2305.12870</link><description>&lt;p&gt;
Lion: &#31169;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#25239;&#24615;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Lion: Adversarial Distillation of Proprietary Large Language Models. (arXiv:2305.12870v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12870
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Lion&#30340;&#23545;&#25239;&#24615;&#33976;&#39311;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#20174;&#22797;&#26434;&#30340;&#31169;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21521;&#32039;&#20945;&#30340;&#24320;&#28304;&#27169;&#22411;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#36845;&#20195;&#22320;&#25552;&#39640;&#23398;&#29983;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#21033;&#29992;&#25945;&#24072;&#27169;&#22411;&#30340;&#36866;&#24212;&#24615;&#26469;&#29983;&#25104;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#25351;&#20196;&#65292;&#24182;&#36890;&#36807;&#27169;&#20223;&#12289;&#36776;&#21035;&#21644;&#29983;&#25104;&#30340;&#19977;&#38454;&#27573;&#23545;&#25239;&#24490;&#29615;&#26469;&#20419;&#36827;&#30693;&#35782;&#30340;&#36801;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#22797;&#26434;&#30340;&#31169;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21521;&#32039;&#20945;&#30340;&#24320;&#28304;LLM&#36716;&#31227;&#30693;&#35782;&#30340;&#20570;&#27861;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#36890;&#36807;&#23558;&#23398;&#29983;&#27169;&#22411;&#30340;&#21709;&#24212;&#19982;&#25945;&#24072;&#27169;&#22411;&#30340;&#21709;&#24212;&#23545;&#40784;&#21040;&#19968;&#32452;&#25351;&#20196;&#26469;&#36827;&#34892;&#21333;&#21521;&#30693;&#35782;&#33976;&#39311;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#24573;&#35270;&#20102;&#22312;&#25193;&#23637;&#23398;&#20064;&#20013;&#23398;&#29983;&#27169;&#22411;&#34920;&#29616;&#19981;&#20339;&#30340;&#25361;&#25112;&#24615;&#25351;&#20196;&#30340;&#21487;&#33021;&#24615;&#65292;&#20174;&#32780;&#21487;&#20197;&#36845;&#20195;&#22320;&#25552;&#39640;&#23398;&#29983;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#25239;&#24615;&#33976;&#39311;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;&#36890;&#36807;&#21033;&#29992;LLMs&#30340;&#22810;&#21151;&#33021;&#35282;&#33394;&#36866;&#24212;&#24615;&#65292;&#25105;&#20204;&#20419;&#20351;&#25945;&#24072;&#27169;&#22411;&#35782;&#21035;&#8220;&#38590;&#8221;&#30340;&#25351;&#20196;&#24182;&#20026;&#23398;&#29983;&#27169;&#22411;&#29983;&#25104;&#26032;&#30340;&#8220;&#38590;&#8221;&#30340;&#25351;&#20196;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#19977;&#38454;&#27573;&#30340;&#23545;&#25239;&#24490;&#29615;&#65306;&#27169;&#20223;&#12289;&#36776;&#21035;&#21644;&#29983;&#25104;&#12290;&#36890;&#36807;&#24212;&#29992;&#36825;&#31181;&#23545;&#25239;&#24615;&#26694;&#26550;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#30693;&#35782;&#30340;&#36801;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
The practice of transferring knowledge from a sophisticated, proprietary large language model (LLM) to a compact, open-source LLM has garnered considerable attention. Previous works have focused on a unidirectional knowledge distillation way by aligning the responses of the student model with those of the teacher model to a set of instructions. Nevertheless, they overlooked the possibility of incorporating any reciprocal "feedback"--identifying challenging instructions where the student model's performance falls short--to boost the student model's proficiency iteratively. To this end, we propose a novel adversarial distillation framework for a more efficient knowledge transfer. Leveraging the versatile role adaptability of LLMs, we prompt the teacher model to identify "hard" instructions and generate new "hard" instructions for the student model, creating a three-stage adversarial loop of imitation, discrimination, and generation. By applying this adversarial framework, we successfully
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#23545;&#22810;&#22810;&#27169;&#24577;&#25688;&#35201;&#20219;&#21153;&#65288;M$^3$S&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#30693;&#35782;&#33976;&#39311;&#21644;&#38754;&#21521;&#30446;&#26631;&#30340;&#35270;&#35273;&#24314;&#27169;&#26694;&#26550;&#26469;&#35299;&#20915;&#35813;&#20219;&#21153;&#65292;&#23427;&#21516;&#26102;&#25552;&#39640;&#20102;&#22810;&#27169;&#24577;&#21333;&#35821;&#25688;&#35201;&#65288;MMS&#65289;&#21644;&#22810;&#27169;&#24577;&#36328;&#35821;&#35328;&#25688;&#35201;&#65288;MXLS&#65289;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.12767</link><description>&lt;p&gt;
D$^2$TV: &#21452;&#30693;&#35782;&#33976;&#39311;&#21644;&#38754;&#21521;&#30446;&#26631;&#30340;&#35270;&#35273;&#24314;&#27169;&#29992;&#20110;&#22810;&#23545;&#22810;&#22810;&#27169;&#24577;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
D$^2$TV: Dual Knowledge Distillation and Target-oriented Vision Modeling for Many-to-Many Multimodal Summarization. (arXiv:2305.12767v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12767
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#23545;&#22810;&#22810;&#27169;&#24577;&#25688;&#35201;&#20219;&#21153;&#65288;M$^3$S&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#30693;&#35782;&#33976;&#39311;&#21644;&#38754;&#21521;&#30446;&#26631;&#30340;&#35270;&#35273;&#24314;&#27169;&#26694;&#26550;&#26469;&#35299;&#20915;&#35813;&#20219;&#21153;&#65292;&#23427;&#21516;&#26102;&#25552;&#39640;&#20102;&#22810;&#27169;&#24577;&#21333;&#35821;&#25688;&#35201;&#65288;MMS&#65289;&#21644;&#22810;&#27169;&#24577;&#36328;&#35821;&#35328;&#25688;&#35201;&#65288;MXLS&#65289;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23545;&#22810;&#22810;&#27169;&#24577;&#25688;&#35201;(M$^3$S)&#20219;&#21153;&#26088;&#22312;&#29983;&#25104;&#20219;&#20309;&#35821;&#35328;&#30340;&#25688;&#35201;&#65292;&#24182;&#20351;&#29992;&#20219;&#20309;&#35821;&#35328;&#30340;&#25991;&#26723;&#36755;&#20837;&#21644;&#30456;&#24212;&#30340;&#22270;&#20687;&#24207;&#21015;&#65292;&#20854;&#26412;&#36136;&#19978;&#21253;&#25324;&#22810;&#27169;&#24577;&#21333;&#35821;&#25688;&#35201;(MMS)&#21644;&#22810;&#27169;&#24577;&#36328;&#35821;&#35328;&#25688;&#35201;(MXLS)&#20219;&#21153;&#12290;&#34429;&#28982;&#36817;&#24180;&#26469;&#24050;&#32463;&#26377;&#24456;&#22810;&#24037;&#20316;&#33268;&#21147;&#20110;MMS&#25110;MXLS&#65292;&#24182;&#19988;&#33719;&#24471;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#20851;&#27880;M$^3$S&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#20197;&#19979;&#20004;&#20010;&#26041;&#38754;&#65306;1)&#21033;&#29992;MMS&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#26469;&#25552;&#39640;MXLS&#30340;&#24615;&#33021;&#65292;&#20294;&#24573;&#30053;&#20102;MMS&#30340;&#24615;&#33021;&#65307;2)&#36890;&#36807;&#38544;&#24335;&#23398;&#20064;&#25110;&#26174;&#24335;&#22797;&#26434;&#30340;&#35757;&#32451;&#30446;&#26631;&#26469;&#25913;&#36827;MMS&#27169;&#22411;&#65292;&#20197;&#36807;&#28388;&#19982;&#25688;&#35201;&#26080;&#20851;&#30340;&#35270;&#35273;&#29305;&#24449;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#29992;&#19988;&#23454;&#29992;&#30340;&#20219;&#21153;&#65292;&#21363;M$^3$S&#12290;&#36827;&#19968;&#27493;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;M$^3$S&#20219;&#21153;&#30340;&#21452;&#30693;&#35782;&#33976;&#39311;&#21644;&#38754;&#21521;&#30446;&#26631;&#30340;&#35270;&#35273;&#24314;&#27169;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#21452;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#30830;&#20445;&#20102;&#21516;&#26102;&#25552;&#39640;MMS&#21644;MXLS&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many-to-many multimodal summarization (M$^3$S) task aims to generate summaries in any language with document inputs in any language and the corresponding image sequence, which essentially comprises multimodal monolingual summarization (MMS) and multimodal cross-lingual summarization (MXLS) tasks. Although much work has been devoted to either MMS or MXLS and has obtained increasing attention in recent years, little research pays attention to the M$^3$S task. Besides, existing studies mainly focus on 1) utilizing MMS to enhance MXLS via knowledge distillation without considering the performance of MMS or 2) improving MMS models by filtering summary-unrelated visual features with implicit learning or explicitly complex training objectives. In this paper, we first introduce a general and practical task, i.e., M$^3$S. Further, we propose a dual knowledge distillation and target-oriented vision modeling framework for the M$^3$S task. Specifically, the dual knowledge distillation method guara
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#36923;&#36753;&#39537;&#21160;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;&#21407;&#22987;&#25991;&#26412;&#36716;&#25442;&#20026;&#25277;&#35937;&#24847;&#20041;&#34920;&#36848;&#22270;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#36923;&#36753;&#20462;&#25913;&#21644;&#36716;&#25442;&#65292;&#29983;&#25104;&#22686;&#24378;&#25968;&#25454;&#65292;&#20174;&#32780;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#20307;&#31995;&#32467;&#26500;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.12599</link><description>&lt;p&gt;
&#36890;&#36807;&#36923;&#36753;&#39537;&#21160;&#30340;&#25968;&#25454;&#22686;&#24378;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Enhancing Logical Reasoning of Large Language Models through Logic-Driven Data Augmentation. (arXiv:2305.12599v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#36923;&#36753;&#39537;&#21160;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;&#21407;&#22987;&#25991;&#26412;&#36716;&#25442;&#20026;&#25277;&#35937;&#24847;&#20041;&#34920;&#36848;&#22270;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#36923;&#36753;&#20462;&#25913;&#21644;&#36716;&#25442;&#65292;&#29983;&#25104;&#22686;&#24378;&#25968;&#25454;&#65292;&#20174;&#32780;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#20307;&#31995;&#32467;&#26500;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#36923;&#36753;&#25512;&#29702;&#30456;&#32467;&#21512;&#21487;&#20197;&#22686;&#24378;&#23427;&#20204;&#22312;&#38382;&#39064;&#35299;&#20915;&#20013;&#30340;&#33021;&#21147;&#65292;&#20351;&#20854;&#26356;&#21152;&#24378;&#22823;&#21644;&#21487;&#38752;&#12290;&#28982;&#32780;&#65292;&#36923;&#36753;&#25512;&#29702;&#30340;&#22797;&#26434;&#24615;&#20351;&#24471;&#20174;&#32593;&#39029;&#19978;&#25910;&#38598;&#21487;&#38752;&#30340;&#25968;&#25454;&#26469;&#24314;&#31435;&#20840;&#38754;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#38754;&#20020;&#22256;&#38590;&#65292;&#36827;&#32780;&#24433;&#21709;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36923;&#36753;&#39537;&#21160;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;AMR-LDA&#12290;AMR-LDA&#23558;&#21407;&#22987;&#25991;&#26412;&#36716;&#25442;&#25104;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#65288;AMR&#65289;&#22270;&#65292;&#36825;&#26159;&#19968;&#31181;&#32467;&#26500;&#21270;&#30340;&#35821;&#20041;&#34920;&#31034;&#65292;&#21253;&#21547;&#20102;&#21477;&#23376;&#30340;&#36923;&#36753;&#32467;&#26500;&#65292;&#28982;&#21518;&#23545;&#35813;&#22270;&#36827;&#34892;&#25805;&#20316;&#20197;&#29983;&#25104;&#36923;&#36753;&#20462;&#25913;&#21518;&#30340;AMR&#22270;&#12290;&#20462;&#25913;&#21518;&#30340;AMR&#22270;&#38543;&#21518;&#34987;&#36716;&#25442;&#22238;&#25991;&#26412;&#65292;&#20174;&#32780;&#21019;&#24314;&#22686;&#24378;&#25968;&#25454;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20307;&#31995;&#32467;&#26500;&#26080;&#20851;&#65292;&#24182;&#36890;&#36807;&#25552;&#31034;&#22686;&#24378;&#26469;&#22686;&#24378;&#29983;&#25104;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT-3.5&#21644;GPT-4&#65289;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#26469;&#22686;&#24378;&#21028;&#21035;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Combining large language models with logical reasoning enhance their capacity to address problems in a robust and reliable manner. Nevertheless, the intricate nature of logical reasoning poses challenges to gathering reliable data from web for building comprehensive training datasets, subsequently affecting the performance on downstream tasks. To address this, we introduce a novel logic-driven data augmentation approach, AMR-LDA. AMR-LDA converts the original text into an Abstract Meaning Representation (AMR) graph, a structured semantic representation that encapsulates the logic structure of the sentence, upon which operations are performed to generate logically modified AMR graphs. The modified AMR graphs are subsequently converted back into texts to create augmented data. Notably, our methodology is architecture-agnostic and enhances generative large language models, such as GPT-3.5 and GPT-4, through prompt augmentation, and fine-tuning discriminative large language models through 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35299;&#37322;AI&#20013;&#24120;&#29992;&#30340;&#24037;&#20855;LIME&#22312;&#25991;&#26412;&#25968;&#25454;&#19978;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;XAIFooler&#26469;&#25200;&#21160;&#25991;&#26412;&#36755;&#20837;&#24182;&#25805;&#32437;&#35299;&#37322;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.12351</link><description>&lt;p&gt;
&#20320;&#30340;&#35299;&#37322;&#21487;&#38752;&#21527;&#65311;&#36890;&#36807;&#34701;&#21512;XAI&#21644;&#23545;&#25239;&#25915;&#20987;&#26469;&#25506;&#31350;LIME&#22312;&#35299;&#37322;&#25991;&#26412;&#20998;&#31867;&#22120;&#20013;&#30340;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Are Your Explanations Reliable? Investigating the Stability of LIME in Explaining Text Classifiers by Marrying XAI and Adversarial Attack. (arXiv:2305.12351v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12351
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35299;&#37322;AI&#20013;&#24120;&#29992;&#30340;&#24037;&#20855;LIME&#22312;&#25991;&#26412;&#25968;&#25454;&#19978;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;XAIFooler&#26469;&#25200;&#21160;&#25991;&#26412;&#36755;&#20837;&#24182;&#25805;&#32437;&#35299;&#37322;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LIME&#24050;&#32463;&#25104;&#20026;&#21487;&#35299;&#37322;AI&#65288;XAI&#65289;&#26694;&#26550;&#20013;&#26368;&#24120;&#34987;&#24341;&#29992;&#30340;&#24037;&#20855;&#20043;&#19968;&#65292;&#22312;&#20851;&#38190;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#38598;&#25104;&#20854;&#20013;&#65292;&#20363;&#22914;&#21307;&#30103;&#20445;&#20581;&#21644;&#37329;&#34701;&#12290;&#28982;&#32780;&#65292;&#23588;&#20854;&#26159;&#22312;&#25991;&#26412;&#25968;&#25454;&#30340;&#32972;&#26223;&#19979;&#65292;&#20854;&#31283;&#23450;&#24615;&#20173;&#28982;&#40092;&#20026;&#20154;&#30693;&#65292;&#36825;&#26159;&#30001;&#20110;&#25991;&#26412;&#31354;&#38388;&#30340;&#29420;&#29305;&#32422;&#26463;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#39318;&#20808;&#35780;&#20272;&#20102;LIME&#22312;&#25991;&#26412;&#25968;&#25454;&#19978;&#22266;&#26377;&#30340;&#19981;&#31283;&#23450;&#24615;&#65292;&#20197;&#24314;&#31435;&#22522;&#20934;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;XAIFooler&#65292;&#20197;&#25200;&#21160;&#25991;&#26412;&#36755;&#20837;&#24182;&#25805;&#32437;&#35299;&#37322;&#65292;&#23558;LIME&#30340;&#31283;&#23450;&#24615;&#20316;&#20026;&#19968;&#20010;&#25991;&#26412;&#25200;&#21160;&#20248;&#21270;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#12290;XAIFooler&#31526;&#21512;&#32422;&#26463;&#26465;&#20214;&#65292;&#20445;&#30041;&#20102;&#25991;&#26412;&#35821;&#20041;&#21644;&#21407;&#22987;&#39044;&#27979;&#65292;&#24182;&#24341;&#20837;&#20102;Rank-biased Overlap&#65288;RBO&#65289;&#20316;&#20026;XAIFooler&#20248;&#21270;&#30340;&#20851;&#38190;&#37096;&#20998;&#65292;&#20197;&#28385;&#36275;&#25152;&#26377;&#35299;&#37322;&#30456;&#20284;&#24230;&#27979;&#37327;&#30340;&#35201;&#27714;&#12290;&#22312;&#30495;&#23454;&#30340;&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;XAIFool&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
LIME has emerged as one of the most commonly referenced tools in explainable AI (XAI) frameworks that is integrated into critical machine learning applications--e.g., healthcare and finance. However, its stability remains little explored, especially in the context of text data, due to the unique text-space constraints. To address these challenges, in this paper, we first evaluate the inherent instability of LIME on text data to establish a baseline, and then propose a novel algorithm XAIFooler to perturb text inputs and manipulate explanations that casts investigation on the stability of LIME as a text perturbation optimization problem. XAIFooler conforms to the constraints to preserve text semantics and original prediction with small perturbations, and introduces Rank-biased Overlap (RBO) as a key part to guide the optimization of XAIFooler that satisfies all the requirements for explanation similarity measure. Extensive experiments on real-world text datasets demonstrate that XAIFool
&lt;/p&gt;</description></item><item><title>DisCo&#26159;&#19968;&#20010;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#24494;&#35843;&#30001;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#23567;&#22411;&#23398;&#29983;&#27169;&#22411;&#65292;&#37319;&#29992;&#21327;&#21516;&#35757;&#32451;&#25216;&#26415;&#65292;&#36890;&#36807;&#22810;&#35270;&#35282;&#30340;&#30693;&#35782;&#20849;&#20139;&#26469;&#20248;&#21270;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;DisCo&#30456;&#23545;&#20110;&#24050;&#26377;&#26041;&#27861;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#25928;&#26524;&#21644;&#26356;&#23567;&#30340;&#27169;&#22411;&#23610;&#23544;&#12290;</title><link>http://arxiv.org/abs/2305.12074</link><description>&lt;p&gt;
DisCo: &#20351;&#29992;&#33976;&#39311;&#32858;&#21512;&#21327;&#21516;&#35757;&#32451;&#21322;&#30417;&#30563;&#25991;&#26412;&#25366;&#25496;&#30340;&#36731;&#37327;&#32423;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DisCo: Distilled Student Models Co-training for Semi-supervised Text Mining. (arXiv:2305.12074v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12074
&lt;/p&gt;
&lt;p&gt;
DisCo&#26159;&#19968;&#20010;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#24494;&#35843;&#30001;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#23567;&#22411;&#23398;&#29983;&#27169;&#22411;&#65292;&#37319;&#29992;&#21327;&#21516;&#35757;&#32451;&#25216;&#26415;&#65292;&#36890;&#36807;&#22810;&#35270;&#35282;&#30340;&#30693;&#35782;&#20849;&#20139;&#26469;&#20248;&#21270;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;DisCo&#30456;&#23545;&#20110;&#24050;&#26377;&#26041;&#27861;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#25928;&#26524;&#21644;&#26356;&#23567;&#30340;&#27169;&#22411;&#23610;&#23544;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#25991;&#26412;&#25366;&#25496;&#27169;&#22411;&#26159;&#36890;&#36807;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#24494;&#35843;&#22823;&#22411;&#28145;&#24230;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#26500;&#24314;&#30340;&#12290;&#28982;&#32780;&#65292;&#24403;&#25105;&#20204;&#20351;&#29992;&#20855;&#26377;&#26377;&#38480;&#26631;&#35760;&#26679;&#26412;&#30340;&#36731;&#37327;&#32423;&#27169;&#22411;&#26102;&#65292;&#20854;&#20013;&#37325;&#35201;&#30340;&#25361;&#25112;&#26159;&#20445;&#25345;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DisCo&#65292;&#36825;&#26159;&#19968;&#31181;&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#24494;&#35843;&#30001;&#22823;&#22411;PLM&#29983;&#25104;&#30340;&#23567;&#22411;&#23398;&#29983;&#27169;&#22411;&#38431;&#21015;&#65292;&#35813;&#38431;&#21015;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#23519;&#21147;&#26159;&#20849;&#20139;&#31934;&#21326;&#30693;&#35782;&#20197;&#20419;&#36827;&#20854;SSL&#26377;&#25928;&#24615;&#30340;&#33976;&#39311;&#23398;&#29983;&#38431;&#21015;&#20043;&#38388;&#30340;&#30693;&#35782;&#20849;&#20139;&#12290;DisCo&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#21327;&#21516;&#35757;&#32451;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#19981;&#21516;&#30340;&#33976;&#39311;&#31574;&#30053;&#21644;&#21508;&#31181;&#36755;&#20837;&#22686;&#24378;&#20135;&#29983;&#30340;&#27169;&#22411;&#35270;&#22270;&#21644;&#25968;&#25454;&#35270;&#22270;&#19979;&#20419;&#36827;&#23398;&#29983;&#20043;&#38388;&#30340;&#30693;&#35782;&#20849;&#20139;&#26469;&#20248;&#21270;&#22810;&#20010;&#23567;&#23398;&#29983;&#27169;&#22411;&#12290;&#25105;&#20204;&#38024;&#23545;&#21322;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#21644;&#25552;&#21462;&#24335;&#24635;&#32467;&#20219;&#21153;&#23545;DisCo&#36827;&#34892;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DisCo&#21487;&#20197;&#20135;&#29983;&#27604;&#21407;&#22987;&#27169;&#22411;&#23567;7.6&#20493;&#21644;&#27604;&#24050;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many text mining models are constructed by fine-tuning a large deep pre-trained language model (PLM) in downstream tasks. However, a significant challenge is maintaining performance when we use a lightweight model with limited labeled samples. We present DisCo, a semi-supervised learning (SSL) framework for fine-tuning a cohort of small student models generated from a large PLM using knowledge distillation. Our key insight is to share complementary knowledge among distilled student cohorts to promote their SSL effectiveness. DisCo employs a novel co-training technique to optimize multiple small student models by promoting knowledge sharing among students under diversified views: model views produced by different distillation strategies and data views produced by various input augmentations. We evaluate DisCo on both semi-supervised text classification and extractive summarization tasks. Experimental results show that DisCo can produce student models that are 7.6 times smaller and 4.8 t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#24605;&#36335;&#38142;&#32034;&#24341;&#30340;&#26041;&#24335;&#26469;&#21709;&#24212;&#29992;&#25143;&#29366;&#24577;&#65292;&#20197;&#25552;&#20379;&#26356;&#20010;&#24615;&#21270;&#21644;&#26356;&#26377;&#21560;&#24341;&#21147;&#30340;&#29992;&#25143;&#20307;&#39564;&#65292;&#29992;&#35821;&#20041;&#30456;&#20284;&#24615;&#32780;&#38750;&#27979;&#35797;&#26597;&#35810;&#20570;&#20013;&#38388;&#25512;&#29702;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2305.11792</link><description>&lt;p&gt;
LLM&#30340;&#24605;&#36335;&#38142;&#32034;&#24341;&#29992;&#20110;&#22238;&#31572;&#28145;&#20837;&#23545;&#35805;&#38382;&#39064;&#30340;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Chain-of-thought prompting for responding to in-depth dialogue questions with LLM. (arXiv:2305.11792v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#24605;&#36335;&#38142;&#32034;&#24341;&#30340;&#26041;&#24335;&#26469;&#21709;&#24212;&#29992;&#25143;&#29366;&#24577;&#65292;&#20197;&#25552;&#20379;&#26356;&#20010;&#24615;&#21270;&#21644;&#26356;&#26377;&#21560;&#24341;&#21147;&#30340;&#29992;&#25143;&#20307;&#39564;&#65292;&#29992;&#35821;&#20041;&#30456;&#20284;&#24615;&#32780;&#38750;&#27979;&#35797;&#26597;&#35810;&#20570;&#20013;&#38388;&#25512;&#29702;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#25552;&#38382;&#30340;&#26041;&#24335;&#21644;&#20869;&#23481;&#21487;&#20197;&#27934;&#23519;&#20182;&#20204;&#30340;&#24403;&#21069;&#29366;&#24577;&#65292;&#21253;&#25324;&#20154;&#26684;&#12289;&#24773;&#24863;&#21644;&#24515;&#29702;&#29366;&#24577;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#24605;&#36335;&#38142;&#32034;&#24341;&#30340;&#26041;&#24335;&#26469;&#24110;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#21644;&#35268;&#21010;&#65292;&#20197;&#21709;&#24212;&#29992;&#25143;&#29366;&#24577;&#65292;&#20197;&#25552;&#20379;&#26356;&#20010;&#24615;&#21270;&#21644;&#26356;&#26377;&#21560;&#24341;&#21147;&#30340;&#29992;&#25143;&#20307;&#39564;&#12290;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#20010;&#21253;&#25324;6&#20010;&#33521;&#35821;&#21644;&#20013;&#25991;&#30340;&#23545;&#35805;&#25110;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#29992;&#25143;&#29366;&#24577;&#30340;3&#20010;&#19981;&#21516;&#26041;&#38754;&#65288;&#21253;&#25324;&#20154;&#26684;&#12289;&#24773;&#24863;&#21644;&#24515;&#29702;&#65289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20851;&#20110;&#29992;&#25143;&#29366;&#24577;&#30340;&#21709;&#24212;&#20316;&#20026;&#20013;&#38388;&#25512;&#29702;&#22788;&#29702;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20013;&#38388;&#25512;&#29702;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#32780;&#38750;&#27979;&#35797;&#26597;&#35810;&#30340;&#26032;&#39062;&#28436;&#31034;&#36873;&#25321;&#31574;&#30053;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The way and content in which users ask questions can provide insight into their current status, including their personality, emotions, and psychology. Instead of directly prompting the large language models (LLMs), we explore how chain-of-thought prompting helps in this scenario to perform reasoning and planning according to user status, aiming to provide a more personalized and engaging experience for the user query. To this end, we first construct a benchmark of 6 dialogue or question-answering datasets in both English and Chinese, covering 3 different aspects of user status (\textit{including} \textit{personality}, \textit{emotion}, and \textit{psychology}). Then we prompt the LLMs to generate the response regarding the user status as intermediate reasoning processing. We propose a novel demonstration selection strategy using the semantic similarity of intermediate reasoning instead of test queries. To evaluate the effectiveness and robustness of our approach, we conduct extensive e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#34892;&#19994;&#20113;&#29305;&#23450;QA&#25968;&#25454;&#38598; MSQA&#65292;&#35813;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#35780;&#20272;&#26088;&#22312;&#25552;&#39640;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29305;&#23450;&#39046;&#22495;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#20132;&#20114;&#33539;&#24335;&#65292;&#21487;&#20197;&#20351;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#20854;&#19981;&#25797;&#38271;&#30340;&#29305;&#23450;&#20219;&#21153;&#19978;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.11541</link><description>&lt;p&gt;
&#25552;&#21319;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#24037;&#19994;&#39046;&#22495;&#29305;&#23450;&#38382;&#31572;&#20013;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Empower Large Language Model to Perform Better on Industrial Domain-Specific Question Answering. (arXiv:2305.11541v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#34892;&#19994;&#20113;&#29305;&#23450;QA&#25968;&#25454;&#38598; MSQA&#65292;&#35813;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#35780;&#20272;&#26088;&#22312;&#25552;&#39640;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29305;&#23450;&#39046;&#22495;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#20132;&#20114;&#33539;&#24335;&#65292;&#21487;&#20197;&#20351;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#20854;&#19981;&#25797;&#38271;&#30340;&#29305;&#23450;&#20219;&#21153;&#19978;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#24320;&#25918;&#39046;&#22495;&#20219;&#21153;&#20013;&#33719;&#24471;&#20102;&#24191;&#27867;&#24212;&#29992;&#21644;&#21331;&#36234;&#30340;&#25104;&#26524;&#65292;&#20294;&#20854;&#22312;&#30495;&#23454;&#30340;&#24037;&#19994;&#29305;&#23450;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#36890;&#24120;&#24456;&#24179;&#24248;&#65292;&#22240;&#20026;&#23427;&#32570;&#20047;&#29305;&#23450;&#39046;&#22495;&#30340;&#30693;&#35782;&#12290;&#36825;&#20010;&#38382;&#39064;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#30456;&#20851;&#22522;&#20934;&#27979;&#35797;&#24456;&#23569;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#21517;&#20026;MSQA&#30340;&#22522;&#20934;&#38382;&#31572;&#65288;QA&#65289;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#28041;&#21450;Microsoft&#20135;&#21697;&#21644;&#23458;&#25143;&#36935;&#21040;&#30340;IT&#25216;&#26415;&#38382;&#39064;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#34892;&#19994;&#20113;&#30340;&#29305;&#23450;QA&#30693;&#35782;&#65292;&#36825;&#23545;&#20110;&#19968;&#33324;&#30340;LLM&#26469;&#35828;&#26159;&#19981;&#21487;&#29992;&#30340;&#65292;&#22240;&#27492;&#38750;&#24120;&#36866;&#21512;&#35780;&#20272;&#26088;&#22312;&#25552;&#39640;LLM&#29305;&#23450;&#39046;&#22495;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#20132;&#20114;&#33539;&#24335;&#65292;&#21487;&#20197;&#20351;LLM&#22312;&#20854;&#19981;&#25797;&#38271;&#30340;&#29305;&#23450;&#20219;&#21153;&#19978;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36981;&#24490;&#25105;&#20204;&#30340;&#27169;&#22411;&#34701;&#21512;&#26694;&#26550;&#30340;&#26041;&#27861;&#27604;&#20351;&#29992;&#26816;&#32034;&#26041;&#27861;&#30340;&#24120;&#29992;LLM&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Model (LLM) has gained popularity and achieved remarkable results in open-domain tasks, but its performance in real industrial domain-specific scenarios is average since there is no specific knowledge in it. This issue has attracted widespread attention, but there are few relevant benchmarks available. In this paper, we provide a benchmark Question Answering (QA) dataset named MSQA, which is about Microsoft products and IT technical problems encountered by customers. This dataset contains industry cloud-specific QA knowledge, which is not available for general LLM, so it is well suited for evaluating methods aimed at improving domain-specific capabilities of LLM. In addition, we propose a new model interaction paradigm that can empower LLM to achieve better performance on domain-specific tasks where it is not proficient. Extensive experiments demonstrate that the approach following our model fusion framework outperforms the commonly used LLM with retrieval methods.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#36827;&#34892;&#32467;&#26500;&#26356;&#25913;&#12289;&#39069;&#22806;&#35757;&#32451;&#12289;&#25110;&#35757;&#32451;&#19987;&#38376;&#32593;&#32476;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;LLM&#26469;&#35835;&#21462;&#21644;&#29983;&#25104;&#20687;&#25991;&#26412;&#19968;&#26679;&#30340;&#22270;&#20687;&#65292;&#24182;&#24212;&#29992;&#20110;&#33016;&#37096;X&#32447;&#65288;CXR&#65289;&#22270;&#20687;&#30340;&#29983;&#25104;&#20219;&#21153;&#20013;&#12290;</title><link>http://arxiv.org/abs/2305.11490</link><description>&lt;p&gt;
LLM&#33258;&#36523;&#21487;&#35835;&#21462;&#21644;&#29983;&#25104;CXR&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
LLM Itself Can Read and Generate CXR Images. (arXiv:2305.11490v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11490
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#36827;&#34892;&#32467;&#26500;&#26356;&#25913;&#12289;&#39069;&#22806;&#35757;&#32451;&#12289;&#25110;&#35757;&#32451;&#19987;&#38376;&#32593;&#32476;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;LLM&#26469;&#35835;&#21462;&#21644;&#29983;&#25104;&#20687;&#25991;&#26412;&#19968;&#26679;&#30340;&#22270;&#20687;&#65292;&#24182;&#24212;&#29992;&#20110;&#33016;&#37096;X&#32447;&#65288;CXR&#65289;&#22270;&#20687;&#30340;&#29983;&#25104;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20511;&#21161;&#20110;&#36817;&#26399;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26174;&#33879;&#21457;&#23637;&#65292;&#20154;&#20204;&#27491;&#31215;&#26497;&#23581;&#35797;&#23558;LLMs&#30340;&#23454;&#29992;&#24615;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#20219;&#21153;&#12290;&#24050;&#32463;&#26377;&#20154;&#23581;&#35797;&#36830;&#25509;&#35821;&#35328;&#21644;&#35270;&#35273;&#20449;&#24687;&#65292;&#24182;&#19988;&#20063;&#22312;&#19981;&#26029;&#23581;&#35797;&#20026;LLMs&#28155;&#21152;&#35270;&#35273;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23581;&#35797;&#21482;&#20351;&#29992;LLMs&#20316;&#20026;&#22270;&#20687;&#35299;&#30721;&#22120;&#65292;&#27809;&#26377;&#23581;&#35797;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#26469;&#29983;&#25104;&#22270;&#20687;&#12290;&#36890;&#36807;&#37319;&#29992;VQ-GAN&#26694;&#26550;&#65292;&#23558;&#22270;&#20687;&#30340;&#28508;&#22312;&#34920;&#31034;&#35270;&#20026;&#19968;&#31181;&#25991;&#26412;&#26631;&#35760;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;LLM&#65292;&#20197;&#20687;&#25991;&#26412;&#19968;&#26679;&#35835;&#21462;&#21644;&#29983;&#25104;&#22270;&#20687;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#32467;&#26500;&#26356;&#25913;&#12289;&#39069;&#22806;&#30340;&#35757;&#32451;&#30446;&#26631;&#25110;&#35757;&#32451;&#19987;&#38376;&#30340;&#32593;&#32476;&#65292;&#21516;&#26102;&#20173;&#20445;&#30041;LLM&#30340;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;&#27492;&#26694;&#26550;&#24212;&#29992;&#20110;&#33016;&#37096;X&#32447;&#65288;CXR&#65289;&#22270;&#20687;&#30340;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#22240;&#20026;&#36825;&#26159;&#19968;&#20010;&#22797;&#26434;&#20449;&#24687;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#20043;&#38388;&#32763;&#35793;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building on the recent remarkable development of large language models (LLMs), active attempts are being made to extend the utility of LLMs to multimodal tasks. There have been previous efforts to link language and visual information, and attempts to add visual capabilities to LLMs are ongoing as well. However, existing attempts use LLMs only as image decoders and no attempt has been made to generate images in the same line as the natural language. By adopting a VQ-GAN framework in which latent representations of images are treated as a kind of text tokens, we present a novel method to fine-tune a pre-trained LLM to read and generate images like text without any structural changes, extra training objectives, or the need for training an ad-hoc network while still preserving the of the instruction-following capability of the LLM. We apply this framework to chest X-ray (CXR) image and report generation tasks as it is a domain in which translation of complex information between visual and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#36127;&#37319;&#26679;&#20998;&#24067;&#23545;&#23545;&#27604;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#32771;&#34385;&#30828;&#24230;&#21644;&#32467;&#26500;&#30340;&#23545;&#27604;&#65288;HaSa&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#21435;&#38500;&#20551;&#36127;&#26679;&#26412;&#65292;&#25552;&#39640;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10563</link><description>&lt;p&gt;
&#25506;&#31350;&#30828;&#36127;&#37319;&#26679;&#20998;&#24067;&#23545;&#23545;&#27604;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Investigating the Effect of Hard Negative Sample Distribution on Contrastive Knowledge Graph Embedding. (arXiv:2305.10563v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#36127;&#37319;&#26679;&#20998;&#24067;&#23545;&#23545;&#27604;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#32771;&#34385;&#30828;&#24230;&#21644;&#32467;&#26500;&#30340;&#23545;&#27604;&#65288;HaSa&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#21435;&#38500;&#20551;&#36127;&#26679;&#26412;&#65292;&#25552;&#39640;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20219;&#21153;&#30340;&#25104;&#21151;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65288;KGEs&#65289;&#30340;&#36136;&#37327;&#65292;&#23427;&#20381;&#36182;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#29992;&#36127;&#19977;&#20803;&#32452;&#22686;&#24378;&#25968;&#25454;&#38598;&#12290;&#22312;&#36127;&#37319;&#26679;&#30340;&#23545;&#27604;&#25439;&#22833;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#39640;&#36136;&#37327;&#65288;&#21363;&#30828;&#65289;&#36127;&#37319;&#26679;&#30340;&#21551;&#21457;&#24335;&#29983;&#25104;&#20043;&#38388;&#23384;&#22312;&#24046;&#36317;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20462;&#25913;&#20102;InfoNCE&#25439;&#22833;&#65292;&#26174;&#24335;&#32771;&#34385;&#20102;&#36127;&#37319;&#26679;&#20998;&#24067;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#29992;&#30828;&#36127;&#26679;&#26412;&#26368;&#23567;&#21270;InfoNCE&#25439;&#22833;&#21487;&#20197;&#26368;&#22823;&#21270;&#32473;&#23450;&#19977;&#20803;&#32452;&#21644;&#36127;&#19977;&#20803;&#32452;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#20063;&#35777;&#26126;&#30828;&#36127;&#26679;&#26412;&#20250;&#23548;&#33268;&#20551;&#36127;&#26679;&#26412;&#65288;&#21363;&#38169;&#35823;&#30340;&#20107;&#23454;&#19977;&#20803;&#32452;&#65289;&#24182;&#38477;&#20302;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#30340;&#22270;&#32467;&#26500;&#21435;&#38500;&#20551;&#36127;&#19977;&#20803;&#32452;&#30340;&#26032;&#22411;&#36127;&#37319;&#26679;&#20998;&#24067;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#31639;&#27861;&#31216;&#20026;&#32771;&#34385;&#30828;&#24230;&#21644;&#32467;&#26500;&#30340;&#23545;&#27604;&#65288;HaSa&#65289;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of the knowledge graph completion task heavily depends on the quality of the knowledge graph embeddings (KGEs), which relies on self-supervised learning and augmenting the dataset with negative triples. There is a gap in literature between the theoretical analysis of negative samples on contrastive loss and heuristic generation of quality (i.e., hard) negative triples. In this paper, we modify the InfoNCE loss to explicitly account for the negative sample distribution. We show minimizing InfoNCE loss with hard negatives maximizes the KL-divergence between the given and negative triple embedding. However, we also show that hard negatives can lead to false negatives (i.e., accidentally factual triples) and reduce downstream task performance. To address this issue, we propose a novel negative sample distribution that uses the graph structure of the knowledge graph to remove the false negative triples. We call our algorithm Hardness and Structure-aware (\textbf{HaSa}) contrasti
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20351;&#29992;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23545;&#31216;&#21152;&#23494;&#30340;&#31639;&#27861;&#65288;SELM&#65289;&#65292;&#20854;&#20013;&#31639;&#27861;&#21487;&#20197;&#23558;&#20219;&#24847;&#25968;&#25454;&#32534;&#30721;&#20026;&#32039;&#20945;&#30340;&#23454;&#20540;&#21521;&#37327;&#65288;&#21363;&#21152;&#23494;&#65289;&#65292;&#28982;&#21518;&#36890;&#36807;&#38543;&#26426;&#23376;&#31354;&#38388;&#20248;&#21270;&#21644;&#36138;&#24515;&#35299;&#30721;&#23558;&#21521;&#37327;&#26080;&#25439;&#35299;&#30721;&#20026;&#21407;&#22987;&#28040;&#24687;&#65288;&#21363;&#35299;&#23494;&#65289;&#65292;&#24182;&#19988;SELM&#22312;&#21152;&#23494;&#20998;&#26512;&#26041;&#38754;&#30340;&#23433;&#20840;&#24615;&#33021;&#36739;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.10445</link><description>&lt;p&gt;
&#35760;&#24518;&#26377;&#30410;&#65306;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#30340;&#21152;&#23494;
&lt;/p&gt;
&lt;p&gt;
Memorization for Good: Encryption with Autoregressive Language Models. (arXiv:2305.10445v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10445
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20351;&#29992;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23545;&#31216;&#21152;&#23494;&#30340;&#31639;&#27861;&#65288;SELM&#65289;&#65292;&#20854;&#20013;&#31639;&#27861;&#21487;&#20197;&#23558;&#20219;&#24847;&#25968;&#25454;&#32534;&#30721;&#20026;&#32039;&#20945;&#30340;&#23454;&#20540;&#21521;&#37327;&#65288;&#21363;&#21152;&#23494;&#65289;&#65292;&#28982;&#21518;&#36890;&#36807;&#38543;&#26426;&#23376;&#31354;&#38388;&#20248;&#21270;&#21644;&#36138;&#24515;&#35299;&#30721;&#23558;&#21521;&#37327;&#26080;&#25439;&#35299;&#30721;&#20026;&#21407;&#22987;&#28040;&#24687;&#65288;&#21363;&#35299;&#23494;&#65289;&#65292;&#24182;&#19988;SELM&#22312;&#21152;&#23494;&#20998;&#26512;&#26041;&#38754;&#30340;&#23433;&#20840;&#24615;&#33021;&#36739;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#21487;&#20197;&#35760;&#24518;&#21644;&#32972;&#35829;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#12290;&#34429;&#28982;&#36825;&#31181;&#35760;&#24518;&#36890;&#24120;&#34987;&#35748;&#20026;&#20855;&#26377;&#19981;&#33391;&#23646;&#24615;&#65292;&#20363;&#22914;&#36807;&#24230;&#25311;&#21512;&#21644;&#20449;&#24687;&#27844;&#28431;&#65292;&#20294;&#25105;&#20204;&#30340;&#24037;&#20316;&#23558;&#35760;&#24518;&#35270;&#20026;LM&#30340;&#19968;&#31181;&#26410;&#24320;&#21457;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20351;&#29992;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23545;&#31216;&#21152;&#23494;&#30340;&#31639;&#27861;&#65288;SELM&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#33258;&#22238;&#24402;LM&#21487;&#20197;&#23558;&#20219;&#24847;&#25968;&#25454;&#32534;&#30721;&#20026;&#32039;&#20945;&#30340;&#23454;&#20540;&#21521;&#37327;&#65288;&#21363;&#21152;&#23494;&#65289;&#65292;&#28982;&#21518;&#36890;&#36807;&#38543;&#26426;&#23376;&#31354;&#38388;&#20248;&#21270;&#21644;&#36138;&#24515;&#35299;&#30721;&#23558;&#21521;&#37327;&#26080;&#25439;&#35299;&#30721;&#20026;&#21407;&#22987;&#28040;&#24687;&#65288;&#21363;&#35299;&#23494;&#65289;&#12290;&#34429;&#28982;SELM&#19981;&#26131;&#21463;&#20256;&#32479;&#21152;&#23494;&#20998;&#26512;&#26041;&#27861;&#25915;&#30772;&#65292;&#20294;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#23454;&#35777;&#21464;&#20307;&#65292;&#30740;&#31350;&#23427;&#30340;&#23433;&#20840;&#24615;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#21487;&#22312;https://github.com/OSU-NLP-Group/SELM &#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over-parameterized neural language models (LMs) can memorize and recite long sequences of training data. While such memorization is normally associated with undesired properties such as overfitting and information leaking, our work casts memorization as an unexplored capability of LMs. We propose the first symmetric encryption algorithm with autoregressive language models (SELM). We show that autoregressive LMs can encode arbitrary data into a compact real-valued vector (i.e., encryption) and then losslessly decode the vector to the original message (i.e., decryption) via random subspace optimization and greedy decoding. While SELM is not amenable to conventional cryptanalysis, we investigate its security through a novel empirical variant of the classic IND-CPA (indistinguishability under chosen-plaintext attack) game. Our code and datasets are available at https://github.com/OSU-NLP-Group/SELM.
&lt;/p&gt;</description></item><item><title>SuperDialseg&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#30417;&#30563;&#23545;&#35805;&#20998;&#21106;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#25991;&#26723;&#30340;&#23545;&#35805;&#20197;&#21450;&#20934;&#30830;&#23450;&#20041;&#23545;&#35805;&#20998;&#21106;&#28857;&#26469;&#35299;&#20915;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#38480;&#21046;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#30417;&#30563;&#23398;&#20064;&#22312;&#23545;&#35805;&#20998;&#21106;&#20219;&#21153;&#20013;&#34920;&#29616;&#26497;&#20854;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2305.08371</link><description>&lt;p&gt;
SuperDialseg&#65306;&#19968;&#20010;&#29992;&#20110;&#30417;&#30563;&#23545;&#35805;&#20998;&#21106;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
SuperDialseg: A Large-scale Dataset for Supervised Dialogue Segmentation. (arXiv:2305.08371v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08371
&lt;/p&gt;
&lt;p&gt;
SuperDialseg&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#30417;&#30563;&#23545;&#35805;&#20998;&#21106;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#25991;&#26723;&#30340;&#23545;&#35805;&#20197;&#21450;&#20934;&#30830;&#23450;&#20041;&#23545;&#35805;&#20998;&#21106;&#28857;&#26469;&#35299;&#20915;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#38480;&#21046;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#30417;&#30563;&#23398;&#20064;&#22312;&#23545;&#35805;&#20998;&#21106;&#20219;&#21153;&#20013;&#34920;&#29616;&#26497;&#20854;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#20998;&#21106;&#26159;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#23545;&#35805;&#25991;&#26412;&#12290;&#23613;&#31649;&#26080;&#30417;&#30563;&#23545;&#35805;&#20998;&#21106;&#26041;&#27861;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#35757;&#32451;&#20013;&#32570;&#20047;&#26126;&#30830;&#30340;&#30417;&#30563;&#20449;&#21495;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#21463;&#21040;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#23545;&#35805;&#20998;&#21106;&#28857;&#30340;&#31934;&#30830;&#23450;&#20041;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22686;&#21152;&#20102;&#25163;&#21160;&#27880;&#37322;&#30340;&#38590;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#25991;&#26723;&#30340;&#23545;&#35805;&#26469;&#25552;&#20379;&#23545;&#35805;&#20998;&#21106;&#28857;&#30340;&#21487;&#34892;&#23450;&#20041;&#65292;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#21517;&#20026;SuperDialseg&#30340;&#22823;&#35268;&#27169;&#30417;&#30563;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;9478&#20010;&#23545;&#35805;&#65292;&#22522;&#20110;&#20004;&#20010;&#27969;&#34892;&#30340;&#22522;&#20110;&#25991;&#26723;&#30340;&#23545;&#35805;&#35821;&#26009;&#24211;&#65292;&#24182;&#32487;&#25215;&#20102;&#23427;&#20204;&#30340;&#26377;&#29992;&#30340;&#23545;&#35805;&#30456;&#20851;&#27880;&#37322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#21547;&#26377;18&#20010;&#27169;&#22411;&#30340;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#20116;&#20010;&#31867;&#21035;&#30340;&#23545;&#35805;&#20998;&#21106;&#20219;&#21153;&#65292;&#24182;&#38468;&#24102;&#20960;&#31181;&#36866;&#24403;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#30417;&#30563;&#23398;&#20064;&#22312;&#23545;&#35805;&#20998;&#21106;&#20219;&#21153;&#20013;&#34920;&#29616;&#26497;&#20854;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialogue segmentation is a crucial task for dialogue systems allowing a better understanding of conversational texts. Despite recent progress in unsupervised dialogue segmentation methods, their performances are limited by the lack of explicit supervised signals for training. Furthermore, the precise definition of segmentation points in conversations still remains as a challenging problem, increasing the difficulty of collecting manual annotations. In this paper, we provide a feasible definition of dialogue segmentation points with the help of document-grounded dialogues and release a large-scale supervised dataset called SuperDialseg, containing 9,478 dialogues based on two prevalent document-grounded dialogue corpora, and also inherit their useful dialogue-related annotations. Moreover, we provide a benchmark including 18 models across five categories for the dialogue segmentation task with several proper evaluation metrics. Empirical studies show that supervised learning is extremel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#35821;&#31181;&#25968;&#25454;&#38598;MultiClaim&#65292;&#29992;&#20110;&#20808;&#21069;&#20107;&#23454;&#26680;&#26597;&#30340;&#26816;&#32034;&#65292;&#36890;&#36807;&#19981;&#21516;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#21450;&#19968;&#20010;&#32463;&#36807;&#30417;&#30563;&#24494;&#35843;&#30340;&#26041;&#27861;&#23545;&#20854;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2305.07991</link><description>&lt;p&gt;
&#22810;&#35821;&#31181;&#20808;&#21069;&#20107;&#23454;&#26680;&#26597;&#32034;&#24341;
&lt;/p&gt;
&lt;p&gt;
Multilingual Previously Fact-Checked Claim Retrieval. (arXiv:2305.07991v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#35821;&#31181;&#25968;&#25454;&#38598;MultiClaim&#65292;&#29992;&#20110;&#20808;&#21069;&#20107;&#23454;&#26680;&#26597;&#30340;&#26816;&#32034;&#65292;&#36890;&#36807;&#19981;&#21516;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#21450;&#19968;&#20010;&#32463;&#36807;&#30417;&#30563;&#24494;&#35843;&#30340;&#26041;&#27861;&#23545;&#20854;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#23454;&#26680;&#26597;&#21592;&#24120;&#24120;&#21463;&#21040;&#38656;&#35201;&#36827;&#34892;&#26680;&#26597;&#30340;&#22312;&#32447;&#20869;&#23481;&#25968;&#37327;&#30340;&#38480;&#21046;&#12290;NLP&#21487;&#20197;&#36890;&#36807;&#26816;&#32034;&#19982;&#27491;&#22312;&#35843;&#26597;&#30340;&#20869;&#23481;&#30456;&#20851;&#30340;&#24050;&#23384;&#22312;&#30340;&#20107;&#23454;&#26680;&#26597;&#26469;&#24110;&#21161;&#20182;&#20204;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#35821;&#31181;&#25968;&#25454;&#38598;MultiClaim&#8212;&#8212;&#29992;&#20110;&#20808;&#21069;&#20107;&#23454;&#26680;&#26597;&#26816;&#32034;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#26469;&#33258;&#31038;&#20132;&#23186;&#20307;&#30340;27&#31181;&#35821;&#35328;&#30340;28k&#31687;&#25991;&#31456;&#12289;39&#31181;&#35821;&#35328;&#30340;206k&#31687;&#30001;&#19987;&#19994;&#20107;&#23454;&#26680;&#26597;&#21592;&#25776;&#20889;&#30340;&#20107;&#23454;&#26680;&#26597;&#20197;&#21450;&#36825;&#20004;&#20010;&#32452;&#20043;&#38388;&#30340;31k&#20010;&#36830;&#25509;&#12290;&#36825;&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#24191;&#27867;&#12289;&#35821;&#35328;&#22810;&#20803;&#21270;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#19981;&#21516;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#22312;&#36825;&#20010;&#25968;&#25454;&#38598;&#21450;&#20854;&#21508;&#33258;&#30340;&#32500;&#24230;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35780;&#20272;&#36825;&#26679;&#19968;&#20010;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#19988;&#22312;&#35299;&#37322;&#32467;&#26524;&#20043;&#21069;&#38656;&#35201;&#37319;&#21462;&#36866;&#24403;&#30340;&#25514;&#26045;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#19968;&#31181;&#32463;&#36807;&#30417;&#30563;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#22312;&#36825;&#20010;&#26041;&#27861;&#19978;&#24471;&#21040;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fact-checkers are often hampered by the sheer amount of online content that needs to be fact-checked. NLP can help them by retrieving already existing fact-checks relevant to the content being investigated. This paper introduces a new multilingual dataset -- MultiClaim -- for previously fact-checked claim retrieval. We collected 28k posts in 27 languages from social media, 206k fact-checks in 39 languages written by professional fact-checkers, as well as 31k connections between these two groups. This is the most extensive and the most linguistically diverse dataset of this kind to date. We evaluated how different unsupervised methods fare on this dataset and its various dimensions. We show that evaluating such a diverse dataset has its complexities and proper care needs to be taken before interpreting the results. We also evaluated a supervised fine-tuning approach, improving upon the unsupervised method significantly.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#36924;&#36817;&#29983;&#25104;&#31163;&#25955;&#28508;&#21464;&#37327;&#30340;&#21442;&#25968;&#30340;&#26799;&#24230;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#19968;&#20123;&#25968;&#20540;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20108;&#38454;&#31934;&#24230;&#65292;&#21462;&#24471;&#20102;&#23454;&#39564;&#19978;&#30340;&#25345;&#32493;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2304.08612</link><description>&lt;p&gt;
&#31163;&#25955;&#19982;&#21453;&#21521;&#20256;&#25773;&#30340;&#26725;&#26753;&#65306;&#30452;&#36890;&#27861;&#19982;&#20854;&#23427;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Bridging Discrete and Backpropagation: Straight-Through and Beyond. (arXiv:2304.08612v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08612
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#36924;&#36817;&#29983;&#25104;&#31163;&#25955;&#28508;&#21464;&#37327;&#30340;&#21442;&#25968;&#30340;&#26799;&#24230;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#19968;&#20123;&#25968;&#20540;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20108;&#38454;&#31934;&#24230;&#65292;&#21462;&#24471;&#20102;&#23454;&#39564;&#19978;&#30340;&#25345;&#32493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21521;&#20256;&#25773;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#22522;&#30707;&#65292;&#20294;&#20854;&#20165;&#38480;&#20110;&#35745;&#31639;&#36830;&#32493;&#21464;&#37327;&#30340;&#26799;&#24230;&#65292;&#38480;&#21046;&#20102;&#28041;&#21450;&#31163;&#25955;&#28508;&#21464;&#37327;&#30340;&#38382;&#39064;&#30340;&#30740;&#31350;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#36817;&#20284;&#29983;&#25104;&#31163;&#25955;&#28508;&#21464;&#37327;&#30340;&#21442;&#25968;&#30340;&#26799;&#24230;&#12290;&#25105;&#20204;&#39318;&#20808;&#32771;&#23519;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340; Straight-Through&#65288;ST&#65289;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#23427;&#20316;&#20026;&#26799;&#24230;&#30340;&#19968;&#38454;&#36817;&#20284;&#20540;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026; ReinMax&#65292;&#23427;&#38598;&#25104;&#20102; Heun's Method&#65292;&#19968;&#31181;&#35299;ODE&#30340;&#20108;&#38454;&#25968;&#20540;&#26041;&#27861;&#65292;&#20197;&#36817;&#20284;&#26799;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#20108;&#38454;&#31934;&#24230;&#65292;&#32780;&#19981;&#38656;&#35201; Hessian &#25110;&#20854;&#20182;&#20108;&#38454;&#23548;&#25968;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#32467;&#26500;&#21270;&#36755;&#20986;&#39044;&#27979;&#21644;&#26080;&#30417;&#30563;&#29983;&#25104;&#24314;&#27169;&#20219;&#21153;&#30340;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;\ours &#22312;&#29616;&#26377;&#25216;&#26415;&#20013;&#24102;&#26469;&#20102;&#25345;&#32493;&#30340;&#25913;&#36827;&#65292;&#21253;&#25324; ST &#21644; Straight-Through Gum&#12290;
&lt;/p&gt;
&lt;p&gt;
Backpropagation, the cornerstone of deep learning, is limited to computing gradients solely for continuous variables. This limitation hinders various research on problems involving discrete latent variables. To address this issue, we propose a novel approach for approximating the gradient of parameters involved in generating discrete latent variables. First, we examine the widely used Straight-Through (ST) heuristic and demonstrate that it works as a first-order approximation of the gradient. Guided by our findings, we propose a novel method called ReinMax, which integrates Heun's Method, a second-order numerical method for solving ODEs, to approximate the gradient. Our method achieves second-order accuracy without requiring Hessian or other second-order derivatives. We conduct experiments on structured output prediction and unsupervised generative modeling tasks. Our results show that \ours brings consistent improvements over the state of the art, including ST and Straight-Through Gum
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21033;&#29992;&#23545;&#27604;&#20256;&#36882;&#26694;&#26550;&#21644;&#20256;&#25773;&#32467;&#26500;&#65292;&#23558;&#20174;&#20805;&#36275;&#36164;&#28304;&#30340;&#35875;&#35328;&#25968;&#25454;&#23398;&#21040;&#30340;&#29305;&#24449;&#36866;&#24212;&#20110;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#26816;&#27979;&#21040;&#36328;&#36234;&#35821;&#35328;&#21644;&#39046;&#22495;&#30028;&#38480;&#30340;&#35875;&#35328;&#12290;</title><link>http://arxiv.org/abs/2304.01492</link><description>&lt;p&gt;
&#32479;&#19968;&#23545;&#27604;&#20256;&#36882;&#26694;&#26550;&#19982;&#20256;&#25773;&#32467;&#26500;&#29992;&#20110;&#25552;&#39640;&#20302;&#36164;&#28304;&#35875;&#35328;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
A Unified Contrastive Transfer Framework with Propagation Structure for Boosting Low-Resource Rumor Detection. (arXiv:2304.01492v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01492
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21033;&#29992;&#23545;&#27604;&#20256;&#36882;&#26694;&#26550;&#21644;&#20256;&#25773;&#32467;&#26500;&#65292;&#23558;&#20174;&#20805;&#36275;&#36164;&#28304;&#30340;&#35875;&#35328;&#25968;&#25454;&#23398;&#21040;&#30340;&#29305;&#24449;&#36866;&#24212;&#20110;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#26816;&#27979;&#21040;&#36328;&#36234;&#35821;&#35328;&#21644;&#39046;&#22495;&#30028;&#38480;&#30340;&#35875;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37327;&#30340;&#35875;&#35328;&#20276;&#38543;&#30528;&#31361;&#21457;&#26032;&#38395;&#25110;&#28909;&#38376;&#35805;&#39064;&#32780;&#20256;&#25773;&#65292;&#36825;&#20005;&#37325;&#38459;&#30861;&#20102;&#30495;&#30456;&#30340;&#20256;&#25773;&#12290;&#29616;&#26377;&#30340;&#35875;&#35328;&#26816;&#27979;&#31639;&#27861;&#23637;&#31034;&#20102;&#22312;&#21069;&#20960;&#22825;&#26032;&#38395;&#19978;&#33391;&#22909;&#24615;&#33021;&#30340;&#21069;&#26223;&#65292;&#20294;&#26159;&#30001;&#20110;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#21644;&#20808;&#21069;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#23427;&#20204;&#24456;&#38590;&#21457;&#29616;&#19982;&#39044;&#26399;&#20107;&#20214;&#26377;&#20851;&#30340;&#35875;&#35328;&#65292;&#29305;&#21035;&#26159;&#22312;&#19981;&#21516;&#35821;&#35328;&#65288;&#21363;&#20302;&#36164;&#28304;&#29615;&#22659;&#65289;&#20013;&#20256;&#25773;&#30340;&#35875;&#35328;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#23545;&#27604;&#20256;&#36882;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20174;&#20805;&#36275;&#36164;&#28304;&#30340;&#35875;&#35328;&#25968;&#25454;&#23398;&#21040;&#30340;&#29305;&#24449;&#36866;&#24212;&#20110;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#30340;&#29305;&#24449;&#26469;&#26816;&#27979;&#35875;&#35328;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#20256;&#25773;&#30340;&#35875;&#35328;&#34920;&#31034;&#20026;&#26080;&#21521;&#25299;&#25169;&#32467;&#26500;&#65292;&#28982;&#21518;&#36890;&#36807;&#32479;&#19968;&#23545;&#27604;&#33539;&#24335;&#36827;&#34892;Multi-scale&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#26126;&#30830;&#22320;&#31361;&#30772;&#20102;&#39046;&#22495;&#21644;/&#25110;&#35821;&#35328;&#38382;&#39064;&#30340;&#38556;&#30861;&#65292;&#36890;&#36807;&#35821;&#35328;&#23545;&#40784;&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
The truth is significantly hampered by massive rumors that spread along with breaking news or popular topics. Since there is sufficient corpus gathered from the same domain for model training, existing rumor detection algorithms show promising performance on yesterday's news. However, due to a lack of training data and prior expert knowledge, they are poor at spotting rumors concerning unforeseen events, especially those propagated in different languages (i.e., low-resource regimes). In this paper, we propose a unified contrastive transfer framework to detect rumors by adapting the features learned from well-resourced rumor data to that of the low-resourced. More specifically, we first represent rumor circulated on social media as an undirected topology, and then train a Multi-scale Graph Convolutional Network via a unified contrastive paradigm. Our model explicitly breaks the barriers of the domain and/or language issues, via language alignment and a novel domain-adaptive contrastive 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#35757;&#32451;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#27169;&#22411;&#30340;&#21453;&#21521;&#26041;&#27861;&#65292;&#21033;&#29992;&#30456;&#23545;&#20934;&#30830;&#24615;&#30340;&#31574;&#30053;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;Polytuplet Loss&#20989;&#25968;&#26469;&#30830;&#20445;&#20248;&#20808;&#23398;&#20064;&#31572;&#26696;&#36873;&#25321;&#30340;&#30456;&#23545;&#27491;&#30830;&#24615;&#65292;&#33719;&#24471;&#20102;&#19981;&#38169;&#30340;&#25104;&#26524;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#19968;&#33324;&#24615;&#30340;&#35757;&#32451;&#26041;&#27861;&#21644;&#27169;&#22411;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2304.01046</link><description>&lt;p&gt;
&#8220;Polytuplet Loss: &#35757;&#32451;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#27169;&#22411;&#30340;&#21453;&#21521;&#26041;&#27861;&#8221;
&lt;/p&gt;
&lt;p&gt;
Polytuplet Loss: A Reverse Approach to Training Reading Comprehension and Logical Reasoning Models. (arXiv:2304.01046v1 [cs.CL] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#35757;&#32451;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#27169;&#22411;&#30340;&#21453;&#21521;&#26041;&#27861;&#65292;&#21033;&#29992;&#30456;&#23545;&#20934;&#30830;&#24615;&#30340;&#31574;&#30053;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;Polytuplet Loss&#20989;&#25968;&#26469;&#30830;&#20445;&#20248;&#20808;&#23398;&#20064;&#31572;&#26696;&#36873;&#25321;&#30340;&#30456;&#23545;&#27491;&#30830;&#24615;&#65292;&#33719;&#24471;&#20102;&#19981;&#38169;&#30340;&#25104;&#26524;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#19968;&#33324;&#24615;&#30340;&#35757;&#32451;&#26041;&#27861;&#21644;&#27169;&#22411;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25972;&#20010;&#23398;&#26657;&#25945;&#32946;&#36807;&#31243;&#20013;&#65292;&#23398;&#29983;&#20204;&#23558;&#21463;&#21040;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#30340;&#32771;&#39564;&#12290;&#23398;&#29983;&#20204;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#31574;&#30053;&#26469;&#23436;&#25104;&#27492;&#31867;&#32771;&#35797;&#65292;&#20854;&#20013;&#26377;&#20123;&#34987;&#35748;&#20026;&#26159;&#36890;&#24120;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#31574;&#30053;&#30340;&#12290;&#36825;&#26679;&#19968;&#31181;&#31574;&#30053;&#28041;&#21450;&#24378;&#35843;&#30456;&#23545;&#20934;&#30830;&#24615;&#32780;&#38750;&#32477;&#23545;&#20934;&#30830;&#24615;&#65292;&#29702;&#35770;&#19978;&#21487;&#20197;&#22312;&#19981;&#23436;&#20840;&#25484;&#25569;&#35299;&#39064;&#25152;&#38656;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#24471;&#20986;&#27491;&#30830;&#31572;&#26696;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#24212;&#29992;&#36825;&#31181;&#31574;&#30053;&#26469;&#35757;&#32451;&#36801;&#31227;&#23398;&#20064;&#27169;&#22411;&#20197;&#35299;&#20915;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#38382;&#39064;&#30340;&#26377;&#25928;&#24615;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#22522;&#20934;&#25968;&#25454;&#38598;ReClor&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23613;&#31649;&#20197;&#21069;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#36923;&#36753;&#25512;&#29702;&#25216;&#33021;&#65292;&#20294;&#25105;&#20204;&#19987;&#27880;&#20110;&#19968;&#31181;&#36890;&#29992;&#30340;&#35757;&#32451;&#26041;&#27861;&#21644;&#27169;&#22411;&#26550;&#26500;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Polytuplet Loss&#20989;&#25968;&#65292;&#26159;&#19977;&#20803;&#32452;&#25439;&#22833;&#20989;&#25968;&#30340;&#25193;&#23637;&#65292;&#20197;&#30830;&#20445;&#20248;&#20808;&#23398;&#20064;&#31572;&#26696;&#36873;&#25321;&#30340;&#30456;&#23545;&#27491;&#30830;&#24615;&#32780;&#38750;&#23398;&#20064;&#32477;&#23545;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Throughout schooling, students are tested on reading comprehension and logical reasoning. Students have developed various strategies for completing such exams, some of which are generally thought to outperform others. One such strategy involves emphasizing relative accuracy over absolute accuracy and can theoretically produce the correct answer without full knowledge of the information required to solve the question. This paper examines the effectiveness of applying such a strategy to train transfer learning models to solve reading comprehension and logical reasoning questions. The models were evaluated on the ReClor dataset, a challenging reading comprehension and logical reasoning benchmark. While previous studies targeted logical reasoning skills, we focus on a general training method and model architecture. We propose the polytuplet loss function, an extension of the triplet loss function, to ensure prioritization of learning the relative correctness of answer choices over learning
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20219;&#21153;&#21644;&#39046;&#22495;&#29305;&#23450;&#25552;&#31034;&#26469;&#20248;&#21270;ChatGPT&#22312;&#22797;&#26434;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#30740;&#31350;&#21457;&#29616;&#28201;&#24230;&#35774;&#32622;&#21644;&#20219;&#21153;&#20449;&#24687;&#23545;ChatGPT&#34920;&#29616;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2303.13780</link><description>&lt;p&gt;
&#20248;&#21270;ChatGPT&#22312;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Towards Making the Most of ChatGPT for Machine Translation. (arXiv:2303.13780v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20219;&#21153;&#21644;&#39046;&#22495;&#29305;&#23450;&#25552;&#31034;&#26469;&#20248;&#21270;ChatGPT&#22312;&#22797;&#26434;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#30740;&#31350;&#21457;&#29616;&#28201;&#24230;&#35774;&#32622;&#21644;&#20219;&#21153;&#20449;&#24687;&#23545;ChatGPT&#34920;&#29616;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#22312;&#26426;&#22120;&#32763;&#35793;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23427;&#22312;&#39640;&#36164;&#28304;&#35821;&#35328;&#26041;&#38754;&#21487;&#20197;&#36798;&#21040;&#21830;&#19994;&#31995;&#32479;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#20294;&#22312;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#65288;&#20363;&#22914;&#20302;&#36164;&#28304;&#21644;&#36828;&#31243;&#35821;&#35328;&#23545;&#32763;&#35793;&#65289;&#33853;&#21518;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#37319;&#29992;&#31616;&#21333;&#30340;&#25552;&#31034;&#65292;&#26080;&#27861;&#20805;&#20998;&#21457;&#25381;ChatGPT&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#37325;&#26032;&#23457;&#35270;&#28201;&#24230;&#12289;&#20219;&#21153;&#20449;&#24687;&#21644;&#39046;&#22495;&#20449;&#24687;&#31561;&#20960;&#20010;&#26041;&#38754;&#65292;&#36827;&#19968;&#27493;&#25366;&#25496;ChatGPT&#30340;&#32763;&#35793;&#33021;&#21147;&#65292;&#24182;&#30456;&#24212;&#22320;&#25552;&#20986;&#20004;&#31181;&#65288;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#65289;&#25552;&#31034;&#65306;&#20219;&#21153;&#29305;&#23450;&#25552;&#31034;&#65288;TSP&#65289;&#21644;&#39046;&#22495;&#29305;&#23450;&#25552;&#31034;&#65288;DSP&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT shows remarkable capabilities for machine translation (MT). Several prior studies have shown that it achieves comparable results to commercial systems for high-resource languages, but lags behind in complex tasks, e.g, low-resource and distant-language-pairs translation. However, they usually adopt simple prompts which can not fully elicit the capability of ChatGPT. In this report, we aim to further mine ChatGPT's translation ability by revisiting several aspects: temperature, task information, and domain information, and correspondingly propose two (simple but effective) prompts: Task-Specific Prompts (TSP) and Domain-Specific Prompts (DSP). We show that: 1) The performance of ChatGPT depends largely on temperature, and a lower temperature usually can achieve better performance; 2) Emphasizing the task information further improves ChatGPT's performance, particularly in complex MT tasks; 3) Introducing domain information can elicit ChatGPT's generalization ability and improve i
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;KD-NAS&#26041;&#27861;&#65292;&#20351;&#29992;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#25351;&#23548;&#30693;&#35782;&#33976;&#39311;&#36807;&#31243;&#65292;&#24182;&#25214;&#21040;&#26368;&#20248;&#30340;&#23398;&#29983;&#27169;&#22411;&#65292;&#20174;&#32780;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#19979;&#23454;&#29616;&#39640;&#25928;&#24072;&#29983;&#30693;&#35782;&#36716;&#31227;&#65292;&#36229;&#36807;&#25163;&#24037;&#35774;&#35745;&#30340;&#23398;&#29983;&#27169;&#22411;&#21644;&#22823;&#22411;&#25945;&#24072;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.09639</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20013;&#39640;&#25928;&#24072;&#29983;&#30693;&#35782;&#36716;&#31227;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Neural Architecture Search for Effective Teacher-Student Knowledge Transfer in Language Models. (arXiv:2303.09639v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;KD-NAS&#26041;&#27861;&#65292;&#20351;&#29992;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#25351;&#23548;&#30693;&#35782;&#33976;&#39311;&#36807;&#31243;&#65292;&#24182;&#25214;&#21040;&#26368;&#20248;&#30340;&#23398;&#29983;&#27169;&#22411;&#65292;&#20174;&#32780;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#19979;&#23454;&#29616;&#39640;&#25928;&#24072;&#29983;&#30693;&#35782;&#36716;&#31227;&#65292;&#36229;&#36807;&#25163;&#24037;&#35774;&#35745;&#30340;&#23398;&#29983;&#27169;&#22411;&#21644;&#22823;&#22411;&#25945;&#24072;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25104;&#26524;&#12290;&#23567;&#22411;&#23398;&#29983;&#25104;&#20026;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#37096;&#32626;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24050;&#32463;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#36873;&#20986;&#30340;&#23398;&#29983;&#27169;&#22411;&#20250;&#23548;&#33268;&#30693;&#35782;&#36716;&#31227;&#30340;&#20302;&#25928;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#25351;&#23548;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#36807;&#31243;&#20174;&#32780;&#25214;&#21040;&#26368;&#20248;&#23398;&#29983;&#27169;&#22411;&#30340;KD-NAS&#26041;&#27861;&#12290;&#22312;&#25628;&#32034;&#36807;&#31243;&#30340;&#27599;&#20010;episode&#20013;&#65292;NAS&#25511;&#21046;&#22120;&#26681;&#25454;&#19979;&#28216;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#21644;&#25512;&#29702;&#24310;&#36831;&#30340;&#32452;&#21512;&#39044;&#27979;&#22870;&#21169;&#12290;&#28982;&#21518;&#65292;&#23545;&#25490;&#21517;&#38752;&#21069;&#30340;&#20505;&#36873;&#26550;&#26500;&#36827;&#34892;&#33976;&#39311;&#22788;&#29702;&#12290;&#26368;&#21518;&#36873;&#25321;&#26368;&#39640;&#22870;&#21169;&#30340;&#26550;&#26500;&#24182;&#22266;&#23450;&#36827;&#34892;&#30693;&#35782;&#33976;&#39311;&#65292;&#21046;&#20316;&#20986;&#23398;&#29983;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;KD-NAS&#33021;&#22815;&#25214;&#21040;&#26377;&#25928;&#30340;&#23398;&#29983;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#36164;&#28304;&#21463;&#38480;&#22330;&#26223;&#19979;&#34920;&#29616;&#36229;&#36234;&#25152;&#26377;&#25163;&#24037;&#35774;&#35745;&#30340;&#23398;&#29983;&#27169;&#22411;&#65292;&#29978;&#33267;&#36229;&#36234;&#22823;&#22411;&#25945;&#24072;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pre-trained language models have achieved state-of-the-art results on a variety of downstream tasks. Knowledge Distillation (KD) of a smaller student model addresses their inefficiency, allowing for deployment in resource-constraint environments. KD however remains ineffective, as the student is manually selected from a set of existing options already pre-trained on large corpora, a sub-optimal choice within the space of all possible student architectures. This paper proposes KD-NAS, the use of Neural Architecture Search (NAS) guided by the Knowledge Distillation process to find the optimal student model for distillation from a teacher, for a given natural language task. In each episode of the search process, a NAS controller predicts a reward based on a combination of accuracy on the downstream task and latency of inference. The top candidate architectures are then distilled from the teacher on a small proxy set. Finally the architecture(s) with the highest reward is selected, a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23450;&#20041;&#20102;&#19968;&#31181;MTC&#20998;&#31867;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;CFG&#27169;&#22411;&#30340;&#25277;&#21462;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;ICL&#33258;&#21160;&#25552;&#21462;&#21644;&#26631;&#20934;&#21270;DUGs&#20013;&#30340;MTC&#65292;&#26377;&#26395;&#36890;&#36807;&#23450;&#20041;&#23433;&#20840;&#30340;&#24739;&#32773;&#27963;&#21160;&#27169;&#24335;&#26469;&#25512;&#36827;&#20197;&#24739;&#32773;&#20026;&#20013;&#24515;&#30340;&#21307;&#30103;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.09366</link><description>&lt;p&gt;
&#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#21307;&#23398;&#26102;&#38388;&#32422;&#26463;&#25277;&#21462;&#33539;&#22260;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Scope of In-Context Learning for the Extraction of Medical Temporal Constraints. (arXiv:2303.09366v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23450;&#20041;&#20102;&#19968;&#31181;MTC&#20998;&#31867;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;CFG&#27169;&#22411;&#30340;&#25277;&#21462;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;ICL&#33258;&#21160;&#25552;&#21462;&#21644;&#26631;&#20934;&#21270;DUGs&#20013;&#30340;MTC&#65292;&#26377;&#26395;&#36890;&#36807;&#23450;&#20041;&#23433;&#20840;&#30340;&#24739;&#32773;&#27963;&#21160;&#27169;&#24335;&#26469;&#25512;&#36827;&#20197;&#24739;&#32773;&#20026;&#20013;&#24515;&#30340;&#21307;&#30103;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#27835;&#30103;&#36890;&#24120;&#23545;&#24739;&#32773;&#30340;&#26085;&#24120;&#27963;&#21160;&#26045;&#21152;&#26102;&#38388;&#32422;&#26463;&#12290;&#36829;&#21453;&#21307;&#23398;&#26102;&#38388;&#32422;&#26463;&#65288;MTC&#65289;&#20250;&#23548;&#33268;&#32570;&#20047;&#27835;&#30103;&#20381;&#20174;&#24615;&#65292;&#20197;&#21450;&#19981;&#33391;&#30340;&#20581;&#24247;&#32467;&#26524;&#21644;&#22686;&#21152;&#30340;&#21307;&#30103;&#36153;&#29992;&#12290;&#36825;&#20123;MTC&#22312;&#24739;&#32773;&#25945;&#32946;&#26448;&#26009;&#21644;&#20020;&#24202;&#25991;&#26412;&#20013;&#30340;&#33647;&#29289;&#20351;&#29992;&#25351;&#21335;&#65288;DUGs&#65289;&#20013;&#34987;&#21457;&#29616;&#12290;&#36890;&#36807;&#22312;&#35745;&#31639;&#19978;&#34920;&#31034;DUGs&#20013;&#30340;MTC&#65292;&#23558;&#26377;&#21161;&#20110;&#36890;&#36807;&#24110;&#21161;&#23450;&#20041;&#23433;&#20840;&#30340;&#24739;&#32773;&#27963;&#21160;&#27169;&#24335;&#26469;&#25512;&#36827;&#20197;&#24739;&#32773;&#20026;&#20013;&#24515;&#30340;&#21307;&#30103;&#24212;&#29992;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;DUGs&#20013;&#21457;&#29616;&#30340;MTC&#20998;&#31867;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#65288;CFG&#65289;&#30340;&#27169;&#22411;&#26469;&#35745;&#31639;&#22320;&#34920;&#31034;MTC&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#19977;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#20849;&#35745;N = 836&#20010;&#24102;&#26631;&#20934;&#21270;&#30340;MTC&#26631;&#35760;&#30340;DUGs&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#33258;&#21160;&#25552;&#21462;&#21644;&#26631;&#20934;&#21270;DUGs&#20013;&#21457;&#29616;&#30340;MTC&#65292;&#36328;&#25152;&#26377;&#25968;&#25454;&#38598;&#23454;&#29616;&#20102;&#24179;&#22343;F1&#24471;&#20998;0.62&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;ICL&#27169;&#22411;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medications often impose temporal constraints on everyday patient activity. Violations of such medical temporal constraints (MTCs) lead to a lack of treatment adherence, in addition to poor health outcomes and increased healthcare expenses. These MTCs are found in drug usage guidelines (DUGs) in both patient education materials and clinical texts. Computationally representing MTCs in DUGs will advance patient-centric healthcare applications by helping to define safe patient activity patterns. We define a novel taxonomy of MTCs found in DUGs and develop a novel context-free grammar (CFG) based model to computationally represent MTCs from unstructured DUGs. Additionally, we release three new datasets with a combined total of N = 836 DUGs labeled with normalized MTCs. We develop an in-context learning (ICL) solution for automatically extracting and normalizing MTCs found in DUGs, achieving an average F1 score of 0.62 across all datasets. Finally, we rigorously investigate ICL model perfor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#23454;&#38469;&#19978;&#36827;&#34892;&#35299;&#26512;&#20197;&#21450;&#20026;&#20160;&#20040;&#33021;&#25429;&#25417;&#35299;&#26512;&#32467;&#26500;&#65292;&#35777;&#26126;&#20102;&#31867;&#20284;&#20110;BERT&#25110;RoBERTa&#36825;&#26679;&#30340;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36817;&#20284;&#25191;&#34892;&#33521;&#35821;PCFG&#30340;Inside-Outside&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.08117</link><description>&lt;p&gt;
&#36716;&#25442;&#22120;&#22312;&#39044;&#27979;&#25513;&#30721;&#21333;&#35789;&#26102;&#26159;&#21542;&#35299;&#26512;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Transformers Parse while Predicting the Masked Word?. (arXiv:2303.08117v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08117
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#23454;&#38469;&#19978;&#36827;&#34892;&#35299;&#26512;&#20197;&#21450;&#20026;&#20160;&#20040;&#33021;&#25429;&#25417;&#35299;&#26512;&#32467;&#26500;&#65292;&#35777;&#26126;&#20102;&#31867;&#20284;&#20110;BERT&#25110;RoBERTa&#36825;&#26679;&#30340;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36817;&#20284;&#25191;&#34892;&#33521;&#35821;PCFG&#30340;Inside-Outside&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#35777;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#20351;&#29992;&#31867;&#20284;&#20110;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#36825;&#26679;&#30340;&#26080;&#30417;&#30563;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#21487;&#20197;&#23545;&#35821;&#35328;&#32467;&#26500;&#36827;&#34892;&#32534;&#30721;&#65292;&#20363;&#22914;&#20381;&#36182;&#20851;&#31995;&#21644;&#32452;&#25104;&#25104;&#20998;&#20998;&#26512;&#26641;&#12290;&#20294;&#26159;&#20154;&#20204;&#23545;&#20110;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#23454;&#38469;&#19978;&#36827;&#34892;&#35299;&#26512;&#25110;&#20165;&#36827;&#34892;&#19982;&#35299;&#26512;&#24369;&#30456;&#20851;&#30340;&#19968;&#20123;&#35745;&#31639;&#23384;&#22312;&#30097;&#38382;&#12290;&#26412;&#25991;&#22312;&#29983;&#25104;&#24314;&#27169;&#30340;&#19978;&#19979;&#25991;&#20013;&#19968;&#27493;&#27493;&#22238;&#31572;&#20102;&#19978;&#36848;&#38382;&#39064;&#65292;&#25506;&#35752;&#20102;(a)&#26159;&#21542;&#26377;&#21487;&#33021;&#26126;&#30830;&#25551;&#36848;&#20855;&#26377;&#29616;&#23454;&#23884;&#20837;&#32500;&#24230;&#65292;&#22836;&#25968;&#31561;&#30340;&#36716;&#25442;&#22120;&#65292;&#33021;&#22815;&#36827;&#34892;&#35299;&#26512;&#29978;&#33267;&#36817;&#20284;&#35299;&#26512;&#65307;(b)&#39044;&#35757;&#32451;&#27169;&#22411;&#20026;&#20160;&#20040;&#33021;&#22815;&#25429;&#25417;&#35299;&#26512;&#32467;&#26500;&#65311;&#25105;&#20204;&#23637;&#31034;&#20102;&#31867;&#20284;&#20110;BERT&#25110;RoBERTa&#36825;&#26679;&#30340;&#20013;&#31561;&#22823;&#23567;&#30340;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36817;&#20284;&#25191;&#34892;&#33521;&#35821;PCFG&#65288;Marcus&#31561;&#65292;1993&#65289;&#30340;Inside-Outside&#31639;&#27861;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#65292;&#22312;PCFG&#29983;&#25104;&#35821;&#35328;&#24314;&#27169;&#25439;&#22833;&#19978;&#65292;Inside-Outside&#31639;&#27861;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models have been shown to encode linguistic structures, e.g. dependency and constituency parse trees, in their embeddings while being trained on unsupervised loss functions like masked language modeling. Some doubts have been raised whether the models actually are doing parsing or only some computation weakly correlated with it. We study questions: (a) Is it possible to explicitly describe transformers with realistic embedding dimension, number of heads, etc. that are capable of doing parsing -- or even approximate parsing? (b) Why do pre-trained models capture parsing structure? This paper takes a step toward answering these questions in the context of generative modeling with PCFGs. We show that masked language models like BERT or RoBERTa of moderate sizes can approximately execute the Inside-Outside algorithm for the English PCFG [Marcus et al, 1993]. We also show that the Inside-Outside algorithm is optimal for masked language modeling loss on the PCFG-generate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#32929;&#31080;&#20215;&#26684;&#30340;&#30456;&#20851;&#24615;&#24314;&#31435;&#20851;&#31995;&#65292;&#23454;&#29616;&#37329;&#34701;&#20998;&#26512;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#35814;&#32454;&#21644;&#20934;&#30830;&#30340;&#20102;&#35299;&#24773;&#24863;&#20998;&#26512;&#19982;&#32929;&#31080;&#20215;&#26684;&#20043;&#38388;&#20851;&#31995;&#30340;&#26041;&#27861;&#65292;&#23545;&#20110;&#25237;&#36164;&#32773;&#21644;&#37329;&#34701;&#20998;&#26512;&#24072;&#20570;&#20986;&#26126;&#26234;&#20915;&#31574;&#38750;&#24120;&#26377;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.02563</link><description>&lt;p&gt;
FinXABSA: &#36890;&#36807;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#37329;&#34701;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
FinXABSA: Explainable Finance through Aspect-Based Sentiment Analysis. (arXiv:2303.02563v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#32929;&#31080;&#20215;&#26684;&#30340;&#30456;&#20851;&#24615;&#24314;&#31435;&#20851;&#31995;&#65292;&#23454;&#29616;&#37329;&#34701;&#20998;&#26512;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#35814;&#32454;&#21644;&#20934;&#30830;&#30340;&#20102;&#35299;&#24773;&#24863;&#20998;&#26512;&#19982;&#32929;&#31080;&#20215;&#26684;&#20043;&#38388;&#20851;&#31995;&#30340;&#26041;&#27861;&#65292;&#23545;&#20110;&#25237;&#36164;&#32773;&#21644;&#37329;&#34701;&#20998;&#26512;&#24072;&#20570;&#20986;&#26126;&#26234;&#20915;&#31574;&#38750;&#24120;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an aspect-based sentiment analysis approach to achieve explainability in financial analysis by establishing a relationship with stock prices using the Pearson correlation coefficient. The proposed methodology provides a more detailed and accurate understanding of the relationship between sentiment analysis and stock prices, which can be useful for investors and financial analysts in making informed decisions.
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;Pearson&#30456;&#20851;&#31995;&#25968;&#24314;&#31435;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#19982;&#32929;&#31080;&#20215;&#26684;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#23454;&#29616;&#37329;&#34701;&#20998;&#26512;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#28041;&#21450;&#20174;&#37329;&#34701;&#26032;&#38395;&#25991;&#31456;&#20013;&#26500;&#24314;&#26041;&#38754;&#21015;&#34920;&#65292;&#24182;&#20998;&#26512;&#27599;&#20010;&#26041;&#38754;&#30340;&#24773;&#24863;&#24378;&#24230;&#24471;&#20998;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;Pearson&#31995;&#25968;&#23558;&#36825;&#20123;&#24471;&#20998;&#19982;&#30456;&#20851;&#20844;&#21496;&#30340;&#32929;&#31080;&#20215;&#26684;&#36827;&#34892;&#27604;&#36739;&#65292;&#20197;&#30830;&#23450;&#20219;&#20309;&#26174;&#33879;&#30340;&#30456;&#20851;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#35814;&#32454;&#21644;&#20934;&#30830;&#30340;&#20102;&#35299;&#24773;&#24863;&#20998;&#26512;&#19982;&#32929;&#31080;&#20215;&#26684;&#20043;&#38388;&#20851;&#31995;&#30340;&#26041;&#27861;&#65292;&#36825;&#23545;&#20110;&#25237;&#36164;&#32773;&#21644;&#37329;&#34701;&#20998;&#26512;&#24072;&#20570;&#20986;&#26126;&#26234;&#20915;&#31574;&#38750;&#24120;&#26377;&#29992;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#36879;&#26126;&#19988;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#26469;&#35299;&#37322;&#24773;&#24863;&#20998;&#26512;&#32467;&#26524;&#21450;&#20854;&#23545;&#32929;&#31080;&#20215;&#26684;&#30340;&#24433;&#21709;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#26412;&#25991;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21487;&#35299;&#37322;&#24615;&#22312;&#37329;&#34701;&#20998;&#26512;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel approach for explainability in financial analysis by utilizing the Pearson correlation coefficient to establish a relationship between aspect-based sentiment analysis and stock prices. The proposed methodology involves constructing an aspect list from financial news articles and analyzing sentiment intensity scores for each aspect. These scores are then compared to the stock prices for the relevant companies using the Pearson coefficient to determine any significant correlations. The results indicate that the proposed approach provides a more detailed and accurate understanding of the relationship between sentiment analysis and stock prices, which can be useful for investors and financial analysts in making informed decisions. Additionally, this methodology offers a transparent and interpretable way to explain the sentiment analysis results and their impact on stock prices. Overall, the findings of this paper demonstrate the importance of explainability in f
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TadNER&#30340;&#31867;&#22411;&#24863;&#30693;&#30340;&#20998;&#35299;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#38169;&#35823;&#36328;&#24230;&#21644;&#19981;&#20934;&#30830;&#21644;&#19981;&#31283;&#23450;&#30340;&#21407;&#22411;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.06397</link><description>&lt;p&gt;
&#31867;&#22411;&#24863;&#30693;&#30340;&#20998;&#35299;&#26694;&#26550;&#29992;&#20110;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Type-Aware Decomposed Framework for Few-Shot Named Entity Recognition. (arXiv:2302.06397v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TadNER&#30340;&#31867;&#22411;&#24863;&#30693;&#30340;&#20998;&#35299;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#38169;&#35823;&#36328;&#24230;&#21644;&#19981;&#20934;&#30830;&#21644;&#19981;&#31283;&#23450;&#30340;&#21407;&#22411;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#23569;&#26679;&#26412;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#20013;&#65292;&#20960;&#20010;&#20004;&#38454;&#27573;&#21407;&#22411;&#32593;&#32476;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#32467;&#26524;&#65292;&#20294;&#22312;&#27573;&#26816;&#27979;&#38454;&#27573;&#20986;&#29616;&#20102;&#36807;&#22810;&#30340;&#38169;&#35823;&#36328;&#24230;&#65292;&#24182;&#19988;&#22312;&#31867;&#22411;&#20998;&#31867;&#38454;&#27573;&#20986;&#29616;&#20102;&#19981;&#20934;&#30830;&#21644;&#19981;&#31283;&#23450;&#30340;&#21407;&#22411;&#65292;&#36825;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31867;&#22411;&#24863;&#30693;&#30340;&#20998;&#35299;&#26694;&#26550;&#65292;&#21363;TadNER&#65292;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#31867;&#22411;&#24863;&#30693;&#30340;&#36328;&#24230;&#36807;&#28388;&#31574;&#30053;&#65292;&#36890;&#36807;&#31227;&#38500;&#37027;&#20123;&#19982;&#31867;&#22411;&#21517;&#31216;&#35821;&#20041;&#19978;&#30456;&#36317;&#36739;&#36828;&#30340;&#20551;&#36328;&#24230;&#26469;&#36807;&#28388;&#25481;&#38169;&#35823;&#30340;&#36328;&#24230;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31867;&#22411;&#24863;&#30693;&#30340;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#32852;&#21512;&#21033;&#29992;&#25903;&#25345;&#26679;&#26412;&#21644;&#31867;&#22411;&#21517;&#31216;&#20316;&#20026;&#21442;&#32771;&#26469;&#26500;&#24314;&#26356;&#20934;&#30830;&#21644;&#31283;&#23450;&#30340;&#21407;&#22411;&#12290;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;TadNER&#26694;&#26550;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#23558;&#22312;https://github.com/NLPWM-WHU/TadNER&#19978;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the recent success achieved by several two-stage prototypical networks in few-shot named entity recognition (NER) task, the overdetected false spans at the span detection stage and the inaccurate and unstable prototypes at the type classification stage remain to be challenging problems. In this paper, we propose a novel Type-Aware Decomposed framework, namely TadNER, to solve these problems. We first present a type-aware span filtering strategy to filter out false spans by removing those semantically far away from type names. We then present a type-aware contrastive learning strategy to construct more accurate and stable prototypes by jointly exploiting support samples and type names as references. Extensive experiments on various benchmarks prove that our proposed TadNER framework yields a new state-of-the-art performance. Our code and data will be available at https://github.com/NLPWM-WHU/TadNER.
&lt;/p&gt;</description></item><item><title>XLM-V&#36890;&#36807;&#20811;&#26381;&#22810;&#35821;&#35328;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35789;&#27719;&#29942;&#39048;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#12290;&#20351;&#29992;&#19968;&#20010;&#19968;&#30334;&#19975;&#26631;&#35760;&#35789;&#27719;&#34920;&#65292;XLM-V&#22312;&#21508;&#39033;&#20219;&#21153;&#20013;&#22343;&#20248;&#20110;XLM-R&#12290;</title><link>http://arxiv.org/abs/2301.10472</link><description>&lt;p&gt;
XLM-V: &#20811;&#26381;&#22810;&#35821;&#35328;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35789;&#27719;&#29942;&#39048;
&lt;/p&gt;
&lt;p&gt;
XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models. (arXiv:2301.10472v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10472
&lt;/p&gt;
&lt;p&gt;
XLM-V&#36890;&#36807;&#20811;&#26381;&#22810;&#35821;&#35328;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35789;&#27719;&#29942;&#39048;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#12290;&#20351;&#29992;&#19968;&#20010;&#19968;&#30334;&#19975;&#26631;&#35760;&#35789;&#27719;&#34920;&#65292;XLM-V&#22312;&#21508;&#39033;&#20219;&#21153;&#20013;&#22343;&#20248;&#20110;XLM-R&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#20381;&#36182;&#20110;&#19968;&#20010;&#22312;100&#22810;&#31181;&#35821;&#35328;&#38388;&#20849;&#20139;&#30340;&#21333;&#19968;&#35789;&#27719;&#34920;&#12290;&#38543;&#30528;&#36825;&#20123;&#27169;&#22411;&#21442;&#25968;&#21644;&#28145;&#24230;&#30340;&#22686;&#21152;&#65292;&#35789;&#27719;&#22823;&#23567;&#22522;&#26412;&#20445;&#25345;&#19981;&#21464;&#12290;&#36825;&#31181;"&#35789;&#27719;&#29942;&#39048;"&#38480;&#21046;&#20102;XLM-R&#31561;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20943;&#23569;&#22312;&#35789;&#27719;&#19978;&#30340;&#36328;&#35821;&#35328;&#20849;&#20139;&#65292;&#20026;&#27599;&#31181;&#35821;&#35328;&#20998;&#37197;&#36275;&#22815;&#30340;&#35206;&#30422;&#33021;&#21147;&#65292;&#20174;&#32780;&#25193;&#23637;&#21040;&#38750;&#24120;&#22823;&#30340;&#22810;&#35821;&#35328;&#35789;&#27719;&#34920;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#35789;&#27719;&#36827;&#34892;&#20998;&#35789;&#36890;&#24120;&#27604;XLM-R&#26356;&#35821;&#20041;&#26377;&#24847;&#20041;&#19988;&#26356;&#30701;&#12290;&#21033;&#29992;&#36825;&#20010;&#25913;&#36827;&#30340;&#35789;&#27719;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#20855;&#26377;100&#19975;&#20010;&#26631;&#35760;&#35789;&#27719;&#34920;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;XLM-V&#12290;XLM-V&#22312;&#25105;&#20204;&#27979;&#35797;&#30340;&#27599;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#20110;XLM-R&#65292;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;XNLI&#65289;&#12289;&#38382;&#31572;&#65288;MLQA&#65292;XQuAD&#65292;TyDiQA&#65289;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;WikiAnn&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large multilingual language models typically rely on a single vocabulary shared across 100+ languages. As these models have increased in parameter count and depth, vocabulary size has remained largely unchanged. This \textit{vocabulary bottleneck} limits the representational capabilities of multilingual models like XLM-R. In this paper, we introduce a new approach for scaling to very large multilingual vocabularies by de-emphasizing token sharing between languages with little lexical overlap and assigning vocabulary capacity to achieve sufficient coverage for each individual language. Tokenizations using our vocabulary are typically more semantically meaningful and shorter compared to XLM-R. Leveraging this improved vocabulary, we train XLM-V, a multilingual language model with a one million token vocabulary. XLM-V outperforms XLM-R on every task we tested on ranging from natural language inference (XNLI), question answering (MLQA, XQuAD, TyDiQA), to named entity recognition (WikiAnn).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#26412;&#22320;&#21270;&#19982;&#32534;&#36753;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#23558;&#20107;&#23454;&#26412;&#22320;&#21270;&#21040;&#29305;&#23450;&#27169;&#22411;&#21442;&#25968;&#24182;&#19981;&#33021;&#25552;&#20379;&#32534;&#36753;&#25351;&#23548;&#12290;&#22240;&#26524;&#36861;&#36394;&#26041;&#27861;&#24182;&#19981;&#33021;&#25351;&#23548;&#32534;&#36753;&#21738;&#20010;&#27169;&#22411;&#23618;&#26469;&#35206;&#30422;&#23384;&#20648;&#30340;&#20107;&#23454;&#12290;</title><link>http://arxiv.org/abs/2301.04213</link><description>&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#26412;&#22320;&#21270;&#21644;&#22522;&#20110;&#30693;&#35782;&#32534;&#36753;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20196;&#20154;&#24778;&#35766;&#30340;&#24046;&#24322;&#65292;&#25506;&#35752;&#20102;&#26159;&#21542;&#26412;&#22320;&#21270;&#33021;&#22815;&#25552;&#20379;&#32534;&#36753;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models. (arXiv:2301.04213v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.04213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#26412;&#22320;&#21270;&#19982;&#32534;&#36753;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#23558;&#20107;&#23454;&#26412;&#22320;&#21270;&#21040;&#29305;&#23450;&#27169;&#22411;&#21442;&#25968;&#24182;&#19981;&#33021;&#25552;&#20379;&#32534;&#36753;&#25351;&#23548;&#12290;&#22240;&#26524;&#36861;&#36394;&#26041;&#27861;&#24182;&#19981;&#33021;&#25351;&#23548;&#32534;&#36753;&#21738;&#20010;&#27169;&#22411;&#23618;&#26469;&#35206;&#30422;&#23384;&#20648;&#30340;&#20107;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#23398;&#20064;&#21040;&#22823;&#37327;&#30340;&#20107;&#23454;&#20449;&#24687;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#23558;&#36825;&#20123;&#20449;&#24687;&#23450;&#20301;&#21040;&#27169;&#22411;&#30340;&#29305;&#23450;&#26435;&#37325;&#65292;&#22914;&#20013;&#38388;&#23618;MLP&#26435;&#37325;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#36890;&#36807;&#32534;&#36753;&#19981;&#21516;&#20110;&#29616;&#26377;&#26041;&#27861;&#25152;&#24314;&#35758;&#30340;&#23384;&#20648;&#20107;&#23454;&#20301;&#32622;&#30340;&#26435;&#37325;&#65292;&#21487;&#20197;&#25913;&#21464;&#27169;&#22411;&#20013;&#30340;&#20107;&#23454;&#23384;&#20648;&#26041;&#24335;&#12290;&#36825;&#19968;&#21457;&#29616;&#20196;&#20154;&#24847;&#22806;&#65292;&#22240;&#20026;&#25105;&#20204;&#21407;&#26412;&#26399;&#26395;&#23558;&#20107;&#23454;&#26412;&#22320;&#21270;&#21040;&#29305;&#23450;&#30340;&#27169;&#22411;&#21442;&#25968;&#21487;&#20197;&#21578;&#35785;&#25105;&#20204;&#22312;&#27169;&#22411;&#20013;&#22914;&#20309;&#25805;&#32437;&#30693;&#35782;&#65292;&#36825;&#19968;&#20551;&#35774;&#26366;&#28608;&#21457;&#36807;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#30340;&#30740;&#31350;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#34920;&#31034;&#21435;&#22122;&#65288;&#20063;&#31216;&#20026;&#22240;&#26524;&#36861;&#36394;&#65289;&#25152;&#24471;&#20986;&#30340;&#26412;&#22320;&#21270;&#32467;&#35770;&#24182;&#19981;&#33021;&#25552;&#20379;&#20219;&#20309;&#20851;&#20110;&#24212;&#35813;&#22312;&#21738;&#20010;&#27169;&#22411;MLP&#23618;&#36827;&#34892;&#32534;&#36753;&#20197;&#35206;&#30422;&#29616;&#26377;&#23384;&#20648;&#20107;&#23454;&#30340;&#26032;&#20107;&#23454;&#30340;&#35265;&#35299;&#12290;&#36825;&#19968;&#21457;&#29616;&#23545;&#36807;&#21435;&#30340;&#30740;&#31350;&#22914;&#20309;&#20381;&#36182;&#22240;&#26524;&#36861;&#36394;&#26469;&#36873;&#25321;&#38656;&#35201;&#32534;&#36753;&#30340;&#27169;&#22411;&#23618;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20960;&#31181;&#32534;&#36753;&#26041;&#27861;&#30340;&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models learn a great quantity of factual information during pretraining, and recent work localizes this information to specific model weights like mid-layer MLP weights. In this paper, we find that we can change how a fact is stored in a model by editing weights that are in a different location than where existing methods suggest that the fact is stored. This is surprising because we would expect that localizing facts to specific model parameters would tell us where to manipulate knowledge in models, and this assumption has motivated past work on model editing methods. Specifically, we show that localization conclusions from representation denoising (also known as Causal Tracing) do not provide any insight into which model MLP layer would be best to edit in order to override an existing stored fact with a new one. This finding raises questions about how past work relies on Causal Tracing to select which model layers to edit. Next, we consider several variants of the editing pr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25910;&#38598;&#20154;&#31867;&#21453;&#39304;&#20449;&#24687;&#65292;&#25506;&#35752;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#26469;&#25552;&#39640;&#25688;&#35201;&#30340;&#29983;&#25104;&#36136;&#37327;&#21644;&#29992;&#25143;&#20559;&#22909;&#19968;&#33268;&#24615;&#65292;&#29305;&#21035;&#20851;&#27880;&#25688;&#35201;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#65292;&#24182;&#36890;&#36807;&#19977;&#20010;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#36827;&#34892;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2212.09968</link><description>&lt;p&gt;
&#20851;&#20110;&#20174;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#20013;&#25552;&#39640;&#25688;&#35201;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Improving Summarization Factual Consistency from Natural Language Feedback. (arXiv:2212.09968v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09968
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25910;&#38598;&#20154;&#31867;&#21453;&#39304;&#20449;&#24687;&#65292;&#25506;&#35752;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#26469;&#25552;&#39640;&#25688;&#35201;&#30340;&#29983;&#25104;&#36136;&#37327;&#21644;&#29992;&#25143;&#20559;&#22909;&#19968;&#33268;&#24615;&#65292;&#29305;&#21035;&#20851;&#27880;&#25688;&#35201;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#65292;&#24182;&#36890;&#36807;&#19977;&#20010;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#36827;&#34892;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#30340;&#36755;&#20986;&#24182;&#19981;&#24635;&#26159;&#31526;&#21512;&#29992;&#25143;&#30340;&#26399;&#26395;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26159;&#21542;&#21487;&#20197;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#30340;&#20449;&#24687;&#21453;&#39304;&#26469;&#25552;&#39640;&#29983;&#25104;&#36136;&#37327;&#21644;&#29992;&#25143;&#20559;&#22909;&#30340;&#19968;&#33268;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#25688;&#35201;&#20013;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#20316;&#20026;&#29992;&#25143;&#26399;&#26395;&#30340;&#36136;&#37327;&#25351;&#26631;&#65292;&#21363;&#25688;&#35201;&#21482;&#24212;&#21253;&#21547;&#30001;&#36755;&#20837;&#25991;&#26723;&#25903;&#25345;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;DeFacto&#65292;&#20854;&#20013;&#21253;&#21547;&#20154;&#31867;&#30340;&#28436;&#31034;&#21644;&#26377;&#20851;&#25688;&#35201;&#20107;&#23454;&#19968;&#33268;&#24615;&#30340;&#32416;&#27491;&#24615;&#25351;&#31034;&#12289;&#32534;&#36753;&#21518;&#30340;&#25688;&#35201;&#21644;&#35299;&#37322;&#12290;&#21033;&#29992;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19977;&#20010;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#65306;&#65288;1&#65289;&#26681;&#25454;&#20154;&#31867;&#21453;&#39304;&#32534;&#36753;&#25688;&#35201;&#65292;&#65288;2&#65289;&#20026;&#32534;&#36753;&#21407;&#22987;&#25688;&#35201;&#29983;&#25104;&#20154;&#31867;&#21453;&#39304;&#65292;&#65288;3&#65289;&#36890;&#36807;&#29983;&#25104;&#20154;&#31867;&#21453;&#39304;&#21644;&#32534;&#36753;&#26469;&#20462;&#27491;&#21021;&#22987;&#25688;&#35201;&#30340;&#20107;&#23454;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the recent progress in language generation models, their outputs may not always meet user expectations. In this work, we study whether informational feedback in natural language can be leveraged to improve generation quality and user preference alignment. To this end, we consider factual consistency in summarization, the quality that the summary should only contain information supported by the input documents, as the user-expected preference. We collect a high-quality dataset, DeFacto, containing human demonstrations and informational natural language feedback consisting of corrective instructions, edited summaries, and explanations with respect to the factual consistency of the summary. Using our dataset, we study three natural language generation tasks: (1) editing a summary by following the human feedback, (2) generating human feedback for editing the original summary, and (3) revising the initial summary to correct factual errors by generating both the human feedback and ed
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25277;&#21462;&#24335;&#38382;&#39064;&#22238;&#31572;&#30340;&#21160;&#37327;&#23545;&#27604;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#21305;&#37197;&#22635;&#31354;&#24335;&#21644;&#33258;&#28982;&#26597;&#35810;-&#25991;&#31456;&#26679;&#26412;&#23545;&#30340;&#31572;&#26696;&#27010;&#29575;&#65292;&#33021;&#26356;&#22909;&#22320;&#23558;&#22312;&#22635;&#31354;&#24335;&#26679;&#26412;&#20013;&#23398;&#21040;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#22238;&#31572;&#33258;&#28982;&#38382;&#39064;&#19978;&#12290;</title><link>http://arxiv.org/abs/2212.05762</link><description>&lt;p&gt;
&#29992;&#20110;&#38382;&#39064;&#22238;&#31572;&#30340;&#21160;&#37327;&#23545;&#27604;&#39044;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Momentum Contrastive Pre-training for Question Answering. (arXiv:2212.05762v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05762
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25277;&#21462;&#24335;&#38382;&#39064;&#22238;&#31572;&#30340;&#21160;&#37327;&#23545;&#27604;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#21305;&#37197;&#22635;&#31354;&#24335;&#21644;&#33258;&#28982;&#26597;&#35810;-&#25991;&#31456;&#26679;&#26412;&#23545;&#30340;&#31572;&#26696;&#27010;&#29575;&#65292;&#33021;&#26356;&#22909;&#22320;&#23558;&#22312;&#22635;&#31354;&#24335;&#26679;&#26412;&#20013;&#23398;&#21040;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#22238;&#31572;&#33258;&#28982;&#38382;&#39064;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#25277;&#21462;&#24335;&#38382;&#31572;&#65288;QA&#65289;&#39044;&#35757;&#32451;&#26041;&#27861;&#29983;&#25104;&#31867;&#20284;&#20110;&#22635;&#31354;&#39064;&#30340;&#26597;&#35810;&#65292;&#20854;&#35821;&#27861;&#32467;&#26500;&#19982;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#19981;&#21516;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#39044;&#35757;&#32451;&#27169;&#22411;&#23545;&#31616;&#21333;&#30340;&#20851;&#38190;&#35789;&#21305;&#37197;&#36807;&#25311;&#21512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21160;&#37327;&#23545;&#27604;&#24335;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#21363;MCROSS&#65288;Momentum Contrastive pRe-training fOr queStion anSwering&#65289;&#29992;&#20110;&#25277;&#21462;&#24335;&#30340;&#38382;&#39064;&#22238;&#31572;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;MCROSS&#24341;&#20837;&#20102;&#21160;&#37327;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#26469;&#21305;&#37197;&#22635;&#31354;&#24335;&#21644;&#33258;&#28982;&#26597;&#35810;-&#25991;&#31456;&#26679;&#26412;&#23545;&#20043;&#38388;&#30340;&#31572;&#26696;&#27010;&#29575;&#12290;&#22240;&#27492;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#33021;&#22815;&#26356;&#22909;&#22320;&#23558;&#22312;&#22635;&#31354;&#24335;&#26679;&#26412;&#20013;&#23398;&#21040;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#22238;&#31572;&#33258;&#28982;&#38382;&#39064;&#19978;&#12290;&#22312;&#19977;&#20010;&#22522;&#20934;QA&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#25152;&#26377;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26377;&#30417;&#30563;&#21644;&#38646;-shot&#22330;&#26223;&#19979;&#22343;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing pre-training methods for extractive Question Answering (QA) generate cloze-like queries different from natural questions in syntax structure, which could overfit pre-trained models to simple keyword matching. In order to address this problem, we propose a novel Momentum Contrastive pRe-training fOr queStion anSwering (MCROSS) method for extractive QA. Specifically, MCROSS introduces a momentum contrastive learning framework to align the answer probability between cloze-like and natural query-passage sample pairs. Hence, the pre-trained models can better transfer the knowledge learned in cloze-like samples to answering natural questions. Experimental results on three benchmarking QA datasets show that our method achieves noticeable improvement compared with all baselines in both supervised and zero-shot scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#25991;&#26412;&#23450;&#20301;&#30340;&#25991;&#26723;&#32423;OCR&#27169;&#22411;&#65292;&#21487;&#20197;&#23558;&#25910;&#25454;&#22270;&#20687;&#20013;&#30340;&#25152;&#26377;&#23383;&#31526;&#36716;&#24405;&#20026;&#26377;&#24207;&#24207;&#21015;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2212.05525</link><description>&lt;p&gt;
&#25193;&#23637;TrOCR&#23454;&#29616;&#26080;&#25991;&#26412;&#23450;&#20301;&#30340;&#20840;&#39029;&#25195;&#25551;&#25910;&#25454;&#22270;&#20687;OCR
&lt;/p&gt;
&lt;p&gt;
Extending TrOCR for Text Localization-Free OCR of Full-Page Scanned Receipt Images. (arXiv:2212.05525v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05525
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#25991;&#26412;&#23450;&#20301;&#30340;&#25991;&#26723;&#32423;OCR&#27169;&#22411;&#65292;&#21487;&#20197;&#23558;&#25910;&#25454;&#22270;&#20687;&#20013;&#30340;&#25152;&#26377;&#23383;&#31526;&#36716;&#24405;&#20026;&#26377;&#24207;&#24207;&#21015;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25195;&#25551;&#25910;&#25454;&#30340;&#25968;&#23383;&#21270;&#26088;&#22312;&#20174;&#25910;&#25454;&#22270;&#20687;&#20013;&#25552;&#21462;&#25991;&#26412;&#65292;&#24182;&#23558;&#20854;&#20445;&#23384;&#21040;&#32467;&#26500;&#21270;&#25991;&#26723;&#20013;&#12290;&#36825;&#36890;&#24120;&#20998;&#20026;&#20004;&#20010;&#23376;&#20219;&#21153;&#65306;&#25991;&#26412;&#23450;&#20301;&#21644;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#65288;OCR&#65289;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;OCR&#27169;&#22411;&#21482;&#20851;&#27880;&#35009;&#21098;&#21518;&#30340;&#25991;&#26412;&#23454;&#20363;&#22270;&#20687;&#65292;&#36825;&#38656;&#35201;&#25991;&#26412;&#21306;&#22495;&#26816;&#27979;&#27169;&#22411;&#25552;&#20379;&#30340;&#36793;&#30028;&#26694;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#23558;&#39069;&#22806;&#30340;&#26816;&#27979;&#22120;&#24341;&#20837;&#20197;&#39044;&#20808;&#35782;&#21035;&#25991;&#26412;&#23454;&#20363;&#22270;&#20687;&#20250;&#22686;&#21152;&#22797;&#26434;&#24615;&#65292;&#20294;&#22522;&#20110;&#23454;&#20363;&#30340;OCR&#27169;&#22411;&#22312;&#22788;&#29702;&#25972;&#20010;&#22270;&#20687;&#30340;&#25991;&#26723;&#32423;OCR&#26102;&#65292;&#22914;&#21253;&#21547;&#22810;&#34892;&#25991;&#26412;&#20197;&#21508;&#31181;&#24067;&#23616;&#26041;&#24335;&#25490;&#21015;&#30340;&#25910;&#25454;&#22270;&#20687;&#65292;&#20934;&#30830;&#29575;&#38750;&#24120;&#20302;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#23450;&#20301;&#30340;&#25991;&#26723;&#32423;OCR&#27169;&#22411;&#65292;&#23558;&#25910;&#25454;&#22270;&#20687;&#20013;&#30340;&#25152;&#26377;&#23383;&#31526;&#36716;&#24405;&#20026;&#26377;&#24207;&#24207;&#21015;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#38543;&#26426;&#35009;&#21098;&#30340;&#22270;&#20687;&#22359;&#23545;&#39044;&#35757;&#32451;&#30340;&#23454;&#20363;&#32423;&#27169;&#22411;TrOCR&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#36880;&#28176;&#22686;&#21152;&#22270;&#20687;&#22359;&#30340;&#22823;&#23567;&#20197;&#25512;&#24191;&#35813;&#27169;&#22411;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Digitization of scanned receipts aims to extract text from receipt images and save it into structured documents. This is usually split into two sub-tasks: text localization and optical character recognition (OCR). Most existing OCR models only focus on the cropped text instance images, which require the bounding box information provided by a text region detection model. Introducing an additional detector to identify the text instance images in advance adds complexity, however instance-level OCR models have very low accuracy when processing the whole image for the document-level OCR, such as receipt images containing multiple text lines arranged in various layouts. To this end, we propose a localization-free document-level OCR model for transcribing all the characters in a receipt image into an ordered sequence end-to-end. Specifically, we finetune the pretrained instance-level model TrOCR with randomly cropped image chunks, and gradually increase the image chunk size to generalize the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#26631;&#35760;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#23558;&#39044;&#35757;&#32451;&#30340;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#25913;&#36896;&#20026;&#39044;&#35757;&#32451;&#30340;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#27169;&#22411;&#39044;&#35757;&#32451;&#19982;&#19979;&#28216;&#24494;&#35843;&#20043;&#38388;&#30340;&#24046;&#24322;&#21270;&#38382;&#39064;&#12290;PMR &#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#31168;&#65292;&#33021;&#26377;&#25928;&#25552;&#39640;&#39044;&#27979;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.04755</link><description>&lt;p&gt;
&#20174;Clozing&#21040;&#29702;&#35299;&#65306;&#23558;&#39044;&#35757;&#32451;&#30340;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#25913;&#36896;&#20026;&#39044;&#35757;&#32451;&#30340;&#26426;&#22120;&#38405;&#35835;&#22120;
&lt;/p&gt;
&lt;p&gt;
From Clozing to Comprehending: Retrofitting Pre-trained Masked Language Model to Pre-trained Machine Reader. (arXiv:2212.04755v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#26631;&#35760;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#23558;&#39044;&#35757;&#32451;&#30340;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#25913;&#36896;&#20026;&#39044;&#35757;&#32451;&#30340;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#27169;&#22411;&#39044;&#35757;&#32451;&#19982;&#19979;&#28216;&#24494;&#35843;&#20043;&#38388;&#30340;&#24046;&#24322;&#21270;&#38382;&#39064;&#12290;PMR &#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#31168;&#65292;&#33021;&#26377;&#25928;&#25552;&#39640;&#39044;&#27979;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Pre-trained Machine Reader (PMR)&#8221;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#39044;&#35757;&#32451;&#30340;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411; (MLMs) &#25913;&#36896;&#20026;&#39044;&#35757;&#32451;&#30340;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299; (MRC) &#27169;&#22411;&#65292;&#26080;&#38656;&#33719;&#21462;&#26631;&#35760;&#25968;&#25454;&#12290;&#36890;&#36807;&#20351;&#29992; Wikipedia &#36229;&#38142;&#25509;&#26500;&#24314;&#20102;&#22823;&#37327;&#36890;&#29992;&#19988;&#39640;&#36136;&#37327;&#30340; MRC &#39118;&#26684;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010; Wiki Anchor Extraction &#20219;&#21153;&#26469;&#25351;&#23548; MRC &#39118;&#26684;&#30340;&#39044;&#35757;&#32451;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#29616;&#26377; MLMs &#27169;&#22411;&#39044;&#35757;&#32451;&#19982;&#19979;&#28216;&#24494;&#35843;&#20043;&#38388;&#30340;&#24046;&#24322;&#21270;&#38382;&#39064;&#12290;&#38500;&#20102;&#31616;&#21333;&#26131;&#29992;&#65292;PMR &#36824;&#33021;&#26377;&#25928;&#35299;&#20915;&#19968;&#20123;&#22914;&#25277;&#21462;&#24335;&#38382;&#31572;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#31561;&#20219;&#21153;&#12290;PMR &#22312;&#29616;&#26377;&#26041;&#27861;&#26041;&#38754;&#26174;&#31034;&#20102;&#24040;&#22823;&#30340;&#25913;&#36827;&#65292;&#29305;&#21035;&#26159;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#12290;&#24403;&#24212;&#29992;&#20110; MRC &#20844;&#24335;&#20013;&#30340;&#24207;&#21015;&#20998;&#31867;&#20219;&#21153;&#26102;&#65292;PMR &#33021;&#22815;&#25552;&#21462;&#39640;&#36136;&#37327;&#30340;&#35777;&#26126;&#26448;&#26009;&#26469;&#35299;&#37322;&#20998;&#31867;&#36807;&#31243;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#22909;&#30340;&#39044;&#27979;&#21487;&#35299;&#37322;&#24615;&#12290;PMR &#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20063;&#27604;&#29616;&#26377;&#30340;&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Pre-trained Machine Reader (PMR), a novel method for retrofitting pre-trained masked language models (MLMs) to pre-trained machine reading comprehension (MRC) models without acquiring labeled data. PMR can resolve the discrepancy between model pre-training and downstream fine-tuning of existing MLMs. To build the proposed PMR, we constructed a large volume of general-purpose and high-quality MRC-style training data by using Wikipedia hyperlinks and designed a Wiki Anchor Extraction task to guide the MRC-style pre-training. Apart from its simplicity, PMR effectively solves extraction tasks, such as Extractive Question Answering and Named Entity Recognition. PMR shows tremendous improvements over existing approaches, especially in low-resource scenarios. When applied to the sequence classification task in the MRC formulation, PMR enables the extraction of high-quality rationales to explain the classification process, thereby providing greater prediction explainability. PMR als
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20302;&#36164;&#28304;&#35821;&#38899;&#29702;&#35299;&#30340;&#21452;&#21521;&#34920;&#31034;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#20016;&#23500;&#30340;&#21452;&#21521;&#32534;&#30721;&#65292;&#21487;&#20197;&#22312;&#24847;&#22270;&#39044;&#27979;&#31561;&#20219;&#21153;&#19978;&#21462;&#24471;&#36739;&#22909;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#39030;&#23618;&#25913;&#36827;&#20102;&#27969;&#30021;&#35821;&#38899;&#21629;&#20196;&#25968;&#25454;&#38598;&#19978;&#30340;&#24403;&#21069;&#25216;&#26415;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2211.14320</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#35821;&#38899;&#29702;&#35299;&#30340;&#21452;&#21521;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Bidirectional Representations for Low Resource Spoken Language Understanding. (arXiv:2211.14320v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14320
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20302;&#36164;&#28304;&#35821;&#38899;&#29702;&#35299;&#30340;&#21452;&#21521;&#34920;&#31034;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#20016;&#23500;&#30340;&#21452;&#21521;&#32534;&#30721;&#65292;&#21487;&#20197;&#22312;&#24847;&#22270;&#39044;&#27979;&#31561;&#20219;&#21153;&#19978;&#21462;&#24471;&#36739;&#22909;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#39030;&#23618;&#25913;&#36827;&#20102;&#27969;&#30021;&#35821;&#38899;&#21629;&#20196;&#25968;&#25454;&#38598;&#19978;&#30340;&#24403;&#21069;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#35821;&#38899;&#29702;&#35299;&#31995;&#32479;&#20351;&#29992;&#30001;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#25509;&#21475;&#21644;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#27169;&#22359;&#32452;&#25104;&#30340;&#27969;&#27700;&#32447;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#23558;&#36830;&#32493;&#36755;&#20837;&#36716;&#25442;&#20026;&#31163;&#25955;&#35821;&#35328;&#31526;&#21495;&#26102;&#20250;&#20135;&#29983;&#22256;&#38590;&#20915;&#31574;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#34920;&#31034;&#27169;&#22411;&#65292;&#29992;&#20110;&#23558;&#35821;&#38899;&#32534;&#30721;&#20026;&#20016;&#23500;&#30340;&#21452;&#21521;&#32534;&#30721;&#65292;&#21487;&#29992;&#20110;&#35832;&#22914;&#24847;&#22270;&#39044;&#27979;&#20043;&#31867;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#25513;&#34109;&#35821;&#35328;&#24314;&#27169;&#30446;&#26631;&#26469;&#23398;&#20064;&#34920;&#31034;&#65292;&#22240;&#27492;&#20174;&#24038;&#21491;&#19978;&#19979;&#25991;&#20013;&#21463;&#30410;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#26410;&#32463;&#24494;&#35843;&#30340;&#29983;&#25104;&#30340;&#32534;&#30721;&#24615;&#33021;&#20248;&#20110;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#21487;&#27604;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#24494;&#35843;&#34920;&#31034;&#27169;&#22411;&#30340;&#39030;&#23618;&#65292;&#25913;&#36827;&#20102;&#27969;&#30021;&#35821;&#38899;&#21629;&#20196;&#25968;&#25454;&#38598;&#19978;&#30340;&#24403;&#21069;&#25216;&#26415;&#27700;&#24179;&#65292;&#24403;&#20351;&#29992;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#26102;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31867;&#21035;&#27880;&#24847;&#21147;&#20316;&#20026;&#19968;&#31181;&#35821;&#38899;&#29702;&#35299;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most spoken language understanding systems use a pipeline approach composed of an automatic speech recognition interface and a natural language understanding module. This approach forces hard decisions when converting continuous inputs into discrete language symbols. Instead, we propose a representation model to encode speech in rich bidirectional encodings that can be used for downstream tasks such as intent prediction. The approach uses a masked language modelling objective to learn the representations, and thus benefits from both the left and right contexts. We show that the performance of the resulting encodings before fine-tuning is better than comparable models on multiple datasets, and that fine-tuning the top layers of the representation model improves the current state of the art on the Fluent Speech Command dataset, also in a low-data regime, when a limited amount of labelled data is used for training. Furthermore, we propose class attention as a spoken language understanding
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#34920;&#31034;&#31354;&#38388;&#30340;&#35282;&#24230;&#23545;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#25216;&#26415;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#36890;&#36807;&#20998;&#31867;&#21644;&#35752;&#35770;&#19981;&#21516;&#30340;&#25968;&#23398;&#35282;&#24230;&#21644;&#26041;&#27861;&#65292;&#20171;&#32461;&#20102;KGE&#27169;&#22411;&#21450;&#20854;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2211.03536</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65306;&#22522;&#20110;&#34920;&#31034;&#31354;&#38388;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Embedding: A Survey from the Perspective of Representation Spaces. (arXiv:2211.03536v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.03536
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#34920;&#31034;&#31354;&#38388;&#30340;&#35282;&#24230;&#23545;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#25216;&#26415;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#36890;&#36807;&#20998;&#31867;&#21644;&#35752;&#35770;&#19981;&#21516;&#30340;&#25968;&#23398;&#35282;&#24230;&#21644;&#26041;&#27861;&#65292;&#20171;&#32461;&#20102;KGE&#27169;&#22411;&#21450;&#20854;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65288;KGE&#65289;&#26159;&#19968;&#31181;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#30340;&#25216;&#26415;&#65292;&#26088;&#22312;&#23558;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#34920;&#31034;&#20026;&#20302;&#32500;&#35821;&#20041;&#31354;&#38388;&#65292;&#29992;&#20110;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#22914;&#38142;&#25509;&#39044;&#27979;&#65292;&#30693;&#35782;&#25512;&#29702;&#21644;&#30693;&#35782;&#34917;&#20840;&#12290;&#26412;&#25991;&#20174;&#34920;&#31034;&#31354;&#38388;&#30340;&#35282;&#24230;&#23545;&#29616;&#26377;&#30340;KGE&#25216;&#26415;&#36827;&#34892;&#20102;&#31995;&#32479;&#32508;&#36848;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#22522;&#20110;&#34920;&#31034;&#31354;&#38388;&#30340;&#19977;&#20010;&#25968;&#23398;&#35282;&#24230;&#65288;&#20195;&#25968;&#35282;&#24230;&#12289;&#20960;&#20309;&#35282;&#24230;&#21644;&#20998;&#26512;&#35282;&#24230;&#65289;&#26500;&#24314;&#20102;&#19968;&#20010;&#32454;&#31890;&#24230;&#20998;&#31867;&#65292;&#20171;&#32461;&#20102;&#22522;&#26412;&#25968;&#23398;&#31354;&#38388;&#30340;&#20005;&#26684;&#23450;&#20041;&#65292;&#28982;&#21518;&#28145;&#20837;&#30740;&#31350;&#20102;KGE&#27169;&#22411;&#21450;&#20854;&#25968;&#23398;&#29305;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;&#19977;&#20010;&#31867;&#21035;&#20013;&#30340;&#19981;&#21516;KGE&#26041;&#27861;&#65292;&#24182;&#24635;&#32467;&#20102;&#31354;&#38388;&#20248;&#21183;&#22312;&#19981;&#21516;&#23884;&#20837;&#38656;&#27714;&#19978;&#30340;&#20316;&#29992;&#12290;&#36890;&#36807;&#25972;&#29702;&#26469;&#33258;&#19979;&#28216;&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;KGE&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph embedding (KGE) is an increasingly popular technique that aims to represent entities and relations of knowledge graphs into low-dimensional semantic spaces for a wide spectrum of applications such as link prediction, knowledge reasoning and knowledge completion. In this paper, we provide a systematic review of existing KGE techniques based on representation spaces. Particularly, we build a fine-grained classification to categorise the models based on three mathematical perspectives of the representation spaces: (1) Algebraic perspective, (2) Geometric perspective, and (3) Analytical perspective. We introduce the rigorous definitions of fundamental mathematical spaces before diving into KGE models and their mathematical properties. We further discuss different KGE methods over the three categories, as well as summarise how spatial advantages work over different embedding needs. By collating the experimental results from downstream tasks, we also explore the advantages of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#25551;&#36848;&#30693;&#35782;&#22270;&#20013;&#20851;&#31995;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#25910;&#38598;&#19968;&#20010;&#26032;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#20855;&#26377;&#22810;&#26679;&#21270;&#12289;&#26126;&#30830;&#19988;&#26377;&#24847;&#20041;&#30340;&#25968;&#25454;&#26631;&#31614;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#25551;&#36848;&#26032;&#39062;&#12289;&#26410;&#35265;&#36807;&#30340;&#20851;&#31995;&#26102;&#20855;&#26377;&#24778;&#20154;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.07373</link><description>&lt;p&gt;
&#27880;&#24847;&#26631;&#31614;&#65306;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#25551;&#36848;&#30693;&#35782;&#22270;&#20013;&#30340;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Mind the Labels: Describing Relations in Knowledge Graphs With Pretrained Models. (arXiv:2210.07373v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#25551;&#36848;&#30693;&#35782;&#22270;&#20013;&#20851;&#31995;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#25910;&#38598;&#19968;&#20010;&#26032;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#20855;&#26377;&#22810;&#26679;&#21270;&#12289;&#26126;&#30830;&#19988;&#26377;&#24847;&#20041;&#30340;&#25968;&#25454;&#26631;&#31614;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#25551;&#36848;&#26032;&#39062;&#12289;&#26410;&#35265;&#36807;&#30340;&#20851;&#31995;&#26102;&#20855;&#26377;&#24778;&#20154;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#27867;&#21270;&#21040;&#39046;&#22495;&#22806;&#30340;&#26679;&#26412;&#65292;&#29992;&#20110;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#21487;&#20197;&#20351;&#29992;&#20154;&#21487;&#35835;&#30340;&#25968;&#25454;&#26631;&#31614;&#65292;&#22914;&#21015;&#26631;&#39064;&#12289;&#38190;&#25110;&#20851;&#31995;&#21517;&#31216;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#36825;&#20123;&#26631;&#31614;&#19981;&#26126;&#30830;&#25110;&#19981;&#23436;&#25972;&#65292;&#27169;&#22411;&#24120;&#24120;&#20250;&#20135;&#29983;&#35821;&#20041;&#38169;&#35823;&#30340;&#36755;&#20986;&#65292;&#22312;&#25968;&#25454;&#21040;&#25991;&#26412;&#25968;&#25454;&#38598;&#20013;&#32463;&#24120;&#20986;&#29616;&#36825;&#31181;&#24773;&#20917;&#12290;&#26412;&#25991;&#23601;&#25551;&#36848;&#20004;&#20010;&#23454;&#20307;&#20043;&#38388;&#20851;&#31995;&#30340;&#20219;&#21153;&#25581;&#31034;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#20026;&#20102;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35814;&#32454;&#35828;&#26126;&#26469;&#33258;&#19977;&#20010;&#22823;&#35268;&#27169;&#30693;&#35782;&#22270;&#35889;(Wikidata&#12289;DBPedia&#12289;YAGO)&#30340;1,522&#20010;&#29420;&#29305;&#20851;&#31995;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#19981;&#28165;&#26970;&#30340;&#24773;&#20917;&#19979;&#39044;&#26399;&#20250;&#22833;&#36133;&#65292;&#20294;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#20102;&#22823;&#37327;&#21508;&#31181;&#20851;&#31995;&#26631;&#31614;&#30340;&#27169;&#22411;&#22312;&#35814;&#32454;&#35828;&#26126;&#26032;&#39062;&#12289;&#26410;&#35265;&#36807;&#30340;&#20851;&#31995;&#26102;&#34920;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#20351;&#29992;&#20855;&#26377;&#22810;&#26679;&#21270;&#12289;&#26126;&#30830;&#19988;&#26377;&#24847;&#20041;&#30340;&#25968;&#25454;&#26631;&#31614;&#26159;&#35757;&#32451;&#33021;&#22815;&#27867;&#21270;&#21040;&#26032;&#39046;&#22495;&#30340;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;&#31995;&#32479;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained language models (PLMs) for data-to-text (D2T) generation can use human-readable data labels such as column headings, keys, or relation names to generalize to out-of-domain examples. However, the models are well-known in producing semantically inaccurate outputs if these labels are ambiguous or incomplete, which is often the case in D2T datasets. In this paper, we expose this issue on the task of descibing a relation between two entities. For our experiments, we collect a novel dataset for verbalizing a diverse set of 1,522 unique relations from three large-scale knowledge graphs (Wikidata, DBPedia, YAGO). We find that although PLMs for D2T generation expectedly fail on unclear cases, models trained with a large variety of relation labels are surprisingly robust in verbalizing novel, unseen relations. We argue that using data with a diverse set of clear and meaningful labels is key to training D2T generation systems capable of generalizing to novel domains.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26816;&#32034;&#36719;&#25552;&#31034;&#26377;&#25928;&#36741;&#21161;&#30828;&#25552;&#31034;&#65292;&#22312;&#22686;&#21152;&#23569;&#37327;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2210.03029</link><description>&lt;p&gt;
&#36890;&#36807;&#26816;&#32034;&#36719;&#25552;&#31034;&#22686;&#24378;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#34920;&#29616;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Efficiently Enhancing Zero-Shot Performance of Instruction Following Model via Retrieval of Soft Prompt. (arXiv:2210.03029v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03029
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26816;&#32034;&#36719;&#25552;&#31034;&#26377;&#25928;&#36741;&#21161;&#30828;&#25552;&#31034;&#65292;&#22312;&#22686;&#21152;&#23569;&#37327;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#21319;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#34920;&#29616;&#25928;&#29575;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#65292;&#35201;&#20040;&#36890;&#36807;&#25193;&#23637;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#24635;&#25968;&#65292;&#35201;&#20040;&#22686;&#21152;&#27169;&#22411;&#30340;&#22823;&#23567;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#31034;&#24494;&#35843;&#33719;&#21462;&#36719;&#25552;&#31034;&#65292;&#25506;&#32034;&#20102;&#22914;&#20309;&#36890;&#36807;&#26816;&#32034;&#36719;&#25552;&#31034;&#26377;&#25928;&#36741;&#21161;&#30828;&#25552;&#31034;&#26469;&#36827;&#34892;&#38646;&#26679;&#26412;&#20219;&#21153;&#27867;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#31034;&#24494;&#35843;&#20026;&#27599;&#20010;&#25552;&#31034;&#35757;&#32451;&#36719;&#25552;&#31034;&#23884;&#20837;&#65292;&#23384;&#20648;&#19982;&#25552;&#31034;&#23884;&#20837;&#26144;&#23556;&#30340;&#35757;&#32451;&#23454;&#20363;&#26679;&#26412;&#65292;&#24182;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#26816;&#32034;&#26368;&#25509;&#36817;&#26597;&#35810;&#23454;&#20363;&#30340;&#35757;&#32451;&#23454;&#20363;&#23545;&#24212;&#30340;&#25552;&#31034;&#23884;&#20837;&#12290;&#34429;&#28982;&#21482;&#22686;&#21152;&#20102;0.007%&#30340;&#39069;&#22806;&#21442;&#25968;&#65292;&#26816;&#32034;&#36719;&#25552;&#31034;&#25552;&#39640;&#20102;T0&#22312;&#26410;&#35265;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#22312;11&#20010;&#25968;&#25454;&#38598;&#20013;&#26377;10&#20010;&#34920;&#29616;&#20248;&#20110;T0&#65292;&#24182;&#19988;&#23558;T0&#22312;BIG-bench&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;2.39&#20010;&#30334;&#20998;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25253;&#21578;&#20102;&#19968;&#20010;&#26377;&#24847;&#24605;&#30340;&#21457;&#29616;&#65292;&#21363;&#26816;&#32034;&#22312;&#30456;&#20284;&#31572;&#26696;&#36873;&#25321;&#26684;&#24335;&#19978;&#35757;&#32451;&#30340;&#28304;&#23884;&#20837;&#27604;&#25552;&#31034;&#23884;&#20837;&#26356;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Enhancing the zero-shot performance of instruction-following models requires heavy computation, either by scaling the total number of training datasets or the model size. In this work, we explore how retrieval of soft prompts obtained through prompt tuning can efficiently assist hard prompts in zero-shot task generalization. Specifically, we train soft prompt embeddings for each prompt through prompt tuning, store the samples of the training instances mapped with the prompt embeddings, and retrieve the corresponding prompt embedding of the training instance closest to the query instance during inference. While only adding 0.007% additional parameters, retrieval of soft prompt enhances the performance of T0 on unseen tasks by outperforming it on 10 out of 11 datasets as well as improving the mean accuracy of T0 on BIG-bench benchmark by 2.39% points. Also, we report an interesting finding that retrieving source embeddings trained on similar answer choice formats is more important than t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;Twitter&#25991;&#26412;&#20013;&#20998;&#31867;&#21069;&#25552;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;RoBERTa&#27169;&#22411;&#22312;&#21069;&#25552;&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.03851</link><description>&lt;p&gt;
5q032e@SMM4H'22: &#22522;&#20110;Transformer&#30340;COVID-19&#30456;&#20851;&#25512;&#25991;&#21069;&#25552;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
5q032e@SMM4H'22: Transformer-based classification of premise in tweets related to COVID-19. (arXiv:2209.03851v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.03851
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;Twitter&#25991;&#26412;&#20013;&#20998;&#31867;&#21069;&#25552;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;RoBERTa&#27169;&#22411;&#22312;&#21069;&#25552;&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#32593;&#32476;&#25968;&#25454;&#35780;&#20272;&#30340;&#33258;&#21160;&#21270;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#32463;&#20856;&#25361;&#25112;&#20043;&#19968;&#12290;&#22312;COVID-19&#22823;&#27969;&#34892;&#26399;&#38388;&#65292;&#20174;&#20844;&#20849;&#20449;&#24687;&#20013;&#25366;&#25496;&#20154;&#20204;&#30340;&#31435;&#22330;&#23545;&#20110;&#29702;&#35299;&#23545;&#20581;&#24247;&#21629;&#20196;&#30340;&#24577;&#24230;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#20316;&#32773;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#29992;&#20110;&#23545;&#25512;&#29305;&#25991;&#26412;&#20013;&#21069;&#25552;&#30340;&#20998;&#31867;&#12290;&#26412;&#24037;&#20316;&#26159;&#20316;&#20026;2022&#24180;Social Media Mining for Health (SMM4H)&#30740;&#35752;&#20250;&#30340;&#19968;&#37096;&#20998;&#23436;&#25104;&#30340;&#12290;&#25105;&#20204;&#22312;&#26500;&#24314;&#39640;&#25928;&#25429;&#25417;&#25512;&#25991;&#35821;&#20041;&#30340;&#27969;&#31243;&#26102;&#65292;&#25506;&#32034;&#20102;&#29616;&#20195;&#22522;&#20110;Transformer&#30340;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#22312;&#19968;&#20010;&#25512;&#29305;&#25968;&#25454;&#38598;&#19978;&#34920;&#26126;&#65292;&#22312;&#21069;&#25552;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;RoBERTa&#27169;&#22411;&#20248;&#20110;&#20854;&#20182;Transformer&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#22312;ROC AUC&#20540;&#20026;0.807&#65292;F1&#20998;&#25968;&#20026;0.7648&#26041;&#38754;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automation of social network data assessment is one of the classic challenges of natural language processing. During the COVID-19 pandemic, mining people's stances from public messages have become crucial regarding understanding attitudes towards health orders. In this paper, the authors propose the predictive model based on transformer architecture to classify the presence of premise in Twitter texts. This work is completed as part of the Social Media Mining for Health (SMM4H) Workshop 2022. We explored modern transformer-based classifiers in order to construct the pipeline efficiently capturing tweets semantics. Our experiments on a Twitter dataset showed that RoBERTa is superior to the other transformer models in the case of the premise prediction task. The model achieved competitive performance with respect to ROC AUC value 0.807, and 0.7648 for the F1 score.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;APEL&#26694;&#26550;&#65292;&#38750;&#31243;&#24207;&#21592;&#21487;&#20197;&#36890;&#36807;&#26816;&#26597;&#36755;&#20837;-&#36755;&#20986;&#31034;&#20363;&#38388;&#25509;&#36873;&#25321;&#22797;&#26434;&#31243;&#24207;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#30340;&#27880;&#37322;&#65292;&#24182;&#19988;&#22312;&#37325;&#26032;&#27880;&#37322;&#25991;&#26412;&#21040;SQL&#25968;&#25454;&#38598;&#26102;&#36798;&#21040;&#20102;&#19982;&#19987;&#23478;&#30456;&#21516;&#30340;&#20934;&#30830;&#24230;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#21407;&#22987;&#27880;&#37322;&#20013;&#30340;&#32454;&#24494;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2205.12422</link><description>&lt;p&gt;
&#38388;&#25509;&#36890;&#36807;&#20027;&#21160;&#31034;&#20363;&#20026;&#38750;&#31243;&#24207;&#21592;&#28155;&#21152;&#26631;&#31614;&#31243;&#24207;&#65306;&#20197;Text-to-SQL&#20026;&#20363;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Labeling Programs with Non-Programmers Indirectly via Active Examples: A Case Study with Text-to-SQL. (arXiv:2205.12422v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12422
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;APEL&#26694;&#26550;&#65292;&#38750;&#31243;&#24207;&#21592;&#21487;&#20197;&#36890;&#36807;&#26816;&#26597;&#36755;&#20837;-&#36755;&#20986;&#31034;&#20363;&#38388;&#25509;&#36873;&#25321;&#22797;&#26434;&#31243;&#24207;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#30340;&#27880;&#37322;&#65292;&#24182;&#19988;&#22312;&#37325;&#26032;&#27880;&#37322;&#25991;&#26412;&#21040;SQL&#25968;&#25454;&#38598;&#26102;&#36798;&#21040;&#20102;&#19982;&#19987;&#23478;&#30456;&#21516;&#30340;&#20934;&#30830;&#24230;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#21407;&#22987;&#27880;&#37322;&#20013;&#30340;&#32454;&#24494;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#31243;&#24207;&#21592;&#33021;&#21542;&#20351;&#29992;&#22797;&#26434;&#31243;&#24207;&#23545;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#36827;&#34892;&#27880;&#37322;&#20197;&#34920;&#31034;&#20854;&#21547;&#20041;&#65311;&#25105;&#20204;&#20171;&#32461;&#20102;APEL&#65292;&#36825;&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#38750;&#31243;&#24207;&#21592;&#20174;&#30001;&#31181;&#23376;&#35821;&#20041;&#35299;&#26512;&#22120;&#65288;&#20363;&#22914;Codex&#65289;&#29983;&#25104;&#30340;&#20505;&#36873;&#31243;&#24207;&#20013;&#36873;&#25321;&#12290;&#30001;&#20110;&#20182;&#20204;&#26080;&#27861;&#29702;&#35299;&#20505;&#36873;&#31243;&#24207;&#65292;&#25105;&#20204;&#35201;&#27714;&#20182;&#20204;&#36890;&#36807;&#26816;&#26597;&#31243;&#24207;&#30340;&#36755;&#20837;-&#36755;&#20986;&#31034;&#20363;&#26469;&#38388;&#25509;&#36873;&#25321;&#12290;&#23545;&#20110;&#27599;&#20010;&#34920;&#36798;&#24335;&#65292;APEL&#20027;&#21160;&#25628;&#32034;&#19968;&#20010;&#31616;&#21333;&#30340;&#36755;&#20837;&#65292;&#20505;&#36873;&#31243;&#24207;&#22312;&#36825;&#20010;&#36755;&#20837;&#19978;&#26356;&#20542;&#21521;&#20110;&#20135;&#29983;&#19981;&#21516;&#30340;&#36755;&#20986;&#12290;&#28982;&#21518;&#65292;&#23427;&#21482;&#35201;&#27714;&#38750;&#31243;&#24207;&#21592;&#36873;&#25321;&#36866;&#24403;&#30340;&#36755;&#20986;&#65292;&#20174;&#32780;&#20801;&#35768;&#25105;&#20204;&#25512;&#26029;&#20986;&#21738;&#20010;&#31243;&#24207;&#26159;&#27491;&#30830;&#30340;&#65292;&#21487;&#20197;&#29992;&#20110;&#35843;&#20248;&#35299;&#26512;&#22120;&#12290;&#20316;&#20026;&#31532;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#25307;&#21215;&#20102;&#20154;&#31867;&#38750;&#31243;&#24207;&#21592;&#20351;&#29992;APEL&#37325;&#26032;&#27880;&#37322;SPIDER&#65292;&#19968;&#20010;&#25991;&#26412;&#21040;SQL&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#19982;&#21407;&#22987;&#19987;&#23478;&#27880;&#37322;&#32773;&#30456;&#21516;&#30340;&#27880;&#37322;&#20934;&#30830;&#24230;&#65288;75%&#65289;&#65292;&#24182;&#25581;&#31034;&#20102;&#21407;&#22987;&#27880;&#37322;&#20013;&#35768;&#22810;&#24494;&#23567;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can non-programmers annotate natural language utterances with complex programs that represent their meaning? We introduce APEL, a framework in which non-programmers select among candidate programs generated by a seed semantic parser (e.g., Codex). Since they cannot understand the candidate programs, we ask them to select indirectly by examining the programs' input-ouput examples. For each utterance, APEL actively searches for a simple input on which the candidate programs tend to produce different outputs. It then asks the non-programmers only to choose the appropriate output, thus allowing us to infer which program is correct and could be used to fine-tune the parser. As a first case study, we recruited human non-programmers to use APEL to re-annotate SPIDER, a text-to-SQL dataset. Our approach achieved the same annotation accuracy as the original expert annotators (75%) and exposed many subtle errors in the original annotations.
&lt;/p&gt;</description></item><item><title>Speculative Decoding&#26159;&#19968;&#31181;&#26032;&#22411;&#35299;&#30721;&#33539;&#24335;&#65292;&#32467;&#21512;&#20102;&#33258;&#22238;&#24402;&#32763;&#35793;&#65288;AT&#65289;&#21644;&#38750;&#33258;&#22238;&#24402;&#32763;&#35793;&#65288;NAT&#65289;&#30340;&#20248;&#21183;&#65292;&#25552;&#20379;&#20102;&#26080;&#25439;&#21152;&#36895;&#30340;&#32763;&#35793;&#26041;&#27861;&#12290;&#22312;&#27599;&#20010;&#35299;&#30721;&#27493;&#39588;&#20013;&#65292;&#23427;&#25512;&#27979;&#24615;&#22320;&#39044;&#27979;&#19979;&#19968;&#20010;&#26631;&#35760;&#65292;&#24182;&#20351;&#29992;&#39564;&#35777;&#27169;&#22411;&#30830;&#20445;&#32763;&#35793;&#32467;&#26524;&#19982;AT&#23436;&#20840;&#30456;&#21516;&#12290;&#36890;&#36807;&#25512;&#27979;&#35299;&#30721;&#21644;&#39564;&#35777;&#30340;&#21327;&#20316;&#65292;&#23454;&#29616;&#20102;&#26356;&#24555;&#30340;&#35299;&#30721;&#36895;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#32763;&#35793;&#36136;&#37327;&#19981;&#21464;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#21407;&#22987;&#30340;SpecDec&#19982;AT&#36138;&#23146;&#35299;&#30721;&#30340;&#32467;&#26524;&#23436;&#20840;&#30456;&#21516;&#12290;</title><link>http://arxiv.org/abs/2203.16487</link><description>&lt;p&gt;
Speculative Decoding: &#26080;&#25439;&#21152;&#36895;&#33258;&#22238;&#24402;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Speculative Decoding: Lossless Speedup of Autoregressive Translation. (arXiv:2203.16487v5 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.16487
&lt;/p&gt;
&lt;p&gt;
Speculative Decoding&#26159;&#19968;&#31181;&#26032;&#22411;&#35299;&#30721;&#33539;&#24335;&#65292;&#32467;&#21512;&#20102;&#33258;&#22238;&#24402;&#32763;&#35793;&#65288;AT&#65289;&#21644;&#38750;&#33258;&#22238;&#24402;&#32763;&#35793;&#65288;NAT&#65289;&#30340;&#20248;&#21183;&#65292;&#25552;&#20379;&#20102;&#26080;&#25439;&#21152;&#36895;&#30340;&#32763;&#35793;&#26041;&#27861;&#12290;&#22312;&#27599;&#20010;&#35299;&#30721;&#27493;&#39588;&#20013;&#65292;&#23427;&#25512;&#27979;&#24615;&#22320;&#39044;&#27979;&#19979;&#19968;&#20010;&#26631;&#35760;&#65292;&#24182;&#20351;&#29992;&#39564;&#35777;&#27169;&#22411;&#30830;&#20445;&#32763;&#35793;&#32467;&#26524;&#19982;AT&#23436;&#20840;&#30456;&#21516;&#12290;&#36890;&#36807;&#25512;&#27979;&#35299;&#30721;&#21644;&#39564;&#35777;&#30340;&#21327;&#20316;&#65292;&#23454;&#29616;&#20102;&#26356;&#24555;&#30340;&#35299;&#30721;&#36895;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#32763;&#35793;&#36136;&#37327;&#19981;&#21464;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#21407;&#22987;&#30340;SpecDec&#19982;AT&#36138;&#23146;&#35299;&#30721;&#30340;&#32467;&#26524;&#23436;&#20840;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20043;&#21069;&#19968;&#20123;&#29306;&#29298;&#32763;&#35793;&#36136;&#37327;&#21152;&#36895;&#33258;&#22238;&#24402;&#32763;&#35793;&#65288;AT&#65289;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Speculative Decoding&#65288;SpecDec&#65289;-&#19968;&#31181;&#21463;&#35745;&#31639;&#26426;&#20307;&#31995;&#32467;&#26500;&#20013;&#30340;&#25512;&#27979;&#25191;&#34892;&#21551;&#21457;&#30340;&#26032;&#22411;&#35299;&#30721;&#33539;&#24335;&#65292;&#23427;&#32467;&#21512;&#20102;AT&#21644;&#38750;&#33258;&#22238;&#24402;&#32763;&#35793;&#65288;NAT&#65289;&#30340;&#21508;&#33258;&#20248;&#21183;&#65292;&#23454;&#29616;&#20102;&#22312;&#32763;&#35793;&#36807;&#31243;&#20013;&#30340;&#26080;&#25439;&#21152;&#36895;&#12290;&#22312;&#27599;&#20010;&#35299;&#30721;&#27493;&#39588;&#20013;&#65292;SpecDec&#39318;&#20808;&#20351;&#29992;NAT&#27169;&#22411;&#25512;&#27979;&#24615;&#22320;&#39044;&#27979;&#65288;&#21363;&#35299;&#30721;&#65289;&#19979;&#19968;&#20010;k&#20010;&#26631;&#35760;&#65292;&#28982;&#21518;&#20351;&#29992;AT&#27169;&#22411;&#39564;&#35777;&#36825;&#20123;&#26631;&#35760;&#65292;&#21482;&#26377;&#36890;&#36807;&#39564;&#35777;&#30340;&#39044;&#27979;&#26631;&#35760;&#25165;&#20250;&#34987;&#25509;&#21463;&#20316;&#20026;&#35299;&#30721;&#32467;&#26524;&#65292;&#20197;&#30830;&#20445;&#20854;&#32763;&#35793;&#32467;&#26524;&#19982;AT&#23436;&#20840;&#30456;&#21516;&#12290;NAT&#30340;&#25512;&#27979;&#21644;AT&#30340;&#39564;&#35777;&#20043;&#38388;&#30340;&#21327;&#20316;&#20351;&#24471;&#35299;&#30721;&#36895;&#24230;&#22823;&#22823;&#25552;&#39640;&#65292;&#21516;&#26102;&#19981;&#25439;&#22833;&#32763;&#35793;&#36136;&#37327;&#65292;&#36825;&#26159;&#30001;&#20110;&#25512;&#27979;&#35299;&#30721;&#25152;&#25903;&#25345;&#30340;&#24182;&#34892;&#35745;&#31639;&#12290;&#25105;&#20204;&#22312;4&#20010;&#26631;&#20934;WMT&#32763;&#35793;&#22522;&#20934;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#35777;&#23454;&#21407;&#22987;&#30340;SpecDec&#19982;AT&#36138;&#23146;&#35299;&#30721;&#30340;&#32467;&#26524;&#23436;&#20840;&#30456;&#21516;&#65292;&#36895;&#24230;&#25552;&#39640;&#20102;&#32422; $k$&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Different from some previous work accelerating autoregressive translation (AT) at the sacrifice of quality, we propose Speculative Decoding (SpecDec) -a novel decoding paradigm inspired by speculative execution in computer architecture, which combines respective advantages of AT and non-autoregressive translation (NAT) for lossless speedup of translation. At each decoding step, SpecDec first speculatively drafts (i.e. decodes) next $k$ tokens with an NAT model and then verifies them with an AT model, where only the drafted tokens passing the verification are accepted as decoded tokens for guaranteeing its translation result is exactly the same as AT. The collaboration of NAT drafting and AT verification leads to a much higher decoding speed without quality loss due to parallel computing enabled by speculative decoding.  We conduct experiments in 4 standard WMT translation benchmarks and confirm the vanilla SpecDec yields exactly the same results as AT greedy decoding with an around $
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21453;&#20107;&#23454;&#35760;&#24518;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#25551;&#36848;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#24573;&#30053;&#20102;&#21738;&#20123;&#29305;&#23450;&#25991;&#26723;&#30340;&#39044;&#27979;&#21464;&#21270;&#12290;&#36890;&#36807;&#30740;&#31350;&#26631;&#20934;&#25991;&#26412;&#25968;&#25454;&#38598;&#20013;&#30340;&#21453;&#20107;&#23454;&#35760;&#24518;&#35757;&#32451;&#26679;&#26412;&#65292;&#25105;&#20204;&#21487;&#20197;&#20272;&#35745;&#27599;&#20010;&#35760;&#24518;&#26679;&#26412;&#23545;&#39564;&#35777;&#38598;&#21644;&#29983;&#25104;&#25991;&#26412;&#30340;&#24433;&#21709;&#65292;&#24182;&#30452;&#25509;&#25552;&#20379;&#35760;&#24518;&#26469;&#28304;&#30340;&#35777;&#25454;&#12290;</title><link>http://arxiv.org/abs/2112.12938</link><description>&lt;p&gt;
&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21453;&#20107;&#23454;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Memorization in Neural Language Models. (arXiv:2112.12938v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.12938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21453;&#20107;&#23454;&#35760;&#24518;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#25551;&#36848;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#24573;&#30053;&#20102;&#21738;&#20123;&#29305;&#23450;&#25991;&#26723;&#30340;&#39044;&#27979;&#21464;&#21270;&#12290;&#36890;&#36807;&#30740;&#31350;&#26631;&#20934;&#25991;&#26412;&#25968;&#25454;&#38598;&#20013;&#30340;&#21453;&#20107;&#23454;&#35760;&#24518;&#35757;&#32451;&#26679;&#26412;&#65292;&#25105;&#20204;&#21487;&#20197;&#20272;&#35745;&#27599;&#20010;&#35760;&#24518;&#26679;&#26412;&#23545;&#39564;&#35777;&#38598;&#21644;&#29983;&#25104;&#25991;&#26412;&#30340;&#24433;&#21709;&#65292;&#24182;&#30452;&#25509;&#25552;&#20379;&#35760;&#24518;&#26469;&#28304;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#29616;&#20195;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#35760;&#24518;&#25935;&#24863;&#20449;&#24687;&#30340;&#39118;&#38505;&#12290;&#29702;&#35299;&#36825;&#31181;&#35760;&#24518;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#21644;&#23398;&#20064;&#29702;&#35770;&#30340;&#35282;&#24230;&#37117;&#24456;&#37325;&#35201;&#12290;&#22312;&#20808;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#35760;&#24518;&#30740;&#31350;&#20013;&#23384;&#22312;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#65292;&#21363;&#22914;&#20309;&#36807;&#28388;&#25481;&#8220;&#24120;&#35265;&#8221;&#35760;&#24518;&#12290;&#20107;&#23454;&#19978;&#65292;&#22823;&#22810;&#25968;&#35760;&#24518;&#26631;&#20934;&#19982;&#22312;&#35757;&#32451;&#38598;&#20013;&#20986;&#29616;&#30340;&#27425;&#25968;&#24378;&#28872;&#30456;&#20851;&#65292;&#25429;&#25417;&#21040;&#24120;&#35265;&#30701;&#35821;&#12289;&#20844;&#20849;&#30693;&#35782;&#12289;&#27169;&#26495;&#21270;&#25991;&#26412;&#25110;&#20854;&#20182;&#37325;&#22797;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21453;&#20107;&#23454;&#35760;&#24518;&#30340;&#27010;&#24565;&#65292;&#25551;&#36848;&#20102;&#27169;&#22411;&#30340;&#39044;&#27979;&#22312;&#30465;&#30053;&#29305;&#23450;&#25991;&#26723;&#36827;&#34892;&#35757;&#32451;&#26102;&#22914;&#20309;&#25913;&#21464;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;&#25991;&#26412;&#25968;&#25454;&#38598;&#20013;&#30830;&#23450;&#24182;&#30740;&#31350;&#20102;&#21453;&#20107;&#23454;&#35760;&#24518;&#30340;&#35757;&#32451;&#26679;&#26412;&#12290;&#25105;&#20204;&#20272;&#35745;&#20102;&#27599;&#20010;&#35760;&#24518;&#35757;&#32451;&#26679;&#26412;&#23545;&#39564;&#35777;&#38598;&#21644;&#29983;&#25104;&#25991;&#26412;&#30340;&#24433;&#21709;&#65292;&#23637;&#31034;&#20102;&#36825;&#22914;&#20309;&#30452;&#25509;&#25552;&#20379;&#35760;&#24518;&#26469;&#28304;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern neural language models that are widely used in various NLP tasks risk memorizing sensitive information from their training data. Understanding this memorization is important in real world applications and also from a learning-theoretical perspective. An open question in previous studies of language model memorization is how to filter out "common" memorization. In fact, most memorization criteria strongly correlate with the number of occurrences in the training set, capturing memorized familiar phrases, public knowledge, templated texts, or other repeated data. We formulate a notion of counterfactual memorization which characterizes how a model's predictions change if a particular document is omitted during training. We identify and study counterfactually-memorized training examples in standard text datasets. We estimate the influence of each memorized training example on the validation set and on generated texts, showing how this can provide direct evidence of the source of memo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22270;&#24418;&#26368;&#22823;&#21270;&#20989;&#25968;&#65292;&#29992;&#20110;&#20219;&#21153;&#29305;&#23450;&#30340;&#25991;&#26412;&#29983;&#25104;&#12290;&#35813;&#20989;&#25968;&#32467;&#21512;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#23616;&#30693;&#35782;&#21644;&#29305;&#23450;&#22330;&#26223;&#35821;&#26009;&#24211;&#30340;&#23616;&#37096;&#30693;&#35782;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#30340;&#26041;&#24335;&#24212;&#29992;&#20110;&#20256;&#32479;&#30340;softmax&#20989;&#25968;&#65292;&#20197;&#20805;&#20998;&#21033;&#29992;&#20849;&#29616;&#20449;&#24687;&#65292;&#25552;&#39640;&#29983;&#25104;&#25991;&#26412;&#30340;&#20027;&#39064;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2101.00153</link><description>&lt;p&gt;
&#22270;&#24418;&#26368;&#22823;&#21270;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Graphmax for Text Generation. (arXiv:2101.00153v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2101.00153
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22270;&#24418;&#26368;&#22823;&#21270;&#20989;&#25968;&#65292;&#29992;&#20110;&#20219;&#21153;&#29305;&#23450;&#30340;&#25991;&#26412;&#29983;&#25104;&#12290;&#35813;&#20989;&#25968;&#32467;&#21512;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#23616;&#30693;&#35782;&#21644;&#29305;&#23450;&#22330;&#26223;&#35821;&#26009;&#24211;&#30340;&#23616;&#37096;&#30693;&#35782;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#30340;&#26041;&#24335;&#24212;&#29992;&#20110;&#20256;&#32479;&#30340;softmax&#20989;&#25968;&#65292;&#20197;&#20805;&#20998;&#21033;&#29992;&#20849;&#29616;&#20449;&#24687;&#65292;&#25552;&#39640;&#29983;&#25104;&#25991;&#26412;&#30340;&#20027;&#39064;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#26412;&#29983;&#25104;&#20013;&#65292;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#20165;&#22522;&#20110;&#19978;&#19979;&#25991;&#20013;&#20808;&#21069;&#36873;&#25321;&#30340;&#20869;&#23481;&#65292;&#20351;&#29992;softmax&#20989;&#25968;&#36873;&#25321;&#27599;&#20010;&#26032;&#35789;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#29305;&#23450;&#22330;&#26223;&#35821;&#26009;&#24211;&#30340;&#24182;&#21457;&#35789;&#30340;&#38142;&#25509;&#32479;&#35745;&#20449;&#24687;&#23545;&#36873;&#25321;&#19979;&#19968;&#20010;&#35789;&#26159;&#26377;&#20215;&#20540;&#30340;&#65292;&#21487;&#20197;&#24110;&#21161;&#30830;&#20445;&#29983;&#25104;&#25991;&#26412;&#30340;&#20027;&#39064;&#19982;&#24403;&#21069;&#20219;&#21153;&#30456;&#19968;&#33268;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#20849;&#29616;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20219;&#21153;&#29305;&#23450;&#25991;&#26412;&#29983;&#25104;&#30340;&#22270;&#24418;&#26368;&#22823;&#21270;&#20989;&#25968;&#12290;&#20351;&#29992;&#22522;&#20110;&#22270;&#30340;&#27491;&#21017;&#21270;&#65292;&#22270;&#24418;&#26368;&#22823;&#21270;&#20351;&#26368;&#32456;&#35789;&#30340;&#36873;&#25321;&#30001;LM&#30340;&#20840;&#23616;&#30693;&#35782;&#21644;&#29305;&#23450;&#22330;&#26223;&#35821;&#26009;&#24211;&#30340;&#23616;&#37096;&#30693;&#35782;&#20849;&#21516;&#30830;&#23450;&#12290;&#20256;&#32479;&#30340;softmax&#20989;&#25968;&#36890;&#36807;&#22270;&#24635;&#21464;&#21270;&#65288;GTV&#65289;&#39033;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#23558;&#23616;&#37096;&#30693;&#35782;&#34701;&#20837;&#21040;LM&#20013;&#65292;&#24182;&#40723;&#21169;&#27169;&#22411;&#32771;&#34385;&#29305;&#23450;&#22330;&#26223;&#35821;&#26009;&#24211;&#20013;&#21333;&#35789;&#20043;&#38388;&#30340;&#32479;&#35745;&#20851;&#31995;&#12290;&#25152;&#25552;&#20986;&#30340;&#22270;&#24418;&#26368;&#22823;&#21270;&#21151;&#33021;&#22810;&#26679;&#19988;&#26131;&#20110;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In text generation, a large language model (LM) makes a choice of each new word based only on the former selection of its context using the softmax function. Nevertheless, the link statistics information of concurrent words based on a scene-specific corpus is valuable in choosing the next word, which can help to ensure the topic of the generated text to be aligned with the current task. To fully explore the co-occurrence information,we propose a graphmax function for task-specific text generation. Using the graph-based regularization, graphmax enables the final word choice to be determined by both the global knowledge from the LM and the local knowledge from the scene-specific corpus. The traditional softmax function is regularized with a graph total variation (GTV) term, which incorporates the local knowledge into the LM and encourages the model to consider the statistical relationships between words in a scene-specific corpus. The proposed graphmax is versatile and can be readily plu
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;&#26102;&#38388;&#21367;&#31215;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;TCAN&#65289;&#30340;&#26550;&#26500;&#65292;&#23427;&#32467;&#21512;&#20102;&#26102;&#38388;&#21367;&#31215;&#32593;&#32476;&#21644;&#27880;&#24847;&#26426;&#21046;&#65292;&#26082;&#33021;&#26367;&#20195;&#24490;&#29615;&#32593;&#32476;&#65292;&#21448;&#33021;&#21560;&#25910;&#21069;&#39304;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#26368;&#26032;&#30340;&#22256;&#24785;&#24230;&#32467;&#26524;&#21040;30.28&#65288;&#22522;&#20110;&#21333;&#35789;&#30340;PTB&#65289;&#65292;1.092&#65288;&#22522;&#20110;&#23383;&#31526;&#30340;PTB&#65289;&#12290;</title><link>http://arxiv.org/abs/2002.12530</link><description>&lt;p&gt;
&#26102;&#38388;&#21367;&#31215;&#27880;&#24847;&#21147;&#32593;&#32476;&#29992;&#20110;&#24207;&#21015;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Temporal Convolutional Attention-based Network For Sequence Modeling. (arXiv:2002.12530v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2002.12530
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;&#26102;&#38388;&#21367;&#31215;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;TCAN&#65289;&#30340;&#26550;&#26500;&#65292;&#23427;&#32467;&#21512;&#20102;&#26102;&#38388;&#21367;&#31215;&#32593;&#32476;&#21644;&#27880;&#24847;&#26426;&#21046;&#65292;&#26082;&#33021;&#26367;&#20195;&#24490;&#29615;&#32593;&#32476;&#65292;&#21448;&#33021;&#21560;&#25910;&#21069;&#39304;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#26368;&#26032;&#30340;&#22256;&#24785;&#24230;&#32467;&#26524;&#21040;30.28&#65288;&#22522;&#20110;&#21333;&#35789;&#30340;PTB&#65289;&#65292;1.092&#65288;&#22522;&#20110;&#23383;&#31526;&#30340;PTB&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21069;&#39304;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#29992;&#20110;&#24207;&#21015;&#24314;&#27169;&#30340;&#40664;&#35748;&#27169;&#22411;&#36880;&#28176;&#28436;&#21464;&#20026;&#21462;&#20195;&#24490;&#29615;&#32593;&#32476;&#12290;&#35768;&#22810;&#22522;&#20110;&#21367;&#31215;&#32593;&#32476;&#21644;&#27880;&#24847;&#26426;&#21046;&#30340;&#24378;&#22823;&#21069;&#39304;&#27169;&#22411;&#34987;&#25552;&#20986;&#65292;&#24182;&#26174;&#31034;&#20986;&#26356;&#22810;&#22788;&#29702;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#24819;&#30693;&#36947;&#26159;&#21542;&#26377;&#19968;&#31181;&#26550;&#26500;&#26082;&#33021;&#23454;&#29616;&#23545;&#24490;&#29615;&#32593;&#32476;&#30340;&#36817;&#20284;&#26367;&#20195;&#65292;&#21448;&#33021;&#21560;&#25910;&#21069;&#39304;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25506;&#32034;&#24615;&#26550;&#26500;&#65292;&#31216;&#20026;&#26102;&#38388;&#21367;&#31215;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;TCAN&#65289;&#65292;&#23427;&#32467;&#21512;&#20102;&#26102;&#38388;&#21367;&#31215;&#32593;&#32476;&#21644;&#27880;&#24847;&#26426;&#21046;&#12290;TCAN&#21253;&#25324;&#20004;&#20010;&#37096;&#20998;&#65292;&#19968;&#20010;&#26159;&#26102;&#38388;&#27880;&#24847;&#21147;&#65288;TA&#65289;&#65292;&#29992;&#20110;&#25429;&#25417;&#24207;&#21015;&#20869;&#30340;&#30456;&#20851;&#29305;&#24449;&#65292;&#21478;&#19968;&#20010;&#26159;&#22686;&#24378;&#27531;&#24046;&#65288;ER&#65289;&#65292;&#29992;&#20110;&#25552;&#21462;&#27973;&#23618;&#30340;&#37325;&#35201;&#20449;&#24687;&#24182;&#20256;&#36882;&#32473;&#28145;&#23618;&#12290;&#25105;&#20204;&#23558;bpc/&#22256;&#24785;&#24230;&#30340;&#26368;&#26032;&#32467;&#26524;&#25913;&#36827;&#21040;30.28&#65288;&#22522;&#20110;&#21333;&#35789;&#30340;PTB&#65289;&#65292;1.092&#65288;&#22522;&#20110;&#23383;&#31526;&#30340;PTB&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of feed-forward models, the default model for sequence modeling has gradually evolved to replace recurrent networks. Many powerful feed-forward models based on convolutional networks and attention mechanism were proposed and show more potential to handle sequence modeling tasks. We wonder that is there an architecture that can not only achieve an approximate substitution of recurrent network, but also absorb the advantages of feed-forward models. So we propose an exploratory architecture referred to Temporal Convolutional Attention-based Network (TCAN) which combines temporal convolutional network and attention mechanism. TCAN includes two parts, one is Temporal Attention (TA) which captures relevant features inside the sequence, the other is Enhanced Residual (ER) which extracts shallow layer's important information and transfers to deep layers. We improve the state-of-the-art results of bpc/perplexity to 30.28 on word-level PTB, 1.092 on character-level PTB, and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#20351;&#29992;&#19981;&#21516;&#31867;&#22411;&#30340;&#35789;&#21521;&#37327;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#21152;&#26435;&#24179;&#22343;&#35789;&#21521;&#37327;&#29305;&#24449;&#26469;&#23398;&#20064;&#21644;&#20272;&#35745;&#35780;&#35770;&#30340;&#26497;&#24615;&#65292;&#21516;&#26102;&#19982;&#24050;&#26377;&#26041;&#27861;&#36827;&#34892;&#23545;&#27604;&#12290;</title><link>http://arxiv.org/abs/2002.05606</link><description>&lt;p&gt;
&#20351;&#29992;&#21152;&#26435;&#24179;&#22343;&#35789;&#21521;&#37327;&#29305;&#24449;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Sentiment Analysis Using Averaged Weighted Word Vector Features. (arXiv:2002.05606v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2002.05606
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#20351;&#29992;&#19981;&#21516;&#31867;&#22411;&#30340;&#35789;&#21521;&#37327;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#21152;&#26435;&#24179;&#22343;&#35789;&#21521;&#37327;&#29305;&#24449;&#26469;&#23398;&#20064;&#21644;&#20272;&#35745;&#35780;&#35770;&#30340;&#26497;&#24615;&#65292;&#21516;&#26102;&#19982;&#24050;&#26377;&#26041;&#27861;&#36827;&#34892;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#24191;&#27867;&#20351;&#29992;&#20114;&#32852;&#32593;&#20998;&#20139;&#20182;&#20204;&#23545;&#20135;&#21697;&#12289;&#26381;&#21153;&#25110;&#26053;&#34892;&#30446;&#30340;&#22320;&#30340;&#20307;&#39564;&#12290;&#22312;&#32447;&#21453;&#39304;&#35780;&#35770;&#30340;&#25991;&#23383;&#23545;&#20110;&#28040;&#36153;&#32773;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#65292;&#21487;&#20197;&#20316;&#20026;&#34913;&#37327;&#20135;&#21697;&#25110;&#26381;&#21153;&#28385;&#24847;&#24230;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;&#24773;&#24863;&#20998;&#26512;&#26159;&#35782;&#21035;&#36825;&#20123;&#25991;&#26412;&#29255;&#27573;&#20013;&#34920;&#36798;&#30340;&#35266;&#28857;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#31181;&#26041;&#27861;&#65292;&#23558;&#19981;&#21516;&#31867;&#22411;&#30340;&#35789;&#21521;&#37327;&#32467;&#21512;&#36215;&#26469;&#23398;&#20064;&#21644;&#20272;&#35745;&#35780;&#35770;&#30340;&#26497;&#24615;&#12290;&#25105;&#20204;&#20174;&#35789;&#21521;&#37327;&#20013;&#21019;&#24314;&#24179;&#22343;&#35780;&#35770;&#21521;&#37327;&#65292;&#24182;&#22312;&#27491;&#38754;&#21644;&#36127;&#38754;&#25935;&#24863;&#26631;&#35760;&#30340;&#35780;&#35770;&#20013;&#20351;&#29992;&#35789;&#39057;&#32473;&#36825;&#20123;&#35780;&#35770;&#21521;&#37327;&#28155;&#21152;&#26435;&#37325;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#26041;&#27861;&#24212;&#29992;&#20110;&#22810;&#20010;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#34987;&#29992;&#20316;&#24773;&#24863;&#20998;&#26512;&#30340;&#26631;&#20934;&#22522;&#20934;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#25216;&#26415;&#19982;&#20854;&#20182;&#25216;&#26415;&#21644;&#24050;&#26377;&#26041;&#27861;&#36827;&#34892;&#32452;&#21512;&#65292;&#24182;&#19982;&#25991;&#29486;&#20013;&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
People use the world wide web heavily to share their experience with entities such as products, services, or travel destinations. Texts that provide online feedback in the form of reviews and comments are essential to make consumer decisions. These comments create a valuable source that may be used to measure satisfaction related to products or services. Sentiment analysis is the task of identifying opinions expressed in such text fragments. In this work, we develop two methods that combine different types of word vectors to learn and estimate polarity of reviews. We develop average review vectors from word vectors and add weights to this review vectors using word frequencies in positive and negative sensitivity-tagged reviews. We applied the methods to several datasets from different domains that are used as standard benchmarks for sentiment analysis. We ensemble the techniques with each other and existing methods, and we make a comparison with the approaches in the literature. The re
&lt;/p&gt;</description></item></channel></rss>