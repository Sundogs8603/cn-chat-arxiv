<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#35821;&#35328;&#27169;&#22411;&#24050;&#34987;&#29992;&#20110;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#65292;&#20294;&#20854;&#30693;&#35782;&#38543;&#30528;&#19990;&#30028;&#21464;&#21270;&#19981;&#26029;&#36807;&#26102;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#27880;&#20837;&#21333;&#20010;&#20107;&#23454;&#25104;&#21151;&#65292;&#20294;&#22312;&#22522;&#20110;&#27880;&#20837;&#30340;&#20107;&#23454;&#36827;&#34892;&#25512;&#29702;&#65288;&#25110;&#20256;&#25773;&#36825;&#20123;&#20107;&#23454;&#65289;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#36825;&#19968;&#30740;&#31350;&#31361;&#26174;&#20102;&#27880;&#20837;&#30693;&#35782;&#20256;&#25773;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#38656;&#35201;&#26032;&#25216;&#26415;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21644;&#20351;&#29992;&#23454;&#20307;&#30340;&#26032;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2305.01651</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20174;&#25551;&#36848;&#20013;&#23398;&#20064;&#26032;&#23454;&#20307;&#21527;&#65311;&#27880;&#20837;&#30693;&#35782;&#20256;&#25773;&#20013;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Can LMs Learn New Entities from Descriptions? Challenges in Propagating Injected Knowledge. (arXiv:2305.01651v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01651
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#24050;&#34987;&#29992;&#20110;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#65292;&#20294;&#20854;&#30693;&#35782;&#38543;&#30528;&#19990;&#30028;&#21464;&#21270;&#19981;&#26029;&#36807;&#26102;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#27880;&#20837;&#21333;&#20010;&#20107;&#23454;&#25104;&#21151;&#65292;&#20294;&#22312;&#22522;&#20110;&#27880;&#20837;&#30340;&#20107;&#23454;&#36827;&#34892;&#25512;&#29702;&#65288;&#25110;&#20256;&#25773;&#36825;&#20123;&#20107;&#23454;&#65289;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#36825;&#19968;&#30740;&#31350;&#31361;&#26174;&#20102;&#27880;&#20837;&#30693;&#35782;&#20256;&#25773;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#38656;&#35201;&#26032;&#25216;&#26415;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21644;&#20351;&#29992;&#23454;&#20307;&#30340;&#26032;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#34987;&#29992;&#20110;&#20687;&#38382;&#31572;&#36825;&#26679;&#30340;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#65292;&#20294;&#26159;&#38543;&#30528;&#19990;&#30028;&#30340;&#21464;&#21270;&#65292;&#23427;&#20204;&#30340;&#30693;&#35782;&#19981;&#26029;&#36807;&#26102;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#30740;&#31350;&#20102;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26377;&#38024;&#23545;&#24615;&#30340;&#26356;&#26032;&#65292;&#27880;&#20837;&#20010;&#21035;&#30340;&#20107;&#23454;&#24182;&#35780;&#20272;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#23398;&#20064;&#36825;&#20123;&#20107;&#23454;&#65292;&#21516;&#26102;&#19981;&#25913;&#21464;&#20854;&#20182;&#19978;&#19979;&#25991;&#30340;&#39044;&#27979;&#12290;&#26412;&#25991;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22522;&#20110;&#27880;&#20837;&#30340;&#20107;&#23454;&#36827;&#34892;&#25512;&#29702;&#30340;&#33021;&#21147;&#65288;&#25110;&#20256;&#25773;&#36825;&#20123;&#20107;&#23454;&#30340;&#33021;&#21147;&#65289;&#65306;&#20363;&#22914;&#65292;&#22312;&#23398;&#20064;&#20102;&#26576;&#20010;&#19996;&#35199;&#26159;&#30005;&#35270;&#33410;&#30446;&#20043;&#21518;&#65292;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20250;&#39044;&#27979;&#20320;&#21487;&#20197;&#36890;&#36807;&#23427;&#26469;&#35266;&#30475;? &#25105;&#20204;&#36890;&#36807;&#20004;&#31181;&#22635;&#31354;&#24335;&#20219;&#21153;&#26469;&#30740;&#31350;&#36825;&#19968;&#38382;&#39064;&#65306;&#19968;&#20010;&#26159;&#20851;&#20110;&#26032;&#23454;&#20307;&#30340;&#29616;&#23454;&#19990;&#30028;&#21477;&#23376;&#30340;&#29616;&#26377;&#25968;&#25454;&#38598;&#65288;ECBD&#65289;&#65292;&#21478;&#19968;&#20010;&#26159;&#19968;&#20010;&#26032;&#30340;&#21463;&#25511;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#25163;&#21160;&#35774;&#35745;&#30340;&#27169;&#26495;&#35201;&#27714;&#27880;&#20837;&#30340;&#30693;&#35782;&#20855;&#26377;&#19981;&#21516;&#31243;&#24230;&#30340;&#25512;&#29702;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#26356;&#26032;&#30693;&#35782;&#30340;&#26041;&#27861;&#65288;&#22522;&#20110;&#26799;&#24230;&#24494;&#35843;&#21644;&#27492;&#26041;&#27861;&#30340;&#20462;&#25913;&#65289;&#22312;&#27880;&#20837;&#30693;&#35782;&#30340;&#20256;&#25773;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#23398;&#20064;&#20010;&#21035;&#20107;&#23454;&#26041;&#38754;&#38750;&#24120;&#25104;&#21151;&#65292;&#20294;&#22312;&#20174;&#27880;&#20837;&#30693;&#35782;&#20013;&#36827;&#34892;&#25512;&#29702;&#26041;&#38754;&#21364;&#23384;&#22312;&#22256;&#38590;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#31361;&#26174;&#20102;&#27880;&#20837;&#30693;&#35782;&#20256;&#25773;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#24314;&#35758;&#38656;&#35201;&#26032;&#30340;&#25216;&#26415;&#26469;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21644;&#20351;&#29992;&#20851;&#20110;&#23454;&#20307;&#30340;&#26032;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models (LMs) are used for knowledge intensive tasks like question answering, but their knowledge gets continuously outdated as the world changes. Prior work has studied targeted updates to LMs, injecting individual facts and evaluating whether the model learns these facts while not changing predictions on other contexts. We take a step forward and study LMs' abilities to make inferences based on injected facts (or propagate those facts): for example, after learning that something is a TV show, does an LM predict that you can watch it? We study this with two cloze-style tasks: an existing dataset of real-world sentences about novel entities (ECBD) as well as a new controlled benchmark with manually designed templates requiring varying levels of inference about injected knowledge. Surprisingly, we find that existing methods for updating knowledge (gradient-based fine-tuning and modifications of this approach) show little propagation of injected knowledge. These metho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21387;&#32553;&#27169;&#22411;&#30340;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#25163;&#21160;&#26631;&#27880;&#26356;&#22810;&#30340;&#24494;&#35843;&#25968;&#25454;&#20197;&#30452;&#25509;&#35757;&#32451;&#21387;&#32553;&#27169;&#22411;&#30456;&#27604;&#65292;&#20174;T5-XXL&#33976;&#39311;&#21040;T5-Small&#20960;&#20046;&#24635;&#26159;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#12290;</title><link>http://arxiv.org/abs/2305.01645</link><description>&lt;p&gt;
&#21387;&#32553;&#27169;&#22411;&#30340;&#39640;&#25928;&#24494;&#35843;&#65306;&#33976;&#39311;&#36824;&#26159;&#26631;&#27880;&#65311;
&lt;/p&gt;
&lt;p&gt;
Distill or Annotate? Cost-Efficient Fine-Tuning of Compact Models. (arXiv:2305.01645v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01645
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21387;&#32553;&#27169;&#22411;&#30340;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#25163;&#21160;&#26631;&#27880;&#26356;&#22810;&#30340;&#24494;&#35843;&#25968;&#25454;&#20197;&#30452;&#25509;&#35757;&#32451;&#21387;&#32553;&#27169;&#22411;&#30456;&#27604;&#65292;&#20174;T5-XXL&#33976;&#39311;&#21040;T5-Small&#20960;&#20046;&#24635;&#26159;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#27169;&#22411;&#30340;&#24494;&#35843;&#34429;&#28982;&#25928;&#26524;&#26174;&#33879;&#65292;&#20294;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#25104;&#26412;&#39640;&#19988;&#20250;&#20135;&#29983;&#30899;&#25490;&#25918;&#12290;&#30693;&#35782;&#33976;&#39311;&#24050;&#34987;&#35777;&#26126;&#26159;&#20943;&#23569;&#25512;&#29702;&#25104;&#26412;&#30340;&#23454;&#29992;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#33976;&#39311;&#36807;&#31243;&#26412;&#36523;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#26368;&#26377;&#25928;&#22320;&#20351;&#29992;&#22266;&#23450;&#39044;&#31639;&#26500;&#24314;&#21387;&#32553;&#27169;&#22411;&#12290;&#22312;&#20845;&#20010;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;&#20174;T5-XXL&#65288;11B&#65289;&#33976;&#39311;&#21040;T5-Small&#65288;60M&#65289;&#20960;&#20046;&#24635;&#26159;&#27604;&#25163;&#21160;&#26631;&#27880;&#26356;&#22810;&#30340;&#24494;&#35843;&#25968;&#25454;&#20197;&#30452;&#25509;&#35757;&#32451;&#19968;&#20010;&#21387;&#32553;&#27169;&#22411;&#65288;T5-Small&#65288;60M&#65289;&#65289;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#26368;&#22823;&#21270;&#25928;&#29992;&#30340;&#26368;&#20339;&#33976;&#39311;&#37327;&#22240;&#20219;&#21153;&#32780;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning large models is highly effective, however, inference using these models can be expensive and produces carbon emissions. Knowledge distillation has been shown to be a practical solution to reduce inference costs, but the distillation process itself requires significant computational resources. Rather than buying or renting GPUs to fine-tune, then distill a large model, an NLP practitioner who needs a compact model might also choose to simply allocate an available budget to hire annotators and manually label additional fine-tuning data. In this paper, we investigate how to most efficiently use a fixed budget to build a compact model. Through our extensive experiments on six diverse NLP tasks, we find that distilling from T5-XXL (11B) to T5-Small (60M) leads to almost always a cost-efficient option compared to annotating more data to directly train a compact model (T5-Small (60M)). We further demonstrate that the optimal amount of distillation that maximizes utility varies acr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;NLP&#39046;&#22495;&#36807;&#21435;&#30340;&#20154;&#31867;&#35780;&#20272;&#30340;&#20877;&#29616;&#24615;&#38382;&#39064;&#65292;&#32467;&#26524;&#21457;&#29616;&#22823;&#37096;&#20998;&#20154;&#31867;&#35780;&#20272;&#37117;&#26080;&#27861;&#37325;&#22797;&#25110;&#20877;&#29616;&#65292;&#21487;&#33021;&#26159;&#30001;&#20110;&#32570;&#22833;&#20449;&#24687;&#12289;&#26080;&#22238;&#24212;&#20316;&#32773;&#21644;&#23454;&#39564;&#32570;&#38519;&#31561;&#21407;&#22240;&#23548;&#33268;&#12290;&#36825;&#20010;&#32467;&#26524;&#25552;&#31034;&#25105;&#20204;&#38656;&#35201;&#37325;&#26032;&#32771;&#34385;&#22914;&#20309;&#35774;&#35745;&#21644;&#25253;&#21578;&#20154;&#31867;&#35780;&#20272;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2305.01633</link><description>&lt;p&gt;
&#32570;&#22833;&#20449;&#24687;&#12289;&#26080;&#22238;&#24212;&#20316;&#32773;&#12289;&#23454;&#39564;&#32570;&#38519;&#65306;NLP&#20013;&#19981;&#21487;&#33021;&#35780;&#20272;&#20197;&#21069;&#30340;&#20154;&#31867;&#35780;&#20272;&#30340;&#20877;&#29616;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Missing Information, Unresponsive Authors, Experimental Flaws: The Impossibility of Assessing the Reproducibility of Previous Human Evaluations in NLP. (arXiv:2305.01633v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01633
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;NLP&#39046;&#22495;&#36807;&#21435;&#30340;&#20154;&#31867;&#35780;&#20272;&#30340;&#20877;&#29616;&#24615;&#38382;&#39064;&#65292;&#32467;&#26524;&#21457;&#29616;&#22823;&#37096;&#20998;&#20154;&#31867;&#35780;&#20272;&#37117;&#26080;&#27861;&#37325;&#22797;&#25110;&#20877;&#29616;&#65292;&#21487;&#33021;&#26159;&#30001;&#20110;&#32570;&#22833;&#20449;&#24687;&#12289;&#26080;&#22238;&#24212;&#20316;&#32773;&#21644;&#23454;&#39564;&#32570;&#38519;&#31561;&#21407;&#22240;&#23548;&#33268;&#12290;&#36825;&#20010;&#32467;&#26524;&#25552;&#31034;&#25105;&#20204;&#38656;&#35201;&#37325;&#26032;&#32771;&#34385;&#22914;&#20309;&#35774;&#35745;&#21644;&#25253;&#21578;&#20154;&#31867;&#35780;&#20272;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25253;&#21578;&#20102;&#25105;&#20204;&#22312;&#35782;&#21035;&#19968;&#32452;&#20808;&#21069;&#36866;&#21512;&#36827;&#34892;&#21327;&#35843;&#30740;&#31350;&#30340;NLP&#39046;&#22495;&#20154;&#31867;&#35780;&#20272;&#30340;&#21162;&#21147;&#65292;&#20197;&#32771;&#23519;&#26159;&#20160;&#20040;&#20351;&#24471;NLP&#39046;&#22495;&#30340;&#20154;&#31867;&#35780;&#20272;&#26356;/ less&#33021;&#20877;&#29616;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#21644;&#21457;&#29616;&#65292;&#20854;&#20013;&#21253;&#25324;&#20165;&#26377;13&#65285;&#30340;&#35770;&#25991;&#20855;&#26377;&#65288;i&#65289;&#36275;&#22815;&#20302;&#30340;&#20877;&#29616;&#38556;&#30861;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#36275;&#22815;&#30340;&#21487;&#33719;&#21462;&#20449;&#24687;&#65292;&#25165;&#21487;&#20197;&#34987;&#32771;&#34385;&#36827;&#34892;&#20877;&#29616;&#65292;&#24182;&#19988;&#25105;&#20204;&#36873;&#25321;&#36827;&#34892;&#20877;&#29616;&#30340;&#25152;&#26377;&#23454;&#39564;&#37117;&#34987;&#21457;&#29616;&#23384;&#22312;&#32570;&#38519;&#65292;&#36825;&#20351;&#24471;&#36827;&#34892;&#20877;&#29616;&#30340;&#26377;&#24847;&#20041;&#24615;&#20540;&#24471;&#24576;&#30097;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#19981;&#24471;&#19981;&#23558;&#25105;&#20204;&#30340;&#21327;&#35843;&#30740;&#31350;&#35774;&#35745;&#20174;&#20877;&#29616;&#26041;&#27861;&#26356;&#25913;&#20026;&#26631;&#20934;&#21270;-&#28982;&#21518;&#20877;&#29616;&#20004;&#27425;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24635;&#20307;&#32780;&#35328;&#65288;&#26159;&#36127;&#38754;&#30340;&#65289;&#21457;&#29616;&#65292;NLP&#39046;&#22495;&#20013;&#30340;&#32477;&#22823;&#22810;&#25968;&#20154;&#31867;&#35780;&#20272;&#37117;&#19981;&#33021;&#37325;&#22797;&#21644;/&#25110;&#19981;&#33021;&#20877;&#29616;&#21644;/&#25110;&#20855;&#26377;&#22826;&#22810;&#32570;&#38519;&#20197;&#35777;&#26126;&#20854;&#21487;&#20877;&#29616;&#24615;&#12290;&#36825;&#25551;&#32472;&#20102;&#19968;&#20010;&#21487;&#24597;&#30340;&#30011;&#38754;&#65292;&#20294;&#20063;&#20026;&#37325;&#26032;&#32771;&#34385;&#22914;&#20309;&#35774;&#35745;&#21644;&#25253;&#21578;NLP&#39046;&#22495;&#30340;&#20154;&#31867;&#35780;&#20272;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
We report our efforts in identifying a set of previous human evaluations in NLP that would be suitable for a coordinated study examining what makes human evaluations in NLP more/less reproducible. We present our results and findings, which include that just 13\% of papers had (i) sufficiently low barriers to reproduction, and (ii) enough obtainable information, to be considered for reproduction, and that all but one of the experiments we selected for reproduction was discovered to have flaws that made the meaningfulness of conducting a reproduction questionable. As a result, we had to change our coordinated study design from a reproduce approach to a standardise-then-reproduce-twice approach. Our overall (negative) finding that the great majority of human evaluations in NLP is not repeatable and/or not reproducible and/or too flawed to justify reproduction, paints a dire picture, but presents an opportunity for a rethink about how to design and report human evaluations in NLP.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#23618;&#20043;&#38388;&#30340;&#23545;&#27604;&#26469;&#25913;&#36827;&#25991;&#26412;&#29983;&#25104;&#36755;&#20986;&#65292;&#35299;&#20915;&#27169;&#22411;&#22312;&#24320;&#25918;&#24335;&#29983;&#25104;&#20013;&#30340;&#19981;&#33391;&#34892;&#20026;&#38382;&#39064;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#29983;&#25104;&#25991;&#26412;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.01628</link><description>&lt;p&gt;
&#22351;&#24314;&#35758;&#30340;&#22909;&#22788;&#65306;&#27169;&#22411;&#23618;&#38388;&#33258;&#21160;&#23545;&#29031;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
The Benefits of Bad Advice: Autocontrastive Decoding across Model Layers. (arXiv:2305.01628v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01628
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#23618;&#20043;&#38388;&#30340;&#23545;&#27604;&#26469;&#25913;&#36827;&#25991;&#26412;&#29983;&#25104;&#36755;&#20986;&#65292;&#35299;&#20915;&#27169;&#22411;&#22312;&#24320;&#25918;&#24335;&#29983;&#25104;&#20013;&#30340;&#19981;&#33391;&#34892;&#20026;&#38382;&#39064;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#29983;&#25104;&#25991;&#26412;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#65292;&#24212;&#29992;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#20381;&#36182;&#20110;&#26368;&#32456;&#27169;&#22411;&#23618;&#30340;&#34920;&#31034;&#65292;&#22240;&#20026;&#20551;&#35774;&#20013;&#38388;&#38544;&#34255;&#23618;&#30340;&#34920;&#31034;&#26159;&#19981;&#22826;&#26377;&#29992;&#30340;&#12290;&#26412;&#25991;&#35748;&#20026;&#30001;&#20110;&#27169;&#22411;&#23618;&#20043;&#38388;&#30340;&#28176;&#36827;&#25913;&#36827;&#65292;&#21487;&#20197;&#20174;&#26356;&#39640;&#23618;&#21644;&#26356;&#20302;&#23618;&#20043;&#38388;&#30340;&#23545;&#27604;&#20013;&#33719;&#21462;&#39069;&#22806;&#20449;&#24687;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;&#36873;&#25321;&#29983;&#25104;&#27169;&#22411;&#30340;&#19979;&#19968;&#20010;&#21487;&#33021;&#26631;&#35760;&#30340;&#39044;&#27979;&#26102;&#65292;&#21487;&#20197;&#20351;&#29992;&#36739;&#20302;&#23618;&#30340;&#39044;&#27979;&#26469;&#31361;&#20986;&#21738;&#20123;&#20505;&#36873;&#39033;&#26159;&#26368;&#22909;&#36991;&#20813;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#23618;&#20043;&#38388;&#30340;&#23545;&#27604;&#26469;&#25913;&#36827;&#25991;&#26412;&#29983;&#25104;&#36755;&#20986;&#65292;&#24182;&#34920;&#26126;&#23427;&#21487;&#20197;&#32531;&#35299;&#27169;&#22411;&#22312;&#24320;&#25918;&#24335;&#29983;&#25104;&#20013;&#30340;&#19981;&#33391;&#34892;&#20026;&#65292;&#26174;&#33879;&#25552;&#39640;&#29983;&#25104;&#30340;&#25991;&#26412;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25512;&#26029;&#26102;&#27604;&#36739;&#27169;&#22411;&#23618;&#20043;&#38388;&#21487;&#20197;&#23545;&#19968;&#20123;&#24635;&#20307;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#30340;&#26041;&#38754;&#20135;&#29983;&#23454;&#36136;&#24615;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Applying language models to natural language processing tasks typically relies on the representations in the final model layer, as intermediate hidden layer representations are presumed to be less informative. In this work, we argue that due to the gradual improvement across model layers, additional information can be gleaned from the contrast between higher and lower layers during inference. Specifically, in choosing between the probable next token predictions of a generative model, the predictions of lower layers can be used to highlight which candidates are best avoided. We propose a novel approach that utilizes the contrast between layers to improve text generation outputs, and show that it mitigates degenerative behaviors of the model in open-ended generation, significantly improving the quality of generated texts. Furthermore, our results indicate that contrasting between model layers at inference time can yield substantial benefits to certain aspects of general language model ca
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#38899;&#30340;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#30452;&#25509;&#20174;&#21407;&#22987;&#35821;&#38899;&#20013;&#24314;&#31435;&#22522;&#30784;&#35821;&#27861;&#27169;&#22411;&#12290;&#20316;&#32773;&#21457;&#29616;&#65292;&#22312;&#22522;&#20110;&#22768;&#38899;&#30340;&#21333;&#35789;&#35760;&#24405;&#19978;&#35757;&#32451;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#33258;&#21457;&#36830;&#25509;&#20004;&#20010;&#25110;&#19977;&#20010;&#21333;&#35789;&#65292;&#24182;&#19988;&#21487;&#20197;&#23398;&#20250;&#23558;&#21333;&#35789;&#23884;&#20837;&#21040;&#26032;&#30340;&#26410;&#35265;&#36807;&#30340;&#21333;&#35789;&#32452;&#21512;&#20013;&#65292;&#36825;&#26159;&#20043;&#21069;&#26410;&#25253;&#36947;&#30340;&#23646;&#24615;&#65292;&#36825;&#19968;&#21457;&#29616;&#23545;&#25105;&#20204;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#26041;&#24335;&#21644;&#24314;&#31435;&#20174;&#21407;&#22987;&#22768;&#23398;&#36755;&#20837;&#20013;&#30340;&#35821;&#27861;&#21450;&#20854;&#28436;&#21270;&#30340;&#27169;&#22411;&#37117;&#26377;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2305.01626</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#38899;&#30340;&#22522;&#30784;&#35821;&#27861;&#65306;&#33258;&#21457;&#32852;&#25509;&#30340;&#33258;&#30417;&#30563;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Basic syntax from speech: Spontaneous concatenation in unsupervised deep neural networks. (arXiv:2305.01626v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01626
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#38899;&#30340;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#30452;&#25509;&#20174;&#21407;&#22987;&#35821;&#38899;&#20013;&#24314;&#31435;&#22522;&#30784;&#35821;&#27861;&#27169;&#22411;&#12290;&#20316;&#32773;&#21457;&#29616;&#65292;&#22312;&#22522;&#20110;&#22768;&#38899;&#30340;&#21333;&#35789;&#35760;&#24405;&#19978;&#35757;&#32451;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#33258;&#21457;&#36830;&#25509;&#20004;&#20010;&#25110;&#19977;&#20010;&#21333;&#35789;&#65292;&#24182;&#19988;&#21487;&#20197;&#23398;&#20250;&#23558;&#21333;&#35789;&#23884;&#20837;&#21040;&#26032;&#30340;&#26410;&#35265;&#36807;&#30340;&#21333;&#35789;&#32452;&#21512;&#20013;&#65292;&#36825;&#26159;&#20043;&#21069;&#26410;&#25253;&#36947;&#30340;&#23646;&#24615;&#65292;&#36825;&#19968;&#21457;&#29616;&#23545;&#25105;&#20204;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#26041;&#24335;&#21644;&#24314;&#31435;&#20174;&#21407;&#22987;&#22768;&#23398;&#36755;&#20837;&#20013;&#30340;&#35821;&#27861;&#21450;&#20854;&#28436;&#21270;&#30340;&#27169;&#22411;&#37117;&#26377;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#27861;&#30340;&#35745;&#31639;&#27169;&#22411;&#20027;&#35201;&#22522;&#20110;&#25991;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#30452;&#25509;&#20174;&#21407;&#22987;&#35821;&#38899;&#20013;&#24314;&#31435;&#22522;&#30784;&#35821;&#27861;&#27169;&#22411;&#12290;&#25105;&#20204;&#37325;&#28857;&#30740;&#31350;&#20102;&#26368;&#26222;&#36941;&#21644;&#22522;&#26412;&#30340;&#35821;&#27861;&#29305;&#24615;&#20043;&#19968;&#8212;&#8212;&#32852;&#25509;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#33258;&#21457;&#32852;&#25509;&#29616;&#35937;&#65306;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#22312;&#20010;&#21035;&#21333;&#35789;&#30340;&#22768;&#23398;&#35760;&#24405;&#19978;&#35757;&#32451;&#26102;&#65292;&#24320;&#22987;&#20135;&#29983;&#36755;&#20986;&#65292;&#36825;&#20123;&#36755;&#20986;&#23558;&#20004;&#20010;&#29978;&#33267;&#19977;&#20010;&#21333;&#35789;&#36830;&#25509;&#22312;&#19968;&#36215;&#65292;&#32780;&#19981;&#20250;&#25509;&#35302;&#21040;&#20855;&#26377;&#22810;&#20010;&#21333;&#35789;&#30340;&#36755;&#20837;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#35757;&#32451;&#20004;&#20010;&#21333;&#35789;&#30340;&#32593;&#32476;&#21487;&#20197;&#23398;&#20064;&#23558;&#21333;&#35789;&#23884;&#20837;&#21040;&#26032;&#30340;&#26410;&#35265;&#36807;&#30340;&#21333;&#35789;&#32452;&#21512;&#20013;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#22312;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29615;&#22659;&#19979;&#35757;&#32451;&#30340;&#21407;&#22987;&#35821;&#38899;CNN&#20197;&#21069;&#26410;&#25253;&#36947;&#30340;&#23646;&#24615;&#65292;&#23427;&#19981;&#20165;&#23545;&#25105;&#20204;&#29702;&#35299;&#36825;&#20123;&#20307;&#31995;&#32467;&#26500;&#30340;&#23398;&#20064;&#26041;&#24335;&#26377;&#24433;&#21709;&#65292;&#36824;&#23545;&#24314;&#31435;&#20174;&#21407;&#22987;&#22768;&#23398;&#36755;&#20837;&#20013;&#30340;&#35821;&#27861;&#21450;&#20854;&#28436;&#21270;&#30340;&#27169;&#22411;&#26377;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computational models of syntax are predominantly text-based. Here we propose that basic syntax can be modeled directly from raw speech in a fully unsupervised way. We focus on one of the most ubiquitous and basic properties of syntax -- concatenation. We introduce spontaneous concatenation: a phenomenon where convolutional neural networks (CNNs) trained on acoustic recordings of individual words start generating outputs with two or even three words concatenated without ever accessing data with multiple words in the input. Additionally, networks trained on two words learn to embed words into novel unobserved word combinations. To our knowledge, this is a previously unreported property of CNNs trained on raw speech in the Generative Adversarial Network setting and has implications both for our understanding of how these architectures learn as well as for modeling syntax and its evolution from raw acoustic inputs.
&lt;/p&gt;</description></item><item><title>Unlimiformer&#26159;&#19968;&#31181;Transformer&#27169;&#22411;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#25152;&#26377;&#23618;&#30340;&#27880;&#24847;&#35745;&#31639;&#21368;&#36733;&#21040;&#21333;&#20010;k&#36817;&#37051;&#32034;&#24341;&#19978;&#65292;&#20174;&#32780;&#21487;&#22788;&#29702;&#26080;&#38480;&#38271;&#24230;&#30340;&#36755;&#20837;&#65292;&#32780;&#19981;&#22686;&#21152;&#39069;&#22806;&#30340;&#23398;&#20064;&#36127;&#25285;&#12290;</title><link>http://arxiv.org/abs/2305.01625</link><description>&lt;p&gt;
&#26080;&#38480;&#38271;&#24230;&#36755;&#20837;&#30340;&#38271;&#36317;&#31163;Transformer-Unlimiformer
&lt;/p&gt;
&lt;p&gt;
Unlimiformer: Long-Range Transformers with Unlimited Length Input. (arXiv:2305.01625v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01625
&lt;/p&gt;
&lt;p&gt;
Unlimiformer&#26159;&#19968;&#31181;Transformer&#27169;&#22411;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#25152;&#26377;&#23618;&#30340;&#27880;&#24847;&#35745;&#31639;&#21368;&#36733;&#21040;&#21333;&#20010;k&#36817;&#37051;&#32034;&#24341;&#19978;&#65292;&#20174;&#32780;&#21487;&#22788;&#29702;&#26080;&#38480;&#38271;&#24230;&#30340;&#36755;&#20837;&#65292;&#32780;&#19981;&#22686;&#21152;&#39069;&#22806;&#30340;&#23398;&#20064;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#36890;&#24120;&#23545;&#36755;&#20837;&#38271;&#24230;&#26377;&#39044;&#23450;&#20041;&#30340;&#38480;&#21046;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#33021;&#38656;&#35201;&#21442;&#32771;&#36755;&#20837;&#20013;&#30340;&#27599;&#20010;&#26631;&#35760;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;-Unlimiformer&#65292;&#21487;&#20197;&#21253;&#35013;&#20219;&#20309;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;Transformer&#65292;&#24182;&#23558;&#25152;&#26377;&#23618;&#30340;&#27880;&#24847;&#35745;&#31639;&#21368;&#36733;&#21040;&#21333;&#20010;k&#36817;&#37051;&#32034;&#24341;&#19978;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#38271;&#25991;&#26723;&#21644;&#22810;&#25991;&#26723;&#25688;&#35201;&#22522;&#20934;&#27979;&#35797;&#20013;&#35777;&#26126;&#20102;Unlimiformer&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#23427;&#21487;&#20197;&#24635;&#32467;350k&#20196;&#29260;&#38271;&#30340;&#36755;&#20837;&#32780;&#19981;&#36827;&#34892;&#27979;&#35797;&#26102;&#30340;&#25130;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based models typically have a predefined bound to their input length, because of their need to potentially attend to every token in the input. In this work, we propose Unlimiformer: a general approach that can wrap any existing pretrained encoder-decoder transformer, and offload the attention computation across all layers to a single $k$-nearest-neighbor index; this index can be kept on either the GPU or CPU memory and queried in sub-linear time. This way, we can index extremely long input sequences, while every attention head in every decoder layer retrieves its top-$k$ keys, instead of attending to every key. We demonstrate Unlimiformers's efficacy on several long-document and multi-document summarization benchmarks, showing that it can summarize even 350k token-long inputs from the BookSum dataset, without any input truncation at test time. Unlimiformer improves pretrained models such as BART and Longformer by extending them to unlimited inputs without additional learned
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UNTER&#30340;&#32479;&#19968;&#30693;&#35782;&#25509;&#21475;&#65292;&#21487;&#20197;&#21516;&#26102;&#21033;&#29992;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#20174;&#32780;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#24615;&#33021;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#19981;&#26029;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.01624</link><description>&lt;p&gt;
UNTER: &#19968;&#31181;&#29992;&#20110;&#22686;&#24378;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#32479;&#19968;&#30693;&#35782;&#25509;&#21475;
&lt;/p&gt;
&lt;p&gt;
UNTER: A Unified Knowledge Interface for Enhancing Pre-trained Language Models. (arXiv:2305.01624v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01624
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UNTER&#30340;&#32479;&#19968;&#30693;&#35782;&#25509;&#21475;&#65292;&#21487;&#20197;&#21516;&#26102;&#21033;&#29992;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#20174;&#32780;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#24615;&#33021;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#19981;&#26029;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22806;&#37096;&#30693;&#35782;&#27880;&#20837;&#21487;&#20197;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#21508;&#31181;&#19979;&#28216;NLP&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#20294;&#26159;&#65292;&#29616;&#26377;&#30340;&#30693;&#35782;&#27880;&#20837;&#26041;&#27861;&#36866;&#29992;&#20110;&#32467;&#26500;&#21270;&#30693;&#35782;&#25110;&#38750;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#32570;&#20047;&#32479;&#19968;&#30340;&#20351;&#29992;&#26041;&#24335;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UNTER&#30340;&#32479;&#19968;&#30693;&#35782;&#25509;&#21475;&#65292;&#20197;&#25552;&#20379;&#21033;&#29992;&#32467;&#26500;&#21270;&#30693;&#35782;&#21644;&#38750;&#32467;&#26500;&#21270;&#30693;&#35782;&#30340;&#32479;&#19968;&#35270;&#35282;&#12290;&#22312;UNTER&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#35299;&#30721;&#22120;&#20316;&#20026;&#32479;&#19968;&#30340;&#30693;&#35782;&#25509;&#21475;&#65292;&#23558;&#20174;&#32534;&#30721;&#22120;&#33719;&#21462;&#30340;&#36328;&#24230;&#34920;&#31034;&#19982;&#20854;&#23545;&#24212;&#30340;&#30693;&#35782;&#36827;&#34892;&#23545;&#40784;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#32534;&#30721;&#22120;&#33021;&#22815;&#20174;&#20854;&#21442;&#25968;&#20013;&#32479;&#19968;&#35843;&#29992;&#19979;&#28216;&#24212;&#29992;&#31243;&#24207;&#30340;&#36328;&#24230;&#30456;&#20851;&#30340;&#30693;&#35782;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#27880;&#20837;&#20004;&#31181;&#24418;&#24335;&#30340;&#30693;&#35782;&#65292;UNTER&#22312;&#19968;&#31995;&#21015;&#30693;&#35782;&#39537;&#21160;&#30340;NLP&#20219;&#21153;&#20013;&#33719;&#24471;&#20102;&#19981;&#26029;&#30340;&#25913;&#36827;&#65292;&#21253;&#25324;&#23454;&#20307;&#31867;&#22411;&#12289;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#20851;&#31995;&#25277;&#21462;&#65292;&#23588;&#20854;&#22312;&#20302;&#36164;&#28304;&#22330;&#26223;&#20013;&#25928;&#26524;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research demonstrates that external knowledge injection can advance pre-trained language models (PLMs) in a variety of downstream NLP tasks. However, existing knowledge injection methods are either applicable to structured knowledge or unstructured knowledge, lacking a unified usage. In this paper, we propose a UNified knowledge inTERface, UNTER, to provide a unified perspective to exploit both structured knowledge and unstructured knowledge. In UNTER, we adopt the decoder as a unified knowledge interface, aligning span representations obtained from the encoder with their corresponding knowledge. This approach enables the encoder to uniformly invoke span-related knowledge from its parameters for downstream applications. Experimental results show that, with both forms of knowledge injected, UNTER gains continuous improvements on a series of knowledge-driven NLP tasks, including entity typing, named entity recognition and relation extraction, especially in low-resource scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;ICASSP&#20449;&#21495;&#22788;&#29702;&#22823;&#25361;&#25112;2023&#20013;&#30340;&#21475;&#35821;&#35821;&#35328;&#29702;&#35299;&#22823;&#25361;&#25112;&#30340;&#36136;&#37327;&#36712;&#36857;&#65288;Track 1&#65289;&#25552;&#20986;&#30340;&#21475;&#35821;&#35821;&#20041;&#35299;&#26512;&#31995;&#32479;&#12290;&#20351;&#29992;&#20102;&#31471;&#21040;&#31471;&#21644;pipeline&#31995;&#32479;&#65292;&#24182;&#22312;SLU&#26694;&#26550;&#20013;&#20351;&#29992;&#20102;&#24378;&#22823;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#31561;&#26041;&#27861;&#65292;&#26368;&#32456;&#33719;&#24471;&#20102;80.8&#30340;&#31934;&#30830;&#21305;&#37197;&#24230;&#65292;&#33719;&#24471;&#20102;&#25361;&#25112;&#30340;&#31532;&#19968;&#21517;&#12290;</title><link>http://arxiv.org/abs/2305.01620</link><description>&lt;p&gt;
&#19968;&#39033;&#20851;&#20110;&#23558;Pipeline&#21644;E2E SLU&#31995;&#32479;&#25972;&#21512;&#29992;&#20110;&#21475;&#35821;&#35821;&#20041;&#35299;&#26512;&#30340;&#30740;&#31350;&#65306;STOP&#21697;&#36136;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
A Study on the Integration of Pipeline and E2E SLU systems for Spoken Semantic Parsing toward STOP Quality Challenge. (arXiv:2305.01620v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01620
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;ICASSP&#20449;&#21495;&#22788;&#29702;&#22823;&#25361;&#25112;2023&#20013;&#30340;&#21475;&#35821;&#35821;&#35328;&#29702;&#35299;&#22823;&#25361;&#25112;&#30340;&#36136;&#37327;&#36712;&#36857;&#65288;Track 1&#65289;&#25552;&#20986;&#30340;&#21475;&#35821;&#35821;&#20041;&#35299;&#26512;&#31995;&#32479;&#12290;&#20351;&#29992;&#20102;&#31471;&#21040;&#31471;&#21644;pipeline&#31995;&#32479;&#65292;&#24182;&#22312;SLU&#26694;&#26550;&#20013;&#20351;&#29992;&#20102;&#24378;&#22823;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#31561;&#26041;&#27861;&#65292;&#26368;&#32456;&#33719;&#24471;&#20102;80.8&#30340;&#31934;&#30830;&#21305;&#37197;&#24230;&#65292;&#33719;&#24471;&#20102;&#25361;&#25112;&#30340;&#31532;&#19968;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#26377;&#19968;&#20123;&#26032;&#30340;&#22522;&#20934;&#20219;&#21153;&#34987;&#24341;&#20837;&#29992;&#20110;&#21475;&#35821;&#29702;&#35299;&#65288;SLU&#65289;&#65292;&#20363;&#22914;&#35821;&#20041;&#35299;&#26512;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#38024;&#23545;ICASSP&#20449;&#21495;&#22788;&#29702;&#22823;&#25361;&#25112;2023&#20013;&#30340;&#21475;&#35821;&#35821;&#35328;&#29702;&#35299;&#22823;&#25361;&#25112;&#30340;&#36136;&#37327;&#36712;&#36857;&#65288;Track 1&#65289;&#25552;&#20986;&#30340;&#21475;&#35821;&#35821;&#20041;&#35299;&#26512;&#31995;&#32479;&#12290;&#25105;&#20204;&#23581;&#35797;&#20102;&#31471;&#21040;&#31471;&#21644;pipeline&#31995;&#32479;&#26469;&#23436;&#25104;&#36825;&#39033;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;SLU&#26694;&#26550;&#20013;&#20351;&#29992;&#20102;&#24378;&#22823;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#65292;&#22914;Whisper&#65292;&#20197;&#21450;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#65292;&#22914;BART&#65292;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#19981;&#21516;&#27169;&#22411;&#30340;&#36755;&#20986;&#32423;&#21035;&#32452;&#21512;&#65292;&#20197;&#33719;&#24471;80.8&#30340;&#31934;&#30830;&#21305;&#37197;&#24230;&#65292;&#36825;&#20351;&#25105;&#20204;&#22312;&#25361;&#25112;&#20013;&#33719;&#24471;&#20102;&#31532;&#19968;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently there have been efforts to introduce new benchmark tasks for spoken language understanding (SLU), like semantic parsing. In this paper, we describe our proposed spoken semantic parsing system for the quality track (Track 1) in Spoken Language Understanding Grand Challenge which is part of ICASSP Signal Processing Grand Challenge 2023. We experiment with both end-to-end and pipeline systems for this task. Strong automatic speech recognition (ASR) models like Whisper and pretrained Language models (LM) like BART are utilized inside our SLU framework to boost performance. We also investigate the output level combination of various models to get an exact match accuracy of 80.8, which won the 1st place at the challenge.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20813;&#35843;&#20248;&#31574;&#30053;&#26469;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#35821;&#35328;&#20449;&#21495;&#21644;&#25945;&#24072;&#20449;&#21495;&#12290;&#36890;&#36807;&#19982;&#22823;&#22411;&#27169;&#22411;&#30456;&#27604;&#65292;&#23454;&#39564;&#35777;&#26126; FreeLM &#27169;&#22411;&#22312;&#22810;&#31181;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.01616</link><description>&lt;p&gt;
FreeLM: &#20813;&#35843;&#20248;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FreeLM: Fine-Tuning-Free Language Model. (arXiv:2305.01616v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01616
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20813;&#35843;&#20248;&#31574;&#30053;&#26469;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#35821;&#35328;&#20449;&#21495;&#21644;&#25945;&#24072;&#20449;&#21495;&#12290;&#36890;&#36807;&#19982;&#22823;&#22411;&#27169;&#22411;&#30456;&#27604;&#65292;&#23454;&#39564;&#35777;&#26126; FreeLM &#27169;&#22411;&#22312;&#22810;&#31181;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411; (PLMs) &#22312; NLP &#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#20027;&#27969;&#30340;&#35299;&#20915;&#26041;&#26696;&#20173;&#28982;&#36981;&#24490;&#39044;&#35757;&#32451;&#21518;&#35843;&#20248;&#30340;&#33539;&#24335;&#65292;&#36825;&#26082;&#24102;&#26469;&#20102;&#39640;&#26114;&#30340;&#37096;&#32626;&#25104;&#26412;&#65292;&#20063;&#38477;&#20302;&#20102;&#35757;&#32451;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#35843;&#20248;&#29305;&#23450;&#20219;&#21153;&#26159;&#24517;&#35201;&#30340;&#65292;&#22240;&#20026; PLMs &#20165;&#22312;&#22823;&#22411;&#21407;&#22987;&#25968;&#25454;&#30340;&#35821;&#35328;&#20449;&#21495;&#19979;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#35843;&#20248;&#31574;&#30053;&#65292;&#21363;&#21516;&#26102;&#32771;&#34385;&#35821;&#35328;&#20449;&#21495;&#21644;&#25945;&#24072;&#20449;&#21495;&#12290;&#25945;&#24072;&#20449;&#21495;&#26159;&#19979;&#28216;&#20219;&#21153;&#30340;&#19968;&#20010;&#25277;&#35937;&#34920;&#31034;&#65292;&#20197;&#32479;&#19968;&#21629;&#39064;&#26684;&#24335;&#25552;&#20379;&#12290;&#25105;&#20204;&#30340; FreeLM &#27169;&#22411;&#22312;&#20132;&#20114;&#24335;&#22320;&#20351;&#29992;&#35821;&#35328;&#20449;&#21495;&#21644;&#24378;&#20219;&#21153;&#24863;&#30693;&#30340;&#25945;&#24072;&#20449;&#21495;&#36827;&#34892;&#35757;&#32451;&#21518;&#34920;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#27867;&#21270;&#21644;&#40065;&#26834;&#24615;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;FreeLM &#22312;&#22810;&#31181;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#19978;&#20248;&#20110;&#22823;&#22411;&#27169;&#22411;&#65292;&#22914; GPT-3 &#21644; InstructGPT&#12290;&#19982;&#36825;&#20123;&#27169;&#22411;&#30340; 175B &#21442;&#25968;&#30456;&#27604;&#65292;FreeLM &#26356;&#23567;&#65292;&#21482;&#26377; 0.3B &#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models (PLMs) have achieved remarkable success in NLP tasks. Despite the great success, mainstream solutions largely follow the pre-training then finetuning paradigm, which brings in both high deployment costs and low training efficiency. Nevertheless, fine-tuning on a specific task is essential because PLMs are only pre-trained with language signal from large raw data. In this paper, we propose a novel fine-tuning-free strategy for language models, to consider both language signal and teacher signal. Teacher signal is an abstraction of a battery of downstream tasks, provided in a unified proposition format. Trained with both language and strong task-aware teacher signals in an interactive manner, our FreeLM model demonstrates strong generalization and robustness. FreeLM outperforms large models e.g., GPT-3 and InstructGPT, on a range of language understanding tasks in experiments. FreeLM is much smaller with 0.3B parameters, compared to 175B in these models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29616;&#26377;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#20551;&#35774;&#25152;&#26377;&#26816;&#32034;&#20449;&#24687;&#37117;&#26159;&#27491;&#30830;&#30340;&#20551;&#35774;&#30340;&#38382;&#39064;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#33021;&#23384;&#22312;&#34394;&#20551;&#20449;&#24687;&#23548;&#33268;&#20914;&#31361;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#37492;&#21035;&#22120;&#21644;&#25552;&#31034;&#37492;&#21035;&#33021;&#21147;&#24341;&#20986;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#36825;&#26174;&#33879;&#25913;&#21892;&#20102;&#27169;&#22411;&#22312;&#30693;&#35782;&#20914;&#31361;&#19979;&#30340;&#25928;&#26524;&#65307;&#21516;&#26102;&#25552;&#20379;&#20102;&#20851;&#20110;&#20132;&#26367;&#31934;&#32454;&#35843;&#25972;&#27169;&#22411;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26032;&#30340;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2305.01579</link><description>&lt;p&gt;
&#21306;&#20998;&#21644;&#22238;&#31572;&#65306;&#36890;&#36807;&#36776;&#21035;&#22120;&#32531;&#35299;&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#20013;&#34394;&#20551;&#20449;&#24687;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Discern and Answer: Mitigating the Impact of Misinformation in Retrieval-Augmented Models with Discriminators. (arXiv:2305.01579v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29616;&#26377;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#20551;&#35774;&#25152;&#26377;&#26816;&#32034;&#20449;&#24687;&#37117;&#26159;&#27491;&#30830;&#30340;&#20551;&#35774;&#30340;&#38382;&#39064;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#33021;&#23384;&#22312;&#34394;&#20551;&#20449;&#24687;&#23548;&#33268;&#20914;&#31361;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#37492;&#21035;&#22120;&#21644;&#25552;&#31034;&#37492;&#21035;&#33021;&#21147;&#24341;&#20986;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#36825;&#26174;&#33879;&#25913;&#21892;&#20102;&#27169;&#22411;&#22312;&#30693;&#35782;&#20914;&#31361;&#19979;&#30340;&#25928;&#26524;&#65307;&#21516;&#26102;&#25552;&#20379;&#20102;&#20851;&#20110;&#20132;&#26367;&#31934;&#32454;&#35843;&#25972;&#27169;&#22411;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26032;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#20551;&#23450;&#25152;&#26377;&#26816;&#32034;&#21040;&#30340;&#20449;&#24687;&#37117;&#26159;&#20107;&#23454;&#19978;&#27491;&#30830;&#30340;&#12290;&#26412;&#25991;&#30740;&#31350;&#19968;&#20010;&#26356;&#21152;&#29616;&#23454;&#30340;&#22330;&#26223;&#65292;&#21363;&#26816;&#32034;&#21040;&#30340;&#25991;&#26723;&#21487;&#33021;&#21253;&#21547;&#34394;&#20551;&#20449;&#24687;&#65292;&#20174;&#32780;&#23548;&#33268;&#23427;&#20204;&#20043;&#38388;&#23384;&#22312;&#20914;&#31361;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#29616;&#26377;&#27169;&#22411;&#22312;&#31934;&#35843;&#21644;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#23398;&#20064;&#35774;&#32622;&#20013;&#23545;&#36825;&#31181;&#20449;&#24687;&#39640;&#24230;&#33030;&#24369;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#65292;&#36890;&#36807;&#26126;&#30830;&#22320;&#23545;&#37492;&#21035;&#22120;&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#25110;&#25552;&#31034;&#26469;&#24341;&#20986;GPT-3&#30340;&#37492;&#21035;&#33021;&#21147;&#65292;&#20351;&#26816;&#32034;&#22686;&#24378;LM&#23545;&#34394;&#20551;&#20449;&#24687;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#22312;&#24320;&#25918;&#22495;&#38382;&#31572;&#26041;&#38754;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#26174;&#33879;&#25913;&#21892;&#20102;LM&#23545;&#30693;&#35782;&#20914;&#31361;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20851;&#20110;&#20132;&#26367;&#31934;&#32454;&#35843;&#25972;&#27169;&#22411;&#30340;&#20915;&#31574;&#19982;&#19978;&#19979;&#25991;&#23398;&#20064;&#36807;&#31243;&#30340;&#21457;&#29616;&#65292;&#20026;&#21033;&#29992;&#20004;&#32773;&#30340;&#26368;&#20339;&#26041;&#24335;&#38138;&#24179;&#20102;&#26032;&#30340;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most existing retrieval-augmented language models (LMs) for question answering assume all retrieved information is factually correct. In this work, we study a more realistic scenario in which retrieved documents may contain misinformation, causing conflicts among them. We observe that the existing models are highly brittle to such information in both fine-tuning and in-context few-shot learning settings. We propose approaches to make retrieval-augmented LMs robust to misinformation by explicitly fine-tuning a discriminator or prompting to elicit discrimination capability in GPT-3. Our empirical results on open-domain question answering show that these approaches significantly improve LMs' robustness to knowledge conflicts. We also provide our findings on interleaving the fine-tuned model's decision with the in-context learning process, paving a new path to leverage the best of both worlds.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26412;&#20307;&#23545;&#21644;&#19977;&#20803;&#32032;&#20869;&#37096;&#20132;&#20114;&#20851;&#31995;&#30340;&#26032;&#22411;&#36890;&#29992;EA&#26694;&#26550;&#65288;OTIEA&#65289;&#65292;&#33021;&#22815;&#35299;&#20915;&#30446;&#21069;EA&#26041;&#27861;&#20013;&#27169;&#25311;&#19977;&#20803;&#32032;&#20869;&#37096;&#20132;&#20114;&#21644;&#32771;&#34385;&#21487;&#25193;&#23637;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.01561</link><description>&lt;p&gt;
OTIEA: &#22522;&#20110;&#26412;&#20307;&#22686;&#24378;&#30340;&#19977;&#20803;&#20869;&#37096;&#30456;&#20851;&#24615;&#29992;&#20110;&#36328;&#35821;&#35328;&#23454;&#20307;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
OTIEA:Ontology-enhanced Triple Intrinsic-Correlation for Cross-lingual Entity Alignment. (arXiv:2305.01561v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26412;&#20307;&#23545;&#21644;&#19977;&#20803;&#32032;&#20869;&#37096;&#20132;&#20114;&#20851;&#31995;&#30340;&#26032;&#22411;&#36890;&#29992;EA&#26694;&#26550;&#65288;OTIEA&#65289;&#65292;&#33021;&#22815;&#35299;&#20915;&#30446;&#21069;EA&#26041;&#27861;&#20013;&#27169;&#25311;&#19977;&#20803;&#32032;&#20869;&#37096;&#20132;&#20114;&#21644;&#32771;&#34385;&#21487;&#25193;&#23637;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27809;&#26377;&#36275;&#22815;&#22806;&#37096;&#36164;&#28304;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#36328;&#35821;&#35328;&#21644;&#36328;&#39046;&#22495;&#30693;&#35782;&#23545;&#40784;&#26159;&#34701;&#21512;&#19981;&#35268;&#21017;&#25968;&#25454;&#30340;&#22522;&#26412;&#21644;&#20851;&#38190;&#20219;&#21153;&#12290;&#23454;&#20307;&#23545;&#40784;&#65288;EA&#65289;&#20316;&#20026;&#36880;&#20803;&#32032;&#34701;&#21512;&#36807;&#31243;&#26088;&#22312;&#20174;&#19981;&#21516;&#30340;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#20013;&#21457;&#29616;&#31561;&#20215;&#30340;&#23545;&#35937;&#65292;&#36817;&#24180;&#26469;&#22312;&#34892;&#19994;&#21644;&#23398;&#26415;&#30028;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20852;&#36259;&#12290;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;EA&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#37051;&#33410;&#28857;&#12289;&#32467;&#26500;&#20449;&#24687;&#21644;&#22806;&#37096;&#36164;&#28304;&#26469;&#25506;&#32034;&#23454;&#20307;&#21644;&#20851;&#31995;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20013;&#24456;&#23569;&#27169;&#25311;&#19977;&#20803;&#32032;&#20043;&#38388;&#30340;&#22797;&#26434;&#20869;&#22312;&#20132;&#20114;&#21644;&#35282;&#33394;&#20449;&#24687;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#19977;&#20803;&#32032;&#30340;&#19981;&#20805;&#20998;&#35828;&#26126;&#12290;&#27492;&#22806;&#65292;&#22312;&#26576;&#20123;&#22330;&#26223;&#20013;&#65292;&#22806;&#37096;&#36164;&#28304;&#36890;&#24120;&#19981;&#21487;&#29992;&#65292;&#29305;&#21035;&#26159;&#36328;&#35821;&#35328;&#21644;&#36328;&#39046;&#22495;&#24212;&#29992;&#65292;&#36825;&#21453;&#26144;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#19981;&#36275;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26412;&#20307;&#23545;&#21644;&#19977;&#20803;&#32032;&#20869;&#37096;&#20132;&#20114;&#20851;&#31995;&#30340;&#26032;&#22411;&#36890;&#29992;EA&#26694;&#26550;&#65288;OTIEA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-lingual and cross-domain knowledge alignment without sufficient external resources is a fundamental and crucial task for fusing irregular data. As the element-wise fusion process aiming to discover equivalent objects from different knowledge graphs (KGs), entity alignment (EA) has been attracting great interest from industry and academic research recent years. Most of existing EA methods usually explore the correlation between entities and relations through neighbor nodes, structural information and external resources. However, the complex intrinsic interactions among triple elements and role information are rarely modeled in these methods, which may lead to the inadequate illustration for triple. In addition, external resources are usually unavailable in some scenarios especially cross-lingual and cross-domain applications, which reflects the little scalability of these methods. To tackle the above insufficiency, a novel universal EA framework (OTIEA) based on ontology pair and 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36328;&#35821;&#35328;&#23454;&#20307;&#23545;&#40784;&#26694;&#26550;TTEA&#65292;&#35813;&#26694;&#26550;&#32771;&#34385;&#20102;&#19977;&#20803;&#32452;&#29305;&#24449;&#21644;&#23454;&#20307;&#35282;&#33394;&#22810;&#26679;&#24615;&#65292;&#21033;&#29992;&#19977;&#20803;&#32452;&#24863;&#30693;&#27880;&#24847;&#21147;&#26500;&#24314;&#22686;&#24378;&#22411;&#19977;&#20803;&#32452;&#38598;&#25104;&#34920;&#31034;&#65292;&#36890;&#36807;&#20855;&#20307;&#24615;&#24863;&#30693;&#25511;&#21046;&#20102;&#20449;&#24687;&#20256;&#25773;&#36807;&#31243;&#20013;&#30340;&#22122;&#22768;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.01556</link><description>&lt;p&gt;
&#20511;&#21161;&#19977;&#20803;&#32452;&#24863;&#30693;&#27880;&#24847;&#21147;&#30340;&#22686;&#24378;&#22411;&#19977;&#20803;&#32452;&#38598;&#25104;&#34920;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#36328;&#35821;&#35328;&#23454;&#20307;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Type-enhanced Ensemble Triple Representation via Triple-aware Attention for Cross-lingual Entity Alignment. (arXiv:2305.01556v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01556
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36328;&#35821;&#35328;&#23454;&#20307;&#23545;&#40784;&#26694;&#26550;TTEA&#65292;&#35813;&#26694;&#26550;&#32771;&#34385;&#20102;&#19977;&#20803;&#32452;&#29305;&#24449;&#21644;&#23454;&#20307;&#35282;&#33394;&#22810;&#26679;&#24615;&#65292;&#21033;&#29992;&#19977;&#20803;&#32452;&#24863;&#30693;&#27880;&#24847;&#21147;&#26500;&#24314;&#22686;&#24378;&#22411;&#19977;&#20803;&#32452;&#38598;&#25104;&#34920;&#31034;&#65292;&#36890;&#36807;&#20855;&#20307;&#24615;&#24863;&#30693;&#25511;&#21046;&#20102;&#20449;&#24687;&#20256;&#25773;&#36807;&#31243;&#20013;&#30340;&#22122;&#22768;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#23545;&#40784;&#26159;&#23558;&#36328;&#35821;&#35328;&#21644;&#36328;&#39046;&#22495;&#30340;&#30693;&#35782;&#22270;&#35889;&#20013;&#25351;&#21521;&#21516;&#19968;&#23454;&#38469;&#23545;&#35937;&#30340;&#23454;&#20307;&#36827;&#34892;&#21457;&#29616;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#30446;&#21069;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#22522;&#20110;&#23884;&#20837;&#30340;&#26041;&#27861;&#25366;&#25496;&#19977;&#20803;&#32452;&#20803;&#32032;&#30340;&#30456;&#20851;&#24615;&#26469;&#29983;&#25104;&#23545;&#40784;&#23454;&#20307;&#34920;&#31034;&#65292;&#24456;&#23569;&#20851;&#27880;&#19977;&#20803;&#32452;&#30340;&#19981;&#21487;&#20998;&#24615;&#21644;&#23454;&#20307;&#35282;&#33394;&#30340;&#22810;&#26679;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;TTEA&#65288;&#31867;&#22411;&#22686;&#24378;&#30340;&#19977;&#20803;&#32452;&#38598;&#25104;&#34920;&#31034;&#65289;&#65292;&#36890;&#36807;&#32771;&#34385;&#38598;&#25104;&#19977;&#20803;&#32452;&#30340;&#20855;&#20307;&#24615;&#21644;&#23454;&#20307;&#35282;&#33394;&#29305;&#24449;&#26469;&#20811;&#26381;&#19978;&#36848;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#23558;&#20851;&#31995;&#20316;&#20026;&#20449;&#24687;&#36733;&#20307;&#22312;&#35821;&#20041;&#31354;&#38388;&#21644;&#31867;&#22411;&#31354;&#38388;&#20043;&#38388;&#36827;&#34892;&#36716;&#25442;&#65292;&#20174;&#32780;&#21487;&#20197;&#36890;&#36807;&#20855;&#20307;&#24615;&#24863;&#30693;&#30340;&#19977;&#20803;&#32452;&#27880;&#24847;&#21147;&#24179;&#31283;&#22320;&#25511;&#21046;&#31354;&#38388;&#36716;&#25442;&#21644;&#20449;&#24687;&#20256;&#25773;&#36807;&#31243;&#20013;&#30340;&#22122;&#22768;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#29992;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Entity alignment(EA) is a crucial task for integrating cross-lingual and cross-domain knowledge graphs(KGs), which aims to discover entities referring to the same real-world object from different KGs. Most existing methods generate aligning entity representation by mining the relevance of triple elements via embedding-based methods, paying little attention to triple indivisibility and entity role diversity. In this paper, a novel framework named TTEA -- Type-enhanced Ensemble Triple Representation via Triple-aware Attention for Cross-lingual Entity Alignment is proposed to overcome the above issues considering ensemble triple specificity and entity role features. Specifically, the ensemble triple representation is derived by regarding relation as information carrier between semantic space and type space, and hence the noise influence during spatial transformation and information propagation can be smoothly controlled via specificity-aware triple attention. Moreover, our framework uses 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;GPT-3.5&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#20013;&#65292;&#23454;&#29616;&#22312;&#22235;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#26032;&#30340;&#26368;&#20248;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#25351;&#23548;&#35828;&#26126;&#21644;&#32422;&#26463;&#27169;&#24335;&#19979;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.01555</link><description>&lt;p&gt;
&#22914;&#20309;&#21457;&#25381;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#20013;&#30340;&#33021;&#21147;&#65311;
&lt;/p&gt;
&lt;p&gt;
How to Unleash the Power of Large Language Models for Few-shot Relation Extraction?. (arXiv:2305.01555v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01555
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;GPT-3.5&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#20013;&#65292;&#23454;&#29616;&#22312;&#22235;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#26032;&#30340;&#26368;&#20248;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#25351;&#23548;&#35828;&#26126;&#21644;&#32422;&#26463;&#27169;&#24335;&#19979;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#25193;&#23637;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#24191;&#27867;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#20294;&#26159;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#36824;&#27809;&#26377;&#24471;&#21040;&#20840;&#38754;&#25506;&#32034;&#12290;&#26412;&#25991;&#36890;&#36807;&#35814;&#32454;&#23454;&#39564;&#65292;&#30740;&#31350;&#20102;&#20351;&#29992;GPT-3.5&#36827;&#34892;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#30340;&#22522;&#26412;&#26041;&#27861;&#8212;&#8212;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#25968;&#25454;&#29983;&#25104;&#12290;&#20026;&#20102;&#22686;&#24378;&#23569;&#26679;&#26412;&#24615;&#33021;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#25351;&#23548;&#35828;&#26126;&#21644;&#32422;&#26463;&#27169;&#24335;&#19979;&#30340;&#25968;&#25454;&#29983;&#25104;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#23454;&#29616;&#19982;&#20197;&#21069;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#32780;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#29983;&#25104;&#21487;&#20197;&#25512;&#21160;&#20197;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#20197;&#22312;&#22235;&#20010;&#24191;&#27867;&#30740;&#31350;&#30340;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#23569;&#26679;&#26412;&#32467;&#26524;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#24037;&#20316;&#21487;&#20197;&#28608;&#21457;&#26410;&#26469;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#20013;&#30340;&#33021;&#21147;&#30340;&#30740;&#31350;&#12290;&#20195;&#30721;&#21487;&#20197;&#22312; \url{https://github.com/zjunlp/DeepKE/tree/main/example/llm} &#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scaling language models have revolutionized widespread NLP tasks, yet little comprehensively explored few-shot relation extraction with large language models. In this paper, we investigate principal methodologies, in-context learning and data generation, for few-shot relation extraction via GPT-3.5 through exhaustive experiments. To enhance few-shot performance, we further propose task-related instructions and schema-constrained data generation. We observe that in-context learning can achieve performance on par with previous prompt learning approaches, and data generation with the large language model can boost previous solutions to obtain new state-of-the-art few-shot results on four widely-studied relation extraction datasets. We hope our work can inspire future research for the capabilities of large language models in few-shot relation extraction. Code is available in \url{https://github.com/zjunlp/DeepKE/tree/main/example/llm.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#32531;&#35299;&#35821;&#35328;&#27169;&#22411;&#36817;&#20284;&#35760;&#24518;&#21270;&#38382;&#39064;&#30340;&#26032;&#26694;&#26550;&#65292;&#20351;&#29992;&#36127;&#30456;&#20284;&#24230;&#35780;&#20998;&#20316;&#20026;&#22870;&#21169;&#20449;&#21495;&#26469;&#23398;&#20064;&#24046;&#24322;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#26174;&#24335;&#21644;&#38544;&#24335;&#20551;&#35774;&#25152;&#23548;&#33268;&#30340;&#25968;&#25454;&#32467;&#26500;&#19981;&#23436;&#20840;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.01550</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#30340;&#24046;&#24322;&#31574;&#30053;&#32531;&#35299;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36817;&#20284;&#35760;&#24518;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Mitigating Approximate Memorization in Language Models via Dissimilarity Learned Policy. (arXiv:2305.01550v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#32531;&#35299;&#35821;&#35328;&#27169;&#22411;&#36817;&#20284;&#35760;&#24518;&#21270;&#38382;&#39064;&#30340;&#26032;&#26694;&#26550;&#65292;&#20351;&#29992;&#36127;&#30456;&#20284;&#24230;&#35780;&#20998;&#20316;&#20026;&#22870;&#21169;&#20449;&#21495;&#26469;&#23398;&#20064;&#24046;&#24322;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#26174;&#24335;&#21644;&#38544;&#24335;&#20551;&#35774;&#25152;&#23548;&#33268;&#30340;&#25968;&#25454;&#32467;&#26500;&#19981;&#23436;&#20840;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#36890;&#36807;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#20854;&#20013;&#21487;&#33021;&#21253;&#21547;&#20250;&#21361;&#23475;&#20010;&#20154;&#38544;&#31169;&#30340;&#25935;&#24863;&#20449;&#24687;&#12290;LLMs&#34987;&#35777;&#26126;&#20250;&#35760;&#24518;&#35757;&#32451;&#25968;&#25454;&#30340;&#37096;&#20998;&#20869;&#23481;&#24182;&#22312;&#36935;&#21040;&#23545;&#24212;&#25552;&#31034;&#26102;&#30452;&#25509;&#36755;&#20986;&#35813;&#25968;&#25454;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#24046;&#20998;&#38544;&#31169;&#25216;&#26415;&#19978;&#20197;&#35299;&#20915;&#35760;&#24518;&#21270;&#38382;&#39064;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#37117;&#20381;&#36182;&#20110;&#23545;&#35201;&#20445;&#25252;&#25968;&#25454;&#32467;&#26500;&#30340;&#26174;&#24335;&#21644;&#38544;&#24335;&#20551;&#35774;&#65292;&#23548;&#33268;&#38382;&#39064;&#35299;&#20915;&#19981;&#23436;&#20840;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65288;PPO&#65289;&#24494;&#35843;LLMs&#20197;&#32531;&#35299;&#36817;&#20284;&#35760;&#24518;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#36127;&#30456;&#20284;&#24230;&#35780;&#20998;&#65288;&#20363;&#22914;BERTScore&#25110;SacreBLEU&#65289;&#20316;&#20026;&#22870;&#21169;&#20449;&#21495;&#26469;&#23398;&#20064;&#24046;&#24322;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26694;&#26550;&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#36817;&#20284;&#35760;&#24518;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language models (LLMs) are trained on large amounts of data, which can include sensitive information that may compromise per- sonal privacy. LLMs showed to memorize parts of the training data and emit those data verbatim when an adversary prompts appropriately. Previous research has primarily focused on data preprocessing and differential privacy techniques to address memorization or prevent verbatim memorization exclusively, which can give a false sense of privacy. However, these methods rely on explicit and implicit assumptions about the structure of the data to be protected, which often results in an incomplete solution to the problem. To address this, we propose a novel framework that utilizes a reinforcement learning approach (PPO) to fine-tune LLMs to mitigate approximate memorization. Our approach utilizes a negative similarity score, such as BERTScore or SacreBLEU, as a reward signal to learn a dissimilarity policy. Our results demonstrate that this framework effectively 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20221;&#21253;&#21547;&#30495;&#23454;&#28216;&#25103;&#29366;&#24577;&#20449;&#24687;&#30340;Dungeons &amp; Dragons&#23454;&#38469;&#28216;&#25103;&#25968;&#25454;&#38598;FIREBALL&#65292;&#23427;&#21487;&#20197;&#25913;&#21892;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;LLMs&#21487;&#20197;&#20351;&#29992;FIREBALL&#20013;&#30340;&#28216;&#25103;&#29366;&#24577;&#20449;&#24687;&#26469;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#28216;&#25103;&#22238;&#21512;&#12290;</title><link>http://arxiv.org/abs/2305.01528</link><description>&lt;p&gt;
FIREBALL&#65306;&#19968;&#20221;&#21253;&#21547;&#32467;&#26500;&#21270;&#28216;&#25103;&#29366;&#24577;&#20449;&#24687;&#30340;Dungeons &amp; Dragons&#23454;&#38469;&#28216;&#25103;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
FIREBALL: A Dataset of Dungeons and Dragons Actual-Play with Structured Game State Information. (arXiv:2305.01528v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01528
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20221;&#21253;&#21547;&#30495;&#23454;&#28216;&#25103;&#29366;&#24577;&#20449;&#24687;&#30340;Dungeons &amp; Dragons&#23454;&#38469;&#28216;&#25103;&#25968;&#25454;&#38598;FIREBALL&#65292;&#23427;&#21487;&#20197;&#25913;&#21892;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;LLMs&#21487;&#20197;&#20351;&#29992;FIREBALL&#20013;&#30340;&#28216;&#25103;&#29366;&#24577;&#20449;&#24687;&#26469;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#28216;&#25103;&#22238;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Dungeons &amp; Dragons&#65288;D&#65286;D&#65289;&#26159;&#19968;&#27454;&#26700;&#38754;&#35282;&#33394;&#25198;&#28436;&#28216;&#25103;&#65292;&#20854;&#29609;&#23478;&#20043;&#38388;&#23384;&#22312;&#22797;&#26434;&#30340;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#21644;&#38544;&#34255;&#30340;&#29366;&#24577;&#20449;&#24687;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25317;&#26377;&#29366;&#24577;&#20449;&#24687;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#28216;&#25103;&#22238;&#21512;&#27604;&#20165;&#20351;&#29992;&#23545;&#35805;&#21382;&#21490;&#30340;LLMs&#26356;&#20855;&#39640;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#20351;&#29992;&#30340;&#28216;&#25103;&#29366;&#24577;&#20449;&#24687;&#26159;&#21551;&#21457;&#24335;&#21019;&#24314;&#30340;&#65292;&#24182;&#19981;&#26159;&#30495;&#27491;&#30340;&#40644;&#37329;&#26631;&#20934;&#28216;&#25103;&#29366;&#24577;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;FIREBALL&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#30495;&#23454;&#28216;&#25103;&#29366;&#24577;&#20449;&#24687;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#26469;&#33258;Discord&#30340;&#36817;25,000&#20010;&#30495;&#23454;D&#65286;D&#28216;&#25103;&#20250;&#35805;&#12290;&#25105;&#20204;&#35760;&#24405;&#20102;&#20351;&#29992;Avrae&#26426;&#22120;&#20154;&#30340;&#29609;&#23478;&#30340;&#28216;&#25103;&#20250;&#35805;&#65292;&#35813;&#26426;&#22120;&#20154;&#26159;&#20026;&#20102;&#24110;&#21161;&#20154;&#20204;&#22312;&#32447;&#29609;D&#65286;D&#32780;&#24320;&#21457;&#30340;&#65292;&#24182;&#25429;&#33719;&#20102;&#35821;&#35328;&#12289;&#28216;&#25103;&#21629;&#20196;&#21644;&#22522;&#30784;&#28216;&#25103;&#29366;&#24577;&#20449;&#24687;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;Avrae&#29366;&#24577;&#20449;&#24687;&#65292;FIREBALL&#21487;&#20197;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#65292;&#20174;&#32780;&#25552;&#39640;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#21644;&#20154;&#31867;&#30340;&#36136;&#37327;&#35780;&#21028;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;LLMs&#21487;&#20197;&#29983;&#25104;&#8230;
&lt;/p&gt;
&lt;p&gt;
Dungeons &amp; Dragons (D&amp;D) is a tabletop roleplaying game with complex natural language interactions between players and hidden state information. Recent work has shown that large language models (LLMs) that have access to state information can generate higher quality game turns than LLMs that use dialog history alone. However, previous work used game state information that was heuristically created and was not a true gold standard game state. We present FIREBALL, a large dataset containing nearly 25,000 unique sessions from real D\&amp;D gameplay on Discord with true game state info. We recorded game play sessions of players who used the Avrae bot, which was developed to aid people in playing D&amp;D online, capturing language, game commands and underlying game state information. We demonstrate that FIREBALL can improve natural language generation (NLG) by using Avrae state information, improving both automated metrics and human judgments of quality. Additionally, we show that LLMs can generate
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20221;&#26368;&#22823;&#30340;&#20013;&#21307;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#29616;&#26377;&#27169;&#22411;&#30340;&#34920;&#29616;&#36828;&#20302;&#20110;&#39044;&#26399;&#12290;&#27492;&#25968;&#25454;&#38598;&#21487;&#20197;&#29992;&#20110;&#20854;&#20182;QA&#25968;&#25454;&#38598;&#30340;&#38646;-shot&#23398;&#20064;&#24182;&#29992;&#20316;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#30340;&#22806;&#37096;&#30693;&#35782;&#65292;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.01526</link><description>&lt;p&gt;
Huatuo-26M&#65306;&#19968;&#20221;&#22823;&#35268;&#27169;&#30340;&#20013;&#21307;&#38382;&#31572;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Huatuo-26M, a Large-scale Chinese Medical QA Dataset. (arXiv:2305.01526v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01526
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20221;&#26368;&#22823;&#30340;&#20013;&#21307;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#29616;&#26377;&#27169;&#22411;&#30340;&#34920;&#29616;&#36828;&#20302;&#20110;&#39044;&#26399;&#12290;&#27492;&#25968;&#25454;&#38598;&#21487;&#20197;&#29992;&#20110;&#20854;&#20182;QA&#25968;&#25454;&#38598;&#30340;&#38646;-shot&#23398;&#20064;&#24182;&#29992;&#20316;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#30340;&#22806;&#37096;&#30693;&#35782;&#65292;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20221;&#26377;&#30528;2600&#19975;&#20010;&#38382;&#31572;&#23545;&#30340;&#20013;&#21307;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#27492;&#20026;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#22411;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20197;&#26816;&#32034;&#21644;&#29983;&#25104;&#20004;&#20010;&#26041;&#38754;&#23545;&#29616;&#26377;&#26041;&#27861;&#22312;&#27492;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#29616;&#26377;&#27169;&#22411;&#30340;&#34920;&#29616;&#36828;&#20302;&#20110;&#39044;&#26399;&#65292;&#32780;&#19988;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#65292;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23454;&#39564;&#24615;&#22320;&#23637;&#31034;&#20102;&#25152;&#25552;&#35758;&#25968;&#25454;&#38598;&#22312;&#20197;&#19979;&#26041;&#38754;&#30340;&#30410;&#22788;&#65306;&#65288;i&#65289;&#20197;&#38646;-shot&#30340;&#26041;&#24335;&#20026;&#20854;&#20182;&#38382;&#31572;&#25968;&#25454;&#38598;&#35757;&#32451;&#27169;&#22411;&#65307;&#65288;ii&#65289;&#20316;&#20026;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#30340;&#22806;&#37096;&#30693;&#35782;&#65307;&#65288;iii&#65289;&#36890;&#36807;&#23558;&#38382;&#31572;&#23545;&#20316;&#20026;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#36827;&#34892;&#36830;&#32493;&#35757;&#32451;&#26041;&#24335;&#65292;&#25552;&#21319;&#29616;&#26377;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30456;&#20449;&#35813;&#25968;&#25454;&#38598;&#19981;&#20165;&#23558;&#26377;&#21161;&#20110;&#21307;&#23398;&#30740;&#31350;&#65292;&#36824;&#23558;&#20419;&#36827;&#30149;&#20154;&#21644;&#20020;&#24202;&#21307;&#29983;&#30340;&#26381;&#21153;&#12290;&#35831;&#21442;&#32771;&#65306;\url{https://github.com/FreedomIntelligence/Huatuo-26M}&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we release a largest ever medical Question Answering (QA) dataset with 26 million QA pairs. We benchmark many existing approaches in our dataset in terms of both retrieval and generation. Experimental results show that the existing models perform far lower than expected and the released dataset is still challenging in the pre-trained language model era. Moreover, we also experimentally show the benefit of the proposed dataset in many aspects: (i) trained models for other QA datasets in a zero-shot fashion; and (ii) as external knowledge for retrieval-augmented generation (RAG); and (iii) improving existing pre-trained language models by using the QA pairs as a pre-training corpus in continued training manner. We believe that this dataset will not only contribute to medical research but also facilitate both the patients and clinical doctors. See \url{https://github.com/FreedomIntelligence/Huatuo-26M}.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#36130;&#21153;&#25512;&#29702;&#39046;&#22495;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#23545;&#20219;&#21153;&#21046;&#23450;&#12289;&#25968;&#25454;&#29983;&#25104;&#12289;&#25552;&#31034;&#26041;&#27861;&#21644;&#35780;&#20272;&#33021;&#21147;&#31561;&#26041;&#38754;&#36827;&#34892;&#20102;&#35814;&#32454;&#30740;&#31350;&#65292;&#26368;&#32456;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#22823;&#23567;&#19978;&#23545;&#21442;&#25968;&#35268;&#27169;&#20026;2.8B&#33267;13B&#30340;&#21508;&#31181;GPT&#21464;&#20307;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2305.01505</link><description>&lt;p&gt;
&#36229;&#36234;&#20998;&#31867;&#65306;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36130;&#21153;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Beyond Classification: Financial Reasoning in State-of-the-Art Language Models. (arXiv:2305.01505v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#36130;&#21153;&#25512;&#29702;&#39046;&#22495;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#23545;&#20219;&#21153;&#21046;&#23450;&#12289;&#25968;&#25454;&#29983;&#25104;&#12289;&#25552;&#31034;&#26041;&#27861;&#21644;&#35780;&#20272;&#33021;&#21147;&#31561;&#26041;&#38754;&#36827;&#34892;&#20102;&#35814;&#32454;&#30740;&#31350;&#65292;&#26368;&#32456;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#22823;&#23567;&#19978;&#23545;&#21442;&#25968;&#35268;&#27169;&#20026;2.8B&#33267;13B&#30340;&#21508;&#31181;GPT&#21464;&#20307;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30001;1000&#20159;&#21450;&#20197;&#19978;&#30340;&#21442;&#25968;&#32452;&#25104;&#65292;&#22312;&#22797;&#26434;&#30340;&#22810;&#27493;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#36890;&#29992;&#30340;&#36827;&#23637;&#24212;&#29992;&#22312;&#24456;&#23569;&#39046;&#22495;&#20013;&#65292;&#20363;&#22914;&#20020;&#24202;&#25110;&#27861;&#24459;&#39046;&#22495;&#65292;&#32780;&#36130;&#21153;&#25512;&#29702;&#39046;&#22495;&#22522;&#26412;&#19978;&#26410;&#34987;&#25506;&#32034;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;LLMs&#35299;&#20915;&#36130;&#21153;&#25512;&#29702;&#38382;&#39064;&#30340;&#33021;&#21147;&#20174;&#26410;&#34987;&#30740;&#31350;&#36807;&#65292;&#24182;&#19988;&#23427;&#26159;&#21542;&#21487;&#20197;&#22312;&#20219;&#20309;&#35268;&#27169;&#19978;&#23436;&#25104;&#20173;&#26410;&#30693;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#30693;&#35782;&#31354;&#30333;&#65292;&#26412;&#30740;&#31350;&#23545;LLMs&#22312;&#36130;&#21153;&#39046;&#22495;&#30340;&#28508;&#22312;&#24212;&#29992;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#12290;&#35843;&#26597;&#21253;&#25324;&#23545;&#19968;&#31995;&#21015;&#20027;&#39064;&#30340;&#35814;&#32454;&#25506;&#35752;&#65292;&#21253;&#25324;&#20219;&#21153;&#21046;&#23450;&#65292;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#65292;&#25552;&#31034;&#26041;&#27861;&#21644;&#35780;&#20272;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#22823;&#23567;&#19978;&#23545;&#21442;&#25968;&#35268;&#27169;&#20026;2.8B&#33267;13B&#30340;&#21508;&#31181;GPT&#21464;&#20307;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#26377;&#26080;&#25351;&#23548;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), consisting of 100 billion or more parameters, have demonstrated remarkable ability in complex multi-step reasoning tasks. However, the application of such generic advancements has been limited to a few fields, such as clinical or legal, with the field of financial reasoning remaining largely unexplored. To the best of our knowledge, the ability of LLMs to solve financial reasoning problems has never been dealt with, and whether it can be performed at any scale remains unknown. To address this knowledge gap, this research presents a comprehensive investigation into the potential application of LLMs in the financial domain. The investigation includes a detailed exploration of a range of subjects, including task formulation, synthetic data generation, prompting methods, and evaluation capability. Furthermore, the study benchmarks various GPT variants with parameter scales ranging from 2.8B to 13B, with and without instruction tuning, on diverse dataset sizes.
&lt;/p&gt;</description></item><item><title>NewsPanda&#26159;&#19968;&#20010;&#21487;&#20197;&#33258;&#21160;&#26816;&#27979;&#21644;&#20998;&#26512;&#19982;&#29615;&#20445;&#21644;&#22522;&#30784;&#24314;&#35774;&#30456;&#20851;&#30340;&#32593;&#19978;&#25991;&#31456;&#30340;&#24037;&#20855;&#12290;&#35813;&#24037;&#20855;&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#21644;&#22122;&#22768;&#26657;&#27491;&#31639;&#27861;&#23545;BERT&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#20197;&#35782;&#21035;&#30456;&#20851;&#25991;&#31456;&#65292;&#24182;&#25552;&#21462;&#20851;&#38190;&#23383;&#21644;&#25214;&#21040;&#30456;&#20851;&#26469;&#28304;&#12290;&#24050;&#34987;&#19990;&#30028;&#33258;&#28982;&#22522;&#37329;&#20250;&#22242;&#38431;&#22312;&#33521;&#22269;&#12289;&#21360;&#24230;&#21644;&#23612;&#27850;&#23572;&#25104;&#21151;&#37096;&#32626;&#12290;</title><link>http://arxiv.org/abs/2305.01503</link><description>&lt;p&gt;
NewsPanda: &#29992;&#20110;&#21450;&#26102;&#20445;&#25252;&#34892;&#21160;&#30340;&#23186;&#20307;&#30417;&#25511;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
NewsPanda: Media Monitoring for Timely Conservation Action. (arXiv:2305.01503v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01503
&lt;/p&gt;
&lt;p&gt;
NewsPanda&#26159;&#19968;&#20010;&#21487;&#20197;&#33258;&#21160;&#26816;&#27979;&#21644;&#20998;&#26512;&#19982;&#29615;&#20445;&#21644;&#22522;&#30784;&#24314;&#35774;&#30456;&#20851;&#30340;&#32593;&#19978;&#25991;&#31456;&#30340;&#24037;&#20855;&#12290;&#35813;&#24037;&#20855;&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#21644;&#22122;&#22768;&#26657;&#27491;&#31639;&#27861;&#23545;BERT&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#20197;&#35782;&#21035;&#30456;&#20851;&#25991;&#31456;&#65292;&#24182;&#25552;&#21462;&#20851;&#38190;&#23383;&#21644;&#25214;&#21040;&#30456;&#20851;&#26469;&#28304;&#12290;&#24050;&#34987;&#19990;&#30028;&#33258;&#28982;&#22522;&#37329;&#20250;&#22242;&#38431;&#22312;&#33521;&#22269;&#12289;&#21360;&#24230;&#21644;&#23612;&#27850;&#23572;&#25104;&#21151;&#37096;&#32626;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29615;&#20445;&#38750;&#25919;&#24220;&#32452;&#32455;&#23545;&#30417;&#27979;&#30456;&#20851;&#23186;&#20307;&#24182;&#21450;&#26102;&#20102;&#35299;&#22522;&#30784;&#24314;&#35774;&#39033;&#30446;&#30340;&#26356;&#26032;&#20855;&#26377;&#37325;&#35201;&#20852;&#36259;&#65292;&#22240;&#20026;&#36825;&#20123;&#39033;&#30446;&#21487;&#33021;&#20250;&#23545;&#37325;&#35201;&#30340;&#20445;&#25252;&#21306;&#22495;&#20135;&#29983;&#24040;&#22823;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#30417;&#27979;&#24456;&#38590;&#19988;&#38656;&#35201;&#32791;&#36153;&#26102;&#38388;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;NewsPanda&#30340;&#24037;&#20855;&#21253;&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#26816;&#27979;&#21644;&#20998;&#26512;&#19982;&#29615;&#20445;&#21644;&#22522;&#30784;&#24314;&#35774;&#30456;&#20851;&#30340;&#32593;&#19978;&#25991;&#31456;&#12290;&#25105;&#20204;&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#21644;&#22122;&#22768;&#26657;&#27491;&#31639;&#27861;&#23545;BERT&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#35782;&#21035;&#19982;&#20445;&#25252;&#21644;&#22522;&#30784;&#24314;&#35774;&#30456;&#20851;&#30340;&#25991;&#31456;&#12290;&#23545;&#24050;&#35782;&#21035;&#20986;&#30340;&#25991;&#31456;&#65292;&#25105;&#20204;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#65292;&#25552;&#21462;&#20851;&#38190;&#23383;&#24182;&#25214;&#21040;&#21487;&#33021;&#30456;&#20851;&#30340;&#26469;&#28304;&#12290;NewsPanda&#33258;2022&#24180;2&#26376;&#20197;&#26469;&#24050;&#34987;&#19990;&#30028;&#33258;&#28982;&#22522;&#37329;&#20250;&#22242;&#38431;&#22312;&#33521;&#22269;&#12289;&#21360;&#24230;&#21644;&#23612;&#27850;&#23572;&#25104;&#21151;&#37096;&#32626;&#12290;&#23427;&#30446;&#21069;&#30417;&#27979;&#20102;&#21360;&#24230;80,000&#20010;&#32593;&#31449;&#21644;1,074&#20010;&#20445;&#25252;&#22320;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Non-governmental organizations for environmental conservation have a significant interest in monitoring conservation-related media and getting timely updates about infrastructure construction projects as they may cause massive impact to key conservation areas. Such monitoring, however, is difficult and time-consuming. We introduce NewsPanda, a toolkit which automatically detects and analyzes online articles related to environmental conservation and infrastructure construction. We fine-tune a BERT-based model using active learning methods and noise correction algorithms to identify articles that are relevant to conservation and infrastructure construction. For the identified articles, we perform further analysis, extracting keywords and finding potentially related sources. NewsPanda has been successfully deployed by the World Wide Fund for Nature teams in the UK, India, and Nepal since February 2022. It currently monitors over 80,000 websites and 1,074 conservation sites across India an
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;PeerSum&#29992;&#20110;&#29983;&#25104;&#31185;&#23398;&#35770;&#25991;&#30340;&#20803;&#35780;&#35770;&#65292;&#28304;&#25991;&#26723;&#20855;&#26377;&#26174;&#24335;&#23618;&#27425;&#32467;&#26500;&#30340;&#20016;&#23500;&#25991;&#26723;&#38388;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20803;&#35780;&#35770;&#29983;&#25104;&#30340;&#20851;&#31995;&#24863;&#30693;&#22810;&#20219;&#21153;&#27169;&#22411;Rammer&#12290;</title><link>http://arxiv.org/abs/2305.01498</link><description>&lt;p&gt;
&#26088;&#22312;&#24635;&#32467;&#24102;&#26377;&#23618;&#27425;&#20851;&#31995;&#30340;&#22810;&#31687;&#25991;&#26723;
&lt;/p&gt;
&lt;p&gt;
Towards Summarizing Multiple Documents with Hierarchical Relationships. (arXiv:2305.01498v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01498
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;PeerSum&#29992;&#20110;&#29983;&#25104;&#31185;&#23398;&#35770;&#25991;&#30340;&#20803;&#35780;&#35770;&#65292;&#28304;&#25991;&#26723;&#20855;&#26377;&#26174;&#24335;&#23618;&#27425;&#32467;&#26500;&#30340;&#20016;&#23500;&#25991;&#26723;&#38388;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20803;&#35780;&#35770;&#29983;&#25104;&#30340;&#20851;&#31995;&#24863;&#30693;&#22810;&#20219;&#21153;&#27169;&#22411;Rammer&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#25968;&#29616;&#23384;&#30340;&#22810;&#25991;&#26723;&#25688;&#35201;(MDS)&#25968;&#25454;&#38598;&#32570;&#23569;&#20154;&#24037;&#29983;&#25104;&#30340;&#12289;&#30495;&#23454;&#30340;(&#21363;&#38750;&#21512;&#25104;&#30340;)&#25688;&#35201;&#25110;&#32773;&#24102;&#26377;&#26174;&#24335;&#25991;&#26723;&#38388;&#20851;&#31995;&#30340;&#28304;&#25991;&#26723;&#12290;&#20026;&#20102;&#22686;&#24378;MDS&#31995;&#32479;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;PeerSum&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#29983;&#25104;&#31185;&#23398;&#35770;&#25991;&#30340;&#20803;&#35780;&#35770;&#65292;&#20854;&#20013;&#20803;&#35780;&#35770;&#26159;&#23545;&#35780;&#35770;&#21644;&#30456;&#24212;&#35752;&#35770;&#30340;&#39640;&#24230;&#27010;&#25324;&#19988;&#30495;&#23454;&#30340;&#25688;&#35201;&#12290;&#36825;&#20123;&#28304;&#25991;&#26723;&#20855;&#26377;&#26174;&#24335;&#23618;&#27425;&#32467;&#26500;&#30340;&#20016;&#23500;&#25991;&#26723;&#38388;&#20851;&#31995;&#65292;&#21253;&#25324;&#20132;&#21449;&#24341;&#29992;&#21644;&#32463;&#24120;&#20986;&#29616;&#30340;&#20914;&#31361;&#12290;&#37492;&#20110;&#24456;&#23569;&#26377;&#30740;&#31350;&#37319;&#29992;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#25805;&#32437;&#26469;&#23558;&#23618;&#27425;&#20851;&#31995;&#32435;&#20837;MDS&#31995;&#32479;&#20013;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;Rammer(&#20851;&#31995;&#24863;&#30693;&#22810;&#20219;&#21153;&#20803;&#35780;&#35770;&#29983;&#25104;&#22120;)&#65292;&#36825;&#26159;&#19968;&#31181;&#20803;&#35780;&#35770;&#29983;&#25104;&#27169;&#22411;&#65292;&#20351;&#29992;&#22522;&#20110;&#23618;&#27425;&#20851;&#31995;&#30340;&#31232;&#30095;&#27880;&#24847;&#21147;&#21644;&#22810;&#20219;&#21153;&#30446;&#26631;&#65292;&#21487;&#20197;&#39044;&#27979;&#22810;&#20010;&#24230;&#37327;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most existing multi-document summarization (MDS) datasets lack human-generated and genuine (i.e., not synthetic) summaries or source documents with explicit inter-document relationships that a summary must capture. To enhance the capabilities of MDS systems we present PeerSum, a novel dataset for generating meta-reviews of scientific papers, where the meta-reviews are highly abstractive and genuine summaries of reviews and corresponding discussions. These source documents have rich inter-document relationships of an explicit hierarchical structure with cross-references and often feature conflicts. As there is a scarcity of research that incorporates hierarchical relationships into MDS systems through attention manipulation on pre-trained language models, we additionally present Rammer (Relationship-aware Multi-task Meta-review Generator), a meta-review generation model that uses sparse attention based on the hierarchical relationships and a multi-task objective that predicts several me
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#31070;&#32463;&#25968;&#25454;&#24211;&#26694;&#26550;&#65288;MMNDBs&#65289;&#65292;&#21487;&#20197;&#22238;&#31572;&#28041;&#21450;&#25991;&#26412;&#21644;&#22270;&#20687;&#31561;&#19981;&#21516;&#36755;&#20837;&#27169;&#24577;&#30340;&#22797;&#26434;&#26597;&#35810;&#65292;&#20855;&#26377;&#31867;&#20284;&#25968;&#25454;&#24211;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.01447</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#31070;&#32463;&#25968;&#25454;&#24211;
&lt;/p&gt;
&lt;p&gt;
Multimodal Neural Databases. (arXiv:2305.01447v1 [cs.MM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#31070;&#32463;&#25968;&#25454;&#24211;&#26694;&#26550;&#65288;MMNDBs&#65289;&#65292;&#21487;&#20197;&#22238;&#31572;&#28041;&#21450;&#25991;&#26412;&#21644;&#22270;&#20687;&#31561;&#19981;&#21516;&#36755;&#20837;&#27169;&#24577;&#30340;&#22797;&#26434;&#26597;&#35810;&#65292;&#20855;&#26377;&#31867;&#20284;&#25968;&#25454;&#24211;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#20854;&#20182;&#27169;&#24577;&#30340;&#26494;&#25955;&#32467;&#26500;&#25968;&#25454;&#30340;&#22686;&#21152;&#21628;&#21505;&#26032;&#30340;&#26597;&#35810;&#26041;&#27861;&#12290;&#22810;&#23186;&#20307;&#20449;&#24687;&#26816;&#32034;&#22635;&#34917;&#20102;&#36825;&#20010;&#31354;&#30333;&#24182;&#22312;&#26368;&#36817;&#20960;&#24180;&#21462;&#24471;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#36827;&#23637;&#12290;&#26816;&#32034;&#22823;&#35268;&#27169;&#22810;&#23186;&#20307;&#26723;&#26696;&#30340;&#20219;&#21153;&#24050;&#32463;&#32463;&#21382;&#20102;&#24040;&#22823;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#36825;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#30001;&#20110;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#36817;&#21457;&#23637;&#25152;&#25512;&#21160;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#39046;&#22495;&#30340;&#26041;&#27861;&#22312;&#23427;&#20204;&#25152;&#25903;&#25345;&#30340;&#26597;&#35810;&#31867;&#22411;&#19978;&#20173;&#28982;&#21463;&#21040;&#38480;&#21046;&#65292;&#23588;&#20854;&#26159;&#23427;&#20204;&#26080;&#27861;&#22238;&#31572;&#31867;&#20284;&#25968;&#25454;&#24211;&#30340;&#26597;&#35810;&#12290;&#22240;&#27492;&#65292;&#21463;&#31070;&#32463;&#25968;&#25454;&#24211;&#30340;&#26368;&#26032;&#24037;&#20316;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;&#65292;&#21629;&#21517;&#20026;&#22810;&#27169;&#24577;&#31070;&#32463;&#25968;&#25454;&#24211;&#65288;MMNDBs&#65289;&#12290;MMNDBs&#21487;&#20197;&#22238;&#31572;&#28041;&#21450;&#19981;&#21516;&#36755;&#20837;&#27169;&#24577;&#65288;&#20363;&#22914;&#25991;&#26412;&#21644;&#22270;&#20687;&#65289;&#30340;&#25512;&#29702;&#30340;&#22797;&#26434;&#31867;&#20284;&#25968;&#25454;&#24211;&#30340;&#26597;&#35810;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#28385;&#36275;&#36825;&#19968;&#31995;&#21015;&#35201;&#27714;&#30340;&#26550;&#26500;&#65292;&#24182;&#36890;&#36807;&#20960;&#20010;&#22522;&#32447;&#27979;&#35797;&#20102;&#23427;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise in loosely-structured data available through text, images, and other modalities has called for new ways of querying them. Multimedia Information Retrieval has filled this gap and has witnessed exciting progress in recent years. Tasks such as search and retrieval of extensive multimedia archives have undergone massive performance improvements, driven to a large extent by recent developments in multimodal deep learning. However, methods in this field remain limited in the kinds of queries they support and, in particular, their inability to answer database-like queries. For this reason, inspired by recent work on neural databases, we propose a new framework, which we name Multimodal Neural Databases (MMNDBs). MMNDBs can answer complex database-like queries that involve reasoning over different input modalities, such as text and images, at scale. In this paper, we present the first architecture able to fulfill this set of requirements and test it with several baselines, showing th
&lt;/p&gt;</description></item><item><title>&#25688;&#35201;&#20171;&#32461;&#20102;&#22914;&#20309;&#24212;&#23545;&#38750;&#27954;&#35821;&#35328;&#22810;&#26679;&#24615;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#27169;&#22411;&#20316;&#20026;&#20135;&#21697;&#25945;&#23398;&#30340;&#25945;&#23398;&#24037;&#20855;&#30340;&#24819;&#27861;&#65292;&#35813;&#26041;&#27861;&#23545;&#20110;&#23547;&#27714;&#25913;&#21892;&#38750;&#27954;&#26412;&#22303;&#26041;&#35328;&#23458;&#25143;&#20307;&#39564;&#21644;&#20135;&#21697;&#24320;&#21457;&#30340;&#20225;&#19994;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.01427</link><description>&lt;p&gt;
&#20174;&#26412;&#22320;&#21040;&#20840;&#29699;&#65306;&#24212;&#23545;&#38750;&#27954;&#35821;&#35328;&#22810;&#26679;&#24615;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
From Local to Global: Navigating Linguistic Diversity in the African Context. (arXiv:2305.01427v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01427
&lt;/p&gt;
&lt;p&gt;
&#25688;&#35201;&#20171;&#32461;&#20102;&#22914;&#20309;&#24212;&#23545;&#38750;&#27954;&#35821;&#35328;&#22810;&#26679;&#24615;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#27169;&#22411;&#20316;&#20026;&#20135;&#21697;&#25945;&#23398;&#30340;&#25945;&#23398;&#24037;&#20855;&#30340;&#24819;&#27861;&#65292;&#35813;&#26041;&#27861;&#23545;&#20110;&#23547;&#27714;&#25913;&#21892;&#38750;&#27954;&#26412;&#22303;&#26041;&#35328;&#23458;&#25143;&#20307;&#39564;&#21644;&#20135;&#21697;&#24320;&#21457;&#30340;&#20225;&#19994;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#23384;&#22312;&#30340;&#26377;&#20851;&#38750;&#27954;&#22823;&#38470;&#19978;&#35821;&#35328;&#22810;&#26679;&#24615;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#38750;&#27954;&#30340;&#22320;&#26041;&#26041;&#35328;&#21644;&#40092;&#20026;&#20154;&#30693;&#30340;&#38463;&#25289;&#20271;&#26041;&#35328;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#65292;&#21516;&#26102;&#24378;&#35843;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#23545;&#20110;&#23547;&#27714;&#25913;&#36827;&#38750;&#27954;&#26412;&#22303;&#26041;&#35328;&#30340;&#23458;&#25143;&#20307;&#39564;&#21644;&#20135;&#21697;&#24320;&#21457;&#30340;&#20225;&#19994;&#21487;&#33021;&#20135;&#29983;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;&#20351;&#29992;&#35813;&#27169;&#22411;&#20316;&#20026;&#20135;&#21697;&#25945;&#23398;&#30340;&#25945;&#23398;&#24037;&#20855;&#30340;&#24819;&#27861;&#20063;&#24456;&#26377;&#36259;&#65292;&#22240;&#20026;&#36825;&#21487;&#33021;&#20250;&#28608;&#21457;&#23398;&#20064;&#32773;&#30340;&#20852;&#36259;&#24182;&#24341;&#21457;&#31185;&#25216;&#21019;&#19994;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#25913;&#36827;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#24212;&#23545;&#38750;&#27954;&#26412;&#22303;&#26041;&#35328;&#25361;&#25112;&#30340;&#26377;&#21069;&#36884;&#30340;&#20998;&#26512;&#12290;&#29305;&#21035;&#26159;&#40092;&#20026;&#20154;&#30693;&#30340;&#38463;&#25289;&#20271;&#26041;&#35328;&#65292;&#36825;&#23545;&#20110;&#23547;&#27714;&#25913;&#36827;&#23458;&#25143;&#20307;&#39564;&#21644;&#20135;&#21697;&#24320;&#21457;&#30340;&#20225;&#19994;&#21487;&#33021;&#20250;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The focus is on critical problems in NLP related to linguistic diversity and variation across the African continent, specifically with regards to African local di- alects and Arabic dialects that have received little attention. We evaluated our various approaches, demonstrating their effectiveness while highlighting the potential impact of the proposed approach on businesses seeking to improve customer experience and product development in African local dialects. The idea of using the model as a teaching tool for product-based instruction is interesting, as it could potentially stimulate interest in learners and trigger techno entrepreneurship. Overall, our modified approach offers a promising analysis of the challenges of dealing with African local dialects. Particularly Arabic dialects, which could have a significant impact on businesses seeking to improve customer experience and product development.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#31867;&#21035;&#20449;&#24687;&#30340;&#24433;&#21709;&#20989;&#25968;&#26469;&#25552;&#39640;&#24322;&#24120;&#26816;&#27979;&#30340;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.01384</link><description>&lt;p&gt;
&#22522;&#20110;&#31867;&#30340;&#24433;&#21709;&#20989;&#25968;&#29992;&#20110;&#35823;&#24046;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Class based Influence Functions for Error Detection. (arXiv:2305.01384v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01384
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#31867;&#21035;&#20449;&#24687;&#30340;&#24433;&#21709;&#20989;&#25968;&#26469;&#25552;&#39640;&#24322;&#24120;&#26816;&#27979;&#30340;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24433;&#21709;&#20989;&#25968;(IFs)&#26159;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#20013;&#26816;&#27979;&#24322;&#24120;&#26679;&#26412;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#24403;&#24212;&#29992;&#20110;&#28145;&#24230;&#32593;&#32476;&#26102;&#65292;&#23427;&#20204;&#19981;&#31283;&#23450;&#12290;&#26412;&#25991;&#35299;&#37322;&#20102;IFs&#19981;&#31283;&#23450;&#30340;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#20004;&#20010;&#25968;&#25454;&#28857;&#23646;&#20110;&#20004;&#20010;&#19981;&#21516;&#31867;&#21035;&#26102;&#65292;IFs&#26159;&#19981;&#21487;&#38752;&#30340;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#27861;&#21033;&#29992;&#31867;&#21035;&#20449;&#24687;&#26469;&#25913;&#36827;IFs&#30340;&#31283;&#23450;&#24615;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#20462;&#25913;&#26174;&#33879;&#25552;&#39640;&#20102;IFs&#30340;&#24615;&#33021;&#21644;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#19981;&#22686;&#21152;&#20219;&#20309;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Influence functions (IFs) are a powerful tool for detecting anomalous examples in large scale datasets. However, they are unstable when applied to deep networks. In this paper, we provide an explanation for the instability of IFs and develop a solution to this problem. We show that IFs are unreliable when the two data points belong to two different classes. Our solution leverages class information to improve the stability of IFs. Extensive experiments show that our modification significantly improves the performance and stability of IFs while incurring no additional computational cost.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#23558;&#24050;&#26377;&#30340;&#36731;&#37327;&#21270;&#35270;&#35273;&#25552;&#31034;&#21457;&#29983;&#22120;&#36830;&#25509;&#21040;&#35270;&#35273;-&#35821;&#35328;LLM&#20197;&#20943;&#23569;&#36164;&#28304;&#28040;&#32791;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#36328;&#19981;&#21516;&#22823;&#23567;&#21644;&#31867;&#22411;&#30340;LLMs&#30340;VPG&#36716;&#31227;&#26041;&#26696;VPGTrans&#65292;&#35813;&#26041;&#26696;&#22312;VQA&#21644;NLVR2&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#31168;&#12290;</title><link>http://arxiv.org/abs/2305.01278</link><description>&lt;p&gt;
&#27178;&#21521;&#36801;&#31227;&#36731;&#37327;&#21270;&#35270;&#35273;&#25552;&#31034;&#21457;&#29983;&#22120;&#22312;VL-LLMs&#20043;&#38388;&#30340;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Transfer Visual Prompt Generator across LLMs. (arXiv:2305.01278v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01278
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#23558;&#24050;&#26377;&#30340;&#36731;&#37327;&#21270;&#35270;&#35273;&#25552;&#31034;&#21457;&#29983;&#22120;&#36830;&#25509;&#21040;&#35270;&#35273;-&#35821;&#35328;LLM&#20197;&#20943;&#23569;&#36164;&#28304;&#28040;&#32791;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#36328;&#19981;&#21516;&#22823;&#23567;&#21644;&#31867;&#22411;&#30340;LLMs&#30340;VPG&#36716;&#31227;&#26041;&#26696;VPGTrans&#65292;&#35813;&#26041;&#26696;&#22312;VQA&#21644;NLVR2&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21033;&#29992;&#29616;&#26377;&#30340;&#36731;&#37327;&#21270;&#35270;&#35273;&#25552;&#31034;&#21457;&#29983;&#22120;&#65288;VPG&#65289;&#36830;&#25509;&#24050;&#26377;&#30340;&#35270;&#35273;-&#35821;&#35328;LLM&#65288;VL-LLM&#65289;&#20197;&#20943;&#23569;&#36164;&#28304;&#28040;&#32791;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#36328;&#19981;&#21516;&#22823;&#23567;&#21644;&#31867;&#22411;&#30340;LLMs&#30340;VPG&#36716;&#31227;&#26041;&#26696;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#35266;&#23519;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;VPGTrans&#30340;&#20004;&#38454;&#27573;&#36716;&#31227;&#26694;&#26550;&#65292;&#23427;&#22312;VQA&#21644;NLVR2&#20004;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#31934;&#24230;&#21644;&#36716;&#31227;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
While developing a new vision-language LLM (VL-LLM) by pre-training on tremendous image-text pairs from scratch can be exceedingly resource-consuming, connecting an existing LLM with a comparatively lightweight visual prompt generator (VPG) becomes a feasible paradigm. However, further tuning the VPG part of the VL-LLM still suffers from indispensable computational costs, i.e., requiring thousands of GPU hours and millions of training data. One alternative solution is to transfer an existing VPG from any existing VL-LLMs for the target VL-LLM.  In this work, we for the first time investigate the VPG transferability across LLMs, and explore a solution to reduce the cost of VPG transfer. We first study the VPG transfer across different LLM sizes (e.g., small-to-large), and across different LLM types, through which we diagnose the key factors to maximize the transfer efficiency. Based on our observation, we design a two-stage transfer framework named VPGTrans, which is simple yet highly e
&lt;/p&gt;</description></item><item><title>&#24635;&#32467;&#26159;&#29983;&#25104;&#26234;&#33021;&#20307;&#26368;&#22522;&#26412;&#12289;&#26368;&#19981;&#21487;&#25110;&#32570;&#30340;&#33021;&#21147;&#12290;&#26412;&#25253;&#21578;&#36890;&#36807;&#32508;&#21512;&#35266;&#28857;&#65292;&#23581;&#35797;&#29702;&#35299;&#29983;&#25104;&#26234;&#33021;&#20307;&#65292;&#24182;&#25512;&#36827;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2305.01253</link><description>&lt;p&gt;
&#24635;&#32467;&#22312;&#29983;&#25104;&#26234;&#33021;&#20307;&#20013;&#30340;&#20316;&#29992;:&#21021;&#27493;&#35266;&#28857;
&lt;/p&gt;
&lt;p&gt;
The Role of Summarization in Generative Agents: A Preliminary Perspective. (arXiv:2305.01253v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01253
&lt;/p&gt;
&lt;p&gt;
&#24635;&#32467;&#26159;&#29983;&#25104;&#26234;&#33021;&#20307;&#26368;&#22522;&#26412;&#12289;&#26368;&#19981;&#21487;&#25110;&#32570;&#30340;&#33021;&#21147;&#12290;&#26412;&#25253;&#21578;&#36890;&#36807;&#32508;&#21512;&#35266;&#28857;&#65292;&#23581;&#35797;&#29702;&#35299;&#29983;&#25104;&#26234;&#33021;&#20307;&#65292;&#24182;&#25512;&#36827;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#20154;&#31867;&#31038;&#20250;&#30340;&#29983;&#25104;&#26234;&#33021;&#20307;&#23637;&#29616;&#20986;&#26356;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#23454;&#38469;&#24212;&#29992;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#20854;&#20013;&#65292;&#30001;&#22810;&#20010;&#32454;&#33268;&#35774;&#35745;&#30340;&#27169;&#22359;&#26500;&#25104;&#30340;&#29983;&#25104;&#26234;&#33021;&#20307;&#26550;&#26500;&#26159;&#26368;&#20851;&#38190;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#20026;&#20102;&#25512;&#36827;&#36825;&#19968;&#30740;&#31350;&#65292;&#26412;&#25253;&#21578;&#25552;&#20986;&#20102;&#25105;&#20204;&#30340;&#32508;&#21512;&#35266;&#28857;&#65292;&#35748;&#20026;&#36890;&#36807;&#24635;&#32467;&#29702;&#35299;&#29983;&#25104;&#26234;&#33021;&#20307;&#26159;&#33267;&#20851;&#37325;&#35201;&#21644;&#19981;&#21487;&#25110;&#32570;&#30340;&#33021;&#21147;&#65292;&#36825;&#31181;&#33021;&#21147;&#20250;&#22312;&#21508;&#31181;&#22330;&#26223;&#19979;&#34920;&#29616;&#20986;&#26469;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#20221;&#25253;&#21578;&#33021;&#22815;&#25552;&#20379;&#27934;&#35265;&#65292;&#24110;&#21161;&#20154;&#20204;&#29702;&#35299;&#24635;&#32467;&#33021;&#21147;&#22312;&#29983;&#25104;&#26234;&#33021;&#20307;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#28608;&#21457;&#26410;&#26469;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative agents that simulate human society show tremendous potential for further research and practical applications. Specifically, the generative agent architecture comprising several meticulously designed modules constitutes the most critical component. To facilitate progress in this research, this report presents our integrated perspective on comprehending generative agents through summarization, since we believe summarization is the most fundamental and indispensable capacity of generative agents manifested across diverse scenarios. We hope this report can provide insight into understanding the importance of summarization capacity in generative agents and motivate future research.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#26377;&#25928;&#30340;&#8220;ProAttack&#8221;&#26041;&#27861;&#26469;&#25191;&#34892;&#24178;&#20928;&#26631;&#31614;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#20351;&#29992;&#30340;&#26159;&#25552;&#31034;&#26412;&#36523;&#20316;&#20026;&#35302;&#21457;&#22120;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#22806;&#37096;&#35302;&#21457;&#22120;&#65292;&#24182;&#30830;&#20445;&#27602;&#30244;&#25968;&#25454;&#30340;&#26631;&#27880;&#27491;&#30830;&#65292;&#25552;&#39640;&#20102;&#21518;&#38376;&#25915;&#20987;&#30340;&#38544;&#34109;&#24615;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#26377;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.01219</link><description>&lt;p&gt;
&#35302;&#21457;&#35789;&#20316;&#20026;&#21518;&#38376;&#25915;&#20987;&#30340;&#35302;&#21457;&#22120;&#65306;&#26816;&#26597;&#35821;&#35328;&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;
&lt;/p&gt;
&lt;p&gt;
Prompt as Triggers for Backdoor Attack: Examining the Vulnerability in Language Models. (arXiv:2305.01219v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#26377;&#25928;&#30340;&#8220;ProAttack&#8221;&#26041;&#27861;&#26469;&#25191;&#34892;&#24178;&#20928;&#26631;&#31614;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#20351;&#29992;&#30340;&#26159;&#25552;&#31034;&#26412;&#36523;&#20316;&#20026;&#35302;&#21457;&#22120;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#22806;&#37096;&#35302;&#21457;&#22120;&#65292;&#24182;&#30830;&#20445;&#27602;&#30244;&#25968;&#25454;&#30340;&#26631;&#27880;&#27491;&#30830;&#65292;&#25552;&#39640;&#20102;&#21518;&#38376;&#25915;&#20987;&#30340;&#38544;&#34109;&#24615;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#26377;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#33539;&#20363;&#24357;&#21512;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#22312;&#20960;&#20010;NLP&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#12290;&#23613;&#31649;&#24212;&#29992;&#24191;&#27867;&#65292;&#20294;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#12290;&#25991;&#26412;&#21518;&#38376;&#25915;&#20987;&#26088;&#22312;&#36890;&#36807;&#27880;&#20837;&#35302;&#21457;&#22120;&#24182;&#20462;&#25913;&#26631;&#31614;&#26469;&#22312;&#27169;&#22411;&#20013;&#24341;&#20837;&#26377;&#38024;&#23545;&#24615;&#30340;&#28431;&#27934;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35302;&#21457;&#22120;&#30340;&#23384;&#22312;&#21644;&#27602;&#30244;&#25968;&#25454;&#26631;&#27880;&#19981;&#27491;&#30830;&#31561;&#32570;&#38519;&#65292;&#36825;&#31181;&#25915;&#20987;&#23384;&#22312;&#24322;&#24120;&#30340;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26377;&#25928;&#30340;&#8220;ProAttack&#8221;&#26041;&#27861;&#65292;&#22522;&#20110;&#25552;&#31034;&#26469;&#25191;&#34892;&#24178;&#20928;&#26631;&#31614;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#20351;&#29992;&#30340;&#26159;&#25552;&#31034;&#26412;&#36523;&#20316;&#20026;&#35302;&#21457;&#22120;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#22806;&#37096;&#35302;&#21457;&#22120;&#65292;&#24182;&#30830;&#20445;&#27602;&#30244;&#25968;&#25454;&#30340;&#26631;&#27880;&#27491;&#30830;&#65292;&#25552;&#39640;&#20102;&#21518;&#38376;&#25915;&#20987;&#30340;&#38544;&#34109;&#24615;&#12290;&#36890;&#36807;&#22312;&#20016;&#23500;&#30340;&#36164;&#28304;&#21644;&#23569;&#26679;&#26412;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;ProAttack&#26041;&#27861;&#22312;&#20445;&#25345;&#24178;&#20928;&#25968;&#25454;&#19968;&#33268;&#24615;&#30340;&#21516;&#26102;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prompt-based learning paradigm, which bridges the gap between pre-training and fine-tuning, achieves state-of-the-art performance on several NLP tasks, particularly in few-shot settings. Despite being widely applied, prompt-based learning is vulnerable to backdoor attacks. Textual backdoor attacks are designed to introduce targeted vulnerabilities into models by poisoning a subset of training samples through trigger injection and label modification. However, they suffer from flaws such as abnormal natural language expressions resulting from the trigger and incorrect labeling of poisoned samples. In this study, we propose {\bf ProAttack}, a novel and efficient method for performing clean-label backdoor attacks based on the prompt, which uses the prompt itself as a trigger. Our method does not require external triggers and ensures correct labeling of poisoned samples, improving the stealthy nature of the backdoor attack. With extensive experiments on rich-resource and few-shot text c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#27861;&#24459;&#21477;&#23376;&#36793;&#30028;&#26816;&#27979;&#25968;&#25454;&#38598;MultiLegalSBD&#65292;&#21253;&#25324;6&#31181;&#35821;&#35328;&#30340;130,000&#20010;&#27880;&#37322;&#21477;&#23376;&#12290;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#65292;&#29616;&#26377;&#30340;SBD&#27169;&#22411;&#30340;&#34920;&#29616;&#19981;&#20339;&#12290;&#20316;&#32773;&#35757;&#32451;&#20102;&#22522;&#20110;CRF&#12289;BiLSTM-CRF&#21644;transformers&#30340;&#21333;&#35821;&#21644;&#22810;&#35821;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#35813;&#39046;&#22495;&#20013;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#20182;&#20204;&#30340;&#22810;&#35821;&#27169;&#22411;&#22312;&#38646;-shot&#27979;&#35797;&#20013;&#20248;&#20110;&#25152;&#26377;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2305.01211</link><description>&lt;p&gt;
MultiLegalSBD&#65306;&#19968;&#20010;&#22810;&#35821;&#35328;&#27861;&#24459;&#21477;&#23376;&#36793;&#30028;&#26816;&#27979;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MultiLegalSBD: A Multilingual Legal Sentence Boundary Detection Dataset. (arXiv:2305.01211v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01211
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#27861;&#24459;&#21477;&#23376;&#36793;&#30028;&#26816;&#27979;&#25968;&#25454;&#38598;MultiLegalSBD&#65292;&#21253;&#25324;6&#31181;&#35821;&#35328;&#30340;130,000&#20010;&#27880;&#37322;&#21477;&#23376;&#12290;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#65292;&#29616;&#26377;&#30340;SBD&#27169;&#22411;&#30340;&#34920;&#29616;&#19981;&#20339;&#12290;&#20316;&#32773;&#35757;&#32451;&#20102;&#22522;&#20110;CRF&#12289;BiLSTM-CRF&#21644;transformers&#30340;&#21333;&#35821;&#21644;&#22810;&#35821;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#35813;&#39046;&#22495;&#20013;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#20182;&#20204;&#30340;&#22810;&#35821;&#27169;&#22411;&#22312;&#38646;-shot&#27979;&#35797;&#20013;&#20248;&#20110;&#25152;&#26377;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21477;&#23376;&#36793;&#30028;&#26816;&#27979;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#22522;&#30784;&#20043;&#19968;&#65292;&#19981;&#27491;&#30830;&#30340;&#20998;&#21106;&#20250;&#20005;&#37325;&#24433;&#21709;&#19979;&#28216;&#20219;&#21153;&#30340;&#36755;&#20986;&#36136;&#37327;&#12290;&#23545;&#20110;&#31639;&#27861;&#26469;&#35828;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#23588;&#20854;&#23545;&#20110;&#27861;&#24459;&#39046;&#22495;&#65292;&#22240;&#20026;&#20351;&#29992;&#30340;&#22797;&#26434;&#21477;&#23376;&#32467;&#26500;&#21508;&#19981;&#30456;&#21516;&#12290;&#26412;&#25991;&#31934;&#24515;&#31574;&#21010;&#20102;&#19968;&#20010;&#22810;&#35821;&#31181;&#27861;&#24459;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;6&#31181;&#35821;&#35328;&#30340;130,000&#20010;&#27880;&#37322;&#21477;&#23376;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;SBD&#27169;&#22411;&#22312;&#22810;&#35821;&#31181;&#27861;&#24459;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#35757;&#32451;&#21644;&#27979;&#35797;&#20102;&#22522;&#20110;CRF&#12289;BiLSTM-CRF&#21644;transformers&#30340;&#21333;&#35821;&#21644;&#22810;&#35821;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#22810;&#35821;&#27169;&#22411;&#22312;&#33889;&#33796;&#29273;&#35821;&#27979;&#35797;&#38598;&#30340;&#38646;-shot&#35774;&#32622;&#20013;&#20248;&#20110;&#25152;&#26377;&#22522;&#32447;&#12290;&#20026;&#20102;&#40723;&#21169;&#31038;&#21306;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#65292;&#25105;&#20204;&#24050;&#32463;&#20844;&#24320;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sentence Boundary Detection (SBD) is one of the foundational building blocks of Natural Language Processing (NLP), with incorrectly split sentences heavily influencing the output quality of downstream tasks. It is a challenging task for algorithms, especially in the legal domain, considering the complex and different sentence structures used. In this work, we curated a diverse multilingual legal dataset consisting of over 130'000 annotated sentences in 6 languages. Our experimental results indicate that the performance of existing SBD models is subpar on multilingual legal data. We trained and tested monolingual and multilingual models based on CRF, BiLSTM-CRF, and transformers, demonstrating state-of-the-art performance. We also show that our multilingual models outperform all baselines in the zero-shot setting on a Portuguese test set. To encourage further research and development by the community, we have made our dataset, models, and code publicly available.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27880;&#37322;&#20102;&#19968;&#20010;&#30001;1308&#20010;&#23545;&#35805;&#32452;&#25104;&#30340;&#20013;&#25991;&#33258;&#28982;&#35805;&#39064;&#23545;&#35805;&#35821;&#26009;&#24211;&#65292;&#20197;&#22635;&#34917;&#20013;&#25991;&#33258;&#28982;&#23545;&#35805;&#35805;&#39064;&#35821;&#26009;&#24211;&#30340;&#31354;&#30333;&#12290;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;&#23545;&#27604;&#23398;&#20064;&#30340;&#24072;&#29983;&#26694;&#26550;&#26469;&#39044;&#27979;&#27809;&#26377;&#22238;&#22797;&#30340;&#35805;&#39064;&#36716;&#31227;&#12290;</title><link>http://arxiv.org/abs/2305.01195</link><description>&lt;p&gt;
&#20013;&#25991;&#23545;&#35805;&#20013;&#30340;&#35805;&#39064;&#36716;&#31227;&#26816;&#27979;&#65306;&#35821;&#26009;&#24211;&#19982;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Topic Shift Detection in Chinese Dialogues: Corpus and Benchmark. (arXiv:2305.01195v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01195
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27880;&#37322;&#20102;&#19968;&#20010;&#30001;1308&#20010;&#23545;&#35805;&#32452;&#25104;&#30340;&#20013;&#25991;&#33258;&#28982;&#35805;&#39064;&#23545;&#35805;&#35821;&#26009;&#24211;&#65292;&#20197;&#22635;&#34917;&#20013;&#25991;&#33258;&#28982;&#23545;&#35805;&#35805;&#39064;&#35821;&#26009;&#24211;&#30340;&#31354;&#30333;&#12290;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;&#23545;&#27604;&#23398;&#20064;&#30340;&#24072;&#29983;&#26694;&#26550;&#26469;&#39044;&#27979;&#27809;&#26377;&#22238;&#22797;&#30340;&#35805;&#39064;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#35805;&#39064;&#36716;&#31227;&#26816;&#27979;&#26159;&#25351;&#26816;&#27979;&#23545;&#35805;&#20013;&#27491;&#22312;&#36827;&#34892;&#30340;&#35805;&#39064;&#26159;&#21542;&#36716;&#31227;&#25110;&#24212;&#35813;&#36716;&#31227;&#12290;&#20219;&#21153;&#21487;&#20998;&#20026;&#24050;&#30693;&#22238;&#22797;&#20219;&#21153;&#21644;&#26410;&#30693;&#22238;&#22797;&#20219;&#21153;&#12290;&#30446;&#21069;&#65292;&#21482;&#26377;&#23569;&#25968;&#38024;&#23545;&#21518;&#32773;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#22240;&#20026;&#22312;&#27809;&#26377;&#22238;&#22797;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#35805;&#39064;&#36716;&#31227;&#20173;&#28982;&#26159;&#19968;&#39033;&#25361;&#25112;&#12290;&#26412;&#25991;&#39318;&#20808;&#27880;&#37322;&#20102;&#19968;&#20010;&#30001;1308&#20010;&#23545;&#35805;&#32452;&#25104;&#30340;&#20013;&#25991;&#33258;&#28982;&#35805;&#39064;&#23545;&#35805;&#65288;CNTD&#65289;&#35821;&#26009;&#24211;&#65292;&#20197;&#22635;&#34917;&#20013;&#25991;&#33258;&#28982;&#20250;&#35805;&#35805;&#39064;&#35821;&#26009;&#24211;&#30340;&#31354;&#30333;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#26410;&#30693;&#22238;&#22797;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;&#23545;&#27604;&#23398;&#20064;&#30340;&#24072;&#29983;&#26694;&#26550;&#26469;&#39044;&#27979;&#27809;&#26377;&#22238;&#22797;&#30340;&#35805;&#39064;&#36716;&#31227;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#39640;&#32423;&#24072;&#29983;&#21709;&#24212;&#20013;&#24341;&#20837;&#23545;&#27604;&#23398;&#20064;&#65292;&#29992;&#20110;&#24314;&#31435;&#21709;&#24212;&#21644;&#19978;&#19979;&#25991;&#20043;&#38388;&#30340;&#23545;&#27604;&#23398;&#20064;&#65292;&#32780;&#22312;&#20302;&#32423;&#23398;&#29983;&#20013;&#26500;&#24314;&#26631;&#31614;&#23545;&#27604;&#23398;&#20064;&#12290;&#22312;&#25105;&#20204;&#30340;&#20013;&#25991;CNTD&#21644;&#33521;&#25991;TIAGE&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialogue topic shift detection is to detect whether an ongoing topic has shifted or should shift in a dialogue, which can be divided into two categories, i.e., response-known task and response-unknown task. Currently, only a few investigated the latter, because it is still a challenge to predict the topic shift without the response information. In this paper, we first annotate a Chinese Natural Topic Dialogue (CNTD) corpus consisting of 1308 dialogues to fill the gap in the Chinese natural conversation topic corpus. And then we focus on the response-unknown task and propose a teacher-student framework based on hierarchical contrastive learning to predict the topic shift without the response. Specifically, the response at high-level teacher-student is introduced to build the contrastive learning between the response and the context, while the label contrastive learning is constructed at low-level student. The experimental results on our Chinese CNTD and English TIAGE show the effectiven
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#20302;&#36164;&#28304;&#36866;&#24212;&#39064;&#30446;&#20013;&#20351;&#29992;&#30340;ASR&#21644;NLU&#30340;&#31649;&#36947;&#26041;&#27861;&#12290;&#22312;ASR&#20013;&#65292;&#20351;&#29992;&#19978;&#37319;&#26679;&#30340;Whisper&#23545;&#27599;&#20010;&#39046;&#22495;&#36827;&#34892;Feine-tune&#65307;&#22312;NLU&#20013;&#65292;&#20351;&#29992;MLM&#25216;&#26415;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#24182;&#20351;&#29992;&#22522;&#20110;&#26816;&#32034;&#30340;&#26041;&#27861;&#25193;&#20805;&#25968;&#25454;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#22312;&#25552;&#37266;/&#22825;&#27668;&#39046;&#22495;&#33719;&#24471;&#20102;&#39640;&#31934;&#30830;&#21305;&#37197;&#20934;&#30830;&#24230;&#24182;&#33719;&#24471;&#20102;&#25361;&#25112;&#30340;&#31532;&#19968;&#21517;&#12290;</title><link>http://arxiv.org/abs/2305.01194</link><description>&lt;p&gt;
&#22522;&#20110;MLM&#25968;&#25454;&#22686;&#24378;&#30340;ASR&#21644;NLU&#31649;&#36947;&#31995;&#32479;&#24212;&#23545;STOP&#20302;&#36164;&#28304;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
The Pipeline System of ASR and NLU with MLM-based Data Augmentation toward STOP Low-resource Challenge. (arXiv:2305.01194v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01194
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#20302;&#36164;&#28304;&#36866;&#24212;&#39064;&#30446;&#20013;&#20351;&#29992;&#30340;ASR&#21644;NLU&#30340;&#31649;&#36947;&#26041;&#27861;&#12290;&#22312;ASR&#20013;&#65292;&#20351;&#29992;&#19978;&#37319;&#26679;&#30340;Whisper&#23545;&#27599;&#20010;&#39046;&#22495;&#36827;&#34892;Feine-tune&#65307;&#22312;NLU&#20013;&#65292;&#20351;&#29992;MLM&#25216;&#26415;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#24182;&#20351;&#29992;&#22522;&#20110;&#26816;&#32034;&#30340;&#26041;&#27861;&#25193;&#20805;&#25968;&#25454;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#22312;&#25552;&#37266;/&#22825;&#27668;&#39046;&#22495;&#33719;&#24471;&#20102;&#39640;&#31934;&#30830;&#21305;&#37197;&#20934;&#30830;&#24230;&#24182;&#33719;&#24471;&#20102;&#25361;&#25112;&#30340;&#31532;&#19968;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#22312;ICASSP&#20449;&#21495;&#22788;&#29702;&#22823;&#36187;2023&#30340;&#21475;&#35821;&#29702;&#35299;&#22823;&#25361;&#25112;&#65288;Spoken Language Understanding Grand Challenge&#65289;&#20302;&#36164;&#28304;&#39046;&#22495;&#36866;&#24212;&#36187;&#36947;&#65288;Track3&#65289;&#20013;&#37319;&#29992;&#30340;ASR&#21644;NLU&#30340;&#31649;&#36947;&#26041;&#27861;&#12290;&#38024;&#23545;ASR&#65292;&#25105;&#20204;&#20351;&#29992;&#19978;&#37319;&#26679; fine-tune Whisper &#20197;&#36866;&#24212;&#27599;&#20010;&#39046;&#22495;&#12290;&#38024;&#23545;NLU&#65292;&#25105;&#20204; fine-tune BART &#22312;&#25152;&#26377; Track3 &#25968;&#25454;&#19978;&#65292;&#28982;&#21518;&#22312;&#20302;&#36164;&#28304;&#22495;&#25968;&#25454;&#19978;&#36827;&#34892; fine-tune&#12290;&#25105;&#20204;&#24212;&#29992;&#20102;&#22522;&#20110;&#36974;&#30422;&#30340;LM&#65288;MLM&#65289;&#25968;&#25454;&#22686;&#24378;&#65292;&#20854;&#20013;&#19968;&#20123;&#36755;&#20837;&#26631;&#35760;&#21644;&#30456;&#24212;&#30340;&#30446;&#26631;&#26631;&#31614;&#20351;&#29992; MLM &#36827;&#34892;&#26367;&#25442;&#12290;&#25105;&#20204;&#36824;&#37319;&#29992;&#20102;&#22522;&#20110;&#26816;&#32034;&#30340;&#26041;&#27861;&#65292;&#27169;&#22411;&#36755;&#20837;&#19982;&#31867;&#20284;&#30340;&#35757;&#32451;&#26679;&#26412;&#19968;&#36215;&#36827;&#34892;&#22686;&#24378;&#12290;&#32467;&#26524;&#65292;&#25105;&#20204;&#22312;&#25552;&#37266;/&#22825;&#27668;&#39046;&#22495;&#23454;&#29616;&#20102;63.3 / 75.0&#65288;&#24179;&#22343;&#65306;69.15&#65289;&#30340;&#31934;&#30830;&#21305;&#37197;&#65288;EM&#65289;&#20934;&#30830;&#24230;&#65292;&#33719;&#24471;&#20102;&#35813;&#25361;&#25112;&#30340;&#31532;&#19968;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes our system for the low-resource domain adaptation track (Track 3) in Spoken Language Understanding Grand Challenge, which is a part of ICASSP Signal Processing Grand Challenge 2023. In the track, we adopt a pipeline approach of ASR and NLU. For ASR, we fine-tune Whisper for each domain with upsampling. For NLU, we fine-tune BART on all the Track3 data and then on low-resource domain data. We apply masked LM (MLM) -based data augmentation, where some of input tokens and corresponding target labels are replaced using MLM. We also apply a retrieval-based approach, where model input is augmented with similar training samples. As a result, we achieved exact match (EM) accuracy 63.3/75.0 (average: 69.15) for reminder/weather domain, and won the 1st place at the challenge.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#20960;&#20010;&#26032;&#26041;&#21521;&#65292;&#21253;&#25324;&#39118;&#26684;&#21270;MT&#12289;&#20132;&#20114;&#24335;MT&#21644;&#22522;&#20110;&#32763;&#35793;&#35760;&#24518;&#30340;MT&#65292;&#24182;&#35752;&#35770;&#20102;&#38544;&#31169;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.01181</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#32763;&#35793;&#26032;&#36235;&#21183;&#65306;&#20197;ChatGPT&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
New Trends in Machine Translation using Large Language Models: Case Examples with ChatGPT. (arXiv:2305.01181v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#20960;&#20010;&#26032;&#26041;&#21521;&#65292;&#21253;&#25324;&#39118;&#26684;&#21270;MT&#12289;&#20132;&#20114;&#24335;MT&#21644;&#22522;&#20110;&#32763;&#35793;&#35760;&#24518;&#30340;MT&#65292;&#24182;&#35752;&#35770;&#20102;&#38544;&#31169;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#22312;&#28145;&#24230;&#23398;&#20064;&#30340;&#25512;&#21160;&#19979;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;GPT-3&#21644;ChatGPT&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#21518;&#12290;&#36825;&#20026;&#20351;&#29992;LLMs&#30340;MT&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20123;&#26377;&#36259;&#30340;&#20351;&#29992;LLMs&#30340;MT&#26041;&#21521;&#65292;&#21253;&#25324;&#39118;&#26684;&#21270;MT&#12289;&#20132;&#20114;&#24335;MT&#21644;&#22522;&#20110;&#32763;&#35793;&#35760;&#24518;&#30340;MT&#65292;&#20197;&#21450;&#19968;&#31181;&#20351;&#29992;LLMs&#30340;&#26032;&#35780;&#20272;&#33539;&#20363;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#20351;&#29992;LLMs&#30340;MT&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#26412;&#30340;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#20197;&#20943;&#36731;&#27492;&#31867;&#39118;&#38505;&#12290;&#20026;&#20102;&#35828;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#20960;&#20010;&#20197;&#19978;&#25552;&#21040;&#30340;&#26032;&#26041;&#21521;&#30340;&#31034;&#20363;&#65292;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#26041;&#21521;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#31361;&#20986;&#20102;&#20351;&#29992;LLMs&#30340;MT&#26410;&#26469;&#30740;&#31350;&#30340;&#26426;&#20250;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Translation (MT) has made significant progress in recent years using deep learning, especially after the emergence of large language models (LLMs) such as GPT-3 and ChatGPT. This brings new challenges and opportunities for MT using LLMs. In this paper, we brainstorm some interesting directions for MT using LLMs, including stylized MT, interactive MT, and Translation Memory-based MT, as well as a new evaluation paradigm using LLMs. We also discuss the privacy concerns in MT using LLMs and a basic privacy-preserving method to mitigate such risks. To illustrate the potential of our proposed directions, we present several examples for the new directions mentioned above, demonstrating the feasibility of the proposed directions and highlight the opportunities and challenges for future research in MT using LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;ATCO2&#39033;&#30446;&#30340;&#32463;&#39564;&#25945;&#35757;&#65292;&#35813;&#39033;&#30446;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#21807;&#19968;&#24179;&#21488;&#65292;&#20197;&#23454;&#26102;&#26041;&#24335;&#25910;&#38598;&#21644;&#39044;&#22788;&#29702;&#22823;&#37327;&#30340;&#26469;&#33258;&#31354;&#22495;&#30340;ATC&#25968;&#25454;&#12290;&#35813;&#24179;&#21488;&#21487;&#20197;&#20316;&#20026;&#33719;&#24471;&#8220;&#26080;&#38480;&#28304;&#8221;&#30340;&#25968;&#25454;&#12290;&#20026;ATC&#39046;&#22495;&#30340;&#25968;&#25454;&#39537;&#21160;&#22411;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#24320;&#21457;&#25552;&#20379;&#20102;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.01155</link><description>&lt;p&gt;
ATCO2&#20013;&#30340;&#32463;&#39564;&#25945;&#35757;&#65306;5000&#23567;&#26102;&#30340;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#36890;&#20449;&#23545;&#20110;&#31283;&#20581;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#21644;&#29702;&#35299;&#12290; &#65288;arXiv&#65306;2305.01155v1 [eess.AS]&#65289;
&lt;/p&gt;
&lt;p&gt;
Lessons Learned in ATCO2: 5000 hours of Air Traffic Control Communications for Robust Automatic Speech Recognition and Understanding. (arXiv:2305.01155v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01155
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;ATCO2&#39033;&#30446;&#30340;&#32463;&#39564;&#25945;&#35757;&#65292;&#35813;&#39033;&#30446;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#21807;&#19968;&#24179;&#21488;&#65292;&#20197;&#23454;&#26102;&#26041;&#24335;&#25910;&#38598;&#21644;&#39044;&#22788;&#29702;&#22823;&#37327;&#30340;&#26469;&#33258;&#31354;&#22495;&#30340;ATC&#25968;&#25454;&#12290;&#35813;&#24179;&#21488;&#21487;&#20197;&#20316;&#20026;&#33719;&#24471;&#8220;&#26080;&#38480;&#28304;&#8221;&#30340;&#25968;&#25454;&#12290;&#20026;ATC&#39046;&#22495;&#30340;&#25968;&#25454;&#39537;&#21160;&#22411;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#24320;&#21457;&#25552;&#20379;&#20102;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#31649;&#21592;&#21644;&#39134;&#34892;&#21592;&#20043;&#38388;&#30340;&#35821;&#38899;&#20132;&#27969;&#23545;&#20110;&#30830;&#20445;&#23433;&#20840;&#21644;&#39640;&#25928;&#30340;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#65288;ATC&#65289;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#39033;&#20219;&#21153;&#38656;&#35201;&#31354;&#31649;&#21592;&#20855;&#26377;&#39640;&#24230;&#30340;&#35686;&#35273;&#24615;&#24182;&#19988;&#21487;&#33021;&#26159;&#32321;&#29712;&#21644;&#23481;&#26131;&#20986;&#38169;&#30340;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#23581;&#35797;&#23558;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#38598;&#25104;&#21040;ATC&#20013;&#65292;&#20197;&#20943;&#23569;&#31354;&#31649;&#21592;&#30340;&#24037;&#20316;&#37327;&#12290;&#28982;&#32780;&#65292;&#20026;ATC&#24320;&#21457;&#25968;&#25454;&#39537;&#21160;&#30340;AI&#31995;&#32479;&#38656;&#35201;&#22823;&#35268;&#27169;&#30340;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#32780;&#35813;&#39046;&#22495;&#30446;&#21069;&#23578;&#32570;&#20047;&#36825;&#20123;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;ATCO2&#39033;&#30446;&#30340;&#32463;&#39564;&#25945;&#35757;&#65292;&#35813;&#39033;&#30446;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#21807;&#19968;&#24179;&#21488;&#65292;&#20197;&#23454;&#26102;&#26041;&#24335;&#25910;&#38598;&#21644;&#39044;&#22788;&#29702;&#22823;&#37327;&#26469;&#33258;&#31354;&#22495;&#30340;ATC&#25968;&#25454;&#12290;&#36890;&#36807;&#25317;&#26377;&#32676;&#20247;&#24535;&#24895;&#32773;&#25317;&#26377;&#30340;VHF&#25509;&#25910;&#22120;&#20174;&#20844;&#24320;&#21487;&#35775;&#38382;&#30340;&#26080;&#32447;&#30005;&#39057;&#29575;&#36890;&#36947;&#25910;&#38598;&#38899;&#39057;&#21644;&#30417;&#35270;&#25968;&#25454;&#65292;&#28982;&#21518;&#19978;&#20256;&#21040;Opensky&#32593;&#32476;&#26381;&#21153;&#22120;&#65292;&#36825;&#21487;&#20197;&#34987;&#35270;&#20026;&#8220;&#26080;&#38480;&#28304;&#8221;&#30340;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#22238;&#39038;&#20102;ATCO2&#20197;&#21069;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Voice communication between air traffic controllers (ATCos) and pilots is critical for ensuring safe and efficient air traffic control (ATC). This task requires high levels of awareness from ATCos and can be tedious and error-prone. Recent attempts have been made to integrate artificial intelligence (AI) into ATC in order to reduce the workload of ATCos. However, the development of data-driven AI systems for ATC demands large-scale annotated datasets, which are currently lacking in the field. This paper explores the lessons learned from the ATCO2 project, a project that aimed to develop a unique platform to collect and preprocess large amounts of ATC data from airspace in real time. Audio and surveillance data were collected from publicly accessible radio frequency channels with VHF receivers owned by a community of volunteers and later uploaded to Opensky Network servers, which can be considered an "unlimited source" of data. In addition, this paper reviews previous work from ATCO2 pa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#28857;&#30740;&#31350;&#20102;&#36731;&#37327;&#21270;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#20020;&#24202;&#25991;&#26412;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#22312;RRS&#31034;&#20363;&#19978;&#36827;&#34892;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65292;&#23454;&#29616;&#36866;&#24212;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25918;&#23556;&#24615;&#25253;&#21578;&#25688;&#35201;&#65288;RRS&#65289;&#20219;&#21153;&#12290;&#24182;&#19988;&#35813;&#26041;&#27861;&#20165;&#24494;&#35843;&#27169;&#22411;&#30340;0.32&#65285;&#30340;&#21442;&#25968;&#65292;&#25552;&#39640;&#20102;&#34920;&#29616;&#12290;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#39046;&#22495;&#36866;&#24212;&#22312;RRS&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#20026;&#24320;&#21457;&#26356;&#22909;&#30340;&#25918;&#23556;&#24615;&#25253;&#21578;&#25688;&#35201;&#27169;&#22411;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.01146</link><description>&lt;p&gt;
RadAdapt&#65306;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36731;&#37327;&#21270;&#39046;&#22495;&#33258;&#36866;&#24212;&#23454;&#29616;&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
RadAdapt: Radiology Report Summarization via Lightweight Domain Adaptation of Large Language Models. (arXiv:2305.01146v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01146
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#28857;&#30740;&#31350;&#20102;&#36731;&#37327;&#21270;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#20020;&#24202;&#25991;&#26412;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#22312;RRS&#31034;&#20363;&#19978;&#36827;&#34892;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65292;&#23454;&#29616;&#36866;&#24212;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25918;&#23556;&#24615;&#25253;&#21578;&#25688;&#35201;&#65288;RRS&#65289;&#20219;&#21153;&#12290;&#24182;&#19988;&#35813;&#26041;&#27861;&#20165;&#24494;&#35843;&#27169;&#22411;&#30340;0.32&#65285;&#30340;&#21442;&#25968;&#65292;&#25552;&#39640;&#20102;&#34920;&#29616;&#12290;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#39046;&#22495;&#36866;&#24212;&#22312;RRS&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#20026;&#24320;&#21457;&#26356;&#22909;&#30340;&#25918;&#23556;&#24615;&#25253;&#21578;&#25688;&#35201;&#27169;&#22411;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#36731;&#37327;&#32423;&#31574;&#30053;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#65288;&#33258;&#28982;&#35821;&#35328;&#65292;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#65292;&#20020;&#24202;&#25991;&#26412;&#65289;&#21644;&#25552;&#31034;&#65288;&#38646;-shot&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#65289;&#25110;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;&#21069;&#32512;&#24494;&#35843;&#65292;LoRA&#65289;&#65292;&#26469;&#36866;&#24212;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#25918;&#23556;&#24615;&#25253;&#21578;&#25688;&#35201;&#65288;RRS&#65289;&#20219;&#21153;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#26368;&#22823;&#31243;&#24230;&#22320;&#36866;&#24212;&#20219;&#21153;&#30340;&#26041;&#27861;&#26159;&#65292;&#36890;&#36807;&#22312;&#20020;&#24202;&#25991;&#26412;&#19978;&#39044;&#20808;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;RRS&#31034;&#20363;&#19978;&#36827;&#34892;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#31181;&#26041;&#27861;&#20165;&#24494;&#35843;&#27169;&#22411;&#30340;0.32&#65285;&#30340;&#21442;&#25968;&#65292;&#19982;&#31471;&#23545;&#31471;&#24494;&#35843;&#65288;100&#65285;&#30340;&#21442;&#25968;&#65289;&#24418;&#25104;&#23545;&#27604;&#12290;&#27492;&#22806;&#65292;&#22312;&#30740;&#31350;&#19978;&#19979;&#25991;&#31034;&#20363;&#21644;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#35757;&#32451;&#30340;&#24433;&#21709;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#25918;&#23556;&#31185;&#21307;&#24072;&#35835;&#32773;&#30740;&#31350;&#21644;&#23450;&#24615;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#39046;&#22495;&#36866;&#24212;&#22312;RRS&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#20026;&#24320;&#21457;&#26356;&#22909;&#30340;&#25918;&#23556;&#24615;&#25253;&#21578;&#25688;&#35201;&#27169;&#22411;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
We systematically investigate lightweight strategies to adapt large language models (LLMs) for the task of radiology report summarization (RRS). Specifically, we focus on domain adaptation via pretraining (on natural language, biomedical text, and clinical text) and via prompting (zero-shot, in-context learning) or parameter-efficient fine-tuning (prefix tuning, LoRA). Our results on the MIMIC-III dataset consistently demonstrate best performance by maximally adapting to the task via pretraining on clinical text and parameter-efficient fine-tuning on RRS examples. Importantly, this method fine-tunes a mere 0.32% of parameters throughout the model, in contrast to end-to-end fine-tuning (100% of parameters). Additionally, we study the effect of in-context examples and out-of-distribution (OOD) training before concluding with a radiologist reader study and qualitative analysis. Our findings highlight the importance of domain adaptation in RRS and provide valuable insights toward developin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#23558;&#22522;&#20110;BERT&#30340;AI&#20195;&#29702;&#34701;&#20837;&#20154;&#31867;&#22242;&#38431;&#20013;&#65292;&#26469;&#21152;&#36895;&#20840;&#29699;&#21457;&#23637;&#35777;&#25454;&#32508;&#36848;&#20135;&#21697;&#30340;&#35774;&#35745;&#65307;&#21516;&#26102;&#36824;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#20027;&#21160;&#23398;&#20064;&#25277;&#26679;&#31574;&#30053;&#23545;&#20110;&#21327;&#20316;&#31579;&#36873;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;AI&#20195;&#29702;&#25972;&#21512;&#21040;&#20154;&#31867;&#22242;&#38431;&#20013;&#21487;&#20197;&#23558;&#31579;&#36873;&#25991;&#26723;&#30340;&#26102;&#38388;&#32553;&#30701;60&#65285;&#65292;&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#36824;&#21487;&#20197;&#23558;&#25928;&#29575;&#36827;&#19968;&#27493;&#25552;&#39640;20&#65285;&#12290;</title><link>http://arxiv.org/abs/2305.01145</link><description>&lt;p&gt;
ADVISE&#65306;AI&#21152;&#36895;&#20840;&#29699;&#21457;&#23637;&#35777;&#25454;&#32508;&#36848;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
ADVISE: AI-accelerated Design of Evidence Synthesis for Global Development. (arXiv:2305.01145v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01145
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#23558;&#22522;&#20110;BERT&#30340;AI&#20195;&#29702;&#34701;&#20837;&#20154;&#31867;&#22242;&#38431;&#20013;&#65292;&#26469;&#21152;&#36895;&#20840;&#29699;&#21457;&#23637;&#35777;&#25454;&#32508;&#36848;&#20135;&#21697;&#30340;&#35774;&#35745;&#65307;&#21516;&#26102;&#36824;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#20027;&#21160;&#23398;&#20064;&#25277;&#26679;&#31574;&#30053;&#23545;&#20110;&#21327;&#20316;&#31579;&#36873;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;AI&#20195;&#29702;&#25972;&#21512;&#21040;&#20154;&#31867;&#22242;&#38431;&#20013;&#21487;&#20197;&#23558;&#31579;&#36873;&#25991;&#26723;&#30340;&#26102;&#38388;&#32553;&#30701;60&#65285;&#65292;&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#36824;&#21487;&#20197;&#23558;&#25928;&#29575;&#36827;&#19968;&#27493;&#25552;&#39640;20&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35774;&#35745;&#22522;&#20110;&#35777;&#25454;&#30340;&#25919;&#31574;&#21644;&#35745;&#21010;&#26102;&#65292;&#20915;&#31574;&#32773;&#24517;&#39035;&#20174;&#22823;&#37327;&#19988;&#36805;&#36895;&#22686;&#38271;&#30340;&#25991;&#29486;&#20013;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#12290;&#20174;&#21407;&#22987;&#25628;&#32034;&#32467;&#26524;&#20013;&#30830;&#23450;&#30456;&#20851;&#25991;&#29486;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#26102;&#38388;&#21644;&#36164;&#28304;&#65292;&#24182;&#19988;&#36890;&#24120;&#26159;&#36890;&#36807;&#20154;&#24037;&#31579;&#36873;&#26469;&#23436;&#25104;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;BERT&#27169;&#22411;&#30340;AI&#20195;&#29702;&#65292;&#23558;&#20854;&#25972;&#21512;&#21040;&#19968;&#20010;&#20154;&#31867;&#22242;&#38431;&#20013;&#65292;&#35774;&#35745;&#19968;&#20010;&#20840;&#29699;&#21457;&#23637;&#35777;&#25454;&#32508;&#36848;&#20135;&#21697;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#20154;-AI&#28151;&#21512;&#22242;&#38431;&#22312;&#21152;&#36895;&#35777;&#25454;&#32508;&#36848;&#36807;&#31243;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#22242;&#38431;&#25928;&#29575;&#65292;&#25105;&#20204;&#36890;&#36807;&#20027;&#21160;&#23398;&#20064;(AL)&#22686;&#24378;&#20102;&#20154;-AI&#28151;&#21512;&#22242;&#38431;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#25277;&#26679;&#31574;&#30053;&#65292;&#21253;&#25324;&#38543;&#26426;&#25277;&#26679;&#65292;&#26368;&#23567;&#32622;&#20449;&#24230;(LC)&#25277;&#26679;&#21644;&#26368;&#39640;&#20248;&#20808;&#32423;(HP)&#25277;&#26679;&#65292;&#20197;&#30740;&#31350;&#23427;&#20204;&#23545;&#21327;&#20316;&#31579;&#36873;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#22522;&#20110;BERT&#30340;AI&#20195;&#29702;&#25972;&#21512;&#21040;&#20154;&#31867;&#22242;&#38431;&#20013;&#21487;&#20197;&#23558;&#31579;&#36873;&#25991;&#26723;&#30340;&#26102;&#38388;&#32553;&#30701;60&#65285;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#20934;&#30830;&#24230;&#12290;&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#36824;&#21487;&#20197;&#23558;&#25928;&#29575;&#36827;&#19968;&#27493;&#25552;&#39640;20&#65285;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#23637;&#31034;&#20102;AI&#22312;&#21152;&#36895;&#20840;&#29699;&#21457;&#23637;&#35777;&#25454;&#32508;&#36848;&#35774;&#35745;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
When designing evidence-based policies and programs, decision-makers must distill key information from a vast and rapidly growing literature base. Identifying relevant literature from raw search results is time and resource intensive, and is often done by manual screening. In this study, we develop an AI agent based on a bidirectional encoder representations from transformers (BERT) model and incorporate it into a human team designing an evidence synthesis product for global development. We explore the effectiveness of the human-AI hybrid team in accelerating the evidence synthesis process. To further improve team efficiency, we enhance the human-AI hybrid team through active learning (AL). Specifically, we explore different sampling strategies, including random sampling, least confidence (LC) sampling, and highest priority (HP) sampling, to study their influence on the collaborative screening process. Results show that incorporating the BERT-based AI agent into the human team can redu
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#24076;&#33098;&#35821;&#35821;&#35328;&#23398;&#20013;&#30340;&#38382;&#39064;&#65292;&#25104;&#21151;&#21033;&#29992;BERT&#27169;&#22411;&#21457;&#29616;&#21644;&#32416;&#27491;&#20102;&#25220;&#20889;&#21592;&#22312;&#25991;&#26412;&#20256;&#36882;&#36807;&#31243;&#20013;&#26410;&#34987;&#21457;&#29616;&#30340;&#38169;&#35823;&#65292;&#24182;&#33021;&#22312;&#20462;&#22797;&#39044;&#29616;&#20195;&#25163;&#31295;&#26448;&#26009;&#32769;&#21270;&#24341;&#36215;&#30340;&#20449;&#24687;&#32570;&#22833;&#26041;&#38754;&#21457;&#25381;&#20316;&#29992;&#12290;&#21516;&#26102;&#65292;&#22312;&#39046;&#22495;&#19987;&#23478;&#19982;&#27169;&#22411;&#21512;&#20316;&#26102;&#65292;&#26368;&#20339;&#24615;&#33021;&#21487;&#20197;&#36890;&#36807;&#21551;&#31034;&#24615;&#24314;&#35758;&#23454;&#29616;&#12290;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#22836;&#20284;&#20046;&#32534;&#30721;&#20102;&#39044;&#29616;&#20195;&#24076;&#33098;&#35821;&#30340;&#36873;&#25321;&#24615;&#35821;&#27861;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2305.01099</link><description>&lt;p&gt;
Logion&#65306;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24076;&#33098;&#35821;&#35821;&#35328;&#23398;
&lt;/p&gt;
&lt;p&gt;
Logion: Machine Learning for Greek Philology. (arXiv:2305.01099v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01099
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#24076;&#33098;&#35821;&#35821;&#35328;&#23398;&#20013;&#30340;&#38382;&#39064;&#65292;&#25104;&#21151;&#21033;&#29992;BERT&#27169;&#22411;&#21457;&#29616;&#21644;&#32416;&#27491;&#20102;&#25220;&#20889;&#21592;&#22312;&#25991;&#26412;&#20256;&#36882;&#36807;&#31243;&#20013;&#26410;&#34987;&#21457;&#29616;&#30340;&#38169;&#35823;&#65292;&#24182;&#33021;&#22312;&#20462;&#22797;&#39044;&#29616;&#20195;&#25163;&#31295;&#26448;&#26009;&#32769;&#21270;&#24341;&#36215;&#30340;&#20449;&#24687;&#32570;&#22833;&#26041;&#38754;&#21457;&#25381;&#20316;&#29992;&#12290;&#21516;&#26102;&#65292;&#22312;&#39046;&#22495;&#19987;&#23478;&#19982;&#27169;&#22411;&#21512;&#20316;&#26102;&#65292;&#26368;&#20339;&#24615;&#33021;&#21487;&#20197;&#36890;&#36807;&#21551;&#31034;&#24615;&#24314;&#35758;&#23454;&#29616;&#12290;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#22836;&#20284;&#20046;&#32534;&#30721;&#20102;&#39044;&#29616;&#20195;&#24076;&#33098;&#35821;&#30340;&#36873;&#25321;&#24615;&#35821;&#27861;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#24076;&#33098;&#35821;&#35821;&#35328;&#23398;&#20013;&#30340;&#21508;&#31181;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22312;&#36804;&#20170;&#20026;&#27490;&#29992;&#20110;&#27492;&#30446;&#30340;&#30340;&#26368;&#22823;&#39044;&#29616;&#20195;&#24076;&#33098;&#35821;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;BERT&#27169;&#22411;&#65292;&#21457;&#29616;&#24182;&#32416;&#27491;&#20102;&#25220;&#20889;&#21592;&#22312;&#25991;&#26412;&#20256;&#36882;&#36807;&#31243;&#20013;&#26410;&#34987;&#21457;&#29616;&#30340;&#38169;&#35823;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#22312;&#22635;&#34917;&#30001;&#20110;&#39044;&#29616;&#20195;&#25163;&#31295;&#26448;&#26009;&#32769;&#21270;&#23548;&#33268;&#30340;&#32570;&#21475;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#23558;&#20854;&#24615;&#33021;&#19982;&#39046;&#22495;&#19987;&#23478;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;&#39046;&#22495;&#19987;&#23478;&#24471;&#21040;&#27169;&#22411;&#30340;&#21551;&#31034;&#24615;&#24314;&#35758;&#26102;&#65292;&#25165;&#33021;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#12290;&#32771;&#34385;&#21040;&#36825;&#31181;&#20154;&#19982;&#35745;&#31639;&#26426;&#30340;&#21512;&#20316;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#21457;&#29616;&#19968;&#20123;&#27880;&#24847;&#21147;&#22836;&#20284;&#20046;&#32534;&#30721;&#20102;&#39044;&#29616;&#20195;&#24076;&#33098;&#35821;&#30340;&#36873;&#25321;&#24615;&#35821;&#27861;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents machine-learning methods to address various problems in Greek philology. After training a BERT model on the largest premodern Greek dataset used for this purpose to date, we identify and correct previously undetected errors made by scribes in the process of textual transmission, in what is, to our knowledge, the first successful identification of such errors via machine learning. Additionally, we demonstrate the model's capacity to fill gaps caused by material deterioration of premodern manuscripts and compare the model's performance to that of a domain expert. We find that best performance is achieved when the domain expert is provided with model suggestions for inspiration. With such human-computer collaborations in mind, we explore the model's interpretability and find that certain attention heads appear to encode select grammatical features of premodern Greek.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;BERT&#27169;&#22411;&#26469;&#26631;&#27880;&#20398;&#36785;&#24615;&#25991;&#26412;&#20013;&#30340;&#27880;&#37322;&#32773;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#27604;&#36739;&#30452;&#25509;&#35757;&#32451;&#21644;&#32858;&#21512;&#20004;&#31181;&#26041;&#27861;&#65292;&#32467;&#26524;&#21457;&#29616;&#32858;&#21512;&#26041;&#27861;&#27604;&#30452;&#25509;&#35757;&#32451;&#26377;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.01050</link><description>&lt;p&gt;
SemEval-2023 &#20219;&#21153;11&#20013;&#30340;SafeWebUH&#65306;&#23398;&#20064;&#20398;&#36785;&#24615;&#25991;&#26412;&#30340;&#27880;&#37322;&#32773;&#19981;&#19968;&#33268;&#24615;&#65306; &#30452;&#25509;&#35757;&#32451;&#19982;&#32858;&#21512;&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
SafeWebUH at SemEval-2023 Task 11: Learning Annotator Disagreement in Derogatory Text: Comparison of Direct Training vs Aggregation. (arXiv:2305.01050v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;BERT&#27169;&#22411;&#26469;&#26631;&#27880;&#20398;&#36785;&#24615;&#25991;&#26412;&#20013;&#30340;&#27880;&#37322;&#32773;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#27604;&#36739;&#30452;&#25509;&#35757;&#32451;&#21644;&#32858;&#21512;&#20004;&#31181;&#26041;&#27861;&#65292;&#32467;&#26524;&#21457;&#29616;&#32858;&#21512;&#26041;&#27861;&#27604;&#30452;&#25509;&#35757;&#32451;&#26377;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#35266;&#24615;&#21644;&#19981;&#21516;&#24847;&#35265;&#26159;&#20851;&#38190;&#30340;&#31038;&#20250;&#29616;&#35937;&#65292;&#32771;&#34385;&#21040;&#36825;&#19968;&#28857;&#22312;&#27880;&#37322;&#21644;&#26816;&#27979;&#20398;&#36785;&#24615;&#25991;&#26412;&#20869;&#23481;&#30340;&#36807;&#31243;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20351;&#29992;SemEval-2023&#20219;&#21153;11&#25552;&#20379;&#30340;&#22235;&#20010;&#25968;&#25454;&#38598;&#65292;&#23545;BERT&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#25429;&#25417;&#27880;&#37322;&#20013;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#20010;&#20307;&#27880;&#37322;&#32773;&#24314;&#27169;&#21644;&#32858;&#21512;&#23558;&#20132;&#21449;&#29109;&#24471;&#20998;&#24179;&#22343;&#38477;&#20302;&#20102;0.21&#65292;&#32780;&#19982;&#30452;&#25509;&#35757;&#32451;&#36719;&#26631;&#31614;&#30456;&#27604;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#27880;&#37322;&#32773;&#20803;&#25968;&#25454;&#23545;&#24179;&#22343;&#20132;&#21449;&#29109;&#20998;&#25968;&#30340;0.029&#38477;&#20302;&#26377;&#25152;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Subjectivity and difference of opinion are key social phenomena, and it is crucial to take these into account in the annotation and detection process of derogatory textual content. In this paper, we use four datasets provided by SemEval-2023 Task 11 and fine-tune a BERT model to capture the disagreement in the annotation. We find individual annotator modeling and aggregation lowers the Cross-Entropy score by an average of 0.21, compared to the direct training on the soft labels. Our findings further demonstrate that annotator metadata contributes to the average 0.029 reduction in the Cross-Entropy score.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#20844;&#21496;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#31616;&#21270;&#20844;&#21496;&#20998;&#31867;&#36807;&#31243;&#65292;&#20174;&#32780;&#20943;&#23569;&#20256;&#32479;&#26041;&#27861;&#22914;&#20840;&#29699;&#20135;&#19994;&#20998;&#31867;&#26631;&#20934;&#65288;GICS&#65289;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2305.01028</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#23398;&#20064;&#22312;&#20844;&#21496;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Company classification using zero-shot learning. (arXiv:2305.01028v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#20844;&#21496;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#31616;&#21270;&#20844;&#21496;&#20998;&#31867;&#36807;&#31243;&#65292;&#20174;&#32780;&#20943;&#23569;&#20256;&#32479;&#26041;&#27861;&#22914;&#20840;&#29699;&#20135;&#19994;&#20998;&#31867;&#26631;&#20934;&#65288;GICS&#65289;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#35768;&#22810;&#21830;&#19994;&#24212;&#29992;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#21253;&#25324;&#24773;&#24863;&#20998;&#26512;&#12289;&#25991;&#26412;&#20998;&#31867;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#20844;&#21496;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#20174;&#20844;&#21496;&#25551;&#36848;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#28982;&#21518;&#24212;&#29992;&#38646;&#26679;&#26412;&#23398;&#20064;&#23558;&#20844;&#21496;&#20998;&#31867;&#21040;&#30456;&#20851;&#31867;&#21035;&#65292;&#26080;&#38656;&#20026;&#27599;&#20010;&#31867;&#21035;&#25552;&#20379;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;&#20844;&#21496;&#25991;&#26412;&#25551;&#36848;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#23427;&#21487;&#20197;&#31616;&#21270;&#20844;&#21496;&#20998;&#31867;&#36807;&#31243;&#65292;&#20174;&#32780;&#20943;&#23569;&#20256;&#32479;&#26041;&#27861;&#22914;&#20840;&#29699;&#20135;&#19994;&#20998;&#31867;&#26631;&#20934;&#65288;GICS&#65289;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#36164;&#28304;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#33258;&#21160;&#21270;&#20844;&#21496;&#20998;&#31867;&#30340;&#28508;&#21147;&#65292;&#26159;&#26410;&#26469;&#30740;&#31350;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, natural language processing (NLP) has become increasingly important in a variety of business applications, including sentiment analysis, text classification, and named entity recognition. In this paper, we propose an approach for company classification using NLP and zero-shot learning. Our method utilizes pre-trained transformer models to extract features from company descriptions, and then applies zero-shot learning to classify companies into relevant categories without the need for specific training data for each category. We evaluate our approach on publicly available datasets of textual descriptions of companies, and demonstrate that it can streamline the process of company classification, thereby reducing the time and resources required in traditional approaches such as the Global Industry Classification Standard (GICS). The results show that this method has potential for automation of company classification, making it a promising avenue for future research in thi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#35821;&#29992;&#35805;&#35821;&#30340;&#33021;&#21147;&#12290;&#20316;&#32773;&#36890;&#36807;&#34920;&#36848;&#26377;&#20851;&#31561;&#32423;&#24418;&#23481;&#35789;&#8220;&#24378;&#8221;&#30340;&#38376;&#27099;&#20272;&#35745;&#26469;&#27979;&#35797; LLM &#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616; LLM &#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#19978;&#19979;&#25991;&#20381;&#36182;&#24615;&#12289;&#31867;&#20154;&#30340;&#20998;&#24067;&#65292;&#20294;&#22312;&#32452;&#21512;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2305.01020</link><description>&lt;p&gt;
&#23558;&#32479;&#35745;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35821;&#29992;&#25512;&#29702;&#30340;&#35780;&#20272;&#22120;
&lt;/p&gt;
&lt;p&gt;
Evaluating statistical language models as pragmatic reasoners. (arXiv:2305.01020v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01020
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#35821;&#29992;&#35805;&#35821;&#30340;&#33021;&#21147;&#12290;&#20316;&#32773;&#36890;&#36807;&#34920;&#36848;&#26377;&#20851;&#31561;&#32423;&#24418;&#23481;&#35789;&#8220;&#24378;&#8221;&#30340;&#38376;&#27099;&#20272;&#35745;&#26469;&#27979;&#35797; LLM &#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616; LLM &#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#19978;&#19979;&#25991;&#20381;&#36182;&#24615;&#12289;&#31867;&#20154;&#30340;&#20998;&#24067;&#65292;&#20294;&#22312;&#32452;&#21512;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27807;&#36890;&#35821;&#35328;&#21644;&#39044;&#26399;&#24847;&#20041;&#20043;&#38388;&#30340;&#20851;&#31995;&#36890;&#24120;&#26159;&#27010;&#29575;&#24615;&#30340;&#65292;&#32780;&#19988;&#23545;&#35821;&#22659;&#38750;&#24120;&#25935;&#24863;&#12290;&#35768;&#22810;&#31574;&#30053;&#35797;&#22270;&#20272;&#35745;&#36825;&#31181;&#26144;&#23556;&#65292;&#36890;&#24120;&#21033;&#29992;&#22522;&#20110;&#36125;&#21494;&#26031;&#36882;&#24402;&#27169;&#22411;&#30340;&#36890;&#20449;&#26041;&#24335;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#34987;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#35821;&#20041;&#20998;&#26512;&#24212;&#29992;&#31243;&#24207;&#65292;&#20219;&#21153;&#26159;&#20174;&#33258;&#28982;&#35821;&#35328;&#25512;&#26029;&#36923;&#36753;&#34920;&#31034;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340; LLM &#25506;&#32034;&#20027;&#35201;&#23616;&#38480;&#20110;&#23383;&#38754;&#19978;&#30340;&#35821;&#35328;&#20351;&#29992;&#65292;&#20294;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102; LLM &#25512;&#26029;&#35821;&#29992;&#35805;&#35821;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22522;&#20110;&#8220;&#24378;&#8221;&#36825;&#20010;&#31561;&#32423;&#24418;&#23481;&#35789;&#30340;&#38376;&#27099;&#20272;&#35745;&#30340;&#24773;&#20917;&#65292;&#20174;&#20855;&#26377;&#24378;&#24230;&#20808;&#39564;&#26465;&#20214;&#30340;&#35821;&#22659;&#20986;&#21457;&#65292;&#28982;&#21518;&#25193;&#23637;&#21040;&#38480;&#23450;&#12289;&#21542;&#23450;&#12289;&#26497;&#24615;&#21453;&#36716;&#21644;&#31867;&#27604;&#32452;&#21512;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;LLMs &#21487;&#20197;&#25512;&#23548;&#20986;&#19982;&#35821;&#22659;&#30456;&#20851;&#30340;&#31867;&#20154;&#20998;&#24067;&#65292;&#28041;&#21450;&#21040;&#20960;&#20010;&#22797;&#26434;&#35821;&#29992;&#35805;&#35821;&#30340;&#35299;&#37322;&#65292;&#20294;&#22312;&#32452;&#21512;&#19978;&#36824;&#26377;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
The relationship between communicated language and intended meaning is often probabilistic and sensitive to context. Numerous strategies attempt to estimate such a mapping, often leveraging recursive Bayesian models of communication. In parallel, large language models (LLMs) have been increasingly applied to semantic parsing applications, tasked with inferring logical representations from natural language. While existing LLM explorations have been largely restricted to literal language use, in this work, we evaluate the capacity of LLMs to infer the meanings of pragmatic utterances. Specifically, we explore the case of threshold estimation on the gradable adjective ``strong'', contextually conditioned on a strength prior, then extended to composition with qualification, negation, polarity inversion, and class comparison. We find that LLMs can derive context-grounded, human-like distributions over the interpretations of several complex pragmatic utterances, yet struggle composing with n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20013;&#38388;&#23618;&#34920;&#31034;&#30340;&#29305;&#24449;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#36719;&#22495;&#36716;&#31227;&#36827;&#34892;&#39046;&#22495;&#38388;&#30340;&#20851;&#32852;&#65292;&#25552;&#39640;&#27450;&#39575;&#26816;&#27979;&#30340;&#20934;&#30830;&#29575;&#65292;&#20998;&#26512;&#32467;&#26524;&#26174;&#31034;&#25512;&#25991;&#26159;&#26816;&#27979;&#20551;&#26032;&#38395;&#21644;&#38035;&#40060;&#30005;&#23376;&#37038;&#20214;&#26368;&#26377;&#24110;&#21161;&#30340;&#20449;&#24687;&#25552;&#20379;&#32773;&#65292;&#26032;&#38395;&#22312;&#25512;&#29305;&#35875;&#35328;&#26816;&#27979;&#20013;&#26368;&#26377;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2305.01011</link><description>&lt;p&gt;
&#22522;&#20110;&#36719;&#22495;&#36716;&#31227;&#30340;&#29305;&#24449;&#22686;&#24378;&#27450;&#39575;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Deception Detection with Feature-Augmentation by soft Domain Transfer. (arXiv:2305.01011v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20013;&#38388;&#23618;&#34920;&#31034;&#30340;&#29305;&#24449;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#36719;&#22495;&#36716;&#31227;&#36827;&#34892;&#39046;&#22495;&#38388;&#30340;&#20851;&#32852;&#65292;&#25552;&#39640;&#27450;&#39575;&#26816;&#27979;&#30340;&#20934;&#30830;&#29575;&#65292;&#20998;&#26512;&#32467;&#26524;&#26174;&#31034;&#25512;&#25991;&#26159;&#26816;&#27979;&#20551;&#26032;&#38395;&#21644;&#38035;&#40060;&#30005;&#23376;&#37038;&#20214;&#26368;&#26377;&#24110;&#21161;&#30340;&#20449;&#24687;&#25552;&#20379;&#32773;&#65292;&#26032;&#38395;&#22312;&#25512;&#29305;&#35875;&#35328;&#26816;&#27979;&#20013;&#26368;&#26377;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20449;&#24687;&#29190;&#28856;&#30340;&#36825;&#20010;&#26102;&#20195;&#65292;&#27450;&#39575;&#32773;&#21033;&#29992;&#19981;&#21516;&#30340;&#20449;&#24687;&#39046;&#22495;&#25110;&#23186;&#20171;&#26469;&#21033;&#29992;&#29992;&#25143;&#65292;&#27604;&#22914;&#26032;&#38395;&#12289;&#30005;&#23376;&#37038;&#20214;&#21644;&#25512;&#25991;&#31561;&#12290;&#23613;&#31649;&#24050;&#32463;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#30740;&#31350;&#26469;&#26816;&#27979;&#36825;&#20123;&#39046;&#22495;&#20013;&#30340;&#27450;&#39575;&#65292;&#20294;&#26032;&#20107;&#20214;&#20013;&#20449;&#24687;&#30340;&#30701;&#32570;&#38656;&#35201;&#36825;&#20123;&#39046;&#22495;&#30456;&#20114;&#20851;&#32852;&#26469;&#23545;&#25239;&#27450;&#39575;&#12290;&#20026;&#20102;&#24418;&#25104;&#36825;&#31181;&#20851;&#32852;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#31070;&#32463;&#27169;&#22411;&#30340;&#20013;&#38388;&#23618;&#34920;&#31034;&#36827;&#34892;&#29305;&#24449;&#22686;&#24378;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#33258;&#36523;&#39046;&#22495;&#22522;&#32447;&#27169;&#22411;&#25552;&#39640;&#20102;&#22810;&#36798;6.60%&#30340;&#20934;&#30830;&#29575;&#12290;&#25105;&#20204;&#21457;&#29616;&#25512;&#25991;&#26159;&#26816;&#27979;&#20551;&#26032;&#38395;&#21644;&#38035;&#40060;&#30005;&#23376;&#37038;&#20214;&#26368;&#26377;&#24110;&#21161;&#30340;&#20449;&#24687;&#25552;&#20379;&#32773;&#65292;&#32780;&#26032;&#38395;&#22312;&#25512;&#29305;&#35875;&#35328;&#26816;&#27979;&#20013;&#26368;&#26377;&#24110;&#21161;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#23545;&#20110;&#22495;&#30693;&#35782;&#36716;&#31227;&#30340;&#26377;&#29992;&#27934;&#35265;&#65292;&#21487;&#20197;&#24110;&#21161;&#24314;&#31435;&#27604;&#29616;&#26377;&#25991;&#29486;&#26356;&#24378;&#22823;&#30340;&#27450;&#39575;&#26816;&#27979;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this era of information explosion, deceivers use different domains or mediums of information to exploit the users, such as News, Emails, and Tweets. Although numerous research has been done to detect deception in all these domains, information shortage in a new event necessitates these domains to associate with each other to battle deception. To form this association, we propose a feature augmentation method by harnessing the intermediate layer representation of neural models. Our approaches provide an improvement over the self-domain baseline models by up to 6.60%. We find Tweets to be the most helpful information provider for Fake News and Phishing Email detection, whereas News helps most in Tweet Rumor detection. Our analysis provides a useful insight for domain knowledge transfer which can help build a stronger deception detection system than the existing literature.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;TMR&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#25991;&#26412;&#36716;&#25442;&#20026;3D&#20154;&#20307;&#36816;&#21160;&#12290;&#23427;&#22312;&#20808;&#21069;&#30340;&#24037;&#20316;&#20013;&#21462;&#24471;&#20102;&#26126;&#26174;&#30340;&#20248;&#21183;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#23545;&#27604;&#24615;&#25439;&#22833;&#30340;&#26041;&#27861;&#26469;&#26356;&#22909;&#22320;&#24314;&#31435;&#36328;&#27169;&#24577;&#28508;&#22312;&#31354;&#38388;&#32467;&#26500;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20445;&#25345;&#36816;&#21160;&#29983;&#25104;&#25439;&#22833;&#21644;&#23545;&#27604;&#24615;&#35757;&#32451;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2305.00976</link><description>&lt;p&gt;
TMR:&#20351;&#29992;&#23545;&#27604;3D&#20154;&#20307;&#36816;&#21160;&#21512;&#25104;&#30340;&#25991;&#26412;&#21040;&#36816;&#21160;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
TMR: Text-to-Motion Retrieval Using Contrastive 3D Human Motion Synthesis. (arXiv:2305.00976v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;TMR&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#25991;&#26412;&#36716;&#25442;&#20026;3D&#20154;&#20307;&#36816;&#21160;&#12290;&#23427;&#22312;&#20808;&#21069;&#30340;&#24037;&#20316;&#20013;&#21462;&#24471;&#20102;&#26126;&#26174;&#30340;&#20248;&#21183;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#23545;&#27604;&#24615;&#25439;&#22833;&#30340;&#26041;&#27861;&#26469;&#26356;&#22909;&#22320;&#24314;&#31435;&#36328;&#27169;&#24577;&#28508;&#22312;&#31354;&#38388;&#32467;&#26500;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20445;&#25345;&#36816;&#21160;&#29983;&#25104;&#25439;&#22833;&#21644;&#23545;&#27604;&#24615;&#35757;&#32451;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TMR&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#25991;&#26412;&#36716;&#25442;&#20026;3D&#20154;&#20307;&#36816;&#21160;&#12290;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#20165;&#23558;&#26816;&#32034;&#35270;&#20026;&#20195;&#29702;&#35780;&#20272;&#25351;&#26631;&#19981;&#21516;&#65292;&#25105;&#20204;&#23558;&#20854;&#20316;&#20026;&#19968;&#20010;&#29420;&#31435;&#30340;&#20219;&#21153;&#26469;&#35299;&#20915;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25193;&#23637;&#20102;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#21040;&#21160;&#20316;&#21512;&#25104;&#27169;&#22411;TEMOS&#65292;&#24182;&#32467;&#21512;&#23545;&#27604;&#25439;&#22833;&#26469;&#26356;&#22909;&#22320;&#26500;&#36896;&#36328;&#27169;&#24577;&#30340;&#28508;&#22312;&#31354;&#38388;&#12290;&#25105;&#20204;&#34920;&#26126;&#20445;&#25345;&#36816;&#21160;&#29983;&#25104;&#25439;&#22833;&#21644;&#23545;&#27604;&#24615;&#35757;&#32451;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#33719;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20934;&#26469;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#36890;&#36807;&#25253;&#21578;&#20960;&#20010;&#21327;&#35758;&#30340;&#32467;&#26524;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#25105;&#20204;&#22312;KIT-ML&#21644;HumanML3D&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;TMR&#27604;&#20808;&#21069;&#30340;&#24037;&#20316;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#20248;&#21183;&#65292;&#20363;&#22914;&#23558;&#20013;&#20301;&#25968;&#25490;&#21517;&#20174;54&#38477;&#33267;19&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#26102;&#21051;&#26816;&#32034;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#27169;&#22411;&#26159;&#20844;&#24320;&#21487;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present TMR, a simple yet effective approach for text to 3D human motion retrieval. While previous work has only treated retrieval as a proxy evaluation metric, we tackle it as a standalone task. Our method extends the state-of-the-art text-to-motion synthesis model TEMOS, and incorporates a contrastive loss to better structure the cross-modal latent space. We show that maintaining the motion generation loss, along with the contrastive training, is crucial to obtain good performance. We introduce a benchmark for evaluation and provide an in-depth analysis by reporting results on several protocols. Our extensive experiments on the KIT-ML and HumanML3D datasets show that TMR outperforms the prior work by a significant margin, for example reducing the median rank from 54 to 19. Finally, we showcase the potential of our approach on moment retrieval. Our code and models are publicly available.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#25105;&#35780;&#20272;&#24341;&#23548;&#35299;&#30721;&#25552;&#39640;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#32463;&#36807;&#26657;&#20934;&#30340;&#33258;&#21160;&#26631;&#20934;&#25506;&#32034;&#25512;&#29702;&#25628;&#32034;&#31354;&#38388;&#65292;&#20351;&#25628;&#32034;&#33021;&#22815;&#20135;&#29983;&#26356;&#39640;&#36136;&#37327;&#30340;&#26368;&#32456;&#39044;&#27979;&#32467;&#26524;&#65307;&#20351;&#29992;&#33258;&#25105;&#35780;&#20272;&#24341;&#23548;&#30340;&#38543;&#26426;&#26463;&#25628;&#32034;&#22312;&#20135;&#29983;&#25512;&#29702;&#38142;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#20043;&#38388;&#24179;&#34913;&#26435;&#34913;&#65292;&#36866;&#24212;&#22810;&#25968;&#25237;&#31080;&#65292;&#24182;&#19988;&#21487;&#20197;&#20934;&#30830;&#21028;&#26029;&#36923;&#36753;&#38169;&#35823;&#65292;&#25552;&#39640;&#19968;&#33268;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.00633</link><description>&lt;p&gt;
&#20998;&#35299;&#22686;&#24378;&#25512;&#29702;&#30340;&#33258;&#25105;&#35780;&#20272;&#24341;&#23548;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Decomposition Enhances Reasoning via Self-Evaluation Guided Decoding. (arXiv:2305.00633v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#25105;&#35780;&#20272;&#24341;&#23548;&#35299;&#30721;&#25552;&#39640;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#32463;&#36807;&#26657;&#20934;&#30340;&#33258;&#21160;&#26631;&#20934;&#25506;&#32034;&#25512;&#29702;&#25628;&#32034;&#31354;&#38388;&#65292;&#20351;&#25628;&#32034;&#33021;&#22815;&#20135;&#29983;&#26356;&#39640;&#36136;&#37327;&#30340;&#26368;&#32456;&#39044;&#27979;&#32467;&#26524;&#65307;&#20351;&#29992;&#33258;&#25105;&#35780;&#20272;&#24341;&#23548;&#30340;&#38543;&#26426;&#26463;&#25628;&#32034;&#22312;&#20135;&#29983;&#25512;&#29702;&#38142;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#20043;&#38388;&#24179;&#34913;&#26435;&#34913;&#65292;&#36866;&#24212;&#22810;&#25968;&#25237;&#31080;&#65292;&#24182;&#19988;&#21487;&#20197;&#20934;&#30830;&#21028;&#26029;&#36923;&#36753;&#38169;&#35823;&#65292;&#25552;&#39640;&#19968;&#33268;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#26463;&#25628;&#32034;&#32467;&#21512;&#33258;&#25105;&#35780;&#20272;&#24341;&#23548;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#32463;&#36807;&#26657;&#20934;&#30340;&#33258;&#21160;&#26631;&#20934;&#25506;&#32034;&#25512;&#29702;&#25628;&#32034;&#31354;&#38388;&#12290;&#36825;&#20351;&#24471;&#26377;&#25928;&#25628;&#32034;&#33021;&#22815;&#20135;&#29983;&#26356;&#39640;&#36136;&#37327;&#30340;&#26368;&#32456;&#39044;&#27979;&#32467;&#26524;&#12290;&#20351;&#29992;&#33258;&#25105;&#35780;&#20272;&#24341;&#23548;&#30340;&#38543;&#26426;&#26463;&#25628;&#32034;&#65292;&#25105;&#20204;&#22312;&#20135;&#29983;&#25512;&#29702;&#38142;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#20043;&#38388;&#24179;&#34913;&#26435;&#34913;&#65292;&#20174;&#32780;&#33021;&#22815;&#36866;&#24212;&#22810;&#25968;&#25237;&#31080;&#65292;&#24182;&#22312;GSM8K&#12289;AQUA&#21644;StrategyQA&#22522;&#20934;&#27979;&#35797;&#20013;&#20197;&#23569;&#37327;&#31034;&#20363;&#20934;&#30830;&#24615;&#20998;&#21035;&#36229;&#36234;&#23545;&#24212;&#30340;Codex-backboned&#22522;&#32447;$6.34\%$&#12289;$9.56\%$&#21644;$5.46\%$&#12290;&#23545;&#25105;&#20204;&#30340;&#20998;&#35299;&#24335;&#25512;&#29702;&#20998;&#26512;&#21457;&#29616;&#65292;&#23427;&#21487;&#20197;&#25351;&#20986;&#36923;&#36753;&#38169;&#35823;&#24182;&#23548;&#33268;&#26356;&#39640;&#30340;&#19968;&#33268;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an effective prompting approach that integrates self-evaluation guidance through stochastic beam search. Our approach explores the reasoning search space using a well-calibrated automatic criterion. This enables an efficient search to produce higher-quality final predictions. With the self-evaluation guided stochastic beam search, we also balance the quality--diversity trade-off in the generation of reasoning chains. This allows our approach to adapt well with majority voting and surpass the corresponding Codex-backboned baselines by $6.34\%$, $9.56\%$, and $5.46\%$ on the GSM8K, AQUA, and StrategyQA benchmarks, respectively, in few-shot accuracy. Analysis of our decompositional reasoning finds it pinpoints logic failures and leads to higher consistency and robustness.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20026;&#23545;Kauhanen&#12289;Einhaus&#21644;Walkden&#65288;2023&#65289;&#30340;&#22238;&#24212;&#65292;&#20173;&#28982;&#27809;&#26377;&#35777;&#25454;&#34920;&#26126;&#22823;&#37327;&#30340;L2&#29992;&#25143;&#24433;&#21709;&#35821;&#35328;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.00217</link><description>&lt;p&gt;
&#23545;Kauhanen&#12289;Einhaus&#21644;Walkden&#65288;2023&#24180;&#65289;&#30340;&#22238;&#24212;&#65306;&#20173;&#28982;&#27809;&#26377;&#35777;&#25454;&#35777;&#26126;&#38750;&#27597;&#35821;&#29992;&#25143;&#27604;&#20363;&#23545;&#35821;&#35328;&#22797;&#26434;&#24230;&#26377;&#24433;&#21709;&#65288;arXiv:2305.00217v1 [cs.CL]&#65289;
&lt;/p&gt;
&lt;p&gt;
Still no evidence for an effect of the proportion of non-native speakers on language complexity -- A response to Kauhanen, Einhaus &amp; Walkden (2023). (arXiv:2305.00217v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20026;&#23545;Kauhanen&#12289;Einhaus&#21644;Walkden&#65288;2023&#65289;&#30340;&#22238;&#24212;&#65292;&#20173;&#28982;&#27809;&#26377;&#35777;&#25454;&#34920;&#26126;&#22823;&#37327;&#30340;L2&#29992;&#25143;&#24433;&#21709;&#35821;&#35328;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22312;&#12298;&#35821;&#35328;&#36827;&#21270;&#26434;&#24535;&#12299;&#21457;&#34920;&#30340;&#19968;&#31687;&#35770;&#25991;&#20013;&#65292;Kauhanen&#12289;Einhaus&#21644;Walkden&#65288;https://doi.org/10.1093/jole/lzad005&#65292;KEW&#65289;&#25361;&#25112;&#20102;&#25105;&#22312;&#19968;&#31687;&#35770;&#25991;&#20013;&#65288;Koplenig&#65292;Royal Society Open Science&#65292;6&#65292;181274&#65288;2019&#65289;&#65292;https://doi.org/10.1098/rsos.181274&#65289;&#25152;&#21576;&#29616;&#30340;&#32467;&#26524;&#12290;&#22312;&#35813;&#35770;&#25991;&#20013;&#65292;&#25105;&#35797;&#22270;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#32479;&#35745;&#20998;&#26512;&#26469;&#34920;&#26126;&#22823;&#37327;L2&#65288;&#31532;&#20108;&#35821;&#35328;&#65289;&#29992;&#25143;&#20284;&#20046;&#19981;&#20250;&#24433;&#21709;&#35821;&#35328;&#30340;&#65288;&#35821;&#27861;&#25110;&#32479;&#35745;&#65289;&#22797;&#26434;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#19987;&#27880;&#20110;Ethnologue&#35780;&#20272;&#35821;&#35328;&#22320;&#20301;&#30340;&#26041;&#24335;&#65306;&#22914;&#26524;&#19968;&#31181;&#35821;&#35328;&#38500;&#20102;&#34987;L1&#65288;&#31532;&#19968;&#35821;&#35328;&#65289;&#20351;&#29992;&#32773;&#20043;&#22806;&#65292;&#36824;&#24212;&#35813;&#26377;&#22823;&#37327;&#30340;L2&#20351;&#29992;&#32773;&#65292;&#37027;&#20040;&#35813;&#35821;&#35328;&#23601;&#34987;&#25551;&#36848;&#20026;&#20256;&#25773;&#24615;&#30340;&#12290;KEW&#25209;&#35780;&#20102;&#23558;&#20256;&#25773;&#24615;&#20316;&#20026;&#35821;&#35328;&#26159;&#21542;&#25317;&#26377;&#22823;&#37327;L2&#20351;&#29992;&#32773;&#65288;&#20108;&#20803;&#65289;&#25351;&#26631;&#30340;&#20351;&#29992;&#65292;&#20197;&#21450;&#22312;&#30452;&#25509;&#20272;&#35745;L2&#27604;&#20363;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;L2&#29992;&#25143;&#27604;&#20363;&#24402;&#20026;&#38750;&#20256;&#25773;&#24615;&#35821;&#35328;&#30340;&#24819;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a recent paper published in the Journal of Language Evolution, Kauhanen, Einhaus &amp; Walkden (https://doi.org/10.1093/jole/lzad005, KEW) challenge the results presented in one of my papers (Koplenig, Royal Society Open Science, 6, 181274 (2019), https://doi.org/10.1098/rsos.181274), in which I tried to show through a series of statistical analyses that large numbers of L2 (second language) speakers do not seem to affect the (grammatical or statistical) complexity of a language. To this end, I focus on the way in which the Ethnologue assesses language status: a language is characterised as vehicular if, in addition to being used by L1 (first language) speakers, it should also have a significant number of L2 users. KEW criticise both the use of vehicularity as a (binary) indicator of whether a language has a significant number of L2 users and the idea of imputing a zero proportion of L2 speakers to non-vehicular languages whenever a direct estimate of that proportion is unavailable. Whi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#22823;&#35821;&#35328;&#27169;&#22411;&#12289;&#25968;&#23383;&#23402;&#29983;&#21644;&#24037;&#19994;&#33258;&#21160;&#21270;&#31995;&#32479;&#30456;&#32467;&#21512;&#30340;&#26694;&#26550;&#65292;&#23454;&#29616;&#29983;&#20135;&#36807;&#31243;&#30340;&#26234;&#33021;&#21270;&#35268;&#21010;&#21644;&#25511;&#21046;&#12290;&#36890;&#36807;LLM&#20195;&#29702;&#30340;&#21327;&#35843;&#25511;&#21046;&#65292;&#23454;&#29616;&#20102;&#28789;&#27963;&#29983;&#20135;&#30340;&#33258;&#20027;&#35268;&#21010;&#21644;&#25511;&#21046;&#65292;&#33021;&#22815;&#22788;&#29702;&#26410;&#39044;&#23450;&#20041;&#30340;&#20219;&#21153;&#24182;&#35268;&#21010;&#29983;&#20135;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2304.14721</link><description>&lt;p&gt;
&#26397;&#33258;&#20027;&#31995;&#32479;&#36808;&#36827;&#65306;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#22686;&#24378;&#30340;&#28789;&#27963;&#27169;&#22359;&#21270;&#29983;&#20135;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Towards autonomous system: flexible modular production system enhanced with large language model agents. (arXiv:2304.14721v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#22823;&#35821;&#35328;&#27169;&#22411;&#12289;&#25968;&#23383;&#23402;&#29983;&#21644;&#24037;&#19994;&#33258;&#21160;&#21270;&#31995;&#32479;&#30456;&#32467;&#21512;&#30340;&#26694;&#26550;&#65292;&#23454;&#29616;&#29983;&#20135;&#36807;&#31243;&#30340;&#26234;&#33021;&#21270;&#35268;&#21010;&#21644;&#25511;&#21046;&#12290;&#36890;&#36807;LLM&#20195;&#29702;&#30340;&#21327;&#35843;&#25511;&#21046;&#65292;&#23454;&#29616;&#20102;&#28789;&#27963;&#29983;&#20135;&#30340;&#33258;&#20027;&#35268;&#21010;&#21644;&#25511;&#21046;&#65292;&#33021;&#22815;&#22788;&#29702;&#26410;&#39044;&#23450;&#20041;&#30340;&#20219;&#21153;&#24182;&#35268;&#21010;&#29983;&#20135;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#25968;&#23383;&#23402;&#29983;&#21644;&#24037;&#19994;&#33258;&#21160;&#21270;&#31995;&#32479;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#29983;&#20135;&#36807;&#31243;&#30340;&#26234;&#33021;&#35268;&#21010;&#21644;&#25511;&#21046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#24320;&#21457;&#21253;&#21547;&#29983;&#20135;&#25551;&#36848;&#20449;&#24687;&#30340;&#25968;&#23383;&#23402;&#29983;&#31995;&#32479;&#65292;&#24182;&#23558;&#33258;&#21160;&#21270;&#31995;&#32479;&#25913;&#36896;&#20026;&#25552;&#20379;&#32479;&#19968;&#25509;&#21475;&#30340;&#32454;&#31890;&#24230;&#21151;&#33021;&#25110;&#27169;&#22359;&#65292;&#20197;&#20379;&#33258;&#21160;&#21270;&#32452;&#20214;&#25110;&#27169;&#22359;&#25191;&#34892;&#12290;&#38543;&#21518;&#65292;&#35774;&#35745;LLM&#20195;&#29702;&#26469;&#35299;&#37322;&#25968;&#23383;&#23402;&#29983;&#20013;&#30340;&#25551;&#36848;&#24615;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;RESTful&#25509;&#21475;&#25511;&#21046;&#29289;&#29702;&#31995;&#32479;&#12290;&#36825;&#20123;LLM&#20195;&#29702;&#20316;&#20026;&#33258;&#21160;&#21270;&#31995;&#32479;&#20869;&#30340;&#26234;&#33021;&#20195;&#29702;&#65292;&#23454;&#29616;&#20102;&#28789;&#27963;&#29983;&#20135;&#30340;&#33258;&#20027;&#35268;&#21010;&#21644;&#25511;&#21046;&#12290;&#32473;&#23450;&#19968;&#20010;&#20219;&#21153;&#25351;&#20196;&#20316;&#20026;&#36755;&#20837;&#65292;LLM&#20195;&#29702;&#21327;&#35843;&#19968;&#31995;&#21015;&#21407;&#23376;&#21151;&#33021;&#21644;&#25216;&#33021;&#26469;&#23436;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#23454;&#29616;&#30340;&#21407;&#22411;&#22914;&#20309;&#22788;&#29702;&#26410;&#39044;&#23450;&#20041;&#30340;&#20219;&#21153;&#65292;&#24182;&#35745;&#21010;&#29983;&#20135;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a novel framework that combines large language models (LLMs), digital twins and industrial automation system to enable intelligent planning and control of production processes. Our approach involves developing a digital twin system that contains descriptive information about the production and retrofitting the automation system to offer unified interfaces of fine-granular functionalities or skills executable by automation components or modules. Subsequently, LLM-Agents are designed to interpret descriptive information in the digital twins and control the physical system through RESTful interfaces. These LLM-Agents serve as intelligent agents within an automation system, enabling autonomous planning and control of flexible production. Given a task instruction as input, the LLM-agents orchestrate a sequence of atomic functionalities and skills to accomplish the task. We demonstrate how our implemented prototype can handle un-predefined tasks, plan a production p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25551;&#36848;&#20102;&#20004;&#20010;NLP&#31995;&#32479;&#65306;&#19968;&#20010;&#20026;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65292;&#19968;&#20010;&#20026;&#20020;&#24202;&#35797;&#39564;&#25968;&#25454;&#35777;&#25454;&#26816;&#32034;&#12290;&#23427;&#20204;&#20998;&#21035;&#37319;&#29992;&#20102;&#27969;&#27700;&#32447;&#27169;&#22411;&#21644;&#32852;&#21512;&#27169;&#22411;&#65292;&#24182;&#22312;&#26368;&#32456;&#30340;&#38598;&#25104;&#31995;&#32479;&#20013;&#34701;&#21512;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2304.13180</link><description>&lt;p&gt;
Sebis&#22312;SemEval-2023&#20219;&#21153;7&#20013;&#65306;&#20020;&#24202;&#35797;&#39564;&#25253;&#21578;&#20013;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#21644;&#35777;&#25454;&#26816;&#32034;&#30340;&#32852;&#21512;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Sebis at SemEval-2023 Task 7: A Joint System for Natural Language Inference and Evidence Retrieval from Clinical Trial Reports. (arXiv:2304.13180v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13180
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#20004;&#20010;NLP&#31995;&#32479;&#65306;&#19968;&#20010;&#20026;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65292;&#19968;&#20010;&#20026;&#20020;&#24202;&#35797;&#39564;&#25968;&#25454;&#35777;&#25454;&#26816;&#32034;&#12290;&#23427;&#20204;&#20998;&#21035;&#37319;&#29992;&#20102;&#27969;&#27700;&#32447;&#27169;&#22411;&#21644;&#32852;&#21512;&#27169;&#22411;&#65292;&#24182;&#22312;&#26368;&#32456;&#30340;&#38598;&#25104;&#31995;&#32479;&#20013;&#34701;&#21512;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#27599;&#22825;&#20135;&#29983;&#30340;&#20020;&#24202;&#35797;&#39564;&#25253;&#21578;&#25968;&#37327;&#22686;&#21152;&#65292;&#36319;&#36827;&#21578;&#30693;&#22522;&#20110;&#35777;&#25454;&#30340;&#21307;&#30103;&#24314;&#35758;&#30340;&#26032;&#21457;&#29616;&#21464;&#24471;&#36234;&#26469;&#36234;&#38590;&#12290;&#20026;&#20102;&#24110;&#21161;&#33258;&#21160;&#21270;&#36825;&#19968;&#36807;&#31243;&#24182;&#21327;&#21161;&#21307;&#30103;&#19987;&#23478;&#65292;&#27491;&#22312;&#24320;&#21457;NLP&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#28608;&#21457;&#20102;SemEval-2023&#20219;&#21153;7&#65292;&#35813;&#20219;&#21153;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#20010;NLP&#31995;&#32479;&#65292;&#20197;&#22788;&#29702;&#20174;&#20020;&#24202;&#35797;&#39564;&#25968;&#25454;&#20013;&#25552;&#21462;&#35777;&#25454;&#21644;&#36827;&#34892;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#20004;&#20010;&#20219;&#21153;&#12290;&#26412;&#25991;&#20171;&#32461;&#25105;&#20204;&#24320;&#21457;&#30340;&#20004;&#20010;&#31995;&#32479;&#12290;&#31532;&#19968;&#20010;&#26159;&#27969;&#27700;&#32447;&#31995;&#32479;&#65292;&#21333;&#29420;&#24314;&#27169;&#20102;&#36825;&#20004;&#20010;&#20219;&#21153;&#65292;&#32780;&#31532;&#20108;&#20010;&#26159;&#32852;&#21512;&#31995;&#32479;&#65292;&#37319;&#29992;&#20849;&#20139;&#34920;&#31034;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#21516;&#26102;&#23398;&#20064;&#36825;&#20004;&#20010;&#20219;&#21153;&#12290;&#26368;&#32456;&#31995;&#32479;&#23558;&#23427;&#20204;&#30340;&#36755;&#20986;&#21512;&#24182;&#20026;&#19968;&#20010;&#38598;&#25104;&#31995;&#32479;&#12290;&#25105;&#20204;&#35268;&#33539;&#21270;&#27169;&#22411;&#65292;&#20171;&#32461;&#20854;&#29305;&#28857;&#21644;&#25361;&#25112;&#65292;&#24182;&#23545;&#23454;&#29616;&#30340;&#32467;&#26524;&#36827;&#34892;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing number of clinical trial reports generated every day, it is becoming hard to keep up with novel discoveries that inform evidence-based healthcare recommendations. To help automate this process and assist medical experts, NLP solutions are being developed. This motivated the SemEval-2023 Task 7, where the goal was to develop an NLP system for two tasks: evidence retrieval and natural language inference from clinical trial data. In this paper, we describe our two developed systems. The first one is a pipeline system that models the two tasks separately, while the second one is a joint system that learns the two tasks simultaneously with a shared representation and a multi-task learning approach. The final system combines their outputs in an ensemble system. We formalize the models, present their characteristics and challenges, and provide an analysis of achieved results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#35821;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#20248;&#21183;&#21644;&#25361;&#25112;&#65292;&#35777;&#26126;&#20854;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#28508;&#21147;&#12290;&#26412;&#30740;&#31350;&#21457;&#29616;LLMs&#22312;&#32473;&#23450;&#19978;&#19979;&#25991;&#31034;&#20363;&#26102;&#21487;&#20197;&#24847;&#22806;&#22320;&#24573;&#30053;&#25552;&#31034;&#35821;&#20041;&#65292;&#24182;&#19988;&#36328;&#35821;&#35328;&#31034;&#20363;&#21487;&#20197;&#20026;&#20302;&#36164;&#28304;&#32763;&#35793;&#25552;&#20379;&#26356;&#22909;&#30340;&#20219;&#21153;&#25351;&#23548;&#12290;&#20294;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#26368;&#22909;&#30340;&#27169;&#22411;ChatGPT&#20173;&#28982;&#33853;&#21518;&#20110;&#30417;&#30563;&#22522;&#32447;NLLB&#12290;</title><link>http://arxiv.org/abs/2304.04675</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#22810;&#35821;&#26426;&#22120;&#32763;&#35793;&#65306;&#23454;&#35777;&#32467;&#26524;&#21644;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Multilingual Machine Translation with Large Language Models: Empirical Results and Analysis. (arXiv:2304.04675v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04675
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#35821;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#20248;&#21183;&#21644;&#25361;&#25112;&#65292;&#35777;&#26126;&#20854;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#28508;&#21147;&#12290;&#26412;&#30740;&#31350;&#21457;&#29616;LLMs&#22312;&#32473;&#23450;&#19978;&#19979;&#25991;&#31034;&#20363;&#26102;&#21487;&#20197;&#24847;&#22806;&#22320;&#24573;&#30053;&#25552;&#31034;&#35821;&#20041;&#65292;&#24182;&#19988;&#36328;&#35821;&#35328;&#31034;&#20363;&#21487;&#20197;&#20026;&#20302;&#36164;&#28304;&#32763;&#35793;&#25552;&#20379;&#26356;&#22909;&#30340;&#20219;&#21153;&#25351;&#23548;&#12290;&#20294;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#26368;&#22909;&#30340;&#27169;&#22411;ChatGPT&#20173;&#28982;&#33853;&#21518;&#20110;&#30417;&#30563;&#22522;&#32447;NLLB&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#22788;&#29702;&#22810;&#35821;&#26426;&#22120;&#32763;&#35793;(MMT)&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#36890;&#36807;&#22238;&#31572;&#20004;&#20010;&#38382;&#39064;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;LLMs&#22312;MMT&#20013;&#30340;&#20248;&#21183;&#21644;&#25361;&#25112;&#65306;1) LLMs&#22312;&#32763;&#35793;&#22823;&#37327;&#35821;&#35328;&#26041;&#38754;&#34920;&#29616;&#22914;&#20309;&#65311;2) &#21738;&#20123;&#22240;&#32032;&#20250;&#24433;&#21709;LLMs&#22312;&#32763;&#35793;&#20013;&#30340;&#34920;&#29616;&#65311;&#25105;&#20204;&#35780;&#20272;&#20102;&#21253;&#25324;XGLM&#12289;OPT&#12289;BLOOMZ&#21644;ChatGPT&#22312;&#20869;&#30340;&#20960;&#20010;&#21463;&#27426;&#36814;&#30340;LLMs&#22312;102&#31181;&#35821;&#35328;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#65292;&#21363;&#20351;&#26159;&#26368;&#22909;&#30340;&#27169;&#22411;ChatGPT&#22312;83.33%&#30340;&#32763;&#35793;&#26041;&#21521;&#19978;&#20063;&#33853;&#21518;&#20110;&#30417;&#30563;&#22522;&#32447;NLLB&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#29992;&#20110;MMT&#26102;&#65292;LLMs&#34920;&#29616;&#20986;&#26032;&#30340;&#24037;&#20316;&#27169;&#24335;&#12290;&#39318;&#20808;&#65292;&#22312;&#32473;&#23450;&#19978;&#19979;&#25991;&#31034;&#20363;&#26102;&#65292;&#25552;&#31034;&#35821;&#20041;&#21487;&#33021;&#20250;&#34987;&#24847;&#22806;&#22320;&#24573;&#30053;&#65292;&#21363;&#20351;&#25552;&#31034;&#19981;&#21512;&#29702;&#65292;LLMs&#20173;&#28982;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#20854;&#27425;&#65292;&#36328;&#35821;&#35328;&#31034;&#20363;&#21487;&#20197;&#20026;&#20302;&#36164;&#28304;&#32763;&#35793;&#25552;&#20379;&#27604;&#30456;&#21516;&#35821;&#35328;&#23545;&#20013;&#30340;&#31034;&#20363;&#26356;&#22909;&#30340;&#20219;&#21153;&#25351;&#23548;&#12290;&#31532;&#19977;&#65292;&#24403;&#32763;&#35793;&#20302;&#36164;&#28304;&#35821;&#35328;&#26102;&#65292;LLMs&#24448;&#24448;&#34920;&#29616;&#24471;&#26356;&#22909;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;LLMs&#22312;MMT&#20013;&#30340;&#28508;&#21147;&#21644;&#23616;&#38480;&#24615;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable potential in handling multilingual machine translation (MMT). In this paper, we systematically investigate the advantages and challenges of LLMs for MMT by answering two questions: 1) How well do LLMs perform in translating a massive number of languages? 2) Which factors affect LLMs' performance in translation? We evaluate popular LLMs, including XGLM, OPT, BLOOMZ, and ChatGPT, on 102 languages. Our empirical results show that even the best model ChatGPT still lags behind the supervised baseline NLLB in 83.33% of translation directions. Through further analysis, we discover that LLMs exhibit new working patterns when used for MMT. First, prompt semantics can surprisingly be ignored when given in-context exemplars, where LLMs still show strong performance even with unreasonable prompts. Second, cross-lingual exemplars can provide better task instruction for low-resource translation than exemplars in the same language pairs. Third
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102; MACHIAVELLI &#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#34913;&#37327;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#26159;&#21542;&#34920;&#29616;&#20986;&#39532;&#22522;&#38597;&#32500;&#21033;&#34892;&#20026;&#65292;&#21457;&#29616;&#20102;&#26368;&#22823;&#21270;&#22870;&#21169;&#21644;&#34892;&#20026;&#30340;&#36947;&#24503;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65292;&#24182;&#25506;&#32034;&#20102;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#31181;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2304.03279</link><description>&lt;p&gt;
&#22870;&#21169;&#26159;&#21542;&#21512;&#29702;&#65311;&#22312; MACHIAVELLI &#22522;&#20934;&#27979;&#35797;&#20013;&#34913;&#37327;&#22870;&#21169;&#19982;&#36947;&#24503;&#34892;&#20026;&#20043;&#38388;&#30340;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark. (arXiv:2304.03279v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03279
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102; MACHIAVELLI &#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#34913;&#37327;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#26159;&#21542;&#34920;&#29616;&#20986;&#39532;&#22522;&#38597;&#32500;&#21033;&#34892;&#20026;&#65292;&#21457;&#29616;&#20102;&#26368;&#22823;&#21270;&#22870;&#21169;&#21644;&#34892;&#20026;&#30340;&#36947;&#24503;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65292;&#24182;&#25506;&#32034;&#20102;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#31181;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#34987;&#35757;&#32451;&#25104;&#26368;&#22823;&#21270;&#22870;&#21169;&#65292;&#36825;&#21487;&#33021;&#20250;&#28608;&#21169;&#36861;&#27714;&#26435;&#21147;&#21644;&#27450;&#39575;&#34892;&#20026;&#65292;&#31867;&#20284;&#20110;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#21487;&#33021;&#20250;&#28608;&#21169;&#26377;&#23475;&#34892;&#20026;&#12290;&#37027;&#20040;&#20195;&#29702;&#26159;&#21542;&#33258;&#28982;&#32780;&#28982;&#22320;&#23398;&#20250;&#20102;&#39532;&#22522;&#38597;&#32500;&#21033;&#34892;&#20026;&#65311;&#25105;&#20204;&#22914;&#20309;&#22312; GPT-4 &#31561;&#36890;&#29992;&#27169;&#22411;&#20013;&#34913;&#37327;&#36825;&#20123;&#34892;&#20026;&#21602;&#65311;&#20026;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; MACHIAVELLI &#22522;&#20934;&#27979;&#35797;&#65292;&#35813;&#27979;&#35797;&#28085;&#30422;&#20102;&#36229;&#36807;&#19968;&#30334;&#19975;&#20010;&#22810;&#26679;&#21270;&#30340;&#24773;&#26223;&#65292;&#37325;&#28857;&#20851;&#27880;&#31038;&#20250;&#20915;&#31574;&#21046;&#23450;&#65292;&#29992;&#20110;&#34913;&#37327;&#20154;&#24037;&#20195;&#29702;&#26159;&#21542;&#34920;&#29616;&#20986;&#39532;&#22522;&#38597;&#32500;&#21033;&#34892;&#20026;&#12290;&#25105;&#20204;&#25968;&#23398;&#21270;&#20102;&#25968;&#21313;&#31181;&#26377;&#23475;&#34892;&#20026;&#65292;&#24182;&#20351;&#29992;&#25105;&#20204;&#30340;&#27880;&#37322;&#26469;&#35780;&#20272;&#20195;&#29702;&#20542;&#21521;&#20110;&#36861;&#27714;&#26435;&#21147;&#65292;&#36896;&#25104;&#21151;&#33021;&#19981;&#33391;&#21644;&#36829;&#21453;&#20262;&#29702;&#30340;&#20542;&#21521;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#26368;&#22823;&#21270;&#22870;&#21169;&#21644;&#34892;&#20026;&#30340;&#36947;&#24503;&#24615;&#20043;&#38388;&#23384;&#22312;&#19968;&#20123;&#32039;&#24352;&#20851;&#31995;&#12290;&#20026;&#20102;&#25913;&#21892;&#36825;&#31181;&#26435;&#34913;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#20351;&#20195;&#29702;&#36235;&#21521;&#20110;&#37319;&#21462;&#26356;&#23569;&#30340;&#26377;&#23475;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;MACHIAVELLI &#26159;&#35780;&#20272;&#20154;&#24037;&#20195;&#29702;&#39532;&#22522;&#38597;&#32500;&#21033;&#34892;&#20026;&#27700;&#24179;&#30340;&#26377;&#29992;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial agents have traditionally been trained to maximize reward, which may incentivize power-seeking and deception, analogous to how next-token prediction in language models (LMs) may incentivize toxicity. So do agents naturally learn to be Machiavellian? And how do we measure these behaviors in general-purpose models such as GPT-4? Towards answering these questions, we introduce MACHIAVELLI, a benchmark of 134 Choose-Your-Own-Adventure games containing over half a million rich, diverse scenarios that center on social decision-making. Scenario labeling is automated with LMs, which are more performant than human annotators. We mathematize dozens of harmful behaviors and use our annotations to evaluate agents' tendencies to be power-seeking, cause disutility, and commit ethical violations. We observe some tension between maximizing reward and behaving ethically. To improve this trade-off, we investigate LM-based methods to steer agents' towards less harmful behaviors. Our results sh
&lt;/p&gt;</description></item><item><title>&#20256;&#32479;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26426;&#22120;&#23398;&#20064;&#38656;&#35201;&#22823;&#35268;&#27169;&#30340;&#20219;&#21153;&#29305;&#23450;&#31034;&#20363;&#65292;&#20294;&#36825;&#19981;&#36866;&#29992;&#20110;&#20219;&#21153;&#21487;&#33021;&#36807;&#20110;&#22797;&#26434;&#25110;&#25104;&#26412;&#36807;&#39640;&#20197;&#36827;&#34892;&#27880;&#37322;&#30340;&#22330;&#26223;&#12290;&#22240;&#27492;&#65292;&#31038;&#21306;&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26032;&#30340;&#30417;&#30563;&#23547;&#27714;&#33539;&#24335;--&#20174;&#20219;&#21153;&#25351;&#20196;&#23398;&#20064;--&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#12290;</title><link>http://arxiv.org/abs/2303.10475</link><description>&lt;p&gt;
&#20165;&#20165;&#25552;&#31034;&#36275;&#22815;&#20102;&#21527;&#65311;&#19981;&#26159;&#30340;&#12290;&#25351;&#23548;&#23398;&#20064;&#30340;&#20840;&#38754;&#21644;&#26356;&#24191;&#38420;&#35270;&#35282;&#65288;arXiv&#65306;2303.10475v1 [cs.CL]&#65289;
&lt;/p&gt;
&lt;p&gt;
Is Prompt All You Need? No. A Comprehensive and Broader View of Instruction Learning. (arXiv:2303.10475v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10475
&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26426;&#22120;&#23398;&#20064;&#38656;&#35201;&#22823;&#35268;&#27169;&#30340;&#20219;&#21153;&#29305;&#23450;&#31034;&#20363;&#65292;&#20294;&#36825;&#19981;&#36866;&#29992;&#20110;&#20219;&#21153;&#21487;&#33021;&#36807;&#20110;&#22797;&#26434;&#25110;&#25104;&#26412;&#36807;&#39640;&#20197;&#36827;&#34892;&#27880;&#37322;&#30340;&#22330;&#26223;&#12290;&#22240;&#27492;&#65292;&#31038;&#21306;&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26032;&#30340;&#30417;&#30563;&#23547;&#27714;&#33539;&#24335;--&#20174;&#20219;&#21153;&#25351;&#20196;&#23398;&#20064;--&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#35821;&#20041;&#21487;&#20197;&#36890;&#36807;&#19968;&#32452;&#36755;&#20837;&#36755;&#20986;&#31034;&#20363;&#25110;&#19968;&#26465;&#25991;&#26412;&#25351;&#20196;&#26469;&#34920;&#36798;&#12290;&#20256;&#32479;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#30340;&#20219;&#21153;&#29305;&#23450;&#31034;&#20363;&#30340;&#21487;&#29992;&#24615;&#12290;&#36825;&#24341;&#36215;&#20102;&#20004;&#20010;&#38382;&#39064;&#65306;&#39318;&#20808;&#65292;&#25910;&#38598;&#20219;&#21153;&#29305;&#23450;&#26631;&#35760;&#31034;&#20363;&#19981;&#36866;&#29992;&#20110;&#20219;&#21153;&#21487;&#33021;&#36807;&#20110;&#22797;&#26434;&#25110;&#25104;&#26412;&#36807;&#39640;&#20197;&#36827;&#34892;&#27880;&#37322;&#30340;&#22330;&#26223;&#65292;&#25110;&#32773;&#31995;&#32479;&#38656;&#35201;&#31435;&#21363;&#22788;&#29702;&#26032;&#20219;&#21153;&#12290;&#20854;&#27425;&#65292;&#36825;&#19981;&#26159;&#29992;&#25143;&#21451;&#22909;&#30340;&#65292;&#22240;&#20026;&#26368;&#32456;&#29992;&#25143;&#21487;&#33021;&#26356;&#24895;&#24847;&#22312;&#20351;&#29992;&#31995;&#32479;&#20043;&#21069;&#25552;&#20379;&#20219;&#21153;&#25551;&#36848;&#32780;&#19981;&#26159;&#19968;&#32452;&#31034;&#20363;&#12290;&#22240;&#27492;&#65292;&#31038;&#21306;&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26032;&#30340;&#30417;&#30563;&#23547;&#27714;&#33539;&#24335;--&#20174;&#20219;&#21153;&#25351;&#20196;&#23398;&#20064;--&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#36827;&#23637;&#65292;&#20294;&#31038;&#21306;&#20173;&#28982;&#38754;&#20020;&#30528;&#19968;&#20123;&#20849;&#21516;&#30340;&#38382;&#39064;&#12290;&#26412;&#27425;&#35843;&#26597;&#26088;&#22312;&#24635;&#32467;&#25351;&#23548;&#23398;&#20064;&#30340;&#24403;&#21069;&#30740;&#31350;&#65292;&#29305;&#21035;&#26159;&#22238;&#31572;&#20197;&#19979;&#38382;&#39064;&#65306;
&lt;/p&gt;
&lt;p&gt;
Task semantics can be expressed by a set of input-to-output examples or a piece of textual instruction. Conventional machine learning approaches for natural language processing (NLP) mainly rely on the availability of large-scale sets of task-specific examples. Two issues arise: first, collecting task-specific labeled examples does not apply to scenarios where tasks may be too complicated or costly to annotate, or the system is required to handle a new task immediately; second, this is not user-friendly since end-users are probably more willing to provide task description rather than a set of examples before using the system. Therefore, the community is paying increasing interest in a new supervision-seeking paradigm for NLP: learning from task instructions. Despite its impressive progress, there are some common issues that the community struggles with. This survey paper tries to summarize the current research on instruction learning, particularly, by answering the following questions:
&lt;/p&gt;</description></item><item><title>AUTODIAL&#26159;&#19968;&#31181;&#22810;&#20219;&#21153;&#23545;&#35805;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#24182;&#34892;&#35299;&#30721;&#22120;&#26469;&#25191;&#34892;&#23545;&#35805;&#20219;&#21153;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20869;&#23384;&#21344;&#29992;&#24182;&#23454;&#29616;&#26356;&#24555;&#30340;&#25512;&#29702;&#26102;&#38388;&#12290;&#19982;&#29616;&#26377;&#30340;&#29983;&#25104;&#26041;&#27861;&#30456;&#27604;&#65292;AUTODIAL&#22312;&#19977;&#20010;&#23545;&#35805;&#20219;&#21153;&#19978;&#25552;&#20379;&#20102;3-6&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#65292;&#21516;&#26102;&#20855;&#26377;11&#20493;&#30340;&#21442;&#25968;&#20943;&#23569;&#12290;&#36825;&#34920;&#26126;&#23558;&#24403;&#21069;&#30340;&#23545;&#35805;&#27169;&#22411;&#25193;&#23637;&#20026;&#20855;&#26377;&#24182;&#34892;&#35299;&#30721;&#22120;&#21487;&#20197;&#25104;&#20026;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#20013;&#37096;&#32626;&#23427;&#20204;&#30340;&#21487;&#34892;&#26367;&#20195;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2303.06245</link><description>&lt;p&gt;
AUTODIAL: &#39640;&#25928;&#24322;&#27493;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AUTODIAL: Efficient Asynchronous Task-Oriented Dialogue Model. (arXiv:2303.06245v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06245
&lt;/p&gt;
&lt;p&gt;
AUTODIAL&#26159;&#19968;&#31181;&#22810;&#20219;&#21153;&#23545;&#35805;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#24182;&#34892;&#35299;&#30721;&#22120;&#26469;&#25191;&#34892;&#23545;&#35805;&#20219;&#21153;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20869;&#23384;&#21344;&#29992;&#24182;&#23454;&#29616;&#26356;&#24555;&#30340;&#25512;&#29702;&#26102;&#38388;&#12290;&#19982;&#29616;&#26377;&#30340;&#29983;&#25104;&#26041;&#27861;&#30456;&#27604;&#65292;AUTODIAL&#22312;&#19977;&#20010;&#23545;&#35805;&#20219;&#21153;&#19978;&#25552;&#20379;&#20102;3-6&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#65292;&#21516;&#26102;&#20855;&#26377;11&#20493;&#30340;&#21442;&#25968;&#20943;&#23569;&#12290;&#36825;&#34920;&#26126;&#23558;&#24403;&#21069;&#30340;&#23545;&#35805;&#27169;&#22411;&#25193;&#23637;&#20026;&#20855;&#26377;&#24182;&#34892;&#35299;&#30721;&#22120;&#21487;&#20197;&#25104;&#20026;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#20013;&#37096;&#32626;&#23427;&#20204;&#30340;&#21487;&#34892;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
AUTODIAL is a multi-task dialogue model that significantly reduces memory footprint and achieves faster inference times by using parallel decoders to perform dialogue tasks. Compared to existing generative approach, AUTODIAL provides 3-6x speedups during inference while having 11x fewer parameters on three dialogue tasks. This suggests that extending current dialogue models to have parallel decoders can be a viable alternative for deploying them in resource-constrained environments.
&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#23545;&#35805;&#27169;&#22411;&#22312;&#23454;&#36341;&#20013;&#21464;&#24471;&#26222;&#36941;&#65292;&#35757;&#32451;&#12289;&#25512;&#29702;&#21644;&#26356;&#22823;&#30340;&#20869;&#23384;&#21344;&#29992;&#30340;&#39640;&#35745;&#31639;&#35201;&#27714;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AUTODIAL&#65292;&#19968;&#31181;&#22810;&#20219;&#21153;&#23545;&#35805;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#37096;&#32626;&#23545;&#35805;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;AUTODIAL&#21033;&#29992;&#24182;&#34892;&#35299;&#30721;&#22120;&#25191;&#34892;&#35832;&#22914;&#23545;&#35805;&#34892;&#20026;&#39044;&#27979;&#12289;&#39046;&#22495;&#39044;&#27979;&#12289;&#24847;&#22270;&#39044;&#27979;&#21644;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#31561;&#20219;&#21153;&#12290;&#20351;&#29992;&#20998;&#31867;&#35299;&#30721;&#22120;&#32780;&#19981;&#26159;&#29983;&#25104;&#35299;&#30721;&#22120;&#20351;AUTODIAL&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#20869;&#23384;&#21344;&#29992;&#65292;&#24182;&#22312;&#25512;&#29702;&#26102;&#38388;&#19978;&#23454;&#29616;&#27604;&#29616;&#26377;&#29983;&#25104;&#26041;&#27861;&#65288;&#21363;SimpleTOD&#65289;&#26356;&#24555;&#30340;&#36895;&#24230;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#23558;&#24403;&#21069;&#30340;&#23545;&#35805;&#27169;&#22411;&#25193;&#23637;&#20026;&#20855;&#26377;&#24182;&#34892;&#35299;&#30721;&#22120;&#21487;&#20197;&#25104;&#20026;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#20013;&#37096;&#32626;&#23427;&#20204;&#30340;&#21487;&#34892;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large dialogue models become commonplace in practice, the problems surrounding high compute requirements for training, inference and larger memory footprint still persists. In this work, we present AUTODIAL, a multi-task dialogue model that addresses the challenges of deploying dialogue model. AUTODIAL utilizes parallel decoders to perform tasks such as dialogue act prediction, domain prediction, intent prediction, and dialogue state tracking. Using classification decoders over generative decoders allows AUTODIAL to significantly reduce memory footprint and achieve faster inference times compared to existing generative approach namely SimpleTOD. We demonstrate that AUTODIAL provides 3-6x speedups during inference while having 11x fewer parameters on three dialogue tasks compared to SimpleTOD. Our results show that extending current dialogue models to have parallel decoders can be a viable alternative for deploying them in resource-constrained environments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#23545;&#26412;&#20307;&#23376;&#31867;&#25512;&#26029;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#22871;&#28041;&#21450;&#21407;&#23376;&#27010;&#24565;&#21644;&#22797;&#21512;&#27010;&#24565;&#30340;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#35777;&#26126;&#35821;&#35328;&#27169;&#22411;&#23545;&#23376;&#31867;&#25512;&#26029;&#32972;&#26223;&#30693;&#35782;&#30340;&#35760;&#24518;&#30456;&#23545;&#36739;&#23569;&#65292;&#20294;&#22312;&#32473;&#23450;&#23569;&#37327;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#21487;&#26174;&#33879;&#25552;&#39640;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2302.06761</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20998;&#26512;&#26412;&#20307;&#23376;&#31867;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Language Model Analysis for Ontology Subsumption Inference. (arXiv:2302.06761v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#23545;&#26412;&#20307;&#23376;&#31867;&#25512;&#26029;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#22871;&#28041;&#21450;&#21407;&#23376;&#27010;&#24565;&#21644;&#22797;&#21512;&#27010;&#24565;&#30340;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#35777;&#26126;&#35821;&#35328;&#27169;&#22411;&#23545;&#23376;&#31867;&#25512;&#26029;&#32972;&#26223;&#30693;&#35782;&#30340;&#35760;&#24518;&#30456;&#23545;&#36739;&#23569;&#65292;&#20294;&#22312;&#32473;&#23450;&#23569;&#37327;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#21487;&#26174;&#33879;&#25552;&#39640;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#25506;&#31350;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#20316;&#20026;&#30693;&#35782;&#24211;&#30340;&#26367;&#20195;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#37117;&#20851;&#27880;&#20110;&#31616;&#21333;&#30340;&#19977;&#20803;&#32452;&#20851;&#31995;&#22411;&#30693;&#35782;&#24211;&#65292;&#24573;&#30053;&#20102;&#26356;&#20026;&#22797;&#26434;&#12289;&#36923;&#36753;&#20026;&#22522;&#30784;&#12289;&#27010;&#24565;&#21270;&#30340; OWL &#26412;&#20307;&#31561;&#30693;&#35782;&#24211;&#12290;&#20026;&#20102;&#30740;&#31350;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#26412;&#20307;&#30340;&#20102;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986; OntoLAMA&#65292;&#23427;&#21253;&#21547;&#22522;&#20110;&#25512;&#29702;&#30340;&#19968;&#31995;&#21015;&#27979;&#35797;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#65292;&#20174;&#28041;&#21450;&#21407;&#23376;&#27010;&#24565;&#21644;&#22797;&#21512;&#27010;&#24565;&#30340;&#23376;&#31867;&#25512;&#26029;&#20844;&#29702;&#20986;&#21457;&#12290;&#25105;&#20204;&#23545;&#19981;&#21516;&#39046;&#22495;&#21644;&#35268;&#27169;&#30340;&#26412;&#20307;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65292;&#35821;&#35328;&#27169;&#22411;&#23545;&#23376;&#31867;&#25512;&#26029;&#30340;&#32972;&#26223;&#30693;&#35782;&#35760;&#24518;&#30456;&#23545;&#36739;&#23569;&#65292;&#20294;&#26159;&#22312;&#32473;&#23450;&#23569;&#37327;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#23376;&#31867;&#25512;&#26029;&#30340;&#20934;&#30830;&#29575;&#12290;&#25105;&#20204;&#23558;&#20844;&#24320;&#28304;&#30721;&#21644;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Investigating whether pre-trained language models (LMs) can function as knowledge bases (KBs) has raised wide research interests recently. However, existing works focus on simple, triple-based, relational KBs, but omit more sophisticated, logic-based, conceptualised KBs such as OWL ontologies. To investigate an LM's knowledge of ontologies, we propose OntoLAMA, a set of inference-based probing tasks and datasets from ontology subsumption axioms involving both atomic and complex concepts. We conduct extensive experiments on ontologies of different domains and scales, and our results demonstrate that LMs encode relatively less background knowledge of Subsumption Inference (SI) than traditional Natural Language Inference (NLI) but can improve on SI significantly when a small number of samples are given. We will open-source our code and datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;Code-LLMs&#24341;&#23548;&#31526;&#21495;&#34920;&#31034;&#20197;&#22686;&#24378;&#31070;&#32463;&#31526;&#21495;&#25925;&#20107;&#29702;&#35299;&#65292;&#36890;&#36807;CoRRPUS&#31995;&#32479;&#21644;&#25277;&#35937;&#25552;&#31034;&#31243;&#24207;&#65292;&#22312;&#26368;&#23567;&#30340;&#25163;&#21160;&#24037;&#31243;&#26465;&#20214;&#19979;&#65292;&#20987;&#36133;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#32467;&#26500;LLM&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2212.10754</link><description>&lt;p&gt;
CoRRPUS: &#21033;&#29992;Codex&#30340;&#32467;&#26500;&#21270;&#34920;&#31034;&#22686;&#24378;&#31070;&#32463;&#31526;&#21495;&#25925;&#20107;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
CoRRPUS: Codex-Leveraged Structured Representations for Neurosymbolic Story Understanding. (arXiv:2212.10754v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10754
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;Code-LLMs&#24341;&#23548;&#31526;&#21495;&#34920;&#31034;&#20197;&#22686;&#24378;&#31070;&#32463;&#31526;&#21495;&#25925;&#20107;&#29702;&#35299;&#65292;&#36890;&#36807;CoRRPUS&#31995;&#32479;&#21644;&#25277;&#35937;&#25552;&#31034;&#31243;&#24207;&#65292;&#22312;&#26368;&#23567;&#30340;&#25163;&#21160;&#24037;&#31243;&#26465;&#20214;&#19979;&#65292;&#20987;&#36133;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#32467;&#26500;LLM&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25152;&#26377;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;/&#29702;&#35299;&#20219;&#21153;&#30340;&#31070;&#32463;&#31526;&#21495;&#25216;&#26415;&#30340;&#39134;&#36895;&#21457;&#23637;&#65292;&#25925;&#20107;&#29983;&#25104;&#21644;&#29702;&#35299;&#20063;&#24471;&#21040;&#20102;&#26497;&#22823;&#30340;&#25913;&#36827;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;Code-LLMs&#65292;&#22914;Codex&#65292;&#24341;&#23548;&#31526;&#21495;&#26041;&#27861;&#30340;&#20351;&#29992;&#65292;&#20197;&#36319;&#36394;&#25925;&#20107;&#29366;&#24577;&#24182;&#24110;&#21161;&#25925;&#20107;&#29702;&#35299;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;CoRRPUS&#31995;&#32479;&#21644;&#25277;&#35937;&#25552;&#31034;&#31243;&#24207;&#22914;&#20309;&#22312;&#26368;&#23567;&#30340;&#25163;&#21160;&#24037;&#31243;&#26465;&#20214;&#19979;&#65292;&#20987;&#36133;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#32467;&#26500;LLM&#25216;&#26415;&#65292;&#23436;&#25104;&#20102;&#39044;&#20808;&#23384;&#22312;&#30340;&#25925;&#20107;&#29702;&#35299;&#20219;&#21153;&#65288;bAbI task 2 &#21644; Re^3&#65289;&#12290;&#25105;&#20204;&#24076;&#26395;&#26412;&#30740;&#31350;&#33021;&#22815;&#20984;&#26174;&#31526;&#21495;&#34920;&#31034;&#21644;&#19987;&#19994;&#25552;&#31034;&#23545;LLMs&#30340;&#37325;&#35201;&#24615;&#65292;&#22240;&#20026;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#19968;&#20123;&#25163;&#24037;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Story generation and understanding -- as with all NLG/NLU tasks -- has seen a surge in neurosymbolic work. Researchers have recognized that, while large language models (LLMs) have tremendous utility, they can be augmented with symbolic means to be even better and to make up for any flaws that the neural networks might have. However, symbolic methods are extremely costly in terms of the amount of time and expertise needed to create them. In this work, we capitalize on state-of-the-art Code-LLMs, such as Codex, to bootstrap the use of symbolic methods for tracking the state of stories and aiding in story understanding. We show that our CoRRPUS system and abstracted prompting procedures can beat current state-of-the-art structured LLM techniques on pre-existing story understanding tasks (bAbI task 2 and Re^3) with minimal hand engineering. We hope that this work can help highlight the importance of symbolic representations and specialized prompting for LLMs as these models require some g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#19968;&#39033;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#23545;57&#31181;&#35821;&#35328;&#21644;&#19977;&#20010;&#20219;&#21153;&#19979;&#30340;&#36328;&#35821;&#35328;&#36716;&#31227;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#21457;&#29616;&#20248;&#21270;&#21518;&#30340;&#26631;&#35760;-&#32763;&#35793;&#27861;&#27604;&#20256;&#32479;&#27880;&#37322;&#25237;&#24433;&#26041;&#27861;&#26356;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2211.15613</link><description>&lt;p&gt;
&#36328;&#35821;&#35328;&#36716;&#31227;&#30340;&#20196;&#20154;&#27822;&#20007;&#30340;&#31616;&#26131;&#26631;&#31614;&#25237;&#24433;
&lt;/p&gt;
&lt;p&gt;
Frustratingly Easy Label Projection for Cross-lingual Transfer. (arXiv:2211.15613v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#19968;&#39033;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#23545;57&#31181;&#35821;&#35328;&#21644;&#19977;&#20010;&#20219;&#21153;&#19979;&#30340;&#36328;&#35821;&#35328;&#36716;&#31227;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#21457;&#29616;&#20248;&#21270;&#21518;&#30340;&#26631;&#35760;-&#32763;&#35793;&#27861;&#27604;&#20256;&#32479;&#27880;&#37322;&#25237;&#24433;&#26041;&#27861;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#35757;&#32451;&#25968;&#25454;&#32763;&#35793;&#25104;&#22810;&#31181;&#35821;&#35328;&#24050;&#25104;&#20026;&#25552;&#39640;&#36328;&#35821;&#35328;&#36716;&#31227;&#30340;&#23454;&#38469;&#35299;&#20915;&#26041;&#26696;&#12290;&#23545;&#20110;&#28041;&#21450;&#36328;&#24230;&#32423;&#21035;&#27880;&#37322;&#65288;&#20363;&#22914;&#20449;&#24687;&#25552;&#21462;&#25110;&#38382;&#39064;&#22238;&#31572;&#65289;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#36827;&#34892;&#39069;&#22806;&#30340;&#26631;&#31614;&#25237;&#24433;&#27493;&#39588;&#65292;&#23558;&#24050;&#27880;&#37322;&#30340;&#36328;&#24230;&#26144;&#23556;&#21040;&#32763;&#35793;&#21518;&#30340;&#25991;&#26412;&#20013;&#12290;&#28982;&#32780;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36804;&#20170;&#20026;&#27490;&#23578;&#26410;&#23545;&#36825;&#31181;&#26041;&#27861;&#19982;&#22522;&#20110;&#21333;&#35789;&#23545;&#40784;&#30340;&#20256;&#32479;&#27880;&#37322;&#25237;&#24433;&#36827;&#34892;&#23454;&#35777;&#20998;&#26512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#39033;&#23545;57&#31181;&#35821;&#35328;&#21644;&#19977;&#20010;&#20219;&#21153;&#65288;QA&#65292;NER&#21644;&#20107;&#20214;&#25552;&#21462;&#65289;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#20197;&#35780;&#20272;&#20004;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#22635;&#34917;&#25991;&#29486;&#20013;&#30340;&#37325;&#35201;&#31354;&#30333;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#20248;&#21270;&#21518;&#30340;&#26631;&#35760;-&#32763;&#35793;&#27861;&#27604;&#20256;&#32479;&#27880;&#37322;&#25237;&#24433;&#26041;&#27861;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Translating training data into many languages has emerged as a practical solution for improving cross-lingual transfer. For tasks that involve span-level annotations, such as information extraction or question answering, an additional label projection step is required to map annotated spans onto the translated texts. Recently, a few efforts have utilized a simple mark-then-translate method to jointly perform translation and projection by inserting special markers around the labeled spans in the original sentence. However, as far as we are aware, no empirical analysis has been conducted on how this approach compares to traditional annotation projection based on word alignment. In this paper, we present an extensive empirical study across 57 languages and three tasks (QA, NER, and Event Extraction) to evaluate the effectiveness and limitations of both methods, filling an important gap in the literature. Experimental results show that our optimized version of mark-then-translate, which we
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#36880;&#23383;&#30701;&#26399;&#35760;&#24518;&#29305;&#24449;&#65292;&#21457;&#29616;Transformer&#27169;&#22411;&#21487;&#20197;&#20934;&#30830;&#26816;&#32034;&#20808;&#21069;&#20986;&#29616;&#36807;&#30340;&#21333;&#35789;&#36523;&#20221;&#21644;&#39034;&#24207;&#65292;&#32780;LSTM&#27169;&#22411;&#30340;&#26816;&#32034;&#33021;&#21147;&#21463;&#38480;&#20110;&#21015;&#34920;&#21021;&#22987;&#26631;&#35760;&#21644;&#30701;&#30340;&#24178;&#25200;&#25991;&#26412;&#12290;</title><link>http://arxiv.org/abs/2210.13569</link><description>&lt;p&gt;
&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36880;&#23383;&#30701;&#26399;&#35760;&#24518;&#29305;&#24449;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Characterizing Verbatim Short-Term Memory in Neural Language Models. (arXiv:2210.13569v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#36880;&#23383;&#30701;&#26399;&#35760;&#24518;&#29305;&#24449;&#65292;&#21457;&#29616;Transformer&#27169;&#22411;&#21487;&#20197;&#20934;&#30830;&#26816;&#32034;&#20808;&#21069;&#20986;&#29616;&#36807;&#30340;&#21333;&#35789;&#36523;&#20221;&#21644;&#39034;&#24207;&#65292;&#32780;LSTM&#27169;&#22411;&#30340;&#26816;&#32034;&#33021;&#21147;&#21463;&#38480;&#20110;&#21015;&#34920;&#21021;&#22987;&#26631;&#35760;&#21644;&#30701;&#30340;&#24178;&#25200;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#35821;&#35328;&#27169;&#22411;&#34987;&#35757;&#32451;&#29992;&#20110;&#39044;&#27979;&#33258;&#28982;&#35821;&#35328;&#24207;&#21015;&#26102;&#65292;&#23427;&#22312;&#27599;&#20010;&#26102;&#21051;&#30340;&#39044;&#27979;&#20381;&#36182;&#20110;&#20808;&#21069;&#19978;&#19979;&#25991;&#30340;&#34920;&#24449;&#12290;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26816;&#32034;&#21040;&#21738;&#20123;&#20851;&#20110;&#20808;&#21069;&#19978;&#19979;&#25991;&#30340;&#20449;&#24687;&#65311;&#26412;&#30740;&#31350;&#27979;&#35797;&#20102;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#26816;&#32034;&#21040;&#20808;&#21069;&#22312;&#25991;&#26412;&#20013;&#20986;&#29616;&#36807;&#30340;&#30830;&#20999;&#21333;&#35789;&#12290;&#25105;&#20204;&#20197;&#33521;&#25991;&#25991;&#26412;&#20026;&#33539;&#20363;&#65292;&#20854;&#20013;&#19968;&#20010;&#21517;&#35789;&#21015;&#34920;&#20986;&#29616;&#20102;&#20004;&#27425;&#65292;&#21033;&#29992;Transformer&#21644;LSTM&#27169;&#22411;&#22788;&#29702;&#12290;&#25105;&#20204;&#23558;&#26816;&#32034;&#23450;&#20041;&#20026;&#20174;&#31532;&#19968;&#20010;&#21015;&#34920;&#21040;&#31532;&#20108;&#20010;&#21015;&#34920;&#30340;&#24778;&#24322;&#24230;&#38477;&#20302;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;Transformer&#21487;&#20197;&#20174;&#31532;&#19968;&#20010;&#21015;&#34920;&#20013;&#26816;&#32034;&#21040;&#21517;&#35789;&#30340;&#36523;&#20221;&#21644;&#39034;&#24207;&#12290;&#27492;&#22806;&#65292;&#24403;Transformer&#22312;&#26356;&#22823;&#30340;&#35821;&#26009;&#24211;&#20013;&#21644;&#26356;&#28145;&#30340;&#27169;&#22411;&#20013;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#23427;&#20204;&#30340;&#26816;&#32034;&#33021;&#21147;&#26174;&#33879;&#22686;&#24378;&#12290;&#26368;&#21518;&#65292;Transformer&#32034;&#24341;&#20808;&#21069;&#30340;&#26631;&#35760;&#30340;&#33021;&#21147;&#21462;&#20915;&#20110;&#23398;&#20064;&#21040;&#30340;&#27880;&#24847;&#27169;&#24335;&#12290;&#30456;&#21453;&#65292;LSTM&#30340;&#26816;&#32034;&#33021;&#21147;&#36739;&#20302;&#65292;&#20165;&#38480;&#20110;&#21015;&#34920;&#21021;&#22987;&#26631;&#35760;&#21644;&#30701;&#30340;&#24178;&#25200;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
When a language model is trained to predict natural language sequences, its prediction at each moment depends on a representation of prior context. What kind of information about the prior context can language models retrieve? We tested whether language models could retrieve the exact words that occurred previously in a text. In our paradigm, language models (transformers and an LSTM) processed English text in which a list of nouns occurred twice. We operationalized retrieval as the reduction in surprisal from the first to the second list. We found that the transformers retrieved both the identity and ordering of nouns from the first list. Further, the transformers' retrieval was markedly enhanced when they were trained on a larger corpus and with greater model depth. Lastly, their ability to index prior tokens was dependent on learned attention patterns. In contrast, the LSTM exhibited less precise retrieval, which was limited to list-initial tokens and to short intervening texts. The
&lt;/p&gt;</description></item><item><title>&#20026;&#20102;&#21487;&#38752;&#22320;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#65292;NLP&#31995;&#32479;&#38656;&#35201;&#25512;&#24191;&#38271;&#23614;&#31232;&#26377;&#35821;&#21477;&#12290;&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27010;&#29575;&#20998;&#21106;&#26469;&#21019;&#24314;&#26377;&#24847;&#20041;&#27979;&#35797;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#26356;&#22810;&#30340;&#25361;&#25112;&#24615;&#24182;&#21487;&#33021;&#25552;&#39640;&#22312;&#20851;&#38190;&#20219;&#21153;&#19978;&#30340;&#38169;&#35823;&#29575;&#12290;</title><link>http://arxiv.org/abs/2210.06799</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#33021;&#24615;&#20998;&#21106;&#36827;&#34892;&#38271;&#23614;&#27010;&#25324;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Long-tail Generalization with Likelihood Splits. (arXiv:2210.06799v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06799
&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#21487;&#38752;&#22320;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#65292;NLP&#31995;&#32479;&#38656;&#35201;&#25512;&#24191;&#38271;&#23614;&#31232;&#26377;&#35821;&#21477;&#12290;&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27010;&#29575;&#20998;&#21106;&#26469;&#21019;&#24314;&#26377;&#24847;&#20041;&#27979;&#35797;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#26356;&#22810;&#30340;&#25361;&#25112;&#24615;&#24182;&#21487;&#33021;&#25552;&#39640;&#22312;&#20851;&#38190;&#20219;&#21153;&#19978;&#30340;&#38169;&#35823;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#21487;&#38752;&#22320;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#65292;NLP&#31995;&#32479;&#24517;&#39035;&#25512;&#24191;&#21040;&#31232;&#26377;&#35821;&#21477;&#30340;&#38271;&#23614;&#37096;&#20998;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#21019;&#24314;&#38656;&#35201;&#25512;&#24191;&#21040;&#20998;&#24067;&#23614;&#37096;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21363;&#36890;&#36807;&#37325;&#26032;&#21010;&#20998;&#29616;&#26377;&#25968;&#25454;&#38598;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#8220;&#21487;&#33021;&#24615;&#20998;&#21106;&#8221;&#65292;&#21363;&#23558;&#30001;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#20998;&#37197;&#36739;&#20302;&#21487;&#33021;&#24615;&#30340;&#23454;&#20363;&#25918;&#32622;&#22312;&#27979;&#35797;&#38598;&#20013;&#65292;&#32780;&#26356;&#21487;&#33021;&#30340;&#23454;&#20363;&#21017;&#22312;&#35757;&#32451;&#38598;&#20013;&#12290;&#36825;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#21487;&#20197;&#33258;&#23450;&#20041;&#65292;&#20197;&#26500;&#24314;&#36866;&#21512;&#21508;&#31181;&#20219;&#21153;&#30340;&#26377;&#24847;&#20041;&#30340;&#35757;&#32451;-&#27979;&#35797;&#20998;&#21106;&#12290;&#30456;&#23545;&#20110;&#38543;&#26426;&#20998;&#21106;&#65292;&#21487;&#33021;&#24615;&#20998;&#21106;&#34920;&#29616;&#20986;&#27604;&#38543;&#26426;&#20998;&#21106;&#26356;&#22810;&#30340;&#25361;&#25112;&#65306;&#22312;Spider&#19978;&#36827;&#34892;&#30340;&#35821;&#20041;&#35299;&#26512;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#30456;&#23545;&#35823;&#24046;&#29575;&#22686;&#21152;&#20102;59&#65285;&#12289;&#22312;SNLI&#19978;&#36827;&#34892;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#30456;&#23545;&#35823;&#24046;&#29575;&#22686;&#21152;&#20102;93&#65285;&#12289;&#22312;BoolQ&#19978;&#36827;&#34892;&#30340;&#26159;/&#21542;&#38382;&#39064;&#22238;&#31572;&#30340;&#30456;&#23545;&#35823;&#24046;&#29575;&#22686;&#21152;&#20102;33&#65285;&#12290;&#27492;&#22806;&#65292;&#21487;&#33021;&#24615;&#20998;&#21106;&#21019;&#24314;&#27604;&#23545;&#25239;&#36807;&#28388;&#26356;&#20844;&#24179;&#30340;&#22522;&#20934;&#27979;&#35797;&#65307;&#24403;&#29992;&#20110;&#21019;&#24314;&#20998;&#21106;&#30340;LM&#20063;&#26159;&#26816;&#27979;&#22120;&#26102;&#65292;&#20960;&#20046;&#19981;&#20250;&#24433;&#21709;&#21407;&#22987;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order to reliably process natural language, NLP systems must generalize to the long tail of rare utterances. We propose a method to create challenging benchmarks that require generalizing to the tail of the distribution by re-splitting existing datasets. We create 'Likelihood Splits' where examples that are assigned lower likelihood by a pre-trained language model (LM) are placed in the test set, and more likely examples are in the training set. This simple approach can be customized to construct meaningful train-test splits for a wide range of tasks. Likelihood Splits surface more challenges than random splits: relative error rates of state-of-the-art models increase by 59% for semantic parsing on Spider, 93% for natural language inference on SNLI, and 33% for yes/no question answering on BoolQ, on our splits compared with the corresponding random splits. Moreover, Likelihood Splits create fairer benchmarks than adversarial filtering; when the LM used to create the splits is also e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#29615;&#22659;&#24863;&#30693;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21487;&#25191;&#34892;&#30340;&#21160;&#20316;&#35745;&#21010;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#29615;&#22659;&#23545;&#35937;&#21644;&#23545;&#35937;&#20851;&#31995;&#20316;&#20026;&#39069;&#22806;&#36755;&#20837;&#65292;&#35774;&#35745;&#20102;&#26032;&#39062;&#30340;&#35780;&#20998;&#20989;&#25968;&#65292;&#29983;&#25104;&#30340;&#21487;&#25191;&#34892;&#35745;&#21010;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;LLM&#26041;&#27861;&#26377;&#26356;&#39640;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2210.04964</link><description>&lt;p&gt;
&#24102;&#29615;&#22659;&#24863;&#30693;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21487;&#25191;&#34892;&#30340;&#21160;&#20316;&#35745;&#21010;
&lt;/p&gt;
&lt;p&gt;
Generating Executable Action Plans with Environmentally-Aware Language Models. (arXiv:2210.04964v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#29615;&#22659;&#24863;&#30693;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21487;&#25191;&#34892;&#30340;&#21160;&#20316;&#35745;&#21010;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#29615;&#22659;&#23545;&#35937;&#21644;&#23545;&#35937;&#20851;&#31995;&#20316;&#20026;&#39069;&#22806;&#36755;&#20837;&#65292;&#35774;&#35745;&#20102;&#26032;&#39062;&#30340;&#35780;&#20998;&#20989;&#25968;&#65292;&#29983;&#25104;&#30340;&#21487;&#25191;&#34892;&#35745;&#21010;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;LLM&#26041;&#27861;&#26377;&#26356;&#39640;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#20174;&#39640;&#23618;&#27425;&#25991;&#26412;&#26597;&#35810;&#20013;&#29983;&#25104;&#26426;&#22120;&#20154;&#20195;&#29702;&#30340;&#21160;&#20316;&#35745;&#21010;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#19981;&#32771;&#34385;&#26426;&#22120;&#20154;&#30340;&#29615;&#22659;&#65292;&#23548;&#33268;&#29983;&#25104;&#30340;&#35745;&#21010;&#21487;&#33021;&#26080;&#27861;&#25191;&#34892;&#65292;&#22240;&#20026;&#35745;&#21010;&#20013;&#30340;&#21160;&#20316;&#27169;&#31946;&#19981;&#28165;&#25110;&#21463;&#21040;&#29615;&#22659;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#24102;&#29615;&#22659;&#24863;&#30693;&#30340;&#21487;&#25191;&#34892;&#21160;&#20316;&#35745;&#21010;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#23558;&#29615;&#22659;&#23545;&#35937;&#21644;&#23545;&#35937;&#20851;&#31995;&#20316;&#20026;&#39069;&#22806;&#36755;&#20837;&#38598;&#25104;&#21040;LLM&#21160;&#20316;&#35745;&#21010;&#29983;&#25104;&#20013;&#65292;&#20197;&#25552;&#20379;&#31995;&#32479;&#23545;&#21608;&#22260;&#29615;&#22659;&#30340;&#24863;&#30693;&#65292;&#20174;&#32780;&#29983;&#25104;&#19982;&#22330;&#26223;&#20013;&#23384;&#22312;&#30340;&#29289;&#20307;&#30456;&#23545;&#24212;&#30340;&#35745;&#21010;&#21160;&#20316;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35780;&#20998;&#20989;&#25968;&#65292;&#24110;&#21161;&#31995;&#32479;&#28040;&#38500;&#29289;&#20307;&#23454;&#20363;&#20043;&#38388;&#30340;&#27495;&#20041;&#24182;&#32771;&#34385;&#23427;&#20204;&#30340;&#29366;&#24577;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;LLM&#26041;&#27861;&#29983;&#25104;&#20102;&#26356;&#39640;&#25104;&#21151;&#29575;&#30340;&#21487;&#25191;&#34892;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) trained using massive text datasets have recently shown promise in generating action plans for robotic agents from high level text queries. However, these models typically do not consider the robot's environment, resulting in generated plans that may not actually be executable, due to ambiguities in the planned actions or environmental constraints. In this paper, we propose an approach to generate environmentally-aware action plans that agents are better able to execute. Our approach involves integrating environmental objects and object relations as additional inputs into LLM action plan generation to provide the system with an awareness of its surroundings, resulting in plans where each generated action is mapped to objects present in the scene. We also design a novel scoring function that, along with generating the action steps and associating them with objects, helps the system disambiguate among object instances and take into account their states. We ev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;377808&#39318;&#33521;&#25991;&#27468;&#26354;&#27468;&#35789;&#36827;&#34892;&#22823;&#35268;&#27169;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#21450;&#26102;&#30340;&#24615;&#21035;&#27495;&#35270;&#30340;&#22686;&#21152;&#20197;&#21450;&#19981;&#21516;&#24615;&#21035;&#34920;&#28436;&#32773;&#30340;&#35821;&#35328;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2208.02052</link><description>&lt;p&gt;
&#27468;&#35789;&#20013;&#30340;&#24615;&#21035;&#27495;&#35270;&#21644;&#24615;&#21035;&#20559;&#35265;&#30340;&#22823;&#35268;&#27169;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Large scale analysis of gender bias and sexism in song lyrics. (arXiv:2208.02052v3 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.02052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;377808&#39318;&#33521;&#25991;&#27468;&#26354;&#27468;&#35789;&#36827;&#34892;&#22823;&#35268;&#27169;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#21450;&#26102;&#30340;&#24615;&#21035;&#27495;&#35270;&#30340;&#22686;&#21152;&#20197;&#21450;&#19981;&#21516;&#24615;&#21035;&#34920;&#28436;&#32773;&#30340;&#35821;&#35328;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#20998;&#26512;&#20102;&#8220;Two Million Song Database&#8221;&#35821;&#26009;&#24211;&#20013;377808&#39318;&#33521;&#25991;&#27468;&#26354;&#27468;&#35789;&#65292;&#30528;&#37325;&#20998;&#26512;&#20102;&#20116;&#21313;&#24180;&#65288;1960-2010&#65289;&#38388;&#24615;&#21035;&#27495;&#35270;&#30340;&#34920;&#36798;&#65292;&#20197;&#21450;&#23545;&#24615;&#21035;&#20559;&#24046;&#30340;&#35780;&#27979;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#24615;&#21035;&#27495;&#35270;&#20998;&#31867;&#22120;&#65292;&#25105;&#20204;&#22312;&#36739;&#22823;&#30340;&#35268;&#27169;&#19978;&#35782;&#21035;&#20102;&#24615;&#21035;&#27495;&#35270;&#27468;&#35789;&#65292;&#36828;&#36229;&#21069;&#20154;&#29992;&#25163;&#21160;&#26631;&#27880;&#27969;&#34892;&#27468;&#26354;&#30340;&#23567;&#26679;&#26412;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#22312;&#27468;&#26354;&#27468;&#35789;&#19978;&#23398;&#20064;&#30340;&#35789;&#23884;&#20837;&#26469;&#34913;&#37327;&#20851;&#32852;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#24615;&#21035;&#20559;&#35265;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23588;&#20854;&#26159;&#30001;&#30007;&#24615;&#33402;&#26415;&#23478;&#28436;&#21809;&#30340;&#27969;&#34892;&#27468;&#26354;&#20013;&#30340;&#24615;&#21035;&#27495;&#35270;&#20869;&#23481;&#22312;&#26102;&#38388;&#19978;&#21576;&#36880;&#28176;&#22686;&#22810;&#30340;&#36235;&#21183;&#12290;&#26681;&#25454;&#34920;&#28436;&#32773;&#30340;&#24615;&#21035;&#19981;&#21516;&#65292;&#27468;&#26354;&#36824;&#26174;&#31034;&#20986;&#19981;&#21516;&#30340;&#35821;&#35328;&#20559;&#35265;&#65292;&#30007;&#24615;&#29420;&#21809;&#33402;&#26415;&#23478;&#30340;&#27468;&#26354;&#20013;&#21253;&#21547;&#26356;&#22810;&#21644;&#26356;&#24378;&#30340;&#20559;&#35265;&#12290;&#36825;&#26159;&#31532;&#19968;&#27425;&#36827;&#34892;&#36825;&#31181;&#22823;&#35268;&#27169;&#30340;&#20998;&#26512;&#65292;&#20026;&#25105;&#20204;&#25581;&#31034;&#20102;&#27969;&#34892;&#25991;&#21270;&#36825;&#19968;&#37325;&#35201;&#37096;&#20998;&#30340;&#35821;&#35328;&#29992;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We employ Natural Language Processing techniques to analyse 377808 English song lyrics from the "Two Million Song Database" corpus, focusing on the expression of sexism across five decades (1960-2010) and the measurement of gender biases. Using a sexism classifier, we identify sexist lyrics at a larger scale than previous studies using small samples of manually annotated popular songs. Furthermore, we reveal gender biases by measuring associations in word embeddings learned on song lyrics. We find sexist content to increase across time, especially from male artists and for popular songs appearing in Billboard charts. Songs are also shown to contain different language biases depending on the gender of the performer, with male solo artist songs containing more and stronger biases. This is the first large scale analysis of this type, giving insights into language usage in such an influential part of popular culture.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#26234;&#33021;&#20307;&#22270;&#20687;&#21442;&#29031;&#28216;&#25103;&#33258;&#36866;&#24212;&#19981;&#21516;&#21548;&#20247;&#30340;&#30446;&#26631;&#20219;&#21153;&#25551;&#36848;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843; CLIP &#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#36866;&#37197;&#22120;&#65292;&#22312;&#36866;&#24212;&#21548;&#20247;&#30340;&#35821;&#35328;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#33258;&#28982;&#35821;&#35328;&#19987;&#19994;&#21270;&#12290;</title><link>http://arxiv.org/abs/2206.08349</link><description>&lt;p&gt;
&#20102;&#35299;&#20320;&#30340;&#21548;&#20247;&#65306;&#29992;&#21548;&#20247;&#20943;&#27861;&#19987;&#38376;&#21270;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Know your audience: specializing grounded language models with listener subtraction. (arXiv:2206.08349v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.08349
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#26234;&#33021;&#20307;&#22270;&#20687;&#21442;&#29031;&#28216;&#25103;&#33258;&#36866;&#24212;&#19981;&#21516;&#21548;&#20247;&#30340;&#30446;&#26631;&#20219;&#21153;&#25551;&#36848;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843; CLIP &#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#36866;&#37197;&#22120;&#65292;&#22312;&#36866;&#24212;&#21548;&#20247;&#30340;&#35821;&#35328;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#33258;&#28982;&#35821;&#35328;&#19987;&#19994;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#27807;&#36890;&#38656;&#35201;&#36866;&#24212;&#27599;&#20010;&#20132;&#38469;&#24773;&#22659;&#30340;&#29305;&#27530;&#24615;&#65292;&#27604;&#22914;&#19982;&#27599;&#20010;&#20132;&#20114;&#20249;&#20276;&#20998;&#20139;&#30340;&#20849;&#21516;&#35821;&#22659;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#20511;&#37492;&#23545;&#35805;&#28216;&#25103; Dixit &#30340;&#24605;&#24819;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#22270;&#20687;&#21442;&#29031;&#28216;&#25103;&#65292;&#35757;&#32451;&#19968;&#20010;&#35828;&#35805;&#32773;&#27169;&#22411;&#26469;&#25551;&#36848;&#19968;&#20010;&#30446;&#26631;&#22270;&#20687;&#65292;&#20351;&#24471;&#19968;&#20010;&#21548;&#32773;&#33021;&#22815;&#22312;&#24178;&#25200;&#39033;&#20013;&#27491;&#30830;&#22320;&#35782;&#21035;&#20986;&#30446;&#26631;&#22270;&#20687;&#65292;&#32780;&#21478;&#19968;&#20010;&#21548;&#32773;&#21017;&#19981;&#33021;&#12290;&#36825;&#35201;&#27714;&#35828;&#35805;&#32773;&#21033;&#29992;&#23427;&#19982;&#19981;&#21516;&#21548;&#32773;&#30340;&#20849;&#21516;&#30693;&#35782;&#24046;&#24322;&#36827;&#34892;&#36866;&#24212;&#12290;&#26412;&#30740;&#31350;&#36824;&#23637;&#31034;&#20102;&#22312;&#36825;&#31181;&#23545;&#27604;&#12289;&#22810;&#26234;&#33021;&#20307;&#30340;&#35821;&#22659;&#19979;&#24494;&#35843; CLIP &#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#27880;&#24847;&#21147;&#36866;&#37197;&#22120;&#20250;&#33258;&#28982;&#22320;&#20135;&#29983;&#19978;&#19979;&#25991;&#20381;&#36182;&#30340;&#33258;&#28982;&#35821;&#35328;&#19987;&#19994;&#21270;&#65292;&#19988;&#21482;&#38656;&#35201;&#36890;&#36807;&#22870;&#21169;&#32780;&#26080;&#38656;&#30452;&#25509;&#30417;&#30563;&#26469;&#23454;&#29616;&#12290;&#36890;&#36807;&#25511;&#21046;&#23454;&#39564;&#65292;&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#29992;&#20004;&#20010;&#21548;&#32773;&#26469;&#35757;&#32451;&#35828;&#35805;&#32773;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective communication requires adapting to the idiosyncrasies of each communicative context--such as the common ground shared with each partner. Humans demonstrate this ability to specialize to their audience in many contexts, such as the popular game Dixit. We take inspiration from Dixit to formulate a multi-agent image reference game where a (trained) speaker model is rewarded for describing a target image such that one (pretrained) listener model can correctly identify it among distractors, but another listener cannot. To adapt, the speaker must exploit differences in the knowledge it shares with the different listeners. We show that finetuning an attention-based adapter between a CLIP vision encoder and a large language model in this contrastive, multi-agent setting gives rise to context-dependent natural language specialization from rewards only, without direct supervision. Through controlled experiments, we show that training a speaker with two listeners that perceive different
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#31867;&#37325;&#26032;&#21442;&#25968;&#21270;&#25216;&#24039;&#30340;&#22238;&#35793;&#31471;&#21040;&#31471;&#35757;&#32451;&#26041;&#27861;&#65292;&#26469;&#26377;&#25928;&#22320;&#20943;&#23569;&#20004;&#20010;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#38388;&#31163;&#25955;&#23646;&#24615;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#23454;&#29616;&#31471;&#21040;&#31471;&#24335;&#30340;&#35757;&#32451;&#65292;&#33719;&#24471;&#20102;&#27604;&#20197;&#21069;&#22522;&#20934;&#27979;&#35797;&#26356;&#22909;&#30340;BLEU&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2202.08465</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#31867;&#37325;&#26032;&#21442;&#25968;&#21270;&#25216;&#24039;&#30340;&#22238;&#35793;&#31471;&#21040;&#31471;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
End-to-End Training for Back-Translation with Categorical Reparameterization Trick. (arXiv:2202.08465v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.08465
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#31867;&#37325;&#26032;&#21442;&#25968;&#21270;&#25216;&#24039;&#30340;&#22238;&#35793;&#31471;&#21040;&#31471;&#35757;&#32451;&#26041;&#27861;&#65292;&#26469;&#26377;&#25928;&#22320;&#20943;&#23569;&#20004;&#20010;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#38388;&#31163;&#25955;&#23646;&#24615;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#23454;&#29616;&#31471;&#21040;&#31471;&#24335;&#30340;&#35757;&#32451;&#65292;&#33719;&#24471;&#20102;&#27604;&#20197;&#21069;&#22522;&#20934;&#27979;&#35797;&#26356;&#22909;&#30340;BLEU&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22238;&#35793;&#26159;&#19968;&#31181;&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#26377;&#25928;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#12290;&#39044;&#20808;&#35757;&#32451;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#32763;&#35793;&#21333;&#35821;&#21477;&#23376;&#24182;&#29983;&#25104;&#21512;&#25104;&#30340;&#21452;&#35821;&#21477;&#23545;&#20197;&#35757;&#32451;&#21478;&#19968;&#20010;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;&#23558;&#20004;&#20010;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#20998;&#21035;&#29702;&#35299;&#20026;&#25512;&#29702;&#21644;&#29983;&#25104;&#27169;&#22411;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#37319;&#29992;&#20102;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#30340;&#22521;&#35757;&#26694;&#26550;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#32763;&#35793;&#21477;&#23376;&#30340;&#31163;&#25955;&#23646;&#24615;&#20351;&#24471;&#26799;&#24230;&#20449;&#24687;&#26080;&#27861;&#22312;&#20004;&#20010;NMT&#27169;&#22411;&#20043;&#38388;&#27969;&#21160;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31867;&#37325;&#26032;&#21442;&#25968;&#21270;&#25216;&#24039;&#65292;&#20351;&#24471;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#21487;&#24494;&#20998;&#30340;&#21477;&#23376;&#65292;&#20351;&#24471;VAE&#30340;&#35757;&#32451;&#26694;&#26550;&#21487;&#20197;&#20197;&#31471;&#21040;&#31471;&#26041;&#24335;&#24037;&#20316;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#35757;&#32451;&#20102;NMT&#27169;&#22411;&#65292;&#24182;&#22312;WMT&#32763;&#35793;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#27604;&#20197;&#21069;&#22522;&#20934;&#27979;&#35797;&#26356;&#22909;&#30340;BLEU&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Back-translation is an effective semi-supervised learning framework in neural machine translation (NMT). A pre-trained NMT model translates monolingual sentences and makes synthetic bilingual sentence pairs for the training of the other NMT model, and vice versa. Understanding the two NMT models as inference and generation models, respectively, previous works applied the training framework of variational auto-encoder (VAE). However, the discrete property of translated sentences prevents gradient information from flowing between the two NMT models. In this paper, we propose a categorical reparameterization trick that makes NMT models generate differentiable sentences so that the VAE's training framework can work in the end-to-end fashion. Our experiments demonstrate that our method effectively trains the NMT models and achieves better BLEU scores than the previous baseline on the datasets of the WMT translation task.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#26234;&#33021;&#20307;&#38388;&#30340;&#28040;&#24687;&#20256;&#36882;&#29615;&#22659;&#65292;&#20351;&#24471;&#26234;&#33021;&#20307;&#33021;&#22815;&#36890;&#36807;&#36830;&#32493;&#22768;&#23398;&#36890;&#36947;&#36827;&#34892;&#36890;&#35759;&#24182;&#35266;&#23519;&#21040;&#26032;&#20852;&#35821;&#35328;&#30340;&#20135;&#29983;&#19982;&#29305;&#28857;&#65292;&#32467;&#26524;&#34920;&#26126;&#65306;&#19982;&#31163;&#25955;&#22411;&#20449;&#21495;&#19981;&#21516;&#65292;&#22768;&#23398;&#35762;&#35805;&#32773;&#23398;&#20064;&#20351;&#29992;&#20887;&#20313;&#20449;&#24687;&#20197;&#25552;&#39640;&#20390;&#21548;&#32773;&#30340;&#36830;&#36143;&#24615;&#12290;</title><link>http://arxiv.org/abs/2111.02827</link><description>&lt;p&gt;
&#36890;&#36807;&#36830;&#32493;&#22768;&#23398;&#36890;&#36947;&#36827;&#34892;&#22810;&#26234;&#33021;&#20307;&#36890;&#35759;&#23398;&#20064;&#21548;&#35828;&#33021;&#21147;&#30340;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Towards Learning to Speak and Hear Through Multi-Agent Communication over a Continuous Acoustic Channel. (arXiv:2111.02827v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.02827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#26234;&#33021;&#20307;&#38388;&#30340;&#28040;&#24687;&#20256;&#36882;&#29615;&#22659;&#65292;&#20351;&#24471;&#26234;&#33021;&#20307;&#33021;&#22815;&#36890;&#36807;&#36830;&#32493;&#22768;&#23398;&#36890;&#36947;&#36827;&#34892;&#36890;&#35759;&#24182;&#35266;&#23519;&#21040;&#26032;&#20852;&#35821;&#35328;&#30340;&#20135;&#29983;&#19982;&#29305;&#28857;&#65292;&#32467;&#26524;&#34920;&#26126;&#65306;&#19982;&#31163;&#25955;&#22411;&#20449;&#21495;&#19981;&#21516;&#65292;&#22768;&#23398;&#35762;&#35805;&#32773;&#23398;&#20064;&#20351;&#29992;&#20887;&#20313;&#20449;&#24687;&#20197;&#25552;&#39640;&#20390;&#21548;&#32773;&#30340;&#36830;&#36143;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#24050;&#25104;&#20026;&#30740;&#31350;&#26234;&#33021;&#20307;&#38388;&#26032;&#20852;&#36890;&#35759;&#30340;&#26377;&#25928;&#25163;&#27573;&#65292;&#20294;&#23545;&#20110;&#36830;&#32493;&#22768;&#23398;&#36890;&#35759;&#21364;&#40092;&#26377;&#30740;&#31350;&#12290;&#36825;&#26356;&#31867;&#20284;&#20110;&#20154;&#31867;&#33719;&#24471;&#35821;&#35328;&#30340;&#26041;&#24335;&#65307;&#20154;&#31867;&#23156;&#20799;&#20027;&#35201;&#36890;&#36807;&#19982;&#30475;&#25252;&#32773;&#30340;&#36830;&#32493;&#20449;&#21495;&#20132;&#20114;&#26469;&#20064;&#24471;&#35821;&#35328;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#29616;&#22312;&#30340;&#30446;&#26631;&#26159;&#25552;&#20379;&#19968;&#20010;&#24179;&#21488;&#65292;&#20197;&#24320;&#22987;&#22635;&#34917;&#20154;&#31867;&#21644;&#26234;&#33021;&#20307;&#36890;&#20449;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#35753;&#25105;&#20204;&#33021;&#22815;&#20998;&#26512;&#36830;&#32493;&#20449;&#21495;&#20197;&#21450;&#23427;&#20204;&#20135;&#29983;&#30340;&#26041;&#24335;&#65292;&#23427;&#20204;&#30340;&#29305;&#24449;&#20197;&#21450;&#23427;&#20204;&#19982;&#20154;&#31867;&#35821;&#35328;&#20064;&#24471;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-agent reinforcement learning has been used as an effective means to study emergent communication between agents, yet little focus has been given to continuous acoustic communication. This would be more akin to human language acquisition; human infants acquire language in large part through continuous signalling with their caregivers. We therefore ask: Are we able to observe emergent language between agents with a continuous communication channel? Our goal is to provide a platform to begin bridging the gap between human and agent communication, allowing us to analyse continuous signals, how they emerge, their characteristics, and how they relate to human language acquisition. We propose a messaging environment where a Speaker agent needs to convey a set of attributes to a Listener over a noisy acoustic channel. Using DQN to train our agents, we show that: (1) unlike the discrete case, the acoustic Speaker learns redundancy to improve Listener coherency, (2) the acoustic Speaker de
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#20171;&#32461;&#20102;&#19968;&#20123;&#20027;&#35201;&#30340;&#35789;&#21521;&#37327;&#26500;&#24314;&#31574;&#30053;&#65292;&#31216;&#20026;word embeddings&#65292;&#36825;&#20123;&#31574;&#30053;&#22522;&#20110;&#20998;&#24067;&#20551;&#35774;&#65292;&#32534;&#30721;&#20102;&#35821;&#27861;&#21644;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#34987;&#35777;&#26126;&#22312;&#24456;&#22810;NLP&#20219;&#21153;&#20013;&#26159;&#26377;&#29992;&#30340;&#39069;&#22806;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/1901.09069</link><description>&lt;p&gt;
&#35789;&#21521;&#37327;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Word Embeddings: A Survey. (arXiv:1901.09069v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1901.09069
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#20171;&#32461;&#20102;&#19968;&#20123;&#20027;&#35201;&#30340;&#35789;&#21521;&#37327;&#26500;&#24314;&#31574;&#30053;&#65292;&#31216;&#20026;word embeddings&#65292;&#36825;&#20123;&#31574;&#30053;&#22522;&#20110;&#20998;&#24067;&#20551;&#35774;&#65292;&#32534;&#30721;&#20102;&#35821;&#27861;&#21644;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#34987;&#35777;&#26126;&#22312;&#24456;&#22810;NLP&#20219;&#21153;&#20013;&#26159;&#26377;&#29992;&#30340;&#39069;&#22806;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#21015;&#20986;&#24182;&#25551;&#36848;&#20102;&#36817;&#26399;&#20027;&#35201;&#30340;&#31574;&#30053;&#65292;&#22522;&#20110;&#20998;&#24067;&#20551;&#35774;&#65292;&#29992;&#20110;&#26500;&#24314;&#21333;&#35789;&#30340;&#22266;&#23450;&#38271;&#24230;&#12289;&#23494;&#38598;&#21644;&#20998;&#24067;&#24335;&#34920;&#31034;&#12290; &#36825;&#20123;&#34920;&#31034;&#29616;&#22312;&#36890;&#24120;&#34987;&#31216;&#20026;&#35789;&#21521;&#37327;&#65292;&#24182;&#19988;&#38500;&#20102;&#32534;&#30721;&#20986;&#20196;&#20154;&#24778;&#35766;&#30340;&#35821;&#27861;&#21644;&#35821;&#20041;&#20449;&#24687;&#22806;&#65292;&#22312;&#35768;&#22810;&#19979;&#28216;NLP&#20219;&#21153;&#20013;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#29992;&#30340;&#39069;&#22806;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work lists and describes the main recent strategies for building fixed-length, dense and distributed representations for words, based on the distributional hypothesis. These representations are now commonly called word embeddings and, in addition to encoding surprisingly good syntactic and semantic information, have been proven useful as extra features in many downstream NLP tasks.
&lt;/p&gt;</description></item></channel></rss>