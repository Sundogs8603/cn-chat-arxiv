<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#25968;&#25454;&#21462;&#20195;&#21644;&#25968;&#25454;&#31215;&#32047;&#20004;&#31181;&#24773;&#20917;&#65292;&#21457;&#29616;&#32047;&#31215;&#25968;&#25454;&#21487;&#20197;&#38450;&#27490;&#27169;&#22411;&#23849;&#28291;&#12290;</title><link>https://arxiv.org/abs/2404.01413</link><description>&lt;p&gt;
&#27169;&#22411;&#23849;&#28291;&#26159;&#21542;&#19981;&#21487;&#36991;&#20813;&#65311;&#36890;&#36807;&#32047;&#31215;&#30495;&#23454;&#21644;&#21512;&#25104;&#25968;&#25454;&#25171;&#30772;&#36882;&#24402;&#30340;&#35781;&#21650;
&lt;/p&gt;
&lt;p&gt;
Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01413
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#25968;&#25454;&#21462;&#20195;&#21644;&#25968;&#25454;&#31215;&#32047;&#20004;&#31181;&#24773;&#20917;&#65292;&#21457;&#29616;&#32047;&#31215;&#25968;&#25454;&#21487;&#20197;&#38450;&#27490;&#27169;&#22411;&#23849;&#28291;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#27169;&#22411;&#30340;&#28608;&#22686;&#65292;&#20197;&#21450;&#22312;&#32593;&#32476;&#35268;&#27169;&#25968;&#25454;&#19978;&#30340;&#39044;&#35757;&#32451;&#65292;&#19968;&#20010;&#21450;&#26102;&#30340;&#38382;&#39064;&#28014;&#20986;&#27700;&#38754;&#65306;&#24403;&#36825;&#20123;&#27169;&#22411;&#34987;&#35757;&#32451;&#22312;&#23427;&#20204;&#33258;&#24049;&#29983;&#25104;&#30340;&#36755;&#20986;&#19978;&#26102;&#20250;&#21457;&#29983;&#20160;&#20040;&#65311;&#26368;&#36817;&#23545;&#27169;&#22411;&#25968;&#25454;&#21453;&#39304;&#24490;&#29615;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#36825;&#26679;&#30340;&#24490;&#29615;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#23849;&#28291;&#65292;&#21363;&#24615;&#33021;&#38543;&#30528;&#27599;&#27425;&#27169;&#22411;&#25311;&#21512;&#36845;&#20195;&#36880;&#28176;&#19979;&#38477;&#65292;&#30452;&#21040;&#26368;&#26032;&#30340;&#27169;&#22411;&#21464;&#24471;&#26080;&#29992;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#20960;&#31687;&#30740;&#31350;&#27169;&#22411;&#23849;&#28291;&#30340;&#35770;&#25991;&#37117;&#20551;&#35774;&#38543;&#30528;&#26102;&#38388;&#25512;&#31227;&#65292;&#26032;&#25968;&#25454;&#20250;&#21462;&#20195;&#26087;&#25968;&#25454;&#65292;&#32780;&#19981;&#26159;&#20551;&#35774;&#25968;&#25454;&#20250;&#38543;&#26102;&#38388;&#32047;&#31215;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#36825;&#20004;&#31181;&#24773;&#20917;&#65292;&#24182;&#34920;&#26126;&#31215;&#32047;&#25968;&#25454;&#21487;&#20197;&#38450;&#27490;&#27169;&#22411;&#23849;&#28291;&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#19968;&#20010;&#35299;&#26512;&#21487;&#22788;&#29702;&#30340;&#35774;&#32622;&#65292;&#20854;&#20013;&#19968;&#31995;&#21015;&#32447;&#24615;&#27169;&#22411;&#25311;&#21512;&#21040;&#20808;&#21069;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#22914;&#26524;&#25968;&#25454;&#34987;&#26367;&#25442;&#65292;&#27979;&#35797;&#35823;&#24046;&#20250;&#38543;&#30528;&#27169;&#22411;&#25311;&#21512;&#36845;&#20195;&#27425;&#25968;&#32447;&#24615;&#22686;&#21152;&#65307;&#25105;&#20204;&#25193;&#23637;&#20102;&#36825;&#20010;&#30740;&#31350;&#25506;&#35752;&#20102;&#25968;&#25454;&#36880;&#28176;&#32047;&#31215;&#30340;&#24773;&#20917;&#19979;&#20250;&#21457;&#29983;&#20160;&#20040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01413v1 Announce Type: cross  Abstract: The proliferation of generative models, combined with pretraining on web-scale data, raises a timely question: what happens when these models are trained on their own generated outputs? Recent investigations into model-data feedback loops discovered that such loops can lead to model collapse, a phenomenon where performance progressively degrades with each model-fitting iteration until the latest model becomes useless. However, several recent papers studying model collapse assumed that new data replace old data over time rather than assuming data accumulate over time. In this paper, we compare these two settings and show that accumulating data prevents model collapse. We begin by studying an analytically tractable setup in which a sequence of linear models are fit to the previous models' predictions. Previous work showed if data are replaced, the test error increases linearly with the number of model-fitting iterations; we extend this r
&lt;/p&gt;</description></item><item><title>NUMTEMP&#26159;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#65292;&#19987;&#27880;&#20110;&#39564;&#35777;&#22797;&#26434;&#30340;&#25968;&#23383;&#35770;&#28857;&#65292;&#37327;&#21270;&#20102;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#30495;&#23454;&#19990;&#30028;&#25968;&#23383;&#35770;&#28857;&#39564;&#35777;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.17169</link><description>&lt;p&gt;
NUMTEMP&#65306;&#19968;&#20010;&#29992;&#20110;&#39564;&#35777;&#24102;&#26377;&#32479;&#35745;&#21644;&#26102;&#38388;&#34920;&#36798;&#24335;&#30340;&#35770;&#28857;&#30340;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
NUMTEMP: A real-world benchmark to verify claims with statistical and temporal expressions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17169
&lt;/p&gt;
&lt;p&gt;
NUMTEMP&#26159;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#65292;&#19987;&#27880;&#20110;&#39564;&#35777;&#22797;&#26434;&#30340;&#25968;&#23383;&#35770;&#28857;&#65292;&#37327;&#21270;&#20102;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#30495;&#23454;&#19990;&#30028;&#25968;&#23383;&#35770;&#28857;&#39564;&#35777;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#20107;&#23454;&#26816;&#26597;&#22312;&#25968;&#23383;&#26102;&#20195;&#24212;&#23545;&#19981;&#26029;&#22686;&#38271;&#30340;&#38169;&#35823;&#20449;&#24687;&#26041;&#38754;&#24341;&#36215;&#20102;&#26497;&#22823;&#20852;&#36259;&#12290;&#29616;&#26377;&#31995;&#32479;&#20027;&#35201;&#19987;&#27880;&#20110;&#32500;&#22522;&#30334;&#31185;&#19978;&#30340;&#21512;&#25104;&#35770;&#28857;&#65292;&#24182;&#19988;&#22312;&#30495;&#23454;&#19990;&#30028;&#35770;&#28857;&#19978;&#20063;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;Numtemp&#65292;&#19968;&#20010;&#22810;&#26679;&#21270;&#12289;&#22810;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#65292;&#19987;&#38376;&#20851;&#27880;&#25968;&#23383;&#35770;&#28857;&#65292;&#21253;&#25324;&#26102;&#38388;&#12289;&#32479;&#35745;&#21644;&#22810;&#26679;&#21270;&#26041;&#38754;&#30340;&#32454;&#31890;&#24230;&#20803;&#25968;&#25454;&#65292;&#24182;&#19988;&#20855;&#26377;&#19981;&#27844;&#38706;&#30340;&#35777;&#25454;&#25910;&#38598;&#12290;&#36825;&#35299;&#20915;&#20102;&#39564;&#35777;&#30495;&#23454;&#19990;&#30028;&#25968;&#23383;&#35770;&#28857;&#30340;&#25361;&#25112;&#65292;&#36825;&#20123;&#35770;&#28857;&#22797;&#26434;&#65292;&#24448;&#24448;&#32570;&#20047;&#31934;&#30830;&#20449;&#24687;&#65292;&#36825;&#26159;&#29616;&#26377;&#20316;&#21697;&#20027;&#35201;&#20851;&#27880;&#21512;&#25104;&#35770;&#28857;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35780;&#20272;&#24182;&#37327;&#21270;&#20102;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#22312;&#39564;&#35777;&#25968;&#23383;&#35770;&#28857;&#20219;&#21153;&#20013;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#22522;&#20110;&#35770;&#28857;&#20998;&#35299;&#30340;&#26041;&#27861;&#12289;&#22522;&#20110;&#25968;&#23383;&#29702;&#35299;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#26368;&#20339;&#22522;&#32447;&#23454;&#29616;&#20102;58.32&#30340;&#23439;F1&#20998;&#25968;&#12290;&#36825;&#35777;&#26126;&#20102;Numtemp&#30340;&#20851;&#38190;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17169v1 Announce Type: cross  Abstract: Automated fact checking has gained immense interest to tackle the growing misinformation in the digital era. Existing systems primarily focus on synthetic claims on Wikipedia, and noteworthy progress has also been made on real-world claims. In this work, we release Numtemp, a diverse, multi-domain dataset focused exclusively on numerical claims, encompassing temporal, statistical and diverse aspects with fine-grained metadata and an evidence collection without leakage. This addresses the challenge of verifying real-world numerical claims, which are complex and often lack precise information, not addressed by existing works that mainly focus on synthetic claims. We evaluate and quantify the limitations of existing solutions for the task of verifying numerical claims. We also evaluate claim decomposition based methods, numerical understanding based models and our best baselines achieves a macro-F1 of 58.32. This demonstrates that Numtemp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#40065;&#26834;&#24615;&#28151;&#21512;&#20195;&#30721;&#32763;&#35793;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#21644;&#32852;&#21512;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;&#24320;&#21457;&#20102;Hinglish&#21040;&#33521;&#35821;&#30340;&#24179;&#34892;&#35821;&#26009;&#24211;&#20197;&#21450;&#25552;&#20986;&#30340;&#33021;&#22815;&#22788;&#29702;&#22122;&#22768;&#30340;&#32852;&#21512;&#35757;&#32451;&#27169;&#22411;RCMT&#12290;</title><link>https://arxiv.org/abs/2403.16771</link><description>&lt;p&gt;
&#29992;&#20110;&#40065;&#26834;&#24615;&#28151;&#21512;&#20195;&#30721;&#32763;&#35793;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#21644;&#32852;&#21512;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Synthetic Data Generation and Joint Learning for Robust Code-Mixed Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#40065;&#26834;&#24615;&#28151;&#21512;&#20195;&#30721;&#32763;&#35793;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#21644;&#32852;&#21512;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;&#24320;&#21457;&#20102;Hinglish&#21040;&#33521;&#35821;&#30340;&#24179;&#34892;&#35821;&#26009;&#24211;&#20197;&#21450;&#25552;&#20986;&#30340;&#33021;&#22815;&#22788;&#29702;&#22122;&#22768;&#30340;&#32852;&#21512;&#35757;&#32451;&#27169;&#22411;RCMT&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22810;&#35821;&#35328;&#19990;&#30028;&#20013;&#30340;&#24191;&#27867;&#32593;&#32476;&#20132;&#27969;&#20026;&#22312;&#21333;&#20010;&#35805;&#35821;&#20013;&#28151;&#21512;&#22810;&#31181;&#35821;&#35328;&#65288;&#21448;&#31216;&#28151;&#21512;&#20195;&#30721;&#35821;&#35328;&#65289;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;&#30001;&#20110;&#26631;&#27880;&#25968;&#25454;&#30340;&#31232;&#32570;&#21644;&#22122;&#38899;&#30340;&#23384;&#22312;&#65292;&#36825;&#32473;&#35745;&#31639;&#27169;&#22411;&#24102;&#26469;&#20102;&#20005;&#23803;&#25361;&#25112;&#12290;&#22312;&#36164;&#28304;&#21294;&#20047;&#30340;&#29615;&#22659;&#20013;&#32531;&#35299;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#26159;&#36890;&#36807;&#32763;&#35793;&#21033;&#29992;&#36164;&#28304;&#20016;&#23500;&#35821;&#35328;&#20013;&#30340;&#29616;&#26377;&#25968;&#25454;&#12290;&#26412;&#25991;&#38024;&#23545;&#28151;&#21512;&#20195;&#30721;&#65288;&#21360;&#22320;&#35821;&#21644;&#23391;&#21152;&#25289;&#35821;&#65289;&#21040;&#33521;&#35821;&#30340;&#26426;&#22120;&#32763;&#35793;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21512;&#25104;&#24320;&#21457;&#20102;HINMIX&#19968;&#20010;&#21360;&#22320;&#35821;&#21040;&#33521;&#35821;&#30340;&#24179;&#34892;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547;&#32422;420&#19975;&#20010;&#21477;&#23545;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RCMT&#65292;&#19968;&#31181;&#22522;&#20110;&#24378;&#20581;&#25200;&#21160;&#30340;&#32852;&#21512;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#24178;&#20928;&#21644;&#24102;&#22122;&#22768;&#21333;&#35789;&#20043;&#38388;&#20849;&#20139;&#21442;&#25968;&#65292;&#23398;&#20064;&#22788;&#29702;&#29616;&#23454;&#19990;&#30028;&#28151;&#21512;&#20195;&#30721;&#25991;&#26412;&#20013;&#30340;&#22122;&#22768;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;RCMT&#22312;&#38646;-shot&#35774;&#32622;&#20013;&#23545;&#23391;&#21152;&#25289;&#35821;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16771v1 Announce Type: new  Abstract: The widespread online communication in a modern multilingual world has provided opportunities to blend more than one language (aka code-mixed language) in a single utterance. This has resulted a formidable challenge for the computational models due to the scarcity of annotated data and presence of noise. A potential solution to mitigate the data scarcity problem in low-resource setup is to leverage existing data in resource-rich language through translation. In this paper, we tackle the problem of code-mixed (Hinglish and Bengalish) to English machine translation. First, we synthetically develop HINMIX, a parallel corpus of Hinglish to English, with ~4.2M sentence pairs. Subsequently, we propose RCMT, a robust perturbation based joint-training model that learns to handle noise in the real-world code-mixed text by parameter sharing across clean and noisy words. Further, we show the adaptability of RCMT in a zero-shot setup for Bengalish t
&lt;/p&gt;</description></item><item><title>Linguacodus&#26159;&#19968;&#31181;&#21019;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#37096;&#32626;&#21160;&#24577;&#27969;&#27700;&#32447;&#21644;&#31934;&#32454;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23558;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#36716;&#25442;&#20026;&#20195;&#30721;&#30340;&#33258;&#21160;&#21270;&#36807;&#31243;&#65292;&#26497;&#22823;&#22320;&#25512;&#36827;&#20102;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.11585</link><description>&lt;p&gt;
Linguacodus&#65306;&#19968;&#31181;&#22312;&#26426;&#22120;&#23398;&#20064;&#27969;&#27700;&#32447;&#20013;&#36827;&#34892;&#21464;&#38761;&#24615;&#20195;&#30721;&#29983;&#25104;&#30340;&#21327;&#21516;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Linguacodus: A Synergistic Framework for Transformative Code Generation in Machine Learning Pipelines
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11585
&lt;/p&gt;
&lt;p&gt;
Linguacodus&#26159;&#19968;&#31181;&#21019;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#37096;&#32626;&#21160;&#24577;&#27969;&#27700;&#32447;&#21644;&#31934;&#32454;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23558;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#36716;&#25442;&#20026;&#20195;&#30721;&#30340;&#33258;&#21160;&#21270;&#36807;&#31243;&#65292;&#26497;&#22823;&#22320;&#25512;&#36827;&#20102;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#26029;&#21457;&#23637;&#30340;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#65292;&#23558;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26080;&#32541;&#36716;&#21270;&#20026;&#21487;&#25191;&#34892;&#20195;&#30721;&#20173;&#28982;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Linguacodus&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#24615;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#37096;&#32626;&#19968;&#20010;&#21160;&#24577;&#27969;&#27700;&#32447;&#65292;&#36890;&#36807;&#39640;&#32423;&#25968;&#25454;&#22609;&#24418;&#25351;&#20196;&#65292;&#23558;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#36845;&#20195;&#22320;&#36716;&#25442;&#20026;&#20195;&#30721;&#26469;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#12290;Linguacodus&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#32463;&#36807;&#31934;&#32454;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#33021;&#22815;&#35780;&#20272;&#21508;&#31181;&#38382;&#39064;&#30340;&#22810;&#26679;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#20026;&#29305;&#23450;&#20219;&#21153;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;&#31934;&#32454;&#35843;&#25972;&#36807;&#31243;&#65292;&#24182;&#38416;&#26126;&#20102;&#22914;&#20309;&#23558;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#36716;&#21270;&#20026;&#21151;&#33021;&#24615;&#20195;&#30721;&#12290;Linguacodus&#20195;&#34920;&#20102;&#33258;&#21160;&#21270;&#20195;&#30721;&#29983;&#25104;&#30340;&#37325;&#22823;&#39134;&#36291;&#65292;&#26377;&#25928;&#22320;&#24357;&#21512;&#20102;&#20219;&#21153;&#25551;&#36848;&#21644;&#21487;&#25191;&#34892;&#20195;&#30721;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#23427;&#23545;&#25512;&#36827;&#36328;&#19981;&#21516;&#39046;&#22495;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11585v1 Announce Type: cross  Abstract: In the ever-evolving landscape of machine learning, seamless translation of natural language descriptions into executable code remains a formidable challenge. This paper introduces Linguacodus, an innovative framework designed to tackle this challenge by deploying a dynamic pipeline that iteratively transforms natural language task descriptions into code through high-level data-shaping instructions. The core of Linguacodus is a fine-tuned large language model (LLM), empowered to evaluate diverse solutions for various problems and select the most fitting one for a given task. This paper details the fine-tuning process, and sheds light on how natural language descriptions can be translated into functional code. Linguacodus represents a substantial leap towards automated code generation, effectively bridging the gap between task descriptions and executable code. It holds great promise for advancing machine learning applications across div
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LAB&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#20811;&#26381;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#65292;&#36890;&#36807;&#20998;&#31867;&#27861;&#25351;&#23548;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#21644;&#22810;&#38454;&#27573;&#35843;&#25972;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#23545;&#26114;&#36149;&#20154;&#24037;&#26631;&#27880;&#21644;GPT-4&#31561;&#19987;&#26377;&#27169;&#22411;&#20381;&#36182;&#36739;&#23569;&#30340;&#22823;&#35268;&#27169;&#23545;&#40784;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#12289;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#19981;&#20250;&#20986;&#29616;&#28798;&#38590;&#24615;&#36951;&#24536;&#24773;&#20917;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;LLM&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.01081</link><description>&lt;p&gt;
LAB&#65306;&#38024;&#23545;ChatBots&#30340;&#22823;&#35268;&#27169;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
LAB: Large-Scale Alignment for ChatBots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01081
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LAB&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#20811;&#26381;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#65292;&#36890;&#36807;&#20998;&#31867;&#27861;&#25351;&#23548;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#21644;&#22810;&#38454;&#27573;&#35843;&#25972;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#23545;&#26114;&#36149;&#20154;&#24037;&#26631;&#27880;&#21644;GPT-4&#31561;&#19987;&#26377;&#27169;&#22411;&#20381;&#36182;&#36739;&#23569;&#30340;&#22823;&#35268;&#27169;&#23545;&#40784;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#12289;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#19981;&#20250;&#20986;&#29616;&#28798;&#38590;&#24615;&#36951;&#24536;&#24773;&#20917;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;LLM&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;LAB&#65288;ChatBots&#30340;&#22823;&#35268;&#27169;&#23545;&#40784;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26088;&#22312;&#20811;&#26381;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35757;&#32451;&#20013;&#25351;&#20196;&#35843;&#25972;&#38454;&#27573;&#30340;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;&#20998;&#31867;&#27861;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#21644;&#22810;&#38454;&#27573;&#35843;&#25972;&#26694;&#26550;&#65292;LAB&#26174;&#33879;&#20943;&#23569;&#23545;&#26114;&#36149;&#30340;&#20154;&#31867;&#27880;&#37322;&#21644;&#35832;&#22914;GPT-4&#20043;&#31867;&#30340;&#19987;&#26377;&#27169;&#22411;&#30340;&#20381;&#36182;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#20351;&#29992;LAB&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#20960;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24615;&#33021;&#21487;&#20197;&#19982;&#20351;&#29992;&#20256;&#32479;&#20154;&#31867;&#27880;&#37322;&#25110;GPT-4&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#22240;&#27492;&#65292;&#22312;&#19981;&#20250;&#20986;&#29616;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#12289;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#22686;&#24378;LLM&#30340;&#33021;&#21147;&#21644;&#25351;&#20196;&#36981;&#24490;&#34892;&#20026;&#65292;&#26631;&#24535;&#30528;&#22312;&#39640;&#25928;&#35757;&#32451;&#21508;&#31181;&#24212;&#29992;&#30340;LLM&#26041;&#38754;&#36808;&#20986;&#20102;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01081v1 Announce Type: new  Abstract: This work introduces LAB (Large-scale Alignment for chatBots), a novel methodology designed to overcome the scalability challenges in the instruction-tuning phase of large language model (LLM) training. Leveraging a taxonomy-guided synthetic data generation process and a multi-phase tuning framework, LAB significantly reduces reliance on expensive human annotations and proprietary models like GPT-4. We demonstrate that LAB-trained models can achieve competitive performance across several benchmarks compared to models trained with traditional human-annotated or GPT-4 generated synthetic data. Thus offering a scalable, cost-effective solution for enhancing LLM capabilities and instruction-following behaviors without the drawbacks of catastrophic forgetting, marking a step forward in the efficient training of LLMs for a wide range of applications.
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;&#38754;&#21521;&#23454;&#20307;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#20219;&#21153;&#21644;&#23545;&#40784;&#26694;&#26550;&#65292;&#25552;&#39640;&#20102;&#26032;&#38395;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#20219;&#21153;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.19404</link><description>&lt;p&gt;
&#38754;&#21521;&#23454;&#20307;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#26694;&#26550;&#29992;&#20110;&#26032;&#38395;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Entity-Aware Multimodal Alignment Framework for News Image Captioning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19404
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;&#38754;&#21521;&#23454;&#20307;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#20219;&#21153;&#21644;&#23545;&#40784;&#26694;&#26550;&#65292;&#25552;&#39640;&#20102;&#26032;&#38395;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#20219;&#21153;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#38395;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#20219;&#21153;&#26159;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#20219;&#21153;&#30340;&#19968;&#20010;&#21464;&#20307;&#65292;&#35201;&#27714;&#27169;&#22411;&#29983;&#25104;&#19968;&#20010;&#26356;&#20855;&#20449;&#24687;&#24615;&#30340;&#23383;&#24149;&#65292;&#20854;&#20013;&#21253;&#21547;&#26032;&#38395;&#22270;&#20687;&#21644;&#30456;&#20851;&#26032;&#38395;&#25991;&#31456;&#12290;&#36817;&#24180;&#26469;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21457;&#23637;&#36805;&#36895;&#65292;&#24182;&#22312;&#26032;&#38395;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21069;&#26223;&#12290;&#28982;&#32780;&#65292;&#26681;&#25454;&#25105;&#20204;&#30340;&#23454;&#39564;&#65292;&#24120;&#35265;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#35774;&#23450;&#19979;&#29983;&#25104;&#23454;&#20307;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#21363;&#20351;&#22312;&#26032;&#38395;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#31616;&#21333;&#24494;&#35843;&#65292;&#23427;&#20204;&#22788;&#29702;&#23454;&#20307;&#20449;&#24687;&#30340;&#33021;&#21147;&#20173;&#28982;&#26377;&#38480;&#12290;&#20026;&#20102;&#33719;&#24471;&#19968;&#20010;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#26469;&#22788;&#29702;&#22810;&#27169;&#24577;&#23454;&#20307;&#20449;&#24687;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#20010;&#22810;&#27169;&#24577;&#23454;&#20307;&#24863;&#30693;&#23545;&#40784;&#20219;&#21153;&#21644;&#19968;&#20010;&#23545;&#40784;&#26694;&#26550;&#65292;&#20197;&#23545;&#40784;&#27169;&#22411;&#24182;&#29983;&#25104;&#26032;&#38395;&#22270;&#20687;&#23383;&#24149;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;GoodNews&#25968;&#25454;&#38598;&#19978;&#23558;CIDEr&#20998;&#25968;&#25552;&#39640;&#21040;86.29&#65288;&#20174;72.33&#65289;&#65292;&#22312;NYTimes800k&#25968;&#25454;&#38598;&#19978;&#23558;&#20854;&#25552;&#39640;&#21040;85.61&#65288;&#20174;70.83&#65289;&#65292;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19404v1 Announce Type: cross  Abstract: News image captioning task is a variant of image captioning task which requires model to generate a more informative caption with news image and the associated news article. Multimodal Large Language models have developed rapidly in recent years and is promising in news image captioning task. However, according to our experiments, common MLLMs are not good at generating the entities in zero-shot setting. Their abilities to deal with the entities information are still limited after simply fine-tuned on news image captioning dataset. To obtain a more powerful model to handle the multimodal entity information, we design two multimodal entity-aware alignment tasks and an alignment framework to align the model and generate the news image captions. Our method achieves better results than previous state-of-the-art models in CIDEr score (72.33 -&gt; 86.29) on GoodNews dataset and (70.83 -&gt; 85.61) on NYTimes800k dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30740;&#31350;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20010;&#20154;&#20215;&#20540;&#22312;&#19981;&#21516;&#32972;&#26223;&#19979;&#30340;&#34920;&#36798;&#31283;&#23450;&#24615;&#65292;&#36890;&#36807;&#27169;&#25311;&#23545;&#35805;&#30340;&#26041;&#24335;&#36827;&#34892;&#35780;&#20272;&#65292;&#23545;19&#20010;LLMs&#36827;&#34892;&#27604;&#36739;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2402.14846</link><description>&lt;p&gt;
&#22362;&#25345;&#20320;&#30340;&#35282;&#33394;&#65281;&#20010;&#20154;&#20215;&#20540;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Stick to your Role! Stability of Personal Values Expressed in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14846
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30740;&#31350;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20010;&#20154;&#20215;&#20540;&#22312;&#19981;&#21516;&#32972;&#26223;&#19979;&#30340;&#34920;&#36798;&#31283;&#23450;&#24615;&#65292;&#36890;&#36807;&#27169;&#25311;&#23545;&#35805;&#30340;&#26041;&#24335;&#36827;&#34892;&#35780;&#20272;&#65292;&#23545;19&#20010;LLMs&#36827;&#34892;&#27604;&#36739;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20934;&#27979;&#35797;&#25110;&#24515;&#29702;&#38382;&#21367;&#30340;&#26631;&#20934;&#26041;&#24335;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26159;&#25552;&#20379;&#35768;&#22810;&#26469;&#28304;&#20110;&#31867;&#20284;&#26368;&#23567;&#32972;&#26223;&#30340;&#19981;&#21516;&#26597;&#35810;&#65288;&#20363;&#22914;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65289;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;LLM&#39640;&#24230;&#20381;&#36182;&#20110;&#32972;&#26223;&#65292;&#22240;&#27492;&#20174;&#36825;&#31181;&#26368;&#23567;&#32972;&#26223;&#35780;&#20272;&#20013;&#24471;&#20986;&#30340;&#32467;&#35770;&#21487;&#33021;&#23545;&#27169;&#22411;&#22312;&#37096;&#32626;&#20013;&#30340;&#34892;&#20026;&#65288;&#22312;&#37027;&#37324;&#23427;&#23558;&#26292;&#38706;&#20110;&#35768;&#22810;&#26032;&#32972;&#26223;&#65289;&#30340;&#35828;&#26126;&#24456;&#23569;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20381;&#36182;&#20110;&#32972;&#26223;&#30340;&#29305;&#24615;&#24212;&#35813;&#20316;&#20026;LLM&#27604;&#36739;&#30340;&#21478;&#19968;&#20010;&#32500;&#24230;&#26469;&#30740;&#31350;&#65292;&#32780;&#19981;&#26159;&#20854;&#20182;&#32500;&#24230;&#65292;&#22914;&#35748;&#30693;&#33021;&#21147;&#12289;&#30693;&#35782;&#25110;&#27169;&#22411;&#22823;&#23567;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#22312;&#19981;&#21516;&#32972;&#26223;&#19979;&#65288;&#27169;&#25311;&#23545;&#19981;&#21516;&#35805;&#39064;&#30340;&#23545;&#35805;&#65289;&#20215;&#20540;&#34920;&#36798;&#31283;&#23450;&#24615;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#24182;&#20351;&#29992;&#26631;&#20934;&#24515;&#29702;&#23398;&#38382;&#21367;&#65288;PVQ&#65289;&#21644;&#34892;&#20026;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#27979;&#37327;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#26469;&#33258;&#20116;&#20010;&#23478;&#26063;&#30340;19&#20010;&#24320;&#28304;LLM&#12290;&#20511;&#37492;&#24515;&#29702;&#23398;&#26041;&#27861;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#31561;&#32423;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14846v1 Announce Type: cross  Abstract: The standard way to study Large Language Models (LLMs) through benchmarks or psychology questionnaires is to provide many different queries from similar minimal contexts (e.g. multiple choice questions). However, due to LLM's highly context-dependent nature, conclusions from such minimal-context evaluations may be little informative about the model's behavior in deployment (where it will be exposed to many new contexts). We argue that context-dependence should be studied as another dimension of LLM comparison alongside others such as cognitive abilities, knowledge, or model size. In this paper, we present a case-study about the stability of value expression over different contexts (simulated conversations on different topics), and as measured using a standard psychology questionnaire (PVQ) and a behavioral downstream task. We consider 19 open-sourced LLMs from five families. Reusing methods from psychology, we study Rank-order stabilit
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;Factiverse AI&#27169;&#22411;&#65292;&#21487;&#20197;&#36827;&#34892;&#36328;&#35821;&#35328;&#30340;&#31471;&#21040;&#31471;&#20107;&#23454;&#26680;&#26597;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#20026;&#20107;&#23454;&#26680;&#26597;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#20248;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.12147</link><description>&lt;p&gt;
&#36328;&#35821;&#35328;&#35268;&#27169;&#30340;&#31471;&#21040;&#31471;&#20107;&#23454;&#26680;&#26597;
&lt;/p&gt;
&lt;p&gt;
End-to-end multilingual fact-checking at scale
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12147
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;Factiverse AI&#27169;&#22411;&#65292;&#21487;&#20197;&#36827;&#34892;&#36328;&#35821;&#35328;&#30340;&#31471;&#21040;&#31471;&#20107;&#23454;&#26680;&#26597;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#20026;&#20107;&#23454;&#26680;&#26597;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#20248;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#22914;&#20309;&#20351;&#29992;Factiverse AI&#27169;&#22411;&#22312;100&#22810;&#31181;&#35821;&#35328;&#20013;&#36827;&#34892;&#31471;&#21040;&#31471;&#20107;&#23454;&#26680;&#26597;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23454;&#39564;&#24615;&#22522;&#20934;&#27979;&#35797;&#23637;&#31034;&#65292;&#20026;&#20107;&#23454;&#26680;&#26597;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#32988;&#36807;GPT-4&#12289;GPT-3.5-Turbo&#21644;Mistral-7b&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12147v1 Announce Type: cross  Abstract: In this article, we describe how you can perform end-to-end fact-checking in over 100 languages using Factiverse AI models. We also show through an experimental benchmark that fine-tuned models tailored for fact-checking tasks outperform Large Language Models such as GPT-4, GPT-3.5-Turbo, and Mistral-7b.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#22522;&#30784;&#19990;&#30028;&#27169;&#22411;&#23545;&#20110;&#20855;&#36523;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#65292;&#24403;&#21069;&#30340;&#22522;&#30784;&#27169;&#22411;&#26080;&#27861;&#20934;&#30830;&#24314;&#27169;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#29289;&#29702;&#30456;&#20114;&#20316;&#29992;&#12290;&#22240;&#26524;&#20851;&#31995;&#30340;&#30740;&#31350;&#26377;&#21161;&#20110;&#26500;&#24314;&#30495;&#23454;&#19990;&#30028;&#27169;&#22411;&#65292;&#25552;&#39640;&#23545;&#21487;&#33021;&#30456;&#20114;&#20316;&#29992;&#32467;&#26524;&#30340;&#20934;&#30830;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.06665</link><description>&lt;p&gt;
&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#22522;&#30784;&#19990;&#30028;&#27169;&#22411;&#22312;&#20855;&#36523;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
The Essential Role of Causality in Foundation World Models for Embodied AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06665
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#22522;&#30784;&#19990;&#30028;&#27169;&#22411;&#23545;&#20110;&#20855;&#36523;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#65292;&#24403;&#21069;&#30340;&#22522;&#30784;&#27169;&#22411;&#26080;&#27861;&#20934;&#30830;&#24314;&#27169;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#29289;&#29702;&#30456;&#20114;&#20316;&#29992;&#12290;&#22240;&#26524;&#20851;&#31995;&#30340;&#30740;&#31350;&#26377;&#21161;&#20110;&#26500;&#24314;&#30495;&#23454;&#19990;&#30028;&#27169;&#22411;&#65292;&#25552;&#39640;&#23545;&#21487;&#33021;&#30456;&#20114;&#20316;&#29992;&#32467;&#26524;&#30340;&#20934;&#30830;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#22522;&#30784;&#27169;&#22411;&#20013;&#21462;&#24471;&#30340;&#36827;&#23637;&#65292;&#23588;&#20854;&#26159;&#22312;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#21644;&#23545;&#35805;&#20195;&#29702;&#26041;&#38754;&#65292;&#24341;&#21457;&#20102;&#23545;&#20855;&#22791;&#26222;&#36941;&#33021;&#21147;&#30340;&#20855;&#36523;&#20195;&#29702;&#20154;&#28508;&#21147;&#30340;&#20852;&#36259;&#12290;&#36825;&#26679;&#30340;&#20195;&#29702;&#20154;&#38656;&#35201;&#33021;&#22815;&#22312;&#35768;&#22810;&#19981;&#21516;&#30340;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#25191;&#34892;&#26032;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#22522;&#30784;&#27169;&#22411;&#26410;&#33021;&#20934;&#30830;&#24314;&#27169;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#29289;&#29702;&#30456;&#20114;&#20316;&#29992;&#65292;&#22240;&#27492;&#23545;&#20110;&#20855;&#36523;&#20154;&#24037;&#26234;&#33021;&#32780;&#35328;&#26159;&#19981;&#22815;&#30340;&#12290;&#22240;&#26524;&#20851;&#31995;&#30340;&#30740;&#31350;&#26377;&#21161;&#20110;&#26500;&#24314;&#30495;&#23454;&#19990;&#30028;&#27169;&#22411;&#65292;&#36825;&#23545;&#20110;&#20934;&#30830;&#39044;&#27979;&#21487;&#33021;&#30456;&#20114;&#20316;&#29992;&#30340;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#30528;&#37325;&#25506;&#35752;&#20102;&#20026;&#21363;&#23558;&#21040;&#26469;&#30340;&#20855;&#36523;&#20195;&#29702;&#29983;&#25104;&#22522;&#30784;&#19990;&#30028;&#27169;&#22411;&#30340;&#21069;&#26223;&#65292;&#24182;&#23545;&#20854;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#30340;&#37325;&#35201;&#24615;&#25552;&#20986;&#20102;&#26032;&#30340;&#35266;&#28857;&#12290;&#25105;&#20204;&#35748;&#20026;&#25972;&#21512;&#22240;&#26524;&#20851;&#31995;&#26159;&#20419;&#36827;&#19982;&#19990;&#30028;&#30340;&#26377;&#24847;&#20041;&#30340;&#29289;&#29702;&#30456;&#20114;&#20316;&#29992;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#36825;&#19968;&#32972;&#26223;&#19979;&#23545;&#22240;&#26524;&#20851;&#31995;&#30340;&#35823;&#35299;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#23545;&#26410;&#26469;&#30340;&#23637;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in foundation models, especially in large multi-modal models and conversational agents, have ignited interest in the potential of generally capable embodied agents. Such agents would require the ability to perform new tasks in many different real-world environments. However, current foundation models fail to accurately model physical interactions with the real world thus not sufficient for Embodied AI. The study of causality lends itself to the construction of veridical world models, which are crucial for accurately predicting the outcomes of possible interactions. This paper focuses on the prospects of building foundation world models for the upcoming generation of embodied agents and presents a novel viewpoint on the significance of causality within these. We posit that integrating causal considerations is vital to facilitate meaningful physical interactions with the world. Finally, we demystify misconceptions about causality in this context and present our outlook fo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;LLM&#26159;&#21542;&#21487;&#20197;&#29992;&#20316;&#23545;&#25239;&#29983;&#25104;&#24335;&#21407;&#29983;&#24191;&#21578;&#30340;&#23545;&#31574;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#24191;&#21578;&#20542;&#21521;&#26597;&#35810;&#25968;&#25454;&#38598;&#21644;&#24102;&#33258;&#21160;&#25972;&#21512;&#24191;&#21578;&#30340;&#29983;&#25104;&#31572;&#26696;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#12290;</title><link>https://arxiv.org/abs/2402.04889</link><description>&lt;p&gt;
&#21457;&#29616;&#23545;&#35805;&#24335;&#25628;&#32034;&#20013;&#30340;&#29983;&#25104;&#24335;&#21407;&#29983;&#24191;&#21578;
&lt;/p&gt;
&lt;p&gt;
Detecting Generated Native Ads in Conversational Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;LLM&#26159;&#21542;&#21487;&#20197;&#29992;&#20316;&#23545;&#25239;&#29983;&#25104;&#24335;&#21407;&#29983;&#24191;&#21578;&#30340;&#23545;&#31574;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#24191;&#21578;&#20542;&#21521;&#26597;&#35810;&#25968;&#25454;&#38598;&#21644;&#24102;&#33258;&#21160;&#25972;&#21512;&#24191;&#21578;&#30340;&#29983;&#25104;&#31572;&#26696;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#24335;&#25628;&#32034;&#24341;&#25806;&#22914;YouChat&#21644;Microsoft Copilot&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20026;&#26597;&#35810;&#29983;&#25104;&#31572;&#26696;&#12290;&#23558;&#27492;&#25216;&#26415;&#29992;&#20110;&#29983;&#25104;&#24182;&#25972;&#21512;&#24191;&#21578;&#65292;&#32780;&#19981;&#26159;&#23558;&#24191;&#21578;&#19982;&#26377;&#26426;&#25628;&#32034;&#32467;&#26524;&#20998;&#24320;&#25918;&#32622;&#65292;&#21482;&#26159;&#19968;&#23567;&#27493;&#12290;&#36825;&#31181;&#31867;&#22411;&#30340;&#24191;&#21578;&#31867;&#20284;&#20110;&#21407;&#29983;&#24191;&#21578;&#21644;&#20135;&#21697;&#25918;&#32622;&#65292;&#20004;&#32773;&#37117;&#26159;&#38750;&#24120;&#26377;&#25928;&#30340;&#24494;&#22937;&#21644;&#25805;&#32437;&#24615;&#24191;&#21578;&#24418;&#24335;&#12290;&#22312;&#32771;&#34385;&#21040;&#19982;LLM&#30456;&#20851;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#26102;&#65292;&#20449;&#24687;&#25628;&#32034;&#32773;&#23558;&#24456;&#21487;&#33021;&#22312;&#19981;&#20037;&#30340;&#23558;&#26469;&#38754;&#20020;&#36825;&#31181;LLM&#25216;&#26415;&#30340;&#20351;&#29992;&#65292;&#22240;&#27492;&#20379;&#24212;&#21830;&#38656;&#35201;&#24320;&#21457;&#21487;&#25345;&#32493;&#30340;&#21830;&#19994;&#27169;&#24335;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;LLM&#26159;&#21542;&#20063;&#21487;&#20197;&#29992;&#20316;&#23545;&#25239;&#29983;&#25104;&#24335;&#21407;&#29983;&#24191;&#21578;&#30340;&#23545;&#31574;&#65292;&#21363;&#38459;&#27490;&#23427;&#20204;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#32534;&#21046;&#20102;&#19968;&#20010;&#22823;&#22411;&#30340;&#24191;&#21578;&#20542;&#21521;&#26597;&#35810;&#25968;&#25454;&#38598;&#21644;&#24102;&#33258;&#21160;&#25972;&#21512;&#24191;&#21578;&#30340;&#29983;&#25104;&#31572;&#26696;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational search engines such as YouChat and Microsoft Copilot use large language models (LLMs) to generate answers to queries. It is only a small step to also use this technology to generate and integrate advertising within these answers - instead of placing ads separately from the organic search results. This type of advertising is reminiscent of native advertising and product placement, both of which are very effective forms of subtle and manipulative advertising. It is likely that information seekers will be confronted with such use of LLM technology in the near future, especially when considering the high computational costs associated with LLMs, for which providers need to develop sustainable business models. This paper investigates whether LLMs can also be used as a countermeasure against generated native ads, i.e., to block them. For this purpose we compile a large dataset of ad-prone queries and of generated answers with automatically integrated ads to experiment with fin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;MEME&#8221;&#30340;&#22810;&#37325;&#23884;&#20837;&#27169;&#22411;&#65292;&#23558;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#35270;&#20026;&#22810;&#27169;&#24577;&#25968;&#25454;&#12290;&#36890;&#36807;&#32467;&#21512;&#8220;&#20266;&#31508;&#35760;&#8221;&#21644;&#22810;&#27169;&#24577;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#22312;&#32039;&#24613;&#31185;&#23460;&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#21333;&#27169;&#24577;&#23884;&#20837;&#26041;&#27861;&#21644;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#35813;&#27169;&#22411;&#22312;&#19981;&#21516;&#21307;&#38498;&#26426;&#26500;&#20043;&#38388;&#23384;&#22312;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00160</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#37325;&#23884;&#20837;&#27169;&#22411;&#30340;&#22810;&#27169;&#24335;&#20020;&#24202;&#20266;&#31508;&#35760;&#29992;&#20110;&#32039;&#24613;&#31185;&#23460;&#39044;&#27979;&#20219;&#21153;&#30340;&#21307;&#30103;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Multimodal Clinical Pseudo-notes for Emergency Department Prediction Tasks using Multiple Embedding Model for EHR (MEME)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;MEME&#8221;&#30340;&#22810;&#37325;&#23884;&#20837;&#27169;&#22411;&#65292;&#23558;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#35270;&#20026;&#22810;&#27169;&#24577;&#25968;&#25454;&#12290;&#36890;&#36807;&#32467;&#21512;&#8220;&#20266;&#31508;&#35760;&#8221;&#21644;&#22810;&#27169;&#24577;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#22312;&#32039;&#24613;&#31185;&#23460;&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#21333;&#27169;&#24577;&#23884;&#20837;&#26041;&#27861;&#21644;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#35813;&#27169;&#22411;&#22312;&#19981;&#21516;&#21307;&#38498;&#26426;&#26500;&#20043;&#38388;&#23384;&#22312;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#38024;&#23545;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#30340;&#22810;&#37325;&#23884;&#20837;&#27169;&#22411;&#65288;MEME&#65289;&#65292;&#36825;&#31181;&#26041;&#27861;&#23558;EHR&#35270;&#20026;&#22810;&#27169;&#24577;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#8220;&#20266;&#31508;&#35760;&#8221;&#65292;&#21363;&#23545;&#34920;&#26684;&#24418;&#24335;&#30340;EHR&#27010;&#24565;&#65288;&#22914;&#35786;&#26029;&#21644;&#33647;&#29289;&#65289;&#36827;&#34892;&#25991;&#26412;&#34920;&#31034;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#26377;&#25928;&#22320;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;EHR&#34920;&#31034;&#12290;&#35813;&#26694;&#26550;&#36824;&#37319;&#29992;&#20102;&#22810;&#27169;&#24577;&#26041;&#27861;&#65292;&#20998;&#21035;&#23884;&#20837;&#27599;&#20010;EHR&#27169;&#24577;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#22810;&#20010;&#21307;&#38498;&#31995;&#32479;&#30340;&#24613;&#35786;&#31185;&#20013;&#24212;&#29992;MEME&#26469;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;MEME&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#21333;&#27169;&#24577;&#23884;&#20837;&#26041;&#27861;&#21644;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#25152;&#26377;&#27979;&#35797;&#27169;&#22411;&#22312;&#19981;&#21516;&#21307;&#38498;&#26426;&#26500;&#20043;&#38388;&#30340;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#26126;&#26174;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce Multiple Embedding Model for EHR (MEME), an approach that views Electronic Health Records (EHR) as multimodal data. This approach incorporates "pseudo-notes", textual representations of tabular EHR concepts such as diagnoses and medications, and allows us to effectively employ Large Language Models (LLMs) for EHR representation. This framework also adopts a multimodal approach, embedding each EHR modality separately. We demonstrate the effectiveness of MEME by applying it to several tasks within the Emergency Department across multiple hospital systems. Our findings show that MEME surpasses the performance of both single modality embedding methods and traditional machine learning approaches. However, we also observe notable limitations in generalizability across hospital institutions for all tested models.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;NOLA&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#32593;&#32476;&#34920;&#31034;&#20026;&#20302;&#31209;&#38543;&#26426;&#22522;&#21521;&#37327;&#30340;&#32447;&#24615;&#32452;&#21512;&#26469;&#20943;&#23569;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#25968;&#37327;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#30340;&#36866;&#24212;&#21644;&#23384;&#20648;&#12290;</title><link>http://arxiv.org/abs/2310.02556</link><description>&lt;p&gt;
NOLA: &#32593;&#32476;&#20316;&#20026;&#20302;&#31209;&#38543;&#26426;&#22522;&#21521;&#37327;&#30340;&#32447;&#24615;&#32452;&#21512;
&lt;/p&gt;
&lt;p&gt;
NOLA: Networks as Linear Combination of Low Rank Random Basis. (arXiv:2310.02556v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02556
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;NOLA&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#32593;&#32476;&#34920;&#31034;&#20026;&#20302;&#31209;&#38543;&#26426;&#22522;&#21521;&#37327;&#30340;&#32447;&#24615;&#32452;&#21512;&#26469;&#20943;&#23569;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#25968;&#37327;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#30340;&#36866;&#24212;&#21644;&#23384;&#20648;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#30340;&#24778;&#20154;&#23569;&#26679;&#26412;&#24615;&#33021;&#65292;&#23427;&#20204;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26816;&#26597;&#28857;&#30340;&#24222;&#22823;&#22823;&#23567;&#65288;&#20363;&#22914;GPT-3&#30340;350GB&#65289;&#65292;&#23545;&#25152;&#26377;&#21442;&#25968;&#36827;&#34892;&#24494;&#35843;&#24182;&#20026;&#27599;&#20010;&#19979;&#28216;&#20219;&#21153;&#25110;&#39046;&#22495;&#23384;&#20648;&#19968;&#20010;&#21807;&#19968;&#27169;&#22411;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#12290;&#24403;&#21069;&#30340;&#25991;&#29486;&#65292;&#20363;&#22914;LoRA&#65292;&#23637;&#31034;&#20102;&#23545;LLM&#30340;&#21407;&#22987;&#26435;&#37325;&#36827;&#34892;&#20302;&#31209;&#20462;&#25913;&#30340;&#28508;&#21147;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#30340;&#27169;&#22411;&#30340;&#39640;&#25928;&#36866;&#24212;&#21644;&#23384;&#20648;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#23558;&#24494;&#35843;LLM&#25152;&#38656;&#30340;&#21442;&#25968;&#25968;&#37327;&#20943;&#23569;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#38480;&#21046;&#65306;1&#65289;&#21442;&#25968;&#20943;&#23569;&#21463;&#21040;&#31209;&#19968;&#20998;&#35299;&#30340;&#19979;&#30028;&#38480;&#21046;&#65292;2&#65289;&#20943;&#23569;&#30340;&#31243;&#24230;&#21463;&#21040;&#27169;&#22411;&#26550;&#26500;&#21644;&#36873;&#25321;&#30340;&#31209;&#30340;&#20005;&#37325;&#24433;&#21709;&#12290;&#20363;&#22914;&#65292;&#22312;&#26356;&#22823;&#27169;&#22411;&#20013;&#65292;&#21363;&#20351;&#26159;&#31209;&#19968;&#20998;&#35299;&#65292;&#21442;&#25968;&#30340;&#25968;&#37327;&#20063;&#21487;&#33021;&#36229;&#36807;&#30495;&#27491;&#38656;&#35201;&#36827;&#34892;&#36866;&#24212;&#30340;&#21442;&#25968;&#25968;&#37327;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;NOLA&#65292;&#23427;&#36890;&#36807;&#23558;&#32593;&#32476;&#34920;&#31034;&#20026;&#20302;&#31209;&#38543;&#26426;&#22522;&#21521;&#37327;&#30340;&#32447;&#24615;&#32452;&#21512;&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have recently gained popularity due to their impressive few-shot performance across various downstream tasks. However, fine-tuning all parameters and storing a unique model for each downstream task or domain becomes impractical because of the massive size of checkpoints (e.g., 350GB in GPT-3). Current literature, such as LoRA, showcases the potential of low-rank modifications to the original weights of an LLM, enabling efficient adaptation and storage for task-specific models. These methods can reduce the number of parameters needed to fine-tune an LLM by several orders of magnitude. Yet, these methods face two primary limitations: 1) the parameter reduction is lower-bounded by the rank one decomposition, and 2) the extent of reduction is heavily influenced by both the model architecture and the chosen rank. For instance, in larger models, even a rank one decomposition might exceed the number of parameters truly needed for adaptation. In this paper, we intr
&lt;/p&gt;</description></item></channel></rss>