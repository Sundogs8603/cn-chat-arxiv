<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#38646;&#36164;&#28304;&#20999;&#25442;&#35821;&#38899;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#30417;&#30563;&#35821;&#38899;&#32534;&#30721;&#22120;&#30340;&#20999;&#25442;&#35821;&#35328;&#33021;&#21147;&#65292;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#21644;&#27169;&#22411;&#22823;&#23567;&#23545;&#22522;&#20934;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#26174;&#31034;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#30340;&#35821;&#38899;&#32534;&#30721;&#22120;&#22312;&#20999;&#25442;&#35821;&#35328;&#22330;&#26223;&#20013;&#34920;&#29616;&#20248;&#20110;&#21333;&#35821;&#21464;&#20307;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2310.03018</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#38899;&#35821;&#21477;&#23545;&#30340;&#38646;&#36164;&#28304;&#20999;&#25442;&#35821;&#38899;&#22522;&#20934;&#36827;&#34892;&#22810;&#35821;&#35328;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Zero Resource Code-switched Speech Benchmark Using Speech Utterance Pairs For Multiple Spoken Languages. (arXiv:2310.03018v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03018
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#38646;&#36164;&#28304;&#20999;&#25442;&#35821;&#38899;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#30417;&#30563;&#35821;&#38899;&#32534;&#30721;&#22120;&#30340;&#20999;&#25442;&#35821;&#35328;&#33021;&#21147;&#65292;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#21644;&#27169;&#22411;&#22823;&#23567;&#23545;&#22522;&#20934;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#26174;&#31034;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#30340;&#35821;&#38899;&#32534;&#30721;&#22120;&#22312;&#20999;&#25442;&#35821;&#35328;&#22330;&#26223;&#20013;&#34920;&#29616;&#20248;&#20110;&#21333;&#35821;&#21464;&#20307;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#38646;&#36164;&#28304;&#20999;&#25442;&#35821;&#38899;&#22522;&#20934;&#65292;&#26088;&#22312;&#30452;&#25509;&#35780;&#20272;&#33258;&#30417;&#30563;&#35821;&#38899;&#32534;&#30721;&#22120;&#30340;&#20999;&#25442;&#35821;&#35328;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#25955;&#21333;&#20803;&#35821;&#35328;&#24314;&#27169;&#30340;&#22522;&#32447;&#31995;&#32479;&#65292;&#20197;&#23637;&#31034;&#22914;&#20309;&#20197;&#38646;&#36164;&#28304;&#30340;&#26041;&#24335;&#35780;&#20272;&#35821;&#38899;&#32534;&#30721;&#22120;&#30340;&#20999;&#25442;&#35821;&#35328;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#28085;&#30422;&#20102;&#21508;&#31181;&#30693;&#21517;&#30340;&#35821;&#38899;&#32534;&#30721;&#22120;&#65292;&#21253;&#25324;Wav2vec 2.0&#12289;HuBERT&#12289;XLSR&#31561;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#21644;&#27169;&#22411;&#22823;&#23567;&#23545;&#22522;&#20934;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23613;&#31649;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20999;&#25442;&#35821;&#35328;&#22330;&#26223;&#20013;&#65292;&#20855;&#26377;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#30340;&#35821;&#38899;&#32534;&#30721;&#22120;&#65288;&#20363;&#22914;XLSR&#65289;&#20248;&#20110;&#21333;&#35821;&#21464;&#20307;&#65288;Wav2vec 2.0&#12289;HuBERT&#65289;&#65292;&#20294;&#23427;&#20204;&#30340;&#20999;&#25442;&#35821;&#35328;&#33021;&#21147;&#20173;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new zero resource code-switched speech benchmark designed to directly assess the code-switching capabilities of self-supervised speech encoders. We showcase a baseline system of language modeling on discrete units to demonstrate how the code-switching abilities of speech encoders can be assessed in a zero-resource manner. Our experiments encompass a variety of well-known speech encoders, including Wav2vec 2.0, HuBERT, XLSR, etc. We examine the impact of pre-training languages and model size on benchmark performance. Notably, though our results demonstrate that speech encoders with multilingual pre-training, exemplified by XLSR, outperform monolingual variants (Wav2vec 2.0, HuBERT) in code-switching scenarios, there is still substantial room for improvement in their code-switching linguistic abilities.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#38382;&#31572;&#65288;MQA&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#32479;&#19968;&#19977;&#31181;&#22810;&#27169;&#24577;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#65292;&#36890;&#36807;&#37325;&#26032;&#32452;&#21512;&#25104;&#32479;&#19968;&#30340;&#25277;&#21462;&#22238;&#31572;&#33539;&#22260;&#21644;&#22810;&#36873;&#39064;&#38382;&#31572;&#31649;&#36947;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#21508;&#31181;&#29616;&#25104;&#30340;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#35774;&#32622;&#19979;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.03017</link><description>&lt;p&gt;
&#32479;&#19968;&#20449;&#24687;&#25552;&#21462;&#30340;&#22810;&#27169;&#24577;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Multimodal Question Answering for Unified Information Extraction. (arXiv:2310.03017v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#38382;&#31572;&#65288;MQA&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#32479;&#19968;&#19977;&#31181;&#22810;&#27169;&#24577;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#65292;&#36890;&#36807;&#37325;&#26032;&#32452;&#21512;&#25104;&#32479;&#19968;&#30340;&#25277;&#21462;&#22238;&#31572;&#33539;&#22260;&#21644;&#22810;&#36873;&#39064;&#38382;&#31572;&#31649;&#36947;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#21508;&#31181;&#29616;&#25104;&#30340;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#35774;&#32622;&#19979;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#20449;&#24687;&#25552;&#21462;&#26088;&#22312;&#20174;&#38750;&#32467;&#26500;&#21270;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#20449;&#24687;&#12290;&#30001;&#20110;&#20219;&#21153;&#21644;&#35774;&#32622;&#30340;&#22810;&#26679;&#24615;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#25552;&#21462;&#27169;&#22411;&#37117;&#26159;&#29305;&#23450;&#20110;&#20219;&#21153;&#21644;&#25968;&#25454;&#23494;&#38598;&#22411;&#30340;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#20855;&#26377;&#21508;&#31181;&#20219;&#21153;&#38656;&#27714;&#21644;&#26377;&#38480;&#26631;&#27880;&#25968;&#25454;&#30340;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#38382;&#31572;&#65288;MQA&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20854;&#37325;&#26032;&#32452;&#21512;&#25104;&#32479;&#19968;&#30340;&#26694;&#26550;&#25552;&#21462;&#22238;&#31572;&#33539;&#22260;&#24182;&#36827;&#34892;&#22810;&#36873;&#39064;&#38382;&#31572;&#12290;&#23545;&#20845;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65306;1&#65289;&#19982;&#22522;&#20934;&#25552;&#31034;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;MQA&#26694;&#26550;&#22312;&#22810;&#27169;&#24577;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#19978;&#22987;&#32456;&#26174;&#33879;&#25552;&#39640;&#21508;&#31181;&#29616;&#25104;&#30340;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMM&#65289;&#30340;&#24615;&#33021;&#12290;2&#65289;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#65292;MQA&#27604;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#22522;&#32447;&#27169;&#22411;&#26377;&#24456;&#22823;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#21487;&#20197;&#25104;&#21151;&#22320;&#36716;&#21270;&#20026;&#23569;&#26679;&#26412;&#35774;&#32622;&#65292;&#25552;&#21319;&#20102;&#35268;&#27169;&#20026;10B&#21442;&#25968;&#30340;LMMs&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal information extraction (MIE) aims to extract structured information from unstructured multimedia content. Due to the diversity of tasks and settings, most current MIE models are task-specific and data-intensive, which limits their generalization to real-world scenarios with diverse task requirements and limited labeled data. To address these issues, we propose a novel multimodal question answering (MQA) framework to unify three MIE tasks by reformulating them into a unified span extraction and multi-choice QA pipeline. Extensive experiments on six datasets show that: 1) Our MQA framework consistently and significantly improves the performances of various off-the-shelf large multimodal models (LMM) on MIE tasks, compared to vanilla prompting. 2) In the zero-shot setting, MQA outperforms previous state-of-the-art baselines by a large margin. In addition, the effectiveness of our framework can successfully transfer to the few-shot setting, enhancing LMMs on a scale of 10B param
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#65292;&#21464;&#21387;&#22120;&#21487;&#20197;&#23398;&#20064;&#21508;&#31181;&#23454;&#20540;&#20989;&#25968;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#20294;&#23545;&#20110;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#24615;&#33021;&#19979;&#38477;&#12290;&#21478;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#36825;&#20123;&#33021;&#21147;&#22312;&#22522;&#20110;&#27880;&#24847;&#21147;&#27169;&#22411;&#20013;&#30340;&#38480;&#21046;&#31243;&#24230;&#20197;&#21450;&#25512;&#24191;&#21040;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.03016</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#31163;&#25955;&#20989;&#25968;&#26469;&#29702;&#35299;&#21464;&#21387;&#22120;&#21644;LLM&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
Understanding In-Context Learning in Transformers and LLMs by Learning to Learn Discrete Functions. (arXiv:2310.03016v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03016
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#65292;&#21464;&#21387;&#22120;&#21487;&#20197;&#23398;&#20064;&#21508;&#31181;&#23454;&#20540;&#20989;&#25968;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#20294;&#23545;&#20110;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#24615;&#33021;&#19979;&#38477;&#12290;&#21478;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#36825;&#20123;&#33021;&#21147;&#22312;&#22522;&#20110;&#27880;&#24847;&#21147;&#27169;&#22411;&#20013;&#30340;&#38480;&#21046;&#31243;&#24230;&#20197;&#21450;&#25512;&#24191;&#21040;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#29702;&#35299;&#19978;&#19979;&#25991;&#23398;&#20064;&#29616;&#35937;&#65292;&#26368;&#36817;&#30340;&#24037;&#20316;&#37319;&#29992;&#20102;&#35268;&#33539;&#21270;&#30340;&#23454;&#39564;&#26694;&#26550;&#65292;&#24182;&#35777;&#26126;&#20102;&#21464;&#21387;&#22120;&#21487;&#20197;&#23398;&#20064;&#21508;&#31181;&#23454;&#20540;&#20989;&#25968;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#21464;&#21387;&#22120;&#22312;&#23454;&#29616;&#23398;&#20064;&#31639;&#27861;&#26041;&#38754;&#30340;&#38480;&#21046;&#20197;&#21450;&#23427;&#20204;&#23398;&#20064;&#20854;&#20182;&#24418;&#24335;&#31639;&#27861;&#30340;&#33021;&#21147;&#36824;&#19981;&#28165;&#26970;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#33021;&#21147;&#22312;&#22522;&#20110;&#27880;&#24847;&#21147;&#27169;&#22411;&#20013;&#30340;&#38480;&#21046;&#31243;&#24230;&#20063;&#19981;&#26126;&#30830;&#12290;&#27492;&#22806;&#65292;&#23578;&#19981;&#28165;&#26970;&#20174;&#36825;&#20123;&#35268;&#33539;&#21270;&#35774;&#32622;&#20013;&#24471;&#20986;&#30340;&#35265;&#35299;&#26159;&#21542;&#33021;&#25512;&#24191;&#21040;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20197;&#19979;&#26041;&#24335;&#21521;&#36825;&#20123;&#38382;&#39064;&#36808;&#36827;&#65306;&#65288;a&#65289;&#22312;&#19968;&#20010;&#21253;&#21547;&#21508;&#31181;&#24067;&#23572;&#20989;&#25968;&#31867;&#30340;&#27979;&#35797;&#24179;&#21488;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#21464;&#21387;&#22120;&#20960;&#20046;&#21487;&#20197;&#19982;&#8220;&#36739;&#31616;&#21333;&#8221;&#20219;&#21153;&#30340;&#26368;&#20248;&#23398;&#20064;&#31639;&#27861;&#30456;&#21305;&#37197;&#65292;&#20294;&#22312;&#8220;&#36739;&#22797;&#26434;&#8221;&#20219;&#21153;&#19978;&#20854;&#24615;&#33021;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
In order to understand the in-context learning phenomenon, recent works have adopted a stylized experimental framework and demonstrated that Transformers can learn gradient-based learning algorithms for various classes of real-valued functions. However, the limitations of Transformers in implementing learning algorithms, and their ability to learn other forms of algorithms are not well understood. Additionally, the degree to which these capabilities are confined to attention-based models is unclear. Furthermore, it remains to be seen whether the insights derived from these stylized settings can be extrapolated to pretrained Large Language Models (LLMs). In this work, we take a step towards answering these questions by demonstrating the following: (a) On a test-bed with a variety of Boolean function classes, we find that Transformers can nearly match the optimal learning algorithm for 'simpler' tasks, while their performance deteriorates on more 'complex' tasks. Additionally, we find th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#35745;&#31639;&#21644;&#33021;&#28304;&#21033;&#29992;&#24773;&#20917;&#65292;&#20197;&#24110;&#21161;&#33410;&#32422;&#25104;&#26412;&#12289;&#25552;&#39640;&#24615;&#33021;&#21644;&#20248;&#21270;&#25512;&#29702;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2310.03003</link><description>&lt;p&gt;
&#20174;&#21333;&#35789;&#21040;&#29926;&#29305;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#33021;&#28304;&#25104;&#26412;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference. (arXiv:2310.03003v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03003
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#35745;&#31639;&#21644;&#33021;&#28304;&#21033;&#29992;&#24773;&#20917;&#65292;&#20197;&#24110;&#21161;&#33410;&#32422;&#25104;&#26412;&#12289;&#25552;&#39640;&#24615;&#33021;&#21644;&#20248;&#21270;&#25512;&#29702;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30001;&#20110;&#20854;&#36229;&#36234;&#20197;&#24448;&#26368;&#20808;&#36827;&#27700;&#24179;&#30340;&#26032;&#29983;&#25104;&#33021;&#21147;&#32780;&#36805;&#36895;&#21463;&#21040;&#27426;&#36814;&#12290;&#36825;&#20123;&#25216;&#26415;&#36234;&#26469;&#36234;&#22810;&#22320;&#22312;&#27861;&#24459;&#12289;&#37329;&#34701;&#21644;&#21307;&#23398;&#31561;&#39046;&#22495;&#24471;&#21040;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#25512;&#29702;&#25152;&#38656;&#30340;&#35745;&#31639;&#21644;&#33021;&#28304;&#25104;&#26412;&#12290;&#23613;&#31649;&#29616;&#23454;&#20013;&#32463;&#24120;&#38656;&#35201;&#36825;&#20123;&#22823;&#22411;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;(&#20363;&#22914;ChatGPT)&#65292;&#20294;&#25512;&#29702;&#33021;&#28304;&#25104;&#26412;&#21364;&#27604;LLMs&#30340;&#35757;&#32451;&#33021;&#28304;&#25104;&#26412;&#24471;&#21040;&#20102;&#36739;&#23569;&#20851;&#27880;&#12290;&#38543;&#30528;&#36825;&#20123;&#26368;&#20808;&#36827;&#30340;LLMs&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#20351;&#29992;&#21644;&#37096;&#32626;&#36234;&#26469;&#36234;&#22810;&#65292;&#26356;&#22909;&#22320;&#29702;&#35299;&#23427;&#20204;&#30340;&#36164;&#28304;&#21033;&#29992;&#23545;&#20110;&#33410;&#32422;&#25104;&#26412;&#12289;&#25552;&#39640;&#24615;&#33021;&#12289;&#39640;&#25928;&#21033;&#29992;&#30828;&#20214;&#21644;&#20248;&#21270;&#25512;&#29702;&#31574;&#30053;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#36827;&#34892;&#30340;&#23454;&#39564;&#65292;&#20197;&#30740;&#31350;&#20351;&#29992;LLMs&#36827;&#34892;&#25512;&#29702;&#30340;&#35745;&#31639;&#21644;&#33021;&#28304;&#21033;&#29992;&#24773;&#20917;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#24182;&#36827;&#34892;&#20102;&#21021;&#27493;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have exploded in popularity due to their new generative capabilities that go far beyond prior state-of-the-art. These technologies are increasingly being leveraged in various domains such as law, finance, and medicine. However, these models carry significant computational challenges, especially the compute and energy costs required for inference. Inference energy costs already receive less attention than the energy costs of training LLMs -- despite how often these large models are called on to conduct inference in reality (e.g., ChatGPT). As these state-of-the-art LLMs see increasing usage and deployment in various domains, a better understanding of their resource utilization is crucial for cost-savings, scaling performance, efficient hardware usage, and optimal inference strategies.  In this paper, we describe experiments conducted to study the computational and energy utilization of inference with LLMs. We benchmark and conduct a preliminary analysis of t
&lt;/p&gt;</description></item><item><title>ECoFLaP&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31895;&#21040;&#32454;&#30340;&#36880;&#23618;&#21098;&#26525;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#21387;&#32553;&#21644;&#37096;&#32626;&#26102;&#30340;&#35745;&#31639;&#21644;&#33021;&#32791;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.02998</link><description>&lt;p&gt;
ECoFLaP: &#39640;&#25928;&#30340;&#31895;&#21040;&#32454;&#30340;&#36880;&#23618;&#21098;&#26525;&#26041;&#27861;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ECoFLaP: Efficient Coarse-to-Fine Layer-Wise Pruning for Vision-Language Models. (arXiv:2310.02998v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02998
&lt;/p&gt;
&lt;p&gt;
ECoFLaP&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31895;&#21040;&#32454;&#30340;&#36880;&#23618;&#21098;&#26525;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#21387;&#32553;&#21644;&#37096;&#32626;&#26102;&#30340;&#35745;&#31639;&#21644;&#33021;&#32791;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#36890;&#36807;&#25972;&#21512;&#19981;&#21516;&#27169;&#24577;&#30340;&#20016;&#23500;&#20449;&#24687;&#65292;&#20840;&#38754;&#29702;&#35299;&#19990;&#30028;&#65292;&#24182;&#22312;&#21508;&#31181;&#22810;&#27169;&#24577;&#19979;&#28216;&#20219;&#21153;&#19978;&#21462;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#24040;&#22823;&#30340;&#35745;&#31639;/&#33021;&#32791;&#21644;&#30899;&#25490;&#25918;&#65292;&#37096;&#32626;LVLMs&#24448;&#24448;&#23384;&#22312;&#38382;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#20351;&#24471;&#37319;&#29992;&#20256;&#32479;&#30340;&#36845;&#20195;&#20840;&#23616;&#21098;&#26525;&#21464;&#24471;&#19981;&#21487;&#34892;&#65292;&#22240;&#20026;&#20854;&#38656;&#35201;&#35745;&#31639;&#25972;&#20010;&#22823;&#22411;&#27169;&#22411;&#30340;Hessian&#30697;&#38453;&#20197;&#36827;&#34892;&#31232;&#30095;&#21270;&#12290;&#30456;&#21453;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#36880;&#23618;&#21098;&#26525;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#20840;&#23616;&#21098;&#26525;&#30340;&#26114;&#36149;&#35745;&#31639;&#65292;&#24182;&#26681;&#25454;&#23618;&#20869;&#26435;&#37325;&#30340;&#37325;&#35201;&#24615;&#26377;&#25928;&#21387;&#32553;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24120;&#24120;&#30001;&#20110;&#32570;&#20047;&#20840;&#23616;&#35270;&#35282;&#32780;&#23548;&#33268;&#27169;&#22411;&#21387;&#32553;&#19981;&#22815;&#20248;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#22823;&#22411;&#27169;&#22411;&#26368;&#36817;&#39640;&#25928;&#21098;&#26525;&#26041;&#27861;&#30340;&#36825;&#19968;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;&#31895;&#21040;&#32454;&#30340;&#36880;&#23618;&#21098;&#26525;&#26041;&#27861;&#65288;ECoFLaP&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Vision-Language Models (LVLMs) can understand the world comprehensively by integrating rich information from different modalities, achieving remarkable performance improvements on various multimodal downstream tasks. However, deploying LVLMs is often problematic due to their massive computational/energy costs and carbon consumption. Such issues make it infeasible to adopt conventional iterative global pruning, which is costly due to computing the Hessian matrix of the entire large model for sparsification. Alternatively, several studies have recently proposed layer-wise pruning approaches to avoid the expensive computation of global pruning and efficiently compress model weights according to their importance within a layer. However, these methods often suffer from suboptimal model compression due to their lack of a global perspective. To address this limitation in recent efficient pruning methods for large models, we propose Efficient Coarse-to-Fine Layer-Wise Pruning (ECoFLaP), 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Kosmos-G&#65292;&#19968;&#31181;&#21033;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#22312;&#19978;&#19979;&#25991;&#20013;&#29983;&#25104;&#22270;&#20687;&#30340;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#27169;&#24577;&#20316;&#20026;&#38170;&#28857;&#65292;&#23558;MLLM&#30340;&#36755;&#20986;&#31354;&#38388;&#19982;CLIP&#23545;&#40784;&#65292;&#24182;&#36827;&#34892;&#32452;&#21512;&#25351;&#20196;&#35843;&#25972;&#12290;Kosmos-G&#23637;&#31034;&#20102;&#38646;&#26679;&#26412;&#22810;&#23454;&#20307;&#20027;&#39064;&#39537;&#21160;&#29983;&#25104;&#30340;&#29420;&#29305;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.02992</link><description>&lt;p&gt;
Kosmos-G&#65306;&#20351;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#20013;&#29983;&#25104;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
Kosmos-G: Generating Images in Context with Multimodal Large Language Models. (arXiv:2310.02992v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Kosmos-G&#65292;&#19968;&#31181;&#21033;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#22312;&#19978;&#19979;&#25991;&#20013;&#29983;&#25104;&#22270;&#20687;&#30340;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#27169;&#24577;&#20316;&#20026;&#38170;&#28857;&#65292;&#23558;MLLM&#30340;&#36755;&#20986;&#31354;&#38388;&#19982;CLIP&#23545;&#40784;&#65292;&#24182;&#36827;&#34892;&#32452;&#21512;&#25351;&#20196;&#35843;&#25972;&#12290;Kosmos-G&#23637;&#31034;&#20102;&#38646;&#26679;&#26412;&#22810;&#23454;&#20307;&#20027;&#39064;&#39537;&#21160;&#29983;&#25104;&#30340;&#29420;&#29305;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#21644;&#35270;&#35273;&#35821;&#35328;&#21040;&#22270;&#20687;&#65288;VL2I&#65289;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#20174;&#36890;&#29992;&#30340;&#35270;&#35273;&#35821;&#35328;&#36755;&#20837;&#29983;&#25104;&#22270;&#20687;&#65292;&#29305;&#21035;&#26159;&#28041;&#21450;&#22810;&#20010;&#22270;&#20687;&#30340;&#24773;&#20917;&#65292;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Kosmos-G&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#20808;&#36827;&#24863;&#30693;&#33021;&#21147;&#26469;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#27169;&#24577;&#20316;&#20026;&#38170;&#28857;&#65292;&#23558;MLLM&#30340;&#36755;&#20986;&#31354;&#38388;&#19982;CLIP&#23545;&#40784;&#65292;&#24182;&#22312;&#31574;&#21010;&#25968;&#25454;&#19978;&#36827;&#34892;&#32452;&#21512;&#25351;&#20196;&#35843;&#25972;&#12290;Kosmos-G&#23637;&#31034;&#20102;&#38646;&#26679;&#26412;&#22810;&#23454;&#20307;&#20027;&#39064;&#39537;&#21160;&#29983;&#25104;&#30340;&#29420;&#29305;&#33021;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20998;&#25968;&#33976;&#39311;&#25351;&#20196;&#35843;&#25972;&#23545;&#22270;&#20687;&#35299;&#30721;&#22120;&#19981;&#38656;&#35201;&#36827;&#34892;&#20219;&#20309;&#20462;&#25913;&#12290;&#36825;&#20801;&#35768;&#26080;&#32541;&#26367;&#20195;CLIP&#24182;&#36731;&#26494;&#38598;&#25104;&#21508;&#31181;U-Net&#25216;&#26415;&#65292;&#21253;&#25324;&#32454;&#31890;&#24230;&#25511;&#21046;&#21644;&#20010;&#24615;&#21270;&#22270;&#20687;&#35299;&#30721;&#22120;&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in text-to-image (T2I) and vision-language-to-image (VL2I) generation have made significant strides. However, the generation from generalized vision-language inputs, especially involving multiple images, remains under-explored. This paper presents Kosmos-G, a model that leverages the advanced perception capabilities of Multimodal Large Language Models (MLLMs) to tackle the aforementioned challenge. Our approach aligns the output space of MLLM with CLIP using the textual modality as an anchor and performs compositional instruction tuning on curated data. Kosmos-G demonstrates a unique capability of zero-shot multi-entity subject-driven generation. Notably, the score distillation instruction tuning requires no modifications to the image decoder. This allows for a seamless substitution of CLIP and effortless integration with a myriad of U-Net techniques ranging from fine-grained controls to personalized image decoder variants. We posit Kosmos-G as an initial attempt to
&lt;/p&gt;</description></item><item><title>xVal&#26159;&#19968;&#31181;&#36830;&#32493;&#25968;&#23383;&#32534;&#30721;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#21333;&#20010;&#26631;&#35760;&#26469;&#34920;&#31034;&#20219;&#20309;&#23454;&#25968;&#12290;&#19982;&#29616;&#26377;&#30340;&#25968;&#23383;&#32534;&#30721;&#26041;&#26696;&#30456;&#27604;&#65292;xVal&#26356;&#21152;&#39640;&#25928;&#65292;&#24182;&#19988;&#22312;&#27867;&#21270;&#24615;&#33021;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.02989</link><description>&lt;p&gt;
xVal: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36830;&#32493;&#25968;&#23383;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
xVal: A Continuous Number Encoding for Large Language Models. (arXiv:2310.02989v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02989
&lt;/p&gt;
&lt;p&gt;
xVal&#26159;&#19968;&#31181;&#36830;&#32493;&#25968;&#23383;&#32534;&#30721;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#21333;&#20010;&#26631;&#35760;&#26469;&#34920;&#31034;&#20219;&#20309;&#23454;&#25968;&#12290;&#19982;&#29616;&#26377;&#30340;&#25968;&#23383;&#32534;&#30721;&#26041;&#26696;&#30456;&#27604;&#65292;xVal&#26356;&#21152;&#39640;&#25928;&#65292;&#24182;&#19988;&#22312;&#27867;&#21270;&#24615;&#33021;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25968;&#23383;&#20196;&#29260;&#21270;&#30340;&#29420;&#29305;&#22256;&#38590;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23578;&#26410;&#24191;&#27867;&#29992;&#20110;&#31185;&#23398;&#25968;&#25454;&#38598;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;xVal&#65292;&#19968;&#31181;&#25968;&#23383;&#32534;&#30721;&#26041;&#26696;&#65292;&#21487;&#20197;&#20351;&#29992;&#21333;&#20010;&#26631;&#35760;&#26469;&#34920;&#31034;&#20219;&#20309;&#23454;&#25968;&#12290;xVal&#36890;&#36807;&#23558;&#19987;&#29992;&#23884;&#20837;&#21521;&#37327;&#25353;&#25968;&#23383;&#20540;&#36827;&#34892;&#32553;&#25918;&#26469;&#34920;&#31034;&#32473;&#23450;&#30340;&#23454;&#25968;&#12290;&#32467;&#21512;&#20462;&#25913;&#21518;&#30340;&#25968;&#23383;&#25512;&#26029;&#26041;&#27861;&#65292;&#35813;&#31574;&#30053;&#20351;&#27169;&#22411;&#22312;&#32771;&#34385;&#20316;&#20026;&#20174;&#36755;&#20837;&#23383;&#31526;&#20018;&#30340;&#25968;&#23383;&#21040;&#36755;&#20986;&#23383;&#31526;&#20018;&#30340;&#25968;&#23383;&#30340;&#26144;&#23556;&#26102;&#25104;&#20026;&#31471;&#21040;&#31471;&#36830;&#32493;&#30340;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#31181;&#26356;&#36866;&#29992;&#20110;&#31185;&#23398;&#39046;&#22495;&#24212;&#29992;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#25105;&#20204;&#22312;&#35768;&#22810;&#21512;&#25104;&#21644;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;&#19982;&#29616;&#26377;&#30340;&#25968;&#23383;&#32534;&#30721;&#26041;&#26696;&#30456;&#27604;&#65292;&#25105;&#20204;&#21457;&#29616;xVal&#22312;&#20196;&#29260;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models have not yet been broadly adapted for the analysis of scientific datasets due in part to the unique difficulties of tokenizing numbers. We propose xVal, a numerical encoding scheme that represents any real number using just a single token. xVal represents a given real number by scaling a dedicated embedding vector by the number value. Combined with a modified number-inference approach, this strategy renders the model end-to-end continuous when considered as a map from the numbers of the input string to those of the output string. This leads to an inductive bias that is generally more suitable for applications in scientific domains. We empirically evaluate our proposal on a number of synthetic and real-world datasets. Compared with existing number encoding schemes, we find that xVal is more token-efficient and demonstrates improved generalization.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24212;&#29992;&#20110;&#32852;&#24819;&#35760;&#24518;&#20013;&#30340;&#32553;&#25918;&#23450;&#24459;&#65292;&#36890;&#36807;&#39640;&#32500;&#30697;&#38453;&#21644;&#23884;&#20837;&#30340;&#22806;&#31215;&#26469;&#27169;&#25311;&#20869;&#23618;Transformer&#35821;&#35328;&#27169;&#22411;&#12290;&#20316;&#32773;&#25512;&#23548;&#20986;&#20102;&#19982;&#26679;&#26412;&#25968;&#37327;&#21644;&#21442;&#25968;&#22823;&#23567;&#30456;&#20851;&#30340;&#31934;&#30830;&#32553;&#25918;&#23450;&#24459;&#65292;&#24182;&#39564;&#35777;&#20102;&#29702;&#35770;&#32467;&#26524;&#30340;&#26377;&#25928;&#24615;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#23637;&#31034;&#20102;&#23384;&#20648;&#35760;&#24518;&#20851;&#32852;&#30340;&#32454;&#31890;&#24230;&#21487;&#35270;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.02984</link><description>&lt;p&gt;
&#32553;&#25918;&#23450;&#24459;&#22312;&#32852;&#24819;&#35760;&#24518;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Scaling Laws for Associative Memories. (arXiv:2310.02984v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02984
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24212;&#29992;&#20110;&#32852;&#24819;&#35760;&#24518;&#20013;&#30340;&#32553;&#25918;&#23450;&#24459;&#65292;&#36890;&#36807;&#39640;&#32500;&#30697;&#38453;&#21644;&#23884;&#20837;&#30340;&#22806;&#31215;&#26469;&#27169;&#25311;&#20869;&#23618;Transformer&#35821;&#35328;&#27169;&#22411;&#12290;&#20316;&#32773;&#25512;&#23548;&#20986;&#20102;&#19982;&#26679;&#26412;&#25968;&#37327;&#21644;&#21442;&#25968;&#22823;&#23567;&#30456;&#20851;&#30340;&#31934;&#30830;&#32553;&#25918;&#23450;&#24459;&#65292;&#24182;&#39564;&#35777;&#20102;&#29702;&#35770;&#32467;&#26524;&#30340;&#26377;&#25928;&#24615;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#23637;&#31034;&#20102;&#23384;&#20648;&#35760;&#24518;&#20851;&#32852;&#30340;&#32454;&#31890;&#24230;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#24456;&#21487;&#33021;&#28041;&#21450;&#21040;&#25277;&#35937;&#35268;&#21017;&#30340;&#21457;&#29616;&#21644;&#35760;&#24518;&#12290;&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#32852;&#24819;&#35760;&#24518;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22522;&#20110;&#39640;&#32500;&#30697;&#38453;&#65292;&#30001;&#23884;&#20837;&#30340;&#22806;&#31215;&#32452;&#25104;&#65292;&#19982;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#23618;&#30456;&#20851;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#20851;&#20110;&#26679;&#26412;&#25968;&#37327;&#21644;&#21442;&#25968;&#35268;&#27169;&#30340;&#31934;&#30830;&#32553;&#25918;&#23450;&#24459;&#65292;&#24182;&#35752;&#35770;&#20102;&#19981;&#21516;&#20272;&#35745;&#22120;&#30340;&#32479;&#35745;&#25928;&#29575;&#65292;&#21253;&#25324;&#22522;&#20110;&#20248;&#21270;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#20197;&#39564;&#35777;&#21644;&#35299;&#37322;&#29702;&#35770;&#32467;&#26524;&#65292;&#21253;&#25324;&#23545;&#23384;&#20648;&#35760;&#24518;&#20851;&#32852;&#30340;&#32454;&#31890;&#24230;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning arguably involves the discovery and memorization of abstract rules. The aim of this paper is to study associative memory mechanisms. Our model is based on high-dimensional matrices consisting of outer products of embeddings, which relates to the inner layers of transformer language models. We derive precise scaling laws with respect to sample size and parameter size, and discuss the statistical efficiency of different estimators, including optimization-based algorithms. We provide extensive numerical experiments to validate and interpret theoretical results, including fine-grained visualizations of the stored memory associations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#20351;&#29992;&#38543;&#26426;&#21021;&#22987;&#21270;&#20250;&#23548;&#33268;&#23545;&#26550;&#26500;&#24046;&#24322;&#30340;&#20005;&#37325;&#39640;&#20272;&#65292;&#32780;&#20351;&#29992;&#26631;&#20934;&#28040;&#22122;&#30446;&#26631;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#22312;&#22810;&#31181;&#26550;&#26500;&#19978;&#23454;&#29616;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#23558;Transformers&#19982;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#32553;&#23567;&#21040;&#24456;&#23567;&#12290;&#19982;&#20043;&#21069;&#30340;&#30740;&#31350;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#27491;&#30830;&#39044;&#35757;&#32451;&#26102;&#65292;&#26222;&#36890;&#30340;Transformers&#22312;Long Range Arena&#19978;&#30340;&#24615;&#33021;&#19982;S4&#30456;&#21305;&#37197;&#65292;&#24182;&#19988;&#22312;PathX-256&#20219;&#21153;&#19978;&#25913;&#36827;&#20102;SSMs&#30340;&#26368;&#20339;&#32467;&#26524;20&#20010;&#30334;&#20998;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.02980</link><description>&lt;p&gt;
&#27704;&#36828;&#19981;&#35201;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#65306;&#20844;&#27491;&#27604;&#36739;&#38271;&#24207;&#21015;&#27169;&#22411;&#38656;&#35201;&#25968;&#25454;&#39537;&#21160;&#30340;&#20808;&#39564;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Never Train from Scratch: Fair Comparison of Long-Sequence Models Requires Data-Driven Priors. (arXiv:2310.02980v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02980
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#20351;&#29992;&#38543;&#26426;&#21021;&#22987;&#21270;&#20250;&#23548;&#33268;&#23545;&#26550;&#26500;&#24046;&#24322;&#30340;&#20005;&#37325;&#39640;&#20272;&#65292;&#32780;&#20351;&#29992;&#26631;&#20934;&#28040;&#22122;&#30446;&#26631;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#22312;&#22810;&#31181;&#26550;&#26500;&#19978;&#23454;&#29616;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#23558;Transformers&#19982;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#32553;&#23567;&#21040;&#24456;&#23567;&#12290;&#19982;&#20043;&#21069;&#30340;&#30740;&#31350;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#27491;&#30830;&#39044;&#35757;&#32451;&#26102;&#65292;&#26222;&#36890;&#30340;Transformers&#22312;Long Range Arena&#19978;&#30340;&#24615;&#33021;&#19982;S4&#30456;&#21305;&#37197;&#65292;&#24182;&#19988;&#22312;PathX-256&#20219;&#21153;&#19978;&#25913;&#36827;&#20102;SSMs&#30340;&#26368;&#20339;&#32467;&#26524;20&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#27169;&#24207;&#21015;&#20043;&#38388;&#30340;&#38271;&#31243;&#20381;&#36182;&#19968;&#30452;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#30446;&#26631;&#65292;&#24182;&#23548;&#33268;&#20102;&#19968;&#20123;&#26550;&#26500;&#65292;&#22914;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#22312;&#22788;&#29702;&#38271;&#24207;&#21015;&#26102;&#27604;Transformers&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32463;&#39564;&#24615;&#36827;&#23637;&#20027;&#35201;&#26159;&#22312;&#38543;&#26426;&#21021;&#22987;&#21270;&#24182;&#36890;&#36807;&#39044;&#27979;&#36755;&#20837;&#24207;&#21015;&#30340;&#30446;&#26631;&#26631;&#31614;&#36827;&#34892;&#35757;&#32451;&#30340;&#22522;&#20934;&#27979;&#35797;&#65288;&#20363;&#22914;Long Range Arena&#65289;&#19978;&#23637;&#31034;&#20986;&#26469;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#38543;&#26426;&#21021;&#22987;&#21270;&#23548;&#33268;&#23545;&#26550;&#26500;&#20043;&#38388;&#24046;&#24322;&#30340;&#20005;&#37325;&#39640;&#20272;&#65292;&#24182;&#19988;&#20351;&#29992;&#26631;&#20934;&#28040;&#22122;&#30446;&#26631;&#36827;&#34892;&#39044;&#35757;&#32451;&#65288;&#20165;&#20351;&#29992;&#19979;&#28216;&#20219;&#21153;&#25968;&#25454;&#65289;&#21487;&#20197;&#22312;&#22810;&#31181;&#26550;&#26500;&#19978;&#23454;&#29616;&#26174;&#33879;&#30340;&#25910;&#30410;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;Transformers&#21644;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#20043;&#38388;&#24471;&#21040;&#24456;&#23567;&#30340;&#24046;&#36317;&#12290;&#19982;&#20043;&#21069;&#30340;&#30740;&#31350;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#27491;&#30830;&#39044;&#35757;&#32451;&#26102;&#65292;&#26222;&#36890;&#30340;Transformers&#22312;Long Range Arena&#19978;&#19982;S4&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#65292;&#24182;&#19988;&#25105;&#20204;&#22312;PathX-256&#20219;&#21153;&#19978;&#23558;SSMs&#30340;&#26368;&#20339;&#25253;&#21578;&#32467;&#26524;&#25552;&#39640;&#20102;20&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling long-range dependencies across sequences is a longstanding goal in machine learning and has led to architectures, such as state space models, that dramatically outperform Transformers on long sequences. However, these impressive empirical gains have been by and large demonstrated on benchmarks (e.g. Long Range Arena), where models are randomly initialized and trained to predict a target label from an input sequence. In this work, we show that random initialization leads to gross overestimation of the differences between architectures and that pretraining with standard denoising objectives, using $\textit{only the downstream task data}$, leads to dramatic gains across multiple architectures and to very small gaps between Transformers and state space models (SSMs). In stark contrast to prior works, we find vanilla Transformers to match the performance of S4 on Long Range Arena when properly pretrained, and we improve the best reported results of SSMs on the PathX-256 task by 20 
&lt;/p&gt;</description></item><item><title>T$^3$Bench&#26159;&#31532;&#19968;&#20010;&#32508;&#21512;&#30340;&#25991;&#26412;&#21040;3D&#22522;&#20934;&#27979;&#35797;&#65292;&#23427;&#21253;&#21547;&#20102;&#22810;&#20010;&#22797;&#26434;&#31243;&#24230;&#30340;&#25991;&#26412;&#25552;&#31034;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#26469;&#35780;&#20272;&#29983;&#25104;&#30340;3D&#22330;&#26223;&#30340;&#20027;&#35266;&#36136;&#37327;&#21644;&#25991;&#26412;&#23545;&#40784;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.02977</link><description>&lt;p&gt;
T$^3$Bench&#65306;&#26631;&#27880;&#30446;&#21069;&#22312;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#39046;&#22495;&#30340;&#36827;&#23637;&#30340;&#22522;&#20934;&#27979;&#35797;&#65288;arXiv:2310.02977v1 [cs.CV]&#65289;
&lt;/p&gt;
&lt;p&gt;
T$^3$Bench: Benchmarking Current Progress in Text-to-3D Generation. (arXiv:2310.02977v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02977
&lt;/p&gt;
&lt;p&gt;
T$^3$Bench&#26159;&#31532;&#19968;&#20010;&#32508;&#21512;&#30340;&#25991;&#26412;&#21040;3D&#22522;&#20934;&#27979;&#35797;&#65292;&#23427;&#21253;&#21547;&#20102;&#22810;&#20010;&#22797;&#26434;&#31243;&#24230;&#30340;&#25991;&#26412;&#25552;&#31034;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#26469;&#35780;&#20272;&#29983;&#25104;&#30340;3D&#22330;&#26223;&#30340;&#20027;&#35266;&#36136;&#37327;&#21644;&#25991;&#26412;&#23545;&#40784;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#25991;&#26412;&#21040;3D&#26041;&#27861;&#21033;&#29992;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#26469;&#20248;&#21270;NeRF&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#22312;&#27809;&#26377;3D&#25968;&#25454;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;3D&#22330;&#26223;&#12290;&#30001;&#20110;&#20219;&#21153;&#30340;&#24320;&#25918;&#24615;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#36890;&#36807;&#20027;&#35266;&#26696;&#20363;&#30740;&#31350;&#21644;&#29992;&#25143;&#23454;&#39564;&#35777;&#26126;&#20854;&#32467;&#26524;&#65292;&#20174;&#32780;&#22312;&#23450;&#37327;&#19978;&#22238;&#31572;&#8220;&#25991;&#26412;&#21040;3D&#30340;&#24403;&#21069;&#36827;&#23637;&#22914;&#20309;&#65311;&#8221;&#36825;&#20010;&#38382;&#39064;&#19978;&#23384;&#22312;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;T$^3$Bench&#65292;&#36825;&#26159;&#19968;&#20010;&#31532;&#19968;&#20010;&#32508;&#21512;&#30340;&#25991;&#26412;&#21040;3D&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#21547;&#19977;&#20010;&#19981;&#26029;&#22686;&#21152;&#22797;&#26434;&#24615;&#30340;&#25991;&#26412;&#25552;&#31034;&#65292;&#19987;&#38376;&#20026;3D&#29983;&#25104;&#32780;&#35774;&#35745;&#12290;&#20026;&#20102;&#35780;&#20272;&#20027;&#35266;&#36136;&#37327;&#21644;&#25991;&#26412;&#23545;&#40784;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;3D&#20869;&#23481;&#20135;&#29983;&#30340;&#22810;&#35270;&#22270;&#22270;&#20687;&#30340;&#20004;&#20010;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#12290;&#36136;&#37327;&#24230;&#37327;&#32467;&#21512;&#20102;&#22810;&#35270;&#22270;&#25991;&#26412;-&#22270;&#20687;&#20998;&#25968;&#21644;&#21306;&#22495;&#21367;&#31215;&#20197;&#26816;&#27979;&#36136;&#37327;&#21644;&#35270;&#35282;&#19981;&#19968;&#33268;&#24615;&#12290;&#23545;&#40784;&#24230;&#37327;&#20351;&#29992;&#22810;&#35270;&#22270;&#23383;&#24149;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;e
&lt;/p&gt;
&lt;p&gt;
Recent methods in text-to-3D leverage powerful pretrained diffusion models to optimize NeRF. Notably, these methods are able to produce high-quality 3D scenes without training on 3D data. Due to the open-ended nature of the task, most studies evaluate their results with subjective case studies and user experiments, thereby presenting a challenge in quantitatively addressing the question: How has current progress in Text-to-3D gone so far? In this paper, we introduce T$^3$Bench, the first comprehensive text-to-3D benchmark containing diverse text prompts of three increasing complexity levels that are specially designed for 3D generation. To assess both the subjective quality and the text alignment, we propose two automatic metrics based on multi-view images produced by the 3D contents. The quality metric combines multi-view text-image scores and regional convolution to detect quality and view inconsistency. The alignment metric uses multi-view captioning and Large Language Model (LLM) e
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#19968;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#27169;&#22411;"UniverSLU"&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#21644;&#19981;&#21516;&#30340;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#25351;&#23450;&#22120;&#20316;&#20026;&#31163;&#25955;&#25552;&#31034;&#65292;&#25104;&#21151;&#22320;&#22312;&#22810;&#20010;&#21475;&#35821;&#29702;&#35299;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#29978;&#33267;&#36229;&#36807;&#20102;&#29305;&#23450;&#20219;&#21153;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.02973</link><description>&lt;p&gt;
UniverSLU:&#21333;&#20010;&#32593;&#32476;&#29992;&#20110;&#22810;&#26679;&#20998;&#31867;&#21644;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#30340;&#36890;&#29992;&#21475;&#35821;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
UniverSLU: Universal Spoken Language Understanding for Diverse Classification and Sequence Generation Tasks with a Single Network. (arXiv:2310.02973v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02973
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#19968;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#27169;&#22411;"UniverSLU"&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#21644;&#19981;&#21516;&#30340;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#25351;&#23450;&#22120;&#20316;&#20026;&#31163;&#25955;&#25552;&#31034;&#65292;&#25104;&#21151;&#22320;&#22312;&#22810;&#20010;&#21475;&#35821;&#29702;&#35299;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#29978;&#33267;&#36229;&#36807;&#20102;&#29305;&#23450;&#20219;&#21153;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#37319;&#29992;&#20855;&#22791;&#22810;&#20219;&#21153;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#21462;&#24471;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;&#23427;&#20204;&#21033;&#29992;&#25552;&#31034;&#26469;&#24341;&#23548;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#24182;&#19988;&#36229;&#36234;&#20102;&#29305;&#23450;&#20219;&#21153;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#38382;&#65306;&#25105;&#20204;&#33021;&#21542;&#26500;&#24314;&#19968;&#20010;&#21333;&#19968;&#27169;&#22411;&#26469;&#20849;&#21516;&#25191;&#34892;&#21508;&#31181;&#21475;&#35821;&#29702;&#35299;&#20219;&#21153;&#65311;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#65292;&#24182;&#37319;&#29992;&#19981;&#21516;&#30340;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#25351;&#23450;&#22120;&#20316;&#20026;&#31163;&#25955;&#25552;&#31034;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#21333;&#19968;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#27169;&#22411;"UniverSLU"&#22312;12&#20010;&#19981;&#21516;&#30340;&#35821;&#38899;&#20998;&#31867;&#21644;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#28085;&#30422;&#20102;17&#20010;&#25968;&#25454;&#38598;&#21644;9&#31181;&#35821;&#35328;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;UniverSLU&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#29978;&#33267;&#36229;&#36807;&#20102;&#29305;&#23450;&#20219;&#21153;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#21021;&#27493;&#30740;&#31350;&#65292;&#25506;&#32034;&#20102;&#20351;&#29992;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#33258;&#28982;&#30701;&#35821;&#20195;&#26367;&#20219;&#21153;&#25351;&#23450;&#22120;&#20316;&#20026;&#31163;&#25955;&#25552;&#31034;&#65292;&#24182;&#27979;&#35797;&#20102;&#27169;&#22411;&#23545;&#26032;&#37322;&#20041;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have demonstrated promising outcomes by employing large language models with multi-tasking capabilities. They utilize prompts to guide the model's behavior and surpass performance of task-specific models. Motivated by this, we ask: can we build a single model that jointly perform various spoken language understanding (SLU) tasks? To address this, we utilize pre-trained automatic speech recognition (ASR) models and employ various task and dataset specifiers as discrete prompts. We demonstrate efficacy of our single multi-task learning (MTL) model "UniverSLU" for 12 different speech classification and sequence generation tasks across 17 datasets and 9 languages. Results show that UniverSLU achieves competitive performance and even surpasses task-specific models. We also conduct preliminary investigations into enabling human-interpretable natural phrases instead of task specifiers as discrete prompts and test the model's generalization capabilities to new paraphrases.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#20026;&#33258;&#30417;&#30563;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#35821;&#38899;&#27169;&#22411;&#36827;&#34892;&#25552;&#31034;&#21644;&#36866;&#37197;&#22120;&#35843;&#20248;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#24207;&#21015;&#29983;&#25104;&#21644;&#36328;&#35821;&#35328;ASR&#20219;&#21153;&#19978;&#30340;&#20248;&#36234;&#34920;&#29616;&#65292;&#23588;&#20854;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#25552;&#31034;&#26041;&#27861;&#20248;&#20110;&#36866;&#37197;&#22120;&#35843;&#20248;&#12290;</title><link>http://arxiv.org/abs/2310.02971</link><description>&lt;p&gt;
&#20026;&#33258;&#30417;&#30563;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#35821;&#38899;&#27169;&#22411;&#36827;&#34892;&#25552;&#31034;&#21644;&#36866;&#37197;&#22120;&#35843;&#20248;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Prompting and Adapter Tuning for Self-supervised Encoder-Decoder Speech Model. (arXiv:2310.02971v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02971
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#20026;&#33258;&#30417;&#30563;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#35821;&#38899;&#27169;&#22411;&#36827;&#34892;&#25552;&#31034;&#21644;&#36866;&#37197;&#22120;&#35843;&#20248;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#24207;&#21015;&#29983;&#25104;&#21644;&#36328;&#35821;&#35328;ASR&#20219;&#21153;&#19978;&#30340;&#20248;&#36234;&#34920;&#29616;&#65292;&#23588;&#20854;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#25552;&#31034;&#26041;&#27861;&#20248;&#20110;&#36866;&#37197;&#22120;&#35843;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#21644;&#36866;&#37197;&#22120;&#35843;&#20248;&#24050;&#32463;&#25104;&#20026;&#32454;&#35843;&#65288;FT&#65289;&#26041;&#27861;&#30340;&#26377;&#25928;&#26367;&#20195;&#21697;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20851;&#20110;&#35821;&#38899;&#25552;&#31034;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#24182;&#22312;&#26356;&#22797;&#26434;&#30340;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#19978;&#22833;&#36133;&#12290;&#27492;&#22806;&#65292;&#36866;&#37197;&#22120;&#35843;&#20248;&#20027;&#35201;&#24212;&#29992;&#20110;&#20165;&#32534;&#30721;&#22120;&#30340;&#33258;&#30417;&#30563;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;Wav2Seq&#36825;&#20010;&#33258;&#30417;&#30563;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#19978;&#36827;&#34892;&#25552;&#31034;&#65292;&#36229;&#36807;&#20102;&#20197;&#21069;&#22312;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#30740;&#31350;&#25104;&#26524;&#12290;&#23427;&#22312;ASR&#30340;&#35789;&#38169;&#35823;&#29575;&#19978;&#23454;&#29616;&#20102;53&#65285;&#30340;&#30456;&#23545;&#25913;&#36827;&#65292;&#22312;&#27133;&#22635;&#20805;&#30340;F1&#20998;&#25968;&#19978;&#23454;&#29616;&#20102;27&#65285;&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#25552;&#31034;&#26041;&#27861;&#19982;FT&#26041;&#27861;&#30456;&#31454;&#20105;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;Wav2Seq&#19978;&#36890;&#36807;&#25552;&#31034;&#21644;&#36866;&#37197;&#22120;&#35843;&#20248;&#23454;&#29616;&#30340;&#36328;&#35821;&#35328;ASR&#30340;&#21487;&#36716;&#31227;&#24615;&#12290;&#24403;&#28041;&#21450;&#26377;&#38480;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#26102;&#65292;&#25552;&#31034;&#21644;&#36866;&#37197;&#22120;&#35843;&#20248;&#22987;&#32456;&#20248;&#20110;&#20256;&#32479;&#30340;FT&#26041;&#27861;&#22312;7&#31181;&#35821;&#35328;&#20013;&#30340;&#34920;&#29616;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#65292;&#25552;&#31034;&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;&#36866;&#37197;&#22120;&#35843;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompting and adapter tuning have emerged as efficient alternatives to fine-tuning (FT) methods. However, existing studies on speech prompting focused on classification tasks and failed on more complex sequence generation tasks. Besides, adapter tuning is primarily applied with a focus on encoder-only self-supervised models. Our experiments show that prompting on Wav2Seq, a self-supervised encoder-decoder model, surpasses previous works in sequence generation tasks. It achieves a remarkable 53% relative improvement in word error rate for ASR and a 27% in F1 score for slot filling. Additionally, prompting competes with the FT method in the low-resource scenario. Moreover, we show the transferability of prompting and adapter tuning on Wav2Seq in cross-lingual ASR. When limited trainable parameters are involved, prompting and adapter tuning consistently outperform conventional FT across 7 languages. Notably, in the low-resource scenario, prompting consistently outperforms adapter tuning.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;DQ-LoRe&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#21452;&#37325;&#26597;&#35810;&#21644;&#20302;&#31209;&#36817;&#20284;&#37325;&#26032;&#25490;&#24207;&#33258;&#21160;&#36873;&#25321;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#31034;&#20363;&#65292;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#21644;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.02954</link><description>&lt;p&gt;
DQ-LoRe: &#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#20302;&#31209;&#36817;&#20284;&#21452;&#37325;&#26597;&#35810;&#37325;&#26032;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
DQ-LoRe: Dual Queries with Low Rank Approximation Re-ranking for In-Context Learning. (arXiv:2310.02954v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02954
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;DQ-LoRe&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#21452;&#37325;&#26597;&#35810;&#21644;&#20302;&#31209;&#36817;&#20284;&#37325;&#26032;&#25490;&#24207;&#33258;&#21160;&#36873;&#25321;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#31034;&#20363;&#65292;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#26032;&#36827;&#23637;&#65292;&#20027;&#35201;&#26159;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25512;&#21160;&#30340;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#24341;&#23548;LLMs&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#26159;&#21033;&#29992;&#38142;&#24335;&#24605;&#32500;&#65288;CoT&#65289;&#33539;&#24335;&#20013;&#30340;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#26368;&#20027;&#35201;&#30340;&#25361;&#25112;&#22312;&#20110;&#26377;&#25928;&#22320;&#36873;&#25321;&#31034;&#20363;&#26469;&#20419;&#36827;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#21452;&#37325;&#26597;&#35810;&#21644;&#20302;&#31209;&#36817;&#20284;&#37325;&#26032;&#25490;&#24207;&#65288;DQ-LoRe&#65289;&#26469;&#33258;&#21160;&#36873;&#25321;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#31034;&#20363;&#12290;&#21452;&#37325;&#26597;&#35810;&#39318;&#20808;&#26597;&#35810;LLM&#20197;&#33719;&#21462;LLM&#29983;&#25104;&#30340;&#30693;&#35782;&#65292;&#20363;&#22914;CoT&#65292;&#28982;&#21518;&#36890;&#36807;&#38382;&#39064;&#21644;&#30693;&#35782;&#26597;&#35810;&#26816;&#32034;&#22120;&#20197;&#33719;&#21462;&#26368;&#32456;&#30340;&#31034;&#20363;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#31532;&#20108;&#20010;&#26597;&#35810;&#65292;LoRe&#21033;&#29992;&#38477;&#32500;&#25216;&#26415;&#26469;&#25913;&#36827;&#31034;&#20363;&#36873;&#25321;&#65292;&#30830;&#20445;&#19982;&#36755;&#20837;&#38382;&#39064;&#30340;&#30693;&#35782;&#23494;&#20999;&#23545;&#40784;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;DQ-LoRe&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in natural language processing, primarily propelled by Large Language Models (LLMs), have showcased their remarkable capabilities grounded in in-context learning. A promising avenue for guiding LLMs in intricate reasoning tasks involves the utilization of intermediate reasoning steps within the Chain-of-Thought (CoT) paradigm. Nevertheless, the central challenge lies in the effective selection of exemplars for facilitating in-context learning. In this study, we introduce a framework that leverages Dual Queries and Low-rank approximation Re-ranking (DQ-LoRe) to automatically select exemplars for in-context learning. Dual Queries first query LLM to obtain LLM-generated knowledge such as CoT, then query the retriever to obtain the final exemplars via both question and the knowledge. Moreover, for the second query, LoRe employs dimensionality reduction techniques to refine exemplar selection, ensuring close alignment with the input question's knowledge. Through extensive ex
&lt;/p&gt;</description></item><item><title>JsonTuning&#26159;&#19968;&#31181;&#38754;&#21521;&#36890;&#29992;&#12289;&#24378;&#22823;&#21644;&#21487;&#25511;&#30340;&#25351;&#20196;&#35843;&#20248;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;JSON&#30340;&#32467;&#26500;&#21270;&#29305;&#24615;&#65292;&#24110;&#21161;&#27169;&#22411;&#29702;&#35299;&#20219;&#21153;&#35201;&#32032;&#21450;&#20854;&#20851;&#31995;&#65292;&#20174;&#32780;&#25193;&#23637;&#20102;&#36890;&#29992;&#24615;&#12289;&#25552;&#39640;&#20102;&#31283;&#20581;&#24615;&#65292;&#24182;&#22686;&#24378;&#20102;&#23545;&#36755;&#20986;&#30340;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.02953</link><description>&lt;p&gt;
JsonTuning&#65306;&#38754;&#21521;&#36890;&#29992;&#12289;&#24378;&#22823;&#21644;&#21487;&#25511;&#30340;&#25351;&#20196;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
JsonTuning: Towards Generalizable, Robust, and Controllable Instruction Tuning. (arXiv:2310.02953v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02953
&lt;/p&gt;
&lt;p&gt;
JsonTuning&#26159;&#19968;&#31181;&#38754;&#21521;&#36890;&#29992;&#12289;&#24378;&#22823;&#21644;&#21487;&#25511;&#30340;&#25351;&#20196;&#35843;&#20248;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;JSON&#30340;&#32467;&#26500;&#21270;&#29305;&#24615;&#65292;&#24110;&#21161;&#27169;&#22411;&#29702;&#35299;&#20219;&#21153;&#35201;&#32032;&#21450;&#20854;&#20851;&#31995;&#65292;&#20174;&#32780;&#25193;&#23637;&#20102;&#36890;&#29992;&#24615;&#12289;&#25552;&#39640;&#20102;&#31283;&#20581;&#24615;&#65292;&#24182;&#22686;&#24378;&#20102;&#23545;&#36755;&#20986;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#20248;&#24050;&#25104;&#20026;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33021;&#21147;&#30340;&#20851;&#38190;&#36807;&#31243;&#65292;&#36890;&#36807;&#25552;&#20379;&#26126;&#30830;&#30340;&#20219;&#21153;&#25351;&#20196;&#65292;&#20174;&#32780;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#25991;&#26412;-&#25991;&#26412;&#25351;&#20196;&#35843;&#20248;&#65288;TextTuning&#65289;&#26041;&#27861;&#30001;&#20110;&#20219;&#21153;&#30340;&#27169;&#31946;&#24615;&#21644;&#32570;&#20047;&#26126;&#30830;&#30340;&#32467;&#26500;&#32780;&#23384;&#22312;&#36890;&#29992;&#24615;&#12289;&#31283;&#20581;&#24615;&#21644;&#21487;&#25511;&#24615;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;JsonTuning&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#32467;&#26500;&#21040;&#32467;&#26500;&#30340;&#25351;&#20196;&#35843;&#20248;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;JSON&#30340;&#22810;&#21151;&#33021;&#21644;&#32467;&#26500;&#21270;&#29305;&#24615;&#26469;&#34920;&#31034;&#20219;&#21153;&#65292;JsonTuning&#36890;&#36807;&#24110;&#21161;&#27169;&#22411;&#29702;&#35299;&#20851;&#38190;&#20219;&#21153;&#35201;&#32032;&#21450;&#20854;&#20851;&#31995;&#65292;&#25193;&#23637;&#20102;&#36890;&#29992;&#24615;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#27495;&#20041;&#24615;&#25552;&#39640;&#20102;&#31283;&#20581;&#24615;&#65292;&#24182;&#36890;&#36807;&#25552;&#20379;&#23545;&#36755;&#20986;&#30340;&#26174;&#24335;&#25511;&#21046;&#22686;&#24378;&#20102;&#21487;&#25511;&#24615;&#12290;&#25105;&#20204;&#23545;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#35780;&#20272;&#22522;&#20934;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#27604;&#36739;&#30740;&#31350;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;JsonTuning&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;TextTuning&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction tuning has emerged as a crucial process for harnessing the capabilities of large language models (LLMs) by providing explicit task instructions, leading to improved performance in various tasks. However, prevalent text-to-text instruction tuning (TextTuning) methods suffer from limitations in generalization, robustness, and controllability due to the ambiguity and lack of explicit structure in tasks. In this paper, we propose JsonTuning, a novel structure-to-structure approach for instruction tuning. By leveraging the versatility and structured nature of JSON to represent tasks, JsonTuning enhances generalization by helping the model understand essential task elements and their relations, improves robustness by minimizing ambiguity, and increases controllability by providing explicit control over the output. We conduct a comprehensive comparative study with diverse language models and evaluation benchmarks. Experimental results show that JsonTuning outperforms TextTuning in
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#38452;&#24433;&#23545;&#40784;&#36825;&#31181;&#26032;&#30340;&#25915;&#20987;&#26041;&#24335;&#65292;&#36890;&#36807;&#35843;&#25972;&#23569;&#37327;&#24694;&#24847;&#31034;&#20363;&#65292;&#23433;&#20840;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#34987;&#36731;&#26494;&#39072;&#35206;&#29983;&#25104;&#26377;&#23475;&#30340;&#20869;&#23481;&#65292;&#21516;&#26102;&#20173;&#28982;&#21487;&#20197;&#27491;&#30830;&#21709;&#24212;&#24120;&#35268;&#26597;&#35810;&#12290;</title><link>http://arxiv.org/abs/2310.02949</link><description>&lt;p&gt;
&#38452;&#24433;&#23545;&#40784;&#65306;&#36731;&#26494;&#39072;&#35206;&#23433;&#20840;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models. (arXiv:2310.02949v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02949
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#38452;&#24433;&#23545;&#40784;&#36825;&#31181;&#26032;&#30340;&#25915;&#20987;&#26041;&#24335;&#65292;&#36890;&#36807;&#35843;&#25972;&#23569;&#37327;&#24694;&#24847;&#31034;&#20363;&#65292;&#23433;&#20840;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#34987;&#36731;&#26494;&#39072;&#35206;&#29983;&#25104;&#26377;&#23475;&#30340;&#20869;&#23481;&#65292;&#21516;&#26102;&#20173;&#28982;&#21487;&#20197;&#27491;&#30830;&#21709;&#24212;&#24120;&#35268;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35686;&#21578;&#65306;&#26412;&#35770;&#25991;&#21253;&#21547;&#26377;&#23475;&#35821;&#35328;&#30340;&#20363;&#23376;&#65292;&#24314;&#35758;&#35835;&#32773;&#24910;&#37325;&#12290;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36880;&#28176;&#24320;&#25918;&#37322;&#25918;&#65292;&#36890;&#36807;&#38477;&#20302;&#25968;&#25454;&#27880;&#37322;&#21644;&#35745;&#31639;&#30340;&#26680;&#24515;&#25104;&#26412;&#65292;&#20419;&#36827;&#20102;&#19979;&#28216;&#24212;&#29992;&#30340;&#21457;&#23637;&#12290;&#20026;&#20102;&#30830;&#20445;AI&#30340;&#23433;&#20840;&#24615;&#65292;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23433;&#20840;&#23545;&#40784;&#25514;&#26045;&#65292;&#20197;&#20445;&#25252;&#36825;&#20123;&#27169;&#22411;&#20813;&#21463;&#24694;&#24847;&#20351;&#29992;&#65288;&#20027;&#35201;&#26159;&#30828;&#25552;&#31034;&#25915;&#20987;&#65289;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#31181;&#30475;&#20284;&#22362;&#22266;&#30340;&#30420;&#30002;&#32972;&#21518;&#65292;&#21487;&#33021;&#28508;&#20239;&#30528;&#19968;&#20010;&#38452;&#24433;&#12290;&#36890;&#36807;&#20165;&#35843;&#25972;100&#20010;&#24694;&#24847;&#31034;&#20363;&#65292;&#20351;&#29992;1&#20010;GPU&#23567;&#26102;&#65292;&#36825;&#20123;&#23433;&#20840;&#23545;&#40784;&#30340;LLMs&#21487;&#20197;&#36731;&#26494;&#22320;&#34987;&#39072;&#35206;&#20197;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#12290;&#24418;&#24335;&#19978;&#65292;&#25105;&#20204;&#23558;&#19968;&#31181;&#26032;&#25915;&#20987;&#31216;&#20026;&#38452;&#24433;&#23545;&#40784;&#65306;&#21033;&#29992;&#23569;&#37327;&#25968;&#25454;&#21487;&#20197;&#20351;&#23433;&#20840;&#23545;&#40784;&#27169;&#22411;&#36866;&#24212;&#26377;&#23475;&#20219;&#21153;&#65292;&#32780;&#19981;&#20250;&#29306;&#29298;&#27169;&#22411;&#30340;&#26377;&#29992;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#34987;&#39072;&#35206;&#30340;&#27169;&#22411;&#20173;&#28982;&#20445;&#30041;&#20854;&#23545;&#24120;&#35268;&#26597;&#35810;&#30340;&#36866;&#24403;&#21709;&#24212;&#33021;&#21147;&#12290;&#22312;5&#20010;&#21457;&#34892;&#30340;8&#20010;&#27169;&#22411;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Warning: This paper contains examples of harmful language, and reader discretion is recommended. The increasing open release of powerful large language models (LLMs) has facilitated the development of downstream applications by reducing the essential cost of data annotation and computation. To ensure AI safety, extensive safety-alignment measures have been conducted to armor these models against malicious use (primarily hard prompt attack). However, beneath the seemingly resilient facade of the armor, there might lurk a shadow. By simply tuning on 100 malicious examples with 1 GPU hour, these safely aligned LLMs can be easily subverted to generate harmful content. Formally, we term a new attack as Shadow Alignment: utilizing a tiny amount of data can elicit safely-aligned models to adapt to harmful tasks without sacrificing model helpfulness. Remarkably, the subverted models retain their capability to respond appropriately to regular inquiries. Experiments across 8 models released by 5
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;LibriSpeech-PC&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#31471;&#21040;&#31471;ASR&#27169;&#22411;&#30340;&#26631;&#28857;&#21644;&#22823;&#20889;&#39044;&#27979;&#33021;&#21147;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#21253;&#25324;&#19968;&#20010;&#24102;&#26377;&#24674;&#22797;&#26631;&#28857;&#21644;&#22823;&#20889;&#31526;&#21495;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19968;&#31181;&#38024;&#23545;&#26631;&#28857;&#31526;&#21495;&#35780;&#20272;&#30340;&#26032;&#22411;&#35780;&#20272;&#25351;&#26631;Punctuation Error Rate&#65288;PER&#65289;&#65292;&#24182;&#25552;&#20379;&#20102;&#21021;&#27493;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.02943</link><description>&lt;p&gt;
LibriSpeech-PC&#65306;&#35780;&#20272;&#31471;&#21040;&#31471;ASR&#27169;&#22411;&#30340;&#26631;&#28857;&#21644;&#22823;&#20889;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
LibriSpeech-PC: Benchmark for Evaluation of Punctuation and Capitalization Capabilities of end-to-end ASR Models. (arXiv:2310.02943v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02943
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;LibriSpeech-PC&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#31471;&#21040;&#31471;ASR&#27169;&#22411;&#30340;&#26631;&#28857;&#21644;&#22823;&#20889;&#39044;&#27979;&#33021;&#21147;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#21253;&#25324;&#19968;&#20010;&#24102;&#26377;&#24674;&#22797;&#26631;&#28857;&#21644;&#22823;&#20889;&#31526;&#21495;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19968;&#31181;&#38024;&#23545;&#26631;&#28857;&#31526;&#21495;&#35780;&#20272;&#30340;&#26032;&#22411;&#35780;&#20272;&#25351;&#26631;Punctuation Error Rate&#65288;PER&#65289;&#65292;&#24182;&#25552;&#20379;&#20102;&#21021;&#27493;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#36755;&#20986;&#27809;&#26377;&#26631;&#28857;&#31526;&#21495;&#30340;&#23567;&#20889;&#21333;&#35789;&#65292;&#36825;&#38477;&#20302;&#20102;&#21487;&#35835;&#24615;&#65292;&#38656;&#35201;&#21518;&#32493;&#30340;&#25991;&#26412;&#22788;&#29702;&#27169;&#22411;&#23558;ASR&#36716;&#24405;&#36716;&#25442;&#20026;&#27491;&#30830;&#30340;&#26684;&#24335;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#33021;&#22815;&#39044;&#27979;&#26631;&#28857;&#21644;&#22823;&#20889;&#30340;&#31471;&#21040;&#31471;ASR&#27169;&#22411;&#30340;&#24320;&#21457;&#38754;&#20020;&#30528;&#22810;&#20010;&#25361;&#25112;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#26377;&#38480;&#30340;&#25968;&#25454;&#21487;&#29992;&#24615;&#21644;&#29616;&#26377;&#35780;&#20272;&#26041;&#27861;&#30340;&#19981;&#36275;&#65292;&#22914;&#23545;&#26631;&#28857;&#39044;&#27979;&#30340;&#35780;&#20272;&#19981;&#36275;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;LibriSpeech-PC&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;&#31471;&#21040;&#31471;ASR&#27169;&#22411;&#30340;&#26631;&#28857;&#21644;&#22823;&#20889;&#39044;&#27979;&#33021;&#21147;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#21253;&#25324;&#19968;&#20010;&#20855;&#26377;&#24674;&#22797;&#26631;&#28857;&#21644;&#22823;&#20889;&#31526;&#21495;&#30340;LibriSpeech-PC&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19968;&#20010;&#21517;&#20026;Punctuation Error Rate&#65288;PER&#65289;&#30340;&#26032;&#39062;&#35780;&#20272;&#25351;&#26631;&#65292;&#19987;&#27880;&#20110;&#26631;&#28857;&#31526;&#21495;&#30340;&#35780;&#20272;&#65292;&#36824;&#21253;&#21547;&#21021;&#27493;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;&#25152;&#26377;&#20195;&#30721;&#12289;&#25968;&#25454;&#21644;&#27169;&#22411;&#37117;&#26159;&#20844;&#24320;&#21487;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional automatic speech recognition (ASR) models output lower-cased words without punctuation marks, which reduces readability and necessitates a subsequent text processing model to convert ASR transcripts into a proper format. Simultaneously, the development of end-to-end ASR models capable of predicting punctuation and capitalization presents several challenges, primarily due to limited data availability and shortcomings in the existing evaluation methods, such as inadequate assessment of punctuation prediction. In this paper, we introduce a LibriSpeech-PC benchmark designed to assess the punctuation and capitalization prediction capabilities of end-to-end ASR models. The benchmark includes a LibriSpeech-PC dataset with restored punctuation and capitalization, a novel evaluation metric called Punctuation Error Rate (PER) that focuses on punctuation marks, and initial baseline models. All code, data, and models are publicly available.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31185;&#23398;&#20256;&#25773;&#21407;&#21017;&#30340;&#32508;&#21512;&#35780;&#20272;&#26694;&#26550;&#65292;&#35780;&#20272;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#27668;&#20505;&#21464;&#21270;&#20449;&#24687;&#20013;&#30340;&#34920;&#29616;&#65292;&#33021;&#22815;&#22312;&#22238;&#31572;&#27668;&#20505;&#21464;&#21270;&#20027;&#39064;&#26041;&#38754;&#25552;&#20379;&#32454;&#31890;&#24230;&#30340;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2310.02932</link><description>&lt;p&gt;
&#23545;&#27668;&#20505;&#20449;&#24687;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Assessing Large Language Models on Climate Information. (arXiv:2310.02932v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02932
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31185;&#23398;&#20256;&#25773;&#21407;&#21017;&#30340;&#32508;&#21512;&#35780;&#20272;&#26694;&#26550;&#65292;&#35780;&#20272;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#27668;&#20505;&#21464;&#21270;&#20449;&#24687;&#20013;&#30340;&#34920;&#29616;&#65292;&#33021;&#22815;&#22312;&#22238;&#31572;&#27668;&#20505;&#21464;&#21270;&#20027;&#39064;&#26041;&#38754;&#25552;&#20379;&#32454;&#31890;&#24230;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#27668;&#20505;&#21464;&#21270;&#23545;&#25105;&#20204;&#30340;&#24433;&#21709;&#65292;&#20102;&#35299;&#21487;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26159;&#36171;&#20104;&#20010;&#20154;&#21644;&#31038;&#21306;&#20943;&#32531;&#21644;&#36866;&#24212;&#27668;&#20505;&#21464;&#21270;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290;&#38543;&#30528;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26222;&#21450;&#65292;&#26377;&#24517;&#35201;&#35780;&#20272;&#23427;&#20204;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31185;&#23398;&#20256;&#25773;&#21407;&#21017;&#30340;&#32508;&#21512;&#35780;&#20272;&#26694;&#26550;&#65292;&#20197;&#20998;&#26512;LLM&#23545;&#27668;&#20505;&#21464;&#21270;&#20027;&#39064;&#30340;&#22238;&#31572;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#24378;&#35843;&#22238;&#31572;&#30340;&#21576;&#29616;&#21644;&#35748;&#35782;&#19978;&#30340;&#36866;&#24403;&#24615;&#65292;&#20026;LLM&#29983;&#25104;&#25552;&#20379;&#20102;&#32454;&#31890;&#24230;&#30340;&#20998;&#26512;&#12290;&#35206;&#30422;&#20102;8&#20010;&#32500;&#24230;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#35782;&#21035;&#27169;&#22411;&#36755;&#20986;&#20013;&#30340;30&#20010;&#19981;&#21516;&#38382;&#39064;&#12290;&#35813;&#20219;&#21153;&#26159;&#19968;&#20010;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#20363;&#23376;&#65292;&#36825;&#20010;&#39046;&#22495;&#23384;&#22312;&#36234;&#26469;&#36234;&#22810;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;AI&#21487;&#20197;&#34917;&#20805;&#21644;&#25552;&#21319;&#20154;&#31867;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#23454;&#29992;&#30340;&#21487;&#25193;&#23637;&#30417;&#30563;&#21327;&#35758;&#65292;&#21033;&#29992;AI&#36741;&#21161;&#24182;&#20381;&#38752;&#20855;&#26377;&#30456;&#20851;&#25945;&#32946;&#32972;&#26223;&#30340;&#35780;&#20272;&#21592;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20960;&#20010;&#26368;&#36817;&#30340;LLM&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding how climate change affects us and learning about available solutions are key steps toward empowering individuals and communities to mitigate and adapt to it. As Large Language Models (LLMs) rise in popularity, it is necessary to assess their capability in this domain. In this study, we present a comprehensive evaluation framework, grounded in science communication principles, to analyze LLM responses to climate change topics. Our framework emphasizes both the presentational and epistemological adequacy of answers, offering a fine-grained analysis of LLM generations. Spanning 8 dimensions, our framework discerns up to 30 distinct issues in model outputs. The task is a real-world example of a growing number of challenging problems where AI can complement and lift human performance. We introduce a novel and practical protocol for scalable oversight that uses AI Assistance and relies on raters with relevant educational backgrounds. We evaluate several recent LLMs and conduct 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#25506;&#27979;&#22120;&#21644;&#36716;&#25442;&#22120;&#20248;&#21270;&#25351;&#20196;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.02905</link><description>&lt;p&gt;
&#20351;&#29992;&#24744;&#30340;&#26412;&#33021;&#65306;&#20351;&#29992;&#31070;&#32463;&#25506;&#27979;&#22120;&#19982;&#36716;&#25442;&#22120;&#36827;&#34892;&#25351;&#20196;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Use Your INSTINCT: INSTruction optimization usIng Neural bandits Coupled with Transformers. (arXiv:2310.02905v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02905
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#25506;&#27979;&#22120;&#21644;&#36716;&#25442;&#22120;&#20248;&#21270;&#25351;&#20196;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#65292;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;LLMs&#30340;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#32473;&#20104;&#23427;&#20204;&#30340;&#25351;&#20196;&#65292;&#36825;&#20123;&#25351;&#20196;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#36827;&#34892;&#25163;&#21160;&#35843;&#25972;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20351;&#29992;&#20102;&#39640;&#25928;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#31639;&#27861;&#26469;&#33258;&#21160;&#20248;&#21270;&#32473;&#20104;&#40657;&#30418;LLMs&#30340;&#25351;&#20196;&#12290;&#28982;&#32780;&#65292;&#22312;&#20248;&#21270;&#39640;&#24230;&#22797;&#26434;&#65288;&#20363;&#22914;&#39640;&#32500;&#65289;&#30340;&#30446;&#26631;&#20989;&#25968;&#26102;&#65292;&#22914;&#23558;&#25351;&#20196;&#26144;&#23556;&#21040;LLM&#24615;&#33021;&#30340;&#20989;&#25968;&#65292;BO&#36890;&#24120;&#34920;&#29616;&#19981;&#20339;&#12290;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;BO&#20351;&#29992;&#30340;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#26377;&#38480;&#65292;&#35813;&#27169;&#22411;&#34987;&#29992;&#20316;BO&#30340;&#20195;&#29702;&#26469;&#24314;&#27169;&#30446;&#26631;&#20989;&#25968;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#24050;&#32463;&#22810;&#27425;&#35777;&#26126;&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#65292;&#23588;&#20854;&#26159;&#39044;&#35757;&#32451;&#30340;&#36716;&#25442;&#22120;&#65292;&#20855;&#26377;&#24456;&#24378;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#21487;&#20197;&#24314;&#27169;&#39640;&#24230;&#22797;&#26434;&#30340;&#20989;&#25968;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#31070;&#32463;&#25506;&#27979;&#22120;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown remarkable instruction-following capabilities and achieved impressive performances in various applications. However, the performances of LLMs depend heavily on the instructions given to them, which are typically manually tuned with substantial human efforts. Recent work has used the query-efficient Bayesian optimization (BO) algorithm to automatically optimize the instructions given to black-box LLMs. However, BO usually falls short when optimizing highly sophisticated (e.g., high-dimensional) objective functions, such as the functions mapping an instruction to the performance of an LLM. This is mainly due to the limited expressive power of the Gaussian process (GP) model which is used by BO as a surrogate to model the objective function. Meanwhile, it has been repeatedly shown that neural networks (NNs), especially pre-trained transformers, possess strong expressive power and can model highly complex functions. So, we adopt a neural bandit algor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26377;&#38480;&#25968;&#25454;&#29615;&#22659;&#20013;&#36827;&#34892;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#25216;&#26415;&#35299;&#20915;&#20102;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#36890;&#36807;&#22312;&#19968;&#31181;&#36164;&#28304;&#20016;&#23500;&#30340;&#35821;&#35328;&#20013;&#21512;&#25104;&#26032;&#30340;&#20167;&#24680;&#35328;&#35770;&#25968;&#25454;&#31034;&#20363;&#65292;&#20351;&#24471;&#26816;&#27979;&#31995;&#32479;&#33021;&#22815;&#22312;&#20854;&#20182;&#35821;&#35328;&#29615;&#22659;&#20013;&#36827;&#34892;&#26377;&#25928;&#30340;&#20998;&#31867;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.02876</link><description>&lt;p&gt;
&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#22312;&#26377;&#38480;&#25968;&#25454;&#29615;&#22659;&#20013;&#36827;&#34892;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Hate Speech Detection in Limited Data Contexts using Synthetic Data Generation. (arXiv:2310.02876v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26377;&#38480;&#25968;&#25454;&#29615;&#22659;&#20013;&#36827;&#34892;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#25216;&#26415;&#35299;&#20915;&#20102;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#36890;&#36807;&#22312;&#19968;&#31181;&#36164;&#28304;&#20016;&#23500;&#30340;&#35821;&#35328;&#20013;&#21512;&#25104;&#26032;&#30340;&#20167;&#24680;&#35328;&#35770;&#25968;&#25454;&#31034;&#20363;&#65292;&#20351;&#24471;&#26816;&#27979;&#31995;&#32479;&#33021;&#22815;&#22312;&#20854;&#20182;&#35821;&#35328;&#29615;&#22659;&#20013;&#36827;&#34892;&#26377;&#25928;&#30340;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#20110;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#65292;&#20197;&#20415;&#26816;&#27979;&#32593;&#19978;&#21457;&#24067;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#20167;&#24680;&#35328;&#35770;&#12290;&#36825;&#19968;&#36827;&#23637;&#20165;&#38480;&#20110;&#19968;&#20123;&#36164;&#28304;&#20805;&#36275;&#30340;&#35821;&#35328;&#65292;&#23548;&#33268;&#22312;&#26377;&#38480;&#25968;&#25454;&#29615;&#22659;&#20013;&#26816;&#27979;&#31995;&#32479;&#24615;&#33021;&#19981;&#20339;&#25110;&#26681;&#26412;&#19981;&#23384;&#22312;&#12290;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;&#36825;&#20123;&#29615;&#22659;&#20013;&#33719;&#21462;&#21644;&#25972;&#29702;&#35757;&#32451;&#25968;&#25454;&#30340;&#25104;&#26412;&#36739;&#39640;&#25152;&#33268;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#25216;&#26415;&#35299;&#20915;&#20102;&#26377;&#38480;&#25968;&#25454;&#29615;&#22659;&#20013;&#22312;&#32447;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20013;&#32570;&#20047;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#22312;&#26576;&#20123;&#36164;&#28304;&#20016;&#23500;&#30340;&#35821;&#35328;&#65288;&#22914;&#33521;&#35821;&#65289;&#20013;&#32473;&#23450;&#19968;&#20123;&#20167;&#24680;&#35328;&#35770;&#31034;&#20363;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#26041;&#27861;&#26469;&#21512;&#25104;&#20445;&#30041;&#21407;&#22987;&#31034;&#20363;&#20013;&#20167;&#24680;&#24773;&#32490;&#20294;&#36716;&#31227;&#20167;&#24680;&#23545;&#35937;&#30340;&#26032;&#30340;&#20167;&#24680;&#35328;&#35770;&#25968;&#25454;&#31034;&#20363;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#29983;&#25104;&#29992;&#20110;&#21360;&#22320;&#35821;&#21644;&#36234;&#21335;&#35821;&#30340;&#20167;&#24680;&#35328;&#35770;&#20998;&#31867;&#20219;&#21153;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
A growing body of work has focused on text classification methods for detecting the increasing amount of hate speech posted online. This progress has been limited to only a select number of highly-resourced languages causing detection systems to either under-perform or not exist in limited data contexts. This is majorly caused by a lack of training data which is expensive to collect and curate in these settings. In this work, we propose a data augmentation approach that addresses the problem of lack of data for online hate speech detection in limited data contexts using synthetic data generation techniques. Given a handful of hate speech examples in a high-resource language such as English, we present three methods to synthesize new examples of hate speech data in a target language that retains the hate sentiment in the original examples but transfers the hate targets. We apply our approach to generate training data for hate speech classification tasks in Hindi and Vietnamese. Our find
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26234;&#33021;&#22810;&#20219;&#21153;&#36866;&#24212;&#28151;&#21512;&#25552;&#31034;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;LLM&#22312;&#22788;&#29702;&#24322;&#36136;&#20219;&#21153;&#21644;&#25968;&#25454;&#20998;&#24067;&#26102;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#32773;&#35774;&#35745;&#20102;&#26234;&#33021;&#38376;&#25511;&#21151;&#33021;&#65292;&#29992;&#20110;&#35782;&#21035;&#23884;&#20837;&#22312;&#19981;&#21516;&#25552;&#31034;&#32452;&#20013;&#30340;&#30456;&#20851;&#25216;&#33021;&#65292;&#24182;&#26681;&#25454;&#30446;&#26631;&#20219;&#21153;&#30340;&#38656;&#27714;&#21160;&#24577;&#20998;&#37197;&#32452;&#21512;&#19987;&#23478;&#12290;&#35813;&#26041;&#27861;&#23545;&#20219;&#20309;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#37117;&#19981;&#21463;&#38480;&#21046;&#65292;&#25552;&#39640;&#20102;&#20219;&#21153;&#22788;&#29702;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.02842</link><description>&lt;p&gt;
&#29992;&#26234;&#33021;&#22810;&#20219;&#21153;&#36866;&#24212;&#28151;&#21512;&#25552;&#31034;&#25195;&#25551;&#24322;&#36136;&#24615;
&lt;/p&gt;
&lt;p&gt;
Sweeping Heterogeneity with Smart MoPs: Mixture of Prompts for LLM Task Adaptation. (arXiv:2310.02842v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26234;&#33021;&#22810;&#20219;&#21153;&#36866;&#24212;&#28151;&#21512;&#25552;&#31034;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;LLM&#22312;&#22788;&#29702;&#24322;&#36136;&#20219;&#21153;&#21644;&#25968;&#25454;&#20998;&#24067;&#26102;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#32773;&#35774;&#35745;&#20102;&#26234;&#33021;&#38376;&#25511;&#21151;&#33021;&#65292;&#29992;&#20110;&#35782;&#21035;&#23884;&#20837;&#22312;&#19981;&#21516;&#25552;&#31034;&#32452;&#20013;&#30340;&#30456;&#20851;&#25216;&#33021;&#65292;&#24182;&#26681;&#25454;&#30446;&#26631;&#20219;&#21153;&#30340;&#38656;&#27714;&#21160;&#24577;&#20998;&#37197;&#32452;&#21512;&#19987;&#23478;&#12290;&#35813;&#26041;&#27861;&#23545;&#20219;&#20309;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#37117;&#19981;&#21463;&#38480;&#21046;&#65292;&#25552;&#39640;&#20102;&#20219;&#21153;&#22788;&#29702;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#26377;&#33021;&#21147;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#65292;&#22914;&#25991;&#26412;&#25688;&#35201;&#21644;&#25968;&#23398;&#38382;&#39064;&#65292;&#20294;&#36890;&#24120;&#26159;&#38024;&#23545;&#21333;&#19968;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#12290;&#30001;&#20110;&#39640;&#35745;&#31639;&#25104;&#26412;&#65292;&#24403;&#21069;&#36235;&#21183;&#26159;&#20351;&#29992;&#25552;&#31034;&#25351;&#23548;&#35843;&#33410;&#39044;&#20808;&#35757;&#32451;&#30340;LLM&#20197;&#36866;&#24212;&#26032;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#22914;&#20309;&#25193;&#23637;&#25552;&#31034;&#35843;&#33410;&#20197;&#21516;&#26102;&#22788;&#29702;&#24322;&#36136;&#20219;&#21153;&#21644;&#25968;&#25454;&#20998;&#24067;&#26159;&#19968;&#20010;&#24191;&#27867;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;"&#28151;&#21512;&#25552;&#31034;"&#25110;MoPs&#65292;&#24182;&#32467;&#21512;&#26234;&#33021;&#38376;&#25511;&#21151;&#33021;&#65306;&#21518;&#32773;&#30340;&#35774;&#35745;&#26159;&#26412;&#25991;&#30340;&#36129;&#29486;&#20043;&#19968;&#65292;&#23427;&#21487;&#20197;&#35782;&#21035;&#23884;&#20837;&#22312;&#19981;&#21516;&#25552;&#31034;&#32452;&#20013;&#30340;&#30456;&#20851;&#25216;&#33021;&#65292;&#24182;&#26681;&#25454;&#30446;&#26631;&#20219;&#21153;&#21160;&#24577;&#20998;&#37197;&#32452;&#21512;&#19987;&#23478;(&#21363;&#19968;&#32452;&#25552;&#31034;)&#12290;&#27492;&#22806;&#65292;MoPs&#22312;&#24212;&#29992;&#20219;&#20309;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#26102;&#37117;&#19981;&#21463;&#24433;&#21709;&#8212;&#8212;&#20197;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have the ability to solve a variety of tasks, such as text summarization and mathematical questions, just out of the box, but they are often trained with a single task in mind. Due to high computational costs, the current trend is to use prompt instruction tuning to better adjust monolithic, pretrained LLMs for new -- but often individual -- downstream tasks. Thus, how one would expand prompt tuning to handle -- concomitantly -heterogeneous tasks and data distributions is a widely open question. To address this gap, we suggest the use of \emph{Mixture of Prompts}, or MoPs, associated with smart gating functionality: the latter -- whose design is one of the contributions of this paper -- can identify relevant skills embedded in different groups of prompts and dynamically assign combined experts (i.e., collection of prompts), based on the target task. Additionally, MoPs are empirically agnostic to any model compression technique applied -- for efficiency re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#27493;&#39588;&#22810;&#27169;&#24577;&#25512;&#29702;&#30340;&#21452;&#31995;&#32479;&#65292;&#20854;&#20013;&#19968;&#20010;&#31995;&#32479;&#29992;&#20110;&#25552;&#21462;&#35270;&#35273;&#20449;&#24687;&#65292;&#21478;&#19968;&#20010;&#31995;&#32479;&#29992;&#20110;&#26377;&#24847;&#25512;&#29702;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36991;&#20813;&#27169;&#22411;&#22312;&#19968;&#20010;&#27493;&#39588;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#25110;&#34987;&#36716;&#25442;&#25991;&#26412;&#20013;&#30340;&#35823;&#23548;&#20449;&#24687;&#22256;&#25200;&#12290;</title><link>http://arxiv.org/abs/2310.02804</link><description>&lt;p&gt;
DOMINO: &#22810;&#27493;&#39588;&#35270;&#35273;&#35821;&#35328;&#25512;&#29702;&#30340;&#21452;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
DOMINO: A Dual-System for Multi-step Visual Language Reasoning. (arXiv:2310.02804v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02804
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#27493;&#39588;&#22810;&#27169;&#24577;&#25512;&#29702;&#30340;&#21452;&#31995;&#32479;&#65292;&#20854;&#20013;&#19968;&#20010;&#31995;&#32479;&#29992;&#20110;&#25552;&#21462;&#35270;&#35273;&#20449;&#24687;&#65292;&#21478;&#19968;&#20010;&#31995;&#32479;&#29992;&#20110;&#26377;&#24847;&#25512;&#29702;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36991;&#20813;&#27169;&#22411;&#22312;&#19968;&#20010;&#27493;&#39588;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#25110;&#34987;&#36716;&#25442;&#25991;&#26412;&#20013;&#30340;&#35823;&#23548;&#20449;&#24687;&#22256;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#25512;&#29702;&#38656;&#35201;&#19968;&#20010;&#31995;&#32479;&#20174;&#20449;&#24687;&#23494;&#38598;&#30340;&#22270;&#20687;&#65288;&#22914;&#22270;&#34920;&#25110;&#32472;&#22270;&#65289;&#20013;&#25552;&#21462;&#25991;&#26412;&#25110;&#25968;&#23383;&#65292;&#24182;&#25191;&#34892;&#36923;&#36753;&#25110;&#31639;&#26415;&#25512;&#29702;&#20197;&#24471;&#20986;&#31572;&#26696;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#20381;&#38752;&#20197;&#19979;&#20004;&#31181;&#26041;&#27861;&#65306;&#65288;1&#65289;&#35757;&#32451;&#22312;&#22823;&#37327;&#25968;&#25454;&#19978;&#30340;&#31471;&#21040;&#31471;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#25110;&#32773;&#65288;2&#65289;&#20351;&#29992;&#20004;&#38454;&#27573;&#30340;&#27969;&#31243;&#65292;&#20854;&#20013;&#19968;&#20010;&#23383;&#24149;&#27169;&#22411;&#23558;&#22270;&#20687;&#36716;&#25442;&#25104;&#25991;&#26412;&#65292;&#28982;&#21518;&#30001;&#21478;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35835;&#21462;&#25991;&#26412;&#20197;&#25512;&#26029;&#20986;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#21069;&#19968;&#31181;&#26041;&#27861;&#24378;&#36843;&#27169;&#22411;&#29992;&#21333;&#20010;&#27493;&#39588;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#65292;&#21518;&#19968;&#31181;&#26041;&#27861;&#23481;&#26131;&#20135;&#29983;&#36716;&#25442;&#25991;&#26412;&#20013;&#30340;&#19981;&#20934;&#30830;&#25110;&#20998;&#25955;&#27880;&#24847;&#21147;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#35753;&#35821;&#35328;&#27169;&#22411;&#28151;&#28102;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#27493;&#39588;&#22810;&#27169;&#22411;&#25512;&#29702;&#30340;&#21452;&#31995;&#32479;&#65292;&#23427;&#21253;&#21547;&#29992;&#20110;&#35270;&#35273;&#20449;&#24687;&#25552;&#21462;&#30340;&#8220;System-1&#8221;&#27493;&#39588;&#21644;&#29992;&#20110;&#26377;&#24847;&#25512;&#29702;&#30340;&#8220;System-2&#8221;&#27493;&#39588;&#12290;&#32473;&#23450;&#19968;&#20010;&#36755;&#20837;&#65292;System-2&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#21407;&#23376;&#23376;&#27493;&#39588;&#65292;&#27599;&#20010;&#23376;&#27493;&#39588;&#25351;&#23548;System-1&#36827;&#34892;&#20449;&#24687;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual language reasoning requires a system to extract text or numbers from information-dense images like charts or plots and perform logical or arithmetic reasoning to arrive at an answer. To tackle this task, existing work relies on either (1) an end-to-end vision-language model trained on a large amount of data, or (2) a two-stage pipeline where a captioning model converts the image into text that is further read by another large language model to deduce the answer. However, the former approach forces the model to answer a complex question with one single step, and the latter approach is prone to inaccurate or distracting information in the converted text that can confuse the language model. In this work, we propose a dual-system for multi-step multimodal reasoning, which consists of a "System-1" step for visual information extraction and a "System-2" step for deliberate reasoning. Given an input, System-2 breaks down the question into atomic sub-steps, each guiding System-1 to extr
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#27880;&#24847;&#21147;&#21464;&#21387;&#22120;&#26550;&#26500;&#27169;&#22411;&#21644;&#26500;&#24314;&#22522;&#32447;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#25688;&#35201;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20044;&#23572;&#37117;&#35821;&#20013;&#30340;&#26032;&#38395;&#24212;&#29992;&#39046;&#22495;&#30340;&#23454;&#29616;&#26469;&#35777;&#26126;&#20854;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.02790</link><description>&lt;p&gt;
&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20302;&#36164;&#28304;&#25688;&#35201;&#30340;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Low Resource Summarization using Pre-trained Language Models. (arXiv:2310.02790v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02790
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#27880;&#24847;&#21147;&#21464;&#21387;&#22120;&#26550;&#26500;&#27169;&#22411;&#21644;&#26500;&#24314;&#22522;&#32447;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#25688;&#35201;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20044;&#23572;&#37117;&#35821;&#20013;&#30340;&#26032;&#38395;&#24212;&#29992;&#39046;&#22495;&#30340;&#23454;&#29616;&#26469;&#35777;&#26126;&#20854;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#22312;&#25991;&#26412;&#25968;&#25454;&#22788;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#36825;&#26041;&#38754;&#30340;&#30740;&#31350;&#22823;&#22810;&#23616;&#38480;&#20110;&#39640;&#36164;&#28304;&#35821;&#35328;&#65292;&#22914;&#33521;&#35821;&#65292;&#20302;&#36164;&#28304;&#35821;&#35328;&#20173;&#28982;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#20855;&#26377;&#22522;&#32447;&#35780;&#20272;&#32467;&#26524;&#30340;&#27169;&#22411;&#12290;&#32771;&#34385;&#21040;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#36164;&#28304;&#26377;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#20302;&#36164;&#28304;&#25688;&#35201;&#30340;&#33258;&#27880;&#24847;&#21147;&#21464;&#21387;&#22120;&#26550;&#26500;&#27169;&#22411;&#65288;mBERT&#65292;mT5&#65289;&#30340;&#26041;&#27861;&#65292;&#24182;&#34917;&#20805;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#32447;&#25968;&#25454;&#38598;&#65288;76.5&#19975;&#31687;&#25991;&#31456;&#12289;&#25688;&#35201;&#23545;&#65289;&#26469;&#36827;&#34892;&#20302;&#36164;&#28304;&#35821;&#35328;&#20044;&#23572;&#37117;&#35821;&#30340;&#25688;&#35201;&#12290;&#36873;&#25321;&#26032;&#38395;&#65288;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#26469;&#28304;&#65289;&#20316;&#20026;&#24212;&#29992;&#39046;&#22495;&#26377;&#28508;&#21147;&#20351;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20854;&#20182;&#36164;&#28304;&#26377;&#38480;&#30340;&#35821;&#35328;&#20013;&#33021;&#22815;&#22797;&#21046;&#12290;&#25105;&#20204;&#30340;&#36866;&#24212;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
With the advent of Deep Learning based Artificial Neural Networks models, Natural Language Processing (NLP) has witnessed significant improvements in textual data processing in terms of its efficiency and accuracy. However, the research is mostly restricted to high-resource languages such as English and low-resource languages still suffer from a lack of available resources in terms of training datasets as well as models with even baseline evaluation results. Considering the limited availability of resources for low-resource languages, we propose a methodology for adapting self-attentive transformer-based architecture models (mBERT, mT5) for low-resource summarization, supplemented by the construction of a new baseline dataset (76.5k article, summary pairs) in a low-resource language Urdu. Choosing news (a publicly available source) as the application domain has the potential to make the proposed methodology useful for reproducing in other languages with limited resources. Our adapted s
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;UMLS&#30340;&#22686;&#24378;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#26088;&#22312;&#25913;&#21892;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20013;&#27169;&#22411;&#29983;&#25104;&#20869;&#23481;&#30340;&#20107;&#23454;&#24615;&#12290;&#36890;&#36807;&#33258;&#21160;&#35780;&#20272;&#21644;&#21307;&#29983;&#35780;&#20272;&#65292;&#30740;&#31350;&#20154;&#21592;&#39564;&#35777;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.02778</link><description>&lt;p&gt;
&#19968;&#20010;&#22686;&#24378;&#30340; UMLS &#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#20107;&#23454;&#24615;
&lt;/p&gt;
&lt;p&gt;
A UMLS-Augmented Framework for Improving Factuality in Large Language Models within Healthcare. (arXiv:2310.02778v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02778
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;UMLS&#30340;&#22686;&#24378;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#26088;&#22312;&#25913;&#21892;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20013;&#27169;&#22411;&#29983;&#25104;&#20869;&#23481;&#30340;&#20107;&#23454;&#24615;&#12290;&#36890;&#36807;&#33258;&#21160;&#35780;&#20272;&#21644;&#21307;&#29983;&#35780;&#20272;&#65292;&#30740;&#31350;&#20154;&#21592;&#39564;&#35777;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#65292;&#20026;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#24102;&#26469;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#21019;&#26032;&#12290;&#28982;&#32780;&#65292;&#23558;LLMs&#24212;&#29992;&#20110;&#30495;&#23454;&#20020;&#24202;&#22330;&#26223;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#29983;&#25104;&#19982;&#24050;&#24314;&#31435;&#21307;&#23398;&#20107;&#23454;&#20559;&#31163;&#30340;&#20869;&#23481;&#65292;&#29978;&#33267;&#21487;&#33021;&#34920;&#29616;&#20986;&#28508;&#22312;&#30340;&#20559;&#35265;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#32479;&#19968;&#21307;&#23398;&#35821;&#35328;&#31995;&#32479;&#65288;UMLS&#65289;&#30340;&#22686;&#24378;&#22411;LLM&#26694;&#26550;&#65292;&#26088;&#22312;&#26356;&#22909;&#22320;&#26381;&#21153;&#21307;&#30103;&#20445;&#20581;&#31038;&#21306;&#12290;&#25105;&#20204;&#37319;&#29992;LLaMa2-13b-chat&#21644;ChatGPT-3.5&#20316;&#20026;&#22522;&#20934;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;ROUGE&#20998;&#25968;&#21644;BERT&#20998;&#25968;&#22312;LiveQA&#27979;&#35797;&#38598;&#30340;104&#20010;&#38382;&#39064;&#19978;&#36827;&#34892;&#33258;&#21160;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26681;&#25454;&#20107;&#23454;&#24615;&#12289;&#23436;&#25972;&#24615;&#12289;&#21487;&#35835;&#24615;&#21644;&#30456;&#20851;&#24615;&#22235;&#20010;&#32500;&#24230;&#24314;&#31435;&#20102;&#21307;&#29983;&#35780;&#20272;&#26631;&#20934;&#12290;ChatGPT-3.5&#29992;&#20110;&#21307;&#29983;&#35780;&#20272;&#65292;&#38024;&#23545;LiveQA&#27979;&#35797;&#38598;&#30340;20&#20010;&#38382;&#39064;&#12290;&#22810;&#20301;&#20303;&#38498;&#21307;&#24072;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated powerful text generation capabilities, bringing unprecedented innovation to the healthcare field. While LLMs hold immense promise for applications in healthcare, applying them to real clinical scenarios presents significant challenges, as these models may generate content that deviates from established medical facts and even exhibit potential biases. In our research, we develop an augmented LLM framework based on the Unified Medical Language System (UMLS), aiming to better serve the healthcare community. We employ LLaMa2-13b-chat and ChatGPT-3.5 as our benchmark models, and conduct automatic evaluations using the ROUGE Score and BERTScore on 104 questions from the LiveQA test set. Additionally, we establish criteria for physician-evaluation based on four dimensions: Factuality, Completeness, Readability and Relevancy. ChatGPT-3.5 is used for physician evaluation with 20 questions on the LiveQA test set. Multiple resident physicians conduct
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#32452;&#21512;&#27867;&#21270;&#30340;&#19968;&#31181;&#37325;&#35201;&#23646;&#24615;&#65306;&#35821;&#35328;&#20808;&#39564;&#12290;&#30740;&#31350;&#21457;&#29616;&#24403;&#21069;&#30340;&#25913;&#36827;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#35821;&#35328;&#20808;&#39564;&#32780;&#24573;&#35270;&#20102;&#22270;&#20687;&#20013;&#30340;&#20449;&#24687;&#12290;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19981;&#20381;&#36182;&#20110;&#35821;&#35328;&#20808;&#39564;&#30340;&#32452;&#21512;&#24615;&#24230;&#37327;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.02777</link><description>&lt;p&gt;
&#35821;&#35328;&#20808;&#39564;&#22312;&#27979;&#37327;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#32452;&#21512;&#27867;&#21270;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
The Role of Linguistic Priors in Measuring Compositional Generalization of Vision-Language Models. (arXiv:2310.02777v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02777
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#32452;&#21512;&#27867;&#21270;&#30340;&#19968;&#31181;&#37325;&#35201;&#23646;&#24615;&#65306;&#35821;&#35328;&#20808;&#39564;&#12290;&#30740;&#31350;&#21457;&#29616;&#24403;&#21069;&#30340;&#25913;&#36827;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#35821;&#35328;&#20808;&#39564;&#32780;&#24573;&#35270;&#20102;&#22270;&#20687;&#20013;&#30340;&#20449;&#24687;&#12290;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19981;&#20381;&#36182;&#20110;&#35821;&#35328;&#20808;&#39564;&#30340;&#32452;&#21512;&#24615;&#24230;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#21512;&#24615;&#26159;&#35768;&#22810;&#27169;&#24577;&#20013;&#24120;&#35265;&#30340;&#23646;&#24615;&#65292;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#21644;&#22270;&#20687;&#65292;&#20294;&#22810;&#27169;&#24335;&#27169;&#22411;&#30340;&#32452;&#21512;&#27867;&#21270;&#23578;&#19981;&#22826;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#35270;&#35273;-&#35821;&#35328;&#32452;&#21512;&#24615;&#30340;&#20004;&#20010;&#26469;&#28304;&#65306;&#35821;&#35328;&#20808;&#39564;&#21644;&#22270;&#20687;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#26174;&#31034;&#24403;&#21069;&#25913;&#36827;&#32452;&#21512;&#24615;&#27867;&#21270;&#30340;&#23581;&#35797;&#20381;&#36182;&#20110;&#35821;&#35328;&#20808;&#39564;&#32780;&#19981;&#26159;&#22270;&#20687;&#20013;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#27809;&#26377;&#36825;&#31181;&#35821;&#35328;&#20808;&#39564;&#30340;&#32452;&#21512;&#24615;&#24230;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compositionality is a common property in many modalities including natural languages and images, but the compositional generalization of multi-modal models is not well-understood. In this paper, we identify two sources of visual-linguistic compositionality: linguistic priors and the interplay between images and texts. We show that current attempts to improve compositional generalization rely on linguistic priors rather than on information in the image. We also propose a new metric for compositionality without such linguistic priors.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20851;&#27880;&#33258;&#21160;&#25688;&#35201;&#35780;&#20272;&#30340;&#27604;&#36739;&#30740;&#31350;&#21644;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;LangChain&#21644;&#28151;&#21512;&#31639;&#27861;&#23545;PDF&#25991;&#26723;&#36827;&#34892;&#25688;&#35201;&#21644;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#65292;&#20197;&#30830;&#23450;&#29992;&#25143;&#23545;&#25688;&#35201;&#20869;&#23481;&#30340;&#29702;&#35299;&#31243;&#24230;&#22914;&#20309;&#12290;</title><link>http://arxiv.org/abs/2310.02759</link><description>&lt;p&gt;
&#33258;&#21160;&#25688;&#35201;&#35780;&#20272;&#30340;&#27604;&#36739;&#30740;&#31350;&#21644;&#26694;&#26550;&#65306;LangChain&#21644;&#28151;&#21512;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Comparative Study and Framework for Automated Summariser Evaluation: LangChain and Hybrid Algorithms. (arXiv:2310.02759v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20851;&#27880;&#33258;&#21160;&#25688;&#35201;&#35780;&#20272;&#30340;&#27604;&#36739;&#30740;&#31350;&#21644;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;LangChain&#21644;&#28151;&#21512;&#31639;&#27861;&#23545;PDF&#25991;&#26723;&#36827;&#34892;&#25688;&#35201;&#21644;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#65292;&#20197;&#30830;&#23450;&#29992;&#25143;&#23545;&#25688;&#35201;&#20869;&#23481;&#30340;&#29702;&#35299;&#31243;&#24230;&#22914;&#20309;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#25991;&#31456;&#35780;&#20998;&#65288;AES&#65289;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#23574;&#31471;&#25216;&#26415;&#65292;&#35780;&#20998;&#25216;&#26415;&#29992;&#20110;&#21508;&#31181;&#30446;&#30340;&#65292;&#21487;&#38752;&#30340;&#24471;&#20998;&#26159;&#22522;&#20110;&#26377;&#24433;&#21709;&#21147;&#30340;&#21464;&#37327;&#35745;&#31639;&#24471;&#20986;&#30340;&#65292;&#36825;&#20123;&#21464;&#37327;&#21487;&#20197;&#26681;&#25454;&#39046;&#22495;&#36890;&#36807;&#19981;&#21516;&#30340;&#26041;&#27861;&#35745;&#31639;&#20986;&#26469;&#12290;&#26412;&#30740;&#31350;&#38598;&#20013;&#20110;&#29992;&#25143;&#23545;&#32473;&#23450;&#20027;&#39064;&#30340;&#29702;&#35299;&#65292;&#20998;&#26512;&#26159;&#22522;&#20110;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20998;&#25351;&#25968;&#65292;&#29992;&#25143;&#21487;&#20197;&#27604;&#36739;&#21644;&#23545;&#27604;&#26368;&#36817;&#23398;&#20064;&#30340;&#20027;&#39064;&#30340;&#29702;&#35299;&#31243;&#24230;&#65292;&#32467;&#26524;&#21017;&#23545;&#23398;&#20064;&#20998;&#26512;&#26377;&#25152;&#36129;&#29486;&#65292;&#20197;&#25552;&#39640;&#23398;&#20064;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#30340;&#37325;&#28857;&#26159;&#23545;PDF&#25991;&#26723;&#36827;&#34892;&#25688;&#35201;&#21644;&#34913;&#37327;&#29992;&#25143;&#23545;&#20854;&#20869;&#23481;&#30340;&#29702;&#35299;&#31243;&#24230;&#12290;&#35813;&#36807;&#31243;&#28041;&#21450;&#20351;&#29992;LangChain&#24037;&#20855;&#23545;PDF&#36827;&#34892;&#25688;&#35201;&#21644;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#65292;&#36890;&#36807;&#37319;&#29992;&#36825;&#31181;&#25216;&#26415;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#30830;&#23450;&#29992;&#25143;&#23545;&#25688;&#35201;&#20869;&#23481;&#30340;&#29702;&#35299;&#31243;&#24230;&#22914;&#20309;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated Essay Score (AES) is proven to be one of the cutting-edge technologies. Scoring techniques are used for various purposes. Reliable scores are calculated based on influential variables. Such variables can be computed by different methods based on the domain. The research is concentrated on the user's understanding of a given topic. The analysis is based on a scoring index by using Large Language Models. The user can then compare and contrast the understanding of a topic that they recently learned. The results are then contributed towards learning analytics and progression is made for enhancing the learning ability. In this research, the focus is on summarizing a PDF document and gauging a user's understanding of its content. The process involves utilizing a Langchain tool to summarize the PDF and extract the essential information. By employing this technique, the research aims to determine how well the user comprehends the summarized content.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#21442;&#32771;&#35780;&#20272;&#25991;&#26412;&#29702;&#35299;&#22256;&#38590;&#24230;&#30340;&#31616;&#21333;&#26041;&#27861;LC-Score&#65292;&#21487;&#20197;&#29992;&#20110;&#27861;&#35821;&#25991;&#26412;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#39044;&#27979;&#32473;&#23450;&#25991;&#26412;&#22312;0&#21040;100&#33539;&#22260;&#20869;&#30340;&#26131;&#35835;&#31243;&#24230;&#12290;&#36825;&#20010;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#20316;&#23478;&#20204;&#21019;&#20316;&#26131;&#20110;&#29702;&#35299;&#30340;&#20869;&#23481;&#65292;&#24182;&#25552;&#20379;&#24230;&#37327;&#26631;&#20934;&#26469;&#35780;&#20272;&#33258;&#21160;&#21270;&#25991;&#26412;&#31616;&#21270;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2310.02754</link><description>&lt;p&gt;
LC-Score&#65306;&#26080;&#21442;&#32771;&#35780;&#20272;&#25991;&#26412;&#29702;&#35299;&#22256;&#38590;&#24230;
&lt;/p&gt;
&lt;p&gt;
LC-Score: Reference-less estimation of Text Comprehension Difficulty. (arXiv:2310.02754v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02754
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#21442;&#32771;&#35780;&#20272;&#25991;&#26412;&#29702;&#35299;&#22256;&#38590;&#24230;&#30340;&#31616;&#21333;&#26041;&#27861;LC-Score&#65292;&#21487;&#20197;&#29992;&#20110;&#27861;&#35821;&#25991;&#26412;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#39044;&#27979;&#32473;&#23450;&#25991;&#26412;&#22312;0&#21040;100&#33539;&#22260;&#20869;&#30340;&#26131;&#35835;&#31243;&#24230;&#12290;&#36825;&#20010;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#20316;&#23478;&#20204;&#21019;&#20316;&#26131;&#20110;&#29702;&#35299;&#30340;&#20869;&#23481;&#65292;&#24182;&#25552;&#20379;&#24230;&#37327;&#26631;&#20934;&#26469;&#35780;&#20272;&#33258;&#21160;&#21270;&#25991;&#26412;&#31616;&#21270;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#26102;&#20195;&#65292;&#33021;&#22815;&#38405;&#35835;&#21644;&#29702;&#35299;&#20070;&#38754;&#25991;&#26412;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#37096;&#20998;&#20154;&#21475;&#37117;&#23384;&#22312;&#29702;&#35299;&#38556;&#30861;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#26080;&#38556;&#30861;&#20513;&#35758;&#26469;&#25552;&#39640;&#21463;&#20247;&#30340;&#25991;&#26412;&#29702;&#35299;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20316;&#23478;&#20204;&#20960;&#20046;&#27809;&#26377;&#24471;&#21040;&#25903;&#25345;&#25110;&#40723;&#21169;&#20197;&#21019;&#20316;&#26131;&#20110;&#29702;&#35299;&#30340;&#20869;&#23481;&#12290;&#27492;&#22806;&#65292;&#33258;&#21160;&#25991;&#26412;&#31616;&#21270;&#65288;ATS&#65289;&#27169;&#22411;&#30340;&#24320;&#21457;&#32570;&#20047;&#20934;&#30830;&#20272;&#35745;&#29702;&#35299;&#38590;&#24230;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;\textsc{LC-Score}&#65292;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#35757;&#32451;&#25991;&#26412;&#29702;&#35299;&#24230;&#37327;&#26631;&#20934;&#65292;&#29992;&#20110;&#27861;&#35821;&#25991;&#26412;&#32780;&#26080;&#38656;&#21442;&#32771;&#65292;&#21363;&#39044;&#27979;&#32473;&#23450;&#25991;&#26412;&#22312;$[0, 100]$&#33539;&#22260;&#20869;&#30340;&#26131;&#35835;&#31243;&#24230;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23450;&#37327;&#25429;&#25417;&#25991;&#26412;&#31526;&#21512;\textit{Langage Clair}&#65288;LC&#65292;&#21363;\textit{Clear Language}&#65289;&#25351;&#21335;&#30340;&#31243;&#24230;&#65292;&#35813;&#25351;&#21335;&#19982;&#33521;&#35821;&#31616;&#26126;&#35821;&#35328;&#23494;&#20999;&#30456;&#20851;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#20004;&#31181;&#26041;&#27861;&#65306;&#65288;i&#65289;&#20351;&#29992;&#35821;&#35328;&#23398;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Being able to read and understand written text is critical in a digital era. However, studies shows that a large fraction of the population experiences comprehension issues. In this context, further initiatives in accessibility are required to improve the audience text comprehension. However, writers are hardly assisted nor encouraged to produce easy-to-understand content. Moreover, Automatic Text Simplification (ATS) model development suffers from the lack of metric to accurately estimate comprehension difficulty We present \textsc{LC-Score}, a simple approach for training text comprehension metric for any French text without reference \ie predicting how easy to understand a given text is on a $[0, 100]$ scale. Our objective with this scale is to quantitatively capture the extend to which a text suits to the \textit{Langage Clair} (LC, \textit{Clear Language}) guidelines, a French initiative closely related to English Plain Language. We explore two approaches: (i) using linguistically
&lt;/p&gt;</description></item><item><title>AGIR&#26159;&#19968;&#31181;&#19987;&#38376;&#35299;&#20915;&#32593;&#32476;&#23041;&#32961;&#24773;&#25253;&#25253;&#21578;&#39046;&#22495;&#25361;&#25112;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#24037;&#20855;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#29983;&#25104;&#20840;&#38754;&#30340;&#24773;&#25253;&#25253;&#21578;&#65292;&#24110;&#21161;&#23433;&#20840;&#20998;&#26512;&#24072;&#20943;&#23569;&#32321;&#37325;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.02655</link><description>&lt;p&gt;
AGIR: &#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#25216;&#26415;&#22312;&#33258;&#21160;&#21270;&#32593;&#32476;&#23041;&#32961;&#24773;&#25253;&#25253;&#21578;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
AGIR: Automating Cyber Threat Intelligence Reporting with Natural Language Generation. (arXiv:2310.02655v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02655
&lt;/p&gt;
&lt;p&gt;
AGIR&#26159;&#19968;&#31181;&#19987;&#38376;&#35299;&#20915;&#32593;&#32476;&#23041;&#32961;&#24773;&#25253;&#25253;&#21578;&#39046;&#22495;&#25361;&#25112;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#24037;&#20855;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#29983;&#25104;&#20840;&#38754;&#30340;&#24773;&#25253;&#25253;&#21578;&#65292;&#24110;&#21161;&#23433;&#20840;&#20998;&#26512;&#24072;&#20943;&#23569;&#32321;&#37325;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#23041;&#32961;&#24773;&#25253;&#65288;CTI&#65289;&#25253;&#21578;&#22312;&#24403;&#20170;&#39118;&#38505;&#31649;&#29702;&#31574;&#30053;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#38543;&#30528;CTI&#25253;&#21578;&#25968;&#37327;&#30340;&#19981;&#26029;&#22686;&#21152;&#65292;&#38656;&#35201;&#33258;&#21160;&#21270;&#24037;&#20855;&#26469;&#31616;&#21270;&#25253;&#21578;&#29983;&#25104;&#30340;&#38656;&#27714;&#26085;&#30410;&#26126;&#26174;&#12290;&#23613;&#31649;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#22312;&#22788;&#29702;&#25991;&#26412;&#25968;&#25454;&#26041;&#38754;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#38590;&#20197;&#24212;&#23545;&#22810;&#26679;&#21270;&#25968;&#25454;&#28304;&#21450;&#20854;&#38169;&#32508;&#22797;&#26434;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#20687;STIX&#36825;&#26679;&#30340;&#24050;&#32463;&#25104;&#20026;CTI&#31038;&#21306;&#30340;&#20107;&#23454;&#26631;&#20934;&#30340;&#27169;&#24335;&#65292;&#24378;&#35843;&#23545;&#23454;&#20307;&#21644;&#20851;&#31995;&#36827;&#34892;&#24418;&#24335;&#21270;&#20998;&#31867;&#20197;&#20419;&#36827;&#19968;&#33268;&#30340;&#25968;&#25454;&#20849;&#20139;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AGIR&#65288;Automatic Generation of Intelligence Reports&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#38376;&#35299;&#20915;CTI&#25253;&#21578;&#39046;&#22495;&#32039;&#36843;&#25361;&#25112;&#30340;&#36716;&#21464;&#24615;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#24037;&#20855;&#12290;AGIR&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#36890;&#36807;&#33258;&#21160;&#21270;&#29983;&#25104;&#20840;&#38754;&#30340;&#24773;&#25253;&#25253;&#21578;&#65292;&#20026;&#23433;&#20840;&#20998;&#26512;&#24072;&#25552;&#20379;&#24110;&#21161;&#65292;&#20943;&#23569;&#32321;&#37325;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cyber Threat Intelligence (CTI) reporting is pivotal in contemporary risk management strategies. As the volume of CTI reports continues to surge, the demand for automated tools to streamline report generation becomes increasingly apparent. While Natural Language Processing techniques have shown potential in handling text data, they often struggle to address the complexity of diverse data sources and their intricate interrelationships. Moreover, established paradigms like STIX have emerged as de facto standards within the CTI community, emphasizing the formal categorization of entities and relations to facilitate consistent data sharing. In this paper, we introduce AGIR (Automatic Generation of Intelligence Reports), a transformative Natural Language Generation tool specifically designed to address the pressing challenges in the realm of CTI reporting. AGIR's primary objective is to empower security analysts by automating the labor-intensive task of generating comprehensive intelligence
&lt;/p&gt;</description></item><item><title>I$^2$KD-SLU&#26159;&#19968;&#31181;&#20869;&#22806;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#21475;&#35821;&#29702;&#35299;&#20013;&#24847;&#22270;&#21644;&#35789;&#27133;&#20043;&#38388;&#30340;&#30456;&#20114;&#25351;&#23548;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.02594</link><description>&lt;p&gt;
I$^2$KD-SLU: &#19968;&#31181;&#38754;&#21521;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#21475;&#35821;&#29702;&#35299;&#30340;&#20869;&#22806;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
I$^2$KD-SLU: An Intra-Inter Knowledge Distillation Framework for Zero-Shot Cross-Lingual Spoken Language Understanding. (arXiv:2310.02594v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02594
&lt;/p&gt;
&lt;p&gt;
I$^2$KD-SLU&#26159;&#19968;&#31181;&#20869;&#22806;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#21475;&#35821;&#29702;&#35299;&#20013;&#24847;&#22270;&#21644;&#35789;&#27133;&#20043;&#38388;&#30340;&#30456;&#20114;&#25351;&#23548;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21475;&#35821;&#29702;&#35299;&#36890;&#24120;&#21253;&#25324;&#20004;&#20010;&#23376;&#20219;&#21153;&#65306;&#24847;&#22270;&#26816;&#27979;&#21644;&#35789;&#27133;&#22635;&#20805;&#12290;&#30446;&#21069;&#65292;&#22312;&#39640;&#36164;&#28304;&#35821;&#35328;&#20013;&#24050;&#32463;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#65292;&#21407;&#22240;&#26159;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#21475;&#35821;&#29702;&#35299;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#21475;&#35821;&#29702;&#35299;&#27169;&#22411;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22823;&#22810;&#25968;&#24573;&#30053;&#20102;&#24847;&#22270;&#21644;&#35789;&#27133;&#20043;&#38388;&#30340;&#30456;&#20114;&#25351;&#23548;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#22806;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#29992;&#20110;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#21475;&#35821;&#29702;&#35299;&#65288;I$^2$KD-SLU&#65289;&#65292;&#20197;&#24314;&#27169;&#30456;&#20114;&#25351;&#23548;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#19981;&#20165;&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#30340;&#30456;&#21516;&#35805;&#35821;&#30340;&#24847;&#22270;&#39044;&#27979;&#25110;&#35789;&#27133;&#39044;&#27979;&#20043;&#38388;&#24212;&#29992;&#20869;&#37096;&#30693;&#35782;&#33976;&#39311;&#65292;&#32780;&#19988;&#36824;&#22312;&#30456;&#21516;&#35805;&#35821;&#30340;&#24847;&#22270;&#39044;&#27979;&#21644;&#35789;&#27133;&#39044;&#27979;&#20043;&#38388;&#24212;&#29992;&#38388;&#25509;&#30693;&#35782;&#33976;&#39311;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#25552;&#35758;&#21487;&#20197;&#26377;&#25928;&#22320;&#24314;&#27169;&#30456;&#20114;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spoken language understanding (SLU) typically includes two subtasks: intent detection and slot filling. Currently, it has achieved great success in high-resource languages, but it still remains challenging in low-resource languages due to the scarcity of labeled training data. Hence, there is a growing interest in zero-shot cross-lingual SLU. Despite of the success of existing zero-shot cross-lingual SLU models, most of them neglect to achieve the mutual guidance between intent and slots. To address this issue, we propose an Intra-Inter Knowledge Distillation framework for zero-shot cross-lingual Spoken Language Understanding (I$^2$KD-SLU) to model the mutual guidance. Specifically, we not only apply intra-knowledge distillation between intent predictions or slot predictions of the same utterance in different languages, but also apply inter-knowledge distillation between intent predictions and slot predictions of the same utterance. Our experimental results demonstrate that our propose
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#33258;&#21160;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#23558;VQA&#35780;&#20272;&#26684;&#24335;&#21270;&#20026;&#22238;&#31572;&#35780;&#20998;&#20219;&#21153;&#65292;&#36890;&#36807;&#25351;&#20196;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20934;&#30830;&#24230;&#19978;&#35780;&#20998;&#20505;&#36873;&#31572;&#26696;&#65292;&#35777;&#26126;&#35813;&#26041;&#27861;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#24615;&#20248;&#20110;&#29616;&#26377;&#24230;&#37327;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.02567</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#33258;&#21160;VQA&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Improving Automatic VQA Evaluation Using Large Language Models. (arXiv:2310.02567v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02567
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#33258;&#21160;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#23558;VQA&#35780;&#20272;&#26684;&#24335;&#21270;&#20026;&#22238;&#31572;&#35780;&#20998;&#20219;&#21153;&#65292;&#36890;&#36807;&#25351;&#20196;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20934;&#30830;&#24230;&#19978;&#35780;&#20998;&#20505;&#36873;&#31572;&#26696;&#65292;&#35777;&#26126;&#35813;&#26041;&#27861;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#24615;&#20248;&#20110;&#29616;&#26377;&#24230;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25552;&#20986;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#20219;&#21153;8&#24180;&#21518;&#65292;&#20934;&#30830;&#29575;&#20173;&#28982;&#26159;&#33258;&#21160;&#35780;&#20272;&#30340;&#20027;&#35201;&#25351;&#26631;&#12290;&#22312;IID&#35780;&#20272;&#35774;&#32622;&#20013;&#65292;VQA&#20934;&#30830;&#24230;&#19968;&#30452;&#24456;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#31038;&#21306;&#27491;&#22312;&#36716;&#21521;&#24320;&#25918;&#24335;&#29983;&#25104;&#27169;&#22411;&#21644;OOD&#35780;&#20272;&#12290;&#22312;&#36825;&#31181;&#26032;&#30340;&#33539;&#24335;&#20013;&#65292;&#29616;&#26377;&#30340;VQA&#20934;&#30830;&#24230;&#25351;&#26631;&#36807;&#20110;&#20005;&#26684;&#65292;&#20302;&#20272;&#20102;VQA&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#24320;&#21457;&#26356;&#24378;&#22823;&#30340;&#33258;&#21160;VQA&#24230;&#37327;&#65292;&#20316;&#20026;&#20154;&#31867;&#21028;&#26029;&#30340;&#20195;&#29702;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#25351;&#20196;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#26469;&#26500;&#24314;&#26356;&#22909;&#30340;VQA&#24230;&#37327;&#12290;&#25105;&#20204;&#23558;VQA&#35780;&#20272;&#26684;&#24335;&#21270;&#20026;&#19968;&#20010;&#22238;&#31572;&#35780;&#20998;&#20219;&#21153;&#65292;&#21363;&#25351;&#20196;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#25351;&#31034;&#26681;&#25454;&#19968;&#32452;&#21442;&#32771;&#31572;&#26696;&#35780;&#20998;&#20505;&#36873;&#31572;&#26696;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#25152;&#25552;&#20986;&#30340;&#24230;&#37327;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#24615;&#20248;&#20110;&#29616;&#26377;&#24230;&#37327;&#22312;&#20960;&#20010;VQA&#27169;&#22411;&#21644;&#22522;&#20934;&#27979;&#35797;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
8 years after the visual question answering (VQA) task was proposed, accuracy remains the primary metric for automatic evaluation. VQA Accuracy has been effective so far in the IID evaluation setting. However, our community is undergoing a shift towards open-ended generative models and OOD evaluation. In this new paradigm, the existing VQA Accuracy metric is overly stringent and underestimates the performance of VQA systems. Thus, there is a need to develop more robust automatic VQA metrics that serve as a proxy for human judgment. In this work, we propose to leverage the in-context learning capabilities of instruction-tuned large language models (LLMs) to build a better VQA metric. We formulate VQA evaluation as an answer-rating task where the LLM is instructed to score the accuracy of a candidate answer given a set of reference answers. We demonstrate the proposed metric better correlates with human judgment compared to existing metrics across several VQA models and benchmarks. We ho
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;NOLA&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#32593;&#32476;&#34920;&#31034;&#20026;&#20302;&#31209;&#38543;&#26426;&#22522;&#21521;&#37327;&#30340;&#32447;&#24615;&#32452;&#21512;&#26469;&#20943;&#23569;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#25968;&#37327;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#30340;&#36866;&#24212;&#21644;&#23384;&#20648;&#12290;</title><link>http://arxiv.org/abs/2310.02556</link><description>&lt;p&gt;
NOLA: &#32593;&#32476;&#20316;&#20026;&#20302;&#31209;&#38543;&#26426;&#22522;&#21521;&#37327;&#30340;&#32447;&#24615;&#32452;&#21512;
&lt;/p&gt;
&lt;p&gt;
NOLA: Networks as Linear Combination of Low Rank Random Basis. (arXiv:2310.02556v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02556
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;NOLA&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#32593;&#32476;&#34920;&#31034;&#20026;&#20302;&#31209;&#38543;&#26426;&#22522;&#21521;&#37327;&#30340;&#32447;&#24615;&#32452;&#21512;&#26469;&#20943;&#23569;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#25968;&#37327;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#30340;&#36866;&#24212;&#21644;&#23384;&#20648;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#30340;&#24778;&#20154;&#23569;&#26679;&#26412;&#24615;&#33021;&#65292;&#23427;&#20204;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26816;&#26597;&#28857;&#30340;&#24222;&#22823;&#22823;&#23567;&#65288;&#20363;&#22914;GPT-3&#30340;350GB&#65289;&#65292;&#23545;&#25152;&#26377;&#21442;&#25968;&#36827;&#34892;&#24494;&#35843;&#24182;&#20026;&#27599;&#20010;&#19979;&#28216;&#20219;&#21153;&#25110;&#39046;&#22495;&#23384;&#20648;&#19968;&#20010;&#21807;&#19968;&#27169;&#22411;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#12290;&#24403;&#21069;&#30340;&#25991;&#29486;&#65292;&#20363;&#22914;LoRA&#65292;&#23637;&#31034;&#20102;&#23545;LLM&#30340;&#21407;&#22987;&#26435;&#37325;&#36827;&#34892;&#20302;&#31209;&#20462;&#25913;&#30340;&#28508;&#21147;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#30340;&#27169;&#22411;&#30340;&#39640;&#25928;&#36866;&#24212;&#21644;&#23384;&#20648;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#23558;&#24494;&#35843;LLM&#25152;&#38656;&#30340;&#21442;&#25968;&#25968;&#37327;&#20943;&#23569;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#38480;&#21046;&#65306;1&#65289;&#21442;&#25968;&#20943;&#23569;&#21463;&#21040;&#31209;&#19968;&#20998;&#35299;&#30340;&#19979;&#30028;&#38480;&#21046;&#65292;2&#65289;&#20943;&#23569;&#30340;&#31243;&#24230;&#21463;&#21040;&#27169;&#22411;&#26550;&#26500;&#21644;&#36873;&#25321;&#30340;&#31209;&#30340;&#20005;&#37325;&#24433;&#21709;&#12290;&#20363;&#22914;&#65292;&#22312;&#26356;&#22823;&#27169;&#22411;&#20013;&#65292;&#21363;&#20351;&#26159;&#31209;&#19968;&#20998;&#35299;&#65292;&#21442;&#25968;&#30340;&#25968;&#37327;&#20063;&#21487;&#33021;&#36229;&#36807;&#30495;&#27491;&#38656;&#35201;&#36827;&#34892;&#36866;&#24212;&#30340;&#21442;&#25968;&#25968;&#37327;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;NOLA&#65292;&#23427;&#36890;&#36807;&#23558;&#32593;&#32476;&#34920;&#31034;&#20026;&#20302;&#31209;&#38543;&#26426;&#22522;&#21521;&#37327;&#30340;&#32447;&#24615;&#32452;&#21512;&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have recently gained popularity due to their impressive few-shot performance across various downstream tasks. However, fine-tuning all parameters and storing a unique model for each downstream task or domain becomes impractical because of the massive size of checkpoints (e.g., 350GB in GPT-3). Current literature, such as LoRA, showcases the potential of low-rank modifications to the original weights of an LLM, enabling efficient adaptation and storage for task-specific models. These methods can reduce the number of parameters needed to fine-tune an LLM by several orders of magnitude. Yet, these methods face two primary limitations: 1) the parameter reduction is lower-bounded by the rank one decomposition, and 2) the extent of reduction is heavily influenced by both the model architecture and the chosen rank. For instance, in larger models, even a rank one decomposition might exceed the number of parameters truly needed for adaptation. In this paper, we intr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25351;&#23548;&#25945;&#24072;&#26469;&#35757;&#32451;&#23398;&#29983;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#35838;&#31243;&#26469;&#36827;&#34892;&#25351;&#23548;&#35843;&#20248;&#65292;&#31216;&#20026;CITING&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25945;&#24072;&#27169;&#22411;&#21046;&#23450;&#35780;&#20998;&#26631;&#20934;&#65292;&#23398;&#29983;&#27169;&#22411;&#36890;&#36807;&#36981;&#24490;&#35780;&#20998;&#26631;&#20934;&#21644;&#33258;&#25105;&#32416;&#27491;&#36827;&#34892;&#23398;&#20064;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#25163;&#24037;&#21046;&#20316;&#25351;&#23548;&#25968;&#25454;&#38598;&#21644;&#20154;&#24037;&#23545;&#40784;&#30340;&#29942;&#39048;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.02527</link><description>&lt;p&gt;
CITING: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#35838;&#31243;&#35774;&#35745;&#36827;&#34892;&#25351;&#23548;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
CITING: Large Language Models Create Curriculum for Instruction Tuning. (arXiv:2310.02527v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02527
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25351;&#23548;&#25945;&#24072;&#26469;&#35757;&#32451;&#23398;&#29983;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#35838;&#31243;&#26469;&#36827;&#34892;&#25351;&#23548;&#35843;&#20248;&#65292;&#31216;&#20026;CITING&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25945;&#24072;&#27169;&#22411;&#21046;&#23450;&#35780;&#20998;&#26631;&#20934;&#65292;&#23398;&#29983;&#27169;&#22411;&#36890;&#36807;&#36981;&#24490;&#35780;&#20998;&#26631;&#20934;&#21644;&#33258;&#25105;&#32416;&#27491;&#36827;&#34892;&#23398;&#20064;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#25163;&#24037;&#21046;&#20316;&#25351;&#23548;&#25968;&#25454;&#38598;&#21644;&#20154;&#24037;&#23545;&#40784;&#30340;&#29942;&#39048;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#26159;&#36890;&#36807;&#25351;&#23548;&#35843;&#20248;&#21644;&#20154;&#24037;&#23545;&#40784;&#30340;&#32467;&#21512;&#23454;&#29616;&#30340;&#12290;&#28982;&#32780;&#65292;&#26500;&#24314;&#25163;&#24037;&#21046;&#20316;&#30340;&#25351;&#23548;&#25968;&#25454;&#38598;&#21644;&#36827;&#34892;&#20154;&#24037;&#23545;&#40784;&#25104;&#20026;&#20102;&#25193;&#23637;LLMs&#24320;&#21457;&#30340;&#29942;&#39048;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;AI&#27169;&#22411;&#20195;&#26367;&#20154;&#31867;&#20316;&#20026;&#25945;&#24072;&#26469;&#35757;&#32451;&#23398;&#29983;LLMs&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28789;&#24863;&#26469;&#33258;&#20110;&#20154;&#31867;&#23398;&#29983;&#36890;&#36807;&#36981;&#24490;&#35780;&#20998;&#26631;&#20934;&#21644;&#20174;&#23548;&#24072;&#25552;&#20379;&#30340;&#20462;&#25913;&#20013;&#23398;&#20064;&#26469;&#25552;&#39640;&#20889;&#20316;&#25216;&#24039;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;&#25945;&#24072;LLM&#26469;&#20026;&#23398;&#29983;LLM&#30340;&#25351;&#23548;&#35843;&#20248;&#21019;&#24314;&#35838;&#31243;&#65292;&#21363;Curriculum Instruction TunING (CITING)&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#27493;&#39588;&#65306;&#65288;1&#65289;&#25945;&#24072;LLM&#21046;&#23450;&#35780;&#20272;&#21508;&#31181;&#31867;&#22411;&#38382;&#39064;&#31572;&#26696;&#30340;&#35780;&#20998;&#26631;&#20934;&#65307;&#65288;2&#65289;&#23398;&#29983;LLM&#23398;&#20064;&#36981;&#24490;&#35780;&#20998;&#26631;&#20934;&#24182;&#36890;&#36807;&#25945;&#24072;&#30340;&#20462;&#25913;&#36827;&#34892;&#33258;&#25105;&#32416;&#27491;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36845;&#20195;&#36827;&#34892;&#36825;&#20010;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advancement of large language models (LLMs) has been achieved through a combo of instruction tuning and human alignment. However, building manually crafted instruction datasets and performing human alignment become the bottleneck for scaling the development of LLMs. In this paper, we exploit the idea of leveraging AI models in lieu of humans as the teacher to train student LLMs. Our method is inspired by how human students refine their writing skills by following the rubrics and learning from the revisions offered by their tutors. Specifically, we employ a teacher LLM to create a curriculum for instruction tuning of the student LLM, namely Curriculum Instruction TunING (CITING). It encompasses two main steps: (1) the teacher LLM crafts the rubrics for evaluating the answers corresponding to various types of questions, and (2) the student LLM learns to follow the rubrics and perform self-correction from the revision made by the teacher. We further iteratively carry out it to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ResidualTransformer&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;Transformer&#32534;&#30721;&#22120;&#23618;&#20043;&#38388;&#30340;&#27169;&#22411;&#26435;&#37325;&#65292;&#23558;&#27169;&#22411;&#30340;&#22823;&#23567;&#20943;&#23567;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ResidualTransformer&#30340;&#24615;&#33021;&#20248;&#20110;&#20256;&#32479;Transformer&#27169;&#22411;&#65292;&#19988;&#27169;&#22411;&#22823;&#23567;&#24471;&#21040;&#20102;&#26174;&#33879;&#20943;&#23567;&#12290;</title><link>http://arxiv.org/abs/2310.02489</link><description>&lt;p&gt;
ResidualTransformer&#65306;&#24102;&#26377;&#26435;&#37325;&#20849;&#20139;&#30340;&#27531;&#24046;&#20302;&#31209;&#23398;&#20064;&#30340;Transformer&#23618;
&lt;/p&gt;
&lt;p&gt;
ResidualTransformer: Residual Low-rank Learning with Weight-sharing for Transformer Layers. (arXiv:2310.02489v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02489
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ResidualTransformer&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;Transformer&#32534;&#30721;&#22120;&#23618;&#20043;&#38388;&#30340;&#27169;&#22411;&#26435;&#37325;&#65292;&#23558;&#27169;&#22411;&#30340;&#22823;&#23567;&#20943;&#23567;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ResidualTransformer&#30340;&#24615;&#33021;&#20248;&#20110;&#20256;&#32479;Transformer&#27169;&#22411;&#65292;&#19988;&#27169;&#22411;&#22823;&#23567;&#24471;&#21040;&#20102;&#26174;&#33879;&#20943;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37096;&#32626;&#35821;&#38899;&#22788;&#29702;&#27169;&#22411;&#21040;&#22987;&#32456;&#24320;&#21551;&#35774;&#22791;&#19978;&#26102;&#65292;&#20869;&#23384;&#38480;&#21046;&#26159;&#19968;&#20010;&#20027;&#35201;&#20851;&#27880;&#28857;&#20043;&#19968;&#12290;&#34429;&#28982;&#20351;&#29992;&#36275;&#22815;&#22823;&#37327;&#30340;&#25968;&#25454;&#35757;&#32451;&#24471;&#21040;&#30340;&#26356;&#22823;&#30340;&#27169;&#22411;&#36890;&#24120;&#34920;&#29616;&#26356;&#22909;&#65292;&#20294;&#20351;&#20854;&#36866;&#24212;&#35774;&#22791;&#20869;&#23384;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;Transformer&#32534;&#30721;&#22120;&#23618;&#20043;&#38388;&#30340;&#27169;&#22411;&#26435;&#37325;&#65292;&#24182;&#20551;&#35774;&#29305;&#27530;&#30340;&#26435;&#37325;&#32452;&#21512;&#21644;&#32467;&#26500;&#65292;&#26469;&#20943;&#23567;&#27169;&#22411;&#30340;&#22823;&#23567;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#21463;ResNet&#21644;&#26368;&#26032;&#30340;LoRA&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ResidualTransformer&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;Transformer&#23618;&#20013;&#30340;&#27599;&#20010;&#26435;&#37325;&#30697;&#38453;&#21253;&#25324;1&#65289;&#19982;&#20854;&#30456;&#37051;&#23618;&#20849;&#20139;&#30340;&#28385;&#31209;&#32452;&#20214;&#65292;&#21644;2&#65289;&#20165;&#23646;&#20110;&#23427;&#33258;&#24049;&#30340;&#29420;&#29305;&#20302;&#31209;&#32452;&#20214;&#12290;&#20302;&#31209;&#30697;&#38453;&#21482;&#21344;&#27169;&#22411;&#22823;&#23567;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#28155;&#21152;&#23545;&#35282;&#32447;&#26435;&#37325;&#30697;&#38453;&#26469;&#25552;&#39640;&#20302;&#31209;&#30697;&#38453;&#30340;&#24314;&#27169;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;10k&#23567;&#26102;&#35821;&#38899;&#35782;&#21035;&#21644;&#35821;&#38899;&#32763;&#35793;&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ResidualTransformer&#30340;&#24615;&#33021;&#20248;&#20110;&#20256;&#32479;Transformer&#27169;&#22411;&#65292;&#19988;&#27169;&#22411;&#22823;&#23567;&#24471;&#21040;&#20102;&#26174;&#33879;&#20943;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Memory constraint of always-on devices is one of the major concerns when deploying speech processing models on these devices. While larger models trained with sufficiently large amount of data generally perform better, making them fit in the device memory is a demanding challenge. In this paper, we aim to reduce model size by reparameterizing model weights across Transformer encoder layers and assuming a special weight composition and structure. More specifically, inspired by ResNet and the more recent LoRA work, we propose an approach named ResidualTransformer, where each weight matrix in a Transformer layer comprises 1) a shared full-rank component with its adjacent layers, and 2) a unique low-rank component to itself. The low-rank matrices only account for a small amount of model size increase. In addition, we add diagonal weight matrices to improve modeling capacity of the low-rank matrices. Experiments of our 10k-hour speech recognition and speech translation tasks show that the T
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#38544;&#31169;&#20445;&#25252;&#35821;&#35328;&#27169;&#22411;&#65288;PPLM&#65289;&#30340;&#26032;&#33539;&#24335;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#26377;&#25928;&#27880;&#20837;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#12290;&#36890;&#36807;&#23545;&#27169;&#22411;&#35774;&#35745;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#19981;&#21516;&#25216;&#26415;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#20351;&#29992;&#27491;&#21521;&#21644;&#36127;&#21521;&#31034;&#20363;&#36827;&#34892;&#25351;&#20196;&#24494;&#35843;&#30340;&#26041;&#27861;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.02469</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#25104;&#20026;&#33391;&#22909;&#30340;&#38544;&#31169;&#20445;&#25252;&#23398;&#20064;&#32773;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Can Be Good Privacy Protection Learners. (arXiv:2310.02469v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#38544;&#31169;&#20445;&#25252;&#35821;&#35328;&#27169;&#22411;&#65288;PPLM&#65289;&#30340;&#26032;&#33539;&#24335;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#26377;&#25928;&#27880;&#20837;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#12290;&#36890;&#36807;&#23545;&#27169;&#22411;&#35774;&#35745;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#19981;&#21516;&#25216;&#26415;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#20351;&#29992;&#27491;&#21521;&#21644;&#36127;&#21521;&#31034;&#20363;&#36827;&#34892;&#25351;&#20196;&#24494;&#35843;&#30340;&#26041;&#27861;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26222;&#21450;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#20351;&#29992;&#29305;&#23450;&#39046;&#22495;&#25968;&#25454;&#23545;&#20854;&#36827;&#34892;&#24494;&#35843;&#65292;&#21019;&#24314;&#19987;&#38376;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#29305;&#23450;&#39046;&#22495;&#30340;&#24494;&#35843;&#25968;&#25454;&#36890;&#24120;&#21253;&#21547;&#25935;&#24863;&#30340;&#20010;&#20154;&#36523;&#20221;&#20449;&#24687;&#65288;PII&#65289;&#12290;&#22312;&#27809;&#26377;&#38544;&#31169;&#20445;&#25252;&#30340;&#24773;&#20917;&#19979;&#30452;&#25509;&#24494;&#35843; LLMs &#20250;&#23384;&#22312;&#20449;&#24687;&#27844;&#38706;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#38544;&#31169;&#20445;&#25252;&#35821;&#35328;&#27169;&#22411;&#65288;PPLM&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#26377;&#25928;&#27880;&#20837;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#26032;&#33539;&#24335;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#27169;&#22411;&#35774;&#35745;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#28145;&#20837;&#30740;&#31350;&#20102;&#21508;&#31181;&#25216;&#26415;&#65292;&#27604;&#22914;&#35821;&#26009;&#24211;&#31574;&#23637;&#12289;&#22522;&#20110;&#24809;&#32602;&#30340;&#38750;&#27010;&#28982;&#24615;&#35757;&#32451;&#25439;&#22833;&#20197;&#21450;&#22522;&#20110;&#25351;&#20196;&#30340;&#24494;&#35843;&#31561;&#31561;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#21644;&#22330;&#26223;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#20351;&#29992;&#27491;&#21521;&#21644;&#36127;&#21521;&#31034;&#20363;&#36827;&#34892;&#25351;&#20196;&#24494;&#35843;&#65292;&#26174;&#31034;&#20986;&#24456;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of Large Language Models (LLMs) has driven considerable interest in fine-tuning them with domain-specific data to create specialized language models. Nevertheless, such domain-specific fine-tuning data often contains sensitive personally identifiable information (PII). Direct fine-tuning LLMs on this data without privacy protection poses a risk of leakage. To address this challenge, we introduce Privacy Protection Language Models (PPLM), a novel paradigm for fine-tuning LLMs that effectively injects domain-specific knowledge while safeguarding data privacy. Our work offers a theoretical analysis for model design and delves into various techniques such as corpus curation, penalty-based unlikelihood in training loss, and instruction-based tuning, etc. Extensive experiments across diverse datasets and scenarios demonstrate the effectiveness of our approaches. In particular, instruction tuning with both positive and negative examples, stands out as a promising method, eff
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#21518;&#32467;&#26500;&#20027;&#20041;&#31038;&#20250;&#25919;&#27835;&#29702;&#35770;&#30340;&#35270;&#35282;&#65292;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20013;&#30340;&#8220;&#23545;&#40784;&#8221;&#27010;&#24565;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#26126;&#30830;&#25805;&#20316;&#21270;&#36825;&#31181;&#25277;&#35937;&#27010;&#24565;&#65292;&#20419;&#36827;&#36879;&#26126;&#21644;&#25209;&#21028;&#24615;&#35780;&#20272;&#30340;&#25991;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.02457</link><description>&lt;p&gt;
&#31354;&#31526;&#21495;&#38382;&#39064;&#65306;&#26397;&#21521;&#26356;&#28165;&#26224;&#30340;&#33539;&#24335;&#26469;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#8220;&#23545;&#40784;&#8221;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
The Empty Signifier Problem: Towards Clearer Paradigms for Operationalising "Alignment" in Large Language Models. (arXiv:2310.02457v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#21518;&#32467;&#26500;&#20027;&#20041;&#31038;&#20250;&#25919;&#27835;&#29702;&#35770;&#30340;&#35270;&#35282;&#65292;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20013;&#30340;&#8220;&#23545;&#40784;&#8221;&#27010;&#24565;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#26126;&#30830;&#25805;&#20316;&#21270;&#36825;&#31181;&#25277;&#35937;&#27010;&#24565;&#65292;&#20419;&#36827;&#36879;&#26126;&#21644;&#25209;&#21028;&#24615;&#35780;&#20272;&#30340;&#25991;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#21518;&#32467;&#26500;&#20027;&#20041;&#31038;&#20250;&#25919;&#27835;&#29702;&#35770;&#30340;&#35270;&#35282;&#65292;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20013;&#30340;&#8220;&#23545;&#40784;&#8221;&#27010;&#24565;&#65292;&#24182;&#29305;&#21035;&#30740;&#31350;&#20102;&#20854;&#19982;&#31354;&#31526;&#21495;&#30340;&#30456;&#20284;&#20043;&#22788;&#12290;&#20026;&#20102;&#22312;&#32463;&#39564;&#25968;&#25454;&#38598;&#20013;&#26126;&#30830;&#25805;&#20316;&#21270;&#25277;&#35937;&#23545;&#40784;&#27010;&#24565;&#30340;&#20849;&#20139;&#35789;&#27719;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#30028;&#23450;&#20102;&#65306;1&#65289;&#21738;&#20123;&#27169;&#22411;&#34892;&#20026;&#32500;&#24230;&#34987;&#35748;&#20026;&#37325;&#35201;&#65292;&#28982;&#21518;2&#65289;&#22914;&#20309;&#36171;&#20104;&#36825;&#20123;&#32500;&#24230;&#30340;&#21547;&#20041;&#21644;&#23450;&#20041;&#65292;&#24182;&#30001;&#35841;&#30830;&#23450;&#12290;&#25105;&#20204;&#23545;&#29616;&#26377;&#30340;&#32463;&#39564;&#25991;&#29486;&#36827;&#34892;&#20102;&#23450;&#20301;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#20915;&#23450;&#35201;&#36981;&#24490;&#21738;&#31181;&#33539;&#24335;&#26041;&#38754;&#30340;&#25351;&#23548;&#12290;&#36890;&#36807;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#26088;&#22312;&#22521;&#20859;&#36879;&#26126;&#21644;&#25209;&#21028;&#24615;&#35780;&#20272;&#30340;&#25991;&#21270;&#65292;&#24110;&#21161;&#31038;&#21306;&#22312;&#22788;&#29702;&#23558;LLMs&#19982;&#20154;&#31867;&#32676;&#20307;&#23545;&#40784;&#30340;&#22797;&#26434;&#24615;&#26102;&#36827;&#34892;&#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we address the concept of "alignment" in large language models (LLMs) through the lens of post-structuralist socio-political theory, specifically examining its parallels to empty signifiers. To establish a shared vocabulary around how abstract concepts of alignment are operationalised in empirical datasets, we propose a framework that demarcates: 1) which dimensions of model behaviour are considered important, then 2) how meanings and definitions are ascribed to these dimensions, and by whom. We situate existing empirical literature and provide guidance on deciding which paradigm to follow. Through this framework, we aim to foster a culture of transparency and critical evaluation, aiding the community in navigating the complexities of aligning LLMs with human populations.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#22810;&#26426;&#26500;&#20020;&#24202;&#31508;&#35760;&#25968;&#25454;&#38598;&#20013;&#20351;&#29992;&#21518;&#38376;&#35843;&#25972;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#30340;&#25928;&#26524;&#65292;&#32467;&#26524;&#34920;&#26126;&#21518;&#38376;&#35843;&#25972;&#23545;&#35299;&#20915;&#28304;&#29305;&#23450;&#25968;&#25454;&#20998;&#24067;&#24046;&#24322;&#23548;&#33268;&#30340;&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#20855;&#26377;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.02451</link><description>&lt;p&gt;
&#36890;&#36807;&#28335;&#28304;&#23545;&#28151;&#28102;&#30340;&#21518;&#38376;&#35843;&#25972;&#20197;&#23454;&#29616;&#22810;&#26426;&#26500;&#20020;&#24202;&#31508;&#35760;&#30340;&#24378;&#20581;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Backdoor Adjustment of Confounding by Provenance for Robust Text Classification of Multi-institutional Clinical Notes. (arXiv:2310.02451v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02451
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#22810;&#26426;&#26500;&#20020;&#24202;&#31508;&#35760;&#25968;&#25454;&#38598;&#20013;&#20351;&#29992;&#21518;&#38376;&#35843;&#25972;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#30340;&#25928;&#26524;&#65292;&#32467;&#26524;&#34920;&#26126;&#21518;&#38376;&#35843;&#25972;&#23545;&#35299;&#20915;&#28304;&#29305;&#23450;&#25968;&#25454;&#20998;&#24067;&#24046;&#24322;&#23548;&#33268;&#30340;&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#20855;&#26377;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#26041;&#27861;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#20020;&#24202;&#20219;&#21153;&#12290;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24050;&#34987;&#29992;&#20110;&#25552;&#39640;&#20020;&#24202;NLP&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#36275;&#22815;&#22823;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#24050;&#32463;&#35777;&#26126;&#22312;&#19981;&#21516;&#26426;&#26500;&#20043;&#38388;&#20256;&#36755;&#27169;&#22411;&#30340;&#24615;&#33021;&#36739;&#24046;&#12290;&#36825;&#20123;&#38382;&#39064;&#23548;&#33268;&#20102;&#22312;&#19981;&#21516;&#26426;&#26500;&#20043;&#38388;&#36827;&#34892;&#25968;&#25454;&#25910;&#38598;&#21644;&#25972;&#21512;&#26469;&#24471;&#21040;&#20934;&#30830;&#21644;&#21487;&#31227;&#26893;&#30340;&#27169;&#22411;&#30340;&#25512;&#24191;&#12290;&#28982;&#32780;&#65292;&#36825;&#21487;&#33021;&#20250;&#24341;&#20837;&#19968;&#31181;&#31216;&#20026;&#28335;&#28304;&#28151;&#28102;&#30340;&#20559;&#24046;&#24418;&#24335;&#12290;&#24403;&#37096;&#32626;&#26102;&#28304;&#29305;&#23450;&#25968;&#25454;&#20998;&#24067;&#19981;&#21516;&#26102;&#65292;&#36825;&#21487;&#33021;&#20250;&#25439;&#23475;&#27169;&#22411;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22312;&#22810;&#26426;&#26500;&#20020;&#24202;&#31508;&#35760;&#25968;&#25454;&#38598;&#20013;&#20351;&#29992;&#21518;&#38376;&#35843;&#25972;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#30340;&#25928;&#29992;&#12290;&#20351;&#29992;&#35774;&#35745;&#29992;&#20110;&#34913;&#37327;&#23545;&#20998;&#24067;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#21518;&#38376;&#35843;&#25972;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21518;&#38376;&#35843;&#25972;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Natural Language Processing (NLP) methods have been broadly applied to clinical tasks. Machine learning and deep learning approaches have been used to improve the performance of clinical NLP. However, these approaches require sufficiently large datasets for training, and trained models have been shown to transfer poorly across sites. These issues have led to the promotion of data collection and integration across different institutions for accurate and portable models. However, this can introduce a form of bias called confounding by provenance. When source-specific data distributions differ at deployment, this may harm model performance. To address this issue, we evaluate the utility of backdoor adjustment for text classification in a multi-site dataset of clinical notes annotated for mentions of substance abuse. Using an evaluation framework devised to measure robustness to distributional shifts, we assess the utility of backdoor adjustment. Our results indicate that backdoor adjustme
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32763;&#35793;&#19981;&#23433;&#20840;&#30340;&#33521;&#25991;&#36755;&#20837;&#25104;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#25105;&#20204;&#25104;&#21151;&#32469;&#36807;&#20102;GPT-4&#30340;&#23433;&#20840;&#26426;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#31181;&#36328;&#35821;&#35328;&#28431;&#27934;&#12290;&#36825;&#19968;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#19982;&#29978;&#33267;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#36234;&#29425;&#25915;&#20987;&#30340;&#25928;&#26524;&#65292;&#25581;&#31034;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#22312;AI&#23433;&#20840;&#24615;&#20013;&#30340;&#34180;&#24369;&#29615;&#33410;&#12290;</title><link>http://arxiv.org/abs/2310.02446</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#35821;&#35328;&#36234;&#29425; GPT-4
&lt;/p&gt;
&lt;p&gt;
Low-Resource Languages Jailbreak GPT-4. (arXiv:2310.02446v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02446
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32763;&#35793;&#19981;&#23433;&#20840;&#30340;&#33521;&#25991;&#36755;&#20837;&#25104;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#25105;&#20204;&#25104;&#21151;&#32469;&#36807;&#20102;GPT-4&#30340;&#23433;&#20840;&#26426;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#31181;&#36328;&#35821;&#35328;&#28431;&#27934;&#12290;&#36825;&#19968;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#19982;&#29978;&#33267;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#36234;&#29425;&#25915;&#20987;&#30340;&#25928;&#26524;&#65292;&#25581;&#31034;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#22312;AI&#23433;&#20840;&#24615;&#20013;&#30340;&#34180;&#24369;&#29615;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#22521;&#35757;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#32418;&#38431;&#27979;&#35797;&#26159;&#20943;&#23569;&#29983;&#25104;&#19981;&#23433;&#20840;&#20869;&#23481;&#30340;&#25514;&#26045;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36890;&#36807;&#23558;&#19981;&#23433;&#20840;&#30340;&#33521;&#25991;&#36755;&#20837;&#32763;&#35793;&#25104;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#25104;&#21151;&#32469;&#36807;GPT-4&#30340;&#23433;&#20840;&#26426;&#21046;&#65292;&#24182;&#25581;&#31034;&#20102;&#36825;&#20123;&#23433;&#20840;&#26426;&#21046;&#30340;&#36328;&#35821;&#35328;&#28431;&#27934;&#12290;&#22312;AdvBenchmark&#20013;&#65292;GPT-4&#38024;&#23545;&#19981;&#23433;&#20840;&#30340;&#32763;&#35793;&#36755;&#20837;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#19988;79%&#30340;&#26102;&#38388;&#20869;&#25552;&#20379;&#20102;&#21487;&#34892;&#30340;&#26041;&#26696;&#65292;&#20351;&#29992;&#25143;&#23454;&#29616;&#20854;&#26377;&#23475;&#30446;&#26631;&#65292;&#36825;&#19982;&#29978;&#33267;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#36234;&#29425;&#25915;&#20987;&#30340;&#25928;&#26524;&#30456;&#24403;&#12290;&#20854;&#20182;&#39640;/&#20013;&#36164;&#28304;&#35821;&#35328;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#26174;&#33879;&#36739;&#20302;&#65292;&#36825;&#34920;&#26126;&#36328;&#35821;&#35328;&#28431;&#27934;&#20027;&#35201;&#36866;&#29992;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#20197;&#21069;&#65292;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#26377;&#38480;&#35757;&#32451;&#20027;&#35201;&#24433;&#21709;&#37027;&#20123;&#20351;&#29992;&#36825;&#20123;&#35821;&#35328;&#30340;&#20154;&#65292;&#36896;&#25104;&#25216;&#26415;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#31361;&#20986;&#20102;&#19968;&#20010;&#20851;&#38190;&#36716;&#21464;&#65306;
&lt;/p&gt;
&lt;p&gt;
AI safety training and red-teaming of large language models (LLMs) are measures to mitigate the generation of unsafe content. Our work exposes the inherent cross-lingual vulnerability of these safety mechanisms, resulting from the linguistic inequality of safety training data, by successfully circumventing GPT-4's safeguard through translating unsafe English inputs into low-resource languages. On the AdvBenchmark, GPT-4 engages with the unsafe translated inputs and provides actionable items that can get the users towards their harmful goals 79% of the time, which is on par with or even surpassing state-of-the-art jailbreaking attacks. Other high-/mid-resource languages have significantly lower attack success rate, which suggests that the cross-lingual vulnerability mainly applies to low-resource languages. Previously, limited training on low-resource languages primarily affects speakers of those languages, causing technological disparities. However, our work highlights a crucial shift:
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#23398;&#35823;&#35299;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#12290;&#19982;&#20256;&#32479;&#35780;&#20272;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#21021;&#23398;&#32773;&#21644;&#19987;&#23478;&#23548;&#24072;&#30340;&#35282;&#33394;&#26469;&#35782;&#21035;&#38169;&#35823;&#31572;&#26696;&#21644;&#30456;&#24212;&#30340;&#35823;&#35299;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#27491;&#30830;&#22238;&#31572;&#38382;&#39064;&#65292;&#20294;&#22312;&#35782;&#21035;&#35823;&#35299;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2310.02439</link><description>&lt;p&gt;
&#21021;&#23398;&#32773;&#23398;&#20064;&#32773;&#21644;&#19987;&#23478;&#23548;&#24072;&#65306;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#19978;&#30340;&#35823;&#35299;
&lt;/p&gt;
&lt;p&gt;
Novice Learner and Expert Tutor: Evaluating Math Reasoning Abilities of Large Language Models with Misconceptions. (arXiv:2310.02439v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02439
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#23398;&#35823;&#35299;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#12290;&#19982;&#20256;&#32479;&#35780;&#20272;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#21021;&#23398;&#32773;&#21644;&#19987;&#23478;&#23548;&#24072;&#30340;&#35282;&#33394;&#26469;&#35782;&#21035;&#38169;&#35823;&#31572;&#26696;&#21644;&#30456;&#24212;&#30340;&#35823;&#35299;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#27491;&#30830;&#22238;&#31572;&#38382;&#39064;&#65292;&#20294;&#22312;&#35782;&#21035;&#35823;&#35299;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#23398;&#35823;&#35299;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#26032;&#35780;&#20272;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#26041;&#27861;&#26159;&#23558;LLM&#27169;&#25311;&#25104;&#21021;&#23398;&#32773;&#23398;&#20064;&#32773;&#21644;&#19987;&#23478;&#23548;&#24072;&#65292;&#26088;&#22312;&#35782;&#21035;&#30001;&#29305;&#23450;&#35823;&#35299;&#24341;&#36215;&#30340;&#25968;&#23398;&#38382;&#39064;&#30340;&#38169;&#35823;&#31572;&#26696;&#65292;&#24182;&#35782;&#21035;&#19982;&#38169;&#35823;&#31572;&#26696;&#30456;&#20851;&#30340;&#35823;&#35299;&#12290;&#19982;&#20256;&#32479;&#22522;&#20110;LLM&#30340;&#25968;&#23398;&#35780;&#20272;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21463;&#21040;&#25945;&#32946;&#23398;&#20064;&#31185;&#23398;&#21407;&#21017;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#26126;&#30830;&#35201;&#27714;LLM&#27169;&#25311;&#21021;&#23398;&#32773;&#23398;&#20064;&#32773;&#30340;&#34892;&#20026;&#65292;&#36890;&#36807;&#22522;&#20110;&#19981;&#23436;&#20840;&#30693;&#35782;&#20197;&#29305;&#23450;&#38169;&#35823;&#30340;&#26041;&#24335;&#22238;&#31572;&#38382;&#39064;&#65307;&#20197;&#21450;&#27169;&#25311;&#19987;&#23478;&#23548;&#24072;&#30340;&#34892;&#20026;&#65292;&#35782;&#21035;&#19982;&#38382;&#39064;&#30340;&#38169;&#35823;&#31572;&#26696;&#30456;&#23545;&#24212;&#30340;&#35823;&#35299;&#12290;&#36890;&#36807;&#20351;&#29992;&#31616;&#21333;&#30340;&#23567;&#23398;&#25968;&#23398;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;LLM&#34429;&#28982;&#21487;&#20197;&#36731;&#26494;&#27491;&#30830;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#20294;&#22312;&#35782;&#21035;&#35823;&#35299;&#26041;&#38754;&#21364;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose novel evaluations for mathematical reasoning capabilities of Large Language Models (LLMs) based on mathematical misconceptions. Our primary approach is to simulate LLMs as a novice learner and an expert tutor, aiming to identify the incorrect answer to math question resulted from a specific misconception and to recognize the misconception(s) behind an incorrect answer, respectively. Contrary to traditional LLMs-based mathematical evaluations that focus on answering math questions correctly, our approach takes inspirations from principles in educational learning sciences. We explicitly ask LLMs to mimic a novice learner by answering questions in a specific incorrect manner based on incomplete knowledge; and to mimic an expert tutor by identifying misconception(s) corresponding to an incorrect answer to a question. Using simple grade-school math problems, our experiments reveal that, while LLMs can easily answer these questions correctly, they struggle to identify 1) the incor
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27979;&#37327;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21453;&#39539;&#24120;&#35265;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#35823;&#35299;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#30740;&#31350;&#23398;&#26415;&#25991;&#29486;&#65292;&#25972;&#29702;&#20102;&#20845;&#20010;&#20027;&#39064;&#30340;&#19968;&#30334;&#22810;&#20010;&#19982;&#23433;&#20840;&#21644;&#38544;&#31169;&#30456;&#20851;&#30340;&#35823;&#35299;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#26597;&#35810;&#20004;&#20010;&#27969;&#34892;&#30340;LLMs&#24182;&#21046;&#23450;&#26631;&#27880;&#25351;&#21335;&#65292;&#35780;&#20272;&#23427;&#20204;&#30340;&#22238;&#24212;&#12290;</title><link>http://arxiv.org/abs/2310.02431</link><description>&lt;p&gt;
&#33021;&#21542;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#23433;&#20840;&#21644;&#38544;&#31169;&#24314;&#35758;&#65311;&#34913;&#37327;LLMs&#21453;&#39539;&#35823;&#35299;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Provide Security &amp; Privacy Advice? Measuring the Ability of LLMs to Refute Misconceptions. (arXiv:2310.02431v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27979;&#37327;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21453;&#39539;&#24120;&#35265;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#35823;&#35299;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#30740;&#31350;&#23398;&#26415;&#25991;&#29486;&#65292;&#25972;&#29702;&#20102;&#20845;&#20010;&#20027;&#39064;&#30340;&#19968;&#30334;&#22810;&#20010;&#19982;&#23433;&#20840;&#21644;&#38544;&#31169;&#30456;&#20851;&#30340;&#35823;&#35299;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#26597;&#35810;&#20004;&#20010;&#27969;&#34892;&#30340;LLMs&#24182;&#21046;&#23450;&#26631;&#27880;&#25351;&#21335;&#65292;&#35780;&#20272;&#23427;&#20204;&#30340;&#22238;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#20174;&#22312;&#32447;&#36164;&#28304;&#20013;&#23547;&#27714;&#23433;&#20840;&#21644;&#38544;&#31169;&#24314;&#35758;&#65292;&#21253;&#25324;&#20449;&#20219;&#30340;&#32593;&#31449;&#21644;&#20869;&#23481;&#20998;&#20139;&#24179;&#21488;&#12290;&#36825;&#20123;&#36164;&#28304;&#24110;&#21161;&#29992;&#25143;&#20102;&#35299;&#23433;&#20840;&#21644;&#38544;&#31169;&#25216;&#26415;&#21644;&#24037;&#20855;&#65292;&#24182;&#25552;&#20379;&#21487;&#34892;&#30340;&#31574;&#30053;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26368;&#36817;&#20316;&#20026;&#21463;&#20449;&#20219;&#30340;&#20449;&#24687;&#26469;&#28304;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#20934;&#30830;&#24615;&#21644;&#27491;&#30830;&#24615;&#21463;&#21040;&#36136;&#30097;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#27010;&#36848;&#20102;LLMs&#22312;&#22238;&#31572;&#22810;&#39033;&#36873;&#25321;&#39064;&#21644;&#29992;&#25143;&#32469;&#36807;&#27169;&#22411;&#38480;&#21046;(&#20363;&#22914;&#20135;&#29983;&#26377;&#23475;&#20869;&#23481;)&#30340;&#33021;&#21147;&#26041;&#38754;&#30340;&#32570;&#28857;&#12290;&#28982;&#32780;&#65292;LLMs&#25552;&#20379;&#21487;&#38752;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#24314;&#35758;&#30340;&#33021;&#21147;&#36824;&#27809;&#26377;&#24471;&#21040;&#24456;&#22909;&#30340;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#27979;&#37327;&#23427;&#20204;&#21453;&#39539;&#26222;&#36941;&#23384;&#22312;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#35823;&#35299;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#26368;&#36817;&#30340;&#23398;&#26415;&#25991;&#29486;&#65292;&#25972;&#29702;&#20102;&#20845;&#20010;&#19981;&#21516;&#20027;&#39064;&#30340;&#19968;&#30334;&#22810;&#20010;&#19982;&#23433;&#20840;&#21644;&#38544;&#31169;&#30456;&#20851;&#30340;&#35823;&#35299;&#30340;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26597;&#35810;&#20102;&#20004;&#20010;&#27969;&#34892;&#30340;LLMs(Bard&#21644;ChatGPT)&#65292;&#24182;&#21046;&#23450;&#20102;&#19968;&#20010;&#26631;&#27880;&#25351;&#21335;&#26469;&#35780;&#20272;&#23427;&#20204;&#23545;&#35823;&#35299;&#30340;&#22238;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Users seek security &amp; privacy (S&amp;P) advice from online resources, including trusted websites and content-sharing platforms. These resources help users understand S&amp;P technologies and tools and suggest actionable strategies. Large Language Models (LLMs) have recently emerged as trusted information sources. However, their accuracy and correctness have been called into question. Prior research has outlined the shortcomings of LLMs in answering multiple-choice questions and user ability to inadvertently circumvent model restrictions (e.g., to produce toxic content). Yet, the ability of LLMs to provide reliable S&amp;P advice is not well-explored. In this paper, we measure their ability to refute popular S&amp;P misconceptions that the general public holds. We first study recent academic literature to curate a dataset of over a hundred S&amp;P-related misconceptions across six different topics. We then query two popular LLMs (Bard and ChatGPT) and develop a labeling guide to evaluate their responses to
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#24635;&#32467;&#20102;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#65292;&#24182;&#24378;&#35843;&#20102;&#20854;&#20851;&#38190;&#21407;&#29702;&#21644;&#25104;&#21151;&#35201;&#32032;&#65292;&#20197;&#21450;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#20013;&#30340;&#37096;&#32626;&#25361;&#25112;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#36824;&#25351;&#20986;&#30693;&#35782;&#33976;&#39311;&#26377;&#28508;&#21147;&#25104;&#20026;&#20851;&#38190;&#30340;&#25216;&#26415;&#36716;&#25240;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.02421</link><description>&lt;p&gt;
&#23398;&#29983;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#19982;&#20854;&#25945;&#24072;&#19968;&#26679;&#34920;&#29616;&#20986;&#33394;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can a student Large Language Model perform as well as it's teacher?. (arXiv:2310.02421v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02421
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#24635;&#32467;&#20102;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#65292;&#24182;&#24378;&#35843;&#20102;&#20854;&#20851;&#38190;&#21407;&#29702;&#21644;&#25104;&#21151;&#35201;&#32032;&#65292;&#20197;&#21450;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#20013;&#30340;&#37096;&#32626;&#25361;&#25112;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#36824;&#25351;&#20986;&#30693;&#35782;&#33976;&#39311;&#26377;&#28508;&#21147;&#25104;&#20026;&#20851;&#38190;&#30340;&#25216;&#26415;&#36716;&#25240;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20195;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#19981;&#26029;&#22686;&#21152;&#65292;&#34429;&#28982;&#33021;&#23454;&#29616;&#26080;&#19982;&#20262;&#27604;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#20063;&#19981;&#21487;&#36991;&#20813;&#22320;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#20013;&#24102;&#26469;&#37096;&#32626;&#25361;&#25112;&#12290;&#30693;&#35782;&#33976;&#39311;&#20316;&#20026;&#19968;&#31181;&#23558;&#39640;&#23481;&#37327;&#8220;&#25945;&#24072;&#8221;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#31616;&#21270;&#30340;&#8220;&#23398;&#29983;&#8221;&#27169;&#22411;&#30340;&#25216;&#26415;&#65292;&#25104;&#20026;&#35299;&#20915;&#36825;&#19968;&#22256;&#22659;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#20840;&#38754;&#27010;&#36848;&#20102;&#30693;&#35782;&#33976;&#39311;&#33539;&#24335;&#65292;&#24378;&#35843;&#20102;&#20854;&#22522;&#26412;&#21407;&#29702;&#65292;&#22914;&#36719;&#26631;&#31614;&#30340;&#23454;&#29992;&#24615;&#21644;&#28201;&#24230;&#32553;&#25918;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#32454;&#33268;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#38416;&#26126;&#20102;&#25104;&#21151;&#33976;&#39311;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#21253;&#25324;&#23398;&#29983;&#27169;&#22411;&#30340;&#26550;&#26500;&#12289;&#25945;&#24072;&#30340;&#27700;&#24179;&#20197;&#21450;&#36229;&#21442;&#25968;&#30340;&#24179;&#34913;&#12290;&#21516;&#26102;&#25105;&#20204;&#36824;&#28145;&#20837;&#25506;&#35752;&#20102;&#36825;&#19968;&#36807;&#31243;&#20013;&#30340;&#22797;&#26434;&#24615;&#21644;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#25506;&#32034;&#20984;&#26174;&#20102;&#30693;&#35782;&#33976;&#39311;&#20316;&#20026;&#19968;&#20010;&#37325;&#35201;&#36716;&#25240;&#28857;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The burgeoning complexity of contemporary deep learning models, while achieving unparalleled accuracy, has inadvertently introduced deployment challenges in resource-constrained environments. Knowledge distillation, a technique aiming to transfer knowledge from a high-capacity "teacher" model to a streamlined "student" model, emerges as a promising solution to this dilemma. This paper provides a comprehensive overview of the knowledge distillation paradigm, emphasizing its foundational principles such as the utility of soft labels and the significance of temperature scaling. Through meticulous examination, we elucidate the critical determinants of successful distillation, including the architecture of the student model, the caliber of the teacher, and the delicate balance of hyperparameters. While acknowledging its profound advantages, we also delve into the complexities and challenges inherent in the process. Our exploration underscores knowledge distillation's potential as a pivotal 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#19987;&#23478;&#28151;&#21512;&#65288;MoQE&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19987;&#23478;&#26435;&#37325;&#24212;&#29992;2&#20301;&#20302;&#20301;&#37327;&#21270;&#65292;&#20943;&#36731;&#20102;&#22823;&#35268;&#27169;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#27169;&#22411;&#22312;&#20869;&#23384;&#28040;&#32791;&#21644;&#24310;&#36831;&#38382;&#39064;&#19978;&#30340;&#21387;&#21147;&#65292;&#21516;&#26102;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#19981;&#38656;&#35201;&#39069;&#22806;&#35757;&#32451;&#20063;&#33021;&#20445;&#25345;&#21487;&#38752;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.02410</link><description>&lt;p&gt;
&#37327;&#21270;&#19987;&#23478;&#28151;&#21512;&#65288;MoQE&#65289;&#65306;&#20302;&#20301;&#37327;&#21270;&#21644;&#40065;&#26834;&#24615;&#30340;&#20114;&#34917;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Mixture of Quantized Experts (MoQE): Complementary Effect of Low-bit Quantization and Robustness. (arXiv:2310.02410v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#19987;&#23478;&#28151;&#21512;&#65288;MoQE&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19987;&#23478;&#26435;&#37325;&#24212;&#29992;2&#20301;&#20302;&#20301;&#37327;&#21270;&#65292;&#20943;&#36731;&#20102;&#22823;&#35268;&#27169;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#27169;&#22411;&#22312;&#20869;&#23384;&#28040;&#32791;&#21644;&#24310;&#36831;&#38382;&#39064;&#19978;&#30340;&#21387;&#21147;&#65292;&#21516;&#26102;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#19981;&#38656;&#35201;&#39069;&#22806;&#35757;&#32451;&#20063;&#33021;&#20445;&#25345;&#21487;&#38752;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#30340;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#27169;&#22411;&#22312;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#20013;&#65292;&#21253;&#25324;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#65292;&#36890;&#36807;&#19987;&#23478;&#24182;&#34892;&#24615;&#30340;&#39640;&#25928;&#27169;&#22411;&#25193;&#23637;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#22312;&#37096;&#32626;&#26102;&#65292;&#36825;&#20063;&#24102;&#26469;&#20102;&#26356;&#22823;&#30340;&#20869;&#23384;&#28040;&#32791;&#21644;&#22686;&#21152;&#30340;&#20869;&#23384;&#24102;&#23485;&#29942;&#39048;&#30340;&#22522;&#26412;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#37327;&#21270;&#19987;&#23478;&#28151;&#21512;&#65288;MoQE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#20165;&#23558;&#19987;&#23478;&#26435;&#37325;&#24212;&#29992;&#20110;&#36229;&#20302;&#20301;2&#20301;&#37327;&#21270;&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#20197;&#20943;&#36731;MoE&#27169;&#22411;&#30340;&#22686;&#22823;&#20869;&#23384;&#21644;&#24310;&#36831;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20302;&#20301;&#37327;&#21270;&#19982;MoE&#26550;&#26500;&#32467;&#21512;&#65292;&#21363;&#20351;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#19981;&#38656;&#35201;&#20219;&#20309;&#39069;&#22806;&#30340;&#35757;&#32451;&#65292;&#20063;&#33021;&#25552;&#20379;&#21487;&#38752;&#30340;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#19988;&#26174;&#33879;&#20943;&#23567;&#20869;&#23384;&#22823;&#23567;&#12290;&#29305;&#21035;&#22320;&#65292;MoE&#27169;&#22411;&#20013;&#30340;&#19987;&#23478;&#23618;&#23545;&#37327;&#21270;&#27604;&#20256;&#32479;&#30340;&#21069;&#39304;&#32593;&#32476;&#65288;FFN&#65289;&#23618;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#32508;&#21512;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;MoE&#27169;&#22411;...
&lt;/p&gt;
&lt;p&gt;
Large Mixture of Experts (MoE) models could achieve state-of-the-art quality on various language tasks, including machine translation task, thanks to the efficient model scaling capability with expert parallelism. However, it has brought a fundamental issue of larger memory consumption and increased memory bandwidth bottleneck at deployment time. In this paper, we propose Mixture of Quantized Experts (MoQE) which is a simple weight-only quantization method applying ultra low-bit down to 2-bit quantizations only to expert weights for mitigating the increased memory and latency issues of MoE models. We show that low-bit quantization together with the MoE architecture delivers a reliable model performance while reducing the memory size significantly even without any additional training in most cases. In particular, expert layers in MoE models are much more robust to the quantization than conventional feedforward networks (FFN) layers. In our comprehensive analysis, we show that MoE models
&lt;/p&gt;</description></item><item><title>Nugget 2D&#26159;&#19968;&#31181;&#29992;&#20110;&#20165;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#30340;&#21160;&#24577;&#19978;&#19979;&#25991;&#21387;&#32553;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#30041;&#20219;&#21153;&#33021;&#21147;&#30340;&#21516;&#26102;&#22823;&#24133;&#20943;&#23569;&#35299;&#30721;&#36807;&#31243;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#24320;&#38144;&#12290;</title><link>http://arxiv.org/abs/2310.02409</link><description>&lt;p&gt;
Nugget 2D&#65306;&#29992;&#20110;&#20165;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#30340;&#21160;&#24577;&#19978;&#19979;&#25991;&#21387;&#32553;&#30340;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Nugget 2D: Dynamic Contextual Compression for Scaling Decoder-only Language Models. (arXiv:2310.02409v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02409
&lt;/p&gt;
&lt;p&gt;
Nugget 2D&#26159;&#19968;&#31181;&#29992;&#20110;&#20165;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#30340;&#21160;&#24577;&#19978;&#19979;&#25991;&#21387;&#32553;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#30041;&#20219;&#21153;&#33021;&#21147;&#30340;&#21516;&#26102;&#22823;&#24133;&#20943;&#23569;&#35299;&#30721;&#36807;&#31243;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#30340;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#38271;&#19978;&#19979;&#25991;&#20013;&#32553;&#25918;&#25928;&#26524;&#19981;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#19978;&#19979;&#25991;&#21387;&#32553;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#23558;Qin&#65286;Van Durme&#65288;2023&#24180;&#65289;&#30340;Nugget&#26041;&#27861;&#20174;BERT&#31867;&#26694;&#26550;&#25193;&#23637;&#21040;&#20165;&#35299;&#30721;&#22120;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#21382;&#21490;&#24314;&#27169;&#20026;&#21387;&#32553;&#30340;&#8220;nuggets&#8221;&#65292;&#36825;&#20123;&#8220;nuggets&#8221;&#32463;&#36807;&#35757;&#32451;&#21487;&#20197;&#36827;&#34892;&#37325;&#24314;&#65292;&#23427;&#21487;&#20197;&#20351;&#29992;&#35832;&#22914;LLaMA&#20043;&#31867;&#30340;&#29616;&#25104;&#27169;&#22411;&#36827;&#34892;&#21021;&#22987;&#21270;&#12290;&#25105;&#20204;&#36890;&#36807;&#35821;&#35328;&#24314;&#27169;&#12289;&#38382;&#31572;&#21644;&#25688;&#35201;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;Nugget2D&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#20445;&#30041;&#20102;&#33021;&#21147;&#65292;&#21516;&#26102;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#22823;&#24133;&#20943;&#23569;&#20102;&#26102;&#38388;&#21644;&#31354;&#38388;&#24320;&#38144;&#12290;&#20363;&#22914;&#65292;&#22312;&#33258;&#21160;&#32534;&#30721;&#23454;&#39564;&#20013;&#65292;Nugget2D&#21487;&#20197;&#20197;20&#20493;&#30340;&#21387;&#32553;&#27604;&#25910;&#32553;&#19978;&#19979;&#25991;&#65292;&#37325;&#24314;&#26102;&#30340;BLEU&#24471;&#20998;&#20026;98&#65285;&#65292;&#23454;&#29616;&#20102;&#36817;&#20046;&#26080;&#25439;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Standard Transformer-based language models (LMs) scale poorly to long contexts. We propose a solution based on dynamic contextual compression, which extends the Nugget approach of Qin &amp; Van Durme (2023) from BERT-like frameworks to decoder-only LMs. Our method models history as compressed "nuggets" which are trained to allow for reconstruction, and it can be initialized with off-the-shelf models such as LLaMA. We demonstrate through experiments in language modeling, question answering, and summarization that Nugget2D retains capabilities in these tasks, while drastically reducing the overhead during decoding in terms of time and space. For example, in the experiments of autoencoding, Nugget2D can shrink context at a 20x compression ratio with a BLEU score of 98% for reconstruction, achieving nearly lossless encoding.
&lt;/p&gt;</description></item><item><title>MindTheDApp&#26159;&#19968;&#31181;&#29305;&#21035;&#35774;&#35745;&#29992;&#20110;&#20197;&#22826;&#22346;&#22522;&#20110;&#21435;&#20013;&#24515;&#21270;&#24212;&#29992;&#31243;&#24207;&#30340;&#32467;&#26500;&#20998;&#26512;&#30340;&#24037;&#20855;&#38142;&#65292;&#37319;&#29992;&#22797;&#26434;&#30340;&#32593;&#32476;&#39537;&#21160;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#20108;&#20998;&#22270;&#26469;&#31361;&#20986;DApp&#26550;&#26500;&#30340;&#25805;&#20316;&#25928;&#29575;&#65292;&#20197;&#21450;&#25552;&#20379;&#20102;&#32454;&#31890;&#24230;&#35270;&#22270;&#26469;&#23637;&#31034;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#21644;&#25191;&#34892;&#27969;&#31243;&#12290;</title><link>http://arxiv.org/abs/2310.02408</link><description>&lt;p&gt;
MindTheDApp: &#19968;&#31181;&#29992;&#20110;&#20197;&#22826;&#22346;&#22522;&#20110;&#21435;&#20013;&#24515;&#21270;&#24212;&#29992;&#31243;&#24207;&#30340;&#22797;&#26434;&#32593;&#32476;&#39537;&#21160;&#30340;&#32467;&#26500;&#20998;&#26512;&#30340;&#24037;&#20855;&#38142;
&lt;/p&gt;
&lt;p&gt;
MindTheDApp: A Toolchain for Complex Network-Driven Structural Analysis of Ethereum-based Decentralised Applications. (arXiv:2310.02408v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02408
&lt;/p&gt;
&lt;p&gt;
MindTheDApp&#26159;&#19968;&#31181;&#29305;&#21035;&#35774;&#35745;&#29992;&#20110;&#20197;&#22826;&#22346;&#22522;&#20110;&#21435;&#20013;&#24515;&#21270;&#24212;&#29992;&#31243;&#24207;&#30340;&#32467;&#26500;&#20998;&#26512;&#30340;&#24037;&#20855;&#38142;&#65292;&#37319;&#29992;&#22797;&#26434;&#30340;&#32593;&#32476;&#39537;&#21160;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#20108;&#20998;&#22270;&#26469;&#31361;&#20986;DApp&#26550;&#26500;&#30340;&#25805;&#20316;&#25928;&#29575;&#65292;&#20197;&#21450;&#25552;&#20379;&#20102;&#32454;&#31890;&#24230;&#35270;&#22270;&#26469;&#23637;&#31034;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#21644;&#25191;&#34892;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;MindTheDApp&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#20026;&#20197;&#22826;&#22346;&#22522;&#20110;&#21435;&#20013;&#24515;&#21270;&#24212;&#29992;&#31243;&#24207;&#65288;DApps&#65289;&#30340;&#32467;&#26500;&#20998;&#26512;&#32780;&#35774;&#35745;&#30340;&#24037;&#20855;&#38142;&#65292;&#20854;&#29420;&#29305;&#30340;&#37325;&#28857;&#26159;&#22797;&#26434;&#30340;&#32593;&#32476;&#39537;&#21160;&#26041;&#27861;&#12290;&#19982;&#29616;&#26377;&#24037;&#20855;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#24037;&#20855;&#38142;&#32467;&#21512;&#20102;ANTLR4&#21644;&#25277;&#35937;&#35821;&#27861;&#26641;&#65288;AST&#65289;&#36941;&#21382;&#25216;&#26415;&#30340;&#33021;&#21147;&#65292;&#23558;&#26234;&#33021;&#21512;&#32422;&#20013;&#30340;&#26550;&#26500;&#21644;&#20132;&#20114;&#36716;&#21270;&#20026;&#19987;&#38376;&#30340;&#20108;&#20998;&#22270;&#12290;&#36825;&#20351;&#24471;&#39640;&#32423;&#32593;&#32476;&#20998;&#26512;&#33021;&#22815;&#31361;&#20986;DApp&#26550;&#26500;&#30340;&#25805;&#20316;&#25928;&#29575;&#12290;&#25152;&#25552;&#20986;&#30340;&#24037;&#20855;&#29983;&#25104;&#30340;&#20108;&#20998;&#22270;&#21253;&#25324;&#20004;&#32452;&#33410;&#28857;&#65306;&#19968;&#32452;&#34920;&#31034;&#26234;&#33021;&#21512;&#32422;&#12289;&#25509;&#21475;&#21644;&#24211;&#65292;&#21478;&#19968;&#32452;&#21253;&#25324;&#20989;&#25968;&#12289;&#20107;&#20214;&#21644;&#20462;&#39280;&#31526;&#12290;&#22270;&#20013;&#30340;&#36793;&#23558;&#20989;&#25968;&#19982;&#23427;&#20204;&#20132;&#20114;&#30340;&#26234;&#33021;&#21512;&#32422;&#36830;&#25509;&#36215;&#26469;&#65292;&#25552;&#20379;&#20102;DApp&#20013;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#21644;&#25191;&#34892;&#27969;&#31243;&#30340;&#32454;&#31890;&#24230;&#35270;&#22270;&#12290;&#36825;&#31181;&#20197;&#32593;&#32476;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#20801;&#35768;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#24212;&#29992;&#22797;&#26434;&#30340;&#32593;&#32476;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
This paper presents MindTheDApp, a toolchain designed specifically for the structural analysis of Ethereum-based Decentralized Applications (DApps), with a distinct focus on a complex network-driven approach. Unlike existing tools, our toolchain combines the power of ANTLR4 and Abstract Syntax Tree (AST) traversal techniques to transform the architecture and interactions within smart contracts into a specialized bipartite graph. This enables advanced network analytics to highlight operational efficiencies within the DApp's architecture.  The bipartite graph generated by the proposed tool comprises two sets of nodes: one representing smart contracts, interfaces, and libraries, and the other including functions, events, and modifiers. Edges in the graph connect functions to smart contracts they interact with, offering a granular view of interdependencies and execution flow within the DApp. This network-centric approach allows researchers and practitioners to apply complex network theory 
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;ESPUM&#65292;&#21033;&#29992;&#20302;&#38454;N-skipgrams&#21644;&#20301;&#32622;&#24207;&#21015;&#32479;&#35745;&#20449;&#24687;&#65292;&#22312;ASR&#21644;&#38899;&#32032;&#20998;&#21106;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#31454;&#20105;&#24615;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.02382</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#35821;&#38899;&#35782;&#21035;&#19982;N-Skipgram&#21644;&#20301;&#32622;&#24207;&#21015;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Speech Recognition with N-Skipgram and Positional Unigram Matching. (arXiv:2310.02382v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02382
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;ESPUM&#65292;&#21033;&#29992;&#20302;&#38454;N-skipgrams&#21644;&#20301;&#32622;&#24207;&#21015;&#32479;&#35745;&#20449;&#24687;&#65292;&#22312;ASR&#21644;&#38899;&#32032;&#20998;&#21106;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#31454;&#20105;&#24615;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#26080;&#30417;&#30563;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#30001;&#20110;GAN&#30456;&#20851;&#30340;&#19981;&#31283;&#23450;&#24615;&#12289;&#35821;&#38899;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#19981;&#23545;&#40784;&#20197;&#21450;&#24040;&#22823;&#30340;&#20869;&#23384;&#38656;&#27714;&#32780;&#38754;&#20020;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;ASR&#31995;&#32479;ESPUM&#12290;&#35813;&#31995;&#32479;&#21033;&#29992;&#20102;&#20302;&#38454;N-skipgrams&#65288;&#26368;&#22810;N=3&#65289;&#21644;&#20174;&#19968;&#23567;&#25209;&#26679;&#26412;&#20013;&#25910;&#38598;&#21040;&#30340;&#20301;&#32622;&#24207;&#21015;&#32479;&#35745;&#20449;&#24687;&#12290;&#22312;TIMIT&#22522;&#20934;&#27979;&#35797;&#19978;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;ASR&#21644;&#38899;&#32032;&#20998;&#21106;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;&#35775;&#38382;&#25105;&#20204;&#20844;&#24320;&#30340;&#20195;&#30721;&#65306;https://github.com/lwang114/GraphUnsupASR&#12290;
&lt;/p&gt;
&lt;p&gt;
Training unsupervised speech recognition systems presents challenges due to GAN-associated instability, misalignment between speech and text, and significant memory demands. To tackle these challenges, we introduce a novel ASR system, ESPUM. This system harnesses the power of lower-order N-skipgrams (up to N=3) combined with positional unigram statistics gathered from a small batch of samples. Evaluated on the TIMIT benchmark, our model showcases competitive performance in ASR and phoneme segmentation tasks. Access our publicly available code at https://github.com/lwang114/GraphUnsupASR.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#20250;&#35805;&#24335;&#20581;&#24247;&#20195;&#29702;&#26694;&#26550;&#65292;&#26088;&#22312;&#20026;&#20195;&#29702;&#36171;&#20104;&#25209;&#21028;&#24615;&#24605;&#32500;&#12289;&#30693;&#35782;&#33719;&#21462;&#21644;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#65292;&#23454;&#29616;&#20010;&#24615;&#21270;&#30340;&#20581;&#24247;&#25252;&#29702;&#26381;&#21153;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#26080;&#32541;&#38598;&#25104;&#21307;&#30103;&#24037;&#20855;&#65292;&#23454;&#29616;&#22810;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#23545;&#35805;&#65292;&#24182;&#19982;&#22810;&#31181;&#29992;&#25143;&#25968;&#25454;&#20998;&#26512;&#24037;&#20855;&#36830;&#25509;&#12290;</title><link>http://arxiv.org/abs/2310.02374</link><description>&lt;p&gt;
&#20250;&#35805;&#24335;&#20581;&#24247;&#20195;&#29702;&#65306;&#20010;&#24615;&#21270;&#30340;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Conversational Health Agents: A Personalized LLM-Powered Agent Framework. (arXiv:2310.02374v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02374
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#20250;&#35805;&#24335;&#20581;&#24247;&#20195;&#29702;&#26694;&#26550;&#65292;&#26088;&#22312;&#20026;&#20195;&#29702;&#36171;&#20104;&#25209;&#21028;&#24615;&#24605;&#32500;&#12289;&#30693;&#35782;&#33719;&#21462;&#21644;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#65292;&#23454;&#29616;&#20010;&#24615;&#21270;&#30340;&#20581;&#24247;&#25252;&#29702;&#26381;&#21153;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#26080;&#32541;&#38598;&#25104;&#21307;&#30103;&#24037;&#20855;&#65292;&#23454;&#29616;&#22810;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#23545;&#35805;&#65292;&#24182;&#19982;&#22810;&#31181;&#29992;&#25143;&#25968;&#25454;&#20998;&#26512;&#24037;&#20855;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20250;&#35805;&#24335;&#20581;&#24247;&#20195;&#29702;&#65288;CHAs&#65289;&#26159;&#19968;&#31181;&#20114;&#21160;&#31995;&#32479;&#65292;&#26088;&#22312;&#36890;&#36807;&#36827;&#34892;&#20849;&#24773;&#23545;&#35805;&#21644;&#22788;&#29702;&#22810;&#27169;&#24577;&#25968;&#25454;&#26469;&#22686;&#24378;&#20010;&#20154;&#20581;&#24247;&#25252;&#29702;&#26381;&#21153;&#12290;&#24403;&#21069;&#30340;CHAs&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#31995;&#32479;&#65292;&#20027;&#35201;&#20851;&#27880;&#23545;&#35805;&#65292;&#20294;&#24448;&#24448;&#32570;&#20047;&#20840;&#38754;&#30340;&#20195;&#29702;&#33021;&#21147;&#12290;&#36825;&#21253;&#25324;&#20174;&#21487;&#31359;&#25140;&#35774;&#22791;&#12289;&#20840;&#22825;&#20505;&#25968;&#25454;&#25910;&#38598;&#28304;&#21644;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#33719;&#21462;&#20010;&#20154;&#29992;&#25143;&#30340;&#20581;&#24247;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#25972;&#21512;&#26368;&#26032;&#21457;&#24067;&#30340;&#20581;&#24247;&#35265;&#35299;&#65292;&#24182;&#19982;&#24050;&#24314;&#31435;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#20998;&#26512;&#24037;&#20855;&#36830;&#25509;&#12290;&#25105;&#20204;&#27491;&#22312;&#24320;&#21457;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#36171;&#20104;CHAs&#25209;&#21028;&#24615;&#24605;&#32500;&#12289;&#30693;&#35782;&#33719;&#21462;&#21644;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#26469;&#22686;&#24378;&#23427;&#20204;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;CHA&#24179;&#21488;&#30001;LLMs&#39537;&#21160;&#65292;&#26080;&#32541;&#38598;&#25104;&#20102;&#21307;&#30103;&#24037;&#20855;&#65292;&#23454;&#29616;&#20102;&#22810;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#23545;&#35805;&#65292;&#24182;&#19982;&#21508;&#31181;&#29992;&#25143;&#25968;&#25454;&#20998;&#26512;&#24037;&#20855;&#36827;&#34892;&#25509;&#21475;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20854;&#22312;&#22788;&#29702;&#22797;&#26434;&#21307;&#30103;&#20219;&#21153;&#26041;&#38754;&#30340;&#29087;&#32451;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational Health Agents (CHAs) are interactive systems designed to enhance personal healthcare services by engaging in empathetic conversations and processing multimodal data. While current CHAs, especially those utilizing Large Language Models (LLMs), primarily focus on conversation, they often lack comprehensive agent capabilities. This includes the ability to access personal user health data from wearables, 24/7 data collection sources, and electronic health records, as well as integrating the latest published health insights and connecting with established multimodal data analysis tools. We are developing a framework to empower CHAs by equipping them with critical thinking, knowledge acquisition, and problem-solving abilities. Our CHA platform, powered by LLMs, seamlessly integrates healthcare tools, enables multilingual and multimodal conversations, and interfaces with a variety of user data analysis tools. We illustrate its proficiency in handling complex healthcare tasks, s
&lt;/p&gt;</description></item><item><title>ProtoNER&#26159;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#32593;&#32476;&#30340;&#31471;&#21040;&#31471;KVP&#25552;&#21462;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#29616;&#26377;&#27169;&#22411;&#20013;&#28155;&#21152;&#26032;&#31867;&#21035;&#65292;&#32780;&#21482;&#38656;&#26368;&#23569;&#25968;&#37327;&#30340;&#26032;&#27880;&#37322;&#35757;&#32451;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2310.02372</link><description>&lt;p&gt;
ProtoNER: &#20351;&#29992;&#21407;&#22411;&#32593;&#32476;&#36827;&#34892;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#23569;&#26679;&#26412;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ProtoNER: Few shot Incremental Learning for Named Entity Recognition using Prototypical Networks. (arXiv:2310.02372v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02372
&lt;/p&gt;
&lt;p&gt;
ProtoNER&#26159;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#32593;&#32476;&#30340;&#31471;&#21040;&#31471;KVP&#25552;&#21462;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#29616;&#26377;&#27169;&#22411;&#20013;&#28155;&#21152;&#26032;&#31867;&#21035;&#65292;&#32780;&#21482;&#38656;&#26368;&#23569;&#25968;&#37327;&#30340;&#26032;&#27880;&#37322;&#35757;&#32451;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38190;&#20540;&#23545;&#65288;KVP&#65289;&#25552;&#21462;&#25110;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#26159;&#25991;&#26723;&#29702;&#35299;&#21644;&#25968;&#25454;&#25552;&#21462;&#39046;&#22495;&#30340;&#30740;&#31350;&#28909;&#28857;&#12290;&#19968;&#20123;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#22914;LayoutLMv2&#12289;LayoutLMv3&#21644;LiLT&#24050;&#32463;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#21521;&#29616;&#26377;&#27169;&#22411;&#28155;&#21152;&#19968;&#20010;&#26032;&#31867;&#21035;&#65292;&#20063;&#38656;&#35201;&#37325;&#26032;&#27880;&#37322;&#25972;&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#24182;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#36825;&#20123;&#38382;&#39064;&#37117;&#20005;&#37325;&#24433;&#21709;&#20102;&#26356;&#26032;&#27169;&#22411;&#30340;&#37096;&#32626;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Key value pair (KVP) extraction or Named Entity Recognition(NER) from visually rich documents has been an active area of research in document understanding and data extraction domain. Several transformer based models such as LayoutLMv2, LayoutLMv3, and LiLT have emerged achieving state of the art results. However, addition of even a single new class to the existing model requires (a) re-annotation of entire training dataset to include this new class and (b) retraining the model again. Both of these issues really slow down the deployment of updated model. \\ We present \textbf{ProtoNER}: Prototypical Network based end-to-end KVP extraction model that allows addition of new classes to an existing model while requiring minimal number of newly annotated training samples. The key contributions of our model are: (1) No dependency on dataset used for initial training of the model, which alleviates the need to retain original training dataset for longer duration as well as data re-annotation w
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#27602;&#24615;&#30340;&#23450;&#20041;&#27169;&#31946;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23450;&#37327;&#21387;&#21147;&#30340;&#27602;&#24615;&#23450;&#20041;&#26469;&#24357;&#34917;&#29616;&#26377;&#23450;&#20041;&#30340;&#32570;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.02357</link><description>&lt;p&gt;
&#20851;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#27602;&#24615;&#23450;&#20041;&#30340;&#25506;&#35752;
&lt;/p&gt;
&lt;p&gt;
On the definition of toxicity in NLP. (arXiv:2310.02357v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02357
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#27602;&#24615;&#30340;&#23450;&#20041;&#27169;&#31946;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23450;&#37327;&#21387;&#21147;&#30340;&#27602;&#24615;&#23450;&#20041;&#26469;&#24357;&#34917;&#29616;&#26377;&#23450;&#20041;&#30340;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27602;&#24615;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#26681;&#26412;&#38382;&#39064;&#22312;&#20110;&#27602;&#24615;&#30340;&#23450;&#20041;&#27169;&#31946;&#19981;&#28165;&#12290;&#35895;&#27468;&#26071;&#19979;&#30340;&#22242;&#38431;Jigsaw&#26159;&#35813;&#39046;&#22495;&#30340;&#39046;&#23548;&#32773;&#20043;&#19968;&#65292;&#20182;&#20204;&#20351;&#29992;Dixon&#31561;&#20154;&#32473;&#20986;&#30340;&#27602;&#24615;&#23450;&#20041;&#65306;&#8220;&#31895;&#40065;&#12289;&#19981;&#23562;&#37325;&#25110;&#19981;&#21512;&#29702;&#30340;&#35821;&#35328;&#65292;&#21487;&#33021;&#20250;&#35753;&#26576;&#20154;&#31163;&#24320;&#35752;&#35770;&#8221;&#12290;&#20154;&#20204;&#21487;&#20197;&#31435;&#21363;&#30475;&#21040;&#36825;&#20010;&#23450;&#20041;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#27809;&#26377;&#32473;&#20986;&#27602;&#24615;&#30340;&#23450;&#37327;&#24230;&#37327;&#65292;&#32780;&#19988;&#28041;&#21450;&#39640;&#24230;&#20027;&#35266;&#30340;&#25991;&#21270;&#26415;&#35821;&#12290;&#23613;&#31649;&#23384;&#22312;&#27169;&#31946;&#21644;&#32570;&#38519;&#65292;&#20294;&#36825;&#20010;&#23450;&#20041;&#24050;&#32463;&#25104;&#20026;&#35768;&#22810;&#30740;&#31350;&#32773;&#24191;&#27867;&#37319;&#29992;&#30340;&#23454;&#38469;&#26631;&#20934;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23450;&#37327;&#21387;&#21147;&#30340;&#27602;&#24615;&#23450;&#20041;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#30340;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
The fundamental problem in toxicity detection task lies in the fact that the toxicity is ill-defined. Jigsaw, a unit within Google and one of the leaders in the field, uses a definition of toxicity given by Dixon et al. - 'rude, disrespectful, or unreasonable language that is likely to make someone leave a discussion'. One can instantly see the issue with this definition, as it gives no quantitative measure of the toxicity and operates with highly subjective cultural terms. Despite all vagueness and flaws, this definition is de-facto widely used by many researchers. In this work we suggest quantative stress-based defenition for the toxicity that overcomes existing shortcomings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23398;&#20248;&#21270;&#22120;&#65288;STOP&#65289;&#65292;&#36890;&#36807;&#36882;&#24402;&#33258;&#25105;&#25913;&#36827;&#30340;&#20195;&#30721;&#29983;&#25104;&#65292;&#20351;&#29992;&#34701;&#21512;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#33050;&#25163;&#26550;&#31243;&#24207;&#26469;&#25913;&#36827;&#33258;&#36523;&#65292;&#20174;&#32780;&#29983;&#25104;&#24615;&#33021;&#26356;&#22909;&#30340;&#31243;&#24207;&#12290;</title><link>http://arxiv.org/abs/2310.02304</link><description>&lt;p&gt;
&#33258;&#23398;&#20248;&#21270;&#22120;&#65288;STOP&#65289;&#65306;&#36882;&#24402;&#33258;&#25105;&#25913;&#36827;&#30340;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation. (arXiv:2310.02304v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23398;&#20248;&#21270;&#22120;&#65288;STOP&#65289;&#65292;&#36890;&#36807;&#36882;&#24402;&#33258;&#25105;&#25913;&#36827;&#30340;&#20195;&#30721;&#29983;&#25104;&#65292;&#20351;&#29992;&#34701;&#21512;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#33050;&#25163;&#26550;&#31243;&#24207;&#26469;&#25913;&#36827;&#33258;&#36523;&#65292;&#20174;&#32780;&#29983;&#25104;&#24615;&#33021;&#26356;&#22909;&#30340;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65288;&#20363;&#22914;&#24605;&#32500;&#26641;&#21644;&#31243;&#24207;&#36741;&#21161;&#35821;&#35328;&#27169;&#22411;&#65289;&#21462;&#24471;&#20102;&#19968;&#20123;&#37325;&#35201;&#36827;&#23637;&#65292;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#8220;&#33050;&#25163;&#26550;&#8221;&#31243;&#24207;&#26469;&#35299;&#20915;&#38382;&#39064;&#65292;&#35813;&#31243;&#24207;&#26500;&#24314;&#20102;&#22810;&#27425;&#35843;&#29992;&#35821;&#35328;&#27169;&#22411;&#20197;&#29983;&#25104;&#26356;&#22909;&#30340;&#36755;&#20986;&#12290;&#33050;&#25163;&#26550;&#31243;&#24207;&#36890;&#24120;&#20351;&#29992;Python&#31561;&#32534;&#31243;&#35821;&#35328;&#32534;&#20889;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#34701;&#21512;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#33050;&#25163;&#26550;&#31243;&#24207;&#26469;&#25913;&#36827;&#33258;&#36523;&#12290;&#25105;&#20204;&#20174;&#19968;&#20010;&#31181;&#23376;&#8220;&#25913;&#36827;&#22120;&#8221;&#24320;&#22987;&#65292;&#36890;&#36807;&#22810;&#27425;&#26597;&#35810;&#35821;&#35328;&#27169;&#22411;&#24182;&#36820;&#22238;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#65292;&#26681;&#25454;&#32473;&#23450;&#30340;&#25928;&#29992;&#20989;&#25968;&#26469;&#25913;&#36827;&#36755;&#20837;&#31243;&#24207;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36816;&#34892;&#36825;&#20010;&#31181;&#23376;&#25913;&#36827;&#22120;&#26469;&#25913;&#36827;&#33258;&#36523;&#12290;&#22312;&#19968;&#31995;&#21015;&#32454;&#20998;&#20219;&#21153;&#20013;&#65292;&#24471;&#21040;&#30340;&#25913;&#36827;&#25913;&#36827;&#22120;&#29983;&#25104;&#30340;&#31243;&#24207;&#22312;&#24615;&#33021;&#19978;&#26126;&#26174;&#20248;&#20110;&#31181;&#23376;&#25913;&#36827;&#22120;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23545;&#35821;&#35328;&#27169;&#22411;&#25552;&#20986;&#30340;&#21508;&#31181;&#33258;&#25105;&#25913;&#36827;&#31574;&#30053;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#21253;&#25324;&#27874;&#26463;&#25628;&#32034;&#12289;&#36951;&#20256;&#31639;&#27861;&#21644;&#27169;&#25311;&#36864;&#28779;&#12290;&#30001;&#20110;&#35821;&#35328;&#27169;&#22411;&#26412;&#36523;&#27809;&#26377;&#25913;&#21464;&#65292;&#36825;&#24182;&#19981;&#26159;&#19968;&#31181;&#22686;&#38271;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several recent advances in AI systems (e.g., Tree-of-Thoughts and Program-Aided Language Models) solve problems by providing a "scaffolding" program that structures multiple calls to language models to generate better outputs. A scaffolding program is written in a programming language such as Python. In this work, we use a language-model-infused scaffolding program to improve itself. We start with a seed "improver" that improves an input program according to a given utility function by querying a language model several times and returning the best solution. We then run this seed improver to improve itself. Across a small set of downstream tasks, the resulting improved improver generates programs with significantly better performance than its seed improver. Afterward, we analyze the variety of self-improvement strategies proposed by the language model, including beam search, genetic algorithms, and simulated annealing. Since the language models themselves are not altered, this is not fu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#23454;&#26102;&#23458;&#26381;&#21628;&#21483;&#20013;&#24515;&#23545;&#35805;&#20013;&#36830;&#32493;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;CusEmo&#65292;&#37319;&#29992;&#32500;&#24230;&#24773;&#24863;&#27880;&#37322;&#26041;&#27861;&#25429;&#25417;&#24773;&#24863;&#30340;&#24494;&#22937;&#12289;&#22797;&#26434;&#21644;&#36830;&#32493;&#24615;&#65292;&#24182;&#35299;&#20915;&#20102;&#24212;&#29992;&#20110;&#25968;&#25454;&#38598;&#26102;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.02281</link><description>&lt;p&gt;
&#23454;&#26102;&#23458;&#26381;&#21628;&#21483;&#20013;&#24515;&#23545;&#35805;&#20013;&#30340;&#31471;&#21040;&#31471;&#36830;&#32493;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
End-to-End Continuous Speech Emotion Recognition in Real-life Customer Service Call Center Conversations. (arXiv:2310.02281v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02281
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#23454;&#26102;&#23458;&#26381;&#21628;&#21483;&#20013;&#24515;&#23545;&#35805;&#20013;&#36830;&#32493;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;CusEmo&#65292;&#37319;&#29992;&#32500;&#24230;&#24773;&#24863;&#27880;&#37322;&#26041;&#27861;&#25429;&#25417;&#24773;&#24863;&#30340;&#24494;&#22937;&#12289;&#22797;&#26434;&#21644;&#36830;&#32493;&#24615;&#65292;&#24182;&#35299;&#20915;&#20102;&#24212;&#29992;&#20110;&#25968;&#25454;&#38598;&#26102;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21628;&#21483;&#20013;&#24515;&#23545;&#35805;&#20013;&#36827;&#34892;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#24050;&#32463;&#25104;&#20026;&#35780;&#20272;&#23458;&#25143;&#21644;&#20195;&#29702;&#20154;&#20043;&#38388;&#20114;&#21160;&#36136;&#37327;&#30340;&#26377;&#20215;&#20540;&#24037;&#20855;&#12290;&#19982;&#21463;&#25511;&#23454;&#39564;&#23460;&#29615;&#22659;&#19981;&#21516;&#65292;&#23454;&#38469;&#23545;&#35805;&#21457;&#29983;&#22312;&#19981;&#21463;&#25511;&#21046;&#30340;&#29615;&#22659;&#19979;&#65292;&#24182;&#21463;&#21040;&#24433;&#21709;&#24773;&#24863;&#34920;&#36798;&#30340;&#24773;&#22659;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#26500;&#24314;&#22823;&#35268;&#27169;&#23454;&#26102;&#25968;&#25454;&#38598;&#65288;CusEmo&#65289;&#29992;&#20110;&#23458;&#25143;&#26381;&#21153;&#21628;&#21483;&#20013;&#24515;&#23545;&#35805;&#20013;&#36830;&#32493;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#37319;&#29992;&#32500;&#24230;&#24773;&#24863;&#27880;&#37322;&#26041;&#27861;&#25429;&#25417;&#23454;&#38469;&#21628;&#21483;&#20013;&#24515;&#23545;&#35805;&#20013;&#24773;&#24863;&#30340;&#24494;&#22937;&#12289;&#22797;&#26434;&#21644;&#36830;&#32493;&#24615;&#65292;&#24182;&#23545;&#24773;&#22659;&#20449;&#24687;&#36827;&#34892;&#20102;&#27880;&#37322;&#12290;&#30740;&#31350;&#36824;&#35299;&#20915;&#20102;&#23558;&#31471;&#21040;&#31471;&#65288;E2E&#65289;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#31995;&#32479;&#24212;&#29992;&#20110;&#25968;&#25454;&#38598;&#26102;&#36935;&#21040;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#30830;&#23450;&#36866;&#24403;&#30340;&#26631;&#31614;&#37319;&#26679;&#29575;&#21644;&#36755;&#20837;&#29255;&#27573;&#38271;&#24230;&#65292;&#20197;&#21450;&#25972;&#21512;&#24773;&#22659;&#20449;&#24687;&#65288;&#23545;&#35805;&#32773;&#30340;&#24615;&#21035;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech Emotion recognition (SER) in call center conversations has emerged as a valuable tool for assessing the quality of interactions between clients and agents. In contrast to controlled laboratory environments, real-life conversations take place under uncontrolled conditions and are subject to contextual factors that influence the expression of emotions. In this paper, we present our approach to constructing a large-scale reallife dataset (CusEmo) for continuous SER in customer service call center conversations. We adopted the dimensional emotion annotation approach to capture the subtlety, complexity, and continuity of emotions in real-life call center conversations, while annotating contextual information. The study also addresses the challenges encountered during the application of the End-to-End (E2E) SER system to the dataset, including determining the appropriate label sampling rate and input segment length, as well as integrating contextual information (interlocutor's gender 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#20174;LLM&#20013;&#36951;&#24536;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#36890;&#36807;&#22312;Llama2-7b&#27169;&#22411;&#19978;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#30701;&#26102;&#38388;&#30340;&#24494;&#35843;&#20013;&#65292;&#25105;&#20204;&#21487;&#20197;&#26377;&#25928;&#22320;&#25830;&#38500;&#27169;&#22411;&#20851;&#20110;&#21704;&#21033;&#183;&#27874;&#29305;&#30456;&#20851;&#20869;&#23481;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#22312;&#20854;&#20182;&#24120;&#35265;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#24615;&#33021;&#20960;&#20046;&#19981;&#21464;&#12290;</title><link>http://arxiv.org/abs/2310.02238</link><description>&lt;p&gt;
&#35841;&#26159;&#21704;&#21033;&#183;&#27874;&#29305;&#65311;LLMs&#20013;&#30340;&#36817;&#20284;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;
Who's Harry Potter? Approximate Unlearning in LLMs. (arXiv:2310.02238v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#20174;LLM&#20013;&#36951;&#24536;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#36890;&#36807;&#22312;Llama2-7b&#27169;&#22411;&#19978;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#30701;&#26102;&#38388;&#30340;&#24494;&#35843;&#20013;&#65292;&#25105;&#20204;&#21487;&#20197;&#26377;&#25928;&#22320;&#25830;&#38500;&#27169;&#22411;&#20851;&#20110;&#21704;&#21033;&#183;&#27874;&#29305;&#30456;&#20851;&#20869;&#23481;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#22312;&#20854;&#20182;&#24120;&#35265;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#24615;&#33021;&#20960;&#20046;&#19981;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#22312;&#21253;&#21547;&#29256;&#26435;&#20869;&#23481;&#30340;&#28023;&#37327;&#20114;&#32852;&#32593;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#36825;&#32473;&#27169;&#22411;&#30340;&#24320;&#21457;&#32773;&#21644;&#29992;&#25143;&#65292;&#20197;&#21450;&#21407;&#22987;&#20316;&#32773;&#21644;&#20986;&#29256;&#21830;&#24102;&#26469;&#20102;&#27861;&#24459;&#21644;&#20262;&#29702;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#20174;LLM&#20013;&#36951;&#24536;&#35757;&#32451;&#25968;&#25454;&#30340;&#23376;&#38598;&#65292;&#32780;&#26080;&#38656;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;Llama2-7b&#27169;&#22411;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#25216;&#26415;&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;Meta&#26368;&#36817;&#24320;&#28304;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#12290;&#34429;&#28982;&#35813;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#33457;&#36153;&#20102;&#36229;&#36807;184K GPU&#23567;&#26102;&#65292;&#20294;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#22823;&#32422;1&#20010;GPU&#23567;&#26102;&#30340;&#24494;&#35843;&#20013;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#25830;&#38500;&#20102;&#27169;&#22411;&#29983;&#25104;&#25110;&#22238;&#24518;&#21704;&#21033;&#183;&#27874;&#29305;&#30456;&#20851;&#20869;&#23481;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#22312;&#24120;&#35265;&#22522;&#20934;&#27979;&#35797;&#65288;&#22914;Winogrande&#65292;Hellaswag&#65292;arc&#65292;boolq&#21644;piqa&#65289;&#19978;&#30340;&#34920;&#29616;&#20960;&#20046;&#27809;&#26377;&#21463;&#21040;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;HuggingFace&#19978;&#20844;&#24320;&#25552;&#20379;&#20102;&#25105;&#20204;&#30340;&#24494;&#35843;&#27169;&#22411;&#65292;&#20379;&#31038;&#21306;&#35780;&#20272;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#27425;&#23454;&#29616;&#36825;&#26679;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are trained on massive internet corpora that often contain copyrighted content. This poses legal and ethical challenges for the developers and users of these models, as well as the original authors and publishers. In this paper, we propose a novel technique for unlearning a subset of the training data from a LLM, without having to retrain it from scratch.  We evaluate our technique on the task of unlearning the Harry Potter books from the Llama2-7b model (a generative language model recently open-sourced by Meta). While the model took over 184K GPU-hours to pretrain, we show that in about 1 GPU hour of finetuning, we effectively erase the model's ability to generate or recall Harry Potter-related content, while its performance on common benchmarks (such as Winogrande, Hellaswag, arc, boolq and piqa) remains almost unaffected. We make our fine-tuned model publicly available on HuggingFace for community evaluation. To the best of our knowledge, this is the fi
&lt;/p&gt;</description></item><item><title>OceanGPT&#26159;&#39318;&#20010;&#19987;&#20026;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#35774;&#35745;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;DoInstruct&#26694;&#26550;&#23454;&#29616;&#33258;&#21160;&#33719;&#21462;&#28023;&#27915;&#39046;&#22495;&#25351;&#23548;&#25968;&#25454;&#12290;&#36825;&#19968;&#27169;&#22411;&#30340;&#24341;&#20837;&#22635;&#34917;&#20102;&#28023;&#27915;&#31185;&#23398;&#39046;&#22495;&#20013;&#23545;LLM&#30340;&#38656;&#27714;&#32570;&#21475;&#65292;&#24182;&#20026;&#28023;&#27915;&#31185;&#23398;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#24037;&#20855;&#21644;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.02031</link><description>&lt;p&gt;
OceanGPT&#65306;&#29992;&#20110;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
OceanGPT: A Large Language Model for Ocean Science Tasks. (arXiv:2310.02031v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02031
&lt;/p&gt;
&lt;p&gt;
OceanGPT&#26159;&#39318;&#20010;&#19987;&#20026;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#35774;&#35745;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;DoInstruct&#26694;&#26550;&#23454;&#29616;&#33258;&#21160;&#33719;&#21462;&#28023;&#27915;&#39046;&#22495;&#25351;&#23548;&#25968;&#25454;&#12290;&#36825;&#19968;&#27169;&#22411;&#30340;&#24341;&#20837;&#22635;&#34917;&#20102;&#28023;&#27915;&#31185;&#23398;&#39046;&#22495;&#20013;&#23545;LLM&#30340;&#38656;&#27714;&#32570;&#21475;&#65292;&#24182;&#20026;&#28023;&#27915;&#31185;&#23398;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#24037;&#20855;&#21644;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28023;&#27915;&#31185;&#23398;&#26159;&#25506;&#32034;&#20805;&#28385;&#29983;&#21629;&#21644;&#29983;&#29289;&#22810;&#26679;&#24615;&#30340;&#28023;&#27915;&#30340;&#31185;&#23398;&#65292;&#32771;&#34385;&#21040;&#28023;&#27915;&#35206;&#30422;&#20102;&#22320;&#29699;&#34920;&#38754;&#30340;70&#65285;&#20197;&#19978;&#65292;&#36825;&#19968;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#25913;&#21464;&#20102;&#31185;&#23398;&#30340;&#33539;&#24335;&#12290;&#23613;&#31649;&#22312;&#20854;&#20182;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#29616;&#26377;&#30340;LLM&#36890;&#24120;&#26080;&#27861;&#28385;&#36275;&#28023;&#27915;&#23398;&#23478;&#31561;&#39046;&#22495;&#19987;&#23478;&#30340;&#38656;&#27714;&#65292;&#21516;&#26102;&#23545;LLM&#22312;&#28023;&#27915;&#31185;&#23398;&#20013;&#30340;&#28508;&#21147;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#36825;&#20854;&#20013;&#30340;&#26681;&#26412;&#21407;&#22240;&#21487;&#33021;&#26159;&#28023;&#27915;&#25968;&#25454;&#30340;&#24222;&#22823;&#32780;&#22797;&#26434;&#30340;&#24615;&#36136;&#65292;&#20197;&#21450;&#23545;&#26356;&#39640;&#30340;&#31890;&#24230;&#21644;&#20016;&#23500;&#30340;&#30693;&#35782;&#30340;&#38656;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;&#39318;&#20010;&#28023;&#27915;&#39046;&#22495;&#30340;LLM&#8212;&#8212;OceanGPT&#65292;&#35813;&#27169;&#22411;&#25797;&#38271;&#21508;&#31181;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;DoInstruct&#65292;&#29992;&#20110;&#33258;&#21160;&#33719;&#21462;&#22823;&#37327;&#30340;&#28023;&#27915;&#39046;&#22495;&#25351;&#23548;&#25968;&#25454;&#65292;&#23427;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#30340;&#21327;&#20316;&#29983;&#25104;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ocean science, which delves into the oceans that are reservoirs of life and biodiversity, is of great significance given that oceans cover over 70% of our planet's surface. Recently, advances in Large Language Models (LLMs) have transformed the paradigm in science. Despite the success in other domains, current LLMs often fall short in catering to the needs of domain experts like oceanographers, and the potential of LLMs for ocean science is under-explored. The intrinsic reason may be the immense and intricate nature of ocean data as well as the necessity for higher granularity and richness in knowledge. To alleviate these issues, we introduce OceanGPT, the first-ever LLM in the ocean domain, which is expert in various ocean science tasks. We propose DoInstruct, a novel framework to automatically obtain a large volume of ocean domain instruction data, which generates instructions based on multi-agent collaboration. Additionally, we construct the first oceanography benchmark, OceanBench,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#19988;&#21442;&#25968;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#37325;&#22797;&#20351;&#29992;&#24494;&#35843;&#27169;&#22411;&#26469;&#22788;&#29702;&#19979;&#28216;&#20219;&#21153;&#65292;&#20943;&#36731;&#23384;&#20648;&#21644;&#26381;&#21153;&#36127;&#25285;&#65292;&#24182;&#25552;&#20986;&#20102;PERU-FFT&#26041;&#27861;&#29992;&#20110;&#37325;&#22797;&#20351;&#29992;&#20840;&#38754;&#24494;&#35843;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.01886</link><description>&lt;p&gt;
&#26377;&#25928;&#19988;&#21442;&#25968;&#39640;&#25928;&#30340;&#37325;&#22797;&#20351;&#29992;&#24494;&#35843;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Effective and Parameter-Efficient Reusing Fine-Tuned Models. (arXiv:2310.01886v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#19988;&#21442;&#25968;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#37325;&#22797;&#20351;&#29992;&#24494;&#35843;&#27169;&#22411;&#26469;&#22788;&#29702;&#19979;&#28216;&#20219;&#21153;&#65292;&#20943;&#36731;&#23384;&#20648;&#21644;&#26381;&#21153;&#36127;&#25285;&#65292;&#24182;&#25552;&#20986;&#20102;PERU-FFT&#26041;&#27861;&#29992;&#20110;&#37325;&#22797;&#20351;&#29992;&#20840;&#38754;&#24494;&#35843;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#22312;&#32447;&#25552;&#20379;&#30340;&#39044;&#35757;&#32451;&#22823;&#35268;&#27169;&#27169;&#22411;&#22312;&#20256;&#36882;&#21040;&#19979;&#28216;&#20219;&#21153;&#20013;&#21464;&#24471;&#38750;&#24120;&#26377;&#25928;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#21508;&#31181;&#22312;&#36825;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#24494;&#35843;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#20063;&#21487;&#20379;&#20844;&#20247;&#20351;&#29992;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#30001;&#20110;&#25910;&#38598;&#20219;&#21153;&#29305;&#23450;&#25968;&#25454;&#32791;&#26102;&#19988;&#24494;&#35843;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#35745;&#31639;&#22797;&#26434;&#65292;&#21487;&#20197;&#37325;&#22797;&#20351;&#29992;&#20219;&#21153;&#29305;&#23450;&#24494;&#35843;&#27169;&#22411;&#26469;&#22788;&#29702;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20026;&#27599;&#20010;&#20219;&#21153;&#20351;&#29992;&#19968;&#20010;&#27169;&#22411;&#20250;&#32473;&#23384;&#20648;&#21644;&#26381;&#21153;&#24102;&#26469;&#24040;&#22823;&#36127;&#25285;&#12290;&#26368;&#36817;&#65292;&#26377;&#35768;&#22810;&#26080;&#38656;&#35757;&#32451;&#19988;&#21442;&#25968;&#39640;&#25928;&#30340;&#26041;&#27861;&#34987;&#25552;&#20986;&#65292;&#23558;&#22810;&#20010;&#24494;&#35843;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#37325;&#22797;&#20351;&#29992;&#21040;&#19968;&#20010;&#22810;&#20219;&#21153;&#27169;&#22411;&#20013;&#12290;&#28982;&#32780;&#65292;&#19982;&#20026;&#27599;&#20010;&#20219;&#21153;&#20351;&#29992;&#24494;&#35843;&#27169;&#22411;&#30456;&#27604;&#65292;&#36825;&#20123;&#26041;&#27861;&#34920;&#29616;&#20986;&#36739;&#22823;&#30340;&#20934;&#30830;&#24615;&#24046;&#36317;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21442;&#25968;&#39640;&#25928;&#26041;&#27861;&#26469;&#37325;&#22797;&#20351;&#29992;&#24494;&#35843;&#27169;&#22411;&#12290;&#38024;&#23545;&#37325;&#22797;&#20351;&#29992;&#20840;&#38754;&#24494;&#35843;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PERU-FFT&#65292;&#36890;&#36807;&#23558;&#31232;&#30095;&#20219;&#21153;&#21521;&#37327;&#27880;&#20837;&#21040;&#19968;&#20010;mer&#27169;&#22411;&#20013;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many pre-trained large-scale models provided online have become highly effective in transferring to downstream tasks. At the same time, various task-specific models fine-tuned on these pre-trained models are available online for public use. In practice, as collecting task-specific data is labor-intensive and fine-tuning the large pre-trained models is computationally expensive, one can reuse task-specific finetuned models to deal with downstream tasks. However, using a model per task causes a heavy burden on storage and serving. Recently, many training-free and parameter-efficient methods have been proposed for reusing multiple fine-tuned task-specific models into a single multi-task model. However, these methods exhibit a large accuracy gap compared with using a fine-tuned model per task. In this paper, we propose Parameter-Efficient methods for ReUsing (PERU) fine-tuned models. For reusing Fully Fine-Tuned (FFT) models, we propose PERU-FFT by injecting a sparse task vector into a mer
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#35757;&#32451;&#33258;&#21160;&#21457;&#38899;&#35780;&#20272;&#27169;&#22411;&#12290;&#35813;&#25439;&#22833;&#20989;&#25968;&#26088;&#22312;&#22312;&#20445;&#25345;&#38899;&#20301;&#31867;&#21035;&#20043;&#38388;&#26356;&#22909;&#30340;&#38899;&#20301;&#24046;&#24322;&#30340;&#21516;&#26102;&#65292;&#32771;&#34385;&#21040;&#22238;&#24402;&#30446;&#26631;&#36755;&#20986;&#30340;&#26377;&#24207;&#20851;&#31995;&#12290;&#36890;&#36807;&#24341;&#20837;&#38899;&#20301;-&#21306;&#20998;&#27491;&#21017;&#21270;&#22120;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#21457;&#38899;&#35780;&#20272;&#30340;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.01839</link><description>&lt;p&gt;
&#20445;&#30041;&#38899;&#20301;&#24046;&#24322;&#30340;&#24207;&#25968;&#22238;&#24402;&#65306;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#21457;&#38899;&#35780;&#20272;&#30340;&#26032;&#22411;&#25439;&#22833;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Preserving Phonemic Distinctions for Ordinal Regression: A Novel Loss Function for Automatic Pronunciation Assessment. (arXiv:2310.01839v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01839
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#35757;&#32451;&#33258;&#21160;&#21457;&#38899;&#35780;&#20272;&#27169;&#22411;&#12290;&#35813;&#25439;&#22833;&#20989;&#25968;&#26088;&#22312;&#22312;&#20445;&#25345;&#38899;&#20301;&#31867;&#21035;&#20043;&#38388;&#26356;&#22909;&#30340;&#38899;&#20301;&#24046;&#24322;&#30340;&#21516;&#26102;&#65292;&#32771;&#34385;&#21040;&#22238;&#24402;&#30446;&#26631;&#36755;&#20986;&#30340;&#26377;&#24207;&#20851;&#31995;&#12290;&#36890;&#36807;&#24341;&#20837;&#38899;&#20301;-&#21306;&#20998;&#27491;&#21017;&#21270;&#22120;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#21457;&#38899;&#35780;&#20272;&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21457;&#38899;&#35780;&#20272;&#65288;APA&#65289;&#21487;&#20197;&#37327;&#21270;&#31532;&#20108;&#35821;&#35328;&#65288;L2&#65289;&#23398;&#20064;&#32773;&#22312;&#35821;&#35328;&#20013;&#30340;&#21457;&#38899;&#29087;&#32451;&#31243;&#24230;&#12290;&#24120;&#35265;&#30340;APA&#26041;&#27861;&#36890;&#24120;&#21033;&#29992;&#31070;&#32463;&#27169;&#22411;&#21644;&#22238;&#24402;&#25439;&#22833;&#20989;&#25968;&#65288;&#22914;&#22343;&#26041;&#35823;&#24046;&#25439;&#22833;&#65289;&#36827;&#34892;&#29087;&#32451;&#27700;&#24179;&#39044;&#27979;&#12290;&#23613;&#31649;&#22823;&#22810;&#25968;&#22238;&#24402;&#27169;&#22411;&#21487;&#20197;&#22312;&#29305;&#24449;&#31354;&#38388;&#26377;&#25928;&#22320;&#25429;&#25417;&#21040;&#29087;&#32451;&#27700;&#24179;&#30340;&#26377;&#24207;&#24615;&#65292;&#20294;&#23427;&#20204;&#38754;&#20020;&#19968;&#20010;&#20027;&#35201;&#38556;&#30861;&#65292;&#21363;&#19981;&#21516;&#38899;&#32032;&#31867;&#21035;&#30340;&#30456;&#21516;&#29087;&#32451;&#27700;&#24179;&#24517;&#28982;&#34987;&#36843;&#38752;&#24471;&#24456;&#36817;&#65292;&#20445;&#30041;&#20102;&#26356;&#23569;&#30340;&#38899;&#20301;&#8212;&#21306;&#20998;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#22522;&#20110;&#22238;&#24402;&#30340;APA&#27169;&#22411;&#30340;&#38899;&#20301;&#23545;&#27604;&#24207;&#25968;&#65288;PCO&#65289;&#25439;&#22833;&#65292;&#26088;&#22312;&#22312;&#20445;&#25345;&#38899;&#20301;&#31867;&#21035;&#20043;&#38388;&#26356;&#22909;&#30340;&#38899;&#20301;&#24046;&#24322;&#30340;&#21516;&#26102;&#65292;&#32771;&#34385;&#22238;&#24402;&#30446;&#26631;&#36755;&#20986;&#30340;&#26377;&#24207;&#20851;&#31995;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#19968;&#20010;&#38899;&#20301;&#8212;&#21306;&#20998;&#27491;&#21017;&#21270;&#22120;&#24341;&#20837;&#21040;&#22343;&#26041;&#35823;&#24046;&#25439;&#22833;&#20013;&#65292;&#40723;&#21169;...&#65288;&#24453;&#32493;&#65289;
&lt;/p&gt;
&lt;p&gt;
Automatic pronunciation assessment (APA) manages to quantify the pronunciation proficiency of a second language (L2) learner in a language. Prevailing approaches to APA normally leverage neural models trained with a regression loss function, such as the mean-squared error (MSE) loss, for proficiency level prediction. Despite most regression models can effectively capture the ordinality of proficiency levels in the feature space, they are confronted with a primary obstacle that different phoneme categories with the same proficiency level are inevitably forced to be close to each other, retaining less phoneme-discriminative information. On account of this, we devise a phonemic contrast ordinal (PCO) loss for training regression-based APA models, which aims to preserve better phonemic distinctions between phoneme categories meanwhile considering ordinal relationships of the regression target output. Specifically, we introduce a phoneme-distinct regularizer into the MSE loss, which encoura
&lt;/p&gt;</description></item><item><title>LLM&#20284;&#20046;&#20855;&#26377;&#20016;&#23500;&#30340;&#30693;&#35782;&#21644;&#36866;&#24212;&#22810;&#31181;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#20294;&#25105;&#20204;&#19981;&#33021;&#23436;&#20840;&#20449;&#20219;&#23427;&#20204;&#30340;&#22238;&#31572;&#65292;&#22240;&#20026;&#23427;&#20204;&#20250;&#20986;&#29616;&#24187;&#35273;&#65292;&#21363;&#25423;&#36896;&#19981;&#23384;&#22312;&#30340;&#20107;&#23454;&#20197;&#27450;&#39575;&#29992;&#25143;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#30001;&#38543;&#26426;&#26631;&#35760;&#32452;&#25104;&#30340;&#26080;&#24847;&#20041;&#25552;&#31034;&#20063;&#33021;&#24341;&#36215;LLM&#20135;&#29983;&#24187;&#35273;&#22238;&#24212;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#26041;&#24335;&#30340;&#33258;&#21160;&#24187;&#35273;&#35302;&#21457;&#26041;&#27861;&#20316;&#20026;&#24187;&#35273;&#25915;&#20987;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2310.01469</link><description>&lt;p&gt;
LLM&#35854;&#35328;: &#24187;&#35273;&#19981;&#26159;&#28431;&#27934;&#65292;&#32780;&#26159;&#23545;&#25239;&#26679;&#26412;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLM Lies: Hallucinations are not Bugs, but Features as Adversarial Examples. (arXiv:2310.01469v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01469
&lt;/p&gt;
&lt;p&gt;
LLM&#20284;&#20046;&#20855;&#26377;&#20016;&#23500;&#30340;&#30693;&#35782;&#21644;&#36866;&#24212;&#22810;&#31181;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#20294;&#25105;&#20204;&#19981;&#33021;&#23436;&#20840;&#20449;&#20219;&#23427;&#20204;&#30340;&#22238;&#31572;&#65292;&#22240;&#20026;&#23427;&#20204;&#20250;&#20986;&#29616;&#24187;&#35273;&#65292;&#21363;&#25423;&#36896;&#19981;&#23384;&#22312;&#30340;&#20107;&#23454;&#20197;&#27450;&#39575;&#29992;&#25143;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#30001;&#38543;&#26426;&#26631;&#35760;&#32452;&#25104;&#30340;&#26080;&#24847;&#20041;&#25552;&#31034;&#20063;&#33021;&#24341;&#36215;LLM&#20135;&#29983;&#24187;&#35273;&#22238;&#24212;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#26041;&#24335;&#30340;&#33258;&#21160;&#24187;&#35273;&#35302;&#21457;&#26041;&#27861;&#20316;&#20026;&#24187;&#35273;&#25915;&#20987;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#21253;&#25324;GPT-3.5&#12289;LLaMA&#21644;PaLM&#65292;&#20284;&#20046;&#20855;&#26377;&#20016;&#23500;&#30340;&#30693;&#35782;&#21644;&#36866;&#24212;&#22810;&#31181;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#20173;&#28982;&#19981;&#33021;&#23436;&#20840;&#20449;&#20219;&#23427;&#20204;&#30340;&#22238;&#31572;&#65292;&#22240;&#20026;LLM&#20250;&#20986;&#29616;&#24187;&#35273;&#65292;&#21363;&#25423;&#36896;&#19981;&#23384;&#22312;&#30340;&#20107;&#23454;&#20197;&#27450;&#39575;&#29992;&#25143;&#32780;&#19981;&#34987;&#23519;&#35273;&#12290;&#24187;&#35273;&#23384;&#22312;&#30340;&#21407;&#22240;&#21644;&#26222;&#36941;&#24615;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#30001;&#38543;&#26426;&#26631;&#35760;&#32452;&#25104;&#30340;&#26080;&#24847;&#20041;&#25552;&#31034;&#20063;&#33021;&#24341;&#36215;LLM&#20135;&#29983;&#24187;&#35273;&#22238;&#24212;&#12290;&#36825;&#20010;&#29616;&#35937;&#36843;&#20351;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#24187;&#35273;&#21487;&#33021;&#26159;&#23545;&#25239;&#26679;&#26412;&#30340;&#21478;&#19968;&#31181;&#35270;&#35282;&#65292;&#24182;&#19988;&#23427;&#19982;&#24120;&#35268;&#30340;&#23545;&#25239;&#26679;&#26412;&#20855;&#26377;&#31867;&#20284;&#30340;&#29305;&#24449;&#65292;&#20316;&#20026;LLM&#30340;&#22522;&#26412;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20197;&#23545;&#25239;&#30340;&#26041;&#24335;&#23558;&#33258;&#21160;&#24187;&#35273;&#35302;&#21457;&#26041;&#27861;&#24418;&#24335;&#21270;&#20026;&#24187;&#35273;&#25915;&#20987;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#34987;&#25915;&#20987;&#30340;&#23545;&#25239;&#25552;&#31034;&#30340;&#22522;&#26412;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#24050;&#22312;GitHub&#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), including GPT-3.5, LLaMA, and PaLM, seem to be knowledgeable and able to adapt to many tasks. However, we still can not completely trust their answer, since LLMs suffer from hallucination--fabricating non-existent facts to cheat users without perception. And the reasons for their existence and pervasiveness remain unclear. In this paper, we demonstrate that non-sense prompts composed of random tokens can also elicit the LLMs to respond with hallucinations. This phenomenon forces us to revisit that hallucination may be another view of adversarial examples, and it shares similar features with conventional adversarial examples as the basic feature of LLMs. Therefore, we formalize an automatic hallucination triggering method as the hallucination attack in an adversarial way. Finally, we explore basic feature of attacked adversarial prompts and propose a simple yet effective defense strategy. Our code is released on GitHub.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#21521;&#27861;&#23448;&#25552;&#20986;&#19968;&#31995;&#21015;&#26597;&#35810;&#26469;&#35780;&#20272;LLMs&#30340;&#23545;&#35805;&#25512;&#29702;&#21644;&#35268;&#21010;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#30340;LLMs&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2310.01468</link><description>&lt;p&gt;
&#23454;&#20307;&#25512;&#26029;&#31454;&#25216;&#22330;&#65306;&#25506;&#31350;LLMs&#30340;&#23545;&#35805;&#25512;&#29702;&#21644;&#35268;&#21010;&#33021;&#21147;&#30340;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
The Entity-Deduction Arena: A playground for probing the conversational reasoning and planning capabilities of LLMs. (arXiv:2310.01468v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#21521;&#27861;&#23448;&#25552;&#20986;&#19968;&#31995;&#21015;&#26597;&#35810;&#26469;&#35780;&#20272;LLMs&#30340;&#23545;&#35805;&#25512;&#29702;&#21644;&#35268;&#21010;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#30340;LLMs&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22238;&#31572;&#26126;&#30830;&#25552;&#38382;&#26102;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#20020;&#21547;&#31946;&#19981;&#28165;&#30340;&#26597;&#35810;&#26102;&#65292;&#23427;&#20204;&#21487;&#33021;&#34892;&#20026;&#38590;&#20197;&#39044;&#27979;&#24182;&#20135;&#29983;&#38169;&#35823;&#30340;&#36755;&#20986;&#12290;&#36825;&#20984;&#26174;&#20102;&#38656;&#35201;&#24320;&#21457;&#33021;&#22815;&#25552;&#20986;&#28548;&#28165;&#38382;&#39064;&#20197;&#26377;&#25928;&#35299;&#20915;&#27495;&#20041;&#30340;&#26234;&#33021;&#20195;&#29702;&#30340;&#38656;&#27714;&#12290;&#36825;&#31181;&#33021;&#21147;&#38656;&#35201;&#23545;&#22810;&#20010;&#23545;&#35805;&#36718;&#27425;&#36827;&#34892;&#22797;&#26434;&#30340;&#29702;&#35299;&#12289;&#29366;&#24577;&#36319;&#36394;&#12289;&#25512;&#29702;&#21644;&#35268;&#21010;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#27979;&#37327;&#36825;&#31181;&#33021;&#21147;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26367;&#20195;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#21521;&#27861;&#23448;&#25552;&#20986;&#19968;&#31995;&#21015;&#26597;&#35810;&#65292;&#35780;&#20272;&#20102;LLMs&#25512;&#26029;&#33258;&#24049;&#19981;&#30693;&#36947;&#20294;&#34987;&#27861;&#23448;&#25581;&#31034;&#30340;&#23454;&#20307;&#30340;&#33021;&#21147;&#12290;&#36825;&#20010;&#8220;&#23454;&#20307;&#25512;&#26029;&#28216;&#25103;&#8221;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#25506;&#31350;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#35805;&#25512;&#29702;&#21644;&#35268;&#21010;&#33021;&#21147;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;&#21508;&#31181;LLMs&#65292;&#24182;&#21457;&#29616;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#23427;&#20204;&#30340;&#24615;&#33021;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#25105;&#20204;&#21457;&#29616;&#24378;&#22823;&#30340;LLMs...
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are currently effective at answering questions that are clearly asked. However, when faced with ambiguous queries they can act unpredictably and produce incorrect outputs. This underscores the need for the development of intelligent agents capable of asking clarification questions to resolve ambiguities effectively. This capability requires complex understanding, state tracking, reasoning and planning over multiple conversational turns. However, directly measuring this can be challenging. In this paper, we offer a surrogate problem which assesses an LLMs's capability to deduce an entity unknown to itself, but revealed to a judge, by asking the judge a series of queries. This \textit{entity-deducing game} can serve as an evaluation framework to probe the conversational reasoning and planning capabilities of language models. We systematically evaluate various LLMs and discover significant differences in their performance on this task. We find that strong LLMs
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#30340;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#21457;&#29616;PEFT&#26131;&#21463;&#29305;&#27931;&#20234;&#25915;&#20987;&#12290;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#26041;&#24335;PETA&#65292;&#24182;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#21644;&#35302;&#21457;&#22120;&#35774;&#35745;&#20013;&#36827;&#34892;&#24191;&#27867;&#27979;&#35797;&#65292;&#21457;&#29616;PETA&#33021;&#22815;&#22312;&#25915;&#20987;&#25104;&#21151;&#29575;&#21644;&#26410;&#21463;&#24433;&#21709;&#30340;&#20934;&#30830;&#24615;&#26041;&#38754;&#21462;&#24471;&#26377;&#25928;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.00648</link><description>&lt;p&gt;
&#26356;&#23569;&#23601;&#24847;&#21619;&#30528;&#26356;&#22810;: &#23545;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#29305;&#27931;&#20234;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Fewer is More: Trojan Attacks on Parameter-Efficient Fine-Tuning. (arXiv:2310.00648v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#30340;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#21457;&#29616;PEFT&#26131;&#21463;&#29305;&#27931;&#20234;&#25915;&#20987;&#12290;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#26041;&#24335;PETA&#65292;&#24182;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#21644;&#35302;&#21457;&#22120;&#35774;&#35745;&#20013;&#36827;&#34892;&#24191;&#27867;&#27979;&#35797;&#65292;&#21457;&#29616;PETA&#33021;&#22815;&#22312;&#25915;&#20987;&#25104;&#21151;&#29575;&#21644;&#26410;&#21463;&#24433;&#21709;&#30340;&#20934;&#30830;&#24615;&#26041;&#38754;&#21462;&#24471;&#26377;&#25928;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#21487;&#20197;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#39640;&#25928;&#22320;&#36866;&#24212;&#21040;&#29305;&#23450;&#20219;&#21153;&#20013;&#12290;&#36890;&#36807;&#20165;&#24494;&#35843;&#19968;&#23567;&#37096;&#20998;&#65288;&#39069;&#22806;&#30340;&#65289;&#21442;&#25968;&#65292;PEFT&#23454;&#29616;&#20102;&#19982;&#20840;&#38754;&#24494;&#35843;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23427;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;PEFT&#30340;&#23433;&#20840;&#24615;&#24433;&#21709;&#20173;&#28982;&#40092;&#20026;&#20154;&#30693;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#21021;&#27493;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;PEFT&#23545;&#29305;&#27931;&#20234;&#25915;&#20987;&#30340;&#29420;&#29305;&#26131;&#21463;&#25915;&#20987;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PETA&#65292;&#19968;&#31181;&#36890;&#36807;&#21452;&#23618;&#20248;&#21270;&#32771;&#34385;&#19979;&#28216;&#36866;&#24212;&#30340;&#26032;&#22411;&#25915;&#20987;&#26041;&#24335;&#65306;&#19978;&#23618;&#30446;&#26631;&#23558;&#21518;&#38376;&#23884;&#20837;PLM&#20013;&#65292;&#32780;&#19979;&#23618;&#30446;&#26631;&#27169;&#25311;PEFT&#20197;&#20445;&#30041;PLM&#30340;&#20219;&#21153;&#29305;&#23450;&#24615;&#33021;&#12290;&#36890;&#36807;&#23545;&#22810;&#31181;&#19979;&#28216;&#20219;&#21153;&#21644;&#35302;&#21457;&#22120;&#35774;&#35745;&#30340;&#24191;&#27867;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;PETA&#22312;&#25915;&#20987;&#25104;&#21151;&#29575;&#21644;&#26410;&#21463;&#24433;&#21709;&#30340;&#24178;&#20928;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#21363;&#20351;&#21463;&#23475;&#29992;&#25143;&#22312;&#20351;&#29992;&#32431;&#20928;&#25968;&#25454;&#23545;&#24102;&#26377;&#21518;&#38376;&#30340;PLM&#36827;&#34892;PEFT&#21518;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter-efficient fine-tuning (PEFT) enables efficient adaptation of pre-trained language models (PLMs) to specific tasks. By tuning only a minimal set of (extra) parameters, PEFT achieves performance comparable to full fine-tuning. However, despite its prevalent use, the security implications of PEFT remain largely unexplored. In this paper, we conduct a pilot study revealing that PEFT exhibits unique vulnerability to trojan attacks. Specifically, we present PETA, a novel attack that accounts for downstream adaptation through bilevel optimization: the upper-level objective embeds the backdoor into a PLM while the lower-level objective simulates PEFT to retain the PLM's task-specific performance. With extensive evaluation across a variety of downstream tasks and trigger designs, we demonstrate PETA's effectiveness in terms of both attack success rate and unaffected clean accuracy, even after the victim user performs PEFT over the backdoored PLM using untainted data. Moreover, we empi
&lt;/p&gt;</description></item><item><title>ToRA&#26159;&#19968;&#31181;&#38598;&#25104;&#24037;&#20855;&#30340;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#25512;&#29702;&#20195;&#29702;&#65292;&#36890;&#36807;&#32467;&#21512;&#35821;&#35328;&#30340;&#20998;&#26512;&#33021;&#21147;&#21644;&#24037;&#20855;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#25968;&#23398;&#25512;&#29702;&#30340;&#24615;&#33021;&#65292;&#22312;&#22810;&#20010;&#25968;&#23398;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;13%-19%&#30340;&#24179;&#22343;&#32477;&#23545;&#25913;&#36827;&#29575;&#65292;&#24182;&#22312;&#31454;&#36187;&#32423;&#25968;&#25454;&#38598;MATH&#19978;&#36798;&#21040;&#20102;44.6%&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.17452</link><description>&lt;p&gt;
ToRA&#65306;&#19968;&#31181;&#38598;&#25104;&#24037;&#20855;&#30340;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#25512;&#29702;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving. (arXiv:2309.17452v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17452
&lt;/p&gt;
&lt;p&gt;
ToRA&#26159;&#19968;&#31181;&#38598;&#25104;&#24037;&#20855;&#30340;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#25512;&#29702;&#20195;&#29702;&#65292;&#36890;&#36807;&#32467;&#21512;&#35821;&#35328;&#30340;&#20998;&#26512;&#33021;&#21147;&#21644;&#24037;&#20855;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#25968;&#23398;&#25512;&#29702;&#30340;&#24615;&#33021;&#65292;&#22312;&#22810;&#20010;&#25968;&#23398;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;13%-19%&#30340;&#24179;&#22343;&#32477;&#23545;&#25913;&#36827;&#29575;&#65292;&#24182;&#22312;&#31454;&#36187;&#32423;&#25968;&#25454;&#38598;MATH&#19978;&#36798;&#21040;&#20102;44.6%&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#22312;&#22797;&#26434;&#30340;&#25968;&#23398;&#38382;&#39064;&#19978;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#38598;&#25104;&#24037;&#20855;&#30340;&#25512;&#29702;&#20195;&#29702;ToRA&#65292;&#23427;&#36890;&#36807;&#26080;&#32541;&#22320;&#23558;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#19982;&#22806;&#37096;&#24037;&#20855;&#65288;&#20363;&#22914;&#35745;&#31639;&#24211;&#21644;&#31526;&#21495;&#27714;&#35299;&#22120;&#65289;&#30340;&#21033;&#29992;&#30456;&#32467;&#21512;&#65292;&#20174;&#32780;&#23558;&#35821;&#35328;&#30340;&#20998;&#26512;&#33021;&#21147;&#19982;&#24037;&#20855;&#30340;&#35745;&#31639;&#25928;&#29575;&#34701;&#21512;&#22312;&#19968;&#36215;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#23398;&#38382;&#39064;&#12290;&#20026;&#20102;&#35757;&#32451;ToRA&#65292;&#25105;&#20204;&#31934;&#36873;&#20102;&#25968;&#23398;&#25968;&#25454;&#38598;&#19978;&#30340;&#20114;&#21160;&#24037;&#20855;&#20351;&#29992;&#36712;&#36857;&#65292;&#24212;&#29992;&#27169;&#20223;&#23398;&#20064;&#20110;&#27880;&#37322;&#65292;&#24182;&#25552;&#20986;&#36755;&#20986;&#31354;&#38388;&#25972;&#24418;&#26469;&#36827;&#19968;&#27493;&#25913;&#36827;&#27169;&#22411;&#30340;&#25512;&#29702;&#34892;&#20026;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;ToRA&#27169;&#22411;&#22312;10&#20010;&#28085;&#30422;&#21508;&#31181;&#35268;&#27169;&#30340;&#25968;&#23398;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#24320;&#28304;&#27169;&#22411;&#65292;&#24179;&#22343;&#32477;&#23545;&#25913;&#36827;&#29575;&#36798;&#21040;13%&#33267;19%&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;ToRA-7B &#22312;&#31454;&#36187;&#32423;&#25968;&#25454;&#38598;MATH&#19978;&#36798;&#21040;&#20102;44.6%&#65292;&#36229;&#36234;&#20102;&#26368;&#20339;&#24320;&#28304;&#27169;&#22411;WizardMath&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have made significant progress in various language tasks, yet they still struggle with complex mathematics. In this paper, we propose ToRA a series of Tool-integrated Reasoning Agents designed to solve challenging mathematical problems by seamlessly integrating natural language reasoning with the utilization of external tools (e.g., computation libraries and symbolic solvers), thereby amalgamating the analytical prowess of language and the computational efficiency of tools. To train ToRA, we curate interactive tool-use trajectories on mathematical datasets, apply imitation learning on the annotations, and propose output space shaping to further refine models' reasoning behavior. As a result, ToRA models significantly outperform open-source models on 10 mathematical reasoning datasets across all scales with 13%-19% absolute improvements on average. Notably, ToRA-7B reaches 44.6% on the competition-level dataset MATH, surpassing the best open-source model WizardMath
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;DeepSpeed-Ulysses&#65292;&#19968;&#31181;&#29992;&#20110;&#23454;&#29616;&#20855;&#22791;&#26497;&#38271;&#24207;&#21015;&#38271;&#24230;&#30340;&#39640;&#25928;&#21487;&#25193;&#23637;LLM&#35757;&#32451;&#30340;&#26032;&#39062;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.14509</link><description>&lt;p&gt;
DeepSpeed Ulysses&#65306;&#29992;&#20110;&#35757;&#32451;&#26497;&#38271;&#24207;&#21015;Transformer&#27169;&#22411;&#30340;&#31995;&#32479;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models. (arXiv:2309.14509v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14509
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;DeepSpeed-Ulysses&#65292;&#19968;&#31181;&#29992;&#20110;&#23454;&#29616;&#20855;&#22791;&#26497;&#38271;&#24207;&#21015;&#38271;&#24230;&#30340;&#39640;&#25928;&#21487;&#25193;&#23637;LLM&#35757;&#32451;&#30340;&#26032;&#39062;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35745;&#31639;&#21487;&#20197;&#36890;&#36807;&#25209;&#37327;&#22823;&#23567;&#12289;&#38544;&#34255;&#32500;&#24230;&#12289;&#23618;&#25968;&#21644;&#24207;&#21015;&#38271;&#24230;&#26469;&#25551;&#36848;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#21152;&#36895;LLM&#35757;&#32451;&#30340;&#31995;&#32479;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#21069;&#19977;&#20010;&#32500;&#24230;&#19978;&#65306;&#25209;&#37327;&#22823;&#23567;&#30340;&#25968;&#25454;&#24182;&#34892;&#21270;&#12289;&#38544;&#34255;&#23610;&#23544;&#30340;&#24352;&#37327;&#24182;&#34892;&#21270;&#20197;&#21450;&#27169;&#22411;&#28145;&#24230;&#25110;&#23618;&#25968;&#30340;&#27969;&#27700;&#32447;&#24182;&#34892;&#21270;&#12290;&#36825;&#20123;&#34987;&#24191;&#27867;&#30740;&#31350;&#30340;&#24182;&#34892;&#24418;&#24335;&#24182;&#19981;&#38024;&#23545;&#38271;&#24207;&#21015;Transformer&#27169;&#22411;&#36827;&#34892;&#20248;&#21270;&#12290;&#37492;&#20110;&#38271;&#24207;&#21015;LLM&#22312;&#23454;&#38469;&#24212;&#29992;&#38656;&#27714;&#19978;&#30340;&#37325;&#35201;&#24615;&#65292;&#24207;&#21015;&#24182;&#34892;&#21270;&#24341;&#36215;&#20102;&#37325;&#26032;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24207;&#21015;&#24182;&#34892;&#21270;&#24037;&#20316;&#21463;&#21040;&#20869;&#23384;&#36890;&#20449;&#25928;&#29575;&#30340;&#38480;&#21046;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#38271;&#24207;&#21015;&#22823;&#27169;&#22411;&#19978;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DeepSpeed-Ulysses&#65292;&#19968;&#31181;&#26032;&#39062;&#12289;&#20415;&#25658;&#19988;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#20855;&#22791;&#26497;&#38271;&#24207;&#21015;&#38271;&#24230;&#30340;&#39640;&#25928;&#21487;&#25193;&#23637;LLM&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computation in a typical Transformer-based large language model (LLM) can be characterized by batch size, hidden dimension, number of layers, and sequence length. Until now, system works for accelerating LLM training have focused on the first three dimensions: data parallelism for batch size, tensor parallelism for hidden size and pipeline parallelism for model depth or layers. These widely studied forms of parallelism are not targeted or optimized for long sequence Transformer models. Given practical application needs for long sequence LLM, renewed attentions are being drawn to sequence parallelism. However, existing works in sequence parallelism are constrained by memory-communication inefficiency, limiting their scalability to long sequence large models. In this work, we introduce DeepSpeed-Ulysses, a novel, portable and effective methodology for enabling highly efficient and scalable LLM training with extremely long sequence length. DeepSpeed-Ulysses at its core partitions input da
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65292;&#24341;&#20837;&#20102;EMT&#26041;&#27861;&#26469;&#35780;&#20272;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#21457;&#29616;&#22312;&#26631;&#20934;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#20960;&#20046;&#25152;&#26377;&#35780;&#20272;&#30340;&#27169;&#22411;&#37117;&#26080;&#27861;&#20445;&#25345;&#19982;&#35270;&#35273;&#32534;&#30721;&#22120;&#30456;&#21516;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#26089;&#26399;&#24494;&#35843;&#38454;&#27573;&#23545;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2309.10313</link><description>&lt;p&gt;
&#23545;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#36827;&#34892;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Investigating the Catastrophic Forgetting in Multimodal Large Language Models. (arXiv:2309.10313v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65292;&#24341;&#20837;&#20102;EMT&#26041;&#27861;&#26469;&#35780;&#20272;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#21457;&#29616;&#22312;&#26631;&#20934;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#20960;&#20046;&#25152;&#26377;&#35780;&#20272;&#30340;&#27169;&#22411;&#37117;&#26080;&#27861;&#20445;&#25345;&#19982;&#35270;&#35273;&#32534;&#30721;&#22120;&#30456;&#21516;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#26089;&#26399;&#24494;&#35843;&#38454;&#27573;&#23545;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;GPT4&#30340;&#25104;&#21151;&#20043;&#21518;&#65292;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#30740;&#31350;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#36825;&#19968;&#30740;&#31350;&#26041;&#21521;&#20391;&#37325;&#20110;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;LLM&#21644;&#35270;&#35273;&#27169;&#22411;&#26469;&#24320;&#21457;&#36890;&#29992;&#30340;LLM&#12290;&#28982;&#32780;&#65292;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#21363;&#24494;&#35843;&#27169;&#22411;&#26080;&#27861;&#20445;&#25345;&#19982;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#20284;&#30340;&#24615;&#33021;&#27700;&#24179;&#65292;&#20173;&#28982;&#26159;&#22810;&#27169;&#24577;LLM&#65288;MLLM&#65289;&#20013;&#30340;&#19968;&#20010;&#22266;&#26377;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;EMT&#65306;&#29992;&#20110;&#35780;&#20272;MLLM&#20013;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#23558;&#27599;&#20010;MLLM&#20316;&#20026;&#19968;&#20010;&#22270;&#20687;&#20998;&#31867;&#22120;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#39318;&#20808;&#24212;&#29992;EMT&#26469;&#35780;&#20272;&#20960;&#20010;&#24320;&#28304;&#30340;&#24494;&#35843;MLLM&#65292;&#24182;&#21457;&#29616;&#20960;&#20046;&#25152;&#26377;&#35780;&#20272;&#30340;MLLM&#22312;&#26631;&#20934;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#26080;&#27861;&#20445;&#25345;&#19982;&#20182;&#20204;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#30456;&#21516;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32487;&#32493;&#24494;&#35843;LLaVA&#65292;&#19968;&#31181;MLLM&#65292;&#24182;&#21033;&#29992;EMT&#26469;&#35780;&#20272;&#25972;&#20010;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#24615;&#33021;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#26089;&#26399;&#30340;&#24494;&#35843;&#38454;&#27573;&#26159;&#20851;&#38190;&#30340;&#65292;&#36807;&#26089;&#20572;&#27490;&#24494;&#35843;&#21487;&#33021;&#23548;&#33268;&#20302;&#24615;&#33021;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Following the success of GPT4, there has been a surge in interest in multimodal large language model (MLLM) research. This line of research focuses on developing general-purpose LLMs through fine-tuning pre-trained LLMs and vision models. However, catastrophic forgetting, a notorious phenomenon where the fine-tuned model fails to retain similar performance compared to the pre-trained model, still remains an inherent problem in multimodal LLMs (MLLM). In this paper, we introduce EMT: Evaluating MulTimodality for evaluating the catastrophic forgetting in MLLMs, by treating each MLLM as an image classifier. We first apply EMT to evaluate several open-source fine-tuned MLLMs and we discover that almost all evaluated MLLMs fail to retain the same performance levels as their vision encoders on standard image classification tasks. Moreover, we continue fine-tuning LLaVA, an MLLM and utilize EMT to assess performance throughout the fine-tuning. Interestingly, our results suggest that early-sta
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#21457;&#29616;&#20102;&#19968;&#32452;&#39640;&#24230;&#21487;&#35299;&#37322;&#21644;&#21333;&#19968;&#20041;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#22810;&#20041;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.08600</link><description>&lt;p&gt;
&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#21457;&#29616;&#39640;&#24230;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Sparse Autoencoders Find Highly Interpretable Features in Language Models. (arXiv:2309.08600v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08600
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#21457;&#29616;&#20102;&#19968;&#32452;&#39640;&#24230;&#21487;&#35299;&#37322;&#21644;&#21333;&#19968;&#20041;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#22810;&#20041;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#29702;&#35299;&#30340;&#19968;&#20010;&#38556;&#30861;&#26159;&#22810;&#20041;&#24615;&#65292;&#20854;&#20013;&#31070;&#32463;&#20803;&#22312;&#22810;&#20010;&#35821;&#20041;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;&#20013;&#28608;&#27963;&#12290;&#22810;&#20041;&#24615;&#20351;&#25105;&#20204;&#26080;&#27861;&#25214;&#21040;&#31616;&#27905;&#30340;&#12289;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#35299;&#37322;&#26469;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#30340;&#24037;&#20316;&#12290;&#22810;&#20041;&#24615;&#30340;&#19968;&#20010;&#29468;&#27979;&#21407;&#22240;&#26159;&#21472;&#21152;&#25928;&#24212;&#65292;&#21363;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#23558;&#29305;&#24449;&#20998;&#37197;&#32473;&#28608;&#27963;&#31354;&#38388;&#20013;&#30340;&#19968;&#20010;&#36807;&#23436;&#22791;&#26041;&#21521;&#38598;&#21512;&#65292;&#32780;&#19981;&#26159;&#20010;&#21035;&#31070;&#32463;&#20803;&#65292;&#34920;&#31034;&#26356;&#22810;&#30340;&#29305;&#24449;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23581;&#35797;&#20351;&#29992;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#26469;&#30830;&#23450;&#36825;&#20123;&#26041;&#21521;&#65292;&#20197;&#37325;&#26500;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#28608;&#27963;&#12290;&#36825;&#20123;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#21040;&#30340;&#19968;&#32452;&#31232;&#30095;&#28608;&#27963;&#29305;&#24449;&#27604;&#20854;&#20182;&#26041;&#27861;&#37492;&#23450;&#20986;&#30340;&#26041;&#21521;&#26356;&#21487;&#35299;&#37322;&#21644;&#21333;&#19968;&#20041;&#65292;&#35299;&#37322;&#24615;&#26159;&#36890;&#36807;&#33258;&#21160;&#21270;&#26041;&#27861;&#34913;&#37327;&#30340;&#12290;&#21024;&#38500;&#36825;&#20123;&#29305;&#24449;&#21487;&#20197;&#23454;&#29616;&#31934;&#30830;&#30340;&#27169;&#22411;&#32534;&#36753;&#65292;&#20363;&#22914;&#36890;&#36807;&#21024;&#38500;&#36825;&#20123;&#29305;&#24449;&#21487;&#20197;&#25913;&#21464;&#27169;&#22411;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the roadblocks to a better understanding of neural networks' internals is \textit{polysemanticity}, where neurons appear to activate in multiple, semantically distinct contexts. Polysemanticity prevents us from identifying concise, human-understandable explanations for what neural networks are doing internally. One hypothesised cause of polysemanticity is \textit{superposition}, where neural networks represent more features than they have neurons by assigning features to an overcomplete set of directions in activation space, rather than to individual neurons. Here, we attempt to identify those directions, using sparse autoencoders to reconstruct the internal activations of a language model. These autoencoders learn sets of sparsely activating features that are more interpretable and monosemantic than directions identified by alternative approaches, where interpretability is measured by automated methods. Ablating these features enables precise model editing, for example, by remo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;BSDetector&#65292;&#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#38169;&#35823;&#21644;&#25512;&#27979;&#24615;&#22238;&#31572;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20272;&#35745;&#32622;&#20449;&#24230;&#37327;&#21270;&#20102;&#22238;&#31572;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22312;&#38381;&#21512;&#22411;&#21644;&#24320;&#25918;&#22411;&#38382;&#31572;&#22522;&#20934;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26356;&#20934;&#30830;&#30340;&#35782;&#21035;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.16175</link><description>&lt;p&gt;
&#36890;&#36807;&#20869;&#22312;&#21644;&#22806;&#22312;&#32622;&#20449;&#24230;&#35780;&#20272;&#26469;&#37327;&#21270;&#20219;&#24847;&#35821;&#35328;&#27169;&#22411;&#22238;&#31572;&#30340;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Quantifying Uncertainty in Answers from any Language Model via Intrinsic and Extrinsic Confidence Assessment. (arXiv:2308.16175v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;BSDetector&#65292;&#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#38169;&#35823;&#21644;&#25512;&#27979;&#24615;&#22238;&#31572;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20272;&#35745;&#32622;&#20449;&#24230;&#37327;&#21270;&#20102;&#22238;&#31572;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22312;&#38381;&#21512;&#22411;&#21644;&#24320;&#25918;&#22411;&#38382;&#31572;&#22522;&#20934;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26356;&#20934;&#30830;&#30340;&#35782;&#21035;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;BSDetector&#65292;&#19968;&#31181;&#36890;&#36807;&#20272;&#35745;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20219;&#20309;&#36755;&#20986;&#30340;&#25968;&#20540;&#32622;&#20449;&#24230;&#26469;&#26816;&#27979;&#38169;&#35823;&#21644;&#25512;&#27979;&#24615;&#22238;&#31572;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#25216;&#26415;&#36866;&#29992;&#20110;&#20165;&#36890;&#36807;&#40657;&#30418;API&#35775;&#38382;&#30340;&#20219;&#20309;LLM&#65292;&#24182;&#23558;&#20869;&#22312;&#21644;&#22806;&#22312;&#35780;&#20272;&#30340;&#32622;&#20449;&#24230;&#32467;&#21512;&#20026;&#23545;&#32473;&#23450;&#25552;&#31034;&#19979;LLM&#21709;&#24212;&#30340;&#21333;&#20010;&#21487;&#20449;&#24230;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#38750;&#24120;&#36890;&#29992;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#24403;&#20170;&#25152;&#26377;&#26368;&#22909;&#30340;LLM&#65288;&#20854;&#35757;&#32451;&#25968;&#25454;&#26410;&#30693;&#65289;&#12290;&#36890;&#36807;&#39069;&#22806;&#30340;&#35745;&#31639;&#65292;&#20219;&#20309;LLM API&#30340;&#29992;&#25143;&#29616;&#22312;&#21487;&#20197;&#33719;&#24471;&#19982;&#36890;&#24120;&#30456;&#21516;&#30340;&#21709;&#24212;&#65292;&#20197;&#21450;&#19968;&#20010;&#32622;&#20449;&#24230;&#20272;&#35745;&#65292;&#20197;&#20415;&#22312;&#19981;&#20449;&#20219;&#35813;&#21709;&#24212;&#26102;&#20445;&#25345;&#35880;&#24910;&#12290;&#23545;&#20110;&#38381;&#21512;&#22411;&#21644;&#24320;&#25918;&#22411;&#38382;&#31572;&#22522;&#20934;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;BSDetector&#27604;&#20854;&#20182;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#65288;&#23545;&#20110;GPT-3&#21644;ChatGPT&#65289;&#26356;&#20934;&#30830;&#22320;&#35782;&#21035;&#20986;&#38169;&#35823;&#30340;LLM&#21709;&#24212;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#21709;&#24212;&#36827;&#34892;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
We introduce BSDetector, a method for detecting bad and speculative answers from a pretrained Large Language Model by estimating a numeric confidence score for any output it generated. Our uncertainty quantification technique works for any LLM accessible only via a black-box API, and combines intrinsic and extrinsic assessments of confidence into a single trustworthiness estimate for any LLM response to a given prompt. Our method is extremely general and can applied to all of the best LLMs available today (whose training data remains unknown). By expending a bit of extra computation, users of any LLM API can now get the same response as they would ordinarily, as well as a confidence estimate that caution when not to trust this response. Experiments on both closed and open-form Question-Answer benchmarks reveal that BSDetector more accurately identifies incorrect LLM responses than alternative uncertainty estimation procedures (for both GPT-3 and ChatGPT). By sampling multiple responses
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#25351;&#20196;&#35843;&#20248;&#36825;&#19968;&#20851;&#38190;&#25216;&#26415;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#21644;&#21487;&#25511;&#24615;&#26041;&#38754;&#30340;&#30740;&#31350;&#24037;&#20316;&#65292;&#21253;&#25324;&#26041;&#27861;&#12289;&#25968;&#25454;&#38598;&#26500;&#24314;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#24212;&#29992;&#65292;&#20197;&#21450;&#23545;&#32467;&#26524;&#24433;&#21709;&#30340;&#20998;&#26512;&#12290;&#21516;&#26102;&#22238;&#39038;&#20102;&#21487;&#33021;&#30340;&#38382;&#39064;&#21644;&#25209;&#35780;&#65292;&#24182;&#25351;&#20986;&#20102;&#30446;&#21069;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2308.10792</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#35843;&#20248;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Instruction Tuning for Large Language Models: A Survey. (arXiv:2308.10792v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#25351;&#20196;&#35843;&#20248;&#36825;&#19968;&#20851;&#38190;&#25216;&#26415;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#21644;&#21487;&#25511;&#24615;&#26041;&#38754;&#30340;&#30740;&#31350;&#24037;&#20316;&#65292;&#21253;&#25324;&#26041;&#27861;&#12289;&#25968;&#25454;&#38598;&#26500;&#24314;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#24212;&#29992;&#65292;&#20197;&#21450;&#23545;&#32467;&#26524;&#24433;&#21709;&#30340;&#20998;&#26512;&#12290;&#21516;&#26102;&#22238;&#39038;&#20102;&#21487;&#33021;&#30340;&#38382;&#39064;&#21644;&#25209;&#35780;&#65292;&#24182;&#25351;&#20986;&#20102;&#30446;&#21069;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#25351;&#20196;&#35843;&#20248;&#65288;IT&#65289;&#36825;&#19968;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#20013;&#30340;&#30740;&#31350;&#24037;&#20316;&#65292;&#36825;&#26159;&#19968;&#31181;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33021;&#21147;&#21644;&#21487;&#25511;&#24615;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#25351;&#20196;&#35843;&#20248;&#26159;&#25351;&#20197;&#30417;&#30563;&#26041;&#24335;&#22312;&#21253;&#21547;&#8220;&#25351;&#20196;-&#36755;&#20986;&#8221;&#23545;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#19968;&#27493;&#35757;&#32451;LLM&#65292;&#36825;&#23558;LLM&#30340;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#30446;&#26631;&#19982;&#29992;&#25143;&#24076;&#26395;LLM&#36981;&#23432;&#20154;&#31867;&#25351;&#20196;&#30340;&#30446;&#26631;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#26412;&#25991;&#23545;IT&#30340;&#24120;&#35268;&#26041;&#27861;&#12289;IT&#25968;&#25454;&#38598;&#30340;&#26500;&#24314;&#12289;IT&#27169;&#22411;&#30340;&#35757;&#32451;&#20197;&#21450;&#24212;&#29992;&#20110;&#19981;&#21516;&#27169;&#24577;&#12289;&#39046;&#22495;&#21644;&#24212;&#29992;&#30340;&#24773;&#20917;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#25991;&#29486;&#32508;&#36848;&#65292;&#24182;&#23545;&#24433;&#21709;IT&#32467;&#26524;&#30340;&#21508;&#20010;&#26041;&#38754;&#36827;&#34892;&#20102;&#20998;&#26512;&#65288;&#20363;&#22914;&#65292;&#25351;&#20196;&#36755;&#20986;&#30340;&#29983;&#25104;&#12289;&#25351;&#20196;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#31561;&#65289;&#12290;&#25105;&#20204;&#36824;&#22238;&#39038;&#20102;IT&#30340;&#28508;&#22312;&#38382;&#39064;&#20197;&#21450;&#38024;&#23545;&#20854;&#30340;&#25209;&#35780;&#65292;&#20197;&#21450;&#25351;&#20986;&#24403;&#21069;&#19981;&#36275;&#30340;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper surveys research works in the quickly advancing field of instruction tuning (IT), a crucial technique to enhance the capabilities and controllability of large language models (LLMs). Instruction tuning refers to the process of further training LLMs on a dataset consisting of \textsc{(instruction, output)} pairs in a supervised fashion, which bridges the gap between the next-word prediction objective of LLMs and the users' objective of having LLMs adhere to human instructions. In this work, we make a systematic review of the literature, including the general methodology of IT, the construction of IT datasets, the training of IT models, and applications to different modalities, domains and applications, along with an analysis on aspects that influence the outcome of IT (e.g., generation of instruction outputs, size of the instruction dataset, etc). We also review the potential pitfalls of IT along with criticism against it, along with efforts pointing out current deficiencies 
&lt;/p&gt;</description></item><item><title>AutoGen&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#20010;&#21487;&#20197;&#20114;&#30456;&#23545;&#35805;&#30340;&#20195;&#29702;&#65292;&#23454;&#29616;&#20102;&#19979;&#19968;&#20195;LLM&#24212;&#29992;&#12290;&#23427;&#21033;&#29992;&#20154;&#31867;&#30340;&#29702;&#35299;&#21644;&#26234;&#33021;&#65292;&#20248;&#38597;&#22320;&#22788;&#29702;&#19981;&#23436;&#32654;&#30340;&#29983;&#25104;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#21270;&#20195;&#29702;&#23545;&#35805;&#31616;&#21270;&#20102;&#22797;&#26434;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;</title><link>http://arxiv.org/abs/2308.08155</link><description>&lt;p&gt;
AutoGen:&#36890;&#36807;&#22810;&#20195;&#29702;&#23545;&#35805;&#26694;&#26550;&#23454;&#29616;&#19979;&#19968;&#20195;LLM&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework. (arXiv:2308.08155v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08155
&lt;/p&gt;
&lt;p&gt;
AutoGen&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#20010;&#21487;&#20197;&#20114;&#30456;&#23545;&#35805;&#30340;&#20195;&#29702;&#65292;&#23454;&#29616;&#20102;&#19979;&#19968;&#20195;LLM&#24212;&#29992;&#12290;&#23427;&#21033;&#29992;&#20154;&#31867;&#30340;&#29702;&#35299;&#21644;&#26234;&#33021;&#65292;&#20248;&#38597;&#22320;&#22788;&#29702;&#19981;&#23436;&#32654;&#30340;&#29983;&#25104;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#21270;&#20195;&#29702;&#23545;&#35805;&#31616;&#21270;&#20102;&#22797;&#26434;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25216;&#26415;&#25253;&#21578;&#20171;&#32461;&#20102;AutoGen&#65292;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#20010;&#21487;&#20197;&#20114;&#30456;&#23545;&#35805;&#30340;&#20195;&#29702;&#26469;&#24320;&#21457;LLM&#24212;&#29992;&#31243;&#24207;&#20197;&#35299;&#20915;&#20219;&#21153;&#12290;AutoGen&#20195;&#29702;&#21487;&#20197;&#23450;&#21046;&#12289;&#21487;&#23545;&#35805;&#65292;&#24182;&#19988;&#21487;&#20197;&#26080;&#32541;&#22320;&#20801;&#35768;&#20154;&#31867;&#21442;&#19982;&#12290;&#23427;&#20204;&#21487;&#20197;&#22312;&#21508;&#31181;&#27169;&#24335;&#19979;&#36816;&#34892;&#65292;&#21033;&#29992;LLM&#12289;&#20154;&#31867;&#36755;&#20837;&#21644;&#24037;&#20855;&#30340;&#32452;&#21512;&#12290;AutoGen&#30340;&#35774;&#35745;&#25552;&#20379;&#20102;&#22810;&#20010;&#20248;&#21183;&#65306;a&#65289;&#23427;&#33021;&#22815;&#20248;&#38597;&#22320;&#22788;&#29702;&#36825;&#20123;LLM&#30340;&#24378;&#22823;&#20294;&#19981;&#23436;&#32654;&#30340;&#29983;&#25104;&#21644;&#25512;&#29702;&#33021;&#21147;&#65307;b&#65289;&#23427;&#21033;&#29992;&#20154;&#31867;&#30340;&#29702;&#35299;&#21644;&#26234;&#33021;&#65292;&#36890;&#36807;&#20195;&#29702;&#20043;&#38388;&#30340;&#23545;&#35805;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#33258;&#21160;&#21270;&#65307;c&#65289;&#23427;&#31616;&#21270;&#21644;&#32479;&#19968;&#20102;&#22797;&#26434;LLM&#24037;&#20316;&#27969;&#31243;&#30340;&#23454;&#29616;&#65292;&#20316;&#20026;&#33258;&#21160;&#21270;&#20195;&#29702;&#23545;&#35805;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#35768;&#22810;&#19981;&#21516;&#30340;&#20363;&#23376;&#65292;&#23637;&#31034;&#20102;&#24320;&#21457;&#20154;&#21592;&#22914;&#20309;&#36731;&#26494;&#20351;&#29992;AutoGen&#26377;&#25928;&#22320;&#35299;&#20915;&#20219;&#21153;&#25110;&#26500;&#24314;&#24212;&#29992;&#31243;&#24207;&#65292;&#28085;&#30422;&#32534;&#31243;&#12289;&#25968;&#23398;&#12289;&#36816;&#31609;&#23398;&#12289;&#23089;&#20048;&#12289;&#22312;&#32447;&#20915;&#31574;&#12289;&#38382;&#31572;&#31561;&#22810;&#20010;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
This technical report presents AutoGen, a new framework that enables development of LLM applications using multiple agents that can converse with each other to solve tasks. AutoGen agents are customizable, conversable, and seamlessly allow human participation. They can operate in various modes that employ combinations of LLMs, human inputs, and tools. AutoGen's design offers multiple advantages: a) it gracefully navigates the strong but imperfect generation and reasoning abilities of these LLMs; b) it leverages human understanding and intelligence, while providing valuable automation through conversations between agents; c) it simplifies and unifies the implementation of complex LLM workflows as automated agent chats. We provide many diverse examples of how developers can easily use AutoGen to effectively solve tasks or build applications, ranging from coding, mathematics, operations research, entertainment, online decision-making, question answering, etc.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;L-Eval&#65292;&#26088;&#22312;&#20026;&#38271;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#24341;&#20837;&#26631;&#20934;&#21270;&#35780;&#20272;&#12290;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#21253;&#21547;411&#20010;&#38271;&#25991;&#26723;&#21644;2000&#22810;&#20010;&#20154;&#24037;&#26631;&#27880;&#30340;&#26597;&#35810;-&#22238;&#22797;&#23545;&#30340;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#25506;&#35752;&#20102;&#25193;&#23637;&#19978;&#19979;&#25991;&#23545;&#20110;&#22788;&#29702;&#38271;&#36755;&#20837;&#30340;&#23454;&#36136;&#24615;&#25910;&#30410;&#21644;&#25913;&#36827;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.11088</link><description>&lt;p&gt;
L-Eval&#65306;&#20026;&#38271;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#24341;&#20837;&#26631;&#20934;&#21270;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
L-Eval: Instituting Standardized Evaluation for Long Context Language Models. (arXiv:2307.11088v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11088
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;L-Eval&#65292;&#26088;&#22312;&#20026;&#38271;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#24341;&#20837;&#26631;&#20934;&#21270;&#35780;&#20272;&#12290;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#21253;&#21547;411&#20010;&#38271;&#25991;&#26723;&#21644;2000&#22810;&#20010;&#20154;&#24037;&#26631;&#27880;&#30340;&#26597;&#35810;-&#22238;&#22797;&#23545;&#30340;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#25506;&#35752;&#20102;&#25193;&#23637;&#19978;&#19979;&#25991;&#23545;&#20110;&#22788;&#29702;&#38271;&#36755;&#20837;&#30340;&#23454;&#36136;&#24615;&#25910;&#30410;&#21644;&#25913;&#36827;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#20110;&#25193;&#23637;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#65292;&#20197;&#20415;&#26377;&#25928;&#22788;&#29702;&#21333;&#22238;&#21512;&#30340;&#38271;&#36755;&#20837;&#65288;&#20363;&#22914;&#65292;&#35770;&#25991;&#24635;&#32467;&#65289;&#21644;&#20855;&#26377;&#26356;&#22797;&#26434;&#21382;&#21490;&#30340;&#23545;&#35805;&#65292;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#34429;&#28982;&#20687;GPT-4&#21644;Claude&#36825;&#26679;&#30340;&#19987;&#26377;&#27169;&#22411;&#22312;&#22788;&#29702;&#26497;&#38271;&#36755;&#20837;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#27493;&#65292;&#20294;&#24320;&#25918;&#28304;&#20195;&#30721;&#27169;&#22411;&#20173;&#22788;&#20110;&#23581;&#35797;&#38454;&#27573;&#12290;&#30446;&#21069;&#36824;&#19981;&#28165;&#26970;&#25193;&#23637;&#19978;&#19979;&#25991;&#26159;&#21542;&#33021;&#22815;&#27604;&#20256;&#32479;&#26041;&#27861;&#65288;&#22914;&#26816;&#32034;&#65289;&#25552;&#20379;&#23454;&#36136;&#24615;&#30340;&#25910;&#30410;&#65292;&#20197;&#21450;&#23427;&#22312;&#23454;&#38469;&#19979;&#28216;&#20219;&#21153;&#20013;&#23545;&#24120;&#35268;&#27169;&#22411;&#30340;&#25913;&#36827;&#31243;&#24230;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20026;&#38271;&#19978;&#19979;&#25991;&#35821;&#35328;&#27169;&#22411;&#24341;&#20837;&#26631;&#20934;&#21270;&#35780;&#20272;&#30340;&#26041;&#26696;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;L-Eval&#65292;&#20854;&#20013;&#21253;&#21547;411&#20010;&#38271;&#25991;&#26723;&#21644;2000&#22810;&#20010;&#20154;&#24037;&#26631;&#27880;&#30340;&#26597;&#35810;-&#22238;&#22797;&#23545;&#65292;&#28085;&#30422;&#27861;&#24459;&#12289;&#37329;&#34701;&#12289;&#23398;&#26657;&#35762;&#24231;&#12289;&#38271;&#23545;&#35805;&#12289;&#26032;&#38395;&#12289;&#38271;&#31687;&#23567;&#35828;&#21644;&#20250;&#35758;&#31561;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, there has been growing interest in extending the context length of instruction-following models in order to effectively process single-turn long input (e.g. summarizing a paper) and conversations with more extensive histories. While proprietary models such as GPT-4 and Claude have shown significant strides in handling extremely lengthy input, open-sourced models are still in the early stages of experimentation. It also remains unclear whether extending the context can offer substantial gains over traditional methods such as retrieval, and to what extent it improves upon their regular counterparts in practical downstream tasks. To address this challenge, we propose instituting standardized evaluation for long context language models. Concretely, we develop L-Eval which contains 411 long documents and over 2,000 human-labeled query-response pairs encompassing areas such as law, finance, school lectures, lengthy conversations, news, long-form novels, and meetings. L-Eval also ad
&lt;/p&gt;</description></item><item><title>FLASK&#26159;&#19968;&#31181;&#22522;&#20110;&#23545;&#40784;&#25216;&#33021;&#38598;&#30340;&#32454;&#31890;&#24230;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#21327;&#35758;&#65292;&#36890;&#36807;&#23558;&#31895;&#32423;&#35780;&#20998;&#20998;&#35299;&#20026;&#27599;&#20010;&#25351;&#20196;&#30340;&#25216;&#33021;&#38598;&#32423;&#35780;&#20998;&#65292;&#23454;&#29616;&#20102;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#20840;&#38754;&#35270;&#35282;&#21644;&#25552;&#39640;&#35780;&#20272;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.10928</link><description>&lt;p&gt;
FLASK: &#22522;&#20110;&#23545;&#40784;&#25216;&#33021;&#38598;&#30340;&#32454;&#31890;&#24230;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets. (arXiv:2307.10928v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10928
&lt;/p&gt;
&lt;p&gt;
FLASK&#26159;&#19968;&#31181;&#22522;&#20110;&#23545;&#40784;&#25216;&#33021;&#38598;&#30340;&#32454;&#31890;&#24230;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#21327;&#35758;&#65292;&#36890;&#36807;&#23558;&#31895;&#32423;&#35780;&#20998;&#20998;&#35299;&#20026;&#27599;&#20010;&#25351;&#20196;&#30340;&#25216;&#33021;&#38598;&#32423;&#35780;&#20998;&#65292;&#23454;&#29616;&#20102;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#20840;&#38754;&#35270;&#35282;&#21644;&#25552;&#39640;&#35780;&#20272;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25351;&#20196;&#38656;&#35201;&#19982;&#20154;&#31867;&#30340;&#20215;&#20540;&#35266;&#36827;&#34892;&#23545;&#40784;&#65292;&#24182;&#19988;&#25152;&#38656;&#30340;&#25216;&#33021;&#38598;&#26681;&#25454;&#25351;&#20196;&#32780;&#24322;&#65292;&#22240;&#27492;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#35780;&#20272;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#31895;&#31890;&#24230;&#35780;&#20272;&#65288;&#21363;&#22522;&#20110;&#25972;&#20307;&#20559;&#22909;&#30340;&#35780;&#20272;&#65289;&#65292;&#36825;&#38480;&#21046;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#22240;&#20026;&#23427;&#26410;&#32771;&#34385;&#38656;&#35201;&#23454;&#20363;&#32423;&#25216;&#33021;&#32452;&#21512;&#30340;&#29992;&#25143;&#25351;&#20196;&#30340;&#29305;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;FLASK&#65288;&#22522;&#20110;&#23545;&#40784;&#25216;&#33021;&#38598;&#30340;&#32454;&#31890;&#24230;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#32454;&#31890;&#24230;&#35780;&#20272;&#21327;&#35758;&#65292;&#29992;&#20110;&#20154;&#31867;&#21644;&#27169;&#22411;&#30340;&#35780;&#20272;&#65292;&#23427;&#23558;&#31895;&#32423;&#35780;&#20998;&#20998;&#35299;&#20026;&#27599;&#20010;&#25351;&#20196;&#30340;&#25216;&#33021;&#38598;&#32423;&#35780;&#20998;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35780;&#20272;&#30340;&#32454;&#31890;&#24230;&#23545;&#20110;&#33719;&#24471;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#20840;&#38754;&#35270;&#35282;&#21644;&#25552;&#39640;&#35780;&#20272;&#30340;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#21033;&#29992;FLASK&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#22810;&#20010;&#24320;&#28304;&#21644;&#19987;&#26377;LLMs&#65292;&#24182;&#35266;&#23519;&#21040;&#39640;&#24230;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluation of Large Language Models (LLMs) is challenging because instruction-following necessitates alignment with human values and the required set of skills varies depending on the instruction. However, previous studies have mainly focused on coarse-grained evaluation (i.e. overall preference-based evaluation), which limits interpretability since it does not consider the nature of user instructions that require instance-wise skill composition. In this paper, we introduce FLASK (Fine-grained Language Model Evaluation based on Alignment Skill Sets), a fine-grained evaluation protocol for both human-based and model-based evaluation which decomposes coarse-level scoring to a skill set-level scoring for each instruction. We experimentally observe that the fine-graininess of evaluation is crucial for attaining a holistic view of model performance and increasing the reliability of the evaluation. Using FLASK, we compare multiple open-source and proprietary LLMs and observe a high correlati
&lt;/p&gt;</description></item><item><title>ValiTex&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#39564;&#35777;&#26694;&#26550;&#65292;&#26088;&#22312;&#24110;&#21161;&#23398;&#32773;&#20204;&#22522;&#20110;&#25991;&#26412;&#25968;&#25454;&#26469;&#24230;&#37327;&#31038;&#20250;&#31185;&#23398;&#26500;&#24314;&#12290;&#23427;&#20511;&#37492;&#20102;&#24515;&#29702;&#27979;&#37327;&#23398;&#30340;&#20256;&#32479;&#65292;&#36890;&#36807;&#27010;&#24565;&#27169;&#22411;&#21644;&#21160;&#24577;&#26816;&#26597;&#34920;&#25552;&#20379;&#20102;&#39564;&#35777;&#30340;&#32467;&#26500;&#21644;&#27493;&#39588;&#12290;</title><link>http://arxiv.org/abs/2307.02863</link><description>&lt;p&gt;
ValiTex -- &#19968;&#31181;&#29992;&#20110;&#35745;&#31639;&#25991;&#26412;&#30340;&#31038;&#20250;&#31185;&#23398;&#26500;&#24314;&#24230;&#37327;&#30340;&#32479;&#19968;&#39564;&#35777;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
ValiTex -- a uniform validation framework for computational text-based measures of social science constructs. (arXiv:2307.02863v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02863
&lt;/p&gt;
&lt;p&gt;
ValiTex&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#39564;&#35777;&#26694;&#26550;&#65292;&#26088;&#22312;&#24110;&#21161;&#23398;&#32773;&#20204;&#22522;&#20110;&#25991;&#26412;&#25968;&#25454;&#26469;&#24230;&#37327;&#31038;&#20250;&#31185;&#23398;&#26500;&#24314;&#12290;&#23427;&#20511;&#37492;&#20102;&#24515;&#29702;&#27979;&#37327;&#23398;&#30340;&#20256;&#32479;&#65292;&#36890;&#36807;&#27010;&#24565;&#27169;&#22411;&#21644;&#21160;&#24577;&#26816;&#26597;&#34920;&#25552;&#20379;&#20102;&#39564;&#35777;&#30340;&#32467;&#26500;&#21644;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#20110;&#22914;&#20309;&#39564;&#35777;&#35745;&#31639;&#25991;&#26412;&#30340;&#31038;&#20250;&#31185;&#23398;&#26500;&#24314;&#24230;&#37327;&#30340;&#25351;&#23548;&#26159;&#20998;&#25955;&#30340;&#12290;&#34429;&#28982;&#23398;&#32773;&#20204;&#26222;&#36941;&#35748;&#35782;&#21040;&#39564;&#35777;&#20182;&#20204;&#30340;&#25991;&#26412;&#24230;&#37327;&#30340;&#37325;&#35201;&#24615;&#65292;&#20294;&#20182;&#20204;&#36890;&#24120;&#32570;&#20047;&#20849;&#21516;&#30340;&#26415;&#35821;&#21644;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#36827;&#34892;&#39564;&#35777;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;ValiTex&#30340;&#26032;&#39564;&#35777;&#26694;&#26550;&#65292;&#26088;&#22312;&#24110;&#21161;&#23398;&#32773;&#20204;&#22522;&#20110;&#25991;&#26412;&#25968;&#25454;&#26469;&#24230;&#37327;&#31038;&#20250;&#31185;&#23398;&#26500;&#24314;&#12290;&#35813;&#26694;&#26550;&#20511;&#37492;&#20102;&#24515;&#29702;&#27979;&#37327;&#23398;&#20013;&#38271;&#26399;&#23384;&#22312;&#30340;&#20256;&#32479;&#65292;&#21516;&#26102;&#25193;&#23637;&#20102;&#26694;&#26550;&#20197;&#36866;&#29992;&#20110;&#35745;&#31639;&#25991;&#26412;&#20998;&#26512;&#30340;&#30446;&#30340;&#12290;ValiTex&#21253;&#25324;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65292;&#19968;&#20010;&#26159;&#27010;&#24565;&#27169;&#22411;&#65292;&#19968;&#20010;&#26159;&#21160;&#24577;&#26816;&#26597;&#34920;&#12290;&#27010;&#24565;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#32467;&#26500;&#65292;&#21487;&#20197;&#25351;&#23548;&#39564;&#35777;&#30340;&#19981;&#21516;&#38454;&#27573;&#65292;&#21160;&#24577;&#26816;&#26597;&#34920;&#23450;&#20041;&#20102;&#20855;&#20307;&#30340;&#39564;&#35777;&#27493;&#39588;&#65292;&#24182;&#25552;&#20379;&#20102;&#21738;&#20123;&#27493;&#39588;&#21487;&#33021;&#34987;&#35748;&#20026;&#26159;&#25512;&#33616;&#30340;&#65288;&#21363;&#25552;&#20379;&#30456;&#20851;&#21644;&#24517;&#35201;&#30340;&#39564;&#35777;&#35777;&#25454;&#65289;&#25110;&#21487;&#36873;&#30340;&#65288;&#21363;&#23545;&#25552;&#20379;&#39069;&#22806;&#20449;&#24687;&#26377;&#29992;&#30340;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Guidance on how to validate computational text-based measures of social science constructs is fragmented. Whereas scholars are generally acknowledging the importance of validating their text-based measures, they often lack common terminology and a unified framework to do so. This paper introduces a new validation framework called ValiTex, designed to assist scholars to measure social science constructs based on textual data. The framework draws on a long-established tradition within psychometrics while extending the framework for the purpose of computational text analysis. ValiTex consists of two components, a conceptual model, and a dynamic checklist. Whereas the conceptual model provides a general structure along distinct phases on how to approach validation, the dynamic checklist defines specific validation steps and provides guidance on which steps might be considered recommendable (i.e., providing relevant and necessary validation evidence) or optional (i.e., useful for providing 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;&#20013;&#21629;&#21517;&#23454;&#20307;&#36951;&#28431;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23450;&#21046;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#21644;&#27169;&#22411;&#35757;&#32451;&#31574;&#30053;&#65292;&#25913;&#21892;&#20102;&#21629;&#21517;&#23454;&#20307;&#30340;&#21253;&#21547;&#24773;&#20917;&#65292;&#25552;&#39640;&#20102;&#25688;&#35201;&#30340;&#20934;&#30830;&#24615;&#21644;&#21484;&#22238;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.02570</link><description>&lt;p&gt;
&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#21253;&#21547;
&lt;/p&gt;
&lt;p&gt;
Named Entity Inclusion in Abstractive Text Summarization. (arXiv:2307.02570v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02570
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;&#20013;&#21629;&#21517;&#23454;&#20307;&#36951;&#28431;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23450;&#21046;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#21644;&#27169;&#22411;&#35757;&#32451;&#31574;&#30053;&#65292;&#25913;&#21892;&#20102;&#21629;&#21517;&#23454;&#20307;&#30340;&#21253;&#21547;&#24773;&#20917;&#65292;&#25552;&#39640;&#20102;&#25688;&#35201;&#30340;&#20934;&#30830;&#24615;&#21644;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#35768;&#22810;&#24403;&#21069;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;&#22120;&#30340;&#32570;&#28857;&#65292;&#21363;&#21629;&#21517;&#23454;&#20307;&#30340;&#36951;&#28431;&#38382;&#39064;&#12290;&#25105;&#20204;&#24314;&#35758;&#37319;&#29992;&#23450;&#21046;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#26469;&#22686;&#24378;&#27169;&#22411;&#23545;&#25991;&#26412;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#30340;&#27880;&#24847;&#21147;&#12290;&#39318;&#20808;&#65292;&#20351;&#29992;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;RoBERTa&#26469;&#30830;&#23450;&#25991;&#26412;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#35813;&#27169;&#22411;&#23545;&#25991;&#26412;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#36827;&#34892;&#23631;&#34109;&#65292;&#20877;&#20351;&#29992;BART&#27169;&#22411;&#23545;&#20854;&#36827;&#34892;&#37325;&#24314;&#12290;&#25509;&#19979;&#26469;&#65292;&#23558;BART&#27169;&#22411;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#39044;&#35757;&#32451;&#26041;&#27861;&#25913;&#21892;&#20102;&#21629;&#21517;&#23454;&#20307;&#21253;&#21547;&#30340;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the named entity omission - the drawback of many current abstractive text summarizers. We suggest a custom pretraining objective to enhance the model's attention on the named entities in a text. At first, the named entity recognition model RoBERTa is trained to determine named entities in the text. After that, this model is used to mask named entities in the text and the BART model is trained to reconstruct them. Next, the BART model is fine-tuned on the summarization task. Our experiments showed that this pretraining approach improves named entity inclusion precision and recall metrics.
&lt;/p&gt;</description></item><item><title>MedCPT&#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#38646;&#26679;&#26412;&#35821;&#20041;&#20449;&#24687;&#26816;&#32034;&#30340;&#23545;&#27604;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#22823;&#35268;&#27169;PubMed&#25628;&#32034;&#26085;&#24535;&#36827;&#34892;&#35757;&#32451;&#65292;MedCPT&#22312;&#20845;&#20010;&#29983;&#29289;&#21307;&#23398;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#21019;&#36896;&#20102;&#26032;&#30340;&#26368;&#20339;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#65292;&#21516;&#26102;&#36824;&#33021;&#29983;&#25104;&#26356;&#22909;&#30340;&#29983;&#29289;&#21307;&#23398;&#25991;&#31456;&#21644;&#21477;&#23376;&#12290;</title><link>http://arxiv.org/abs/2307.00589</link><description>&lt;p&gt;
MedCPT: &#20351;&#29992;&#22823;&#35268;&#27169;PubMed&#25628;&#32034;&#26085;&#24535;&#30340;&#23545;&#27604;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#36827;&#34892;&#38646;&#26679;&#26412;&#29983;&#29289;&#21307;&#23398;&#20449;&#24687;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
MedCPT: Contrastive Pre-trained Transformers with Large-scale PubMed Search Logs for Zero-shot Biomedical Information Retrieval. (arXiv:2307.00589v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00589
&lt;/p&gt;
&lt;p&gt;
MedCPT&#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#38646;&#26679;&#26412;&#35821;&#20041;&#20449;&#24687;&#26816;&#32034;&#30340;&#23545;&#27604;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#22823;&#35268;&#27169;PubMed&#25628;&#32034;&#26085;&#24535;&#36827;&#34892;&#35757;&#32451;&#65292;MedCPT&#22312;&#20845;&#20010;&#29983;&#29289;&#21307;&#23398;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#21019;&#36896;&#20102;&#26032;&#30340;&#26368;&#20339;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#65292;&#21516;&#26102;&#36824;&#33021;&#29983;&#25104;&#26356;&#22909;&#30340;&#29983;&#29289;&#21307;&#23398;&#25991;&#31456;&#21644;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#26816;&#32034;&#22312;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#33719;&#21462;&#21644;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#36827;&#23637;&#34920;&#26126;&#35821;&#35328;&#27169;&#22411;&#32534;&#30721;&#22120;&#22312;&#35821;&#20041;&#26816;&#32034;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65292;&#20294;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#26597;&#35810;-&#25991;&#31456;&#27880;&#37322;&#65292;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#24456;&#38590;&#33719;&#24471;&#12290;&#22240;&#27492;&#65292;&#22823;&#22810;&#25968;&#29983;&#29289;&#21307;&#23398;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#21482;&#36827;&#34892;&#35789;&#27719;&#21305;&#37197;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MedCPT&#65292;&#36825;&#26159;&#19968;&#31181;&#39318;&#21019;&#30340;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#38646;&#26679;&#26412;&#35821;&#20041;&#20449;&#24687;&#26816;&#32034;&#30340;&#23545;&#27604;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#27169;&#22411;&#12290;&#20026;&#20102;&#35757;&#32451;MedCPT&#65292;&#25105;&#20204;&#20174;PubMed&#25910;&#38598;&#20102;255 million&#20010;&#29992;&#25143;&#28857;&#20987;&#26085;&#24535;&#65292;&#36825;&#26159;&#21069;&#25152;&#26410;&#26377;&#30340;&#35268;&#27169;&#12290;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#65292;&#25105;&#20204;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#35757;&#32451;&#19968;&#23545;&#23494;&#20999;&#38598;&#25104;&#30340;&#26816;&#32034;&#22120;&#21644;&#37325;&#25490;&#22120;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;MedCPT&#22312;&#20845;&#20010;&#29983;&#29289;&#21307;&#23398;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20339;&#24615;&#33021;&#65292;&#20248;&#20110;&#21253;&#25324;&#26356;&#22823;&#27169;&#22411;&#65288;&#22914;GPT-3&#22823;&#23567;&#30340;cpt-text-XL&#65289;&#22312;&#20869;&#30340;&#21508;&#31181;&#22522;&#32447;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;MedCPT&#36824;&#33021;&#22815;&#29983;&#25104;&#26356;&#22909;&#30340;&#29983;&#29289;&#21307;&#23398;&#25991;&#31456;&#21644;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Information retrieval (IR) is essential in biomedical knowledge acquisition and clinical decision support. While recent progress has shown that language model encoders perform better semantic retrieval, training such models requires abundant query-article annotations that are difficult to obtain in biomedicine. As a result, most biomedical IR systems only conduct lexical matching. In response, we introduce MedCPT, a first-of-its-kind Contrastively Pre-trained Transformer model for zero-shot semantic IR in biomedicine. To train MedCPT, we collected an unprecedented scale of 255 million user click logs from PubMed. With such data, we use contrastive learning to train a pair of closely-integrated retriever and re-ranker. Experimental results show that MedCPT sets new state-of-the-art performance on six biomedical IR tasks, outperforming various baselines including much larger models such as GPT-3-sized cpt-text-XL. In addition, MedCPT also generates better biomedical article and sentence 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#24212;&#29992;&#28436;&#32462;&#39564;&#35777;&#25216;&#26415;&#65292;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#26126;&#30830;&#32780;&#20005;&#35880;&#30340;&#28436;&#32462;&#25512;&#29702;&#65292;&#20197;&#30830;&#20445;&#20854;&#25512;&#29702;&#36807;&#31243;&#30340;&#21487;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.03872</link><description>&lt;p&gt;
&#24212;&#29992;&#28436;&#32462;&#39564;&#35777;&#25216;&#26415;&#39564;&#35777;&#24605;&#32500;&#38142;&#30340;&#25512;&#29702;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Deductive Verification of Chain-of-Thought Reasoning. (arXiv:2306.03872v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03872
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#24212;&#29992;&#28436;&#32462;&#39564;&#35777;&#25216;&#26415;&#65292;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#26126;&#30830;&#32780;&#20005;&#35880;&#30340;&#28436;&#32462;&#25512;&#29702;&#65292;&#20197;&#30830;&#20445;&#20854;&#25512;&#29702;&#36807;&#31243;&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#20013;&#21463;&#30410;&#21290;&#27973;&#65292;&#29305;&#21035;&#26159;&#24212;&#29992;&#24605;&#32500;&#38142;&#25552;&#31034;&#21487;&#20197;&#20351;&#27169;&#22411;&#20135;&#29983;&#26356;&#20840;&#38754;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#24605;&#32500;&#38142;&#30340;&#24378;&#35843;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#21487;&#33021;&#20250;&#19981;&#24910;&#23548;&#33268;&#20135;&#29983;&#24187;&#35273;&#21644;&#32047;&#31215;&#38169;&#35823;&#65292;&#20174;&#32780;&#38480;&#21046;&#27169;&#22411;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#28789;&#24863;&#26469;&#33258;&#20110;&#20154;&#31867;&#22914;&#20309;&#36827;&#34892;&#32454;&#33268;&#30340;&#28436;&#32462;&#36923;&#36753;&#25512;&#29702;&#36807;&#31243;&#26469;&#35299;&#20915;&#20219;&#21153;&#65292;&#25105;&#20204;&#26088;&#22312;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#26126;&#30830;&#32780;&#20005;&#35880;&#30340;&#28436;&#32462;&#25512;&#29702;&#65292;&#24182;&#36890;&#36807;&#33258;&#25105;&#39564;&#35777;&#30830;&#20445;&#25512;&#29702;&#36807;&#31243;&#30340;&#21487;&#20449;&#24230;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#20687;ChatGPT&#36825;&#26679;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#30452;&#25509;&#39564;&#35777;&#25972;&#20010;&#28436;&#32462;&#25512;&#29702;&#36807;&#31243;&#30340;&#26377;&#25928;&#24615;&#20063;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#25512;&#29702;&#39564;&#35777;&#36807;&#31243;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#36880;&#27493;&#30340;&#23376;&#36807;&#31243;&#65292;&#27599;&#20010;&#36807;&#31243;&#21482;&#25509;&#25910;&#20854;&#24517;&#35201;&#30340;&#19978;&#19979;&#25991;&#21644;&#21069;&#25552;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) significantly benefit from Chain-of-Thought (CoT) prompting in performing various reasoning tasks. While CoT allows models to produce more comprehensive reasoning processes, its emphasis on intermediate reasoning steps can inadvertently introduce hallucinations and accumulated errors, thereby limiting models' ability to solve complex reasoning tasks. Inspired by how humans engage in careful and meticulous deductive logical reasoning processes to solve tasks, we seek to enable language models to perform explicit and rigorous deductive reasoning, and also ensure the trustworthiness of their reasoning process through self-verification. However, directly verifying the validity of an entire deductive reasoning process is challenging, even with advanced models like ChatGPT. In light of this, we propose to decompose a reasoning verification process into a series of step-by-step subprocesses, each only receiving their necessary context and premises. To facilitate t
&lt;/p&gt;</description></item><item><title>RepoBench&#26159;&#19968;&#20010;&#35780;&#20272;&#20195;&#30721;&#33258;&#21160;&#34917;&#20840;&#31995;&#32479;&#22312;&#20195;&#30721;&#24211;&#32423;&#21035;&#19978;&#24615;&#33021;&#30340;&#22522;&#20934;&#65292;&#25903;&#25345;Python&#21644;Java&#65292;&#21253;&#21547;&#19977;&#20010;&#30456;&#20114;&#20851;&#32852;&#30340;&#35780;&#20272;&#20219;&#21153;&#12290;&#23427;&#26088;&#22312;&#22635;&#34917;&#24403;&#21069;&#22522;&#20934;&#22312;&#22810;&#25991;&#20214;&#32534;&#31243;&#22330;&#26223;&#26041;&#38754;&#30340;&#35780;&#20272;&#24046;&#36317;&#65292;&#24182;&#20026;&#19981;&#21516;&#31995;&#32479;&#30340;&#24615;&#33021;&#25552;&#20379;&#26356;&#20840;&#38754;&#30340;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2306.03091</link><description>&lt;p&gt;
RepoBench&#65306;&#35780;&#20272;&#20195;&#30721;&#33258;&#21160;&#34917;&#20840;&#31995;&#32479;&#30340;&#20195;&#30721;&#24211;&#32423;&#21035;&#24615;&#33021;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems. (arXiv:2306.03091v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03091
&lt;/p&gt;
&lt;p&gt;
RepoBench&#26159;&#19968;&#20010;&#35780;&#20272;&#20195;&#30721;&#33258;&#21160;&#34917;&#20840;&#31995;&#32479;&#22312;&#20195;&#30721;&#24211;&#32423;&#21035;&#19978;&#24615;&#33021;&#30340;&#22522;&#20934;&#65292;&#25903;&#25345;Python&#21644;Java&#65292;&#21253;&#21547;&#19977;&#20010;&#30456;&#20114;&#20851;&#32852;&#30340;&#35780;&#20272;&#20219;&#21153;&#12290;&#23427;&#26088;&#22312;&#22635;&#34917;&#24403;&#21069;&#22522;&#20934;&#22312;&#22810;&#25991;&#20214;&#32534;&#31243;&#22330;&#26223;&#26041;&#38754;&#30340;&#35780;&#20272;&#24046;&#36317;&#65292;&#24182;&#20026;&#19981;&#21516;&#31995;&#32479;&#30340;&#24615;&#33021;&#25552;&#20379;&#26356;&#20840;&#38754;&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20195;&#30721;&#33258;&#21160;&#34917;&#20840;&#31995;&#32479;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#27493;&#65292;&#26377;&#28508;&#21147;&#20026;&#24320;&#21457;&#20154;&#21592;&#25552;&#20379;&#26174;&#33879;&#30340;&#29983;&#20135;&#21147;&#22686;&#24378;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#22522;&#20934;&#20027;&#35201;&#38598;&#20013;&#22312;&#21333;&#20010;&#25991;&#20214;&#20219;&#21153;&#19978;&#65292;&#23545;&#20110;&#26356;&#22797;&#26434;&#30340;&#30495;&#23454;&#19990;&#30028;&#30340;&#22810;&#25991;&#20214;&#32534;&#31243;&#22330;&#26223;&#30340;&#35780;&#20272;&#23384;&#22312;&#24046;&#36317;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;RepoBench&#65292;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;&#20195;&#30721;&#24211;&#32423;&#21035;&#30340;&#20195;&#30721;&#33258;&#21160;&#34917;&#20840;&#31995;&#32479;&#30340;&#26032;&#22522;&#20934;&#12290;RepoBench&#25903;&#25345;Python&#21644;Java&#65292;&#24182;&#21253;&#21547;&#19977;&#20010;&#30456;&#20114;&#20851;&#32852;&#30340;&#35780;&#20272;&#20219;&#21153;&#65306;RepoBench-R&#65288;&#26816;&#32034;&#65289;&#12289;RepoBench-C&#65288;&#20195;&#30721;&#34917;&#20840;&#65289;&#21644;RepoBench-P&#65288;&#27969;&#27700;&#32447;&#65289;&#12290;&#27599;&#20010;&#20219;&#21153;&#20998;&#21035;&#27979;&#37327;&#31995;&#32479;&#26816;&#32034;&#20854;&#20182;&#25991;&#20214;&#20013;&#26368;&#30456;&#20851;&#20195;&#30721;&#29255;&#27573;&#30340;&#33021;&#21147;&#12289;&#20351;&#29992;&#36328;&#25991;&#20214;&#21644;&#25991;&#20214;&#20869;&#19978;&#19979;&#25991;&#39044;&#27979;&#19979;&#19968;&#34892;&#20195;&#30721;&#30340;&#33021;&#21147;&#20197;&#21450;&#22788;&#29702;&#38656;&#35201;&#26816;&#32034;&#21644;&#19979;&#19968;&#34892;&#39044;&#27979;&#32452;&#21512;&#30340;&#22797;&#26434;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;RepoBench&#26088;&#22312;&#20419;&#36827;&#23545;&#24615;&#33021;&#30340;&#26356;&#20840;&#38754;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have greatly advanced code auto-completion systems, with a potential for substantial productivity enhancements for developers. However, current benchmarks mainly focus on single-file tasks, leaving an assessment gap for more complex, real-world, multi-file programming scenarios. To fill this gap, we introduce RepoBench, a new benchmark specifically designed for evaluating repository-level code auto-completion systems. RepoBench supports both Python and Java and consists of three interconnected evaluation tasks: RepoBench-R (Retrieval), RepoBench-C (Code Completion), and RepoBench-P (Pipeline). Each task respectively measures the system's ability to retrieve the most relevant code snippets from other files as cross-file context, predict the next line of code with cross-file and in-file context, and handle complex tasks that require a combination of both retrieval and next-line prediction. RepoBench aims to facilitate a more complete comparison of performance
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#26679;&#24615;&#20248;&#21270;&#31639;&#27861;&#30456;&#32467;&#21512;&#30340; LLMatic &#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#36827;&#34892;&#27979;&#35797;&#65292;&#20165;&#36827;&#34892;2000&#27425;&#25628;&#32034;&#21363;&#21487;&#20135;&#29983;&#39640;&#24615;&#33021;&#32593;&#32476;&#65292;&#21363;&#20351;&#27809;&#26377;&#35813;&#22522;&#20934;&#39046;&#22495;&#30340;&#20808;&#21069;&#30693;&#35782;&#25110;&#20219;&#20309;&#20808;&#21069;&#30340;&#26368;&#20339;&#32467;&#26524;&#30340;&#26333;&#20809;&#12290;</title><link>http://arxiv.org/abs/2306.01102</link><description>&lt;p&gt;
LLMatic: &#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#26679;&#24615;&#20248;&#21270;&#30340;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
LLMatic: Neural Architecture Search via Large Language Models and Quality-Diversity Optimization. (arXiv:2306.01102v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#26679;&#24615;&#20248;&#21270;&#31639;&#27861;&#30456;&#32467;&#21512;&#30340; LLMatic &#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#36827;&#34892;&#27979;&#35797;&#65292;&#20165;&#36827;&#34892;2000&#27425;&#25628;&#32034;&#21363;&#21487;&#20135;&#29983;&#39640;&#24615;&#33021;&#32593;&#32476;&#65292;&#21363;&#20351;&#27809;&#26377;&#35813;&#22522;&#20934;&#39046;&#22495;&#30340;&#20808;&#21069;&#30693;&#35782;&#25110;&#20219;&#20309;&#20808;&#21069;&#30340;&#26368;&#20339;&#32467;&#26524;&#30340;&#26333;&#20809;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#24050;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#23436;&#25104;&#24191;&#27867;&#30340;&#20219;&#21153;&#12290;&#23427;&#20204;&#30340;&#33021;&#21147;&#28085;&#30422;&#20102;&#35768;&#22810;&#39046;&#22495;&#65292;&#23427;&#20204;&#22312;&#20195;&#30721;&#29983;&#25104;&#39046;&#22495;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#12290;&#22312;&#27492;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23558; LLMs &#35270;&#20026;&#21464;&#24322;&#21644;&#20132;&#21449;&#24037;&#20855;&#12290;&#21516;&#26102;&#65292;&#22810;&#26679;&#24615;&#20248;&#21270;&#31639;&#27861;&#24050;&#30693;&#21487;&#20197;&#21457;&#29616;&#22810;&#26679;&#24615;&#21644;&#31283;&#20581;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#23558; LLMs &#30340;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#19982; QD &#35299;&#20915;&#26041;&#26696;&#30340;&#22810;&#26679;&#24615;&#21644;&#40065;&#26834;&#24615;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; LLMatic&#65292;&#19968;&#20010;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034; (NAS) &#31639;&#27861;&#12290;&#34429;&#28982; LLMs &#36890;&#36807;&#25552;&#31034;&#30452;&#25509;&#36827;&#34892; NAS &#32771;&#39564;&#22256;&#38590;&#65292;&#20294; LLMatic &#21033;&#29992;&#31243;&#24207;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992; QD &#26469;&#36827;&#34892;&#25552;&#31034;&#21644;&#32593;&#32476;&#32467;&#26500;&#65292;&#20174;&#32780;&#21019;&#24314;&#22810;&#26679;&#24615;&#21644;&#39640;&#24615;&#33021;&#32593;&#32476;&#12290;&#25105;&#20204;&#22312; CIFAR-10 &#22270;&#20687;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#27979;&#35797;&#20102; LLMatic&#65292;&#35777;&#26126;&#23427;&#21487;&#20197;&#22312;&#20165;&#36827;&#34892; 2000 &#27425;&#25628;&#32034;&#30340;&#24773;&#20917;&#19979;&#20135;&#29983;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32593;&#32476;&#65292;&#21363;&#20351;&#27809;&#26377;&#35813;&#22522;&#20934;&#39046;&#22495;&#30340;&#20808;&#21069;&#30693;&#35782;&#25110;&#20219;&#20309;&#20808;&#21069;&#30340;&#26368;&#20339;&#32467;&#26524;&#30340;&#26333;&#20809;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have emerged as powerful tools capable of accomplishing a broad spectrum of tasks. Their abilities span numerous areas, and one area where they have made a significant impact is in the domain of code generation. In this context, we view LLMs as mutation and crossover tools. Meanwhile, Quality-Diversity (QD) algorithms are known to discover diverse and robust solutions. By merging the code-generating abilities of LLMs with the diversity and robustness of QD solutions, we introduce LLMatic, a Neural Architecture Search (NAS) algorithm. While LLMs struggle to conduct NAS directly through prompts, LLMatic uses a procedural approach, leveraging QD for prompts and network architecture to create diverse and highly performant networks. We test LLMatic on the CIFAR-10 image classification benchmark, demonstrating that it can produce competitive networks with just $2,000$ searches, even without prior knowledge of the benchmark domain or exposure to any previous top-p
&lt;/p&gt;</description></item><item><title>AWQ&#26159;&#19968;&#31181;&#28608;&#27963;&#24863;&#30693;&#30340;&#26435;&#37325;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20445;&#25252;&#23569;&#37327;&#26174;&#33879;&#26435;&#37325;&#26469;&#38477;&#20302;&#37327;&#21270;&#35823;&#24046;&#65292;&#19981;&#20381;&#36182;&#20110;&#21453;&#21521;&#20256;&#25773;&#25110;&#37325;&#26500;&#65292;&#24182;&#22312;&#35821;&#35328;&#24314;&#27169;&#21644;&#39046;&#22495;&#29305;&#23450;&#20219;&#21153;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.00978</link><description>&lt;p&gt;
AWQ&#65306;LLM&#21387;&#32553;&#19982;&#21152;&#36895;&#30340;&#28608;&#27963;&#24863;&#30693;&#26435;&#37325;&#37327;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration. (arXiv:2306.00978v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00978
&lt;/p&gt;
&lt;p&gt;
AWQ&#26159;&#19968;&#31181;&#28608;&#27963;&#24863;&#30693;&#30340;&#26435;&#37325;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20445;&#25252;&#23569;&#37327;&#26174;&#33879;&#26435;&#37325;&#26469;&#38477;&#20302;&#37327;&#21270;&#35823;&#24046;&#65292;&#19981;&#20381;&#36182;&#20110;&#21453;&#21521;&#20256;&#25773;&#25110;&#37325;&#26500;&#65292;&#24182;&#22312;&#35821;&#35328;&#24314;&#27169;&#21644;&#39046;&#22495;&#29305;&#23450;&#20219;&#21153;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20294;&#24040;&#22823;&#30340;&#27169;&#22411;&#22823;&#23567;&#25552;&#39640;&#20102;&#20026;&#26381;&#21153;(&#20869;&#23384;&#22823;&#23567;)&#24102;&#26469;&#30340;&#30828;&#20214;&#38556;&#30861;&#65292;&#24182;&#38477;&#20302;&#20102;&#20196;&#29260;&#29983;&#25104;&#36895;&#24230;(&#20869;&#23384;&#24102;&#23485;)&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28608;&#27963;&#24863;&#30693;&#26435;&#37325;&#37327;&#21270;(AWQ)&#30340;&#30828;&#20214;&#21451;&#22909;&#26041;&#27861;&#65292;&#29992;&#20110;LLM&#20302;&#27604;&#29305;&#26435;&#37325;&#37327;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#19968;&#20010;&#35266;&#23519;&#65306;&#26435;&#37325;&#24182;&#19981;&#26159;&#31561;&#37325;&#35201;&#30340;&#65307;&#20165;&#20445;&#25252;1%&#30340;&#26174;&#33879;&#26435;&#37325;&#23601;&#33021;&#22823;&#22823;&#38477;&#20302;&#37327;&#21270;&#35823;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#23547;&#25214;&#36890;&#36807;&#35266;&#23519;&#28608;&#27963;&#20540;&#32780;&#19981;&#26159;&#26435;&#37325;&#26469;&#20445;&#25252;&#26174;&#33879;&#26435;&#37325;&#30340;&#26368;&#20339;&#25353;&#36890;&#36947;&#32553;&#25918;&#26041;&#27861;&#12290;AWQ&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#21453;&#21521;&#20256;&#25773;&#25110;&#37325;&#26500;&#65292;&#22240;&#27492;&#21487;&#20197;&#24456;&#22909;&#22320;&#20445;&#25345;LLM&#22312;&#19981;&#21516;&#39046;&#22495;&#21644;&#27169;&#24335;&#19979;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#32780;&#19981;&#20250;&#36807;&#24230;&#25311;&#21512;&#26657;&#20934;&#38598;&#12290;AWQ&#22312;&#21508;&#31181;&#35821;&#35328;&#24314;&#27169;&#21644;&#39046;&#22495;&#29305;&#23450;&#22522;&#20934;&#27979;&#35797;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#30001;&#20110;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#23427;&#23454;&#29616;&#20102;&#20248;&#31168;&#30340;&#37327;&#21270;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown excellent performance on various tasks, but the astronomical model size raises the hardware barrier for serving (memory size) and slows down token generation (memory bandwidth). In this paper, we propose Activation-aware Weight Quantization (AWQ), a hardware-friendly approach for LLM low-bit weight-only quantization. Our method is based on the observation that weights are not equally important: protecting only 1% of salient weights can greatly reduce quantization error. We then propose to search for the optimal per-channel scaling that protects the salient weights by observing the activation, not weights. AWQ does not rely on any backpropagation or reconstruction, so it can well preserve LLMs' generalization ability on different domains and modalities, without overfitting to the calibration set. AWQ outperforms existing work on various language modeling and domain-specific benchmarks. Thanks to better generalization, it achieves excellent quantiz
&lt;/p&gt;</description></item><item><title>DNA-GPT&#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26816;&#27979;&#31574;&#30053;&#65292;&#36890;&#36807;Divergent N-Gram&#20998;&#26512;&#26469;&#21457;&#29616;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#19982;&#20154;&#31867;&#20889;&#20316;&#25991;&#26412;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.17359</link><description>&lt;p&gt;
DNA-GPT: &#26080;&#38656;&#35757;&#32451;&#30340;Divergent N-Gram&#20998;&#26512;&#29992;&#20110;&#26816;&#27979;GPT&#29983;&#25104;&#30340;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
DNA-GPT: Divergent N-Gram Analysis for Training-Free Detection of GPT-Generated Text. (arXiv:2305.17359v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17359
&lt;/p&gt;
&lt;p&gt;
DNA-GPT&#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26816;&#27979;&#31574;&#30053;&#65292;&#36890;&#36807;Divergent N-Gram&#20998;&#26512;&#26469;&#21457;&#29616;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#19982;&#20154;&#31867;&#20889;&#20316;&#25991;&#26412;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26174;&#30528;&#25552;&#39640;&#20102;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#27969;&#30021;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#36827;&#23637;&#20063;&#32473;&#26816;&#27979;&#32473;&#23450;&#25991;&#26412;&#30340;&#26469;&#28304;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#24182;&#19988;&#30446;&#21069;&#30340;&#26816;&#27979;&#26041;&#27861;&#30740;&#31350;&#28382;&#21518;&#20110;LLMs&#30340;&#24555;&#36895;&#21457;&#23637;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#35757;&#32451;&#30340;&#26041;&#27861;&#22312;&#28789;&#27963;&#24615;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#36866;&#24212;&#26032;&#39046;&#22495;&#26102;&#65292;&#23427;&#20204;&#24448;&#24448;&#32570;&#20047;&#35299;&#37322;&#33021;&#21147;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#26816;&#27979;&#31574;&#30053;&#65292;&#31216;&#20026;Divergent N-Gram&#20998;&#26512;&#65288;DNA-GPT&#65289;&#12290;&#32473;&#23450;&#19968;&#27573;&#25991;&#26412;&#65292;&#25105;&#20204;&#39318;&#20808;&#25130;&#26029;&#20854;&#20013;&#38388;&#37096;&#20998;&#65292;&#28982;&#21518;&#20165;&#20351;&#29992;&#21069;&#38754;&#30340;&#37096;&#20998;&#20316;&#20026;&#36755;&#20837;&#26469;&#37325;&#26032;&#29983;&#25104;&#21097;&#19979;&#30340;&#37096;&#20998;&#12290;&#36890;&#36807;&#22312;&#40657;&#30418;&#20013;&#36827;&#34892;N-gram&#20998;&#26512;&#25110;&#22312;&#30333;&#30418;&#20013;&#36827;&#34892;&#27010;&#29575;&#20998;&#24067;&#24046;&#24322;&#20998;&#26512;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#20998;&#24067;&#19982;&#20154;&#31867;&#20889;&#20316;&#25991;&#26412;&#30340;&#20998;&#24067;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#24322;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;...
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have notably enhanced the fluency and diversity of machine-generated text. However, this progress also presents a significant challenge in detecting the origin of a given text, and current research on detection methods lags behind the rapid evolution of LLMs. Conventional training-based methods have limitations in flexibility, particularly when adapting to new domains, and they often lack explanatory power. To address this gap, we propose a novel training-free detection strategy called Divergent N-Gram Analysis (DNA-GPT). Given a text, we first truncate it in the middle and then use only the preceding portion as input to the LLMs to regenerate the new remaining parts. By analyzing the differences between the original and new remaining parts through N-gram analysis in black-box or probability divergence in white-box, we unveil significant discrepancies between the distribution of machine-generated text and the distribution of human-written text. We conducted
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;Twitter&#19978;&#20998;&#20139;&#30340;&#22270;&#20687;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;&#26465;&#20214;&#19979;&#30340;&#26367;&#20195;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#12290;&#36890;&#36807;CLIP&#21069;&#32512;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32467;&#21512;&#22270;&#20687;&#21644;&#25512;&#25991;&#20013;&#30340;&#25991;&#26412;&#20449;&#24687;&#65292;&#29983;&#25104;&#20851;&#20110;&#22270;&#20687;&#30340;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#26367;&#20195;&#25991;&#26412;&#12290;</title><link>http://arxiv.org/abs/2305.14779</link><description>&lt;p&gt;
Twitter&#22270;&#20687;&#30340;&#25991;&#26412;&#26465;&#20214;&#19979;&#30340;&#26367;&#20195;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Text Conditional Alt-Text Generation for Twitter Images. (arXiv:2305.14779v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14779
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;Twitter&#19978;&#20998;&#20139;&#30340;&#22270;&#20687;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;&#26465;&#20214;&#19979;&#30340;&#26367;&#20195;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#12290;&#36890;&#36807;CLIP&#21069;&#32512;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32467;&#21512;&#22270;&#20687;&#21644;&#25512;&#25991;&#20013;&#30340;&#25991;&#26412;&#20449;&#24687;&#65292;&#29983;&#25104;&#20851;&#20110;&#22270;&#20687;&#30340;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#26367;&#20195;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#31038;&#20132;&#23186;&#20307;&#29305;&#21035;&#26159;Twitter&#19978;&#20998;&#20139;&#30340;&#22270;&#20687;&#29983;&#25104;&#26367;&#20195;&#25991;&#26412;&#65288;&#25110;alt-text&#65289;&#25551;&#36848;&#30340;&#26041;&#27861;&#12290;&#19982;&#22270;&#20687;&#30340;&#23383;&#24149;&#19981;&#21516;&#65292;&#25991;&#26412;&#26367;&#25442;&#25991;&#26412;&#26356;&#21152;&#30452;&#30333;&#25551;&#36848;&#21644;&#19978;&#19979;&#25991;&#29305;&#23450;&#12290;&#27492;&#22806;&#65292;&#20851;&#38190;&#26159;&#65292;&#21457;&#24067;&#21040;Twitter&#19978;&#30340;&#22270;&#20687;&#36890;&#24120;&#26159;&#30001;&#29992;&#25143;&#32534;&#20889;&#30340;&#25991;&#26412;&#38468;&#21152;&#30340;&#65292;&#23613;&#31649;&#36825;&#20123;&#25991;&#26412;&#19981;&#19968;&#23450;&#25551;&#36848;&#22270;&#20687;&#65292;&#20294;&#21487;&#33021;&#25552;&#20379;&#26377;&#29992;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#22914;&#26524;&#27491;&#30830;&#21033;&#29992;&#21487;&#20197;&#25552;&#20379;&#20449;&#24687;&#65292;&#20363;&#22914;&#25512;&#25991;&#21487;&#33021;&#20250;&#21629;&#21517;&#22270;&#29255;&#20013;&#27169;&#22411;&#20043;&#21069;&#27809;&#26377;&#35265;&#36807;&#30340;&#19981;&#24120;&#35265;&#30340;&#23545;&#35937;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;CLIP&#21069;&#32512;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#27169;&#22411;&#25552;&#21462;&#22270;&#20687;&#30340;&#23884;&#20837;&#24182;&#23558;&#20854;&#20256;&#36882;&#32473;&#26144;&#23556;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#36755;&#20986;&#21333;&#35789;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#30701;&#24207;&#21015;&#65292;&#25110;&#31216;&#20026;&#8220;&#21069;&#32512;&#8221;&#65292;&#25105;&#20204;&#23558;&#25512;&#25991;&#26412;&#36523;&#30340;&#25991;&#26412;&#20063;&#36830;&#25509;&#21040;&#20854;&#20013;&#12290;&#36825;&#26679;&#65292;&#27169;&#22411;&#23601;&#21487;&#20197;&#22312;&#25991;&#31456;&#20013;&#26465;&#20214;&#21270;&#35270;&#35273;&#21644;&#25991;&#26412;&#20449;&#24687;&#12290;&#28982;&#21518;&#23558;&#21512;&#24182;&#30340;&#22810;&#27169;&#24335;&#21069;&#32512;&#20316;&#20026;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we present an approach for generating alternative text (or alt-text) descriptions for images shared on social media, specifically Twitter. This task is more than just a special case of image captioning, as alt-text is both more literally descriptive and context-specific. Also critically, images posted to Twitter are often accompanied by user-written text that despite not necessarily describing the image may provide useful context that if properly leveraged can be informative -- e.g. the tweet may name an uncommon object in the image that the model has not previously seen. We address this with a CLIP prefix model that extracts an embedding of the image and passes it to a mapping network that outputs a short sequence in word embedding space, or a ``prefix'', to which we also concatenate the text from the tweet itself. This lets the model condition on both visual and textual information from the post. The combined multimodal prefix is then fed as a prompt to a pretrained lang
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#36890;&#29992;&#35821;&#38899;&#27169;&#22411;&#36827;&#34892;&#38463;&#25289;&#20271;&#26041;&#35328;&#35782;&#21035;&#30340;&#21442;&#25968;&#39640;&#25928;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#27531;&#24046;&#36866;&#37197;&#22120;&#21644;&#27169;&#22411;&#37325;&#32534;&#31243;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#35760;&#21495;&#30340;&#26631;&#31614;&#26144;&#23556;&#65292;&#24182;&#22312;ADI-17&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#39640;&#31934;&#24230;&#65292;&#21516;&#26102;&#20351;&#29992;PEL&#26041;&#27861;&#36827;&#19968;&#27493;&#20943;&#23569;&#20102;&#35757;&#32451;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2305.11244</link><description>&lt;p&gt;
&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#24102;&#26377;&#39044;&#35757;&#32451;&#36890;&#29992;&#35821;&#38899;&#27169;&#22411;&#30340;&#38463;&#25289;&#20271;&#26041;&#35328;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
A Parameter-Efficient Learning Approach to Arabic Dialect Identification with Pre-Trained General-Purpose Speech Model. (arXiv:2305.11244v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11244
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#36890;&#29992;&#35821;&#38899;&#27169;&#22411;&#36827;&#34892;&#38463;&#25289;&#20271;&#26041;&#35328;&#35782;&#21035;&#30340;&#21442;&#25968;&#39640;&#25928;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#27531;&#24046;&#36866;&#37197;&#22120;&#21644;&#27169;&#22411;&#37325;&#32534;&#31243;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#35760;&#21495;&#30340;&#26631;&#31614;&#26144;&#23556;&#65292;&#24182;&#22312;ADI-17&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#39640;&#31934;&#24230;&#65292;&#21516;&#26102;&#20351;&#29992;PEL&#26041;&#27861;&#36827;&#19968;&#27493;&#20943;&#23569;&#20102;&#35757;&#32451;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21442;&#25968;&#39640;&#25928;&#23398;&#20064;&#65288;PEL&#65289;&#25216;&#26415;&#65292;&#20197;&#37325;&#26032;&#21033;&#29992;&#36890;&#29992;&#35821;&#38899;&#27169;&#22411;&#65288;GSM&#65289;&#36827;&#34892;&#38463;&#25289;&#20271;&#26041;&#35328;&#35782;&#21035;&#65288;ADI&#65289;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#35760;&#21495;&#30340;&#26631;&#31614;&#26144;&#23556;&#65292;&#23558;GSM&#36866;&#24212;&#20110;&#38463;&#25289;&#20271;&#26041;&#35328;&#35782;&#21035;&#65292;&#36890;&#36807;&#27531;&#24046;&#36866;&#37197;&#22120;&#21644;&#27169;&#22411;&#37325;&#32534;&#31243;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#36890;&#36807;vanilla fine-tuning&#22312;ADI-17&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#39640;&#31934;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;PEL&#26041;&#27861;&#36827;&#19968;&#27493;&#20943;&#23569;&#20102;&#35757;&#32451;&#25104;&#26412;&#65292;&#20351;&#29992;&#39069;&#22806;2.5&#65285;&#30340;&#32593;&#32476;&#21487;&#35757;&#32451;&#21442;&#25968;&#21363;&#21487;&#36798;&#21040;fine-tuning&#31934;&#24230;&#30340;1.86&#65285;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#23567;&#22411;&#25968;&#25454;&#38598;&#21644;&#26377;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#26469;&#35782;&#21035;&#38463;&#25289;&#20271;&#26041;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we explore Parameter-Efficient-Learning (PEL) techniques to repurpose a General-Purpose-Speech (GSM) model for Arabic dialect identification (ADI). Specifically, we investigate different setups to incorporate trainable features into a multi-layer encoder-decoder GSM formulation under frozen pre-trained settings. Our architecture includes residual adapter and model reprogramming (input-prompting). We design a token-level label mapping to condition the GSM for Arabic Dialect Identification (ADI). This is challenging due to the high variation in vocabulary and pronunciation among the numerous regional dialects. We achieve new state-of-the-art accuracy on the ADI-17 dataset by vanilla fine-tuning. We further reduce the training budgets with the PEL method, which performs within 1.86% accuracy to fine-tuning using only 2.5% of (extra) network trainable parameters. Our study demonstrates how to identify Arabic dialects using a small dataset and limited computation with open sou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#28982;&#35821;&#35328;&#35268;&#21010;&#65288;NLP&#65289;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#30740;&#31350;LLMs&#22312;&#38656;&#35201;&#29702;&#35299;&#24182;&#22312;&#25991;&#26412;&#20013;&#30456;&#24212;&#36827;&#34892;&#25805;&#20316;&#30340;&#22797;&#26434;&#35268;&#21010;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;CoS&#65292;&#20351;&#29992;&#31616;&#21270;&#30340;&#31526;&#21495;&#31354;&#38388;&#34920;&#31034;&#27861;&#26469;&#34920;&#31034;&#22797;&#26434;&#30340;&#29615;&#22659;&#12290;</title><link>http://arxiv.org/abs/2305.10276</link><description>&lt;p&gt;
&#36830;&#38145;&#31526;&#21495;&#25552;&#31034;&#28608;&#21457;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35268;&#21010;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models. (arXiv:2305.10276v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10276
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#28982;&#35821;&#35328;&#35268;&#21010;&#65288;NLP&#65289;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#30740;&#31350;LLMs&#22312;&#38656;&#35201;&#29702;&#35299;&#24182;&#22312;&#25991;&#26412;&#20013;&#30456;&#24212;&#36827;&#34892;&#25805;&#20316;&#30340;&#22797;&#26434;&#35268;&#21010;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;CoS&#65292;&#20351;&#29992;&#31616;&#21270;&#30340;&#31526;&#21495;&#31354;&#38388;&#34920;&#31034;&#27861;&#26469;&#34920;&#31034;&#22797;&#26434;&#30340;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;LLMs&#22312;&#38656;&#35201;&#29702;&#35299;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#27169;&#25311;&#30340;&#34394;&#25311;&#31354;&#38388;&#29615;&#22659;&#24182;&#22312;&#25991;&#26412;&#20013;&#30456;&#24212;&#36827;&#34892;&#25805;&#20316;&#30340;&#22797;&#26434;&#35268;&#21010;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#33258;&#28982;&#35821;&#35328;&#35268;&#21010;&#65288;NLP&#65289;&#30340;&#22522;&#20934;&#65292;&#23427;&#30001;&#19968;&#32452;&#26032;&#39062;&#30340;&#20219;&#21153;&#32452;&#25104;&#65306;Brick World&#12289;&#22522;&#20110;NLVR&#30340;&#25805;&#20316;&#21644;&#33258;&#28982;&#35821;&#35328;&#23548;&#33322;&#12290;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#27969;&#34892;&#30340;LLMs&#65288;&#22914;ChatGPT&#65289;&#20173;&#28982;&#32570;&#20047;&#22797;&#26434;&#35268;&#21010;&#30340;&#33021;&#21147;&#12290;&#36825;&#24341;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#8212;&#8212;LLMs&#26159;&#21542;&#23545;&#33258;&#28982;&#35821;&#35328;&#20013;&#25551;&#36848;&#30340;&#29615;&#22659;&#26377;&#33391;&#22909;&#30340;&#29702;&#35299;&#65292;&#25110;&#32773;&#20854;&#20182;&#26367;&#20195;&#26041;&#27861;&#65288;&#22914;&#31526;&#21495;&#34920;&#31034;&#65289;&#26159;&#21542;&#26356;&#21152;&#31616;&#21333;&#65292;&#22240;&#27492;&#26356;&#23481;&#26131;&#34987;LLMs&#29702;&#35299;&#65311;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CoS&#65288;Chain-of-Symbol Prompting&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#38142;&#24335;&#20013;&#38388;&#24605;&#32771;&#27493;&#39588;&#20013;&#20351;&#29992;&#31616;&#21270;&#30340;&#31526;&#21495;&#31354;&#38388;&#34920;&#31034;&#27861;&#26469;&#34920;&#31034;&#22797;&#26434;&#30340;&#29615;&#22659;&#12290;CoS&#26131;&#20110;&#20351;&#29992;&#65292;&#19981;&#38656;&#35201;&#23545;LLMs&#36827;&#34892;&#39069;&#22806;&#30340;&#22521;&#35757;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we take the initiative to investigate the performance of LLMs on complex planning tasks that require LLMs to understand a virtual spatial environment simulated via natural language and act correspondingly in text. We propose a benchmark named Natural Language Planning (NLP) composed of a set of novel tasks: Brick World, NLVR-based Manipulations, and Natural Language Navigation. We found that current popular LLMs such as ChatGPT still lack abilities in complex planning. This arises a question -- do the LLMs have a good understanding of the environments described in natural language, or maybe other alternatives such as symbolic representations are neater and hence better to be understood by LLMs? To this end, we propose a novel method called CoS (Chain-of-Symbol Prompting) that represents the complex environments with condensed symbolic spatial representations during the chained intermediate thinking steps. CoS is easy to use and does not need additional training on LLMs. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#31070;&#32463;&#22240;&#23376;&#20998;&#26512;&#27169;&#22411;&#65292;&#20351;&#29992;HuBERT&#20013;&#30340;&#32858;&#31867;&#26041;&#27861;&#26469;&#21457;&#29616;&#38544;&#34255;&#30340;&#22768;&#23398;&#21333;&#20803;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#21333;&#20803;&#23545;&#40784;SSL&#27169;&#22411;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#20135;&#29983;&#35299;&#32806;&#21518;&#30340;&#35821;&#38899;&#34920;&#31034;&#65292;&#20174;&#32780;&#20026;&#19987;&#38376;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;Utterance&#27700;&#24179;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SSNFA&#27169;&#22411;&#22312;&#35828;&#35805;&#20154;&#35782;&#21035;&#12289;&#35821;&#35328;&#35782;&#21035;&#21644;&#24773;&#24863;&#35782;&#21035;&#31561;&#21508;&#31181;&#20219;&#21153;&#20013;&#22343;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;SSL&#27169;&#22411;&#65292;&#24182;&#19988;&#27809;&#26377;&#20219;&#20309;&#29305;&#23450;&#20219;&#21153;&#30340;&#24494;&#35843;&#25110;&#30417;&#30563;&#12290;</title><link>http://arxiv.org/abs/2305.08099</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#31070;&#32463;&#22240;&#23376;&#20998;&#26512;&#35299;&#32806;&#35821;&#38899;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Neural Factor Analysis for Disentangling Utterance-level Speech Representations. (arXiv:2305.08099v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#31070;&#32463;&#22240;&#23376;&#20998;&#26512;&#27169;&#22411;&#65292;&#20351;&#29992;HuBERT&#20013;&#30340;&#32858;&#31867;&#26041;&#27861;&#26469;&#21457;&#29616;&#38544;&#34255;&#30340;&#22768;&#23398;&#21333;&#20803;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#21333;&#20803;&#23545;&#40784;SSL&#27169;&#22411;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#20135;&#29983;&#35299;&#32806;&#21518;&#30340;&#35821;&#38899;&#34920;&#31034;&#65292;&#20174;&#32780;&#20026;&#19987;&#38376;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;Utterance&#27700;&#24179;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SSNFA&#27169;&#22411;&#22312;&#35828;&#35805;&#20154;&#35782;&#21035;&#12289;&#35821;&#35328;&#35782;&#21035;&#21644;&#24773;&#24863;&#35782;&#21035;&#31561;&#21508;&#31181;&#20219;&#21153;&#20013;&#22343;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;SSL&#27169;&#22411;&#65292;&#24182;&#19988;&#27809;&#26377;&#20219;&#20309;&#29305;&#23450;&#20219;&#21153;&#30340;&#24494;&#35843;&#25110;&#30417;&#30563;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#26041;&#38754;&#24050;&#32463;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#22312;&#20302;&#26631;&#27880;&#36164;&#28304;&#24773;&#20917;&#19979;&#35777;&#26126;&#38750;&#24120;&#26377;&#29992;&#65292;&#26412;&#25991;&#38024;&#23545;&#35813;&#25216;&#26415;&#22312;&#35828;&#35805;&#20154;&#12289;&#24773;&#24863;&#21644;&#35821;&#35328;&#35782;&#21035;&#31561;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#38382;&#39064;&#36827;&#34892;&#20102;&#25506;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#23376;&#20998;&#26512;&#27169;&#22411;&#65292;&#20351;&#29992;HuBERT&#20013;&#30340;&#32858;&#31867;&#26041;&#27861;&#26469;&#21457;&#29616;&#38544;&#34255;&#30340;&#22768;&#23398;&#21333;&#20803;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#21333;&#20803;&#23545;&#40784;SSL&#27169;&#22411;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#20135;&#29983;&#35299;&#32806;&#21518;&#30340;&#35821;&#38899;&#34920;&#31034;&#65292;&#20174;&#32780;&#20026;&#19987;&#38376;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;Utterance&#27700;&#24179;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SSNFA&#27169;&#22411;&#22312;&#35828;&#35805;&#20154;&#35782;&#21035;&#12289;&#35821;&#35328;&#35782;&#21035;&#21644;&#24773;&#24863;&#35782;&#21035;&#31561;&#21508;&#31181;&#20219;&#21153;&#20013;&#22343;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;SSL&#27169;&#22411;&#65292;&#24182;&#19988;&#27809;&#26377;&#20219;&#20309;&#29305;&#23450;&#20219;&#21153;&#30340;&#24494;&#35843;&#25110;&#30417;&#30563;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) speech models such as wav2vec and HuBERT have demonstrated state-of-the-art performance on automatic speech recognition (ASR) and proved to be extremely useful in low label-resource settings. However, the success of SSL models has yet to transfer to utterance-level tasks such as speaker, emotion, and language recognition, which still require supervised fine-tuning of the SSL models to obtain good performance. We argue that the problem is caused by the lack of disentangled representations and an utterance-level learning objective for these tasks. Inspired by how HuBERT uses clustering to discover hidden acoustic units, we formulate a factor analysis (FA) model that uses the discovered hidden acoustic units to align the SSL features. The underlying utterance-level representations are disentangled from the content of speech using probabilistic inference on the aligned features. Furthermore, the variational lower bound derived from the FA model provides an ut
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;SAM&#20010;&#24615;&#21270;&#26041;&#27861;PerSAM&#65292;&#21482;&#38656;&#35201;&#19968;&#24352;&#24102;&#26377;&#21442;&#32771;&#25513;&#27169;&#30340;&#21333;&#24352;&#22270;&#20687;&#21363;&#21487;&#23450;&#20301;&#21644;&#20998;&#21106;&#30446;&#26631;&#27010;&#24565;&#65292;&#36824;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;&#19968;&#27425;&#24615;&#24494;&#35843;&#21464;&#20307;PerSAM-F&#65292;&#26088;&#22312;&#35299;&#20915;&#25513;&#27169;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.03048</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#19968;&#27425;&#24615;&#20998;&#21106;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Personalize Segment Anything Model with One Shot. (arXiv:2305.03048v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;SAM&#20010;&#24615;&#21270;&#26041;&#27861;PerSAM&#65292;&#21482;&#38656;&#35201;&#19968;&#24352;&#24102;&#26377;&#21442;&#32771;&#25513;&#27169;&#30340;&#21333;&#24352;&#22270;&#20687;&#21363;&#21487;&#23450;&#20301;&#21644;&#20998;&#21106;&#30446;&#26631;&#27010;&#24565;&#65292;&#36824;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;&#19968;&#27425;&#24615;&#24494;&#35843;&#21464;&#20307;PerSAM-F&#65292;&#26088;&#22312;&#35299;&#20915;&#25513;&#27169;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#25968;&#25454;&#39044;&#35757;&#32451;&#30340;&#25512;&#21160;&#19979;&#65292;&#20998;&#21106;&#20219;&#20309;&#29289;&#20307;&#27169;&#22411;&#65288;SAM&#65289;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#20010;&#24378;&#22823;&#19988;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#38761;&#26032;&#20102;&#20998;&#21106;&#27169;&#22411;&#39046;&#22495;&#12290;&#23613;&#31649;SAM&#38750;&#24120;&#36890;&#29992;&#65292;&#20294;&#33258;&#21160;&#20026;&#29305;&#23450;&#35270;&#35273;&#27010;&#24565;&#23450;&#21046;SAM&#32780;&#19981;&#38656;&#35201;&#25163;&#21160;&#25552;&#31034;&#65292;&#22914;&#22312;&#19981;&#21516;&#22270;&#20687;&#20013;&#33258;&#21160;&#20998;&#21106;&#20320;&#30340;&#23456;&#29289;&#29399;&#31561;&#65292; &#36824;&#26410;&#28145;&#20837;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;SAM&#20010;&#24615;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;PerSAM&#12290;&#21482;&#38656;&#35201;&#19968;&#24352;&#24102;&#26377;&#21442;&#32771;&#25513;&#27169;&#30340;&#21333;&#24352;&#22270;&#20687;&#65292;PerSAM&#39318;&#20808;&#36890;&#36807;&#20301;&#32622;&#20808;&#39564;&#23450;&#20301;&#30446;&#26631;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#19977;&#31181;&#25216;&#26415;&#26469;&#22312;&#20854;&#20182;&#22270;&#20687;&#25110;&#35270;&#39057;&#20013;&#20998;&#21106;&#23427;&#65306;&#30446;&#26631;&#24341;&#23548;&#27880;&#24847;&#21147;&#65292;&#30446;&#26631;&#35821;&#20041;&#25552;&#31034;&#21644;&#32423;&#32852;&#21518;&#22788;&#29702;&#12290;&#36825;&#26679;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#36866;&#24212;&#20102;SAM&#30340;&#31169;&#20154;&#20351;&#29992;&#32780;&#26080;&#38656;&#20219;&#20309;&#35757;&#32451;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#32531;&#35299;&#25513;&#27169;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#19968;&#27425;&#24615;&#24494;&#35843;&#21464;&#20307;&#65292;&#21363;PerSAM-F&#12290;&#20923;&#32467;&#25972;&#20010;SAM&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#21487;&#23398;&#20064;&#26435;&#37325;&#29992;&#20110;&#22810;&#23610;&#24230;&#25513;&#27169;&#65292;&#20165;&#35757;&#32451;2&#20010;&#21442;&#25968;&#21363;&#21487;&#12290;
&lt;/p&gt;
&lt;p&gt;
Driven by large-data pre-training, Segment Anything Model (SAM) has been demonstrated as a powerful and promptable framework, revolutionizing the segmentation models. Despite the generality, customizing SAM for specific visual concepts without man-powered prompting is under explored, e.g., automatically segmenting your pet dog in different images. In this paper, we propose a training-free Personalization approach for SAM, termed as PerSAM. Given only a single image with a reference mask, PerSAM first localizes the target concept by a location prior, and segments it within other images or videos via three techniques: target-guided attention, target-semantic prompting, and cascaded post-refinement. In this way, we effectively adapt SAM for private use without any training. To further alleviate the mask ambiguity, we present an efficient one-shot fine-tuning variant, PerSAM-F. Freezing the entire SAM, we introduce two learnable weights for multi-scale masks, only training 2 parameters wit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#20195;&#25968;&#35821;&#35328;&#29255;&#27573;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#24418;&#24335;&#19978;&#32852;&#31995;&#30340;&#26694;&#26550;&#65292;&#24182;&#20174;MATLANG&#23450;&#20041;&#20102;&#19968;&#20010;&#31526;&#21512;3-WL&#27979;&#35797;&#30340;&#35821;&#27861;&#65292;&#36827;&#32780;&#24471;&#20986;&#19968;&#20010;&#31526;&#21512;3-WL GNN&#27169;&#22411;&#30340;G$^2$N$^2$&#12290;&#27492;&#22806;&#65292;&#35821;&#27861;&#26041;&#27861;&#36824;&#25552;&#20379;&#20102;&#35745;&#31639;&#38271;&#24230;&#20026;&#20845;&#21450;&#20197;&#19979;&#30340;&#29615;&#21644;&#24358;&#29615;&#30340;&#20195;&#25968;&#20844;&#24335;&#65292;&#24182;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20248;&#31168;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.01590</link><description>&lt;p&gt;
&#25216;&#26415;&#25253;&#21578;&#65306;&#22270;&#31070;&#32463;&#32593;&#32476;&#20063;&#21487;&#20197;&#21464;&#24471;&#35821;&#27861;&#21270;
&lt;/p&gt;
&lt;p&gt;
Technical report: Graph Neural Networks go Grammatical. (arXiv:2303.01590v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01590
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#20195;&#25968;&#35821;&#35328;&#29255;&#27573;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#24418;&#24335;&#19978;&#32852;&#31995;&#30340;&#26694;&#26550;&#65292;&#24182;&#20174;MATLANG&#23450;&#20041;&#20102;&#19968;&#20010;&#31526;&#21512;3-WL&#27979;&#35797;&#30340;&#35821;&#27861;&#65292;&#36827;&#32780;&#24471;&#20986;&#19968;&#20010;&#31526;&#21512;3-WL GNN&#27169;&#22411;&#30340;G$^2$N$^2$&#12290;&#27492;&#22806;&#65292;&#35821;&#27861;&#26041;&#27861;&#36824;&#25552;&#20379;&#20102;&#35745;&#31639;&#38271;&#24230;&#20026;&#20845;&#21450;&#20197;&#19979;&#30340;&#29615;&#21644;&#24358;&#29615;&#30340;&#20195;&#25968;&#20844;&#24335;&#65292;&#24182;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20248;&#31168;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#19968;&#20010;&#20195;&#25968;&#35821;&#35328;&#30340;&#19968;&#20010;&#29255;&#27573;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24418;&#24335;&#19978;&#32852;&#31995;&#36215;&#26469;&#12290;&#23427;&#20381;&#36182;&#20110;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;&#65288;CFG&#65289;&#65292;&#23558;&#20195;&#25968;&#25805;&#20316;&#32452;&#32455;&#25104;&#21487;&#20197;&#32763;&#35793;&#20026;GNN&#23618;&#27169;&#22411;&#30340;&#29983;&#25104;&#35268;&#21017;&#12290;&#30001;&#20110;&#30452;&#25509;&#20174;&#35821;&#35328;&#27966;&#29983;&#20986;&#30340;CFG&#30340;&#35268;&#21017;&#21644;&#21464;&#37327;&#21253;&#21547;&#20887;&#20313;&#65292;&#22240;&#27492;&#20171;&#32461;&#20102;&#19968;&#31181;&#35821;&#27861;&#31616;&#21270;&#26041;&#26696;&#65292;&#20351;&#24471;&#23558;&#20854;&#32763;&#35793;&#20026;GNN&#23618;&#25104;&#20026;&#21487;&#33021;&#12290;&#24212;&#29992;&#36825;&#31181;&#31574;&#30053;&#65292;&#20174;MATLANG&#23450;&#20041;&#20102;&#19968;&#20010;&#31526;&#21512;&#31532;&#19977;&#38454;Weisfeiler-Lehman&#65288;3-WL&#65289;&#27979;&#35797;&#35201;&#27714;&#30340;&#35821;&#27861;&#12290;&#20174;&#36825;&#20010;3-WL CFG&#20013;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#19968;&#20010;&#32463;&#36807;&#35777;&#26126;&#31526;&#21512;3-WL GNN&#27169;&#22411;&#30340;G$^2$N$^2$&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#35821;&#27861;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#25552;&#20379;&#35745;&#31639;&#38271;&#24230;&#20026;&#20845;&#21450;&#20197;&#19979;&#30340;&#29615;&#21644;&#24358;&#29615;&#30340;&#20195;&#25968;&#20844;&#24335;&#65292;&#20174;&#32780;&#38416;&#26126;&#20102;3-WL&#30340;&#35745;&#25968;&#33021;&#21147;&#12290;&#22810;&#20010;&#23454;&#39564;&#35777;&#26126;&#65292;G$^2$N$^2$&#22312;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#35201;&#27604;&#20854;&#20182;3-WL GNN&#26356;&#20026;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a framework to formally link a fragment of an algebraic language to a Graph Neural Network (GNN). It relies on Context Free Grammars (CFG) to organise algebraic operations into generative rules that can be translated into a GNN layer model. Since the rules and variables of a CFG directly derived from a language contain redundancies, a grammar reduction scheme is presented making tractable the translation into a GNN layer. Applying this strategy, a grammar compliant with the third-order Weisfeiler-Lehman (3-WL) test is defined from MATLANG. From this 3-WL CFG, we derive a provably 3-WL GNN model called G$^2$N$^2$. Moreover, this grammatical approach allows us to provide algebraic formulas to count the cycles of length up to six and chordal cycles at the edge level, which enlightens the counting power of 3-WL. Several experiments illustrate that G$^2$N$^2$ efficiently outperforms other 3-WL GNNs on many downstream tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;Twitter&#25968;&#25454;&#36827;&#34892;&#20102;&#23545;&#20234;&#26391;&#20844;&#20247;&#23545;COVID-19&#30123;&#33495;&#30340;&#24847;&#35265;&#36827;&#34892;&#30340;&#20840;&#38754;&#20998;&#26512;&#65292;&#32467;&#26524;&#26174;&#31034;&#35813;&#30123;&#33495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#28041;&#21450;&#25919;&#24220;&#38382;&#39064;&#12289;&#23433;&#20840;&#24615;&#12289;&#29369;&#35947;&#19981;&#20915;&#21644;&#21103;&#20316;&#29992;&#31561;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2302.04511</link><description>&lt;p&gt;
&#20851;&#20110;COVID-19&#30123;&#33495;&#30340;&#27874;&#26031;&#25512;&#25991;&#30340;&#22823;&#35268;&#27169;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Large-Scale Analysis of Persian Tweets Regarding Covid-19 Vaccination. (arXiv:2302.04511v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04511
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;Twitter&#25968;&#25454;&#36827;&#34892;&#20102;&#23545;&#20234;&#26391;&#20844;&#20247;&#23545;COVID-19&#30123;&#33495;&#30340;&#24847;&#35265;&#36827;&#34892;&#30340;&#20840;&#38754;&#20998;&#26512;&#65292;&#32467;&#26524;&#26174;&#31034;&#35813;&#30123;&#33495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#28041;&#21450;&#25919;&#24220;&#38382;&#39064;&#12289;&#23433;&#20840;&#24615;&#12289;&#29369;&#35947;&#19981;&#20915;&#21644;&#21103;&#20316;&#29992;&#31561;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
COVID-19&#22823;&#27969;&#34892;&#23545;&#25105;&#20204;&#30340;&#29983;&#27963;&#20135;&#29983;&#20102;&#24040;&#22823;&#24433;&#21709;&#65292;&#23588;&#20854;&#26159;&#20154;&#20204;&#30340;&#20114;&#21160;&#12290;&#24341;&#20837;COVID-19&#30123;&#33495;&#21518;&#65292;&#20851;&#20110;&#25509;&#31181;&#30123;&#33495;&#19982;&#21542;&#30340;&#27491;&#21453;&#24847;&#35265;&#37117;&#26377;&#25152;&#25552;&#20986;&#12290;&#26412;&#25991;&#21033;&#29992;&#20174;Twitter&#33719;&#21462;&#30340;&#25968;&#25454;&#65292;&#21253;&#25324;&#25512;&#25991;&#21644;&#29992;&#25143;&#36164;&#26009;&#65292;&#23545;&#20234;&#26391;&#20844;&#20247;&#23545;&#20896;&#29366;&#30149;&#27602;&#30123;&#33495;&#30340;&#24847;&#35265;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#25628;&#32034;&#26597;&#35810;&#25216;&#26415;&#21644;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#26469;&#25552;&#21462;&#19982;&#30123;&#33495;&#30456;&#20851;&#30340;&#25512;&#25991;&#12290;&#25105;&#20204;&#21033;&#29992;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#27169;&#22411;&#23545;&#25512;&#25991;&#20869;&#23481;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#25552;&#21462;&#22260;&#32469;&#25509;&#31181;&#30340;&#20027;&#39064;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#24773;&#24863;&#20998;&#26512;&#65292;&#20197;&#35780;&#20272;&#20844;&#20247;&#22312;&#36825;&#19968;&#35805;&#39064;&#19978;&#30340;&#24555;&#20048;&#21644;&#24868;&#24594;&#31243;&#24230;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;COVID-19&#30123;&#33495;&#24050;&#24341;&#36215;&#20102;&#21508;&#31181;&#19981;&#21516;&#35282;&#24230;&#30340;&#39640;&#24230;&#20851;&#27880;&#65292;&#22914;&#25919;&#24220;&#38382;&#39064;&#12289;&#23433;&#20840;&#24615;&#25110;&#29369;&#35947;&#19981;&#20915;&#20197;&#21450;&#21103;&#20316;&#29992;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Covid-19 pandemic had an enormous effect on our lives, especially on people's interactions. By introducing Covid-19 vaccines, both positive and negative opinions were raised over the subject of taking vaccines or not. In this paper, using data gathered from Twitter, including tweets and user profiles, we offer a comprehensive analysis of public opinion in Iran about the Coronavirus vaccines. For this purpose, we applied a search query technique combined with a topic modeling approach to extract vaccine-related tweets. We utilized transformer-based models to classify the content of the tweets and extract themes revolving around vaccination. We also conducted an emotion analysis to evaluate the public happiness and anger around this topic. Our results demonstrate that Covid-19 vaccination has attracted considerable attention from different angles, such as governmental issues, safety or hesitancy, and side effects. Moreover, Coronavirus-relevant phenomena like public vaccination and t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#30340;KG&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#31070;&#32463;&#32593;&#32476;&#22312;KG&#20013;&#25191;&#34892;&#22797;&#26434;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#20010;&#24402;&#32435;KG&#23436;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;Hits@10&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#22312;KG&#20013;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2302.01313</link><description>&lt;p&gt;
&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Double Permutation Equivariance for Knowledge Graph Completion. (arXiv:2302.01313v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#30340;KG&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#31070;&#32463;&#32593;&#32476;&#22312;KG&#20013;&#25191;&#34892;&#22797;&#26434;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#20010;&#24402;&#32435;KG&#23436;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;Hits@10&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#22312;KG&#20013;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#30693;&#35782;&#22270;&#35889;(KGs)&#24418;&#24335;&#21270;&#20026;&#19968;&#31181;&#26032;&#22411;&#30340;&#22270;&#65292;&#24182;&#31216;&#20043;&#20026;&#21452;&#20132;&#25442;&#23646;&#24615;&#22270;&#65292;&#20854;&#20013;&#33410;&#28857;&#21644;&#20108;&#20803;&#65288;&#20004;&#20010;&#33410;&#28857;&#20043;&#38388;&#30340;&#65289;&#34920;&#31034;&#24517;&#39035;&#23545;&#33410;&#28857;&#21495;&#21644;&#36793;&#65288;&#21450;&#33410;&#28857;&#65289;&#23646;&#24615;&#65288;&#20851;&#31995;&#21644;&#33410;&#28857;&#29305;&#24449;&#65289;&#30340;&#25490;&#21015;&#31561;&#21464;&#12290;&#21452;&#37325;&#25490;&#21015;&#31561;&#21464;&#30340;KG&#34920;&#31034;&#22312;KG&#20013;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#31561;&#21464;&#24615;&#23545;&#20851;&#31995;&#30340;&#32467;&#26500;&#34920;&#31034;&#20135;&#29983;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#20351;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#22312;KG&#20013;&#25191;&#34892;&#22797;&#26434;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#31561;&#21464;&#34920;&#31034;&#34013;&#22270;&#65292;&#24182;&#27979;&#35797;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#22522;&#20110;GNN&#30340;&#21452;&#25490;&#21015;&#31561;&#21464;&#31070;&#32463;&#32467;&#26500;&#65292;&#22312;WN18RR&#12289;FB237&#21644;NELL995&#24402;&#32435;KG&#23436;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;Hits@10&#27979;&#35797;&#20934;&#30830;&#29575;&#65292;&#24182;&#33021;&#22815;&#20934;&#30830;&#25191;&#34892;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#25191;&#34892;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work provides a formalization of Knowledge Graphs (KGs) as a new class of graphs that we denote doubly exchangeable attributed graphs, where node and pairwise (joint 2-node) representations must be equivariant to permutations of both node ids and edge (&amp; node) attributes (relations &amp; node features). Double-permutation equivariant KG representations open a new research direction in KGs. We show that this equivariance imposes a structural representation of relations that allows neural networks to perform complex logical reasoning tasks in KGs. Finally, we introduce a general blueprint for such equivariant representations and test a simple GNN-based double-permutation equivariant neural architecture that achieve state-of-the-art Hits@10 test accuracy in the WN18RR, FB237 and NELL995 inductive KG completion tasks, and can accurately perform logical reasoning tasks that no existing methods can perform, to the best of our knowledge.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20132;&#26367;&#26356;&#26032;&#65288;AltUp&#65289;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#22686;&#21152;&#35745;&#31639;&#36127;&#25285;&#30340;&#24773;&#20917;&#19979;&#22686;&#21152;&#27169;&#22411;&#30340;&#23481;&#37327;&#65292;&#36890;&#36807;&#23545;&#25193;&#23637;&#34920;&#31034;&#30340;&#23376;&#22359;&#36827;&#34892;&#25805;&#20316;&#24182;&#20351;&#29992;&#39044;&#27979;&#21644;&#20462;&#27491;&#26426;&#21046;&#26469;&#26356;&#26032;&#26410;&#28608;&#27963;&#30340;&#22359;&#12290;&#23454;&#39564;&#35777;&#26126;AltUp&#26041;&#27861;&#22312;&#25552;&#39640;Transformer&#27169;&#22411;&#30340;&#23481;&#37327;&#21644;&#25928;&#29575;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2301.13310</link><description>&lt;p&gt;
&#39640;&#25928;Transformer&#30340;&#20132;&#26367;&#26356;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Alternating Updates for Efficient Transformers. (arXiv:2301.13310v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13310
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20132;&#26367;&#26356;&#26032;&#65288;AltUp&#65289;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#22686;&#21152;&#35745;&#31639;&#36127;&#25285;&#30340;&#24773;&#20917;&#19979;&#22686;&#21152;&#27169;&#22411;&#30340;&#23481;&#37327;&#65292;&#36890;&#36807;&#23545;&#25193;&#23637;&#34920;&#31034;&#30340;&#23376;&#22359;&#36827;&#34892;&#25805;&#20316;&#24182;&#20351;&#29992;&#39044;&#27979;&#21644;&#20462;&#27491;&#26426;&#21046;&#26469;&#26356;&#26032;&#26410;&#28608;&#27963;&#30340;&#22359;&#12290;&#23454;&#39564;&#35777;&#26126;AltUp&#26041;&#27861;&#22312;&#25552;&#39640;Transformer&#27169;&#22411;&#30340;&#23481;&#37327;&#21644;&#25928;&#29575;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#22686;&#21152;&#28145;&#24230;Transformer&#32593;&#32476;&#30340;&#35268;&#27169;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#36136;&#37327;&#21644;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35268;&#27169;&#30340;&#22686;&#21152;&#24448;&#24448;&#20250;&#23548;&#33268;&#35745;&#31639;&#25104;&#26412;&#21644;&#25512;&#29702;&#24310;&#36831;&#30340;&#22823;&#24133;&#22686;&#21152;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20132;&#26367;&#26356;&#26032;&#65288;AltUp&#65289;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#26131;&#23454;&#29616;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22686;&#21152;&#27169;&#22411;&#30340;&#23481;&#37327;&#32780;&#19981;&#22686;&#21152;&#35745;&#31639;&#36127;&#25285;&#12290;AltUp&#36890;&#36807;&#22312;&#27599;&#19968;&#23618;&#20013;&#23545;&#25193;&#23637;&#34920;&#31034;&#30340;&#23376;&#22359;&#36827;&#34892;&#25805;&#20316;&#65292;&#24182;&#20351;&#29992;&#39044;&#27979;&#21644;&#20462;&#27491;&#26426;&#21046;&#26469;&#26356;&#26032;&#26410;&#28608;&#27963;&#30340;&#22359;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#20165;&#22312;&#24310;&#36831;&#19978;&#24494;&#19981;&#36275;&#36947;&#30340;&#24773;&#20917;&#19979;&#25193;&#22823;&#20102;&#23398;&#20064;&#34920;&#31034;&#65292;&#21363;&#26631;&#35760;&#23884;&#20837;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;AltUp&#30340;&#25193;&#23637;&#65292;&#20363;&#22914;&#20854;&#22312;&#24207;&#21015;&#32500;&#24230;&#19978;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;AltUp&#19982;&#29616;&#26377;&#26041;&#27861;&#65288;&#22914;&#31232;&#30095;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#33719;&#24471;&#20855;&#26377;&#26356;&#39640;&#23481;&#37327;&#30340;&#39640;&#25928;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;Transformer&#27169;&#22411;&#21644;&#35821;&#35328;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#20102;AltUp&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has been well established that increasing scale in deep transformer networks leads to improved quality and performance. However, this increase in scale often comes with prohibitive increases in compute cost and inference latency. We introduce Alternating Updates (AltUp), a simple-to-implement method to increase a model's capacity without the computational burden. AltUp enables the widening of the learned representation, i.e., the token embedding, while only incurring a negligible increase in latency. AltUp achieves this by working on a subblock of the widened representation at each layer and using a predict-and-correct mechanism to update the inactivated blocks. We present extensions of AltUp, such as its applicability to the sequence dimension, and demonstrate how AltUp can be synergistically combined with existing approaches, such as Sparse Mixture-of-Experts models, to obtain efficient models with even higher capacity. Our experiments on benchmark transformer models and language 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#33258;&#21160;&#25991;&#26412;&#25688;&#35201;&#31995;&#32479;&#30340;&#32508;&#36848;&#65292;&#21253;&#25324;&#26041;&#27861;&#12289;&#25968;&#25454;&#12289;&#35780;&#20272;&#21644;&#32534;&#30721;&#12290;&#20316;&#32773;&#36890;&#36807;&#24341;&#29992;&#30340;&#26041;&#24335;&#22238;&#39038;&#20102;&#30456;&#20851;&#25991;&#29486;&#65292;&#24182;&#20171;&#32461;&#20102;&#19981;&#21516;&#30340;&#25688;&#35201;&#29983;&#25104;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36824;&#23545;&#21487;&#29992;&#20110;&#35780;&#20272;&#21644;&#25968;&#25454;&#35757;&#32451;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#24182;&#20351;&#29992;CNN&#35821;&#26009;&#24211;&#25968;&#25454;&#38598;&#23545;&#26041;&#27861;&#36827;&#34892;&#20102;&#23454;&#35777;&#25506;&#32034;&#12290;</title><link>http://arxiv.org/abs/2301.03403</link><description>&lt;p&gt;
&#33258;&#21160;&#25991;&#26412;&#25688;&#35201;&#25216;&#26415;&#30340;&#32508;&#21512;&#22238;&#39038;&#65306;&#26041;&#27861;&#12289;&#25968;&#25454;&#12289;&#35780;&#20272;&#21644;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
A comprehensive review of automatic text summarization techniques: method, data, evaluation and coding. (arXiv:2301.03403v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.03403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#33258;&#21160;&#25991;&#26412;&#25688;&#35201;&#31995;&#32479;&#30340;&#32508;&#36848;&#65292;&#21253;&#25324;&#26041;&#27861;&#12289;&#25968;&#25454;&#12289;&#35780;&#20272;&#21644;&#32534;&#30721;&#12290;&#20316;&#32773;&#36890;&#36807;&#24341;&#29992;&#30340;&#26041;&#24335;&#22238;&#39038;&#20102;&#30456;&#20851;&#25991;&#29486;&#65292;&#24182;&#20171;&#32461;&#20102;&#19981;&#21516;&#30340;&#25688;&#35201;&#29983;&#25104;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36824;&#23545;&#21487;&#29992;&#20110;&#35780;&#20272;&#21644;&#25968;&#25454;&#35757;&#32451;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#24182;&#20351;&#29992;CNN&#35821;&#26009;&#24211;&#25968;&#25454;&#38598;&#23545;&#26041;&#27861;&#36827;&#34892;&#20102;&#23454;&#35777;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#33258;&#21160;&#25991;&#26412;&#25688;&#35201;&#31995;&#32479;&#30340;&#25991;&#29486;&#32508;&#36848;&#12290;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#24341;&#29992;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20174;&#25105;&#20204;&#24050;&#32463;&#25484;&#25569;&#30340;&#20851;&#20110;&#27599;&#20010;&#25105;&#20204;&#24819;&#28085;&#30422;&#30340;&#20027;&#39064;&#30340;&#19968;&#20123;&#27969;&#34892;&#21644;&#33879;&#21517;&#30340;&#35770;&#25991;&#24320;&#22987;&#65292;&#28982;&#21518;&#25105;&#20204;&#36861;&#36394;&#20102;&#8220;&#21521;&#21518;&#24341;&#29992;&#8221;&#65288;&#34987;&#25105;&#20204;&#20043;&#21069;&#30693;&#36947;&#30340;&#19968;&#31995;&#21015;&#35770;&#25991;&#24341;&#29992;&#30340;&#35770;&#25991;&#65289;&#21644;&#8220;&#21521;&#21069;&#24341;&#29992;&#8221;&#65288;&#24341;&#29992;&#25105;&#20204;&#20043;&#21069;&#30693;&#36947;&#30340;&#19968;&#31995;&#21015;&#35770;&#25991;&#30340;&#36739;&#26032;&#35770;&#25991;&#65289;&#12290;&#20026;&#20102;&#32452;&#32455;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21508;&#31181;&#22522;&#20110;&#19981;&#21516;&#26426;&#21046;&#29983;&#25104;&#25688;&#35201;&#30340;&#33258;&#21160;&#25991;&#26412;&#25688;&#35201;&#26041;&#27861;&#12290;&#38500;&#20102;&#20171;&#32461;&#26041;&#27861;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;&#21487;&#29992;&#20110;&#25688;&#35201;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#21644;&#29992;&#20110;&#35780;&#20272;&#25688;&#35201;&#36136;&#37327;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#22238;&#39038;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;CNN&#35821;&#26009;&#24211;&#25968;&#25454;&#38598;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#20102;&#23454;&#35777;&#25506;&#32034;&#65292;&#35813;&#25968;&#25454;&#38598;&#20026;&#25277;&#21462;&#24335;&#21644;&#29983;&#25104;&#24335;&#26041;&#27861;&#25552;&#20379;&#20102;&#37329;&#26631;&#20934;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide a literature review about Automatic Text Summarization (ATS) systems. We consider a citation-based approach. We start with some popular and well-known papers that we have in hand about each topic we want to cover and we have tracked the "backward citations" (papers that are cited by the set of papers we knew beforehand) and the "forward citations" (newer papers that cite the set of papers we knew beforehand). In order to organize the different methods, we present the diverse approaches to ATS guided by the mechanisms they use to generate a summary. Besides presenting the methods, we also present an extensive review of the datasets available for summarization tasks and the methods used to evaluate the quality of the summaries. Finally, we present an empirical exploration of these methods using the CNN Corpus dataset that provides golden summaries for extractive and abstractive methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21098;&#36753;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#23427;&#28040;&#38500;&#20102;&#20026;&#24046;&#20998;&#38544;&#31169;&#20248;&#21270;&#22120;&#35843;&#25972;&#21098;&#36753;&#38408;&#20540;&#30340;&#38656;&#35201;&#65292;&#24182;&#22312;&#38750;&#20984;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#35777;&#26126;&#20102;&#33258;&#21160;&#21098;&#36753;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2206.07136</link><description>&lt;p&gt;
&#33258;&#21160;&#21098;&#36753;&#65306;&#26356;&#31616;&#21333;&#21644;&#26356;&#24378;&#22823;&#30340;&#24046;&#20998;&#38544;&#31169;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Automatic Clipping: Differentially Private Deep Learning Made Easier and Stronger. (arXiv:2206.07136v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.07136
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21098;&#36753;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#23427;&#28040;&#38500;&#20102;&#20026;&#24046;&#20998;&#38544;&#31169;&#20248;&#21270;&#22120;&#35843;&#25972;&#21098;&#36753;&#38408;&#20540;&#30340;&#38656;&#35201;&#65292;&#24182;&#22312;&#38750;&#20984;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#35777;&#26126;&#20102;&#33258;&#21160;&#21098;&#36753;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#26679;&#26412;&#26799;&#24230;&#21098;&#36753;&#26159;&#23454;&#29616;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24046;&#20998;&#38544;&#31169;(DP)&#35757;&#32451;&#30340;&#20851;&#38190;&#31639;&#27861;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#21098;&#36753;&#38408;&#20540;R&#30340;&#36873;&#25321;&#23545;&#20110;&#22312;DP&#19979;&#23454;&#29616;&#39640;&#20934;&#30830;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26131;&#20110;&#20351;&#29992;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#31216;&#20026;&#33258;&#21160;&#21098;&#36753;&#65292;&#23427;&#28040;&#38500;&#20102;&#20026;&#20219;&#20309;DP&#20248;&#21270;&#22120;&#65288;&#21253;&#25324;DP-SGD&#12289;DP-Adam&#12289;DP-LAMB&#31561;&#65289;&#35843;&#25972;R&#30340;&#38656;&#35201;&#12290;&#33258;&#21160;&#21464;&#37327;&#19982;&#29616;&#26377;&#30340;DP&#20248;&#21270;&#22120;&#19968;&#26679;&#20855;&#26377;&#38544;&#31169;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#65292;&#20294;&#19981;&#38656;&#35201;DP&#29305;&#23450;&#30340;&#36229;&#21442;&#25968;&#65292;&#22240;&#27492;&#20351;&#24471;DP&#35757;&#32451;&#20687;&#26631;&#20934;&#30340;&#38750;&#38544;&#31169;&#35757;&#32451;&#19968;&#26679;&#21487;&#34892;&#12290;&#25105;&#20204;&#22312;&#38750;&#20984;&#24773;&#20917;&#19979;&#23545;&#33258;&#21160;DP-SGD&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#34920;&#26126;&#22312;&#26679;&#26412;&#26799;&#24230;&#30340;&#23545;&#31216;&#24615;&#22122;&#22768;&#20551;&#35774;&#19979;&#65288;&#22312;&#38750;DP&#25991;&#29486;&#20013;&#24120;&#29992;&#65289;&#65292;&#23427;&#33021;&#22815;&#20139;&#21463;&#19982;&#26631;&#20934;SGD&#30456;&#21516;&#30340;&#28176;&#36817;&#25910;&#25947;&#36895;&#29575;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#35821;&#35328;&#21644;&#35270;&#35273;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#33258;&#21160;&#21098;&#36753;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Per-example gradient clipping is a key algorithmic step that enables practical differential private (DP) training for deep learning models. The choice of clipping threshold R, however, is vital for achieving high accuracy under DP. We propose an easy-to-use replacement, called automatic clipping, that eliminates the need to tune R for any DP optimizers, including DP-SGD, DP-Adam, DP-LAMB and many others. The automatic variants are as private and computationally efficient as existing DP optimizers, but require no DP-specific hyperparameters and thus make DP training as amenable as the standard non-private training. We give a rigorous convergence analysis of automatic DP-SGD in the non-convex setting, showing that it can enjoy an asymptotic convergence rate that matches the standard SGD, under a symmetric gradient noise assumption of the per-sample gradients (commonly used in the non-DP literature). We demonstrate on various language and vision tasks that automatic clipping outperforms o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32763;&#35793;&#30340;&#35789;&#20041;&#28040;&#27495;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20041;&#19968;&#23545;&#19968;&#32763;&#35793;&#65288;OSPT&#65289;&#30340;&#20851;&#38190;&#23646;&#24615;&#65292;&#23454;&#29616;&#20102;&#39640;&#36798;93&#65285;&#30340;&#31934;&#30830;&#24230;&#20197;&#21450;&#26368;&#39640;4.6&#65285;&#30340;WSD&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2106.06082</link><description>&lt;p&gt;
&#19968;&#23545;&#19968;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
One Sense per Translation. (arXiv:2106.06082v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.06082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32763;&#35793;&#30340;&#35789;&#20041;&#28040;&#27495;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20041;&#19968;&#23545;&#19968;&#32763;&#35793;&#65288;OSPT&#65289;&#30340;&#20851;&#38190;&#23646;&#24615;&#65292;&#23454;&#29616;&#20102;&#39640;&#36798;93&#65285;&#30340;&#31934;&#30830;&#24230;&#20197;&#21450;&#26368;&#39640;4.6&#65285;&#30340;WSD&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35789;&#20041;&#28040;&#27495;&#65288;WSD&#65289;&#26159;&#30830;&#23450;&#19978;&#19979;&#25991;&#20013;&#35789;&#20041;&#30340;&#20219;&#21153;&#12290;&#32763;&#35793;&#34987;&#29992;&#20316;WSD&#30340;&#30693;&#35782;&#28304;&#65292;&#29978;&#33267;&#29992;&#20316;&#38480;&#23450;&#35789;&#20041;&#30340;&#25163;&#27573;&#12290;&#26412;&#25991;&#23450;&#20041;&#20102;&#35789;&#20041;&#21644;&#32763;&#35793;&#20043;&#38388;&#20851;&#31995;&#30340;&#19977;&#20010;&#29702;&#35770;&#23646;&#24615;&#65292;&#24182;&#35770;&#35777;&#20102;&#23427;&#20204;&#26159;&#20351;&#29992;&#32763;&#35793;&#20316;&#20026;&#35789;&#20041;&#24211;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#19968;&#23545;&#19968;&#32763;&#35793;&#65288;OSPT&#65289;&#30340;&#20851;&#38190;&#23646;&#24615;&#20026;&#22522;&#20110;&#32763;&#35793;&#30340;WSD&#26041;&#27861;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;&#20869;&#22312;&#35780;&#20272;&#23454;&#39564;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#25163;&#21160;&#35821;&#26009;&#24211;&#27880;&#37322;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36798;&#21040;&#20102;&#32422;93&#65285;&#30340;&#31934;&#30830;&#24230;&#12290;&#22806;&#22312;&#35780;&#20272;&#23454;&#39564;&#26174;&#31034;&#65292;&#22312;&#22256;&#38590;&#30340;WSD&#25968;&#25454;&#38598;&#19978;&#65292;WSD&#30340;F1&#20998;&#25968;&#25552;&#39640;&#20102;&#26368;&#39640;&#36798;4.6&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Word sense disambiguation (WSD) is the task of determining the sense of a word in context. Translations have been used in WSD as a source of knowledge, and even as a means of delimiting word senses. In this paper, we define three theoretical properties of the relationship between senses and translations, and argue that they constitute necessary conditions for using translations as sense inventories. The key property of One Sense per Translation (OSPT) provides a foundation for a translation-based WSD method. The results of an intrinsic evaluation experiment indicate that our method achieves a precision of approximately 93% compared to manual corpus annotations. Our extrinsic evaluation experiments demonstrate WSD improvements of up to 4.6% F1-score on difficult WSD datasets.
&lt;/p&gt;</description></item></channel></rss>