<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>FigCaps-HF&#26159;&#19968;&#20010;&#22270;&#20687;&#29983;&#25104;&#26631;&#39064;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#34701;&#20837;&#39046;&#22495;&#19987;&#23478;&#30340;&#21453;&#39304;&#24847;&#35265;&#65292;&#29983;&#25104;&#31526;&#21512;&#35835;&#32773;&#20559;&#22909;&#30340;&#39640;&#36136;&#37327;&#22270;&#20687;&#26631;&#39064;&#12290;&#23558;&#33258;&#21160;&#35780;&#20272;&#21644;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#25913;&#21892;&#29983;&#25104;&#30340;&#26631;&#39064;&#19982;&#35835;&#32773;&#20559;&#22909;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.10867</link><description>&lt;p&gt;
FigCaps-HF:&#19968;&#20010;&#22522;&#20110;&#20154;&#31867;&#21453;&#39304;&#30340;&#22270;&#20687;&#29983;&#25104;&#26631;&#39064;&#26694;&#26550;&#21644;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
FigCaps-HF: A Figure-to-Caption Generative Framework and Benchmark with Human Feedback. (arXiv:2307.10867v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10867
&lt;/p&gt;
&lt;p&gt;
FigCaps-HF&#26159;&#19968;&#20010;&#22270;&#20687;&#29983;&#25104;&#26631;&#39064;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#34701;&#20837;&#39046;&#22495;&#19987;&#23478;&#30340;&#21453;&#39304;&#24847;&#35265;&#65292;&#29983;&#25104;&#31526;&#21512;&#35835;&#32773;&#20559;&#22909;&#30340;&#39640;&#36136;&#37327;&#22270;&#20687;&#26631;&#39064;&#12290;&#23558;&#33258;&#21160;&#35780;&#20272;&#21644;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#25913;&#21892;&#29983;&#25104;&#30340;&#26631;&#39064;&#19982;&#35835;&#32773;&#20559;&#22909;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#39064;&#23545;&#20110;&#29702;&#35299;&#31185;&#23398;&#21487;&#35270;&#21270;&#21644;&#25991;&#26723;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#31185;&#23398;&#22270;&#20687;&#29983;&#25104;&#26631;&#39064;&#26041;&#27861;&#20381;&#36182;&#20110;&#20174;&#25991;&#26723;&#20013;&#25552;&#21462;&#30340;&#22270;&#20687;-&#26631;&#39064;&#37197;&#23545;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#20854;&#20013;&#35768;&#22810;&#37197;&#23545;&#22312;&#24110;&#21161;&#24615;&#12289;&#35299;&#37322;&#24615;&#21644;&#35270;&#35273;&#25551;&#36848;&#24615;&#31561;&#25351;&#26631;&#19978;&#23384;&#22312;&#19981;&#36275;&#65292;&#23548;&#33268;&#29983;&#25104;&#30340;&#26631;&#39064;&#19982;&#35835;&#32773;&#20559;&#22909;&#19981;&#19968;&#33268;&#12290;&#20026;&#20102;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#26631;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;FigCaps-HF&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#22270;&#20687;&#29983;&#25104;&#26631;&#39064;&#26694;&#26550;&#65292;&#21487;&#20197;&#34701;&#20837;&#39046;&#22495;&#19987;&#23478;&#30340;&#21453;&#39304;&#24847;&#35265;&#65292;&#20197;&#29983;&#25104;&#20248;&#21270;&#20102;&#35835;&#32773;&#20559;&#22909;&#30340;&#26631;&#39064;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#21547;1&#65289;&#19968;&#31181;&#35780;&#20272;&#22270;&#20687;-&#26631;&#39064;&#37197;&#23545;&#36136;&#37327;&#30340;&#33258;&#21160;&#26041;&#27861;&#65292;2&#65289;&#19968;&#31181;&#22522;&#20110;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#29983;&#25104;&#24335;&#22270;&#20687;&#29983;&#25104;&#26631;&#39064;&#27169;&#22411;&#20197;&#31526;&#21512;&#35835;&#32773;&#20559;&#22909;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#27169;&#22411;&#19978;&#25913;&#36827;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#31616;&#21333;&#30340;&#23398;&#20064;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Captions are crucial for understanding scientific visualizations and documents. Existing captioning methods for scientific figures rely on figure-caption pairs extracted from documents for training, many of which fall short with respect to metrics like helpfulness, explainability, and visual-descriptiveness [15] leading to generated captions being misaligned with reader preferences. To enable the generation of high-quality figure captions, we introduce FigCaps-HF a new framework for figure-caption generation that can incorporate domain expert feedback in generating captions optimized for reader preferences. Our framework comprises of 1) an automatic method for evaluating quality of figure-caption pairs, 2) a novel reinforcement learning with human feedback (RLHF) method to optimize a generative figure-to-caption model for reader preferences. We demonstrate the effectiveness of our simple learning framework by improving performance over standard fine-tuning across different types of mod
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#20998;&#21106;&#19982;&#32465;&#23450;"&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#25913;&#36827;&#29983;&#25104;&#35821;&#20041;&#25252;&#29702;&#30340;&#25928;&#26524;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#26032;&#30340;&#25439;&#22833;&#30446;&#26631;&#65292;&#21253;&#25324;&#20851;&#27880;&#20002;&#22833;&#21644;&#32465;&#23450;&#20002;&#22833;&#65292;&#20197;&#35299;&#20915;&#22797;&#26434;&#25552;&#31034;&#21644;&#19981;&#36866;&#24403;&#23646;&#24615;&#32465;&#23450;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.10864</link><description>&lt;p&gt;
&#23558;&#27880;&#24847;&#21147;&#20998;&#21106;&#19982;&#32465;&#23450;&#29992;&#20110;&#25913;&#36827;&#29983;&#25104;&#35821;&#20041;&#25252;&#29702;
&lt;/p&gt;
&lt;p&gt;
Divide &amp; Bind Your Attention for Improved Generative Semantic Nursing. (arXiv:2307.10864v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#20998;&#21106;&#19982;&#32465;&#23450;"&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#25913;&#36827;&#29983;&#25104;&#35821;&#20041;&#25252;&#29702;&#30340;&#25928;&#26524;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#26032;&#30340;&#25439;&#22833;&#30446;&#26631;&#65292;&#21253;&#25324;&#20851;&#27880;&#20002;&#22833;&#21644;&#32465;&#23450;&#20002;&#22833;&#65292;&#20197;&#35299;&#20915;&#22797;&#26434;&#25552;&#31034;&#21644;&#19981;&#36866;&#24403;&#23646;&#24615;&#32465;&#23450;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#20852;&#30340;&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;&#31283;&#23450;&#25193;&#25955;&#65288;SD&#65289;&#65292;&#23637;&#31034;&#20102;&#39640;&#24230;&#36924;&#30495;&#30340;&#21387;&#20498;&#24615;&#32467;&#26524;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#20173;&#28982;&#38590;&#20197;&#23436;&#20840;&#20381;&#29031;&#36755;&#20837;&#25552;&#31034;&#29983;&#25104;&#22270;&#20687;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#8212;&#8212;&#20851;&#27880;&#19982;&#28608;&#21457;&#65292;&#24341;&#20837;&#20102;&#29983;&#25104;&#35821;&#20041;&#25252;&#29702;&#65288;GSN&#65289;&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#22312;&#25512;&#26029;&#26102;&#20248;&#21270;&#36328;&#27880;&#24847;&#21147;&#20197;&#26356;&#22909;&#22320;&#34701;&#20837;&#35821;&#20041;&#12290;&#23427;&#22312;&#29983;&#25104;&#31616;&#21333;&#25552;&#31034;&#65292;&#22914;&#8220;&#19968;&#21482;&#29483;&#21644;&#19968;&#21482;&#29399;&#8221;&#65292;&#26041;&#38754;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#22788;&#29702;&#26356;&#22797;&#26434;&#30340;&#25552;&#31034;&#20197;&#21450;&#35299;&#20915;&#19981;&#36866;&#24403;&#30340;&#23646;&#24615;&#32465;&#23450;&#38382;&#39064;&#26041;&#38754;&#30340;&#21151;&#25928;&#26377;&#25152;&#19979;&#38477;&#12290;&#20026;&#20102;&#24212;&#23545;&#22797;&#26434;&#25552;&#31034;&#25110;&#28041;&#21450;&#22810;&#20010;&#23454;&#20307;&#30340;&#22330;&#26223;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#24182;&#23454;&#29616;&#25913;&#36827;&#30340;&#23646;&#24615;&#32465;&#23450;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20998;&#21106;&#19982;&#32465;&#23450;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;GSN&#25439;&#22833;&#30446;&#26631;&#65306;&#19968;&#31181;&#26032;&#30340;&#20851;&#27880;&#20002;&#22833;&#21644;&#19968;&#31181;&#32465;&#23450;&#20002;&#22833;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20854;&#33021;&#22815;&#26356;&#22909;&#22320;&#23558;&#35821;&#20041;&#32435;&#20837;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#29305;&#28857;&#19978;&#33073;&#39062;&#32780;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emerging large-scale text-to-image generative models, e.g., Stable Diffusion (SD), have exhibited overwhelming results with high fidelity. Despite the magnificent progress, current state-of-the-art models still struggle to generate images fully adhering to the input prompt. Prior work, Attend &amp; Excite, has introduced the concept of Generative Semantic Nursing (GSN), aiming to optimize cross-attention during inference time to better incorporate the semantics. It demonstrates promising results in generating simple prompts, e.g., ``a cat and a dog''. However, its efficacy declines when dealing with more complex prompts, and it does not explicitly address the problem of improper attribute binding. To address the challenges posed by complex prompts or scenarios involving multiple entities and to achieve improved attribute binding, we propose Divide &amp; Bind. We introduce two novel loss objectives for GSN: a novel attendance loss and a binding loss. Our approach stands out in its ability to fa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20998;&#26512;&#20102;Yelp&#35780;&#35770;&#21644;&#39135;&#29289;&#31867;&#22411;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#19981;&#21516;&#31867;&#22411;&#30340;&#39135;&#29289;&#22312;&#35780;&#20998;&#12289;&#24773;&#24863;&#21644;&#20027;&#39064;&#19978;&#26377;&#19981;&#21516;&#30340;&#21464;&#21270;&#21644;&#20998;&#24067;&#12290;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#22235;&#31867;&#39135;&#29289;&#31867;&#22411;&#65292;&#24182;&#21457;&#29616;&#35780;&#35770;&#32773;&#22312;&#35780;&#35770;&#19981;&#21516;&#31867;&#22411;&#30340;&#39135;&#29289;&#26102;&#20542;&#21521;&#20110;&#20851;&#27880;&#19981;&#21516;&#30340;&#20027;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.10826</link><description>&lt;p&gt;
Yelp&#35780;&#35770;&#21644;&#39135;&#29289;&#31867;&#22411;: &#23545;&#35780;&#20998;&#12289;&#24773;&#24863;&#21644;&#20027;&#39064;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Yelp Reviews and Food Types: A Comparative Analysis of Ratings, Sentiments, and Topics. (arXiv:2307.10826v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10826
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20998;&#26512;&#20102;Yelp&#35780;&#35770;&#21644;&#39135;&#29289;&#31867;&#22411;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#19981;&#21516;&#31867;&#22411;&#30340;&#39135;&#29289;&#22312;&#35780;&#20998;&#12289;&#24773;&#24863;&#21644;&#20027;&#39064;&#19978;&#26377;&#19981;&#21516;&#30340;&#21464;&#21270;&#21644;&#20998;&#24067;&#12290;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#22235;&#31867;&#39135;&#29289;&#31867;&#22411;&#65292;&#24182;&#21457;&#29616;&#35780;&#35770;&#32773;&#22312;&#35780;&#35770;&#19981;&#21516;&#31867;&#22411;&#30340;&#39135;&#29289;&#26102;&#20542;&#21521;&#20110;&#20851;&#27880;&#19981;&#21516;&#30340;&#20027;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#23519;&#20102;Yelp&#35780;&#35770;&#21644;&#39135;&#29289;&#31867;&#22411;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#30740;&#31350;&#20102;&#35780;&#20998;&#12289;&#24773;&#24863;&#21644;&#20027;&#39064;&#22312;&#19981;&#21516;&#31867;&#22411;&#39135;&#29289;&#20043;&#38388;&#30340;&#21464;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#35780;&#35770;&#35780;&#20998;&#21644;&#24773;&#24863;&#22312;&#19981;&#21516;&#39135;&#29289;&#31867;&#22411;&#20013;&#30340;&#21464;&#21270;&#65292;&#22522;&#20110;&#35780;&#20998;&#21644;&#24773;&#24863;&#23545;&#39135;&#29289;&#31867;&#22411;&#36827;&#34892;&#32858;&#31867;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25512;&#26029;&#35780;&#35770;&#20027;&#39064;&#65292;&#24182;&#27604;&#36739;&#19981;&#21516;&#39135;&#29289;&#31867;&#22411;&#30340;&#20027;&#39064;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#19968;&#20123;&#39135;&#29289;&#31867;&#22411;&#20855;&#26377;&#30456;&#20284;&#30340;&#35780;&#20998;&#12289;&#24773;&#24863;&#21644;&#20027;&#39064;&#20998;&#24067;&#65292;&#32780;&#20854;&#20182;&#31867;&#22411;&#20855;&#26377;&#19981;&#21516;&#30340;&#27169;&#24335;&#12290;&#25105;&#20204;&#22522;&#20110;&#35780;&#20998;&#21644;&#24773;&#24863;&#37492;&#21035;&#20986;&#20102;&#22235;&#31867;&#39135;&#29289;&#31867;&#22411;&#65292;&#24182;&#21457;&#29616;&#35780;&#35770;&#32773;&#22312;&#35780;&#35770;&#26576;&#20123;&#39135;&#29289;&#31867;&#22411;&#26102;&#20542;&#21521;&#20110;&#20851;&#27880;&#19981;&#21516;&#30340;&#20027;&#39064;&#12290;&#36825;&#20123;&#21457;&#29616;&#23545;&#20110;&#20102;&#35299;&#29992;&#25143;&#34892;&#20026;&#12289;&#25991;&#21270;&#23545;&#25968;&#23383;&#23186;&#20307;&#24179;&#21488;&#30340;&#24433;&#21709;&#20197;&#21450;&#20419;&#36827;&#36328;&#25991;&#21270;&#29702;&#35299;&#21644;&#27427;&#36175;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study examines the relationship between Yelp reviews and food types, investigating how ratings, sentiments, and topics vary across different types of food. Specifically, we analyze how ratings and sentiments of reviews vary across food types, cluster food types based on ratings and sentiments, infer review topics using machine learning models, and compare topic distributions among different food types. Our analyses reveal that some food types have similar ratings, sentiments, and topics distributions, while others have distinct patterns. We identify four clusters of food types based on ratings and sentiments and find that reviewers tend to focus on different topics when reviewing certain food types. These findings have important implications for understanding user behavior and cultural influence on digital media platforms and promoting cross-cultural understanding and appreciation.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#35821;&#35328;&#30340;&#34920;&#29616;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36328;&#35821;&#35328;&#21644;&#22810;&#35821;&#31181;&#30340;&#35821;&#38899;&#24773;&#32490;&#35782;&#21035;&#12290;&#32467;&#26524;&#26174;&#31034;&#38463;&#22982;&#21704;&#25289;&#35821;&#21644;&#33521;&#35821;&#22312;&#24773;&#32490;&#35782;&#21035;&#19978;&#34920;&#29616;&#30456;&#20284;&#12290;</title><link>http://arxiv.org/abs/2307.10814</link><description>&lt;p&gt;
&#35821;&#26009;&#24211;&#36328;&#35821;&#35328;&#22810;&#35821;&#31181;&#35821;&#38899;&#24773;&#32490;&#35782;&#21035;&#65306;&#38463;&#22982;&#21704;&#25289;&#35821;&#19982;&#20854;&#20182;&#35821;&#35328;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Cross-Corpus Multilingual Speech Emotion Recognition: Amharic vs. Other Languages. (arXiv:2307.10814v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10814
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#35821;&#35328;&#30340;&#34920;&#29616;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36328;&#35821;&#35328;&#21644;&#22810;&#35821;&#31181;&#30340;&#35821;&#38899;&#24773;&#32490;&#35782;&#21035;&#12290;&#32467;&#26524;&#26174;&#31034;&#38463;&#22982;&#21704;&#25289;&#35821;&#21644;&#33521;&#35821;&#22312;&#24773;&#32490;&#35782;&#21035;&#19978;&#34920;&#29616;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24120;&#35268;&#30340;&#35821;&#38899;&#24773;&#32490;&#35782;&#21035;&#65288;SER&#65289;&#20219;&#21153;&#20013;&#65292;&#38024;&#23545;&#32473;&#23450;&#35821;&#35328;&#30340;&#20998;&#31867;&#22120;&#26159;&#22312;&#35813;&#35821;&#35328;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#26576;&#20123;&#35821;&#35328;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#20351;&#29992;&#20854;&#20182;&#35821;&#35328;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#23581;&#35797;&#20102;&#36328;&#35821;&#35328;&#21644;&#22810;&#35821;&#31181;&#30340;SER&#65292;&#20351;&#29992;&#20102;&#38463;&#22982;&#21704;&#25289;&#35821;&#12289;&#33521;&#35821;&#12289;&#24503;&#35821;&#21644;&#20044;&#23572;&#37117;&#35821;&#12290;&#23545;&#20110;&#38463;&#22982;&#21704;&#25289;&#35821;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#25105;&#20204;&#33258;&#24049;&#25552;&#20379;&#30340;&#38463;&#22982;&#21704;&#25289;&#35821;&#24773;&#32490;&#35821;&#38899;&#25968;&#25454;&#38598;&#65288;ASED&#65289;&#12290;&#23545;&#20110;&#33521;&#35821;&#12289;&#24503;&#35821;&#21644;&#20044;&#23572;&#37117;&#35821;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#29616;&#26377;&#30340;RAVDESS&#12289;EMO-DB&#21644;URDU&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36981;&#24490;&#20102;&#20197;&#21069;&#30340;&#30740;&#31350;&#65292;&#23558;&#25152;&#26377;&#25968;&#25454;&#38598;&#30340;&#26631;&#31614;&#26144;&#23556;&#20026;&#20004;&#20010;&#31867;&#21035;&#65306;&#31215;&#26497;&#21644;&#28040;&#26497;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21487;&#20197;&#30452;&#25509;&#27604;&#36739;&#19981;&#21516;&#35821;&#35328;&#30340;&#24615;&#33021;&#65292;&#24182;&#23558;&#19981;&#21516;&#35821;&#35328;&#29992;&#20110;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;&#22312;&#31532;&#19968;&#23454;&#39564;&#20013;&#65292;&#20351;&#29992;&#19977;&#20010;&#20998;&#31867;&#22120;&#65288;AlexNet&#12289;VGGE&#21644;ResNet50&#65289;&#36827;&#34892;&#20102;&#21333;&#35821;&#31181;SER&#35797;&#39564;&#12290;&#23545;&#20110;ASED&#21644;RAVDESS&#65292;&#19977;&#20010;&#27169;&#22411;&#30340;&#24179;&#22343;&#32467;&#26524;&#38750;&#24120;&#30456;&#20284;&#65292;&#36825;&#34920;&#26126;&#38463;&#22982;&#21704;&#25289;&#35821;&#21644;&#33521;&#35821;&#22312;&#24773;&#32490;&#35782;&#21035;&#19978;&#30340;&#34920;&#29616;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a conventional Speech emotion recognition (SER) task, a classifier for a given language is trained on a pre-existing dataset for that same language. However, where training data for a language does not exist, data from other languages can be used instead. We experiment with cross-lingual and multilingual SER, working with Amharic, English, German and URDU. For Amharic, we use our own publicly-available Amharic Speech Emotion Dataset (ASED). For English, German and Urdu we use the existing RAVDESS, EMO-DB and URDU datasets. We followed previous research in mapping labels for all datasets to just two classes, positive and negative. Thus we can compare performance on different languages directly, and combine languages for training and testing. In Experiment 1, monolingual SER trials were carried out using three classifiers, AlexNet, VGGE (a proposed variant of VGG), and ResNet50. Results averaged for the three models were very similar for ASED and RAVDESS, suggesting that Amharic and E
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#19977;&#33410;&#27425;&#30340;&#23450;&#24615;&#30740;&#31350;&#65292;&#25506;&#31350;&#20102;&#20154;&#31867;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#20889;&#36807;&#31243;&#20013;&#30340;&#21512;&#20316;&#27169;&#24335;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#20010;&#19977;&#38454;&#27573;&#30340;&#20154;&#26426;&#20849;&#21019;&#36807;&#31243;&#65306;&#26500;&#24605;&#12289;&#21551;&#21457;&#21644;&#23454;&#26045;&#12290;&#22312;&#36825;&#20010;&#21512;&#20316;&#36807;&#31243;&#20013;&#65292;&#20154;&#31867;&#25198;&#28436;&#30528;&#20027;&#23548;&#35282;&#33394;&#12290;</title><link>http://arxiv.org/abs/2307.10811</link><description>&lt;p&gt;
"&#24863;&#35273;&#20687;&#26377;&#31532;&#20108;&#20010;&#24605;&#32500;": &#25506;&#31350;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#21019;&#24847;&#21487;&#20889;&#24615;&#39044;&#20889;&#30340;&#20154;&#26426;&#20849;&#21019;
&lt;/p&gt;
&lt;p&gt;
"It Felt Like Having a Second Mind": Investigating Human-AI Co-creativity in Prewriting with Large Language Models. (arXiv:2307.10811v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10811
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19977;&#33410;&#27425;&#30340;&#23450;&#24615;&#30740;&#31350;&#65292;&#25506;&#31350;&#20102;&#20154;&#31867;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#20889;&#36807;&#31243;&#20013;&#30340;&#21512;&#20316;&#27169;&#24335;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#20010;&#19977;&#38454;&#27573;&#30340;&#20154;&#26426;&#20849;&#21019;&#36807;&#31243;&#65306;&#26500;&#24605;&#12289;&#21551;&#21457;&#21644;&#23454;&#26045;&#12290;&#22312;&#36825;&#20010;&#21512;&#20316;&#36807;&#31243;&#20013;&#65292;&#20154;&#31867;&#25198;&#28436;&#30528;&#20027;&#23548;&#35282;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#20889;&#26159;&#22312;&#31532;&#19968;&#31295;&#20043;&#21069;&#21457;&#29616;&#21644;&#21457;&#23637;&#24605;&#24819;&#30340;&#36807;&#31243;&#65292;&#23427;&#38656;&#35201;&#21457;&#25955;&#24615;&#24605;&#32500;&#65292;&#36890;&#24120;&#28041;&#21450;&#21040;&#26080;&#32467;&#26500;&#30340;&#31574;&#30053;&#65292;&#22914;&#22270;&#34920;&#12289;&#27010;&#36848;&#21644;&#33258;&#30001;&#20889;&#20316;&#31561;&#12290;&#34429;&#28982;&#24050;&#32463;&#35777;&#26126;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#37117;&#26159;&#26377;&#29992;&#30340;&#65292;&#21253;&#25324;&#21019;&#24847;&#20889;&#20316;&#65292;&#20294;&#23545;&#29992;&#25143;&#22914;&#20309;&#19982;LLMs&#21512;&#20316;&#26469;&#25903;&#25345;&#39044;&#20889;&#30340;&#26041;&#24335;&#30693;&#20043;&#29978;&#23569;&#12290;&#22312;&#36825;&#31181;&#21019;&#36896;&#24615;&#36807;&#31243;&#20013;&#65292;LLMs&#30340;&#39318;&#36873;&#21512;&#20316;&#35282;&#33394;&#21644;&#20027;&#21160;&#24615;&#20063;&#19981;&#26126;&#30830;&#12290;&#20026;&#20102;&#30740;&#31350;&#20154;&#31867;&#19982;LLMs&#22312;&#39044;&#20889;&#36807;&#31243;&#20013;&#30340;&#21512;&#20316;&#27169;&#24335;&#21644;&#21160;&#21147;&#23398;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#19977;&#33410;&#27425;&#30340;&#23450;&#24615;&#30740;&#31350;&#65292;&#19982;15&#20301;&#21442;&#19982;&#32773;&#36827;&#34892;&#20102;&#20004;&#20010;&#21019;&#36896;&#24615;&#20219;&#21153;&#65306;&#20889;&#25925;&#20107;&#21644;&#20889;&#21475;&#21495;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21512;&#20316;&#30340;&#39044;&#20889;&#36807;&#31243;&#20013;&#65292;&#20284;&#20046;&#23384;&#22312;&#30528;&#19968;&#20010;&#19977;&#38454;&#27573;&#36845;&#20195;&#30340;&#20154;&#26426;&#20849;&#21019;&#36807;&#31243;&#65292;&#21253;&#25324;&#26500;&#24605;&#12289;&#21551;&#21457;&#21644;&#23454;&#26045;&#38454;&#27573;&#12290;&#36825;&#20010;&#21512;&#20316;&#36807;&#31243;&#20197;&#20154;&#31867;&#22312;&#20027;&#23548;&#35282;&#33394;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prewriting is the process of discovering and developing ideas before a first draft, which requires divergent thinking and often implies unstructured strategies such as diagramming, outlining, free-writing, etc. Although large language models (LLMs) have been demonstrated to be useful for a variety of tasks including creative writing, little is known about how users would collaborate with LLMs to support prewriting. The preferred collaborative role and initiative of LLMs during such a creativity process is also unclear. To investigate human-LLM collaboration patterns and dynamics during prewriting, we conducted a three-session qualitative study with 15 participants in two creative tasks: story writing and slogan writing. The findings indicated that during collaborative prewriting, there appears to be a three-stage iterative Human-AI Co-creativity process that includes Ideation, Illumination, and Implementation stages. This collaborative process champions the human in a dominant role, in
&lt;/p&gt;</description></item><item><title>Meta-Transformer&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#19968;&#20010;&#20923;&#32467;&#30340;&#32534;&#30721;&#22120;&#36827;&#34892;&#22810;&#27169;&#24577;&#24863;&#30693;&#65292;&#22312;&#27809;&#26377;&#25104;&#23545;&#22810;&#27169;&#24577;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#22788;&#29702;&#21508;&#31181;&#27169;&#24577;&#65292;&#24182;&#19988;&#33021;&#22815;&#25552;&#21462;&#36755;&#20837;&#25968;&#25454;&#30340;&#39640;&#32423;&#35821;&#20041;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2307.10802</link><description>&lt;p&gt;
Meta-Transformer: &#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Meta-Transformer: A Unified Framework for Multimodal Learning. (arXiv:2307.10802v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10802
&lt;/p&gt;
&lt;p&gt;
Meta-Transformer&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#19968;&#20010;&#20923;&#32467;&#30340;&#32534;&#30721;&#22120;&#36827;&#34892;&#22810;&#27169;&#24577;&#24863;&#30693;&#65292;&#22312;&#27809;&#26377;&#25104;&#23545;&#22810;&#27169;&#24577;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#22788;&#29702;&#21508;&#31181;&#27169;&#24577;&#65292;&#24182;&#19988;&#33021;&#22815;&#25552;&#21462;&#36755;&#20837;&#25968;&#25454;&#30340;&#39640;&#32423;&#35821;&#20041;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23398;&#20064;&#26088;&#22312;&#26500;&#24314;&#33021;&#22815;&#22788;&#29702;&#21644;&#20851;&#32852;&#22810;&#31181;&#27169;&#24577;&#30340;&#20449;&#24687;&#30340;&#27169;&#22411;&#12290;&#23613;&#31649;&#22312;&#36825;&#20010;&#39046;&#22495;&#24050;&#32463;&#26377;&#22810;&#24180;&#30340;&#21457;&#23637;&#65292;&#20294;&#30001;&#20110;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#22266;&#26377;&#24046;&#36317;&#65292;&#35774;&#35745;&#19968;&#20010;&#29992;&#20110;&#22788;&#29702;&#21508;&#31181;&#27169;&#24577;&#30340;&#32479;&#19968;&#32593;&#32476;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Meta-Transformer&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#19968;&#20010;&#20923;&#32467;&#30340;&#32534;&#30721;&#22120;&#22312;&#27809;&#26377;&#25104;&#23545;&#22810;&#27169;&#24577;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#22810;&#27169;&#24577;&#24863;&#30693;&#12290;&#22312;Meta-Transformer&#20013;&#65292;&#26469;&#33258;&#21508;&#31181;&#27169;&#24577;&#30340;&#21407;&#22987;&#36755;&#20837;&#25968;&#25454;&#34987;&#26144;&#23556;&#21040;&#19968;&#20010;&#20849;&#20139;&#30340;&#26631;&#35760;&#31354;&#38388;&#20013;&#65292;&#20351;&#24471;&#21518;&#32493;&#30340;&#32534;&#30721;&#22120;&#21487;&#20197;&#25552;&#21462;&#36755;&#20837;&#25968;&#25454;&#30340;&#39640;&#32423;&#35821;&#20041;&#29305;&#24449;&#12290;&#30001;&#19977;&#20010;&#20027;&#35201;&#32452;&#20214;&#32452;&#25104;&#65306;&#19968;&#20010;&#32479;&#19968;&#30340;&#25968;&#25454;&#26631;&#35760;&#22120;&#65292;&#19968;&#20010;&#27169;&#24577;&#20849;&#20139;&#30340;&#32534;&#30721;&#22120;&#21644;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#29305;&#23450;&#20219;&#21153;&#22836;&#65292;Meta-Transformer&#26159;&#31532;&#19968;&#20010;&#22312;12&#31181;&#27169;&#24577;&#19978;&#36827;&#34892;&#32479;&#19968;&#23398;&#20064;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal learning aims to build models that can process and relate information from multiple modalities. Despite years of development in this field, it still remains challenging to design a unified network for processing various modalities ($\textit{e.g.}$ natural language, 2D images, 3D point clouds, audio, video, time series, tabular data) due to the inherent gaps among them. In this work, we propose a framework, named Meta-Transformer, that leverages a $\textbf{frozen}$ encoder to perform multimodal perception without any paired multimodal training data. In Meta-Transformer, the raw input data from various modalities are mapped into a shared token space, allowing a subsequent encoder with frozen parameters to extract high-level semantic features of the input data. Composed of three main components: a unified data tokenizer, a modality-shared encoder, and task-specific heads for downstream tasks, Meta-Transformer is the first framework to perform unified learning across 12 modaliti
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#32423;&#34920;&#31034;&#34701;&#21512;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#21319;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#22312;&#32452;&#21512;&#27867;&#21270;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#22686;&#24378;&#22522;&#20110;&#20196;&#29260;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#32780;&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#20154;&#31867;&#37027;&#26679;&#36866;&#24403;&#22320;&#32452;&#21512;&#21644;&#20351;&#29992;&#24207;&#21015;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#20174;&#36817;&#26399;&#30340;&#20851;&#20110;&#35757;&#32451;&#26356;&#28145;Transformer&#30340;&#30740;&#31350;&#32467;&#26524;&#26469;&#30475;&#65292;&#32416;&#32544;&#38382;&#39064;&#20027;&#35201;&#26159;&#30001;&#20110;&#27531;&#24046;&#36830;&#25509;&#30340;&#8220;&#27973;&#23618;&#8221;&#21644;&#31616;&#21333;&#30340;&#21333;&#27493;&#25805;&#20316;&#23548;&#33268;&#19981;&#33021;&#26377;&#25928;&#22320;&#34701;&#21512;&#21069;&#38754;&#23618;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2307.10799</link><description>&lt;p&gt;
Layer-wise Representation Fusion for Compositional Generalization. (arXiv:2307.10799v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
Layer-wise Representation Fusion for Compositional Generalization. (arXiv:2307.10799v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10799
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#32423;&#34920;&#31034;&#34701;&#21512;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#21319;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#22312;&#32452;&#21512;&#27867;&#21270;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#22686;&#24378;&#22522;&#20110;&#20196;&#29260;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#32780;&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#20154;&#31867;&#37027;&#26679;&#36866;&#24403;&#22320;&#32452;&#21512;&#21644;&#20351;&#29992;&#24207;&#21015;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#20174;&#36817;&#26399;&#30340;&#20851;&#20110;&#35757;&#32451;&#26356;&#28145;Transformer&#30340;&#30740;&#31350;&#32467;&#26524;&#26469;&#30475;&#65292;&#32416;&#32544;&#38382;&#39064;&#20027;&#35201;&#26159;&#30001;&#20110;&#27531;&#24046;&#36830;&#25509;&#30340;&#8220;&#27973;&#23618;&#8221;&#21644;&#31616;&#21333;&#30340;&#21333;&#27493;&#25805;&#20316;&#23548;&#33268;&#19981;&#33021;&#26377;&#25928;&#22320;&#34701;&#21512;&#21069;&#38754;&#23618;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#22312;&#24191;&#27867;&#30340;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20854;&#26500;&#24314;&#30340;&#35299;&#20915;&#26041;&#26696;&#34987;&#35748;&#20026;&#22312;&#32452;&#21512;&#27867;&#21270;&#26041;&#38754;&#19981;&#22914;&#20154;&#31867;&#12290;&#36234;&#26469;&#36234;&#22810;&#30340;&#35777;&#25454;&#34920;&#26126;&#65292;&#38459;&#30861;&#32452;&#21512;&#27867;&#21270;&#30340;&#19968;&#20010;&#21407;&#22240;&#26159;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#26368;&#19978;&#23618;&#30340;&#34920;&#31034;&#34987;&#32416;&#32544;&#22312;&#19968;&#36215;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#24207;&#21015;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#34920;&#31034;&#34987;&#19981;&#36866;&#24403;&#22320;&#25197;&#26354;&#20102;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20197;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#20110;&#22686;&#24378;&#22522;&#20110;&#20196;&#29260;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#20197;&#32531;&#35299;&#34920;&#31034;&#32416;&#32544;&#38382;&#39064;&#65292;&#32780;&#19981;&#26159;&#20687;&#20154;&#31867;&#37027;&#26679;&#36866;&#24403;&#22320;&#32452;&#21512;&#21644;&#20351;&#29992;&#24207;&#21015;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#36817;&#26399;&#20851;&#20110;&#35757;&#32451;&#26356;&#28145;Transformer&#30340;&#30740;&#31350;&#30340;&#35282;&#24230;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#32416;&#32544;&#38382;&#39064;&#23384;&#22312;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#8220;&#27973;&#23618;&#8221;&#27531;&#24046;&#36830;&#25509;&#21644;&#20854;&#31616;&#21333;&#30340;&#21333;&#27493;&#25805;&#20316;&#23548;&#33268;&#26080;&#27861;&#26377;&#25928;&#22320;&#34701;&#21512;&#21069;&#38754;&#23618;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite successes across a broad range of applications, sequence-to-sequence models' construct of solutions are argued to be less compositional than human-like generalization. There is mounting evidence that one of the reasons hindering compositional generalization is representations of the encoder and decoder uppermost layer are entangled. In other words, the syntactic and semantic representations of sequences are twisted inappropriately. However, most previous studies mainly concentrate on enhancing token-level semantic information to alleviate the representations entanglement problem, rather than composing and using the syntactic and semantic representations of sequences appropriately as humans do. In addition, we explain why the entanglement problem exists from the perspective of recent studies about training deeper Transformer, mainly owing to the ``shallow'' residual connections and its simple, one-step operations, which fails to fuse previous layers' information effectively. Sta
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26497;&#31471;&#22810;&#26631;&#31614;&#25216;&#33021;&#25552;&#21462;&#35757;&#32451;&#65292;&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#25216;&#33021;&#25552;&#21462;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.10778</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26497;&#31471;&#22810;&#26631;&#31614;&#25216;&#33021;&#25552;&#21462;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Extreme Multi-Label Skill Extraction Training using Large Language Models. (arXiv:2307.10778v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10778
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26497;&#31471;&#22810;&#26631;&#31614;&#25216;&#33021;&#25552;&#21462;&#35757;&#32451;&#65292;&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#25216;&#33021;&#25552;&#21462;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#32844;&#20301;&#24191;&#21578;&#26159;&#25216;&#33021;&#38656;&#27714;&#20449;&#24687;&#30340;&#23453;&#36149;&#26469;&#28304;&#65292;&#23545;&#21171;&#21160;&#24066;&#22330;&#20998;&#26512;&#21644;&#30005;&#23376;&#25307;&#32856;&#36807;&#31243;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#30001;&#20110;&#36825;&#20123;&#24191;&#21578;&#36890;&#24120;&#20197;&#33258;&#30001;&#25991;&#26412;&#26684;&#24335;&#21576;&#29616;&#65292;&#22240;&#27492;&#38656;&#35201;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25216;&#26415;&#26469;&#33258;&#21160;&#22788;&#29702;&#23427;&#20204;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#26816;&#27979;&#25216;&#33021;&#65288;&#26126;&#30830;&#25552;&#21040;&#25110;&#38544;&#21547;&#25551;&#36848;&#65289;&#24182;&#23558;&#20854;&#38142;&#25509;&#21040;&#22823;&#22411;&#25216;&#33021;&#26412;&#20307;&#35770;&#30340;&#20219;&#21153;&#65292;&#36825;&#25104;&#20026;&#20102;&#26497;&#31471;&#22810;&#26631;&#31614;&#20998;&#31867;&#65288;XMLC&#65289;&#30340;&#19968;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#12290;&#32771;&#34385;&#21040;&#36825;&#20010;&#29305;&#23450;&#30340;XMLC&#20219;&#21153;&#27809;&#26377;&#22823;&#35268;&#27169;&#30340;&#26631;&#35760;&#65288;&#35757;&#32451;&#65289;&#25968;&#25454;&#38598;&#21487;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#36890;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#36153;&#29992;&#25928;&#30410;&#39640;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#19968;&#20010;&#20934;&#30830;&#30340;&#12289;&#23436;&#20840;&#21512;&#25104;&#30340;&#29992;&#20110;&#25216;&#33021;&#25552;&#21462;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#22312;&#20219;&#21153;&#20013;&#35777;&#26126;&#26377;&#25928;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#25216;&#33021;&#25552;&#21462;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;15%&#21040;25%&#20043;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online job ads serve as a valuable source of information for skill requirements, playing a crucial role in labor market analysis and e-recruitment processes. Since such ads are typically formatted in free text, natural language processing (NLP) technologies are required to automatically process them. We specifically focus on the task of detecting skills (mentioned literally, or implicitly described) and linking them to a large skill ontology, making it a challenging case of extreme multi-label classification (XMLC). Given that there is no sizable labeled (training) dataset are available for this specific XMLC task, we propose techniques to leverage general Large Language Models (LLMs). We describe a cost-effective approach to generate an accurate, fully synthetic labeled dataset for skill extraction, and present a contrastive learning strategy that proves effective in the task. Our results across three skill extraction benchmarks show a consistent increase of between 15 to 25 percentag
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#32039;&#20945;&#39640;&#25928;&#39044;&#35757;&#32451;&#27169;&#22411;Vesper&#65292;&#36890;&#36807;&#20248;&#21270;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#29983;&#25104;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#65292;&#32771;&#34385;&#24773;&#24863;&#29305;&#24449;&#24182;&#37319;&#29992;&#24773;&#24863;&#24341;&#23548;&#30340;&#25513;&#34109;&#31574;&#30053;&#26469;&#22686;&#24378;&#23545;&#24773;&#24863;&#20449;&#24687;&#30340;&#25935;&#24863;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.10757</link><description>&lt;p&gt;
Vesper&#65306;&#19968;&#31181;&#29992;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#32039;&#20945;&#39640;&#25928;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Vesper: A Compact and Effective Pretrained Model for Speech Emotion Recognition. (arXiv:2307.10757v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10757
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#32039;&#20945;&#39640;&#25928;&#39044;&#35757;&#32451;&#27169;&#22411;Vesper&#65292;&#36890;&#36807;&#20248;&#21270;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#29983;&#25104;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#65292;&#32771;&#34385;&#24773;&#24863;&#29305;&#24449;&#24182;&#37319;&#29992;&#24773;&#24863;&#24341;&#23548;&#30340;&#25513;&#34109;&#31574;&#30053;&#26469;&#22686;&#24378;&#23545;&#24773;&#24863;&#20449;&#24687;&#30340;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#36890;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;PTMs&#65289;&#36866;&#24212;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#20219;&#21153;&#30340;&#33539;&#24335;&#12290;&#23613;&#31649;PTMs&#20026;&#20154;&#24037;&#26234;&#33021;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#65292;&#20294;&#23427;&#20204;&#26159;&#20026;&#36890;&#29992;&#20219;&#21153;&#26500;&#24314;&#30340;&#65292;&#22240;&#27492;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#25928;&#26524;&#20173;&#26377;&#25552;&#21319;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;PTMs&#20307;&#31215;&#36739;&#22823;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20351;&#29992;&#21487;&#33021;&#38754;&#20020;&#25361;&#25112;&#12290;&#38024;&#23545;&#20197;&#19978;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#21478;&#19968;&#20010;&#30740;&#31350;&#26041;&#21521;&#65292;&#21363;&#20248;&#21270;&#22823;&#35268;&#27169;PTMs&#20197;&#29983;&#25104;&#32039;&#20945;&#19988;&#39640;&#25928;&#30340;&#20219;&#21153;&#29305;&#23450;PTMs&#12290;&#26412;&#25991;&#32858;&#28966;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#24773;&#24863;&#29305;&#23450;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;Vesper&#12290;Vesper&#22312;&#22522;&#20110;WavLM&#30340;&#35821;&#38899;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#32771;&#34385;&#20102;&#24773;&#24863;&#29305;&#24449;&#12290;&#20026;&#20102;&#22686;&#24378;&#23545;&#24773;&#24863;&#20449;&#24687;&#30340;&#25935;&#24863;&#24615;&#65292;Vesper&#37319;&#29992;&#24773;&#24863;&#24341;&#23548;&#30340;&#25513;&#34109;&#31574;&#30053;&#26469;&#35782;&#21035;&#38656;&#35201;&#25513;&#34109;&#30340;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a paradigm that adapts general large-scale pretrained models (PTMs) to speech emotion recognition task. Although PTMs shed new light on artificial general intelligence, they are constructed with general tasks in mind, and thus, their efficacy for specific tasks can be further improved. Additionally, employing PTMs in practical applications can be challenging due to their considerable size. Above limitations spawn another research direction, namely, optimizing large-scale PTMs for specific tasks to generate task-specific PTMs that are both compact and effective. In this paper, we focus on the speech emotion recognition task and propose an improved emotion-specific pretrained encoder called Vesper. Vesper is pretrained on a speech dataset based on WavLM and takes into account emotional characteristics. To enhance sensitivity to emotional information, Vesper employs an emotion-guided masking strategy to identify the regions that need masking. Subsequently, Vesper emplo
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#23545;&#30693;&#35782;&#24037;&#20316;&#30340;&#21019;&#36896;&#21147;&#26377;&#30528;&#28145;&#36828;&#30340;&#24433;&#21709;&#65292;&#20294;&#26159;&#23545;&#29983;&#25104;&#27169;&#22411;&#30340;&#25209;&#35780;&#32773;&#35748;&#20026;&#20854;&#36755;&#20986;&#21482;&#26159;&#38543;&#26426;&#30340;&#25220;&#34989;&#21644;&#28151;&#25645;&#12290;&#28982;&#32780;&#65292;&#21019;&#36896;&#21147;&#21644;&#21407;&#21019;&#24615;&#30340;&#23450;&#20041;&#26159;&#22797;&#26434;&#30340;&#65292;&#21487;&#33021;&#26159;&#19968;&#20010;&#36807;&#31243;&#12289;&#19968;&#20010;&#20316;&#32773;&#25110;&#19968;&#20010;&#35266;&#30475;&#32773;&#30340;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.10751</link><description>&lt;p&gt;
&#25506;&#35752;&#20154;&#24037;&#26234;&#33021;&#23545;&#30693;&#35782;&#24037;&#20316;&#21019;&#36896;&#21147;&#30340;&#24433;&#21709;&#65306;&#36229;&#36234;&#26426;&#26800;&#21270;&#25220;&#34989;&#21644;&#38543;&#26426;&#40550;&#40521;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Exploring Perspectives on the Impact of Artificial Intelligence on the Creativity of Knowledge Work: Beyond Mechanised Plagiarism and Stochastic Parrots. (arXiv:2307.10751v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10751
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#23545;&#30693;&#35782;&#24037;&#20316;&#30340;&#21019;&#36896;&#21147;&#26377;&#30528;&#28145;&#36828;&#30340;&#24433;&#21709;&#65292;&#20294;&#26159;&#23545;&#29983;&#25104;&#27169;&#22411;&#30340;&#25209;&#35780;&#32773;&#35748;&#20026;&#20854;&#36755;&#20986;&#21482;&#26159;&#38543;&#26426;&#30340;&#25220;&#34989;&#21644;&#28151;&#25645;&#12290;&#28982;&#32780;&#65292;&#21019;&#36896;&#21147;&#21644;&#21407;&#21019;&#24615;&#30340;&#23450;&#20041;&#26159;&#22797;&#26434;&#30340;&#65292;&#21487;&#33021;&#26159;&#19968;&#20010;&#36807;&#31243;&#12289;&#19968;&#20010;&#20316;&#32773;&#25110;&#19968;&#20010;&#35266;&#30475;&#32773;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#65292;&#23588;&#20854;&#26159;&#29983;&#25104;&#27169;&#22411;&#65292;&#26159;&#30693;&#35782;&#24037;&#20316;&#30340;&#21464;&#38761;&#24615;&#24037;&#20855;&#12290;&#23427;&#20204;&#38382;&#39064;&#21270;&#20102;&#21019;&#36896;&#21147;&#12289;&#21407;&#21019;&#24615;&#12289;&#25220;&#34989;&#12289;&#24402;&#23646;&#26435;&#21644;&#29256;&#26435;&#25152;&#26377;&#26435;&#30340;&#27010;&#24565;&#12290;&#23545;&#29983;&#25104;&#27169;&#22411;&#30340;&#25209;&#35780;&#32773;&#24378;&#35843;&#20854;&#20381;&#36182;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#23558;&#36825;&#20123;&#27169;&#22411;&#30340;&#36755;&#20986;&#35270;&#20026;&#38543;&#26426;&#30340;&#25220;&#34989;&#12289;&#28151;&#25645;&#25110;&#32773;&#25340;&#36148;&#28304;&#25968;&#25454;&#12290;&#22522;&#20110;&#36825;&#20123;&#29702;&#30001;&#65292;&#35768;&#22810;&#20154;&#20027;&#24352;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#37096;&#32626;&#12289;&#20351;&#29992;&#21644;&#36755;&#20986;&#30340;&#24402;&#23646;&#21152;&#24378;&#30417;&#31649;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#38382;&#39064;&#24182;&#19981;&#26159;&#20154;&#24037;&#26234;&#33021;&#29420;&#26377;&#30340;&#65292;&#20063;&#19981;&#26159;&#26032;&#38382;&#39064;&#12290;&#22312;&#36825;&#31687;&#31435;&#22330;&#35770;&#25991;&#20013;&#65292;&#25105;&#20351;&#29992;&#25991;&#23398;&#25209;&#35780;&#12289;&#33402;&#26415;&#21490;&#21644;&#29256;&#26435;&#27861;&#30340;&#20363;&#23376;&#65292;&#23637;&#31034;&#20102;&#21019;&#36896;&#21147;&#21644;&#21407;&#21019;&#24615;&#22914;&#20309;&#25269;&#25239;&#34987;&#23450;&#20041;&#20026;&#19968;&#20010;&#23545;&#35937;&#30340;&#21487;&#27880;&#37322;&#25110;&#20449;&#24687;&#35770;&#23646;&#24615;&#65292;&#32780;&#21487;&#35265;&#20026;&#19968;&#20010;&#36807;&#31243;&#12289;&#19968;&#20010;&#20316;&#32773;&#25110;&#19968;&#20010;&#35266;&#30475;&#32773;&#30340;&#23646;&#24615;&#12290;&#19968;&#20123;&#26367;&#20195;&#35266;&#28857;&#35748;&#20026;&#21019;&#36896;&#24615;&#24037;&#20316;&#23454;&#36136;&#19978;&#26159;&#19968;&#31181;&#38656;&#27714;&#19968;&#23450;&#31243;&#24230;&#30340;&#27169;&#20223;&#12289;&#39072;&#20498;&#21644;&#37325;&#26032;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI), and in particular generative models, are transformative tools for knowledge work. They problematise notions of creativity, originality, plagiarism, the attribution of credit, and copyright ownership. Critics of generative models emphasise the reliance on large amounts of training data, and view the output of these models as no more than randomised plagiarism, remix, or collage of the source data. On these grounds, many have argued for stronger regulations on the deployment, use, and attribution of the output of these models. However, these issues are not new or unique to artificial intelligence. In this position paper, using examples from literary criticism, the history of art, and copyright law, I show how creativity and originality resist definition as a notatable or information-theoretic property of an object, and instead can be seen as the property of a process, an author, or a viewer. Further alternative views hold that all creative work is essentiall
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35770;&#25991;&#25968;&#37327;&#24613;&#21095;&#22686;&#21152;&#65292;&#30740;&#31350;&#37325;&#28857;&#36880;&#28176;&#36716;&#21521;&#31038;&#20250;&#24433;&#21709;&#12290;&#19982;LLM&#30456;&#20851;&#30340;&#35770;&#25991;&#21576;&#29616;&#25345;&#32493;&#22686;&#38271;&#30340;&#36235;&#21183;&#65292;&#26032;&#21457;&#34920;&#20851;&#20110;LLM&#30340;&#20316;&#32773;&#26356;&#27880;&#37325;&#24212;&#29992;&#21644;&#31038;&#20250;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.10700</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22609;&#36896;&#24182;&#21463;&#21040;&#31038;&#20250;&#30340;&#24433;&#21709;&#65306;arXiv&#20986;&#29256;&#27169;&#24335;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Large language models shape and are shaped by society: A survey of arXiv publication patterns. (arXiv:2307.10700v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10700
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35770;&#25991;&#25968;&#37327;&#24613;&#21095;&#22686;&#21152;&#65292;&#30740;&#31350;&#37325;&#28857;&#36880;&#28176;&#36716;&#21521;&#31038;&#20250;&#24433;&#21709;&#12290;&#19982;LLM&#30456;&#20851;&#30340;&#35770;&#25991;&#21576;&#29616;&#25345;&#32493;&#22686;&#38271;&#30340;&#36235;&#21183;&#65292;&#26032;&#21457;&#34920;&#20851;&#20110;LLM&#30340;&#20316;&#32773;&#26356;&#27880;&#37325;&#24212;&#29992;&#21644;&#31038;&#20250;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35770;&#25991;&#25968;&#37327;&#36817;&#24180;&#26469;&#21576;&#24613;&#21095;&#22686;&#21152;&#65292;&#36825;&#31181;&#21464;&#21270;&#23545;&#31185;&#23398;&#39046;&#22495;&#20135;&#29983;&#20102;&#25103;&#21095;&#24615;&#30340;&#24433;&#21709;&#65292;&#20294;&#30446;&#21069;&#36824;&#27809;&#26377;&#36827;&#34892;&#35814;&#32454;&#30340;&#25991;&#29486;&#35745;&#37327;&#20998;&#26512;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;CS&#21644;Stat arXiv&#19978;&#21457;&#24067;&#30340;388K&#31687;&#35770;&#25991;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;2023&#24180;&#19982;2018-2022&#24180;&#20043;&#38388;&#21457;&#34920;&#27169;&#24335;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;LLM&#35770;&#25991;&#30340;&#27604;&#20363;&#22686;&#21152;&#24773;&#20917;&#65292;&#24471;&#21040;&#20102;&#26368;&#22810;&#20851;&#27880;&#30340;&#19982;LLM&#30456;&#20851;&#30340;&#20027;&#39064;&#65292;&#25776;&#20889;LLM&#35770;&#25991;&#30340;&#20316;&#32773;&#65292;&#20316;&#32773;&#30340;&#30740;&#31350;&#20027;&#39064;&#19982;&#32972;&#26223;&#30340;&#30456;&#20851;&#24615;&#65292;&#21306;&#20998;&#39640;&#34987;&#24341;&#29992;LLM&#35770;&#25991;&#30340;&#22240;&#32032;&#65292;&#20197;&#21450;&#22269;&#38469;&#21512;&#20316;&#30340;&#27169;&#24335;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LLM&#30740;&#31350;&#36234;&#26469;&#36234;&#20851;&#27880;&#31038;&#20250;&#24433;&#21709;&#65306;&#22312;&#35745;&#31639;&#26426;&#19982;&#31038;&#20250;&#23376;arXiv&#19978;&#65292;&#19982;LLM&#30456;&#20851;&#30340;&#35770;&#25991;&#27604;&#20363;&#22686;&#21152;&#20102;18&#20493;&#65292;&#26032;&#21457;&#34920;&#20851;&#20110;LLM&#30340;&#20316;&#32773;&#26356;&#20542;&#21521;&#20110;&#20851;&#27880;&#24212;&#29992;&#21644;&#31038;&#20250;&#24433;&#21709;&#12290;LLM&#30740;&#31350;&#20063;&#21463;&#21040;&#31038;&#20250;&#21160;&#24577;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been a steep recent increase in the number of large language model (LLM) papers, producing a dramatic shift in the scientific landscape which remains largely undocumented through bibliometric analysis. Here, we analyze 388K papers posted on the CS and Stat arXivs, focusing on changes in publication patterns in 2023 vs. 2018-2022. We analyze how the proportion of LLM papers is increasing; the LLM-related topics receiving the most attention; the authors writing LLM papers; how authors' research topics correlate with their backgrounds; the factors distinguishing highly cited LLM papers; and the patterns of international collaboration. We show that LLM research increasingly focuses on societal impacts: there has been an 18x increase in the proportion of LLM-related papers on the Computers and Society sub-arXiv, and authors newly publishing on LLMs are more likely to focus on applications and societal impacts than more experienced authors. LLM research is also shaped by social dyn
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#25463;&#20811;&#26032;&#38395;&#25991;&#26412;&#20998;&#31867;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#32447;&#27169;&#22411;&#12290;&#36890;&#36807;&#24341;&#20837;CZEch~NEws~Classification~dataset&#65288;CZE-NEC&#65289;&#65292;&#36825;&#20010;&#36328;&#36234;20&#22810;&#24180;&#30340;&#26368;&#22823;&#25463;&#20811;&#26032;&#38395;&#20998;&#31867;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#26356;&#20005;&#26684;&#22320;&#35780;&#20272;&#25463;&#20811;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22522;&#20110;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#22522;&#32447;&#34920;&#29616;&#20248;&#20110;&#20154;&#31867;&#65292;&#24182;&#19988;&#35821;&#35328;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#20998;&#26512;&#20063;&#36229;&#36807;&#20102;&#21830;&#19994;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.10666</link><description>&lt;p&gt;
&#12298;&#25463;&#20811;&#26032;&#38395;&#25991;&#26412;&#20998;&#31867;&#30340;&#25968;&#25454;&#38598;&#21644;&#24378;&#22522;&#32447;&#12299;
&lt;/p&gt;
&lt;p&gt;
A Dataset and Strong Baselines for Classification of Czech News Texts. (arXiv:2307.10666v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10666
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#25463;&#20811;&#26032;&#38395;&#25991;&#26412;&#20998;&#31867;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#32447;&#27169;&#22411;&#12290;&#36890;&#36807;&#24341;&#20837;CZEch~NEws~Classification~dataset&#65288;CZE-NEC&#65289;&#65292;&#36825;&#20010;&#36328;&#36234;20&#22810;&#24180;&#30340;&#26368;&#22823;&#25463;&#20811;&#26032;&#38395;&#20998;&#31867;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#26356;&#20005;&#26684;&#22320;&#35780;&#20272;&#25463;&#20811;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22522;&#20110;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#22522;&#32447;&#34920;&#29616;&#20248;&#20110;&#20154;&#31867;&#65292;&#24182;&#19988;&#35821;&#35328;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#20998;&#26512;&#20063;&#36229;&#36807;&#20102;&#21830;&#19994;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25463;&#20811;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#36890;&#24120;&#22312;&#32431;&#35821;&#35328;&#20219;&#21153;&#65288;&#35789;&#24615;&#26631;&#27880;&#12289;&#21477;&#27861;&#20998;&#26512;&#12289;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65289;&#21644;&#30456;&#23545;&#31616;&#21333;&#30340;&#20998;&#31867;&#20219;&#21153;&#65288;&#24773;&#24863;&#20998;&#31867;&#12289;&#26469;&#33258;&#21333;&#19968;&#26032;&#38395;&#28304;&#30340;&#25991;&#31456;&#20998;&#31867;&#65289;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#20316;&#20026;&#26367;&#20195;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CZEch~NEws~Classification~dataset&#65288;CZE-NEC&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26368;&#22823;&#30340;&#25463;&#20811;&#20998;&#31867;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#26469;&#33258;&#21508;&#31181;&#26469;&#28304;&#12289;&#36328;&#36234;&#20108;&#21313;&#22810;&#24180;&#30340;&#26032;&#38395;&#25991;&#31456;&#65292;&#21487;&#20197;&#26356;&#20005;&#26684;&#22320;&#35780;&#20272;&#36825;&#31181;&#27169;&#22411;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#22235;&#20010;&#20998;&#31867;&#20219;&#21153;&#65306;&#26032;&#38395;&#28304;&#12289;&#26032;&#38395;&#31867;&#21035;&#12289;&#25512;&#26029;&#20316;&#32773;&#30340;&#24615;&#21035;&#21644;&#26143;&#26399;&#20960;&#12290;&#20026;&#20102;&#39564;&#35777;&#20219;&#21153;&#30340;&#38590;&#24230;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20154;&#24037;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#20154;&#31867;&#30340;&#34920;&#29616;&#33853;&#21518;&#20110;&#22522;&#20110;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#27169;&#22411;&#26500;&#24314;&#30340;&#24378;&#22823;&#26426;&#22120;&#23398;&#20064;&#22522;&#32447;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#35821;&#35328;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#20998;&#26512;&#20248;&#20110;&#36873;&#23450;&#30340;&#21830;&#19994;&#21487;&#29992;&#30340;&#22823;&#35268;&#27169;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained models for Czech Natural Language Processing are often evaluated on purely linguistic tasks (POS tagging, parsing, NER) and relatively simple classification tasks such as sentiment classification or article classification from a single news source. As an alternative, we present CZEch~NEws~Classification~dataset (CZE-NEC), one of the largest Czech classification datasets, composed of news articles from various sources spanning over twenty years, which allows a more rigorous evaluation of such models. We define four classification tasks: news source, news category, inferred author's gender, and day of the week. To verify the task difficulty, we conducted a human evaluation, which revealed that human performance lags behind strong machine-learning baselines built upon pre-trained transformer models. Furthermore, we show that language-specific pre-trained encoder analysis outperforms selected commercially available large-scale generative language models.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#31995;&#32479;&#20998;&#31867;&#21644;&#20998;&#26512;&#20102;ACL Anthology&#20013;&#30340;&#30740;&#31350;&#35770;&#25991;&#65292;&#25552;&#20379;&#20102;&#23545;&#30740;&#31350;&#39046;&#22495;&#30340;&#32467;&#26500;&#21270;&#27010;&#36848;&#21644;NLP&#39046;&#22495;&#30340;&#20998;&#31867;&#23398;&#12290;&#26412;&#30740;&#31350;&#24635;&#32467;&#20102;&#26368;&#26032;&#30340;NLP&#21457;&#23637;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#24037;&#20316;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2307.10652</link><description>&lt;p&gt;
&#25506;&#35752;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#39046;&#22495;&#30340;&#21457;&#23637;&#36235;&#21183;
&lt;/p&gt;
&lt;p&gt;
Exploring the Landscape of Natural Language Processing Research. (arXiv:2307.10652v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10652
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#31995;&#32479;&#20998;&#31867;&#21644;&#20998;&#26512;&#20102;ACL Anthology&#20013;&#30340;&#30740;&#31350;&#35770;&#25991;&#65292;&#25552;&#20379;&#20102;&#23545;&#30740;&#31350;&#39046;&#22495;&#30340;&#32467;&#26500;&#21270;&#27010;&#36848;&#21644;NLP&#39046;&#22495;&#30340;&#20998;&#31867;&#23398;&#12290;&#26412;&#30740;&#31350;&#24635;&#32467;&#20102;&#26368;&#26032;&#30340;NLP&#21457;&#23637;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#24037;&#20316;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20316;&#20026;&#29702;&#35299;&#12289;&#29983;&#25104;&#21644;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#30340;&#19968;&#31181;&#39640;&#25928;&#26041;&#27861;&#65292;&#22312;&#36817;&#24180;&#26469;&#24471;&#21040;&#20102;&#24555;&#36895;&#20256;&#25773;&#21644;&#24191;&#27867;&#24212;&#29992;&#12290;&#37492;&#20110;&#35813;&#39046;&#22495;&#30740;&#31350;&#24037;&#20316;&#30340;&#19981;&#26029;&#22686;&#21152;&#65292;&#30740;&#31350;&#30028;&#24050;&#23545;&#25968;&#20010;&#19982;NLP&#30456;&#20851;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;&#28982;&#32780;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#20173;&#32570;&#23569;&#19968;&#39033;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#23545;&#24050;&#24314;&#31435;&#30340;&#20027;&#39064;&#36827;&#34892;&#20998;&#31867;&#12289;&#35782;&#21035;&#36235;&#21183;&#24182;&#27010;&#25324;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#23545;ACL Anthology&#20013;&#21253;&#21547;&#30340;&#30740;&#31350;&#35770;&#25991;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#31867;&#21644;&#20998;&#26512;&#12290;&#32467;&#26524;&#21576;&#29616;&#20102;&#30740;&#31350;&#39046;&#22495;&#30340;&#32467;&#26500;&#21270;&#27010;&#36848;&#65292;&#20026;NLP&#39046;&#22495;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#20998;&#31867;&#23398;&#65292;&#20998;&#26512;&#20102;NLP&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#24635;&#32467;&#20102;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#24182;&#31361;&#20986;&#20102;&#26410;&#26469;&#24037;&#20316;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
As an efficient approach to understand, generate, and process natural language texts, research in natural language processing (NLP) has exhibited a rapid spread and wide adoption in recent years. Given the increasing amount of research work in this area, several NLP-related approaches have been surveyed in the research community. However, a comprehensive study that categorizes established topics, identifies trends, and outlines areas for future research remains absent to this day. Contributing to closing this gap, we have systematically classified and analyzed research papers included in the ACL Anthology. As a result, we present a structured overview of the research landscape, provide a taxonomy of fields-of-study in NLP, analyze recent developments in NLP, summarize our findings, and highlight directions for future work.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;SciBench&#30340;&#22522;&#20934;&#22871;&#20214;&#65292;&#26088;&#22312;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#23398;&#27700;&#24179;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#36827;&#34892;&#35780;&#20272;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#20379;&#22797;&#26434;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#26041;&#38754;&#36824;&#26377;&#19981;&#36275;&#20043;&#22788;&#12290;</title><link>http://arxiv.org/abs/2307.10635</link><description>&lt;p&gt;
SciBench: &#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#22823;&#23398;&#27700;&#24179;&#30340;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models. (arXiv:2307.10635v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10635
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;SciBench&#30340;&#22522;&#20934;&#22871;&#20214;&#65292;&#26088;&#22312;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#23398;&#27700;&#24179;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#36827;&#34892;&#35780;&#20272;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#20379;&#22797;&#26434;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#26041;&#38754;&#36824;&#26377;&#19981;&#36275;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#36827;&#23637;&#22312;&#35768;&#22810;&#25968;&#23398;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#27493;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20934;&#22823;&#22810;&#21482;&#21253;&#21547;&#21021;&#39640;&#20013;&#31185;&#30446;&#30340;&#38382;&#39064;&#65292;&#20165;&#21253;&#21547;&#22810;&#39033;&#36873;&#25321;&#39064;&#65292;&#24182;&#19988;&#20165;&#38480;&#20110;&#22522;&#26412;&#31639;&#26415;&#36816;&#31639;&#33539;&#22260;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#24191;&#27867;&#30340;&#22522;&#20934;&#22871;&#20214;SciBench&#65292;&#26088;&#22312;&#31995;&#32479;&#22320;&#26816;&#27979;&#22797;&#26434;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#25152;&#38656;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;SciBench&#21253;&#21547;&#20004;&#20010;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#65306;&#19968;&#20010;&#24320;&#25918;&#38598;&#65292;&#21253;&#25324;&#20174;&#25968;&#23398;&#12289;&#21270;&#23398;&#21644;&#29289;&#29702;&#25945;&#31185;&#20070;&#20013;&#25688;&#24405;&#30340;&#22823;&#23398;&#27700;&#24179;&#30340;&#31185;&#23398;&#38382;&#39064;&#65292;&#20197;&#21450;&#19968;&#20010;&#23553;&#38381;&#38598;&#65292;&#21253;&#21547;&#26469;&#33258;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#25968;&#23398;&#26412;&#31185;&#32771;&#35797;&#30340;&#38382;&#39064;&#12290;&#22522;&#20110;&#36825;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23545;&#20004;&#20010;&#20195;&#34920;&#24615;&#30340;LLM&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#22522;&#20934;&#30740;&#31350;&#65292;&#24182;&#37319;&#29992;&#19981;&#21516;&#30340;&#25552;&#31034;&#31574;&#30053;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#21069;&#30340;LLMs&#22312;&#25552;&#20379;&#22797;&#26434;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#26041;&#38754;&#36824;&#23384;&#22312;&#19981;&#36275;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in large language models (LLMs) have demonstrated notable progress on many mathematical benchmarks. However, most of these benchmarks only feature problems grounded in junior and senior high school subjects, contain only multiple-choice questions, and are confined to a limited scope of elementary arithmetic operations. To address these issues, this paper introduces an expansive benchmark suite SciBench that aims to systematically examine the reasoning capabilities required for complex scientific problem solving. SciBench contains two carefully curated datasets: an open set featuring a range of collegiate-level scientific problems drawn from mathematics, chemistry, and physics textbooks, and a closed set comprising problems from undergraduate-level exams in computer science and mathematics. Based on the two datasets, we conduct an in-depth benchmark study of two representative LLMs with various prompting strategies. The results reveal that current LLMs fall short of deli
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#20154;&#31867;&#22522;&#22240;&#30340;&#26680;&#33527;&#37240;&#24207;&#21015;&#65292;&#22635;&#34917;&#20102;DNA&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#30740;&#31350;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2307.10634</link><description>&lt;p&gt;
&#20154;&#31867;&#22522;&#22240;&#26680;&#33527;&#37240;&#24207;&#21015;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Generative Language Models on Nucleotide Sequences of Human Genes. (arXiv:2307.10634v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#20154;&#31867;&#22522;&#22240;&#30340;&#26680;&#33527;&#37240;&#24207;&#21015;&#65292;&#22635;&#34917;&#20102;DNA&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#30740;&#31350;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;DNA&#30456;&#20851;&#30340;&#29983;&#29289;&#20449;&#24687;&#23398;&#39046;&#22495;&#65292;&#29983;&#25104;&#27169;&#22411;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#31867;&#20284;&#20110;GPT-3&#30340;&#33258;&#22238;&#24402;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#20154;&#31867;&#22522;&#22240;&#30340;&#26680;&#33527;&#37240;&#24207;&#21015;&#12290;&#32771;&#34385;&#21040;&#22788;&#29702;&#25972;&#20010;DNA&#24207;&#21015;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#65292;&#25105;&#20204;&#20915;&#23450;&#22312;&#26356;&#23567;&#30340;&#23610;&#24230;&#19978;&#36827;&#34892;&#30740;&#31350;&#65292;&#37325;&#28857;&#20851;&#27880;&#20154;&#31867;&#22522;&#22240;&#30340;&#26680;&#33527;&#37240;&#24207;&#21015;&#65292;&#32780;&#19981;&#26159;&#25972;&#20010;DNA&#12290;&#36825;&#20010;&#20915;&#31574;&#24182;&#19981;&#25913;&#21464;&#38382;&#39064;&#30340;&#32467;&#26500;&#65292;&#22240;&#20026;DNA&#21644;&#22522;&#22240;&#37117;&#21487;&#20197;&#30475;&#20316;&#30001;&#22235;&#31181;&#19981;&#21516;&#30340;&#26680;&#33527;&#37240;&#32452;&#25104;&#30340;&#19968;&#32500;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models, primarily transformer-based ones, obtained colossal success in NLP. To be more precise, studies like BERT in NLU and works such as GPT-3 for NLG are very crucial. DNA sequences are very close to natural language in terms of structure, so if the DNA-related bioinformatics domain is concerned, discriminative models, like DNABert, exist. Yet, the generative side of the coin is mainly unexplored to the best of our knowledge. Consequently, we focused on developing an autoregressive generative language model like GPT-3 for DNA sequences. Because working with whole DNA sequences is challenging without substantial computational resources, we decided to carry out our study on a smaller scale, focusing on nucleotide sequences of human genes, unique parts in DNA with specific functionalities, instead of the whole DNA. This decision did not change the problem structure a lot due to the fact that both DNA and genes can be seen as 1D sequences consisting of four different nucleotide
&lt;/p&gt;</description></item><item><title>&#22810;&#26041;&#27861;&#33258;&#35757;&#32451;&#21487;&#20197;&#36890;&#36807;&#22312;&#19981;&#21516;&#26041;&#27861;&#20043;&#38388;&#35757;&#32451;&#21644;&#29983;&#25104;&#25968;&#25454;&#26469;&#25913;&#21892;&#20195;&#30721;&#29983;&#25104;&#30340;&#24615;&#33021;&#65292;&#20351;&#27169;&#22411;&#26356;&#26131;&#20110;&#20351;&#29992;&#65292;&#24182;&#25552;&#39640;&#30456;&#20851;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.10633</link><description>&lt;p&gt;
&#22810;&#26041;&#27861;&#33258;&#35757;&#32451;&#65306;&#36890;&#36807;&#25991;&#26412;&#25913;&#36827;&#20195;&#30721;&#29983;&#25104;&#65292;&#21453;&#20043;&#20134;&#28982;
&lt;/p&gt;
&lt;p&gt;
Multi-Method Self-Training: Improving Code Generation With Text, And Vice Versa. (arXiv:2307.10633v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10633
&lt;/p&gt;
&lt;p&gt;
&#22810;&#26041;&#27861;&#33258;&#35757;&#32451;&#21487;&#20197;&#36890;&#36807;&#22312;&#19981;&#21516;&#26041;&#27861;&#20043;&#38388;&#35757;&#32451;&#21644;&#29983;&#25104;&#25968;&#25454;&#26469;&#25913;&#21892;&#20195;&#30721;&#29983;&#25104;&#30340;&#24615;&#33021;&#65292;&#20351;&#27169;&#22411;&#26356;&#26131;&#20110;&#20351;&#29992;&#65292;&#24182;&#25552;&#39640;&#30456;&#20851;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#35768;&#22810;&#35299;&#20915;&#21516;&#19968;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36825;&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#20248;&#28857;&#65288;&#19981;&#21516;&#30340;&#26041;&#27861;&#21487;&#33021;&#23545;&#19981;&#21516;&#30340;&#38382;&#39064;&#26377;&#25928;&#65289;&#65292;&#20197;&#21450;&#32570;&#28857;&#65288;&#29992;&#25143;&#21487;&#33021;&#38590;&#20197;&#30693;&#36947;&#20351;&#29992;&#21738;&#31181;&#26041;&#27861;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22810;&#26041;&#27861;&#33258;&#35757;&#32451;&#65288;MMST&#65289;&#26041;&#27861;&#65292;&#20854;&#20013;&#19968;&#31181;&#26041;&#27861;&#26159;&#22312;&#21478;&#19968;&#31181;&#26041;&#27861;&#30340;&#31579;&#36873;&#36755;&#20986;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#22686;&#24378;&#27599;&#31181;&#26041;&#27861;&#30340;&#20248;&#28857;&#24182;&#25913;&#21892;&#23427;&#20204;&#30340;&#32570;&#28857;&#12290;&#20351;&#29992;176B&#21442;&#25968;&#30340;&#35821;&#35328;&#21644;&#20195;&#30721;&#35757;&#32451;&#27169;&#22411;&#65292;&#25105;&#20204;&#35777;&#26126;MMST&#21487;&#20197;1&#65289;&#25913;&#21892;&#24615;&#33021;&#36739;&#24046;&#30340;&#26041;&#27861;&#65288;&#39640;&#36798;30%&#65289;&#65292;&#20351;&#27169;&#22411;&#26356;&#26131;&#20110;&#20351;&#29992;&#65292;2&#65289;&#25913;&#21892;&#24615;&#33021;&#36739;&#22909;&#30340;&#26041;&#27861;&#65288;&#39640;&#36798;32.2%&#65289;&#65292;&#20351;&#27169;&#22411;&#24615;&#33021;&#26356;&#20248;&#31168;&#65292;&#20197;&#21450;3&#65289;&#36890;&#36807;&#25913;&#21892;&#27169;&#22411;&#29983;&#25104;&#35299;&#37322;&#33021;&#21147;&#65292;&#25552;&#39640;&#30456;&#20851;&#20294;&#19981;&#21516;&#20219;&#21153;&#30340;&#24615;&#33021;&#65288;&#39640;&#36798;10.3%&#65289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#28040;&#34701;&#20998;&#26512;&#65292;&#25506;&#35752;MMST&#30340;&#24037;&#20316;&#21407;&#29702;&#12290;&#25105;&#20204;&#35777;&#26126;MMST&#27604;&#20256;&#32479;&#30340;&#33258;&#35757;&#32451;&#26041;&#27861;&#29983;&#25104;&#26356;&#22810;&#25968;&#25454;&#65292;&#20294;&#24615;&#33021;&#25552;&#21319;&#26356;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models have many methods for solving the same problem. This introduces novel strengths (different methods may work well for different problems) and weaknesses (it may be difficult for users to know which method to use). In this paper, we introduce Multi-Method Self-Training (MMST), where one method is trained on the filtered outputs of another, allowing us to augment the strengths and ameliorate the weaknesses of each method. Using a 176B parameter model trained on both language and code, we show that MMST can 1) improve the less performant method (up to 30%) making the model easier to use, 2) improve the more performant method (up to 32.2%) making the model more performant, and 3) improve the performance of related but distinct tasks (up to 10.3%) by improving the ability of the model to generate rationales. We then conduct ablation analyses to explore why MMST works. We show that MMST generates more data than traditional self-training, but the improvement in performanc
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23545;NPTEL MOOC&#35270;&#39057;&#20013;&#25104;&#21315;&#19978;&#19975;&#20010;&#21333;&#35789;&#38169;&#35823;&#29575;&#24046;&#24322;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#21457;&#29616;&#30001;&#20110;&#35828;&#35805;&#32773;&#30340;&#35821;&#38899;&#29305;&#24615;&#23384;&#22312;&#24046;&#24322;&#65292;&#26368;&#20808;&#36827;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#22312;&#26576;&#20123;&#22320;&#21306;&#25110;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#30340;&#35828;&#35805;&#32773;&#36523;&#19978;&#38754;&#20020;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2307.10587</link><description>&lt;p&gt;
&#23545;NPTEL MOOC&#35270;&#39057;&#20013;&#25104;&#21315;&#19978;&#19975;&#20010;&#21333;&#35789;&#38169;&#35823;&#29575;&#24046;&#24322;&#30340;&#28145;&#20837;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Deep Dive into the Disparity of Word Error Rates Across Thousands of NPTEL MOOC Videos. (arXiv:2307.10587v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10587
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23545;NPTEL MOOC&#35270;&#39057;&#20013;&#25104;&#21315;&#19978;&#19975;&#20010;&#21333;&#35789;&#38169;&#35823;&#29575;&#24046;&#24322;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#21457;&#29616;&#30001;&#20110;&#35828;&#35805;&#32773;&#30340;&#35821;&#38899;&#29305;&#24615;&#23384;&#22312;&#24046;&#24322;&#65292;&#26368;&#20808;&#36827;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#22312;&#26576;&#20123;&#22320;&#21306;&#25110;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#30340;&#35828;&#35805;&#32773;&#36523;&#19978;&#38754;&#20020;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#26088;&#22312;&#23558;&#21475;&#35821;&#36716;&#24405;&#20026;&#20070;&#38754;&#25991;&#26412;&#65292;&#24182;&#22312;&#35821;&#38899;&#21161;&#25163;&#21644;&#36716;&#24405;&#26381;&#21153;&#31561;&#21508;&#31181;&#24212;&#29992;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#35266;&#23519;&#21040;&#23613;&#31649;&#26368;&#20808;&#36827;&#30340;ASR&#31995;&#32479;&#22312;&#25552;&#20379;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#22522;&#20934;&#32467;&#26524;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#30001;&#20110;&#35828;&#35805;&#32773;&#30340;&#35821;&#38899;&#29305;&#24615;&#23384;&#22312;&#24046;&#24322;&#65292;&#23427;&#20204;&#22312;&#26576;&#20123;&#22320;&#21306;&#25110;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#30340;&#35828;&#35805;&#32773;&#36523;&#19978;&#38754;&#20020;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#24222;&#22823;&#30340;&#35821;&#38899;&#25968;&#25454;&#38598;&#30340;&#31574;&#21010;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;8740&#23567;&#26102;&#30340;&#25216;&#26415;&#35762;&#24231;&#65292;&#20197;&#21450;&#30001;&#20195;&#34920;&#21360;&#24230;&#21508;&#20010;&#22320;&#21306;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#30340;&#35762;&#24072;&#36827;&#34892;&#30340;&#35762;&#24231;&#30340;&#35760;&#24405;&#12290;&#35813;&#25968;&#25454;&#38598;&#26469;&#28304;&#20110;&#38750;&#24120;&#27969;&#34892;&#30340;NPTEL MOOC&#24179;&#21488;&#12290;&#25105;&#20204;&#20351;&#29992;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#34913;&#37327;&#21360;&#24230;&#35828;&#35805;&#32773;&#30340;&#22810;&#26679;&#24615;&#20154;&#21475;&#32479;&#35745;&#29305;&#24449;&#65292;&#26469;&#34913;&#37327;YouTube&#33258;&#21160;&#23383;&#24149;&#21644;OpenAI Whisper&#27169;&#22411;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic speech recognition (ASR) systems are designed to transcribe spoken language into written text and find utility in a variety of applications including voice assistants and transcription services. However, it has been observed that state-of-the-art ASR systems which deliver impressive benchmark results, struggle with speakers of certain regions or demographics due to variation in their speech properties. In this work, we describe the curation of a massive speech dataset of 8740 hours consisting of $\sim9.8$K technical lectures in the English language along with their transcripts delivered by instructors representing various parts of Indian demography. The dataset is sourced from the very popular NPTEL MOOC platform. We use the curated dataset to measure the existing disparity in YouTube Automatic Captions and OpenAI Whisper model performance across the diverse demographic traits of speakers in India. While there exists disparity due to gender, native region, age and speech rate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25351;&#31034;&#36981;&#24490;&#35780;&#20272;&#21327;&#35758;&#65292;&#21475;&#36848;&#32773;&#25805;&#20316;&#65292;&#36890;&#36807;&#21475;&#36848;&#20219;&#21153;&#26631;&#31614;&#26469;&#26816;&#26597;&#27169;&#22411;&#23545;&#20808;&#39564;&#30693;&#35782;&#30340;&#20381;&#36182;&#31243;&#24230;&#65292;&#20197;&#21450;&#35206;&#30422;&#23427;&#20204;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#20934;&#30830;&#22320;&#36827;&#34892;&#25351;&#31034;&#36981;&#24490;&#12290;</title><link>http://arxiv.org/abs/2307.10558</link><description>&lt;p&gt;
&#36890;&#36807;&#21475;&#36848;&#26041;&#24335;&#25805;&#20316;&#36827;&#34892;&#25351;&#31034;&#36981;&#24490;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Instruction-following Evaluation through Verbalizer Manipulation. (arXiv:2307.10558v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10558
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25351;&#31034;&#36981;&#24490;&#35780;&#20272;&#21327;&#35758;&#65292;&#21475;&#36848;&#32773;&#25805;&#20316;&#65292;&#36890;&#36807;&#21475;&#36848;&#20219;&#21153;&#26631;&#31614;&#26469;&#26816;&#26597;&#27169;&#22411;&#23545;&#20808;&#39564;&#30693;&#35782;&#30340;&#20381;&#36182;&#31243;&#24230;&#65292;&#20197;&#21450;&#35206;&#30422;&#23427;&#20204;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#20934;&#30830;&#22320;&#36827;&#34892;&#25351;&#31034;&#36981;&#24490;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#35843;&#25972;&#25351;&#20196;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#20934;&#30830;&#35780;&#20272;&#20854;&#36981;&#24490;&#25351;&#20196;&#30340;&#33021;&#21147;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#30340;&#22522;&#20934;&#20027;&#35201;&#20851;&#27880;&#19982;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23398;&#20064;&#30340;&#20869;&#23481;&#30456;&#21563;&#21512;&#30340;&#24120;&#35265;&#25351;&#20196;&#12290;&#28982;&#32780;&#65292;&#23545;&#36825;&#20123;&#25351;&#20196;&#30340;&#22238;&#24212;&#33021;&#21147;&#24182;&#19981;&#19968;&#23450;&#24847;&#21619;&#30528;&#24378;&#22823;&#30340;&#36981;&#24490;&#25351;&#20196;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25351;&#31034;&#36981;&#24490;&#35780;&#20272;&#21327;&#35758;&#65292;&#31216;&#20026;&#21475;&#36848;&#32773;&#25805;&#20316;&#12290;&#23427;&#35201;&#27714;&#27169;&#22411;&#29992;&#19982;&#27169;&#22411;&#20808;&#39564;&#30693;&#35782;&#19981;&#21516;&#31243;&#24230;&#21563;&#21512;&#30340;&#21333;&#35789;&#21475;&#36848;&#20219;&#21153;&#26631;&#31614;&#65292;&#20174;&#39640;&#24230;&#21563;&#21512;&#65288;&#20363;&#22914;&#65292;&#23545;&#20110;&#31215;&#26497;&#24773;&#32490;&#36755;&#20986;&#8220;&#31215;&#26497;&#8221;&#65289;&#21040;&#26368;&#23569;&#21563;&#21512;&#65288;&#20363;&#22914;&#65292;&#23545;&#20110;&#31215;&#26497;&#24773;&#32490;&#36755;&#20986;&#8220;&#28040;&#26497;&#8221;&#65289;&#12290;&#21475;&#36848;&#32773;&#25805;&#20316;&#21487;&#20197;&#19982;&#20219;&#20309;&#20998;&#31867;&#22522;&#20934;&#26080;&#32541;&#38598;&#25104;&#65292;&#20197;&#26816;&#26597;&#27169;&#22411;&#23545;&#20808;&#39564;&#30693;&#35782;&#30340;&#20381;&#36182;&#31243;&#24230;&#20197;&#21450;&#35206;&#30422;&#23427;&#20204;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#20934;&#30830;&#22320;&#36827;&#34892;&#25351;&#31034;&#36981;&#24490;&#12290;
&lt;/p&gt;
&lt;p&gt;
While instruction-tuned models have shown remarkable success in various natural language processing tasks, accurately evaluating their ability to follow instructions remains challenging. Existing benchmarks primarily focus on common instructions that align well with what the model learned during training. However, proficiency in responding to these instructions does not necessarily imply strong ability in instruction following. In this paper, we propose a novel instruction-following evaluation protocol called verbalizer manipulation. It instructs the model to verbalize the task label with words aligning with model priors to different extents, adopting verbalizers from highly aligned (e.g., outputting ``postive'' for positive sentiment), to minimally aligned (e.g., outputting ``negative'' for positive sentiment). Verbalizer manipulation can be seamlessly integrated with any classification benchmark to examine the model's reliance on priors and its ability to override them to accurately 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#22312;&#21306;&#22359;&#38142;&#19978;&#35757;&#32451;&#21644;&#37096;&#32626;&#21160;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#20998;&#24067;&#24335;&#35745;&#31639;&#21644;&#25552;&#20379;&#26080;&#27861;&#31713;&#25913;&#30340;&#20132;&#26131;&#20998;&#31867;&#24080;&#30340;&#21306;&#22359;&#38142;&#25216;&#26415;&#65292;&#21487;&#20351;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#36830;&#32493;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#20026;&#19979;&#19968;&#20195;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#21644;&#21551;&#31034;&#12290;</title><link>http://arxiv.org/abs/2307.10549</link><description>&lt;p&gt;
&#21306;&#22359;&#38142;&#19978;&#30340;&#21160;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Dynamic Large Language Models on Blockchains. (arXiv:2307.10549v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22312;&#21306;&#22359;&#38142;&#19978;&#35757;&#32451;&#21644;&#37096;&#32626;&#21160;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#20998;&#24067;&#24335;&#35745;&#31639;&#21644;&#25552;&#20379;&#26080;&#27861;&#31713;&#25913;&#30340;&#20132;&#26131;&#20998;&#31867;&#24080;&#30340;&#21306;&#22359;&#38142;&#25216;&#26415;&#65292;&#21487;&#20351;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#36830;&#32493;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#20026;&#19979;&#19968;&#20195;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#21644;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#21644;&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#22240;&#20026;&#35821;&#35328;&#27169;&#22411;&#21253;&#21547;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#65292;&#25991;&#26412;&#25317;&#26377;&#25968;&#21315;&#20010;&#26631;&#35760;&#12290;&#21478;&#19968;&#20010;&#38382;&#39064;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#38745;&#24577;&#30340;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#21518;&#23601;&#34987;&#22266;&#23450;&#19979;&#26469;&#20102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#22312;&#21306;&#22359;&#38142;&#19978;&#35757;&#32451;&#21644;&#37096;&#32626;&#21160;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21306;&#22359;&#38142;&#20855;&#26377;&#39640;&#35745;&#31639;&#24615;&#33021;&#24182;&#20998;&#24067;&#22312;&#19968;&#20010;&#35745;&#31639;&#26426;&#32593;&#32476;&#19978;&#12290;&#21306;&#22359;&#38142;&#26159;&#19968;&#20010;&#23433;&#20840;&#12289;&#20998;&#25955;&#21644;&#36879;&#26126;&#30340;&#31995;&#32479;&#65292;&#20801;&#35768;&#21019;&#24314;&#19968;&#20010;&#26080;&#27861;&#31713;&#25913;&#30340;&#20132;&#26131;&#20998;&#31867;&#24080;&#65292;&#26080;&#38656;&#20013;&#20171;&#26426;&#26500;&#12290;&#21160;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20043;&#21518;&#19981;&#26029;&#20174;&#29992;&#25143;&#36755;&#20837;&#20013;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#24320;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#20026;&#19979;&#19968;&#20195;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#24102;&#26469;&#20102;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training and deploying the large language models requires a large mount of computational resource because the language models contain billions of parameters and the text has thousands of tokens. Another problem is that the large language models are static. They are fixed after the training process. To tackle these issues, in this paper, we propose to train and deploy the dynamic large language model on blockchains, which have high computation performance and are distributed across a network of computers. A blockchain is a secure, decentralized, and transparent system that allows for the creation of a tamper-proof ledger for transactions without the need for intermediaries. The dynamic large language models can continuously learn from the user input after the training process. Our method provides a new way to develop the large language models and also sheds a light on the next generation artificial intelligence systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Gender-tuning&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19979;&#28216;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#21435;&#20559;&#32622;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;&#19981;&#20165;&#33021;&#22312;PLMs&#30340;&#24615;&#21035;&#20559;&#35265;&#24471;&#20998;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#32780;&#19988;&#20165;&#20351;&#29992;&#19979;&#28216;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#21363;&#21487;&#25552;&#39640;PLMs&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.10522</link><description>&lt;p&gt;
Gender-tuning: &#25480;&#26435;&#24494;&#35843;&#29992;&#20110;&#21435;&#20559;&#32622;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Gender-tuning: Empowering Fine-tuning for Debiasing Pre-trained Language Models. (arXiv:2307.10522v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Gender-tuning&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19979;&#28216;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#21435;&#20559;&#32622;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;&#19981;&#20165;&#33021;&#22312;PLMs&#30340;&#24615;&#21035;&#20559;&#35265;&#24471;&#20998;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#32780;&#19988;&#20165;&#20351;&#29992;&#19979;&#28216;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#21363;&#21487;&#25552;&#39640;PLMs&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24191;&#27867;&#20351;&#29992;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#20174;&#22823;&#22411;&#38750;&#35843;&#25511;&#30340;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#20256;&#25773;&#31038;&#20250;&#20559;&#35265;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#38656;&#35201;&#21435;&#20559;&#32622;&#30340;&#35757;&#32451;&#36807;&#31243;&#21644;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#37117;&#38656;&#35201;&#22823;&#37327;&#36164;&#28304;&#21644;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#20250;&#25439;&#23475;PLMs&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Gender-tuning&#65292;&#36890;&#36807;&#22312;&#19979;&#28216;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#26469;&#21435;&#20559;&#32622;PLMs&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;Gender-tuning&#23558;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#65288;MLM&#65289;&#30340;&#35757;&#32451;&#30446;&#26631;&#38598;&#25104;&#21040;&#24494;&#35843;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;Gender-tuning&#22312;PLMs&#30340;&#24179;&#22343;&#24615;&#21035;&#20559;&#35265;&#24471;&#20998;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#32447;&#65292;&#21516;&#26102;&#20165;&#20351;&#29992;&#19979;&#28216;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#26469;&#25552;&#39640;PLMs&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;Gender-tuning&#26159;&#19968;&#20010;&#21487;&#37096;&#32626;&#30340;&#21435;&#20559;&#32622;&#24037;&#20855;&#65292;&#36866;&#29992;&#20110;&#20219;&#20309;&#37319;&#29992;&#21407;&#26377;&#24494;&#35843;&#26041;&#27861;&#30340;PLM&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have revealed that the widely-used Pre-trained Language Models (PLMs) propagate societal biases from the large unmoderated pre-training corpora. Existing solutions require debiasing training processes and datasets for debiasing, which are resource-intensive and costly. Furthermore, these methods hurt the PLMs' performance on downstream tasks. In this study, we propose Gender-tuning, which debiases the PLMs through fine-tuning on downstream tasks' datasets. For this aim, Gender-tuning integrates Masked Language Modeling (MLM) training objectives into fine-tuning's training process. Comprehensive experiments show that Gender-tuning outperforms the state-of-the-art baselines in terms of average gender bias scores in PLMs while improving PLMs' performance on downstream tasks solely using the downstream tasks' dataset. Also, Gender-tuning is a deployable debiasing tool for any PLM that works with original fine-tuning.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31038;&#21306;&#21442;&#19982;&#30340;&#21162;&#21147;&#65292;&#22312;&#21360;&#24230;&#31038;&#20250;&#32972;&#26223;&#19979;&#25193;&#23637;&#20102;&#23545;&#21051;&#26495;&#21360;&#35937;&#20260;&#23475;&#30340;&#35780;&#20272;&#36164;&#28304;&#12290;&#36825;&#20010;&#24037;&#20316;&#24378;&#35843;&#20102;&#21253;&#23481;&#19981;&#21516;&#25991;&#21270;&#21644;&#31038;&#20250;&#32972;&#26223;&#30340;&#20154;&#20204;&#21644;&#32463;&#39564;&#65292;&#20197;&#36991;&#20813;&#23545;&#20260;&#23475;&#27979;&#37327;&#30340;&#20005;&#37325;&#20302;&#20272;&#25110;&#25197;&#26354;&#12290;</title><link>http://arxiv.org/abs/2307.10514</link><description>&lt;p&gt;
&#36890;&#36807;&#31038;&#21306;&#21442;&#19982;&#26500;&#24314;&#19982;&#31038;&#20250;&#25991;&#21270;&#21253;&#23481;&#24615;&#30340;&#21051;&#26495;&#21360;&#35937;&#36164;&#28304;
&lt;/p&gt;
&lt;p&gt;
Building Socio-culturally Inclusive Stereotype Resources with Community Engagement. (arXiv:2307.10514v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10514
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31038;&#21306;&#21442;&#19982;&#30340;&#21162;&#21147;&#65292;&#22312;&#21360;&#24230;&#31038;&#20250;&#32972;&#26223;&#19979;&#25193;&#23637;&#20102;&#23545;&#21051;&#26495;&#21360;&#35937;&#20260;&#23475;&#30340;&#35780;&#20272;&#36164;&#28304;&#12290;&#36825;&#20010;&#24037;&#20316;&#24378;&#35843;&#20102;&#21253;&#23481;&#19981;&#21516;&#25991;&#21270;&#21644;&#31038;&#20250;&#32972;&#26223;&#30340;&#20154;&#20204;&#21644;&#32463;&#39564;&#65292;&#20197;&#36991;&#20813;&#23545;&#20260;&#23475;&#27979;&#37327;&#30340;&#20005;&#37325;&#20302;&#20272;&#25110;&#25197;&#26354;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#30340;&#36805;&#36895;&#21457;&#23637;&#21644;&#37096;&#32626;&#65292;&#25105;&#20204;&#36843;&#20999;&#38656;&#35201;&#22312;&#27979;&#37327;&#20260;&#23475;&#26041;&#38754;&#36827;&#34892;&#25193;&#23637;&#65292;&#19981;&#20165;&#21253;&#25324;&#35206;&#30422;&#30340;&#20260;&#23475;&#25968;&#37327;&#21644;&#31867;&#22411;&#65292;&#36824;&#35201;&#32771;&#34385;&#21040;&#24403;&#22320;&#25991;&#21270;&#32972;&#26223;&#65292;&#21253;&#25324;&#36793;&#32536;&#21270;&#36523;&#20221;&#21644;&#20182;&#20204;&#25152;&#32463;&#21382;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#24403;&#21069;&#30340;&#35780;&#20272;&#33539;&#24335;&#22312;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#19978;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#19981;&#20195;&#34920;&#22810;&#20803;&#21270;&#12289;&#26412;&#22320;&#21270;&#20294;&#20840;&#29699;&#21270;&#30340;&#31038;&#20250;&#25991;&#21270;&#35270;&#35282;&#12290;&#20026;&#20102;&#36991;&#20813;&#23545;&#20260;&#23475;&#27979;&#37327;&#20135;&#29983;&#20005;&#37325;&#20302;&#20272;&#25110;&#25197;&#26354;&#65292;&#25105;&#20204;&#36843;&#20999;&#38656;&#35201;&#36890;&#36807;&#21253;&#21547;&#26469;&#33258;&#19981;&#21516;&#25991;&#21270;&#21644;&#31038;&#20250;&#30340;&#20154;&#20204;&#21644;&#32463;&#39564;&#26469;&#22686;&#24378;&#21644;&#26657;&#20934;&#25105;&#20204;&#30340;&#35780;&#20272;&#36164;&#28304;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;&#21360;&#24230;&#31038;&#20250;&#32972;&#26223;&#19979;&#23637;&#31034;&#20102;&#23545;&#21051;&#26495;&#21360;&#35937;&#20260;&#23475;&#23454;&#26045;&#31038;&#20250;&#25991;&#21270;&#24847;&#35782;&#30340;&#35780;&#20272;&#36164;&#28304;&#30340;&#25193;&#23637;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31038;&#21306;&#21442;&#19982;&#30340;&#21162;&#21147;&#26469;&#26500;&#24314;&#19968;&#20010;&#21253;&#21547;&#21051;&#26495;&#21360;&#35937;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
With rapid development and deployment of generative language models in global settings, there is an urgent need to also scale our measurements of harm, not just in the number and types of harms covered, but also how well they account for local cultural contexts, including marginalized identities and the social biases experienced by them. Current evaluation paradigms are limited in their abilities to address this, as they are not representative of diverse, locally situated but global, socio-cultural perspectives. It is imperative that our evaluation resources are enhanced and calibrated by including people and experiences from different cultures and societies worldwide, in order to prevent gross underestimations or skews in measurements of harm. In this work, we demonstrate a socio-culturally aware expansion of evaluation resources in the Indian societal context, specifically for the harm of stereotyping. We devise a community engaged effort to build a resource which contains stereotype
&lt;/p&gt;</description></item><item><title>IvyGPT&#26159;&#19968;&#31181;&#22522;&#20110;&#21307;&#23398;&#39046;&#22495;&#30340;&#20013;&#25991;&#20114;&#21160;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#39640;&#36136;&#37327;&#30340;&#21307;&#23398;&#38382;&#31572;&#31034;&#20363;&#21644;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#23427;&#33021;&#22815;&#36755;&#20986;&#26356;&#20016;&#23500;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#31572;&#26696;&#65292;&#20174;&#32780;&#22312;&#21307;&#23398;GPT&#27169;&#22411;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2307.10512</link><description>&lt;p&gt;
IvyGPT&#65306;&#22522;&#20110;&#21307;&#23398;&#39046;&#22495;&#30340;&#20114;&#21160;&#24335;&#20013;&#25991;&#36335;&#24452;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
IvyGPT: InteractiVe Chinese pathwaY language model in medical domain. (arXiv:2307.10512v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10512
&lt;/p&gt;
&lt;p&gt;
IvyGPT&#26159;&#19968;&#31181;&#22522;&#20110;&#21307;&#23398;&#39046;&#22495;&#30340;&#20013;&#25991;&#20114;&#21160;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#39640;&#36136;&#37327;&#30340;&#21307;&#23398;&#38382;&#31572;&#31034;&#20363;&#21644;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#23427;&#33021;&#22815;&#36755;&#20986;&#26356;&#20016;&#23500;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#31572;&#26696;&#65292;&#20174;&#32780;&#22312;&#21307;&#23398;GPT&#27169;&#22411;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#33324;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;ChatGPT&#24050;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#31934;&#24230;&#19981;&#39640;&#21644;&#26080;&#27861;&#25552;&#20379;&#21307;&#30103;&#24314;&#35758;&#65292;&#36825;&#20123;LLM&#22312;&#21307;&#23398;&#39046;&#22495;&#24182;&#26410;&#24191;&#27867;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLaMA&#30340;LLM IvyGPT&#65292;&#23427;&#36890;&#36807;&#39640;&#36136;&#37327;&#30340;&#21307;&#23398;&#38382;&#31572;&#65288;QA&#65289;&#31034;&#20363;&#21644;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#36827;&#34892;&#35757;&#32451;&#21644;&#24494;&#35843;&#12290;&#32463;&#36807;&#26377;&#30417;&#30563;&#24494;&#35843;&#21518;&#65292;IvyGPT&#20855;&#26377;&#33391;&#22909;&#30340;&#22810;&#36718;&#23545;&#35805;&#33021;&#21147;&#65292;&#20294;&#22312;&#32508;&#21512;&#35786;&#26029;&#31561;&#20854;&#20182;&#26041;&#38754;&#19981;&#33021;&#20687;&#21307;&#29983;&#19968;&#26679;&#36816;&#34892;&#12290;&#36890;&#36807;RLHF&#65292;IvyGPT&#21487;&#20197;&#36755;&#20986;&#26356;&#20016;&#23500;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#31572;&#26696;&#65292;&#26356;&#25509;&#36817;&#20110;&#20154;&#31867;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;QLoRA&#22312;&#23569;&#37327;NVIDIA A100&#65288;80GB) GPU&#19978;&#35757;&#32451;&#20102;330&#20159;&#20010;&#21442;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;IvyGPT&#22312;&#21307;&#23398;GPT&#27169;&#22411;&#20013;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
General large language models (LLMs) such as ChatGPT have shown remarkable success. However, such LLMs have not been widely adopted for medical purposes, due to poor accuracy and inability to provide medical advice. We propose IvyGPT, an LLM based on LLaMA that is trained and fine-tuned with high-quality medical question-answer (QA) instances and Reinforcement Learning from Human Feedback (RLHF). After supervised fine-tuning, IvyGPT has good multi-turn conversation capabilities, but it cannot perform like a doctor in other aspects, such as comprehensive diagnosis. Through RLHF, IvyGPT can output richer diagnosis and treatment answers that are closer to human. In the training, we used QLoRA to train 33 billion parameters on a small number of NVIDIA A100 (80GB) GPUs. Experimental results show that IvyGPT has outperformed other medical GPT models.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#21435;&#20559;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#65292;&#36890;&#36807;&#20943;&#23569;&#27169;&#22411;&#23545;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#20381;&#36182;&#65292;&#25552;&#39640;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#27169;&#22411;&#30340;&#38750;&#20998;&#24067;&#22806;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.10511</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#30340;&#36890;&#29992;&#21435;&#20559;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
General Debiasing for Multimodal Sentiment Analysis. (arXiv:2307.10511v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10511
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#21435;&#20559;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#65292;&#36890;&#36807;&#20943;&#23569;&#27169;&#22411;&#23545;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#20381;&#36182;&#65292;&#25552;&#39640;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#27169;&#22411;&#30340;&#38750;&#20998;&#24067;&#22806;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#24037;&#20316;&#21033;&#29992;&#22810;&#27169;&#24577;&#20449;&#24687;&#36827;&#34892;&#39044;&#27979;&#65292;&#20294;&#19981;&#21487;&#36991;&#20813;&#22320;&#21463;&#21040;&#22810;&#27169;&#24577;&#29305;&#24449;&#21644;&#24773;&#24863;&#26631;&#31614;&#20043;&#38388;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#24433;&#21709;&#12290;&#20363;&#22914;&#65292;&#22914;&#26524;&#25968;&#25454;&#38598;&#20013;&#22823;&#22810;&#25968;&#24102;&#26377;&#34013;&#33394;&#32972;&#26223;&#30340;&#35270;&#39057;&#37117;&#26377;&#27491;&#38754;&#26631;&#31614;&#65292;&#27169;&#22411;&#23558;&#20381;&#36182;&#20110;&#36825;&#26679;&#30340;&#30456;&#20851;&#24615;&#36827;&#34892;&#39044;&#27979;&#65292;&#32780;&#8220;&#34013;&#33394;&#32972;&#26223;&#8221;&#24182;&#19981;&#26159;&#19968;&#20010;&#19982;&#24773;&#24863;&#30456;&#20851;&#30340;&#29305;&#24449;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#21435;&#20559;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#65292;&#26088;&#22312;&#36890;&#36807;&#20943;&#23569;&#27169;&#22411;&#23545;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#20381;&#36182;&#65292;&#25552;&#39640;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#27169;&#22411;&#30340;&#38750;&#20998;&#24067;&#22806;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20498;&#25968;&#27010;&#29575;&#21152;&#26435;&#65288;Inverse Probability Weighting&#65292;IPW&#65289;&#30340;&#36890;&#29992;&#21435;&#20559;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33258;&#36866;&#24212;&#22320;&#20026;&#20855;&#26377;&#26356;&#22823;&#20559;&#24046;&#65288;&#21363;&#26356;&#20005;&#37325;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#65289;&#30340;&#26679;&#26412;&#20998;&#37197;&#36739;&#23567;&#30340;&#26435;&#37325;&#12290;&#36825;&#20010;&#21435;&#20559;&#26694;&#26550;&#30340;&#20851;&#38190;&#22312;&#20110;&#20272;&#35745;&#27599;&#20010;&#26679;&#26412;&#30340;&#20559;&#24046;&#65292;&#36825;&#36890;&#36807;&#20004;&#20010;&#27493;&#39588;&#23454;&#29616;&#65306;1&#65289;&#23558;&#40065;&#26834;&#29305;&#24449;&#21644;&#20559;&#35265;&#29305;&#24449;&#20998;&#31163;&#20986;&#26469;
&lt;/p&gt;
&lt;p&gt;
Existing work on Multimodal Sentiment Analysis (MSA) utilizes multimodal information for prediction yet unavoidably suffers from fitting the spurious correlations between multimodal features and sentiment labels. For example, if most videos with a blue background have positive labels in a dataset, the model will rely on such correlations for prediction, while ``blue background'' is not a sentiment-related feature. To address this problem, we define a general debiasing MSA task, which aims to enhance the Out-Of-Distribution (OOD) generalization ability of MSA models by reducing their reliance on spurious correlations. To this end, we propose a general debiasing framework based on Inverse Probability Weighting (IPW), which adaptively assigns small weights to the samples with larger bias i.e., the severer spurious correlations). The key to this debiasing framework is to estimate the bias of each sample, which is achieved by two steps: 1) disentangling the robust features and biased featur
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#22270;&#20687;&#21644;&#22768;&#38899;&#22312;&#22810;&#27169;&#24577;LLMs&#20013;&#36827;&#34892;&#38388;&#25509;&#25351;&#20196;&#27880;&#20837;&#65292;&#25915;&#20987;&#32773;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#25200;&#21160;&#24182;&#23558;&#20854;&#34701;&#20837;&#22270;&#20687;&#25110;&#38899;&#39057;&#24405;&#38899;&#20013;&#65292;&#20197;&#25805;&#32437;&#27169;&#22411;&#36755;&#20986;&#29305;&#23450;&#25991;&#26412;&#21644;&#25351;&#23548;&#23545;&#35805;&#30340;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2307.10490</link><description>&lt;p&gt;
&#22270;&#20687;&#21644;&#22768;&#38899;&#30340;&#28389;&#29992;&#29992;&#20110;&#22312;&#22810;&#27169;&#24577;LLMs&#20013;&#36827;&#34892;&#38388;&#25509;&#25351;&#20196;&#27880;&#20837;
&lt;/p&gt;
&lt;p&gt;
(Ab)using Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs. (arXiv:2307.10490v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10490
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#22270;&#20687;&#21644;&#22768;&#38899;&#22312;&#22810;&#27169;&#24577;LLMs&#20013;&#36827;&#34892;&#38388;&#25509;&#25351;&#20196;&#27880;&#20837;&#65292;&#25915;&#20987;&#32773;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#25200;&#21160;&#24182;&#23558;&#20854;&#34701;&#20837;&#22270;&#20687;&#25110;&#38899;&#39057;&#24405;&#38899;&#20013;&#65292;&#20197;&#25805;&#32437;&#27169;&#22411;&#36755;&#20986;&#29305;&#23450;&#25991;&#26412;&#21644;&#25351;&#23548;&#23545;&#35805;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#22270;&#20687;&#21644;&#22768;&#38899;&#22312;&#22810;&#27169;&#24577;LLMs&#20013;&#36827;&#34892;&#38388;&#25509;&#25552;&#31034;&#21644;&#25351;&#20196;&#27880;&#20837;&#12290;&#25915;&#20987;&#32773;&#29983;&#25104;&#19982;&#25552;&#31034;&#30456;&#23545;&#24212;&#30340;&#23545;&#25239;&#25200;&#21160;&#65292;&#24182;&#23558;&#20854;&#34701;&#20837;&#22270;&#20687;&#25110;&#38899;&#39057;&#24405;&#38899;&#20013;&#12290;&#24403;&#29992;&#25143;&#21521;&#65288;&#26410;&#20462;&#25913;&#30340;&#33391;&#24615;&#65289;&#27169;&#22411;&#35810;&#38382;&#34987;&#25200;&#21160;&#30340;&#22270;&#20687;&#25110;&#38899;&#39057;&#26102;&#65292;&#25200;&#21160;&#20250;&#24341;&#23548;&#27169;&#22411;&#36755;&#20986;&#25915;&#20987;&#32773;&#36873;&#25321;&#30340;&#25991;&#26412;&#21644;/&#25110;&#20351;&#21518;&#32493;&#23545;&#35805;&#36981;&#24490;&#25915;&#20987;&#32773;&#30340;&#25351;&#20196;&#12290;&#25105;&#20204;&#29992;&#20960;&#20010;&#27010;&#24565;&#39564;&#35777;&#31034;&#20363;&#38024;&#23545;LLaVa&#21644;PandaGPT&#26469;&#35828;&#26126;&#36825;&#31181;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
We demonstrate how images and sounds can be used for indirect prompt and instruction injection in multi-modal LLMs. An attacker generates an adversarial perturbation corresponding to the prompt and blends it into an image or audio recording. When the user asks the (unmodified, benign) model about the perturbed image or audio, the perturbation steers the model to output the attacker-chosen text and/or make the subsequent dialog follow the attacker's instruction. We illustrate this attack with several proof-of-concept examples targeting LLaVa and PandaGPT.
&lt;/p&gt;</description></item><item><title>SPRINT&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;Python&#24037;&#20855;&#21253;&#65292;&#22522;&#20110;Pyserini&#21644;Lucene&#65292;&#25903;&#25345;&#35780;&#20272;&#21644;&#35299;&#26512;&#38646;&#26679;&#26412;&#31070;&#32463;&#31232;&#30095;&#26816;&#32034;&#12290;&#23427;&#35299;&#20915;&#20102;&#32570;&#20047;&#32479;&#19968;&#29615;&#22659;&#21644;&#23454;&#29616;&#22312;&#26410;&#35265;&#36807;&#39046;&#22495;&#19978;&#30340;&#26816;&#32034;&#33021;&#21147;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.10488</link><description>&lt;p&gt;
SPRINT: &#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#21644;&#35299;&#26512;&#38646;&#26679;&#26412;&#31070;&#32463;&#31232;&#30095;&#26816;&#32034;&#30340;&#32479;&#19968;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
SPRINT: A Unified Toolkit for Evaluating and Demystifying Zero-shot Neural Sparse Retrieval. (arXiv:2307.10488v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10488
&lt;/p&gt;
&lt;p&gt;
SPRINT&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;Python&#24037;&#20855;&#21253;&#65292;&#22522;&#20110;Pyserini&#21644;Lucene&#65292;&#25903;&#25345;&#35780;&#20272;&#21644;&#35299;&#26512;&#38646;&#26679;&#26412;&#31070;&#32463;&#31232;&#30095;&#26816;&#32034;&#12290;&#23427;&#35299;&#20915;&#20102;&#32570;&#20047;&#32479;&#19968;&#29615;&#22659;&#21644;&#23454;&#29616;&#22312;&#26410;&#35265;&#36807;&#39046;&#22495;&#19978;&#30340;&#26816;&#32034;&#33021;&#21147;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#31232;&#30095;&#26816;&#32034;&#31995;&#32479;&#20381;&#36182;&#20110;&#35789;&#27719;&#34920;&#31034;&#26469;&#26816;&#32034;&#25991;&#26723;&#65292;&#22914;BM25&#65292;&#22312;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#21344;&#20027;&#23548;&#22320;&#20301;&#12290;&#38543;&#30528;&#35832;&#22914;BERT&#36825;&#26679;&#30340;&#39044;&#35757;&#32451;transformer&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#31070;&#32463;&#31232;&#30095;&#26816;&#32034;&#24341;&#39046;&#20102;&#26816;&#32034;&#20013;&#30340;&#26032;&#33539;&#24335;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#30446;&#21069;&#32570;&#20047;&#25903;&#25345;&#19981;&#21516;&#31232;&#30095;&#26816;&#32034;&#22120;&#22312;&#32479;&#19968;&#30340;&#29615;&#22659;&#20013;&#36816;&#34892;&#30340;&#36719;&#20214;&#12290;&#36825;&#22952;&#30861;&#20102;&#23454;&#36341;&#32773;&#20844;&#27491;&#22320;&#27604;&#36739;&#19981;&#21516;&#30340;&#31232;&#30095;&#27169;&#22411;&#65292;&#24182;&#33719;&#24471;&#30495;&#23454;&#30340;&#35780;&#20272;&#32467;&#26524;&#12290;&#36824;&#26377;&#19968;&#20010;&#32570;&#22833;&#30340;&#37096;&#20998;&#26159;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#24037;&#20316;&#26159;&#23545;&#31232;&#30095;&#26816;&#32034;&#27169;&#22411;&#36827;&#34892;&#22495;&#20869;&#26816;&#32034;&#35780;&#20272;&#65292;&#21363;&#20165;&#22312;&#19968;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65306;MS MARCO&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#26816;&#32034;&#31995;&#32479;&#20013;&#65292;&#19968;&#20010;&#37325;&#35201;&#30340;&#35201;&#27714;&#26159;&#27169;&#22411;&#33021;&#22815;&#22312;&#26410;&#35265;&#36807;&#30340;&#22495;&#22806;&#65292;&#21363;&#38646;&#26679;&#26412;&#26816;&#32034;&#20219;&#21153;&#20013;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;SPRINT&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;Pyserini&#21644;Lucene&#30340;&#32479;&#19968;Python&#24037;&#20855;&#21253;&#65292;&#25903;&#25345;&#31070;&#32463;&#31232;&#30095;&#26816;&#32034;&#30340;&#36890;&#29992;&#25509;&#21475;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditionally, sparse retrieval systems relied on lexical representations to retrieve documents, such as BM25, dominated information retrieval tasks. With the onset of pre-trained transformer models such as BERT, neural sparse retrieval has led to a new paradigm within retrieval. Despite the success, there has been limited software supporting different sparse retrievers running in a unified, common environment. This hinders practitioners from fairly comparing different sparse models and obtaining realistic evaluation results. Another missing piece is, that a majority of prior work evaluates sparse retrieval models on in-domain retrieval, i.e. on a single dataset: MS MARCO. However, a key requirement in practical retrieval systems requires models that can generalize well to unseen out-of-domain, i.e. zero-shot retrieval tasks. In this work, we provide SPRINT, a unified Python toolkit based on Pyserini and Lucene, supporting a common interface for evaluating neural sparse retrieval. The 
&lt;/p&gt;</description></item><item><title>FinGPT&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#23558;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;&#37329;&#34701;&#25968;&#25454;&#27665;&#20027;&#21270;&#20026;&#37329;&#34701;&#22823;&#35821;&#35328;&#27169;&#22411;&#12290;&#23427;&#25552;&#20379;&#20102;&#33258;&#21160;&#25910;&#38598;&#21644;&#25972;&#29702;&#23454;&#26102;&#37329;&#34701;&#25968;&#25454;&#30340;&#21151;&#33021;&#65292;&#35299;&#20915;&#20102;&#37329;&#34701;&#25991;&#26412;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.10485</link><description>&lt;p&gt;
FinGPT: &#23558;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;&#37329;&#34701;&#25968;&#25454;&#27665;&#20027;&#21270;&#20026;&#37329;&#34701;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FinGPT: Democratizing Internet-scale Data for Financial Large Language Models. (arXiv:2307.10485v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10485
&lt;/p&gt;
&lt;p&gt;
FinGPT&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#23558;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;&#37329;&#34701;&#25968;&#25454;&#27665;&#20027;&#21270;&#20026;&#37329;&#34701;&#22823;&#35821;&#35328;&#27169;&#22411;&#12290;&#23427;&#25552;&#20379;&#20102;&#33258;&#21160;&#25910;&#38598;&#21644;&#25972;&#29702;&#23454;&#26102;&#37329;&#34701;&#25968;&#25454;&#30340;&#21151;&#33021;&#65292;&#35299;&#20915;&#20102;&#37329;&#34701;&#25991;&#26412;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#25991;&#26412;&#26041;&#38754;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#36825;&#21487;&#33021;&#20250;&#24443;&#24213;&#25913;&#21464;&#37329;&#34701;&#34892;&#19994;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;LLM&#22312;&#37329;&#34701;&#39046;&#22495;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#65292;&#20027;&#35201;&#21407;&#22240;&#26159;&#19968;&#33324;&#25991;&#26412;&#25968;&#25454;&#19982;&#37329;&#34701;&#25991;&#26412;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#29616;&#26377;&#30340;&#37329;&#34701;&#25991;&#26412;&#25968;&#25454;&#38598;&#25968;&#37327;&#26377;&#38480;&#65288;&#22823;&#23567;&#36739;&#23567;&#65289;&#65292;&#32780;&#31532;&#19968;&#20010;&#37329;&#34701;LLM&#65288;FinLLM&#65289;BloombergGPT&#26159;&#23553;&#38381;&#30340;&#65288;&#21482;&#21457;&#24067;&#20102;&#35757;&#32451;&#26085;&#24535;&#65289;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;Internet&#35268;&#27169;&#30340;&#37329;&#34701;&#25968;&#25454;&#23558;LLM&#27665;&#20027;&#21270;&#65292;&#30001;&#20110;&#25968;&#25454;&#26469;&#28304;&#22810;&#26679;&#12289;&#20449;&#22122;&#27604;&#20302;&#21644;&#26102;&#38388;&#26377;&#25928;&#24615;&#39640;&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#25918;&#24615;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#24320;&#28304;&#21644;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#26694;&#26550;&#8220;&#37329;&#34701;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;FinGPT&#65289;&#8221;&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#25910;&#38598;&#21644;&#25972;&#29702;&#26469;&#33258;&#20114;&#32852;&#32593;&#19978;&#36229;&#36807;34&#20010;&#19981;&#21516;&#26469;&#28304;&#30340;&#23454;&#26102;&#37329;&#34701;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable proficiency in understanding and generating human-like texts, which may potentially revolutionize the finance industry. However, existing LLMs often fall short in the financial field, which is mainly attributed to the disparities between general text data and financial text data. Unfortunately, there is only a limited number of financial text datasets available (quite small size), and BloombergGPT, the first financial LLM (FinLLM), is close-sourced (only the training logs were released). In light of this, we aim to democratize Internet-scale financial data for LLMs, which is an open challenge due to diverse data sources, low signal-to-noise ratio, and high time-validity. To address the challenges, we introduce an open-sourced and data-centric framework, \textit{Financial Generative Pre-trained Transformer (FinGPT)}, that automates the collection and curation of real-time financial data from &gt;34 diverse sources on the Internet, p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#27844;&#38706;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#21024;&#38500;&#26368;&#23481;&#26131;&#34987;&#25552;&#21462;&#30340;&#29992;&#25143;&#25968;&#25454;&#21518;&#65292;&#26032;&#30340;&#25968;&#25454;&#28857;&#20173;&#28982;&#23481;&#26131;&#34987;&#25552;&#21462;&#12290;&#31934;&#35843;&#27169;&#22411;&#19981;&#20165;&#27844;&#38706;&#35757;&#32451;&#25968;&#25454;&#65292;&#36824;&#20250;&#27844;&#38706;&#39044;&#35757;&#32451;&#38454;&#27573;&#35760;&#24518;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#21644;&#20010;&#20154;&#36523;&#20221;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2307.10476</link><description>&lt;p&gt;
&#25968;&#25454;&#27844;&#38706;&#21644;&#36951;&#24536;&#23545;&#27861;&#24459;&#30340;&#21551;&#31034;
&lt;/p&gt;
&lt;p&gt;
What can we learn from Data Leakage and Unlearning for Law?. (arXiv:2307.10476v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10476
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#27844;&#38706;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#21024;&#38500;&#26368;&#23481;&#26131;&#34987;&#25552;&#21462;&#30340;&#29992;&#25143;&#25968;&#25454;&#21518;&#65292;&#26032;&#30340;&#25968;&#25454;&#28857;&#20173;&#28982;&#23481;&#26131;&#34987;&#25552;&#21462;&#12290;&#31934;&#35843;&#27169;&#22411;&#19981;&#20165;&#27844;&#38706;&#35757;&#32451;&#25968;&#25454;&#65292;&#36824;&#20250;&#27844;&#38706;&#39044;&#35757;&#32451;&#38454;&#27573;&#35760;&#24518;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#21644;&#20010;&#20154;&#36523;&#20221;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#38544;&#31169;&#38382;&#39064;&#65292;&#20250;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#27844;&#38706;&#35757;&#32451;&#25968;&#25454;&#65288;&#21253;&#25324;&#20010;&#20154;&#36523;&#20221;&#20449;&#24687;&#22914;&#30005;&#23376;&#37038;&#20214;&#21644;&#30005;&#35805;&#21495;&#30721;&#65289;&#12290;&#20026;&#20102;&#36981;&#23432;&#38544;&#31169;&#27861;&#24459;&#65292;&#21487;&#20197;&#21024;&#38500;&#26368;&#23481;&#26131;&#34987;&#25552;&#21462;&#30340;&#29992;&#25143;&#25968;&#25454;&#12290;&#28982;&#32780;&#25105;&#20204;&#21457;&#29616;&#65292;&#19968;&#26086;&#21024;&#38500;&#20102;&#26368;&#23481;&#26131;&#34987;&#25552;&#21462;&#30340;&#25968;&#25454;&#65292;&#19968;&#32452;&#26032;&#30340;&#25968;&#25454;&#28857;&#23601;&#20250;&#21464;&#24471;&#23481;&#26131;&#34987;&#25552;&#21462;&#12290;&#30446;&#21069;&#23545;&#20110;&#31934;&#35843;&#27169;&#22411;&#30340;&#35760;&#24518;&#24615;&#23578;&#26410;&#24341;&#36215;&#36275;&#22815;&#30340;&#20851;&#27880;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#31934;&#35843;&#27169;&#22411;&#19981;&#20165;&#27844;&#38706;&#20854;&#35757;&#32451;&#25968;&#25454;&#65292;&#32780;&#19988;&#36824;&#20250;&#27844;&#38706;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#35760;&#24518;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#65288;&#20197;&#21450;&#20010;&#20154;&#36523;&#20221;&#20449;&#24687;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have a privacy concern because they memorize training data (including personally identifiable information (PII) like emails and phone numbers) and leak it during inference. A company can train an LLM on its domain-customized data which can potentially also include their users' PII. In order to comply with privacy laws such as the "right to be forgotten", the data points of users that are most vulnerable to extraction could be deleted. We find that once the most vulnerable points are deleted, a new set of points become vulnerable to extraction. So far, little attention has been given to understanding memorization for fine-tuned models. In this work, we also show that not only do fine-tuned models leak their training data but they also leak the pre-training data (and PII) memorized during the pre-training phase. The property of new data points becoming vulnerable to extraction after unlearning and leakage of pre-training data through fine-tuned models can pos
&lt;/p&gt;</description></item><item><title>Factify 2&#36827;&#34892;&#20102;&#19968;&#39033;&#22810;&#27169;&#24577;&#20551;&#26032;&#38395;&#26816;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#27604;&#36739;&#31038;&#20132;&#23186;&#20307;&#22768;&#26126;&#21644;&#25903;&#25345;&#25991;&#20214;&#30340;&#25991;&#26412;&#21644;&#22270;&#20687;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#23545;&#20551;&#26032;&#38395;&#30340;&#33258;&#21160;&#26816;&#27979;&#21644;&#39564;&#35777;&#65292;&#26368;&#20339;&#24615;&#33021;&#36798;&#21040;&#20102;81.82%&#30340;F1&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2307.10475</link><description>&lt;p&gt;
Factify 2&#35843;&#26597;&#25253;&#21578;: &#22810;&#27169;&#24577;&#20551;&#26032;&#38395;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Findings of Factify 2: Multimodal Fake News Detection. (arXiv:2307.10475v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10475
&lt;/p&gt;
&lt;p&gt;
Factify 2&#36827;&#34892;&#20102;&#19968;&#39033;&#22810;&#27169;&#24577;&#20551;&#26032;&#38395;&#26816;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#27604;&#36739;&#31038;&#20132;&#23186;&#20307;&#22768;&#26126;&#21644;&#25903;&#25345;&#25991;&#20214;&#30340;&#25991;&#26412;&#21644;&#22270;&#20687;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#23545;&#20551;&#26032;&#38395;&#30340;&#33258;&#21160;&#26816;&#27979;&#21644;&#39564;&#35777;&#65292;&#26368;&#20339;&#24615;&#33021;&#36798;&#21040;&#20102;81.82%&#30340;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31038;&#20132;&#23186;&#20307;&#30340;&#20351;&#29992;&#22312;&#36807;&#21435;&#20960;&#24180;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#20551;&#26032;&#38395;&#20063;&#21464;&#24471;&#38750;&#24120;&#26222;&#36941;&#12290;&#20551;&#26032;&#38395;&#30340;&#26377;&#23475;&#24433;&#21709;&#24378;&#35843;&#20102;&#30740;&#31350;&#33258;&#21160;&#26816;&#27979;&#38169;&#35823;&#20449;&#24687;&#24182;&#39564;&#35777;&#20854;&#20934;&#30830;&#24615;&#30340;&#38656;&#27714;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21576;&#29616;&#20102;Factify 2&#30340;&#32467;&#26524;&#65292;&#36825;&#26159;&#20316;&#20026;AAAI'23&#30340;DeFactify 2&#24037;&#20316;&#22346;&#30340;&#19968;&#37096;&#20998;&#25552;&#20379;&#30340;&#22810;&#27169;&#24577;&#20107;&#23454;&#39564;&#35777;&#21644;&#35773;&#21050;&#26032;&#38395;&#25968;&#25454;&#38598;&#12290;&#25968;&#25454;&#21628;&#21796;&#19968;&#31181;&#22522;&#20110;&#27604;&#36739;&#30340;&#26041;&#27861;&#26469;&#37197;&#23545;&#31038;&#20132;&#23186;&#20307;&#22768;&#26126;&#21644;&#25903;&#25345;&#25991;&#20214;&#65292;&#21253;&#25324;&#25991;&#26412;&#21644;&#22270;&#20687;&#65292;&#26681;&#25454;&#22810;&#27169;&#24577;&#20851;&#31995;&#20998;&#20026;5&#31867;&#12290;&#22312;&#36825;&#20010;&#20219;&#21153;&#30340;&#31532;&#20108;&#27425;&#36845;&#20195;&#20013;&#65292;&#25105;&#20204;&#26377;&#36229;&#36807;60&#20010;&#21442;&#19982;&#32773;&#21644;9&#20010;&#32456;&#26399;&#27979;&#35797;&#25552;&#20132;&#12290;&#26368;&#22909;&#30340;&#34920;&#29616;&#26469;&#33258;&#20110;&#22312;&#25991;&#26412;&#26041;&#38754;&#20351;&#29992;DeBERTa&#65292;&#22312;&#22270;&#20687;&#26041;&#38754;&#20351;&#29992;Swinv2&#21644;CLIP&#12290;&#25152;&#26377;&#20116;&#20010;&#31867;&#21035;&#30340;F1&#20998;&#25968;&#24179;&#22343;&#36798;&#21040;&#20102;81.82%&#12290;
&lt;/p&gt;
&lt;p&gt;
With social media usage growing exponentially in the past few years, fake news has also become extremely prevalent. The detrimental impact of fake news emphasizes the need for research focused on automating the detection of false information and verifying its accuracy. In this work, we present the outcome of the Factify 2 shared task, which provides a multi-modal fact verification and satire news dataset, as part of the DeFactify 2 workshop at AAAI'23. The data calls for a comparison based approach to the task by pairing social media claims with supporting documents, with both text and image, divided into 5 classes based on multi-modal relations. In the second iteration of this task we had over 60 participants and 9 final test-set submissions. The best performances came from the use of DeBERTa for text and Swinv2 and CLIP for image. The highest F1 score averaged for all five classes was 81.82%.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#38646;&#26679;&#26412;&#25552;&#31034;&#35780;&#20272;&#25351;&#20196;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#35782;&#21035;&#20559;&#35265;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;Alpaca 7B&#22312;&#20559;&#35265;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#26368;&#20339;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#25193;&#22823;&#27169;&#22411;&#22823;&#23567;&#21644;&#25968;&#25454;&#22810;&#26679;&#24615;&#21487;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.10472</link><description>&lt;p&gt;
&#36890;&#36807;&#25552;&#31034;&#65292;&#25351;&#20196;&#24494;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#35782;&#21035;&#31038;&#20250;&#20559;&#35265;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Instruction Fine-Tuned Language Models Identify Social Bias through Prompting?. (arXiv:2307.10472v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#38646;&#26679;&#26412;&#25552;&#31034;&#35780;&#20272;&#25351;&#20196;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#35782;&#21035;&#20559;&#35265;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;Alpaca 7B&#22312;&#20559;&#35265;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#26368;&#20339;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#25193;&#22823;&#27169;&#22411;&#22823;&#23567;&#21644;&#25968;&#25454;&#22810;&#26679;&#24615;&#21487;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#30340;&#24191;&#24230;&#21644;&#28145;&#24230;&#19981;&#26029;&#25193;&#23637;&#65292;&#26500;&#24314;&#26377;&#25928;&#30340;&#26694;&#26550;&#26469;&#34913;&#37327;&#21644;&#20943;&#36731;&#36825;&#20123;&#27169;&#22411;&#23398;&#20064;&#25110;&#32487;&#25215;&#30340;&#31038;&#20250;&#20559;&#35265;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#25351;&#20196;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#38646;&#26679;&#26412;&#25552;&#31034;&#65288;&#21253;&#25324;&#24605;&#32500;&#38142;&#25552;&#31034;&#65289;&#35782;&#21035;&#20559;&#35265;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#22312;LLaMA&#21450;&#20854;&#20004;&#20010;&#25351;&#20196;&#24494;&#35843;&#29256;&#26412;&#20013;&#65292;Alpaca 7B&#22312;&#20559;&#35265;&#35782;&#21035;&#20219;&#21153;&#20013;&#34920;&#29616;&#26368;&#22909;&#65292;&#20934;&#30830;&#29575;&#36798;56.7%&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25193;&#22823;&#35821;&#35328;&#27169;&#22411;&#22823;&#23567;&#21644;&#25968;&#25454;&#22810;&#26679;&#24615;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;&#36825;&#26159;&#25105;&#20204;&#20559;&#35265;&#32531;&#35299;&#26694;&#26550;&#30340;&#31532;&#19968;&#37096;&#20998;&#65292;&#27491;&#22312;&#36827;&#34892;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#23558;&#26681;&#25454;&#33719;&#24471;&#30340;&#26356;&#22810;&#32467;&#26524;&#19981;&#26029;&#26356;&#26032;&#26412;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the breadth and depth of language model applications continue to expand rapidly, it is increasingly important to build efficient frameworks for measuring and mitigating the learned or inherited social biases of these models. In this paper, we present our work on evaluating instruction fine-tuned language models' ability to identify bias through zero-shot prompting, including Chain-of-Thought (CoT) prompts. Across LLaMA and its two instruction fine-tuned versions, Alpaca 7B performs best on the bias identification task with an accuracy of 56.7%. We also demonstrate that scaling up LLM size and data diversity could lead to further performance gain. This is a work-in-progress presenting the first component of our bias mitigation framework. We will keep updating this work as we get more results.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Mask-tuning&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;Masked Language Modeling (MLM)&#35757;&#32451;&#30446;&#26631;&#25972;&#21512;&#21040;&#24494;&#35843;&#36807;&#31243;&#20013;&#26469;&#22686;&#24378;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;Mask-tuning&#22312;&#38750;&#20998;&#24067;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#65292;&#24182;&#25552;&#39640;&#20102;PLMs&#22312;&#20998;&#24067;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.10457</link><description>&lt;p&gt;
&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Pre-trained Language Models' Generalization. (arXiv:2307.10457v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10457
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Mask-tuning&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;Masked Language Modeling (MLM)&#35757;&#32451;&#30446;&#26631;&#25972;&#21512;&#21040;&#24494;&#35843;&#36807;&#31243;&#20013;&#26469;&#22686;&#24378;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;Mask-tuning&#22312;&#38750;&#20998;&#24067;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#65292;&#24182;&#25552;&#39640;&#20102;PLMs&#22312;&#20998;&#24067;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#21487;&#37325;&#22797;&#20351;&#29992;&#24615;&#36890;&#24120;&#21463;&#21040;&#20854;&#27867;&#21270;&#38382;&#39064;&#30340;&#38480;&#21046;&#65292;&#21363;&#24403;&#22312;&#19982;&#35757;&#32451;&#25968;&#25454;&#38598;&#19981;&#21516;&#30340;&#31034;&#20363;&#19978;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;&#20854;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#65292;&#36825;&#31181;&#31034;&#20363;&#34987;&#31216;&#20026;&#8220;&#38750;&#20998;&#24067;/&#26410;&#35265;&#31034;&#20363;&#8221;&#12290;&#36825;&#19968;&#38480;&#21046;&#28304;&#20110;PLMs&#23545;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#20381;&#36182;&#65292;&#34394;&#20551;&#30456;&#20851;&#24615;&#23545;&#20110;&#24120;&#35265;&#31034;&#20363;&#31867;&#22411;&#25928;&#26524;&#33391;&#22909;&#65292;&#20294;&#23545;&#20110;&#19968;&#33324;&#31034;&#20363;&#25928;&#26524;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Mask-tuning&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#36974;&#34109;&#35821;&#35328;&#24314;&#27169;&#65288;MLM&#65289;&#35757;&#32451;&#30446;&#26631;&#25972;&#21512;&#21040;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;&#20197;&#22686;&#24378;PLMs&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;Mask-tuning&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#65292;&#24182;&#22686;&#24378;&#20102;PLMs&#23545;&#38750;&#20998;&#24067;&#25968;&#25454;&#38598;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#23427;&#20204;&#22312;&#20998;&#24067;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;Mask-tuning&#25552;&#39640;&#20102;PLMs&#22312;&#26410;&#35265;&#25968;&#25454;&#19978;&#30340;&#21487;&#37325;&#22797;&#20351;&#29992;&#24615;&#65292;&#20351;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#26356;&#21152;&#23454;&#29992;&#21644;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
The reusability of state-of-the-art Pre-trained Language Models (PLMs) is often limited by their generalization problem, where their performance drastically decreases when evaluated on examples that differ from the training dataset, known as Out-of-Distribution (OOD)/unseen examples. This limitation arises from PLMs' reliance on spurious correlations, which work well for frequent example types but not for general examples. To address this issue, we propose a training approach called Mask-tuning, which integrates Masked Language Modeling (MLM) training objectives into the fine-tuning process to enhance PLMs' generalization. Comprehensive experiments demonstrate that Mask-tuning surpasses current state-of-the-art techniques and enhances PLMs' generalization on OOD datasets while improving their performance on in-distribution datasets. The findings suggest that Mask-tuning improves the reusability of PLMs on unseen data, making them more practical and effective for real-world applications
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#20351;&#29992;&#22270;&#22686;&#24378;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#23558;&#20174;&#24322;&#26500;&#22270;&#20013;&#23548;&#20986;&#30340;&#25512;&#29702;&#30693;&#35782;&#25972;&#21512;&#21040;&#21464;&#21387;&#22120;&#26550;&#26500;&#20013;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#21464;&#21387;&#22120;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#38480;&#21046;&#12290;&#36890;&#36807;&#20840;&#23616;-&#23616;&#37096;&#27880;&#24847;&#21147;&#12289;&#22270;&#27880;&#24847;&#21147;&#21644;&#20851;&#31995;&#31867;&#22411;&#32771;&#34385;&#65292;&#20248;&#21270;&#20102;&#23454;&#20307;&#21644;&#21333;&#35789;&#20043;&#38388;&#30340;&#27880;&#24847;&#21147;&#12290;&#35813;&#27169;&#24335;&#19982;&#30456;&#23545;&#20301;&#32622;&#26631;&#31614;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#19982;LUKE&#30340;&#23454;&#20307;&#24863;&#30693;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30456;&#38598;&#25104;&#12290;</title><link>http://arxiv.org/abs/2307.10443</link><description>&lt;p&gt;
&#20351;&#29992;&#30456;&#23545;&#20301;&#32622;&#26631;&#31614;&#23558;&#24322;&#26500;&#22270;&#19982;&#23454;&#20307;&#24863;&#30693;&#33258;&#27880;&#24847;&#21147;&#30456;&#32467;&#21512;&#30340;&#38405;&#35835;&#29702;&#35299;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Integrating a Heterogeneous Graph with Entity-aware Self-attention using Relative Position Labels for Reading Comprehension Model. (arXiv:2307.10443v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#20351;&#29992;&#22270;&#22686;&#24378;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#23558;&#20174;&#24322;&#26500;&#22270;&#20013;&#23548;&#20986;&#30340;&#25512;&#29702;&#30693;&#35782;&#25972;&#21512;&#21040;&#21464;&#21387;&#22120;&#26550;&#26500;&#20013;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#21464;&#21387;&#22120;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#38480;&#21046;&#12290;&#36890;&#36807;&#20840;&#23616;-&#23616;&#37096;&#27880;&#24847;&#21147;&#12289;&#22270;&#27880;&#24847;&#21147;&#21644;&#20851;&#31995;&#31867;&#22411;&#32771;&#34385;&#65292;&#20248;&#21270;&#20102;&#23454;&#20307;&#21644;&#21333;&#35789;&#20043;&#38388;&#30340;&#27880;&#24847;&#21147;&#12290;&#35813;&#27169;&#24335;&#19982;&#30456;&#23545;&#20301;&#32622;&#26631;&#31614;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#19982;LUKE&#30340;&#23454;&#20307;&#24863;&#30693;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30456;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21464;&#21387;&#22120;&#27169;&#22411;&#22312;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#36755;&#20837;&#24207;&#21015;&#20013;&#32570;&#23569;&#26174;&#24335;&#30693;&#35782;&#65292;&#23427;&#20204;&#20173;&#28982;&#38754;&#20020;&#22788;&#29702;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#26469;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#23427;&#21033;&#29992;&#22686;&#24378;&#22270;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#23558;&#30001;&#24322;&#26500;&#22270;&#23548;&#20986;&#30340;&#25512;&#29702;&#30693;&#35782;&#25972;&#21512;&#21040;&#21464;&#21387;&#22120;&#26550;&#26500;&#20013;&#12290;&#25552;&#20986;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#35201;&#32032;&#65306;&#21333;&#35789;&#26631;&#35760;&#30340;&#20840;&#23616;-&#23616;&#37096;&#27880;&#24847;&#21147;&#65292;&#23545;&#23454;&#20307;&#26631;&#35760;&#30340;&#22270;&#27880;&#24847;&#21147;&#65292;&#23454;&#20307;&#26631;&#35760;&#23545;&#30456;&#20851;&#32852;&#30340;&#26631;&#35760;&#26174;&#31034;&#24378;&#28872;&#30340;&#27880;&#24847;&#21147;&#32780;&#23545;&#19981;&#30456;&#20851;&#30340;&#26631;&#35760;&#26174;&#31034;&#36739;&#24369;&#30340;&#27880;&#24847;&#21147;&#65292;&#20197;&#21450;&#32771;&#34385;&#27599;&#20010;&#23454;&#20307;&#26631;&#35760;&#19982;&#21333;&#35789;&#26631;&#35760;&#20043;&#38388;&#30340;&#20851;&#31995;&#31867;&#22411;&#12290;&#36825;&#26679;&#65292;&#22914;&#26524;&#23384;&#22312;&#20851;&#31995;&#65292;&#21017;&#21487;&#20197;&#20248;&#21270;&#20004;&#32773;&#20043;&#38388;&#30340;&#27880;&#24847;&#21147;&#12290;&#35813;&#27169;&#24335;&#19982;&#29305;&#27530;&#30340;&#30456;&#23545;&#20301;&#32622;&#26631;&#31614;&#30456;&#32467;&#21512;&#65292;&#20351;&#20854;&#33021;&#22815;&#19982;LUKE&#30340;&#23454;&#20307;&#24863;&#30693;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30456;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the significant progress made by transformer models in machine reading comprehension tasks, they still face limitations in handling complex reasoning tasks due to the absence of explicit knowledge in the input sequence. This paper proposes a novel attention pattern to overcome this limitation, which integrates reasoning knowledge derived from a heterogeneous graph into the transformer architecture using a graph-enhanced self-attention mechanism. The proposed attention pattern comprises three key elements: global-local attention for word tokens, graph attention for entity tokens that exhibit strong attention towards tokens connected in the graph as opposed to those unconnected, and the consideration of the type of relationship between each entity token and word token. This results in optimized attention between the two if a relationship exists. The pattern is coupled with special relative position labels, allowing it to integrate with LUKE's entity-aware self-attention mechanism
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#20363;&#32423;&#30340;&#33258;&#36866;&#24212;&#25512;&#21160;&#22806;&#37096;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#34913;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#27700;&#24179;&#65292;&#24182;&#21033;&#29992;Thrust&#25351;&#26631;&#36827;&#34892;&#20449;&#24687;&#26816;&#32034;&#65292;&#23454;&#29616;&#26356;&#39640;&#30340;&#25104;&#26412;&#25928;&#30410;&#12290;</title><link>http://arxiv.org/abs/2307.10442</link><description>&lt;p&gt;
Thrust: &#29992;&#22806;&#37096;&#30693;&#35782;&#33258;&#36866;&#24212;&#25512;&#21160;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Thrust: Adaptively Propels Large Language Models with External Knowledge. (arXiv:2307.10442v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#20363;&#32423;&#30340;&#33258;&#36866;&#24212;&#25512;&#21160;&#22806;&#37096;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#34913;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#27700;&#24179;&#65292;&#24182;&#21033;&#29992;Thrust&#25351;&#26631;&#36827;&#34892;&#20449;&#24687;&#26816;&#32034;&#65292;&#23454;&#29616;&#26356;&#39640;&#30340;&#25104;&#26412;&#25928;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PTLM&#65289;&#24050;&#34987;&#35777;&#26126;&#22312;&#20854;&#27169;&#22411;&#21442;&#25968;&#20013;&#32534;&#30721;&#20102;&#20016;&#23500;&#30340;&#30693;&#35782;&#65292;&#20294;PTLM&#20013;&#30340;&#20869;&#22312;&#30693;&#35782;&#21487;&#33021;&#26159;&#19981;&#36879;&#26126;&#25110;&#38745;&#24577;&#30340;&#65292;&#22240;&#27492;&#38656;&#35201;&#22806;&#37096;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20449;&#24687;&#26816;&#32034;&#25216;&#26415;&#21487;&#33021;&#25104;&#26412;&#39640;&#26114;&#65292;&#29978;&#33267;&#21487;&#33021;&#24341;&#20837;&#22122;&#38899;&#21644;&#35823;&#23548;&#24615;&#30693;&#35782;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23454;&#20363;&#32423;&#30340;&#33258;&#36866;&#24212;&#25512;&#21160;&#22806;&#37096;&#30693;&#35782;&#65288;IAPEK&#65289;&#65292;&#21482;&#26377;&#22312;&#24517;&#35201;&#26102;&#25165;&#36827;&#34892;&#26816;&#32034;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;Thrust&#65292;&#21033;&#29992;&#23569;&#37327;&#24050;&#35265;&#23454;&#20363;&#30340;&#34920;&#31034;&#20998;&#24067;&#26469;&#34913;&#37327;PTLM&#26159;&#21542;&#21253;&#21547;&#36275;&#22815;&#30340;&#30693;&#35782;&#26469;&#35299;&#20915;&#19968;&#20010;&#23454;&#20363;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;Thrust&#26159;&#34913;&#37327;PTLM&#27169;&#22411;&#23454;&#20363;&#32423;&#30693;&#35782;&#33021;&#21147;&#30340;&#33391;&#22909;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;Thrust&#20998;&#25968;&#20316;&#20026;&#26816;&#32034;&#25351;&#26631;&#21487;&#20197;&#23454;&#29616;&#26174;&#33879;&#30340;&#25104;&#26412;&#25928;&#30410;&#65292;&#39640;&#20110;&#23545;&#22806;&#37096;&#30693;&#35782;&#30340;&#26420;&#32032;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although large-scale pre-trained language models (PTLMs) are shown to encode rich knowledge in their model parameters, the inherent knowledge in PTLMs can be opaque or static, making external knowledge necessary. However, the existing information retrieval techniques could be costly and may even introduce noisy and sometimes misleading knowledge. To address these challenges, we propose the instance-level adaptive propulsion of external knowledge (IAPEK), where we only conduct the retrieval when necessary. To achieve this goal, we propose measuring whether a PTLM contains enough knowledge to solve an instance with a novel metric, Thrust, which leverages the representation distribution of a small number of seen instances. Extensive experiments demonstrate that thrust is a good measurement of PTLM models' instance-level knowledgeability. Moreover, we can achieve significantly higher cost-efficiency with the Thrust score as the retrieval indicator than the naive usage of external knowledge
&lt;/p&gt;</description></item><item><title>PharmacyGPT&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#20223;&#30495;&#20020;&#24202;&#33647;&#24072;&#30340;&#35282;&#33394;&#12290;&#36890;&#36807;&#29983;&#25104;&#24739;&#32773;&#32676;&#38598;&#12289;&#21046;&#23450;&#29992;&#33647;&#35745;&#21010;&#21644;&#39044;&#27979;&#24739;&#32773;&#32467;&#26524;&#65292;PharmacyGPT&#22312;&#20020;&#24202;&#33647;&#23398;&#20013;&#20855;&#26377;&#28508;&#22312;&#24212;&#29992;&#21644;&#38480;&#21046;&#65292;&#20026;&#20419;&#36827;&#36127;&#36131;&#20219;&#21644;&#26377;&#25928;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#20570;&#20986;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2307.10432</link><description>&lt;p&gt;
PharmacyGPT&#65306;AI&#33647;&#24072;
&lt;/p&gt;
&lt;p&gt;
PharmacyGPT: The AI Pharmacist. (arXiv:2307.10432v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10432
&lt;/p&gt;
&lt;p&gt;
PharmacyGPT&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#20223;&#30495;&#20020;&#24202;&#33647;&#24072;&#30340;&#35282;&#33394;&#12290;&#36890;&#36807;&#29983;&#25104;&#24739;&#32773;&#32676;&#38598;&#12289;&#21046;&#23450;&#29992;&#33647;&#35745;&#21010;&#21644;&#39044;&#27979;&#24739;&#32773;&#32467;&#26524;&#65292;PharmacyGPT&#22312;&#20020;&#24202;&#33647;&#23398;&#20013;&#20855;&#26377;&#28508;&#22312;&#24212;&#29992;&#21644;&#38480;&#21046;&#65292;&#20026;&#20419;&#36827;&#36127;&#36131;&#20219;&#21644;&#26377;&#25928;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;PharmacyGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65288;&#22914;ChatGPT&#21644;GPT-4&#65289;&#22312;&#20223;&#30495;&#20020;&#24202;&#33647;&#24072;&#35282;&#33394;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#21033;&#29992;LLM&#29983;&#25104;&#21487;&#29702;&#35299;&#30340;&#24739;&#32773;&#32676;&#38598;&#12289;&#21046;&#23450;&#29992;&#33647;&#35745;&#21010;&#21644;&#39044;&#27979;&#24739;&#32773;&#32467;&#26524;&#12290;&#25105;&#20204;&#20351;&#29992;&#20174;&#21271;&#21345;&#32599;&#26469;&#32435;&#22823;&#23398;&#25945;&#22530;&#23665;&#21307;&#38498;&#65288;UNC&#65289;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;&#65288;ICU&#65289;&#33719;&#21462;&#30340;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#35843;&#26597;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#23545;LLM&#22312;&#20020;&#24202;&#33647;&#23398;&#39046;&#22495;&#28508;&#22312;&#24212;&#29992;&#21644;&#38480;&#21046;&#30340;&#26377;&#20215;&#20540;&#35265;&#35299;&#65292;&#23545;&#24739;&#32773;&#25252;&#29702;&#21644;&#26410;&#26469;&#22522;&#20110;AI&#30340;&#21307;&#30103;&#35299;&#20915;&#26041;&#26696;&#30340;&#24320;&#21457;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#36890;&#36807;&#35780;&#20272;PharmacyGPT&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#26088;&#22312;&#20026;&#26377;&#20851;&#22312;&#21307;&#30103;&#20445;&#20581;&#29615;&#22659;&#20013;&#25972;&#21512;&#20154;&#24037;&#26234;&#33021;&#30340;&#25345;&#32493;&#35752;&#35770;&#20570;&#20986;&#36129;&#29486;&#65292;&#26368;&#32456;&#20419;&#36827;&#36127;&#36131;&#20219;&#21644;&#26377;&#25928;&#20351;&#29992;&#27492;&#31867;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we introduce PharmacyGPT, a novel framework to assess the capabilities of large language models (LLMs) such as ChatGPT and GPT-4 in emulating the role of clinical pharmacists. Our methodology encompasses the utilization of LLMs to generate comprehensible patient clusters, formulate medication plans, and forecast patient outcomes. We conduct our investigation using real data acquired from the intensive care unit (ICU) at the University of North Carolina Chapel Hill (UNC) Hospital. Our analysis offers valuable insights into the potential applications and limitations of LLMs in the field of clinical pharmacy, with implications for both patient care and the development of future AI-driven healthcare solutions. By evaluating the performance of PharmacyGPT, we aim to contribute to the ongoing discourse surrounding the integration of artificial intelligence in healthcare settings, ultimately promoting the responsible and efficacious use of such technologies.
&lt;/p&gt;</description></item><item><title>IncDSI&#26159;&#19968;&#31181;&#36882;&#22686;&#21487;&#26356;&#26032;&#30340;&#25991;&#26723;&#26816;&#32034;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#26368;&#23567;&#25913;&#21464;&#32593;&#32476;&#21442;&#25968;&#30340;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#65292;&#23454;&#29616;&#23454;&#26102;&#28155;&#21152;&#25991;&#26723;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25972;&#20010;&#27169;&#22411;&#65292;&#20855;&#26377;&#19982;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#30456;&#31454;&#20105;&#30340;&#36895;&#24230;&#65292;&#33021;&#22815;&#23454;&#26102;&#26356;&#26032;&#30340;&#25991;&#26723;&#26816;&#32034;&#31995;&#32479;&#30340;&#24320;&#21457;&#12290;</title><link>http://arxiv.org/abs/2307.10323</link><description>&lt;p&gt;
IncDSI&#65306;&#36882;&#22686;&#21487;&#26356;&#26032;&#30340;&#25991;&#26723;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
IncDSI: Incrementally Updatable Document Retrieval. (arXiv:2307.10323v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10323
&lt;/p&gt;
&lt;p&gt;
IncDSI&#26159;&#19968;&#31181;&#36882;&#22686;&#21487;&#26356;&#26032;&#30340;&#25991;&#26723;&#26816;&#32034;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#26368;&#23567;&#25913;&#21464;&#32593;&#32476;&#21442;&#25968;&#30340;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#65292;&#23454;&#29616;&#23454;&#26102;&#28155;&#21152;&#25991;&#26723;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25972;&#20010;&#27169;&#22411;&#65292;&#20855;&#26377;&#19982;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#30456;&#31454;&#20105;&#30340;&#36895;&#24230;&#65292;&#33021;&#22815;&#23454;&#26102;&#26356;&#26032;&#30340;&#25991;&#26723;&#26816;&#32034;&#31995;&#32479;&#30340;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;iable&#25628;&#32034;&#32034;&#24341;&#26159;&#26368;&#36817;&#25552;&#20986;&#30340;&#19968;&#31181;&#25991;&#26723;&#26816;&#32034;&#33539;&#20363;&#65292;&#23427;&#23558;&#25991;&#26723;&#35821;&#26009;&#24211;&#30340;&#20449;&#24687;&#32534;&#30721;&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;&#21442;&#25968;&#20013;&#65292;&#24182;&#30452;&#25509;&#23558;&#26597;&#35810;&#26144;&#23556;&#21040;&#30456;&#24212;&#30340;&#25991;&#26723;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#35768;&#22810;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#19968;&#20010;&#37325;&#35201;&#38480;&#21046;&#65306;&#22312;&#35757;&#32451;&#27169;&#22411;&#20043;&#21518;&#28155;&#21152;&#26032;&#25991;&#26723;&#24182;&#19981;&#23481;&#26131;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;IncDSI&#65292;&#19968;&#31181;&#23454;&#26102;&#28155;&#21152;&#25991;&#26723;&#30340;&#26041;&#27861;&#65288;&#27599;&#20010;&#25991;&#26723;&#32422;20-50&#27627;&#31186;&#65289;&#65292;&#32780;&#26080;&#38656;&#23545;&#25972;&#20010;&#25968;&#25454;&#38598;&#65288;&#29978;&#33267;&#37096;&#20998;&#25968;&#25454;&#38598;&#65289;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#23558;&#28155;&#21152;&#25991;&#26723;&#30340;&#36807;&#31243;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#22312;&#32593;&#32476;&#21442;&#25968;&#19978;&#36827;&#34892;&#26368;&#23567;&#25913;&#21464;&#30340;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#12290;&#34429;&#28982;&#36895;&#24230;&#26356;&#24555;&#20960;&#20010;&#25968;&#37327;&#32423;&#65292;&#20294;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#22312;&#25972;&#20010;&#25968;&#25454;&#38598;&#19978;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#30456;&#31454;&#20105;&#65292;&#24182;&#19988;&#21487;&#20197;&#23454;&#26102;&#26356;&#26032;&#30340;&#25991;&#26723;&#26816;&#32034;&#31995;&#32479;&#30340;&#24320;&#21457;&#12290;&#25105;&#20204;&#30340;IncDSI&#20195;&#30721;
&lt;/p&gt;
&lt;p&gt;
Differentiable Search Index is a recently proposed paradigm for document retrieval, that encodes information about a corpus of documents within the parameters of a neural network and directly maps queries to corresponding documents. These models have achieved state-of-the-art performances for document retrieval across many benchmarks. These kinds of models have a significant limitation: it is not easy to add new documents after a model is trained. We propose IncDSI, a method to add documents in real time (about 20-50ms per document), without retraining the model on the entire dataset (or even parts thereof). Instead we formulate the addition of documents as a constrained optimization problem that makes minimal changes to the network parameters. Although orders of magnitude faster, our approach is competitive with re-training the model on the whole dataset and enables the development of document retrieval systems that can be updated with new information in real-time. Our code for IncDSI
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#23391;&#21152;&#25289;&#27468;&#26354;&#30340;&#27468;&#35789;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#23545;&#36825;&#20123;&#27468;&#26354;&#30340;&#24773;&#32490;&#36827;&#34892;&#22810;&#31867;&#20998;&#31867;&#65292;&#21253;&#25324;&#24555;&#20048;&#12289;&#24754;&#20260;&#12289;&#28010;&#28459;&#21644;&#25918;&#26494;&#65292;&#20026;&#20351;&#38899;&#20048;&#26356;&#36148;&#36817;&#20154;&#20204;&#30340;&#24773;&#24863;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2307.10314</link><description>&lt;p&gt;
&#22522;&#20110;&#27468;&#35789;&#30340;&#23391;&#21152;&#25289;&#27468;&#26354;&#24773;&#32490;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Mood Classification of Bangla Songs Based on Lyrics. (arXiv:2307.10314v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10314
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#23391;&#21152;&#25289;&#27468;&#26354;&#30340;&#27468;&#35789;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#23545;&#36825;&#20123;&#27468;&#26354;&#30340;&#24773;&#32490;&#36827;&#34892;&#22810;&#31867;&#20998;&#31867;&#65292;&#21253;&#25324;&#24555;&#20048;&#12289;&#24754;&#20260;&#12289;&#28010;&#28459;&#21644;&#25918;&#26494;&#65292;&#20026;&#20351;&#38899;&#20048;&#26356;&#36148;&#36817;&#20154;&#20204;&#30340;&#24773;&#24863;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#20048;&#33021;&#21796;&#36215;&#21508;&#31181;&#24773;&#32490;&#65292;&#38543;&#30528;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#20154;&#20204;&#23545;&#38899;&#20048;&#30340;&#25509;&#35302;&#20063;&#36234;&#26469;&#36234;&#22810;&#12290;&#28982;&#32780;&#23545;&#20110;&#23637;&#29616;&#19981;&#21516;&#20154;&#31867;&#24773;&#24863;&#30340;&#23391;&#21152;&#25289;&#38899;&#20048;&#65292;&#30456;&#20851;&#30340;&#30740;&#31350;&#23578;&#19981;&#36275;&#22815;&#12290;&#26412;&#25991;&#30340;&#20316;&#32773;&#26088;&#22312;&#36890;&#36807;&#20998;&#26512;&#23391;&#21152;&#25289;&#27468;&#26354;&#30340;&#27468;&#35789;&#26469;&#20998;&#31867;&#20854;&#24773;&#32490;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#30740;&#31350;&#20154;&#21592;&#25910;&#38598;&#20102;4000&#39318;&#23391;&#21152;&#25289;&#27468;&#26354;&#30340;&#27468;&#35789;&#21644;&#27969;&#27966;&#65292;&#24182;&#36816;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;BERT&#31639;&#27861;&#26469;&#20998;&#26512;&#25968;&#25454;&#12290;&#22312;&#36825;4000&#39318;&#27468;&#26354;&#20013;&#65292;1513&#39318;&#20195;&#34920;&#24754;&#20260;&#24773;&#32490;&#65292;1362&#39318;&#20195;&#34920;&#28010;&#28459;&#24773;&#32490;&#65292;886&#39318;&#20195;&#34920;&#24555;&#20048;&#65292;&#20854;&#20313;&#30340;239&#39318;&#34987;&#24402;&#31867;&#20026;&#25918;&#26494;&#12290;&#36890;&#36807;&#23884;&#20837;&#27468;&#35789;&#65292;&#20316;&#32773;&#23558;&#36825;&#20123;&#27468;&#26354;&#20998;&#20026;&#22235;&#31181;&#24773;&#32490;&#65306;&#24555;&#20048;&#12289;&#24754;&#20260;&#12289;&#28010;&#28459;&#21644;&#25918;&#26494;&#12290;&#35813;&#30740;&#31350;&#23545;&#20110;&#23454;&#29616;&#38899;&#20048;&#30340;&#22810;&#31867;&#24773;&#32490;&#20998;&#31867;&#33267;&#20851;&#37325;&#35201;&#65292;&#20351;&#38899;&#20048;&#26356;&#33021;&#19982;&#20154;&#20204;&#30340;&#24773;&#24863;&#20135;&#29983;&#20849;&#40483;&#12290;&#35813;&#25991;&#31456;&#35814;&#32454;&#25551;&#36848;&#20102;&#36890;&#36807;&#27468;&#35789;&#20934;&#30830;&#25512;&#23548;&#20986;&#30340;&#22235;&#31181;&#24773;&#32490;&#30340;&#33258;&#21160;&#21270;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Music can evoke various emotions, and with the advancement of technology, it has become more accessible to people. Bangla music, which portrays different human emotions, lacks sufficient research. The authors of this article aim to analyze Bangla songs and classify their moods based on the lyrics. To achieve this, this research has compiled a dataset of 4000 Bangla song lyrics, genres, and used Natural Language Processing and the Bert Algorithm to analyze the data. Among the 4000 songs, 1513 songs are represented for the sad mood, 1362 for the romantic mood, 886 for happiness, and the rest 239 are classified as relaxation. By embedding the lyrics of the songs, the authors have classified the songs into four moods: Happy, Sad, Romantic, and Relaxed. This research is crucial as it enables a multi-class classification of songs' moods, making the music more relatable to people's emotions. The article presents the automated result of the four moods accurately derived from the song lyrics.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#22810;&#31181;&#20307;&#32946;&#36187;&#20107;&#30340;&#23454;&#26102;&#35780;&#35770;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#33258;&#21160;&#35782;&#21035;&#20027;&#35201;&#34892;&#21160;&#65292;&#24182;&#36890;&#36807;&#20998;&#31867;&#21644;&#24773;&#24863;&#20998;&#26512;&#25552;&#21462;&#27934;&#35265;&#12290;</title><link>http://arxiv.org/abs/2307.10303</link><description>&lt;p&gt;
&#20998;&#26512;&#20307;&#32946;&#35780;&#35770;&#20197;&#23454;&#29616;&#33258;&#21160;&#35782;&#21035;&#20107;&#20214;&#24182;&#25552;&#21462;&#27934;&#35265;
&lt;/p&gt;
&lt;p&gt;
Analyzing sports commentary in order to automatically recognize events and extract insights. (arXiv:2307.10303v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10303
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#22810;&#31181;&#20307;&#32946;&#36187;&#20107;&#30340;&#23454;&#26102;&#35780;&#35770;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#33258;&#21160;&#35782;&#21035;&#20027;&#35201;&#34892;&#21160;&#65292;&#24182;&#36890;&#36807;&#20998;&#31867;&#21644;&#24773;&#24863;&#20998;&#26512;&#25552;&#21462;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20180;&#32454;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#22810;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#21644;&#26041;&#27861;&#65292;&#20197;&#33258;&#21160;&#35782;&#21035;&#20307;&#32946;&#36187;&#20107;&#20013;&#30340;&#20027;&#35201;&#34892;&#21160;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#19981;&#21516;&#26469;&#28304;&#30340;&#29616;&#22330;&#20307;&#32946;&#35780;&#35770;&#65292;&#24182;&#23558;&#36825;&#20123;&#20027;&#35201;&#34892;&#21160;&#20998;&#31867;&#21040;&#19981;&#21516;&#30340;&#31867;&#21035;&#20013;&#65292;&#26469;&#25552;&#21462;&#27934;&#35265;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#24773;&#24863;&#20998;&#26512;&#26159;&#21542;&#21487;&#20197;&#24110;&#21161;&#26816;&#27979;&#36825;&#20123;&#20027;&#35201;&#34892;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we carefully investigate how we can use multiple different Natural Language Processing techniques and methods in order to automatically recognize the main actions in sports events. We aim to extract insights by analyzing live sport commentaries from different sources and by classifying these major actions into different categories. We also study if sentiment analysis could help detect these main actions.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#25991;&#31456;&#23545;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20351;&#29992;&#30340;&#26415;&#35821;&#36827;&#34892;&#20102;&#24314;&#35774;&#24615;&#25209;&#35780;&#65292;&#25351;&#20986;AI&#30340;&#35752;&#35770;&#32570;&#20047;&#23545;&#38544;&#21947;&#30340;&#25209;&#21028;&#24615;&#36317;&#31163;&#65292;&#23548;&#33268;&#23545;&#36131;&#20219;&#21644;&#28508;&#22312;&#29992;&#36884;&#30340;&#21453;&#24605;&#34987;&#25197;&#26354;&#12290;&#25991;&#31456;&#36890;&#36807;&#25552;&#20986;&#26356;&#21512;&#36866;&#30340;&#26415;&#35821;&#26469;&#20419;&#36827;&#26356;&#23500;&#26377;&#25104;&#26524;&#30340;&#36777;&#35770;&#12290;</title><link>http://arxiv.org/abs/2307.10292</link><description>&lt;p&gt;
&#35821;&#35328;&#36855;&#23467;&#65306;&#23545;&#20154;&#24037;&#26234;&#33021;&#35805;&#35821;&#20013;&#26415;&#35821;&#20351;&#29992;&#30340;&#24314;&#35774;&#24615;&#25209;&#35780;
&lt;/p&gt;
&lt;p&gt;
The Language Labyrinth: Constructive Critique on the Terminology Used in the AI Discourse. (arXiv:2307.10292v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10292
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25991;&#31456;&#23545;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20351;&#29992;&#30340;&#26415;&#35821;&#36827;&#34892;&#20102;&#24314;&#35774;&#24615;&#25209;&#35780;&#65292;&#25351;&#20986;AI&#30340;&#35752;&#35770;&#32570;&#20047;&#23545;&#38544;&#21947;&#30340;&#25209;&#21028;&#24615;&#36317;&#31163;&#65292;&#23548;&#33268;&#23545;&#36131;&#20219;&#21644;&#28508;&#22312;&#29992;&#36884;&#30340;&#21453;&#24605;&#34987;&#25197;&#26354;&#12290;&#25991;&#31456;&#36890;&#36807;&#25552;&#20986;&#26356;&#21512;&#36866;&#30340;&#26415;&#35821;&#26469;&#20419;&#36827;&#26356;&#23500;&#26377;&#25104;&#26524;&#30340;&#36777;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36328;&#23398;&#31185;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#39046;&#22495;&#20013;&#65292;&#26415;&#35821;&#26126;&#30830;&#24615;&#30340;&#38382;&#39064;&#23588;&#20026;&#37325;&#35201;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;AI&#30340;&#35752;&#35770;&#20173;&#28982;&#32570;&#20047;&#23545;&#35832;&#22914;&#8220;&#35757;&#32451;&#8221;&#12289;&#8220;&#23398;&#20064;&#8221;&#25110;&#8220;&#20915;&#31574;&#8221;&#31561;&#38544;&#21947;&#30340;&#25209;&#21028;&#24615;&#36317;&#31163;&#12290;&#22240;&#27492;&#65292;&#20851;&#20110;&#36131;&#20219;&#25110;&#28508;&#22312;&#29992;&#36884;&#30340;&#21453;&#24605;&#34987;&#20005;&#37325;&#25197;&#26354;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#30456;&#20851;&#20915;&#31574;&#32773;&#30456;&#20449;AI&#21487;&#20197;&#21457;&#23637;&#8220;&#29702;&#35299;&#8221;&#25110;&#27491;&#30830;&#8220;&#35299;&#37322;&#8221;&#38382;&#39064;&#65292;&#37027;&#20040;&#23427;&#22312;&#20915;&#23450;&#31038;&#20250;&#31119;&#21033;&#25110;&#23457;&#21028;&#26696;&#20214;&#31561;&#25935;&#24863;&#20219;&#21153;&#26102;&#30340;&#24120;&#35268;&#20351;&#29992;&#23558;&#20250;&#20986;&#29616;&#12290;&#26412;&#31456;&#36890;&#36807;&#20998;&#26512;AI&#36777;&#35770;&#30340;&#26680;&#24515;&#27010;&#24565;&#26469;&#25903;&#25345;&#20854;&#35266;&#28857;&#65292;&#24182;&#36890;&#36807;&#25552;&#20986;&#26356;&#21512;&#36866;&#30340;&#26415;&#35821;&#26469;&#20419;&#36827;&#26356;&#23500;&#26377;&#25104;&#26524;&#30340;&#36777;&#35770;&#12290;&#23427;&#26159;&#19968;&#39033;&#22312;&#25209;&#21028;&#24615;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#35821;&#35328;&#21746;&#23398;&#20043;&#38388;&#20132;&#21449;&#30340;&#27010;&#24565;&#24615;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the interdisciplinary field of artificial intelligence (AI) the problem of clear terminology is especially momentous. This paper claims, that AI debates are still characterised by a lack of critical distance to metaphors like 'training', 'learning' or 'deciding'. As consequence, reflections regarding responsibility or potential use-cases are greatly distorted. Yet, if relevant decision-makers are convinced that AI can develop an 'understanding' or properly 'interpret' issues, its regular use for sensitive tasks like deciding about social benefits or judging court cases looms. The chapter argues its claim by analysing central notions of the AI debate and tries to contribute by proposing more fitting terminology and hereby enabling more fruitful debates. It is a conceptual work at the intersection of critical computer science and philosophy of language.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#20998;&#26512;&#26041;&#27861;&#65292;&#23558;&#21477;&#23376;&#20998;&#31867;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#32467;&#21512;&#36215;&#26469;&#65292;&#24182;&#25581;&#31034;&#20102;&#36825;&#20004;&#20010;&#20449;&#24687;&#25552;&#21462;&#23376;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20114;&#22686;&#24378;&#25928;&#24212;&#12290;</title><link>http://arxiv.org/abs/2307.10291</link><description>&lt;p&gt;
&#26085;&#35821;&#21477;&#23376;&#20998;&#31867;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#30456;&#20114;&#22686;&#24378;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Mutual Reinforcement Effects in Japanese Sentence Classification and Named Entity Recognition Tasks. (arXiv:2307.10291v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10291
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#20998;&#26512;&#26041;&#27861;&#65292;&#23558;&#21477;&#23376;&#20998;&#31867;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#32467;&#21512;&#36215;&#26469;&#65292;&#24182;&#25581;&#31034;&#20102;&#36825;&#20004;&#20010;&#20449;&#24687;&#25552;&#21462;&#23376;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20114;&#22686;&#24378;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#25552;&#21462;&#65288;IE&#65289;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#37325;&#35201;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20256;&#32479;&#20998;&#27573;&#26041;&#27861;&#22312;&#21477;&#23376;&#20998;&#31867;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#22797;&#26434;&#20114;&#21160;&#65292;&#30446;&#21069;&#30740;&#31350;&#23578;&#19981;&#20805;&#20998;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#20998;&#26512;&#26041;&#27861;&#65292;&#23558;&#21477;&#23376;&#20998;&#31867;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#32467;&#21512;&#36215;&#26469;&#65292;&#26088;&#22312;&#25581;&#31034;&#21644;&#29702;&#35299;&#36825;&#20004;&#20010;&#20449;&#24687;&#25552;&#21462;&#23376;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20114;&#22686;&#24378;&#25928;&#24212;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21477;&#23376;&#20998;&#31867;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#22810;&#20219;&#21153;&#65288;SCNM&#65289;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#21477;&#23376;&#20998;&#31867;&#65288;SC&#65289;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#12290;&#25105;&#20204;&#20026;SCNM&#24320;&#21457;&#20102;&#19968;&#20010;&#21477;&#23376;&#21040;&#26631;&#31614;&#29983;&#25104;&#65288;SLG&#65289;&#26694;&#26550;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;SC&#21644;NER&#30340;&#32500;&#22522;&#30334;&#31185;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#26684;&#24335;&#36716;&#25442;&#22120;&#32479;&#19968;&#36755;&#20837;&#26684;&#24335;&#65292;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;SC&#26631;&#31614;&#12289;NER&#26631;&#31614;&#21644;&#30456;&#20851;&#25991;&#26412;&#27573;&#33853;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24212;&#29992;&#20110;&#26085;&#35821;&#21477;&#23376;&#20998;&#31867;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#20219;&#21153;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#20102;&#30456;&#20114;&#22686;&#24378;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Information extraction(IE) is a crucial subfield within natural language processing. However, for the traditionally segmented approach to sentence classification and Named Entity Recognition, the intricate interactions between these individual subtasks remain largely uninvestigated. In this study, we propose an integrative analysis, converging sentence classification with Named Entity Recognition, with the objective to unveil and comprehend the mutual reinforcement effect within these two information extraction subtasks. To achieve this, we introduce a Sentence Classification and Named Entity Recognition Multi-task (SCNM) approach that combines Sentence Classification (SC) and Named Entity Recognition (NER). We develop a Sentence-to-Label Generation (SLG) framework for SCNM and construct a Wikipedia dataset containing both SC and NER. Using a format converter, we unify input formats and employ a generative model to generate SC-labels, NER-labels, and associated text segments. We propos
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#39046;&#22495;&#25935;&#24863;&#35821;&#38899;&#35782;&#21035;&#26041;&#27861;&#65292;&#21033;&#29992;&#25991;&#26412;&#25552;&#31034;&#26469;&#29983;&#25104;&#39046;&#22495;&#25935;&#24863;&#27169;&#22411;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#23454;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#39046;&#22495;&#21644;&#25552;&#31034;&#19978;&#19979;&#25991;&#19979;&#22343;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#35789;&#38169;&#35823;&#29575;&#38477;&#20302;&#36798;&#21040;&#26368;&#39640;33%&#12290;&#36890;&#36807;&#20165;&#20351;&#29992;&#25991;&#26412;&#36827;&#34892;&#24494;&#35843;&#65292;&#35813;&#27169;&#22411;&#22312;&#21307;&#23398;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#30340;&#35782;&#21035;&#25928;&#26524;&#26368;&#20339;&#65292;&#35789;&#38169;&#35823;&#29575;&#38477;&#20302;&#36798;&#21040;29%&#12290;</title><link>http://arxiv.org/abs/2307.10274</link><description>&lt;p&gt;
&#20351;&#29992;&#25552;&#31034;&#26465;&#20214;&#24494;&#35843;&#23454;&#29616;&#38646;&#26679;&#26412;&#39046;&#22495;&#25935;&#24863;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Domain-sensitive Speech Recognition with Prompt-conditioning Fine-tuning. (arXiv:2307.10274v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10274
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#39046;&#22495;&#25935;&#24863;&#35821;&#38899;&#35782;&#21035;&#26041;&#27861;&#65292;&#21033;&#29992;&#25991;&#26412;&#25552;&#31034;&#26469;&#29983;&#25104;&#39046;&#22495;&#25935;&#24863;&#27169;&#22411;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#23454;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#39046;&#22495;&#21644;&#25552;&#31034;&#19978;&#19979;&#25991;&#19979;&#22343;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#35789;&#38169;&#35823;&#29575;&#38477;&#20302;&#36798;&#21040;&#26368;&#39640;33%&#12290;&#36890;&#36807;&#20165;&#20351;&#29992;&#25991;&#26412;&#36827;&#34892;&#24494;&#35843;&#65292;&#35813;&#27169;&#22411;&#22312;&#21307;&#23398;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#30340;&#35782;&#21035;&#25928;&#26524;&#26368;&#20339;&#65292;&#35789;&#38169;&#35823;&#29575;&#38477;&#20302;&#36798;&#21040;29%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#21019;&#24314;&#21033;&#29992;&#25991;&#26412;&#39046;&#22495;&#20449;&#24687;&#30340;&#39046;&#22495;&#25935;&#24863;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#20854;&#29983;&#25104;&#26465;&#20214;&#21270;&#22312;&#32473;&#23450;&#30340;&#25991;&#26412;&#25552;&#31034;&#19978;&#23454;&#29616;&#12290;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#65288;Whisper&#65289;&#36827;&#34892;&#24494;&#35843;&#65292;&#20174;&#25552;&#31034;&#31034;&#20363;&#20013;&#23398;&#20064;&#65292;&#36825;&#19968;&#30446;&#26631;&#24471;&#20197;&#23454;&#29616;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#33021;&#21147;&#21487;&#20197;&#25512;&#24191;&#21040;&#19981;&#21516;&#30340;&#39046;&#22495;&#21644;&#21508;&#31181;&#25552;&#31034;&#19978;&#19979;&#25991;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#26410;&#35265;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#22810;&#36798;33&#65285;&#30340;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#38477;&#20302;&#65292;&#20363;&#22914;&#21307;&#23398;&#23545;&#35805;&#65292;&#31354;&#20013;&#20132;&#36890;&#25511;&#21046;&#36890;&#20449;&#21644;&#37329;&#34701;&#20250;&#35758;&#31561;&#12290;&#32771;&#34385;&#21040;&#38899;&#39057;-&#25991;&#26412;&#23545;&#25968;&#25454;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#25193;&#23637;&#21040;&#20165;&#25991;&#26412;&#24494;&#35843;&#65292;&#20197;&#23454;&#29616;&#39046;&#22495;&#25935;&#24863;&#24615;&#21644;&#39046;&#22495;&#36866;&#24212;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#20165;&#25991;&#26412;&#24494;&#35843;&#27169;&#22411;&#20063;&#21487;&#20197;&#20851;&#27880;&#21508;&#31181;&#25552;&#31034;&#19978;&#19979;&#25991;&#65292;&#35813;&#27169;&#22411;&#22312;&#21307;&#23398;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#30340;WER&#38477;&#20302;&#26368;&#22810;&#36798;&#21040;29&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a method to create domain-sensitive speech recognition models that utilize textual domain information by conditioning its generation on a given text prompt. This is accomplished by fine-tuning a pre-trained, end-to-end model (Whisper) to learn from demonstrations with prompt examples. We show that this ability can be generalized to different domains and even various prompt contexts, with our model gaining a Word Error Rate (WER) reduction of up to 33% on unseen datasets from various domains, such as medical conversation, air traffic control communication, and financial meetings. Considering the limited availability of audio-transcript pair data, we further extend our method to text-only fine-tuning to achieve domain sensitivity as well as domain adaptation. We demonstrate that our text-only fine-tuned model can also attend to various prompt contexts, with the model reaching the most WER reduction of 29% on the medical conversation dataset.
&lt;/p&gt;</description></item><item><title>NaRuto&#26159;&#19968;&#20010;&#31995;&#32479;&#65292;&#21487;&#20197;&#20174;&#21465;&#20107;&#25991;&#26412;&#20013;&#33258;&#21160;&#25552;&#21462;&#32467;&#26500;&#21270;&#20107;&#20214;&#65292;&#24182;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#34892;&#21160;&#27169;&#22411;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#23436;&#20840;&#33258;&#21160;&#21270;&#26041;&#27861;&#21644;&#19982;&#21322;&#33258;&#21160;&#21270;&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;</title><link>http://arxiv.org/abs/2307.10247</link><description>&lt;p&gt;
&#20174;&#21465;&#20107;&#25991;&#26412;&#20013;&#33258;&#21160;&#33719;&#21462;&#34892;&#21160;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Automated Action Model Acquisition from Narrative Texts. (arXiv:2307.10247v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10247
&lt;/p&gt;
&lt;p&gt;
NaRuto&#26159;&#19968;&#20010;&#31995;&#32479;&#65292;&#21487;&#20197;&#20174;&#21465;&#20107;&#25991;&#26412;&#20013;&#33258;&#21160;&#25552;&#21462;&#32467;&#26500;&#21270;&#20107;&#20214;&#65292;&#24182;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#34892;&#21160;&#27169;&#22411;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#23436;&#20840;&#33258;&#21160;&#21270;&#26041;&#27861;&#21644;&#19982;&#21322;&#33258;&#21160;&#21270;&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34892;&#21160;&#27169;&#22411;&#20197;&#21069;&#25552;/&#25928;&#26524;&#20844;&#29702;&#30340;&#24418;&#24335;&#23384;&#22312;&#65292;&#20026;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#25552;&#20379;&#34892;&#21160;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#32852;&#21644;&#21160;&#26426;&#36830;&#25509;&#12290;&#34892;&#21160;&#27169;&#22411;&#33719;&#21462;&#34987;&#35748;&#20026;&#26159;&#35745;&#21010;&#25216;&#26415;&#24212;&#29992;&#20013;&#30340;&#29942;&#39048;&#65292;&#29305;&#21035;&#26159;&#22312;&#21465;&#20107;&#35745;&#21010;&#20013;&#12290;&#20174;&#21465;&#20107;&#25991;&#26412;&#20013;&#20197;&#33258;&#21160;&#21270;&#30340;&#26041;&#24335;&#33719;&#21462;&#34892;&#21160;&#27169;&#22411;&#26159;&#24517;&#35201;&#30340;&#65292;&#20294;&#30001;&#20110;&#36825;&#26679;&#30340;&#25991;&#26412;&#26412;&#36136;&#19978;&#22797;&#26434;&#65292;&#22240;&#27492;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;NaRuto&#65292;&#19968;&#20010;&#31995;&#32479;&#65292;&#23427;&#21487;&#20197;&#20174;&#21465;&#20107;&#25991;&#26412;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#20107;&#20214;&#65292;&#24182;&#22522;&#20110;&#24120;&#35782;&#20107;&#20214;&#20851;&#31995;&#30340;&#39044;&#27979;&#20197;&#21450;&#25991;&#26412;&#19978;&#30340;&#30683;&#30462;&#21644;&#30456;&#20284;&#24615;&#26080;&#30417;&#30563;&#22320;&#29983;&#25104;&#35745;&#21010;&#35821;&#35328;&#39118;&#26684;&#30340;&#34892;&#21160;&#27169;&#22411;&#12290;&#32463;&#20856;&#30340;&#21465;&#20107;&#35745;&#21010;&#39046;&#22495;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;NaRuto&#21487;&#20197;&#29983;&#25104;&#36136;&#37327;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#23436;&#20840;&#33258;&#21160;&#21270;&#26041;&#27861;&#30340;&#34892;&#21160;&#27169;&#22411;&#65292;&#29978;&#33267;&#19982;&#21322;&#33258;&#21160;&#21270;&#26041;&#27861;&#30340;&#34892;&#21160;&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;
Action models, which take the form of precondition/effect axioms, facilitate causal and motivational connections between actions for AI agents. Action model acquisition has been identified as a bottleneck in the application of planning technology, especially within narrative planning. Acquiring action models from narrative texts in an automated way is essential, but challenging because of the inherent complexities of such texts. We present NaRuto, a system that extracts structured events from narrative text and subsequently generates planning-language-style action models based on predictions of commonsense event relations, as well as textual contradictions and similarities, in an unsupervised manner. Experimental results in classical narrative planning domains show that NaRuto can generate action models of significantly better quality than existing fully automated methods, and even on par with those of semi-automated methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#33041;&#23545;&#40784;&#30340;&#30740;&#31350;&#65292;&#37325;&#28857;&#22312;&#20110;&#33041;&#32534;&#30721;&#21644;&#35299;&#30721;&#27169;&#22411;&#30340;&#24212;&#29992;&#12290;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#29702;&#35299;&#22823;&#33041;&#30340;&#20449;&#24687;&#22788;&#29702;&#26426;&#21046;&#20197;&#21450;&#35774;&#35745;&#33041;&#26426;&#25509;&#21475;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2307.10246</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#33041;&#23545;&#40784;&#65306;&#33041;&#32534;&#30721;&#21644;&#35299;&#30721;&#65288;&#32508;&#36848;&#65289;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks and Brain Alignment: Brain Encoding and Decoding (Survey). (arXiv:2307.10246v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#33041;&#23545;&#40784;&#30340;&#30740;&#31350;&#65292;&#37325;&#28857;&#22312;&#20110;&#33041;&#32534;&#30721;&#21644;&#35299;&#30721;&#27169;&#22411;&#30340;&#24212;&#29992;&#12290;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#29702;&#35299;&#22823;&#33041;&#30340;&#20449;&#24687;&#22788;&#29702;&#26426;&#21046;&#20197;&#21450;&#35774;&#35745;&#33041;&#26426;&#25509;&#21475;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#33041;&#22914;&#20309;&#34920;&#31034;&#19981;&#21516;&#30340;&#20449;&#24687;&#27169;&#24335;&#65311;&#25105;&#20204;&#33021;&#21542;&#35774;&#35745;&#20986;&#19968;&#20010;&#21487;&#20197;&#33258;&#21160;&#29702;&#35299;&#29992;&#25143;&#24605;&#32771;&#20869;&#23481;&#30340;&#31995;&#32479;&#65311;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#30740;&#31350;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;fMRI&#65289;&#31561;&#22823;&#33041;&#35760;&#24405;&#26469;&#22238;&#31572;&#12290;&#20316;&#20026;&#31532;&#19968;&#27493;&#65292;&#31070;&#32463;&#31185;&#23398;&#30028;&#20026;&#34987;&#21160;&#38405;&#35835;/&#21548;&#35273;/&#35266;&#30475;&#27010;&#24565;&#35789;&#27719;&#12289;&#21465;&#36848;&#12289;&#22270;&#29255;&#21644;&#30005;&#24433;&#30456;&#20851;&#30340;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#25968;&#25454;&#38598;&#20316;&#20986;&#20102;&#36129;&#29486;&#12290;&#36807;&#21435;&#20108;&#21313;&#24180;&#20013;&#65292;&#36824;&#25552;&#20986;&#20102;&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#32534;&#30721;&#21644;&#35299;&#30721;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#20316;&#20026;&#22522;&#30784;&#30740;&#31350;&#20013;&#30340;&#39069;&#22806;&#24037;&#20855;&#65292;&#22312;&#35748;&#30693;&#31185;&#23398;&#21644;&#31070;&#32463;&#31185;&#23398;&#39046;&#22495;&#26377;&#30528;&#22810;&#31181;&#23454;&#38469;&#24212;&#29992;&#12290;&#32534;&#30721;&#27169;&#22411;&#26088;&#22312;&#33258;&#21160;&#22320;&#29983;&#25104;fMRI&#22823;&#33041;&#34920;&#24449;&#65292;&#32473;&#23450;&#19968;&#20010;&#21050;&#28608;&#12290;&#23427;&#20204;&#22312;&#35780;&#20272;&#21644;&#35786;&#26029;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#20197;&#21450;&#35774;&#35745;&#22823;&#33041;&#25439;&#20260;&#27835;&#30103;&#26041;&#27861;&#26041;&#38754;&#26377;&#30528;&#22810;&#31181;&#23454;&#38469;&#24212;&#29992;&#12290;&#35299;&#30721;&#27169;&#22411;&#35299;&#20915;&#20102;&#26681;&#25454;fMRI&#37325;&#26500;&#21050;&#28608;&#30340;&#36870;&#38382;&#39064;&#12290;&#23427;&#20204;&#23545;&#20110;&#29702;&#35299;&#22823;&#33041;&#22914;&#20309;&#22788;&#29702;&#20449;&#24687;&#20197;&#21450;&#35774;&#35745;&#33041;&#26426;&#25509;&#21475;&#30340;&#21457;&#23637;&#37117;&#26377;&#30528;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
How does the brain represent different modes of information? Can we design a system that automatically understands what the user is thinking? Such questions can be answered by studying brain recordings like functional magnetic resonance imaging (fMRI). As a first step, the neuroscience community has contributed several large cognitive neuroscience datasets related to passive reading/listening/viewing of concept words, narratives, pictures and movies. Encoding and decoding models using these datasets have also been proposed in the past two decades. These models serve as additional tools for basic research in cognitive science and neuroscience. Encoding models aim at generating fMRI brain representations given a stimulus automatically. They have several practical applications in evaluating and diagnosing neurological conditions and thus also help design therapies for brain damage. Decoding models solve the inverse problem of reconstructing the stimuli given the fMRI. They are useful for 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#19981;&#30830;&#23450;&#24615;&#30340;&#35282;&#24230;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#22312;&#25506;&#32034;&#21644;&#25269;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#33391;&#34892;&#20026;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.10236</link><description>&lt;p&gt;
&#19977;&#24605;&#32780;&#21518;&#34892;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#27979;&#37327;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Look Before You Leap: An Exploratory Study of Uncertainty Measurement for Large Language Models. (arXiv:2307.10236v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10236
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#19981;&#30830;&#23450;&#24615;&#30340;&#35282;&#24230;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#22312;&#25506;&#32034;&#21644;&#25269;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#33391;&#34892;&#20026;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#36817;&#24615;&#33021;&#31361;&#30772;&#20026;&#20247;&#22810;&#24037;&#19994;&#24212;&#29992;&#21644;&#39046;&#22495;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#36935;&#12290;&#28982;&#32780;&#65292;LLMs&#30340;&#38169;&#35823;&#29983;&#25104;&#65292;&#22914;&#34394;&#20551;&#39044;&#27979;&#12289;&#38169;&#35823;&#20449;&#24687;&#21644;&#24187;&#35273;&#65292;&#20063;&#24341;&#21457;&#20102;&#23545;LLMs&#21487;&#38752;&#24615;&#30340;&#20005;&#37325;&#20851;&#27880;&#65292;&#23588;&#20854;&#22312;&#23545;&#23433;&#20840;&#12289;&#21487;&#38752;&#24615;&#26377;&#25935;&#24863;&#30340;&#22330;&#26223;&#20013;&#65292;&#21487;&#33021;&#38459;&#30861;&#20854;&#22312;&#23454;&#38469;&#20013;&#30340;&#24212;&#29992;&#12290;&#23613;&#31649;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24050;&#32463;&#26174;&#31034;&#20986;&#20854;&#22312;&#35299;&#37322;&#19968;&#33324;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#30340;&#39044;&#27979;&#39118;&#38505;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#20294;&#20851;&#20110;&#23427;&#26159;&#21542;&#20197;&#21450;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#26377;&#21161;&#20110;&#25506;&#32034;LLMs&#30340;&#33021;&#21147;&#21644;&#25269;&#21046;&#20854;&#19981;&#33391;&#34892;&#20026;&#26041;&#38754;&#30693;&#20043;&#29978;&#23569;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#26412;&#25991;&#20174;&#19981;&#30830;&#23450;&#24615;&#30340;&#35282;&#24230;&#24320;&#23637;&#20102;&#20851;&#20110;LLMs&#39118;&#38505;&#35780;&#20272;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;12&#31181;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#21644;4&#20010;LLMs&#22312;4&#20010;&#37325;&#35201;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#20197;&#35843;&#26597;&#19981;&#30830;&#23450;&#24615;&#22312;&#25506;&#32034;LLMs&#33021;&#21147;&#21644;&#23545;&#25239;&#20854;&#19981;&#33391;&#34892;&#20026;&#26041;&#38754;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent performance leap of Large Language Models (LLMs) opens up new opportunities across numerous industrial applications and domains. However, erroneous generations, such as false predictions, misinformation, and hallucination made by LLMs, have also raised severe concerns for the trustworthiness of LLMs', especially in safety-, security- and reliability-sensitive scenarios, potentially hindering real-world adoptions. While uncertainty estimation has shown its potential for interpreting the prediction risks made by general machine learning (ML) models, little is known about whether and to what extent it can help explore an LLM's capabilities and counteract its undesired behavior. To bridge the gap, in this paper, we initiate an exploratory study on the risk assessment of LLMs from the lens of uncertainty. In particular, we experiment with twelve uncertainty estimation methods and four LLMs on four prominent natural language processing (NLP) tasks to investigate to what extent unc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;GPT&#36827;&#34892;&#39640;&#32423;&#24773;&#24863;&#20998;&#26512;&#65292;&#24182;&#32771;&#23519;&#20854;&#19982;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#24046;&#24322;&#65292;&#21457;&#29616;GPT&#26041;&#27861;&#30456;&#36739;&#20110;&#20854;&#20182;&#27169;&#22411;&#22312;&#39044;&#27979;&#24615;&#33021;&#19978;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#65292;&#24182;&#26377;&#25928;&#35299;&#20915;&#20102;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#19968;&#20123;&#25361;&#25112;&#65292;&#22914;&#29702;&#35299;&#19978;&#19979;&#25991;&#21644;&#26816;&#27979;&#35773;&#21050;&#12290;</title><link>http://arxiv.org/abs/2307.10234</link><description>&lt;p&gt;
SentimentGPT&#65306;&#21033;&#29992;GPT&#36827;&#34892;&#39640;&#32423;&#24773;&#24863;&#20998;&#26512;&#21450;&#20854;&#19982;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
SentimentGPT: Exploiting GPT for Advanced Sentiment Analysis and its Departure from Current Machine Learning. (arXiv:2307.10234v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;GPT&#36827;&#34892;&#39640;&#32423;&#24773;&#24863;&#20998;&#26512;&#65292;&#24182;&#32771;&#23519;&#20854;&#19982;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#24046;&#24322;&#65292;&#21457;&#29616;GPT&#26041;&#27861;&#30456;&#36739;&#20110;&#20854;&#20182;&#27169;&#22411;&#22312;&#39044;&#27979;&#24615;&#33021;&#19978;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#65292;&#24182;&#26377;&#25928;&#35299;&#20915;&#20102;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#19968;&#20123;&#25361;&#25112;&#65292;&#22914;&#29702;&#35299;&#19978;&#19979;&#25991;&#21644;&#26816;&#27979;&#35773;&#21050;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#24773;&#24863;&#20998;&#26512;&#20013;&#21508;&#31181;&#29983;&#25104;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#65288;GPT&#65289;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#32771;&#23519;&#65292;&#29305;&#21035;&#26159;&#22312;SemEval 2017&#25968;&#25454;&#38598;&#30340;&#20219;&#21153;4&#20013;&#12290;&#37319;&#29992;&#20102;&#19977;&#31181;&#20027;&#35201;&#31574;&#30053;&#65306;1&#65289;&#20351;&#29992;GPT-3.5 Turbo&#36827;&#34892;&#25552;&#31034;&#24037;&#31243;&#65292;2&#65289;&#23545;GPT&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;3&#65289;&#37319;&#29992;&#21019;&#26032;&#30340;&#23884;&#20837;&#20998;&#31867;&#26041;&#27861;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#36825;&#20123;&#31574;&#30053;&#21644;&#20010;&#21035;GPT&#27169;&#22411;&#20043;&#38388;&#30340;&#35814;&#32454;&#27604;&#36739;&#35265;&#35299;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#29420;&#29305;&#30340;&#20248;&#21183;&#21644;&#28508;&#22312;&#30340;&#23616;&#38480;&#24615;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#23558;&#36825;&#20123;&#22522;&#20110;GPT&#30340;&#26041;&#27861;&#19982;&#20854;&#20182;&#21516;&#26102;&#20195;&#12289;&#39640;&#24615;&#33021;&#30340;&#27169;&#22411;&#22312;&#30456;&#21516;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;GPT&#26041;&#27861;&#22312;&#39044;&#27979;&#24615;&#33021;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#65292;&#30456;&#36739;&#20110;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;F1&#20998;&#25968;&#22686;&#21152;&#20102;22%&#20197;&#19978;&#12290;&#27492;&#22806;&#65292;&#26412;&#35770;&#25991;&#36824;&#25506;&#35752;&#20102;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#24120;&#35265;&#25361;&#25112;&#65292;&#22914;&#29702;&#35299;&#19978;&#19979;&#25991;&#21644;&#26816;&#27979;&#35773;&#21050;&#12290;&#30740;&#31350;&#24378;&#35843;&#20102;GPT&#26041;&#27861;&#30340;&#37325;&#35201;&#20215;&#20540;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents a thorough examination of various Generative Pretrained Transformer (GPT) methodologies in sentiment analysis, specifically in the context of Task 4 on the SemEval 2017 dataset. Three primary strategies are employed: 1) prompt engineering using the advanced GPT-3.5 Turbo, 2) fine-tuning GPT models, and 3) an inventive approach to embedding classification. The research yields detailed comparative insights among these strategies and individual GPT models, revealing their unique strengths and potential limitations. Additionally, the study compares these GPT-based methodologies with other contemporary, high-performing models previously used with the same dataset. The results illustrate the significant superiority of the GPT approaches in terms of predictive performance, more than 22% in F1-score compared to the state-of-the-art. Further, the paper addresses common challenges in sentiment analysis tasks, such as understanding context and detecting sarcasm. It underscores
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#27493;&#39588;&#30340;&#26041;&#27861;&#26469;&#20943;&#23569;&#22312;&#32447;&#23545;&#35805;&#20013;&#30340;&#20559;&#35265;&#21644;&#20167;&#24680;&#35328;&#35770;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20808;&#20351;&#29992;&#20998;&#31867;&#22120;&#26816;&#27979;&#20167;&#24680;&#35328;&#35770;&#65292;&#28982;&#21518;&#21033;&#29992;&#25552;&#31034;&#29983;&#25104;&#26356;&#23569;&#20559;&#35265;&#25110;&#26080;&#20559;&#35265;&#30340;&#26367;&#20195;&#35821;&#35328;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#36127;&#38754;&#24433;&#21709;&#65292;&#20026;&#20943;&#23569;&#22312;&#32447;&#35752;&#35770;&#20013;&#30340;&#20559;&#35265;&#65292;&#20419;&#36827;&#26356;&#20855;&#21253;&#23481;&#24615;&#21644;&#20844;&#24179;&#24615;&#30340;&#27807;&#36890;&#29615;&#22659;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2307.10213</link><description>&lt;p&gt;
&#22312;&#23545;&#35805;&#20013;&#20943;&#23569;&#20559;&#35265;&#65306;&#19968;&#20010;&#24102;&#25552;&#31034;&#30340;&#20167;&#24680;&#35328;&#35770;&#20998;&#31867;&#22120;&#21644;&#21435;&#20559;&#35265;&#22120;
&lt;/p&gt;
&lt;p&gt;
Mitigating Bias in Conversations: A Hate Speech Classifier and Debiaser with Prompts. (arXiv:2307.10213v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10213
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#27493;&#39588;&#30340;&#26041;&#27861;&#26469;&#20943;&#23569;&#22312;&#32447;&#23545;&#35805;&#20013;&#30340;&#20559;&#35265;&#21644;&#20167;&#24680;&#35328;&#35770;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20808;&#20351;&#29992;&#20998;&#31867;&#22120;&#26816;&#27979;&#20167;&#24680;&#35328;&#35770;&#65292;&#28982;&#21518;&#21033;&#29992;&#25552;&#31034;&#29983;&#25104;&#26356;&#23569;&#20559;&#35265;&#25110;&#26080;&#20559;&#35265;&#30340;&#26367;&#20195;&#35821;&#35328;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#36127;&#38754;&#24433;&#21709;&#65292;&#20026;&#20943;&#23569;&#22312;&#32447;&#35752;&#35770;&#20013;&#30340;&#20559;&#35265;&#65292;&#20419;&#36827;&#26356;&#20855;&#21253;&#23481;&#24615;&#21644;&#20844;&#24179;&#24615;&#30340;&#27807;&#36890;&#29615;&#22659;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27495;&#35270;&#24615;&#35821;&#35328;&#21644;&#20559;&#35265;&#36890;&#24120;&#22312;&#23545;&#35805;&#20013;&#23384;&#22312;&#65292;&#36825;&#36890;&#24120;&#23545;&#22522;&#20110;&#31181;&#26063;&#12289;&#24615;&#21035;&#21644;&#23447;&#25945;&#30340;&#30446;&#26631;&#32676;&#20307;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#27493;&#39588;&#30340;&#26041;&#27861;&#65306;&#39318;&#20808;&#65292;&#20351;&#29992;&#20998;&#31867;&#22120;&#26816;&#27979;&#20167;&#24680;&#35328;&#35770;&#65292;&#28982;&#21518;&#21033;&#29992;&#19968;&#20010;&#21435;&#20559;&#35265;&#32452;&#20214;&#36890;&#36807;&#25552;&#31034;&#29983;&#25104;&#26356;&#23569;&#20559;&#35265;&#25110;&#26080;&#20559;&#35265;&#30340;&#26367;&#20195;&#35821;&#35328;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#35266;&#23519;&#21040;&#20167;&#24680;&#35328;&#35770;&#23548;&#33268;&#30340;&#36127;&#38754;&#24615;&#20943;&#23569;&#12290;&#35813;&#26041;&#27861;&#23545;&#20110;&#20943;&#23569;&#22312;&#32447;&#35752;&#35770;&#20013;&#30340;&#20559;&#35265;&#65292;&#20419;&#36827;&#26356;&#20855;&#21253;&#23481;&#24615;&#21644;&#20844;&#24179;&#24615;&#30340;&#27807;&#36890;&#29615;&#22659;&#30340;&#21162;&#21147;&#26377;&#25152;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discriminatory language and biases are often present in hate speech during conversations, which usually lead to negative impacts on targeted groups such as those based on race, gender, and religion. To tackle this issue, we propose an approach that involves a two-step process: first, detecting hate speech using a classifier, and then utilizing a debiasing component that generates less biased or unbiased alternatives through prompts. We evaluated our approach on a benchmark dataset and observed reduction in negativity due to hate speech comments. The proposed method contributes to the ongoing efforts to reduce biases in online discourse and promote a more inclusive and fair environment for communication.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#25968;&#25454;&#38598;&#35282;&#24230;&#35299;&#20915;&#39046;&#22495;&#36866;&#24212;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#31616;&#21333;&#30340;&#35789;&#27719;&#36716;&#25442;&#65292;&#20943;&#23569;&#20102;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#39046;&#22495;&#24046;&#24322;&#12290;&#22312;&#25512;&#29305;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#26080;&#30417;&#30563;&#35789;&#24615;&#26631;&#27880;&#20934;&#30830;&#29575;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#21512;&#25104;&#25512;&#29305;&#25991;&#26412;&#24182;&#22686;&#21152;&#25968;&#25454;&#38598;&#65292;&#36824;&#23454;&#29616;&#20102;&#35789;&#24615;&#26631;&#27880;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.10210</link><description>&lt;p&gt;
&#20351;&#29992;&#35789;&#27719;&#36716;&#25442;&#21644;&#26631;&#31614;&#27880;&#20837;&#36827;&#34892;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#30340;&#25512;&#29305;&#25968;&#25454;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Domain Adaptation using Lexical Transformations and Label Injection for Twitter Data. (arXiv:2307.10210v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10210
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#25968;&#25454;&#38598;&#35282;&#24230;&#35299;&#20915;&#39046;&#22495;&#36866;&#24212;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#31616;&#21333;&#30340;&#35789;&#27719;&#36716;&#25442;&#65292;&#20943;&#23569;&#20102;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#39046;&#22495;&#24046;&#24322;&#12290;&#22312;&#25512;&#29305;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#26080;&#30417;&#30563;&#35789;&#24615;&#26631;&#27880;&#20934;&#30830;&#29575;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#21512;&#25104;&#25512;&#29305;&#25991;&#26412;&#24182;&#22686;&#21152;&#25968;&#25454;&#38598;&#65292;&#36824;&#23454;&#29616;&#20102;&#35789;&#24615;&#26631;&#27880;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#36866;&#24212;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#37325;&#35201;&#19988;&#24191;&#27867;&#30740;&#31350;&#30340;&#38382;&#39064;&#12290;&#22823;&#37327;&#25991;&#29486;&#23581;&#35797;&#36890;&#36807;&#23558;&#28304;&#39046;&#22495;&#35757;&#32451;&#30340;&#27169;&#22411;&#36866;&#24212;&#21040;&#30446;&#26631;&#39046;&#22495;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#26412;&#25991;&#20174;&#25968;&#25454;&#38598;&#30340;&#35282;&#24230;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#31616;&#21333;&#30340;&#35789;&#27719;&#36716;&#25442;&#20462;&#25913;&#28304;&#39046;&#22495;&#25968;&#25454;&#38598;&#65292;&#20197;&#20943;&#23569;&#28304;&#25968;&#25454;&#38598;&#20998;&#24067;&#21644;&#30446;&#26631;&#25968;&#25454;&#38598;&#20998;&#24067;&#20043;&#38388;&#30340;&#39046;&#22495;&#24046;&#24322;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#36716;&#25442;&#21518;&#30340;&#28304;&#39046;&#22495;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#27604;&#38646;&#26679;&#26412;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;&#36890;&#36807;&#23558;&#26631;&#20934;&#33521;&#35821;&#36716;&#25442;&#20026;&#25512;&#29305;&#25991;&#26412;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;92.14%&#30340;&#26080;&#30417;&#30563;&#35789;&#24615;&#26631;&#27880;&#20934;&#30830;&#29575;&#65288;&#30456;&#27604;&#20110;81.54%&#30340;&#38646;&#26679;&#26412;&#20934;&#30830;&#29575;&#65289;&#65292;&#20165;&#30053;&#20302;&#20110;94.45%&#30340;&#26377;&#30417;&#30563;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#25552;&#20986;&#30340;&#36716;&#25442;&#26041;&#27861;&#21512;&#25104;&#25512;&#29305;&#25991;&#26412;&#24182;&#22686;&#21152;&#25512;&#29305;&#25968;&#25454;&#38598;&#65292;&#20197;&#23454;&#29616;&#35789;&#24615;&#26631;&#27880;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain adaptation is an important and widely studied problem in natural language processing. A large body of literature tries to solve this problem by adapting models trained on the source domain to the target domain. In this paper, we instead solve this problem from a dataset perspective. We modify the source domain dataset with simple lexical transformations to reduce the domain shift between the source dataset distribution and the target dataset distribution. We find that models trained on the transformed source domain dataset performs significantly better than zero-shot models. Using our proposed transformations to convert standard English to tweets, we reach an unsupervised part-of-speech (POS) tagging accuracy of 92.14% (from 81.54% zero shot accuracy), which is only slightly below the supervised performance of 94.45%. We also use our proposed transformations to synthetically generate tweets and augment the Twitter dataset to achieve state-of-the-art performance for POS tagging.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#31163;&#23130;&#27861;&#24237;&#35785;&#35772;&#65292;&#25506;&#32034;&#20102;&#24615;&#21035;&#19981;&#24179;&#31561;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#20559;&#35265;&#38382;&#39064;&#12290;&#38656;&#35201;&#23545;&#29616;&#26377;&#36164;&#28304;&#36827;&#34892;&#20462;&#27491;&#26469;&#37327;&#21270;&#31038;&#20250;&#19981;&#24179;&#31561;&#12290;</title><link>http://arxiv.org/abs/2307.10200</link><description>&lt;p&gt;
&#20174;&#27169;&#22411;&#20559;&#35265;&#20013;&#20998;&#31163;&#31038;&#20250;&#19981;&#24179;&#31561;&#65306;&#31163;&#23130;&#27861;&#24237;&#35785;&#35772;&#20013;&#30340;&#24615;&#21035;&#19981;&#24179;&#31561;
&lt;/p&gt;
&lt;p&gt;
Disentangling Societal Inequality from Model Biases: Gender Inequality in Divorce Court Proceedings. (arXiv:2307.10200v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#31163;&#23130;&#27861;&#24237;&#35785;&#35772;&#65292;&#25506;&#32034;&#20102;&#24615;&#21035;&#19981;&#24179;&#31561;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#20559;&#35265;&#38382;&#39064;&#12290;&#38656;&#35201;&#23545;&#29616;&#26377;&#36164;&#28304;&#36827;&#34892;&#20462;&#27491;&#26469;&#37327;&#21270;&#31038;&#20250;&#19981;&#24179;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#23130;&#26159;&#27861;&#38498;&#27861;&#24459;&#35299;&#38500;&#23130;&#23035;&#20851;&#31995;&#30340;&#36807;&#31243;&#12290;&#30001;&#20110;&#36825;&#36890;&#24120;&#26159;&#23130;&#23035;&#32852;&#21512;&#30340;&#19981;&#24841;&#24555;&#32467;&#26524;&#65292;&#27599;&#19968;&#26041;&#37117;&#21487;&#33021;&#26377;&#29702;&#30001;&#35201;&#27714;&#36864;&#20986;&#20915;&#23450;&#65292;&#36825;&#36890;&#24120;&#22312;&#27861;&#24237;&#35785;&#35772;&#20013;&#26377;&#35814;&#32454;&#35760;&#24405;&#12290;&#36890;&#36807;&#19968;&#20221;&#21253;&#21547;17,306&#20221;&#27861;&#24237;&#35785;&#35772;&#30340;&#22823;&#37327;&#35821;&#26009;&#24211;&#65292;&#26412;&#25991;&#36890;&#36807;&#31163;&#23130;&#27861;&#24237;&#35785;&#35772;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#24615;&#21035;&#19981;&#24179;&#31561;&#38382;&#39064;&#12290;&#34429;&#28982;&#26032;&#20852;&#30340;&#25968;&#25454;&#26469;&#28304;&#65288;&#20363;&#22914;&#20844;&#20849;&#27861;&#24237;&#35760;&#24405;&#65289;&#22312;&#36741;&#21161;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#20559;&#35265;&#21487;&#33021;&#20250;&#24178;&#25200;&#25110;&#24433;&#21709;&#27492;&#31867;&#30740;&#31350;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#38656;&#35201;&#23545;&#29616;&#26377;NLP&#36164;&#28304;&#20013;&#30340;&#28508;&#22312;&#24046;&#36317;&#21644;&#38480;&#21046;&#36827;&#34892;&#24443;&#24213;&#20998;&#26512;&#12290;&#22312;&#26041;&#27861;&#35770;&#19978;&#65292;&#26412;&#25991;&#35777;&#26126;&#20102;&#29616;&#26377;NLP&#36164;&#28304;&#38656;&#35201;&#36827;&#34892;&#20960;&#20010;&#38750;&#24179;&#20961;&#30340;&#20462;&#25913;&#65292;&#20197;&#37327;&#21270;&#31038;&#20250;&#19981;&#24179;&#31561;&#12290;&#22312;&#23454;&#36136;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#23613;&#31649;&#22823;&#37327;&#30340;&#27861;&#24237;&#26696;&#20214;&#21487;&#33021;&#26263;&#31034;&#30528;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
Divorce is the legal dissolution of a marriage by a court. Since this is usually an unpleasant outcome of a marital union, each party may have reasons to call the decision to quit which is generally documented in detail in the court proceedings. Via a substantial corpus of 17,306 court proceedings, this paper investigates gender inequality through the lens of divorce court proceedings. While emerging data sources (e.g., public court records) on sensitive societal issues hold promise in aiding social science research, biases present in cutting-edge natural language processing (NLP) methods may interfere with or affect such studies. We thus require a thorough analysis of potential gaps and limitations present in extant NLP resources. In this paper, on the methodological side, we demonstrate that existing NLP resources required several non-trivial modifications to quantify societal inequalities. On the substantive side, we find that while a large number of court cases perhaps suggest chan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;ChatGPT&#23545;&#25968;&#23383;&#21462;&#35777;&#39046;&#22495;&#30340;&#24433;&#21709;&#21644;&#28508;&#21147;&#65292;&#29305;&#21035;&#20851;&#27880;&#20854;&#26368;&#26032;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-4&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#21457;&#29616;&#20102;ChatGPT&#22312;&#25968;&#23383;&#21462;&#35777;&#29992;&#20363;&#20013;&#30340;&#20248;&#21183;&#21644;&#39118;&#38505;&#65292;&#24182;&#24471;&#20986;&#20102;&#19968;&#20123;&#24635;&#20307;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2307.10195</link><description>&lt;p&gt;
ChatGPT&#29992;&#20110;&#25968;&#23383;&#21462;&#35777;&#35843;&#26597;: &#22909;&#30340;&#65292;&#22351;&#30340;&#21644;&#26410;&#30693;&#30340; (arXiv:2307.10195v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
ChatGPT for Digital Forensic Investigation: The Good, The Bad, and The Unknown. (arXiv:2307.10195v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10195
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;ChatGPT&#23545;&#25968;&#23383;&#21462;&#35777;&#39046;&#22495;&#30340;&#24433;&#21709;&#21644;&#28508;&#21147;&#65292;&#29305;&#21035;&#20851;&#27880;&#20854;&#26368;&#26032;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-4&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#21457;&#29616;&#20102;ChatGPT&#22312;&#25968;&#23383;&#21462;&#35777;&#29992;&#20363;&#20013;&#30340;&#20248;&#21183;&#21644;&#39118;&#38505;&#65292;&#24182;&#24471;&#20986;&#20102;&#19968;&#20123;&#24635;&#20307;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31185;&#23398;&#30028;&#21644;&#31038;&#20250;&#20013;&#65292;ChatGPT (GPT-3.5&#65292;GPT-4) &#22312;&#21508;&#31181;&#39046;&#22495;&#30340;&#30772;&#22351;&#24615;&#24212;&#29992;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#24191;&#27867;&#35752;&#35770;&#30340;&#35805;&#39064;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;BERT&#12289;Bard&#12289;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPTs&#65289;&#12289;LLaMA&#31561;&#65292;&#20855;&#26377;&#26681;&#25454;&#22823;&#37327;&#25991;&#26412;&#35757;&#32451;&#25968;&#25454;&#25509;&#25910;&#29992;&#25143;&#25351;&#20196;&#25110;&#25552;&#31034;&#65292;&#24182;&#29983;&#25104;&#31572;&#26696;&#21644;&#35299;&#20915;&#26041;&#26696;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;ChatGPT&#23545;&#25968;&#23383;&#21462;&#35777;&#39046;&#22495;&#30340;&#24433;&#21709;&#21644;&#28508;&#21147;&#65292;&#29305;&#21035;&#20851;&#27880;&#20854;&#26368;&#26032;&#39044;&#35757;&#32451;LLM&#8212;&#8212;GPT-4&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;&#20854;&#22312;&#22810;&#20010;&#25968;&#23383;&#21462;&#35777;&#29992;&#20363;&#20013;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#35777;&#25454;&#29702;&#35299;&#12289;&#35777;&#25454;&#25628;&#32034;&#12289;&#20195;&#30721;&#29983;&#25104;&#12289;&#24322;&#24120;&#26816;&#27979;&#12289;&#20107;&#20214;&#21709;&#24212;&#21644;&#25945;&#32946;&#12290;&#25991;&#31456;&#38416;&#36848;&#20102;&#23427;&#22312;&#36825;&#20123;&#39046;&#22495;&#20013;&#30340;&#20248;&#21183;&#21644;&#39118;&#38505;&#65292;&#24182;&#24471;&#20986;&#20102;&#19968;&#20123;&#24635;&#20307;&#32467;&#35770;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#26412;&#25991;&#30340;&#32467;&#35770;&#26159;&#65292;&#34429;&#28982;&#26377;&#19968;&#20123;&#28508;&#22312;&#30340;&#20302;&#39118;&#38505;&#24212;&#29992;&#21487;&#33021;&#24615;&#65292;
&lt;/p&gt;
&lt;p&gt;
The disruptive application of ChatGPT (GPT-3.5, GPT-4) to a variety of domains has become a topic of much discussion in the scientific community and society at large. Large Language Models (LLMs), e.g., BERT, Bard, Generative Pre-trained Transformers (GPTs), LLaMA, etc., have the ability to take instructions, or prompts, from users and generate answers and solutions based on very large volumes of text-based training data. This paper assesses the impact and potential impact of ChatGPT on the field of digital forensics, specifically looking at its latest pre-trained LLM, GPT-4. A series of experiments are conducted to assess its capability across several digital forensic use cases including artefact understanding, evidence searching, code generation, anomaly detection, incident response, and education. Across these topics, its strengths and risks are outlined and a number of general conclusions are drawn. Overall this paper concludes that while there are some potential low-risk applicati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CrowdOpinion&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#27719;&#38598;&#26631;&#31614;&#20998;&#24067;&#20013;&#30456;&#20284;&#30340;&#39033;&#30446;&#65292;&#25581;&#31034;&#22312;&#32676;&#20307;&#20013;&#23384;&#22312;&#30340;&#26377;&#24847;&#20041;&#30340;&#35266;&#28857;&#20998;&#27495;&#65292;&#29305;&#21035;&#26159;&#22312;&#26631;&#27880;&#32773;&#20154;&#32676;&#20013;&#21487;&#33021;&#24050;&#32463;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#32676;&#20307;&#20013;&#12290;</title><link>http://arxiv.org/abs/2307.10189</link><description>&lt;p&gt;
&#20027;&#35266;&#25968;&#25454;&#30340;&#20027;&#35266;&#20154;&#32676;&#20998;&#27495;&#65306;&#36890;&#36807;&#32676;&#20307;&#32423;&#23398;&#20064;&#25581;&#31034;&#26377;&#24847;&#20041;&#30340;&#32676;&#20307;&#24847;&#35265;
&lt;/p&gt;
&lt;p&gt;
Subjective Crowd Disagreements for Subjective Data: Uncovering Meaningful CrowdOpinion with Population-level Learning. (arXiv:2307.10189v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CrowdOpinion&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#27719;&#38598;&#26631;&#31614;&#20998;&#24067;&#20013;&#30456;&#20284;&#30340;&#39033;&#30446;&#65292;&#25581;&#31034;&#22312;&#32676;&#20307;&#20013;&#23384;&#22312;&#30340;&#26377;&#24847;&#20041;&#30340;&#35266;&#28857;&#20998;&#27495;&#65292;&#29305;&#21035;&#26159;&#22312;&#26631;&#27880;&#32773;&#20154;&#32676;&#20013;&#21487;&#33021;&#24050;&#32463;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#32676;&#20307;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26631;&#27880;&#25968;&#25454;&#22312;AI&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#21253;&#25324;&#22788;&#29702;&#25913;&#21464;&#20154;&#20204;&#29983;&#27963;&#30340;&#20915;&#31574;&#25110;&#31649;&#29702;&#20154;&#31867;&#21019;&#24314;&#30340;&#32593;&#32476;/&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#30340;&#31995;&#32479;&#12290;&#20256;&#32479;&#19978;&#65292;&#22312;&#36827;&#34892;&#20219;&#20309;&#23398;&#20064;&#20043;&#21069;&#65292;&#20250;&#35299;&#20915;&#26631;&#27880;&#32773;&#20043;&#38388;&#30340;&#20998;&#27495;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#20154;&#21592;&#36234;&#26469;&#36234;&#22810;&#22320;&#35748;&#35782;&#21040;&#26631;&#27880;&#32773;&#20043;&#38388;&#30340;&#20998;&#27495;&#26159;&#26222;&#36941;&#23384;&#22312;&#19988;&#26377;&#24847;&#20041;&#30340;&#12290;&#20182;&#20204;&#36824;&#36136;&#30097;&#31995;&#32479;&#22312;&#26631;&#27880;&#32773;&#20998;&#27495;&#26102;&#30340;&#24615;&#33021;&#12290;&#23588;&#20854;&#26159;&#22312;&#24573;&#35270;&#23569;&#25968;&#35266;&#28857;&#26102;&#65292;&#23588;&#20854;&#26159;&#22312;&#26631;&#27880;&#32773;&#20154;&#32676;&#20013;&#21487;&#33021;&#24050;&#32463;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#32676;&#20307;&#20013;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;\emph{CrowdOpinion}&#65292;&#23427;&#20351;&#29992;&#35821;&#35328;&#29305;&#24449;&#21644;&#26631;&#31614;&#20998;&#24067;&#23558;&#30456;&#20284;&#30340;&#39033;&#30446;&#27719;&#38598;&#25104;&#36739;&#22823;&#30340;&#26631;&#31614;&#20998;&#24067;&#26679;&#26412;&#12290;&#25105;&#20204;&#23581;&#35797;&#20102;&#22235;&#31181;&#29983;&#25104;&#26041;&#27861;&#21644;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#20116;&#20010;&#26631;&#31614;&#20998;&#24067;&#21644;&#29305;&#24449;&#30340;&#32447;&#24615;&#32452;&#21512;&#12290;&#25105;&#20204;&#20351;&#29992;&#20116;&#20010;p
&lt;/p&gt;
&lt;p&gt;
Human-annotated data plays a critical role in the fairness of AI systems, including those that deal with life-altering decisions or moderating human-created web/social media content. Conventionally, annotator disagreements are resolved before any learning takes place. However, researchers are increasingly identifying annotator disagreement as pervasive and meaningful. They also question the performance of a system when annotators disagree. Particularly when minority views are disregarded, especially among groups that may already be underrepresented in the annotator population. In this paper, we introduce \emph{CrowdOpinion}\footnote{Accepted for publication at ACL 2023}, an unsupervised learning based approach that uses language features and label distributions to pool similar items into larger samples of label distributions. We experiment with four generative and one density-based clustering method, applied to five linear combinations of label distributions and features. We use five p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26159;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20960;&#20010;&#31867;&#21035;&#36827;&#34892;&#30340;&#31616;&#30701;&#35843;&#26597;&#65292;&#25552;&#20379;&#20102;&#21508;&#31867;LLM&#30340;&#26368;&#26032;&#21457;&#23637;&#21644;&#21162;&#21147;&#30340;&#27010;&#36848;&#65292;&#21253;&#25324;&#22810;&#35821;&#35328;LLMs&#12289;&#35270;&#35273;&#35821;&#35328;LLMs&#21644;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#31561;&#12290;&#21516;&#26102;&#65292;&#36824;&#31361;&#20986;&#20102;&#22312;&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#34394;&#25311;&#21161;&#25163;&#39046;&#22495;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#22914;&#25552;&#21319;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#21644;&#35299;&#20915;&#36947;&#24503;&#21644;&#27861;&#24459;&#22256;&#22659;&#12290;</title><link>http://arxiv.org/abs/2307.10188</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20960;&#20010;&#31867;&#21035;&#65306;&#31616;&#30701;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Several categories of Large Language Models (LLMs): A Short Survey. (arXiv:2307.10188v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10188
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20960;&#20010;&#31867;&#21035;&#36827;&#34892;&#30340;&#31616;&#30701;&#35843;&#26597;&#65292;&#25552;&#20379;&#20102;&#21508;&#31867;LLM&#30340;&#26368;&#26032;&#21457;&#23637;&#21644;&#21162;&#21147;&#30340;&#27010;&#36848;&#65292;&#21253;&#25324;&#22810;&#35821;&#35328;LLMs&#12289;&#35270;&#35273;&#35821;&#35328;LLMs&#21644;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#31561;&#12290;&#21516;&#26102;&#65292;&#36824;&#31361;&#20986;&#20102;&#22312;&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#34394;&#25311;&#21161;&#25163;&#39046;&#22495;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#22914;&#25552;&#21319;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#21644;&#35299;&#20915;&#36947;&#24503;&#21644;&#27861;&#24459;&#22256;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26377;&#25928;&#24037;&#20855;&#65292;&#24182;&#22312;&#35768;&#22810;&#19981;&#21516;&#39046;&#22495;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;&#26412;&#25991;&#23545;&#21508;&#31181;LLM&#23376;&#31867;&#36827;&#34892;&#20102;&#31616;&#27905;&#30340;&#24635;&#32467;&#12290;&#35813;&#35843;&#26597;&#24378;&#35843;&#20102;&#21508;&#31181;LLM&#31867;&#22411;&#30340;&#26368;&#26032;&#21457;&#23637;&#21644;&#21162;&#21147;&#65292;&#21253;&#25324;&#22522;&#20110;&#20219;&#21153;&#30340;&#37329;&#34701;LLM&#65292;&#22810;&#35821;&#35328;LLM&#65292;&#29983;&#29289;&#21307;&#23398;&#21644;&#20020;&#24202;LLM&#65292;&#35270;&#35273;&#35821;&#35328;LLM&#21644;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#12290;&#35813;&#35843;&#26597;&#23545;&#27599;&#20010;LLM&#31867;&#21035;&#20013;&#24212;&#29992;&#30340;&#26041;&#27861;&#12289;&#23646;&#24615;&#12289;&#25968;&#25454;&#38598;&#12289;&#21464;&#21387;&#22120;&#27169;&#22411;&#21644;&#27604;&#36739;&#25351;&#26631;&#36827;&#34892;&#20102;&#27010;&#36848;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#31361;&#20986;&#20102;&#22312;&#24320;&#21457;&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#34394;&#25311;&#21161;&#25163;&#39046;&#22495;&#23384;&#22312;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#65292;&#22914;&#25552;&#21319;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#65292;&#22686;&#24378;&#32842;&#22825;&#26426;&#22120;&#20154;&#26234;&#33021;&#24615;&#20197;&#21450;&#35299;&#20915;&#36947;&#24503;&#21644;&#27861;&#24459;&#22256;&#22659;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#20026;&#23545;&#22522;&#20110;LLM&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#34394;&#25311;&#26234;&#33021;&#21161;&#25163;&#25216;&#26415;&#24863;&#20852;&#36259;&#30340;&#35835;&#32773;&#12289;&#24320;&#21457;&#20154;&#21592;&#12289;&#23398;&#26415;&#30028;&#20154;&#22763;&#21644;&#29992;&#25143;&#25552;&#20379;&#26377;&#29992;&#30340;&#20449;&#24687;&#21644;&#26410;&#26469;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models(LLMs)have become effective tools for natural language processing and have been used in many different fields. This essay offers a succinct summary of various LLM subcategories. The survey emphasizes recent developments and efforts made for various LLM kinds, including task-based financial LLMs, multilingual language LLMs, biomedical and clinical LLMs, vision language LLMs, and code language models. The survey gives a general summary of the methods, attributes, datasets, transformer models, and comparison metrics applied in each category of LLMs. Furthermore, it highlights unresolved problems in the field of developing chatbots and virtual assistants, such as boosting natural language processing, enhancing chatbot intelligence, and resolving moral and legal dilemmas. The purpose of this study is to provide readers, developers, academics, and users interested in LLM-based chatbots and virtual intelligent assistant technologies with useful information and future dire
&lt;/p&gt;</description></item><item><title>DialogStudio&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#19988;&#26368;&#22810;&#26679;&#21270;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#21512;&#65292;&#21253;&#21547;&#20174;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#21040;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#12289;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#20250;&#35805;&#25512;&#33616;&#12289;&#23545;&#35805;&#25688;&#35201;&#21644;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#30340;&#25968;&#25454;&#12290;&#23427;&#20026;&#23545;&#35805;&#30740;&#31350;&#21644;&#27169;&#22411;&#35757;&#32451;&#25552;&#20379;&#20102;&#20016;&#23500;&#32780;&#22810;&#26679;&#21270;&#30340;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2307.10172</link><description>&lt;p&gt;
DialogStudio&#65306;&#38754;&#21521;&#20250;&#35805; AI &#30340;&#26368;&#20016;&#23500;&#21644;&#26368;&#22810;&#26679;&#21270;&#30340;&#32479;&#19968;&#25968;&#25454;&#38598;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
DialogStudio: Towards Richest and Most Diverse Unified Dataset Collection for Conversational AI. (arXiv:2307.10172v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10172
&lt;/p&gt;
&lt;p&gt;
DialogStudio&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#19988;&#26368;&#22810;&#26679;&#21270;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#21512;&#65292;&#21253;&#21547;&#20174;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#21040;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#12289;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#20250;&#35805;&#25512;&#33616;&#12289;&#23545;&#35805;&#25688;&#35201;&#21644;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#30340;&#25968;&#25454;&#12290;&#23427;&#20026;&#23545;&#35805;&#30740;&#31350;&#21644;&#27169;&#22411;&#35757;&#32451;&#25552;&#20379;&#20102;&#20016;&#23500;&#32780;&#22810;&#26679;&#21270;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20250;&#35805; AI &#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#22810;&#26679;&#21270;&#30340;&#23545;&#35805;&#20219;&#21153;&#26102;&#38754;&#20020;&#25361;&#25112;&#65292;&#29616;&#26377;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#24448;&#24448;&#32570;&#20047;&#22810;&#26679;&#24615;&#21644;&#20840;&#38754;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; DialogStudio&#65306;&#26368;&#22823;&#12289;&#26368;&#22810;&#26679;&#21270;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#38598;&#21512;&#65292;&#20197;&#19968;&#33268;&#30340;&#26684;&#24335;&#32479;&#19968;&#65292;&#21516;&#26102;&#20445;&#30041;&#20854;&#21407;&#22987;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#38598;&#21512;&#21253;&#25324;&#26469;&#33258;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#12289;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#12289;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#20250;&#35805;&#25512;&#33616;&#12289;&#23545;&#35805;&#25688;&#35201;&#21644;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#30340;&#25968;&#25454;&#65292;&#20026;&#23545;&#35805;&#30740;&#31350;&#21644;&#27169;&#22411;&#35757;&#32451;&#25552;&#20379;&#20102;&#38750;&#24120;&#20016;&#23500;&#21644;&#22810;&#26679;&#21270;&#30340;&#36164;&#28304;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378; DialogStudio &#30340;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#20026;&#27599;&#20010;&#25968;&#25454;&#38598;&#30830;&#23450;&#20102;&#35768;&#21487;&#35777;&#65292;&#24182;&#20026;&#36873;&#23450;&#23545;&#35805;&#35774;&#35745;&#20102;&#39046;&#22495;&#24863;&#30693;&#25552;&#31034;&#65292;&#20197;&#20415;&#20419;&#36827;&#25351;&#23548;&#24863;&#30693;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#25968;&#25454;&#38598;&#38598;&#21512;&#24320;&#21457;&#20102;&#20250;&#35805; AI &#27169;&#22411;&#65292;&#24182;&#22312;&#38646;&#25688;&#35201;&#29983;&#25104;&#21644;&#20998;&#24067;&#24335;&#25991;&#23383;&#22522;&#20934;&#23545;&#35805;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite advancements in conversational AI, language models encounter challenges to handle diverse conversational tasks, and existing dialogue dataset collections often lack diversity and comprehensiveness. To tackle these issues, we introduce DialogStudio: the largest and most diverse collection of dialogue datasets, unified under a consistent format while preserving their original information. Our collection encompasses data from open-domain dialogues, task-oriented dialogues, natural language understanding, conversational recommendation, dialogue summarization, and knowledge-grounded dialogues, making it an incredibly rich and diverse resource for dialogue research and model training. To further enhance the utility of DialogStudio, we identify the licenses for each dataset and design domain-aware prompts for selected dialogues to facilitate instruction-aware fine-tuning. Furthermore, we develop conversational AI models using the dataset collection, and our experiments in both zero-sh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#25506;&#32034;&#20102;LLMs&#26159;&#21542;&#21487;&#20197;&#22797;&#21046;&#26356;&#22797;&#26434;&#30340;&#20247;&#21253;&#27969;&#27700;&#32447;&#65292;&#24182;&#21457;&#29616;&#29616;&#20195;LLMs&#22312;&#27169;&#25311;&#20154;&#31867;&#35745;&#31639;&#31639;&#27861;&#20013;&#30340;&#33021;&#21147;&#19978;&#26377;&#19968;&#23450;&#30340;&#25104;&#21151;&#65292;&#20294;&#21463;&#22810;&#31181;&#22240;&#32032;&#24433;&#21709;&#12290;&#25991;&#31456;&#24378;&#35843;&#20102;&#20026;LLMs&#25552;&#20379;&#20154;&#31867;&#38754;&#21521;&#30340;&#23433;&#20840;&#20445;&#38556;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#35757;&#32451;&#20154;&#31867;&#21644;LLMs&#20114;&#34917;&#25216;&#33021;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.10168</link><description>&lt;p&gt;
LLM&#20316;&#20026;&#20154;-&#35745;&#31639;&#31639;&#27861;&#20013;&#30340;&#24037;&#20316;&#32773;&#65311;&#29992;LLM&#22797;&#21046;&#20247;&#21253;&#27969;&#27700;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLMs as Workers in Human-Computational Algorithms? Replicating Crowdsourcing Pipelines with LLMs. (arXiv:2307.10168v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#25506;&#32034;&#20102;LLMs&#26159;&#21542;&#21487;&#20197;&#22797;&#21046;&#26356;&#22797;&#26434;&#30340;&#20247;&#21253;&#27969;&#27700;&#32447;&#65292;&#24182;&#21457;&#29616;&#29616;&#20195;LLMs&#22312;&#27169;&#25311;&#20154;&#31867;&#35745;&#31639;&#31639;&#27861;&#20013;&#30340;&#33021;&#21147;&#19978;&#26377;&#19968;&#23450;&#30340;&#25104;&#21151;&#65292;&#20294;&#21463;&#22810;&#31181;&#22240;&#32032;&#24433;&#21709;&#12290;&#25991;&#31456;&#24378;&#35843;&#20102;&#20026;LLMs&#25552;&#20379;&#20154;&#31867;&#38754;&#21521;&#30340;&#23433;&#20840;&#20445;&#38556;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#35757;&#32451;&#20154;&#31867;&#21644;LLMs&#20114;&#34917;&#25216;&#33021;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#20247;&#21253;&#20219;&#21153;&#20013;&#22797;&#21046;&#20154;&#31867;&#34892;&#20026;&#30340;&#28508;&#21147;&#65292;&#32780;&#36825;&#20123;&#20219;&#21153;&#20197;&#21069;&#34987;&#35748;&#20026;&#21482;&#26377;&#20154;&#31867;&#25165;&#33021;&#23436;&#25104;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#31616;&#21333;&#30340;&#21407;&#23376;&#20219;&#21153;&#19978;&#12290;&#25105;&#20204;&#25506;&#32034;LLM&#26159;&#21542;&#21487;&#20197;&#22797;&#21046;&#26356;&#22797;&#26434;&#30340;&#20247;&#21253;&#27969;&#27700;&#32447;&#12290;&#25105;&#20204;&#21457;&#29616;&#29616;&#20195;LLM&#21487;&#20197;&#27169;&#25311;&#26576;&#20123;&#20247;&#21253;&#24037;&#20316;&#32773;&#22312;&#36825;&#20123;&#8220;&#20154;&#31867;&#35745;&#31639;&#31639;&#27861;&#8221;&#20013;&#30340;&#33021;&#21147;&#65292;&#20294;&#25104;&#21151;&#30340;&#31243;&#24230;&#26159;&#21487;&#21464;&#30340;&#65292;&#24182;&#21463;&#21040;&#35831;&#27714;&#32773;&#23545;LLM&#33021;&#21147;&#30340;&#29702;&#35299;&#12289;&#23376;&#20219;&#21153;&#25152;&#38656;&#30340;&#29305;&#23450;&#25216;&#33021;&#20197;&#21450;&#25191;&#34892;&#36825;&#20123;&#23376;&#20219;&#21153;&#30340;&#26368;&#20339;&#20132;&#20114;&#26041;&#24335;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21453;&#24605;&#20102;&#20154;&#31867;&#21644;LLM&#23545;&#25351;&#31034;&#30340;&#19981;&#21516;&#25935;&#24863;&#24615;&#65292;&#24378;&#35843;&#20026;LLM&#25552;&#20379;&#38754;&#21521;&#20154;&#31867;&#30340;&#23433;&#20840;&#20445;&#38556;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#35757;&#32451;&#20855;&#26377;&#20114;&#34917;&#25216;&#33021;&#30340;&#20154;&#31867;&#21644;LLM&#30340;&#28508;&#21147;&#12290;&#20851;&#38190;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22797;&#21046;&#20247;&#21253;&#27969;&#27700;&#32447;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#20215;&#20540;&#30340;&#24179;&#21488;&#26469;&#30740;&#31350;LLM&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;&#30456;&#23545;&#20248;&#21183;&#65288;&#36890;&#36807;&#20132;&#21449;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
LLMs have shown promise in replicating human-like behavior in crowdsourcing tasks that were previously thought to be exclusive to human abilities. However, current efforts focus mainly on simple atomic tasks. We explore whether LLMs can replicate more complex crowdsourcing pipelines. We find that modern LLMs can simulate some of crowdworkers' abilities in these "human computation algorithms," but the level of success is variable and influenced by requesters' understanding of LLM capabilities, the specific skills required for sub-tasks, and the optimal interaction modality for performing these sub-tasks. We reflect on human and LLMs' different sensitivities to instructions, stress the importance of enabling human-facing safeguards for LLMs, and discuss the potential of training humans and LLMs with complementary skill sets. Crucially, we show that replicating crowdsourcing pipelines offers a valuable platform to investigate (1) the relative strengths of LLMs on different tasks (by cross
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#20351;&#29992;&#27491;&#21017;&#34920;&#36798;&#24335;&#21644;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#26469;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#25991;&#26412;&#29983;&#25104;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.09702</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;LLM&#24341;&#23548;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Efficient Guided Generation for LLMs. (arXiv:2307.09702v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#20351;&#29992;&#27491;&#21017;&#34920;&#36798;&#24335;&#21644;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#26469;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#25991;&#26412;&#29983;&#25104;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#20351;&#29992;&#27491;&#21017;&#34920;&#36798;&#24335;&#21644;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#26469;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#25991;&#26412;&#29983;&#25104;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26631;&#35760;&#24207;&#21015;&#29983;&#25104;&#36807;&#31243;&#20013;&#20960;&#20046;&#19981;&#22686;&#21152;&#20219;&#20309;&#24320;&#38144;&#65292;&#24182;&#20351;&#24471;&#24341;&#23548;&#29983;&#25104;&#22312;&#23454;&#38469;&#20013;&#21487;&#34892;&#12290;&#22312;&#24320;&#28304;Python&#24211;Outlines&#20013;&#25552;&#20379;&#20102;&#19968;&#20010;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this article we describe an efficient approach to guiding language model text generation with regular expressions and context-free grammars. Our approach adds little to no overhead to the token sequence generation process, and makes guided generation feasible in practice. An implementation is provided in the open source Python library Outlines.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;ChatGPT&#21644;Bing Chat&#22312;&#28385;&#36275;&#36234;&#21335;&#23398;&#29983;&#38656;&#27714;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;Bing Chat&#22312;&#38500;&#25991;&#23398;&#22806;&#30340;&#22810;&#20010;&#23398;&#31185;&#34920;&#29616;&#20248;&#20110;ChatGPT&#12290;Bing Chat&#37319;&#29992;&#26356;&#20808;&#36827;&#30340;GPT-4&#25216;&#26415;&#65292;&#33021;&#22815;&#25552;&#39640;&#25991;&#26412;&#30340;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#29983;&#25104;&#21019;&#36896;&#24615;&#12289;&#20449;&#24687;&#20016;&#23500;&#30340;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2307.08272</link><description>&lt;p&gt;
ChatGPT&#24456;&#22909;&#65292;&#20294;&#23545;&#20110;&#36234;&#21335;&#23398;&#29983;&#26469;&#35828;&#65292;Bing Chat&#26356;&#22909;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is Good but Bing Chat is Better for Vietnamese Students. (arXiv:2307.08272v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;ChatGPT&#21644;Bing Chat&#22312;&#28385;&#36275;&#36234;&#21335;&#23398;&#29983;&#38656;&#27714;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;Bing Chat&#22312;&#38500;&#25991;&#23398;&#22806;&#30340;&#22810;&#20010;&#23398;&#31185;&#34920;&#29616;&#20248;&#20110;ChatGPT&#12290;Bing Chat&#37319;&#29992;&#26356;&#20808;&#36827;&#30340;GPT-4&#25216;&#26415;&#65292;&#33021;&#22815;&#25552;&#39640;&#25991;&#26412;&#30340;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#29983;&#25104;&#21019;&#36896;&#24615;&#12289;&#20449;&#24687;&#20016;&#23500;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20004;&#20010;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21363;ChatGPT&#21644;&#24494;&#36719;Bing Chat&#65288;BingChat&#65289;&#65292;&#22312;&#28385;&#36275;&#36234;&#21335;&#23398;&#29983;&#38656;&#27714;&#26041;&#38754;&#30340;&#21151;&#25928;&#12290;&#23613;&#31649;ChatGPT&#22312;&#22810;&#20010;&#23398;&#31185;&#20013;&#23637;&#29616;&#20102;&#39640;&#27700;&#20934;&#30340;&#33021;&#21147;&#65292;&#20294;Bing Chat&#34987;&#35748;&#20026;&#26159;&#26356;&#26377;&#20248;&#21183;&#30340;&#36873;&#25321;&#12290;&#25105;&#20204;&#23545;&#23427;&#20204;&#22312;&#25968;&#23398;&#12289;&#25991;&#23398;&#12289;&#33521;&#35821;&#12289;&#29289;&#29702;&#12289;&#21270;&#23398;&#12289;&#29983;&#29289;&#12289;&#21382;&#21490;&#12289;&#22320;&#29702;&#21644;&#20844;&#27665;&#25945;&#32946;&#31561;&#21508;&#20010;&#23398;&#31185;&#30340;&#23398;&#19994;&#25104;&#23601;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;BingChat&#22312;&#22810;&#20010;&#23398;&#31185;&#19978;&#26174;&#31034;&#20986;&#27604;ChatGPT&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#21807;&#29420;&#22312;&#25991;&#23398;&#26041;&#38754;&#65292;ChatGPT&#30340;&#34920;&#29616;&#26356;&#22909;&#19968;&#20123;&#12290;&#27492;&#22806;&#65292;&#19982;&#22522;&#20110;GPT-3.5&#26500;&#24314;&#30340;ChatGPT&#30456;&#27604;&#65292;BingChat&#37319;&#29992;&#20102;&#26356;&#20808;&#36827;&#30340;GPT-4&#25216;&#26415;&#65292;&#36825;&#20351;&#20854;&#33021;&#22815;&#25552;&#39640;&#25991;&#26412;&#30340;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#29983;&#25104;&#21019;&#36896;&#24615;&#12289;&#20449;&#24687;&#20016;&#23500;&#30340;&#25991;&#26412;&#12290;&#27492;&#22806;&#65292;BingChat&#22312;&#36234;&#21335;&#22320;&#21306;&#21487;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study examines the efficacy of two SOTA large language models (LLMs), namely ChatGPT and Microsoft Bing Chat (BingChat), in catering to the needs of Vietnamese students. Although ChatGPT exhibits proficiency in multiple disciplines, Bing Chat emerges as the more advantageous option. We conduct a comparative analysis of their academic achievements in various disciplines, encompassing mathematics, literature, English language, physics, chemistry, biology, history, geography, and civic education. The results of our study suggest that BingChat demonstrates superior performance compared to ChatGPT across a wide range of subjects, with the exception of literature, where ChatGPT exhibits better performance. Additionally, BingChat utilizes the more advanced GPT-4 technology in contrast to ChatGPT, which is built upon GPT-3.5. This allows BingChat to improve to comprehension, reasoning and generation of creative and informative text. Moreover, the fact that BingChat is accessible in Vietna
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#20196;&#29260;&#32423;&#21035;&#21644;&#36328;&#24230;&#32423;&#21035;&#30417;&#30563;&#30340;CDAP&#32593;&#32476;&#29992;&#20110;&#23569;&#26679;&#26412;&#24207;&#21015;&#26631;&#27880;&#65292;&#36890;&#36807;&#22312;&#19981;&#21516;&#31890;&#24230;&#19978;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#21644;&#19968;&#33268;&#25439;&#22833;&#65292;&#23454;&#29616;&#20102;&#20004;&#20010;&#32593;&#32476;&#30340;&#21327;&#21516;&#23398;&#20064;&#65292;&#22312;&#25512;&#29702;&#38454;&#27573;&#20351;&#29992;&#20102;&#19968;&#33268;&#36138;&#23146;&#25512;&#29702;&#31639;&#27861;&#26469;&#36873;&#25321;&#38750;&#37325;&#21472;&#36328;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.07946</link><description>&lt;p&gt;
&#32852;&#21512;&#20196;&#29260;&#32423;&#21035;&#21644;&#36328;&#24230;&#32423;&#21035;&#30340;&#30417;&#30563;&#29992;&#20110;&#23569;&#26679;&#26412;&#24207;&#21015;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
Unifying Token and Span Level Supervisions for Few-Shot Sequence Labeling. (arXiv:2307.07946v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07946
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#20196;&#29260;&#32423;&#21035;&#21644;&#36328;&#24230;&#32423;&#21035;&#30417;&#30563;&#30340;CDAP&#32593;&#32476;&#29992;&#20110;&#23569;&#26679;&#26412;&#24207;&#21015;&#26631;&#27880;&#65292;&#36890;&#36807;&#22312;&#19981;&#21516;&#31890;&#24230;&#19978;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#21644;&#19968;&#33268;&#25439;&#22833;&#65292;&#23454;&#29616;&#20102;&#20004;&#20010;&#32593;&#32476;&#30340;&#21327;&#21516;&#23398;&#20064;&#65292;&#22312;&#25512;&#29702;&#38454;&#27573;&#20351;&#29992;&#20102;&#19968;&#33268;&#36138;&#23146;&#25512;&#29702;&#31639;&#27861;&#26469;&#36873;&#25321;&#38750;&#37325;&#21472;&#36328;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#24207;&#21015;&#26631;&#27880;&#26088;&#22312;&#20165;&#20381;&#25454;&#23569;&#37327;&#26631;&#27880;&#26679;&#26412;&#26469;&#35782;&#21035;&#26032;&#30340;&#31867;&#21035;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#36890;&#36807;&#35774;&#35745;&#22522;&#20110;&#24230;&#37327;&#23398;&#20064;&#30340;&#20196;&#29260;&#32423;&#21035;&#25110;&#36328;&#24230;&#32423;&#21035;&#26631;&#27880;&#27169;&#22411;&#26469;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21482;&#22312;&#21333;&#19968;&#31890;&#24230;&#19978;&#35757;&#32451;&#65288;&#21363;&#20196;&#29260;&#32423;&#21035;&#25110;&#36328;&#24230;&#32423;&#21035;&#65289;&#65292;&#24182;&#19988;&#20855;&#26377;&#30456;&#24212;&#31890;&#24230;&#30340;&#19968;&#20123;&#24369;&#28857;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#32479;&#19968;&#20102;&#20196;&#29260;&#32423;&#21035;&#21644;&#36328;&#24230;&#32423;&#21035;&#30340;&#30417;&#30563;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#33268;&#21452;&#33258;&#36866;&#24212;&#21407;&#22411;&#32593;&#32476;&#65288;CDAP&#65289;&#29992;&#20110;&#23569;&#26679;&#26412;&#24207;&#21015;&#26631;&#27880;&#12290;CDAP&#21253;&#21547;&#20196;&#29260;&#32423;&#21035;&#21644;&#36328;&#24230;&#32423;&#21035;&#30340;&#32593;&#32476;&#65292;&#22312;&#19981;&#21516;&#31890;&#24230;&#19978;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#12290;&#20026;&#20102;&#20351;&#20004;&#20010;&#32593;&#32476;&#30340;&#36755;&#20986;&#20445;&#25345;&#19968;&#33268;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#33268;&#25439;&#22833;&#65292;&#20351;&#23427;&#20204;&#21487;&#20197;&#20114;&#30456;&#23398;&#20064;&#12290;&#22312;&#25512;&#29702;&#38454;&#27573;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#33268;&#36138;&#23146;&#25512;&#29702;&#31639;&#27861;&#65292;&#39318;&#20808;&#35843;&#25972;&#39044;&#27979;&#27010;&#29575;&#65292;&#28982;&#21518;&#36138;&#23146;&#22320;&#36873;&#25321;&#20855;&#26377;&#26368;&#22823;&#27010;&#29575;&#30340;&#38750;&#37325;&#21472;&#36328;&#24230;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
Few-shot sequence labeling aims to identify novel classes based on only a few labeled samples. Existing methods solve the data scarcity problem mainly by designing token-level or span-level labeling models based on metric learning. However, these methods are only trained at a single granularity (i.e., either token level or span level) and have some weaknesses of the corresponding granularity. In this paper, we first unify token and span level supervisions and propose a Consistent Dual Adaptive Prototypical (CDAP) network for few-shot sequence labeling. CDAP contains the token-level and span-level networks, jointly trained at different granularities. To align the outputs of two networks, we further propose a consistent loss to enable them to learn from each other. During the inference phase, we propose a consistent greedy inference algorithm that first adjusts the predicted probability and then greedily selects non-overlapping spans with maximum probability. Extensive experiments show t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;OpenAI ChatGPT&#12289;Microsoft Bing Chat&#21644;Google Bard&#36825;&#19977;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;VNHSGE&#33521;&#25991;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#32467;&#26524;&#26174;&#31034;Bing Chat&#20248;&#20110;ChatGPT&#21644;Bard&#12290;&#30740;&#31350;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#22312;&#33521;&#35821;&#35821;&#35328;&#25945;&#32946;&#20013;&#20855;&#26377;&#28508;&#21147;&#65292;&#21487;&#20197;&#20316;&#20026;&#39640;&#20013;&#33521;&#35821;&#25945;&#23398;&#21644;&#23398;&#20064;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2307.02288</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;VNHSGE&#33521;&#25991;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#27604;&#36739;&#65306;OpenAI ChatGPT&#12289;Microsoft Bing Chat&#21644;Google Bard
&lt;/p&gt;
&lt;p&gt;
Performance Comparison of Large Language Models on VNHSGE English Dataset: OpenAI ChatGPT, Microsoft Bing Chat, and Google Bard. (arXiv:2307.02288v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02288
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;OpenAI ChatGPT&#12289;Microsoft Bing Chat&#21644;Google Bard&#36825;&#19977;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;VNHSGE&#33521;&#25991;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#32467;&#26524;&#26174;&#31034;Bing Chat&#20248;&#20110;ChatGPT&#21644;Bard&#12290;&#30740;&#31350;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#22312;&#33521;&#35821;&#35821;&#35328;&#25945;&#32946;&#20013;&#20855;&#26377;&#28508;&#21147;&#65292;&#21487;&#20197;&#20316;&#20026;&#39640;&#20013;&#33521;&#35821;&#25945;&#23398;&#21644;&#23398;&#20064;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19977;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20998;&#21035;&#26159;OpenAI ChatGPT&#12289;Microsoft Bing Chat&#21644;Google Bard&#65292;&#22312;VNHSGE&#33521;&#25991;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;Bing Chat&#20248;&#20110;ChatGPT&#21644;Bard&#12290;&#22240;&#27492;&#65292;&#22312;ChatGPT&#23578;&#26410;&#22312;&#36234;&#21335;&#27491;&#24335;&#21457;&#24067;&#20043;&#21069;&#65292;Bing Chat&#21644;Bard&#21487;&#20197;&#26367;&#20195;&#23427;&#12290;&#30740;&#31350;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;ChatGPT&#12289;Bing Chat&#21644;Bard&#22312;&#33521;&#35821;&#35821;&#35328;&#33021;&#21147;&#26041;&#38754;&#36229;&#36807;&#20102;&#36234;&#21335;&#23398;&#29983;&#12290;&#26412;&#30740;&#31350;&#30340;&#21457;&#29616;&#26377;&#21161;&#20110;&#29702;&#35299;LLMs&#22312;&#33521;&#35821;&#35821;&#35328;&#25945;&#32946;&#20013;&#30340;&#28508;&#21147;&#12290;ChatGPT&#12289;Bing Chat&#21644;Bard&#30340;&#20986;&#33394;&#34920;&#29616;&#35777;&#26126;&#20102;&#23427;&#20204;&#20316;&#20026;&#39640;&#20013;&#33521;&#35821;&#25945;&#23398;&#21644;&#23398;&#20064;&#30340;&#26377;&#25928;&#24037;&#20855;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a performance comparison of three large language models (LLMs), namely OpenAI ChatGPT, Microsoft Bing Chat, and Google Bard, on the VNHSGE English dataset. The results show that BingChat is better than ChatGPT and Bard. Therefore, BingChat and Bard can replace ChatGPT while ChatGPT is not yet officially available in Vietnam. The results also indicate that ChatGPT, Bing Chat, and Bard outperform Vietnamese students in English language proficiency. The findings of this study contribute to the understanding of the potential of LLMs in English language education. The remarkable performance of ChatGPT, Bing Chat, and Bard demonstrates their potential as effective tools for teaching and learning English at the high school level.
&lt;/p&gt;</description></item><item><title>PatternGPT&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#24335;&#39537;&#21160;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25991;&#26412;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#21462;&#33021;&#21147;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#27169;&#24335;&#65292;&#24182;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#30340;&#24605;&#24819;&#23454;&#29616;&#27169;&#24335;&#20849;&#20139;&#65292;&#26368;&#32456;&#36890;&#36807;&#25628;&#32034;&#39640;&#36136;&#37327;&#27169;&#24335;&#25351;&#23548;&#29983;&#25104;&#27169;&#22411;&#12290;&#35813;&#26694;&#26550;&#20855;&#26377;&#29983;&#25104;&#22810;&#26679;&#21270;&#27169;&#24335;&#12289;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12289;&#32467;&#21512;&#22806;&#37096;&#30693;&#35782;&#31561;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.00470</link><description>&lt;p&gt;
PatternGPT: &#19968;&#31181;&#22522;&#20110;&#27169;&#24335;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25991;&#26412;&#29983;&#25104;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
PatternGPT :A Pattern-Driven Framework for Large Language Model Text Generation. (arXiv:2307.00470v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00470
&lt;/p&gt;
&lt;p&gt;
PatternGPT&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#24335;&#39537;&#21160;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25991;&#26412;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#21462;&#33021;&#21147;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#27169;&#24335;&#65292;&#24182;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#30340;&#24605;&#24819;&#23454;&#29616;&#27169;&#24335;&#20849;&#20139;&#65292;&#26368;&#32456;&#36890;&#36807;&#25628;&#32034;&#39640;&#36136;&#37327;&#27169;&#24335;&#25351;&#23548;&#29983;&#25104;&#27169;&#22411;&#12290;&#35813;&#26694;&#26550;&#20855;&#26377;&#29983;&#25104;&#22810;&#26679;&#21270;&#27169;&#24335;&#12289;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12289;&#32467;&#21512;&#22806;&#37096;&#30693;&#35782;&#31561;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#65292;&#33021;&#22815;&#20026;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#29983;&#25104;&#27969;&#30021;&#30340;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#20851;&#38190;&#20219;&#21153;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#23481;&#26131;&#20986;&#29616;&#24187;&#35273;&#65292;&#24182;&#19988;&#26080;&#27861;&#30452;&#25509;&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#12290;&#20026;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;PatternGPT&#65292;&#19968;&#31181;&#22522;&#20110;&#27169;&#24335;&#39537;&#21160;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25991;&#26412;&#29983;&#25104;&#26694;&#26550;&#12290;&#39318;&#20808;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#21462;&#33021;&#21147;&#29983;&#25104;&#20016;&#23500;&#22810;&#26679;&#30340;&#27169;&#24335;&#65292;&#28982;&#21518;&#20511;&#37492;&#32852;&#37030;&#23398;&#20064;&#30340;&#24605;&#24819;&#65292;&#20351;&#29992;&#22810;&#20010;&#20195;&#29702;&#23454;&#29616;&#20849;&#20139;&#20197;&#33719;&#21462;&#26356;&#22810;&#26679;&#30340;&#27169;&#24335;&#12290;&#26368;&#21518;&#65292;&#23427;&#20351;&#29992;&#21028;&#26029;&#26631;&#20934;&#21644;&#20248;&#21270;&#31639;&#27861;&#25628;&#32034;&#39640;&#36136;&#37327;&#30340;&#27169;&#24335;&#65292;&#24182;&#20351;&#29992;&#25628;&#32034;&#21040;&#30340;&#27169;&#24335;&#25351;&#23548;&#27169;&#22411;&#36827;&#34892;&#29983;&#25104;&#12290;&#35813;&#26694;&#26550;&#20855;&#26377;&#29983;&#25104;&#22810;&#26679;&#21270;&#27169;&#24335;&#12289;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12289;&#32467;&#21512;&#22806;&#37096;&#30693;&#35782;&#31561;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models(LLMS) have shown excellent text generation capabilities,capable of generating fluent responses for many downstream tasks. However,applying large language models to real-world critical tasks remains challenging due to their susceptibility to hallucinations and inability to directly use external knowledge. To address the above challenges,this paper proposes PatternGPT, a pattern-driven text generation framework for large language models. First,the framework utilizes the extraction capabilities of large language models to generate rich and diverse patterns and later draws on the idea of federated learning. Using multiple agents to achieve sharing to obtain more diverse patterns. Finally, it searches for high-quality patterns using judgment criteria and optimization algorithms and uses the searched patterns to guide the model for generation. This framework has the advantages of generating diversified patterns, protecting data privacy,combining external knowledge, and 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22522;&#20110;&#23454;&#20307;&#30340;&#20851;&#32852;&#27169;&#22411;&#65288;EBRM&#65289;&#30340;&#26032;&#27169;&#22411;&#65292;&#23558;&#26597;&#35810;-&#21830;&#21697;&#20851;&#32852;&#38382;&#39064;&#20998;&#35299;&#20026;&#22810;&#20010;&#26597;&#35810;-&#23454;&#20307;&#20851;&#32852;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#36719;&#36923;&#36753;&#32858;&#21512;&#32467;&#26524;&#65292;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#25512;&#29702;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.00370</link><description>&lt;p&gt;
&#25552;&#21319;&#30005;&#23376;&#21830;&#21153;&#25628;&#32034;&#20013;&#30340;&#25991;&#26412;&#21305;&#37197;&#33021;&#21147;&#65306;&#22522;&#20110;&#21487;&#29702;&#35299;&#12289;&#21487;&#24178;&#39044;&#21644;&#24555;&#36895;&#23454;&#20307;&#20851;&#32852;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Improving Text Matching in E-Commerce Search with A Rationalizable, Intervenable and Fast Entity-Based Relevance Model. (arXiv:2307.00370v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22522;&#20110;&#23454;&#20307;&#30340;&#20851;&#32852;&#27169;&#22411;&#65288;EBRM&#65289;&#30340;&#26032;&#27169;&#22411;&#65292;&#23558;&#26597;&#35810;-&#21830;&#21697;&#20851;&#32852;&#38382;&#39064;&#20998;&#35299;&#20026;&#22810;&#20010;&#26597;&#35810;-&#23454;&#20307;&#20851;&#32852;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#36719;&#36923;&#36753;&#32858;&#21512;&#32467;&#26524;&#65292;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#25512;&#29702;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30005;&#23376;&#21830;&#21153;&#25628;&#32034;&#31995;&#32479;&#20013;&#65292;&#20174;&#22823;&#37327;&#21830;&#21697;&#20013;&#21457;&#29616;&#29992;&#25143;&#26597;&#35810;&#30340;&#30446;&#26631;&#21830;&#21697;&#26159;&#20027;&#35201;&#30446;&#26631;&#20043;&#19968;&#12290;&#20851;&#32852;&#39044;&#27979;&#23545;&#20110;&#25628;&#32034;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#26377;&#21161;&#20110;&#25552;&#39640;&#24615;&#33021;&#12290;&#30446;&#21069;&#65292;&#24191;&#27867;&#20351;&#29992;&#30340;&#27169;&#22411;&#22914;Bi-encoder&#21644;Cross-encoder&#20998;&#21035;&#22312;&#20934;&#30830;&#24615;&#25110;&#25512;&#29702;&#36895;&#24230;&#19978;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65292;&#31216;&#20026;&#22522;&#20110;&#23454;&#20307;&#30340;&#20851;&#32852;&#27169;&#22411;&#65288;EBRM&#65289;&#12290;&#25105;&#20204;&#35782;&#21035;&#21830;&#21697;&#20013;&#21253;&#21547;&#30340;&#23454;&#20307;&#65292;&#24182;&#23558;QI&#65288;&#26597;&#35810;-&#21830;&#21697;&#65289;&#20851;&#32852;&#38382;&#39064;&#20998;&#35299;&#20026;&#22810;&#20010;QE&#65288;&#26597;&#35810;-&#23454;&#20307;&#65289;&#20851;&#32852;&#38382;&#39064;&#65307;&#28982;&#21518;&#20351;&#29992;&#36719;&#36923;&#36753;&#24418;&#24335;&#32858;&#21512;&#20854;&#32467;&#26524;&#20197;&#24418;&#25104;QI&#30340;&#39044;&#27979;&#12290;&#20998;&#35299;&#20801;&#35768;&#25105;&#20204;&#20351;&#29992;Cross-encoder QE&#20851;&#32852;&#27169;&#22359;&#20197;&#33719;&#24471;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#20026;&#24555;&#36895;&#22312;&#32447;&#25512;&#29702;&#32531;&#23384;QE&#39044;&#27979;&#12290;&#21033;&#29992;&#36719;&#36923;&#36753;&#20351;&#39044;&#27979;&#36807;&#31243;&#21487;&#35299;&#37322;&#24615;&#39640;&#24182;&#20855;&#26377;&#24178;&#39044;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering the intended items of user queries from a massive repository of items is one of the main goals of an e-commerce search system. Relevance prediction is essential to the search system since it helps improve performance. When online serving a relevance model, the model is required to perform fast and accurate inference. Currently, the widely used models such as Bi-encoder and Cross-encoder have their limitations in accuracy or inference speed respectively. In this work, we propose a novel model called the Entity-Based Relevance Model (EBRM). We identify the entities contained in an item and decompose the QI (query-item) relevance problem into multiple QE (query-entity) relevance problems; we then aggregate their results to form the QI prediction using a soft logic formulation. The decomposition allows us to use a Cross-encoder QE relevance module for high accuracy as well as cache QE predictions for fast online inference. Utilizing soft logic makes the prediction procedure int
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;ChatGPT&#36827;&#34892;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#23454;&#39564;&#30740;&#31350;&#65292;&#36890;&#36807;&#35774;&#35745;&#21407;&#21017;&#21644;&#20989;&#25968;&#24211;&#30340;&#32467;&#21512;&#65292;ChatGPT&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.17582</link><description>&lt;p&gt;
ChatGPT&#29992;&#20110;&#26426;&#22120;&#20154;&#25216;&#26415;&#65306;&#35774;&#35745;&#21407;&#21017;&#21644;&#27169;&#22411;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
ChatGPT for Robotics: Design Principles and Model Abilities. (arXiv:2306.17582v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;ChatGPT&#36827;&#34892;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#23454;&#39564;&#30740;&#31350;&#65292;&#36890;&#36807;&#35774;&#35745;&#21407;&#21017;&#21644;&#20989;&#25968;&#24211;&#30340;&#32467;&#21512;&#65292;ChatGPT&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;OpenAI&#30340;ChatGPT&#36827;&#34892;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#23454;&#39564;&#30740;&#31350;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#19968;&#31181;&#31574;&#30053;&#65292;&#23558;&#25552;&#31034;&#24037;&#31243;&#30340;&#35774;&#35745;&#21407;&#21017;&#19982;&#39640;&#32423;&#20989;&#25968;&#24211;&#30340;&#21019;&#24314;&#30456;&#32467;&#21512;&#65292;&#20351;ChatGPT&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#12289;&#27169;&#25311;&#22120;&#21644;&#24418;&#24577;&#12290;&#25105;&#20204;&#37325;&#28857;&#35780;&#20272;&#20102;&#19981;&#21516;&#30340;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#21644;&#23545;&#35805;&#31574;&#30053;&#23545;&#25191;&#34892;&#21508;&#31181;&#31867;&#22411;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;ChatGPT&#20351;&#29992;&#33258;&#30001;&#24418;&#24335;&#23545;&#35805;&#12289;&#35299;&#26512;XML&#26631;&#35760;&#21644;&#21512;&#25104;&#20195;&#30721;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#20351;&#29992;&#20219;&#21153;&#29305;&#23450;&#25552;&#31034;&#20989;&#25968;&#21644;&#36890;&#36807;&#23545;&#35805;&#36827;&#34892;&#38381;&#29615;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#28085;&#30422;&#20102;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#19968;&#31995;&#21015;&#20219;&#21153;&#65292;&#20174;&#22522;&#26412;&#30340;&#36923;&#36753;&#12289;&#20960;&#20309;&#21644;&#25968;&#23398;&#25512;&#29702;&#21040;&#22797;&#26434;&#30340;&#39046;&#22495;&#65292;&#22914;&#31354;&#20013;&#23548;&#33322;&#12289;&#25805;&#32437;&#21644;&#20855;&#36523;&#20195;&#29702;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;ChatGPT&#22312;&#35299;&#20915;&#36825;&#20123;&#20219;&#21153;&#26041;&#38754;&#21487;&#20197;&#21462;&#24471;&#26377;&#25928;&#32467;&#26524;&#65292;&#21516;&#26102;&#20351;&#25105;&#20204;&#33021;&#22815;&#36827;&#34892;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an experimental study regarding the use of OpenAI's ChatGPT for robotics applications. We outline a strategy that combines design principles for prompt engineering and the creation of a high-level function library which allows ChatGPT to adapt to different robotics tasks, simulators, and form factors. We focus our evaluations on the effectiveness of different prompt engineering techniques and dialog strategies towards the execution of various types of robotics tasks. We explore ChatGPT's ability to use free-form dialog, parse XML tags, and to synthesize code, in addition to the use of task-specific prompting functions and closed-loop reasoning through dialogues. Our study encompasses a range of tasks within the robotics domain, from basic logical, geometrical, and mathematical reasoning all the way to complex domains such as aerial navigation, manipulation, and embodied agents. We show that ChatGPT can be effective at solving several of such tasks, while allowing us
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MotionGPT&#65292;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#22810;&#20010;&#19982;&#21160;&#20316;&#30456;&#20851;&#20219;&#21153;&#30340;&#32479;&#19968;&#12289;&#22810;&#21151;&#33021;&#19988;&#26131;&#20110;&#20351;&#29992;&#30340;&#21160;&#20316;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#23558;&#35821;&#35328;&#25968;&#25454;&#19982;&#22823;&#35268;&#27169;&#21160;&#20316;&#27169;&#22411;&#34701;&#21512;&#65292;&#23454;&#29616;&#20102;&#21160;&#20316;&#35821;&#35328;&#39044;&#35757;&#32451;&#65292;&#25552;&#21319;&#20102;&#21160;&#20316;&#30456;&#20851;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.14795</link><description>&lt;p&gt;
MotionGPT: &#20154;&#20307;&#36816;&#21160;&#20316;&#20026;&#19968;&#38376;&#22806;&#35821;
&lt;/p&gt;
&lt;p&gt;
MotionGPT: Human Motion as a Foreign Language. (arXiv:2306.14795v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MotionGPT&#65292;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#22810;&#20010;&#19982;&#21160;&#20316;&#30456;&#20851;&#20219;&#21153;&#30340;&#32479;&#19968;&#12289;&#22810;&#21151;&#33021;&#19988;&#26131;&#20110;&#20351;&#29992;&#30340;&#21160;&#20316;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#23558;&#35821;&#35328;&#25968;&#25454;&#19982;&#22823;&#35268;&#27169;&#21160;&#20316;&#27169;&#22411;&#34701;&#21512;&#65292;&#23454;&#29616;&#20102;&#21160;&#20316;&#35821;&#35328;&#39044;&#35757;&#32451;&#65292;&#25552;&#21319;&#20102;&#21160;&#20316;&#30456;&#20851;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#19981;&#26029;&#23637;&#29616;&#65292;&#20294;&#26500;&#24314;&#19968;&#20010;&#29992;&#20110;&#35821;&#35328;&#21644;&#20854;&#20182;&#22810;&#27169;&#24577;&#25968;&#25454;&#65288;&#22914;&#21160;&#20316;&#65289;&#30340;&#32479;&#19968;&#27169;&#22411;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#23578;&#26410;&#35302;&#21450;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#20154;&#20307;&#36816;&#21160;&#26174;&#31034;&#20986;&#19968;&#31181;&#35821;&#20041;&#32806;&#21512;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#35821;&#35328;&#65292;&#24120;&#34987;&#35270;&#20026;&#19968;&#31181;&#32930;&#20307;&#35821;&#35328;&#24418;&#24335;&#12290;&#36890;&#36807;&#23558;&#35821;&#35328;&#25968;&#25454;&#19982;&#22823;&#35268;&#27169;&#21160;&#20316;&#27169;&#22411;&#34701;&#21512;&#65292;&#21487;&#20197;&#23454;&#29616;&#22686;&#24378;&#19982;&#21160;&#20316;&#30456;&#20851;&#20219;&#21153;&#24615;&#33021;&#30340;&#21160;&#20316;&#35821;&#35328;&#39044;&#35757;&#32451;&#12290;&#22522;&#20110;&#36825;&#19968;&#27934;&#23519;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MotionGPT&#65292;&#19968;&#31181;&#32479;&#19968;&#12289;&#22810;&#21151;&#33021;&#19988;&#26131;&#20110;&#20351;&#29992;&#30340;&#21160;&#20316;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#22810;&#20010;&#19982;&#21160;&#20316;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#31163;&#25955;&#21521;&#37327;&#37327;&#21270;&#26469;&#22788;&#29702;&#20154;&#20307;&#36816;&#21160;&#65292;&#24182;&#23558;&#19977;&#32500;&#21160;&#20316;&#36716;&#25442;&#20026;&#21160;&#20316;&#20196;&#29260;&#65292;&#31867;&#20284;&#20110;&#35789;&#20196;&#29260;&#30340;&#29983;&#25104;&#36807;&#31243;&#12290;&#22522;&#20110;&#36825;&#20010;"&#21160;&#20316;&#35789;&#27719;"&#65292;&#25105;&#20204;&#20197;&#32479;&#19968;&#30340;&#26041;&#24335;&#23545;&#21160;&#20316;&#21644;&#25991;&#26412;&#36827;&#34892;&#35821;&#35328;&#24314;&#27169;&#65292;&#23558;&#20154;&#20307;&#36816;&#21160;&#35270;&#20026;&#19968;&#31181;&#29305;&#23450;&#30340;&#35821;&#35328;&#12290;&#27492;&#22806;&#65292;&#21463;&#25552;&#31034;&#30340;&#21551;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
Though the advancement of pre-trained large language models unfolds, the exploration of building a unified model for language and other multi-modal data, such as motion, remains challenging and untouched so far. Fortunately, human motion displays a semantic coupling akin to human language, often perceived as a form of body language. By fusing language data with large-scale motion models, motion-language pre-training that can enhance the performance of motion-related tasks becomes feasible. Driven by this insight, we propose MotionGPT, a unified, versatile, and user-friendly motion-language model to handle multiple motion-relevant tasks. Specifically, we employ the discrete vector quantization for human motion and transfer 3D motion into motion tokens, similar to the generation process of word tokens. Building upon this "motion vocabulary", we perform language modeling on both motion and text in a unified manner, treating human motion as a specific language. Moreover, inspired by prompt
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20171;&#32461;$\alpha$-$\beta$-&#20998;&#35299;&#30340;&#27010;&#24565;&#65292;&#23558;Simon&#21516;&#20313;&#29305;&#24449;&#21270;&#20026;$1$-&#26222;&#36941;&#24615;&#21333;&#35789;&#65292;&#24182;&#24212;&#29992;&#20110;&#20108;&#20803;&#21333;&#35789;&#30340;&#23436;&#20840;&#21051;&#30011;&#21644;&#21516;&#20313;&#25351;&#25968;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2306.14192</link><description>&lt;p&gt;
$\alpha$-$\beta$-&#20998;&#35299;&#21450;Simon&#21516;&#20313;&#30340;&#20108;&#20803;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
$\alpha$-$\beta$-Factorization and the Binary Case of Simon's Congruence. (arXiv:2306.14192v2 [math.CO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14192
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20171;&#32461;$\alpha$-$\beta$-&#20998;&#35299;&#30340;&#27010;&#24565;&#65292;&#23558;Simon&#21516;&#20313;&#29305;&#24449;&#21270;&#20026;$1$-&#26222;&#36941;&#24615;&#21333;&#35789;&#65292;&#24182;&#24212;&#29992;&#20110;&#20108;&#20803;&#21333;&#35789;&#30340;&#23436;&#20840;&#21051;&#30011;&#21644;&#21516;&#20313;&#25351;&#25968;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;1991&#24180;&#65292;H\'ebrard&#24341;&#20837;&#20102;&#19968;&#31181;&#21333;&#35789;&#30340;&#20998;&#35299;&#26041;&#27861;&#65292;&#34987;&#35777;&#26126;&#26159;&#30740;&#31350;&#21333;&#35789;&#30340;&#31163;&#25955;&#22240;&#23376;&#65288;&#20063;&#31216;&#20026;&#65288;&#31163;&#25955;&#65289;&#23376;&#20018;&#25110;&#23376;&#24207;&#21015;&#65289;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#22522;&#20110;&#27492;&#65292;Karandikar&#21644;Schnoebelen&#39318;&#20808;&#24341;&#20837;&#20102;$k$-&#20016;&#23500;&#24615;&#30340;&#27010;&#24565;&#65292;&#38543;&#21518;Barker&#31561;&#20154;&#24341;&#20837;&#20102;$k$-&#26222;&#36941;&#24615;&#30340;&#27010;&#24565;&#12290;&#22312;2022&#24180;&#65292;Fleischmann&#31561;&#20154;&#36890;&#36807;&#20132;&#38598;&#21270;&#21333;&#35789;&#30340;&#25329;&#24418;&#20998;&#35299;&#21644;&#20854;&#36870;&#24207;&#26469;&#25512;&#24191;&#20102;&#25329;&#24418;&#20998;&#35299;&#12290;&#23613;&#31649;&#20316;&#32773;&#20165;&#20165;&#20351;&#29992;&#36825;&#31181;&#20998;&#35299;&#26041;&#27861;&#26469;&#30740;&#31350;&#26368;&#30701;&#30340;&#32570;&#22833;&#31163;&#25955;&#22240;&#23376;&#65292;&#20294;&#22312;&#26412;&#30740;&#31350;&#20013;&#25105;&#20204;&#23558;&#23545;&#36825;&#31181;&#26032;&#30340;$\alpha$-$\beta$-&#20998;&#35299;&#36827;&#34892;&#30740;&#31350;&#12290;&#25105;&#20204;&#22312;$k$-&#26222;&#36941;&#24615;&#21333;&#35789;&#30340;$\alpha$-$\beta$-&#20998;&#35299;&#20013;&#23558;&#33879;&#21517;&#30340;Simon&#21516;&#20313;&#29305;&#24449;&#21270;&#20026;$1$-&#26222;&#36941;&#24615;&#21333;&#35789;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#32467;&#26524;&#24212;&#29992;&#20110;&#20108;&#20803;&#21333;&#35789;&#12290;&#22312;&#36825;&#31181;&#29305;&#27530;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#31867;&#21035;&#30340;&#23436;&#20840;&#21051;&#30011;&#24182;&#35745;&#31639;&#20102;&#21516;&#20313;&#30340;&#25351;&#25968;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24320;&#22987;&#30740;&#31350;&#19977;&#20803;&#24773;&#20917;&#65292;&#23637;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
In 1991 H\'ebrard introduced a factorization of words that turned out to be a powerful tool for the investigation of a word's scattered factors (also known as (scattered) subwords or subsequences). Based on this, first Karandikar and Schnoebelen introduced the notion of $k$-richness and later on Barker et al. the notion of $k$-universality. In 2022 Fleischmann et al. presented a generalization of the arch factorization by intersecting the arch factorization of a word and its reverse. While the authors merely used this factorization for the investigation of shortest absent scattered factors, in this work we investigate this new $\alpha$-$\beta$-factorization as such. We characterize the famous Simon congruence of $k$-universal words in terms of $1$-universal words. Moreover, we apply these results to binary words. In this special case, we obtain a full characterization of the classes and calculate the index of the congruence. Lastly, we start investigating the ternary case, present a fu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#36164;&#28304;&#21294;&#20047;&#30340;&#21360;&#24230;&#35821;&#35328;&#39532;&#25289;&#22320;&#35821;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#22411;&#28151;&#21512;&#39532;&#25289;&#22320;&#35821;-&#33521;&#35821;&#35821;&#26009;&#24211;&#65292;&#20197;&#21450;&#39044;&#35757;&#32451;&#30340;&#28151;&#21512;BERT&#27169;&#22411;&#21644;&#38024;&#23545;&#28151;&#21512;&#35821;&#35328;&#19979;&#28216;&#20219;&#21153;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#35813;&#35821;&#26009;&#24211;&#35757;&#32451;&#30340;&#27169;&#22411;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;BERT&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.14030</link><description>&lt;p&gt;
My Boli&#65306;&#28151;&#21512;&#39532;&#25289;&#22320;&#35821;-&#33521;&#35821;&#30340;&#35821;&#26009;&#24211;&#12289;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
My Boli: Code-mixed Marathi-English Corpora, Pretrained Language Models and Evaluation Benchmarks. (arXiv:2306.14030v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#36164;&#28304;&#21294;&#20047;&#30340;&#21360;&#24230;&#35821;&#35328;&#39532;&#25289;&#22320;&#35821;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#22411;&#28151;&#21512;&#39532;&#25289;&#22320;&#35821;-&#33521;&#35821;&#35821;&#26009;&#24211;&#65292;&#20197;&#21450;&#39044;&#35757;&#32451;&#30340;&#28151;&#21512;BERT&#27169;&#22411;&#21644;&#38024;&#23545;&#28151;&#21512;&#35821;&#35328;&#19979;&#28216;&#20219;&#21153;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#35813;&#35821;&#26009;&#24211;&#35757;&#32451;&#30340;&#27169;&#22411;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;BERT&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#32570;&#20047;&#19987;&#38376;&#30340;&#28151;&#21512;&#35821;&#26009;&#24211;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#28151;&#21512;&#35821;&#35328;&#25968;&#25454;&#30340;&#30740;&#31350;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#36164;&#28304;&#21294;&#20047;&#30340;&#21360;&#24230;&#35821;&#35328;&#39532;&#25289;&#22320;&#35821;&#65292;&#36825;&#20010;&#35821;&#35328;&#20043;&#21069;&#27809;&#26377;&#20219;&#20309;&#28151;&#21512;&#35821;&#35328;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;L3Cube-MeCorpus&#65292;&#19968;&#20010;&#21253;&#21547;500&#19975;&#26465;&#25512;&#29305;&#30340;&#22823;&#22411;&#28151;&#21512;&#39532;&#25289;&#22320;&#35821;-&#33521;&#35821;(Mr-En)&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;L3Cube-MeBERT&#21644;MeRoBERTa&#65292;&#22522;&#20110;BERT&#30340;&#28151;&#21512;&#27169;&#22411;&#65292;&#22312;MeCorpus&#19978;&#39044;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19977;&#20010;&#26377;&#30417;&#30563;&#30340;&#25968;&#25454;&#38598;MeHate&#12289;MeSent&#21644;MeLID&#65292;&#29992;&#20110;&#28151;&#21512;Mr-En&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#12289;&#24773;&#24863;&#20998;&#26512;&#21644;&#35821;&#35328;&#35782;&#21035;&#31561;&#19979;&#28216;&#20219;&#21153;&#12290;&#36825;&#20123;&#35780;&#20272;&#25968;&#25454;&#38598;&#20998;&#21035;&#21253;&#21547;&#25163;&#21160;&#27880;&#37322;&#30340;\url{~}12,000&#26465;&#39532;&#25289;&#22320;&#35821;-&#33521;&#35821;&#28151;&#21512;&#25512;&#29305;&#12290;&#21066;&#20943;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#20010;&#26032;&#35821;&#26009;&#24211;&#35757;&#32451;&#30340;&#27169;&#22411;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;BERT&#27169;&#22411;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#25552;&#20379;&#28151;&#21512;&#39532;&#25289;&#22320;&#35821;&#30340;&#20195;&#30721;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
The research on code-mixed data is limited due to the unavailability of dedicated code-mixed datasets and pre-trained language models. In this work, we focus on the low-resource Indian language Marathi which lacks any prior work in code-mixing. We present L3Cube-MeCorpus, a large code-mixed Marathi-English (Mr-En) corpus with 5 million tweets for pretraining. We also release L3Cube-MeBERT and MeRoBERTa, code-mixed BERT-based transformer models pre-trained on MeCorpus. Furthermore, for benchmarking, we present three supervised datasets MeHate, MeSent, and MeLID for downstream tasks like code-mixed Mr-En hate speech detection, sentiment analysis, and language identification respectively. These evaluation datasets individually consist of manually annotated \url{~}12,000 Marathi-English code-mixed tweets. Ablations show that the models trained on this novel corpus significantly outperform the existing state-of-the-art BERT models. This is the first work that presents artifacts for code-mix
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#31614;&#29983;&#25104;&#26041;&#27861;&#30340;&#22686;&#37327;&#20998;&#31867;&#23398;&#20064;&#65288;CIL&#65289;&#26041;&#27861;&#65288;VAG&#65289;&#65292;&#22823;&#24133;&#20943;&#23569;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#65288;CF&#65289;&#65292;&#24182;&#26356;&#22909;&#22320;&#20445;&#30041;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21487;&#25512;&#24191;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2306.12619</link><description>&lt;p&gt;
&#22522;&#20110;&#26631;&#31614;&#29983;&#25104;&#30340;&#22686;&#37327;&#20998;&#31867;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Class-Incremental Learning based on Label Generation. (arXiv:2306.12619v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12619
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#31614;&#29983;&#25104;&#26041;&#27861;&#30340;&#22686;&#37327;&#20998;&#31867;&#23398;&#20064;&#65288;CIL&#65289;&#26041;&#27861;&#65288;VAG&#65289;&#65292;&#22823;&#24133;&#20943;&#23569;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#65288;CF&#65289;&#65292;&#24182;&#26356;&#22909;&#22320;&#20445;&#30041;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21487;&#25512;&#24191;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#23545;&#20110;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#65288;CIL&#65289;&#35774;&#32622;&#65292;&#30001;&#20110;&#28798;&#38590;&#24615;&#36951;&#24536;&#65288;CF&#65289;&#65292;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#36830;&#32493;&#23398;&#20064;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#22914;&#26524;&#23558;CIL&#23450;&#24335;&#20026;&#19968;&#20010;&#36830;&#32493;&#30340;&#26631;&#31614;&#29983;&#25104;&#38382;&#39064;&#65292;&#21017;&#21487;&#20197;&#22823;&#24133;&#20943;&#23569;CF&#24182;&#26356;&#22909;&#22320;&#20445;&#30041;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21487;&#25512;&#24191;&#34920;&#31034;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;CIL&#26041;&#27861;&#65288;VAG&#65289;&#65292;&#35813;&#26041;&#27861;&#36824;&#21033;&#29992;&#20102;&#35789;&#27719;&#34920;&#30340;&#31232;&#30095;&#24615;&#20197;&#20415;&#20110;&#29983;&#25104;&#65292;&#24182;&#20351;&#29992;&#26631;&#31614;&#35821;&#20041;&#21019;&#24314;&#20266;&#37325;&#25773;&#26679;&#26412;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;VAG&#30340;&#24615;&#33021;&#27604;&#22522;&#32447;&#22823;&#24133;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the great success of pre-trained language models, it is still a challenge to use these models for continual learning, especially for the class-incremental learning (CIL) setting due to catastrophic forgetting (CF). This paper reports our finding that if we formulate CIL as a continual label generation problem, CF is drastically reduced and the generalizable representations of pre-trained models can be better retained. We thus propose a new CIL method (VAG) that also leverages the sparsity of vocabulary to focus the generation and creates pseudo-replay samples by using label semantics. Experimental results show that VAG outperforms baselines by a large margin.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20351;&#29992;&#25552;&#31034;&#24037;&#31243;&#30340;&#26041;&#27861;&#25351;&#23548;ChatGPT&#23545;&#31185;&#23398;&#25991;&#29486;&#36827;&#34892;&#33258;&#21160;&#21270;&#25991;&#26412;&#25366;&#25496;&#65292;&#20197;&#33719;&#24471;&#37329;&#23646;-&#26377;&#26426;&#26694;&#26550;&#65288;MOF&#65289;&#21512;&#25104;&#26465;&#20214;&#12290;&#36890;&#36807;&#35813;&#31995;&#32479;&#65292;&#21487;&#20197;&#39640;&#31934;&#30830;&#22320;&#25552;&#21462;&#22823;&#37327;&#21512;&#25104;&#21442;&#25968;&#65292;&#20026;MOF&#21512;&#25104;&#25552;&#20379;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2306.11296</link><description>&lt;p&gt;
ChatGPT&#21270;&#23398;&#21161;&#25163;&#29992;&#20110;&#25991;&#26412;&#25366;&#25496;&#21644;MOF&#21512;&#25104;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
ChatGPT Chemistry Assistant for Text Mining and Prediction of MOF Synthesis. (arXiv:2306.11296v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11296
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20351;&#29992;&#25552;&#31034;&#24037;&#31243;&#30340;&#26041;&#27861;&#25351;&#23548;ChatGPT&#23545;&#31185;&#23398;&#25991;&#29486;&#36827;&#34892;&#33258;&#21160;&#21270;&#25991;&#26412;&#25366;&#25496;&#65292;&#20197;&#33719;&#24471;&#37329;&#23646;-&#26377;&#26426;&#26694;&#26550;&#65288;MOF&#65289;&#21512;&#25104;&#26465;&#20214;&#12290;&#36890;&#36807;&#35813;&#31995;&#32479;&#65292;&#21487;&#20197;&#39640;&#31934;&#30830;&#22320;&#25552;&#21462;&#22823;&#37327;&#21512;&#25104;&#21442;&#25968;&#65292;&#20026;MOF&#21512;&#25104;&#25552;&#20379;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#25552;&#31034;&#24037;&#31243;&#26469;&#25351;&#23548;ChatGPT&#33258;&#21160;&#21270;&#22320;&#20174;&#31185;&#23398;&#25991;&#29486;&#20013;&#25366;&#25496;&#22810;&#26679;&#30340;&#37329;&#23646;-&#26377;&#26426;&#26694;&#26550;&#65288;MOF&#65289;&#21512;&#25104;&#26465;&#20214;&#12290;&#36825;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;ChatGPT&#22312;&#31185;&#23398;&#39046;&#22495;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26102;&#20986;&#29616;&#20449;&#24687;&#20135;&#29983;&#35823;&#24046;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#36890;&#36807;ChatGPT&#26412;&#36523;&#32534;&#31243;&#26469;&#24320;&#21457;&#23454;&#26045;&#25991;&#26412;&#25366;&#25496;&#30340;&#19977;&#20010;&#19981;&#21516;&#36807;&#31243;&#12290;&#25152;&#26377;&#36825;&#20123;&#36807;&#31243;&#37117;&#21487;&#20197;&#35299;&#26512;&#12289;&#25628;&#32034;&#12289;&#36807;&#28388;&#12289;&#20998;&#31867;&#12289;&#25688;&#35201;&#21644;&#25968;&#25454;&#32479;&#19968;&#65292;&#20294;&#22312;&#21171;&#21160;&#21147;&#12289;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#26377;&#19981;&#21516;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#37096;&#32626;&#20102;&#35813;&#31995;&#32479;&#65292;&#20174;&#21516;&#34892;&#35780;&#23457;&#30340;&#30740;&#31350;&#25991;&#31456;&#20013;&#25552;&#21462;&#20102;26257&#20010;&#19981;&#21516;&#30340;&#21512;&#25104;&#21442;&#25968;&#65292;&#28041;&#21450;&#22823;&#32422;800&#20010;MOF&#12290;&#36825;&#20010;&#36807;&#31243;&#21253;&#21547;&#20102;&#25105;&#20204;&#30340;ChemPrompt&#24037;&#31243;&#31574;&#30053;&#65292;&#20197;&#25351;&#23548;ChatGPT&#36827;&#34892;&#25991;&#26412;&#25366;&#25496;&#65292;&#32467;&#26524;&#26174;&#31034;&#20986;&#20102;90-99%&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;F1&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
We use prompt engineering to guide ChatGPT in the automation of text mining of metal-organic frameworks (MOFs) synthesis conditions from diverse formats and styles of the scientific literature. This effectively mitigates ChatGPT's tendency to hallucinate information -- an issue that previously made the use of Large Language Models (LLMs) in scientific fields challenging. Our approach involves the development of a workflow implementing three different processes for text mining, programmed by ChatGPT itself. All of them enable parsing, searching, filtering, classification, summarization, and data unification with different tradeoffs between labor, speed, and accuracy. We deploy this system to extract 26,257 distinct synthesis parameters pertaining to approximately 800 MOFs sourced from peer-reviewed research articles. This process incorporates our ChemPrompt Engineering strategy to instruct ChatGPT in text mining, resulting in impressive precision, recall, and F1 scores of 90-99%. Furthe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30693;&#35782;&#38142;&#25552;&#31034;&#65288;CoK&#65289;&#26041;&#27861;&#65292;&#26088;&#22312;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26126;&#30830;&#30340;&#30693;&#35782;&#35777;&#25454;&#65292;&#20197;&#25552;&#21319;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;F^2-Verification&#26041;&#27861;&#35780;&#20272;&#25512;&#29702;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.06427</link><description>&lt;p&gt;
&#20351;&#29992;&#30693;&#35782;&#38142;&#25512;&#21160;&#25552;&#21319;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Boosting Language Models Reasoning with Chain-of-Knowledge Prompting. (arXiv:2306.06427v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06427
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30693;&#35782;&#38142;&#25552;&#31034;&#65288;CoK&#65289;&#26041;&#27861;&#65292;&#26088;&#22312;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26126;&#30830;&#30340;&#30693;&#35782;&#35777;&#25454;&#65292;&#20197;&#25552;&#21319;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;F^2-Verification&#26041;&#27861;&#35780;&#20272;&#25512;&#29702;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38142;&#24335;&#24605;&#32500;&#65288;CoT&#65289;&#25552;&#31034;&#22312;&#22797;&#26434;&#30340;&#25512;&#29702;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20854;&#30446;&#26631;&#26159;&#35774;&#35745;&#19968;&#20010;&#31616;&#21333;&#30340;&#25552;&#31034;&#65292;&#22914;&#8220;&#25105;&#20204;&#19968;&#27493;&#19968;&#27493;&#22320;&#24605;&#32771;&#8221;&#25110;&#22810;&#20010;&#19978;&#19979;&#25991;&#31034;&#20363;&#65292;&#20197;&#21450;&#35774;&#35745;&#33391;&#22909;&#30340;&#29702;&#30001;&#65292;&#20197;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#30340;&#29702;&#30001;&#24448;&#24448;&#24102;&#26377;&#38169;&#35823;&#65292;&#23548;&#33268;&#19981;&#20934;&#30830;&#21644;&#19981;&#21487;&#20449;&#30340;&#25512;&#29702;&#38142;&#12290;&#20026;&#20102;&#20943;&#23569;&#36825;&#31181;&#33030;&#24369;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#38142;&#25552;&#31034;&#65288;CoK&#65289;&#65292;&#26088;&#22312;&#24341;&#23548;LLM&#29983;&#25104;&#26174;&#24615;&#30340;&#30693;&#35782;&#35777;&#25454;&#65292;&#20197;&#32467;&#26500;&#21270;&#19977;&#20803;&#32452;&#30340;&#24418;&#24335;&#21576;&#29616;&#12290;&#36825;&#19968;&#28789;&#24863;&#26469;&#33258;&#20110;&#20154;&#31867;&#34892;&#20026;&#65292;&#21363;&#22312;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#20043;&#21069;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#33041;&#28023;&#20013;&#32472;&#21046;&#24605;&#32500;&#23548;&#22270;&#25110;&#30693;&#35782;&#22270;&#20316;&#20026;&#25512;&#29702;&#35777;&#25454;&#12290;&#36890;&#36807;&#20351;&#29992;CoK&#65292;&#25105;&#20204;&#39069;&#22806;&#24341;&#20837;&#20102;&#19968;&#31181;F^2-Verification&#26041;&#27861;&#26469;&#20272;&#35745;&#25512;&#29702;&#38142;&#30340;&#21487;&#38752;&#24615;&#65292;&#21253;&#25324;&#20934;&#30830;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;&#23545;&#20110;&#19981;&#21487;&#38752;&#30340;&#22238;&#31572;&#65292;&#38169;&#35823;&#30340;&#35777;&#25454;&#21487;&#20197;
&lt;/p&gt;
&lt;p&gt;
Recently, Chain-of-Thought (CoT) prompting has delivered success on complex reasoning tasks, which aims at designing a simple prompt like ``Let's think step by step'' or multiple in-context exemplars with well-designed rationales to elicit Large Language Models (LLMs) to generate intermediate reasoning steps. However, the generated rationales often come with mistakes, making unfactual and unfaithful reasoning chains. To mitigate this brittleness, we propose a novel Chain-of-Knowledge (CoK) prompting, where we aim at eliciting LLMs to generate explicit pieces of knowledge evidence in the form of structure triple. This is inspired by our human behaviors, i.e., we can draw a mind map or knowledge map as the reasoning evidence in the brain before answering a complex question. Benefiting from CoK, we additionally introduce a F^2-Verification method to estimate the reliability of the reasoning chains in terms of factuality and faithfulness. For the unreliable response, the wrong evidence can
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#22238;&#39038;&#20102;&#29983;&#25104;AI&#23545;&#31185;&#23398;&#30740;&#31350;&#25152;&#24102;&#26469;&#30340;&#35748;&#35782;&#35770;&#25361;&#25112;&#12289;&#20262;&#29702;&#21644;&#35802;&#20449;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#21313;&#39033;&#24314;&#35758;&#65292;&#20197;&#22312;AI&#26102;&#20195;&#20419;&#36827;&#26356;&#36127;&#36131;&#20219;&#30340;&#30740;&#31350;&#36827;&#34892;&#12290;</title><link>http://arxiv.org/abs/2305.15299</link><description>&lt;p&gt;
&#22312;ChatGPT&#12289;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#29983;&#25104;AI&#26102;&#20195;&#30340;&#31185;&#23398;&#65306;&#30740;&#31350;&#20262;&#29702;&#30340;&#25361;&#25112;&#21450;&#24212;&#23545;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Science in the Era of ChatGPT, Large Language Models and Generative AI: Challenges for Research Ethics and How to Respond. (arXiv:2305.15299v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15299
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#22238;&#39038;&#20102;&#29983;&#25104;AI&#23545;&#31185;&#23398;&#30740;&#31350;&#25152;&#24102;&#26469;&#30340;&#35748;&#35782;&#35770;&#25361;&#25112;&#12289;&#20262;&#29702;&#21644;&#35802;&#20449;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#21313;&#39033;&#24314;&#35758;&#65292;&#20197;&#22312;AI&#26102;&#20195;&#20419;&#36827;&#26356;&#36127;&#36131;&#20219;&#30340;&#30740;&#31350;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#22312;&#31185;&#23398;&#30740;&#31350;&#20013;&#20855;&#26377;&#26174;&#33879;&#20294;&#26377;&#20105;&#35758;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#29983;&#25104;AI&#26102;&#20195;&#31185;&#23398;&#30740;&#31350;&#30340;&#35748;&#35782;&#35770;&#25361;&#25112;&#12289;&#20262;&#29702;&#21644;&#35802;&#20449;&#39118;&#38505;&#65292;&#24182;&#26088;&#22312;&#20026;&#39640;&#36136;&#37327;&#30340;&#30740;&#31350;&#20262;&#29702;&#23457;&#26597;&#22880;&#23450;&#26032;&#30340;&#21450;&#26102;&#22522;&#30784;&#12290;&#23545;AI&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#30740;&#31350;&#24037;&#20855;&#21644;&#30740;&#31350;&#23545;&#35937;&#30340;&#35282;&#33394;&#36827;&#34892;&#20102;&#35814;&#32454;&#23457;&#26597;&#65292;&#24182;&#35752;&#35770;&#20102;&#23545;&#31185;&#23398;&#23478;&#12289;&#21442;&#19982;&#32773;&#21644;&#35780;&#23457;&#20154;&#21592;&#30340;&#20262;&#29702;&#24433;&#21709;&#12290;&#35752;&#35770;&#20102;&#30740;&#31350;&#20262;&#29702;&#23457;&#26597;&#30340;&#26032;&#20852;&#23454;&#36341;&#65292;&#24182;&#32473;&#20986;&#20102;&#21313;&#39033;&#24314;&#35758;&#65292;&#20026;&#22312;AI&#26102;&#20195;&#26356;&#36127;&#36131;&#20219;&#30340;&#30740;&#31350;&#36827;&#34892;&#22238;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models of artificial intelligence (AI), such as ChatGPT, find remarkable but controversial applicability in science and research. This paper reviews epistemological challenges, ethical and integrity risks in science conduct in the advent of generative AI. This is with the aim to lay new timely foundations for a high-quality research ethics review. The role of AI language models as a research instrument and subject is scrutinized along with ethical implications for scientists, participants and reviewers. New emerging practices for research ethics review are discussed, concluding with ten recommendations that shape a response for a more responsible research conduct in the era of AI.
&lt;/p&gt;</description></item><item><title>AlignAtt&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;SimulST&#31574;&#30053;&#65292;&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#38899;&#39057;&#32763;&#35793;&#23545;&#40784;&#26469;&#25351;&#23548;&#27169;&#22411;&#65292;&#22312;BLEU&#21644;&#24310;&#36831;&#26041;&#38754;&#22343;&#20248;&#20110;&#20043;&#21069;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2305.11408</link><description>&lt;p&gt;
AlignAtt&#65306;&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#38899;&#39057;&#32763;&#35793;&#23545;&#40784;&#20316;&#20026;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;&#30340;&#25351;&#23548;
&lt;/p&gt;
&lt;p&gt;
AlignAtt: Using Attention-based Audio-Translation Alignments as a Guide for Simultaneous Speech Translation. (arXiv:2305.11408v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11408
&lt;/p&gt;
&lt;p&gt;
AlignAtt&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;SimulST&#31574;&#30053;&#65292;&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#38899;&#39057;&#32763;&#35793;&#23545;&#40784;&#26469;&#25351;&#23548;&#27169;&#22411;&#65292;&#22312;BLEU&#21644;&#24310;&#36831;&#26041;&#38754;&#22343;&#20248;&#20110;&#20043;&#21069;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#26159;&#24403;&#20170;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26368;&#24120;&#29992;&#30340;&#26550;&#26500;&#30340;&#26680;&#24515;&#26426;&#21046;&#65292;&#24182;&#24050;&#20174;&#35768;&#22810;&#35282;&#24230;&#36827;&#34892;&#20998;&#26512;&#65292;&#21253;&#25324;&#20854;&#22312;&#26426;&#22120;&#32763;&#35793;&#30456;&#20851;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#36825;&#20123;&#30740;&#31350;&#20013;&#65292;&#27880;&#24847;&#21147;&#22312;&#36755;&#20837;&#25991;&#26412;&#34987;&#26367;&#25442;&#20026;&#38899;&#39057;&#29255;&#27573;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#26159;&#33719;&#21462;&#26377;&#20851;&#21333;&#35789;&#23545;&#40784;&#30340;&#26377;&#29992;&#20449;&#24687;&#30340;&#19968;&#31181;&#26041;&#24335;&#65292;&#20363;&#22914;&#35821;&#38899;&#32763;&#35793;&#65288;ST&#65289;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AlignAtt&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#21516;&#26102;ST&#65288;SimulST&#65289;&#31574;&#30053;&#65292;&#23427;&#21033;&#29992;&#27880;&#24847;&#21147;&#20449;&#24687;&#26469;&#29983;&#25104;&#28304;-&#30446;&#26631;&#23545;&#40784;&#65292;&#20197;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#25351;&#23548;&#27169;&#22411;&#12290;&#36890;&#36807;&#23545;MuST-C v1.0&#30340;8&#31181;&#35821;&#35328;&#23545;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#32447;&#19979;&#35757;&#32451;&#30340;&#27169;&#22411;&#19978;&#24212;&#29992;&#20808;&#21069;&#30340;&#26368;&#26032;SimulST&#31574;&#30053;&#65292;AlignAtt&#22312;BLEU&#26041;&#38754;&#33719;&#24471;&#20102;2&#20010;&#20998;&#25968;&#30340;&#25552;&#39640;&#65292;&#24182;&#19988;8&#31181;&#35821;&#35328;&#30340;&#24310;&#36831;&#32553;&#20943;&#22312;0.5&#31186;&#21040;0.8&#31186;&#20043;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention is the core mechanism of today's most used architectures for natural language processing and has been analyzed from many perspectives, including its effectiveness for machine translation-related tasks. Among these studies, attention resulted to be a useful source of information to get insights about word alignment also when the input text is substituted with audio segments, as in the case of the speech translation (ST) task. In this paper, we propose AlignAtt, a novel policy for simultaneous ST (SimulST) that exploits the attention information to generate source-target alignments that guide the model during inference. Through experiments on the 8 language pairs of MuST-C v1.0, we show that AlignAtt outperforms previous state-of-the-art SimulST policies applied to offline-trained models with gains in terms of BLEU of 2 points and latency reductions ranging from 0.5s to 0.8s across the 8 languages.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#28857;&#30740;&#31350;&#20102;&#36731;&#37327;&#21270;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#20020;&#24202;&#25991;&#26412;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#22312;RRS&#31034;&#20363;&#19978;&#36827;&#34892;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65292;&#23454;&#29616;&#36866;&#24212;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25918;&#23556;&#24615;&#25253;&#21578;&#25688;&#35201;&#65288;RRS&#65289;&#20219;&#21153;&#12290;&#24182;&#19988;&#35813;&#26041;&#27861;&#20165;&#24494;&#35843;&#27169;&#22411;&#30340;0.32&#65285;&#30340;&#21442;&#25968;&#65292;&#25552;&#39640;&#20102;&#34920;&#29616;&#12290;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#39046;&#22495;&#36866;&#24212;&#22312;RRS&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#20026;&#24320;&#21457;&#26356;&#22909;&#30340;&#25918;&#23556;&#24615;&#25253;&#21578;&#25688;&#35201;&#27169;&#22411;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.01146</link><description>&lt;p&gt;
RadAdapt&#65306;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36731;&#37327;&#21270;&#39046;&#22495;&#33258;&#36866;&#24212;&#23454;&#29616;&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
RadAdapt: Radiology Report Summarization via Lightweight Domain Adaptation of Large Language Models. (arXiv:2305.01146v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01146
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#28857;&#30740;&#31350;&#20102;&#36731;&#37327;&#21270;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#20020;&#24202;&#25991;&#26412;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#22312;RRS&#31034;&#20363;&#19978;&#36827;&#34892;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65292;&#23454;&#29616;&#36866;&#24212;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25918;&#23556;&#24615;&#25253;&#21578;&#25688;&#35201;&#65288;RRS&#65289;&#20219;&#21153;&#12290;&#24182;&#19988;&#35813;&#26041;&#27861;&#20165;&#24494;&#35843;&#27169;&#22411;&#30340;0.32&#65285;&#30340;&#21442;&#25968;&#65292;&#25552;&#39640;&#20102;&#34920;&#29616;&#12290;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#39046;&#22495;&#36866;&#24212;&#22312;RRS&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#20026;&#24320;&#21457;&#26356;&#22909;&#30340;&#25918;&#23556;&#24615;&#25253;&#21578;&#25688;&#35201;&#27169;&#22411;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#36731;&#37327;&#32423;&#31574;&#30053;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#65288;&#33258;&#28982;&#35821;&#35328;&#65292;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#65292;&#20020;&#24202;&#25991;&#26412;&#65289;&#21644;&#25552;&#31034;&#65288;&#38646;-shot&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#65289;&#25110;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;&#21069;&#32512;&#24494;&#35843;&#65292;LoRA&#65289;&#65292;&#26469;&#36866;&#24212;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#25918;&#23556;&#24615;&#25253;&#21578;&#25688;&#35201;&#65288;RRS&#65289;&#20219;&#21153;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#26368;&#22823;&#31243;&#24230;&#22320;&#36866;&#24212;&#20219;&#21153;&#30340;&#26041;&#27861;&#26159;&#65292;&#36890;&#36807;&#22312;&#20020;&#24202;&#25991;&#26412;&#19978;&#39044;&#20808;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;RRS&#31034;&#20363;&#19978;&#36827;&#34892;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#31181;&#26041;&#27861;&#20165;&#24494;&#35843;&#27169;&#22411;&#30340;0.32&#65285;&#30340;&#21442;&#25968;&#65292;&#19982;&#31471;&#23545;&#31471;&#24494;&#35843;&#65288;100&#65285;&#30340;&#21442;&#25968;&#65289;&#24418;&#25104;&#23545;&#27604;&#12290;&#27492;&#22806;&#65292;&#22312;&#30740;&#31350;&#19978;&#19979;&#25991;&#31034;&#20363;&#21644;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#35757;&#32451;&#30340;&#24433;&#21709;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#25918;&#23556;&#31185;&#21307;&#24072;&#35835;&#32773;&#30740;&#31350;&#21644;&#23450;&#24615;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#39046;&#22495;&#36866;&#24212;&#22312;RRS&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#20026;&#24320;&#21457;&#26356;&#22909;&#30340;&#25918;&#23556;&#24615;&#25253;&#21578;&#25688;&#35201;&#27169;&#22411;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
We systematically investigate lightweight strategies to adapt large language models (LLMs) for the task of radiology report summarization (RRS). Specifically, we focus on domain adaptation via pretraining (on natural language, biomedical text, and clinical text) and via prompting (zero-shot, in-context learning) or parameter-efficient fine-tuning (prefix tuning, LoRA). Our results on the MIMIC-III dataset consistently demonstrate best performance by maximally adapting to the task via pretraining on clinical text and parameter-efficient fine-tuning on RRS examples. Importantly, this method fine-tunes a mere 0.32% of parameters throughout the model, in contrast to end-to-end fine-tuning (100% of parameters). Additionally, we study the effect of in-context examples and out-of-distribution (OOD) training before concluding with a radiologist reader study and qualitative analysis. Our findings highlight the importance of domain adaptation in RRS and provide valuable insights toward developin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;AI&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#25351;&#20986;&#32570;&#20047;AI&#20844;&#24179;&#24615;&#20250;&#21152;&#28145;&#20559;&#35265;&#25104;&#20026;&#31038;&#20250;&#21387;&#21147;&#22240;&#32032;&#65292;&#21487;&#33021;&#23545;&#31038;&#20250;&#20135;&#29983;&#38271;&#26399;&#24433;&#21709;&#65292;&#22240;&#27492;&#38656;&#35201;&#23547;&#27714;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.09826</link><description>&lt;p&gt;
AI&#30340;&#20844;&#24179;&#24615;&#21450;&#20854;&#23545;&#31038;&#20250;&#30340;&#38271;&#26399;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Fairness in AI and Its Long-Term Implications on Society. (arXiv:2304.09826v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09826
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;AI&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#25351;&#20986;&#32570;&#20047;AI&#20844;&#24179;&#24615;&#20250;&#21152;&#28145;&#20559;&#35265;&#25104;&#20026;&#31038;&#20250;&#21387;&#21147;&#22240;&#32032;&#65292;&#21487;&#33021;&#23545;&#31038;&#20250;&#20135;&#29983;&#38271;&#26399;&#24433;&#21709;&#65292;&#22240;&#27492;&#38656;&#35201;&#23547;&#27714;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#22312;&#21508;&#31181;&#35774;&#32622;&#20013;&#30340;&#25104;&#21151;&#37096;&#32626;&#24050;&#32463;&#20026;&#20010;&#20154;&#21644;&#31038;&#20250;&#24102;&#26469;&#20102;&#35768;&#22810;&#31215;&#26497;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39044;&#27979;&#30340;&#20559;&#35265;&#65292;AI&#31995;&#32479;&#20063;&#34987;&#35777;&#26126;&#23545;&#37096;&#20998;&#20154;&#21475;&#36896;&#25104;&#20102;&#20260;&#23475;&#12290;&#25105;&#20204;&#30528;&#30524;&#20110;AI&#30340;&#20844;&#24179;&#24615;&#65292;&#20998;&#26512;&#20102;&#32570;&#20047;AI&#20844;&#24179;&#24615;&#26102;&#22914;&#20309;&#23548;&#33268;&#20559;&#35265;&#38543;&#30528;&#26102;&#38388;&#30340;&#21152;&#28145;&#32780;&#25104;&#20026;&#31038;&#20250;&#21387;&#21147;&#22240;&#32032;&#12290;&#22914;&#26524;&#38382;&#39064;&#25345;&#32493;&#23384;&#22312;&#65292;&#21487;&#33021;&#20250;&#23545;&#31038;&#20250;&#20135;&#29983;&#19981;&#33391;&#30340;&#38271;&#26399;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#19982;&#20854;&#20182;&#39118;&#38505;&#30340;&#20132;&#20114;&#26469;&#21152;&#24378;&#12290;&#25105;&#20204;&#26816;&#26597;&#20102;&#25552;&#39640;AI&#20844;&#24179;&#24615;&#30340;&#24403;&#21069;&#31574;&#30053;&#65292;&#24182;&#35780;&#20272;&#23427;&#20204;&#22312;&#23454;&#38469;&#37096;&#32626;&#26041;&#38754;&#30340;&#38480;&#21046;&#65292;&#24182;&#25506;&#35752;&#20102;&#30830;&#20445;&#25105;&#20204;&#22312;&#19981;&#25439;&#23475;&#31038;&#20250;&#37325;&#35201;&#37096;&#20998;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;AI&#30340;&#22909;&#22788;&#30340;&#28508;&#22312;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Successful deployment of artificial intelligence (AI) in various settings has led to numerous positive outcomes for individuals and society. However, AI systems have also been shown to harm parts of the population due to biased predictions. We take a closer look at AI fairness and analyse how lack of AI fairness can lead to deepening of biases over time and act as a social stressor. If the issues persist, it could have undesirable long-term implications on society, reinforced by interactions with other risks. We examine current strategies for improving AI fairness, assess their limitations in terms of real-world deployment, and explore potential paths forward to ensure we reap AI's benefits without harming significant parts of the society.
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#33889;&#33796;&#29273;&#35821;&#36827;&#34892;&#21333;&#35821;&#35328;&#39044;&#35757;&#32451;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#35268;&#27169;&#21512;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#36136;&#37327;&#65292;&#24182;&#33021;&#22815;&#22312;&#19968;&#31995;&#21015;&#33889;&#33796;&#29273;&#35821;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#21644;&#22810;&#35821;&#35328;&#30340;&#23545;&#25163;&#65292;&#26368;&#22909;&#30340;&#27169;&#22411;&#30340;&#34920;&#29616;&#19982;GPT-3.5-turbo&#25345;&#24179;&#12290;</title><link>http://arxiv.org/abs/2304.07880</link><description>&lt;p&gt;
Sabi&#225;: &#33889;&#33796;&#29273;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Sabi\'a: Portuguese Large Language Models. (arXiv:2304.07880v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07880
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#33889;&#33796;&#29273;&#35821;&#36827;&#34892;&#21333;&#35821;&#35328;&#39044;&#35757;&#32451;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#35268;&#27169;&#21512;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#36136;&#37327;&#65292;&#24182;&#33021;&#22815;&#22312;&#19968;&#31995;&#21015;&#33889;&#33796;&#29273;&#35821;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#21644;&#22810;&#35821;&#35328;&#30340;&#23545;&#25163;&#65292;&#26368;&#22909;&#30340;&#27169;&#22411;&#30340;&#34920;&#29616;&#19982;GPT-3.5-turbo&#25345;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#30340;&#19981;&#26029;&#25552;&#39640;&#65292;&#8221;&#19968;&#20992;&#20999;&#8220;&#30340;&#27169;&#22411;&#20173;&#28982;&#26159;&#20027;&#27969;&#12290;&#23588;&#20854;&#26159;&#32771;&#34385;&#21040;&#20840;&#29699;&#20351;&#29992;&#30340;&#35821;&#35328;&#25968;&#37327;&#38750;&#24120;&#24222;&#22823;&#65292;&#24182;&#19988;&#20854;&#20013;&#24456;&#22810;&#35821;&#35328;&#37117;&#26159;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#20027;&#35201;&#30340;&#20570;&#27861;&#26159;&#23545;&#22810;&#31181;&#35821;&#35328;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#26412;&#25991;&#23545;&#36825;&#31181;&#20570;&#27861;&#25552;&#20986;&#20102;&#36136;&#30097;&#65292;&#35777;&#26126;&#20102;&#38024;&#23545;&#30446;&#26631;&#35821;&#35328;&#36827;&#34892;&#21333;&#35821;&#35328;&#39044;&#35757;&#32451;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#35268;&#27169;&#21512;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#36827;&#19968;&#27493;&#20171;&#32461;&#20102;&#29992;3%&#25110;&#26356;&#23569;&#30340;&#21407;&#22987;&#39044;&#35757;&#32451;&#39044;&#31639;&#22312;&#33889;&#33796;&#29273;&#35821;&#25991;&#26412;&#19978;&#36827;&#19968;&#27493;&#39044;&#35757;&#32451;GPT-J&#21644;LLaMA&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;Poeta&#65288;&#19968;&#22871;&#30001;14&#20010;&#33889;&#33796;&#29273;&#35821;&#25968;&#25454;&#38598;&#32452;&#25104;&#30340;&#22871;&#20214;&#65289;&#19978;&#36827;&#34892;&#20102;&#23569;&#26679;&#26412;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#34920;&#29616;&#19978;&#36828;&#20248;&#20110;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#21644;&#22810;&#35821;&#35328;&#30340;&#23545;&#25163;&#12290;&#25105;&#20204;&#30340;&#26368;&#20339;&#27169;&#22411;Sabi&#225;-65B&#30340;&#34920;&#29616;&#19982;GPT-3.5-turbo&#25345;&#24179;&#12290;&#25105;&#20204;&#22312;&#30446;&#26631;&#35821;&#35328;&#20013;&#24050;&#32463;&#35774;&#24819;&#20102;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#32463;&#36807;&#32763;&#35793;&#30340;&#25968;&#25454;&#38598;&#19978;&#37117;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the capabilities of language models continue to advance, it is conceivable that "one-size-fits-all" model will remain as the main paradigm. For instance, given the vast number of languages worldwide, many of which are low-resource, the prevalent practice is to pretrain a single model on multiple languages. In this paper, we add to the growing body of evidence that challenges this practice, demonstrating that monolingual pretraining on the target language significantly improves models already extensively trained on diverse corpora. More specifically, we further pretrain GPT-J and LLaMA models on Portuguese texts using 3% or less of their original pretraining budget. Few-shot evaluations on Poeta, a suite of 14 Portuguese datasets, reveal that our models outperform English-centric and multilingual counterparts by a significant margin. Our best model, Sabi\'a-65B, performs on par with GPT-3.5-turbo. By evaluating on datasets originally conceived in the target language as well as transl
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#26631;&#39064;&#35780;&#20272;&#25351;&#26631;PAC-S&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#22270;&#20687;&#21644;&#35270;&#39057;&#30340;&#26631;&#39064;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#25351;&#26631;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#65307;&#28304;&#20195;&#30721;&#21644;&#35757;&#32451;&#27169;&#22411;&#24050;&#32463;&#20844;&#24320;&#12290;</title><link>http://arxiv.org/abs/2303.12112</link><description>&lt;p&gt;
&#22522;&#20110;&#27491;&#26679;&#26412;&#22686;&#24378;&#23545;&#27604;&#23398;&#20064;&#30340;&#22270;&#20687;&#35270;&#39057;&#26631;&#39064;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Positive-Augmented Constrastive Learning for Image and Video Captioning Evaluation. (arXiv:2303.12112v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#26631;&#39064;&#35780;&#20272;&#25351;&#26631;PAC-S&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#22270;&#20687;&#21644;&#35270;&#39057;&#30340;&#26631;&#39064;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#25351;&#26631;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#65307;&#28304;&#20195;&#30721;&#21644;&#35757;&#32451;&#27169;&#22411;&#24050;&#32463;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;CLIP&#27169;&#22411;&#22312;&#24456;&#22810;&#36328;&#27169;&#24577;&#20219;&#21153;&#19978;&#37117;&#38750;&#24120;&#26377;&#25928;&#65292;&#21253;&#25324;&#20174;&#35270;&#35273;&#21644;&#35821;&#35328;&#32467;&#26500;&#20013;&#29983;&#25104;&#30340;&#26631;&#39064;&#35780;&#20272;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#23545;&#27604;&#24230;&#30340;&#22270;&#20687;&#26631;&#39064;&#35780;&#20272;&#25351;&#26631;&#37197;&#26041;&#65292;&#21363;&#27491;&#26679;&#26412;&#22686;&#24378;&#30340;&#23545;&#27604;&#24230;&#23398;&#20064;&#20998;&#25968;&#65288;PAC-S&#65289;&#65292;&#20197;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#24335;&#32479;&#19968;&#20102;&#23545;&#27604;&#24230;&#35270;&#35273;-&#35821;&#20041;&#31354;&#38388;&#30340;&#23398;&#20064;&#21644;&#31574;&#23637;&#25968;&#25454;&#19978;&#29983;&#25104;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#28155;&#21152;&#12290;&#36328;&#36234;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26032;&#25351;&#26631;&#22312;&#22270;&#20687;&#21644;&#35270;&#39057;&#19978;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#26368;&#39640;&#65292;&#20248;&#20110;&#29616;&#26377;&#21442;&#32771;&#25351;&#26631;&#65288;&#22914;CIDEr&#21644;SPICE&#65289;&#21644;&#26080;&#21442;&#32771;&#25351;&#26631;&#65288;&#22914;CLIP-Score&#65289;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#27969;&#34892;&#30340;&#22270;&#20687;&#26631;&#39064;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#37319;&#29992;&#19981;&#21516;&#36328;&#27169;&#24577;&#29305;&#24449;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#28304;&#20195;&#30721;&#21644;&#35757;&#32451;&#27169;&#22411;&#26159;&#20844;&#24320;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The CLIP model has been recently proven to be very effective for a variety of cross-modal tasks, including the evaluation of captions generated from vision-and-language architectures. In this paper, we propose a new recipe for a contrastive-based evaluation metric for image captioning, namely Positive-Augmented Contrastive learning Score (PAC-S), that in a novel way unifies the learning of a contrastive visual-semantic space with the addition of generated images and text on curated data. Experiments spanning several datasets demonstrate that our new metric achieves the highest correlation with human judgments on both images and videos, outperforming existing reference-based metrics like CIDEr and SPICE and reference-free metrics like CLIP-Score. Finally, we test the system-level correlation of the proposed metric when considering popular image captioning approaches, and assess the impact of employing different cross-modal features. Our source code and trained models are publicly availa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;ChatGPT&#21644;GPT-4&#30340;&#25968;&#23398;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#26032;&#30340;&#26041;&#27861;&#20197;&#21450;&#21457;&#24067;&#20004;&#20010;&#26032;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#21508;&#20010;&#32500;&#24230;&#30340;&#25968;&#23398;&#25512;&#29702;&#19978;&#30340;&#34920;&#29616;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#28085;&#30422;&#30740;&#31350;&#29983;&#32423;&#25968;&#23398;&#24182;&#30001;&#25968;&#23398;&#30740;&#31350;&#20154;&#21592;&#31574;&#21010;&#30340;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#20063;&#27979;&#35797;&#20102;&#23427;&#20204;&#20316;&#20026;&#19987;&#19994;&#25968;&#23398;&#23478;&#21161;&#25163;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2301.13867</link><description>&lt;p&gt;
ChatGPT&#30340;&#25968;&#23398;&#33021;&#21147;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Mathematical Capabilities of ChatGPT. (arXiv:2301.13867v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13867
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;ChatGPT&#21644;GPT-4&#30340;&#25968;&#23398;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#26032;&#30340;&#26041;&#27861;&#20197;&#21450;&#21457;&#24067;&#20004;&#20010;&#26032;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#21508;&#20010;&#32500;&#24230;&#30340;&#25968;&#23398;&#25512;&#29702;&#19978;&#30340;&#34920;&#29616;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#28085;&#30422;&#30740;&#31350;&#29983;&#32423;&#25968;&#23398;&#24182;&#30001;&#25968;&#23398;&#30740;&#31350;&#20154;&#21592;&#31574;&#21010;&#30340;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#20063;&#27979;&#35797;&#20102;&#23427;&#20204;&#20316;&#20026;&#19987;&#19994;&#25968;&#23398;&#23478;&#21161;&#25163;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23545;ChatGPT&#65288;&#21457;&#24067;&#20110;2023&#24180;1&#26376;9&#26085;&#21644;1&#26376;30&#26085;&#65289;&#21644;GPT-4&#30340;&#25968;&#23398;&#33021;&#21147;&#36827;&#34892;&#20102;&#35843;&#26597;&#21644;&#27979;&#35797;&#65292;&#20351;&#29992;&#20102;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#20197;&#21450;&#25163;&#24037;&#21046;&#20316;&#30340;&#25968;&#25454;&#38598;&#12290;&#19982;&#27491;&#24335;&#25968;&#23398;&#19981;&#21516;&#65292;&#27491;&#24335;&#35777;&#26126;&#30340;&#22823;&#22411;&#25968;&#25454;&#24211;&#21487;&#20379;&#20351;&#29992;&#65288;&#20363;&#22914;&#65292;Lean&#25968;&#23398;&#24211;&#65289;&#65292;&#24403;&#21069;&#29992;&#20110;&#22522;&#20934;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#25968;&#23398;&#25968;&#25454;&#38598;&#35201;&#20040;&#21482;&#28085;&#30422;&#22522;&#30784;&#25968;&#23398;&#65292;&#35201;&#20040;&#38750;&#24120;&#23567;&#12290;&#25105;&#20204;&#36890;&#36807;&#20844;&#24320;&#21457;&#24067;&#20004;&#20010;&#26032;&#25968;&#25454;&#38598;&#65306;GHOSTS&#21644;miniGHOSTS&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36825;&#26159;&#30001;&#25968;&#23398;&#30740;&#31350;&#20154;&#21592;&#31934;&#24515;&#31574;&#21010;&#30340;&#31532;&#19968;&#20010;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#28085;&#30422;&#30740;&#31350;&#29983;&#32423;&#25968;&#23398;&#12289;&#25552;&#20379;&#23545;&#35821;&#35328;&#27169;&#22411;&#25968;&#23398;&#33021;&#21147;&#30340;&#25972;&#20307;&#27010;&#36848;&#65292;&#24182;&#21306;&#20998;&#25968;&#23398;&#25512;&#29702;&#30340;&#22810;&#20010;&#26041;&#38754;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#36824;&#27979;&#35797;&#20102;ChatGPT&#21644;GPT-4&#26159;&#21542;&#21487;&#20197;&#25104;&#20026;&#19987;&#19994;&#25968;&#23398;&#23478;&#30340;&#26377;&#29992;&#21161;&#25163;&#65292;&#27169;&#25311;&#20854;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the mathematical capabilities of two iterations of ChatGPT (released 9-January-2023 and 30-January-2023) and of GPT-4 by testing them on publicly available datasets, as well as hand-crafted ones, using a novel methodology. In contrast to formal mathematics, where large databases of formal proofs are available (e.g., the Lean Mathematical Library), current datasets of natural-language mathematics, used to benchmark language models, either cover only elementary mathematics or are very small. We address this by publicly releasing two new datasets: GHOSTS and miniGHOSTS. These are the first natural-language datasets curated by working researchers in mathematics that (1) aim to cover graduate-level mathematics, (2) provide a holistic overview of the mathematical capabilities of language models, and (3) distinguish multiple dimensions of mathematical reasoning. These datasets also test whether ChatGPT and GPT-4 can be helpful assistants to professional mathematicians by emulat
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;PPOCoder&#26694;&#26550;&#23558;&#39044;&#35757;&#32451;&#30340;&#32534;&#31243;&#35821;&#35328;&#27169;&#22411;&#21644;Proximal Policy Optimization&#25216;&#26415;&#32467;&#21512;&#65292;&#36890;&#36807;&#21033;&#29992;&#20195;&#30721;&#25191;&#34892;&#21644;&#32467;&#26500;&#23545;&#40784;&#30340;&#38750;&#21487;&#24494;&#21453;&#39304;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#20195;&#30721;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2301.13816</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22522;&#20110;&#25191;&#34892;&#30340;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Execution-based Code Generation using Deep Reinforcement Learning. (arXiv:2301.13816v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13816
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;PPOCoder&#26694;&#26550;&#23558;&#39044;&#35757;&#32451;&#30340;&#32534;&#31243;&#35821;&#35328;&#27169;&#22411;&#21644;Proximal Policy Optimization&#25216;&#26415;&#32467;&#21512;&#65292;&#36890;&#36807;&#21033;&#29992;&#20195;&#30721;&#25191;&#34892;&#21644;&#32467;&#26500;&#23545;&#40784;&#30340;&#38750;&#21487;&#24494;&#21453;&#39304;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#20195;&#30721;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22312;&#22823;&#35268;&#27169;&#20195;&#30721;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;&#30340;&#32534;&#31243;&#35821;&#35328;&#65288;PL&#65289;&#27169;&#22411;&#65292;&#20316;&#20026;&#33258;&#21160;&#21270;&#36719;&#20214;&#24037;&#31243;&#36807;&#31243;&#30340;&#25163;&#27573;&#65292;&#22312;&#20195;&#30721;&#23436;&#25104;&#12289;&#20195;&#30721;&#32763;&#35793;&#21644;&#31243;&#24207;&#21512;&#25104;&#31561;&#21508;&#31181;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#30456;&#24403;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#20174;&#25991;&#26412;&#29983;&#25104;&#20013;&#20511;&#29992;&#30340;&#30417;&#30563;&#24494;&#35843;&#30446;&#26631;&#65292;&#24573;&#35270;&#20102;&#20195;&#30721;&#30340;&#29420;&#29305;&#24207;&#21015;&#32423;&#29305;&#24449;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#21487;&#32534;&#35793;&#24615;&#20197;&#21450;&#35821;&#27861;&#21644;&#21151;&#33021;&#27491;&#30830;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PPOCoder&#65292;&#19968;&#31181;&#26032;&#30340;&#20195;&#30721;&#29983;&#25104;&#26694;&#26550;&#65292;&#23427;&#23558;&#39044;&#35757;&#32451;&#30340;PL&#27169;&#22411;&#19982;Proximal Policy Optimization&#65288;PPO&#65289;&#30456;&#32467;&#21512;&#65292;PPO&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#12290;&#36890;&#36807;&#21033;&#29992;&#20195;&#30721;&#25191;&#34892;&#21644;&#32467;&#26500;&#23545;&#40784;&#30340;&#38750;&#21487;&#24494;&#21453;&#39304;&#65292;PPOCoder&#23558;&#22806;&#37096;&#20195;&#30721;&#29305;&#23450;&#30693;&#35782;&#26080;&#32541;&#38598;&#25104;&#21040;&#27169;&#22411;&#20248;&#21270;&#36807;&#31243;&#20013;&#12290;&#36825;&#26159;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The utilization of programming language (PL) models, pre-trained on large-scale code corpora, as a means of automating software engineering processes has demonstrated considerable potential in streamlining various code generation tasks such as code completion, code translation, and program synthesis. However, current approaches mainly rely on supervised fine-tuning objectives borrowed from text generation, neglecting unique sequence-level characteristics of code, including but not limited to compilability as well as syntactic and functional correctness. To address this limitation, we propose PPOCoder, a new framework for code generation that synergistically combines pre-trained PL models with Proximal Policy Optimization (PPO) which is a widely used deep reinforcement learning technique. By utilizing non-differentiable feedback from code execution and structure alignment, PPOCoder seamlessly integrates external code-specific knowledge into the model optimization process. It's important
&lt;/p&gt;</description></item><item><title>ThoughtSource&#26159;&#19968;&#20010;&#29992;&#20110;&#36830;&#32493;&#24605;&#32771;&#25512;&#29702;&#30340;&#20803;&#25968;&#25454;&#38598;&#21644;&#36719;&#20214;&#24211;&#65292;&#26088;&#22312;&#36890;&#36807;&#20419;&#36827;&#23545;&#36830;&#32493;&#24605;&#32771;&#30340;&#23450;&#24615;&#29702;&#35299;&#12289;&#23454;&#35777;&#35780;&#20272;&#21644;&#25552;&#20379;&#35757;&#32451;&#25968;&#25454;&#65292;&#25913;&#36827;&#26410;&#26469;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2301.11596</link><description>&lt;p&gt;
ThoughtSource:&#19968;&#20010;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#25968;&#25454;&#30340;&#20013;&#22830;&#26530;&#32445;&#12290;
&lt;/p&gt;
&lt;p&gt;
ThoughtSource: A central hub for large language model reasoning data. (arXiv:2301.11596v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11596
&lt;/p&gt;
&lt;p&gt;
ThoughtSource&#26159;&#19968;&#20010;&#29992;&#20110;&#36830;&#32493;&#24605;&#32771;&#25512;&#29702;&#30340;&#20803;&#25968;&#25454;&#38598;&#21644;&#36719;&#20214;&#24211;&#65292;&#26088;&#22312;&#36890;&#36807;&#20419;&#36827;&#23545;&#36830;&#32493;&#24605;&#32771;&#30340;&#23450;&#24615;&#29702;&#35299;&#12289;&#23454;&#35777;&#35780;&#20272;&#21644;&#25552;&#20379;&#35757;&#32451;&#25968;&#25454;&#65292;&#25913;&#36827;&#26410;&#26469;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20687;GPT-4&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#19978;&#20173;&#23384;&#22312;&#38480;&#21046;&#65292;&#23427;&#20204;&#30340;&#25512;&#29702;&#36807;&#31243;&#19981;&#36879;&#26126;&#65292;&#23481;&#26131;&#20135;&#29983;&#8220;&#24187;&#35273;&#8221;&#20107;&#23454;&#65292;&#24182;&#19988;&#23384;&#22312;&#20854;&#28508;&#22312;&#20559;&#35265;&#30340;&#25285;&#24551;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#36830;&#32493;&#24605;&#32771;&#25552;&#31034;&#30340;&#25216;&#26415;&#65292;&#35753;&#27169;&#22411;&#20197;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#34920;&#36798;&#25512;&#29702;&#27493;&#39588;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ThoughtSource&#65292;&#19968;&#20010;&#29992;&#20110;&#36830;&#32493;&#24605;&#32771;&#25512;&#29702;&#30340;&#20803;&#25968;&#25454;&#38598;&#21644;&#36719;&#20214;&#24211;&#12290;ThoughtSource&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20419;&#36827;&#23545;&#36830;&#32493;&#24605;&#32771;&#30340;&#23450;&#24615;&#29702;&#35299;&#12289;&#23454;&#35777;&#35780;&#20272;&#21644;&#25552;&#20379;&#35757;&#32451;&#25968;&#25454;&#26469;&#25913;&#36827;&#26410;&#26469;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;ThoughtSource&#30340;&#39318;&#27425;&#21457;&#24067;&#38598;&#25104;&#20102;&#20845;&#20010;&#31185;&#23398;/&#21307;&#23398;&#12289;&#19977;&#20010;&#36890;&#29992;&#39046;&#22495;&#21644;&#20116;&#20010;&#25968;&#23398;&#39064;&#31572;&#26696;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) such as GPT-4 have recently demonstrated impressive results across a wide range of tasks. LLMs are still limited, however, in that they frequently fail at complex reasoning, their reasoning processes are opaque, they are prone to 'hallucinate' facts, and there are concerns about their underlying biases. Letting models verbalize reasoning steps as natural language, a technique known as chain-of-thought prompting, has recently been proposed as a way to address some of these issues. Here we present ThoughtSource, a meta-dataset and software library for chain-of-thought (CoT) reasoning. The goal of ThoughtSource is to improve future artificial intelligence systems by facilitating qualitative understanding of CoTs, enabling empirical evaluations, and providing training data. This first release of ThoughtSource integrates six scientific/medical, three general-domain and five math word question answering datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#25991;&#26412;&#30340;&#35821;&#38899;&#27604;&#36739;&#24230;&#37327;&#26041;&#27861;&#65292;&#21033;&#29992;&#35821;&#38899;&#21040;&#21333;&#20803;&#32534;&#30721;&#22120;&#36716;&#25442;&#35821;&#38899;&#34920;&#36848;&#20026;&#22768;&#23398;&#21333;&#20803;&#65292;&#24182;&#25552;&#20986;&#20102;&#19982;&#25991;&#26412;&#22522;&#20934;&#23494;&#20999;&#23545;&#24212;&#30340;&#22522;&#20110;&#35821;&#38899;&#30340;&#24230;&#37327;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#21475;&#36848;&#35821;&#35328;&#21644;&#27809;&#26377;&#21487;&#38752;ASR&#31995;&#32479;&#30340;&#35821;&#38899;&#32763;&#35793;&#35780;&#20272;&#65292;&#36991;&#20813;&#20351;&#29992;ASR&#36716;&#24405;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2210.11835</link><description>&lt;p&gt;
&#19968;&#31181;&#26080;&#25991;&#26412;&#30340;&#35821;&#38899;&#27604;&#36739;&#24230;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Textless Metric for Speech-to-Speech Comparison. (arXiv:2210.11835v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.11835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#25991;&#26412;&#30340;&#35821;&#38899;&#27604;&#36739;&#24230;&#37327;&#26041;&#27861;&#65292;&#21033;&#29992;&#35821;&#38899;&#21040;&#21333;&#20803;&#32534;&#30721;&#22120;&#36716;&#25442;&#35821;&#38899;&#34920;&#36848;&#20026;&#22768;&#23398;&#21333;&#20803;&#65292;&#24182;&#25552;&#20986;&#20102;&#19982;&#25991;&#26412;&#22522;&#20934;&#23494;&#20999;&#23545;&#24212;&#30340;&#22522;&#20110;&#35821;&#38899;&#30340;&#24230;&#37327;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#21475;&#36848;&#35821;&#35328;&#21644;&#27809;&#26377;&#21487;&#38752;ASR&#31995;&#32479;&#30340;&#35821;&#38899;&#32763;&#35793;&#35780;&#20272;&#65292;&#36991;&#20813;&#20351;&#29992;ASR&#36716;&#24405;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#29992;&#20110;&#27604;&#36739;&#35821;&#38899;&#34920;&#36848;&#32780;&#26080;&#38656;&#20381;&#36182;&#25991;&#26412;&#36716;&#24405;&#12290;&#25105;&#20204;&#30340;&#35821;&#38899;&#23545;&#35821;&#38899;&#27604;&#36739;&#24230;&#37327;&#21033;&#29992;&#20102;&#26368;&#20808;&#36827;&#30340;&#35821;&#38899;&#21040;&#21333;&#20803;&#32534;&#30721;&#22120;&#65288;&#22914;HuBERT&#65289;&#23558;&#35821;&#38899;&#34920;&#36848;&#36716;&#25442;&#20026;&#31163;&#25955;&#30340;&#22768;&#23398;&#21333;&#20803;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26131;&#22797;&#21046;&#30340;&#31070;&#32463;&#26550;&#26500;&#65292;&#23398;&#20064;&#19968;&#31181;&#19982;&#25991;&#26412;&#22522;&#20934;&#24230;&#37327;&#23494;&#20999;&#23545;&#24212;&#30340;&#22522;&#20110;&#35821;&#38899;&#30340;&#24230;&#37327;&#12290;&#36825;&#31181;&#26080;&#25991;&#26412;&#24230;&#37327;&#20855;&#26377;&#35768;&#22810;&#28508;&#22312;&#24212;&#29992;&#65292;&#21253;&#25324;&#35780;&#20272;&#21475;&#22836;&#35821;&#35328;&#12289;&#27809;&#26377;&#21487;&#38752;ASR&#31995;&#32479;&#30340;&#35821;&#35328;&#30340;&#35821;&#38899;&#32763;&#35793;&#65292;&#25110;&#32773;&#23436;&#20840;&#36991;&#20813;&#20351;&#29992;ASR&#36716;&#24405;&#30340;&#38656;&#27714;&#12290;&#26412;&#25991;&#36824;&#34920;&#26126;&#65292;&#22312;&#35821;&#38899;&#32763;&#35793;&#35780;&#20272;&#20013;&#65292;ASR-BLEU&#65288;&#36890;&#36807;&#33258;&#21160;&#36716;&#24405;&#35821;&#38899;&#20551;&#35774;&#21644;&#21442;&#32771;&#35821;&#38899;&#65292;&#24182;&#35745;&#31639;&#36716;&#24405;&#20043;&#38388;&#30340;&#21477;&#23376;&#32423;BLEU&#65289;&#21363;&#20351;ASR&#31995;&#32479;&#24378;&#22823;&#65292;&#20063;&#26159;&#30495;&#23454;&#25991;&#26412;BLEU&#30340;&#19968;&#20010;&#36739;&#24046;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a new and simple method for comparing speech utterances without relying on text transcripts. Our speech-to-speech comparison metric utilizes state-of-the-art speech2unit encoders like HuBERT to convert speech utterances into discrete acoustic units. We then propose a simple and easily replicable neural architecture that learns a speech-based metric that closely corresponds to its text-based counterpart. This textless metric has numerous potential applications, including evaluating speech-to-speech translation for oral languages, languages without dependable ASR systems, or to avoid the need for ASR transcription altogether. This paper also shows that for speech-to-speech translation evaluation, ASR-BLEU (which consists in automatically transcribing both speech hypothesis and reference and compute sentence-level BLEU between transcripts) is a poor proxy to real text-BLEU even when ASR system is strong.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#27010;&#29575;&#20998;&#24067;&#32534;&#30721;&#22120;&#36827;&#34892;&#22810;&#27169;&#24577;&#19981;&#30830;&#23450;&#24615;&#24314;&#27169;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;MAP&#65292;&#35813;&#27169;&#22411;&#22312;&#22810;&#39033;&#19979;&#28216;&#20219;&#21153;&#20013;&#22343;&#34920;&#29616;&#20248;&#24322;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2210.05335</link><description>&lt;p&gt;
MAP&#65306;&#22810;&#27169;&#24577;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MAP: Multimodal Uncertainty-Aware Vision-Language Pre-training Model. (arXiv:2210.05335v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#27010;&#29575;&#20998;&#24067;&#32534;&#30721;&#22120;&#36827;&#34892;&#22810;&#27169;&#24577;&#19981;&#30830;&#23450;&#24615;&#24314;&#27169;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;MAP&#65292;&#35813;&#27169;&#22411;&#22312;&#22810;&#39033;&#19979;&#28216;&#20219;&#21153;&#20013;&#22343;&#34920;&#29616;&#20248;&#24322;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#35821;&#20041;&#29702;&#35299;&#32463;&#24120;&#38656;&#35201;&#22788;&#29702;&#19981;&#30830;&#23450;&#24615;&#65292;&#23548;&#33268;&#25152;&#33719;&#24471;&#30340;&#20449;&#24687;&#20542;&#21521;&#20110;&#28041;&#21450;&#22810;&#20010;&#30446;&#26631;&#12290;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#23545;&#25105;&#20204;&#30340;&#35299;&#37322;&#26469;&#35828;&#26159;&#26377;&#38382;&#39064;&#30340;&#65292;&#21253;&#25324;&#36328;&#27169;&#24577;&#21644;&#20869;&#37096;&#27169;&#24577;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#40092;&#26377;&#30740;&#31350;&#25506;&#35752;&#35813;&#19981;&#30830;&#23450;&#24615;&#30340;&#24314;&#27169;&#65292;&#29305;&#21035;&#26159;&#22312;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#21644;&#22312;&#29305;&#23450;&#20219;&#21153;&#19979;&#28216;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#24207;&#21015;&#32423;&#20132;&#20114;&#36890;&#36807;&#27010;&#29575;&#20998;&#24067;&#32534;&#30721;&#22120;&#65288;PDE&#65289;&#23558;&#25152;&#26377;&#27169;&#24577;&#30340;&#34920;&#31034;&#25237;&#24433;&#20026;&#27010;&#29575;&#20998;&#24067;&#12290;&#19982;&#29616;&#26377;&#30340;&#30830;&#23450;&#24615;&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#24314;&#27169;&#21487;&#20197;&#20256;&#36882;&#26356;&#20016;&#23500;&#30340;&#22810;&#27169;&#24577;&#35821;&#20041;&#20449;&#24687;&#21644;&#26356;&#22797;&#26434;&#30340;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#19981;&#30830;&#23450;&#24615;&#24314;&#27169;&#19982;&#27969;&#34892;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#32467;&#21512;&#36215;&#26469;&#65292;&#24182;&#25552;&#20986;&#36866;&#21512;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;:&#22522;&#20110;&#20998;&#24067;&#30340;&#35270;&#35273;&#35821;&#35328;&#23545;&#27604;&#65288;D-VLC&#65289;&#12289;&#22522;&#20110;&#20998;&#24067;&#30340;&#25513;&#34109;&#35821;&#35328;&#24314;&#27169;&#65288;D-MLM&#65289;&#21644;&#20998;&#24067;&#24335;&#22270;&#20687;&#26816;&#32034;&#65288;D-IR&#65289;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#8212;&#8212;&#22810;&#27169;&#24577;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;MAP&#65289;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21253;&#25324;&#35270;&#35273;&#38382;&#31572;&#12289;&#22270;&#20687;&#23383;&#24149;&#21644;&#25351;&#31216;&#34920;&#36798;&#29702;&#35299;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MAP&#27604;&#20854;&#30830;&#23450;&#24615;&#23545;&#24212;&#29289;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#22312;&#20960;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal semantic understanding often has to deal with uncertainty, which means the obtained messages tend to refer to multiple targets. Such uncertainty is problematic for our interpretation, including inter- and intra-modal uncertainty. Little effort has studied the modeling of this uncertainty, particularly in pre-training on unlabeled datasets and fine-tuning in task-specific downstream datasets. In this paper, we project the representations of all modalities as probabilistic distributions via a Probability Distribution Encoder (PDE) by utilizing sequence-level interactions. Compared to the existing deterministic methods, such uncertainty modeling can convey richer multimodal semantic information and more complex relationships. Furthermore, we integrate uncertainty modeling with popular pre-training frameworks and propose suitable pre-training tasks: Distribution-based Vision-Language Contrastive learning (D-VLC), Distribution-based Masked Language Modeling (D-MLM), and Distribut
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#22312;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#39044;&#27979;&#24615;&#38382;&#39064;&#22238;&#31572;&#12290;&#35813;&#20219;&#21153;&#23545;&#20110;&#20154;&#20204;&#23547;&#27714;&#26410;&#26469;&#35745;&#21010;&#38750;&#24120;&#37325;&#35201;&#65292;&#24182;&#19988;&#26159;&#20808;&#21069;&#30740;&#31350;&#20013;&#26410;&#34987;&#25506;&#32034;&#30340;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2208.06501</link><description>&lt;p&gt;
ForecastTKGQuestions: &#19968;&#20010;&#29992;&#20110;&#26102;&#38388;&#24615;&#38382;&#39064;&#22238;&#31572;&#21644;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#39044;&#27979;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
ForecastTKGQuestions: A Benchmark for Temporal Question Answering and Forecasting over Temporal Knowledge Graphs. (arXiv:2208.06501v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.06501
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#22312;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#39044;&#27979;&#24615;&#38382;&#39064;&#22238;&#31572;&#12290;&#35813;&#20219;&#21153;&#23545;&#20110;&#20154;&#20204;&#23547;&#27714;&#26410;&#26469;&#35745;&#21010;&#38750;&#24120;&#37325;&#35201;&#65292;&#24182;&#19988;&#26159;&#20808;&#21069;&#30740;&#31350;&#20013;&#26410;&#34987;&#25506;&#32034;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#20110;&#26102;&#38388;&#24615;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#65288;TKGQA&#65289;&#30340;&#20852;&#36259;&#36880;&#28176;&#22686;&#21152;&#12290;TKGQA&#38656;&#35201;&#26102;&#38388;&#25512;&#29702;&#25216;&#26415;&#26469;&#20174;&#26102;&#38388;&#30693;&#35782;&#24211;&#20013;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#12290;&#29616;&#26377;&#30340;TKGQA&#25968;&#25454;&#38598;&#21482;&#21253;&#21547;&#22522;&#20110;&#22266;&#23450;&#26102;&#38388;&#27573;&#30340;&#26102;&#38388;&#24615;&#38382;&#39064;&#65292;&#35813;&#26102;&#38388;&#27573;&#20869;&#30340;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#65288;TKG&#65289;&#21487;&#20197;&#23436;&#20840;&#29992;&#20110;&#31572;&#26696;&#25512;&#29702;&#65292;&#20801;&#35768;TKGQA&#27169;&#22411;&#21033;&#29992;&#26410;&#26469;&#30693;&#35782;&#26469;&#22238;&#31572;&#22522;&#20110;&#36807;&#21435;&#20107;&#23454;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#37492;&#20110;&#21040;&#30446;&#21069;&#20026;&#27490;&#30340;&#30693;&#35782;&#65292;&#25105;&#20204;&#20063;&#24076;&#26395;TKGQA&#31995;&#32479;&#33021;&#22815;&#22238;&#31572;&#20851;&#20110;&#26410;&#26469;&#30340;&#38382;&#39064;&#12290;&#30001;&#20110;&#20154;&#20204;&#19981;&#26029;&#23547;&#27714;&#26410;&#26469;&#30340;&#35745;&#21010;&#65292;&#26500;&#24314;&#33021;&#22815;&#22238;&#31572;&#36825;&#31181;&#39044;&#27979;&#24615;&#38382;&#39064;&#30340;TKGQA&#31995;&#32479;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#20808;&#21069;&#30340;&#30740;&#31350;&#20013;&#65292;&#36825;&#19968;&#39046;&#22495;&#20173;&#28982;&#26410;&#34987;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65306;&#39044;&#27979;&#24615;&#38382;&#39064;&#22238;&#31572;&#30340;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Question answering over temporal knowledge graphs (TKGQA) has recently found increasing interest. TKGQA requires temporal reasoning techniques to extract the relevant information from temporal knowledge bases. The only existing TKGQA dataset, i.e., CronQuestions, consists of temporal questions based on the facts from a fixed time period, where a temporal knowledge graph (TKG) spanning the same period can be fully used for answer inference, allowing the TKGQA models to use even the future knowledge to answer the questions based on the past facts. In real-world scenarios, however, it is also common that given the knowledge until now, we wish the TKGQA systems to answer the questions asking about the future. As humans constantly seek plans for the future, building TKGQA systems for answering such forecasting questions is important. Nevertheless, this has still been unexplored in previous research. In this paper, we propose a novel task: forecasting question answering over temporal knowled
&lt;/p&gt;</description></item><item><title>ABNIRML&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#65292;&#20998;&#26512;&#20102;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#21253;&#25324;&#20889;&#20316;&#39118;&#26684;&#12289;&#20107;&#23454;&#24615;&#12289;&#23545;&#25913;&#20889;&#21644;&#35789;&#24207;&#30340;&#25935;&#24863;&#24615;&#31561;&#29305;&#24449;&#12290;&#36890;&#36807;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#25506;&#31350;&#20102;&#31070;&#32463;&#27169;&#22411;&#22686;&#30410;&#30340;&#22240;&#32032;&#65292;&#24182;&#21457;&#29616;&#20102;&#28508;&#22312;&#30340;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2011.00696</link><description>&lt;p&gt;
ABNIRML: &#20998;&#26512;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#30340;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
ABNIRML: Analyzing the Behavior of Neural IR Models. (arXiv:2011.00696v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2011.00696
&lt;/p&gt;
&lt;p&gt;
ABNIRML&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#65292;&#20998;&#26512;&#20102;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#21253;&#25324;&#20889;&#20316;&#39118;&#26684;&#12289;&#20107;&#23454;&#24615;&#12289;&#23545;&#25913;&#20889;&#21644;&#35789;&#24207;&#30340;&#25935;&#24863;&#24615;&#31561;&#29305;&#24449;&#12290;&#36890;&#36807;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#25506;&#31350;&#20102;&#31070;&#32463;&#27169;&#22411;&#22686;&#30410;&#30340;&#22240;&#32032;&#65292;&#24182;&#21457;&#29616;&#20102;&#28508;&#22312;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#19978;&#19979;&#25991;&#21270;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;BERT&#21644;T5&#65289;&#24050;&#32463;&#24314;&#31435;&#20102;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#30340;&#26032;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#36824;&#19981;&#23436;&#20840;&#29702;&#35299;&#20026;&#20160;&#20040;&#36825;&#20123;&#26041;&#27861;&#22914;&#27492;&#26377;&#25928;&#65292;&#26159;&#20160;&#20040;&#20351;&#19968;&#20123;&#21464;&#31181;&#27604;&#20854;&#20182;&#21464;&#31181;&#26356;&#26377;&#25928;&#65292;&#20197;&#21450;&#23427;&#20204;&#21487;&#33021;&#23384;&#22312;&#21738;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#26469;&#20998;&#26512;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#34892;&#20026;&#65288;ABNIRML&#65289;&#65292;&#21253;&#25324;&#26032;&#30340;&#35786;&#26029;&#25506;&#38024;&#31867;&#22411;&#65292;&#20801;&#35768;&#25105;&#20204;&#27979;&#35797;&#20808;&#21069;&#25216;&#26415;&#26410;&#35299;&#20915;&#30340;&#20960;&#20010;&#29305;&#24449;&#65292;&#20363;&#22914;&#20889;&#20316;&#39118;&#26684;&#65292;&#20107;&#23454;&#24615;&#65292;&#23545;&#25913;&#20889;&#21644;&#35789;&#24207;&#30340;&#25935;&#24863;&#24615;&#31561;&#12290;&#20026;&#20102;&#23637;&#31034;&#36825;&#20010;&#26694;&#26550;&#30340;&#20215;&#20540;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#24471;&#20986;&#20102;&#23545;&#31070;&#32463;&#27169;&#22411;&#22686;&#30410;&#22240;&#32032;&#30340;&#35265;&#35299;&#65292;&#24182;&#30830;&#23450;&#20102;&#27169;&#22411;&#21487;&#33021;&#23384;&#22312;&#30340;&#28508;&#22312;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#19968;&#20123;&#32467;&#26524;&#35777;&#23454;&#20102;&#20256;&#32479;&#30340;&#35266;&#28857;&#65292;&#20363;&#22914;&#26368;&#36817;&#30340;&#31070;&#32463;&#25490;&#24207;&#27169;&#22411;&#23545;&#26597;&#35810;&#30340;&#30830;&#20999;&#26415;&#35821;&#21305;&#37197;&#30340;&#20381;&#36182;&#31243;&#24230;&#36739;&#20302;&#65292;&#32780;&#26159;&#21033;&#29992;&#26356;&#20016;&#23500;&#30340;&#35821;&#35328;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained contextualized language models such as BERT and T5 have established a new state-of-the-art for ad-hoc search. However, it is not yet well-understood why these methods are so effective, what makes some variants more effective than others, and what pitfalls they may have. We present a new comprehensive framework for Analyzing the Behavior of Neural IR ModeLs (ABNIRML), which includes new types of diagnostic probes that allow us to test several characteristics -- such as writing styles, factuality, sensitivity to paraphrasing and word order -- that are not addressed by previous techniques. To demonstrate the value of the framework, we conduct an extensive empirical study that yields insights into the factors that contribute to the neural model's gains, and identify potential unintended biases the models exhibit. Some of our results confirm conventional wisdom, like that recent neural ranking models rely less on exact term overlap with the query, and instead leverage richer ling
&lt;/p&gt;</description></item></channel></rss>