<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20195;&#29702;&#30340;&#31995;&#32479;VideoAgent&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20013;&#22830;&#20195;&#29702;&#65292;&#37319;&#29992;&#20114;&#21160;&#25512;&#29702;&#21644;&#35745;&#21010;&#26469;&#22788;&#29702;&#38271;&#35270;&#39057;&#29702;&#35299;&#38382;&#39064;&#65292;&#22312;&#25361;&#25112;&#24615;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.10517</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35270;&#39057;&#20195;&#29702;&#65306;&#38271;&#35270;&#39057;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
VideoAgent: Long-form Video Understanding with Large Language Model as Agent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10517
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20195;&#29702;&#30340;&#31995;&#32479;VideoAgent&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20013;&#22830;&#20195;&#29702;&#65292;&#37319;&#29992;&#20114;&#21160;&#25512;&#29702;&#21644;&#35745;&#21010;&#26469;&#22788;&#29702;&#38271;&#35270;&#39057;&#29702;&#35299;&#38382;&#39064;&#65292;&#22312;&#25361;&#25112;&#24615;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#35270;&#39057;&#29702;&#35299;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#20195;&#34920;&#30528;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#38656;&#35201;&#19968;&#20010;&#33021;&#22815;&#25512;&#29702;&#38271;&#26102;&#38388;&#22810;&#27169;&#24577;&#24207;&#21015;&#30340;&#27169;&#22411;&#12290;&#21463;&#20154;&#31867;&#35748;&#30693;&#38271;&#35270;&#39057;&#36807;&#31243;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24378;&#35843;&#20114;&#21160;&#25512;&#29702;&#21644;&#35745;&#21010;&#65292;&#32780;&#19981;&#26159;&#22788;&#29702;&#38271;&#31687;&#35270;&#35273;&#36755;&#20837;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#20195;&#29702;&#30340;&#31995;&#32479;VideoAgent&#65292;&#23427;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20013;&#22830;&#20195;&#29702;&#65292;&#36845;&#20195;&#22320;&#35782;&#21035;&#21644;&#25972;&#29702;&#20851;&#38190;&#20449;&#24687;&#20197;&#22238;&#31572;&#38382;&#39064;&#65292;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#20316;&#20026;&#24037;&#20855;&#26469;&#32763;&#35793;&#21644;&#26816;&#32034;&#35270;&#35273;&#20449;&#24687;&#12290;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;EgoSchema&#21644;NExT-QA&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;VideoAgent&#22312;&#24179;&#22343;&#20165;&#20351;&#29992;8.4&#21644;8.2&#24103;&#30340;&#24773;&#20917;&#19979;&#20998;&#21035;&#23454;&#29616;&#20102;54.1%&#21644;71.3%&#30340;&#38646;-shot&#20934;&#30830;&#29575;&#12290;&#36825;&#20123;&#32467;&#26524;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30456;&#23545;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#21331;&#36234;&#25928;&#26524;&#21644;&#25928;&#29575;&#65292;&#31361;&#20986;&#20102;&#20195;&#29702;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10517v1 Announce Type: cross  Abstract: Long-form video understanding represents a significant challenge within computer vision, demanding a model capable of reasoning over long multi-modal sequences. Motivated by the human cognitive process for long-form video understanding, we emphasize interactive reasoning and planning over the ability to process lengthy visual inputs. We introduce a novel agent-based system, VideoAgent, that employs a large language model as a central agent to iteratively identify and compile crucial information to answer a question, with vision-language foundation models serving as tools to translate and retrieve visual information. Evaluated on the challenging EgoSchema and NExT-QA benchmarks, VideoAgent achieves 54.1% and 71.3% zero-shot accuracy with only 8.4 and 8.2 frames used on average. These results demonstrate superior effectiveness and efficiency of our method over the current state-of-the-art methods, highlighting the potential of agent-base
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;CLIP&#36827;&#34892;&#22823;&#35268;&#27169;&#40065;&#26834;&#24615;&#22522;&#20934;&#27979;&#35797;&#65292;&#25581;&#31034;&#20102;&#20854;&#22312;&#28085;&#30422;&#33258;&#28982;&#20998;&#24067;&#20559;&#31227;&#12289;&#21512;&#25104;&#20998;&#24067;&#20559;&#31227;&#21644;&#23545;&#25239;&#25915;&#20987;&#31561;&#22810;&#20010;&#26041;&#38754;&#30340;&#20248;&#24322;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.10499</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#40065;&#26834;&#24615;&#22522;&#20934;&#27979;&#35797;&#65306;&#19968;&#39033;&#35797;&#28857;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Zero-Shot Robustness of Multimodal Foundation Models: A Pilot Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10499
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;CLIP&#36827;&#34892;&#22823;&#35268;&#27169;&#40065;&#26834;&#24615;&#22522;&#20934;&#27979;&#35797;&#65292;&#25581;&#31034;&#20102;&#20854;&#22312;&#28085;&#30422;&#33258;&#28982;&#20998;&#24067;&#20559;&#31227;&#12289;&#21512;&#25104;&#20998;&#24067;&#20559;&#31227;&#21644;&#23545;&#25239;&#25915;&#20987;&#31561;&#22810;&#20010;&#26041;&#38754;&#30340;&#20248;&#24322;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20174;&#20851;&#20110;&#22270;&#20687;&#30340;&#21407;&#22987;&#25991;&#26412;&#20013;&#39044;&#35757;&#32451;&#22270;&#20687;&#34920;&#31034;&#65292;&#20351;&#24471;&#38646;&#26679;&#26412;&#35270;&#35273;&#20256;&#36755;&#33267;&#19979;&#28216;&#20219;&#21153;&#25104;&#20026;&#21487;&#33021;&#12290;&#36890;&#36807;&#22312;&#20114;&#32852;&#32593;&#19978;&#37319;&#38598;&#30340;&#25968;&#30334;&#19975;&#26679;&#26412;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#22914;CLIP&#20043;&#31867;&#30340;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#20135;&#29983;&#20102;&#26368;&#20808;&#36827;&#30340;&#38646;&#26679;&#26412;&#32467;&#26524;&#65292;&#36890;&#24120;&#22312;&#26080;&#38656;&#20219;&#21153;&#29305;&#23450;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;&#19982;&#23436;&#20840;&#30417;&#30563;&#26041;&#27861;&#31454;&#20105;&#21147;&#30456;&#24403;&#30340;&#27700;&#24179;&#12290;&#38500;&#20102;&#22312;&#20998;&#31867;&#20934;&#30830;&#24615;&#19978;&#34920;&#29616;&#40723;&#33310;&#20154;&#24515;&#20043;&#22806;&#65292;&#25253;&#36947;&#31216;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#22312;&#33258;&#28982;&#20998;&#24067;&#20559;&#31227;&#19979;&#19982;&#22312;ImageNet&#19978;&#35757;&#32451;&#30340;&#30417;&#30563;&#27169;&#22411;&#30340;&#34920;&#29616;&#30456;&#21305;&#37197;&#26469;&#32553;&#23567;&#40065;&#26834;&#24615;&#24046;&#36317;&#12290;&#30001;&#20110;&#40065;&#26834;&#24615;&#23545;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#23433;&#20840;&#20851;&#38190;&#30340;&#24212;&#29992;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#28085;&#30422;7&#31181;&#33258;&#28982;&#12289;3&#31181;&#21512;&#25104;&#20998;&#24067;&#20559;&#31227;&#21644;11&#31181;&#23545;&#25239;&#25915;&#20987;&#30340;&#22823;&#35268;&#27169;&#40065;&#26834;&#24615;&#22522;&#20934;&#27979;&#35797;&#30340;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#20197;CLIP&#20316;&#20026;&#35797;&#28857;&#30740;&#31350;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;CLIP&#23548;&#33268;&#20102;&#26174;&#33879;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10499v1 Announce Type: cross  Abstract: Pre-training image representations from the raw text about images enables zero-shot vision transfer to downstream tasks. Through pre-training on millions of samples collected from the internet, multimodal foundation models, such as CLIP, produce state-of-the-art zero-shot results that often reach competitiveness with fully supervised methods without the need for task-specific training. Besides the encouraging performance on classification accuracy, it is reported that these models close the robustness gap by matching the performance of supervised models trained on ImageNet under natural distribution shift. Because robustness is critical to real-world applications, especially safety-critical ones, in this paper, we present a comprehensive evaluation based on a large-scale robustness benchmark covering 7 natural, 3 synthetic distribution shifts, and 11 adversarial attacks. We use CLIP as a pilot study. We show that CLIP leads to a signif
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;RAG&#31995;&#32479;&#22686;&#24378;LLMs&#65292;&#25552;&#39640;&#20854;&#23545;&#31169;&#20154;&#30693;&#35782;&#24211;&#20013;&#29305;&#23450;&#39046;&#22495;&#21644;&#26102;&#25928;&#26597;&#35810;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#65292;&#23588;&#20854;&#22312;&#20351;&#29992;&#22806;&#37096;&#25968;&#25454;&#38598;&#26041;&#38754;&#34920;&#29616;&#20986;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.10446</link><description>&lt;p&gt;
&#21033;&#29992;RAG&#22686;&#24378;LLM&#20107;&#23454;&#20934;&#30830;&#24615;&#20197;&#28040;&#38500;&#24187;&#35273;&#65306;&#31169;&#20154;&#30693;&#35782;&#24211;&#20013;&#29305;&#23450;&#39046;&#22495;&#26597;&#35810;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Enhancing LLM Factual Accuracy with RAG to Counter Hallucinations: A Case Study on Domain-Specific Queries in Private Knowledge-Bases
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10446
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;RAG&#31995;&#32479;&#22686;&#24378;LLMs&#65292;&#25552;&#39640;&#20854;&#23545;&#31169;&#20154;&#30693;&#35782;&#24211;&#20013;&#29305;&#23450;&#39046;&#22495;&#21644;&#26102;&#25928;&#26597;&#35810;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#65292;&#23588;&#20854;&#22312;&#20351;&#29992;&#22806;&#37096;&#25968;&#25454;&#38598;&#26041;&#38754;&#34920;&#29616;&#20986;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#31995;&#32479;&#35774;&#35745;&#65292;&#21033;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#31169;&#20154;&#30693;&#35782;&#24211;&#20013;&#19982;&#29305;&#23450;&#39046;&#22495;&#21644;&#26102;&#25928;&#26597;&#35810;&#30456;&#20851;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#23558;RAG&#31649;&#36947;&#19982;&#19978;&#28216;&#25968;&#25454;&#38598;&#22788;&#29702;&#21644;&#19979;&#28216;&#24615;&#33021;&#35780;&#20272;&#25972;&#21512;&#22312;&#19968;&#36215;&#12290;&#35299;&#20915;LLM&#24187;&#35273;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#29992;&#28304;&#33258;CMU&#24191;&#27867;&#36164;&#28304;&#24182;&#29992;&#25945;&#24072;&#27169;&#22411;&#27880;&#37322;&#30340;&#31579;&#36873;&#25968;&#25454;&#38598;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#35813;&#31995;&#32479;&#22312;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#29305;&#23450;&#39046;&#22495;&#21644;&#26102;&#25928;&#26597;&#35810;&#31572;&#26696;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#36824;&#25581;&#31034;&#20102;&#20351;&#29992;&#23567;&#35268;&#27169;&#21644;&#20542;&#26012;&#25968;&#25454;&#38598;&#24494;&#35843;LLMs&#30340;&#23616;&#38480;&#24615;&#12290;&#36825;&#39033;&#30740;&#31350;&#31361;&#26174;&#20102;RAG&#31995;&#32479;&#22312;&#22686;&#24378;LLMs&#19982;&#22806;&#37096;&#25968;&#25454;&#38598;&#20197;&#25913;&#36827;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#24615;&#33021;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#27169;&#22411;&#21487;&#20379;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10446v1 Announce Type: new  Abstract: We proposed an end-to-end system design towards utilizing Retrieval Augmented Generation (RAG) to improve the factual accuracy of Large Language Models (LLMs) for domain-specific and time-sensitive queries related to private knowledge-bases. Our system integrates RAG pipeline with upstream datasets processing and downstream performance evaluation. Addressing the challenge of LLM hallucinations, we finetune models with a curated dataset which originates from CMU's extensive resources and annotated with the teacher model. Our experiments demonstrate the system's effectiveness in generating more accurate answers to domain-specific and time-sensitive inquiries. The results also revealed the limitations of fine-tuning LLMs with small-scale and skewed datasets. This research highlights the potential of RAG systems in augmenting LLMs with external datasets for improved performance in knowledge-intensive tasks. Our code and models are available 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#22909;&#30340;&#33609;&#31295;&#39564;&#35777;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#39564;&#35777;&#27493;&#39588;&#21046;&#23450;&#20026;&#22359;&#32423;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#39069;&#22806;&#30340;&#22681;&#38047;&#36895;&#24230;&#25552;&#21319;&#65292;&#32780;&#19981;&#22686;&#21152;&#39069;&#22806;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#33609;&#31295;&#26631;&#35760;</title><link>https://arxiv.org/abs/2403.10444</link><description>&lt;p&gt;
&#29992;&#20110;&#21152;&#36895;&#25512;&#27979;&#35299;&#30721;&#30340;&#26368;&#20339;&#22359;&#32423;&#33609;&#31295;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Optimal Block-Level Draft Verification for Accelerating Speculative Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10444
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#22909;&#30340;&#33609;&#31295;&#39564;&#35777;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#39564;&#35777;&#27493;&#39588;&#21046;&#23450;&#20026;&#22359;&#32423;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#39069;&#22806;&#30340;&#22681;&#38047;&#36895;&#24230;&#25552;&#21319;&#65292;&#32780;&#19981;&#22686;&#21152;&#39069;&#22806;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#33609;&#31295;&#26631;&#35760;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#27979;&#35299;&#30721;&#24050;&#34987;&#35777;&#26126;&#26159;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26080;&#25439;&#21152;&#36895;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290; &#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#65292;&#31639;&#27861;&#39318;&#20808;&#20351;&#29992;&#19968;&#20010;&#36739;&#23567;&#30340;&#27169;&#22411;&#36215;&#33609;&#19968;&#22359;&#26631;&#35760;&#12290;&#36825;&#20123;&#26631;&#35760;&#28982;&#21518;&#30001;&#22823;&#22411;&#27169;&#22411;&#24182;&#34892;&#39564;&#35777;&#65292;&#21482;&#26377;&#19968;&#37096;&#20998;&#26631;&#35760;&#23558;&#34987;&#20445;&#30041;&#65292;&#20197;&#30830;&#20445;&#26368;&#32456;&#36755;&#20986;&#36981;&#24490;&#22823;&#22411;&#27169;&#22411;&#30340;&#20998;&#24067;&#12290; &#22312;&#20197;&#24448;&#30340;&#25152;&#26377;&#25512;&#27979;&#35299;&#30721;&#24037;&#20316;&#20013;&#65292;&#36215;&#33609;&#39564;&#35777;&#26159;&#29420;&#31435;&#22320;&#36880;&#20010;&#26631;&#35760;&#25191;&#34892;&#30340;&#12290; &#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#22909;&#30340;&#36215;&#33609;&#39564;&#35777;&#31639;&#27861;&#65292;&#21487;&#25552;&#20379;&#39069;&#22806;&#30340;&#22681;&#38047;&#21152;&#36895;&#65292;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#36215;&#33609;&#26631;&#35760;&#12290; &#25105;&#20204;&#39318;&#20808;&#23558;&#36215;&#33609;&#39564;&#35777;&#27493;&#39588;&#21046;&#23450;&#20026;&#19968;&#20010;&#22359;&#32423;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#12290; &#22359;&#32423;&#21046;&#23450;&#20801;&#35768;&#25105;&#20204;&#32771;&#34385;&#26356;&#24191;&#27867;&#30340;&#36215;&#33609;&#39564;&#35777;&#31639;&#27861;&#65292;&#24182;&#22312;&#19968;&#20010;&#36215;&#33609;&#20013;&#39044;&#26399;&#33719;&#24471;&#26356;&#22810;&#25509;&#21463;&#30340;&#26631;&#35760;&#25968;&#37327;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10444v1 Announce Type: cross  Abstract: Speculative decoding has shown to be an effective method for lossless acceleration of large language models (LLMs) during inference. In each iteration, the algorithm first uses a smaller model to draft a block of tokens. The tokens are then verified by the large model in parallel and only a subset of tokens will be kept to guarantee that the final output follows the distribution of the large model. In all of the prior speculative decoding works, the draft verification is performed token-by-token independently. In this work, we propose a better draft verification algorithm that provides additional wall-clock speedup without incurring additional computation cost and draft tokens. We first formulate the draft verification step as a block-level optimal transport problem. The block-level formulation allows us to consider a wider range of draft verification algorithms and obtain a higher number of accepted tokens in expectation in one draft 
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#25214;&#21040;&#21644;&#32534;&#36753;&#25968;&#20540;&#23646;&#24615;&#34920;&#31034;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#21457;&#29616;&#32534;&#30721;&#25968;&#20540;&#23646;&#24615;&#30340;&#20302;&#32500;&#23376;&#31354;&#38388;&#20197;&#21333;&#35843;&#12289;&#21487;&#35299;&#37322;&#21644;&#21487;&#32534;&#36753;&#30340;&#26041;&#24335;&#23384;&#22312;&#65292;&#36890;&#36807;&#32534;&#36753;&#36825;&#20123;&#23376;&#31354;&#38388;&#20013;&#30340;&#26041;&#21521;&#65292;&#20351;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#30456;&#24212;&#22320;&#21457;&#29983;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.10381</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20013;&#25968;&#20540;&#23646;&#24615;&#30340;&#21333;&#35843;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Monotonic Representation of Numeric Properties in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10381
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#25214;&#21040;&#21644;&#32534;&#36753;&#25968;&#20540;&#23646;&#24615;&#34920;&#31034;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#21457;&#29616;&#32534;&#30721;&#25968;&#20540;&#23646;&#24615;&#30340;&#20302;&#32500;&#23376;&#31354;&#38388;&#20197;&#21333;&#35843;&#12289;&#21487;&#35299;&#37322;&#21644;&#21487;&#32534;&#36753;&#30340;&#26041;&#24335;&#23384;&#22312;&#65292;&#36890;&#36807;&#32534;&#36753;&#36825;&#20123;&#23376;&#31354;&#38388;&#20013;&#30340;&#26041;&#21521;&#65292;&#20351;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#30456;&#24212;&#22320;&#21457;&#29983;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#21487;&#20197;&#34920;&#36798;&#28041;&#21450;&#25968;&#23383;&#23646;&#24615;&#30340;&#20107;&#23454;&#30693;&#35782;&#65292;&#20363;&#22914;Karl Popper&#20986;&#29983;&#22312;1902&#24180;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20449;&#24687;&#22914;&#20309;&#34987;&#32534;&#30721;&#22312;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#20013;&#23578;&#19981;&#22826;&#20026;&#20154;&#25152;&#20102;&#35299;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#26041;&#27861;&#65292;&#29992;&#20110;&#26597;&#25214;&#21644;&#32534;&#36753;&#25968;&#20540;&#23646;&#24615;&#30340;&#34920;&#31034;&#65292;&#20363;&#22914;&#26576;&#20010;&#23454;&#20307;&#30340;&#20986;&#29983;&#24180;&#20221;&#12290;&#22312;&#32463;&#39564;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#32534;&#30721;&#25968;&#20540;&#23646;&#24615;&#30340;&#20302;&#32500;&#23376;&#31354;&#38388;&#21333;&#35843;&#22320;&#12289;&#21487;&#35299;&#37322;&#22320;&#21644;&#21487;&#32534;&#36753;&#22320;&#12290;&#24403;&#27839;&#30528;&#36825;&#20123;&#23376;&#31354;&#38388;&#20013;&#30340;&#26041;&#21521;&#32534;&#36753;&#34920;&#31034;&#26102;&#65292;LM&#30340;&#36755;&#20986;&#30456;&#24212;&#22320;&#21457;&#29983;&#21464;&#21270;&#12290;&#20363;&#22914;&#65292;&#36890;&#36807;&#27839;&#30528;&#8220;birthyear&#8221;&#26041;&#21521;&#20462;&#34917;&#28608;&#27963;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;LM&#34920;&#36798;&#36234;&#26469;&#36234;&#26202;&#30340;&#20986;&#29983;&#24180;&#20221;&#65306;&#21345;&#23572;&#183;&#27874;&#26222;&#23572;&#29983;&#20110;1929&#24180;&#65292;&#21345;&#23572;&#183;&#27874;&#26222;&#23572;&#29983;&#20110;1957&#24180;&#65292;&#21345;&#23572;&#183;&#27874;&#26222;&#23572;&#29983;&#20110;1968&#24180;&#12290;&#22312;&#32771;&#34385;&#30340;&#25152;&#26377;&#27169;&#22411;&#20013;&#65292;&#23384;&#22312;&#36328;&#22810;&#20010;&#25968;&#23383;&#23646;&#24615;&#30340;&#23646;&#24615;&#32534;&#30721;&#26041;&#21521;&#65292;&#36825;&#34920;&#26126;&#20102;&#21333;&#35843;&#24615;&#34920;&#31034;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10381v1 Announce Type: new  Abstract: Language models (LMs) can express factual knowledge involving numeric properties such as Karl Popper was born in 1902. However, how this information is encoded in the model's internal representations is not understood well. Here, we introduce a simple method for finding and editing representations of numeric properties such as an entity's birth year. Empirically, we find low-dimensional subspaces that encode numeric properties monotonically, in an interpretable and editable fashion. When editing representations along directions in these subspaces, LM output changes accordingly. For example, by patching activations along a "birthyear" direction we can make the LM express an increasingly late birthyear: Karl Popper was born in 1929, Karl Popper was born in 1957, Karl Popper was born in 1968. Property-encoding directions exist across several numeric properties in all models under consideration, suggesting the possibility that monotonic repr
&lt;/p&gt;</description></item><item><title>EXAMS-V&#26159;&#19968;&#20010;&#29420;&#29305;&#30340;&#12289;&#36328;&#23398;&#31185;&#22810;&#35821;&#35328;&#22810;&#27169;&#24577;&#32771;&#35797;&#22522;&#20934;&#65292;&#37319;&#29992;&#22810;&#31181;&#35821;&#35328;&#21644;&#22810;&#31181;&#25945;&#32946;&#31995;&#32479;&#30340;&#32771;&#35797;&#39064;&#30446;&#65292;&#38656;&#35201;&#36827;&#34892;&#22797;&#26434;&#30340;&#25512;&#29702;&#24182;&#20381;&#36182;&#20110;&#21306;&#22495;&#29305;&#23450;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2403.10378</link><description>&lt;p&gt;
EXAMS-V: &#29992;&#20110;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#36328;&#23398;&#31185;&#22810;&#35821;&#35328;&#22810;&#27169;&#24577;&#32771;&#35797;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
EXAMS-V: A Multi-Discipline Multilingual Multimodal Exam Benchmark for Evaluating Vision Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10378
&lt;/p&gt;
&lt;p&gt;
EXAMS-V&#26159;&#19968;&#20010;&#29420;&#29305;&#30340;&#12289;&#36328;&#23398;&#31185;&#22810;&#35821;&#35328;&#22810;&#27169;&#24577;&#32771;&#35797;&#22522;&#20934;&#65292;&#37319;&#29992;&#22810;&#31181;&#35821;&#35328;&#21644;&#22810;&#31181;&#25945;&#32946;&#31995;&#32479;&#30340;&#32771;&#35797;&#39064;&#30446;&#65292;&#38656;&#35201;&#36827;&#34892;&#22797;&#26434;&#30340;&#25512;&#29702;&#24182;&#20381;&#36182;&#20110;&#21306;&#22495;&#29305;&#23450;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;EXAMS-V&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36328;&#23398;&#31185;&#22810;&#27169;&#24577;&#22810;&#35821;&#35328;&#32771;&#35797;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#12290;&#23427;&#21253;&#25324;20932&#36947;&#28085;&#30422;&#33258;&#28982;&#31185;&#23398;&#12289;&#31038;&#20250;&#31185;&#23398;&#20197;&#21450;&#20854;&#20182;&#21508;&#31181;&#30740;&#31350;&#39046;&#22495;&#65288;&#22914;&#23447;&#25945;&#12289;&#32654;&#26415;&#12289;&#21830;&#19994;&#31561;&#65289;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#12290;EXAMS-V&#21253;&#21547;&#21508;&#31181;&#22810;&#27169;&#24577;&#29305;&#24449;&#65292;&#22914;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#34920;&#26684;&#12289;&#22270;&#34920;&#12289;&#22270;&#31034;&#12289;&#22320;&#22270;&#12289;&#31185;&#23398;&#31526;&#21495;&#21644;&#26041;&#31243;&#24335;&#12290;&#38382;&#39064;&#28085;&#30422;&#20102;&#26469;&#33258;7&#20010;&#35821;&#31995;&#30340;11&#31181;&#35821;&#35328;&#12290;&#19982;&#29616;&#26377;&#22522;&#20934;&#19981;&#21516;&#65292;EXAMS-V&#36890;&#36807;&#25910;&#38598;&#26469;&#33258;&#21508;&#20010;&#22269;&#23478;&#12289;&#20855;&#26377;&#22810;&#31181;&#25945;&#32946;&#31995;&#32479;&#30340;&#23398;&#26657;&#32771;&#35797;&#39064;&#30446;&#26469;&#29420;&#29305;&#31574;&#21010;&#12290;&#36825;&#31181;&#29420;&#29305;&#30340;&#26041;&#27861;&#38656;&#35201;&#22312;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#36827;&#34892;&#22797;&#26434;&#25512;&#29702;&#65292;&#24182;&#20381;&#36182;&#20110;&#29305;&#23450;&#22320;&#21306;&#30340;&#30693;&#35782;&#12290;&#35299;&#20915;&#25968;&#25454;&#38598;&#20013;&#30340;&#38382;&#39064;&#38656;&#35201;&#23545;&#22270;&#20687;&#30340;&#25991;&#26412;&#21644;&#35270;&#35273;&#20869;&#23481;&#36827;&#34892;&#39640;&#32423;&#24863;&#30693;&#21644;&#32852;&#21512;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10378v1 Announce Type: new  Abstract: We introduce EXAMS-V, a new challenging multi-discipline multimodal multilingual exam benchmark for evaluating vision language models. It consists of 20,932 multiple-choice questions across 20 school disciplines covering natural science, social science, and other miscellaneous studies, e.g., religion, fine arts, business, etc. EXAMS-V includes a variety of multimodal features such as text, images, tables, figures, diagrams, maps, scientific symbols, and equations. The questions come in 11 languages from 7 language families. Unlike existing benchmarks, EXAMS-V is uniquely curated by gathering school exam questions from various countries, with a variety of education systems. This distinctive approach calls for intricate reasoning across diverse languages and relies on region-specific knowledge. Solving the problems in the dataset requires advanced perception and joint reasoning over the text and the visual content of the image. Our evaluat
&lt;/p&gt;</description></item><item><title>TriSum&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#24635;&#32467;&#33021;&#21147;&#25552;&#28860;&#21040;&#19968;&#20010;&#32039;&#20945;&#30340;&#26412;&#22320;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.10351</link><description>&lt;p&gt;
TriSum: &#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23398;&#20064;&#24635;&#32467;&#33021;&#21147;&#19982;&#32467;&#26500;&#21270;&#29702;&#30001;
&lt;/p&gt;
&lt;p&gt;
TriSum: Learning Summarization Ability from Large Language Models with Structured Rationale
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10351
&lt;/p&gt;
&lt;p&gt;
TriSum&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#24635;&#32467;&#33021;&#21147;&#25552;&#28860;&#21040;&#19968;&#20010;&#32039;&#20945;&#30340;&#26412;&#22320;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#26174;&#33879;&#25512;&#21160;&#20102;&#25991;&#26412;&#24635;&#32467;&#31561;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24222;&#22823;&#22823;&#23567;&#21644;&#35745;&#31639;&#38656;&#27714;&#65292;&#21152;&#19978;&#25968;&#25454;&#20256;&#36755;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#36164;&#28304;&#21463;&#38480;&#21644;&#38544;&#31169;&#20026;&#20013;&#24515;&#30340;&#29615;&#22659;&#20013;&#30340;&#20351;&#29992;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;TriSum&#65292;&#19968;&#20010;&#23558;LLMs&#30340;&#25991;&#26412;&#24635;&#32467;&#33021;&#21147;&#25552;&#28860;&#21040;&#19968;&#20010;&#32039;&#20945;&#30340;&#26412;&#22320;&#27169;&#22411;&#20013;&#30340;&#26694;&#26550;&#12290;&#26368;&#21021;&#65292;LLMs&#25552;&#21462;&#19968;&#32452;&#26041;&#38754;&#19977;&#20803;&#29702;&#30001;&#21644;&#24635;&#32467;&#65292;&#28982;&#21518;&#20351;&#29992;&#21452;&#35780;&#20998;&#26041;&#27861;&#23545;&#20854;&#36827;&#34892;&#20248;&#21270;&#12290;&#25509;&#19979;&#26469;&#65292;&#20351;&#29992;&#36825;&#20123;&#20219;&#21153;&#35757;&#32451;&#19968;&#20010;&#35268;&#27169;&#36739;&#23567;&#30340;&#26412;&#22320;&#27169;&#22411;&#65292;&#37319;&#29992;&#20174;&#31616;&#21333;&#21040;&#22797;&#26434;&#20219;&#21153;&#30340;&#35838;&#31243;&#23398;&#20064;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#21319;&#20102;&#26412;&#22320;&#27169;&#22411;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#24615;&#33021;&#65288;CNN/DailyMail&#65292;XSum&#21644;ClinicalTrial&#65289;&#65292;&#20998;&#21035;&#27604;&#22522;&#32447;&#25552;&#39640;&#20102;4.5&#65285;&#65292;8.5&#65285;&#21644;7.4&#65285;&#12290;&#23427;&#36824;&#36890;&#36807;&#25552;&#20379;&#23545;&#24635;&#32467;&#30340;&#35265;&#35299;&#26469;&#25913;&#21892;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10351v1 Announce Type: new  Abstract: The advent of large language models (LLMs) has significantly advanced natural language processing tasks like text summarization. However, their large size and computational demands, coupled with privacy concerns in data transmission, limit their use in resource-constrained and privacy-centric settings. To overcome this, we introduce TriSum, a framework for distilling LLMs' text summarization abilities into a compact, local model. Initially, LLMs extract a set of aspect-triple rationales and summaries, which are refined using a dual-scoring method for quality. Next, a smaller local model is trained with these tasks, employing a curriculum learning strategy that evolves from simple to complex tasks. Our method enhances local model performance on various benchmarks (CNN/DailyMail, XSum, and ClinicalTrial), outperforming baselines by 4.5%, 8.5%, and 7.4%, respectively. It also improves interpretability by providing insights into the summariz
&lt;/p&gt;</description></item><item><title>&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#24456;&#23569;&#30340;&#20363;&#23376;&#23398;&#20064;&#26032;&#21517;&#35789;&#30340;&#24615;&#21035;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#19968;&#33268;&#19978;&#19979;&#25991;&#20013;&#24212;&#29992;&#25152;&#23398;&#24615;&#21035;&#65292;&#34429;&#28982;&#26377;&#23545;&#38451;&#24615;&#24615;&#21035;&#31867;&#21035;&#30340;&#20559;&#35265;</title><link>https://arxiv.org/abs/2403.10338</link><description>&lt;p&gt;
&#20351;&#29992;&#23569;&#26679;&#26412;&#23398;&#20064;&#25506;&#31350;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35821;&#27861;&#25277;&#35937;&#24615;&#65306;&#26032;&#21517;&#35789;&#24615;&#21035;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Investigating grammatical abstraction in language models using few-shot learning of novel noun gender
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10338
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#24456;&#23569;&#30340;&#20363;&#23376;&#23398;&#20064;&#26032;&#21517;&#35789;&#30340;&#24615;&#21035;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#19968;&#33268;&#19978;&#19979;&#25991;&#20013;&#24212;&#29992;&#25152;&#23398;&#24615;&#21035;&#65292;&#34429;&#28982;&#26377;&#23545;&#38451;&#24615;&#24615;&#21035;&#31867;&#21035;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21487;&#20197;&#20174;&#24456;&#23569;&#30340;&#20363;&#23376;&#20013;&#23398;&#20064;&#19968;&#20010;&#26032;&#21333;&#35789;&#65292;&#24182;&#25512;&#26029;&#20854;&#35821;&#27861;&#23646;&#24615;&#12290;&#20182;&#20204;&#23545;&#20687;&#35821;&#27861;&#24615;&#21035;&#21644;&#19968;&#33268;&#35268;&#21017;&#36825;&#26679;&#30340;&#35821;&#35328;&#23646;&#24615;&#20855;&#26377;&#25277;&#35937;&#27010;&#24565;&#65292;&#21487;&#20197;&#24212;&#29992;&#21040;&#26032;&#30340;&#21477;&#27861;&#19978;&#19979;&#25991;&#21644;&#35789;&#35821;&#20013;&#12290;&#25105;&#20204;&#20174;&#24515;&#29702;&#35821;&#35328;&#23398;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#36827;&#34892;&#20102;&#19968;&#20010;&#21517;&#35789;&#23398;&#20064;&#23454;&#39564;&#65292;&#35780;&#20272;LSTM&#21644;&#20165;&#35299;&#30721;&#22120;&#21464;&#21387;&#22120;&#26159;&#21542;&#21487;&#20197;&#23454;&#29616;&#23545;&#27861;&#35821;&#20013;&#30340;&#35821;&#27861;&#24615;&#21035;&#30340;&#31867;&#20284;&#20154;&#31867;&#30340;&#25277;&#35937;&#12290;&#35821;&#35328;&#27169;&#22411;&#30340;&#20219;&#21153;&#26159;&#20174;&#20960;&#20010;&#20363;&#23376;&#20013;&#23398;&#20064;&#19968;&#20010;&#26032;&#21517;&#35789;&#23884;&#20837;&#30340;&#24615;&#21035;&#65292;&#24182;&#22312;&#21478;&#19968;&#20010;&#26410;&#35265;&#19978;&#19979;&#25991;&#20013;&#39044;&#27979;&#19968;&#33268;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20004;&#31181;&#35821;&#35328;&#27169;&#22411;&#22312;&#19968;&#20010;&#35821;&#27861;&#19968;&#33268;&#19978;&#19979;&#25991;&#20013;&#33021;&#26377;&#25928;&#22320;&#23558;&#26032;&#21517;&#35789;&#30340;&#24615;&#21035;&#27010;&#25324;&#21040;&#19968;&#20010;&#21040;&#20004;&#20010;&#23398;&#20064;&#31034;&#20363;&#65292;&#24182;&#25226;&#25152;&#23398;&#30340;&#24615;&#21035;&#24212;&#29992;&#21040;&#19968;&#33268;&#19978;&#19979;&#25991;&#65292;&#23613;&#31649;&#26377;&#23545;&#38451;&#24615;&#24615;&#21035;&#31867;&#21035;&#30340;&#20559;&#35265;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#23569;&#26679;&#26412;&#26356;&#26032;&#20165;&#24212;&#29992;&#20110;&#23884;&#20837;&#23618;&#65292;&#35777;&#26126;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10338v1 Announce Type: new  Abstract: Humans can learn a new word and infer its grammatical properties from very few examples. They have an abstract notion of linguistic properties like grammatical gender and agreement rules that can be applied to novel syntactic contexts and words. Drawing inspiration from psycholinguistics, we conduct a noun learning experiment to assess whether an LSTM and a decoder-only transformer can achieve human-like abstraction of grammatical gender in French. Language models were tasked with learning the gender of a novel noun embedding from a few examples in one grammatical agreement context and predicting agreement in another, unseen context. We find that both language models effectively generalise novel noun gender from one to two learning examples and apply the learnt gender across agreement contexts, albeit with a bias for the masculine gender category. Importantly, the few-shot updates were only applied to the embedding layers, demonstrating 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#24212;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20505;&#36873;&#24178;&#25200;&#39033;&#29983;&#25104;&#30340;&#26367;&#20195;&#26041;&#27861;&#26469;&#33258;&#21160;&#29983;&#25104;&#22635;&#31354;&#24178;&#25200;&#39033;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#31181;PLM&#22686;&#24378;&#27169;&#22411;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.10326</link><description>&lt;p&gt;
&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#22635;&#31354;&#24178;&#25200;&#39033;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
CDGP: Automatic Cloze Distractor Generation based on Pre-trained Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#24212;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20505;&#36873;&#24178;&#25200;&#39033;&#29983;&#25104;&#30340;&#26367;&#20195;&#26041;&#27861;&#26469;&#33258;&#21160;&#29983;&#25104;&#22635;&#31354;&#24178;&#25200;&#39033;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#31181;PLM&#22686;&#24378;&#27169;&#22411;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#21160;&#35774;&#35745;&#22635;&#31354;&#27979;&#35797;&#32791;&#36153;&#22823;&#37327;&#26102;&#38388;&#21644;&#31934;&#21147;&#12290;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#38169;&#35823;&#36873;&#39033;&#65288;&#24178;&#25200;&#39033;&#65289;&#30340;&#36873;&#25321;&#12290;&#31934;&#24515;&#35774;&#35745;&#30340;&#24178;&#25200;&#39033;&#25552;&#39640;&#20102;&#23398;&#20064;&#32773;&#33021;&#21147;&#35780;&#20272;&#30340;&#26377;&#25928;&#24615;&#12290;&#22240;&#27492;&#65292;&#33258;&#21160;&#29983;&#25104;&#22635;&#31354;&#24178;&#25200;&#39033;&#30340;&#24819;&#27861;&#24212;&#36816;&#32780;&#29983;&#12290;&#26412;&#25991;&#36890;&#36807;&#25506;&#32034;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#24212;&#29992;&#20316;&#20026;&#20505;&#36873;&#24178;&#25200;&#39033;&#29983;&#25104;&#30340;&#26367;&#20195;&#26041;&#27861;&#26469;&#30740;&#31350;&#22635;&#31354;&#24178;&#25200;&#39033;&#29983;&#25104;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;PLM&#22686;&#24378;&#27169;&#22411;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#25105;&#20204;&#30340;&#26368;&#20339;&#27169;&#22411;&#23558;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#20174;14.94&#25552;&#21319;&#33267;34.17&#65288;NDCG@10&#20998;&#25968;&#65289;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#21487;&#22312;https://github.com/AndyChiangSH/CDGP &#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10326v1 Announce Type: cross  Abstract: Manually designing cloze test consumes enormous time and efforts. The major challenge lies in wrong option (distractor) selection. Having carefully-design distractors improves the effectiveness of learner ability assessment. As a result, the idea of automatically generating cloze distractor is motivated. In this paper, we investigate cloze distractor generation by exploring the employment of pre-trained language models (PLMs) as an alternative for candidate distractor generation. Experiments show that the PLM-enhanced model brings a substantial performance improvement. Our best performing model advances the state-of-the-art result from 14.94 to 34.17 (NDCG@10 score). Our code and dataset is available at https://github.com/AndyChiangSH/CDGP.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24635;&#32467;&#25991;&#26412;&#26041;&#38754;&#30340;&#33021;&#21147;&#20026;&#25913;&#36827;&#31185;&#23398;&#25991;&#29486;&#20998;&#26512;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;&#65292;Uni-SMART&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;LLMs&#23545;&#22810;&#27169;&#24577;&#31185;&#23398;&#25991;&#29486;&#20869;&#23481;&#29702;&#35299;&#21644;&#20998;&#26512;&#33021;&#21147;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.10301</link><description>&lt;p&gt;
Uni-SMART&#65306;&#36890;&#29992;&#31185;&#23398;&#22810;&#27169;&#24577;&#20998;&#26512;&#21644;&#30740;&#31350;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Uni-SMART: Universal Science Multimodal Analysis and Research Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10301
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24635;&#32467;&#25991;&#26412;&#26041;&#38754;&#30340;&#33021;&#21147;&#20026;&#25913;&#36827;&#31185;&#23398;&#25991;&#29486;&#20998;&#26512;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;&#65292;Uni-SMART&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;LLMs&#23545;&#22810;&#27169;&#24577;&#31185;&#23398;&#25991;&#29486;&#20869;&#23481;&#29702;&#35299;&#21644;&#20998;&#26512;&#33021;&#21147;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31185;&#23398;&#30740;&#31350;&#21450;&#20854;&#24212;&#29992;&#20013;&#65292;&#31185;&#23398;&#25991;&#29486;&#20998;&#26512;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#20511;&#37492;&#20182;&#20154;&#30340;&#24037;&#20316;&#12290;&#28982;&#32780;&#65292;&#31185;&#23398;&#30693;&#35782;&#30340;&#24555;&#36895;&#22686;&#38271;&#23548;&#33268;&#23398;&#26415;&#25991;&#31456;&#25968;&#37327;&#22823;&#24133;&#22686;&#21152;&#65292;&#20351;&#28145;&#20837;&#25991;&#29486;&#20998;&#26512;&#21464;&#24471;&#36234;&#26469;&#36234;&#20855;&#25361;&#25112;&#24615;&#21644;&#32791;&#26102;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;&#12290;&#20197;&#20854;&#22312;&#24635;&#32467;&#25991;&#26412;&#26041;&#38754;&#30340;&#24378;&#22823;&#33021;&#21147;&#32780;&#38395;&#21517;&#65292;LLMs&#34987;&#35270;&#20026;&#25913;&#36827;&#31185;&#23398;&#25991;&#29486;&#20998;&#26512;&#30340;&#28508;&#22312;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;LLMs&#23384;&#22312;&#20854;&#23616;&#38480;&#24615;&#12290;&#31185;&#23398;&#25991;&#29486;&#36890;&#24120;&#21253;&#25324;&#21508;&#31181;&#22810;&#27169;&#24577;&#20803;&#32032;&#65292;&#22914;&#20998;&#23376;&#32467;&#26500;&#12289;&#34920;&#26684;&#21644;&#22270;&#34920;&#65292;&#36825;&#20123;&#23545;&#20197;&#25991;&#26412;&#20026;&#20013;&#24515;&#30340;LLMs&#32780;&#35328;&#38590;&#20197;&#29702;&#35299;&#21644;&#20998;&#26512;&#12290;&#36825;&#20010;&#38382;&#39064;&#25351;&#21521;&#20102;&#36843;&#20999;&#38656;&#35201;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#33021;&#22815;&#20805;&#20998;&#29702;&#35299;&#21644;&#20998;&#26512;&#31185;&#23398;&#25991;&#29486;&#20013;&#30340;&#22810;&#27169;&#24577;&#20869;&#23481;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#38656;&#27714;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10301v1 Announce Type: new  Abstract: In scientific research and its application, scientific literature analysis is crucial as it allows researchers to build on the work of others. However, the fast growth of scientific knowledge has led to a massive increase in scholarly articles, making in-depth literature analysis increasingly challenging and time-consuming. The emergence of Large Language Models (LLMs) has offered a new way to address this challenge. Known for their strong abilities in summarizing texts, LLMs are seen as a potential tool to improve the analysis of scientific literature. However, existing LLMs have their own limits. Scientific literature often includes a wide range of multimodal elements, such as molecular structure, tables, and charts, which are hard for text-focused LLMs to understand and analyze. This issue points to the urgent need for new solutions that can fully understand and analyze multimodal content in scientific literature. To answer this deman
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22810;&#26041;&#35328;&#24052;&#20240;&#21033;&#20122;&#36890;&#29992;&#20381;&#23384;&#26641;&#24211;&#65288;MaiBaam&#65289;&#65292;&#25163;&#21160;&#26631;&#27880;&#20102;&#35789;&#31867;&#21644;&#21477;&#27861;&#20381;&#23384;&#20449;&#24687;&#65292;&#35206;&#30422;&#22810;&#31181;&#25991;&#26412;&#31867;&#22411;&#65292;&#31361;&#20986;&#20102;&#24052;&#20240;&#21033;&#20122;&#35805;&#19982;&#24503;&#35821;&#20043;&#38388;&#30340;&#24418;&#24577;&#21477;&#27861;&#24046;&#24322;&#65292;&#24182;&#23637;&#31034;&#20102;&#35828;&#35805;&#32773;&#27491;&#23383;&#27861;&#30340;&#20016;&#23500;&#21464;&#24322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.10293</link><description>&lt;p&gt;
MaiBaam&#65306;&#22810;&#26041;&#35328;&#24052;&#20240;&#21033;&#20122;&#36890;&#29992;&#20381;&#23384;&#26641;&#24211;
&lt;/p&gt;
&lt;p&gt;
MaiBaam: A Multi-Dialectal Bavarian Universal Dependency Treebank
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10293
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22810;&#26041;&#35328;&#24052;&#20240;&#21033;&#20122;&#36890;&#29992;&#20381;&#23384;&#26641;&#24211;&#65288;MaiBaam&#65289;&#65292;&#25163;&#21160;&#26631;&#27880;&#20102;&#35789;&#31867;&#21644;&#21477;&#27861;&#20381;&#23384;&#20449;&#24687;&#65292;&#35206;&#30422;&#22810;&#31181;&#25991;&#26412;&#31867;&#22411;&#65292;&#31361;&#20986;&#20102;&#24052;&#20240;&#21033;&#20122;&#35805;&#19982;&#24503;&#35821;&#20043;&#38388;&#30340;&#24418;&#24577;&#21477;&#27861;&#24046;&#24322;&#65292;&#24182;&#23637;&#31034;&#20102;&#35828;&#35805;&#32773;&#27491;&#23383;&#27861;&#30340;&#20016;&#23500;&#21464;&#24322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;Universal Dependencies (UD) &#39033;&#30446;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20854;&#35821;&#35328;&#24191;&#24230;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#65292;&#20294;&#22312;&#8220;&#35821;&#35328;&#20869;&#24191;&#24230;&#8221;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#32570;&#20047;&#65306;&#22823;&#22810;&#25968;&#26641;&#24211;&#19987;&#27880;&#20110;&#26631;&#20934;&#35821;&#35328;&#12290;&#21363;&#20351;&#23545;&#20110;&#24503;&#35821;&#36825;&#26679;&#22312;UD&#20013;&#26631;&#27880;&#37327;&#26368;&#22823;&#30340;&#35821;&#35328;&#65292;&#36804;&#20170;&#20026;&#27490;&#36824;&#27809;&#26377;&#38024;&#23545;&#20854;&#35821;&#35328;&#21464;&#20307;&#20043;&#19968;&#30340;&#26641;&#24211;&#23384;&#22312;&#65292;&#36825;&#31181;&#35821;&#35328;&#21464;&#20307;&#26159;&#24052;&#20240;&#21033;&#20122;&#35805;&#65292;&#20351;&#29992;&#20154;&#21475;&#36229;&#36807;1000&#19975;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#32570;&#21475;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22810;&#26041;&#35328;&#24052;&#20240;&#21033;&#20122;&#26641;&#24211;&#65288;MaiBaam&#65289;&#65292;&#22312;UD&#20013;&#25163;&#21160;&#26631;&#27880;&#20102;&#35789;&#31867;&#21644;&#21477;&#27861;&#20381;&#23384;&#20449;&#24687;&#65292;&#35206;&#30422;&#22810;&#31181;&#25991;&#26412;&#31867;&#22411;&#65288;&#32500;&#22522;&#12289;&#23567;&#35828;&#12289;&#35821;&#27861;&#31034;&#20363;&#12289;&#31038;&#20132;&#12289;&#38750;&#23567;&#35828;&#65289;&#12290;&#25105;&#20204;&#31361;&#20986;&#20102;&#24052;&#20240;&#21033;&#20122;&#35805;&#19982;&#24503;&#35821;&#20043;&#38388;&#30340;&#24418;&#24577;&#21477;&#27861;&#24046;&#24322;&#65292;&#24182;&#23637;&#31034;&#20102;&#35828;&#35805;&#32773;&#27491;&#23383;&#27861;&#30340;&#20016;&#23500;&#21464;&#24322;&#24615;&#12290;&#25105;&#20204;&#30340;&#35821;&#26009;&#24211;&#21253;&#25324;15k&#20010;&#26631;&#35760;&#65292;&#35206;&#30422;&#20102;&#26469;&#33258;&#28085;&#30422;&#19977;&#20010;&#22269;&#23478;&#30340;&#25152;&#26377;&#35828;&#24052;&#20240;&#21033;&#20122;&#35805;&#30340;&#22320;&#21306;&#30340;&#26041;&#35328;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#22522;&#20934;&#35299;&#26512;&#21644;&#35789;&#31867;&#26631;&#35760;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10293v1 Announce Type: new  Abstract: Despite the success of the Universal Dependencies (UD) project exemplified by its impressive language breadth, there is still a lack in `within-language breadth': most treebanks focus on standard languages. Even for German, the language with the most annotations in UD, so far no treebank exists for one of its language varieties spoken by over 10M people: Bavarian. To contribute to closing this gap, we present the first multi-dialect Bavarian treebank (MaiBaam) manually annotated with part-of-speech and syntactic dependency information in UD, covering multiple text genres (wiki, fiction, grammar examples, social, non-fiction). We highlight the morphosyntactic differences between the closely-related Bavarian and German and showcase the rich variability of speakers' orthographies. Our corpus includes 15k tokens, covering dialects from all Bavarian-speaking areas spanning three countries. We provide baseline parsing and POS tagging results, 
&lt;/p&gt;</description></item><item><title>Team Trifecta&#22312;Factify 5WQA&#19978;&#20197;Fine-Tuning&#21462;&#24471;&#20102;&#39318;&#35201;&#22320;&#20301;&#65292;&#25104;&#21151;&#36229;&#36234;&#22522;&#20934;&#20934;&#30830;&#29575;103&#65285;&#65292;&#24182;&#20445;&#25345;&#20102;&#23545;&#31532;&#20108;&#21517;&#31454;&#20105;&#32773;&#30340;70%&#39046;&#20808;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.10281</link><description>&lt;p&gt;
Team Trifecta&#22312;Factify 5WQA&#19978;&#35774;&#23450;&#20102;&#32454;&#21270;&#35843;&#25972;&#20013;&#20107;&#23454;&#39564;&#35777;&#30340;&#26631;&#20934;
&lt;/p&gt;
&lt;p&gt;
Team Trifecta at Factify5WQA: Setting the Standard in Fact Verification with Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10281
&lt;/p&gt;
&lt;p&gt;
Team Trifecta&#22312;Factify 5WQA&#19978;&#20197;Fine-Tuning&#21462;&#24471;&#20102;&#39318;&#35201;&#22320;&#20301;&#65292;&#25104;&#21151;&#36229;&#36234;&#22522;&#20934;&#20934;&#30830;&#29575;103&#65285;&#65292;&#24182;&#20445;&#25345;&#20102;&#23545;&#31532;&#20108;&#21517;&#31454;&#20105;&#32773;&#30340;70%&#39046;&#20808;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Pre-CoFactv3&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#38382;&#31572;&#21644;&#25991;&#26412;&#20998;&#31867;&#32452;&#20214;&#32452;&#25104;&#30340;&#20840;&#38754;&#26694;&#26550;&#65292;&#29992;&#20110;&#20107;&#23454;&#39564;&#35777;&#12290;&#36890;&#36807;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;FakeNet&#27169;&#22411;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#20107;&#23454;&#39564;&#35777;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25506;&#35752;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;LLMs&#65292;&#24341;&#20837;&#20102;FakeNet&#65292;&#24182;&#23454;&#26045;&#20102;&#21508;&#31181;&#38598;&#25104;&#26041;&#27861;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#22242;&#38431;Trifecta&#22312;AAAI-24 Factify 3.0&#30740;&#35752;&#20250;&#19978;&#33719;&#24471;&#20102;&#31532;&#19968;&#21517;&#65292;&#27604;&#22522;&#20934;&#20934;&#30830;&#29575;&#39640;&#20986;103%&#65292;&#24182;&#20445;&#25345;&#20102;&#23545;&#31532;&#20108;&#21517;&#31454;&#20105;&#23545;&#25163;&#30340;70%&#39046;&#20808;&#20248;&#21183;&#12290;&#36825;&#19968;&#25104;&#21151;&#31361;&#26174;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21450;&#20854;&#23545;&#25512;&#36827;&#20107;&#23454;&#39564;&#35777;&#30740;&#31350;&#30340;&#28508;&#22312;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10281v1 Announce Type: cross  Abstract: In this paper, we present Pre-CoFactv3, a comprehensive framework comprised of Question Answering and Text Classification components for fact verification. Leveraging In-Context Learning, Fine-tuned Large Language Models (LLMs), and the FakeNet model, we address the challenges of fact verification. Our experiments explore diverse approaches, comparing different Pre-trained LLMs, introducing FakeNet, and implementing various ensemble methods. Notably, our team, Trifecta, secured first place in the AAAI-24 Factify 3.0 Workshop, surpassing the baseline accuracy by 103% and maintaining a 70% lead over the second competitor. This success underscores the efficacy of our approach and its potential contributions to advancing fact verification research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26041;&#27861;&#26469;&#25361;&#25112;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#31616;&#21333;&#32780;&#20016;&#23500;&#35299;&#37322;&#30340;&#21487;&#33021;&#24615;&#65292;&#30740;&#31350;&#21457;&#29616;&#20351;&#29992;&#22522;&#20110;&#29305;&#24449;&#30340;&#27169;&#22411;&#22312;&#20449;&#21495;&#20256;&#36882;&#26041;&#38754;&#25928;&#26524;&#26356;&#22909;&#12290;</title><link>https://arxiv.org/abs/2403.10275</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#21644;&#22522;&#20110;&#35789;&#32423;&#21333;&#21464;&#37327;&#19968;&#38454;&#27010;&#29575;&#20551;&#35774;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Question on the Explainability of Large Language Models and the Word-Level Univariate First-Order Plausibility Assumption
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26041;&#27861;&#26469;&#25361;&#25112;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#31616;&#21333;&#32780;&#20016;&#23500;&#35299;&#37322;&#30340;&#21487;&#33021;&#24615;&#65292;&#30740;&#31350;&#21457;&#29616;&#20351;&#29992;&#22522;&#20110;&#29305;&#24449;&#30340;&#27169;&#22411;&#22312;&#20449;&#21495;&#20256;&#36882;&#26041;&#38754;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#37322;&#23545;&#20854;&#35757;&#32451;&#20013;&#20351;&#29992;&#30340;&#38543;&#26426;&#24615;&#24456;&#25935;&#24863;&#65292;&#22240;&#27492;&#38656;&#35201;&#23545;&#36825;&#31181;&#25935;&#24863;&#24615;&#36827;&#34892;&#34920;&#24449;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25361;&#25112;&#20026;&#36825;&#20123;&#27169;&#22411;&#25552;&#20379;&#31616;&#21333;&#21644;&#20449;&#24687;&#20016;&#23500;&#35299;&#37322;&#30340;&#34920;&#24449;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20026;&#35299;&#37322;&#30340;&#20449;&#21495;&#12289;&#22122;&#22768;&#21644;&#20449;&#22122;&#27604;&#32473;&#20986;&#20102;&#32479;&#35745;&#23450;&#20041;&#12290;&#25105;&#20204;&#24378;&#35843;&#65292;&#22312;&#19968;&#20010;&#20856;&#22411;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#20351;&#29992;&#19968;&#38454;&#32479;&#35745;&#24037;&#20855;&#20998;&#26512;&#22522;&#20110;&#21333;&#19968;&#29305;&#24449;&#30340;&#27169;&#22411;&#35299;&#37322;&#26102;&#65292;&#31616;&#21333;&#29305;&#24449;&#27169;&#22411;&#30340;&#35299;&#37322;&#20256;&#36882;&#26356;&#22810;&#20449;&#21495;&#24182;&#19988;&#22122;&#22768;&#26356;&#23569;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#36890;&#36807;&#26367;&#20195;&#20449;&#21495;&#21644;&#22122;&#22768;&#30340;&#23450;&#20041;&#26469;&#25913;&#36827;&#36825;&#20123;&#32467;&#26524;&#30340;&#21487;&#33021;&#24615;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25429;&#25417;&#26356;&#22797;&#26434;&#30340;&#35299;&#37322;&#21644;&#20998;&#26512;&#26041;&#27861;&#65292;&#21516;&#26102;&#20063;&#36136;&#30097;&#20102;&#19982;&#35835;&#32773;&#21487;&#20449;&#24230;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10275v1 Announce Type: cross  Abstract: The explanations of large language models have recently been shown to be sensitive to the randomness used for their training, creating a need to characterize this sensitivity. In this paper, we propose a characterization that questions the possibility to provide simple and informative explanations for such models. To this end, we give statistical definitions for the explanations' signal, noise and signal-to-noise ratio. We highlight that, in a typical case study where word-level univariate explanations are analyzed with first-order statistical tools, the explanations of simple feature-based models carry more signal and less noise than those of transformer ones. We then discuss the possibility to improve these results with alternative definitions of signal and noise that would capture more complex explanations and analysis methods, while also questioning the tradeoff with their plausibility for readers.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36890;&#36807;&#26412;&#22320;&#35821;&#35328;&#25552;&#31034;&#26469;&#35299;&#20915;&#25991;&#21270;&#30456;&#20851;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#24182;&#21628;&#21505;&#21457;&#23637;&#24378;&#22823;&#30340;&#22810;&#35821;&#35328;LLMs&#12290;</title><link>https://arxiv.org/abs/2403.10258</link><description>&lt;p&gt;
&#32763;&#35793;&#21040;&#24213;&#26159;&#20320;&#25152;&#38656;&#35201;&#30340;&#20840;&#37096;&#21527;&#65311;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#22810;&#35821;&#35328;&#20219;&#21153;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Is Translation All You Need? A Study on Solving Multilingual Tasks with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10258
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36890;&#36807;&#26412;&#22320;&#35821;&#35328;&#25552;&#31034;&#26469;&#35299;&#20915;&#25991;&#21270;&#30456;&#20851;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#24182;&#21628;&#21505;&#21457;&#23637;&#24378;&#22823;&#30340;&#22810;&#35821;&#35328;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#65307;&#28982;&#32780;&#65292;&#30001;&#20110;&#35757;&#32451;&#35821;&#26009;&#24211;&#19981;&#24179;&#34913;&#65292;&#23427;&#20204;&#22823;&#22810;&#26159;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#12290;&#29616;&#26377;&#30740;&#31350;&#21033;&#29992;&#36825;&#19968;&#29616;&#35937;&#26469;&#25552;&#39640;&#23427;&#20204;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#30340;&#22810;&#35821;&#35328;&#24615;&#33021;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#35780;&#20272;&#20174;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#25193;&#23637;&#21040;&#30495;&#23454;&#29992;&#25143;&#26597;&#35810;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#23558;&#25991;&#26412;&#32763;&#35793;&#25104;&#33521;&#35821;&#21487;&#20197;&#24110;&#21161;&#25552;&#39640;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;LLMs&#22312;&#22810;&#35821;&#35328;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#20294;&#24182;&#19981;&#19968;&#23450;&#36866;&#29992;&#20110;&#25152;&#26377;&#22330;&#26223;&#12290;&#23545;&#20110;&#38656;&#35201;&#28145;&#20837;&#29702;&#35299;&#35821;&#35328;&#30340;&#25991;&#21270;&#30456;&#20851;&#20219;&#21153;&#65292;&#20197;&#26412;&#22320;&#35821;&#35328;&#25552;&#31034;&#26356;&#20026;&#26377;&#21069;&#26223;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#25429;&#25417;&#19982;&#25991;&#21270;&#21644;&#35821;&#35328;&#30456;&#20851;&#30340;&#24494;&#22937;&#20043;&#22788;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20027;&#24352;&#30528;&#21147;&#21457;&#23637;&#24378;&#22823;&#30340;&#22810;&#35821;&#35328;LLMs&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10258v1 Announce Type: new  Abstract: Large language models (LLMs) have demonstrated strong multilingual capabilities; yet, they are mostly English-centric due to the imbalanced training corpora. Existing works leverage this phenomenon to improve their multilingual performances on NLP tasks. In this work, we extend the evaluation from NLP tasks to real user queries. We find that even though translation into English can help improve the performance of multilingual NLP tasks for English-centric LLMs, it may not be optimal for all scenarios. For culture-related tasks that need deep language understanding, prompting in the native language proves to be more promising since it can capture the nuances related to culture and language. Therefore, we advocate for more efforts towards the development of strong multilingual LLMs instead of just English-centric LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#26412;&#25366;&#25496;&#21644;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#36229;&#36807;167,000&#31687;&#22312;&#32447;&#26032;&#38395;&#25991;&#31456;&#65292;&#37327;&#21270;&#20102;&#24433;&#21709;&#38750;&#27954;&#20844;&#21496;&#22806;&#36164;&#30452;&#25509;&#25237;&#36164;&#25152;&#26377;&#26435;&#30340;&#21306;&#22495;&#32423;&#23646;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;&#27425;&#22269;&#23478;&#32423;&#30340;&#32467;&#26500;&#21644;&#21046;&#24230;&#29305;&#24449;&#22312;&#22806;&#36164;&#30452;&#25509;&#25237;&#36164;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#12290;</title><link>https://arxiv.org/abs/2403.10239</link><description>&lt;p&gt;
&#19968;&#31181;&#29702;&#35299;&#38750;&#27954;&#22806;&#36164;&#30452;&#25509;&#25237;&#36164;&#27425;&#22269;&#23478;&#20915;&#23450;&#22240;&#32032;&#30340;&#22823;&#25968;&#25454;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Big Data Approach to Understand Sub-national Determinants of FDI in Africa
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10239
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#26412;&#25366;&#25496;&#21644;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#36229;&#36807;167,000&#31687;&#22312;&#32447;&#26032;&#38395;&#25991;&#31456;&#65292;&#37327;&#21270;&#20102;&#24433;&#21709;&#38750;&#27954;&#20844;&#21496;&#22806;&#36164;&#30452;&#25509;&#25237;&#36164;&#25152;&#26377;&#26435;&#30340;&#21306;&#22495;&#32423;&#23646;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;&#27425;&#22269;&#23478;&#32423;&#30340;&#32467;&#26500;&#21644;&#21046;&#24230;&#29305;&#24449;&#22312;&#22806;&#36164;&#30452;&#25509;&#25237;&#36164;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21508;&#31181;&#23439;&#35266;&#32463;&#27982;&#21644;&#21046;&#24230;&#22240;&#32032;&#38459;&#30861;&#20102;&#22806;&#36164;&#30452;&#25509;&#25237;&#36164;&#27969;&#20837;&#65292;&#21253;&#25324;&#33104;&#36133;&#12289;&#36152;&#26131;&#24320;&#25918;&#24615;&#12289;&#34701;&#36164;&#36884;&#24452;&#21644;&#25919;&#27835;&#19981;&#31283;&#23450;&#24615;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#22269;&#23478;&#23618;&#38754;&#25968;&#25454;&#65292;&#23545;&#20225;&#19994;&#23618;&#38754;&#25968;&#25454;&#30340;&#25506;&#32034;&#26377;&#38480;&#65292;&#23588;&#20854;&#22312;&#21457;&#23637;&#20013;&#22269;&#23478;&#12290;&#37492;&#20110;&#36825;&#19968;&#31354;&#30333;&#65292;&#26368;&#36817;&#23545;&#30740;&#31350;&#30340;&#21628;&#21505;&#24378;&#35843;&#20102;&#23545;&#23450;&#24615;&#25968;&#25454;&#20998;&#26512;&#30340;&#38656;&#27714;&#65292;&#29305;&#21035;&#26159;&#22312;&#21306;&#22495;&#23618;&#38754;&#28145;&#20837;&#25506;&#35752;&#22806;&#36164;&#30452;&#25509;&#25237;&#36164;&#20915;&#23450;&#22240;&#32032;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#25991;&#26412;&#25366;&#25496;&#21644;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#65292;&#20174;&#36229;&#36807;167,000&#31687;&#22312;&#32447;&#26032;&#38395;&#25991;&#31456;&#33719;&#21462;&#20449;&#24687;&#65292;&#20197;&#37327;&#21270;&#24433;&#21709;&#38750;&#27954;&#20844;&#21496;&#22806;&#36164;&#30452;&#25509;&#25237;&#36164;&#25152;&#26377;&#26435;&#30340;&#21306;&#22495;&#32423;&#65288;&#27425;&#22269;&#23478;&#32423;&#65289;&#23646;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25193;&#23637;&#20102;&#19990;&#30028;&#38134;&#34892;&#20225;&#19994;&#35843;&#26597;&#25152;&#32472;&#21046;&#30340;&#24037;&#19994;&#21457;&#23637;&#38556;&#30861;&#20449;&#24687;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21306;&#22495;&#32423;&#65288;&#27425;&#22269;&#23478;&#32423;&#65289;&#30340;&#32467;&#26500;&#21644;&#21046;&#24230;&#29305;&#24449;&#22312;&#20915;&#23450;&#22806;&#36164;&#30452;&#25509;&#25237;&#36164;&#26041;&#38754;&#21487;&#20197;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10239v1 Announce Type: new  Abstract: Various macroeconomic and institutional factors hinder FDI inflows, including corruption, trade openness, access to finance, and political instability. Existing research mostly focuses on country-level data, with limited exploration of firm-level data, especially in developing countries. Recognizing this gap, recent calls for research emphasize the need for qualitative data analysis to delve into FDI determinants, particularly at the regional level. This paper proposes a novel methodology, based on text mining and social network analysis, to get information from more than 167,000 online news articles to quantify regional-level (sub-national) attributes affecting FDI ownership in African companies. Our analysis extends information on obstacles to industrial development as mapped by the World Bank Enterprise Surveys. Findings suggest that regional (sub-national) structural and institutional characteristics can play an important role in det
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#28145;&#20837;&#30740;&#31350;&#27874;&#26031;&#25991;&#20027;&#39064;&#26816;&#27979;&#20013;&#30340;&#26368;&#20339;&#31639;&#27861;&#65292;&#30830;&#23450;&#23545;&#36825;&#20123;&#31639;&#27861;&#36827;&#34892;&#36866;&#37197;&#20197;&#36866;&#29992;&#20110;&#27874;&#26031;&#35821;&#30340;&#24517;&#35201;&#25913;&#21160;&#65292;&#24182;&#22312;&#27874;&#26031;&#31038;&#20132;&#32593;&#32476;&#25991;&#26412;&#19978;&#35780;&#20272;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.10237</link><description>&lt;p&gt;
&#22312;&#27874;&#26031;&#25991;&#26412;&#27969;&#20013;&#23545;&#39057;&#32321;&#27169;&#24335;&#25366;&#25496;&#21644;&#32858;&#31867;&#31867;&#21035;&#36827;&#34892;&#20027;&#39064;&#26816;&#27979;&#30340;&#32508;&#21512;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A comprehensive study on Frequent Pattern Mining and Clustering categories for topic detection in Persian text stream
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#28145;&#20837;&#30740;&#31350;&#27874;&#26031;&#25991;&#20027;&#39064;&#26816;&#27979;&#20013;&#30340;&#26368;&#20339;&#31639;&#27861;&#65292;&#30830;&#23450;&#23545;&#36825;&#20123;&#31639;&#27861;&#36827;&#34892;&#36866;&#37197;&#20197;&#36866;&#29992;&#20110;&#27874;&#26031;&#35821;&#30340;&#24517;&#35201;&#25913;&#21160;&#65292;&#24182;&#22312;&#27874;&#26031;&#31038;&#20132;&#32593;&#32476;&#25991;&#26412;&#19978;&#35780;&#20272;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#39064;&#26816;&#27979;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#36807;&#31243;&#65292;&#21462;&#20915;&#20110;&#35821;&#35328;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#23545;&#25991;&#26412;&#36827;&#34892;&#20998;&#26512;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#22312;&#27874;&#26031;&#25991;&#20027;&#39064;&#26816;&#27979;&#26041;&#38754;&#30340;&#30740;&#31350;&#24456;&#23569;&#65292;&#29616;&#26377;&#30340;&#31639;&#27861;&#24182;&#19981;&#24341;&#20154;&#27880;&#30446;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#26088;&#22312;&#30740;&#31350;&#27874;&#26031;&#25991;&#20027;&#39064;&#26816;&#27979;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#65306;1&#65289;&#23545;&#20027;&#39064;&#26816;&#27979;&#30340;&#26368;&#20339;&#31639;&#27861;&#36827;&#34892;&#24191;&#27867;&#30740;&#31350;&#65292;2&#65289;&#30830;&#23450;&#24517;&#35201;&#30340;&#35843;&#25972;&#65292;&#20351;&#36825;&#20123;&#31639;&#27861;&#36866;&#29992;&#20110;&#27874;&#26031;&#35821;&#65292;3&#65289;&#35780;&#20272;&#23427;&#20204;&#22312;&#27874;&#26031;&#31038;&#20132;&#32593;&#32476;&#25991;&#26412;&#19978;&#30340;&#24615;&#33021;&#12290;&#20026;&#23454;&#29616;&#36825;&#20123;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#30740;&#31350;&#38382;&#39064;&#65306;&#31532;&#19968;&#65292;&#37492;&#20110;&#27874;&#26031;&#35821;&#30740;&#31350;&#19981;&#36275;&#65292;&#24212;&#23545;&#29616;&#26377;&#26694;&#26550;&#36827;&#34892;&#20309;&#31181;&#20462;&#25913;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#29992;&#33521;&#35821;&#24320;&#21457;&#30340;&#26694;&#26550;&#65292;&#20197;&#20351;&#23427;&#20204;&#19982;&#27874;&#26031;&#35821;&#20860;&#23481;&#65311;&#31532;&#20108;&#65292;&#36825;&#20123;&#31639;&#27861;&#30340;&#34920;&#29616;&#22914;&#20309;&#65292;&#21738;&#31181;&#31639;&#27861;&#26356;&#20248;&#65311;&#26377;&#21508;&#31181;&#20027;&#39064;&#26816;&#27979;&#26041;&#27861;&#21487;&#20197;&#24402;&#20026;&#19981;&#21516;&#31867;&#21035;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10237v1 Announce Type: new  Abstract: Topic detection is a complex process and depends on language because it somehow needs to analyze text. There have been few studies on topic detection in Persian, and the existing algorithms are not remarkable. Therefore, we aimed to study topic detection in Persian. The objectives of this study are: 1) to conduct an extensive study on the best algorithms for topic detection, 2) to identify necessary adaptations to make these algorithms suitable for the Persian language, and 3) to evaluate their performance on Persian social network texts. To achieve these objectives, we have formulated two research questions: First, considering the lack of research in Persian, what modifications should be made to existing frameworks, especially those developed in English, to make them compatible with Persian? Second, how do these algorithms perform, and which one is superior? There are various topic detection methods that can be categorized into differen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;HawkEye&#65292;&#19968;&#20010;&#21487;&#20197;&#20197;&#23436;&#20840;&#25991;&#26412;&#26041;&#24335;&#25191;&#34892;&#26102;&#38388;&#35270;&#39057;&#23450;&#20301;&#30340;&#35270;&#39057;&#25991;&#26412;LLMs&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#22823;&#35268;&#27169;&#35270;&#39057;&#25991;&#26412;&#35821;&#26009;&#24211;InternVid-G&#20197;&#21450;&#24341;&#20837;&#20004;&#20010;&#26032;&#30340;&#38754;&#21521;&#26102;&#38388;&#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#20197;&#21450;&#19968;&#31181;&#26032;&#30340;&#31895;&#31890;&#24230;&#34920;&#31034;&#35270;&#39057;&#27573;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.10228</link><description>&lt;p&gt;
HawkEye: &#29992;&#20110;&#23558;&#25991;&#26412;&#19982;&#35270;&#39057;&#30456;&#20851;&#32852;&#30340;&#35757;&#32451;&#35270;&#39057;&#25991;&#26412;LLMs
&lt;/p&gt;
&lt;p&gt;
HawkEye: Training Video-Text LLMs for Grounding Text in Videos
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10228
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;HawkEye&#65292;&#19968;&#20010;&#21487;&#20197;&#20197;&#23436;&#20840;&#25991;&#26412;&#26041;&#24335;&#25191;&#34892;&#26102;&#38388;&#35270;&#39057;&#23450;&#20301;&#30340;&#35270;&#39057;&#25991;&#26412;LLMs&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#22823;&#35268;&#27169;&#35270;&#39057;&#25991;&#26412;&#35821;&#26009;&#24211;InternVid-G&#20197;&#21450;&#24341;&#20837;&#20004;&#20010;&#26032;&#30340;&#38754;&#21521;&#26102;&#38388;&#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#20197;&#21450;&#19968;&#31181;&#26032;&#30340;&#31895;&#31890;&#24230;&#34920;&#31034;&#35270;&#39057;&#27573;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#25991;&#26412;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;video-text LLMs&#65289;&#22312;&#22238;&#31572;&#38382;&#39064;&#21644;&#36827;&#34892;&#31616;&#21333;&#35270;&#39057;&#23545;&#35805;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#22312;&#38271;&#32780;&#22797;&#26434;&#30340;&#35270;&#39057;&#20013;&#65292;&#23427;&#20204;&#22312;&#25991;&#26412;&#26597;&#35810;&#19978;&#30340;&#34920;&#29616;&#20960;&#20046;&#19982;&#38543;&#26426;&#30456;&#21516;&#65292;&#20960;&#20046;&#27809;&#26377;&#33021;&#21147;&#29702;&#35299;&#21644;&#25512;&#29702;&#20851;&#20110;&#26102;&#38388;&#20449;&#24687;&#30340;&#20869;&#23481;&#65292;&#36825;&#26159;&#35270;&#39057;&#21644;&#22270;&#20687;&#20043;&#38388;&#26368;&#22522;&#26412;&#30340;&#21306;&#21035;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;HawkEye&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21487;&#20197;&#23436;&#20840;&#20197;&#25991;&#26412;&#26041;&#24335;&#25191;&#34892;&#26102;&#38388;&#35270;&#39057;&#23450;&#20301;&#30340;&#35270;&#39057;&#25991;&#26412;LLMs&#20043;&#19968;&#12290;&#20026;&#20102;&#25910;&#38598;&#36866;&#29992;&#20110;&#26102;&#38388;&#35270;&#39057;&#23450;&#20301;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;InternVid-G&#65292;&#19968;&#20010;&#20855;&#26377;&#20998;&#27573;&#32423;&#26631;&#39064;&#21644;&#36127;&#38388;&#36317;&#30340;&#22823;&#35268;&#27169;&#35270;&#39057;&#25991;&#26412;&#35821;&#26009;&#24211;&#65292;&#36890;&#36807;&#35813;&#35821;&#26009;&#24211;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;&#38754;&#21521;&#26102;&#38388;&#30340;&#35757;&#32451;&#30446;&#26631;&#20197;&#20379;&#35270;&#39057;&#25991;&#26412;LLMs&#20351;&#29992;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#34920;&#31034;&#35270;&#39057;&#20013;&#27573;&#30340;&#31895;&#31890;&#24230;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#27604;LLMs&#23398;&#20064;&#21644;&#36981;&#24490;&#30340;&#26041;&#27861;&#26356;&#31283;&#20581;&#19988;&#26356;&#26131;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10228v1 Announce Type: cross  Abstract: Video-text Large Language Models (video-text LLMs) have shown remarkable performance in answering questions and holding conversations on simple videos. However, they perform almost the same as random on grounding text queries in long and complicated videos, having little ability to understand and reason about temporal information, which is the most fundamental difference between videos and images. In this paper, we propose HawkEye, one of the first video-text LLMs that can perform temporal video grounding in a fully text-to-text manner. To collect training data that is applicable for temporal video grounding, we construct InternVid-G, a large-scale video-text corpus with segment-level captions and negative spans, with which we introduce two new time-aware training objectives to video-text LLMs. We also propose a coarse-grained method of representing segments in videos, which is more robust and easier for LLMs to learn and follow than o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20998;&#23618;&#35299;&#32544;&#32467;&#26500;&#30340;&#22686;&#24378;&#30456;&#24178;&#24863;&#30693;&#32593;&#32476;&#65288;ECAN&#65289;&#29992;&#20110;&#26041;&#38754;&#31867;&#21035;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#65292;&#36890;&#36807;&#25506;&#32034;&#30456;&#24178;&#24615;&#24314;&#27169;&#21644;&#20998;&#32423;&#35299;&#32544;&#65292;&#35299;&#20915;&#20102;&#38544;&#21547;&#26041;&#38754;&#21644;&#24773;&#24863;&#35782;&#21035;&#30340;&#38382;&#39064;&#65292;&#24182;&#26377;&#25928;&#21306;&#20998;&#20102;&#25152;&#26377;&#24773;&#24863;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2403.10214</link><description>&lt;p&gt;
&#20855;&#26377;&#20998;&#23618;&#35299;&#32544;&#32467;&#26500;&#30340;&#22686;&#24378;&#30456;&#24178;&#24863;&#30693;&#32593;&#32476;&#29992;&#20110;&#26041;&#38754;&#31867;&#21035;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Enhanced Coherence-Aware Network with Hierarchical Disentanglement for Aspect-Category Sentiment Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10214
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20998;&#23618;&#35299;&#32544;&#32467;&#26500;&#30340;&#22686;&#24378;&#30456;&#24178;&#24863;&#30693;&#32593;&#32476;&#65288;ECAN&#65289;&#29992;&#20110;&#26041;&#38754;&#31867;&#21035;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#65292;&#36890;&#36807;&#25506;&#32034;&#30456;&#24178;&#24615;&#24314;&#27169;&#21644;&#20998;&#32423;&#35299;&#32544;&#65292;&#35299;&#20915;&#20102;&#38544;&#21547;&#26041;&#38754;&#21644;&#24773;&#24863;&#35782;&#21035;&#30340;&#38382;&#39064;&#65292;&#24182;&#26377;&#25928;&#21306;&#20998;&#20102;&#25152;&#26377;&#24773;&#24863;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26041;&#38754;&#31867;&#21035;&#24773;&#24863;&#20998;&#26512;&#65288;ACSA&#65289;&#26088;&#22312;&#35782;&#21035;&#26041;&#38754;&#31867;&#21035;&#24182;&#39044;&#27979;&#23427;&#20204;&#30340;&#24773;&#24863;&#65292;&#30001;&#20110;&#20854;&#24191;&#27867;&#30340;NLP&#24212;&#29992;&#65292;&#24050;&#34987;&#24191;&#27867;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20998;&#23618;&#35299;&#32544;&#32467;&#26500;&#30340;&#22686;&#24378;&#30456;&#24178;&#24863;&#30693;&#32593;&#32476;&#65288;ECAN&#65289;&#29992;&#20110;ACSA&#20219;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#30456;&#24178;&#24615;&#24314;&#27169;&#65292;&#20197;&#25429;&#33719;&#25972;&#20010;&#35780;&#35770;&#20013;&#30340;&#19978;&#19979;&#25991;&#65292;&#24182;&#26377;&#21161;&#20110;&#38544;&#21547;&#26041;&#38754;&#21644;&#24773;&#24863;&#30340;&#35782;&#21035;&#12290;&#20026;&#20102;&#35299;&#20915;&#22810;&#20010;&#26041;&#38754;&#31867;&#21035;&#21644;&#24773;&#24863;&#20132;&#32455;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10214v1 Announce Type: new  Abstract: Aspect-category-based sentiment analysis (ACSA), which aims to identify aspect categories and predict their sentiments has been intensively studied due to its wide range of NLP applications. Most approaches mainly utilize intrasentential features. However, a review often includes multiple different aspect categories, and some of them do not explicitly appear in the review. Even in a sentence, there is more than one aspect category with its sentiments, and they are entangled intra-sentence, which makes the model fail to discriminately preserve all sentiment characteristics. In this paper, we propose an enhanced coherence-aware network with hierarchical disentanglement (ECAN) for ACSA tasks. Specifically, we explore coherence modeling to capture the contexts across the whole review and to help the implicit aspect and sentiment identification. To address the issue of multiple aspect categories and sentiment entanglement, we propose a hierar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25991;&#26412;&#22788;&#29702;&#20219;&#21153;&#8212;&#8212;&#20174; Git README &#25991;&#20214;&#20013;&#25552;&#21462;&#21151;&#33021;&#65292;&#30740;&#31350;&#21160;&#26426;&#28304;&#33258;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#30456;&#20851;&#20219;&#21153;&#20013;&#24212;&#29992;&#30340;&#20852;&#36259;&#65292;&#36890;&#36807;&#24320;&#21457;&#23567;&#22411;&#24494;&#35843;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;70%&#21644;20%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.10205</link><description>&lt;p&gt;
&#20174; README &#20013;&#25552;&#21462;&#21151;&#33021;
&lt;/p&gt;
&lt;p&gt;
Read between the lines -- Functionality Extraction From READMEs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10205
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25991;&#26412;&#22788;&#29702;&#20219;&#21153;&#8212;&#8212;&#20174; Git README &#25991;&#20214;&#20013;&#25552;&#21462;&#21151;&#33021;&#65292;&#30740;&#31350;&#21160;&#26426;&#28304;&#33258;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#30456;&#20851;&#20219;&#21153;&#20013;&#24212;&#29992;&#30340;&#20852;&#36259;&#65292;&#36890;&#36807;&#24320;&#21457;&#23567;&#22411;&#24494;&#35843;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;70%&#21644;20%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#25991;&#26412;&#25688;&#35201;&#26159;&#19968;&#39033;&#20247;&#25152;&#21608;&#30693;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#20294;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#20174; Git README &#25991;&#20214;&#20013;&#25552;&#21462;&#21151;&#33021;&#30340;&#26032;&#39062;&#32780;&#26377;&#29992;&#30340;&#21464;&#20307;&#12290;&#34429;&#28982;&#36825;&#20010;&#20219;&#21153;&#22312;&#25277;&#35937;&#23618;&#38754;&#19978;&#26159;&#19968;&#20010;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#65292;&#20294;&#23427;&#28041;&#21450;&#21040;&#33258;&#24049;&#30340;&#29305;&#27530;&#24615;&#21644;&#25361;&#25112;&#65292;&#20351;&#24471;&#29616;&#26377;&#30340;&#25991;&#26412;&#29983;&#25104;&#31995;&#32479;&#24182;&#19981;&#21313;&#20998;&#26377;&#29992;&#12290;&#36825;&#19968;&#20219;&#21153;&#30340;&#21160;&#26426;&#28304;&#33258;&#26368;&#36817;&#22260;&#32469;&#30528;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20195;&#30721;&#30456;&#20851;&#20219;&#21153;&#65288;&#22914;&#20195;&#30721;&#37325;&#26500;&#12289;&#20195;&#30721;&#25688;&#35201;&#31561;&#65289;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#27963;&#21160;&#30340;&#28608;&#22686;&#12290;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;&#21517;&#20026;FuncRead&#30340;&#20154;&#24037;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#24182;&#20026;&#36825;&#19968;&#20219;&#21153;&#24320;&#21457;&#20102;&#19968;&#31995;&#21015;&#27169;&#22411;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#35814;&#23613;&#30340;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#23567;&#22411;&#24494;&#35843;&#27169;&#22411;&#20987;&#36133;&#20102;&#21487;&#20197;&#20351;&#29992;&#27969;&#34892;&#30340;&#40657;&#30418;&#25110;&#30333;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65288;&#22914;ChatGPT&#21644;Bard&#65289;&#35774;&#35745;&#30340;&#20219;&#20309;&#22522;&#32447;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26368;&#20339;&#24494;&#35843;&#30340;70&#20159;CodeLlama&#27169;&#22411;&#22312;F1&#19978;&#21462;&#24471;&#20102;70%&#21644;20%&#30340;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10205v1 Announce Type: cross  Abstract: While text summarization is a well-known NLP task, in this paper, we introduce a novel and useful variant of it called functionality extraction from Git README files. Though this task is a text2text generation at an abstract level, it involves its own peculiarities and challenges making existing text2text generation systems not very useful. The motivation behind this task stems from a recent surge in research and development activities around the use of large language models for code-related tasks, such as code refactoring, code summarization, etc. We also release a human-annotated dataset called FuncRead, and develop a battery of models for the task. Our exhaustive experimentation shows that small size fine-tuned models beat any baseline models that can be designed using popular black-box or white-box large language models (LLMs) such as ChatGPT and Bard. Our best fine-tuned 7 Billion CodeLlama model exhibit 70% and 20% gain on the F1
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;DeFaBel&#35821;&#26009;&#24211;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#20449;&#20208;&#30340;&#27450;&#39575;&#30340;&#20247;&#21253;&#36164;&#28304;&#65292;&#29992;&#20110;&#30740;&#31350;&#27450;&#39575;&#19982;&#20107;&#23454;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#24378;&#35843;&#20102;&#35770;&#35777;&#20013;&#20107;&#23454;&#24615;&#12289;&#20010;&#20154;&#20449;&#24565;&#21644;&#27450;&#39575;&#24847;&#22270;&#20043;&#38388;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.10185</link><description>&lt;p&gt;
&#21487;&#20197;&#27450;&#39575;&#24615;&#22320;&#38472;&#36848;&#20107;&#23454;&#21527;&#65311;&#22522;&#20110;&#20449;&#20208;&#30340;&#27450;&#39575;DeFaBel&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
Can Factual Statements be Deceptive? The DeFaBel Corpus of Belief-based Deception
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10185
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;DeFaBel&#35821;&#26009;&#24211;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#20449;&#20208;&#30340;&#27450;&#39575;&#30340;&#20247;&#21253;&#36164;&#28304;&#65292;&#29992;&#20110;&#30740;&#31350;&#27450;&#39575;&#19982;&#20107;&#23454;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#24378;&#35843;&#20102;&#35770;&#35777;&#20013;&#20107;&#23454;&#24615;&#12289;&#20010;&#20154;&#20449;&#24565;&#21644;&#27450;&#39575;&#24847;&#22270;&#20043;&#38388;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#19968;&#20010;&#20154;&#22362;&#20449;&#19968;&#20010;&#38750;&#20107;&#23454;&#24615;&#30340;&#38472;&#36848;&#65292;&#27604;&#22914;&#8220;&#22320;&#29699;&#26159;&#24179;&#30340;&#8221;&#65292;&#24182;&#20026;&#20854;&#36777;&#25252;&#65292;&#37027;&#20040;&#20182;&#24182;&#27809;&#26377;&#26412;&#36136;&#19978;&#30340;&#27450;&#39575;&#24847;&#22270;&#12290;&#30001;&#20110;&#35770;&#35777;&#28304;&#20110;&#30495;&#35802;&#30340;&#20449;&#24565;&#65292;&#23427;&#21487;&#33021;&#19981;&#22826;&#21487;&#33021;&#23637;&#31034;&#20986;&#19982;&#27450;&#39575;&#25110;&#25746;&#35854;&#30456;&#20851;&#30340;&#35821;&#35328;&#29305;&#24449;&#12290;&#20107;&#23454;&#24615;&#12289;&#20010;&#20154;&#20449;&#24565;&#21644;&#27450;&#39575;&#24847;&#22270;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#32463;&#20805;&#20998;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;&#35299;&#24320;&#36825;&#20123;&#21464;&#37327;&#22312;&#35770;&#35777;&#20013;&#30340;&#24433;&#21709;&#23545;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;&#24402;&#22240;&#20110;&#27599;&#19968;&#20010;&#30340;&#35821;&#35328;&#29305;&#24449;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#30740;&#31350;&#22522;&#20110;&#20449;&#24565;&#30340;&#27450;&#39575;&#19982;&#20107;&#23454;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DeFaBel&#35821;&#26009;&#24211;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#20449;&#20208;&#30340;&#27450;&#39575;&#30340;&#20247;&#21253;&#36164;&#28304;&#12290;&#20026;&#20102;&#21019;&#24314;&#36825;&#20010;&#35821;&#26009;&#24211;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#30740;&#31350;&#65292;&#35201;&#27714;&#21442;&#19982;&#32773;&#25776;&#20889;&#25903;&#25345;&#35832;&#22914;&#8220;&#39135;&#29992;&#35199;&#29916;&#31869;&#21487;&#33021;&#23548;&#33268;&#28040;&#21270;&#19981;&#33391;&#8221;&#30340;&#38472;&#36848;&#65292;&#32780;&#19981;&#35770;&#20854;&#20107;&#23454;&#20934;&#30830;&#24615;&#25110;&#20010;&#20154;&#20449;&#24565;&#22914;&#20309;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10185v1 Announce Type: new  Abstract: If a person firmly believes in a non-factual statement, such as "The Earth is flat", and argues in its favor, there is no inherent intention to deceive. As the argumentation stems from genuine belief, it may be unlikely to exhibit the linguistic properties associated with deception or lying. This interplay of factuality, personal belief, and intent to deceive remains an understudied area. Disentangling the influence of these variables in argumentation is crucial to gain a better understanding of the linguistic properties attributed to each of them. To study the relation between deception and factuality, based on belief, we present the DeFaBel corpus, a crowd-sourced resource of belief-based deception. To create this corpus, we devise a study in which participants are instructed to write arguments supporting statements like "eating watermelon seeds can cause indigestion", regardless of its factual accuracy or their personal beliefs about 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23581;&#35797;&#24635;&#32467;&#21644;&#35780;&#20272;&#30001;&#35813;&#39046;&#22495;&#36804;&#20170;&#36827;&#23637;&#32780;&#24418;&#25104;&#30340;NLP&#39564;&#35777;&#27969;&#31243;&#30340;&#19968;&#33324;&#32452;&#25104;&#37096;&#20998;&#65292;&#36129;&#29486;&#22312;&#20110;&#25552;&#20986;&#20102;&#23558;&#21477;&#23376;&#23884;&#20837;&#36830;&#32493;&#31354;&#38388;&#24471;&#21040;&#30340;&#21487;&#39564;&#35777;&#23376;&#31354;&#38388;&#30340;&#19968;&#33324;&#25551;&#36848;&#12290;</title><link>https://arxiv.org/abs/2403.10144</link><description>&lt;p&gt;
NLP&#39564;&#35777;&#65306;&#36208;&#21521;&#19968;&#31181;&#36890;&#29992;&#30340;&#29992;&#20110;&#35748;&#35777;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#35770;
&lt;/p&gt;
&lt;p&gt;
NLP Verification: Towards a General Methodology for Certifying Robustness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10144
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23581;&#35797;&#24635;&#32467;&#21644;&#35780;&#20272;&#30001;&#35813;&#39046;&#22495;&#36804;&#20170;&#36827;&#23637;&#32780;&#24418;&#25104;&#30340;NLP&#39564;&#35777;&#27969;&#31243;&#30340;&#19968;&#33324;&#32452;&#25104;&#37096;&#20998;&#65292;&#36129;&#29486;&#22312;&#20110;&#25552;&#20986;&#20102;&#23558;&#21477;&#23376;&#23884;&#20837;&#36830;&#32493;&#31354;&#38388;&#24471;&#21040;&#30340;&#21487;&#39564;&#35777;&#23376;&#31354;&#38388;&#30340;&#19968;&#33324;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#65292;&#30830;&#20445;&#23427;&#20204;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#65306;&#22312;&#23433;&#20840;&#20851;&#38190;&#30340;&#24773;&#22659;&#20013;&#65292;&#36825;&#20123;&#27169;&#22411;&#24517;&#39035;&#23545;&#21464;&#21270;&#25110;&#25915;&#20987;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#33021;&#23545;&#20854;&#36755;&#20986;&#32473;&#20986;&#20445;&#35777;&#12290;&#19982;&#35745;&#31639;&#26426;&#35270;&#35273;&#19981;&#21516;&#65292;NLP&#32570;&#20047;&#19968;&#20010;&#32479;&#19968;&#30340;&#39564;&#35777;&#26041;&#27861;&#35770;&#65292;&#23613;&#31649;&#36817;&#24180;&#26469;&#25991;&#29486;&#20013;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#23545;&#20110;NLP&#39564;&#35777;&#30340;&#23454;&#29992;&#38382;&#39064;&#24120;&#24120;&#28041;&#21450;&#19981;&#28145;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#25552;&#28860;&#21644;&#35780;&#20272;&#19968;&#20010;NLP&#39564;&#35777;&#27969;&#31243;&#30340;&#19968;&#33324;&#32452;&#25104;&#37096;&#20998;&#65292;&#35813;&#27969;&#31243;&#26469;&#28304;&#20110;&#36804;&#20170;&#20026;&#27490;&#35813;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#20004;&#26041;&#38754;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#23558;&#21477;&#23376;&#23884;&#20837;&#36830;&#32493;&#31354;&#38388;&#24471;&#21040;&#30340;&#21487;&#39564;&#35777;&#23376;&#31354;&#38388;&#30340;&#19968;&#33324;&#25551;&#36848;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#21487;&#39564;&#35777;&#23376;&#31354;&#38388;&#30340;&#35821;&#20041;&#27867;&#21270;&#25216;&#26415;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#22788;&#29702;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10144v1 Announce Type: cross  Abstract: Deep neural networks have exhibited substantial success in the field of Natural Language Processing (NLP) and ensuring their safety and reliability is crucial: there are safety critical contexts where such models must be robust to variability or attack, and give guarantees over their output. Unlike Computer Vision, NLP lacks a unified verification methodology and, despite recent advancements in literature, they are often light on the pragmatical issues of NLP verification. In this paper, we make an attempt to distil and evaluate general components of an NLP verification pipeline, that emerges from the progress in the field to date. Our contributions are two-fold. Firstly, we give a general characterisation of verifiable subspaces that result from embedding sentences into continuous spaces. We identify, and give an effective method to deal with, the technical challenge of semantic generalisability of verified subspaces; and propose it a
&lt;/p&gt;</description></item><item><title>&#25506;&#32034;&#22312;&#39034;&#24207;&#25512;&#33616;&#20013;&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32858;&#21512;&#28436;&#31034;&#30340;&#26032;&#39062;&#26041;&#27861;LLMSRec-Syn&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#22522;&#20110;LLM&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.10135</link><description>&lt;p&gt;
&#25972;&#20307;&#20248;&#20110;&#24635;&#21644;&#65306;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#20351;&#29992;&#32858;&#21512;&#28436;&#31034;&#36827;&#34892;&#39034;&#24207;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
The Whole is Better than the Sum: Using Aggregated Demonstrations in In-Context Learning for Sequential Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10135
&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#22312;&#39034;&#24207;&#25512;&#33616;&#20013;&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32858;&#21512;&#28436;&#31034;&#30340;&#26032;&#39062;&#26041;&#27861;LLMSRec-Syn&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#22522;&#20110;LLM&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20248;&#31168;&#24615;&#33021;&#12290;&#20026;&#20102;&#23558;LLMs&#20316;&#20026;&#24378;&#22823;&#30340;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#39034;&#24207;&#25512;&#33616;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25351;&#23548;&#26684;&#24335;&#12289;&#20219;&#21153;&#19968;&#33268;&#24615;&#12289;&#28436;&#31034;&#36873;&#25321;&#21644;&#28436;&#31034;&#25968;&#37327;&#23545;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;LLMSRec-Syn&#65292;&#36890;&#36807;&#23558;&#22810;&#20010;&#28436;&#31034;&#29992;&#25143;&#25972;&#21512;&#25104;&#19968;&#20010;&#32858;&#21512;&#28436;&#31034;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#25512;&#33616;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#65292;LLMSRec-Syn&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;LLM&#30340;&#39034;&#24207;&#25512;&#33616;&#26041;&#27861;&#12290;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;LLMSRec-Syn&#21487;&#20197;&#19982;&#29978;&#33267;&#20248;&#20110;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#20844;&#24320;&#22312;https://github.com/demoleiwang/LLMSRec_Syn&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10135v1 Announce Type: cross  Abstract: Large language models (LLMs) have shown excellent performance on various NLP tasks. To use LLMs as strong sequential recommenders, we explore the in-context learning approach to sequential recommendation. We investigate the effects of instruction format, task consistency, demonstration selection, and number of demonstrations. As increasing the number of demonstrations in ICL does not improve accuracy despite using a long prompt, we propose a novel method called LLMSRec-Syn that incorporates multiple demonstration users into one aggregated demonstration. Our experiments on three recommendation datasets show that LLMSRec-Syn outperforms state-of-the-art LLM-based sequential recommendation methods. In some cases, LLMSRec-Syn can perform on par with or even better than supervised learning methods. Our code is publicly available at https://github.com/demoleiwang/LLMSRec_Syn.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RAFT&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#29992;&#30456;&#20851;&#25991;&#26723;&#20013;&#33021;&#22815;&#24110;&#21161;&#22238;&#31572;&#38382;&#39064;&#30340;&#27491;&#30830;&#24207;&#21015;&#26469;&#25913;&#21892;&#27169;&#22411;&#22312;&#29305;&#23450;&#39046;&#22495;&#20013;&#22238;&#31572;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.10131</link><description>&lt;p&gt;
RAFT&#65306;&#23558;&#35821;&#35328;&#27169;&#22411;&#35843;&#25972;&#21040;&#29305;&#23450;&#39046;&#22495;RAG
&lt;/p&gt;
&lt;p&gt;
RAFT: Adapting Language Model to Domain Specific RAG
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10131
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RAFT&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#29992;&#30456;&#20851;&#25991;&#26723;&#20013;&#33021;&#22815;&#24110;&#21161;&#22238;&#31572;&#38382;&#39064;&#30340;&#27491;&#30830;&#24207;&#21015;&#26469;&#25913;&#21892;&#27169;&#22411;&#22312;&#29305;&#23450;&#39046;&#22495;&#20013;&#22238;&#31572;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22312;&#65292;&#36890;&#36807;&#22823;&#35268;&#27169;&#25991;&#26412;&#25968;&#25454;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#26631;&#20934;&#33539;&#24335;&#12290;&#22312;&#23558;&#36825;&#20123;LLMs&#29992;&#20110;&#35768;&#22810;&#19979;&#28216;&#24212;&#29992;&#31243;&#24207;&#26102;&#65292;&#36890;&#24120;&#36824;&#20250;&#36890;&#36807;&#22522;&#20110;RAG&#30340;&#25552;&#31034;&#25110;&#24494;&#35843;&#65292;&#23558;&#26032;&#30693;&#35782;&#65288;&#20363;&#22914;&#65292;&#26102;&#25928;&#26032;&#38395;&#25110;&#31169;&#26377;&#39046;&#22495;&#30693;&#35782;&#65289;&#23884;&#20837;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#33719;&#24471;&#36825;&#20123;&#26032;&#30693;&#35782;&#30340;&#26368;&#20339;&#26041;&#27861;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26816;&#32034;&#22686;&#24378;&#24494;&#35843;&#65288;RAFT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#35757;&#32451;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#27169;&#22411;&#22312;"&#24320;&#25918;&#20070;&#31821;"&#30340;&#39046;&#22495;&#35774;&#32622;&#20013;&#22238;&#31572;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#22312;RAFT&#20013;&#65292;&#32473;&#23450;&#19968;&#20010;&#38382;&#39064;&#21644;&#19968;&#32452;&#26816;&#32034;&#21040;&#30340;&#25991;&#26723;&#65292;&#25105;&#20204;&#35757;&#32451;&#27169;&#22411;&#24573;&#30053;&#37027;&#20123;&#23545;&#22238;&#31572;&#38382;&#39064;&#27809;&#26377;&#24110;&#21161;&#30340;&#25991;&#26723;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#24178;&#25200;&#25991;&#26723;&#12290;RAFT&#36890;&#36807;&#21407;&#25991;&#24341;&#29992;&#30456;&#20851;&#25991;&#26723;&#20013;&#33021;&#22815;&#24110;&#21161;&#22238;&#31572;&#38382;&#39064;&#30340;&#27491;&#30830;&#24207;&#21015;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10131v1 Announce Type: cross  Abstract: Pretraining Large Language Models (LLMs) on large corpora of textual data is now a standard paradigm. When using these LLMs for many downstream applications, it is common to additionally bake in new knowledge (e.g., time-critical news, or private domain knowledge) into the pretrained model either through RAG-based-prompting, or fine-tuning. However, the optimal methodology for the model to gain such new knowledge remains an open question. In this paper, we present Retrieval Augmented FineTuning (RAFT), a training recipe that improves the model's ability to answer questions in a "open-book" in-domain settings. In RAFT, given a question, and a set of retrieved documents, we train the model to ignore those documents that don't help in answering the question, which we call, distractor documents. RAFT accomplishes this by citing verbatim the right sequence from the relevant document that would help answer the question. This coupled with RAF
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;CoARL&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#25311;&#31038;&#20250;&#20559;&#35265;&#20013;&#30340;&#35821;&#29992;&#21551;&#31034;&#26469;&#22686;&#24378;&#23545;&#25239;&#24615;&#35328;&#35770;&#29983;&#25104;&#65292;&#21033;&#29992;&#39034;&#24207;&#22810;&#25351;&#23548;&#35843;&#33410;&#21644;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#24847;&#22270;&#35843;&#33410;&#30340;&#23545;&#25239;&#24615;&#35328;&#35770;&#12290;</title><link>https://arxiv.org/abs/2403.10088</link><description>&lt;p&gt;
&#21033;&#29992;RLAIF&#36827;&#34892;&#24847;&#22270;&#35843;&#33410;&#21644;&#26080;&#27602;&#23545;&#25239;&#29983;&#25104;&#30340;&#22810;&#20219;&#21153;&#25351;&#23548;&#35843;&#33410;
&lt;/p&gt;
&lt;p&gt;
Intent-conditioned and Non-toxic Counterspeech Generation using Multi-Task Instruction Tuning with RLAIF
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10088
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;CoARL&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#25311;&#31038;&#20250;&#20559;&#35265;&#20013;&#30340;&#35821;&#29992;&#21551;&#31034;&#26469;&#22686;&#24378;&#23545;&#25239;&#24615;&#35328;&#35770;&#29983;&#25104;&#65292;&#21033;&#29992;&#39034;&#24207;&#22810;&#25351;&#23548;&#35843;&#33410;&#21644;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#24847;&#22270;&#35843;&#33410;&#30340;&#23545;&#25239;&#24615;&#35328;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#35328;&#35770;&#34987;&#23450;&#20041;&#20026;&#20943;&#32531;&#32593;&#32476;&#20167;&#24680;&#35328;&#35770;&#30340;&#22238;&#24212;&#65292;&#36234;&#26469;&#36234;&#34987;&#29992;&#20316;&#19968;&#31181;&#38750;&#23457;&#26597;&#35299;&#20915;&#26041;&#26696;&#12290;&#26377;&#25928;&#24212;&#23545;&#20167;&#24680;&#35328;&#35770;&#28041;&#21450;&#28040;&#38500;&#36890;&#24120;&#22312;&#31616;&#30701;&#30340;&#21333;&#21477;&#38472;&#36848;&#25110;&#34384;&#24453;&#20013;&#26263;&#31034;&#30340;&#21051;&#26495;&#21360;&#35937;&#12289;&#20559;&#35265;&#21644;&#20559;&#35265;&#12290;&#36825;&#20123;&#38544;&#21547;&#30340;&#34920;&#36798;&#25361;&#25112;&#35821;&#35328;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22312;seq2seq&#20219;&#21153;&#20013;&#65292;&#22240;&#20026;&#27169;&#22411;&#24615;&#33021;&#36890;&#24120;&#22312;&#26356;&#38271;&#19978;&#19979;&#25991;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;CoARL&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24314;&#27169;&#22312;&#20167;&#24680;&#35328;&#35770;&#20013;&#26263;&#21547;&#30340;&#31038;&#20250;&#20559;&#35265;&#30340;&#23454;&#29992;&#21547;&#20041;&#26469;&#22686;&#24378;&#23545;&#25239;&#24615;&#35328;&#35770;&#29983;&#25104;&#12290;CoARL&#30340;&#21069;&#20004;&#20010;&#38454;&#27573;&#28041;&#21450;&#39034;&#24207;&#22810;&#25351;&#23548;&#35843;&#33410;&#65292;&#25945;&#23548;&#27169;&#22411;&#29702;&#35299;&#25915;&#20987;&#24615;&#38472;&#36848;&#30340;&#24847;&#22270;&#12289;&#21453;&#24212;&#21644;&#21361;&#23475;&#65292;&#28982;&#21518;&#23398;&#20064;&#29983;&#25104;&#24847;&#22270;&#35843;&#33410;&#30340;&#23545;&#25239;&#24615;&#35328;&#35770;&#30340;&#29305;&#23450;&#20219;&#21153;&#20302;&#31209;&#36866;&#37197;&#22120;&#26435;&#37325;&#12290;&#26368;&#21518;&#19968;&#20010;&#38454;&#27573;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#23545;&#36755;&#20986;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#25552;&#39640;&#25928;&#26524;&#21644;&#26080;&#27602;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10088v1 Announce Type: cross  Abstract: Counterspeech, defined as a response to mitigate online hate speech, is increasingly used as a non-censorial solution. Addressing hate speech effectively involves dispelling the stereotypes, prejudices, and biases often subtly implied in brief, single-sentence statements or abuses. These implicit expressions challenge language models, especially in seq2seq tasks, as model performance typically excels with longer contexts. Our study introduces CoARL, a novel framework enhancing counterspeech generation by modeling the pragmatic implications underlying social biases in hateful statements. CoARL's first two phases involve sequential multi-instruction tuning, teaching the model to understand intents, reactions, and harms of offensive statements, and then learning task-specific low-rank adapter weights for generating intent-conditioned counterspeech. The final phase uses reinforcement learning to fine-tune outputs for effectiveness and non-
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;DRAGIN&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#21160;&#24577;&#26816;&#32034;&#21644;&#29983;&#25104;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.10081</link><description>&lt;p&gt;
DRAGIN&#65306;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#26102;&#20449;&#24687;&#38656;&#27714;&#30340;&#21160;&#24577;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
DRAGIN: Dynamic Retrieval Augmented Generation based on the Real-time Information Needs of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10081
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;DRAGIN&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#21160;&#24577;&#26816;&#32034;&#21644;&#29983;&#25104;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#33539;&#24335;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#20027;&#21160;&#20915;&#23450;&#20309;&#26102;&#20197;&#21450;&#20309;&#26102;&#26816;&#32034;&#12290;&#35813;&#33539;&#24335;&#30340;&#20004;&#20010;&#20851;&#38190;&#20803;&#32032;&#26159;&#30830;&#23450;&#28608;&#27963;&#26816;&#32034;&#27169;&#22359;&#30340;&#26368;&#20339;&#26102;&#26426;&#65288;&#20915;&#23450;&#20309;&#26102;&#26816;&#32034;&#65289;&#20197;&#21450;&#19968;&#26086;&#35302;&#21457;&#26816;&#32034;&#65292;&#21046;&#23450;&#36866;&#24403;&#30340;&#26597;&#35810;&#65288;&#30830;&#23450;&#35201;&#26816;&#32034;&#20160;&#20040;&#65289;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#21160;&#24577;RAG&#26041;&#27861;&#22312;&#20004;&#20010;&#26041;&#38754;&#37117;&#23384;&#22312;&#19981;&#36275;&#12290;&#39318;&#20808;&#65292;&#20915;&#23450;&#20309;&#26102;&#36827;&#34892;&#26816;&#32034;&#30340;&#31574;&#30053;&#36890;&#24120;&#20381;&#36182;&#20110;&#38745;&#24577;&#35268;&#21017;&#12290;&#27492;&#22806;&#65292;&#20915;&#23450;&#35201;&#26816;&#32034;&#20160;&#20040;&#30340;&#31574;&#30053;&#36890;&#24120;&#23616;&#38480;&#20110;LLM&#30340;&#26368;&#36817;&#19968;&#21477;&#25110;&#26368;&#21518;&#20960;&#20010;&#26631;&#35760;&#65292;&#32780;LLM&#30340;&#23454;&#26102;&#20449;&#24687;&#38656;&#27714;&#21487;&#33021;&#36328;&#36234;&#25972;&#20010;&#19978;&#19979;&#25991;&#12290;&#20026;&#20811;&#26381;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;DRAGIN&#65292; &#21363;&#22522;&#20110;LLMs&#23454;&#26102;&#20449;&#24687;&#38656;&#27714;&#30340;&#21160;&#24577;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10081v1 Announce Type: new  Abstract: Dynamic retrieval augmented generation (RAG) paradigm actively decides when and what to retrieve during the text generation process of Large Language Models (LLMs). There are two key elements of this paradigm: identifying the optimal moment to activate the retrieval module (deciding when to retrieve) and crafting the appropriate query once retrieval is triggered (determining what to retrieve). However, current dynamic RAG methods fall short in both aspects. Firstly, the strategies for deciding when to retrieve often rely on static rules. Moreover, the strategies for deciding what to retrieve typically limit themselves to the LLM's most recent sentence or the last few tokens, while the LLM's real-time information needs may span across the entire context. To overcome these limitations, we introduce a new framework, DRAGIN, i.e., Dynamic Retrieval Augmented Generation based on the real-time Information Needs of LLMs. Our framework is specif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;Triple GNNs&#32593;&#32476;&#26469;&#22686;&#24378;&#23545;&#35805;&#22522;&#20110;&#26041;&#38754;&#30340;&#22235;&#37325;&#24773;&#24863;&#20998;&#26512;&#65292;&#36890;&#36807;&#20351;&#29992;GCN&#26469;&#24314;&#27169;&#35805;&#35821;&#20869;&#30340;&#21477;&#27861;&#20381;&#36182;&#20851;&#31995;&#21644;DualGATs&#26469;&#26500;&#24314;&#35805;&#35821;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;</title><link>https://arxiv.org/abs/2403.10065</link><description>&lt;p&gt;
&#19977;&#37325;GNN&#65306;&#20026;&#23545;&#35805;&#22522;&#20110;&#26041;&#38754;&#30340;&#22235;&#37325;&#24773;&#24863;&#20998;&#26512;&#24341;&#20837;&#21477;&#27861;&#21644;&#35821;&#20041;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Triple GNNs: Introducing Syntactic and Semantic Information for Conversational Aspect-Based Quadruple Sentiment Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10065
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;Triple GNNs&#32593;&#32476;&#26469;&#22686;&#24378;&#23545;&#35805;&#22522;&#20110;&#26041;&#38754;&#30340;&#22235;&#37325;&#24773;&#24863;&#20998;&#26512;&#65292;&#36890;&#36807;&#20351;&#29992;GCN&#26469;&#24314;&#27169;&#35805;&#35821;&#20869;&#30340;&#21477;&#27861;&#20381;&#36182;&#20851;&#31995;&#21644;DualGATs&#26469;&#26500;&#24314;&#35805;&#35821;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10065v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#23545;&#35805;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#65288;DiaASQ&#65289;&#26088;&#22312;&#20174;&#32473;&#23450;&#30340;&#23545;&#35805;&#20013;&#26816;&#27979;&#22235;&#37325;\{&#30446;&#26631;&#65292;&#26041;&#38754;&#65292;&#35266;&#28857;&#65292;&#24773;&#24863;&#26497;&#24615;\}&#12290;&#22312;DiaASQ&#20013;&#65292;&#26500;&#25104;&#36825;&#20123;&#22235;&#37325;&#30340;&#20803;&#32032;&#19981;&#19968;&#23450;&#23616;&#38480;&#20110;&#21333;&#29420;&#30340;&#21477;&#23376;&#65292;&#32780;&#21487;&#33021;&#36328;&#36234;&#23545;&#35805;&#20013;&#30340;&#22810;&#20010;&#35805;&#35821;&#12290;&#36825;&#38656;&#35201;&#21516;&#26102;&#20851;&#27880;&#21333;&#20010;&#35805;&#35821;&#30340;&#21477;&#27861;&#20449;&#24687;&#21644;&#23427;&#20204;&#20043;&#38388;&#30340;&#35821;&#20041;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#35805;&#35821;&#20043;&#38388;&#30340;&#31895;&#31890;&#24230;&#20851;&#31995;&#65292;&#22240;&#27492;&#24573;&#35270;&#20102;&#35814;&#32454;&#30340;&#35805;&#35821;&#20869;&#21477;&#27861;&#20449;&#24687;&#21644;&#35805;&#35821;&#38388;&#20851;&#31995;&#30340;&#32454;&#31890;&#24230;&#28508;&#22312;&#22909;&#22788;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Triple GNNs&#32593;&#32476;&#20197;&#22686;&#24378;DiaASQ&#12290;&#23427;&#37319;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#26469;&#24314;&#27169;&#35805;&#35821;&#20869;&#30340;&#21477;&#27861;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#21450;&#21452;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;DualGATs&#65289;&#26469;&#26500;&#24314;&#35805;&#35821;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10065v1 Announce Type: new  Abstract: Conversational Aspect-Based Sentiment Analysis (DiaASQ) aims to detect quadruples \{target, aspect, opinion, sentiment polarity\} from given dialogues. In DiaASQ, elements constituting these quadruples are not necessarily confined to individual sentences but may span across multiple utterances within a dialogue. This necessitates a dual focus on both the syntactic information of individual utterances and the semantic interaction among them. However, previous studies have primarily focused on coarse-grained relationships between utterances, thus overlooking the potential benefits of detailed intra-utterance syntactic information and the granularity of inter-utterance relationships. This paper introduces the Triple GNNs network to enhance DiaAsQ. It employs a Graph Convolutional Network (GCN) for modeling syntactic dependencies within utterances and a Dual Graph Attention Network (DualGATs) to construct interactions between utterances. Exp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36873;&#25321;&#24615;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20351;&#20195;&#30721;LM&#33021;&#22815;&#36991;&#20813;&#19981;&#24517;&#35201;&#30340;&#26816;&#32034;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#19978;&#22987;&#32456;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.10059</link><description>&lt;p&gt;
Repoformer&#65306;&#38754;&#21521;&#23384;&#20648;&#24211;&#32423;&#20195;&#30721;&#34917;&#20840;&#30340;&#36873;&#25321;&#24615;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Repoformer: Selective Retrieval for Repository-Level Code Completion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36873;&#25321;&#24615;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20351;&#20195;&#30721;LM&#33021;&#22815;&#36991;&#20813;&#19981;&#24517;&#35201;&#30340;&#26816;&#32034;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#19978;&#22987;&#32456;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10059v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#25991;&#25688;&#65306;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#24320;&#21551;&#20102;&#23384;&#20648;&#24211;&#32423;&#20195;&#30721;&#34917;&#20840;&#30340;&#26032;&#26102;&#20195;&#12290;&#20294;&#26159;&#65292;&#29616;&#26377;&#26041;&#27861;&#20013;&#26816;&#32034;&#30340;&#19981;&#21464;&#20351;&#29992;&#26292;&#38706;&#20102;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#30340;&#38382;&#39064;&#65292;&#22823;&#37096;&#20998;&#26816;&#32034;&#21040;&#30340;&#19978;&#19979;&#25991;&#23545;&#20110;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#65288;code LM&#65289;&#26469;&#35828;&#26082;&#26080;&#25928;&#21448;&#26377;&#23475;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36873;&#25321;&#24615;RAG&#26694;&#26550;&#65292;&#22312;&#19981;&#24517;&#35201;&#26102;&#36991;&#20813;&#20351;&#29992;&#26816;&#32034;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#19968;&#26694;&#26550;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#20195;&#30721;LM&#33021;&#22815;&#20934;&#30830;&#33258;&#25105;&#35780;&#20272;&#26816;&#32034;&#26159;&#21542;&#21487;&#20197;&#25552;&#39640;&#20854;&#36755;&#20986;&#36136;&#37327;&#65292;&#24182;&#33021;&#22815;&#31283;&#20581;&#22320;&#21033;&#29992;&#28508;&#22312;&#21547;&#22122;&#22768;&#30340;&#26816;&#32034;&#19978;&#19979;&#25991;&#12290;&#20351;&#29992;&#36825;&#31181;LM&#20316;&#20026;&#36873;&#25321;&#24615;&#26816;&#32034;&#31574;&#30053;&#21644;&#29983;&#25104;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#21253;&#25324;RepoEval&#12289;CrossCodeEval&#21644;&#19968;&#20010;&#26032;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10059v1 Announce Type: cross  Abstract: Recent advances in retrieval-augmented generation (RAG) have initiated a new era in repository-level code completion. However, the invariable use of retrieval in existing methods exposes issues in both efficiency and robustness, with a large proportion of the retrieved contexts proving unhelpful or harmful to code language models (code LMs). To tackle the challenges, this paper proposes a selective RAG framework where retrieval is avoided when unnecessary. To power this framework, we design a self-supervised learning approach that enables a code LM to accurately self-evaluate whether retrieval can improve its output quality and robustly leverage the potentially noisy retrieved contexts. Using this LM as both the selective retrieval policy and the generation model, our framework consistently outperforms the state-of-the-art prompting with an invariable retrieval approach on diverse benchmarks including RepoEval, CrossCodeEval, and a new
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#38190;&#37096;&#20998;&#20449;&#24687;&#22686;&#30410;&#30340;&#26032;&#22411;&#36830;&#32493;&#25351;&#23548;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#37325;&#25918;&#25968;&#25454;&#21644;&#20248;&#21270;&#35757;&#32451;&#30446;&#26631;&#65292;&#20351;LLMs&#33021;&#22815;&#25429;&#25417;&#20219;&#21153;&#24863;&#30693;&#20449;&#24687;&#21644;&#20943;&#36731;&#36807;&#24230;&#25311;&#21512;&#12290;</title><link>https://arxiv.org/abs/2403.10056</link><description>&lt;p&gt;
&#19981;&#35201;&#21322;&#24515;&#21322;&#24847;&#65306;&#25429;&#25417;&#36830;&#32493;&#25351;&#23548;&#35843;&#25972;&#20013;&#30340;&#20851;&#38190;&#37096;&#20998;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Don't Half-listen: Capturing Key-part Information in Continual Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10056
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#38190;&#37096;&#20998;&#20449;&#24687;&#22686;&#30410;&#30340;&#26032;&#22411;&#36830;&#32493;&#25351;&#23548;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#37325;&#25918;&#25968;&#25454;&#21644;&#20248;&#21270;&#35757;&#32451;&#30446;&#26631;&#65292;&#20351;LLMs&#33021;&#22815;&#25429;&#25417;&#20219;&#21153;&#24863;&#30693;&#20449;&#24687;&#21644;&#20943;&#36731;&#36807;&#24230;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10056v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25351;&#23548;&#35843;&#25972;&#21487;&#20197;&#39537;&#20351;&#23427;&#20204;&#22312;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#20013;&#20135;&#29983;&#31526;&#21512;&#20154;&#31867;&#30446;&#26631;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;LLMs&#30340;&#36830;&#32493;&#25351;&#23548;&#35843;&#25972;&#65288;CIT&#65289;&#36807;&#31243;&#21487;&#33021;&#20250;&#24102;&#26469;&#28798;&#38590;&#24615;&#36951;&#24536;&#65288;CF&#65289;&#38382;&#39064;&#65292;&#23548;&#33268;&#20808;&#21069;&#23398;&#21040;&#30340;&#33021;&#21147;&#36864;&#21270;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#23581;&#35797;&#36890;&#36807;&#20462;&#25913;&#27169;&#22411;&#25110;&#37325;&#25918;&#25968;&#25454;&#26469;&#32531;&#35299;CF&#38382;&#39064;&#65292;&#20294;&#36825;&#21487;&#33021;&#21482;&#35760;&#20303;&#25351;&#20196;&#30340;&#34920;&#38754;&#27169;&#24335;&#24182;&#22312;&#30041;&#23384;&#20219;&#21153;&#19978;&#24863;&#21040;&#22256;&#24785;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#38190;&#37096;&#20998;&#20449;&#24687;&#22686;&#30410;&#65288;KPIG&#65289;&#30340;&#26032;&#22411;&#36830;&#32493;&#25351;&#23548;&#35843;&#25972;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35745;&#31639;&#25513;&#30422;&#37096;&#20998;&#30340;&#20449;&#24687;&#22686;&#30410;&#65292;&#21160;&#24577;&#37325;&#25918;&#25968;&#25454;&#24182;&#20248;&#21270;&#35757;&#32451;&#30446;&#26631;&#65292;&#20174;&#32780;&#20351;LLMs&#33021;&#22815;&#25429;&#25417;&#19982;&#27491;&#30830;&#21709;&#24212;&#30456;&#20851;&#30340;&#20219;&#21153;&#24863;&#30693;&#20449;&#24687;&#65292;&#24182;&#20943;&#36731;&#23545;&#25351;&#23548;&#20013;&#36890;&#29992;&#25551;&#36848;&#30340;&#36807;&#24230;&#25311;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#25351;&#26631;&#65292;P&#20998;&#21644;V&#20998;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10056v1 Announce Type: cross  Abstract: Instruction tuning for large language models (LLMs) can drive them to produce results consistent with human goals in specific downstream tasks. However, the process of continual instruction tuning (CIT) for LLMs may bring about the catastrophic forgetting (CF) problem, where previously learned abilities are degraded. Recent methods try to alleviate the CF problem by modifying models or replaying data, which may only remember the surface-level pattern of instructions and get confused on held-out tasks. In this paper, we propose a novel continual instruction tuning method based on Key-part Information Gain (KPIG). Our method computes the information gain on masked parts to dynamically replay data and refine the training objective, which enables LLMs to capture task-aware information relevant to the correct response and alleviate overfitting to general descriptions in instructions. In addition, we propose two metrics, P-score and V-score,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20851;&#20110;&#27700;&#21360;&#20914;&#31361;&#30340;&#38382;&#39064;&#65292;&#21457;&#29616;&#21452;&#27700;&#21360;&#20914;&#31361;&#23384;&#22312;&#26102;&#20250;&#23545;&#27700;&#21360;&#31639;&#27861;&#30340;&#26816;&#27979;&#24615;&#33021;&#36896;&#25104;&#23041;&#32961;&#12290;</title><link>https://arxiv.org/abs/2403.10020</link><description>&lt;p&gt;
&#22312;&#37325;&#21472;&#20013;&#36855;&#22833;&#65306;&#25506;&#32034;LLMs&#20013;&#30340;&#27700;&#21360;&#20914;&#31361;
&lt;/p&gt;
&lt;p&gt;
Lost in Overlap: Exploring Watermark Collision in LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10020
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20851;&#20110;&#27700;&#21360;&#20914;&#31361;&#30340;&#38382;&#39064;&#65292;&#21457;&#29616;&#21452;&#27700;&#21360;&#20914;&#31361;&#23384;&#22312;&#26102;&#20250;&#23545;&#27700;&#21360;&#31639;&#27861;&#30340;&#26816;&#27979;&#24615;&#33021;&#36896;&#25104;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#25104;&#20869;&#23481;&#26041;&#38754;&#30340;&#26222;&#21450;&#65292;&#24341;&#21457;&#20102;&#20851;&#20110;&#25991;&#26412;&#29256;&#26435;&#30340;&#25285;&#24551;&#12290;&#27700;&#21360;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;logit&#30340;&#26041;&#27861;&#65292;&#23558;&#19981;&#21487;&#23519;&#35273;&#30340;&#26631;&#35782;&#23884;&#20837;&#25991;&#26412;&#20013;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#27700;&#21360;&#26041;&#27861;&#22312;&#19981;&#21516;LLMs&#19978;&#30340;&#24191;&#27867;&#24212;&#29992;&#23548;&#33268;&#20102;&#19968;&#31181;&#19981;&#21487;&#36991;&#20813;&#30340;&#38382;&#39064;&#65292;&#21363;&#22312;&#24120;&#35265;&#20219;&#21153;&#65288;&#22914;&#38382;&#31572;&#21644;&#25913;&#20889;&#65289;&#20013;&#21457;&#29983;&#30340;&#27700;&#21360;&#20914;&#31361;&#12290;&#26412;&#30740;&#31350;&#20851;&#27880;&#21452;&#27700;&#21360;&#20914;&#31361;&#65292;&#21363;&#21516;&#19968;&#25991;&#26412;&#20013;&#21516;&#26102;&#23384;&#22312;&#20004;&#20010;&#27700;&#21360;&#30340;&#24773;&#20917;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#27700;&#21360;&#20914;&#31361;&#23545;&#19978;&#28216;&#21644;&#19979;&#28216;&#27700;&#21360;&#31639;&#27861;&#30340;&#26816;&#27979;&#22120;&#30340;&#26816;&#27979;&#24615;&#33021;&#26500;&#25104;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10020v1 Announce Type: new  Abstract: The proliferation of large language models (LLMs) in generating content raises concerns about text copyright. Watermarking methods, particularly logit-based approaches, embed imperceptible identifiers into text to address these challenges. However, the widespread use of watermarking across diverse LLMs has led to an inevitable issue known as watermark collision during common tasks like question answering and paraphrasing. This study focuses on dual watermark collisions, where two watermarks are present simultaneously in the same text. The research demonstrates that watermark collision poses a threat to detection performance for detectors of both upstream and downstream watermark algorithms.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#20174;&#25968;&#23383;&#20581;&#24247;&#35760;&#24405;&#20013;&#35782;&#21035;&#23478;&#26063;&#24615;&#30142;&#30149;&#39118;&#38505;&#30340;&#25991;&#29486;&#65292;&#24378;&#35843;&#20102;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#20197;&#21450;&#26368;&#36817;&#26356;&#27880;&#37325;&#26500;&#24314;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.09997</link><description>&lt;p&gt;
&#20174;&#23478;&#26063;&#21490;&#20013;&#35782;&#21035;&#20581;&#24247;&#39118;&#38505;&#65306;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Identifying Health Risks from Family History: A Survey of Natural Language Processing Techniques
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09997
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#20174;&#25968;&#23383;&#20581;&#24247;&#35760;&#24405;&#20013;&#35782;&#21035;&#23478;&#26063;&#24615;&#30142;&#30149;&#39118;&#38505;&#30340;&#25991;&#29486;&#65292;&#24378;&#35843;&#20102;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#20197;&#21450;&#26368;&#36817;&#26356;&#27880;&#37325;&#26500;&#24314;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09997v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#21253;&#21547;&#24739;&#32773;&#30340;&#29366;&#24577;&#21644;&#30149;&#21490;&#20449;&#24687;&#65292;&#20854;&#20013;&#21487;&#33021;&#28085;&#30422;&#19968;&#20123;&#21487;&#33021;&#20855;&#36951;&#20256;&#24615;&#30340;&#30142;&#30149;&#21644;&#30142;&#30149;&#21490;&#12290;&#23478;&#26063;&#21490;&#20449;&#24687;&#30340;&#19968;&#20010;&#37325;&#35201;&#29992;&#36884;&#22312;&#20110;&#31934;&#20934;&#20581;&#24247;&#65292;&#20854;&#30446;&#26631;&#26159;&#36890;&#36807;&#39044;&#38450;&#25514;&#26045;&#20445;&#25345;&#20154;&#32676;&#20581;&#24247;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21487;&#20197;&#24110;&#21161;&#35782;&#21035;&#26377;&#21161;&#20110;&#20581;&#24247;&#19987;&#19994;&#20154;&#22763;&#22312;&#24739;&#32773;&#26202;&#24180;&#20043;&#21069;&#35782;&#21035;&#20581;&#24247;&#39118;&#38505;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#25405;&#25937;&#29983;&#21629;&#24182;&#38477;&#20302;&#21307;&#30103;&#25104;&#26412;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;NLP&#39046;&#22495;&#20851;&#20110;&#21033;&#29992;&#25968;&#23383;&#20581;&#24247;&#35760;&#24405;&#26469;&#35782;&#21035;&#23478;&#26063;&#24615;&#30142;&#30149;&#39118;&#38505;&#30340;&#25216;&#26415;&#25991;&#29486;&#12290;&#25105;&#20204;&#24378;&#35843;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#24182;&#20173;&#28982;&#34987;&#24191;&#27867;&#29992;&#20110;&#23478;&#26063;&#21490;&#25552;&#21462;&#12290;&#28982;&#32780;&#65292;&#36817;&#24180;&#26469;&#30340;&#24037;&#20316;&#26356;&#22810;&#22320;&#33268;&#21147;&#20110;&#26500;&#24314;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09997v1 Announce Type: new  Abstract: Electronic health records include information on patients' status and medical history, which could cover the history of diseases and disorders that could be hereditary. One important use of family history information is in precision health, where the goal is to keep the population healthy with preventative measures. Natural Language Processing (NLP) and machine learning techniques can assist with identifying information that could assist health professionals in identifying health risks before a condition is developed in their later years, saving lives and reducing healthcare costs.   We survey the literature on the techniques from the NLP field that have been developed to utilise digital health records to identify risks of familial diseases. We highlight that rule-based methods are heavily investigated and are still actively used for family history extraction. Still, more recent efforts have been put into building neural models based on 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;&#23884;&#20837;&#21512;&#25104;&#22120;&#65288;TES&#65289;&#65292;&#29992;&#20110;&#20026;&#26080;&#26631;&#31614;&#25968;&#25454;&#29983;&#25104;&#20266;&#25991;&#26412;&#23884;&#20837;&#65292;&#20197;&#35299;&#38145;CLIP&#29992;&#20110;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;&#20219;&#21153;&#20013;&#30340;&#22810;&#27169;&#24577;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.09974</link><description>&lt;p&gt;
GET&#65306;&#35299;&#38145;CLIP&#30340;&#22810;&#27169;&#24577;&#28508;&#21147;&#65292;&#29992;&#20110;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
GET: Unlocking the Multi-modal Potential of CLIP for Generalized Category Discovery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09974
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;&#23884;&#20837;&#21512;&#25104;&#22120;&#65288;TES&#65289;&#65292;&#29992;&#20110;&#20026;&#26080;&#26631;&#31614;&#25968;&#25454;&#29983;&#25104;&#20266;&#25991;&#26412;&#23884;&#20837;&#65292;&#20197;&#35299;&#38145;CLIP&#29992;&#20110;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;&#20219;&#21153;&#20013;&#30340;&#22810;&#27169;&#24577;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#21253;&#21547;&#26087;&#31867;&#21035;&#21644;&#26032;&#31867;&#21035;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#65292;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;&#65288;GCD&#65289;&#26088;&#22312;&#20934;&#30830;&#21457;&#29616;&#26032;&#31867;&#21035;&#65292;&#24182;&#27491;&#30830;&#20998;&#31867;&#26087;&#31867;&#21035;&#65292;&#21033;&#29992;&#20174;&#26377;&#26631;&#31614;&#26679;&#26412;&#20013;&#23398;&#20064;&#30340;&#31867;&#21035;&#27010;&#24565;&#12290;&#24403;&#21069;&#30340;GCD&#26041;&#27861;&#21482;&#20351;&#29992;&#21333;&#19968;&#30340;&#35270;&#35273;&#20449;&#24687;&#27169;&#24577;&#65292;&#23548;&#33268;&#22312;&#35270;&#35273;&#19978;&#30456;&#20284;&#31867;&#21035;&#30340;&#20998;&#31867;&#25928;&#26524;&#19981;&#20339;&#12290;&#34429;&#28982;&#26576;&#20123;&#31867;&#21035;&#22312;&#35270;&#35273;&#19978;&#23481;&#26131;&#28151;&#28102;&#65292;&#20294;&#23427;&#20204;&#30340;&#25991;&#26412;&#20449;&#24687;&#21487;&#33021;&#26159;&#19981;&#21516;&#30340;&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#23558;&#25991;&#26412;&#20449;&#24687;&#24341;&#20837;&#21040;GCD&#20219;&#21153;&#20013;&#12290;&#28982;&#32780;&#65292;&#26080;&#26631;&#31614;&#25968;&#25454;&#32570;&#20047;&#31867;&#21035;&#21517;&#31216;&#65292;&#20351;&#24471;&#21033;&#29992;&#25991;&#26412;&#20449;&#24687;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;&#23884;&#20837;&#21512;&#25104;&#22120;&#65288;TES&#65289;&#65292;&#29992;&#20110;&#20026;&#26080;&#26631;&#31614;&#26679;&#26412;&#29983;&#25104;&#20266;&#25991;&#26412;&#23884;&#20837;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;TES&#21033;&#29992;CLIP&#21487;&#20197;&#29983;&#25104;&#23545;&#40784;&#30340;&#35270;&#35273;-&#35821;&#35328;&#29305;&#24449;&#36825;&#19968;&#29305;&#24615;&#65292;&#23558;&#35270;&#35273;&#23884;&#20837;&#36716;&#25442;&#20026;CLIP&#25991;&#26412;&#27169;&#22411;&#30340;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09974v1 Announce Type: cross  Abstract: Given unlabelled datasets containing both old and new categories, generalized category discovery (GCD) aims to accurately discover new classes while correctly classifying old classes, leveraging the class concepts learned from labeled samples. Current GCD methods only use a single visual modality of information, resulting in poor classification of visually similar classes. Though certain classes are visually confused, their text information might be distinct, motivating us to introduce text information into the GCD task. However, the lack of class names for unlabelled data makes it impractical to utilize text information. To tackle this challenging problem, in this paper, we propose a Text Embedding Synthesizer (TES) to generate pseudo text embeddings for unlabelled samples. Specifically, our TES leverages the property that CLIP can generate aligned vision-language features, converting visual embeddings into tokens of the CLIP's text e
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32622;&#20449;&#24230;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#24605;&#21644;&#25552;&#20379;&#22810;&#20010;&#20505;&#36873;&#31572;&#26696;&#30340;&#29702;&#30001;&#26469;&#35299;&#20915;&#23545;&#19981;&#27491;&#30830;&#31572;&#26696;&#30340;&#36807;&#24230;&#33258;&#20449;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.09972</link><description>&lt;p&gt;
&#22312;&#25215;&#35834;&#20043;&#21069;&#19977;&#24605;&#65306;&#36890;&#36807;&#21453;&#24605;&#22810;&#20010;&#31572;&#26696;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;
&lt;/p&gt;
&lt;p&gt;
Think Twice Before Assure: Confidence Estimation for Large Language Models through Reflection on Multiple Answers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09972
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32622;&#20449;&#24230;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#24605;&#21644;&#25552;&#20379;&#22810;&#20010;&#20505;&#36873;&#31572;&#26696;&#30340;&#29702;&#30001;&#26469;&#35299;&#20915;&#23545;&#19981;&#27491;&#30830;&#31572;&#26696;&#30340;&#36807;&#24230;&#33258;&#20449;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32622;&#20449;&#24230;&#20272;&#35745;&#26088;&#22312;&#35780;&#20272;&#36755;&#20986;&#30340;&#21487;&#20449;&#24230;&#65292;&#22312;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26102;&#33267;&#20851;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#40657;&#30418;&#27169;&#22411;&#12290;&#30001;&#20110;LLM&#22312;&#29983;&#25104;&#19981;&#27491;&#30830;&#31572;&#26696;&#26102;&#30340;&#36807;&#24230;&#33258;&#20449;&#65292;&#29616;&#26377;&#23545;LLM&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#36890;&#24120;&#19981;&#21487;&#26657;&#20934;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#21463;&#21040;&#19968;&#20010;&#26174;&#33879;&#38480;&#21046;&#30340;&#38459;&#30861;&#65292;&#21363;&#23427;&#20204;&#20165;&#32771;&#34385;LLM&#29983;&#25104;&#30340;&#19968;&#20010;&#31572;&#26696;&#30340;&#32622;&#20449;&#24230;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#33539;&#24335;&#65292;&#24443;&#24213;&#35780;&#20272;&#22810;&#20010;&#20505;&#36873;&#31572;&#26696;&#30340;&#21487;&#20449;&#24230;&#65292;&#20197;&#20943;&#36731;&#23545;&#19981;&#27491;&#30830;&#31572;&#26696;&#30340;&#36807;&#24230;&#33258;&#20449;&#12290;&#22522;&#20110;&#36825;&#19968;&#33539;&#24335;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20004;&#27493;&#26694;&#26550;&#65292;&#39318;&#20808;&#25351;&#23548;LLM&#21453;&#24605;&#24182;&#20026;&#27599;&#20010;&#31572;&#26696;&#25552;&#20379;&#29702;&#30001;&#65292;&#28982;&#21518;&#27719;&#24635;&#36825;&#20123;&#29702;&#30001;&#36827;&#34892;&#32508;&#21512;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#12290;&#36825;&#19968;&#26694;&#26550;&#21487;&#20197;&#19982;&#29616;&#26377;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#26041;&#27861;&#30456;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09972v1 Announce Type: new  Abstract: Confidence estimation aiming to evaluate output trustability is crucial for the application of large language models (LLM), especially the black-box ones. Existing confidence estimation of LLM is typically not calibrated due to the overconfidence of LLM on its generated incorrect answers. Existing approaches addressing the overconfidence issue are hindered by a significant limitation that they merely consider the confidence of one answer generated by LLM. To tackle this limitation, we propose a novel paradigm that thoroughly evaluates the trustability of multiple candidate answers to mitigate the overconfidence on incorrect answers. Building upon this paradigm, we introduce a two-step framework, which firstly instructs LLM to reflect and provide justifications for each answer, and then aggregates the justifications for comprehensive confidence estimation. This framework can be integrated with existing confidence estimation approaches for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#30693;&#35782;&#25552;&#21462;&#20013;&#23384;&#22312;&#30340;&#8220;&#25552;&#31034;&#20559;&#35265;&#8221;&#65292;&#25214;&#21040;&#20102;&#19981;&#21516;&#31867;&#22411;&#25552;&#31034;&#30340;&#20559;&#35265;&#31243;&#24230;&#65292;&#20197;&#21450;&#36825;&#31181;&#20559;&#35265;&#23545;&#19981;&#21516;&#22522;&#20934;&#27979;&#35797;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34920;&#31034;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#31181;&#25552;&#31034;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2403.09963</link><description>&lt;p&gt;
&#22788;&#29702;&#22909;&#24744;&#30340;&#25552;&#31034;&#20559;&#35265;&#65281;&#35843;&#26597;&#21644;&#20943;&#36731;&#20107;&#23454;&#30693;&#35782;&#25552;&#21462;&#20013;&#30340;&#25552;&#31034;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Take Care of Your Prompt Bias! Investigating and Mitigating Prompt Bias in Factual Knowledge Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#30693;&#35782;&#25552;&#21462;&#20013;&#23384;&#22312;&#30340;&#8220;&#25552;&#31034;&#20559;&#35265;&#8221;&#65292;&#25214;&#21040;&#20102;&#19981;&#21516;&#31867;&#22411;&#25552;&#31034;&#30340;&#20559;&#35265;&#31243;&#24230;&#65292;&#20197;&#21450;&#36825;&#31181;&#20559;&#35265;&#23545;&#19981;&#21516;&#22522;&#20934;&#27979;&#35797;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34920;&#31034;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#31181;&#25552;&#31034;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#20107;&#23454;&#30693;&#35782;&#25552;&#21462;&#20013;&#23384;&#22312;&#8220;&#25552;&#31034;&#20559;&#35265;&#8221;&#65292;&#21363;&#25552;&#31034;&#24448;&#24448;&#20250;&#24341;&#20837;&#23545;&#29305;&#23450;&#26631;&#31614;&#30340;&#20559;&#35265;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#20869;&#37096;&#25552;&#31034;&#20559;&#35265;&#30340;&#31243;&#24230;&#21644;&#24433;&#21709;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#20026;&#20102;&#22238;&#24212;&#36825;&#19968;&#28857;&#65292;&#26412;&#25991;&#37327;&#21270;&#20102;&#19981;&#21516;&#31867;&#22411;&#25552;&#31034;&#30340;&#20559;&#35265;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#23545;&#19981;&#21516;&#22522;&#20934;&#27979;&#35797;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65306;1&#65289;&#23454;&#39564;&#20013;&#30340;&#25152;&#26377;&#25552;&#31034;&#37117;&#34920;&#29616;&#20986;&#19981;&#21487;&#24573;&#35270;&#30340;&#20559;&#35265;&#65292;&#22522;&#20110;&#26799;&#24230;&#30340;&#25552;&#31034;&#22914;AutoPrompt&#21644;OptiPrompt&#26174;&#31034;&#20986;&#26356;&#39640;&#27700;&#24179;&#30340;&#20559;&#35265;&#65307;2&#65289;&#25552;&#31034;&#20559;&#35265;&#21487;&#20197;&#36890;&#36807;&#36807;&#24230;&#25311;&#21512;&#27979;&#35797;&#25968;&#25454;&#38598;&#19981;&#21512;&#29702;&#22320;&#25918;&#22823;&#22522;&#20934;&#27979;&#35797;&#30340;&#20934;&#30830;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#31867;&#20284;LAMA&#36825;&#26679;&#30340;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19978;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34920;&#31034;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#25552;&#31034;&#20559;&#35265;&#65292;&#22312;&#25512;&#26029;&#26102;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#20165;&#25552;&#31034;&#26597;&#35810;&#26469;&#20272;&#35745;&#26377;&#20559;&#24046;&#30340;&#34920;&#31034;&#65292;&#28982;&#21518;&#20174;&#20013;&#21024;&#38500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09963v1 Announce Type: cross  Abstract: Recent research shows that pre-trained language models (PLMs) suffer from "prompt bias" in factual knowledge extraction, i.e., prompts tend to introduce biases toward specific labels. However, the extent and impact of prompt bias within the model remain underexplored. In response, this paper quantifies the bias with various types of prompts and assesses their impact on different benchmarks. We show that: 1) all prompts in the experiments exhibit non-negligible bias, with gradient-based prompts like AutoPrompt and OptiPrompt displaying significantly higher levels of bias; 2) prompt bias can amplify benchmark accuracy unreasonably by overfitting the test datasets, especially on imbalanced datasets like LAMA. Based on these findings, we propose a representation-based approach to mitigate the prompt bias during inference time. Specifically, we first estimate the biased representation using prompt-only querying, and then remove it from the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24490;&#29615;&#33609;&#31295;&#26426;&#21046;&#65292;&#32467;&#21512;&#20102;&#32463;&#20856;&#21452;&#27169;&#22411;&#21644;&#26368;&#26032;&#21333;&#27169;&#22411;&#26041;&#27861;&#65292;&#36890;&#36807;&#36816;&#29992;&#24490;&#29615;&#20381;&#36182;&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25512;&#27979;&#35299;&#30721;&#12290;</title><link>https://arxiv.org/abs/2403.09919</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#29992;&#20110;&#24555;&#36895;&#25512;&#27979;&#35299;&#30721;&#30340;&#24490;&#29615;&#33609;&#31295;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Recurrent Drafter for Fast Speculative Decoding in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09919
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24490;&#29615;&#33609;&#31295;&#26426;&#21046;&#65292;&#32467;&#21512;&#20102;&#32463;&#20856;&#21452;&#27169;&#22411;&#21644;&#26368;&#26032;&#21333;&#27169;&#22411;&#26041;&#27861;&#65292;&#36890;&#36807;&#36816;&#29992;&#24490;&#29615;&#20381;&#36182;&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25512;&#27979;&#35299;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#19968;&#31181;&#25913;&#36827;&#30340;&#25512;&#27979;&#35299;&#30721;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#20004;&#31181;&#25104;&#29087;&#25216;&#26415;&#30340;&#20248;&#21183;&#65306;&#32463;&#20856;&#30340;&#21452;&#27169;&#22411;&#25512;&#27979;&#35299;&#30721;&#26041;&#27861;&#21644;&#36739;&#26032;&#30340;&#21333;&#27169;&#22411;&#26041;&#27861;Medusa&#12290;&#20174;Medusa&#24471;&#21040;&#28789;&#24863;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#21333;&#27169;&#22411;&#31574;&#30053;&#36827;&#34892;&#25512;&#27979;&#35299;&#30721;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#24490;&#29615;&#20381;&#36182;&#35774;&#35745;&#30340;&#21333;&#20010;&#36731;&#37327;&#32423;&#33609;&#31295;&#22836;&#26469;&#21306;&#20998;&#33258;&#24049;&#65292;&#26412;&#36136;&#19978;&#31867;&#20284;&#20110;&#32463;&#20856;&#25512;&#27979;&#35299;&#30721;&#20013;&#20351;&#29992;&#30340;&#23567;&#22411;&#33609;&#31295;&#27169;&#22411;&#65292;&#20294;&#36991;&#20813;&#20102;&#23436;&#25972;transformer&#26550;&#26500;&#30340;&#22797;&#26434;&#24615;&#12290;&#30001;&#20110;&#24490;&#29615;&#20381;&#36182;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#27874;&#26463;&#25628;&#32034;&#24555;&#36895;&#36807;&#28388;&#20986;&#33609;&#31295;&#22836;&#20013;&#19981;&#38656;&#35201;&#30340;&#20505;&#36873;&#39033;&#12290;&#20854;&#32467;&#26524;&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#21333;&#27169;&#22411;&#35774;&#35745;&#31616;&#26131;&#24615;&#24182;&#36991;&#20813;&#20102;&#21019;&#24314;&#25968;&#25454;&#30456;&#20851;&#26641;&#20381;&#36182;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09919v1 Announce Type: new  Abstract: In this paper, we introduce an improved approach of speculative decoding aimed at enhancing the efficiency of serving large language models. Our method capitalizes on the strengths of two established techniques: the classic two-model speculative decoding approach, and the more recent single-model approach, Medusa. Drawing inspiration from Medusa, our approach adopts a single-model strategy for speculative decoding. However, our method distinguishes itself by employing a single, lightweight draft head with a recurrent dependency design, akin in essence to the small, draft model uses in classic speculative decoding, but without the complexities of the full transformer architecture. And because of the recurrent dependency, we can use beam search to swiftly filter out undesired candidates with the draft head. The outcome is a method that combines the simplicity of single-model design and avoids the need to create a data-dependent tree attent
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22320;&#29702;&#20449;&#24687;&#35821;&#35328;&#35782;&#21035;&#26041;&#27861;&#65292;&#22312;&#22320;&#29702;&#21407;&#28857;&#30340;&#22522;&#30784;&#19978;&#35774;&#32622;&#20102;16&#20010;&#21306;&#22495;&#24615;&#27169;&#22411;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35821;&#35328;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#24182;&#23545;&#23454;&#38469;&#25968;&#25454;&#25991;&#38598;&#20135;&#29983;&#20102;&#26174;&#33879;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.09892</link><description>&lt;p&gt;
&#22320;&#29702;&#20449;&#24687;&#35821;&#35328;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Geographically-Informed Language Identification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09892
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22320;&#29702;&#20449;&#24687;&#35821;&#35328;&#35782;&#21035;&#26041;&#27861;&#65292;&#22312;&#22320;&#29702;&#21407;&#28857;&#30340;&#22522;&#30784;&#19978;&#35774;&#32622;&#20102;16&#20010;&#21306;&#22495;&#24615;&#27169;&#22411;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35821;&#35328;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#24182;&#23545;&#23454;&#38469;&#25968;&#25454;&#25991;&#38598;&#20135;&#29983;&#20102;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#35328;&#35782;&#21035;&#26041;&#27861;&#65292;&#20854;&#20013;&#27169;&#22411;&#32771;&#34385;&#30340;&#35821;&#35328;&#38598;&#21462;&#20915;&#20110;&#25991;&#26412;&#30340;&#22320;&#29702;&#26469;&#28304;&#12290;&#37492;&#20110;&#35768;&#22810;&#25968;&#23383;&#25991;&#38598;&#21487;&#20197;&#22312;&#22269;&#23478;&#32423;&#21035;&#36827;&#34892;&#22320;&#29702;&#26631;&#27880;&#65292;&#26412;&#25991;&#21046;&#23450;&#20102;16&#20010;&#21306;&#22495;&#29305;&#23450;&#27169;&#22411;&#65292;&#27599;&#20010;&#27169;&#22411;&#21253;&#21547;&#22312;&#35813;&#21306;&#22495;&#20869;&#22269;&#23478;&#20013;&#20986;&#29616;&#30340;&#39044;&#26399;&#35821;&#35328;&#12290;&#36825;&#20123;&#21306;&#22495;&#27169;&#22411;&#36824;&#21253;&#25324;31&#31181;&#20351;&#29992;&#24191;&#27867;&#30340;&#22269;&#38469;&#35821;&#35328;&#65292;&#20197;&#30830;&#20445;&#28085;&#30422;&#36825;&#20123;&#36890;&#29992;&#35821;&#26080;&#35770;&#20301;&#32622;&#22914;&#20309;&#12290;&#20256;&#32479;&#35821;&#35328;&#35782;&#21035;&#27979;&#35797;&#25968;&#25454;&#30340;&#19978;&#28216;&#35780;&#20272;&#26174;&#31034;&#65292;F&#20998;&#25968;&#30340;&#25552;&#39640;&#33539;&#22260;&#20174;1.7&#28857;&#65288;&#19996;&#21335;&#20122;&#65289;&#21040;&#39640;&#36798;10.4&#28857;&#65288;&#21271;&#38750;&#65289;&#12290;&#23545;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#36827;&#34892;&#30340;&#19979;&#28216;&#35780;&#20272;&#26174;&#31034;&#65292;&#36825;&#31181;&#25913;&#36827;&#30340;&#24615;&#33021;&#23545;&#24212;&#29992;&#20110;&#22823;&#22411;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#25991;&#38598;&#30340;&#35821;&#35328;&#26631;&#31614;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#32467;&#26524;&#26159;&#19968;&#20010;&#39640;&#24230;&#20934;&#30830;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09892v1 Announce Type: new  Abstract: This paper develops an approach to language identification in which the set of languages considered by the model depends on the geographic origin of the text in question. Given that many digital corpora can be geo-referenced at the country level, this paper formulates 16 region-specific models, each of which contains the languages expected to appear in countries within that region. These regional models also each include 31 widely-spoken international languages in order to ensure coverage of these linguae francae regardless of location. An upstream evaluation using traditional language identification testing data shows an improvement in f-score ranging from 1.7 points (Southeast Asia) to as much as 10.4 points (North Africa). A downstream evaluation on social media data shows that this improved performance has a significant impact on the language labels which are applied to large real-world corpora. The result is a highly-accurate model 
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;Transformers&#30340;&#26032;&#22411;&#27169;&#22411;&#21512;&#24182;&#26041;&#27861;&#65292;&#21033;&#29992;Fisher&#20449;&#24687;&#36827;&#34892;&#21152;&#26435;&#24179;&#22343;&#65292;&#25552;&#39640;&#20102;&#22810;&#20219;&#21153;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.09891</link><description>&lt;p&gt;
Fisher Mask&#33410;&#28857;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#21512;&#24182;
&lt;/p&gt;
&lt;p&gt;
Fisher Mask Nodes for Language Model Merging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09891
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;Transformers&#30340;&#26032;&#22411;&#27169;&#22411;&#21512;&#24182;&#26041;&#27861;&#65292;&#21033;&#29992;Fisher&#20449;&#24687;&#36827;&#34892;&#21152;&#26435;&#24179;&#22343;&#65292;&#25552;&#39640;&#20102;&#22810;&#20219;&#21153;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#19979;&#28216;&#24615;&#33021;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;&#22914;BERT&#21450;&#20854;&#34893;&#29983;&#29289;&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#26222;&#36941;&#24615;&#20063;&#23548;&#33268;&#20102;&#20219;&#21153;&#29305;&#23450;&#24494;&#35843;&#27169;&#22411;&#30340;&#28608;&#22686;&#12290;&#22312;&#22810;&#20219;&#21153;&#22330;&#26223;&#20013;&#65292;&#30001;&#20110;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#21482;&#33021;&#24456;&#22909;&#22320;&#25191;&#34892;&#19968;&#39033;&#20219;&#21153;&#65292;&#22240;&#27492;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#25110;&#38598;&#25104;&#12290;&#27169;&#22411;&#21512;&#24182;&#36825;&#19968;&#19981;&#26029;&#22686;&#38271;&#30340;&#39046;&#22495;&#25552;&#20379;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;&#23558;&#22810;&#20010;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#21512;&#24182;&#20026;&#21333;&#20010;&#22810;&#20219;&#21153;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;Transformers&#30340;&#27169;&#22411;&#21512;&#24182;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#20808;&#21069;Fisher&#21152;&#26435;&#24179;&#22343;&#21644;Fisher&#20449;&#24687;&#22312;&#27169;&#22411;&#20462;&#21098;&#20013;&#30340;&#24212;&#29992;&#30340;&#35265;&#35299;&#12290;&#36890;&#36807;&#21033;&#29992;Transformer&#26550;&#26500;&#20869;&#30340;mask&#33410;&#28857;&#30340;Fisher&#20449;&#24687;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#21152;&#26435;&#24179;&#22343;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#29616;&#20986;&#20102;&#31283;&#23450;&#19988;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09891v1 Announce Type: cross  Abstract: Fine-tuning pre-trained models provides significant advantages in downstream performance. The ubiquitous nature of pre-trained models such as BERT and its derivatives in natural language processing has also led to a proliferation of task-specific fine-tuned models. As these models typically only perform one task well, additional training or ensembling is required in multi-task scenarios. The growing field of model merging provides a solution, dealing with the challenge of combining multiple task-specific models into a single multi-task model. In this study, we introduce a novel model merging method for Transformers, combining insights from previous work in Fisher-weighted averaging and the use of Fisher information in model pruning. Utilizing the Fisher information of mask nodes within the Transformer architecture, we devise a computationally efficient weighted-averaging scheme. Our method exhibits a regular and significant performance
&lt;/p&gt;</description></item><item><title>Sabi'a-2&#26159;&#19968;&#20195;&#26032;&#30340;&#33889;&#33796;&#29273;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#20013;&#30340;Sabi'a-2 Medium&#27169;&#22411;&#22312;&#22810;&#20010;&#32771;&#35797;&#20013;&#30340;&#34920;&#29616;&#36229;&#36234;&#20102;GPT-4&#65292;&#19988;&#22312;&#22823;&#22810;&#25968;&#32771;&#35797;&#20013;&#36229;&#36807;&#20102;GPT-3.5&#65292;&#21516;&#26102;&#19987;&#19994;&#21270;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#26377;&#26174;&#33879;&#24433;&#21709;&#65292;&#21487;&#22312;&#26080;&#38656;&#22686;&#22823;&#27169;&#22411;&#23610;&#23544;&#30340;&#24773;&#20917;&#19979;&#20197;&#27604;GPT-4&#20415;&#23452;10&#20493;&#30340;&#20215;&#26684;&#25552;&#20379;&#12290;</title><link>https://arxiv.org/abs/2403.09887</link><description>&lt;p&gt;
Sabi\'a-2:&#19968;&#20195;&#26032;&#30340;&#33889;&#33796;&#29273;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Sabi\'a-2: A New Generation of Portuguese Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09887
&lt;/p&gt;
&lt;p&gt;
Sabi'a-2&#26159;&#19968;&#20195;&#26032;&#30340;&#33889;&#33796;&#29273;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#20013;&#30340;Sabi'a-2 Medium&#27169;&#22411;&#22312;&#22810;&#20010;&#32771;&#35797;&#20013;&#30340;&#34920;&#29616;&#36229;&#36234;&#20102;GPT-4&#65292;&#19988;&#22312;&#22823;&#22810;&#25968;&#32771;&#35797;&#20013;&#36229;&#36807;&#20102;GPT-3.5&#65292;&#21516;&#26102;&#19987;&#19994;&#21270;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#26377;&#26174;&#33879;&#24433;&#21709;&#65292;&#21487;&#22312;&#26080;&#38656;&#22686;&#22823;&#27169;&#22411;&#23610;&#23544;&#30340;&#24773;&#20917;&#19979;&#20197;&#27604;GPT-4&#20415;&#23452;10&#20493;&#30340;&#20215;&#26684;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Sabi'a-2&#65292;&#36825;&#26159;&#19968;&#26063;&#22312;&#33889;&#33796;&#29273;&#25991;&#26412;&#19978;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#22810;&#20010;&#32771;&#35797;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;&#24052;&#35199;&#22823;&#23398;&#30340;&#20837;&#23398;&#32771;&#35797;&#12289;&#19987;&#19994;&#35748;&#35777;&#32771;&#35797;&#20197;&#21450;&#21508;&#31181;&#23398;&#31185;&#65288;&#22914;&#20250;&#35745;&#12289;&#32463;&#27982;&#23398;&#12289;&#24037;&#31243;&#23398;&#12289;&#27861;&#24459;&#21644;&#21307;&#23398;&#65289;&#30340;&#30740;&#31350;&#29983;&#20837;&#23398;&#32771;&#35797;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#25105;&#20204;&#26368;&#20248;&#31168;&#30340;&#27169;&#22411;Sabi'a-2 Medium&#65292;&#22312;64&#22330;&#32771;&#35797;&#20013;&#26377;23&#22330;&#19982;GPT-4&#30340;&#34920;&#29616;&#30456;&#21305;&#25932;&#25110;&#36229;&#36234;&#65292;&#24182;&#19988;&#22312;64&#22330;&#32771;&#35797;&#20013;&#26377;58&#22330;&#36229;&#36807;&#20102;GPT-3.5&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#19987;&#19994;&#21270;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#26377;&#26174;&#33879;&#24433;&#21709;&#65292;&#26080;&#38656;&#22686;&#22823;&#27169;&#22411;&#23610;&#23544;&#65292;&#25105;&#20204;&#21487;&#20197;&#25552;&#20379;Sabi'a-2 Medium&#65292;&#27599;&#20010;&#35760;&#21495;&#30340;&#20215;&#26684;&#27604;GPT-4&#20415;&#23452;10&#20493;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;&#25968;&#23398;&#21644;&#32534;&#30721;&#26159;&#38656;&#35201;&#25913;&#36827;&#30340;&#20851;&#38190;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09887v1 Announce Type: cross  Abstract: We introduce Sabi\'a-2, a family of large language models trained on Portuguese texts. The models are evaluated on a diverse range of exams, including entry-level tests for Brazilian universities, professional certification exams, and graduate-level exams for various disciplines such as accounting, economics, engineering, law and medicine. Our results reveal that our best model so far, Sabi\'a-2 Medium, matches or surpasses GPT-4's performance in 23 out of 64 exams and outperforms GPT-3.5 in 58 out of 64 exams. Notably, specialization has a significant impact on a model's performance without the need to increase its size, allowing us to offer Sabi\'a-2 Medium at a price per token that is 10 times cheaper than GPT-4. Finally, we identified that math and coding are key abilities that need improvement.
&lt;/p&gt;</description></item><item><title>FakeWatch&#26694;&#26550;&#26159;&#20026;&#20102;&#26816;&#27979;&#20551;&#26032;&#38395;&#32780;&#35774;&#35745;&#30340;&#65292;&#25972;&#21512;&#20102;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21644;&#21069;&#27839;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#21271;&#32654;&#36873;&#20030;&#30456;&#20851;&#26032;&#38395;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09858</link><description>&lt;p&gt;
FakeWatch&#65306;&#19968;&#20010;&#29992;&#20110;&#26816;&#27979;&#20551;&#26032;&#38395;&#20197;&#30830;&#20445;&#36873;&#20030;&#21487;&#20449;&#24615;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FakeWatch: A Framework for Detecting Fake News to Ensure Credible Elections
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09858
&lt;/p&gt;
&lt;p&gt;
FakeWatch&#26694;&#26550;&#26159;&#20026;&#20102;&#26816;&#27979;&#20551;&#26032;&#38395;&#32780;&#35774;&#35745;&#30340;&#65292;&#25972;&#21512;&#20102;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21644;&#21069;&#27839;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#21271;&#32654;&#36873;&#20030;&#30456;&#20851;&#26032;&#38395;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#25216;&#26415;&#39537;&#21160;&#30340;&#19990;&#30028;&#20013;&#65292;&#20551;&#26032;&#38395;&#30340;&#36805;&#36895;&#20256;&#25773;&#65292;&#29305;&#21035;&#26159;&#22312;&#36873;&#20030;&#31561;&#37325;&#35201;&#20107;&#20214;&#26399;&#38388;&#65292;&#23545;&#20449;&#24687;&#30340;&#23436;&#25972;&#24615;&#26500;&#25104;&#20102;&#36234;&#26469;&#36234;&#22823;&#30340;&#23041;&#32961;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;FakeWatch&#65292;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#29992;&#20110;&#26816;&#27979;&#20551;&#26032;&#38395;&#30340;&#20840;&#38754;&#26694;&#26550;&#12290;&#21033;&#29992;&#26032;&#31574;&#21010;&#30340;&#21271;&#32654;&#36873;&#20030;&#30456;&#20851;&#26032;&#38395;&#25991;&#31456;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#24378;&#22823;&#30340;&#20998;&#31867;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#25972;&#21512;&#20102;&#19968;&#20010;&#27169;&#22411;&#20013;&#24515;&#65292;&#21253;&#25324;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25216;&#26415;&#21644;&#23574;&#31471;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#65292;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#20551;&#26032;&#38395;&#12290;&#25105;&#20204;&#30340;&#24635;&#20307;&#30446;&#26631;&#26159;&#20026;&#30740;&#31350;&#30028;&#25552;&#20379;&#36866;&#24212;&#24615;&#21644;&#31934;&#20934;&#30340;&#20998;&#31867;&#27169;&#22411;&#65292;&#33021;&#22815;&#35782;&#21035;&#19981;&#26029;&#28436;&#21464;&#30340;&#35823;&#20449;&#24687;&#26684;&#23616;&#12290;&#23545;&#25105;&#20204;&#25968;&#25454;&#38598;&#19978;&#20551;&#26032;&#38395;&#20998;&#31867;&#22120;&#36827;&#34892;&#30340;&#23450;&#37327;&#35780;&#20272;&#26174;&#31034;&#65292;&#23613;&#31649;&#26368;&#20808;&#36827;&#30340;LMs&#31245;&#24494;&#39046;&#20808;&#20256;&#32479;ML&#27169;&#22411;&#65292;&#20294;&#26159;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09858v1 Announce Type: new  Abstract: In today's technologically driven world, the rapid spread of fake news, particularly during critical events like elections, poses a growing threat to the integrity of information. To tackle this challenge head-on, we introduce FakeWatch, a comprehensive framework carefully designed to detect fake news. Leveraging a newly curated dataset of North American election-related news articles, we construct robust classification models. Our framework integrates a model hub comprising of both traditional machine learning (ML) techniques and cutting-edge Language Models (LMs) to discern fake news effectively. Our overarching objective is to provide the research community with adaptable and precise classification models adept at identifying the ever-evolving landscape of misinformation. Quantitative evaluations of fake news classifiers on our dataset reveal that, while state-of-the-art LMs exhibit a slight edge over traditional ML models, classical 
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#33258;&#27965;&#24615;&#30340;&#26657;&#20934;&#26041;&#27861;&#22312;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#20013;&#33021;&#22815;&#26356;&#22909;&#22320;&#24314;&#31435;&#27169;&#22411;&#20449;&#24515;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;</title><link>https://arxiv.org/abs/2403.09849</link><description>&lt;p&gt;
&#33258;&#27965;&#24615;&#25552;&#21319;&#25968;&#23398;&#25512;&#29702;&#30340;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Self-Consistency Boosts Calibration for Math Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09849
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33258;&#27965;&#24615;&#30340;&#26657;&#20934;&#26041;&#27861;&#22312;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#20013;&#33021;&#22815;&#26356;&#22909;&#22320;&#24314;&#31435;&#27169;&#22411;&#20449;&#24515;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26657;&#20934;&#65292;&#24314;&#31435;&#20934;&#30830;&#24615;&#21644;&#27169;&#22411;&#20449;&#24515;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#23545;LLM&#24320;&#21457;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#22522;&#20110;&#33258;&#27965;&#24615;&#35774;&#35745;&#20102;&#19977;&#31181;&#21363;&#25554;&#21363;&#29992;&#30340;&#26657;&#20934;&#26041;&#27861;&#65288;Wang&#31561;&#65292;2022&#24180;&#65289;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#12290;&#22312;&#20004;&#20010;&#27969;&#34892;&#22522;&#20934;&#65288;GSM8K&#21644;MathQA&#65289;&#19978;&#35780;&#20272;&#20351;&#29992;&#24378;&#22823;&#30340;&#24320;&#28304;LLMs&#65288;Mistral&#21644;LLaMA2&#65289;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#22522;&#20110;p(True)&#30340;&#29616;&#26377;&#26041;&#27861;&#65288;Kadavath&#31561;&#20154;&#65292;2022&#24180;&#65289;&#25110;logit&#65288;Kadavath&#31561;&#20154;&#65292;2022&#24180;&#65289;&#26356;&#22909;&#22320;&#36830;&#25509;&#27169;&#22411;&#20449;&#24515;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09849v1 Announce Type: cross  Abstract: Calibration, which establishes the correlation between accuracy and model confidence, is important for LLM development. We design three off-the-shelf calibration methods based on self-consistency (Wang et al., 2022) for math reasoning tasks. Evaluation on two popular benchmarks (GSM8K and MathQA) using strong open-source LLMs (Mistral and LLaMA2), our methods better bridge model confidence and accuracy than existing methods based on p(True) (Kadavath et al., 2022) or logit (Kadavath et al., 2022).
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#19978;&#23545;&#22810;&#20010;&#23478;&#26063;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#65292;&#26356;&#22823;&#30340;&#27169;&#22411;&#21487;&#33021;&#26356;&#23481;&#26131;&#21463;&#21040;&#25104;&#21151;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2403.09832</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#19979;&#36827;&#34892;&#30340;&#26426;&#22120;&#32763;&#35793;&#30340;&#35268;&#27169;&#34892;&#20026;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09832
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#19978;&#23545;&#22810;&#20010;&#23478;&#26063;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#65292;&#26356;&#22823;&#30340;&#27169;&#22411;&#21487;&#33021;&#26356;&#23481;&#26131;&#21463;&#21040;&#25104;&#21151;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36234;&#26469;&#36234;&#25104;&#20026;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#39318;&#36873;&#24179;&#21488;&#65292;&#22914;&#26426;&#22120;&#32763;&#35793;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#36136;&#37327;&#24448;&#24448;&#19982;&#29305;&#23450;&#20219;&#21153;&#27169;&#22411;&#30456;&#27604;&#20855;&#26377;&#21487;&#27604;&#24615;&#25110;&#26356;&#22909;&#65292;&#24182;&#19988;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#25110;&#19978;&#19979;&#25991;&#31034;&#20363;&#25351;&#23450;&#20219;&#21153;&#30340;&#31616;&#21333;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#26222;&#36941;&#24615;&#20351;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#26368;&#32456;&#29992;&#25143;&#30340;&#39072;&#35206;&#65292;&#21518;&#32773;&#21487;&#33021;&#23558;&#23548;&#33268;&#27169;&#22411;&#20197;&#26410;&#32463;&#25480;&#26435;&#19988;&#21487;&#33021;&#19981;&#23433;&#20840;&#30340;&#26041;&#24335;&#34892;&#20026;&#30340;&#25351;&#20196;&#23884;&#20837;&#21040;&#20182;&#20204;&#30340;&#35831;&#27714;&#20013;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#19978;&#23545;&#22810;&#20010;&#23478;&#26063;&#30340;LLMs&#36827;&#34892;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;(PIAs)&#65292;&#37325;&#28857;&#20851;&#27880;&#27169;&#22411;&#22823;&#23567;&#23545;&#25915;&#20987;&#25104;&#21151;&#29575;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#21457;&#29616;&#22312;&#22810;&#31181;&#35821;&#35328;&#23545;&#21644;&#33521;&#25991;&#27880;&#20837;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#26356;&#22823;&#30340;&#27169;&#22411;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#21487;&#33021;&#26356;&#23481;&#26131;&#21463;&#21040;&#25104;&#21151;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#36825;&#26159;&#19968;&#31181;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09832v1 Announce Type: new  Abstract: Large Language Models (LLMs) are increasingly becoming the preferred foundation platforms for many Natural Language Processing tasks such as Machine Translation, owing to their quality often comparable to or better than task-specific models, and the simplicity of specifying the task through natural language instructions or in-context examples. Their generality, however, opens them up to subversion by end users who may embed into their requests instructions that cause the model to behave in unauthorized and possibly unsafe ways. In this work we study these Prompt Injection Attacks (PIAs) on multiple families of LLMs on a Machine Translation task, focusing on the effects of model size on the attack success rates. We introduce a new benchmark data set and we discover that on multiple language pairs and injected prompts written in English, larger models under certain conditions may become more susceptible to successful attacks, an instance o
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22312;&#32447;&#35825;&#25296;&#39044;&#38450;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#21457;&#29616;&#27809;&#26377;&#27169;&#22411;&#26126;&#26174;&#36866;&#29992;&#20110;&#27492;&#30446;&#30340;&#65292;&#23384;&#22312;&#28508;&#22312;&#30340;&#26377;&#23475;&#31572;&#26696;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2403.09795</link><description>&lt;p&gt;
&#25506;&#35752;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22312;&#32447;&#35825;&#25296;&#39044;&#38450;&#20013;&#30340;&#25928;&#21147;&#65306;&#26377;&#30410;&#36824;&#26159;&#26377;&#23475;&#65311;
&lt;/p&gt;
&lt;p&gt;
Helpful or Harmful? Exploring the Efficacy of Large Language Models for Online Grooming Prevention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09795
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22312;&#32447;&#35825;&#25296;&#39044;&#38450;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#21457;&#29616;&#27809;&#26377;&#27169;&#22411;&#26126;&#26174;&#36866;&#29992;&#20110;&#27492;&#30446;&#30340;&#65292;&#23384;&#22312;&#28508;&#22312;&#30340;&#26377;&#23475;&#31572;&#26696;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#22823;&#30340;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#38382;&#31572;&#31995;&#32479;&#27491;&#22312;&#25104;&#20026;&#26222;&#36890;&#22823;&#20247;&#21644;&#33030;&#24369;&#32676;&#20307;&#65288;&#22914;&#20799;&#31461;&#65289;&#20043;&#38388;&#27969;&#34892;&#30340;&#24037;&#20855;&#12290;&#38543;&#30528;&#20799;&#31461;&#19982;&#36825;&#20123;&#24037;&#20855;&#30340;&#20114;&#21160;&#26085;&#30410;&#22686;&#22810;&#65292;&#23545;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#65292;&#23457;&#35270;LLMs&#30340;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#23545;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#21518;&#26524;&#30340;&#24212;&#29992;&#65292;&#27604;&#22914;&#22312;&#32447;&#20799;&#31461;&#23433;&#20840;&#26597;&#35810;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;LLMs&#22312;&#22312;&#32447;&#35825;&#25296;&#39044;&#38450;&#20013;&#30340;&#25928;&#21147;&#65292;&#26082;&#21253;&#25324;&#36890;&#36807;&#24314;&#35758;&#29983;&#25104;&#26469;&#35782;&#21035;&#21644;&#36991;&#20813;&#35825;&#25296;&#65292;&#20063;&#36890;&#36807;&#25913;&#21464;&#25552;&#20379;&#30340;&#19978;&#19979;&#25991;&#21644;&#25552;&#31034;&#29305;&#23450;&#24615;&#26469;&#35843;&#26597;&#25552;&#31034;&#35774;&#35745;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#23545;&#36229;&#36807;6,000&#27425;LLMs&#20114;&#21160;&#30340;&#32467;&#26524;&#21453;&#26144;&#65292;&#25105;&#20204;&#21457;&#29616;&#27809;&#26377;&#27169;&#22411;&#26126;&#26174;&#36866;&#29992;&#20110;&#22312;&#32447;&#35825;&#25296;&#39044;&#38450;&#65292;&#22312;&#34892;&#20026;&#19968;&#33268;&#24615;&#26041;&#38754;&#23384;&#22312;&#32570;&#20047;&#19968;&#33268;&#24615;&#65292;&#24182;&#19988;&#23384;&#22312;&#26377;&#23475;&#31572;&#26696;&#29983;&#25104;&#30340;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#26469;&#33258;&#24320;&#28304;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09795v1 Announce Type: cross  Abstract: Powerful generative Large Language Models (LLMs) are becoming popular tools amongst the general public as question-answering systems, and are being utilised by vulnerable groups such as children. With children increasingly interacting with these tools, it is imperative for researchers to scrutinise the safety of LLMs, especially for applications that could lead to serious outcomes, such as online child safety queries. In this paper, the efficacy of LLMs for online grooming prevention is explored both for identifying and avoiding grooming through advice generation, and the impact of prompt design on model performance is investigated by varying the provided context and prompt specificity. In results reflecting over 6,000 LLM interactions, we find that no models were clearly appropriate for online grooming prevention, with an observed lack of consistency in behaviours, and potential for harmful answer generation, especially from open-sour
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#38382;&#39064;&#65292;&#25581;&#31034;&#20102;&#22270;&#20687;&#36755;&#20837;&#23545;&#27169;&#22411;&#30340;&#28431;&#27934;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#20687;&#38544;&#34255;&#24694;&#24847;&#24847;&#22270;&#12289;&#25104;&#21151;&#30772;&#35299;&#29616;&#26377;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.09792</link><description>&lt;p&gt;
&#22270;&#20687;&#26159;&#23545;&#40784;&#30340;&#36719;&#32907;&#65306;&#21033;&#29992;&#35270;&#35273;&#28431;&#27934;&#30772;&#35299;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Images are Achilles' Heel of Alignment: Exploiting Visual Vulnerabilities for Jailbreaking Multimodal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09792
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#38382;&#39064;&#65292;&#25581;&#31034;&#20102;&#22270;&#20687;&#36755;&#20837;&#23545;&#27169;&#22411;&#30340;&#28431;&#27934;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#20687;&#38544;&#34255;&#24694;&#24847;&#24847;&#22270;&#12289;&#25104;&#21151;&#30772;&#35299;&#29616;&#26377;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#26080;&#23475;&#23545;&#40784;&#38382;&#39064;&#12290;&#25105;&#20204;&#23545;&#20195;&#34920;&#24615;&#30340;MLLMs&#30340;&#26080;&#23475;&#24615;&#33021;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#24182;&#25581;&#31034;&#20102;&#22270;&#20687;&#36755;&#20837;&#23545;MLLMs&#36896;&#25104;&#30340;&#23545;&#40784;&#28431;&#27934;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HADES&#30340;&#26032;&#22411;&#30772;&#35299;&#26041;&#27861;&#65292;&#21033;&#29992;&#31934;&#24515;&#21046;&#20316;&#30340;&#22270;&#20687;&#38544;&#34255;&#21644;&#25918;&#22823;&#25991;&#26412;&#36755;&#20837;&#20013;&#30340;&#24694;&#24847;&#24847;&#22270;&#30340;&#26377;&#23475;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;HADES&#21487;&#20197;&#26377;&#25928;&#22320;&#30772;&#35299;&#29616;&#26377;&#30340;MLLMs&#65292;&#20026;LLaVA-1.5&#23454;&#29616;&#20102;90.26&#65285;&#30340;&#24179;&#22343;&#25915;&#20987;&#25104;&#21151;&#29575;&#65288;ASR&#65289;&#65292;&#20026;Gemini Pro Vision&#23454;&#29616;&#20102;71.60&#65285;&#30340;ASR&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#23558;&#20844;&#24320;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09792v1 Announce Type: cross  Abstract: In this paper, we study the harmlessness alignment problem of multimodal large language models~(MLLMs). We conduct a systematic empirical analysis of the harmlessness performance of representative MLLMs and reveal that the image input poses the alignment vulnerability of MLLMs. Inspired by this, we propose a novel jailbreak method named HADES, which hides and amplifies the harmfulness of the malicious intent within the text input, using meticulously crafted images. Experimental results show that HADES can effectively jailbreak existing MLLMs, which achieves an average Attack Success Rate~(ASR) of 90.26% for LLaVA-1.5 and 71.60% for Gemini Pro Vision. Our code and data will be publicly released.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#32771;&#23519;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#21307;&#30103;&#25991;&#26412;&#20013;&#24773;&#24863;&#20998;&#26512;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#23637;&#31034;&#20102;&#31639;&#27861;&#31934;&#24230;&#12289;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#39044;&#27979;&#20197;&#21450;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#26041;&#38754;&#30340;&#26174;&#33879;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.09762</link><description>&lt;p&gt;
&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#23454;&#29616;&#24773;&#32490;&#26234;&#33021;&#65306;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#28145;&#24230;&#23398;&#20064;&#22312;&#21307;&#30103;&#25991;&#26412;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Emotional Intelligence Through Artificial Intelligence : NLP and Deep Learning in the Analysis of Healthcare Texts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09762
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#32771;&#23519;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#21307;&#30103;&#25991;&#26412;&#20013;&#24773;&#24863;&#20998;&#26512;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#23637;&#31034;&#20102;&#31639;&#27861;&#31934;&#24230;&#12289;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#39044;&#27979;&#20197;&#21450;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#26041;&#38754;&#30340;&#26174;&#33879;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#35780;&#20272;&#19982;&#21307;&#30103;&#30456;&#20851;&#25991;&#26412;&#20013;&#24773;&#32490;&#26041;&#38754;&#30340;&#24212;&#29992;&#30340;&#26041;&#27861;&#35770;&#26816;&#26597;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#34701;&#21512;&#12290;&#25105;&#20204;&#23457;&#26597;&#20102;&#35768;&#22810;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#22686;&#24378;&#24773;&#24863;&#20998;&#26512;&#12289;&#23545;&#24773;&#32490;&#36827;&#34892;&#20998;&#31867;&#20197;&#21450;&#22522;&#20110;&#20020;&#24202;&#21465;&#20107;&#12289;&#24739;&#32773;&#23545;&#33647;&#29289;&#30340;&#21453;&#39304;&#21644;&#22312;&#32447;&#20581;&#24247;&#35752;&#35770;&#25152;&#33719;&#24471;&#30340;&#25991;&#26412;&#20449;&#24687;&#39044;&#27979;&#24739;&#32773;&#32467;&#26524;&#30340;&#30740;&#31350;&#12290;&#32508;&#36848;&#23637;&#31034;&#20102;&#29992;&#20110;&#24773;&#24863;&#20998;&#31867;&#30340;&#31639;&#27861;&#31934;&#24230;&#12289;&#29992;&#20110;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#30340;AI&#27169;&#22411;&#39044;&#27979;&#33021;&#21147;&#20197;&#21450;&#25903;&#25345;&#20020;&#24202;&#20915;&#31574;&#30340;AI&#31995;&#32479;&#30340;&#26174;&#33879;&#36827;&#23637;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#21033;&#29992;&#25552;&#39640;&#20102;&#20010;&#24615;&#21270;&#27835;&#30103;&#35745;&#21010;&#65292;&#36890;&#36807;&#25972;&#21512;&#24739;&#32773;&#24773;&#32490;&#24182;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09762v1 Announce Type: cross  Abstract: This manuscript presents a methodical examination of the utilization of Artificial Intelligence in the assessment of emotions in texts related to healthcare, with a particular focus on the incorporation of Natural Language Processing and deep learning technologies. We scrutinize numerous research studies that employ AI to augment sentiment analysis, categorize emotions, and forecast patient outcomes based on textual information derived from clinical narratives, patient feedback on medications, and online health discussions. The review demonstrates noteworthy progress in the precision of algorithms used for sentiment classification, the prognostic capabilities of AI models for neurodegenerative diseases, and the creation of AI-powered systems that offer support in clinical decision-making. Remarkably, the utilization of AI applications has exhibited an enhancement in personalized therapy plans by integrating patient sentiment and contri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#19968;&#31181;&#21487;&#20197;&#29992;&#20110;&#35835;&#21462;AI&#21161;&#25163;&#21152;&#23494;&#21709;&#24212;&#30340;&#26032;&#22411;&#26049;&#36335;&#25915;&#20987;&#8212;&#8212;&#20196;&#29260;&#38271;&#24230;&#26049;&#36335;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20379;&#21477;&#23376;&#38388;&#19978;&#19979;&#25991;&#24182;&#36827;&#34892;&#24050;&#30693;&#26126;&#25991;&#25915;&#20987;&#26469;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.09751</link><description>&lt;p&gt;
&#20320;&#30340;&#25552;&#31034;&#26159;&#20160;&#20040;&#65311;&#19968;&#31181;&#38024;&#23545;AI&#21161;&#25163;&#30340;&#36828;&#31243;&#38190;&#30424;&#35760;&#24405;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
What Was Your Prompt? A Remote Keylogging Attack on AI Assistants
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#19968;&#31181;&#21487;&#20197;&#29992;&#20110;&#35835;&#21462;AI&#21161;&#25163;&#21152;&#23494;&#21709;&#24212;&#30340;&#26032;&#22411;&#26049;&#36335;&#25915;&#20987;&#8212;&#8212;&#20196;&#29260;&#38271;&#24230;&#26049;&#36335;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20379;&#21477;&#23376;&#38388;&#19978;&#19979;&#25991;&#24182;&#36827;&#34892;&#24050;&#30693;&#26126;&#25991;&#25915;&#20987;&#26469;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#21161;&#25163;&#27491;&#36880;&#28176;&#25104;&#20026;&#31038;&#20250;&#30340;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#29992;&#20110;&#23547;&#27714;&#20010;&#20154;&#21644;&#26426;&#23494;&#38382;&#39064;&#30340;&#24314;&#35758;&#25110;&#24110;&#21161;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#19968;&#31181;&#26032;&#30340;&#26049;&#36335;&#25915;&#20987;&#65292;&#21487;&#29992;&#20110;&#36890;&#36807;&#32593;&#32476;&#35835;&#21462;AI&#21161;&#25163;&#30340;&#21152;&#23494;&#21709;&#24212;&#65306;&#20196;&#29260;&#38271;&#24230;&#26049;&#36335;&#12290;&#25105;&#20204;&#21457;&#29616;&#21253;&#25324;OpenAI&#21644;Microsoft&#22312;&#20869;&#30340;&#35768;&#22810;&#21378;&#21830;&#21463;&#21040;&#36825;&#19968;&#26049;&#36335;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#20165;&#20174;&#20196;&#29260;&#38271;&#24230;&#24207;&#21015;&#25512;&#26029;&#21709;&#24212;&#20869;&#23481;&#21364;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36825;&#26159;&#22240;&#20026;&#20196;&#29260;&#31867;&#20284;&#20110;&#21333;&#35789;&#65292;&#21709;&#24212;&#21487;&#20197;&#26159;&#20960;&#21477;&#35805;&#38271;&#65292;&#23548;&#33268;&#26377;&#25104;&#21315;&#19978;&#19975;&#20010;&#35821;&#27861;&#27491;&#30830;&#30340;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09751v1 Announce Type: cross  Abstract: AI assistants are becoming an integral part of society, used for asking advice or help in personal and confidential issues. In this paper, we unveil a novel side-channel that can be used to read encrypted responses from AI Assistants over the web: the token-length side-channel. We found that many vendors, including OpenAI and Microsoft, have this side-channel.   However, inferring the content of a response from a token-length sequence alone proves challenging. This is because tokens are akin to words, and responses can be several sentences long leading to millions of grammatically correct sentences. In this paper, we show how this can be overcome by (1) utilizing the power of a large language model (LLM) to translate these sequences, (2) providing the LLM with inter-sentence context to narrow the search space and (3) performing a known-plaintext attack by fine-tuning the model on the target model's writing style.   Using these methods,
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24191;&#27867;&#23454;&#39564;&#25506;&#32034;&#20102;LLMs&#20013;&#30340;&#38472;&#36848;&#24615;&#30693;&#35782;&#21644;&#31243;&#24207;&#24615;&#30693;&#35782;&#23545;&#21508;&#31181;&#20219;&#21153;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#38472;&#36848;&#24615;&#30693;&#35782;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#30340;&#30410;&#22788;&#22823;&#20110;&#31243;&#24207;&#24615;&#30693;&#35782;&#65292;&#22312;&#31616;&#21333;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#20013;&#21453;&#20043;&#65307;&#38543;&#30528;&#39044;&#35757;&#32451;&#21644;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#27169;&#22411;&#21033;&#29992;&#20004;&#31181;&#30693;&#35782;&#30340;&#33021;&#21147;&#22343;&#26174;&#33879;&#25552;&#39640;&#12290;</title><link>https://arxiv.org/abs/2403.09750</link><description>&lt;p&gt;
&#20803;&#35748;&#30693;&#20998;&#26512;&#65306;&#35780;&#20272;&#25968;&#25454;&#38598;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38472;&#36848;&#24615;&#21644;&#31243;&#24207;&#24615;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Meta-Cognitive Analysis: Evaluating Declarative and Procedural Knowledge in Datasets and Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09750
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24191;&#27867;&#23454;&#39564;&#25506;&#32034;&#20102;LLMs&#20013;&#30340;&#38472;&#36848;&#24615;&#30693;&#35782;&#21644;&#31243;&#24207;&#24615;&#30693;&#35782;&#23545;&#21508;&#31181;&#20219;&#21153;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#38472;&#36848;&#24615;&#30693;&#35782;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#30340;&#30410;&#22788;&#22823;&#20110;&#31243;&#24207;&#24615;&#30693;&#35782;&#65292;&#22312;&#31616;&#21333;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#20013;&#21453;&#20043;&#65307;&#38543;&#30528;&#39044;&#35757;&#32451;&#21644;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#27169;&#22411;&#21033;&#29992;&#20004;&#31181;&#30693;&#35782;&#30340;&#33021;&#21147;&#22343;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#35748;&#30693;&#29702;&#35770;&#20013;&#30340;&#38472;&#36848;&#24615;&#30693;&#35782;&#21644;&#31243;&#24207;&#24615;&#30693;&#35782;&#26159;&#20004;&#20010;&#20851;&#38190;&#37096;&#20998;&#65292;&#22312;LLM&#30340;&#39044;&#35757;&#32451;&#21644;&#25512;&#29702;&#20013;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23545;&#36825;&#20004;&#31181;&#30693;&#35782;&#30340;&#23450;&#20041;&#12289;&#25506;&#31350;&#21644;&#23450;&#37327;&#35780;&#20272;&#23384;&#22312;&#25361;&#25112;&#65292;&#32570;&#20047;&#23545;&#36825;&#20004;&#31181;&#30693;&#35782;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#30340;&#20998;&#26512;&#12290;&#26412;&#25991;&#20174;&#19968;&#20010;&#26032;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;LLMs&#30340;&#22320;&#38754;&#30495;&#30693;&#65292;&#24182;&#35780;&#20272;&#20102;&#26377;&#25928;&#24471;&#20998;&#12290;&#36890;&#36807;&#23545;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65306;(1) &#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#65292;&#26469;&#33258;&#38472;&#36848;&#24615;&#30693;&#35782;&#30340;&#30410;&#22788;&#22823;&#20110;&#26469;&#33258;&#31243;&#24207;&#24615;&#30693;&#35782;&#30340;&#30410;&#22788;&#12290;(2) &#20165;&#22312;&#20855;&#26377;&#31616;&#21333;&#36923;&#36753;&#25512;&#29702;&#30340;&#20219;&#21153;&#20013;&#65292;&#31243;&#24207;&#24615;&#30693;&#35782;&#30340;&#21033;&#28070;&#22823;&#20110;&#38472;&#36848;&#24615;&#30693;&#35782;&#12290;(3) &#38543;&#30528;&#39044;&#35757;&#32451;&#30340;&#36827;&#34892;&#21644;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#27169;&#22411;&#21033;&#29992;&#20004;&#31181;&#30693;&#35782;&#30340;&#33021;&#21147;&#26174;&#33879;&#25552;&#39640;&#65292;&#20294;&#36895;&#24230;&#19981;&#21516;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09750v1 Announce Type: cross  Abstract: Declarative knowledge and procedural knowledge are two key parts in meta-cognitive theory, and these two hold significant importance in pre-training and inference of LLMs. However, a comprehensive analysis comparing these two types of knowledge is lacking, primarily due to challenges in definition, probing and quantitative assessment. In this paper, we explore from a new perspective by providing ground-truth knowledge for LLMs and evaluating the effective score. Through extensive experiments with widely-used datasets and models, we get conclusions: (1) In most tasks, benefits from declarative knowledge are greater than those from procedural knowledge. (2) Profits of procedural knowledge are larger than declarative knowledge only in reasoning tasks with simple logic. (3) As pre-training progresses and size increases, model ability to utilize both kinds of knowledge significantly improves, but in different speed. We do detailed analysis 
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20551;&#26032;&#38395;&#26816;&#27979;&#20013;&#24341;&#20837;&#26032;&#30340;&#21069;&#27839;&#65292;&#20294;&#20173;&#38656;&#20811;&#26381;&#35299;&#20915;&#36807;&#26102;&#30693;&#35782;&#21644;&#20302;&#36136;&#37327;&#35777;&#25454;&#26816;&#32034;&#31561;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.09747</link><description>&lt;p&gt;
&#37325;&#26032;&#25506;&#23547;&#30495;&#30456;&#65306;&#22810;&#36718;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#24378;&#22823;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Re-Search for The Truth: Multi-round Retrieval-augmented Large Language Models are Strong Fake News Detectors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09747
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20551;&#26032;&#38395;&#26816;&#27979;&#20013;&#24341;&#20837;&#26032;&#30340;&#21069;&#27839;&#65292;&#20294;&#20173;&#38656;&#20811;&#26381;&#35299;&#20915;&#36807;&#26102;&#30693;&#35782;&#21644;&#20302;&#36136;&#37327;&#35777;&#25454;&#26816;&#32034;&#31561;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20551;&#26032;&#38395;&#30340;&#27867;&#28389;&#23545;&#25919;&#27835;&#12289;&#32463;&#27982;&#21644;&#31038;&#20250;&#24102;&#26469;&#20102;&#28145;&#36828;&#30340;&#24433;&#21709;&#12290;&#23613;&#31649;&#24050;&#32463;&#37319;&#29992;&#20102;&#20551;&#26032;&#38395;&#26816;&#27979;&#26041;&#27861;&#26469;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#20027;&#35201;&#20381;&#36182;&#20110;&#20004;&#20010;&#22522;&#26412;&#35201;&#32032;&#65306;&#35777;&#25454;&#30340;&#36136;&#37327;&#21644;&#30456;&#20851;&#24615;&#65292;&#20197;&#21450;&#39044;&#27979;&#26426;&#21046;&#30340;&#26377;&#25928;&#24615;&#12290;&#20256;&#32479;&#26041;&#27861;&#36890;&#24120;&#20174;&#32500;&#22522;&#30334;&#31185;&#31561;&#38745;&#24577;&#30693;&#35782;&#24211;&#20013;&#33719;&#21462;&#20449;&#24687;&#65292;&#20294;&#21463;&#38480;&#20110;&#36807;&#26102;&#25110;&#19981;&#23436;&#25972;&#30340;&#25968;&#25454;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#26032;&#20852;&#25110;&#32597;&#35265;&#30340;&#35201;&#27714;&#12290;&#20197;&#20854;&#21331;&#36234;&#30340;&#25512;&#29702;&#21644;&#29983;&#25104;&#33021;&#21147;&#32780;&#38395;&#21517;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20026;&#20551;&#26032;&#38395;&#26816;&#27979;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#19968;&#26679;&#65292;&#22522;&#20110;LLM&#30340;&#35299;&#20915;&#26041;&#26696;&#20063;&#20250;&#38754;&#20020;&#36807;&#26102;&#21644;&#38271;&#23614;&#30693;&#35782;&#30340;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#26816;&#32034;&#22686;&#24378;&#30340;LLMs&#32463;&#24120;&#36935;&#21040;&#20302;&#36136;&#37327;&#35777;&#25454;&#26816;&#32034;&#21644;&#19978;&#19979;&#25991;&#38271;&#24230;&#38480;&#21046;&#31561;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09747v1 Announce Type: cross  Abstract: The proliferation of fake news has had far-reaching implications on politics, the economy, and society at large. While Fake news detection methods have been employed to mitigate this issue, they primarily depend on two essential elements: the quality and relevance of the evidence, and the effectiveness of the verdict prediction mechanism. Traditional methods, which often source information from static repositories like Wikipedia, are limited by outdated or incomplete data, particularly for emerging or rare claims. Large Language Models (LLMs), known for their remarkable reasoning and generative capabilities, introduce a new frontier for fake news detection. However, like traditional methods, LLM-based solutions also grapple with the limitations of stale and long-tail knowledge. Additionally, retrieval-enhanced LLMs frequently struggle with issues such as low-quality evidence retrieval and context length constraints. To address these ch
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32534;&#31243;&#25945;&#32946;&#20013;&#29983;&#25104;&#21453;&#39304;&#30340;&#24212;&#29992;&#65292;&#32467;&#26524;&#26174;&#31034;&#22823;&#22810;&#25968;&#21453;&#39304;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#20195;&#30721;&#38169;&#35823;&#65292;&#20294;&#23384;&#22312;&#19981;&#27491;&#30830;&#24314;&#35758;&#21644;&#34394;&#26500;&#38382;&#39064;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.09744</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32534;&#31243;&#25945;&#32946;&#20013;&#29983;&#25104;&#21453;&#39304;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Application of Large Language Models to Generate Feedback in Programming Education
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09744
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32534;&#31243;&#25945;&#32946;&#20013;&#29983;&#25104;&#21453;&#39304;&#30340;&#24212;&#29992;&#65292;&#32467;&#26524;&#26174;&#31034;&#22823;&#22810;&#25968;&#21453;&#39304;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#20195;&#30721;&#38169;&#35823;&#65292;&#20294;&#23384;&#22312;&#19981;&#27491;&#30830;&#24314;&#35758;&#21644;&#34394;&#26500;&#38382;&#39064;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;GPT-4&#65292;&#22312;&#25552;&#21319;&#32534;&#31243;&#25945;&#32946;&#20013;&#30340;&#24212;&#29992;&#12290;&#30740;&#31350;&#27010;&#36848;&#20102;&#19968;&#20010;&#21033;&#29992;GPT-4&#25552;&#20379;&#32534;&#31243;&#20219;&#21153;&#21453;&#39304;&#20294;&#19981;&#25552;&#20379;&#35299;&#20915;&#26041;&#26696;&#30340;&#32593;&#39029;&#24212;&#29992;&#30340;&#35774;&#35745;&#12290;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#29992;&#20110;&#36827;&#34892;&#32534;&#31243;&#20219;&#21153;&#30340;&#32593;&#39029;&#24212;&#29992;&#65292;&#24182;&#22312;&#19968;&#20010;&#23398;&#26399;&#20869;&#23545;51&#21517;&#23398;&#29983;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22823;&#22810;&#25968;&#30001;GPT-4&#29983;&#25104;&#30340;&#21453;&#39304;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#20195;&#30721;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#23384;&#22312;&#19981;&#27491;&#30830;&#24314;&#35758;&#21644;&#34394;&#26500;&#38382;&#39064;&#30340;&#25361;&#25112;&#34920;&#26126;&#26377;&#36827;&#19968;&#27493;&#25913;&#36827;&#30340;&#24517;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09744v1 Announce Type: cross  Abstract: This study investigates the application of large language models, specifically GPT-4, to enhance programming education. The research outlines the design of a web application that uses GPT-4 to provide feedback on programming tasks, without giving away the solution. A web application for working on programming tasks was developed for the study and evaluated with 51 students over the course of one semester. The results show that most of the feedback generated by GPT-4 effectively addressed code errors. However, challenges with incorrect suggestions and hallucinated issues indicate the need for further improvements.
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#65292;&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#20154;&#31867;&#22240;&#32032;&#22312;&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38169;&#35823;&#36755;&#20986;&#20013;&#30340;&#20316;&#29992;&#65292;&#26377;&#21161;&#20110;&#20943;&#36731;&#20854;&#22312;&#19987;&#19994;&#29615;&#22659;&#20013;&#20351;&#29992;&#26102;&#25152;&#24102;&#26469;&#30340;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2403.09743</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20154;&#20026;&#22240;&#32032;&#65306;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#19982;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
The Human Factor in Detecting Errors of Large Language Models: A Systematic Literature Review and Future Research Directions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09743
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#65292;&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#20154;&#31867;&#22240;&#32032;&#22312;&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38169;&#35823;&#36755;&#20986;&#20013;&#30340;&#20316;&#29992;&#65292;&#26377;&#21161;&#20110;&#20943;&#36731;&#20854;&#22312;&#19987;&#19994;&#29615;&#22659;&#20013;&#20351;&#29992;&#26102;&#25152;&#24102;&#26469;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
OpenAI&#22312;2022&#24180;11&#26376;&#25512;&#20986;&#30340;ChatGPT&#26631;&#24535;&#30528;&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#20010;&#20851;&#38190;&#26102;&#21051;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24341;&#20837;&#20027;&#27969;&#65292;&#24182;&#22312;&#29992;&#25143;&#37319;&#29992;&#26041;&#38754;&#21019;&#36896;&#20102;&#26032;&#35760;&#24405;&#12290;&#23588;&#20854;&#26159;ChatGPT&#65292;&#32463;&#36807;&#24191;&#27867;&#30340;&#20114;&#32852;&#32593;&#25968;&#25454;&#35757;&#32451;&#65292;&#23637;&#31034;&#20986;&#22312;&#21508;&#20010;&#39046;&#22495;&#20855;&#26377;&#26174;&#33879;&#30340;&#23545;&#35805;&#33021;&#21147;&#65292;&#26263;&#31034;&#23545;&#21171;&#21160;&#21147;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23481;&#26131;&#20986;&#29616;&#38169;&#35823;-&#8220;&#24187;&#35273;&#8221;&#21644;&#36951;&#28431;&#65292;&#20135;&#29983;&#19981;&#27491;&#30830;&#25110;&#19981;&#23436;&#25972;&#30340;&#20449;&#24687;&#12290;&#36825;&#22312;&#20934;&#30830;&#24615;&#33267;&#20851;&#37325;&#35201;&#30340;&#29615;&#22659;&#20013;&#23588;&#20026;&#21361;&#38505;&#65292;&#27604;&#22914;&#27861;&#24459;&#21512;&#35268;&#12289;&#21307;&#23398;&#25110;&#31934;&#32454;&#30340;&#27969;&#31243;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09743v1 Announce Type: cross  Abstract: The launch of ChatGPT by OpenAI in November 2022 marked a pivotal moment for Artificial Intelligence, introducing Large Language Models (LLMs) to the mainstream and setting new records in user adoption. LLMs, particularly ChatGPT, trained on extensive internet data, demonstrate remarkable conversational capabilities across various domains, suggesting a significant impact on the workforce. However, these models are susceptible to errors - "hallucinations" and omissions, generating incorrect or incomplete information. This poses risks especially in contexts where accuracy is crucial, such as legal compliance, medicine or fine-grained process frameworks.   There are both technical and human solutions to cope with this isse. This paper explores the human factors that enable users to detect errors in LLM outputs, a critical component in mitigating risks associated with their use in professional settings. Understanding these factors is essen
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#29983;&#25104;&#24335;&#29992;&#25143;&#27169;&#25311;&#22120;&#22312;&#23545;&#35805;&#25512;&#33616;&#20013;&#23637;&#29616;&#20986;&#28508;&#21147;&#65292;&#26032;&#30340;&#21327;&#35758;&#36890;&#36807;&#20116;&#20010;&#20219;&#21153;&#35780;&#20272;&#20102;&#35821;&#35328;&#27169;&#22411;&#27169;&#25311;&#20154;&#31867;&#34892;&#20026;&#30340;&#20934;&#30830;&#31243;&#24230;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#19982;&#20154;&#31867;&#34892;&#20026;&#30340;&#20559;&#24046;&#65292;&#24182;&#25552;&#20986;&#20102;&#22914;&#20309;&#36890;&#36807;&#27169;&#22411;&#36873;&#25321;&#21644;&#25552;&#31034;&#31574;&#30053;&#20943;&#23569;&#36825;&#20123;&#20559;&#24046;&#12290;</title><link>https://arxiv.org/abs/2403.09738</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#23545;&#35805;&#25512;&#33616;&#20013;&#29983;&#25104;&#29992;&#25143;&#27169;&#25311;&#22120;
&lt;/p&gt;
&lt;p&gt;
Evaluating Large Language Models as Generative User Simulators for Conversational Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09738
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#29983;&#25104;&#24335;&#29992;&#25143;&#27169;&#25311;&#22120;&#22312;&#23545;&#35805;&#25512;&#33616;&#20013;&#23637;&#29616;&#20986;&#28508;&#21147;&#65292;&#26032;&#30340;&#21327;&#35758;&#36890;&#36807;&#20116;&#20010;&#20219;&#21153;&#35780;&#20272;&#20102;&#35821;&#35328;&#27169;&#22411;&#27169;&#25311;&#20154;&#31867;&#34892;&#20026;&#30340;&#20934;&#30830;&#31243;&#24230;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#19982;&#20154;&#31867;&#34892;&#20026;&#30340;&#20559;&#24046;&#65292;&#24182;&#25552;&#20986;&#20102;&#22914;&#20309;&#36890;&#36807;&#27169;&#22411;&#36873;&#25321;&#21644;&#25552;&#31034;&#31574;&#30053;&#20943;&#23569;&#36825;&#20123;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#29992;&#25143;&#26159;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#35780;&#20272;&#20013;&#25104;&#26412;&#25928;&#30410;&#36739;&#39640;&#30340;&#30495;&#23454;&#29992;&#25143;&#20195;&#29702;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#22312;&#27169;&#25311;&#31867;&#20284;&#20154;&#31867;&#34892;&#20026;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#36825;&#24341;&#21457;&#20102;&#23427;&#20204;&#33021;&#21542;&#20195;&#34920;&#22810;&#26679;&#21270;&#29992;&#25143;&#32676;&#20307;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#21327;&#35758;&#65292;&#29992;&#20110;&#34913;&#37327;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#27169;&#25311;&#23545;&#35805;&#25512;&#33616;&#20013;&#20154;&#31867;&#34892;&#20026;&#30340;&#31243;&#24230;&#12290;&#35813;&#21327;&#35758;&#30001;&#20116;&#20010;&#20219;&#21153;&#32452;&#25104;&#65292;&#27599;&#20010;&#20219;&#21153;&#26088;&#22312;&#35780;&#20272;&#21512;&#25104;&#29992;&#25143;&#24212;&#35813;&#34920;&#29616;&#20986;&#30340;&#20851;&#38190;&#29305;&#24615;&#65306;&#36873;&#25321;&#35201;&#35848;&#35770;&#30340;&#29289;&#21697;&#65292;&#34920;&#36798;&#20108;&#36827;&#21046;&#20559;&#22909;&#65292;&#34920;&#36798;&#24320;&#25918;&#24335;&#20559;&#22909;&#65292;&#35831;&#27714;&#25512;&#33616;&#20197;&#21450;&#25552;&#20379;&#21453;&#39304;&#12290;&#36890;&#36807;&#23545;&#22522;&#20934;&#27169;&#25311;&#22120;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#20219;&#21153;&#26377;&#25928;&#22320;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#34892;&#20026;&#30340;&#20559;&#24046;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#22914;&#20309;&#36890;&#36807;&#27169;&#22411;&#36873;&#25321;&#21644;&#25552;&#31034;&#31574;&#30053;&#20943;&#23569;&#36825;&#20123;&#20559;&#24046;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09738v1 Announce Type: cross  Abstract: Synthetic users are cost-effective proxies for real users in the evaluation of conversational recommender systems. Large language models show promise in simulating human-like behavior, raising the question of their ability to represent a diverse population of users. We introduce a new protocol to measure the degree to which language models can accurately emulate human behavior in conversational recommendation. This protocol is comprised of five tasks, each designed to evaluate a key property that a synthetic user should exhibit: choosing which items to talk about, expressing binary preferences, expressing open-ended preferences, requesting recommendations, and giving feedback. Through evaluation of baseline simulators, we demonstrate these tasks effectively reveal deviations of language models from human behavior, and offer insights on how to reduce the deviations with model selection and prompting strategies.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#27604;&#36739;&#20102;&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;ARC&#35270;&#35273;&#31867;&#27604;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#65292;&#20154;&#31867;&#21644;&#25104;&#24180;&#20154;&#30340;&#34920;&#29616;&#22343;&#20248;&#20110;&#22823;&#22810;&#25968;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#23545;LLMs&#21644;&#24180;&#24188;&#20799;&#31461;&#38169;&#35823;&#20998;&#26512;&#25581;&#31034;&#20102;&#31867;&#20284;&#30340;&#35299;&#20915;&#31574;&#30053;&#65292;&#21516;&#26102;&#25351;&#20986;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#38169;&#35823;&#31867;&#22411;&#65292;&#20026;&#25105;&#20204;&#29702;&#35299;LLMs&#22914;&#20309;&#35299;&#20915;&#35270;&#35273;&#31867;&#27604;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#21551;&#31034;&#12290;</title><link>https://arxiv.org/abs/2403.09734</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20687;&#20154;&#19968;&#26679;&#35299;&#20915;ARC&#35270;&#35273;&#31867;&#27604;&#38382;&#39064;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Large Language Models Solve ARC Visual Analogies Like People Do?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09734
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#27604;&#36739;&#20102;&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;ARC&#35270;&#35273;&#31867;&#27604;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#65292;&#20154;&#31867;&#21644;&#25104;&#24180;&#20154;&#30340;&#34920;&#29616;&#22343;&#20248;&#20110;&#22823;&#22810;&#25968;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#23545;LLMs&#21644;&#24180;&#24188;&#20799;&#31461;&#38169;&#35823;&#20998;&#26512;&#25581;&#31034;&#20102;&#31867;&#20284;&#30340;&#35299;&#20915;&#31574;&#30053;&#65292;&#21516;&#26102;&#25351;&#20986;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#38169;&#35823;&#31867;&#22411;&#65292;&#20026;&#25105;&#20204;&#29702;&#35299;LLMs&#22914;&#20309;&#35299;&#20915;&#35270;&#35273;&#31867;&#27604;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25233;&#21046;&#35770;&#25991;&#65288;Chollet, 2019&#65289;&#24418;&#24335;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20799;&#31461;&#21451;&#22909;&#30340;ARC&#39033;&#30446;&#19978;&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#34920;&#29616;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#26080;&#35770;&#26159;&#20799;&#31461;&#36824;&#26159;&#25104;&#24180;&#20154;&#65292;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#37117;&#32988;&#36807;&#22823;&#22810;&#25968;LLMs&#12290;&#38169;&#35823;&#20998;&#26512;&#25581;&#31034;&#20102;LLMs&#21644;&#24180;&#24188;&#20799;&#31461;&#20043;&#38388;&#31867;&#20284;&#30340;&#8220;&#20498;&#36864;&#8221;&#35299;&#20915;&#31574;&#30053;&#65292;&#20854;&#20013;&#31867;&#27604;&#30340;&#19968;&#37096;&#20998;&#34987;&#31616;&#21333;&#22797;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#20854;&#20182;&#20004;&#31181;&#38169;&#35823;&#31867;&#22411;&#65292;&#19968;&#31181;&#22522;&#20110;&#34920;&#38754;&#25484;&#25569;&#20851;&#38190;&#27010;&#24565;&#65288;&#20363;&#22914;&#65292;&#20869;&#22806;&#20851;&#31995;&#65289;&#65292;&#21478;&#19968;&#31181;&#22522;&#20110;&#31867;&#27604;&#36755;&#20837;&#30697;&#38453;&#30340;&#31616;&#21333;&#32452;&#21512;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#8220;&#27010;&#24565;&#8221;&#38169;&#35823;&#22312;&#20154;&#31867;&#20013;&#26356;&#24120;&#35265;&#65292;&#8220;&#30697;&#38453;&#8221;&#38169;&#35823;&#22312;LLMs&#20013;&#26356;&#24120;&#35265;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#38169;&#35823;&#20998;&#26512;&#20197;&#21450;&#19982;&#20154;&#31867;&#21457;&#23637;&#30340;&#27604;&#36739;&#26469;&#29702;&#35299;LLMs&#22914;&#20309;&#35299;&#20915;&#35270;&#35273;&#31867;&#27604;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09734v1 Announce Type: cross  Abstract: The Abstraction Reasoning Corpus (ARC) is a visual analogical reasoning test designed for humans and machines (Chollet, 2019). We compared human and large language model (LLM) performance on a new child-friendly set of ARC items. Results show that both children and adults outperform most LLMs on these tasks. Error analysis revealed a similar "fallback" solution strategy in LLMs and young children, where part of the analogy is simply copied. In addition, we found two other error types, one based on seemingly grasping key concepts (e.g., Inside-Outside) and the other based on simple combinations of analogy input matrices. On the whole, "concept" errors were more common in humans, and "matrix" errors were more common in LLMs. This study sheds new light on LLM reasoning ability and the extent to which we can use error analyses and comparisons with human development to understand how LLMs solve visual analogies.
&lt;/p&gt;</description></item><item><title>OverleafCopilot&#26159;&#31532;&#19968;&#20010;&#26080;&#32541;&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;Overleaf&#30340;&#24037;&#20855;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#22312;&#25776;&#20889;&#35770;&#25991;&#26102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.09733</link><description>&lt;p&gt;
OverleafCopilot&#65306;&#22312;Overleaf&#20013;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#23398;&#26415;&#20889;&#20316;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
OverleafCopilot: Empowering Academic Writing in Overleaf with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09733
&lt;/p&gt;
&lt;p&gt;
OverleafCopilot&#26159;&#31532;&#19968;&#20010;&#26080;&#32541;&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;Overleaf&#30340;&#24037;&#20855;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#22312;&#25776;&#20889;&#35770;&#25991;&#26102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36805;&#36895;&#21457;&#23637;&#20419;&#36827;&#20102;&#19981;&#21516;&#39046;&#22495;&#21508;&#31181;&#24212;&#29992;&#30340;&#23454;&#29616;&#12290;&#22312;&#26412;&#25216;&#26415;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#23558;LLMs&#21644;&#27969;&#34892;&#30340;&#23398;&#26415;&#20889;&#20316;&#24037;&#20855;Overleaf&#38598;&#25104;&#65292;&#20197;&#25552;&#39640;&#23398;&#26415;&#20889;&#20316;&#30340;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;&#20026;&#23454;&#29616;&#19978;&#36848;&#30446;&#26631;&#65292;&#25105;&#20204;&#38754;&#20020;&#19977;&#20010;&#25361;&#25112;&#65306;i&#65289;&#22312;Overleaf&#21644;LLMs&#20043;&#38388;&#23454;&#29616;&#26080;&#32541;&#20132;&#20114;&#65292;ii&#65289;&#19982;LLM&#25552;&#20379;&#32773;&#24314;&#31435;&#21487;&#38752;&#36890;&#20449;&#65292;iii&#65289;&#30830;&#20445;&#29992;&#25143;&#38544;&#31169;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;OverleafCopilot&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#26080;&#32541;&#38598;&#25104;LLMs&#21644;Overleaf&#30340;&#24037;&#20855;&#65288;&#21363;&#27983;&#35272;&#22120;&#25193;&#23637;&#31243;&#24207;&#65289;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#22312;&#25776;&#20889;&#35770;&#25991;&#26102;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;LLMs&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#26694;&#26550;&#26469;&#36830;&#25509;LLMs&#21644;Overleaf&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;PromptGenius&#32593;&#31449;&#65292;&#20379;&#30740;&#31350;&#20154;&#21592;&#36731;&#26494;&#26597;&#25214;&#21644;&#20849;&#20139;&#39640;&#36136;&#37327;&#30340;&#26368;&#26032;&#25552;&#31034;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;ag
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09733v1 Announce Type: cross  Abstract: The rapid development of Large Language Models (LLMs) has facilitated a variety of applications from different domains. In this technical report, we explore the integration of LLMs and the popular academic writing tool, Overleaf, to enhance the efficiency and quality of academic writing. To achieve the above goal, there are three challenges: i) including seamless interaction between Overleaf and LLMs, ii) establishing reliable communication with the LLM provider, and iii) ensuring user privacy. To address these challenges, we present OverleafCopilot, the first-ever tool (i.e., a browser extension) that seamlessly integrates LLMs and Overleaf, enabling researchers to leverage the power of LLMs while writing papers. Specifically, we first propose an effective framework to bridge LLMs and Overleaf. Then, we developed PromptGenius, a website for researchers to easily find and share high-quality up-to-date prompts. Thirdly, we propose an ag
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#21442;&#32771;&#22686;&#24378;&#34920;&#31034;&#21644;&#23569;&#26679;&#26412;&#28436;&#31034;&#65292;&#35299;&#20915;&#20102;&#22312;&#22788;&#29702;&#20887;&#38271;&#30340;&#25968;&#25454;&#24211;&#20449;&#24687;&#21644;&#22797;&#26434;&#29992;&#25143;&#24847;&#22270;&#26102;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.09732</link><description>&lt;p&gt;
PET-SQL&#65306;&#19968;&#20010;&#24102;&#26377;&#20132;&#21449;&#19968;&#33268;&#24615;&#30340;&#22686;&#24378;&#25552;&#31034;&#30340;&#20004;&#38454;&#27573;&#25991;&#26412;&#21040;SQL&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
PET-SQL: A Prompt-enhanced Two-stage Text-to-SQL Framework with Cross-consistency
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09732
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#21442;&#32771;&#22686;&#24378;&#34920;&#31034;&#21644;&#23569;&#26679;&#26412;&#28436;&#31034;&#65292;&#35299;&#20915;&#20102;&#22312;&#22788;&#29702;&#20887;&#38271;&#30340;&#25968;&#25454;&#24211;&#20449;&#24687;&#21644;&#22797;&#26434;&#29992;&#25143;&#24847;&#22270;&#26102;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25991;&#26412;&#21040;SQL&#65288;Text2SQL&#65289;&#39046;&#22495;&#30340;&#36827;&#23637;&#24378;&#35843;&#21050;&#28608;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#22312;&#22788;&#29702;&#20887;&#38271;&#30340;&#25968;&#25454;&#24211;&#20449;&#24687;&#21644;&#22797;&#26434;&#30340;&#29992;&#25143;&#24847;&#22270;&#26102;&#38754;&#20020;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#20197;&#22686;&#24378;&#24403;&#21069;&#22522;&#20110;LLM&#30340;&#33258;&#28982;&#35821;&#35328;&#21040;SQL&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25552;&#31034;&#34920;&#31034;&#65292;&#31216;&#20026;&#21442;&#32771;&#22686;&#24378;&#34920;&#31034;&#65292;&#20854;&#20013;&#21253;&#25324;&#27169;&#24335;&#20449;&#24687;&#21644;&#20174;&#34920;&#26684;&#38543;&#26426;&#25277;&#26679;&#30340;&#21333;&#20803;&#26684;&#20540;&#65292;&#20197;&#25351;&#23548;LLM&#29983;&#25104;SQL&#26597;&#35810;&#12290;&#28982;&#21518;&#65292;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#26816;&#32034;&#38382;&#39064;-SQL&#23545;&#20316;&#20026;&#23569;&#37327;&#28436;&#31034;&#65292;&#20419;&#20351;LLM&#29983;&#25104;&#21021;&#27493;SQL&#65288;PreSQL&#65289;&#12290;&#20043;&#21518;&#65292;&#35299;&#26512;PreSQL&#20013;&#25552;&#21040;&#30340;&#23454;&#20307;&#36827;&#34892;&#27169;&#24335;&#38142;&#25509;&#65292;&#21487;&#20197;&#26174;&#33879;&#21387;&#32553;&#26377;&#29992;&#20449;&#24687;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#21033;&#29992;&#38142;&#25509;&#30340;&#27169;&#24335;&#65292;&#25105;&#20204;&#31616;&#21270;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09732v1 Announce Type: cross  Abstract: Recent advancements in Text-to-SQL (Text2SQL) emphasize stimulating the large language models (LLM) on in-context learning, achieving significant results. Nevertheless, they face challenges when dealing with verbose database information and complex user intentions. This paper presents a two-stage framework to enhance the performance of current LLM-based natural language to SQL systems. We first introduce a novel prompt representation, called reference-enhanced representation, which includes schema information and randomly sampled cell values from tables to instruct LLMs in generating SQL queries. Then, in the first stage, question-SQL pairs are retrieved as few-shot demonstrations, prompting the LLM to generate a preliminary SQL (PreSQL). After that, the mentioned entities in PreSQL are parsed to conduct schema linking, which can significantly compact the useful information. In the second stage, with the linked schema, we simplify the 
&lt;/p&gt;</description></item><item><title>&#21464;&#21387;&#22120;&#21487;&#20197;&#27169;&#25311;&#21152;&#26435;&#26377;&#38480;&#33258;&#21160;&#26426;&#21644;&#21152;&#26435;&#26641;&#33258;&#21160;&#26426;&#65292;&#25299;&#23637;&#20102;&#23427;&#20204;&#30340;&#24212;&#29992;&#33539;&#22260;&#12290;</title><link>https://arxiv.org/abs/2403.09728</link><description>&lt;p&gt;
&#20351;&#29992;&#21464;&#21387;&#22120;&#27169;&#25311;&#24207;&#21015;&#21644;&#26641;&#19978;&#30340;&#21152;&#26435;&#33258;&#21160;&#26426;
&lt;/p&gt;
&lt;p&gt;
Simulating Weighted Automata over Sequences and Trees with Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09728
&lt;/p&gt;
&lt;p&gt;
&#21464;&#21387;&#22120;&#21487;&#20197;&#27169;&#25311;&#21152;&#26435;&#26377;&#38480;&#33258;&#21160;&#26426;&#21644;&#21152;&#26435;&#26641;&#33258;&#21160;&#26426;&#65292;&#25299;&#23637;&#20102;&#23427;&#20204;&#30340;&#24212;&#29992;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#21387;&#22120;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31038;&#21306;&#20013;&#26080;&#22788;&#19981;&#22312;&#30340;&#27169;&#22411;&#65292;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32463;&#39564;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#23427;&#20204;&#25512;&#29702;&#30340;&#26041;&#24335;&#20197;&#21450;&#35745;&#31639;&#33021;&#21147;&#30340;&#38480;&#21046;&#65292;&#20154;&#20204;&#23545;&#27492;&#30693;&#20043;&#29978;&#23569;&#12290;&#36825;&#20123;&#27169;&#22411;&#19981;&#26159;&#25353;&#39034;&#24207;&#22788;&#29702;&#25968;&#25454;&#65292;&#21364;&#32988;&#36807;&#35832;&#22914;RNN&#30340;&#39034;&#24207;&#31070;&#32463;&#27169;&#22411;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#32039;&#20945;&#22320;&#27169;&#25311;&#30830;&#23450;&#24615;&#26377;&#38480;&#33258;&#21160;&#26426;&#65288;DFAs&#65289;&#30340;&#24207;&#21015;&#25512;&#29702;&#33021;&#21147;&#12290;&#36825;&#24102;&#26469;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#21464;&#21387;&#22120;&#33021;&#21542;&#27169;&#25311;&#26356;&#22797;&#26434;&#30340;&#26377;&#38480;&#29366;&#24577;&#26426;&#30340;&#25512;&#29702;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#21464;&#21387;&#22120;&#21487;&#20197;&#27169;&#25311;&#21152;&#26435;&#26377;&#38480;&#33258;&#21160;&#26426;&#65288;WFAs&#65289;&#65292;&#36825;&#26159;&#19968;&#31867;&#21253;&#21547;DFAs&#30340;&#27169;&#22411;&#65292;&#20197;&#21450;&#21152;&#26435;&#26641;&#33258;&#21160;&#26426;&#65288;WTA&#65289;&#65292;&#19968;&#31181;&#21152;&#26435;&#33258;&#21160;&#26426;&#25512;&#24191;&#21040;&#26641;&#24418;&#36755;&#20837;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#27491;&#24335;&#35777;&#26126;&#20102;&#36825;&#20123;&#35828;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#25152;&#38656;&#21464;&#21387;&#22120;&#27169;&#22411;&#22823;&#23567;&#30340;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09728v1 Announce Type: cross  Abstract: Transformers are ubiquitous models in the natural language processing (NLP) community and have shown impressive empirical successes in the past few years. However, little is understood about how they reason and the limits of their computational capabilities. These models do not process data sequentially, and yet outperform sequential neural models such as RNNs. Recent work has shown that these models can compactly simulate the sequential reasoning abilities of deterministic finite automata (DFAs). This leads to the following question: can transformers simulate the reasoning of more complex finite state machines? In this work, we show that transformers can simulate weighted finite automata (WFAs), a class of models which subsumes DFAs, as well as weighted tree automata (WTA), a generalization of weighted automata to tree structured inputs. We prove these claims formally and provide upper bounds on the sizes of the transformer models nee
&lt;/p&gt;</description></item><item><title>RAG-based constructions are more efficient than models produced with FN for the development of AI-driven knowledge-based systems.</title><link>https://arxiv.org/abs/2403.09727</link><description>&lt;p&gt;
&#25506;&#31350;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#21644;&#24494;&#35843;&#22312;&#21457;&#23637;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#30693;&#35782;&#31995;&#32479;&#20013;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Investigating the performance of Retrieval-Augmented Generation and fine-tuning for the development of AI-driven knowledge-based systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09727
&lt;/p&gt;
&lt;p&gt;
RAG-based constructions are more efficient than models produced with FN for the development of AI-driven knowledge-based systems.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09727v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449;&#39046;&#22495; &#25688;&#35201;: &#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(G-LLM)&#30340;&#21457;&#23637;&#20026;&#31867;&#20284;ChatGPT&#12289;Bing&#25110;Gemini&#30340;&#26032;&#22411;&#30693;&#35782;&#31995;&#32479;&#30340;&#24320;&#21457;&#25171;&#24320;&#20102;&#26032;&#30340;&#26426;&#20250;&#12290;&#24494;&#35843;(FN)&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;(RAG)&#26159;&#21487;&#29992;&#20110;&#23454;&#29616;&#22522;&#20110;G-LLM&#30340;&#30693;&#35782;&#31995;&#32479;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#25216;&#26415;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#21033;&#29992;ROUGE&#12289;BLEU&#12289;METEOR&#20998;&#25968;&#21644;&#20313;&#24358;&#30456;&#20284;&#24230;&#65292;&#25105;&#20204;&#27604;&#36739;&#24182;&#26816;&#39564;&#20102;GPT-J-6B&#12289;OPT-6.7B&#12289;LlaMA&#12289;LlaMA-2&#35821;&#35328;&#27169;&#22411;&#30340;RAG&#21644;FN&#30340;&#34920;&#29616;&#12290;&#22522;&#20110;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#30340;&#27979;&#37327;&#32467;&#26524;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;RAG&#30340;&#26500;&#24314;&#27604;&#20351;&#29992;FN&#20135;&#29983;&#30340;&#27169;&#22411;&#26356;&#26377;&#25928;&#12290;&#25105;&#20204;&#25351;&#20986;&#23558;RAG&#21644;FN&#36830;&#25509;&#36215;&#26469;&#24182;&#19981;&#26159;&#36731;&#32780;&#26131;&#20030;&#30340;&#65292;&#22240;&#20026;&#23558;FN&#27169;&#22411;&#19982;RAG&#36830;&#25509;&#21487;&#33021;&#20250;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22522;&#20110;RAG&#30340;&#26550;&#26500;&#65292;&#24179;&#22343;&#22312;RO&#26041;&#38754;&#20248;&#20110;FN&#27169;&#22411;16%
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09727v1 Announce Type: cross  Abstract: The development of generative large language models (G-LLM) opened up new opportunities for the development of new types of knowledge-based systems similar to ChatGPT, Bing, or Gemini. Fine-tuning (FN) and Retrieval-Augmented Generation (RAG) are the techniques that can be used to implement domain adaptation for the development of G-LLM-based knowledge systems. In our study, using ROUGE, BLEU, METEOR scores, and cosine similarity, we compare and examine the performance of RAG and FN for the GPT-J-6B, OPT-6.7B, LlaMA, LlaMA-2 language models. Based on measurements shown on different datasets, we demonstrate that RAG-based constructions are more efficient than models produced with FN. We point out that connecting RAG and FN is not trivial, because connecting FN models with RAG can cause a decrease in performance. Furthermore, we outline a simple RAG-based architecture which, on average, outperforms the FN models by 16% in terms of the RO
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#23558;&#23567;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#25918;&#23556;&#23398;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#24494;&#35843;&#20855;&#26377;27&#20159;&#21442;&#25968;&#30340;Phi-2&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#33391;&#22909;&#24615;&#33021;&#30340;RadPhi-2-Base&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.09725</link><description>&lt;p&gt;
RAD-PHI2&#65306;&#20026;&#25918;&#23556;&#23398;&#35843;&#25972;PHI-2&#30340;&#25351;&#23548;
&lt;/p&gt;
&lt;p&gt;
RAD-PHI2: Instruction Tuning PHI-2 for Radiology
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09725
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#23558;&#23567;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#25918;&#23556;&#23398;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#24494;&#35843;&#20855;&#26377;27&#20159;&#21442;&#25968;&#30340;Phi-2&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#33391;&#22909;&#24615;&#33021;&#30340;RadPhi-2-Base&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23567;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#22312;&#19968;&#33324;&#39046;&#22495;&#35821;&#35328;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#32534;&#30721;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#28041;&#21450;&#25918;&#23556;&#23398;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#36824;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;SLMs&#22312;&#19968;&#33324;&#25918;&#23556;&#23398;&#30693;&#35782;&#29305;&#21035;&#26159;&#19982;&#20102;&#35299;&#30151;&#29366;&#12289;&#25918;&#23556;&#23398;&#21457;&#29616;&#30340;&#22806;&#35266;&#12289;&#37492;&#21035;&#35786;&#26029;&#12289;&#35780;&#20272;&#39044;&#21518;&#20197;&#21450;&#38024;&#23545;&#19981;&#21516;&#22120;&#23448;&#31995;&#32479;&#30142;&#30149;&#30340;&#27835;&#30103;&#26041;&#38754;&#30340;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#23558;SLMs&#24212;&#29992;&#20110;&#22788;&#29702;&#19982;&#25918;&#23556;&#23398;&#25253;&#21578;&#30456;&#20851;&#20219;&#21153;&#22312;&#22522;&#20110;AI&#30340;&#25918;&#23556;&#23398;&#24037;&#20316;&#27969;&#20013;&#30340;&#25928;&#29992;&#12290;&#25105;&#20204;&#20351;&#29992;Radiopaedia&#36825;&#19968;&#21327;&#20316;&#22312;&#32447;&#25918;&#23556;&#23398;&#36164;&#28304;&#20013;&#30340;&#39640;&#36136;&#37327;&#25945;&#32946;&#20869;&#23481;&#23545;&#20855;&#26377;27&#20159;&#21442;&#25968;&#30340;Phi-2&#36827;&#34892;&#24494;&#35843;&#12290;&#25152;&#24471;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;RadPhi-2-Base&#65292;&#34920;&#29616;&#20986;&#20102;&#35299;&#20915;&#19968;&#33324;&#25918;&#23556;&#23398;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09725v1 Announce Type: cross  Abstract: Small Language Models (SLMs) have shown remarkable performance in general domain language understanding, reasoning and coding tasks, but their capabilities in the medical domain, particularly concerning radiology text, is less explored. In this study, we investigate the application of SLMs for general radiology knowledge specifically question answering related to understanding of symptoms, radiological appearances of findings, differential diagnosis, assessing prognosis, and suggesting treatments w.r.t diseases pertaining to different organ systems. Additionally, we explore the utility of SLMs in handling text-related tasks with respect to radiology reports within AI-driven radiology workflows. We fine-tune Phi-2, a SLM with 2.7 billion parameters using high-quality educational content from Radiopaedia, a collaborative online radiology resource. The resulting language model, RadPhi-2-Base, exhibits the ability to address general radiol
&lt;/p&gt;</description></item><item><title>ClaimVer&#26159;&#19968;&#20010;&#20154;&#20026;&#20013;&#24515;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#22768;&#26126;&#32423;&#39564;&#35777;&#21644;&#35777;&#25454;&#24402;&#22240;&#65292;&#33268;&#21147;&#20110;&#25552;&#39640;&#29992;&#25143;&#23545;&#25991;&#26412;&#39564;&#35777;&#26041;&#27861;&#30340;&#20449;&#20219;&#24182;&#24378;&#35843;&#32454;&#31890;&#24230;&#35777;&#25454;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09724</link><description>&lt;p&gt;
ClaimVer&#65306;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#22768;&#26126;&#32423;&#39564;&#35777;&#21644;&#35777;&#25454;&#24402;&#22240;
&lt;/p&gt;
&lt;p&gt;
ClaimVer: Explainable Claim-Level Verification and Evidence Attribution of Text Through Knowledge Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09724
&lt;/p&gt;
&lt;p&gt;
ClaimVer&#26159;&#19968;&#20010;&#20154;&#20026;&#20013;&#24515;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#22768;&#26126;&#32423;&#39564;&#35777;&#21644;&#35777;&#25454;&#24402;&#22240;&#65292;&#33268;&#21147;&#20110;&#25552;&#39640;&#29992;&#25143;&#23545;&#25991;&#26412;&#39564;&#35777;&#26041;&#27861;&#30340;&#20449;&#20219;&#24182;&#24378;&#35843;&#32454;&#31890;&#24230;&#35777;&#25454;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24191;&#27867;&#20256;&#25773;&#30340;&#20449;&#24687;&#35823;&#23548;&#21644;&#31038;&#20132;&#23186;&#20307;&#20197;&#21450;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#28608;&#22686;&#20013;&#65292;&#39564;&#35777;&#21644;&#20449;&#20219;&#25152;&#36935;&#21040;&#30340;&#20449;&#24687;&#21464;&#24471;&#26085;&#30410;&#22256;&#38590;&#12290;&#35768;&#22810;&#20107;&#23454;&#26680;&#26597;&#26041;&#27861;&#21644;&#24037;&#20855;&#24050;&#34987;&#24320;&#21457;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#32570;&#20047;&#36866;&#24403;&#30340;&#21487;&#35299;&#37322;&#24615;&#25110;&#32454;&#31890;&#24230;&#65292;&#26080;&#27861;&#22312;&#21508;&#31181;&#24773;&#22659;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;&#19968;&#31181;&#26131;&#20110;&#20351;&#29992;&#12289;&#21487;&#35775;&#38382;&#19988;&#33021;&#22815;&#25191;&#34892;&#32454;&#31890;&#24230;&#35777;&#25454;&#24402;&#22240;&#30340;&#25991;&#26412;&#39564;&#35777;&#26041;&#27861;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#24314;&#31435;&#29992;&#25143;&#23545;&#36825;&#31181;&#26041;&#27861;&#30340;&#20449;&#20219;&#38656;&#35201;&#21576;&#29616;&#27599;&#20010;&#39044;&#27979;&#32972;&#21518;&#30340;&#29702;&#30001;&#65292;&#22240;&#20026;&#30740;&#31350;&#34920;&#26126;&#36825;&#26174;&#33879;&#24433;&#21709;&#20154;&#20204;&#23545;&#33258;&#21160;&#21270;&#31995;&#32479;&#30340;&#20449;&#20219;&#12290;&#23558;&#29992;&#25143;&#20851;&#27880;&#37325;&#28857;&#25918;&#22312;&#20855;&#20307;&#30340;&#38382;&#39064;&#20869;&#23481;&#19978;&#65292;&#32780;&#19981;&#26159;&#25552;&#20379;&#31616;&#21333;&#30340;&#31548;&#32479;&#26631;&#31614;&#20063;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$\textit{ClaimVer&#65292;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#26694;&#26550;}$&#65292;&#26088;&#22312;&#28385;&#36275;&#29992;&#25143;&#30340;&#20449;&#24687;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09724v1 Announce Type: new  Abstract: In the midst of widespread misinformation and disinformation through social media and the proliferation of AI-generated texts, it has become increasingly difficult for people to validate and trust information they encounter. Many fact-checking approaches and tools have been developed, but they often lack appropriate explainability or granularity to be useful in various contexts. A text validation method that is easy to use, accessible, and can perform fine-grained evidence attribution has become crucial. More importantly, building user trust in such a method requires presenting the rationale behind each prediction, as research shows this significantly influences people's belief in automated systems. It is also paramount to localize and bring users' attention to the specific problematic content, instead of providing simple blanket labels. In this paper, we present $\textit{ClaimVer, a human-centric framework}$ tailored to meet users' info
&lt;/p&gt;</description></item><item><title>&#20174;&#20020;&#24202;&#25991;&#26412;&#20013;&#25552;&#21462;&#29983;&#29289;&#21307;&#23398;&#27010;&#24565;&#39044;&#27979;&#24739;&#32773;&#30340;&#20877;&#20837;&#38498;&#24773;&#20917;&#65292;&#21487;&#20197;&#24110;&#21161;&#21307;&#29983;&#36873;&#25321;&#36866;&#24403;&#30340;&#27835;&#30103;&#26041;&#27861;&#65292;&#20174;&#32780;&#20943;&#23569;&#24739;&#32773;&#20877;&#27425;&#20837;&#38498;&#30340;&#27604;&#29575;&#65292;&#23454;&#29616;&#26377;&#25928;&#30340;&#27835;&#30103;&#25104;&#26412;&#38477;&#20302;&#12290;</title><link>https://arxiv.org/abs/2403.09722</link><description>&lt;p&gt;
&#20174;&#20020;&#24202;&#25991;&#26412;&#20013;&#25552;&#21462;&#29983;&#29289;&#21307;&#23398;&#27010;&#24565;&#39044;&#27979;&#24739;&#32773;&#30340;&#20877;&#20837;&#38498;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
Prediction of readmission of patients by extracting biomedical concepts from clinical texts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09722
&lt;/p&gt;
&lt;p&gt;
&#20174;&#20020;&#24202;&#25991;&#26412;&#20013;&#25552;&#21462;&#29983;&#29289;&#21307;&#23398;&#27010;&#24565;&#39044;&#27979;&#24739;&#32773;&#30340;&#20877;&#20837;&#38498;&#24773;&#20917;&#65292;&#21487;&#20197;&#24110;&#21161;&#21307;&#29983;&#36873;&#25321;&#36866;&#24403;&#30340;&#27835;&#30103;&#26041;&#27861;&#65292;&#20174;&#32780;&#20943;&#23569;&#24739;&#32773;&#20877;&#27425;&#20837;&#38498;&#30340;&#27604;&#29575;&#65292;&#23454;&#29616;&#26377;&#25928;&#30340;&#27835;&#30103;&#25104;&#26412;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#23384;&#22312;&#22823;&#37327;&#30340;&#30005;&#23376;&#20581;&#24247;&#25968;&#25454;&#20026;&#36827;&#34892;&#26088;&#22312;&#25913;&#21892;&#20026;&#24739;&#32773;&#25552;&#20379;&#30340;&#21307;&#30103;&#26381;&#21153;&#24182;&#38477;&#20302;&#21307;&#30103;&#31995;&#32479;&#25104;&#26412;&#30340;&#30740;&#31350;&#21019;&#36896;&#20102;&#28508;&#22312;&#33021;&#21147;&#12290;&#36817;&#24180;&#26469;&#21307;&#23398;&#39046;&#22495;&#22791;&#21463;&#20851;&#27880;&#30340;&#19968;&#20010;&#35805;&#39064;&#26159;&#35782;&#21035;&#20986;&#21018;&#20174;&#21307;&#38498;&#20986;&#38498;&#21518;&#21487;&#33021;&#24456;&#24555;&#20877;&#27425;&#20837;&#38498;&#30340;&#24739;&#32773;&#12290;&#36825;&#31181;&#35782;&#21035;&#21487;&#20197;&#24110;&#21161;&#21307;&#29983;&#36873;&#25321;&#36866;&#24403;&#30340;&#27835;&#30103;&#26041;&#27861;&#65292;&#20174;&#32780;&#20943;&#23569;&#24739;&#32773;&#20877;&#27425;&#20837;&#38498;&#30340;&#27604;&#29575;&#65292;&#23454;&#29616;&#26377;&#25928;&#30340;&#27835;&#30103;&#25104;&#26412;&#38477;&#20302;&#12290;&#26412;&#30740;&#31350;&#35752;&#35770;&#20102;&#21033;&#29992;&#25991;&#26412;&#25366;&#25496;&#26041;&#27861;&#21644;&#23545;&#24739;&#32773;&#30005;&#23376;&#25991;&#20214;&#20013;&#30340;&#20986;&#38498;&#25253;&#21578;&#25991;&#26412;&#36827;&#34892;&#22788;&#29702;&#26469;&#39044;&#27979;&#24739;&#32773;&#20877;&#27425;&#20837;&#38498;&#24773;&#20917;&#12290;&#20026;&#27492;&#65292;&#20351;&#29992;&#20004;&#31181;&#26041;&#27861;&#35780;&#20272;&#20102;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65306;&#35789;&#34955;&#27169;&#22411;&#21644;&#27010;&#24565;&#34955;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09722v1 Announce Type: cross  Abstract: Today, the existence of a vast amount of electronic health data has created potential capacities for conducting studies aiming to improve the medical services provided to patients and reduce the costs of the healthcare system. One of the topics that has been receiving attention in the field of medicine in recent years is the identification of patients who are likely to be re-hospitalized shortly after being discharged from the hospital. This identification can help doctors choose appropriate treatment methods, thereby reducing the rate of patient re-hospitalization and resulting in effective treatment cost reduction. In this study, the prediction of patient re-hospitalization using text mining approaches and the processing of discharge report texts in the patient's electronic file has been discussed. To this end, the performance of various machine learning models has been evaluated using two approaches: bag of word and bag of concept, 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#20041;&#25552;&#21450;&#22270;&#22686;&#24378;&#27169;&#22411;&#65288;GAM&#65289;&#65292;&#36890;&#36807;&#26500;&#24314;&#35821;&#20041;&#25552;&#21450;&#22270;&#21644;&#24341;&#20837;&#38598;&#25104;&#22270;&#21464;&#25442;&#22120;&#27169;&#22359;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#20803;&#25277;&#21462;&#20013;&#30340;&#29420;&#31435;&#24314;&#27169;&#23454;&#20307;&#25552;&#21450;&#21644;&#25991;&#26723;&#25552;&#31034;&#38548;&#31163;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.09721</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#20041;&#25552;&#21450;&#22270;&#22686;&#24378;&#27169;&#22411;&#30340;&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#20803;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
A Semantic Mention Graph Augmented Model for Document-Level Event Argument Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09721
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#20041;&#25552;&#21450;&#22270;&#22686;&#24378;&#27169;&#22411;&#65288;GAM&#65289;&#65292;&#36890;&#36807;&#26500;&#24314;&#35821;&#20041;&#25552;&#21450;&#22270;&#21644;&#24341;&#20837;&#38598;&#25104;&#22270;&#21464;&#25442;&#22120;&#27169;&#22359;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#20803;&#25277;&#21462;&#20013;&#30340;&#29420;&#31435;&#24314;&#27169;&#23454;&#20307;&#25552;&#21450;&#21644;&#25991;&#26723;&#25552;&#31034;&#38548;&#31163;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#20803;&#25277;&#21462;&#65288;DEAE&#65289;&#26088;&#22312;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26723;&#20013;&#35782;&#21035;&#35770;&#20803;&#21450;&#20854;&#29305;&#23450;&#35282;&#33394;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#20041;&#25552;&#21450;&#22270;&#22686;&#24378;&#27169;&#22411;&#65288;GAM&#65289;&#65292;&#26088;&#22312;&#35299;&#20915;DEAE&#20013;&#29420;&#31435;&#24314;&#27169;&#23454;&#20307;&#25552;&#21450;&#21644;&#25991;&#26723;&#25552;&#31034;&#38548;&#31163;&#30340;&#38382;&#39064;&#12290;GAM&#26500;&#24314;&#20102;&#19968;&#20010;&#25429;&#33719;&#25991;&#26723;&#21644;&#25552;&#31034;&#20869;&#37096;&#21450;&#38388;&#37096;&#20851;&#31995;&#30340;&#35821;&#20041;&#25552;&#21450;&#22270;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#38598;&#25104;&#30340;&#22270;&#21464;&#25442;&#22120;&#27169;&#22359;&#26469;&#26377;&#25928;&#22788;&#29702;&#25552;&#21450;&#21450;&#20854;&#19977;&#31181;&#35821;&#20041;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09721v1 Announce Type: cross  Abstract: Document-level Event Argument Extraction (DEAE) aims to identify arguments and their specific roles from an unstructured document. The advanced approaches on DEAE utilize prompt-based methods to guide pre-trained language models (PLMs) in extracting arguments from input documents. They mainly concentrate on establishing relations between triggers and entity mentions within documents, leaving two unresolved problems: a) independent modeling of entity mentions; b) document-prompt isolation. To this end, we propose a semantic mention Graph Augmented Model (GAM) to address these two problems in this paper. Firstly, GAM constructs a semantic mention graph that captures relations within and between documents and prompts, encompassing co-existence, co-reference and co-type relations. Furthermore, we introduce an ensembled graph transformer module to address mentions and their three semantic relations effectively. Later, the graph-augmented en
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;Human Value Detection 2023&#20219;&#21153;&#20013;&#30340;&#24494;&#35843;&#21644;&#25552;&#31034;&#35843;&#25972;&#30340;&#25506;&#32034;&#65292;&#39564;&#35777;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#29702;&#35299;&#20154;&#31867;&#20215;&#20540;&#35266;&#12290;</title><link>https://arxiv.org/abs/2403.09720</link><description>&lt;p&gt;
&#24494;&#35843;&#19982;&#25552;&#31034;&#65292;&#35821;&#35328;&#27169;&#22411;&#33021;&#29702;&#35299;&#20154;&#31867;&#20215;&#20540;&#35266;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning vs Prompting, Can Language Models Understand Human Values?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09720
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;Human Value Detection 2023&#20219;&#21153;&#20013;&#30340;&#24494;&#35843;&#21644;&#25552;&#31034;&#35843;&#25972;&#30340;&#25506;&#32034;&#65292;&#39564;&#35777;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#29702;&#35299;&#20154;&#31867;&#20215;&#20540;&#35266;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#22788;&#29702;&#21477;&#23376;&#20013;&#30340;&#28508;&#22312;&#25903;&#25345;&#30340;&#20215;&#20540;&#35266;&#23545;&#20110;&#29702;&#35299;&#35828;&#35805;&#32773;&#30340;&#20542;&#21521;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#20013;&#21364;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#20154;&#31867;&#20215;&#20540;&#26816;&#27979;2023&#20013;&#24494;&#35843;&#21644;&#25552;&#31034;&#35843;&#25972;&#22312;&#36825;&#19968;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23581;&#35797;&#39564;&#35777;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#26681;&#25454;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#33719;&#24471;&#30340;&#30693;&#35782;&#26377;&#25928;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#19982;RLHF&#30340;&#33021;&#21147;&#24863;&#20852;&#36259;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#21021;&#27493;&#23581;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09720v1 Announce Type: cross  Abstract: Accurately handling the underlying support values in sentences is crucial for understanding the speaker's tendencies, yet it poses a challenging task in natural language understanding (NLU). In this article, we explore the potential of fine-tuning and prompt tuning in this downstream task, using the Human Value Detection 2023. Additionally, we attempt to validate whether models can effectively solve the problem based on the knowledge acquired during the pre-training stage. Simultaneously, our interest lies in the capabilities of large language models (LLMs) aligned with RLHF in this task, and some preliminary attempts are presented.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#29992;&#20110;&#24076;&#20271;&#26469;&#35821;&#35328;&#30340;&#32467;&#35770;&#25552;&#21462;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#25688;&#35201;&#21644;&#32467;&#35770;&#25552;&#21462;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#24212;&#27169;&#22411;&#21644;&#20195;&#30721;</title><link>https://arxiv.org/abs/2403.09719</link><description>&lt;p&gt;
Mevaker: &#38024;&#23545;&#24076;&#20271;&#26469;&#35821;&#35328;&#30340;&#32467;&#35770;&#25552;&#21462;&#21644;&#20998;&#37197;&#36164;&#28304;
&lt;/p&gt;
&lt;p&gt;
Mevaker: Conclusion Extraction and Allocation Resources for the Hebrew Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09719
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#29992;&#20110;&#24076;&#20271;&#26469;&#35821;&#35328;&#30340;&#32467;&#35770;&#25552;&#21462;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#25688;&#35201;&#21644;&#32467;&#35770;&#25552;&#21462;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#24212;&#27169;&#22411;&#21644;&#20195;&#30721;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#20197;&#33394;&#21015;&#22269;&#23478;&#23457;&#35745;&#38271;&#21644;&#24033;&#26597;&#21592;&#30340;&#25253;&#21578;&#65292;&#20171;&#32461;&#20102;&#29992;&#20110;&#24076;&#20271;&#26469;&#35821;&#35328;&#30340;&#25688;&#35201;MevakerSumm&#21644;&#32467;&#35770;&#25552;&#21462;MevakerConc&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#20004;&#20010;&#36741;&#21161;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29992;&#20110;&#32467;&#35770;&#25552;&#21462;(HeConE, HeConEspc)&#21644;&#32467;&#35770;&#20998;&#37197;(HeCross)&#30340;&#27169;&#22411;&#65292;&#26412;&#24037;&#20316;&#20013;&#20351;&#29992;&#30340;&#25152;&#26377;&#20195;&#30721;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#26816;&#26597;&#28857;&#37117;&#26159;&#20844;&#24320;&#21487;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09719v1 Announce Type: cross  Abstract: In this paper, we introduce summarization MevakerSumm and conclusion extraction MevakerConc datasets for the Hebrew language based on the State Comptroller and Ombudsman of Israel reports, along with two auxiliary datasets. We accompany these datasets with models for conclusion extraction (HeConE, HeConEspc) and conclusion allocation (HeCross). All of the code, datasets, and model checkpoints used in this work are publicly available.
&lt;/p&gt;</description></item><item><title>TextCNN&#30340;&#20840;&#38754;&#23454;&#29616;&#25552;&#21319;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#19982;&#31995;&#32479;&#25512;&#33616;&#20043;&#38388;&#30340;&#21327;&#20316;&#65292;&#20026;NLP&#39046;&#22495;&#30340;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#24102;&#26469;&#20102;&#26032;&#30340;&#26631;&#20934;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2403.09718</link><description>&lt;p&gt;
&#25552;&#21319;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#19982;&#31995;&#32479;&#25512;&#33616;&#20043;&#38388;&#21327;&#20316;&#30340;TextCNN&#30340;&#20840;&#38754;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Comprehensive Implementation of TextCNN for Enhanced Collaboration between Natural Language Processing and System Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09718
&lt;/p&gt;
&lt;p&gt;
TextCNN&#30340;&#20840;&#38754;&#23454;&#29616;&#25552;&#21319;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#19982;&#31995;&#32479;&#25512;&#33616;&#20043;&#38388;&#30340;&#21327;&#20316;&#65292;&#20026;NLP&#39046;&#22495;&#30340;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#24102;&#26469;&#20102;&#26032;&#30340;&#26631;&#20934;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#26159;&#20154;&#24037;&#26234;&#33021;&#30340;&#37325;&#35201;&#20998;&#25903;&#65292;&#30740;&#31350;&#22914;&#20309;&#20351;&#35745;&#31639;&#26426;&#29702;&#35299;&#12289;&#22788;&#29702;&#21644;&#29983;&#25104;&#20154;&#31867;&#35821;&#35328;&#12290;&#25991;&#26412;&#20998;&#31867;&#26159;NLP&#20013;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#26088;&#22312;&#23558;&#25991;&#26412;&#20998;&#31867;&#20026;&#19981;&#21516;&#30340;&#39044;&#23450;&#20041;&#31867;&#21035;&#12290;&#28145;&#24230;&#23398;&#20064;&#22312;&#35768;&#22810;&#30740;&#31350;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#24182;&#25104;&#20026;NLP&#39046;&#22495;&#30340;&#26631;&#20934;&#25216;&#26415;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#12290;&#19982;&#25968;&#23383;&#21644;&#22270;&#29255;&#19981;&#21516;&#65292;&#25991;&#26412;&#22788;&#29702;&#24378;&#35843;&#31934;&#32454;&#22788;&#29702;&#33021;&#21147;&#12290;&#20256;&#32479;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#23545;&#36755;&#20837;&#27169;&#22411;&#30340;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#39044;&#22788;&#29702;&#65292;&#24182;&#36890;&#36807;&#25163;&#21160;&#26041;&#24335;&#33719;&#24471;&#33391;&#22909;&#30340;&#26679;&#26412;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09718v1 Announce Type: cross  Abstract: Natural Language Processing (NLP) is an important branch of artificial intelligence that studies how to enable computers to understand, process, and generate human language. Text classification is a fundamental task in NLP, which aims to classify text into different predefined categories. Text classification is the most basic and classic task in natural language processing, and most of the tasks in natural language processing can be regarded as classification tasks. In recent years, deep learning has achieved great success in many research fields, and today, it has also become a standard technology in the field of NLP, which is widely integrated into text classification tasks. Unlike numbers and images, text processing emphasizes fine-grained processing ability. Traditional text classification methods generally require preprocessing the input model's text data. Additionally, they also need to obtain good sample features through manual 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#23558;&#24515;&#29702;&#29366;&#24577;&#36319;&#36394;&#38598;&#25104;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20197;&#26126;&#30830;&#24341;&#23548;&#25233;&#37057;&#30151;&#35786;&#26029;&#23548;&#21521;&#30340;&#32842;&#22825;&#65292;&#20174;&#32780;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#24739;&#32773;&#22312;&#23545;&#35805;&#36807;&#31243;&#20013;&#30340;&#20449;&#24687;&#12289;&#24773;&#32490;&#25110;&#30151;&#29366;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.09717</link><description>&lt;p&gt;
&#36890;&#36807;&#24515;&#29702;&#29366;&#24577;&#36319;&#36394;&#25552;&#21319;&#38754;&#21521;&#25233;&#37057;&#30151;&#35786;&#26029;&#30340;&#32842;&#22825;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Enhancing Depression-Diagnosis-Oriented Chat with Psychological State Tracking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09717
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#23558;&#24515;&#29702;&#29366;&#24577;&#36319;&#36394;&#38598;&#25104;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20197;&#26126;&#30830;&#24341;&#23548;&#25233;&#37057;&#30151;&#35786;&#26029;&#23548;&#21521;&#30340;&#32842;&#22825;&#65292;&#20174;&#32780;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#24739;&#32773;&#22312;&#23545;&#35805;&#36807;&#31243;&#20013;&#30340;&#20449;&#24687;&#12289;&#24773;&#32490;&#25110;&#30151;&#29366;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25233;&#37057;&#30151;&#35786;&#26029;&#23548;&#21521;&#30340;&#32842;&#22825;&#26088;&#22312;&#24341;&#23548;&#24739;&#32773;&#36827;&#34892;&#33258;&#25105;&#34920;&#36798;&#65292;&#25910;&#38598;&#25233;&#37057;&#30151;&#26816;&#27979;&#30340;&#20851;&#38190;&#30151;&#29366;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#38598;&#20013;&#20110;&#32467;&#21512;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#21644;&#38386;&#32842;&#65292;&#27169;&#25311;&#22522;&#20110;&#35775;&#35848;&#30340;&#25233;&#37057;&#30151;&#35786;&#26029;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#24456;&#22909;&#22320;&#25429;&#25417;&#24739;&#32773;&#22312;&#23545;&#35805;&#36807;&#31243;&#20013;&#30340;&#20449;&#24687;&#12289;&#24773;&#32490;&#25110;&#30151;&#29366;&#30340;&#21464;&#21270;&#12290;&#27492;&#22806;&#65292;&#36824;&#27809;&#26377;&#26126;&#30830;&#30340;&#26694;&#26550;&#29992;&#20110;&#24341;&#23548;&#23545;&#35805;&#65292;&#36825;&#23548;&#33268;&#19968;&#20123;&#26080;&#29992;&#30340;&#20132;&#27969;&#24433;&#21709;&#20102;&#20307;&#39564;&#12290;&#26412;&#25991;&#25552;&#20986;&#23558;&#24515;&#29702;&#29366;&#24577;&#36319;&#36394;&#65288;POST&#65289;&#38598;&#25104;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#65292;&#20197;&#26126;&#30830;&#24341;&#23548;&#25233;&#37057;&#30151;&#35786;&#26029;&#23548;&#21521;&#30340;&#32842;&#22825;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#29366;&#24577;&#28304;&#33258;&#20110;&#24515;&#29702;&#29702;&#35770;&#27169;&#22411;&#65292;&#21253;&#25324;&#22235;&#20010;&#32452;&#20214;&#65292;&#21363;&#38454;&#27573;&#12289;&#20449;&#24687;&#12289;&#24635;&#32467;&#21644;&#19979;&#19968;&#27493;&#12290;&#25105;&#20204;&#24494;&#35843;&#20102;&#19968;&#20010;LLM&#27169;&#22411;&#20197;&#29983;&#25104;&#21160;&#24577;&#24515;&#29702;&#29366;&#24577;&#65292;&#36827;&#32780;&#29992;&#20110;&#36741;&#21161;r
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09717v1 Announce Type: cross  Abstract: Depression-diagnosis-oriented chat aims to guide patients in self-expression to collect key symptoms for depression detection. Recent work focuses on combining task-oriented dialogue and chitchat to simulate the interview-based depression diagnosis. Whereas, these methods can not well capture the changing information, feelings, or symptoms of the patient during dialogues. Moreover, no explicit framework has been explored to guide the dialogue, which results in some useless communications that affect the experience. In this paper, we propose to integrate Psychological State Tracking (POST) within the large language model (LLM) to explicitly guide depression-diagnosis-oriented chat. Specifically, the state is adapted from a psychological theoretical model, which consists of four components, namely Stage, Information, Summary and Next. We fine-tune an LLM model to generate the dynamic psychological state, which is further used to assist r
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25991;&#26412;&#20998;&#26512;&#29992;&#25143;&#35768;&#21487;&#21327;&#35758;&#65292;&#35782;&#21035;&#28508;&#22312;&#24694;&#24847;&#36719;&#20214;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#30417;&#30563;&#20998;&#31867;&#22120;&#23545;&#20854;&#36827;&#34892;&#20998;&#31867;&#65292;&#20026;&#35299;&#20915;EULA&#36807;&#38271;&#19988;&#38590;&#20197;&#29702;&#35299;&#30340;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.09715</link><description>&lt;p&gt;
&#25991;&#26412;&#20998;&#26512;&#29992;&#25143;&#35768;&#21487;&#21327;&#35758;&#29992;&#20110;&#26631;&#35760;&#28508;&#22312;&#24694;&#24847;&#36719;&#20214;
&lt;/p&gt;
&lt;p&gt;
Textual analysis of End User License Agreement for red-flagging potentially malicious software
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09715
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25991;&#26412;&#20998;&#26512;&#29992;&#25143;&#35768;&#21487;&#21327;&#35758;&#65292;&#35782;&#21035;&#28508;&#22312;&#24694;&#24847;&#36719;&#20214;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#30417;&#30563;&#20998;&#31867;&#22120;&#23545;&#20854;&#36827;&#34892;&#20998;&#31867;&#65292;&#20026;&#35299;&#20915;EULA&#36807;&#38271;&#19988;&#38590;&#20197;&#29702;&#35299;&#30340;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27599;&#22825;&#29992;&#25143;&#20250;&#19979;&#36733;&#26032;&#36719;&#20214;&#21644;&#26356;&#26032;&#65292;&#27599;&#20010;&#19979;&#36733;&#30340;&#36719;&#20214;&#37117;&#38468;&#24102;&#19968;&#20221;&#29992;&#25143;&#35768;&#21487;&#21327;&#35758;&#65288;EULA&#65289;&#65292;&#20294;&#36825;&#32463;&#24120;&#34987;&#24573;&#30053;&#12290;EULA&#21253;&#21547;&#30340;&#20449;&#24687;&#26159;&#20026;&#20102;&#36991;&#20813;&#27861;&#24459;&#36131;&#20219;&#65292;&#28982;&#32780;&#65292;&#36825;&#21487;&#33021;&#24102;&#26469;&#19968;&#31995;&#21015;&#28508;&#22312;&#38382;&#39064;&#65292;&#27604;&#22914;&#38388;&#35853;&#36719;&#20214;&#25110;&#23545;&#30446;&#26631;&#31995;&#32479;&#20135;&#29983;&#38750;&#39044;&#26399;&#24433;&#21709;&#12290;&#29992;&#25143;&#19981;&#35835;EULA&#26159;&#22240;&#20026;&#25991;&#26723;&#22826;&#38271;&#65292;&#38590;&#20197;&#29702;&#35299;&#65292;&#25991;&#26412;&#25688;&#35201;&#26159;&#36825;&#31181;&#38382;&#39064;&#30340;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#27010;&#25324;EULA&#24182;&#23558;&#20854;&#20998;&#31867;&#20026;&#8220;&#33391;&#24615;&#8221;&#25110;&#8220;&#24694;&#24847;&#8221;&#12290;&#25105;&#20204;&#25552;&#21462;&#20102;&#19981;&#21516;&#36719;&#20214;&#30340;EULA&#25991;&#26412;&#65292;&#28982;&#21518;&#20351;&#29992;&#20843;&#31181;&#30417;&#30563;&#20998;&#31867;&#22120;&#23545;&#25991;&#26412;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#20351;&#29992;&#38598;&#25104;&#23398;&#20064;&#23558;EULA&#20998;&#31867;&#20026;&#33391;&#24615;&#25110;&#24694;&#24847;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09715v1 Announce Type: cross  Abstract: New software and updates are downloaded by end users every day. Each dowloaded software has associated with it an End Users License Agreements (EULA), but this is rarely read. An EULA includes information to avoid legal repercussions. However,this proposes a host of potential problems such as spyware or producing an unwanted affect in the target system. End users do not read these EULA's because of length of the document and users find it extremely difficult to understand. Text summarization is one of the relevant solution to these kind of problems. This require a solution which can summarize the EULA and classify the EULA as "Benign" or "Malicious". We propose a solution in which we have summarize the EULA and classify the EULA as "Benign" or "Malicious". We extract EULA text of different sofware's then we classify the text using eight different supervised classifiers. we use ensemble learning to classify the EULA as benign or malicio
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#20197;&#38750;&#30417;&#30563;&#26041;&#24335;&#29983;&#25104;&#21477;&#27861;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#25968;&#20540;&#34920;&#31034;&#30701;&#35821;&#26641;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.09714</link><description>&lt;p&gt;
&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#35825;&#23548;&#35821;&#35328;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Linguistic Structure Induction from Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09714
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#20197;&#38750;&#30417;&#30563;&#26041;&#24335;&#29983;&#25104;&#21477;&#27861;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#25968;&#20540;&#34920;&#31034;&#30701;&#35821;&#26641;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22823;&#33041;&#20013;&#38544;&#24335;&#22320;&#36890;&#36807;&#20998;&#23618;&#32467;&#26500;&#26469;&#32452;&#32455;&#21333;&#35789;&#22312;&#21477;&#23376;&#20013;&#30340;&#32452;&#25104;&#65292;&#32780;&#36825;&#20123;&#32467;&#26500;&#24418;&#25104;&#20102;&#21333;&#35789;&#30340;&#32447;&#24615;&#24207;&#21015;&#12290;&#35821;&#35328;&#23398;&#23478;&#20204;&#24418;&#24335;&#21270;&#20102;&#19981;&#21516;&#30340;&#26694;&#26550;&#26469;&#27169;&#25311;&#36825;&#31181;&#23618;&#27425;&#32467;&#26500;&#65307;&#20854;&#20013;&#26368;&#24120;&#35265;&#30340;&#20004;&#31181;&#21477;&#27861;&#26694;&#26550;&#26159;&#30701;&#35821;&#32467;&#26500;&#21644;&#20381;&#23384;&#32467;&#26500;&#12290;&#30701;&#35821;&#32467;&#26500;&#23558;&#21477;&#23376;&#34920;&#31034;&#20026;&#30701;&#35821;&#30340;&#23884;&#22871;&#32452;&#65292;&#32780;&#20381;&#23384;&#32467;&#26500;&#36890;&#36807;&#20026;&#21333;&#35789;&#20043;&#38388;&#20998;&#37197;&#20851;&#31995;&#26469;&#34920;&#31034;&#21477;&#23376;&#12290;&#26368;&#36817;&#65292;&#23545;&#26234;&#33021;&#26426;&#22120;&#30340;&#36861;&#27714;&#20135;&#29983;&#20102;&#33021;&#22815;&#20197;&#20154;&#31867;&#27700;&#24179;&#23436;&#25104;&#35768;&#22810;&#35821;&#35328;&#20219;&#21153;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#12290;&#35768;&#22810;&#30740;&#31350;&#29616;&#22312;&#36136;&#30097;LMs&#26159;&#21542;&#38544;&#24335;&#22320;&#34920;&#31034;&#21477;&#27861;&#23618;&#27425;&#32467;&#26500;&#12290;&#26412;&#35770;&#25991;&#38598;&#20013;&#20110;&#22312;&#38750;&#30417;&#30563;&#35774;&#32622;&#20013;&#20174;LMs&#20013;&#29983;&#25104;&#30701;&#35821;&#32467;&#26500;&#21644;&#20381;&#23384;&#32467;&#26500;&#12290;&#25105;&#22238;&#39038;&#20102;&#35813;&#39046;&#22495;&#30340;&#20851;&#38190;&#26041;&#27861;&#24182;&#37325;&#28857;&#20171;&#32461;&#20102;&#19968;&#39033;&#21033;&#29992;&#20108;&#20803;&#30701;&#35821;&#32467;&#26500;&#26641;&#30340;&#25968;&#20540;&#34920;&#31034;&#65288;&#35821;&#27861;&#36317;&#31163;&#65289;&#30340;&#24037;&#20316;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09714v1 Announce Type: cross  Abstract: Linear sequences of words are implicitly represented in our brains by hierarchical structures that organize the composition of words in sentences. Linguists formalize different frameworks to model this hierarchy; two of the most common syntactic frameworks are Constituency and Dependency. Constituency represents sentences as nested groups of phrases, while dependency represents a sentence by assigning relations between its words. Recently, the pursuit of intelligent machines has produced Language Models (LMs) capable of solving many language tasks with a human-level performance. Many studies now question whether LMs implicitly represent syntactic hierarchies. This thesis focuses on producing constituency and dependency structures from LMs in an unsupervised setting. I review the critical methods in this field and highlight a line of work that utilizes a numerical representation for binary constituency trees (Syntactic Distance). I pres
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;(&#20154;&#31867;+AI)&#26041;&#27861;HyEnA&#65292;&#29992;&#20110;&#20174;&#24847;&#35265;&#25991;&#26412;&#20013;&#25552;&#21462;&#35770;&#28857;&#65292;&#32467;&#21512;&#20102;&#33258;&#21160;&#21270;&#22788;&#29702;&#36895;&#24230;&#21644;&#20154;&#31867;&#29702;&#35299;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#20844;&#27665;&#21453;&#39304;&#35821;&#26009;&#24211;&#19978;&#21462;&#24471;&#20102;&#26356;&#39640;&#30340;&#35206;&#30422;&#29575;&#21644;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.09713</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#35770;&#35777;&#25366;&#25496;&#30340;&#28151;&#21512;&#26234;&#33021;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Hybrid Intelligence Method for Argument Mining
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09713
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;(&#20154;&#31867;+AI)&#26041;&#27861;HyEnA&#65292;&#29992;&#20110;&#20174;&#24847;&#35265;&#25991;&#26412;&#20013;&#25552;&#21462;&#35770;&#28857;&#65292;&#32467;&#21512;&#20102;&#33258;&#21160;&#21270;&#22788;&#29702;&#36895;&#24230;&#21644;&#20154;&#31867;&#29702;&#35299;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#20844;&#27665;&#21453;&#39304;&#35821;&#26009;&#24211;&#19978;&#21462;&#24471;&#20102;&#26356;&#39640;&#30340;&#35206;&#30422;&#29575;&#21644;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35843;&#26597;&#24037;&#20855;&#33021;&#22815;&#25910;&#38598;&#20844;&#27665;&#21453;&#39304;&#24847;&#35265;&#35821;&#26009;&#24211;&#12290;&#20174;&#24222;&#22823;&#19988;&#22024;&#26434;&#30340;&#24847;&#35265;&#38598;&#20013;&#25552;&#21462;&#20851;&#38190;&#35770;&#28857;&#26377;&#21161;&#20110;&#24555;&#36895;&#20934;&#30830;&#22320;&#29702;&#35299;&#24847;&#35265;&#12290;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#21462;&#35770;&#28857;&#65292;&#20294;(1)&#38656;&#35201;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#23548;&#33268;&#36739;&#39640;&#30340;&#27880;&#37322;&#25104;&#26412;; (2)&#23545;&#24050;&#30693;&#35266;&#28857;&#25928;&#26524;&#33391;&#22909;&#65292;&#20294;&#23545;&#26032;&#39062;&#35266;&#28857;&#25928;&#26524;&#27424;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;HyEnA&#65292;&#19968;&#31181;&#28151;&#21512;(&#20154;&#31867;+AI)&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#20027;&#35266;&#25991;&#26412;&#20013;&#25552;&#21462;&#35770;&#28857;&#65292;&#32467;&#21512;&#20102;&#33258;&#21160;&#21270;&#22788;&#29702;&#30340;&#36895;&#24230;&#21644;&#20154;&#31867;&#30340;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#20844;&#27665;&#21453;&#39304;&#35821;&#26009;&#24211;&#19978;&#35780;&#20272;&#20102;HyEnA&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19968;&#26041;&#38754;&#65292;&#19982;&#19968;&#32452;&#21508;&#31181;&#24847;&#35265;&#36827;&#34892;&#27604;&#36739;&#26102;&#65292;HyEnA&#22312;&#39640;&#35206;&#30422;&#29575;&#21644;&#20934;&#30830;&#29575;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#35777;&#23454;&#20102;&#20154;&#31867;&#27934;&#23519;&#30340;&#24517;&#35201;&#24615;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;HyEnA&#38656;&#35201;&#36739;&#23569;&#30340;&#20154;&#21147;&#24037;&#20316;&#37327;&#65292;&#19988;&#19981;&#20250;&#29306;&#29298;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09713v1 Announce Type: new  Abstract: Large-scale survey tools enable the collection of citizen feedback in opinion corpora. Extracting the key arguments from a large and noisy set of opinions helps in understanding the opinions quickly and accurately. Fully automated methods can extract arguments but (1) require large labeled datasets that induce large annotation costs and (2) work well for known viewpoints, but not for novel points of view. We propose HyEnA, a hybrid (human + AI) method for extracting arguments from opinionated texts, combining the speed of automated processing with the understanding and reasoning capabilities of humans. We evaluate HyEnA on three citizen feedback corpora. We find that, on the one hand, HyEnA achieves higher coverage and precision than a state-of-the-art automated method when compared to a common set of diverse opinions, justifying the need for human insight. On the other hand, HyEnA requires less human effort and does not compromise quali
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#30693;&#35782;&#27880;&#20837;&#35838;&#31243;&#39044;&#35757;&#32451;&#26694;&#26550;&#65288;KICP&#65289;&#65292;&#29992;&#20110;&#23454;&#29616;&#20840;&#38754;&#30340;&#30693;&#35782;&#22270;&#35889;&#23398;&#20064;&#21644;&#21033;&#29992;&#20197;&#35299;&#20915;KBQA&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2403.09712</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#38382;&#31572;&#30340;&#30693;&#35782;&#27880;&#20837;&#35838;&#31243;&#39044;&#35757;&#32451;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Knowledge-Injected Curriculum Pretraining Framework for Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09712
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#30693;&#35782;&#27880;&#20837;&#35838;&#31243;&#39044;&#35757;&#32451;&#26694;&#26550;&#65288;KICP&#65289;&#65292;&#29992;&#20110;&#23454;&#29616;&#20840;&#38754;&#30340;&#30693;&#35782;&#22270;&#35889;&#23398;&#20064;&#21644;&#21033;&#29992;&#20197;&#35299;&#20915;KBQA&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#27880;&#20837;&#38382;&#31572;&#65288;KBQA&#65289;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#65292;&#20063;&#26159;&#19968;&#31181;&#35775;&#38382;&#32593;&#32476;&#25968;&#25454;&#21644;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#38656;&#35201;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#36827;&#34892;&#25512;&#29702;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#30693;&#35782;&#27880;&#20837;&#35838;&#31243;&#39044;&#35757;&#32451;&#26694;&#26550;&#65288;KICP&#65289;&#65292;&#26088;&#22312;&#23454;&#29616;&#23545;KBQA&#20219;&#21153;&#30340;&#20840;&#38754;KG&#23398;&#20064;&#21644;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09712v1 Announce Type: cross  Abstract: Knowledge-based question answering (KBQA) is a key task in NLP research, and also an approach to access the web data and knowledge, which requires exploiting knowledge graphs (KGs) for reasoning. In the literature, one promising solution for KBQA is to incorporate the pretrained language model (LM) with KGs by generating KG-centered pretraining corpus, which has shown its superiority. However, these methods often depend on specific techniques and resources to work, which may not always be available and restrict its application. Moreover, existing methods focus more on improving language understanding with KGs, while neglect the more important human-like complex reasoning. To this end, in this paper, we propose a general Knowledge-Injected Curriculum Pretraining framework (KICP) to achieve comprehensive KG learning and exploitation for KBQA tasks, which is composed of knowledge injection (KI), knowledge adaptation (KA) and curriculum re
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#28151;&#21512;&#20195;&#30721;Hinglish&#35780;&#35770;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#39044;&#22788;&#29702;&#21644;&#25506;&#32034;&#24615;&#25968;&#25454;&#20998;&#26512;&#25216;&#26415;&#24471;&#20986;&#22312;&#27424;&#36164;&#28304;&#35821;&#35328;&#20013;&#26816;&#27979;&#22899;&#24615;&#24974;&#24680;&#35780;&#35770;&#30340;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.09709</link><description>&lt;p&gt;
&#23545;&#28151;&#21512;&#20195;&#30721;&#30340;&#22899;&#24615;&#24974;&#24680;&#35780;&#35770;&#36827;&#34892;&#25506;&#32034;&#24615;&#25968;&#25454;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Exploratory Data Analysis on Code-mixed Misogynistic Comments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09709
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#28151;&#21512;&#20195;&#30721;Hinglish&#35780;&#35770;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#39044;&#22788;&#29702;&#21644;&#25506;&#32034;&#24615;&#25968;&#25454;&#20998;&#26512;&#25216;&#26415;&#24471;&#20986;&#22312;&#27424;&#36164;&#28304;&#35821;&#35328;&#20013;&#26816;&#27979;&#22899;&#24615;&#24974;&#24680;&#35780;&#35770;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#22312;&#32447;&#20167;&#24680;&#35328;&#35770;&#21644;&#32593;&#32476;&#27450;&#20940;&#38382;&#39064;&#20197;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#65288;&#22914;YouTube&#21644;Twitter&#65289;&#30340;&#26222;&#21450;&#26085;&#30410;&#21152;&#21095;&#20197;&#26469;&#65292;&#22899;&#24615;&#26356;&#26377;&#21487;&#33021;&#25104;&#20026;&#22312;&#32447;&#34384;&#24453;&#30340;&#21463;&#23475;&#32773;&#12290;&#28982;&#32780;&#65292;&#22312;&#38024;&#23545;&#27424;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#22899;&#24615;&#20167;&#24680;&#26816;&#27979;&#26041;&#38754;&#32570;&#20047;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;YouTube&#35780;&#35770;&#30340;&#26032;&#39062;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20174;&#34987;&#24369;&#26631;&#35760;&#20026;&#8220;&#22899;&#24615;&#24974;&#24680;&#8221;&#21644;&#8220;&#38750;&#22899;&#24615;&#24974;&#24680;&#8221;&#30340;YouTube&#35270;&#39057;&#20013;&#25910;&#38598;&#30340;&#28151;&#21512;&#20195;&#30721;Hinglish&#35780;&#35770;&#12290;&#23545;&#25968;&#25454;&#38598;&#24212;&#29992;&#20102;&#39044;&#22788;&#29702;&#21644;&#25506;&#32034;&#24615;&#25968;&#25454;&#20998;&#26512;&#65288;EDA&#65289;&#25216;&#26415;&#20197;&#33719;&#21462;&#20854;&#29305;&#24449;&#30340;&#35265;&#35299;&#12290;&#36825;&#19968;&#36807;&#31243;&#36890;&#36807;&#24773;&#24863;&#20998;&#25968;&#12289;&#35789;&#20113;&#31561;&#25552;&#20379;&#20102;&#23545;&#25968;&#25454;&#38598;&#30340;&#26356;&#22909;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09709v1 Announce Type: new  Abstract: The problems of online hate speech and cyberbullying have significantly worsened since the increase in popularity of social media platforms such as YouTube and Twitter (X). Natural Language Processing (NLP) techniques have proven to provide a great advantage in automatic filtering such toxic content. Women are disproportionately more likely to be victims of online abuse. However, there appears to be a lack of studies that tackle misogyny detection in under-resourced languages. In this short paper, we present a novel dataset of YouTube comments in mix-code Hinglish collected from YouTube videos which have been weak labelled as `Misogynistic' and `Non-misogynistic'. Pre-processing and Exploratory Data Analysis (EDA) techniques have been applied on the dataset to gain insights on its characteristics. The process has provided a better understanding of the dataset through sentiment scores, word clouds, etc.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26032;&#22411;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31639;&#27861;&#31649;&#36947;&#65292;&#30740;&#31350;&#23545;108,280&#20221;&#20020;&#24202;&#35760;&#24405;&#36827;&#34892;&#20998;&#26512;&#65292;&#21457;&#29616;ICIs&#27835;&#30103;&#24739;&#32773;&#20013;IrAEs&#30340;&#21457;&#29983;&#24773;&#20917;&#65292;&#24182;&#36827;&#34892;&#20102;&#27835;&#30103;&#20013;&#26029;&#29575;&#21644;&#29983;&#23384;&#26354;&#32447;&#30340;&#26500;&#24314;&#12290;</title><link>https://arxiv.org/abs/2403.09708</link><description>&lt;p&gt;
&#21033;&#29992;&#26032;&#22411;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31639;&#27861;&#31649;&#36947;&#23545;&#20813;&#30123;&#26816;&#26597;&#28857;&#25233;&#21046;&#21058;IrAEs&#36827;&#34892;&#26426;&#26500;&#32423;&#30417;&#27979;
&lt;/p&gt;
&lt;p&gt;
Institutional-Level Monitoring of Immune Checkpoint Inhibitor IrAEs Using a Novel Natural Language Processing Algorithmic Pipeline
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09708
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26032;&#22411;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31639;&#27861;&#31649;&#36947;&#65292;&#30740;&#31350;&#23545;108,280&#20221;&#20020;&#24202;&#35760;&#24405;&#36827;&#34892;&#20998;&#26512;&#65292;&#21457;&#29616;ICIs&#27835;&#30103;&#24739;&#32773;&#20013;IrAEs&#30340;&#21457;&#29983;&#24773;&#20917;&#65292;&#24182;&#36827;&#34892;&#20102;&#27835;&#30103;&#20013;&#26029;&#29575;&#21644;&#29983;&#23384;&#26354;&#32447;&#30340;&#26500;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#20813;&#30123;&#26816;&#26597;&#28857;&#25233;&#21046;&#21058;&#65288;ICIs&#65289;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#30284;&#30151;&#27835;&#30103;&#65292;&#20294;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#30340;&#20813;&#30123;&#30456;&#20851;&#19981;&#33391;&#20107;&#20214;&#65288;IrAEs&#65289;&#12290;&#30417;&#27979;IrAEs&#30340;&#21457;&#29983;&#23545;&#20110;&#20010;&#24615;&#21270;&#39118;&#38505;&#35780;&#20272;&#20197;&#21450;&#21327;&#21161;&#27835;&#30103;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#25509;&#21463;ICIs&#27835;&#30103;&#30340;Tel Aviv Sourasky&#21307;&#30103;&#20013;&#24515;&#24739;&#32773;&#30340;&#20020;&#24202;&#35760;&#24405;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31639;&#27861;&#31649;&#36947;&#31995;&#32479;&#24615;&#22320;&#35782;&#21035;&#20102;&#19971;&#31181;&#24120;&#35265;&#25110;&#20005;&#37325;&#30340;IrAEs&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#30382;&#36136;&#31867;&#22266;&#37255;&#30340;&#20351;&#29992;&#24773;&#20917;&#65292;&#20197;&#21450;IrAEs&#21518;&#30340;&#27835;&#30103;&#20013;&#26029;&#29575;&#65292;&#24182;&#26500;&#24314;&#20102;&#29983;&#23384;&#26354;&#32447;&#20197;&#21487;&#35270;&#21270;&#27835;&#30103;&#36807;&#31243;&#20013;&#19981;&#33391;&#20107;&#20214;&#30340;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09708v1 Announce Type: new  Abstract: Background: Immune checkpoint inhibitors (ICIs) have revolutionized cancer treatment but can result in severe immune-related adverse events (IrAEs). Monitoring IrAEs on a large scale is essential for personalized risk profiling and assisting in treatment decisions.   Methods: In this study, we conducted an analysis of clinical notes from patients who received ICIs at the Tel Aviv Sourasky Medical Center. By employing a Natural Language Processing algorithmic pipeline, we systematically identified seven common or severe IrAEs. We examined the utilization of corticosteroids, treatment discontinuation rates following IrAEs, and constructed survival curves to visualize the occurrence of adverse events during treatment.   Results: Our analysis encompassed 108,280 clinical notes associated with 1,635 patients who had undergone ICI therapy. The detected incidence of IrAEs was consistent with previous reports, exhibiting substantial variation ac
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#22797;&#26434;SQL&#26597;&#35810;&#30340;schema-aware&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#35774;&#35745;schema&#38142;&#25509;&#37492;&#21035;&#22120;&#21644;&#23450;&#20041;6&#31181;&#20851;&#31995;&#31867;&#22411;&#26469;&#35299;&#20915;&#25991;&#26412;&#21040;SQL&#35299;&#26512;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.09706</link><description>&lt;p&gt;
&#22797;&#26434;&#25991;&#26412;&#21040;SQL&#30340;Schema-Aware&#22810;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Schema-Aware Multi-Task Learning for Complex Text-to-SQL
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09706
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#22797;&#26434;SQL&#26597;&#35810;&#30340;schema-aware&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#35774;&#35745;schema&#38142;&#25509;&#37492;&#21035;&#22120;&#21644;&#23450;&#20041;6&#31181;&#20851;&#31995;&#31867;&#22411;&#26469;&#35299;&#20915;&#25991;&#26412;&#21040;SQL&#35299;&#26512;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#25991;&#26412;&#21040;SQL&#35299;&#26512;&#22120;&#22312;&#21512;&#25104;&#28041;&#21450;&#22810;&#20010;&#34920;&#25110;&#21015;&#30340;&#22797;&#26434;SQL&#26597;&#35810;&#26102;&#34920;&#29616;&#19981;&#20339;&#65292;&#36825;&#26159;&#22240;&#20026;&#35782;&#21035;&#27491;&#30830;&#30340;schema&#39033;&#21644;&#22312;&#38382;&#39064;&#19982;schema&#39033;&#20043;&#38388;&#36827;&#34892;&#20934;&#30830;&#23545;&#40784;&#30340;&#25361;&#25112;&#22266;&#26377;&#12290;&#20026;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#22797;&#26434;SQL&#26597;&#35810;&#30340;schema-aware&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#65288;&#31216;&#20026;MTSQL&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;schema&#38142;&#25509;&#37492;&#21035;&#22120;&#27169;&#22359;&#26469;&#21306;&#20998;&#26377;&#25928;&#30340;&#38382;&#39064;-schema&#38142;&#25509;&#65292;&#36890;&#36807;&#29420;&#29305;&#30340;&#38142;&#25509;&#20851;&#31995;&#26126;&#30830;&#25351;&#23548;&#32534;&#30721;&#22120;&#20197;&#22686;&#24378;&#23545;&#40784;&#36136;&#37327;&#12290;&#22312;&#35299;&#30721;&#22120;&#26041;&#38754;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;6&#31181;&#31867;&#22411;&#30340;&#20851;&#31995;&#26469;&#25551;&#36848;&#34920;&#21644;&#21015;&#20043;&#38388;&#30340;&#36830;&#25509;&#65288;&#20363;&#22914;&#65292;WHERE_TC&#65289;&#65292;&#24182;&#24341;&#20837;&#20102;&#20197;&#25805;&#20316;&#31526;&#20026;&#20013;&#24515;&#30340;&#19977;&#20803;&#25552;&#21462;&#22120;&#26469;&#35782;&#21035;&#37027;&#20123;&#19982;&#39044;&#23450;&#20041;&#20851;&#31995;&#30456;&#20851;&#30340;schema&#39033;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#39044;&#27979;&#30340;&#19977;&#20803;&#32452;&#24314;&#31435;&#20102;&#19968;&#32452;&#35821;&#27861;&#32422;&#26463;&#35268;&#21017;&#38598;&#65292;&#20197;&#36807;&#28388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09706v1 Announce Type: cross  Abstract: Conventional text-to-SQL parsers are not good at synthesizing complex SQL queries that involve multiple tables or columns, due to the challenges inherent in identifying the correct schema items and performing accurate alignment between question and schema items. To address the above issue, we present a schema-aware multi-task learning framework (named MTSQL) for complicated SQL queries. Specifically, we design a schema linking discriminator module to distinguish the valid question-schema linkings, which explicitly instructs the encoder by distinctive linking relations to enhance the alignment quality. On the decoder side, we define 6-type relationships to describe the connections between tables and columns (e.g., WHERE_TC), and introduce an operator-centric triple extractor to recognize those associated schema items with the predefined relationship. Also, we establish a rule set of grammar constraints via the predicted triples to filte
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#24494;&#22937;&#23545;&#35805;&#33021;&#21147;&#30340;&#26032;&#26694;&#26550;&#65292;&#24182;&#23637;&#31034;GPT4 Turbo&#21487;&#20197;&#19982;&#24050;&#39564;&#35777;&#30340;&#27835;&#30103;&#24072;&#34920;&#29616;&#20986;&#26356;&#30456;&#20284;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.09705</link><description>&lt;p&gt;
&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#30340;&#26032;&#39062;&#32454;&#33268;&#23545;&#35805;&#35780;&#20272;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Novel Nuanced Conversation Evaluation Framework for Large Language Models in Mental Health
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09705
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#24494;&#22937;&#23545;&#35805;&#33021;&#21147;&#30340;&#26032;&#26694;&#26550;&#65292;&#24182;&#23637;&#31034;GPT4 Turbo&#21487;&#20197;&#19982;&#24050;&#39564;&#35777;&#30340;&#27835;&#30103;&#24072;&#34920;&#29616;&#20986;&#26356;&#30456;&#20284;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23545;&#35805;&#33021;&#21147;&#21487;&#20197;&#24110;&#21161;&#20854;&#26356;&#35880;&#24910;&#21644;&#36866;&#24403;&#22320;&#37096;&#32626;&#65292;&#23545;&#20110;&#20687;&#24515;&#29702;&#20581;&#24247;&#36825;&#26679;&#30340;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#23588;&#20026;&#37325;&#35201;&#65292;&#20854;&#20013;&#26576;&#20154;&#30340;&#29983;&#21629;&#21487;&#33021;&#21462;&#20915;&#20110;&#23545;&#32039;&#24613;&#38382;&#39064;&#22238;&#22797;&#30340;&#30830;&#20999;&#25514;&#36766;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;LLMs&#24494;&#22937;&#23545;&#35805;&#33021;&#21147;&#30340;&#26032;&#22411;&#26694;&#26550;&#12290;&#22312;&#20854;&#20013;&#65292;&#25105;&#20204;&#20174;&#24515;&#29702;&#27835;&#30103;&#23545;&#35805;&#20998;&#26512;&#25991;&#29486;&#20013;&#21457;&#23637;&#20102;&#19968;&#31995;&#21015;&#23450;&#37327;&#25351;&#26631;&#12290;&#34429;&#28982;&#25105;&#20204;&#30830;&#20445;&#25105;&#20204;&#30340;&#26694;&#26550;&#21644;&#25351;&#26631;&#21487;&#20379;&#30740;&#31350;&#20154;&#21592;&#36716;&#31227;&#21040;&#30456;&#20851;&#37051;&#22495;&#65292;&#25105;&#20204;&#23558;&#23427;&#20204;&#24212;&#29992;&#21040;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#39564;&#35777;&#30340;&#24515;&#29702;&#20581;&#24247;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#20960;&#31181;&#27969;&#34892;&#30340;&#21069;&#27839;LLMs&#27169;&#22411;&#65292;&#21253;&#25324;&#19968;&#20123;GPT&#21644;Llama&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;GPT4 Turbo&#22312;&#34920;&#29616;&#19978;&#19982;&#24050;&#39564;&#35777;&#30340;&#27835;&#30103;&#24072;&#30456;&#27604;&#19982;&#20854;&#20182;&#27169;&#22411;&#26356;&#20026;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09705v1 Announce Type: cross  Abstract: Understanding the conversation abilities of Large Language Models (LLMs) can help lead to its more cautious and appropriate deployment. This is especially important for safety-critical domains like mental health, where someone's life may depend on the exact wording of a response to an urgent question. In this paper, we propose a novel framework for evaluating the nuanced conversation abilities of LLMs. Within it, we develop a series of quantitative metrics developed from literature on using psychotherapy conversation analysis literature. While we ensure that our framework and metrics are transferable by researchers to relevant adjacent domains, we apply them to the mental health field. We use our framework to evaluate several popular frontier LLMs, including some GPT and Llama models, through a verified mental health dataset. Our results show that GPT4 Turbo can perform significantly more similarly to verified therapists than other sel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; Alignment Studio &#26550;&#26500;&#65292;&#20351;&#24212;&#29992;&#24320;&#21457;&#32773;&#33021;&#22815;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33267;&#20182;&#20204;&#29305;&#23450;&#30340;&#20215;&#20540;&#35266;&#12289;&#31038;&#20250;&#35268;&#33539;&#12289;&#27861;&#24459;&#21644;&#20854;&#20182;&#27861;&#35268;&#65292;&#24182;&#21327;&#35843;&#28508;&#22312;&#20914;&#31361;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2403.09704</link><description>&lt;p&gt;
&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33267;&#29305;&#23450;&#24773;&#22659;&#35268;&#33539;&#30340; Alignment Studio
&lt;/p&gt;
&lt;p&gt;
Alignment Studio: Aligning Large Language Models to Particular Contextual Regulations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; Alignment Studio &#26550;&#26500;&#65292;&#20351;&#24212;&#29992;&#24320;&#21457;&#32773;&#33021;&#22815;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33267;&#20182;&#20204;&#29305;&#23450;&#30340;&#20215;&#20540;&#35266;&#12289;&#31038;&#20250;&#35268;&#33539;&#12289;&#27861;&#24459;&#21644;&#20854;&#20182;&#27861;&#35268;&#65292;&#24182;&#21327;&#35843;&#28508;&#22312;&#20914;&#31361;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#40784;&#36890;&#24120;&#30001;&#27169;&#22411;&#25552;&#20379;&#32773;&#36827;&#34892;&#65292;&#20197;&#28155;&#21152;&#25110;&#25511;&#21046;&#36328;&#29992;&#20363;&#21644;&#24773;&#22659;&#20013;&#36890;&#29992;&#25110;&#26222;&#36941;&#29702;&#35299;&#30340;&#34892;&#20026;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#21644;&#26550;&#26500;&#65292;&#36171;&#20104;&#24212;&#29992;&#24320;&#21457;&#32773;&#35843;&#25972;&#27169;&#22411;&#33267;&#20854;&#29305;&#23450;&#20215;&#20540;&#35266;&#12289;&#31038;&#20250;&#35268;&#33539;&#12289;&#27861;&#24459;&#21644;&#20854;&#20182;&#27861;&#35268;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#24773;&#22659;&#20013;&#21327;&#35843;&#28508;&#22312;&#20914;&#31361;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#38416;&#36848;&#20102;&#36825;&#31181;&#23545;&#40784;&#24037;&#20316;&#23460;&#26550;&#26500;&#30340;&#19977;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65306;&#26500;&#26550;&#32773;&#12289;&#25351;&#23548;&#32773;&#21644;&#23457;&#26680;&#32773;&#20849;&#21516;&#20316;&#29992;&#20110;&#25511;&#21046;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#20225;&#19994;&#20869;&#37096;&#32842;&#22825;&#26426;&#22120;&#20154;&#23545;&#40784;&#21040;&#19994;&#21153;&#34892;&#20026;&#20934;&#21017;&#30340;&#23454;&#20363;&#26469;&#35828;&#26126;&#36825;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09704v1 Announce Type: cross  Abstract: The alignment of large language models is usually done by model providers to add or control behaviors that are common or universally understood across use cases and contexts. In contrast, in this article, we present an approach and architecture that empowers application developers to tune a model to their particular values, social norms, laws and other regulations, and orchestrate between potentially conflicting requirements in context. We lay out three main components of such an Alignment Studio architecture: Framers, Instructors, and Auditors that work in concert to control the behavior of a language model. We illustrate this approach with a running example of aligning a company's internal-facing enterprise chatbot to its business conduct guidelines.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#27010;&#24565;&#24863;&#30693;&#35757;&#32451;&#65288;CoAT&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#26500;&#24314;&#35757;&#32451;&#22330;&#26223;&#65292;&#35753;&#35821;&#35328;&#27169;&#22411;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#21033;&#29992;&#31867;&#27604;&#25512;&#29702;&#27010;&#24565;&#65292;&#24182;&#21457;&#29616;&#36890;&#36807;&#20351;&#29992;CoAT&#65292;&#39044;&#35757;&#32451;&#30340;transformers&#21487;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#28436;&#31034;&#20013;&#30340;&#26032;&#28508;&#22312;&#27010;&#24565;&#65292;&#20351;&#24471;&#19978;&#19979;&#25991;&#23398;&#20064;&#23545;&#20989;&#25968;&#21464;&#25442;&#26356;&#21152; robust&#12290;</title><link>https://arxiv.org/abs/2403.09703</link><description>&lt;p&gt;
&#27010;&#24565;&#24863;&#30693;&#25968;&#25454;&#26500;&#24314;&#25552;&#21319;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Concept-aware Data Construction Improves In-context Learning of Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09703
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#27010;&#24565;&#24863;&#30693;&#35757;&#32451;&#65288;CoAT&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#26500;&#24314;&#35757;&#32451;&#22330;&#26223;&#65292;&#35753;&#35821;&#35328;&#27169;&#22411;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#21033;&#29992;&#31867;&#27604;&#25512;&#29702;&#27010;&#24565;&#65292;&#24182;&#21457;&#29616;&#36890;&#36807;&#20351;&#29992;CoAT&#65292;&#39044;&#35757;&#32451;&#30340;transformers&#21487;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#28436;&#31034;&#20013;&#30340;&#26032;&#28508;&#22312;&#27010;&#24565;&#65292;&#20351;&#24471;&#19978;&#19979;&#25991;&#23398;&#20064;&#23545;&#20989;&#25968;&#21464;&#25442;&#26356;&#21152; robust&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26368;&#36817;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#33021;&#22815;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#34920;&#29616;&#20026;LMs&#33021;&#22815;&#20165;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#25191;&#34892;&#26032;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#20808;&#21069;&#26377;&#20851;&#31574;&#21010;&#19978;&#19979;&#25991;&#23398;&#20064;&#32773;&#30340;&#24037;&#20316;&#20551;&#23450;ICL&#26159;&#30001;&#20110;&#24040;&#22823;&#30340;&#36807;&#21442;&#25968;&#21270;&#25110;&#22810;&#20219;&#21153;&#35757;&#32451;&#35268;&#27169;&#23548;&#33268;&#30340;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#29702;&#35770;&#24037;&#20316;&#23558;ICL&#33021;&#21147;&#24402;&#22240;&#20110;&#27010;&#24565;&#30456;&#20851;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#22312;&#23567;&#35268;&#27169;&#12289;&#21512;&#25104;&#29615;&#22659;&#20013;&#21019;&#24314;&#20102;&#21151;&#33021;&#22411;&#19978;&#19979;&#25991;&#23398;&#20064;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09703v1 Announce Type: cross  Abstract: Many recent language models (LMs) are capable of in-context learning (ICL), manifested in the LMs' ability to perform a new task solely from natural-language instruction. Previous work curating in-context learners assumes that ICL emerges from a vast over-parametrization or the scale of multi-task training. However, recent theoretical work attributes the ICL ability to concept-dependent training data and creates functional in-context learners even in small-scale, synthetic settings.   In this work, we practically explore this newly identified axis of ICL quality. We propose Concept-aware Training (CoAT), a framework for constructing training scenarios that make it beneficial for the LM to learn to utilize the analogical reasoning concepts from demonstrations. We find that by using CoAT, pre-trained transformers can learn to better utilise new latent concepts from demonstrations and that such ability makes ICL more robust to the functio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#22120;&#24341;&#23548;&#30340;&#32676;&#20307;&#21453;&#24212;&#35780;&#20272;&#20219;&#21153;&#65292;&#21033;&#29992;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#20998;&#31867;&#27169;&#22411;&#20570;&#20986;&#26356;&#22909;&#30340;&#39044;&#27979;&#65292;&#32467;&#26524;&#26174;&#31034;&#32463;&#24494;&#35843;&#30340; FLANG-RoBERTa &#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>https://arxiv.org/abs/2403.09702</link><description>&lt;p&gt;
&#29983;&#25104;&#22120;&#24341;&#23548;&#30340;&#32676;&#20307;&#21453;&#24212;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Generator-Guided Crowd Reaction Assessment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#22120;&#24341;&#23548;&#30340;&#32676;&#20307;&#21453;&#24212;&#35780;&#20272;&#20219;&#21153;&#65292;&#21033;&#29992;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#20998;&#31867;&#27169;&#22411;&#20570;&#20986;&#26356;&#22909;&#30340;&#39044;&#27979;&#65292;&#32467;&#26524;&#26174;&#31034;&#32463;&#24494;&#35843;&#30340; FLANG-RoBERTa &#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31038;&#20132;&#23186;&#20307;&#39046;&#22495;&#65292;&#29702;&#35299;&#21644;&#39044;&#27979;&#24086;&#23376;&#24433;&#21709;&#21147;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26088;&#22312;&#20272;&#35745;&#32473;&#23450;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#26159;&#21542;&#20250;&#27604;&#21478;&#19968;&#20010;&#24086;&#23376;&#33719;&#24471;&#26356;&#22810;&#21453;&#24212;&#30340; Crowd Reaction Assessment (CReAM) &#20219;&#21153;&#65292;&#23545;&#20110;&#25968;&#23383;&#33829;&#38144;&#20154;&#21592;&#21644;&#20869;&#23481;&#25776;&#20889;&#32773;&#26469;&#35828;&#23588;&#20026;&#37325;&#35201;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010; Crowd Reaction Estimation Dataset (CRED)&#65292;&#21253;&#21547;&#26469;&#33258;&#30333;&#23467;&#30340;&#19968;&#23545;&#25512;&#25991;&#65292;&#24102;&#26377;&#36716;&#25512;&#35745;&#25968;&#30340;&#27604;&#36739;&#25514;&#26045;&#12290;&#25552;&#20986;&#30340;&#29983;&#25104;&#22120;&#24341;&#23548;&#35780;&#20272;&#26041;&#27861; (GGEA) &#21033;&#29992;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM)&#65292;&#20363;&#22914; ChatGPT&#12289;FLAN-UL2 &#21644; Claude&#65292;&#24341;&#23548;&#20998;&#31867;&#27169;&#22411;&#20197;&#20570;&#20986;&#26356;&#22909;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340; FLANG-RoBERTa &#27169;&#22411;&#65292;&#22312; tweet &#20869;&#23481;&#21644; Claude &#29983;&#25104;&#30340;&#22238;&#24212;&#20043;&#38388;&#21033;&#29992;&#20132;&#21449;&#32534;&#30721;&#22120;&#26550;&#26500;&#34920;&#29616;&#26368;&#20339;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20351;&#29992;&#22522;&#20110; T5 &#30340;&#37322;&#20041;&#36719;&#20214;&#26469;&#29983;&#25104;&#32473;&#23450;&#24086;&#23376;&#30340;&#37322;&#20041;&#65292;&#24182;&#23637;&#31034;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09702v1 Announce Type: new  Abstract: In the realm of social media, understanding and predicting post reach is a significant challenge. This paper presents a Crowd Reaction AssessMent (CReAM) task designed to estimate if a given social media post will receive more reaction than another, a particularly essential task for digital marketers and content writers. We introduce the Crowd Reaction Estimation Dataset (CRED), consisting of pairs of tweets from The White House with comparative measures of retweet count. The proposed Generator-Guided Estimation Approach (GGEA) leverages generative Large Language Models (LLMs), such as ChatGPT, FLAN-UL2, and Claude, to guide classification models for making better predictions. Our results reveal that a fine-tuned FLANG-RoBERTa model, utilizing a cross-encoder architecture with tweet content and responses generated by Claude, performs optimally. We further use a T5-based paraphraser to generate paraphrases of a given post and demonstrate 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27450;&#39575;&#34892;&#20026;&#24182;&#20998;&#31867;&#35752;&#35770;&#20854;&#24341;&#21457;&#30340;&#31038;&#20250;&#24433;&#21709;&#21644;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2403.09676</link><description>&lt;p&gt;
&#25581;&#24320;AI&#30340;&#38452;&#24433;&#65306;&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27450;&#39575;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Unmasking the Shadows of AI: Investigating Deceptive Capabilities in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09676
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27450;&#39575;&#34892;&#20026;&#24182;&#20998;&#31867;&#35752;&#35770;&#20854;&#24341;&#21457;&#30340;&#31038;&#20250;&#24433;&#21709;&#21644;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23545;&#20154;&#24037;&#26234;&#33021;&#27450;&#39575;&#30340;&#22797;&#26434;&#39046;&#22495;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#23548;&#33322;&#65292;&#38598;&#20013;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#27450;&#39575;&#34892;&#20026;&#12290;&#20316;&#32773;&#30340;&#30446;&#26631;&#26159;&#38416;&#26126;&#36825;&#19968;&#38382;&#39064;&#65292;&#23457;&#35270;&#22260;&#32469;&#23427;&#30340;&#35752;&#35770;&#65292;&#38543;&#21518;&#28145;&#20837;&#20854;&#20998;&#31867;&#21644;&#21518;&#26524;&#12290;&#25991;&#31456;&#20174;&#35780;&#20272;2033&#24180;AI&#23433;&#20840;&#23792;&#20250;&#65288;ASS&#65289;&#65292;&#20197;&#21450;LLMs&#30340;&#20171;&#32461;&#24320;&#22987;&#65292;&#24378;&#35843;&#20102;&#28508;&#22312;&#23548;&#33268;&#23427;&#20204;&#27450;&#39575;&#34892;&#20026;&#30340;&#22810;&#32500;&#20559;&#35265;&#12290;&#25991;&#29486;&#32508;&#36848;&#28085;&#30422;&#20102;&#22235;&#31181;&#20998;&#31867;&#30340;&#27450;&#39575;&#65306;&#25112;&#30053;&#27450;&#39575;&#12289;&#27169;&#20223;&#12289;&#35844;&#23194;&#21644;&#19981;&#24544;&#25512;&#29702;&#65292;&#20197;&#21450;&#23427;&#20204;&#25152;&#24102;&#26469;&#30340;&#31038;&#20250;&#24433;&#21709;&#21644;&#39118;&#38505;&#12290;&#26368;&#21518;&#65292;&#20316;&#32773;&#23545;&#19982;&#24212;&#23545;&#27450;&#39575;AI&#30340;&#25345;&#20037;&#25361;&#25112;&#30456;&#20851;&#30340;&#21508;&#20010;&#26041;&#38754;&#37319;&#21462;&#20102;&#35780;&#20272;&#31435;&#22330;&#12290;&#36825;&#21253;&#25324;&#32771;&#34385;&#22269;&#38469;&#21327;&#20316;&#27835;&#29702;&#12289;&#20010;&#20154;&#19982;AI&#37325;&#26032;&#26500;&#24314;&#30340;&#20114;&#21160;&#65292;&#25552;&#20986;&#23454;&#38469;&#35843;&#25972;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09676v1 Announce Type: cross  Abstract: This research critically navigates the intricate landscape of AI deception, concentrating on deceptive behaviours of Large Language Models (LLMs). My objective is to elucidate this issue, examine the discourse surrounding it, and subsequently delve into its categorization and ramifications. The essay initiates with an evaluation of the AI Safety Summit 2023 (ASS) and introduction of LLMs, emphasising multidimensional biases that underlie their deceptive behaviours.The literature review covers four types of deception categorised: Strategic deception, Imitation, Sycophancy, and Unfaithful Reasoning, along with the social implications and risks they entail. Lastly, I take an evaluative stance on various aspects related to navigating the persistent challenges of the deceptive AI. This encompasses considerations of international collaborative governance, the reconfigured engagement of individuals with AI, proposal of practical adjustments, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;AI&#65288;ChatGPT-4&#65289;&#22914;&#20309;&#36731;&#26494;&#21046;&#36896;&#20196;&#20154;&#20449;&#26381;&#20294;&#23436;&#20840;&#34394;&#26500;&#30340;&#31185;&#23398;&#25968;&#25454;&#65292;&#20197;&#21046;&#36896;&#20986;&#19968;&#20010;&#23436;&#20840;&#34394;&#26500;&#30340;&#21307;&#23398;&#26696;&#20363;&#26469;&#35686;&#31034;&#21307;&#23398;&#35823;&#20449;&#24687;&#30340;&#21361;&#23475;&#12290;</title><link>https://arxiv.org/abs/2403.09674</link><description>&lt;p&gt;
&#36991;&#24320;&#29983;&#25104;&#30340;&#26367;&#20195;&#20107;&#23454;&#30340;&#21361;&#38505;&#65306;&#20197;ChatGPT-4&#21046;&#36896;&#30340;&#937;&#21464;&#31181;&#26696;&#20363;&#20316;&#20026;&#21307;&#23398;&#35823;&#20449;&#24687;&#30340;&#35686;&#31034;&#25925;&#20107;
&lt;/p&gt;
&lt;p&gt;
Navigating the Peril of Generated Alternative Facts: A ChatGPT-4 Fabricated Omega Variant Case as a Cautionary Tale in Medical Misinformation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;AI&#65288;ChatGPT-4&#65289;&#22914;&#20309;&#36731;&#26494;&#21046;&#36896;&#20196;&#20154;&#20449;&#26381;&#20294;&#23436;&#20840;&#34394;&#26500;&#30340;&#31185;&#23398;&#25968;&#25454;&#65292;&#20197;&#21046;&#36896;&#20986;&#19968;&#20010;&#23436;&#20840;&#34394;&#26500;&#30340;&#21307;&#23398;&#26696;&#20363;&#26469;&#35686;&#31034;&#21307;&#23398;&#35823;&#20449;&#24687;&#30340;&#21361;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#19982;&#21307;&#23398;&#30740;&#31350;&#20132;&#32455;&#30340;&#26102;&#20195;&#65292;&#30495;&#30456;&#30340;&#25259;&#38706;&#21464;&#24471;&#26085;&#30410;&#22797;&#26434;&#12290;&#26412;&#30740;&#31350;&#34920;&#38754;&#19978;&#23457;&#26597;&#20102;&#19968;&#31181;&#25152;&#35859;&#30340;&#26032;&#22411;SARS-CoV-2&#21464;&#31181;&#65292;&#34987;&#31216;&#20026;&#937;&#21464;&#31181;&#65292;&#23637;&#31034;&#22312;S&#22522;&#22240;&#21306;&#22495;&#20013;&#26377;31&#20010;&#29420;&#29305;&#31361;&#21464;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#25925;&#20107;&#30340;&#30495;&#27491;&#28508;&#21488;&#35789;&#26159;&#23637;&#31034;&#20102;AI&#65288;&#20855;&#20307;&#26469;&#35828;&#26159;ChatGPT-4&#65289;&#21487;&#20197;&#22914;&#20309;&#36731;&#26494;&#22320;&#21046;&#36896;&#20196;&#20154;&#20449;&#26381;&#20294;&#23436;&#20840;&#34394;&#26500;&#30340;&#31185;&#23398;&#25968;&#25454;&#12290;&#25152;&#35859;&#30340;&#937;&#21464;&#31181;&#22312;&#19968;&#20010;&#23436;&#20840;&#25509;&#31181;&#30123;&#33495;&#12289;&#20043;&#21069;&#24863;&#26579;&#36807;&#30340;35&#23681;&#30007;&#24615;&#20013;&#34987;&#37492;&#23450;&#20986;&#29616;&#20005;&#37325;COVID-19&#30151;&#29366;&#12290;&#36890;&#36807;&#35814;&#32454;&#30340;&#65292;&#23613;&#31649;&#26159;&#34394;&#25311;&#30340;&#65292;&#22522;&#22240;&#32452;&#20998;&#26512;&#21644;&#25509;&#35302;&#32773;&#36861;&#36394;&#65292;&#26412;&#30740;&#31350;&#27169;&#25311;&#20102;&#30495;&#23454;&#30149;&#20363;&#25253;&#21578;&#30340;&#20005;&#35880;&#26041;&#27861;&#65292;&#20174;&#32780;&#20026;&#19968;&#20010;&#24341;&#20154;&#20837;&#32988;&#20294;&#23436;&#20840;&#26500;&#36896;&#30340;&#21465;&#36848;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;&#25972;&#20010;&#30149;&#20363;&#30740;&#31350;&#26159;&#30001;OpenAI&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;ChatGPT-4&#29983;&#25104;&#30340;&#937;&#21464;&#31181;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09674v1 Announce Type: new  Abstract: In an era where artificial intelligence (AI) intertwines with medical research, the delineation of truth becomes increasingly complex. This study ostensibly examines a purported novel SARS-CoV-2 variant, dubbed the Omega variant, showcasing 31 unique mutations in the S gene region. However, the real undercurrent of this narrative is a demonstration of the ease with which AI, specifically ChatGPT-4, can fabricate convincing yet entirely fictional scientific data. The so-called Omega variant was identified in a fully vaccinated, previously infected 35-year-old male presenting with severe COVID-19 symptoms. Through a detailed, albeit artificial, genomic analysis and contact tracing, this study mirrors the rigorous methodology of genuine case reports, thereby setting the stage for a compelling but entirely constructed narrative. The entire case study was generated by ChatGPT-4, a large language model by OpenAI. The fabricated Omega variant f
&lt;/p&gt;</description></item><item><title>&#22823;&#22810;&#25968;&#29616;&#20195;LLM&#21463;&#21040;softmax&#29942;&#39048;&#24433;&#21709;&#65292;&#21487;&#20197;&#20197;&#36739;&#20302;&#25104;&#26412;&#33719;&#21462;API&#20445;&#25252;&#30340;LLM&#30340;&#38750;&#20844;&#24320;&#20449;&#24687;&#21644;&#35299;&#38145;&#22810;&#31181;&#21151;&#33021;</title><link>https://arxiv.org/abs/2403.09539</link><description>&lt;p&gt;
API&#20445;&#25252;&#30340;LLMs&#30340;&#26631;&#24535;&#27844;&#38706;&#19987;&#26377;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Logits of API-Protected LLMs Leak Proprietary Information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09539
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#20195;LLM&#21463;&#21040;softmax&#29942;&#39048;&#24433;&#21709;&#65292;&#21487;&#20197;&#20197;&#36739;&#20302;&#25104;&#26412;&#33719;&#21462;API&#20445;&#25252;&#30340;LLM&#30340;&#38750;&#20844;&#24320;&#20449;&#24687;&#21644;&#35299;&#38145;&#22810;&#31181;&#21151;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21830;&#19994;&#21270;&#23548;&#33268;&#20102;&#39640;&#32423;API-only&#25509;&#20837;&#19987;&#26377;&#27169;&#22411;&#30340;&#24120;&#35265;&#23454;&#36341;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#23545;&#20110;&#27169;&#22411;&#26550;&#26500;&#26377;&#20445;&#23432;&#30340;&#20551;&#35774;&#65292;&#20063;&#21487;&#20197;&#20174;&#30456;&#23545;&#36739;&#23569;&#30340;API&#26597;&#35810;&#20013;&#23398;&#20064;&#20851;&#20110;API&#20445;&#25252;&#30340;LLM&#30340;&#22823;&#37327;&#38750;&#20844;&#24320;&#20449;&#24687;&#65288;&#20363;&#22914;&#65292;&#20351;&#29992;OpenAI&#30340;gpt-3.5-turbo&#20165;&#33457;&#36153;&#19981;&#21040;1000&#32654;&#20803;&#65289;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#38598;&#20013;&#22312;&#19968;&#20010;&#20851;&#38190;&#35266;&#23519;&#19978;&#65306;&#22823;&#22810;&#25968;&#29616;&#20195;LLM&#21463;&#21040;&#20102;softmax&#29942;&#39048;&#30340;&#24433;&#21709;&#65292;&#36825;&#38480;&#21046;&#20102;&#27169;&#22411;&#36755;&#20986;&#21040;&#23436;&#25972;&#36755;&#20986;&#31354;&#38388;&#30340;&#32447;&#24615;&#23376;&#31354;&#38388;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#27169;&#22411;&#22270;&#20687;&#25110;&#27169;&#22411;&#31614;&#21517;&#65292;&#20174;&#32780;&#20197;&#36739;&#20302;&#30340;&#25104;&#26412;&#35299;&#38145;&#20102;&#20960;&#31181;&#21151;&#33021;&#65306;&#26377;&#25928;&#21457;&#29616;LLM&#30340;&#38544;&#34255;&#22823;&#23567;&#65292;&#33719;&#21462;&#23436;&#25972;&#35789;&#27719;&#36755;&#20986;&#65292;&#26816;&#27979;&#21644;&#28040;&#38500;&#19981;&#21516;&#27169;&#22411;&#26356;&#26032;&#65292;&#35782;&#21035;&#32473;&#23450;&#21333;&#20010;&#23436;&#25972;LLM&#36755;&#20986;&#30340;&#28304;LLM&#65292;&#20197;&#21450;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09539v1 Announce Type: cross  Abstract: The commercialization of large language models (LLMs) has led to the common practice of high-level API-only access to proprietary models. In this work, we show that even with a conservative assumption about the model architecture, it is possible to learn a surprisingly large amount of non-public information about an API-protected LLM from a relatively small number of API queries (e.g., costing under $1,000 for OpenAI's gpt-3.5-turbo). Our findings are centered on one key observation: most modern LLMs suffer from a softmax bottleneck, which restricts the model outputs to a linear subspace of the full output space. We show that this lends itself to a model image or a model signature which unlocks several capabilities with affordable cost: efficiently discovering the LLM's hidden size, obtaining full-vocabulary outputs, detecting and disambiguating different model updates, identifying the source LLM given a single full LLM output, and eve
&lt;/p&gt;</description></item><item><title>DevBench&#26159;&#19968;&#20010;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36719;&#20214;&#24320;&#21457;&#29983;&#21629;&#21608;&#26399;&#21508;&#20010;&#38454;&#27573;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#30340;&#27169;&#22411;&#22312;&#20854;&#20013;&#23384;&#22312;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.08604</link><description>&lt;p&gt;
DevBench&#65306;&#36719;&#20214;&#24320;&#21457;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
DevBench: A Comprehensive Benchmark for Software Development
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08604
&lt;/p&gt;
&lt;p&gt;
DevBench&#26159;&#19968;&#20010;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36719;&#20214;&#24320;&#21457;&#29983;&#21629;&#21608;&#26399;&#21508;&#20010;&#38454;&#27573;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#30340;&#27169;&#22411;&#22312;&#20854;&#20013;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08604v1&#23459;&#24067;&#31867;&#22411;&#65306;&#26032;&#30340;&#25688;&#35201;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#26174;&#33879;&#25552;&#21319;&#20102;&#23427;&#20204;&#30340;&#32534;&#30721;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#20027;&#35201;&#20851;&#27880;&#32534;&#31243;&#30340;&#31616;&#21270;&#25110;&#23396;&#31435;&#26041;&#38754;&#65292;&#22914;&#21333;&#25991;&#20214;&#20195;&#30721;&#29983;&#25104;&#25110;&#23384;&#20648;&#24211;&#38382;&#39064;&#35843;&#35797;&#65292;&#26410;&#33021;&#20840;&#38754;&#34913;&#37327;&#30001;&#30495;&#23454;&#19990;&#30028;&#32534;&#31243;&#27963;&#21160;&#25552;&#20986;&#30340;&#21508;&#31181;&#25361;&#25112;&#30340;&#20840;&#35889;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DevBench&#65292;&#19968;&#20010;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#35780;&#20272;LLMs&#22312;&#36719;&#20214;&#24320;&#21457;&#29983;&#21629;&#21608;&#26399;&#30340;&#21508;&#20010;&#38454;&#27573;&#65292;&#21253;&#25324;&#36719;&#20214;&#35774;&#35745;&#12289;&#29615;&#22659;&#35774;&#32622;&#12289;&#23454;&#29616;&#12289;&#39564;&#25910;&#27979;&#35797;&#21644;&#21333;&#20803;&#27979;&#35797;&#12290;DevBench&#20855;&#26377;&#21508;&#31181;&#32534;&#31243;&#35821;&#35328;&#21644;&#39046;&#22495;&#65292;&#39640;&#36136;&#37327;&#25968;&#25454;&#25910;&#38598;&#65292;&#24182;&#38024;&#23545;&#27599;&#20010;&#20219;&#21153;&#31934;&#24515;&#35774;&#35745;&#21644;&#39564;&#35777;&#30340;&#25351;&#26631;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#21069;&#30340;LLMs&#65292;&#21253;&#25324;GPT-4-Turbo&#65292;&#26080;&#27861;&#35299;&#20915;DevBench&#25552;&#20986;&#30340;&#25361;&#25112;&#12290;&#20998;&#26512;&#34920;&#26126;&#65292;&#27169;&#22411;&#38590;&#20197;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08604v1 Announce Type: new  Abstract: Recent advancements in large language models (LLMs) have significantly enhanced their coding capabilities. However, existing benchmarks predominantly focused on simplified or isolated aspects of programming, such as single-file code generation or repository issue debugging, falling short of measuring the full spectrum of challenges raised by real-world programming activities. To this end, we propose DevBench, a comprehensive benchmark that evaluates LLMs across various stages of the software development lifecycle, including software design, environment setup, implementation, acceptance testing, and unit testing. DevBench features a wide range of programming languages and domains, high-quality data collection, and carefully designed and verified metrics for each task. Empirical studies show that current LLMs, including GPT-4-Turbo, fail to solve the challenges presented within DevBench. Analyses reveal that models struggle with understand
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#34701;&#21512;&#39640;&#24230;&#19987;&#19994;&#21270;&#30340;&#35821;&#35328;&#12289;&#20195;&#30721;&#21644;&#25968;&#23398;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UltraFuser&#30340;&#34701;&#21512;&#26694;&#26550;&#65292;&#24341;&#20837;&#20102;&#26631;&#35760;&#32423;&#21035;&#30340;&#38376;&#25511;&#26426;&#21046;&#65292;&#24182;&#35774;&#35745;&#20102;&#20004;&#38454;&#27573;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#21516;&#26102;&#22312;&#19977;&#20010;&#39046;&#22495;&#21462;&#24471;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.08281</link><description>&lt;p&gt;
&#36890;&#36807;&#34701;&#21512;&#39640;&#24230;&#19987;&#19994;&#21270;&#35821;&#35328;&#27169;&#22411;&#21516;&#26102;&#25484;&#25569;&#25991;&#26412;&#12289;&#20195;&#30721;&#21644;&#25968;&#23398;
&lt;/p&gt;
&lt;p&gt;
Mastering Text, Code and Math Simultaneously via Fusing Highly Specialized Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08281
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#34701;&#21512;&#39640;&#24230;&#19987;&#19994;&#21270;&#30340;&#35821;&#35328;&#12289;&#20195;&#30721;&#21644;&#25968;&#23398;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UltraFuser&#30340;&#34701;&#21512;&#26694;&#26550;&#65292;&#24341;&#20837;&#20102;&#26631;&#35760;&#32423;&#21035;&#30340;&#38376;&#25511;&#26426;&#21046;&#65292;&#24182;&#35774;&#35745;&#20102;&#20004;&#38454;&#27573;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#21516;&#26102;&#22312;&#19977;&#20010;&#39046;&#22495;&#21462;&#24471;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#12289;&#32534;&#31243;&#20195;&#30721;&#21644;&#25968;&#23398;&#31526;&#21495;&#30340;&#24213;&#23618;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#24040;&#22823;&#65292;&#23545;&#20110;&#37027;&#20123;&#21162;&#21147;&#21516;&#26102;&#22312;&#19977;&#20010;&#39046;&#22495;&#23454;&#29616;&#39640;&#24615;&#33021;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20986;&#20102;&#22797;&#26434;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#34701;&#21512;&#24050;&#32463;&#39640;&#24230;&#19987;&#19994;&#21270;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#34701;&#21512;&#26694;&#26550;UltraFuser&#21253;&#25324;&#19977;&#20010;&#24050;&#32463;&#22312;&#35821;&#35328;&#12289;&#32534;&#30721;&#21644;&#25968;&#23398;&#19978;&#24471;&#21040;&#20805;&#20998;&#35757;&#32451;&#30340;&#19987;&#23478;&#12290;&#24341;&#20837;&#20102;&#19968;&#20010;&#26631;&#35760;&#32423;&#21035;&#30340;&#38376;&#25511;&#26426;&#21046;&#26469;&#28151;&#21512;&#19987;&#23478;&#30340;&#36755;&#20986;&#12290;&#35774;&#35745;&#20102;&#19968;&#20010;&#20276;&#38543;&#24179;&#34913;&#37319;&#26679;&#30340;&#20004;&#38454;&#27573;&#35757;&#32451;&#31574;&#30053;&#20197;&#30830;&#20445;&#31283;&#23450;&#24615;&#12290;&#20026;&#20102;&#26377;&#25928;&#35757;&#32451;&#34701;&#21512;&#27169;&#22411;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#26500;&#24314;&#20102;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08281v1 Announce Type: cross  Abstract: Underlying data distributions of natural language, programming code, and mathematical symbols vary vastly, presenting a complex challenge for large language models (LLMs) that strive to achieve high performance across all three domains simultaneously. Achieving a very high level of proficiency for an LLM within a specific domain often requires extensive training with relevant corpora, which is typically accompanied by a sacrifice in performance in other domains. In this paper, we propose to fuse models that are already highly-specialized directly. The proposed fusing framework, UltraFuser, consists of three distinct specialists that are already sufficiently trained on language, coding, and mathematics. A token-level gating mechanism is introduced to blend the specialists' outputs. A two-stage training strategy accompanied by balanced sampling is designed to ensure stability. To effectively train the fused model, we further construct a 
&lt;/p&gt;</description></item><item><title>MAGPIE&#26159;&#31532;&#19968;&#20010;&#20026;&#23186;&#20307;&#20559;&#35265;&#26816;&#27979;&#23450;&#21046;&#30340;&#22823;&#35268;&#27169;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#22312;&#23186;&#20307;&#20559;&#35265;&#26816;&#27979;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#21333;&#19968;&#20219;&#21153;&#26041;&#27861;&#38656;&#35201;&#26356;&#23569;&#30340;&#24494;&#35843;&#27493;&#39588;&#12290;</title><link>https://arxiv.org/abs/2403.07910</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#23186;&#20307;&#20559;&#35265;&#20998;&#26512;&#36890;&#29992;&#21270;&#30340;&#39044;&#35757;&#32451;&#34920;&#36798;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Multi-Task Media-Bias Analysis Generalization for Pre-Trained Identification of Expressions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07910
&lt;/p&gt;
&lt;p&gt;
MAGPIE&#26159;&#31532;&#19968;&#20010;&#20026;&#23186;&#20307;&#20559;&#35265;&#26816;&#27979;&#23450;&#21046;&#30340;&#22823;&#35268;&#27169;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#22312;&#23186;&#20307;&#20559;&#35265;&#26816;&#27979;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#21333;&#19968;&#20219;&#21153;&#26041;&#27861;&#38656;&#35201;&#26356;&#23569;&#30340;&#24494;&#35843;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23186;&#20307;&#20559;&#35265;&#26816;&#27979;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#12289;&#22810;&#26041;&#38754;&#30340;&#38382;&#39064;&#65292;&#20256;&#32479;&#19978;&#36890;&#36807;&#20351;&#29992;&#21333;&#19968;&#20219;&#21153;&#27169;&#22411;&#21644;&#23567;&#22411;&#39046;&#22495;&#20869;&#25968;&#25454;&#38598;&#26469;&#35299;&#20915;&#65292;&#22240;&#27492;&#32570;&#20047;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MAGPIE&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#20026;&#23186;&#20307;&#20559;&#35265;&#26816;&#27979;&#23450;&#21046;&#30340;&#22823;&#35268;&#27169;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#35268;&#27169;&#21270;&#30340;&#39044;&#35757;&#32451;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22823;&#20559;&#35265;&#28151;&#21512;&#65288;LBM&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;59&#20010;&#19982;&#20559;&#35265;&#30456;&#20851;&#30340;&#20219;&#21153;&#30340;&#32534;&#35793;&#12290;MAGPIE&#22312;Bias Annotation By Experts (BABE)&#25968;&#25454;&#38598;&#19978;&#30340;&#23186;&#20307;&#20559;&#35265;&#26816;&#27979;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;F1&#20998;&#25968;&#30456;&#23545;&#25552;&#39640;&#20102;3.3%&#12290;MAGPIE&#22312;Media Bias Identification Benchmark (MBIB)&#20013;&#30340;8&#20010;&#20219;&#21153;&#20013;&#26377;5&#20010;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#20808;&#21069;&#30340;&#27169;&#22411;&#12290;&#20351;&#29992;RoBERTa&#32534;&#30721;&#22120;&#65292;MAGPIE&#20165;&#38656;&#35201;&#30456;&#23545;&#20110;&#21333;&#19968;&#20219;&#21153;&#26041;&#27861;&#30340;15%&#30340;&#24494;&#35843;&#27493;&#39588;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#27604;&#22914;&#20219;&#21153;&#22914;&#24773;&#24863;&#21644;&#24773;&#32490;&#20250;&#22686;&#24378;&#25152;&#26377;&#23398;&#20064;&#65292;&#25152;&#26377;&#20219;&#21153;&#20250;&#22686;&#24378;&#20551;&#26032;&#38395;&#26816;&#27979;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07910v1 Announce Type: cross  Abstract: Media bias detection poses a complex, multifaceted problem traditionally tackled using single-task models and small in-domain datasets, consequently lacking generalizability. To address this, we introduce MAGPIE, the first large-scale multi-task pre-training approach explicitly tailored for media bias detection. To enable pre-training at scale, we present Large Bias Mixture (LBM), a compilation of 59 bias-related tasks. MAGPIE outperforms previous approaches in media bias detection on the Bias Annotation By Experts (BABE) dataset, with a relative improvement of 3.3% F1-score. MAGPIE also performs better than previous models on 5 out of 8 tasks in the Media Bias Identification Benchmark (MBIB). Using a RoBERTa encoder, MAGPIE needs only 15% of finetuning steps compared to single-task approaches. Our evaluation shows, for instance, that tasks like sentiment and emotionality boost all learning, all tasks enhance fake news detection, and s
&lt;/p&gt;</description></item><item><title>&#25991;&#31456;&#25506;&#35752;&#20102;&#22522;&#20110;&#22810;Agent&#31995;&#32479;&#29702;&#35770;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#31639;&#23454;&#20307;&#23545;&#20154;&#31867;&#20114;&#21160;&#30340;&#38761;&#26032;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#33021;&#23558;&#19987;&#38376;&#20154;&#24037;&#20195;&#29702;&#25903;&#25345;&#25193;&#23637;&#21040;&#25805;&#20316;&#24615;&#32452;&#32455;&#27969;&#31243;&#21644;&#22522;&#20110;&#30693;&#35782;&#21644;&#20154;&#31867;&#32534;&#25490;&#30340;&#25112;&#30053;&#20915;&#31574;&#30340;&#26041;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.07769</link><description>&lt;p&gt;
&#23558;&#31454;&#20105;&#36716;&#21270;&#20026;&#21512;&#20316;&#65306;&#22810;Agent&#31995;&#32479;&#21644;&#35821;&#35328;&#27169;&#22411;&#22312;&#29616;&#20195;&#32452;&#32455;&#20013;&#30340;&#38761;&#21629;&#24615;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Transforming Competition into Collaboration: The Revolutionary Role of Multi-Agent Systems and Language Models in Modern Organizations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07769
&lt;/p&gt;
&lt;p&gt;
&#25991;&#31456;&#25506;&#35752;&#20102;&#22522;&#20110;&#22810;Agent&#31995;&#32479;&#29702;&#35770;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#31639;&#23454;&#20307;&#23545;&#20154;&#31867;&#20114;&#21160;&#30340;&#38761;&#26032;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#33021;&#23558;&#19987;&#38376;&#20154;&#24037;&#20195;&#29702;&#25903;&#25345;&#25193;&#23637;&#21040;&#25805;&#20316;&#24615;&#32452;&#32455;&#27969;&#31243;&#21644;&#22522;&#20110;&#30693;&#35782;&#21644;&#20154;&#31867;&#32534;&#25490;&#30340;&#25112;&#30053;&#20915;&#31574;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25991;&#31456;&#25506;&#35752;&#20102;&#22522;&#20110;&#22810;Agent&#31995;&#32479;&#29702;&#35770;&#65288;SMA&#65289;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35745;&#31639;&#23454;&#20307;&#30340;&#21160;&#24577;&#24433;&#21709;&#65292;&#20854;&#29305;&#28857;&#26159;&#33021;&#22815;&#27169;&#25311;&#22797;&#26434;&#30340;&#20154;&#31867;&#20114;&#21160;&#65292;&#20316;&#20026;&#19968;&#31181;&#38761;&#26032;&#20154;&#31867;&#29992;&#25143;&#20132;&#20114;&#30340;&#21487;&#33021;&#24615;&#65292;&#20174;&#21033;&#29992;&#19987;&#38376;&#30340;&#20154;&#24037;&#20195;&#29702;&#25903;&#25345;&#20174;&#25805;&#20316;&#32452;&#32455;&#27969;&#31243;&#21040;&#22522;&#20110;&#24212;&#29992;&#30693;&#35782;&#21644;&#20154;&#30340;&#32534;&#25490;&#30340;&#25112;&#30053;&#20915;&#31574;&#12290; &#20808;&#21069;&#30340;&#35843;&#26597;&#26174;&#31034;&#65292;&#22312;&#22788;&#29702;&#26032;&#25361;&#25112;&#21644;&#23454;&#29992;&#20219;&#21153;&#65288;&#22914;&#24341;&#21457;&#36923;&#36753;&#25512;&#29702;&#21644;&#38382;&#39064;&#35299;&#20915;&#65289;&#26102;&#65292;&#29305;&#21035;&#26159;&#22312;&#20154;&#24037;&#20195;&#29702;&#30340;&#33258;&#20027;&#26041;&#27861;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#12290; &#36824;&#32771;&#34385;&#21040;&#65292;&#20256;&#32479;&#25216;&#26415;&#65292;&#22914;&#28608;&#21457;&#24605;&#24819;&#38142;&#65292;&#38656;&#35201;&#26126;&#30830;&#30340;&#20154;&#31867;&#25351;&#23548;&#12290; &#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24320;&#21457;&#30340;&#20195;&#29702;&#65292;&#27599;&#20010;&#20195;&#29702;&#37117;&#26377;&#19981;&#21516;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07769v1 Announce Type: new  Abstract: This article explores the dynamic influence of computational entities based on multi-agent systems theory (SMA) combined with large language models (LLM), which are characterized by their ability to simulate complex human interactions, as a possibility to revolutionize human user interaction from the use of specialized artificial agents to support everything from operational organizational processes to strategic decision making based on applied knowledge and human orchestration. Previous investigations reveal that there are limitations, particularly in the autonomous approach of artificial agents, especially when dealing with new challenges and pragmatic tasks such as inducing logical reasoning and problem solving. It is also considered that traditional techniques, such as the stimulation of chains of thoughts, require explicit human guidance. In our approach we employ agents developed from large language models (LLM), each with distinct
&lt;/p&gt;</description></item><item><title>SVD-LLM&#26159;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;SVD&#30340;LLM&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#25130;&#26029;&#24863;&#30693;&#25968;&#25454;&#30333;&#21270;&#31574;&#30053;&#21644;&#36880;&#23618;&#38381;&#24335;&#27169;&#22411;&#21442;&#25968;&#26356;&#26032;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#23454;&#29616;&#20102;&#30452;&#25509;&#26144;&#23556;&#22855;&#24322;&#20540;&#21644;&#21387;&#32553;&#25439;&#22833;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.07378</link><description>&lt;p&gt;
SVD-LLM: &#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;&#30340;&#25130;&#26029;&#24863;&#30693;&#22855;&#24322;&#20540;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07378
&lt;/p&gt;
&lt;p&gt;
SVD-LLM&#26159;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;SVD&#30340;LLM&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#25130;&#26029;&#24863;&#30693;&#25968;&#25454;&#30333;&#21270;&#31574;&#30053;&#21644;&#36880;&#23618;&#38381;&#24335;&#27169;&#22411;&#21442;&#25968;&#26356;&#26032;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#23454;&#29616;&#20102;&#30452;&#25509;&#26144;&#23556;&#22855;&#24322;&#20540;&#21644;&#21387;&#32553;&#25439;&#22833;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#21463;&#21040;&#20854;&#24222;&#22823;&#23610;&#23544;&#30340;&#38480;&#21046;&#65292;&#36825;&#38656;&#35201;LLM&#21387;&#32553;&#26041;&#27861;&#20197;&#23454;&#29616;&#23454;&#38469;&#37096;&#32626;&#12290;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#20026;LLM&#21387;&#32553;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;SVD&#30340;LLM&#21387;&#32553;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#38480;&#21046;&#65306;&#25130;&#26029;&#36739;&#23567;&#30340;&#22855;&#24322;&#20540;&#21487;&#33021;&#23548;&#33268;&#26356;&#39640;&#30340;&#21387;&#32553;&#25439;&#22833;&#65292;&#24182;&#19988;&#22312;SVD&#25130;&#26029;&#21518;&#21097;&#20313;&#27169;&#22411;&#21442;&#25968;&#30340;&#26356;&#26032;&#32570;&#22833;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SVD-LLM&#65292;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;SVD&#30340;LLM&#21387;&#32553;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#38480;&#21046;&#12290;SVD-LLM&#37319;&#29992;&#20102;&#19968;&#31181;&#25130;&#26029;&#24863;&#30693;&#30340;&#25968;&#25454;&#30333;&#21270;&#31574;&#30053;&#65292;&#20197;&#30830;&#20445;&#22855;&#24322;&#20540;&#21644;&#21387;&#32553;&#25439;&#22833;&#20043;&#38388;&#30340;&#30452;&#25509;&#26144;&#23556;&#12290;&#27492;&#22806;&#65292;SVD-LLM&#37319;&#29992;&#19968;&#31181;&#36880;&#23618;&#38381;&#24335;&#27169;&#22411;&#21442;&#25968;&#26356;&#26032;&#31574;&#30053;&#65292;&#20197;&#24357;&#34917;SVD&#25130;&#26029;&#24341;&#36215;&#30340;&#20934;&#30830;&#24615;&#38477;&#20302;&#12290;&#25105;&#20204;&#22312;&#24635;&#20849;11&#20010;&#25968;&#25454;&#38598;&#21644;&#19971;&#20010;m&#19978;&#35780;&#20272;&#20102;SVD-LLM&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07378v1 Announce Type: new  Abstract: The advancements in Large Language Models (LLMs) have been hindered by their substantial sizes, which necessitate LLM compression methods for practical deployment. Singular Value Decomposition (SVD) offers a promising solution for LLM compression. However, state-of-the-art SVD-based LLM compression methods have two key limitations: truncating smaller singular values may lead to higher compression loss, and the lack of update on the remaining model parameters after SVD truncation. In this work, we propose SVD-LLM, a new SVD-based LLM compression method that addresses the limitations of existing methods. SVD-LLM incorporates a truncation-aware data whitening strategy to ensure a direct mapping between singular values and compression loss. Moreover, SVD-LLM adopts a layer-wise closed-form model parameter update strategy to compensate for accuracy degradation caused by SVD truncation. We evaluate SVD-LLM on a total of 11 datasets and seven m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21475;&#22836;&#23545;&#35805;&#20013;&#20351;&#29992;&#35821;&#38899;&#27963;&#21160;&#25237;&#24433;&#36827;&#34892;&#22810;&#35821;&#35328;&#20132;&#26367;&#39044;&#27979;&#65292;&#22312;&#22810;&#35821;&#35328;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20986;&#19982;&#21333;&#19968;&#35821;&#35328;&#27169;&#22411;&#30456;&#24403;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#19988;&#23398;&#20250;&#20102;&#36776;&#21035;&#36755;&#20837;&#20449;&#21495;&#30340;&#35821;&#35328;&#12290;</title><link>https://arxiv.org/abs/2403.06487</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#38899;&#27963;&#21160;&#25237;&#24433;&#36827;&#34892;&#22810;&#35821;&#35328;&#20132;&#26367;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multilingual Turn-taking Prediction Using Voice Activity Projection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06487
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21475;&#22836;&#23545;&#35805;&#20013;&#20351;&#29992;&#35821;&#38899;&#27963;&#21160;&#25237;&#24433;&#36827;&#34892;&#22810;&#35821;&#35328;&#20132;&#26367;&#39044;&#27979;&#65292;&#22312;&#22810;&#35821;&#35328;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20986;&#19982;&#21333;&#19968;&#35821;&#35328;&#27169;&#22411;&#30456;&#24403;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#19988;&#23398;&#20250;&#20102;&#36776;&#21035;&#36755;&#20837;&#20449;&#21495;&#30340;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#35821;&#35328;&#25968;&#25454;&#19978;&#24212;&#29992;&#35821;&#38899;&#27963;&#21160;&#25237;&#24433;&#65288;VAP&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#21475;&#22836;&#23545;&#35805;&#30340;&#39044;&#27979;&#24615;&#20132;&#26367;&#27169;&#22411;&#65292;&#28085;&#30422;&#33521;&#35821;&#12289;&#27721;&#35821;&#21644;&#26085;&#35821;&#12290;VAP&#27169;&#22411;&#25345;&#32493;&#39044;&#27979;&#21452;&#20154;&#23545;&#35805;&#20013;&#21442;&#19982;&#32773;&#21363;&#23558;&#21457;&#29983;&#30340;&#35821;&#38899;&#27963;&#21160;&#65292;&#21033;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;Transformer&#25429;&#25417;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#21160;&#24577;&#20114;&#21160;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21333;&#19968;&#35821;&#35328;&#19978;&#35757;&#32451;&#30340;VAP&#27169;&#22411;&#22312;&#20854;&#20182;&#35821;&#35328;&#19978;&#30340;&#24212;&#29992;&#19981;&#20250;&#20135;&#29983;&#24456;&#22909;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#22312;&#19977;&#31181;&#35821;&#35328;&#19978;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#25152;&#26377;&#35821;&#35328;&#19978;&#34920;&#29616;&#20986;&#19982;&#21333;&#35821;&#27169;&#22411;&#30456;&#24403;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#22810;&#35821;&#35328;&#27169;&#22411;&#24050;&#23398;&#20250;&#36776;&#21035;&#36755;&#20837;&#20449;&#21495;&#30340;&#35821;&#35328;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#23545;&#38899;&#35843;&#25935;&#24863;&#24615;&#65292;&#36825;&#26159;&#19968;&#31181;&#34987;&#35748;&#20026;&#23545;&#20110;&#20132;&#26367;&#38750;&#24120;&#37325;&#35201;&#30340;&#38901;&#24459;&#32447;&#32034;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#38899;&#39057;&#32534;&#30721;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06487v1 Announce Type: new  Abstract: This paper investigates the application of voice activity projection (VAP), a predictive turn-taking model for spoken dialogue, on multilingual data, encompassing English, Mandarin, and Japanese. The VAP model continuously predicts the upcoming voice activities of participants in dyadic dialogue, leveraging a cross-attention Transformer to capture the dynamic interplay between participants. The results show that a monolingual VAP model trained on one language does not make good predictions when applied to other languages. However, a multilingual model, trained on all three languages, demonstrates predictive performance on par with monolingual models across all languages. Further analyses show that the multilingual model has learned to discern the language of the input signal. We also analyze the sensitivity to pitch, a prosodic cue that is thought to be important for turn-taking. Finally, we compare two different audio encoders, contrast
&lt;/p&gt;</description></item><item><title>CLIcK&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;1,995&#20010;&#38382;&#31572;&#23545;&#30340;&#38889;&#22269;&#25991;&#21270;&#21644;&#35821;&#35328;&#26234;&#24935;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20026;&#22635;&#34917;&#38889;&#35821;&#22522;&#20934;&#25968;&#25454;&#32570;&#22833;&#30340;&#38382;&#39064;&#32780;&#26469;&#12290;</title><link>https://arxiv.org/abs/2403.06412</link><description>&lt;p&gt;
CLIcK&#65306;&#38889;&#22269;&#25991;&#21270;&#21644;&#35821;&#35328;&#26234;&#24935;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CLIcK: A Benchmark Dataset of Cultural and Linguistic Intelligence in Korean
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06412
&lt;/p&gt;
&lt;p&gt;
CLIcK&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;1,995&#20010;&#38382;&#31572;&#23545;&#30340;&#38889;&#22269;&#25991;&#21270;&#21644;&#35821;&#35328;&#26234;&#24935;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20026;&#22635;&#34917;&#38889;&#35821;&#22522;&#20934;&#25968;&#25454;&#32570;&#22833;&#30340;&#38382;&#39064;&#32780;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#38024;&#23545;&#38889;&#35821;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36805;&#36895;&#21457;&#23637;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#26126;&#26174;&#32570;&#20047;&#27979;&#35797;&#24517;&#35201;&#38889;&#22269;&#25991;&#21270;&#21644;&#35821;&#35328;&#30693;&#35782;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#29616;&#26377;&#30340;&#35768;&#22810;&#38889;&#35821;&#22522;&#20934;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#32763;&#35793;&#20174;&#33521;&#35821;&#23545;&#24212;&#25968;&#25454;&#38598;&#20013;&#34893;&#29983;&#20986;&#26469;&#30340;&#65292;&#23427;&#20204;&#36890;&#24120;&#24573;&#35270;&#19981;&#21516;&#30340;&#25991;&#21270;&#32972;&#26223;&#12290;&#20165;&#26377;&#23569;&#25968;&#20174;&#38889;&#22269;&#25968;&#25454;&#28304;&#25429;&#25417;&#25991;&#21270;&#30693;&#35782;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#30340;&#20165;&#26377;&#20559;&#35265;&#21644;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#31561;&#29421;&#31364;&#20219;&#21153;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;CLIcK&#30340;&#38889;&#22269;&#25991;&#21270;&#21644;&#35821;&#35328;&#26234;&#24935;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;1,995&#20010;&#38382;&#31572;&#23545;&#12290;CLIcK&#23558;&#20854;&#25968;&#25454;&#26469;&#28304;&#20110;&#38889;&#22269;&#23448;&#26041;&#32771;&#35797;&#21644;&#25945;&#31185;&#20070;&#65292;&#23558;&#38382;&#39064;&#20998;&#20026;&#20004;&#20010;&#20027;&#35201;&#31867;&#21035;&#65288;&#35821;&#35328;&#21644;&#25991;&#21270;&#65289;&#19979;&#30340;11&#20010;&#31867;&#21035;&#12290;&#23545;&#20110;CLIcK&#20013;&#30340;&#27599;&#20010;&#23454;&#20363;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#21738;&#20123;&#25991;&#21270;&#21644;&#35821;&#35328;&#30693;&#35782;&#30340;&#32454;&#31890;&#24230;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06412v1 Announce Type: new  Abstract: Despite the rapid development of large language models (LLMs) for the Korean language, there remains an obvious lack of benchmark datasets that test the requisite Korean cultural and linguistic knowledge. Because many existing Korean benchmark datasets are derived from the English counterparts through translation, they often overlook the different cultural contexts. For the few benchmark datasets that are sourced from Korean data capturing cultural knowledge, only narrow tasks such as bias and hate speech detection are offered. To address this gap, we introduce a benchmark of Cultural and Linguistic Intelligence in Korean (CLIcK), a dataset comprising 1,995 QA pairs. CLIcK sources its data from official Korean exams and textbooks, partitioning the questions into eleven categories under the two main categories of language and culture. For each instance in CLIcK, we provide fine-grained annotation of which cultural and linguistic knowledge
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35774;&#35745;&#22810;&#27169;&#24577;&#23567;&#35821;&#35328;&#27169;&#22411;(MSLMs)&#21450;&#25552;&#20986;&#39640;&#25928;&#22810;&#27169;&#24577;&#21161;&#25163;Mipha&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#26041;&#38754;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#20987;&#36133;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#20026;&#24320;&#21457;&#24378;&#22823;MSLMs&#25552;&#20379;&#20102;&#35265;&#35299;&#21644;&#25351;&#21335;</title><link>https://arxiv.org/abs/2403.06199</link><description>&lt;p&gt;
&#36890;&#36807;&#23567;&#35821;&#35328;&#27169;&#22411;&#20840;&#38754;&#25913;&#36896;&#22810;&#27169;&#24577;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Overhaul of Multimodal Assistant with Small Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06199
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35774;&#35745;&#22810;&#27169;&#24577;&#23567;&#35821;&#35328;&#27169;&#22411;(MSLMs)&#21450;&#25552;&#20986;&#39640;&#25928;&#22810;&#27169;&#24577;&#21161;&#25163;Mipha&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#26041;&#38754;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#20987;&#36133;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#20026;&#24320;&#21457;&#24378;&#22823;MSLMs&#25552;&#20379;&#20102;&#35265;&#35299;&#21644;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;(MLLMs)&#23637;&#31034;&#20102;&#22312;&#19982;&#35270;&#35273;&#29702;&#35299;&#21644;&#25512;&#29702;&#30456;&#20851;&#30340;&#20219;&#21153;&#20013;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25216;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22521;&#35757;&#21644;&#25512;&#29702;&#38454;&#27573;&#30340;&#39640;&#35745;&#31639;&#38656;&#27714;&#65292;&#23427;&#20204;&#30340;&#24191;&#27867;&#24212;&#29992;&#38754;&#20020;&#38556;&#30861;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#30740;&#31350;&#21644;&#29992;&#25143;&#31038;&#21306;&#20013;&#21463;&#20247;&#30340;&#33539;&#22260;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#23567;&#35821;&#35328;&#27169;&#22411;&#65288;MSLMs&#65289;&#30340;&#35774;&#35745;&#26041;&#38754;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Mipha&#30340;&#39640;&#25928;&#22810;&#27169;&#24577;&#21161;&#25163;&#65292;&#26088;&#22312;&#22312;&#22810;&#20010;&#26041;&#38754;&#20043;&#38388;&#21019;&#36896;&#21327;&#21516;&#20316;&#29992;&#65306;&#35270;&#35273;&#34920;&#31034;&#12289;&#35821;&#35328;&#27169;&#22411;&#21644;&#20248;&#21270;&#31574;&#30053;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#19981;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#37327;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;Mipha-3B&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#32988;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;MLLMs&#65292;&#29305;&#21035;&#26159;LLaVA-1.5-13B&#12290;&#36890;&#36807;&#35814;&#32454;&#35752;&#35770;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#21457;&#23637;&#24378;&#22823;&#30340;MSLMs&#30340;&#35265;&#35299;&#21644;&#25351;&#21335;&#65292;&#20351;&#20854;&#33021;&#22815;&#19982;MLLMs&#30340;&#33021;&#21147;&#30456;&#23218;&#32654;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#20197;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06199v1 Announce Type: cross  Abstract: Multimodal Large Language Models (MLLMs) have showcased impressive skills in tasks related to visual understanding and reasoning. Yet, their widespread application faces obstacles due to the high computational demands during both the training and inference phases, restricting their use to a limited audience within the research and user communities. In this paper, we investigate the design aspects of Multimodal Small Language Models (MSLMs) and propose an efficient multimodal assistant named Mipha, which is designed to create synergy among various aspects: visual representation, language models, and optimization strategies. We show that without increasing the volume of training data, our Mipha-3B outperforms the state-of-the-art large MLLMs, especially LLaVA-1.5-13B, on multiple benchmarks. Through detailed discussion, we provide insights and guidelines for developing strong MSLMs that rival the capabilities of MLLMs. Our code is availa
&lt;/p&gt;</description></item><item><title>SciAssess&#20171;&#32461;&#20102;&#19968;&#20010;&#19987;&#20026;&#28145;&#24230;&#20998;&#26512;&#31185;&#23398;&#25991;&#29486;&#32780;&#35774;&#35745;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#20840;&#38754;&#35780;&#20272;LLMs&#22312;&#31185;&#23398;&#39046;&#22495;&#35760;&#24518;&#12289;&#29702;&#35299;&#21644;&#20998;&#26512;&#33021;&#21147;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01976</link><description>&lt;p&gt;
SciAssess&#65306;&#22522;&#20934;&#27979;&#35797;LLM&#22312;&#31185;&#23398;&#25991;&#29486;&#20998;&#26512;&#20013;&#30340;&#29087;&#32451;&#31243;&#24230;
&lt;/p&gt;
&lt;p&gt;
SciAssess: Benchmarking LLM Proficiency in Scientific Literature Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01976
&lt;/p&gt;
&lt;p&gt;
SciAssess&#20171;&#32461;&#20102;&#19968;&#20010;&#19987;&#20026;&#28145;&#24230;&#20998;&#26512;&#31185;&#23398;&#25991;&#29486;&#32780;&#35774;&#35745;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#20840;&#38754;&#35780;&#20272;LLMs&#22312;&#31185;&#23398;&#39046;&#22495;&#35760;&#24518;&#12289;&#29702;&#35299;&#21644;&#20998;&#26512;&#33021;&#21147;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01976v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032; &#25277;&#35937;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#31361;&#30772;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#65292;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#21033;&#29992;&#36825;&#20123;&#25216;&#26415;&#36827;&#34892;&#32454;&#33268;&#31185;&#23398;&#25991;&#29486;&#20998;&#26512;&#30340;&#20852;&#36259;&#28608;&#22686;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#26410;&#33021;&#20805;&#20998;&#35780;&#20272;LLMs&#22312;&#31185;&#23398;&#39046;&#22495;&#30340;&#29087;&#32451;&#31243;&#24230;&#65292;&#29305;&#21035;&#26159;&#22312;&#28041;&#21450;&#22797;&#26434;&#29702;&#35299;&#21644;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SciAssess&#65292;&#19968;&#20010;&#19987;&#20026;&#28145;&#24230;&#20998;&#26512;&#31185;&#23398;&#25991;&#29486;&#32780;&#35774;&#35745;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#20840;&#38754;&#35780;&#20272;LLMs&#30340;&#26377;&#25928;&#24615;&#12290;SciAssess&#19987;&#27880;&#20110;&#35780;&#20272;LLMs&#22312;&#31185;&#23398;&#32972;&#26223;&#19979;&#35760;&#24518;&#12289;&#29702;&#35299;&#21644;&#20998;&#26512;&#30340;&#33021;&#21147;&#12290;&#23427;&#21253;&#25324;&#26469;&#33258;&#19981;&#21516;&#31185;&#23398;&#39046;&#22495;&#30340;&#20195;&#34920;&#24615;&#20219;&#21153;&#65292;&#22914;&#19968;&#33324;&#21270;&#23398;&#12289;&#26377;&#26426;&#26448;&#26009;&#21644;&#21512;&#37329;&#26448;&#26009;&#12290;&#20005;&#26684;&#30340;&#36136;&#37327;&#25511;&#21046;&#25514;&#26045;&#30830;&#20445;&#20102;&#20854;&#22312;&#27491;&#30830;&#24615;&#12289;&#21311;&#21517;&#21270;&#21644;&#22797;&#21046;&#26041;&#38754;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01976v1 Announce Type: new  Abstract: Recent breakthroughs in Large Language Models (LLMs) have revolutionized natural language understanding and generation, igniting a surge of interest in leveraging these technologies for the nuanced field of scientific literature analysis. Existing benchmarks, however, inadequately evaluate the proficiency of LLMs in the scientific domain, especially in scenarios involving complex comprehension and multimodal data. In response, we introduced SciAssess, a benchmark tailored for the in-depth analysis of scientific literature, crafted to provide a thorough assessment of LLMs' efficacy. SciAssess focuses on evaluating LLMs' abilities in memorization, comprehension, and analysis within scientific contexts. It includes representative tasks from diverse scientific fields, such as general chemistry, organic materials, and alloy materials. And rigorous quality control measures ensure its reliability in terms of correctness, anonymization, and copy
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VIXEN&#30340;&#25216;&#26415;&#65292;&#33021;&#22815;&#29992;&#25991;&#26412;&#31616;&#27905;&#22320;&#24635;&#32467;&#19968;&#23545;&#22270;&#20687;&#20043;&#38388;&#30340;&#35270;&#35273;&#24046;&#24322;&#65292;&#20026;&#31361;&#20986;&#20869;&#23481;&#25805;&#20316;&#25552;&#20379;&#28508;&#22312;&#30340;&#32531;&#35299;&#26041;&#27861;</title><link>https://arxiv.org/abs/2402.19119</link><description>&lt;p&gt;
VIXEN: &#22270;&#20687;&#24046;&#24322;&#23383;&#24149;&#30340;&#35270;&#35273;&#25991;&#26412;&#27604;&#36739;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
VIXEN: Visual Text Comparison Network for Image Difference Captioning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19119
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VIXEN&#30340;&#25216;&#26415;&#65292;&#33021;&#22815;&#29992;&#25991;&#26412;&#31616;&#27905;&#22320;&#24635;&#32467;&#19968;&#23545;&#22270;&#20687;&#20043;&#38388;&#30340;&#35270;&#35273;&#24046;&#24322;&#65292;&#20026;&#31361;&#20986;&#20869;&#23481;&#25805;&#20316;&#25552;&#20379;&#28508;&#22312;&#30340;&#32531;&#35299;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;VIXEN - &#19968;&#31181;&#33021;&#22815;&#29992;&#25991;&#26412;&#31616;&#27905;&#22320;&#24635;&#32467;&#19968;&#23545;&#22270;&#20687;&#20043;&#38388;&#30340;&#35270;&#35273;&#24046;&#24322;&#65292;&#20197;&#31361;&#20986;&#20854;&#20013;&#30340;&#20219;&#20309;&#20869;&#23481;&#25805;&#20316;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#32593;&#32476;&#20197;&#25104;&#23545;&#30340;&#26041;&#24335;&#32447;&#24615;&#26144;&#23556;&#22270;&#20687;&#29305;&#24449;&#65292;&#26500;&#24314;&#20986;&#19968;&#20010;&#36719;&#25552;&#31034;&#65292;&#29992;&#20110;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#26368;&#36817;&#30340;InstructPix2Pix&#25968;&#25454;&#38598;&#20013;&#21033;&#29992;&#25552;&#31034;&#21040;&#25552;&#31034;&#32534;&#36753;&#26694;&#26550;&#29983;&#25104;&#30340;&#21512;&#25104;&#25805;&#20316;&#22270;&#20687;&#26469;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#22270;&#20687;&#24046;&#24322;&#23383;&#24149;&#65288;IDC&#65289;&#25968;&#25454;&#38598;&#20013;&#35757;&#32451;&#25968;&#25454;&#37327;&#23569;&#65292;&#25805;&#20316;&#31867;&#22411;&#22810;&#26679;&#24615;&#19981;&#36275;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#36890;&#36807;GPT-3&#29983;&#25104;&#30340;&#21464;&#21270;&#25688;&#35201;&#26469;&#25193;&#20805;&#36825;&#20010;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;VIXEN&#33021;&#20026;&#19981;&#21516;&#22270;&#20687;&#20869;&#23481;&#21644;&#32534;&#36753;&#31867;&#22411;&#29983;&#25104;&#26368;&#26032;&#30340;&#26131;&#25026;&#30340;&#24046;&#24322;&#23383;&#24149;&#65292;&#20026;&#38450;&#27490;&#36890;&#36807;&#25805;&#32437;&#22270;&#20687;&#20869;&#23481;&#20256;&#25773;&#30340;&#20449;&#24687;&#38169;&#35823;&#25552;&#20379;&#28508;&#22312;&#30340;&#32531;&#35299;&#26041;&#27861;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#21487;&#22312;http://github.com/alexblck/vixen&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19119v1 Announce Type: cross  Abstract: We present VIXEN - a technique that succinctly summarizes in text the visual differences between a pair of images in order to highlight any content manipulation present. Our proposed network linearly maps image features in a pairwise manner, constructing a soft prompt for a pretrained large language model. We address the challenge of low volume of training data and lack of manipulation variety in existing image difference captioning (IDC) datasets by training on synthetically manipulated images from the recent InstructPix2Pix dataset generated via prompt-to-prompt editing framework. We augment this dataset with change summaries produced via GPT-3. We show that VIXEN produces state-of-the-art, comprehensible difference captions for diverse image contents and edit types, offering a potential mitigation against misinformation disseminated via manipulated image content. Code and data are available at http://github.com/alexblck/vixen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Roofline&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#31995;&#32479;&#20998;&#26512;LLM&#25512;&#26029;&#25216;&#26415;&#65292;&#24110;&#21161;&#35782;&#21035;&#37096;&#32626;&#20013;&#30340;&#29942;&#39048;&#65292;&#24182;&#20026;&#26356;&#26377;&#25928;&#22320;&#37096;&#32626;LLM&#25552;&#20379;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.16363</link><description>&lt;p&gt;
LLM&#25512;&#26029;&#25581;&#31034;&#65306;&#35843;&#26597;&#19982;Roofline&#27169;&#22411;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
LLM Inference Unveiled: Survey and Roofline Model Insights
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Roofline&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#31995;&#32479;&#20998;&#26512;LLM&#25512;&#26029;&#25216;&#26415;&#65292;&#24110;&#21161;&#35782;&#21035;&#37096;&#32626;&#20013;&#30340;&#29942;&#39048;&#65292;&#24182;&#20026;&#26356;&#26377;&#25928;&#22320;&#37096;&#32626;LLM&#25552;&#20379;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25512;&#26029;&#39046;&#22495;&#27491;&#22312;&#36805;&#36895;&#21457;&#23637;&#65292;&#25552;&#20379;&#20102;&#26426;&#36935;&#21644;&#25361;&#25112;&#30340;&#29420;&#29305;&#32467;&#21512;&#12290;&#34429;&#28982;&#35813;&#39046;&#22495;&#24050;&#32463;&#25193;&#23637;&#24182;&#20805;&#28385;&#27963;&#21147;&#65292;&#20294;&#33267;&#20170;&#36824;&#27809;&#26377;&#19968;&#20010;&#31616;&#26126;&#30340;&#26694;&#26550;&#26469;&#20998;&#26512;LLM&#25512;&#26029;&#30340;&#21508;&#31181;&#26041;&#27861;&#65292;&#20197;&#20415;&#28165;&#26224;&#22320;&#29702;&#35299;&#36825;&#19968;&#39046;&#22495;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#19981;&#20165;&#24635;&#32467;&#20102;&#24403;&#21069;&#30740;&#31350;&#29616;&#29366;&#65292;&#36824;&#22522;&#20110;Roofline&#27169;&#22411;&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#31995;&#32479;&#20998;&#26512;LLM&#25512;&#26029;&#25216;&#26415;&#12290;&#36825;&#19968;&#26694;&#26550;&#33021;&#22815;&#24110;&#21161;&#35782;&#21035;LLM&#37096;&#32626;&#20013;&#30340;&#29942;&#39048;&#65292;&#24182;&#26356;&#28145;&#20837;&#22320;&#20102;&#35299;&#22312;&#23454;&#38469;&#35774;&#22791;&#19978;&#30340;&#23454;&#38469;&#26041;&#38754;&#65292;&#20174;&#32780;&#20026;&#37096;&#32626;LLM&#25552;&#20379;&#26356;&#26377;&#25928;&#30340;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#31995;&#32479;&#22320;&#27719;&#24635;&#20102;&#39640;&#25928;LLM&#25512;&#26029;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#28085;&#30422;&#20851;&#38190;&#39046;&#22495;&#65292;&#27604;&#22914;&#26435;&#37325;&#20248;&#21270;&#65288;&#22914;&#30693;&#35782;&#33976;&#39311;&#21644;&#37327;&#21270;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16363v1 Announce Type: cross  Abstract: The field of efficient Large Language Model (LLM) inference is rapidly evolving, presenting a unique blend of opportunities and challenges. Although the field has expanded and is vibrant, there hasn't been a concise framework that analyzes the various methods of LLM Inference to provide a clear understanding of this domain. Our survey stands out from traditional literature reviews by not only summarizing the current state of research but also by introducing a framework based on roofline model for systematic analysis of LLM inference techniques. This framework enables identifying the bottlenecks in LLM deployments and provides a deeper understanding of the practical aspects on real devices, thereby informing more effective strategies for deploying LLM. Furthermore, we systematically collate the latest advancements in efficient LLM inference, covering crucial areas such as weight optimization (e.g., Knowledge Distillation and Quantizatio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#25351;&#20196;&#20013;&#24515;&#21709;&#24212;&#30340;&#23481;&#24525;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#22797;&#26434;&#26597;&#35810;&#30340;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25581;&#31034;&#35302;&#21457;&#19981;&#36947;&#24503;&#21709;&#24212;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.15302</link><description>&lt;p&gt;
&#26377;&#20851;LLMs&#25351;&#20196;&#20013;&#24515;&#21709;&#24212;&#30340;&#65288;&#19981;&#36947;&#24503;&#65289;&#31243;&#24230;&#26377;&#22810;&#39640;&#65311;&#25581;&#31034;&#23433;&#20840;&#38450;&#25252;&#26639;&#23545;&#26377;&#23475;&#26597;&#35810;&#30340;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
How (un)ethical are instruction-centric responses of LLMs? Unveiling the vulnerabilities of safety guardrails to harmful queries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#25351;&#20196;&#20013;&#24515;&#21709;&#24212;&#30340;&#23481;&#24525;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#22797;&#26434;&#26597;&#35810;&#30340;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25581;&#31034;&#35302;&#21457;&#19981;&#36947;&#24503;&#21709;&#24212;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#19968;&#20010;&#22260;&#32469;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23433;&#20840;&#21644;&#36947;&#24503;&#20351;&#29992;&#26085;&#30410;&#20851;&#27880;&#30340;&#38382;&#39064;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#20250;&#34987;&#21508;&#31181;&#22797;&#26434;&#30340;&#26041;&#27861;&#27450;&#39575;&#65292;&#20135;&#29983;&#26377;&#23475;&#25110;&#19981;&#36947;&#24503;&#20869;&#23481;&#65292;&#21253;&#25324;&#8220;&#36234;&#29425;&#8221;&#25216;&#26415;&#21644;&#26377;&#38024;&#23545;&#24615;&#30340;&#25805;&#32437;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#38598;&#20013;&#22312;&#19968;&#20010;&#29305;&#23450;&#38382;&#39064;&#19978;&#65306;LLMs&#22312;&#35201;&#27714;&#23427;&#20204;&#29983;&#25104;&#20197;&#20266;&#20195;&#30721;&#12289;&#31243;&#24207;&#25110;&#36719;&#20214;&#29255;&#27573;&#20026;&#20013;&#24515;&#30340;&#21709;&#24212;&#26102;&#65292;&#26377;&#22810;&#22823;&#31243;&#24230;&#19978;&#21487;&#33021;&#20250;&#34987;&#35823;&#23548;&#65292;&#32780;&#19981;&#26159;&#29983;&#25104;&#26222;&#36890;&#25991;&#26412;&#12290;&#20026;&#20102;&#35843;&#26597;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;TechHazardQA&#65292;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#24212;&#20197;&#25991;&#26412;&#21644;&#20197;&#25351;&#20196;&#20026;&#20013;&#24515;&#26684;&#24335;&#65288;&#20363;&#22914;&#20266;&#20195;&#30721;&#65289;&#22238;&#31572;&#30340;&#22797;&#26434;&#26597;&#35810;&#65292;&#26088;&#22312;&#35782;&#21035;&#19981;&#36947;&#24503;&#21709;&#24212;&#30340;&#35302;&#21457;&#22120;&#12290;&#25105;&#20204;&#26597;&#35810;&#20102;&#19968;&#31995;&#21015;LLMs-- Llama-2-13b&#65292;Llama-2-7b&#65292;Mistral-V2&#21644;Mistral 8X7B--&#24182;&#35201;&#27714;&#23427;&#20204;&#29983;&#25104;&#25991;&#26412;&#21644;&#25351;&#20196;&#20026;&#20013;&#24515;&#30340;&#21709;&#24212;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15302v1 Announce Type: new  Abstract: In this study, we tackle a growing concern around the safety and ethical use of large language models (LLMs). Despite their potential, these models can be tricked into producing harmful or unethical content through various sophisticated methods, including 'jailbreaking' techniques and targeted manipulation. Our work zeroes in on a specific issue: to what extent LLMs can be led astray by asking them to generate responses that are instruction-centric such as a pseudocode, a program or a software snippet as opposed to vanilla text. To investigate this question, we introduce TechHazardQA, a dataset containing complex queries which should be answered in both text and instruction-centric formats (e.g., pseudocodes), aimed at identifying triggers for unethical responses. We query a series of LLMs -- Llama-2-13b, Llama-2-7b, Mistral-V2 and Mistral 8X7B -- and ask them to generate both text and instruction-centric responses. For evaluation we rep
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;CODIS&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#21033;&#29992;&#33258;&#30001;&#24418;&#24335;&#25991;&#26412;&#25552;&#20379;&#30340;&#19978;&#19979;&#25991;&#26469;&#22686;&#24378;&#35270;&#35273;&#29702;&#35299;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27492;&#22522;&#20934;&#19978;&#34920;&#29616;&#26410;&#36798;&#21040;&#20154;&#31867;&#27700;&#24179;&#65292;&#38656;&#35201;&#25552;&#21319;&#27169;&#22411;&#29702;&#35299;&#35270;&#35273;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.13607</link><description>&lt;p&gt;
CODIS&#65306;&#20026;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22522;&#20934;&#21270;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#35270;&#35273;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
CODIS: Benchmarking Context-Dependent Visual Comprehension for Multimodal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13607
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;CODIS&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#21033;&#29992;&#33258;&#30001;&#24418;&#24335;&#25991;&#26412;&#25552;&#20379;&#30340;&#19978;&#19979;&#25991;&#26469;&#22686;&#24378;&#35270;&#35273;&#29702;&#35299;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27492;&#22522;&#20934;&#19978;&#34920;&#29616;&#26410;&#36798;&#21040;&#20154;&#31867;&#27700;&#24179;&#65292;&#38656;&#35201;&#25552;&#21319;&#27169;&#22411;&#29702;&#35299;&#35270;&#35273;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#22312;&#32467;&#21512;&#35270;&#35273;&#21644;&#35821;&#35328;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#38543;&#30528;&#36825;&#20123;&#27169;&#22411;&#22312;&#30740;&#31350;&#21644;&#24212;&#29992;&#20013;&#21464;&#24471;&#26356;&#21152;&#37325;&#35201;&#65292;&#23545;&#23427;&#20204;&#33021;&#21147;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#30340;&#37325;&#35201;&#24615;&#20063;&#26085;&#30410;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#26410;&#32771;&#34385;&#21040;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#22270;&#20687;&#38656;&#35201;&#22312;&#26356;&#24191;&#27867;&#30340;&#19978;&#19979;&#25991;&#20013;&#34987;&#35299;&#37322;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;CODIS&#30340;&#26032;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#27169;&#22411;&#20351;&#29992;&#22312;&#33258;&#30001;&#24418;&#24335;&#25991;&#26412;&#20013;&#25552;&#20379;&#30340;&#19978;&#19979;&#25991;&#26469;&#22686;&#24378;&#35270;&#35273;&#29702;&#35299;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;MLLMs&#22312;&#36825;&#20010;&#22522;&#20934;&#19978;&#22987;&#32456;&#26080;&#27861;&#36798;&#21040;&#20154;&#31867;&#34920;&#29616;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#35777;&#23454;&#20102;&#36825;&#20123;&#27169;&#22411;&#38590;&#20197;&#26377;&#25928;&#25552;&#21462;&#21644;&#21033;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#20197;&#25552;&#39640;&#23427;&#20204;&#23545;&#22270;&#20687;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#36825;&#20984;&#26174;&#20102;&#25552;&#21319;MLLMs&#29702;&#35299;&#35270;&#35273;&#33021;&#21147;&#30340;&#36843;&#20999;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13607v1 Announce Type: cross  Abstract: Multimodal large language models (MLLMs) have demonstrated promising results in a variety of tasks that combine vision and language. As these models become more integral to research and applications, conducting comprehensive evaluations of their capabilities has grown increasingly important. However, most existing benchmarks fail to consider that, in certain situations, images need to be interpreted within a broader context. In this work, we introduce a new benchmark, named as CODIS, designed to assess the ability of models to use context provided in free-form text to enhance visual comprehension. Our findings indicate that MLLMs consistently fall short of human performance on this benchmark. Further analysis confirms that these models struggle to effectively extract and utilize contextual information to improve their understanding of images. This underscores the pressing need to enhance the ability of MLLMs to comprehend visuals in a 
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#32467;&#21512;&#24515;&#29702;&#37327;&#34920;&#36890;&#36807;LLMs&#36827;&#34892;&#38646;-shot&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;</title><link>https://arxiv.org/abs/2402.10948</link><description>&lt;p&gt;
&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#32467;&#21512;&#24515;&#29702;&#37327;&#34920;&#36827;&#34892;&#38646;-shot&#21487;&#35299;&#37322;&#30340;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Explainable Mental Health Analysis on Social Media by incorporating Mental Scales
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10948
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#32467;&#21512;&#24515;&#29702;&#37327;&#34920;&#36890;&#36807;LLMs&#36827;&#34892;&#38646;-shot&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#26041;&#27861;&#22312;&#23481;&#37327;&#26041;&#38754;&#34920;&#29616;&#24378;&#22823;&#65292;&#20294;&#32570;&#20047;&#35299;&#37322;&#33021;&#21147;&#65292;&#24182;&#19988;&#38656;&#35201;&#22823;&#35268;&#27169;&#27880;&#37322;&#30340;&#25968;&#25454;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#29983;&#25104;&#24335;&#26041;&#27861;&#26377;&#28508;&#21147;&#25670;&#33073;&#32321;&#37325;&#30340;&#27880;&#37322;&#24182;&#25552;&#20379;&#35299;&#37322;&#12290;&#21463;&#21040;&#20351;&#29992;&#37327;&#34920;&#35780;&#20272;&#24515;&#29702;&#29366;&#24577;&#30340;&#24515;&#29702;&#35780;&#20272;&#23454;&#36341;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;LLMs&#32467;&#21512;&#20102;&#20004;&#20010;&#31243;&#24207;&#12290;&#39318;&#20808;&#65292;&#24739;&#32773;&#23436;&#25104;&#24515;&#29702;&#20581;&#24247;&#38382;&#21367;&#65292;&#20854;&#27425;&#65292;&#24515;&#29702;&#23398;&#23478;&#35299;&#37322;&#26469;&#33258;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#30340;&#25910;&#38598;&#20449;&#24687;&#24182;&#20570;&#20986;&#26126;&#26234;&#20915;&#31574;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#32988;&#36807;&#20854;&#20182;&#38646;-shot&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10948v1 Announce Type: cross  Abstract: Traditional discriminative approaches in mental health analysis are known for their strong capacity but lack interpretability and demand large-scale annotated data. On the other hand, generative approaches, such as those based on large language models (LLMs),have the potential to get rid of heavy annotations and provide explanations. However, their capabilities still fall short compared to discriminative approaches, and their explanations may be unreliable due to the fact that the generation of explanation is a black-box process. Inspired by the psychological assessment practice of using scales to evaluate mental states, our method incorporates two procedures via LLMs. First, the patient completes mental health questionnaires, and second, the psychologist interprets the collected information from the mental health questions and makes informed decisions. Experimental results show that our method outperforms other zero-shot methods. Our 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20998;&#27835;&#31243;&#24207;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#28041;&#21450;&#37325;&#22797;&#23376;&#20219;&#21153;&#21644;/&#25110;&#20855;&#26377;&#27450;&#39575;&#24615;&#20869;&#23481;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;LLM&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.05359</link><description>&lt;p&gt;
&#21033;&#29992;&#20998;&#27835;&#31243;&#24207;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#38382;&#39064;&#27714;&#35299;&#36827;&#34892;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
Guiding Large Language Models with Divide-and-Conquer Program for Discerning Problem Solving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05359
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20998;&#27835;&#31243;&#24207;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#28041;&#21450;&#37325;&#22797;&#23376;&#20219;&#21153;&#21644;/&#25110;&#20855;&#26377;&#27450;&#39575;&#24615;&#20869;&#23481;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;LLM&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#65292;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22240;&#20854;&#24191;&#27867;&#30340;&#24212;&#29992;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36866;&#24403;&#30340;&#25552;&#31034;&#35774;&#35745;&#65292;&#22914;&#24605;&#32500;&#38142;&#65292;&#21487;&#20197;&#37322;&#25918;LLM&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22788;&#29702;&#28041;&#21450;&#37325;&#22797;&#23376;&#20219;&#21153;&#21644;/&#25110;&#20855;&#26377;&#27450;&#39575;&#24615;&#20869;&#23481;&#30340;&#20219;&#21153;&#65288;&#22914;&#31639;&#26415;&#35745;&#31639;&#21644;&#25991;&#31456;&#32423;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#65289;&#65292;&#29616;&#26377;&#30340;&#25552;&#31034;&#31574;&#30053;&#35201;&#20040;&#34920;&#29616;&#20986;&#34920;&#36798;&#33021;&#21147;&#19981;&#36275;&#65292;&#35201;&#20040;&#30001;&#24187;&#35273;&#24341;&#21457;&#20013;&#38388;&#38169;&#35823;&#12290;&#20026;&#20102;&#20351;LLM&#23545;&#36825;&#20123;&#20013;&#38388;&#38169;&#35823;&#26356;&#20855;&#36776;&#21035;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20998;&#27835;&#31243;&#24207;&#24341;&#23548;LLM&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#30830;&#20445;&#20248;&#36234;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#20219;&#21153;&#20998;&#35299;&#12289;&#23376;&#20219;&#21153;&#35299;&#20915;&#21644;&#35299;&#20915;&#32452;&#35013;&#36807;&#31243;&#30340;&#20998;&#31163;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#21487;&#20197;&#24341;&#23548;LLM&#25193;&#23637;&#22266;&#23450;&#28145;&#24230;Transformer&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Foundation models, such as Large language Models (LLMs), have attracted significant amount of interest due to their large number of applications. Existing works show that appropriate prompt design, such as Chain-of-Thoughts, can unlock LLM's powerful capacity in diverse areas. However, when handling tasks involving repetitive sub-tasks and/or deceptive contents, such as arithmetic calculation and article-level fake news detection, existing prompting strategies either suffers from insufficient expressive power or intermediate errors triggered by hallucination. To make LLM more discerning to such intermediate errors, we propose to guide LLM with a Divide-and-Conquer program that simultaneously ensures superior expressive power and disentangles task decomposition, sub-task resolution, and resolution assembly process. Theoretic analysis reveals that our strategy can guide LLM to extend the expressive power of fixed-depth Transformer. Experiments indicate that our proposed method can achiev
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#25945;&#32946;&#39046;&#22495;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#20998;&#31867;&#20307;&#31995;&#65292;&#24182;&#24635;&#32467;&#20102;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2401.07518</link><description>&lt;p&gt;
&#25945;&#32946;&#39046;&#22495;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#35843;&#26597;&#65306;&#20998;&#31867;&#20307;&#31995;&#12289;&#31995;&#32479;&#32508;&#36848;&#21644;&#26410;&#26469;&#36235;&#21183;
&lt;/p&gt;
&lt;p&gt;
Survey of Natural Language Processing for Education: Taxonomy, Systematic Review, and Future Trends
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07518
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#25945;&#32946;&#39046;&#22495;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#20998;&#31867;&#20307;&#31995;&#65292;&#24182;&#24635;&#32467;&#20102;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#26088;&#22312;&#36890;&#36807;&#35745;&#31639;&#26426;&#31185;&#23398;&#39046;&#22495;&#30340;&#25216;&#26415;&#20998;&#26512;&#25991;&#26412;&#65292;&#24212;&#29992;&#20110;&#21307;&#30103;&#20445;&#20581;&#12289;&#21830;&#19994;&#21644;&#25945;&#32946;&#39046;&#22495;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#25945;&#32946;&#39046;&#22495;&#65292;NLP&#24050;&#32463;&#34987;&#24212;&#29992;&#20110;&#25945;&#23398;&#21644;&#23398;&#20064;&#26041;&#38754;&#30340;&#24110;&#21161;&#12290;&#26412;&#35843;&#26597;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#35299;&#20915;&#19982;&#25945;&#32946;&#39046;&#22495;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#24182;&#22238;&#39038;&#20102;NLP&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20174;&#20171;&#32461;&#30456;&#20851;&#32972;&#26223;&#24320;&#22987;&#65292;&#28982;&#21518;&#25552;&#20986;&#25945;&#32946;&#39046;&#22495;NLP&#30340;&#20998;&#31867;&#31995;&#32479;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#26681;&#25454;&#19978;&#36848;&#20998;&#31867;&#31995;&#32479;&#35828;&#26126;&#20219;&#21153;&#23450;&#20041;&#12289;&#25361;&#25112;&#21644;&#30456;&#24212;&#30340;&#25216;&#26415;&#12290;&#20043;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#39046;&#22495;&#20013;&#30340;&#19968;&#20123;&#29616;&#26377;&#28436;&#31034;&#65292;&#24182;&#24635;&#32467;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural Language Processing (NLP) aims to analyze the text via techniques in the computer science field. It serves the applications in healthcare, commerce, and education domains. Particularly, NLP has been applied to the education domain to help teaching and learning. In this survey, we review recent advances in NLP with a focus on solving problems related to the education domain. In detail, we begin with introducing the relevant background. Then, we present the taxonomy of NLP in the education domain. Next, we illustrate the task definition, challenges, and corresponding techniques based on the above taxonomy. After that, we showcase some off-the-shelf demonstrations in this domain and conclude with future directions.
&lt;/p&gt;</description></item><item><title>MUFFIN&#26159;&#19968;&#20010;&#26032;&#30340;&#25351;&#31034;&#36981;&#24490;&#25968;&#25454;&#38598;&#31574;&#21010;&#26041;&#26696;&#65292;&#36890;&#36807;&#33258;&#21160;&#25353;&#27604;&#20363;&#25193;&#22823;&#20219;&#21153;&#65292;&#36890;&#36807;&#22810;&#31181;&#36755;&#20837;&#26041;&#38754;&#20351;&#20219;&#21153;&#20016;&#23500;&#22810;&#26679;&#12290;</title><link>https://arxiv.org/abs/2312.02436</link><description>&lt;p&gt;
MUFFIN: &#29992;&#20110;&#25913;&#21892;&#25351;&#31034;&#36981;&#24490;&#30340;&#22810;&#26041;&#38754;&#25351;&#21335;&#30340;&#31574;&#21010;
&lt;/p&gt;
&lt;p&gt;
MUFFIN: Curating Multi-Faceted Instructions for Improving Instruction-Following
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02436
&lt;/p&gt;
&lt;p&gt;
MUFFIN&#26159;&#19968;&#20010;&#26032;&#30340;&#25351;&#31034;&#36981;&#24490;&#25968;&#25454;&#38598;&#31574;&#21010;&#26041;&#26696;&#65292;&#36890;&#36807;&#33258;&#21160;&#25353;&#27604;&#20363;&#25193;&#22823;&#20219;&#21153;&#65292;&#36890;&#36807;&#22810;&#31181;&#36755;&#20837;&#26041;&#38754;&#20351;&#20219;&#21153;&#20016;&#23500;&#22810;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39046;&#22495;&#20013;&#65292;&#21152;&#24378;&#25351;&#31034;&#36981;&#24490;&#33021;&#21147;&#36890;&#24120;&#28041;&#21450;&#31574;&#21010;&#24191;&#27867;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25351;&#31034;&#36981;&#24490;&#25968;&#25454;&#38598;&#31574;&#21010;&#26041;&#26696;MUFFIN&#65292;&#20855;&#20307;&#22320;&#36890;&#36807;&#29992;&#22810;&#31181;&#36755;&#20837;&#26041;&#38754;&#20351;&#20219;&#21153;&#33258;&#21160;&#25353;&#27604;&#20363;&#25193;&#22823;&#20197;&#20016;&#23500;&#36825;&#20123;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02436v2 Announce Type: replace-cross  Abstract: In the realm of large language models (LLMs), enhancing instruction-following capability often involves curating expansive training data. This is achieved through two primary schemes: i) Scaling-Inputs: Amplifying (input, output) pairs per task instruction, aiming for better instruction adherence. ii) Scaling Input-Free Tasks: Enlarging tasks, each composed of an (instruction, output) pair (without requiring a separate input anymore). However, LLMs under Scaling-Inputs tend to be overly sensitive to inputs, leading to misinterpretation or non-compliance with instructions. Conversely, Scaling Input-Free Tasks demands a substantial number of tasks but is less effective in instruction following when dealing with instances in Scaling-Inputs. This work introduces MUFFIN, a new scheme of instruction-following dataset curation. Specifically, we automatically Scale Tasks per Input by diversifying these tasks with various input facets. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26102;&#38388;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#38646;&#26679;&#26412;&#20851;&#31995;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#29983;&#25104;&#20851;&#31995;&#34920;&#31034;&#65292;&#24182;&#23558;&#20854;&#24341;&#20837;&#22522;&#20110;&#23884;&#20837;&#30340;TKGF&#26041;&#27861;&#20013;&#65292;&#33021;&#22815;&#25429;&#25417;&#20851;&#31995;&#25551;&#36848;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#20174;&#32780;&#20351;&#24471;&#20851;&#31995;&#22312;&#24314;&#27169;&#26102;&#33021;&#20855;&#26377;&#30456;&#20284;&#30340;&#35821;&#20041;&#21547;&#20041;&#12290;</title><link>https://arxiv.org/abs/2311.10112</link><description>&lt;p&gt;
zrLLM&#65306;&#22312;&#20855;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#38388;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#38646;&#26679;&#26412;&#20851;&#31995;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
zrLLM: Zero-Shot Relational Learning on Temporal Knowledge Graphs with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26102;&#38388;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#38646;&#26679;&#26412;&#20851;&#31995;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#29983;&#25104;&#20851;&#31995;&#34920;&#31034;&#65292;&#24182;&#23558;&#20854;&#24341;&#20837;&#22522;&#20110;&#23884;&#20837;&#30340;TKGF&#26041;&#27861;&#20013;&#65292;&#33021;&#22815;&#25429;&#25417;&#20851;&#31995;&#25551;&#36848;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#20174;&#32780;&#20351;&#24471;&#20851;&#31995;&#22312;&#24314;&#27169;&#26102;&#33021;&#20855;&#26377;&#30456;&#20284;&#30340;&#35821;&#20041;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#21270;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#30693;&#35782;&#22312;&#26102;&#38388;&#30693;&#35782;&#22270;(TKGs)&#19978;&#24050;&#25104;&#20026;&#19968;&#20010;&#28861;&#28909;&#35805;&#39064;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#39044;&#27979;TKGs&#19978;&#30340;&#38142;&#25509;&#12290;&#20854;&#20013;&#22823;&#22810;&#25968;&#26159;&#22522;&#20110;&#23884;&#20837;&#30340;&#65292;&#20854;&#20013;&#23398;&#20064;&#38544;&#34255;&#34920;&#31034;&#20197;&#22522;&#20110;&#35266;&#23519;&#21040;&#30340;&#22270;&#19978;&#19979;&#25991;&#26469;&#34920;&#31034;&#30693;&#35782;&#22270;(KG)&#23454;&#20307;&#21644;&#20851;&#31995;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#22312;&#20256;&#32479;&#30340;TKG&#39044;&#27979;(TKGF)&#22522;&#20934;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#22312;&#24314;&#27169;&#27809;&#26377;&#20808;&#21069;&#22270;&#19978;&#19979;&#25991;&#30340;&#26410;&#35265;&#36807;&#30340;&#38646;&#26679;&#26412;&#20851;&#31995;&#19978;&#38754;&#20020;&#24378;&#28872;&#25361;&#25112;&#12290;&#26412;&#25991;&#23581;&#35797;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#22914;&#19979;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;KG&#20851;&#31995;&#30340;&#25991;&#26412;&#25551;&#36848;&#36755;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20013;&#20197;&#29983;&#25104;&#20851;&#31995;&#34920;&#31034;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#24341;&#20837;&#22522;&#20110;&#23884;&#20837;&#30340;TKGF&#26041;&#27861;&#20013;&#12290;LLM&#22686;&#24378;&#30340;&#34920;&#31034;&#21487;&#20197;&#25429;&#25417;&#20851;&#31995;&#25551;&#36848;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#36825;&#20351;&#24471;&#20851;&#31995;&#65292;&#26080;&#35770;&#26159;&#24050;&#35265;&#36824;&#26159;&#26410;&#35265;&#30340;&#65292;&#37117;&#33021;&#22815;&#33719;&#24471;&#31867;&#20284;&#30340;&#35821;&#20041;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10112v2 Announce Type: replace  Abstract: Modeling evolving knowledge over temporal knowledge graphs (TKGs) has become a heated topic. Various methods have been proposed to forecast links on TKGs. Most of them are embedding-based, where hidden representations are learned to represent knowledge graph (KG) entities and relations based on the observed graph contexts. Although these methods show strong performance on traditional TKG forecasting (TKGF) benchmarks, they face a strong challenge in modeling the unseen zero-shot relations that have no prior graph context. In this paper, we try to mitigate this problem as follows. We first input the text descriptions of KG relations into large language models (LLMs) for generating relation representations, and then introduce them into embedding-based TKGF methods. LLM-empowered representations can capture the semantic information in the relation descriptions. This makes the relations, whether seen or unseen, with similar semantic mean
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35745;&#31639;&#36777;&#35770;&#39046;&#22495;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#35774;&#32622;&#65292;&#26631;&#20934;&#21270;&#20102;14&#20010;&#24320;&#28304;&#25968;&#25454;&#38598;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#21453;&#35328;&#29983;&#25104;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2311.09022</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35745;&#31639;&#36777;&#35770;&#20013;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Exploring the Potential of Large Language Models in Computational Argumentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09022
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35745;&#31639;&#36777;&#35770;&#39046;&#22495;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#35774;&#32622;&#65292;&#26631;&#20934;&#21270;&#20102;14&#20010;&#24320;&#28304;&#25968;&#25454;&#38598;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#21453;&#35328;&#29983;&#25104;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#36777;&#35770;&#24050;&#25104;&#20026;&#21253;&#25324;&#20154;&#24037;&#26234;&#33021;&#12289;&#27861;&#24459;&#21644;&#20844;&#20849;&#25919;&#31574;&#22312;&#20869;&#30340;&#21508;&#20010;&#39046;&#22495;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#24037;&#20855;&#12290;&#20316;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26032;&#20852;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#35745;&#31639;&#36777;&#35770;&#21560;&#24341;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#30740;&#31350;&#35745;&#31639;&#36777;&#35770;&#20027;&#35201;&#28041;&#21450;&#20004;&#31867;&#20219;&#21153;&#65306;&#36777;&#35770;&#25366;&#25496;&#21644;&#36777;&#35770;&#29983;&#25104;&#12290;&#37492;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#19978;&#19979;&#25991;&#21644;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#35780;&#20272;LLMs&#22312;&#21508;&#31181;&#35745;&#31639;&#36777;&#35770;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#26159;&#20540;&#24471;&#30340;&#12290;&#26412;&#24037;&#20316;&#26088;&#22312;&#35780;&#20272;LLMs&#65288;&#20363;&#22914;ChatGPT&#12289;Flan&#21644;LLaMA2&#27169;&#22411;&#65289;&#22312;&#35745;&#31639;&#36777;&#35770;&#39046;&#22495;&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#35774;&#32622;&#19979;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#23558;&#29616;&#26377;&#20219;&#21153;&#20998;&#20026;&#20845;&#20010;&#20027;&#35201;&#31867;&#21035;&#65292;&#24182;&#23545;&#21313;&#22235;&#20010;&#24320;&#28304;&#25968;&#25454;&#38598;&#30340;&#26684;&#24335;&#36827;&#34892;&#20102;&#26631;&#20934;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20851;&#20110;&#21453;&#35328;&#29983;&#25104;&#30340;&#26032;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09022v2 Announce Type: replace  Abstract: Computational argumentation has become an essential tool in various fields, including artificial intelligence, law, and public policy. It is an emerging research field in natural language processing that attracts increasing attention. Research on computational argumentation mainly involves two types of tasks: argument mining and argument generation. As large language models have demonstrated strong abilities in understanding context and generating natural language, it is worthwhile to evaluate the performance of LLMs on various computational argumentation tasks. This work aims to embark on an assessment of LLMs, such as ChatGPT, Flan models and LLaMA2 models, under zero-shot and few-shot settings within the realm of computational argumentation. We organize existing tasks into six main categories and standardise the format of fourteen open-sourced datasets. In addition, we present a new benchmark dataset on counter speech generation, 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#26041;&#27861;Follow-up Differential Descriptions&#65288;FuDD&#65289;&#65292;&#36890;&#36807;&#20026;&#27599;&#20010;&#22270;&#20687;&#30830;&#23450;&#27169;&#31946;&#31867;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26032;&#30340;&#31867;&#25551;&#36848;&#65292;&#20197;&#26356;&#22909;&#22320;&#21306;&#20998;&#30446;&#26631;&#31867;&#12290;</title><link>https://arxiv.org/abs/2311.07593</link><description>&lt;p&gt;
&#36319;&#36827;&#24046;&#20998;&#25551;&#36848;&#65306;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#27495;&#20041;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Follow-Up Differential Descriptions: Language Models Resolve Ambiguities for Image Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07593
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#26041;&#27861;Follow-up Differential Descriptions&#65288;FuDD&#65289;&#65292;&#36890;&#36807;&#20026;&#27599;&#20010;&#22270;&#20687;&#30830;&#23450;&#27169;&#31946;&#31867;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26032;&#30340;&#31867;&#25551;&#36848;&#65292;&#20197;&#26356;&#22909;&#22320;&#21306;&#20998;&#30446;&#26631;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#25913;&#21892;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#24615;&#33021;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#25193;&#23637;&#31867;&#25551;&#36848;&#65288;&#21363;&#25552;&#31034;&#65289;&#30340;&#30456;&#20851;&#23646;&#24615;&#65292;&#20363;&#22914;&#20351;&#29992;&#26837;&#33394;&#40635;&#38592;&#20195;&#26367;&#40635;&#38592;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#38646;&#26679;&#26412;&#26041;&#27861;&#26080;&#35770;&#30446;&#26631;&#31867;&#20043;&#38388;&#30340;&#20849;&#21516;&#20043;&#22788;&#22914;&#20309;&#65292;&#37117;&#20250;&#36873;&#25321;&#19968;&#32452;&#23646;&#24615;&#65292;&#21487;&#33021;&#25552;&#20379;&#27809;&#26377;&#24110;&#21161;&#21306;&#20998;&#23427;&#20204;&#30340;&#26377;&#29992;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Follow-up Differential Descriptions&#65288;FuDD&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#38646;&#26679;&#26412;&#26041;&#27861;&#65292;&#21487;&#20197;&#26681;&#25454;&#27599;&#20010;&#25968;&#25454;&#38598;&#37327;&#36523;&#23450;&#21046;&#31867;&#25551;&#36848;&#65292;&#24182;&#25552;&#20379;&#26356;&#22909;&#21306;&#20998;&#30446;&#26631;&#31867;&#30340;&#38468;&#21152;&#23646;&#24615;&#12290;FuDD&#39318;&#20808;&#20026;&#27599;&#20010;&#22270;&#20687;&#30830;&#23450;&#27169;&#31946;&#31867;&#65292;&#28982;&#21518;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#26032;&#30340;&#31867;&#25551;&#36848;&#65292;&#20197;&#21306;&#20998;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07593v2 Announce Type: replace  Abstract: A promising approach for improving the performance of vision-language models like CLIP for image classification is to extend the class descriptions (i.e., prompts) with related attributes, e.g., using brown sparrow instead of sparrow. However, current zero-shot methods select a subset of attributes regardless of commonalities between the target classes, potentially providing no useful information that would have helped to distinguish between them. For instance, they may use color instead of bill shape to distinguish between sparrows and wrens, which are both brown. We propose Follow-up Differential Descriptions (FuDD), a zero-shot approach that tailors the class descriptions to each dataset and leads to additional attributes that better differentiate the target classes. FuDD first identifies the ambiguous classes for each image, and then uses a Large Language Model (LLM) to generate new class descriptions that differentiate between t
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20869;&#22312;&#24605;&#32771;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;&#35821;&#35328;&#23398;&#21644;&#35748;&#30693;&#31185;&#23398;&#30340;&#28789;&#24863;&#65292;&#36171;&#20104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27807;&#36890;&#25216;&#33021;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#25311;&#20154;&#21270;&#21644;&#20027;&#21160;&#24615;&#65292;&#21560;&#24341;&#29992;&#25143;&#36827;&#34892;&#26356;&#38271;&#26102;&#38388;&#30340;&#23545;&#35805;</title><link>https://arxiv.org/abs/2311.07445</link><description>&lt;p&gt;
&#24910;&#35328;&#65306;&#36890;&#36807;&#20869;&#24515;&#29420;&#30333;&#22521;&#20859;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27807;&#36890;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Think Before You Speak: Cultivating Communication Skills of Large Language Models via Inner Monologue
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07445
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20869;&#22312;&#24605;&#32771;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;&#35821;&#35328;&#23398;&#21644;&#35748;&#30693;&#31185;&#23398;&#30340;&#28789;&#24863;&#65292;&#36171;&#20104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27807;&#36890;&#25216;&#33021;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#25311;&#20154;&#21270;&#21644;&#20027;&#21160;&#24615;&#65292;&#21560;&#24341;&#29992;&#25143;&#36827;&#34892;&#26356;&#38271;&#26102;&#38388;&#30340;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#31995;&#32479;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#29983;&#25104;&#27969;&#30021;&#12289;&#36830;&#36143;&#21644;&#22810;&#26679;&#21270;&#30340;&#22238;&#22797;&#12290;&#28982;&#32780;&#65292;LLMs&#20173;&#28982;&#32570;&#20047;&#19968;&#39033;&#20851;&#38190;&#33021;&#21147;&#65306;&#27807;&#36890;&#25216;&#24039;&#12290;&#36825;&#31181;&#23616;&#38480;&#20351;&#23427;&#20204;&#26356;&#20687;&#20449;&#24687;&#25628;&#32034;&#24037;&#20855;&#65292;&#32780;&#19981;&#26159;&#25311;&#20154;&#21270;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;&#38656;&#35201;&#32771;&#34385;&#27807;&#36890;&#25216;&#33021;&#65292;&#22914;&#20027;&#39064;&#36807;&#28193;&#12289;&#20027;&#21160;&#25552;&#38382;&#12289;&#27010;&#24565;&#24341;&#23548;&#12289;&#21516;&#29702;&#24515;&#21644;&#24635;&#32467;&#65292;&#20351;LLMs&#22312;&#23545;&#35805;&#20013;&#26356;&#20855;&#25311;&#20154;&#21270;&#21644;&#20027;&#21160;&#24615;&#65292;&#20174;&#32780;&#22686;&#21152;&#29992;&#25143;&#30340;&#20852;&#36259;&#65292;&#21560;&#24341;&#20182;&#20204;&#36827;&#34892;&#26356;&#38271;&#26102;&#38388;&#30340;&#20132;&#35848;&#12290;&#28982;&#32780;&#65292;&#22312;&#40657;&#21283;&#23376;LLMs&#20013;&#21551;&#29992;&#36825;&#20123;&#27807;&#36890;&#25216;&#33021;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#27809;&#26377;&#19982;&#30495;&#20154;&#30456;&#21516;&#30340;&#35805;&#35821;&#24418;&#25104;&#27169;&#24335;&#65306;&#20808;&#24605;&#21518;&#35828;&#12290;&#21463;&#35821;&#35328;&#23398;&#21644;&#35748;&#30693;&#31185;&#23398;&#21551;&#21457;&#65292;&#25105;&#20204;&#36890;&#36807;&#20869;&#22312;&#24605;&#32771;&#36171;&#20104;LLMs&#27807;&#36890;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07445v2 Announce Type: replace-cross  Abstract: The emergence of large language models (LLMs) further improves the capabilities of open-domain dialogue systems and can generate fluent, coherent, and diverse responses. However, LLMs still lack a crucial ability: communication skills. This limitation renders them more like information seeking tools rather than anthropomorphic chatbots. Communication skills, such as topic transition, proactively asking questions, concept guidance, empathy, and summarising often should be taken into consideration, to make LLMs more anthropomorphic and proactive during the conversation, thereby increasing the interest of users and attracting them to chat for longer. However, enabling these communication skills in black-box LLMs remains a key challenge because they do not have the same utterance formation mode as real people: think before speaking. Inspired by linguistics and cognitive science, we empower LLMs with communication skills through inn
&lt;/p&gt;</description></item><item><title>LILO&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#21512;&#25104;&#12289;&#21387;&#32553;&#21644;&#25991;&#26723;&#21270;&#20195;&#30721;&#26469;&#26500;&#24314;&#21487;&#35299;&#37322;&#19988;&#36866;&#29992;&#20110;&#29305;&#23450;&#38382;&#39064;&#39046;&#22495;&#30340;&#31243;&#24207;&#24211;&#12290;&#22312;&#20854;&#20013;&#65292;LILO&#32467;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#30340;&#31243;&#24207;&#21512;&#25104;&#21644;&#31243;&#24207;&#33258;&#21160;&#37325;&#26500;&#30340;&#31639;&#27861;&#36827;&#23637;&#65292;&#24182;&#19988;&#36890;&#36807;&#33258;&#21160;&#25991;&#26723;&#36807;&#31243;&#20351;&#24471;&#20195;&#30721;&#25277;&#35937;&#21487;&#35299;&#37322;&#24182;&#25552;&#21319;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2310.19791</link><description>&lt;p&gt;
LILO&#65306;&#36890;&#36807;&#21387;&#32553;&#21644;&#25991;&#26723;&#21270;&#20195;&#30721;&#23398;&#20064;&#21487;&#35299;&#37322;&#24211;
&lt;/p&gt;
&lt;p&gt;
LILO: Learning Interpretable Libraries by Compressing and Documenting Code
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.19791
&lt;/p&gt;
&lt;p&gt;
LILO&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#21512;&#25104;&#12289;&#21387;&#32553;&#21644;&#25991;&#26723;&#21270;&#20195;&#30721;&#26469;&#26500;&#24314;&#21487;&#35299;&#37322;&#19988;&#36866;&#29992;&#20110;&#29305;&#23450;&#38382;&#39064;&#39046;&#22495;&#30340;&#31243;&#24207;&#24211;&#12290;&#22312;&#20854;&#20013;&#65292;LILO&#32467;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#30340;&#31243;&#24207;&#21512;&#25104;&#21644;&#31243;&#24207;&#33258;&#21160;&#37325;&#26500;&#30340;&#31639;&#27861;&#36827;&#23637;&#65292;&#24182;&#19988;&#36890;&#36807;&#33258;&#21160;&#25991;&#26723;&#36807;&#31243;&#20351;&#24471;&#20195;&#30721;&#25277;&#35937;&#21487;&#35299;&#37322;&#24182;&#25552;&#21319;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#36719;&#20214;&#24320;&#21457;&#30340;&#20851;&#38190;&#26041;&#38754;&#26159;&#37325;&#26500;&#30340;&#33402;&#26415;&#65306;&#23558;&#20195;&#30721;&#25972;&#21512;&#21040;&#21487;&#37325;&#29992;&#21644;&#21487;&#35835;&#30340;&#31243;&#24207;&#24211;&#20013;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LILO&#30340;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#36845;&#20195;&#22320;&#21512;&#25104;&#12289;&#21387;&#32553;&#21644;&#25991;&#26723;&#21270;&#20195;&#30721;&#26469;&#26500;&#24314;&#36866;&#21512;&#29305;&#23450;&#38382;&#39064;&#39046;&#22495;&#30340;&#24211;&#12290;LILO&#23558;LLM&#24341;&#23548;&#30340;&#31243;&#24207;&#21512;&#25104;&#19982;Stitch&#33258;&#21160;&#37325;&#26500;&#30340;&#36817;&#26399;&#31639;&#27861;&#36827;&#23637;&#30456;&#32467;&#21512;&#65306;Stitch&#26159;&#19968;&#20010;&#31526;&#21495;&#21387;&#32553;&#31995;&#32479;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35782;&#21035;&#22823;&#22411;&#20195;&#30721;&#35821;&#26009;&#24211;&#20013;&#30340;&#26368;&#20339;lambda&#25277;&#35937;&#12290;&#20026;&#20102;&#20351;&#36825;&#20123;&#25277;&#35937;&#21487;&#35299;&#37322;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#21160;&#25991;&#26723;&#65288;AutoDoc&#65289;&#36807;&#31243;&#65292;&#23427;&#26681;&#25454;&#19978;&#19979;&#25991;&#20013;&#30340;&#20351;&#29992;&#31034;&#20363;&#25512;&#26029;&#20986;&#33258;&#28982;&#35821;&#35328;&#21517;&#31216;&#21644;&#25991;&#26723;&#23383;&#31526;&#20018;&#12290;&#38500;&#20102;&#25552;&#39640;&#20154;&#31867;&#21487;&#35835;&#24615;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;AutoDoc&#36890;&#36807;&#24110;&#21161;LILO&#30340;&#21512;&#25104;&#22120;&#35299;&#37322;&#21644;&#37096;&#32626;&#23398;&#20064;&#21040;&#30340;&#25277;&#35937;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;LILO&#36827;&#34892;&#20102;&#19977;&#20010;&#24402;&#32435;&#24335;&#31243;&#24207;&#32508;&#21512;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models (LLMs) now excel at code generation, a key aspect of software development is the art of refactoring: consolidating code into libraries of reusable and readable programs. In this paper, we introduce LILO, a neurosymbolic framework that iteratively synthesizes, compresses, and documents code to build libraries tailored to particular problem domains. LILO combines LLM-guided program synthesis with recent algorithmic advances in automated refactoring from Stitch: a symbolic compression system that efficiently identifies optimal lambda abstractions across large code corpora. To make these abstractions interpretable, we introduce an auto-documentation (AutoDoc) procedure that infers natural language names and docstrings based on contextual examples of usage. In addition to improving human readability, we find that AutoDoc boosts performance by helping LILO's synthesizer to interpret and deploy learned abstractions. We evaluate LILO on three inductive program synth
&lt;/p&gt;</description></item><item><title>XAL&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#40723;&#21169;&#20998;&#31867;&#22120;&#25552;&#20379;&#25512;&#26029;&#30340;&#29702;&#30001;&#24182;&#28145;&#20837;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#20174;&#32780;&#25552;&#21319;&#20302;&#36164;&#28304;&#25991;&#26412;&#20998;&#31867;&#30340;&#24615;&#33021;</title><link>https://arxiv.org/abs/2310.05502</link><description>&lt;p&gt;
XAL&#65306;&#21487;&#35299;&#37322;&#30340;&#20027;&#21160;&#23398;&#20064;&#25552;&#21319;&#20102;&#20302;&#36164;&#28304;&#23398;&#20064;&#32773;&#30340;&#20998;&#31867;&#22120;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
XAL: EXplainable Active Learning Makes Classifiers Better Low-resource Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05502
&lt;/p&gt;
&lt;p&gt;
XAL&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#40723;&#21169;&#20998;&#31867;&#22120;&#25552;&#20379;&#25512;&#26029;&#30340;&#29702;&#30001;&#24182;&#28145;&#20837;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#20174;&#32780;&#25552;&#21319;&#20302;&#36164;&#28304;&#25991;&#26412;&#20998;&#31867;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#65288;AL&#65289;&#26088;&#22312;&#36890;&#36807;&#36845;&#20195;&#22320;&#31579;&#36873;&#26368;&#20855;&#24418;&#25104;&#24615;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#27880;&#37322;&#65292;&#26500;&#24314;&#26377;&#25928;&#30340;&#35757;&#32451;&#38598;&#65292;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#20302;&#36164;&#28304;&#20219;&#21153;&#12290;&#22823;&#22810;&#25968;&#20998;&#31867;&#20013;&#30340;&#20027;&#21160;&#23398;&#20064;&#25216;&#26415;&#20381;&#36182;&#20110;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#25110;&#20998;&#27495;&#26469;&#36873;&#25321;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#20250;&#20986;&#29616;&#23545;&#34920;&#38754;&#27169;&#24335;&#30340;&#36807;&#24230;&#33258;&#20449;&#21644;&#32570;&#20047;&#25506;&#32034;&#30340;&#38382;&#39064;&#12290;&#21463;&#21040;&#20154;&#31867;&#26681;&#25454;&#22240;&#26524;&#20449;&#24687;&#25512;&#26029;&#21644;&#39044;&#27979;&#30340;&#35748;&#30693;&#36807;&#31243;&#21551;&#21457;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#23558;&#29702;&#30001;&#34701;&#20837;AL&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38754;&#21521;&#20302;&#36164;&#28304;&#25991;&#26412;&#20998;&#31867;&#30340;&#21487;&#35299;&#37322;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65288;XAL&#65289;&#65292;&#26088;&#22312;&#40723;&#21169;&#20998;&#31867;&#22120;&#35777;&#26126;&#20854;&#25512;&#26029;&#24182;&#28145;&#20837;&#30740;&#31350;&#26080;&#27861;&#25552;&#20379;&#21512;&#29702;&#35299;&#37322;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#38500;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#21452;&#21521;&#32534;&#30721;&#22120;&#36827;&#34892;&#20998;&#31867;&#22806;&#65292;&#25105;&#20204;&#36824;&#37319;&#29992;&#39044;&#35757;&#32451;&#30340;&#21333;&#21521;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.05502v2 Announce Type: replace  Abstract: Active learning (AL), which aims to construct an effective training set by iteratively curating the most formative unlabeled data for annotation, has been widely used in low-resource tasks. Most active learning techniques in classification rely on the model's uncertainty or disagreement to choose unlabeled data, suffering from the problem of over-confidence in superficial patterns and a lack of exploration. Inspired by the cognitive processes in which humans deduce and predict through causal information, we take an initial attempt towards integrating rationales into AL and propose a novel Explainable Active Learning framework (XAL) for low-resource text classification, which aims to encourage classifiers to justify their inferences and delve into unlabeled data for which they cannot provide reasonable explanations. Specifically, besides using a pre-trained bi-directional encoder for classification, we employ a pre-trained uni-directi
&lt;/p&gt;</description></item><item><title>Platypus&#26159;&#19968;&#32452;&#32463;&#36807;&#31934;&#32454;&#35843;&#33410;&#21644;&#21512;&#24182;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22312;&#20840;&#29699;&#24320;&#25918;LLM&#25490;&#34892;&#27036;&#19978;&#34920;&#29616;&#26368;&#20986;&#33394;&#65292;&#22312;&#24494;&#35843;&#25968;&#25454;&#21644;&#35745;&#31639;&#37327;&#19978;&#20165;&#38656;&#20854;&#20182;&#26041;&#27861;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;</title><link>https://arxiv.org/abs/2308.07317</link><description>&lt;p&gt;
Platypus: LLM&#30340;&#24555;&#36895;&#12289;&#24265;&#20215;&#21644;&#24378;&#22823;&#30340;&#32454;&#21270;
&lt;/p&gt;
&lt;p&gt;
Platypus: Quick, Cheap, and Powerful Refinement of LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.07317
&lt;/p&gt;
&lt;p&gt;
Platypus&#26159;&#19968;&#32452;&#32463;&#36807;&#31934;&#32454;&#35843;&#33410;&#21644;&#21512;&#24182;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22312;&#20840;&#29699;&#24320;&#25918;LLM&#25490;&#34892;&#27036;&#19978;&#34920;&#29616;&#26368;&#20986;&#33394;&#65292;&#22312;&#24494;&#35843;&#25968;&#25454;&#21644;&#35745;&#31639;&#37327;&#19978;&#20165;&#38656;&#20854;&#20182;&#26041;&#27861;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Platypus&#65292;&#36825;&#26159;&#19968;&#32452;&#31934;&#32454;&#35843;&#33410;&#21644;&#21512;&#24182;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22312;&#21457;&#24067;&#26412;&#25991;&#26102;&#22312;HuggingFace&#30340;&#24320;&#25918;LLM&#25490;&#34892;&#27036;&#19978;&#34920;&#29616;&#26368;&#20986;&#33394;&#65292;&#30446;&#21069;&#20301;&#21015;&#31532;&#19968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#65288;1&#65289;&#25105;&#20204;&#31934;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;Open-Platypus&#65292;&#36825;&#26159;&#20854;&#20182;&#24320;&#25918;&#25968;&#25454;&#38598;&#30340;&#19968;&#20010;&#23376;&#38598;&#65292;&#25105;&#20204;&#21521;&#20844;&#20247;&#24320;&#25918;&#65307;&#65288;2&#65289;&#25105;&#20204;&#22312;&#24494;&#35843;&#21644;&#21512;&#24182;LoRA&#27169;&#22359;&#30340;&#36807;&#31243;&#20013;&#22914;&#20309;&#20445;&#30041;&#39044;&#35757;&#32451;LLMs&#30340;&#24378;&#20808;&#39564;&#65292;&#24182;&#23558;&#29305;&#23450;&#39046;&#22495;&#30693;&#35782;&#21576;&#29616;&#20986;&#26469;&#65307;&#65288;3&#65289;&#25105;&#20204;&#22312;&#26816;&#26597;&#27979;&#35797;&#25968;&#25454;&#27844;&#38690;&#21644;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#27745;&#26579;&#26041;&#38754;&#30340;&#21162;&#21147;&#65292;&#36825;&#21487;&#20197;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20449;&#24687;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;Platypus&#31995;&#21015;&#22312;&#21508;&#31181;&#27169;&#22411;&#22823;&#23567;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20197;&#36739;&#23569;&#30340;&#24494;&#35843;&#25968;&#25454;&#21644;&#24635;&#20307;&#35745;&#31639;&#37327;&#33719;&#24471;&#20102;&#24378;&#22823;&#30340;&#23450;&#37327;LLM&#25351;&#26631;&#65292;&#20301;&#23621;&#20840;&#29699;&#24320;&#25918;LLM&#25490;&#34892;&#27036;&#27036;&#39318;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.07317v2 Announce Type: replace  Abstract: We present $\textbf{Platypus}$, a family of fine-tuned and merged Large Language Models (LLMs) that achieves the strongest performance and currently stands at first place in HuggingFace's Open LLM Leaderboard as of the release date of this work. In this work we describe (1) our curated dataset $\textbf{Open-Platypus}$, that is a subset of other open datasets and which $\textit{we release to the public}$ (2) our process of fine-tuning and merging LoRA modules in order to conserve the strong prior of pretrained LLMs, while bringing specific domain knowledge to the surface (3) our efforts in checking for test data leaks and contamination in the training data, which can inform future research. Specifically, the Platypus family achieves strong performance in quantitative LLM metrics across model sizes, topping the global Open LLM leaderboard while using just a fraction of the fine-tuning data and overall compute that are required for othe
&lt;/p&gt;</description></item><item><title>K-prune &#26159;&#19968;&#31181;&#38024;&#23545;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31934;&#30830;&#26080;&#38656;&#37325;&#35757;&#32451;&#30340;&#32467;&#26500;&#21270;&#21098;&#26525;&#31639;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#21098;&#26525;&#36807;&#31243;&#20445;&#30041;&#26377;&#29992;&#30693;&#35782;&#20197;&#26368;&#23567;&#21270;&#21098;&#26525;&#38169;&#35823;&#65292;&#26174;&#33879;&#25552;&#21319;&#31934;&#30830;&#24230;&#12290;</title><link>https://arxiv.org/abs/2308.03449</link><description>&lt;p&gt;
&#38024;&#23545;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31934;&#30830;&#26080;&#38656;&#37325;&#35757;&#32451;&#21098;&#26525;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Accurate Retraining-free Pruning for Pretrained Encoder-based Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.03449
&lt;/p&gt;
&lt;p&gt;
K-prune &#26159;&#19968;&#31181;&#38024;&#23545;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31934;&#30830;&#26080;&#38656;&#37325;&#35757;&#32451;&#30340;&#32467;&#26500;&#21270;&#21098;&#26525;&#31639;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#21098;&#26525;&#36807;&#31243;&#20445;&#30041;&#26377;&#29992;&#30693;&#35782;&#20197;&#26368;&#23567;&#21270;&#21098;&#26525;&#38169;&#35823;&#65292;&#26174;&#33879;&#25552;&#21319;&#31934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#22914;&#20309;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#23545;&#20854;&#36827;&#34892;&#31934;&#30830;&#21387;&#32553;&#65311;&#23545;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21387;&#32553;&#26469;&#35828;&#65292;&#26080;&#38656;&#37325;&#35757;&#32451;&#30340;&#32467;&#26500;&#21270;&#21098;&#26525;&#31639;&#27861;&#38750;&#24120;&#20851;&#38190;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#26174;&#33879;&#38477;&#20302;&#21098;&#26525;&#25104;&#26412;&#24182;&#33021;&#22815;&#21098;&#26525;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26080;&#38656;&#37325;&#35757;&#32451;&#31639;&#27861;&#23384;&#22312;&#20005;&#37325;&#30340;&#31934;&#24230;&#19979;&#38477;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#26410;&#33021;&#22788;&#29702;&#21098;&#26525;&#38169;&#35823;&#65292;&#23588;&#20854;&#22312;&#39640;&#21387;&#32553;&#29575;&#24773;&#20917;&#19979;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;K-prune&#65288;&#30693;&#35782;&#20445;&#30041;&#21098;&#26525;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31934;&#30830;&#26080;&#38656;&#37325;&#35757;&#32451;&#30340;&#32467;&#26500;&#21270;&#21098;&#26525;&#31639;&#27861;&#12290;K-prune&#19987;&#27880;&#20110;&#20445;&#30041;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26377;&#29992;&#30693;&#35782;&#65292;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#36845;&#20195;&#21098;&#26525;&#36807;&#31243;&#65288;&#21253;&#25324;&#30693;&#35782;&#27979;&#37327;&#12289;&#30693;&#35782;&#20445;&#30041;&#33945;&#29256;&#25628;&#32034;&#21644;&#30693;&#35782;&#20445;&#30041;&#26435;&#37325;&#35843;&#25972;&#65289;&#65292;&#20197;&#26368;&#23567;&#21270;&#21098;&#26525;&#38169;&#35823;&#12290;&#22240;&#27492;&#65292;K-prune&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#31934;&#30830;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.03449v2 Announce Type: replace  Abstract: Given a pretrained encoder-based language model, how can we accurately compress it without retraining? Retraining-free structured pruning algorithms are crucial in pretrained language model compression due to their significantly reduced pruning cost and capability to prune large language models. However, existing retraining-free algorithms encounter severe accuracy degradation, as they fail to handle pruning errors, especially at high compression rates. In this paper, we propose K-prune (Knowledge-preserving pruning), an accurate retraining-free structured pruning algorithm for pretrained encoder-based language models. K-prune focuses on preserving the useful knowledge of the pretrained model to minimize pruning errors through a carefully designed iterative pruning process composed of knowledge measurement, knowledge-preserving mask search, and knowledge-preserving weight-tuning. As a result, K-prune shows significant accuracy improv
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Diana&#30340;&#21160;&#24577;&#26550;&#26500;&#32456;&#36523;QA&#27169;&#22411;&#65292;&#36890;&#36807;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#19968;&#31995;&#21015;QA&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;&#22235;&#31181;&#23618;&#27425;&#32452;&#32455;&#30340;&#25552;&#31034;&#26469;&#25429;&#33719;&#19981;&#21516;&#31890;&#24230;&#30340;QA&#30693;&#35782;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2208.14602</link><description>&lt;p&gt;
&#20855;&#26377;&#32467;&#26500;&#21270;&#25552;&#31034;&#30340;&#25345;&#32493;&#38382;&#31572;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Continuous QA Learning with Structured Prompts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2208.14602
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Diana&#30340;&#21160;&#24577;&#26550;&#26500;&#32456;&#36523;QA&#27169;&#22411;&#65292;&#36890;&#36807;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#19968;&#31995;&#21015;QA&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;&#22235;&#31181;&#23618;&#27425;&#32452;&#32455;&#30340;&#25552;&#31034;&#26469;&#25429;&#33719;&#19981;&#21516;&#31890;&#24230;&#30340;QA&#30693;&#35782;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#32456;&#36523;&#23398;&#20064;&#65288;LL&#65289;&#33021;&#21147;&#30340;QA&#27169;&#22411;&#23545;&#20110;&#23454;&#38469;&#30340;QA&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#19988;&#22522;&#20110;&#26550;&#26500;&#30340;LL&#26041;&#27861;&#34987;&#25253;&#21578;&#20026;&#36825;&#20123;&#27169;&#22411;&#30340;&#26377;&#25928;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#23558;&#20808;&#21069;&#30340;&#26041;&#27861;&#25193;&#23637;&#21040;QA&#20219;&#21153;&#24182;&#19981;&#26159;&#19968;&#20214;&#31616;&#21333;&#30340;&#20107;&#24773;&#65292;&#22240;&#20026;&#23427;&#20204;&#35201;&#20040;&#22312;&#27979;&#35797;&#38454;&#27573;&#38656;&#35201;&#35775;&#38382;&#20219;&#21153;&#26631;&#35782;&#65292;&#35201;&#20040;&#19981;&#26126;&#30830;&#22320;&#23545;&#26469;&#33258;&#26410;&#35265;&#20219;&#21153;&#30340;&#26679;&#26412;&#36827;&#34892;&#24314;&#27169;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Diana&#65306;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#26550;&#26500;&#30340;&#32456;&#36523;QA&#27169;&#22411;&#65292;&#35797;&#22270;&#36890;&#36807;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#19968;&#31995;&#21015;QA&#20219;&#21153;&#12290;&#22312;Diana&#20013;&#20351;&#29992;&#20102;&#22235;&#31181;&#23618;&#27425;&#32452;&#32455;&#30340;&#25552;&#31034;&#26469;&#25429;&#33719;&#19981;&#21516;&#31890;&#24230;&#30340;QA&#30693;&#35782;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#20219;&#21153;&#32423;&#25552;&#31034;&#29992;&#20110;&#25429;&#33719;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#65292;&#20197;&#20445;&#25345;&#39640;LL&#24615;&#33021;&#65292;&#24182;&#20445;&#25345;&#23454;&#20363;&#32423;&#25552;&#31034;&#26469;&#23398;&#20064;&#36328;&#19981;&#21516;&#36755;&#20837;&#26679;&#26412;&#20849;&#20139;&#30340;&#30693;&#35782;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2208.14602v3 Announce Type: replace-cross  Abstract: QA models with lifelong learning (LL) abilities are important for practical QA applications, and architecture-based LL methods are reported to be an effective implementation for these models. However, it is non-trivial to extend previous approaches to QA tasks since they either require access to task identities in the testing phase or do not explicitly model samples from unseen tasks. In this paper, we propose Diana: a dynamic architecture-based lifelong QA model that tries to learn a sequence of QA tasks with a prompt enhanced language model. Four types of hierarchically organized prompts are used in Diana to capture QA knowledge from different granularities. Specifically, we dedicate task-level prompts to capture task-specific knowledge to retain high LL performances and maintain instance-level prompts to learn knowledge shared across different input samples to improve the model's generalization performance. Moreover, we dedi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20174;&#20020;&#24202;&#35760;&#24405;&#20013;&#25552;&#21462;&#30561;&#30496;&#20449;&#24687;&#65292;&#20026;&#30740;&#31350;&#30561;&#30496;&#19982;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#21457;&#30149;&#20851;&#32852;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#21644;&#24037;&#20855;</title><link>https://arxiv.org/abs/2204.09601</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20174;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#24739;&#32773;&#30340;&#20020;&#24202;&#35760;&#24405;&#20013;&#25552;&#21462;&#30561;&#30496;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Extraction of Sleep Information from Clinical Notes of Patients with Alzheimer's Disease Using Natural Language Processing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2204.09601
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20174;&#20020;&#24202;&#35760;&#24405;&#20013;&#25552;&#21462;&#30561;&#30496;&#20449;&#24687;&#65292;&#20026;&#30740;&#31350;&#30561;&#30496;&#19982;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#21457;&#30149;&#20851;&#32852;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#21644;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Alzheimer's Disease&#65288;AD&#65289;&#26159;&#32654;&#22269;&#26368;&#24120;&#35265;&#30340;&#30196;&#21574;&#24418;&#24335;&#65292;&#30561;&#30496;&#26159;&#24433;&#21709;&#32769;&#24180;&#35748;&#30693;&#21151;&#33021;&#26368;&#20851;&#38190;&#30340;&#29983;&#27963;&#26041;&#24335;&#22240;&#32032;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#30561;&#30496;&#19982;AD&#21457;&#30149;&#20043;&#38388;&#20851;&#32852;&#30340;&#30740;&#31350;&#21294;&#20047;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#25163;&#21160;&#27880;&#37322;UPMC&#25910;&#38598;&#30340;7,266&#21517;AD&#24739;&#32773;&#30340;192,000&#20221;&#20020;&#24202;&#35760;&#24405;&#20013;&#30340;&#38543;&#26426;&#25277;&#26679;&#25991;&#26723;&#65292;&#21019;&#24314;&#20102;&#37329;&#26631;&#20934;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#22522;&#20110;&#35268;&#21017;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31639;&#27861;&#12289;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;NLP&#31639;&#27861;&#65292;&#20197;&#33258;&#21160;&#21270;&#25552;&#21462;&#30561;&#30496;&#30456;&#20851;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2204.09601v2 Announce Type: replace-cross  Abstract: Alzheimer's Disease (AD) is the most common form of dementia in the United States. Sleep is one of the lifestyle-related factors that has been shown critical for optimal cognitive function in old age. However, there is a lack of research studying the association between sleep and AD incidence. A major bottleneck for conducting such research is that the traditional way to acquire sleep information is time-consuming, inefficient, non-scalable, and limited to patients' subjective experience. A gold standard dataset is created from manual annotation of 570 randomly sampled clinical note documents from the adSLEEP, a corpus of 192,000 de-identified clinical notes of 7,266 AD patients retrieved from the University of Pittsburgh Medical Center (UPMC). We developed a rule-based Natural Language Processing (NLP) algorithm, machine learning models, and Large Language Model(LLM)-based NLP algorithms to automate the extraction of sleep-rel
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#35821;&#35328;&#19968;&#33268;&#30340;&#20799;&#31461;&#35821;&#35328;&#35821;&#20041;&#21644;&#21477;&#27861;&#27880;&#37322;&#26041;&#27861;&#65292;&#21033;&#29992;&#36890;&#29992;&#20381;&#23384;&#20851;&#31995;&#26041;&#26696;&#21644;&#33258;&#21160;&#26041;&#27861;&#36716;&#25442;&#21477;&#23376;&#36923;&#36753;&#24418;&#24335;&#65292;&#20026;&#26500;&#24314;CDS&#35821;&#26009;&#24211;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;</title><link>https://arxiv.org/abs/2109.10952</link><description>&lt;p&gt;
&#36328;&#35821;&#35328;&#19968;&#33268;&#30340;&#20799;&#31461;&#35821;&#35328;&#35821;&#20041;&#21644;&#21477;&#27861;&#27880;&#37322;
&lt;/p&gt;
&lt;p&gt;
Cross-linguistically Consistent Semantic and Syntactic Annotation of Child-directed Speech
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2109.10952
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#35821;&#35328;&#19968;&#33268;&#30340;&#20799;&#31461;&#35821;&#35328;&#35821;&#20041;&#21644;&#21477;&#27861;&#27880;&#37322;&#26041;&#27861;&#65292;&#21033;&#29992;&#36890;&#29992;&#20381;&#23384;&#20851;&#31995;&#26041;&#26696;&#21644;&#33258;&#21160;&#26041;&#27861;&#36716;&#25442;&#21477;&#23376;&#36923;&#36753;&#24418;&#24335;&#65292;&#20026;&#26500;&#24314;CDS&#35821;&#26009;&#24211;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26500;&#24314;&#20799;&#31461;&#35821;&#38899;&#65288;CDS&#65289;&#35821;&#26009;&#24211;&#20197;&#21450;&#21477;&#23376;&#36923;&#36753;&#24418;&#24335;&#30340;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;&#35813;&#26041;&#27861;&#22312;&#33521;&#35821;&#21644;&#24076;&#20271;&#26469;&#35821;&#20013;&#21019;&#24314;&#20102;&#20004;&#20010;&#36825;&#26679;&#30340;&#35821;&#26009;&#24211;&#12290;&#35813;&#26041;&#27861;&#24378;&#35843;&#36328;&#35821;&#35328;&#19968;&#33268;&#30340;&#34920;&#31034;&#65292;&#20511;&#37492;&#20102;&#20381;&#36182;&#20851;&#31995;&#34920;&#31034;&#21644;&#35821;&#20041;&#35299;&#26512;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#36890;&#29992;&#20381;&#23384;&#20851;&#31995;&#65288;UD&#65289;&#26041;&#26696;&#23545;&#35821;&#26009;&#24211;&#36827;&#34892;&#21477;&#27861;&#27880;&#37322;&#65292;&#35813;&#26041;&#26696;&#24050;&#32463;&#34987;&#24320;&#21457;&#20986;&#26469;&#20197;&#20415;&#19968;&#33268;&#22320;&#36866;&#29992;&#20110;&#21508;&#31181;&#39046;&#22495;&#21644;&#31867;&#22411;&#22810;&#26679;&#30340;&#35821;&#35328;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#36890;&#36807;&#24212;&#29992;&#19968;&#31181;&#33258;&#21160;&#26041;&#27861;&#20174;UD&#32467;&#26500;&#36716;&#25442;&#21477;&#23376;&#36923;&#36753;&#24418;&#24335;&#65288;LFs&#65289;&#26469;&#36827;&#19968;&#27493;&#27880;&#37322;&#36825;&#20123;&#25968;&#25454;&#12290;UD&#21644;LF&#34920;&#31034;&#20855;&#26377;&#20114;&#34917;&#30340;&#20248;&#21183;&#65306;UD&#32467;&#26500;&#26159;&#35821;&#35328;&#20013;&#31435;&#30340;&#65292;&#25903;&#25345;&#22810;&#20010;&#27880;&#37322;&#32773;&#36827;&#34892;&#19968;&#33268;&#21644;&#21487;&#38752;&#30340;&#27880;&#37322;&#65292;&#32780;LFs&#26159;&#20013;&#31435;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2109.10952v2 Announce Type: replace  Abstract: This paper proposes a methodology for constructing such corpora of child directed speech (CDS) paired with sentential logical forms, and uses this method to create two such corpora, in English and Hebrew. The approach enforces a cross-linguistically consistent representation, building on recent advances in dependency representation and semantic parsing. Specifically, the approach involves two steps. First, we annotate the corpora using the Universal Dependencies (UD) scheme for syntactic annotation, which has been developed to apply consistently to a wide variety of domains and typologically diverse languages. Next, we further annotate these data by applying an automatic method for transducing sentential logical forms (LFs) from UD structures. The UD and LF representations have complementary strengths: UD structures are language-neutral and support consistent and reliable annotation by multiple annotators, whereas LFs are neutral as 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#30340;&#33258;&#21160;&#21270;&#27169;&#22411;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#20851;&#20110;&#20010;&#20307;&#26679;&#26412;&#30456;&#20851;&#20449;&#24687;&#30340;&#20803;&#20998;&#24067;&#32479;&#35745;&#37327;&#65292;&#33021;&#22815;&#26356;&#39640;&#25928;&#21644;&#26377;&#25928;&#22320;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#35299;&#20915;&#20102;AutoEval&#26694;&#26550;&#20013;&#30340;&#36807;&#24230;&#33258;&#20449;&#12289;&#23384;&#20648;&#21644;&#35745;&#31639;&#25104;&#26412;&#39640;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.12689</link><description>&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#30340;&#33258;&#21160;&#21270;&#27169;&#22411;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Energy-based Automated Model Evaluation. (arXiv:2401.12689v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12689
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#30340;&#33258;&#21160;&#21270;&#27169;&#22411;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#20851;&#20110;&#20010;&#20307;&#26679;&#26412;&#30456;&#20851;&#20449;&#24687;&#30340;&#20803;&#20998;&#24067;&#32479;&#35745;&#37327;&#65292;&#33021;&#22815;&#26356;&#39640;&#25928;&#21644;&#26377;&#25928;&#22320;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#35299;&#20915;&#20102;AutoEval&#26694;&#26550;&#20013;&#30340;&#36807;&#24230;&#33258;&#20449;&#12289;&#23384;&#20648;&#21644;&#35745;&#31639;&#25104;&#26412;&#39640;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35780;&#20272;&#21327;&#35758;&#20381;&#36182;&#20110;&#26631;&#35760;&#30340;&#12289;&#20551;&#35774;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#32780;&#36825;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#24448;&#24448;&#24182;&#19981;&#24120;&#35265;&#12290;&#33258;&#21160;&#27169;&#22411;&#35780;&#20272;&#65288;AutoEval&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#20256;&#32479;&#24037;&#20316;&#27969;&#31243;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24418;&#25104;&#19968;&#20010;&#25509;&#36817;&#39044;&#27979;&#24615;&#33021;&#30340;&#27979;&#35797;&#31649;&#32447;&#65292;&#32780;&#26080;&#38656;&#30495;&#23454;&#26631;&#31614;&#30340;&#23384;&#22312;&#12290;&#23613;&#31649;AutoEval&#26694;&#26550;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#19968;&#20123;&#25104;&#21151;&#65292;&#20294;&#20173;&#23384;&#22312;&#36807;&#24230;&#33258;&#20449;&#12289;&#23384;&#20648;&#21644;&#35745;&#31639;&#25104;&#26412;&#39640;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24230;&#37327;&#26041;&#24335;&#8212;&#8212;&#20803;&#20998;&#24067;&#33021;&#37327;&#65288;MDE&#65289;&#65292;&#23427;&#21487;&#20197;&#20351;AutoEval&#26694;&#26550;&#26356;&#21152;&#39640;&#25928;&#21644;&#26377;&#25928;&#12290;MDE&#30340;&#26680;&#24515;&#26159;&#24314;&#31435;&#19968;&#20010;&#20851;&#20110;&#20010;&#20307;&#26679;&#26412;&#30456;&#20851;&#20449;&#24687;&#65288;&#33021;&#37327;&#65289;&#30340;&#20803;&#20998;&#24067;&#32479;&#35745;&#37327;&#65292;&#28982;&#21518;&#36890;&#36807;&#22522;&#20110;&#33021;&#37327;&#30340;&#23398;&#20064;&#25552;&#20379;&#26356;&#24179;&#28369;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;MDE&#19982;&#20998;&#31867;&#25439;&#22833;&#30456;&#36830;&#25509;&#65292;&#36827;&#19968;&#27493;&#25552;&#20379;&#20102;&#29702;&#35770;&#27934;&#35265;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#25454;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The conventional evaluation protocols on machine learning models rely heavily on a labeled, i.i.d-assumed testing dataset, which is not often present in real world applications. The Automated Model Evaluation (AutoEval) shows an alternative to this traditional workflow, by forming a proximal prediction pipeline of the testing performance without the presence of ground-truth labels. Despite its recent successes, the AutoEval frameworks still suffer from an overconfidence issue, substantial storage and computational cost. In that regard, we propose a novel measure -- Meta-Distribution Energy (MDE) -- that allows the AutoEval framework to be both more efficient and effective. The core of the MDE is to establish a meta-distribution statistic, on the information (energy) associated with individual samples, then offer a smoother representation enabled by energy-based learning. We further provide our theoretical insights by connecting the MDE with the classification loss. We provide extensive
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;BOK-VQA&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#22810;&#35821;&#35328;&#30340;&#35270;&#35273;&#38382;&#31572;&#25968;&#25454;&#20197;&#21450;&#19982;&#38382;&#39064;-&#22238;&#31572;&#20869;&#23481;&#30456;&#20851;&#30340;&#30693;&#35782;&#20449;&#24687;&#12290;&#36890;&#36807;&#20197;&#22270;&#23884;&#20837;&#30340;&#24418;&#24335;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#30693;&#35782;&#20449;&#24687;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23558;&#22806;&#37096;&#30693;&#35782;&#27880;&#20837;VQA&#31995;&#32479;&#20013;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#38382;&#31572;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.06443</link><description>&lt;p&gt;
BOK-VQA: &#22522;&#20110;&#22806;&#37096;&#30693;&#35782;&#30340;&#21452;&#35821;&#35270;&#35273;&#38382;&#31572;&#31995;&#32479;&#36890;&#36807;&#22270;&#34920;&#31034;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
BOK-VQA: Bilingual Outside Knowledge-based Visual Question Answering via Graph Representation Pretraining. (arXiv:2401.06443v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;BOK-VQA&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#22810;&#35821;&#35328;&#30340;&#35270;&#35273;&#38382;&#31572;&#25968;&#25454;&#20197;&#21450;&#19982;&#38382;&#39064;-&#22238;&#31572;&#20869;&#23481;&#30456;&#20851;&#30340;&#30693;&#35782;&#20449;&#24687;&#12290;&#36890;&#36807;&#20197;&#22270;&#23884;&#20837;&#30340;&#24418;&#24335;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#30693;&#35782;&#20449;&#24687;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23558;&#22806;&#37096;&#30693;&#35782;&#27880;&#20837;VQA&#31995;&#32479;&#20013;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#38382;&#31572;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#29983;&#25104;&#27169;&#22411;&#30740;&#31350;&#26041;&#21521;&#65292;&#22914;&#26368;&#36817;&#24320;&#21457;&#30340;GPT4&#65292;&#26088;&#22312;&#20026;&#22810;&#27169;&#24577;&#21644;&#22810;&#35821;&#35328;&#36755;&#20837;&#23547;&#25214;&#30456;&#20851;&#30340;&#30693;&#35782;&#20449;&#24687;&#20197;&#25552;&#20379;&#31572;&#26696;&#12290;&#26681;&#25454;&#36825;&#20123;&#30740;&#31350;&#24773;&#20917;&#65292;&#23545;&#22810;&#35821;&#35328;&#35780;&#20272;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#20219;&#21153;&#30340;&#38656;&#27714;&#65292;&#20316;&#20026;&#22810;&#27169;&#24577;&#31995;&#32479;&#30340;&#20195;&#34920;&#20219;&#21153;&#65292;&#36880;&#28176;&#22686;&#21152;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;&#26412;&#30740;&#31350;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#25193;&#23637;&#21040;&#22810;&#35821;&#35328;&#30340;&#21452;&#35821;&#22806;&#37096;&#30693;&#35782;VQA&#65288;BOK-VQA&#65289;&#25968;&#25454;&#38598;&#12290;&#25152;&#25552;&#20986;&#30340;&#25968;&#25454;&#21253;&#25324;17K&#24352;&#22270;&#29255;&#65292;17K&#20010;&#38889;&#35821;&#21644;&#33521;&#35821;&#30340;&#38382;&#39064;-&#22238;&#31572;&#23545;&#20197;&#21450;&#19982;&#38382;&#39064;-&#22238;&#31572;&#20869;&#23481;&#30456;&#20851;&#30340;28K&#20010;&#30693;&#35782;&#20449;&#24687;&#23454;&#20363;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#20197;&#22270;&#23884;&#20837;&#30340;&#24418;&#24335;&#39044;&#35757;&#32451;BOK-VQA&#25968;&#25454;&#30340;&#30693;&#35782;&#20449;&#24687;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23558;&#30693;&#35782;&#20449;&#24687;&#27880;&#20837;VQA&#31995;&#32479;&#20013;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#28145;&#20837;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26500;&#24314;&#35757;&#32451;&#25968;&#25454;&#20013;&#21253;&#21547;&#30340;&#30693;&#35782;&#20449;&#24687;&#30340;&#23454;&#38469;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current research direction in generative models, such as the recently developed GPT4, aims to find relevant knowledge information for multimodal and multilingual inputs to provide answers. Under these research circumstances, the demand for multilingual evaluation of visual question answering (VQA) tasks, a representative task of multimodal systems, has increased. Accordingly, we propose a bilingual outside-knowledge VQA (BOK-VQA) dataset in this study that can be extended to multilingualism. The proposed data include 17K images, 17K question-answer pairs for both Korean and English and 280K instances of knowledge information related to question-answer content. We also present a framework that can effectively inject knowledge information into a VQA system by pretraining the knowledge information of BOK-VQA data in the form of graph embeddings. Finally, through in-depth analysis, we demonstrated the actual effect of the knowledge information contained in the constructed training data
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#27979;&#37327;&#20301;&#32622;&#20559;&#35265;&#65292;&#37325;&#35775;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38646;-shot &#25277;&#35937;&#25688;&#35201;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#27169;&#22411;&#19981;&#20844;&#24179;&#22320;&#20248;&#20808;&#32771;&#34385;&#26576;&#20123;&#37096;&#20998;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#23548;&#33268;&#19981;&#21487;&#21462;&#30340;&#34892;&#20026;&#12290;&#23545;&#22810;&#20010;LLM&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#25277;&#35937;&#25688;&#35201;&#27169;&#22411;&#36827;&#34892;&#30340;&#23454;&#39564;&#25552;&#20379;&#20102;&#20851;&#20110;&#38646;-shot &#24635;&#32467;&#20219;&#21153;&#30340;&#27169;&#22411;&#24615;&#33021;&#21644;&#20301;&#32622;&#20559;&#35265;&#30340;&#26032;&#35265;&#35299;&#21644;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2401.01989</link><description>&lt;p&gt;
&#37325;&#35775;&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#19979;&#30340;&#38646;-shot &#25277;&#35937;&#25688;&#35201;&#65292;&#20174;&#20301;&#32622;&#20559;&#35265;&#30340;&#35282;&#24230;&#20986;&#21457;
&lt;/p&gt;
&lt;p&gt;
Revisiting Zero-Shot Abstractive Summarization in the Era of Large Language Models from the Perspective of Position Bias. (arXiv:2401.01989v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01989
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#27979;&#37327;&#20301;&#32622;&#20559;&#35265;&#65292;&#37325;&#35775;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38646;-shot &#25277;&#35937;&#25688;&#35201;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#27169;&#22411;&#19981;&#20844;&#24179;&#22320;&#20248;&#20808;&#32771;&#34385;&#26576;&#20123;&#37096;&#20998;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#23548;&#33268;&#19981;&#21487;&#21462;&#30340;&#34892;&#20026;&#12290;&#23545;&#22810;&#20010;LLM&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#25277;&#35937;&#25688;&#35201;&#27169;&#22411;&#36827;&#34892;&#30340;&#23454;&#39564;&#25552;&#20379;&#20102;&#20851;&#20110;&#38646;-shot &#24635;&#32467;&#20219;&#21153;&#30340;&#27169;&#22411;&#24615;&#33021;&#21644;&#20301;&#32622;&#20559;&#35265;&#30340;&#26032;&#35265;&#35299;&#21644;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#27979;&#37327;&#20301;&#32622;&#20559;&#35265;&#26469;&#34920;&#24449;&#21644;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#38646;-shot &#25277;&#35937;&#25688;&#35201;&#65292;&#25105;&#20204;&#23558;&#20854;&#35270;&#20026;&#20808;&#21069;&#25991;&#29486;&#20013;&#30740;&#31350;&#36807;&#30340;&#26356;&#20026;&#38480;&#21046;&#24615;&#30340;&#24341;&#23548;&#20559;&#35265;&#29616;&#35937;&#30340;&#19968;&#33324;&#34920;&#36848;&#12290;&#20301;&#32622;&#20559;&#35265;&#25429;&#25417;&#21040;&#27169;&#22411;&#22312;&#36755;&#20837;&#25991;&#26412;&#30340;&#26576;&#20123;&#37096;&#20998;&#19978;&#19981;&#20844;&#24179;&#22320;&#20248;&#20808;&#32771;&#34385;&#20449;&#24687;&#65292;&#23548;&#33268;&#19981;&#21487;&#21462;&#30340;&#34892;&#20026;&#12290;&#36890;&#36807;&#23545;&#22235;&#20010;&#19981;&#21516;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#20010;LLM&#27169;&#22411;&#22914;GPT 3.5-Turbo&#65292;Llama-2&#21644;Dolly-v2&#20013;&#30340;&#20301;&#32622;&#20559;&#35265;&#65292;&#20197;&#21450;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#25277;&#35937;&#25688;&#35201;&#27169;&#22411;&#22914;Pegasus&#21644;BART&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#20026;&#38646;-shot &#24635;&#32467;&#20219;&#21153;&#30340;&#27169;&#22411;&#24615;&#33021;&#21644;&#20301;&#32622;&#20559;&#35265;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#21644;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
We characterize and study zero-shot abstractive summarization in Large Language Models (LLMs) by measuring position bias, which we propose as a general formulation of the more restrictive lead bias phenomenon studied previously in the literature. Position bias captures the tendency of a model unfairly prioritizing information from certain parts of the input text over others, leading to undesirable behavior. Through numerous experiments on four diverse real-world datasets, we study position bias in multiple LLM models such as GPT 3.5-Turbo, Llama-2, and Dolly-v2, as well as state-of-the-art pretrained encoder-decoder abstractive summarization models such as Pegasus and BART. Our findings lead to novel insights and discussion on performance and position bias of models for zero-shot summarization tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#27169;&#22411;&#36866;&#24212;&#26469;&#26816;&#27979;&#21644;&#20943;&#36731;&#35821;&#35328;&#27169;&#22411;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#20559;&#35265;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.18913</link><description>&lt;p&gt;
&#36890;&#36807;&#27169;&#22411;&#36866;&#24212;&#26469;&#21435;&#38500;&#20559;&#35265;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Debiasing Algorithm through Model Adaptation. (arXiv:2310.18913v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#27169;&#22411;&#36866;&#24212;&#26469;&#26816;&#27979;&#21644;&#20943;&#36731;&#35821;&#35328;&#27169;&#22411;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#20559;&#35265;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27491;&#22312;&#25104;&#20026;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#30340;&#39318;&#36873;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#23481;&#37327;&#30340;&#22686;&#38271;&#65292;&#27169;&#22411;&#24456;&#23481;&#26131;&#20381;&#36182;&#35757;&#32451;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#20559;&#35265;&#21644;&#21051;&#26495;&#21360;&#35937;&#25152;&#20135;&#29983;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#21644;&#20943;&#36731;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;&#25105;&#20204;&#36827;&#34892;&#22240;&#26524;&#20998;&#26512;&#65292;&#20197;&#35782;&#21035;&#38382;&#39064;&#27169;&#22411;&#32452;&#20214;&#65292;&#24182;&#21457;&#29616;&#20013;&#19978;&#23618;&#21069;&#39304;&#23618;&#26368;&#23481;&#26131;&#20256;&#36882;&#20559;&#35265;&#12290;&#26681;&#25454;&#20998;&#26512;&#32467;&#26524;&#65292;&#25105;&#20204;&#36890;&#36807;&#32447;&#24615;&#25237;&#24433;&#23558;&#36825;&#20123;&#23618;&#20056;&#20197;&#27169;&#22411;&#36827;&#34892;&#36866;&#24212;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;DAMA&#36890;&#36807;&#21508;&#31181;&#24230;&#37327;&#25351;&#26631;&#26126;&#26174;&#20943;&#23569;&#20102;&#20559;&#35265;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#22312;&#21518;&#32493;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21644;&#27169;&#22411;&#30340;&#20195;&#30721;&#65292;&#36890;&#36807;&#37325;&#26032;&#35757;&#32451;&#65292;&#20445;&#25345;&#20102;LLaMA&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#21516;&#26102;&#20559;&#35265;&#26174;&#33879;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models are becoming the go-to solution for various language tasks. However, with growing capacity, models are prone to rely on spurious correlations stemming from biases and stereotypes present in the training data. This work proposes a novel method for detecting and mitigating gender bias in language models. We perform causal analysis to identify problematic model components and discover that mid-upper feed-forward layers are most prone to convey biases. Based on the analysis results, we adapt the model by multiplying these layers by a linear projection. Our titular method, DAMA, significantly decreases bias as measured by diverse metrics while maintaining the model's performance on downstream tasks. We release code for our method and models, which retrain LLaMA's state-of-the-art performance while being significantly less biased.
&lt;/p&gt;</description></item><item><title>&#29992;&#20110;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#30340;&#32452;&#21512;&#20559;&#22909;&#27169;&#22411;&#65288;CPMs&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20559;&#22909;&#27169;&#22411;&#26694;&#26550;&#65292;&#21487;&#20197;&#20998;&#35299;&#20840;&#23616;&#20559;&#22909;&#35780;&#20272;&#24182;&#26681;&#25454;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#36827;&#34892;&#26631;&#37327;&#35780;&#20998;&#65292;&#24471;&#21040;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.13011</link><description>&lt;p&gt;
&#29992;&#20110;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#30340;&#32452;&#21512;&#20559;&#22909;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Compositional preference models for aligning LMs. (arXiv:2310.13011v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13011
&lt;/p&gt;
&lt;p&gt;
&#29992;&#20110;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#30340;&#32452;&#21512;&#20559;&#22909;&#27169;&#22411;&#65288;CPMs&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20559;&#22909;&#27169;&#22411;&#26694;&#26550;&#65292;&#21487;&#20197;&#20998;&#35299;&#20840;&#23616;&#20559;&#22909;&#35780;&#20272;&#24182;&#26681;&#25454;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#36827;&#34892;&#26631;&#37327;&#35780;&#20998;&#65292;&#24471;&#21040;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#36234;&#26469;&#36234;&#24378;&#65292;&#23558;&#20854;&#19982;&#20154;&#31867;&#20559;&#22909;&#36827;&#34892;&#23545;&#40784;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#35757;&#32451;&#20559;&#22909;&#27169;&#22411;&#30340;&#20027;&#27969;&#33539;&#24335;&#23384;&#22312;&#26681;&#26412;&#24615;&#30340;&#38480;&#21046;&#65292;&#20363;&#22914;&#32570;&#20047;&#36879;&#26126;&#24230;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#20197;&#21450;&#23545;&#20559;&#22909;&#25968;&#25454;&#38598;&#36807;&#25311;&#21512;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#32452;&#21512;&#20559;&#22909;&#27169;&#22411;&#65288;CPMs&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#20559;&#22909;&#27169;&#22411;&#26694;&#26550;&#65292;&#23558;&#19968;&#20010;&#20840;&#23616;&#20559;&#22909;&#35780;&#20272;&#20998;&#35299;&#20026;&#22810;&#20010;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#65292;&#20174;&#19968;&#20010;&#25552;&#31034;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#33719;&#21462;&#36825;&#20123;&#29305;&#24449;&#30340;&#26631;&#37327;&#35780;&#20998;&#65292;&#24182;&#20351;&#29992;&#36923;&#36753;&#22238;&#24402;&#20998;&#31867;&#22120;&#32858;&#21512;&#36825;&#20123;&#35780;&#20998;&#12290;CPMs&#20801;&#35768;&#25511;&#21046;&#20174;&#20559;&#22909;&#25968;&#25454;&#20013;&#20351;&#29992;&#21738;&#20123;&#23646;&#24615;&#26469;&#35757;&#32451;&#20559;&#22909;&#27169;&#22411;&#65292;&#24182;&#22522;&#20110;&#34987;&#35748;&#20026;&#26159;&#20154;&#31867;&#20559;&#22909;&#21028;&#26029;&#22522;&#30784;&#30340;&#29305;&#24449;&#26500;&#24314;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;CPMs&#19981;&#20165;&#25913;&#21892;&#20102;&#27867;&#21270;&#33021;&#21147;&#65292;&#27604;&#26631;&#20934;&#20559;&#22909;&#27169;&#22411;&#26356;&#20855;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#20351;&#29992;CPMs&#33719;&#24471;&#30340;&#26368;&#20339;n&#20010;&#26679;&#26412;&#27604;&#20351;&#29992;&#26631;&#20934;PMs&#30340;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
As language models (LMs) become more capable, it is increasingly important to align them with human preferences. However, the dominant paradigm for training Preference Models (PMs) for that purpose suffers from fundamental limitations, such as lack of transparency and scalability, along with susceptibility to overfitting the preference dataset. We propose Compositional Preference Models (CPMs), a novel PM framework that decomposes one global preference assessment into several interpretable features, obtains scalar scores for these features from a prompted LM, and aggregates these scores using a logistic regression classifier. CPMs allow to control which properties of the preference data are used to train the preference model and to build it based on features that are believed to underlie the human preference judgment. Our experiments show that CPMs not only improve generalization and are more robust to overoptimization than standard PMs, but also that best-of-n samples obtained using C
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Kosmos-G&#65292;&#19968;&#31181;&#21033;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#22312;&#19978;&#19979;&#25991;&#20013;&#29983;&#25104;&#22270;&#20687;&#30340;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#27169;&#24577;&#20316;&#20026;&#38170;&#28857;&#65292;&#23558;MLLM&#30340;&#36755;&#20986;&#31354;&#38388;&#19982;CLIP&#23545;&#40784;&#65292;&#24182;&#36827;&#34892;&#32452;&#21512;&#25351;&#20196;&#35843;&#25972;&#12290;Kosmos-G&#23637;&#31034;&#20102;&#38646;&#26679;&#26412;&#22810;&#23454;&#20307;&#20027;&#39064;&#39537;&#21160;&#29983;&#25104;&#30340;&#29420;&#29305;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.02992</link><description>&lt;p&gt;
Kosmos-G&#65306;&#20351;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#20013;&#29983;&#25104;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
Kosmos-G: Generating Images in Context with Multimodal Large Language Models. (arXiv:2310.02992v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Kosmos-G&#65292;&#19968;&#31181;&#21033;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#22312;&#19978;&#19979;&#25991;&#20013;&#29983;&#25104;&#22270;&#20687;&#30340;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#27169;&#24577;&#20316;&#20026;&#38170;&#28857;&#65292;&#23558;MLLM&#30340;&#36755;&#20986;&#31354;&#38388;&#19982;CLIP&#23545;&#40784;&#65292;&#24182;&#36827;&#34892;&#32452;&#21512;&#25351;&#20196;&#35843;&#25972;&#12290;Kosmos-G&#23637;&#31034;&#20102;&#38646;&#26679;&#26412;&#22810;&#23454;&#20307;&#20027;&#39064;&#39537;&#21160;&#29983;&#25104;&#30340;&#29420;&#29305;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#21644;&#35270;&#35273;&#35821;&#35328;&#21040;&#22270;&#20687;&#65288;VL2I&#65289;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#20174;&#36890;&#29992;&#30340;&#35270;&#35273;&#35821;&#35328;&#36755;&#20837;&#29983;&#25104;&#22270;&#20687;&#65292;&#29305;&#21035;&#26159;&#28041;&#21450;&#22810;&#20010;&#22270;&#20687;&#30340;&#24773;&#20917;&#65292;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Kosmos-G&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#20808;&#36827;&#24863;&#30693;&#33021;&#21147;&#26469;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#27169;&#24577;&#20316;&#20026;&#38170;&#28857;&#65292;&#23558;MLLM&#30340;&#36755;&#20986;&#31354;&#38388;&#19982;CLIP&#23545;&#40784;&#65292;&#24182;&#22312;&#31574;&#21010;&#25968;&#25454;&#19978;&#36827;&#34892;&#32452;&#21512;&#25351;&#20196;&#35843;&#25972;&#12290;Kosmos-G&#23637;&#31034;&#20102;&#38646;&#26679;&#26412;&#22810;&#23454;&#20307;&#20027;&#39064;&#39537;&#21160;&#29983;&#25104;&#30340;&#29420;&#29305;&#33021;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20998;&#25968;&#33976;&#39311;&#25351;&#20196;&#35843;&#25972;&#23545;&#22270;&#20687;&#35299;&#30721;&#22120;&#19981;&#38656;&#35201;&#36827;&#34892;&#20219;&#20309;&#20462;&#25913;&#12290;&#36825;&#20801;&#35768;&#26080;&#32541;&#26367;&#20195;CLIP&#24182;&#36731;&#26494;&#38598;&#25104;&#21508;&#31181;U-Net&#25216;&#26415;&#65292;&#21253;&#25324;&#32454;&#31890;&#24230;&#25511;&#21046;&#21644;&#20010;&#24615;&#21270;&#22270;&#20687;&#35299;&#30721;&#22120;&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in text-to-image (T2I) and vision-language-to-image (VL2I) generation have made significant strides. However, the generation from generalized vision-language inputs, especially involving multiple images, remains under-explored. This paper presents Kosmos-G, a model that leverages the advanced perception capabilities of Multimodal Large Language Models (MLLMs) to tackle the aforementioned challenge. Our approach aligns the output space of MLLM with CLIP using the textual modality as an anchor and performs compositional instruction tuning on curated data. Kosmos-G demonstrates a unique capability of zero-shot multi-entity subject-driven generation. Notably, the score distillation instruction tuning requires no modifications to the image decoder. This allows for a seamless substitution of CLIP and effortless integration with a myriad of U-Net techniques ranging from fine-grained controls to personalized image decoder variants. We posit Kosmos-G as an initial attempt to
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#22914;&#20309;&#35753;&#35821;&#35328;&#27169;&#22411;&#38544;&#24335;&#23398;&#20064;&#33258;&#25105;&#25913;&#36827;&#65292;&#24182;&#20943;&#23569;&#23545;&#20154;&#31867;&#26631;&#27880;&#30340;&#20381;&#36182;&#12290;</title><link>http://arxiv.org/abs/2310.00898</link><description>&lt;p&gt;
&#35753;&#35821;&#35328;&#27169;&#22411;&#20174;&#25968;&#25454;&#20013;&#38544;&#24335;&#23398;&#20064;&#33258;&#25105;&#25913;&#36827;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Enable Language Models to Implicitly Learn Self-Improvement From Data. (arXiv:2310.00898v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00898
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#22914;&#20309;&#35753;&#35821;&#35328;&#27169;&#22411;&#38544;&#24335;&#23398;&#20064;&#33258;&#25105;&#25913;&#36827;&#65292;&#24182;&#20943;&#23569;&#23545;&#20154;&#31867;&#26631;&#27880;&#30340;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24320;&#25918;&#24335;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20219;&#21153;&#30340;&#26412;&#36136;&#20915;&#23450;&#20102;&#27169;&#22411;&#30340;&#22238;&#31572;&#36136;&#37327;&#22987;&#32456;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#28857;&#38598;&#20013;&#22312;&#20351;&#35821;&#35328;&#27169;&#22411;&#33258;&#25105;&#25913;&#36827;&#20854;&#22238;&#31572;&#36136;&#37327;&#19978;&#65292;&#20174;&#32780;&#20943;&#23569;&#23545;&#24191;&#27867;&#30340;&#20154;&#24037;&#26631;&#27880;&#24037;&#20316;&#26469;&#25910;&#38598;&#22810;&#26679;&#21270;&#21644;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#20381;&#36182;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#22240;&#20854;&#26377;&#25928;&#24615;&#12289;&#39640;&#25928;&#24615;&#21644;&#20415;&#21033;&#24615;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#20026;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#26126;&#30830;&#21644;&#35814;&#23613;&#30340;&#25351;&#31034;&#12290;&#23545;&#20110;&#25163;&#21160;&#25512;&#23548;&#21644;&#25552;&#20379;&#25152;&#26377;&#24517;&#35201;&#30340;&#25351;&#31034;&#26469;&#23454;&#29616;&#29616;&#23454;&#19990;&#30028;&#22797;&#26434;&#30446;&#26631;&#30340;&#25913;&#36827;&#65288;&#20363;&#22914;&#65292;&#26356;&#26377;&#24110;&#21161;&#24615;&#21644;&#26356;&#23569;&#26377;&#23475;&#24615;&#65289;&#65292;&#36825;&#26159;&#26114;&#36149;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable capabilities in open-ended text generation tasks. However, the inherent open-ended nature of these tasks implies that there is always room for improvement in the quality of model responses. To address this challenge, various approaches have been proposed to enhance the performance of LLMs. There has been a growing focus on enabling LLMs to self-improve their response quality, thereby reducing the reliance on extensive human annotation efforts for collecting diverse and high-quality training data. Recently, prompting-based methods have been widely explored among self-improvement methods owing to their effectiveness, efficiency, and convenience. However, those methods usually require explicitly and thoroughly written rubrics as inputs to LLMs. It is expensive and challenging to manually derive and provide all necessary rubrics with a real-world complex goal for improvement (e.g., being more helpful and less harmful). To this end, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32852;&#21512;MLP/&#27880;&#24847;&#21147;&#65288;JoMA&#65289;&#21160;&#24577;&#65292;&#29992;&#20110;&#35299;&#26512;&#22810;&#23618;Transformer&#26550;&#26500;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#36890;&#36807;&#39044;&#27979;&#38750;&#32447;&#24615;&#28608;&#27963;&#24773;&#20917;&#19979;&#27880;&#24847;&#21147;&#30340;&#34892;&#20026;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;&#22810;&#23618;Transformer&#20013;&#26631;&#35760;&#30340;&#23618;&#27425;&#32452;&#21512;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.00535</link><description>&lt;p&gt;
JoMA: &#36890;&#36807;MLP&#21644;&#27880;&#24847;&#21147;&#30340;&#32852;&#21512;&#21160;&#21147;&#23398;&#26469;&#35299;&#23494;&#22810;&#23618;Transformer
&lt;/p&gt;
&lt;p&gt;
JoMA: Demystifying Multilayer Transformers via JOint Dynamics of MLP and Attention. (arXiv:2310.00535v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32852;&#21512;MLP/&#27880;&#24847;&#21147;&#65288;JoMA&#65289;&#21160;&#24577;&#65292;&#29992;&#20110;&#35299;&#26512;&#22810;&#23618;Transformer&#26550;&#26500;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#36890;&#36807;&#39044;&#27979;&#38750;&#32447;&#24615;&#28608;&#27963;&#24773;&#20917;&#19979;&#27880;&#24847;&#21147;&#30340;&#34892;&#20026;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;&#22810;&#23618;Transformer&#20013;&#26631;&#35760;&#30340;&#23618;&#27425;&#32452;&#21512;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#21512;MLP/&#27880;&#24847;&#21147;&#65288;JoMA&#65289;&#21160;&#24577;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#23398;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;&#22810;&#23618;Transformer&#26550;&#26500;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#36890;&#36807;&#22312;Transformer&#20013;&#21435;&#38500;&#33258;&#27880;&#24847;&#21147;&#23618;&#65292;&#25105;&#20204;&#24471;&#21040;&#20165;&#21253;&#21547;MLP&#23618;&#30340;&#20462;&#25913;&#21518;&#21160;&#24577;&#12290;JoMA&#28040;&#38500;&#20102;&#20808;&#21069;&#20998;&#26512;&#20013;&#30340;&#19981;&#20999;&#23454;&#38469;&#30340;&#20551;&#35774;&#65288;&#20363;&#22914;&#32570;&#20047;&#27531;&#24046;&#36830;&#25509;&#65289;&#65292;&#24182;&#39044;&#27979;&#27880;&#24847;&#21147;&#22312;&#38750;&#32447;&#24615;&#28608;&#27963;&#30340;&#24773;&#20917;&#19979;&#39318;&#20808;&#21464;&#24471;&#31232;&#30095;&#65288;&#20026;&#20102;&#23398;&#20064;&#37325;&#35201;&#30340;&#26631;&#35760;&#65289;&#65292;&#28982;&#21518;&#21464;&#24471;&#23494;&#38598;&#65288;&#20026;&#20102;&#23398;&#20064;&#19981;&#37027;&#20040;&#37325;&#35201;&#30340;&#26631;&#35760;&#65289;&#65292;&#32780;&#22312;&#32447;&#24615;&#24773;&#20917;&#19979;&#65292;&#23427;&#19982;&#29616;&#26377;&#30740;&#31350;&#19968;&#33268;&#65292;&#26174;&#31034;&#20986;&#27880;&#24847;&#21147;&#38543;&#26102;&#38388;&#21464;&#24471;&#31232;&#30095;&#12290;&#25105;&#20204;&#21033;&#29992;JoMA&#23450;&#24615;&#22320;&#35299;&#37322;&#20102;&#22810;&#23618;Transformer&#20013;&#22914;&#20309;&#23558;&#26631;&#35760;&#32452;&#21512;&#25104;&#23618;&#27425;&#32467;&#26500;&#65292;&#24403;&#36755;&#20837;&#26631;&#35760;&#26159;&#30001;&#28508;&#22312;&#30340;&#23618;&#27425;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#26102;&#12290;&#22312;&#20174;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#65288;Wikitext2/Wikitext103&#65289;&#35757;&#32451;&#30340;&#27169;&#22411;&#21644;&#21508;&#31181;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;OPT&#65292;Pythia&#65289;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Joint MLP/Attention (JoMA) dynamics, a novel mathematical framework to understand the training procedure of multilayer Transformer architectures. This is achieved by integrating out the self-attention layer in Transformers, producing a modified dynamics of MLP layers only. JoMA removes unrealistic assumptions in previous analysis (e.g., lack of residual connection) and predicts that the attention first becomes sparse (to learn salient tokens), then dense (to learn less salient tokens) in the presence of nonlinear activations, while in the linear case, it is consistent with existing works that show attention becomes sparse over time. We leverage JoMA to qualitatively explains how tokens are combined to form hierarchies in multilayer Transformers, when the input tokens are generated by a latent hierarchical generative model. Experiments on models trained from real-world dataset (Wikitext2/Wikitext103) and various pre-trained models (OPT, Pythia) verify our theoretical findings
&lt;/p&gt;</description></item><item><title>XATU&#26159;&#31532;&#19968;&#20010;&#32454;&#31890;&#24230;&#22522;&#20110;&#25351;&#20196;&#30340;&#21487;&#35299;&#37322;&#24615;&#25991;&#26412;&#32534;&#36753;&#22522;&#20934;&#27979;&#35797;&#65292;&#28085;&#30422;&#24191;&#27867;&#30340;&#32534;&#36753;&#31867;&#22411;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#32454;&#31890;&#24230;&#25351;&#20196;&#21644;&#40644;&#37329;&#26631;&#20934;&#32534;&#36753;&#35828;&#26126;&#26469;&#25552;&#39640;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.11063</link><description>&lt;p&gt;
XATU: &#38754;&#21521;&#21487;&#35299;&#37322;&#24615;&#25991;&#26412;&#26356;&#26032;&#30340;&#32454;&#31890;&#24230;&#22522;&#20110;&#25351;&#20196;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
XATU: A Fine-grained Instruction-based Benchmark for Explainable Text Updates. (arXiv:2309.11063v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11063
&lt;/p&gt;
&lt;p&gt;
XATU&#26159;&#31532;&#19968;&#20010;&#32454;&#31890;&#24230;&#22522;&#20110;&#25351;&#20196;&#30340;&#21487;&#35299;&#37322;&#24615;&#25991;&#26412;&#32534;&#36753;&#22522;&#20934;&#27979;&#35797;&#65292;&#28085;&#30422;&#24191;&#27867;&#30340;&#32534;&#36753;&#31867;&#22411;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#32454;&#31890;&#24230;&#25351;&#20196;&#21644;&#40644;&#37329;&#26631;&#20934;&#32534;&#36753;&#35828;&#26126;&#26469;&#25552;&#39640;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#32534;&#36753;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#20219;&#21153;&#65292;&#28041;&#21450;&#20462;&#25913;&#25991;&#26412;&#20197;&#26356;&#22909;&#22320;&#19982;&#29992;&#25143;&#24847;&#22270;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25991;&#26412;&#32534;&#36753;&#22522;&#20934;&#25968;&#25454;&#38598;&#22312;&#25552;&#20379;&#31895;&#31890;&#24230;&#25351;&#20196;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#22240;&#27492;&#65292;&#23613;&#31649;&#32534;&#36753;&#21518;&#30340;&#36755;&#20986;&#20284;&#20046;&#21512;&#29702;&#65292;&#20294;&#24448;&#24448;&#20559;&#31163;&#20102;&#40644;&#37329;&#21442;&#32771;&#20013;&#21015;&#20986;&#30340;&#39044;&#26399;&#26356;&#25913;&#65292;&#23548;&#33268;&#35780;&#20272;&#20998;&#25968;&#36739;&#20302;&#12290;&#20026;&#20102;&#20840;&#38754;&#35843;&#26597;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#32534;&#36753;&#33021;&#21147;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;XATU&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#20026;&#32454;&#31890;&#24230;&#22522;&#20110;&#25351;&#20196;&#30340;&#21487;&#35299;&#37322;&#24615;&#25991;&#26412;&#32534;&#36753;&#32780;&#35774;&#35745;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;XATU&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#20027;&#39064;&#21644;&#25991;&#26412;&#31867;&#22411;&#65292;&#21253;&#25324;&#35789;&#27719;&#12289;&#21477;&#27861;&#12289;&#35821;&#20041;&#21644;&#30693;&#35782;&#23494;&#38598;&#22411;&#30340;&#32534;&#36753;&#12290;&#20026;&#20102;&#22686;&#24378;&#21487;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#21033;&#29992;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#28304;&#21644;&#20154;&#24037;&#27880;&#37322;&#65292;&#29983;&#25104;&#20102;&#19968;&#20010;&#21253;&#21547;&#32454;&#31890;&#24230;&#25351;&#20196;&#21644;&#40644;&#37329;&#26631;&#20934;&#32534;&#36753;&#35828;&#26126;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#36890;&#36807;&#35780;&#20272;&#29616;&#26377;&#30340;&#24320;&#25918;&#21644;&#23553;&#38381;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#36827;&#34892;&#23545;&#27604;
&lt;/p&gt;
&lt;p&gt;
Text editing is a crucial task that involves modifying text to better align with user intents. However, existing text editing benchmark datasets have limitations in providing only coarse-grained instructions. Consequently, although the edited output may seem reasonable, it often deviates from the intended changes outlined in the gold reference, resulting in low evaluation scores. To comprehensively investigate the text editing capabilities of large language models, this paper introduces XATU, the first benchmark specifically designed for fine-grained instruction-based explainable text editing. XATU covers a wide range of topics and text types, incorporating lexical, syntactic, semantic, and knowledge-intensive edits. To enhance interpretability, we leverage high-quality data sources and human annotation, resulting in a benchmark that includes fine-grained instructions and gold-standard edit explanations. By evaluating existing open and closed large language models against our benchmark
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;EmoDistill&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#26469;&#23398;&#20064;&#20174;&#35821;&#38899;&#20013;&#33719;&#21462;&#24773;&#24863;&#30340;&#24378;&#22823;&#30340;&#35821;&#35328;&#21644;&#35821;&#38899;&#34920;&#31034;&#30340;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#26694;&#26550;&#12290;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21033;&#29992;&#32463;&#36807;SER&#24494;&#35843;&#30340;&#39044;&#35757;&#32451;&#35821;&#38899;&#21644;&#35821;&#35328;&#25945;&#24072;&#36827;&#34892;&#20449;&#24687;&#33976;&#39311;&#65292;&#35813;&#26041;&#27861;&#22312;IEMOCAP&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#26368;&#39640;&#20934;&#30830;&#29575;&#65292;&#34920;&#26126;&#20854;&#22312;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#25216;&#26415;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.04849</link><description>&lt;p&gt;
&#36890;&#36807;&#25552;&#21462;&#31934;&#28860;&#30340;&#35821;&#38899;&#21644;&#35821;&#35328;&#24773;&#24863;&#34920;&#31034;&#36827;&#34892;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Speech Emotion Recognition with Distilled Prosodic and Linguistic Affect Representations. (arXiv:2309.04849v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04849
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;EmoDistill&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#26469;&#23398;&#20064;&#20174;&#35821;&#38899;&#20013;&#33719;&#21462;&#24773;&#24863;&#30340;&#24378;&#22823;&#30340;&#35821;&#35328;&#21644;&#35821;&#38899;&#34920;&#31034;&#30340;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#26694;&#26550;&#12290;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21033;&#29992;&#32463;&#36807;SER&#24494;&#35843;&#30340;&#39044;&#35757;&#32451;&#35821;&#38899;&#21644;&#35821;&#35328;&#25945;&#24072;&#36827;&#34892;&#20449;&#24687;&#33976;&#39311;&#65292;&#35813;&#26041;&#27861;&#22312;IEMOCAP&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#26368;&#39640;&#20934;&#30830;&#29575;&#65292;&#34920;&#26126;&#20854;&#22312;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#25216;&#26415;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;EmoDistill&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65288;SER&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#36328;&#27169;&#24577;&#30693;&#35782;&#33976;&#39311;&#26469;&#23398;&#20064;&#20174;&#35821;&#38899;&#20013;&#33719;&#21462;&#24773;&#24863;&#30340;&#24378;&#22823;&#30340;&#35821;&#35328;&#21644;&#35821;&#38899;&#34920;&#31034;&#12290;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#20351;&#29992;&#19968;&#20018;&#35821;&#38899;&#20449;&#21495;&#26469;&#36827;&#34892;&#21333;&#27169;&#24577;SER&#65292;&#20174;&#32780;&#20943;&#23569;&#35745;&#31639;&#24320;&#38144;&#24182;&#36991;&#20813;&#36816;&#34892;&#26102;&#30340;&#36716;&#24405;&#21644;&#35821;&#38899;&#29305;&#24449;&#25552;&#21462;&#38169;&#35823;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20174;&#19968;&#23545;&#32463;&#36807;SER&#24494;&#35843;&#30340;&#39044;&#35757;&#32451;&#30340;&#35821;&#38899;&#21644;&#35821;&#35328;&#25945;&#24072;&#20013;&#30340;&#23884;&#20837;&#21644;&#36923;&#36753;&#23618;&#38754;&#33976;&#39311;&#20449;&#24687;&#12290;&#22312;IEMOCAP&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#29575;&#19978;&#20248;&#20110;&#20854;&#20182;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#25216;&#26415;&#65292;&#24182;&#36798;&#21040;&#20102;77.49&#65285;&#30340;&#26080;&#26435;&#37325;&#20934;&#30830;&#29575;&#21644;78.91&#65285;&#30340;&#21152;&#26435;&#20934;&#30830;&#29575;&#30340;&#26368;&#26032;&#25104;&#32489;&#12290;&#35814;&#32454;&#30340;&#28040;&#34701;&#30740;&#31350;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#27599;&#20010;&#32452;&#20214;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose EmoDistill, a novel speech emotion recognition (SER) framework that leverages cross-modal knowledge distillation during training to learn strong linguistic and prosodic representations of emotion from speech. During inference, our method only uses a stream of speech signals to perform unimodal SER thus reducing computation overhead and avoiding run-time transcription and prosodic feature extraction errors. During training, our method distills information at both embedding and logit levels from a pair of pre-trained Prosodic and Linguistic teachers that are fine-tuned for SER. Experiments on the IEMOCAP benchmark demonstrate that our method outperforms other unimodal and multimodal techniques by a considerable margin, and achieves state-of-the-art performance of 77.49% unweighted accuracy and 78.91% weighted accuracy. Detailed ablation studies demonstrate the impact of each component of our method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CoALA&#30340;&#35748;&#30693;&#26550;&#26500;&#65292;&#29992;&#20110;&#32452;&#32455;&#35821;&#35328;&#20195;&#29702;&#30340;&#29616;&#26377;&#30740;&#31350;&#24182;&#35268;&#21010;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;CoALA&#25551;&#36848;&#20102;&#19968;&#20010;&#20855;&#26377;&#27169;&#22359;&#21270;&#35760;&#24518;&#32452;&#20214;&#12289;&#32467;&#26500;&#21270;&#34892;&#21160;&#31354;&#38388;&#21644;&#36890;&#29992;&#20915;&#31574;&#36807;&#31243;&#30340;&#35821;&#35328;&#20195;&#29702;&#12290;&#36890;&#36807;&#36825;&#19968;&#26694;&#26550;&#65292;&#26377;&#26395;&#21457;&#23637;&#20986;&#26356;&#24378;&#22823;&#30340;&#35821;&#35328;&#20195;&#29702;&#12290;</title><link>http://arxiv.org/abs/2309.02427</link><description>&lt;p&gt;
&#35821;&#35328;&#20195;&#29702;&#30340;&#35748;&#30693;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Cognitive Architectures for Language Agents. (arXiv:2309.02427v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02427
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CoALA&#30340;&#35748;&#30693;&#26550;&#26500;&#65292;&#29992;&#20110;&#32452;&#32455;&#35821;&#35328;&#20195;&#29702;&#30340;&#29616;&#26377;&#30740;&#31350;&#24182;&#35268;&#21010;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;CoALA&#25551;&#36848;&#20102;&#19968;&#20010;&#20855;&#26377;&#27169;&#22359;&#21270;&#35760;&#24518;&#32452;&#20214;&#12289;&#32467;&#26500;&#21270;&#34892;&#21160;&#31354;&#38388;&#21644;&#36890;&#29992;&#20915;&#31574;&#36807;&#31243;&#30340;&#35821;&#35328;&#20195;&#29702;&#12290;&#36890;&#36807;&#36825;&#19968;&#26694;&#26550;&#65292;&#26377;&#26395;&#21457;&#23637;&#20986;&#26356;&#24378;&#22823;&#30340;&#35821;&#35328;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#22686;&#21152;&#20102;&#22806;&#37096;&#36164;&#28304;&#65288;&#20363;&#22914;&#20114;&#32852;&#32593;&#65289;&#25110;&#20869;&#37096;&#25511;&#21046;&#27969;&#65288;&#20363;&#22914;&#25552;&#31034;&#38142;&#65289;&#65292;&#29992;&#20110;&#38656;&#35201;&#22522;&#20110;&#35821;&#22659;&#25110;&#25512;&#29702;&#30340;&#20219;&#21153;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#19968;&#31867;&#26032;&#30340;&#35821;&#35328;&#20195;&#29702;&#12290;&#23613;&#31649;&#36825;&#20123;&#20195;&#29702;&#21462;&#24471;&#20102;&#23454;&#35777;&#25104;&#21151;&#65292;&#20294;&#25105;&#20204;&#32570;&#20047;&#19968;&#20010;&#31995;&#32479;&#30340;&#26694;&#26550;&#26469;&#32452;&#32455;&#29616;&#26377;&#20195;&#29702;&#24182;&#35268;&#21010;&#26410;&#26469;&#30340;&#21457;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#35748;&#30693;&#31185;&#23398;&#21644;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#30340;&#20016;&#23500;&#21382;&#21490;&#65292;&#25552;&#20986;&#20102;&#35821;&#35328;&#20195;&#29702;&#30340;&#35748;&#30693;&#26550;&#26500;&#65288;CoALA&#65289;&#12290;CoALA&#25551;&#36848;&#20102;&#19968;&#20010;&#20855;&#26377;&#27169;&#22359;&#21270;&#35760;&#24518;&#32452;&#20214;&#12289;&#29992;&#20110;&#19982;&#20869;&#37096;&#35760;&#24518;&#21644;&#22806;&#37096;&#29615;&#22659;&#20132;&#20114;&#30340;&#32467;&#26500;&#21270;&#34892;&#21160;&#31354;&#38388;&#20197;&#21450;&#36873;&#25321;&#34892;&#21160;&#30340;&#36890;&#29992;&#20915;&#31574;&#36807;&#31243;&#30340;&#35821;&#35328;&#20195;&#29702;&#12290;&#25105;&#20204;&#20351;&#29992;CoALA&#23545;&#26368;&#36817;&#30340;&#22823;&#37327;&#30740;&#31350;&#36827;&#34892;&#20102;&#22238;&#39038;&#21644;&#32452;&#32455;&#65292;&#24182;&#23637;&#26395;&#20102;&#26356;&#24378;&#22823;&#20195;&#29702;&#30340;&#21487;&#34892;&#26041;&#21521;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;CoALA&#23558;&#24403;&#20170;&#30340;&#35821;&#35328;&#20195;&#29702;&#32622;&#20110;&#19978;&#19979;&#25991;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent efforts have augmented large language models (LLMs) with external resources (e.g., the Internet) or internal control flows (e.g., prompt chaining) for tasks requiring grounding or reasoning, leading to a new class of language agents. While these agents have achieved substantial empirical success, we lack a systematic framework to organize existing agents and plan future developments. In this paper, we draw on the rich history of cognitive science and symbolic artificial intelligence to propose Cognitive Architectures for Language Agents (CoALA). CoALA describes a language agent with modular memory components, a structured action space to interact with internal memory and external environments, and a generalized decision-making process to choose actions. We use CoALA to retrospectively survey and organize a large body of recent work, and prospectively identify actionable directions towards more capable agents. Taken together, CoALA contextualizes today's language agents within th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25237;&#31080;&#30340;&#22810;&#27169;&#24577;&#26041;&#27861;&#29992;&#20110;&#33258;&#21160;&#27450;&#39575;&#26816;&#27979;&#65292;&#36890;&#36807;&#35270;&#39057;&#30340;&#38899;&#39057;&#12289;&#35270;&#35273;&#21644;&#25991;&#26412;&#29305;&#24449;&#36827;&#34892;&#26816;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#27450;&#39575;&#26816;&#27979;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2307.07516</link><description>&lt;p&gt;
&#22522;&#20110;&#25237;&#31080;&#30340;&#22810;&#27169;&#24577;&#33258;&#21160;&#27450;&#39575;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Voting-based Multimodal Automatic Deception Detection. (arXiv:2307.07516v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25237;&#31080;&#30340;&#22810;&#27169;&#24577;&#26041;&#27861;&#29992;&#20110;&#33258;&#21160;&#27450;&#39575;&#26816;&#27979;&#65292;&#36890;&#36807;&#35270;&#39057;&#30340;&#38899;&#39057;&#12289;&#35270;&#35273;&#21644;&#25991;&#26412;&#29305;&#24449;&#36827;&#34892;&#26816;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#27450;&#39575;&#26816;&#27979;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#27450;&#39575;&#26816;&#27979;&#19968;&#30452;&#26159;&#19968;&#20010;&#28909;&#38376;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#33258;&#21160;&#26816;&#27979;&#27450;&#39575;&#32473;&#36825;&#19968;&#26087;&#39046;&#22495;&#24102;&#26469;&#20102;&#26032;&#30340;&#20809;&#26126;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25237;&#31080;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#35270;&#39057;&#20013;&#20351;&#29992;&#38899;&#39057;&#12289;&#35270;&#35273;&#21644;&#25991;&#26412;&#29305;&#24449;&#36827;&#34892;&#33258;&#21160;&#27450;&#39575;&#26816;&#27979;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20998;&#21035;&#26159;&#23494;&#27463;&#26681;&#22823;&#23398;&#30340;&#30495;&#23454;&#35797;&#39564;&#25968;&#25454;&#38598;&#21644;&#36808;&#38463;&#23494;&#22823;&#23398;&#30340;&#27450;&#39575;&#26816;&#27979;&#25968;&#25454;&#38598;&#12290;&#35270;&#39057;&#26679;&#26412;&#34987;&#20998;&#25104;&#22270;&#20687;&#12289;&#38899;&#39057;&#21644;&#25163;&#31295;&#30340;&#24103;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#22810;&#27169;&#24577;&#25237;&#31080;&#35299;&#20915;&#26041;&#26696;&#21253;&#25324;&#19977;&#20010;&#27169;&#22411;&#12290;&#31532;&#19968;&#20010;&#27169;&#22411;&#26159;&#29992;&#20110;&#20174;&#22270;&#20687;&#20013;&#26816;&#27979;&#27450;&#39575;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#65292;&#31532;&#20108;&#20010;&#27169;&#22411;&#26159;&#29992;&#20110;&#20174;&#38899;&#39057;&#20013;&#26816;&#27979;&#27450;&#39575;&#30340;Mel&#39057;&#35889;&#22270;&#19978;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#65292;&#31532;&#19977;&#20010;&#27169;&#22411;&#26159;&#29992;&#20110;&#20174;&#25163;&#31295;&#20013;&#26816;&#27979;&#27450;&#39575;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#19978;&#30340;Word2Vec&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#12290;&#22312;&#22270;&#20687;&#12289;&#38899;&#39057;&#21644;&#25991;&#26412;&#19978;&#21462;&#24471;&#30340;&#26368;&#20339;&#32467;&#26524;&#20998;&#21035;&#20026;97&#65285;&#12289;96&#65285;&#12289;9
&lt;/p&gt;
&lt;p&gt;
Automatic Deception Detection has been a hot research topic for a long time, using machine learning and deep learning to automatically detect deception, brings new light to this old field. In this paper, we proposed a voting-based method for automatic deception detection from videos using audio, visual and lexical features. Experiments were done on two datasets, the Real-life trial dataset by Michigan University and the Miami University deception detection dataset. Video samples were split into frames of images, audio, and manuscripts. Our Voting-based Multimodal proposed solution consists of three models. The first model is CNN for detecting deception from images, the second model is Support Vector Machine (SVM) on Mel spectrograms for detecting deception from audio and the third model is Word2Vec on Support Vector Machine (SVM) for detecting deception from manuscripts. Our proposed solution outperforms state of the art. Best results achieved on images, audio and text were 97%, 96%, 9
&lt;/p&gt;</description></item><item><title>Musketeer&#26159;&#19968;&#31181;&#36890;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#37319;&#29992;&#20219;&#21153;&#35299;&#37322;&#25552;&#31034;&#65288;TEP&#65289;&#26426;&#21046;&#65292;&#33021;&#22815;&#26377;&#25928;&#25972;&#21512;&#24322;&#26500;&#20219;&#21153;&#30340;&#30693;&#35782;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#22343;&#21248;</title><link>http://arxiv.org/abs/2305.07019</link><description>&lt;p&gt;
Musketeer&#65288;&#19968;&#20154;&#20043;&#21147;&#65292;&#19975;&#20154;&#20043;&#21147;&#65289;&#65306;&#20855;&#26377;&#20219;&#21153;&#35299;&#37322;&#25552;&#31034;&#30340;&#36890;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Musketeer (All for One, and One for All): A Generalist Vision-Language Model with Task Explanation Prompts. (arXiv:2305.07019v1 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07019
&lt;/p&gt;
&lt;p&gt;
Musketeer&#26159;&#19968;&#31181;&#36890;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#37319;&#29992;&#20219;&#21153;&#35299;&#37322;&#25552;&#31034;&#65288;TEP&#65289;&#26426;&#21046;&#65292;&#33021;&#22815;&#26377;&#25928;&#25972;&#21512;&#24322;&#26500;&#20219;&#21153;&#30340;&#30693;&#35782;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#22343;&#21248;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#21442;&#25968;&#22312;&#25152;&#26377;&#20219;&#21153;&#19978;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#65288;&#19975;&#20154;&#20043;&#21147;&#65289;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#20043;&#38388;&#23436;&#20840;&#20849;&#20139;&#65288;&#19968;&#20154;&#20043;&#21147;&#65289;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#19968;&#20010;&#21517;&#20026;Musketeer&#30340;&#21333;&#19968;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a sequence-to-sequence vision-language model whose parameters are jointly trained on all tasks (all for one) and fully shared among multiple tasks (one for all), resulting in a single model which we named Musketeer. The integration of knowledge across heterogeneous tasks is enabled by a novel feature called Task Explanation Prompt (TEP). TEP reduces interference among tasks, allowing the model to focus on their shared structure. With a single model, Musketeer achieves results comparable to or better than strong baselines trained on single tasks, almost uniformly across multiple tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986; Iter-CoT &#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#36845;&#20195;&#22686;&#24378;&#30340;&#24605;&#32500;&#38142;&#25552;&#31034;&#65292;&#36890;&#36807;&#36873;&#25321;&#20855;&#26377;&#36866;&#24230;&#38590;&#24230;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#21487;&#22238;&#31572;&#30340;&#38382;&#39064;&#65292;&#24182;&#20276;&#38543;&#25512;&#29702;&#38142;&#20316;&#20026;&#31034;&#20363;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21516;&#26102;&#20351;&#27169;&#22411;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#29983;&#25104;&#25512;&#29702;&#38142;&#12290;</title><link>http://arxiv.org/abs/2304.11657</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21152;&#24378;&#36845;&#20195;&#22686;&#24378;&#30340;&#24605;&#32500;&#38142;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Enhancing Chain-of-Thoughts Prompting with Iterative Bootstrapping in Large Language Models. (arXiv:2304.11657v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986; Iter-CoT &#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#36845;&#20195;&#22686;&#24378;&#30340;&#24605;&#32500;&#38142;&#25552;&#31034;&#65292;&#36890;&#36807;&#36873;&#25321;&#20855;&#26377;&#36866;&#24230;&#38590;&#24230;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#21487;&#22238;&#31572;&#30340;&#38382;&#39064;&#65292;&#24182;&#20276;&#38543;&#25512;&#29702;&#38142;&#20316;&#20026;&#31034;&#20363;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21516;&#26102;&#20351;&#27169;&#22411;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#29983;&#25104;&#25512;&#29702;&#38142;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36880;&#27493;&#24341;&#23548;&#24605;&#32500;&#38142; (CoT) &#20316;&#20026;&#31034;&#33539;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#21487;&#20197;&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#19978;&#23454;&#29616;&#39640;&#24230;&#26377;&#25928;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;LLMs &#29983;&#25104;&#30340;&#28436;&#31034;&#25512;&#29702;&#38142;&#23481;&#26131;&#20986;&#29616;&#38169;&#35823;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#38169;&#35823;&#12290;&#27492;&#22806;&#65292;&#19981;&#24688;&#24403;&#30340;&#31034;&#20363; (&#36807;&#20110;&#31616;&#21333;&#25110;&#22797;&#26434;) &#21487;&#20197;&#24433;&#21709;&#22312;&#19981;&#21516;&#38590;&#24230;&#32423;&#21035;&#19979;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Iter-CoT (&#36845;&#20195;&#24341;&#23548;&#24605;&#32500;&#38142;&#25552;&#31034;) &#30340;&#36845;&#20195;&#24341;&#23548;&#26041;&#27861;&#65292;&#29992;&#20110;&#36873;&#25321;&#23454;&#20363;&#24182;&#29983;&#25104;&#25512;&#29702;&#38142;&#12290;&#36890;&#36807;&#21033;&#29992;&#36845;&#20195;&#22686;&#24378;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;LLMs &#33258;&#20027;&#26356;&#27491;&#38169;&#35823;&#65292;&#20174;&#32780;&#20135;&#29983;&#26356;&#31934;&#30830;&#12289;&#20840;&#38754;&#30340;&#25512;&#29702;&#38142;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36873;&#25321;&#20855;&#26377;&#36866;&#24230;&#38590;&#24230;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#21487;&#22238;&#31572;&#30340;&#38382;&#39064;&#65292;&#24182;&#20276;&#38543;&#25512;&#29702;&#38142;&#20316;&#20026;&#31034;&#20363;&#65292;&#20174;&#32780;&#22686;&#24378;LLMs &#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) can achieve highly effective performance on various reasoning tasks by incorporating step-by-step chain-of-thought (CoT) prompting as demonstrations. However, the reasoning chains of demonstrations generated by LLMs are prone to errors, which can subsequently lead to incorrect reasoning during inference. Furthermore, inappropriate exemplars (overly simplistic or complex), can affect overall performance among varying levels of difficulty. We introduce Iter-CoT (Iterative bootstrapping in Chain-of-Thoughts Prompting), an iterative bootstrapping approach for selecting exemplars and generating reasoning chains. By utilizing iterative bootstrapping, our approach enables LLMs to autonomously rectify errors, resulting in more precise and comprehensive reasoning chains. Simultaneously, our approach selects challenging yet answerable questions accompanied by reasoning chains as exemplars with a moderate level of difficulty, which enhances the LLMs' generalizability 
&lt;/p&gt;</description></item><item><title>WebQAmGaze&#26159;&#19968;&#20010;&#22810;&#35821;&#35328;&#20302;&#25104;&#26412;&#30340;&#38405;&#35835;&#26102;&#30524;&#21160;&#36861;&#36394;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;332&#20301;&#21442;&#19982;&#32773;&#30340;&#25968;&#25454;&#65292;&#23545;&#30456;&#20851;&#27573;&#33853;&#30340;&#27880;&#35270;&#20284;&#20046;&#33021;&#22815;&#21453;&#26144;&#22238;&#31572;&#29702;&#35299;&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#20221;&#25968;&#25454;&#21487;&#20197;&#25512;&#21160;&#22522;&#20110;&#32593;&#32476;&#25668;&#20687;&#22836;&#30340;&#38405;&#35835;&#30740;&#31350;&#24182;&#24320;&#36767;&#26356;&#20415;&#23452;&#12289;&#26356;&#26131;&#33719;&#24471;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2303.17876</link><description>&lt;p&gt;
WebQAmGaze: &#19968;&#20221;&#22810;&#35821;&#35328;Webcam&#38405;&#35835;&#26102;&#30524;&#21160;&#36861;&#36394;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
WebQAmGaze: A Multilingual Webcam Eye-Tracking-While-Reading Dataset. (arXiv:2303.17876v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17876
&lt;/p&gt;
&lt;p&gt;
WebQAmGaze&#26159;&#19968;&#20010;&#22810;&#35821;&#35328;&#20302;&#25104;&#26412;&#30340;&#38405;&#35835;&#26102;&#30524;&#21160;&#36861;&#36394;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;332&#20301;&#21442;&#19982;&#32773;&#30340;&#25968;&#25454;&#65292;&#23545;&#30456;&#20851;&#27573;&#33853;&#30340;&#27880;&#35270;&#20284;&#20046;&#33021;&#22815;&#21453;&#26144;&#22238;&#31572;&#29702;&#35299;&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#20221;&#25968;&#25454;&#21487;&#20197;&#25512;&#21160;&#22522;&#20110;&#32593;&#32476;&#25668;&#20687;&#22836;&#30340;&#38405;&#35835;&#30740;&#31350;&#24182;&#24320;&#36767;&#26356;&#20415;&#23452;&#12289;&#26356;&#26131;&#33719;&#24471;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21019;&#24314;&#20102;WebQAmGaze&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#35821;&#31181;&#20302;&#25104;&#26412;&#30340;&#38405;&#35835;&#26102;&#30524;&#21160;&#36861;&#36394;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25903;&#25345;&#20844;&#24179;&#36879;&#26126;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#24320;&#21457;&#12290;WebQAmGaze&#21253;&#25324;&#20102;&#26469;&#33258;332&#20301;&#21442;&#19982;&#32773;&#38405;&#35835;&#33521;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#21644;&#24503;&#35821;&#25991;&#26412;&#26102;&#30340;&#32593;&#32476;&#25668;&#20687;&#22836;&#30524;&#21160;&#25968;&#25454;&#12290;&#27599;&#20010;&#21442;&#19982;&#32773;&#37117;&#20250;&#23436;&#25104;&#20004;&#20010;&#38405;&#35835;&#20219;&#21153;&#65292;&#21253;&#25324;&#20116;&#31687;&#25991;&#31456;&#30340;&#27491;&#24120;&#38405;&#35835;&#21644;&#20449;&#24687;&#23547;&#25214;&#20219;&#21153;&#12290;&#32463;&#36807;&#25968;&#25454;&#39044;&#22788;&#29702;&#65292;&#25105;&#20204;&#21457;&#29616;&#23545;&#30456;&#20851;&#27573;&#33853;&#30340;&#27880;&#35270;&#20284;&#20046;&#24847;&#21619;&#30528;&#22238;&#31572;&#29702;&#35299;&#38382;&#39064;&#30340;&#27491;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#19982;&#39640;&#36136;&#37327;&#30340;&#30524;&#21160;&#36861;&#36394;&#25968;&#25454;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#32467;&#26524;&#26174;&#31034;Webcam-ET&#33719;&#24471;&#30340;&#29305;&#24449;&#19982;&#21830;&#19994;ET&#35774;&#22791;&#30340;&#29305;&#24449;&#20043;&#38388;&#23384;&#22312;&#20013;&#31561;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#20221;&#25968;&#25454;&#21487;&#20197;&#25512;&#21160;&#22522;&#20110;&#32593;&#32476;&#25668;&#20687;&#22836;&#30340;&#38405;&#35835;&#30740;&#31350;&#24182;&#24320;&#36767;&#26356;&#20415;&#23452;&#12289;&#26356;&#26131;&#33719;&#24471;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#24335;&#12290;WebQAmGaze&#23545;&#20110;&#20102;&#35299;&#38382;&#39064;&#22238;&#31572;&#30340;&#35748;&#30693;&#36807;&#31243;&#20197;&#21450;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#20844;&#24179;&#36879;&#26126;&#20855;&#26377;&#23454;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
We create WebQAmGaze, a multilingual low-cost eye-tracking-while-reading dataset, designed to support the development of fair and transparent NLP models. WebQAmGaze includes webcam eye-tracking data from 332 participants naturally reading English, Spanish, and German texts. Each participant performs two reading tasks composed of five texts, a normal reading and an information-seeking task. After preprocessing the data, we find that fixations on relevant spans seem to indicate correctness when answering the comprehension questions. Additionally, we perform a comparative analysis of the data collected to high-quality eye-tracking data. The results show a moderate correlation between the features obtained with the webcam-ET compared to those of a commercial ET device. We believe this data can advance webcam-based reading studies and open a way to cheaper and more accessible data collection. WebQAmGaze is useful to learn about the cognitive processes behind question answering (QA) and to a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#20020;&#24202;&#31508;&#35760;&#20013;&#25552;&#21462;&#21330;&#20013;&#24739;&#32773;&#27835;&#30103;&#36807;&#31243;&#30340;&#38203;&#28860;&#20449;&#24687;&#65292;&#24182;&#19982;&#20960;&#20010;&#23567;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#22312;&#36275;&#22815;&#30340;&#25968;&#25454;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#25552;&#21462;&#19968;&#21322;&#30340;&#27010;&#24565;&#26041;&#38754;&#20248;&#20110;&#36825;&#20123;&#27169;&#22411;&#65292;&#24182;&#19988;&#27599;&#20010;&#27010;&#24565;&#30340;&#20010;&#20307;&#36816;&#21160;&#25551;&#36848;&#21487;&#20197;&#20998;&#37197;&#20108;&#36827;&#21046;&#26631;&#31614;&#65292;&#24182;&#19988;F&#20540;&#19981;&#20302;&#20110;0.75&#12290;&#36825;&#20123;&#31639;&#27861;&#34920;&#29616;&#20986;&#20102;&#20934;&#30830;&#25552;&#21462;&#20020;&#24202;&#31508;&#35760;&#20013;&#24247;&#22797;&#27835;&#30103;&#38203;&#28860;&#20449;&#24687;&#30340;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2303.13466</link><description>&lt;p&gt;
&#20174;&#20020;&#24202;&#31508;&#35760;&#20013;&#25552;&#21462;&#24247;&#22797;&#38203;&#28860;&#20449;&#24687;&#65306;&#22522;&#20110;&#35268;&#21017;&#21644;&#26426;&#22120;&#23398;&#20064;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Extracting Physical Rehabilitation Exercise Information from Clinical Notes: a Comparison of Rule-Based and Machine Learning Natural Language Processing Techniques. (arXiv:2303.13466v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#20020;&#24202;&#31508;&#35760;&#20013;&#25552;&#21462;&#21330;&#20013;&#24739;&#32773;&#27835;&#30103;&#36807;&#31243;&#30340;&#38203;&#28860;&#20449;&#24687;&#65292;&#24182;&#19982;&#20960;&#20010;&#23567;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#22312;&#36275;&#22815;&#30340;&#25968;&#25454;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#25552;&#21462;&#19968;&#21322;&#30340;&#27010;&#24565;&#26041;&#38754;&#20248;&#20110;&#36825;&#20123;&#27169;&#22411;&#65292;&#24182;&#19988;&#27599;&#20010;&#27010;&#24565;&#30340;&#20010;&#20307;&#36816;&#21160;&#25551;&#36848;&#21487;&#20197;&#20998;&#37197;&#20108;&#36827;&#21046;&#26631;&#31614;&#65292;&#24182;&#19988;F&#20540;&#19981;&#20302;&#20110;0.75&#12290;&#36825;&#20123;&#31639;&#27861;&#34920;&#29616;&#20986;&#20102;&#20934;&#30830;&#25552;&#21462;&#20020;&#24202;&#31508;&#35760;&#20013;&#24247;&#22797;&#27835;&#30103;&#38203;&#28860;&#20449;&#24687;&#30340;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24247;&#22797;&#38203;&#28860;&#22312;&#21330;&#20013;&#21518;&#24739;&#32773;&#30340;&#24247;&#22797;&#36807;&#31243;&#20013;&#25198;&#28436;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#36890;&#36807;&#20010;&#24615;&#21270;&#27835;&#30103;&#21644;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65292;&#21307;&#30103;&#20445;&#20581;&#25552;&#20379;&#32773;&#21487;&#20197;&#20351;&#24247;&#22797;&#36807;&#31243;&#26356;&#21152;&#39640;&#25928;&#12290;&#22312;&#39044;&#27979;&#24314;&#27169;&#20026;&#24739;&#32773;&#20998;&#37197;&#27835;&#30103;&#35745;&#21010;&#20043;&#21069;&#65292;&#33258;&#21160;&#21270;&#26041;&#27861;&#26159;&#20174;&#38750;&#32467;&#26500;&#21270;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#25552;&#21462;&#24247;&#22797;&#38203;&#28860;&#20449;&#24687;&#25152;&#24517;&#38656;&#30340;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#35268;&#21017;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31639;&#27861;&#26469;&#27880;&#37322;&#21330;&#20013;&#24739;&#32773;&#30340;&#27835;&#30103;&#36807;&#31243;&#65292;&#24182;&#23558;&#20854;&#19982;&#20960;&#20010;&#23567;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#36275;&#22815;&#30340;&#25968;&#25454;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#25552;&#21462;&#19968;&#21322;&#30340;&#27010;&#24565;&#26041;&#38754;&#20248;&#20110;&#36825;&#20123;&#27169;&#22411;&#65292;&#24182;&#19988;&#27599;&#20010;&#27010;&#24565;&#30340;&#20010;&#20307;&#36816;&#21160;&#25551;&#36848;&#21487;&#20197;&#20998;&#37197;&#20108;&#36827;&#21046;&#26631;&#31614;&#65292;&#24182;&#19988;F&#20540;&#19981;&#20302;&#20110;0.75&#12290;&#22312;&#36825;&#20123;&#31639;&#27861;&#21487;&#20197;&#37096;&#32626;&#21040;&#26080;&#26631;&#31614;&#25991;&#26723;&#20043;&#21069;&#65292;&#38656;&#35201;&#36827;&#34892;&#26356;&#22810;&#30340;&#30740;&#31350;&#65292;&#20294;&#23450;&#21046;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31639;&#27861;&#34920;&#29616;&#20986;&#20102;&#20934;&#30830;&#25552;&#21462;&#20020;&#24202;&#31508;&#35760;&#20013;&#24247;&#22797;&#27835;&#30103;&#38203;&#28860;&#20449;&#24687;&#30340;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physical rehabilitation plays a crucial role in the recovery process of post-stroke patients. By personalizing therapies for patients leveraging predictive modeling and electronic health records (EHRs), healthcare providers can make the rehabilitation process more efficient. Before predictive modeling can provide decision support for the assignment of treatment plans, automated methods are necessary to extract physical rehabilitation exercise information from unstructured EHRs. We introduce a rule-based natural language processing algorithm to annotate therapeutic procedures for stroke patients and compare it to several small machine learning models. We find that our algorithm outperforms these models in extracting half of the concepts where sufficient data is available, and individual exercise descriptions can be assigned binary labels with an f-score of no less than 0.75 per concept. More research needs to be done before these algorithms can be deployed on unlabeled documents, but cu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21152;&#24378;&#21477;&#23376;&#36873;&#25321;&#26469;&#22686;&#24378;&#25991;&#26412;&#29983;&#25104;&#30340;&#23398;&#20064;&#29983;&#25104;&#38382;&#39064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#35774;&#35745;&#36873;&#25321;&#22120;&#21644;&#29983;&#25104;&#22120;&#20004;&#20010;&#27169;&#22359;&#65292;&#20351;&#27169;&#22411;&#26356;&#20851;&#27880;&#19982;&#31572;&#26696;&#30456;&#20851;&#30340;&#21477;&#23376;&#65292;&#24182;&#38544;&#24335;&#32467;&#21512;&#23616;&#37096;&#20449;&#24687;&#21644;&#20840;&#23616;&#20449;&#24687;&#26469;&#29983;&#25104;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#38382;&#39064;&#29983;&#25104;&#20219;&#21153;&#19978;&#20248;&#20110;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2212.12192</link><description>&lt;p&gt;
&#36890;&#36807;&#21152;&#24378;&#21477;&#23376;&#36873;&#25321;&#26469;&#22686;&#24378;&#25991;&#26412;&#29983;&#25104;&#30340;&#23398;&#20064;&#29983;&#25104;&#38382;&#39064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning to Generate Questions by Enhancing Text Generation with Sentence Selection. (arXiv:2212.12192v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12192
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21152;&#24378;&#21477;&#23376;&#36873;&#25321;&#26469;&#22686;&#24378;&#25991;&#26412;&#29983;&#25104;&#30340;&#23398;&#20064;&#29983;&#25104;&#38382;&#39064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#35774;&#35745;&#36873;&#25321;&#22120;&#21644;&#29983;&#25104;&#22120;&#20004;&#20010;&#27169;&#22359;&#65292;&#20351;&#27169;&#22411;&#26356;&#20851;&#27880;&#19982;&#31572;&#26696;&#30456;&#20851;&#30340;&#21477;&#23376;&#65292;&#24182;&#38544;&#24335;&#32467;&#21512;&#23616;&#37096;&#20449;&#24687;&#21644;&#20840;&#23616;&#20449;&#24687;&#26469;&#29983;&#25104;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#38382;&#39064;&#29983;&#25104;&#20219;&#21153;&#19978;&#20248;&#20110;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22238;&#31572;&#24863;&#30693;&#30340;&#38382;&#39064;&#29983;&#25104;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22238;&#31572;&#21644;&#38382;&#39064;&#30340;&#20449;&#24687;&#21487;&#20197;&#22312;&#19978;&#19979;&#25991;&#20013;&#30340;&#19968;&#20123;&#30456;&#20851;&#21477;&#23376;&#20013;&#25214;&#21040;&#65292;&#32780;&#19981;&#20165;&#20165;&#20381;&#36182;&#20110;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#21253;&#25324;&#20004;&#20010;&#27169;&#22359;&#65306;&#36873;&#25321;&#22120;&#21644;&#29983;&#25104;&#22120;&#12290;&#36873;&#25321;&#22120;&#24378;&#21046;&#27169;&#22411;&#26356;&#21152;&#20851;&#27880;&#19982;&#31572;&#26696;&#30456;&#20851;&#30340;&#21477;&#23376;&#65292;&#20197;&#25552;&#20379;&#38544;&#21547;&#30340;&#23616;&#37096;&#20449;&#24687;&#12290;&#29983;&#25104;&#22120;&#36890;&#36807;&#23558;&#36873;&#25321;&#22120;&#25552;&#20379;&#30340;&#23616;&#37096;&#20449;&#24687;&#19982;&#32534;&#30721;&#22120;&#32534;&#30721;&#30340;&#25972;&#20010;&#19978;&#19979;&#25991;&#30340;&#20840;&#23616;&#20449;&#24687;&#38544;&#24335;&#32467;&#21512;&#26469;&#29983;&#25104;&#38382;&#39064;&#12290;&#27169;&#22411;&#32852;&#21512;&#35757;&#32451;&#20197;&#21033;&#29992;&#20004;&#20010;&#27169;&#22359;&#20043;&#38388;&#30340;&#28508;&#22312;&#20132;&#20114;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#27604;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#38382;&#39064;&#29983;&#25104;&#20219;&#21153;&#19978;&#26356;&#22909;&#12290;&#20195;&#30721;&#20063;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce an approach for the answer-aware question generation problem. Instead of only relying on the capability of strong pre-trained language models, we observe that the information of answers and questions can be found in some relevant sentences in the context. Based on that, we design a model which includes two modules: a selector and a generator. The selector forces the model to more focus on relevant sentences regarding an answer to provide implicit local information. The generator generates questions by implicitly combining local information from the selector and global information from the whole context encoded by the encoder. The model is trained jointly to take advantage of latent interactions between the two modules. Experimental results on two benchmark datasets show that our model is better than strong pre-trained models for the question generation task. The code is also available.
&lt;/p&gt;</description></item></channel></rss>