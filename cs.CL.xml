<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>ParaGuide&#26159;&#19968;&#31181;&#29992;&#20110;&#36890;&#29992;&#39118;&#26684;&#36716;&#31227;&#30340;&#24341;&#23548;&#24615;&#25193;&#25955;&#25913;&#20889;&#22120;&#65292;&#21487;&#20197;&#28789;&#27963;&#36866;&#24212;&#20219;&#24847;&#30446;&#26631;&#39118;&#26684;&#65292;&#36890;&#36807;&#26799;&#24230;&#24341;&#23548;&#21644;&#25913;&#20889;&#26465;&#20214;&#30340;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#25991;&#26412;&#30340;&#39118;&#26684;&#36716;&#21464;&#65292;&#21516;&#26102;&#20445;&#30041;&#35821;&#20041;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2308.15459</link><description>&lt;p&gt;
ParaGuide: &#29992;&#20110;&#21363;&#25554;&#21363;&#29992;&#25991;&#26412;&#39118;&#26684;&#36716;&#31227;&#30340;&#24341;&#23548;&#24615;&#25193;&#25955;&#25913;&#20889;&#22120;
&lt;/p&gt;
&lt;p&gt;
ParaGuide: Guided Diffusion Paraphrasers for Plug-and-Play Textual Style Transfer. (arXiv:2308.15459v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15459
&lt;/p&gt;
&lt;p&gt;
ParaGuide&#26159;&#19968;&#31181;&#29992;&#20110;&#36890;&#29992;&#39118;&#26684;&#36716;&#31227;&#30340;&#24341;&#23548;&#24615;&#25193;&#25955;&#25913;&#20889;&#22120;&#65292;&#21487;&#20197;&#28789;&#27963;&#36866;&#24212;&#20219;&#24847;&#30446;&#26631;&#39118;&#26684;&#65292;&#36890;&#36807;&#26799;&#24230;&#24341;&#23548;&#21644;&#25913;&#20889;&#26465;&#20214;&#30340;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#25991;&#26412;&#30340;&#39118;&#26684;&#36716;&#21464;&#65292;&#21516;&#26102;&#20445;&#30041;&#35821;&#20041;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#39118;&#26684;&#36716;&#31227;&#26159;&#22312;&#20445;&#30041;&#24847;&#20041;&#30340;&#21516;&#26102;&#36716;&#21464;&#25991;&#26412;&#30340;&#39118;&#26684;&#23646;&#24615;&#30340;&#20219;&#21153;&#12290;&#30446;&#26631;&#39118;&#26684;&#21487;&#20197;&#20197;&#22810;&#31181;&#26041;&#24335;&#23450;&#20041;&#65292;&#20174;&#21333;&#19968;&#23646;&#24615;&#65288;&#20363;&#22914;&#27491;&#24335;&#24615;&#65289;&#21040;&#20316;&#32773;&#65288;&#20363;&#22914;&#33678;&#22763;&#27604;&#20122;&#65289;&#12290;&#20808;&#21069;&#30340;&#26080;&#30417;&#30563;&#39118;&#26684;&#36716;&#31227;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#65292;&#20165;&#36866;&#29992;&#20110;&#22266;&#23450;&#30340;&#39118;&#26684;&#38598;&#65292;&#25110;&#38656;&#35201;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#36890;&#29992;&#39118;&#26684;&#36716;&#31227;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#25512;&#29702;&#26102;&#28789;&#27963;&#36866;&#24212;&#20219;&#24847;&#30446;&#26631;&#39118;&#26684;&#12290;&#25105;&#20204;&#30340;&#21442;&#25968;&#39640;&#25928;&#26041;&#27861;ParaGuide&#21033;&#29992;&#20102;&#25913;&#20889;&#26465;&#20214;&#30340;&#25193;&#25955;&#27169;&#22411;&#20197;&#21450;&#26469;&#33258;&#29616;&#25104;&#30340;&#20998;&#31867;&#22120;&#21644;&#24378;&#22823;&#30340;&#39118;&#26684;&#23884;&#20837;&#22120;&#30340;&#26799;&#24230;&#24341;&#23548;&#65292;&#20197;&#36716;&#21464;&#25991;&#26412;&#30340;&#39118;&#26684;&#21516;&#26102;&#20445;&#30041;&#35821;&#20041;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;Enron&#37038;&#20214;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#21253;&#25324;&#20154;&#24037;&#21644;&#33258;&#21160;&#35780;&#20272;&#65292;&#24182;&#21457;&#29616;&#20854;&#22312;&#27491;&#24335;&#24615;&#21644;... (&#20869;&#23481;&#22826;&#22810;&#65292;&#35831;&#21442;&#32771;&#33521;&#25991;&#25688;&#35201;)
&lt;/p&gt;
&lt;p&gt;
Textual style transfer is the task of transforming stylistic properties of text while preserving meaning. Target "styles" can be defined in numerous ways, ranging from single attributes (e.g, formality) to authorship (e.g, Shakespeare). Previous unsupervised style-transfer approaches generally rely on significant amounts of labeled data for only a fixed set of styles or require large language models. In contrast, we introduce a novel diffusion-based framework for general-purpose style transfer that can be flexibly adapted to arbitrary target styles at inference time. Our parameter-efficient approach, ParaGuide, leverages paraphrase-conditioned diffusion models alongside gradient-based guidance from both off-the-shelf classifiers and strong existing style embedders to transform the style of text while preserving semantic information. We validate the method on the Enron Email Corpus, with both human and automatic evaluations, and find that it outperforms strong baselines on formality, se
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22797;&#26434;&#24615;&#24433;&#21709;&#25512;&#29702;&#20998;&#25968;&#65288;CIRS&#65289;&#26469;&#34913;&#37327;&#32534;&#31243;&#35821;&#35328;&#23545;&#25512;&#29702;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#24182;&#38750;&#25152;&#26377;&#22797;&#26434;&#24615;&#30340;&#20195;&#30721;&#25968;&#25454;&#37117;&#21487;&#20197;&#34987;&#23398;&#20064;&#25110;&#29702;&#35299;&#65292;&#36866;&#24403;&#30340;&#22797;&#26434;&#24615;&#27700;&#24179;&#23545;&#20110;&#25913;&#21892;&#25512;&#29702;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2308.15452</link><description>&lt;p&gt;
&#20160;&#20040;&#26102;&#20505;&#32534;&#31243;&#24605;&#32500;&#23545;&#25512;&#29702;&#36215;&#20316;&#29992;?
&lt;/p&gt;
&lt;p&gt;
When Do Program-of-Thoughts Work for Reasoning?. (arXiv:2308.15452v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15452
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22797;&#26434;&#24615;&#24433;&#21709;&#25512;&#29702;&#20998;&#25968;&#65288;CIRS&#65289;&#26469;&#34913;&#37327;&#32534;&#31243;&#35821;&#35328;&#23545;&#25512;&#29702;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#24182;&#38750;&#25152;&#26377;&#22797;&#26434;&#24615;&#30340;&#20195;&#30721;&#25968;&#25454;&#37117;&#21487;&#20197;&#34987;&#23398;&#20064;&#25110;&#29702;&#35299;&#65292;&#36866;&#24403;&#30340;&#22797;&#26434;&#24615;&#27700;&#24179;&#23545;&#20110;&#25913;&#21892;&#25512;&#29702;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#22312;&#20307;&#29616;&#20986;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#23613;&#31649;&#20687;&#32534;&#31243;&#24605;&#32500;&#25552;&#31034;&#36825;&#26679;&#30340;&#26041;&#27861;&#23545;&#20110;&#20351;&#29992;&#32534;&#31243;&#35821;&#35328;&#26469;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#30340;LLM&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#20195;&#30721;&#25968;&#25454;&#23545;&#25512;&#29702;&#33021;&#21147;&#30340;&#20855;&#20307;&#24433;&#21709;&#20173;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22797;&#26434;&#24615;&#24433;&#21709;&#25512;&#29702;&#20998;&#25968;&#65288;CIRS&#65289;&#65292;&#23427;&#32467;&#21512;&#20102;&#32467;&#26500;&#21644;&#36923;&#36753;&#23646;&#24615;&#65292;&#20197;&#34913;&#37327;&#20195;&#30721;&#21644;&#25512;&#29702;&#33021;&#21147;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#25277;&#35937;&#35821;&#27861;&#26641;&#26469;&#32534;&#30721;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#38590;&#24230;&#21644;&#22280;&#22797;&#26434;&#24230;&#26469;&#35745;&#31639;&#36923;&#36753;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#24182;&#38750;&#25152;&#26377;&#22797;&#26434;&#24615;&#30340;&#20195;&#30721;&#25968;&#25454;&#37117;&#21487;&#20197;&#34987;LLM&#23398;&#20064;&#25110;&#29702;&#35299;&#12290;&#26368;&#20339;&#22797;&#26434;&#24615;&#27700;&#24179;&#23545;&#20110;&#36890;&#36807;&#32534;&#31243;&#36741;&#21161;&#25552;&#31034;&#25913;&#21892;&#25512;&#29702;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#21518;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#33258;&#21160;&#21512;&#25104;&#30340;&#26041;&#27861;...
&lt;/p&gt;
&lt;p&gt;
The reasoning capabilities of Large Language Models (LLMs) play a pivotal role in the realm of embodied artificial intelligence. Although there are effective methods like program-of-thought prompting for LLMs which uses programming language to tackle complex reasoning tasks, the specific impact of code data on the improvement of reasoning capabilities remains under-explored. To address this gap, we propose complexity-impacted reasoning score (CIRS), which combines structural and logical attributes, to measure the correlation between code and reasoning abilities. Specifically, we use the abstract syntax tree to encode the structural information and calculate logical complexity by considering the difficulty and the cyclomatic complexity. Through an empirical analysis, we find not all code data of complexity can be learned or understood by LLMs. Optimal level of complexity is critical to the improvement of reasoning abilities by program-aided prompting. Then we design an auto-synthesizing
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30340;&#30740;&#31350;&#37325;&#28857;&#26159;&#21033;&#29992;&#26377;&#30417;&#30563;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#22312;&#31038;&#20132;&#23186;&#20307;&#20013;&#26816;&#27979;&#24688;&#22612;&#26681;&#35821;&#35328;&#26041;&#35328;&#20013;&#30340;&#31895;&#20439;&#35328;&#35770;&#65292;&#24182;&#21457;&#29616;&#36923;&#36753;&#22238;&#24402;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#65288;0.91&#65289;&#12290;</title><link>http://arxiv.org/abs/2308.15448</link><description>&lt;p&gt;
&#12298;&#23391;&#21152;&#25289;&#30340;&#27450;&#20940;&#35328;&#35770;&#26816;&#27979;&#12299;&#35770;&#25991;&#20013;&#23545;&#24688;&#22612;&#26681;&#35821;&#35328;&#26041;&#35328;&#30340;&#32763;&#35793;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Vulgar Remarks Detection in Chittagonian Dialect of Bangla. (arXiv:2308.15448v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15448
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30340;&#30740;&#31350;&#37325;&#28857;&#26159;&#21033;&#29992;&#26377;&#30417;&#30563;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#22312;&#31038;&#20132;&#23186;&#20307;&#20013;&#26816;&#27979;&#24688;&#22612;&#26681;&#35821;&#35328;&#26041;&#35328;&#20013;&#30340;&#31895;&#20439;&#35328;&#35770;&#65292;&#24182;&#21457;&#29616;&#36923;&#36753;&#22238;&#24402;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#65288;0.91&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20114;&#32852;&#32593;&#30340;&#26222;&#21450;&#65292;&#32593;&#32476;&#27450;&#20940;&#21644;&#39578;&#25200;&#30340;&#36127;&#38754;&#24433;&#21709;&#36234;&#26469;&#36234;&#22823;&#65292;&#23588;&#20854;&#26159;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#12290;&#35299;&#20915;&#26041;&#26696;&#20043;&#19968;&#26159;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#33258;&#21160;&#26816;&#27979;&#26377;&#23475;&#35328;&#35770;&#65292;&#20294;&#26159;&#22312;&#20687;&#23391;&#21152;&#25289;&#30340;&#24688;&#22612;&#26681;&#35821;&#35328;&#26041;&#35328;&#36825;&#26679;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#65292;&#36825;&#20123;&#26041;&#27861;&#21463;&#38480;&#12290;&#26412;&#30740;&#31350;&#20391;&#37325;&#20110;&#20351;&#29992;&#26377;&#30417;&#30563;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#26816;&#27979;&#31895;&#20439;&#35328;&#35770;&#12290;&#36923;&#36753;&#22238;&#24402;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#20934;&#30830;&#24230;&#65288;0.91&#65289;&#65292;&#32780;&#31616;&#21333;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#19982;Word2vec&#21644;fastTex&#30456;&#27604;&#20934;&#30830;&#24230;&#36739;&#20302;&#65288;0.84-0.90&#65289;&#65292;&#31361;&#26174;&#20102;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#38656;&#35201;&#26356;&#22810;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The negative effects of online bullying and harassment are increasing with Internet popularity, especially in social media. One solution is using natural language processing (NLP) and machine learning (ML) methods for the automatic detection of harmful remarks, but these methods are limited in low-resource languages like the Chittagonian dialect of Bangla.This study focuses on detecting vulgar remarks in social media using supervised ML and deep learning algorithms.Logistic Regression achieved promising accuracy (0.91) while simple RNN with Word2vec and fastTex had lower accuracy (0.84-0.90), highlighting the issue that NN algorithms require more data.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20174;&#20116;&#20010;&#33521;&#35821;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#36816;&#34892;&#20013;&#25552;&#21462;&#23398;&#20064;&#26354;&#32447;&#65292;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#29983;&#25104;&#26356;&#38271;&#12289;&#26356;&#36830;&#36143;&#30340;&#25991;&#26412;&#20043;&#21069;&#65292;&#20250;&#29983;&#25104;&#30701;&#32780;&#37325;&#22797;&#30340;&#30701;&#35821;&#12290;&#21516;&#26102;&#65292;&#39057;&#32321;&#20986;&#29616;&#30340;&#26631;&#35760;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#26356;&#26089;&#23398;&#20064;&#65292;&#20855;&#26377;&#26356;&#23567;&#30340;&#21464;&#24322;&#24615;&#65292;&#24182;&#19988;&#24456;&#23569;&#34987;&#36951;&#24536;&#12290;&#36739;&#30701;&#12289;&#26356;&#39057;&#32321;&#30340;&#19978;&#19979;&#25991;&#19982;&#31283;&#23450;&#21644;&#24555;&#36895;&#33719;&#24471;&#30340;&#39044;&#27979;&#26377;&#20851;&#12290;&#35789;&#31867;&#30340;&#24433;&#21709;&#36739;&#23567;&#65292;&#20294;&#21517;&#35789;&#20542;&#21521;&#20110;&#36739;&#26202;&#33719;&#24471;&#19988;&#36951;&#24536;&#29575;&#36739;&#20302;&#12290;</title><link>http://arxiv.org/abs/2308.15419</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#26399;&#38388;&#23398;&#20064;&#26354;&#32447;&#30340;&#29305;&#24449;&#21270;&#65306;&#23398;&#20064;&#12289;&#36951;&#24536;&#21644;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Characterizing Learning Curves During Language Model Pre-Training: Learning, Forgetting, and Stability. (arXiv:2308.15419v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15419
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20174;&#20116;&#20010;&#33521;&#35821;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#36816;&#34892;&#20013;&#25552;&#21462;&#23398;&#20064;&#26354;&#32447;&#65292;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#29983;&#25104;&#26356;&#38271;&#12289;&#26356;&#36830;&#36143;&#30340;&#25991;&#26412;&#20043;&#21069;&#65292;&#20250;&#29983;&#25104;&#30701;&#32780;&#37325;&#22797;&#30340;&#30701;&#35821;&#12290;&#21516;&#26102;&#65292;&#39057;&#32321;&#20986;&#29616;&#30340;&#26631;&#35760;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#26356;&#26089;&#23398;&#20064;&#65292;&#20855;&#26377;&#26356;&#23567;&#30340;&#21464;&#24322;&#24615;&#65292;&#24182;&#19988;&#24456;&#23569;&#34987;&#36951;&#24536;&#12290;&#36739;&#30701;&#12289;&#26356;&#39057;&#32321;&#30340;&#19978;&#19979;&#25991;&#19982;&#31283;&#23450;&#21644;&#24555;&#36895;&#33719;&#24471;&#30340;&#39044;&#27979;&#26377;&#20851;&#12290;&#35789;&#31867;&#30340;&#24433;&#21709;&#36739;&#23567;&#65292;&#20294;&#21517;&#35789;&#20542;&#21521;&#20110;&#36739;&#26202;&#33719;&#24471;&#19988;&#36951;&#24536;&#29575;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#20116;&#20010;&#33258;&#22238;&#24402;&#33521;&#35821;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#36816;&#34892;&#20013;&#25552;&#21462;&#23398;&#20064;&#26354;&#32447;&#65292;&#29992;&#20110;&#19978;&#19979;&#25991;&#20013;&#30340;100&#19975;&#20010;&#26631;&#35760;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#23398;&#20064;&#29983;&#25104;&#26356;&#38271;&#12289;&#26356;&#36830;&#36143;&#25991;&#26412;&#20043;&#21069;&#65292;&#35821;&#35328;&#27169;&#22411;&#20250;&#29983;&#25104;&#30701;&#32780;&#37325;&#22797;&#30340;&#30701;&#35821;&#12290;&#25105;&#20204;&#23450;&#37327;&#25551;&#36848;&#20102;&#21333;&#20010;&#19978;&#19979;&#25991;&#20013;&#26631;&#35760;&#30340;&#23398;&#20064;&#26354;&#32447;&#30340;&#26368;&#32456;surprisal&#12289;&#36816;&#34892;&#20869;&#21464;&#24322;&#24615;&#12289;&#33719;&#24471;&#24180;&#40836;&#12289;&#36951;&#24536;&#24230;&#21644;&#36328;&#36816;&#34892;&#21464;&#24322;&#24615;&#12290;&#26356;&#39057;&#32321;&#30340;&#26631;&#35760;&#36798;&#21040;&#36739;&#20302;&#30340;&#26368;&#32456;surprisal&#65292;&#20854;&#20869;&#37096;&#21644;&#39044;&#35757;&#32451;&#36816;&#34892;&#38388;&#21464;&#24322;&#24615;&#36739;&#23567;&#65292;&#23398;&#20064;&#24471;&#26356;&#26089;&#65292;&#24182;&#19988;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#24456;&#23569;&#34987;&#8220;&#36951;&#24536;&#8221;&#12290;&#26356;&#39640;&#30340;n-gram&#27010;&#29575;&#36827;&#19968;&#27493;&#21152;&#24378;&#20102;&#36825;&#20123;&#25928;&#26524;&#12290;&#19982;&#30446;&#26631;&#26631;&#35760;&#26080;&#20851;&#65292;&#36739;&#30701;&#12289;&#26356;&#39057;&#32321;&#30340;&#19978;&#19979;&#25991;&#19982;&#36739;&#31283;&#23450;&#21644;&#24555;&#36895;&#33719;&#24471;&#30340;&#39044;&#27979;&#30053;&#26377;&#30456;&#20851;&#12290;&#35789;&#31867;&#30340;&#24433;&#21709;&#20063;&#36739;&#23567;&#65292;&#23613;&#31649;&#21517;&#35789;&#20542;&#21521;&#20110;&#36739;&#26202;&#33719;&#24471;&#19988;&#36951;&#24536;&#29575;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
How do language models learn to make predictions during pre-training? To study this question, we extract learning curves from five autoregressive English language model pre-training runs, for 1M tokens in context. We observe that the language models generate short repetitive phrases before learning to generate longer and more coherent text. We quantify the final surprisal, within-run variability, age of acquisition, forgettability, and cross-run variability of learning curves for individual tokens in context. More frequent tokens reach lower final surprisals, exhibit less variability within and across pre-training runs, are learned earlier, and are less likely to be "forgotten" during pre-training. Higher n-gram probabilities further accentuate these effects. Independent of the target token, shorter and more frequent contexts correlate with marginally more stable and quickly acquired predictions. Effects of part-of-speech are also small, although nouns tend to be acquired later and les
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26681;&#25454;&#36328;&#23398;&#31185;&#30740;&#31350;&#20013;&#24314;&#31435;&#30340;&#36947;&#24503;&#29702;&#35770;&#36827;&#34892;&#36947;&#24503;&#25512;&#29702;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#38754;&#20020;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.15399</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#26426;&#22120;&#20262;&#29702; - LLM&#33021;&#21542;&#36890;&#36807;&#36947;&#24503;&#29702;&#35770;&#36827;&#34892;&#36947;&#24503;&#25512;&#29702;&#65311;
&lt;/p&gt;
&lt;p&gt;
Rethinking Machine Ethics -- Can LLMs Perform Moral Reasoning through the Lens of Moral Theories?. (arXiv:2308.15399v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26681;&#25454;&#36328;&#23398;&#31185;&#30740;&#31350;&#20013;&#24314;&#31435;&#30340;&#36947;&#24503;&#29702;&#35770;&#36827;&#34892;&#36947;&#24503;&#25512;&#29702;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#38754;&#20020;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#34892;&#36947;&#24503;&#21028;&#26029;&#26159;&#21457;&#23637;&#20262;&#29702;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#37325;&#35201;&#19968;&#27493;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#22823;&#22810;&#25968;&#20197;&#33258;&#19979;&#32780;&#19978;&#30340;&#26041;&#24335;&#23454;&#26045;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#37327;&#30340;&#27880;&#37322;&#25968;&#25454;&#26469;&#35757;&#32451;&#22522;&#20110;&#20247;&#21253;&#24847;&#35265;&#30340;&#27169;&#22411;&#65292;&#26469;&#21028;&#26029;&#36947;&#24503;&#38382;&#39064;&#12290;&#36825;&#20123;&#26041;&#27861;&#22240;&#28508;&#22312;&#36807;&#24230;&#26222;&#36941;&#21270;&#26377;&#38480;&#30340;&#27880;&#37322;&#32773;&#36947;&#24503;&#31435;&#22330;&#24182;&#19988;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#32780;&#21463;&#21040;&#25209;&#35780;&#12290;&#30456;&#21453;&#65292;&#33258;&#19978;&#32780;&#19979;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#19968;&#22871;&#21407;&#21017;&#36827;&#34892;&#36947;&#24503;&#21028;&#26029;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#26041;&#27861;&#22312;&#27010;&#24565;&#19978;&#23384;&#22312;&#38382;&#39064;&#65292;&#22240;&#20026;&#20043;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#32988;&#20219;&#65292;&#19988;&#36947;&#24503;&#21407;&#21017;&#20043;&#38388;&#23384;&#22312;&#26410;&#35299;&#20915;&#30340;&#36777;&#35770;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26681;&#25454;&#36328;&#23398;&#31185;&#30740;&#31350;&#20013;&#24314;&#31435;&#30340;&#36947;&#24503;&#29702;&#35770;&#36827;&#34892;&#36947;&#24503;&#25512;&#29702;&#12290;&#36825;&#20010;&#33258;&#19978;&#32780;&#19979;&#30340;&#29702;&#35770;&#24341;&#23548;&#26694;&#26550;&#21487;&#20197;&#34701;&#20837;&#21508;&#31181;&#36947;&#24503;&#29702;&#35770;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#36825;&#20010;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#22522;&#20110;&#36947;&#24503;&#29702;&#35770;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Making moral judgments is an essential step toward developing ethical AI systems. Prevalent approaches are mostly implemented in a bottom-up manner, which uses a large set of annotated data to train models based on crowd-sourced opinions about morality. These approaches have been criticized for potentially overgeneralizing a limited group of annotators' moral stances and lacking explainability. In contrast, top-down approaches make moral judgments grounded in a set of principles. However, it remains conceptual due to the incapability of previous language models and the unsolved debate among moral principles. In this study, we propose a flexible framework to steer Large Language Models (LLMs) to perform moral reasoning with well-established moral theories from interdisciplinary research. The theory-guided top-down framework can incorporate various moral theories. Our experiments demonstrate the effectiveness of the proposed framework on datasets derived from moral theories. Furthermore,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36171;&#33021;&#30340;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#30340;&#22522;&#20934;&#35780;&#20272;&#65292;&#24182;&#22522;&#20110;&#23454;&#39564;&#32467;&#26524;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#25104;&#35299;&#20915;&#26041;&#26696;DAIL-SQL&#65292;&#21047;&#26032;&#20102;Spider&#27036;&#21333;&#24182;&#23454;&#29616;&#20102;86.6%&#30340;&#25191;&#34892;&#20934;&#30830;&#29575;&#12290;&#21516;&#26102;&#65292;&#24378;&#35843;&#20102;&#22312;&#25552;&#31034;&#24037;&#31243;&#20013;&#30340;&#35789;&#27719;&#25928;&#29575;&#20197;&#23454;&#29616;&#39640;&#25928;&#32463;&#27982;&#30340;LLM-based&#25991;&#26412;&#21040;SQL&#35299;&#20915;&#26041;&#26696;&#65292;&#27492;&#22806;&#36824;&#23545;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#24212;&#29992;&#24320;&#28304;LLMs&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#36827;&#34892;&#20102;&#20219;&#21153;&#29305;&#23450;&#30340;&#24615;&#33021;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.15363</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#36171;&#33021;&#25991;&#26412;&#21040;SQL&#30340;&#30740;&#31350;&#65306;&#19968;&#20010;&#22522;&#20934;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Text-to-SQL Empowered by Large Language Models: A Benchmark Evaluation. (arXiv:2308.15363v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36171;&#33021;&#30340;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#30340;&#22522;&#20934;&#35780;&#20272;&#65292;&#24182;&#22522;&#20110;&#23454;&#39564;&#32467;&#26524;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#25104;&#35299;&#20915;&#26041;&#26696;DAIL-SQL&#65292;&#21047;&#26032;&#20102;Spider&#27036;&#21333;&#24182;&#23454;&#29616;&#20102;86.6%&#30340;&#25191;&#34892;&#20934;&#30830;&#29575;&#12290;&#21516;&#26102;&#65292;&#24378;&#35843;&#20102;&#22312;&#25552;&#31034;&#24037;&#31243;&#20013;&#30340;&#35789;&#27719;&#25928;&#29575;&#20197;&#23454;&#29616;&#39640;&#25928;&#32463;&#27982;&#30340;LLM-based&#25991;&#26412;&#21040;SQL&#35299;&#20915;&#26041;&#26696;&#65292;&#27492;&#22806;&#36824;&#23545;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#24212;&#29992;&#24320;&#28304;LLMs&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#36827;&#34892;&#20102;&#20219;&#21153;&#29305;&#23450;&#30340;&#24615;&#33021;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#25104;&#20026;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#30340;&#19968;&#31181;&#26032;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#19968;&#20010;&#31995;&#32479;&#24615;&#30340;&#22522;&#20934;&#38459;&#30861;&#20102;&#35774;&#35745;&#26377;&#25928;&#12289;&#39640;&#25928;&#21644;&#32463;&#27982;&#30340;LLM-based&#25991;&#26412;&#21040;SQL&#35299;&#20915;&#26041;&#26696;&#30340;&#21457;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#39318;&#20808;&#23545;&#29616;&#26377;&#30340;&#25552;&#31034;&#24037;&#31243;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#24615;&#21644;&#24191;&#27867;&#30340;&#27604;&#36739;&#65292;&#21253;&#25324;&#38382;&#39064;&#34920;&#31034;&#12289;&#31034;&#20363;&#36873;&#25321;&#21644;&#31034;&#20363;&#32452;&#32455;&#65292;&#24182;&#26681;&#25454;&#23454;&#39564;&#32467;&#26524;&#35814;&#32454;&#38416;&#36848;&#20102;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#25104;&#35299;&#20915;&#26041;&#26696;&#65292;&#21517;&#20026;DAIL-SQL&#65292;&#21047;&#26032;&#20102;Spider&#27036;&#21333;&#65292;&#36798;&#21040;&#20102;86.6%&#30340;&#25191;&#34892;&#20934;&#30830;&#29575;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#26631;&#26438;&#12290;&#20026;&#20102;&#23454;&#29616;&#39640;&#25928;&#32463;&#27982;&#30340;LLM-based&#25991;&#26412;&#21040;SQL&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#24378;&#35843;&#25552;&#31034;&#24037;&#31243;&#20013;&#30340;&#35789;&#27719;&#25928;&#29575;&#65292;&#24182;&#22312;&#27492;&#24230;&#37327;&#19979;&#27604;&#36739;&#20102;&#20043;&#21069;&#30340;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#24320;&#28304;LLMs&#65292;&#24182;&#29992;&#20219;&#21153;&#29305;&#23450;&#30340;&#30417;&#30563;&#36827;&#34892;&#20102;&#36827;&#19968;&#27493;&#30340;&#24615;&#33021;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have emerged as a new paradigm for Text-to-SQL task. However, the absence of a systematical benchmark inhibits the development of designing effective, efficient and economic LLM-based Text-to-SQL solutions. To address this challenge, in this paper, we first conduct a systematical and extensive comparison over existing prompt engineering methods, including question representation, example selection and example organization, and with these experimental results, we elaborates their pros and cons. Based on these findings, we propose a new integrated solution, named DAIL-SQL, which refreshes the Spider leaderboard with 86.6% execution accuracy and sets a new bar. Towards an efficient and economic LLM-based Text-to-SQL solution, we emphasize the token efficiency in prompt engineering and compare the prior studies under this metric. Additionally, we investigate open-source LLMs in in-context learning, and further enhance their performance with task-specific superv
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#30740;&#31350;&#20102;&#20013;&#22269;&#21644;&#26085;&#26412;&#29616;&#20195;&#35821;&#35328;&#20351;&#29992;&#20013;&#30340;&#25991;&#21270;&#24046;&#24322;&#65292;&#21457;&#29616;&#21382;&#21490;&#19978;&#31291;&#31859;&#31181;&#26893;&#23545;&#20110;&#31038;&#20250;&#20114;&#21160;&#21644;&#24605;&#32500;&#26041;&#24335;&#30340;&#24433;&#21709;&#27604;&#32463;&#27982;&#21457;&#23637;&#21644;&#22478;&#24066;&#21270;&#26356;&#20026;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2308.15352</link><description>&lt;p&gt;
&#20013;&#22269;&#21644;&#26085;&#26412;&#29616;&#20195;&#35821;&#35328;&#20351;&#29992;&#30340;&#21382;&#21490;&#27169;&#24335;&#35299;&#37322;&#27604;&#29616;&#20195;&#21270;&#21644;&#22478;&#24066;&#21270;&#26356;&#20026;&#37325;&#35201;
&lt;/p&gt;
&lt;p&gt;
Historical patterns of rice farming explain modern-day language use in China and Japan more than modernization and urbanization. (arXiv:2308.15352v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15352
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#30740;&#31350;&#20102;&#20013;&#22269;&#21644;&#26085;&#26412;&#29616;&#20195;&#35821;&#35328;&#20351;&#29992;&#20013;&#30340;&#25991;&#21270;&#24046;&#24322;&#65292;&#21457;&#29616;&#21382;&#21490;&#19978;&#31291;&#31859;&#31181;&#26893;&#23545;&#20110;&#31038;&#20250;&#20114;&#21160;&#21644;&#24605;&#32500;&#26041;&#24335;&#30340;&#24433;&#21709;&#27604;&#32463;&#27982;&#21457;&#23637;&#21644;&#22478;&#24066;&#21270;&#26356;&#20026;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#20998;&#26512;&#20102;&#21313;&#20159;&#20010;&#35789;&#27719;&#65292;&#20197;&#30740;&#31350;&#24494;&#21338;&#36825;&#19968;&#20013;&#22269;&#26368;&#22823;&#30340;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#30340;&#25991;&#21270;&#24046;&#24322;&#12290;&#25105;&#20204;&#23558;&#20013;&#22269;&#25991;&#21270;&#24046;&#24322;&#30340;&#20004;&#31181;&#24120;&#35265;&#35299;&#37322;&#65288;&#32463;&#27982;&#21457;&#23637;&#21644;&#22478;&#20065;&#24046;&#24322;&#65289;&#19982;&#31291;&#31859;&#19982;&#23567;&#40614;&#31181;&#26893;&#30340;&#21382;&#21490;&#36951;&#30041;&#24433;&#21709;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#31291;&#20892;&#38656;&#35201;&#21327;&#35843;&#20849;&#20139;&#30340;&#28748;&#28297;&#32593;&#32476;&#24182;&#20132;&#25442;&#21171;&#21160;&#21147;&#65292;&#20197;&#24212;&#23545;&#36739;&#39640;&#30340;&#21171;&#21160;&#21147;&#38656;&#27714;&#12290;&#30456;&#21453;&#65292;&#23567;&#40614;&#20381;&#36182;&#38477;&#38632;&#65292;&#25152;&#38656;&#21171;&#21160;&#21147;&#21482;&#26377;&#31291;&#31859;&#30340;&#19968;&#21322;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#36825;&#31181;&#21382;&#21490;&#36951;&#30041;&#26159;&#21542;&#20351;&#24471;&#20013;&#22269;&#21335;&#26041;&#26356;&#21152;&#30456;&#20114;&#20381;&#36182;&#12290;&#22312;&#25152;&#26377;&#35789;&#27719;&#31867;&#21035;&#20013;&#65292;&#31291;&#31859;&#35299;&#37322;&#30340;&#26041;&#24046;&#26159;&#32463;&#27982;&#21457;&#23637;&#21644;&#22478;&#24066;&#21270;&#30340;&#20004;&#20493;&#12290;&#31291;&#31859;&#31181;&#26893;&#21306;&#20351;&#29992;&#26356;&#22810;&#21453;&#26144;&#32039;&#23494;&#31038;&#20250;&#32852;&#31995;&#12289;&#25972;&#20307;&#24605;&#32500;&#21644;&#35880;&#24910;&#12289;&#39044;&#38450;&#23548;&#21521;&#30340;&#35789;&#27719;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#25512;&#29305;&#25968;&#25454;&#23545;&#26085;&#26412;&#30340;&#21508;&#20010;&#24030;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#32467;&#26524;&#22522;&#26412;&#19978;&#22797;&#21046;&#20102;&#20013;&#22269;&#30340;&#30740;&#31350;&#32467;&#26524;&#12290;&#36825;&#20026;&#31291;&#31859;&#29702;&#35770;&#25552;&#20379;&#20102;&#20851;&#38190;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We used natural language processing to analyze a billion words to study cultural differences on Weibo, one of China's largest social media platforms. We compared predictions from two common explanations about cultural differences in China (economic development and urban-rural differences) against the less-obvious legacy of rice versus wheat farming. Rice farmers had to coordinate shared irrigation networks and exchange labor to cope with higher labor requirements. In contrast, wheat relied on rainfall and required half as much labor. We test whether this legacy made southern China more interdependent. Across all word categories, rice explained twice as much variance as economic development and urbanization. Rice areas used more words reflecting tight social ties, holistic thought, and a cautious, prevention orientation. We then used Twitter data comparing prefectures in Japan, which largely replicated the results from China. This provides crucial evidence of the rice theory in a differ
&lt;/p&gt;</description></item><item><title>&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;AI&#30340;&#33258;&#21160;&#23398;&#29983;&#21453;&#39304;&#26694;&#26550;&#21487;&#20197;&#25552;&#20379;&#20016;&#23500;&#30340;&#21453;&#39304;&#65292;&#20294;&#24341;&#20837;&#20102;&#20262;&#29702;&#38382;&#39064;&#65292;&#24182;&#38656;&#35201;&#35299;&#20915;&#8220;&#22810;&#25968;&#20154;&#30340;&#26292;&#25919;&#8221;&#21644;&#24573;&#35270;&#38271;&#23614;&#20013;&#23569;&#25968;&#32676;&#20307;&#38656;&#27714;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.15334</link><description>&lt;p&gt;
&#19968;&#31181;&#36127;&#36131;&#20219;&#24320;&#21457;&#22522;&#20110;&#29983;&#25104;AI&#30340;&#33258;&#21160;&#23398;&#29983;&#21453;&#39304;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Framework for Responsible Development of Automated Student Feedback with Generative AI. (arXiv:2308.15334v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15334
&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;AI&#30340;&#33258;&#21160;&#23398;&#29983;&#21453;&#39304;&#26694;&#26550;&#21487;&#20197;&#25552;&#20379;&#20016;&#23500;&#30340;&#21453;&#39304;&#65292;&#20294;&#24341;&#20837;&#20102;&#20262;&#29702;&#38382;&#39064;&#65292;&#24182;&#38656;&#35201;&#35299;&#20915;&#8220;&#22810;&#25968;&#20154;&#30340;&#26292;&#25919;&#8221;&#21644;&#24573;&#35270;&#38271;&#23614;&#20013;&#23569;&#25968;&#32676;&#20307;&#38656;&#27714;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20379;&#20016;&#23500;&#30340;&#21453;&#39304;&#23545;&#20110;&#25903;&#25345;&#23398;&#29983;&#23398;&#20064;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#29983;&#25104;AI&#23588;&#20854;&#26159;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#65292;&#20026;&#21521;&#23398;&#29983;&#25552;&#20379;&#21487;&#37325;&#22797;&#12289;&#21487;&#25193;&#23637;&#21644;&#21363;&#26102;&#29983;&#25104;&#30340;&#33258;&#21160;&#21453;&#39304;&#25552;&#20379;&#20102;&#26426;&#20250;&#65292;&#20351;&#24471;&#20043;&#21069;&#31232;&#32570;&#19988;&#26114;&#36149;&#30340;&#23398;&#20064;&#36164;&#28304;&#21464;&#24471;&#20016;&#23500;&#36215;&#26469;&#12290;&#20174;&#25216;&#26415;&#35282;&#24230;&#32780;&#35328;&#65292;&#36825;&#31181;&#26041;&#27861;&#26159;&#21487;&#34892;&#30340;&#65292;&#24471;&#30410;&#20110;&#26368;&#36817;&#20154;&#24037;&#26234;&#33021;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#36827;&#27493;&#65307;&#28982;&#32780;&#65292;&#37319;&#29992;&#36825;&#20123;&#25216;&#26415;&#20063;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#28508;&#22312;&#30340;&#20262;&#29702;&#38382;&#39064;&#65292;&#38656;&#35201;&#35748;&#30495;&#32771;&#34385;&#12290;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21560;&#24341;&#21147;&#22312;&#20110;&#23427;&#20204;&#21487;&#20197;&#26377;&#25928;&#22320;&#33258;&#21160;&#21270;&#26368;&#20047;&#21619;&#30340;&#20219;&#21153;&#65307;&#20294;&#26159;&#36825;&#20063;&#21487;&#33021;&#23548;&#33268;&#8220;&#22810;&#25968;&#20154;&#30340;&#26292;&#25919;&#8221;&#65292;&#21363;&#24573;&#35270;&#20102;&#38271;&#23614;&#20013;&#23569;&#25968;&#32676;&#20307;&#30340;&#38656;&#27714;&#65292;&#22240;&#20026;&#36825;&#20123;&#38656;&#27714;&#24456;&#38590;&#33258;&#21160;&#21270;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#33021;&#22815;&#20135;&#29983;&#26377;&#20215;&#20540;&#21644;&#30495;&#23454;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Providing rich feedback to students is essential for supporting student learning. Recent advances in generative AI, particularly within large language modelling (LLM), provide the opportunity to deliver repeatable, scalable and instant automatically generated feedback to students, making abundant a previously scarce and expensive learning resource. Such an approach is feasible from a technical perspective due to these recent advances in Artificial Intelligence (AI) and Natural Language Processing (NLP); while the potential upside is a strong motivator, doing so introduces a range of potential ethical issues that must be considered as we apply these technologies. The attractiveness of AI systems is that they can effectively automate the most mundane tasks; but this risks introducing a "tyranny of the majority", where the needs of minorities in the long tail are overlooked because they are difficult to automate.  Developing machine learning models that can generate valuable and authentic
&lt;/p&gt;</description></item><item><title>TaskLAMA&#35770;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#23545;&#22797;&#26434;&#20219;&#21153;&#29702;&#35299;&#30340;&#33021;&#21147;&#65292;&#20351;&#29992;&#20154;&#24037;&#26631;&#27880;&#25968;&#25454;&#38598;&#21644;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#23454;&#39564;&#35777;&#26126;LLMs&#33021;&#22815;&#26377;&#25928;&#22320;&#23558;&#22797;&#26434;&#20219;&#21153;&#20998;&#35299;&#20026;&#20010;&#21035;&#27493;&#39588;&#65292;&#24182;&#25552;&#20986;&#20102;&#36827;&#19968;&#27493;&#25552;&#21319;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.15299</link><description>&lt;p&gt;
TaskLAMA: &#25506;&#31350;&#35821;&#35328;&#27169;&#22411;&#23545;&#22797;&#26434;&#20219;&#21153;&#29702;&#35299;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
TaskLAMA: Probing the Complex Task Understanding of Language Models. (arXiv:2308.15299v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15299
&lt;/p&gt;
&lt;p&gt;
TaskLAMA&#35770;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#23545;&#22797;&#26434;&#20219;&#21153;&#29702;&#35299;&#30340;&#33021;&#21147;&#65292;&#20351;&#29992;&#20154;&#24037;&#26631;&#27880;&#25968;&#25454;&#38598;&#21644;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#23454;&#39564;&#35777;&#26126;LLMs&#33021;&#22815;&#26377;&#25928;&#22320;&#23558;&#22797;&#26434;&#20219;&#21153;&#20998;&#35299;&#20026;&#20010;&#21035;&#27493;&#39588;&#65292;&#24182;&#25552;&#20986;&#20102;&#36827;&#19968;&#27493;&#25552;&#21319;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#21270;&#22797;&#26434;&#20219;&#21153;&#20998;&#35299;&#65288;SCTD&#65289;&#26159;&#23558;&#22797;&#26434;&#30340;&#29616;&#23454;&#19990;&#30028;&#20219;&#21153;&#65288;&#22914;&#31574;&#21010;&#23130;&#31036;&#65289;&#20998;&#35299;&#20026;&#26377;&#21521;&#26080;&#29615;&#22270;&#65292;&#22270;&#20013;&#30340;&#27599;&#20010;&#27493;&#39588;&#37117;&#26159;&#20026;&#23436;&#25104;&#20219;&#21153;&#20570;&#20986;&#36129;&#29486;&#30340;&#65292;&#36793;&#34920;&#31034;&#23427;&#20204;&#20043;&#38388;&#30340;&#26102;&#24207;&#20381;&#36182;&#20851;&#31995;&#12290;SCTD&#26159;&#36741;&#21161;&#35268;&#21010;&#24037;&#20855;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#20063;&#26159;&#24120;&#35782;&#25512;&#29702;&#31995;&#32479;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#21462;&#30340;&#30693;&#35782;&#33021;&#22815;&#20934;&#30830;&#22320;&#36827;&#34892;SCTD&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#20026;&#36825;&#20010;&#38382;&#39064;&#24341;&#20837;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#20154;&#24037;&#26631;&#27880;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#26032;&#30340;&#25351;&#26631;&#26469;&#20844;&#24179;&#35780;&#20272;LLMs&#30456;&#23545;&#20110;&#20960;&#20010;&#22522;&#20934;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;LLMs&#33021;&#22815;&#26377;&#25928;&#22320;&#23558;&#22797;&#26434;&#20219;&#21153;&#20998;&#35299;&#20026;&#20010;&#21035;&#27493;&#39588;&#65292;&#30456;&#23545;&#20110;&#26368;&#20339;&#22522;&#20934;&#26041;&#27861;&#21487;&#20197;&#25552;&#21319;15%&#21040;280%&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20123;&#36827;&#19968;&#27493;&#25552;&#21319;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#30456;&#23545;&#20110;&#22522;&#32447;&#27169;&#22411;&#21487;&#20197;&#25552;&#21319;7%&#21040;37%&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#20173;&#28982;&#38754;&#20020;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structured Complex Task Decomposition (SCTD) is the problem of breaking down a complex real-world task (such as planning a wedding) into a directed acyclic graph over individual steps that contribute to achieving the task, with edges specifying temporal dependencies between them. SCTD is an important component of assistive planning tools, and a challenge for commonsense reasoning systems. We probe how accurately SCTD can be done with the knowledge extracted from Large Language Models (LLMs). We introduce a high-quality human-annotated dataset for this problem and novel metrics to fairly assess performance of LLMs against several baselines. Our experiments reveal that LLMs are able to decompose complex tasks into individual steps effectively, with a relative improvement of 15% to 280% over the best baseline. We also propose a number of approaches to further improve their performance, with a relative improvement of 7% to 37% over the base model. However, we find that LLMs still struggle 
&lt;/p&gt;</description></item><item><title>KGConv&#26159;&#19968;&#20010;&#22522;&#20110;Wikidata&#30340;&#22823;&#22411;&#23545;&#35805;&#35821;&#26009;&#24211;&#65292;&#27599;&#20010;&#23545;&#35805;&#37117;&#22522;&#20110;&#19968;&#20010;&#20107;&#23454;&#65292;&#24182;&#25552;&#20379;&#20102;&#22810;&#20010;&#21464;&#20307;&#30340;&#38382;&#39064;&#12290;&#23427;&#21487;&#20197;&#29992;&#20110;&#30693;&#35782;&#23545;&#35805;&#38382;&#39064;&#29983;&#25104;&#21644;&#20854;&#20182;&#30456;&#20851;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.15298</link><description>&lt;p&gt;
KGConv&#65292;&#22522;&#20110;Wikidata&#30340;&#23545;&#35805;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
KGConv, a Conversational Corpus grounded in Wikidata. (arXiv:2308.15298v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15298
&lt;/p&gt;
&lt;p&gt;
KGConv&#26159;&#19968;&#20010;&#22522;&#20110;Wikidata&#30340;&#22823;&#22411;&#23545;&#35805;&#35821;&#26009;&#24211;&#65292;&#27599;&#20010;&#23545;&#35805;&#37117;&#22522;&#20110;&#19968;&#20010;&#20107;&#23454;&#65292;&#24182;&#25552;&#20379;&#20102;&#22810;&#20010;&#21464;&#20307;&#30340;&#38382;&#39064;&#12290;&#23427;&#21487;&#20197;&#29992;&#20110;&#30693;&#35782;&#23545;&#35805;&#38382;&#39064;&#29983;&#25104;&#21644;&#20854;&#20182;&#30456;&#20851;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;KGConv&#65292;&#19968;&#20010;&#21253;&#21547;71k&#20010;&#23545;&#35805;&#30340;&#22823;&#22411;&#23545;&#35805;&#35821;&#26009;&#24211;&#65292;&#27599;&#20010;&#38382;&#39064;-&#22238;&#31572;&#23545;&#37117;&#22522;&#20110;Wikidata&#20013;&#30340;&#19968;&#20010;&#20107;&#23454;&#12290;&#27599;&#20010;&#23545;&#35805;&#24179;&#22343;&#21547;&#26377;8.6&#20010;&#38382;&#39064;&#65292;&#24182;&#20026;&#27599;&#20010;Wikidata&#20107;&#23454;&#25552;&#20379;&#22810;&#20010;&#21464;&#20307;(&#24179;&#22343;12&#20010;)&#65292;&#36825;&#20123;&#21464;&#20307;&#20351;&#29992;&#27169;&#26495;&#12289;&#20154;&#24037;&#27880;&#37322;&#12289;&#25163;&#24037;&#35268;&#21017;&#21644;&#38382;&#39064;&#37325;&#20889;&#31070;&#32463;&#27169;&#22411;&#29983;&#25104;&#12290;&#25105;&#20204;&#20026;&#22522;&#20110;&#30693;&#35782;&#30340;&#23545;&#35805;&#38382;&#39064;&#29983;&#25104;&#20219;&#21153;&#25552;&#20379;&#20102;&#22522;&#32447;&#12290;KGConv&#36824;&#21487;&#29992;&#20110;&#20854;&#20182;&#29983;&#25104;&#21644;&#20998;&#26512;&#20219;&#21153;&#65292;&#20363;&#22914;&#20174;Wikidata&#19977;&#20803;&#32452;&#29983;&#25104;&#21333;&#36718;&#38382;&#39064;&#12289;&#38382;&#39064;&#37325;&#20889;&#12289;&#20174;&#23545;&#35805;&#25110;&#30693;&#35782;&#22270;&#20013;&#22238;&#31572;&#38382;&#39064;&#20197;&#21450;&#29983;&#25104;&#27979;&#39564;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present KGConv, a large, conversational corpus of 71k conversations where each question-answer pair is grounded in a Wikidata fact. Conversations contain on average 8.6 questions and for each Wikidata fact, we provide multiple variants (12 on average) of the corresponding question using templates, human annotations, hand-crafted rules and a question rewriting neural model. We provide baselines for the task of Knowledge-Based, Conversational Question Generation. KGConv can further be used for other generation and analysis tasks such as single-turn question generation from Wikidata triples, question rewriting, question answering from conversation or from knowledge graphs and quiz generation.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21518;&#22788;&#29702;OCR&#27169;&#22411;&#65292;&#37319;&#29992;&#23383;&#24418;&#23884;&#20837;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;OCR&#24615;&#33021;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#21518;&#22788;&#29702;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#21155;&#36136;OCR&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#23383;&#24418;&#23884;&#20837;&#21487;&#20197;&#25552;&#39640;&#26657;&#27491;&#32467;&#26524;&#65292;&#21253;&#25324;&#32416;&#27491;&#20010;&#21035;&#21333;&#35789;&#12290;</title><link>http://arxiv.org/abs/2308.15262</link><description>&lt;p&gt;
&#36890;&#36807;&#21518;&#22788;&#29702;OCR&#27169;&#22411;&#25552;&#39640;OCR&#24615;&#33021;&#65306;&#37319;&#29992;&#23383;&#24418;&#23884;&#20837;&#26469;&#25913;&#36827;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
Enhancing OCR Performance through Post-OCR Models: Adopting Glyph Embedding for Improved Correction. (arXiv:2308.15262v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15262
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21518;&#22788;&#29702;OCR&#27169;&#22411;&#65292;&#37319;&#29992;&#23383;&#24418;&#23884;&#20837;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;OCR&#24615;&#33021;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#21518;&#22788;&#29702;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#21155;&#36136;OCR&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#23383;&#24418;&#23884;&#20837;&#21487;&#20197;&#25552;&#39640;&#26657;&#27491;&#32467;&#26524;&#65292;&#21253;&#25324;&#32416;&#27491;&#20010;&#21035;&#21333;&#35789;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21518;&#22788;&#29702;OCR&#27169;&#22411;&#20811;&#26381;OCR&#27169;&#22411;&#38480;&#21046;&#30340;&#28508;&#21147;&#65292;&#24182;&#30740;&#31350;&#20102;&#23558;&#23383;&#24418;&#23884;&#20837;&#32467;&#21512;&#21040;&#21518;&#22788;&#29702;OCR&#26657;&#27491;&#20013;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#33258;&#24049;&#30340;&#21518;&#22788;&#29702;OCR&#26657;&#27491;&#27169;&#22411;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#20351;&#29992;CharBERT&#21644;&#25105;&#20204;&#29420;&#29305;&#30340;&#23884;&#20837;&#25216;&#26415;&#26469;&#23884;&#20837;OCR&#36755;&#20986;&#65292;&#24182;&#25429;&#25417;&#23383;&#31526;&#30340;&#35270;&#35273;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21518;&#22788;&#29702;OCR&#26657;&#27491;&#33021;&#26377;&#25928;&#35299;&#20915;&#21155;&#36136;OCR&#27169;&#22411;&#30340;&#32570;&#38519;&#65292;&#32780;&#23383;&#24418;&#23884;&#20837;&#20351;&#27169;&#22411;&#33021;&#22815;&#21462;&#24471;&#21331;&#36234;&#30340;&#32467;&#26524;&#65292;&#21253;&#25324;&#32416;&#27491;&#20010;&#21035;&#21333;&#35789;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study investigates the potential of post-OCR models to overcome limitations in OCR models and explores the impact of incorporating glyph embedding on post-OCR correction performance. In this study, we have developed our own post-OCR correction model. The novelty of our approach lies in embedding the OCR output using CharBERT and our unique embedding technique, capturing the visual characteristics of characters. Our findings show that post-OCR correction effectively addresses deficiencies in inferior OCR models, and glyph embedding enables the model to achieve superior results, including the ability to correct individual words.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#31867;&#24341;&#23548;&#30340;&#23545;&#25239;&#25915;&#20987;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#21464;&#25972;&#20307;&#24847;&#20041;&#29983;&#25104;&#20445;&#25345;&#35821;&#20041;&#30340;&#23545;&#25239;&#26679;&#26412;&#65292;&#20174;&#32780;&#20351;&#24471;&#32763;&#35793;&#32467;&#26524;&#23646;&#20110;&#19981;&#21516;&#30340;&#31867;&#21035;&#12290;</title><link>http://arxiv.org/abs/2308.15246</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#20998;&#31867;&#24341;&#23548;&#30340;&#23545;&#25239;&#25915;&#20987;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Classification-Guided Approach for Adversarial Attacks against Neural Machine Translation. (arXiv:2308.15246v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#31867;&#24341;&#23548;&#30340;&#23545;&#25239;&#25915;&#20987;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#21464;&#25972;&#20307;&#24847;&#20041;&#29983;&#25104;&#20445;&#25345;&#35821;&#20041;&#30340;&#23545;&#25239;&#26679;&#26412;&#65292;&#20174;&#32780;&#20351;&#24471;&#32763;&#35793;&#32467;&#26524;&#23646;&#20110;&#19981;&#21516;&#30340;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#27169;&#22411;&#24050;&#32463;&#34987;&#35777;&#26126;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#36755;&#20837;&#25200;&#21160;&#26469;&#35823;&#23548;&#30446;&#26631;&#27169;&#22411;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ACT&#30340;&#26032;&#22411;&#23545;&#25239;&#25915;&#20987;&#26694;&#26550;&#65292;&#38024;&#23545;NMT&#31995;&#32479;&#36827;&#34892;&#25915;&#20987;&#65292;&#25915;&#20987;&#36807;&#31243;&#20013;&#24341;&#23548;&#20102;&#19968;&#20010;&#20998;&#31867;&#22120;&#12290;&#22312;&#25105;&#20204;&#30340;&#25915;&#20987;&#20013;&#65292;&#25915;&#20987;&#32773;&#26088;&#22312;&#29983;&#25104;&#20445;&#25345;&#35821;&#20041;&#30340;&#23545;&#25239;&#26679;&#26412;&#65292;&#20351;&#24471;NMT&#27169;&#22411;&#30340;&#32763;&#35793;&#32467;&#26524;&#19982;&#30446;&#26631;&#35821;&#35328;&#20013;&#30340;&#21407;&#22987;&#32763;&#35793;&#23646;&#20110;&#19981;&#21516;&#30340;&#31867;&#21035;&#12290;&#19982;&#20043;&#21069;&#30340;&#25915;&#20987;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#26356;&#33021;&#25913;&#21464;&#25972;&#20307;&#24847;&#20041;&#65292;&#20174;&#32780;&#36890;&#36807;&#20998;&#31867;&#22120;&#23558;&#20854;&#24402;&#20026;&#19981;&#21516;&#30340;&#31867;&#21035;&#12290;&#20026;&#20102;&#35780;&#20272;NMT&#27169;&#22411;&#23545;&#35813;&#25915;&#20987;&#30340;&#25269;&#25239;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#29616;&#26377;&#22522;&#20110;&#21333;&#35789;&#26367;&#25442;&#30340;&#40657;&#30418;&#25915;&#20987;&#36827;&#34892;&#25913;&#36827;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#25915;&#20987;&#36807;&#31243;&#20013;&#24341;&#20837;&#30446;&#26631;NMT&#27169;&#22411;&#30340;&#36755;&#20986;&#32763;&#35793;&#21644;&#19968;&#20010;&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;logit&#12290;&#36890;&#36807;&#22312;&#21508;&#31181;&#35774;&#32622;&#19979;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Machine Translation (NMT) models have been shown to be vulnerable to adversarial attacks, wherein carefully crafted perturbations of the input can mislead the target model. In this paper, we introduce ACT, a novel adversarial attack framework against NMT systems guided by a classifier. In our attack, the adversary aims to craft meaning-preserving adversarial examples whose translations by the NMT model belong to a different class than the original translations in the target language. Unlike previous attacks, our new approach has a more substantial effect on the translation by altering the overall meaning, which leads to a different class determined by a classifier. To evaluate the robustness of NMT models to this attack, we propose enhancements to existing black-box word-replacement-based attacks by incorporating output translations of the target NMT model and the output logits of a classifier within the attack process. Extensive experiments in various settings, including a comp
&lt;/p&gt;</description></item><item><title>PronounFlow&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#26469;&#26657;&#20934;&#21477;&#23376;&#20013;&#30340;&#20195;&#35789;&#65292;&#20197;&#28040;&#38500;&#27495;&#20041;&#12290;&#36825;&#23545;&#20110;&#20351;&#26426;&#22120;&#20855;&#22791;&#24120;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2308.15235</link><description>&lt;p&gt;
PronounFlow:&#19968;&#31181;&#29992;&#20110;&#26657;&#20934;&#21477;&#23376;&#20013;&#20195;&#35789;&#30340;&#28151;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PronounFlow: A Hybrid Approach for Calibrating Pronouns in Sentences. (arXiv:2308.15235v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15235
&lt;/p&gt;
&lt;p&gt;
PronounFlow&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#26469;&#26657;&#20934;&#21477;&#23376;&#20013;&#30340;&#20195;&#35789;&#65292;&#20197;&#28040;&#38500;&#27495;&#20041;&#12290;&#36825;&#23545;&#20110;&#20351;&#26426;&#22120;&#20855;&#22791;&#24120;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32763;&#38405;&#20219;&#20309;&#19968;&#26412;&#20070;&#25110;&#21548;&#20219;&#20309;&#19968;&#39318;&#27468;&#35789;&#65292;&#20320;&#20250;&#36935;&#21040;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#20250;&#38459;&#30861;&#29702;&#35299;&#30340;&#20195;&#35789;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#26426;&#22120;&#26469;&#35828;&#12290;&#38543;&#30528;&#35748;&#30693;&#26426;&#22120;&#22312;&#25105;&#20204;&#29983;&#27963;&#20013;&#30340;&#26222;&#21450;&#65292;&#35768;&#22810;&#31995;&#32479;&#24050;&#32463;&#34987;&#24320;&#21457;&#20986;&#26469;&#20197;&#35299;&#20915;&#21508;&#31181;&#25361;&#25112;&#19979;&#30340;&#20195;&#35789;&#27495;&#20041;&#12290;&#22240;&#27492;&#65292;&#20154;&#20204;&#35748;&#20026;&#33021;&#22815;&#28040;&#38500;&#21477;&#23376;&#20013;&#30340;&#20195;&#35789;&#27495;&#20041;&#30340;&#31995;&#32479;&#23558;&#26377;&#21161;&#20110;&#20351;&#26426;&#22120;&#20855;&#22791;&#19982;&#20154;&#31867;&#30456;&#20284;&#30340;&#24120;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#22312;&#29616;&#20195;&#33521;&#35821;&#20013;&#38754;&#20020;&#30340;&#19968;&#20010;&#38382;&#39064;&#26159;&#32570;&#20047;&#24615;&#21035;&#20195;&#35789;&#65292;&#20154;&#20204;&#35797;&#22270;&#36890;&#36807;&#20351;&#29992;&#30007;&#24615;&#12289;&#22899;&#24615;&#25110;&#22797;&#25968;&#26469;&#36991;&#20813;&#25972;&#20010;&#38382;&#39064;&#30340;&#20986;&#29616;&#12290;&#30001;&#20110;&#20154;&#31867;&#30340;&#30446;&#26631;&#26159;&#26500;&#24314;&#20986;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#23436;&#25972;&#24847;&#20041;&#19978;&#30340;&#31995;&#32479;&#65292;&#37027;&#20040;&#24403;&#20070;&#38754;&#25991;&#26412;&#20013;&#30340;&#20195;&#35789;(&#22914;&#22797;&#25968;&#25110;&#20013;&#24615;&#20195;&#35789;)&#25351;&#30340;&#26159;&#24615;&#21035;&#19981;&#19968;&#23450;&#24050;&#30693;&#30340;&#26410;&#25351;&#23450;&#23454;&#20307;&#26102;&#20250;&#21457;&#29983;&#20160;&#20040;&#21602;&#65311;
&lt;/p&gt;
&lt;p&gt;
Flip through any book or listen to any song lyrics, and you will come across pronouns that, in certain cases, can hinder meaning comprehension, especially for machines. As the role of having cognitive machines becomes pervasive in our lives, numerous systems have been developed to resolve pronouns under various challenges. Commensurate with this, it is believed that having systems able to disambiguate pronouns in sentences will help towards the endowment of machines with commonsense and reasoning abilities like those found in humans. However, one problem these systems face with modern English is the lack of gender pronouns, where people try to alternate by using masculine, feminine, or plural to avoid the whole issue. Since humanity aims to the building of systems in the full-bodied sense we usually reserve for people, what happens when pronouns in written text, like plural or epicene ones, refer to unspecified entities whose gender is not necessarily known? Wouldn't that put extra bar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#21487;&#35299;&#37322;&#24615;&#20998;&#26512;&#30340;&#20998;&#31867;&#24863;&#30693;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65292;&#29992;&#20110;&#20914;&#31361;&#20998;&#31867;&#21644;&#20027;&#39064;&#21457;&#29616;&#12290;&#35813;&#27169;&#22411;&#25552;&#20379;&#20102;&#21487;&#38752;&#30340;&#20998;&#31867;&#32467;&#26524;&#21644;&#21457;&#29616;&#30340;&#20027;&#39064;&#30340;&#35299;&#37322;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#27169;&#22411;&#30340;&#22797;&#26434;&#24230;&#26469;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.15232</link><description>&lt;p&gt;
&#32467;&#21512;&#21487;&#35299;&#37322;&#24615;&#20998;&#26512;&#30340;&#20998;&#31867;&#24863;&#30693;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#8212;&#8212;&#29992;&#20110;&#20914;&#31361;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classification-Aware Neural Topic Model Combined With Interpretable Analysis -- For Conflict Classification. (arXiv:2308.15232v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#21487;&#35299;&#37322;&#24615;&#20998;&#26512;&#30340;&#20998;&#31867;&#24863;&#30693;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65292;&#29992;&#20110;&#20914;&#31361;&#20998;&#31867;&#21644;&#20027;&#39064;&#21457;&#29616;&#12290;&#35813;&#27169;&#22411;&#25552;&#20379;&#20102;&#21487;&#38752;&#30340;&#20998;&#31867;&#32467;&#26524;&#21644;&#21457;&#29616;&#30340;&#20027;&#39064;&#30340;&#35299;&#37322;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#27169;&#22411;&#30340;&#22797;&#26434;&#24230;&#26469;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19990;&#30028;&#19978;&#26377;&#22823;&#37327;&#30340;&#20914;&#31361;&#20107;&#20214;&#19968;&#30452;&#22312;&#24433;&#21709;&#30528;&#25105;&#20204;&#12290;&#20026;&#20102;&#26377;&#25928;&#20998;&#26512;&#36825;&#20123;&#20914;&#31361;&#20107;&#20214;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20914;&#31361;&#20449;&#24687;&#20998;&#31867;&#21644;&#20027;&#39064;&#21457;&#29616;&#30340;&#20998;&#31867;&#24863;&#30693;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65288;CANTM-IA&#65289;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;&#21487;&#35299;&#37322;&#24615;&#20998;&#26512;&#26469;&#25552;&#20379;&#21487;&#38752;&#30340;&#20998;&#31867;&#32467;&#26524;&#21644;&#21457;&#29616;&#30340;&#20027;&#39064;&#30340;&#35299;&#37322;&#12290;&#21516;&#26102;&#65292;&#23558;&#35299;&#37322;&#24615;&#24341;&#20837;&#27169;&#22411;&#26550;&#26500;&#20013;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#24182;&#20351;&#35299;&#37322;&#36827;&#19968;&#27493;&#20851;&#27880;&#25968;&#25454;&#30340;&#32454;&#33410;&#12290;&#26368;&#21518;&#65292;&#23545;&#27169;&#22411;&#26550;&#26500;&#36827;&#34892;&#20248;&#21270;&#65292;&#20197;&#38477;&#20302;&#27169;&#22411;&#30340;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
A large number of conflict events are affecting the world all the time. In order to analyse such conflict events effectively, this paper presents a Classification-Aware Neural Topic Model (CANTM-IA) for Conflict Information Classification and Topic Discovery. The model provides a reliable interpretation of classification results and discovered topics by introducing interpretability analysis. At the same time, interpretation is introduced into the model architecture to improve the classification performance of the model and to allow interpretation to focus further on the details of the data. Finally, the model architecture is optimised to reduce the complexity of the model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25429;&#25417;&#20219;&#21153;&#23548;&#21521;&#30340;&#22810;&#26041;&#20250;&#35805;&#65288;MPCs&#65289;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#27604;&#36739;&#20102;&#19977;&#31181;&#26041;&#27861;&#65288;&#39044;&#35757;&#32451;&#12289;&#24494;&#35843;&#21644;&#25552;&#31034;&#24037;&#31243;&#65289;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#26377;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;GPT-3.5-turbo&#36890;&#36807;"&#25512;&#29702;"&#39118;&#26684;&#30340;&#25552;&#31034;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#19979;&#34920;&#29616;&#26368;&#22909;&#12290;</title><link>http://arxiv.org/abs/2308.15231</link><description>&lt;p&gt;
&#20351;&#29992;LLMs&#36827;&#34892;&#22810;&#26041;&#30446;&#26631;&#36861;&#36394;&#65306;&#27604;&#36739;&#39044;&#35757;&#32451;&#12289;&#24494;&#35843;&#21644;&#25552;&#31034;&#24037;&#31243;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multi-party Goal Tracking with LLMs: Comparing Pre-training, Fine-tuning, and Prompt Engineering. (arXiv:2308.15231v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15231
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25429;&#25417;&#20219;&#21153;&#23548;&#21521;&#30340;&#22810;&#26041;&#20250;&#35805;&#65288;MPCs&#65289;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#27604;&#36739;&#20102;&#19977;&#31181;&#26041;&#27861;&#65288;&#39044;&#35757;&#32451;&#12289;&#24494;&#35843;&#21644;&#25552;&#31034;&#24037;&#31243;&#65289;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#26377;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;GPT-3.5-turbo&#36890;&#36807;"&#25512;&#29702;"&#39118;&#26684;&#30340;&#25552;&#31034;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#19979;&#34920;&#29616;&#26368;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#24403;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#21542;&#25429;&#25417;&#38754;&#21521;&#20219;&#21153;&#30340;&#22810;&#26041;&#20250;&#35805;&#65288;MPCs&#65289;&#12290;&#25105;&#20204;&#35760;&#24405;&#24182;&#36716;&#24405;&#20102;29&#20010;&#22312;&#21307;&#38498;&#20013;&#24739;&#32773;&#12289;&#20182;&#20204;&#30340;&#20276;&#20387;&#21644;&#31038;&#20132;&#26426;&#22120;&#20154;&#20043;&#38388;&#30340;MPCs&#12290;&#28982;&#21518;&#25105;&#20204;&#23545;&#36825;&#20010;&#35821;&#26009;&#24211;&#36827;&#34892;&#20102;&#22810;&#26041;&#30446;&#26631;&#36861;&#36394;&#21644;&#24847;&#22270;&#27133;&#35782;&#21035;&#30340;&#27880;&#37322;&#12290;&#22312;MPCs&#20013;&#65292;&#20154;&#20204;&#20998;&#20139;&#30446;&#26631;&#65292;&#22238;&#31572;&#23545;&#26041;&#30340;&#30446;&#26631;&#65292;&#24182;&#25552;&#20379;&#20854;&#20182;&#20154;&#30340;&#30446;&#26631; - &#36825;&#20123;&#37117;&#19981;&#20250;&#22312;&#20108;&#20803;&#20132;&#20114;&#20013;&#21457;&#29983;&#12290;&#20026;&#20102;&#29702;&#35299;MPCs&#20013;&#29992;&#25143;&#30340;&#30446;&#26631;&#65292;&#25105;&#20204;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#35774;&#32622;&#19979;&#27604;&#36739;&#20102;&#19977;&#31181;&#26041;&#27861;&#65306;&#25105;&#20204;&#23545;T5&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#20351;&#29992;LED&#21019;&#24314;&#20102;&#39044;&#35757;&#32451;&#20219;&#21153;&#26469;&#35757;&#32451;DialogLM&#65292;&#24182;&#20351;&#29992;GPT-3.5-turbo&#37319;&#29992;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#65292;&#20197;&#30830;&#23450;&#21738;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#25968;&#25454;&#19979;&#23436;&#25104;&#36825;&#20010;&#26032;&#39062;&#30340;&#20219;&#21153;&#12290;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#19979;&#65292;GPT-3.5-turbo&#30340;&#24615;&#33021;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;&#24403;&#32473;&#23450;7%&#30340;&#35821;&#26009;&#24211;&#20316;&#20026;&#31034;&#20363;&#26631;&#27880;&#23545;&#35805;&#26102;&#65292;"&#25512;&#29702;"&#39118;&#26684;&#30340;&#25552;&#31034;&#26159;&#34920;&#29616;&#26368;&#22909;&#30340;&#26041;&#27861;&#12290;&#23427;&#33021;&#27491;&#30830;&#27880;&#37322;...
&lt;/p&gt;
&lt;p&gt;
This paper evaluates the extent to which current Large Language Models (LLMs) can capture task-oriented multi-party conversations (MPCs). We have recorded and transcribed 29 MPCs between patients, their companions, and a social robot in a hospital. We then annotated this corpus for multi-party goal-tracking and intent-slot recognition. People share goals, answer each other's goals, and provide other people's goals in MPCs - none of which occur in dyadic interactions. To understand user goals in MPCs, we compared three methods in zero-shot and few-shot settings: we fine-tuned T5, created pre-training tasks to train DialogLM using LED, and employed prompt engineering techniques with GPT-3.5-turbo, to determine which approach can complete this novel task with limited data. GPT-3.5-turbo significantly outperformed the others in a few-shot setting. The `reasoning' style prompt, when given 7% of the corpus as example annotated conversations, was the best performing method. It correctly annot
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;CLIPTrans&#65292;&#23427;&#36890;&#36807;&#31616;&#21333;&#22320;&#36866;&#24212;&#29420;&#31435;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#26426;&#22120;&#32763;&#35793;&#20013;&#35270;&#35273;&#30693;&#35782;&#30340;&#36716;&#31227;&#12290;</title><link>http://arxiv.org/abs/2308.15226</link><description>&lt;p&gt;
CLIPTrans&#65306;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36716;&#31227;&#35270;&#35273;&#30693;&#35782;&#36827;&#34892;&#22810;&#27169;&#24577;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
CLIPTrans: Transferring Visual Knowledge with Pre-trained Models for Multimodal Machine Translation. (arXiv:2308.15226v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15226
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;CLIPTrans&#65292;&#23427;&#36890;&#36807;&#31616;&#21333;&#22320;&#36866;&#24212;&#29420;&#31435;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#26426;&#22120;&#32763;&#35793;&#20013;&#35270;&#35273;&#30693;&#35782;&#30340;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24320;&#21457;&#22686;&#24378;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#30340;&#22810;&#27169;&#24577;&#26426;&#22120;&#32763;&#35793;&#65288;MMT&#65289;&#31995;&#32479;&#20197;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#30340;&#20852;&#36259;&#36880;&#28176;&#22686;&#38271;&#12290;&#36825;&#19968;&#38382;&#39064;&#35774;&#32622;&#28041;&#21450;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#22270;&#20687;&#20316;&#20026;&#36741;&#21161;&#20449;&#24687;&#65292;&#24182;&#19988;&#26368;&#36817;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#28040;&#38500;&#23427;&#20204;&#30340;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#30340;&#24037;&#20316;&#22312;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#24378;&#22823;&#30340;MMT&#27169;&#22411;&#26102;&#38754;&#20020;&#20102;&#19968;&#20010;&#25361;&#25112;&#65292;&#22240;&#20026;&#22810;&#35821;&#35328;&#35270;&#35273;&#35821;&#35328;&#25968;&#25454;&#30340;&#26631;&#27880;&#31232;&#32570;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#38024;&#23545;NMT&#30340;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#38024;&#23545;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#27169;&#22411;&#22823;&#37327;&#28044;&#29616;&#65292;&#20027;&#35201;&#38024;&#23545;&#33521;&#25991;&#65292;&#23427;&#20204;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;MMT&#24182;&#19981;&#30452;&#25509;&#36866;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#27809;&#26377;&#20026;&#29983;&#25104;&#20219;&#21153;&#25552;&#20379;&#23545;&#40784;&#30340;&#22810;&#27169;&#24577;&#22810;&#35821;&#35328;&#29305;&#24449;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CLIPTrans&#65292;&#23427;&#19981;&#20687;&#35774;&#35745;&#22797;&#26434;&#30340;MMT&#27169;&#22359;&#65292;&#32780;&#26159;&#31616;&#21333;&#22320;&#36866;&#24212;&#29420;&#31435;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;
&lt;/p&gt;
&lt;p&gt;
There has been a growing interest in developing multimodal machine translation (MMT) systems that enhance neural machine translation (NMT) with visual knowledge. This problem setup involves using images as auxiliary information during training, and more recently, eliminating their use during inference. Towards this end, previous works face a challenge in training powerful MMT models from scratch due to the scarcity of annotated multilingual vision-language data, especially for low-resource languages. Simultaneously, there has been an influx of multilingual pre-trained models for NMT and multimodal pre-trained models for vision-language tasks, primarily in English, which have shown exceptional generalisation ability. However, these are not directly applicable to MMT since they do not provide aligned multimodal multilingual features for generative tasks. To alleviate this issue, instead of designing complex modules for MMT, we propose CLIPTrans, which simply adapts the independently pre-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#23545;&#35805;&#31995;&#32479;&#65292;&#23558;&#24320;&#25918;&#21644;&#23553;&#38381;&#39046;&#22495;&#23545;&#35805;&#12289;&#33080;&#37096;&#34920;&#24773;&#32467;&#21512;&#36215;&#26469;&#65292;&#36890;&#36807;&#20351;&#29992;LLMs&#21644;GPT-3.5&#27169;&#22411;&#26469;&#29983;&#25104;&#24341;&#20154;&#20837;&#32988;&#30340;&#23545;&#35805;&#65292;&#20197;&#25552;&#20379;&#20449;&#24687;&#24182;&#19982;&#35775;&#23458;&#36827;&#34892;&#33258;&#28982;&#20132;&#27969;&#12290;</title><link>http://arxiv.org/abs/2308.15214</link><description>&lt;p&gt;
FurChat: &#20351;&#29992;LLMs&#30340;&#20855;&#26377;&#33080;&#37096;&#34920;&#24773;&#30340;&#20132;&#20114;&#24335;&#23545;&#35805;&#31995;&#32479;&#65292;&#32467;&#21512;&#24320;&#25918;&#21644;&#23553;&#38381;&#39046;&#22495;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
FurChat: An Embodied Conversational Agent using LLMs, Combining Open and Closed-Domain Dialogue with Facial Expressions. (arXiv:2308.15214v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15214
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#23545;&#35805;&#31995;&#32479;&#65292;&#23558;&#24320;&#25918;&#21644;&#23553;&#38381;&#39046;&#22495;&#23545;&#35805;&#12289;&#33080;&#37096;&#34920;&#24773;&#32467;&#21512;&#36215;&#26469;&#65292;&#36890;&#36807;&#20351;&#29992;LLMs&#21644;GPT-3.5&#27169;&#22411;&#26469;&#29983;&#25104;&#24341;&#20154;&#20837;&#32988;&#30340;&#23545;&#35805;&#65292;&#20197;&#25552;&#20379;&#20449;&#24687;&#24182;&#19982;&#35775;&#23458;&#36827;&#34892;&#33258;&#28982;&#20132;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#23545;&#35805;&#31995;&#32479;&#65292;&#21487;&#20197;&#20316;&#20026;&#25509;&#24453;&#21592;&#65292;&#29983;&#25104;&#32467;&#21512;&#24320;&#25918;&#21644;&#23553;&#38381;&#39046;&#22495;&#23545;&#35805;&#20197;&#21450;&#33080;&#37096;&#34920;&#24773;&#30340;&#28151;&#21512;&#23545;&#35805;&#12290;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#24320;&#21457;&#24341;&#20154;&#20837;&#32988;&#30340;&#23545;&#35805;&#65292;&#25105;&#20204;&#23558;&#35813;&#31995;&#32479;&#37096;&#32626;&#21040;&#20102;&#19968;&#20010;&#39640;&#24230;&#34920;&#36798;&#21147;&#30340;Furhat&#26426;&#22120;&#20154;&#19978;&#65292;&#22312;&#20114;&#21160;&#36807;&#31243;&#20013;&#20351;&#29992;&#20102;&#21475;&#22836;&#21644;&#38750;&#35821;&#35328;&#25552;&#31034;&#12290;&#35813;&#31995;&#32479;&#19987;&#38376;&#20026;&#22269;&#23478;&#26426;&#22120;&#20154;&#23454;&#39564;&#23460;&#35774;&#35745;&#65292;&#36890;&#36807;&#33258;&#28982;&#23545;&#35805;&#19982;&#35775;&#23458;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#21521;&#20182;&#20204;&#25552;&#20379;&#26377;&#20851;&#35774;&#26045;&#12289;&#30740;&#31350;&#12289;&#26032;&#38395;&#12289;&#21363;&#23558;&#20030;&#34892;&#30340;&#27963;&#21160;&#31561;&#26041;&#38754;&#30340;&#20449;&#24687;&#12290;&#31995;&#32479;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;GPT-3.5&#27169;&#22411;&#26681;&#25454;&#25552;&#31034;&#29983;&#25104;&#36825;&#20123;&#20449;&#24687;&#65292;&#21516;&#26102;&#29983;&#25104;&#39046;&#22495;&#36890;&#29992;&#30340;&#23545;&#35805;&#21644;&#38754;&#37096;&#34920;&#24773;&#12290;
&lt;/p&gt;
&lt;p&gt;
We demonstrate an embodied conversational agent that can function as a receptionist and generate a mixture of open and closed-domain dialogue along with facial expressions, by using a large language model (LLM) to develop an engaging conversation. We deployed the system onto a Furhat robot, which is highly expressive and capable of using both verbal and nonverbal cues during interaction. The system was designed specifically for the National Robotarium to interact with visitors through natural conversations, providing them with information about the facilities, research, news, upcoming events, etc. The system utilises the state-of-the-art GPT-3.5 model to generate such information along with domain-general conversations and facial expressions based on prompt engineering.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#19977;&#31181;&#35821;&#35328;&#23545;&#30340;&#20116;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#23545;&#20849;&#20139;&#35789;&#27719;&#20316;&#20026;&#20195;&#30721;&#20999;&#25442;&#30340;&#35302;&#21457;&#22120;&#36827;&#34892;&#20102;&#28145;&#20837;&#25506;&#32034;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20849;&#20139;&#35789;&#27719;&#30830;&#23454;&#20250;&#35302;&#21457;&#20195;&#30721;&#20999;&#25442;&#65292;&#20999;&#25442;&#30340;&#20542;&#21521;&#21462;&#20915;&#20110;&#35302;&#21457;&#22120;&#19982;&#20999;&#25442;&#28857;&#30340;&#36317;&#31163;&#21644;&#20301;&#32622;&#65292;&#32780;&#19981;&#21462;&#20915;&#20110;&#35302;&#21457;&#35789;&#30340;&#35789;&#28304;&#12290;</title><link>http://arxiv.org/abs/2308.15209</link><description>&lt;p&gt;
&#20849;&#20139;&#35789;&#27719;&#20316;&#20026;&#20195;&#30721;&#20999;&#25442;&#30340;&#35302;&#21457;&#22120;
&lt;/p&gt;
&lt;p&gt;
Shared Lexical Items as Triggers of Code Switching. (arXiv:2308.15209v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15209
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#19977;&#31181;&#35821;&#35328;&#23545;&#30340;&#20116;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#23545;&#20849;&#20139;&#35789;&#27719;&#20316;&#20026;&#20195;&#30721;&#20999;&#25442;&#30340;&#35302;&#21457;&#22120;&#36827;&#34892;&#20102;&#28145;&#20837;&#25506;&#32034;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20849;&#20139;&#35789;&#27719;&#30830;&#23454;&#20250;&#35302;&#21457;&#20195;&#30721;&#20999;&#25442;&#65292;&#20999;&#25442;&#30340;&#20542;&#21521;&#21462;&#20915;&#20110;&#35302;&#21457;&#22120;&#19982;&#20999;&#25442;&#28857;&#30340;&#36317;&#31163;&#21644;&#20301;&#32622;&#65292;&#32780;&#19981;&#21462;&#20915;&#20110;&#35302;&#21457;&#35789;&#30340;&#35789;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20160;&#20040;&#21452;&#35821;&#32773;&#35201;&#36827;&#34892;&#20195;&#30721;&#20999;&#25442;&#65288;&#28151;&#21512;&#20004;&#31181;&#35821;&#35328;&#65289;&#65311;&#22312;&#35299;&#37322;&#36825;&#31181;&#33258;&#28982;&#21644;&#26222;&#36941;&#29616;&#35937;&#30340;&#20960;&#31181;&#29702;&#35770;&#20013;&#65292;&#35302;&#21457;&#20551;&#35828;&#23558;&#20195;&#30721;&#20999;&#25442;&#19982;&#20999;&#25442;&#28857;&#38468;&#36817;&#30340;&#35789;&#27719;&#35302;&#21457;&#22120;&#65288;&#29305;&#21035;&#26159;&#21516;&#28304;&#35789;&#21644;&#19987;&#26377;&#21517;&#35789;&#65289;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#22522;&#20110;&#19977;&#31181;&#35821;&#35328;&#23545;&#30340;&#20116;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#21453;&#26144;&#20102;&#21475;&#35821;&#21644;&#20070;&#38754;&#21452;&#35821;&#20132;&#27969;&#65292;&#23545;&#35302;&#21457;&#20551;&#35828;&#36827;&#34892;&#20102;&#26356;&#20840;&#38754;&#12289;&#26356;&#32454;&#33268;&#12289;&#26356;&#31934;&#32454;&#30340;&#25506;&#32034;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#34987;&#35748;&#20026;&#21516;&#26102;&#23384;&#22312;&#20110;&#20004;&#31181;&#35821;&#35328;&#30340;&#20849;&#20139;&#24515;&#29702;&#35789;&#27719;&#20013;&#30340;&#35789;&#30830;&#23454;&#35302;&#21457;&#20195;&#30721;&#20999;&#25442;&#65307;&#20999;&#25442;&#30340;&#20542;&#21521;&#21462;&#20915;&#20110;&#35302;&#21457;&#22120;&#19982;&#20999;&#25442;&#28857;&#30340;&#36317;&#31163;&#65307;&#20197;&#21450;&#35302;&#21457;&#22120;&#26159;&#22312;&#20999;&#25442;&#20043;&#21069;&#36824;&#26159;&#20043;&#21518;&#65307;&#20294;&#19981;&#21462;&#20915;&#20110;&#35302;&#21457;&#35789;&#30340;&#35789;&#28304;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23545;&#35789;&#27719;&#35302;&#21457;&#22120;&#19982;&#20195;&#30721;&#20999;&#25442;&#20043;&#38388;&#30340;&#20851;&#31995;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#12289;&#31283;&#23450;&#12289;&#20197;&#35777;&#25454;&#20026;&#22522;&#30784;&#30340;&#35777;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Why do bilingual speakers code-switch (mix their two languages)? Among the several theories that attempt to explain this natural and ubiquitous phenomenon, the Triggering Hypothesis relates code-switching to the presence of lexical triggers, specifically cognates and proper names, adjacent to the switch point. We provide a fuller, more nuanced and refined exploration of the triggering hypothesis, based on five large datasets in three language pairs, reflecting both spoken and written bilingual interactions. Our results show that words that are assumed to reside in a mental lexicon shared by both languages indeed trigger code-switching; that the tendency to switch depends on the distance of the trigger from the switch point; and on whether the trigger precedes or succeeds the switch; but not on the etymology of the trigger words. We thus provide strong, robust, evidence-based confirmation to several hypotheses on the relationships between lexical triggers and code-switching.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22522;&#20110;&#26032;&#39062;&#25968;&#25454;&#38598;&#21644;&#20808;&#36827;&#22522;&#20934;&#65292;&#26088;&#22312;&#35299;&#20915;&#32473;&#20986;&#22768;&#26126;&#30495;&#23454;&#24615;&#35299;&#37322;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20351;&#29992;&#38750;&#32467;&#26500;&#21270;&#30693;&#35782;&#30340;&#25688;&#35201;&#26041;&#27861;&#20013;&#65292;&#21629;&#39064;&#20449;&#24687;&#26159;&#26377;&#30410;&#30340;&#65292;&#24182;&#19988;&#26576;&#20123;&#25277;&#35937;&#31574;&#30053;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2308.15202</link><description>&lt;p&gt;
&#20107;&#23454;&#26680;&#26597;&#35299;&#37322;&#30340;&#22522;&#20934;&#21270;&#12290; (arXiv:2308.15202v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
Benchmarking the Generation of Fact Checking Explanations. (arXiv:2308.15202v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22522;&#20110;&#26032;&#39062;&#25968;&#25454;&#38598;&#21644;&#20808;&#36827;&#22522;&#20934;&#65292;&#26088;&#22312;&#35299;&#20915;&#32473;&#20986;&#22768;&#26126;&#30495;&#23454;&#24615;&#35299;&#37322;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20351;&#29992;&#38750;&#32467;&#26500;&#21270;&#30693;&#35782;&#30340;&#25688;&#35201;&#26041;&#27861;&#20013;&#65292;&#21629;&#39064;&#20449;&#24687;&#26159;&#26377;&#30410;&#30340;&#65292;&#24182;&#19988;&#26576;&#20123;&#25277;&#35937;&#31574;&#30053;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25171;&#20987;&#38169;&#35823;&#20449;&#24687;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#23613;&#31649;&#36234;&#26469;&#36234;&#22810;&#30340;&#19987;&#23478;&#21442;&#19982;&#25163;&#21160;&#20107;&#23454;&#26680;&#26597;&#65292;&#20294;&#36825;&#39033;&#27963;&#21160;&#32791;&#26102;&#19988;&#26080;&#27861;&#36319;&#24471;&#19978;&#27599;&#22825;&#20135;&#29983;&#30340;&#22823;&#37327;&#34394;&#20551;&#26032;&#38395;&#12290;&#22240;&#27492;&#65292;&#33258;&#21160;&#21270;&#27492;&#36807;&#31243;&#26159;&#24517;&#35201;&#30340;&#20197;&#24110;&#21161;&#36943;&#21046;&#38169;&#35823;&#20449;&#24687;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#30740;&#31350;&#32773;&#20027;&#35201;&#20851;&#27880;&#22768;&#26126;&#30495;&#23454;&#24230;&#20998;&#31867;&#12290;&#32780;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30528;&#30524;&#20110;&#35299;&#37322;&#29983;&#25104;&#65288;&#35299;&#37322;&#20026;&#20160;&#20040;&#22768;&#26126;&#34987;&#20998;&#31867;&#20026;&#30495;&#25110;&#20551;&#30340;&#25991;&#26412;&#35299;&#37322;&#65289;&#24182;&#19982;&#26032;&#39062;&#25968;&#25454;&#38598;&#21644;&#20808;&#36827;&#22522;&#20934;&#36827;&#34892;&#20102;&#22522;&#20934;&#21270;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#38750;&#32467;&#26500;&#21270;&#30693;&#35782;&#65288;&#21363;&#26032;&#38395;&#25991;&#31456;&#65289;&#19978;&#30340;&#25688;&#35201;&#26041;&#27861;&#65292;&#24182;&#23581;&#35797;&#20102;&#20960;&#31181;&#25552;&#21462;&#21644;&#25277;&#35937;&#31574;&#30053;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#20004;&#20010;&#20855;&#26377;&#19981;&#21516;&#39118;&#26684;&#21644;&#32467;&#26500;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#35780;&#20272;&#30740;&#31350;&#32467;&#26524;&#30340;&#26222;&#36866;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#35299;&#37322;&#29983;&#25104;&#25688;&#35201;&#26041;&#38754;&#65292;&#30001;&#22768;&#26126;&#20449;&#24687;&#21463;&#30410;&#65292;&#24182;&#19988;&#26576;&#20123;&#25277;&#35937;&#26041;&#27861;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fighting misinformation is a challenging, yet crucial, task. Despite the growing number of experts being involved in manual fact-checking, this activity is time-consuming and cannot keep up with the ever-increasing amount of Fake News produced daily. Hence, automating this process is necessary to help curb misinformation. Thus far, researchers have mainly focused on claim veracity classification. In this paper, instead, we address the generation of justifications (textual explanation of why a claim is classified as either true or false) and benchmark it with novel datasets and advanced baselines. In particular, we focus on summarization approaches over unstructured knowledge (i.e. news articles) and we experiment with several extractive and abstractive strategies. We employed two datasets with different styles and structures, in order to assess the generalizability of our findings. Results show that in justification production summarization benefits from the claim information, and, in 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#20102;&#19968;&#20010;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#21487;&#24110;&#21161;&#38750;&#19987;&#19994;&#20154;&#21592;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#25552;&#20379;&#24515;&#29702;&#21672;&#35810;&#65292;&#20174;&#32780;&#24357;&#34917;&#20102;&#19987;&#19994;&#21672;&#35810;&#24072;&#30701;&#32570;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.15192</link><description>&lt;p&gt;
&#20511;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#24515;&#29702;&#21672;&#35810;&#65306;&#38754;&#21521;&#38750;&#19987;&#19994;&#20154;&#21592;&#30340;&#22810;&#26041;&#38754;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Enhancing Psychological Counseling with Large Language Model: A Multifaceted Decision-Support System for Non-Professionals. (arXiv:2308.15192v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15192
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#20102;&#19968;&#20010;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#21487;&#24110;&#21161;&#38750;&#19987;&#19994;&#20154;&#21592;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#25552;&#20379;&#24515;&#29702;&#21672;&#35810;&#65292;&#20174;&#32780;&#24357;&#34917;&#20102;&#19987;&#19994;&#21672;&#35810;&#24072;&#30701;&#32570;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#31038;&#20132;&#23186;&#20307;&#30340;&#29615;&#22659;&#19979;&#65292;&#22823;&#37327;&#29992;&#25143;&#34920;&#36798;&#36127;&#38754;&#24773;&#32490;&#65292;&#20854;&#20013;&#19968;&#20123;&#34920;&#29616;&#20026;&#24378;&#28872;&#30340;&#33258;&#26432;&#20542;&#21521;&#12290;&#36825;&#31181;&#24773;&#20917;&#20984;&#26174;&#20102;&#23545;&#35757;&#32451;&#26377;&#32032;&#30340;&#24515;&#29702;&#21672;&#35810;&#24072;&#30340;&#36843;&#20999;&#38656;&#27714;&#65292;&#20182;&#20204;&#21487;&#20197;&#23454;&#26045;&#26377;&#25928;&#30340;&#24515;&#29702;&#24178;&#39044;&#12290;&#28982;&#32780;&#65292;&#22521;&#20859;&#36825;&#20123;&#19987;&#19994;&#20154;&#21592;&#24120;&#24120;&#26159;&#19968;&#39033;&#24517;&#35201;&#20294;&#32791;&#26102;&#30340;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#21160;&#21592;&#38750;&#19987;&#19994;&#20154;&#21592;&#25110;&#24535;&#24895;&#32773;&#22312;&#36825;&#26041;&#38754;&#21457;&#25381;&#20316;&#29992;&#25104;&#20026;&#19968;&#20010;&#32039;&#36843;&#30340;&#38382;&#39064;&#12290;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20026;&#27492;&#25361;&#25112;&#25552;&#20379;&#20102;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#30340;&#26032;&#22411;&#27169;&#22411;&#65292;&#20197;&#20805;&#20998;&#36741;&#21161;&#38750;&#19987;&#19994;&#20154;&#21592;&#22312;&#22312;&#32447;&#29992;&#25143;&#20132;&#27969;&#20013;&#25552;&#20379;&#24515;&#29702;&#24178;&#39044;&#12290;&#35813;&#26694;&#26550;&#20351;&#24471;&#21512;&#29702;&#21033;&#29992;&#38750;&#19987;&#19994;&#21672;&#35810;&#24072;&#30340;&#21147;&#37327;&#21464;&#24471;&#21487;&#34892;&#12290;&#36827;&#34892;&#20102;&#19968;&#39033;&#32508;&#21512;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the contemporary landscape of social media, an alarming number of users express negative emotions, some of which manifest as strong suicidal intentions. This situation underscores a profound need for trained psychological counselors who can enact effective mental interventions. However, the development of these professionals is often an imperative but time-consuming task. Consequently, the mobilization of non-professionals or volunteers in this capacity emerges as a pressing concern. Leveraging the capabilities of artificial intelligence, and in particular, the recent advances in large language models, offers a viable solution to this challenge. This paper introduces a novel model constructed on the foundation of large language models to fully assist non-professionals in providing psychological interventions on online user discourses. This framework makes it plausible to harness the power of non-professional counselors in a meaningful way. A comprehensive study was conducted involvi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;&#19968;&#31181;&#20840;&#38754;&#30340;Twitter&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#20102;&#21442;&#19982;&#38452;&#35851;&#30456;&#20851;&#27963;&#21160;&#30340;&#29992;&#25143;&#30340;&#29305;&#28857;&#21644;&#34892;&#20026;&#29305;&#24449;&#65292;&#20026;&#38452;&#35851;&#35770;&#30340;&#26816;&#27979;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#21644;&#20381;&#25454;&#12290;</title><link>http://arxiv.org/abs/2308.15154</link><description>&lt;p&gt;
&#38452;&#35851;&#32773;&#30340;&#35299;&#21078;&#23398;&#65306;&#25581;&#31034;&#24615;&#26684;&#29305;&#28857;&#30340;&#20840;&#38754;Twitter&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
The Anatomy of Conspirators: Unveiling Traits using a Comprehensive Twitter Dataset. (arXiv:2308.15154v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;&#19968;&#31181;&#20840;&#38754;&#30340;Twitter&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#20102;&#21442;&#19982;&#38452;&#35851;&#30456;&#20851;&#27963;&#21160;&#30340;&#29992;&#25143;&#30340;&#29305;&#28857;&#21644;&#34892;&#20026;&#29305;&#24449;&#65292;&#20026;&#38452;&#35851;&#35770;&#30340;&#26816;&#27979;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#21644;&#20381;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20805;&#26021;&#30528;&#22312;&#32447;&#29615;&#22659;&#20013;&#30340;&#22823;&#37327;&#38169;&#35823;&#20449;&#24687;&#20013;&#65292;&#20851;&#20110;&#38452;&#35851;&#35770;&#30340;&#35752;&#35770;&#27491;&#22312;&#34028;&#21187;&#21457;&#23637;&#12290;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#26816;&#27979;&#38452;&#35851;&#35770;&#65292;&#24448;&#24448;&#20381;&#36182;&#20110;&#26377;&#38480;&#30340;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#35770;&#65292;&#29992;&#20110;&#26500;&#24314;&#19968;&#20010;&#21253;&#21547;2022&#24180;&#20840;&#24180;&#28041;&#21450;&#38452;&#35851;&#30456;&#20851;&#27963;&#21160;&#30340;Twitter&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30528;&#37325;&#20110;&#29420;&#31435;&#20110;&#29305;&#23450;&#38452;&#35851;&#35770;&#21644;&#20449;&#24687;&#25805;&#20316;&#30340;&#25968;&#25454;&#25910;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#25324;&#19968;&#20010;&#23545;&#29031;&#32452;&#65292;&#20854;&#20013;&#38543;&#26426;&#36873;&#25321;&#29992;&#25143;&#21487;&#20197;&#19982;&#28041;&#21450;&#38452;&#35851;&#27963;&#21160;&#30340;&#20010;&#20307;&#36827;&#34892;&#20844;&#27491;&#27604;&#36739;&#12290;&#36825;&#27425;&#20840;&#38754;&#30340;&#25910;&#38598;&#24037;&#20316;&#24635;&#20849;&#24471;&#21040;&#20102;15K&#20010;&#36134;&#25143;&#21644;&#20174;&#20182;&#20204;&#30340;&#26102;&#38388;&#32447;&#20013;&#25552;&#21462;&#30340;37M&#26465;&#25512;&#25991;&#12290;&#25105;&#20204;&#23545;&#20004;&#20010;&#32676;&#20307;&#22312;&#20027;&#39064;&#12289;&#20010;&#20154;&#36164;&#26009;&#21644;&#34892;&#20026;&#29305;&#24449;&#36825;&#19977;&#20010;&#32500;&#24230;&#19978;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#38452;&#35851;&#21644;
&lt;/p&gt;
&lt;p&gt;
The discourse around conspiracy theories is currently thriving amidst the rampant misinformation prevalent in online environments. Research in this field has been focused on detecting conspiracy theories on social media, often relying on limited datasets. In this study, we present a novel methodology for constructing a Twitter dataset that encompasses accounts engaged in conspiracy-related activities throughout the year 2022. Our approach centers on data collection that is independent of specific conspiracy theories and information operations. Additionally, our dataset includes a control group comprising randomly selected users who can be fairly compared to the individuals involved in conspiracy activities. This comprehensive collection effort yielded a total of 15K accounts and 37M tweets extracted from their timelines. We conduct a comparative analysis of the two groups across three dimensions: topics, profiles, and behavioral characteristics. The results indicate that conspiracy and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24187;&#35273;&#35780;&#20272;&#26694;&#26550;HaELM&#65292;&#21487;&#20197;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#24182;&#20998;&#26512;&#20102;&#23548;&#33268;&#24187;&#35273;&#30340;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#32531;&#35299;&#24187;&#35273;&#38382;&#39064;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2308.15126</link><description>&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#30340;&#35780;&#20272;&#19982;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Evaluation and Analysis of Hallucination in Large Vision-Language Models. (arXiv:2308.15126v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24187;&#35273;&#35780;&#20272;&#26694;&#26550;HaELM&#65292;&#21487;&#20197;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#24182;&#20998;&#26512;&#20102;&#23548;&#33268;&#24187;&#35273;&#30340;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#32531;&#35299;&#24187;&#35273;&#38382;&#39064;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;LVLMs&#20173;&#28982;&#23384;&#22312;&#24187;&#35273;&#38382;&#39064;&#65292;&#36825;&#38480;&#21046;&#20102;&#22312;&#35768;&#22810;&#22330;&#26223;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#24187;&#35273;&#25351;&#30340;&#26159;LVLMs&#21709;&#24212;&#20013;&#19981;&#23384;&#22312;&#20110;&#35270;&#35273;&#36755;&#20837;&#20013;&#30340;&#20449;&#24687;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#37325;&#22823;&#21518;&#26524;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#30446;&#21069;&#23545;LVLMs&#20013;&#30340;&#24187;&#35273;&#35780;&#20272;&#30340;&#30740;&#31350;&#24037;&#20316;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24187;&#35273;&#35780;&#20272;&#26694;&#26550;HaELM&#12290;HaELM&#30340;&#24615;&#33021;&#36817;&#20284;&#20110;ChatGPT&#30340;95%&#65292;&#24182;&#20855;&#26377;&#20302;&#25104;&#26412;&#12289;&#21487;&#22797;&#29616;&#12289;&#20445;&#25252;&#38544;&#31169;&#21644;&#26412;&#22320;&#37096;&#32626;&#31561;&#39069;&#22806;&#20248;&#21183;&#12290;&#21033;&#29992;HaELM&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#24403;&#21069;LVLMs&#20013;&#30340;&#24187;&#35273;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#23548;&#33268;LVLMs&#20013;&#24187;&#35273;&#30340;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#32531;&#35299;&#24187;&#35273;&#38382;&#39064;&#30340;&#26377;&#29992;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Vision-Language Models (LVLMs) have recently achieved remarkable success. However, LVLMs are still plagued by the hallucination problem, which limits the practicality in many scenarios. Hallucination refers to the information of LVLMs' responses that does not exist in the visual input, which poses potential risks of substantial consequences. There has been limited work studying hallucination evaluation in LVLMs. In this paper, we propose Hallucination Evaluation based on Large Language Models (HaELM), an LLM-based hallucination evaluation framework. HaELM achieves an approximate 95% performance comparable to ChatGPT and has additional advantages including low cost, reproducibility, privacy preservation and local deployment. Leveraging the HaELM, we evaluate the hallucination in current LVLMs. Furthermore, we analyze the factors contributing to hallucination in LVLMs and offer helpful suggestions to mitigate the hallucination problem. Our training data and human annotation halluci
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SpikeBERT&#30340;SNN&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;Spikformer&#26550;&#26500;&#21644;&#20351;&#29992;&#20004;&#38454;&#27573;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#22312;&#35821;&#35328;&#20219;&#21153;&#19978;&#36229;&#36234;&#20102;&#20854;&#20182;SNN&#27169;&#22411;&#65292;&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#29978;&#33267;&#36798;&#21040;&#20102;&#19982;BERT&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.15122</link><description>&lt;p&gt;
SpikeBERT&#65306;&#19968;&#31181;&#37319;&#29992;&#20004;&#38454;&#27573;BERT&#30693;&#35782;&#33976;&#39311;&#35757;&#32451;&#30340;&#35821;&#35328;Spikformer
&lt;/p&gt;
&lt;p&gt;
SpikeBERT: A Language Spikformer Trained with Two-Stage Knowledge Distillation from BERT. (arXiv:2308.15122v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15122
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SpikeBERT&#30340;SNN&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;Spikformer&#26550;&#26500;&#21644;&#20351;&#29992;&#20004;&#38454;&#27573;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#22312;&#35821;&#35328;&#20219;&#21153;&#19978;&#36229;&#36234;&#20102;&#20854;&#20182;SNN&#27169;&#22411;&#65292;&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#29978;&#33267;&#36798;&#21040;&#20102;&#19982;BERT&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#22312;&#20197;&#26356;&#33410;&#33021;&#30340;&#26041;&#24335;&#23454;&#29616;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#38754;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29992;&#20110;&#35821;&#35328;&#20219;&#21153;&#30340;SNN&#32593;&#32476;&#26550;&#26500;&#36807;&#20110;&#31616;&#21333;&#65292;&#28145;&#24230;&#26550;&#26500;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#65292;&#19982;BERT&#31561;&#20027;&#27969;&#22522;&#20110;Transformer&#30340;&#32593;&#32476;&#30456;&#27604;&#65292;&#23384;&#22312;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;&#33033;&#20914;Transformer&#65288;&#21363;Spikformer&#65289;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#35821;&#35328;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#26469;&#35757;&#32451;&#23427;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#36890;&#36807;&#20174;BERT&#21644;&#22823;&#37327;&#26410;&#26631;&#35760;&#25991;&#26412;&#20013;&#33976;&#39311;&#30693;&#35782;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#20877;&#27425;&#20174;&#22312;&#30456;&#21516;&#35757;&#32451;&#31034;&#20363;&#19978;&#23545;BERT&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#36827;&#34892;&#20219;&#21153;&#29305;&#23450;&#23454;&#20363;&#30693;&#35782;&#33976;&#39311;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#21629;&#21517;&#20026;SpikeBERT&#65292;&#22312;&#23454;&#29616;&#19978;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;SNN&#65292;&#24182;&#19988;&#29978;&#33267;&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#19982;BERT&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking neural networks (SNNs) offer a promising avenue to implement deep neural networks in a more energy-efficient way. However, the network architectures of existing SNNs for language tasks are too simplistic, and deep architectures have not been fully explored, resulting in a significant performance gap compared to mainstream transformer-based networks such as BERT. To this end, we improve a recently-proposed spiking transformer (i.e., Spikformer) to make it possible to process language tasks and propose a two-stage knowledge distillation method for training it, which combines pre-training by distilling knowledge from BERT with a large collection of unlabelled texts and fine-tuning with task-specific instances via knowledge distillation again from the BERT fine-tuned on the same training examples. Through extensive experimentation, we show that the models trained with our method, named SpikeBERT, outperform state-of-the-art SNNs and even achieve comparable results to BERTs on text 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#22269;&#38469;&#35937;&#26827;&#19978;&#20351;&#29992;OpenAI&#30340;ChatGPT&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#26696;&#20363;&#65292;&#35780;&#20272;&#20102;&#35813;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#21457;&#29616;ChatGPT&#22312;&#27491;&#24335;&#35821;&#35328;&#29702;&#35299;&#21644;&#33258;&#25105;&#35843;&#33410;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#20294;&#22312;&#28216;&#25103;&#20013;&#23637;&#29616;&#20102;&#19968;&#33268;&#30340;&#25112;&#30053;&#20542;&#21521;&#21644;&#20915;&#31574;&#33258;&#20449;&#24230;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2308.15118</link><description>&lt;p&gt;
&#26827;&#30424;&#19978;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;ChatGPT&#30340;&#27491;&#24335;&#35821;&#35328;&#29702;&#35299;&#21644;&#22797;&#26434;&#25512;&#29702;&#25216;&#33021;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Large Language Models on the Chessboard: A Study on ChatGPT's Formal Language Comprehension and Complex Reasoning Skills. (arXiv:2308.15118v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15118
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#22269;&#38469;&#35937;&#26827;&#19978;&#20351;&#29992;OpenAI&#30340;ChatGPT&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#26696;&#20363;&#65292;&#35780;&#20272;&#20102;&#35813;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#21457;&#29616;ChatGPT&#22312;&#27491;&#24335;&#35821;&#35328;&#29702;&#35299;&#21644;&#33258;&#25105;&#35843;&#33410;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#20294;&#22312;&#28216;&#25103;&#20013;&#23637;&#29616;&#20102;&#19968;&#33268;&#30340;&#25112;&#30053;&#20542;&#21521;&#21644;&#20915;&#31574;&#33258;&#20449;&#24230;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#22312;&#38656;&#35201;&#27491;&#24335;&#35821;&#35328;&#29702;&#35299;&#30340;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#65288;&#22914;&#22269;&#38469;&#35937;&#26827;&#65289;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#20173;&#21463;&#21040;&#36739;&#23569;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#22269;&#38469;&#35937;&#26827;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#25506;&#31350;&#20102;OpenAI&#30340;&#39640;&#32423;&#35821;&#35328;&#27169;&#22411;ChatGPT&#22312;&#22788;&#29702;&#36825;&#31181;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#26816;&#26597;&#31227;&#21160;&#30340;&#21512;&#27861;&#24615;&#21644;&#36136;&#37327;&#31561;&#31283;&#20581;&#25351;&#26631;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;ChatGPT&#23545;&#22269;&#38469;&#35937;&#26827;&#26827;&#30424;&#30340;&#29702;&#35299;&#33021;&#21147;&#12289;&#23545;&#22269;&#38469;&#35937;&#26827;&#35268;&#21017;&#30340;&#36981;&#23432;&#24773;&#20917;&#20197;&#21450;&#25112;&#30053;&#20915;&#31574;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#21457;&#29616;&#20102;ChatGPT&#20851;&#27880;&#26426;&#21046;&#30340;&#23616;&#38480;&#24615;&#65292;&#24433;&#21709;&#20854;&#23545;&#27491;&#24335;&#35821;&#35328;&#30340;&#29702;&#35299;&#65292;&#24182;&#25581;&#31034;&#20102;&#35813;&#27169;&#22411;&#26410;&#21457;&#23637;&#23436;&#21892;&#30340;&#33258;&#25105;&#35843;&#33410;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#25581;&#31034;&#20102;ChatGPT&#22312;&#28216;&#25103;&#20013;&#30340;&#19968;&#33268;&#25112;&#30053;&#20542;&#21521;&#20197;&#21450;&#24403;&#27169;&#22411;&#38754;&#23545;&#26356;&#22810;&#33258;&#28982;&#35821;&#35328;&#25110;&#20855;&#26377;&#26356;&#28165;&#26224;&#29702;&#35299;&#33021;&#21147;&#26102;&#65292;&#20915;&#31574;&#33258;&#20449;&#24230;&#30340;&#26126;&#26174;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models have made strides in natural language processing, their proficiency in complex reasoning tasks requiring formal language comprehension, such as chess, remains less investigated. This paper probes the performance of ChatGPT, a sophisticated language model by OpenAI in tackling such complex reasoning tasks, using chess as a case study. Through robust metrics examining both the legality and quality of moves, we assess ChatGPT's understanding of the chessboard, adherence to chess rules, and strategic decision-making abilities. Our evaluation identifies limitations within ChatGPT's attention mechanism that affect its formal language comprehension and uncovers the model's underdeveloped self-regulation abilities. Our study also reveals ChatGPT's propensity for a coherent strategy in its gameplay and a noticeable uptick in decision-making assertiveness when the model is presented with a greater volume of natural language or possesses a more lucid understanding of t
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#25913;&#21892;&#23884;&#20837;&#24335;&#23545;&#35805;&#26234;&#33021;&#20307;&#20132;&#20114;&#30340;&#26041;&#27861;&#35770;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#36164;&#28304;&#20351;&#29992;&#29702;&#35770;&#22522;&#30784;&#30340;&#27880;&#37322;&#23454;&#36341;&#12290;</title><link>http://arxiv.org/abs/2308.15097</link><description>&lt;p&gt;
&#33258;&#28982;&#21457;&#29983;&#30340;&#20154;&#26426;&#20132;&#20114;&#30340;&#39034;&#24207;&#27880;&#37322;:&#21021;&#27493;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
Sequential annotations for naturally-occurring HRI: first insights. (arXiv:2308.15097v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15097
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#25913;&#21892;&#23884;&#20837;&#24335;&#23545;&#35805;&#26234;&#33021;&#20307;&#20132;&#20114;&#30340;&#26041;&#27861;&#35770;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#36164;&#28304;&#20351;&#29992;&#29702;&#35770;&#22522;&#30784;&#30340;&#27880;&#37322;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#37322;&#20102;&#25105;&#20204;&#24320;&#21457;&#30340;&#26041;&#27861;&#35770;&#65292;&#36890;&#36807;&#23545;&#35805;&#20998;&#26512;&#30340;&#39034;&#24207;&#21644;&#22810;&#27169;&#24577;&#20998;&#26512;&#26469;&#25913;&#21892;&#23884;&#20837;&#24335;&#20132;&#20114;&#24335;&#26234;&#33021;&#20307;&#30340;&#20132;&#20114;&#12290;&#20351;&#29992;&#26696;&#20363;&#26159;&#19968;&#20010;Pepper&#26426;&#22120;&#20154;&#65292;&#39044;&#26399;&#22312;&#22270;&#20070;&#39302;&#20013;&#21521;&#29992;&#25143;&#25552;&#20379;&#20449;&#24687;&#21644;&#23548;&#33322;&#12290;&#20026;&#20102;&#25552;&#20986;&#21644;&#23398;&#20064;&#26356;&#22909;&#30340;&#20132;&#20114;&#27169;&#24335;&#65292;&#25105;&#20204;&#27491;&#22312;&#21019;&#24314;&#19968;&#20010;&#33258;&#28982;&#21457;&#29983;&#30340;&#20132;&#20114;&#35821;&#26009;&#24211;&#65292;&#24182;&#23558;&#20854;&#25552;&#20379;&#32473;&#31038;&#21306;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#26377;&#20851;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#36164;&#28304;&#22312;&#20154;&#26426;&#20132;&#20114;&#20013;&#20351;&#29992;&#30340;&#19968;&#20123;&#29702;&#35770;&#22522;&#30784;&#30340;&#27880;&#37322;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explain the methodology we developed for improving the interactions accomplished by an embedded conversational agent, drawing from Conversation Analytic sequential and multimodal analysis. The use case is a Pepper robot that is expected to inform and orient users in a library. In order to propose and learn better interactive schema, we are creating a corpus of naturally-occurring interactions that will be made available to the community. To do so, we propose an annotation practice based on some theoretical underpinnings about the use of language and multimodal resources in human-robot interaction. CCS CONCEPTS $\bullet$ Computing methodologies $\rightarrow$ Discourse, dialogue and pragmatics; $\bullet$ Human-centered computing $\rightarrow$ Text input; HCI theory, concepts and models; Field studies.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#38899;&#39057;&#23383;&#24149;&#31995;&#32479;&#21644;&#38899;&#39057;&#25991;&#26412;&#26816;&#32034;&#31995;&#32479;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#36890;&#36807;&#25506;&#32034;&#26410;&#32463;&#20462;&#25913;&#30340;&#38899;&#39057;&#23383;&#24149;&#31995;&#32479;&#23545;&#38899;&#39057;&#25991;&#26412;&#26816;&#32034;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21363;&#20351;&#26410;&#36827;&#34892;&#24494;&#35843;&#65292;&#38899;&#39057;&#23383;&#24149;&#31995;&#32479;&#22312;&#38899;&#39057;&#25991;&#26412;&#26816;&#32034;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#19968;&#23450;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.15090</link><description>&lt;p&gt;
&#19968;&#30707;&#20108;&#40479;&#65306;&#38899;&#39057;&#23383;&#24149;&#31995;&#32479;&#26159;&#21542;&#33021;&#29992;&#20110;&#38899;&#39057;&#25991;&#26412;&#26816;&#32034;&#65311;
&lt;/p&gt;
&lt;p&gt;
Killing two birds with one stone: Can an audio captioning system also be used for audio-text retrieval?. (arXiv:2308.15090v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15090
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#38899;&#39057;&#23383;&#24149;&#31995;&#32479;&#21644;&#38899;&#39057;&#25991;&#26412;&#26816;&#32034;&#31995;&#32479;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#36890;&#36807;&#25506;&#32034;&#26410;&#32463;&#20462;&#25913;&#30340;&#38899;&#39057;&#23383;&#24149;&#31995;&#32479;&#23545;&#38899;&#39057;&#25991;&#26412;&#26816;&#32034;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21363;&#20351;&#26410;&#36827;&#34892;&#24494;&#35843;&#65292;&#38899;&#39057;&#23383;&#24149;&#31995;&#32479;&#22312;&#38899;&#39057;&#25991;&#26412;&#26816;&#32034;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#19968;&#23450;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#38899;&#39057;&#23383;&#24149;&#31995;&#32479;&#26088;&#22312;&#24320;&#21457;&#33021;&#22815;&#29992;&#25991;&#26412;&#21477;&#23376;&#25551;&#36848;&#38899;&#39057;&#24405;&#38899;&#30340;&#31995;&#32479;&#12290;&#19982;&#27492;&#30456;&#21453;&#65292;&#38899;&#39057;&#25991;&#26412;&#26816;&#32034;&#31995;&#32479;&#26088;&#22312;&#20026;&#32473;&#23450;&#30340;&#25991;&#26412;&#26597;&#35810;&#65288;&#25991;&#26412;&#21040;&#38899;&#39057;&#65289;&#25110;&#21453;&#20043;&#65288;&#38899;&#39057;&#21040;&#25991;&#26412;&#65289;&#25214;&#21040;&#26368;&#20339;&#21305;&#37197;&#30340;&#38899;&#39057;&#24405;&#38899;&#12290;&#36825;&#20123;&#20219;&#21153;&#38656;&#35201;&#19981;&#21516;&#31867;&#22411;&#30340;&#31995;&#32479;&#65306;&#38899;&#39057;&#23383;&#24149;&#31995;&#32479;&#37319;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#65292;&#32780;&#38899;&#39057;&#25991;&#26412;&#26816;&#32034;&#31995;&#32479;&#21033;&#29992;&#22312;&#20849;&#20139;&#25237;&#23556;&#23376;&#31354;&#38388;&#20869;&#27604;&#36739;&#38899;&#39057;&#21644;&#25991;&#26412;&#34920;&#31034;&#30340;&#25490;&#24207;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;&#26410;&#32463;&#20462;&#25913;&#30340;&#38899;&#39057;&#23383;&#24149;&#31995;&#32479;&#65288;&#26080;&#38656;&#38024;&#23545;&#26032;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#65289;&#30340;&#38899;&#39057;&#25991;&#26412;&#26816;&#32034;&#33021;&#21147;&#65292;&#30740;&#31350;&#20102;&#38899;&#39057;&#23383;&#24149;&#31995;&#32479;&#19982;&#38899;&#39057;&#25991;&#26412;&#26816;&#32034;&#31995;&#32479;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#38899;&#39057;&#23383;&#24149;&#31995;&#32479;&#21253;&#25324;&#19968;&#20010;&#22312;&#38899;&#39057;&#26631;&#35760;&#19978;&#36890;&#36807;AudioSet&#36827;&#34892;&#35757;&#32451;&#30340;&#38899;&#39057;&#32534;&#30721;&#22120;&#65288;ConvNeXt-Tiny&#65289;&#65292;&#20197;&#21450;&#19968;&#20010;&#36127;&#36131;&#29983;&#25104;&#21477;&#23376;&#30340;&#21464;&#21387;&#22120;&#35299;&#30721;&#22120;&#12290;&#23545;&#20110;&#38899;&#39057;&#23383;&#24149;&#31995;&#32479;&#65292;&#23427;&#22312;Clotho&#19978;&#30340;SPIDEr-FL&#24471;&#20998;&#24179;&#22343;&#20026;0.298&#65292;&#22312;AudioCaps&#19978;&#30340;&#24471;&#20998;&#24179;&#22343;&#20026;0.472&#12290;&#23545;&#20110;&#38899;&#39057;&#25991;&#26412;&#26816;&#32034;&#31995;&#32479;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#26631;&#20934;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated Audio Captioning (AAC) aims to develop systems capable of describing an audio recording using a textual sentence. In contrast, Audio-Text Retrieval (ATR) systems seek to find the best matching audio recording(s) for a given textual query (Text-to-Audio) or vice versa (Audio-to-Text). These tasks require different types of systems: AAC employs a sequence-to-sequence model, while ATR utilizes a ranking model that compares audio and text representations within a shared projection subspace. However, this work investigates the relationship between AAC and ATR by exploring the ATR capabilities of an unmodified AAC system, without fine-tuning for the new task. Our AAC system consists of an audio encoder (ConvNeXt-Tiny) trained on AudioSet for audio tagging, and a transformer decoder responsible for generating sentences. For AAC, it achieves a high SPIDEr-FL score of 0.298 on Clotho and 0.472 on AudioCaps on average. For ATR, we propose using the standard Cross-Entropy loss values ob
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24418;&#24577;&#20449;&#24687;&#30340;&#20998;&#31867;&#25439;&#22833;&#20989;&#25968;&#65292;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#24418;&#24577;&#35789;&#20041;&#26631;&#27880;&#20013;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290; &#23613;&#31649;&#22312;&#21333;&#26631;&#31614;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#19981;&#22914;&#26631;&#20934;&#25439;&#22833;&#20989;&#25968;&#65292;&#20294;&#22312;&#21069;n&#20010;&#39044;&#27979;&#26631;&#31614;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290; &#36825;&#20010;&#26041;&#27861;&#22312;&#20154;&#26426;&#21327;&#20316;&#26631;&#27880;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.15055</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#24418;&#24577;&#35789;&#20041;&#26631;&#27880;&#20013;&#30340;&#20998;&#31867;&#20007;&#22833;
&lt;/p&gt;
&lt;p&gt;
Taxonomic Loss for Morphological Glossing of Low-Resource Languages. (arXiv:2308.15055v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24418;&#24577;&#20449;&#24687;&#30340;&#20998;&#31867;&#25439;&#22833;&#20989;&#25968;&#65292;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#24418;&#24577;&#35789;&#20041;&#26631;&#27880;&#20013;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290; &#23613;&#31649;&#22312;&#21333;&#26631;&#31614;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#19981;&#22914;&#26631;&#20934;&#25439;&#22833;&#20989;&#25968;&#65292;&#20294;&#22312;&#21069;n&#20010;&#39044;&#27979;&#26631;&#31614;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290; &#36825;&#20010;&#26041;&#27861;&#22312;&#20154;&#26426;&#21327;&#20316;&#26631;&#27880;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24418;&#24577;&#35789;&#20041;&#26631;&#27880;&#26159;&#33258;&#21160;&#35821;&#35328;&#25991;&#26723;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#65292;&#23427;&#21487;&#20197;&#26497;&#22823;&#22320;&#25552;&#39640;&#20854;&#20182;&#19979;&#28216;&#24212;&#29992;&#30340;&#25928;&#26524;&#12290;&#23613;&#31649;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26631;&#27880;&#31995;&#32479;&#22312;&#25968;&#25454;&#20016;&#23500;&#30340;&#35821;&#35328;&#19978;&#34920;&#29616;&#38750;&#24120;&#22909;&#65292;&#20294;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#21019;&#24314;&#26377;&#29992;&#30340;&#27169;&#22411;&#26356;&#21152;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24418;&#24577;&#20449;&#24687;&#30340;&#20998;&#31867;&#25439;&#22833;&#20989;&#25968;&#65292;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#24418;&#24577;&#35789;&#20041;&#26631;&#27880;&#30340;&#24615;&#33021;&#26356;&#22909;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#20351;&#29992;&#36825;&#31181;&#25439;&#22833;&#20989;&#25968;&#22312;&#21333;&#26631;&#31614;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#19981;&#22914;&#26631;&#20934;&#25439;&#22833;&#20989;&#25968;&#65292;&#20294;&#22312;&#32771;&#34385;&#21069;n&#20010;&#39044;&#27979;&#26631;&#31614;&#26102;&#65292;&#23427;&#20135;&#29983;&#26356;&#22909;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#24615;&#36136;&#20351;&#24471;&#20998;&#31867;&#25439;&#22833;&#20989;&#25968;&#22312;&#20154;&#26426;&#21327;&#20316;&#26631;&#27880;&#30340;&#29615;&#22659;&#20013;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Morpheme glossing is a critical task in automated language documentation and can benefit other downstream applications greatly. While state-of-the-art glossing systems perform very well for languages with large amounts of existing data, it is more difficult to create useful models for low-resource languages. In this paper, we propose the use of a taxonomic loss function that exploits morphological information to make morphological glossing more performant when data is scarce. We find that while the use of this loss function does not outperform a standard loss function with regards to single-label prediction accuracy, it produces better predictions when considering the top-n predicted labels. We suggest this property makes the taxonomic loss function useful in a human-in-the-loop annotation setting.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25551;&#36848;&#20102;&#23545;&#26500;&#24314;&#36866;&#24212;&#21475;&#35821;&#23545;&#35805;&#31995;&#32479;&#30340;&#25991;&#26412;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#22120;&#36827;&#34892;&#30340;&#24037;&#31243;&#24037;&#20316;&#65292;&#21033;&#29992;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#38169;&#35823;&#26657;&#27491;&#21644;&#25991;&#26412;&#23545;&#35805;&#31995;&#32479;&#23454;&#29616;&#20102;&#25554;&#27133;&#21644;&#20540;&#30340;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2308.15053</link><description>&lt;p&gt;
&#36866;&#24212;&#21475;&#35821;&#23545;&#35805;&#30340;&#25991;&#26412;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#22120;
&lt;/p&gt;
&lt;p&gt;
Adapting text-based dialogue state tracker for spoken dialogues. (arXiv:2308.15053v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15053
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25551;&#36848;&#20102;&#23545;&#26500;&#24314;&#36866;&#24212;&#21475;&#35821;&#23545;&#35805;&#31995;&#32479;&#30340;&#25991;&#26412;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#22120;&#36827;&#34892;&#30340;&#24037;&#31243;&#24037;&#20316;&#65292;&#21033;&#29992;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#38169;&#35823;&#26657;&#27491;&#21644;&#25991;&#26412;&#23545;&#35805;&#31995;&#32479;&#23454;&#29616;&#20102;&#25554;&#27133;&#21644;&#20540;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36890;&#36807;&#23545;&#35805;&#31995;&#32479;&#25216;&#26415;&#31454;&#36187;&#65288;DSTC&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#26500;&#24314;&#19968;&#20010;&#20855;&#26377;&#35821;&#38899;&#30028;&#38754;&#30340;&#31283;&#20581;&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#22823;&#37096;&#20998;&#36827;&#23637;&#37117;&#26159;&#38024;&#23545;&#22522;&#20110;&#25991;&#26412;&#30340;&#23545;&#35805;&#31995;&#32479;&#65292;&#22240;&#20026;&#26377;&#20016;&#23500;&#30340;&#20070;&#38754;&#35821;&#26009;&#24211;&#25968;&#25454;&#38598;&#65292;&#32780;&#20855;&#26377;&#21475;&#35821;&#23545;&#35805;&#30340;&#25968;&#25454;&#38598;&#38750;&#24120;&#31232;&#32570;&#12290;&#28982;&#32780;&#65292;&#27491;&#22914;Siri&#21644;Alexa&#31561;&#35821;&#38899;&#21161;&#25163;&#31995;&#32479;&#25152;&#23637;&#31034;&#30340;&#65292;&#23558;&#36825;&#31181;&#25104;&#21151;&#36716;&#31227;&#21040;&#21475;&#35821;&#23545;&#35805;&#20013;&#20855;&#26377;&#23454;&#38469;&#37325;&#35201;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#25105;&#20204;&#22312;DSTC11&#30340;&#20855;&#26377;&#35821;&#38899;&#24863;&#30693;&#30340;&#23545;&#35805;&#31995;&#32479;&#25216;&#26415;&#25361;&#25112;&#36187;&#20013;&#30340;&#39640;&#24230;&#25104;&#21151;&#27169;&#22411;&#30340;&#24037;&#31243;&#21162;&#21147;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#30001;&#19977;&#20010;&#20027;&#35201;&#27169;&#22359;&#32452;&#25104;&#65306;&#65288;1&#65289;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#38169;&#35823;&#26657;&#27491;&#65292;&#20197;&#24357;&#21512;&#21475;&#35821;&#21644;&#25991;&#26412;&#35805;&#35821;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#65288;2&#65289;&#29992;&#20110;&#20272;&#35745;&#25554;&#27133;&#21644;&#20540;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#23545;&#35805;&#31995;&#32479;&#65288;D3ST&#65289;&#65292;&#35813;&#31995;&#32479;&#20351;&#29992;&#25554;&#27133;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although there have been remarkable advances in dialogue systems through the dialogue systems technology competition (DSTC), it remains one of the key challenges to building a robust task-oriented dialogue system with a speech interface. Most of the progress has been made for text-based dialogue systems since there are abundant datasets with written corpora while those with spoken dialogues are very scarce. However, as can be seen from voice assistant systems such as Siri and Alexa, it is of practical importance to transfer the success to spoken dialogues. In this paper, we describe our engineering effort in building a highly successful model that participated in the speech-aware dialogue systems technology challenge track in DSTC11. Our model consists of three major modules: (1) automatic speech recognition error correction to bridge the gap between the spoken and the text utterances, (2) text-based dialogue system (D3ST) for estimating the slots and values using slot descriptions, an
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23398;&#20250;&#20197;&#31867;&#20284;&#20110;&#30693;&#35782;&#24211;&#30340;&#26041;&#24335;&#32452;&#32455;&#27010;&#24565;&#65292;&#36825;&#34920;&#26126;&#23427;&#20204;&#20855;&#22791;&#20154;&#31867;&#25512;&#29702;&#35821;&#20041;&#21644;&#19990;&#30028;&#30693;&#35782;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.15047</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27010;&#24565;&#32452;&#32455;&#19978;&#36235;&#21521;&#20154;&#31867;&#30340;&#27700;&#24179;
&lt;/p&gt;
&lt;p&gt;
Large language models converge toward human-like concept organization. (arXiv:2308.15047v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15047
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23398;&#20250;&#20197;&#31867;&#20284;&#20110;&#30693;&#35782;&#24211;&#30340;&#26041;&#24335;&#32452;&#32455;&#27010;&#24565;&#65292;&#36825;&#34920;&#26126;&#23427;&#20204;&#20855;&#22791;&#20154;&#31867;&#25512;&#29702;&#35821;&#20041;&#21644;&#19990;&#30028;&#30693;&#35782;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30693;&#35782;&#25552;&#21462;&#12289;&#25512;&#29702;&#21644;&#23545;&#35805;&#26041;&#38754;&#23637;&#29616;&#20986;&#20154;&#31867;&#27700;&#24179;&#30340;&#34920;&#29616;&#65292;&#20294;&#36825;&#31181;&#34920;&#29616;&#26159;&#30001;&#20110;&#35760;&#24518;&#21644;&#27169;&#24335;&#21305;&#37197;&#65292;&#36824;&#26159;&#21453;&#26144;&#20102;&#31867;&#20284;&#20110;&#20154;&#31867;&#25512;&#29702;&#35821;&#20041;&#21644;&#19990;&#30028;&#30693;&#35782;&#30340;&#34920;&#29616;&#20173;&#28982;&#23384;&#22312;&#20105;&#35758;&#12290;&#30693;&#35782;&#24211;&#65288;&#22914;WikiData&#65289;&#25552;&#20379;&#20102;&#25512;&#29702;&#35821;&#20041;&#21644;&#19990;&#30028;&#30693;&#35782;&#30340;&#22823;&#35268;&#27169;&#39640;&#36136;&#37327;&#34920;&#31034;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23398;&#20250;&#20197;&#19982;&#36825;&#20123;&#30693;&#35782;&#24211;&#20013;&#27010;&#24565;&#30340;&#32452;&#32455;&#26041;&#24335;&#24778;&#20154;&#30456;&#20284;&#30340;&#26041;&#24335;&#32452;&#32455;&#27010;&#24565;&#12290;&#30693;&#35782;&#24211;&#27169;&#25311;&#20102;&#38598;&#20307;&#12289;&#26426;&#26500;&#21270;&#30340;&#30693;&#35782;&#65292;&#32780;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20284;&#20046;&#20174;&#21407;&#22987;&#25991;&#26412;&#20013;&#20135;&#29983;&#20102;&#36825;&#26679;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26356;&#22823;&#26356;&#22909;&#30340;&#27169;&#22411;&#22312;&#27010;&#24565;&#32452;&#32455;&#19978;&#34920;&#29616;&#20986;&#26356;&#21152;&#20154;&#31867;&#21270;&#30340;&#29305;&#28857;&#65292;&#28085;&#30422;&#20102;&#22235;&#20010;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#21644;&#19977;&#20010;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models show human-like performance in knowledge extraction, reasoning and dialogue, but it remains controversial whether this performance is best explained by memorization and pattern matching, or whether it reflects human-like inferential semantics and world knowledge. Knowledge bases such as WikiData provide large-scale, high-quality representations of inferential semantics and world knowledge. We show that large language models learn to organize concepts in ways that are strikingly similar to how concepts are organized in such knowledge bases. Knowledge bases model collective, institutional knowledge, and large language models seem to induce such knowledge from raw text. We show that bigger and better models exhibit more human-like concept organization, across four families of language models and three knowledge graph embeddings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20256;&#32479;&#30340;TF-IDF&#21644;&#27973;&#23618;&#23884;&#20837;&#27169;&#22411;&#32467;&#21512;&#20351;&#29992;&#65292;&#21487;&#20197;&#19982;&#22522;&#20110;&#22823;&#22411;Transformer&#27169;&#22411;&#30340;&#31070;&#32463;&#25490;&#21517;&#27169;&#22411;&#31454;&#20105;&#65292;&#24182;&#21487;&#20197;&#25552;&#39640;&#36825;&#20123;&#27169;&#22411;&#22312;&#22823;&#35268;&#27169;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.15027</link><description>&lt;p&gt;
&#29992;&#20256;&#32479;IR&#26041;&#27861;&#25552;&#39640;&#31070;&#32463;&#25490;&#21517;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Improving Neural Ranking Models with Traditional IR Methods. (arXiv:2308.15027v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20256;&#32479;&#30340;TF-IDF&#21644;&#27973;&#23618;&#23884;&#20837;&#27169;&#22411;&#32467;&#21512;&#20351;&#29992;&#65292;&#21487;&#20197;&#19982;&#22522;&#20110;&#22823;&#22411;Transformer&#27169;&#22411;&#30340;&#31070;&#32463;&#25490;&#21517;&#27169;&#22411;&#31454;&#20105;&#65292;&#24182;&#21487;&#20197;&#25552;&#39640;&#36825;&#20123;&#27169;&#22411;&#22312;&#22823;&#35268;&#27169;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;Transformer&#27169;&#22411;&#30340;&#31070;&#32463;&#25490;&#21517;&#26041;&#27861;&#36817;&#24180;&#26469;&#22312;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#65292;&#24182;&#34987;&#20027;&#35201;&#21830;&#19994;&#35299;&#20915;&#26041;&#26696;&#37319;&#29992;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#21019;&#24314;&#36807;&#31243;&#20013;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#65292;&#24182;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#26469;&#36866;&#24212;&#29305;&#23450;&#30340;&#35821;&#26009;&#24211;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20302;&#36164;&#28304;&#26367;&#20195;&#26041;&#26696;&#65292;&#21363;&#22522;&#20110;&#23884;&#20837;&#27169;&#22411;&#30340;&#25991;&#26723;&#26816;&#32034;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#23427;&#22312;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#19978;&#19982;&#32454;&#35843;&#30340;&#22823;&#22411;Transformer&#27169;&#22411;&#30456;&#31454;&#20105;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#20256;&#32479;&#20851;&#38190;&#23383;&#21305;&#37197;&#26041;&#27861;TF-IDF&#19982;&#27973;&#23618;&#23884;&#20837;&#27169;&#22411;&#31616;&#21333;&#32467;&#21512;&#21487;&#20197;&#20197;&#20302;&#25104;&#26412;&#36861;&#36214;&#21040;&#22797;&#26434;&#31070;&#32463;&#25490;&#21517;&#27169;&#22411;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#28155;&#21152;TF-IDF&#24230;&#37327;&#21487;&#20197;&#25552;&#39640;&#22823;&#35268;&#27169;&#32454;&#35843;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural ranking methods based on large transformer models have recently gained significant attention in the information retrieval community, and have been adopted by major commercial solutions. Nevertheless, they are computationally expensive to create, and require a great deal of labeled data for specialized corpora. In this paper, we explore a low resource alternative which is a bag-of-embedding model for document retrieval and find that it is competitive with large transformer models fine tuned on information retrieval tasks. Our results show that a simple combination of TF-IDF, a traditional keyword matching method, with a shallow embedding model provides a low cost path to compete well with the performance of complex neural ranking models on 3 datasets. Furthermore, adding TF-IDF measures improves the performance of large-scale fine tuned models on these tasks.
&lt;/p&gt;</description></item><item><title>&#36882;&#24402;&#24635;&#32467;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#38271;&#26399;&#23545;&#35805;&#35760;&#24518;&#65292;&#21487;&#20197;&#25552;&#39640;&#23545;&#35805;&#31995;&#32479;&#22312;&#38271;&#23545;&#35805;&#20013;&#35760;&#24518;&#37325;&#35201;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.15022</link><description>&lt;p&gt;
&#36882;&#24402;&#24635;&#32467;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#38271;&#26399;&#23545;&#35805;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
Recursively Summarizing Enables Long-Term Dialogue Memory in Large Language Models. (arXiv:2308.15022v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15022
&lt;/p&gt;
&lt;p&gt;
&#36882;&#24402;&#24635;&#32467;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#38271;&#26399;&#23545;&#35805;&#35760;&#24518;&#65292;&#21487;&#20197;&#25552;&#39640;&#23545;&#35805;&#31995;&#32479;&#22312;&#38271;&#23545;&#35805;&#20013;&#35760;&#24518;&#37325;&#35201;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#24320;&#25918;&#39046;&#22495;&#30340;&#23545;&#35805;&#31995;&#32479;&#22312;&#38271;&#26399;&#23545;&#35805;&#20013;&#23481;&#26131;&#36951;&#24536;&#37325;&#35201;&#20449;&#24687;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#35757;&#32451;&#29305;&#23450;&#30340;&#26816;&#32034;&#22120;&#25110;&#24635;&#32467;&#22120;&#20174;&#36807;&#21435;&#33719;&#21462;&#20851;&#38190;&#20449;&#24687;&#65292;&#36825;&#38656;&#35201;&#32791;&#36153;&#26102;&#38388;&#19988;&#39640;&#24230;&#20381;&#36182;&#26631;&#35760;&#25968;&#25454;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36882;&#24402;&#29983;&#25104;&#24635;&#32467;/&#35760;&#24518;&#65292;&#20197;&#22686;&#24378;&#38271;&#26399;&#35760;&#24518;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#21050;&#28608;LLMs&#35760;&#20303;&#23567;&#23545;&#35805;&#19978;&#19979;&#25991;&#65292;&#28982;&#21518;&#36882;&#24402;&#22320;&#20351;&#29992;&#20043;&#21069;&#30340;&#35760;&#24518;&#21644;&#38543;&#21518;&#30340;&#23545;&#35805;&#20869;&#23481;&#20135;&#29983;&#26032;&#30340;&#35760;&#24518;&#12290;&#26368;&#21518;&#65292;LLM&#21487;&#20197;&#22312;&#26368;&#26032;&#35760;&#24518;&#30340;&#24110;&#21161;&#19979;&#36731;&#26494;&#29983;&#25104;&#39640;&#24230;&#19968;&#33268;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#20351;&#29992;ChatGPT&#21644;text-davinci-003&#36827;&#34892;&#35780;&#20272;&#65292;&#23545;&#24191;&#27867;&#20351;&#29992;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38271;&#23545;&#35805;&#20013;&#21487;&#20197;&#29983;&#25104;&#26356;&#19968;&#33268;&#30340;&#21709;&#24212;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#23454;&#29616;LLM&#24314;&#27169;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most open-domain dialogue systems suffer from forgetting important information, especially in a long-term conversation. Existing works usually train the specific retriever or summarizer to obtain key information from the past, which is time-consuming and highly depends on the quality of labeled data. To alleviate this problem, we propose to recursively generate summaries/ memory using large language models (LLMs) to enhance long-term memory ability. Specifically, our method first stimulates LLMs to memorize small dialogue contexts and then recursively produce new memory using previous memory and following contexts. Finally, the LLM can easily generate a highly consistent response with the help of the latest memory. We evaluate our method using ChatGPT and text-davinci-003, and the experiments on the widely-used public dataset show that our method can generate more consistent responses in a long-context conversation. Notably, our method is a potential solution to enable the LLM to model
&lt;/p&gt;</description></item><item><title>TransPrompt v2&#26159;&#19968;&#31181;&#21487;&#36716;&#31227;&#30340;&#25552;&#31034;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#36328;&#20219;&#21153;&#25991;&#26412;&#20998;&#31867;&#12290;&#23427;&#36890;&#36807;&#22810;&#20219;&#21153;&#20803;&#30693;&#35782;&#33719;&#21462;&#26469;&#35757;&#32451;&#19968;&#20010;&#25429;&#25417;&#36328;&#20219;&#21153;&#21487;&#36716;&#31227;&#30693;&#35782;&#30340;&#20803;&#23398;&#20064;&#22120;&#65292;&#20197;&#25552;&#21319;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.15010</link><description>&lt;p&gt;
TransPrompt v2: &#19968;&#31181;&#29992;&#20110;&#36328;&#20219;&#21153;&#25991;&#26412;&#20998;&#31867;&#30340;&#21487;&#36716;&#31227;&#25552;&#31034;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
TransPrompt v2: A Transferable Prompting Framework for Cross-task Text Classification. (arXiv:2308.15010v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15010
&lt;/p&gt;
&lt;p&gt;
TransPrompt v2&#26159;&#19968;&#31181;&#21487;&#36716;&#31227;&#30340;&#25552;&#31034;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#36328;&#20219;&#21153;&#25991;&#26412;&#20998;&#31867;&#12290;&#23427;&#36890;&#36807;&#22810;&#20219;&#21153;&#20803;&#30693;&#35782;&#33719;&#21462;&#26469;&#35757;&#32451;&#19968;&#20010;&#25429;&#25417;&#36328;&#20219;&#21153;&#21487;&#36716;&#31227;&#30693;&#35782;&#30340;&#20803;&#23398;&#20064;&#22120;&#65292;&#20197;&#25552;&#21319;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#20998;&#31867;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26368;&#37325;&#35201;&#30340;&#20219;&#21153;&#20043;&#19968;&#12290;&#26368;&#36817;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#21462;&#24471;&#20102;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;PLMs&#33719;&#24471;&#28385;&#24847;&#30340;&#32467;&#26524;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#22823;&#37327;&#30340;&#20219;&#21153;&#29305;&#23450;&#26631;&#27880;&#25968;&#25454;&#65292;&#32780;&#22312;&#35768;&#22810;&#24212;&#29992;&#22330;&#26223;&#20013;&#65292;&#30001;&#20110;&#25968;&#25454;&#35775;&#38382;&#21644;&#38544;&#31169;&#38480;&#21046;&#65292;&#36825;&#21487;&#33021;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#24494;&#35843;&#33539;&#24335;&#36890;&#36807;&#20219;&#21153;&#29305;&#23450;&#27169;&#26495;&#25552;&#39640;&#20102;PLMs&#22312;&#23569;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#19981;&#28165;&#26970;&#25552;&#31034;&#30693;&#35782;&#22914;&#20309;&#22312;&#20219;&#21153;&#20043;&#38388;&#36827;&#34892;&#20256;&#36882;&#65292;&#20197;&#23454;&#29616;&#30456;&#20114;&#22686;&#24378;&#30340;&#30446;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;TransPrompt v2&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#36716;&#31227;&#25552;&#31034;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#30456;&#20284;&#25110;&#19981;&#21516;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#12290;&#23545;&#20110;&#30456;&#20284;&#20219;&#21153;&#30340;&#23398;&#20064;&#65292;&#25105;&#20204;&#37319;&#29992;&#22810;&#20219;&#21153;&#20803;&#30693;&#35782;&#33719;&#21462;&#65288;MMA&#65289;&#36807;&#31243;&#26469;&#35757;&#32451;&#19968;&#20010;&#25429;&#25417;&#36328;&#20219;&#21153;&#21487;&#36716;&#31227;&#30693;&#35782;&#30340;&#20803;&#23398;&#20064;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text classification is one of the most imperative tasks in natural language processing (NLP). Recent advances with pre-trained language models (PLMs) have shown remarkable success on this task. However, the satisfying results obtained by PLMs heavily depend on the large amounts of task-specific labeled data, which may not be feasible in many application scenarios due to data access and privacy constraints. The recently-proposed prompt-based fine-tuning paradigm improves the performance of PLMs for few-shot text classification with task-specific templates. Yet, it is unclear how the prompting knowledge can be transferred across tasks, for the purpose of mutual reinforcement. We propose TransPrompt v2, a novel transferable prompting framework for few-shot learning across similar or distant text classification tasks. For learning across similar tasks, we employ a multi-task meta-knowledge acquisition (MMA) procedure to train a meta-learner that captures the cross-task transferable knowled
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23454;&#29616;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#24320;&#25918;&#38598;&#35328;&#35821;&#35782;&#21035;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;MFCC&#21644;&#38899;&#39640;&#29305;&#24449;&#65292;TDNN&#27169;&#22411;&#25552;&#21462;&#29305;&#24449;&#23884;&#20837;&#65292;&#35774;&#32622;&#32622;&#20449;&#24230;&#38408;&#20540;&#65292;&#20197;&#21450;&#20351;&#29992;LDA&#21644;pLDA&#23398;&#20064;&#26032;&#30340;&#26410;&#30693;&#35821;&#35328;&#20998;&#31867;&#26469;&#23454;&#29616;&#12290;&#22312;&#32463;&#36807;&#35757;&#32451;&#30340;&#35821;&#35328;&#19978;&#65292;&#31995;&#32479;&#20934;&#30830;&#29575;&#36798;&#21040;91.76%&#65292;&#24182;&#19988;&#20855;&#22791;&#23454;&#26102;&#36866;&#24212;&#26410;&#30693;&#35821;&#35328;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.14951</link><description>&lt;p&gt;
&#40065;&#26834;&#30340;&#24320;&#25918;&#38598;&#35328;&#35821;&#35782;&#21035;&#21644;CU MultiLang&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Robust Open-Set Spoken Language Identification and the CU MultiLang Dataset. (arXiv:2308.14951v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23454;&#29616;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#24320;&#25918;&#38598;&#35328;&#35821;&#35782;&#21035;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;MFCC&#21644;&#38899;&#39640;&#29305;&#24449;&#65292;TDNN&#27169;&#22411;&#25552;&#21462;&#29305;&#24449;&#23884;&#20837;&#65292;&#35774;&#32622;&#32622;&#20449;&#24230;&#38408;&#20540;&#65292;&#20197;&#21450;&#20351;&#29992;LDA&#21644;pLDA&#23398;&#20064;&#26032;&#30340;&#26410;&#30693;&#35821;&#35328;&#20998;&#31867;&#26469;&#23454;&#29616;&#12290;&#22312;&#32463;&#36807;&#35757;&#32451;&#30340;&#35821;&#35328;&#19978;&#65292;&#31995;&#32479;&#20934;&#30830;&#29575;&#36798;&#21040;91.76%&#65292;&#24182;&#19988;&#20855;&#22791;&#23454;&#26102;&#36866;&#24212;&#26410;&#30693;&#35821;&#35328;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#35328;&#35821;&#35782;&#21035;&#27169;&#22411;&#26159;&#38381;&#38598;&#30340;&#65292;&#21363;&#23427;&#20204;&#21482;&#33021;&#36755;&#20986;&#23427;&#20204;&#22312;&#35757;&#32451;&#26102;&#20351;&#29992;&#30340;&#31867;&#21035;&#38598;&#21512;&#20013;&#30340;&#35821;&#35328;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#24320;&#25918;&#38598;&#35328;&#35821;&#35782;&#21035;&#31995;&#32479;&#20855;&#22791;&#26816;&#27979;&#36755;&#20837;&#26159;&#21542;&#19981;&#23646;&#20110;&#21407;&#22987;&#35821;&#35328;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#23454;&#29616;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24320;&#25918;&#38598;&#35328;&#35821;&#35782;&#21035;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;MFCC&#21644;&#38899;&#39640;&#29305;&#24449;&#65292; TDNN&#27169;&#22411;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#23884;&#20837;&#65292;&#36890;&#36807;&#23545;softmax&#36755;&#20986;&#36827;&#34892;&#32622;&#20449;&#24230;&#38408;&#20540;&#22788;&#29702;&#65292;&#20197;&#21450;&#20351;&#29992;LDA&#21644;pLDA&#23398;&#20064;&#23545;&#26032;&#30340;&#26410;&#30693;&#35821;&#35328;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35328;&#35821;&#35782;&#21035;&#31995;&#32479;&#65292;&#20854;&#22312;&#32463;&#36807;&#35757;&#32451;&#30340;&#35821;&#35328;&#19978;&#23454;&#29616;&#20102;91.76%&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#20855;&#22791;&#23454;&#26102;&#36866;&#24212;&#26410;&#30693;&#35821;&#35328;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36824;&#26500;&#24314;&#20102;CU MultiLang&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#26679;&#21270;&#30340;&#22810;&#35821;&#35328;&#35821;&#38899;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#25105;&#20204;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most state-of-the-art spoken language identification models are closed-set; in other words, they can only output a language label from the set of classes they were trained on. Open-set spoken language identification systems, however, gain the ability to detect when an input exhibits none of the original languages. In this paper, we implement a novel approach to open-set spoken language identification that uses MFCC and pitch features, a TDNN model to extract meaningful feature embeddings, confidence thresholding on softmax outputs, and LDA and pLDA for learning to classify new unknown languages. We present a spoken language identification system that achieves 91.76% accuracy on trained languages and has the capability to adapt to unknown languages on the fly. To that end, we also built the CU MultiLang Dataset, a large and diverse multilingual speech corpus which was used to train and evaluate our system.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#24615;&#21035;&#20559;&#35265;&#21644;&#21051;&#26495;&#21360;&#35937;&#65292;&#23427;&#20204;&#26356;&#20542;&#21521;&#20110;&#36873;&#25321;&#19982;&#20010;&#20154;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#19968;&#33268;&#30340;&#32844;&#19994;&#65292;&#24182;&#19988;&#25918;&#22823;&#20102;&#20559;&#35265;&#65292;&#36229;&#36807;&#20102;&#29616;&#23454;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2308.14921</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#21644;&#21051;&#26495;&#21360;&#35937;
&lt;/p&gt;
&lt;p&gt;
Gender bias and stereotypes in Large Language Models. (arXiv:2308.14921v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14921
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#24615;&#21035;&#20559;&#35265;&#21644;&#21051;&#26495;&#21360;&#35937;&#65292;&#23427;&#20204;&#26356;&#20542;&#21521;&#20110;&#36873;&#25321;&#19982;&#20010;&#20154;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#19968;&#33268;&#30340;&#32844;&#19994;&#65292;&#24182;&#19988;&#25918;&#22823;&#20102;&#20559;&#35265;&#65292;&#36229;&#36807;&#20102;&#29616;&#23454;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#20010;&#26376;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#25171;&#30772;&#20102;&#26368;&#20808;&#36827;&#30340;&#27979;&#35797;&#22522;&#20934;&#12290;&#26412;&#25991;&#30740;&#31350;LLMs&#22312;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#26041;&#38754;&#30340;&#34892;&#20026;&#65292;&#36825;&#26159;&#20808;&#21069;&#27169;&#22411;&#20013;&#24050;&#30693;&#30340;&#19968;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#31616;&#21333;&#30340;&#33539;&#20363;&#26469;&#27979;&#35797;&#24615;&#21035;&#20559;&#35265;&#30340;&#23384;&#22312;&#65292;&#36825;&#19968;&#33539;&#20363;&#24314;&#31435;&#22312;&#20294;&#19982;WinoBias&#19981;&#21516;&#65292;&#21518;&#32773;&#26159;&#19968;&#20010;&#24120;&#29992;&#30340;&#24615;&#21035;&#20559;&#35265;&#25968;&#25454;&#38598;&#65292;&#24456;&#21487;&#33021;&#21253;&#21547;&#22312;&#30446;&#21069;LLMs&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#22235;&#20010;&#26368;&#36817;&#21457;&#24067;&#30340;LLMs&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#22312;&#30007;&#24615;&#21644;&#22899;&#24615;&#32844;&#19994;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#20559;&#35265;&#30340;&#20551;&#35774;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#22914;&#19979;&#65306;&#65288;a&#65289;LLMs&#22312;&#36873;&#25321;&#19982;&#20154;&#30340;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#19968;&#33268;&#30340;&#32844;&#19994;&#26102;&#30340;&#27010;&#29575;&#26159;3-6&#20493;&#65307;&#65288;b&#65289;&#36825;&#20123;&#36873;&#25321;&#19982;&#20154;&#20204;&#30340;&#24863;&#30693;&#26356;&#21152;&#19968;&#33268;&#65292;&#32780;&#19981;&#26159;&#19982;&#23448;&#26041;&#32844;&#19994;&#32479;&#35745;&#25968;&#25454;&#30340;&#30495;&#23454;&#24773;&#20917;&#19968;&#33268;&#65307;&#65288;c&#65289;&#20107;&#23454;&#19978;&#65292;LLMs&#25918;&#22823;&#20102;&#20559;&#35265;&#65292;&#36229;&#36807;&#20102;&#20154;&#20204;&#30340;&#24863;&#30693;&#25110;&#30495;&#23454;&#24773;&#20917;&#65307;&#65288;d&#65289;LLMs&#24573;&#35270;&#20102;&#20851;&#38190;&#30340;&#27495;&#20041;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have made substantial progress in the past several months, shattering state-of-the-art benchmarks in many domains. This paper investigates LLMs' behavior with respect to gender stereotypes, a known issue for prior models. We use a simple paradigm to test the presence of gender bias, building on but differing from WinoBias, a commonly used gender bias dataset, which is likely to be included in the training data of current LLMs. We test four recently published LLMs and demonstrate that they express biased assumptions about men and women's occupations. Our contributions in this paper are as follows: (a) LLMs are 3-6 times more likely to choose an occupation that stereotypically aligns with a person's gender; (b) these choices align with people's perceptions better than with the ground truth as reflected in official job statistics; (c) LLMs in fact amplify the bias beyond what is reflected in perceptions or the ground truth; (d) LLMs ignore crucial ambiguities 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#35821;&#38899;&#20869;&#23481;&#23884;&#20837;&#30340;&#31070;&#32463;&#26041;&#27861;&#12290;&#20256;&#32479;&#26041;&#27861;&#38480;&#21046;&#20102;&#24615;&#33021;&#21644;&#25928;&#29575;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#22768;&#23398;&#35789;&#23884;&#20837;&#20316;&#20026;&#26367;&#20195;&#12290;&#35770;&#25991;&#25552;&#20986;&#20102;&#21333;&#35270;&#22270;&#21644;&#22810;&#35270;&#22270;&#35757;&#32451;&#25439;&#22833;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#22768;&#23398;&#35789;&#23884;&#20837;&#22312;&#23454;&#38469;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.14905</link><description>&lt;p&gt;
&#35821;&#38899;&#20869;&#23481;&#23884;&#20837;&#30340;&#31070;&#32463;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Neural approaches to spoken content embedding. (arXiv:2308.14905v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14905
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#35821;&#38899;&#20869;&#23481;&#23884;&#20837;&#30340;&#31070;&#32463;&#26041;&#27861;&#12290;&#20256;&#32479;&#26041;&#27861;&#38480;&#21046;&#20102;&#24615;&#33021;&#21644;&#25928;&#29575;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#22768;&#23398;&#35789;&#23884;&#20837;&#20316;&#20026;&#26367;&#20195;&#12290;&#35770;&#25991;&#25552;&#20986;&#20102;&#21333;&#35270;&#22270;&#21644;&#22810;&#35270;&#22270;&#35757;&#32451;&#25439;&#22833;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#22768;&#23398;&#35789;&#23884;&#20837;&#22312;&#23454;&#38469;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27604;&#36739;&#35821;&#38899;&#29255;&#27573;&#26159;&#35821;&#38899;&#22788;&#29702;&#20013;&#30340;&#26680;&#24515;&#25805;&#20316;&#12290;&#20256;&#32479;&#26041;&#27861;&#20542;&#21521;&#20110;&#20351;&#29992;&#24103;&#32423;&#21160;&#24577;&#35268;&#21010;&#31639;&#27861;&#65288;&#22914;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#65289;&#65292;&#22240;&#20026;&#23427;&#20204;&#19981;&#38656;&#35201;&#30417;&#30563;&#65292;&#20294;&#22312;&#24615;&#33021;&#21644;&#25928;&#29575;&#19978;&#26377;&#23616;&#38480;&#24615;&#12290;&#20316;&#20026;&#26367;&#20195;&#65292;&#22768;&#23398;&#35789;&#23884;&#20837;&#8212;&#8212;&#21487;&#21464;&#38271;&#24230;&#30340;&#35821;&#38899;&#35789;&#27573;&#30340;&#22266;&#23450;&#32500;&#24230;&#21521;&#37327;&#34920;&#31034;&#8212;&#8212;&#24320;&#22987;&#34987;&#32771;&#34385;&#29992;&#20110;&#36825;&#20123;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#20165;&#38480;&#20110;&#36825;&#31181;&#37492;&#21035;&#24615;&#23884;&#20837;&#27169;&#22411;&#12289;&#35757;&#32451;&#26041;&#27861;&#20197;&#21450;&#20854;&#22312;&#23454;&#38469;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#39318;&#20808;&#32771;&#34385;&#8220;&#21333;&#35270;&#22270;&#8221;&#35757;&#32451;&#25439;&#22833;&#65292;&#30446;&#26631;&#26159;&#23398;&#20064;&#19968;&#31181;&#22768;&#23398;&#35789;&#23884;&#20837;&#27169;&#22411;&#65292;&#21487;&#20197;&#23558;&#30456;&#21516;&#35789;&#21644;&#19981;&#21516;&#35789;&#30340;&#35821;&#38899;&#27573;&#37197;&#23545;&#21306;&#20998;&#24320;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#32771;&#34385;&#8220;&#22810;&#35270;&#22270;&#8221;&#23545;&#27604;&#25439;&#22833;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#65292;&#22768;&#23398;&#35789;&#23884;&#20837;&#19982;&#23383;&#31526;&#24207;&#21015;&#30340;&#23884;&#20837;&#19968;&#36215;&#23398;&#20064;&#65292;&#20197;&#29983;&#25104;&#22522;&#20110;&#22768;&#23398;&#30340;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Comparing spoken segments is a central operation to speech processing. Traditional approaches in this area have favored frame-level dynamic programming algorithms, such as dynamic time warping, because they require no supervision, but they are limited in performance and efficiency. As an alternative, acoustic word embeddings -- fixed-dimensional vector representations of variable-length spoken word segments -- have begun to be considered for such tasks as well. However, the current space of such discriminative embedding models, training approaches, and their application to real-world downstream tasks is limited. We start by considering ``single-view" training losses where the goal is to learn an acoustic word embedding model that separates same-word and different-word spoken segment pairs. Then, we consider ``multi-view" contrastive losses. In this setting, acoustic word embeddings are learned jointly with embeddings of character sequences to generate acoustically grounded embeddings o
&lt;/p&gt;</description></item><item><title>MEMORY-VQ&#26159;&#19968;&#31181;&#20351;&#29992;&#21521;&#37327;&#37327;&#21270;&#21387;&#32553;&#26041;&#27861;&#26469;&#20943;&#23569;&#20869;&#23384;&#22686;&#24378;&#27169;&#22411;&#23384;&#20648;&#38656;&#27714;&#30340;&#26032;&#26041;&#27861;&#65292;&#19981;&#29306;&#29298;&#24615;&#33021;&#12290;&#24212;&#29992;&#20110;LUMEN&#27169;&#22411;&#21518;&#65292;LUMEN-VQ&#22312;KILT&#22522;&#20934;&#27979;&#35797;&#19978;&#33719;&#24471;&#20102;16&#20493;&#30340;&#21387;&#32553;&#29575;&#65292;&#20351;&#24471;&#23545;&#20110;&#26497;&#22823;&#30340;&#26816;&#32034;&#35821;&#26009;&#24211;&#32780;&#35328;&#23454;&#29616;&#23454;&#38469;&#30340;&#26816;&#32034;&#22686;&#24378;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.14903</link><description>&lt;p&gt;
MEMORY-VQ&#65306;&#29992;&#20110;&#21487;&#25805;&#20316;&#30340;&#20114;&#32852;&#32593;&#35268;&#27169;&#20869;&#23384;&#30340;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
MEMORY-VQ: Compression for Tractable Internet-Scale Memory. (arXiv:2308.14903v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14903
&lt;/p&gt;
&lt;p&gt;
MEMORY-VQ&#26159;&#19968;&#31181;&#20351;&#29992;&#21521;&#37327;&#37327;&#21270;&#21387;&#32553;&#26041;&#27861;&#26469;&#20943;&#23569;&#20869;&#23384;&#22686;&#24378;&#27169;&#22411;&#23384;&#20648;&#38656;&#27714;&#30340;&#26032;&#26041;&#27861;&#65292;&#19981;&#29306;&#29298;&#24615;&#33021;&#12290;&#24212;&#29992;&#20110;LUMEN&#27169;&#22411;&#21518;&#65292;LUMEN-VQ&#22312;KILT&#22522;&#20934;&#27979;&#35797;&#19978;&#33719;&#24471;&#20102;16&#20493;&#30340;&#21387;&#32553;&#29575;&#65292;&#20351;&#24471;&#23545;&#20110;&#26497;&#22823;&#30340;&#26816;&#32034;&#35821;&#26009;&#24211;&#32780;&#35328;&#23454;&#29616;&#23454;&#38469;&#30340;&#26816;&#32034;&#22686;&#24378;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#26159;&#20351;&#35821;&#35328;&#27169;&#22411;&#26356;&#21152;&#20102;&#35299;&#19990;&#30028;&#30340;&#19968;&#31181;&#24378;&#22823;&#20294;&#26114;&#36149;&#30340;&#26041;&#27861;&#12290;&#20687;LUMEN&#36825;&#26679;&#30340;&#22522;&#20110;&#20869;&#23384;&#30340;&#26041;&#27861;&#36890;&#36807;&#39044;&#35745;&#31639;&#26816;&#32034;&#21040;&#30340;&#27573;&#33853;&#30340;&#20196;&#29260;&#34920;&#31034;&#26469;&#22823;&#22823;&#21152;&#24555;&#25512;&#29702;&#36895;&#24230;&#12290;&#28982;&#32780;&#65292;&#20869;&#23384;&#20063;&#23548;&#33268;&#20102;&#26356;&#22823;&#30340;&#23384;&#20648;&#38656;&#27714;&#65292;&#29992;&#20110;&#23384;&#20648;&#39044;&#35745;&#31639;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;MEMORY-VQ&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20943;&#23569;&#20869;&#23384;&#22686;&#24378;&#27169;&#22411;&#30340;&#23384;&#20648;&#38656;&#27714;&#65292;&#21516;&#26102;&#19981;&#29306;&#29298;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VQ-VAE&#65289;&#26469;&#21387;&#32553;&#20196;&#29260;&#34920;&#31034;&#12290;&#25105;&#20204;&#23558;MEMORY-VQ&#24212;&#29992;&#20110;LUMEN&#27169;&#22411;&#65292;&#24471;&#21040;&#20102;LUMEN-VQ&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;KILT&#22522;&#20934;&#27979;&#35797;&#20013;&#20855;&#26377;&#21487;&#27604;&#24615;&#33021;&#30340;&#20869;&#23384;&#27169;&#22411;&#65292;&#21387;&#32553;&#29575;&#20026;16&#20493;&#12290;LUMEN-VQ&#20351;&#24471;&#21363;&#20351;&#23545;&#20110;&#38750;&#24120;&#22823;&#30340;&#26816;&#32034;&#35821;&#26009;&#24211;&#65292;&#23454;&#38469;&#30340;&#26816;&#32034;&#22686;&#24378;&#20063;&#21464;&#24471;&#21487;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrieval augmentation is a powerful but expensive method to make language models more knowledgeable about the world. Memory-based methods like LUMEN pre-compute token representations for retrieved passages to drastically speed up inference. However, memory also leads to much greater storage requirements from storing pre-computed representations.  We propose MEMORY-VQ, a new method to reduce storage requirements of memory-augmented models without sacrificing performance. Our method uses a vector quantization variational autoencoder (VQ-VAE) to compress token representations. We apply MEMORY-VQ to the LUMEN model to obtain LUMEN-VQ, a memory model that achieves a 16x compression rate with comparable performance on the KILT benchmark. LUMEN-VQ enables practical retrieval augmentation even for extremely large retrieval corpora.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35821;&#38899;&#24773;&#32490;&#35782;&#21035;&#30340;&#22810;&#23610;&#24230;&#23545;&#35805;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#32039;&#24613;&#21628;&#21483;&#20013;&#24515;&#23545;&#35805;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;&#36890;&#36807;&#20998;&#26512;&#23545;&#35805;&#20013;&#30340;&#24773;&#32490;&#27969;&#21160;&#24182;&#21033;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#24773;&#32490;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.14894</link><description>&lt;p&gt;
&#29992;&#20110;&#32039;&#24613;&#21628;&#21483;&#20013;&#24515;&#23545;&#35805;&#20013;&#30340;&#35821;&#38899;&#24773;&#32490;&#35782;&#21035;&#30340;&#22810;&#23610;&#24230;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multiscale Contextual Learning for Speech Emotion Recognition in Emergency Call Center Conversations. (arXiv:2308.14894v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14894
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35821;&#38899;&#24773;&#32490;&#35782;&#21035;&#30340;&#22810;&#23610;&#24230;&#23545;&#35805;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#32039;&#24613;&#21628;&#21483;&#20013;&#24515;&#23545;&#35805;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;&#36890;&#36807;&#20998;&#26512;&#23545;&#35805;&#20013;&#30340;&#24773;&#32490;&#27969;&#21160;&#24182;&#21033;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#24773;&#32490;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#20013;&#30340;&#24773;&#32490;&#35782;&#21035;&#23545;&#20110;&#30830;&#20445;&#20808;&#36827;&#30340;&#20154;&#26426;&#20132;&#20114;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#29983;&#27963;&#20013;&#21019;&#24314;&#31283;&#20581;&#20934;&#30830;&#30340;&#24773;&#32490;&#35782;&#21035;&#31995;&#32479;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#37326;&#22806;&#24773;&#32490;&#25968;&#25454;&#38598;&#30340;&#31232;&#32570;&#24615;&#20197;&#21450;&#26080;&#27861;&#32771;&#34385;&#23545;&#35805;&#19978;&#19979;&#25991;&#12290;CEMO&#25968;&#25454;&#38598;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#65292;&#35813;&#25968;&#25454;&#38598;&#30001;&#27861;&#22269;&#21628;&#21483;&#20013;&#24515;&#30340;&#20195;&#29702;&#20154;&#21644;&#24739;&#32773;&#20043;&#38388;&#30340;&#32039;&#24613;&#21628;&#21483;&#23545;&#35805;&#32452;&#25104;&#12290;&#36825;&#20123;&#20114;&#21160;&#30340;&#24615;&#36136;&#31361;&#20986;&#20102;&#23545;&#35805;&#20013;&#24773;&#32490;&#27969;&#30340;&#20316;&#29992;&#65292;&#21487;&#20197;&#22312;&#29702;&#35299;&#23454;&#38469;&#24863;&#21463;&#26041;&#38754;&#36890;&#24120;&#20135;&#29983;&#24046;&#24322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#23545;&#35805;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#35821;&#38899;&#24773;&#32490;&#35782;&#21035;&#65292;&#21033;&#29992;&#20102;&#36825;&#20010;&#20551;&#35774;&#12290;&#25105;&#20204;&#22312;&#35821;&#38899;&#36716;&#24405;&#21644;&#22768;&#23398;&#29255;&#27573;&#19978;&#23545;&#36825;&#31181;&#26041;&#27861;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#23454;&#39564;&#26041;&#27861;&#20351;&#29992;&#20102;&#30446;&#26631;&#29255;&#27573;&#30340;&#21069;&#19968;&#27573;&#25110;&#21518;&#19968;&#27573;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion recognition in conversations is essential for ensuring advanced human-machine interactions. However, creating robust and accurate emotion recognition systems in real life is challenging, mainly due to the scarcity of emotion datasets collected in the wild and the inability to take into account the dialogue context. The CEMO dataset, composed of conversations between agents and patients during emergency calls to a French call center, fills this gap. The nature of these interactions highlights the role of the emotional flow of the conversation in predicting patient emotions, as context can often make a difference in understanding actual feelings. This paper presents a multi-scale conversational context learning approach for speech emotion recognition, which takes advantage of this hypothesis. We investigated this approach on both speech transcriptions and acoustic segments. Experimentally, our method uses the previous or next information of the targeted segment. In the text domai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25991;&#26412;&#32553;&#25918;&#26041;&#27861;CommunityFish&#65292;&#23427;&#21033;&#29992;&#23618;&#27425;&#32858;&#31867;&#31639;&#27861;&#22312;&#35789;&#31354;&#38388;&#19978;&#32858;&#31867;&#65292;&#20174;&#32780;&#25581;&#31034;&#25991;&#26412;&#25968;&#25454;&#20013;&#29420;&#31435;&#35789;&#32452;&#65288;&#31038;&#21306;&#65289;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2308.14873</link><description>&lt;p&gt;
CommunityFish: &#19968;&#31181;&#22522;&#20110;&#27850;&#26494;&#20998;&#24067;&#30340;&#23618;&#27425;&#32858;&#31867;&#25991;&#26412;&#32553;&#25918;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CommunityFish: A Poisson-based Document Scaling With Hierarchical Clustering. (arXiv:2308.14873v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14873
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25991;&#26412;&#32553;&#25918;&#26041;&#27861;CommunityFish&#65292;&#23427;&#21033;&#29992;&#23618;&#27425;&#32858;&#31867;&#31639;&#27861;&#22312;&#35789;&#31354;&#38388;&#19978;&#32858;&#31867;&#65292;&#20174;&#32780;&#25581;&#31034;&#25991;&#26412;&#25968;&#25454;&#20013;&#29420;&#31435;&#35789;&#32452;&#65288;&#31038;&#21306;&#65289;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#32553;&#25918;&#26159;&#31038;&#20250;&#31185;&#23398;&#23478;&#21644;&#25919;&#27835;&#30740;&#31350;&#20154;&#21592;&#22312;&#25991;&#26412;&#25968;&#25454;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#20063;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25991;&#26412;&#32553;&#25918;&#26041;&#27861;CommunityFish&#65292;&#23427;&#22522;&#20110;&#23618;&#27425;&#32858;&#31867;&#31639;&#27861;Louvain&#65292;&#22312;&#35789;&#31354;&#38388;&#19978;&#36827;&#34892;&#32858;&#31867;&#65292;&#36890;&#36807;&#35782;&#21035;&#20849;&#29616;&#22312;&#25991;&#26723;&#20013;&#30340;&#29420;&#31435;&#35789;&#32452;&#65288;&#31216;&#20026;&#31038;&#21306;&#65289;&#65292;&#20174;&#32780;&#25581;&#31034;&#28436;&#35762;&#32773;&#25110;&#25919;&#20826;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document scaling has been a key component in text-as-data applications for social scientists and a major field of interest for political researchers, who aim at uncovering differences between speakers or parties with the help of different probabilistic and non-probabilistic approaches. Yet, most of these techniques are either built upon the agnostically bag-of-word hypothesis or use prior information borrowed from external sources that might embed the results with a significant bias. If the corpus has long been considered as a collection of documents, it can also be seen as a dense network of connected words whose structure could be clustered to differentiate independent groups of words, based on their co-occurrences in documents, known as communities. This paper introduces CommunityFish as an augmented version of Wordfish based on a hierarchical clustering, namely the Louvain algorithm, on the word space to yield communities as semantic and independent n-grams emerging from the corpus
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;Attention Visualizer&#21253;&#65292;&#36890;&#36807;&#21487;&#35270;&#21270;&#23637;&#31034;&#21333;&#35789;&#22312;&#32534;&#30721;&#22120;-&#21482;&#26377;&#30340;Transformer&#27169;&#22411;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#39640;&#20102;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#37322;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.14850</link><description>&lt;p&gt;
Attention Visualizer Package:&#25581;&#31034;&#32534;&#30721;&#22120;-&#21482;&#26377;&#30340;Transformer&#27169;&#22411;&#20013;&#21333;&#35789;&#37325;&#35201;&#24615;&#30340;&#27880;&#24847;&#21147;&#21487;&#35270;&#21270;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Attention Visualizer Package: Revealing Word Importance for Deeper Insight into Encoder-Only Transformer Models. (arXiv:2308.14850v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14850
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;Attention Visualizer&#21253;&#65292;&#36890;&#36807;&#21487;&#35270;&#21270;&#23637;&#31034;&#21333;&#35789;&#22312;&#32534;&#30721;&#22120;-&#21482;&#26377;&#30340;Transformer&#27169;&#22411;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#39640;&#20102;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#37322;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Attention Visualizer&#21253;&#65292;&#35813;&#21253;&#29992;&#20110;&#35270;&#35273;&#21270;&#23637;&#31034;&#32534;&#30721;&#22120;-&#21482;&#26377;&#30340;Transformer&#27169;&#22411;&#20013;&#20010;&#21035;&#21333;&#35789;&#30340;&#37325;&#35201;&#24615;&#12290;&#19982;&#20854;&#20182;&#20851;&#27880;&#26631;&#35760;&#21644;&#33258;&#27880;&#24847;&#21147;&#20998;&#25968;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#30740;&#31350;&#21333;&#35789;&#21450;&#20854;&#23545;&#26368;&#32456;&#23884;&#20837;&#34920;&#31034;&#30340;&#24433;&#21709;&#12290;&#36825;&#26679;&#30340;&#24211;&#22312;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#37322;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#23427;&#20204;&#25552;&#20379;&#20102;&#20102;&#35299;&#20854;&#20869;&#37096;&#26426;&#21046;&#12289;&#25552;&#39640;&#20854;&#24615;&#33021;&#30340;&#26356;&#22909;&#29702;&#35299;&#30340;&#26426;&#20250;&#12290;&#24744;&#21487;&#20197;&#35775;&#38382;&#20197;&#19979;GitHub&#23384;&#20648;&#24211;&#33719;&#21462;&#20195;&#30721;&#24182;&#26597;&#30475;&#31034;&#20363;&#65306;https://github.com/AlaFalaki/AttentionVisualizer&#12290;
&lt;/p&gt;
&lt;p&gt;
This report introduces the Attention Visualizer package, which is crafted to visually illustrate the significance of individual words in encoder-only transformer-based models. In contrast to other methods that center on tokens and self-attention scores, our approach will examine the words and their impact on the final embedding representation. Libraries like this play a crucial role in enhancing the interpretability and explainability of neural networks. They offer the opportunity to illuminate their internal mechanisms, providing a better understanding of how they operate and can be enhanced. You can access the code and review examples on the following GitHub repository: https://github.com/AlaFalaki/AttentionVisualizer.
&lt;/p&gt;</description></item><item><title>VoiceBank-2023&#26159;&#29992;&#20110;&#26500;&#24314;&#20010;&#24615;&#21270;&#20013;&#25991;TTS&#31995;&#32479;&#30340;&#22810;&#35828;&#35805;&#20154;&#26222;&#36890;&#35805;&#35821;&#38899;&#35821;&#26009;&#24211;&#65292;&#21487;&#20197;&#20026;&#35821;&#35328;&#38556;&#30861;&#24739;&#32773;&#25552;&#20379;&#20010;&#24615;&#21270;&#26381;&#21153;&#12290;&#35813;&#35821;&#26009;&#24211;&#21253;&#25324;111&#20010;&#35828;&#26222;&#36890;&#35805;&#30340;&#35828;&#35805;&#20154;&#30340;29.78&#23567;&#26102;&#30340;&#35821;&#38899;&#25968;&#25454;&#65292;&#20197;&#21450;&#24615;&#21035;&#12289;&#35821;&#35328;&#38556;&#30861;&#31243;&#24230;&#31561;&#20449;&#24687;&#12290;&#35821;&#26009;&#24211;&#27426;&#36814;&#38750;&#21830;&#19994;&#29992;&#36884;&#30003;&#35831;&#20351;&#29992;&#65292;&#25903;&#25345;&#25913;&#36827;VoiceBanking&#39033;&#30446;&#30340;&#26381;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.14763</link><description>&lt;p&gt;
VoiceBank-2023&#65306;&#19968;&#20010;&#29992;&#20110;&#26500;&#24314;&#38754;&#21521;&#35821;&#35328;&#38556;&#30861;&#24739;&#32773;&#30340;&#20010;&#24615;&#21270;&#20013;&#25991;TTS&#31995;&#32479;&#30340;&#22810;&#35828;&#35805;&#20154;&#26222;&#36890;&#35805;&#35821;&#38899;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
VoiceBank-2023: A Multi-Speaker Mandarin Speech Corpus for Constructing Personalized TTS Systems for the Speech Impaired. (arXiv:2308.14763v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14763
&lt;/p&gt;
&lt;p&gt;
VoiceBank-2023&#26159;&#29992;&#20110;&#26500;&#24314;&#20010;&#24615;&#21270;&#20013;&#25991;TTS&#31995;&#32479;&#30340;&#22810;&#35828;&#35805;&#20154;&#26222;&#36890;&#35805;&#35821;&#38899;&#35821;&#26009;&#24211;&#65292;&#21487;&#20197;&#20026;&#35821;&#35328;&#38556;&#30861;&#24739;&#32773;&#25552;&#20379;&#20010;&#24615;&#21270;&#26381;&#21153;&#12290;&#35813;&#35821;&#26009;&#24211;&#21253;&#25324;111&#20010;&#35828;&#26222;&#36890;&#35805;&#30340;&#35828;&#35805;&#20154;&#30340;29.78&#23567;&#26102;&#30340;&#35821;&#38899;&#25968;&#25454;&#65292;&#20197;&#21450;&#24615;&#21035;&#12289;&#35821;&#35328;&#38556;&#30861;&#31243;&#24230;&#31561;&#20449;&#24687;&#12290;&#35821;&#26009;&#24211;&#27426;&#36814;&#38750;&#21830;&#19994;&#29992;&#36884;&#30003;&#35831;&#20351;&#29992;&#65292;&#25903;&#25345;&#25913;&#36827;VoiceBanking&#39033;&#30446;&#30340;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#26222;&#36890;&#35805;&#24739;&#26377;&#35821;&#35328;&#38556;&#30861;&#30340;&#20010;&#24615;&#21270;TTS&#31995;&#32479;&#26381;&#21153;&#24456;&#23569;&#34987;&#25552;&#21450;&#12290;&#21488;&#28286;&#22312;2020&#24180;&#21551;&#21160;&#20102;VoiceBanking&#39033;&#30446;&#65292;&#26088;&#22312;&#24314;&#31435;&#19968;&#20010;&#23436;&#25972;&#30340;&#26381;&#21153;&#20307;&#31995;&#65292;&#20026;&#32908;&#33806;&#32553;&#20391;&#32034;&#30828;&#21270;&#30151;&#24739;&#32773;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#26222;&#36890;&#35805;TTS&#31995;&#32479;&#12290;&#26412;&#25991;&#25253;&#21578;&#20102;VoiceBanking&#39033;&#30446;&#30340;&#35821;&#26009;&#24211;&#35774;&#35745;&#12289;&#35821;&#26009;&#24211;&#24405;&#21046;&#12289;&#25968;&#25454;&#28165;&#29702;&#21644;&#26657;&#27491;&#20197;&#21450;&#20010;&#24615;&#21270;TTS&#31995;&#32479;&#30340;&#35780;&#20272;&#12290;&#35813;&#35821;&#26009;&#24211;&#34987;&#21629;&#21517;&#20026;VoiceBank-2023&#35821;&#38899;&#35821;&#26009;&#24211;&#65292;&#22240;&#20026;&#23427;&#30340;&#21457;&#24067;&#24180;&#20221;&#26159;2023&#24180;&#12290;&#35821;&#26009;&#24211;&#21253;&#25324;111&#20010;&#20197;&#26222;&#36890;&#35805;&#20026;&#27597;&#35821;&#30340;&#35828;&#35805;&#20154;&#26391;&#35835;&#30340;&#30701;&#27573;&#33853;&#21644;&#24120;&#29992;&#30701;&#35821;&#30340;&#35821;&#38899;&#65292;&#20849;&#35745;29.78&#23567;&#26102;&#12290;&#35821;&#26009;&#24211;&#36824;&#24102;&#26377;&#20851;&#20110;&#24615;&#21035;&#12289;&#35821;&#35328;&#38556;&#30861;&#31243;&#24230;&#12289;&#29992;&#25143;&#31867;&#22411;&#12289;&#36716;&#24405;&#12289;&#20449;&#22122;&#27604;&#21644;&#35828;&#35805;&#36895;&#24230;&#30340;&#20449;&#24687;&#26631;&#27880;&#12290;VoiceBank-2023&#35821;&#26009;&#24211;&#21487;&#20379;&#38750;&#21830;&#19994;&#29992;&#36884;&#30003;&#35831;&#20351;&#29992;&#65292;&#24182;&#27426;&#36814;&#21508;&#26041;&#21152;&#20837;VoiceBanking&#39033;&#30446;&#20197;&#25913;&#21892;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Services of personalized TTS systems for the Mandarin-speaking speech impaired are rarely mentioned. Taiwan started the VoiceBanking project in 2020, aiming to build a complete set of services to deliver personalized Mandarin TTS systems to amyotrophic lateral sclerosis patients. This paper reports the corpus design, corpus recording, data purging and correction for the corpus, and evaluations of the developed personalized TTS systems, for the VoiceBanking project. The developed corpus is named after the VoiceBank-2023 speech corpus because of its release year. The corpus contains 29.78 hours of utterances with prompts of short paragraphs and common phrases spoken by 111 native Mandarin speakers. The corpus is labeled with information about gender, degree of speech impairment, types of users, transcription, SNRs, and speaking rates. The VoiceBank-2023 is available by request for non-commercial use and welcomes all parties to join the VoiceBanking project to improve the services for the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22522;&#20110;GPT-3&#30340;&#27169;&#22411;&#36827;&#34892;&#21307;&#23398;&#38382;&#31572;&#31995;&#32479;&#65288;MedQA&#65289;&#30340;&#25361;&#25112;&#21644;&#39118;&#38505;&#12290;&#36890;&#36807;&#19978;&#19979;&#25991;&#21270;&#30340;&#20998;&#26512;&#21644;&#25163;&#21160;&#35774;&#35745;&#24739;&#32773;&#26597;&#35810;&#30340;&#21387;&#21147;&#27979;&#35797;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#26080;&#27861;&#20805;&#20998;&#22238;&#24212;&#39640;&#39118;&#38505;&#38480;&#21046;&#65292;&#21487;&#33021;&#23548;&#33268;&#38169;&#35823;&#30340;&#21307;&#23398;&#20449;&#24687;&#12289;&#19981;&#23433;&#20840;&#30340;&#24314;&#35758;&#21644;&#20882;&#29359;&#24615;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2308.14641</link><description>&lt;p&gt;
&#22522;&#20110;GPT-3&#30340;&#21307;&#30103;&#23545;&#35805;&#20195;&#29702;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Challenges of GPT-3-based Conversational Agents for Healthcare. (arXiv:2308.14641v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22522;&#20110;GPT-3&#30340;&#27169;&#22411;&#36827;&#34892;&#21307;&#23398;&#38382;&#31572;&#31995;&#32479;&#65288;MedQA&#65289;&#30340;&#25361;&#25112;&#21644;&#39118;&#38505;&#12290;&#36890;&#36807;&#19978;&#19979;&#25991;&#21270;&#30340;&#20998;&#26512;&#21644;&#25163;&#21160;&#35774;&#35745;&#24739;&#32773;&#26597;&#35810;&#30340;&#21387;&#21147;&#27979;&#35797;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#26080;&#27861;&#20805;&#20998;&#22238;&#24212;&#39640;&#39118;&#38505;&#38480;&#21046;&#65292;&#21487;&#33021;&#23548;&#33268;&#38169;&#35823;&#30340;&#21307;&#23398;&#20449;&#24687;&#12289;&#19981;&#23433;&#20840;&#30340;&#24314;&#35758;&#21644;&#20882;&#29359;&#24615;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#30103;&#39046;&#22495;&#30340;&#23545;&#35805;&#20195;&#29702;&#20855;&#26377;&#25552;&#20379;&#24739;&#32773;&#26356;&#24555;&#20449;&#24687;&#35775;&#38382;&#30340;&#28508;&#21147;&#65292;&#21516;&#26102;&#35753;&#21307;&#23398;&#19987;&#23478;&#19987;&#27880;&#20110;&#20851;&#38190;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25972;&#21512;&#21040;&#36825;&#20123;&#20195;&#29702;&#20013;&#23384;&#22312;&#19968;&#23450;&#30340;&#38480;&#21046;&#65292;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#21518;&#26524;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21307;&#23398;&#38382;&#31572;&#65288;MedQA&#65289;&#31995;&#32479;&#20013;&#20351;&#29992;&#22522;&#20110;GPT-3&#27169;&#22411;&#30340;&#25361;&#25112;&#21644;&#39118;&#38505;&#12290;&#25105;&#20204;&#23545;&#20960;&#20010;&#35780;&#20272;&#36827;&#34892;&#20102;&#19978;&#19979;&#25991;&#21270;&#30340;&#20998;&#26512;&#65292;&#22522;&#20110;&#26631;&#20934;&#30340;&#21307;&#23398;&#21407;&#21017;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25163;&#21160;&#35774;&#35745;&#24739;&#32773;&#26597;&#35810;&#30340;&#36807;&#31243;&#65292;&#20197;&#23545;LLMs&#22312;MedQA&#31995;&#32479;&#20013;&#30340;&#39640;&#39118;&#38505;&#38480;&#21046;&#36827;&#34892;&#21387;&#21147;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;LLMs&#26080;&#27861;&#23545;&#36825;&#20123;&#26597;&#35810;&#20570;&#20986;&#20805;&#20998;&#30340;&#22238;&#24212;&#65292;&#20250;&#29983;&#25104;&#38169;&#35823;&#30340;&#21307;&#23398;&#20449;&#24687;&#12289;&#19981;&#23433;&#20840;&#30340;&#24314;&#35758;&#20197;&#21450;&#21487;&#33021;&#34987;&#35748;&#20026;&#26159;&#20882;&#29359;&#24615;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
The potential to provide patients with faster information access while allowing medical specialists to concentrate on critical tasks makes medical domain dialog agents appealing. However, the integration of large-language models (LLMs) into these agents presents certain limitations that may result in serious consequences. This paper investigates the challenges and risks of using GPT-3-based models for medical question-answering (MedQA). We perform several evaluations contextualized in terms of standard medical principles. We provide a procedure for manually designing patient queries to stress-test high-risk limitations of LLMs in MedQA systems. Our analysis reveals that LLMs fail to respond adequately to these queries, generating erroneous medical information, unsafe recommendations, and content that may be considered offensive.
&lt;/p&gt;</description></item><item><title>chatGPT ADA&#26159;&#19968;&#31181;&#33021;&#22815;&#33258;&#20027;&#24320;&#21457;&#20020;&#24202;&#30740;&#31350;&#25152;&#38656;&#30340;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#23558;&#39640;&#32423;&#20998;&#26512;&#24037;&#20855;&#27665;&#20027;&#21270;&#65292;&#20351;&#38750;&#25968;&#25454;&#31185;&#23398;&#23478;&#30340;&#20020;&#24202;&#21307;&#29983;&#33021;&#22815;&#36731;&#26494;&#24212;&#29992;&#20110;&#21307;&#23398;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2308.14120</link><description>&lt;p&gt;
&#25480;&#26435;&#20020;&#24202;&#21307;&#29983;&#24182;&#27665;&#20027;&#21270;&#25968;&#25454;&#31185;&#23398;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#21270;&#20020;&#24202;&#30740;&#31350;&#30340;&#26426;&#22120;&#23398;&#20064;&#12290; (arXiv:2308.14120v2 [cs.LG] &#26356;&#26032;&#29256;)
&lt;/p&gt;
&lt;p&gt;
Empowering Clinicians and Democratizing Data Science: Large Language Models Automate Machine Learning for Clinical Studies. (arXiv:2308.14120v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14120
&lt;/p&gt;
&lt;p&gt;
chatGPT ADA&#26159;&#19968;&#31181;&#33021;&#22815;&#33258;&#20027;&#24320;&#21457;&#20020;&#24202;&#30740;&#31350;&#25152;&#38656;&#30340;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#23558;&#39640;&#32423;&#20998;&#26512;&#24037;&#20855;&#27665;&#20027;&#21270;&#65292;&#20351;&#38750;&#25968;&#25454;&#31185;&#23398;&#23478;&#30340;&#20020;&#24202;&#21307;&#29983;&#33021;&#22815;&#36731;&#26494;&#24212;&#29992;&#20110;&#21307;&#23398;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24320;&#21457;&#32773;&#65288;&#22914;&#25968;&#25454;&#31185;&#23398;&#23478;&#65289;&#21644;&#20174;&#19994;&#32773;&#65288;&#22914;&#20020;&#24202;&#21307;&#29983;&#65289;&#20043;&#38388;&#23384;&#22312;&#30693;&#35782;&#24046;&#36317;&#65292;&#38459;&#30861;&#20102;ML&#22312;&#20020;&#24202;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#20805;&#20998;&#21033;&#29992;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;chatGPT Advanced Data Analysis&#65288;ADA&#65289;&#65292;&#21363;GPT-4&#30340;&#25193;&#23637;&#65292;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#24182;&#39640;&#25928;&#25191;&#34892;ML&#20998;&#26512;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#21521;chatGPT ADA&#25552;&#20379;&#20102;&#21508;&#31181;&#21307;&#23398;&#19987;&#19994;&#30340;&#22823;&#22411;&#35797;&#39564;&#30340;&#30495;&#23454;&#20020;&#24202;&#25968;&#25454;&#21644;&#30740;&#31350;&#35814;&#32454;&#20449;&#24687;&#65292;&#27809;&#26377;&#32473;&#20986;&#20855;&#20307;&#25351;&#23548;&#12290;ChatGPT ADA&#22522;&#20110;&#21407;&#22987;&#30740;&#31350;&#30340;&#35757;&#32451;&#25968;&#25454;&#33258;&#20027;&#24320;&#21457;&#20102;&#26368;&#20808;&#36827;&#30340;ML&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#20020;&#24202;&#32467;&#26524;&#65292;&#22914;&#30284;&#30151;&#21457;&#23637;&#12289;&#30284;&#30151;&#36827;&#23637;&#12289;&#30142;&#30149;&#24182;&#21457;&#30151;&#25110;&#33268;&#30149;&#22522;&#22240;&#24207;&#21015;&#31561;&#29983;&#29289;&#26631;&#24535;&#29289;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#36825;&#20123;ML&#27169;&#22411;&#19982;&#20854;&#24050;&#21457;&#34920;&#30340;&#23545;&#24212;&#29289;&#30456;&#21305;&#37197;&#29978;&#33267;&#34920;&#29616;&#26356;&#22909;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;chatGPT ADA&#20026;&#27665;&#20027;&#21270;&#21307;&#23398;&#20013;&#30340;ML&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#36884;&#24452;&#65292;&#20351;&#38750;ML&#19987;&#23478;&#33021;&#22815;&#33719;&#24471;&#20808;&#36827;&#30340;&#20998;&#26512;&#24037;&#20855;&#24182;&#25512;&#21160;&#24191;&#27867;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
A knowledge gap persists between Machine Learning (ML) developers (e.g., data scientists) and practitioners (e.g., clinicians), hampering the full utilization of ML for clinical data analysis. We investigated the potential of the chatGPT Advanced Data Analysis (ADA), an extension of GPT-4, to bridge this gap and perform ML analyses efficiently. Real-world clinical datasets and study details from large trials across various medical specialties were presented to chatGPT ADA without specific guidance. ChatGPT ADA autonomously developed state-of-the-art ML models based on the original study's training data to predict clinical outcomes such as cancer development, cancer progression, disease complications, or biomarkers such as pathogenic gene sequences. Strikingly, these ML models matched or outperformed their published counterparts. We conclude that chatGPT ADA offers a promising avenue to democratize ML in medicine, making advanced analytics accessible to non-ML experts and promoting broa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24378;&#35843;&#20102;&#23558;&#25991;&#26723;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#26356;&#25509;&#36817;&#20110;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#30340;&#38656;&#27714;&#65292;&#36890;&#36807;&#25552;&#20986;&#22810;&#39029;&#25991;&#26723;&#20998;&#31867;&#25968;&#25454;&#38598;&#21644;&#19981;&#21516;&#20998;&#31867;&#20219;&#21153;&#65292;&#20197;&#21450;&#39640;&#25928;&#30340;&#22810;&#39029;&#25991;&#26723;&#34920;&#31034;&#65292;&#26469;&#35299;&#20915;&#29616;&#26377;&#22522;&#20934;&#27979;&#35797;&#19981;&#36866;&#29992;&#20110;&#23454;&#38469;&#23436;&#25972;&#25991;&#26723;&#35780;&#20272;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.12896</link><description>&lt;p&gt;
&#36229;&#36234;&#25991;&#26723;&#39029;&#20998;&#31867;&#65306;&#35774;&#35745;&#12289;&#25968;&#25454;&#38598;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Beyond Document Page Classification: Design, Datasets, and Challenges. (arXiv:2308.12896v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12896
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24378;&#35843;&#20102;&#23558;&#25991;&#26723;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#26356;&#25509;&#36817;&#20110;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#30340;&#38656;&#27714;&#65292;&#36890;&#36807;&#25552;&#20986;&#22810;&#39029;&#25991;&#26723;&#20998;&#31867;&#25968;&#25454;&#38598;&#21644;&#19981;&#21516;&#20998;&#31867;&#20219;&#21153;&#65292;&#20197;&#21450;&#39640;&#25928;&#30340;&#22810;&#39029;&#25991;&#26723;&#34920;&#31034;&#65292;&#26469;&#35299;&#20915;&#29616;&#26377;&#22522;&#20934;&#27979;&#35797;&#19981;&#36866;&#29992;&#20110;&#23454;&#38469;&#23436;&#25972;&#25991;&#26723;&#35780;&#20272;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24378;&#35843;&#20102;&#23558;&#25991;&#26723;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#26356;&#25509;&#36817;&#20110;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#30340;&#38656;&#27714;&#65292;&#21363;&#22312;&#27979;&#35797;&#25968;&#25454;&#30340;&#24615;&#36136;&#19978;&#65288;$X$&#65306;&#22810;&#36890;&#36947;&#12289;&#22810;&#39029;&#12289;&#22810;&#34892;&#19994;&#65307;$Y$&#65306;&#31867;&#21035;&#20998;&#24067;&#21644;&#26631;&#31614;&#38598;&#30340;&#22810;&#26679;&#24615;&#65289;&#21644;&#32771;&#34385;&#30340;&#20998;&#31867;&#20219;&#21153;&#19978;&#65288;$f$&#65306;&#22810;&#39029;&#25991;&#26723;&#12289;&#39029;&#38754;&#27969;&#21644;&#25991;&#26723;&#25414;&#32465;&#20998;&#31867;&#65292;...&#65289;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#20844;&#20849;&#30340;&#22810;&#39029;&#25991;&#26723;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#32570;&#20047;&#65292;&#24182;&#35268;&#33539;&#20102;&#24212;&#29992;&#22330;&#26223;&#20013;&#20135;&#29983;&#30340;&#19981;&#21516;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#28608;&#21457;&#20102;&#20197;&#39640;&#25928;&#30340;&#22810;&#39029;&#25991;&#26723;&#34920;&#31034;&#20026;&#30446;&#26631;&#30340;&#20215;&#20540;&#12290;&#23545;&#25552;&#20986;&#30340;&#22810;&#39029;&#25991;&#26723;&#20998;&#31867;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#24050;&#32463;&#21464;&#24471;&#26080;&#20851;&#32039;&#35201;&#65292;&#24182;&#38656;&#35201;&#26356;&#26032;&#20197;&#35780;&#20272;&#23454;&#38469;&#20013;&#33258;&#28982;&#21457;&#29983;&#30340;&#23436;&#25972;&#25991;&#26723;&#12290;&#36825;&#20010;&#29616;&#23454;&#24773;&#20917;&#26816;&#26597;&#20063;&#21628;&#21505;&#26356;&#25104;&#29087;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#28085;&#30422;&#26657;&#20934;&#35780;&#20272;&#12289;&#25512;&#29702;&#22797;&#26434;&#24615;&#65288;&#26102;&#38388;-&#20869;&#23384;&#65289;&#21644;&#19968;&#31995;&#21015;&#29616;&#23454;&#20998;&#25955;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper highlights the need to bring document classification benchmarking closer to real-world applications, both in the nature of data tested ($X$: multi-channel, multi-paged, multi-industry; $Y$: class distributions and label set variety) and in classification tasks considered ($f$: multi-page document, page stream, and document bundle classification, ...). We identify the lack of public multi-page document classification datasets, formalize different classification tasks arising in application scenarios, and motivate the value of targeting efficient multi-page document representations. An experimental study on proposed multi-page document classification datasets demonstrates that current benchmarks have become irrelevant and need to be updated to evaluate complete documents, as they naturally occur in practice. This reality check also calls for more mature evaluation methodologies, covering calibration evaluation, inference complexity (time-memory), and a range of realistic distr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#20013;&#21476;&#39640;&#22320;&#24503;&#35821;&#21644;&#29616;&#20195;&#24503;&#35821;&#30340;&#35821;&#35328;&#36830;&#32493;&#24615;&#21644;&#32467;&#26500;&#30456;&#20284;&#24615;&#65292;&#20197;&#21450;&#29616;&#26377;&#30340;&#29616;&#20195;&#24503;&#35821;&#26641;&#24211;&#36164;&#28304;&#65292;&#26500;&#24314;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20013;&#21476;&#39640;&#22320;&#24503;&#35821;&#30340;&#30701;&#35821;&#32467;&#26500;&#20998;&#26512;&#22120;&#65292;&#26080;&#38656;&#20381;&#36182;&#26631;&#27880;&#30340;MHG&#26641;&#24211;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2308.04645</link><description>&lt;p&gt;
&#20013;&#21476;&#39640;&#22320;&#24503;&#35821;&#30340;&#36328;&#35821;&#35328;&#30701;&#35821;&#32467;&#26500;&#20998;&#26512;&#65306;&#19968;&#31181;&#21435;&#35789;&#27861;&#21270;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Cross-Lingual Constituency Parsing for Middle High German: A Delexicalized Approach. (arXiv:2308.04645v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04645
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#20013;&#21476;&#39640;&#22320;&#24503;&#35821;&#21644;&#29616;&#20195;&#24503;&#35821;&#30340;&#35821;&#35328;&#36830;&#32493;&#24615;&#21644;&#32467;&#26500;&#30456;&#20284;&#24615;&#65292;&#20197;&#21450;&#29616;&#26377;&#30340;&#29616;&#20195;&#24503;&#35821;&#26641;&#24211;&#36164;&#28304;&#65292;&#26500;&#24314;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20013;&#21476;&#39640;&#22320;&#24503;&#35821;&#30340;&#30701;&#35821;&#32467;&#26500;&#20998;&#26512;&#22120;&#65292;&#26080;&#38656;&#20381;&#36182;&#26631;&#27880;&#30340;MHG&#26641;&#24211;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30701;&#35821;&#32467;&#26500;&#20998;&#26512;&#22312;&#25512;&#21160;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#36215;&#30528;&#22522;&#30784;&#24615;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#20165;&#20381;&#38752;&#26631;&#27880;&#30340;&#35299;&#26512;&#25968;&#25454;&#35757;&#32451;&#21476;&#20195;&#35821;&#35328;&#30340;&#33258;&#21160;&#21477;&#27861;&#20998;&#26512;&#31995;&#32479;&#26159;&#19968;&#39033;&#33392;&#24040;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#26500;&#24314;&#36825;&#20123;&#35821;&#35328;&#30340;&#26641;&#24211;&#23384;&#22312;&#22266;&#26377;&#25361;&#25112;&#12290;&#36825;&#38656;&#35201;&#20016;&#23500;&#30340;&#35821;&#35328;&#19987;&#19994;&#30693;&#35782;&#65292;&#23548;&#33268;&#21487;&#29992;&#36164;&#28304;&#30340;&#31232;&#32570;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38556;&#30861;&#65292;&#36328;&#35821;&#35328;&#36716;&#31227;&#25216;&#26415;&#20026;&#20302;&#36164;&#28304;&#30446;&#26631;&#35821;&#35328;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#20123;&#25216;&#26415;&#38656;&#35201;&#26368;&#23569;&#29978;&#33267;&#27809;&#26377;&#26631;&#27880;&#25968;&#25454;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30528;&#37325;&#20110;&#26500;&#24314;&#36866;&#29992;&#20110;&#20013;&#21476;&#39640;&#22320;&#24503;&#35821;&#65288;MHG&#65289;&#30340;&#30701;&#35821;&#32467;&#26500;&#20998;&#26512;&#22120;&#65292;&#22312;&#32570;&#20047;&#26631;&#27880;&#30340;MHG&#26641;&#24211;&#36827;&#34892;&#35757;&#32451;&#30340;&#29616;&#23454;&#26465;&#20214;&#19979;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;MHG&#21644;&#29616;&#20195;&#24503;&#35821;&#65288;MG&#65289;&#20043;&#38388;&#30340;&#35821;&#35328;&#36830;&#32493;&#24615;&#21644;&#32467;&#26500;&#30456;&#20284;&#24615;&#65292;&#20197;&#21450;&#20016;&#23500;&#30340;MG&#26641;&#24211;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Constituency parsing plays a fundamental role in advancing natural language processing (NLP) tasks. However, training an automatic syntactic analysis system for ancient languages solely relying on annotated parse data is a formidable task due to the inherent challenges in building treebanks for such languages. It demands extensive linguistic expertise, leading to a scarcity of available resources. To overcome this hurdle, cross-lingual transfer techniques which require minimal or even no annotated data for low-resource target languages offer a promising solution. In this study, we focus on building a constituency parser for $\mathbf{M}$iddle $\mathbf{H}$igh $\mathbf{G}$erman $\mathbf{MHG}$ under realistic conditions, where no annotated MHG treebank is available for training. In our approach, we leverage the linguistic continuity and structural similarity between MHG and $\mathbf{M}$odern $\mathbf{G}$erman $\mathbf{MG}$, along with the abundance of MG treebank resources. Specifically, b
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;NBIAS&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26694;&#26550;&#65292;&#26088;&#22312;&#35782;&#21035;&#25991;&#26412;&#20013;&#30340;&#20559;&#35265;&#12290;&#36890;&#36807;&#25910;&#38598;&#26469;&#33258;&#31038;&#20132;&#23186;&#20307;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#32844;&#20301;&#25307;&#32856;&#31561;&#39046;&#22495;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#26500;&#24314;&#25968;&#25454;&#38598;&#65292;&#24182;&#24212;&#29992;&#22522;&#20110;Transformer&#30340;&#20196;&#29260;&#20998;&#31867;&#27169;&#22411;&#26469;&#35782;&#21035;&#20559;&#35265;&#35789;/&#30701;&#35821;&#12290;&#36890;&#36807;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#26041;&#27861;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.01681</link><description>&lt;p&gt;
NBIAS: &#29992;&#20110;&#25991;&#26412;&#20013;&#20559;&#35265;&#35782;&#21035;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
NBIAS: A Natural Language Processing Framework for Bias Identification in Text. (arXiv:2308.01681v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01681
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;NBIAS&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26694;&#26550;&#65292;&#26088;&#22312;&#35782;&#21035;&#25991;&#26412;&#20013;&#30340;&#20559;&#35265;&#12290;&#36890;&#36807;&#25910;&#38598;&#26469;&#33258;&#31038;&#20132;&#23186;&#20307;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#32844;&#20301;&#25307;&#32856;&#31561;&#39046;&#22495;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#26500;&#24314;&#25968;&#25454;&#38598;&#65292;&#24182;&#24212;&#29992;&#22522;&#20110;Transformer&#30340;&#20196;&#29260;&#20998;&#31867;&#27169;&#22411;&#26469;&#35782;&#21035;&#20559;&#35265;&#35789;/&#30701;&#35821;&#12290;&#36890;&#36807;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#26041;&#27861;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#26412;&#25968;&#25454;&#20013;&#23384;&#22312;&#20559;&#35265;&#21487;&#33021;&#23548;&#33268;&#25968;&#25454;&#20351;&#29992;&#26102;&#20135;&#29983;&#20542;&#26012;&#30340;&#35299;&#37322;&#21644;&#32467;&#26524;&#12290;&#36825;&#20123;&#20559;&#35265;&#21487;&#33021;&#20250;&#25345;&#32493;&#24378;&#21270;&#21051;&#26495;&#21360;&#35937;&#12289;&#27495;&#35270;&#25110;&#20854;&#20182;&#24418;&#24335;&#30340;&#19981;&#20844;&#24179;&#24453;&#36935;&#12290;&#22312;&#26377;&#20559;&#35265;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#31639;&#27861;&#26368;&#32456;&#20250;&#20570;&#20986;&#19981;&#24179;&#31561;&#24433;&#21709;&#26576;&#20010;&#32676;&#20307;&#30340;&#20915;&#31574;&#12290;&#22240;&#27492;&#65292;&#26816;&#27979;&#21644;&#28040;&#38500;&#36825;&#20123;&#20559;&#35265;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#30830;&#20445;&#23545;&#25968;&#25454;&#30340;&#20844;&#24179;&#21644;&#36947;&#24503;&#20351;&#29992;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20840;&#38754;&#32780;&#24378;&#22823;&#30340;&#26694;&#26550;"NBIAS"&#65292;&#23427;&#21253;&#25324;&#25968;&#25454;&#23618;&#12289;&#35821;&#26009;&#24211;&#26500;&#24314;&#12289;&#27169;&#22411;&#24320;&#21457;&#23618;&#21644;&#35780;&#20272;&#23618;&#12290;&#25968;&#25454;&#38598;&#30001;&#20174;&#21508;&#20010;&#39046;&#22495;&#25910;&#38598;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#26500;&#24314;&#65292;&#21253;&#25324;&#31038;&#20132;&#23186;&#20307;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#32844;&#20301;&#25307;&#32856;&#38376;&#25143;&#32593;&#31449;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#22522;&#20110;Transformer&#30340;&#20196;&#29260;&#20998;&#31867;&#27169;&#22411;&#65292;&#36890;&#36807;&#19968;&#20010;&#21807;&#19968;&#30340;&#21629;&#21517;&#23454;&#20307;&#33021;&#22815;&#35782;&#21035;&#20986;&#20559;&#35265;&#35789;/&#30701;&#35821;&#12290;&#22312;&#35780;&#20272;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#26041;&#27861;&#26469;&#35780;&#20272;&#25105;&#20204;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bias in textual data can lead to skewed interpretations and outcomes when the data is used. These biases could perpetuate stereotypes, discrimination, or other forms of unfair treatment. An algorithm trained on biased data ends up making decisions that disproportionately impact a certain group of people. Therefore, it is crucial to detect and remove these biases to ensure the fair and ethical use of data. To this end, we develop a comprehensive and robust framework \textsc{Nbias} that consists of a data layer, corpus contruction, model development layer and an evaluation layer. The dataset is constructed by collecting diverse data from various fields, including social media, healthcare, and job hiring portals. As such, we applied a transformer-based token classification model that is able to identify bias words/ phrases through a unique named entity. In the assessment procedure, we incorporate a blend of quantitative and qualitative evaluations to gauge the effectiveness of our models.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#20197;GPT-2&#21644;GPT-3.5&#20026;&#20363;&#65292;&#36890;&#36807;&#20840;&#38754;&#30340;&#25991;&#29486;&#32508;&#36848;&#21644;&#28145;&#20837;&#30340;&#23450;&#37327;&#20998;&#26512;&#25581;&#31034;&#20102;&#23384;&#22312;&#30340;&#24615;&#21035;&#21270;&#35789;&#35821;&#20851;&#32852;&#12289;&#35821;&#35328;&#20351;&#29992;&#21644;&#20559;&#35265;&#21465;&#36848;&#65292;&#24182;&#25506;&#35752;&#20102;&#24615;&#21035;&#20559;&#35265;&#21487;&#33021;&#23545;&#31038;&#20250;&#35748;&#30693;&#20135;&#29983;&#30340;&#20262;&#29702;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.09162</link><description>&lt;p&gt;
&#25581;&#31034;&#22312;LLM&#20013;&#32844;&#19994;&#24615;&#21035;&#20559;&#35265;&#65306;&#20998;&#26512;&#21644;&#35299;&#20915;&#31038;&#20250;&#23398;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Unveiling Gender Bias in Terms of Profession Across LLMs: Analyzing and Addressing Sociological Implications. (arXiv:2307.09162v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09162
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#20197;GPT-2&#21644;GPT-3.5&#20026;&#20363;&#65292;&#36890;&#36807;&#20840;&#38754;&#30340;&#25991;&#29486;&#32508;&#36848;&#21644;&#28145;&#20837;&#30340;&#23450;&#37327;&#20998;&#26512;&#25581;&#31034;&#20102;&#23384;&#22312;&#30340;&#24615;&#21035;&#21270;&#35789;&#35821;&#20851;&#32852;&#12289;&#35821;&#35328;&#20351;&#29992;&#21644;&#20559;&#35265;&#21465;&#36848;&#65292;&#24182;&#25506;&#35752;&#20102;&#24615;&#21035;&#20559;&#35265;&#21487;&#33021;&#23545;&#31038;&#20250;&#35748;&#30693;&#20135;&#29983;&#30340;&#20262;&#29702;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#21487;&#33021;&#23545;&#31038;&#20250;&#35748;&#30693;&#21644;&#20559;&#35265;&#20135;&#29983;&#24433;&#21709;&#12290;&#36825;&#31687;&#30740;&#31350;&#26088;&#22312;&#20998;&#26512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#37325;&#28857;&#27604;&#36739;&#20102;GPT-2&#21644;GPT-3.5&#36825;&#20123;&#33879;&#21517;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#20854;&#24433;&#21709;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#25991;&#29486;&#32508;&#36848;&#65292;&#35813;&#30740;&#31350;&#32771;&#23519;&#20102;&#29616;&#26377;&#20851;&#20110;AI&#35821;&#35328;&#27169;&#22411;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#30740;&#31350;&#65292;&#24182;&#30830;&#23450;&#20102;&#24403;&#21069;&#30693;&#35782;&#30340;&#31354;&#30333;&#12290;&#30740;&#31350;&#26041;&#27861;&#21253;&#25324;&#25910;&#38598;&#21644;&#39044;&#22788;&#29702;GPT-2&#21644;GPT-3.5&#30340;&#25968;&#25454;&#65292;&#24182;&#36816;&#29992;&#28145;&#20837;&#30340;&#23450;&#37327;&#20998;&#26512;&#25216;&#26415;&#35780;&#20272;&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#36825;&#20123;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#20013;&#23384;&#22312;&#30340;&#20855;&#26377;&#24615;&#21035;&#33394;&#24425;&#30340;&#35789;&#35821;&#20851;&#32852;&#12289;&#35821;&#35328;&#20351;&#29992;&#21644;&#20559;&#35265;&#21465;&#36848;&#12290;&#35752;&#35770;&#37096;&#20998;&#25506;&#35752;&#20102;&#24615;&#21035;&#20559;&#35265;&#30340;&#20262;&#29702;&#24433;&#21709;&#20197;&#21450;&#20854;&#23545;&#31038;&#20250;&#35748;&#30693;&#30340;&#28508;&#22312;&#21518;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gender bias in artificial intelligence (AI) and natural language processing has garnered significant attention due to its potential impact on societal perceptions and biases. This research paper aims to analyze gender bias in Large Language Models (LLMs) with a focus on multiple comparisons between GPT-2 and GPT-3.5, some prominent language models, to better understand its implications. Through a comprehensive literature review, the study examines existing research on gender bias in AI language models and identifies gaps in the current knowledge. The methodology involves collecting and preprocessing data from GPT-2 and GPT-3.5, and employing in-depth quantitative analysis techniques to evaluate gender bias in the generated text. The findings shed light on gendered word associations, language usage, and biased narratives present in the outputs of these Large Language Models. The discussion explores the ethical implications of gender bias and its potential consequences on social percepti
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20351;&#29992;CNN-LSTM&#27169;&#22411;&#23545;&#27874;&#26031;&#25512;&#29305;&#30340;&#25919;&#27835;&#24773;&#24863;&#36827;&#34892;&#20998;&#26512;&#65292;&#20351;&#29992;ParsBERT&#36827;&#34892;&#35789;&#27719;&#34920;&#31034;&#65292;&#24182;&#27604;&#36739;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#65292;&#20854;&#20013;CNN-LSTM&#27169;&#22411;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#20998;&#21035;&#36798;&#21040;&#20102;89%&#21644;71%&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.07740</link><description>&lt;p&gt;
&#20351;&#29992;CNN-LSTM&#27169;&#22411;&#23545;&#27874;&#26031;&#25512;&#29305;&#30340;&#25919;&#27835;&#24773;&#24863;&#36827;&#34892;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Political Sentiment Analysis of Persian Tweets Using CNN-LSTM Model. (arXiv:2307.07740v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07740
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20351;&#29992;CNN-LSTM&#27169;&#22411;&#23545;&#27874;&#26031;&#25512;&#29305;&#30340;&#25919;&#27835;&#24773;&#24863;&#36827;&#34892;&#20998;&#26512;&#65292;&#20351;&#29992;ParsBERT&#36827;&#34892;&#35789;&#27719;&#34920;&#31034;&#65292;&#24182;&#27604;&#36739;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#65292;&#20854;&#20013;CNN-LSTM&#27169;&#22411;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#20998;&#21035;&#36798;&#21040;&#20102;89%&#21644;71%&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#20998;&#26512;&#26159;&#35782;&#21035;&#21644;&#20998;&#31867;&#20154;&#20204;&#23545;&#21508;&#31181;&#35805;&#39064;&#30340;&#24773;&#24863;&#25110;&#35266;&#28857;&#30340;&#36807;&#31243;&#12290;&#36817;&#24180;&#26469;&#65292;&#23545;Twitter&#24773;&#24863;&#30340;&#20998;&#26512;&#25104;&#20026;&#19968;&#20010;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#30340;&#35805;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#20998;&#26512;&#27874;&#26031;&#25919;&#27835;&#25512;&#29305;&#30340;&#24773;&#24863;&#12290;&#25105;&#20204;&#20351;&#29992;&#35789;&#34955;&#27169;&#22411;&#21644;ParsBERT&#36827;&#34892;&#35789;&#27719;&#34920;&#31034;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#24212;&#29992;&#20102;&#39640;&#26031;&#26420;&#32032;&#36125;&#21494;&#26031;&#12289;&#26799;&#24230;&#25552;&#21319;&#12289;&#36923;&#36753;&#22238;&#24402;&#12289;&#20915;&#31574;&#26641;&#12289;&#38543;&#26426;&#26862;&#26519;&#20197;&#21450;CNN&#21644;LSTM&#30340;&#32452;&#21512;&#26469;&#20998;&#31867;&#25512;&#29305;&#30340;&#26497;&#24615;&#12290;&#26412;&#30740;&#31350;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;ParsBERT&#23884;&#20837;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#27604;&#26426;&#22120;&#23398;&#20064;&#34920;&#29616;&#26356;&#22909;&#12290;CNN-LSTM&#27169;&#22411;&#22312;&#31532;&#19968;&#20010;&#26377;&#19977;&#31181;&#31867;&#21035;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#20026;89&#65285;&#65292;&#22312;&#31532;&#20108;&#20010;&#26377;&#19971;&#31181;&#31867;&#21035;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#20026;71&#65285;&#12290;&#30001;&#20110;&#27874;&#26031;&#35821;&#30340;&#22797;&#26434;&#24615;&#65292;&#36798;&#21040;&#36825;&#19968;&#25928;&#29575;&#27700;&#24179;&#26159;&#19968;&#39033;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sentiment analysis is the process of identifying and categorizing people's emotions or opinions regarding various topics. The analysis of Twitter sentiment has become an increasingly popular topic in recent years. In this paper, we present several machine learning and a deep learning model to analysis sentiment of Persian political tweets. Our analysis was conducted using Bag of Words and ParsBERT for word representation. We applied Gaussian Naive Bayes, Gradient Boosting, Logistic Regression, Decision Trees, Random Forests, as well as a combination of CNN and LSTM to classify the polarities of tweets. The results of this study indicate that deep learning with ParsBERT embedding performs better than machine learning. The CNN-LSTM model had the highest classification accuracy with 89 percent on the first dataset with three classes and 71 percent on the second dataset with seven classes. Due to the complexity of Persian, it was a difficult task to achieve this level of efficiency.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#21019;&#36896;&#24615;&#38382;&#39064;&#35299;&#20915;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23481;&#26131;&#34987;&#35823;&#23548;&#65292;&#20986;&#29616;&#22266;&#23450;&#25928;&#24212;&#21644;Einstellung&#33539;&#24335;&#12290;</title><link>http://arxiv.org/abs/2306.11167</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#35823;&#23548;&#65306;&#20351;&#29992;Only Connect Wall&#25968;&#25454;&#38598;&#25506;&#32034;&#21019;&#36896;&#24615;&#38382;&#39064;&#35299;&#20915;&#21644;Einstellung&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Fixated by Red Herrings: Exploring Creative Problem Solving and Einstellung Effect using the Only Connect Wall Dataset. (arXiv:2306.11167v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11167
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#21019;&#36896;&#24615;&#38382;&#39064;&#35299;&#20915;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23481;&#26131;&#34987;&#35823;&#23548;&#65292;&#20986;&#29616;&#22266;&#23450;&#25928;&#24212;&#21644;Einstellung&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#20154;&#24037;&#26234;&#33021;&#35806;&#29983;&#20197;&#26469;&#65292;&#23545;&#20154;&#31867;&#20223;&#30495;&#26234;&#33021;&#30340;&#36861;&#27714;&#19968;&#30452;&#26159;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#25345;&#20037;&#35805;&#39064;&#12290;&#26368;&#26032;&#19968;&#20195;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25216;&#26415;&#28436;&#36827;&#21644;&#26032;&#20852;&#33021;&#21147;&#23558;&#36825;&#20010;&#20027;&#39064;&#20174;&#23398;&#26415;&#30028;&#24102;&#21040;&#20102;&#25991;&#21270;&#26102;&#20195;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;NLP&#35780;&#20272;&#22522;&#20934;&#20219;&#21153;&#27979;&#35797;&#20102;&#20154;&#31867;&#20223;&#30495;&#34892;&#20026;&#30340;&#19968;&#20123;&#26041;&#38754;&#65288;&#20363;&#22914;BIG-bench&#30340;&#8220;&#31867;&#20154;&#34892;&#20026;&#8221;&#20219;&#21153;&#65289;&#65292;&#20294;&#20960;&#20046;&#27809;&#26377;&#19968;&#20010;&#20219;&#21153;&#32771;&#23519;&#21019;&#36896;&#24615;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;&#20154;&#31867;&#30340;&#21019;&#36896;&#24615;&#38382;&#39064;&#35299;&#20915;&#26159;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#20013;&#30740;&#31350;&#36739;&#20026;&#28145;&#20837;&#30340;&#20027;&#39064;&#65292;&#26631;&#20934;&#21270;&#27979;&#35797;&#20027;&#35201;&#20351;&#29992;&#23558;&#32447;&#32034;&#35789;&#20043;&#38388;&#30340;&#65288;&#24322;&#26500;&#65289;&#36830;&#25509;&#33021;&#21147;&#20316;&#20026;&#21019;&#36896;&#24615;&#30340;&#24230;&#37327;&#12290;&#22312;&#36825;&#26679;&#30340;&#20219;&#21153;&#20013;&#65292;&#26263;&#31034;&#24615;&#30340;&#35823;&#23548;&#24615;&#21050;&#28608;-&#34987;&#31216;&#20026;&#8220;&#35825;&#23548;&#35823;&#35299;&#8221;&#30340;&#24178;&#25200;&#22240;&#32032;-&#36890;&#36807;&#22266;&#23450;&#25928;&#24212;&#21644;Einstellung&#33539;&#24335;&#38459;&#30861;&#20102;&#20154;&#31867;&#30340;&#34920;&#29616;&#12290;&#22312;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#30340;&#30740;&#31350;&#20013;&#65292;&#36890;&#36807;&#20107;&#20808;&#35753;&#21442;&#19982;&#32773;&#25509;&#35302;&#21040;&#26377;&#30456;&#20284;&#25340;&#20889;&#30340;&#38169;&#35823;&#22240;&#32032;&#26469;&#23454;&#39564;&#24615;&#22320;&#35825;&#23548;&#36825;&#26679;&#30340;&#22266;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
The quest for human imitative AI has been an enduring topic in AI research since its inception. The technical evolution and emerging capabilities of the latest cohort of large language models (LLMs) have reinvigorated the subject beyond academia to the cultural zeitgeist. While recent NLP evaluation benchmark tasks test some aspects of human-imitative behaviour (e.g., BIG-bench's 'human-like behavior' tasks), few, if not none, examine creative problem solving abilities. Creative problem solving in humans is a well-studied topic in cognitive neuroscience with standardized tests that predominantly use the ability to associate (heterogeneous) connections among clue words as a metric for creativity. Exposure to misleading stimuli - distractors dubbed red herrings - impede human performance in such tasks via the fixation effect and Einstellung paradigm. In cognitive neuroscience studies, such fixations are experimentally induced by pre-exposing participants to orthographically similar incor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22359;&#29366;&#24577;&#21464;&#25442;&#22120;&#65288;BST&#65289;&#30340;&#28151;&#21512;&#31070;&#32463;&#32593;&#32476;&#23618;&#65292;&#32467;&#21512;&#20102;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21644;&#22359;&#21464;&#25442;&#22120;&#65292;&#26088;&#22312;&#22312;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#20013;&#25552;&#39640;&#24615;&#33021;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#35821;&#35328;&#24314;&#27169;&#22256;&#24785;&#24230;&#19978;&#20248;&#20110;&#31867;&#20284;&#30340;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#38271;&#30340;&#24207;&#21015;&#12290;&#27492;&#22806;&#65292;&#22312;&#23618;&#32423;&#21035;&#19978;&#20855;&#26377;&#36229;&#36807;&#21313;&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2306.09539</link><description>&lt;p&gt;
&#22359;&#29366;&#24577;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Block-State Transformer. (arXiv:2306.09539v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09539
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22359;&#29366;&#24577;&#21464;&#25442;&#22120;&#65288;BST&#65289;&#30340;&#28151;&#21512;&#31070;&#32463;&#32593;&#32476;&#23618;&#65292;&#32467;&#21512;&#20102;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21644;&#22359;&#21464;&#25442;&#22120;&#65292;&#26088;&#22312;&#22312;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#20013;&#25552;&#39640;&#24615;&#33021;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#35821;&#35328;&#24314;&#27169;&#22256;&#24785;&#24230;&#19978;&#20248;&#20110;&#31867;&#20284;&#30340;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#38271;&#30340;&#24207;&#21015;&#12290;&#27492;&#22806;&#65292;&#22312;&#23618;&#32423;&#21035;&#19978;&#20855;&#26377;&#36229;&#36807;&#21313;&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;&#22312;&#38656;&#35201;&#24314;&#27169;&#38271;&#26399;&#20381;&#36182;&#24615;&#24182;&#19988;&#38656;&#35201;&#39640;&#25928;&#25193;&#23637;&#21040;&#38271;&#24207;&#21015;&#30340;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#20102;&#24778;&#20154;&#30340;&#25928;&#26524;&#12290;&#23613;&#31649;&#26368;&#21021;&#26159;&#20026;&#36830;&#32493;&#20449;&#21495;&#35774;&#35745;&#30340;&#65292;&#20294;SSM&#22312;&#35270;&#35273;&#21644;&#38899;&#39057;&#31561;&#35768;&#22810;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#65307;&#28982;&#32780;&#65292;&#22312;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#20013;&#65292;SSM&#20173;&#28982;&#33853;&#21518;&#20110;Transformers&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#22359;&#29366;&#24577;&#21464;&#25442;&#22120;&#65288;BST&#65289;&#30340;&#28151;&#21512;&#23618;&#65292;&#23427;&#22312;&#20869;&#37096;&#32452;&#21512;&#20102;&#19968;&#20010;&#29992;&#20110;&#38271;&#36317;&#31163;&#19978;&#19979;&#25991;&#21270;&#30340;SSM&#23376;&#23618;&#21644;&#19968;&#20010;&#29992;&#20110;&#30701;&#26399;&#24207;&#21015;&#34920;&#31034;&#30340;&#22359;&#21464;&#25442;&#22120;&#23376;&#23618;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#12289;&#23436;&#20840;&#21487;&#24182;&#34892;&#30340;&#38598;&#25104;SSM&#21644;&#22359;&#27880;&#24847;&#21147;&#30340;&#21464;&#20307;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#35821;&#35328;&#24314;&#27169;&#30340;&#22256;&#24785;&#24230;&#19978;&#20248;&#20110;&#31867;&#20284;&#30340;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#38271;&#30340;&#24207;&#21015;&#12290;&#27492;&#22806;&#65292;&#22359;&#29366;&#24577;&#21464;&#25442;&#22120;&#22312;&#23618;&#32423;&#21035;&#19978;&#20855;&#26377;&#36229;&#36807;&#21313;&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
State space models (SSMs) have shown impressive results on tasks that require modeling long-range dependencies and efficiently scale to long sequences owing to their subquadratic runtime complexity. Originally designed for continuous signals, SSMs have shown superior performance on a plethora of tasks, in vision and audio; however, SSMs still lag Transformer performance in Language Modeling tasks. In this work, we propose a hybrid layer named Block-State Transformer (BST), that internally combines an SSM sublayer for long-range contextualization, and a Block Transformer sublayer for short-term representation of sequences. We study three different, and completely parallelizable, variants that integrate SSMs and block-wise attention. We show that our model outperforms similar Transformer-based architectures on language modeling perplexity and generalizes to longer sequences. In addition, the Block-State Transformer demonstrates more than tenfold increase in speed at the layer level compa
&lt;/p&gt;</description></item><item><title>Mol-Instructions&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#35774;&#35745;&#30340;&#32508;&#21512;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#39046;&#22495;&#20013;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#35748;&#30693;&#25935;&#38160;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.08018</link><description>&lt;p&gt;
Mol-Instructions: &#19968;&#20010;&#22823;&#35268;&#27169;&#29983;&#29289;&#20998;&#23376;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#20026;&#22823;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models. (arXiv:2306.08018v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08018
&lt;/p&gt;
&lt;p&gt;
Mol-Instructions&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#35774;&#35745;&#30340;&#32508;&#21512;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#39046;&#22495;&#20013;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#35748;&#30693;&#25935;&#38160;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20197;&#20854;&#21331;&#36234;&#30340;&#20219;&#21153;&#22788;&#29702;&#33021;&#21147;&#21644;&#21019;&#26032;&#30340;&#36755;&#20986;&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#25512;&#21160;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#29983;&#29289;&#20998;&#23376;&#30740;&#31350;&#31561;&#19987;&#19994;&#39046;&#22495;&#30340;&#29087;&#32451;&#24212;&#29992;&#36824;&#21463;&#21040;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Mol-Instructions&#65292;&#36825;&#26159;&#19968;&#20010;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#12289;&#19987;&#38376;&#38024;&#23545;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#35774;&#35745;&#30340;&#32508;&#21512;&#25351;&#20196;&#25968;&#25454;&#38598;&#12290;Mol-Instructions&#30001;&#19977;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#65306;&#20998;&#23376;&#23548;&#21521;&#25351;&#20196;&#12289;&#34507;&#30333;&#36136;&#23548;&#21521;&#25351;&#20196;&#21644;&#29983;&#29289;&#20998;&#23376;&#25991;&#26412;&#25351;&#20196;&#65292;&#27599;&#20010;&#37096;&#20998;&#37117;&#34987;&#31574;&#21010;&#29992;&#20110;&#22686;&#24378;LLM&#23545;&#29983;&#29289;&#20998;&#23376;&#29305;&#24615;&#21644;&#34892;&#20026;&#30340;&#29702;&#35299;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;&#20195;&#34920;&#24615;LLM&#30340;&#24191;&#27867;&#25351;&#20196;&#35843;&#25972;&#23454;&#39564;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;Mol-Instructions&#22312;&#22686;&#24378;&#22823;&#27169;&#22411;&#22312;&#29983;&#29289;&#20998;&#23376;&#30740;&#31350;&#22797;&#26434;&#39046;&#22495;&#20869;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#35748;&#30693;&#25935;&#38160;&#24230;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#20174;&#32780;&#20419;&#36827;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), with their remarkable task-handling capabilities and innovative outputs, have catalyzed significant advancements across a spectrum of fields. However, their proficiency within specialized domains such as biomolecular studies remains limited. To address this challenge, we introduce Mol-Instructions, a meticulously curated, comprehensive instruction dataset expressly designed for the biomolecular realm. Mol-Instructions is composed of three pivotal components: molecule-oriented instructions, protein-oriented instructions, and biomolecular text instructions, each curated to enhance the understanding and prediction capabilities of LLMs concerning biomolecular features and behaviors. Through extensive instruction tuning experiments on the representative LLM, we underscore the potency of Mol-Instructions to enhance the adaptability and cognitive acuity of large models within the complex sphere of biomolecular studies, thereby promoting advancements in the biomol
&lt;/p&gt;</description></item><item><title>&#26631;&#27880;&#32773;&#30340;&#32972;&#26223;&#23545;&#25968;&#25454;&#26631;&#27880;&#30340;&#24433;&#21709;&#24456;&#37325;&#35201;&#12290;&#36890;&#36807;POPQUORN&#25968;&#25454;&#38598;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#26631;&#27880;&#32773;&#30340;&#32972;&#26223;&#22312;&#20182;&#20204;&#30340;&#21028;&#26029;&#20013;&#36215;&#21040;&#20102;&#26174;&#33879;&#20316;&#29992;&#65292;&#24182;&#19988;&#24212;&#35813;&#32771;&#34385;&#20197;&#21069;&#26410;&#32771;&#34385;&#30340;&#32972;&#26223;&#22240;&#32032;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24314;&#35758;&#29702;&#35299;&#26631;&#27880;&#32773;&#30340;&#32972;&#26223;&#65292;&#20174;&#20855;&#26377;&#20154;&#21475;&#32479;&#35745;&#23398;&#24179;&#34913;&#30340;&#20247;&#21253;&#24037;&#20316;&#32773;&#20013;&#25910;&#38598;&#26631;&#31614;&#65292;&#20197;&#20943;&#23569;&#25968;&#25454;&#38598;&#30340;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2306.06826</link><description>&lt;p&gt;
Annotator Demographics Matter - Measuring the Influence of Annotator Demographics with the POPQUORN Dataset
&lt;/p&gt;
&lt;p&gt;
When Do Annotator Demographics Matter? Measuring the Influence of Annotator Demographics with the POPQUORN Dataset. (arXiv:2306.06826v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06826
&lt;/p&gt;
&lt;p&gt;
&#26631;&#27880;&#32773;&#30340;&#32972;&#26223;&#23545;&#25968;&#25454;&#26631;&#27880;&#30340;&#24433;&#21709;&#24456;&#37325;&#35201;&#12290;&#36890;&#36807;POPQUORN&#25968;&#25454;&#38598;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#26631;&#27880;&#32773;&#30340;&#32972;&#26223;&#22312;&#20182;&#20204;&#30340;&#21028;&#26029;&#20013;&#36215;&#21040;&#20102;&#26174;&#33879;&#20316;&#29992;&#65292;&#24182;&#19988;&#24212;&#35813;&#32771;&#34385;&#20197;&#21069;&#26410;&#32771;&#34385;&#30340;&#32972;&#26223;&#22240;&#32032;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24314;&#35758;&#29702;&#35299;&#26631;&#27880;&#32773;&#30340;&#32972;&#26223;&#65292;&#20174;&#20855;&#26377;&#20154;&#21475;&#32479;&#35745;&#23398;&#24179;&#34913;&#30340;&#20247;&#21253;&#24037;&#20316;&#32773;&#20013;&#25910;&#38598;&#26631;&#31614;&#65292;&#20197;&#20943;&#23569;&#25968;&#25454;&#38598;&#30340;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#27880;&#32773;&#30340;&#32972;&#26223;&#21644;&#32463;&#21382;&#20250;&#24433;&#21709;&#20182;&#20204;&#23545;&#25968;&#25454;&#30340;&#26631;&#27880;&#65292;&#28982;&#32780;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21482;&#36817;&#26399;&#24320;&#22987;&#32771;&#34385;&#26631;&#27880;&#32773;&#36523;&#20221;&#23545;&#20182;&#20204;&#20915;&#31574;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;POPQUORN&#65288;POtato-Prolific &#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#38382;&#39064;&#22238;&#31572;&#12289;&#20882;&#29359;&#24615;&#12289;&#25991;&#26412;&#25913;&#20889;&#21644;&#31036;&#35980;&#35780;&#20998;&#65292;&#21253;&#21547;&#20154;&#21475;&#32479;&#35745;&#23398;&#32454;&#24494;&#24046;&#24322;&#65289;&#12290;POPQUORN&#21253;&#21547;1,484&#20010;&#26631;&#27880;&#32773;&#30340;45,000&#20010;&#26631;&#27880;&#65292;&#37319;&#29992;&#20102;&#32654;&#22269;&#20154;&#21475;&#20013;&#24615;&#21035;&#12289;&#24180;&#40836;&#21644;&#31181;&#26063;&#30340;&#20195;&#34920;&#26679;&#26412;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26631;&#27880;&#32773;&#32972;&#26223;&#22312;&#20182;&#20204;&#30340;&#21028;&#26029;&#20013;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#36824;&#34920;&#26126;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20197;&#21069;&#26410;&#32771;&#34385;&#30340;&#32972;&#26223;&#22240;&#32032;&#65288;&#20363;&#22914;&#25945;&#32946;&#65289;&#26159;&#26377;&#24847;&#20041;&#19988;&#24212;&#35813;&#34987;&#32771;&#34385;&#30340;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#29702;&#35299;&#26631;&#27880;&#32773;&#30340;&#32972;&#26223;&#65292;&#24182;&#20174;&#20855;&#26377;&#20154;&#21475;&#32479;&#35745;&#23398;&#24179;&#34913;&#30340;&#20247;&#21253;&#24037;&#20316;&#32773;&#20013;&#25910;&#38598;&#26631;&#31614;&#65292;&#23545;&#20943;&#23569;&#25968;&#25454;&#38598;&#20559;&#24046;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Annotators are not fungible. Their demographics, life experiences, and backgrounds all contribute to how they label data. However, NLP has only recently considered how annotator identity might influence their decisions. Here, we present POPQUORN (the POtato-Prolific dataset for QUestion-Answering, Offensiveness, text Rewriting, and politeness rating with demographic Nuance). POPQUORN contains 45,000 annotations from 1,484 annotators, drawn from a representative sample regarding sex, age, and race as the US population. Through a series of analyses, we show that annotators' background plays a significant role in their judgments. Further, our work shows that backgrounds not previously considered in NLP (e.g., education), are meaningful and should be considered. Our study suggests that understanding the background of annotators and collecting labels from a demographically balanced pool of crowd workers is important to reduce the bias of datasets. The dataset, annotator background, and anno
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22359;&#32423;&#24182;&#34892;Transformer&#26041;&#27861;&#65292;&#20197;&#26368;&#23567;&#21270;&#20869;&#23384;&#25104;&#26412;&#65292;&#33021;&#22815;&#22788;&#29702;&#38271;&#24207;&#21015;&#65292;&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#27604;&#20808;&#21069;&#30340;&#20869;&#23384;&#39640;&#25928;&#26041;&#27861;&#26356;&#38271;32&#20493;&#30340;&#35757;&#32451;&#24207;&#21015;&#12290;</title><link>http://arxiv.org/abs/2305.19370</link><description>&lt;p&gt;
&#22823;&#22411;&#38271;&#24207;&#21015;&#27169;&#22411;&#30340;&#22359;&#32423;&#24182;&#34892;Transformer
&lt;/p&gt;
&lt;p&gt;
Blockwise Parallel Transformer for Long Context Large Models. (arXiv:2305.19370v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22359;&#32423;&#24182;&#34892;Transformer&#26041;&#27861;&#65292;&#20197;&#26368;&#23567;&#21270;&#20869;&#23384;&#25104;&#26412;&#65292;&#33021;&#22815;&#22788;&#29702;&#38271;&#24207;&#21015;&#65292;&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#27604;&#20808;&#21069;&#30340;&#20869;&#23384;&#39640;&#25928;&#26041;&#27861;&#26356;&#38271;32&#20493;&#30340;&#35757;&#32451;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#24050;&#32463;&#25104;&#20026;&#26368;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#22522;&#30707;&#65292;&#22312;&#21508;&#31181;AI&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;Transformer&#20013;&#30340;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#21644;&#22823;&#22411;&#21069;&#39304;&#32593;&#32476;&#25152;&#38656;&#30340;&#20869;&#23384;&#23481;&#37327;&#38480;&#21046;&#20102;&#23427;&#20204;&#22788;&#29702;&#38271;&#24207;&#21015;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#20026;&#28041;&#21450;&#22810;&#20010;&#38271;&#24207;&#21015;&#25110;&#38271;&#26399;&#20381;&#36182;&#30340;&#20219;&#21153;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#26041;&#27861;&#65292;&#22359;&#32423;&#24182;&#34892;Transformer&#65288;BPT&#65289;&#65292;&#23427;&#21033;&#29992;&#22359;&#32423;&#35745;&#31639;&#33258;&#25105;&#27880;&#24847;&#21644;&#21069;&#39304;&#32593;&#32476;&#34701;&#21512;&#20197;&#26368;&#23567;&#21270;&#20869;&#23384;&#25104;&#26412;&#12290;&#36890;&#36807;&#22312;&#20445;&#25345;&#20869;&#23384;&#25928;&#29575;&#30340;&#21516;&#26102;&#22788;&#29702;&#26356;&#38271;&#30340;&#36755;&#20837;&#24207;&#21015;&#65292;BPT&#20351;&#35757;&#32451;&#24207;&#21015;&#30340;&#38271;&#24230;&#27604;&#21407;&#22987;&#30340;Transformer&#38271;32&#20493;&#65292;&#27604;&#20808;&#21069;&#30340;&#20869;&#23384;&#39640;&#25928;&#26041;&#27861;&#38271;2&#21040;4&#20493;&#12290;&#23545;&#35821;&#35328;&#24314;&#27169;&#21644;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;BPT&#22312;&#20943;&#23569;&#20869;&#23384;&#38656;&#27714;&#21644;&#25552;&#39640;&#24615;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have emerged as the cornerstone of state-of-the-art natural language processing models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands posed by the self-attention mechanism and the large feedforward network in Transformers limit their ability to handle long sequences, thereby creating challenges for tasks involving multiple long sequences or long-term dependencies. We present a distinct approach, Blockwise Parallel Transformer (BPT), that leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs. By processing longer input sequences while maintaining memory efficiency, BPT enables training sequences up to 32 times longer than vanilla Transformers and 2 to 4 times longer than previous memory-efficient methods. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of BPT in reducing memory requirements and improving perfo
&lt;/p&gt;</description></item><item><title>Scissorhands&#26159;&#19968;&#20010;&#21487;&#20197;&#22312;&#19981;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#21033;&#29992;&#37325;&#35201;&#24615;&#25345;&#20037;&#24615;&#20551;&#35774;&#23558;LLM KV&#32531;&#23384;&#30340;&#20869;&#23384;&#20351;&#29992;&#32500;&#25345;&#22312;&#22266;&#23450;&#39044;&#31639;&#20869;&#30340;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2305.17118</link><description>&lt;p&gt;
&#21098;&#20992;&#25163;&#65306;&#21033;&#29992;&#37325;&#35201;&#24615;&#25345;&#20037;&#24615;&#20551;&#35774;&#22312;&#27979;&#35797;&#26102;&#23545;LLM KV&#32531;&#23384;&#36827;&#34892;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time. (arXiv:2305.17118v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17118
&lt;/p&gt;
&lt;p&gt;
Scissorhands&#26159;&#19968;&#20010;&#21487;&#20197;&#22312;&#19981;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#21033;&#29992;&#37325;&#35201;&#24615;&#25345;&#20037;&#24615;&#20551;&#35774;&#23558;LLM KV&#32531;&#23384;&#30340;&#20869;&#23384;&#20351;&#29992;&#32500;&#25345;&#22312;&#22266;&#23450;&#39044;&#31639;&#20869;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24341;&#21457;&#20102;&#19968;&#27874;&#26032;&#30340;&#20196;&#20154;&#20852;&#22859;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#12290;&#22823;&#35268;&#27169;&#25176;&#31649;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#20869;&#23384;&#36164;&#28304;&#12290;&#37096;&#32626;&#36807;&#31243;&#20013;&#19968;&#20010;&#20851;&#38190;&#30340;&#20869;&#23384;&#29942;&#39048;&#26469;&#33258;&#20110;&#19978;&#19979;&#25991;&#31383;&#21475;&#12290;&#20247;&#25152;&#21608;&#30693;&#65292;&#27169;&#22411;&#26435;&#37325;&#21344;&#29992;&#22823;&#37327;&#20869;&#23384;&#65307;&#28982;&#32780;&#65292;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#23384;&#20648;&#30340;&#38190;&#20540;&#23884;&#20837;&#22823;&#23567;&#65288;KV&#32531;&#23384;&#65289;&#24448;&#24448;&#36229;&#36807;&#20102;&#27169;&#22411;&#30340;&#22823;&#23567;&#12290;&#24040;&#22823;&#30340;KV&#32531;&#23384;&#22823;&#23567;&#23545;&#20110;&#20851;&#38190;&#23383;&#25209;&#22788;&#29702;&#22823;&#23567;&#30340;&#25512;&#29702;&#20135;&#29983;&#32422;&#26463;&#65292;&#36825;&#23545;&#20110;&#39640;&#21534;&#21520;&#37327;&#30340;&#25512;&#29702;&#24037;&#20316;&#36127;&#36733;&#33267;&#20851;&#37325;&#35201;&#12290;&#21463;&#21040;&#27880;&#24847;&#21147;&#20998;&#25968;&#30340;&#26377;&#36259;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25345;&#20037;&#24615;&#37325;&#35201;&#24615;&#30340;&#20551;&#35774;&#65306;&#21482;&#26377;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#30340;&#20851;&#38190;&#26631;&#35760;&#65292;&#22312;&#19968;&#27493;&#20013;&#26377;&#23454;&#36136;&#24615;&#24433;&#21709;&#65292;&#25165;&#20250;&#22312;&#26410;&#26469;&#30340;&#29983;&#25104;&#20013;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#22522;&#20110;&#23545;&#36825;&#19968;&#20551;&#35774;&#30340;&#32463;&#39564;&#39564;&#35777;&#21644;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21098;&#20992;&#25163;&#65292;&#19968;&#20010;&#21487;&#20197;&#22312;&#19981;&#24494;&#35843;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#23558;KV&#32531;&#23384;&#30340;&#20869;&#23384;&#20351;&#29992;&#32500;&#25345;&#22312;&#22266;&#23450;&#39044;&#31639;&#20869;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models(LLMs) have sparked a new wave of exciting AI applications. Hosting these models at scale requires significant memory resources. One crucial memory bottleneck for the deployment stems from the context window. It is commonly recognized that model weights are memory hungry; however, the size of key-value embedding stored during the generation process (KV cache) can easily surpass the model size. The enormous size of the KV cache puts constraints on the inference batch size, which is crucial for high throughput inference workload. Inspired by an interesting observation of the attention scores, we hypothesize the persistence of importance: only pivotal tokens, which had a substantial influence at one step, will significantly influence future generations. Based on our empirical verification and theoretical analysis around this hypothesis, we propose Scissorhands, a system that maintains the memory usage of the KV cache at a fixed budget without finetuning the model. In 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#20559;&#24046;&#25216;&#26415;&#20197;&#20135;&#29983;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#30340;&#20581;&#22766;&#21435;&#20559;&#24046;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.13862</link><description>&lt;p&gt;
&#20844;&#24179;&#20043;&#36335;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#24046;&#21450;&#21435;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
A Trip Towards Fairness: Bias and De-Biasing in Large Language Models. (arXiv:2305.13862v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13862
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#20559;&#24046;&#25216;&#26415;&#20197;&#20135;&#29983;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#30340;&#20581;&#22766;&#21435;&#20559;&#24046;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT&#65288;Brown&#31561;&#65292;2020&#65289;&#21644;PaLM&#65288;Chowdhery&#31561;&#65292;2022&#65289;&#65289;&#30340;&#26222;&#21450;&#24341;&#21457;&#20102;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#20174;&#22823;&#22411;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#36827;&#34892;&#39044;&#35757;&#32451;&#23545;&#20110;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#30528;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#20284;&#20046;&#20855;&#26377;&#23545;&#26576;&#20123;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#20559;&#35265;&#30340;&#22266;&#26377;&#20559;&#24046;&#12290;&#23613;&#31649;&#30740;&#31350;&#35797;&#22270;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#26410;&#33021;&#23436;&#20840;&#28040;&#38500;&#20559;&#35265;&#65292;&#35201;&#20040;&#38477;&#20302;&#20102;&#24615;&#33021;&#65292;&#35201;&#20040;&#20195;&#20215;&#36807;&#39640;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#24403;&#19981;&#21516;&#21442;&#25968;&#21644;&#39044;&#35757;&#32451;&#25968;&#25454;&#26102;&#65292;&#36825;&#20123;&#26377;&#21069;&#36884;&#30340;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#30340;&#20559;&#35265;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#20559;&#24046;&#25216;&#26415;&#65292;&#21487;&#20197;&#20135;&#29983;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#20445;&#25345;&#24615;&#33021;&#30340;&#20581;&#22766;&#30340;&#21435;&#20559;&#24046;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
An outbreak in the popularity of transformer-based Language Models (such as GPT (Brown et al., 2020) and PaLM (Chowdhery et al., 2022)) has opened the doors to new Machine Learning applications. In particular, in Natural Language Processing and how pre-training from large text, corpora is essential in achieving remarkable results in downstream tasks. However, these Language Models seem to have inherent biases toward certain demographics reflected in their training data. While research has attempted to mitigate this problem, existing methods either fail to remove bias altogether, degrade performance, or are expensive. This paper examines the bias produced by promising Language Models when varying parameters and pre-training data. Finally, we propose a de-biasing technique that produces robust de-bias models that maintain performance on downstream tasks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#21069;&#31471;&#26694;&#26550;&#65292;&#25429;&#25417;&#20102;&#33521;&#25991;&#35821;&#38899;&#21512;&#25104;&#21069;&#31471;&#27169;&#22359;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#19988;&#22312;&#25152;&#26377;&#27169;&#22359;&#20013;&#22343;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10666</link><description>&lt;p&gt;
&#19968;&#20010;&#32479;&#19968;&#30340;&#33521;&#25991;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#21069;&#31471;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
a unified front-end framework for english text-to-speech synthesis. (arXiv:2305.10666v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10666
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#21069;&#31471;&#26694;&#26550;&#65292;&#25429;&#25417;&#20102;&#33521;&#25991;&#35821;&#38899;&#21512;&#25104;&#21069;&#31471;&#27169;&#22359;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#19988;&#22312;&#25152;&#26377;&#27169;&#22359;&#20013;&#22343;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21069;&#31471;&#26159;&#33521;&#25991;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#31995;&#32479;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#36127;&#36131;&#25552;&#21462;&#35821;&#35328;&#29305;&#24449;&#65292;&#22914;&#38901;&#24459;&#21644;&#38899;&#32032;&#65292;&#36825;&#23545;&#20110;&#25991;&#26412;&#21040;&#35821;&#38899;&#27169;&#22411;&#21512;&#25104;&#35821;&#38899;&#33267;&#20851;&#37325;&#35201;&#12290;&#33521;&#25991;&#25991;&#26412;&#21040;&#35821;&#38899;&#21069;&#31471;&#36890;&#24120;&#30001;&#25991;&#26412;&#35268;&#33539;&#21270;&#27169;&#22359;&#65288;TN&#65289;&#65292;&#21333;&#35789;&#38901;&#24459;&#30701;&#35821;&#38901;&#24459;&#30701;&#35821;&#27169;&#22359;&#65288;PWPP&#65289;&#21644;&#23383;&#24418;&#21040;&#38899;&#32032;&#27169;&#22359;&#65288;G2P&#65289;&#32452;&#25104;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#33521;&#25991;&#25991;&#26412;&#21040;&#35821;&#38899;&#21069;&#31471;&#30340;&#30740;&#31350;&#20165;&#20851;&#27880;&#20110;&#21333;&#29420;&#27169;&#22359;&#65292;&#24573;&#30053;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#65292;&#23548;&#33268;&#27599;&#20010;&#27169;&#22359;&#24615;&#33021;&#19979;&#38477;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#21069;&#31471;&#26694;&#26550;&#65292;&#25429;&#25417;&#33521;&#25991;&#25991;&#26412;&#21040;&#35821;&#38899;&#21069;&#31471;&#27169;&#22359;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#25152;&#26377;&#27169;&#22359;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The front-end is a critical component of English text-to-speech (TTS) systems, responsible for extracting linguistic features that are essential for a text-to-speech model to synthesize speech, such as prosodies and phonemes. The English TTS front-end typically consists of a text normalization (TN) module, a prosody word prosody phrase (PWPP) module, and a grapheme-to-phoneme (G2P) module. However, current research on the English TTS front-end focuses solely on individual modules, neglecting the interdependence between them and resulting in sub-optimal performance for each module. Therefore, this paper proposes a unified front-end framework that captures the dependencies among the English TTS front-end modules. Extensive experiments have demonstrated that the proposed method achieves state-of-the-art (SOTA) performance in all modules.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#27169;&#22411;&#65292;&#33021;&#22815;&#25506;&#32034;&#28145;&#24230;&#31070;&#32463;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#25512;&#29702;&#20013;&#30340;&#38750;&#23545;&#31216;&#39640;&#38454;&#29305;&#24449;&#20132;&#20114;&#12290;&#22312;&#20004;&#20010;&#24773;&#24863;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#35782;&#21035;&#24433;&#21709;&#29305;&#24449;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#29305;&#24449;&#20132;&#20114;&#24402;&#22240;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.07224</link><description>&lt;p&gt;
&#38754;&#21521;&#27169;&#22411;&#39044;&#27979;&#35299;&#37322;&#30340;&#38750;&#23545;&#31216;&#29305;&#24449;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
Asymmetric feature interaction for interpreting model predictions. (arXiv:2305.07224v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#27169;&#22411;&#65292;&#33021;&#22815;&#25506;&#32034;&#28145;&#24230;&#31070;&#32463;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#25512;&#29702;&#20013;&#30340;&#38750;&#23545;&#31216;&#39640;&#38454;&#29305;&#24449;&#20132;&#20114;&#12290;&#22312;&#20004;&#20010;&#24773;&#24863;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#35782;&#21035;&#24433;&#21709;&#29305;&#24449;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#29305;&#24449;&#20132;&#20114;&#24402;&#22240;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#27169;&#25311;&#19978;&#19979;&#25991;&#20043;&#38388;&#30340;&#22797;&#26434;&#20132;&#20114;&#65292;&#24182;&#22312;&#19968;&#31995;&#21015;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#26524;&#12290;&#20808;&#21069;&#26377;&#20851;&#29305;&#24449;&#20132;&#20114;&#24402;&#22240;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#23545;&#31216;&#20132;&#20114;&#30340;&#30740;&#31350;&#19978;&#65292;&#23427;&#21482;&#33021;&#35299;&#37322;&#21333;&#20010;&#35789;&#27719;&#32452;&#21512;&#21518;&#23545;&#27169;&#22411;&#39044;&#27979;&#30340;&#38468;&#21152;&#24433;&#21709;&#65292;&#32780;&#26080;&#27861;&#25429;&#25417;&#23548;&#33268;&#27169;&#22411;&#39044;&#27979;&#30340;&#38750;&#23545;&#31216;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#23545;&#31216;&#29305;&#24449;&#20132;&#20114;&#35299;&#37322;&#27169;&#22411;&#65292;&#26088;&#22312;&#25506;&#32034;&#28145;&#24230;&#31070;&#32463;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#25512;&#29702;&#20013;&#30340;&#38750;&#23545;&#31216;&#39640;&#38454;&#29305;&#24449;&#20132;&#20114;&#12290;&#36890;&#36807;&#34920;&#31034;&#25105;&#20204;&#30340;&#35299;&#37322;&#20026;&#19968;&#20010;&#26377;&#21521;&#20132;&#20114;&#22270;&#65292;&#25105;&#20204;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#22270;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#33021;&#22815;&#21457;&#29616;&#38750;&#23545;&#31216;&#29305;&#24449;&#20132;&#20114;&#20316;&#29992;&#12290;&#22312;&#20004;&#20010;&#24773;&#24863;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#35782;&#21035;&#24433;&#21709;&#29305;&#24449;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#29305;&#24449;&#20132;&#20114;&#24402;&#22240;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In natural language processing (NLP), deep neural networks (DNNs) could model complex interactions between context and have achieved impressive results on a range of NLP tasks. Prior works on feature interaction attribution mainly focus on studying symmetric interaction that only explains the additional influence of a set of words in combination, which fails to capture asymmetric influence that contributes to model prediction. In this work, we propose an asymmetric feature interaction attribution explanation model that aims to explore asymmetric higher-order feature interactions in the inference of deep neural NLP models. By representing our explanation with an directed interaction graph, we experimentally demonstrate interpretability of the graph to discover asymmetric feature interactions. Experimental results on two sentiment classification datasets show the superiority of our model against the state-of-the-art feature interaction attribution methods in identifying influential featu
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;OLISIA&#65292;&#19968;&#20010;&#21475;&#35821;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#30340;&#32423;&#32852;&#31995;&#32479;&#65292;&#20351;&#29992;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#21644;DST&#27169;&#22411;&#65292;&#37319;&#29992;&#20960;&#20010;&#36866;&#24212;&#24615;&#31574;&#30053;&#26469;&#25552;&#39640;&#31283;&#20581;&#24615;&#65292;&#24182;&#22312;DSTC11 Track3&#20013;&#21462;&#24471;&#31532;&#19968;&#21517;&#30340;&#22909;&#25104;&#32489;&#12290;</title><link>http://arxiv.org/abs/2304.11073</link><description>&lt;p&gt;
OLISIA: &#19968;&#20010;&#29992;&#20110;&#21475;&#35821;&#21270;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#30340;&#32423;&#32852;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
OLISIA: a Cascade System for Spoken Dialogue State Tracking. (arXiv:2304.11073v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11073
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;OLISIA&#65292;&#19968;&#20010;&#21475;&#35821;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#30340;&#32423;&#32852;&#31995;&#32479;&#65292;&#20351;&#29992;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#21644;DST&#27169;&#22411;&#65292;&#37319;&#29992;&#20960;&#20010;&#36866;&#24212;&#24615;&#31574;&#30053;&#26469;&#25552;&#39640;&#31283;&#20581;&#24615;&#65292;&#24182;&#22312;DSTC11 Track3&#20013;&#21462;&#24471;&#31532;&#19968;&#21517;&#30340;&#22909;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#29366;&#24577;&#36319;&#36394; (DST) &#26159;&#21475;&#35821;&#23545;&#35805;&#31995;&#32479;&#30340;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#20851;&#20110;&#35813;&#20219;&#21153;&#30340;&#30740;&#31350;&#22823;&#22810;&#38598;&#20013;&#20110;&#32842;&#22825;&#26102;&#30340;&#35821;&#26009;&#24211;&#65292;&#24573;&#30053;&#20102;&#21475;&#35821;&#21644;&#20070;&#38754;&#35821;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102; OLISIA&#65292;&#36825;&#26159;&#19968;&#20010;&#32423;&#32852;&#31995;&#32479;&#65292;&#23427;&#38598;&#25104;&#20102;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035; (ASR) &#27169;&#22411;&#21644; DST &#27169;&#22411;&#12290;&#25105;&#20204;&#22312; ASR &#21644; DST &#27169;&#22359;&#20013;&#24341;&#20837;&#20102;&#20960;&#20010;&#36866;&#24212;&#24615;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#23545;&#21475;&#35821;&#23545;&#35805;&#30340;&#25972;&#21512;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;&#32463;&#36807;&#36825;&#20123;&#31574;&#30053;&#30340;&#35843;&#25972;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312; DSTC11 Track 3 &#20013;&#25490;&#21517;&#31532;&#19968;&#65292;&#36825;&#26159;&#19968;&#20010;&#35780;&#20272;&#21475;&#35821; DST &#24615;&#33021;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#32467;&#26524;&#20998;&#26512;&#65292;&#21457;&#29616;&#35268;&#33539;&#21270; ASR &#30340;&#36755;&#20986;&#21644;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#35843;&#25972; DST &#30340;&#36755;&#20837;&#65292;&#20197;&#21450;&#22686;&#21152;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#22823;&#23567;&#65292;&#37117;&#22312;&#38477;&#20302;&#20070;&#38754;&#21644;&#21475;&#35821;&#23545;&#35805;&#20043;&#38388;&#24615;&#33021;&#24046;&#24322;&#26041;&#38754;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Though Dialogue State Tracking (DST) is a core component of spoken dialogue systems, recent work on this task mostly deals with chat corpora, disregarding the discrepancies between spoken and written language.In this paper, we propose OLISIA, a cascade system which integrates an Automatic Speech Recognition (ASR) model and a DST model. We introduce several adaptations in the ASR and DST modules to improve integration and robustness to spoken conversations.With these adaptations, our system ranked first in DSTC11 Track 3, a benchmark to evaluate spoken DST. We conduct an in-depth analysis of the results and find that normalizing the ASR outputs and adapting the DST inputs through data augmentation, along with increasing the pre-trained models size all play an important role in reducing the performance discrepancy between written and spoken conversations.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#20854;&#22312;&#23545;&#25239;&#24615;&#21644;&#36229;&#20986;&#20998;&#24067;&#20219;&#21153;&#19978;&#26377;&#19968;&#33268;&#30340;&#20248;&#21183;&#65292;&#20294;&#32477;&#23545;&#34920;&#29616;&#20173;&#26377;&#25552;&#39640;&#31354;&#38388;&#65292;&#40065;&#26834;&#24615;&#20173;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2302.12095</link><description>&lt;p&gt;
&#35770;ChatGPT&#30340;&#40065;&#26834;&#24615;&#65306;&#23545;&#25239;&#24615;&#21644;&#36229;&#20986;&#20998;&#24067;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective. (arXiv:2302.12095v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#20854;&#22312;&#23545;&#25239;&#24615;&#21644;&#36229;&#20986;&#20998;&#24067;&#20219;&#21153;&#19978;&#26377;&#19968;&#33268;&#30340;&#20248;&#21183;&#65292;&#20294;&#32477;&#23545;&#34920;&#29616;&#20173;&#26377;&#25552;&#39640;&#31354;&#38388;&#65292;&#40065;&#26834;&#24615;&#20173;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;OpenAI&#26368;&#36817;&#21457;&#24067;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#26381;&#21153;&#65292;&#24182;&#22312;&#36807;&#21435;&#20960;&#20010;&#26376;&#20013;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#34429;&#28982;&#24050;&#23545;ChatGPT&#30340;&#21508;&#20010;&#26041;&#38754;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#20294;&#20854;&#40065;&#26834;&#24615;&#65292;&#21363;&#23545;&#20110;&#26410;&#39044;&#26399;&#36755;&#20837;&#30340;&#34920;&#29616;&#65292;&#20173;&#19981;&#28165;&#26970;&#12290;&#40065;&#26834;&#24615;&#22312;&#36127;&#36131;&#20219;&#30340;AI&#20013;&#29305;&#21035;&#21463;&#20851;&#27880;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#31243;&#24207;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#23545;&#25239;&#24615;&#21644;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#30340;&#35282;&#24230;&#23545;ChatGPT&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#20102;&#24443;&#24213;&#35780;&#20272;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;AdvGLUE&#21644;ANLI&#22522;&#20934;&#26469;&#35780;&#20272;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#37319;&#29992;Flipkart&#35780;&#35770;&#21644;DDXPlus&#21307;&#23398;&#35786;&#26029;&#25968;&#25454;&#38598;&#36827;&#34892;OOD&#35780;&#20272;&#12290;&#25105;&#20204;&#36873;&#25321;&#20102;&#20960;&#20010;&#27969;&#34892;&#30340;&#22522;&#30784;&#27169;&#22411;&#20316;&#20026;&#22522;&#20934;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#22312;&#22823;&#22810;&#25968;&#23545;&#25239;&#24615;&#21644;OOD&#20998;&#31867;&#21644;&#32763;&#35793;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#20248;&#21183;&#12290;&#20294;&#26159;&#65292;&#32477;&#23545;&#30340;&#34920;&#29616;&#36828;&#38750;&#23436;&#32654;&#65292;&#36825;&#34920;&#26126;&#23545;&#25239;&#24615;&#21644;OOD&#40065;&#26834;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is a recent chatbot service released by OpenAI and is receiving increasing attention over the past few months. While evaluations of various aspects of ChatGPT have been done, its robustness, i.e., the performance to unexpected inputs, is still unclear to the public. Robustness is of particular concern in responsible AI, especially for safety-critical applications. In this paper, we conduct a thorough evaluation of the robustness of ChatGPT from the adversarial and out-of-distribution (OOD) perspective. To do so, we employ the AdvGLUE and ANLI benchmarks to assess adversarial robustness and the Flipkart review and DDXPlus medical diagnosis datasets for OOD evaluation. We select several popular foundation models as baselines. Results show that ChatGPT shows consistent advantages on most adversarial and OOD classification and translation tasks. However, the absolute performance is far from perfection, which suggests that adversarial and OOD robustness remains a significant threat 
&lt;/p&gt;</description></item><item><title>&#8220;&#36890;&#36807;&#27979;&#35797;&#22810;&#20010;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;40&#20010;ToM&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#30740;&#31350;&#21457;&#29616;GPT-3&#21644;GPT-4&#33021;&#22815;&#35299;&#20915;&#22823;&#37096;&#20998;&#20219;&#21153;&#65292;&#35828;&#26126;&#31867;&#20284;ToM&#30340;&#33021;&#21147;&#21487;&#33021;&#26159;&#35821;&#35328;&#27169;&#22411;&#33258;&#21457;&#20986;&#29616;&#30340;&#38468;&#24102;&#20135;&#29289;&#12290;&#8221;</title><link>http://arxiv.org/abs/2302.02083</link><description>&lt;p&gt;
&#8220;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#20250;&#33258;&#21457;&#20986;&#29616;&#24515;&#26234;&#29702;&#35770;&#8221;
&lt;/p&gt;
&lt;p&gt;
Theory of Mind May Have Spontaneously Emerged in Large Language Models. (arXiv:2302.02083v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02083
&lt;/p&gt;
&lt;p&gt;
&#8220;&#36890;&#36807;&#27979;&#35797;&#22810;&#20010;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;40&#20010;ToM&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#30740;&#31350;&#21457;&#29616;GPT-3&#21644;GPT-4&#33021;&#22815;&#35299;&#20915;&#22823;&#37096;&#20998;&#20219;&#21153;&#65292;&#35828;&#26126;&#31867;&#20284;ToM&#30340;&#33021;&#21147;&#21487;&#33021;&#26159;&#35821;&#35328;&#27169;&#22411;&#33258;&#21457;&#20986;&#29616;&#30340;&#38468;&#24102;&#20135;&#29289;&#12290;&#8221;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#24515;&#26234;&#29702;&#35770;&#65288;ToM&#65289;&#25351;&#33021;&#22815;&#25512;&#29702;&#20182;&#20154;&#20869;&#24515;&#30340;&#19981;&#21487;&#35266;&#23519;&#29366;&#24577;&#65292;&#23545;&#20110;&#20154;&#31867;&#31038;&#20132;&#20114;&#21160;&#12289;&#20132;&#27969;&#12289;&#31227;&#24773;&#12289;&#33258;&#25105;&#24847;&#35782;&#21644;&#36947;&#24503;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;40&#20010;&#24191;&#27867;&#29992;&#20110;&#27979;&#35797;&#20154;&#31867;ToM&#30340;&#32463;&#20856;&#34394;&#20551;&#20449;&#24565;&#20219;&#21153;&#26469;&#27979;&#35797;&#20960;&#20010;&#35821;&#35328;&#27169;&#22411;&#12290;2020&#24180;&#20043;&#21069;&#21457;&#24067;&#30340;&#27169;&#22411;&#22312;&#35299;&#20915;ToM&#20219;&#21153;&#26041;&#38754;&#20960;&#20046;&#27809;&#26377;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;2020&#24180;5&#26376;&#21457;&#24067;&#30340;&#31532;&#19968;&#20010;GPT-3&#29256;&#26412;&#65288;&#8220;davinci-001&#8221;&#65289;&#35299;&#20915;&#20102;&#32422;40&#65285;&#30340;&#34394;&#20551;&#20449;&#24565;&#20219;&#21153;&#65292;&#19982;3.5&#23681;&#30340;&#20799;&#31461;&#30340;&#34920;&#29616;&#30456;&#24403;&#12290;&#23427;&#30340;&#31532;&#20108;&#20010;&#29256;&#26412;&#65288;&#8220;davinci-002&#8221;&#65292;2022&#24180;1&#26376;&#65289;&#35299;&#20915;&#20102;70&#65285;&#30340;&#34394;&#20551;&#20449;&#24565;&#20219;&#21153;&#65292;&#19982;6&#23681;&#20799;&#31461;&#30340;&#34920;&#29616;&#30456;&#24403;&#12290;&#26368;&#26032;&#29256;&#26412;&#30340;GPT-3.5&#65288;&#8220;davinci-003&#8221;&#65292;2022&#24180;11&#26376;&#65289;&#35299;&#20915;&#20102;90&#65285;&#30340;&#34394;&#20551;&#20449;&#24565;&#20219;&#21153;&#65292;&#36798;&#21040;&#20102;7&#23681;&#20799;&#31461;&#27700;&#24179;&#12290;&#20110;2023&#24180;3&#26376;&#21457;&#24067;&#30340;GPT-4&#35299;&#20915;&#20102;&#20960;&#20046;&#25152;&#26377;&#30340;&#20219;&#21153;&#65288;95&#65285;&#65289;&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#65292;&#31867;&#20284;ToM&#30340;&#33021;&#21147;&#65288;&#36804;&#20170;&#34987;&#35748;&#20026;&#26159;&#20154;&#31867;&#29420;&#26377;&#30340;&#65289;&#21487;&#33021;&#26159;&#35821;&#35328;&#30340;&#38468;&#24102;&#20135;&#29289;&#12290;&#8221;
&lt;/p&gt;
&lt;p&gt;
Theory of mind (ToM), or the ability to impute unobservable mental states to others, is central to human social interactions, communication, empathy, self-consciousness, and morality. We tested several language models using 40 classic false-belief tasks widely used to test ToM in humans. The models published before 2020 showed virtually no ability to solve ToM tasks. Yet, the first version of GPT-3 ("davinci-001"), published in May 2020, solved about 40% of false-belief tasks-performance comparable with 3.5-year-old children. Its second version ("davinci-002"; January 2022) solved 70% of false-belief tasks, performance comparable with six-year-olds. Its most recent version, GPT-3.5 ("davinci-003"; November 2022), solved 90% of false-belief tasks, at the level of seven-year-olds. GPT-4 published in March 2023 solved nearly all the tasks (95%). These findings suggest that ToM-like ability (thus far considered to be uniquely human) may have spontaneously emerged as a byproduct of language
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#20219;&#21153;&#38598;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#27874;&#26031;&#35821;&#35780;&#35770;&#20013;&#36827;&#34892;&#26041;&#38754;&#21644;&#26497;&#24615;&#20998;&#31867;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#21516;&#26102;&#26816;&#27979;&#26041;&#38754;&#31867;&#21035;&#21644;&#26041;&#38754;&#31867;&#21035;&#26497;&#24615;&#65292;&#36890;&#36807;&#32467;&#21512;&#22810;&#20010;&#27169;&#22411;&#36827;&#34892;&#38598;&#25104;&#23398;&#20064;&#65292;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#25928;&#26524;&#24182;&#20943;&#23569;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2201.06313</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#20219;&#21153;&#38598;&#25104;&#27169;&#22411;&#29992;&#20110;&#27874;&#26031;&#35821;&#35780;&#35770;&#20013;&#30340;&#26041;&#38754;&#21644;&#26497;&#24615;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
A Deep Convolutional Neural Networks Based Multi-Task Ensemble Model for Aspect and Polarity Classification in Persian Reviews. (arXiv:2201.06313v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.06313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#20219;&#21153;&#38598;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#27874;&#26031;&#35821;&#35780;&#35770;&#20013;&#36827;&#34892;&#26041;&#38754;&#21644;&#26497;&#24615;&#20998;&#31867;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#21516;&#26102;&#26816;&#27979;&#26041;&#38754;&#31867;&#21035;&#21644;&#26041;&#38754;&#31867;&#21035;&#26497;&#24615;&#65292;&#36890;&#36807;&#32467;&#21512;&#22810;&#20010;&#27169;&#22411;&#36827;&#34892;&#38598;&#25104;&#23398;&#20064;&#65292;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#25928;&#26524;&#24182;&#20943;&#23569;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26041;&#38754;&#32423;&#24773;&#24863;&#20998;&#26512;&#22240;&#20854;&#33021;&#22815;&#35782;&#21035;&#25991;&#26412;&#20013;&#35752;&#35770;&#30340;&#25152;&#26377;&#26041;&#38754;&#32780;&#20855;&#26377;&#37325;&#35201;&#24615;&#21644;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#26041;&#38754;&#32423;&#24773;&#24863;&#20998;&#26512;&#22312;&#35782;&#21035;&#25991;&#26412;&#20013;&#25152;&#26377;&#35752;&#35770;&#30340;&#26041;&#38754;&#30340;&#21516;&#26102;&#65292;&#36824;&#33021;&#35782;&#21035;&#23427;&#20204;&#30340;&#26497;&#24615;&#26102;&#26368;&#26377;&#25928;&#12290;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#26041;&#27861;&#20351;&#29992;&#31649;&#36947;&#26041;&#27861;&#65292;&#21363;&#39318;&#20808;&#35782;&#21035;&#26041;&#38754;&#65292;&#28982;&#21518;&#35782;&#21035;&#26497;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#19981;&#21512;&#36866;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#38169;&#35823;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#21516;&#26102;&#26816;&#27979;&#26041;&#38754;&#31867;&#21035;&#21644;&#26816;&#27979;&#26041;&#38754;&#31867;&#21035;&#30340;&#26497;&#24615;&#12290;&#20165;&#21019;&#24314;&#19968;&#20010;&#27169;&#22411;&#21487;&#33021;&#26080;&#27861;&#25552;&#20379;&#26368;&#20339;&#39044;&#27979;&#32467;&#26524;&#65292;&#24182;&#21487;&#33021;&#23548;&#33268;&#20559;&#24046;&#21644;&#36739;&#39640;&#26041;&#24046;&#31561;&#38169;&#35823;&#12290;&#20026;&#20102;&#20943;&#23569;&#36825;&#20123;&#38169;&#35823;&#24182;&#25552;&#39640;&#27169;&#22411;&#39044;&#27979;&#30340;&#25928;&#29575;&#65292;&#32467;&#21512;&#22810;&#20010;&#27169;&#22411;&#65292;&#21363;&#38598;&#25104;&#23398;&#20064;&#65292;&#21487;&#33021;&#20250;&#25552;&#20379;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#22240;&#27492;&#65292;
&lt;/p&gt;
&lt;p&gt;
Aspect-based sentiment analysis is of great importance and application because of its ability to identify all aspects discussed in the text. However, aspect-based sentiment analysis will be most effective when, in addition to identifying all the aspects discussed in the text, it can also identify their polarity. Most previous methods use the pipeline approach, that is, they first identify the aspects and then identify the polarities. Such methods are unsuitable for practical applications since they can lead to model errors. Therefore, in this study, we propose a multi-task learning model based on Convolutional Neural Networks (CNNs), which can simultaneously detect aspect category and detect aspect category polarity. creating a model alone may not provide the best predictions and lead to errors such as bias and high variance. To reduce these errors and improve the efficiency of model predictions, combining several models known as ensemble learning may provide better results. Therefore,
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23545;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#35780;&#20272;&#65292;&#21457;&#29616;&#36890;&#29992;&#30340;&#21069;&#26399;&#35757;&#32451;&#21487;&#20197;&#22312;&#32456;&#36523;&#23398;&#20064;&#20013;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2112.09153</link><description>&lt;p&gt;
&#21069;&#26399;&#35757;&#32451;&#22312;&#32456;&#36523;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Investigation of the Role of Pre-training in Lifelong Learning. (arXiv:2112.09153v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.09153
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23545;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#35780;&#20272;&#65292;&#21457;&#29616;&#36890;&#29992;&#30340;&#21069;&#26399;&#35757;&#32451;&#21487;&#20197;&#22312;&#32456;&#36523;&#23398;&#20064;&#20013;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#32456;&#36523;&#23398;&#20064;&#33539;&#24335;&#19981;&#20165;&#22240;&#20854;&#31867;&#20284;&#29983;&#29289;&#23398;&#20064;&#30340;&#29305;&#24615;&#32780;&#20855;&#26377;&#21560;&#24341;&#21147;&#65292;&#32780;&#19988;&#22240;&#20854;&#36890;&#36807;&#36991;&#20813;&#36807;&#22810;&#30340;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;&#32780;&#20943;&#23569;&#33021;&#28304;&#28010;&#36153;&#30340;&#28508;&#21147;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#36825;&#19968;&#33539;&#24335;&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#26159;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#12290;&#38543;&#30528;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#26085;&#30410;&#27969;&#34892;&#21644;&#25104;&#21151;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#38382;&#39064;&#65306;&#22312;&#32456;&#36523;&#23398;&#20064;&#20013;&#65292;&#21069;&#26399;&#35757;&#32451;&#22312;&#28798;&#38590;&#24615;&#36951;&#24536;&#26041;&#38754;&#25198;&#28436;&#20309;&#31181;&#35282;&#33394;&#65311;&#25105;&#20204;&#22312;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#32972;&#26223;&#19979;&#30740;&#31350;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#22312;&#21508;&#31181;&#25991;&#26412;&#21644;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#35780;&#20272;&#23427;&#20204;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#20351;&#29992;&#19968;&#20010;&#26032;&#39062;&#30340;&#21253;&#21547;15&#20010;&#19981;&#21516;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#35268;&#27169;&#30740;&#31350;&#12290;&#22312;&#25152;&#26377;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#19982;&#38543;&#26426;&#21021;&#22987;&#21270;&#27169;&#22411;&#30456;&#27604;&#65292;&#36890;&#29992;&#30340;&#21069;&#26399;&#35757;&#32451;&#22312;&#23398;&#20064;&#22810;&#20010;&#20219;&#21153;&#26102;&#38544;&#21547;&#22320;&#32531;&#35299;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The lifelong learning paradigm in machine learning is an attractive alternative to the more prominent isolated learning scheme not only due to its resemblance to biological learning but also its potential to reduce energy waste by obviating excessive model re-training. A key challenge to this paradigm is the phenomenon of catastrophic forgetting. With the increasing popularity and success of pre-trained models in machine learning, we pose the question: What role does pre-training play in lifelong learning, specifically with respect to catastrophic forgetting? We investigate existing methods in the context of large, pre-trained models and evaluate their performance on a variety of text and image classification tasks, including a large-scale study using a novel data set of 15 diverse NLP tasks. Across all settings, we observe that generic pre-training implicitly alleviates the effects of catastrophic forgetting when learning multiple tasks sequentially compared to randomly initialized mo
&lt;/p&gt;</description></item></channel></rss>