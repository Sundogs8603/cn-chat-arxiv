<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#25552;&#20986;&#20102;AQuA&#65292;&#19968;&#31181;&#32508;&#21512;&#30340;&#30923;&#21830;&#36136;&#37327;&#24471;&#20998;&#35745;&#31639;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#22810;&#20010;&#25351;&#26631;&#20013;&#25552;&#21462;&#21508;&#20010;&#35752;&#35770;&#24086;&#23376;&#30340;&#32479;&#19968;&#24471;&#20998;&#65292;&#20445;&#30041;&#20102;&#35780;&#35770;&#20013;&#30923;&#21830;&#26041;&#38754;&#30340;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#12290;</title><link>https://arxiv.org/abs/2404.02761</link><description>&lt;p&gt;
AQuA --&#32467;&#21512;&#19987;&#23478;&#21644;&#38750;&#19987;&#23478;&#35266;&#28857;&#65292;&#21033;&#29992;LLMs&#35780;&#20272;&#22312;&#32447;&#35752;&#35770;&#20013;&#30340;&#30923;&#21830;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
AQuA -- Combining Experts' and Non-Experts' Views To Assess Deliberation Quality in Online Discussions Using LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02761
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;AQuA&#65292;&#19968;&#31181;&#32508;&#21512;&#30340;&#30923;&#21830;&#36136;&#37327;&#24471;&#20998;&#35745;&#31639;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#22810;&#20010;&#25351;&#26631;&#20013;&#25552;&#21462;&#21508;&#20010;&#35752;&#35770;&#24086;&#23376;&#30340;&#32479;&#19968;&#24471;&#20998;&#65292;&#20445;&#30041;&#20102;&#35780;&#35770;&#20013;&#30923;&#21830;&#26041;&#38754;&#30340;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25919;&#27835;&#22312;&#32447;&#35752;&#35770;&#20013;&#34913;&#37327;&#36129;&#29486;&#36136;&#37327;&#23545;&#20110;&#30740;&#31350;&#30923;&#21830;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#33267;&#20851;&#37325;&#35201;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#27493;&#65292;&#33258;&#21160;&#34913;&#37327;&#36825;&#20123;&#25351;&#26631;&#21464;&#24471;&#21487;&#34892;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;AQuA&#65292;&#23427;&#26159;&#19968;&#20010;&#28155;&#21152;&#20998;&#25968;&#65292;&#20174;&#22810;&#20010;&#25351;&#26631;&#20013;&#35745;&#31639;&#27599;&#20010;&#35752;&#35770;&#24086;&#23376;&#30340;&#32479;&#19968;&#30923;&#21830;&#36136;&#37327;&#24471;&#20998;&#12290;&#19982;&#20854;&#20182;&#29305;&#23450;&#20998;&#25968;&#19981;&#21516;&#65292;AQuA&#20445;&#30041;&#20102;&#35780;&#35770;&#20013;&#23384;&#22312;&#30340;&#30923;&#21830;&#26041;&#38754;&#30340;&#20449;&#24687;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02761v1 Announce Type: cross  Abstract: Measuring the quality of contributions in political online discussions is crucial in deliberation research and computer science. Research has identified various indicators to assess online discussion quality, and with deep learning advancements, automating these measures has become feasible. While some studies focus on analyzing specific quality indicators, a comprehensive quality score incorporating various deliberative aspects is often preferred. In this work, we introduce AQuA, an additive score that calculates a unified deliberative quality score from multiple indices for each discussion post. Unlike other singular scores, AQuA preserves information on the deliberative aspects present in comments, enhancing model transparency. We develop adapter models for 20 deliberative indices, and calculate correlation coefficients between experts' annotations and the perceived deliberativeness by non-experts to weigh the individual indices int
&lt;/p&gt;</description></item><item><title>&#22312;&#22810;&#26679;&#21270;&#34920;&#31034;&#25216;&#24039;&#30340;&#27880;&#35299;&#22120;&#24314;&#27169;&#39046;&#22495;&#65292;&#23545;&#25968;&#25454;&#38598;&#30340;&#32454;&#31890;&#24230;&#29305;&#24449;&#36827;&#34892;&#30740;&#31350;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;</title><link>https://arxiv.org/abs/2404.02340</link><description>&lt;p&gt;
&#27880;&#35299;&#22120;&#24314;&#27169;&#19982;&#25193;&#23637;&#30340;&#35821;&#26009;&#24211;&#32771;&#34385;
&lt;/p&gt;
&lt;p&gt;
Corpus Considerations for Annotator Modeling and Scaling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02340
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26679;&#21270;&#34920;&#31034;&#25216;&#24039;&#30340;&#27880;&#35299;&#22120;&#24314;&#27169;&#39046;&#22495;&#65292;&#23545;&#25968;&#25454;&#38598;&#30340;&#32454;&#31890;&#24230;&#29305;&#24449;&#36827;&#34892;&#30740;&#31350;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#21644;&#27880;&#37322;&#20219;&#21153;&#30340;&#26368;&#26032;&#36235;&#21183;&#30830;&#35748;&#20102;&#20174;&#20256;&#32479;&#20381;&#36182;&#21333;&#19968;&#8220;&#30495;&#30456;&#26631;&#31614;&#8221;&#36716;&#21521;&#20851;&#27880;&#20010;&#20307;&#35270;&#35282;&#65292;&#23588;&#20854;&#26159;&#22312;&#20027;&#35266;&#20219;&#21153;&#20013;&#12290;&#22312;&#27880;&#37322;&#20219;&#21153;&#26088;&#22312;&#21253;&#21547;&#22810;&#26679;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#20165;&#20381;&#36182;&#20110;&#22810;&#25968;&#31867;&#21035;&#26631;&#31614;&#30340;&#27169;&#22411;&#21487;&#33021;&#26080;&#24847;&#20013;&#24573;&#35270;&#26377;&#20215;&#20540;&#30340;&#23569;&#25968;&#27966;&#35266;&#28857;&#12290;&#36825;&#31181;&#30095;&#28431;&#21487;&#33021;&#23548;&#33268;&#20851;&#38190;&#20449;&#24687;&#30340;&#36951;&#28431;&#65292;&#24182;&#22312;&#26356;&#24191;&#27867;&#30340;&#32972;&#26223;&#19979;&#65292;&#21487;&#33021;&#25200;&#20081;&#26356;&#22823;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#24179;&#34913;&#12290;&#38543;&#30528;&#27880;&#35299;&#22120;&#24314;&#27169;&#30340;&#22810;&#26679;&#24615;&#20195;&#34920;&#25216;&#26415;&#30340;&#20986;&#29616;&#65292;&#26377;&#24517;&#35201;&#30740;&#31350;&#23427;&#20204;&#19982;&#25968;&#25454;&#38598;&#30340;&#31934;&#32454;&#29305;&#24449;&#32467;&#21512;&#30340;&#26377;&#25928;&#24615;&#12290;&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#25506;&#35752;&#20102;&#21508;&#31181;&#27880;&#35299;&#22120;&#24314;&#27169;&#25216;&#26415;&#65292;&#24182;&#27604;&#36739;&#20102;&#23427;&#20204;&#22312;&#19971;&#20010;&#35821;&#26009;&#24211;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02340v1 Announce Type: new  Abstract: Recent trends in natural language processing research and annotation tasks affirm a paradigm shift from the traditional reliance on a single ground truth to a focus on individual perspectives, particularly in subjective tasks. In scenarios where annotation tasks are meant to encompass diversity, models that solely rely on the majority class labels may inadvertently disregard valuable minority perspectives. This oversight could result in the omission of crucial information and, in a broader context, risk disrupting the balance within larger ecosystems. As the landscape of annotator modeling unfolds with diverse representation techniques, it becomes imperative to investigate their effectiveness with the fine-grained features of the datasets in view. This study systematically explores various annotator modeling techniques and compares their performance across seven corpora.   From our findings, we show that the commonly used user token mode
&lt;/p&gt;</description></item><item><title>AttentionStore&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36890;&#36807;&#23454;&#29616;KV&#32531;&#23384;&#30340;&#22797;&#29992;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26381;&#21153;&#20013;&#26174;&#33879;&#38477;&#20302;&#20102;&#22810;&#36718;&#23545;&#35805;&#20013;&#30340;&#37325;&#22797;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2403.19708</link><description>&lt;p&gt;
AttentionStore: &#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26381;&#21153;&#20013;&#23454;&#29616;&#22810;&#36718;&#23545;&#35805;&#20013;&#30340;&#27880;&#24847;&#21147;&#25104;&#26412;&#25928;&#30410;&#22797;&#29992;
&lt;/p&gt;
&lt;p&gt;
AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19708
&lt;/p&gt;
&lt;p&gt;
AttentionStore&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36890;&#36807;&#23454;&#29616;KV&#32531;&#23384;&#30340;&#22797;&#29992;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26381;&#21153;&#20013;&#26174;&#33879;&#38477;&#20302;&#20102;&#22810;&#36718;&#23545;&#35805;&#20013;&#30340;&#37325;&#22797;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22810;&#36718;&#23545;&#35805;&#19982;&#20154;&#31867;&#36827;&#34892;&#20132;&#20114;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#22522;&#26412;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38656;&#35201;&#37325;&#22797;&#35745;&#31639;&#21382;&#21490;&#35760;&#21495;&#30340;&#38190;&#20540;&#65288;KV&#65289;&#32531;&#23384;&#65292;&#23548;&#33268;&#29616;&#26377;&#29992;&#20110;&#25191;&#34892;&#22810;&#36718;&#23545;&#35805;&#30340;LLM&#26381;&#21153;&#24341;&#25806;&#25928;&#29575;&#20302;&#19979;&#65292;&#20135;&#29983;&#39640;&#26114;&#30340;&#26381;&#21153;&#25104;&#26412;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;AttentionStore&#65292;&#19968;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#36328;&#22810;&#36718;&#23545;&#35805;&#30340;KV&#32531;&#23384;&#22797;&#29992;&#65288;&#21363; &#27880;&#24847;&#21147;&#22797;&#29992;&#65289;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#37325;&#22797;&#35745;&#31639;&#24320;&#38144;&#12290;AttentionStore&#32500;&#25252;&#20102;&#19968;&#20010;&#23618;&#27425;&#32467;&#26500;&#30340;KV&#32531;&#23384;&#31995;&#32479;&#65292;&#21033;&#29992;&#25104;&#26412;&#25928;&#30410;&#30340;&#20869;&#23384;/&#23384;&#20648;&#20171;&#36136;&#20026;&#25152;&#26377;&#35831;&#27714;&#20445;&#23384;KV&#32531;&#23384;&#12290;&#20026;&#20102;&#20943;&#23569;&#24930;&#36895;&#20171;&#36136;&#30340;KV&#32531;&#23384;&#35775;&#38382;&#24320;&#38144;&#65292;AttentionStore&#37319;&#29992;&#36880;&#23618;&#39044;&#21152;&#36733;&#21644;&#24322;&#27493;&#20445;&#23384;&#26041;&#26696;&#65292;&#23558;KV&#32531;&#23384;&#35775;&#38382;&#19982;GPU&#35745;&#31639;&#37325;&#21472;&#12290;&#20026;&#30830;&#20445;&#35201;&#35775;&#38382;&#30340;KV&#32531;&#23384;&#8230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19708v1 Announce Type: new  Abstract: Interacting with humans through multi-turn conversations is a fundamental feature of large language models (LLMs). However, existing LLM serving engines for executing multi-turn conversations are inefficient due to the need to repeatedly compute the key-value (KV) caches of historical tokens, incurring high serving costs. To address the problem, this paper proposes AttentionStore, a new attention mechanism that enables the reuse of KV caches (i.e., attention reuse) across multi-turn conversations, significantly reducing the repetitive computation overheads. AttentionStore maintains a hierarchical KV caching system that leverages cost-effective memory/storage mediums to save KV caches for all requests. To reduce KV cache access overheads from slow mediums, AttentionStore employs layer-wise pre-loading and asynchronous saving schemes to overlap the KV cache access with the GPU computation. To ensure that the KV caches to be accessed are pl
&lt;/p&gt;</description></item><item><title>HateCOT&#25968;&#25454;&#38598;&#36890;&#36807;GPT-3.5-Turbo&#29983;&#25104;&#35299;&#37322;&#65292;&#23558;52,000&#20010;&#26679;&#26412;&#25968;&#25454;&#29992;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#22312;&#19981;&#21516;&#39046;&#22495;&#21644;&#20219;&#21153;&#19979;&#30340;&#25915;&#20987;&#24615;&#20869;&#23481;&#26816;&#27979;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.11456</link><description>&lt;p&gt;
HateCOT&#65306;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#27867;&#21270;&#25915;&#20987;&#24615;&#35328;&#35770;&#26816;&#27979;&#30340;&#35299;&#37322;&#22686;&#24378;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
HateCOT: An Explanation-Enhanced Dataset for Generalizable Offensive Speech Detection via Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11456
&lt;/p&gt;
&lt;p&gt;
HateCOT&#25968;&#25454;&#38598;&#36890;&#36807;GPT-3.5-Turbo&#29983;&#25104;&#35299;&#37322;&#65292;&#23558;52,000&#20010;&#26679;&#26412;&#25968;&#25454;&#29992;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#22312;&#19981;&#21516;&#39046;&#22495;&#21644;&#20219;&#21153;&#19979;&#30340;&#25915;&#20987;&#24615;&#20869;&#23481;&#26816;&#27979;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#30340;&#26222;&#21450;&#23548;&#33268;&#20102;&#23545;&#25915;&#20987;&#24615;&#20869;&#23481;&#30340;&#21487;&#38752;&#39640;&#25928;&#26816;&#27979;&#30340;&#38656;&#27714;&#65292;&#20026;&#20102;&#38480;&#21046;&#20854;&#26377;&#23475;&#24433;&#21709;&#12290;&#36825;&#23548;&#33268;&#20102;&#22823;&#37327;&#19982;&#26816;&#27979;&#25915;&#20987;&#24615;&#20869;&#23481;&#30456;&#20851;&#30340;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#30340;&#20986;&#29616;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;HateCOT&#65292;&#36825;&#26159;&#20174;&#22810;&#26679;&#21270;&#29616;&#26377;&#26469;&#28304;&#20013;&#25277;&#21462;&#30340;5.2&#19975;&#20010;&#26679;&#26412;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#30001;GPT-3.5-Turbo&#21644;&#20154;&#24037;&#31934;&#24515;&#21046;&#20316;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;HateCOT&#19978;&#20026;&#25915;&#20987;&#24615;&#20869;&#23481;&#26816;&#27979;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#38646;-shot&#21644;few-shot&#35774;&#32622;&#19979;&#26174;&#33879;&#25913;&#36827;&#20102;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#23613;&#31649;&#22312;&#39046;&#22495;&#21644;&#20219;&#21153;&#26041;&#38754;&#23384;&#22312;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11456v1 Announce Type: cross  Abstract: The ubiquitousness of social media has led to the need for reliable and efficient detection of offensive content to limit harmful effects. This has led to a proliferation of datasets and models related to detecting offensive content. While sophisticated models have attained strong performance on individual datasets, these models often do not generalize due to differences between how "offensive content" is conceptualized, and the resulting differences in how these datasets are labeled. In this paper, we introduce HateCOT, a dataset of 52,000 samples drawn from diverse existing sources with explanations generated by GPT-3.5-Turbo and human-curated. We show that pre-training models for the detection of offensive content on HateCOT significantly boots open-sourced Language Models on three benchmark datasets in both zero and few-shot settings, despite differences in domain and task.} We further find that HateCOT enables effective K-shot fin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#21451;&#22909;&#30340;&#36830;&#32493;&#27169;&#22411;&#32534;&#36753;&#19982;&#25209;&#37327;&#25903;&#25345;&#30340;&#26041;&#27861;COMEBA-HK&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.05330</link><description>&lt;p&gt;
&#36830;&#32493;&#27169;&#22411;&#32534;&#36753;&#19982;&#25209;&#37327;&#25903;&#25345;&#30340;HooK&#23618;
&lt;/p&gt;
&lt;p&gt;
Consecutive Model Editing with Batch alongside HooK Layers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05330
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#21451;&#22909;&#30340;&#36830;&#32493;&#27169;&#22411;&#32534;&#36753;&#19982;&#25209;&#37327;&#25903;&#25345;&#30340;&#26041;&#27861;COMEBA-HK&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20856;&#22411;&#30340;&#37325;&#26032;&#35757;&#32451;&#33539;&#24335;&#32791;&#26102;&#19988;&#28040;&#32791;&#36164;&#28304;&#65292;&#30740;&#31350;&#20154;&#21592;&#27491;&#22312;&#36716;&#21521;&#27169;&#22411;&#32534;&#36753;&#65292;&#20197;&#23547;&#25214;&#19968;&#31181;&#26377;&#25928;&#30340;&#12289;&#36830;&#32493;&#30340;&#12289;&#24182;&#25903;&#25345;&#25209;&#37327;&#26041;&#24335;&#30452;&#25509;&#32534;&#36753;&#27169;&#22411;&#34892;&#20026;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23384;&#22312;&#25152;&#26377;&#36825;&#20123;&#23454;&#29992;&#26399;&#26395;&#65292;&#29616;&#26377;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#21364;&#26410;&#33021;&#23454;&#29616;&#25152;&#26377;&#36825;&#20123;&#30446;&#26631;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#36825;&#31181;&#25903;&#25345;&#36830;&#32493;&#24615;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#30340;&#20869;&#23384;&#38656;&#27714;&#24448;&#24448;&#26159;&#31105;&#27490;&#24615;&#30340;&#65292;&#32463;&#24120;&#38656;&#35201;&#38543;&#30528;&#26102;&#38388;&#30340;&#22686;&#38271;&#36880;&#27493;&#22686;&#21152;&#22806;&#37096;&#20869;&#23384;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COMEBA-HK&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26082;&#26159;&#36830;&#32493;&#30340;&#21448;&#25903;&#25345;&#25209;&#37327;&#12290;COMEBA-HK&#23545;&#20110;&#23384;&#20648;&#20960;&#20010;&#20855;&#26377;&#26356;&#26032;&#26435;&#37325;&#30340;hook&#23618;&#20165;&#38656;&#23569;&#37327;&#20869;&#23384;&#65292;&#26159;&#20869;&#23384;&#21451;&#22909;&#30340;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21333;&#36718;&#21644;&#36830;&#32493;&#25209;&#37327;&#32534;&#36753;&#22330;&#26223;&#19979;&#20248;&#20110;&#20854;&#20182;&#25903;&#25345;&#25209;&#37327;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05330v1 Announce Type: new  Abstract: As the typical retraining paradigm is unacceptably time- and resource-consuming, researchers are turning to model editing in order to seek an effective, consecutive, and batch-supportive way to edit the model behavior directly. Despite all these practical expectations, existing model editing methods fail to realize all of them. Furthermore, the memory demands for such succession-supportive model editing approaches tend to be prohibitive, frequently necessitating an external memory that grows incrementally over time. To cope with these challenges, we propose COMEBA-HK, a model editing method that is both consecutive and batch-supportive. COMEBA-HK is memory-friendly as it only needs a small amount of it to store several hook layers with updated weights. Experimental results demonstrate the superiority of our method over other batch-supportive model editing methods under both single-round and consecutive batch editing scenarios. Extensive 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#20851;&#38190;&#28857;&#39537;&#21160;&#30340;&#25968;&#25454;&#21512;&#25104;&#26694;&#26550;(KPDDS)&#65292;&#21019;&#36896;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#35268;&#27169;&#30340;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;KPMath&#65292;&#20197;&#21450;&#36827;&#19968;&#27493;&#22686;&#24378;&#30340;KPMath-Plus&#25968;&#25454;&#38598;&#65292;&#23454;&#29616;&#20102;&#38646;-shot PASS@1&#31934;&#24230;&#20026;39.3%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.02333</link><description>&lt;p&gt;
&#22522;&#20110;&#20851;&#38190;&#28857;&#39537;&#21160;&#30340;&#25968;&#25454;&#21512;&#25104;&#21450;&#20854;&#22312;&#25968;&#23398;&#25512;&#29702;&#19978;&#30340;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Key-Point-Driven Data Synthesis with its Enhancement on Mathematical Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02333
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#20851;&#38190;&#28857;&#39537;&#21160;&#30340;&#25968;&#25454;&#21512;&#25104;&#26694;&#26550;(KPDDS)&#65292;&#21019;&#36896;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#35268;&#27169;&#30340;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;KPMath&#65292;&#20197;&#21450;&#36827;&#19968;&#27493;&#22686;&#24378;&#30340;KPMath-Plus&#25968;&#25454;&#38598;&#65292;&#23454;&#29616;&#20102;&#38646;-shot PASS@1&#31934;&#24230;&#20026;39.3%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#20854;&#24615;&#33021;&#36890;&#24120;&#21463;&#21040;&#39640;&#36136;&#37327;&#12289;&#20197;&#25512;&#29702;&#20026;&#37325;&#28857;&#30340;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#30340;&#24433;&#21709;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#20851;&#38190;&#28857;&#39537;&#21160;&#30340;&#25968;&#25454;&#21512;&#25104;&#65288;KPDDS&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#21512;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#30495;&#23454;&#25968;&#25454;&#28304;&#30340;&#20851;&#38190;&#28857;&#21644;&#31034;&#20363;&#23545;&#29983;&#25104;&#38382;&#39064;-&#31572;&#26696;&#23545;&#12290;KPDDS&#30830;&#20445;&#36890;&#36807;&#20005;&#26684;&#30340;&#36136;&#37327;&#25511;&#21046;&#21644;&#22823;&#35268;&#27169;&#24615;&#33021;&#30340;&#29983;&#25104;&#26032;&#39062;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;KPMath&#65292;&#36804;&#20170;&#20026;&#27490;&#37327;&#36523;&#23450;&#21046;&#30340;&#26368;&#24191;&#27867;&#30340;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#19968;&#30334;&#19975;&#20010;&#20197;&#19978;&#30340;&#38382;&#39064;-&#31572;&#26696;&#23545;&#12290;&#21033;&#29992;KPMath&#24182;&#23558;&#20854;&#19982;&#20854;&#20182;&#25512;&#29702;&#23494;&#38598;&#30340;&#35821;&#26009;&#24211;&#36827;&#34892;&#25193;&#20805;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#20840;&#38754;&#30340;KPMath-Plus&#25968;&#25454;&#38598;&#12290;&#23558;Mistral-7B&#27169;&#22411;&#22312;KPMath-Plus&#19978;&#24494;&#35843;&#65292;&#20351;&#20854;&#22312;MATH&#27979;&#35797;&#38598;&#19978;&#23454;&#29616;&#38646;-shot PASS@1&#31934;&#24230;&#36798;&#21040;39.3%&#65292;&#36825;&#26159;&#19968;&#39033;&#31361;&#30772;&#24615;&#25104;&#23601;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02333v1 Announce Type: cross  Abstract: Large language models (LLMs) have shown great potential in complex reasoning tasks, yet their performance is often hampered by the scarcity of high-quality, reasoning-focused training datasets. Addressing this challenge, we propose Key-Point-Driven Data Synthesis (KPDDS), a novel data synthesis framework that synthesizes question-answer pairs by leveraging key points and exemplar pairs from authentic data sources. KPDDS ensures the generation of novel questions with rigorous quality control and substantial scalability. As a result, we present KPMath, the most extensive synthetic dataset tailored for mathematical reasoning to date, comprising over one million question-answer pairs. Utilizing KPMath and augmenting it with additional reasoning-intensive corpora, we create the comprehensive KPMath-Plus dataset. Fine-tuning the Mistral-7B model on KPMath-Plus yields a zero-shot PASS@1 accuracy of 39.3% on the MATH test set, a performance th
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#20449;&#24687;&#27969;&#36335;&#30001;&#22270;&#26469;&#25581;&#31034;&#27169;&#22411;&#20869;&#37096;&#30340;&#20851;&#38190;&#33410;&#28857;&#21644;&#25805;&#20316;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#28608;&#27963;&#20462;&#34917;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#24402;&#22240;&#23454;&#29616;&#65292;&#22312;&#19981;&#38656;&#35201;&#20154;&#24037;&#24178;&#39044;&#35774;&#35745;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#26377;&#25928;&#22320;&#20998;&#26512;&#27169;&#22411;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2403.00824</link><description>&lt;p&gt;
&#20449;&#24687;&#27969;&#36335;&#30001;&#65306;&#33258;&#21160;&#35299;&#37322;&#35268;&#27169;&#21270;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Information Flow Routes: Automatically Interpreting Language Models at Scale
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00824
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#20449;&#24687;&#27969;&#36335;&#30001;&#22270;&#26469;&#25581;&#31034;&#27169;&#22411;&#20869;&#37096;&#30340;&#20851;&#38190;&#33410;&#28857;&#21644;&#25805;&#20316;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#28608;&#27963;&#20462;&#34917;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#24402;&#22240;&#23454;&#29616;&#65292;&#22312;&#19981;&#38656;&#35201;&#20154;&#24037;&#24178;&#39044;&#35774;&#35745;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#26377;&#25928;&#22320;&#20998;&#26512;&#27169;&#22411;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27169;&#22411;&#23454;&#29616;&#30340;&#26426;&#21046;&#65292;&#20449;&#24687;&#36890;&#36807;&#32593;&#32476;&#20869;&#37096;&#30340;&#36335;&#30001;&#36827;&#34892;&#20256;&#36755;&#12290;&#36825;&#20123;&#36335;&#30001;&#21487;&#20197;&#34987;&#34920;&#31034;&#20026;&#22270;&#65292;&#20854;&#20013;&#33410;&#28857;&#23545;&#24212;&#20110;&#26631;&#35760;&#34920;&#31034;&#65292;&#36793;&#23545;&#24212;&#20110;&#32593;&#32476;&#20869;&#37096;&#30340;&#25805;&#20316;&#12290;&#25105;&#20204;&#20197;&#33258;&#39030;&#21521;&#19979;&#30340;&#26041;&#24335;&#33258;&#21160;&#26500;&#24314;&#36825;&#20123;&#22270;&#65292;&#38024;&#23545;&#27599;&#19968;&#20010;&#39044;&#27979;&#21482;&#20445;&#30041;&#26368;&#37325;&#35201;&#30340;&#33410;&#28857;&#21644;&#36793;&#12290;&#19982;&#29616;&#26377;&#30340;&#20381;&#36182;&#20110;&#28608;&#27963;&#20462;&#34917;&#30340;&#24037;&#20316;&#27969;&#30456;&#27604;&#65292;&#25105;&#20204;&#36890;&#36807;&#24402;&#22240;&#26469;&#20570;&#21040;&#36825;&#19968;&#28857;&#65306;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#20165;&#36890;&#36807;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#26377;&#25928;&#22320;&#25581;&#31034;&#29616;&#26377;&#30340;&#30005;&#36335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#36828;&#36828;&#36229;&#20986;&#20102;&#20462;&#34917;&#65306;&#25105;&#20204;&#19981;&#38656;&#35201;&#20154;&#31867;&#20180;&#32454;&#35774;&#35745;&#39044;&#27979;&#27169;&#26495;&#65292;&#21487;&#20197;&#20026;&#20219;&#20309;&#39044;&#27979;&#25552;&#21462;&#20449;&#24687;&#27969;&#36335;&#30001;&#65288;&#19981;&#20165;&#20165;&#26159;&#22312;&#20801;&#35768;&#30340;&#27169;&#26495;&#20043;&#38388;&#30340;&#39044;&#27979;&#65289;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21487;&#20197;&#23601;&#27169;&#22411;&#34892;&#20026;&#36827;&#34892;&#19968;&#33324;&#24615;&#35752;&#35770;&#65292;&#38024;&#23545;&#29305;&#23450;&#31867;&#22411;&#30340;&#39044;&#27979;&#25110;&#19981;&#21516;&#30340;&#39046;&#22495;&#12290;&#25105;&#20204;&#22312;Llama 2&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#19968;&#26041;&#27861;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00824v1 Announce Type: cross  Abstract: Information flows by routes inside the network via mechanisms implemented in the model. These routes can be represented as graphs where nodes correspond to token representations and edges to operations inside the network. We automatically build these graphs in a top-down manner, for each prediction leaving only the most important nodes and edges. In contrast to the existing workflows relying on activation patching, we do this through attribution: this allows us to efficiently uncover existing circuits with just a single forward pass. Additionally, the applicability of our method is far beyond patching: we do not need a human to carefully design prediction templates, and we can extract information flow routes for any prediction (not just the ones among the allowed templates). As a result, we can talk about model behavior in general, for specific types of predictions, or different domains. We experiment with Llama 2 and show that the rol
&lt;/p&gt;</description></item><item><title>JMLR&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#21307;&#23398;&#39046;&#22495;&#25552;&#39640;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#24615;&#33021;&#65292;&#38477;&#20302;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#65292;&#22686;&#24378;&#27169;&#22411;&#21033;&#29992;&#21307;&#30103;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#21644;&#22238;&#31572;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.17887</link><description>&lt;p&gt;
JMLR&#65306;&#32852;&#21512;&#21307;&#30103;LLM&#21644;&#26816;&#32034;&#35757;&#32451;&#20197;&#22686;&#24378;&#25512;&#29702;&#21644;&#19987;&#19994;&#38382;&#39064;&#22238;&#31572;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
JMLR: Joint Medical LLM and Retrieval Training for Enhancing Reasoning and Professional Question Answering Capability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17887
&lt;/p&gt;
&lt;p&gt;
JMLR&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#21307;&#23398;&#39046;&#22495;&#25552;&#39640;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#24615;&#33021;&#65292;&#38477;&#20302;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#65292;&#22686;&#24378;&#27169;&#22411;&#21033;&#29992;&#21307;&#30103;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#21644;&#22238;&#31572;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21307;&#30103;&#25968;&#25454;&#30340;&#29190;&#28856;&#24615;&#22686;&#38271;&#21644;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#31934;&#20934;&#21307;&#23398;&#24050;&#32463;&#25104;&#20026;&#22686;&#24378;&#21307;&#30103;&#26381;&#21153;&#36136;&#37327;&#21644;&#25928;&#29575;&#30340;&#20851;&#38190;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21307;&#30103;&#30693;&#35782;&#33719;&#21462;&#21644;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#20013;&#21457;&#25381;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#36825;&#20123;&#31995;&#32479;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#22312;&#24494;&#35843;&#38454;&#27573;&#21516;&#26102;&#35757;&#32451;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#31995;&#32479;&#21644;LLM&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#32852;&#21512;&#21307;&#30103;LLM&#21644;&#26816;&#32034;&#35757;&#32451;&#65288;JMLR&#65289;&#30340;&#26041;&#27861;&#26088;&#22312;&#20811;&#26381;&#20256;&#32479;&#27169;&#22411;&#22312;&#22788;&#29702;&#21307;&#23398;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#37319;&#29992;&#21516;&#27493;&#35757;&#32451;&#26426;&#21046;&#65292;JMLR&#20943;&#23569;&#20102;&#23545;&#35745;&#31639;&#36164;&#28304;&#30340;&#38656;&#27714;&#65292;&#24182;&#22686;&#24378;&#20102;&#27169;&#22411;&#21033;&#29992;&#21307;&#30103;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#21644;&#22238;&#31572;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17887v1 Announce Type: new  Abstract: With the explosive growth of medical data and the rapid development of artificial intelligence technology, precision medicine has emerged as a key to enhancing the quality and efficiency of healthcare services. In this context, Large Language Models (LLMs) play an increasingly vital role in medical knowledge acquisition and question-answering systems. To further improve the performance of these systems in the medical domain, we introduce an innovative method that jointly trains an Information Retrieval (IR) system and an LLM during the fine-tuning phase. This approach, which we call Joint Medical LLM and Retrieval Training (JMLR), is designed to overcome the challenges faced by traditional models in handling medical question-answering tasks. By employing a synchronized training mechanism, JMLR reduces the demand for computational resources and enhances the model's ability to leverage medical knowledge for reasoning and answering question
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35270;&#35282;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25552;&#31034;&#20248;&#21270;&#22120;&#26469;&#25913;&#36827;&#20219;&#21153;&#25552;&#31034;&#65292;&#36890;&#36807;&#31867;&#27604;&#22522;&#20110;&#26799;&#24230;&#30340;&#27169;&#22411;&#20248;&#21270;&#22120;&#65292;&#35774;&#35745;&#20102;&#25913;&#36827;&#30340;LLM-based&#25552;&#31034;&#20248;&#21270;&#22120;&#31574;&#30053;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#22522;&#20110;&#26799;&#24230;&#21551;&#21457;&#30340;LLM-based&#25552;&#31034;&#20248;&#21270;&#22120;GPO&#12290;</title><link>https://arxiv.org/abs/2402.17564</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#37322;&#25918;&#20026;&#25552;&#31034;&#20248;&#21270;&#22120;&#30340;&#28508;&#21147;&#65306;&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#27169;&#22411;&#20248;&#21270;&#22120;&#30340;&#31867;&#27604;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Unleashing the Potential of Large Language Models as Prompt Optimizers: An Analogical Analysis with Gradient-based Model Optimizers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35270;&#35282;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25552;&#31034;&#20248;&#21270;&#22120;&#26469;&#25913;&#36827;&#20219;&#21153;&#25552;&#31034;&#65292;&#36890;&#36807;&#31867;&#27604;&#22522;&#20110;&#26799;&#24230;&#30340;&#27169;&#22411;&#20248;&#21270;&#22120;&#65292;&#35774;&#35745;&#20102;&#25913;&#36827;&#30340;LLM-based&#25552;&#31034;&#20248;&#21270;&#22120;&#31574;&#30053;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#22522;&#20110;&#26799;&#24230;&#21551;&#21457;&#30340;LLM-based&#25552;&#31034;&#20248;&#21270;&#22120;GPO&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#26159;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24615;&#33021;&#30340;&#37325;&#35201;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;LLMs&#20316;&#20026;&#25552;&#31034;&#20248;&#21270;&#22120;&#20855;&#26377;&#28508;&#21147;&#65292;&#21487;&#20197;&#36890;&#36807;&#36845;&#20195;&#25913;&#36827;&#29983;&#25104;&#25913;&#36827;&#30340;&#20219;&#21153;&#25552;&#31034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35270;&#35282;&#65292;&#36890;&#36807;&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#27169;&#22411;&#20248;&#21270;&#22120;&#36827;&#34892;&#31867;&#27604;&#26469;&#30740;&#31350;&#22522;&#20110;LLM&#30340;&#25552;&#31034;&#20248;&#21270;&#22120;&#30340;&#35774;&#35745;&#12290;&#20026;&#20102;&#36830;&#25509;&#36825;&#20004;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#30830;&#23450;&#27169;&#22411;&#21442;&#25968;&#23398;&#20064;&#20013;&#30340;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#65306;&#26356;&#26032;&#26041;&#21521;&#21644;&#26356;&#26032;&#26041;&#27861;&#12290;&#19987;&#27880;&#20110;&#36825;&#20004;&#20010;&#26041;&#38754;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#26799;&#24230;&#20248;&#21270;&#30340;&#29702;&#35770;&#26694;&#26550;&#21644;&#23398;&#20064;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#25913;&#36827;&#30340;LLM-based&#25552;&#31034;&#20248;&#21270;&#22120;&#31574;&#30053;&#12290;&#36890;&#36807;&#31995;&#32479;&#20998;&#26512;&#20016;&#23500;&#30340;&#25913;&#36827;&#31574;&#30053;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#19968;&#20010;&#33021;&#21147;&#24378;&#22823;&#30340;&#22522;&#20110;&#26799;&#24230;&#21551;&#21457;&#30340;LLM-based&#25552;&#31034;&#20248;&#21270;&#22120;&#65292;&#31216;&#20026;GPO&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17564v1 Announce Type: new  Abstract: Automatic prompt optimization is an important approach to improving the performance of large language models (LLMs). Recent research demonstrates the potential of using LLMs as prompt optimizers, which can generate improved task prompts via iterative refinement. In this paper, we propose a novel perspective to investigate the design of LLM-based prompt optimizers, by drawing an analogy with gradient-based model optimizers. To connect these two approaches, we identify two pivotal factors in model parameter learning: update direction and update method. Focused on the two aspects, we borrow the theoretical framework and learning methods from gradient-based optimization to design improved strategies for LLM-based prompt optimizers. By systematically analyzing a rich set of improvement strategies, we further develop a capable Gradient-inspired LLM-based Prompt Optimizer called GPO. At each step, it first retrieves relevant prompts from the op
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;mEdIT&#65292;&#36825;&#26159;CoEdIT&#30340;&#22810;&#35821;&#35328;&#25193;&#23637;&#65292;&#20351;&#29992;&#25351;&#20196;&#35843;&#25972;&#23545;&#22810;&#35821;&#35328;&#25991;&#26412;&#32534;&#36753;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#35757;&#32451;&#65292;&#22312;&#22810;&#35821;&#35328;&#25991;&#26412;&#32534;&#36753;&#20219;&#21153;&#20013;&#34920;&#29616;&#24378;&#21170;&#12290;</title><link>https://arxiv.org/abs/2402.16472</link><description>&lt;p&gt;
mEdIT: &#36890;&#36807;&#25351;&#20196;&#35843;&#25972;&#23454;&#29616;&#22810;&#35821;&#35328;&#25991;&#26412;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
mEdIT: Multilingual Text Editing via Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16472
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;mEdIT&#65292;&#36825;&#26159;CoEdIT&#30340;&#22810;&#35821;&#35328;&#25193;&#23637;&#65292;&#20351;&#29992;&#25351;&#20196;&#35843;&#25972;&#23545;&#22810;&#35821;&#35328;&#25991;&#26412;&#32534;&#36753;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#35757;&#32451;&#65292;&#22312;&#22810;&#35821;&#35328;&#25991;&#26412;&#32534;&#36753;&#20219;&#21153;&#20013;&#34920;&#29616;&#24378;&#21170;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;mEdIT&#65292;&#36825;&#26159;CoEdIT&#30340;&#19968;&#20010;&#22810;&#35821;&#35328;&#25193;&#23637;&#65292;CoEdIT&#26159;&#26368;&#36817;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#32534;&#36753;&#27169;&#22411;&#65292;&#29992;&#20110;&#20889;&#20316;&#36741;&#21161;&#12290;mEdIT&#27169;&#22411;&#36890;&#36807;&#25351;&#20196;&#35843;&#25972;&#23545;&#22810;&#35821;&#35328;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#24494;&#35843;&#35757;&#32451;&#12290;&#23427;&#20204;&#26088;&#22312;&#25509;&#25910;&#29992;&#25143;&#20174;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#20013;&#25351;&#23450;&#25152;&#38656;&#25991;&#26412;&#23646;&#24615;&#30340;&#25351;&#20196;&#65292;&#20363;&#22914;Grammatik korrigieren&#65288;&#24503;&#35821;&#65289;&#25110;Parafrasee la oraci&#243;n&#65288;&#35199;&#29677;&#29273;&#35821;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#20174;&#22810;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#20154;&#24037;&#27880;&#37322;&#25991;&#26412;&#32534;&#36753;&#25968;&#25454;&#38598;&#20013;&#31574;&#21010;&#25968;&#25454;&#65292;&#38024;&#23545;&#20845;&#31181;&#19981;&#21516;&#35821;&#31995;&#30340;&#22810;&#35821;&#35328;&#65292;&#20026;&#19977;&#20010;&#25991;&#26412;&#32534;&#36753;&#20219;&#21153;&#65288;&#35821;&#27861;&#38169;&#35823;&#26657;&#27491;&#65288;GEC&#65289;&#12289;&#25991;&#26412;&#31616;&#21270;&#21644;&#25913;&#20889;&#65289;&#26500;&#24314;&#20102;mEdIT&#12290;&#25105;&#20204;&#35814;&#32454;&#35828;&#26126;&#20102;mEdIT&#27169;&#22411;&#30340;&#35774;&#35745;&#21644;&#35757;&#32451;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#35768;&#22810;&#22810;&#35821;&#35328;&#25991;&#26412;&#32534;&#36753;&#22522;&#20934;&#38598;&#19978;&#19982;&#20854;&#20182;&#22810;&#35821;&#35328;LLMs&#30456;&#27604;&#30340;&#24378;&#22823;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;mEdIT gen
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16472v1 Announce Type: cross  Abstract: We introduce mEdIT, a multi-lingual extension to CoEdIT -- the recent state-of-the-art text editing models for writing assistance. mEdIT models are trained by fine-tuning multi-lingual large, pre-trained language models (LLMs) via instruction tuning. They are designed to take instructions from the user specifying the attributes of the desired text in the form of natural language instructions, such as Grammatik korrigieren (German) or Parafrasee la oraci\'on (Spanish). We build mEdIT by curating data from multiple publicly available human-annotated text editing datasets for three text editing tasks (Grammatical Error Correction (GEC), Text Simplification, and Paraphrasing) across diverse languages belonging to six different language families. We detail the design and training of mEdIT models and demonstrate their strong performance on many multi-lingual text editing benchmarks against other multilingual LLMs. We also find that mEdIT gen
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;LLMBind&#65292;&#19968;&#31181;&#32479;&#19968;&#30340;&#27169;&#24577;&#20219;&#21153;&#38598;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#20219;&#21153;&#27169;&#22411;&#32465;&#23450;&#22312;&#19968;&#36215;&#65292;&#23454;&#29616;&#20102;&#22810;&#31181;&#27169;&#24577;&#20219;&#21153;&#30340;&#28789;&#27963;&#36755;&#20837;&#21644;&#36755;&#20986;&#32452;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.14891</link><description>&lt;p&gt;
LLMBind: &#19968;&#31181;&#32479;&#19968;&#30340;&#27169;&#24577;&#20219;&#21153;&#38598;&#25104;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
LLMBind: A Unified Modality-Task Integration Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14891
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;LLMBind&#65292;&#19968;&#31181;&#32479;&#19968;&#30340;&#27169;&#24577;&#20219;&#21153;&#38598;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#20219;&#21153;&#27169;&#22411;&#32465;&#23450;&#22312;&#19968;&#36215;&#65292;&#23454;&#29616;&#20102;&#22810;&#31181;&#27169;&#24577;&#20219;&#21153;&#30340;&#28789;&#27963;&#36755;&#20837;&#21644;&#36755;&#20986;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#20110;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#21508;&#31181;&#27169;&#24577;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#23545;&#20110;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#38598;&#25104;&#33021;&#21147;&#26377;&#38480;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24102;&#22836;&#25506;&#32034;&#24182;&#25552;&#20986;&#20102;LLMBind&#65292;&#19968;&#31181;&#29992;&#20110;&#27169;&#24577;&#20219;&#21153;&#38598;&#25104;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#30456;&#24212;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#27169;&#22411;&#19982;&#20219;&#21153;&#29305;&#23450;&#30340;&#26631;&#35760;&#32465;&#23450;&#22312;&#19968;&#36215;&#12290;&#22240;&#27492;&#65292;LLMBind&#21487;&#20197;&#20197;&#22810;&#31181;&#22270;&#20687;&#12289;&#25991;&#26412;&#12289;&#35270;&#39057;&#21644;&#38899;&#39057;&#30340;&#32452;&#21512;&#35299;&#37322;&#36755;&#20837;&#24182;&#29983;&#25104;&#36755;&#20986;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#19987;&#23478;&#28151;&#21512;&#25216;&#26415;&#65292;&#36890;&#36807;&#19981;&#21516;&#19987;&#23478;&#20043;&#38388;&#30340;&#21327;&#20316;&#23454;&#29616;&#19981;&#21516;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#26377;&#25928;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;40&#19975;&#26465;&#25351;&#20196;&#25968;&#25454;&#30340;&#22810;&#20219;&#21153;&#25968;&#25454;&#38598;&#65292;&#35299;&#38145;&#20102;&#20132;&#20114;&#24335;&#35270;&#35273;&#29983;&#25104;&#21644;&#32534;&#36753;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14891v1 Announce Type: cross  Abstract: While recent progress in multimodal large language models tackles various modality tasks, they posses limited integration capabilities for complex multi-modality tasks, consequently constraining the development of the field. In this work, we take the initiative to explore and propose the LLMBind, a unified framework for modality task integration, which binds Large Language Models and corresponding pre-trained task models with task-specific tokens. Consequently, LLMBind can interpret inputs and produce outputs in versatile combinations of image, text, video, and audio. Specifically, we introduce a Mixture-of-Experts technique to enable effective learning for different multimodal tasks through collaboration among diverse experts. Furthermore, we create a multi-task dataset comprising 400k instruction data, which unlocks the ability for interactive visual generation and editing tasks. Extensive experiments show the effectiveness of our fr
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#27969;&#31243;&#65292;&#21033;&#29992;GPT-3.5&#21644;GPT-4&#29983;&#25104;&#39640;&#36136;&#37327;&#21453;&#39304;&#65292;&#20197;&#22686;&#24378;&#20020;&#24202;&#31508;&#35760;&#25688;&#35201;&#20013;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#65292;&#24357;&#34917;&#20102;&#19987;&#23478;&#27880;&#37322;&#25968;&#25454;&#30340;&#39640;&#25104;&#26412;&#21644;&#26377;&#38480;&#21487;&#29992;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.13919</link><description>&lt;p&gt;
SYNFAC-EDIT: &#29992;&#20110;&#20020;&#24202;&#25688;&#35201;&#20013;&#30340;&#20107;&#23454;&#23545;&#40784;&#30340;&#21512;&#25104;&#27169;&#20223;&#32534;&#36753;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
SYNFAC-EDIT: Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13919
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#27969;&#31243;&#65292;&#21033;&#29992;GPT-3.5&#21644;GPT-4&#29983;&#25104;&#39640;&#36136;&#37327;&#21453;&#39304;&#65292;&#20197;&#22686;&#24378;&#20020;&#24202;&#31508;&#35760;&#25688;&#35201;&#20013;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#65292;&#24357;&#34917;&#20102;&#19987;&#23478;&#27880;&#37322;&#25968;&#25454;&#30340;&#39640;&#25104;&#26412;&#21644;&#26377;&#38480;&#21487;&#29992;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT&#21644;Llama&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#22312;&#20107;&#23454;&#19981;&#20934;&#30830;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#36825;&#26159;&#20020;&#24202;NLP&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#38169;&#35823;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#21518;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#20107;&#23454;&#23545;&#40784;&#30340;&#19987;&#23478;&#27880;&#37322;&#25968;&#25454;&#25104;&#26412;&#39640;&#26114;&#19988;&#26377;&#38480;&#30340;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#27969;&#31243;&#65292;&#21033;&#29992;GPT-3.5&#21644;GPT-4&#29983;&#25104;&#39640;&#36136;&#37327;&#21453;&#39304;&#65292;&#26088;&#22312;&#22686;&#24378;&#20020;&#24202;&#31508;&#35760;&#25688;&#35201;&#20013;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#32534;&#36753;&#21453;&#39304;&#65292;&#22312;&#27809;&#26377;&#39069;&#22806;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#65292;&#27169;&#25311;&#20102;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#25913;&#21892;AI&#31995;&#32479;&#36755;&#20986;&#30340;&#23454;&#38469;&#22330;&#26223;&#12290;&#23613;&#31649;GPT&#22312;&#21508;&#31181;&#20020;&#24202;NLP&#20219;&#21153;&#20013;&#37117;&#34920;&#29616;&#20986;&#20102;&#19987;&#19994;&#27700;&#24179;&#65292;&#27604;&#22914;&#21307;&#23398;&#25191;&#29031;&#32771;&#35797;&#65292;&#20294;&#23545;&#20854;&#25552;&#20379;&#25913;&#21892;&#36739;&#24369;LM&#25110;LLM&#29983;&#25104;&#36136;&#37327;&#30340;&#19987;&#19994;&#32423;&#32534;&#36753;&#21453;&#39304;&#30340;&#30740;&#31350;&#24456;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13919v1 Announce Type: cross  Abstract: Large Language Models (LLMs) such as GPT and Llama have demonstrated significant achievements in summarization tasks but struggle with factual inaccuracies, a critical issue in clinical NLP applications where errors could lead to serious consequences. To counter the high costs and limited availability of expert-annotated data for factual alignment, this study introduces an innovative pipeline that utilizes GPT-3.5 and GPT-4 to generate high-quality feedback aimed at enhancing factual consistency in clinical note summarization. Our research primarily focuses on edit feedback, mirroring the practical scenario in which medical professionals refine AI system outputs without the need for additional annotations. Despite GPT's proven expertise in various clinical NLP tasks, such as the Medical Licensing Examination, there is scant research on its capacity to deliver expert-level edit feedback for improving weaker LMs or LLMs generation qualit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReAlign&#30340;&#31616;&#21333;&#26377;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#26684;&#24335;&#21270;&#25351;&#23548;&#25968;&#25454;&#30340;&#21709;&#24212;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#23545;&#40784;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.12219</link><description>&lt;p&gt;
&#37325;&#26032;&#26684;&#24335;&#21270;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Reformatted Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReAlign&#30340;&#31616;&#21333;&#26377;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#26684;&#24335;&#21270;&#25351;&#23548;&#25968;&#25454;&#30340;&#21709;&#24212;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#23545;&#40784;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#24494;&#35843;&#25968;&#25454;&#23545;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#33267;&#20851;&#37325;&#35201;&#12290;&#24403;&#21069;&#25913;&#21892;&#25968;&#25454;&#36136;&#37327;&#30340;&#26041;&#27861;&#35201;&#20040;&#32791;&#26102;&#36153;&#21147;&#65292;&#35201;&#20040;&#23481;&#26131;&#21463;&#21040;LLM&#24187;&#35273;&#24341;&#36215;&#30340;&#20107;&#23454;&#38169;&#35823;&#24433;&#21709;&#12290;&#26412;&#25991;&#25506;&#35752;&#25552;&#21319;&#29616;&#26377;&#25351;&#23548;&#25968;&#25454;&#36136;&#37327;&#20197;&#26356;&#22909;&#22320;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;ReAlign&#30340;&#31616;&#21333;&#26377;&#25928;&#26041;&#27861;&#65292;&#23427;&#23558;&#25351;&#23548;&#25968;&#25454;&#30340;&#21709;&#24212;&#37325;&#26032;&#26684;&#24335;&#21270;&#20026;&#26356;&#31526;&#21512;&#39044;&#20808;&#24314;&#31435;&#26631;&#20934;&#21644;&#32534;&#35793;&#35777;&#25454;&#30340;&#26684;&#24335;&#12290;&#35813;&#26041;&#27861;&#26368;&#23567;&#21270;&#20102;&#20154;&#31867;&#27880;&#37322;&#12289;&#24187;&#35273;&#21644;&#25193;&#23637;&#22256;&#38590;&#65292;&#19982;&#29616;&#26377;&#23545;&#40784;&#25216;&#26415;&#27491;&#20132;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ReAlign&#26174;&#33879;&#25552;&#21319;&#20102;LLMs&#30340;&#25972;&#20307;&#23545;&#40784;&#33021;&#21147;&#12289;&#25968;&#23398;&#25512;&#29702;&#12289;&#20107;&#23454;&#24615;&#21644;&#21487;&#35835;&#24615;&#12290;&#20196;&#20154;&#40723;&#33310;&#30340;&#26159;&#65292;&#22312;&#19981;&#24341;&#20837;&#20219;&#20309;&#39069;&#22806;&#25968;&#25454;&#25110;&#20808;&#36827;&#35757;&#32451;&#25216;&#26415;&#30340;&#24773;&#20917;&#19979;&#65292;&#20165;&#36890;&#36807;&#37325;&#26032;&#26684;&#24335;&#21270;&#21709;&#24212;&#65292;LLaMA-2-13
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12219v1 Announce Type: cross  Abstract: The quality of finetuning data is crucial for aligning large language models (LLMs) with human values. Current methods to improve data quality are either labor-intensive or prone to factual errors caused by LLM hallucinations. This paper explores elevating the quality of existing instruction data to better align with human values, introducing a simple and effective approach named ReAlign, which reformats the responses of instruction data into a format that better aligns with pre-established criteria and the collated evidence. This approach minimizes human annotation, hallucination, and the difficulty in scaling, remaining orthogonal to existing alignment techniques. Experimentally, ReAlign significantly boosts the general alignment ability, math reasoning, factuality, and readability of the LLMs.   Encouragingly, without introducing any additional data or advanced training techniques, and merely by reformatting the response, LLaMA-2-13
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#24863;&#30693;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#26174;&#24335;&#21644;&#38544;&#24335;&#26041;&#24335;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#30693;&#35782;&#30340;&#35748;&#35782;&#65292;&#21253;&#25324;&#35757;&#32451;&#27169;&#22411;&#26126;&#30830;&#35782;&#21035;&#31572;&#26696;&#20013;&#30340;&#30693;&#35782;&#19977;&#20803;&#32452;&#21644;&#38544;&#24335;&#21306;&#20998;&#21487;&#38752;&#21644;&#19981;&#21487;&#38752;&#30340;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2402.11176</link><description>&lt;p&gt;
KnowTuning&#65306;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#24863;&#30693;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
KnowTuning: Knowledge-aware Fine-tuning for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11176
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#24863;&#30693;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#26174;&#24335;&#21644;&#38544;&#24335;&#26041;&#24335;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#30693;&#35782;&#30340;&#35748;&#35782;&#65292;&#21253;&#25324;&#35757;&#32451;&#27169;&#22411;&#26126;&#30830;&#35782;&#21035;&#31572;&#26696;&#20013;&#30340;&#30693;&#35782;&#19977;&#20803;&#32452;&#21644;&#38544;&#24335;&#21306;&#20998;&#21487;&#38752;&#21644;&#19981;&#21487;&#38752;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#19978;&#21462;&#24471;&#25104;&#21151;&#65292;&#20294;&#20173;&#28982;&#38590;&#20197;&#26377;&#25928;&#21033;&#29992;&#30693;&#35782;&#36827;&#34892;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#65292;&#34920;&#29616;&#20986;&#29983;&#25104;&#19981;&#23436;&#25972;&#12289;&#38750;&#20107;&#23454;&#24615;&#25110;&#19981;&#21512;&#36923;&#36753;&#30340;&#31572;&#26696;&#31561;&#38480;&#21046;&#12290;&#36825;&#20123;&#38480;&#21046;&#28304;&#20110;LLMs&#22312;&#26222;&#36890;&#24494;&#35843;&#26399;&#38388;&#23545;&#30693;&#35782;&#30340;&#35748;&#35782;&#19981;&#36275;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#24863;&#30693;&#24494;&#35843;&#65288;KnowTuning&#65289;&#26041;&#27861;&#65292;&#20197;&#26126;&#30830;&#21644;&#38544;&#24335;&#22320;&#25913;&#21892;LLMs&#30340;&#30693;&#35782;&#35748;&#35782;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26174;&#24335;&#30693;&#35782;&#24863;&#30693;&#29983;&#25104;&#38454;&#27573;&#65292;&#35757;&#32451;LLMs&#26126;&#30830;&#35782;&#21035;&#31572;&#26696;&#20013;&#30340;&#30693;&#35782;&#19977;&#20803;&#32452;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#38544;&#24335;&#30693;&#35782;&#24863;&#30693;&#27604;&#36739;&#38454;&#27573;&#65292;&#35757;&#32451;LLMs&#38544;&#24335;&#21306;&#20998;&#21487;&#38752;&#21644;&#19981;&#21487;&#38752;&#30340;&#30693;&#35782;&#65292;&#21253;&#25324;&#23436;&#25972;&#24615;&#12289;&#20107;&#23454;&#24615;&#21644;&#36923;&#36753;&#24615;&#19977;&#20010;&#26041;&#38754;&#12290;&#23545;&#36890;&#29992;&#21644;&#21307;&#23398;&#38382;&#31572;&#65288;QA&#65289;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11176v1 Announce Type: cross  Abstract: Despite their success at many natural language processing (NLP) tasks, large language models (LLMs) still struggle to effectively leverage knowledge for knowledge-intensive tasks, manifesting limitations such as generating incomplete, non-factual, or illogical answers. These limitations stem from inadequate knowledge awareness of LLMs during vanilla fine-tuning. To address these problems, we propose a knowledge-aware fine-tuning (KnowTuning) method to explicitly and implicitly improve the knowledge awareness of LLMs. We devise an explicit knowledge-aware generation stage to train LLMs to explicitly identify knowledge triples in answers. We also propose an implicit knowledge-aware comparison stage to train LLMs to implicitly distinguish between reliable and unreliable knowledge, in three aspects: completeness, factuality, and logicality. Extensive experiments on both generic and medical question answering (QA) datasets confirm the effec
&lt;/p&gt;</description></item><item><title>DPR&#24494;&#35843;&#39044;&#35757;&#32451;&#32593;&#32476;&#20197;&#22686;&#24378;&#26597;&#35810;&#21644;&#30456;&#20851;&#25991;&#26412;&#25968;&#25454;&#20043;&#38388;&#30340;&#23884;&#20837;&#23545;&#40784;&#65292;&#21457;&#29616;&#35757;&#32451;&#20013;&#30693;&#35782;&#21435;&#20013;&#24515;&#21270;&#65292;&#20294;&#20063;&#25581;&#31034;&#20102;&#27169;&#22411;&#20869;&#37096;&#30693;&#35782;&#30340;&#23616;&#38480;&#24615;</title><link>https://arxiv.org/abs/2402.11035</link><description>&lt;p&gt;
&#23494;&#38598;&#36890;&#36947;&#26816;&#32034;&#65306;&#23494;&#38598;&#36890;&#36947;&#26816;&#32034;&#26159;&#21542;&#22312;&#26816;&#32034;&#20013;&#65311;
&lt;/p&gt;
&lt;p&gt;
Retrieval-Augmented Generation: Is Dense Passage Retrieval Retrieving?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11035
&lt;/p&gt;
&lt;p&gt;
DPR&#24494;&#35843;&#39044;&#35757;&#32451;&#32593;&#32476;&#20197;&#22686;&#24378;&#26597;&#35810;&#21644;&#30456;&#20851;&#25991;&#26412;&#25968;&#25454;&#20043;&#38388;&#30340;&#23884;&#20837;&#23545;&#40784;&#65292;&#21457;&#29616;&#35757;&#32451;&#20013;&#30693;&#35782;&#21435;&#20013;&#24515;&#21270;&#65292;&#20294;&#20063;&#25581;&#31034;&#20102;&#27169;&#22411;&#20869;&#37096;&#30693;&#35782;&#30340;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23494;&#38598;&#36890;&#36947;&#26816;&#32034;&#65288;DPR&#65289;&#26159;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24615;&#33021;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#33539;&#24335;&#20013;&#30340;&#31532;&#19968;&#27493;&#12290; DPR&#24494;&#35843;&#39044;&#35757;&#32451;&#32593;&#32476;&#65292;&#20197;&#22686;&#24378;&#26597;&#35810;&#21644;&#30456;&#20851;&#25991;&#26412;&#25968;&#25454;&#20043;&#38388;&#30340;&#23884;&#20837;&#23545;&#40784;&#12290;&#23545;DPR&#24494;&#35843;&#30340;&#28145;&#20837;&#29702;&#35299;&#23558;&#38656;&#35201;&#20174;&#26681;&#26412;&#19978;&#37322;&#25918;&#35813;&#26041;&#27861;&#30340;&#20840;&#37096;&#28508;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#25506;&#38024;&#12289;&#23618;&#28608;&#27963;&#20998;&#26512;&#21644;&#27169;&#22411;&#32534;&#36753;&#30340;&#32452;&#21512;&#65292;&#26426;&#26800;&#22320;&#25506;&#32034;&#20102;DPR&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;DPR&#35757;&#32451;&#20351;&#32593;&#32476;&#20013;&#23384;&#20648;&#30693;&#35782;&#30340;&#26041;&#24335;&#21435;&#20013;&#24515;&#21270;&#65292;&#21019;&#24314;&#20102;&#35775;&#38382;&#30456;&#21516;&#20449;&#24687;&#30340;&#22810;&#20010;&#36335;&#24452;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#36825;&#31181;&#35757;&#32451;&#39118;&#26684;&#30340;&#23616;&#38480;&#24615;&#65306;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20869;&#37096;&#30693;&#35782;&#38480;&#21046;&#20102;&#26816;&#32034;&#27169;&#22411;&#21487;&#20197;&#26816;&#32034;&#30340;&#20869;&#23481;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#23494;&#38598;&#26816;&#32034;&#25552;&#20986;&#20102;&#19968;&#20123;&#21487;&#33021;&#30340;&#26041;&#21521;&#65306;&#65288;1&#65289;&#26292;&#38706;DPR&#35757;&#32451;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11035v1 Announce Type: new  Abstract: Dense passage retrieval (DPR) is the first step in the retrieval augmented generation (RAG) paradigm for improving the performance of large language models (LLM). DPR fine-tunes pre-trained networks to enhance the alignment of the embeddings between queries and relevant textual data. A deeper understanding of DPR fine-tuning will be required to fundamentally unlock the full potential of this approach. In this work, we explore DPR-trained models mechanistically by using a combination of probing, layer activation analysis, and model editing. Our experiments show that DPR training decentralizes how knowledge is stored in the network, creating multiple access pathways to the same information. We also uncover a limitation in this training style: the internal knowledge of the pre-trained model bounds what the retrieval model can retrieve. These findings suggest a few possible directions for dense retrieval: (1) expose the DPR training process 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;MANGO&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#27010;&#24565;&#21644;&#25991;&#21270;&#20004;&#20010;&#20837;&#21475;&#28857;&#35880;&#24910;&#32780;&#36845;&#20195;&#22320;&#25552;&#31034;LLMs&#65292;&#25552;&#28860;&#39640;&#20934;&#30830;&#24230;&#12289;&#39640;&#21484;&#22238;&#29575;&#30340;&#25991;&#21270;&#30693;&#35782;&#26029;&#35328;&#65292;&#25552;&#20379;&#20102;&#22823;&#37327;&#39640;&#20934;&#30830;&#24230;&#26029;&#35328;&#65292;&#33021;&#22815;&#25913;&#21892;&#23545;&#35805;&#31995;&#32479;&#22238;&#24212;&#30340;&#36136;&#37327;&#12289;&#29305;&#24322;&#24615;&#21644;&#25991;&#21270;&#25935;&#24863;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10689</link><description>&lt;p&gt;
&#22810;&#20803;&#25991;&#21270;&#24120;&#35782;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Multi-Cultural Commonsense Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10689
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;MANGO&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#27010;&#24565;&#21644;&#25991;&#21270;&#20004;&#20010;&#20837;&#21475;&#28857;&#35880;&#24910;&#32780;&#36845;&#20195;&#22320;&#25552;&#31034;LLMs&#65292;&#25552;&#28860;&#39640;&#20934;&#30830;&#24230;&#12289;&#39640;&#21484;&#22238;&#29575;&#30340;&#25991;&#21270;&#30693;&#35782;&#26029;&#35328;&#65292;&#25552;&#20379;&#20102;&#22823;&#37327;&#39640;&#20934;&#30830;&#24230;&#26029;&#35328;&#65292;&#33021;&#22815;&#25913;&#21892;&#23545;&#35805;&#31995;&#32479;&#22238;&#24212;&#30340;&#36136;&#37327;&#12289;&#29305;&#24322;&#24615;&#21644;&#25991;&#21270;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#36817;&#21462;&#24471;&#20102;&#19968;&#23450;&#36827;&#23637;&#65292;&#20294;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20173;&#28982;&#38754;&#20020;&#30528;&#36866;&#24403;&#24212;&#23545;&#31038;&#20250;&#21644;&#25991;&#21270;&#24815;&#20363;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;MANGO&#65292;&#19968;&#31181;&#29992;&#20110;&#25552;&#28860;&#39640;&#20934;&#30830;&#24230;&#12289;&#39640;&#21484;&#22238;&#29575;&#25991;&#21270;&#30693;&#35782;&#26029;&#35328;&#30340;&#26041;&#27861;&#35770;&#12290;&#25105;&#20204;&#20174;&#27010;&#24565;&#21644;&#25991;&#21270;&#20004;&#20010;&#20837;&#21475;&#28857;&#35880;&#24910;&#32780;&#36845;&#20195;&#22320;&#25552;&#31034;LLMs&#36827;&#34892;&#36825;&#19968;&#30446;&#30340;&#12290;&#36890;&#36807;&#32858;&#31867;&#21644;&#29983;&#25104;&#25688;&#35201;&#23558;&#36755;&#20986;&#32467;&#26524;&#24041;&#22266;&#12290;&#36816;&#34892;MANGO&#26041;&#27861;&#65292;&#20197;GPT-3.5&#20316;&#20026;&#24213;&#23618;LLM&#65292;&#20026;30K&#20010;&#27010;&#24565;&#21644;11K&#20010;&#25991;&#21270;&#25552;&#20379;&#20102;167K&#20010;&#39640;&#20934;&#30830;&#24230;&#26029;&#35328;&#65292;&#22823;&#24133;&#36229;&#36807;&#20808;&#21069;&#30340;&#36164;&#28304;&#12290;&#20026;&#20102;&#22806;&#37096;&#35780;&#20272;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#23558;&#23545;&#35805;&#31995;&#32479;&#19982;&#25991;&#21270;&#30693;&#35782;&#26029;&#35328;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#28155;&#21152;&#26469;&#33258;MANGO&#30340;&#30693;&#35782;&#21487;&#20197;&#25552;&#21319;&#23545;&#35805;&#22238;&#24212;&#30340;&#25972;&#20307;&#36136;&#37327;&#12289;&#29305;&#24322;&#24615;&#21644;&#25991;&#21270;&#25935;&#24863;&#24615;&#65292;&#36825;&#26159;&#30001;&#20154;&#31867;&#26631;&#27880;&#32773;&#35780;&#21028;&#30340;&#12290;&#25968;&#25454;&#21644;&#20195;&#30721;&#21487;&#20379;&#19979;&#36733;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10689v1 Announce Type: new  Abstract: Despite recent progress, large language models (LLMs) still face the challenge of appropriately reacting to the intricacies of social and cultural conventions. This paper presents MANGO, a methodology for distilling high-accuracy, high-recall assertions of cultural knowledge. We judiciously and iteratively prompt LLMs for this purpose from two entry points, concepts and cultures. Outputs are consolidated via clustering and generative summarization. Running the MANGO method with GPT-3.5 as underlying LLM yields 167K high-accuracy assertions for 30K concepts and 11K cultures, surpassing prior resources by a large margin. For extrinsic evaluation, we explore augmenting dialogue systems with cultural knowledge assertions. We find that adding knowledge from MANGO improves the overall quality, specificity, and cultural sensitivity of dialogue responses, as judged by human annotators. Data and code are available for download.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#26469;&#30740;&#31350;LLM&#21644;&#20154;&#31867;&#35009;&#21028;&#30340;&#20559;&#35265;&#65292;&#25581;&#31034;&#20154;&#31867;&#21644;LLM&#35009;&#21028;&#22312;&#38754;&#23545;&#24178;&#25200;&#26102;&#30340;&#33030;&#24369;&#24615;&#65292;&#24378;&#35843;&#35780;&#20272;&#29616;&#26377;LLM&#24615;&#33021;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.10669</link><description>&lt;p&gt;
&#20154;&#31867;&#36824;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35009;&#21028;&#65311;&#19968;&#39033;&#20851;&#20110;&#21028;&#20915;&#20559;&#35265;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Humans or LLMs as the Judge? A Study on Judgement Biases
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10669
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#26469;&#30740;&#31350;LLM&#21644;&#20154;&#31867;&#35009;&#21028;&#30340;&#20559;&#35265;&#65292;&#25581;&#31034;&#20154;&#31867;&#21644;LLM&#35009;&#21028;&#22312;&#38754;&#23545;&#24178;&#25200;&#26102;&#30340;&#33030;&#24369;&#24615;&#65292;&#24378;&#35843;&#35780;&#20272;&#29616;&#26377;LLM&#24615;&#33021;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37319;&#29992;&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;&#35009;&#21028;&#65288;&#21363;&#20154;&#31867;&#21644;LLM&#20316;&#20026;&#35009;&#21028;&#65289;&#26469;&#35780;&#20272;&#29616;&#26377;LLM&#24615;&#33021;&#30340;&#20570;&#27861;&#36817;&#26469;&#22791;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#21516;&#26102;&#21487;&#33021;&#24341;&#20837;&#20154;&#31867;&#21644;LLM&#35009;&#21028;&#30340;&#28508;&#22312;&#20559;&#35265;&#65292;&#36136;&#30097;&#35780;&#20272;&#32467;&#26524;&#30340;&#21487;&#38752;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#30740;&#31350;LLM&#21644;&#20154;&#31867;&#35009;&#21028;&#30340;5&#31181;&#20559;&#35265;&#12290;&#25105;&#20204;&#25972;&#29702;&#20102;&#19968;&#20010;&#21253;&#21547;142&#20010;&#26679;&#26412;&#30340;&#25968;&#25454;&#38598;&#65292;&#28041;&#21450;&#20462;&#35746;&#30340;&#24067;&#21346;&#22982;&#20998;&#31867;&#27861;&#65292;&#24182;&#36827;&#34892;&#20102;&#25104;&#21315;&#19978;&#19975;&#27425;&#30340;&#20154;&#31867;&#21644;LLM&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20154;&#31867;&#21644;LLM&#35009;&#21028;&#22312;&#19981;&#21516;&#31243;&#24230;&#19978;&#37117;&#23481;&#26131;&#21463;&#21040;&#24178;&#25200;&#65292;&#21363;&#20351;&#26368;&#23574;&#31471;&#30340;&#35009;&#21028;&#20063;&#23384;&#22312;&#30456;&#24403;&#22823;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21033;&#29992;&#20182;&#20204;&#30340;&#24369;&#28857;&#23545;LLM&#35009;&#21028;&#36827;&#34892;&#25915;&#20987;&#12290;&#24076;&#26395;&#25105;&#20204;&#30340;&#24037;&#20316;&#33021;&#25552;&#37266;&#31038;&#32676;&#20851;&#20110;&#20154;&#31867;&#21644;LLM&#20316;&#20026;&#35009;&#21028;&#22312;&#38754;&#23545;&#24178;&#25200;&#26102;&#30340;&#33030;&#24369;&#24615;&#65292;&#20197;&#21450;&#21457;&#23637;&#30340;&#32039;&#36843;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10669v1 Announce Type: new  Abstract: Adopting human and large language models (LLM) as judges (\textit{a.k.a} human- and LLM-as-a-judge) for evaluating the performance of existing LLMs has recently gained attention. Nonetheless, this approach concurrently introduces potential biases from human and LLM judges, questioning the reliability of the evaluation results. In this paper, we propose a novel framework for investigating 5 types of biases for LLM and human judges. We curate a dataset with 142 samples referring to the revised Bloom's Taxonomy and conduct thousands of human and LLM evaluations. Results show that human and LLM judges are vulnerable to perturbations to various degrees, and that even the most cutting-edge judges possess considerable biases. We further exploit their weakness and conduct attacks on LLM judges. We hope that our work can notify the community of the vulnerability of human- and LLM-as-a-judge against perturbations, as well as the urgency of develop
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#29983;&#25104;&#34920;&#31034;&#25351;&#20196;&#35843;&#25972;&#65288;GRIT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#20196;&#21306;&#20998;&#29983;&#25104;&#21644;&#23884;&#20837;&#20219;&#21153;&#65292;&#35757;&#32451;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21516;&#26102;&#22788;&#29702;&#36825;&#20004;&#31181;&#20219;&#21153;&#12290;&#19982;&#20854;&#20182;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;GritLM 7B&#22312;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#27979;&#35797;&#19978;&#36798;&#21040;&#26368;&#26032;&#30340;&#25216;&#26415;&#27700;&#24179;&#65292;&#24182;&#22312;&#22810;&#31181;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#25193;&#22823;&#35268;&#27169;&#65292;&#25105;&#20204;&#30340;GritLM 8x7B&#25104;&#20026;&#26368;&#20339;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20043;&#19968;&#65292;&#21516;&#26102;&#20173;&#28982;&#26159;&#26368;&#22909;&#30340;&#23884;&#20837;&#27169;&#22411;&#20043;&#19968;&#12290;GRIT&#30340;&#32479;&#19968;&#20063;&#22823;&#22823;&#25552;&#39640;&#20102;RAG&#22312;&#38271;&#25991;&#26723;&#19978;&#30340;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.09906</link><description>&lt;p&gt;
&#29983;&#25104;&#34920;&#31034;&#25351;&#20196;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Generative Representational Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09906
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#29983;&#25104;&#34920;&#31034;&#25351;&#20196;&#35843;&#25972;&#65288;GRIT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#20196;&#21306;&#20998;&#29983;&#25104;&#21644;&#23884;&#20837;&#20219;&#21153;&#65292;&#35757;&#32451;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21516;&#26102;&#22788;&#29702;&#36825;&#20004;&#31181;&#20219;&#21153;&#12290;&#19982;&#20854;&#20182;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;GritLM 7B&#22312;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#27979;&#35797;&#19978;&#36798;&#21040;&#26368;&#26032;&#30340;&#25216;&#26415;&#27700;&#24179;&#65292;&#24182;&#22312;&#22810;&#31181;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#25193;&#22823;&#35268;&#27169;&#65292;&#25105;&#20204;&#30340;GritLM 8x7B&#25104;&#20026;&#26368;&#20339;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20043;&#19968;&#65292;&#21516;&#26102;&#20173;&#28982;&#26159;&#26368;&#22909;&#30340;&#23884;&#20837;&#27169;&#22411;&#20043;&#19968;&#12290;GRIT&#30340;&#32479;&#19968;&#20063;&#22823;&#22823;&#25552;&#39640;&#20102;RAG&#22312;&#38271;&#25991;&#26723;&#19978;&#30340;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25152;&#26377;&#22522;&#20110;&#25991;&#26412;&#30340;&#35821;&#35328;&#38382;&#39064;&#37117;&#21487;&#20197;&#24402;&#32467;&#20026;&#29983;&#25104;&#25110;&#23884;&#20837;&#12290;&#30446;&#21069;&#30340;&#27169;&#22411;&#21482;&#33021;&#22312;&#20854;&#20013;&#19968;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#29983;&#25104;&#34920;&#31034;&#25351;&#20196;&#35843;&#25972;&#65288;GRIT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#20196;&#26469;&#21306;&#20998;&#29983;&#25104;&#21644;&#23884;&#20837;&#20219;&#21153;&#65292;&#20174;&#32780;&#35757;&#32451;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21516;&#26102;&#22788;&#29702;&#36825;&#20004;&#31181;&#20219;&#21153;&#12290;&#19982;&#20854;&#20182;&#24320;&#25918;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;GritLM 7B&#22312;&#22823;&#35268;&#27169;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#27979;&#35797;&#65288;MTEB&#65289;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#25216;&#26415;&#27700;&#24179;&#65292;&#24182;&#22312;&#22810;&#31181;&#29983;&#25104;&#20219;&#21153;&#20013;&#36229;&#36807;&#20102;&#21516;&#31561;&#35268;&#27169;&#30340;&#25152;&#26377;&#27169;&#22411;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#25193;&#22823;&#35268;&#27169;&#65292;GritLM 8x7B&#22312;&#23581;&#35797;&#30340;&#25152;&#26377;&#24320;&#25918;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20013;&#34920;&#29616;&#26368;&#20339;&#65292;&#21516;&#26102;&#20173;&#28982;&#26159;&#26368;&#22909;&#30340;&#23884;&#20837;&#27169;&#22411;&#20043;&#19968;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;GRIT&#21487;&#20197;&#19982;&#20165;&#22312;&#29983;&#25104;&#25110;&#23884;&#20837;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#23218;&#32654;&#65292;&#22240;&#27492;&#25105;&#20204;&#21487;&#20197;&#22312;&#19981;&#25439;&#22833;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#32479;&#19968;&#20004;&#32773;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;&#36890;&#36807;GRIT&#30340;&#32479;&#19968;&#21487;&#20197;&#23558;RAG&#65288;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65289;&#22312;&#38271;&#25991;&#26723;&#19978;&#30340;&#36895;&#24230;&#25552;&#39640;60%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09906v1 Announce Type: cross  Abstract: All text-based language problems can be reduced to either generation or embedding. Current models only perform well at one or the other. We introduce generative representational instruction tuning (GRIT) whereby a large language model is trained to handle both generative and embedding tasks by distinguishing between them through instructions. Compared to other open models, our resulting GritLM 7B sets a new state of the art on the Massive Text Embedding Benchmark (MTEB) and outperforms all models up to its size on a range of generative tasks. By scaling up further, GritLM 8x7B outperforms all open generative language models that we tried while still being among the best embedding models. Notably, we find that GRIT matches training on only generative or embedding data, thus we can unify both at no performance loss. Among other benefits, the unification via GRIT speeds up Retrieval-Augmented Generation (RAG) by &gt; 60% for long documents, 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#22810;&#27493;&#20219;&#21153;&#20013;&#38598;&#25104;&#20154;&#31867;&#21453;&#39304;&#21644;&#20559;&#22909;&#23545;&#40784;&#30340;PRompt&#20248;&#21270;&#26041;&#27861;&#12290;&#23427;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#26694;&#26550;&#65292;&#32467;&#21512;&#20154;&#31867;&#21453;&#39304;&#33258;&#21160;&#25552;&#20986;&#20248;&#21270;&#24314;&#35758;&#24182;&#35299;&#20915;&#20102;&#22797;&#26434;&#30340;&#25552;&#31034;&#20869;&#23481;&#20998;&#26512;&#12289;&#21333;&#27493;&#35780;&#20272;&#21644;&#20219;&#21153;&#25191;&#34892;&#20559;&#22909;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.08702</link><description>&lt;p&gt;
PRompt Optimization in Multi-Step Tasks (PROMST): Integrating Human Feedback and Preference Alignment
&lt;/p&gt;
&lt;p&gt;
PRompt Optimization in Multi-Step Tasks (PROMST): Integrating Human Feedback and Preference Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08702
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#22810;&#27493;&#20219;&#21153;&#20013;&#38598;&#25104;&#20154;&#31867;&#21453;&#39304;&#21644;&#20559;&#22909;&#23545;&#40784;&#30340;PRompt&#20248;&#21270;&#26041;&#27861;&#12290;&#23427;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#26694;&#26550;&#65292;&#32467;&#21512;&#20154;&#31867;&#21453;&#39304;&#33258;&#21160;&#25552;&#20986;&#20248;&#21270;&#24314;&#35758;&#24182;&#35299;&#20915;&#20102;&#22797;&#26434;&#30340;&#25552;&#31034;&#20869;&#23481;&#20998;&#26512;&#12289;&#21333;&#27493;&#35780;&#20272;&#21644;&#20219;&#21153;&#25191;&#34892;&#20559;&#22909;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Prompt optimization aims to find the best prompt for a language model (LLM) in multi-step tasks. This paper introduces a genetic algorithm framework that incorporates human feedback to automatically suggest prompt improvements. It addresses challenges such as complex prompt content analysis, evaluation of individual steps, and variations in task execution preferences.
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08702v1 Announce Type: cross Abstract: Prompt optimization aims to find the best prompt to a large language model (LLM) for a given task. LLMs have been successfully used to help find and improve prompt candidates for single-step tasks. However, realistic tasks for agents are multi-step and introduce new challenges: (1) Prompt content is likely to be more extensive and complex, making it more difficult for LLMs to analyze errors, (2) the impact of an individual step is difficult to evaluate, and (3) different people may have varied preferences about task execution. While humans struggle to optimize prompts, they are good at providing feedback about LLM outputs; we therefore introduce a new LLM-driven discrete prompt optimization framework that incorporates human-designed feedback rules about potential errors to automatically offer direct suggestions for improvement. Our framework is stylized as a genetic algorithm in which an LLM generates new candidate prompts from a parent
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;&#26041;&#27861;&#65292;&#22312;&#27492;&#20219;&#21153;&#19978;&#36890;&#36807;&#24341;&#20837;AESOP&#24230;&#37327;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#23558;&#25972;&#20010;&#25552;&#21462;&#20219;&#21153;&#20998;&#35299;&#20026;&#22810;&#20010;&#38454;&#27573;&#65292;&#30456;&#36739;&#20110;&#22522;&#20934;&#27169;&#22411;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#20026;&#26410;&#26469;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.04437</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Structured Entity Extraction Using Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;&#26041;&#27861;&#65292;&#22312;&#27492;&#20219;&#21153;&#19978;&#36890;&#36807;&#24341;&#20837;AESOP&#24230;&#37327;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#23558;&#25972;&#20010;&#25552;&#21462;&#20219;&#21153;&#20998;&#35299;&#20026;&#22810;&#20010;&#38454;&#27573;&#65292;&#30456;&#36739;&#20110;&#22522;&#20934;&#27169;&#22411;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#20026;&#26410;&#26469;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#26174;&#33879;&#24433;&#21709;&#20102;&#20449;&#24687;&#25552;&#21462;&#39046;&#22495;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#20449;&#24687;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#24403;&#21069;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;&#26041;&#27861;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#21644;&#35268;&#33539;&#21270;&#20102;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;&#65288;SEE&#65289;&#20219;&#21153;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#35813;&#20219;&#21153;&#30340;&#36817;&#20284;&#23454;&#20307;&#38598;&#37325;&#21472;&#65288;AESOP&#65289;&#24230;&#37327;&#65292;&#20197;&#36866;&#24403;&#35780;&#20272;&#27169;&#22411;&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#25972;&#20010;&#25552;&#21462;&#20219;&#21153;&#20998;&#35299;&#20026;&#22810;&#20010;&#38454;&#27573;&#65292;&#21033;&#29992;LLMs&#30340;&#24378;&#22823;&#21151;&#33021;&#26469;&#25552;&#39640;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;&#23450;&#37327;&#35780;&#20272;&#21644;&#20154;&#24037;&#24182;&#34892;&#35780;&#20272;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#65292;&#20026;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;&#39046;&#22495;&#30340;&#26410;&#26469;&#36827;&#23637;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in machine learning have significantly impacted the field of information extraction, with Large Language Models (LLMs) playing a pivotal role in extracting structured information from unstructured text. This paper explores the challenges and limitations of current methodologies in structured entity extraction and introduces a novel approach to address these issues. We contribute to the field by first introducing and formalizing the task of Structured Entity Extraction (SEE), followed by proposing Approximate Entity Set OverlaP (AESOP) Metric designed to appropriately assess model performance on this task. Later, we propose a new model that harnesses the power of LLMs for enhanced effectiveness and efficiency through decomposing the entire extraction task into multiple stages. Quantitative evaluation and human side-by-side evaluation confirm that our model outperforms baselines, offering promising directions for future advancements in structured entity extraction.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27979;&#35797;&#20102;5&#31181;&#19981;&#21516;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#25512;&#29702;&#38382;&#39064;&#19978;&#30340;&#25512;&#29702;&#28145;&#24230;&#65292;&#24182;&#21457;&#29616;&#20102;LLMs&#30340;&#23616;&#38480;&#24615;&#12289;&#20559;&#35265;&#21644;&#23646;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;LLMs&#23545;&#20110;&#33410;&#28857;&#36941;&#21382;&#33258;&#30001;&#24230;&#30340;&#24179;&#22343;&#24230;&#25968;&#21576;&#21453;&#21521;&#20851;&#31995;&#65292;k-shot&#25552;&#31034;&#23545;&#22270;&#25512;&#29702;&#20219;&#21153;&#26377;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#19988;LLMs&#23384;&#22312;&#31215;&#26497;&#30340;&#22238;&#24212;&#20559;&#24046;&#65292;&#26080;&#27861;&#35782;&#21035;&#26377;&#25928;&#35299;&#30340;&#32570;&#22833;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#25512;&#29702;&#25552;&#31034;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2402.01805</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#22270;&#25512;&#29702;&#30340;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploring the Limitations of Graph Reasoning in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01805
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27979;&#35797;&#20102;5&#31181;&#19981;&#21516;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#25512;&#29702;&#38382;&#39064;&#19978;&#30340;&#25512;&#29702;&#28145;&#24230;&#65292;&#24182;&#21457;&#29616;&#20102;LLMs&#30340;&#23616;&#38480;&#24615;&#12289;&#20559;&#35265;&#21644;&#23646;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;LLMs&#23545;&#20110;&#33410;&#28857;&#36941;&#21382;&#33258;&#30001;&#24230;&#30340;&#24179;&#22343;&#24230;&#25968;&#21576;&#21453;&#21521;&#20851;&#31995;&#65292;k-shot&#25552;&#31034;&#23545;&#22270;&#25512;&#29702;&#20219;&#21153;&#26377;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#19988;LLMs&#23384;&#22312;&#31215;&#26497;&#30340;&#22238;&#24212;&#20559;&#24046;&#65292;&#26080;&#27861;&#35782;&#21035;&#26377;&#25928;&#35299;&#30340;&#32570;&#22833;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#25512;&#29702;&#25552;&#31034;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20165;&#36890;&#36807;&#22522;&#20110;&#35821;&#35328;&#30340;&#25552;&#31034;&#23601;&#23637;&#31034;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22270;&#25512;&#29702;&#38382;&#39064;&#27979;&#35797;&#20102;5&#31181;&#19981;&#21516;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;GPT-4&#65292;GPT-3.5&#65292;Claude-2&#65292;Llama-2&#21644;Palm-2&#65289;&#30340;&#25512;&#29702;&#28145;&#24230;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;10&#20010;&#19981;&#21516;&#30340;&#22270;&#36941;&#21382;&#38382;&#39064;&#65292;&#27599;&#20010;&#38382;&#39064;&#20195;&#34920;&#30528;&#36880;&#27493;&#22686;&#21152;&#30340;&#22797;&#26434;&#24615;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#19981;&#21516;&#22270;&#22823;&#23567;&#20197;&#21450;&#19981;&#21516;&#24418;&#24335;&#30340;k-shot&#25552;&#31034;&#30340;&#35774;&#32622;&#20998;&#26512;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#36825;&#20010;&#22522;&#20934;&#27979;&#35797;&#36807;&#31243;&#65292;&#25105;&#20204;&#20984;&#26174;&#20102;LLMs&#30340;&#21508;&#31181;&#23616;&#38480;&#24615;&#12289;&#20559;&#35265;&#21644;&#23646;&#24615;&#65292;&#27604;&#22914;&#19982;&#27599;&#20010;&#33410;&#28857;&#30340;&#36941;&#21382;&#33258;&#30001;&#24230;&#30340;&#24179;&#22343;&#24230;&#25968;&#21576;&#21453;&#21521;&#20851;&#31995;&#65292;k-shot&#25552;&#31034;&#23545;&#22270;&#25512;&#29702;&#20219;&#21153;&#30340;&#25972;&#20307;&#36127;&#38754;&#24433;&#21709;&#65292;&#20197;&#21450;&#31215;&#26497;&#30340;&#22238;&#24212;&#20559;&#24046;&#23548;&#33268;LLMs&#26080;&#27861;&#35782;&#21035;&#26377;&#25928;&#35299;&#30340;&#32570;&#22833;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#65292;&#19987;&#38376;&#29992;&#20110;&#22270;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained Large Language Models have demonstrated various types of reasoning capabilities through language-based prompts alone. However, in this paper, we test the depth of graph reasoning for 5 different LLMs (GPT-4, GPT-3.5, Claude-2, Llama-2 and Palm-2) through the problems of graph reasoning. In particular, we design 10 distinct problems of graph traversal, each representing increasing levels of complexity. Further, we analyze the performance of models across various settings such as varying sizes of graphs as well as different forms of k-shot prompting. We highlight various limitations, biases, and properties of LLMs through this benchmarking process, such as an inverse relation to the average degrees of freedom of traversal per node in graphs, the overall negative impact of k-shot prompting on graph reasoning tasks, and a positive response bias which prevents LLMs from identifying the absence of a valid solution. Finally, we propose a new prompting technique specially designed f
&lt;/p&gt;</description></item><item><title>LLMs&#36890;&#36807;&#21019;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#23454;&#29616;&#20102;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;&#33539;&#24335;&#30340;&#36716;&#21464;&#65292;&#20197;&#22823;&#22823;&#25552;&#39640;&#19979;&#28216;NLP&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2312.03740</link><description>&lt;p&gt;
&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Prompting in Autoregressive Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03740
&lt;/p&gt;
&lt;p&gt;
LLMs&#36890;&#36807;&#21019;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#23454;&#29616;&#20102;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;&#33539;&#24335;&#30340;&#36716;&#21464;&#65292;&#20197;&#22823;&#22823;&#25552;&#39640;&#19979;&#28216;NLP&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26684;&#23616;&#12290;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;&#33539;&#24335;&#24050;&#32463;&#21462;&#20195;&#20102;&#35768;&#22810;&#19979;&#28216;NLP&#20219;&#21153;&#24120;&#35268;&#30340;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#26041;&#27861;&#12290;&#36825;&#31181;&#36716;&#21464;&#20027;&#35201;&#24471;&#30410;&#20110;LLMs&#21644;&#21019;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#12290;LLMs&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#65292;&#36825;&#24402;&#21151;&#20110;&#23427;&#20204;&#22312;&#39044;&#35757;&#32451;&#20013;&#20351;&#29992;&#30340;&#22823;&#37327;&#21442;&#25968;&#21644;&#24222;&#22823;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#20805;&#20998;&#21457;&#25381;&#23427;&#20204;&#30340;&#28508;&#21147;&#65292;&#24517;&#39035;&#24341;&#23548;&#23427;&#20204;&#30340;&#36755;&#20986;&#26397;&#30528;&#26399;&#26395;&#30340;&#32467;&#26524;&#12290;&#25552;&#31034;&#65292;&#21363;&#25552;&#20379;&#29305;&#23450;&#30340;&#36755;&#20837;&#25110;&#25351;&#20196;&#26469;&#24341;&#23548;LLMs&#26397;&#30528;&#39044;&#26399;&#36755;&#20986;&#30340;&#26041;&#21521;&#21457;&#23637;&#65292;&#24050;&#25104;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#24037;&#20855;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#24050;&#34987;&#24212;&#29992;&#26469;&#20805;&#20998;&#21033;&#29992;LLMs&#28508;&#21147;&#30340;&#21508;&#31181;&#25552;&#31034;&#25216;&#26415;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#29616;&#26377;&#25991;&#29486;&#20013;&#20851;&#20110;&#25552;&#31034;&#25216;&#26415;&#30340;&#20998;&#31867;&#65292;&#24182;&#26681;&#25454;&#20854;&#36827;&#34892;&#20102;&#31616;&#26126;&#35843;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03740v1 Announce Type: cross  Abstract: Autoregressive Large Language Models have transformed the landscape of Natural Language Processing. Pre-train and prompt paradigm has replaced the conventional approach of pre-training and fine-tuning for many downstream NLP tasks. This shift has been possible largely due to LLMs and innovative prompting techniques. LLMs have shown great promise for a variety of downstream tasks owing to their vast parameters and huge datasets that they are pre-trained on. However, in order to fully realize their potential, their outputs must be guided towards the desired outcomes. Prompting, in which a specific input or instruction is provided to guide the LLMs toward the intended output, has become a tool for achieving this goal. In this paper, we discuss the various prompting techniques that have been applied to fully harness the power of LLMs. We present a taxonomy of existing literature on prompting techniques and provide a concise survey based on
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;PeTailor&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#26816;&#32034;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#23450;&#21046;&#30340;&#20998;&#22359;&#35780;&#20998;&#22120;&#20174;&#39044;&#20808;&#26500;&#24314;&#30340;&#20998;&#22359;&#25968;&#25454;&#24211;&#20013;&#26816;&#32034;&#30456;&#20851;&#25991;&#26723;&#65292;&#24182;&#23558;&#26816;&#32034;&#21040;&#30340;&#20449;&#24687;&#38598;&#25104;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36755;&#20837;&#20013;&#65292;&#20197;&#25913;&#36827;&#29983;&#29289;&#21307;&#23398;&#19977;&#20803;&#32452;&#25552;&#21462;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2310.18463</link><description>&lt;p&gt;
PeTailor&#65306;&#36890;&#36807;&#23450;&#21046;&#30340;&#20998;&#22359;&#35780;&#20998;&#22120;&#25913;&#36827;&#29983;&#29289;&#21307;&#23398;&#19977;&#20803;&#32452;&#25552;&#21462;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PeTailor: Improving Large Language Model by Tailored Chunk Scorer in Biomedical Triple Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.18463
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;PeTailor&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#26816;&#32034;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#23450;&#21046;&#30340;&#20998;&#22359;&#35780;&#20998;&#22120;&#20174;&#39044;&#20808;&#26500;&#24314;&#30340;&#20998;&#22359;&#25968;&#25454;&#24211;&#20013;&#26816;&#32034;&#30456;&#20851;&#25991;&#26723;&#65292;&#24182;&#23558;&#26816;&#32034;&#21040;&#30340;&#20449;&#24687;&#38598;&#25104;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36755;&#20837;&#20013;&#65292;&#20197;&#25913;&#36827;&#29983;&#29289;&#21307;&#23398;&#19977;&#20803;&#32452;&#25552;&#21462;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#19977;&#20803;&#32452;&#25552;&#21462;&#31995;&#32479;&#26088;&#22312;&#33258;&#21160;&#25552;&#21462;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#21644;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#34429;&#28982;&#24403;&#21069;&#30340;&#32479;&#19968;&#20449;&#24687;&#25552;&#21462;&#27169;&#22411;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#29702;&#35299;&#22797;&#26434;&#29983;&#29289;&#21307;&#23398;&#21477;&#23376;&#20013;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#32570;&#20047;&#39640;&#36136;&#37327;&#30340;&#29983;&#29289;&#21307;&#23398;&#19977;&#20803;&#32452;&#25552;&#21462;&#25968;&#25454;&#38598;&#38459;&#30861;&#20102;&#31283;&#20581;&#30340;&#19977;&#20803;&#32452;&#25552;&#21462;&#31995;&#32479;&#30340;&#24320;&#21457;&#36827;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36866;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#19977;&#20803;&#32452;&#25552;&#21462;&#30340;&#22522;&#20110;&#26816;&#32034;&#30340;&#26694;&#26550;&#65292;&#21517;&#20026;PeTailor&#65292;&#23427;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#23450;&#21046;&#20998;&#22359;&#35780;&#20998;&#22120;&#20174;&#25105;&#20204;&#39044;&#20808;&#26500;&#24314;&#30340;&#22810;&#26679;&#20998;&#22359;&#25968;&#25454;&#24211;&#20013;&#26174;&#24335;&#22320;&#26816;&#32034;&#30456;&#20851;&#25991;&#26723;&#65292;&#24182;&#23558;&#26816;&#32034;&#21040;&#30340;&#20449;&#24687;&#38598;&#25104;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36755;&#20837;&#20013;&#65292;&#20026;&#36755;&#20837;&#30340;&#21477;&#23376;&#29983;&#25104;&#30456;&#24212;&#30340;&#19977;&#20803;&#32452;&#65288;&#22836;&#23454;&#20307;&#65292;&#20851;&#31995;&#65292;&#23614;&#23454;&#20307;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;GM-CIHT&#65292;&#19968;&#31181;&#19987;&#23478;&#26631;&#27880;&#30340;&#29983;&#29289;&#21307;&#23398;&#19977;&#20803;&#32452;&#25552;&#21462;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#25903;&#25345;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#23454;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biomedical triple extraction systems aim to automatically extract biomedical entities and relations between entities. While current unified information extraction models showcase state-of-the-art performance, they face challenges in understanding relationships between entities within intricate biomedical sentences. Furthermore, the absence of a high-quality biomedical triple extraction dataset impedes the progress in developing robust triple extraction systems. To tackle these challenges, we propose a novel retrieval-based framework for biomedical triple extraction, namely PeTailor, which explicitly retrieves the relevant document from our pre-built diverse chunk database using a novel tailored chunk scorer and integrates the retrieved information into the input of a Large Language Model (LLM) to generate the corresponding triple (head entity, relation, tail entity) for the input sentence. Additionally, we present GM-CIHT, an expert-annotated biomedical triple extraction dataset that c
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33021;&#22815;&#35299;&#20915;&#23454;&#20307;&#23545;&#40784;&#35299;&#30721;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#33021;&#37327;&#26469;&#20248;&#21270;&#35299;&#30721;&#36807;&#31243;&#65292;&#20197;&#23454;&#29616;&#22270;&#21516;&#36136;&#24615;&#65292;&#24182;&#19988;&#20165;&#20381;&#36182;&#20110;&#23454;&#20307;&#23884;&#20837;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#36890;&#29992;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.12798</link><description>&lt;p&gt;
&#33021;&#37327;&#30340;&#26799;&#24230;&#27969;&#65306;&#23454;&#20307;&#23545;&#40784;&#35299;&#30721;&#30340;&#36890;&#29992;&#39640;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Gradient Flow of Energy: A General and Efficient Approach for Entity Alignment Decoding. (arXiv:2401.12798v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12798
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33021;&#22815;&#35299;&#20915;&#23454;&#20307;&#23545;&#40784;&#35299;&#30721;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#33021;&#37327;&#26469;&#20248;&#21270;&#35299;&#30721;&#36807;&#31243;&#65292;&#20197;&#23454;&#29616;&#22270;&#21516;&#36136;&#24615;&#65292;&#24182;&#19988;&#20165;&#20381;&#36182;&#20110;&#23454;&#20307;&#23884;&#20837;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#36890;&#29992;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#23545;&#40784;&#65288;EA&#65289;&#26159;&#22312;&#38598;&#25104;&#22810;&#28304;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#20013;&#30340;&#20851;&#38190;&#36807;&#31243;&#65292;&#26088;&#22312;&#35782;&#21035;&#36825;&#20123;&#22270;&#35889;&#20013;&#30340;&#31561;&#20215;&#23454;&#20307;&#23545;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#23558;EA&#35270;&#20026;&#22270;&#34920;&#31034;&#23398;&#20064;&#20219;&#21153;&#65292;&#19987;&#27880;&#20110;&#22686;&#24378;&#22270;&#32534;&#30721;&#22120;&#12290;&#28982;&#32780;&#65292;&#22312;EA&#20013;&#35299;&#30721;&#36807;&#31243;-&#23545;&#20110;&#26377;&#25928;&#30340;&#25805;&#20316;&#21644;&#23545;&#40784;&#20934;&#30830;&#24615;&#33267;&#20851;&#37325;&#35201;-&#24471;&#21040;&#20102;&#26377;&#38480;&#30340;&#20851;&#27880;&#65292;&#24182;&#19988;&#20173;&#28982;&#38024;&#23545;&#29305;&#23450;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#26550;&#26500;&#36827;&#34892;&#23450;&#21046;&#65292;&#38656;&#35201;&#23454;&#20307;&#21644;&#39069;&#22806;&#30340;&#26174;&#24335;&#20851;&#31995;&#23884;&#20837;&#12290;&#36825;&#31181;&#29305;&#27530;&#24615;&#38480;&#21046;&#20102;&#23427;&#30340;&#36866;&#29992;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#22522;&#20110;GNN&#30340;&#27169;&#22411;&#20013;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#12289;&#36890;&#29992;&#21644;&#39640;&#25928;&#30340;EA&#35299;&#30721;&#26041;&#27861;&#65292;&#20165;&#20381;&#36182;&#20110;&#23454;&#20307;&#23884;&#20837;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#29380;&#21033;&#20811;&#38647;&#33021;&#37327;&#26469;&#20248;&#21270;&#35299;&#30721;&#36807;&#31243;&#65292;&#22312;&#22270;&#20869;&#24341;&#23548;&#26799;&#24230;&#27969;&#65292;&#20197;&#20419;&#36827;&#22270;&#21516;&#36136;&#24615;&#12290;&#26799;&#24230;&#27969;&#30340;&#31163;&#25955;&#21270;&#20135;&#29983;&#20102;&#19968;&#31181;&#24555;&#36895;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#19977;&#20803;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity alignment (EA), a pivotal process in integrating multi-source Knowledge Graphs (KGs), seeks to identify equivalent entity pairs across these graphs. Most existing approaches regard EA as a graph representation learning task, concentrating on enhancing graph encoders. However, the decoding process in EA - essential for effective operation and alignment accuracy - has received limited attention and remains tailored to specific datasets and model architectures, necessitating both entity and additional explicit relation embeddings. This specificity limits its applicability, particularly in GNN-based models. To address this gap, we introduce a novel, generalized, and efficient decoding approach for EA, relying solely on entity embeddings. Our method optimizes the decoding process by minimizing Dirichlet energy, leading to the gradient flow within the graph, to promote graph homophily. The discretization of the gradient flow produces a fast and scalable approach, termed Triple Feature
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#35774;&#22791;&#19978;&#23454;&#29616;&#33258;&#25105;&#30417;&#30563;&#25968;&#25454;&#36873;&#25321;&#21644;&#21512;&#25104;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20010;&#24615;&#21270;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#36873;&#25321;&#21644;&#23384;&#20648;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;&#25968;&#25454;&#26469;&#35299;&#20915;&#31232;&#30095;&#27880;&#37322;&#21644;&#26377;&#38480;&#30340;&#35774;&#22791;&#23384;&#20648;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2311.12275</link><description>&lt;p&gt;
&#22312;&#35774;&#22791;&#19978;&#23454;&#29616;&#33258;&#25105;&#30417;&#30563;&#25968;&#25454;&#36873;&#25321;&#21644;&#21512;&#25104;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20010;&#24615;&#21270;
&lt;/p&gt;
&lt;p&gt;
Enabling On-Device Large Language Model Personalization with Self-Supervised Data Selection and Synthesis. (arXiv:2311.12275v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.12275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#35774;&#22791;&#19978;&#23454;&#29616;&#33258;&#25105;&#30417;&#30563;&#25968;&#25454;&#36873;&#25321;&#21644;&#21512;&#25104;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20010;&#24615;&#21270;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#36873;&#25321;&#21644;&#23384;&#20648;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;&#25968;&#25454;&#26469;&#35299;&#20915;&#31232;&#30095;&#27880;&#37322;&#21644;&#26377;&#38480;&#30340;&#35774;&#22791;&#23384;&#20648;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23558;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#37096;&#32626;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#21518;&#65292;&#24076;&#26395;&#36825;&#20123;&#35774;&#22791;&#33021;&#20174;&#29992;&#25143;&#29983;&#25104;&#30340;&#23545;&#35805;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#20197;&#23454;&#26102;&#29983;&#25104;&#38024;&#23545;&#29992;&#25143;&#30340;&#20010;&#24615;&#21270;&#22238;&#24212;&#12290;&#28982;&#32780;&#65292;&#29992;&#25143;&#29983;&#25104;&#30340;&#25968;&#25454;&#36890;&#24120;&#21253;&#21547;&#25935;&#24863;&#21644;&#31169;&#23494;&#20449;&#24687;&#65292;&#32780;&#23558;&#27492;&#31867;&#25968;&#25454;&#19978;&#20256;&#21040;&#20113;&#31471;&#36827;&#34892;&#27880;&#37322;&#24182;&#19981;&#34987;&#25512;&#33616;&#65292;&#29978;&#33267;&#26159;&#31105;&#27490;&#30340;&#12290;&#34429;&#28982;&#21487;&#20197;&#36890;&#36807;&#30452;&#25509;&#35810;&#38382;&#29992;&#25143;&#25552;&#20379;&#39318;&#36873;&#22238;&#24212;&#26469;&#22312;&#26412;&#22320;&#33719;&#21462;&#27880;&#37322;&#65292;&#20294;&#36825;&#31181;&#27880;&#37322;&#24517;&#39035;&#31232;&#30095;&#20197;&#19981;&#24433;&#21709;&#29992;&#25143;&#20307;&#39564;&#12290;&#27492;&#22806;&#65292;&#36793;&#32536;&#35774;&#22791;&#30340;&#23384;&#20648;&#36890;&#24120;&#22826;&#26377;&#38480;&#65292;&#26080;&#27861;&#36827;&#34892;&#20840;&#38754;&#30340;&#22823;&#35268;&#27169;&#24494;&#35843;&#12290;&#22914;&#20309;&#22312;&#32771;&#34385;&#31232;&#30095;&#27880;&#37322;&#21644;&#21463;&#38480;&#30340;&#35774;&#22791;&#23384;&#20648;&#26465;&#20214;&#19979;&#23454;&#29616;&#22312;&#35774;&#22791;&#19978;&#30340;LLM&#20010;&#24615;&#21270;&#20173;&#28982;&#26159;&#19968;&#20010;&#24453;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#20197;&#33258;&#25105;&#30417;&#30563;&#30340;&#26041;&#24335;&#22312;&#32447;&#36873;&#25321;&#21644;&#23384;&#20648;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;&#25968;&#25454;&#12290;&#36825;&#31181;&#25968;&#25454;&#20855;&#26377;&#36739;&#23567;&#30340;&#20869;&#23384;&#21344;&#29992;&#65292;&#24182;&#20801;&#35768;&#24456;&#23569;&#30340;&#23384;&#20648;&#21344;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
After a large language model (LLM) is deployed on edge devices, it is desirable for these devices to learn from user-generated conversation data to generate user-specific and personalized responses in real-time. However, user-generated data usually contains sensitive and private information, and uploading such data to the cloud for annotation is not preferred if not prohibited. While it is possible to obtain annotation locally by directly asking users to provide preferred responses, such annotations have to be sparse to not affect user experience. In addition, the storage of edge devices is usually too limited to enable large-scale fine-tuning with full user-generated data. It remains an open question how to enable on-device LLM personalization, considering sparse annotation and limited on-device storage. In this paper, we propose a novel framework to select and store the most representative data online in a self-supervised way. Such data has a small memory footprint and allows infrequ
&lt;/p&gt;</description></item><item><title>Qilin-Med&#26159;&#19968;&#20010;&#22810;&#38454;&#27573;&#35757;&#32451;&#30340;&#21307;&#30103;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#39046;&#22495;&#29305;&#23450;&#32487;&#32493;&#39044;&#35757;&#32451;&#12289;&#30417;&#30563;&#24494;&#35843;&#21644;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#21253;&#21547;&#21307;&#23398;&#38382;&#31572;&#12289;&#32431;&#25991;&#26412;&#12289;&#30693;&#35782;&#22270;&#35889;&#21644;&#23545;&#35805;&#30340;3Gb&#20013;&#21307;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2310.09089</link><description>&lt;p&gt;
Qilin-Med: &#22810;&#38454;&#27573;&#30693;&#35782;&#27880;&#20837;&#20808;&#36827;&#30340;&#21307;&#30103;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Qilin-Med: Multi-stage Knowledge Injection Advanced Medical Large Language Model. (arXiv:2310.09089v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09089
&lt;/p&gt;
&lt;p&gt;
Qilin-Med&#26159;&#19968;&#20010;&#22810;&#38454;&#27573;&#35757;&#32451;&#30340;&#21307;&#30103;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#39046;&#22495;&#29305;&#23450;&#32487;&#32493;&#39044;&#35757;&#32451;&#12289;&#30417;&#30563;&#24494;&#35843;&#21644;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#21253;&#21547;&#21307;&#23398;&#38382;&#31572;&#12289;&#32431;&#25991;&#26412;&#12289;&#30693;&#35782;&#22270;&#35889;&#21644;&#23545;&#35805;&#30340;3Gb&#20013;&#21307;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#24212;&#29992;&#20110;&#21307;&#30103;&#39046;&#22495;&#26377;&#30528;&#28508;&#21147;&#20294;&#20063;&#38754;&#20020;&#25361;&#25112;&#12290;&#30452;&#25509;&#20026;&#20687;&#21307;&#23398;&#36825;&#26679;&#30340;&#39046;&#22495;&#36827;&#34892;&#39044;&#35757;&#32451;&#38656;&#35201;&#22823;&#37327;&#36164;&#28304;&#65292;&#26377;&#26102;&#19981;&#21487;&#34892;&#12290;&#20165;&#20381;&#36182;&#20110;&#30417;&#30563;&#24494;&#35843; (SFT) &#21487;&#33021;&#23548;&#33268;&#36807;&#20110;&#33258;&#20449;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#26080;&#27861;&#21033;&#29992;&#39046;&#22495;&#29305;&#23450;&#30340;&#35265;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#38454;&#27573;&#35757;&#32451;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#39046;&#22495;&#29305;&#23450;&#32487;&#32493;&#39044;&#35757;&#32451; (DCPT)&#12289;SFT &#21644;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270; (DPO)&#12290;&#25105;&#20204;&#30740;&#31350;&#30340;&#19968;&#20010;&#26174;&#33879;&#36129;&#29486;&#26159;&#24341;&#20837;&#20102;&#19968;&#20010; 3Gb &#30340;&#20013;&#21307;&#25968;&#25454;&#38598; (ChiMed)&#65292;&#21253;&#25324;&#21307;&#23398;&#38382;&#31572;&#12289;&#32431;&#25991;&#26412;&#12289;&#30693;&#35782;&#22270;&#35889;&#21644;&#23545;&#35805;&#65292;&#20998;&#20026;&#19977;&#20010;&#35757;&#32451;&#38454;&#27573;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#35757;&#32451;&#27969;&#31243;&#35757;&#32451;&#30340;&#21307;&#23398;LLM&#65292;Qilin-Med&#65292;&#22312;CPT&#21644;SFT&#38454;&#27573;&#22312;CMExam&#19978;&#20998;&#21035;&#36798;&#21040;&#20102;38.4%&#21644;40.0%&#30340;&#20934;&#30830;&#29575;&#65292;&#36229;&#36807;&#20102;Baichuan-7B&#30340;33.5%&#12290;&#22312;DPO&#38454;&#27573;&#65292;&#22312;Huatuo-26M&#27979;&#35797;&#38598;&#19978;&#24471;&#20998;&#20026;16.66&#12290;
&lt;/p&gt;
&lt;p&gt;
Integrating large language models (LLMs) into healthcare presents potential but faces challenges. Directly pre-training LLMs for domains like medicine is resource-heavy and sometimes unfeasible. Sole reliance on Supervised Fine-tuning (SFT) can result in overconfident predictions and may not tap into domain specific insights. Addressing these challenges, we present a multi-stage training method combining Domain-specific Continued Pre-training (DCPT), SFT, and Direct Preference Optimization (DPO). A notable contribution of our study is the introduction of a 3Gb Chinese Medicine (ChiMed) dataset, encompassing medical question answering, plain texts, knowledge graphs, and dialogues, segmented into three training stages. The medical LLM trained with our pipeline, Qilin-Med, exhibits significant performance boosts. In the CPT and SFT phases, it achieves 38.4% and 40.0% accuracy on the CMExam, surpassing Baichuan-7B's 33.5%. In the DPO phase, on the Huatuo-26M test set, it scores 16.66 in BL
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GenTKG&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#26102;&#38388;&#36923;&#36753;&#35268;&#21017;&#30340;&#26816;&#32034;&#31574;&#30053;&#21644;&#36731;&#37327;&#32423;&#30340;&#21442;&#25968;&#25928;&#29575;&#25351;&#23548;&#65292;&#20811;&#26381;&#20102;&#22797;&#26434;&#30340;&#26102;&#38388;&#22270;&#25968;&#25454;&#32467;&#26500;&#21644;&#24222;&#22823;&#30340;&#25968;&#25454;&#37327;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.07793</link><description>&lt;p&gt;
GenTKG: &#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
GenTKG: Generative Forecasting on Temporal Knowledge Graph. (arXiv:2310.07793v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07793
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GenTKG&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#26102;&#38388;&#36923;&#36753;&#35268;&#21017;&#30340;&#26816;&#32034;&#31574;&#30053;&#21644;&#36731;&#37327;&#32423;&#30340;&#21442;&#25968;&#25928;&#29575;&#25351;&#23548;&#65292;&#20811;&#26381;&#20102;&#22797;&#26434;&#30340;&#26102;&#38388;&#22270;&#25968;&#25454;&#32467;&#26500;&#21644;&#24222;&#22823;&#30340;&#25968;&#25454;&#37327;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#24555;&#36895;&#21457;&#23637;&#24341;&#21457;&#20102;&#23545;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;(tKG)&#39046;&#22495;&#30340;&#20852;&#36259;&#65292;&#20854;&#20013;&#20256;&#32479;&#30340;&#22522;&#20110;&#23884;&#20837;&#21644;&#35268;&#21017;&#30340;&#27169;&#22411;&#21344;&#20027;&#23548;&#22320;&#20301;&#12290;&#30446;&#21069;&#20173;&#28982;&#23384;&#22312;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#39044;&#35757;&#32451;&#30340;LLM&#26159;&#21542;&#33021;&#22815;&#29702;&#35299;&#32467;&#26500;&#21270;&#30340;&#26102;&#38388;&#20851;&#31995;&#25968;&#25454;&#65292;&#24182;&#21462;&#20195;&#23427;&#20204;&#25104;&#20026;&#26102;&#38388;&#20851;&#31995;&#39044;&#27979;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#26102;&#38388;&#30693;&#35782;&#39044;&#27979;&#24341;&#20837;&#29983;&#25104;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#22312;&#22797;&#26434;&#30340;&#26102;&#38388;&#22270;&#25968;&#25454;&#32467;&#26500;&#21644;LLM&#21487;&#20197;&#22788;&#29702;&#30340;&#24207;&#21015;&#33258;&#28982;&#34920;&#36798;&#20043;&#38388;&#23384;&#22312;&#24040;&#22823;&#30340;&#40511;&#27807;&#65292;&#22312;tKG&#30340;&#24222;&#22823;&#25968;&#25454;&#37327;&#21644;&#24494;&#35843;LLM&#30340;&#24040;&#22823;&#35745;&#31639;&#25104;&#26412;&#20043;&#38388;&#20063;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#26694;&#26550;&#65292;&#31216;&#20026;GenTKG&#65292;&#23427;&#22312;tKG&#19978;&#25191;&#34892;&#29983;&#25104;&#24335;&#39044;&#27979;&#65292;&#32467;&#21512;&#20102;&#22522;&#20110;&#26102;&#38388;&#36923;&#36753;&#35268;&#21017;&#30340;&#26816;&#32034;&#31574;&#30053;&#21644;&#36731;&#37327;&#32423;&#30340;&#21442;&#25968;&#25928;&#29575;&#25351;&#23548;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;GenTKG&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advancements in large language models (LLMs) have ignited interest in the temporal knowledge graph (tKG) domain, where conventional carefully designed embedding-based and rule-based models dominate. The question remains open of whether pre-trained LLMs can understand structured temporal relational data and replace them as the foundation model for temporal relational forecasting. Therefore, we bring temporal knowledge forecasting into the generative setting. However, challenges occur in the huge chasms between complex temporal graph data structure and sequential natural expressions LLMs can handle, and between the enormous data sizes of tKGs and heavy computation costs of finetuning LLMs. To address these challenges, we propose a novel retrieval augmented generation framework that performs generative forecasting on tKGs named GenTKG, which combines a temporal logical rule-based retrieval strategy and lightweight parameter-efficient instruction tuning. Extensive experiments hav
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#34429;&#28982;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#34920;&#29616;&#36739;&#24046;&#65292;&#20294;&#23427;&#20204;&#23637;&#31034;&#20102;&#20851;&#38190;&#32780;&#19968;&#33268;&#30340;&#20219;&#21153;&#24615;&#33021;&#25913;&#36827;&#65292;&#36825;&#19968;&#25913;&#36827;&#26080;&#27861;&#36890;&#36807;&#20256;&#32479;&#30340;&#35780;&#20272;&#31574;&#30053;&#26469;&#25429;&#25417;&#65292;&#22240;&#20026;&#35780;&#20272;&#30340;&#31934;&#24230;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2310.03262</link><description>&lt;p&gt;
&#35299;&#38145;&#20174;&#26032;&#20852;&#33021;&#21147;&#20013;&#21487;&#39044;&#27979;&#30340;&#25193;&#23637;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Unlock Predictable Scaling from Emergent Abilities. (arXiv:2310.03262v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03262
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#34429;&#28982;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#34920;&#29616;&#36739;&#24046;&#65292;&#20294;&#23427;&#20204;&#23637;&#31034;&#20102;&#20851;&#38190;&#32780;&#19968;&#33268;&#30340;&#20219;&#21153;&#24615;&#33021;&#25913;&#36827;&#65292;&#36825;&#19968;&#25913;&#36827;&#26080;&#27861;&#36890;&#36807;&#20256;&#32479;&#30340;&#35780;&#20272;&#31574;&#30053;&#26469;&#25429;&#25417;&#65292;&#22240;&#20026;&#35780;&#20272;&#30340;&#31934;&#24230;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#31185;&#23398;&#25193;&#23637;&#65292;&#38656;&#35201;&#20840;&#38754;&#20102;&#35299;&#23427;&#20204;&#30340;&#25193;&#23637;&#29305;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25991;&#29486;&#20851;&#20110;&#25193;&#23637;&#29305;&#24615;&#30340;&#30740;&#31350;&#21482;&#33021;&#24471;&#20986;&#19968;&#20010;&#19981;&#23436;&#25972;&#30340;&#31572;&#26696;&#65306;&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#65292;&#20248;&#21270;&#25439;&#22833;&#21487;&#39044;&#27979;&#22320;&#20943;&#23569;&#65292;&#31526;&#21512;&#24050;&#24314;&#31435;&#30340;&#32553;&#25918;&#23450;&#24459;&#65307;&#28982;&#32780;&#65292;&#20219;&#21153;&#30340;&#32553;&#25918;&#23450;&#24459;&#23578;&#26410;&#24314;&#31435;&#65292;&#20219;&#21153;&#34920;&#29616;&#22312;&#25193;&#23637;&#36807;&#31243;&#20013;&#36828;&#38750;&#21487;&#39044;&#27979;&#12290;&#20219;&#21153;&#34920;&#29616;&#36890;&#24120;&#22312;&#23567;&#27169;&#22411;&#19978;&#26174;&#31034;&#20986;&#36731;&#24494;&#22686;&#30410;&#65292;&#30452;&#21040;&#27169;&#22411;&#36229;&#36807;&#26576;&#20010;&#22823;&#23567;&#38408;&#20540;&#21518;&#25165;&#20986;&#29616;&#26174;&#33879;&#25913;&#36827;&#65292;&#23637;&#31034;&#20102;&#8220;&#26032;&#20852;&#33021;&#21147;&#8221;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#34429;&#28982;&#23567;&#27169;&#22411;&#34920;&#29616;&#20986;&#36731;&#24494;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#23637;&#29616;&#20102;&#20851;&#38190;&#32780;&#19968;&#33268;&#30340;&#20219;&#21153;&#24615;&#33021;&#25913;&#36827;&#65292;&#36825;&#20123;&#25913;&#36827;&#26080;&#27861;&#34987;&#20256;&#32479;&#35780;&#20272;&#31574;&#30053;&#25429;&#25417;&#21040;&#65292;&#22240;&#20026;&#27979;&#37327;&#20998;&#36776;&#29575;&#19981;&#36275;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#31181;&#25913;&#36827;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;PassUntil&#65292;&#22312;&#35299;&#30721;&#38454;&#27573;&#36890;&#36807;&#22823;&#35268;&#27169;&#25277;&#26679;&#36827;&#34892;&#35780;&#20272;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
The scientific scale-up of large language models (LLMs) necessitates a comprehensive understanding of their scaling properties. However, the existing literature on the scaling properties only yields an incomplete answer: optimization loss decreases predictably as the model size increases, in line with established scaling law; yet no scaling law for task has been established and the task performances are far from predictable during scaling. Task performances typically show minor gains on small models until they improve dramatically once models exceed a size threshold, exemplifying the ``emergent abilities''. In this study, we discover that small models, although they exhibit minor performance, demonstrate critical and consistent task performance improvements that are not captured by conventional evaluation strategies due to insufficient measurement resolution. To measure such improvements, we introduce PassUntil, an evaluation strategy through massive sampling in the decoding phase. We 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#22320;&#20998;&#26512;&#24182;&#25506;&#32034;&#20102;&#24503;&#35821;&#21644;&#33521;&#35821;&#30340;ChatGPT&#22238;&#24212;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#24615;&#21035;&#20559;&#35265;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#23545;&#31995;&#32479;&#22810;&#27425;&#25552;&#20379;&#30456;&#21516;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#65292;&#22238;&#24212;&#23384;&#22312;&#24046;&#24322;&#12290;&#20351;&#29992;ChatGPT&#26469;&#24110;&#21161;&#38750;IT&#29992;&#25143;&#25776;&#20889;&#24037;&#20316;&#25991;&#26412;&#38750;&#24120;&#26377;&#29992;&#65292;&#20294;&#29992;&#25143;&#38656;&#35201;&#20805;&#20998;&#32771;&#34385;&#31995;&#32479;&#30340;&#22266;&#26377;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.03031</link><description>&lt;p&gt;
ChatGPT&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#26377;&#22810;&#26222;&#36941;&#65311;&#8212;&#8212; &#25506;&#32034;&#24503;&#35821;&#21644;&#33521;&#35821;ChatGPT&#30340;&#22238;&#24212;
&lt;/p&gt;
&lt;p&gt;
How Prevalent is Gender Bias in ChatGPT? -- Exploring German and English ChatGPT Responses. (arXiv:2310.03031v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#22320;&#20998;&#26512;&#24182;&#25506;&#32034;&#20102;&#24503;&#35821;&#21644;&#33521;&#35821;&#30340;ChatGPT&#22238;&#24212;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#24615;&#21035;&#20559;&#35265;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#23545;&#31995;&#32479;&#22810;&#27425;&#25552;&#20379;&#30456;&#21516;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#65292;&#22238;&#24212;&#23384;&#22312;&#24046;&#24322;&#12290;&#20351;&#29992;ChatGPT&#26469;&#24110;&#21161;&#38750;IT&#29992;&#25143;&#25776;&#20889;&#24037;&#20316;&#25991;&#26412;&#38750;&#24120;&#26377;&#29992;&#65292;&#20294;&#29992;&#25143;&#38656;&#35201;&#20805;&#20998;&#32771;&#34385;&#31995;&#32479;&#30340;&#22266;&#26377;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;ChatGPT&#30340;&#25512;&#20986;&#65292;OpenAI&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21487;&#20379;&#20855;&#26377;&#26377;&#38480;IT&#19987;&#19994;&#30693;&#35782;&#30340;&#29992;&#25143;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#27809;&#26377;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#32972;&#26223;&#30340;&#29992;&#25143;&#21487;&#33021;&#32570;&#20047;&#23545;LLM&#30340;&#36866;&#24403;&#29702;&#35299;&#12290;&#22240;&#27492;&#65292;&#22312;&#22788;&#29702;&#31995;&#32479;&#36755;&#20986;&#26102;&#65292;&#32570;&#20047;&#23545;&#20854;&#22266;&#26377;&#38480;&#21046;&#30340;&#24847;&#35782;&#65292;&#23558;&#25509;&#21463;&#31995;&#32479;&#36755;&#20986;&#30340;&#34920;&#38754;&#20215;&#20540;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#20998;&#26512;&#36755;&#20837;&#25552;&#31034;&#21644;&#29983;&#25104;&#30340;&#22238;&#24212;&#65292;&#20197;&#35782;&#21035;&#21487;&#33021;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#20851;&#27880;&#24615;&#21035;&#20559;&#35265;&#38382;&#39064;&#65292;&#29992;&#25143;&#22312;&#22788;&#29702;&#31995;&#32479;&#36755;&#20986;&#26102;&#38656;&#35201;&#24847;&#35782;&#21040;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;ChatGPT&#22312;&#33521;&#35821;&#21644;&#24503;&#35821;&#20013;&#30340;&#21453;&#24212;&#65292;&#24182;&#25552;&#20379;&#20102;&#22899;&#24615;&#12289;&#30007;&#24615;&#25110;&#20013;&#31435;&#35282;&#24230;&#30340;&#25351;&#20196;&#26102;&#65292;&#22238;&#22797;&#30340;&#26159;&#21542;&#26377;&#24046;&#24322;&#12290;&#36890;&#36807;&#28145;&#20837;&#35843;&#26597;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20123;&#36873;&#25321;&#30340;&#25552;&#31034;&#65292;&#24182;&#20998;&#26512;&#20102;&#31995;&#32479;&#22312;&#30456;&#21516;&#26041;&#24335;&#19979;&#22810;&#27425;&#25552;&#20379;&#25351;&#20196;&#26102;&#22238;&#24212;&#30340;&#24046;&#24322;&#31243;&#24230;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#24110;&#21161;&#38750;IT&#29992;&#25143;&#25776;&#20889;&#26085;&#24120;&#24037;&#20316;&#25991;&#26412;&#65292;ChatGPT&#30830;&#23454;&#38750;&#24120;&#26377;&#29992;&#12290;&#28982;&#32780;&#65292;&#24403;&#28982;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#35201;&#24847;&#35782;&#21040;&#65292;&#24403;&#22788;&#29702;&#31995;&#32479;&#36755;&#20986;&#26102;&#65292;&#29992;&#25143;&#38656;&#35201;&#20805;&#20998;&#32771;&#34385;&#21040;&#20854;&#22266;&#26377;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the introduction of ChatGPT, OpenAI made large language models (LLM) accessible to users with limited IT expertise. However, users with no background in natural language processing (NLP) might lack a proper understanding of LLMs. Thus the awareness of their inherent limitations, and therefore will take the systems' output at face value. In this paper, we systematically analyse prompts and the generated responses to identify possible problematic issues with a special focus on gender biases, which users need to be aware of when processing the system's output. We explore how ChatGPT reacts in English and German if prompted to answer from a female, male, or neutral perspective. In an in-depth investigation, we examine selected prompts and analyse to what extent responses differ if the system is prompted several times in an identical way. On this basis, we show that ChatGPT is indeed useful for helping non-IT users draft texts for their daily work. However, it is absolutely crucial to 
&lt;/p&gt;</description></item><item><title>T$^3$Bench&#26159;&#31532;&#19968;&#20010;&#32508;&#21512;&#30340;&#25991;&#26412;&#21040;3D&#22522;&#20934;&#27979;&#35797;&#65292;&#23427;&#21253;&#21547;&#20102;&#22810;&#20010;&#22797;&#26434;&#31243;&#24230;&#30340;&#25991;&#26412;&#25552;&#31034;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#26469;&#35780;&#20272;&#29983;&#25104;&#30340;3D&#22330;&#26223;&#30340;&#20027;&#35266;&#36136;&#37327;&#21644;&#25991;&#26412;&#23545;&#40784;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.02977</link><description>&lt;p&gt;
T$^3$Bench&#65306;&#26631;&#27880;&#30446;&#21069;&#22312;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#39046;&#22495;&#30340;&#36827;&#23637;&#30340;&#22522;&#20934;&#27979;&#35797;&#65288;arXiv:2310.02977v1 [cs.CV]&#65289;
&lt;/p&gt;
&lt;p&gt;
T$^3$Bench: Benchmarking Current Progress in Text-to-3D Generation. (arXiv:2310.02977v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02977
&lt;/p&gt;
&lt;p&gt;
T$^3$Bench&#26159;&#31532;&#19968;&#20010;&#32508;&#21512;&#30340;&#25991;&#26412;&#21040;3D&#22522;&#20934;&#27979;&#35797;&#65292;&#23427;&#21253;&#21547;&#20102;&#22810;&#20010;&#22797;&#26434;&#31243;&#24230;&#30340;&#25991;&#26412;&#25552;&#31034;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#26469;&#35780;&#20272;&#29983;&#25104;&#30340;3D&#22330;&#26223;&#30340;&#20027;&#35266;&#36136;&#37327;&#21644;&#25991;&#26412;&#23545;&#40784;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#25991;&#26412;&#21040;3D&#26041;&#27861;&#21033;&#29992;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#26469;&#20248;&#21270;NeRF&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#22312;&#27809;&#26377;3D&#25968;&#25454;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;3D&#22330;&#26223;&#12290;&#30001;&#20110;&#20219;&#21153;&#30340;&#24320;&#25918;&#24615;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#36890;&#36807;&#20027;&#35266;&#26696;&#20363;&#30740;&#31350;&#21644;&#29992;&#25143;&#23454;&#39564;&#35777;&#26126;&#20854;&#32467;&#26524;&#65292;&#20174;&#32780;&#22312;&#23450;&#37327;&#19978;&#22238;&#31572;&#8220;&#25991;&#26412;&#21040;3D&#30340;&#24403;&#21069;&#36827;&#23637;&#22914;&#20309;&#65311;&#8221;&#36825;&#20010;&#38382;&#39064;&#19978;&#23384;&#22312;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;T$^3$Bench&#65292;&#36825;&#26159;&#19968;&#20010;&#31532;&#19968;&#20010;&#32508;&#21512;&#30340;&#25991;&#26412;&#21040;3D&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#21547;&#19977;&#20010;&#19981;&#26029;&#22686;&#21152;&#22797;&#26434;&#24615;&#30340;&#25991;&#26412;&#25552;&#31034;&#65292;&#19987;&#38376;&#20026;3D&#29983;&#25104;&#32780;&#35774;&#35745;&#12290;&#20026;&#20102;&#35780;&#20272;&#20027;&#35266;&#36136;&#37327;&#21644;&#25991;&#26412;&#23545;&#40784;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;3D&#20869;&#23481;&#20135;&#29983;&#30340;&#22810;&#35270;&#22270;&#22270;&#20687;&#30340;&#20004;&#20010;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#12290;&#36136;&#37327;&#24230;&#37327;&#32467;&#21512;&#20102;&#22810;&#35270;&#22270;&#25991;&#26412;-&#22270;&#20687;&#20998;&#25968;&#21644;&#21306;&#22495;&#21367;&#31215;&#20197;&#26816;&#27979;&#36136;&#37327;&#21644;&#35270;&#35282;&#19981;&#19968;&#33268;&#24615;&#12290;&#23545;&#40784;&#24230;&#37327;&#20351;&#29992;&#22810;&#35270;&#22270;&#23383;&#24149;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;e
&lt;/p&gt;
&lt;p&gt;
Recent methods in text-to-3D leverage powerful pretrained diffusion models to optimize NeRF. Notably, these methods are able to produce high-quality 3D scenes without training on 3D data. Due to the open-ended nature of the task, most studies evaluate their results with subjective case studies and user experiments, thereby presenting a challenge in quantitatively addressing the question: How has current progress in Text-to-3D gone so far? In this paper, we introduce T$^3$Bench, the first comprehensive text-to-3D benchmark containing diverse text prompts of three increasing complexity levels that are specially designed for 3D generation. To assess both the subjective quality and the text alignment, we propose two automatic metrics based on multi-view images produced by the 3D contents. The quality metric combines multi-view text-image scores and regional convolution to detect quality and view inconsistency. The alignment metric uses multi-view captioning and Large Language Model (LLM) e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35789;&#34955;&#27169;&#22411;&#33258;&#21160;&#20272;&#35745;&#35838;&#22530;&#25945;&#23398;&#25903;&#25345;&#65292;&#20197;&#25552;&#20379;&#26356;&#20855;&#20307;&#12289;&#39057;&#32321;&#21644;&#21487;&#34892;&#21160;&#30340;&#21453;&#39304;&#32473;&#25945;&#24072;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20934;&#30830;&#24615;&#25509;&#36817;&#20110;&#20154;&#24037;&#20114;&#35780;&#21487;&#38752;&#24615;&#65292;LLM&#27169;&#22411;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#21040;&#25945;&#23398;&#25903;&#25345;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2310.01132</link><description>&lt;p&gt;
&#20351;&#29992;LLM&#21644;BoWs&#33258;&#21160;&#35780;&#20272;&#35838;&#22530;&#25945;&#23398;&#25903;&#25345;&#65306;&#23558;&#20840;&#23616;&#39044;&#27979;&#19982;&#20855;&#20307;&#21453;&#39304;&#30456;&#36830;&#25509;
&lt;/p&gt;
&lt;p&gt;
Automated Evaluation of Classroom Instructional Support with LLMs and BoWs: Connecting Global Predictions to Specific Feedback. (arXiv:2310.01132v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01132
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35789;&#34955;&#27169;&#22411;&#33258;&#21160;&#20272;&#35745;&#35838;&#22530;&#25945;&#23398;&#25903;&#25345;&#65292;&#20197;&#25552;&#20379;&#26356;&#20855;&#20307;&#12289;&#39057;&#32321;&#21644;&#21487;&#34892;&#21160;&#30340;&#21453;&#39304;&#32473;&#25945;&#24072;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20934;&#30830;&#24615;&#25509;&#36817;&#20110;&#20154;&#24037;&#20114;&#35780;&#21487;&#38752;&#24615;&#65292;LLM&#27169;&#22411;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#21040;&#25945;&#23398;&#25903;&#25345;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#21521;&#25945;&#24072;&#25552;&#20379;&#26356;&#20855;&#20307;&#12289;&#26356;&#39057;&#32321;&#21644;&#21487;&#34892;&#21160;&#30340;&#21453;&#39304;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#20272;&#35745;&#8220;&#25945;&#23398;&#25903;&#25345;&#8221;&#39046;&#22495;&#30340;CLASS&#35838;&#22530;&#35780;&#20272;&#24471;&#20998;&#65292;&#35813;&#35780;&#20272;&#26041;&#27861;&#26159;&#24191;&#27867;&#20351;&#29992;&#30340;&#35266;&#27979;&#21327;&#35758;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#65292;&#20351;&#29992;Meta&#30340;Llama2&#30340;&#38646;-shot&#25552;&#31034;&#65292;&#21644;/&#25110;&#32463;&#20856;&#30340;&#35789;&#34955;&#65288;BoW&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#23545;&#25945;&#24072;&#35328;&#35821;&#30340;&#20010;&#21035;&#35805;&#35821;&#65288;&#20351;&#29992;OpenAI&#30340;Whisper&#36827;&#34892;&#33258;&#21160;&#36716;&#24405;&#65289;&#36827;&#34892;&#20998;&#31867;&#65292;&#20197;&#30830;&#23450;&#26159;&#21542;&#23384;&#22312;&#25945;&#23398;&#25903;&#25345;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#35805;&#35821;&#32423;&#30340;&#21028;&#26029;&#32467;&#26524;&#22312;&#25972;&#20010;15&#20998;&#38047;&#30340;&#35266;&#23519;&#20250;&#35805;&#20013;&#36827;&#34892;&#32858;&#21512;&#65292;&#20197;&#20272;&#35745;&#20840;&#23616;CLASS&#24471;&#20998;&#12290;&#22312;&#24188;&#20799;&#22253;&#21644;&#23398;&#21069;&#29677;&#25945;&#23460;&#30340;&#20004;&#20010;&#32463;&#36807;CLASS&#32534;&#30721;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65306;&#65288;1&#65289;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33258;&#21160;&#20272;&#35745;CLASS&#25945;&#23398;&#25903;&#25345;&#30340;&#20934;&#30830;&#24615;&#65288;Pearson R&#39640;&#36798;0.47&#65289;&#25509;&#36817;&#20154;&#24037;&#20114;&#35780;&#21487;&#38752;&#24615;&#65288;&#26368;&#39640;R=0.55&#65289;&#65307;&#65288;2&#65289;LLM&#27169;&#22411;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#21040;&#23567;&#29677;&#25945;&#23460;&#20013;&#30340;&#25945;&#23398;&#25903;&#25345;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the aim to provide teachers with more specific, frequent, and actionable feedback about their teaching, we explore how Large Language Models (LLMs) can be used to estimate ``Instructional Support'' domain scores of the CLassroom Assessment Scoring System (CLASS), a widely used observation protocol. We design a machine learning architecture that uses either zero-shot prompting of Meta's Llama2, and/or a classic Bag of Words (BoW) model, to classify individual utterances of teachers' speech (transcribed automatically using OpenAI's Whisper) for the presence of Instructional Support. Then, these utterance-level judgments are aggregated over an entire 15-min observation session to estimate a global CLASS score. Experiments on two CLASS-coded datasets of toddler and pre-kindergarten classrooms indicate that (1) automatic CLASS Instructional Support estimation accuracy using the proposed method (Pearson $R$ up to $0.47$) approaches human inter-rater reliability (up to $R=0.55$); (2) LLM
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Contextualized Bi-Directional Dual Transformer (CBDT) Classifier&#30340;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#25991;&#26412;&#20013;&#30340;&#20559;&#35265;&#26816;&#27979;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#20102;&#20004;&#20010;Transformer&#32593;&#32476;&#65292;&#33021;&#22815;&#20934;&#30830;&#21306;&#20998;&#26377;&#20559;&#35265;&#21644;&#20013;&#31435;&#30340;&#38472;&#36848;&#65292;&#24182;&#25214;&#20986;&#20855;&#20307;&#30340;&#26377;&#20559;&#35265;&#35789;&#27719;&#12290;CBDT&#27169;&#22411;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#26041;&#27861;&#65292;&#24615;&#33021;&#25552;&#21319;&#20102;2-4&#65285;&#65292;&#21516;&#26102;&#36824;&#20026;&#22312;&#19981;&#21516;&#35821;&#35328;&#21644;&#25991;&#21270;&#29615;&#22659;&#20013;&#24212;&#29992;&#35813;&#27169;&#22411;&#25171;&#24320;&#20102;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.00347</link><description>&lt;p&gt;
&#35299;&#38145;&#20559;&#35265;&#26816;&#27979;&#65306;&#21033;&#29992;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#36827;&#34892;&#20869;&#23481;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Unlocking Bias Detection: Leveraging Transformer-Based Models for Content Analysis. (arXiv:2310.00347v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00347
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Contextualized Bi-Directional Dual Transformer (CBDT) Classifier&#30340;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#25991;&#26412;&#20013;&#30340;&#20559;&#35265;&#26816;&#27979;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#20102;&#20004;&#20010;Transformer&#32593;&#32476;&#65292;&#33021;&#22815;&#20934;&#30830;&#21306;&#20998;&#26377;&#20559;&#35265;&#21644;&#20013;&#31435;&#30340;&#38472;&#36848;&#65292;&#24182;&#25214;&#20986;&#20855;&#20307;&#30340;&#26377;&#20559;&#35265;&#35789;&#27719;&#12290;CBDT&#27169;&#22411;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#26041;&#27861;&#65292;&#24615;&#33021;&#25552;&#21319;&#20102;2-4&#65285;&#65292;&#21516;&#26102;&#36824;&#20026;&#22312;&#19981;&#21516;&#35821;&#35328;&#21644;&#25991;&#21270;&#29615;&#22659;&#20013;&#24212;&#29992;&#35813;&#27169;&#22411;&#25171;&#24320;&#20102;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20559;&#35265;&#23545;&#20110;&#24378;&#21270;&#36127;&#38754;&#21051;&#26495;&#21360;&#35937;&#12289;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#21644;&#24433;&#21709;&#20915;&#31574;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#22240;&#27492;&#22312;&#25991;&#26412;&#20013;&#36827;&#34892;&#20559;&#35265;&#26816;&#27979;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#24448;&#24448;&#22312;&#36229;&#20986;&#20854;&#35757;&#32451;&#38598;&#33539;&#22260;&#26102;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Contextualized Bi-Directional Dual Transformer&#65288;CBDT&#65289;&#20998;&#31867;&#22120;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#21033;&#29992;&#20102;&#20004;&#20010;&#21327;&#21516;&#24037;&#20316;&#30340;Transformer&#32593;&#32476;&#65306;Context Transformer&#21644;Entity Transformer&#65292;&#26088;&#22312;&#22686;&#24378;&#20559;&#35265;&#26816;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#20934;&#22791;&#36981;&#24490;FAIR&#21407;&#21017;&#65292;&#30830;&#20445;&#25968;&#25454;&#20351;&#29992;&#20855;&#26377;&#36947;&#24503;&#24615;&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;&#25968;&#25454;&#38598;&#36827;&#34892;&#20005;&#26684;&#27979;&#35797;&#65292;CBDT&#23637;&#31034;&#20102;&#20854;&#22312;&#21306;&#20998;&#26377;&#20559;&#35265;&#19982;&#20013;&#31435;&#38472;&#36848;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#25351;&#20986;&#20855;&#20307;&#30340;&#26377;&#20559;&#35265;&#35789;&#27719;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#22312;&#22522;&#20934;&#24615;&#33021;&#19978;&#23454;&#29616;&#20102;2-4&#65285;&#30340;&#25552;&#21319;&#12290;&#36825;&#20026;&#23558;CBDT&#27169;&#22411;&#22312;&#19981;&#21516;&#30340;&#35821;&#35328;&#21644;&#25991;&#21270;&#29615;&#22659;&#20013;&#36827;&#34892;&#36866;&#24212;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bias detection in text is imperative due to its role in reinforcing negative stereotypes, disseminating misinformation, and influencing decisions. Current language models often fall short in generalizing beyond their training sets. In response, we introduce the Contextualized Bi-Directional Dual Transformer (CBDT) Classifier. This novel architecture utilizes two synergistic transformer networks: the Context Transformer and the Entity Transformer, aiming for enhanced bias detection. Our dataset preparation follows the FAIR principles, ensuring ethical data usage. Through rigorous testing on various datasets, CBDT showcases its ability in distinguishing biased from neutral statements, while also pinpointing exact biased lexemes. Our approach outperforms existing methods, achieving a 2-4\% increase over benchmark performances. This opens avenues for adapting the CBDT model across diverse linguistic and cultural landscapes.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;&#26694;&#26550;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#34892;&#20026;&#65292;&#21457;&#29616;&#27169;&#22411;&#23545;&#32422;&#26463;&#26631;&#35760;&#30340;&#20851;&#27880;&#31243;&#24230;&#19982;&#20107;&#23454;&#20934;&#30830;&#24615;&#24378;&#27491;&#30456;&#20851;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#21487;&#20197;&#39044;&#27979;&#32422;&#26463;&#28385;&#36275;&#21644;&#20107;&#23454;&#38169;&#35823;&#65292;&#24182;&#20801;&#35768;&#26089;&#26399;&#38169;&#35823;&#35782;&#21035;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.15098</link><description>&lt;p&gt;
&#28385;&#36275;&#20851;&#27880;&#65306;&#23545;&#35821;&#35328;&#27169;&#22411;&#20107;&#23454;&#38169;&#35823;&#30340;&#32422;&#26463;&#28385;&#36275;&#35270;&#35282;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Attention Satisfies: A Constraint-Satisfaction Lens on Factual Errors of Language Models. (arXiv:2309.15098v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;&#26694;&#26550;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#34892;&#20026;&#65292;&#21457;&#29616;&#27169;&#22411;&#23545;&#32422;&#26463;&#26631;&#35760;&#30340;&#20851;&#27880;&#31243;&#24230;&#19982;&#20107;&#23454;&#20934;&#30830;&#24615;&#24378;&#27491;&#30456;&#20851;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#21487;&#20197;&#39044;&#27979;&#32422;&#26463;&#28385;&#36275;&#21644;&#20107;&#23454;&#38169;&#35823;&#65292;&#24182;&#20801;&#35768;&#26089;&#26399;&#38169;&#35823;&#35782;&#21035;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#29983;&#25104;&#20107;&#23454;&#19978;&#38169;&#35823;&#30340;&#25991;&#26412;&#26102;&#30340;&#20869;&#37096;&#34892;&#20026;&#12290;&#25105;&#20204;&#23558;&#20107;&#23454;&#26597;&#35810;&#24314;&#27169;&#20026;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#36825;&#19968;&#26694;&#26550;&#30740;&#31350;&#27169;&#22411;&#22914;&#20309;&#19982;&#20107;&#23454;&#32422;&#26463;&#36827;&#34892;&#20869;&#37096;&#20132;&#20114;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#23545;&#32422;&#26463;&#26631;&#35760;&#30340;&#20851;&#27880;&#31243;&#24230;&#19982;&#20854;&#21709;&#24212;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#23384;&#22312;&#24378;&#27491;&#30456;&#20851;&#20851;&#31995;&#12290;&#22312;&#25105;&#20204;&#30340;11&#20010;&#25968;&#25454;&#38598;&#20013;&#65292;&#24635;&#35745;&#36229;&#36807;40,000&#20010;&#25552;&#31034;&#30340;&#31934;&#24515;&#31574;&#21010;&#22871;&#35013;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;Llama-2&#31995;&#21015;&#22312;&#25152;&#26377;&#35268;&#27169;&#65288;7B&#65292;13B&#65292;70B&#65289;&#19978;&#39044;&#27979;&#20107;&#23454;&#38169;&#35823;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SAT Probe&#65292;&#19968;&#31181;&#25506;&#26597;&#33258;&#27880;&#24847;&#27169;&#24335;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#39044;&#27979;&#32422;&#26463;&#28385;&#36275;&#21644;&#20107;&#23454;&#38169;&#35823;&#65292;&#24182;&#20801;&#35768;&#26089;&#26399;&#38169;&#35823;&#35782;&#21035;&#12290;&#36825;&#19968;&#26041;&#27861;&#21644;&#21457;&#29616;&#34920;&#26126;&#65292;&#21033;&#29992;&#23545;LLM&#20013;&#20107;&#23454;&#24615;&#30340;&#26426;&#26800;&#29702;&#35299;&#21487;&#20197;&#22686;&#24378;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the internal behavior of Transformer-based Large Language Models (LLMs) when they generate factually incorrect text. We propose modeling factual queries as Constraint Satisfaction Problems and use this framework to investigate how the model interacts internally with factual constraints. Specifically, we discover a strong positive relation between the model's attention to constraint tokens and the factual accuracy of its responses. In our curated suite of 11 datasets with over 40,000 prompts, we study the task of predicting factual errors with the Llama-2 family across all scales (7B, 13B, 70B). We propose SAT Probe, a method probing self-attention patterns, that can predict constraint satisfaction and factual errors, and allows early error identification. The approach and findings demonstrate how using the mechanistic understanding of factuality in LLMs can enhance reliability.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#21035;&#29305;&#23450;&#26426;&#22120;&#32763;&#35793;&#30740;&#31350;&#21457;&#29616;&#65292;LLaMa&#33021;&#22815;&#20197;&#31454;&#20105;&#24615;&#20934;&#30830;&#24615;&#21644;&#24615;&#21035;&#20559;&#24046;&#32531;&#35299;&#29983;&#25104;&#24615;&#21035;&#29305;&#23450;&#30340;&#32763;&#35793;&#65292;&#24182;&#22312;&#24615;&#21035;&#27169;&#31946;&#30340;&#24773;&#22659;&#20013;&#20445;&#25345;&#31283;&#20581;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.03175</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#21035;&#29305;&#23450;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Gender-specific Machine Translation with Large Language Models. (arXiv:2309.03175v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03175
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#21035;&#29305;&#23450;&#26426;&#22120;&#32763;&#35793;&#30740;&#31350;&#21457;&#29616;&#65292;LLaMa&#33021;&#22815;&#20197;&#31454;&#20105;&#24615;&#20934;&#30830;&#24615;&#21644;&#24615;&#21035;&#20559;&#24046;&#32531;&#35299;&#29983;&#25104;&#24615;&#21035;&#29305;&#23450;&#30340;&#32763;&#35793;&#65292;&#24182;&#22312;&#24615;&#21035;&#27169;&#31946;&#30340;&#24773;&#22659;&#20013;&#20445;&#25345;&#31283;&#20581;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#30721;&#22120;&#19987;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#28508;&#21147;&#65292;&#23613;&#31649;&#24615;&#33021;&#30053;&#20302;&#20110;&#20256;&#32479;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;LLM&#20855;&#26377;&#29420;&#29305;&#30340;&#20248;&#21183;&#65306;&#36890;&#36807;&#25552;&#31034;&#25511;&#21046;&#36755;&#20986;&#30340;&#29305;&#24615;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#28789;&#27963;&#24615;&#26469;&#25506;&#32034;LLaMa&#22312;&#20855;&#26377;&#35821;&#27861;&#24615;&#21035;&#30340;&#35821;&#35328;&#20013;&#29983;&#25104;&#24615;&#21035;&#29305;&#23450;&#32763;&#35793;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#22810;&#35821;&#31181;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;NLLB&#30456;&#27604;&#65292;LLaMa&#21487;&#20197;&#20197;&#26377;&#31454;&#20105;&#21147;&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#21035;&#20559;&#24046;&#32531;&#35299;&#29983;&#25104;&#24615;&#21035;&#29305;&#23450;&#30340;&#32763;&#35793;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;LLaMa&#30340;&#32763;&#35793;&#32467;&#26524;&#26159;&#31283;&#20581;&#30340;&#65292;&#22312;&#24615;&#21035;&#27169;&#31946;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;&#35780;&#20272;&#19982;&#30456;&#21453;&#24615;&#21035;&#21442;&#32771;&#32763;&#35793;&#26102;&#20250;&#20986;&#29616;&#26174;&#33879;&#24615;&#33021;&#19979;&#38477;&#65292;&#20294;&#22312;&#19981;&#22826;&#27169;&#31946;&#30340;&#19978;&#19979;&#25991;&#20013;&#20445;&#25345;&#19968;&#33268;&#12290;&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#20351;&#29992;LLM&#36827;&#34892;&#24615;&#21035;&#29305;&#23450;&#32763;&#35793;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decoder-only Large Language Models (LLMs) have demonstrated potential in machine translation (MT), albeit with performance slightly lagging behind traditional encoder-decoder Neural Machine Translation (NMT) systems. However, LLMs offer a unique advantage: the ability to control the properties of the output through prompts. In this study, we harness this flexibility to explore LLaMa's capability to produce gender-specific translations for languages with grammatical gender. Our results indicate that LLaMa can generate gender-specific translations with competitive accuracy and gender bias mitigation when compared to NLLB, a state-of-the-art multilingual NMT system. Furthermore, our experiments reveal that LLaMa's translations are robust, showing significant performance drops when evaluated against opposite-gender references in gender-ambiguous datasets but maintaining consistency in less ambiguous contexts. This research provides insights into the potential and challenges of using LLMs f
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#24050;&#32463;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;DNN&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#24182;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#35821;&#35328;&#29702;&#35299;&#12289;&#29983;&#25104;&#12289;&#25512;&#29702;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.02046</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#25512;&#33616;&#31995;&#32479; (LLMs)
&lt;/p&gt;
&lt;p&gt;
Recommender Systems in the Era of Large Language Models (LLMs). (arXiv:2307.02046v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02046
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#24050;&#32463;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;DNN&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#24182;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#35821;&#35328;&#29702;&#35299;&#12289;&#29983;&#25104;&#12289;&#25512;&#29702;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#30005;&#23376;&#21830;&#21153;&#21644;&#32593;&#32476;&#24212;&#29992;&#30340;&#32321;&#33635;&#65292;&#25512;&#33616;&#31995;&#32479;&#65288;RecSys&#65289;&#24050;&#32463;&#25104;&#20026;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#20013;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#20010;&#24615;&#21270;&#24314;&#35758;&#20197;&#28385;&#36275;&#20854;&#21916;&#22909;&#12290;&#23613;&#31649;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#36890;&#36807;&#27169;&#25311;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#21644;&#25972;&#21512;&#25991;&#26412;&#20391;&#20449;&#24687;&#22312;&#25552;&#21319;&#25512;&#33616;&#31995;&#32479;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#20294;&#26159;DNN&#26041;&#27861;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#20363;&#22914;&#29702;&#35299;&#29992;&#25143;&#20852;&#36259;&#12289;&#25429;&#25417;&#25991;&#26412;&#20391;&#20449;&#24687;&#30340;&#22256;&#38590;&#65292;&#20197;&#21450;&#22312;&#19981;&#21516;&#25512;&#33616;&#22330;&#26223;&#20013;&#27867;&#21270;&#21644;&#25512;&#29702;&#33021;&#21147;&#30340;&#19981;&#36275;&#31561;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65288;&#20363;&#22914;ChatGPT&#21644;GPT4&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#30340;&#22522;&#26412;&#32844;&#36131;&#19978;&#26377;&#30528;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#27867;&#21270;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the prosperity of e-commerce and web applications, Recommender Systems (RecSys) have become an important component of our daily life, providing personalized suggestions that cater to user preferences. While Deep Neural Networks (DNNs) have made significant advancements in enhancing recommender systems by modeling user-item interactions and incorporating textual side information, DNN-based methods still face limitations, such as difficulties in understanding users' interests and capturing textual side information, inabilities in generalizing to various recommendation scenarios and reasoning on their predictions, etc. Meanwhile, the emergence of Large Language Models (LLMs), such as ChatGPT and GPT4, has revolutionized the fields of Natural Language Processing (NLP) and Artificial Intelligence (AI), due to their remarkable abilities in fundamental responsibilities of language understanding and generation, as well as impressive generalization and reasoning capabilities. As a result, 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#35270;&#39057;&#26816;&#32034;&#27169;&#22411;&#20013;&#30340;&#32452;&#21512;&#21644;&#35821;&#20041;&#29702;&#35299;&#65292;&#24182;&#36890;&#36807;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;&#36825;&#20123;&#32452;&#25104;&#37096;&#20998;&#23545;&#35270;&#39057;&#26816;&#32034;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.16533</link><description>&lt;p&gt;
ICSVR: &#22312;&#35270;&#39057;&#26816;&#32034;&#27169;&#22411;&#20013;&#30740;&#31350;&#32452;&#21512;&#21644;&#35821;&#20041;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
ICSVR: Investigating Compositional and Semantic Understanding in Video Retrieval Models. (arXiv:2306.16533v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16533
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#35270;&#39057;&#26816;&#32034;&#27169;&#22411;&#20013;&#30340;&#32452;&#21512;&#21644;&#35821;&#20041;&#29702;&#35299;&#65292;&#24182;&#36890;&#36807;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;&#36825;&#20123;&#32452;&#25104;&#37096;&#20998;&#23545;&#35270;&#39057;&#26816;&#32034;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#26816;&#32034;&#65288;VR&#65289;&#28041;&#21450;&#26681;&#25454;&#25991;&#26412;&#26631;&#39064;&#26816;&#32034;&#35270;&#39057;&#25968;&#25454;&#24211;&#20013;&#30340;&#30495;&#23454;&#35270;&#39057;&#65292;&#25110;&#21453;&#20043;&#20134;&#28982;&#12290;&#21512;&#25104;&#24615;&#30340;&#20004;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65306;&#23545;&#35937;&#21644;&#23646;&#24615;&#20197;&#21450;&#21160;&#20316;&#65292;&#20351;&#29992;&#27491;&#30830;&#30340;&#35821;&#20041;&#32852;&#32467;&#20197;&#24418;&#25104;&#27491;&#30830;&#30340;&#25991;&#26412;&#26597;&#35810;&#12290;&#36825;&#20123;&#32452;&#25104;&#37096;&#20998;&#65288;&#23545;&#35937;&#21644;&#23646;&#24615;&#12289;&#21160;&#20316;&#21644;&#35821;&#20041;&#65289;&#21508;&#33258;&#22312;&#24110;&#21161;&#21306;&#20998;&#35270;&#39057;&#21644;&#26816;&#32034;&#27491;&#30830;&#30340;&#30495;&#23454;&#35270;&#39057;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32452;&#25104;&#37096;&#20998;&#23545;&#35270;&#39057;&#26816;&#32034;&#24615;&#33021;&#30340;&#24433;&#21709;&#23578;&#19981;&#28165;&#26970;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#31995;&#32479;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#35270;&#39057;&#26816;&#32034;&#27169;&#22411;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#19978;&#23545;&#32452;&#21512;&#21644;&#35821;&#20041;&#29702;&#35299;&#30340;&#33021;&#21147;&#65292;&#22914;MSRVTT&#12289;MSVD&#21644;DIDEMO&#12290;&#35813;&#30740;&#31350;&#38024;&#23545;&#20004;&#31867;&#35270;&#39057;&#26816;&#32034;&#27169;&#22411;&#36827;&#34892;&#20102;&#65292;&#19968;&#31867;&#26159;&#22312;&#35270;&#39057;&#25991;&#26412;&#23545;&#19978;&#39044;&#35757;&#32451;&#24182;&#22312;&#19979;&#28216;&#35270;&#39057;&#26816;&#32034;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#65288;&#20363;&#22914;&#65292;Frozen-in-Time&#12289;Violet&#12289;MCQ&#31561;&#65289;&#65292;&#21478;&#19968;&#31867;&#26159;&#36866;&#24212;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#25991;&#26412;&#34920;&#31034;&#65288;&#22914;CLIP&#65289;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video retrieval (VR) involves retrieving the ground truth video from the video database given a text caption or vice-versa. The two important components of compositionality: objects \&amp; attributes and actions are joined using correct semantics to form a proper text query. These components (objects \&amp; attributes, actions and semantics) each play an important role to help distinguish among videos and retrieve the correct ground truth video. However, it is unclear what is the effect of these components on the video retrieval performance. We therefore, conduct a systematic study to evaluate the compositional and semantic understanding of video retrieval models on standard benchmarks such as MSRVTT, MSVD and DIDEMO. The study is performed on two categories of video retrieval models: (i) which are pre-trained on video-text pairs and fine-tuned on downstream video retrieval datasets (Eg. Frozen-in-Time, Violet, MCQ etc.) (ii) which adapt pre-trained image-text representations like CLIP for vid
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65288;Corr2Cause&#65289;&#65292;&#29992;&#20110;&#27979;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22240;&#26524;&#25512;&#26029;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#24456;&#24046;&#12290;</title><link>http://arxiv.org/abs/2306.05836</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#20174;&#30456;&#20851;&#24615;&#20013;&#25512;&#26029;&#20986;&#22240;&#26524;&#20851;&#31995;?
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Infer Causation from Correlation?. (arXiv:2306.05836v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65288;Corr2Cause&#65289;&#65292;&#29992;&#20110;&#27979;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22240;&#26524;&#25512;&#26029;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#24456;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25512;&#26029;&#26159;&#20154;&#31867;&#26234;&#24935;&#30340;&#26631;&#24535;&#20043;&#19968;&#12290;&#34429;&#28982;CausalNLP&#39046;&#22495;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;NLP&#20013;&#29616;&#26377;&#30340;&#22240;&#26524;&#25512;&#26029;&#25968;&#25454;&#38598;&#20027;&#35201;&#20381;&#36182;&#20110;&#20174;&#32463;&#39564;&#30693;&#35782;&#65288;&#20363;&#22914;&#24120;&#35782;&#30693;&#35782;&#65289;&#20013;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#32431;&#22240;&#26524;&#25512;&#26029;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;Corr2Cause&#65292;&#23427;&#37319;&#29992;&#19968;&#32452;&#30456;&#20851;&#35821;&#21477;&#24182;&#30830;&#23450;&#21464;&#37327;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#25105;&#20204;&#31574;&#21010;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;400K&#20010;&#26679;&#26412;&#65292;&#25105;&#20204;&#22312;&#20854;&#20013;&#35780;&#20272;&#20102;17&#20010;&#29616;&#26377;&#30340;LLMs&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;LLMs&#22312;&#22240;&#26524;&#25512;&#26029;&#25216;&#33021;&#26041;&#38754;&#30340;&#19968;&#20010;&#20851;&#38190;&#32570;&#38519;&#65292;&#24182;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#35813;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#20960;&#20046;&#25509;&#36817;&#38543;&#26426;&#12290;&#24403;&#25105;&#20204;&#23581;&#35797;&#36890;&#36807;&#24494;&#35843;&#23558;LLMs&#37325;&#26032;&#29992;&#20110;&#36825;&#31181;&#25216;&#33021;&#26102;&#65292;&#36825;&#31181;&#32570;&#38519;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#24471;&#21040;&#20102;&#32531;&#35299;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#20173;&#28982;&#22833;&#36133;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal inference is one of the hallmarks of human intelligence. While the field of CausalNLP has attracted much interest in the recent years, existing causal inference datasets in NLP primarily rely on discovering causality from empirical knowledge (e.g., commonsense knowledge). In this work, we propose the first benchmark dataset to test the pure causal inference skills of large language models (LLMs). Specifically, we formulate a novel task Corr2Cause, which takes a set of correlational statements and determines the causal relationship between the variables. We curate a large-scale dataset of more than 400K samples, on which we evaluate seventeen existing LLMs. Through our experiments, we identify a key shortcoming of LLMs in terms of their causal inference skills, and show that these models achieve almost close to random performance on the task. This shortcoming is somewhat mitigated when we try to re-purpose LLMs for this skill via finetuning, but we find that these models still fa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65288;CPL-NoViD&#65289;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#23558;&#19978;&#19979;&#25991;&#34701;&#20837;&#21040;&#27169;&#22411;&#20013;&#65292;&#29992;&#20110;&#22312;&#32447;&#31038;&#21306;&#20013;&#30340;&#36829;&#35268;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#31038;&#21306;&#20013;&#30340;&#21508;&#31181;&#35268;&#21017;&#21644;&#35299;&#37322;&#30340;&#24046;&#24322;&#65292;&#22312;&#36328;&#35268;&#21017;&#31867;&#22411;&#21644;&#36328;&#31038;&#21306;&#30340;&#36829;&#35268;&#34892;&#20026;&#26816;&#27979;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#19968;&#23450;&#30340;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.09846</link><description>&lt;p&gt;
&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#25552;&#31034;&#24335;&#23398;&#20064;&#29992;&#20110;&#22312;&#32447;&#31038;&#21306;&#36829;&#35268;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
CPL-NoViD: Context-Aware Prompt-based Learning for Norm Violation Detection in Online Communities. (arXiv:2305.09846v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09846
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65288;CPL-NoViD&#65289;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#23558;&#19978;&#19979;&#25991;&#34701;&#20837;&#21040;&#27169;&#22411;&#20013;&#65292;&#29992;&#20110;&#22312;&#32447;&#31038;&#21306;&#20013;&#30340;&#36829;&#35268;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#31038;&#21306;&#20013;&#30340;&#21508;&#31181;&#35268;&#21017;&#21644;&#35299;&#37322;&#30340;&#24046;&#24322;&#65292;&#22312;&#36328;&#35268;&#21017;&#31867;&#22411;&#21644;&#36328;&#31038;&#21306;&#30340;&#36829;&#35268;&#34892;&#20026;&#26816;&#27979;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#19968;&#23450;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#31038;&#21306;&#20013;&#26816;&#27979;&#36829;&#35268;&#34892;&#20026;&#23545;&#20110;&#32500;&#25252;&#20581;&#24247;&#21644;&#23433;&#20840;&#30340;&#22312;&#32447;&#35752;&#35770;&#31354;&#38388;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24448;&#24448;&#38590;&#20197;&#36866;&#24212;&#19981;&#21516;&#31038;&#21306;&#20043;&#38388;&#21508;&#31181;&#35268;&#21017;&#21644;&#35299;&#37322;&#30340;&#24046;&#24322;&#65292;&#22240;&#20026;&#20026;&#36825;&#31181;&#29305;&#23450;&#19978;&#19979;&#25991;&#30340;&#20219;&#21153;&#24494;&#35843;&#27169;&#22411;&#20855;&#26377;&#22256;&#38590;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#19978;&#19979;&#25991;&#25552;&#31034;&#30340;&#23398;&#20064;&#29992;&#20110;&#26816;&#27979;&#19981;&#21516;&#31867;&#22411;&#35268;&#21017;&#19979;&#30340;&#36829;&#35268;&#34892;&#20026;&#65288;CPL-NoViD&#65289;&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;CPL-NoViD&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#26469;&#23558;&#19978;&#19979;&#25991;&#34701;&#20837;&#21040;&#27169;&#22411;&#20013;&#65292;&#23545;&#19981;&#21516;&#31867;&#22411;&#35268;&#21017;&#30340;&#34920;&#29616;&#20063;&#24471;&#21040;&#20102;&#25913;&#21892;&#65292;&#19981;&#20165;&#22312;&#36328;&#35268;&#21017;&#31867;&#22411;&#21644;&#36328;&#31038;&#21306;&#30340;&#36829;&#35268;&#34892;&#20026;&#26816;&#27979;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;&#19988;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#22330;&#26223;&#20013;&#20063;&#34920;&#29616;&#20986;&#19968;&#23450;&#30340;&#36866;&#24212;&#24615;&#12290;&#23588;&#20854;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23427;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#36829;&#35268;&#26816;&#27979;&#26032;&#30340;&#26368;&#39640;&#27700;&#24179;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting norm violations in online communities is critical to maintaining healthy and safe spaces for online discussions. Existing machine learning approaches often struggle to adapt to the diverse rules and interpretations across different communities due to the inherent challenges of fine-tuning models for such context-specific tasks. In this paper, we introduce Context-aware Prompt-based Learning for Norm Violation Detection (CPL-NoViD), a novel method that employs prompt-based learning to detect norm violations across various types of rules. CPL-NoViD outperforms the baseline by incorporating context through natural language prompts and demonstrates improved performance across different rule types. Significantly, it not only excels in cross-rule-type and cross-community norm violation detection but also exhibits adaptability in few-shot learning scenarios. Most notably, it establishes a new state-of-the-art in norm violation detection, surpassing existing benchmarks. Our work high
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;MRC&#27169;&#22411;&#23545;&#20302;&#36164;&#28304;&#22320;&#21306;&#37325;&#21629;&#21517;&#23454;&#20307;&#30340;&#40065;&#26834;&#24615;&#65292;&#25552;&#20986;&#20102;EntSwap&#25200;&#21160;&#27979;&#35797;&#38598;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#22823;&#22411;&#27169;&#22411;&#20855;&#26377;&#36739;&#24378;&#30340;&#26032;&#23454;&#20307;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.03145</link><description>&lt;p&gt;
&#35780;&#20272;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#27169;&#22411;&#23545;&#20302;&#36164;&#28304;&#23454;&#20307;&#37325;&#21629;&#21517;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Robustness of Machine Reading Comprehension Models to Low Resource Entity Renaming. (arXiv:2304.03145v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03145
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;MRC&#27169;&#22411;&#23545;&#20302;&#36164;&#28304;&#22320;&#21306;&#37325;&#21629;&#21517;&#23454;&#20307;&#30340;&#40065;&#26834;&#24615;&#65292;&#25552;&#20986;&#20102;EntSwap&#25200;&#21160;&#27979;&#35797;&#38598;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#22823;&#22411;&#27169;&#22411;&#20855;&#26377;&#36739;&#24378;&#30340;&#26032;&#23454;&#20307;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#31572;&#65288;QA&#65289;&#27169;&#22411;&#22312;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#65288;MRC&#65289;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#20449;&#26381;&#30340;&#32467;&#26524;&#12290;&#26368;&#36817;&#65292;&#36825;&#20123;&#27169;&#22411;&#24050;&#32463;&#35777;&#26126;&#22312;&#22914;SQuAD&#31561;&#25968;&#25454;&#38598;&#30340;&#27979;&#35797;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#20154;&#31867;&#65292;&#20294;&#23427;&#20204;&#30340;&#31283;&#20581;&#24615;&#24182;&#19981;&#20445;&#35777;&#12290;&#24403;&#20351;&#29992;&#23545;&#25239;&#29983;&#25104;&#30340;&#31034;&#20363;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;QA&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;&#20250;&#26292;&#38706;&#20986;&#26469;&#65292;&#34920;&#29616;&#20986;&#24615;&#33021;&#19979;&#38477;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;MRC&#27169;&#22411;&#23545;&#26469;&#33258;&#20302;&#36164;&#28304;&#22320;&#21306;&#65288;&#22914;&#38750;&#27954;&#65289;&#30340;&#23454;&#20307;&#37325;&#21629;&#21517;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;EntSwap&#65292;&#19968;&#31181;&#27979;&#35797;&#26102;&#25200;&#21160;&#26041;&#27861;&#65292;&#29992;&#20110;&#21019;&#24314;&#19968;&#20010;&#23454;&#20307;&#24050;&#34987;&#37325;&#21629;&#21517;&#30340;&#27979;&#35797;&#38598;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#37325;&#21629;&#21517;&#31867;&#22411;&#20026;&#22269;&#23478;&#65292;&#20154;&#29289;&#65292;&#22269;&#31821;&#65292;&#20301;&#32622;&#65292;&#32452;&#32455;&#21644;&#22478;&#24066;&#30340;&#23454;&#20307;&#65292;&#20197;&#21019;&#24314;AfriSQuAD2&#12290;&#20351;&#29992;&#25200;&#21160;&#27979;&#35797;&#38598;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19977;&#31181;&#27969;&#34892;&#30340;MRC&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19982;&#22522;&#20934;&#27169;&#22411;&#30456;&#27604;&#65292;&#22823;&#27169;&#22411;&#22312;&#26032;&#23454;&#20307;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#20154;&#21517;&#23454;&#20307;&#31867;&#22411;&#20855;&#26377;&#39640;&#24230;&#30340;&#29305;&#24322;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Question answering (QA) models have shown compelling results in the task of Machine Reading Comprehension (MRC). Recently these systems have proved to perform better than humans on held-out test sets of datasets e.g. SQuAD, but their robustness is not guaranteed. The QA model's brittleness is exposed when evaluated on adversarial generated examples by a performance drop. In this study, we explore the robustness of MRC models to entity renaming, with entities from low-resource regions such as Africa. We propose EntSwap, a method for test-time perturbations, to create a test set whose entities have been renamed. In particular, we rename entities of type: country, person, nationality, location, organization, and city, to create AfriSQuAD2. Using the perturbed test set, we evaluate the robustness of three popular MRC models. We find that compared to base models, large models perform well comparatively on novel entities. Furthermore, our analysis indicates that entity type person highly cha
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#22522;&#20110;&#31867;&#27604;&#30340;&#25512;&#29702;&#20013;&#30340;&#31867;&#27604;&#26816;&#27979;&#21644;&#35299;&#20915;&#20004;&#20010;&#20219;&#21153;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#29983;&#25104;&#20043;&#21069;&#19981;&#23384;&#22312;&#20110;&#25968;&#25454;&#38598;&#20013;&#30340;&#31867;&#27604;&#12290;</title><link>http://arxiv.org/abs/2303.18062</link><description>&lt;p&gt;
&#35299;&#20915;&#24418;&#24577;&#23398;&#31867;&#27604;&#38382;&#39064;&#65306;&#20174;&#26816;&#32034;&#21040;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Solving morphological analogies: from retrieval to generation. (arXiv:2303.18062v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18062
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#22522;&#20110;&#31867;&#27604;&#30340;&#25512;&#29702;&#20013;&#30340;&#31867;&#27604;&#26816;&#27979;&#21644;&#35299;&#20915;&#20004;&#20010;&#20219;&#21153;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#29983;&#25104;&#20043;&#21069;&#19981;&#23384;&#22312;&#20110;&#25968;&#25454;&#38598;&#20013;&#30340;&#31867;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#27604;&#25512;&#29702;&#26159;&#20154;&#31867;&#24605;&#32500;&#30340;&#19968;&#31181;&#38750;&#20961;&#33021;&#21147;&#65292;&#24182;&#19988;&#24050;&#34987;&#29992;&#26469;&#35299;&#20915;&#38590;&#20197;&#29702;&#35299;&#30340;&#20219;&#21153;&#12290; &#22522;&#20110;&#31867;&#27604;&#30340;&#25512;&#29702;&#65288;AR&#65289;&#21463;&#21040;&#20102;&#20154;&#24037;&#26234;&#33021;&#31038;&#21306;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#24182;&#22312;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20854;&#28508;&#21147;&#65292;&#20363;&#22914;&#20998;&#31867;&#65292;&#20915;&#31574;&#21644;&#20855;&#26377;&#31454;&#20105;&#24615;&#32467;&#26524;&#30340;&#25512;&#33616;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;AR&#20013;&#30340;&#20004;&#20010;&#20851;&#38190;&#20219;&#21153;&#65306;&#31867;&#27604;&#26816;&#27979;&#21644;&#35299;&#20915;&#12290;&#35813;&#26694;&#26550;&#22312;&#25972;&#20010;Siganalogies&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#27979;&#35797;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#21333;&#35789;&#20043;&#38388;&#30340;&#24418;&#24577;&#23398;&#31867;&#27604;&#27604;&#20363;&#65288;APs&#65289;&#65292;&#24182;&#19988;&#22312;&#35768;&#22810;&#35821;&#35328;&#20013;&#26174;&#31034;&#20986;&#20248;&#20110;&#31526;&#21495;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290; &#20043;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#25506;&#32034;&#20102;&#20998;&#31867;&#38382;&#39064;&#19978;&#30340;&#31867;&#27604;&#31070;&#32463;&#32593;&#32476;&#34892;&#20026;&#65288;ANNc&#65289;&#21644;&#26816;&#32034;&#38382;&#39064;&#19978;&#30340;&#31867;&#27604;&#31070;&#32463;&#32593;&#32476;&#34892;&#20026;&#65288;ANNr&#65289;&#65292;&#20197;&#21450;&#33258;&#32534;&#30721;&#22120;&#65288;AE&#65289;&#22312;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#21333;&#35789;&#19978;&#30340;&#28508;&#21147;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#22522;&#20110;&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;CVAE&#65289;&#30340;&#32479;&#19968;&#26694;&#26550;&#26469;&#24635;&#32467;&#24182;&#25193;&#23637;&#20197;&#21069;&#30340;&#24037;&#20316;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#20849;&#21516;&#35299;&#20915;&#20004;&#20010;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#21487;&#20197;&#29983;&#25104;&#22312;&#25968;&#25454;&#38598;&#20013;&#20197;&#21069;&#19981;&#23384;&#22312;&#30340;&#31867;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analogical inference is a remarkable capability of human reasoning, and has been used to solve hard reasoning tasks. Analogy based reasoning (AR) has gained increasing interest from the artificial intelligence community and has shown its potential in multiple machine learning tasks such as classification, decision making and recommendation with competitive results. We propose a deep learning (DL) framework to address and tackle two key tasks in AR: analogy detection and solving. The framework is thoroughly tested on the Siganalogies dataset of morphological analogical proportions (APs) between words, and shown to outperform symbolic approaches in many languages. Previous work have explored the behavior of the Analogy Neural Network for classification (ANNc) on analogy detection and of the Analogy Neural Network for retrieval (ANNr) on analogy solving by retrieval, as well as the potential of an autoencoder (AE) for analogy solving by generating the solution word. In this article we sum
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#35821;&#31181;&#35270;&#35273;&#38382;&#31572;&#25968;&#25454;&#38598;EVJVQA&#65292;&#21253;&#25324;&#36234;&#21335;&#35821;&#65292;&#33521;&#35821;&#21644;&#26085;&#35821;&#30340;33,000+&#38382;&#31572;&#23545;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#22810;&#35821;&#35328;VQA&#31995;&#32479;&#25110;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.11752</link><description>&lt;p&gt;
VLSP2022-EVJVQA&#25361;&#25112;&#65306;&#22810;&#35821;&#31181;&#35270;&#35273;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
VLSP2022-EVJVQA Challenge: Multilingual Visual Question Answering. (arXiv:2302.11752v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11752
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#35821;&#31181;&#35270;&#35273;&#38382;&#31572;&#25968;&#25454;&#38598;EVJVQA&#65292;&#21253;&#25324;&#36234;&#21335;&#35821;&#65292;&#33521;&#35821;&#21644;&#26085;&#35821;&#30340;33,000+&#38382;&#31572;&#23545;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#22810;&#35821;&#35328;VQA&#31995;&#32479;&#25110;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#30340;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#21560;&#24341;&#20102;&#30740;&#31350;&#20154;&#21592;&#30340;&#37325;&#35270;&#12290; &#33521;&#35821;&#26159;&#19968;&#20010;&#36164;&#28304;&#20016;&#23500;&#30340;&#35821;&#35328;&#65292;&#22312;&#35270;&#35273;&#38382;&#31572;&#30340;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#26041;&#38754;&#26377;&#30528;&#21508;&#31181;&#21457;&#23637;&#12290; &#20854;&#20182;&#35821;&#35328;&#30340;&#35270;&#35273;&#38382;&#31572;&#20063;&#23558;&#20250;&#26377;&#36164;&#28304;&#21644;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290; &#27492;&#22806;&#65292;&#36824;&#27809;&#26377;&#38024;&#23545;&#29305;&#23450;&#22269;&#23478;&#30340;&#35270;&#35273;&#20869;&#23481;&#21644;&#25991;&#21270;&#29305;&#28857;&#25552;&#20379;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21517;&#20026;EVJVQA&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#22312;&#36234;&#21335;&#25293;&#25668;&#30340;&#32422;5,000&#24352;&#22270;&#29255;&#19978;&#30340;&#19977;&#31181;&#35821;&#35328;&#65288;&#36234;&#21335;&#35821;&#65292;&#33521;&#35821;&#21644;&#26085;&#35821;&#65289;&#30340;33,000&#22810;&#23545;&#38382;&#31572;&#23545;&#65292;&#20197;&#35780;&#20272;&#22810;&#35821;&#35328;VQA&#31995;&#32479;&#25110;&#27169;&#22411;&#12290; EVJVQA&#20316;&#20026;&#25361;&#25112;&#22810;&#35821;&#31181;&#35270;&#35273;&#38382;&#31572;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#22312;&#31532;9&#23626;&#36234;&#21335;&#35821;&#35328;&#21644;&#35821;&#38899;&#22788;&#29702;&#30740;&#35752;&#20250;&#65288;VLSP2022&#65289;&#19978;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual Question Answering (VQA) is a challenging task of natural language processing (NLP) and computer vision (CV), attracting significant attention from researchers. English is a resource-rich language that has witnessed various developments in datasets and models for visual question answering. Visual question answering in other languages also would be developed for resources and models. In addition, there is no multilingual dataset targeting the visual content of a particular country with its own objects and cultural characteristics. To address the weakness, we provide the research community with a benchmark dataset named EVJVQA, including 33,000+ pairs of question-answer over three languages: Vietnamese, English, and Japanese, on approximately 5,000 images taken from Vietnam for evaluating multilingual VQA systems or models. EVJVQA is used as a benchmark dataset for the challenge of multilingual visual question answering at the 9th Workshop on Vietnamese Language and Speech Process
&lt;/p&gt;</description></item></channel></rss>