<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#29702;&#35770;&#12289;&#24212;&#29992;&#21644;&#20998;&#31867;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#35814;&#32454;&#21644;&#30495;&#23454;&#30340;&#20998;&#31867;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.15703</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65306;&#20174;&#29702;&#35770;&#21040;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Uncertainty in Natural Language Generation: From Theory to Applications. (arXiv:2307.15703v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#29702;&#35770;&#12289;&#24212;&#29992;&#21644;&#20998;&#31867;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#35814;&#32454;&#21644;&#30495;&#23454;&#30340;&#20998;&#31867;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#20351;&#24471;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#20316;&#20026;&#19968;&#31181;&#37325;&#35201;&#25216;&#26415;&#23853;&#38706;&#22836;&#35282;&#65292;&#23427;&#19981;&#20165;&#21487;&#20197;&#25191;&#34892;&#20256;&#32479;&#20219;&#21153;&#22914;&#25688;&#35201;&#25110;&#32763;&#35793;&#65292;&#20063;&#21487;&#20197;&#20316;&#20026;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#25509;&#21475;&#24212;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#12290;&#22240;&#27492;&#65292;NLG&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#20449;&#24230;&#33267;&#20851;&#37325;&#35201;&#65292;&#20363;&#22914;&#22312;&#21487;&#33021;&#20986;&#38169;&#26102;&#25351;&#31034;&#65292;&#24182;&#25903;&#25345;&#22810;&#31181;&#35266;&#28857;&#12289;&#32972;&#26223;&#21644;&#20889;&#20316;&#39118;&#26684; - &#21453;&#26144;&#22810;&#20803;&#21270;&#20154;&#31867;&#20122;&#32676;&#20307;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#23545;&#19981;&#30830;&#23450;&#24615;&#30340;&#21407;&#21017;&#24615;&#22788;&#29702;&#33021;&#22815;&#24110;&#21161;&#21019;&#24314;&#19982;&#36825;&#20123;&#30446;&#26631;&#26356;&#22909;&#22320;&#23545;&#40784;&#30340;&#31995;&#32479;&#21644;&#35780;&#20272;&#21327;&#35758;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#34920;&#31034;&#19981;&#30830;&#23450;&#24615;&#25152;&#38656;&#30340;&#22522;&#26412;&#29702;&#35770;&#12289;&#26694;&#26550;&#21644;&#35789;&#27719;&#12290;&#28982;&#21518;&#20174;&#35821;&#35328;&#23398;&#30340;&#35282;&#24230;&#25551;&#36848;NLG&#20013;&#30340;&#20027;&#35201;&#19981;&#30830;&#23450;&#24615;&#28304;&#65292;&#24182;&#25552;&#20986;&#19968;&#20010;&#27604;&#27969;&#34892;&#30340;&#38543;&#26426;&#24615;/&#35748;&#35782;&#24615;&#20108;&#20998;&#27861;&#26356;&#35814;&#32454;&#21644;&#30495;&#23454;&#30340;&#20108;&#32500;&#20998;&#31867;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances of powerful Language Models have allowed Natural Language Generation (NLG) to emerge as an important technology that can not only perform traditional tasks like summarisation or translation, but also serve as a natural language interface to a variety of applications. As such, it is crucial that NLG systems are trustworthy and reliable, for example by indicating when they are likely to be wrong; and supporting multiple views, backgrounds and writing styles -- reflecting diverse human sub-populations. In this paper, we argue that a principled treatment of uncertainty can assist in creating systems and evaluation protocols better aligned with these goals. We first present the fundamental theory, frameworks and vocabulary required to represent uncertainty. We then characterise the main sources of uncertainty in NLG from a linguistic perspective, and propose a two-dimensional taxonomy that is more informative and faithful than the popular aleatoric/epistemic dichotomy. Final
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#20013;&#29983;&#25104;&#22823;&#35268;&#27169;&#25968;&#25454;&#30340;&#26377;&#25928;&#33539;&#24335;&#12290;&#36890;&#36807;&#21033;&#29992;&#36924;&#30495;&#30340;&#29615;&#22659;&#21644;&#32593;&#32476;&#36164;&#28304;&#65292;&#21512;&#25104;&#20102;490&#19975;&#20010;&#25351;&#20196;&#36712;&#36857;&#23545;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#27169;&#20223;&#23398;&#20064;&#65292;&#24050;&#23384;&#22312;&#30340;&#20195;&#29702;&#30340;&#24615;&#33021;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#21319;&#33267;80%&#12290;</title><link>http://arxiv.org/abs/2307.15644</link><description>&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#20013;&#30340;&#25968;&#25454;&#29983;&#25104;&#35268;&#27169;&#21270;
&lt;/p&gt;
&lt;p&gt;
Scaling Data Generation in Vision-and-Language Navigation. (arXiv:2307.15644v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15644
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#20013;&#29983;&#25104;&#22823;&#35268;&#27169;&#25968;&#25454;&#30340;&#26377;&#25928;&#33539;&#24335;&#12290;&#36890;&#36807;&#21033;&#29992;&#36924;&#30495;&#30340;&#29615;&#22659;&#21644;&#32593;&#32476;&#36164;&#28304;&#65292;&#21512;&#25104;&#20102;490&#19975;&#20010;&#25351;&#20196;&#36712;&#36857;&#23545;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#27169;&#20223;&#23398;&#20064;&#65292;&#24050;&#23384;&#22312;&#30340;&#20195;&#29702;&#30340;&#24615;&#33021;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#21319;&#33267;80%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#35821;&#35328;&#24341;&#23548;&#30340;&#35270;&#35273;&#23548;&#33322;&#30740;&#31350;&#20013;&#65292;&#23545;&#20110;&#36941;&#21382;&#29615;&#22659;&#30340;&#22810;&#26679;&#24615;&#21644;&#35757;&#32451;&#21487;&#27867;&#21270;&#20195;&#29702;&#30340;&#30417;&#30563;&#25968;&#37327;&#26377;&#20102;&#26126;&#26174;&#38656;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#29616;&#26377;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#25968;&#25454;&#38598;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#33539;&#24335;&#65292;&#29992;&#20110;&#29983;&#25104;&#29992;&#20110;&#23398;&#20064;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#12290;&#25105;&#20204;&#24212;&#29992;&#20102;HM3D&#21644;Gibson&#25968;&#25454;&#38598;&#20013;&#30340;1200&#22810;&#20010;&#36924;&#30495;&#30340;&#29615;&#22659;&#65292;&#24182;&#21033;&#29992;&#32593;&#32476;&#19978;&#30340;&#36164;&#28304;&#21512;&#25104;&#20102;490&#19975;&#20010;&#25351;&#20196;&#36712;&#36857;&#23545;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#33539;&#24335;&#20013;&#27599;&#20010;&#32452;&#25104;&#37096;&#20998;&#23545;&#20195;&#29702;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#30740;&#31350;&#20102;&#22914;&#20309;&#24688;&#24403;&#22320;&#24212;&#29992;&#25193;&#22686;&#25968;&#25454;&#26469;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20195;&#29702;&#12290;&#24471;&#30410;&#20110;&#25105;&#20204;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#27169;&#20223;&#23398;&#20064;&#65292;&#29616;&#26377;&#20195;&#29702;&#30340;&#24615;&#33021;&#21487;&#20197;&#22823;&#24133;&#25552;&#21319;&#65288;&#30456;&#23545;&#20110;&#20043;&#21069;&#30340;&#26368;&#20339;&#32467;&#26524;&#32477;&#23545;&#20540;&#22686;&#21152;&#20102;11%&#65289;&#65292;&#22312;R2R&#27979;&#35797;&#38598;&#20013;&#21333;&#27425;&#36816;&#34892;&#25104;&#21151;&#29575;&#26174;&#33879;&#25552;&#21319;&#33267;80%&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research in language-guided visual navigation has demonstrated a significant demand for the diversity of traversable environments and the quantity of supervision for training generalizable agents. To tackle the common data scarcity issue in existing vision-and-language navigation datasets, we propose an effective paradigm for generating large-scale data for learning, which applies 1200+ photo-realistic environments from HM3D and Gibson datasets and synthesizes 4.9 million instruction trajectory pairs using fully-accessible resources on the web. Importantly, we investigate the influence of each component in this paradigm on the agent's performance and study how to adequately apply the augmented data to pre-train and fine-tune an agent. Thanks to our large-scale dataset, the performance of an existing agent can be pushed up (+11% absolute with regard to previous SoTA) to a significantly new best of 80% single-run success rate on the R2R test split by simple imitation learning. The
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#28155;&#21152;&#40065;&#26834;&#26080;&#30072;&#21464;&#27700;&#21360;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26144;&#23556;&#38543;&#26426;&#25968;&#24207;&#21015;&#21040;&#35821;&#35328;&#27169;&#22411;&#30340;&#26679;&#26412;&#65292;&#21487;&#20197;&#23454;&#29616;&#22312;&#19981;&#25913;&#21464;&#25991;&#26412;&#20998;&#24067;&#30340;&#21069;&#25552;&#19979;&#23545;&#27700;&#21360;&#25991;&#26412;&#36827;&#34892;&#26816;&#27979;&#65292;&#24182;&#19988;&#22312;&#22810;&#31181;&#25913;&#20889;&#25915;&#20987;&#19979;&#20381;&#28982;&#20445;&#25345;&#36739;&#39640;&#30340;&#40065;&#26834;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;40-50%&#30340;&#38543;&#26426;&#25200;&#21160;&#19979;&#20173;&#21487;&#21487;&#38752;&#22320;&#26816;&#27979;&#21040;&#27700;&#21360;&#25991;&#26412;&#12290;</title><link>http://arxiv.org/abs/2307.15593</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#26080;&#30072;&#21464;&#27700;&#21360;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Robust Distortion-free Watermarks for Language Models. (arXiv:2307.15593v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15593
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#28155;&#21152;&#40065;&#26834;&#26080;&#30072;&#21464;&#27700;&#21360;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26144;&#23556;&#38543;&#26426;&#25968;&#24207;&#21015;&#21040;&#35821;&#35328;&#27169;&#22411;&#30340;&#26679;&#26412;&#65292;&#21487;&#20197;&#23454;&#29616;&#22312;&#19981;&#25913;&#21464;&#25991;&#26412;&#20998;&#24067;&#30340;&#21069;&#25552;&#19979;&#23545;&#27700;&#21360;&#25991;&#26412;&#36827;&#34892;&#26816;&#27979;&#65292;&#24182;&#19988;&#22312;&#22810;&#31181;&#25913;&#20889;&#25915;&#20987;&#19979;&#20381;&#28982;&#20445;&#25345;&#36739;&#39640;&#30340;&#40065;&#26834;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;40-50%&#30340;&#38543;&#26426;&#25200;&#21160;&#19979;&#20173;&#21487;&#21487;&#38752;&#22320;&#26816;&#27979;&#21040;&#27700;&#21360;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#20013;&#28155;&#21152;&#27700;&#21360;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#36825;&#20123;&#27700;&#21360;&#23545;&#25200;&#21160;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#32780;&#19981;&#20250;&#25913;&#21464;&#25991;&#26412;&#30340;&#20998;&#24067;&#65292;&#21516;&#26102;&#20445;&#35777;&#29983;&#25104;&#39044;&#31639;&#22312;&#19968;&#23450;&#33539;&#22260;&#20869;&#12290;&#25105;&#20204;&#29992;&#38543;&#26426;&#27700;&#21360;&#23494;&#38053;&#35745;&#31639;&#30340;&#38543;&#26426;&#25968;&#24207;&#21015;&#26144;&#23556;&#21040;&#35821;&#35328;&#27169;&#22411;&#30340;&#26679;&#26412;&#26469;&#29983;&#25104;&#24102;&#27700;&#21360;&#30340;&#25991;&#26412;&#12290;&#35201;&#26816;&#27979;&#27700;&#21360;&#25991;&#26412;&#65292;&#21482;&#35201;&#30693;&#36947;&#23494;&#38053;&#30340;&#20219;&#20309;&#19968;&#26041;&#37117;&#21487;&#20197;&#23558;&#25991;&#26412;&#19982;&#38543;&#26426;&#25968;&#24207;&#21015;&#23545;&#40784;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#31181;&#37319;&#26679;&#26041;&#26696;&#26469;&#23454;&#20363;&#21270;&#27700;&#21360;&#26041;&#27861;&#65306;&#21453;&#21464;&#25442;&#37319;&#26679;&#21644;&#25351;&#25968;&#26368;&#23567;&#37319;&#26679;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#27700;&#21360;&#24212;&#29992;&#20110;&#19977;&#20010;&#35821;&#35328;&#27169;&#22411;&#8212;&#8212;OPT-1.3B&#12289;LLaMA-7B&#21644;Alpaca-7B&#65292;&#20197;&#23454;&#39564;&#35777;&#26126;&#23427;&#20204;&#30340;&#32479;&#35745;&#21151;&#25928;&#21644;&#23545;&#21508;&#31181;&#25913;&#20889;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23545;&#20110;OPT-1.3B&#21644;LLaMA-7B&#27169;&#22411;&#65292;&#21363;&#20351;&#22312;&#38543;&#26426;&#25200;&#21160;&#20102;40-50%&#30340;&#35789;&#20803;&#21518;&#65292;&#25105;&#20204;&#20173;&#28982;&#21487;&#20197;&#21487;&#38752;&#22320;&#26816;&#27979;&#21040;&#24102;&#27700;&#21360;&#30340;&#25991;&#26412;&#65288;$p \leq 0.01$&#65289;&#65292;&#21482;&#38656;&#35201;35&#20010;&#35789;&#20803;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a methodology for planting watermarks in text from an autoregressive language model that are robust to perturbations without changing the distribution over text up to a certain maximum generation budget. We generate watermarked text by mapping a sequence of random numbers -- which we compute using a randomized watermark key -- to a sample from the language model. To detect watermarked text, any party who knows the key can align the text to the random number sequence. We instantiate our watermark methodology with two sampling schemes: inverse transform sampling and exponential minimum sampling. We apply these watermarks to three language models -- OPT-1.3B, LLaMA-7B and Alpaca-7B -- to experimentally validate their statistical power and robustness to various paraphrasing attacks. Notably, for both the OPT-1.3B and LLaMA-7B models, we find we can reliably detect watermarked text ($p \leq 0.01$) from $35$ tokens even after corrupting between $40$-$50$\% of the tokens via random
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23558;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24212;&#29992;&#20110;&#39044;&#27979;&#21516;&#34892;&#36741;&#23548;&#20013;&#36991;&#20813;&#20882;&#29359;&#35821;&#20986;&#29616;&#30340;&#24773;&#20917;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#25429;&#25417;&#20808;&#21069;&#36716;&#25442;&#30340;&#35821;&#20041;&#20449;&#24687;&#30340;&#23884;&#20837;&#23618;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#20351;&#29992;Shapley&#20540;&#36827;&#34892;&#29305;&#24449;&#35299;&#37322;&#65292;&#30740;&#31350;&#21457;&#29616;&#30446;&#20809;&#27880;&#35270;&#23545;&#20110;&#36991;&#20813;&#20882;&#29359;&#35821;&#30340;&#39044;&#27979;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.15582</link><description>&lt;p&gt;
&#22312;&#21516;&#34892;&#36741;&#23548;&#20013;&#20309;&#26102;&#20135;&#29983;&#36991;&#20813;&#20882;&#29359;&#35821;
&lt;/p&gt;
&lt;p&gt;
When to generate hedges in peer-tutoring interactions. (arXiv:2307.15582v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23558;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24212;&#29992;&#20110;&#39044;&#27979;&#21516;&#34892;&#36741;&#23548;&#20013;&#36991;&#20813;&#20882;&#29359;&#35821;&#20986;&#29616;&#30340;&#24773;&#20917;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#25429;&#25417;&#20808;&#21069;&#36716;&#25442;&#30340;&#35821;&#20041;&#20449;&#24687;&#30340;&#23884;&#20837;&#23618;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#20351;&#29992;Shapley&#20540;&#36827;&#34892;&#29305;&#24449;&#35299;&#37322;&#65292;&#30740;&#31350;&#21457;&#29616;&#30446;&#20809;&#27880;&#35270;&#23545;&#20110;&#36991;&#20813;&#20882;&#29359;&#35821;&#30340;&#39044;&#27979;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24212;&#29992;&#20110;&#39044;&#27979;&#21516;&#34892;&#36741;&#23548;&#20013;&#36991;&#20813;&#20882;&#29359;&#35821;&#20986;&#29616;&#30340;&#24773;&#20917;&#12290;&#30740;&#31350;&#20351;&#29992;&#20102;&#19968;&#20010;&#33258;&#28982;&#38754;&#23545;&#38754;&#30340;&#25968;&#25454;&#38598;&#65292;&#23545;&#33258;&#28982;&#35821;&#35328;&#30340;&#36716;&#25442;&#12289;&#23545;&#35805;&#31574;&#30053;&#12289;&#36741;&#23548;&#31574;&#30053;&#21644;&#38750;&#35328;&#35821;&#34892;&#20026;&#36827;&#34892;&#20102;&#27880;&#37322;&#12290;&#36825;&#20123;&#20803;&#32032;&#34987;&#22788;&#29702;&#25104;&#20808;&#21069;&#36716;&#25442;&#30340;&#21521;&#37327;&#34920;&#31034;&#65292;&#20316;&#20026;&#36755;&#20837;&#32473;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25429;&#25417;&#20808;&#21069;&#36716;&#25442;&#30340;&#35821;&#20041;&#20449;&#24687;&#30340;&#23884;&#20837;&#23618;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;Shapley&#20540;&#36827;&#34892;&#29305;&#24449;&#35299;&#37322;&#65292;&#25552;&#20379;&#20102;&#26377;&#20851;&#21508;&#31181;&#29305;&#24449;&#65288;&#22914;&#20154;&#38469;&#20851;&#31995;&#21644;&#38750;&#35328;&#35821;&#34892;&#20026;&#65289;&#22312;&#39044;&#27979;&#36991;&#20813;&#20882;&#29359;&#35821;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#30340;&#28145;&#20837;&#27934;&#23519;&#12290;&#25105;&#20204;&#21457;&#29616;&#23548;&#24072;&#21644;&#23398;&#21592;&#30340;&#30446;&#20809;&#27880;&#35270;&#23545;&#20110;&#36991;&#20813;&#20882;&#29359;&#35821;&#30340;&#39044;&#27979;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#21518;&#32493;&#30340;&#20999;&#21106;&#30740;&#31350;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the application of machine learning techniques to predict where hedging occurs in peer-tutoring interactions. The study uses a naturalistic face-to-face dataset annotated for natural language turns, conversational strategies, tutoring strategies, and nonverbal behaviours. These elements are processed into a vector representation of the previous turns, which serves as input to several machine learning models. Results show that embedding layers, that capture the semantic information of the previous turns, significantly improves the model's performance. Additionally, the study provides insights into the importance of various features, such as interpersonal rapport and nonverbal behaviours, in predicting hedges by using Shapley values for feature explanation. We discover that the eye gaze of both the tutor and the tutee has a significant impact on hedge prediction. We further validate this observation through a follow-up ablation study.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#29305;&#24449;&#34701;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#21512;&#25104;&#35821;&#38899;&#26816;&#27979;&#20219;&#21153;&#12290;&#36890;&#36807;&#34701;&#21512;&#19977;&#20010;&#19981;&#21516;&#30340;&#29305;&#24449;&#38598;&#65292;&#35813;&#27169;&#22411;&#30456;&#27604;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#19981;&#21516;&#22330;&#26223;&#21644;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.15555</link><description>&lt;p&gt;
&#20247;&#24535;&#25104;&#22478;&#65306;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#29305;&#24449;&#34701;&#21512;&#29992;&#20110;&#21512;&#25104;&#35821;&#38899;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
All-for-One and One-For-All: Deep learning-based feature fusion for Synthetic Speech Detection. (arXiv:2307.15555v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15555
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#29305;&#24449;&#34701;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#21512;&#25104;&#35821;&#38899;&#26816;&#27979;&#20219;&#21153;&#12290;&#36890;&#36807;&#34701;&#21512;&#19977;&#20010;&#19981;&#21516;&#30340;&#29305;&#24449;&#38598;&#65292;&#35813;&#27169;&#22411;&#30456;&#27604;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#19981;&#21516;&#22330;&#26223;&#21644;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#28145;&#24230;&#23398;&#20064;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#36827;&#27493;&#20351;&#24471;&#22810;&#23186;&#20307;&#20869;&#23481;&#30340;&#21512;&#25104;&#21644;&#20266;&#36896;&#21464;&#24471;&#26356;&#21152;&#26131;&#20110;&#23454;&#29616;&#65292;&#21487;&#33021;&#24341;&#21457;&#26469;&#33258;&#24694;&#24847;&#29992;&#25143;&#30340;&#23041;&#32961;&#21644;&#21361;&#38505;&#12290;&#22312;&#38899;&#39057;&#39046;&#22495;&#65292;&#25105;&#20204;&#30446;&#30585;&#20102;&#35821;&#38899;&#28145;&#20266;&#36896;&#29983;&#25104;&#25216;&#26415;&#30340;&#22686;&#38271;&#65292;&#36825;&#20419;&#20351;&#30740;&#31350;&#21512;&#25104;&#35821;&#38899;&#26816;&#27979;&#31639;&#27861;&#20197;&#24212;&#23545;&#21487;&#33021;&#30340;&#24694;&#24847;&#20351;&#29992;&#65292;&#20363;&#22914;&#27450;&#35784;&#25110;&#36523;&#20221;&#30423;&#31363;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#19977;&#31181;&#19981;&#21516;&#30340;&#29305;&#24449;&#38598;&#65292;&#29992;&#20110;&#21512;&#25104;&#35821;&#38899;&#26816;&#27979;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#23427;&#20204;&#30340;&#27169;&#22411;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#25972;&#20307;&#24615;&#33021;&#26356;&#22909;&#12290;&#36890;&#36807;&#23545;&#19981;&#21516;&#22330;&#26223;&#21644;&#25968;&#25454;&#38598;&#36827;&#34892;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;&#35813;&#31995;&#32479;&#23545;&#25239;&#21453;&#21462;&#35777;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#20197;&#21450;&#20854;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in deep learning and computer vision have made the synthesis and counterfeiting of multimedia content more accessible than ever, leading to possible threats and dangers from malicious users. In the audio field, we are witnessing the growth of speech deepfake generation techniques, which solicit the development of synthetic speech detection algorithms to counter possible mischievous uses such as frauds or identity thefts. In this paper, we consider three different feature sets proposed in the literature for the synthetic speech detection task and present a model that fuses them, achieving overall better performances with respect to the state-of-the-art solutions. The system was tested on different scenarios and datasets to prove its robustness to anti-forensic attacks and its generalization capabilities.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#27169;&#24577;&#23545;&#35805;&#27169;&#22411;&#22788;&#29702;&#28548;&#28165;&#20132;&#27969;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#22522;&#20110;&#35821;&#35328;&#30340;&#27169;&#22411;&#22312;&#22788;&#29702;&#19982;&#23545;&#35805;&#21382;&#21490;&#30456;&#20851;&#30340;&#28548;&#28165;&#20132;&#27969;&#26102;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;&#22810;&#27169;&#24577;&#27169;&#22411;&#21017;&#33021;&#21033;&#29992;&#39069;&#22806;&#30340;&#23398;&#20064;&#30446;&#26631;&#33719;&#21462;&#20998;&#35299;&#30340;&#23545;&#35937;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2307.15554</link><description>&lt;p&gt;
&#8220;&#20320;&#22312;&#25351;&#20160;&#20040;&#65311;&#8221;&#35780;&#20272;&#22810;&#27169;&#24577;&#23545;&#35805;&#27169;&#22411;&#22788;&#29702;&#28548;&#28165;&#20132;&#27969;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
'What are you referring to?' Evaluating the Ability of Multi-Modal Dialogue Models to Process Clarificational Exchanges. (arXiv:2307.15554v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15554
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#27169;&#24577;&#23545;&#35805;&#27169;&#22411;&#22788;&#29702;&#28548;&#28165;&#20132;&#27969;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#22522;&#20110;&#35821;&#35328;&#30340;&#27169;&#22411;&#22312;&#22788;&#29702;&#19982;&#23545;&#35805;&#21382;&#21490;&#30456;&#20851;&#30340;&#28548;&#28165;&#20132;&#27969;&#26102;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;&#22810;&#27169;&#24577;&#27169;&#22411;&#21017;&#33021;&#21033;&#29992;&#39069;&#22806;&#30340;&#23398;&#20064;&#30446;&#26631;&#33719;&#21462;&#20998;&#35299;&#30340;&#23545;&#35937;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#19968;&#20010;&#25351;&#31216;&#34920;&#36798;&#26080;&#27861;&#30830;&#23450;&#21807;&#19968;&#22320;&#30830;&#23450;&#21457;&#35328;&#32773;&#30340;&#24847;&#22270;&#26102;&#65292;&#23545;&#35805;&#20013;&#20986;&#29616;&#25351;&#31216;&#27495;&#20041;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#34987;&#23547;&#22336;&#20154;&#20250;&#31435;&#21363;&#21457;&#29616;&#36825;&#31181;&#27495;&#20041;&#65292;&#24182;&#19982;&#21457;&#35328;&#32773;&#19968;&#36215;&#36890;&#36807;&#20803;&#27807;&#36890;&#28548;&#28165;&#20132;&#27969;&#65288;CE&#65289;&#26469;&#20462;&#22797;&#12290;CE&#21253;&#25324;&#28548;&#28165;&#35831;&#27714;&#65288;CR&#65289;&#21644;&#22238;&#24212;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#29983;&#25104;&#21644;&#22238;&#24212;CR&#23545;&#22810;&#27169;&#24577;&#19988;&#20197;&#35270;&#35273;&#20026;&#22522;&#30784;&#30340;&#23545;&#35805;&#27169;&#22411;&#30340;&#26550;&#26500;&#21644;&#30446;&#26631;&#20989;&#25968;&#26045;&#21152;&#20102;&#29305;&#23450;&#30340;&#32422;&#26463;&#12290;&#25105;&#20204;&#20351;&#29992;SIMMC 2.0&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#19981;&#21516;&#26368;&#20808;&#36827;&#27169;&#22411;&#26550;&#26500;&#22788;&#29702;CE&#30340;&#33021;&#21147;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#25351;&#26631;&#26469;&#25506;&#27979;&#27169;&#22411;&#20013;&#30001;CE&#24341;&#36215;&#30340;&#19978;&#19979;&#25991;&#26356;&#26032;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22522;&#20110;&#35821;&#35328;&#30340;&#27169;&#22411;&#33021;&#22815;&#32534;&#30721;&#31616;&#21333;&#30340;&#22810;&#27169;&#24577;&#35821;&#20041;&#20449;&#24687;&#24182;&#22788;&#29702;&#19968;&#20123;CE&#65292;&#29305;&#21035;&#26159;&#19982;&#23545;&#35805;&#21382;&#21490;&#30456;&#20851;&#30340;CE&#12290;&#32780;&#22810;&#27169;&#24577;&#27169;&#22411;&#21017;&#21487;&#20197;&#20351;&#29992;&#39069;&#22806;&#30340;&#23398;&#20064;&#30446;&#26631;&#26469;&#33719;&#21462;&#20998;&#35299;&#30340;&#23545;&#35937;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Referential ambiguities arise in dialogue when a referring expression does not uniquely identify the intended referent for the addressee. Addressees usually detect such ambiguities immediately and work with the speaker to repair it using meta-communicative, Clarificational Exchanges (CE): a Clarification Request (CR) and a response. Here, we argue that the ability to generate and respond to CRs imposes specific constraints on the architecture and objective functions of multi-modal, visually grounded dialogue models. We use the SIMMC 2.0 dataset to evaluate the ability of different state-of-the-art model architectures to process CEs, with a metric that probes the contextual updates that arise from them in the model. We find that language-based models are able to encode simple multi-modal semantic information and process some CEs, excelling with those related to the dialogue history, whilst multi-modal models can use additional learning objectives to obtain disentangled object representa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;Coq&#35777;&#26126;&#21161;&#25163;&#20013;&#30340;&#24402;&#32435;&#26500;&#36896;&#28436;&#31639;&#65288;CIC&#65289;&#20013;&#65292;&#21457;&#23637;&#20102;Oracle&#35745;&#31639;&#21487;&#34892;&#24615;&#21644;&#22270;&#28789;&#21487;&#24402;&#32422;&#30340;&#21512;&#25104;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#26426;&#22120;&#39564;&#35777;&#30340;&#26041;&#24335;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#36825;&#20026;&#22788;&#29702;&#39640;&#38454;oracle&#35745;&#31639;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#29992;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2307.15543</link><description>&lt;p&gt;
Oracle&#35745;&#31639;&#21487;&#34892;&#24615;&#21644;&#22270;&#28789;&#21487;&#24402;&#32422;&#22312;&#24402;&#32435;&#26500;&#36896;&#28436;&#31639;&#20013;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Oracle Computability and Turing Reducibility in the Calculus of Inductive Constructions. (arXiv:2307.15543v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;Coq&#35777;&#26126;&#21161;&#25163;&#20013;&#30340;&#24402;&#32435;&#26500;&#36896;&#28436;&#31639;&#65288;CIC&#65289;&#20013;&#65292;&#21457;&#23637;&#20102;Oracle&#35745;&#31639;&#21487;&#34892;&#24615;&#21644;&#22270;&#28789;&#21487;&#24402;&#32422;&#30340;&#21512;&#25104;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#26426;&#22120;&#39564;&#35777;&#30340;&#26041;&#24335;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#36825;&#20026;&#22788;&#29702;&#39640;&#38454;oracle&#35745;&#31639;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#29992;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;Coq&#35777;&#26126;&#21161;&#25163;&#20013;&#30340;&#24402;&#32435;&#26500;&#36896;&#28436;&#31639;&#65288;CIC&#65289;&#20013;&#65292;&#21457;&#23637;&#20102;Oracle&#35745;&#31639;&#21487;&#34892;&#24615;&#21644;&#22270;&#28789;&#21487;&#24402;&#32422;&#30340;&#21512;&#25104;&#27010;&#24565;&#12290;&#22312;&#21512;&#25104;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#20803;&#32423;&#20989;&#25968;&#32780;&#19981;&#26159;&#22522;&#20110;&#23545;&#35937;&#32423;&#35745;&#31639;&#27169;&#22411;&#30340;oracle&#35745;&#31639;&#23450;&#20041;&#65292;&#20381;&#36182;&#20110;&#26500;&#36896;&#31995;&#32479;&#20013;&#30340;&#21487;&#35745;&#31639;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#38750;&#24120;&#36866;&#21512;&#20110;&#26426;&#22120;&#39564;&#35777;&#30340;&#35777;&#26126;&#65292;&#25105;&#20204;&#22312;Coq&#20013;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#22312;&#25214;&#21040;&#21512;&#36866;&#30340;&#39640;&#38454;oracle&#35745;&#31639;&#30340;&#21512;&#25104;&#34920;&#31034;&#26102;&#23384;&#22312;&#19968;&#20123;&#32039;&#24352;&#20851;&#31995;&#12290;&#19968;&#26041;&#38754;&#65292;&#23427;&#24517;&#39035;&#36275;&#22815;&#20449;&#24687;&#20016;&#23500;&#20197;&#35777;&#26126;&#20013;&#24515;&#32467;&#26524;&#65292;&#30830;&#20445;&#25152;&#26377;&#27010;&#24565;&#37117;&#34987;&#24544;&#23454;&#22320;&#25429;&#25417;&#21040;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23427;&#24517;&#39035;&#21463;&#38480;&#20197;&#33719;&#21462;&#21512;&#25104;&#21487;&#35745;&#31639;&#24615;&#30340;&#20844;&#29702;&#30340;&#22909;&#22788;&#65292;&#36825;&#20123;&#20844;&#29702;&#36890;&#24120;&#28041;&#21450;&#19968;&#38454;&#23545;&#35937;&#12290;&#20511;&#37492;Andrey Bauer&#22522;&#20110;&#19968;&#38454;&#23545;&#35937;&#30340;&#23450;&#20041;&#65292;&#25105;&#20204;&#24471;&#21040;&#21551;&#21457;...
&lt;/p&gt;
&lt;p&gt;
We develop synthetic notions of oracle computability and Turing reducibility in the Calculus of Inductive Constructions (CIC), the constructive type theory underlying the Coq proof assistant. As usual in synthetic approaches, we employ a definition of oracle computations based on meta-level functions rather than object-level models of computation, relying on the fact that in constructive systems such as CIC all definable functions are computable by construction. Such an approach lends itself well to machine-checked proofs, which we carry out in Coq.  There is a tension in finding a good synthetic rendering of the higher-order notion of oracle computability. On the one hand, it has to be informative enough to prove central results, ensuring that all notions are faithfully captured. On the other hand, it has to be restricted enough to benefit from axioms for synthetic computability, which usually concern first-order objects. Drawing inspiration from a definition by Andrej Bauer based on 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#35814;&#32454;&#30340;&#35780;&#20272;&#26041;&#27861;&#21644;&#24230;&#37327;&#26631;&#20934;&#65292;&#23545;&#22686;&#37327;&#24207;&#21015;&#26631;&#27880;&#20013;&#30340;&#20462;&#35746;&#25919;&#31574;&#36827;&#34892;&#20102;&#30740;&#31350;&#21644;&#20998;&#26512;&#65292;&#24182;&#24212;&#29992;&#20110;Transformer&#32534;&#30721;&#22120;&#30340;&#34892;&#20026;&#20998;&#26512;&#65292;&#20026;&#26410;&#26469;&#30340;&#20462;&#35746;&#25919;&#31574;&#25552;&#20379;&#20102;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2307.15508</link><description>&lt;p&gt;
&#36136;&#37327;&#20043;&#36335;&#26159;&#30001;&#33391;&#22909;&#20462;&#35746;&#38138;&#24320;&#30340;&#65306;&#22686;&#37327;&#24207;&#21015;&#26631;&#27880;&#20013;&#20462;&#35746;&#25919;&#31574;&#30340;&#35814;&#32454;&#35780;&#20272;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
The Road to Quality is Paved with Good Revisions: A Detailed Evaluation Methodology for Revision Policies in Incremental Sequence Labelling. (arXiv:2307.15508v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15508
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#35814;&#32454;&#30340;&#35780;&#20272;&#26041;&#27861;&#21644;&#24230;&#37327;&#26631;&#20934;&#65292;&#23545;&#22686;&#37327;&#24207;&#21015;&#26631;&#27880;&#20013;&#30340;&#20462;&#35746;&#25919;&#31574;&#36827;&#34892;&#20102;&#30740;&#31350;&#21644;&#20998;&#26512;&#65292;&#24182;&#24212;&#29992;&#20110;Transformer&#32534;&#30721;&#22120;&#30340;&#34892;&#20026;&#20998;&#26512;&#65292;&#20026;&#26410;&#26469;&#30340;&#20462;&#35746;&#25919;&#31574;&#25552;&#20379;&#20102;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#37327;&#23545;&#35805;&#27169;&#22411;&#32452;&#20214;&#22522;&#20110;&#36755;&#20837;&#29983;&#25104;&#36755;&#20986;&#21069;&#32512;&#30340;&#24207;&#21015;&#12290;&#30001;&#20110;&#23616;&#37096;&#27495;&#20041;&#25110;&#38169;&#35823;&#30340;&#20551;&#35774;&#65292;&#21487;&#33021;&#20250;&#21457;&#29983;&#38169;&#35823;&#65292;&#22240;&#27492;&#20462;&#35746;&#36807;&#21435;&#30340;&#36755;&#20986;&#33021;&#21147;&#25104;&#20026;&#21487;&#31649;&#29702;&#30340;&#21487;&#21462;&#24615;&#36136;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#22686;&#37327;&#24207;&#21015;&#26631;&#27880;&#20013;&#30340;&#32534;&#36753;&#21644;&#20462;&#35746;&#36827;&#34892;&#24418;&#24335;&#21270;&#21644;&#29305;&#24449;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#35780;&#20272;&#20462;&#35746;&#25919;&#31574;&#30340;&#25351;&#26631;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#26469;&#20998;&#26512;&#19977;&#20010;&#22522;&#20110;Transformer&#30340;&#32534;&#30721;&#22120;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#22686;&#37327;&#34892;&#20026;&#65292;&#20026;&#26356;&#22909;&#30340;&#20462;&#35746;&#25919;&#31574;&#38138;&#24179;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incremental dialogue model components produce a sequence of output prefixes based on incoming input. Mistakes can occur due to local ambiguities or to wrong hypotheses, making the ability to revise past outputs a desirable property that can be governed by a policy. In this work, we formalise and characterise edits and revisions in incremental sequence labelling and propose metrics to evaluate revision policies. We then apply our methodology to profile the incremental behaviour of three Transformer-based encoders in various tasks, paving the road for better revision policies.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#26684;&#24335;&#19968;&#33268;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#32479;&#19968;&#25351;&#20196;&#35843;&#25972;&#65288;UIT&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#21160;&#26684;&#24335;&#36716;&#25442;&#26469;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;&#35813;&#30740;&#31350;&#24378;&#35843;&#20102;&#26684;&#24335;&#19968;&#33268;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.15504</link><description>&lt;p&gt;
&#25506;&#32034;&#25351;&#20196;&#35843;&#25972;&#30340;&#26684;&#24335;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploring Format Consistency for Instruction Tuning. (arXiv:2307.15504v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15504
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#26684;&#24335;&#19968;&#33268;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#32479;&#19968;&#25351;&#20196;&#35843;&#25972;&#65288;UIT&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#21160;&#26684;&#24335;&#36716;&#25442;&#26469;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;&#35813;&#30740;&#31350;&#24378;&#35843;&#20102;&#26684;&#24335;&#19968;&#33268;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36981;&#24490;&#20154;&#31867;&#25351;&#20196;&#33021;&#21147;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#20013;&#25351;&#20196;&#30340;&#22810;&#26679;&#24615;&#21644;&#25968;&#37327;&#21487;&#20197;&#25345;&#32493;&#25552;&#21319;&#27867;&#21270;&#24615;&#33021;&#65292;&#20174;&#32780;&#20419;&#36827;&#20102;&#26368;&#36817;&#30340;&#19968;&#39033;&#21162;&#21147;&#65292;&#21363;&#25910;&#38598;&#21508;&#31181;&#25351;&#20196;&#24182;&#23558;&#29616;&#26377;&#30340;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#25972;&#21512;&#21040;&#26356;&#22823;&#30340;&#38598;&#21512;&#20013;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#29992;&#25143;&#26377;&#20854;&#29420;&#29305;&#30340;&#34920;&#36798;&#25351;&#20196;&#30340;&#26041;&#24335;&#65292;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#36890;&#24120;&#23384;&#22312;&#25351;&#20196;&#39118;&#26684;&#21644;&#26684;&#24335;&#30340;&#21464;&#21270;&#65292;&#21363;&#26684;&#24335;&#19981;&#19968;&#33268;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26684;&#24335;&#19981;&#19968;&#33268;&#24615;&#22914;&#20309;&#24433;&#21709;&#25351;&#20196;&#35843;&#25972;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#32479;&#19968;&#25351;&#20196;&#35843;&#25972;&#8221;&#65288;UIT&#65289;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35843;&#29992;OpenAI&#30340;API&#23454;&#29616;&#22312;&#19981;&#21516;&#30340;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#33258;&#21160;&#26684;&#24335;&#36716;&#25442;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;UIT&#25104;&#21151;&#25552;&#39640;&#20102;&#22312;&#26410;&#35265;&#25351;&#20196;&#19978;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#24378;&#35843;&#20102;&#26684;&#24335;&#19968;&#33268;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction tuning has emerged as a promising approach to enhancing large language models in following human instructions. It is shown that increasing the diversity and number of instructions in the training data can consistently enhance generalization performance, which facilitates a recent endeavor to collect various instructions and integrate existing instruction tuning datasets into larger collections. However, different users have their unique ways of expressing instructions, and there often exist variations across different datasets in the instruction styles and formats, i.e., format inconsistency. In this work, we study how format inconsistency may impact the performance of instruction tuning. We propose a framework called "Unified Instruction Tuning" (UIT), which calls OpenAI APIs for automatic format transfer among different instruction tuning datasets. We show that UIT successfully improves the generalization performance on unseen instructions, which highlights the importance
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ETHER&#65292;&#36890;&#36807;&#23545;&#40784;&#32039;&#24613;&#27807;&#36890;&#26469;&#35299;&#20915;&#22238;&#39038;&#24615;&#32463;&#39564;&#37325;&#28436;&#20013;&#30340;&#38382;&#39064;&#65292;&#20811;&#26381;&#20102;&#20808;&#21069;&#26550;&#26500;&#20381;&#36182;&#39044;&#35774;&#20989;&#25968;&#30340;&#38480;&#21046;&#65292;&#24182;&#25552;&#39640;&#20102;&#25968;&#25454;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.15494</link><description>&lt;p&gt;
ETHER: &#23545;&#20110;&#22238;&#39038;&#24615;&#32463;&#39564;&#37325;&#28436;&#30340;&#32039;&#23494;&#27807;&#36890;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
ETHER: Aligning Emergent Communication for Hindsight Experience Replay. (arXiv:2307.15494v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ETHER&#65292;&#36890;&#36807;&#23545;&#40784;&#32039;&#24613;&#27807;&#36890;&#26469;&#35299;&#20915;&#22238;&#39038;&#24615;&#32463;&#39564;&#37325;&#28436;&#20013;&#30340;&#38382;&#39064;&#65292;&#20811;&#26381;&#20102;&#20808;&#21069;&#26550;&#26500;&#20381;&#36182;&#39044;&#35774;&#20989;&#25968;&#30340;&#38480;&#21046;&#65292;&#24182;&#25552;&#39640;&#20102;&#25968;&#25454;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#30340;&#36319;&#38543;&#23545;&#20110;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#21644;&#20154;&#31867;&#20043;&#38388;&#30340;&#21512;&#20316;&#33267;&#20851;&#37325;&#35201;&#12290;&#33258;&#28982;&#35821;&#35328;&#26465;&#20214;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#23637;&#31034;&#20102;&#33258;&#28982;&#35821;&#35328;&#30340;&#29305;&#24615;&#65292;&#22914;&#32452;&#21512;&#24615;&#65292;&#33021;&#22815;&#25552;&#20379;&#23398;&#20064;&#22797;&#26434;&#31574;&#30053;&#30340;&#24378;&#24402;&#32435;&#20559;&#22909;&#12290;&#20808;&#21069;&#30340;&#26550;&#26500;&#22914;HIGhER&#32467;&#21512;&#20102;&#35821;&#35328;&#26465;&#20214;&#19982;&#22238;&#39038;&#24615;&#32463;&#39564;&#37325;&#28436;&#65288;HER&#65289;&#26469;&#22788;&#29702;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#12290;&#28982;&#32780;&#65292;&#19982;HER&#31867;&#20284;&#65292;HIGhER&#20381;&#36182;&#20110;&#19968;&#20010;&#39044;&#35774;&#30340;&#20989;&#25968;&#26469;&#25552;&#20379;&#21453;&#39304;&#20449;&#21495;&#65292;&#25351;&#31034;&#21738;&#31181;&#35821;&#35328;&#25551;&#36848;&#22312;&#21738;&#31181;&#29366;&#24577;&#19979;&#26377;&#25928;&#12290;&#36825;&#31181;&#20381;&#36182;&#20110;&#39044;&#35774;&#20989;&#25968;&#30340;&#38480;&#21046;&#38480;&#21046;&#20102;&#20854;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;HIGhER&#21482;&#21033;&#29992;&#25104;&#21151;&#30340;&#24378;&#21270;&#23398;&#20064;&#36712;&#36857;&#20013;&#21253;&#21547;&#30340;&#35821;&#35328;&#20449;&#24687;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#20854;&#26368;&#32456;&#24615;&#33021;&#21644;&#25968;&#25454;&#25928;&#29575;&#12290;&#27809;&#26377;&#26089;&#26399;&#25104;&#21151;&#36712;&#36857;&#65292;HIGhER&#24182;&#19981;&#27604;&#20854;&#26500;&#24314;&#20110;&#20043;&#19978;&#30340;DQN&#26356;&#22909;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32039;&#23494;&#25991;&#26412;&#22238;&#39038;&#24615;&#32463;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language instruction following is paramount to enable collaboration between artificial agents and human beings. Natural language-conditioned reinforcement learning (RL) agents have shown how natural languages' properties, such as compositionality, can provide a strong inductive bias to learn complex policies. Previous architectures like HIGhER combine the benefit of language-conditioning with Hindsight Experience Replay (HER) to deal with sparse rewards environments. Yet, like HER, HIGhER relies on an oracle predicate function to provide a feedback signal highlighting which linguistic description is valid for which state. This reliance on an oracle limits its application. Additionally, HIGhER only leverages the linguistic information contained in successful RL trajectories, thus hurting its final performance and data-efficiency. Without early successful trajectories, HIGhER is no better than DQN upon which it is built. In this paper, we propose the Emergent Textual Hindsight Ex
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#26102;&#24207;&#21644;&#37325;&#21472;&#38382;&#39064;&#23545;&#35821;&#38899;&#35782;&#21035;&#21644;&#24847;&#22270;&#35782;&#21035;&#30340;&#24433;&#21709;&#65292;&#35780;&#20272;&#20102;5&#20010;&#20027;&#35201;&#21830;&#19994;ASR&#31995;&#32479;&#30340;&#23545;&#35805;&#21644;&#22810;&#35821;&#35328;&#25903;&#25345;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#35789;&#38169;&#35823;&#29575;&#21644;&#37325;&#21472;&#20173;&#28982;&#26159;&#20851;&#38190;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.15493</link><description>&lt;p&gt;
&#26102;&#24207;&#29942;&#39048;&#65306;&#20026;&#20160;&#20040;&#26102;&#24207;&#21644;&#37325;&#21472;&#23545;&#35805;&#31995;&#32479;&#12289;&#35821;&#38899;&#35782;&#21035;&#21644;&#23545;&#35805;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;
&lt;/p&gt;
&lt;p&gt;
The timing bottleneck: Why timing and overlap are mission-critical for conversational user interfaces, speech recognition and dialogue systems. (arXiv:2307.15493v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15493
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#26102;&#24207;&#21644;&#37325;&#21472;&#38382;&#39064;&#23545;&#35821;&#38899;&#35782;&#21035;&#21644;&#24847;&#22270;&#35782;&#21035;&#30340;&#24433;&#21709;&#65292;&#35780;&#20272;&#20102;5&#20010;&#20027;&#35201;&#21830;&#19994;ASR&#31995;&#32479;&#30340;&#23545;&#35805;&#21644;&#22810;&#35821;&#35328;&#25903;&#25345;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#35789;&#38169;&#35823;&#29575;&#21644;&#37325;&#21472;&#20173;&#28982;&#26159;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#26159;&#35821;&#38899;&#39537;&#21160;&#30340;&#20154;&#26426;&#20132;&#20114;&#30340;&#20851;&#38190;&#20013;&#38388;&#20214;&#12290;&#23613;&#31649;&#35821;&#38899;&#35782;&#21035;&#22312;&#26080;&#27745;&#26579;&#30340;&#21333;&#21521;&#38899;&#39057;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#24320;&#25918;&#24335;&#20114;&#21160;&#29615;&#22659;&#20013;&#30340;&#30495;&#23454;&#24212;&#29992;&#22330;&#26223;&#20173;&#28982;&#23384;&#22312;&#35768;&#22810;&#25361;&#25112;&#12290;&#25105;&#20204;&#35748;&#20026;&#26102;&#24207;&#23545;&#23545;&#35805;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#35780;&#20272;&#20102;5&#20010;&#20027;&#35201;&#21830;&#19994;ASR&#31995;&#32479;&#22312;&#23545;&#35805;&#21644;&#22810;&#35821;&#35328;&#25903;&#25345;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#21457;&#29616;6&#31181;&#35821;&#35328;&#30340;&#33258;&#28982;&#23545;&#35805;&#25968;&#25454;&#30340;&#35789;&#38169;&#35823;&#29575;&#20173;&#28982;&#24456;&#39640;&#65292;&#24182;&#19988;&#37325;&#21472;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65288;&#30740;&#31350;1&#65289;&#12290;&#36825;&#23588;&#20854;&#24433;&#21709;&#23545;&#35805;&#35789;&#30340;&#35782;&#21035;&#65288;&#30740;&#31350;2&#65289;&#65292;&#36827;&#32780;&#23545;&#21518;&#32493;&#30340;&#24847;&#22270;&#35782;&#21035;&#20135;&#29983;&#20005;&#37325;&#21518;&#26524;&#65288;&#30740;&#31350;3&#65289;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26377;&#21161;&#20110;&#35780;&#20272;&#24403;&#21069;&#23545;&#35805;ASR&#30340;&#29366;&#24577;&#65292;&#20026;&#22810;&#32500;&#24230;&#38169;&#35823;&#20998;&#26512;&#21644;&#35780;&#20272;&#20570;&#20986;&#36129;&#29486;&#65292;&#24182;&#30830;&#23450;&#26368;&#38656;&#35201;&#20851;&#27880;&#30340;&#29616;&#35937;&#20197;&#26500;&#24314;&#24378;&#22823;&#30340;&#20132;&#20114;&#24335;&#35821;&#38899;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech recognition systems are a key intermediary in voice-driven human-computer interaction. Although speech recognition works well for pristine monologic audio, real-life use cases in open-ended interactive settings still present many challenges. We argue that timing is mission-critical for dialogue systems, and evaluate 5 major commercial ASR systems for their conversational and multilingual support. We find that word error rates for natural conversational data in 6 languages remain abysmal, and that overlap remains a key challenge (study 1). This impacts especially the recognition of conversational words (study 2), and in turn has dire consequences for downstream intent recognition (study 3). Our findings help to evaluate the current state of conversational ASR, contribute towards multidimensional error analysis and evaluation, and identify phenomena that need most attention on the way to build robust interactive speech technologies.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#35821;&#38899;&#21512;&#25104;&#26041;&#27861;&#26469;&#35299;&#20915;&#33258;&#22238;&#24402;&#21644;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#20013;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#35821;&#20041;&#32534;&#30721;&#26041;&#38754;&#36827;&#34892;&#20102;&#27604;&#36739;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.15484</link><description>&lt;p&gt;
&#29992;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#21644;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26368;&#23567;&#30417;&#30563;&#35821;&#38899;&#21512;&#25104;&#65306;&#22522;&#20110;&#35821;&#20041;&#32534;&#30721;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Minimally-Supervised Speech Synthesis with Conditional Diffusion Model and Language Model: A Comparative Study of Semantic Coding. (arXiv:2307.15484v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15484
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#35821;&#38899;&#21512;&#25104;&#26041;&#27861;&#26469;&#35299;&#20915;&#33258;&#22238;&#24402;&#21644;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#20013;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#35821;&#20041;&#32534;&#30721;&#26041;&#38754;&#36827;&#34892;&#20102;&#27604;&#36739;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23545;&#20110;&#33021;&#22815;&#37319;&#29992;&#26368;&#23567;&#30417;&#30563;&#35757;&#32451;&#26041;&#27861;&#30340;&#25991;&#26412;&#21040;&#35821;&#38899;(TTS)&#25216;&#26415;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#20004;&#31181;&#31163;&#25955;&#35821;&#38899;&#34920;&#31034;&#24182;&#20351;&#29992;&#20004;&#31181;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#26469;&#35299;&#32806;TTS&#12290;&#20026;&#20102;&#35299;&#20915;&#31163;&#25955;&#34920;&#31034;&#20013;&#30340;&#39640;&#32500;&#24230;&#21644;&#27874;&#24418;&#22833;&#30495;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Diff-LM-Speech&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#23558;&#35821;&#20041;&#23884;&#20837;&#27169;&#22411;&#20026;&#22522;&#20110;mel&#39057;&#35889;&#22270;&#65292;&#24182;&#24341;&#20837;&#22522;&#20110;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#38901;&#24459;&#29942;&#39048;&#30340;&#25552;&#31034;&#32534;&#30721;&#32467;&#26500;&#65292;&#20197;&#25552;&#39640;&#25552;&#31034;&#34920;&#31034;&#33021;&#21147;&#12290;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#24120;&#24120;&#36935;&#21040;&#32570;&#22833;&#21644;&#37325;&#22797;&#21333;&#35789;&#30340;&#38382;&#39064;&#65292;&#32780;&#38750;&#33258;&#22238;&#24402;&#26694;&#26550;&#30001;&#20110;&#39044;&#27979;&#27169;&#22411;&#30340;&#23384;&#22312;&#23548;&#33268;&#34920;&#36798;&#24179;&#22343;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Tetra-Diff-Speech&#65292;&#35813;&#26041;&#27861;&#35774;&#35745;&#20102;&#19968;&#20010;&#26102;&#38271;&#25193;&#25955;&#27169;&#22411;&#20197;&#23454;&#29616;&#22810;&#26679;&#21270;&#30340;&#38901;&#24459;&#34920;&#36798;&#12290;&#25105;&#20204;&#26399;&#26395;&#35821;&#20041;&#32534;&#30721;&#30340;&#20449;&#24687;&#20869;&#23481;&#20171;&#20110;...
&lt;/p&gt;
&lt;p&gt;
Recently, there has been a growing interest in text-to-speech (TTS) methods that can be trained with minimal supervision by combining two types of discrete speech representations and using two sequence-to-sequence tasks to decouple TTS. To address the challenges associated with high dimensionality and waveform distortion in discrete representations, we propose Diff-LM-Speech, which models semantic embeddings into mel-spectrogram based on diffusion models and introduces a prompt encoder structure based on variational autoencoders and prosody bottlenecks to improve prompt representation capabilities. Autoregressive language models often suffer from missing and repeated words, while non-autoregressive frameworks face expression averaging problems due to duration prediction models. To address these issues, we propose Tetra-Diff-Speech, which designs a duration diffusion model to achieve diverse prosodic expressions. While we expect the information content of semantic coding to be between t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#27169;&#24577;&#27010;&#24565;&#23398;&#20064;&#21644;&#25512;&#29702;&#65288;CCLI&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;CLIP&#30340;&#25991;&#26412;-&#22270;&#20687;&#30456;&#20851;&#24615;&#33021;&#21147;&#65292;&#20174;&#22270;&#20687;&#20013;&#33258;&#21160;&#23398;&#20064;&#19968;&#32452;&#29420;&#29305;&#30340;&#35270;&#35273;&#27010;&#24565;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#27010;&#24565;&#26500;&#24314;&#20102;&#22270;&#20687;&#30340;&#21028;&#21035;&#24335;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2307.15460</link><description>&lt;p&gt;
&#36328;&#27169;&#24577;&#27010;&#24565;&#23398;&#20064;&#21644;&#25512;&#29702;&#29992;&#20110;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Cross-Modal Concept Learning and Inference for Vision-Language Models. (arXiv:2307.15460v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15460
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#27169;&#24577;&#27010;&#24565;&#23398;&#20064;&#21644;&#25512;&#29702;&#65288;CCLI&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;CLIP&#30340;&#25991;&#26412;-&#22270;&#20687;&#30456;&#20851;&#24615;&#33021;&#21147;&#65292;&#20174;&#22270;&#20687;&#20013;&#33258;&#21160;&#23398;&#20064;&#19968;&#32452;&#29420;&#29305;&#30340;&#35270;&#35273;&#27010;&#24565;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#27010;&#24565;&#26500;&#24314;&#20102;&#22270;&#20687;&#30340;&#21028;&#21035;&#24335;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#65292;&#22914;CLIP&#65292;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#36890;&#36807;&#24494;&#35843;&#24314;&#31435;&#20102;&#25991;&#26412;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#22312;&#29616;&#26377;&#30340;&#24494;&#35843;&#26041;&#27861;&#20013;&#65292;&#23558;&#29305;&#23450;&#31867;&#21035;&#30340;&#25991;&#26412;&#25551;&#36848;&#19982;&#25972;&#20010;&#22270;&#20687;&#36827;&#34892;&#21305;&#37197;&#12290;&#25105;&#20204;&#35748;&#35782;&#21040;&#25972;&#20010;&#22270;&#20687;&#21305;&#37197;&#24182;&#19981;&#26377;&#25928;&#65292;&#22240;&#20026;&#21516;&#19968;&#31867;&#21035;&#30340;&#22270;&#20687;&#36890;&#24120;&#21253;&#21547;&#19968;&#32452;&#19981;&#21516;&#30340;&#35821;&#20041;&#23545;&#35937;&#65292;&#32780;&#23545;&#35937;&#21448;&#21253;&#21547;&#19968;&#32452;&#35821;&#20041;&#37096;&#20998;&#25110;&#27010;&#24565;&#12290;&#20010;&#21035;&#30340;&#35821;&#20041;&#37096;&#20998;&#25110;&#27010;&#24565;&#21487;&#33021;&#20986;&#29616;&#22312;&#19981;&#21516;&#31867;&#21035;&#30340;&#22270;&#20687;&#26679;&#26412;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#36328;&#27169;&#24577;&#27010;&#24565;&#23398;&#20064;&#21644;&#25512;&#29702;&#65288;CCLI&#65289;&#12290;&#21033;&#29992;CLIP&#30340;&#24378;&#22823;&#30340;&#25991;&#26412;-&#22270;&#20687;&#30456;&#20851;&#24615;&#33021;&#21147;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#19968;&#32452;&#35821;&#20041;&#25991;&#26412;&#27010;&#24565;&#20174;&#22270;&#20687;&#20013;&#33258;&#21160;&#23398;&#20064;&#19968;&#32452;&#29420;&#29305;&#30340;&#35270;&#35273;&#27010;&#24565;&#12290;&#22522;&#20110;&#36825;&#20123;&#35270;&#35273;&#27010;&#24565;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#22270;&#20687;&#30340;&#21028;&#21035;&#24335;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale pre-trained Vision-Language Models (VLMs), such as CLIP, establish the correlation between texts and images, achieving remarkable success on various downstream tasks with fine-tuning. In existing fine-tuning methods, the class-specific text description is matched against the whole image. We recognize that this whole image matching is not effective since images from the same class often contain a set of different semantic objects, and an object further consists of a set of semantic parts or concepts. Individual semantic parts or concepts may appear in image samples from different classes. To address this issue, in this paper, we develop a new method called cross-model concept learning and inference (CCLI). Using the powerful text-image correlation capability of CLIP, our method automatically learns a large set of distinctive visual concepts from images using a set of semantic text concepts. Based on these visual concepts, we construct a discriminative representation of image
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Trie&#19978;&#19979;&#25991;&#22686;&#24378;&#30340;&#20010;&#24615;&#21270;&#26597;&#35810;&#33258;&#21160;&#34917;&#20840;&#31639;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#30701;&#21069;&#32512;&#21644;&#26410;&#35265;&#21069;&#32512;&#30340;&#38382;&#39064;&#65292;&#24182;&#26377;&#25928;&#21033;&#29992;&#21382;&#21490;&#26597;&#35810;&#30340;&#27969;&#34892;&#24230;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2307.15455</link><description>&lt;p&gt;
&#22522;&#20110;Trie&#19978;&#19979;&#25991;&#22686;&#24378;&#30340;&#20010;&#24615;&#21270;&#26597;&#35810;&#33258;&#21160;&#34917;&#20840;&#31639;&#27861;&#65292;&#20197;&#25552;&#21319;&#23545;&#30701;&#21069;&#32512;&#21644;&#26410;&#35265;&#21069;&#32512;&#30340;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
Trie-NLG: Trie Context Augmentation to Improve Personalized Query Auto-Completion for Short and Unseen Prefixes. (arXiv:2307.15455v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15455
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Trie&#19978;&#19979;&#25991;&#22686;&#24378;&#30340;&#20010;&#24615;&#21270;&#26597;&#35810;&#33258;&#21160;&#34917;&#20840;&#31639;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#30701;&#21069;&#32512;&#21644;&#26410;&#35265;&#21069;&#32512;&#30340;&#38382;&#39064;&#65292;&#24182;&#26377;&#25928;&#21033;&#29992;&#21382;&#21490;&#26597;&#35810;&#30340;&#27969;&#34892;&#24230;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26597;&#35810;&#33258;&#21160;&#34917;&#20840;(QAC)&#26088;&#22312;&#20026;&#32473;&#23450;&#30340;&#26597;&#35810;&#21069;&#32512;&#25552;&#20379;&#21512;&#29702;&#30340;&#34917;&#20840;&#24314;&#35758;&#12290;&#20256;&#32479;&#30340;QAC&#31995;&#32479;&#21033;&#29992;&#21382;&#21490;&#26597;&#35810;&#26085;&#24535;&#20013;&#30340;Trie&#25968;&#25454;&#32467;&#26500;&#26469;&#25552;&#20379;&#26368;&#21463;&#27426;&#36814;&#30340;&#34917;&#20840;&#24314;&#35758;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20219;&#20309;QAC&#31995;&#32479;&#26469;&#35828;&#65292;&#26377;&#20004;&#31181;&#29305;&#23450;&#30340;&#22330;&#26223;&#24456;&#38590;&#22788;&#29702;&#65306;&#30701;&#21069;&#32512;(&#26412;&#36136;&#19978;&#23384;&#22312;&#27495;&#20041;)&#21644;&#26410;&#35265;&#21069;&#32512;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#20010;&#24615;&#21270;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;(NLG)&#27169;&#22411;&#65292;&#21033;&#29992;&#21069;&#19968;&#20010;&#20250;&#35805;&#30340;&#26597;&#35810;&#20316;&#20026;&#19978;&#19979;&#25991;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;NLG&#27169;&#22411;&#23384;&#22312;&#20004;&#20010;&#32570;&#28857;&#65306;(1)&#21069;&#36848;&#20250;&#35805;&#26597;&#35810;&#21487;&#33021;&#19982;&#24403;&#21069;&#21069;&#32512;&#30340;&#29992;&#25143;&#24847;&#22270;&#26080;&#20851;&#19988;&#21253;&#21547;&#22122;&#22768;&#65307;(2)NLG&#27169;&#22411;&#26080;&#27861;&#30452;&#25509;&#34701;&#21512;&#21382;&#21490;&#26597;&#35810;&#30340;&#27969;&#34892;&#24230;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;QAC&#31639;&#27861;Trie-NLG&#65292;&#35813;&#31639;&#27861;&#21516;&#26102;&#21033;&#29992;Trie&#20013;&#30340;&#27969;&#34892;&#24230;&#20449;&#24687;&#21644;&#21069;&#19968;&#20010;&#20250;&#35805;&#26597;&#35810;&#20013;&#30340;&#20010;&#24615;&#21270;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Query auto-completion (QAC) aims at suggesting plausible completions for a given query prefix. Traditionally, QAC systems have leveraged tries curated from historical query logs to suggest most popular completions. In this context, there are two specific scenarios that are difficult to handle for any QAC system: short prefixes (which are inherently ambiguous) and unseen prefixes. Recently, personalized Natural Language Generation (NLG) models have been proposed to leverage previous session queries as context for addressing these two challenges. However, such NLG models suffer from two drawbacks: (1) some of the previous session queries could be noisy and irrelevant to the user intent for the current prefix, and (2) NLG models cannot directly incorporate historical query popularity. This motivates us to propose a novel NLG model for QAC, Trie-NLG, which jointly leverages popularity signals from trie and personalization signals from previous session queries. We train the Trie-NLG model b
&lt;/p&gt;</description></item><item><title>CompLog&#26159;&#19968;&#31181;&#22522;&#20110;&#22797;&#26434;&#24615;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#36890;&#36807;&#35745;&#31639;Kolmogorov&#22797;&#26434;&#24615;&#26367;&#20195;&#27010;&#29575;&#25512;&#29702;&#65292;&#23454;&#29616;&#35745;&#31639;&#26576;&#31181;&#24773;&#20917;&#24847;&#22806;&#24615;&#30340;&#24230;&#37327;&#65292;&#24182;&#36890;&#36807;&#35268;&#33539;&#30340;&#19990;&#30028;&#21644;&#24515;&#26234;&#27169;&#22411;&#30340;&#25551;&#36848;&#29983;&#25104;&#30456;&#20851;&#25551;&#36848;&#65292;&#24182;&#25552;&#20379;&#23545;&#26512;&#21462;&#21644;&#21542;&#23450;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.15453</link><description>&lt;p&gt;
&#20174;&#27010;&#29575;&#32534;&#31243;&#21040;&#22522;&#20110;&#22797;&#26434;&#24615;&#30340;&#32534;&#31243;
&lt;/p&gt;
&lt;p&gt;
From Probabilistic Programming to Complexity-based Programming. (arXiv:2307.15453v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15453
&lt;/p&gt;
&lt;p&gt;
CompLog&#26159;&#19968;&#31181;&#22522;&#20110;&#22797;&#26434;&#24615;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#36890;&#36807;&#35745;&#31639;Kolmogorov&#22797;&#26434;&#24615;&#26367;&#20195;&#27010;&#29575;&#25512;&#29702;&#65292;&#23454;&#29616;&#35745;&#31639;&#26576;&#31181;&#24773;&#20917;&#24847;&#22806;&#24615;&#30340;&#24230;&#37327;&#65292;&#24182;&#36890;&#36807;&#35268;&#33539;&#30340;&#19990;&#30028;&#21644;&#24515;&#26234;&#27169;&#22411;&#30340;&#25551;&#36848;&#29983;&#25104;&#30456;&#20851;&#25551;&#36848;&#65292;&#24182;&#25552;&#20379;&#23545;&#26512;&#21462;&#21644;&#21542;&#23450;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CompLog&#30340;&#26032;&#22411;&#35745;&#31639;&#26694;&#26550;&#30340;&#20027;&#35201;&#29305;&#28857;&#21644;&#21021;&#27493;&#23454;&#29616;&#12290;CompLog&#20511;&#37492;&#20102;&#27010;&#29575;&#32534;&#31243;&#31995;&#32479;&#65288;&#22914;ProbLog&#65289;&#30340;&#25512;&#29702;&#26426;&#21046;&#65292;&#24182;&#22522;&#20110;Simplicity&#29702;&#35770;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25512;&#29702;&#26426;&#21046;&#65292;&#36890;&#36807;ASP&#31243;&#24207;&#30340;min-path&#25628;&#32034;&#35745;&#31639;&#20004;&#31181;Kolmogorov&#22797;&#26434;&#24615;&#65292;&#32780;&#19981;&#26159;&#27010;&#29575;&#25512;&#29702;&#12290;&#35813;&#31995;&#32479;&#20351;&#29992;&#25143;&#33021;&#22815;&#35745;&#31639;&#26576;&#20010;&#24773;&#20917;&#24847;&#22806;&#24615;&#30340;ex-post&#21644;ex-ante&#24230;&#37327;&#65292;&#20998;&#21035;&#23545;&#24212;&#20110;&#21518;&#39564;&#21644;&#20808;&#39564;&#20027;&#35266;&#27010;&#29575;&#12290;&#35745;&#31639;&#22522;&#20110;&#36890;&#36807;&#25551;&#36848;&#24615;&#35859;&#35789;&#20043;&#38388;&#30340;&#22240;&#26524;&#21644;&#25551;&#36848;&#24615;&#20851;&#31995;&#21152;&#26435;&#30340;&#19990;&#30028;&#21644;&#24515;&#26234;&#27169;&#22411;&#30340;&#35268;&#33539;&#12290;&#26412;&#25991;&#36824;&#38416;&#36848;&#20102;&#20960;&#20010;&#24212;&#29992;&#31034;&#20363;&#65306;&#29983;&#25104;&#30456;&#20851;&#25551;&#36848;&#65292;&#24182;&#25552;&#20379;&#23545;&#26512;&#21462;&#21644;&#21542;&#23450;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper presents the main characteristics and a preliminary implementation of a novel computational framework named CompLog. Inspired by probabilistic programming systems like ProbLog, CompLog builds upon the inferential mechanisms proposed by Simplicity Theory, relying on the computation of two Kolmogorov complexities (here implemented as min-path searches via ASP programs) rather than probabilistic inference. The proposed system enables users to compute ex-post and ex-ante measures of unexpectedness of a certain situation, mapping respectively to posterior and prior subjective probabilities. The computation is based on the specification of world and mental models by means of causal and descriptive relations between predicates weighted by complexity. The paper illustrates a few examples of application: generating relevant descriptions, and providing alternative approaches to disjunction and to negation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#24773;&#32490;&#36716;&#31227;&#24863;&#30693;&#30340;&#36328;&#27169;&#24577;&#34701;&#21512;&#32593;&#32476;&#65288;CFN-ESA&#65289;&#29992;&#20110;&#23545;&#35805;&#24773;&#32490;&#35782;&#21035;&#65292;&#36890;&#36807;&#23558;&#25991;&#26412;&#27169;&#24577;&#20316;&#20026;&#20027;&#35201;&#24773;&#24863;&#20449;&#24687;&#30340;&#26469;&#28304;&#65292;&#35270;&#35273;&#21644;&#22768;&#23398;&#27169;&#24577;&#20316;&#20026;&#27425;&#35201;&#20449;&#24687;&#30340;&#26469;&#28304;&#65292;&#24182;&#24341;&#20837;&#24773;&#32490;&#36716;&#31227;&#27169;&#22359;&#26469;&#35299;&#20915;&#24773;&#32490;&#36716;&#31227;&#22330;&#26223;&#19979;&#24773;&#24863;&#35782;&#21035;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.15432</link><description>&lt;p&gt;
CFN-ESA&#65306;&#19968;&#31181;&#20855;&#26377;&#24773;&#32490;&#36716;&#31227;&#24863;&#30693;&#30340;&#36328;&#27169;&#24577;&#34701;&#21512;&#32593;&#32476;&#29992;&#20110;&#23545;&#35805;&#24773;&#32490;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
CFN-ESA: A Cross-Modal Fusion Network with Emotion-Shift Awareness for Dialogue Emotion Recognition. (arXiv:2307.15432v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15432
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#24773;&#32490;&#36716;&#31227;&#24863;&#30693;&#30340;&#36328;&#27169;&#24577;&#34701;&#21512;&#32593;&#32476;&#65288;CFN-ESA&#65289;&#29992;&#20110;&#23545;&#35805;&#24773;&#32490;&#35782;&#21035;&#65292;&#36890;&#36807;&#23558;&#25991;&#26412;&#27169;&#24577;&#20316;&#20026;&#20027;&#35201;&#24773;&#24863;&#20449;&#24687;&#30340;&#26469;&#28304;&#65292;&#35270;&#35273;&#21644;&#22768;&#23398;&#27169;&#24577;&#20316;&#20026;&#27425;&#35201;&#20449;&#24687;&#30340;&#26469;&#28304;&#65292;&#24182;&#24341;&#20837;&#24773;&#32490;&#36716;&#31227;&#27169;&#22359;&#26469;&#35299;&#20915;&#24773;&#32490;&#36716;&#31227;&#22330;&#26223;&#19979;&#24773;&#24863;&#35782;&#21035;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23545;&#35805;&#24773;&#32490;&#35782;&#21035;&#26041;&#38754;&#65292;&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#21463;&#21040;&#20102;&#21508;&#39046;&#22495;&#30740;&#31350;&#30028;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#24773;&#32490;&#36716;&#31227;&#24863;&#30693;&#30340;&#36328;&#27169;&#24577;&#34701;&#21512;&#32593;&#32476;&#65288;CFN-ESA&#65289;&#29992;&#20110;&#23545;&#35805;&#24773;&#32490;&#35782;&#21035;&#12290;&#29616;&#26377;&#26041;&#27861;&#22343;&#24179;&#31561;&#22320;&#20351;&#29992;&#27599;&#20010;&#27169;&#24577;&#32780;&#26080;&#27861;&#21306;&#20998;&#24773;&#24863;&#20449;&#24687;&#30340;&#22810;&#23569;&#65292;&#20174;&#32780;&#38590;&#20197;&#20805;&#20998;&#25552;&#21462;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#30340;&#20114;&#34917;&#21644;&#20851;&#32852;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;CFN-ESA&#20013;&#65292;&#25991;&#26412;&#27169;&#24577;&#34987;&#35270;&#20026;&#24773;&#24863;&#20449;&#24687;&#30340;&#20027;&#35201;&#26469;&#28304;&#65292;&#32780;&#35270;&#35273;&#21644;&#22768;&#23398;&#27169;&#24577;&#21017;&#34987;&#35270;&#20026;&#27425;&#35201;&#26469;&#28304;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#27169;&#22411;&#24573;&#35270;&#20102;&#24773;&#32490;&#36716;&#31227;&#20449;&#24687;&#65292;&#36807;&#24230;&#20851;&#27880;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#23548;&#33268;&#22312;&#24773;&#32490;&#36716;&#31227;&#22330;&#26223;&#19979;&#24773;&#24863;&#35782;&#21035;&#22833;&#36133;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#24773;&#32490;&#36716;&#31227;&#27169;&#22359;&#26469;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#12290;CFN-ESA&#20027;&#35201;&#21253;&#25324;&#21333;&#27169;&#24577;&#32534;&#30721;&#22120;&#65288;RUME&#65289;&#12289;&#36328;&#27169;&#24577;&#32534;&#30721;&#22120;&#65288;ACME&#65289;&#21644;&#24773;&#32490;&#36716;&#31227;&#27169;&#22359;&#65288;LESM&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Emotion Recognition in Conversation (ERC) has garnered growing attention from research communities in various fields. In this paper, we propose a cross-modal fusion network with emotion-shift awareness (CFN-ESA) for ERC. Extant approaches employ each modality equally without distinguishing the amount of emotional information, rendering it hard to adequately extract complementary and associative information from multimodal data. To cope with this problem, in CFN-ESA, textual modalities are treated as the primary source of emotional information, while visual and acoustic modalities are taken as the secondary sources. Besides, most multimodal ERC models ignore emotion-shift information and overfocus on contextual information, leading to the failure of emotion recognition under emotion-shift scenario. We elaborate an emotion-shift module to address this challenge. CFN-ESA mainly consists of the unimodal encoder (RUME), cross-modal encoder (ACME), and emotion-shift module (LESM).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19987;&#19994;&#32534;&#35793;&#35821;&#35328;&#27169;&#22411;&#21644;&#36890;&#29992;&#27169;&#22411;&#22312;&#26816;&#27979;&#21487;&#25345;&#32493;&#21457;&#23637;&#30446;&#26631;&#26041;&#38754;&#30340;&#27604;&#25928;&#29575;&#65292;&#20851;&#27880;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#30340;&#20559;&#35265;&#21644;&#25935;&#24863;&#24615;&#25361;&#25112;&#12290;&#30740;&#31350;&#24378;&#35843;&#20102;&#19987;&#19994;&#22521;&#35757;&#23545;&#20110;&#31934;&#30830;&#26080;&#20559;&#30340;&#20998;&#26512;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#36890;&#36807;&#23545;&#19968;&#20010;&#20844;&#21496;&#25551;&#36848;&#25968;&#25454;&#38598;&#30340;&#26696;&#20363;&#30740;&#31350;&#23545;&#27604;&#20102;GPT-3.5&#21644;&#19987;&#19994;SDG&#26816;&#27979;&#27169;&#22411;&#30340;&#24046;&#24322;&#12290;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#22312;&#27169;&#22411;&#36873;&#25321;&#26102;&#38656;&#35201;&#32771;&#34385;&#20219;&#21153;&#35201;&#27714;&#12289;&#25104;&#26412;&#12289;&#22797;&#26434;&#24615;&#21644;&#36879;&#26126;&#24230;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#21151;&#33021;&#24615;&#65292;&#20294;&#20316;&#32773;&#24314;&#35758;&#22312;&#38656;&#35201;&#31934;&#30830;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#20219;&#21153;&#20013;&#20351;&#29992;&#19987;&#19994;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.15425</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#38190;&#35780;&#35770;&#65306;&#25935;&#24863;&#24615;&#12289;&#20559;&#35265;&#21644;&#36890;&#21521;&#19987;&#19994;&#20154;&#24037;&#26234;&#33021;&#30340;&#36947;&#36335;
&lt;/p&gt;
&lt;p&gt;
A Critical Review of Large Language Models: Sensitivity, Bias, and the Path Toward Specialized AI. (arXiv:2307.15425v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19987;&#19994;&#32534;&#35793;&#35821;&#35328;&#27169;&#22411;&#21644;&#36890;&#29992;&#27169;&#22411;&#22312;&#26816;&#27979;&#21487;&#25345;&#32493;&#21457;&#23637;&#30446;&#26631;&#26041;&#38754;&#30340;&#27604;&#25928;&#29575;&#65292;&#20851;&#27880;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#30340;&#20559;&#35265;&#21644;&#25935;&#24863;&#24615;&#25361;&#25112;&#12290;&#30740;&#31350;&#24378;&#35843;&#20102;&#19987;&#19994;&#22521;&#35757;&#23545;&#20110;&#31934;&#30830;&#26080;&#20559;&#30340;&#20998;&#26512;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#36890;&#36807;&#23545;&#19968;&#20010;&#20844;&#21496;&#25551;&#36848;&#25968;&#25454;&#38598;&#30340;&#26696;&#20363;&#30740;&#31350;&#23545;&#27604;&#20102;GPT-3.5&#21644;&#19987;&#19994;SDG&#26816;&#27979;&#27169;&#22411;&#30340;&#24046;&#24322;&#12290;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#22312;&#27169;&#22411;&#36873;&#25321;&#26102;&#38656;&#35201;&#32771;&#34385;&#20219;&#21153;&#35201;&#27714;&#12289;&#25104;&#26412;&#12289;&#22797;&#26434;&#24615;&#21644;&#36879;&#26126;&#24230;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#21151;&#33021;&#24615;&#65292;&#20294;&#20316;&#32773;&#24314;&#35758;&#22312;&#38656;&#35201;&#31934;&#30830;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#20219;&#21153;&#20013;&#20351;&#29992;&#19987;&#19994;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#19987;&#19994;&#32534;&#35793;&#35821;&#35328;&#27169;&#22411;&#21644;OpenAI&#30340;GPT-3.5&#31561;&#36890;&#29992;&#27169;&#22411;&#22312;&#26816;&#27979;&#25991;&#26412;&#25968;&#25454;&#20013;&#30340;&#21487;&#25345;&#32493;&#21457;&#23637;&#30446;&#26631;&#65288;SDGs&#65289;&#26041;&#38754;&#30340;&#27604;&#25928;&#29575;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#23427;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#20102;&#20851;&#38190;&#35780;&#35770;&#65292;&#35299;&#20915;&#20102;&#19982;&#20559;&#35265;&#21644;&#25935;&#24863;&#24615;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;&#24378;&#35843;&#20102;&#31934;&#30830;&#12289;&#26080;&#20559;&#35265;&#20998;&#26512;&#30340;&#19987;&#19994;&#22521;&#35757;&#30340;&#24517;&#35201;&#24615;&#12290;&#20351;&#29992;&#20844;&#21496;&#25551;&#36848;&#25968;&#25454;&#38598;&#30340;&#26696;&#20363;&#30740;&#31350;&#25581;&#31034;&#20102;GPT-3.5&#21644;&#19987;&#19994;SDG&#26816;&#27979;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#34429;&#28982;GPT-3.5&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#35206;&#30422;&#33539;&#22260;&#65292;&#20294;&#21487;&#33021;&#20250;&#35782;&#21035;&#19982;&#20844;&#21496;&#27963;&#21160;&#30456;&#20851;&#24615;&#26377;&#38480;&#30340;SDGs&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#19987;&#19994;&#27169;&#22411;&#32858;&#28966;&#20110;&#39640;&#24230;&#30456;&#20851;&#30340;SDGs&#12290;&#24378;&#35843;&#20102;&#28145;&#24605;&#29087;&#34385;&#30340;&#27169;&#22411;&#36873;&#25321;&#30340;&#37325;&#35201;&#24615;&#65292;&#32771;&#34385;&#20219;&#21153;&#35201;&#27714;&#12289;&#25104;&#26412;&#12289;&#22797;&#26434;&#24615;&#21644;&#36879;&#26126;&#24230;&#12290;&#23613;&#31649;LLMs&#30340;&#22810;&#21151;&#33021;&#24615;&#65292;&#20294;&#24314;&#35758;&#22312;&#38656;&#35201;&#31934;&#30830;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#20219;&#21153;&#20013;&#20351;&#29992;&#19987;&#19994;&#27169;&#22411;&#12290;&#30740;&#31350;&#26368;&#21518;&#40723;&#21169;&#20102;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
This paper examines the comparative effectiveness of a specialized compiled language model and a general-purpose model like OpenAI's GPT-3.5 in detecting SDGs within text data. It presents a critical review of Large Language Models (LLMs), addressing challenges related to bias and sensitivity. The necessity of specialized training for precise, unbiased analysis is underlined. A case study using a company descriptions dataset offers insight into the differences between the GPT-3.5 and the specialized SDG detection model. While GPT-3.5 boasts broader coverage, it may identify SDGs with limited relevance to the companies' activities. In contrast, the specialized model zeroes in on highly pertinent SDGs. The importance of thoughtful model selection is emphasized, taking into account task requirements, cost, complexity, and transparency. Despite the versatility of LLMs, the use of specialized models is suggested for tasks demanding precision and accuracy. The study concludes by encouraging 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#39044;&#27979;&#26694;&#26550;DSN&#65292;&#36890;&#36807;&#21516;&#26102;&#21033;&#29992;&#24086;&#23376;&#20869;&#37096;&#21644;&#24086;&#23376;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#26469;&#25913;&#36827;&#31038;&#20132;&#23186;&#20307;&#21463;&#27426;&#36814;&#31243;&#24230;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.15413</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#20010;&#24086;&#23376;&#20381;&#36182;&#20851;&#31995;&#25913;&#36827;&#31038;&#20132;&#23186;&#20307;&#21463;&#27426;&#36814;&#31243;&#24230;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Improving Social Media Popularity Prediction with Multiple Post Dependencies. (arXiv:2307.15413v1 [cs.MM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15413
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#39044;&#27979;&#26694;&#26550;DSN&#65292;&#36890;&#36807;&#21516;&#26102;&#21033;&#29992;&#24086;&#23376;&#20869;&#37096;&#21644;&#24086;&#23376;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#26469;&#25913;&#36827;&#31038;&#20132;&#23186;&#20307;&#21463;&#27426;&#36814;&#31243;&#24230;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#21463;&#27426;&#36814;&#31243;&#24230;&#39044;&#27979;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#23545;&#35768;&#22810;&#19981;&#21516;&#24212;&#29992;&#26377;&#30528;&#28145;&#36828;&#30340;&#24433;&#21709;&#65292;&#22914;&#25512;&#33616;&#31995;&#32479;&#21644;&#22810;&#23186;&#20307;&#24191;&#21578;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#19968;&#20123;&#21162;&#21147;&#21033;&#29992;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#30340;&#20869;&#23481;&#26469;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#20294;&#35768;&#22810;&#29616;&#26377;&#27169;&#22411;&#26410;&#33021;&#20805;&#20998;&#21033;&#29992;&#24086;&#23376;&#20043;&#38388;&#30340;&#22810;&#20010;&#20381;&#36182;&#20851;&#31995;&#65292;&#32780;&#36825;&#20123;&#20381;&#36182;&#20851;&#31995;&#23545;&#20110;&#20840;&#38754;&#25552;&#21462;&#24086;&#23376;&#20869;&#23481;&#20449;&#24687;&#24456;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#20381;&#36182;&#24863;&#30693;&#24207;&#21015;&#32593;&#32476;&#65288;DSN&#65289;&#8221;&#30340;&#26032;&#22411;&#39044;&#27979;&#26694;&#26550;&#65292;&#23427;&#21516;&#26102;&#21033;&#29992;&#24086;&#23376;&#20869;&#37096;&#20381;&#36182;&#21644;&#24086;&#23376;&#38388;&#20381;&#36182;&#12290;&#23545;&#20110;&#24086;&#23376;&#20869;&#37096;&#20381;&#36182;&#65292;DSN&#37319;&#29992;&#19968;&#31181;&#22810;&#27169;&#24577;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#39640;&#25928;&#24494;&#35843;&#31574;&#30053;&#65292;&#20174;&#24086;&#23376;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#20449;&#24687;&#20013;&#33719;&#21462;&#20219;&#21153;&#29305;&#23450;&#30340;&#34920;&#31034;&#12290;&#23545;&#20110;&#24086;&#23376;&#38388;&#20381;&#36182;&#65292;DSN&#20351;&#29992;&#19968;&#31181;&#20998;&#23618;&#20449;&#24687;&#20256;&#25773;&#26041;&#27861;&#26469;&#23398;&#20064;&#33021;&#26356;&#22909;&#25551;&#36848;&#24086;&#23376;&#20043;&#38388;&#24046;&#24322;&#30340;&#31867;&#21035;&#34920;&#31034;&#12290;DSN&#36824;&#21033;&#29992;...
&lt;/p&gt;
&lt;p&gt;
Social Media Popularity Prediction has drawn a lot of attention because of its profound impact on many different applications, such as recommendation systems and multimedia advertising. Despite recent efforts to leverage the content of social media posts to improve prediction accuracy, many existing models fail to fully exploit the multiple dependencies between posts, which are important to comprehensively extract content information from posts. To tackle this problem, we propose a novel prediction framework named Dependency-aware Sequence Network (DSN) that exploits both intra- and inter-post dependencies. For intra-post dependency, DSN adopts a multimodal feature extractor with an efficient fine-tuning strategy to obtain task-specific representations from images and textual information of posts. For inter-post dependency, DSN uses a hierarchical information propagation method to learn category representations that could better describe the difference between posts. DSN also exploits 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#30417;&#30563;&#23398;&#20064;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#34892;&#20026;&#19978;&#21463;&#21040;&#37329;&#26631;&#31614;&#30340;&#26174;&#33879;&#24433;&#21709;&#65292;&#20294;&#23545;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#35828;&#65292;&#26631;&#31614;&#19981;&#24179;&#34913;&#24433;&#21709;&#36739;&#23567;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#19978;&#19979;&#25991;&#23398;&#20064;&#23545;&#26631;&#31614;&#25200;&#21160;&#30340;&#25935;&#24863;&#24615;&#36739;&#20302;&#12290;</title><link>http://arxiv.org/abs/2307.15411</link><description>&lt;p&gt;
&#30740;&#31350;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#23398;&#20064;&#34892;&#20026;&#65306;&#19982;&#30417;&#30563;&#23398;&#20064;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Investigating the Learning Behaviour of In-context Learning: A Comparison with Supervised Learning. (arXiv:2307.15411v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15411
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#30417;&#30563;&#23398;&#20064;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#34892;&#20026;&#19978;&#21463;&#21040;&#37329;&#26631;&#31614;&#30340;&#26174;&#33879;&#24433;&#21709;&#65292;&#20294;&#23545;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#35828;&#65292;&#26631;&#31614;&#19981;&#24179;&#34913;&#24433;&#21709;&#36739;&#23567;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#19978;&#19979;&#25991;&#23398;&#20064;&#23545;&#26631;&#31614;&#25200;&#21160;&#30340;&#25935;&#24863;&#24615;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#20196;&#20154;&#27880;&#30446;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#33021;&#21147;&#65292;&#22312;&#27809;&#26377;&#26126;&#30830;&#39044;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#20165;&#36890;&#36807;&#23569;&#37327;&#35757;&#32451;&#26679;&#20363;&#23601;&#21487;&#20197;&#23398;&#20064;&#26032;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;LLM&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#23545;&#20110;ICL&#22914;&#20309;&#20174;&#32473;&#23450;&#30340;&#25552;&#31034;&#20013;&#23398;&#20064;&#30693;&#35782;&#30340;&#20102;&#35299;&#36824;&#24456;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;ICL&#30340;&#23398;&#20064;&#34892;&#20026;&#65292;&#25105;&#20204;&#20351;&#29992;&#30456;&#21516;&#30340;&#28436;&#31034;&#26679;&#20363;&#36890;&#36807;ICL&#21644;&#30417;&#30563;&#23398;&#20064;&#65288;SL&#65289;&#20998;&#21035;&#35757;&#32451;&#30456;&#21516;&#30340;LLM&#65292;&#24182;&#30740;&#31350;&#23427;&#20204;&#22312;&#19968;&#31995;&#21015;&#20998;&#31867;&#20219;&#21153;&#19978;&#22312;&#26631;&#31614;&#25200;&#21160;&#65288;&#22122;&#22768;&#26631;&#31614;&#21644;&#26631;&#31614;&#19981;&#24179;&#34913;&#65289;&#19979;&#30340;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#37329;&#26631;&#31614;&#23545;&#20110;&#19979;&#28216;&#30340;&#19978;&#19979;&#25991;&#24615;&#33021;&#26377;&#37325;&#22823;&#24433;&#21709;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65307;&#28982;&#32780;&#65292;&#23545;&#20110;ICL&#26469;&#35828;&#65292;&#26631;&#31614;&#19981;&#24179;&#34913;&#23545;&#25152;&#26377;&#27169;&#22411;&#22823;&#23567;&#37117;&#19981;&#22826;&#37325;&#35201;&#12290;&#20854;&#27425;&#65292;&#22312;&#19982;SL&#36827;&#34892;&#27604;&#36739;&#26102;&#65292;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#34920;&#26126;ICL&#23545;&#26631;&#31614;&#25200;&#21160;&#30340;&#25935;&#24863;&#24615;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown remarkable capacity for in-context learning (ICL), where learning a new task from just a few training examples is done without being explicitly pre-trained. However, despite the success of LLMs, there has been little understanding of how ICL learns the knowledge from the given prompts. In this paper, to make progress toward understanding the learning behaviour of ICL, we train the same LLMs with the same demonstration examples via ICL and supervised learning (SL), respectively, and investigate their performance under label perturbations (i.e., noisy labels and label imbalance) on a range of classification tasks. First, via extensive experiments, we find that gold labels have significant impacts on the downstream in-context performance, especially for large language models; however, imbalanced labels matter little to ICL across all model sizes. Second, when comparing with SL, we show empirically that ICL is less sensitive to label perturbations th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#23545;&#35805;&#24847;&#22270;&#35825;&#23548;&#26694;&#26550;&#65292;&#36890;&#36807;&#39044;&#22788;&#29702;&#23545;&#35805;&#35821;&#26009;&#24211;&#21644;&#30740;&#31350;&#26368;&#24120;&#35265;&#30340;&#24207;&#21015;&#26469;&#25552;&#21462;&#24847;&#22270;&#30340;&#23545;&#35805;&#27969;&#31243;&#65292;&#36866;&#29992;&#20110;&#21508;&#34892;&#19994;&#30340;&#23454;&#38469;&#23458;&#25143;&#25903;&#25345;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.15410</link><description>&lt;p&gt;
&#22312;&#23458;&#25143;&#25903;&#25345;&#23545;&#35805;&#20013;&#23454;&#29616;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#24847;&#22270;&#35825;&#23548;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Towards a Fully Unsupervised Framework for Intent Induction in Customer Support Dialogues. (arXiv:2307.15410v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#23545;&#35805;&#24847;&#22270;&#35825;&#23548;&#26694;&#26550;&#65292;&#36890;&#36807;&#39044;&#22788;&#29702;&#23545;&#35805;&#35821;&#26009;&#24211;&#21644;&#30740;&#31350;&#26368;&#24120;&#35265;&#30340;&#24207;&#21015;&#26469;&#25552;&#21462;&#24847;&#22270;&#30340;&#23545;&#35805;&#27969;&#31243;&#65292;&#36866;&#29992;&#20110;&#21508;&#34892;&#19994;&#30340;&#23454;&#38469;&#23458;&#25143;&#25903;&#25345;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#24847;&#22270;&#35825;&#23548;&#27169;&#22411;&#38656;&#35201;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#27880;&#37322;&#23545;&#35805;&#32791;&#26102;&#12289;&#32321;&#29712;&#19988;&#26114;&#36149;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#23545;&#35805;&#24847;&#22270;&#35825;&#23548;&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#39044;&#22788;&#29702;&#23545;&#35805;&#35821;&#26009;&#24211;&#20197;&#25552;&#39640;&#32467;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#30740;&#31350;&#26368;&#24120;&#35265;&#30340;&#24207;&#21015;&#26469;&#25552;&#21462;&#24847;&#22270;&#30340;&#23545;&#35805;&#27969;&#31243;&#12290;&#23613;&#31649;&#25105;&#20204;&#22312;MultiWOZ&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#24037;&#20316;&#65292;&#20294;&#35813;&#26694;&#26550;&#19981;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#65292;&#22240;&#27492;&#36866;&#29992;&#20110;&#20219;&#20309;&#21487;&#33021;&#30340;&#29992;&#20363;&#65292;&#36825;&#20351;&#24471;&#23427;&#22312;&#21508;&#34892;&#19994;&#30340;&#23454;&#38469;&#23458;&#25143;&#25903;&#25345;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
State of the art models in intent induction require annotated datasets. However, annotating dialogues is time-consuming, laborious and expensive. In this work, we propose a completely unsupervised framework for intent induction within a dialogue. In addition, we show how pre-processing the dialogue corpora can improve results. Finally, we show how to extract the dialogue flows of intentions by investigating the most common sequences. Although we test our work in the MultiWOZ dataset, the fact that this framework requires no prior knowledge make it applicable to any possible use case, making it very relevant to real world customer support applications across industry.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;ChatGPT&#22312;&#32763;&#35793;&#33521;&#35821;&#25104;&#21360;&#22320;&#35821;&#12289;&#27888;&#21346;&#22266;&#35821;&#21644;&#21345;&#32435;&#36798;&#35821;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#21360;&#22320;&#35821;&#32763;&#35793;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#27969;&#30021;&#24615;&#65292;&#32780;&#27888;&#21346;&#22266;&#35821;&#32763;&#35793;&#34920;&#29616;&#19981;&#20339;&#12290;</title><link>http://arxiv.org/abs/2307.15376</link><description>&lt;p&gt;
&#20351;&#29992;ChatGPT&#36827;&#34892;&#22810;&#35821;&#35328;&#26053;&#28216;&#36741;&#21161;&#65306;&#27604;&#36739;&#21360;&#22320;&#35821;&#12289;&#27888;&#21346;&#22266;&#35821;&#21644;&#21345;&#32435;&#36798;&#35821;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Multilingual Tourist Assistance using ChatGPT: Comparing Capabilities in Hindi, Telugu, and Kannada. (arXiv:2307.15376v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15376
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;ChatGPT&#22312;&#32763;&#35793;&#33521;&#35821;&#25104;&#21360;&#22320;&#35821;&#12289;&#27888;&#21346;&#22266;&#35821;&#21644;&#21345;&#32435;&#36798;&#35821;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#21360;&#22320;&#35821;&#32763;&#35793;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#27969;&#30021;&#24615;&#65292;&#32780;&#27888;&#21346;&#22266;&#35821;&#32763;&#35793;&#34920;&#29616;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;OpenAI&#30340;AI&#35821;&#35328;&#27169;&#22411;ChatGPT&#22312;&#23558;&#33521;&#35821;&#32763;&#35793;&#25104;&#21360;&#22320;&#35821;&#12289;&#27888;&#21346;&#22266;&#35821;&#21644;&#21345;&#32435;&#36798;&#35821;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#26088;&#22312;&#24110;&#21161;&#21360;&#24230;&#22810;&#35821;&#35328;&#29615;&#22659;&#20013;&#30340;&#28216;&#23458;&#12290;&#20026;&#20102;&#35780;&#20272;&#32763;&#35793;&#36136;&#37327;&#65292;&#20351;&#29992;&#20102;&#21253;&#25324;&#24120;&#35782;&#12289;&#39135;&#29289;&#21644;&#26053;&#34892;&#31561;&#39046;&#22495;&#30340;50&#20010;&#38382;&#39064;&#30340;&#27979;&#35797;&#38598;&#12290;&#36825;&#20123;&#38382;&#39064;&#30001;&#20116;&#21517;&#24535;&#24895;&#32773;&#35780;&#20272;&#20854;&#20934;&#30830;&#24615;&#21644;&#27969;&#30021;&#24615;&#65292;&#24182;&#23558;&#24471;&#20998;&#36716;&#25442;&#20026;BLEU&#20998;&#25968;&#12290;BLEU&#20998;&#25968;&#35780;&#20272;&#26426;&#22120;&#29983;&#25104;&#30340;&#32763;&#35793;&#19982;&#20154;&#24037;&#32763;&#35793;&#30340;&#25509;&#36817;&#31243;&#24230;&#65292;&#36739;&#39640;&#30340;&#20998;&#25968;&#34920;&#31034;&#26356;&#22909;&#30340;&#32763;&#35793;&#36136;&#37327;&#12290;&#32467;&#26524;&#26174;&#31034;&#21360;&#22320;&#35821;&#32763;&#35793;&#20248;&#20110;&#20854;&#20182;&#35821;&#35328;&#65292;&#20934;&#30830;&#24615;&#21644;&#27969;&#30021;&#24615;&#26356;&#22909;&#65292;&#32780;&#27888;&#21346;&#22266;&#35821;&#32763;&#35793;&#33853;&#21518;&#12290;&#20154;&#24037;&#35780;&#20272;&#21592;&#35780;&#20215;&#20102;&#32763;&#35793;&#30340;&#20934;&#30830;&#24615;&#21644;&#27969;&#30021;&#24615;&#65292;&#20026;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#29616;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research investigates the effectiveness of ChatGPT, an AI language model by OpenAI, in translating English into Hindi, Telugu, and Kannada languages, aimed at assisting tourists in India's linguistically diverse environment. To measure the translation quality, a test set of 50 questions from diverse fields such as general knowledge, food, and travel was used. These were assessed by five volunteers for accuracy and fluency, and the scores were subsequently converted into a BLEU score. The BLEU score evaluates the closeness of a machine-generated translation to a human translation, with a higher score indicating better translation quality. The Hindi translations outperformed others, showcasing superior accuracy and fluency, whereas Telugu translations lagged behind. Human evaluators rated both the accuracy and fluency of translations, offering a comprehensive perspective on the language model's performance.
&lt;/p&gt;</description></item><item><title>Med-HALT&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#21644;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#20943;&#23569;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#21307;&#30103;&#39046;&#22495;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21253;&#25324;&#22810;&#31181;&#21019;&#26032;&#30340;&#27979;&#35797;&#27169;&#24335;&#65292;&#24182;&#35780;&#20272;&#20102;&#39046;&#20808;&#30340;LLMs&#22312;&#24615;&#33021;&#19978;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2307.15343</link><description>&lt;p&gt;
Med-HALT:&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#21307;&#30103;&#39046;&#22495;&#24187;&#35273;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Med-HALT: Medical Domain Hallucination Test for Large Language Models. (arXiv:2307.15343v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15343
&lt;/p&gt;
&lt;p&gt;
Med-HALT&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#21644;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#20943;&#23569;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#21307;&#30103;&#39046;&#22495;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21253;&#25324;&#22810;&#31181;&#21019;&#26032;&#30340;&#27979;&#35797;&#27169;&#24335;&#65292;&#24182;&#35780;&#20272;&#20102;&#39046;&#20808;&#30340;LLMs&#22312;&#24615;&#33021;&#19978;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35770;&#25991;&#20851;&#27880;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#24187;&#35273;&#38382;&#39064;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#30103;&#39046;&#22495;&#30340;&#32972;&#26223;&#19979;&#12290;&#24187;&#35273;&#25351;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#20102;&#21512;&#29702;&#20294;&#26410;&#32463;&#39564;&#35777;&#25110;&#38169;&#35823;&#30340;&#20449;&#24687;&#65292;&#36825;&#21487;&#33021;&#23545;&#21307;&#30103;&#24212;&#29992;&#20135;&#29983;&#20005;&#37325;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#21644;&#25968;&#25454;&#38598;&#65292;Med-HALT&#65288;&#21307;&#30103;&#39046;&#22495;&#24187;&#35273;&#27979;&#35797;&#65289;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;&#21644;&#20943;&#23569;&#24187;&#35273;&#12290;Med-HALT&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#20803;&#21270;&#30340;&#36328;&#22269;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#26469;&#33258;&#19981;&#21516;&#22269;&#23478;&#30340;&#21307;&#30103;&#26816;&#26597;&#65292;&#21253;&#25324;&#22810;&#31181;&#21019;&#26032;&#30340;&#27979;&#35797;&#27169;&#24335;&#12290;Med-HALT&#21253;&#25324;&#20004;&#31867;&#27979;&#35797;&#65306;&#25512;&#29702;&#21644;&#22522;&#20110;&#35760;&#24518;&#30340;&#24187;&#35273;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;LLMs&#30340;&#38382;&#39064;&#35299;&#20915;&#21644;&#20449;&#24687;&#26816;&#32034;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35780;&#20272;&#20102;&#25991;&#26412;Davinci&#65292;GPT-3.5&#65292;LlaMa-2&#65292;MPT&#21644;Falcon&#31561;&#39046;&#20808;&#30340;LLMs&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#24615;&#33021;&#19978;&#30340;&#26174;&#33879;&#24046;&#24322;&#12290;&#36825;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#26377;&#20851;&#25968;&#25454;&#38598;&#30340;&#35814;&#32454;&#35265;&#35299;&#65292;&#20419;&#36827;&#20102;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research paper focuses on the challenges posed by hallucinations in large language models (LLMs), particularly in the context of the medical domain. Hallucination, wherein these models generate plausible yet unverified or incorrect information, can have serious consequences in healthcare applications. We propose a new benchmark and dataset, Med-HALT (Medical Domain Hallucination Test), designed specifically to evaluate and reduce hallucinations. Med-HALT provides a diverse multinational dataset derived from medical examinations across various countries and includes multiple innovative testing modalities. Med-HALT includes two categories of tests reasoning and memory-based hallucination tests, designed to assess LLMs's problem-solving and information retrieval abilities.  Our study evaluated leading LLMs, including Text Davinci, GPT-3.5, LlaMa-2, MPT, and Falcon, revealing significant differences in their performance. The paper provides detailed insights into the dataset, promoting
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#30340;&#30446;&#26631;&#26159;&#25506;&#35752;&#24403;&#21069;&#35770;&#35777;&#35745;&#31639;&#27169;&#22411;&#25552;&#20379;&#30340;&#21453;&#39304;&#32500;&#24230;&#65292;&#20197;&#25552;&#39640;&#23398;&#20064;&#32773;&#30340;&#25209;&#21028;&#24615;&#24605;&#32500;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.15341</link><description>&lt;p&gt;
&#25945;&#25105;&#22914;&#20309;&#25552;&#39640;&#25105;&#30340;&#35770;&#35777;&#33021;&#21147;&#65306;&#20851;&#20110;&#35770;&#35777;&#20013;&#30340;&#21453;&#39304;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Teach Me How to Improve My Argumentation Skills: A Survey on Feedback in Argumentation. (arXiv:2307.15341v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15341
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#30340;&#30446;&#26631;&#26159;&#25506;&#35752;&#24403;&#21069;&#35770;&#35777;&#35745;&#31639;&#27169;&#22411;&#25552;&#20379;&#30340;&#21453;&#39304;&#32500;&#24230;&#65292;&#20197;&#25552;&#39640;&#23398;&#20064;&#32773;&#30340;&#25209;&#21028;&#24615;&#24605;&#32500;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#35777;&#22312;&#25945;&#32946;&#20013;&#30340;&#24212;&#29992;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#25552;&#39640;&#23398;&#29983;&#31561;&#26368;&#32456;&#29992;&#25143;&#30340;&#25209;&#21028;&#24615;&#24605;&#32500;&#33021;&#21147;&#65292;&#24182;&#19988;&#24050;&#32463;&#24320;&#21457;&#20102;&#29992;&#20110;&#36741;&#21161;&#35813;&#36807;&#31243;&#30340;&#35770;&#35777;&#30340;&#35745;&#31639;&#27169;&#22411;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#35780;&#20272;&#35770;&#35777;&#30340;&#36136;&#37327;&#38750;&#24120;&#26377;&#29992;&#65292;&#20294;&#24448;&#24448;&#26080;&#27861;&#35299;&#37322;&#20026;&#20160;&#20040;&#26576;&#20010;&#29305;&#23450;&#30340;&#35770;&#35777;&#34987;&#35748;&#20026;&#26159;&#24046;&#30340;&#25110;&#19981;&#22909;&#65292;&#36825;&#20351;&#24471;&#38590;&#20197;&#21521;&#29992;&#25143;&#25552;&#20379;&#26377;&#24314;&#35774;&#24615;&#30340;&#21453;&#39304;&#26469;&#22686;&#24378;&#20182;&#20204;&#30340;&#25209;&#21028;&#24615;&#24605;&#32500;&#33021;&#21147;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25506;&#32034;&#24403;&#21069;&#35770;&#35777;&#35745;&#31639;&#27169;&#22411;&#25552;&#20379;&#30340;&#19981;&#21516;&#21453;&#39304;&#32500;&#24230;&#65288;&#20016;&#23500;&#24615;&#12289;&#21487;&#35270;&#21270;&#12289;&#20114;&#21160;&#24615;&#21644;&#20010;&#24615;&#21270;&#65289;&#65292;&#20197;&#21450;&#22686;&#24378;&#36825;&#20123;&#27169;&#22411;&#35299;&#37322;&#33021;&#21147;&#30340;&#21487;&#33021;&#24615;&#65292;&#26368;&#32456;&#24110;&#21161;&#23398;&#20064;&#32773;&#25552;&#39640;&#25209;&#21028;&#24615;&#24605;&#32500;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of argumentation in education has been shown to improve critical thinking skills for end-users such as students, and computational models for argumentation have been developed to assist in this process. Although these models are useful for evaluating the quality of an argument, they oftentimes cannot explain why a particular argument is considered poor or not, which makes it difficult to provide constructive feedback to users to strengthen their critical thinking skills. In this survey, we aim to explore the different dimensions of feedback (Richness, Visualization, Interactivity, and Personalization) provided by the current computational models for argumentation, and the possibility of enhancing the power of explanations of such models, ultimately helping learners improve their critical thinking skills.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#24605;&#32500;&#30340;&#39592;&#26550;&#8221;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#24182;&#34892;&#35299;&#30721;&#26469;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#24310;&#36831;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#26174;&#33879;&#25552;&#39640;&#20102;&#36895;&#24230;&#65292;&#36824;&#21487;&#20197;&#28508;&#22312;&#22320;&#25552;&#39640;&#31572;&#26696;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.15337</link><description>&lt;p&gt;
&#24605;&#32500;&#30340;&#39592;&#26550;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36827;&#34892;&#24182;&#34892;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding. (arXiv:2307.15337v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15337
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#24605;&#32500;&#30340;&#39592;&#26550;&#8221;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#24182;&#34892;&#35299;&#30721;&#26469;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#24310;&#36831;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#26174;&#33879;&#25552;&#39640;&#20102;&#36895;&#24230;&#65292;&#36824;&#21487;&#20197;&#28508;&#22312;&#22320;&#25552;&#39640;&#31572;&#26696;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#31471;&#21040;&#31471;&#29983;&#25104;&#24310;&#36831;&#12290;&#39640;&#29983;&#25104;&#24310;&#36831;&#30340;&#19968;&#20010;&#20027;&#35201;&#21407;&#22240;&#26159;&#20960;&#20046;&#25152;&#26377;&#26368;&#20808;&#36827;&#30340;LLMs&#37117;&#37319;&#29992;&#20102;&#39034;&#24207;&#35299;&#30721;&#26041;&#27861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#21463;&#21040;&#20154;&#31867;&#30340;&#24605;&#32771;&#21644;&#20889;&#20316;&#36807;&#31243;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#24605;&#32500;&#30340;&#39592;&#26550;&#8221;&#65288;SoT&#65289;&#65292;&#23427;&#25351;&#23548;LLMs&#39318;&#20808;&#29983;&#25104;&#31572;&#26696;&#30340;&#39592;&#26550;&#65292;&#28982;&#21518;&#36890;&#36807;&#24182;&#34892;API&#35843;&#29992;&#25110;&#25209;&#37327;&#35299;&#30721;&#26469;&#24182;&#34892;&#23436;&#25104;&#27599;&#20010;&#39592;&#26550;&#28857;&#30340;&#20869;&#23481;&#12290;SoT&#19981;&#20165;&#26174;&#33879;&#25552;&#39640;&#20102;&#36895;&#24230;&#65288;&#22312;11&#20010;&#19981;&#21516;&#30340;LLMs&#19978;&#25552;&#39640;&#20102;&#26368;&#22810;2.39&#20493;&#65289;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#28508;&#22312;&#22320;&#25552;&#39640;&#22312;&#22810;&#20010;&#38382;&#39064;&#31867;&#21035;&#19978;&#30340;&#31572;&#26696;&#36136;&#37327;&#65292;&#21253;&#25324;&#22810;&#26679;&#24615;&#21644;&#30456;&#20851;&#24615;&#12290;SoT&#26159;&#19968;&#31181;&#38024;&#23545;&#25928;&#29575;&#30340;&#25968;&#25454;&#23548;&#21521;&#20248;&#21270;&#30340;&#21021;&#27493;&#23581;&#35797;&#65292;&#24182;&#25581;&#31034;&#20102;&#23558;LLMs&#25512;&#21160;&#26356;&#20687;&#20154;&#31867;&#24605;&#32771;&#20197;&#25552;&#39640;&#31572;&#26696;&#36136;&#37327;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work aims at decreasing the end-to-end generation latency of large language models (LLMs). One of the major causes of the high generation latency is the sequential decoding approach adopted by almost all state-of-the-art LLMs. In this work, motivated by the thinking and writing process of humans, we propose "Skeleton-of-Thought" (SoT), which guides LLMs to first generate the skeleton of the answer, and then conducts parallel API calls or batched decoding to complete the contents of each skeleton point in parallel. Not only does SoT provide considerable speed-up (up to 2.39x across 11 different LLMs), but it can also potentially improve the answer quality on several question categories in terms of diversity and relevance. SoT is an initial attempt at data-centric optimization for efficiency, and reveal the potential of pushing LLMs to think more like a human for answer quality.
&lt;/p&gt;</description></item><item><title>BARTPhoBEiT&#26159;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#36234;&#21335;&#27169;&#22411;&#65292;&#38024;&#23545;&#36234;&#21335;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#36827;&#34892;&#20102;&#25913;&#36827;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#20934;&#30830;&#24230;&#12289;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#12289;F1-score&#12289;WUPS 0.0&#21644;WUPS 0.9&#31561;&#20845;&#20010;&#25351;&#26631;&#19978;&#20248;&#20110;&#24378;&#22522;&#32447;&#27169;&#22411;&#65292;&#25552;&#21319;&#20102;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2307.15335</link><description>&lt;p&gt;
BARTPhoBEiT: &#39044;&#35757;&#32451;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#21644;&#22270;&#20687;Transformer&#27169;&#22411;&#22312;&#36234;&#21335;&#35270;&#35273;&#38382;&#31572;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
BARTPhoBEiT: Pre-trained Sequence-to-Sequence and Image Transformers Models for Vietnamese Visual Question Answering. (arXiv:2307.15335v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15335
&lt;/p&gt;
&lt;p&gt;
BARTPhoBEiT&#26159;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#36234;&#21335;&#27169;&#22411;&#65292;&#38024;&#23545;&#36234;&#21335;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#36827;&#34892;&#20102;&#25913;&#36827;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#20934;&#30830;&#24230;&#12289;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#12289;F1-score&#12289;WUPS 0.0&#21644;WUPS 0.9&#31561;&#20845;&#20010;&#25351;&#26631;&#19978;&#20248;&#20110;&#24378;&#22522;&#32447;&#27169;&#22411;&#65292;&#25552;&#21319;&#20102;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#26159;&#19968;&#20010;&#22797;&#26434;&#19988;&#35201;&#27714;&#39640;&#30340;&#20219;&#21153;&#65292;&#23558;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#30456;&#32467;&#21512;&#65292;&#24341;&#36215;&#20102;&#30740;&#31350;&#32773;&#30340;&#20852;&#36259;&#12290; &#33521;&#35821;&#20316;&#20026;&#36164;&#28304;&#20016;&#23500;&#30340;&#35821;&#35328;&#65292;&#22312;VQA&#30340;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#35774;&#35745;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290; &#20294;&#26159;&#65292;&#32570;&#23569;&#38024;&#23545;&#36234;&#21335;&#31561;&#29305;&#23450;&#22269;&#23478;&#30340;&#27169;&#22411;&#12290; &#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#36234;&#21335;&#27169;&#22411;&#65292;&#21517;&#20026;BARTPhoBEiT&#12290; &#35813;&#27169;&#22411;&#21253;&#25324;&#39044;&#35757;&#32451;&#30340;&#36234;&#21335;&#35821;&#24207;&#21015;&#21040;&#24207;&#21015;&#21644;&#21452;&#21521;&#32534;&#30721;&#22120;&#22270;&#20687;Transformer&#65292;&#24182;&#35780;&#20272;&#20102;&#36234;&#21335;VQA&#25968;&#25454;&#38598;&#12290; &#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#20845;&#20010;&#25351;&#26631;&#65306;&#20934;&#30830;&#24230;&#65292;&#31934;&#30830;&#24230;&#65292;&#21484;&#22238;&#29575;&#65292;F1-score&#65292;WUPS 0.0&#21644;WUPS 0.9&#19978;&#20248;&#20110;&#24378;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#25913;&#36827;&#20102;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual Question Answering (VQA) is an intricate and demanding task that integrates natural language processing (NLP) and computer vision (CV), capturing the interest of researchers. The English language, renowned for its wealth of resources, has witnessed notable advancements in both datasets and models designed for VQA. However, there is a lack of models that target specific countries such as Vietnam. To address this limitation, we introduce a transformer-based Vietnamese model named BARTPhoBEiT. This model includes pre-trained Sequence-to-Sequence and bidirectional encoder representation from Image Transformers in Vietnamese and evaluates Vietnamese VQA datasets. Experimental results demonstrate that our proposed model outperforms the strong baseline and improves the state-of-the-art in six metrics: Accuracy, Precision, Recall, F1-score, WUPS 0.0, and WUPS 0.9.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#20004;&#20010;&#25945;&#31243;&#65292;&#20171;&#32461;&#20102;&#20351;&#29992;BERT&#24494;&#35843;&#21644;&#25552;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;Twitter&#31435;&#22330;&#35782;&#21035;&#30340;&#26041;&#27861;&#12290;&#25945;&#31243;&#36890;&#36807;&#23454;&#20363;&#20195;&#30721;&#21644;&#21487;&#35270;&#21270;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#23569;&#26679;&#26412;ChatGPT&#21644;FLAN-T5&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#23545;BERT&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#30340;&#25351;&#23548;&#12290;&#36825;&#20123;&#25945;&#31243;&#20351;&#23398;&#20064;&#32773;&#33021;&#22815;&#25484;&#25569;&#36816;&#29992;&#20808;&#36827;&#26041;&#27861;&#36827;&#34892;&#31435;&#22330;&#35782;&#21035;&#30340;&#23454;&#36341;&#32463;&#39564;&#12290;</title><link>http://arxiv.org/abs/2307.15331</link><description>&lt;p&gt;
&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31435;&#22330;&#35782;&#21035;&#30340;&#25945;&#31243;&#65306;BERT&#24494;&#35843;&#21644;&#25552;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Tutorials on Stance Detection using Pre-trained Language Models: Fine-tuning BERT and Prompting Large Language Models. (arXiv:2307.15331v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15331
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#20004;&#20010;&#25945;&#31243;&#65292;&#20171;&#32461;&#20102;&#20351;&#29992;BERT&#24494;&#35843;&#21644;&#25552;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;Twitter&#31435;&#22330;&#35782;&#21035;&#30340;&#26041;&#27861;&#12290;&#25945;&#31243;&#36890;&#36807;&#23454;&#20363;&#20195;&#30721;&#21644;&#21487;&#35270;&#21270;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#23569;&#26679;&#26412;ChatGPT&#21644;FLAN-T5&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#23545;BERT&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#30340;&#25351;&#23548;&#12290;&#36825;&#20123;&#25945;&#31243;&#20351;&#23398;&#20064;&#32773;&#33021;&#22815;&#25484;&#25569;&#36816;&#29992;&#20808;&#36827;&#26041;&#27861;&#36827;&#34892;&#31435;&#22330;&#35782;&#21035;&#30340;&#23454;&#36341;&#32463;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20004;&#20010;&#29420;&#31435;&#30340;&#25945;&#31243;&#65292;&#20171;&#32461;&#20102;&#20351;&#29992;BERT&#24494;&#35843;&#21644;&#25552;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;Twitter&#25968;&#25454;&#19978;&#36827;&#34892;&#31435;&#22330;&#35782;&#21035;&#12290;&#31532;&#19968;&#20010;&#25945;&#31243;&#35299;&#37322;&#20102;BERT&#30340;&#26550;&#26500;&#21644;&#20998;&#35789;&#65292;&#25351;&#23548;&#29992;&#25143;&#36890;&#36807;&#20351;&#29992;HuggingFace transformers&#35757;&#32451;&#12289;&#35843;&#20248;&#21644;&#35780;&#20272;&#26631;&#20934;&#21644;&#39046;&#22495;&#29305;&#23450;&#30340;BERT&#27169;&#22411;&#12290;&#31532;&#20108;&#20010;&#25945;&#31243;&#20391;&#37325;&#20110;&#26500;&#24314;&#25552;&#31034;&#21644;&#23569;&#26679;&#26412;&#31034;&#20363;&#65292;&#20174;ChatGPT&#21644;&#24320;&#28304;FLAN-T5&#20013;&#24341;&#20986;&#31435;&#22330;&#32780;&#26080;&#38656;&#36827;&#34892;&#24494;&#35843;&#12290;&#37319;&#29992;&#20102;&#21508;&#31181;&#25552;&#31034;&#31574;&#30053;&#65292;&#24182;&#20351;&#29992;&#28151;&#28102;&#30697;&#38453;&#21644;&#23439;F1&#20998;&#25968;&#36827;&#34892;&#35780;&#20272;&#12290;&#36825;&#20123;&#25945;&#31243;&#25552;&#20379;&#20102;&#20195;&#30721;&#12289;&#21487;&#35270;&#21270;&#21644;&#27934;&#23519;&#21147;&#65292;&#25581;&#31034;&#20102;&#23569;&#26679;&#26412;ChatGPT&#21644;FLAN-T5&#30340;&#20248;&#21183;&#65292;&#23427;&#20204;&#32988;&#36807;&#20102;&#24494;&#35843;&#30340;BERT&#12290;&#36890;&#36807;&#20197;&#26131;&#20110;&#29702;&#35299;&#12289;&#23454;&#36341;&#20026;&#23548;&#21521;&#30340;&#26041;&#24335;&#21516;&#26102;&#28085;&#30422;&#27169;&#22411;&#24494;&#35843;&#21644;&#25552;&#31034;&#25216;&#26415;&#65292;&#36825;&#20123;&#25945;&#31243;&#20351;&#23398;&#20064;&#32773;&#33021;&#22815;&#33719;&#24471;&#23545;&#31435;&#22330;&#26816;&#27979;&#30340;&#23574;&#31471;&#26041;&#27861;&#30340;&#23454;&#38469;&#32463;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents two self-contained tutorials on stance detection in Twitter data using BERT fine-tuning and prompting large language models (LLMs). The first tutorial explains BERT architecture and tokenization, guiding users through training, tuning, and evaluating standard and domain-specific BERT models with HuggingFace transformers. The second focuses on constructing prompts and few-shot examples to elicit stances from ChatGPT and open-source FLAN-T5 without fine-tuning. Various prompting strategies are implemented and evaluated using confusion matrices and macro F1 scores. The tutorials provide code, visualizations, and insights revealing the strengths of few-shot ChatGPT and FLAN-T5 which outperform fine-tuned BERTs. By covering both model fine-tuning and prompting-based techniques in an accessible, hands-on manner, these tutorials enable learners to gain applied experience with cutting-edge methods for stance detection.
&lt;/p&gt;</description></item><item><title>TrafficSafetyGPT&#26159;&#19968;&#31181;&#36890;&#36807;&#23558;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#39046;&#22495;&#29305;&#23450;&#30340;&#30417;&#30563;&#24494;&#35843;&#65292;&#20197;&#22312;&#20132;&#36890;&#23433;&#20840;&#39046;&#22495;&#20219;&#21153;&#20013;&#25552;&#20379;&#20934;&#30830;&#21709;&#24212;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.15311</link><description>&lt;p&gt;
TrafficSafetyGPT&#65306;&#23558;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35843;&#25972;&#21040;&#20132;&#36890;&#23433;&#20840;&#39046;&#22495;&#30340;&#19987;&#23478;
&lt;/p&gt;
&lt;p&gt;
TrafficSafetyGPT: Tuning a Pre-trained Large Language Model to a Domain-Specific Expert in Transportation Safety. (arXiv:2307.15311v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15311
&lt;/p&gt;
&lt;p&gt;
TrafficSafetyGPT&#26159;&#19968;&#31181;&#36890;&#36807;&#23558;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#39046;&#22495;&#29305;&#23450;&#30340;&#30417;&#30563;&#24494;&#35843;&#65292;&#20197;&#22312;&#20132;&#36890;&#23433;&#20840;&#39046;&#22495;&#20219;&#21153;&#20013;&#25552;&#20379;&#20934;&#30830;&#21709;&#24212;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#36890;&#29992;&#39046;&#22495;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#25928;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#20132;&#36890;&#23433;&#20840;&#39046;&#22495;&#30340;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#19981;&#20339;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#38656;&#35201;&#19987;&#38376;&#30340;&#20132;&#36890;&#23433;&#20840;&#19987;&#19994;&#30693;&#35782;&#26469;&#20135;&#29983;&#20934;&#30830;&#30340;&#21709;&#24212;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;TrafficSafetyGPT&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#65292;&#32463;&#36807;&#20102;&#20351;&#29992;TrafficSafety-2K&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#30417;&#30563;&#24494;&#35843;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#25919;&#24220;&#20986;&#29256;&#30340;&#25351;&#21335;&#20070;&#21644;ChatGPT&#29983;&#25104;&#30340;&#25351;&#23548;-&#36755;&#20986;&#23545;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;TrafficSafetyGPT&#27169;&#22411;&#21644;TrafficSafety-2K&#35757;&#32451;&#25968;&#25454;&#38598;&#21487;&#20197;&#22312;https://github.com/ozheng1993/TrafficSafetyGPT&#19978;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown remarkable effectiveness in various general-domain natural language processing (NLP) tasks. However, their performance in transportation safety domain tasks has been suboptimal, primarily attributed to the requirement for specialized transportation safety expertise in generating accurate responses [1]. To address this challenge, we introduce TrafficSafetyGPT, a novel LLAMA-based model, which has undergone supervised fine-tuning using TrafficSafety-2K dataset which has human labels from government produced guiding books and ChatGPT-generated instruction-output pairs. Our proposed TrafficSafetyGPT model and TrafficSafety-2K train dataset are accessible at https://github.com/ozheng1993/TrafficSafetyGPT.
&lt;/p&gt;</description></item><item><title>WC-SBERT&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;SBERT&#21644;&#33258;&#35757;&#32451;&#35299;&#20915;&#38646;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32500;&#22522;&#30334;&#31185;&#20316;&#20026;&#35757;&#32451;&#38598;&#21644;&#24314;&#31435;&#31867;&#21035;&#23545;&#20043;&#38388;&#30340;&#27491;&#30456;&#20851;&#20851;&#31995;&#65292;&#23454;&#29616;&#24555;&#36895;&#19988;&#20934;&#30830;&#30340;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2307.15293</link><description>&lt;p&gt;
WC-SBERT: &#21033;&#29992;SBERT&#21644;&#33258;&#35757;&#32451;&#35299;&#20915;&#38646;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#38382;&#39064;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
WC-SBERT: Zero-Shot Text Classification via SBERT with Self-Training for Wikipedia Categories. (arXiv:2307.15293v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15293
&lt;/p&gt;
&lt;p&gt;
WC-SBERT&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;SBERT&#21644;&#33258;&#35757;&#32451;&#35299;&#20915;&#38646;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32500;&#22522;&#30334;&#31185;&#20316;&#20026;&#35757;&#32451;&#38598;&#21644;&#24314;&#31435;&#31867;&#21035;&#23545;&#20043;&#38388;&#30340;&#27491;&#30456;&#20851;&#20851;&#31995;&#65292;&#23454;&#29616;&#24555;&#36895;&#19988;&#20934;&#30830;&#30340;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#19987;&#27880;&#20110;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#38646;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#29305;&#21035;&#24378;&#35843;&#21019;&#26032;&#30340;&#33258;&#35757;&#32451;&#31574;&#30053;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#35757;&#32451;&#31574;&#30053;&#65292;&#20351;&#29992;&#26631;&#31614;&#32780;&#38750;&#25991;&#26412;&#36827;&#34892;&#35757;&#32451;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#27169;&#22411;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;&#32500;&#22522;&#30334;&#31185;&#30340;&#31867;&#21035;&#20316;&#20026;&#35757;&#32451;&#38598;&#65292;&#24182;&#21033;&#29992;SBERT&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#21516;&#19968;&#25991;&#26412;&#20013;&#30340;&#31867;&#21035;&#23545;&#20043;&#38388;&#24314;&#31435;&#27491;&#30456;&#20851;&#20851;&#31995;&#65292;&#20415;&#20110;&#36827;&#34892;&#32852;&#24819;&#35757;&#32451;&#12290;&#23545;&#20110;&#26032;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#21407;&#22987;&#33258;&#35757;&#32451;&#26041;&#27861;&#65292;&#28040;&#38500;&#20102;&#38656;&#35201;&#27599;&#20010;&#30446;&#26631;&#25968;&#25454;&#38598;&#30340;&#20808;&#21069;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#37319;&#29992;&#32500;&#22522;&#30334;&#31185;&#20316;&#20026;&#32479;&#19968;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#26356;&#22909;&#22320;&#36817;&#20284;&#38646;&#26679;&#26412;&#24773;&#22659;&#12290;&#36825;&#31181;&#20462;&#25913;&#26041;&#24335;&#21487;&#20197;&#24555;&#36895;&#36827;&#34892;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#24494;&#35843;&#21644;&#25512;&#26029;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#33258;&#35757;&#32451;&#25152;&#38656;&#30340;&#26102;&#38388;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#27604;&#20197;&#24448;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our research focuses on solving the zero-shot text classification problem in NLP, with a particular emphasis on innovative self-training strategies. To achieve this objective, we propose a novel self-training strategy that uses labels rather than text for training, significantly reducing the model's training time. Specifically, we use categories from Wikipedia as our training set and leverage the SBERT pre-trained model to establish positive correlations between pairs of categories within the same text, facilitating associative training. For new test datasets, we have improved the original self-training approach, eliminating the need for prior training and testing data from each target dataset. Instead, we adopt Wikipedia as a unified training dataset to better approximate the zero-shot scenario. This modification allows for rapid fine-tuning and inference across different datasets, greatly reducing the time required for self-training. Our experimental results demonstrate that this met
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;ChatHome&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#23478;&#23621;&#35013;&#20462;&#39046;&#22495;&#30340;&#29305;&#23450;&#39046;&#22495;&#35821;&#35328;&#27169;&#22411;&#65288;DSLM&#65289;&#30340;&#24320;&#21457;&#21644;&#35780;&#20272;&#12290;&#36890;&#36807;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#25972;&#30340;&#26041;&#27861;&#65292;ChatHome&#21487;&#20197;&#29983;&#25104;&#19982;&#23478;&#23621;&#35013;&#20462;&#30456;&#20851;&#30340;&#39640;&#20445;&#30495;&#12289;&#31934;&#30830;&#30340;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2307.15290</link><description>&lt;p&gt;
ChatHome:&#23478;&#23621;&#35013;&#20462;&#39046;&#22495;&#30340;&#29305;&#23450;&#39046;&#22495;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#21457;&#21644;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
ChatHome: Development and Evaluation of a Domain-Specific Language Model for Home Renovation. (arXiv:2307.15290v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15290
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;ChatHome&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#23478;&#23621;&#35013;&#20462;&#39046;&#22495;&#30340;&#29305;&#23450;&#39046;&#22495;&#35821;&#35328;&#27169;&#22411;&#65288;DSLM&#65289;&#30340;&#24320;&#21457;&#21644;&#35780;&#20272;&#12290;&#36890;&#36807;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#25972;&#30340;&#26041;&#27861;&#65292;ChatHome&#21487;&#20197;&#29983;&#25104;&#19982;&#23478;&#23621;&#35013;&#20462;&#30456;&#20851;&#30340;&#39640;&#20445;&#30495;&#12289;&#31934;&#30830;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;ChatHome&#30340;&#24320;&#21457;&#21644;&#35780;&#20272;&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#22797;&#26434;&#30340;&#23478;&#23621;&#35013;&#20462;&#39046;&#22495;&#35774;&#35745;&#30340;&#29305;&#23450;&#39046;&#22495;&#35821;&#35328;&#27169;&#22411;&#65288;DSLM&#65289;&#12290;&#32771;&#34385;&#21040;&#31867;&#20284;GPT-4&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25104;&#29087;&#33021;&#21147;&#20197;&#21450;&#23545;&#23478;&#23621;&#35013;&#20462;&#30340;&#19981;&#26029;&#20852;&#36259;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#29983;&#25104;&#19968;&#20010;&#19987;&#38376;&#27169;&#22411;&#65292;&#20135;&#29983;&#19982;&#23478;&#23621;&#35013;&#20462;&#39046;&#22495;&#30456;&#20851;&#30340;&#39640;&#20445;&#30495;&#12289;&#31934;&#30830;&#30340;&#36755;&#20986;&#26469;&#35299;&#20915;&#36825;&#20123;&#26041;&#38754;&#30340;&#30683;&#30462;&#12290;ChatHome&#30340;&#21019;&#26032;&#22312;&#20110;&#20854;&#26041;&#27861;&#35770;&#65292;&#23558;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#25972;&#34701;&#21512;&#22312;&#19968;&#20010;&#24191;&#27867;&#25968;&#25454;&#38598;&#19978;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#19982;&#23478;&#23621;&#35013;&#20462;&#30456;&#20851;&#30340;&#19987;&#19994;&#25991;&#31456;&#12289;&#26631;&#20934;&#25991;&#26723;&#21644;&#32593;&#32476;&#20869;&#23481;&#12290;&#36825;&#31181;&#21452;&#37325;&#31574;&#30053;&#26088;&#22312;&#30830;&#20445;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#21560;&#25910;&#20840;&#38754;&#30340;&#39046;&#22495;&#30693;&#35782;&#24182;&#26377;&#25928;&#22320;&#35299;&#20915;&#29992;&#25143;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;&#25968;&#25454;&#38598;&#30340;&#24443;&#24213;&#23454;&#39564;&#65292;&#21253;&#25324;&#26032;&#24341;&#20837;&#30340;&#8220;EvalHome&#8221;&#39046;&#22495;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#23454;&#36136;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the development and evaluation of ChatHome, a domain-specific language model (DSLM) designed for the intricate field of home renovation. Considering the proven competencies of large language models (LLMs) like GPT-4 and the escalating fascination with home renovation, this study endeavors to reconcile these aspects by generating a dedicated model that can yield high-fidelity, precise outputs relevant to the home renovation arena. ChatHome's novelty rests on its methodology, fusing domain-adaptive pretraining and instruction-tuning over an extensive dataset. This dataset includes professional articles, standard documents, and web content pertinent to home renovation. This dual-pronged strategy is designed to ensure that our model can assimilate comprehensive domain knowledge and effectively address user inquiries. Via thorough experimentation on diverse datasets, both universal and domain-specific, including the freshly introduced "EvalHome" domain dataset, we substa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37322;&#20041;&#29983;&#25104;&#30340;&#22810;&#35821;&#35328;&#35789;&#27719;&#31616;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#37322;&#20041;&#29983;&#25104;&#22810;&#26679;&#24615;&#30340;&#35789;&#27719;&#26367;&#20195;&#35789;&#24182;&#20445;&#30041;&#21477;&#23376;&#30340;&#24847;&#20041;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#22312;&#33521;&#35821;&#19978;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.15286</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#35789;&#27719;&#31616;&#21270;&#36890;&#36807;&#37322;&#20041;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Multilingual Lexical Simplification via Paraphrase Generation. (arXiv:2307.15286v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37322;&#20041;&#29983;&#25104;&#30340;&#22810;&#35821;&#35328;&#35789;&#27719;&#31616;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#37322;&#20041;&#29983;&#25104;&#22810;&#26679;&#24615;&#30340;&#35789;&#27719;&#26367;&#20195;&#35789;&#24182;&#20445;&#30041;&#21477;&#23376;&#30340;&#24847;&#20041;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#22312;&#33521;&#35821;&#19978;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#35789;&#27719;&#31616;&#21270;&#26041;&#27861;&#22312;&#20998;&#26512;&#35789;&#27719;&#19978;&#19979;&#25991;&#29615;&#22659;&#20013;&#30340;&#28508;&#22312;&#26367;&#20195;&#35789;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#38024;&#23545;&#19981;&#21516;&#35821;&#35328;&#21333;&#29420;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#19988;&#24573;&#35270;&#20102;&#21477;&#23376;&#24847;&#20041;&#30340;&#20445;&#30041;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#37322;&#20041;&#29983;&#25104;&#30340;&#26032;&#39062;&#30340;&#22810;&#35821;&#35328;&#35789;&#27719;&#31616;&#21270;&#26041;&#27861;&#65292;&#22240;&#20026;&#37322;&#20041;&#25552;&#20379;&#20102;&#35789;&#27719;&#36873;&#25321;&#30340;&#22810;&#26679;&#24615;&#21516;&#26102;&#20445;&#30041;&#20102;&#21477;&#23376;&#30340;&#24847;&#20041;&#12290;&#25105;&#20204;&#23558;&#37322;&#20041;&#35270;&#20026;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#38646;&#32763;&#35793;&#20219;&#21153;&#65292;&#25903;&#25345;&#25968;&#30334;&#31181;&#35821;&#35328;&#12290;&#22312;&#37322;&#20041;&#24314;&#27169;&#30340;&#32534;&#30721;&#22120;&#20013;&#36755;&#20837;&#21477;&#23376;&#21518;&#65292;&#25105;&#20204;&#22522;&#20110;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#30721;&#31574;&#30053;&#29983;&#25104;&#26367;&#20195;&#35789;&#65292;&#35813;&#31574;&#30053;&#20165;&#20851;&#27880;&#22797;&#26434;&#35789;&#27719;&#30340;&#35789;&#27719;&#21464;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#33521;&#35821;&#19978;&#26174;&#33879;&#36229;&#36807;&#20102;&#22522;&#20110;BERT&#30340;&#26041;&#27861;&#21644;&#38646;&#32763;&#35793;GPT3-based&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lexical simplification (LS) methods based on pretrained language models have made remarkable progress, generating potential substitutes for a complex word through analysis of its contextual surroundings. However, these methods require separate pretrained models for different languages and disregard the preservation of sentence meaning. In this paper, we propose a novel multilingual LS method via paraphrase generation, as paraphrases provide diversity in word selection while preserving the sentence's meaning. We regard paraphrasing as a zero-shot translation task within multilingual neural machine translation that supports hundreds of languages. After feeding the input sentence into the encoder of paraphrase modeling, we generate the substitutes based on a novel decoding strategy that concentrates solely on the lexical variations of the complex word. Experimental results demonstrate that our approach surpasses BERT-based methods and zero-shot GPT3-based method significantly on English, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30340;&#24320;&#25918;&#38382;&#39064;&#21644;&#22522;&#26412;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#21152;&#24378;&#31038;&#20250;&#30417;&#30563;&#30340;&#23457;&#35745;&#21644;&#25259;&#38706;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2307.15217</link><description>&lt;p&gt;
&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30340;&#24320;&#25918;&#38382;&#39064;&#21644;&#22522;&#26412;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback. (arXiv:2307.15217v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30340;&#24320;&#25918;&#38382;&#39064;&#21644;&#22522;&#26412;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#21152;&#24378;&#31038;&#20250;&#30417;&#30563;&#30340;&#23457;&#35745;&#21644;&#25259;&#38706;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26159;&#19968;&#31181;&#35757;&#32451;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#19982;&#20154;&#31867;&#30446;&#26631;&#20445;&#25345;&#19968;&#33268;&#30340;&#25216;&#26415;&#12290;RLHF&#24050;&#25104;&#20026;&#24494;&#35843;&#26368;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26680;&#24515;&#26041;&#27861;&#12290;&#23613;&#31649;&#22914;&#27492;&#21463;&#27426;&#36814;&#65292;&#20294;&#31995;&#32479;&#24615;&#22320;&#31995;&#32479;&#21270;&#20854;&#32570;&#38519;&#30340;&#20844;&#24320;&#24037;&#20316;&#30456;&#23545;&#36739;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#65288;1&#65289;&#35843;&#26597;&#20102;RLHF&#21450;&#30456;&#20851;&#26041;&#27861;&#30340;&#24320;&#25918;&#38382;&#39064;&#21644;&#22522;&#26412;&#38480;&#21046;&#65307;&#65288;2&#65289;&#27010;&#36848;&#20102;&#20102;&#35299;&#12289;&#25913;&#36827;&#21644;&#34917;&#20805;RLHF&#30340;&#23454;&#36341;&#25216;&#26415;&#65307;&#20197;&#21450;&#65288;3&#65289;&#25552;&#20986;&#20102;&#23457;&#35745;&#21644;&#25259;&#38706;&#26631;&#20934;&#20197;&#25913;&#36827;RLHF&#31995;&#32479;&#30340;&#31038;&#20250;&#30417;&#30563;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24378;&#35843;&#20102;RLHF&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#20197;&#22810;&#26041;&#38754;&#26041;&#27861;&#24320;&#21457;&#26356;&#23433;&#20840;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning from human feedback (RLHF) is a technique for training AI systems to align with human goals. RLHF has emerged as the central method used to finetune state-of-the-art large language models (LLMs). Despite this popularity, there has been relatively little public work systematizing its flaws. In this paper, we (1) survey open problems and fundamental limitations of RLHF and related methods; (2) overview techniques to understand, improve, and complement RLHF in practice; and (3) propose auditing and disclosure standards to improve societal oversight of RLHF systems. Our work emphasizes the limitations of RLHF and highlights the importance of a multi-faceted approach to the development of safer AI systems.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;PromptStyler&#65292;&#36890;&#36807;&#20351;&#29992;&#25552;&#31034;&#21512;&#25104;&#26679;&#24335;&#29305;&#24449;&#65292;&#35299;&#20915;&#20102;&#26080;&#28304;&#22495;&#27867;&#21270;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#26679;&#24335;&#35789;&#21521;&#37327;&#29983;&#25104;&#22810;&#26679;&#30340;&#26679;&#24335;&#65292;&#24182;&#36890;&#36807;&#24378;&#21046;&#26679;&#24335;&#20869;&#23481;&#29305;&#24449;&#19982;&#20869;&#23481;&#29305;&#24449;&#38752;&#36817;&#26469;&#20445;&#35777;&#26679;&#24335;&#19981;&#20250;&#25197;&#26354;&#20869;&#23481;&#20449;&#24687;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.15199</link><description>&lt;p&gt;
PromptStyler&#65306;&#22522;&#20110;&#25552;&#31034;&#30340;&#26080;&#28304;&#22495;&#27867;&#21270;&#39118;&#26684;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
PromptStyler: Prompt-driven Style Generation for Source-free Domain Generalization. (arXiv:2307.15199v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15199
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;PromptStyler&#65292;&#36890;&#36807;&#20351;&#29992;&#25552;&#31034;&#21512;&#25104;&#26679;&#24335;&#29305;&#24449;&#65292;&#35299;&#20915;&#20102;&#26080;&#28304;&#22495;&#27867;&#21270;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#26679;&#24335;&#35789;&#21521;&#37327;&#29983;&#25104;&#22810;&#26679;&#30340;&#26679;&#24335;&#65292;&#24182;&#36890;&#36807;&#24378;&#21046;&#26679;&#24335;&#20869;&#23481;&#29305;&#24449;&#19982;&#20869;&#23481;&#29305;&#24449;&#38752;&#36817;&#26469;&#20445;&#35777;&#26679;&#24335;&#19981;&#20250;&#25197;&#26354;&#20869;&#23481;&#20449;&#24687;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#21512;&#35270;&#35273;&#35821;&#35328;&#31354;&#38388;&#20013;&#65292;&#25991;&#26412;&#29305;&#24449;&#65288;&#22914;&#8220;&#19968;&#24352;&#29399;&#30340;&#29031;&#29255;&#8221;&#65289;&#21487;&#20197;&#26377;&#25928;&#22320;&#34920;&#31034;&#20854;&#30456;&#20851;&#30340;&#22270;&#20687;&#29305;&#24449;&#65288;&#22914;&#29399;&#30340;&#29031;&#29255;&#65289;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PromptStyler&#65292;&#36890;&#36807;&#20351;&#29992;&#25552;&#31034;&#26469;&#21512;&#25104;&#21508;&#31181;&#26679;&#24335;&#65292;&#32780;&#19981;&#20351;&#29992;&#20219;&#20309;&#22270;&#20687;&#26469;&#22788;&#29702;&#26080;&#28304;&#22495;&#27867;&#21270;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#26679;&#24335;&#35789;&#21521;&#37327;&#20026;&#20266;&#35789;S*&#29983;&#25104;&#22810;&#26679;&#30340;&#26679;&#24335;&#29305;&#24449;&#65288;&#22914;&#8220;a S* style of a&#8221;&#65289;&#12290;&#20026;&#20102;&#30830;&#20445;&#23398;&#20064;&#21040;&#30340;&#26679;&#24335;&#19981;&#20250;&#25197;&#26354;&#20869;&#23481;&#20449;&#24687;&#65292;&#25105;&#20204;&#24378;&#21046;&#35201;&#27714;&#26679;&#24335;&#20869;&#23481;&#29305;&#24449;&#65288;&#22914;&#8220;a S* style of a [class]&#8221;&#65289;&#22312;&#32852;&#21512;&#35270;&#35273;&#35821;&#35328;&#31354;&#38388;&#20013;&#38752;&#36817;&#20854;&#23545;&#24212;&#30340;&#20869;&#23481;&#29305;&#24449;&#65288;&#22914;&#8220;[class]&#8221;&#65289;&#12290;&#22312;&#23398;&#20064;&#26679;&#24335;&#35789;&#21521;&#37327;&#20043;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#30340;&#26679;&#24335;&#20869;&#23481;&#29305;&#24449;&#35757;&#32451;&#19968;&#20010;&#32447;&#24615;&#20998;&#31867;&#22120;&#12290;&#23613;&#31649;PromptStyler&#19981;&#38656;&#35201;&#20351;&#29992;&#20219;&#20309;&#22270;&#20687;&#65292;&#24182;&#19988;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#65292;&#20294;&#22312;PACS&#12289;VLCS&#12289;OfficeHome&#21644;DomainNet&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a joint vision-language space, a text feature (e.g., from "a photo of a dog") could effectively represent its relevant image features (e.g., from dog photos). Inspired by this, we propose PromptStyler which simulates various distribution shifts in the joint space by synthesizing diverse styles via prompts without using any images to deal with source-free domain generalization. Our method learns to generate a variety of style features (from "a S* style of a") via learnable style word vectors for pseudo-words S*. To ensure that learned styles do not distort content information, we force style-content features (from "a S* style of a [class]") to be located nearby their corresponding content features (from "[class]") in the joint vision-language space. After learning style word vectors, we train a linear classifier using synthesized style-content features. PromptStyler achieves the state of the art on PACS, VLCS, OfficeHome and DomainNet, although it does not require any images and take
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;f-DISTILL&#26694;&#26550;&#65292;&#23558;&#24207;&#21015;&#32423;&#30693;&#35782;&#33976;&#39311;&#24314;&#27169;&#20026;&#26368;&#23567;&#21270;&#24191;&#20041;f-&#20998;&#27495;&#20989;&#25968;&#12290;&#36890;&#36807;&#22312;&#35789;&#32423;&#19978;&#35745;&#31639;&#25439;&#22833;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#21387;&#32553;&#35821;&#35328;&#27169;&#22411;&#24182;&#20351;&#23398;&#29983;&#27169;&#22411;&#20174;&#25945;&#24072;&#27169;&#22411;&#20013;&#23398;&#20064;&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2307.15190</link><description>&lt;p&gt;
f-Divergence&#26368;&#23567;&#21270;&#29992;&#20110;&#24207;&#21015;&#32423;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
f-Divergence Minimization for Sequence-Level Knowledge Distillation. (arXiv:2307.15190v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15190
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;f-DISTILL&#26694;&#26550;&#65292;&#23558;&#24207;&#21015;&#32423;&#30693;&#35782;&#33976;&#39311;&#24314;&#27169;&#20026;&#26368;&#23567;&#21270;&#24191;&#20041;f-&#20998;&#27495;&#20989;&#25968;&#12290;&#36890;&#36807;&#22312;&#35789;&#32423;&#19978;&#35745;&#31639;&#25439;&#22833;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#21387;&#32553;&#35821;&#35328;&#27169;&#22411;&#24182;&#20351;&#23398;&#29983;&#27169;&#22411;&#20174;&#25945;&#24072;&#27169;&#22411;&#20013;&#23398;&#20064;&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#26159;&#23558;&#30693;&#35782;&#20174;&#22823;&#27169;&#22411;&#36716;&#31227;&#21040;&#23567;&#27169;&#22411;&#30340;&#36807;&#31243;&#12290;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#30001;&#20110;&#23545;&#19981;&#26029;&#22686;&#38271;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21387;&#32553;&#30340;&#38656;&#27714;&#65292;&#23427;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;f-DISTILL&#26694;&#26550;&#65292;&#23558;&#24207;&#21015;&#32423;&#30693;&#35782;&#33976;&#39311;&#24314;&#27169;&#20026;&#26368;&#23567;&#21270;&#24191;&#20041;f-&#20998;&#27495;&#20989;&#25968;&#12290;&#25105;&#20204;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#19979;&#25552;&#20986;&#20102;&#22235;&#31181;&#33976;&#39311;&#21464;&#31181;&#65292;&#24182;&#34920;&#26126;&#29616;&#26377;&#30340; SeqKD &#21644; ENGINE &#26041;&#27861;&#26159;&#25105;&#20204;f-DISTILL&#26041;&#27861;&#30340;&#36817;&#20284;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25512;&#23548;&#20986;&#20102;&#25105;&#20204;&#30340;f-DISTILL&#30340;&#36880;&#27493;&#20998;&#35299;&#65292;&#23558;&#38590;&#20197;&#22788;&#29702;&#30340;&#24207;&#21015;&#32423;&#20998;&#27495;&#31616;&#21270;&#20026;&#21487;&#20197;&#20197;&#19968;&#31181;&#21487;&#22788;&#29702;&#30340;&#26041;&#24335;&#35745;&#31639;&#30340;&#35789;&#32423;&#25439;&#22833;&#12290;&#22312;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;KD&#26041;&#27861;&#65292;&#24182;&#19988;&#25105;&#20204;&#23545;&#31216;&#30340;&#33976;&#39311;&#25439;&#22833;&#21487;&#20197;&#26356;&#22909;&#22320;&#24378;&#36843;&#23398;&#29983;&#20174;&#25945;&#24072;&#20998;&#24067;&#20013;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation (KD) is the process of transferring knowledge from a large model to a small one. It has gained increasing attention in the natural language processing community, driven by the demands of compressing ever-growing language models. In this work, we propose an f-DISTILL framework, which formulates sequence-level knowledge distillation as minimizing a generalized f-divergence function. We propose four distilling variants under our framework and show that existing SeqKD and ENGINE approaches are approximations of our f-DISTILL methods. We further derive step-wise decomposition for our f-DISTILL, reducing intractable sequence-level divergence to word-level losses that can be computed in a tractable manner. Experiments across four datasets show that our methods outperform existing KD approaches, and that our symmetric distilling losses can better force the student to learn from the teacher distribution.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RCT&#25298;&#32477;&#25277;&#26679;&#30340;&#26032;&#25277;&#26679;&#31639;&#27861;&#65292;&#29992;&#20110;&#22240;&#26524;&#20272;&#35745;&#35780;&#20272;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23376;&#25277;&#26679;&#38543;&#26426;&#25511;&#21046;&#35797;&#39564;(RCT)&#21019;&#24314;&#28151;&#28102;&#30340;&#35266;&#27979;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;RCT&#30340;&#24179;&#22343;&#22240;&#26524;&#25928;&#24212;&#20316;&#20026;&#22522;&#20934;&#30495;&#23454;&#20540;&#65292;&#20197;&#36827;&#34892;&#26377;&#25928;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2307.15176</link><description>&lt;p&gt;
RCT&#25298;&#32477;&#25277;&#26679;&#29992;&#20110;&#22240;&#26524;&#20272;&#35745;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
RCT Rejection Sampling for Causal Estimation Evaluation. (arXiv:2307.15176v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15176
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RCT&#25298;&#32477;&#25277;&#26679;&#30340;&#26032;&#25277;&#26679;&#31639;&#27861;&#65292;&#29992;&#20110;&#22240;&#26524;&#20272;&#35745;&#35780;&#20272;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23376;&#25277;&#26679;&#38543;&#26426;&#25511;&#21046;&#35797;&#39564;(RCT)&#21019;&#24314;&#28151;&#28102;&#30340;&#35266;&#27979;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;RCT&#30340;&#24179;&#22343;&#22240;&#26524;&#25928;&#24212;&#20316;&#20026;&#22522;&#20934;&#30495;&#23454;&#20540;&#65292;&#20197;&#36827;&#34892;&#26377;&#25928;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#28102;&#26159;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#26080;&#20559;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#30340;&#19968;&#20010;&#37325;&#35201;&#38556;&#30861;&#12290;&#23545;&#20110;&#39640;&#32500;&#21327;&#21464;&#37327;&#30340;&#24773;&#20917;&#65292;&#22914;&#25991;&#26412;&#25968;&#25454;&#12289;&#22522;&#22240;&#32452;&#23398;&#25110;&#34892;&#20026;&#31038;&#20250;&#31185;&#23398;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#36866;&#24212;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#22240;&#26524;&#20272;&#35745;&#30340;&#35843;&#25972;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35843;&#25972;&#26041;&#27861;&#30340;&#32463;&#39564;&#35780;&#20272;&#19968;&#30452;&#23384;&#22312;&#22256;&#38590;&#21644;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#32463;&#39564;&#35780;&#20272;&#31574;&#30053;&#65292;&#31616;&#21270;&#20102;&#35780;&#20272;&#35774;&#35745;&#65292;&#24182;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#65306;&#23545;&#38543;&#26426;&#25511;&#21046;&#35797;&#39564;(RCT)&#36827;&#34892;&#23376;&#25277;&#26679;&#65292;&#20197;&#21019;&#24314;&#28151;&#28102;&#30340;&#35266;&#27979;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#20351;&#29992;RCT&#30340;&#24179;&#22343;&#22240;&#26524;&#25928;&#24212;&#20316;&#20026;&#22522;&#20934;&#30495;&#23454;&#20540;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25277;&#26679;&#31639;&#27861;&#65292;&#31216;&#20026;RCT&#25298;&#32477;&#25277;&#26679;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#65292;&#20197;&#30830;&#20445;&#35266;&#27979;&#25968;&#25454;&#30340;&#22240;&#26524;&#35782;&#21035;&#25104;&#31435;&#65292;&#20174;&#32780;&#21487;&#20197;&#19982;&#22522;&#20934;RCT&#36827;&#34892;&#26377;&#25928;&#27604;&#36739;&#12290;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;...
&lt;/p&gt;
&lt;p&gt;
Confounding is a significant obstacle to unbiased estimation of causal effects from observational data. For settings with high-dimensional covariates -- such as text data, genomics, or the behavioral social sciences -researchers have proposed methods to adjust for confounding by adapting machine learning methods to the goal of causal estimation. However, empirical evaluation of these adjustment methods has been challenging and limited. In this work, we build on a promising empirical evaluation strategy that simplifies evaluation design and uses real data: subsampling randomized controlled trials (RCTs) to create confounded observational datasets while using the average causal effects from the RCTs as ground-truth. We contribute a new sampling algorithm, which we call RCT rejection sampling, and provide theoretical guarantees that causal identification holds in the observational data to allow for valid comparisons to the ground-truth RCT. Using synthetic data, we show our algorithm in
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#35789;&#23884;&#20837;&#34920;&#31034;&#30340;&#31574;&#30053;&#26469;&#25429;&#25417;&#22797;&#26434;&#23545;&#35805;&#20013;&#24773;&#32490;&#34920;&#36798;&#30340;&#24494;&#22937;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#22312;&#24773;&#32490;&#26816;&#27979;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#65292;&#24182;&#22312;&#23567;&#26679;&#26412;&#21644;&#19981;&#24179;&#34913;&#30340;&#28151;&#21512;&#30446;&#26631;&#24773;&#32490;&#25968;&#25454;&#38598;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2307.15164</link><description>&lt;p&gt;
VISU&#21442;&#21152;WASSA 2023&#20849;&#20139;&#20219;&#21153;&#65306;&#21033;&#29992;BERT&#21644;&#22534;&#21472;&#23884;&#20837;&#26816;&#27979;&#23545;&#26032;&#38395;&#25925;&#20107;&#30340;&#24773;&#32490;&#21453;&#24212;
&lt;/p&gt;
&lt;p&gt;
VISU at WASSA 2023 Shared Task: Detecting Emotions in Reaction to News Stories Leveraging BERT and Stacked Embeddings. (arXiv:2307.15164v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#35789;&#23884;&#20837;&#34920;&#31034;&#30340;&#31574;&#30053;&#26469;&#25429;&#25417;&#22797;&#26434;&#23545;&#35805;&#20013;&#24773;&#32490;&#34920;&#36798;&#30340;&#24494;&#22937;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#22312;&#24773;&#32490;&#26816;&#27979;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#65292;&#24182;&#22312;&#23567;&#26679;&#26412;&#21644;&#19981;&#24179;&#34913;&#30340;&#28151;&#21512;&#30446;&#26631;&#24773;&#32490;&#25968;&#25454;&#38598;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#31995;&#32479;VISU&#21442;&#21152;&#20102;WASSA 2023&#20849;&#20139;&#20219;&#21153;&#65288;3&#65289;&#65292;&#21363;&#20174;&#23545;&#26032;&#38395;&#25991;&#31456;&#30340;&#21453;&#24212;&#20013;&#20889;&#30340;&#25991;&#31456;&#20013;&#36827;&#34892;&#24773;&#32490;&#20998;&#31867;&#12290;&#20174;&#22797;&#26434;&#23545;&#35805;&#20013;&#26816;&#27979;&#24773;&#32490;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#36890;&#24120;&#38656;&#35201;&#19978;&#19979;&#25991;/&#39046;&#22495;&#30340;&#29702;&#35299;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#24320;&#21457;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#65292;&#20351;&#29992;&#23450;&#21046;&#30340;&#39044;&#22788;&#29702;&#31574;&#30053;&#19982;&#35789;&#23884;&#20837;&#34920;&#31034;&#30340;&#32452;&#21512;&#26469;&#25429;&#25417;&#34920;&#36798;&#30340;&#24773;&#32490;&#30340;&#24494;&#22937;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#20351;&#29992;&#20102;&#38745;&#24577;&#21644;&#19978;&#19979;&#25991;&#23884;&#20837;&#65288;&#21333;&#29420;&#21644;&#22534;&#21472;&#65289;&#19982;&#21452;&#21521;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;BiLSTM&#65289;&#21644;Transformer&#27169;&#22411;&#12290;&#22312;&#24773;&#32490;&#26816;&#27979;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#21344;&#25454;&#20102;&#31532;&#21313;&#21517;&#65292;&#24471;&#20998;&#20026;0.2717&#30340;&#23439;F1-&#20998;&#25968;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#23454;&#26045;&#30340;&#26041;&#27861;&#22312;&#23567;&#26679;&#26412;&#21644;&#19981;&#24179;&#34913;&#30340;&#28151;&#21512;&#30446;&#26631;&#24773;&#32490;&#25968;&#25454;&#38598;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our system, VISU, participated in the WASSA 2023 Shared Task (3) of Emotion Classification from essays written in reaction to news articles. Emotion detection from complex dialogues is challenging and often requires context/domain understanding. Therefore in this research, we have focused on developing deep learning (DL) models using the combination of word embedding representations with tailored prepossessing strategies to capture the nuances of emotions expressed. Our experiments used static and contextual embeddings (individual and stacked) with Bidirectional Long short-term memory (BiLSTM) and Transformer based models. We occupied rank tenth in the emotion detection task by scoring a Macro F1-Score of 0.2717, validating the efficacy of our implemented approaches for small and imbalanced datasets with mixed categories of target emotions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32423;&#32852;&#36328;&#27169;&#24577;Transformer&#27169;&#22411;&#65292;&#32467;&#21512;&#35821;&#38899;&#21644;&#25991;&#26412;&#36716;&#24405;&#65292;&#29992;&#20110;&#26816;&#27979;&#30005;&#35805;&#23545;&#35805;&#20013;&#30340;&#23458;&#25143;&#35831;&#27714;&#21644;&#25237;&#35785;&#12290;&#22312;ACM Multimedia 2023&#35745;&#31639;&#35821;&#35328;&#23398;&#25361;&#25112;&#36187;&#30340;&#35831;&#27714;&#23376;&#25361;&#25112;&#20013;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#25237;&#35785;&#21644;&#35831;&#27714;&#31867;&#21035;&#20013;&#36798;&#21040;&#20102;65.41%&#21644;85.87%&#30340;&#19981;&#24179;&#34913;&#24179;&#22343;&#21484;&#22238;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.15097</link><description>&lt;p&gt;
&#32423;&#32852;&#36328;&#27169;&#24577;Transformer&#29992;&#20110;&#35831;&#27714;&#21644;&#25237;&#35785;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Cascaded Cross-Modal Transformer for Request and Complaint Detection. (arXiv:2307.15097v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15097
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32423;&#32852;&#36328;&#27169;&#24577;Transformer&#27169;&#22411;&#65292;&#32467;&#21512;&#35821;&#38899;&#21644;&#25991;&#26412;&#36716;&#24405;&#65292;&#29992;&#20110;&#26816;&#27979;&#30005;&#35805;&#23545;&#35805;&#20013;&#30340;&#23458;&#25143;&#35831;&#27714;&#21644;&#25237;&#35785;&#12290;&#22312;ACM Multimedia 2023&#35745;&#31639;&#35821;&#35328;&#23398;&#25361;&#25112;&#36187;&#30340;&#35831;&#27714;&#23376;&#25361;&#25112;&#20013;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#25237;&#35785;&#21644;&#35831;&#27714;&#31867;&#21035;&#20013;&#36798;&#21040;&#20102;65.41%&#21644;85.87%&#30340;&#19981;&#24179;&#34913;&#24179;&#22343;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32423;&#32852;&#36328;&#27169;&#24577;Transformer&#65288;CCMT&#65289;&#65292;&#23558;&#35821;&#38899;&#21644;&#25991;&#26412;&#36716;&#24405;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#26816;&#27979;&#30005;&#35805;&#23545;&#35805;&#20013;&#30340;&#23458;&#25143;&#35831;&#27714;&#21644;&#25237;&#35785;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#36716;&#24405;&#35821;&#38899;&#65292;&#24182;&#23558;&#36716;&#24405;&#20214;&#32763;&#35793;&#25104;&#19981;&#21516;&#35821;&#35328;&#26469;&#21033;&#29992;&#22810;&#27169;&#24577;&#33539;&#24335;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23558;&#35821;&#35328;&#29305;&#23450;&#30340;&#22522;&#20110;BERT&#30340;&#27169;&#22411;&#19982;Wav2Vec2.0&#38899;&#39057;&#29305;&#24449;&#22312;&#19968;&#20010;&#26032;&#39062;&#30340;&#32423;&#32852;&#20132;&#21449;&#27880;&#24847;&#21147;Transformer&#27169;&#22411;&#20013;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#31995;&#32479;&#24212;&#29992;&#20110;ACM Multimedia 2023&#35745;&#31639;&#35821;&#35328;&#23398;&#25361;&#25112;&#36187;&#30340;&#35831;&#27714;&#23376;&#25361;&#25112;&#20013;&#65292;&#22312;&#25237;&#35785;&#21644;&#35831;&#27714;&#31867;&#21035;&#20013;&#20998;&#21035;&#36798;&#21040;&#20102;65.41%&#21644;85.87%&#30340;&#19981;&#24179;&#34913;&#24179;&#22343;&#21484;&#22238;&#29575;&#65288;UAR&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel cascaded cross-modal transformer (CCMT) that combines speech and text transcripts to detect customer requests and complaints in phone conversations. Our approach leverages a multimodal paradigm by transcribing the speech using automatic speech recognition (ASR) models and translating the transcripts into different languages. Subsequently, we combine language-specific BERT-based models with Wav2Vec2.0 audio features in a novel cascaded cross-attention transformer model. We apply our system to the Requests Sub-Challenge of the ACM Multimedia 2023 Computational Paralinguistics Challenge, reaching unweighted average recalls (UAR) of 65.41% and 85.87% for the complaint and request classes, respectively.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#21335;&#38750;&#30123;&#33495;&#29369;&#35947;&#30456;&#20851;&#25512;&#25991;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#65292;&#26816;&#27979;&#20986;COVID-19&#30123;&#33495;&#29369;&#35947;&#24773;&#32490;&#30340;&#23384;&#22312;&#24182;&#23545;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#36827;&#34892;&#20998;&#31867;&#12290;&#36825;&#23545;&#20110;&#24212;&#23545;&#30123;&#33495;&#29369;&#35947;&#23545;&#20844;&#20849;&#21355;&#29983;&#24037;&#20316;&#30340;&#24433;&#21709;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2307.15072</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#20174;&#21335;&#38750;Twitter&#25968;&#25454;&#20013;&#26816;&#27979;COVID-19&#30123;&#33495;&#29369;&#35947;&#24773;&#32490;&#30340;&#23384;&#22312;
&lt;/p&gt;
&lt;p&gt;
Detecting the Presence of COVID-19 Vaccination Hesitancy from South African Twitter Data Using Machine Learning. (arXiv:2307.15072v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15072
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#21335;&#38750;&#30123;&#33495;&#29369;&#35947;&#30456;&#20851;&#25512;&#25991;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#65292;&#26816;&#27979;&#20986;COVID-19&#30123;&#33495;&#29369;&#35947;&#24773;&#32490;&#30340;&#23384;&#22312;&#24182;&#23545;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#36827;&#34892;&#20998;&#31867;&#12290;&#36825;&#23545;&#20110;&#24212;&#23545;&#30123;&#33495;&#29369;&#35947;&#23545;&#20844;&#20849;&#21355;&#29983;&#24037;&#20316;&#30340;&#24433;&#21709;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;COVID-19&#22823;&#27969;&#34892;&#26399;&#38388;&#65292;&#20851;&#20110;&#21335;&#38750;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#30340;&#31038;&#20132;&#23186;&#20307;&#30740;&#31350;&#38750;&#24120;&#23569;&#65292;&#20351;&#29992;&#25163;&#21160;&#26631;&#27880;&#26041;&#27861;&#26356;&#26159;&#20964;&#27611;&#40607;&#35282;&#12290;&#30123;&#33495;&#25509;&#31181;&#26159;&#23545;&#25239;&#30123;&#24773;&#30340;&#20027;&#35201;&#25163;&#27573;&#65292;&#20294;&#30123;&#33495;&#29369;&#35947;&#21361;&#21450;&#20844;&#20849;&#21355;&#29983;&#24037;&#20316;&#12290;&#26412;&#30740;&#31350;&#23545;&#19982;&#30123;&#33495;&#29369;&#35947;&#26377;&#20851;&#30340;&#21335;&#38750;&#25512;&#25991;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#65292;&#26088;&#22312;&#35757;&#32451;AI&#23186;&#20171;&#30340;&#20998;&#31867;&#27169;&#22411;&#65292;&#24182;&#35780;&#20272;&#20854;&#22312;&#20998;&#31867;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#26102;&#30340;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#25552;&#21462;&#20102;&#26469;&#33258;&#21335;&#38750;&#30340;3&#19975;&#26465;&#25512;&#25991;&#65292;&#24182;&#23558;&#20854;&#25163;&#21160;&#26631;&#27880;&#20026;&#19977;&#20010;&#24773;&#24863;&#31867;&#21035;&#20043;&#19968;&#65306;&#31215;&#26497;&#12289;&#28040;&#26497;&#12289;&#20013;&#24615;&#12290;&#20351;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21253;&#25324;LSTM&#12289;&#21452;&#21521;LSTM&#12289;SVM&#12289;BERT-base-cased&#21644;RoBERTa-base&#27169;&#22411;&#65292;&#36890;&#36807;WandB&#24179;&#21488;&#31934;&#24515;&#36873;&#25321;&#21644;&#35843;&#25972;&#20102;&#23427;&#20204;&#30340;&#36229;&#21442;&#25968;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#39044;&#22788;&#29702;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65306;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#65292;&#21478;&#19968;&#31181;&#22522;&#20110;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
Very few social media studies have been done on South African user-generated content during the COVID-19 pandemic and even fewer using hand-labelling over automated methods. Vaccination is a major tool in the fight against the pandemic, but vaccine hesitancy jeopardizes any public health effort. In this study, sentiment analysis on South African tweets related to vaccine hesitancy was performed, with the aim of training AI-mediated classification models and assessing their reliability in categorizing UGC. A dataset of 30000 tweets from South Africa were extracted and hand-labelled into one of three sentiment classes: positive, negative, neutral. The machine learning models used were LSTM, bi-LSTM, SVM, BERT-base-cased and the RoBERTa-base models, whereby their hyperparameters were carefully chosen and tuned using the WandB platform. We used two different approaches when we pre-processed our data for comparison: one was semantics-based, while the other was corpus-based. The pre-processi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;&#23569;&#37327;&#31034;&#20363;&#26469;&#23454;&#29616;&#36866;&#24212;&#26032;&#20316;&#23478;&#30340;&#25163;&#20889;&#25991;&#26412;&#35782;&#21035;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;ResNet&#39592;&#24178;&#32593;&#32476;&#21644;LSTM&#25110;Transformer&#24207;&#21015;&#35299;&#30721;&#22120;&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#32467;&#21512;&#27169;&#22411;&#19981;&#21487;&#30693;&#20803;&#23398;&#20064;&#21644;&#20316;&#23478;&#20195;&#30721;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20316;&#23478;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.15071</link><description>&lt;p&gt;
&#36866;&#24212;&#31163;&#32447;&#25991;&#26412;&#35782;&#21035;&#30340;&#20316;&#23478;&#36866;&#24212;&#24615;&#65306;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Writer adaptation for offline text recognition: An exploration of neural network-based methods. (arXiv:2307.15071v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;&#23569;&#37327;&#31034;&#20363;&#26469;&#23454;&#29616;&#36866;&#24212;&#26032;&#20316;&#23478;&#30340;&#25163;&#20889;&#25991;&#26412;&#35782;&#21035;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;ResNet&#39592;&#24178;&#32593;&#32476;&#21644;LSTM&#25110;Transformer&#24207;&#21015;&#35299;&#30721;&#22120;&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#32467;&#21512;&#27169;&#22411;&#19981;&#21487;&#30693;&#20803;&#23398;&#20064;&#21644;&#20316;&#23478;&#20195;&#30721;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20316;&#23478;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#25163;&#20889;&#35782;&#21035;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#19968;&#20010;&#25345;&#20037;&#19981;&#36275;&#20043;&#22788;&#26159;&#23427;&#20204;&#19981;&#25797;&#38271;&#22788;&#29702;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#21270;&#12290;&#22312;&#25163;&#20889;&#25991;&#26412;&#35782;&#21035;&#39046;&#22495;&#65292;&#36825;&#34920;&#29616;&#20026;&#23545;&#20110;&#19982;&#35757;&#32451;&#20013;&#25152;&#35265;&#21040;&#30340;&#20316;&#23478;&#19981;&#30456;&#20284;&#30340;&#20316;&#23478;&#65292;&#35782;&#21035;&#20934;&#30830;&#29575;&#36739;&#20302;&#12290;&#29702;&#24819;&#30340;&#25163;&#20889;&#25991;&#26412;&#35782;&#21035;&#27169;&#22411;&#24212;&#35813;&#33021;&#22815;&#36866;&#24212;&#26032;&#30340;&#20070;&#20889;&#39118;&#26684;&#65292;&#20197;&#22788;&#29702;&#22823;&#37327;&#21487;&#33021;&#30340;&#20070;&#20889;&#39118;&#26684;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#20165;&#20351;&#29992;&#23569;&#37327;&#26469;&#33258;&#26032;&#20316;&#23478;&#30340;&#31034;&#20363;&#65288;&#20363;&#22914;16&#20010;&#31034;&#20363;&#65289;&#26469;&#20351;&#25163;&#20889;&#25991;&#26412;&#35782;&#21035;&#27169;&#22411;&#20855;&#26377;&#20316;&#23478;&#36866;&#24212;&#24615;&#12290;&#22522;&#20110;ResNet&#39592;&#24178;&#32593;&#32476;&#21644;LSTM&#25110;Transformer&#24207;&#21015;&#35299;&#30721;&#22120;&#65292;&#20351;&#29992;&#20102;&#20004;&#31181;HTR&#26550;&#26500;&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#12290;&#20351;&#29992;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#65292;&#32771;&#34385;&#20102;&#20004;&#31181;&#20351;&#23427;&#20204;&#20855;&#26377;&#20316;&#23478;&#36866;&#24212;&#24615;&#30340;&#26041;&#27861;&#65306;1&#65289;&#27169;&#22411;&#19981;&#21487;&#30693;&#20803;&#23398;&#20064;&#65288;MAML&#65289;&#65292;&#24120;&#29992;&#20110;&#23569;&#26679;&#26412;&#20998;&#31867;&#31561;&#20219;&#21153;&#30340;&#31639;&#27861;&#65292;&#21644;2&#65289;&#20316;&#23478;&#20195;&#30721;
&lt;/p&gt;
&lt;p&gt;
Handwriting recognition has seen significant success with the use of deep learning. However, a persistent shortcoming of neural networks is that they are not well-equipped to deal with shifting data distributions. In the field of handwritten text recognition (HTR), this shows itself in poor recognition accuracy for writers that are not similar to those seen during training. An ideal HTR model should be adaptive to new writing styles in order to handle the vast amount of possible writing styles. In this paper, we explore how HTR models can be made writer adaptive by using only a handful of examples from a new writer (e.g., 16 examples) for adaptation. Two HTR architectures are used as base models, using a ResNet backbone along with either an LSTM or Transformer sequence decoder. Using these base models, two methods are considered to make them writer adaptive: 1) model-agnostic meta-learning (MAML), an algorithm commonly used for tasks such as few-shot classification, and 2) writer codes
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#24110;&#21161;&#24739;&#32773;&#21644;&#36716;&#35786;&#21307;&#29983;&#35782;&#21035;&#21512;&#36866;&#30340;&#20020;&#24202;&#35797;&#39564;&#30340;&#28508;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;TrialGPT&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#21512;&#26684;&#24615;&#24182;&#25552;&#20379;&#35299;&#37322;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.15051</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#24739;&#32773;&#19982;&#20020;&#24202;&#35797;&#39564;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Matching Patients to Clinical Trials with Large Language Models. (arXiv:2307.15051v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#24110;&#21161;&#24739;&#32773;&#21644;&#36716;&#35786;&#21307;&#29983;&#35782;&#21035;&#21512;&#36866;&#30340;&#20020;&#24202;&#35797;&#39564;&#30340;&#28508;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;TrialGPT&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#21512;&#26684;&#24615;&#24182;&#25552;&#20379;&#35299;&#37322;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#35797;&#39564;&#22312;&#25512;&#21160;&#33647;&#29289;&#30740;&#21457;&#21644;&#22522;&#20110;&#35777;&#25454;&#30340;&#21307;&#23398;&#26041;&#38754;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#24739;&#32773;&#25307;&#21215;&#24120;&#24120;&#21463;&#21040;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#24110;&#21161;&#24739;&#32773;&#21644;&#36716;&#35786;&#21307;&#29983;&#35782;&#21035;&#21512;&#36866;&#30340;&#20020;&#24202;&#35797;&#39564;&#30340;&#28508;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;TrialGPT&#65292;&#37319;&#29992;LLMs&#39044;&#27979;&#22522;&#20110;&#26631;&#20934;&#30340;&#21512;&#26684;&#24615;&#65292;&#24182;&#25552;&#20379;&#35814;&#32454;&#30340;&#35299;&#37322;&#65292;&#24182;&#26681;&#25454;&#24739;&#32773;&#30149;&#21382;&#20013;&#30340;&#33258;&#30001;&#25991;&#26412;&#26469;&#23545;&#20505;&#36873;&#20020;&#24202;&#35797;&#39564;&#36827;&#34892;&#25490;&#21517;&#21644;&#25490;&#38500;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;184&#21517;&#24739;&#32773;&#21644;18,238&#20010;&#27880;&#37322;&#30340;&#20020;&#24202;&#35797;&#39564;&#30340;&#38431;&#21015;&#19978;&#35780;&#20272;&#20102;TrialGPT&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20960;&#20010;&#20851;&#38190;&#21457;&#29616;&#65306;&#31532;&#19968;&#65292;TrialGPT&#22312;&#26631;&#20934;&#32423;&#21035;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#25552;&#20379;&#20934;&#30830;&#30340;&#35299;&#37322;&#12290;&#31532;&#20108;&#65292;TrialGPT&#30340;&#32508;&#21512;&#35797;&#39564;&#32423;&#21035;&#35780;&#20998;&#19982;&#19987;&#23478;&#26631;&#27880;&#30340;&#21512;&#26684;&#24615;&#39640;&#24230;&#30456;&#20851;&#12290;&#31532;&#19977;&#65292;&#36825;&#20123;&#35780;&#20998;
&lt;/p&gt;
&lt;p&gt;
Clinical trials are vital in advancing drug development and evidence-based medicine, but their success is often hindered by challenges in patient recruitment. In this work, we investigate the potential of large language models (LLMs) to assist individual patients and referral physicians in identifying suitable clinical trials from an extensive selection. Specifically, we introduce TrialGPT, a novel architecture employing LLMs to predict criterion-level eligibility with detailed explanations, which are then aggregated for ranking and excluding candidate clinical trials based on free-text patient notes. We evaluate TrialGPT on three publicly available cohorts of 184 patients and 18,238 annotated clinical trials. The experimental results demonstrate several key findings: First, TrialGPT achieves high criterion-level prediction accuracy with faithful explanations. Second, the aggregated trial-level TrialGPT scores are highly correlated with expert eligibility annotations. Third, these scor
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#39318;&#27425;&#23558;&#27597;&#35821;&#35782;&#21035;&#24212;&#29992;&#20110;&#22303;&#32819;&#20854;&#35821;,&#36890;&#36807;&#20998;&#26512;&#20316;&#32773;&#19981;&#21516;&#35821;&#35328;&#30340;&#20889;&#20316;&#26469;&#39044;&#27979;&#20316;&#32773;&#30340;&#27597;&#35821;&#12290;&#30740;&#31350;&#20351;&#29992;&#20102;&#22303;&#32819;&#20854;&#23398;&#20064;&#32773;&#35821;&#26009;&#24211;&#21644;&#19977;&#20010;&#21477;&#27861;&#29305;&#24449;&#26469;&#23637;&#31034;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.14850</link><description>&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#39318;&#27425;&#23558;&#27597;&#35821;&#35782;&#21035;&#65288;Native Language Identification&#65292;NLI&#65289;&#24212;&#29992;&#20110;&#22303;&#32819;&#20854;&#35821;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Turkish Native Language Identification. (arXiv:2307.14850v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14850
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#39318;&#27425;&#23558;&#27597;&#35821;&#35782;&#21035;&#24212;&#29992;&#20110;&#22303;&#32819;&#20854;&#35821;,&#36890;&#36807;&#20998;&#26512;&#20316;&#32773;&#19981;&#21516;&#35821;&#35328;&#30340;&#20889;&#20316;&#26469;&#39044;&#27979;&#20316;&#32773;&#30340;&#27597;&#35821;&#12290;&#30740;&#31350;&#20351;&#29992;&#20102;&#22303;&#32819;&#20854;&#23398;&#20064;&#32773;&#35821;&#26009;&#24211;&#21644;&#19977;&#20010;&#21477;&#27861;&#29305;&#24449;&#26469;&#23637;&#31034;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23558;&#27597;&#35821;&#35782;&#21035;&#65288;NLI&#65289;&#24212;&#29992;&#20110;&#22303;&#32819;&#20854;&#35821;&#12290;NLI &#26159;&#36890;&#36807;&#20998;&#26512;&#20316;&#32773;&#19981;&#21516;&#35821;&#35328;&#30340;&#20889;&#20316;&#26469;&#39044;&#27979;&#20316;&#32773;&#30340;&#27597;&#35821;&#12290;&#23613;&#31649;&#22823;&#22810;&#25968;NLI&#30740;&#31350;&#37117;&#20391;&#37325;&#20110;&#33521;&#35821;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#23558;&#20854;&#33539;&#22260;&#25193;&#23637;&#21040;&#22303;&#32819;&#20854;&#35821;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#26368;&#36817;&#26500;&#24314;&#30340;&#22303;&#32819;&#20854;&#23398;&#20064;&#32773;&#35821;&#26009;&#24211;&#65292;&#24182;&#32467;&#21512;&#20102;&#19977;&#20010;&#21477;&#27861;&#29305;&#24449;&#65288;CFG &#20135;&#29983;&#35268;&#21017;&#65292;&#35789;&#24615;n-gram&#21644;&#20989;&#25968;&#35789;&#65289;&#19982;L2&#25991;&#26412;&#65292;&#20197;&#23637;&#31034;&#23427;&#20204;&#22312;&#35813;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present the first application of Native Language Identification (NLI) for the Turkish language. NLI involves predicting the writer's first language by analysing their writing in different languages. While most NLI research has focused on English, our study extends its scope to Turkish. We used the recently constructed Turkish Learner Corpus and employed a combination of three syntactic features (CFG production rules, part-of-speech n-grams and function words) with L2 texts to demonstrate their effectiveness in this task.
&lt;/p&gt;</description></item><item><title>ARB&#26159;&#19968;&#20010;&#26032;&#22411;&#22522;&#20934;&#65292;&#21253;&#21547;&#20102;&#25968;&#23398;&#12289;&#29289;&#29702;&#12289;&#29983;&#29289;&#12289;&#21270;&#23398;&#21644;&#27861;&#24459;&#39046;&#22495;&#30340;&#39640;&#32423;&#25512;&#29702;&#38382;&#39064;&#12290;&#30446;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#24471;&#20998;&#36828;&#20302;&#20110;50%&#65292;&#20026;&#20102;&#25552;&#39640;&#35780;&#20272;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#35780;&#20998;&#26631;&#20934;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.13692</link><description>&lt;p&gt;
ARB&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#32423;&#25512;&#29702;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
ARB: Advanced Reasoning Benchmark for Large Language Models. (arXiv:2307.13692v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13692
&lt;/p&gt;
&lt;p&gt;
ARB&#26159;&#19968;&#20010;&#26032;&#22411;&#22522;&#20934;&#65292;&#21253;&#21547;&#20102;&#25968;&#23398;&#12289;&#29289;&#29702;&#12289;&#29983;&#29289;&#12289;&#21270;&#23398;&#21644;&#27861;&#24459;&#39046;&#22495;&#30340;&#39640;&#32423;&#25512;&#29702;&#38382;&#39064;&#12290;&#30446;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#24471;&#20998;&#36828;&#20302;&#20110;50%&#65292;&#20026;&#20102;&#25552;&#39640;&#35780;&#20272;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#35780;&#20998;&#26631;&#20934;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#23450;&#37327;&#25512;&#29702;&#21644;&#30693;&#35782;&#22522;&#20934;&#19978;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22312;&#36825;&#20123;&#39046;&#22495;&#20013;&#36824;&#27809;&#26377;&#36798;&#21040;&#19987;&#23478;&#27700;&#24179;&#65292;&#20294;&#35768;&#22810;&#36825;&#20123;&#22522;&#20934;&#38543;&#30528;LLMs&#33719;&#24471;&#36234;&#26469;&#36234;&#39640;&#30340;&#20998;&#25968;&#32780;&#22833;&#21435;&#20102;&#25928;&#29992;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;ARB&#65292;&#19968;&#20010;&#30001;&#22810;&#20010;&#39046;&#22495;&#30340;&#39640;&#32423;&#25512;&#29702;&#38382;&#39064;&#32452;&#25104;&#30340;&#26032;&#22411;&#22522;&#20934;&#12290;ARB&#25552;&#20379;&#27604;&#20197;&#21069;&#30340;&#22522;&#20934;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#27979;&#35797;&#65292;&#21253;&#25324;&#25968;&#23398;&#12289;&#29289;&#29702;&#12289;&#29983;&#29289;&#12289;&#21270;&#23398;&#21644;&#27861;&#24459;&#39046;&#22495;&#30340;&#38382;&#39064;&#12290;&#20316;&#20026;ARB&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#32452;&#25361;&#25112;&#24615;&#30340;&#25968;&#23398;&#21644;&#29289;&#29702;&#38382;&#39064;&#65292;&#38656;&#35201;&#39640;&#32423;&#31526;&#21495;&#25512;&#29702;&#21644;&#39046;&#22495;&#30693;&#35782;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#26368;&#36817;&#30340;&#27169;&#22411;&#65292;&#22914;GPT-4&#21644;Claude&#22312;ARB&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#35777;&#26126;&#24403;&#21069;&#27169;&#22411;&#22312;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#19978;&#24471;&#20998;&#36828;&#20302;&#20110;50%&#12290;&#20026;&#20102;&#25913;&#36827;&#33258;&#21160;&#21644;&#36741;&#21161;&#35780;&#20272;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#35780;&#20998;&#26631;&#20934;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20801;&#35768;GPT-4&#23545;&#20854;&#33258;&#36523;&#30340;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#35780;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable performance on various quantitative reasoning and knowledge benchmarks. However, many of these benchmarks are losing utility as LLMs get increasingly high scores, despite not yet reaching expert performance in these domains. We introduce ARB, a novel benchmark composed of advanced reasoning problems in multiple fields. ARB presents a more challenging test than prior benchmarks, featuring problems in mathematics, physics, biology, chemistry, and law. As a subset of ARB, we introduce a challenging set of math and physics problems which require advanced symbolic reasoning and domain knowledge. We evaluate recent models such as GPT-4 and Claude on ARB and demonstrate that current models score well below 50% on more demanding tasks. In order to improve both automatic and assisted evaluation capabilities, we introduce a rubric-based evaluation approach, allowing GPT-4 to score its own intermediate reasoning steps. Further, we conduct 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;DSTC 11 Track 4&#20013;&#38024;&#23545;&#24320;&#25918;&#22495;&#23545;&#35805;&#31995;&#32479;&#36827;&#34892;&#40065;&#26834;&#24615;&#21644;&#22810;&#35821;&#35328;&#33258;&#21160;&#35780;&#20272;&#30340;&#25361;&#25112;&#65292;&#20171;&#32461;&#20102;&#25552;&#20379;&#32473;&#21442;&#19982;&#32773;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#32447;&#65292;&#24182;&#24635;&#32467;&#20102;&#34920;&#29616;&#26368;&#20339;&#30340;&#31995;&#32479;&#21450;&#20854;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.12794</link><description>&lt;p&gt;
DSTC 11 Track 4&#20013;&#29992;&#20110;&#24320;&#25918;&#22495;&#23545;&#35805;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#21644;&#22810;&#35821;&#35328;&#33258;&#21160;&#35780;&#20272;&#24230;&#37327;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Overview of Robust and Multilingual Automatic Evaluation Metrics for Open-Domain Dialogue Systems at DSTC 11 Track 4. (arXiv:2306.12794v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;DSTC 11 Track 4&#20013;&#38024;&#23545;&#24320;&#25918;&#22495;&#23545;&#35805;&#31995;&#32479;&#36827;&#34892;&#40065;&#26834;&#24615;&#21644;&#22810;&#35821;&#35328;&#33258;&#21160;&#35780;&#20272;&#30340;&#25361;&#25112;&#65292;&#20171;&#32461;&#20102;&#25552;&#20379;&#32473;&#21442;&#19982;&#32773;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#32447;&#65292;&#24182;&#24635;&#32467;&#20102;&#34920;&#29616;&#26368;&#20339;&#30340;&#31995;&#32479;&#21450;&#20854;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#20986;&#29616;&#21644;&#24555;&#36895;&#21457;&#23637;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#23545;&#35805;&#31995;&#32479;&#30340;&#30740;&#31350;&#65292;&#24182;&#38543;&#20043;&#24341;&#21457;&#20102;&#20851;&#20110;&#20854;&#33258;&#21160;&#35780;&#20272;&#30340;&#21508;&#31181;&#25361;&#25112;&#12290;&#24320;&#25918;&#22495;&#23545;&#35805;&#31995;&#32479;&#30340;&#33258;&#21160;&#35780;&#20272;&#20316;&#20026;&#19968;&#20010;&#24320;&#25918;&#24615;&#25361;&#25112;&#24050;&#32463;&#24341;&#36215;&#20102;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#30340;&#20851;&#27880;&#12290;&#23613;&#31649;&#19968;&#30452;&#22312;&#21162;&#21147;&#25552;&#39640;&#33258;&#21160;&#35780;&#20272;&#24230;&#37327;&#19982;&#20154;&#31867;&#35780;&#20272;&#30340;&#30456;&#20851;&#24615;&#65292;&#20294;&#24456;&#23569;&#26377;&#23581;&#35797;&#35780;&#20272;&#23427;&#20204;&#22312;&#22810;&#20010;&#39046;&#22495;&#21644;&#32500;&#24230;&#19978;&#30340;&#40065;&#26834;&#24615;&#65292;&#32780;&#19988;&#23427;&#20204;&#30340;&#37325;&#28857;&#20027;&#35201;&#38598;&#20013;&#20110;&#33521;&#35821;&#35821;&#35328;&#19978;&#12290;&#25152;&#26377;&#36825;&#20123;&#25361;&#25112;&#20419;&#36827;&#20102;&#24320;&#21457;&#21487;&#38752;&#30340;&#33258;&#21160;&#35780;&#20272;&#24230;&#37327;&#65292;&#22312;&#21508;&#31181;&#39046;&#22495;&#12289;&#32500;&#24230;&#21644;&#35821;&#35328;&#20013;&#37117;&#33021;&#22815;&#20351;&#29992;&#12290;DSTC11&#20013;&#30340;&#36825;&#20010;&#36712;&#36947;&#26159;&#20419;&#36827;&#40065;&#26834;&#21644;&#22810;&#35821;&#35328;&#33258;&#21160;&#35780;&#20272;&#24230;&#37327;&#30340;&#25345;&#32493;&#21162;&#21147;&#30340;&#19968;&#37096;&#20998;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#25552;&#20379;&#32473;&#21442;&#19982;&#32773;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#32447;&#65292;&#24182;&#35752;&#35770;&#20102;&#35813;&#36712;&#36947;&#30340;&#25552;&#20132;&#21644;&#32467;&#26524;&#32454;&#33410;&#12290;&#26412;&#25991;&#36824;&#24635;&#32467;&#20102;&#34920;&#29616;&#26368;&#20339;&#30340;&#31995;&#32479;&#21450;&#20854;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent and fast development of neural networks have revolutionized the research on dialogue systems and subsequently have triggered various challenges regarding their automatic evaluation. Automatic evaluation of open-domain dialogue systems as an open challenge has been the center of the attention of many researchers. Despite the consistent efforts to improve automatic metrics' correlations with human evaluation, there have been very few attempts to assess their robustness over multiple domains and dimensions. Also, their focus is mainly on the English language. All of these challenges prompt the development of automatic evaluation metrics that are reliable in various domains, dimensions, and languages. This track in the 11th Dialogue System Technology Challenge (DSTC11) is part of the ongoing effort to promote robust and multilingual automatic evaluation metrics. This article describes the datasets and baselines provided to participants and discusses the submission and result det
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#35821;&#38899;&#35782;&#21035;&#20013;&#30005;&#35805;&#20998;&#31867;&#30340;&#38382;&#39064;&#65292;&#24182;&#25506;&#32034;&#20102;&#20960;&#32452;&#21487;&#20197;&#29992;&#20110;&#30005;&#35805;&#20998;&#31867;&#30340;&#26412;&#22320;&#35889;&#26102;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;Haar&#29305;&#24449;&#21644;SVM&#20998;&#31867;&#30340;&#26799;&#24230;&#30452;&#26041;&#22270;&#36827;&#34892;&#30005;&#35805;&#20998;&#31867;&#65292;&#24182;&#32473;&#20986;&#20102;&#19968;&#20123;&#21021;&#27493;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.10270</link><description>&lt;p&gt;
&#22686;&#24378;&#26412;&#22320;&#35889;&#26102;&#29305;&#24449;&#29992;&#20110;&#35821;&#38899;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Boosting Local Spectro-Temporal Features for Speech Analysis. (arXiv:2305.10270v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10270
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#35821;&#38899;&#35782;&#21035;&#20013;&#30005;&#35805;&#20998;&#31867;&#30340;&#38382;&#39064;&#65292;&#24182;&#25506;&#32034;&#20102;&#20960;&#32452;&#21487;&#20197;&#29992;&#20110;&#30005;&#35805;&#20998;&#31867;&#30340;&#26412;&#22320;&#35889;&#26102;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;Haar&#29305;&#24449;&#21644;SVM&#20998;&#31867;&#30340;&#26799;&#24230;&#30452;&#26041;&#22270;&#36827;&#34892;&#30005;&#35805;&#20998;&#31867;&#65292;&#24182;&#32473;&#20986;&#20102;&#19968;&#20123;&#21021;&#27493;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#22312;&#35821;&#38899;&#35782;&#21035;&#20013;&#65292;&#30005;&#35805;&#20998;&#31867;&#30340;&#38382;&#39064;&#65292;&#24182;&#25506;&#32034;&#20102;&#20960;&#32452;&#21487;&#20197;&#29992;&#20110;&#30005;&#35805;&#20998;&#31867;&#30340;&#26412;&#22320;&#35889;&#26102;&#29305;&#24449;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#20004;&#32452;&#24120;&#29992;&#20110;&#29289;&#20307;&#26816;&#27979;&#30340;&#29305;&#24449;&#65288;Haar&#29305;&#24449;&#21644;SVM&#20998;&#31867;&#30340;&#26799;&#24230;&#30452;&#26041;&#22270;&#65288;HoG&#65289;&#65289;&#36827;&#34892;&#30005;&#35805;&#20998;&#31867;&#30340;&#19968;&#20123;&#21021;&#27493;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the problem of phone classification in the context of speech recognition, and explore several sets of local spectro-temporal features that can be used for phone classification. In particular, we present some preliminary results for phone classification using two sets of features that are commonly used for object detection: Haar features and SVM-classified Histograms of Gradients (HoG)
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;Distantly-Supervised Named Entity Recognition&#20013;&#38169;&#35823;&#21644;&#19981;&#23436;&#25972;&#26631;&#27880;&#22122;&#22768;&#30340;&#20998;&#31163;&#31574;&#30053;&#65292;&#20351;&#29992;&#19981;&#21516;&#30340;&#27169;&#22411;&#26500;&#24314;&#26469;&#24212;&#23545;&#20004;&#31181;&#31867;&#22411;&#30340;&#22122;&#22768;&#12290;</title><link>http://arxiv.org/abs/2305.04076</link><description>&lt;p&gt;
SANTA&#65306;Distantly-Supervised Named Entity Recognition&#20013;&#22788;&#29702;&#38169;&#35823;&#21644;&#19981;&#23436;&#25972;&#26631;&#27880;&#22122;&#22768;&#30340;&#20998;&#31163;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
SANTA: Separate Strategies for Inaccurate and Incomplete Annotation Noise in Distantly-Supervised Named Entity Recognition. (arXiv:2305.04076v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04076
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;Distantly-Supervised Named Entity Recognition&#20013;&#38169;&#35823;&#21644;&#19981;&#23436;&#25972;&#26631;&#27880;&#22122;&#22768;&#30340;&#20998;&#31163;&#31574;&#30053;&#65292;&#20351;&#29992;&#19981;&#21516;&#30340;&#27169;&#22411;&#26500;&#24314;&#26469;&#24212;&#23545;&#20004;&#31181;&#31867;&#22411;&#30340;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36828;&#31243;&#30417;&#30563;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26377;&#25928;&#22320;&#20943;&#36731;&#20102;&#30417;&#30563;&#35774;&#32622;&#20013;&#32791;&#26102;&#19988;&#26114;&#36149;&#30340;&#27880;&#37322;&#36127;&#25285;&#65292;&#20294;&#26159;&#26080;&#19978;&#19979;&#25991;&#30340;&#21305;&#37197;&#36807;&#31243;&#21644;&#30693;&#35782;&#24211;&#30340;&#26377;&#38480;&#35206;&#30422;&#24341;&#20837;&#20102;&#19981;&#20934;&#30830;&#21644;&#19981;&#23436;&#25972;&#30340;&#26631;&#27880;&#22122;&#38899;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#19981;&#21516;&#30340;&#31574;&#30053;&#26469;&#22788;&#29702;&#20004;&#31181;&#31867;&#22411;&#30340;&#22122;&#22768;&#30340;SANTA&#65292;&#20197;&#35299;&#20915;&#30001;&#19981;&#20934;&#30830;&#21644;&#19981;&#23436;&#25972;&#26631;&#27880;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distantly-Supervised Named Entity Recognition effectively alleviates the burden of time-consuming and expensive annotation in the supervised setting. But the context-free matching process and the limited coverage of knowledge bases introduce inaccurate and incomplete annotation noise respectively. Previous studies either considered only incomplete annotation noise or indiscriminately handle two types of noise with the same strategy. In this paper, we argue that the different causes of two types of noise bring up the requirement of different strategies in model architecture. Therefore, we propose the SANTA to handle these two types of noise separately with (1) Memory-smoothed Focal Loss and Entity-aware KNN to relieve the entity ambiguity problem caused by inaccurate annotation, and (2) Boundary Mixup to alleviate decision boundary shifting problem caused by incomplete annotation and a noise-tolerant loss to improve the robustness. Benefiting from our separate tailored strategies, we co
&lt;/p&gt;</description></item><item><title>VISAR&#26159;&#19968;&#20010;AI&#20889;&#20316;&#21161;&#25163;&#65292;&#26088;&#22312;&#24110;&#21161;&#20316;&#32773;&#25552;&#21319;&#20889;&#20316;&#20307;&#39564;&#21644;&#36755;&#20986;&#12290;&#23427;&#21487;&#20197;&#22312;&#20889;&#20316;&#19978;&#19979;&#25991;&#20013;&#38543;&#26102;&#24110;&#21161;&#20316;&#32773;&#26500;&#24605;&#21644;&#20462;&#25913;&#30446;&#26631;&#65292;&#36890;&#36807;&#21487;&#35270;&#21270;&#32534;&#31243;&#26469;&#32452;&#32455;&#35770;&#35777;&#32467;&#26500;&#65292;&#24182;&#25552;&#20379;&#25512;&#33616;&#26469;&#22686;&#21152;&#35828;&#26381;&#21147;&#12290;&#33258;&#21160;&#33609;&#26696;&#21407;&#22411;&#21487;&#20197;&#29992;&#26469;&#39564;&#35777;&#35745;&#21010;&#12290;</title><link>http://arxiv.org/abs/2304.07810</link><description>&lt;p&gt;
VISAR&#65306;&#19968;&#31181;&#24102;&#26377;&#21487;&#35270;&#21270;&#32534;&#31243;&#21644;&#24555;&#36895;&#33609;&#26696;&#21407;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#35770;&#35777;&#20889;&#20316;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
VISAR: A Human-AI Argumentative Writing Assistant with Visual Programming and Rapid Draft Prototyping. (arXiv:2304.07810v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07810
&lt;/p&gt;
&lt;p&gt;
VISAR&#26159;&#19968;&#20010;AI&#20889;&#20316;&#21161;&#25163;&#65292;&#26088;&#22312;&#24110;&#21161;&#20316;&#32773;&#25552;&#21319;&#20889;&#20316;&#20307;&#39564;&#21644;&#36755;&#20986;&#12290;&#23427;&#21487;&#20197;&#22312;&#20889;&#20316;&#19978;&#19979;&#25991;&#20013;&#38543;&#26102;&#24110;&#21161;&#20316;&#32773;&#26500;&#24605;&#21644;&#20462;&#25913;&#30446;&#26631;&#65292;&#36890;&#36807;&#21487;&#35270;&#21270;&#32534;&#31243;&#26469;&#32452;&#32455;&#35770;&#35777;&#32467;&#26500;&#65292;&#24182;&#25552;&#20379;&#25512;&#33616;&#26469;&#22686;&#21152;&#35828;&#26381;&#21147;&#12290;&#33258;&#21160;&#33609;&#26696;&#21407;&#22411;&#21487;&#20197;&#29992;&#26469;&#39564;&#35777;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36777;&#35770;&#20889;&#20316;&#20013;&#65292;&#20316;&#32773;&#24517;&#39035;&#26500;&#24605;&#20998;&#23618;&#20889;&#20316;&#30446;&#26631;&#65292;&#30830;&#20445;&#20854;&#35770;&#28857;&#30340;&#35828;&#26381;&#21147;&#65292;&#24182;&#36890;&#36807;&#36215;&#33609;&#26469;&#20462;&#35746;&#21644;&#32452;&#32455;&#20182;&#20204;&#30340;&#35745;&#21010;&#12290;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#20351;&#24471;&#36890;&#36807;&#32842;&#22825;&#30028;&#38754;&#36827;&#34892;&#20132;&#20114;&#24335;&#25991;&#26412;&#29983;&#25104;&#65288;&#20363;&#22914;ChatGPT&#65289;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#24120;&#24120;&#24573;&#30053;&#20102;&#38544;&#21547;&#30340;&#20889;&#20316;&#19978;&#19979;&#25991;&#21644;&#29992;&#25143;&#24847;&#22270;&#65292;&#32570;&#20047;&#29992;&#25143;&#25511;&#21046;&#21644;&#33258;&#20027;&#26435;&#65292;&#24182;&#19988;&#25552;&#20379;&#26377;&#38480;&#30340;&#24110;&#21161;&#26469;&#36827;&#34892;&#24847;&#20041;&#26500;&#24314;&#21644;&#20462;&#35746;&#20889;&#20316;&#35745;&#21010;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;VISAR&#65292;&#19968;&#31181;AI&#25903;&#25345;&#30340;&#20889;&#20316;&#21161;&#25163;&#31995;&#32479;&#65292;&#26088;&#22312;&#24110;&#21161;&#20316;&#32773;&#22312;&#20854;&#20889;&#20316;&#19978;&#19979;&#25991;&#20013;&#26500;&#24605;&#21644;&#20462;&#35746;&#20998;&#23618;&#30446;&#26631;&#65292;&#36890;&#36807;&#21516;&#27493;&#25991;&#26412;&#32534;&#36753;&#21644;&#21487;&#35270;&#21270;&#32534;&#31243;&#32452;&#32455;&#35770;&#35777;&#32467;&#26500;&#65292;&#24182;&#36890;&#36807;&#35770;&#35777;&#28779;&#33457;&#25512;&#33616;&#22686;&#24378;&#35828;&#26381;&#21147;&#12290;VISAR&#20801;&#35768;&#29992;&#25143;&#20351;&#29992;&#33258;&#21160;&#33609;&#26696;&#21407;&#22411;&#25506;&#32034;&#12289;&#23454;&#39564;&#21644;&#39564;&#35777;&#20182;&#20204;&#30340;&#20889;&#20316;&#35745;&#21010;&#12290;&#19968;&#20010;&#21463;&#25511;&#23454;&#39564;&#23460;&#30740;&#31350;&#35777;&#23454;&#65292;VISAR&#21487;&#20197;&#36890;&#36807;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20272;&#65292;&#26377;&#25928;&#22320;&#25913;&#21892;&#29992;&#25143;&#30340;&#20889;&#20316;&#20307;&#39564;&#21644;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In argumentative writing, writers must brainstorm hierarchical writing goals, ensure the persuasiveness of their arguments, and revise and organize their plans through drafting. Recent advances in large language models (LLMs) have made interactive text generation through a chat interface (e.g., ChatGPT) possible. However, this approach often neglects implicit writing context and user intent, lacks support for user control and autonomy, and provides limited assistance for sensemaking and revising writing plans. To address these challenges, we introduce VISAR, an AI-enabled writing assistant system designed to help writers brainstorm and revise hierarchical goals within their writing context, organize argument structures through synchronized text editing and visual programming, and enhance persuasiveness with argumentation spark recommendations. VISAR allows users to explore, experiment with, and validate their writing plans using automatic draft prototyping. A controlled lab study confi
&lt;/p&gt;</description></item><item><title>&#26368;&#36817;&#30740;&#31350;&#34920;&#26126;&#65292;&#24320;&#25918;&#39046;&#22495;&#32842;&#22825;&#26426;&#22120;&#20154;&#36890;&#36807;&#25552;&#20379;&#26368;&#23569;&#20449;&#24687;&#26469;&#23454;&#29616;&#26368;&#22823;&#21270;&#30340;&#8220;&#24320;&#25918;&#24615;&#8221;&#65292;&#22312;&#23454;&#36341;&#20013;&#23548;&#33268;&#20102;&#8220;&#24320;&#25918;&#39046;&#22495;&#24726;&#35770;&#8221;&#8212;&#8212;-&#35201;&#27714;&#29992;&#25143;&#8220;&#38386;&#32842;&#20219;&#20309;&#20107;&#24773;&#8221;&#20250;&#23548;&#33268;&#38750;&#24120;&#29421;&#31364;&#30340;&#23545;&#35805;&#24418;&#24335;&#12290;&#27492;&#30740;&#31350;&#36827;&#19968;&#27493;&#35299;&#37322;&#20102;&#20849;&#21516;&#22522;&#30784;&#29702;&#35770;&#19982;&#23454;&#29616;&#31867;&#20284;&#20154;&#31867;&#23545;&#35805;&#30340;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#29616;&#20849;&#21516;&#22522;&#30784;&#30340;&#36335;&#24452;&#12290;</title><link>http://arxiv.org/abs/2303.11708</link><description>&lt;p&gt;
&#24320;&#25918;&#39046;&#22495;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#24726;&#35770;&#65306;&#20849;&#21516;&#22522;&#30784;&#26159;&#23454;&#29616;&#20154;&#31867;&#23545;&#35805;&#30340;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
The Open-domain Paradox for Chatbots: Common Ground as the Basis for Human-like Dialogue. (arXiv:2303.11708v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11708
&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30740;&#31350;&#34920;&#26126;&#65292;&#24320;&#25918;&#39046;&#22495;&#32842;&#22825;&#26426;&#22120;&#20154;&#36890;&#36807;&#25552;&#20379;&#26368;&#23569;&#20449;&#24687;&#26469;&#23454;&#29616;&#26368;&#22823;&#21270;&#30340;&#8220;&#24320;&#25918;&#24615;&#8221;&#65292;&#22312;&#23454;&#36341;&#20013;&#23548;&#33268;&#20102;&#8220;&#24320;&#25918;&#39046;&#22495;&#24726;&#35770;&#8221;&#8212;&#8212;-&#35201;&#27714;&#29992;&#25143;&#8220;&#38386;&#32842;&#20219;&#20309;&#20107;&#24773;&#8221;&#20250;&#23548;&#33268;&#38750;&#24120;&#29421;&#31364;&#30340;&#23545;&#35805;&#24418;&#24335;&#12290;&#27492;&#30740;&#31350;&#36827;&#19968;&#27493;&#35299;&#37322;&#20102;&#20849;&#21516;&#22522;&#30784;&#29702;&#35770;&#19982;&#23454;&#29616;&#31867;&#20284;&#20154;&#31867;&#23545;&#35805;&#30340;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#29616;&#20849;&#21516;&#22522;&#30784;&#30340;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#25512;&#21160;&#20102;&#24320;&#25918;&#39046;&#22495;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21457;&#23637;&#65292;&#20854;&#8220;&#24320;&#25918;&#24615;&#8221;&#39044;&#35745;&#36890;&#36807;&#21521;&#29992;&#25143;&#25552;&#20379;&#26368;&#23569;&#20449;&#24687;&#65292;&#21253;&#25324;&#20551;&#23450;&#30340;&#20849;&#21516;&#27963;&#21160;&#65292;&#26469;&#23454;&#29616;&#26368;&#22823;&#21270;&#12290;&#28982;&#32780;&#65292;&#35777;&#25454;&#34920;&#26126;&#25928;&#26524;&#30456;&#21453;&#12290;&#35201;&#27714;&#29992;&#25143;&#8220;&#38386;&#32842;&#20219;&#20309;&#20107;&#24773;&#8221;&#20250;&#23548;&#33268;&#38750;&#24120;&#29421;&#31364;&#30340;&#23545;&#35805;&#24418;&#24335;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#24320;&#25918;&#39046;&#22495;&#24726;&#35770;&#8221;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20849;&#21516;&#22522;&#30784;&#29702;&#35770;&#35299;&#37322;&#20102;&#36825;&#20010;&#24726;&#35770;&#20316;&#20026;&#23454;&#29616;&#31867;&#20284;&#20154;&#31867;&#23545;&#35805;&#30340;&#22522;&#30784;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36136;&#30097;&#24320;&#25918;&#39046;&#22495;&#32842;&#22825;&#26426;&#22120;&#20154;&#32972;&#21518;&#30340;&#20551;&#35774;&#65292;&#24182;&#30830;&#23450;&#23454;&#29616;&#20154;&#26426;&#23545;&#35805;&#20013;&#20849;&#21516;&#22522;&#30784;&#30340;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a surge in interest in the development of open-domain chatbots, driven by the recent advancements of large language models. The "openness" of the dialogue is expected to be maximized by providing minimal information to the users about the common ground they can expect, including the presumed joint activity. However, evidence suggests that the effect is the opposite. Asking users to "just chat about anything" results in a very narrow form of dialogue, which we refer to as the "open-domain paradox". In this paper, we explain this paradox through the theory of common ground as the basis for human-like communication. Furthermore, we question the assumptions behind open-domain chatbots and identify paths forward for enabling common ground in human-computer dialogue.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#20449;&#24687;&#20998;&#35299;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#37327;&#21270;&#21644;&#24314;&#27169;&#22810;&#27169;&#24577;&#20132;&#20114;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;PID&#32479;&#35745;&#37327;&#26469;&#24230;&#37327;&#36755;&#20837;&#27169;&#24577;&#19982;&#36755;&#20986;&#20219;&#21153;&#20043;&#38388;&#30340;&#20887;&#20313;&#24230;&#12289;&#29420;&#29305;&#24615;&#21644;&#21327;&#21516;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;PID&#32479;&#35745;&#20272;&#35745;&#22120;&#12290;</title><link>http://arxiv.org/abs/2302.12247</link><description>&lt;p&gt;
&#37327;&#21270;&#21644;&#24314;&#27169;&#22810;&#27169;&#24577;&#20132;&#20114;&#65306;&#19968;&#31181;&#20449;&#24687;&#20998;&#35299;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Quantifying &amp; Modeling Multimodal Interactions: An Information Decomposition Framework. (arXiv:2302.12247v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12247
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#20449;&#24687;&#20998;&#35299;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#37327;&#21270;&#21644;&#24314;&#27169;&#22810;&#27169;&#24577;&#20132;&#20114;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;PID&#32479;&#35745;&#37327;&#26469;&#24230;&#37327;&#36755;&#20837;&#27169;&#24577;&#19982;&#36755;&#20986;&#20219;&#21153;&#20043;&#38388;&#30340;&#20887;&#20313;&#24230;&#12289;&#29420;&#29305;&#24615;&#21644;&#21327;&#21516;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;PID&#32479;&#35745;&#20272;&#35745;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35299;&#20915;&#22810;&#27169;&#24577;&#20219;&#21153;&#25152;&#38656;&#30340;&#20132;&#20114;&#22914;&#20309;&#36827;&#34892;&#37327;&#21270;&#65311;&#26368;&#36866;&#21512;&#25429;&#25417;&#36825;&#20123;&#20132;&#20114;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#26159;&#20160;&#20040;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#35770;&#26041;&#27861;&#26469;&#37327;&#21270;&#36755;&#20837;&#27169;&#24577;&#19982;&#36755;&#20986;&#20219;&#21153;&#20043;&#38388;&#30340;&#20887;&#20313;&#24230;&#12289;&#29420;&#29305;&#24615;&#21644;&#21327;&#21516;&#24615;&#12290;&#25105;&#20204;&#23558;&#36825;&#19977;&#20010;&#34913;&#37327;&#26631;&#20934;&#31216;&#20026;&#22810;&#27169;&#24577;&#20998;&#24067;&#65288;&#25110;&#31616;&#31216;PID&#65289;&#30340;PID&#32479;&#35745;&#37327;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;PID&#32479;&#35745;&#20272;&#35745;&#22120;&#65292;&#36866;&#29992;&#20110;&#39640;&#32500;&#20998;&#24067;&#12290;&#20026;&#20102;&#39564;&#35777;PID&#20272;&#35745;&#65292;&#25105;&#20204;&#22312;&#24050;&#30693;PID&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent explosion of interest in multimodal applications has resulted in a wide selection of datasets and methods for representing and integrating information from different modalities. Despite these empirical advances, there remain fundamental research questions: How can we quantify the interactions that are necessary to solve a multimodal task? Subsequently, what are the most suitable multimodal models to capture these interactions? To answer these questions, we propose an information-theoretic approach to quantify the degree of redundancy, uniqueness, and synergy relating input modalities with an output task. We term these three measures as the PID statistics of a multimodal distribution (or PID for short), and introduce two new estimators for these PID statistics that scale to high-dimensional distributions. To validate PID estimation, we conduct extensive experiments on both synthetic datasets where the PID is known and on large-scale multimodal benchmarks where PID estimations
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#38750;&#32467;&#26500;&#21270;&#30340;&#27668;&#20505;&#25253;&#21578;&#20013;&#22238;&#31572;&#27668;&#20505;&#38382;&#21367;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#24341;&#20837;&#20004;&#20010;&#26032;&#30340;&#22823;&#35268;&#27169;&#27668;&#20505;&#38382;&#21367;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#29616;&#26377;&#32467;&#26500;&#35757;&#32451;&#33258;&#30417;&#30563;&#27169;&#22411;&#65292;&#36890;&#36807;&#23454;&#39564;&#21644;&#20154;&#31867;&#35797;&#39564;&#39564;&#35777;&#20102;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#21516;&#26102;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#27668;&#20505;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#22522;&#20934;&#65292;&#20197;&#20419;&#36827;&#27668;&#20505;&#39046;&#22495;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2301.04253</link><description>&lt;p&gt;
&#20174;&#38750;&#32467;&#26500;&#21270;&#30340;&#27668;&#20505;&#25253;&#21578;&#20013;&#22238;&#31572;&#27668;&#20505;&#38382;&#21367;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Answering Climate Questionnaires from Unstructured Climate Reports. (arXiv:2301.04253v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.04253
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#38750;&#32467;&#26500;&#21270;&#30340;&#27668;&#20505;&#25253;&#21578;&#20013;&#22238;&#31572;&#27668;&#20505;&#38382;&#21367;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#24341;&#20837;&#20004;&#20010;&#26032;&#30340;&#22823;&#35268;&#27169;&#27668;&#20505;&#38382;&#21367;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#29616;&#26377;&#32467;&#26500;&#35757;&#32451;&#33258;&#30417;&#30563;&#27169;&#22411;&#65292;&#36890;&#36807;&#23454;&#39564;&#21644;&#20154;&#31867;&#35797;&#39564;&#39564;&#35777;&#20102;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#21516;&#26102;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#27668;&#20505;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#22522;&#20934;&#65292;&#20197;&#20419;&#36827;&#27668;&#20505;&#39046;&#22495;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#27668;&#20505;&#21464;&#21270;&#38382;&#39064;&#32039;&#36843;&#65292;&#20294;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#23545;&#20854;&#30340;&#20851;&#27880;&#26377;&#38480;&#12290;&#34892;&#21160;&#32773;&#21644;&#25919;&#31574;&#21046;&#23450;&#32773;&#38656;&#35201;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#23558;&#24222;&#22823;&#19988;&#24555;&#36895;&#22686;&#38271;&#30340;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#27668;&#20505;&#25253;&#21578;&#36716;&#21270;&#20026;&#32467;&#26500;&#21270;&#24418;&#24335;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;&#22823;&#35268;&#27169;&#27668;&#20505;&#38382;&#21367;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992;&#20854;&#29616;&#26377;&#32467;&#26500;&#26469;&#35757;&#32451;&#33258;&#30417;&#30563;&#27169;&#22411;&#12290;&#25105;&#20204;&#36827;&#34892;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21040;&#23545;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#35265;&#30340;&#19981;&#21516;&#32452;&#32455;&#31867;&#22411;&#30340;&#27668;&#20505;&#25259;&#38706;&#36827;&#34892;&#27867;&#21270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#22312;&#20154;&#31867;&#35797;&#39564;&#20013;&#24110;&#21161;&#23558;&#38750;&#32467;&#26500;&#21270;&#27668;&#20505;&#25991;&#26723;&#20013;&#30340;&#25991;&#26412;&#19982;&#21322;&#32467;&#26500;&#21270;&#38382;&#21367;&#23545;&#40784;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#25903;&#25345;&#27668;&#20505;&#39046;&#22495;&#36827;&#19968;&#27493;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#29616;&#26377;&#27668;&#20505;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#22522;&#20934;&#65292;&#20197;&#26356;&#22909;&#22320;&#35780;&#20272;&#21644;&#27604;&#36739;&#29616;&#26377;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The topic of Climate Change (CC) has received limited attention in NLP despite its urgency. Activists and policymakers need NLP tools to effectively process the vast and rapidly growing unstructured textual climate reports into structured form. To tackle this challenge we introduce two new large-scale climate questionnaire datasets and use their existing structure to train self-supervised models. We conduct experiments to show that these models can learn to generalize to climate disclosures of different organizations types than seen during training. We then use these models to help align texts from unstructured climate documents to the semi-structured questionnaires in a human pilot study. Finally, to support further NLP research in the climate domain we introduce a benchmark of existing climate text classification datasets to better evaluate and compare existing models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#29702;&#25351;&#23548;&#19979;&#30340;&#23569;&#26679;&#26412;&#20998;&#31867;&#65288;RGFS&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#36785;&#39554;&#35821;&#35328;&#12290;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20849;&#21516;&#23398;&#20064;&#26377;&#29702;&#12289;&#30446;&#26631;&#21644;&#26631;&#31614;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;6%&#23439;F1&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2211.17046</link><description>&lt;p&gt;
&#26377;&#29702;&#25351;&#23548;&#19979;&#30340;&#23569;&#26679;&#26412;&#20998;&#31867;&#29992;&#20110;&#26816;&#27979;&#36785;&#39554;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
Rationale-Guided Few-Shot Classification to Detect Abusive Language. (arXiv:2211.17046v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.17046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#29702;&#25351;&#23548;&#19979;&#30340;&#23569;&#26679;&#26412;&#20998;&#31867;&#65288;RGFS&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#36785;&#39554;&#35821;&#35328;&#12290;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20849;&#21516;&#23398;&#20064;&#26377;&#29702;&#12289;&#30446;&#26631;&#21644;&#26631;&#31614;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;6%&#23439;F1&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#31038;&#20132;&#23186;&#20307;&#20013;&#65292;&#36785;&#39554;&#35821;&#35328;&#26159;&#19968;&#20010;&#20196;&#20154;&#25285;&#24551;&#30340;&#38382;&#39064;&#12290;&#36807;&#21435;&#20851;&#20110;&#26816;&#27979;&#36785;&#39554;&#35821;&#35328;&#30340;&#30740;&#31350;&#28085;&#30422;&#20102;&#19981;&#21516;&#30340;&#24179;&#21488;&#12289;&#35821;&#35328;&#12289;&#20154;&#21475;&#32479;&#35745;&#31561;&#12290;&#28982;&#32780;&#65292;&#22312;&#20132;&#21449;&#39046;&#22495;&#35780;&#20272;&#35774;&#32622;&#20013;&#65292;&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#19968;&#31181;&#24120;&#35265;&#31574;&#30053;&#26159;&#20351;&#29992;&#30446;&#26631;&#39046;&#22495;&#20013;&#30340;&#23569;&#37327;&#26679;&#26412;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#65288;&#20132;&#21449;&#39046;&#22495;&#23569;&#26679;&#26412;&#35757;&#32451;&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#36807;&#24230;&#25311;&#21512;&#36825;&#20123;&#26679;&#26412;&#30340;&#29305;&#24449;&#12290;&#19968;&#20010;&#24341;&#20154;&#27880;&#30446;&#30340;&#35299;&#20915;&#26041;&#26696;&#21487;&#33021;&#26159;&#25351;&#23548;&#27169;&#22411;&#26397;&#21521;&#26377;&#29702;&#30340;&#26041;&#21521;&#65292;&#21363;&#21487;&#20197;&#35777;&#26126;&#25991;&#26412;&#26631;&#31614;&#30340;&#25991;&#26412;&#27573;&#33853;&#12290;&#24050;&#32463;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#22312;&#39046;&#22495;&#20869;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#26816;&#27979;&#36785;&#39554;&#35821;&#35328;&#30340;&#26377;&#29702;&#25351;&#23548;&#19979;&#30340;&#23569;&#26679;&#26412;&#20998;&#31867;&#65288;RGFS&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#19968;&#20010;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#65292;&#20849;&#21516;&#23398;&#20064;&#26377;&#29702;&#12289;&#30446;&#26631;&#21644;&#26631;&#31614;&#65292;&#24182;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;6%&#23439;F1&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Abusive language is a concerning problem in online social media. Past research on detecting abusive language covers different platforms, languages, demographies, etc. However, models trained using these datasets do not perform well in cross-domain evaluation settings. To overcome this, a common strategy is to use a few samples from the target domain to train models to get better performance in that domain (cross-domain few-shot training). However, this might cause the models to overfit the artefacts of those samples. A compelling solution could be to guide the models toward rationales, i.e., spans of text that justify the text's label. This method has been found to improve model performance in the in-domain setting across various NLP tasks. In this paper, we propose RGFS (Rationale-Guided Few-Shot Classification) for abusive language detection. We first build a multitask learning setup to jointly learn rationales, targets, and labels, and find a significant improvement of 6% macro F1 o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Passau-SFCH&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;11&#23567;&#26102;&#30340;&#24405;&#38899;&#65292;&#29992;&#20110;&#33258;&#21457;&#24189;&#40664;&#30340;&#39044;&#27979;&#12290;&#36890;&#36807;&#22810;&#27169;&#24577;&#30340;&#20998;&#26512;&#21644;&#29305;&#24449;&#34701;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#24189;&#40664;&#20197;&#21450;&#24189;&#40664;&#24773;&#24863;&#30340;&#33258;&#21160;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2209.14272</link><description>&lt;p&gt;
&#36808;&#21521;&#22810;&#27169;&#24577;&#39044;&#27979;&#33258;&#21457;&#24189;&#40664;&#65306;&#19968;&#20221;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#21644;&#21021;&#27493;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Towards Multimodal Prediction of Spontaneous Humour: A Novel Dataset and First Results. (arXiv:2209.14272v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Passau-SFCH&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;11&#23567;&#26102;&#30340;&#24405;&#38899;&#65292;&#29992;&#20110;&#33258;&#21457;&#24189;&#40664;&#30340;&#39044;&#27979;&#12290;&#36890;&#36807;&#22810;&#27169;&#24577;&#30340;&#20998;&#26512;&#21644;&#29305;&#24449;&#34701;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#24189;&#40664;&#20197;&#21450;&#24189;&#40664;&#24773;&#24863;&#30340;&#33258;&#21160;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24189;&#40664;&#26159;&#20154;&#31867;&#24773;&#24863;&#21644;&#35748;&#30693;&#30340;&#37325;&#35201;&#20803;&#32032;&#12290;&#20854;&#33258;&#21160;&#29702;&#35299;&#21487;&#20197;&#20419;&#36827;&#26356;&#33258;&#28982;&#30340;&#20154;&#26426;&#20132;&#20114;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#20154;&#24615;&#21270;&#12290;&#30446;&#21069;&#30340;&#24189;&#40664;&#26816;&#27979;&#26041;&#27861;&#20165;&#22522;&#20110;&#31574;&#21010;&#25968;&#25454;&#65292;&#19981;&#33021;&#28385;&#36275;&#8220;&#29616;&#23454;&#19990;&#30028;&#8221;&#24212;&#29992;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;Passau-Spontaneous Football Coach Humour&#65288;Passau-SFCH&#65289;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#32422;11&#23567;&#26102;&#30340;&#24405;&#38899;&#65292;&#35299;&#20915;&#20102;&#36825;&#19968;&#32570;&#38519;&#12290;Passau-SFCH&#25968;&#25454;&#38598;&#30340;&#27880;&#37322;&#26681;&#25454;Martin&#30340;&#24189;&#40664;&#39118;&#26684;&#38382;&#21367;&#25552;&#20986;&#30340;&#24189;&#40664;&#23384;&#22312;&#21450;&#20854;&#32500;&#24230;&#65288;&#24773;&#24863;&#21644;&#26041;&#21521;&#65289;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#37319;&#29992;&#39044;&#35757;&#32451;&#30340;Transformer&#12289;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#19987;&#23478;&#35774;&#35745;&#30340;&#29305;&#24449;&#12290;&#20998;&#26512;&#20102;&#33258;&#21457;&#24189;&#40664;&#35782;&#21035;&#30340;&#27599;&#31181;&#27169;&#24577;&#65288;&#25991;&#26412;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#65289;&#30340;&#24615;&#33021;&#65292;&#24182;&#30740;&#31350;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#20114;&#34917;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#24189;&#40664;&#21450;&#20854;&#24773;&#24863;&#30340;&#33258;&#21160;&#20998;&#26512;&#65292;&#22810;&#27169;&#24577;&#32852;&#21512;&#20351;&#29992;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humour is a substantial element of human affect and cognition. Its automatic understanding can facilitate a more naturalistic human-device interaction and the humanisation of artificial intelligence. Current methods of humour detection are solely based on staged data making them inadequate for 'real-world' applications. We address this deficiency by introducing the novel Passau-Spontaneous Football Coach Humour (Passau-SFCH) dataset, comprising of about 11 hours of recordings. The Passau-SFCH dataset is annotated for the presence of humour and its dimensions (sentiment and direction) as proposed in Martin's Humor Style Questionnaire. We conduct a series of experiments, employing pretrained Transformers, convolutional neural networks, and expert-designed features. The performance of each modality (text, audio, video) for spontaneous humour recognition is analysed and their complementarity is investigated. Our findings suggest that for the automatic analysis of humour and its sentiment, 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30456;&#20284;&#24230;&#30340;&#33258;&#36866;&#24212;&#20803;&#23398;&#20064;&#22120;&#26041;&#27861;&#65288;AMGS&#65289;&#65292;&#29992;&#20110;&#25913;&#21892;&#23569;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#30417;&#30563;&#36741;&#21161;&#20219;&#21153;&#33719;&#24471;&#28508;&#22312;&#35821;&#20041;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#26799;&#24230;&#30456;&#20284;&#24230;&#32422;&#26463;&#22522;&#23398;&#20064;&#22120;&#30340;&#26799;&#24230;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2209.04702</link><description>&lt;p&gt;
&#22522;&#20110;&#26799;&#24230;&#30456;&#20284;&#24230;&#30340;&#33258;&#36866;&#24212;&#20803;&#23398;&#20064;&#22120;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Adaptive Meta-learner via Gradient Similarity for Few-shot Text Classification. (arXiv:2209.04702v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.04702
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30456;&#20284;&#24230;&#30340;&#33258;&#36866;&#24212;&#20803;&#23398;&#20064;&#22120;&#26041;&#27861;&#65288;AMGS&#65289;&#65292;&#29992;&#20110;&#25913;&#21892;&#23569;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#30417;&#30563;&#36741;&#21161;&#20219;&#21153;&#33719;&#24471;&#28508;&#22312;&#35821;&#20041;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#26799;&#24230;&#30456;&#20284;&#24230;&#32422;&#26463;&#22522;&#23398;&#20064;&#22120;&#30340;&#26799;&#24230;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#26088;&#22312;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#23545;&#25991;&#26412;&#36827;&#34892;&#20998;&#31867;&#12290;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#26041;&#27861;&#37319;&#29992;&#22522;&#20110;&#20248;&#21270;&#30340;&#20803;&#23398;&#20064;&#26469;&#33719;&#24471;&#20219;&#21153;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24573;&#35270;&#20102;&#23569;&#37327;&#26679;&#26412;&#21644;&#22797;&#26434;&#27169;&#22411;&#20043;&#38388;&#30340;&#21305;&#37197;&#65292;&#20197;&#21450;&#26377;&#29992;&#21644;&#26080;&#29992;&#20219;&#21153;&#29305;&#24449;&#20043;&#38388;&#30340;&#21306;&#21035;&#65292;&#36825;&#20123;&#26041;&#27861;&#36973;&#21463;&#20102;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26799;&#24230;&#30456;&#20284;&#24230;&#30340;&#33258;&#36866;&#24212;&#20803;&#23398;&#20064;&#22120;&#65288;AMGS&#65289;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#23545;&#26032;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25152;&#25552;&#20986;&#30340;AMGS&#20174;&#20004;&#20010;&#26041;&#38754;&#32531;&#35299;&#36807;&#25311;&#21512;&#38382;&#39064;&#65306;&#65288;i&#65289;&#36890;&#36807;&#20869;&#24490;&#29615;&#20013;&#30340;&#33258;&#30417;&#30563;&#36741;&#21161;&#20219;&#21153;&#33719;&#24471;&#26679;&#26412;&#30340;&#28508;&#22312;&#35821;&#20041;&#34920;&#31034;&#24182;&#25913;&#21892;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#65288;ii&#65289;&#36890;&#36807;&#22522;&#20110;&#26799;&#24230;&#30456;&#20284;&#24230;&#30340;&#33258;&#36866;&#24212;&#20803;&#23398;&#20064;&#22120;&#22312;&#22806;&#24490;&#29615;&#20013;&#23545;&#22522;&#23398;&#20064;&#22120;&#33719;&#24471;&#30340;&#26799;&#24230;&#26045;&#21152;&#32422;&#26463;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#24433;&#21709;&#22240;&#32032;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot text classification aims to classify the text under the few-shot scenario. Most of the previous methods adopt optimization-based meta learning to obtain task distribution. However, due to the neglect of matching between the few amount of samples and complicated models, as well as the distinction between useful and useless task features, these methods suffer from the overfitting issue. To address this issue, we propose a novel Adaptive Meta-learner via Gradient Similarity (AMGS) method to improve the model generalization ability to a new task. Specifically, the proposed AMGS alleviates the overfitting based on two aspects: (i) acquiring the potential semantic representation of samples and improving model generalization through the self-supervised auxiliary task in the inner loop, (ii) leveraging the adaptive meta-learner via gradient similarity to add constraints on the gradient obtained by base-learner in the outer loop. Moreover, we make a systematic analysis of the influence
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#39318;&#20010;&#38024;&#23545;&#22303;&#32819;&#20854;&#35821;&#30340;&#33258;&#21160;&#35789;&#27719;&#31616;&#21270;&#31995;&#32479;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#34920;&#31034;&#27169;&#22411;BERT&#21644;&#24418;&#24577;&#29305;&#24449;&#29983;&#25104;&#27491;&#30830;&#30340;&#35821;&#27861;&#21644;&#35821;&#20041;&#31616;&#21270;&#34920;&#36798;&#12290;</title><link>http://arxiv.org/abs/2201.05878</link><description>&lt;p&gt;
Turkish&#33258;&#21160;&#35789;&#27719;&#31616;&#21270;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Automatic Lexical Simplification for Turkish. (arXiv:2201.05878v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.05878
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#39318;&#20010;&#38024;&#23545;&#22303;&#32819;&#20854;&#35821;&#30340;&#33258;&#21160;&#35789;&#27719;&#31616;&#21270;&#31995;&#32479;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#34920;&#31034;&#27169;&#22411;BERT&#21644;&#24418;&#24577;&#29305;&#24449;&#29983;&#25104;&#27491;&#30830;&#30340;&#35821;&#27861;&#21644;&#35821;&#20041;&#31616;&#21270;&#34920;&#36798;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#39318;&#20010;&#38024;&#23545;&#22303;&#32819;&#20854;&#35821;&#30340;&#33258;&#21160;&#35789;&#27719;&#31616;&#21270;&#31995;&#32479;&#12290;&#26368;&#36817;&#30340;&#25991;&#26412;&#31616;&#21270;&#24037;&#20316;&#20381;&#36182;&#20110;&#25163;&#24037;&#26500;&#24314;&#30340;&#31616;&#21270;&#35821;&#26009;&#24211;&#21644;&#20840;&#38754;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#65292;&#21487;&#20197;&#23545;&#30446;&#26631;&#25991;&#26412;&#36827;&#34892;&#35789;&#27719;&#21644;&#21477;&#23376;&#32423;&#21035;&#30340;&#20998;&#26512;&#12290;&#22303;&#32819;&#20854;&#35821;&#26159;&#19968;&#31181;&#24418;&#24577;&#20016;&#23500;&#30340;&#32858;&#21512;&#35821;&#35328;&#65292;&#38656;&#35201;&#32771;&#34385;&#35832;&#22914;&#22788;&#29702;&#21464;&#26684;&#31561;&#29420;&#29305;&#35201;&#27714;&#12290;&#20316;&#20026;&#36164;&#28304;&#26377;&#38480;&#19988;&#32570;&#20047;&#24037;&#19994;&#32423;&#24037;&#20855;&#25903;&#25345;&#30340;&#35821;&#35328;&#65292;&#36825;&#20351;&#24471;&#25991;&#26412;&#31616;&#21270;&#20219;&#21153;&#26356;&#21152;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#34920;&#31034;&#27169;&#22411;BERT&#21644;&#24418;&#24577;&#29305;&#24449;&#30340;&#26032;&#25991;&#26412;&#31616;&#21270;&#27969;&#31243;&#65292;&#29992;&#20110;&#29983;&#25104;&#35821;&#27861;&#27491;&#30830;&#19988;&#35821;&#20041;&#21512;&#36866;&#30340;&#35789;&#32423;&#31616;&#21270;&#34920;&#36798;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present the first automatic lexical simplification system for the Turkish language. Recent text simplification efforts rely on manually crafted simplified corpora and comprehensive NLP tools that can analyse the target text both in word and sentence levels. Turkish is a morphologically rich agglutinative language that requires unique considerations such as the proper handling of inflectional cases. Being a low-resource language in terms of available resources and industrial-strength tools, it makes the text simplification task harder to approach. We present a new text simplification pipeline based on pretrained representation model BERT together with morphological features to generate grammatically correct and semantically appropriate word-level simplifications.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20154;&#31867;&#30699;&#27491;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#26631;&#27880;&#25968;&#25454;&#20013;&#30340;&#22122;&#22768;&#25968;&#25454;&#65292;&#25910;&#38598;&#32416;&#38169;&#20449;&#24687;&#65292;&#24182;&#23558;&#20854;&#27880;&#20837;&#33267;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#25104;&#21151;&#23558;&#25991;&#26412;&#20998;&#31867;&#20934;&#30830;&#24230;&#25552;&#21319;&#20102;1.7&#20010;&#30334;&#20998;&#28857;&#12290;</title><link>http://arxiv.org/abs/2102.00225</link><description>&lt;p&gt;
&#20174;&#20154;&#31867;&#30340;&#32416;&#38169;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning From How Humans Correct. (arXiv:2102.00225v14 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.00225
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20154;&#31867;&#30699;&#27491;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#26631;&#27880;&#25968;&#25454;&#20013;&#30340;&#22122;&#22768;&#25968;&#25454;&#65292;&#25910;&#38598;&#32416;&#38169;&#20449;&#24687;&#65292;&#24182;&#23558;&#20854;&#27880;&#20837;&#33267;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#25104;&#21151;&#23558;&#25991;&#26412;&#20998;&#31867;&#20934;&#30830;&#24230;&#25552;&#21319;&#20102;1.7&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#19994;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#25163;&#21160;&#26631;&#27880;&#30340;&#25968;&#25454;&#20013;&#23384;&#22312;&#19968;&#23450;&#25968;&#37327;&#30340;&#22122;&#22768;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#25214;&#21040;&#22122;&#22768;&#25968;&#25454;&#24182;&#25163;&#21160;&#37325;&#26032;&#26631;&#27880;&#23427;&#20204;&#65292;&#21516;&#26102;&#25910;&#38598;&#32416;&#38169;&#20449;&#24687;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20154;&#31867;&#32416;&#38169;&#20449;&#24687;&#34701;&#20837;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#12290;&#20154;&#31867;&#30693;&#36947;&#22914;&#20309;&#32416;&#27491;&#22122;&#22768;&#25968;&#25454;&#65292;&#22240;&#27492;&#32416;&#38169;&#20449;&#24687;&#21487;&#20197;&#27880;&#20837;&#21040;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#22312;&#33258;&#24049;&#30340;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#25163;&#21160;&#26631;&#27880;&#30340;&#65292;&#22240;&#20026;&#25105;&#20204;&#37325;&#26032;&#26631;&#27880;&#20102;&#25105;&#20204;&#25968;&#25454;&#38598;&#20013;&#30340;&#22122;&#22768;&#25968;&#25454;&#65292;&#20197;&#36866;&#29992;&#20110;&#25105;&#20204;&#30340;&#24037;&#19994;&#24212;&#29992;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#20998;&#31867;&#20934;&#30830;&#24230;&#20174;91.7%&#25552;&#21319;&#21040;92.5%&#12290;91.7%&#30340;&#20934;&#30830;&#24230;&#26159;&#22312;&#20462;&#27491;&#21518;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#65292;&#23427;&#23558;&#22522;&#32447;&#20934;&#30830;&#24230;&#20174;83.3%&#25552;&#21319;&#21040;91.7%&#12290;
&lt;/p&gt;
&lt;p&gt;
In industry NLP application, our manually labeled data has a certain number of noisy data. We present a simple method to find the noisy data and re-label them manually, meanwhile we collect the correction information. Then we present novel method to incorporate the human correction information into deep learning model. Human know how to correct noisy data. So the correction information can be inject into deep learning model. We do the experiment on our own text classification dataset, which is manually labeled, because we re-label the noisy data in our dataset for our industry application. The experiment result shows that our method improve the classification accuracy from 91.7% to 92.5%. The 91.7% accuracy is trained on the corrected dataset, which improve the baseline from 83.3% to 91.7%.
&lt;/p&gt;</description></item></channel></rss>