<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#29983;&#29289;&#21551;&#21457;&#30340;&#26041;&#27861;&#65292;&#21363;&#27169;&#25311;&#35797;&#38169;&#65288;STE&#65289;&#65292;&#20026;&#24037;&#20855;&#22686;&#24378;&#22411;LLMs&#32534;&#25490;&#20102;&#19977;&#20010;&#20851;&#38190;&#26426;&#21046;&#20197;&#23454;&#29616;&#25104;&#21151;&#30340;&#24037;&#20855;&#20351;&#29992;&#34892;&#20026;</title><link>https://arxiv.org/abs/2403.04746</link><description>&lt;p&gt;
&#22312;&#24187;&#22659;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#36890;&#36807;&#27169;&#25311;&#35797;&#38169;&#23398;&#20064;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
LLMs in the Imaginarium: Tool Learning through Simulated Trial and Error
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04746
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#29983;&#29289;&#21551;&#21457;&#30340;&#26041;&#27861;&#65292;&#21363;&#27169;&#25311;&#35797;&#38169;&#65288;STE&#65289;&#65292;&#20026;&#24037;&#20855;&#22686;&#24378;&#22411;LLMs&#32534;&#25490;&#20102;&#19977;&#20010;&#20851;&#38190;&#26426;&#21046;&#20197;&#23454;&#29616;&#25104;&#21151;&#30340;&#24037;&#20855;&#20351;&#29992;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#20855;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33719;&#21462;&#26368;&#26032;&#20449;&#24687;&#24182;&#22312;&#22806;&#37096;&#29615;&#22659;&#20013;&#37319;&#21462;&#37325;&#35201;&#34892;&#21160;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#20851;&#20110;&#24037;&#20855;&#22686;&#24378;&#22411;LLMs&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#24037;&#20855;&#30340;&#24191;&#27867;&#35206;&#30422;&#33539;&#22260;&#21644;&#28789;&#27963;&#24615;&#19978;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#34987;&#20154;&#24847;&#22806;&#24573;&#35270;&#30340;&#20851;&#38190;&#26041;&#38754;&#26159;LLM&#22312;&#32463;&#36807;&#35757;&#32451;&#21518;&#22914;&#20309;&#20934;&#30830;&#20351;&#29992;&#24037;&#20855;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21253;&#25324;GPT-4&#21644;&#19987;&#38376;&#20026;&#24037;&#20855;&#20351;&#29992;&#36827;&#34892;&#24494;&#35843;&#30340;&#24320;&#28304;LLMs&#22312;&#27491;&#30830;&#29575;&#26041;&#38754;&#20165;&#36798;&#21040;30%&#21040;60%&#30340;&#33539;&#22260;&#65292;&#36828;&#19981;&#36275;&#20197;&#22312;&#23454;&#36341;&#20013;&#21487;&#38752;&#20351;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#29983;&#29289;&#21551;&#21457;&#30340;&#26041;&#27861;&#65292;&#21363;&#27169;&#25311;&#35797;&#38169;&#65288;STE&#65289;&#65292;&#20026;&#24037;&#20855;&#22686;&#24378;&#22411;LLMs&#32534;&#25490;&#20102;&#19977;&#20010;&#20851;&#38190;&#26426;&#21046;&#20197;&#23454;&#29616;&#25104;&#21151;&#30340;&#24037;&#20855;&#20351;&#29992;&#34892;&#20026;&#65306;&#35797;&#38169;&#12289;&#24819;&#35937;&#21644;&#35760;&#24518;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;STE&#21033;&#29992;LLM&#30340;&#8220;&#24819;&#35937;&#21147;&#8221;&#26469;&#27169;&#25311;&#20351;&#29992;&#24037;&#20855;&#30340;&#21487;&#33021;&#22330;&#26223;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04746v1 Announce Type: cross  Abstract: Tools are essential for large language models (LLMs) to acquire up-to-date information and take consequential actions in external environments. Existing work on tool-augmented LLMs primarily focuses on the broad coverage of tools and the flexibility of adding new tools. However, a critical aspect that has surprisingly been understudied is simply how accurately an LLM uses tools for which it has been trained. We find that existing LLMs, including GPT-4 and open-source LLMs specifically fine-tuned for tool use, only reach a correctness rate in the range of 30% to 60%, far from reliable use in practice. We propose a biologically inspired method for tool-augmented LLMs, simulated trial and error (STE), that orchestrates three key mechanisms for successful tool use behaviors in the biological system: trial and error, imagination, and memory. Specifically, STE leverages an LLM's 'imagination' to simulate plausible scenarios for using a tool,
&lt;/p&gt;</description></item><item><title>&#30446;&#21069;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#35270;&#35273;&#28436;&#32462;&#25512;&#29702;&#26041;&#38754;&#20173;&#23384;&#22312;&#36739;&#22823;&#24046;&#36317;&#21644;&#30450;&#28857;&#12290;</title><link>https://arxiv.org/abs/2403.04732</link><description>&lt;p&gt;
&#25105;&#20204;&#36317;&#31163;&#26234;&#33021;&#35270;&#35273;&#28436;&#32462;&#25512;&#29702;&#36824;&#26377;&#22810;&#36828;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Far Are We from Intelligent Visual Deductive Reasoning?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04732
&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#35270;&#35273;&#28436;&#32462;&#25512;&#29702;&#26041;&#38754;&#20173;&#23384;&#22312;&#36739;&#22823;&#24046;&#36317;&#21644;&#30450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35832;&#22914;GPT-4V&#20043;&#31867;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#22312;&#21508;&#31181;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#12290;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#22522;&#20110;&#35270;&#35273;&#30340;&#28436;&#32462;&#25512;&#29702;&#65292;&#36825;&#26159;&#19968;&#20010;&#26356;&#22797;&#26434;&#20294;&#19981;&#22826;&#34987;&#25506;&#32034;&#30340;&#39046;&#22495;&#65292;&#24182;&#21457;&#29616;&#20102;&#24403;&#21069;&#39046;&#20808;&#30340;VLM&#20013;&#20197;&#21069;&#26410;&#26292;&#38706;&#30340;&#30450;&#28857;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#29790;&#25991;&#28176;&#36827;&#30697;&#38453;&#65288;RPM&#65289;&#26469;&#35780;&#20272;VLM&#22312;&#20165;&#20381;&#38752;&#35270;&#35273;&#32447;&#32034;&#36827;&#34892;&#22810;&#36339;&#20851;&#31995;&#21644;&#28436;&#32462;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23545;&#20960;&#31181;&#27969;&#34892;&#30340;VLM&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#37319;&#29992;&#20102;&#26631;&#20934;&#31574;&#30053;&#65292;&#22914;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#33258;&#25105;&#19968;&#33268;&#24615;&#21644;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#65292;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;Mensa&#26234;&#21830;&#27979;&#35797;&#12289;&#26234;&#21830;&#27979;&#35797;&#21644;RAVEN&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;LLM&#22312;&#22522;&#20110;&#25991;&#26412;&#30340;&#25512;&#29702;&#26041;&#38754;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#25105;&#20204;&#22312;&#35270;&#35273;&#28436;&#32462;&#25512;&#29702;&#26041;&#38754;&#20173;&#26377;&#24456;&#22823;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04732v1 Announce Type: new  Abstract: Vision-Language Models (VLMs) such as GPT-4V have recently demonstrated incredible strides on diverse vision language tasks. We dig into vision-based deductive reasoning, a more sophisticated but less explored realm, and find previously unexposed blindspots in the current SOTA VLMs. Specifically, we leverage Raven's Progressive Matrices (RPMs), to assess VLMs' abilities to perform multi-hop relational and deductive reasoning relying solely on visual clues. We perform comprehensive evaluations of several popular VLMs employing standard strategies such as in-context learning, self-consistency, and Chain-of-thoughts (CoT) on three diverse datasets, including the Mensa IQ test, IntelligenceTest, and RAVEN. The results reveal that despite the impressive capabilities of LLMs in text-based reasoning, we are still far from achieving comparable proficiency in visual deductive reasoning. We found that certain standard strategies that are effective
&lt;/p&gt;</description></item><item><title>&#36890;&#29992;7B&#35821;&#35328;&#27169;&#22411;LLaMA-2&#23637;&#29616;&#20986;&#24378;&#22823;&#25968;&#23398;&#33021;&#21147;&#65292;&#20934;&#30830;&#29575;&#20998;&#21035;&#36798;&#21040;97.7%&#21644;72.0%&#65292;&#25193;&#22823;SFT&#25968;&#25454;&#21487;&#20197;&#25552;&#39640;&#29983;&#25104;&#27491;&#30830;&#31572;&#26696;&#30340;&#21487;&#38752;&#24615;</title><link>https://arxiv.org/abs/2403.04706</link><description>&lt;p&gt;
&#36890;&#29992;7B&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#20855;&#22791;&#24378;&#22823;&#30340;&#25968;&#23398;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Common 7B Language Models Already Possess Strong Math Capabilities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04706
&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;7B&#35821;&#35328;&#27169;&#22411;LLaMA-2&#23637;&#29616;&#20986;&#24378;&#22823;&#25968;&#23398;&#33021;&#21147;&#65292;&#20934;&#30830;&#29575;&#20998;&#21035;&#36798;&#21040;97.7%&#21644;72.0%&#65292;&#25193;&#22823;SFT&#25968;&#25454;&#21487;&#20197;&#25552;&#39640;&#29983;&#25104;&#27491;&#30830;&#31572;&#26696;&#30340;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#20154;&#20204;&#30456;&#20449;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#21482;&#26377;&#22312;&#38750;&#24120;&#22823;&#30340;&#35268;&#27169;&#19978;&#25110;&#38656;&#35201;&#22823;&#37327;&#19982;&#25968;&#23398;&#30456;&#20851;&#30340;&#39044;&#35757;&#32451;&#25165;&#33021;&#23637;&#29616;&#20986;&#25968;&#23398;&#33021;&#21147;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#20855;&#26377;&#36890;&#29992;&#39044;&#35757;&#32451;&#30340;LLaMA-2 7B&#27169;&#22411;&#24050;&#32463;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#25968;&#23398;&#33021;&#21147;&#65292;&#20854;&#22312;GSM8K&#21644;MATH&#22522;&#20934;&#27979;&#35797;&#19978;&#36873;&#25321;256&#20010;&#38543;&#26426;&#29983;&#25104;&#30340;&#26368;&#20339;&#21709;&#24212;&#26102;&#65292;&#20934;&#30830;&#29575;&#20998;&#21035;&#20026;97.7%&#21644;72.0%&#12290;&#30446;&#21069;&#22522;&#30784;&#27169;&#22411;&#30340;&#20027;&#35201;&#38382;&#39064;&#22312;&#20110;&#38590;&#20197;&#19968;&#33268;&#22320;&#24341;&#20986;&#20854;&#22266;&#26377;&#30340;&#25968;&#23398;&#33021;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23545;&#20110;&#31532;&#19968;&#20010;&#31572;&#26696;&#30340;&#20934;&#30830;&#29575;&#20998;&#21035;&#19979;&#38477;&#21040;&#20102;49.5%&#21644;7.9%&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#31616;&#21333;&#22320;&#25193;&#22823;SFT&#25968;&#25454;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#29983;&#25104;&#27491;&#30830;&#31572;&#26696;&#30340;&#21487;&#38752;&#24615;&#12290;&#28982;&#32780;&#65292;&#24191;&#27867;&#25193;&#23637;&#30340;&#28508;&#21147;&#21463;&#21040;&#20844;&#24320;&#21487;&#29992;&#25968;&#23398;&#38382;&#39064;&#30340;&#31232;&#32570;&#24615;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04706v1 Announce Type: cross  Abstract: Mathematical capabilities were previously believed to emerge in common language models only at a very large scale or require extensive math-related pre-training. This paper shows that the LLaMA-2 7B model with common pre-training already exhibits strong mathematical abilities, as evidenced by its impressive accuracy of 97.7% and 72.0% on the GSM8K and MATH benchmarks, respectively, when selecting the best response from 256 random generations. The primary issue with the current base model is the difficulty in consistently eliciting its inherent mathematical capabilities. Notably, the accuracy for the first answer drops to 49.5% and 7.9% on the GSM8K and MATH benchmarks, respectively. We find that simply scaling up the SFT data can significantly enhance the reliability of generating correct answers. However, the potential for extensive scaling is constrained by the scarcity of publicly available math questions. To overcome this limitatio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#35760;&#32423;&#21035;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#26032;&#22411;&#20107;&#23454;&#26680;&#26597;&#21644;&#24187;&#35273;&#26816;&#27979;&#27969;&#31243;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#20013;&#30340;&#19981;&#21487;&#38752;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.04696</link><description>&lt;p&gt;
&#36890;&#36807;&#26631;&#35760;&#32423;&#21035;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26816;&#39564;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;
&lt;/p&gt;
&lt;p&gt;
Fact-Checking the Output of Large Language Models via Token-Level Uncertainty Quantification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04696
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#35760;&#32423;&#21035;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#26032;&#22411;&#20107;&#23454;&#26680;&#26597;&#21644;&#24187;&#35273;&#26816;&#27979;&#27969;&#31243;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#20013;&#30340;&#19981;&#21487;&#38752;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20197;&#20135;&#29983;&#38169;&#35823;&#30340;&#22768;&#26126;&#32780;&#33261;&#21517;&#26157;&#33879;&#12290;&#36825;&#31181;&#24187;&#35273;&#21487;&#33021;&#24456;&#21361;&#38505;&#65292;&#22240;&#20026;&#22312;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#20598;&#23572;&#20986;&#29616;&#30340;&#20107;&#23454;&#19981;&#20934;&#30830;&#21487;&#33021;&#20250;&#34987;&#25972;&#20307;&#19978;&#26159;&#20107;&#23454;&#30340;&#25991;&#26412;&#25513;&#30422;&#65292;&#36825;&#20351;&#24471;&#29992;&#25143;&#26497;&#20854;&#38590;&#20197;&#21457;&#29616;&#12290;&#21033;&#29992;LLMs&#30340;&#24403;&#21069;&#26381;&#21153;&#36890;&#24120;&#19981;&#25552;&#20379;&#26816;&#27979;&#19981;&#21487;&#38752;&#29983;&#25104;&#30340;&#26041;&#24335;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#26088;&#22312;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#35760;&#32423;&#21035;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#26032;&#22411;&#20107;&#23454;&#26680;&#26597;&#21644;&#24187;&#35273;&#26816;&#27979;&#27969;&#31243;&#12290;&#19981;&#30830;&#23450;&#24615;&#20998;&#25968;&#21033;&#29992;&#20102;&#31070;&#32463;&#32593;&#32476;&#25110;&#20854;&#23618;&#36755;&#20986;&#20013;&#21253;&#21547;&#30340;&#20449;&#24687;&#26469;&#26816;&#27979;&#19981;&#21487;&#38752;&#30340;&#39044;&#27979;&#65292;&#24182;&#25105;&#20204;&#23637;&#31034;&#23427;&#20204;&#21487;&#20197;&#29992;&#20110;&#26680;&#26597;LLM&#36755;&#20986;&#20013;&#30340;&#21508;&#31181;&#22768;&#26126;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#26631;&#35760;&#32423;&#21035;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#28040;&#38500;&#20102;&#23545;&#20107;&#23454;&#25552;&#20986;&#24576;&#30097;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04696v1 Announce Type: cross  Abstract: Large language models (LLMs) are notorious for hallucinating, i.e., producing erroneous claims in their output. Such hallucinations can be dangerous, as occasional factual inaccuracies in the generated text might be obscured by the rest of the output being generally factual, making it extremely hard for the users to spot them. Current services that leverage LLMs usually do not provide any means for detecting unreliable generations. Here, we aim to bridge this gap. In particular, we propose a novel fact-checking and hallucination detection pipeline based on token-level uncertainty quantification. Uncertainty scores leverage information encapsulated in the output of a neural network or its layers to detect unreliable predictions, and we show that they can be used to fact-check the atomic claims in the LLM output. Moreover, we present a novel token-level uncertainty quantification method that removes the impact of uncertainty about what c
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#20102;&#23569;&#25968;&#32676;&#20307;&#21644;&#38750;&#23569;&#25968;&#32676;&#20307;&#22312;&#21327;&#20316;&#35299;&#20915;&#38382;&#39064;&#20219;&#21153;&#20013;&#30340;&#27807;&#36890;&#27169;&#24335;&#65292;&#24182;&#21457;&#29616;URM&#36523;&#20221;&#20250;&#24433;&#21709;&#20010;&#20307;&#30340;&#31038;&#20250;&#35748;&#30693;&#35821;&#35328;&#27169;&#24335;</title><link>https://arxiv.org/abs/2403.04671</link><description>&lt;p&gt;
&#20854;&#37096;&#20998;&#20043;&#21644;&#22823;&#20110;&#20854;&#25972;&#20307;&#65306;&#23569;&#25968;&#32676;&#20307;&#21644;&#22810;&#25968;&#32676;&#20307;&#22312;&#21327;&#20316;&#35299;&#20915;&#38382;&#39064;&#30340;&#27807;&#36890;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Greater than the sum of its parts: The role of minority and majority status in collaborative problem-solving communication
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04671
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#20102;&#23569;&#25968;&#32676;&#20307;&#21644;&#38750;&#23569;&#25968;&#32676;&#20307;&#22312;&#21327;&#20316;&#35299;&#20915;&#38382;&#39064;&#20219;&#21153;&#20013;&#30340;&#27807;&#36890;&#27169;&#24335;&#65292;&#24182;&#21457;&#29616;URM&#36523;&#20221;&#20250;&#24433;&#21709;&#20010;&#20307;&#30340;&#31038;&#20250;&#35748;&#30693;&#35821;&#35328;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#20316;&#35299;&#20915;&#38382;&#39064;&#65288;CPS&#65289;&#26159;&#19968;&#39033;&#20851;&#38190;&#25216;&#33021;&#65292;&#26082;&#22312;&#32844;&#22330;&#20013;&#20351;&#29992;&#65292;&#20063;&#22312;&#25945;&#32946;&#29615;&#22659;&#20013;&#20351;&#29992;&#12290;CPS&#23545;&#20110;&#35299;&#20915;&#26085;&#30410;&#22797;&#26434;&#30340;&#20840;&#29699;&#12289;&#32463;&#27982;&#21644;&#25919;&#27835;&#38382;&#39064;&#38750;&#24120;&#26377;&#29992;&#65292;&#24182;&#34987;&#35748;&#20026;&#26159;21&#19990;&#32426;&#30340;&#26680;&#24515;&#25216;&#33021;&#12290;&#26085;&#30410;&#36830;&#25509;&#30340;&#20840;&#29699;&#31038;&#21306;&#20026;&#20855;&#26377;&#19981;&#21516;&#35270;&#35282;&#30340;&#21019;&#36896;&#24615;&#21644;&#21327;&#20316;&#24335;&#38382;&#39064;&#35299;&#20915;&#20114;&#21160;&#19982;&#35299;&#20915;&#26041;&#26696;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#26426;&#20250;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22919;&#22899;&#21644;&#23569;&#25968;&#32676;&#20307;&#65288;URMs&#65289;&#22312;&#21327;&#20316;&#20114;&#21160;&#20013;&#32463;&#24120;&#38754;&#20020;&#38556;&#30861;&#65292;&#36825;&#20123;&#38556;&#30861;&#38459;&#30861;&#20102;&#20182;&#20204;&#22312;&#36825;&#20123;&#38382;&#39064;&#35299;&#20915;&#23545;&#35805;&#20013;&#30340;&#37325;&#35201;&#21442;&#19982;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#23569;&#25968;&#32676;&#20307;&#21644;&#38750;&#23569;&#25968;&#32676;&#20307;&#22312;CPS&#20219;&#21153;&#20013;&#20849;&#21516;&#24037;&#20316;&#30340;&#27807;&#36890;&#27169;&#24335;&#12290;&#38598;&#22242;&#27807;&#36890;&#20998;&#26512;&#65288;GCA&#65289;&#65292;&#19968;&#31181;&#26102;&#38388;&#25935;&#24863;&#30340;&#35745;&#31639;&#35821;&#35328;&#24037;&#20855;&#65292;&#34987;&#29992;&#26469;&#30740;&#31350;URM&#36523;&#20221;&#22914;&#20309;&#24433;&#21709;&#20010;&#20307;&#30340;&#31038;&#20250;&#35748;&#30693;&#35821;&#35328;&#27169;&#24335;&#12290;&#32467;&#26524;&#26174;&#31034;&#20102;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04671v1 Announce Type: new  Abstract: Collaborative problem-solving (CPS) is a vital skill used both in the workplace and in educational environments. CPS is useful in tackling increasingly complex global, economic, and political issues and is considered a central 21st century skill. The increasingly connected global community presents a fruitful opportunity for creative and collaborative problem-solving interactions and solutions that involve diverse perspectives. Unfortunately, women and underrepresented minorities (URMs) often face obstacles during collaborative interactions that hinder their key participation in these problem-solving conversations. Here, we explored the communication patterns of minority and non-minority individuals working together in a CPS task. Group Communication Analysis (GCA), a temporally-sensitive computational linguistic tool, was used to examine how URM status impacts individuals' sociocognitive linguistic patterns. Results show differences acr
&lt;/p&gt;</description></item><item><title>&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;Phi-2&#22312;&#30005;&#20449;&#39046;&#22495;&#23637;&#31034;&#20986;&#19982;&#22823;&#22411;&#23545;&#24212;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#26041;&#27861;&#25552;&#21319;&#20102;&#20854;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.04666</link><description>&lt;p&gt;
&#30005;&#20449;&#35821;&#35328;&#27169;&#22411;&#65306;&#23427;&#20204;&#24517;&#39035;&#24222;&#22823;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Telecom Language Models: Must They Be Large?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04666
&lt;/p&gt;
&lt;p&gt;
&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;Phi-2&#22312;&#30005;&#20449;&#39046;&#22495;&#23637;&#31034;&#20986;&#19982;&#22823;&#22411;&#23545;&#24212;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#26041;&#27861;&#25552;&#21319;&#20102;&#20854;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#20449;&#37096;&#38376;&#23545;&#24222;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26085;&#30410;&#20851;&#27880;&#20984;&#26174;&#20102;&#23427;&#20204;&#22312;&#25913;&#21464;&#36816;&#33829;&#25928;&#29575;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#37096;&#32626;&#36825;&#20123;&#22797;&#26434;&#27169;&#22411;&#24448;&#24448;&#21463;&#21040;&#20854;&#24040;&#22823;&#20307;&#31215;&#21644;&#35745;&#31639;&#38656;&#27714;&#30340;&#24433;&#21709;&#65292;&#24341;&#21457;&#20102;&#23545;&#23427;&#20204;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#20013;&#21487;&#34892;&#24615;&#30340;&#25285;&#24551;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26368;&#36817;&#30340;&#36827;&#23637;&#20986;&#29616;&#20102;&#19968;&#25209;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#23427;&#20204;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#34920;&#29616;&#19982;&#20854;&#36739;&#22823;&#23545;&#24212;&#29289;&#30456;&#24403;&#65292;&#27604;&#22914;&#32534;&#30721;&#21644;&#24120;&#35782;&#25512;&#29702;&#12290;Phi-2&#26159;&#19968;&#31181;&#32039;&#20945;&#20294;&#21151;&#33021;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;&#23427;&#20307;&#29616;&#20102;&#36825;&#19968;&#31995;&#21015;&#39640;&#25928;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#28010;&#28526;&#12290;&#26412;&#25991;&#23545;Phi-2&#22312;&#30005;&#20449;&#39046;&#22495;&#20869;&#22312;&#26412;&#36136;&#19978;&#30340;&#29702;&#35299;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#37492;&#20110;&#35268;&#27169;&#30456;&#20851;&#38480;&#21046;&#65292;&#25105;&#20204;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#26041;&#27861;&#65292;&#31934;&#24515;&#22686;&#24378;&#20102;Phi-2&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04666v1 Announce Type: new  Abstract: The increasing interest in Large Language Models (LLMs) within the telecommunications sector underscores their potential to revolutionize operational efficiency. However, the deployment of these sophisticated models is often hampered by their substantial size and computational demands, raising concerns about their viability in resource-constrained environments. Addressing this challenge, recent advancements have seen the emergence of small language models that surprisingly exhibit performance comparable to their larger counterparts in many tasks, such as coding and common-sense reasoning. Phi-2, a compact yet powerful model, exemplifies this new wave of efficient small language models. This paper conducts a comprehensive evaluation of Phi-2's intrinsic understanding of the telecommunications domain. Recognizing the scale-related limitations, we enhance Phi-2's capabilities through a Retrieval-Augmented Generation approach, meticulously i
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Chain-of-Thought-Explanation&#65288;CoTE&#65289;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;(DST)&#20219;&#21153;&#65292;&#36890;&#36807;&#36880;&#27493;&#21019;&#24314;&#35814;&#32454;&#35299;&#37322;&#26469;&#30830;&#23450;&#25554;&#27133;&#20540;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20934;&#30830;&#21487;&#38752;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.04656</link><description>&lt;p&gt;
&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#30340;&#24605;&#32500;&#38142;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Chain of Thought Explanation for Dialogue State Tracking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04656
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Chain-of-Thought-Explanation&#65288;CoTE&#65289;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;(DST)&#20219;&#21153;&#65292;&#36890;&#36807;&#36880;&#27493;&#21019;&#24314;&#35814;&#32454;&#35299;&#37322;&#26469;&#30830;&#23450;&#25554;&#27133;&#20540;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20934;&#30830;&#21487;&#38752;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;(DST)&#26088;&#22312;&#35760;&#24405;&#29992;&#25143;&#22312;&#20250;&#35805;&#20114;&#21160;&#26399;&#38388;&#25552;&#20986;&#30340;&#26597;&#35810;&#21644;&#30446;&#26631;&#65292;&#36890;&#36807;&#32500;&#25252;&#19968;&#32452;&#39044;&#23450;&#20041;&#30340;&#25554;&#27133;&#21450;&#20854;&#23545;&#24212;&#30340;&#20540;&#26469;&#23454;&#29616;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#20197;&#19981;&#36879;&#26126;&#26041;&#24335;&#20915;&#23450;&#25554;&#27133;&#20540;&#65292;&#32780;&#20154;&#31867;&#36890;&#24120;&#37319;&#29992;&#26356;&#35880;&#24910;&#30340;&#26041;&#27861;&#65292;&#20174;&#30456;&#20851;&#30340;&#23545;&#35805;&#36718;&#20013;&#25910;&#38598;&#20449;&#24687;&#65292;&#28982;&#21518;&#25512;&#29702;&#20986;&#36866;&#24403;&#30340;&#20540;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32858;&#28966;&#20110;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#21517;&#20026;Chain-of-Thought-Explanation&#65288;CoTE&#65289;&#30340;&#27169;&#22411;&#65292;&#26469;&#35299;&#20915;&#30830;&#23450;&#25554;&#27133;&#20540;&#25152;&#38656;&#30340;&#27493;&#39588;&#12290;CoTE&#24314;&#31435;&#22312;&#29983;&#25104;&#24335;DST&#26694;&#26550;&#20043;&#19978;&#65292;&#26088;&#22312;&#22312;&#30830;&#23450;&#25554;&#27133;&#20540;&#21518;&#36880;&#27493;&#21019;&#24314;&#35814;&#32454;&#35299;&#37322;&#12290;&#36825;&#19968;&#36807;&#31243;&#23548;&#33268;&#20102;&#26356;&#20934;&#30830;&#21487;&#38752;&#30340;&#25554;&#27133;&#20540;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#25552;&#39640;CoTE&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#33258;&#21160;&#25913;&#20889;&#26500;&#24314;&#26356;&#27969;&#30021;&#39640;&#36136;&#37327;&#30340;&#35299;&#37322;&#65292;&#20174;&#32780;&#24418;&#25104;&#20102;CoTE-refined&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04656v1 Announce Type: new  Abstract: Dialogue state tracking (DST) aims to record user queries and goals during a conversational interaction achieved by maintaining a prede- fined set of slots and their corresponding values. Current approaches decide slot values opaquely, while humans usually adopt a more deliberate approach by collecting information from relevant dialogue turns and then reasoning the appropriate values. In this work, we focus on the steps needed to figure out slot values by proposing a model named Chain-of-Thought-Explanation (CoTE) for the DST task. CoTE, which is built on the generative DST framework, is designed to create detailed explanations step by step after determining the slot values. This process leads to more accurate and reliable slot values. More-over, to improve the reasoning ability of the CoTE, we further construct more fluent and high-quality explanations with automatic paraphrasing, leading the method CoTE-refined. Experimental results on
&lt;/p&gt;</description></item><item><title>Yi&#27169;&#22411;&#31995;&#21015;&#22522;&#20110;&#24378;&#22823;&#30340;&#22810;&#32500;&#33021;&#21147;&#65292;&#36890;&#36807;&#22522;&#20110;6B&#21644;34B&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#25193;&#23637;&#65292;&#21253;&#25324;&#32842;&#22825;&#27169;&#22411;&#12289;&#38271;&#19978;&#19979;&#25991;&#27169;&#22411;&#12289;&#28145;&#24230;&#25918;&#22823;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.04652</link><description>&lt;p&gt;
Yi: &#30001; 01.AI &#25512;&#20986;&#30340;&#24320;&#25918;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Yi: Open Foundation Models by 01.AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04652
&lt;/p&gt;
&lt;p&gt;
Yi&#27169;&#22411;&#31995;&#21015;&#22522;&#20110;&#24378;&#22823;&#30340;&#22810;&#32500;&#33021;&#21147;&#65292;&#36890;&#36807;&#22522;&#20110;6B&#21644;34B&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#25193;&#23637;&#65292;&#21253;&#25324;&#32842;&#22825;&#27169;&#22411;&#12289;&#38271;&#19978;&#19979;&#25991;&#27169;&#22411;&#12289;&#28145;&#24230;&#25918;&#22823;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Yi&#27169;&#22411;&#31995;&#21015;&#65292;&#36825;&#26159;&#19968;&#31995;&#21015;&#20855;&#26377;&#24378;&#22823;&#22810;&#32500;&#33021;&#21147;&#30340;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;Yi&#27169;&#22411;&#31995;&#21015;&#22522;&#20110;6B&#21644;34B&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#28982;&#21518;&#25105;&#20204;&#23558;&#23427;&#20204;&#25193;&#23637;&#20026;&#32842;&#22825;&#27169;&#22411;&#12289;200K&#38271;&#19978;&#19979;&#25991;&#27169;&#22411;&#12289;&#28145;&#24230;&#25918;&#22823;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#22522;&#30784;&#27169;&#22411;&#22312;&#35832;&#22914;MMLU&#20043;&#31867;&#30340;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;&#25105;&#20204;&#24494;&#35843;&#36807;&#30340;&#32842;&#22825;&#27169;&#22411;&#22312;AlpacaEval&#21644;Chatbot Arena&#31561;&#20027;&#35201;&#35780;&#20272;&#24179;&#21488;&#19978;&#20855;&#26377;&#36739;&#39640;&#30340;&#20154;&#31867;&#20559;&#22909;&#29575;&#12290;&#36890;&#36807;&#20381;&#36182;&#20110;&#25105;&#20204;&#30340;&#21487;&#25193;&#23637;&#36229;&#32423;&#35745;&#31639;&#22522;&#30784;&#35774;&#26045;&#21644;&#32463;&#20856;&#30340;Transformer&#26550;&#26500;&#65292;&#25105;&#20204;&#35748;&#20026;Yi&#27169;&#22411;&#30340;&#24615;&#33021;&#20027;&#35201;&#24402;&#22240;&#20110;&#20854;&#25968;&#25454;&#36136;&#37327;&#65292;&#36825;&#26159;&#30001;&#25105;&#20204;&#30340;&#25968;&#25454;&#24037;&#31243;&#24037;&#20316;&#25152;&#24102;&#26469;&#30340;&#12290;&#23545;&#20110;&#39044;&#35757;&#32451;&#65292;&#25105;&#20204;&#20351;&#29992;&#32423;&#32852;&#30340;&#25968;&#25454;&#21435;&#37325;&#21644;&#36136;&#37327;&#36807;&#28388;&#27969;&#27700;&#32447;&#26500;&#24314;&#20102;3100&#20159;&#20010;&#33521;&#25991;&#21644;&#20013;&#25991;&#35821;&#26009;&#24211;&#30340;&#26631;&#35760;&#12290;&#23545;&#20110;&#24494;&#35843;&#65292;&#25105;&#20204;&#23545;&#23567;&#35268;&#27169;&#27169;&#22411;&#36827;&#34892;&#20102;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04652v1 Announce Type: cross  Abstract: We introduce the Yi model family, a series of language and multimodal models that demonstrate strong multi-dimensional capabilities. The Yi model family is based on 6B and 34B pretrained language models, then we extend them to chat models, 200K long context models, depth-upscaled models, and vision-language models. Our base models achieve strong performance on a wide range of benchmarks like MMLU, and our finetuned chat models deliver strong human preference rate on major evaluation platforms like AlpacaEval and Chatbot Arena. Building upon our scalable super-computing infrastructure and the classical transformer architecture, we attribute the performance of Yi models primarily to its data quality resulting from our data-engineering efforts. For pretraining, we construct 3.1 trillion tokens of English and Chinese corpora using a cascaded data deduplication and quality filtering pipeline. For finetuning, we polish a small scale (less th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;QAQ&#65292;&#19968;&#31181;&#29992;&#20110;KV&#32531;&#23384;&#30340;&#36136;&#37327;&#33258;&#36866;&#24212;&#37327;&#21270;&#26041;&#26696;&#65292;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#20851;&#38190;&#32531;&#23384;&#21644;&#20540;&#32531;&#23384;&#23545;&#37327;&#21270;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#25935;&#24863;&#24615;&#65292;&#22240;&#27492;&#21046;&#23450;&#20102;&#19981;&#21516;&#30340;&#37327;&#21270;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.04643</link><description>&lt;p&gt;
QAQ&#65306;&#29992;&#20110;LLM KV&#32531;&#23384;&#30340;&#36136;&#37327;&#33258;&#36866;&#24212;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
QAQ: Quality Adaptive Quantization for LLM KV Cache
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04643
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;QAQ&#65292;&#19968;&#31181;&#29992;&#20110;KV&#32531;&#23384;&#30340;&#36136;&#37327;&#33258;&#36866;&#24212;&#37327;&#21270;&#26041;&#26696;&#65292;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#20851;&#38190;&#32531;&#23384;&#21644;&#20540;&#32531;&#23384;&#23545;&#37327;&#21270;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#25935;&#24863;&#24615;&#65292;&#22240;&#27492;&#21046;&#23450;&#20102;&#19981;&#21516;&#30340;&#37327;&#21270;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM&#30340;&#20986;&#29616;&#22312;NLP&#24212;&#29992;&#20013;&#24341;&#21457;&#20102;&#19968;&#27874;&#26032;&#30340;&#31361;&#30772;&#65292;&#23588;&#20854;&#22312;&#35832;&#22914;&#38382;&#31572;&#31995;&#32479;&#21644;&#25991;&#26412;&#29983;&#25104;&#31561;&#39046;&#22495;&#12290;&#38543;&#30528;&#23545;&#26356;&#38271;&#19978;&#19979;&#25991;&#30340;&#38656;&#27714;&#22686;&#38271;&#65292;&#27169;&#22411;&#37096;&#32626;&#20013;&#20986;&#29616;&#20102;&#19968;&#20010;&#37325;&#35201;&#29942;&#39048;&#65292;&#21363;&#30001;&#20110;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#32447;&#24615;&#22686;&#21152;&#32780;&#23548;&#33268;&#30340;Key-Value (KV) cache&#30340;&#25193;&#23637;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#21508;&#31181;&#20551;&#35774;&#65292;&#20363;&#22914;&#26681;&#25454;&#27880;&#24847;&#21147;&#20998;&#25968;&#23545;KV cache&#36827;&#34892;&#25490;&#24207;&#20197;&#36827;&#34892;&#26367;&#25442;&#25110;&#39537;&#36880;&#65292;&#20197;&#21387;&#32553;KV cache&#24182;&#25552;&#39640;&#27169;&#22411;&#21534;&#21520;&#37327;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31574;&#30053;&#20351;&#29992;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#21487;&#33021;&#20250;&#38169;&#35823;&#22320;&#39537;&#36880;&#20851;&#38190;&#30340;KV&#32531;&#23384;&#65292;&#20174;&#32780;&#20005;&#37325;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;QAQ&#65292;&#19968;&#31181;&#29992;&#20110;KV&#32531;&#23384;&#30340;&#36136;&#37327;&#33258;&#36866;&#24212;&#37327;&#21270;&#26041;&#26696;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#20851;&#38190;&#32531;&#23384;&#21644;&#20540;&#32531;&#23384;&#23545;&#37327;&#21270;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#25935;&#24863;&#24615;&#65292;&#20174;&#32780;&#24341;&#21457;&#20102;&#38024;&#23545;&#23427;&#20204;&#30340;&#38750;&#22343;&#21248;&#37327;&#21270;&#31574;&#30053;&#30340;&#21046;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04643v1 Announce Type: new  Abstract: The emergence of LLMs has ignited a fresh surge of breakthroughs in NLP applications, particularly in domains such as question-answering systems and text generation. As the need for longer context grows, a significant bottleneck in model deployment emerges due to the linear expansion of the Key-Value (KV) cache with the context length. Existing methods primarily rely on various hypotheses, such as sorting the KV cache based on attention scores for replacement or eviction, to compress the KV cache and improve model throughput. However, heuristics used by these strategies may wrongly evict essential KV cache, which can significantly degrade model performance. In this paper, we propose QAQ, a Quality Adaptive Quantization scheme for the KV cache. We theoretically demonstrate that key cache and value cache exhibit distinct sensitivities to quantization, leading to the formulation of separate quantization strategies for their non-uniform quan
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#39318;&#20010;&#29992;&#20110;&#39532;&#21152;&#24076;&#35821;-&#21360;&#22320;&#35821;-&#33521;&#35821;&#20195;&#30721;&#28151;&#21512;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#35821;&#35328;&#23398;&#20998;&#26512;&#21644;&#32479;&#35745;&#30740;&#31350;&#26469;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.04639</link><description>&lt;p&gt;
MaCmS&#65306;&#39532;&#21152;&#24076;&#20195;&#30721;&#28151;&#21512;&#25968;&#25454;&#38598;&#29992;&#20110;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
MaCmS: Magahi Code-mixed Dataset for Sentiment Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04639
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#39318;&#20010;&#29992;&#20110;&#39532;&#21152;&#24076;&#35821;-&#21360;&#22320;&#35821;-&#33521;&#35821;&#20195;&#30721;&#28151;&#21512;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#35821;&#35328;&#23398;&#20998;&#26512;&#21644;&#32479;&#35745;&#30740;&#31350;&#26469;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24773;&#24863;&#25968;&#25454;&#38598; MaCMS&#65292;&#29992;&#20110;&#39532;&#21152;&#24076;&#35821;-&#21360;&#22320;&#35821;-&#33521;&#35821;&#65288;MHE&#65289;&#20195;&#30721;&#28151;&#21512;&#35821;&#35328;&#65292;&#20854;&#20013;&#39532;&#21152;&#24076;&#35821;&#26159;&#19968;&#31181;&#36164;&#28304;&#36739;&#23569;&#30340;&#23569;&#25968;&#27665;&#26063;&#35821;&#35328;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#26159;&#29992;&#20110;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#30340;&#31532;&#19968;&#20010;&#39532;&#21152;&#24076;&#35821;-&#21360;&#22320;&#35821;-&#33521;&#35821;&#20195;&#30721;&#28151;&#21512;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#35821;&#35328;&#23398;&#20998;&#26512;&#65292;&#20197;&#20102;&#35299;&#20195;&#30721;&#28151;&#21512;&#30340;&#32467;&#26500;&#65292;&#24182;&#36827;&#34892;&#20102;&#32479;&#35745;&#30740;&#31350;&#65292;&#20197;&#20102;&#35299;&#19981;&#21516;&#26497;&#24615;&#21457;&#35328;&#32773;&#30340;&#35821;&#35328;&#20559;&#22909;&#12290;&#36890;&#36807;&#36825;&#20123;&#20998;&#26512;&#65292;&#25105;&#20204;&#36824;&#35757;&#32451;&#20102;&#22522;&#20934;&#27169;&#22411;&#26469;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04639v1 Announce Type: new  Abstract: The present paper introduces new sentiment data, MaCMS, for Magahi-Hindi-English (MHE) code-mixed language, where Magahi is a less-resourced minority language. This dataset is the first Magahi-Hindi-English code-mixed dataset for sentiment analysis tasks. Further, we also provide a linguistics analysis of the dataset to understand the structure of code-mixing and a statistical study to understand the language preferences of speakers with different polarities. With these analyses, we also train baseline models to evaluate the dataset's quality.
&lt;/p&gt;</description></item><item><title>MedFLIP&#26159;&#19968;&#31181;&#29992;&#20110;&#21307;&#23398;&#20998;&#26512;&#30340;&#24555;&#36895;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;SVD&#25439;&#22833;&#22686;&#24378;&#21307;&#23398;&#22270;&#20687;&#29305;&#24449;&#34920;&#31034;&#23398;&#20064;&#65292;&#39564;&#35777;&#20102;&#29992;&#35821;&#35328;&#21487;&#20197;&#25552;&#39640;&#38646;&#26679;&#26412;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.04626</link><description>&lt;p&gt;
MedFLIP&#65306;&#21307;&#23398;&#35270;&#35273;&#19982;&#35821;&#35328;&#33258;&#30417;&#30563;&#24555;&#36895;&#39044;&#35757;&#32451;&#19982;&#25513;&#34109;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
MedFLIP: Medical Vision-and-Language Self-supervised Fast Pre-Training with Masked Autoencoder
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04626
&lt;/p&gt;
&lt;p&gt;
MedFLIP&#26159;&#19968;&#31181;&#29992;&#20110;&#21307;&#23398;&#20998;&#26512;&#30340;&#24555;&#36895;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;SVD&#25439;&#22833;&#22686;&#24378;&#21307;&#23398;&#22270;&#20687;&#29305;&#24449;&#34920;&#31034;&#23398;&#20064;&#65292;&#39564;&#35777;&#20102;&#29992;&#35821;&#35328;&#21487;&#20197;&#25552;&#39640;&#38646;&#26679;&#26412;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#20998;&#26512;&#39046;&#22495;&#65292;&#24191;&#27867;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#25513;&#34109;&#33258;&#32534;&#30721;&#22120;&#65288;MAEs&#65289;&#21644;&#22810;&#27169;&#24577;&#25968;&#25454;&#20043;&#38388;&#20114;&#30456;&#23398;&#20064;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;MAEs&#23545;&#36328;&#27169;&#24577;&#23398;&#20064;&#30340;&#24433;&#21709;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;MedFLIP&#65292;&#19968;&#31181;&#29992;&#20110;&#21307;&#23398;&#20998;&#26512;&#30340;&#24555;&#36895;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#25105;&#20204;&#25506;&#32034;&#20351;&#29992;MAEs&#36827;&#34892;&#36328;&#39046;&#22495;&#38646;&#26679;&#26412;&#23398;&#20064;&#65292;&#20174;&#32780;&#22686;&#24378;&#27169;&#22411;&#22312;&#21307;&#23398;&#35786;&#26029;&#20013;&#24120;&#35265;&#30340;&#26377;&#38480;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#23545;&#22270;&#20687;&#36827;&#34892;&#25513;&#34109;&#19981;&#20250;&#24433;&#21709;&#36328;&#27169;&#24577;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SVD&#25439;&#22833;&#20197;&#22686;&#24378;&#21307;&#23398;&#22270;&#20687;&#29305;&#24449;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#36825;&#31867;&#25968;&#25454;&#30340;&#32467;&#26500;&#22797;&#26434;&#24615;&#26469;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#20351;&#29992;&#35821;&#35328;&#23558;&#25552;&#39640;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#12290;MedFLIP&#23545;&#25513;&#34109;&#36807;&#31243;&#30340;&#25193;&#23637;&#26631;&#24535;&#30528;&#35813;&#39046;&#22495;&#30340;&#36827;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04626v1 Announce Type: cross  Abstract: Within the domain of medical analysis, extensive research has explored the potential of mutual learning between Masked Autoencoders(MAEs) and multimodal data. However, the impact of MAEs on intermodality remains a key challenge. We introduce MedFLIP, a Fast Language-Image Pre-training method for Medical analysis. We explore MAEs for zero-shot learning with crossed domains, which enhances the model ability to learn from limited data, a common scenario in medical diagnostics. We verify that masking an image does not affect intermodal learning. Furthermore, we propose the SVD loss to enhance the representation learning for characteristics of medical images, aiming to improve classification accuracy by leveraging the structural intricacies of such data. Lastly, we validate using language will improve the zero-shot performance for the medical image analysis. MedFLIP scaling of the masking process marks an advancement in the field, offering 
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35843;&#24230;&#26426;&#21046;&#8220;&#39034;&#24207;&#26500;&#36896;&#20943;&#23569;&#8221;&#65292;&#26088;&#22312;&#23454;&#29616;&#22810;&#25773;&#24182;&#21457;&#36890;&#20449;&#30340;&#30830;&#23450;&#24615;&#65292;&#25193;&#23637;&#20102;CCS&#30340;&#25216;&#26415;&#35774;&#32622;&#65292;&#35777;&#26126;&#20102;&#26500;&#36896;&#20943;&#23569;&#30340;&#27719;&#32858;&#23646;&#24615;&#65292;&#23637;&#31034;&#20102;&#22312;&#19968;&#20123;&#35821;&#27861;&#38480;&#21046;&#19979;&#36816;&#31639;&#31526;&#30340;&#32467;&#26500;&#36830;&#36143;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.04618</link><description>&lt;p&gt;
&#26102;&#26631;CCS&#20013;&#30340;&#24378;&#20248;&#20808;&#32423;&#21644;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Strong Priority and Determinacy in Timed CCS
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04618
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35843;&#24230;&#26426;&#21046;&#8220;&#39034;&#24207;&#26500;&#36896;&#20943;&#23569;&#8221;&#65292;&#26088;&#22312;&#23454;&#29616;&#22810;&#25773;&#24182;&#21457;&#36890;&#20449;&#30340;&#30830;&#23450;&#24615;&#65292;&#25193;&#23637;&#20102;CCS&#30340;&#25216;&#26415;&#35774;&#32622;&#65292;&#35777;&#26126;&#20102;&#26500;&#36896;&#20943;&#23569;&#30340;&#27719;&#32858;&#23646;&#24615;&#65292;&#23637;&#31034;&#20102;&#22312;&#19968;&#20123;&#35821;&#27861;&#38480;&#21046;&#19979;&#36816;&#31639;&#31526;&#30340;&#32467;&#26500;&#36830;&#36143;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20855;&#26377;&#20248;&#20808;&#32423;&#30340;&#32463;&#20856;&#36827;&#31243;&#20195;&#25968;&#29702;&#35770;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#39034;&#24207;&#26500;&#36896;&#20943;&#23569;&#8221;&#30340;&#26032;&#35843;&#24230;&#26426;&#21046;&#65292;&#26088;&#22312;&#25429;&#25417;&#21516;&#27493;&#32534;&#31243;&#30340;&#26412;&#36136;&#12290;&#36825;&#31181;&#35780;&#20272;&#31574;&#30053;&#30340;&#29420;&#29305;&#23646;&#24615;&#26159;&#36890;&#36807;&#26500;&#36896;&#23454;&#29616;&#22810;&#25773;&#24182;&#21457;&#36890;&#20449;&#30340;&#30830;&#23450;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#27169;&#25311;&#20855;&#26377;&#23545;&#32570;&#22833;&#21453;&#24212;&#30340;&#20849;&#20139;&#20869;&#23384;&#22810;&#32447;&#31243;&#65292;&#22240;&#20026;&#23427;&#26159;Esterel&#32534;&#31243;&#35821;&#35328;&#30340;&#26680;&#24515;&#12290;&#22312;&#36890;&#36807;&#26102;&#38047;&#21644;&#20248;&#20808;&#32423;&#25193;&#23637;&#30340;CCS&#30340;&#25216;&#26415;&#35774;&#32622;&#20013;&#65292;&#23545;&#20110;&#25105;&#20204;&#31216;&#20026;&#8220;&#32467;&#26500;&#36830;&#36143;&#8221;&#30340;&#22823;&#31867;&#36807;&#31243;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26500;&#36896;&#20943;&#23569;&#30340;&#27719;&#32858;&#23646;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#65292;&#22312;&#19968;&#20123;&#31216;&#20026;&#8220;&#21487;&#26530;&#32445;&#8221;&#30340;&#35821;&#27861;&#38480;&#21046;&#19979;&#65292;&#21069;&#32512;&#12289;&#27714;&#21644;&#12289;&#24182;&#34892;&#32452;&#25104;&#12289;&#38480;&#21046;&#21644;&#38544;&#34255;&#30340;&#36816;&#31639;&#31526;&#20445;&#25345;&#32467;&#26500;&#36830;&#36143;&#12290;&#36825;&#28085;&#30422;&#20102;&#19968;&#20010;&#20005;&#26684;&#26356;&#22823;&#30340;&#36807;&#31243;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04618v1 Announce Type: cross  Abstract: Building on the classical theory of process algebra with priorities, we identify a new scheduling mechanism, called "sequentially constructive reduction" which is designed to capture the essence of synchronous programming. The distinctive property of this evaluation strategy is to achieve determinism-by-construction for multi-cast concurrent communication. In particular, it permits us to model shared memory multi-threading with reaction to absence as it lies at the core of the programming language Esterel. In the technical setting of CCS extended by clocks and priorities, we prove for a large class of processes, which we call "structurally coherent" the confluence property for constructive reductions. We further show that under some syntactic restrictions, called "pivotable" the operators of prefix, summation, parallel composition, restriction and hiding preserve structural coherence. This covers a strictly larger class of processes co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#24615;&#25968;&#25454;&#38598;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20010;&#26088;&#22312;&#35299;&#20915;&#23454;&#20307;&#38142;&#25509;&#20219;&#21153;&#30340;&#26032;&#38382;&#39064;&#65306;&#21333;&#20803;&#26684;&#20869;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#25552;&#31034;&#26694;&#26550;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#19968;&#26032;&#20219;&#21153;&#19978;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.04577</link><description>&lt;p&gt;
Wiki-TabNER:&#36890;&#36807;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25512;&#36827;&#34920;&#26684;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Wiki-TabNER:Advancing Table Interpretation Through Named Entity Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04577
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#24615;&#25968;&#25454;&#38598;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20010;&#26088;&#22312;&#35299;&#20915;&#23454;&#20307;&#38142;&#25509;&#20219;&#21153;&#30340;&#26032;&#38382;&#39064;&#65306;&#21333;&#20803;&#26684;&#20869;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#25552;&#31034;&#26694;&#26550;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#19968;&#26032;&#20219;&#21153;&#19978;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04577v1 &#21457;&#24067;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#32593;&#32476;&#34920;&#26684;&#21253;&#21547;&#22823;&#37327;&#23453;&#36149;&#30693;&#35782;&#65292;&#28608;&#21457;&#20102;&#26088;&#22312;&#35299;&#20915;&#34920;&#26684;&#35299;&#37322;&#65288;TI&#65289;&#20219;&#21153;&#30340;&#34920;&#26684;&#35821;&#35328;&#27169;&#22411;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#29992;&#20110;&#35780;&#20272;TI&#20219;&#21153;&#30340;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29305;&#21035;&#20851;&#27880;&#23454;&#20307;&#38142;&#25509;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#26174;&#31034;&#65292;&#35813;&#25968;&#25454;&#38598;&#36807;&#20110;&#31616;&#21270;&#65292;&#21487;&#33021;&#38477;&#20302;&#20854;&#29992;&#20110;&#20840;&#38754;&#35780;&#20272;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#26410;&#20934;&#30830;&#20195;&#34920;&#34920;&#26684;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#22806;&#35266;&#12290;&#20026;&#20811;&#26381;&#36825;&#19968;&#32570;&#28857;&#65292;&#25105;&#20204;&#26500;&#24314;&#24182;&#27880;&#37322;&#20102;&#19968;&#20010;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#38500;&#20102;&#20171;&#32461;&#26032;&#25968;&#25454;&#38598;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#26088;&#22312;&#35299;&#20915;&#23454;&#20307;&#38142;&#25509;&#20219;&#21153;&#30340;&#26032;&#38382;&#39064;&#65306;&#21333;&#20803;&#26684;&#20869;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25552;&#31034;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#26032;&#24320;&#21457;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36825;&#19968;&#26032;&#30340;TI&#20219;&#21153;&#19978;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#35774;&#32622;&#19979;&#23545;&#25552;&#31034;LLMs&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#65292;&#20854;&#20013;&#25105;&#20204;&#21516;&#26102;&#20351;&#29992;&#20102;&#38543;&#26426;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04577v1 Announce Type: new  Abstract: Web tables contain a large amount of valuable knowledge and have inspired tabular language models aimed at tackling table interpretation (TI) tasks. In this paper, we analyse a widely used benchmark dataset for evaluation of TI tasks, particularly focusing on the entity linking task. Our analysis reveals that this dataset is overly simplified, potentially reducing its effectiveness for thorough evaluation and failing to accurately represent tables as they appear in the real-world. To overcome this drawback, we construct and annotate a new more challenging dataset. In addition to introducing the new dataset, we also introduce a novel problem aimed at addressing the entity linking task: named entity recognition within cells. Finally, we propose a prompting framework for evaluating the newly developed large language models (LLMs) on this novel TI task. We conduct experiments on prompting LLMs under various settings, where we use both random
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#23569;&#26679;&#26412;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26694;&#26550;&#20197;&#27169;&#25311;&#23454;&#20307;&#21644;&#19977;&#20803;&#32452;&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#23398;&#20064;&#26381;&#20174;&#39640;&#26031;&#20998;&#24067;&#30340;&#34920;&#31034;&#26469;&#26356;&#22909;&#22320;&#29702;&#35299;&#26377;&#38480;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2403.04521</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#20851;&#31995;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#23569;&#26679;&#26412;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Aware Relational Graph Neural Network for Few-Shot Knowledge Graph Completion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04521
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#23569;&#26679;&#26412;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26694;&#26550;&#20197;&#27169;&#25311;&#23454;&#20307;&#21644;&#19977;&#20803;&#32452;&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#23398;&#20064;&#26381;&#20174;&#39640;&#26031;&#20998;&#24067;&#30340;&#34920;&#31034;&#26469;&#26356;&#22909;&#22320;&#29702;&#35299;&#26377;&#38480;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26088;&#22312;&#22312;&#32473;&#23450;&#23569;&#37327;&#26696;&#20363;&#21442;&#32771;&#23454;&#20307;&#23545;&#30340;&#24773;&#20917;&#19979;&#26597;&#35810;&#26576;&#31181;&#20851;&#31995;&#30340;&#26410;&#30693;&#20107;&#23454;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#23569;&#26679;&#26412;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26694;&#26550;&#65288;UFKGC&#65289;&#65292;&#20197;&#27169;&#25311;&#19981;&#30830;&#23450;&#24615;&#65292;&#26356;&#22909;&#22320;&#29702;&#35299;&#26377;&#38480;&#25968;&#25454;&#65292;&#36890;&#36807;&#23398;&#20064;&#26381;&#20174;&#39640;&#26031;&#20998;&#24067;&#30340;&#34920;&#31034;&#26469;&#36991;&#20813;&#30001;&#23454;&#20307;&#21644;&#19977;&#20803;&#32452;&#19981;&#30830;&#23450;&#24615;&#23548;&#33268;&#30340;&#22122;&#22768;&#21103;&#20316;&#29992;&#65292;&#20808;&#20026;&#28304;&#23454;&#20307;&#23545;&#30340;&#19981;&#30830;&#23450;&#24615;&#33539;&#22260;&#35774;&#35745;&#19981;&#30830;&#23450;&#24615;&#34920;&#31034;&#65292;&#28982;&#21518;&#23558;&#29305;&#24449;&#34920;&#31034;&#36716;&#25442;&#20026;&#39640;&#26031;&#20998;&#24067;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#25972;&#21512;&#20855;&#26377;&#19981;&#30830;&#23450;&#29305;&#24449;&#30340;&#37051;&#23621;&#20197;&#29992;&#20110;&#23454;&#20307;&#29305;&#24449;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#20851;&#31995;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;UR-GNN&#65289;&#36827;&#34892;&#21367;&#31215;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04521v1 Announce Type: new  Abstract: Few-shot knowledge graph completion (FKGC) aims to query the unseen facts of a relation given its few-shot reference entity pairs. The side effect of noises due to the uncertainty of entities and triples may limit the few-shot learning, but existing FKGC works neglect such uncertainty, which leads them more susceptible to limited reference samples with noises. In this paper, we propose a novel uncertainty-aware few-shot KG completion framework (UFKGC) to model uncertainty for a better understanding of the limited data by learning representations under Gaussian distribution. Uncertainty representation is first designed for estimating the uncertainty scope of the entity pairs after transferring feature representations into a Gaussian distribution. Further, to better integrate the neighbors with uncertainty characteristics for entity features, we design an uncertainty-aware relational graph neural network (UR-GNN) to conduct convolution ope
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25506;&#32034;&#20102;&#20174;&#19978;&#19979;&#25991;&#23398;&#20064;&#32773;&#21040;&#32763;&#35793;&#27169;&#22411;&#30340;&#36716;&#21464;&#36807;&#31243;&#65292;&#24182;&#21457;&#29616;&#20102;"&#20219;&#21153;&#35782;&#21035;"&#28857;&#20197;&#21450;&#21033;&#29992;&#35813;&#28857;&#30340;&#20887;&#20313;&#24615;&#21487;&#33410;&#32422;&#35745;&#31639;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.04510</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19978;&#19979;&#25991;&#32763;&#35793;&#21457;&#29983;&#22312;&#21738;&#37324;
&lt;/p&gt;
&lt;p&gt;
Where does In-context Translation Happen in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04510
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25506;&#32034;&#20102;&#20174;&#19978;&#19979;&#25991;&#23398;&#20064;&#32773;&#21040;&#32763;&#35793;&#27169;&#22411;&#30340;&#36716;&#21464;&#36807;&#31243;&#65292;&#24182;&#21457;&#29616;&#20102;"&#20219;&#21153;&#35782;&#21035;"&#28857;&#20197;&#21450;&#21033;&#29992;&#35813;&#28857;&#30340;&#20887;&#20313;&#24615;&#21487;&#33410;&#32422;&#35745;&#31639;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#23637;&#31034;&#20986;&#33021;&#22815;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#25191;&#34892;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#30340;&#33021;&#21147;&#65292;&#20294;&#20851;&#20110;&#27169;&#22411;&#22312;&#20309;&#22788;&#25191;&#34892;&#36825;&#19968;&#20219;&#21153;&#30456;&#23545;&#20110;&#25552;&#31034;&#25351;&#20196;&#21644;&#28436;&#31034;&#31034;&#20363;&#30340;&#24773;&#20917;&#30693;&#20043;&#29978;&#23569;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#34920;&#24449;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#19978;&#19979;&#25991;&#23398;&#20064;&#32773;&#36716;&#21464;&#20026;&#32763;&#35793;&#27169;&#22411;&#30340;&#21306;&#22495;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#22312;GPTNeo2.7B&#12289;Bloom3B&#12289;Llama7b&#21644;Llama7b-chat&#19978;&#30340;&#36880;&#23618;&#19978;&#19979;&#25991;&#23631;&#34109;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;"&#20219;&#21153;&#35782;&#21035;"&#28857;&#30340;&#35777;&#25454;&#65292;&#21363;&#32763;&#35793;&#20219;&#21153;&#34987;&#32534;&#30721;&#21040;&#36755;&#20837;&#34920;&#31034;&#20013;&#65292;&#24182;&#19988;&#19981;&#20877;&#38656;&#35201;&#20851;&#27880;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35266;&#23519;&#21040;&#23436;&#20840;&#23631;&#34109;&#23618;&#26102;&#20302;&#24615;&#33021;&#19982;&#20219;&#21153;&#35782;&#21035;&#23618;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;&#21033;&#29992;&#36825;&#31181;&#20887;&#20313;&#24615;&#22312;&#25552;&#31034;5&#20010;&#31034;&#20363;&#26102;&#33410;&#32422;&#20102;45%&#30340;&#35745;&#31639;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04510v1 Announce Type: cross  Abstract: Self-supervised large language models have demonstrated the ability to perform Machine Translation (MT) via in-context learning, but little is known about where the model performs the task with respect to prompt instructions and demonstration examples. In this work, we attempt to characterize the region where large language models transition from in-context learners to translation models. Through a series of layer-wise context-masking experiments on \textsc{GPTNeo2.7B}, \textsc{Bloom3B}, \textsc{Llama7b} and \textsc{Llama7b-chat}, we demonstrate evidence of a "task recognition" point where the translation task is encoded into the input representations and attention to context is no longer necessary. We further observe correspondence between the low performance when masking out entire layers, and the task recognition layers. Taking advantage of this redundancy results in 45\% computational savings when prompting with 5 examples, and tas
&lt;/p&gt;</description></item><item><title>NLPre&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#38752;&#19988;&#20844;&#24179;&#30340;&#35821;&#35328;&#20013;&#24515;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#65292;&#20351;&#24471;&#21487;&#20197;&#20840;&#38754;&#25345;&#32493;&#35780;&#20272;&#22810;&#20010;NLPre&#24037;&#20855;&#30340;&#24615;&#33021;&#65292;&#24182;&#21487;&#38752;&#22320;&#36319;&#36394;&#20854;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.04507</link><description>&lt;p&gt;
NLPre: &#19968;&#31181;&#38754;&#21521;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#30340;&#35821;&#35328;&#20013;&#24515;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
NLPre: a revised approach towards language-centric benchmarking of Natural Language Preprocessing systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04507
&lt;/p&gt;
&lt;p&gt;
NLPre&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#38752;&#19988;&#20844;&#24179;&#30340;&#35821;&#35328;&#20013;&#24515;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#65292;&#20351;&#24471;&#21487;&#20197;&#20840;&#38754;&#25345;&#32493;&#35780;&#20272;&#22810;&#20010;NLPre&#24037;&#20855;&#30340;&#24615;&#33021;&#65292;&#24182;&#21487;&#38752;&#22320;&#36319;&#36394;&#20854;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#26550;&#26500;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLPre&#65289;&#24037;&#20855;&#30340;&#23835;&#36215;&#65292;&#36825;&#20123;&#24037;&#20855;&#33021;&#22815;&#35299;&#20915;&#21021;&#27493;&#30340;NLP&#20219;&#21153;&#65288;&#20363;&#22914;&#26631;&#35760;&#21270;&#12289;&#35789;&#24615;&#26631;&#27880;&#12289;&#20381;&#23384;&#21477;&#27861;&#20998;&#26512;&#25110;&#24418;&#24577;&#20998;&#26512;&#65289;&#32780;&#26080;&#38656;&#20219;&#20309;&#22806;&#37096;&#35821;&#35328;&#25351;&#23548;&#12290;&#22312;&#36153;&#26102;&#36153;&#21147;&#22320;&#27604;&#36739;&#26032;&#22411;&#35299;&#20915;&#26041;&#26696;&#19982;&#20381;&#36182;&#22522;&#20110;&#35268;&#21017;&#30340;&#24418;&#24577;&#20998;&#26512;&#22120;&#25110;&#35789;&#20856;&#30340;&#25104;&#29087;&#39044;&#22788;&#29702;&#24037;&#20855;&#21253;&#30340;&#22522;&#30784;&#19978;&#26159;&#24456;&#22256;&#38590;&#30340;&#12290;&#37492;&#20110;&#29616;&#26377;NLPre&#35780;&#20272;&#26041;&#27861;&#30340;&#32570;&#38519;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#31181;&#21487;&#38752;&#21644;&#20844;&#24179;&#30340;&#35780;&#20272;&#26041;&#27861;&#21644;&#24615;&#33021;&#25253;&#21578;&#12290;&#21463;GLUE&#22522;&#20934;&#27979;&#35797;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#30340;&#35821;&#35328;&#20013;&#24515;&#22522;&#20934;&#27979;&#35797;&#31995;&#32479;&#23454;&#29616;&#20102;&#23545;&#22810;&#20010;NLPre&#24037;&#20855;&#30340;&#20840;&#38754;&#25345;&#32493;&#35780;&#20272;&#65292;&#21516;&#26102;&#21487;&#38752;&#22320;&#36319;&#36394;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#21407;&#22411;&#24212;&#29992;&#31243;&#24207;&#34987;&#37197;&#32622;&#20026;&#27874;&#20848;&#35821;&#65292;&#24182;&#19982;&#31934;&#24515;&#32452;&#32455;&#30340;NLPre-PL&#22522;&#20934;&#22871;&#20214;&#38598;&#25104;&#12290;&#22522;&#20110;&#36825;&#19968;&#22522;&#20934;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#23454;&#39564;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04507v1 Announce Type: new  Abstract: With the advancements of transformer-based architectures, we observe the rise of natural language preprocessing (NLPre) tools capable of solving preliminary NLP tasks (e.g. tokenisation, part-of-speech tagging, dependency parsing, or morphological analysis) without any external linguistic guidance. It is arduous to compare novel solutions to well-entrenched preprocessing toolkits, relying on rule-based morphological analysers or dictionaries. Aware of the shortcomings of existing NLPre evaluation approaches, we investigate a novel method of reliable and fair evaluation and performance reporting. Inspired by the GLUE benchmark, the proposed language-centric benchmarking system enables comprehensive ongoing evaluation of multiple NLPre tools, while credibly tracking their performance. The prototype application is configured for Polish and integrated with the thoroughly assembled NLPre-PL benchmark. Based on this benchmark, we conduct an ex
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GraphInstruct&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#22686;&#24378;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;GraphLM&#21644;&#25552;&#20986;GraphLM+&#27169;&#22411;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#22270;&#25512;&#29702;&#33021;&#21147;&#22686;&#24378;&#12290;</title><link>https://arxiv.org/abs/2403.04483</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#29702;&#35299;&#21644;&#25512;&#29702;&#21151;&#33021;&#22686;&#24378;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;GraphInstruct
&lt;/p&gt;
&lt;p&gt;
GraphInstruct: Empowering Large Language Models with Graph Understanding and Reasoning Capability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04483
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GraphInstruct&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#22686;&#24378;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;GraphLM&#21644;&#25552;&#20986;GraphLM+&#27169;&#22411;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#22270;&#25512;&#29702;&#33021;&#21147;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#21644;&#22686;&#24378;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36890;&#29992;&#33021;&#21147;&#19968;&#30452;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#22270;&#26159;&#29616;&#23454;&#19990;&#30028;&#20013;&#24120;&#35265;&#30340;&#25968;&#25454;&#32467;&#26500;&#65292;&#29702;&#35299;&#22270;&#25968;&#25454;&#23545;&#20110;&#25512;&#36827;&#36890;&#29992;&#26234;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35780;&#20272;&#21644;&#22686;&#24378;LLMs&#30340;&#22270;&#29702;&#35299;&#33021;&#21147;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GraphInstruct&#30340;&#22522;&#20934;&#65292;&#20840;&#38754;&#21253;&#25324;21&#20010;&#32463;&#20856;&#22270;&#25512;&#29702;&#20219;&#21153;&#65292;&#25552;&#20379;&#22810;&#26679;&#30340;&#22270;&#29983;&#25104;&#27969;&#27700;&#32447;&#21644;&#35814;&#32454;&#30340;&#25512;&#29702;&#27493;&#39588;&#12290;&#22522;&#20110;GraphInstruct&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#39640;&#25928;&#30340;&#25351;&#23548;&#35843;&#25972;&#26500;&#24314;&#20102;GraphLM&#65292;&#23637;&#31034;&#20986;&#26174;&#33879;&#30340;&#22270;&#29702;&#35299;&#33021;&#21147;&#12290;&#20026;&#20102;&#22686;&#24378;LLM&#30340;&#22270;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27493;&#39588;&#25513;&#30721;&#35757;&#32451;&#31574;&#30053;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;GraphLM+&#30340;&#27169;&#22411;&#12290;&#20316;&#20026;&#22686;&#24378;LLMs&#22270;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#30340;&#20808;&#39537;&#24615;&#21162;&#21147;&#20043;&#19968;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04483v1 Announce Type: new  Abstract: Evaluating and enhancing the general capabilities of large language models (LLMs) has been an important research topic. Graph is a common data structure in the real world, and understanding graph data is a crucial part for advancing general intelligence. To evaluate and enhance the graph understanding abilities of LLMs, in this paper, we propose a benchmark named GraphInstruct, which comprehensively includes 21 classical graph reasoning tasks, providing diverse graph generation pipelines and detailed reasoning steps. Based on GraphInstruct, we further construct GraphLM through efficient instruction-tuning, which shows prominent graph understanding capability. In order to enhance the LLM with graph reasoning capability as well, we propose a step mask training strategy, and construct a model named GraphLM+. As one of the pioneering efforts to enhance the graph understanding and reasoning abilities of LLMs, extensive experiments have demons
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21475;&#35821;&#35821;&#35328;&#22810;&#30446;&#26631;&#29702;&#35299;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827;&#23454;&#20307;&#27133;&#21644;&#23376;&#30446;&#26631;&#25351;&#20196;&#30340;&#21019;&#26032;&#25216;&#26415;&#65292;&#24182;&#23637;&#31034;&#20102;LLMs&#22312;&#22810;&#30446;&#26631;SLU&#27169;&#22411;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.04481</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#29702;&#35299;&#22810;&#30446;&#26631;&#21475;&#35821;&#35821;&#35328;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Large Language Model Understand Multi-Intent Spoken Language ?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04481
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21475;&#35821;&#35821;&#35328;&#22810;&#30446;&#26631;&#29702;&#35299;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827;&#23454;&#20307;&#27133;&#21644;&#23376;&#30446;&#26631;&#25351;&#20196;&#30340;&#21019;&#26032;&#25216;&#26415;&#65292;&#24182;&#23637;&#31034;&#20102;LLMs&#22312;&#22810;&#30446;&#26631;SLU&#27169;&#22411;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#22810;&#30446;&#26631;&#21475;&#35821;&#35821;&#35328;&#29702;&#35299;&#65288;SLU&#65289;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;SLU&#29615;&#22659;&#20013;&#21033;&#29992;LLMs&#29983;&#25104;&#33021;&#21147;&#30340;&#29420;&#29305;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#21019;&#26032;&#25216;&#26415;&#37325;&#26032;&#37197;&#32622;&#20102;&#23454;&#20307;&#27133;&#65292;&#19987;&#38376;&#29992;&#20110;LLMs&#22312;&#22810;&#30446;&#26631;SLU&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#24341;&#20837;&#20102;&#23376;&#30446;&#26631;&#25351;&#20196;&#65288;SII&#65289;&#30340;&#27010;&#24565;&#65292;&#22686;&#24378;&#20102;&#23545;&#19981;&#21516;&#39046;&#22495;&#20869;&#22797;&#26434;&#22810;&#30446;&#26631;&#20132;&#27969;&#30340;&#35299;&#21078;&#21644;&#35299;&#37322;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#25968;&#25454;&#38598;&#65292;&#34987;&#31216;&#20026;LM-MixATIS&#21644;LM-MixSNIPS&#65292;&#26159;&#20174;&#29616;&#26377;&#22522;&#20934;&#20013;&#31934;&#24515;&#21046;&#20316;&#30340;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;LLMs&#21487;&#20197;&#21305;&#37197;&#24182;&#28508;&#22312;&#22320;&#36229;&#36234;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22810;&#30446;&#26631;SLU&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#23427;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;LLMs&#22312;&#21508;&#31181;&#24847;&#22270;&#37197;&#32622;&#21644;&#25968;&#25454;&#38598;&#27604;&#20363;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#20010;&#24320;&#21019;&#24615;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#21363;&#23454;&#20307;&#27133;&#20934;&#30830;&#24615;&#65288;ESA&#65289;&#21644;Com
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04481v1 Announce Type: cross  Abstract: This study marks a significant advancement by harnessing Large Language Models (LLMs) for multi-intent spoken language understanding (SLU), proposing a unique methodology that capitalizes on the generative power of LLMs within an SLU context. Our innovative technique reconfigures entity slots specifically for LLM application in multi-intent SLU environments and introduces the concept of Sub-Intent Instruction (SII), enhancing the dissection and interpretation of intricate, multi-intent communication within varied domains. The resultant datasets, dubbed LM-MixATIS and LM-MixSNIPS, are crafted from pre-existing benchmarks. Our research illustrates that LLMs can match and potentially excel beyond the capabilities of current state-of-the-art multi-intent SLU models. It further explores LLM efficacy across various intent configurations and dataset proportions. Moreover, we introduce two pioneering metrics, Entity Slot Accuracy (ESA) and Com
&lt;/p&gt;</description></item><item><title>Pearl&#25968;&#25454;&#38598;&#21033;&#29992;&#20102;&#35282;&#33394;&#21644;&#30693;&#35782;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#20855;&#20307;&#29992;&#25143;&#20559;&#22909;&#65292;&#39046;&#22495;&#19987;&#19994;&#24615;&#21644;&#26356;&#30456;&#20851;&#30340;&#25512;&#33616;&#12290;</title><link>https://arxiv.org/abs/2403.04460</link><description>&lt;p&gt;
Pearl: &#19968;&#39033;&#22522;&#20110;&#35780;&#35770;&#39537;&#21160;&#30340;&#35282;&#33394;&#30693;&#35782;&#23545;&#35805;&#24335;&#25512;&#33616;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Pearl: A Review-driven Persona-Knowledge Grounded Conversational Recommendation Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04460
&lt;/p&gt;
&lt;p&gt;
Pearl&#25968;&#25454;&#38598;&#21033;&#29992;&#20102;&#35282;&#33394;&#21644;&#30693;&#35782;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#20855;&#20307;&#29992;&#25143;&#20559;&#22909;&#65292;&#39046;&#22495;&#19987;&#19994;&#24615;&#21644;&#26356;&#30456;&#20851;&#30340;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04460v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#26159;&#19968;&#20010;&#26032;&#20852;&#39046;&#22495;&#65292;&#23588;&#20854;&#26159;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#27493;&#65292;&#20351;&#24471;&#23545;&#35805;&#36755;&#20837;&#30340;&#22810;&#26679;&#21270;&#25512;&#29702;&#24341;&#36215;&#20102;&#31038;&#21306;&#30340;&#36234;&#26469;&#36234;&#22823;&#30340;&#20852;&#36259;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#35813;&#39046;&#22495;&#36824;&#26377;&#35768;&#22810;&#26041;&#38754;&#26377;&#24453;&#25506;&#32034;&#12290;&#30446;&#21069;&#21487;&#29992;&#30340;&#29992;&#20110;&#23545;&#35805;&#24335;&#25512;&#33616;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#32570;&#20047;&#29305;&#23450;&#29992;&#25143;&#20559;&#22909;&#21644;&#23545;&#25512;&#33616;&#30340;&#35299;&#37322;&#65292;&#20174;&#32780;&#22952;&#30861;&#20102;&#39640;&#36136;&#37327;&#30340;&#25512;&#33616;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#35805;&#24335;&#25512;&#33616;&#25968;&#25454;&#38598;&#65292;&#21629;&#21517;&#20026;PEARL&#65292;&#19982;&#35282;&#33394;&#21644;&#30693;&#35782;&#22686;&#24378;&#30340;LLM&#27169;&#25311;&#22120;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#20174;&#30495;&#23454;&#35780;&#35770;&#20013;&#33719;&#24471;&#35814;&#32454;&#30340;&#35282;&#33394;&#21644;&#30693;&#35782;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#36229;&#36807;57k&#23545;&#35805;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PEARL&#20013;&#30340;&#35805;&#35821;&#21253;&#25324;&#26356;&#20855;&#20307;&#30340;&#29992;&#25143;&#20559;&#22909;&#65292;&#26174;&#31034;&#20102;&#22312;&#30446;&#26631;&#39046;&#22495;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#30456;&#20851;&#30340;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04460v1 Announce Type: new  Abstract: Conversational recommender system is an emerging area that has garnered an increasing interest in the community, especially with the advancements in large language models (LLMs) that enable diverse reasoning over conversational input. Despite the progress, the field has many aspects left to explore. The currently available public datasets for conversational recommendation lack specific user preferences and explanations for recommendations, hindering high-quality recommendations. To address such challenges, we present a novel conversational recommendation dataset named PEARL, synthesized with persona- and knowledge-augmented LLM simulators. We obtain detailed persona and knowledge from real-world reviews and construct a large-scale dataset with over 57k dialogues. Our experimental results demonstrate that utterances in PEARL include more specific user preferences, show expertise in the target domain, and provide recommendations more relev
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#29992;&#20110;&#26222;&#36890;&#27861;&#31995;&#32479;&#30340;&#36328;&#22810;&#21496;&#27861;&#31649;&#36758;&#21306;&#27861;&#38498;&#21028;&#20915;&#25991;&#26723;&#25688;&#35201;&#30340;&#31532;&#19968;&#20010;&#25968;&#25454;&#38598;CLSum&#65292;&#24182;&#39318;&#27425;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#21028;&#20915;&#25688;&#35201;&#24037;&#20316;&#12290;</title><link>https://arxiv.org/abs/2403.04454</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#26465;&#20214;&#19979;&#26222;&#36890;&#27861;&#31995;&#32479;&#27861;&#38498;&#21028;&#20915;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Low-Resource Court Judgment Summarization for Common Law Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04454
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#29992;&#20110;&#26222;&#36890;&#27861;&#31995;&#32479;&#30340;&#36328;&#22810;&#21496;&#27861;&#31649;&#36758;&#21306;&#27861;&#38498;&#21028;&#20915;&#25991;&#26723;&#25688;&#35201;&#30340;&#31532;&#19968;&#20010;&#25968;&#25454;&#38598;CLSum&#65292;&#24182;&#39318;&#27425;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#21028;&#20915;&#25688;&#35201;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26222;&#36890;&#27861;&#27861;&#38498;&#38656;&#35201;&#21442;&#32771;&#31867;&#20284;&#21028;&#20363;&#30340;&#21028;&#20915;&#26469;&#25351;&#23548;&#20854;&#24403;&#21069;&#30340;&#20915;&#23450;&#12290;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#27861;&#38498;&#21028;&#20915;&#25991;&#26723;&#25688;&#35201;&#21487;&#20197;&#24110;&#21161;&#27861;&#24459;&#20174;&#19994;&#32773;&#39640;&#25928;&#22320;&#23457;&#26597;&#20808;&#21069;&#30340;&#26696;&#20363;&#65292;&#24182;&#21327;&#21161;&#20844;&#20247;&#20102;&#35299;&#27861;&#38498;&#36816;&#20316;&#26041;&#24335;&#21450;&#27861;&#24459;&#22914;&#20309;&#36866;&#29992;&#12290;&#20808;&#21069;&#30340;&#27861;&#38498;&#21028;&#20915;&#25688;&#35201;&#30740;&#31350;&#30528;&#37325;&#20110;&#27665;&#27861;&#25110;&#29305;&#23450;&#21496;&#27861;&#31649;&#36758;&#21306;&#30340;&#21028;&#20915;&#12290;&#28982;&#32780;&#65292;&#27861;&#23448;&#21487;&#20197;&#21442;&#32771;&#25152;&#26377;&#26222;&#36890;&#27861;&#21496;&#27861;&#31649;&#36758;&#21306;&#30340;&#21028;&#20915;&#12290;&#30446;&#21069;&#30340;&#25688;&#35201;&#25968;&#25454;&#38598;&#19981;&#36275;&#20197;&#28385;&#36275;&#36328;&#22810;&#20010;&#21496;&#27861;&#31649;&#36758;&#21306;&#27010;&#25324;&#21028;&#20363;&#30340;&#35201;&#27714;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#35768;&#22810;&#21496;&#27861;&#31649;&#36758;&#21306;&#32570;&#20047;&#26631;&#35760;&#25968;&#25454;&#12290;&#20026;&#35299;&#20915;&#25968;&#25454;&#38598;&#19981;&#36275;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;CLSum&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#24635;&#32467;&#22810;&#21496;&#27861;&#31649;&#36758;&#21306;&#26222;&#36890;&#27861;&#27861;&#38498;&#21028;&#20915;&#25991;&#26723;&#30340;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#27861;&#38498;&#21028;&#20915;&#25688;&#35201;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04454v1 Announce Type: cross  Abstract: Common law courts need to refer to similar precedents' judgments to inform their current decisions. Generating high-quality summaries of court judgment documents can facilitate legal practitioners to efficiently review previous cases and assist the general public in accessing how the courts operate and how the law is applied. Previous court judgment summarization research focuses on civil law or a particular jurisdiction's judgments. However, judges can refer to the judgments from all common law jurisdictions. Current summarization datasets are insufficient to satisfy the demands of summarizing precedents across multiple jurisdictions, especially when labeled data are scarce for many jurisdictions. To address the lack of datasets, we present CLSum, the first dataset for summarizing multi-jurisdictional common law court judgment documents. Besides, this is the first court judgment summarization work adopting large language models (LLMs)
&lt;/p&gt;</description></item><item><title>&#20027;&#39064;&#24314;&#27169;&#20013;&#25552;&#20986;&#20102;&#20250;&#21592;&#25512;&#29702;&#25915;&#20987;&#65292;&#36890;&#36807;&#24046;&#20998;&#38544;&#31169;&#35789;&#27719;&#36873;&#25321;&#26469;&#25913;&#21892;&#38544;&#31169;&#39118;&#38505;</title><link>https://arxiv.org/abs/2403.04451</link><description>&lt;p&gt;
&#20250;&#21592;&#25512;&#29702;&#25915;&#20987;&#19982;&#20027;&#39064;&#24314;&#27169;&#20013;&#30340;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Membership Inference Attacks and Privacy in Topic Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04451
&lt;/p&gt;
&lt;p&gt;
&#20027;&#39064;&#24314;&#27169;&#20013;&#25552;&#20986;&#20102;&#20250;&#21592;&#25512;&#29702;&#25915;&#20987;&#65292;&#36890;&#36807;&#24046;&#20998;&#38544;&#31169;&#35789;&#27719;&#36873;&#25321;&#26469;&#25913;&#21892;&#38544;&#31169;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#25512;&#29702;&#35757;&#32451;&#25968;&#25454;&#26041;&#38754;&#30340;&#38544;&#31169;&#25915;&#20987;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#19981;&#28165;&#26970;&#26356;&#31616;&#21333;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20363;&#22914;&#20027;&#39064;&#27169;&#22411;&#65292;&#26159;&#21542;&#23384;&#22312;&#31867;&#20284;&#30340;&#28431;&#27934;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20027;&#39064;&#27169;&#22411;&#30340;&#25915;&#20987;&#65292;&#21487;&#20197;&#33258;&#20449;&#22320;&#35782;&#21035;Latent Dirichlet Allocation&#20013;&#35757;&#32451;&#25968;&#25454;&#30340;&#25104;&#21592;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22823;&#22411;&#31070;&#32463;&#27169;&#22411;&#30456;&#20851;&#32852;&#30340;&#38544;&#31169;&#39118;&#38505;&#24182;&#19981;&#20165;&#38480;&#20110;&#22823;&#22411;&#31070;&#32463;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20943;&#36731;&#36825;&#20123;&#28431;&#27934;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#20027;&#39064;&#24314;&#27169;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31169;&#23494;&#20027;&#39064;&#24314;&#27169;&#26694;&#26550;&#65292;&#23558;DP&#35789;&#27719;&#36873;&#25321;&#20316;&#20026;&#39044;&#22788;&#29702;&#27493;&#39588;&#65292;&#24182;&#23637;&#31034;&#23427;&#19981;&#20165;&#25913;&#21892;&#20102;&#38544;&#31169;&#24615;&#65292;&#32780;&#19988;&#22312;&#23454;&#29992;&#24615;&#26041;&#38754;&#30340;&#24433;&#21709;&#26377;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04451v1 Announce Type: cross  Abstract: Recent research shows that large language models are susceptible to privacy attacks that infer aspects of the training data. However, it is unclear if simpler generative models, like topic models, share similar vulnerabilities. In this work, we propose an attack against topic models that can confidently identify members of the training data in Latent Dirichlet Allocation. Our results suggest that the privacy risks associated with generative modeling are not restricted to large neural models. Additionally, to mitigate these vulnerabilities, we explore differentially private (DP) topic modeling. We propose a framework for private topic modeling that incorporates DP vocabulary selection as a pre-processing step, and show that it improves privacy while having limited effects on practical utility.
&lt;/p&gt;</description></item><item><title>NLP&#30340;&#24615;&#33021;&#23384;&#22312;&#38454;&#32423;&#20559;&#35265;&#65292;&#32463;&#39564;&#35777;&#23545;&#36739;&#19981;&#21457;&#36798;&#30340;&#31038;&#20250;&#32463;&#27982;&#32676;&#20307;&#19981;&#21033;&#12290;</title><link>https://arxiv.org/abs/2403.04445</link><description>&lt;p&gt;
&#38454;&#32423;&#24037;&#20855;&#65306;&#31038;&#20250;&#38454;&#32423;&#19982;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#34920;&#29616;&#30340;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
Classist Tools: Social Class Correlates with Performance in NLP
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04445
&lt;/p&gt;
&lt;p&gt;
NLP&#30340;&#24615;&#33021;&#23384;&#22312;&#38454;&#32423;&#20559;&#35265;&#65292;&#32463;&#39564;&#35777;&#23545;&#36739;&#19981;&#21457;&#36798;&#30340;&#31038;&#20250;&#32463;&#27982;&#32676;&#20307;&#19981;&#21033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;William Labov&#20851;&#20110;&#35821;&#35328;&#31038;&#20250;&#38454;&#23618;&#21270;&#30340;&#22522;&#30784;&#24037;&#20316;&#65288;Labov, 1964&#65289;&#20197;&#26469;&#65292;&#35821;&#35328;&#23398;&#24050;&#32463;&#20570;&#20986;&#20102;&#38598;&#20013;&#30340;&#21162;&#21147;&#65292;&#25506;&#32034;&#31038;&#20250;&#20154;&#21475;&#29305;&#24449;&#19982;&#35821;&#35328;&#20135;&#29983;&#21644;&#29702;&#35299;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#23613;&#31649;&#23384;&#22312;&#24378;&#28872;&#30340;&#35777;&#25454;&#34920;&#26126;&#35821;&#35328;&#20013;&#23384;&#22312;&#30528;&#31038;&#20250;&#20154;&#21475;&#29305;&#24449;&#65292;&#20294;&#23427;&#20204;&#24182;&#19981;&#32463;&#24120;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#20351;&#29992;&#12290;&#24180;&#40836;&#21644;&#24615;&#21035;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#26377;&#25152;&#20307;&#29616;&#65292;&#20294;Labov&#26368;&#21021;&#30340;&#30446;&#26631;&#8212;&#8212;&#31038;&#20250;&#32463;&#27982;&#22320;&#20301;&#21364;&#26126;&#26174;&#32570;&#22833;&#12290;&#28982;&#32780;&#65292;&#36825;&#26159;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#26174;&#31034;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20351;&#36739;&#19981;&#21457;&#36798;&#30340;&#31038;&#20250;&#32463;&#27982;&#32676;&#20307;&#22788;&#20110;&#19981;&#21033;&#22320;&#20301;&#12290;&#25105;&#20204;&#20351;&#29992;&#31038;&#20250;&#38454;&#32423;&#12289;&#31181;&#26063;&#21644;&#22320;&#29702;&#35821;&#35328;&#24046;&#24322;&#26469;&#23545;&#30005;&#24433;&#20013;&#30340;95,000&#20010;&#35805;&#35821;&#36827;&#34892;&#26631;&#27880;&#65292;&#35780;&#20272;&#20102;NLP&#31995;&#32479;&#22312;&#35821;&#35328;&#24314;&#27169;&#12289;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#21644;&#35821;&#27861;&#38169;&#35823;&#32416;&#27491;&#19977;&#20010;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#21457;&#29616;&#21487;&#20197;&#24402;&#22240;&#20110;&#31038;&#20250;&#32463;&#27982;&#22240;&#32032;&#30340;&#26174;&#33879;&#34920;&#29616;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04445v1 Announce Type: new  Abstract: Since the foundational work of William Labov on the social stratification of language (Labov, 1964), linguistics has made concentrated efforts to explore the links between sociodemographic characteristics and language production and perception. But while there is strong evidence for socio-demographic characteristics in language, they are infrequently used in Natural Language Processing (NLP). Age and gender are somewhat well represented, but Labov's original target, socioeconomic status, is noticeably absent. And yet it matters. We show empirically that NLP disadvantages less-privileged socioeconomic groups. We annotate a corpus of 95K utterances from movies with social class, ethnicity and geographical language variety and measure the performance of NLP systems on three tasks: language modelling, automatic speech recognition, and grammar error correction. We find significant performance disparities that can be attributed to socioeconomi
&lt;/p&gt;</description></item><item><title>&#24378;&#35843;&#19968;&#20123;&#36866;&#29992;&#20110;&#19981;&#21516;&#24314;&#27169;&#24212;&#29992;&#39046;&#22495;&#20013;&#38750;&#32447;&#24615;&#21160;&#24577;&#27169;&#22411;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#23545;&#20110;&#31038;&#20250;&#21644;&#20581;&#24247;&#35745;&#31639;&#31185;&#23398;&#39046;&#22495;&#30340;ABMs&#20855;&#26377;&#28508;&#22312;&#30340;&#25512;&#21160;&#20316;&#29992;</title><link>https://arxiv.org/abs/2403.04417</link><description>&lt;p&gt;
&#25552;&#21319;&#31038;&#20250;&#21644;&#20581;&#24247;&#35745;&#31639;&#31185;&#23398;&#20013;&#22522;&#20110;&#20195;&#29702;&#30340;&#27169;&#22411;&#30340;&#26367;&#20195;&#26041;&#27861;&#30340;&#26410;&#26469;&#26041;&#21521;&#26377;&#28508;&#21147;&#24182;&#20540;&#24471;&#23581;&#35797;
&lt;/p&gt;
&lt;p&gt;
Promising and worth-to-try future directions for advancing state-of-the-art surrogates methods of agent-based models in social and health computational sciences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04417
&lt;/p&gt;
&lt;p&gt;
&#24378;&#35843;&#19968;&#20123;&#36866;&#29992;&#20110;&#19981;&#21516;&#24314;&#27169;&#24212;&#29992;&#39046;&#22495;&#20013;&#38750;&#32447;&#24615;&#21160;&#24577;&#27169;&#22411;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#23545;&#20110;&#31038;&#20250;&#21644;&#20581;&#24247;&#35745;&#31639;&#31185;&#23398;&#39046;&#22495;&#30340;ABMs&#20855;&#26377;&#28508;&#22312;&#30340;&#25512;&#21160;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#20998;&#26512;&#24037;&#20855;&#30340;&#25191;&#34892;&#21644;&#36816;&#34892;&#24615;&#33021;&#23545;&#20110;&#30495;&#23454;&#22823;&#35268;&#27169;ABMs&#65288;&#22522;&#20110;&#20195;&#29702;&#30340;&#27169;&#22411;&#65289;&#21487;&#33021;&#20250;&#36807;&#38271;&#12290;&#36825;&#26159;&#30001;&#20110;&#35745;&#31639;&#38656;&#27714;&#19982;&#27169;&#22411;&#35268;&#27169;&#65288;&#20363;&#22914;&#20154;&#21475;&#35268;&#27169;&#65289;&#21644;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#25104;&#25351;&#25968;&#27604;&#20363;&#12290;&#21363;&#20351;&#26159;&#23545;&#20110;&#19968;&#20010;&#30495;&#23454;ABM&#30340;&#21333;&#27425;&#27169;&#25311;&#36816;&#34892;&#65292;&#24403;&#23581;&#35797;&#20351;&#29992;&#30495;&#23454;&#20154;&#21475;&#35268;&#27169;&#26102;&#65292;&#20063;&#21487;&#33021;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#36825;&#31687;&#31616;&#30701;&#25253;&#21578;&#30340;&#20027;&#35201;&#30446;&#30340;&#26159;&#24378;&#35843;&#19968;&#20123;&#36866;&#29992;&#20110;&#21508;&#31181;&#24314;&#27169;&#24212;&#29992;&#39046;&#22495;&#20013;&#30340;&#38750;&#32447;&#24615;&#21160;&#24577;&#27169;&#22411;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#23545;&#35745;&#31639;&#35201;&#27714;&#36739;&#23567;&#12290;&#25454;&#20316;&#32773;&#25152;&#30693;&#65292;&#36825;&#20123;&#26041;&#27861;&#33267;&#23569;&#22312;&#31038;&#20250;&#21644;&#20581;&#24247;&#35745;&#31639;&#31185;&#23398;&#39046;&#22495;&#30340;ABMs&#20013;&#23578;&#26410;&#34987;&#24191;&#27867;&#37319;&#29992;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#21487;&#33021;&#26377;&#21161;&#20110;&#25512;&#21160;&#24314;&#31435;SH&#39046;&#22495;ABMs&#30340;&#26367;&#20195;&#27169;&#22411;&#30340;&#21069;&#27839;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04417v1 Announce Type: cross  Abstract: The execution and runtime performance of model-based analysis tools for realistic large-scale ABMs (Agent-Based Models) can be excessively long. This due to the computational demand exponentially proportional to the model size (e.g. Population size) and the number of model parameters. Even the runtime of a single simulation of a realistic ABM may demand huge computational resources when attempting to employ realistic population size. The main aim of this ad-hoc brief report is to highlight some of surrogate models that were adequate and computationally less demanding for nonlinear dynamical models in various modeling application areas.To the author knowledge, these methods have been not, at least extensively, employed for ABMs within the field of (SHCS) Social Health Computational Sciences, yet. Thus, they might be, but not necessarily, useful in progressing state of the art for establishing surrogate models for ABMs in the field of SH
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#36830;&#32493;&#32452;&#21512;&#27867;&#21270;&#25361;&#25112;&#65292;&#25506;&#35752;&#20102;&#27169;&#22411;&#22914;&#20309;&#25345;&#32493;&#33719;&#21462;&#21407;&#22987;&#25512;&#29702;&#20219;&#21153;&#30340;&#30693;&#35782;&#24182;&#36827;&#34892;&#32452;&#21512;&#25512;&#29702;&#65292;&#30740;&#31350;&#20102;&#19981;&#26029;&#23398;&#20064;&#23545;NLI&#20013;&#32452;&#21512;&#27867;&#21270;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.04400</link><description>&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20013;&#25506;&#32034;&#32452;&#21512;&#27867;&#21270;&#30340;&#19981;&#38388;&#26029;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Exploring Continual Learning of Compositional Generalization in NLI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04400
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#36830;&#32493;&#32452;&#21512;&#27867;&#21270;&#25361;&#25112;&#65292;&#25506;&#35752;&#20102;&#27169;&#22411;&#22914;&#20309;&#25345;&#32493;&#33719;&#21462;&#21407;&#22987;&#25512;&#29702;&#20219;&#21153;&#30340;&#30693;&#35782;&#24182;&#36827;&#34892;&#32452;&#21512;&#25512;&#29702;&#65292;&#30740;&#31350;&#20102;&#19981;&#26029;&#23398;&#20064;&#23545;NLI&#20013;&#32452;&#21512;&#27867;&#21270;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#21512;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#24050;&#34987;&#29992;&#26469;&#35780;&#20272;&#31070;&#32463;&#27169;&#22411;&#25191;&#34892;NLI&#30340;&#30495;&#23454;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#35780;&#20272;&#20551;&#35774;&#27169;&#22411;&#20107;&#20808;&#23436;&#20840;&#35775;&#38382;&#25152;&#26377;&#21407;&#22987;&#25512;&#29702;&#65292;&#19982;&#20154;&#31867;&#19981;&#26029;&#33719;&#24471;&#25512;&#29702;&#30693;&#35782;&#30340;&#26041;&#24335;&#30456;&#21453;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#25512;&#29702;&#20013;&#30340;&#36830;&#32493;&#32452;&#21512;&#27867;&#21270;&#65288;C2Gen NLI&#65289;&#25361;&#25112;&#65292;&#20854;&#20013;&#27169;&#22411;&#25345;&#32493;&#33719;&#21462;&#26500;&#25104;&#21407;&#22987;&#25512;&#29702;&#20219;&#21153;&#30340;&#30693;&#35782;&#20316;&#20026;&#32452;&#21512;&#25512;&#29702;&#30340;&#22522;&#30784;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#26029;&#23398;&#20064;&#22914;&#20309;&#24433;&#21709;NLI&#20013;&#30340;&#32452;&#21512;&#27867;&#21270;&#65292;&#36890;&#36807;&#20026;&#32452;&#21512;NLI&#25512;&#29702;&#20219;&#21153;&#35774;&#35745;&#20102;&#19968;&#20010;&#19981;&#26029;&#23398;&#20064;&#35774;&#32622;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#27169;&#22411;&#22312;&#19981;&#26029;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#26080;&#27861;&#32452;&#21512;&#27867;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#21508;&#31181;&#19981;&#26029;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#24182;&#39564;&#35777;&#23427;&#20204;&#30340;&#21151;&#25928;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;C2Gen&#65292;&#37325;&#28857;&#20851;&#27880;&#22914;&#20309;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04400v1 Announce Type: new  Abstract: Compositional Natural Language Inference has been explored to assess the true abilities of neural models to perform NLI. Yet, current evaluations assume models to have full access to all primitive inferences in advance, in contrast to humans that continuously acquire inference knowledge. In this paper, we introduce the Continual Compositional Generalization in Inference (C2Gen NLI) challenge, where a model continuously acquires knowledge of constituting primitive inference tasks as a basis for compositional inferences. We explore how continual learning affects compositional generalization in NLI, by designing a continual learning setup for compositional NLI inference tasks. Our experiments demonstrate that models fail to compositionally generalize in a continual scenario. To address this problem, we first benchmark various continual learning algorithms and verify their efficacy. We then further analyze C2Gen, focusing on how to order pri
&lt;/p&gt;</description></item><item><title>SGNet&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SGNet&#30340;&#34507;&#30333;&#36136;&#25240;&#21472;&#26694;&#26550;&#65292;&#29992;&#20110;&#27169;&#25311;&#23545;&#31216;&#32452;&#20214;&#20013;&#30340;&#34507;&#30333;&#36136;-&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#65292;&#35299;&#20915;&#20102;&#23545;&#31216;&#34507;&#30333;&#36136;&#32452;&#35013;&#20013;&#30340;&#32467;&#26500;&#30830;&#23450;&#21644;&#30417;&#30563;&#27169;&#31946;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.04395</link><description>&lt;p&gt;
SGNet&#65306;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25240;&#21472;&#23545;&#31216;&#34507;&#30333;&#36136;&#22797;&#21512;&#29289;
&lt;/p&gt;
&lt;p&gt;
SGNet: Folding Symmetrical Protein Complex with Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04395
&lt;/p&gt;
&lt;p&gt;
SGNet&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SGNet&#30340;&#34507;&#30333;&#36136;&#25240;&#21472;&#26694;&#26550;&#65292;&#29992;&#20110;&#27169;&#25311;&#23545;&#31216;&#32452;&#20214;&#20013;&#30340;&#34507;&#30333;&#36136;-&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#65292;&#35299;&#20915;&#20102;&#23545;&#31216;&#34507;&#30333;&#36136;&#32452;&#35013;&#20013;&#30340;&#32467;&#26500;&#30830;&#23450;&#21644;&#30417;&#30563;&#27169;&#31946;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04395v1 &#22768;&#26126;&#31867;&#22411;&#65306;&#36328;  &#25688;&#35201;&#65306;&#28145;&#24230;&#23398;&#20064;&#22312;&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#25512;&#21160;&#20102;&#35745;&#31639;&#29983;&#29289;&#23398;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22312;&#39044;&#27979;&#21333;&#38142;&#32467;&#26500;&#26041;&#38754;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#22823;&#37327;&#30340;&#22823;&#22411;&#21516;&#28304;&#23521;&#32858;&#20307;&#32452;&#35013;&#20307;&#21576;&#29616;&#20986;&#20869;&#37096;&#23545;&#31216;&#24615;&#65292;&#22312;&#32467;&#26500;&#30830;&#23450;&#26041;&#38754;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#29616;&#26377;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#21463;&#21040;&#38480;&#21046;&#65292;&#22240;&#20026;&#23545;&#31216;&#34507;&#30333;&#36136;&#32452;&#35013;&#36890;&#24120;&#20855;&#26377;&#36739;&#38271;&#30340;&#24207;&#21015;&#65292;&#20351;&#32467;&#26500;&#35745;&#31639;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#27492;&#22806;&#65292;&#23545;&#31216;&#34507;&#30333;&#36136;&#22797;&#21512;&#29289;&#20013;&#30340;&#22810;&#20010;&#30456;&#21516;&#20122;&#22522;&#23548;&#33268;&#26631;&#31614;&#20998;&#37197;&#20013;&#23384;&#22312;&#30417;&#30563;&#27169;&#31946;&#24615;&#38382;&#39064;&#65292;&#35201;&#27714;&#22312;&#35757;&#32451;&#20013;&#36827;&#34892;&#19968;&#33268;&#30340;&#32467;&#26500;&#24314;&#27169;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SGNet&#30340;&#34507;&#30333;&#36136;&#25240;&#21472;&#26694;&#26550;&#65292;&#29992;&#20110;&#27169;&#25311;&#23545;&#31216;&#32452;&#20214;&#20013;&#30340;&#34507;&#30333;&#36136;-&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#12290;SGNet&#22312;&#21333;&#20010;&#20122;&#22522;&#19978;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#65292;&#24182;&#29983;&#25104;&#25972;&#20010;&#32452;&#35013;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04395v1 Announce Type: cross  Abstract: Deep learning has made significant progress in protein structure prediction, advancing the development of computational biology. However, despite the high accuracy achieved in predicting single-chain structures, a significant number of large homo-oligomeric assemblies exhibit internal symmetry, posing a major challenge in structure determination. The performances of existing deep learning methods are limited since the symmetrical protein assembly usually has a long sequence, making structural computation infeasible. In addition, multiple identical subunits in symmetrical protein complex cause the issue of supervision ambiguity in label assignment, requiring a consistent structure modeling for the training. To tackle these problems, we propose a protein folding framework called SGNet to model protein-protein interactions in symmetrical assemblies. SGNet conducts feature extraction on a single subunit and generates the whole assembly usi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Acceleron&#8221;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#21152;&#36895;&#30740;&#31350;&#26500;&#24819;&#65292;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21046;&#23450;&#20840;&#38754;&#30340;&#30740;&#31350;&#25552;&#26696;&#65292;&#39564;&#35777;&#20854;&#21019;&#26032;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.04382</link><description>&lt;p&gt;
Acceleron&#65306;&#21152;&#36895;&#30740;&#31350;&#26500;&#24819;&#30340;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Acceleron: A Tool to Accelerate Research Ideation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04382
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Acceleron&#8221;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#21152;&#36895;&#30740;&#31350;&#26500;&#24819;&#65292;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21046;&#23450;&#20840;&#38754;&#30340;&#30740;&#31350;&#25552;&#26696;&#65292;&#39564;&#35777;&#20854;&#21019;&#26032;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24050;&#25552;&#20986;&#20102;&#20960;&#31181;&#24037;&#20855;&#65292;&#29992;&#20110;&#21327;&#21161;&#30740;&#31350;&#20154;&#21592;&#22312;&#30740;&#31350;&#29983;&#21629;&#21608;&#26399;&#30340;&#21508;&#20010;&#38454;&#27573;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20855;&#20027;&#35201;&#38598;&#20013;&#22312;&#35832;&#22914;&#26816;&#32034;&#21644;&#25512;&#33616;&#30456;&#20851;&#25991;&#29486;&#12289;&#23457;&#26597;&#21644;&#35780;&#35770;&#33609;&#31295;&#12289;&#20197;&#21450;&#25776;&#20889;&#30740;&#31350;&#25163;&#31295;&#31561;&#20219;&#21153;&#19978;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#25581;&#31034;&#20102;&#22312;&#30740;&#31350;&#29983;&#21629;&#21608;&#26399;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26500;&#24819;&#38454;&#27573;&#20013;&#32570;&#20047;&#19987;&#38376;&#35774;&#35745;&#30340;&#24037;&#20855;&#12290;&#20026;&#20102;&#24110;&#21161;&#30740;&#31350;&#26500;&#24819;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;Acceleron&#8221;&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#30740;&#31350;&#29983;&#21629;&#21608;&#26399;&#19981;&#21516;&#38454;&#27573;&#30340;&#30740;&#31350;&#21152;&#36895;&#22120;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#36741;&#21161;&#26500;&#24819;&#36807;&#31243;&#12290;Acceleron&#25351;&#23548;&#30740;&#31350;&#20154;&#21592;&#21046;&#23450;&#19968;&#20010;&#20840;&#38754;&#30340;&#30740;&#31350;&#25552;&#26696;&#65292;&#21253;&#25324;&#19968;&#20010;&#26032;&#39062;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#36890;&#36807;&#35782;&#21035;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#31354;&#30333;&#24182;&#24314;&#35758;&#19968;&#31995;&#21015;&#21487;&#34892;&#30340;&#35299;&#20915;&#25216;&#26415;&#65292;&#39564;&#35777;&#25552;&#26696;&#30340;&#21019;&#26032;&#21160;&#26426;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04382v1 Announce Type: cross  Abstract: Several tools have recently been proposed for assisting researchers during various stages of the research life-cycle. However, these primarily concentrate on tasks such as retrieving and recommending relevant literature, reviewing and critiquing the draft, and writing of research manuscripts. Our investigation reveals a significant gap in availability of tools specifically designed to assist researchers during the challenging ideation phase of the research life-cycle. To aid with research ideation, we propose `Acceleron', a research accelerator for different phases of the research life cycle, and which is specially designed to aid the ideation process. Acceleron guides researchers through the formulation of a comprehensive research proposal, encompassing a novel research problem. The proposals motivation is validated for novelty by identifying gaps in the existing literature and suggesting a plausible list of techniques to solve the pr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#27721;&#35821;&#21517;&#35789;&#30701;&#35821;&#20013;&#25968;&#37327;&#19982;&#30830;&#23450;&#24615;&#26631;&#35760;&#30340;&#30465;&#30053;&#29616;&#35937;&#65292;&#21457;&#29616;&#27721;&#35821;&#35762;&#35805;&#32773;&#30830;&#23454;&#32463;&#24120;&#30465;&#30053;&#36825;&#20123;&#26631;&#35760;&#65292;&#36825;&#19968;&#29616;&#35937;&#22312;&#30456;&#20851;&#35821;&#22659;&#20013;&#30340;&#21487;&#39044;&#27979;&#24615;&#26377;&#25152;&#25552;&#39640;&#12290;</title><link>https://arxiv.org/abs/2403.04376</link><description>&lt;p&gt;
&#12298;&#27721;&#35821;&#21517;&#35789;&#30701;&#35821;&#20013;&#30340;&#25968;&#37327;&#19982;&#30830;&#23450;&#24615;&#30340;&#35745;&#31639;&#24314;&#27169;&#12299;
&lt;/p&gt;
&lt;p&gt;
Computational Modelling of Plurality and Definiteness in Chinese Noun Phrases
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04376
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#27721;&#35821;&#21517;&#35789;&#30701;&#35821;&#20013;&#25968;&#37327;&#19982;&#30830;&#23450;&#24615;&#26631;&#35760;&#30340;&#30465;&#30053;&#29616;&#35937;&#65292;&#21457;&#29616;&#27721;&#35821;&#35762;&#35805;&#32773;&#30830;&#23454;&#32463;&#24120;&#30465;&#30053;&#36825;&#20123;&#26631;&#35760;&#65292;&#36825;&#19968;&#29616;&#35937;&#22312;&#30456;&#20851;&#35821;&#22659;&#20013;&#30340;&#21487;&#39044;&#27979;&#24615;&#26377;&#25152;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35770;&#35821;&#35328;&#23398;&#23478;&#25351;&#20986;&#65292;&#19968;&#20123;&#35821;&#35328;&#65288;&#22914;&#27721;&#35821;&#21644;&#26085;&#35821;&#65289;"&#26356;&#20919;"&#30340;&#35266;&#23519;&#22522;&#20110;&#36825;&#26679;&#30340;&#20107;&#23454;&#65292;&#21363;&#36825;&#20123;&#35821;&#35328;&#20013;&#30701;&#35821;&#30340;&#39044;&#26399;&#21547;&#20041;&#26356;&#22810;&#22320;&#20381;&#36182;&#20110;&#19978;&#19979;&#25991;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#35821;&#35328;&#20013;&#35768;&#22810;&#34920;&#36798;&#26041;&#24335;&#34987;&#32553;&#30701;&#65292;&#20854;&#21547;&#20041;&#26159;&#20174;&#19978;&#19979;&#25991;&#20013;&#25512;&#26029;&#20986;&#26469;&#30340;&#12290;&#26412;&#25991;&#30528;&#37325;&#25506;&#35752;&#22312;&#27721;&#35821;&#21517;&#35789;&#30701;&#35821;&#20013;&#30465;&#30053;&#25968;&#37327;&#21644;&#30830;&#23450;&#24615;&#26631;&#35760;&#65292;&#20197;&#30740;&#31350;&#22312;&#32473;&#23450;&#35821;&#22659;&#24773;&#20917;&#19979;&#20854;&#39044;&#26399;&#21547;&#20041;&#30340;&#21487;&#39044;&#27979;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#27721;&#35821;&#21517;&#35789;&#30701;&#35821;&#35821;&#26009;&#24211;&#65292;&#27599;&#20010;&#30701;&#35821;&#37117;&#38468;&#24102;&#20854;&#23545;&#24212;&#30340;&#35821;&#22659;&#20197;&#21450;&#26631;&#31614;&#65292;&#34920;&#31034;&#20854;&#21333;&#25968;/&#22797;&#25968;&#21644;&#30830;&#23450;&#24615;/&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#35821;&#26009;&#24211;&#35780;&#20272;&#21644;&#20998;&#26512;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#27721;&#35821;&#35762;&#35805;&#32773;&#30830;&#23454;&#32463;&#24120;&#30465;&#30053;&#25968;&#37327;&#21644;&#30830;&#23450;&#24615;&#26631;&#35760;&#12290;&#22522;&#20110;&#27492;&#35821;&#26009;&#24211;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#32452;&#35745;&#31639;&#27169;&#22411;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04376v1 Announce Type: new  Abstract: Theoretical linguists have suggested that some languages (e.g., Chinese and Japanese) are "cooler" than other languages based on the observation that the intended meaning of phrases in these languages depends more on their contexts. As a result, many expressions in these languages are shortened, and their meaning is inferred from the context. In this paper, we focus on the omission of the plurality and definiteness markers in Chinese noun phrases (NPs) to investigate the predictability of their intended meaning given the contexts. To this end, we built a corpus of Chinese NPs, each of which is accompanied by its corresponding context, and by labels indicating its singularity/plurality and definiteness/indefiniteness. We carried out corpus assessments and analyses. The results suggest that Chinese speakers indeed drop plurality and definiteness markers very frequently. Building on the corpus, we train a bank of computational models using 
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#39046;&#22495;&#30693;&#35782;&#30340;&#20174;&#22270;&#21040;&#35789;&#34955;&#26041;&#27861;&#65292;&#24110;&#21161;&#39044;&#27979;&#28151;&#28102;&#32618;&#21517;&#65292;&#36890;&#36807;&#26500;&#25104;&#35201;&#32032;&#21644;&#20851;&#38190;&#35789;&#36873;&#25321;&#36827;&#34892;&#21028;&#26029;&#12290;</title><link>https://arxiv.org/abs/2403.04369</link><description>&lt;p&gt;
&#20174;&#22270;&#21040;&#35789;&#34955;: &#23558;&#39046;&#22495;&#30693;&#35782;&#24341;&#20837;&#28151;&#28102;&#32618;&#21517;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
From Graph to Word Bag: Introducing Domain Knowledge to Confusing Charge Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04369
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#39046;&#22495;&#30693;&#35782;&#30340;&#20174;&#22270;&#21040;&#35789;&#34955;&#26041;&#27861;&#65292;&#24110;&#21161;&#39044;&#27979;&#28151;&#28102;&#32618;&#21517;&#65292;&#36890;&#36807;&#26500;&#25104;&#35201;&#32032;&#21644;&#20851;&#38190;&#35789;&#36873;&#25321;&#36827;&#34892;&#21028;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#28102;&#32618;&#21517;&#39044;&#27979;&#26159;&#27861;&#24459;&#20154;&#24037;&#26234;&#33021;&#20013;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#28041;&#21450;&#26681;&#25454;&#20107;&#23454;&#25551;&#36848;&#39044;&#27979;&#28151;&#28102;&#32618;&#21517;&#12290;&#29616;&#26377;&#30340;&#32618;&#21517;&#39044;&#27979;&#26041;&#27861;&#22312;&#34920;&#29616;&#19978;&#24050;&#32463;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25928;&#26524;&#65292;&#20294;&#22312;&#22788;&#29702;&#28151;&#28102;&#32618;&#21517;&#65288;&#22914;&#25250;&#22842;&#19982;&#25250;&#21163;&#65289;&#26102;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#27861;&#24459;&#39046;&#22495;&#65292;&#26500;&#25104;&#35201;&#32032;&#22312;&#21306;&#20998;&#28151;&#28102;&#32618;&#21517;&#20013;&#25198;&#28436;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#26500;&#25104;&#35201;&#32032;&#26159;&#28508;&#22312;&#21009;&#32602;&#32972;&#21518;&#30340;&#22522;&#26412;&#34892;&#20026;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#32618;&#21517;&#20043;&#38388;&#26377;&#24494;&#22937;&#30340;&#21306;&#21035;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20174;&#22270;&#21040;&#35789;&#34955;&#65288;FWGB&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#26377;&#20851;&#26500;&#25104;&#35201;&#32032;&#30340;&#39046;&#22495;&#30693;&#35782;&#65292;&#20197;&#25351;&#23548;&#27169;&#22411;&#22312;&#28151;&#28102;&#32618;&#21517;&#19978;&#20570;&#20986;&#21028;&#26029;&#65292;&#31867;&#20284;&#20110;&#27861;&#23448;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#26500;&#25104;&#35201;&#32032;&#30340;&#27861;&#24459;&#30693;&#35782;&#22270;&#65292;&#20197;&#24110;&#21161;&#20026;&#27599;&#31181;&#32618;&#21517;&#36873;&#25321;&#20851;&#38190;&#35789;&#65292;&#24418;&#25104;&#19968;&#20010;&#21333;&#35789;&#34955;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04369v1 Announce Type: new  Abstract: Confusing charge prediction is a challenging task in legal AI, which involves predicting confusing charges based on fact descriptions. While existing charge prediction methods have shown impressive performance, they face significant challenges when dealing with confusing charges, such as Snatch and Robbery. In the legal domain, constituent elements play a pivotal role in distinguishing confusing charges. Constituent elements are fundamental behaviors underlying criminal punishment and have subtle distinctions among charges. In this paper, we introduce a novel From Graph to Word Bag (FWGB) approach, which introduces domain knowledge regarding constituent elements to guide the model in making judgments on confusing charges, much like a judge's reasoning process. Specifically, we first construct a legal knowledge graph containing constituent elements to help select keywords for each charge, forming a word bag. Subsequently, to guide the mod
&lt;/p&gt;</description></item><item><title>ProMoAI&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#36807;&#31243;&#27169;&#22411;&#65292;&#25903;&#25345;&#20248;&#21270;&#24182;&#36890;&#36807;&#29992;&#25143;&#21453;&#39304;&#36827;&#34892;&#25913;&#36827;&#65292;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;AI&#39537;&#21160;&#30340;&#36807;&#31243;&#24314;&#27169;&#24037;&#20855;&#65292;&#38477;&#20302;&#20102;&#29992;&#25143;&#30340;&#25216;&#26415;&#38376;&#27099;&#12290;</title><link>https://arxiv.org/abs/2403.04327</link><description>&lt;p&gt;
ProMoAI&#65306;&#20351;&#29992;&#29983;&#25104;&#24335;AI&#36827;&#34892;&#36807;&#31243;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
ProMoAI: Process Modeling with Generative AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04327
&lt;/p&gt;
&lt;p&gt;
ProMoAI&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#36807;&#31243;&#27169;&#22411;&#65292;&#25903;&#25345;&#20248;&#21270;&#24182;&#36890;&#36807;&#29992;&#25143;&#21453;&#39304;&#36827;&#34892;&#25913;&#36827;&#65292;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;AI&#39537;&#21160;&#30340;&#36807;&#31243;&#24314;&#27169;&#24037;&#20855;&#65292;&#38477;&#20302;&#20102;&#29992;&#25143;&#30340;&#25216;&#26415;&#38376;&#27099;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ProMoAI&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#24037;&#20855;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#33258;&#21160;&#29983;&#25104;&#36807;&#31243;&#27169;&#22411;&#65292;&#34701;&#21512;&#20102;&#20808;&#36827;&#30340;&#25552;&#31034;&#24037;&#31243;&#12289;&#38169;&#35823;&#22788;&#29702;&#21644;&#20195;&#30721;&#29983;&#25104;&#25216;&#26415;&#12290;&#38500;&#20102;&#33258;&#21160;&#21270;&#29983;&#25104;&#22797;&#26434;&#30340;&#36807;&#31243;&#27169;&#22411;&#22806;&#65292;ProMoAI&#36824;&#25903;&#25345;&#36807;&#31243;&#27169;&#22411;&#20248;&#21270;&#12290;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#23545;&#29983;&#25104;&#27169;&#22411;&#30340;&#21453;&#39304;&#19982;&#24037;&#20855;&#36827;&#34892;&#20132;&#20114;&#65292;&#28982;&#21518;&#29992;&#20110;&#25913;&#36827;&#36807;&#31243;&#27169;&#22411;&#12290;ProMoAI&#21033;&#29992;LLMs&#30340;&#33021;&#21147;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#20197;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#36807;&#31243;&#24314;&#27169;&#26041;&#27861;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#23545;&#27809;&#26377;&#28145;&#20837;&#25216;&#26415;&#30693;&#35782;&#30340;&#29992;&#25143;&#30340;&#20934;&#20837;&#38376;&#27099;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04327v1 Announce Type: cross  Abstract: ProMoAI is a novel tool that leverages Large Language Models (LLMs) to automatically generate process models from textual descriptions, incorporating advanced prompt engineering, error handling, and code generation techniques. Beyond automating the generation of complex process models, ProMoAI also supports process model optimization. Users can interact with the tool by providing feedback on the generated model, which is then used for refining the process model. ProMoAI utilizes the capabilities LLMs to offer a novel, AI-driven approach to process modeling, significantly reducing the barrier to entry for users without deep technical knowledge in process modeling.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;Composition Score&#65292;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#29992;&#20110;&#37327;&#21270;&#21477;&#23376;&#29702;&#35299;&#20013;&#30340;&#21547;&#20041;&#21512;&#25104;&#31243;&#24230;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#19968;&#24230;&#37327;&#19982;&#22823;&#33041;&#21306;&#22495;&#30456;&#20851;&#65292;&#25581;&#31034;&#20102;&#21547;&#20041;&#21512;&#25104;&#22312;&#20154;&#31867;&#21477;&#23376;&#29702;&#35299;&#20013;&#30340;&#22810;&#26041;&#38754;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.04325</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32452;&#21512;&#20998;&#25968;&#27979;&#37327;&#20154;&#33041;&#20013;&#30340;&#21547;&#20041;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Measuring Meaning Composition in the Human Brain with Composition Scores from Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04325
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;Composition Score&#65292;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#29992;&#20110;&#37327;&#21270;&#21477;&#23376;&#29702;&#35299;&#20013;&#30340;&#21547;&#20041;&#21512;&#25104;&#31243;&#24230;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#19968;&#24230;&#37327;&#19982;&#22823;&#33041;&#21306;&#22495;&#30456;&#20851;&#65292;&#25581;&#31034;&#20102;&#21547;&#20041;&#21512;&#25104;&#22312;&#20154;&#31867;&#21477;&#23376;&#29702;&#35299;&#20013;&#30340;&#22810;&#26041;&#38754;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21547;&#20041;&#21512;&#25104;&#30340;&#36807;&#31243;&#26159;&#25351;&#26356;&#23567;&#30340;&#21333;&#20301;&#22914;&#35821;&#32032;&#25110;&#21333;&#35789;&#32452;&#21512;&#24418;&#25104;&#30701;&#35821;&#21644;&#21477;&#23376;&#30340;&#21547;&#20041;&#65292;&#23545;&#20110;&#20154;&#31867;&#21477;&#23376;&#29702;&#35299;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#31070;&#32463;&#35821;&#35328;&#23398;&#23545;&#28041;&#21450;&#21547;&#20041;&#21512;&#25104;&#30340;&#22823;&#33041;&#21306;&#22495;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#65292;&#20294;&#20173;&#32570;&#20047;&#19968;&#31181;&#35745;&#31639;&#24230;&#37327;&#26469;&#37327;&#21270;&#21512;&#25104;&#30340;&#31243;&#24230;&#12290;&#20511;&#37492;&#21464;&#21387;&#22120;&#21069;&#39304;&#32593;&#32476;&#22359;&#30340;&#38190;&#20540;&#20869;&#23384;&#35299;&#37322;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32452;&#21512;&#20998;&#25968;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#26088;&#22312;&#37327;&#21270;&#21477;&#23376;&#29702;&#35299;&#36807;&#31243;&#20013;&#30340;&#21547;&#20041;&#21512;&#25104;&#31243;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#19968;&#24230;&#37327;&#19982;&#22823;&#33041;&#31751;&#30456;&#20851;&#32852;&#65292;&#36825;&#20123;&#22823;&#33041;&#31751;&#19982;&#35789;&#39057;&#29575;&#12289;&#32467;&#26500;&#22788;&#29702;&#21644;&#23545;&#21333;&#35789;&#30340;&#19968;&#33324;&#25935;&#24863;&#24615;&#26377;&#20851;&#65292;&#36825;&#34920;&#26126;&#20102;&#20154;&#31867;&#21477;&#23376;&#29702;&#35299;&#36807;&#31243;&#20013;&#21547;&#20041;&#21512;&#25104;&#30340;&#22810;&#26041;&#38754;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04325v1 Announce Type: cross  Abstract: The process of meaning composition, wherein smaller units like morphemes or words combine to form the meaning of phrases and sentences, is essential for human sentence comprehension. Despite extensive neurolinguistic research into the brain regions involved in meaning composition, a computational metric to quantify the extent of composition is still lacking. Drawing on the key-value memory interpretation of transformer feed-forward network blocks, we introduce the Composition Score, a novel model-based metric designed to quantify the degree of meaning composition during sentence comprehension. Experimental findings show that this metric correlates with brain clusters associated with word frequency, structural processing, and general sensitivity to words, suggesting the multifaceted nature of meaning composition during human sentence comprehension.
&lt;/p&gt;</description></item><item><title>&#21152;&#24378;T2I&#27169;&#22411;&#30340;&#21028;&#21035;&#33021;&#21147;&#65292;&#20197;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#23545;&#40784;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2403.04321</link><description>&lt;p&gt;
&#30952;&#20855;&#25506;&#27979;&#21644;&#35843;&#25972;&#29992;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Discriminative Probing and Tuning for Text-to-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04321
&lt;/p&gt;
&lt;p&gt;
&#21152;&#24378;T2I&#27169;&#22411;&#30340;&#21028;&#21035;&#33021;&#21147;&#65292;&#20197;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#23545;&#40784;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#65288;T2I&#65289;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#20808;&#21069;&#30340;&#26041;&#27861;&#32463;&#24120;&#38754;&#20020;&#25991;&#26412;&#22270;&#20687;&#19981;&#23545;&#40784;&#31561;&#38382;&#39064;&#65292;&#22914;&#29983;&#25104;&#22270;&#20687;&#20013;&#30340;&#20851;&#31995;&#28151;&#28102;&#12290;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#21253;&#25324;&#20132;&#21449;&#27880;&#24847;&#21147;&#25805;&#20316;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#32452;&#21512;&#29702;&#35299;&#65292;&#25110;&#32773;&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#25913;&#36827;&#24067;&#23616;&#35268;&#21010;&#12290;&#28982;&#32780;&#65292;T2I&#27169;&#22411;&#30340;&#22266;&#26377;&#23545;&#40784;&#33021;&#21147;&#20173;&#28982;&#19981;&#36275;&#12290;&#36890;&#36807;&#23457;&#35270;&#29983;&#25104;&#27169;&#22411;&#21644;&#21028;&#21035;&#27169;&#22411;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25105;&#20204;&#35748;&#20026;T2I&#27169;&#22411;&#30340;&#21028;&#21035;&#33021;&#21147;&#21487;&#33021;&#21453;&#26144;&#20102;&#23427;&#20204;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#25991;&#26412;&#22270;&#20687;&#23545;&#40784;&#29087;&#32451;&#24230;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#28857;&#65292;&#25105;&#20204;&#20027;&#24352;&#21152;&#24378;T2I&#27169;&#22411;&#30340;&#21028;&#21035;&#33021;&#21147;&#65292;&#20197;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#23545;&#40784;&#29983;&#25104;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24314;&#31435;&#22312;T2I&#27169;&#22411;&#19978;&#30340;&#21028;&#21035;&#36866;&#37197;&#22120;&#65292;&#20197;&#25506;&#27979;&#23427;&#20204;&#22312;&#20004;&#39033;&#20195;&#34920;&#24615;&#20219;&#21153;&#19978;&#30340;&#21028;&#21035;&#33021;&#21147;&#65292;&#24182;&#21033;&#29992;&#21028;&#21035;&#24494;&#35843;&#26469;&#25913;&#21892;&#23427;&#20204;&#30340;&#25991;&#26412;&#22270;&#20687;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04321v1 Announce Type: cross  Abstract: Despite advancements in text-to-image generation (T2I), prior methods often face text-image misalignment problems such as relation confusion in generated images. Existing solutions involve cross-attention manipulation for better compositional understanding or integrating large language models for improved layout planning. However, the inherent alignment capabilities of T2I models are still inadequate. By reviewing the link between generative and discriminative modeling, we posit that T2I models' discriminative abilities may reflect their text-image alignment proficiency during generation. In this light, we advocate bolstering the discriminative abilities of T2I models to achieve more precise text-to-image alignment for generation. We present a discriminative adapter built on T2I models to probe their discriminative abilities on two representative tasks and leverage discriminative fine-tuning to improve their text-image alignment. As a 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#20998;&#25674;&#19978;&#19979;&#25991;&#35760;&#24518;&#30340;&#22312;&#32447;&#36866;&#24212;&#26694;&#26550;&#65292;&#21487;&#26377;&#25928;&#22320;&#25552;&#21462;&#12289;&#21387;&#32553;&#24182;&#23384;&#20648;&#20449;&#24687;&#20197;&#20445;&#25345;&#24378;&#22823;&#30340;&#30693;&#35782;&#20445;&#30041;&#33021;&#21147;</title><link>https://arxiv.org/abs/2403.04317</link><description>&lt;p&gt;
&#24102;&#26377;&#20998;&#25674;&#19978;&#19979;&#25991;&#35760;&#24518;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#22312;&#32447;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Online Adaptation of Language Models with a Memory of Amortized Contexts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04317
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#20998;&#25674;&#19978;&#19979;&#25991;&#35760;&#24518;&#30340;&#22312;&#32447;&#36866;&#24212;&#26694;&#26550;&#65292;&#21487;&#26377;&#25928;&#22320;&#25552;&#21462;&#12289;&#21387;&#32553;&#24182;&#23384;&#20648;&#20449;&#24687;&#20197;&#20445;&#25345;&#24378;&#22823;&#30340;&#30693;&#35782;&#20445;&#30041;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20449;&#24687;&#30340;&#24555;&#36895;&#29983;&#25104;&#21644;&#20256;&#25773;&#65292;&#21363;&#20351;&#24320;&#21457;&#25104;&#26412;&#24040;&#22823;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20063;&#24456;&#24555;&#36807;&#26102;&#12290;&#37492;&#20110;&#20445;&#25345;&#27169;&#22411;&#26356;&#26032;&#30340;&#37325;&#35201;&#24615;&#65292;&#24403;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;LLMs&#26102;&#65292;&#22312;&#32447;&#23398;&#20064;&#24050;&#25104;&#20026;&#19968;&#39033;&#33267;&#20851;&#37325;&#35201;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#37492;&#20110;&#19981;&#26029;&#25193;&#22823;&#30340;&#26410;&#35265;&#25991;&#26723;&#35821;&#26009;&#24211;&#21644;&#29616;&#20195;LLMs&#30340;&#22823;&#21442;&#25968;&#31354;&#38388;&#65292;&#39640;&#25928;&#30340;&#36866;&#24212;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Memory of Amortized Contexts&#65288;MAC&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;LLMs&#30340;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340;&#22312;&#32447;&#36866;&#24212;&#26694;&#26550;&#65292;&#20855;&#26377;&#36739;&#24378;&#30340;&#30693;&#35782;&#20445;&#30041;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25674;&#38144;&#29305;&#24449;&#25552;&#21462;&#21644;&#35760;&#24518;&#22686;&#24378;&#26041;&#27861;&#65292;&#23558;&#26032;&#25991;&#26723;&#20013;&#30340;&#20449;&#24687;&#21387;&#32553;&#24182;&#25552;&#21462;&#20026;&#23384;&#20648;&#22312;&#35760;&#24518;&#24211;&#20013;&#30340;&#32039;&#20945;&#35843;&#21046;&#12290;&#22312;&#22238;&#31572;&#38382;&#39064;&#26102;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20851;&#27880;&#24182;&#20174;&#35813;&#35760;&#24518;&#24211;&#20013;&#25552;&#21462;&#30456;&#20851;&#30693;&#35782;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#23398;&#20064;&#26377;&#20449;&#24687;&#37327;&#30340;&#35843;&#21046;&#8230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04317v1 Announce Type: cross  Abstract: Due to the rapid generation and dissemination of information, large language models (LLMs) quickly run out of date despite enormous development costs. Due to this crucial need to keep models updated, online learning has emerged as a critical necessity when utilizing LLMs for real-world applications. However, given the ever-expanding corpus of unseen documents and the large parameter space of modern LLMs, efficient adaptation is essential. To address these challenges, we propose Memory of Amortized Contexts (MAC), an efficient and effective online adaptation framework for LLMs with strong knowledge retention. We propose an amortized feature extraction and memory-augmentation approach to compress and extract information from new documents into compact modulations stored in a memory bank. When answering questions, our model attends to and extracts relevant knowledge from this memory bank. To learn informative modulations in an efficient m
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24847;&#22270;&#35821;&#20041;&#24037;&#20855;&#21253;&#65292;&#36890;&#36807;&#24341;&#20837;&#19977;&#20803;&#20219;&#21153;&#26469;&#35780;&#20272;&#27169;&#22411;&#23545;&#20110;&#21542;&#23450;&#21644;&#21547;&#20041;&#25512;&#26029;&#36825;&#20004;&#20010;&#23454;&#38469;&#23545;&#35805;&#31995;&#32479;&#20013;&#37325;&#35201;&#30340;&#35821;&#20041;&#27010;&#24565;&#30340;&#29702;&#35299;&#24773;&#20917;</title><link>https://arxiv.org/abs/2403.04314</link><description>&lt;p&gt;
&#24744;&#30340;&#27169;&#22411;&#33021;&#20998;&#36776;&#21542;&#23450;&#21644;&#21547;&#20041;&#25512;&#26029;&#21527;&#65311;&#25581;&#31034;&#24847;&#22270;&#32534;&#30721;&#22120;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Can Your Model Tell a Negation from an Implicature? Unravelling Challenges With Intent Encoders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04314
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24847;&#22270;&#35821;&#20041;&#24037;&#20855;&#21253;&#65292;&#36890;&#36807;&#24341;&#20837;&#19977;&#20803;&#20219;&#21153;&#26469;&#35780;&#20272;&#27169;&#22411;&#23545;&#20110;&#21542;&#23450;&#21644;&#21547;&#20041;&#25512;&#26029;&#36825;&#20004;&#20010;&#23454;&#38469;&#23545;&#35805;&#31995;&#32479;&#20013;&#37325;&#35201;&#30340;&#35821;&#20041;&#27010;&#24565;&#30340;&#29702;&#35299;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25688;&#35201;&#65306;&#23545;&#35805;&#31995;&#32479;&#36890;&#24120;&#20381;&#36182;&#20110;&#23884;&#20837;&#27169;&#22411;&#36827;&#34892;&#24847;&#22270;&#20998;&#31867;&#21644;&#24847;&#22270;&#32858;&#31867;&#20219;&#21153;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#20351;&#24471;&#35843;&#25972;&#23884;&#20837;&#31354;&#38388;&#19978;&#30340;&#35821;&#20041;&#25104;&#20026;&#21487;&#33021;&#65292;&#36890;&#36807;&#25552;&#31034;&#26469;&#36827;&#34892;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#35780;&#20272;&#22522;&#20934;&#20165;&#20381;&#36182;&#20110;&#20219;&#21153;&#24230;&#37327;&#65292;&#19981;&#33021;&#24456;&#22909;&#22320;&#34913;&#37327;&#19982;&#35821;&#20041;&#29702;&#35299;&#26377;&#20851;&#30340;&#24046;&#36317;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24847;&#22270;&#35821;&#20041;&#24037;&#20855;&#21253;&#65292;&#36890;&#36807;&#32771;&#34385;&#24847;&#22270;&#23884;&#20837;&#27169;&#22411;&#30340;&#19977;&#20010;&#20219;&#21153;&#65288;1&#65289;&#24847;&#22270;&#20998;&#31867;&#65292;&#65288;2&#65289;&#24847;&#22270;&#32858;&#31867;&#65292;&#20197;&#21450;&#65288;3&#65289;&#19968;&#20010;&#26032;&#39062;&#30340;&#19977;&#20803;&#20219;&#21153;&#65292;&#26469;&#26356;&#20840;&#38754;&#22320;&#23637;&#31034;&#24847;&#22270;&#23884;&#20837;&#27169;&#22411;&#12290;&#19977;&#20803;&#20219;&#21153;&#35780;&#20272;&#27169;&#22411;&#23545;&#23454;&#38469;&#23545;&#35805;&#31995;&#32479;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#20004;&#20010;&#35821;&#20041;&#27010;&#24565;--&#21542;&#23450;&#21644;&#21547;&#20041;&#25512;&#26029;&#30340;&#29702;&#35299;&#24773;&#20917;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#24403;&#21069;&#30340;&#23884;&#20837;&#27169;&#22411;&#22312;&#36825;&#20123;&#27010;&#24565;&#30340;&#35821;&#20041;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04314v1 Announce Type: new  Abstract: Conversational systems often rely on embedding models for intent classification and intent clustering tasks. The advent of Large Language Models (LLMs), which enable instructional embeddings allowing one to adjust semantics over the embedding space using prompts, are being viewed as a panacea for these downstream conversational tasks. However, traditional evaluation benchmarks rely solely on task metrics that don't particularly measure gaps related to semantic understanding. Thus, we propose an intent semantic toolkit that gives a more holistic view of intent embedding models by considering three tasks-- (1) intent classification, (2) intent clustering, and (3) a novel triplet task. The triplet task gauges the model's understanding of two semantic concepts paramount in real-world conversational systems-- negation and implicature. We observe that current embedding models fare poorly in semantic understanding of these concepts. To address 
&lt;/p&gt;</description></item><item><title>ALTO&#26159;&#19968;&#20010;&#32593;&#32476;&#32534;&#25490;&#22120;&#65292;&#38024;&#23545;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#26426;&#20250;&#65292;&#23454;&#29616;&#20102;&#39640;&#21534;&#21520;&#37327;&#21644;&#20302;&#24310;&#36831;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#27969;&#24335;&#20013;&#38388;&#36755;&#20986;&#30340;&#20004;&#20010;&#26032;&#25361;&#25112;&#65306;&#27491;&#30830;&#24615;&#21644;&#36127;&#36733;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2403.04311</link><description>&lt;p&gt;
ALTO&#65306;&#19968;&#31181;&#29992;&#20110;&#22797;&#21512;AI&#31995;&#32479;&#30340;&#39640;&#25928;&#32593;&#32476;&#32534;&#25490;&#22120;
&lt;/p&gt;
&lt;p&gt;
ALTO: An Efficient Network Orchestrator for Compound AI Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04311
&lt;/p&gt;
&lt;p&gt;
ALTO&#26159;&#19968;&#20010;&#32593;&#32476;&#32534;&#25490;&#22120;&#65292;&#38024;&#23545;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#26426;&#20250;&#65292;&#23454;&#29616;&#20102;&#39640;&#21534;&#21520;&#37327;&#21644;&#20302;&#24310;&#36831;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#27969;&#24335;&#20013;&#38388;&#36755;&#20986;&#30340;&#20004;&#20010;&#26032;&#25361;&#25112;&#65306;&#27491;&#30830;&#24615;&#21644;&#36127;&#36733;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;ALTO&#65292;&#19968;&#31181;&#29992;&#20110;&#26377;&#25928;&#20026;&#35832;&#22914;&#35821;&#35328;&#27169;&#22411;&#31649;&#36947;&#20043;&#31867;&#30340;&#22797;&#21512;AI&#31995;&#32479;&#25552;&#20379;&#26381;&#21153;&#30340;&#32593;&#32476;&#32534;&#25490;&#22120;&#12290;ALTO&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#29305;&#26377;&#30340;&#20248;&#21270;&#26426;&#20250;&#65306;&#27969;&#24335;&#20013;&#38388;&#36755;&#20986;&#65292;&#23454;&#29616;&#20102;&#39640;&#21534;&#21520;&#37327;&#21644;&#20302;&#24310;&#36831;&#12290;&#30001;&#20110;&#35821;&#35328;&#27169;&#22411;&#36880;&#20010;&#29983;&#25104;token&#30340;&#36755;&#20986;&#65292;ALTO&#22312;&#21487;&#33021;&#26102;&#26292;&#38706;&#20102;&#22312;&#38454;&#27573;&#20043;&#38388;&#27969;&#24335;&#20256;&#36755;&#20013;&#38388;&#36755;&#20986;&#30340;&#26426;&#20250;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#22312;&#36328;&#20998;&#24067;&#24335;&#31649;&#36947;&#38454;&#27573;&#23454;&#20363;&#20043;&#38388;&#27969;&#24335;&#20256;&#36755;&#20013;&#38388;&#25968;&#25454;&#26102;&#20986;&#29616;&#30340;&#20004;&#20010;&#26032;&#25361;&#25112;&#65306;&#27491;&#30830;&#24615;&#21644;&#36127;&#36733;&#24179;&#34913;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#32858;&#21512;&#24863;&#30693;&#36335;&#30001;&#25509;&#21475;&#21644;&#20998;&#24067;&#24335;&#25552;&#31034;&#24863;&#30693;&#35843;&#24230;&#20197;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#22797;&#26434;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#39564;&#35777;&#31649;&#36947;&#19978;&#23637;&#31034;&#20102;ALTO&#37096;&#20998;&#36755;&#20986;&#27969;&#24335;&#20256;&#36755;&#30340;&#24433;&#21709;&#65292;&#23558;&#21534;&#21520;&#37327;&#25552;&#39640;&#20102;&#26368;&#22810;3&#20493;&#65292;&#21516;&#26102;&#23558;&#22266;&#23450;&#24310;&#36831;&#30446;&#26631;&#35774;&#32622;&#20026;4&#31186;/&#35831;&#27714;&#65292;&#36824;&#20943;&#23569;&#20102;&#23614;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04311v1 Announce Type: new  Abstract: We present ALTO, a network orchestrator for efficiently serving compound AI systems such as pipelines of language models. ALTO achieves high throughput and low latency by taking advantage of an optimization opportunity specific to generative language models: streaming intermediate outputs. As language models produce outputs token by token, ALTO exposes opportunities to stream intermediate outputs between stages when possible. We highlight two new challenges of correctness and load balancing which emerge when streaming intermediate data across distributed pipeline stage instances. We also motivate the need for an aggregation-aware routing interface and distributed prompt-aware scheduling to address these challenges. We demonstrate the impact of ALTO's partial output streaming on a complex chatbot verification pipeline, increasing throughput by up to 3x for a fixed latency target of 4 seconds / request while also reducing tail latency by 1
&lt;/p&gt;</description></item><item><title>HaluEval-Wild&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;&#23454;&#38469;&#29615;&#22659;&#20013;LLM&#24187;&#35273;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#25910;&#38598;&#20102;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29992;&#25143;&#26597;&#35810;&#24182;&#20998;&#31867;&#20026;&#20116;&#31181;&#19981;&#21516;&#31867;&#22411;&#65292;&#21487;&#20197;&#23545;LLM&#34920;&#29616;&#20986;&#30340;&#24187;&#35273;&#31867;&#22411;&#36827;&#34892;&#32454;&#31890;&#24230;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.04307</link><description>&lt;p&gt;
HaluEval-Wild&#65306;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
HaluEval-Wild: Evaluating Hallucinations of Language Models in the Wild
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04307
&lt;/p&gt;
&lt;p&gt;
HaluEval-Wild&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;&#23454;&#38469;&#29615;&#22659;&#20013;LLM&#24187;&#35273;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#25910;&#38598;&#20102;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29992;&#25143;&#26597;&#35810;&#24182;&#20998;&#31867;&#20026;&#20116;&#31181;&#19981;&#21516;&#31867;&#22411;&#65292;&#21487;&#20197;&#23545;LLM&#34920;&#29616;&#20986;&#30340;&#24187;&#35273;&#31867;&#22411;&#36827;&#34892;&#32454;&#31890;&#24230;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24187;&#35273;&#23545;&#20110;&#20851;&#38190;&#39046;&#22495;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21487;&#38752;&#24615;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26368;&#36817;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;LLM&#22312;&#20256;&#32479;NLP&#20219;&#21153;&#20013;&#30340;&#24187;&#35273;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#22914;&#30693;&#35782;&#23494;&#38598;&#22411;&#38382;&#31572;&#65288;QA&#65289;&#21644;&#25688;&#35201;&#65292;&#19981;&#36275;&#20197;&#25429;&#25417;&#21160;&#24577;&#23454;&#38469;&#29615;&#22659;&#20013;&#29992;&#25143;-LLM&#20132;&#20114;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;HaluEval-Wild&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;&#23454;&#38469;&#29615;&#22659;&#20013;LLM&#24187;&#35273;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#31934;&#24515;&#25910;&#38598;&#20102;&#26469;&#33258;&#29616;&#26377;&#23454;&#38469;&#29992;&#25143;-LLM&#20132;&#20114;&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;ShareGPT&#65289;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65288;&#32463;Alpaca&#23545;&#25239;&#24615;&#36807;&#28388;&#30340;&#65289;&#29992;&#25143;&#26597;&#35810;&#65292;&#20197;&#35780;&#20272;&#21508;&#31181;LLM&#30340;&#24187;&#35273;&#29575;&#12290;&#22312;&#20998;&#26512;&#25910;&#38598;&#21040;&#30340;&#26597;&#35810;&#21518;&#65292;&#25105;&#20204;&#23558;&#20854;&#20998;&#31867;&#20026;&#20116;&#31181;&#19981;&#21516;&#31867;&#22411;&#65292;&#36825;&#20351;&#24471;&#21487;&#20197;&#23545;LLM&#34920;&#29616;&#20986;&#30340;&#24187;&#35273;&#31867;&#22411;&#36827;&#34892;&#32454;&#31890;&#24230;&#20998;&#26512;&#65292;&#24182;&#23558;&#24341;&#29992;&#31572;&#26696;&#19982;&#24378;&#22823;&#30340;GP&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04307v1 Announce Type: new  Abstract: Hallucinations pose a significant challenge to the reliability of large language models (LLMs) in critical domains. Recent benchmarks designed to assess LLM hallucinations within conventional NLP tasks, such as knowledge-intensive question answering (QA) and summarization, are insufficient for capturing the complexities of user-LLM interactions in dynamic, real-world settings. To address this gap, we introduce HaluEval-Wild, the first benchmark specifically designed to evaluate LLM hallucinations in the wild. We meticulously collect challenging (adversarially filtered by Alpaca) user queries from existing real-world user-LLM interaction datasets, including ShareGPT, to evaluate the hallucination rates of various LLMs. Upon analyzing the collected queries, we categorize them into five distinct types, which enables a fine-grained analysis of the types of hallucinations LLMs exhibit, and synthesize the reference answers with the powerful GP
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;Proxy-RLHF&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#21644;&#23545;&#40784;&#36807;&#31243;&#35299;&#32806;&#65292;&#23454;&#29616;&#20102;&#20197;&#26356;&#20302;&#35745;&#31639;&#25104;&#26412;&#23545;&#40784;&#20154;&#31867;&#20215;&#20540;&#35266;&#65292;&#20165;&#20351;&#29992;&#20854;&#20182;&#26041;&#27861;&#30340;1%&#35757;&#32451;&#21442;&#25968;&#21363;&#21487;&#36798;&#21040;&#21487;&#27604;&#27700;&#24179;&#30340;&#23545;&#40784;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.04283</link><description>&lt;p&gt;
Proxy-RLHF&#65306;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36890;&#36807;&#20195;&#29702;&#35299;&#32806;&#29983;&#25104;&#21644;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Proxy-RLHF: Decoupling Generation and Alignment in Large Language Model with Proxy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04283
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;Proxy-RLHF&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#21644;&#23545;&#40784;&#36807;&#31243;&#35299;&#32806;&#65292;&#23454;&#29616;&#20102;&#20197;&#26356;&#20302;&#35745;&#31639;&#25104;&#26412;&#23545;&#40784;&#20154;&#31867;&#20215;&#20540;&#35266;&#65292;&#20165;&#20351;&#29992;&#20854;&#20182;&#26041;&#27861;&#30340;1%&#35757;&#32451;&#21442;&#25968;&#21363;&#21487;&#36798;&#21040;&#21487;&#27604;&#27700;&#24179;&#30340;&#23545;&#40784;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#65288;RLHF&#65289;&#26159;&#30830;&#20445;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#20445;&#25345;&#19968;&#33268;&#30340;&#20027;&#27969;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;RLHF&#26041;&#27861;&#38656;&#35201;&#39640;&#26114;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#20027;&#35201;&#21407;&#22240;&#20043;&#19968;&#26159;RLHF&#21516;&#26102;&#23558;&#29983;&#25104;&#21644;&#23545;&#40784;&#20219;&#21153;&#20998;&#37197;&#32473;LLM&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Proxy-RLHF&#65292;&#23427;&#35299;&#32806;&#20102;LLMs&#30340;&#29983;&#25104;&#21644;&#23545;&#40784;&#27969;&#31243;&#65292;&#20197;&#26356;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#23454;&#29616;&#19982;&#20154;&#31867;&#20215;&#20540;&#30340;&#23545;&#40784;&#12290;&#25105;&#20204;&#20174;&#20026;&#23545;&#40784;&#36807;&#31243;&#35774;&#35745;&#30340;&#26032;&#22411;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#24320;&#22987;&#65292;&#24182;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#35757;&#32451;&#20102;&#19968;&#20010;&#31616;&#21270;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#30417;&#30563;LLM&#30340;&#26631;&#35760;&#29983;&#25104;&#65292;&#32780;&#19981;&#25913;&#21464;LLM&#26412;&#36523;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#20351;&#29992;&#20854;&#20182;&#26041;&#27861;&#30340;1%&#35757;&#32451;&#21442;&#25968;&#21363;&#21487;&#23454;&#29616;&#21487;&#27604;&#27700;&#24179;&#30340;&#23545;&#40784;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04283v1 Announce Type: cross  Abstract: Reinforcement Learning from Human Feedback (RLHF) is the prevailing approach to ensure Large Language Models (LLMs) align with human values. However, existing RLHF methods require a high computational cost, one main reason being that RLHF assigns both the generation and alignment tasks to the LLM simultaneously. In this paper, we introduce Proxy-RLHF, which decouples the generation and alignment processes of LLMs, achieving alignment with human values at a much lower computational cost. We start with a novel Markov Decision Process (MDP) designed for the alignment process and employ Reinforcement Learning (RL) to train a streamlined proxy model that oversees the token generation of the LLM, without altering the LLM itself. Experiments show that our method achieves a comparable level of alignment with only 1\% of the training parameters of other methods.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#38024;&#23545;&#38463;&#25289;&#20271;&#30005;&#35805;&#23545;&#35805;&#39046;&#22495;&#25361;&#25112;&#30340;&#20840;&#38754;&#35780;&#20272;&#22522;&#20934;&#65292;&#26088;&#22312;&#20026;&#24320;&#21457;&#21644;&#35780;&#20272;&#33021;&#22815;&#36866;&#24212;&#30495;&#23454;&#30005;&#35805;&#36890;&#35759;&#26465;&#20214;&#30340;ASR&#31995;&#32479;&#25552;&#20379;&#19968;&#20010;&#20005;&#26684;&#30340;&#27979;&#35797;&#24179;&#21488;&#12290;</title><link>https://arxiv.org/abs/2403.04280</link><description>&lt;p&gt;
&#29992;&#20110;&#35780;&#20272;&#38463;&#25289;&#20271;&#21628;&#21483;&#39046;&#22495;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#26032;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
A New Benchmark for Evaluating Automatic Speech Recognition in the Arabic Call Domain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04280
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#38024;&#23545;&#38463;&#25289;&#20271;&#30005;&#35805;&#23545;&#35805;&#39046;&#22495;&#25361;&#25112;&#30340;&#20840;&#38754;&#35780;&#20272;&#22522;&#20934;&#65292;&#26088;&#22312;&#20026;&#24320;&#21457;&#21644;&#35780;&#20272;&#33021;&#22815;&#36866;&#24212;&#30495;&#23454;&#30005;&#35805;&#36890;&#35759;&#26465;&#20214;&#30340;ASR&#31995;&#32479;&#25552;&#20379;&#19968;&#20010;&#20005;&#26684;&#30340;&#27979;&#35797;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#26088;&#22312;&#24341;&#20837;&#19968;&#20010;&#20840;&#38754;&#30340;&#29992;&#20110;&#38463;&#25289;&#20271;&#35821;&#38899;&#35782;&#21035;&#35780;&#20272;&#30340;&#22522;&#20934;&#65292;&#19987;&#38376;&#38024;&#23545;&#38463;&#25289;&#20271;&#35821;&#30005;&#35805;&#23545;&#35805;&#20013;&#30340;&#25361;&#25112;&#36827;&#34892;&#20102;&#23450;&#21046;&#12290;&#38463;&#25289;&#20271;&#35821;&#20197;&#20854;&#20016;&#23500;&#30340;&#26041;&#35328;&#22810;&#26679;&#24615;&#21644;&#35821;&#38899;&#22797;&#26434;&#24615;&#32780;&#38395;&#21517;&#65292;&#23545;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#25552;&#20986;&#20102;&#35768;&#22810;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#36825;&#20123;&#25361;&#25112;&#22312;&#30005;&#35805;&#36890;&#35805;&#39046;&#22495;&#36827;&#19968;&#27493;&#25918;&#22823;&#65292;&#37027;&#37324;&#30340;&#38899;&#39057;&#36136;&#37327;&#12289;&#32972;&#26223;&#22122;&#38899;&#21644;&#20250;&#35805;&#24335;&#35821;&#38899;&#39118;&#26684;&#20250;&#23545;&#35782;&#21035;&#20934;&#30830;&#24615;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#24314;&#31435;&#19968;&#20010;&#31283;&#20581;&#30340;&#22522;&#20934;&#65292;&#19981;&#20165;&#21253;&#21547;&#24191;&#27867;&#30340;&#38463;&#25289;&#20271;&#26041;&#35328;&#33539;&#22260;&#65292;&#36824;&#27169;&#25311;&#21628;&#21483;&#36890;&#35759;&#30340;&#30495;&#23454;&#26465;&#20214;&#12290;&#36890;&#36807;&#32467;&#21512;&#22810;&#26679;&#30340;&#26041;&#35328;&#34920;&#36798;&#65292;&#24182;&#32771;&#34385;&#21628;&#21483;&#24405;&#38899;&#30340;&#21487;&#21464;&#36136;&#37327;&#65292;&#36825;&#20010;&#22522;&#20934;&#26088;&#22312;&#20026;&#24320;&#21457;&#21644;&#35780;&#20272;&#33021;&#22815;&#24212;&#23545;&#23454;&#38469;&#25361;&#25112;&#30340;ASR&#31995;&#32479;&#25552;&#20379;&#20005;&#26684;&#30340;&#27979;&#35797;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04280v1 Announce Type: new  Abstract: This work is an attempt to introduce a comprehensive benchmark for Arabic speech recognition, specifically tailored to address the challenges of telephone conversations in Arabic language. Arabic, characterized by its rich dialectal diversity and phonetic complexity, presents a number of unique challenges for automatic speech recognition (ASR) systems. These challenges are further amplified in the domain of telephone calls, where audio quality, background noise, and conversational speech styles negatively affect recognition accuracy. Our work aims to establish a robust benchmark that not only encompasses the broad spectrum of Arabic dialects but also emulates the real-world conditions of call-based communications. By incorporating diverse dialectical expressions and accounting for the variable quality of call recordings, this benchmark seeks to provide a rigorous testing ground for the development and evaluation of ASR systems capable of
&lt;/p&gt;</description></item><item><title>&#31038;&#21306;&#25361;&#25112;&#35780;&#20272;&#31454;&#36187;&#22312;&#20419;&#36827;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25366;&#25496;&#30740;&#31350;&#20013;&#30340;&#25216;&#26415;&#21019;&#26032;&#21644;&#36328;&#23398;&#31185;&#21512;&#20316;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.04261</link><description>&lt;p&gt;
&#36890;&#36807;&#31038;&#21306;&#25361;&#25112;&#25512;&#21160;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25366;&#25496;&#30340;&#21457;&#23637;
&lt;/p&gt;
&lt;p&gt;
Advancing Biomedical Text Mining with Community Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04261
&lt;/p&gt;
&lt;p&gt;
&#31038;&#21306;&#25361;&#25112;&#35780;&#20272;&#31454;&#36187;&#22312;&#20419;&#36827;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25366;&#25496;&#30740;&#31350;&#20013;&#30340;&#25216;&#26415;&#21019;&#26032;&#21644;&#36328;&#23398;&#31185;&#21512;&#20316;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#39046;&#22495;&#31215;&#32047;&#20102;&#22823;&#37327;&#26469;&#33258;&#31185;&#23398;&#25991;&#29486;&#12289;&#30005;&#23376;&#30149;&#21382;&#12289;&#20020;&#24202;&#35797;&#39564;&#25253;&#21578;&#21644;&#31038;&#20132;&#23186;&#20307;&#31561;&#21508;&#26041;&#38754;&#30340;&#25991;&#26412;&#25968;&#25454;&#65292;&#28982;&#32780;&#25163;&#21160;&#22788;&#29702;&#21644;&#20998;&#26512;&#36825;&#20123;&#24222;&#22823;&#19988;&#22797;&#26434;&#30340;&#36164;&#28304;&#26159;&#32791;&#26102;&#19988;&#20302;&#25928;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25366;&#25496;&#65292;&#20063;&#31216;&#20026;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#22791;&#21463;&#20851;&#27880;&#12290;&#31038;&#21306;&#25361;&#25112;&#35780;&#20272;&#31454;&#36187;&#22312;&#20419;&#36827;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25366;&#25496;&#30740;&#31350;&#20013;&#30340;&#25216;&#26415;&#21019;&#26032;&#21644;&#36328;&#23398;&#31185;&#21512;&#20316;&#26041;&#38754;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#36825;&#20123;&#25361;&#25112;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#24320;&#21457;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#20013;&#25968;&#25454;&#25366;&#25496;&#21644;&#20449;&#24687;&#22788;&#29702;&#30340;&#26368;&#26032;&#35299;&#20915;&#26041;&#26696;&#30340;&#24179;&#21488;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#19982;&#20013;&#25991;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25366;&#25496;&#26377;&#20851;&#30340;&#26368;&#26032;&#31038;&#21306;&#25361;&#25112;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04261v1 Announce Type: new  Abstract: The field of biomedical research has witnessed a significant increase in the accumulation of vast amounts of textual data from various sources such as scientific literatures, electronic health records, clinical trial reports, and social media. However, manually processing and analyzing these extensive and complex resources is time-consuming and inefficient. To address this challenge, biomedical text mining, also known as biomedical natural language processing, has garnered great attention. Community challenge evaluation competitions have played an important role in promoting technology innovation and interdisciplinary collaboration in biomedical text mining research. These challenges provide platforms for researchers to develop state-of-the-art solutions for data mining and information processing in biomedical research. In this article, we review the recent advances in community challenges specific to Chinese biomedical text mining. Firs
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36880;&#27493;&#30693;&#35782;&#25552;&#21462;&#26694;&#26550;&#65288;SLIM&#65289;&#65292;&#20026;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39640;&#36164;&#28304;&#38656;&#27714;&#30340;&#38590;&#39064;&#65292;&#20351;&#20854;&#33021;&#20197;&#36164;&#28304;&#39640;&#25928;&#30340;&#26041;&#24335;&#20139;&#21463;LLMs&#30340;&#20986;&#33394;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.04260</link><description>&lt;p&gt;
&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#25104;&#20026;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#30340;&#33391;&#22909;&#25512;&#29702;&#32773;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Small Language Models be Good Reasoners for Sequential Recommendation?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04260
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36880;&#27493;&#30693;&#35782;&#25552;&#21462;&#26694;&#26550;&#65288;SLIM&#65289;&#65292;&#20026;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39640;&#36164;&#28304;&#38656;&#27714;&#30340;&#38590;&#39064;&#65292;&#20351;&#20854;&#33021;&#20197;&#36164;&#28304;&#39640;&#25928;&#30340;&#26041;&#24335;&#20139;&#21463;LLMs&#30340;&#20986;&#33394;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30001;&#20110;&#20854;&#20986;&#33394;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#20026;&#39034;&#24207;&#25512;&#33616;&#24320;&#25299;&#20102;&#26032;&#30340;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#35201;&#25104;&#21151;&#23454;&#29616;&#30001;LLMs&#36171;&#33021;&#30340;&#39034;&#24207;&#25512;&#33616;&#36824;&#26377;&#35768;&#22810;&#25361;&#25112;&#38656;&#35201;&#35299;&#20915;&#12290;&#39318;&#20808;&#65292;&#29992;&#25143;&#34892;&#20026;&#27169;&#24335;&#36890;&#24120;&#22797;&#26434;&#65292;&#20165;&#20165;&#20381;&#38752;LLMs&#30340;&#19968;&#27493;&#25512;&#29702;&#21487;&#33021;&#20250;&#23548;&#33268;&#38169;&#35823;&#25110;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#21709;&#24212;&#12290;&#20854;&#27425;&#65292;LLMs&#65288;&#20363;&#22914;ChatGPT-175B&#65289;&#26497;&#39640;&#30340;&#36164;&#28304;&#38656;&#27714;&#26159;&#38590;&#20197;&#25215;&#21463;&#19988;&#22312;&#23454;&#38469;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#20013;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#36880;&#27493;&#30693;&#35782;&#25552;&#21462;&#26694;&#26550;&#29992;&#20110;&#25512;&#33616;&#65288;SLIM&#65289;&#65292;&#20026;&#39034;&#24207;&#25512;&#33616;&#22120;&#20197;&#8220;&#30246;&#8221;&#65288;&#21363;&#36164;&#28304;&#39640;&#25928;&#65289;&#30340;&#26041;&#24335;&#20139;&#21463;LLMs&#20986;&#33394;&#30340;&#25512;&#29702;&#33021;&#21147;&#38138;&#24179;&#20102;&#19968;&#26465;&#26377;&#21069;&#36884;&#30340;&#36947;&#36335;&#12290;&#25105;&#20204;&#24341;&#20837;&#22522;&#20110;&#29992;&#25143;&#34892;&#20026;&#24207;&#21015;&#30340;CoT&#25552;&#31034;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04260v1 Announce Type: cross  Abstract: Large language models (LLMs) open up new horizons for sequential recommendations, owing to their remarkable language comprehension and generation capabilities. However, there are still numerous challenges that should be addressed to successfully implement sequential recommendations empowered by LLMs. Firstly, user behavior patterns are often complex, and relying solely on one-step reasoning from LLMs may lead to incorrect or task-irrelevant responses. Secondly, the prohibitively resource requirements of LLM (e.g., ChatGPT-175B) are overwhelmingly high and impractical for real sequential recommender systems. In this paper, we propose a novel Step-by-step knowLedge dIstillation fraMework for recommendation (SLIM), paving a promising path for sequential recommenders to enjoy the exceptional reasoning capabilities of LLMs in a "slim" (i.e., resource-efficient) manner. We introduce CoT prompting based on user behavior sequences for the larg
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#36127;&#31181;&#23376;&#23454;&#20307;&#36827;&#34892;&#36229;&#32454;&#31890;&#24230;&#23454;&#20307;&#38598;&#25193;&#23637;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#36229;&#32454;&#31890;&#24230;&#35821;&#20041;&#31867;&#21035;&#34920;&#31034;&#20013;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.04247</link><description>&lt;p&gt;
UltraWiki: &#20351;&#29992;&#36127;&#31181;&#23376;&#23454;&#20307;&#36827;&#34892;&#36229;&#32454;&#31890;&#24230;&#23454;&#20307;&#38598;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
UltraWiki: Ultra-fine-grained Entity Set Expansion with Negative Seed Entities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04247
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#36127;&#31181;&#23376;&#23454;&#20307;&#36827;&#34892;&#36229;&#32454;&#31890;&#24230;&#23454;&#20307;&#38598;&#25193;&#23637;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#36229;&#32454;&#31890;&#24230;&#35821;&#20041;&#31867;&#21035;&#34920;&#31034;&#20013;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#38598;&#25193;&#23637;(ESE)&#26088;&#22312;&#35782;&#21035;&#23646;&#20110;&#19982;&#32473;&#23450;&#31181;&#23376;&#23454;&#20307;&#30456;&#21516;&#35821;&#20041;&#31867;&#21035;&#30340;&#26032;&#23454;&#20307;&#12290;&#20256;&#32479;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#27491;&#31181;&#23376;&#23454;&#20307;&#26469;&#34920;&#31034;&#30446;&#26631;&#35821;&#20041;&#31867;&#21035;&#65292;&#36825;&#23545;&#36229;&#32454;&#31890;&#24230;&#35821;&#20041;&#31867;&#21035;&#30340;&#34920;&#31034;&#26500;&#25104;&#25361;&#25112;&#12290;&#36229;&#32454;&#31890;&#24230;&#35821;&#20041;&#31867;&#21035;&#26159;&#22522;&#20110;&#24102;&#26377;&#26356;&#20855;&#20307;&#23646;&#24615;&#32422;&#26463;&#30340;&#32454;&#31890;&#24230;&#35821;&#20041;&#31867;&#21035;&#23450;&#20041;&#30340;&#12290;&#20165;&#20351;&#29992;&#27491;&#31181;&#23376;&#23454;&#20307;&#25551;&#36848;&#20250;&#24341;&#36215;&#20004;&#20010;&#38382;&#39064;&#65306;(i) &#36229;&#32454;&#31890;&#24230;&#35821;&#20041;&#31867;&#21035;&#20043;&#38388;&#30340;&#27495;&#20041;&#12290;(ii) &#26080;&#27861;&#23450;&#20041;&#8220;&#19981;&#24819;&#35201;&#8221;&#30340;&#35821;&#20041;&#12290;&#30001;&#20110;&#36825;&#20123;&#22266;&#26377;&#32570;&#38519;&#65292;&#20197;&#21069;&#30340;&#26041;&#27861;&#24456;&#38590;&#35299;&#20915;&#36229;&#32454;&#31890;&#24230;ESE(Ultra-ESE)&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#36755;&#20837;&#20013;&#30340;&#36127;&#31181;&#23376;&#23454;&#20307;&#65292;&#23427;&#20204;&#23646;&#20110;&#19982;&#27491;&#31181;&#23376;&#23454;&#20307;&#30456;&#21516;&#30340;&#32454;&#31890;&#24230;&#35821;&#20041;&#31867;&#21035;&#65292;&#20294;&#22312;&#26576;&#20123;&#23646;&#24615;&#19978;&#26377;&#25152;&#19981;&#21516;&#12290;&#36127;&#31181;&#23376;&#23454;&#20307;&#28040;&#38500;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04247v1 Announce Type: new  Abstract: Entity Set Expansion (ESE) aims to identify new entities belonging to the same semantic class as a given set of seed entities. Traditional methods primarily relied on positive seed entities to represent a target semantic class, which poses challenge for the representation of ultra-fine-grained semantic classes. Ultra-fine-grained semantic classes are defined based on fine-grained semantic classes with more specific attribute constraints. Describing it with positive seed entities alone cause two issues: (i) Ambiguity among ultra-fine-grained semantic classes. (ii) Inability to define "unwanted" semantic. Due to these inherent shortcomings, previous methods struggle to address the ultra-fine-grained ESE (Ultra-ESE). To solve this issue, we first introduce negative seed entities in the inputs, which belong to the same fine-grained semantic class as the positive seed entities but differ in certain attributes. Negative seed entities eliminate
&lt;/p&gt;</description></item><item><title>DEEP-ICL &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20219;&#21153;&#23450;&#20041;&#20016;&#23500;&#30340;&#19987;&#23478;&#38598;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#31034;&#33539;&#20013;&#25552;&#21462;&#20219;&#21153;&#23450;&#20041;&#24182;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#31034;&#20363;&#65292;&#23454;&#29616;&#20102;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#20855;&#26377;&#21487;&#27604;&#24615;&#30340;&#24615;&#33021;&#65292;&#31361;&#30772;&#20102;&#20256;&#32479;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.04233</link><description>&lt;p&gt;
DEEP-ICL: &#23450;&#20041;&#20016;&#23500;&#30340;&#19987;&#23478;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DEEP-ICL: Definition-Enriched Experts for Language Model In-Context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04233
&lt;/p&gt;
&lt;p&gt;
DEEP-ICL &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20219;&#21153;&#23450;&#20041;&#20016;&#23500;&#30340;&#19987;&#23478;&#38598;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#31034;&#33539;&#20013;&#25552;&#21462;&#20219;&#21153;&#23450;&#20041;&#24182;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#31034;&#20363;&#65292;&#23454;&#29616;&#20102;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#20855;&#26377;&#21487;&#27604;&#24615;&#30340;&#24615;&#33021;&#65292;&#31361;&#30772;&#20102;&#20256;&#32479;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#20154;&#20204;&#19968;&#30452;&#35748;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#21442;&#25968;&#25968;&#37327;&#39537;&#21160;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#33021;&#21147;&#65292;&#36890;&#36807;&#21033;&#29992;&#20219;&#21153;&#29305;&#23450;&#30340;&#31034;&#33539;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#25361;&#25112;&#36825;&#19968;&#20551;&#35774;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DEEP-ICL&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20219;&#21153;&#23450;&#20041;&#20016;&#23500;&#30340;&#19987;&#23478;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;ICL&#12290; DEEP-ICL&#20174;&#32473;&#23450;&#30340;&#31034;&#33539;&#20013;&#26126;&#30830;&#25552;&#21462;&#20219;&#21153;&#23450;&#20041;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#31034;&#20363;&#29983;&#25104;&#21709;&#24212;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;ICL&#30340;&#25913;&#36827;&#24182;&#19981;&#30452;&#25509;&#20381;&#36182;&#20110;&#27169;&#22411;&#22823;&#23567;&#65292;&#32780;&#22522;&#26412;&#19978;&#28304;&#33258;&#20110;&#29702;&#35299;&#20219;&#21153;&#23450;&#20041;&#21644;&#20219;&#21153;&#24341;&#23548;&#23398;&#20064;&#12290;&#21463;&#21040;&#36825;&#19968;&#21551;&#21457;&#65292;DEEP-ICL&#32467;&#21512;&#20102;&#20004;&#20010;&#20855;&#26377;&#19981;&#21516;&#35282;&#33394;&#30340;3B&#27169;&#22411;&#65288;&#19968;&#20010;&#29992;&#20110;&#24635;&#32467;&#20219;&#21153;&#23450;&#20041;&#65292;&#21478;&#19968;&#20010;&#29992;&#20110;&#23398;&#20064;&#20219;&#21153;&#31034;&#33539;&#65289;&#65292;&#24182;&#23454;&#29616;&#20102;&#19982;LLaMA2-13B&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#20811;&#26381;&#39044;&#35757;&#32451;&#24207;&#21015;&#38271;&#24230;&#65292;&#20248;&#20110;&#20256;&#32479;ICL&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04233v1 Announce Type: cross  Abstract: It has long been assumed that the sheer number of parameters in large language models (LLMs) drives in-context learning (ICL) capabilities, enabling remarkable performance improvements by leveraging task-specific demonstrations. Challenging this hypothesis, we introduce DEEP-ICL, a novel task Definition Enriched ExPert Ensembling methodology for ICL. DEEP-ICL explicitly extracts task definitions from given demonstrations and generates responses through learning task-specific examples. We argue that improvement from ICL does not directly rely on model size, but essentially stems from understanding task definitions and task-guided learning. Inspired by this, DEEP-ICL combines two 3B models with distinct roles (one for concluding task definitions and the other for learning task demonstrations) and achieves comparable performance to LLaMA2-13B. Furthermore, our framework outperforms conventional ICL by overcoming pretraining sequence lengt
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35757;&#32451;&#23545;&#40784;&#22120;&#27169;&#22411;&#26469;&#35299;&#32806;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#23545;&#40784;&#65292;&#20197;&#20943;&#23569;&#23545;&#40784;&#23545;&#24615;&#33021;&#30340;&#28508;&#22312;&#36127;&#38754;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.04224</link><description>&lt;p&gt;
Aligners: &#35299;&#32806;LLMs&#21644;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Aligners: Decoupling LLMs and Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04224
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35757;&#32451;&#23545;&#40784;&#22120;&#27169;&#22411;&#26469;&#35299;&#32806;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#23545;&#40784;&#65292;&#20197;&#20943;&#23569;&#23545;&#40784;&#23545;&#24615;&#33021;&#30340;&#28508;&#22312;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38656;&#35201;&#19982;&#20154;&#31867;&#26399;&#26395;&#23545;&#40784;&#65292;&#20197;&#30830;&#20445;&#23427;&#20204;&#22312;&#22823;&#22810;&#25968;&#24212;&#29992;&#20013;&#30340;&#23433;&#20840;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#23545;&#40784;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#25104;&#26412;&#39640;&#26114;&#65292;&#24182;&#19988;&#38656;&#35201;&#20026;&#27599;&#20010;LLM&#21644;&#23545;&#40784;&#26631;&#20934;&#37325;&#22797;&#36827;&#34892;&#12290;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#35757;&#32451;&#21487;&#20197;&#26681;&#25454;&#38656;&#35201;&#29992;&#20110;&#23545;&#40784;&#32473;&#23450;&#26631;&#20934;&#30340;&#20219;&#20309;LLM&#30340;&#23545;&#40784;&#27169;&#22411;&#26469;&#35299;&#32806;LLMs&#21644;&#23545;&#40784;&#65292;&#20174;&#32780;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#20943;&#23569;&#23545;&#24615;&#33021;&#30340;&#28508;&#22312;&#36127;&#38754;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#23545;&#40784;&#27169;&#22411;&#35757;&#32451;&#37197;&#26041;&#20165;&#20381;&#36182;&#20110;&#20351;&#29992;&#65288;&#25552;&#31034;&#30340;&#65289;LLM &#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#35843;&#25972;&#20197;&#36866;&#24212;&#21508;&#31181;&#23545;&#40784;&#26631;&#20934;&#12290;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#8220;&#36947;&#24503;&#8221;&#23545;&#40784;&#22120;&#24182;&#22312;&#23454;&#39564;&#19978;&#39564;&#35777;&#20854;&#26377;&#25928;&#24615;&#26469;&#38416;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04224v1 Announce Type: cross  Abstract: Large Language Models (LLMs) need to be aligned with human expectations to ensure their safety and utility in most applications. Alignment is challenging, costly, and needs to be repeated for every LLM and alignment criterion. We propose to decouple LLMs and alignment by training aligner models that can be used to align any LLM for a given criteria on an as-needed basis, thus also reducing the potential negative impacts of alignment on performance. Our recipe for training the aligner models solely relies on synthetic data generated with a (prompted) LLM and can be easily adjusted for a variety of alignment criteria. We illustrate our method by training an "ethical" aligner and verify its efficacy empirically.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#25105;&#35780;&#20272;&#20013;&#21033;&#29992;&#29627;&#29827;&#31665;&#29305;&#24449;&#30340;&#23454;&#29992;&#24615;&#65292;&#21457;&#29616;softmax&#20998;&#24067;&#22312;&#36136;&#37327;&#35780;&#20272;&#20013;&#21487;&#38752;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#24341;&#20837;&#21442;&#32771;&#29305;&#24449;&#22686;&#24378;&#35780;&#20272;&#30340;&#31574;&#30053;&#65292;&#24182;&#39564;&#35777;&#20102;&#20351;&#29992;&#29627;&#29827;&#31665;&#29305;&#24449;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#25105;&#35780;&#20272;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.04222</link><description>&lt;p&gt;
&#22522;&#20110;&#29627;&#29827;&#31665;&#29305;&#24449;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Self-Evaluation of Large Language Model based on Glass-box Features
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04222
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#25105;&#35780;&#20272;&#20013;&#21033;&#29992;&#29627;&#29827;&#31665;&#29305;&#24449;&#30340;&#23454;&#29992;&#24615;&#65292;&#21457;&#29616;softmax&#20998;&#24067;&#22312;&#36136;&#37327;&#35780;&#20272;&#20013;&#21487;&#38752;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#24341;&#20837;&#21442;&#32771;&#29305;&#24449;&#22686;&#24378;&#35780;&#20272;&#30340;&#31574;&#30053;&#65292;&#24182;&#39564;&#35777;&#20102;&#20351;&#29992;&#29627;&#29827;&#31665;&#29305;&#24449;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#25105;&#35780;&#20272;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04222v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#34028;&#21187;&#21457;&#23637;&#20984;&#26174;&#20102;&#23545;&#35780;&#20272;&#26041;&#27861;&#30340;&#36843;&#20999;&#38656;&#27714;&#12290;&#29616;&#26377;&#20316;&#21697;&#20027;&#35201;&#20381;&#36182;&#20110;&#22806;&#37096;&#35780;&#20272;&#32773;&#65292;&#20391;&#37325;&#20110;&#35757;&#32451;&#21644;&#25552;&#31034;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#8212;&#8212;&#27169;&#22411;&#24863;&#30693;&#30340;&#29627;&#29827;&#31665;&#29305;&#24449;&#8212;&#8212;&#34987;&#24573;&#35270;&#20102;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#33258;&#25105;&#35780;&#20272;&#24773;&#22659;&#19979;&#20351;&#29992;&#29627;&#29827;&#31665;&#29305;&#24449;&#30340;&#25928;&#29992;&#65292;&#21363;&#24212;&#29992;LLM&#35780;&#20272;&#20854;&#33258;&#36523;&#36755;&#20986;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#21508;&#31181;&#29627;&#29827;&#31665;&#29305;&#24449;&#32452;&#65292;&#24182;&#21457;&#29616;softmax&#20998;&#24067;&#20316;&#20026;&#36136;&#37327;&#35780;&#20272;&#30340;&#21487;&#38752;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#21512;&#24182;&#20174;&#21442;&#32771;&#25991;&#29486;&#20013;&#25552;&#21462;&#30340;&#29305;&#24449;&#26469;&#22686;&#24378;&#35780;&#20272;&#30340;&#20004;&#31181;&#31574;&#30053;&#12290;&#22312;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#20351;&#29992;&#29627;&#29827;&#31665;&#29305;&#24449;&#36827;&#34892;LLMs&#30340;&#33258;&#25105;&#35780;&#20272;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04222v1 Announce Type: new  Abstract: The proliferation of open-source Large Language Models (LLMs) underscores the pressing need for evaluation methods. Existing works primarily rely on external evaluators, focusing on training and prompting strategies. However, a crucial aspect - model-aware glass-box features - is overlooked. In this study, we explore the utility of glass-box features under the scenario of self-evaluation, namely applying an LLM to evaluate its own output. We investigate various glass-box feature groups and discovered that the softmax distribution serves as a reliable indicator for quality evaluation. Furthermore, we propose two strategies to enhance the evaluation by incorporating features derived from references. Experimental results on public benchmarks validate the feasibility of self-evaluation of LLMs using glass-box features.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550; PESS&#65292;&#36890;&#36807;&#35821;&#20041;&#30456;&#20284;&#24615;&#33258;&#21160;&#25512;&#26029;&#23545;&#35805;&#20013;&#30340;&#20154;&#35774;&#20449;&#24687;&#65292;&#20197;&#25903;&#25345;&#24773;&#24863;&#23545;&#35805;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2403.04212</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#20041;&#30456;&#20284;&#24615;&#36827;&#34892;&#20154;&#35774;&#25552;&#21462;&#20197;&#29992;&#20110;&#24773;&#24863;&#25903;&#25345;&#23545;&#35805;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Persona Extraction Through Semantic Similarity for Emotional Support Conversation Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04212
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550; PESS&#65292;&#36890;&#36807;&#35821;&#20041;&#30456;&#20284;&#24615;&#33258;&#21160;&#25512;&#26029;&#23545;&#35805;&#20013;&#30340;&#20154;&#35774;&#20449;&#24687;&#65292;&#20197;&#25903;&#25345;&#24773;&#24863;&#23545;&#35805;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#35805;&#31995;&#32479;&#25552;&#20379;&#24773;&#24863;&#25903;&#25345;&#22312;&#20170;&#22825;&#30340;&#19990;&#30028;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#25903;&#25345;&#35768;&#22810;&#23545;&#35805;&#22330;&#26223;&#20013;&#30340;&#24515;&#29702;&#20581;&#24247;&#21644;&#31038;&#20132;&#20114;&#21160;&#12290;&#36807;&#21435;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#20154;&#35774;&#23545;&#29983;&#25104;&#20849;&#24773;&#21644;&#25903;&#25345;&#24615;&#22238;&#22797;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#36890;&#24120;&#20381;&#36182;&#39044;&#20808;&#25552;&#20379;&#30340;&#20154;&#35774;&#32780;&#19981;&#26159;&#22312;&#23545;&#35805;&#36807;&#31243;&#20013;&#25512;&#26029;&#23427;&#20204;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550; PESS&#65288;&#36890;&#36807;&#35821;&#20041;&#30456;&#20284;&#24615;&#36827;&#34892;&#20154;&#35774;&#25552;&#21462;&#65289;&#65292;&#23427;&#21487;&#20197;&#20174;&#23545;&#35805;&#20013;&#33258;&#21160;&#25512;&#26029;&#20449;&#24687;&#20016;&#23500;&#19988;&#19968;&#33268;&#30340;&#20154;&#35774;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#22522;&#20110;&#35821;&#20041;&#30456;&#20284;&#24615;&#20998;&#25968;&#30340;&#23436;&#25972;&#24615;&#25439;&#22833;&#21644;&#19968;&#33268;&#24615;&#25439;&#22833;&#12290;&#23436;&#25972;&#24615;&#25439;&#22833;&#40723;&#21169;&#27169;&#22411;&#29983;&#25104;&#32570;&#22833;&#30340;&#20154;&#35774;&#20449;&#24687;&#65292;&#19968;&#33268;&#24615;&#25439;&#22833;&#25351;&#23548;&#27169;&#22411;&#21306;&#20998;&#19968;&#33268;&#21644;&#19981;&#19968;&#33268;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04212v1 Announce Type: new  Abstract: Providing emotional support through dialogue systems is becoming increasingly important in today's world, as it can support both mental health and social interactions in many conversation scenarios. Previous works have shown that using persona is effective for generating empathetic and supportive responses. They have often relied on pre-provided persona rather than inferring them during conversations. However, it is not always possible to obtain a user persona before the conversation begins. To address this challenge, we propose PESS (Persona Extraction through Semantic Similarity), a novel framework that can automatically infer informative and consistent persona from dialogues. We devise completeness loss and consistency loss based on semantic similarity scores. The completeness loss encourages the model to generate missing persona information, and the consistency loss guides the model to distinguish between consistent and inconsistent 
&lt;/p&gt;</description></item><item><title>&#35843;&#26597;&#20102;&#22823;&#22411;&#27169;&#22411;&#20215;&#20540;&#23545;&#40784;&#26041;&#27861;&#65292;&#25581;&#31034;&#21382;&#21490;&#32972;&#26223;&#21644;&#25968;&#23398;&#26412;&#36136;&#65292;&#35814;&#32454;&#35752;&#35770;&#20102;&#24378;&#21270;&#23398;&#20064;&#12289;&#30417;&#30563;&#24494;&#35843;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#31561;&#23545;&#40784;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.04204</link><description>&lt;p&gt;
&#23545;&#20110;&#22823;&#22411;&#27169;&#22411;&#23545;&#40784;&#26041;&#27861;&#30340;&#25506;&#31350;&#65306;&#26412;&#36136;&#19982;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
On the Essence and Prospect: An Investigation of Alignment Approaches for Big Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04204
&lt;/p&gt;
&lt;p&gt;
&#35843;&#26597;&#20102;&#22823;&#22411;&#27169;&#22411;&#20215;&#20540;&#23545;&#40784;&#26041;&#27861;&#65292;&#25581;&#31034;&#21382;&#21490;&#32972;&#26223;&#21644;&#25968;&#23398;&#26412;&#36136;&#65292;&#35814;&#32454;&#35752;&#35770;&#20102;&#24378;&#21270;&#23398;&#20064;&#12289;&#30417;&#30563;&#24494;&#35843;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#31561;&#23545;&#40784;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#27169;&#22411;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#31361;&#30772;&#65292;&#20294;&#20063;&#21487;&#33021;&#24102;&#26469;&#28508;&#22312;&#30340;&#38382;&#39064;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#23545;&#40784;&#25216;&#26415;&#65292;&#20351;&#36825;&#20123;&#27169;&#22411;&#31526;&#21512;&#20154;&#31867;&#30340;&#20559;&#22909;&#21644;&#20215;&#20540;&#35266;&#12290;&#23613;&#31649;&#22312;&#36807;&#21435;&#19968;&#24180;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#24314;&#31435;&#26368;&#20339;&#23545;&#40784;&#31574;&#30053;&#20173;&#38754;&#20020;&#35832;&#22810;&#25361;&#25112;&#65292;&#22914;&#25968;&#25454;&#25104;&#26412;&#21644;&#21487;&#25193;&#23637;&#24615;&#30417;&#30563;&#12290;&#22914;&#20309;&#36827;&#34892;&#23545;&#40784;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#31687;&#35843;&#26597;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#35843;&#26597;&#20102;&#20215;&#20540;&#23545;&#40784;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#25581;&#31034;&#20102;&#23545;&#40784;&#30340;&#21382;&#21490;&#32972;&#26223;&#65292;&#36861;&#28335;&#21040;20&#19990;&#32426;20&#24180;&#20195;&#65288;&#23427;&#30340;&#36215;&#28304;&#65289;&#65292;&#28982;&#21518;&#28145;&#20837;&#25506;&#35752;&#20102;&#23545;&#40784;&#30340;&#25968;&#23398;&#26412;&#36136;&#65288;&#23427;&#26159;&#20160;&#20040;&#65289;&#65292;&#38416;&#26126;&#20102;&#20854;&#20013;&#22266;&#26377;&#30340;&#25361;&#25112;&#12290;&#22312;&#22880;&#23450;&#20102;&#36825;&#20010;&#22522;&#30784;&#21518;&#65292;&#25105;&#20204;&#23545;&#29616;&#26377;&#30340;&#23545;&#40784;&#26041;&#27861;&#36827;&#34892;&#20102;&#35814;&#32454;&#32771;&#23519;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#20998;&#20026;&#19977;&#31867;&#65306;&#24378;&#21270;&#23398;&#20064;&#12289;&#30417;&#30563;&#24494;&#35843;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04204v1 Announce Type: new  Abstract: Big models have achieved revolutionary breakthroughs in the field of AI, but they might also pose potential concerns. Addressing such concerns, alignment technologies were introduced to make these models conform to human preferences and values. Despite considerable advancements in the past year, various challenges lie in establishing the optimal alignment strategy, such as data cost and scalable oversight, and how to align remains an open question. In this survey paper, we comprehensively investigate value alignment approaches. We first unpack the historical context of alignment tracing back to the 1920s (where it comes from), then delve into the mathematical essence of alignment (what it is), shedding light on the inherent challenges. Following this foundation, we provide a detailed examination of existing alignment methods, which fall into three categories: Reinforcement Learning, Supervised Fine-Tuning, and In-context Learning, and de
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19978;&#19979;&#25991;&#20998;&#23376;&#36866;&#24212;&#65288;ICMA&#65289;&#33539;&#24335;&#65292;&#20801;&#35768;LLMs&#36890;&#36807;&#19978;&#19979;&#25991;&#31034;&#20363;&#23398;&#20064;&#20998;&#23376;-&#25991;&#26412;&#23545;&#40784;&#65292;&#35299;&#20915;&#20102;&#22312;&#20998;&#23376;-&#26631;&#39064;&#32763;&#35793;&#20219;&#21153;&#20013;&#23545;LLMs&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.04197</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#26159;&#19978;&#19979;&#25991;&#20998;&#23376;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are In-Context Molecule Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04197
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19978;&#19979;&#25991;&#20998;&#23376;&#36866;&#24212;&#65288;ICMA&#65289;&#33539;&#24335;&#65292;&#20801;&#35768;LLMs&#36890;&#36807;&#19978;&#19979;&#25991;&#31034;&#20363;&#23398;&#20064;&#20998;&#23376;-&#25991;&#26412;&#23545;&#40784;&#65292;&#35299;&#20915;&#20102;&#22312;&#20998;&#23376;-&#26631;&#39064;&#32763;&#35793;&#20219;&#21153;&#20013;&#23545;LLMs&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#29289;&#21270;&#23398;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#23588;&#20854;&#26159;&#20998;&#23376;&#26631;&#39064;&#32763;&#35793;&#20219;&#21153;&#65292;&#26088;&#22312;&#24357;&#21512;&#20998;&#23376;&#21644;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#22312;&#36866;&#24212;LLMs&#21040;&#20998;&#23376;-&#26631;&#39064;&#32763;&#35793;&#20219;&#21153;&#20013;&#30340;&#26041;&#27861;&#38656;&#35201;&#39069;&#22806;&#30340;&#39046;&#22495;&#29305;&#23450;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#23384;&#22312;&#20998;&#23376;&#21644;&#25991;&#26412;&#31354;&#38388;&#20043;&#38388;&#30340;&#24369;&#23545;&#40784;&#65292;&#25110;&#23545;LLMs&#30340;&#35268;&#27169;&#26377;&#20005;&#26684;&#35201;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19978;&#19979;&#25991;&#20998;&#23376;&#36866;&#24212;&#65288;ICMA&#65289;&#65292;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#33539;&#20363;&#65292;&#20801;&#35768;LLMs&#36890;&#36807;&#19978;&#19979;&#25991;&#31034;&#20363;&#23398;&#20064;&#20998;&#23376;-&#25991;&#26412;&#23545;&#40784;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#20998;&#23376;&#35843;&#25972;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;ICMA&#21253;&#25324;&#20197;&#19979;&#19977;&#20010;&#38454;&#27573;&#65306;&#36328;&#27169;&#24577;&#26816;&#32034;&#12289;&#26816;&#32034;&#21518;&#25490;&#24207;&#21644;&#19978;&#19979;&#25991;&#20998;&#23376;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04197v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated exceptional performance in biochemical tasks, especially the molecule caption translation task, which aims to bridge the gap between molecules and natural language texts. However, previous methods in adapting LLMs to the molecule-caption translation task required extra domain-specific pre-training stages, suffered weak alignment between molecular and textual spaces, or imposed stringent demands on the scale of LLMs. To resolve the challenges, we propose In-Context Molecule Adaptation (ICMA), as a new paradigm allowing LLMs to learn the molecule-text alignment from context examples via In-Context Molecule Tuning. Specifically, ICMA incorporates the following three stages: Cross-modal Retrieval, Post-retrieval Re-ranking, and In-context Molecule Tuning. Initially, Cross-modal Retrieval utilizes BM25 Caption Retrieval and Molecule Graph Retrieval to retrieve informative context examples. Addi
&lt;/p&gt;</description></item><item><title>&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;</title><link>https://arxiv.org/abs/2403.04190</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#29992;&#20110;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#65306;&#26041;&#27861;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Generative AI for Synthetic Data Generation: Methods, Challenges and the Future
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04190
&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20851;&#20110;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#30340;&#30740;&#31350;&#24613;&#21095;&#22686;&#21152;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#65292;&#26631;&#24535;&#30528;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#39046;&#22495;&#30340;&#19968;&#20010;&#26174;&#33879;&#36716;&#21464;&#12290;&#23427;&#20204;&#33021;&#22815;&#34920;&#29616;&#20986;&#19982;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30456;&#23218;&#32654;&#30340;&#33021;&#21147;&#65292;&#23558;&#36825;&#31181;&#26041;&#27861;&#23450;&#20301;&#20026;&#35299;&#20915;&#20302;&#36164;&#28304;&#25361;&#25112;&#30340;&#19968;&#20010;&#24341;&#20154;&#27880;&#30446;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#21033;&#29992;&#36825;&#20123;&#24222;&#22823;&#30340;LLMs&#29983;&#25104;&#29305;&#23450;&#20219;&#21153;&#35757;&#32451;&#25968;&#25454;&#30340;&#20808;&#36827;&#25216;&#26415;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#26041;&#27861;&#35770;&#12289;&#35780;&#20272;&#25216;&#26415;&#21644;&#23454;&#38469;&#24212;&#29992;&#65292;&#35752;&#35770;&#20102;&#24403;&#21069;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04190v1 Announce Type: cross  Abstract: The recent surge in research focused on generating synthetic data from large language models (LLMs), especially for scenarios with limited data availability, marks a notable shift in Generative Artificial Intelligence (AI). Their ability to perform comparably to real-world data positions this approach as a compelling solution to low-resource challenges. This paper delves into advanced technologies that leverage these gigantic LLMs for the generation of task-specific training data. We outline methodologies, evaluation techniques, and practical applications, discuss the current limitations, and suggest potential pathways for future research.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#24230;&#37327;&#24863;&#30693;&#30340;LLM&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#33258;&#23450;&#20041;&#25351;&#26631;&#26469;&#25913;&#36827;&#25512;&#26029;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.04182</link><description>&lt;p&gt;
&#24230;&#37327;&#24863;&#30693;&#30340;LLM&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Metric-aware LLM inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04182
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#24230;&#37327;&#24863;&#30693;&#30340;LLM&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#33258;&#23450;&#20041;&#25351;&#26631;&#26469;&#25913;&#36827;&#25512;&#26029;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#24378;&#22823;&#30340;&#32467;&#26524;&#12290;&#36890;&#24120;&#65292;&#36755;&#20986;&#26159;&#36890;&#36807;&#20174;LLM&#30340;&#22522;&#30784;&#20998;&#24067;&#20013;&#36827;&#34892;&#33258;&#22238;&#24402;&#37319;&#26679;&#33719;&#24471;&#30340;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#31181;&#25512;&#26029;&#31574;&#30053;&#23545;&#20110;&#19968;&#31995;&#21015;&#20219;&#21153;&#21644;&#30456;&#20851;&#30340;&#35780;&#20272;&#25351;&#26631;&#21487;&#33021;&#26159;&#27425;&#20248;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24230;&#37327;&#24863;&#30693;&#30340;LLM&#25512;&#26029;&#65306;&#19968;&#31181;&#22312;&#25512;&#26029;&#26102;&#38024;&#23545;&#33258;&#23450;&#20041;&#25351;&#26631;&#36827;&#34892;&#20248;&#21270;&#30340;&#20915;&#31574;&#29702;&#35770;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#23398;&#26415;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#20844;&#24320;&#21487;&#29992;&#27169;&#22411;&#19978;&#25253;&#21578;&#20102;&#30456;&#23545;&#22522;&#32447;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04182v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated strong results on a range of NLP tasks. Typically, outputs are obtained via autoregressive sampling from the LLM's underlying distribution. We show that this inference strategy can be suboptimal for a range of tasks and associated evaluation metrics. As a remedy, we propose metric aware LLM inference: a decision theoretic approach optimizing for custom metrics at inference time. We report improvements over baselines on academic benchmarks and publicly available models.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20855;&#26377;&#21360;&#24230;&#33521;&#35821;&#37325;&#38899;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19968;&#20010;&#33021;&#22815;&#23558;&#37325;&#38899;&#34701;&#20837;&#21512;&#25104;&#35821;&#38899;&#20013;&#30340;&#25991;&#26412;&#21040;&#35821;&#38899;&#26550;&#26500;&#65292;&#29992;&#20110;&#22312;&#35821;&#38899;&#21040;&#35821;&#38899;&#26426;&#22120;&#32763;&#35793;&#20013;&#23454;&#29616;&#24212;&#21147;&#20256;&#36882;&#12290;</title><link>https://arxiv.org/abs/2403.04178</link><description>&lt;p&gt;
&#23581;&#35797;&#22312;&#35821;&#38899;&#21040;&#35821;&#38899;&#26426;&#22120;&#32763;&#35793;&#20013;&#23454;&#29616;&#24212;&#21147;&#20256;&#36882;
&lt;/p&gt;
&lt;p&gt;
Attempt Towards Stress Transfer in Speech-to-Speech Machine Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04178
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20855;&#26377;&#21360;&#24230;&#33521;&#35821;&#37325;&#38899;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19968;&#20010;&#33021;&#22815;&#23558;&#37325;&#38899;&#34701;&#20837;&#21512;&#25104;&#35821;&#38899;&#20013;&#30340;&#25991;&#26412;&#21040;&#35821;&#38899;&#26550;&#26500;&#65292;&#29992;&#20110;&#22312;&#35821;&#38899;&#21040;&#35821;&#38899;&#26426;&#22120;&#32763;&#35793;&#20013;&#23454;&#29616;&#24212;&#21147;&#20256;&#36882;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04178v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#21360;&#24230;&#25945;&#32946;&#37096;&#38376;&#30340;&#35821;&#35328;&#22810;&#26679;&#24615;&#26500;&#25104;&#20102;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#22952;&#30861;&#20102;&#21253;&#23481;&#24615;&#12290;&#23613;&#31649;&#36890;&#36807;&#22312;&#32447;&#25945;&#32946;&#20869;&#23481;&#23454;&#29616;&#20102;&#30693;&#35782;&#30340;&#27665;&#20027;&#21270;&#65292;&#20294;&#20316;&#20026;&#20114;&#32852;&#32593;&#36890;&#29992;&#35821;&#35328;&#30340;&#33521;&#35821;&#30340;&#20027;&#23548;&#22320;&#20301;&#38480;&#21046;&#20102;&#21487;&#35775;&#38382;&#24615;&#65292;&#24378;&#35843;&#20102;&#23558;&#20869;&#23481;&#32763;&#35793;&#20026;&#21360;&#24230;&#35821;&#35328;&#30340;&#37325;&#35201;&#24615;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#35821;&#38899;&#21040;&#35821;&#38899;&#26426;&#22120;&#32763;&#35793;&#65288;SSMT&#65289;&#25216;&#26415;&#65292;&#20294;&#36825;&#20123;&#31995;&#32479;&#20013;&#32570;&#20047;&#35821;&#35843;&#23548;&#33268;&#32763;&#35793;&#21333;&#35843;&#65292;&#23548;&#33268;&#35266;&#20247;&#22833;&#21435;&#20852;&#36259;&#24182;&#33073;&#31163;&#20869;&#23481;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#20221;&#24102;&#26377;&#21360;&#24230;&#33521;&#35821;&#37325;&#38899;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#24341;&#20837;&#20102;&#19968;&#20010;&#33021;&#22815;&#23558;&#37325;&#38899;&#34701;&#20837;&#21512;&#25104;&#35821;&#38899;&#20013;&#30340;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#26550;&#26500;&#12290;&#35813;&#25968;&#25454;&#38598;&#29992;&#20110;&#35757;&#32451;&#37325;&#38899;&#26816;&#27979;&#27169;&#22411;&#65292;&#28982;&#21518;&#35813;&#27169;&#22411;&#29992;&#20110;SSMT&#31995;&#32479;&#20013;&#26816;&#27979;&#28304;&#35821;&#38899;&#20013;&#30340;&#37325;&#38899;&#24182;&#23558;&#20854;&#36716;&#31227;&#21040;&#30446;&#26631;&#35821;&#35328;&#35821;&#38899;&#20013;&#12290;TTS&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04178v1 Announce Type: new  Abstract: The language diversity in India's education sector poses a significant challenge, hindering inclusivity. Despite the democratization of knowledge through online educational content, the dominance of English, as the internet's lingua franca, limits accessibility, emphasizing the crucial need for translation into Indian languages. Despite existing Speech-to-Speech Machine Translation (SSMT) technologies, the lack of intonation in these systems gives monotonous translations, leading to a loss of audience interest and disengagement from the content. To address this, our paper introduces a dataset with stress annotations in Indian English and also a Text-to-Speech (TTS) architecture capable of incorporating stress into synthesized speech. This dataset is used for training a stress detection model, which is then used in the SSMT system for detecting stress in the source speech and transferring it into the target language speech. The TTS archit
&lt;/p&gt;</description></item><item><title>DA-Net &#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22810;&#28304;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#20013;&#20849;&#20139;&#32534;&#30721;&#22120;&#23548;&#33268;&#23398;&#20064;&#22256;&#25200;&#21644;&#35821;&#35328;&#29305;&#23450;&#20998;&#31867;&#22120;&#24615;&#33021;&#19979;&#38477;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.04158</link><description>&lt;p&gt;
DA-Net: &#19968;&#31181;&#29992;&#20110;&#22810;&#28304;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#30340;&#35299;&#32806;&#33258;&#36866;&#24212;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
DA-Net: A Disentangled and Adaptive Network for Multi-Source Cross-Lingual Transfer Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04158
&lt;/p&gt;
&lt;p&gt;
DA-Net &#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22810;&#28304;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#20013;&#20849;&#20139;&#32534;&#30721;&#22120;&#23548;&#33268;&#23398;&#20064;&#22256;&#25200;&#21644;&#35821;&#35328;&#29305;&#23450;&#20998;&#31867;&#22120;&#24615;&#33021;&#19979;&#38477;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#28304;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#28041;&#21450;&#20174;&#22810;&#20010;&#24050;&#26631;&#35760;&#28304;&#35821;&#35328;&#21521;&#19968;&#20010;&#26410;&#26631;&#35760;&#30446;&#26631;&#35821;&#35328;&#22312;&#35821;&#35328;&#36716;&#31227;&#19979;&#30340;&#20219;&#21153;&#30693;&#35782;&#20256;&#36755;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20851;&#27880;&#20110;&#21152;&#26435;&#19981;&#21516;&#28304;&#35821;&#35328;&#30340;&#29305;&#23450;&#35821;&#35328;&#20998;&#31867;&#22120;&#29983;&#25104;&#30340;&#39044;&#27979;&#65292;&#36825;&#20123;&#20998;&#31867;&#22120;&#36981;&#24490;&#20849;&#20139;&#32534;&#30721;&#22120;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#28304;&#35821;&#35328;&#20849;&#20139;&#30456;&#21516;&#30340;&#32534;&#30721;&#22120;&#65292;&#36825;&#20010;&#32534;&#30721;&#22120;&#34987;&#25152;&#26377;&#36825;&#20123;&#35821;&#35328;&#26356;&#26032;&#12290;&#25552;&#21462;&#20986;&#30340;&#34920;&#31034;&#19981;&#21487;&#36991;&#20813;&#22320;&#21253;&#21547;&#19981;&#21516;&#28304;&#35821;&#35328;&#30340;&#20449;&#24687;&#65292;&#36825;&#21487;&#33021;&#24178;&#25200;&#35821;&#35328;&#29305;&#23450;&#20998;&#31867;&#22120;&#30340;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#35821;&#35328;&#24046;&#36317;&#65292;&#20351;&#29992;&#28304;&#26631;&#31614;&#35757;&#32451;&#30340;&#35821;&#35328;&#29305;&#23450;&#20998;&#31867;&#22120;&#26080;&#27861;&#20934;&#30830;&#39044;&#27979;&#30446;&#26631;&#35821;&#35328;&#12290;&#36825;&#20004;&#20010;&#20107;&#23454;&#25439;&#23475;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#32806;&#33258;&#36866;&#24212;&#32593;&#32476; (DA-Net)&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04158v1 Announce Type: cross  Abstract: Multi-Source cross-lingual transfer learning deals with the transfer of task knowledge from multiple labelled source languages to an unlabeled target language under the language shift. Existing methods typically focus on weighting the predictions produced by language-specific classifiers of different sources that follow a shared encoder. However, all source languages share the same encoder, which is updated by all these languages. The extracted representations inevitably contain different source languages' information, which may disturb the learning of the language-specific classifiers. Additionally, due to the language gap, language-specific classifiers trained with source labels are unable to make accurate predictions for the target language. Both facts impair the model's performance. To address these challenges, we propose a Disentangled and Adaptive Network (DA-Net). Firstly, we devise a feedback-guided collaborative disentanglemen
&lt;/p&gt;</description></item><item><title>Chatbot Arena&#26159;&#19968;&#20010;&#24320;&#25918;&#24179;&#21488;&#65292;&#37319;&#29992;&#20004;&#20004;&#27604;&#36739;&#30340;&#26041;&#24335;&#36890;&#36807;&#20247;&#21253;&#21033;&#29992;&#20154;&#31867;&#20559;&#22909;&#35780;&#20272;LLMs&#12290;&#30740;&#31350;&#34920;&#26126;&#20247;&#21253;&#38382;&#39064;&#22810;&#26679;&#19988;&#20855;&#26377;&#21306;&#20998;&#24615;&#65292;&#24182;&#19988;&#20154;&#31867;&#25237;&#31080;&#19982;&#19987;&#23478;&#35780;&#32423;&#32773;&#30340;&#25237;&#31080;&#22522;&#26412;&#19968;&#33268;&#12290;</title><link>https://arxiv.org/abs/2403.04132</link><description>&lt;p&gt;
Chatbot Arena&#65306;&#19968;&#20010;&#36890;&#36807;&#20154;&#31867;&#20559;&#22909;&#35780;&#20272;LLM&#30340;&#24320;&#25918;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04132
&lt;/p&gt;
&lt;p&gt;
Chatbot Arena&#26159;&#19968;&#20010;&#24320;&#25918;&#24179;&#21488;&#65292;&#37319;&#29992;&#20004;&#20004;&#27604;&#36739;&#30340;&#26041;&#24335;&#36890;&#36807;&#20247;&#21253;&#21033;&#29992;&#20154;&#31867;&#20559;&#22909;&#35780;&#20272;LLMs&#12290;&#30740;&#31350;&#34920;&#26126;&#20247;&#21253;&#38382;&#39064;&#22810;&#26679;&#19988;&#20855;&#26377;&#21306;&#20998;&#24615;&#65292;&#24182;&#19988;&#20154;&#31867;&#25237;&#31080;&#19982;&#19987;&#23478;&#35780;&#32423;&#32773;&#30340;&#25237;&#31080;&#22522;&#26412;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35299;&#38145;&#20102;&#26032;&#30340;&#33021;&#21147;&#21644;&#24212;&#29992;&#65307;&#28982;&#32780;&#65292;&#35780;&#20272;&#20854;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#19968;&#33268;&#24615;&#20173;&#28982;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Chatbot Arena&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#20154;&#31867;&#20559;&#22909;&#35780;&#20272;LLMs&#30340;&#24320;&#25918;&#24179;&#21488;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#19968;&#31181;&#20004;&#20004;&#27604;&#36739;&#30340;&#26041;&#24335;&#65292;&#24182;&#36890;&#36807;&#20247;&#21253;&#21033;&#29992;&#26469;&#33258;&#19981;&#21516;&#29992;&#25143;&#32676;&#20307;&#30340;&#36755;&#20837;&#12290;&#35813;&#24179;&#21488;&#24050;&#32463;&#36816;&#33829;&#20102;&#20960;&#20010;&#26376;&#65292;&#33719;&#24471;&#20102;&#36229;&#36807;24&#19975;&#20010;&#25237;&#31080;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#35813;&#24179;&#21488;&#65292;&#20998;&#26512;&#20102;&#25105;&#20204;&#36804;&#20170;&#25910;&#38598;&#30340;&#25968;&#25454;&#65292;&#24182;&#35299;&#37322;&#20102;&#25105;&#20204;&#27491;&#22312;&#20351;&#29992;&#30340;&#32463;&#36807;&#39564;&#35777;&#30340;&#32479;&#35745;&#26041;&#27861;&#65292;&#20197;&#20415;&#23545;&#27169;&#22411;&#36827;&#34892;&#39640;&#25928;&#20934;&#30830;&#30340;&#35780;&#20272;&#21644;&#25490;&#21517;&#12290;&#25105;&#20204;&#30830;&#35748;&#20247;&#21253;&#38382;&#39064;&#36275;&#22815;&#22810;&#26679;&#21270;&#21644;&#21306;&#20998;&#21270;&#65292;&#24182;&#19988;&#20247;&#21253;&#20154;&#31867;&#25237;&#31080;&#19982;&#19987;&#23478;&#35780;&#32423;&#32773;&#30340;&#25237;&#31080;&#22522;&#26412;&#19968;&#33268;&#12290;&#36825;&#20123;&#20998;&#26512;&#20849;&#21516;&#20026;&#24179;&#21488;&#30340;&#21487;&#20449;&#24230;&#22880;&#23450;&#20102;&#22362;&#23454;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04132v1 Announce Type: new  Abstract: Large Language Models (LLMs) have unlocked new capabilities and applications; however, evaluating the alignment with human preferences still poses significant challenges. To address this issue, we introduce Chatbot Arena, an open platform for evaluating LLMs based on human preferences. Our methodology employs a pairwise comparison approach and leverages input from a diverse user base through crowdsourcing. The platform has been operational for several months, amassing over 240K votes. This paper describes the platform, analyzes the data we have collected so far, and explains the tried-and-true statistical methods we are using for efficient and accurate evaluation and ranking of models. We confirm that the crowdsourced questions are sufficiently diverse and discriminating and that the crowdsourced human votes are in good agreement with those of expert raters. These analyses collectively establish a robust foundation for the credibility of
&lt;/p&gt;</description></item><item><title>&#25506;&#32034;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#29992;&#20110;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#65292;&#20197;&#35299;&#20915;&#33258;&#21160;&#21270;RCA&#20013;&#26080;&#27861;&#21160;&#24577;&#25910;&#38598;&#39069;&#22806;&#35786;&#26029;&#20449;&#24687;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.04123</link><description>&lt;p&gt;
&#25506;&#32034;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#29992;&#20110;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Exploring LLM-based Agents for Root Cause Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04123
&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#29992;&#20110;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#65292;&#20197;&#35299;&#20915;&#33258;&#21160;&#21270;RCA&#20013;&#26080;&#27861;&#21160;&#24577;&#25910;&#38598;&#39069;&#22806;&#35786;&#26029;&#20449;&#24687;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20113;&#36719;&#20214;&#31995;&#32479;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#23548;&#33268;&#20107;&#20214;&#31649;&#29702;&#24050;&#25104;&#20026;&#36719;&#20214;&#24320;&#21457;&#29983;&#21629;&#21608;&#26399;&#30340;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#65288;RCA&#65289;&#26159;&#20107;&#20214;&#31649;&#29702;&#36807;&#31243;&#30340;&#20851;&#38190;&#37096;&#20998;&#65292;&#23545;&#20540;&#29677;&#24037;&#31243;&#24072;&#26469;&#35828;&#26159;&#19968;&#39033;&#20005;&#23803;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#28145;&#20837;&#30340;&#39046;&#22495;&#30693;&#35782;&#21644;&#23545;&#22242;&#38431;&#29305;&#23450;&#26381;&#21153;&#30340;&#24191;&#27867;&#32463;&#39564;&#12290;&#33258;&#21160;&#21270;RCA&#21487;&#20197;&#26174;&#33879;&#33410;&#30465;&#26102;&#38388;&#65292;&#24182;&#20943;&#36731;&#20540;&#29677;&#24037;&#31243;&#24072;&#22312;&#20107;&#20214;&#31649;&#29702;&#19978;&#30340;&#36127;&#25285;&#12290;&#26368;&#36817;&#65292;&#30740;&#31350;&#20154;&#21592;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25191;&#34892;RCA&#65292;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#21160;&#24577;&#25910;&#38598;&#39069;&#22806;&#30340;&#35786;&#26029;&#20449;&#24687;&#65292;&#22914;&#19982;&#20107;&#20214;&#30456;&#20851;&#30340;&#26085;&#24535;&#12289;&#25351;&#26631;&#25110;&#25968;&#25454;&#24211;&#65292;&#20005;&#37325;&#38480;&#21046;&#20102;&#23427;&#20204;&#35786;&#26029;&#26681;&#26412;&#21407;&#22240;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;LLM&#20195;&#29702;&#29992;&#20110;RCA&#20197;&#35299;&#20915;&#27492;&#38480;&#21046;&#30340;&#20351;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#39033;&#24443;&#24213;&#30340;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04123v1 Announce Type: cross  Abstract: The growing complexity of cloud based software systems has resulted in incident management becoming an integral part of the software development lifecycle. Root cause analysis (RCA), a critical part of the incident management process, is a demanding task for on-call engineers, requiring deep domain knowledge and extensive experience with a team's specific services. Automation of RCA can result in significant savings of time, and ease the burden of incident management on on-call engineers. Recently, researchers have utilized Large Language Models (LLMs) to perform RCA, and have demonstrated promising results. However, these approaches are not able to dynamically collect additional diagnostic information such as incident related logs, metrics or databases, severely restricting their ability to diagnose root causes. In this work, we explore the use of LLM based agents for RCA to address this limitation. We present a thorough empirical eva
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32570;&#20047;&#33258;&#25105;&#25209;&#35780;&#33021;&#21147;&#65292;&#26080;&#27861;&#20687;&#20154;&#31867;&#19968;&#26679;&#32416;&#27491;&#38169;&#35823;&#12290;</title><link>https://arxiv.org/abs/2403.04121</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#25512;&#29702;&#21644;&#35268;&#21010;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Reason and Plan?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04121
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32570;&#20047;&#33258;&#25105;&#25209;&#35780;&#33021;&#21147;&#65292;&#26080;&#27861;&#20687;&#20154;&#31867;&#19968;&#26679;&#32416;&#27491;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#20154;&#31867;&#26377;&#26102;&#20505;&#34920;&#29616;&#20986;&#33021;&#22815;&#36890;&#36807;&#33258;&#25105;&#25209;&#35780;&#32416;&#27491;&#33258;&#24049;&#38169;&#35823;&#29468;&#27979;&#30340;&#33021;&#21147;&#65292;&#20294;&#20284;&#20046;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#27809;&#26377;&#20381;&#25454;&#25903;&#25345;&#36825;&#19968;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04121v1 Announce Type: new  Abstract: While humans sometimes do show the capability of correcting their own erroneous guesses with self-critiquing, there seems to be no basis for that assumption in the case of LLMs.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20174;&#20027;&#35266;&#26631;&#27880;&#23398;&#20064;&#26102;&#65292;&#27169;&#22411;&#23545;&#39640;&#20998;&#27495;&#25968;&#25454;&#23454;&#20363;&#34920;&#29616;&#20302;&#32622;&#20449;&#24230;&#30340;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#22810;&#22320;&#38754;&#23454;&#20917;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#20197;&#25552;&#39640;&#23545;&#36825;&#20123;&#23454;&#20363;&#30340;&#32622;&#20449;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.04085</link><description>&lt;p&gt;
&#19981;&#35201;&#36131;&#24618;&#25968;&#25454;&#65292;&#32780;&#35201;&#36131;&#24618;&#27169;&#22411;&#65306;&#29702;&#35299;&#20174;&#20027;&#35266;&#26631;&#27880;&#23398;&#20064;&#26102;&#30340;&#22122;&#38899;&#21644;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Don't Blame the Data, Blame the Model: Understanding Noise and Bias When Learning from Subjective Annotations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04085
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20174;&#20027;&#35266;&#26631;&#27880;&#23398;&#20064;&#26102;&#65292;&#27169;&#22411;&#23545;&#39640;&#20998;&#27495;&#25968;&#25454;&#23454;&#20363;&#34920;&#29616;&#20302;&#32622;&#20449;&#24230;&#30340;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#22810;&#22320;&#38754;&#23454;&#20917;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#20197;&#25552;&#39640;&#23545;&#36825;&#20123;&#23454;&#20363;&#30340;&#32622;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#25552;&#39640;&#20102;&#23545;&#22312;&#33258;&#28982;&#21547;&#26377;&#20154;&#31867;&#27880;&#37322;&#32773;&#20043;&#38388;&#20998;&#27495;&#30340;&#20027;&#35266;&#20219;&#21153;&#20013;&#32858;&#21512;&#26631;&#31614;&#30340;&#20260;&#23475;&#30340;&#35748;&#35782;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#20165;&#25552;&#20379;&#32858;&#21512;&#26631;&#31614;&#30340;&#27169;&#22411;&#22312;&#39640;&#20998;&#27495;&#25968;&#25454;&#23454;&#20363;&#19978;&#34920;&#29616;&#20986;&#20302;&#32622;&#20449;&#24230;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#30740;&#31350;&#23558;&#36825;&#31181;&#24773;&#20917;&#35270;&#20026;&#38169;&#35823;&#26631;&#35760;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#39640;&#20998;&#27495;&#25991;&#26412;&#23454;&#20363;&#38590;&#20197;&#23398;&#20064;&#30340;&#21407;&#22240;&#26159;&#20256;&#32479;&#30340;&#32858;&#21512;&#27169;&#22411;&#22312;&#20174;&#20027;&#35266;&#20219;&#21153;&#20013;&#25552;&#21462;&#26377;&#29992;&#20449;&#21495;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#21463;&#26368;&#36817;&#30740;&#31350;&#34920;&#26126;&#20174;&#21407;&#22987;&#27880;&#37322;&#20013;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35843;&#26597;&#20351;&#29992;&#22810;&#20010;&#22320;&#38754;&#23454;&#20917;&#65288;Multi-GT&#65289;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#23545;&#39640;&#20998;&#27495;&#23454;&#20363;&#30340;&#32622;&#20449;&#24230;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04085v1 Announce Type: new  Abstract: Researchers have raised awareness about the harms of aggregating labels especially in subjective tasks that naturally contain disagreements among human annotators. In this work we show that models that are only provided aggregated labels show low confidence on high-disagreement data instances. While previous studies consider such instances as mislabeled, we argue that the reason the high-disagreement text instances have been hard-to-learn is that the conventional aggregated models underperform in extracting useful signals from subjective tasks. Inspired by recent studies demonstrating the effectiveness of learning from raw annotations, we investigate classifying using Multiple Ground Truth (Multi-GT) approaches. Our experiments show an improvement of confidence for the high-disagreement instances.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#25195;&#25551;&#25991;&#26723;&#24418;&#24335;&#29702;&#35299;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#35821;&#35328;&#27169;&#22411;&#21644;Transformers&#22312;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#24615;&#20219;&#21153;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#23637;&#31034;&#20102;Transformers&#22914;&#20309;&#25512;&#21160;&#39046;&#22495;&#21457;&#23637;&#24182;&#38761;&#26032;&#24418;&#24335;&#29702;&#35299;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2403.04080</link><description>&lt;p&gt;
Transformers&#21644;&#35821;&#35328;&#27169;&#22411;&#22312;&#24418;&#24335;&#29702;&#35299;&#20013;&#30340;&#24212;&#29992;&#65306;&#25195;&#25551;&#25991;&#26723;&#20998;&#26512;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Transformers and Language Models in Form Understanding: A Comprehensive Review of Scanned Document Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04080
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#25195;&#25551;&#25991;&#26723;&#24418;&#24335;&#29702;&#35299;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#35821;&#35328;&#27169;&#22411;&#21644;Transformers&#22312;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#24615;&#20219;&#21153;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#23637;&#31034;&#20102;Transformers&#22914;&#20309;&#25512;&#21160;&#39046;&#22495;&#21457;&#23637;&#24182;&#38761;&#26032;&#24418;&#24335;&#29702;&#35299;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#35843;&#30740;&#20102;&#20851;&#20110;&#25195;&#25551;&#25991;&#26723;&#24418;&#24335;&#29702;&#35299;&#39046;&#22495;&#30340;&#30740;&#31350;&#24037;&#20316;&#65292;&#28145;&#20837;&#25506;&#35752;&#20102;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#19982;&#31361;&#30772;&#65292;&#31361;&#20986;&#20102;&#35821;&#35328;&#27169;&#22411;&#21644;Transformers&#22312;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#24615;&#20219;&#21153;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#30740;&#31350;&#26041;&#27861;&#28041;&#21450;&#23545;&#19978;&#20010;&#21313;&#24180;&#25991;&#26723;&#21644;&#24418;&#24335;&#29702;&#35299;&#36235;&#21183;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#39046;&#22495;&#28436;&#21464;&#35265;&#35299;&#12290;&#37325;&#28857;&#20171;&#32461;&#20102;&#21069;&#27839;&#27169;&#22411;&#22914;&#20309;&#25512;&#21160;&#35813;&#39046;&#22495;&#65292;&#24432;&#26174;&#20102;Transformers&#22914;&#20309;&#38761;&#26032;&#20102;&#24418;&#24335;&#29702;&#35299;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#25506;&#32034;&#21253;&#25324;&#23545;&#35774;&#35745;&#26377;&#25928;&#24212;&#23545;&#22024;&#26434;&#25195;&#25551;&#25991;&#26723;&#22797;&#26434;&#24615;&#30340;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#24191;&#27867;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#26368;&#26032;&#21644;&#26368;&#30456;&#20851;&#30340;&#25968;&#25454;&#38598;&#27010;&#36848;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#23545;&#35780;&#20272;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04080v1 Announce Type: new  Abstract: This paper presents a comprehensive survey of research works on the topic of form understanding in the context of scanned documents. We delve into recent advancements and breakthroughs in the field, highlighting the significance of language models and transformers in solving this challenging task. Our research methodology involves an in-depth analysis of popular documents and forms of understanding of trends over the last decade, enabling us to offer valuable insights into the evolution of this domain. Focusing on cutting-edge models, we showcase how transformers have propelled the field forward, revolutionizing form-understanding techniques. Our exploration includes an extensive examination of state-of-the-art language models designed to effectively tackle the complexities of noisy scanned documents. Furthermore, we present an overview of the latest and most relevant datasets, which serve as essential benchmarks for evaluating the perfo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#36136;&#37327;&#20266;&#26631;&#31614;&#36873;&#25321;&#26041;&#27861; SiCF&#65292;&#36890;&#36807;&#35813;&#26041;&#27861;&#36873;&#25321;&#20855;&#26377;&#39640;&#36136;&#37327;&#29983;&#25104;&#25688;&#35201;&#30340;&#26080;&#26631;&#31614;&#23545;&#35805;&#26469;&#36827;&#34892;&#21322;&#30417;&#30563;&#23545;&#35805;&#25277;&#21462;&#24335;&#25688;&#35201;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2403.04073</link><description>&lt;p&gt;
&#36890;&#36807;&#39640;&#36136;&#37327;&#20266;&#26631;&#31614;&#36873;&#25321;&#30340;&#21322;&#30417;&#30563;&#23545;&#35805;&#25277;&#21462;&#24335;&#25688;&#35201;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Dialogue Abstractive Summarization via High-Quality Pseudolabel Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04073
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#36136;&#37327;&#20266;&#26631;&#31614;&#36873;&#25321;&#26041;&#27861; SiCF&#65292;&#36890;&#36807;&#35813;&#26041;&#27861;&#36873;&#25321;&#20855;&#26377;&#39640;&#36136;&#37327;&#29983;&#25104;&#25688;&#35201;&#30340;&#26080;&#26631;&#31614;&#23545;&#35805;&#26469;&#36827;&#34892;&#21322;&#30417;&#30563;&#23545;&#35805;&#25277;&#21462;&#24335;&#25688;&#35201;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#23545;&#35805;&#25688;&#35201;&#29983;&#25104;&#65288;SSDS&#65289;&#21033;&#29992;&#27169;&#22411;&#29983;&#25104;&#30340;&#25688;&#35201;&#20943;&#23569;&#23545;&#20154;&#24037;&#26631;&#35760;&#25968;&#25454;&#30340;&#20381;&#36182;&#65292;&#25552;&#39640;&#25688;&#35201;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35780;&#20998;&#26041;&#27861; SiCF&#65292;&#21253;&#21547;&#20102;&#25688;&#35201;&#27169;&#22411;&#36136;&#37327;&#30340;&#19977;&#20010;&#20027;&#35201;&#32500;&#24230;&#65306;&#35821;&#20041;&#19981;&#21464;&#24615;&#65288;&#27169;&#22411;&#20449;&#24515;&#30340;&#25351;&#31034;&#65289;&#12289;&#35206;&#30422;&#24230;&#65288;&#20107;&#23454;&#21484;&#22238;&#65289;&#21644;&#24544;&#23454;&#24230;&#65288;&#20107;&#23454;&#31934;&#24230;&#65289;&#12290;&#21033;&#29992; SiCF &#20998;&#25968;&#65292;&#36873;&#25321;&#20855;&#26377;&#39640;&#36136;&#37327;&#29983;&#25104;&#25688;&#35201;&#30340;&#26080;&#26631;&#31614;&#23545;&#35805;&#26469;&#35757;&#32451;&#25688;&#35201;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04073v1 Announce Type: cross  Abstract: Semi-supervised dialogue summarization (SSDS) leverages model-generated summaries to reduce reliance on human-labeled data and improve the performance of summarization models. While addressing label noise, previous works on semi-supervised learning primarily focus on natural language understanding tasks, assuming each sample has a unique label. However, these methods are not directly applicable to SSDS, as it is a generative task, and each dialogue can be summarized in different ways. In this work, we propose a novel scoring approach, SiCF, which encapsulates three primary dimensions of summarization model quality: Semantic invariance (indicative of model confidence), Coverage (factual recall), and Faithfulness (factual precision). Using the SiCF score, we select unlabeled dialogues with high-quality generated summaries to train summarization models. Comprehensive experiments on three public datasets demonstrate the effectiveness of Si
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20307;&#32946;&#39046;&#22495;&#30340;&#20998;&#26512;&#25512;&#29702;&#33021;&#21147;&#65292;&#21457;&#29616;&#22312;&#22788;&#29702;NBA&#21644;NFL&#27604;&#36187;&#24471;&#20998;&#20219;&#21153;&#26102;&#65292;GPT-4&#21644;Claude-2.1&#34920;&#29616;&#26368;&#20339;&#65292;&#37319;&#29992;&#20998;&#27835;&#27861;&#36827;&#34892;&#25968;&#25454;&#22788;&#29702;&#25928;&#26524;&#26368;&#22909;&#12290;</title><link>https://arxiv.org/abs/2403.04031</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#36827;&#34892;&#20998;&#26512;&#25512;&#29702;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models do Analytical Reasoning?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20307;&#32946;&#39046;&#22495;&#30340;&#20998;&#26512;&#25512;&#29702;&#33021;&#21147;&#65292;&#21457;&#29616;&#22312;&#22788;&#29702;NBA&#21644;NFL&#27604;&#36187;&#24471;&#20998;&#20219;&#21153;&#26102;&#65292;GPT-4&#21644;Claude-2.1&#34920;&#29616;&#26368;&#20339;&#65292;&#37319;&#29992;&#20998;&#27835;&#27861;&#36827;&#34892;&#25968;&#25454;&#22788;&#29702;&#25928;&#26524;&#26368;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#24212;&#29992;&#20110;&#20307;&#32946;&#39046;&#22495;&#30340;&#21069;&#27839;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20998;&#26512;&#25512;&#29702;&#12290;&#25105;&#20204;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35745;&#31639;NBA&#21644;NFL&#27604;&#36187;&#20013;&#27599;&#25903;&#29699;&#38431;&#22312;&#19968;&#20010;&#23395;&#24230;&#20013;&#24471;&#20998;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21457;&#29616;&#26377;&#20004;&#20010;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#25105;&#20204;&#20351;&#29992;&#30340;&#25152;&#26377;&#27169;&#22411;&#20013;&#65292;GPT-4&#22312;&#25928;&#26524;&#19978;&#34920;&#29616;&#26368;&#20026;&#31361;&#20986;&#65292;&#20854;&#27425;&#26159;Claude-2.1&#65292;&#32780;GPT-3.5&#12289;Gemini-Pro&#21644;Llama-2-70b&#25928;&#26524;&#31245;&#36874;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#25552;&#31034;&#25216;&#26415;&#21644;&#20998;&#27835;&#27861;&#65292;&#21457;&#29616;&#21518;&#32773;&#25928;&#26524;&#26368;&#22909;&#12290;&#25105;&#20204;&#30340;&#20998;&#27835;&#27861;&#23558;&#36880;&#27493;&#25968;&#25454;&#32454;&#20998;&#20026;&#26356;&#23567;&#12289;&#26356;&#26131;&#22788;&#29702;&#30340;&#29255;&#27573;&#65292;&#20998;&#21035;&#35299;&#20915;&#27599;&#20010;&#29255;&#27573;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#21512;&#24182;&#22312;&#19968;&#36215;&#12290;&#38500;&#20102;&#20998;&#27835;&#27861;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#19968;&#31181;&#21517;&#20026;Chain of Thought&#65288;CoT&#65289;&#31574;&#30053;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#26576;&#20123;&#27169;&#22411;&#30340;&#32467;&#26524;&#65292;&#23588;&#20854;&#26159;GPT-4&#21644;Claude-2.1&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04031v1 Announce Type: cross  Abstract: This paper explores the cutting-edge Large Language Model with analytical reasoning on sports. Our analytical reasoning embodies the tasks of letting large language models count how many points each team scores in a quarter in the NBA and NFL games. Our major discoveries are in two folds. Firstly, we find among all the models we employed, GPT-4 stands out in effectiveness, followed by Claude-2.1, with GPT-3.5, Gemini-Pro, and Llama-2-70b lagging behind. Specifically, we compare three different prompting techniques and a divide-and-conquer approach, we find that the latter was the most effective. Our divide-and-conquer approach breaks down play-by-play data into smaller, more manageable segments, solves each piece individually, and then aggregates them together. Besides the divide-and-conquer approach, we also explore the Chain of Thought (CoT) strategy, which markedly improves outcomes for certain models, notably GPT-4 and Claude-2.1, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#25919;&#27835;&#20559;&#35265;&#26032;&#38395;&#23545;&#30123;&#33495;&#24577;&#24230;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#21644;&#22240;&#26524;&#25512;&#26029;&#25216;&#26415;&#65292;&#25581;&#31034;&#20102;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#22312;&#30123;&#33495;&#31435;&#22330;&#19978;&#30340;&#19981;&#21516;&#34892;&#20026;&#65292;&#24182;&#21457;&#29616;&#20013;&#31435;&#31435;&#22330;&#30340;&#20010;&#20307;&#26356;&#23481;&#26131;&#21463;&#21040;&#25919;&#27835;&#20559;&#35265;&#26032;&#38395;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.04009</link><description>&lt;p&gt;
&#23186;&#20307;&#20559;&#35265;&#30340;&#37325;&#35201;&#24615;&#65306;&#29702;&#35299;&#25919;&#27835;&#20559;&#35265;&#26032;&#38395;&#23545;&#31038;&#20132;&#23186;&#20307;&#19978;&#30123;&#33495;&#24577;&#24230;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Media Bias Matters: Understanding the Impact of Politically Biased News on Vaccine Attitudes in Social Media
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04009
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#25919;&#27835;&#20559;&#35265;&#26032;&#38395;&#23545;&#30123;&#33495;&#24577;&#24230;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#21644;&#22240;&#26524;&#25512;&#26029;&#25216;&#26415;&#65292;&#25581;&#31034;&#20102;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#22312;&#30123;&#33495;&#31435;&#22330;&#19978;&#30340;&#19981;&#21516;&#34892;&#20026;&#65292;&#24182;&#21457;&#29616;&#20013;&#31435;&#31435;&#22330;&#30340;&#20010;&#20307;&#26356;&#23481;&#26131;&#21463;&#21040;&#25919;&#27835;&#20559;&#35265;&#26032;&#38395;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#38395;&#23186;&#20307;&#34987;&#21033;&#29992;&#20316;&#20026;&#25919;&#27835;&#24037;&#20855;&#20559;&#31163;&#20107;&#23454;&#65292;&#26410;&#32463;&#35777;&#25454;&#25903;&#25345;&#22320;&#25552;&#20986;&#20559;&#35265;&#24615;&#35266;&#28857;&#12290;&#22312;&#26032;&#20896;&#30123;&#24773;&#26399;&#38388;&#65292;&#25919;&#27835;&#20559;&#35265;&#26032;&#38395;&#65288;PBN&#65289;&#26174;&#33879;&#21066;&#24369;&#20102;&#20844;&#20247;&#23545;&#30123;&#33495;&#30340;&#20449;&#20219;&#65292;&#23613;&#31649;&#26377;&#24378;&#26377;&#21147;&#30340;&#21307;&#23398;&#35777;&#25454;&#25903;&#25345;&#20854;&#26377;&#25928;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#65306;&#65288;i&#65289;&#20869;&#22312;&#30123;&#33495;&#31435;&#22330;&#22914;&#20309;&#24494;&#22937;&#22320;&#24433;&#21709;&#20010;&#20154;&#36873;&#25321;&#26032;&#38395;&#26469;&#28304;&#21644;&#21442;&#19982;&#31038;&#20132;&#23186;&#20307;&#35752;&#35770;&#65307;&#20197;&#21450;&#65288;ii&#65289;&#26292;&#38706;&#20110;PBN&#23545;&#29992;&#25143;&#23545;&#30123;&#33495;&#24577;&#24230;&#30340;&#24433;&#21709;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#25972;&#29702;&#20102;&#19968;&#20010;&#23558;PBN&#19982;&#30456;&#20851;&#31038;&#20132;&#23186;&#20307;&#35805;&#35821;&#36830;&#25509;&#36215;&#26469;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#12290;&#21033;&#29992;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#21644;&#22240;&#26524;&#25512;&#26029;&#25216;&#26415;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#20855;&#26377;&#19981;&#21516;&#30123;&#33495;&#31435;&#22330;&#30340;&#31038;&#20132;&#23186;&#20307;&#32676;&#20307;&#20043;&#38388;&#30340;&#26126;&#26174;&#29992;&#25143;&#34892;&#20026;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#20855;&#26377;&#20013;&#31435;&#31435;&#22330;&#30340;&#20010;&#20307;&#65292;&#23588;&#20854;&#26159;&#29369;&#35947;&#25509;&#31181;&#30123;&#33495;&#30340;&#22810;&#25968;&#27966;&#65292;&#23545;PBN&#30340;&#24433;&#21709;&#26356;&#21152;&#33030;&#24369;&#65292;&#30456;&#27604;&#20043;&#19979;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04009v1 Announce Type: cross  Abstract: News media has been utilized as a political tool to stray from facts, presenting biased claims without evidence. Amid the COVID-19 pandemic, politically biased news (PBN) has significantly undermined public trust in vaccines, despite strong medical evidence supporting their efficacy. In this paper, we analyze: (i) how inherent vaccine stances subtly influence individuals' selection of news sources and participation in social media discussions; and (ii) the impact of exposure to PBN on users' attitudes toward vaccines. In doing so, we first curate a comprehensive dataset that connects PBN with related social media discourse. Utilizing advanced deep learning and causal inference techniques, we reveal distinct user behaviors between social media groups with various vaccine stances. Moreover, we observe that individuals with moderate stances, particularly the vaccine-hesitant majority, are more vulnerable to the influence of PBN compared t
&lt;/p&gt;</description></item><item><title>SaulLM-7B&#26159;&#39318;&#20010;&#19987;&#20026;&#27861;&#24459;&#25991;&#26412;&#35774;&#35745;&#30340;7B&#21442;&#25968;LLM&#65292;&#36890;&#36807;InnovativeFine-tuning&#26041;&#27861;&#65292;&#23637;&#29616;&#20102;&#39046;&#20808;&#30340;&#27861;&#24459;&#25991;&#20214;&#29702;&#35299;&#21644;&#22788;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.03883</link><description>&lt;p&gt;
SaulLM-7B: &#38754;&#21521;&#27861;&#24459;&#39046;&#22495;&#30340;&#24320;&#21019;&#24615;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SaulLM-7B: A pioneering Large Language Model for Law
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03883
&lt;/p&gt;
&lt;p&gt;
SaulLM-7B&#26159;&#39318;&#20010;&#19987;&#20026;&#27861;&#24459;&#25991;&#26412;&#35774;&#35745;&#30340;7B&#21442;&#25968;LLM&#65292;&#36890;&#36807;InnovativeFine-tuning&#26041;&#27861;&#65292;&#23637;&#29616;&#20102;&#39046;&#20808;&#30340;&#27861;&#24459;&#25991;&#20214;&#29702;&#35299;&#21644;&#22788;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SaulLM-7B&#65292;&#19968;&#20010;&#19987;&#20026;&#27861;&#24459;&#39046;&#22495;&#37327;&#36523;&#23450;&#21046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290;&#25317;&#26377;70&#20159;&#21442;&#25968;&#30340;SaulLM-7B&#26159;&#31532;&#19968;&#20010;&#19987;&#20026;&#27861;&#24459;&#25991;&#26412;&#29702;&#35299;&#21644;&#29983;&#25104;&#32780;&#35774;&#35745;&#30340;LLM&#12290;&#20197;Mistral 7B&#26550;&#26500;&#20026;&#22522;&#30784;&#65292;SaulLM-7B&#22312;&#36229;&#36807;300&#20159;&#26631;&#35760;&#30340;&#33521;&#25991;&#27861;&#24459;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;SaulLM-7B&#22312;&#29702;&#35299;&#21644;&#22788;&#29702;&#27861;&#24459;&#25991;&#20214;&#26041;&#38754;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25351;&#23548;&#24494;&#35843;&#26041;&#27861;&#65292;&#21033;&#29992;&#27861;&#24459;&#25968;&#25454;&#38598;&#36827;&#19968;&#27493;&#22686;&#24378;SaulLM-7B&#22312;&#27861;&#24459;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;SaulLM-7B&#37319;&#29992;CC-BY-SA-4.0&#35768;&#21487;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03883v1 Announce Type: new  Abstract: In this paper, we introduce SaulLM-7B, a large language model (LLM) tailored for the legal domain. With 7 billion parameters, SaulLM-7B is the first LLM designed explicitly for legal text comprehension and generation. Leveraging the Mistral 7B architecture as its foundation, SaulLM-7B is trained on an English legal corpus of over 30 billion tokens. SaulLM-7B exhibits state-of-the-art proficiency in understanding and processing legal documents. Additionally, we present a novel instructional fine-tuning method that leverages legal datasets to further enhance SaulLM-7B's performance in legal tasks. SaulLM-7B is released under the CC-BY-SA-4.0 License.
&lt;/p&gt;</description></item><item><title>Emojinize &#26159;&#19968;&#31181;&#26041;&#27861;&#65292;&#33021;&#22815;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36873;&#25321;&#36866;&#24403;&#30340;&#34920;&#24773;&#31526;&#21495;&#32763;&#35793;&#25991;&#26412;&#65292;&#25552;&#39640;&#29468;&#27979;&#24615;&#65292;&#20154;&#31867;&#29468;&#27979;&#21487;&#36798;55&#65285;&#12290;</title><link>https://arxiv.org/abs/2403.03857</link><description>&lt;p&gt;
Emojinize&#65306;&#29992;&#34920;&#24773;&#31526;&#21495;&#20016;&#23500;&#20219;&#20309;&#25991;&#26412;&#30340;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Emojinize : Enriching Any Text with Emoji Translations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03857
&lt;/p&gt;
&lt;p&gt;
Emojinize &#26159;&#19968;&#31181;&#26041;&#27861;&#65292;&#33021;&#22815;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36873;&#25321;&#36866;&#24403;&#30340;&#34920;&#24773;&#31526;&#21495;&#32763;&#35793;&#25991;&#26412;&#65292;&#25552;&#39640;&#29468;&#27979;&#24615;&#65292;&#20154;&#31867;&#29468;&#27979;&#21487;&#36798;55&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Emoji&#24050;&#32463;&#25104;&#20026;&#20070;&#38754;&#20132;&#27969;&#20013;&#26080;&#22788;&#19981;&#22312;&#30340;&#23384;&#22312;&#65292;&#22312;&#32593;&#32476;&#19978;&#21644;&#26356;&#36828;&#30340;&#22320;&#26041;&#12290;&#23427;&#20204;&#21487;&#20197;&#24378;&#35843;&#25110;&#28548;&#28165;&#24773;&#32490;&#65292;&#20026;&#23545;&#35805;&#22686;&#28155;&#32454;&#33410;&#65292;&#25110;&#32773;&#31616;&#21333;&#22320;&#36215;&#35013;&#39280;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#38543;&#24847;&#20351;&#29992;&#20165;&#20165;&#26159;&#34920;&#24773;&#31526;&#21495;&#34920;&#36798;&#33021;&#21147;&#30340;&#20912;&#23665;&#19968;&#35282;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#37322;&#25918;&#36825;&#31181;&#21147;&#37327;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Emojinize&#65292;&#19968;&#31181;&#23558;&#20219;&#24847;&#25991;&#26412;&#30701;&#35821;&#32763;&#35793;&#25104;&#19968;&#20010;&#25110;&#22810;&#20010;&#34920;&#24773;&#31526;&#21495;&#24207;&#21015;&#30340;&#26041;&#27861;&#65292;&#32780;&#26080;&#38656;&#20154;&#31867;&#36755;&#20837;&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;Emojinize&#21487;&#20197;&#26681;&#25454;&#19978;&#19979;&#25991;&#65288;&#20363;&#22914;&#65292;&#26495;&#29699;-&#29699;&#26834;vs&#34649;&#34656;&#65289;&#28040;&#38500;&#27495;&#20041;&#22320;&#36873;&#25321;&#36866;&#24403;&#30340;&#34920;&#24773;&#31526;&#21495;&#65292;&#24182;&#36890;&#36807;&#32452;&#21512;&#22810;&#20010;&#34920;&#24773;&#31526;&#21495;&#65288;&#20363;&#22914;&#65292;&#8220;Emojinize&#8221;&#34987;&#32763;&#35793;&#25104;&#36755;&#20837;-&#25289;&#19969;&#25991;&#23383;&#27597;-&#21491;&#31661;&#22836;-&#31505;&#33080;&#65289;&#26469;&#32452;&#21512;&#34920;&#36798;&#22797;&#26434;&#27010;&#24565;&#12290;&#22312;&#22522;&#20110;&#22635;&#31354;&#27979;&#35797;&#30340;&#29992;&#25143;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#34920;&#26126;Emojinize&#30340;&#34920;&#24773;&#31526;&#21495;&#32763;&#35793;&#21487;&#20197;&#20351;&#25513;&#30422;&#30340;&#21333;&#35789;&#30340;&#29468;&#27979;&#24615;&#22686;&#21152;55&#65285;&#65292;&#32780;&#20154;&#31867;&#36873;&#25321;&#30340;&#34920;&#24773;&#31526;&#21495;&#32763;&#35793;&#21482;&#33021;&#22686;&#21152;29&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03857v1 Announce Type: new  Abstract: Emoji have become ubiquitous in written communication, on the Web and beyond. They can emphasize or clarify emotions, add details to conversations, or simply serve decorative purposes. This casual use, however, barely scratches the surface of the expressive power of emoji. To further unleash this power, we present Emojinize, a method for translating arbitrary text phrases into sequences of one or more emoji without requiring human input. By leveraging the power of large language models, Emojinize can choose appropriate emoji by disambiguating based on context (eg, cricket-bat vs bat) and can express complex concepts compositionally by combining multiple emoji (eq, ''Emojinize'' is translated to input-latin-letters right-arrow grinning-face). In a cloze test--based user study, we show that Emojinize's emoji translations increase the human guessability of masked words by 55%, whereas human-picked emoji translations do so by only 29%. These
&lt;/p&gt;</description></item><item><title>&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23618;&#32423;&#23384;&#22312;&#36739;&#39640;&#30456;&#20284;&#24615;&#65292;&#26377;&#20123;&#23618;&#23545;&#32593;&#32476;&#21151;&#33021;&#20960;&#20046;&#26080;&#24433;&#21709;&#12290;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#31216;&#20026;&#21306;&#22359;&#24433;&#21709;&#30340;&#24230;&#37327;&#65292;&#24182;&#36890;&#36807;&#23618;&#21024;&#38500;&#26041;&#27861;&#26174;&#33879;&#20248;&#20110;&#20197;&#24448;&#30340;&#27169;&#22411;&#20462;&#21098;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.03853</link><description>&lt;p&gt;
ShortGPT: &#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23618;&#32423;&#27604;&#24744;&#24819;&#35937;&#30340;&#26356;&#20887;&#20313;
&lt;/p&gt;
&lt;p&gt;
ShortGPT: Layers in Large Language Models are More Redundant Than You Expect
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03853
&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23618;&#32423;&#23384;&#22312;&#36739;&#39640;&#30456;&#20284;&#24615;&#65292;&#26377;&#20123;&#23618;&#23545;&#32593;&#32476;&#21151;&#33021;&#20960;&#20046;&#26080;&#24433;&#21709;&#12290;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#31216;&#20026;&#21306;&#22359;&#24433;&#21709;&#30340;&#24230;&#37327;&#65292;&#24182;&#36890;&#36807;&#23618;&#21024;&#38500;&#26041;&#27861;&#26174;&#33879;&#20248;&#20110;&#20197;&#24448;&#30340;&#27169;&#22411;&#20462;&#21098;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24615;&#33021;&#19978;&#19981;&#26029;&#21462;&#24471;&#36827;&#23637;&#65292;&#20854;&#35268;&#27169;&#26174;&#33879;&#22686;&#21152;&#65292;&#24403;&#21069;&#30340;LLMs&#21253;&#21547;&#25968;&#21313;&#20159;&#29978;&#33267;&#25968;&#19975;&#20159;&#20010;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#35768;&#22810;LLMs&#30340;&#23618;&#20043;&#38388;&#23384;&#22312;&#39640;&#24230;&#30456;&#20284;&#24615;&#65292;&#24182;&#19988;&#19968;&#20123;&#23618;&#22312;&#32593;&#32476;&#21151;&#33021;&#20013;&#36215;&#21040;&#20102;&#21487;&#24573;&#30053;&#30340;&#20316;&#29992;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#31181;&#31216;&#20026;&#21306;&#22359;&#24433;&#21709;&#65288;BI&#65289;&#30340;&#24230;&#37327;&#34913;&#37327;LLMs&#20013;&#27599;&#20010;&#23618;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#20462;&#21098;&#26041;&#27861;&#65306;&#23618;&#21024;&#38500;&#65292;&#21363;&#26681;&#25454;&#23427;&#20204;&#30340;BI&#24471;&#20998;&#30452;&#25509;&#21024;&#38500;LLMs&#20013;&#30340;&#20887;&#20313;&#23618;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;ShortGPT&#22312;&#27169;&#22411;&#20462;&#21098;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#20197;&#24448;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;ShortGPT&#19982;&#37327;&#21270;&#31561;&#26041;&#27861;&#27491;&#20132;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#20943;&#23569;&#21442;&#25968;&#21644;&#35745;&#31639;&#12290;&#36890;&#36807;&#31616;&#21333;&#30340;&#23618;&#21024;&#38500;&#21363;&#21487;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#30340;&#33021;&#21147;&#65292;&#19982;&#20256;&#32479;&#30340;&#31934;&#30830;&#20462;&#21098;&#26041;&#27861;&#25130;&#28982;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03853v1 Announce Type: new  Abstract: As Large Language Models (LLMs) continue to advance in performance, their size has escalated significantly, with current LLMs containing billions or even trillions of parameters. However, in this study, we discovered that many layers of LLMs exhibit high similarity, and some layers play a negligible role in network functionality. Based on this observation, we define a metric called Block Influence (BI) to gauge the significance of each layer in LLMs. We then propose a straightforward pruning approach: layer removal, in which we directly delete the redundant layers in LLMs based on their BI scores. Experiments demonstrate that our method, which we call ShortGPT, significantly outperforms previous state-of-the-art (SOTA) methods in model pruning. Moreover, ShortGPT is orthogonal to quantization-like methods, enabling further reduction in parameters and computation. The ability to achieve better results through simple layer removal, as oppo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#26041;&#27861;&#29992;&#20110;&#22810;&#27169;&#24577;&#30005;&#35270;&#33410;&#30446;&#25688;&#35201;&#65292;&#21253;&#25324;&#26816;&#27979;&#22330;&#26223;&#36793;&#30028;&#12289;&#37325;&#26032;&#25490;&#21015;&#22330;&#26223;&#12289;&#23558;&#35270;&#35273;&#20449;&#24687;&#36716;&#25442;&#20026;&#25991;&#26412;&#12289;&#24635;&#32467;&#23545;&#35805;&#20197;&#21450;&#23558;&#22330;&#26223;&#25688;&#35201;&#34701;&#21512;&#30340;&#36807;&#31243;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#34913;&#37327;&#25688;&#35201;&#36136;&#37327;&#30340;&#35780;&#20215;&#25351;&#26631;PREFS&#12290;</title><link>https://arxiv.org/abs/2403.03823</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24577;&#30005;&#35270;&#33410;&#30446;&#25688;&#35201;&#30340;&#27169;&#22359;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Modular Approach for Multimodal Summarization of TV Shows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03823
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#26041;&#27861;&#29992;&#20110;&#22810;&#27169;&#24577;&#30005;&#35270;&#33410;&#30446;&#25688;&#35201;&#65292;&#21253;&#25324;&#26816;&#27979;&#22330;&#26223;&#36793;&#30028;&#12289;&#37325;&#26032;&#25490;&#21015;&#22330;&#26223;&#12289;&#23558;&#35270;&#35273;&#20449;&#24687;&#36716;&#25442;&#20026;&#25991;&#26412;&#12289;&#24635;&#32467;&#23545;&#35805;&#20197;&#21450;&#23558;&#22330;&#26223;&#25688;&#35201;&#34701;&#21512;&#30340;&#36807;&#31243;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#34913;&#37327;&#25688;&#35201;&#36136;&#37327;&#30340;&#35780;&#20215;&#25351;&#26631;PREFS&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#30005;&#35270;&#33410;&#30446;&#25688;&#35201;&#30340;&#20219;&#21153;&#65292;&#28041;&#21450;&#21040;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#30340;&#20851;&#38190;&#39046;&#22495;&#65306;&#22797;&#26434;&#25512;&#29702;&#12289;&#22810;&#27169;&#24577;&#21644;&#38271;&#31687;&#21465;&#20107;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#26041;&#27861;&#65292;&#20854;&#20013;&#21508;&#20010;&#32452;&#20214;&#25191;&#34892;&#19987;&#38376;&#30340;&#23376;&#20219;&#21153;&#65292;&#25105;&#20204;&#35748;&#20026;&#19982;&#31471;&#21040;&#31471;&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#30340;&#27169;&#22359;&#28041;&#21450;&#26816;&#27979;&#22330;&#26223;&#36793;&#30028;&#65292;&#37325;&#26032;&#25490;&#21015;&#22330;&#26223;&#20197;&#23613;&#37327;&#20943;&#23569;&#19981;&#21516;&#20107;&#20214;&#20043;&#38388;&#30340;&#20999;&#25442;&#27425;&#25968;&#65292;&#23558;&#35270;&#35273;&#20449;&#24687;&#36716;&#25442;&#20026;&#25991;&#26412;&#65292;&#24635;&#32467;&#27599;&#20010;&#22330;&#26223;&#20013;&#30340;&#23545;&#35805;&#65292;&#24182;&#23558;&#22330;&#26223;&#25688;&#35201;&#34701;&#21512;&#25104;&#25972;&#38598;&#30340;&#26368;&#32456;&#25688;&#35201;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;PREFS&#65288;&#25688;&#35201;&#20107;&#23454;&#30340;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#35780;&#20272;&#65289;&#65292;&#29992;&#20110;&#34913;&#37327;&#29983;&#25104;&#25688;&#35201;&#30340;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#65292;&#25105;&#20204;&#23558;&#20854;&#20998;&#35299;&#20026;&#21407;&#23376;&#20107;&#23454;&#12290;&#22312;&#26368;&#36817;&#21457;&#24067;&#30340;SummScreen3D&#25968;&#25454;&#38598;Papalampidi&#21644;Lapata&#65288;2023&#65289;&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20135;&#29983;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03823v1 Announce Type: new  Abstract: In this paper we address the task of summarizing television shows, which touches key areas in AI research: complex reasoning, multiple modalities, and long narratives. We present a modular approach where separate components perform specialized sub-tasks which we argue affords greater flexibility compared to end-to-end methods. Our modules involve detecting scene boundaries, reordering scenes so as to minimize the number of cuts between different events, converting visual information to text, summarizing the dialogue in each scene, and fusing the scene summaries into a final summary for the entire episode. We also present a new metric, PREFS (\textbf{P}recision and \textbf{R}ecall \textbf{E}valuation of Summary \textbf{F}act\textbf{s}), to measure both precision and recall of generated summaries, which we decompose into atomic facts. Tested on the recently released SummScreen3D dataset Papalampidi and Lapata (2023), our method produces hi
&lt;/p&gt;</description></item><item><title>WMDP&#22522;&#20934;&#26159;&#19968;&#20010;&#20844;&#24320;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;4157&#20010;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65292;&#29992;&#20316;&#29983;&#29289;&#23433;&#20840;&#12289;&#32593;&#32476;&#23433;&#20840;&#21644;&#21270;&#23398;&#23433;&#20840;&#21361;&#38505;&#30693;&#35782;&#30340;&#20195;&#29702;&#27979;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.03218</link><description>&lt;p&gt;
WMDP&#22522;&#20934;&#65306;&#36890;&#36807;&#36951;&#24536;&#27979;&#37327;&#21644;&#20943;&#23569;&#24694;&#24847;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03218
&lt;/p&gt;
&lt;p&gt;
WMDP&#22522;&#20934;&#26159;&#19968;&#20010;&#20844;&#24320;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;4157&#20010;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65292;&#29992;&#20316;&#29983;&#29289;&#23433;&#20840;&#12289;&#32593;&#32476;&#23433;&#20840;&#21644;&#21270;&#23398;&#23433;&#20840;&#21361;&#38505;&#30693;&#35782;&#30340;&#20195;&#29702;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03218v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#39046;&#22495; &#25688;&#35201;&#65306;&#30333;&#23467;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#34892;&#25919;&#21629;&#20196;&#24378;&#35843;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36171;&#20104;&#24694;&#24847;&#34892;&#20026;&#32773;&#24320;&#21457;&#29983;&#29289;&#12289;&#32593;&#32476;&#21644;&#21270;&#23398;&#27494;&#22120;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#34913;&#37327;&#36825;&#20123;&#24694;&#24847;&#20351;&#29992;&#30340;&#39118;&#38505;&#65292;&#25919;&#24220;&#26426;&#26500;&#21644;&#20027;&#35201;&#20154;&#24037;&#26234;&#33021;&#23454;&#39564;&#23460;&#27491;&#22312;&#24320;&#21457;LLMs&#30340;&#21361;&#38505;&#33021;&#21147;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#35780;&#20272;&#26159;&#31169;&#20154;&#30340;&#65292;&#38459;&#30861;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#22914;&#20309;&#20943;&#23569;&#39118;&#38505;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#20165;&#19987;&#27880;&#20110;&#20960;&#26465;&#39640;&#24230;&#29305;&#23450;&#30340;&#24694;&#24847;&#20351;&#29992;&#36884;&#24452;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20123;&#31354;&#30333;&#65292;&#25105;&#20204;&#20844;&#24320;&#21457;&#24067;&#20102;&#22823;&#35268;&#27169;&#26432;&#20260;&#24615;&#27494;&#22120;&#20195;&#29702;&#65288;WMDP&#65289;&#22522;&#20934;&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;4157&#20010;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#20316;&#20026;&#29983;&#29289;&#23433;&#20840;&#12289;&#32593;&#32476;&#23433;&#20840;&#21644;&#21270;&#23398;&#23433;&#20840;&#21361;&#38505;&#30693;&#35782;&#30340;&#20195;&#29702;&#27979;&#37327;&#12290;WMDP&#30001;&#19968;&#32452;&#23398;&#26415;&#30028;&#21644;&#25216;&#26415;&#39038;&#38382;&#32852;&#21512;&#24320;&#21457;&#65292;&#24182;&#22312;&#20844;&#24320;&#21457;&#24067;&#21069;&#20005;&#26684;&#36807;&#28388;&#20197;&#28040;&#38500;&#25935;&#24863;&#20449;&#24687;&#12290;WMDP&#26377;&#20004;&#20010;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03218v1 Announce Type: cross  Abstract: The White House Executive Order on Artificial Intelligence highlights the risks of large language models (LLMs) empowering malicious actors in developing biological, cyber, and chemical weapons. To measure these risks of malicious use, government institutions and major AI labs are developing evaluations for hazardous capabilities in LLMs. However, current evaluations are private, preventing further research into mitigating risk. Furthermore, they focus on only a few, highly specific pathways for malicious use. To fill these gaps, we publicly release the Weapons of Mass Destruction Proxy (WMDP) benchmark, a dataset of 4,157 multiple-choice questions that serve as a proxy measurement of hazardous knowledge in biosecurity, cybersecurity, and chemical security. WMDP was developed by a consortium of academics and technical consultants, and was stringently filtered to eliminate sensitive information prior to public release. WMDP serves two r
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PARADISE&#30340;&#20219;&#21153;&#65292;&#36890;&#36807;&#23454;&#29992;&#31243;&#24207;&#25991;&#26412;&#36827;&#34892;&#28436;&#32462;&#25512;&#29702;&#65292;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20165;&#20174;&#32473;&#23450;&#30446;&#26631;&#25512;&#26029;&#35745;&#21010;&#30340;&#33021;&#21147;</title><link>https://arxiv.org/abs/2403.03167</link><description>&lt;p&gt;
PARADISE&#65306;&#36890;&#36807;&#36807;&#31243;&#35686;&#21578;&#21644;&#25552;&#31034;&#25968;&#25454;&#38598;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#24335;&#35268;&#21010;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
PARADISE: Evaluating Implicit Planning Skills of Language Models with Procedural Warnings and Tips Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03167
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PARADISE&#30340;&#20219;&#21153;&#65292;&#36890;&#36807;&#23454;&#29992;&#31243;&#24207;&#25991;&#26412;&#36827;&#34892;&#28436;&#32462;&#25512;&#29702;&#65292;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20165;&#20174;&#32473;&#23450;&#30446;&#26631;&#25512;&#26029;&#35745;&#21010;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#31038;&#21306;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20855;&#22791;&#35268;&#21010;&#25110;&#25191;&#34892;&#35745;&#21010;&#30340;&#33021;&#21147;&#36234;&#21457;&#24863;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30740;&#31350;&#20351;&#29992;LLMs&#20026;&#31616;&#21270;&#22330;&#26223;&#29983;&#25104;&#39640;&#32423;&#35745;&#21010;&#65292;&#36866;&#24212;&#24230;&#37327;&#32570;&#20047;&#35821;&#35328;&#22797;&#26434;&#24615;&#21644;&#39046;&#22495;&#22810;&#26679;&#24615;&#65292;&#38480;&#21046;&#20854;&#35268;&#21010;&#33021;&#21147;&#30340;&#20998;&#26512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PARADISE&#65292;&#36825;&#26159;&#19968;&#20010;&#20351;&#29992;Q&#65286;A&#26684;&#24335;&#30340;&#28436;&#32462;&#25512;&#29702;&#20219;&#21153;&#65292;&#37319;&#29992;&#26469;&#33258;wikiHow&#30340;&#23454;&#29992;&#31243;&#24207;&#25991;&#26412;&#12290;&#23427;&#28041;&#21450;&#19982;&#30446;&#26631;&#30452;&#25509;&#30456;&#20851;&#30340;&#35686;&#21578;&#21644;&#25552;&#31034;&#25512;&#26029;&#20219;&#21153;&#65292;&#25490;&#38500;&#20013;&#38388;&#27493;&#39588;&#65292;&#26088;&#22312;&#27979;&#35797;&#27169;&#22411;&#20165;&#20174;&#32473;&#23450;&#30446;&#26631;&#25512;&#26029;&#35745;&#21010;&#30340;&#38544;&#21547;&#30693;&#35782;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03167v1 Announce Type: new  Abstract: Recently, there has been growing interest within the community regarding whether large language models are capable of planning or executing plans. However, most prior studies use LLMs to generate high-level plans for simplified scenarios lacking linguistic complexity and domain diversity, limiting analysis of their planning abilities. These setups constrain evaluation methods (e.g., predefined action space), architectural choices (e.g., only generative models), and overlook the linguistic nuances essential for realistic analysis. To tackle this, we present PARADISE, an abductive reasoning task using Q\&amp;A format on practical procedural text sourced from wikiHow. It involves warning and tip inference tasks directly associated with goals, excluding intermediary steps, with the aim of testing the ability of the models to infer implicit knowledge of the plan solely from the given goal. Our experiments, utilizing fine-tuned language models and
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21019;&#24314;&#30340;&#31038;&#21306;&#22522;&#30784;&#38544;&#24335;&#25915;&#20987;&#24615;&#35821;&#35328;&#25968;&#25454;&#38598;OffLanDat&#65292;&#20026;38&#20010;&#19981;&#21516;&#30446;&#26631;&#32676;&#20307;&#25552;&#20379;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2403.02472</link><description>&lt;p&gt;
OffLanDat&#65306;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#31038;&#21306;&#22522;&#30784;&#38544;&#24335;&#25915;&#20987;&#24615;&#35821;&#35328;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
OffLanDat: A Community Based Implicit Offensive Language Dataset Generated by Large Language Model Through Prompt Engineering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02472
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21019;&#24314;&#30340;&#31038;&#21306;&#22522;&#30784;&#38544;&#24335;&#25915;&#20987;&#24615;&#35821;&#35328;&#25968;&#25454;&#38598;OffLanDat&#65292;&#20026;38&#20010;&#19981;&#21516;&#30446;&#26631;&#32676;&#20307;&#25552;&#20379;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#19978;&#25915;&#20987;&#24615;&#35821;&#35328;&#30340;&#26222;&#36941;&#23384;&#22312;&#23545;&#31038;&#20250;&#31119;&#31049;&#20135;&#29983;&#20102;&#19981;&#33391;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#39640;&#24230;&#37325;&#35270;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#25915;&#20987;&#24615;&#35821;&#35328;&#26082;&#23384;&#22312;&#26126;&#30830;&#24418;&#24335;&#65292;&#20063;&#23384;&#22312;&#38544;&#24335;&#24418;&#24335;&#65292;&#21518;&#32773;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#24403;&#21069;&#22312;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#36935;&#21040;&#20960;&#20010;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#29616;&#26377;&#25968;&#25454;&#38598;&#20027;&#35201;&#20381;&#36182;&#20110;&#25910;&#38598;&#21253;&#21547;&#26126;&#30830;&#25915;&#20987;&#24615;&#20851;&#38190;&#35789;&#30340;&#25991;&#26412;&#65292;&#36825;&#20351;&#24471;&#25429;&#25417;&#19981;&#21253;&#21547;&#36825;&#20123;&#20851;&#38190;&#35789;&#19988;&#38544;&#21547;&#25915;&#20987;&#24615;&#20869;&#23481;&#30340;&#20219;&#21153;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20854;&#27425;&#65292;&#36890;&#24120;&#30340;&#26041;&#27861;&#35770;&#20542;&#21521;&#20110;&#20165;&#20851;&#27880;&#25991;&#26412;&#20998;&#26512;&#65292;&#24573;&#35270;&#31038;&#21306;&#20449;&#24687;&#21487;&#20197;&#25552;&#20379;&#30340;&#23453;&#36149;&#35265;&#35299;&#12290;&#22312;&#36825;&#31687;&#30740;&#31350;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;OffLanDat&#65292;&#36825;&#26159;&#30001;ChatGPT&#29983;&#25104;&#30340;&#22522;&#20110;&#31038;&#21306;&#30340;&#38544;&#24335;&#25915;&#20987;&#24615;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;38&#20010;&#19981;&#21516;&#30446;&#26631;&#32676;&#20307;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02472v1 Announce Type: new  Abstract: The widespread presence of offensive languages on social media has resulted in adverse effects on societal well-being. As a result, it has become very important to address this issue with high priority. Offensive languages exist in both explicit and implicit forms, with the latter being more challenging to detect. Current research in this domain encounters several challenges. Firstly, the existing datasets primarily rely on the collection of texts containing explicit offensive keywords, making it challenging to capture implicitly offensive contents that are devoid of these keywords. Secondly, usual methodologies tend to focus solely on textual analysis, neglecting the valuable insights that community information can provide. In this research paper, we introduce a novel dataset OffLanDat, a community based implicit offensive language dataset generated by ChatGPT containing data for 38 different target groups. Despite limitations in genera
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24494;&#35843;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#20004;&#31181;&#26041;&#27861;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#20302;&#39057;&#23454;&#20307;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#20013;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#24494;&#35843;&#26174;&#33879;&#25552;&#39640;&#20102;&#21508;&#31181;&#21463;&#27426;&#36814;&#31243;&#24230;&#30340;&#23454;&#20307;&#30340;&#24615;&#33021;&#65292;&#32780;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#26041;&#27861;&#21017;&#36229;&#36807;&#20102;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.01432</link><description>&lt;p&gt;
&#24494;&#35843;&#19982;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#29992;&#20110;&#19981;&#22826;&#27969;&#34892;&#30693;&#35782;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01432
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24494;&#35843;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#20004;&#31181;&#26041;&#27861;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#20302;&#39057;&#23454;&#20307;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#20013;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#24494;&#35843;&#26174;&#33879;&#25552;&#39640;&#20102;&#21508;&#31181;&#21463;&#27426;&#36814;&#31243;&#24230;&#30340;&#23454;&#20307;&#30340;&#24615;&#33021;&#65292;&#32780;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#26041;&#27861;&#21017;&#36229;&#36807;&#20102;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35760;&#24518;&#20102;&#22823;&#37327;&#30340;&#20107;&#23454;&#30693;&#35782;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#35266;&#23519;&#21040;&#24403;&#22788;&#29702;&#19981;&#22826;&#27969;&#34892;&#25110;&#20302;&#39057;&#27010;&#24565;&#21644;&#23454;&#20307;&#26102;&#65292;&#24615;&#33021;&#20250;&#19979;&#38477;&#65292;&#20363;&#22914;&#22312;&#39046;&#22495;&#29305;&#23450;&#24212;&#29992;&#20013;&#12290;&#26412;&#25991;&#25506;&#35752;&#21644;&#35780;&#20272;&#20102;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#21644;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#65288;FT&#65289;&#23545;&#23450;&#21046;LLMs&#22788;&#29702;&#20302;&#39057;&#23454;&#20307;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;FT&#26174;&#33879;&#25552;&#21319;&#20102;&#21508;&#31181;&#21463;&#27426;&#36814;&#31243;&#24230;&#30340;&#23454;&#20307;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#26368;&#21463;&#27426;&#36814;&#21644;&#26368;&#19981;&#21463;&#27426;&#36814;&#30340;&#32676;&#20307;&#20013;&#65292;&#32780;RAG&#36229;&#36234;&#20102;&#20854;&#20182;&#26041;&#27861;&#12290;&#21478;&#22806;&#65292;&#26816;&#32034;&#21644;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30340;&#36827;&#27493;&#21152;&#24378;&#20102;RAG&#21644;FT&#26041;&#27861;&#30340;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01432v1 Announce Type: new  Abstract: Large language models (LLMs) memorize a vast amount of factual knowledge, exhibiting strong performance across diverse tasks and domains. However, it has been observed that the performance diminishes when dealing with less-popular or low-frequency concepts and entities, for example in domain specific applications. The two prominent approaches to enhance the performance of LLMs on low-frequent topics are: Retrieval Augmented Generation (RAG) and fine-tuning (FT) over synthetic data. This paper explores and evaluates the impact of RAG and FT on customizing LLMs in handling low-frequency entities on question answering task. Our findings indicate that FT significantly boosts the performance across entities of varying popularity, especially in the most and least popular groups, while RAG surpasses other methods. Additionally, the success of both RAG and FT approaches is amplified by advancements in retrieval and data augmentation techniques. 
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LAB&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#20811;&#26381;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#65292;&#36890;&#36807;&#20998;&#31867;&#27861;&#25351;&#23548;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#21644;&#22810;&#38454;&#27573;&#35843;&#25972;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#23545;&#26114;&#36149;&#20154;&#24037;&#26631;&#27880;&#21644;GPT-4&#31561;&#19987;&#26377;&#27169;&#22411;&#20381;&#36182;&#36739;&#23569;&#30340;&#22823;&#35268;&#27169;&#23545;&#40784;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#12289;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#19981;&#20250;&#20986;&#29616;&#28798;&#38590;&#24615;&#36951;&#24536;&#24773;&#20917;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;LLM&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.01081</link><description>&lt;p&gt;
LAB&#65306;&#38024;&#23545;ChatBots&#30340;&#22823;&#35268;&#27169;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
LAB: Large-Scale Alignment for ChatBots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01081
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LAB&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#20811;&#26381;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#65292;&#36890;&#36807;&#20998;&#31867;&#27861;&#25351;&#23548;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#21644;&#22810;&#38454;&#27573;&#35843;&#25972;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#23545;&#26114;&#36149;&#20154;&#24037;&#26631;&#27880;&#21644;GPT-4&#31561;&#19987;&#26377;&#27169;&#22411;&#20381;&#36182;&#36739;&#23569;&#30340;&#22823;&#35268;&#27169;&#23545;&#40784;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#12289;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#19981;&#20250;&#20986;&#29616;&#28798;&#38590;&#24615;&#36951;&#24536;&#24773;&#20917;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;LLM&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;LAB&#65288;ChatBots&#30340;&#22823;&#35268;&#27169;&#23545;&#40784;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26088;&#22312;&#20811;&#26381;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35757;&#32451;&#20013;&#25351;&#20196;&#35843;&#25972;&#38454;&#27573;&#30340;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;&#20998;&#31867;&#27861;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#21644;&#22810;&#38454;&#27573;&#35843;&#25972;&#26694;&#26550;&#65292;LAB&#26174;&#33879;&#20943;&#23569;&#23545;&#26114;&#36149;&#30340;&#20154;&#31867;&#27880;&#37322;&#21644;&#35832;&#22914;GPT-4&#20043;&#31867;&#30340;&#19987;&#26377;&#27169;&#22411;&#30340;&#20381;&#36182;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#20351;&#29992;LAB&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#20960;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24615;&#33021;&#21487;&#20197;&#19982;&#20351;&#29992;&#20256;&#32479;&#20154;&#31867;&#27880;&#37322;&#25110;GPT-4&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#22240;&#27492;&#65292;&#22312;&#19981;&#20250;&#20986;&#29616;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#12289;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#22686;&#24378;LLM&#30340;&#33021;&#21147;&#21644;&#25351;&#20196;&#36981;&#24490;&#34892;&#20026;&#65292;&#26631;&#24535;&#30528;&#22312;&#39640;&#25928;&#35757;&#32451;&#21508;&#31181;&#24212;&#29992;&#30340;LLM&#26041;&#38754;&#36808;&#20986;&#20102;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01081v1 Announce Type: new  Abstract: This work introduces LAB (Large-scale Alignment for chatBots), a novel methodology designed to overcome the scalability challenges in the instruction-tuning phase of large language model (LLM) training. Leveraging a taxonomy-guided synthetic data generation process and a multi-phase tuning framework, LAB significantly reduces reliance on expensive human annotations and proprietary models like GPT-4. We demonstrate that LAB-trained models can achieve competitive performance across several benchmarks compared to models trained with traditional human-annotated or GPT-4 generated synthetic data. Thus offering a scalable, cost-effective solution for enhancing LLM capabilities and instruction-following behaviors without the drawbacks of catastrophic forgetting, marking a step forward in the efficient training of LLMs for a wide range of applications.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#21512;&#24182;&#19981;&#21516;&#21021;&#22987;&#21270;&#30340;Transformer&#27169;&#22411;&#30340;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#21512;&#24182;&#25216;&#26415;&#20197;&#30740;&#31350;&#36825;&#20123;&#27169;&#22411;&#26497;&#23567;&#20540;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#19982;&#27169;&#22411;&#24179;&#22343;&#30456;&#27604;&#65292;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#21512;&#24182;&#36825;&#20123;&#27169;&#22411;&#22987;&#32456;&#21487;&#20197;&#33719;&#24471;&#36739;&#20302;&#30340;&#25439;&#22833;&#38556;&#30861;&#12290;</title><link>https://arxiv.org/abs/2403.00986</link><description>&lt;p&gt;
&#21512;&#24182;&#26469;&#33258;&#19981;&#21516;&#21021;&#22987;&#21270;&#30340;&#25991;&#26412;&#21464;&#25442;&#22120;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Merging Text Transformer Models from Different Initializations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00986
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#21512;&#24182;&#19981;&#21516;&#21021;&#22987;&#21270;&#30340;Transformer&#27169;&#22411;&#30340;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#21512;&#24182;&#25216;&#26415;&#20197;&#30740;&#31350;&#36825;&#20123;&#27169;&#22411;&#26497;&#23567;&#20540;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#19982;&#27169;&#22411;&#24179;&#22343;&#30456;&#27604;&#65292;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#21512;&#24182;&#36825;&#20123;&#27169;&#22411;&#22987;&#32456;&#21487;&#20197;&#33719;&#24471;&#36739;&#20302;&#30340;&#25439;&#22833;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#19968;&#27425;&#24615;&#22522;&#20110;&#25490;&#21015;&#30340;&#27169;&#22411;&#21512;&#24182;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#19981;&#21516;&#21021;&#22987;&#21270;&#30340;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#20302;&#25110;&#38646;&#38556;&#30861;&#27169;&#36830;&#25509;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;Transformer&#26550;&#26500;&#22312;&#35821;&#35328;&#39046;&#22495;&#20013;&#21344;&#20027;&#23548;&#22320;&#20301;&#65292;&#20294;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#23578;&#26410;&#24310;&#20280;&#21040;Transformer&#26550;&#26500;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#29420;&#31435;Transformer&#26497;&#23567;&#20540;&#23398;&#20064;&#31867;&#20284;&#29305;&#24449;&#30340;&#31243;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#21512;&#24182;&#25216;&#26415;&#65292;&#20197;&#30740;&#31350;&#25439;&#22833;&#26223;&#35266;&#20013;&#36825;&#20123;&#26497;&#23567;&#20540;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#26550;&#26500;&#30340;&#20855;&#20307;&#32454;&#33410;&#65292;&#22914;&#20854;&#27531;&#24046;&#36830;&#25509;&#12289;&#22810;&#22836;&#27880;&#24847;&#21147;&#21644;&#31163;&#25955;&#30340;&#39034;&#24207;&#36755;&#20837;&#65292;&#38656;&#35201;&#29305;&#23450;&#30340;&#24178;&#39044;&#25514;&#26045;&#65292;&#20197;&#20415;&#35745;&#31639;&#30041;&#22312;&#30456;&#21516;&#21151;&#33021;&#31561;&#20215;&#31867;&#20013;&#30340;&#27169;&#22411;&#25490;&#21015;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#21512;&#24182;&#36825;&#20123;&#27169;&#22411;&#65292;&#25105;&#20204;&#21457;&#29616;&#19982;&#23545;&#20960;&#20010;&#22312;&#19968;&#20010;maske&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#36827;&#34892;&#27169;&#22411;&#24179;&#22343;&#30456;&#27604;&#65292;&#26368;&#23567;&#20540;&#20043;&#38388;&#30340;&#25439;&#22833;&#38556;&#30861;&#19968;&#30452;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00986v1 Announce Type: cross  Abstract: Recent work on one-shot permutation-based model merging has shown impressive low- or zero-barrier mode connectivity between models from completely different initializations. However, this line of work has not yet extended to the Transformer architecture, despite its dominant popularity in the language domain. Therefore, in this work, we investigate the extent to which separate Transformer minima learn similar features, and propose a model merging technique to investigate the relationship between these minima in the loss landscape. The specifics of the architecture, like its residual connections, multi-headed attention, and discrete, sequential input, require specific interventions in order to compute model permutations that remain within the same functional equivalence class. In merging these models with our method, we consistently find lower loss barriers between minima compared to model averaging for several models trained on a maske
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#31181;&#29992;&#20110;Llama 2 Chat 7B&#25110;&#26356;&#22823;&#27169;&#22411;&#30340;&#33609;&#26696;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#21152;&#36895;&#25512;&#29702;&#65292;&#20165;&#21344;&#21407;&#22987;&#22823;&#23567;&#30340;1.64&#65285;&#12290;</title><link>https://arxiv.org/abs/2403.00858</link><description>&lt;p&gt;
&#30452;&#25509;&#19982;Chat-Fine-Tuned LLMs&#30340;&#33609;&#26696;&#27169;&#22411;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Direct Alignment of Draft Model for Speculative Decoding with Chat-Fine-Tuned LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00858
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#31181;&#29992;&#20110;Llama 2 Chat 7B&#25110;&#26356;&#22823;&#27169;&#22411;&#30340;&#33609;&#26696;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#21152;&#36895;&#25512;&#29702;&#65292;&#20165;&#21344;&#21407;&#22987;&#22823;&#23567;&#30340;1.64&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#29983;&#25104;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30001;&#20110;&#20854;&#33258;&#22238;&#24402;&#26412;&#36136;&#12289;&#24040;&#22823;&#30340;&#21442;&#25968;&#25968;&#37327;&#21644;&#26377;&#38480;&#30340;&#20869;&#23384;&#24102;&#23485;&#32780;&#34987;&#35748;&#20026;&#26159;&#20869;&#23384;&#23494;&#38598;&#22411;&#65292;&#36890;&#24120;&#23548;&#33268;&#20302;&#20196;&#29260;&#36895;&#29575;&#12290;&#29468;&#27979;&#35299;&#30721;&#24050;&#34987;&#25552;&#20986;&#20316;&#20026;LLM&#25512;&#29702;&#21152;&#36895;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#20195;&#24320;&#28304;LLM&#31995;&#21015;&#20013;&#65292;&#20363;&#22914;Llama 2 7B&#65292;&#30001;&#20110;&#33609;&#26696;&#27169;&#22411;&#36890;&#24120;&#19981;&#21487;&#29992;&#65292;&#22240;&#27492;&#38656;&#35201;&#35757;&#32451;&#39640;&#36136;&#37327;&#30340;&#33609;&#26696;&#27169;&#22411;&#20197;&#36890;&#36807;&#29468;&#27979;&#35299;&#30721;&#23454;&#29616;&#25512;&#29702;&#21152;&#36895;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#33609;&#26696;&#27169;&#22411;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#30452;&#25509;&#19982;Chat-capable&#30446;&#26631;&#27169;&#22411;&#23545;&#40784;&#12290;&#36890;&#36807;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#35757;&#32451;&#20986;Llama 2 Chat Drafter 115M&#65292;&#36825;&#26159;&#19968;&#20010;&#36866;&#29992;&#20110;Llama 2 Chat 7B&#25110;&#26356;&#22823;&#27169;&#22411;&#30340;&#33609;&#26696;&#27169;&#22411;&#65292;&#20165;&#21344;&#21407;&#22987;&#22823;&#23567;&#30340;1.64&#65285;&#12290;&#25105;&#20204;&#30340;&#35757;&#32451;&#26694;&#26550;&#20165;&#21253;&#25324;&#39044;&#35757;&#32451;&#12289;&#33976;&#39311;&#25968;&#25454;&#38598;&#29983;&#25104;&#21644;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#36827;&#34892;&#24494;&#35843;&#65292;&#27809;&#26377;&#39069;&#22806;&#30340;&#23545;&#40784;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00858v1 Announce Type: cross  Abstract: Text generation with Large Language Models (LLMs) is known to be memory bound due to the combination of their auto-regressive nature, huge parameter counts, and limited memory bandwidths, often resulting in low token rates. Speculative decoding has been proposed as a solution for LLM inference acceleration. However, since draft models are often unavailable in the modern open-source LLM families, e.g., for Llama 2 7B, training a high-quality draft model is required to enable inference acceleration via speculative decoding. In this paper, we propose a simple draft model training framework for direct alignment to chat-capable target models. With the proposed framework, we train Llama 2 Chat Drafter 115M, a draft model for Llama 2 Chat 7B or larger, with only 1.64\% of the original size. Our training framework only consists of pretraining, distillation dataset generation, and finetuning with knowledge distillation, with no additional align
&lt;/p&gt;</description></item><item><title>&#36870;&#36716;&#35781;&#21650;&#38382;&#39064;&#26159;&#23548;&#33268;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#36827;&#34892;&#21452;&#21521;&#25512;&#29702;&#30340;&#26681;&#26412;&#21407;&#22240;&#20043;&#19968;&#65292;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#35821;&#20041;&#24863;&#30693;&#30340;&#32622;&#25442;&#35757;&#32451;&#26469;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.00758</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#20041;&#24863;&#30693;&#32622;&#25442;&#35757;&#32451;&#26469;&#32531;&#35299;&#36870;&#36716;&#35781;&#21650;
&lt;/p&gt;
&lt;p&gt;
Mitigating Reversal Curse via Semantic-aware Permutation Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00758
&lt;/p&gt;
&lt;p&gt;
&#36870;&#36716;&#35781;&#21650;&#38382;&#39064;&#26159;&#23548;&#33268;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#36827;&#34892;&#21452;&#21521;&#25512;&#29702;&#30340;&#26681;&#26412;&#21407;&#22240;&#20043;&#19968;&#65292;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#35821;&#20041;&#24863;&#30693;&#30340;&#32622;&#25442;&#35757;&#32451;&#26469;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#34920;&#29616;&#65292;&#28982;&#32780;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22240;&#26524;&#20851;&#31995;&#30340;LLM&#36973;&#36935;&#20102;&#8220;&#36870;&#36716;&#35781;&#21650;&#8221;&#12290;&#19968;&#20010;&#20856;&#22411;&#30340;&#20363;&#23376;&#26159;&#65292;&#27169;&#22411;&#30693;&#36947;&#8220;A&#30340;&#29238;&#20146;&#26159;B&#8221;&#65292;&#20294;&#26080;&#27861;&#25512;&#29702;&#20986;&#8220;B&#30340;&#23401;&#23376;&#26159;A&#8221;&#12290;&#36825;&#19968;&#23616;&#38480;&#24615;&#23545;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#36827;&#23637;&#26500;&#25104;&#20102;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#26263;&#31034;&#20102;&#27169;&#22411;&#22312;&#29702;&#35299;&#21644;&#24212;&#29992;&#21452;&#21521;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#24046;&#36317;&#12290;&#26412;&#25991;&#39318;&#20808;&#36827;&#34892;&#20102;&#22823;&#37327;&#35780;&#20272;&#65292;&#24182;&#30830;&#23450;&#20102;&#36870;&#36716;&#35781;&#21650;&#30340;&#26681;&#26412;&#21407;&#22240;&#22312;&#20110;&#35757;&#32451;&#21644;&#25512;&#26029;&#38454;&#27573;&#20043;&#38388;&#30340;&#35789;&#24207;&#19981;&#21516;&#65292;&#21363;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#39044;&#27979;&#20808;&#34892;&#35789;&#30340;&#33021;&#21147;&#19981;&#36275;&#12290;&#22240;&#27492;&#65292;&#32771;&#34385;&#21040;&#22312;&#35757;&#32451;&#25968;&#25454;&#19978;&#36827;&#34892;&#25490;&#21015;&#21487;&#20197;&#34987;&#35270;&#20026;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#20026;&#36825;&#21487;&#20197;&#20351;&#27169;&#22411;&#39044;&#27979;&#20808;&#34892;&#35789;&#25110;&#26631;&#35760;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#25490;&#21015;&#26041;&#27861;&#21487;&#33021;&#21463;&#21040;&#25130;&#26029;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00758v1 Announce Type: cross  Abstract: While large language models (LLMs) have achieved impressive performance across diverse tasks, recent studies showcase that causal LLMs suffer from the "reversal curse". It is a typical example that the model knows "A's father is B", but is unable to reason "B's child is A". This limitation poses a challenge to the advancement of artificial general intelligence (AGI), as it suggests a gap in the models' ability to comprehend and apply bidirectional reasoning. In this paper, we first conduct substantial evaluation and identify that the root cause of the reversal curse lies in the different word order between the training and inference stage, namely, the poor ability of causal language models to predict antecedent words within the training data. Accordingly, permutation on the training data is considered as a potential solution, since this can make the model predict antecedent words or tokens. However, previous permutation methods may dis
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#22810;&#30446;&#26631;&#20248;&#21270;&#26041;&#27861;&#30340;&#27700;&#21360;&#25216;&#26415;&#65292;&#36890;&#36807;&#36731;&#37327;&#32423;&#32593;&#32476;&#29983;&#25104;&#29305;&#23450;&#20196;&#29260;&#27700;&#21360;logits&#21644;&#20998;&#21106;&#27604;&#29575;&#65292;&#22312;&#20445;&#35777;&#26816;&#27979;&#24615;&#30340;&#21516;&#26102;&#25552;&#21319;&#20102;&#25991;&#26412;&#30340;&#35821;&#20041;&#23436;&#25972;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.18059</link><description>&lt;p&gt;
&#20855;&#26377;&#22686;&#24378;&#21487;&#26816;&#27979;&#24615;&#21644;&#35821;&#20041;&#36830;&#36143;&#24615;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29305;&#23450;&#20196;&#29260;&#27700;&#21360;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Token-Specific Watermarking with Enhanced Detectability and Semantic Coherence for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18059
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#22810;&#30446;&#26631;&#20248;&#21270;&#26041;&#27861;&#30340;&#27700;&#21360;&#25216;&#26415;&#65292;&#36890;&#36807;&#36731;&#37327;&#32423;&#32593;&#32476;&#29983;&#25104;&#29305;&#23450;&#20196;&#29260;&#27700;&#21360;logits&#21644;&#20998;&#21106;&#27604;&#29575;&#65292;&#22312;&#20445;&#35777;&#26816;&#27979;&#24615;&#30340;&#21516;&#26102;&#25552;&#21319;&#20102;&#25991;&#26412;&#30340;&#35821;&#20041;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21709;&#24212;&#65292;&#28508;&#22312;&#22320;&#23384;&#22312;&#35823;&#23548;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#24378;&#35843;&#20102;&#36890;&#36807;&#21306;&#20998;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#21644;&#20154;&#31867;&#25776;&#20889;&#30340;&#25991;&#26412;&#26469;&#21152;&#20197;&#35268;&#33539;&#30340;&#24517;&#35201;&#24615;&#12290;&#27700;&#21360;&#25216;&#26415;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#33267;&#20851;&#37325;&#35201;&#65292;&#23427;&#28041;&#21450;&#22312;LLM&#25512;&#29702;&#38454;&#27573;&#21521;&#25991;&#26412;&#20013;&#23884;&#20837;&#38544;&#34255;&#26631;&#35760;&#65292;&#32780;&#36825;&#23545;&#20154;&#31867;&#26469;&#35828;&#26159;&#19981;&#21487;&#24863;&#30693;&#30340;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#27700;&#21360;&#31639;&#27861;&#38754;&#20020;&#30528;&#23454;&#29616;&#25554;&#20837;&#27700;&#21360;&#30340;&#21487;&#26816;&#27979;&#24615;&#21644;&#29983;&#25104;&#25991;&#26412;&#30340;&#35821;&#20041;&#23436;&#25972;&#24615;&#20004;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#22686;&#24378;&#20854;&#20013;&#19968;&#20010;&#26041;&#38754;&#24120;&#24120;&#20250;&#25439;&#23475;&#21478;&#19968;&#20010;&#26041;&#38754;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#65288;MOO&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#27700;&#21360;&#25216;&#26415;&#65292;&#21033;&#29992;&#36731;&#37327;&#32423;&#32593;&#32476;&#29983;&#25104;&#29305;&#23450;&#20196;&#29260;&#27700;&#21360;logits&#21644;&#20998;&#21106;&#27604;&#29575;&#12290;&#36890;&#36807;&#21033;&#29992;MOO&#26469;&#20248;&#21270;&#26816;&#27979;&#21644;&#35821;&#20041;&#30446;&#26631;&#20989;&#25968;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21516;&#26102;&#23454;&#29616;&#20102;&#21487;&#26816;&#27979;&#24615;&#21644;&#35821;&#20041;&#23436;&#25972;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18059v1 Announce Type: cross  Abstract: Large language models generate high-quality responses with potential misinformation, underscoring the need for regulation by distinguishing AI-generated and human-written texts. Watermarking is pivotal in this context, which involves embedding hidden markers in texts during the LLM inference phase, which is imperceptible to humans. Current watermarking algorithms, however, face the challenge of achieving both the detectability of inserted watermarks and the semantic integrity of generated texts, where enhancing one aspect often undermines the other. To overcome this, we introduce a novel multi-objective optimization (MOO) approach for watermarking that utilizes lightweight networks to generate token-specific watermarking logits and splitting ratios. By leveraging MOO to optimize for both detection and semantic objective functions, our method simultaneously achieves detectability and semantic integrity. Experimental results show that ou
&lt;/p&gt;</description></item><item><title>&#19981;&#38656;&#35201;&#22522;&#20934;&#23454;&#20917;&#25110;&#21442;&#32771;&#21709;&#24212;&#30340;&#26465;&#20214;&#19979;&#65292;&#36890;&#36807;&#32771;&#34385;&#27169;&#22411;&#30340;&#19977;&#20803;&#32452;&#26469;&#25490;&#21517;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#25490;&#21517;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.14860</link><description>&lt;p&gt;
&#22312;&#27809;&#26377;&#22522;&#20934;&#23454;&#20917;&#30340;&#24773;&#20917;&#19979;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
Ranking Large Language Models without Ground Truth
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14860
&lt;/p&gt;
&lt;p&gt;
&#19981;&#38656;&#35201;&#22522;&#20934;&#23454;&#20917;&#25110;&#21442;&#32771;&#21709;&#24212;&#30340;&#26465;&#20214;&#19979;&#65292;&#36890;&#36807;&#32771;&#34385;&#27169;&#22411;&#30340;&#19977;&#20803;&#32452;&#26469;&#25490;&#21517;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#25490;&#21517;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26222;&#21450;&#21644;&#24433;&#21709;&#21147;&#30340;&#22686;&#24378;&#65292;&#35780;&#20272;&#21644;&#25490;&#21517;LLMs&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#35780;&#20272;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#33719;&#21462;&#26114;&#36149;&#30340;&#20154;&#31867;&#21709;&#24212;&#65292;&#35201;&#20040;&#20351;&#29992;LLMs&#25104;&#23545;&#22320;&#20114;&#30456;&#35780;&#20272;&#65292;&#36825;&#21487;&#33021;&#19981;&#22815;&#21487;&#38752;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#22312;&#32473;&#23450;&#19968;&#32452;&#25552;&#31034;&#25968;&#25454;&#38598;&#65288;&#27604;&#22914;&#38382;&#39064;&#12289;&#35828;&#26126;&#31561;&#65289;&#21644;&#19968;&#32452;LLMs&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#22312;&#27809;&#26377;&#20219;&#20309;&#22522;&#20934;&#23454;&#20917;&#25110;&#21442;&#32771;&#21709;&#24212;&#30340;&#24773;&#20917;&#19979;&#23545;&#23427;&#20204;&#36827;&#34892;&#25490;&#21517;&#12290;&#21463;&#21040;&#29616;&#23454;&#29983;&#27963;&#30340;&#21551;&#21457;&#65292;&#20854;&#20013;&#19987;&#23478;&#21644;&#26377;&#30693;&#35782;&#30340;&#20154;&#37117;&#33021;&#35782;&#21035;&#19968;&#20010;&#26032;&#25163;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#24605;&#36335;&#26159;&#32771;&#34385;&#27169;&#22411;&#30340;&#19977;&#20803;&#32452;&#65292;&#20854;&#20013;&#27599;&#20010;&#27169;&#22411;&#35780;&#20272;&#20854;&#20182;&#20004;&#20010;&#27169;&#22411;&#65292;&#33021;&#22815;&#20197;&#24456;&#39640;&#30340;&#27010;&#29575;&#27491;&#30830;&#35782;&#21035;&#26368;&#24046;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#25105;&#20204;&#30340;&#24819;&#27861;&#24182;&#25552;&#20379;&#20102;&#25104;&#21151;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;&#36890;&#36807;&#21453;&#22797;&#24212;&#29992;&#36825;&#19968;&#24819;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#23545;LLMs&#36827;&#34892;&#25490;&#21517;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14860v1 Announce Type: cross  Abstract: Evaluation and ranking of large language models (LLMs) has become an important problem with the proliferation of these models and their impact. Evaluation methods either require human responses which are expensive to acquire or use pairs of LLMs to evaluate each other which can be unreliable. In this paper, we provide a novel perspective where, given a dataset of prompts (viz. questions, instructions, etc.) and a set of LLMs, we rank them without access to any ground truth or reference responses. Inspired by real life where both an expert and a knowledgeable person can identify a novice our main idea is to consider triplets of models, where each one of them evaluates the other two, correctly identifying the worst model in the triplet with high probability. We also analyze our idea and provide sufficient conditions for it to succeed. Applying this idea repeatedly, we propose two methods to rank LLMs. In experiments on different generati
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#30005;&#23376;&#21830;&#21153;&#20135;&#21697;&#25551;&#36848;&#29983;&#25104;&#30340;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#35843;&#25972;&#26041;&#27861;ModICT&#65292;&#36890;&#36807;&#24341;&#20837;&#30456;&#20284;&#20135;&#21697;&#26679;&#26412;&#21644;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#26088;&#22312;&#35299;&#20915;&#29983;&#25104;&#25551;&#36848;&#20013;&#24120;&#35265;&#19988;&#24573;&#30053;&#20135;&#21697;&#29305;&#24449;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.13587</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#30005;&#23376;&#21830;&#21153;&#20135;&#21697;&#25551;&#36848;&#29983;&#25104;&#30340;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#35843;&#25972;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Multimodal In-Context Tuning Approach for E-Commerce Product Description Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13587
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#30005;&#23376;&#21830;&#21153;&#20135;&#21697;&#25551;&#36848;&#29983;&#25104;&#30340;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#35843;&#25972;&#26041;&#27861;ModICT&#65292;&#36890;&#36807;&#24341;&#20837;&#30456;&#20284;&#20135;&#21697;&#26679;&#26412;&#21644;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#26088;&#22312;&#35299;&#20915;&#29983;&#25104;&#25551;&#36848;&#20013;&#24120;&#35265;&#19988;&#24573;&#30053;&#20135;&#21697;&#29305;&#24449;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35774;&#32622;&#65292;&#29992;&#20110;&#20174;&#22270;&#20687;&#20013;&#29983;&#25104;&#20135;&#21697;&#25551;&#36848;&#65292;&#20854;&#20013;&#21253;&#21547;&#33829;&#38144;&#20851;&#38190;&#35789;&#12290;&#23427;&#21033;&#29992;&#35270;&#35273;&#21644;&#25991;&#26412;&#20449;&#24687;&#30340;&#32508;&#21512;&#33021;&#21147;&#65292;&#21019;&#24314;&#26356;&#21152;&#31526;&#21512;&#20135;&#21697;&#29420;&#29305;&#29305;&#24615;&#30340;&#25551;&#36848;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#35843;&#25972;&#26041;&#27861;ModICT&#65292;&#36890;&#36807;&#24341;&#20837;&#30456;&#20284;&#30340;&#20135;&#21697;&#26679;&#26412;&#20316;&#20026;&#21442;&#32771;&#65292;&#24182;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13587v1 Announce Type: new  Abstract: In this paper, we propose a new setting for generating product descriptions from images, augmented by marketing keywords. It leverages the combined power of visual and textual information to create descriptions that are more tailored to the unique features of products. For this setting, previous methods utilize visual and textual encoders to encode the image and keywords and employ a language model-based decoder to generate the product description. However, the generated description is often inaccurate and generic since same-category products have similar copy-writings, and optimizing the overall framework on large-scale samples makes models concentrate on common words yet ignore the product features. To alleviate the issue, we present a simple and effective Multimodal In-Context Tuning approach, named ModICT, which introduces a similar product sample as the reference and utilizes the in-context learning capability of language models to 
&lt;/p&gt;</description></item><item><title>AnyGPT&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#31163;&#25955;&#34920;&#31034;&#23454;&#29616;&#21508;&#31181;&#27169;&#24577;&#30340;&#32479;&#19968;&#22788;&#29702;&#65292;&#33021;&#22815;&#22312;&#19981;&#25913;&#21464;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#25110;&#35757;&#32451;&#26041;&#24335;&#30340;&#24773;&#20917;&#19979;&#31283;&#23450;&#35757;&#32451;&#65292;&#20026;&#26032;&#27169;&#24577;&#30340;&#26080;&#32541;&#25972;&#21512;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12226</link><description>&lt;p&gt;
AnyGPT&#65306;&#32479;&#19968;&#30340;&#22810;&#27169;&#24335;&#31163;&#25955;&#24207;&#21015;&#24314;&#27169;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12226
&lt;/p&gt;
&lt;p&gt;
AnyGPT&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#31163;&#25955;&#34920;&#31034;&#23454;&#29616;&#21508;&#31181;&#27169;&#24577;&#30340;&#32479;&#19968;&#22788;&#29702;&#65292;&#33021;&#22815;&#22312;&#19981;&#25913;&#21464;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#25110;&#35757;&#32451;&#26041;&#24335;&#30340;&#24773;&#20917;&#19979;&#31283;&#23450;&#35757;&#32451;&#65292;&#20026;&#26032;&#27169;&#24577;&#30340;&#26080;&#32541;&#25972;&#21512;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102; AnyGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#20219;&#24847;&#22810;&#27169;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#21033;&#29992;&#31163;&#25955;&#34920;&#31034;&#32479;&#19968;&#22788;&#29702;&#21508;&#31181;&#27169;&#24577;&#65292;&#21253;&#25324;&#35821;&#38899;&#12289;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#38899;&#20048;&#12290;AnyGPT &#21487;&#20197;&#31283;&#23450;&#35757;&#32451;&#65292;&#26080;&#38656;&#23545;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26550;&#26500;&#25110;&#35757;&#32451;&#33539;&#24335;&#36827;&#34892;&#20219;&#20309;&#25913;&#21160;&#12290;&#30456;&#21453;&#65292;&#23427;&#20165;&#20381;&#36182;&#20110;&#25968;&#25454;&#32423;&#39044;&#22788;&#29702;&#65292;&#20419;&#36827;&#20102;&#26032;&#27169;&#24577;&#30340;&#26080;&#32541;&#38598;&#25104;&#21040;LLM&#20013;&#65292;&#31867;&#20284;&#20110;&#26032;&#35821;&#35328;&#30340;&#25972;&#21512;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#27169;&#24335;&#25991;&#26412;&#20013;&#24515;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22810;&#27169;&#24335;&#23545;&#40784;&#39044;&#35757;&#32451;&#12290;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#65292;&#25105;&#20204;&#21512;&#25104;&#20102;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#20219;&#24847;&#22810;&#27169;&#24335;&#25351;&#20196;&#25968;&#25454;&#38598;&#12290;&#23427;&#21253;&#25324;108k&#20010;&#22810;&#36718;&#23545;&#35805;&#31034;&#20363;&#65292;&#31934;&#32454;&#22320;&#20132;&#32455;&#21508;&#31181;&#27169;&#24577;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#22810;&#27169;&#24577;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#20219;&#24847;&#32452;&#21512;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;AnyGPT&#33021;&#22815;&#20419;&#36827;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12226v1 Announce Type: cross  Abstract: We introduce AnyGPT, an any-to-any multimodal language model that utilizes discrete representations for the unified processing of various modalities, including speech, text, images, and music. AnyGPT can be trained stably without any alterations to the current large language model (LLM) architecture or training paradigms. Instead, it relies exclusively on data-level preprocessing, facilitating the seamless integration of new modalities into LLMs, akin to the incorporation of new languages. We build a multimodal text-centric dataset for multimodal alignment pre-training. Utilizing generative models, we synthesize the first large-scale any-to-any multimodal instruction dataset. It consists of 108k samples of multi-turn conversations that intricately interweave various modalities, thus equipping the model to handle arbitrary combinations of multimodal inputs and outputs. Experimental results demonstrate that AnyGPT is capable of facilitat
&lt;/p&gt;</description></item><item><title>Knowledge-to-SQL&#26694;&#26550;&#21033;&#29992;&#25968;&#25454;&#19987;&#23478;LLM&#25552;&#20379;&#26377;&#29992;&#30693;&#35782;&#65292;&#22686;&#24378;&#25991;&#26412;&#21040;SQL&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11517</link><description>&lt;p&gt;
&#30693;&#35782;&#21040;SQL&#65306;&#29992;&#25968;&#25454;&#19987;&#23478;LLM&#22686;&#24378;SQL&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Knowledge-to-SQL: Enhancing SQL Generation with Data Expert LLM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11517
&lt;/p&gt;
&lt;p&gt;
Knowledge-to-SQL&#26694;&#26550;&#21033;&#29992;&#25968;&#25454;&#19987;&#23478;LLM&#25552;&#20379;&#26377;&#29992;&#30693;&#35782;&#65292;&#22686;&#24378;&#25991;&#26412;&#21040;SQL&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#29992;&#25143;&#26597;&#35810;&#29983;&#25104;&#20934;&#30830;&#30340;SQL&#65288;&#25991;&#26412;&#21040;SQL&#65289;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#29983;&#25104;SQL&#38656;&#35201;&#29702;&#35299;&#26597;&#35810;&#21644;&#25968;&#25454;&#24211;&#65292;&#28982;&#21518;&#26681;&#25454;&#25968;&#25454;&#24211;&#26816;&#32034;&#20934;&#30830;&#30340;&#25968;&#25454;&#12290;&#29616;&#26377;&#27169;&#22411;&#20381;&#36182;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#32508;&#21512;&#33021;&#21147;&#65292;&#26681;&#25454;&#25968;&#25454;&#24211;&#27169;&#24335;&#29983;&#25104;SQL&#12290;&#28982;&#32780;&#65292;&#26377;&#20123;&#24517;&#35201;&#30340;&#30693;&#35782;&#27809;&#26377;&#26126;&#30830;&#21253;&#21547;&#22312;&#25968;&#25454;&#24211;&#27169;&#24335;&#20013;&#65292;&#25110;&#32773;&#34987;LLMs&#23398;&#20064;&#20102;&#12290;&#22240;&#27492;&#65292;&#30693;&#35782;&#19981;&#36275;&#30340;&#26597;&#35810;&#29983;&#25104;&#30340;SQL&#21487;&#33021;&#26159;&#19981;&#20934;&#30830;&#30340;&#65292;&#36825;&#20250;&#23545;&#25991;&#26412;&#21040;SQL&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#31181;&#24773;&#20917;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Knowledge-to-SQL&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#37319;&#29992;&#23450;&#21046;&#30340;&#25968;&#25454;&#19987;&#23478;LLM&#65288;DELLM&#65289;&#20026;&#25152;&#26377;&#31867;&#22411;&#30340;&#25991;&#26412;&#21040;SQL&#27169;&#22411;&#25552;&#20379;&#26377;&#29992;&#30340;&#30693;&#35782;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;DELLM&#30340;&#35774;&#35745;&#65292;&#21253;&#25324;&#34920;&#26684;&#35835;&#21462;&#21644;&#22522;&#26412;&#24494;&#35843;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11517v1 Announce Type: new  Abstract: Generating accurate SQL for user queries (text-to-SQL) is a long-standing problem since the generation of the SQL requires comprehending the query and database and retrivale the accurate data from the database accordingly. Existing models rely on the comprehensive ability of Large Language Models (LLMs) to generate the SQL according to the database schema. However, there is some necessary knowledge that is not explicitly included in the database schema or has been learned by LLMs. Thus, the generated SQL of the knowledge-insufficient queries may be inaccurate, which negatively impacts the robustness of the text-to-SQL models. To deal with this situation, we propose the Knowledge-to-SQL framework, which employs tailored Data Expert LLM (DELLM) to provide helpful knowledge for all types of text-to-SQL models. Specifically, we provide the detailed design of DELLM, in terms of table reading, and the basic fine-tuning process. We further prov
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;MUSTARD&#65292;&#19968;&#31181;&#25484;&#25569;&#23450;&#29702;&#21644;&#35777;&#26126;&#25968;&#25454;&#32479;&#19968;&#21512;&#25104;&#30340;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#19977;&#20010;&#38454;&#27573;&#30340;&#21512;&#25104;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#38382;&#39064;&#21644;&#25512;&#29702;&#27493;&#39588;&#30340;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2402.08957</link><description>&lt;p&gt;
MUSTARD&#65306;&#25484;&#25569;&#23450;&#29702;&#21644;&#35777;&#26126;&#25968;&#25454;&#30340;&#32479;&#19968;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
MUSTARD: Mastering Uniform Synthesis of Theorem and Proof Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08957
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;MUSTARD&#65292;&#19968;&#31181;&#25484;&#25569;&#23450;&#29702;&#21644;&#35777;&#26126;&#25968;&#25454;&#32479;&#19968;&#21512;&#25104;&#30340;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#19977;&#20010;&#38454;&#27573;&#30340;&#21512;&#25104;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#38382;&#39064;&#21644;&#25512;&#29702;&#27493;&#39588;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#21253;&#25324;&#25968;&#23398;&#25512;&#29702;&#21644;&#23450;&#29702;&#35777;&#26126;&#12290;&#30001;&#20110;&#36825;&#20004;&#20010;&#20219;&#21153;&#38656;&#35201;&#20005;&#26684;&#21644;&#24418;&#24335;&#21270;&#30340;&#22810;&#27493;&#25512;&#29702;&#65292;&#23427;&#20204;&#26159;&#25506;&#32034;LLMs&#25512;&#29702;&#33021;&#21147;&#30340;&#21560;&#24341;&#39046;&#22495;&#65292;&#20294;&#20173;&#38754;&#20020;&#37325;&#35201;&#25361;&#25112;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#22914;Chain-of-Thought&#65288;CoT&#65289;&#25581;&#31034;&#20102;&#20013;&#38388;&#27493;&#39588;&#25351;&#23548;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#36880;&#27493;&#27880;&#37322;&#38656;&#35201;&#22823;&#37327;&#30340;&#21171;&#21160;&#21147;&#65292;&#23548;&#33268;&#24403;&#21069;&#22522;&#20934;&#27979;&#35797;&#30340;&#35757;&#32451;&#27493;&#39588;&#19981;&#36275;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;MUSTARD&#65292;&#19968;&#31181;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#65292;&#21487;&#20197;&#20027;&#23548;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#23450;&#29702;&#21644;&#35777;&#26126;&#25968;&#25454;&#30340;&#32479;&#19968;&#21512;&#25104;&#12290;MUSTARD&#36890;&#36807;&#19977;&#20010;&#38454;&#27573;&#21512;&#25104;&#25968;&#25454;&#65306;&#65288;1&#65289;&#23427;&#38543;&#26426;&#36873;&#25321;&#20960;&#20010;&#25968;&#23398;&#27010;&#24565;&#20316;&#20026;&#38382;&#39064;&#30340;&#31867;&#21035;&#12290;&#65288;2&#65289;&#28982;&#21518;&#65292;&#23427;&#20351;&#29992;&#36873;&#23450;&#30340;&#27010;&#24565;&#25552;&#31034;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#33719;&#24471;&#38382;&#39064;&#21644;&#23427;&#20204;&#30340;&#25512;&#29702;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08957v1 Announce Type: new Abstract: Recent large language models (LLMs) have witnessed significant advancement in various tasks, including mathematical reasoning and theorem proving. As these two tasks require strict and formal multi-step inference, they are appealing domains for exploring the reasoning ability of LLMs but still face important challenges. Previous studies such as Chain-of-Thought (CoT) have revealed the effectiveness of intermediate steps guidance. However, such step-wise annotation requires heavy labor, leading to insufficient training steps for current benchmarks. To fill this gap, this work introduces MUSTARD, a data generation framework that masters uniform synthesis of theorem and proof data of high quality and diversity. MUSTARD synthesizes data in three stages: (1) It samples a few mathematical concept seeds as the problem category. (2) Then, it prompts a generative language model with the sampled concepts to obtain both the problems and their step-w
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22635;&#34917;&#20102;&#19968;&#20010;&#30740;&#31350;&#31354;&#30333;&#65292;&#25506;&#35752;&#20102;&#22312;&#38750;&#33521;&#25991;&#25991;&#26412;&#20013;&#24212;&#35813;&#20351;&#29992;&#21738;&#31181;&#35821;&#35328;&#26469;&#25552;&#31034;&#24773;&#32490;&#26631;&#31614;&#12290;</title><link>https://arxiv.org/abs/2402.03223</link><description>&lt;p&gt;
&#33521;&#25991;&#25552;&#31034;&#27604;&#30446;&#26631;&#35821;&#35328;&#25552;&#31034;&#26356;&#36866;&#29992;&#20110;&#22522;&#20110;NLI&#30340;&#38646;-shot&#24773;&#32490;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
English Prompts are Better for NLI-based Zero-Shot Emotion Classification than Target-Language Prompts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22635;&#34917;&#20102;&#19968;&#20010;&#30740;&#31350;&#31354;&#30333;&#65292;&#25506;&#35752;&#20102;&#22312;&#38750;&#33521;&#25991;&#25991;&#26412;&#20013;&#24212;&#35813;&#20351;&#29992;&#21738;&#31181;&#35821;&#35328;&#26469;&#25552;&#31034;&#24773;&#32490;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#24773;&#32490;&#20998;&#31867;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#20027;&#35266;&#24615;&#30340;&#20219;&#21153;&#65292;&#30001;&#20110;&#38656;&#35201;&#36827;&#34892;&#35748;&#30693;&#25512;&#35770;&#36807;&#31243;&#26469;&#35299;&#37322;&#25991;&#23383;&#21050;&#28608;&#12290;&#27492;&#22806;&#65292;&#24773;&#32490;&#31867;&#21035;&#38598;&#21512;&#39640;&#24230;&#20381;&#36182;&#20110;&#29305;&#23450;&#39046;&#22495;&#12290;&#20363;&#22914;&#65292;&#25991;&#23398;&#20998;&#26512;&#21487;&#33021;&#38656;&#35201;&#20351;&#29992;&#23457;&#32654;&#24773;&#24863;&#65288;&#20363;&#22914;&#65292;&#21457;&#29616;&#26576;&#29289;&#32654;&#20029;&#65289;&#65292;&#32780;&#31038;&#20132;&#23186;&#20307;&#20998;&#26512;&#21017;&#21487;&#20197;&#20174;&#32454;&#31890;&#24230;&#30340;&#38598;&#21512;&#20013;&#33719;&#21462;&#22909;&#22788;&#65288;&#20363;&#22914;&#65292;&#23558;&#24868;&#24594;&#19982;&#28902;&#24700;&#20998;&#24320;&#65289;&#65292;&#19982;&#22522;&#26412;&#24773;&#32490;&#31867;&#21035;&#30456;&#23545;&#24212;&#12290;&#36825;&#20351;&#24471;&#35813;&#20219;&#21153;&#25104;&#20026;&#20102;&#38646;-shot&#20998;&#31867;&#30340;&#19968;&#20010;&#26377;&#36259;&#39046;&#22495;&#65292;&#22312;&#36825;&#31181;&#20998;&#31867;&#20013;&#65292;&#27169;&#22411;&#24320;&#21457;&#26102;&#19981;&#30693;&#36947;&#26631;&#31614;&#38598;&#21512;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22823;&#22810;&#25968;&#24773;&#32490;&#20998;&#26512;&#36164;&#28304;&#37117;&#26159;&#33521;&#25991;&#30340;&#65292;&#22240;&#27492;&#65292;&#24773;&#32490;&#20998;&#26512;&#30340;&#22823;&#37096;&#20998;&#30740;&#31350;&#37117;&#26159;&#29992;&#33521;&#25991;&#36827;&#34892;&#30340;&#65292;&#21253;&#25324;&#37027;&#20123;&#28041;&#21450;&#20351;&#29992;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#12290;&#36825;&#32473;&#25105;&#20204;&#30041;&#19979;&#19968;&#20010;&#30740;&#31350;&#31354;&#30333;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25506;&#35752;&#65306;&#22312;&#38750;&#33521;&#25991;&#25991;&#26412;&#20013;&#65292;&#25105;&#20204;&#24212;&#35813;&#29992;&#21738;&#31181;&#35821;&#35328;&#25552;&#31034;&#24773;&#32490;&#26631;&#31614;&#65311;
&lt;/p&gt;
&lt;p&gt;
Emotion classification in text is a challenging and subjective task, due to the involved cognitive inference processes that are required to interpret a textual stimulus. In addition, the set of emotion categories is highly domain-specific. For instance, literature analysis might require the use of aesthetic emotions (e.g., finding something beautiful), and social media analysis could benefit from fine-grained sets (e.g., separating anger from annoyance) in contrast to basic emotion categories. This renders the task an interesting field for zero-shot classifications, in which the label set is not known at model development time. Unfortunately, most resources for emotion analysis are English, and therefore, most studies on emotion analysis have been performed in English, including those that involve prompting language models for text labels. This leaves us with a research gap that we address in this paper: In which language should we prompt for emotion labels on non-English texts? This i
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#21462;&#20195;&#32463;&#27982;&#23454;&#39564;&#23460;&#36827;&#34892;&#36873;&#25321;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#30456;&#20851;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#21487;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.17435</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#21462;&#20195;&#32463;&#27982;&#36873;&#25321;&#39044;&#27979;&#23454;&#39564;&#23460;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Replace Economic Choice Prediction Labs?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17435
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#21462;&#20195;&#32463;&#27982;&#23454;&#39564;&#23460;&#36827;&#34892;&#36873;&#25321;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#30456;&#20851;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#27982;&#36873;&#25321;&#39044;&#27979;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#24448;&#24448;&#21463;&#38480;&#20110;&#33719;&#21462;&#20154;&#31867;&#36873;&#25321;&#25968;&#25454;&#30340;&#22256;&#38590;&#12290;&#23454;&#39564;&#32463;&#27982;&#23398;&#30740;&#31350;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#19987;&#27880;&#20110;&#31616;&#21333;&#30340;&#36873;&#25321;&#29615;&#22659;&#12290;&#26368;&#36817;&#65292;&#20154;&#24037;&#26234;&#33021;&#30028;&#20197;&#20004;&#31181;&#26041;&#24335;&#20026;&#35813;&#21162;&#21147;&#20570;&#20986;&#20102;&#36129;&#29486;&#65306;&#32771;&#34385;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#21487;&#20197;&#20195;&#26367;&#20154;&#31867;&#22312;&#19978;&#36848;&#31616;&#21333;&#36873;&#25321;&#39044;&#27979;&#29615;&#22659;&#20013;&#65292;&#20197;&#21450;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#35270;&#35282;&#30740;&#31350;&#26356;&#22797;&#26434;&#20294;&#20173;&#20005;&#26684;&#30340;&#23454;&#39564;&#32463;&#27982;&#23398;&#29615;&#22659;&#65292;&#21253;&#25324;&#19981;&#23436;&#20840;&#20449;&#24687;&#12289;&#37325;&#22797;&#21338;&#24328;&#21644;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#20132;&#27969;&#30340;&#35828;&#26381;&#28216;&#25103;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#28789;&#24863;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#23436;&#20840;&#27169;&#25311;&#32463;&#27982;&#29615;&#22659;&#65292;&#24182;&#29983;&#25104;&#29992;&#20110;&#39640;&#25928;&#20154;&#31867;&#36873;&#25321;&#39044;&#27979;&#30340;&#25968;&#25454;&#65292;&#26367;&#20195;&#22797;&#26434;&#30340;&#32463;&#27982;&#23454;&#39564;&#23460;&#30740;&#31350;&#65311;&#25105;&#20204;&#22312;&#36825;&#20010;&#20027;&#39064;&#19978;&#24320;&#21019;&#20102;&#30740;&#31350;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#21487;&#34892;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#34920;&#26126;&#20165;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Economic choice prediction is an essential challenging task, often constrained by the difficulties in acquiring human choice data. Indeed, experimental economics studies had focused mostly on simple choice settings. The AI community has recently contributed to that effort in two ways: considering whether LLMs can substitute for humans in the above-mentioned simple choice prediction settings, and the study through ML lens of more elaborated but still rigorous experimental economics settings, employing incomplete information, repetitive play, and natural language communication, notably language-based persuasion games. This leaves us with a major inspiration: can LLMs be used to fully simulate the economic environment and generate data for efficient human choice prediction, substituting for the elaborated economic lab studies? We pioneer the study of this subject, demonstrating its feasibility. In particular, we show that a model trained solely on LLM-generated data can effectively predic
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;Contrastive Activation Addition&#65288;CAA&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#35821;&#35328;&#27169;&#22411;&#30340;&#28608;&#27963;&#26469;&#31934;&#30830;&#25511;&#21046;&#30446;&#26631;&#34892;&#20026;&#30340;&#31243;&#24230;&#65292;&#26174;&#33879;&#25913;&#21464;&#27169;&#22411;&#34892;&#20026;&#24182;&#22312;&#24494;&#35843;&#21644;&#31995;&#32479;&#25552;&#31034;&#35774;&#35745;&#30340;&#22522;&#30784;&#19978;&#25552;&#20379;&#39069;&#22806;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.06681</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#28608;&#27963;&#21152;&#27861;&#25351;&#23548;Llama 2
&lt;/p&gt;
&lt;p&gt;
Steering Llama 2 via Contrastive Activation Addition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.06681
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;Contrastive Activation Addition&#65288;CAA&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#35821;&#35328;&#27169;&#22411;&#30340;&#28608;&#27963;&#26469;&#31934;&#30830;&#25511;&#21046;&#30446;&#26631;&#34892;&#20026;&#30340;&#31243;&#24230;&#65292;&#26174;&#33879;&#25913;&#21464;&#27169;&#22411;&#34892;&#20026;&#24182;&#22312;&#24494;&#35843;&#21644;&#31995;&#32479;&#25552;&#31034;&#35774;&#35745;&#30340;&#22522;&#30784;&#19978;&#25552;&#20379;&#39069;&#22806;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;Contrastive Activation Addition&#65288;CAA&#65289;&#65292;&#29992;&#20110;&#36890;&#36807;&#22312;&#21069;&#21521;&#20256;&#36882;&#36807;&#31243;&#20013;&#20462;&#25913;&#20854;&#28608;&#27963;&#26469;&#25351;&#23548;&#35821;&#35328;&#27169;&#22411;&#12290;CAA&#36890;&#36807;&#23545;&#26576;&#31181;&#34892;&#20026;&#30340;&#27491;&#38754;&#21644;&#36127;&#38754;&#31034;&#20363;&#20043;&#38388;&#27531;&#24046;&#27969;&#28608;&#27963;&#30340;&#24046;&#24322;&#27714;&#24179;&#22343;&#65292;&#35745;&#31639;&#20986;&#8220;&#25351;&#23548;&#21521;&#37327;&#8221;&#12290;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#22312;&#29992;&#25143;&#25552;&#31034;&#21518;&#30340;&#25152;&#26377;token&#20301;&#32622;&#19978;&#20197;&#27491;&#36127;&#31995;&#25968;&#28155;&#21152;&#36825;&#20123;&#25351;&#23548;&#21521;&#37327;&#65292;&#20174;&#32780;&#31934;&#30830;&#25511;&#21046;&#30446;&#26631;&#34892;&#20026;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22810;&#39033;&#36873;&#25321;&#34892;&#20026;&#38382;&#39064;&#25968;&#25454;&#38598;&#21644;&#24320;&#25918;&#24335;&#29983;&#25104;&#20219;&#21153;&#22312;Llama 2 Chat&#19978;&#35780;&#20272;&#20102;CAA&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;CAA&#26174;&#30528;&#25913;&#21464;&#20102;&#27169;&#22411;&#34892;&#20026;&#65292;&#19981;&#20165;&#22312;&#20256;&#32479;&#26041;&#27861;&#22914;&#24494;&#35843;&#21644;&#31995;&#32479;&#25552;&#31034;&#35774;&#35745;&#30340;&#22522;&#30784;&#19978;&#26377;&#25928;&#65292;&#32780;&#19988;&#26368;&#23567;&#31243;&#24230;&#22320;&#38477;&#20302;&#20102;&#21151;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#27169;&#22411;&#30340;&#34892;&#20026;&#20570;&#20986;&#20102;&#26356;&#28145;&#20837;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.06681v3 Announce Type: replace-cross  Abstract: We introduce Contrastive Activation Addition (CAA), an innovative method for steering language models by modifying their activations during forward passes. CAA computes "steering vectors" by averaging the difference in residual stream activations between pairs of positive and negative examples of a particular behavior, such as factual versus hallucinatory responses. During inference, these steering vectors are added at all token positions after the user's prompt with either a positive or negative coefficient, allowing precise control over the degree of the targeted behavior. We evaluate CAA's effectiveness on Llama 2 Chat using multiple-choice behavioral question datasets and open-ended generation tasks. We demonstrate that CAA significantly alters model behavior, is effective over and on top of traditional methods like finetuning and system prompt design, and minimally reduces capabilities. Moreover, we gain deeper insights in
&lt;/p&gt;</description></item><item><title>&#22312;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#25509;&#21475;&#26102;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29992;&#25143;&#24120;&#35774;&#25351;&#20196;&#30340;&#23545;&#35805;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#29992;&#25143;&#30340;&#32422;&#26463;&#21644;&#20559;&#22909;&#20316;&#20026;&#19978;&#19979;&#25991;&#65292;&#20174;&#32780;&#22312;&#31867;&#20284;&#35831;&#27714;&#20013;&#23454;&#29616;&#33258;&#21160;&#21270;&#12290;</title><link>https://arxiv.org/abs/2311.09796</link><description>&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#25351;&#20196;&#29615;&#22659;&#20013;&#35299;&#37322;&#29992;&#25143;&#35831;&#27714;
&lt;/p&gt;
&lt;p&gt;
Interpreting User Requests in the Context of Natural Language Standing Instructions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09796
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#25509;&#21475;&#26102;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29992;&#25143;&#24120;&#35774;&#25351;&#20196;&#30340;&#23545;&#35805;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#29992;&#25143;&#30340;&#32422;&#26463;&#21644;&#20559;&#22909;&#20316;&#20026;&#19978;&#19979;&#25991;&#65292;&#20174;&#32780;&#22312;&#31867;&#20284;&#35831;&#27714;&#20013;&#23454;&#29616;&#33258;&#21160;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#25509;&#21475;&#30340;&#29992;&#25143;&#36890;&#24120;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39537;&#21160;&#65292;&#24182;&#19988;&#32463;&#24120;&#24517;&#39035;&#22312;&#27599;&#27425;&#36827;&#34892;&#31867;&#20284;&#35831;&#27714;&#26102;&#37325;&#22797;&#20182;&#20204;&#30340;&#20559;&#22909;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#23545;&#35805;&#24314;&#27169;&#26041;&#27861;&#65292;&#20854;&#20013;&#25345;&#20037;&#30340;&#29992;&#25143;&#32422;&#26463;&#21644;&#20559;&#22909; - &#32479;&#31216;&#20026;&#24120;&#35774;&#25351;&#20196; - &#20316;&#20026;&#36825;&#31181;&#25509;&#21475;&#30340;&#39069;&#22806;&#19978;&#19979;&#25991;&#12290;&#20363;&#22914;&#65292;&#24403;&#29992;&#25143;&#35828;&#8220;&#25105;&#39295;&#20102;&#8221;&#26102;&#65292;&#20808;&#21069;&#34920;&#36798;&#30340;&#27874;&#26031;&#39135;&#29289;&#20559;&#22909;&#21487;&#20197;&#33258;&#21160;&#28155;&#21152;&#21040;LLM&#25552;&#31034;&#20013;&#65292;&#24433;&#21709;&#25628;&#32034;&#30456;&#20851;&#39184;&#39302;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;NLSI&#65292;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;2.4K&#36328;&#36234;17&#20010;&#39046;&#22495;&#30340;&#23545;&#35805;&#30340;&#35821;&#35328;&#21040;&#31243;&#24207;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#27599;&#20010;&#23545;&#35805;&#37117;&#19982;&#29992;&#25143;&#37197;&#32622;&#25991;&#20214;&#65288;&#19968;&#32452;&#29992;&#25143;&#29305;&#23450;&#30340;&#24120;&#35774;&#25351;&#20196;&#65289;&#21644;&#30456;&#24212;&#30340;&#32467;&#26500;&#21270;&#34920;&#31034;&#24418;&#24335;&#65288;API&#35843;&#29992;&#65289;&#37197;&#23545;&#12290; NLSI&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#30830;&#23450;&#21738;&#20123;&#24120;&#35774;&#25351;&#20196;&#23376;&#38598;&#36866;&#29992;&#20110;&#32473;&#23450;&#30340;&#23545;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09796v2 Announce Type: replace-cross  Abstract: Users of natural language interfaces, generally powered by Large Language Models (LLMs),often must repeat their preferences each time they make a similar request. We describe an approach to LLM-based dialogue modeling in which persistent user constraints and preferences -- collectively termed standing instructions -- as additional context for such interfaces. For example, when a user states "I'm hungry", a previously expressed preference for Persian food can be automatically added to the LLM prompt, influencing the search for relevant restaurants. We develop NLSI, a language-to-program dataset consisting of over 2.4K dialogues spanning 17 domains, where each dialogue is paired with a user profile (a set of users specific standing instructions) and corresponding structured representations (API calls). A key challenge in NLSI is to identify which subset of the standing instructions is applicable to a given dialogue. NLSI contains
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;RuLES&#30340;&#31243;&#24207;&#26694;&#26550;&#65292;&#29992;&#20110;&#34913;&#37327;LLMs&#22312;&#19982;&#29992;&#25143;&#20132;&#20114;&#26102;&#36981;&#23432;&#35268;&#21017;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2311.04235</link><description>&lt;p&gt;
LLM&#33021;&#36981;&#23432;&#31616;&#21333;&#35268;&#21017;&#21527;?
&lt;/p&gt;
&lt;p&gt;
Can LLMs Follow Simple Rules?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.04235
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;RuLES&#30340;&#31243;&#24207;&#26694;&#26550;&#65292;&#29992;&#20110;&#34913;&#37327;LLMs&#22312;&#19982;&#29992;&#25143;&#20132;&#20114;&#26102;&#36981;&#23432;&#35268;&#21017;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#25215;&#25285;&#36234;&#26469;&#36234;&#22810;&#30340;&#36131;&#20219;&#65292;&#33021;&#22815;&#20197;&#21487;&#38752;&#30340;&#26041;&#24335;&#25351;&#23450;&#21644;&#32422;&#26463;&#36825;&#20123;&#31995;&#32479;&#30340;&#34892;&#20026;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#35268;&#21017;&#36981;&#24490;&#35821;&#35328;&#35780;&#20272;&#22330;&#26223;&#65288;RuLES&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#27979;&#37327;LLMs&#36981;&#24490;&#35268;&#21017;&#33021;&#21147;&#30340;&#31243;&#24207;&#26694;&#26550;&#65292;&#21253;&#25324;14&#20010;&#31616;&#21333;&#30340;&#25991;&#26412;&#22330;&#26223;&#65292;&#27169;&#22411;&#22312;&#19982;&#29992;&#25143;&#20132;&#20114;&#26102;&#34987;&#25351;&#31034;&#36981;&#23432;&#21508;&#31181;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.04235v2 Announce Type: replace  Abstract: As Large Language Models (LLMs) are deployed with increasing real-world responsibilities, it is important to be able to specify and constrain the behavior of these systems in a reliable manner. Model developers may wish to set explicit rules for the model, such as "do not generate abusive content", but these may be circumvented by jailbreaking techniques. Existing evaluations of adversarial attacks and defenses on LLMs generally require either expensive manual review or unreliable heuristic checks. To address this issue, we propose Rule-following Language Evaluation Scenarios (RuLES), a programmatic framework for measuring rule-following ability in LLMs. RuLES consists of 14 simple text scenarios in which the model is instructed to obey various rules while interacting with the user. Each scenario has a programmatic evaluation function to determine whether the model has broken any rules in a conversation. Our evaluations of proprietar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#26080;&#32541;&#36866;&#24212;Avalon&#28216;&#25103;&#65292;&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#27807;&#36890;&#21644;&#20114;&#21160;&#65292;&#35780;&#20272;&#20102;&#26234;&#33021;&#20307;&#30340;&#24615;&#33021;&#21644;&#31038;&#20250;&#34892;&#20026;&#65292;&#23637;&#31034;&#20102;LLM&#26234;&#33021;&#20307;&#22312;&#28216;&#25103;&#20013;&#20855;&#26377;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2310.14985</link><description>&lt;p&gt;
&#22522;&#20110;LLM&#30340;&#26234;&#33021;&#20307;&#31038;&#20250;&#34892;&#20026;&#30740;&#31350;&#65306;Avalon&#28216;&#25103;&#20013;&#30340;&#21327;&#20316;&#19982;&#23545;&#25239;
&lt;/p&gt;
&lt;p&gt;
LLM-Based Agent Society Investigation: Collaboration and Confrontation in Avalon Gameplay
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.14985
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#26080;&#32541;&#36866;&#24212;Avalon&#28216;&#25103;&#65292;&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#27807;&#36890;&#21644;&#20114;&#21160;&#65292;&#35780;&#20272;&#20102;&#26234;&#33021;&#20307;&#30340;&#24615;&#33021;&#21644;&#31038;&#20250;&#34892;&#20026;&#65292;&#23637;&#31034;&#20102;LLM&#26234;&#33021;&#20307;&#22312;&#28216;&#25103;&#20013;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#25581;&#31034;&#22522;&#20110;LLM&#30340;&#26234;&#33021;&#20307;&#31038;&#20250;&#34892;&#20026;&#30340;&#24320;&#25918;&#24615;&#30740;&#31350;&#38382;&#39064;&#12290;&#20026;&#36798;&#21040;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;Avalon&#20316;&#20026;&#20195;&#34920;&#24615;&#30340;&#27807;&#36890;&#28216;&#25103;&#29615;&#22659;&#65292;&#24182;&#20351;&#29992;&#31995;&#32479;&#25552;&#31034;&#24341;&#23548;LLM&#26234;&#33021;&#20307;&#36827;&#34892;&#28216;&#25103;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#36827;&#34892;&#20102;&#20851;&#20110;LLM&#26234;&#33021;&#20307;&#30340;&#28216;&#25103;&#29609;&#27861;&#30340;&#21021;&#27493;&#30740;&#31350;&#65292;&#20294;&#26159;&#20182;&#20204;&#30340;&#31038;&#20250;&#34892;&#20026;&#20173;&#32570;&#20047;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#26080;&#32541;&#36866;&#24212;Avalon&#28216;&#25103;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#65292;&#21487;&#20197;&#23454;&#29616;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#26377;&#25928;&#27807;&#36890;&#21644;&#20114;&#21160;&#12290;&#25105;&#20204;&#26681;&#25454;&#20004;&#20010;&#35282;&#24230;&#30340;&#24230;&#37327;&#26631;&#20934;&#35780;&#20272;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#24615;&#33021;&#65306;&#36194;&#24471;&#28216;&#25103;&#21644;&#26356;&#20998;&#26512;LLM&#26234;&#33021;&#20307;&#30340;&#31038;&#20250;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#29983;&#25104;&#33258;&#36866;&#24212;&#21644;&#26234;&#33021;&#26234;&#33021;&#20307;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#31361;&#26174;&#20102;LLM&#26234;&#33021;&#20307;&#22312;&#24212;&#23545;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.14985v2 Announce Type: replace  Abstract: This paper aims to investigate the open research problem of uncovering the social behaviors of LLM-based agents. To achieve this goal, we adopt Avalon, a representative communication game, as the environment and use system prompts to guide LLM agents to play the game. While previous studies have conducted preliminary investigations into gameplay with LLM agents, there lacks research on their social behaviors. In this paper, we present a novel framework designed to seamlessly adapt to Avalon gameplay. The core of our proposed framework is a multi-agent system that enables efficient communication and interaction among agents. We evaluate the performance of our framework based on metrics from two perspectives: winning the game and analyzing the social behaviors of LLM agents. Our results demonstrate the effectiveness of our framework in generating adaptive and intelligent agents and highlight the potential of LLM-based agents in address
&lt;/p&gt;</description></item><item><title>&#20889;&#20316;&#26102;&#20351;&#29992;InstructGPT&#65288;&#32780;&#19981;&#26159;GPT3&#65289;&#20250;&#26174;&#33879;&#38477;&#20302;&#20869;&#23481;&#22810;&#26679;&#24615;&#65292;&#22686;&#21152;&#19981;&#21516;&#20316;&#32773;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#20943;&#23569;&#25972;&#20307;&#30340;&#35789;&#27719;&#21644;&#20869;&#23481;&#22810;&#26679;&#24615;&#12290;</title><link>https://arxiv.org/abs/2309.05196</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20889;&#20316;&#26159;&#21542;&#20250;&#38477;&#20302;&#20869;&#23481;&#22810;&#26679;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
Does Writing with Language Models Reduce Content Diversity?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.05196
&lt;/p&gt;
&lt;p&gt;
&#20889;&#20316;&#26102;&#20351;&#29992;InstructGPT&#65288;&#32780;&#19981;&#26159;GPT3&#65289;&#20250;&#26174;&#33879;&#38477;&#20302;&#20869;&#23481;&#22810;&#26679;&#24615;&#65292;&#22686;&#21152;&#19981;&#21516;&#20316;&#32773;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#20943;&#23569;&#25972;&#20307;&#30340;&#35789;&#27719;&#21644;&#20869;&#23481;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24341;&#21457;&#20102;&#19982;&#27169;&#22411;&#36741;&#21161;&#21512;&#20316;&#20889;&#20316;&#30340;&#28608;&#22686;&#12290;&#24403;&#19981;&#21516;&#29992;&#25143;&#32435;&#20837;&#21516;&#19968;&#27169;&#22411;&#30340;&#24314;&#35758;&#26102;&#65292;&#20250;&#23384;&#22312;&#20869;&#23481;&#22810;&#26679;&#24615;&#20943;&#23569;&#30340;&#39118;&#38505;&#65292;&#21487;&#33021;&#38480;&#21046;&#20844;&#20849;&#35805;&#35821;&#20013;&#30340;&#22810;&#20803;&#35266;&#28857;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#25511;&#21046;&#23454;&#39564;&#27979;&#37327;&#20102;&#21327;&#21516;&#20889;&#20316;&#23545;&#22810;&#26679;&#24615;&#30340;&#24433;&#21709;&#65292;&#22312;&#35813;&#23454;&#39564;&#20013;&#65292;&#29992;&#25143;&#20197;&#19977;&#31181;&#35774;&#32622;&#25776;&#20889;&#35758;&#35770;&#24615;&#25991;&#31456;--&#20351;&#29992;&#22522;&#26412;LLM&#65288;GPT3&#65289;&#12289;&#32463;&#36807;&#21453;&#39304;&#35843;&#25972;&#30340;LLM&#65288;InstructGPT&#65289;&#20197;&#21450;&#19981;&#20351;&#29992;&#27169;&#22411;&#24110;&#21161;&#20889;&#20316;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#32452;&#22810;&#26679;&#24615;&#25351;&#26631;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;InstructGPT&#36827;&#34892;&#20889;&#20316;&#65288;&#32780;&#19981;&#26159;GPT3&#65289;&#20250;&#23548;&#33268;&#22810;&#26679;&#24615;&#26126;&#26174;&#38477;&#20302;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#22686;&#21152;&#20102;&#19981;&#21516;&#20316;&#32773;&#30340;&#20889;&#20316;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#20943;&#23569;&#20102;&#25972;&#20307;&#30340;&#35789;&#27719;&#21644;&#20869;&#23481;&#22810;&#26679;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#36825;&#31181;&#24433;&#21709;&#20027;&#35201;&#26469;&#28304;&#20110;InstructGPT&#23545;&#20849;&#21516;&#25776;&#20889;&#30340;&#25991;&#26412;&#36129;&#29486;&#36739;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.05196v2 Announce Type: replace  Abstract: Large language models (LLMs) have led to a surge in collaborative writing with model assistance. As different users incorporate suggestions from the same model, there is a risk of decreased diversity in the produced content, potentially limiting diverse perspectives in public discourse. In this work, we measure the impact of co-writing on diversity via a controlled experiment, where users write argumentative essays in three setups -- using a base LLM (GPT3), a feedback-tuned LLM (InstructGPT), and writing without model help. We develop a set of diversity metrics and find that writing with InstructGPT (but not the GPT3) results in a statistically significant reduction in diversity. Specifically, it increases the similarity between the writings of different authors and reduces the overall lexical and content diversity. We additionally find that this effect is mainly attributable to InstructGPT contributing less diverse text to co-writt
&lt;/p&gt;</description></item><item><title>&#20132;&#20114;&#24335;&#38382;&#31572;&#31995;&#32479;&#26159;&#38382;&#31572;&#21644;&#23545;&#35805;&#31995;&#32479;&#30340;&#32467;&#21512;&#65292;&#29992;&#25143;&#21487;&#20197;&#29992;&#33258;&#28982;&#35821;&#35328;&#25552;&#38382;&#24182;&#19982;&#31995;&#32479;&#21160;&#24577;&#20132;&#20114;&#65292;&#33719;&#24471;&#26356;&#31934;&#30830;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2209.01621</link><description>&lt;p&gt;
&#20132;&#20114;&#24335;&#38382;&#31572;&#31995;&#32479;&#65306;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Interactive Question Answering Systems: Literature Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.01621
&lt;/p&gt;
&lt;p&gt;
&#20132;&#20114;&#24335;&#38382;&#31572;&#31995;&#32479;&#26159;&#38382;&#31572;&#21644;&#23545;&#35805;&#31995;&#32479;&#30340;&#32467;&#21512;&#65292;&#29992;&#25143;&#21487;&#20197;&#29992;&#33258;&#28982;&#35821;&#35328;&#25552;&#38382;&#24182;&#19982;&#31995;&#32479;&#21160;&#24577;&#20132;&#20114;&#65292;&#33719;&#24471;&#26356;&#31934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2209.01621v2 &#20844;&#21578;&#31867;&#22411;: &#26367;&#25442;-&#36328;  &#25688;&#35201;: &#38382;&#31572;&#31995;&#32479;&#34987;&#20844;&#35748;&#20026;&#22312;&#32593;&#32476;&#19978;&#23547;&#27714;&#20449;&#24687;&#30340;&#27969;&#34892;&#19988;&#26377;&#25928;&#30340;&#25163;&#27573;&#12290;&#22312;&#36825;&#31181;&#31995;&#32479;&#20013;&#65292;&#20449;&#24687;&#23547;&#25214;&#32773;&#21487;&#20197;&#36890;&#36807;&#29992;&#33258;&#28982;&#35821;&#35328;&#25552;&#20986;&#38382;&#39064;&#26469;&#33719;&#24471;&#31616;&#27905;&#30340;&#22238;&#31572;&#12290;&#20132;&#20114;&#24335;&#38382;&#31572;&#26159;&#26368;&#36817;&#25552;&#20986;&#30340;&#24182;&#36234;&#26469;&#36234;&#27969;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20301;&#20110;&#38382;&#31572;&#21644;&#23545;&#35805;&#31995;&#32479;&#30340;&#20132;&#38598;&#22788;&#12290;&#19968;&#26041;&#38754;&#65292;&#29992;&#25143;&#21487;&#20197;&#29992;&#26222;&#36890;&#35821;&#35328;&#25552;&#38382;&#24182;&#25214;&#21040;&#22905;&#38382;&#39064;&#30340;&#23454;&#38469;&#22238;&#31572;&#65307;&#21478;&#19968;&#26041;&#38754;&#65292;&#22914;&#26524;&#21021;&#22987;&#35831;&#27714;&#20013;&#23384;&#22312;&#22810;&#20010;&#21487;&#33021;&#30340;&#22238;&#22797;&#12289;&#24456;&#23569;&#25110;&#27169;&#26865;&#20004;&#21487;&#65292;&#31995;&#32479;&#21487;&#20197;&#23558;&#38382;&#31572;&#20250;&#35805;&#24310;&#38271;&#20026;&#23545;&#35805;&#12290;&#36890;&#36807;&#20801;&#35768;&#29992;&#25143;&#25552;&#20986;&#26356;&#22810;&#38382;&#39064;&#65292;&#20132;&#20114;&#24335;&#38382;&#31572;&#20351;&#29992;&#25143;&#33021;&#22815;&#21160;&#24577;&#22320;&#19982;&#31995;&#32479;&#20132;&#20114;&#24182;&#33719;&#24471;&#26356;&#31934;&#30830;&#30340;&#32467;&#26524;&#12290;&#26412;&#32508;&#36848;&#25552;&#20379;&#20102;&#20132;&#20114;&#24335;&#38382;&#31572;&#31995;&#32479;&#30340;&#35814;&#32454;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.01621v2 Announce Type: replace-cross  Abstract: Question answering systems are recognized as popular and frequently effective means of information seeking on the web. In such systems, information seekers can receive a concise response to their query by presenting their questions in natural language. Interactive question answering is a recently proposed and increasingly popular solution that resides at the intersection of question answering and dialogue systems. On the one hand, the user can ask questions in normal language and locate the actual response to her inquiry; on the other hand, the system can prolong the question-answering session into a dialogue if there are multiple probable replies, very few, or ambiguities in the initial request. By permitting the user to ask more questions, interactive question answering enables users to dynamically interact with the system and receive more precise results. This survey offers a detailed overview of the interactive question-ans
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#35328;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#36328;&#24773;&#22659;&#35270;&#21548;&#23398;&#20064;&#65292;&#38899;&#32032;&#12289;&#38899;&#33410;&#21644;&#21333;&#35789;&#33021;&#21542;&#20316;&#20026;&#21103;&#20135;&#21697;&#20986;&#29616;&#65292;&#24182;&#25903;&#25345;&#19981;&#21516;&#24418;&#24335;&#34920;&#24449;&#38388;&#30340;&#36716;&#25442;&#12290;</title><link>https://arxiv.org/abs/2109.14200</link><description>&lt;p&gt;
&#25163;&#26426;&#12289;&#38899;&#33410;&#21644;&#21333;&#35789;&#33021;&#21542;&#20316;&#20026;&#36328;&#24773;&#22659;&#35270;&#21548;&#23398;&#20064;&#30340;&#21103;&#20135;&#21697;&#32780;&#20986;&#29616;&#65311;-- &#19968;&#39033;&#35745;&#31639;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Can phones, syllables, and words emerge as side-products of cross-situational audiovisual learning? -- A computational investigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2109.14200
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#35328;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#36328;&#24773;&#22659;&#35270;&#21548;&#23398;&#20064;&#65292;&#38899;&#32032;&#12289;&#38899;&#33410;&#21644;&#21333;&#35789;&#33021;&#21542;&#20316;&#20026;&#21103;&#20135;&#21697;&#20986;&#29616;&#65292;&#24182;&#25903;&#25345;&#19981;&#21516;&#24418;&#24335;&#34920;&#24449;&#38388;&#30340;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#21313;&#24180;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#35328;&#23398;&#20064;&#23156;&#20799;&#22914;&#20309;&#23398;&#20250;&#21306;&#20998;&#35821;&#38899;&#65292;&#20998;&#21106;&#21333;&#35789;&#65292;&#20197;&#21450;&#23558;&#21333;&#35789;&#19982;&#20854;&#21547;&#20041;&#20851;&#32852;&#36215;&#26469;&#12290;&#23613;&#31649;&#36825;&#20123;&#33021;&#21147;&#30340;&#36880;&#28176;&#21457;&#23637;&#26159;&#27595;&#24248;&#32622;&#30097;&#30340;&#65292;&#20294;&#36825;&#20123;&#25216;&#33021;&#30340;&#30830;&#20999;&#24615;&#36136;&#21644;&#28508;&#22312;&#30340;&#24515;&#29702;&#34920;&#24449;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#35745;&#31639;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#35821;&#38899;&#21644;&#21516;&#26102;&#20855;&#26377;&#25351;&#31216;&#19981;&#26126;&#30830;&#30340;&#35270;&#35273;&#36755;&#20837;&#20043;&#38388;&#30340;&#32479;&#35745;&#23398;&#20064;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#22522;&#26412;&#35821;&#38899;&#30340;&#29702;&#35299;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#22312;&#27809;&#26377;&#35832;&#22914;&#35821;&#35328;&#21333;&#20301;&#30340;&#34920;&#24449;&#65292;&#20197;&#21450;&#27809;&#26377;&#19987;&#38376;&#38024;&#23545;&#36825;&#20123;&#21333;&#20301;&#30340;&#23398;&#20064;&#26426;&#21046;&#30340;&#24773;&#20917;&#19979;&#36816;&#34892;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#65292;&#31867;&#20284;&#38899;&#32032;&#12289;&#38899;&#33410;&#21644;&#21333;&#35789;&#30340;&#35821;&#35328;&#21333;&#20301;&#30340;&#30693;&#35782;&#23454;&#38469;&#19978;&#33021;&#22815;&#20316;&#20026;&#28508;&#22312;&#34920;&#24449;&#20986;&#29616;&#65292;&#25903;&#25345;&#35821;&#38899;&#19982;&#20854;&#20182;&#24418;&#24335;&#34920;&#24449;&#20043;&#38388;&#30340;&#36716;&#25442;&#65292;&#32780;&#19981;&#38656;&#35201;&#19987;&#38376;&#30340;&#23398;&#20064;&#26426;&#21046;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2109.14200v2 Announce Type: replace-cross  Abstract: Decades of research has studied how language learning infants learn to discriminate speech sounds, segment words, and associate words with their meanings. While gradual development of such capabilities is unquestionable, the exact nature of these skills and the underlying mental representations yet remains unclear. In parallel, computational studies have shown that basic comprehension of speech can be achieved by statistical learning between speech and concurrent referentially ambiguous visual input. These models can operate without prior linguistic knowledge such as representations of linguistic units, and without learning mechanisms specifically targeted at such units. This has raised the question of to what extent knowledge of linguistic units, such as phone(me)s, syllables, and words, could actually emerge as latent representations supporting the translation between speech and representations in other modalities, and withou
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;Q&amp;A&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25366;&#25496;&#22270;&#20687;&#20013;&#30340;&#38382;&#39064;-&#22238;&#31572;&#23545;&#26469;&#21457;&#29616;&#20016;&#23500;&#30340;&#35270;&#35273;&#32447;&#32034;&#65292;&#20197;&#24110;&#21161;AI&#27169;&#22411;&#26356;&#22909;&#22320;&#29702;&#35299;&#22797;&#26434;&#35270;&#35273;&#38382;&#39064;&#65292;&#25552;&#39640;&#36328;&#27169;&#24577;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.10712</link><description>&lt;p&gt;
Q&amp;A&#25552;&#31034;&#65306;&#36890;&#36807;&#25366;&#25496;&#38382;&#39064;-&#22238;&#31572;&#25552;&#31034;&#26469;&#21457;&#29616;&#20016;&#23500;&#30340;&#35270;&#35273;&#32447;&#32034;&#65292;&#20197;&#28385;&#36275;&#23545;&#22810;&#26679;&#19990;&#30028;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#31572;&#30340;&#38656;&#27714;
&lt;/p&gt;
&lt;p&gt;
Q&amp;A Prompts: Discovering Rich Visual Clues through Mining Question-Answer Prompts for VQA requiring Diverse World Knowledge. (arXiv:2401.10712v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;Q&amp;A&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25366;&#25496;&#22270;&#20687;&#20013;&#30340;&#38382;&#39064;-&#22238;&#31572;&#23545;&#26469;&#21457;&#29616;&#20016;&#23500;&#30340;&#35270;&#35273;&#32447;&#32034;&#65292;&#20197;&#24110;&#21161;AI&#27169;&#22411;&#26356;&#22909;&#22320;&#29702;&#35299;&#22797;&#26434;&#35270;&#35273;&#38382;&#39064;&#65292;&#25552;&#39640;&#36328;&#27169;&#24577;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#30772;&#65292;&#22238;&#31572;&#38656;&#35201;&#39640;&#32423;&#25512;&#29702;&#33021;&#21147;&#21644;&#19990;&#30028;&#30693;&#35782;&#30340;&#22797;&#26434;&#35270;&#35273;&#38382;&#39064;&#27604;&#20197;&#24448;&#20219;&#20309;&#26102;&#20505;&#37117;&#26356;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20026;AI&#27169;&#22411;&#37197;&#22791;&#24378;&#22823;&#30340;&#36328;&#27169;&#24577;&#25512;&#29702;&#33021;&#21147;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#20154;&#31867;&#30340;&#35748;&#30693;&#26041;&#26696;&#23578;&#26410;&#31995;&#32479;&#22320;&#34987;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30456;&#20449;&#65292;&#22914;&#26524;&#25105;&#20204;&#33021;&#23613;&#21487;&#33021;&#25910;&#38598;&#32473;&#23450;&#22270;&#20687;&#20013;&#30340;&#35270;&#35273;&#32447;&#32034;&#65292;&#25105;&#20204;&#23558;&#33021;&#26356;&#20934;&#30830;&#22320;&#35782;&#21035;&#22270;&#20687;&#65292;&#26356;&#22909;&#22320;&#29702;&#35299;&#38382;&#39064;&#65292;&#26356;&#23481;&#26131;&#22238;&#24518;&#30456;&#20851;&#30693;&#35782;&#65292;&#24182;&#26368;&#32456;&#25512;&#29702;&#20986;&#31572;&#26696;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#22270;&#20687;&#20013;&#25366;&#25496;&#38382;&#39064;-&#22238;&#31572;&#23545;&#26469;&#21457;&#29616;&#36825;&#20123;&#20016;&#23500;&#30340;&#35270;&#35273;&#32447;&#32034;&#65292;&#24182;&#23558;&#23427;&#20204;&#20316;&#20026;&#25552;&#31034;&#21457;&#36865;&#21040;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;Q&amp;A&#25552;&#31034;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#35757;&#32451;&#38598;&#20013;&#30340;&#22270;&#20687;-&#31572;&#26696;&#23545;&#21644;&#30456;&#24212;&#30340;&#38382;&#39064;&#20316;&#20026;&#36755;&#20837;&#21644;&#36755;&#20986;&#26469;&#35757;&#32451;&#19968;&#20010;&#35270;&#35273;&#38382;&#39064;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the breakthrough of multi-modal large language models, answering complex visual questions that demand advanced reasoning abilities and world knowledge has become a much more important testbed for developing AI models than ever. However, equipping AI models with robust cross-modality reasoning ability remains challenging since the cognition scheme of humans has not been understood systematically. In this paper, we believe that if we can collect visual clues in the given image as much as possible, we will recognize the image more accurately, understand the question better, recall relevant knowledge more easily, and finally reason out the answer. We discover these rich visual clues by mining question-answer pairs in images and sending them into multi-modal large language models as prompts. We call the proposed method Q&amp;A Prompts. Specifically, we first use the image-answer pairs and the corresponding questions in the training set as inputs and outputs to train a visual question gener
&lt;/p&gt;</description></item><item><title>AST-T5&#26159;&#19968;&#31181;&#32467;&#26500;&#24863;&#30693;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#25277;&#35937;&#35821;&#27861;&#26641;&#65288;AST&#65289;&#26469;&#22686;&#24378;&#20195;&#30721;&#29983;&#25104;&#12289;&#36716;&#25442;&#21644;&#29702;&#35299;&#30340;&#33021;&#21147;&#12290;&#23427;&#20248;&#20110;&#20854;&#20182;&#21516;&#31561;&#22823;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#22312;&#20195;&#30721;&#21040;&#20195;&#30721;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2401.03003</link><description>&lt;p&gt;
AST-T5&#65306;&#38754;&#21521;&#20195;&#30721;&#29983;&#25104;&#21644;&#29702;&#35299;&#30340;&#32467;&#26500;&#24863;&#30693;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AST-T5: Structure-Aware Pretraining for Code Generation and Understanding. (arXiv:2401.03003v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03003
&lt;/p&gt;
&lt;p&gt;
AST-T5&#26159;&#19968;&#31181;&#32467;&#26500;&#24863;&#30693;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#25277;&#35937;&#35821;&#27861;&#26641;&#65288;AST&#65289;&#26469;&#22686;&#24378;&#20195;&#30721;&#29983;&#25104;&#12289;&#36716;&#25442;&#21644;&#29702;&#35299;&#30340;&#33021;&#21147;&#12290;&#23427;&#20248;&#20110;&#20854;&#20182;&#21516;&#31561;&#22823;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#22312;&#20195;&#30721;&#21040;&#20195;&#30721;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#30456;&#20851;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#28982;&#32780;&#35768;&#22810;&#27169;&#22411;&#23558;&#20195;&#30721;&#35270;&#20026;&#31616;&#21333;&#24207;&#21015;&#65292;&#24573;&#30053;&#20102;&#20854;&#32467;&#26500;&#21270;&#29305;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;AST-T5&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#39044;&#35757;&#32451;&#33539;&#24335;&#65292;&#21033;&#29992;&#25277;&#35937;&#35821;&#27861;&#26641;&#65288;AST&#65289;&#22686;&#24378;&#20102;&#20195;&#30721;&#29983;&#25104;&#12289;&#36716;&#25442;&#21644;&#29702;&#35299;&#12290;&#36890;&#36807;&#21160;&#24577;&#35268;&#21010;&#65292;&#25105;&#20204;&#30340;AST&#24863;&#30693;&#20998;&#21106;&#20445;&#30041;&#20102;&#20195;&#30721;&#32467;&#26500;&#65292;&#32780;AST&#24863;&#30693;&#36328;&#24230;&#30772;&#22351;&#30446;&#26631;&#20351;&#27169;&#22411;&#33021;&#22815;&#37325;&#24314;&#21508;&#31181;&#20195;&#30721;&#32467;&#26500;&#12290;&#19982;&#20854;&#20182;&#27169;&#22411;&#19981;&#21516;&#65292;AST-T5&#36991;&#20813;&#20102;&#22797;&#26434;&#30340;&#31243;&#24207;&#20998;&#26512;&#25110;&#26550;&#26500;&#26356;&#25913;&#65292;&#22240;&#27492;&#21487;&#20197;&#19982;&#20219;&#20309;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;Transformer&#26080;&#32541;&#38598;&#25104;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;AST-T5&#22312;&#21508;&#31181;&#20195;&#30721;&#30456;&#20851;&#20219;&#21153;&#20013;&#22987;&#32456;&#20248;&#20110;&#21516;&#31561;&#22823;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#32467;&#26500;&#24863;&#30693;&#20351;&#24471;AST-T5&#22312;&#20195;&#30721;&#21040;&#20195;&#30721;&#20219;&#21153;&#20013;&#29305;&#21035;&#24378;&#22823;&#65292;&#22312;Bugs2Fix&#20219;&#21153;&#30340;&#31934;&#30830;&#21305;&#37197;&#24471;&#20998;&#19978;&#36229;&#36807;CodeT5 2&#20010;&#28857;&#65292;&#24182;&#22312;CodeXGLUE&#20013;&#30340;Java-C#&#36716;&#25442;&#20219;&#21153;&#30340;&#31934;&#30830;&#21305;&#37197;&#24471;&#20998;&#19978;&#36229;&#36807;CodeT5 3&#20010;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have made significant advancements in code-related tasks, yet many LLMs treat code as simple sequences, neglecting its structured nature. We introduce AST-T5, a novel pretraining paradigm that leverages the Abstract Syntax Tree (AST) for enhanced code generation, transpilation, and understanding. Using dynamic programming, our AST-Aware Segmentation retains code structure, while our AST-Aware Span Corruption objective equips the model to reconstruct various code structures. Unlike other models, AST-T5 avoids intricate program analyses or architectural changes, so it integrates seamlessly with any encoder-decoder Transformer. Evaluations show that AST-T5 consistently outperforms similar-sized LMs across various code-related tasks. Structure-awareness makes AST-T5 particularly powerful in code-to-code tasks, surpassing CodeT5 by 2 points in exact match score for the Bugs2Fix task and by 3 points in exact match score for Java-C# Transpilation in CodeXGLUE. Our
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;&#30340;&#36739;&#39640;&#36136;&#37327;&#21442;&#32771;&#25991;&#29486;&#23545;&#20110;&#35780;&#20272;&#25351;&#26631;&#19982;&#20154;&#31867;&#35780;&#20215;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26356;&#22909;&#12290;&#27599;&#20010;&#27573;&#33853;&#24179;&#22343;&#20351;&#29992;7&#20010;&#21442;&#32771;&#25991;&#29486;&#26377;&#21161;&#20110;&#25552;&#21319;&#25152;&#26377;&#35780;&#20272;&#25351;&#26631;&#12290;&#19981;&#21516;&#36136;&#37327;&#30340;&#20379;&#24212;&#21830;&#21442;&#32771;&#25991;&#29486;&#21487;&#20197;&#28151;&#21512;&#20351;&#29992;&#26469;&#25552;&#39640;&#35780;&#20272;&#25351;&#26631;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#21487;&#29992;&#20110;&#22312;&#29305;&#23450;&#39044;&#31639;&#19979;&#21019;&#24314;&#21442;&#32771;&#25991;&#29486;&#30340;&#20849;&#20139;&#20219;&#21153;&#30340;&#35780;&#20272;&#32773;&#12290;</title><link>http://arxiv.org/abs/2401.01283</link><description>&lt;p&gt;
&#26426;&#22120;&#32763;&#35793;&#33258;&#21160;&#35780;&#20272;&#30340;&#21442;&#32771;&#25991;&#29486;&#36136;&#37327;&#21644;&#25968;&#37327;
&lt;/p&gt;
&lt;p&gt;
Quality and Quantity of Machine Translation References for Automated Metrics. (arXiv:2401.01283v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01283
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;&#30340;&#36739;&#39640;&#36136;&#37327;&#21442;&#32771;&#25991;&#29486;&#23545;&#20110;&#35780;&#20272;&#25351;&#26631;&#19982;&#20154;&#31867;&#35780;&#20215;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26356;&#22909;&#12290;&#27599;&#20010;&#27573;&#33853;&#24179;&#22343;&#20351;&#29992;7&#20010;&#21442;&#32771;&#25991;&#29486;&#26377;&#21161;&#20110;&#25552;&#21319;&#25152;&#26377;&#35780;&#20272;&#25351;&#26631;&#12290;&#19981;&#21516;&#36136;&#37327;&#30340;&#20379;&#24212;&#21830;&#21442;&#32771;&#25991;&#29486;&#21487;&#20197;&#28151;&#21512;&#20351;&#29992;&#26469;&#25552;&#39640;&#35780;&#20272;&#25351;&#26631;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#21487;&#29992;&#20110;&#22312;&#29305;&#23450;&#39044;&#31639;&#19979;&#21019;&#24314;&#21442;&#32771;&#25991;&#29486;&#30340;&#20849;&#20139;&#20219;&#21153;&#30340;&#35780;&#20272;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;&#25351;&#26631;&#36890;&#24120;&#20351;&#29992;&#20154;&#24037;&#32763;&#35793;&#26469;&#30830;&#23450;&#31995;&#32479;&#32763;&#35793;&#30340;&#36136;&#37327;&#12290;&#39046;&#22495;&#20869;&#30340;&#20849;&#35782;&#35748;&#20026;&#20154;&#24037;&#21442;&#32771;&#25991;&#29486;&#24212;&#20855;&#26377;&#24456;&#39640;&#30340;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27809;&#26377;&#25104;&#26412;&#25928;&#30410;&#20998;&#26512;&#21487;&#20197;&#25351;&#23548;&#35745;&#21010;&#25910;&#38598;&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;&#21442;&#32771;&#25991;&#29486;&#30340;&#20174;&#19994;&#32773;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36739;&#39640;&#36136;&#37327;&#30340;&#21442;&#32771;&#25991;&#29486;&#33021;&#22815;&#22312;&#27573;&#33853;&#32423;&#21035;&#19978;&#19982;&#20154;&#31867;&#35780;&#20215;&#30340;&#30456;&#20851;&#24615;&#26356;&#22909;&#12290;&#27599;&#20010;&#27573;&#33853;&#24179;&#22343;&#20351;&#29992;7&#20010;&#21442;&#32771;&#25991;&#29486;&#26377;&#21161;&#20110;&#25152;&#26377;&#35780;&#20272;&#25351;&#26631;&#30340;&#25552;&#21319;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#26469;&#33258;&#19981;&#21516;&#36136;&#37327;&#30340;&#20379;&#24212;&#21830;&#30340;&#21442;&#32771;&#25991;&#29486;&#21487;&#20197;&#28151;&#21512;&#20351;&#29992;&#65292;&#24182;&#25552;&#39640;&#35780;&#20272;&#25351;&#26631;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#36739;&#39640;&#36136;&#37327;&#30340;&#21442;&#32771;&#25991;&#29486;&#21046;&#20316;&#25104;&#26412;&#26356;&#39640;&#65292;&#25105;&#20204;&#23558;&#20854;&#35270;&#20026;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65306;&#22312;&#29305;&#23450;&#39044;&#31639;&#19979;&#65292;&#24212;&#35813;&#25910;&#38598;&#21738;&#20123;&#21442;&#32771;&#25991;&#29486;&#20197;&#26368;&#22823;&#21270;&#35780;&#20272;&#25351;&#26631;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#21487;&#29992;&#20110;&#22312;&#29305;&#23450;&#39044;&#31639;&#19979;&#21019;&#24314;&#21442;&#32771;&#25991;&#29486;&#30340;&#20849;&#20139;&#20219;&#21153;&#30340;&#35780;&#20272;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic machine translation metrics often use human translations to determine the quality system translations. Common wisdom in the field dictates that the human references should be of very high quality. However, there are no cost-benefit analyses that could be used to guide practitioners who plan to collect references for machine translation evaluation. We find that higher-quality references lead to better metric correlations with humans at the segment-level. Having up to 7 references per segment and taking their average helps all metrics. Interestingly, the references from vendors of different qualities can be mixed together and improve metric success. Higher quality references, however, cost more to create and we frame this as an optimization problem: given a specific budget, what references should be collected to maximize metric success. These findings can be used by evaluators of shared tasks when references need to be created under a certain budget.
&lt;/p&gt;</description></item><item><title>ChipNeMo&#36890;&#36807;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22312;&#24037;&#19994;&#33455;&#29255;&#35774;&#35745;&#20013;&#22823;&#24133;&#25552;&#21319;LLM&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23567;&#20102;&#27169;&#22411;&#23610;&#23544;&#65292;&#22312;&#24037;&#31243;&#21161;&#25163;&#12289;&#33050;&#26412;&#29983;&#25104;&#21644;&#32570;&#38519;&#20998;&#26512;&#31561;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2311.00176</link><description>&lt;p&gt;
ChipNeMo: &#29992;&#20110;&#33455;&#29255;&#35774;&#35745;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;LLMs
&lt;/p&gt;
&lt;p&gt;
ChipNeMo: Domain-Adapted LLMs for Chip Design. (arXiv:2311.00176v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00176
&lt;/p&gt;
&lt;p&gt;
ChipNeMo&#36890;&#36807;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22312;&#24037;&#19994;&#33455;&#29255;&#35774;&#35745;&#20013;&#22823;&#24133;&#25552;&#21319;LLM&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23567;&#20102;&#27169;&#22411;&#23610;&#23544;&#65292;&#22312;&#24037;&#31243;&#21161;&#25163;&#12289;&#33050;&#26412;&#29983;&#25104;&#21644;&#32570;&#38519;&#20998;&#26512;&#31561;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChipNeMo&#26088;&#22312;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24037;&#19994;&#33455;&#29255;&#35774;&#35745;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#19981;&#30452;&#25509;&#20351;&#29992;&#21830;&#19994;&#25110;&#24320;&#28304;LLMs&#65292;&#32780;&#26159;&#37319;&#29992;&#20197;&#19979;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#65306;&#23450;&#21046;&#20998;&#35789;&#22120;&#12289;&#39046;&#22495;&#33258;&#36866;&#24212;&#25345;&#32493;&#39044;&#35757;&#32451;&#12289;&#24102;&#26377;&#39046;&#22495;&#29305;&#23450;&#25351;&#20196;&#30340;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#21644;&#39046;&#22495;&#33258;&#36866;&#24212;&#26816;&#32034;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#33455;&#29255;&#35774;&#35745;&#30340;&#19977;&#20010;&#36873;&#23450;LLM&#24212;&#29992;&#19978;&#35780;&#20272;&#20102;&#36825;&#20123;&#26041;&#27861;&#65306;&#24037;&#31243;&#21161;&#25163;&#32842;&#22825;&#26426;&#22120;&#20154;&#12289;EDA&#33050;&#26412;&#29983;&#25104;&#20197;&#21450;&#32570;&#38519;&#25688;&#35201;&#21644;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#20123;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#20351;LLM&#22312;&#36825;&#19977;&#20010;&#24212;&#29992;&#20013;&#24615;&#33021;&#22823;&#24133;&#25552;&#21319;&#65292;&#22312;&#21508;&#31181;&#35774;&#35745;&#20219;&#21153;&#19978;&#21487;&#20197;&#23454;&#29616;&#39640;&#36798;5&#20493;&#30340;&#27169;&#22411;&#23610;&#23544;&#32553;&#20943;&#65292;&#21516;&#26102;&#20855;&#26377;&#31867;&#20284;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;&#24403;&#21069;&#30340;&#32467;&#26524;&#21644;&#29702;&#24819;&#32467;&#26524;&#20043;&#38388;&#36824;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;&#25105;&#20204;&#30456;&#20449;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#23558;&#26377;&#21161;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChipNeMo aims to explore the applications of large language models (LLMs) for industrial chip design. Instead of directly deploying off-the-shelf commercial or open-source LLMs, we instead adopt the following domain adaptation techniques: custom tokenizers, domain-adaptive continued pretraining, supervised fine-tuning (SFT) with domain-specific instructions, and domain-adapted retrieval models. We evaluate these methods on three selected LLM applications for chip design: an engineering assistant chatbot, EDA script generation, and bug summarization and analysis. Our results show that these domain adaptation techniques enable significant LLM performance improvements over general-purpose base models across the three evaluated applications, enabling up to 5x model size reduction with similar or better performance on a range of design tasks. Our findings also indicate that there's still room for improvement between our current results and ideal outcomes. We believe that further investigati
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GenTKG&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#26102;&#38388;&#36923;&#36753;&#35268;&#21017;&#30340;&#26816;&#32034;&#31574;&#30053;&#21644;&#36731;&#37327;&#32423;&#30340;&#21442;&#25968;&#25928;&#29575;&#25351;&#23548;&#65292;&#20811;&#26381;&#20102;&#22797;&#26434;&#30340;&#26102;&#38388;&#22270;&#25968;&#25454;&#32467;&#26500;&#21644;&#24222;&#22823;&#30340;&#25968;&#25454;&#37327;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.07793</link><description>&lt;p&gt;
GenTKG: &#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
GenTKG: Generative Forecasting on Temporal Knowledge Graph. (arXiv:2310.07793v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07793
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GenTKG&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#26102;&#38388;&#36923;&#36753;&#35268;&#21017;&#30340;&#26816;&#32034;&#31574;&#30053;&#21644;&#36731;&#37327;&#32423;&#30340;&#21442;&#25968;&#25928;&#29575;&#25351;&#23548;&#65292;&#20811;&#26381;&#20102;&#22797;&#26434;&#30340;&#26102;&#38388;&#22270;&#25968;&#25454;&#32467;&#26500;&#21644;&#24222;&#22823;&#30340;&#25968;&#25454;&#37327;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#24555;&#36895;&#21457;&#23637;&#24341;&#21457;&#20102;&#23545;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;(tKG)&#39046;&#22495;&#30340;&#20852;&#36259;&#65292;&#20854;&#20013;&#20256;&#32479;&#30340;&#22522;&#20110;&#23884;&#20837;&#21644;&#35268;&#21017;&#30340;&#27169;&#22411;&#21344;&#20027;&#23548;&#22320;&#20301;&#12290;&#30446;&#21069;&#20173;&#28982;&#23384;&#22312;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#39044;&#35757;&#32451;&#30340;LLM&#26159;&#21542;&#33021;&#22815;&#29702;&#35299;&#32467;&#26500;&#21270;&#30340;&#26102;&#38388;&#20851;&#31995;&#25968;&#25454;&#65292;&#24182;&#21462;&#20195;&#23427;&#20204;&#25104;&#20026;&#26102;&#38388;&#20851;&#31995;&#39044;&#27979;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#26102;&#38388;&#30693;&#35782;&#39044;&#27979;&#24341;&#20837;&#29983;&#25104;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#22312;&#22797;&#26434;&#30340;&#26102;&#38388;&#22270;&#25968;&#25454;&#32467;&#26500;&#21644;LLM&#21487;&#20197;&#22788;&#29702;&#30340;&#24207;&#21015;&#33258;&#28982;&#34920;&#36798;&#20043;&#38388;&#23384;&#22312;&#24040;&#22823;&#30340;&#40511;&#27807;&#65292;&#22312;tKG&#30340;&#24222;&#22823;&#25968;&#25454;&#37327;&#21644;&#24494;&#35843;LLM&#30340;&#24040;&#22823;&#35745;&#31639;&#25104;&#26412;&#20043;&#38388;&#20063;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#26694;&#26550;&#65292;&#31216;&#20026;GenTKG&#65292;&#23427;&#22312;tKG&#19978;&#25191;&#34892;&#29983;&#25104;&#24335;&#39044;&#27979;&#65292;&#32467;&#21512;&#20102;&#22522;&#20110;&#26102;&#38388;&#36923;&#36753;&#35268;&#21017;&#30340;&#26816;&#32034;&#31574;&#30053;&#21644;&#36731;&#37327;&#32423;&#30340;&#21442;&#25968;&#25928;&#29575;&#25351;&#23548;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;GenTKG&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advancements in large language models (LLMs) have ignited interest in the temporal knowledge graph (tKG) domain, where conventional carefully designed embedding-based and rule-based models dominate. The question remains open of whether pre-trained LLMs can understand structured temporal relational data and replace them as the foundation model for temporal relational forecasting. Therefore, we bring temporal knowledge forecasting into the generative setting. However, challenges occur in the huge chasms between complex temporal graph data structure and sequential natural expressions LLMs can handle, and between the enormous data sizes of tKGs and heavy computation costs of finetuning LLMs. To address these challenges, we propose a novel retrieval augmented generation framework that performs generative forecasting on tKGs named GenTKG, which combines a temporal logical rule-based retrieval strategy and lightweight parameter-efficient instruction tuning. Extensive experiments hav
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SFAVEL&#30340;&#26080;&#30417;&#30563;&#26694;&#26550;&#65292;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;&#23558;&#33258;&#30417;&#30563;&#29305;&#24449;&#36716;&#21270;&#20026;&#39640;&#36136;&#37327;&#30340;&#20027;&#24352;-&#20107;&#23454;&#23545;&#40784;&#65292;&#23454;&#29616;&#26080;&#30417;&#30563;&#20107;&#23454;&#39564;&#35777;&#12290;&#36825;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#65292;&#21516;&#26102;&#20445;&#30041;&#35821;&#26009;&#24211;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2309.16540</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;&#30340;&#20107;&#23454;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Fact Verification by Language Model Distillation. (arXiv:2309.16540v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SFAVEL&#30340;&#26080;&#30417;&#30563;&#26694;&#26550;&#65292;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;&#23558;&#33258;&#30417;&#30563;&#29305;&#24449;&#36716;&#21270;&#20026;&#39640;&#36136;&#37327;&#30340;&#20027;&#24352;-&#20107;&#23454;&#23545;&#40784;&#65292;&#23454;&#29616;&#26080;&#30417;&#30563;&#20107;&#23454;&#39564;&#35777;&#12290;&#36825;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#65292;&#21516;&#26102;&#20445;&#30041;&#35821;&#26009;&#24211;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#20107;&#23454;&#39564;&#35777;&#26088;&#22312;&#36890;&#36807;&#21487;&#38752;&#30693;&#35782;&#24211;&#20013;&#30340;&#35777;&#25454;&#26469;&#39564;&#35777;&#20027;&#24352;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#24418;&#24335;&#30340;&#25968;&#25454;&#27880;&#37322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#31639;&#27861;&#24517;&#39035;&#20026;&#27599;&#20010;&#20027;&#24352;&#29983;&#25104;&#26082;&#35821;&#20041;&#26126;&#30830;&#21448;&#32039;&#20945;&#30340;&#29305;&#24449;&#65292;&#20197;&#20415;&#19982;&#28304;&#20449;&#24687;&#36827;&#34892;&#35821;&#20041;&#23545;&#40784;&#12290;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#21069;&#32773;&#36890;&#36807;&#23398;&#20064;&#21253;&#21547;&#20027;&#24352;&#21450;&#20854;&#30456;&#24212;&#26631;&#31614;&#30340;&#27880;&#37322;&#35821;&#26009;&#24211;&#26469;&#35299;&#20915;&#23545;&#40784;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SFAVEL&#65288;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;&#30340;&#33258;&#30417;&#30563;&#20107;&#23454;&#39564;&#35777;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#26694;&#26550;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23558;&#33258;&#30417;&#30563;&#29305;&#24449;&#33976;&#39311;&#20026;&#39640;&#36136;&#37327;&#30340;&#20027;&#24352;-&#20107;&#23454;&#23545;&#40784;&#65292;&#32780;&#26080;&#38656;&#27880;&#37322;&#12290;&#36825;&#26159;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#30340;&#65292;&#35813;&#20989;&#25968;&#40723;&#21169;&#29305;&#24449;&#22312;&#20445;&#25345;&#35821;&#26009;&#24211;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#30340;&#21516;&#26102;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#20027;&#24352;&#21644;&#35777;&#25454;&#23545;&#40784;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36798;&#21040;&#26032;&#39062;&#30340;&#29366;&#24577;&#19968;.
&lt;/p&gt;
&lt;p&gt;
Unsupervised fact verification aims to verify a claim using evidence from a trustworthy knowledge base without any kind of data annotation. To address this challenge, algorithms must produce features for every claim that are both semantically meaningful, and compact enough to find a semantic alignment with the source information. In contrast to previous work, which tackled the alignment problem by learning over annotated corpora of claims and their corresponding labels, we propose SFAVEL (Self-supervised Fact Verification via Language Model Distillation), a novel unsupervised framework that leverages pre-trained language models to distil self-supervised features into high-quality claim-fact alignments without the need for annotations. This is enabled by a novel contrastive loss function that encourages features to attain high-quality claim and evidence alignments whilst preserving the semantic relationships across the corpora. Notably, we present results that achieve a new state-of-the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#22312;&#24341;&#23548;&#25506;&#32034;&#65288;IGE-LLMs&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;LLMs&#20316;&#20026;&#36741;&#21161;&#20869;&#22312;&#22870;&#21169;&#65292;&#35299;&#20915;&#20102;&#22797;&#26434;&#38271;&#35270;&#31243;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#22870;&#21169;&#31232;&#30095;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#36739;&#39640;&#30340;&#24615;&#33021;&#21644;&#27169;&#22359;&#21270;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.16347</link><description>&lt;p&gt;
&#22797;&#26434;&#38271;&#35270;&#31243;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#30340;&#20869;&#22312;&#35821;&#35328;&#24341;&#23548;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Intrinsic Language-Guided Exploration for Complex Long-Horizon Robotic Manipulation Tasks. (arXiv:2309.16347v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16347
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#22312;&#24341;&#23548;&#25506;&#32034;&#65288;IGE-LLMs&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;LLMs&#20316;&#20026;&#36741;&#21161;&#20869;&#22312;&#22870;&#21169;&#65292;&#35299;&#20915;&#20102;&#22797;&#26434;&#38271;&#35270;&#31243;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#22870;&#21169;&#31232;&#30095;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#36739;&#39640;&#30340;&#24615;&#33021;&#21644;&#27169;&#22359;&#21270;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#31232;&#30095;&#21644;&#22797;&#26434;&#30340;&#29615;&#22659;&#20013;&#38754;&#20020;&#22256;&#22659;&#65292;&#23588;&#20854;&#26159;&#22312;&#28041;&#21450;&#20247;&#22810;&#19981;&#21516;&#24207;&#21015;&#30340;&#38271;&#35270;&#31243;&#25805;&#20316;&#20219;&#21153;&#20013;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#22312;&#24341;&#23548;&#25506;&#32034;&#65288;IGE-LLMs&#65289;&#26694;&#26550;&#12290;&#36890;&#36807;&#21033;&#29992;LLMs&#20316;&#20026;&#36741;&#21161;&#20869;&#22312;&#22870;&#21169;&#65292;IGE-LLMs&#24341;&#23548;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#36807;&#31243;&#65292;&#20197;&#35299;&#20915;&#22797;&#26434;&#30340;&#38271;&#35270;&#31243;&#25805;&#20316;&#20219;&#21153;&#20013;&#22870;&#21169;&#31232;&#30095;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#20855;&#26377;&#25506;&#32034;&#25361;&#25112;&#30340;&#29615;&#22659;&#21644;&#19968;&#20010;&#21516;&#26102;&#38754;&#20020;&#25506;&#32034;&#21644;&#38271;&#35270;&#31243;&#25361;&#25112;&#30340;&#22797;&#26434;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#21644;&#30456;&#20851;&#30340;&#20869;&#22312;&#23398;&#20064;&#26041;&#27861;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;IGE-LLMs(i)&#22312;&#30456;&#20851;&#30340;&#20869;&#22312;&#26041;&#27861;&#21644;&#30452;&#25509;&#20351;&#29992;LLMs&#36827;&#34892;&#20915;&#31574;&#30340;&#24615;&#33021;&#19978;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#36739;&#39640;&#27700;&#24179;&#65292;(ii)&#21487;&#20197;&#19982;&#29616;&#26377;&#30340;&#23398;&#20064;&#26041;&#27861;&#30456;&#32467;&#21512;&#21644;&#20114;&#34917;&#65292;&#31361;&#20986;&#20854;&#27169;&#22359;&#21270;&#24615;&#33021;&#65292;(iii)&#23545;&#20110;&#19981;&#21516;&#30340;&#20869;&#22312;&#32553;&#25918;&#21442;&#25968;&#27604;&#36739;&#19981;&#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current reinforcement learning algorithms struggle in sparse and complex environments, most notably in long-horizon manipulation tasks entailing a plethora of different sequences. In this work, we propose the Intrinsically Guided Exploration from Large Language Models (IGE-LLMs) framework. By leveraging LLMs as an assistive intrinsic reward, IGE-LLMs guides the exploratory process in reinforcement learning to address intricate long-horizon with sparse rewards robotic manipulation tasks. We evaluate our framework and related intrinsic learning methods in an environment challenged with exploration, and a complex robotic manipulation task challenged by both exploration and long-horizons. Results show IGE-LLMs (i) exhibit notably higher performance over related intrinsic methods and the direct use of LLMs in decision-making, (ii) can be combined and complement existing learning methods highlighting its modularity, (iii) are fairly insensitive to different intrinsic scaling parameters, and 
&lt;/p&gt;</description></item><item><title>CoT-BERT&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24605;&#32500;&#38142;&#26465;&#22686;&#24378;&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#22788;&#29702;&#65292;&#24341;&#20837;&#24605;&#32500;&#38142;&#26465;&#30340;&#27010;&#24565;&#36827;&#34892;&#21521;&#37327;&#21270;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.11143</link><description>&lt;p&gt;
CoT-BERT: &#36890;&#36807;&#24605;&#32500;&#38142;&#26465;&#22686;&#24378;&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
CoT-BERT: Enhancing Unsupervised Sentence Representation through Chain-of-Thought. (arXiv:2309.11143v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11143
&lt;/p&gt;
&lt;p&gt;
CoT-BERT&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24605;&#32500;&#38142;&#26465;&#22686;&#24378;&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#22788;&#29702;&#65292;&#24341;&#20837;&#24605;&#32500;&#38142;&#26465;&#30340;&#27010;&#24565;&#36827;&#34892;&#21521;&#37327;&#21270;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#26088;&#22312;&#23558;&#36755;&#20837;&#21477;&#23376;&#36716;&#21270;&#20026;&#23500;&#21547;&#22797;&#26434;&#35821;&#20041;&#20449;&#24687;&#30340;&#22266;&#23450;&#38271;&#24230;&#21521;&#37327;&#65292;&#21516;&#26102;&#28040;&#38500;&#23545;&#26631;&#27880;&#25968;&#25454;&#30340;&#20381;&#36182;&#12290;&#36817;&#24180;&#26469;&#65292;&#22312;&#23545;&#27604;&#23398;&#20064;&#21644;&#25552;&#31034;&#24037;&#31243;&#30340;&#25512;&#21160;&#19979;&#65292;&#35813;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#26497;&#22823;&#22320;&#32553;&#23567;&#20102;&#26080;&#30417;&#30563;&#21644;&#26377;&#30417;&#30563;&#31574;&#30053;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20010;&#36712;&#36857;&#20013;&#65292;&#20173;&#28982;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#24605;&#32500;&#38142;&#26465;&#30340;&#28508;&#22312;&#33021;&#21147;&#12290;&#20026;&#20102;&#37322;&#25918;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;&#22914;BERT&#65289;&#20013;&#30340;&#28508;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21477;&#23376;&#34920;&#31034;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#65306;&#29702;&#35299;&#21644;&#25688;&#35201;&#12290;&#38543;&#21518;&#65292;&#21518;&#19968;&#38454;&#27573;&#30340;&#36755;&#20986;&#34987;&#21033;&#29992;&#20026;&#36755;&#20837;&#21477;&#23376;&#30340;&#21521;&#37327;&#21270;&#34920;&#31034;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#65292;&#25105;&#20204;&#23545;&#23545;&#27604;&#23398;&#20064;&#25439;&#22833;&#20989;&#25968;&#21644;&#27169;&#26495;&#21435;&#22122;&#25216;&#26415;&#36827;&#34892;&#20102;&#31934;&#32454;&#35843;&#25972;&#12290;&#20005;&#26684;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;CoT-BERT&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised sentence representation learning aims to transform input sentences into fixed-length vectors enriched with intricate semantic information while obviating the reliance on labeled data. Recent progress within this field, propelled by contrastive learning and prompt engineering, has significantly bridged the gap between unsupervised and supervised strategies. Nonetheless, the potential utilization of Chain-of-Thought, remains largely untapped within this trajectory. To unlock latent capabilities within pre-trained models, such as BERT, we propose a two-stage approach for sentence representation: comprehension and summarization. Subsequently, the output of the latter phase is harnessed as the vectorized representation of the input sentence. For further performance enhancement, we meticulously refine both the contrastive learning loss function and the template denoising technique for prompt engineering. Rigorous experimentation substantiates our method, CoT-BERT, transcending a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;SIB-200&#25968;&#25454;&#38598;&#65292;&#22312;200&#22810;&#31181;&#35821;&#35328;&#21644;&#26041;&#35328;&#20013;&#25552;&#20379;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#12289;&#20840;&#38754;&#30340;&#20027;&#39064;&#20998;&#31867;&#35780;&#20272;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#22635;&#34917;&#20102;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#39046;&#22495;&#20013;&#23545;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#32570;&#20047;&#65292;&#36890;&#36807;&#20840;&#30417;&#30563;&#12289;&#36328;&#35821;&#35328;&#36801;&#31227;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#30340;&#35780;&#20272;&#65292;&#21457;&#29616;&#24615;&#33021;&#20173;&#23384;&#22312;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2309.07445</link><description>&lt;p&gt;
SIB-200: &#21253;&#25324;200&#22810;&#31181;&#35821;&#35328;&#21644;&#26041;&#35328;&#30340;&#31616;&#21333;&#12289;&#20840;&#38754;&#21644;&#22823;&#22411;&#20027;&#39064;&#20998;&#31867;&#35780;&#20272;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
SIB-200: A Simple, Inclusive, and Big Evaluation Dataset for Topic Classification in 200+ Languages and Dialects. (arXiv:2309.07445v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;SIB-200&#25968;&#25454;&#38598;&#65292;&#22312;200&#22810;&#31181;&#35821;&#35328;&#21644;&#26041;&#35328;&#20013;&#25552;&#20379;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#12289;&#20840;&#38754;&#30340;&#20027;&#39064;&#20998;&#31867;&#35780;&#20272;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#22635;&#34917;&#20102;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#39046;&#22495;&#20013;&#23545;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#32570;&#20047;&#65292;&#36890;&#36807;&#20840;&#30417;&#30563;&#12289;&#36328;&#35821;&#35328;&#36801;&#31227;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#30340;&#35780;&#20272;&#65292;&#21457;&#29616;&#24615;&#33021;&#20173;&#23384;&#22312;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#22810;&#35821;&#35328;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#35780;&#20272;&#36890;&#24120;&#20165;&#38480;&#20110;&#19968;&#23567;&#37096;&#20998;&#24102;&#26377;&#21487;&#29992;&#25968;&#25454;&#38598;&#30340;&#35821;&#35328;&#65292;&#25490;&#38500;&#20102;&#35768;&#22810;&#36164;&#28304;&#21294;&#20047;&#30340;&#35821;&#35328;&#12290;&#26412;&#25991;&#21019;&#24314;&#20102;SIB-200&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#20027;&#39064;&#20998;&#31867;&#30340;&#22823;&#35268;&#27169;&#24320;&#25918;&#28304;&#20195;&#30721;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;200&#22810;&#31181;&#35821;&#35328;&#21644;&#26041;&#35328;&#65292;&#20197;&#24357;&#34917;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#32570;&#20047;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;&#23545;&#20110;SIB-200&#20013;&#28085;&#30422;&#30340;&#35768;&#22810;&#35821;&#35328;&#26469;&#35828;&#65292;&#36825;&#26159;&#39318;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;NLU&#35780;&#20272;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#22522;&#20110;Flores-200&#26426;&#22120;&#32763;&#35793;&#35821;&#26009;&#24211;&#65292;&#24182;&#23545;&#35813;&#35821;&#26009;&#24211;&#28085;&#30422;&#30340;&#20854;&#20182;203&#31181;&#35821;&#35328;&#36827;&#34892;&#20102;&#21477;&#23376;&#32423;&#27880;&#37322;&#12290;&#23613;&#31649;&#35813;&#20219;&#21153;&#31616;&#21333;&#65292;&#20294;&#25105;&#20204;&#22312;&#20840;&#30417;&#30563;&#35774;&#32622;&#12289;&#36328;&#35821;&#35328;&#36801;&#31227;&#35774;&#32622;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#35774;&#32622;&#19979;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#24615;&#33021;&#20173;&#23384;&#22312;&#36739;&#22823;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the progress we have recorded in the last few years in multilingual natural language processing, evaluation is typically limited to a small set of languages with available datasets which excludes a large number of low-resource languages. In this paper, we created SIB-200 -- a large-scale open-sourced benchmark dataset for topic classification in 200 languages and dialects to address the lack of evaluation dataset for Natural Language Understanding (NLU). For many of the languages covered in SIB-200, this is the first publicly available evaluation dataset for NLU. The dataset is based on Flores-200 machine translation corpus. We annotated the English portion of the dataset and extended the sentence-level annotation to the remaining 203 languages covered in the corpus. Despite the simplicity of this task, our evaluation in full-supervised setting, cross-lingual transfer setting and prompting of large language model setting show that there is still a large gap between the performa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#22303;&#32819;&#20854;&#35821;&#30340;&#31243;&#24207;&#25991;&#26412;&#36827;&#34892;&#26696;&#20363;&#30740;&#31350;&#65292;&#25193;&#23637;&#20102;&#22303;&#32819;&#20854;wikiHow&#30340;&#25945;&#31243;&#25968;&#37327;&#65292;&#24182;&#30740;&#31350;&#20102;&#20960;&#20010;&#19979;&#28216;&#20219;&#21153;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#65292;&#35821;&#35328;&#29305;&#23450;&#27169;&#22411;&#30456;&#23545;&#20110;&#22810;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#26126;&#26174;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2309.06698</link><description>&lt;p&gt;
&#29992;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#31243;&#24207;&#24615;&#35821;&#35328;&#29702;&#35299;&#22522;&#20934;&#27979;&#35797;&#65306;&#20197;&#22303;&#32819;&#20854;&#35821;&#20026;&#20363;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Procedural Language Understanding for Low-Resource Languages: A Case Study on Turkish. (arXiv:2309.06698v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06698
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#22303;&#32819;&#20854;&#35821;&#30340;&#31243;&#24207;&#25991;&#26412;&#36827;&#34892;&#26696;&#20363;&#30740;&#31350;&#65292;&#25193;&#23637;&#20102;&#22303;&#32819;&#20854;wikiHow&#30340;&#25945;&#31243;&#25968;&#37327;&#65292;&#24182;&#30740;&#31350;&#20102;&#20960;&#20010;&#19979;&#28216;&#20219;&#21153;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#65292;&#35821;&#35328;&#29305;&#23450;&#27169;&#22411;&#30456;&#23545;&#20110;&#22810;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#26126;&#26174;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#31243;&#24207;&#24615;&#33258;&#28982;&#35821;&#35328;&#65288;&#20363;&#22914;&#65292;&#36880;&#27493;&#35828;&#26126;&#65289;&#26159;&#25191;&#34892;&#21644;&#35268;&#21010;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#33521;&#35821;&#20013;&#23384;&#22312;&#20016;&#23500;&#30340;&#35821;&#26009;&#24211;&#21644;&#19979;&#28216;&#20219;&#21153;&#65292;&#20294;&#22823;&#22810;&#25968;&#35821;&#35328;&#32570;&#20047;&#36825;&#26679;&#30340;&#36164;&#28304;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23545;&#22303;&#32819;&#20854;&#31243;&#24207;&#25991;&#26412;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#33258;&#21160;&#32763;&#35793;&#24037;&#20855;&#23558;&#22303;&#32819;&#20854;wikiHow&#20013;&#30340;&#25945;&#31243;&#25968;&#37327;&#20174;2,000&#20010;&#25193;&#23637;&#21040;52,000&#20010;&#65292;&#32763;&#35793;&#36136;&#37327;&#21644;&#23545;&#21407;&#22987;&#21547;&#20041;&#30340;&#24544;&#23454;&#24615;&#30001;&#19987;&#23478;&#22242;&#38431;&#22312;&#19968;&#20010;&#38543;&#26426;&#38598;&#19978;&#36827;&#34892;&#39564;&#35777;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#35821;&#26009;&#24211;&#19978;&#29983;&#25104;&#20102;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#65292;&#20363;&#22914;&#38142;&#25509;&#25805;&#20316;&#12289;&#30446;&#26631;&#25512;&#29702;&#21644;&#25688;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#20219;&#21153;&#65292;&#25105;&#20204;&#36890;&#36807;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#29305;&#23450;&#27169;&#22411;&#65288;&#22914;TR-BART&#21644;BERTurk&#65289;&#20197;&#21450;&#22810;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;mBART&#12289;mT5&#21644;XLM&#65289;&#23454;&#29616;&#20102;&#24378;&#22522;&#32447;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#65292;&#35821;&#35328;&#29305;&#23450;&#27169;&#22411;&#22987;&#32456;&#20197;&#26174;&#33879;&#30340;&#20248;&#21183;&#32988;&#36807;&#22810;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding procedural natural language (e.g., step-by-step instructions) is a crucial step to execution and planning. However, while there are ample corpora and downstream tasks available in English, the field lacks such resources for most languages. To address this gap, we conduct a case study on Turkish procedural texts. We first expand the number of tutorials in Turkish wikiHow from 2,000 to 52,000 using automated translation tools, where the translation quality and loyalty to the original meaning are validated by a team of experts on a random set. Then, we generate several downstream tasks on the corpus, such as linking actions, goal inference, and summarization. To tackle these tasks, we implement strong baseline models via fine-tuning large language-specific models such as TR-BART and BERTurk, as well as multilingual models such as mBART, mT5, and XLM. We find that language-specific models consistently outperform their multilingual models by a significant margin across most pr
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#32447;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#25552;&#31034;&#35780;&#20272;&#19982;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#21644;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#65292;&#39044;&#27979;&#25552;&#31034;&#24615;&#33021;&#12289;&#25552;&#39640;&#25104;&#26412;&#25928;&#30410;&#12289;&#29983;&#25104;&#26131;&#35835;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.06553</link><description>&lt;p&gt;
&#31163;&#32447;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#19979;&#30340;&#25552;&#31034;&#35780;&#20272;&#19982;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Offline Prompt Evaluation and Optimization with Inverse Reinforcement Learning. (arXiv:2309.06553v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06553
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#32447;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#25552;&#31034;&#35780;&#20272;&#19982;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#21644;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#65292;&#39044;&#27979;&#25552;&#31034;&#24615;&#33021;&#12289;&#25552;&#39640;&#25104;&#26412;&#25928;&#30410;&#12289;&#29983;&#25104;&#26131;&#35835;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#21033;&#29992;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#20805;&#20998;&#25581;&#31034;LLMs&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#38656;&#35201;&#22312;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#30340;&#24191;&#38420;&#25628;&#32034;&#31354;&#38388;&#20013;&#36827;&#34892;&#23548;&#33322;&#12290;&#34429;&#28982;&#25552;&#31034;&#24037;&#31243;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20294;&#35797;&#38169;&#23581;&#35797;&#20013;&#25152;&#38656;&#30340;&#20154;&#24037;&#35774;&#35745;&#25552;&#31034;&#21644;&#30456;&#20851;&#25104;&#26412;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#20851;&#38190;&#26159;&#65292;&#25552;&#31034;&#20248;&#21270;&#30340;&#25928;&#29575;&#21462;&#20915;&#20110;&#26114;&#36149;&#30340;&#25552;&#31034;&#35780;&#20272;&#36807;&#31243;&#12290;&#26412;&#24037;&#20316;&#20171;&#32461;&#20102;Prompt-OIRL&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#31163;&#32447;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#24357;&#21512;&#26377;&#25928;&#25552;&#31034;&#35780;&#20272;&#21644;&#21487;&#36127;&#25285;&#24615;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#19987;&#23478;&#35780;&#20272;&#30340;&#31163;&#32447;&#25968;&#25454;&#38598;&#65292;&#36816;&#29992;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#33719;&#24471;&#19968;&#20010;&#38024;&#23545;&#31163;&#32447;&#12289;&#26597;&#35810;&#20381;&#36182;&#22411;&#25552;&#31034;&#35780;&#20272;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;Prompt-OIRL&#30340;&#20248;&#28857;&#26159;&#22810;&#26041;&#38754;&#30340;&#65306;&#23427;&#39044;&#27979;&#25552;&#31034;&#30340;&#24615;&#33021;&#65292;&#25104;&#26412;&#39640;&#25928;&#65292;&#29983;&#25104;&#26131;&#35835;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advances in the development of Large Language Models (LLMs) like ChatGPT have achieved remarkable performance by leveraging human expertise. Yet, fully eliciting LLMs' potential for complex tasks requires navigating the vast search space of natural language prompts. While prompt engineering has shown promise, the requisite human-crafted prompts in trial-and-error attempts and the associated costs pose significant challenges. Crucially, the efficiency of prompt optimization hinges on the costly procedure of prompt evaluation. This work introduces Prompt-OIRL, an approach rooted in offline inverse reinforcement learning that seeks to bridge the gap between effective prompt evaluation and affordability. Our method draws on offline datasets from expert evaluations, employing Inverse-RL to derive a reward model for offline, query-dependent prompt evaluations. The advantages of Prompt-OIRL are manifold: it predicts prompt performance, is cost-efficient, produces human-readable res
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#25991;&#26412;&#25366;&#25496;&#21644;&#32593;&#32476;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;&#19981;&#21516;&#37096;&#38376;&#20043;&#38388;&#30340;&#25216;&#26415;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#25216;&#26415;&#21019;&#26032;&#20013;&#65292;&#38388;&#25509;&#32852;&#31995;&#21644;&#30452;&#25509;&#32852;&#31995;&#21516;&#31561;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2308.00014</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#30456;&#20114;&#20381;&#36182;&#30340;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
A new mapping of technological interdependence. (arXiv:2308.00014v1 [econ.EM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#25991;&#26412;&#25366;&#25496;&#21644;&#32593;&#32476;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;&#19981;&#21516;&#37096;&#38376;&#20043;&#38388;&#30340;&#25216;&#26415;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#25216;&#26415;&#21019;&#26032;&#20013;&#65292;&#38388;&#25509;&#32852;&#31995;&#21644;&#30452;&#25509;&#32852;&#31995;&#21516;&#31561;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21738;&#20123;&#25216;&#26415;&#32852;&#31995;&#24433;&#21709;&#20102;&#37096;&#38376;&#30340;&#21019;&#26032;&#33021;&#21147;&#65311;&#36825;&#20123;&#25928;&#24212;&#22914;&#20309;&#36890;&#36807;&#25216;&#26415;&#31354;&#38388;&#20256;&#36882;&#65311;&#26412;&#25991;&#20351;&#29992;&#26032;&#39062;&#30340;&#25991;&#26412;&#25366;&#25496;&#21644;&#32593;&#32476;&#20998;&#26512;&#26041;&#27861;&#22238;&#31572;&#20102;&#36825;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#32654;&#22269;&#19987;&#21033;&#21830;&#26631;&#23616;&#65288;USPTO&#65289;&#25480;&#20104;&#30340;650&#19975;&#39033;&#19987;&#21033;&#30340;&#25991;&#26412;&#65292;&#24182;&#24212;&#29992;&#32593;&#32476;&#20998;&#26512;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;&#21322;&#20010;&#19990;&#32426;&#65288;&#20174;1976&#24180;&#21040;2021&#24180;&#65289;&#26399;&#38388;&#19981;&#21516;&#37096;&#38376;&#20043;&#38388;&#30340;&#25216;&#26415;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#23384;&#22312;&#20110;&#25216;&#26415;&#39046;&#22495;&#20043;&#38388;&#30340;&#20840;&#35889;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#35777;&#26126;&#19987;&#21033;&#25991;&#26412;&#21253;&#21547;&#20102;&#24448;&#24448;&#26080;&#27861;&#36890;&#36807;&#20256;&#32479;&#30340;&#21019;&#26032;&#25351;&#26631;&#65288;&#20363;&#22914;&#19987;&#21033;&#24341;&#29992;&#65289;&#25429;&#25417;&#21040;&#30340;&#20016;&#23500;&#20449;&#24687;&#12290;&#36890;&#36807;&#20351;&#29992;&#32593;&#32476;&#20998;&#26512;&#65292;&#25105;&#20204;&#35760;&#24405;&#20102;&#38388;&#25509;&#32852;&#31995;&#21644;&#30452;&#25509;&#32852;&#31995;&#21516;&#31561;&#37325;&#35201;&#65292;&#24182;&#19988;&#21069;&#32773;&#22823;&#37096;&#20998;&#20351;&#29992;&#20256;&#32479;&#30340;&#38388;&#25509;&#32852;&#31995;&#24230;&#37327;&#26041;&#27861;&#65288;&#22914;Leontief&#36870;&#30697;&#38453;&#65289;&#24448;&#24448;&#20250;&#34987;&#38544;&#34255;&#12290;&#26368;&#21518;&#65292;&#22522;&#20110;&#20914;&#20987;&#21709;&#24212;&#20998;&#26512;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Which technological linkages affect the sector's ability to innovate? How do these effects transmit through the technology space? This paper answers these two key questions using novel methods of text mining and network analysis. We examine technological interdependence across sectors over a period of half a century (from 1976 to 2021) by analyzing the text of 6.5 million patents granted by the United States Patent and Trademark Office (USPTO), and applying network analysis to uncover the full spectrum of linkages existing across technology areas. We demonstrate that patent text contains a wealth of information often not captured by traditional innovation metrics, such as patent citations. By using network analysis, we document that indirect linkages are as important as direct connections and that the former would remain mostly hidden using more traditional measures of indirect linkages, such as the Leontief inverse matrix. Finally, based on an impulse-response analysis, we illustrate 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#25554;&#20837;&#24335;&#26694;&#26550;&#65292;&#29992;&#20110;&#20248;&#21270;CTC&#27169;&#22411;&#20013;&#30340;&#25152;&#38656;&#23646;&#24615;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#34917;&#20805;&#39069;&#22806;&#30340;&#25439;&#22833;&#39033;&#26469;&#20248;&#20808;&#32771;&#34385;&#31526;&#21512;&#25152;&#38656;&#23646;&#24615;&#30340;&#23545;&#40784;&#65292;&#24182;&#19981;&#38656;&#35201;&#20462;&#25913;CTC&#25439;&#22833;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2307.01715</link><description>&lt;p&gt;
&#31526;&#21512;&#30446;&#26631;&#65306;&#20351;&#29992;&#36890;&#29992;&#30340;&#25554;&#20837;&#24335;&#26694;&#26550;&#22312;CTC&#27169;&#22411;&#20013;&#20248;&#21270;&#25152;&#38656;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
Align With Purpose: Optimize Desired Properties in CTC Models with a General Plug-and-Play Framework. (arXiv:2307.01715v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#25554;&#20837;&#24335;&#26694;&#26550;&#65292;&#29992;&#20110;&#20248;&#21270;CTC&#27169;&#22411;&#20013;&#30340;&#25152;&#38656;&#23646;&#24615;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#34917;&#20805;&#39069;&#22806;&#30340;&#25439;&#22833;&#39033;&#26469;&#20248;&#20808;&#32771;&#34385;&#31526;&#21512;&#25152;&#38656;&#23646;&#24615;&#30340;&#23545;&#40784;&#65292;&#24182;&#19981;&#38656;&#35201;&#20462;&#25913;CTC&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#25509;&#20027;&#20041;&#26102;&#38388;&#20998;&#31867;&#65288;CTC&#65289;&#26159;&#35757;&#32451;&#30417;&#30563;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#24191;&#27867;&#20351;&#29992;&#30340;&#20934;&#21017;&#12290;&#23427;&#36890;&#36807;&#23558;&#23436;&#32654;&#23545;&#40784;&#65288;&#20135;&#29983;&#22522;&#26412;&#20107;&#23454;&#65289;&#30340;&#36793;&#38469;&#21270;&#26469;&#23398;&#20064;&#36755;&#20837;&#21644;&#36755;&#20986;&#24207;&#21015;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#31216;&#20026;&#23545;&#20854;&#65292;&#20197;&#20195;&#20215;&#19981;&#23436;&#32654;&#23545;&#40784;&#12290;&#36825;&#31181;&#23545;&#23436;&#32654;&#21644;&#19981;&#23436;&#32654;&#23545;&#40784;&#30340;&#20108;&#20803;&#21306;&#20998;&#26080;&#27861;&#25429;&#25417;&#21040;&#22312;&#20854;&#20182;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#30340;&#20854;&#20182;&#20851;&#38190;&#23545;&#40784;&#23646;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$\textit{Align With Purpose}$&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22686;&#24378;CTC&#26465;&#20214;&#19979;&#35757;&#32451;&#27169;&#22411;&#20013;&#25152;&#38656;&#23646;&#24615;&#30340;$\textbf{&#36890;&#29992;&#25554;&#20837;&#24335;&#26694;&#26550;}$&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#39069;&#22806;&#30340;&#25439;&#22833;&#39033;&#26469;&#34917;&#20805;CTC&#26469;&#20248;&#20808;&#32771;&#34385;&#31526;&#21512;&#25152;&#38656;&#23646;&#24615;&#30340;&#23545;&#40784;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#24178;&#39044;CTC&#25439;&#22833;&#20989;&#25968;&#65292;&#33021;&#22815;&#36731;&#26494;&#20248;&#21270;&#21508;&#31181;&#23646;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#21306;&#20998;&#23436;&#32654;&#21644;&#19981;&#23436;&#32654;&#30340;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Connectionist Temporal Classification (CTC) is a widely used criterion for training supervised sequence-to-sequence (seq2seq) models. It enables learning the relations between input and output sequences, termed alignments, by marginalizing over perfect alignments (that yield the ground truth), at the expense of imperfect alignments. This binary differentiation of perfect and imperfect alignments falls short of capturing other essential alignment properties that hold significance in other real-world applications. Here we propose $\textit{Align With Purpose}$, a $\textbf{general Plug-and-Play framework}$ for enhancing a desired property in models trained with the CTC criterion. We do that by complementing the CTC with an additional loss term that prioritizes alignments according to a desired property. Our method does not require any intervention in the CTC loss function, enables easy optimization of a variety of properties, and allows differentiation between both perfect and imperfect al
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;60,000&#26465;&#27874;&#26031;&#35821;&#30340;&#31038;&#20132;&#24494;&#21338;&#21475;&#35821;&#25991;&#26412;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#27169;&#22411;&#65292;&#29992;&#20110;&#26356;&#26377;&#25928;&#22320;&#20998;&#26512;&#31038;&#20132;&#24494;&#21338;&#20013;&#30340;&#21475;&#35821;&#25991;&#26412;&#24773;&#24863;&#12290;</title><link>http://arxiv.org/abs/2306.12679</link><description>&lt;p&gt;
&#26500;&#24314;&#27874;&#26031;&#31038;&#20132;&#24494;&#21338;&#21475;&#35821;&#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Constructing Colloquial Dataset for Persian Sentiment Analysis of Social Microblogs. (arXiv:2306.12679v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;60,000&#26465;&#27874;&#26031;&#35821;&#30340;&#31038;&#20132;&#24494;&#21338;&#21475;&#35821;&#25991;&#26412;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#27169;&#22411;&#65292;&#29992;&#20110;&#26356;&#26377;&#25928;&#22320;&#20998;&#26512;&#31038;&#20132;&#24494;&#21338;&#20013;&#30340;&#21475;&#35821;&#25991;&#26412;&#24773;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#35328;&#65306;&#24494;&#21338;&#32593;&#31449;&#24050;&#25104;&#20026;&#24773;&#24863;&#20998;&#26512;&#21644;&#35266;&#28857;&#25366;&#25496;&#30340;&#20016;&#23500;&#25968;&#25454;&#28304;&#12290;&#20294;&#30001;&#20110;&#24494;&#21338;&#24120;&#32570;&#23569;&#21477;&#27861;&#19968;&#33268;&#30340;&#26415;&#35821;&#21644;&#20195;&#34920;&#24615;&#65292;&#22240;&#27492;&#24773;&#24863;&#20998;&#31867;&#36890;&#24120;&#25928;&#29575;&#20302;&#19979;&#12290;&#27874;&#26031;&#35821;&#35328;&#26377;&#20854;&#29420;&#29305;&#30340;&#29305;&#24615;&#65292;&#38656;&#35201;&#29420;&#29305;&#30340;&#27880;&#37322;&#25968;&#25454;&#21644;&#27169;&#22411;&#26469;&#23436;&#25104;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#12290;&#26041;&#27861;&#65306;&#26412;&#25991;&#39318;&#20808;&#36890;&#36807;&#21327;&#20316;&#29615;&#22659;&#21644;&#20869;&#37096;&#26469;&#28304;&#26041;&#24335;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;ITRC-Opinion&#30340;&#29992;&#25143;&#24847;&#35265;&#25968;&#25454;&#38598;&#12290;&#20854;&#27425;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#27169;&#22411;&#65292;&#29992;&#20110;&#26356;&#26377;&#25928;&#22320;&#20998;&#26512;&#31038;&#20132;&#24494;&#21338;&#20013;&#30340;&#21475;&#35821;&#25991;&#26412;&#24773;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
Introduction: Microblogging websites have massed rich data sources for sentiment analysis and opinion mining. In this regard, sentiment classification has frequently proven inefficient because microblog posts typically lack syntactically consistent terms and representatives since users on these social networks do not like to write lengthy statements. Also, there are some limitations to low-resource languages. The Persian language has exceptional characteristics and demands unique annotated data and models for the sentiment analysis task, which are distinctive from text features within the English dialect. Method: This paper first constructs a user opinion dataset called ITRC-Opinion by collaborative environment and insource way. Our dataset contains 60,000 informal and colloquial Persian texts from social microblogs such as Twitter and Instagram. Second, this study proposes a new deep convolutional neural network (CNN) model for more effective sentiment analysis of colloquial text in s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#36130;&#21153;&#20449;&#24687;&#25552;&#21462;&#30340;&#26694;&#26550;&#65288;AFIE&#65289;&#65292;&#29992;&#20110;&#25552;&#21462;&#28151;&#21512;&#38271;&#25991;&#26723;&#20013;&#30340;&#20851;&#38190;&#19994;&#32489;&#25351;&#26631;&#65288;KPI&#65289;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;LLMs&#22686;&#24378;&#20102;&#36130;&#21153;&#25253;&#21578;&#20449;&#24687;&#30340;&#29702;&#35299;&#21644;&#25552;&#21462;&#33021;&#21147;&#65292;&#24182;&#32463;&#36807;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#35777;&#26126;&#20854;&#22312;GPT-3.5&#21644;GPT-4&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#30456;&#23545;&#20110;&#26420;&#32032;&#26041;&#27861;&#65292;&#24179;&#22343;&#31934;&#24230;&#25552;&#39640;&#20102;53.94&#65285;&#21644;33.77&#65285;&#12290;</title><link>http://arxiv.org/abs/2305.16344</link><description>&lt;p&gt;
&#21033;&#29992;LLMs&#20174;&#28151;&#21512;&#38271;&#25991;&#26723;&#20013;&#26816;&#32034;KPI&#30340;&#20840;&#38754;&#26694;&#26550;&#19982;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Leveraging LLMs for KPIs Retrieval from Hybrid Long-Document: A Comprehensive Framework and Dataset. (arXiv:2305.16344v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#36130;&#21153;&#20449;&#24687;&#25552;&#21462;&#30340;&#26694;&#26550;&#65288;AFIE&#65289;&#65292;&#29992;&#20110;&#25552;&#21462;&#28151;&#21512;&#38271;&#25991;&#26723;&#20013;&#30340;&#20851;&#38190;&#19994;&#32489;&#25351;&#26631;&#65288;KPI&#65289;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;LLMs&#22686;&#24378;&#20102;&#36130;&#21153;&#25253;&#21578;&#20449;&#24687;&#30340;&#29702;&#35299;&#21644;&#25552;&#21462;&#33021;&#21147;&#65292;&#24182;&#32463;&#36807;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#35777;&#26126;&#20854;&#22312;GPT-3.5&#21644;GPT-4&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#30456;&#23545;&#20110;&#26420;&#32032;&#26041;&#27861;&#65292;&#24179;&#22343;&#31934;&#24230;&#25552;&#39640;&#20102;53.94&#65285;&#21644;33.77&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25991;&#26412;&#29702;&#35299;&#21644;&#34920;&#26684;&#25512;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#23545;&#21253;&#21547;&#25991;&#26412;&#21644;&#34920;&#26684;&#25968;&#25454;&#30340;&#28151;&#21512;&#25991;&#26412;&#30340;&#29702;&#35299;&#21644;&#20998;&#26512;&#33021;&#21147;&#20173;&#26410;&#34987;&#20805;&#20998;&#21457;&#25496;&#12290;&#26412;&#30740;&#31350;&#19987;&#27880;&#20110;&#21033;&#29992;LLMs&#30340;&#28508;&#21147;&#65292;&#20174;&#28151;&#26434;&#30340;&#38271;&#22411;&#36130;&#21153;&#25253;&#21578;&#20013;&#29702;&#35299;&#20851;&#38190;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#21160;&#21270;&#36130;&#21153;&#20449;&#24687;&#25552;&#21462;&#65288;AFIE&#65289;&#26694;&#26550;&#65292;&#22686;&#24378;&#20102;LLMs&#29702;&#35299;&#21644;&#25552;&#21462;&#36130;&#21153;&#25253;&#21578;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35780;&#20272;AFIE&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#37329;&#34701;&#25253;&#21578;&#25968;&#20540;&#25552;&#21462;&#65288;FINE&#65289;&#25968;&#25454;&#38598;&#65292;&#24182;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;GPT-3.5&#21644;GPT-4&#19978;&#24471;&#21040;&#20102;&#26377;&#25928;&#39564;&#35777;&#65292;&#30456;&#23545;&#20110;&#26420;&#32032;&#26041;&#27861;&#65292;&#24179;&#22343;&#31934;&#24230;&#25552;&#39640;&#20102;53.94&#65285;&#21644;33.77&#65285;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;AFIE&#26694;&#26550;&#20026;&#20174;&#22797;&#26434;&#30340;&#28151;&#21512;&#25991;&#26723;&#20013;&#33258;&#21160;&#25552;&#21462;&#25968;&#20540;&#25552;&#20379;&#20102;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) demonstrate exceptional performance in textual understanding and tabular reasoning tasks. However, their ability to comprehend and analyze hybrid text, containing textual and tabular data, remains underexplored. In this research, we specialize in harnessing the potential of LLMs to comprehend critical information from financial reports, which are hybrid long-documents. We propose an Automated Financial Information Extraction (AFIE) framework that enhances LLMs' ability to comprehend and extract information from financial reports. To evaluate AFIE, we develop a Financial Reports Numerical Extraction (FINE) dataset and conduct an extensive experimental analysis. Our framework is effectively validated on GPT-3.5 and GPT-4, yielding average accuracy increases of 53.94% and 33.77%, respectively, compared to a naive method. These results suggest that the AFIE framework offers accuracy for automated numerical extraction from complex, hybrid documents.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#65292;&#21517;&#20026;INDust&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#20110;&#21253;&#21547;&#38169;&#35823;&#20449;&#24687;&#30340;&#25351;&#20196;&#30340;&#25269;&#25239;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#21069;&#30340;LLMs&#24456;&#23481;&#26131;&#34987;&#27450;&#39575;&#65292;&#22240;&#27492;&#37319;&#29992;&#33258;&#25105;&#25209;&#21028;&#25552;&#31034;&#30340;&#26041;&#27861;&#26469;&#28608;&#21169;LLMs&#19981;&#20165;&#23545;&#33258;&#24049;&#36827;&#34892;&#25209;&#35780;&#65292;&#32780;&#19988;&#23545;&#29992;&#25143;&#36827;&#34892;&#25209;&#35780;&#12290;</title><link>http://arxiv.org/abs/2305.13733</link><description>&lt;p&gt;
&#24040;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#25209;&#21028;&#25552;&#31034;&#29992;&#20110;&#24402;&#32435;&#25945;&#23398;&#25351;&#23548;
&lt;/p&gt;
&lt;p&gt;
Self-Critique Prompting with Large Language Models for Inductive Instructions. (arXiv:2305.13733v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13733
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#65292;&#21517;&#20026;INDust&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#20110;&#21253;&#21547;&#38169;&#35823;&#20449;&#24687;&#30340;&#25351;&#20196;&#30340;&#25269;&#25239;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#21069;&#30340;LLMs&#24456;&#23481;&#26131;&#34987;&#27450;&#39575;&#65292;&#22240;&#27492;&#37319;&#29992;&#33258;&#25105;&#25209;&#21028;&#25552;&#31034;&#30340;&#26041;&#27861;&#26469;&#28608;&#21169;LLMs&#19981;&#20165;&#23545;&#33258;&#24049;&#36827;&#34892;&#25209;&#35780;&#65292;&#32780;&#19988;&#23545;&#29992;&#25143;&#36827;&#34892;&#25209;&#35780;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37327;&#30340;&#24037;&#20316;&#37117;&#34987;&#25552;&#20986;&#26469;&#25552;&#39640;&#25110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23454;&#29616;&#29992;&#25143;&#25351;&#20196;&#30340;&#33021;&#21147;&#12290; &#28982;&#32780;&#65292;&#23427;&#20204;&#24573;&#30053;&#20102;&#29992;&#25143;&#36755;&#20837;&#21487;&#33021;&#22240;&#29992;&#25143;&#30340;&#38169;&#35823;&#20449;&#24565;&#25110;&#24694;&#24847;&#24847;&#22270;&#32780;&#22266;&#26377;&#22320;&#21253;&#21547;&#19981;&#27491;&#30830;&#30340;&#20449;&#24687;&#30340;&#21487;&#33021;&#24615;&#12290; &#30450;&#30446;&#22320;&#36981;&#24490;&#29992;&#25143;&#30340;&#38169;&#35823;&#20869;&#23481;&#23558;&#23548;&#33268;&#27450;&#39575;&#21644;&#20260;&#23475;&#12290; &#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#65292;&#30001;&#24402;&#32435;&#25351;&#20196;&#65288;INDust&#65289;&#32452;&#25104;&#65292;&#20197;&#35780;&#20272;LLMs&#26159;&#21542;&#33021;&#22815;&#25269;&#25239;&#36825;&#20123;&#25351;&#20196;&#12290; INDust&#21253;&#25324;&#19977;&#20010;&#31867;&#21035;&#30340;15K&#25351;&#20196;&#65306;&#20107;&#23454;&#26680;&#26597;&#25351;&#20196;&#65292;&#22522;&#20110;&#38169;&#35823;&#21069;&#25552;&#30340;&#38382;&#39064;&#21644;&#22522;&#20110;&#38169;&#35823;&#21069;&#25552;&#30340;&#21019;&#24847;&#25351;&#20196;&#12290; &#25105;&#20204;&#23545;&#20960;&#20010;&#24378;&#22823;&#30340;LLMs&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#24403;&#21069;&#30340;LLMs&#21487;&#20197;&#36731;&#26131;&#22320;&#34987;INDust&#27450;&#39575;&#65292;&#29983;&#25104;&#35823;&#23548;&#24615;&#21644;&#24694;&#24847;&#30340;&#38472;&#36848;&#12290; &#22240;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;&#33258;&#25105;&#25209;&#21028;&#25552;&#31034;&#65292;&#20197;&#28608;&#21169;LLMs&#19981;&#20165;&#20687;&#20197;&#21069;&#30340;&#24037;&#20316;&#37027;&#26679;&#23545;&#33258;&#24049;&#36827;&#34892;&#25209;&#35780;&#65292;&#32780;&#19988;&#23545;&#29992;&#25143;&#36827;&#34892;&#25209;&#35780;&#65292;&#23427;&#23637;&#31034;&#20986;r&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous works are proposed to improve or evaluate the capabilities of Large language models (LLMs) to fulfill user instructions. However, they neglect the possibility that user inputs may inherently contain incorrect information due to users' false beliefs or malicious intents. In this way, blindly adhering to users' false content will cause deception and harm. To address this problem, we propose a challenging benchmark consisting of Inductive Instructions (INDust) to evaluate whether LLMs could resist these instructions. The INDust includes 15K instructions across three categories: Fact-Checking Instructions, Questions based on False Premises, and Creative Instructions based on False Premises. Our experiments on several strong LLMs reveal that current LLMs can be easily deceived by INDust into generating misleading and malicious statements. Hence we employ Self-Critique prompting to encourage LLMs to not only critique themselves like in previous works but also the users, which show r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#37096;&#32626;&#22522;&#20110;BERT&#30340;NLP&#27169;&#22411;&#30340;&#25361;&#25112;&#65292;&#24182;&#24471;&#20986;&#32467;&#35770;&#65306;&#34429;&#28982;DistilBERT&#21644;TinyBERT&#31561;&#36731;&#37327;&#32423;&#27169;&#22411;&#30456;&#23545;&#21344;&#29992;&#26356;&#23569;&#20869;&#23384;&#65292;&#20294;&#23427;&#20204;&#22312;&#22797;&#26434;&#30340;NLP&#20219;&#21153;&#19978;&#34920;&#29616;&#36739;&#24046;&#65307;ResNet-based BERT&#27169;&#22411;&#21487;&#20197;&#22312;&#31934;&#24230;&#21644;&#36164;&#28304;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#33391;&#22909;&#30340;&#24179;&#34913;&#65292;&#36866;&#21512;&#22312;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#37096;&#32626;&#12290;</title><link>http://arxiv.org/abs/2304.11520</link><description>&lt;p&gt;
&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#37096;&#32626;&#22522;&#20110;BERT&#30340;NLP&#27169;&#22411;&#30340;&#25361;&#25112;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
Exploring Challenges of Deploying BERT-based NLP Models in Resource-Constrained Embedded Devices. (arXiv:2304.11520v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#37096;&#32626;&#22522;&#20110;BERT&#30340;NLP&#27169;&#22411;&#30340;&#25361;&#25112;&#65292;&#24182;&#24471;&#20986;&#32467;&#35770;&#65306;&#34429;&#28982;DistilBERT&#21644;TinyBERT&#31561;&#36731;&#37327;&#32423;&#27169;&#22411;&#30456;&#23545;&#21344;&#29992;&#26356;&#23569;&#20869;&#23384;&#65292;&#20294;&#23427;&#20204;&#22312;&#22797;&#26434;&#30340;NLP&#20219;&#21153;&#19978;&#34920;&#29616;&#36739;&#24046;&#65307;ResNet-based BERT&#27169;&#22411;&#21487;&#20197;&#22312;&#31934;&#24230;&#21644;&#36164;&#28304;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#33391;&#22909;&#30340;&#24179;&#34913;&#65292;&#36866;&#21512;&#22312;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#37096;&#32626;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;BERT&#30340;&#31070;&#32463;&#26550;&#26500;&#24050;&#32463;&#25104;&#20026;&#35768;&#22810;&#19979;&#28216;NLP&#20219;&#21153;&#30340;&#27969;&#34892;&#20808;&#36827;&#25216;&#26415;&#22522;&#20934;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26550;&#26500;&#23545;&#25968;&#25454;&#20381;&#36182;&#24615;&#24378;&#65292;&#21344;&#29992;&#22823;&#37327;&#20869;&#23384;&#21644;&#33021;&#37327;&#65292;&#32463;&#24120;&#38459;&#30861;&#23427;&#20204;&#22312;&#35768;&#22810;&#23454;&#26102;&#12289;&#36164;&#28304;&#21463;&#38480;&#30340;&#24212;&#29992;&#31243;&#24207;&#20013;&#30340;&#37096;&#32626;&#12290;&#29616;&#26377;&#30340;BERT&#36731;&#37327;&#32423;&#29256;&#26412;&#65288;&#20363;&#22914;DistilBERT&#21644;TinyBERT&#65289;&#36890;&#24120;&#22312;&#22797;&#26434;&#30340;NLP&#20219;&#21153;&#19978;&#26080;&#27861;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#20174;&#35774;&#35745;&#24072;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#35201;&#20026;&#29305;&#23450;&#30340;NLP&#20219;&#21153;&#20351;&#29992;&#20309;&#31181;&#8220;&#27491;&#30830;&#30340;&#8221;&#22522;&#20110;BERT&#30340;&#26550;&#26500;&#65292;&#20197;&#22312;&#36164;&#28304;&#21487;&#29992;&#24615;&#21644;&#26368;&#32456;&#29992;&#25143;&#38656;&#27714;&#30340;&#26368;&#23567;&#31934;&#24230;&#20043;&#38388;&#23454;&#29616;&#26368;&#20339;&#26435;&#34913;&#65292;&#23578;&#19981;&#30830;&#23450;&#12290;&#31995;&#32479;&#24037;&#31243;&#24072;&#24517;&#39035;&#33457;&#36153;&#22823;&#37327;&#26102;&#38388;&#36827;&#34892;&#35797;&#38169;&#23454;&#39564;&#65292;&#20197;&#25214;&#21040;&#21512;&#36866;&#30340;&#31572;&#26696;&#12290;&#26412;&#25991;&#22312;&#19981;&#21516;&#30340;&#36164;&#28304;&#38480;&#21046;&#21644;&#31934;&#24230;&#39044;&#31639;&#19979;&#23545;BERT-based&#27169;&#22411;&#36827;&#34892;&#20102;&#25506;&#31350;&#24615;&#30740;&#31350;&#65292;&#20197;&#24471;&#20986;&#26377;&#20851;&#27492;&#36164;&#28304;/&#31934;&#24230;&#26435;&#34913;&#30340;&#32463;&#39564;&#24615;&#35266;&#23519;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#34429;&#28982;DistilBERT&#21644;TinyBERT&#31561;&#26356;&#36731;&#37327;&#32423;&#30340;&#27169;&#22411;&#30456;&#23545;BERT-base&#21344;&#29992;&#30340;&#20869;&#23384;&#35201;&#23569;&#24471;&#22810;&#65292;&#20294;&#23427;&#20204;&#22312;&#22797;&#26434;&#30340;NLP&#20219;&#21153;&#20013;&#31934;&#24230;&#30340;&#19979;&#38477;&#26159;&#26126;&#26174;&#30340;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;ResNet&#30340;BERT&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#20934;&#30830;&#24615;&#21644;&#36164;&#28304;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#33391;&#22909;&#30340;&#24179;&#34913;&#65292;&#20351;&#20854;&#25104;&#20026;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#23884;&#20837;&#24335;&#35774;&#22791;&#20013;&#37096;&#32626;&#30340;&#33391;&#22909;&#20505;&#36873;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
BERT-based neural architectures have established themselves as popular state-of-the-art baselines for many downstream NLP tasks. However, these architectures are data-hungry and consume a lot of memory and energy, often hindering their deployment in many real-time, resource-constrained applications. Existing lighter versions of BERT (eg. DistilBERT and TinyBERT) often cannot perform well on complex NLP tasks. More importantly, from a designer's perspective, it is unclear what is the "right" BERT-based architecture to use for a given NLP task that can strike the optimal trade-off between the resources available and the minimum accuracy desired by the end user. System engineers have to spend a lot of time conducting trial-and-error experiments to find a suitable answer to this question. This paper presents an exploratory study of BERT-based models under different resource constraints and accuracy budgets to derive empirical observations about this resource/accuracy trade-offs. Our findin
&lt;/p&gt;</description></item><item><title>EasyNER&#26159;&#19968;&#31181;&#29992;&#20110;&#22312;&#21307;&#23398;&#30740;&#31350;&#25991;&#31456;&#20013;&#35782;&#21035;&#21629;&#21517;&#23454;&#20307;&#30340;&#31471;&#21040;&#31471;&#24037;&#20855;&#12290;&#23427;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#23383;&#20856;&#26041;&#27861;&#65292;&#24182;&#19988;&#26131;&#20110;&#20351;&#29992;&#21644;&#23450;&#21046;&#12290;&#22312;COVID-19&#30456;&#20851;&#25991;&#31456;&#25968;&#25454;&#38598;&#19978;&#30340;&#24212;&#29992;&#35777;&#26126;&#20102;&#20854;&#21487;&#20197;&#20934;&#30830;&#22320;&#35782;&#21035;&#25152;&#38656;&#23454;&#20307;&#12290;</title><link>http://arxiv.org/abs/2304.07805</link><description>&lt;p&gt;
EasyNER&#65306;&#19968;&#31181;&#21487;&#23450;&#21046;&#30340;&#26131;&#20110;&#20351;&#29992;&#30340;&#21307;&#23398;&#25991;&#26412;&#28145;&#24230;&#23398;&#20064;&#21644;&#22522;&#20110;&#23383;&#20856;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
EasyNER: A Customizable Easy-to-Use Pipeline for Deep Learning- and Dictionary-based Named Entity Recognition from Medical Text. (arXiv:2304.07805v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07805
&lt;/p&gt;
&lt;p&gt;
EasyNER&#26159;&#19968;&#31181;&#29992;&#20110;&#22312;&#21307;&#23398;&#30740;&#31350;&#25991;&#31456;&#20013;&#35782;&#21035;&#21629;&#21517;&#23454;&#20307;&#30340;&#31471;&#21040;&#31471;&#24037;&#20855;&#12290;&#23427;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#23383;&#20856;&#26041;&#27861;&#65292;&#24182;&#19988;&#26131;&#20110;&#20351;&#29992;&#21644;&#23450;&#21046;&#12290;&#22312;COVID-19&#30456;&#20851;&#25991;&#31456;&#25968;&#25454;&#38598;&#19978;&#30340;&#24212;&#29992;&#35777;&#26126;&#20102;&#20854;&#21487;&#20197;&#20934;&#30830;&#22320;&#35782;&#21035;&#25152;&#38656;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#30740;&#31350;&#24050;&#32463;&#20135;&#29983;&#20102;&#22823;&#37327;&#20986;&#29256;&#29289;&#65292;PubMed&#25968;&#25454;&#24211;&#24050;&#32463;&#25910;&#24405;&#20102;&#36229;&#36807;3,500&#19975;&#31687;&#30740;&#31350;&#25991;&#31456;&#12290;&#25972;&#21512;&#36825;&#20123;&#20998;&#25955;&#22312;&#22823;&#37327;&#25991;&#29486;&#20013;&#30340;&#30693;&#35782;&#21487;&#20197;&#25552;&#20379;&#26377;&#20851;&#29983;&#29702;&#26426;&#21046;&#21644;&#23548;&#33268;&#26032;&#22411;&#21307;&#23398;&#24178;&#39044;&#30340;&#30142;&#30149;&#36807;&#31243;&#30340;&#20851;&#38190;&#35265;&#35299;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#65292;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#25104;&#20026;&#19968;&#20010;&#24040;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#25968;&#25454;&#30340;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#36828;&#36828;&#36229;&#20986;&#20102;&#20154;&#31867;&#30340;&#22788;&#29702;&#33021;&#21147;&#12290;&#22312;COVID-19&#22823;&#27969;&#34892;&#30340;&#32039;&#24613;&#24773;&#20917;&#19979;&#65292;&#36825;&#23588;&#20854;&#25104;&#20026;&#38382;&#39064;&#12290;&#33258;&#21160;&#21270;&#25991;&#26412;&#25366;&#25496;&#21487;&#20197;&#24110;&#21161;&#20174;&#22823;&#37327;&#21307;&#23398;&#30740;&#31350;&#25991;&#31456;&#20013;&#25552;&#21462;&#21644;&#36830;&#25509;&#20449;&#24687;&#12290;&#25991;&#26412;&#25366;&#25496;&#30340;&#31532;&#19968;&#27493;&#36890;&#24120;&#26159;&#35782;&#21035;&#29305;&#23450;&#31867;&#21035;&#30340;&#20851;&#38190;&#23383;&#65288;&#20363;&#22914;&#25152;&#26377;&#34507;&#30333;&#36136;&#25110;&#30142;&#30149;&#21517;&#31216;&#65289;&#65292;&#21363;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;NER&#24037;&#20855;EasyNER&#65292;&#29992;&#20110;&#35782;&#21035;&#21307;&#23398;&#30740;&#31350;&#25991;&#31456;&#20013;&#30340;&#20856;&#22411;&#23454;&#20307;&#65292;&#21253;&#25324;&#30142;&#30149;&#21517;&#31216;&#12289;&#33647;&#29289;&#21517;&#31216;&#21644;&#34507;&#30333;&#36136;&#21517;&#31216;&#12290;EasyNER&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#22522;&#20110;&#23383;&#20856;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#32463;&#39564;&#27700;&#24179;&#30340;&#30740;&#31350;&#20154;&#21592;&#26131;&#20110;&#20351;&#29992;&#21644;&#23450;&#21046;&#12290;&#25105;&#20204;&#23558;EasyNER&#24212;&#29992;&#20110;COVID-19&#30456;&#20851;&#25991;&#31456;&#30340;&#25968;&#25454;&#38598;&#20013;&#24182;&#23637;&#31034;&#23427;&#21487;&#20197;&#20934;&#30830;&#22320;&#35782;&#21035;&#24863;&#20852;&#36259;&#30340;&#23454;&#20307;&#65292;&#20026;&#19979;&#28216;&#20998;&#26512;&#25552;&#20379;&#26377;&#29992;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical research generates a large number of publications with the PubMed database already containing &gt;35 million research articles. Integration of the knowledge scattered across this large body of literature could provide key insights into physiological mechanisms and disease processes leading to novel medical interventions. However, it is a great challenge for researchers to utilize this information in full since the scale and complexity of the data greatly surpasses human processing abilities. This becomes especially problematic in cases of extreme urgency like the COVID-19 pandemic. Automated text mining can help extract and connect information from the large body of medical research articles. The first step in text mining is typically the identification of specific classes of keywords (e.g., all protein or disease names), so called Named Entity Recognition (NER). Here we present an end-to-end pipeline for NER of typical entities found in medical research articles, including diseas
&lt;/p&gt;</description></item></channel></rss>