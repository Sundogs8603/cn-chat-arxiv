<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38463;&#25289;&#20271;&#31934;&#32454;&#21270;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#29616;&#26377;&#30340;&#38463;&#25289;&#20271;&#21629;&#21517;&#23454;&#20307;&#35821;&#26009;&#24211;&#65292;&#23558;&#22320;&#32536;&#25919;&#27835;&#23454;&#20307;&#12289;&#20301;&#32622;&#12289;&#32452;&#32455;&#21644;&#35774;&#26045;&#31561;&#22235;&#31181;&#20027;&#35201;&#23454;&#20307;&#31867;&#22411;&#25193;&#23637;&#20026;31&#20010;&#23376;&#31867;&#22411;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;&#35780;&#27880;&#32773;&#19968;&#33268;&#24615;&#35777;&#26126;&#20102;&#25193;&#23637;&#21518;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.17333</link><description>&lt;p&gt;
&#38463;&#25289;&#20271;&#31934;&#32454;&#21270;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Arabic Fine-Grained Entity Recognition. (arXiv:2310.17333v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38463;&#25289;&#20271;&#31934;&#32454;&#21270;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#29616;&#26377;&#30340;&#38463;&#25289;&#20271;&#21629;&#21517;&#23454;&#20307;&#35821;&#26009;&#24211;&#65292;&#23558;&#22320;&#32536;&#25919;&#27835;&#23454;&#20307;&#12289;&#20301;&#32622;&#12289;&#32452;&#32455;&#21644;&#35774;&#26045;&#31561;&#22235;&#31181;&#20027;&#35201;&#23454;&#20307;&#31867;&#22411;&#25193;&#23637;&#20026;31&#20010;&#23376;&#31867;&#22411;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;&#35780;&#27880;&#32773;&#19968;&#33268;&#24615;&#35777;&#26126;&#20102;&#25193;&#23637;&#21518;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;NER&#31995;&#32479;&#36890;&#24120;&#34987;&#35757;&#32451;&#26469;&#35782;&#21035;&#31895;&#31890;&#24230;&#23454;&#20307;&#65292;&#24182;&#19988;&#23545;&#23558;&#23454;&#20307;&#20998;&#31867;&#20026;&#32454;&#31890;&#24230;&#30340;&#20302;&#23618;&#32423;&#23376;&#31867;&#22411;&#30340;&#20851;&#27880;&#36739;&#23569;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#31934;&#32454;&#21270;&#23454;&#20307;&#25552;&#21319;&#38463;&#25289;&#20271;NER&#12290;&#25105;&#20204;&#36873;&#25321;&#25193;&#23637;Wojood&#65288;&#19968;&#20010;&#24320;&#28304;&#30340;&#23884;&#22871;&#38463;&#25289;&#20271;&#21629;&#21517;&#23454;&#20307;&#35821;&#26009;&#24211;&#65289;&#30340;&#23376;&#31867;&#22411;&#12290;&#23588;&#20854;&#26159;&#65292;Wojood&#20013;&#30340;&#22235;&#31181;&#20027;&#35201;&#23454;&#20307;&#31867;&#22411;&#65292;&#22320;&#32536;&#25919;&#27835;&#23454;&#20307;&#65288;GPE&#65289;&#65292;&#20301;&#32622;&#65288;LOC&#65289;&#65292;&#32452;&#32455;&#65288;ORG&#65289;&#21644;&#35774;&#26045;&#65288;FAC&#65289;&#65292;&#34987;&#25193;&#23637;&#20026;31&#20010;&#23376;&#31867;&#22411;&#12290;&#20026;&#20102;&#20570;&#21040;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#39318;&#20808;&#20462;&#35746;&#20102;Wojood&#23545;GPE&#65292;LOC&#65292;ORG&#21644;FAC&#30340;&#27880;&#37322;&#65292;&#20351;&#20854;&#19982;LDC&#30340;ACE&#25351;&#21335;&#20860;&#23481;&#65292;&#32467;&#26524;&#26377;5614&#22788;&#26356;&#25913;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20154;&#24037;&#27880;&#37322;&#20102;Wojood&#20013;&#25152;&#26377;GPE&#65292;LOC&#65292;ORG&#21644;FAC&#30340;&#25552;&#21450;&#65288;&#32422;44K&#65289;&#65292;&#20351;&#29992;LDC&#30340;ACE&#23376;&#31867;&#22411;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#25193;&#23637;&#29256;&#26412;&#30340;Wojood&#31216;&#20026;WojoodFine&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#27880;&#37322;&#65292;&#25105;&#20204;&#20351;&#29992;Cohen's Kappa&#21644;F1&#24471;&#20998;&#26469;&#27979;&#37327;&#35780;&#27880;&#32773;&#19968;&#33268;&#24615;&#65288;IAA&#65289;&#65292;&#32467;&#26524;&#20998;&#21035;&#20026;0.9861&#21644;0.9889
&lt;/p&gt;
&lt;p&gt;
Traditional NER systems are typically trained to recognize coarse-grained entities, and less attention is given to classifying entities into a hierarchy of fine-grained lower-level subtypes. This article aims to advance Arabic NER with fine-grained entities. We chose to extend Wojood (an open-source Nested Arabic Named Entity Corpus) with subtypes. In particular, four main entity types in Wojood, geopolitical entity (GPE), location (LOC), organization (ORG), and facility (FAC), are extended with 31 subtypes. To do this, we first revised Wojood's annotations of GPE, LOC, ORG, and FAC to be compatible with the LDC's ACE guidelines, which yielded 5, 614 changes. Second, all mentions of GPE, LOC, ORG, and FAC (~44K) in Wojood are manually annotated with the LDC's ACE sub-types. We refer to this extended version of Wojood as WojoodF ine. To evaluate our annotations, we measured the inter-annotator agreement (IAA) using both Cohen's Kappa and F1 score, resulting in 0.9861 and 0.9889, respect
&lt;/p&gt;</description></item><item><title>PoisonPrompt&#26159;&#19968;&#31181;&#26032;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#33021;&#22815;&#25104;&#21151;&#22320;&#30772;&#22351;&#22522;&#20110;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#25915;&#20987;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12289;&#20445;&#30495;&#24230;&#21644;&#40065;&#26834;&#24615;&#32463;&#36807;&#20102;&#24191;&#27867;&#23454;&#39564;&#39564;&#35777;&#65292;&#24378;&#35843;&#20102;&#22522;&#20110;&#25552;&#31034;&#30340;&#35821;&#35328;&#27169;&#22411;&#38754;&#20020;&#30340;&#23433;&#20840;&#23041;&#32961;&#21644;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12439</link><description>&lt;p&gt;
PoisonPrompt: &#22522;&#20110;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
PoisonPrompt: Backdoor Attack on Prompt-based Large Language Models. (arXiv:2310.12439v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12439
&lt;/p&gt;
&lt;p&gt;
PoisonPrompt&#26159;&#19968;&#31181;&#26032;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#33021;&#22815;&#25104;&#21151;&#22320;&#30772;&#22351;&#22522;&#20110;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#25915;&#20987;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12289;&#20445;&#30495;&#24230;&#21644;&#40065;&#26834;&#24615;&#32463;&#36807;&#20102;&#24191;&#27867;&#23454;&#39564;&#39564;&#35777;&#65292;&#24378;&#35843;&#20102;&#22522;&#20110;&#25552;&#31034;&#30340;&#35821;&#35328;&#27169;&#22411;&#38754;&#20020;&#30340;&#23433;&#20840;&#23041;&#32961;&#21644;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25552;&#31034;&#26174;&#33879;&#25913;&#21892;&#20102;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#20351;&#24471;&#23427;&#20204;&#22312;&#21508;&#31181;LLM&#24212;&#29992;&#22330;&#26223;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#19981;&#21487;&#25110;&#32570;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22522;&#20110;&#25552;&#31034;&#30340;LLM&#32780;&#35328;&#65292;&#21518;&#38376;&#28431;&#27934;&#8212;&#8212;&#19968;&#31181;&#21487;&#20197;&#24694;&#24847;&#26356;&#25913;&#21463;&#23475;&#27169;&#22411;&#27491;&#24120;&#39044;&#27979;&#30340;&#20005;&#37325;&#23433;&#20840;&#23041;&#32961;&#8212;&#8212;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30340;&#25506;&#32034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21518;&#38376;&#25915;&#20987;POISONPROMPT&#65292;&#33021;&#22815;&#25104;&#21151;&#22320;&#30772;&#22351;&#30828;&#20214;&#21644;&#36719;&#20214;&#22522;&#20110;&#25552;&#31034;&#30340;LLM&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#19977;&#31181;&#27969;&#34892;&#30340;&#25552;&#31034;&#26041;&#27861;&#12289;&#20845;&#20010;&#25968;&#25454;&#38598;&#21644;&#19977;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;LLM&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#26469;&#35780;&#20272;POISONPROMPT&#30340;&#26377;&#25928;&#24615;&#12289;&#20445;&#30495;&#24230;&#21644;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#22522;&#20110;&#25552;&#31034;&#30340;LLM&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#30340;&#28508;&#22312;&#23433;&#20840;&#23041;&#32961;&#65292;&#24182;&#24378;&#35843;&#20102;&#22312;&#36825;&#20010;&#39046;&#22495;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompts have significantly improved the performance of pretrained Large Language Models (LLMs) on various downstream tasks recently, making them increasingly indispensable for a diverse range of LLM application scenarios. However, the backdoor vulnerability, a serious security threat that can maliciously alter the victim model's normal predictions, has not been sufficiently explored for prompt-based LLMs. In this paper, we present POISONPROMPT, a novel backdoor attack capable of successfully compromising both hard and soft prompt-based LLMs. We evaluate the effectiveness, fidelity, and robustness of POISONPROMPT through extensive experiments on three popular prompt methods, using six datasets and three widely used LLMs. Our findings highlight the potential security threats posed by backdoor attacks on prompt-based LLMs and emphasize the need for further research in this area.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#20107;&#23454;&#24615;&#38382;&#39064;&#65292;&#21253;&#25324;&#20854;&#21487;&#33021;&#24102;&#26469;&#30340;&#21518;&#26524;&#21644;&#25361;&#25112;&#65292;&#20197;&#21450;&#23384;&#20648;&#12289;&#22788;&#29702;&#21644;&#35780;&#20272;&#20107;&#23454;&#30340;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#20123;&#22686;&#24378;LLM&#20107;&#23454;&#24615;&#30340;&#31574;&#30053;&#65292;&#28085;&#30422;&#20102;&#19981;&#21516;&#39046;&#22495;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.07521</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20107;&#23454;&#24615;&#35843;&#26597;&#65306;&#30693;&#35782;&#12289;&#26816;&#32034;&#21644;&#39046;&#22495;&#19987;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
Survey on Factuality in Large Language Models: Knowledge, Retrieval and Domain-Specificity. (arXiv:2310.07521v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07521
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#20107;&#23454;&#24615;&#38382;&#39064;&#65292;&#21253;&#25324;&#20854;&#21487;&#33021;&#24102;&#26469;&#30340;&#21518;&#26524;&#21644;&#25361;&#25112;&#65292;&#20197;&#21450;&#23384;&#20648;&#12289;&#22788;&#29702;&#21644;&#35780;&#20272;&#20107;&#23454;&#30340;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#20123;&#22686;&#24378;LLM&#20107;&#23454;&#24615;&#30340;&#31574;&#30053;&#65292;&#28085;&#30422;&#20102;&#19981;&#21516;&#39046;&#22495;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#21363;&#20107;&#23454;&#24615;&#12290;&#30001;&#20110;LLMs&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#37117;&#26377;&#24212;&#29992;&#65292;&#23427;&#20204;&#30340;&#36755;&#20986;&#30340;&#21487;&#38752;&#24615;&#21644;&#20934;&#30830;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#23558;&#20107;&#23454;&#24615;&#38382;&#39064;&#23450;&#20041;&#20026;LLMs&#20135;&#29983;&#19982;&#24050;&#30830;&#31435;&#20107;&#23454;&#19981;&#19968;&#33268;&#30340;&#20869;&#23481;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#39318;&#20808;&#28145;&#20837;&#25506;&#35752;&#20102;&#36825;&#20123;&#19981;&#20934;&#30830;&#24615;&#30340;&#21547;&#20041;&#65292;&#31361;&#20986;&#20102;&#20107;&#23454;&#38169;&#35823;&#22312;LLMs&#36755;&#20986;&#20013;&#21487;&#33021;&#24102;&#26469;&#30340;&#28508;&#22312;&#21518;&#26524;&#21644;&#25361;&#25112;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;LLMs&#23384;&#20648;&#21644;&#22788;&#29702;&#20107;&#23454;&#30340;&#26426;&#21046;&#65292;&#23547;&#25214;&#20107;&#23454;&#38169;&#35823;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#25105;&#20204;&#30340;&#35752;&#35770;&#38543;&#21518;&#36716;&#21521;&#35780;&#20272;LLM&#20107;&#23454;&#24615;&#30340;&#26041;&#27861;&#35770;&#65292;&#24378;&#35843;&#20851;&#38190;&#25351;&#26631;&#12289;&#22522;&#20934;&#21644;&#30740;&#31350;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#22686;&#24378;LLM&#20107;&#23454;&#24615;&#30340;&#31574;&#30053;&#65292;&#21253;&#25324;&#38024;&#23545;&#29305;&#23450;&#39046;&#22495;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#20004;&#31181;&#20027;&#35201;&#30340;LLM&#37197;&#32622;&#65292;&#29420;&#31435;LLMs&#21644;&#21033;&#29992;&#22806;&#37096;&#25968;&#25454;&#30340;&#26816;&#32034;&#22686;&#24378;LLMs&#65292;&#35814;&#32454;&#20171;&#32461;&#23427;&#20204;&#30340;&#29420;&#29305;&#25361;&#25112;&#21644;&#28508;&#22312;&#35299;&#20915;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This survey addresses the crucial issue of factuality in Large Language Models (LLMs). As LLMs find applications across diverse domains, the reliability and accuracy of their outputs become vital. We define the Factuality Issue as the probability of LLMs to produce content inconsistent with established facts. We first delve into the implications of these inaccuracies, highlighting the potential consequences and challenges posed by factual errors in LLM outputs. Subsequently, we analyze the mechanisms through which LLMs store and process facts, seeking the primary causes of factual errors. Our discussion then transitions to methodologies for evaluating LLM factuality, emphasizing key metrics, benchmarks, and studies. We further explore strategies for enhancing LLM factuality, including approaches tailored for specific domains. We focus two primary LLM configurations standalone LLMs and Retrieval-Augmented LLMs that utilizes external data, we detail their unique challenges and potential 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#35821;&#35328;&#27169;&#22411;&#65288;RNN LMs&#65289;&#20316;&#20026;&#27010;&#29575;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#21482;&#33021;&#34920;&#31034;&#26377;&#38480;&#29366;&#24577;&#27169;&#22411;&#25152;&#33021;&#34920;&#36798;&#30340;&#27010;&#29575;&#20998;&#24067;&#30340;&#19968;&#20010;&#20005;&#26684;&#23376;&#38598;&#12290;</title><link>http://arxiv.org/abs/2310.05161</link><description>&lt;p&gt;
&#24490;&#29615;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#27010;&#29575;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;
&lt;/p&gt;
&lt;p&gt;
Recurrent Neural Language Models as Probabilistic Finite-state Automata. (arXiv:2310.05161v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05161
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#35821;&#35328;&#27169;&#22411;&#65288;RNN LMs&#65289;&#20316;&#20026;&#27010;&#29575;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#21482;&#33021;&#34920;&#31034;&#26377;&#38480;&#29366;&#24577;&#27169;&#22411;&#25152;&#33021;&#34920;&#36798;&#30340;&#27010;&#29575;&#20998;&#24067;&#30340;&#19968;&#20010;&#20005;&#26684;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20197;&#23481;&#26131;&#29702;&#35299;&#30340;&#24418;&#24335;&#26469;&#30740;&#31350;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#21487;&#20197;&#20351;&#25105;&#20204;&#31934;&#30830;&#22320;&#25551;&#36848;&#23427;&#20204;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#32771;&#23519;&#20102;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#35821;&#35328;&#27169;&#22411;&#22312;&#35782;&#21035;&#26080;&#26435;&#37325;&#24418;&#24335;&#35821;&#35328;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;LMs&#24182;&#19981;&#25551;&#36848;&#26080;&#26435;&#37325;&#24418;&#24335;&#35821;&#35328;&#65292;&#32780;&#26159;&#23450;&#20041;&#20102;&#23545;&#23383;&#31526;&#20018;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;RNN LMs&#21487;&#20197;&#34920;&#31034;&#21738;&#20123;&#31867;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#26356;&#30452;&#25509;&#22320;&#38472;&#36848;&#23427;&#20204;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#31616;&#21333;&#30340;RNN&#31561;&#20215;&#20110;&#27010;&#29575;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#30340;&#19968;&#20010;&#23376;&#31867;&#65292;&#22240;&#27492;&#21482;&#33021;&#27169;&#25311;&#26377;&#38480;&#29366;&#24577;&#27169;&#22411;&#25152;&#33021;&#34920;&#36798;&#30340;&#27010;&#29575;&#20998;&#24067;&#30340;&#19968;&#20010;&#20005;&#26684;&#23376;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#29992;RNNs&#34920;&#31034;&#26377;&#38480;&#29366;&#24577;LMs&#30340;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#20026;&#20102;&#34920;&#31034;&#19968;&#20010;&#20219;&#24847;&#30830;&#23450;&#30340;&#26377;&#38480;&#29366;&#24577;LMs&#65292;&#20854;&#20013;&#26377;$N$&#20010;&#29366;&#24577;&#19988;&#23383;&#31526;&#38598;&#20026;$\Sigma$&#30340;RNN requir
&lt;/p&gt;
&lt;p&gt;
Studying language models (LMs) in terms of well-understood formalisms allows us to precisely characterize their abilities and limitations. Previous work has investigated the representational capacity of recurrent neural network (RNN) LMs in terms of their capacity to recognize unweighted formal languages. However, LMs do not describe unweighted formal languages -- rather, they define probability distributions over strings. In this work, we study what classes of such probability distributions RNN LMs can represent, which allows us to make more direct statements about their capabilities. We show that simple RNNs are equivalent to a subclass of probabilistic finite-state automata, and can thus model a strict subset of probability distributions expressible by finite-state models. Furthermore, we study the space complexity of representing finite-state LMs with RNNs. We show that, to represent an arbitrary deterministic finite-state LM with $N$ states over an alphabet $\Sigma$, an RNN requir
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#38646;&#36164;&#28304;&#20999;&#25442;&#35821;&#38899;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#30417;&#30563;&#35821;&#38899;&#32534;&#30721;&#22120;&#30340;&#20999;&#25442;&#35821;&#35328;&#33021;&#21147;&#65292;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#21644;&#27169;&#22411;&#22823;&#23567;&#23545;&#22522;&#20934;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#26174;&#31034;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#30340;&#35821;&#38899;&#32534;&#30721;&#22120;&#22312;&#20999;&#25442;&#35821;&#35328;&#22330;&#26223;&#20013;&#34920;&#29616;&#20248;&#20110;&#21333;&#35821;&#21464;&#20307;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2310.03018</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#38899;&#35821;&#21477;&#23545;&#30340;&#38646;&#36164;&#28304;&#20999;&#25442;&#35821;&#38899;&#22522;&#20934;&#36827;&#34892;&#22810;&#35821;&#35328;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Zero Resource Code-switched Speech Benchmark Using Speech Utterance Pairs For Multiple Spoken Languages. (arXiv:2310.03018v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03018
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#38646;&#36164;&#28304;&#20999;&#25442;&#35821;&#38899;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#30417;&#30563;&#35821;&#38899;&#32534;&#30721;&#22120;&#30340;&#20999;&#25442;&#35821;&#35328;&#33021;&#21147;&#65292;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#21644;&#27169;&#22411;&#22823;&#23567;&#23545;&#22522;&#20934;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#26174;&#31034;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#30340;&#35821;&#38899;&#32534;&#30721;&#22120;&#22312;&#20999;&#25442;&#35821;&#35328;&#22330;&#26223;&#20013;&#34920;&#29616;&#20248;&#20110;&#21333;&#35821;&#21464;&#20307;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#38646;&#36164;&#28304;&#20999;&#25442;&#35821;&#38899;&#22522;&#20934;&#65292;&#26088;&#22312;&#30452;&#25509;&#35780;&#20272;&#33258;&#30417;&#30563;&#35821;&#38899;&#32534;&#30721;&#22120;&#30340;&#20999;&#25442;&#35821;&#35328;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#25955;&#21333;&#20803;&#35821;&#35328;&#24314;&#27169;&#30340;&#22522;&#32447;&#31995;&#32479;&#65292;&#20197;&#23637;&#31034;&#22914;&#20309;&#20197;&#38646;&#36164;&#28304;&#30340;&#26041;&#24335;&#35780;&#20272;&#35821;&#38899;&#32534;&#30721;&#22120;&#30340;&#20999;&#25442;&#35821;&#35328;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#28085;&#30422;&#20102;&#21508;&#31181;&#30693;&#21517;&#30340;&#35821;&#38899;&#32534;&#30721;&#22120;&#65292;&#21253;&#25324;Wav2vec 2.0&#12289;HuBERT&#12289;XLSR&#31561;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#21644;&#27169;&#22411;&#22823;&#23567;&#23545;&#22522;&#20934;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23613;&#31649;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20999;&#25442;&#35821;&#35328;&#22330;&#26223;&#20013;&#65292;&#20855;&#26377;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#30340;&#35821;&#38899;&#32534;&#30721;&#22120;&#65288;&#20363;&#22914;XLSR&#65289;&#20248;&#20110;&#21333;&#35821;&#21464;&#20307;&#65288;Wav2vec 2.0&#12289;HuBERT&#65289;&#65292;&#20294;&#23427;&#20204;&#30340;&#20999;&#25442;&#35821;&#35328;&#33021;&#21147;&#20173;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new zero resource code-switched speech benchmark designed to directly assess the code-switching capabilities of self-supervised speech encoders. We showcase a baseline system of language modeling on discrete units to demonstrate how the code-switching abilities of speech encoders can be assessed in a zero-resource manner. Our experiments encompass a variety of well-known speech encoders, including Wav2vec 2.0, HuBERT, XLSR, etc. We examine the impact of pre-training languages and model size on benchmark performance. Notably, though our results demonstrate that speech encoders with multilingual pre-training, exemplified by XLSR, outperform monolingual variants (Wav2vec 2.0, HuBERT) in code-switching scenarios, there is still substantial room for improvement in their code-switching linguistic abilities.
&lt;/p&gt;</description></item><item><title>RLAdapter&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#22312;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#20013;&#30340;&#31574;&#30053;&#23398;&#20064;&#24615;&#33021;&#12290;&#36825;&#36890;&#36807;&#35299;&#20915;LLM&#22312;&#29702;&#35299;&#19979;&#28216;&#20219;&#21153;&#26041;&#38754;&#30340;&#22256;&#38590;&#65292;&#20197;&#21450;&#36890;&#36807;&#36991;&#20813;&#20351;&#29992;&#19981;&#21487;&#35775;&#38382;&#30340;&#27169;&#22411;&#26435;&#37325;&#25110;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#26469;&#24494;&#35843;LLM&#30340;&#26041;&#24335;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.17176</link><description>&lt;p&gt;
RLAdapter&#65306;&#22312;&#24320;&#25918;&#29615;&#22659;&#20013;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
RLAdapter: Bridging Large Language Models to Reinforcement Learning in Open Worlds. (arXiv:2309.17176v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17176
&lt;/p&gt;
&lt;p&gt;
RLAdapter&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#22312;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#20013;&#30340;&#31574;&#30053;&#23398;&#20064;&#24615;&#33021;&#12290;&#36825;&#36890;&#36807;&#35299;&#20915;LLM&#22312;&#29702;&#35299;&#19979;&#28216;&#20219;&#21153;&#26041;&#38754;&#30340;&#22256;&#38590;&#65292;&#20197;&#21450;&#36890;&#36807;&#36991;&#20813;&#20351;&#29992;&#19981;&#21487;&#35775;&#38382;&#30340;&#27169;&#22411;&#26435;&#37325;&#25110;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#26469;&#24494;&#35843;LLM&#30340;&#26041;&#24335;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#20915;&#31574;&#38382;&#39064;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#19982;&#29615;&#22659;&#36827;&#34892;&#22823;&#37327;&#30340;&#20132;&#20114;&#65292;&#22312;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#20013;&#23398;&#20064;&#26377;&#24847;&#20041;&#30340;&#31574;&#30053;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21487;&#20197;&#20026;&#20195;&#29702;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#25351;&#23548;&#65292;&#20174;&#32780;&#22686;&#24378;RL&#31639;&#27861;&#22312;&#36825;&#20123;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;LLM&#36890;&#24120;&#22312;&#29702;&#35299;&#19979;&#28216;&#20219;&#21153;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#65292;&#36825;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#26368;&#20248;&#22320;&#24110;&#21161;&#20195;&#29702;&#30340;&#33021;&#21147;&#12290;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#20351;&#29992;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#25968;&#25454;&#26469;&#24494;&#35843;LLM&#65292;&#20351;&#20854;&#33021;&#22815;&#20026;RL&#20195;&#29702;&#25552;&#20379;&#26377;&#29992;&#30340;&#25351;&#23548;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#36935;&#21040;&#20102;&#19968;&#20123;&#22256;&#38590;&#65292;&#27604;&#22914;&#26080;&#27861;&#35775;&#38382;&#30340;&#27169;&#22411;&#26435;&#37325;&#25110;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#20351;&#20854;&#19981;&#20999;&#23454;&#38469;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;RLAdapter&#65292;&#36825;&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#22312;RL&#31639;&#27861;&#21644;LLM&#20043;&#38388;&#24314;&#31435;&#26356;&#22909;&#30340;&#36830;&#25509;&#65292;&#20174;&#32780;&#25972;&#21512;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
While reinforcement learning (RL) shows remarkable success in decision-making problems, it often requires a lot of interactions with the environment, and in sparse-reward environments, it is challenging to learn meaningful policies. Large Language Models (LLMs) can potentially provide valuable guidance to agents in learning policies, thereby enhancing the performance of RL algorithms in such environments. However, LLMs often encounter difficulties in understanding downstream tasks, which hinders their ability to optimally assist agents in these tasks. A common approach to mitigating this issue is to fine-tune the LLMs with task-related data, enabling them to offer useful guidance for RL agents. However, this approach encounters several difficulties, such as inaccessible model weights or the need for significant computational resources, making it impractical. In this work, we introduce RLAdapter, a framework that builds a better connection between RL algorithms and LLMs by incorporating
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#26368;&#23569;&#30417;&#30563;&#30340;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#39640;&#20445;&#30495;&#24230;&#35821;&#38899;&#21512;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#21512;&#31163;&#25955;&#35821;&#38899;&#34920;&#31034;&#21644;&#21033;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;&#35821;&#20041;&#34920;&#31034;&#20013;&#30340;&#20449;&#24687;&#20887;&#20313;&#21644;&#32500;&#24230;&#29190;&#28856;&#20197;&#21450;&#31163;&#25955;&#22768;&#23398;&#34920;&#31034;&#20013;&#30340;&#39640;&#39057;&#27874;&#24418;&#22833;&#30495;&#31561;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#20013;&#30340;&#38750;&#33258;&#22238;&#24402;&#26694;&#26550;&#22686;&#24378;&#20102;&#21487;&#25511;&#24615;&#65292;&#32780;&#25345;&#32493;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#20102;&#38899;&#39057;&#30340;&#22810;&#26679;&#21270;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.15512</link><description>&lt;p&gt;
&#20351;&#29992;&#26368;&#23569;&#30417;&#30563;&#30340;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#39640;&#20445;&#30495;&#24230;&#35821;&#38899;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
High-Fidelity Speech Synthesis with Minimal Supervision: All Using Diffusion Models. (arXiv:2309.15512v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15512
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#26368;&#23569;&#30417;&#30563;&#30340;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#39640;&#20445;&#30495;&#24230;&#35821;&#38899;&#21512;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#21512;&#31163;&#25955;&#35821;&#38899;&#34920;&#31034;&#21644;&#21033;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;&#35821;&#20041;&#34920;&#31034;&#20013;&#30340;&#20449;&#24687;&#20887;&#20313;&#21644;&#32500;&#24230;&#29190;&#28856;&#20197;&#21450;&#31163;&#25955;&#22768;&#23398;&#34920;&#31034;&#20013;&#30340;&#39640;&#39057;&#27874;&#24418;&#22833;&#30495;&#31561;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#20013;&#30340;&#38750;&#33258;&#22238;&#24402;&#26694;&#26550;&#22686;&#24378;&#20102;&#21487;&#25511;&#24615;&#65292;&#32780;&#25345;&#32493;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#20102;&#38899;&#39057;&#30340;&#22810;&#26679;&#21270;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#23383;&#36716;&#35821;&#38899;&#65288;TTS&#65289;&#26041;&#27861;&#22312;&#35821;&#38899;&#20811;&#38534;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#30340;&#25991;&#26412;-&#35821;&#38899;&#23545;&#12290;&#26368;&#23567;&#30417;&#30563;&#30340;&#35821;&#38899;&#21512;&#25104;&#36890;&#36807;&#32452;&#21512;&#20004;&#31181;&#31867;&#22411;&#30340;&#31163;&#25955;&#35821;&#38899;&#34920;&#31034;&#65288;&#35821;&#20041;&#21644;&#22768;&#23398;&#65289;&#65292;&#24182;&#20351;&#29992;&#20004;&#31181;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#65292;&#20197;&#23454;&#29616;&#26368;&#23569;&#30417;&#30563;&#30340;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#35821;&#20041;&#34920;&#31034;&#20013;&#30340;&#20449;&#24687;&#20887;&#20313;&#21644;&#32500;&#24230;&#29190;&#28856;&#65292;&#20197;&#21450;&#31163;&#25955;&#22768;&#23398;&#34920;&#31034;&#20013;&#30340;&#39640;&#39057;&#27874;&#24418;&#22833;&#30495;&#12290;&#33258;&#22238;&#24402;&#26694;&#26550;&#20855;&#26377;&#20856;&#22411;&#30340;&#19981;&#31283;&#23450;&#24615;&#21644;&#19981;&#21487;&#25511;&#24615;&#38382;&#39064;&#12290;&#38750;&#33258;&#22238;&#24402;&#26694;&#26550;&#21463;&#21040;&#25345;&#32493;&#39044;&#27979;&#27169;&#22411;&#24341;&#36215;&#30340;&#38901;&#24459;&#24179;&#22343;&#21270;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#23567;&#30417;&#30563;&#30340;&#39640;&#20445;&#30495;&#24230;&#35821;&#38899;&#21512;&#25104;&#26041;&#27861;&#65292;&#20854;&#20013;&#25152;&#26377;&#27169;&#22359;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#26500;&#24314;&#12290;&#38750;&#33258;&#22238;&#24402;&#26694;&#26550;&#22686;&#24378;&#20102;&#21487;&#25511;&#24615;&#65292;&#32780;&#25345;&#32493;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#20102;&#38899;&#39057;&#30340;&#22810;&#26679;&#21270;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-speech (TTS) methods have shown promising results in voice cloning, but they require a large number of labeled text-speech pairs. Minimally-supervised speech synthesis decouples TTS by combining two types of discrete speech representations(semantic \&amp; acoustic) and using two sequence-to-sequence tasks to enable training with minimal supervision. However, existing methods suffer from information redundancy and dimension explosion in semantic representation, and high-frequency waveform distortion in discrete acoustic representation. Autoregressive frameworks exhibit typical instability and uncontrollability issues. And non-autoregressive frameworks suffer from prosodic averaging caused by duration prediction models. To address these issues, we propose a minimally-supervised high-fidelity speech synthesis method, where all modules are constructed based on the diffusion models. The non-autoregressive framework enhances controllability, and the duration diffusion model enables diver
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#27169;&#24577;&#27880;&#24847;&#22686;&#24378;&#30340;&#25991;&#26412;-&#35270;&#39057;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#65292;&#33021;&#22815;&#20934;&#30830;&#34913;&#37327;&#36328;&#27169;&#24577;&#30456;&#20284;&#24615;&#21644;&#25366;&#25496;&#38590;&#36127;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2309.11082</link><description>&lt;p&gt;
&#21452;&#27169;&#24577;&#27880;&#24847;&#22686;&#24378;&#30340;&#25991;&#26412;-&#35270;&#39057;&#26816;&#32034;&#19982;&#19977;&#20803;&#37096;&#20998;&#36793;&#38469;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Dual-Modal Attention-Enhanced Text-Video Retrieval with Triplet Partial Margin Contrastive Learning. (arXiv:2309.11082v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#27169;&#24577;&#27880;&#24847;&#22686;&#24378;&#30340;&#25991;&#26412;-&#35270;&#39057;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#65292;&#33021;&#22815;&#20934;&#30830;&#34913;&#37327;&#36328;&#27169;&#24577;&#30456;&#20284;&#24615;&#21644;&#25366;&#25496;&#38590;&#36127;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#32593;&#32476;&#35270;&#39057;&#30340;&#29190;&#28856;&#24615;&#22686;&#38271;&#20351;&#24471;&#25991;&#26412;-&#35270;&#39057;&#26816;&#32034;&#23545;&#20110;&#35270;&#39057;&#36807;&#28388;&#12289;&#25512;&#33616;&#21644;&#25628;&#32034;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#21644;&#27969;&#34892;&#12290;&#25991;&#26412;-&#35270;&#39057;&#26816;&#32034;&#26088;&#22312;&#23558;&#30456;&#20851;&#30340;&#25991;&#26412;/&#35270;&#39057;&#25490;&#22312;&#19981;&#30456;&#20851;&#30340;&#25991;&#26412;/&#35270;&#39057;&#20043;&#21069;&#12290;&#35813;&#20219;&#21153;&#30340;&#26680;&#24515;&#26159;&#20934;&#30830;&#34913;&#37327;&#25991;&#26412;&#21644;&#35270;&#39057;&#20043;&#38388;&#30340;&#36328;&#27169;&#24577;&#30456;&#20284;&#24615;&#12290;&#26368;&#36817;&#65292;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#22312;&#25991;&#26412;-&#35270;&#39057;&#26816;&#32034;&#26041;&#38754;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#26041;&#27861;&#20391;&#37325;&#20110;&#26500;&#24314;&#27491;&#36127;&#26679;&#26412;&#23545;&#20197;&#23398;&#20064;&#25991;&#26412;&#21644;&#35270;&#39057;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#22312;&#20851;&#27880;&#38590;&#36127;&#26679;&#26412;&#21644;&#27169;&#25311;&#19981;&#21516;&#23618;&#27425;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#26041;&#38754;&#19981;&#22815;&#65292;&#23384;&#22312;&#20004;&#20010;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#20351;&#29992;&#20004;&#20010;&#26032;&#26041;&#27861;&#25913;&#36827;&#20102;&#23545;&#27604;&#23398;&#20064;&#12290;&#39318;&#20808;&#65292;&#20026;&#20102;&#21033;&#29992;&#33392;&#38590;&#30340;&#20363;&#23376;&#26469;&#25552;&#39640;&#40065;&#26834;&#30340;&#21028;&#21035;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#27169;&#24577;&#27880;&#24847;&#22686;&#24378;&#27169;&#22359;(DMAE)&#65292;&#20174;&#25991;&#26412;&#21644;&#35270;&#35273;&#32447;&#32034;&#20013;&#25366;&#25496;&#38590;&#36127;&#26679;&#26412;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#24341;&#20837;&#19968;&#20010;&#36127;&#38754;&#26679;&#26412;&#31579;&#36873;&#26426;&#21046;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#24314;&#27169;&#19981;&#21516;&#32423;&#21035;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the explosion of web videos makes text-video retrieval increasingly essential and popular for video filtering, recommendation, and search. Text-video retrieval aims to rank relevant text/video higher than irrelevant ones. The core of this task is to precisely measure the cross-modal similarity between texts and videos. Recently, contrastive learning methods have shown promising results for text-video retrieval, most of which focus on the construction of positive and negative pairs to learn text and video representations. Nevertheless, they do not pay enough attention to hard negative pairs and lack the ability to model different levels of semantic similarity. To address these two issues, this paper improves contrastive learning using two novel techniques. First, to exploit hard examples for robust discriminative power, we propose a novel Dual-Modal Attention-Enhanced Module (DMAE) to mine hard negative pairs from textual and visual clues. By further introducing a Negat
&lt;/p&gt;</description></item><item><title>DePT&#36890;&#36807;&#23558;&#36719;&#25552;&#31034;&#20998;&#35299;&#20026;&#36739;&#30701;&#30340;&#36719;&#25552;&#31034;&#21644;&#19968;&#23545;&#20302;&#31209;&#30697;&#38453;&#65292;&#24182;&#29992;&#20004;&#20010;&#19981;&#21516;&#30340;&#23398;&#20064;&#29575;&#26469;&#20248;&#21270;&#65292;&#20197;&#35299;&#20915;&#25552;&#31034;&#35843;&#25972;&#23545;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#20197;&#21450;&#20869;&#23384;&#20351;&#29992;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.05173</link><description>&lt;p&gt;
DePT:&#20998;&#35299;&#25552;&#31034;&#35843;&#25972;&#20197;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning. (arXiv:2309.05173v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05173
&lt;/p&gt;
&lt;p&gt;
DePT&#36890;&#36807;&#23558;&#36719;&#25552;&#31034;&#20998;&#35299;&#20026;&#36739;&#30701;&#30340;&#36719;&#25552;&#31034;&#21644;&#19968;&#23545;&#20302;&#31209;&#30697;&#38453;&#65292;&#24182;&#29992;&#20004;&#20010;&#19981;&#21516;&#30340;&#23398;&#20064;&#29575;&#26469;&#20248;&#21270;&#65292;&#20197;&#35299;&#20915;&#25552;&#31034;&#35843;&#25972;&#23545;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#20197;&#21450;&#20869;&#23384;&#20351;&#29992;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#35843;&#25972;&#65288;PT&#65289;&#26159;&#19968;&#31181;&#23558;&#21487;&#35757;&#32451;&#30340;&#23569;&#37327;&#36719;&#25552;&#31034;&#21521;&#37327;&#38468;&#21152;&#21040;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#36755;&#20837;&#20013;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#65292;&#24050;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#27169;&#22411;&#20013;&#26174;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290; &#19982;&#20854;&#20182;PEFT&#26041;&#27861;&#30456;&#27604;&#65292;PT&#30340;&#31454;&#20105;&#24615;&#33021;&#21487;&#20197;&#22312;&#21487;&#35757;&#32451;&#21442;&#25968;&#26356;&#23569;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#65292;&#24182;&#19988;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#30340;&#25193;&#22823;&#65292;&#20854;&#21442;&#25968;&#24182;&#19981;&#20250;&#26174;&#33879;&#22686;&#21152;&#12290; &#20294;&#26159;&#65292;PT&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#36719;&#25552;&#31034;&#26631;&#35760;&#65292;&#23548;&#33268;&#36755;&#20837;&#24207;&#21015;&#21464;&#38271;&#65292;&#36825;&#23545;&#20110;Transformer&#30340;&#20108;&#27425;&#22797;&#26434;&#24230;&#32780;&#35328;&#65292;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#20197;&#21450;&#20869;&#23384;&#20351;&#29992;&#26041;&#38754;&#20250;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290; &#36825;&#23545;&#20110;&#38754;&#20020;&#22823;&#37327;&#27599;&#26085;&#26597;&#35810;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23588;&#20854;&#20196;&#20154;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt tuning (PT), where a small amount of trainable soft (continuous) prompt vectors is affixed to the input of language models (LM), has shown promising results across various tasks and models for parameter-efficient fine-tuning (PEFT). PT stands out from other PEFT approaches because it maintains competitive performance with fewer trainable parameters and does not drastically scale up its parameters as the model size expands. However, PT introduces additional soft prompt tokens, leading to longer input sequences, which significantly impacts training and inference time and memory usage due to the Transformer's quadratic complexity. Particularly concerning for Large Language Models (LLMs) that face heavy daily querying. To address this issue, we propose Decomposed Prompt Tuning (DePT), which decomposes the soft prompt into a shorter soft prompt and a pair of low-rank matrices that are then optimised with two different learning rates. This allows DePT to achieve better performance whi
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CPSP&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26469;&#20174;&#35821;&#38899;&#20013;&#25552;&#21462;&#32454;&#31890;&#24230;&#30340;&#20013;&#38388;&#34920;&#31034;&#65292;&#20351;&#24471;&#25552;&#21462;&#30340;&#20449;&#24687;&#26082;&#21253;&#21547;&#35821;&#35328;&#20869;&#23481;&#21448;&#21435;&#38500;&#20102;&#21457;&#35328;&#20154;&#36523;&#20221;&#21644;&#22768;&#23398;&#32454;&#33410;&#65292;&#36866;&#29992;&#20110;TTS&#12289;VC&#21644;ASR&#31561;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2309.00424</link><description>&lt;p&gt;
CPSP: &#20174;&#38899;&#32032;&#30417;&#30563;&#20013;&#23398;&#20064;&#35821;&#38899;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
CPSP: Learning Speech Concepts From Phoneme Supervision. (arXiv:2309.00424v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00424
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CPSP&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26469;&#20174;&#35821;&#38899;&#20013;&#25552;&#21462;&#32454;&#31890;&#24230;&#30340;&#20013;&#38388;&#34920;&#31034;&#65292;&#20351;&#24471;&#25552;&#21462;&#30340;&#20449;&#24687;&#26082;&#21253;&#21547;&#35821;&#35328;&#20869;&#23481;&#21448;&#21435;&#38500;&#20102;&#21457;&#35328;&#20154;&#36523;&#20221;&#21644;&#22768;&#23398;&#32454;&#33410;&#65292;&#36866;&#29992;&#20110;TTS&#12289;VC&#21644;ASR&#31561;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35832;&#22914;&#26368;&#23567;&#30417;&#30563;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#65288;TTS&#65289;&#12289;&#35821;&#38899;&#36716;&#25442;&#65288;VC&#65289;&#21644;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31561;&#32454;&#31890;&#24230;&#29983;&#25104;&#21644;&#35782;&#21035;&#20219;&#21153;&#65292;&#20174;&#35821;&#38899;&#20013;&#25552;&#21462;&#30340;&#20013;&#38388;&#34920;&#31034;&#24212;&#21253;&#21547;&#20171;&#20110;&#25991;&#26412;&#32534;&#30721;&#21644;&#22768;&#23398;&#32534;&#30721;&#20043;&#38388;&#30340;&#20449;&#24687;&#12290;&#35821;&#35328;&#20869;&#23481;&#31361;&#20986;&#65292;&#32780;&#21457;&#35328;&#20154;&#36523;&#20221;&#21644;&#22768;&#23398;&#32454;&#33410;&#31561;&#35821;&#38899;&#20449;&#24687;&#24212;&#35813;&#34987;&#21435;&#38500;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20174;&#35821;&#38899;&#20013;&#25552;&#21462;&#32454;&#31890;&#24230;&#20013;&#38388;&#34920;&#31034;&#30340;&#26041;&#27861;&#23384;&#22312;&#20887;&#20313;&#24615;&#36807;&#39640;&#21644;&#32500;&#24230;&#29190;&#28856;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#38899;&#39057;&#39046;&#22495;&#20013;&#29616;&#26377;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#25552;&#21462;&#29992;&#20110;&#19979;&#28216;&#38899;&#39057;&#20998;&#31867;&#20219;&#21153;&#30340;&#20840;&#23616;&#25551;&#36848;&#20449;&#24687;&#65292;&#19981;&#36866;&#21512;TTS&#12289;VC&#21644;ASR&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23545;&#27604;&#38899;&#32032;-&#35821;&#38899;&#39044;&#35757;&#32451;&#65288;CPSP&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#19977;&#20010;&#32534;&#30721;&#22120;&#12289;&#19968;&#20010;&#35299;&#30721;&#22120;&#21644;&#23545;&#27604;&#23398;&#20064;&#26469;&#23558;&#38899;&#32032;&#21644;&#35821;&#38899;&#20449;&#24687;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
For fine-grained generation and recognition tasks such as minimally-supervised text-to-speech (TTS), voice conversion (VC), and automatic speech recognition (ASR), the intermediate representation extracted from speech should contain information that is between text coding and acoustic coding. The linguistic content is salient, while the paralinguistic information such as speaker identity and acoustic details should be removed. However, existing methods for extracting fine-grained intermediate representations from speech suffer from issues of excessive redundancy and dimension explosion. Additionally, existing contrastive learning methods in the audio field focus on extracting global descriptive information for downstream audio classification tasks, making them unsuitable for TTS, VC, and ASR tasks. To address these issues, we propose a method named Contrastive Phoneme-Speech Pretraining (CPSP), which uses three encoders, one decoder, and contrastive learning to bring phoneme and speech
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22797;&#26434;&#24615;&#24433;&#21709;&#25512;&#29702;&#20998;&#25968;&#65288;CIRS&#65289;&#26469;&#34913;&#37327;&#32534;&#31243;&#35821;&#35328;&#23545;&#25512;&#29702;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#24182;&#38750;&#25152;&#26377;&#22797;&#26434;&#24615;&#30340;&#20195;&#30721;&#25968;&#25454;&#37117;&#21487;&#20197;&#34987;&#23398;&#20064;&#25110;&#29702;&#35299;&#65292;&#36866;&#24403;&#30340;&#22797;&#26434;&#24615;&#27700;&#24179;&#23545;&#20110;&#25913;&#21892;&#25512;&#29702;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2308.15452</link><description>&lt;p&gt;
&#20160;&#20040;&#26102;&#20505;&#32534;&#31243;&#24605;&#32500;&#23545;&#25512;&#29702;&#36215;&#20316;&#29992;?
&lt;/p&gt;
&lt;p&gt;
When Do Program-of-Thoughts Work for Reasoning?. (arXiv:2308.15452v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15452
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22797;&#26434;&#24615;&#24433;&#21709;&#25512;&#29702;&#20998;&#25968;&#65288;CIRS&#65289;&#26469;&#34913;&#37327;&#32534;&#31243;&#35821;&#35328;&#23545;&#25512;&#29702;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#24182;&#38750;&#25152;&#26377;&#22797;&#26434;&#24615;&#30340;&#20195;&#30721;&#25968;&#25454;&#37117;&#21487;&#20197;&#34987;&#23398;&#20064;&#25110;&#29702;&#35299;&#65292;&#36866;&#24403;&#30340;&#22797;&#26434;&#24615;&#27700;&#24179;&#23545;&#20110;&#25913;&#21892;&#25512;&#29702;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#22312;&#20307;&#29616;&#20986;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#23613;&#31649;&#20687;&#32534;&#31243;&#24605;&#32500;&#25552;&#31034;&#36825;&#26679;&#30340;&#26041;&#27861;&#23545;&#20110;&#20351;&#29992;&#32534;&#31243;&#35821;&#35328;&#26469;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#30340;LLM&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#20195;&#30721;&#25968;&#25454;&#23545;&#25512;&#29702;&#33021;&#21147;&#30340;&#20855;&#20307;&#24433;&#21709;&#20173;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22797;&#26434;&#24615;&#24433;&#21709;&#25512;&#29702;&#20998;&#25968;&#65288;CIRS&#65289;&#65292;&#23427;&#32467;&#21512;&#20102;&#32467;&#26500;&#21644;&#36923;&#36753;&#23646;&#24615;&#65292;&#20197;&#34913;&#37327;&#20195;&#30721;&#21644;&#25512;&#29702;&#33021;&#21147;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#25277;&#35937;&#35821;&#27861;&#26641;&#26469;&#32534;&#30721;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#38590;&#24230;&#21644;&#22280;&#22797;&#26434;&#24230;&#26469;&#35745;&#31639;&#36923;&#36753;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#24182;&#38750;&#25152;&#26377;&#22797;&#26434;&#24615;&#30340;&#20195;&#30721;&#25968;&#25454;&#37117;&#21487;&#20197;&#34987;LLM&#23398;&#20064;&#25110;&#29702;&#35299;&#12290;&#26368;&#20339;&#22797;&#26434;&#24615;&#27700;&#24179;&#23545;&#20110;&#36890;&#36807;&#32534;&#31243;&#36741;&#21161;&#25552;&#31034;&#25913;&#21892;&#25512;&#29702;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#21518;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#33258;&#21160;&#21512;&#25104;&#30340;&#26041;&#27861;...
&lt;/p&gt;
&lt;p&gt;
The reasoning capabilities of Large Language Models (LLMs) play a pivotal role in the realm of embodied artificial intelligence. Although there are effective methods like program-of-thought prompting for LLMs which uses programming language to tackle complex reasoning tasks, the specific impact of code data on the improvement of reasoning capabilities remains under-explored. To address this gap, we propose complexity-impacted reasoning score (CIRS), which combines structural and logical attributes, to measure the correlation between code and reasoning abilities. Specifically, we use the abstract syntax tree to encode the structural information and calculate logical complexity by considering the difficulty and the cyclomatic complexity. Through an empirical analysis, we find not all code data of complexity can be learned or understood by LLMs. Optimal level of complexity is critical to the improvement of reasoning abilities by program-aided prompting. Then we design an auto-synthesizing
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;OVDEval&#30340;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#65292;&#29992;&#20110;&#20840;&#38754;&#24320;&#25918;&#35789;&#27719;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;9&#20010;&#23376;&#20219;&#21153;&#65292;&#24182;&#24341;&#20837;&#20102;&#24120;&#35782;&#30693;&#35782;&#12289;&#23646;&#24615;&#29702;&#35299;&#12289;&#20301;&#32622;&#29702;&#35299;&#12289;&#23545;&#35937;&#20851;&#31995;&#29702;&#35299;&#31561;&#26041;&#38754;&#30340;&#35780;&#20272;&#12290;&#25968;&#25454;&#38598;&#34987;&#31934;&#24515;&#21019;&#24314;&#20197;&#25552;&#20379;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36127;&#20363;&#65292;&#32771;&#39564;&#27169;&#22411;&#23545;&#35270;&#35273;&#21644;&#35821;&#35328;&#36755;&#20837;&#30340;&#30495;&#27491;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#21457;&#29616;&#20102;&#24179;&#22343;&#31934;&#30830;&#24230;&#65288;AP&#65289;&#25351;&#26631;&#22312;&#36825;&#20123;&#32454;&#31890;&#24230;&#26631;&#31614;&#25968;&#25454;&#38598;&#19978;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2308.13177</link><description>&lt;p&gt;
&#22914;&#20309;&#35780;&#20272;&#26816;&#27979;&#30340;&#27867;&#21270;&#33021;&#21147;&#65311;&#19968;&#31181;&#29992;&#20110;&#20840;&#38754;&#24320;&#25918;&#35789;&#27719;&#26816;&#27979;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
How to Evaluate the Generalization of Detection? A Benchmark for Comprehensive Open-Vocabulary Detection. (arXiv:2308.13177v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13177
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;OVDEval&#30340;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#65292;&#29992;&#20110;&#20840;&#38754;&#24320;&#25918;&#35789;&#27719;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;9&#20010;&#23376;&#20219;&#21153;&#65292;&#24182;&#24341;&#20837;&#20102;&#24120;&#35782;&#30693;&#35782;&#12289;&#23646;&#24615;&#29702;&#35299;&#12289;&#20301;&#32622;&#29702;&#35299;&#12289;&#23545;&#35937;&#20851;&#31995;&#29702;&#35299;&#31561;&#26041;&#38754;&#30340;&#35780;&#20272;&#12290;&#25968;&#25454;&#38598;&#34987;&#31934;&#24515;&#21019;&#24314;&#20197;&#25552;&#20379;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36127;&#20363;&#65292;&#32771;&#39564;&#27169;&#22411;&#23545;&#35270;&#35273;&#21644;&#35821;&#35328;&#36755;&#20837;&#30340;&#30495;&#27491;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#21457;&#29616;&#20102;&#24179;&#22343;&#31934;&#30830;&#24230;&#65288;AP&#65289;&#25351;&#26631;&#22312;&#36825;&#20123;&#32454;&#31890;&#24230;&#26631;&#31614;&#25968;&#25454;&#38598;&#19978;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#30446;&#26631;&#26816;&#27979;&#22312;&#36817;&#24180;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20174;&#22522;&#20110;&#23553;&#38381;&#38598;&#26631;&#31614;&#21040;&#22522;&#20110;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#30340;&#24320;&#25918;&#35789;&#27719;&#26816;&#27979;&#65288;OVD&#65289;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#35780;&#20272;&#26041;&#27861;&#21644;&#25968;&#25454;&#38598;&#20165;&#38480;&#20110;&#27979;&#35797;&#23545;&#35937;&#31867;&#22411;&#21644;&#24341;&#29992;&#34920;&#36798;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#26080;&#27861;&#25552;&#20379;OVD&#27169;&#22411;&#33021;&#21147;&#30340;&#31995;&#32479;&#12289;&#32454;&#31890;&#24230;&#21644;&#20934;&#30830;&#30340;&#22522;&#20934;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;OVDEval&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#65292;&#21253;&#25324;9&#20010;&#23376;&#20219;&#21153;&#65292;&#24182;&#24341;&#20837;&#20102;&#24120;&#35782;&#30693;&#35782;&#12289;&#23646;&#24615;&#29702;&#35299;&#12289;&#20301;&#32622;&#29702;&#35299;&#12289;&#23545;&#35937;&#20851;&#31995;&#29702;&#35299;&#31561;&#26041;&#38754;&#30340;&#35780;&#20272;&#12290;&#25968;&#25454;&#38598;&#34987;&#31934;&#24515;&#21019;&#24314;&#20197;&#25552;&#20379;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36127;&#20363;&#65292;&#32771;&#39564;&#27169;&#22411;&#23545;&#35270;&#35273;&#21644;&#35821;&#35328;&#36755;&#20837;&#30340;&#30495;&#27491;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#22312;&#36825;&#20123;&#32454;&#31890;&#24230;&#26631;&#31614;&#25968;&#25454;&#38598;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#26102;&#26222;&#36941;&#20351;&#29992;&#30340;&#24179;&#22343;&#31934;&#30830;&#24230;&#65288;AP&#65289;&#25351;&#26631;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#38750;&#26497;&#22823;&#20540;&#25233;&#21046;&#30340;&#26032;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Object detection (OD) in computer vision has made significant progress in recent years, transitioning from closed-set labels to open-vocabulary detection (OVD) based on large-scale vision-language pre-training (VLP). However, current evaluation methods and datasets are limited to testing generalization over object types and referral expressions, which do not provide a systematic, fine-grained, and accurate benchmark of OVD models' abilities. In this paper, we propose a new benchmark named OVDEval, which includes 9 sub-tasks and introduces evaluations on commonsense knowledge, attribute understanding, position understanding, object relation comprehension, and more. The dataset is meticulously created to provide hard negatives that challenge models' true understanding of visual and linguistic input. Additionally, we identify a problem with the popular Average Precision (AP) metric when benchmarking models on these fine-grained label datasets and propose a new metric called Non-Maximum Su
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#38899;&#39057;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#20869;&#23481;&#21644;&#39118;&#26684;&#31561;&#39069;&#22806;&#26465;&#20214;&#65292;&#22686;&#24378;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#21487;&#25511;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#31934;&#30830;&#25511;&#21046;&#29983;&#25104;&#38899;&#39057;&#30340;&#26102;&#38388;&#39034;&#24207;&#12289;&#38899;&#39640;&#21644;&#33021;&#37327;&#12290;&#30001;&#20110;&#32570;&#20047;&#21512;&#36866;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#20316;&#32773;&#25972;&#21512;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#24182;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2308.11940</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#38899;&#39057;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Audio Generation with Multiple Conditional Diffusion Model. (arXiv:2308.11940v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11940
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#38899;&#39057;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#20869;&#23481;&#21644;&#39118;&#26684;&#31561;&#39069;&#22806;&#26465;&#20214;&#65292;&#22686;&#24378;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#21487;&#25511;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#31934;&#30830;&#25511;&#21046;&#29983;&#25104;&#38899;&#39057;&#30340;&#26102;&#38388;&#39034;&#24207;&#12289;&#38899;&#39640;&#21644;&#33021;&#37327;&#12290;&#30001;&#20110;&#32570;&#20047;&#21512;&#36866;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#20316;&#32773;&#25972;&#21512;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#24182;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25991;&#26412;&#30340;&#38899;&#39057;&#29983;&#25104;&#27169;&#22411;&#26377;&#20854;&#23616;&#38480;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#21253;&#21547;&#38899;&#39057;&#20013;&#30340;&#25152;&#26377;&#20449;&#24687;&#65292;&#20165;&#20381;&#38752;&#25991;&#26412;&#20250;&#23548;&#33268;&#21463;&#25511;&#24615;&#21463;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#39069;&#22806;&#30340;&#26465;&#20214;&#65288;&#21253;&#25324;&#20869;&#23481;&#65288;&#26102;&#38388;&#25139;&#65289;&#21644;&#39118;&#26684;&#65288;&#38899;&#39640;&#26354;&#32447;&#21644;&#33021;&#37327;&#26354;&#32447;&#65289;&#65289;&#20316;&#20026;&#25991;&#26412;&#30340;&#34917;&#20805;&#65292;&#22686;&#24378;&#20102;&#29616;&#26377;&#39044;&#35757;&#32451;&#25991;&#26412;&#21040;&#38899;&#39057;&#27169;&#22411;&#30340;&#21487;&#25511;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#29983;&#25104;&#38899;&#39057;&#30340;&#26102;&#38388;&#39034;&#24207;&#12289;&#38899;&#39640;&#21644;&#33021;&#37327;&#30340;&#31934;&#32454;&#25511;&#21046;&#12290;&#20026;&#20102;&#20445;&#25345;&#29983;&#25104;&#30340;&#22810;&#26679;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#21487;&#35757;&#32451;&#30340;&#25511;&#21046;&#26465;&#20214;&#32534;&#30721;&#22120;&#65292;&#35813;&#32534;&#30721;&#22120;&#30001;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#21487;&#35757;&#32451;&#30340;&#34701;&#21512;&#32593;&#32476;&#26469;&#32534;&#30721;&#21644;&#34701;&#21512;&#39069;&#22806;&#30340;&#26465;&#20214;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#35757;&#32451;&#25991;&#26412;&#21040;&#38899;&#39057;&#27169;&#22411;&#30340;&#26435;&#37325;&#19981;&#21464;&#12290;&#30001;&#20110;&#32570;&#20047;&#21512;&#36866;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#25105;&#20204;&#23558;&#29616;&#26377;&#25968;&#25454;&#38598;&#25972;&#21512;&#20026;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#38899;&#39057;&#21644;&#30456;&#24212;&#30340;&#26465;&#20214;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#21487;&#35757;&#32451;&#30340;&#25511;&#21046;&#26465;&#20214;&#32534;&#30721;&#22120;&#65292;&#35813;&#32534;&#30721;&#22120;&#30001;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#21487;&#35757;&#32451;&#30340;&#34701;&#21512;&#32593;&#32476;&#26469;&#32534;&#30721;&#21644;&#34701;&#21512;&#39069;&#22806;&#30340;&#26465;&#20214;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#35757;&#32451;&#25991;&#26412;&#21040;&#38899;&#39057;&#27169;&#22411;&#30340;&#26435;&#37325;&#19981;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-based audio generation models have limitations as they cannot encompass all the information in audio, leading to restricted controllability when relying solely on text. To address this issue, we propose a novel model that enhances the controllability of existing pre-trained text-to-audio models by incorporating additional conditions including content (timestamp) and style (pitch contour and energy contour) as supplements to the text. This approach achieves fine-grained control over the temporal order, pitch, and energy of generated audio. To preserve the diversity of generation, we employ a trainable control condition encoder that is enhanced by a large language model and a trainable Fusion-Net to encode and fuse the additional conditions while keeping the weights of the pre-trained text-to-audio model frozen. Due to the lack of suitable datasets and evaluation metrics, we consolidate existing datasets into a new dataset comprising the audio and corresponding conditions and use a 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22270;&#35889;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22810;&#25991;&#26723;&#38382;&#31572;&#20219;&#21153;&#20013;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#31034;&#27491;&#30830;&#30340;&#19978;&#19979;&#25991;&#12290;&#36890;&#36807;&#26500;&#24314;&#22810;&#20010;&#25991;&#26723;&#19978;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#24182;&#35774;&#35745;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#36941;&#21382;&#22120;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#24110;&#21161;LLMs&#22312;MD-QA&#20013;&#36827;&#34892;&#31572;&#26696;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.11730</link><description>&lt;p&gt;
&#22810;&#25991;&#26723;&#38382;&#31572;&#20013;&#30340;&#30693;&#35782;&#22270;&#35889;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Prompting for Multi-Document Question Answering. (arXiv:2308.11730v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11730
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22270;&#35889;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22810;&#25991;&#26723;&#38382;&#31572;&#20219;&#21153;&#20013;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#31034;&#27491;&#30830;&#30340;&#19978;&#19979;&#25991;&#12290;&#36890;&#36807;&#26500;&#24314;&#22810;&#20010;&#25991;&#26723;&#19978;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#24182;&#35774;&#35745;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#36941;&#21382;&#22120;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#24110;&#21161;LLMs&#22312;MD-QA&#20013;&#36827;&#34892;&#31572;&#26696;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#8220;&#39044;&#35757;&#32451;&#12289;&#25552;&#31034;&#12289;&#39044;&#27979;&#8221;&#33539;&#24335;&#22312;&#24320;&#25918;&#22495;&#38382;&#31572;&#65288;OD-QA&#65289;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#24037;&#20316;&#22312;&#22810;&#25991;&#26723;&#38382;&#31572;&#65288;MD-QA&#65289;&#22330;&#26223;&#19979;&#25506;&#32034;&#36825;&#20010;&#33539;&#24335;&#65292;&#36825;&#26159;&#19968;&#20010;&#35201;&#27714;&#23545;&#19981;&#21516;&#25991;&#26723;&#30340;&#20869;&#23481;&#21644;&#32467;&#26500;&#20043;&#38388;&#30340;&#36923;&#36753;&#20851;&#32852;&#26377;&#28145;&#20837;&#29702;&#35299;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#37325;&#35201;&#30340;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22270;&#35889;&#24341;&#23548;&#65288;KGP&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;MD-QA&#20013;&#20026;LLMs&#25552;&#31034;&#27491;&#30830;&#30340;&#19978;&#19979;&#25991;&#65292;&#35813;&#26041;&#27861;&#21253;&#25324;&#22270;&#26500;&#24314;&#27169;&#22359;&#21644;&#22270;&#36941;&#21382;&#27169;&#22359;&#12290;&#23545;&#20110;&#22270;&#26500;&#24314;&#65292;&#25105;&#20204;&#20351;&#29992;&#33410;&#28857;&#26469;&#34920;&#31034;&#25991;&#27573;&#25110;&#25991;&#26723;&#32467;&#26500;&#65288;&#20363;&#22914;&#65292;&#39029;&#38754;/&#34920;&#26684;&#65289;&#65292;&#32780;&#20351;&#29992;&#36793;&#26469;&#34920;&#31034;&#25991;&#27573;&#20043;&#38388;&#30340;&#35821;&#20041;/&#35789;&#27719;&#30456;&#20284;&#24615;&#25110;&#32773;&#25991;&#26723;&#20869;&#30340;&#32467;&#26500;&#20851;&#31995;&#12290;&#23545;&#20110;&#22270;&#36941;&#21382;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;LM&#30340;&#22270;&#36941;&#21382;&#22120;&#65292;&#23427;&#22312;&#33410;&#28857;&#20043;&#38388;&#23548;&#33322;&#24182;&#25910;&#38598;&#25903;&#25345;&#24615;&#30340;&#25991;&#27573;&#65292;&#20197;&#24110;&#21161;LLMs&#22312;MD-QA&#20013;&#36827;&#34892;&#31572;&#26696;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The 'pre-train, prompt, predict' paradigm of large language models (LLMs) has achieved remarkable success in open-domain question answering (OD-QA). However, few works explore this paradigm in the scenario of multi-document question answering (MD-QA), a task demanding a thorough understanding of the logical associations among the contents and structures of different documents. To fill this crucial gap, we propose a Knowledge Graph Prompting (KGP) method to formulate the right context in prompting LLMs for MD-QA, which consists of a graph construction module and a graph traversal module. For graph construction, we create a knowledge graph (KG) over multiple documents with nodes symbolizing passages or document structures (e.g., pages/tables), and edges denoting the semantic/lexical similarity between passages or intra-document structural relations. For graph traversal, we design an LM-guided graph traverser that navigates across nodes and gathers supporting passages assisting LLMs in MD
&lt;/p&gt;</description></item><item><title>BLIVA&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#22810;&#27169;&#24577;LLM&#27169;&#22411;&#65292;&#29992;&#20110;&#26356;&#22909;&#22320;&#22788;&#29702;&#23884;&#20837;&#20102;&#25991;&#26412;&#30340;&#22270;&#20687;&#19978;&#19979;&#25991;&#22330;&#26223;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;InstructBLIP&#30340;&#26597;&#35810;&#23884;&#20837;&#21644;LLaVA&#30340;&#32534;&#30721;&#34917;&#19969;&#23884;&#20837;&#25216;&#26415;&#26469;&#25913;&#36827;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#26377;&#25928;&#35299;&#20915;&#20102;&#25991;&#26412;&#20016;&#23500;&#30340;&#35270;&#35273;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.09936</link><description>&lt;p&gt;
BLIVA: &#19968;&#20010;&#31616;&#21333;&#30340;&#22810;&#27169;&#24577;LLM&#29992;&#20110;&#26356;&#22909;&#22320;&#22788;&#29702;&#25991;&#26412;&#20016;&#23500;&#30340;&#35270;&#35273;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions. (arXiv:2308.09936v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09936
&lt;/p&gt;
&lt;p&gt;
BLIVA&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#22810;&#27169;&#24577;LLM&#27169;&#22411;&#65292;&#29992;&#20110;&#26356;&#22909;&#22320;&#22788;&#29702;&#23884;&#20837;&#20102;&#25991;&#26412;&#30340;&#22270;&#20687;&#19978;&#19979;&#25991;&#22330;&#26223;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;InstructBLIP&#30340;&#26597;&#35810;&#23884;&#20837;&#21644;LLaVA&#30340;&#32534;&#30721;&#34917;&#19969;&#23884;&#20837;&#25216;&#26415;&#26469;&#25913;&#36827;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#26377;&#25928;&#35299;&#20915;&#20102;&#25991;&#26412;&#20016;&#23500;&#30340;&#35270;&#35273;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#36890;&#36807;&#25972;&#21512;&#35270;&#35273;&#29702;&#35299;&#33021;&#21147;&#25193;&#23637;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22312;&#35299;&#20915;&#24320;&#25918;&#24335;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#26080;&#27861;&#20934;&#30830;&#35299;&#37322;&#23884;&#20837;&#25991;&#26412;&#30340;&#22270;&#20687;&#65292;&#36825;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#32463;&#24120;&#21457;&#29983;&#12290;&#20174;&#22270;&#20687;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#26631;&#20934;&#27969;&#31243;&#36890;&#24120;&#28041;&#21450;&#23398;&#20064;&#19968;&#32452;&#22266;&#23450;&#30340;&#26597;&#35810;&#23884;&#20837;&#12290;&#36825;&#20123;&#23884;&#20837;&#34987;&#35774;&#35745;&#20026;&#23553;&#35013;&#22270;&#20687;&#19978;&#19979;&#25991;&#65292;&#24182;&#38543;&#21518;&#29992;&#20316;LLM&#20013;&#30340;&#36719;&#25552;&#31034;&#36755;&#20837;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#36807;&#31243;&#21463;&#20196;&#29260;&#25968;&#37327;&#30340;&#38480;&#21046;&#65292;&#21487;&#33021;&#38480;&#21046;&#23545;&#25991;&#26412;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#22330;&#26223;&#30340;&#35782;&#21035;&#12290;&#20026;&#20102;&#25913;&#36827;&#36825;&#19968;&#28857;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;BLIVA&#65306;InstructBLIP with Visual Assistant&#30340;&#22686;&#24378;&#29256;&#26412;&#12290;BLIVA&#38598;&#25104;&#20102;&#26469;&#33258;InstructBLIP&#30340;&#26597;&#35810;&#23884;&#20837;&#65292;&#24182;&#23558;&#32534;&#30721;&#30340;&#34917;&#19969;&#23884;&#20837;&#30452;&#25509;&#25237;&#24433;&#21040;LLM&#20013;&#65292;&#36825;&#26159;&#21463;&#21040;LLaVA&#30340;&#21551;&#21457;&#30340;&#19968;&#31181;&#25216;&#26415;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#21161;&#20110;&#22788;&#29702;&#25991;&#26412;&#20016;&#23500;&#30340;&#35270;&#35273;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision Language Models (VLMs), which extend Large Language Models (LLM) by incorporating visual understanding capability, have demonstrated significant advancements in addressing open-ended visual question-answering (VQA) tasks. However, these models cannot accurately interpret images infused with text, a common occurrence in real-world scenarios. Standard procedures for extracting information from images often involve learning a fixed set of query embeddings. These embeddings are designed to encapsulate image contexts and are later used as soft prompt inputs in LLMs. Yet, this process is limited to the token count, potentially curtailing the recognition of scenes with text-rich context. To improve upon them, the present study introduces BLIVA: an augmented version of InstructBLIP with Visual Assistant. BLIVA incorporates the query embeddings from InstructBLIP and also directly projects encoded patch embeddings into the LLM, a technique inspired by LLaVA. This approach assists the mode
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#19982;&#20854;&#35757;&#32451;&#20219;&#21153;&#19981;&#30452;&#25509;&#30456;&#20851;&#30340;&#24191;&#27867;&#33021;&#21147;&#65292;&#36890;&#36807;&#38388;&#25509;&#33719;&#21462;&#36807;&#31243;&#20351;&#20854;&#25317;&#26377;&#32508;&#21512;&#33021;&#21147;&#30340;&#21457;&#23637;&#65292;&#36825;&#20123;&#33021;&#21147;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#21487;&#39044;&#27979;&#65292;&#24182;&#19988;&#19982;&#20154;&#31867;&#35748;&#30693;&#26377;&#20851;&#12290;</title><link>http://arxiv.org/abs/2308.09720</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24847;&#24819;&#19981;&#21040;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
On the Unexpected Abilities of Large Language Models. (arXiv:2308.09720v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09720
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#19982;&#20854;&#35757;&#32451;&#20219;&#21153;&#19981;&#30452;&#25509;&#30456;&#20851;&#30340;&#24191;&#27867;&#33021;&#21147;&#65292;&#36890;&#36807;&#38388;&#25509;&#33719;&#21462;&#36807;&#31243;&#20351;&#20854;&#25317;&#26377;&#32508;&#21512;&#33021;&#21147;&#30340;&#21457;&#23637;&#65292;&#36825;&#20123;&#33021;&#21147;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#21487;&#39044;&#27979;&#65292;&#24182;&#19988;&#19982;&#20154;&#31867;&#35748;&#30693;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#23637;&#31034;&#20986;&#19982;&#20854;&#35757;&#32451;&#20219;&#21153;&#65288;&#39044;&#27979;&#20154;&#31867;&#20070;&#20889;&#25991;&#26412;&#30340;&#19979;&#19968;&#20010;&#21333;&#35789;&#65289;&#19981;&#30452;&#25509;&#30456;&#20851;&#30340;&#24191;&#27867;&#33021;&#21147;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#36825;&#31181;&#38388;&#25509;&#33719;&#21462;&#36807;&#31243;&#30340;&#24615;&#36136;&#21450;&#20854;&#19982;&#20854;&#20182;&#24050;&#30693;&#38388;&#25509;&#36807;&#31243;&#30340;&#20851;&#31995;&#12290;&#25991;&#31456;&#20027;&#24352;&#36825;&#31181;&#38388;&#25509;&#33719;&#21462;&#30340;&#19968;&#20010;&#37325;&#35201;&#21103;&#20316;&#29992;&#26159;&#32508;&#21512;&#33021;&#21147;&#30340;&#21457;&#23637;&#12290;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25152;&#24320;&#21457;&#30340;&#33021;&#21147;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#26159;&#21487;&#39044;&#27979;&#30340;&#12290;&#26368;&#21518;&#65292;&#25991;&#31456;&#31616;&#35201;&#35752;&#35770;&#20102;&#36825;&#20123;&#31995;&#32479;&#25152;&#33719;&#24471;&#30340;&#35748;&#30693;&#25216;&#33021;&#19982;&#20154;&#31867;&#35748;&#30693;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models are capable of displaying a wide range of abilities that are not directly connected with the task for which they are trained: predicting the next words of human-written texts. In this article, I discuss the nature of this indirect acquisition process and its relation to other known indirect processes. I argue that an important side effect of such indirect acquisition is the development of integrated abilities. I discuss the extent to which the abilities developed by large language models are predictable. Finally, I briefly discuss the relation between the cognitive skills acquired by these systems and human cognition.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20013;&#25991;&#25340;&#20889;&#32416;&#38169;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#20889;&#35821;&#35328;&#24314;&#27169;&#26469;&#37325;&#26032;&#34920;&#36798;&#25972;&#20010;&#21477;&#23376;&#65292;&#32780;&#19981;&#26159;&#20165;&#20165;&#20381;&#36182;&#38169;&#35823;&#27169;&#24335;&#36827;&#34892;&#23383;&#31526;&#32423;&#21035;&#26631;&#27880;&#65292;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.08796</link><description>&lt;p&gt;
&#12298;&#20013;&#25991;&#25340;&#20889;&#32416;&#38169;&#20316;&#20026;&#25913;&#20889;&#35821;&#35328;&#27169;&#22411;&#12299;
&lt;/p&gt;
&lt;p&gt;
Chinese Spelling Correction as Rephrasing Language Model. (arXiv:2308.08796v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08796
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20013;&#25991;&#25340;&#20889;&#32416;&#38169;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#20889;&#35821;&#35328;&#24314;&#27169;&#26469;&#37325;&#26032;&#34920;&#36798;&#25972;&#20010;&#21477;&#23376;&#65292;&#32780;&#19981;&#26159;&#20165;&#20165;&#20381;&#36182;&#38169;&#35823;&#27169;&#24335;&#36827;&#34892;&#23383;&#31526;&#32423;&#21035;&#26631;&#27880;&#65292;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20013;&#25991;&#25340;&#20889;&#32416;&#38169;&#65288;CSC&#65289;&#65292;&#26088;&#22312;&#26816;&#27979;&#21644;&#32416;&#27491;&#32473;&#23450;&#21477;&#23376;&#20013;&#30340;&#28508;&#22312;&#25340;&#20889;&#38169;&#35823;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#23558;CSC&#35270;&#20026;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#65292;&#24182;&#22312;&#21477;&#23376;&#23545;&#19978;&#24494;&#35843;&#22522;&#20110;BERT&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#22312;&#23558;&#19968;&#20010;&#23383;&#31526;&#26631;&#35760;&#20026;&#21478;&#19968;&#20010;&#23383;&#31526;&#30340;&#36807;&#31243;&#20013;&#23384;&#22312;&#19968;&#20010;&#20851;&#38190;&#32570;&#38519;&#65292;&#21363;&#32416;&#27491;&#36807;&#31243;&#36807;&#20110;&#20381;&#36182;&#38169;&#35823;&#12290;&#36825;&#19982;&#20154;&#31867;&#24605;&#32500;&#30456;&#21453;&#65292;&#20154;&#20204;&#26681;&#25454;&#21477;&#23376;&#30340;&#35821;&#20041;&#37325;&#26032;&#34920;&#36798;&#25972;&#20010;&#21477;&#23376;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#22522;&#20110;&#20043;&#21069;&#35760;&#24518;&#30340;&#38169;&#35823;&#27169;&#24335;&#12290;&#36825;&#31181;&#36829;&#21453;&#30452;&#35273;&#30340;&#23398;&#20064;&#36807;&#31243;&#23548;&#33268;&#26426;&#22120;&#25340;&#20889;&#32416;&#38169;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#21487;&#36801;&#31227;&#24615;&#21463;&#21040;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#25913;&#20889;&#35821;&#35328;&#24314;&#27169;&#8221;&#65288;ReLM&#65289;&#65292;&#20854;&#20013;&#27169;&#22411;&#36890;&#36807;&#22635;&#20805;&#39069;&#22806;&#30340;&#20301;&#32622;&#26469;&#37325;&#26032;&#34920;&#36798;&#25972;&#20010;&#21477;&#23376;&#65292;&#32780;&#19981;&#26159;&#36827;&#34892;&#23383;&#31526;&#32423;&#21035;&#30340;&#26631;&#27880;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#35757;&#32451;&#33539;&#24335;&#22312;&#24494;&#35843;&#21518;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies Chinese Spelling Correction (CSC), which aims to detect and correct potential spelling errors in a given sentence. Current state-of-the-art methods regard CSC as a sequence tagging task and fine-tune BERT-based models on sentence pairs. However, we note a critical flaw in the process of tagging one character to another, that the correction is excessively conditioned on the error. This is opposite from human mindset, where individuals rephrase the complete sentence based on its semantics, rather than solely on the error patterns memorized before. Such a counter-intuitive learning process results in the bottleneck of generalizability and transferability of machine spelling correction. To address this, we propose $Rephrasing Language Modeling$ (ReLM), where the model is trained to rephrase the entire sentence by infilling additional slots, instead of character-to-character tagging. This novel training paradigm achieves the new state-of-the-art results across fine-tuned 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32463;&#27982;&#26377;&#25928;&#30340;&#35821;&#35328;&#27169;&#22411;&#36873;&#25321;&#26694;&#26550;&#65288;CELMOC&#65289;&#65292;&#36890;&#36807;&#20803;&#27169;&#22411;&#39044;&#27979;&#22312;&#19981;&#21516;&#36755;&#20837;&#19978;&#34920;&#29616;&#33391;&#22909;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#22312;&#20302;&#25104;&#26412;&#19979;&#23454;&#29616;&#39640;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.06077</link><description>&lt;p&gt;
&#39134;&#25293;&#25110;&#22823;&#28846;&#65311;&#36890;&#36807;&#20803;&#27169;&#22411;&#36873;&#25321;&#32463;&#27982;&#26377;&#25928;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Fly-Swat or Cannon? Cost-Effective Language Model Choice via Meta-Modeling. (arXiv:2308.06077v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32463;&#27982;&#26377;&#25928;&#30340;&#35821;&#35328;&#27169;&#22411;&#36873;&#25321;&#26694;&#26550;&#65288;CELMOC&#65289;&#65292;&#36890;&#36807;&#20803;&#27169;&#22411;&#39044;&#27979;&#22312;&#19981;&#21516;&#36755;&#20837;&#19978;&#34920;&#29616;&#33391;&#22909;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#22312;&#20302;&#25104;&#26412;&#19979;&#23454;&#29616;&#39640;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#25454;&#31185;&#23398;&#39046;&#22495;&#20013;&#21464;&#24471;&#26080;&#22788;&#19981;&#22312;&#12290;&#23545;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#21487;&#20197;&#23558;&#36755;&#20837;&#20316;&#20026;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#65292;&#36890;&#36807;LM&#30340;&#36755;&#20986;&#26469;&#25552;&#21462;&#35299;&#20915;&#26041;&#26696;&#12290;LM&#30340;&#24615;&#33021;&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#32780;&#19981;&#26029;&#25552;&#39640;&#65292;&#20294;&#21516;&#26102;&#26597;&#35810;&#36234;&#26469;&#36234;&#22823;&#30340;&#27169;&#22411;&#30340;&#32463;&#27982;&#25104;&#26412;&#20063;&#22312;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#19981;&#26159;&#25152;&#26377;&#30340;&#36755;&#20837;&#37117;&#24456;&#38590;&#65306;&#26377;&#20123;&#36755;&#20837;&#38656;&#35201;&#26356;&#22823;&#30340;LM&#25165;&#33021;&#33719;&#24471;&#20196;&#20154;&#28385;&#24847;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#32780;&#23545;&#20110;&#20854;&#20182;&#36755;&#20837;&#65292;&#36739;&#23567;&#30340;LM&#23601;&#36275;&#22815;&#20102;&#12290;&#22522;&#20110;&#36825;&#20010;&#20107;&#23454;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#32463;&#27982;&#26377;&#25928;&#30340;&#35821;&#35328;&#27169;&#22411;&#36873;&#25321;&#26694;&#26550;&#65288;CELMOC&#65289;&#12290;&#32473;&#23450;&#19968;&#32452;&#36755;&#20837;&#21644;&#19968;&#32452;&#20505;&#36873;LM&#65292;CELMOC&#26681;&#25454;&#25152;&#35859;&#30340;&#20803;&#27169;&#22411;&#32874;&#26126;&#22320;&#23558;&#27599;&#20010;&#36755;&#20837;&#20998;&#37197;&#32473;&#19968;&#20010;&#22312;&#35813;&#36755;&#20837;&#19978;&#39044;&#27979;&#34920;&#29616;&#33391;&#22909;&#30340;LM&#65292;&#20197;&#26399;&#22312;&#20302;&#25104;&#26412;&#19979;&#23454;&#29616;&#39640;&#25972;&#20307;&#24615;&#33021;&#12290;&#29992;&#25143;&#21487;&#20197;&#28789;&#27963;&#35843;&#25972;&#25104;&#26412;&#19982;&#24615;&#33021;&#30340;&#26435;&#34913;&#12290;&#36873;&#39033;&#21253;&#25324;&#65292;&#26368;&#22823;&#21270;&#24635;&#20307;&#24615;&#33021;&#65288;&#25110;&#22788;&#29702;&#36755;&#20837;&#30340;&#25968;&#37327;&#65289;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative language models (LMs) have become omnipresent across data science. For a wide variety of tasks, inputs can be phrased as natural language prompts for an LM, from whose output the solution can then be extracted. LM performance has consistently been increasing with model size - but so has the monetary cost of querying the ever larger models. Importantly, however, not all inputs are equally hard: some require larger LMs for obtaining a satisfactory solution, whereas for others smaller LMs suffice. Based on this fact, we design a framework for Cost-Effective Language Model Choice (CELMOC). Given a set of inputs and a set of candidate LMs, CELMOC judiciously assigns each input to an LM predicted to do well on the input according to a so-called meta-model, aiming to achieve high overall performance at low cost. The cost-performance trade-off can be flexibly tuned by the user. Options include, among others, maximizing total expected performance (or the number of processed inputs) w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ETHER&#65292;&#36890;&#36807;&#23545;&#40784;&#32039;&#24613;&#27807;&#36890;&#26469;&#35299;&#20915;&#22238;&#39038;&#24615;&#32463;&#39564;&#37325;&#28436;&#20013;&#30340;&#38382;&#39064;&#65292;&#20811;&#26381;&#20102;&#20808;&#21069;&#26550;&#26500;&#20381;&#36182;&#39044;&#35774;&#20989;&#25968;&#30340;&#38480;&#21046;&#65292;&#24182;&#25552;&#39640;&#20102;&#25968;&#25454;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.15494</link><description>&lt;p&gt;
ETHER: &#23545;&#20110;&#22238;&#39038;&#24615;&#32463;&#39564;&#37325;&#28436;&#30340;&#32039;&#23494;&#27807;&#36890;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
ETHER: Aligning Emergent Communication for Hindsight Experience Replay. (arXiv:2307.15494v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ETHER&#65292;&#36890;&#36807;&#23545;&#40784;&#32039;&#24613;&#27807;&#36890;&#26469;&#35299;&#20915;&#22238;&#39038;&#24615;&#32463;&#39564;&#37325;&#28436;&#20013;&#30340;&#38382;&#39064;&#65292;&#20811;&#26381;&#20102;&#20808;&#21069;&#26550;&#26500;&#20381;&#36182;&#39044;&#35774;&#20989;&#25968;&#30340;&#38480;&#21046;&#65292;&#24182;&#25552;&#39640;&#20102;&#25968;&#25454;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#30340;&#36319;&#38543;&#23545;&#20110;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#21644;&#20154;&#31867;&#20043;&#38388;&#30340;&#21512;&#20316;&#33267;&#20851;&#37325;&#35201;&#12290;&#33258;&#28982;&#35821;&#35328;&#26465;&#20214;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#23637;&#31034;&#20102;&#33258;&#28982;&#35821;&#35328;&#30340;&#29305;&#24615;&#65292;&#22914;&#32452;&#21512;&#24615;&#65292;&#33021;&#22815;&#25552;&#20379;&#23398;&#20064;&#22797;&#26434;&#31574;&#30053;&#30340;&#24378;&#24402;&#32435;&#20559;&#22909;&#12290;&#20808;&#21069;&#30340;&#26550;&#26500;&#22914;HIGhER&#32467;&#21512;&#20102;&#35821;&#35328;&#26465;&#20214;&#19982;&#22238;&#39038;&#24615;&#32463;&#39564;&#37325;&#28436;&#65288;HER&#65289;&#26469;&#22788;&#29702;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#12290;&#28982;&#32780;&#65292;&#19982;HER&#31867;&#20284;&#65292;HIGhER&#20381;&#36182;&#20110;&#19968;&#20010;&#39044;&#35774;&#30340;&#20989;&#25968;&#26469;&#25552;&#20379;&#21453;&#39304;&#20449;&#21495;&#65292;&#25351;&#31034;&#21738;&#31181;&#35821;&#35328;&#25551;&#36848;&#22312;&#21738;&#31181;&#29366;&#24577;&#19979;&#26377;&#25928;&#12290;&#36825;&#31181;&#20381;&#36182;&#20110;&#39044;&#35774;&#20989;&#25968;&#30340;&#38480;&#21046;&#38480;&#21046;&#20102;&#20854;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;HIGhER&#21482;&#21033;&#29992;&#25104;&#21151;&#30340;&#24378;&#21270;&#23398;&#20064;&#36712;&#36857;&#20013;&#21253;&#21547;&#30340;&#35821;&#35328;&#20449;&#24687;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#20854;&#26368;&#32456;&#24615;&#33021;&#21644;&#25968;&#25454;&#25928;&#29575;&#12290;&#27809;&#26377;&#26089;&#26399;&#25104;&#21151;&#36712;&#36857;&#65292;HIGhER&#24182;&#19981;&#27604;&#20854;&#26500;&#24314;&#20110;&#20043;&#19978;&#30340;DQN&#26356;&#22909;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32039;&#23494;&#25991;&#26412;&#22238;&#39038;&#24615;&#32463;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language instruction following is paramount to enable collaboration between artificial agents and human beings. Natural language-conditioned reinforcement learning (RL) agents have shown how natural languages' properties, such as compositionality, can provide a strong inductive bias to learn complex policies. Previous architectures like HIGhER combine the benefit of language-conditioning with Hindsight Experience Replay (HER) to deal with sparse rewards environments. Yet, like HER, HIGhER relies on an oracle predicate function to provide a feedback signal highlighting which linguistic description is valid for which state. This reliance on an oracle limits its application. Additionally, HIGhER only leverages the linguistic information contained in successful RL trajectories, thus hurting its final performance and data-efficiency. Without early successful trajectories, HIGhER is no better than DQN upon which it is built. In this paper, we propose the Emergent Textual Hindsight Ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#35821;&#38899;&#21512;&#25104;&#26041;&#27861;&#26469;&#35299;&#20915;&#33258;&#22238;&#24402;&#21644;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#20013;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#35821;&#20041;&#32534;&#30721;&#26041;&#38754;&#36827;&#34892;&#20102;&#27604;&#36739;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.15484</link><description>&lt;p&gt;
&#29992;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#21644;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26368;&#23567;&#30417;&#30563;&#35821;&#38899;&#21512;&#25104;&#65306;&#22522;&#20110;&#35821;&#20041;&#32534;&#30721;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Minimally-Supervised Speech Synthesis with Conditional Diffusion Model and Language Model: A Comparative Study of Semantic Coding. (arXiv:2307.15484v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15484
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#35821;&#38899;&#21512;&#25104;&#26041;&#27861;&#26469;&#35299;&#20915;&#33258;&#22238;&#24402;&#21644;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#20013;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#35821;&#20041;&#32534;&#30721;&#26041;&#38754;&#36827;&#34892;&#20102;&#27604;&#36739;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23545;&#20110;&#33021;&#22815;&#37319;&#29992;&#26368;&#23567;&#30417;&#30563;&#35757;&#32451;&#26041;&#27861;&#30340;&#25991;&#26412;&#21040;&#35821;&#38899;(TTS)&#25216;&#26415;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#20004;&#31181;&#31163;&#25955;&#35821;&#38899;&#34920;&#31034;&#24182;&#20351;&#29992;&#20004;&#31181;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#26469;&#35299;&#32806;TTS&#12290;&#20026;&#20102;&#35299;&#20915;&#31163;&#25955;&#34920;&#31034;&#20013;&#30340;&#39640;&#32500;&#24230;&#21644;&#27874;&#24418;&#22833;&#30495;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Diff-LM-Speech&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#23558;&#35821;&#20041;&#23884;&#20837;&#27169;&#22411;&#20026;&#22522;&#20110;mel&#39057;&#35889;&#22270;&#65292;&#24182;&#24341;&#20837;&#22522;&#20110;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#38901;&#24459;&#29942;&#39048;&#30340;&#25552;&#31034;&#32534;&#30721;&#32467;&#26500;&#65292;&#20197;&#25552;&#39640;&#25552;&#31034;&#34920;&#31034;&#33021;&#21147;&#12290;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#24120;&#24120;&#36935;&#21040;&#32570;&#22833;&#21644;&#37325;&#22797;&#21333;&#35789;&#30340;&#38382;&#39064;&#65292;&#32780;&#38750;&#33258;&#22238;&#24402;&#26694;&#26550;&#30001;&#20110;&#39044;&#27979;&#27169;&#22411;&#30340;&#23384;&#22312;&#23548;&#33268;&#34920;&#36798;&#24179;&#22343;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Tetra-Diff-Speech&#65292;&#35813;&#26041;&#27861;&#35774;&#35745;&#20102;&#19968;&#20010;&#26102;&#38271;&#25193;&#25955;&#27169;&#22411;&#20197;&#23454;&#29616;&#22810;&#26679;&#21270;&#30340;&#38901;&#24459;&#34920;&#36798;&#12290;&#25105;&#20204;&#26399;&#26395;&#35821;&#20041;&#32534;&#30721;&#30340;&#20449;&#24687;&#20869;&#23481;&#20171;&#20110;...
&lt;/p&gt;
&lt;p&gt;
Recently, there has been a growing interest in text-to-speech (TTS) methods that can be trained with minimal supervision by combining two types of discrete speech representations and using two sequence-to-sequence tasks to decouple TTS. To address the challenges associated with high dimensionality and waveform distortion in discrete representations, we propose Diff-LM-Speech, which models semantic embeddings into mel-spectrogram based on diffusion models and introduces a prompt encoder structure based on variational autoencoders and prosody bottlenecks to improve prompt representation capabilities. Autoregressive language models often suffer from missing and repeated words, while non-autoregressive frameworks face expression averaging problems due to duration prediction models. To address these issues, we propose Tetra-Diff-Speech, which designs a duration diffusion model to achieve diverse prosodic expressions. While we expect the information content of semantic coding to be between t
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#29983;&#25104;&#24335;&#26816;&#32034;&#21644;&#32463;&#20856;&#30340;&#23398;&#20064;&#25490;&#24207;&#33539;&#20363;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#27573;&#33853;&#25490;&#24207;&#25439;&#22833;&#26469;&#35757;&#32451;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#30452;&#25509;&#20248;&#21270;&#33258;&#22238;&#24402;&#27169;&#22411;&#26397;&#30528;&#26368;&#20248;&#35299;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.15222</link><description>&lt;p&gt;
&#23398;&#20064;&#22312;&#29983;&#25104;&#24335;&#26816;&#32034;&#20013;&#36827;&#34892;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Learning to Rank in Generative Retrieval. (arXiv:2306.15222v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15222
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#29983;&#25104;&#24335;&#26816;&#32034;&#21644;&#32463;&#20856;&#30340;&#23398;&#20064;&#25490;&#24207;&#33539;&#20363;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#27573;&#33853;&#25490;&#24207;&#25439;&#22833;&#26469;&#35757;&#32451;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#30452;&#25509;&#20248;&#21270;&#33258;&#22238;&#24402;&#27169;&#22411;&#26397;&#30528;&#26368;&#20248;&#35299;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#26816;&#32034;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#25991;&#26412;&#26816;&#32034;&#33539;&#20363;&#65292;&#23427;&#23558;&#30456;&#20851;&#27573;&#33853;&#30340;&#26631;&#35782;&#31526;&#23383;&#31526;&#20018;&#29983;&#25104;&#20026;&#26816;&#32034;&#30446;&#26631;&#12290;&#36825;&#31181;&#33539;&#20363;&#21033;&#29992;&#24378;&#22823;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#20195;&#34920;&#20102;&#19982;&#20256;&#32479;&#30340;&#23398;&#20064;&#25490;&#24207;&#26041;&#27861;&#26377;&#25152;&#19981;&#21516;&#30340;&#26032;&#33539;&#20363;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20854;&#24555;&#36895;&#21457;&#23637;&#65292;&#24403;&#21069;&#30340;&#29983;&#25104;&#24335;&#26816;&#32034;&#26041;&#27861;&#20173;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#23427;&#20204;&#36890;&#24120;&#20381;&#36182;&#21551;&#21457;&#24335;&#20989;&#25968;&#23558;&#39044;&#27979;&#30340;&#26631;&#35782;&#31526;&#36716;&#25442;&#20026;&#27573;&#33853;&#25490;&#24207;&#21015;&#34920;&#65292;&#36825;&#22312;&#29983;&#25104;&#24335;&#26816;&#32034;&#30340;&#23398;&#20064;&#30446;&#26631;&#19982;&#26399;&#26395;&#30340;&#27573;&#33853;&#25490;&#24207;&#30446;&#26631;&#20043;&#38388;&#20135;&#29983;&#20102;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#25991;&#26412;&#29983;&#25104;&#30340;&#22266;&#26377;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;&#22312;&#29983;&#25104;&#24335;&#26816;&#32034;&#20013;&#20173;&#28982;&#23384;&#22312;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;LTRGR&#65292;&#23427;&#23558;&#29983;&#25104;&#24335;&#26816;&#32034;&#19982;&#32463;&#20856;&#30340;&#23398;&#20064;&#25490;&#24207;&#33539;&#20363;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#27573;&#33853;&#25490;&#24207;&#25439;&#22833;&#35757;&#32451;&#19968;&#20010;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#35813;&#25439;&#22833;&#30452;&#25509;&#20248;&#21270;&#33258;&#22238;&#24402;&#27169;&#22411;&#26397;&#30528;&#26368;&#20248;&#35299;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative retrieval is a promising new paradigm in text retrieval that generates identifier strings of relevant passages as the retrieval target. This paradigm leverages powerful generation models and represents a new paradigm distinct from traditional learning-to-rank methods. However, despite its rapid development, current generative retrieval methods are still limited. They typically rely on a heuristic function to transform predicted identifiers into a passage rank list, which creates a gap between the learning objective of generative retrieval and the desired passage ranking target. Moreover, the inherent exposure bias problem of text generation also persists in generative retrieval. To address these issues, we propose a novel framework, called LTRGR, that combines generative retrieval with the classical learning-to-rank paradigm. Our approach involves training an autoregressive model using a passage rank loss, which directly optimizes the autoregressive model toward the optimal 
&lt;/p&gt;</description></item><item><title>ZeroSCROLLS&#26159;&#19968;&#20010;&#29992;&#20110;&#38271;&#25991;&#26412;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#38646;Shot&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#20845;&#20010;&#20219;&#21153;&#21644;&#22235;&#20010;&#25968;&#25454;&#38598;&#65292;&#33021;&#22815;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#24403;&#21069;&#65292;GPT-4&#30340;&#24179;&#22343;&#24471;&#20998;&#26368;&#39640;&#65292;&#20294;&#22312;&#32858;&#21512;&#20219;&#21153;&#31561;&#22810;&#20010;&#25361;&#25112;&#19978;&#65292;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2305.14196</link><description>&lt;p&gt;
ZeroSCROLLS&#65306;&#19968;&#20010;&#29992;&#20110;&#38271;&#25991;&#26412;&#29702;&#35299;&#30340;&#38646;Shot&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
ZeroSCROLLS: A Zero-Shot Benchmark for Long Text Understanding. (arXiv:2305.14196v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14196
&lt;/p&gt;
&lt;p&gt;
ZeroSCROLLS&#26159;&#19968;&#20010;&#29992;&#20110;&#38271;&#25991;&#26412;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#38646;Shot&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#20845;&#20010;&#20219;&#21153;&#21644;&#22235;&#20010;&#25968;&#25454;&#38598;&#65292;&#33021;&#22815;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#24403;&#21069;&#65292;GPT-4&#30340;&#24179;&#22343;&#24471;&#20998;&#26368;&#39640;&#65292;&#20294;&#22312;&#32858;&#21512;&#20219;&#21153;&#31561;&#22810;&#20010;&#25361;&#25112;&#19978;&#65292;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102; ZeroSCROLLS&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#38271;&#25991;&#26412;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#38646;Shot&#22522;&#20934;&#27979;&#35797;&#65292;&#20165;&#21253;&#21547;&#27979;&#35797;&#38598;&#32780;&#27809;&#26377;&#35757;&#32451;&#25110;&#24320;&#21457;&#25968;&#25454;&#12290;&#25105;&#20204;&#20174;SCROLLS&#22522;&#20934;&#27979;&#35797;&#20013;&#36866;&#24212;&#20102;&#20845;&#20010;&#20219;&#21153;&#65292;&#24182;&#28155;&#21152;&#20102;&#22235;&#20010;&#26032;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#20004;&#20010;&#26032;&#30340;&#20449;&#24687;&#34701;&#21512;&#20219;&#21153;&#65292;&#20363;&#22914;&#32858;&#21512;&#27491;&#38754;&#35780;&#20215;&#30340;&#30334;&#20998;&#27604;&#12290;&#20351;&#29992;ZeroSCROLLS&#65292;&#25105;&#20204;&#23545;&#24320;&#28304;&#21644;&#38381;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;Claude&#20248;&#20110;ChatGPT&#65292;&#24182;&#19988;GPT-4&#33719;&#24471;&#20102;&#26368;&#39640;&#30340;&#24179;&#22343;&#20998;&#25968;&#12290;&#28982;&#32780;&#65292;&#22312;ZeroSCROLLS&#30340;&#22810;&#20010;&#24320;&#25918;&#25361;&#25112;&#26041;&#38754;&#65288;&#20363;&#22914;&#65292;&#32858;&#21512;&#20219;&#21153;&#65289;&#65292;&#36824;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#65292;&#22240;&#20026;&#27169;&#22411;&#24456;&#38590;&#36890;&#36807;&#26420;&#32032;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#30001;&#20110;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#36824;&#22312;&#19981;&#26029;&#26356;&#26032;&#65292;&#25105;&#20204;&#36992;&#35831;&#30740;&#31350;&#20154;&#21592;&#22312;&#23454;&#26102;&#30340;ZeroSCROLLS&#25490;&#34892;&#27036;&#19978;&#35780;&#20272;&#20182;&#20204;&#30340;&#24819;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce ZeroSCROLLS, a zero-shot benchmark for natural language understanding over long texts, which contains only test sets, without training or development data. We adapt six tasks from the SCROLLS benchmark, and add four new datasets, including two novel information fusing tasks, such as aggregating the percentage of positive reviews. Using ZeroSCROLLS, we conduct a comprehensive evaluation of both open-source and closed large language models, finding that Claude outperforms ChatGPT, and that GPT-4 achieves the highest average score. However, there is still room for improvement on multiple open challenges in ZeroSCROLLS, such as aggregation tasks, where models struggle to pass the naive baseline. As the state of the art is a moving target, we invite researchers to evaluate their ideas on the live ZeroSCROLLS leaderboard
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#23545;&#35805;&#31995;&#32479;&#35270;&#35282;&#65292;&#20854;&#20013;&#22270;&#20687;&#35299;&#37322;&#26159;&#22522;&#20110;&#23545;&#35805;&#19978;&#19979;&#25991;&#30340;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#35266;&#28857;&#26469;&#26500;&#24314;&#22810;&#27169;&#24577;&#23545;&#35805;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2305.10512</link><description>&lt;p&gt;
IMAD: &#22270;&#20687;&#22686;&#24378;&#30340;&#22810;&#27169;&#24335;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
IMAD: IMage-Augmented multi-modal Dialogue. (arXiv:2305.10512v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10512
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#23545;&#35805;&#31995;&#32479;&#35270;&#35282;&#65292;&#20854;&#20013;&#22270;&#20687;&#35299;&#37322;&#26159;&#22522;&#20110;&#23545;&#35805;&#19978;&#19979;&#25991;&#30340;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#35266;&#28857;&#26469;&#26500;&#24314;&#22810;&#27169;&#24577;&#23545;&#35805;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#23545;&#35805;&#31995;&#32479;&#24050;&#32463;&#22312;&#22788;&#29702;&#22522;&#20110;&#25991;&#26412;&#30340;&#36890;&#35759;&#26041;&#38754;&#21462;&#24471;&#20102;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36824;&#27809;&#26377;&#26377;&#25928;&#22320;&#34701;&#21512;&#35270;&#35273;&#20449;&#24687;&#65292;&#36825;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#22312;&#23545;&#35805;&#29983;&#25104;&#20013;&#34701;&#21512;&#22270;&#20687;&#30340;&#27169;&#22411;&#19987;&#27880;&#20110;&#35752;&#35770;&#22270;&#20687;&#26412;&#36523;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#23545;&#35805;&#31995;&#32479;&#35270;&#35282;&#65292;&#35299;&#37322;&#20102;&#23545;&#35805;&#20013;&#22270;&#20687;&#30340;&#19978;&#19979;&#25991;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#26088;&#22312;&#25193;&#23637;&#24403;&#21069;&#23545;&#35805;&#31995;&#32479;&#30340;&#33021;&#21147;&#65292;&#20174;&#21333;&#19968;&#27169;&#24335;&#65288;&#25991;&#26412;&#65289;&#21521;&#22810;&#27169;&#24577;&#36716;&#25442;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#21253;&#21547;&#22270;&#20687;&#21644;&#23545;&#35805;&#19978;&#19979;&#25991;&#30340;&#32463;&#36807;&#39564;&#35777;&#30340;&#33521;&#25991;&#25968;&#25454;&#38598;&#26159;&#36825;&#39033;&#20219;&#21153;&#30340;&#38590;&#28857;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#26041;&#27861;&#26469;&#33258;&#21160;&#26500;&#24314;&#22810;&#27169;&#24577;&#23545;&#35805;&#25968;&#25454;&#38598;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#21033;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#30456;&#20284;&#24615;&#21644;&#21477;&#23376;&#30456;&#20284;&#24615;&#26469;&#35782;&#21035;&#21738;&#20123;&#35805;&#35821;&#21487;&#20197;&#29992;&#22270;&#20687;&#26367;&#25442;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#25105;&#20204;&#26367;&#25442;&#37027;&#20123;&#35805;&#35821;&#12290;
&lt;/p&gt;
&lt;p&gt;
Currently, dialogue systems have achieved high performance in processing text-based communication. However, they have not yet effectively incorporated visual information, which poses a significant challenge. Furthermore, existing models that incorporate images in dialogue generation focus on discussing the image itself. Our proposed approach presents a novel perspective on multi-modal dialogue systems, which interprets the image in the context of the dialogue. By doing so, we aim to expand the capabilities of current dialogue systems and transition them from single modality (text) to multi-modality. However, there is a lack of validated English datasets that contain both images and dialogue contexts for this task. Thus, we propose a two-stage approach to automatically construct a multi-modal dialogue dataset. In the first stage, we utilize text-to-image similarity and sentence similarity to identify which utterances could be replaced with an image. In the second stage, we replace those
&lt;/p&gt;</description></item><item><title>ArtGPT-4&#26159;&#19968;&#31181;&#22522;&#20110;&#36866;&#37197;&#22120;&#22686;&#24378;&#30340;MiniGPT-4&#27169;&#22411;&#65292;&#19987;&#27880;&#20110;&#35299;&#20915;&#22270;&#20687;&#29702;&#35299;&#26041;&#38754;&#30340;&#38382;&#39064;&#65292;&#33021;&#22815;&#22312;&#30701;&#26102;&#38388;&#20869;&#35757;&#32451;&#20986;&#20855;&#22791;&#33391;&#22909;&#35270;&#35273;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.07490</link><description>&lt;p&gt;
ArtGPT-4: &#22522;&#20110;&#36866;&#37197;&#22120;&#22686;&#24378;&#30340;MiniGPT-4&#27169;&#22411;&#30340;&#33402;&#26415;&#35270;&#35273;&#35821;&#35328;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
ArtGPT-4: Artistic Vision-Language Understanding with Adapter-enhanced MiniGPT-4. (arXiv:2305.07490v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07490
&lt;/p&gt;
&lt;p&gt;
ArtGPT-4&#26159;&#19968;&#31181;&#22522;&#20110;&#36866;&#37197;&#22120;&#22686;&#24378;&#30340;MiniGPT-4&#27169;&#22411;&#65292;&#19987;&#27880;&#20110;&#35299;&#20915;&#22270;&#20687;&#29702;&#35299;&#26041;&#38754;&#30340;&#38382;&#39064;&#65292;&#33021;&#22815;&#22312;&#30701;&#26102;&#38388;&#20869;&#35757;&#32451;&#20986;&#20855;&#22791;&#33391;&#22909;&#35270;&#35273;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#27604;&#22914;ChatGPT&#21644;GPT-4&#31561;&#27169;&#22411;&#22312;&#22810;&#31181;&#35821;&#35328;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#33021;&#21147;&#12290;&#20294;&#26159;&#65292;&#23545;&#36825;&#26679;&#30340;&#22823;&#35268;&#27169;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#32780;&#25214;&#21040;&#19982;&#27169;&#22411;&#35268;&#27169;&#21305;&#37197;&#30340;&#25968;&#25454;&#38598;&#36890;&#24120;&#20063;&#24456;&#22256;&#38590;&#12290;&#24494;&#35843;&#21644;&#20351;&#29992;&#26032;&#26041;&#27861;&#35757;&#32451;&#21442;&#25968;&#36739;&#23569;&#30340;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;MiniGPT-4&#27169;&#22411;&#20415;&#26159;&#20854;&#20013;&#20043;&#19968;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#36816;&#29992;&#26032;&#39062;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#38761;&#26032;&#24615;&#30340;&#22521;&#35757;&#31574;&#30053;&#23454;&#29616;&#20102;&#19982;GPT-4&#30456;&#24403;&#30340;&#35270;&#35273;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;&#20294;&#26159;&#65292;&#35813;&#27169;&#22411;&#22312;&#22270;&#20687;&#29702;&#35299;&#26041;&#38754;&#20173;&#28982;&#38754;&#20020;&#19968;&#20123;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#33402;&#26415;&#22270;&#29255;&#26041;&#38754;&#12290;ArtGPT-4&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#26088;&#22312;&#24212;&#23545;&#36825;&#20123;&#23616;&#38480;&#12290;ArtGPT-4&#20351;&#29992;Tesla A100&#35774;&#22791;&#23545;&#22270;&#20687;-&#25991;&#26412;&#23545;&#36827;&#34892;&#35757;&#32451;&#65292;&#20165;&#29992;&#20102;&#32422;200GB&#30340;&#25968;&#25454;&#65292;&#22312;2&#23567;&#26102;&#20869;&#23601;&#33021;&#23637;&#31034;&#20986;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, large language models (LLMs) have made significant progress in natural language processing (NLP), with models like ChatGPT and GPT-4 achieving impressive capabilities in various linguistic tasks. However, training models on such a large scale is challenging, and finding datasets that match the model's scale is often difficult. Fine-tuning and training models with fewer parameters using novel methods have emerged as promising approaches to overcome these challenges. One such model is MiniGPT-4, which achieves comparable vision-language understanding to GPT-4 by leveraging novel pre-training models and innovative training strategies. However, the model still faces some challenges in image understanding, particularly in artistic pictures. A novel multimodal model called ArtGPT-4 has been proposed to address these limitations. ArtGPT-4 was trained on image-text pairs using a Tesla A100 device in just 2 hours, using only about 200 GB of data. The model can depict images wit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#30693;&#35782;&#21152;&#24378;&#30340;&#20195;&#29702;&#26694;&#26550;&#65292;&#29992;&#20110;&#21152;&#24378;&#20195;&#29702;&#22312;&#25991;&#26412;&#28216;&#25103;&#20013;&#30340;&#21151;&#33021;&#22522;&#30784;&#65292;&#20855;&#26377;&#35760;&#24518;&#20808;&#21069;&#25805;&#20316;&#21644;&#29615;&#22659;&#23545;&#35937;&#21487;&#34892;&#24615;&#20004;&#39033;&#39046;&#22495;&#30693;&#35782;&#65292;&#25903;&#25345;&#19977;&#20010;&#20195;&#34920;&#24615;&#27169;&#22411;&#31867;&#12290;</title><link>http://arxiv.org/abs/2305.05091</link><description>&lt;p&gt;
&#20132;&#20114;&#24335;&#25991;&#26412;&#28216;&#25103;&#20013;&#22522;&#20110;&#30693;&#35782;&#22686;&#24378;&#30340;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Knowledge-enhanced Agents for Interactive Text Games. (arXiv:2305.05091v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05091
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#30693;&#35782;&#21152;&#24378;&#30340;&#20195;&#29702;&#26694;&#26550;&#65292;&#29992;&#20110;&#21152;&#24378;&#20195;&#29702;&#22312;&#25991;&#26412;&#28216;&#25103;&#20013;&#30340;&#21151;&#33021;&#22522;&#30784;&#65292;&#20855;&#26377;&#35760;&#24518;&#20808;&#21069;&#25805;&#20316;&#21644;&#29615;&#22659;&#23545;&#35937;&#21487;&#34892;&#24615;&#20004;&#39033;&#39046;&#22495;&#30693;&#35782;&#65292;&#25903;&#25345;&#19977;&#20010;&#20195;&#34920;&#24615;&#27169;&#22411;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#30340;&#20132;&#27969;&#26159;&#26234;&#33021;&#19981;&#21487;&#25110;&#32570;&#30340;&#19968;&#20010;&#26041;&#38754;&#65292;&#38656;&#35201;&#35745;&#31639;&#27169;&#22411;&#23398;&#20064;&#21644;&#25512;&#29702;&#26377;&#20851;&#19990;&#30028;&#27010;&#24565;&#30340;&#30693;&#35782;&#65292;&#20854;&#30417;&#30563;&#31243;&#24230;&#20063;&#21508;&#19981;&#30456;&#21516;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20197;&#21152;&#24378;&#20195;&#29702;&#22312;&#22522;&#20110;&#25991;&#26412;&#30340;&#28216;&#25103;&#20013;&#30340;&#21151;&#33021;&#22522;&#30784;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#23558;&#20004;&#31181;&#39046;&#22495;&#30693;&#35782;&#27880;&#20837;&#22522;&#20110;&#23398;&#20064;&#30340;&#20195;&#29702;&#20013;&#65306;&#20808;&#21069;&#27491;&#30830;&#25805;&#20316;&#30340;&#35760;&#24518;&#21644;&#29615;&#22659;&#20013;&#30456;&#20851;&#23545;&#35937;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#25903;&#25345;&#19977;&#20010;&#20195;&#34920;&#24615;&#27169;&#22411;&#31867;&#65306;`&#32431;`&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#22522;&#20110;&#35760;&#24518;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015; (seq2seq) &#27169;&#22411;&#21644;&#22522;&#20110;&#27880;&#24847;&#26426;&#21046;&#30340; seq2seq&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Communication via natural language is a crucial aspect of intelligence, and it requires computational models to learn and reason about world concepts, with varying levels of supervision. While there has been significant progress made on fully-supervised non-interactive tasks, such as question-answering and procedural text understanding, much of the community has turned to various sequential interactive tasks, as in semi-Markov text-based games, which have revealed limitations of existing approaches in terms of coherence, contextual awareness, and their ability to learn effectively from the environment. In this paper, we propose a framework for enabling improved functional grounding of agents in text-based games. Specifically, we consider two forms of domain knowledge that we inject into learning-based agents: memory of previous correct actions and affordances of relevant objects in the environment. Our framework supports three representative model classes: `pure' reinforcement learning
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20449;&#21495;&#25945;&#32946;&#31185;&#23398;&#38382;&#39064;&#22238;&#31572;&#30340;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;&#65292;&#36890;&#36807;&#29983;&#25104;&#39640;&#36136;&#37327;COT&#21512;&#29702;&#21270;&#20449;&#21495;&#65292;&#21516;&#26102;&#38477;&#20302;&#20102;&#20154;&#24037;&#27880;&#37322;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2305.03453</link><description>&lt;p&gt;
T-SciQ: &#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20449;&#21495;&#25945;&#25480;&#22810;&#27169;&#24577;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;&#22312;&#31185;&#23398;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
T-SciQ: Teaching Multimodal Chain-of-Thought Reasoning via Large Language Model Signals for Science Question Answering. (arXiv:2305.03453v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03453
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20449;&#21495;&#25945;&#32946;&#31185;&#23398;&#38382;&#39064;&#22238;&#31572;&#30340;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;&#65292;&#36890;&#36807;&#29983;&#25104;&#39640;&#36136;&#37327;COT&#21512;&#29702;&#21270;&#20449;&#21495;&#65292;&#21516;&#26102;&#38477;&#20302;&#20102;&#20154;&#24037;&#27880;&#37322;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36817;&#26399;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#20182;&#20204;&#36824;&#23637;&#31034;&#20102;&#25191;&#34892;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;&#20197;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#22797;&#26434;&#22810;&#27169;&#24577;&#22330;&#26223;&#19979;&#30340;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;&#65292;&#20363;&#22914;&#36890;&#36807;&#29992;&#39640;&#36136;&#37327;&#20154;&#24037;&#27880;&#37322;&#30340;&#38142;&#24335;&#24605;&#36335;&#26469;&#35843;&#25972;&#22810;&#27169;&#22411;&#27169;&#22411;&#36827;&#34892;&#31185;&#23398;&#38382;&#39064;&#22238;&#31572;&#31561;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#25910;&#38598;&#39640;&#36136;&#37327;COT&#21512;&#29702;&#21270;&#36890;&#24120;&#26159;&#32791;&#26102;&#19988;&#26114;&#36149;&#30340;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#28041;&#21450;&#20887;&#20313;&#20449;&#24687;&#25110;&#20002;&#22833;&#37325;&#35201;&#20449;&#24687;&#65292;&#27880;&#37322;&#21512;&#29702;&#21270;&#36890;&#24120;&#19981;&#22826;&#20934;&#30830;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;T-SciQ&#65292;&#26088;&#22312;&#20351;&#29992;LLM&#20449;&#21495;&#25945;&#25480;&#31185;&#23398;&#38382;&#39064;&#22238;&#31572;&#12290;T-SciQ&#26041;&#27861;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;CoT&#21512;&#29702;&#21270;&#20449;&#21495;&#65292;&#24182;&#20808;&#36827;&#22320;&#35757;&#32451;&#36739;&#23567;&#30340;&#27169;&#22411;&#20197;&#22312;&#22797;&#26434;&#27169;&#24577;&#20013;&#25191;&#34892;CoT&#24605;&#32500;&#25512;&#29702;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26174;&#30528;&#25552;&#39640;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#30340;&#25216;&#26415;&#65292;&#20174;&#32780;&#26174;&#30528;&#20943;&#23569;&#20102;&#23545;&#20154;&#24037;&#27880;&#37322;&#21512;&#29702;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have recently demonstrated exceptional performance in various Natural Language Processing (NLP) tasks. They have also shown the ability to perform chain-of-thought (CoT) reasoning to solve complex problems. Recent studies have explored CoT reasoning in complex multimodal scenarios, such as the science question answering task, by fine-tuning multimodal models with high-quality human-annotated CoT rationales. However, collecting high-quality COT rationales is usually time-consuming and costly. Besides, the annotated rationales are hardly accurate due to the redundant information involved or the essential information missed. To address these issues, we propose a novel method termed \emph{T-SciQ} that aims at teaching science question answering with LLM signals. The T-SciQ approach generates high-quality CoT rationales as teaching signals and is advanced to train much smaller models to perform CoT reasoning in complex modalities. Additionally, we introduce a no
&lt;/p&gt;</description></item><item><title>UPRISE&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#26816;&#32034;&#22120;&#65292;&#21487;&#33258;&#21160;&#20026;&#32473;&#23450;&#30340;&#38646;&#26679;&#26412;&#20219;&#21153;&#36755;&#20837;&#26816;&#32034;&#25552;&#31034;&#65292;&#20174;&#32780;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#35780;&#20272;&#12290;&#23427;&#36890;&#36807;&#36328;&#20219;&#21153;&#21644;&#36328;&#27169;&#22411;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;&#20854;&#36890;&#29992;&#24615;&#21644;&#28508;&#21147;&#65292;&#21516;&#26102;&#34920;&#26126;&#20855;&#26377;&#20943;&#36731;&#24187;&#35273;&#38382;&#39064;&#21644;&#25552;&#39640;LLM&#24615;&#33021;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.08518</link><description>&lt;p&gt;
UPRISE: &#36890;&#29992;&#25552;&#31034;&#26816;&#32034;&#20197;&#25552;&#39640;&#38646;&#26679;&#26412;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation. (arXiv:2303.08518v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08518
&lt;/p&gt;
&lt;p&gt;
UPRISE&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#26816;&#32034;&#22120;&#65292;&#21487;&#33258;&#21160;&#20026;&#32473;&#23450;&#30340;&#38646;&#26679;&#26412;&#20219;&#21153;&#36755;&#20837;&#26816;&#32034;&#25552;&#31034;&#65292;&#20174;&#32780;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#35780;&#20272;&#12290;&#23427;&#36890;&#36807;&#36328;&#20219;&#21153;&#21644;&#36328;&#27169;&#22411;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;&#20854;&#36890;&#29992;&#24615;&#21644;&#28508;&#21147;&#65292;&#21516;&#26102;&#34920;&#26126;&#20855;&#26377;&#20943;&#36731;&#24187;&#35273;&#38382;&#39064;&#21644;&#25552;&#39640;LLM&#24615;&#33021;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22240;&#20854;&#20986;&#33394;&#30340;&#33021;&#21147;&#32780;&#21463;&#27426;&#36814;&#65292;&#20294;&#38656;&#35201;&#29305;&#23450;&#27169;&#22411;&#30340;&#24494;&#35843;&#25110;&#20219;&#21153;&#29305;&#23450;&#25552;&#31034;&#24037;&#31243;&#21487;&#33021;&#20250;&#38459;&#30861;&#20854;&#19968;&#33324;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;UPRISE&#65288;&#36890;&#29992;&#25552;&#31034;&#26816;&#32034;&#20197;&#25552;&#39640;&#38646;&#26679;&#26412;&#35780;&#20272;&#65289;&#65292;&#35813;&#26041;&#27861;&#35843;&#25972;&#20102;&#36731;&#37327;&#32423;&#21644;&#22810;&#21151;&#33021;&#30340;&#26816;&#32034;&#22120;&#65292;&#20197;&#33258;&#21160;&#26816;&#32034;&#32473;&#23450;&#38646;&#26679;&#26412;&#20219;&#21153;&#36755;&#20837;&#30340;&#25552;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#36328;&#20219;&#21153;&#21644;&#36328;&#27169;&#22411;&#26041;&#26696;&#20013;&#23637;&#31034;&#20102;&#36890;&#29992;&#24615;&#65306;&#26816;&#32034;&#22120;&#38024;&#23545;&#21508;&#31181;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#65292;&#20294;&#22312;&#30475;&#19981;&#35265;&#30340;&#20219;&#21153;&#31867;&#22411;&#19978;&#36827;&#34892;&#27979;&#35797;&#65307;&#25105;&#20204;&#22312;&#19968;&#20010;&#23567;&#22411;&#20923;&#32467;LLM&#8212;&#8212;GPT-Neo-2.7B&#19978;&#35843;&#25972;&#26816;&#32034;&#22120;&#65292;&#20294;&#22312;&#19981;&#21516;&#35268;&#27169;&#30340;LLM&#19978;&#27979;&#35797;&#26816;&#32034;&#22120;&#65292;&#20363;&#22914;BLOOM-7.1B&#12289;OPT-66B&#21644;GPT3-175B&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;UPRISE&#22312;&#25105;&#20204;&#19982;ChatGPT&#30340;&#23454;&#39564;&#20013;&#20943;&#36731;&#20102;&#24187;&#35273;&#38382;&#39064;&#65292;&#34920;&#26126;&#23427;&#26377;&#28508;&#21147;&#25913;&#36827;&#29978;&#33267;&#26159;&#26368;&#24378;&#30340;LLM&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are popular for their impressive abilities, but the need for model-specific fine-tuning or task-specific prompt engineering can hinder their generalization. We propose UPRISE (Universal Prompt Retrieval for Improving zero-Shot Evaluation), which tunes a lightweight and versatile retriever that automatically retrieves prompts for a given zero-shot task input. Specifically, we demonstrate universality in a cross-task and cross-model scenario: the retriever is tuned on a diverse set of tasks, but tested on unseen task types; we use a small frozen LLM, GPT-Neo-2.7B, for tuning the retriever, but test the retriever on different LLMs of much larger scales, such as BLOOM-7.1B, OPT-66B and GPT3-175B. Additionally, we show that UPRISE mitigates the hallucination problem in our experiments with ChatGPT, suggesting its potential to improve even the strongest LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#8212;&#8212;&#32534;&#36753;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65292;&#26088;&#22312;&#23454;&#29616;&#23545;KG&#23884;&#20837;&#30340;&#25968;&#25454;&#39640;&#25928;&#21644;&#24555;&#36895;&#26356;&#26032;&#12290;&#38024;&#23545;&#36825;&#19968;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#26041;&#26696;&#8212;&#8212;KGEditor&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#26356;&#26032;&#29305;&#23450;&#20107;&#23454;&#32780;&#19981;&#24433;&#21709;&#20854;&#20313;&#37096;&#20998;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.10405</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Editing Language Model-based Knowledge Graph Embeddings. (arXiv:2301.10405v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10405
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#8212;&#8212;&#32534;&#36753;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65292;&#26088;&#22312;&#23454;&#29616;&#23545;KG&#23884;&#20837;&#30340;&#25968;&#25454;&#39640;&#25928;&#21644;&#24555;&#36895;&#26356;&#26032;&#12290;&#38024;&#23545;&#36825;&#19968;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#26041;&#26696;&#8212;&#8212;KGEditor&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#26356;&#26032;&#29305;&#23450;&#20107;&#23454;&#32780;&#19981;&#24433;&#21709;&#20854;&#20313;&#37096;&#20998;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#20960;&#21313;&#24180;&#26469;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#23884;&#20837;&#24050;&#32463;&#21462;&#24471;&#20102;&#23454;&#35777;&#25104;&#21151;&#12290;&#20294;&#26159;&#65292;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;KG&#23884;&#20837;&#36890;&#24120;&#20316;&#20026;&#38745;&#24577;&#24037;&#20214;&#37096;&#32626;&#65292;&#20462;&#25913;&#36215;&#26469;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#32534;&#36753;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;KG&#23884;&#20837;&#12290;&#35813;&#20219;&#21153;&#26088;&#22312;&#23454;&#29616;&#23545;KG&#23884;&#20837;&#30340;&#25968;&#25454;&#39640;&#25928;&#21644;&#24555;&#36895;&#26356;&#26032;&#65292;&#32780;&#19981;&#24433;&#21709;&#20854;&#20313;&#37096;&#20998;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#22235;&#20010;&#26032;&#25968;&#25454;&#38598;&#65306;E-FB15k237&#12289;A-FB15k237&#12289;E-WN18RR &#21644; A-WN18RR&#65292;&#24182;&#35780;&#20272;&#20102;&#20960;&#31181;&#30693;&#35782;&#32534;&#36753;&#22522;&#32447;&#65292;&#35777;&#26126;&#20102;&#20043;&#21069;&#30340;&#27169;&#22411;&#22788;&#29702;&#35813;&#20219;&#21153;&#30340;&#33021;&#21147;&#26377;&#38480;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#24378;&#22823;&#30340;&#22522;&#32447;&#8212;&#8212;KGEditor&#65292;&#23427;&#21033;&#29992;&#36229;&#32593;&#32476;&#30340;&#38468;&#21152;&#21442;&#25968;&#23618;&#26469;&#32534;&#36753;/&#28155;&#21152;&#20107;&#23454;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#26356;&#26032;&#29305;&#23450;&#20107;&#23454;&#32780;&#19981;&#24433;&#21709;&#20854;&#20313;&#37096;&#20998;&#30340;&#24615;&#33021;&#26102;&#65292;KGEditor &#30340;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently decades have witnessed the empirical success of framing Knowledge Graph (KG) embeddings via language models. However, language model-based KG embeddings are usually deployed as static artifacts, which are challenging to modify without re-training after deployment. To address this issue, we propose a new task of editing language model-based KG embeddings in this paper. The proposed task aims to enable data-efficient and fast updates to KG embeddings without damaging the performance of the rest. We build four new datasets: E-FB15k237, A-FB15k237, E-WN18RR, and A-WN18RR, and evaluate several knowledge editing baselines demonstrating the limited ability of previous models to handle the proposed challenging task. We further propose a simple yet strong baseline dubbed KGEditor, which utilizes additional parametric layers of the hyper network to edit/add facts. Comprehensive experimental results demonstrate that KGEditor can perform better when updating specific facts while not affec
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#20803;&#21453;&#28216;&#25103;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#32452;&#21512;&#23398;&#20064;&#34892;&#20026;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#35299;&#20915;&#32465;&#23450;&#38382;&#39064;&#26469;&#25903;&#25345;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#23637;&#31034;&#32452;&#21512;&#23398;&#20064;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2207.08012</link><description>&lt;p&gt;
&#20803;&#20803;&#21453;&#28216;&#25103;&#23398;&#20064;&#32452;&#21512;&#23398;&#20064;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Meta-Referential Games to Learn Compositional Learning Behaviours. (arXiv:2207.08012v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.08012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#20803;&#21453;&#28216;&#25103;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#32452;&#21512;&#23398;&#20064;&#34892;&#20026;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#35299;&#20915;&#32465;&#23450;&#38382;&#39064;&#26469;&#25903;&#25345;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#23637;&#31034;&#32452;&#21512;&#23398;&#20064;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21033;&#29992;&#32452;&#21512;&#24615;&#20174;&#36807;&#21435;&#30340;&#32463;&#39564;&#20013;&#25512;&#24191;&#21040;&#26032;&#39062;&#30340;&#32463;&#39564;&#12290;&#25105;&#20204;&#20551;&#35774;&#25105;&#20204;&#30340;&#32463;&#39564;&#21487;&#20197;&#20998;&#35299;&#20026;&#22522;&#26412;&#30340;&#21407;&#23376;&#32452;&#20214;&#65292;&#36825;&#20123;&#32452;&#20214;&#21487;&#20197;&#20197;&#26032;&#39062;&#30340;&#26041;&#24335;&#37325;&#26032;&#32452;&#21512;&#65292;&#20197;&#25903;&#25345;&#25105;&#20204;&#21442;&#19982;&#26032;&#39062;&#32463;&#39564;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;&#36825;&#35270;&#20026;&#23398;&#20064;&#20197;&#32452;&#21512;&#26041;&#24335;&#27867;&#21270;&#30340;&#33021;&#21147;&#65292;&#24182;&#23558;&#21033;&#29992;&#36825;&#31181;&#33021;&#21147;&#30340;&#34892;&#20026;&#31216;&#20026;&#32452;&#21512;&#23398;&#20064;&#34892;&#20026;&#65288;CLBs&#65289;&#12290;&#23398;&#20064;CLBs&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#26159;&#35299;&#20915;&#32465;&#23450;&#38382;&#39064;&#65288;BP&#65289;&#12290;&#23613;&#31649;&#36825;&#26159;&#20154;&#31867;&#36731;&#26494;&#23436;&#25104;&#30340;&#26234;&#33021;&#22766;&#20030;&#65292;&#20294;&#23545;&#20110;&#29616;&#26377;&#25216;&#26415;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#26469;&#35828;&#24182;&#38750;&#22914;&#27492;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#26500;&#24314;&#33021;&#22815;&#19982;&#20154;&#31867;&#21512;&#20316;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#65292;&#25105;&#20204;&#24314;&#35758;&#24320;&#21457;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#26469;&#30740;&#31350;&#20195;&#29702;&#21830;&#36890;&#36807;&#35299;&#20915;BP&#30340;&#39046;&#22495;&#26080;&#20851;&#29256;&#26412;&#26469;&#23637;&#31034;CLBs&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#21463;&#21040;&#25351;&#20195;&#28216;&#25103;&#30340;&#35821;&#35328;&#28044;&#29616;&#21644;&#22522;&#30784;&#26550;&#26500;&#26694;&#26550;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20803;&#23398;&#20064;&#25193;&#23637;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Human beings use compositionality to generalise from past experiences to novel experiences. We assume a separation of our experiences into fundamental atomic components that can be recombined in novel ways to support our ability to engage with novel experiences. We frame this as the ability to learn to generalise compositionally, and we will refer to behaviours making use of this ability as compositional learning behaviours (CLBs). A central problem to learning CLBs is the resolution of a binding problem (BP). While it is another feat of intelligence that human beings perform with ease, it is not the case for state-of-the-art artificial agents. Thus, in order to build artificial agents able to collaborate with human beings, we propose to develop a novel benchmark to investigate agents' abilities to exhibit CLBs by solving a domain-agnostic version of the BP. We take inspiration from the language emergence and grounding framework of referential games and propose a meta-learning extensio
&lt;/p&gt;</description></item></channel></rss>