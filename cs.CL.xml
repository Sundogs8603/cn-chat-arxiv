<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#35745;&#22270;&#28789;&#27979;&#35797;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#37327;&#21270;&#20154;&#31867;&#21644;&#26426;&#22120;&#22312;&#32473;&#23450;&#35780;&#20272;&#29615;&#22659;&#19979;&#29983;&#25104;&#20869;&#23481;&#20998;&#24067;&#30340;&#24046;&#24322;&#65292;&#24182;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#35813;&#26694;&#26550;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#22312;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#33021;&#21147;&#26041;&#38754;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2309.08913</link><description>&lt;p&gt;
&#19968;&#20010;&#32479;&#35745;&#22270;&#28789;&#27979;&#35797;&#29992;&#20110;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Statistical Turing Test for Generative Models. (arXiv:2309.08913v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#35745;&#22270;&#28789;&#27979;&#35797;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#37327;&#21270;&#20154;&#31867;&#21644;&#26426;&#22120;&#22312;&#32473;&#23450;&#35780;&#20272;&#29615;&#22659;&#19979;&#29983;&#25104;&#20869;&#23481;&#20998;&#24067;&#30340;&#24046;&#24322;&#65292;&#24182;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#35813;&#26694;&#26550;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#22312;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#33021;&#21147;&#26041;&#38754;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#25991;&#26412;&#12289;&#38899;&#39057;&#21644;&#35270;&#35273;&#31561;&#39046;&#22495;&#30340;&#20869;&#23481;&#29983;&#25104;&#33021;&#21147;&#30340;&#20986;&#29616;&#20652;&#29983;&#20102;&#29992;&#20110;&#21306;&#20998;&#20869;&#23481;&#26469;&#28304;&#20110;&#20154;&#36824;&#26159;&#26426;&#22120;&#30340;&#20998;&#31867;&#22120;&#30340;&#21457;&#23637;&#12290;&#36825;&#20123;&#24037;&#20316;&#30340;&#38544;&#21547;&#20551;&#35774;&#26159;&#20154;&#31867;&#30340;&#29983;&#25104;&#33021;&#21147;&#19982;&#26426;&#22120;&#30340;&#29983;&#25104;&#33021;&#21147;&#23384;&#22312;&#24046;&#24322;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#22312;&#32479;&#35745;&#27169;&#24335;&#35782;&#21035;&#35821;&#35328;&#20013;&#37327;&#21270;&#20154;&#31867;&#21644;&#26426;&#22120;&#29983;&#25104;&#20869;&#23481;&#20998;&#24067;&#24046;&#24322;&#30340;&#26694;&#26550;&#65292;&#24182;&#25551;&#36848;&#20102;&#22914;&#20309;&#22312;&#26694;&#26550;&#20013;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#22312;&#21521;&#20154;&#31867;&#27700;&#24179;&#33021;&#21147;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#28085;&#30422;&#20102;&#22810;&#20010;&#20998;&#26512;&#32500;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of human-like abilities of AI systems for content generation in domains such as text, audio, and vision has prompted the development of classifiers to determine whether content originated from a human or a machine. Implicit in these efforts is an assumption that the generation properties of a human are different from that of the machine. In this work, we provide a framework in the language of statistical pattern recognition that quantifies the difference between the distributions of human and machine-generated content conditioned on an evaluation context. We describe current methods in the context of the framework and demonstrate how to use the framework to evaluate the progression of generative models towards human-like capabilities, among many axes of analysis.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;LLMs&#22312;&#24180;&#40836;&#12289;&#32654;&#20029;&#12289;&#26426;&#26500;&#21644;&#22269;&#31821;&#31561;&#23569;&#30740;&#31350;&#20294;&#20173;&#28982;&#37325;&#35201;&#30340;&#32500;&#24230;&#19978;&#30340;&#20559;&#35265;&#65292;&#36890;&#36807;&#34913;&#37327;&#22312;&#31038;&#20250;&#32676;&#20307;&#21644;&#19981;&#30456;&#20851;&#30340;&#27491;&#36127;&#23646;&#24615;&#20043;&#38388;&#20570;&#20986;&#30340;&#24494;&#22937;&#30456;&#20851;&#20915;&#31574;&#12290;&#30740;&#31350;&#21457;&#29616;LLMs&#22312;&#29305;&#23450;&#31038;&#20250;&#32676;&#20307;&#19978;&#23384;&#22312;&#31867;&#20284;&#20110;&#8220;&#32654;&#20029;&#21363;&#21892;&#8221;&#30340;&#24191;&#27867;&#27491;&#38754;&#25110;&#36127;&#38754;&#24577;&#24230;&#30340;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2309.08902</link><description>&lt;p&gt;
&#35843;&#26597;LLMs&#20013;&#26356;&#24494;&#22937;&#30340;&#20559;&#35265;&#65306;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#24180;&#40836;&#20027;&#20041;&#12289;&#32654;&#20029;&#12289;&#26426;&#26500;&#21644;&#22269;&#31821;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Investigating Subtler Biases in LLMs: Ageism, Beauty, Institutional, and Nationality Bias in Generative Models. (arXiv:2309.08902v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08902
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;LLMs&#22312;&#24180;&#40836;&#12289;&#32654;&#20029;&#12289;&#26426;&#26500;&#21644;&#22269;&#31821;&#31561;&#23569;&#30740;&#31350;&#20294;&#20173;&#28982;&#37325;&#35201;&#30340;&#32500;&#24230;&#19978;&#30340;&#20559;&#35265;&#65292;&#36890;&#36807;&#34913;&#37327;&#22312;&#31038;&#20250;&#32676;&#20307;&#21644;&#19981;&#30456;&#20851;&#30340;&#27491;&#36127;&#23646;&#24615;&#20043;&#38388;&#20570;&#20986;&#30340;&#24494;&#22937;&#30456;&#20851;&#20915;&#31574;&#12290;&#30740;&#31350;&#21457;&#29616;LLMs&#22312;&#29305;&#23450;&#31038;&#20250;&#32676;&#20307;&#19978;&#23384;&#22312;&#31867;&#20284;&#20110;&#8220;&#32654;&#20029;&#21363;&#21892;&#8221;&#30340;&#24191;&#27867;&#27491;&#38754;&#25110;&#36127;&#38754;&#24577;&#24230;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLMs&#36234;&#26469;&#36234;&#24378;&#22823;&#24182;&#24191;&#27867;&#29992;&#20110;&#36741;&#21161;&#29992;&#25143;&#23436;&#25104;&#21508;&#31181;&#20219;&#21153;&#12290;&#36825;&#31181;&#20351;&#29992;&#21487;&#33021;&#20250;&#23558;LLM&#20559;&#35265;&#24341;&#20837;&#21040;&#37325;&#35201;&#20915;&#31574;&#20013;&#65292;&#22914;&#25307;&#32856;&#12289;&#20154;&#21592;&#32489;&#25928;&#35780;&#20272;&#21644;&#21009;&#20107;&#21028;&#20915;&#12290;&#22312;NLP&#31995;&#32479;&#20013;&#30340;&#24615;&#21035;&#21644;&#31181;&#26063;&#31561;&#26041;&#38754;&#30340;&#20559;&#35265;&#24050;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#65292;&#23588;&#20854;&#26159;&#38024;&#23545;&#29305;&#23450;&#21051;&#26495;&#21360;&#35937;&#30340;&#20559;&#35265;&#65288;&#20363;&#22914;&#65292;&#20122;&#27954;&#20154;&#25797;&#38271;&#25968;&#23398;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20123;&#36739;&#23569;&#30740;&#31350;&#20294;&#20173;&#28982;&#37325;&#35201;&#30340;&#32500;&#24230;&#19978;&#30340;&#20559;&#35265;&#65292;&#22914;&#24180;&#40836;&#21644;&#32654;&#20029;&#65292;&#22312;LLMs&#65288;&#29305;&#21035;&#26159;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#65289;&#22312;&#31038;&#20250;&#32676;&#20307;&#21644;&#19981;&#30456;&#20851;&#30340;&#27491;&#36127;&#23646;&#24615;&#20043;&#38388;&#20570;&#20986;&#26356;&#24494;&#22937;&#30340;&#30456;&#20851;&#20915;&#31574;&#12290;&#25105;&#20204;&#38382;LLMs&#26159;&#21542;&#23545;&#29305;&#23450;&#31038;&#20250;&#32676;&#20307;&#25345;&#26377;&#24191;&#27867;&#30340;&#27491;&#38754;&#25110;&#36127;&#38754;&#24577;&#24230;&#30340;&#20559;&#35265;&#65292;&#31867;&#20284;&#20110;&#23454;&#39564;&#24515;&#29702;&#23398;&#20013;&#20154;&#20204;&#21457;&#29616;&#30340;&#8220;&#32654;&#20029;&#21363;&#21892;&#8221;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27169;&#26495;&#29983;&#25104;&#30340;&#21477;&#23376;&#23436;&#25104;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#65292;&#35201;&#27714;&#27169;&#22411;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLMs are increasingly powerful and widely used to assist users in a variety of tasks. This use risks the introduction of LLM biases to consequential decisions such as job hiring, human performance evaluation, and criminal sentencing. Bias in NLP systems along the lines of gender and ethnicity has been widely studied, especially for specific stereotypes (e.g., Asians are good at math). In this paper, we investigate bias along less studied, but still consequential, dimensions, such as age and beauty, measuring subtler correlated decisions that LLMs (specially autoregressive language models) make between social groups and unrelated positive and negative attributes. We ask whether LLMs hold wide-reaching biases of positive or negative sentiment for specific social groups similar to the ``what is beautiful is good'' bias found in people in experimental psychology. We introduce a template-generated dataset of sentence completion tasks that asks the model to select the most appropriate attrib
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36164;&#28304;&#21463;&#38480;&#25991;&#26412;&#25968;&#25454;&#20256;&#36755;&#30340;&#35821;&#20041;&#20449;&#24687;&#25552;&#21462;&#38382;&#39064;&#65292;&#22312;&#19968;&#20010;&#36890;&#20449;&#36164;&#28304;&#21463;&#38480;&#30340;&#32593;&#32476;&#20013;&#65292;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#25552;&#21462;&#21407;&#22987;&#25991;&#26412;&#25968;&#25454;&#65292;&#23558;&#25552;&#21462;&#30340;&#35821;&#20041;&#20449;&#24687;&#25429;&#25417;&#22312;&#19968;&#20010;&#24102;&#26377;&#27010;&#29575;&#32500;&#24230;&#30340;&#30693;&#35782;&#22270;&#20013;&#65292;&#24182;&#36890;&#36807;&#19968;&#20010;&#20248;&#21270;&#26694;&#26550;&#25552;&#21462;&#26368;&#37325;&#35201;&#30340;&#35821;&#20041;&#20449;&#24687;&#36827;&#34892;&#20256;&#36755;&#12290;</title><link>http://arxiv.org/abs/2309.08879</link><description>&lt;p&gt;
&#20351;&#29992;&#27010;&#29575;&#22270;&#36827;&#34892;&#25991;&#26412;&#25968;&#25454;&#30340;&#35821;&#20041;&#20449;&#24687;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Semantic Information Extraction for Text Data with Probability Graph. (arXiv:2309.08879v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08879
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36164;&#28304;&#21463;&#38480;&#25991;&#26412;&#25968;&#25454;&#20256;&#36755;&#30340;&#35821;&#20041;&#20449;&#24687;&#25552;&#21462;&#38382;&#39064;&#65292;&#22312;&#19968;&#20010;&#36890;&#20449;&#36164;&#28304;&#21463;&#38480;&#30340;&#32593;&#32476;&#20013;&#65292;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#25552;&#21462;&#21407;&#22987;&#25991;&#26412;&#25968;&#25454;&#65292;&#23558;&#25552;&#21462;&#30340;&#35821;&#20041;&#20449;&#24687;&#25429;&#25417;&#22312;&#19968;&#20010;&#24102;&#26377;&#27010;&#29575;&#32500;&#24230;&#30340;&#30693;&#35782;&#22270;&#20013;&#65292;&#24182;&#36890;&#36807;&#19968;&#20010;&#20248;&#21270;&#26694;&#26550;&#25552;&#21462;&#26368;&#37325;&#35201;&#30340;&#35821;&#20041;&#20449;&#24687;&#36827;&#34892;&#20256;&#36755;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36164;&#28304;&#21463;&#38480;&#25991;&#26412;&#25968;&#25454;&#20256;&#36755;&#30340;&#35821;&#20041;&#20449;&#24687;&#25552;&#21462;&#38382;&#39064;&#12290;&#22312;&#32771;&#34385;&#30340;&#27169;&#22411;&#20013;&#65292;&#38656;&#35201;&#22312;&#36890;&#20449;&#36164;&#28304;&#21463;&#38480;&#30340;&#32593;&#32476;&#20013;&#20256;&#36755;&#19968;&#31995;&#21015;&#25991;&#26412;&#25968;&#25454;&#65292;&#35813;&#32593;&#32476;&#20165;&#20801;&#35768;&#26377;&#38480;&#30340;&#25968;&#25454;&#20256;&#36755;&#12290;&#22240;&#27492;&#65292;&#22312;&#21457;&#36865;&#31471;&#65292;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#25552;&#21462;&#21407;&#22987;&#25991;&#26412;&#25968;&#25454;&#12290;&#28982;&#21518;&#65292;&#25552;&#21462;&#30340;&#35821;&#20041;&#20449;&#24687;&#34987;&#25429;&#25417;&#22312;&#19968;&#20010;&#30693;&#35782;&#22270;&#20013;&#12290;&#22312;&#36825;&#20010;&#22270;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#27010;&#29575;&#32500;&#24230;&#26469;&#25429;&#25417;&#27599;&#20010;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#12290;&#36825;&#20010;&#35821;&#20041;&#20449;&#24687;&#25552;&#21462;&#38382;&#39064;&#34987;&#25552;&#20986;&#20026;&#19968;&#20010;&#20248;&#21270;&#26694;&#26550;&#65292;&#20854;&#30446;&#26631;&#26159;&#25552;&#21462;&#26368;&#37325;&#35201;&#30340;&#35821;&#20041;&#20449;&#24687;&#36827;&#34892;&#20256;&#36755;&#12290;&#20026;&#20102;&#25214;&#21040;&#36825;&#20010;&#38382;&#39064;&#30340;&#26368;&#20248;&#35299;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Floyd&#31639;&#27861;&#21644;&#39640;&#25928;&#25490;&#24207;&#26426;&#21046;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25968;&#20540;&#32467;&#26524;&#35777;&#26126;&#20102;&#25152;&#25552;&#31639;&#27861;&#22312;&#20004;&#20010;&#26032;&#39062;&#30340;&#24615;&#33021;&#25351;&#26631;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, the problem of semantic information extraction for resource constrained text data transmission is studied. In the considered model, a sequence of text data need to be transmitted within a communication resource-constrained network, which only allows limited data transmission. Thus, at the transmitter, the original text data is extracted with natural language processing techniques. Then, the extracted semantic information is captured in a knowledge graph. An additional probability dimension is introduced in this graph to capture the importance of each information. This semantic information extraction problem is posed as an optimization framework whose goal is to extract most important semantic information for transmission. To find an optimal solution for this problem, a Floyd's algorithm based solution coupled with an efficient sorting mechanism is proposed. Numerical results testify the effectiveness of the proposed algorithm with regards to two novel performance metrics
&lt;/p&gt;</description></item><item><title>X-PARADE&#26159;&#31532;&#19968;&#20010;&#36328;&#35821;&#35328;&#27573;&#33853;&#32423;&#21035;&#20449;&#24687;&#20998;&#27495;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#23558;&#26469;&#28304;&#20110;&#19981;&#21516;&#35821;&#35328;&#30340;&#32500;&#22522;&#30334;&#31185;&#39029;&#38754;&#19978;&#30340;&#27573;&#33853;&#23545;&#40784;&#65292;&#26631;&#27880;&#32773;&#35780;&#20272;&#20102;&#30446;&#26631;&#35821;&#35328;&#27573;&#33853;&#19982;&#28304;&#35821;&#35328;&#27573;&#33853;&#20043;&#38388;&#30340;&#20449;&#24687;&#26159;&#21542;&#30456;&#21516;&#12289;&#26032;&#30340;&#25110;&#32773;&#21487;&#20197;&#25512;&#26029;&#65292;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2309.08873</link><description>&lt;p&gt;
X-PARADE: &#36328;&#35821;&#35328;&#25991;&#26412;&#34164;&#21547;&#21644;&#27573;&#33853;&#20043;&#38388;&#30340;&#20449;&#24687;&#20998;&#27495;
&lt;/p&gt;
&lt;p&gt;
X-PARADE: Cross-Lingual Textual Entailment and Information Divergence across Paragraphs. (arXiv:2309.08873v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08873
&lt;/p&gt;
&lt;p&gt;
X-PARADE&#26159;&#31532;&#19968;&#20010;&#36328;&#35821;&#35328;&#27573;&#33853;&#32423;&#21035;&#20449;&#24687;&#20998;&#27495;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#23558;&#26469;&#28304;&#20110;&#19981;&#21516;&#35821;&#35328;&#30340;&#32500;&#22522;&#30334;&#31185;&#39029;&#38754;&#19978;&#30340;&#27573;&#33853;&#23545;&#40784;&#65292;&#26631;&#27880;&#32773;&#35780;&#20272;&#20102;&#30446;&#26631;&#35821;&#35328;&#27573;&#33853;&#19982;&#28304;&#35821;&#35328;&#27573;&#33853;&#20043;&#38388;&#30340;&#20449;&#24687;&#26159;&#21542;&#30456;&#21516;&#12289;&#26032;&#30340;&#25110;&#32773;&#21487;&#20197;&#25512;&#26029;&#65292;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#20004;&#27573;&#25991;&#26412;&#26159;&#21542;&#20256;&#36798;&#30456;&#21516;&#30340;&#20449;&#24687;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#35768;&#22810;&#23376;&#38382;&#39064;&#30340;&#30446;&#26631;&#65292;&#21253;&#25324;&#25991;&#26412;&#34164;&#21547;&#21644;&#20107;&#23454;&#26680;&#26597;&#12290;&#24403;&#36825;&#20004;&#27573;&#25991;&#26412;&#22788;&#20110;&#19981;&#21516;&#30340;&#35821;&#35328;&#26102;&#65292;&#36825;&#20010;&#38382;&#39064;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;X-PARADE&#65288;&#36328;&#35821;&#35328;&#27573;&#33853;&#32423;&#21035;&#30340;&#20998;&#27495;&#21644;&#34164;&#21547;&#20998;&#26512;&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#36328;&#35821;&#35328;&#27573;&#33853;&#32423;&#21035;&#20449;&#24687;&#20998;&#27495;&#30340;&#25968;&#25454;&#38598;&#12290;&#26631;&#27880;&#32773;&#22312;&#30446;&#26631;&#35821;&#35328;&#19978;&#20197;&#36328;&#24230;&#32423;&#21035;&#26631;&#27880;&#27573;&#33853;&#65292;&#24182;&#19982;&#28304;&#35821;&#35328;&#20013;&#30340;&#30456;&#24212;&#27573;&#33853;&#36827;&#34892;&#35780;&#20272;&#65292;&#34920;&#31034;&#32473;&#23450;&#30340;&#20449;&#24687;&#26159;&#21542;&#30456;&#21516;&#12289;&#26032;&#30340;&#65292;&#25110;&#32773;&#26032;&#30340;&#20294;&#21487;&#20197;&#25512;&#26029;&#12290;&#36825;&#20010;&#27010;&#24565;&#19982;&#36328;&#35821;&#35328;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#24314;&#31435;&#20102;&#32852;&#31995;&#12290;&#23545;&#40784;&#30340;&#27573;&#33853;&#26469;&#33258;&#19981;&#21516;&#35821;&#35328;&#30340;&#32500;&#22522;&#30334;&#31185;&#39029;&#38754;&#65292;&#21453;&#26144;&#20102;&#23454;&#38469;&#35266;&#23519;&#21040;&#30340;&#20449;&#24687;&#20998;&#27495;&#12290;&#20973;&#20511;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31995;&#21015;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#32463;&#20856;&#30340;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#20196;&#29260;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding when two pieces of text convey the same information is a goal touching many subproblems in NLP, including textual entailment and fact-checking. This problem becomes more complex when those two pieces of text are in different languages. Here, we introduce X-PARADE (Cross-lingual Paragraph-level Analysis of Divergences and Entailments), the first cross-lingual dataset of paragraph-level information divergences. Annotators label a paragraph in a target language at the span level and evaluate it with respect to a corresponding paragraph in a source language, indicating whether a given piece of information is the same, new, or new but can be inferred. This last notion establishes a link with cross-language NLI. Aligned paragraphs are sourced from Wikipedia pages in different languages, reflecting real information divergences observed in the wild. Armed with our dataset, we investigate a diverse set of approaches for this problem, including classic token alignment from machine 
&lt;/p&gt;</description></item><item><title>PDFTriage&#26159;&#19968;&#31181;&#22788;&#29702;&#38271;&#31687;&#32467;&#26500;&#21270;&#25991;&#26723;&#38382;&#31572;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32467;&#26500;&#25110;&#20869;&#23481;&#26469;&#26816;&#32034;&#19978;&#19979;&#25991;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38382;&#31572;&#20013;&#36935;&#21040;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.08872</link><description>&lt;p&gt;
PDFTriage: &#23545;&#38271;&#31687;&#32467;&#26500;&#21270;&#25991;&#26723;&#36827;&#34892;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
PDFTriage: Question Answering over Long, Structured Documents. (arXiv:2309.08872v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08872
&lt;/p&gt;
&lt;p&gt;
PDFTriage&#26159;&#19968;&#31181;&#22788;&#29702;&#38271;&#31687;&#32467;&#26500;&#21270;&#25991;&#26723;&#38382;&#31572;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32467;&#26500;&#25110;&#20869;&#23481;&#26469;&#26816;&#32034;&#19978;&#19979;&#25991;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38382;&#31572;&#20013;&#36935;&#21040;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#38271;&#31687;&#25991;&#26723;&#30340;&#38382;&#31572;&#26102;&#23384;&#22312;&#38382;&#39064;&#65292;&#22240;&#20026;&#25991;&#26723;&#26080;&#27861;&#36866;&#24212;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#26041;&#27861;&#38598;&#20013;&#20110;&#20174;&#25991;&#26723;&#20013;&#26816;&#32034;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#65292;&#24182;&#23558;&#20854;&#34920;&#31034;&#20026;&#32431;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#20687;PDF&#12289;&#32593;&#39029;&#21644;&#28436;&#31034;&#25991;&#31295;&#36825;&#26679;&#30340;&#25991;&#26723;&#26159;&#26377;&#32467;&#26500;&#30340;&#65292;&#21253;&#25324;&#19981;&#21516;&#30340;&#39029;&#30721;&#12289;&#34920;&#26684;&#12289;&#31456;&#33410;&#31561;&#12290;&#23558;&#36825;&#26679;&#30340;&#32467;&#26500;&#21270;&#25991;&#26723;&#34920;&#31034;&#20026;&#32431;&#25991;&#26412;&#19982;&#29992;&#25143;&#23545;&#36825;&#20123;&#20855;&#26377;&#20016;&#23500;&#32467;&#26500;&#30340;&#25991;&#26723;&#30340;&#35748;&#30693;&#27169;&#22411;&#19981;&#31526;&#12290;&#24403;&#31995;&#32479;&#38656;&#35201;&#20174;&#25991;&#26723;&#20013;&#26597;&#35810;&#19978;&#19979;&#25991;&#26102;&#65292;&#36825;&#31181;&#19981;&#31526;&#20250;&#26174;&#29616;&#20986;&#26469;&#65292;&#29978;&#33267;&#31616;&#21333;&#30340;&#38382;&#39064;&#20063;&#21487;&#33021;&#20351;&#38382;&#31572;&#31995;&#32479;&#20986;&#38169;&#12290;&#20026;&#20102;&#24357;&#21512;&#22788;&#29702;&#32467;&#26500;&#21270;&#25991;&#26723;&#20013;&#30340;&#22522;&#26412;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PDFTriage&#30340;&#26041;&#27861;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#32467;&#26500;&#25110;&#20869;&#23481;&#26816;&#32034;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;PDFTriage&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have issues with document question answering (QA) in situations where the document is unable to fit in the small context length of an LLM. To overcome this issue, most existing works focus on retrieving the relevant context from the document, representing them as plain text. However, documents such as PDFs, web pages, and presentations are naturally structured with different pages, tables, sections, and so on. Representing such structured documents as plain text is incongruous with the user's mental model of these documents with rich structure. When a system has to query the document for context, this incongruity is brought to the fore, and seemingly trivial questions can trip up the QA system. To bridge this fundamental gap in handling structured documents, we propose an approach called PDFTriage that enables models to retrieve the context based on either structure or content. Our experiments demonstrate the effectiveness of the proposed PDFTriage-augmente
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MHLAT&#30340;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#22810;&#36339;&#26631;&#31614;&#20851;&#27880;&#26469;&#25552;&#20379;&#26356;&#20934;&#30830;&#21644;&#20016;&#23500;&#30340;&#34920;&#31034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25152;&#26377;&#19971;&#20010;&#24230;&#37327;&#26631;&#20934;&#19978;&#37117;&#23454;&#29616;&#20102;&#26126;&#26174;&#26356;&#22909;&#25110;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#38656;&#35201;&#20248;&#21270;&#30340;&#21442;&#25968;&#26356;&#23569;&#12290;</title><link>http://arxiv.org/abs/2309.08868</link><description>&lt;p&gt;
MHLAT: &#33258;&#21160;ICD&#32534;&#30721;&#30340;&#22810;&#36339;&#26631;&#31614;&#20851;&#27880;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MHLAT: Multi-hop Label-wise Attention Model for Automatic ICD Coding. (arXiv:2309.08868v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08868
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MHLAT&#30340;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#22810;&#36339;&#26631;&#31614;&#20851;&#27880;&#26469;&#25552;&#20379;&#26356;&#20934;&#30830;&#21644;&#20016;&#23500;&#30340;&#34920;&#31034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25152;&#26377;&#19971;&#20010;&#24230;&#37327;&#26631;&#20934;&#19978;&#37117;&#23454;&#29616;&#20102;&#26126;&#26174;&#26356;&#22909;&#25110;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#38656;&#35201;&#20248;&#21270;&#30340;&#21442;&#25968;&#26356;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22269;&#38469;&#30142;&#30149;&#20998;&#31867;&#65288;ICD&#65289;&#32534;&#30721;&#26159;&#23558;ICD&#35786;&#26029;&#20195;&#30721;&#20998;&#37197;&#32473;&#20020;&#24202;&#31508;&#35760;&#30340;&#20219;&#21153;&#12290;&#37492;&#20110;&#22823;&#37327;&#30340;&#26631;&#31614;&#65288;&#36817;9000&#20010;&#65289;&#21644;&#24222;&#22823;&#30340;&#25991;&#26412;&#65288;&#22810;&#36798;8000&#20010;&#26631;&#35760;&#65289;&#65292;&#36825;&#21487;&#33021;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#28982;&#32780;&#65292;&#19982;&#20197;&#21069;&#30340;&#30740;&#31350;&#20013;&#30340;&#21333;&#36890;&#35835;&#36807;&#31243;&#19981;&#21516;&#65292;&#20154;&#20204;&#20542;&#21521;&#20110;&#20877;&#27425;&#38405;&#35835;&#25991;&#26412;&#21644;&#26631;&#31614;&#23450;&#20041;&#20197;&#33719;&#24471;&#26356;&#33258;&#20449;&#30340;&#31572;&#26696;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24050;&#34987;&#29992;&#20110;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#30340;&#20869;&#23384;&#20351;&#29992;&#37327;&#24456;&#22823;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#27169;&#22411;&#65292;&#31216;&#20026;&#22810;&#36339;&#26631;&#31614;&#20851;&#27880;&#65288;MHLAT&#65289;&#65292;&#21033;&#29992;&#22810;&#36339;&#26631;&#31614;&#20851;&#27880;&#26469;&#33719;&#21462;&#26356;&#20934;&#30830;&#21644;&#20016;&#23500;&#30340;&#34920;&#31034;&#12290;&#23545;&#19977;&#20010;&#22522;&#20934;MIMIC&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25152;&#26377;&#19971;&#20010;&#24230;&#37327;&#26631;&#20934;&#19978;&#23454;&#29616;&#20102;&#26126;&#26174;&#26356;&#22909;&#25110;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#38656;&#35201;&#20248;&#21270;&#30340;&#21442;&#25968;&#26356;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
International Classification of Diseases (ICD) coding is the task of assigning ICD diagnosis codes to clinical notes. This can be challenging given the large quantity of labels (nearly 9,000) and lengthy texts (up to 8,000 tokens). However, unlike the single-pass reading process in previous works, humans tend to read the text and label definitions again to get more confident answers. Moreover, although pretrained language models have been used to address these problems, they suffer from huge memory usage. To address the above problems, we propose a simple but effective model called the Multi-Hop Label-wise ATtention (MHLAT), in which multi-hop label-wise attention is deployed to get more precise and informative representations. Extensive experiments on three benchmark MIMIC datasets indicate that our method achieves significantly better or competitive performance on all seven metrics, with much fewer parameters to optimize.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#20351;&#29992;&#32654;&#22269;&#22823;&#23398;Subreddit&#25968;&#25454;&#20174;&#30123;&#24773;&#21069;&#21040;&#30123;&#24773;&#26399;&#38388;&#20877;&#21040;&#21518;&#32039;&#24613;&#26399;&#30340;&#20154;&#20204;&#30340;&#24773;&#32490;&#21464;&#21270;&#65292;&#25506;&#35752;&#20102;&#24773;&#32490;&#26159;&#21542;&#24050;&#24674;&#22797;&#21040;&#30123;&#24773;&#21069;&#30340;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2309.08845</link><description>&lt;p&gt;
&#30123;&#24773;&#21069;&#21518;&#24773;&#32490;&#27700;&#24179;&#26159;&#21542;&#24674;&#22797;&#21040;&#30123;&#24773;&#21069;&#30340;&#27700;&#24179;&#65311;&#20351;&#29992;&#32654;&#22269;&#22823;&#23398;Subreddit&#25968;&#25454;&#36827;&#34892;&#24773;&#32490;&#20998;&#26512;&#65288;arXiv:2309.08845v1 [cs.CL]&#65289;
&lt;/p&gt;
&lt;p&gt;
Has Sentiment Returned to the Pre-pandemic Level? A Sentiment Analysis Using U.S. College Subreddit Data from 2019 to 2022. (arXiv:2309.08845v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08845
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#20351;&#29992;&#32654;&#22269;&#22823;&#23398;Subreddit&#25968;&#25454;&#20174;&#30123;&#24773;&#21069;&#21040;&#30123;&#24773;&#26399;&#38388;&#20877;&#21040;&#21518;&#32039;&#24613;&#26399;&#30340;&#20154;&#20204;&#30340;&#24773;&#32490;&#21464;&#21270;&#65292;&#25506;&#35752;&#20102;&#24773;&#32490;&#26159;&#21542;&#24050;&#24674;&#22797;&#21040;&#30123;&#24773;&#21069;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;COVID-19&#30123;&#24773;&#30340;&#20943;&#36864;&#65292;&#20010;&#20154;&#21644;&#31038;&#20250;&#36880;&#28176;&#24674;&#22797;&#21040;&#30123;&#24773;&#21069;&#30340;&#27963;&#21160;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#31350;&#20154;&#20204;&#30340;&#24773;&#32490;&#20174;&#30123;&#24773;&#21069;&#21040;&#30123;&#24773;&#26399;&#38388;&#20877;&#21040;&#21518;&#32039;&#24613;&#26399;&#30340;&#21464;&#21270;&#65292;&#24182;&#30830;&#23450;&#26159;&#21542;&#24674;&#22797;&#21040;&#30123;&#24773;&#21069;&#30340;&#27700;&#24179;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;2019&#24180;&#65288;&#30123;&#24773;&#21069;&#65289;&#12289;2020&#24180;&#65288;&#30123;&#24773;&#39640;&#23792;&#26399;&#65289;&#12289;2021&#24180;&#21644;2022&#24180;&#65288;&#30123;&#24773;&#21518;&#26399;&#65292;&#36807;&#28193;&#26399;&#21040;&#21518;&#32039;&#24613;&#26399;&#65289;&#26469;&#33258;128&#20010;&#32654;&#22269;&#22823;&#23398;Subreddits&#30340;Reddit&#25968;&#25454;&#20197;&#21450;&#19968;&#32452;&#23398;&#26657;&#32423;&#29305;&#24449;&#12290;&#25105;&#20204;&#37319;&#29992;&#39044;&#35757;&#32451;&#30340;RoBERTa&#65288;Robustly Optimized BERT pre-training approach&#65289;&#21644;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;GAT&#65289;&#26469;&#39044;&#27979;&#20004;&#32452;&#24773;&#32490;&#65292;&#24182;&#24212;&#29992;&#36923;&#36753;&#22534;&#21472;&#26041;&#27861;&#33719;&#24471;&#26368;&#32456;&#30340;&#24773;&#32490;&#20998;&#31867;&#12290;&#22312;&#33719;&#24471;&#27599;&#26465;&#28040;&#24687;&#30340;&#24773;&#32490;&#26631;&#31614;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#24191;&#20041;&#32447;&#24615;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#26469;&#20272;&#35745;&#26102;&#38388;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
As impact of COVID-19 pandemic winds down, both individuals and society gradually return to pre-pandemic activities. This study aims to explore how people's emotions have changed from the pre-pandemic during the pandemic to post-emergency period and whether it has returned to pre-pandemic level. We collected Reddit data in 2019 (pre-pandemic), 2020 (peak pandemic), 2021, and 2022 (late stages of pandemic, transitioning period to post-emergency period) from subreddits in 128 universities/colleges in the U.S., and a set of school-level characteristics. We predicted two sets of sentiments from a pre-trained Robustly Optimized BERT pre-training approach (RoBERTa) and graph attention network (GAT) that leverages both rich semantic and relational information among posted messages and then applied a logistic stacking method to obtain the final sentiment classification. After obtaining sentiment label for each message, we used a generalized linear mixed-effects model to estimate temporal trend
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#27010;&#36848;&#20102;&#29616;&#20195;&#32842;&#22825;&#26426;&#22120;&#20154;&#35774;&#35745;&#20013;&#30340;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#24182;&#20171;&#32461;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21382;&#21490;&#12289;&#20559;&#35265;&#26469;&#28304;&#21644;&#20844;&#24179;&#24615;&#20445;&#25252;&#26041;&#38754;&#30340;&#32771;&#34385;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2309.08836</link><description>&lt;p&gt;
&#32842;&#22825;&#26426;&#22120;&#20154;&#20013;&#30340;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;: &#19968;&#31181;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
Bias and Fairness in Chatbots: An Overview. (arXiv:2309.08836v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08836
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#27010;&#36848;&#20102;&#29616;&#20195;&#32842;&#22825;&#26426;&#22120;&#20154;&#35774;&#35745;&#20013;&#30340;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#24182;&#20171;&#32461;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21382;&#21490;&#12289;&#20559;&#35265;&#26469;&#28304;&#21644;&#20844;&#24179;&#24615;&#20445;&#25252;&#26041;&#38754;&#30340;&#32771;&#34385;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32842;&#22825;&#26426;&#22120;&#20154;&#24050;&#32463;&#30740;&#31350;&#20102;&#21322;&#20010;&#22810;&#19990;&#32426;&#12290;&#38543;&#30528;&#36817;&#24180;&#26469;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#29616;&#22312;&#22791;&#21463;&#20851;&#27880;&#12290;&#19982;&#20256;&#32479;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30456;&#27604;&#65292;&#29616;&#20195;&#32842;&#22825;&#26426;&#22120;&#20154;&#26356;&#24378;&#22823;&#65292;&#24182;&#24050;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#20195;&#32842;&#22825;&#26426;&#22120;&#20154;&#35774;&#35745;&#20013;&#23384;&#22312;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#37327;&#24040;&#22823;&#12289;&#27169;&#22411;&#35268;&#27169;&#24222;&#22823;&#19988;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#29616;&#20195;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#20559;&#35265;&#32531;&#35299;&#21644;&#20844;&#24179;&#20445;&#25252;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#23545;&#32842;&#22825;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#30340;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#27010;&#36848;&#12290;&#39318;&#20808;&#22238;&#39038;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21382;&#21490;&#21644;&#31867;&#21035;&#12290;&#28982;&#21518;&#65292;&#20998;&#26512;&#20102;&#24212;&#29992;&#20013;&#30340;&#20559;&#35265;&#26469;&#28304;&#21644;&#28508;&#22312;&#21361;&#23475;&#12290;&#30740;&#31350;&#20102;&#35774;&#35745;&#20844;&#24179;&#21644;&#26080;&#20559;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#32771;&#34385;&#22240;&#32032;&#12290;&#26368;&#21518;&#65292;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chatbots have been studied for more than half a century. With the rapid development of natural language processing (NLP) technologies in recent years, chatbots using large language models (LLMs) have received much attention nowadays. Compared with traditional ones, modern chatbots are more powerful and have been used in real-world applications. There are however, bias and fairness concerns in modern chatbot design. Due to the huge amounts of training data, extremely large model sizes, and lack of interpretability, bias mitigation and fairness preservation of modern chatbots are challenging. Thus, a comprehensive overview on bias and fairness in chatbot systems is given in this paper. The history of chatbots and their categories are first reviewed. Then, bias sources and potential harms in applications are analyzed. Considerations in designing fair and unbiased chatbot systems are examined. Finally, future research directions are discussed.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SLIDE&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#28369;&#21160;&#25991;&#26723;&#31383;&#21475;&#26469;&#35780;&#20272;&#26426;&#22120;&#32763;&#35793;&#36136;&#37327;&#65292;&#35813;&#26041;&#27861;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#29978;&#33267;&#33021;&#28040;&#38500;&#19982;&#21442;&#32771;&#24230;&#37327;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#34920;&#26126;&#28304;&#35821;&#35328;&#19978;&#19979;&#25991;&#21487;&#33021;&#25552;&#20379;&#20102;&#19982;&#20154;&#31867;&#21442;&#32771;&#30456;&#21516;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2309.08832</link><description>&lt;p&gt;
SLIDE: &#20351;&#29992;&#28369;&#21160;&#25991;&#26723;&#31383;&#21475;&#36827;&#34892;&#26080;&#21442;&#32771;&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
SLIDE: Reference-free Evaluation for Machine Translation using a Sliding Document Window. (arXiv:2309.08832v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08832
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SLIDE&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#28369;&#21160;&#25991;&#26723;&#31383;&#21475;&#26469;&#35780;&#20272;&#26426;&#22120;&#32763;&#35793;&#36136;&#37327;&#65292;&#35813;&#26041;&#27861;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#29978;&#33267;&#33021;&#28040;&#38500;&#19982;&#21442;&#32771;&#24230;&#37327;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#34920;&#26126;&#28304;&#35821;&#35328;&#19978;&#19979;&#25991;&#21487;&#33021;&#25552;&#20379;&#20102;&#19982;&#20154;&#31867;&#21442;&#32771;&#30456;&#21516;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21442;&#32771;&#30340;&#24230;&#37327;&#36890;&#24120;&#22312;&#21477;&#23376;&#32423;&#21035;&#19978;&#20248;&#20110;&#20165;&#33021;&#35775;&#38382;&#28304;&#35821;&#35328;&#21644;&#31995;&#32479;&#36755;&#20986;&#30340;&#36136;&#37327;&#20272;&#35745;&#24230;&#37327;&#12290;&#36825;&#24182;&#19981;&#22855;&#24618;&#65292;&#22240;&#20026;&#21442;&#32771;&#33021;&#22815;&#28040;&#38500;&#28304;&#35821;&#35328;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#27495;&#20041;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#26159;&#21542;&#21487;&#20197;&#29992;&#39069;&#22806;&#30340;&#28304;&#35821;&#35328;&#19978;&#19979;&#25991;&#26377;&#25928;&#22320;&#26367;&#20195;&#21442;&#32771;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24230;&#37327;&#26041;&#27861;&#65292;SLIDE&#65288;SLiding Document Evaluator&#65289;&#65292;&#23427;&#36890;&#36807;&#19968;&#20010;&#28369;&#21160;&#31383;&#21475;&#22312;&#27599;&#20010;&#27979;&#35797;&#38598;&#20013;&#30340;&#25991;&#26723;&#19978;&#25805;&#20316;&#65292;&#23558;&#27599;&#20010;&#22359;&#36755;&#20837;&#21040;&#26410;&#20462;&#25913;&#30340;&#29616;&#25104;&#36136;&#37327;&#20272;&#35745;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;SLIDE&#22312;&#31995;&#32479;&#20934;&#30830;&#24615;&#30340;&#25104;&#23545;&#27604;&#36739;&#19978;&#36739;&#21477;&#23376;&#32423;&#21035;&#22522;&#32447;&#26174;&#33879;&#25552;&#39640;&#65292;&#26377;&#20123;&#24773;&#20917;&#19979;&#29978;&#33267;&#28040;&#38500;&#20102;&#19982;&#21442;&#32771;&#24230;&#37327;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#36825;&#34920;&#26126;&#28304;&#35821;&#35328;&#19978;&#19979;&#25991;&#21487;&#33021;&#25552;&#20379;&#20102;&#19982;&#20154;&#31867;&#21442;&#32771;&#30456;&#21516;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reference-based metrics that operate at the sentence level typically outperform quality estimation metrics, which have access only to the source and system output. This is unsurprising, since references resolve ambiguities that may be present in the source. We investigate whether additional source context can effectively substitute for a reference. We present a metric, SLIDE (SLiding Document Evaluator), which operates on blocks of sentences using a window that slides over each document in the test set, feeding each chunk into an unmodified, off-the-shelf quality estimation model. We find that SLIDE obtains significantly higher pairwise system accuracy than its sentence-level baseline, in some cases even eliminating the gap with reference-base metrics. This suggests that source context may provide the same information as a human reference.
&lt;/p&gt;</description></item><item><title>S3-DST&#26159;&#22522;&#20110;LLM&#30340;&#24320;&#25918;&#22495;&#23545;&#35805;&#20013;&#32467;&#26500;&#21270;&#30340;&#23545;&#35805;&#20998;&#27573;&#21644;&#29366;&#24577;&#36319;&#36394;&#26041;&#27861;&#65292;&#21033;&#29992;Pre-Analytical Recollection&#26426;&#21046;&#25552;&#39640;&#38271;&#19978;&#19979;&#25991;&#36319;&#36394;&#12290;</title><link>http://arxiv.org/abs/2309.08827</link><description>&lt;p&gt;
S3-DST: &#22522;&#20110;LLM&#30340;&#32467;&#26500;&#21270;&#24320;&#25918;&#22495;&#23545;&#35805;&#20998;&#27573;&#21644;&#29366;&#24577;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
S3-DST: Structured Open-Domain Dialogue Segmentation and State Tracking in the Era of LLMs. (arXiv:2309.08827v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08827
&lt;/p&gt;
&lt;p&gt;
S3-DST&#26159;&#22522;&#20110;LLM&#30340;&#24320;&#25918;&#22495;&#23545;&#35805;&#20013;&#32467;&#26500;&#21270;&#30340;&#23545;&#35805;&#20998;&#27573;&#21644;&#29366;&#24577;&#36319;&#36394;&#26041;&#27861;&#65292;&#21033;&#29992;Pre-Analytical Recollection&#26426;&#21046;&#25552;&#39640;&#38271;&#19978;&#19979;&#25991;&#36319;&#36394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394; (DST) &#38382;&#39064;&#26088;&#22312;&#36861;&#36394;&#29992;&#25143;&#22312;&#29992;&#25143;-&#20195;&#29702;&#23545;&#35805;&#20013;&#30340;&#20559;&#22909;&#21644;&#24847;&#22270;&#12290;&#23613;&#31649;&#23545;&#20110;&#25903;&#25345;&#29421;&#20041;&#39046;&#22495;&#24212;&#29992;&#30340;&#20219;&#21153;&#23548;&#21521;&#24615;&#23545;&#35805;&#31995;&#32479;&#26469;&#35828;&#24050;&#32463;&#36275;&#22815;&#65292;&#20294;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#30340;&#32842;&#22825;&#31995;&#32479;&#30340;&#20986;&#29616;&#24341;&#20837;&#20102;&#35768;&#22810;&#24320;&#25918;&#22495;&#23545;&#35805;&#20013;&#30340;&#29616;&#23454;&#22797;&#26434;&#24615;&#12290;&#36825;&#20123;&#22797;&#26434;&#24615;&#20307;&#29616;&#22312;&#19978;&#19979;&#25991;&#20132;&#20114;&#30340;&#22686;&#21152;&#22797;&#26434;&#24615;&#12289;&#28085;&#30422;&#21508;&#31181;&#20027;&#39064;&#30340;&#24310;&#38271;&#23545;&#35805;&#20250;&#35805;&#20197;&#21450;&#26356;&#39057;&#32321;&#30340;&#19978;&#19979;&#25991;&#36716;&#21464;&#31561;&#24418;&#24335;&#12290;&#20026;&#20102;&#22788;&#29702;&#22522;&#20110;&#28436;&#21464;&#30340;LLM&#32842;&#22825;&#31995;&#32479;&#24341;&#36215;&#30340;&#36825;&#20123;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#24320;&#25918;&#22495;&#23545;&#35805;&#31995;&#32479;&#20013;&#23545;&#27599;&#20010;&#27573;&#36827;&#34892;&#32852;&#21512;&#23545;&#35805;&#20998;&#21106;&#21644;&#29366;&#24577;&#36319;&#36394;&#12290;&#22312;&#36866;&#21512;&#30495;&#27491;&#30340;&#24320;&#25918;&#22495;&#23545;&#35805;&#31995;&#32479;&#30340;&#38646;-shot&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;S3-DST&#65292;&#36825;&#26159;&#19968;&#31181;&#32467;&#26500;&#21270;&#25552;&#31034;&#25216;&#26415;&#65292;&#21033;&#29992;&#20102;&#25105;&#20204;&#20026;&#25552;&#39640;&#38271;&#19978;&#19979;&#25991;&#36319;&#36394;&#32780;&#35774;&#35745;&#30340;&#19968;&#31181;&#26032;&#30340;&#25509;&#22320;&#26426;&#21046; - Pre-Analytical Recollection&#12290;
&lt;/p&gt;
&lt;p&gt;
The traditional Dialogue State Tracking (DST) problem aims to track user preferences and intents in user-agent conversations. While sufficient for task-oriented dialogue systems supporting narrow domain applications, the advent of Large Language Model (LLM)-based chat systems has introduced many real-world intricacies in open-domain dialogues. These intricacies manifest in the form of increased complexity in contextual interactions, extended dialogue sessions encompassing a diverse array of topics, and more frequent contextual shifts. To handle these intricacies arising from evolving LLM-based chat systems, we propose joint dialogue segmentation and state tracking per segment in open-domain dialogue systems. Assuming a zero-shot setting appropriate to a true open-domain dialogue system, we propose S3-DST, a structured prompting technique that harnesses Pre-Analytical Recollection, a novel grounding mechanism we designed for improving long context tracking. To demonstrate the efficacy o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#33258;&#35757;&#32451;&#22312;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#23454;&#20363;&#36873;&#25321;&#31574;&#30053;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#30740;&#31350;&#20102;&#31574;&#30053;&#21644;&#36229;&#21442;&#25968;&#23545;&#33258;&#35757;&#32451;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.08777</link><description>&lt;p&gt;
&#33258;&#35757;&#32451;&#22312;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#23454;&#20363;&#36873;&#25321;&#31574;&#30053;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study on Instance Selection Strategies in Self-training for Sentiment Analysis. (arXiv:2309.08777v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08777
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#33258;&#35757;&#32451;&#22312;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#23454;&#20363;&#36873;&#25321;&#31574;&#30053;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#30740;&#31350;&#20102;&#31574;&#30053;&#21644;&#36229;&#21442;&#25968;&#23545;&#33258;&#35757;&#32451;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#20998;&#26512;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#65292;&#28041;&#21450;&#20174;&#25991;&#26412;&#20013;&#35782;&#21035;&#21644;&#25552;&#21462;&#20027;&#35266;&#24773;&#24863;&#12290;&#26368;&#36817;&#65292;&#36890;&#36807;&#21033;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#21644;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#33258;&#35757;&#32451;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#32463;&#27982;&#39640;&#25928;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#24320;&#21457;&#24773;&#24863;&#20998;&#26512;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#33258;&#35757;&#32451;&#36807;&#31243;&#30340;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#23454;&#20363;&#36873;&#25321;&#31574;&#30053;&#30340;&#36873;&#25321;&#65292;&#32780;&#36825;&#26041;&#38754;&#30340;&#30740;&#31350;&#36824;&#19981;&#22815;&#20805;&#20998;&#12290;&#26412;&#25991;&#23545;&#33258;&#35757;&#32451;&#30340;&#21508;&#31181;&#23454;&#20363;&#36873;&#25321;&#31574;&#30053;&#22312;&#20004;&#20010;&#20844;&#24320;&#24773;&#24863;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#30740;&#31350;&#20102;&#31574;&#30053;&#21644;&#36229;&#21442;&#25968;&#22312;&#21508;&#31181;&#23569;&#26679;&#26412;&#35774;&#32622;&#19979;&#23545;&#33258;&#35757;&#32451;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sentiment analysis is a crucial task in natural language processing that involves identifying and extracting subjective sentiment from text. Self-training has recently emerged as an economical and efficient technique for developing sentiment analysis models by leveraging a small amount of labeled data and a larger amount of unlabeled data. However, the performance of a self-training procedure heavily relies on the choice of the instance selection strategy, which has not been studied thoroughly. This paper presents an empirical study on various instance selection strategies for self-training on two public sentiment datasets, and investigates the influence of the strategy and hyper-parameters on the performance of self-training in various few-shot settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#38463;&#23572;&#24052;&#23612;&#20122;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#35821;&#26009;&#24211;AlbNER&#65292;&#35813;&#35821;&#26009;&#24211;&#30001;&#38463;&#23572;&#24052;&#23612;&#20122;&#32500;&#22522;&#30334;&#31185;&#25991;&#31456;&#20013;&#25910;&#38598;&#30340;900&#20010;&#24102;&#26377;&#26631;&#35760;&#21629;&#21517;&#23454;&#20307;&#30340;&#21477;&#23376;&#32452;&#25104;&#12290;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#22823;&#23567;&#23545;NER&#24615;&#33021;&#24433;&#21709;&#23567;&#65292;&#32780;&#35821;&#35328;&#36801;&#31227;&#26377;&#30528;&#26174;&#33879;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#36164;&#28304;&#21644;&#32467;&#26524;&#20026;&#26410;&#26469;&#23454;&#39564;&#25552;&#20379;&#20102;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2309.08741</link><description>&lt;p&gt;
AlbNER&#65306;&#19968;&#20010;&#29992;&#20110;&#38463;&#23572;&#24052;&#23612;&#20122;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
AlbNER: A Corpus for Named Entity Recognition in Albanian. (arXiv:2309.08741v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08741
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#38463;&#23572;&#24052;&#23612;&#20122;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#35821;&#26009;&#24211;AlbNER&#65292;&#35813;&#35821;&#26009;&#24211;&#30001;&#38463;&#23572;&#24052;&#23612;&#20122;&#32500;&#22522;&#30334;&#31185;&#25991;&#31456;&#20013;&#25910;&#38598;&#30340;900&#20010;&#24102;&#26377;&#26631;&#35760;&#21629;&#21517;&#23454;&#20307;&#30340;&#21477;&#23376;&#32452;&#25104;&#12290;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#22823;&#23567;&#23545;NER&#24615;&#33021;&#24433;&#21709;&#23567;&#65292;&#32780;&#35821;&#35328;&#36801;&#31227;&#26377;&#30528;&#26174;&#33879;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#36164;&#28304;&#21644;&#32467;&#26524;&#20026;&#26410;&#26469;&#23454;&#39564;&#25552;&#20379;&#20102;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#38463;&#23572;&#24052;&#23612;&#20122;&#31561;&#36164;&#28304;&#21294;&#20047;&#30340;&#35821;&#35328;&#65292;&#22914;&#35745;&#31639;&#35821;&#35328;&#23398;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20013;&#23384;&#22312;&#30528;&#26631;&#27880;&#25991;&#26412;&#35821;&#26009;&#24211;&#30340;&#20005;&#37325;&#38556;&#30861;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;AlbNER&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#38463;&#23572;&#24052;&#23612;&#20122;&#32500;&#22522;&#30334;&#31185;&#25991;&#31456;&#20013;&#25910;&#38598;&#30340;900&#20010;&#24102;&#26377;&#26631;&#35760;&#21629;&#21517;&#23454;&#20307;&#30340;&#21477;&#23376;&#30340;&#35821;&#26009;&#24211;&#12290;&#29992;&#20351;&#29992;AlbNER&#25968;&#25454;&#36827;&#34892;&#32454;&#35843;&#21644;&#27979;&#35797;&#30340;BERT&#21644;RoBERTa&#21464;&#20307;&#30340;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#22823;&#23567;&#23545;NER&#24615;&#33021;&#30340;&#24433;&#21709;&#24456;&#23567;&#65292;&#32780;&#35821;&#35328;&#36801;&#31227;&#30340;&#24433;&#21709;&#24456;&#22823;&#12290;AlbNER&#35821;&#26009;&#24211;&#21644;&#36825;&#20123;&#32467;&#26524;&#24212;&#20316;&#20026;&#26410;&#26469;&#23454;&#39564;&#30340;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scarcity of resources such as annotated text corpora for under-resourced languages like Albanian is a serious impediment in computational linguistics and natural language processing research. This paper presents AlbNER, a corpus of 900 sentences with labeled named entities, collected from Albanian Wikipedia articles. Preliminary results with BERT and RoBERTa variants fine-tuned and tested with AlbNER data indicate that model size has slight impact on NER performance, whereas language transfer has a significant one. AlbNER corpus and these obtained results should serve as baselines for future experiments.
&lt;/p&gt;</description></item><item><title>MusiLingo&#26159;&#19968;&#20010;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23558;&#38899;&#20048;&#21644;&#25991;&#26412;&#30456;&#32467;&#21512;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#29983;&#25104;&#38899;&#20048;&#23383;&#24149;&#21644;&#22238;&#31572;&#38899;&#20048;&#30456;&#20851;&#30340;&#26597;&#35810;&#12290;&#36890;&#36807;&#20351;&#29992;&#25237;&#24433;&#23618;&#23545;&#40784;&#38899;&#20048;&#34920;&#31034;&#65292;&#35813;&#31995;&#32479;&#25104;&#21151;&#22320;&#23558;&#38899;&#20048;&#38899;&#39057;&#21644;&#25991;&#26412;&#29615;&#22659;&#32852;&#31995;&#36215;&#26469;&#65292;&#21516;&#26102;&#20351;&#29992;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#26469;&#25512;&#21160;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2309.08730</link><description>&lt;p&gt;
MusiLingo&#65306;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23558;&#38899;&#20048;&#21644;&#25991;&#26412;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#38899;&#20048;&#23383;&#24149;&#21644;&#26597;&#35810;&#21709;&#24212;
&lt;/p&gt;
&lt;p&gt;
MusiLingo: Bridging Music and Text with Pre-trained Language Models for Music Captioning and Query Response. (arXiv:2309.08730v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08730
&lt;/p&gt;
&lt;p&gt;
MusiLingo&#26159;&#19968;&#20010;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23558;&#38899;&#20048;&#21644;&#25991;&#26412;&#30456;&#32467;&#21512;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#29983;&#25104;&#38899;&#20048;&#23383;&#24149;&#21644;&#22238;&#31572;&#38899;&#20048;&#30456;&#20851;&#30340;&#26597;&#35810;&#12290;&#36890;&#36807;&#20351;&#29992;&#25237;&#24433;&#23618;&#23545;&#40784;&#38899;&#20048;&#34920;&#31034;&#65292;&#35813;&#31995;&#32479;&#25104;&#21151;&#22320;&#23558;&#38899;&#20048;&#38899;&#39057;&#21644;&#25991;&#26412;&#29615;&#22659;&#32852;&#31995;&#36215;&#26469;&#65292;&#21516;&#26102;&#20351;&#29992;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#26469;&#25512;&#21160;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#22312;&#22810;&#27169;&#24577;&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#28982;&#32780;&#25991;&#26412;&#21644;&#38899;&#20048;&#39046;&#22495;&#30340;&#34701;&#21512;&#20173;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MusiLingo&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#38899;&#20048;&#23383;&#24149;&#29983;&#25104;&#21644;&#38899;&#20048;&#30456;&#20851;&#26597;&#35810;&#21709;&#24212;&#30340;&#26032;&#31995;&#32479;&#12290;MusiLingo&#20351;&#29992;&#19968;&#20010;&#25237;&#24433;&#23618;&#26469;&#23545;&#40784;&#39044;&#35757;&#32451;&#30340;&#20923;&#32467;&#38899;&#20048;&#38899;&#39057;&#27169;&#22411;MERT&#21644;&#20923;&#32467;&#30340;LLaMA&#35821;&#35328;&#27169;&#22411;&#30340;&#38899;&#20048;&#34920;&#31034;&#65292;&#23454;&#29616;&#38899;&#20048;&#38899;&#39057;&#21644;&#25991;&#26412;&#29615;&#22659;&#20043;&#38388;&#30340;&#26725;&#26753;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#38899;&#20048;&#23383;&#24149;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#20351;&#29992;&#25351;&#23548;&#24615;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#12290;&#30001;&#20110;&#39640;&#36136;&#37327;&#30340;&#38899;&#20048;&#38382;&#31572;&#25968;&#25454;&#38598;&#31232;&#32570;&#65292;&#25105;&#20204;&#20174;MusicCaps&#21019;&#24314;&#20102;MusicInstruct&#65288;MI&#65289;&#25968;&#25454;&#38598;&#65292;&#19987;&#20026;&#24320;&#25918;&#24335;&#38899;&#20048;&#26597;&#35810;&#32780;&#35774;&#35745;&#12290;&#23454;&#35777;&#35780;&#20272;&#35777;&#26126;&#20102;&#23427;&#22312;&#29983;&#25104;&#38899;&#20048;&#23383;&#24149;&#21644;&#32452;&#32455;&#38899;&#20048;&#30456;&#20851;&#38382;&#31572;&#23545;&#26041;&#38754;&#30340;&#31454;&#20105;&#24615;&#34920;&#29616;&#12290;&#25105;&#20204;&#24341;&#20837;&#30340;&#25968;&#25454;&#38598;&#22312;&#20043;&#21069;&#30340;&#25968;&#25454;&#38598;&#30340;&#22522;&#30784;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown immense potential in multimodal applications, yet the convergence of textual and musical domains remains relatively unexplored. To address this gap, we present MusiLingo, a novel system for music caption generation and music-related query responses. MusiLingo employs a single projection layer to align music representations from the pre-trained frozen music audio model MERT with the frozen LLaMA language model, bridging the gap between music audio and textual contexts. We train it on an extensive music caption dataset and fine-tune it with instructional data. Due to the scarcity of high-quality music Q&amp;A datasets, we created the MusicInstruct (MI) dataset from MusicCaps, tailored for open-ended music inquiries. Empirical evaluations demonstrate its competitive performance in generating music captions and composing music-related Q&amp;A pairs. Our introduced dataset enables notable advancements beyond previous ones.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Lovelace&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#20351;&#29992;&#22270;&#25193;&#23637;&#25991;&#27861;&#29983;&#25104;&#35821;&#20041;&#22270;&#30340;&#35821;&#26009;&#24211;&#12290;&#36825;&#39033;&#24037;&#20855;&#21487;&#20197;&#36890;&#36807;&#29992;&#25143;&#33258;&#23450;&#20041;&#30340;&#25991;&#27861;&#29983;&#25104;&#31526;&#21512;&#35201;&#27714;&#30340;&#33391;&#22909;&#24418;&#24335;&#30340;&#36755;&#20986;&#22270;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#21512;&#25104;&#25968;&#25454;&#22686;&#24378;&#35821;&#26009;&#24211;&#21644;&#25945;&#23398;&#24418;&#24335;&#35821;&#35328;&#29702;&#35770;&#30340;&#29992;&#36884;&#12290;</title><link>http://arxiv.org/abs/2309.08714</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#25193;&#23637;&#25991;&#27861;&#29983;&#25104;&#35821;&#20041;&#22270;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
Generating Semantic Graph Corpora with Graph Expansion Grammar. (arXiv:2309.08714v1 [cs.FL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08714
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Lovelace&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#20351;&#29992;&#22270;&#25193;&#23637;&#25991;&#27861;&#29983;&#25104;&#35821;&#20041;&#22270;&#30340;&#35821;&#26009;&#24211;&#12290;&#36825;&#39033;&#24037;&#20855;&#21487;&#20197;&#36890;&#36807;&#29992;&#25143;&#33258;&#23450;&#20041;&#30340;&#25991;&#27861;&#29983;&#25104;&#31526;&#21512;&#35201;&#27714;&#30340;&#33391;&#22909;&#24418;&#24335;&#30340;&#36755;&#20986;&#22270;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#21512;&#25104;&#25968;&#25454;&#22686;&#24378;&#35821;&#26009;&#24211;&#21644;&#25945;&#23398;&#24418;&#24335;&#35821;&#35328;&#29702;&#35770;&#30340;&#29992;&#36884;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Lovelace&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#21019;&#24314;&#35821;&#20041;&#22270;&#30340;&#35821;&#26009;&#24211;&#12290;&#35813;&#31995;&#32479;&#20351;&#29992;&#22270;&#25193;&#23637;&#25991;&#27861;&#20316;&#20026;&#34920;&#36798;&#35821;&#35328;&#65292;&#20801;&#35768;&#29992;&#25143;&#21046;&#23450;&#25551;&#36848;&#25152;&#38656;&#23646;&#24615;&#30340;&#25991;&#27861;&#26469;&#25551;&#36848;&#35821;&#26009;&#24211;&#12290;&#24403;&#23558;&#36825;&#26679;&#30340;&#25991;&#27861;&#20316;&#20026;&#36755;&#20837;&#26102;&#65292;&#31995;&#32479;&#23558;&#29983;&#25104;&#19968;&#32452;&#26681;&#25454;&#25991;&#27861;&#26500;&#36896;&#33391;&#22909;&#30340;&#36755;&#20986;&#22270;&#65292;&#21363;&#22270;&#24211;&#12290;&#29983;&#25104;&#36807;&#31243;&#21487;&#20197;&#36890;&#36807;&#22810;&#20010;&#21487;&#37197;&#32622;&#21442;&#25968;&#36827;&#34892;&#25511;&#21046;&#65292;&#20363;&#22914;&#65292;&#29992;&#25143;&#21487;&#20197;&#25351;&#23450;&#25152;&#38656;&#30340;&#36755;&#20986;&#22270;&#22823;&#23567;&#33539;&#22260;&#12290;&#26680;&#24515;&#29992;&#20363;&#26159;&#21019;&#24314;&#21512;&#25104;&#25968;&#25454;&#20197;&#25193;&#20805;&#29616;&#26377;&#35821;&#26009;&#24211;&#65292;&#24182;&#20316;&#20026;&#25945;&#25480;&#24418;&#24335;&#35821;&#35328;&#29702;&#35770;&#30340;&#25945;&#23398;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Lovelace, a tool for creating corpora of semantic graphs. The system uses graph expansion grammar as a representational language, thus allowing users to craft a grammar that describes a corpus with desired properties. When given such grammar as input, the system generates a set of output graphs that are well-formed according to the grammar, i.e., a graph bank. The generation process can be controlled via a number of configurable parameters that allow the user to, for example, specify a range of desired output graph sizes. Central use cases are the creation of synthetic data to augment existing corpora, and as a pedagogical tool for teaching formal language theory.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#23884;&#20837;&#21098;&#26525;&#26469;&#20943;&#23567;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#23384;&#21344;&#29992;&#12290;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#27169;&#22411;&#21644;&#20219;&#21153;&#20013;&#37117;&#33021;&#26174;&#33879;&#38477;&#20302;&#20869;&#23384;&#20351;&#29992;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#24403;&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#65292;&#23454;&#29616;&#26356;&#39640;&#25928;&#22320;&#21033;&#29992;&#35745;&#31639;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2309.08708</link><description>&lt;p&gt;
&#32463;&#30001;&#21160;&#24577;&#23884;&#20837;&#21098;&#26525;&#23454;&#29616;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20196;&#20154;&#27822;&#20007;&#22320;&#31616;&#21333;&#30340;&#20869;&#23384;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Frustratingly Simple Memory Efficiency for Pre-trained Language Models via Dynamic Embedding Pruning. (arXiv:2309.08708v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08708
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#23884;&#20837;&#21098;&#26525;&#26469;&#20943;&#23567;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#23384;&#21344;&#29992;&#12290;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#27169;&#22411;&#21644;&#20219;&#21153;&#20013;&#37117;&#33021;&#26174;&#33879;&#38477;&#20302;&#20869;&#23384;&#20351;&#29992;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#24403;&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#65292;&#23454;&#29616;&#26356;&#39640;&#25928;&#22320;&#21033;&#29992;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#24191;&#27867;&#20869;&#23384;&#21344;&#29992;&#20250;&#38459;&#30861;&#20854;&#22312;&#20869;&#23384;&#21463;&#38480;&#29615;&#22659;&#65288;&#22914;&#20113;&#29615;&#22659;&#25110;&#35774;&#22791;&#19978;&#65289;&#30340;&#37096;&#32626;&#12290; PLMs&#20351;&#29992;&#23884;&#20837;&#30697;&#38453;&#26469;&#34920;&#31034;&#24191;&#27867;&#30340;&#35789;&#27719;&#65292;&#26500;&#25104;&#20102;&#27169;&#22411;&#21442;&#25968;&#30340;&#22823;&#37096;&#20998;&#12290;&#23613;&#31649;&#20043;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#32771;&#34385;&#20102;&#22312;Transformer&#23618;&#20869;&#21098;&#26525;&#21442;&#25968;&#20197;&#25552;&#39640;&#21442;&#25968;&#25928;&#29575;&#65292;&#20294;&#22312;&#24494;&#35843;&#25110;&#25512;&#29702;&#36807;&#31243;&#20013;&#21098;&#26525;&#23884;&#20837;&#30697;&#38453;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#26377;&#19968;&#20010;&#26174;&#33879;&#27604;&#20363;&#30340;&#35789;&#27719;&#26410;&#34987;&#20351;&#29992;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#36825;&#19968;&#21457;&#29616;&#26469;&#26368;&#23567;&#21270;&#23884;&#20837;&#30697;&#38453;&#30340;&#20869;&#23384;&#21344;&#29992;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#21508;&#31181;&#27169;&#22411;&#21644;&#20219;&#21153;&#20013;&#37117;&#33021;&#26174;&#33879;&#38477;&#20302;&#20869;&#23384;&#20351;&#29992;&#37327;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20445;&#25345;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#30340;&#21516;&#26102;&#20801;&#35768;&#26356;&#39640;&#25928;&#22320;&#20351;&#29992;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
The extensive memory footprint of pre-trained language models (PLMs) can hinder deployment in memory-constrained settings, such as cloud environments or on-device. PLMs use embedding matrices to represent extensive vocabularies, forming a large proportion of the model parameters. While previous work towards parameter-efficient PLM development has considered pruning parameters within the transformer layers, pruning the embedding matrix as part of fine-tuning or inference has yet to be explored. We first demonstrate that a significant proportion of the vocabulary remains unused in these scenarios. We then propose a simple yet effective approach that leverages this finding to minimize the memory footprint of the embedding matrix. We show that this approach provides substantial reductions in memory usage across a wide range of models and tasks. Notably, our approach maintains equivalent downstream task performance while allowing a more efficient use of compute resources.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22810;&#35821;&#35328;&#25506;&#32034;&#65292;&#35299;&#20915;&#20102;&#27861;&#24459;&#25991;&#20214;&#20013;&#21542;&#23450;&#33539;&#22260;&#35299;&#26512;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20197;&#24448;&#27169;&#22411;&#22312;&#22788;&#29702;&#22810;&#35821;&#35328;&#27861;&#24459;&#25968;&#25454;&#26102;&#34920;&#29616;&#19981;&#20339;&#65292;&#22240;&#27492;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#22871;&#26032;&#30340;&#27861;&#24237;&#21028;&#20915;&#26631;&#27880;&#25968;&#25454;&#29992;&#20110;&#25913;&#36827;&#35299;&#26512;&#25928;&#26524;&#65292;&#24182;&#21462;&#24471;&#20102;&#39640;&#36798;86.7&#65285;&#30340;&#26631;&#35760;&#32423;F1&#20998;&#12290;</title><link>http://arxiv.org/abs/2309.08695</link><description>&lt;p&gt;
&#35299;&#20915;&#27861;&#24459;&#26415;&#35821;&#65306;&#27861;&#24459;&#25991;&#20214;&#20013;&#21542;&#23450;&#33539;&#22260;&#35299;&#26512;&#30340;&#22810;&#35821;&#35328;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Resolving Legalese: A Multilingual Exploration of Negation Scope Resolution in Legal Documents. (arXiv:2309.08695v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22810;&#35821;&#35328;&#25506;&#32034;&#65292;&#35299;&#20915;&#20102;&#27861;&#24459;&#25991;&#20214;&#20013;&#21542;&#23450;&#33539;&#22260;&#35299;&#26512;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20197;&#24448;&#27169;&#22411;&#22312;&#22788;&#29702;&#22810;&#35821;&#35328;&#27861;&#24459;&#25968;&#25454;&#26102;&#34920;&#29616;&#19981;&#20339;&#65292;&#22240;&#27492;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#22871;&#26032;&#30340;&#27861;&#24237;&#21028;&#20915;&#26631;&#27880;&#25968;&#25454;&#29992;&#20110;&#25913;&#36827;&#35299;&#26512;&#25928;&#26524;&#65292;&#24182;&#21462;&#24471;&#20102;&#39640;&#36798;86.7&#65285;&#30340;&#26631;&#35760;&#32423;F1&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21477;&#23376;&#20013;&#35299;&#26512;&#21542;&#23450;&#30340;&#33539;&#22260;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;&#27861;&#24459;&#25991;&#26412;&#30340;&#22797;&#26434;&#24615;&#20197;&#21450;&#32570;&#20047;&#32463;&#36807;&#27880;&#37322;&#30340;&#39046;&#22495;&#20869;&#21542;&#23450;&#35821;&#26009;&#24211;&#32473;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22312;&#22788;&#29702;&#22810;&#35821;&#35328;&#27861;&#24459;&#25968;&#25454;&#19978;&#30340;&#21542;&#23450;&#33539;&#22260;&#35299;&#26512;&#26102;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#39044;&#20808;&#26410;&#20351;&#29992;&#27861;&#24459;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#21542;&#23450;&#33539;&#22260;&#35299;&#26512;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#20351;&#29992;&#20165;&#22312;&#25991;&#23398;&#25991;&#26412;&#21644;&#21307;&#23398;&#25968;&#25454;&#31561;&#39046;&#22495;&#36827;&#34892;&#20102;&#31934;&#32454;&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#19982;&#20043;&#21069;&#30340;&#36328;&#39046;&#22495;&#23454;&#39564;&#20013;&#35760;&#24405;&#30340;&#32467;&#26524;&#30456;&#27604;&#65292;&#25928;&#26524;&#36739;&#24046;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#22871;&#24503;&#35821;&#12289;&#27861;&#35821;&#21644;&#24847;&#22823;&#21033;&#35821;&#30340;&#26631;&#27880;&#27861;&#38498;&#21028;&#20915;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#25913;&#36827;&#38646;&#25668;&#21462;&#21644;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#30340;&#21542;&#23450;&#33539;&#22260;&#35299;&#26512;&#12290;&#22312;&#25105;&#20204;&#30340;&#38646;&#25668;&#21462;&#36328;&#35821;&#35328;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26631;&#35760;&#32423;F1&#20998;&#36798;&#21040;&#20102;86.7&#65285;&#65292;&#20854;&#20013;&#27169;&#22411;&#22312;&#25105;&#20204;&#30340;&#27861;&#24459;&#25968;&#25454;&#38598;&#30340;&#20004;&#31181;&#35821;&#35328;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#31532;&#19977;&#31181;&#35821;&#35328;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Resolving the scope of a negation within a sentence is a challenging NLP task. The complexity of legal texts and the lack of annotated in-domain negation corpora pose challenges for state-of-the-art (SotA) models when performing negation scope resolution on multilingual legal data. Our experiments demonstrate that models pre-trained without legal data underperform in the task of negation scope resolution. Our experiments, using language models exclusively fine-tuned on domains like literary texts and medical data, yield inferior results compared to the outcomes documented in prior cross-domain experiments. We release a new set of annotated court decisions in German, French, and Italian and use it to improve negation scope resolution in both zero-shot and multilingual settings. We achieve token-level F1-scores of up to 86.7% in our zero-shot cross-lingual experiments, where the models are trained on two languages of our legal datasets and evaluated on the third. Our multilingual experim
&lt;/p&gt;</description></item><item><title>&#20551;&#26032;&#38395;&#26816;&#27979;&#22120;&#20542;&#21521;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20869;&#23481;&#26631;&#35760;&#20026;&#20551;&#26032;&#38395;&#65292;&#32780;&#23558;&#20154;&#24037;&#32534;&#20889;&#30340;&#20551;&#26032;&#38395;&#35823;&#20998;&#31867;&#20026;&#30495;&#23454;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25932;&#23545;&#35757;&#32451;&#21644;LLM&#25913;&#20889;&#30340;&#30495;&#23454;&#26032;&#38395;&#31561;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.08674</link><description>&lt;p&gt;
&#20551;&#26032;&#38395;&#26816;&#27979;&#22120;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#23384;&#22312;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Fake News Detectors are Biased against Texts Generated by Large Language Models. (arXiv:2309.08674v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08674
&lt;/p&gt;
&lt;p&gt;
&#20551;&#26032;&#38395;&#26816;&#27979;&#22120;&#20542;&#21521;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20869;&#23481;&#26631;&#35760;&#20026;&#20551;&#26032;&#38395;&#65292;&#32780;&#23558;&#20154;&#24037;&#32534;&#20889;&#30340;&#20551;&#26032;&#38395;&#35823;&#20998;&#31867;&#20026;&#30495;&#23454;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25932;&#23545;&#35757;&#32451;&#21644;LLM&#25913;&#20889;&#30340;&#30495;&#23454;&#26032;&#38395;&#31561;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20551;&#26032;&#38395;&#30340;&#20256;&#25773;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#25439;&#23475;&#20102;&#20449;&#20219;&#24182;&#23545;&#31038;&#20250;&#26500;&#25104;&#23041;&#32961;&#12290;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26102;&#20195;&#65292;&#29983;&#25104;&#21487;&#20449;&#30340;&#20551;&#20869;&#23481;&#30340;&#33021;&#21147;&#21152;&#21095;&#20102;&#36825;&#20123;&#25285;&#24551;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#26469;&#35780;&#20272;&#22312;&#28041;&#21450;&#20154;&#24037;&#32534;&#20889;&#21644;LLM&#29983;&#25104;&#30340;&#38169;&#35823;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#22120;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;&#35768;&#22810;&#29616;&#26377;&#26816;&#27979;&#22120;&#23384;&#22312;&#26174;&#33879;&#30340;&#20559;&#35265;&#65306;&#23427;&#20204;&#26356;&#23481;&#26131;&#23558;LLM&#29983;&#25104;&#30340;&#20869;&#23481;&#26631;&#35760;&#20026;&#20551;&#26032;&#38395;&#65292;&#21516;&#26102;&#24120;&#24120;&#23558;&#20154;&#24037;&#32534;&#20889;&#30340;&#20551;&#26032;&#38395;&#35823;&#20998;&#31867;&#20026;&#30495;&#23454;&#12290;&#36825;&#31181;&#24847;&#22806;&#30340;&#20559;&#35265;&#20284;&#20046;&#28304;&#33258;LLM&#36755;&#20986;&#22266;&#26377;&#30340;&#19981;&#21516;&#35821;&#35328;&#27169;&#24335;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;LLM&#25913;&#20889;&#30340;&#30495;&#23454;&#26032;&#38395;&#36827;&#34892;&#25932;&#23545;&#35757;&#32451;&#30340;&#32531;&#35299;&#31574;&#30053;&#12290;&#32467;&#26524;&#27169;&#22411;&#22312;&#20154;&#24037;&#21644;LLM&#29983;&#25104;&#30340;&#26032;&#38395;&#30340;&#26816;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#22343;&#26377;&#26174;&#33879;&#25552;&#21319;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#20419;&#36827;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#20004;&#20010;&#20840;&#38754;&#30340;...
&lt;/p&gt;
&lt;p&gt;
The spread of fake news has emerged as a critical challenge, undermining trust and posing threats to society. In the era of Large Language Models (LLMs), the capability to generate believable fake content has intensified these concerns. In this study, we present a novel paradigm to evaluate fake news detectors in scenarios involving both human-written and LLM-generated misinformation. Intriguingly, our findings reveal a significant bias in many existing detectors: they are more prone to flagging LLM-generated content as fake news while often misclassifying human-written fake news as genuine. This unexpected bias appears to arise from distinct linguistic patterns inherent to LLM outputs. To address this, we introduce a mitigation strategy that leverages adversarial training with LLM-paraphrased genuine news. The resulting model yielded marked improvements in detection accuracy for both human and LLM-generated news. To further catalyze research in this domain, we release two comprehensiv
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23545;&#21253;&#21547;&#23454;&#20307;&#20132;&#25442;&#30340;&#34920;&#26684;&#36827;&#34892;&#30340;&#23545;&#25239;&#25915;&#20987;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21015;&#31867;&#22411;&#27880;&#37322;&#20219;&#21153;&#30340;&#36867;&#36991;&#24615;&#23454;&#20307;&#20132;&#25442;&#25915;&#20987;&#65292;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110;&#30456;&#20284;&#24230;&#30340;&#37319;&#26679;&#31574;&#30053;&#29983;&#25104;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#25104;&#21151;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#20102;&#39640;&#36798;70%&#12290;</title><link>http://arxiv.org/abs/2309.08650</link><description>&lt;p&gt;
&#23545;&#21253;&#21547;&#23454;&#20307;&#20132;&#25442;&#30340;&#34920;&#26684;&#36827;&#34892;&#30340;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Adversarial Attacks on Tables with Entity Swap. (arXiv:2309.08650v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23545;&#21253;&#21547;&#23454;&#20307;&#20132;&#25442;&#30340;&#34920;&#26684;&#36827;&#34892;&#30340;&#23545;&#25239;&#25915;&#20987;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21015;&#31867;&#22411;&#27880;&#37322;&#20219;&#21153;&#30340;&#36867;&#36991;&#24615;&#23454;&#20307;&#20132;&#25442;&#25915;&#20987;&#65292;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110;&#30456;&#20284;&#24230;&#30340;&#37319;&#26679;&#31574;&#30053;&#29983;&#25104;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#25104;&#21151;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#20102;&#39640;&#36798;70%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#33021;&#21147;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#34920;&#26684;&#34920;&#31034;&#23398;&#20064;&#30340;&#29615;&#22659;&#20013;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#34920;&#26684;&#35821;&#35328;&#27169;&#22411;&#22312;&#34920;&#26684;&#35299;&#37322;&#30340;&#21508;&#31181;&#20219;&#21153;&#19978;&#25253;&#21578;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23545;&#24120;&#29992;&#20110;&#35780;&#20272;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20180;&#32454;&#35266;&#23519;&#21457;&#29616;&#65292;&#35757;&#32451;&#38598;&#20013;&#30340;&#23454;&#20307;&#27844;&#28431;&#33267;&#27979;&#35797;&#38598;&#20013;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#26356;&#30495;&#23454;&#30340;&#25512;&#29702;&#35774;&#32622;&#30340;&#23545;&#25239;&#25915;&#20987;&#12290;&#24050;&#32463;&#35777;&#26126;&#65292;&#23545;&#25991;&#26412;&#30340;&#23545;&#25239;&#25915;&#20987;&#26497;&#22823;&#22320;&#24433;&#21709;&#20102;LLMs&#30340;&#24615;&#33021;&#65292;&#20294;&#30446;&#21069;&#23578;&#26080;&#25915;&#20987;&#38024;&#23545;&#34920;&#26684;&#35821;&#35328;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21015;&#31867;&#22411;&#27880;&#37322;(CTA)&#20219;&#21153;&#30340;&#36867;&#36991;&#24615;&#23454;&#20307;&#20132;&#25442;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;CTA&#25915;&#20987;&#26159;&#23545;&#34920;&#26684;&#30340;&#31532;&#19968;&#27425;&#40657;&#30418;&#25915;&#20987;&#65292;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#30456;&#20284;&#24230;&#30340;&#37319;&#26679;&#31574;&#30053;&#29983;&#25104;&#23545;&#25239;&#24615;&#31034;&#20363;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25152;&#25552;&#20986;&#30340;&#25915;&#20987;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#20102;&#39640;&#36798;70%&#12290;
&lt;/p&gt;
&lt;p&gt;
The capabilities of large language models (LLMs) have been successfully applied in the context of table representation learning. The recently proposed tabular language models have reported state-of-the-art results across various tasks for table interpretation. However, a closer look into the datasets commonly used for evaluation reveals an entity leakage from the train set into the test set. Motivated by this observation, we explore adversarial attacks that represent a more realistic inference setup. Adversarial attacks on text have been shown to greatly affect the performance of LLMs, but currently, there are no attacks targeting tabular language models. In this paper, we propose an evasive entity-swap attack for the column type annotation (CTA) task. Our CTA attack is the first black-box attack on tables, where we employ a similarity-based sampling strategy to generate adversarial examples. The experimental results show that the proposed attack generates up to a 70% drop in performan
&lt;/p&gt;</description></item><item><title>MAPLE&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#36827;&#34892;&#31227;&#21160;&#24212;&#29992;&#39044;&#27979;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20005;&#26684;&#27979;&#35797;&#39564;&#35777;&#20102;&#20854;&#22312;&#35299;&#23494;&#22797;&#26434;&#27169;&#24335;&#21644;&#29702;&#35299;&#29992;&#25143;&#29615;&#22659;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#24378;&#35843;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.08648</link><description>&lt;p&gt;
MAPLE: &#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#30340;&#31227;&#21160;&#24212;&#29992;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
MAPLE: Mobile App Prediction Leveraging Large Language model Embeddings. (arXiv:2309.08648v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08648
&lt;/p&gt;
&lt;p&gt;
MAPLE&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#36827;&#34892;&#31227;&#21160;&#24212;&#29992;&#39044;&#27979;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20005;&#26684;&#27979;&#35797;&#39564;&#35777;&#20102;&#20854;&#22312;&#35299;&#23494;&#22797;&#26434;&#27169;&#24335;&#21644;&#29702;&#35299;&#29992;&#25143;&#29615;&#22659;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#24378;&#35843;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31227;&#21160;&#24212;&#29992;&#30340;&#21457;&#23637;&#36805;&#36895;&#65292;&#20294;&#30001;&#20110;&#22797;&#26434;&#30340;&#29992;&#25143;&#34892;&#20026;&#21644;&#19981;&#26029;&#28436;&#21464;&#30340;&#29615;&#22659;&#65292;&#39044;&#27979;&#24212;&#29992;&#30340;&#20351;&#29992;&#20173;&#28982;&#26159;&#19968;&#20010;&#20005;&#23803;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;Mobile App Prediction Leveraging Large Language Model Embeddings (MAPLE)&#27169;&#22411;&#12290;&#36825;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#26469;&#20934;&#30830;&#39044;&#27979;&#24212;&#29992;&#30340;&#20351;&#29992;&#24773;&#20917;&#12290;&#36890;&#36807;&#23545;&#20004;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#36827;&#34892;&#20005;&#26684;&#27979;&#35797;&#65292;MAPLE&#30340;&#33021;&#21147;&#22312;&#35299;&#23494;&#22797;&#26434;&#27169;&#24335;&#21644;&#29702;&#35299;&#29992;&#25143;&#29615;&#22659;&#26041;&#38754;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;&#36825;&#20123;&#24378;&#22823;&#30340;&#32467;&#26524;&#35777;&#23454;&#20102;MAPLE&#22312;&#19981;&#21516;&#22330;&#26223;&#20013;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#24377;&#24615;&#12290;&#23613;&#31649;&#20854;&#20027;&#35201;&#35774;&#35745;&#38754;&#21521;&#24212;&#29992;&#39044;&#27979;&#65292;&#20294;&#32467;&#26524;&#20063;&#24378;&#35843;&#20102;LLM&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;&#36890;&#36807;&#36825;&#39033;&#30740;&#31350;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;LLM&#22312;&#24212;&#29992;&#20351;&#29992;&#39044;&#27979;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#24314;&#35758;&#22312;&#24314;&#27169;&#21508;&#31181;&#39046;&#22495;&#20013;&#30340;&#20154;&#31867;&#34892;&#20026;&#26041;&#38754;&#65292;&#23427;&#20204;&#20855;&#26377;&#21464;&#38761;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the rapid advancement of mobile applications, predicting app usage remains a formidable challenge due to intricate user behaviours and ever-evolving contexts. To address these issues, this paper introduces the Mobile App Prediction Leveraging Large Language Model Embeddings (MAPLE) model. This innovative approach utilizes Large Language Models (LLMs) to predict app usage accurately. Rigorous testing on two public datasets highlights MAPLE's capability to decipher intricate patterns and comprehend user contexts. These robust results confirm MAPLE's versatility and resilience across various scenarios. While its primary design caters to app prediction, the outcomes also emphasize the broader applicability of LLMs in different domains. Through this research, we emphasize the potential of LLMs in app usage prediction and suggest their transformative capacity in modelling human behaviours across diverse fields.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#36890;&#29992;&#27169;&#22411;&#19982;&#27599;&#20010;&#23458;&#25143;&#30340;&#30456;&#20851;&#24847;&#22270;&#21015;&#34920;&#30456;&#32467;&#21512;&#65292;&#23558;&#24847;&#22270;&#26816;&#27979;&#25193;&#23637;&#21040;&#19981;&#21516;&#30340;&#23458;&#25143;&#12290;&#36825;&#31181;&#26041;&#27861;&#20943;&#23569;&#20102;&#22521;&#35757;&#21644;&#32500;&#25252;&#25104;&#26412;&#65292;&#21516;&#26102;&#20026;&#23458;&#25143;&#25552;&#20379;&#20010;&#24615;&#21270;&#20307;&#39564;&#65292;&#24182;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.08647</link><description>&lt;p&gt;
&#22312;&#35268;&#27169;&#19978;&#36827;&#34892;&#24847;&#22270;&#26816;&#27979;&#65306;&#21033;&#29992;&#30456;&#20851;&#24847;&#22270;&#36827;&#34892;&#36890;&#29992;&#27169;&#22411;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Intent Detection at Scale: Tuning a Generic Model using Relevant Intents. (arXiv:2309.08647v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08647
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#36890;&#29992;&#27169;&#22411;&#19982;&#27599;&#20010;&#23458;&#25143;&#30340;&#30456;&#20851;&#24847;&#22270;&#21015;&#34920;&#30456;&#32467;&#21512;&#65292;&#23558;&#24847;&#22270;&#26816;&#27979;&#25193;&#23637;&#21040;&#19981;&#21516;&#30340;&#23458;&#25143;&#12290;&#36825;&#31181;&#26041;&#27861;&#20943;&#23569;&#20102;&#22521;&#35757;&#21644;&#32500;&#25252;&#25104;&#26412;&#65292;&#21516;&#26102;&#20026;&#23458;&#25143;&#25552;&#20379;&#20010;&#24615;&#21270;&#20307;&#39564;&#65292;&#24182;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#23458;&#25143;&#25903;&#25345;&#35831;&#27714;&#30340;&#24847;&#22270;&#23545;&#20110;&#39640;&#25928;&#30340;&#25903;&#25345;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#65292;&#20351;&#20195;&#29702;&#20154;&#33021;&#22815;&#24555;&#36895;&#29702;&#35299;&#20449;&#24687;&#24182;&#20248;&#20808;&#21709;&#24212;&#12290;&#23613;&#31649;&#23384;&#22312;&#19981;&#21516;&#30340;&#24847;&#22270;&#26816;&#27979;&#26041;&#27861;&#65292;&#20294;&#26159;&#38543;&#30528;&#23458;&#25143;&#32676;&#20307;&#30340;&#25193;&#22823;&#65292;&#32500;&#25252;&#21333;&#29420;&#30340;&#23458;&#25143;&#29305;&#23450;&#25110;&#34892;&#19994;&#29305;&#23450;&#27169;&#22411;&#21487;&#33021;&#25104;&#26412;&#39640;&#26114;&#19988;&#19981;&#20999;&#23454;&#38469;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#22320;&#23558;&#24847;&#22270;&#39044;&#27979;&#25193;&#23637;&#21040;&#21508;&#31181;&#23458;&#25143;&#30340;&#31995;&#32479;&#65292;&#21363;&#36890;&#36807;&#23558;&#21333;&#19968;&#36890;&#29992;&#27169;&#22411;&#19982;&#27599;&#20010;&#23458;&#25143;&#30340;&#30456;&#20851;&#24847;&#22270;&#21015;&#34920;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26368;&#22823;&#38480;&#24230;&#22320;&#20943;&#23569;&#20102;&#22521;&#35757;&#21644;&#32500;&#25252;&#25104;&#26412;&#65292;&#21516;&#26102;&#20026;&#23458;&#25143;&#25552;&#20379;&#20010;&#24615;&#21270;&#20307;&#39564;&#65292;&#23454;&#29616;&#23545;&#20854;&#30456;&#20851;&#24847;&#22270;&#30340;&#21464;&#21270;&#30340;&#26080;&#32541;&#36866;&#24212;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23458;&#25143;&#30456;&#20851;&#24847;&#22270;&#20316;&#20026;&#27169;&#22411;&#29305;&#24449;&#30340;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#23545;&#23458;&#25143;&#30456;&#20851;&#24847;&#22270;&#30340;&#21464;&#21270;&#20855;&#26377;&#38887;&#24615;&#12290;&#26368;&#32456;&#31995;&#32479;&#30340;&#24615;&#33021;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurately predicting the intent of customer support requests is vital for efficient support systems, enabling agents to quickly understand messages and prioritize responses accordingly. While different approaches exist for intent detection, maintaining separate client-specific or industry-specific models can be costly and impractical as the client base expands.  This work proposes a system to scale intent predictions to various clients effectively, by combining a single generic model with a per-client list of relevant intents. Our approach minimizes training and maintenance costs while providing a personalized experience for clients, allowing for seamless adaptation to changes in their relevant intents. Furthermore, we propose a strategy for using the clients relevant intents as model features that proves to be resilient to changes in the relevant intents of clients -- a common occurrence in production environments.  The final system exhibits significantly superior performance compare
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#20849;&#32447;&#32422;&#26463;&#27880;&#24847;&#21147;&#65288;CoCA&#65289;&#32467;&#26500;&#65292;&#35299;&#20915;Transformer&#27169;&#22411;&#20013;&#30340;&#22836;&#30171;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#20986;&#33394;&#30340;&#22806;&#25512;&#24615;&#33021;&#21644;&#25552;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.08646</link><description>&lt;p&gt;
&#36890;&#36807;&#20849;&#32447;&#32422;&#26463;&#27880;&#24847;&#21147;&#35299;&#20915;Transformer&#30340;&#22836;&#30171;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Cure the headache of Transformers via Collinear Constrained Attention. (arXiv:2309.08646v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08646
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#20849;&#32447;&#32422;&#26463;&#27880;&#24847;&#21147;&#65288;CoCA&#65289;&#32467;&#26500;&#65292;&#35299;&#20915;Transformer&#27169;&#22411;&#20013;&#30340;&#22836;&#30171;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#20986;&#33394;&#30340;&#22806;&#25512;&#24615;&#33021;&#21644;&#25552;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#38469;&#24212;&#29992;&#30340;&#24555;&#36895;&#36827;&#23637;&#65292;&#25512;&#26029;&#24615;&#33021;&#30340;&#22806;&#25512;&#21464;&#24471;&#22312;&#30740;&#31350;&#39046;&#22495;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;Transformer&#27169;&#22411;&#20013;&#30340;&#19968;&#20010;&#34987;&#20043;&#21069;&#24573;&#35270;&#30340;&#24322;&#24120;&#34892;&#20026;&#65292;&#23548;&#33268;&#20102;&#26368;&#25509;&#36817;&#30340;&#26631;&#35760;&#20043;&#38388;&#30340;&#28151;&#20081;&#65292;&#36825;&#20123;&#26631;&#35760;&#25658;&#24102;&#20102;&#26368;&#37325;&#35201;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#23558;&#36825;&#19968;&#21457;&#29616;&#31216;&#20026;&#8220;Transformer&#30340;&#22836;&#30171;&#38382;&#39064;&#8221;&#12290;&#20026;&#20102;&#20174;&#26681;&#26412;&#19978;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#27880;&#24847;&#32467;&#26500;&#65292;&#21629;&#21517;&#20026;Collinear Constrained Attention&#65288;CoCA&#65289;&#12290;&#36825;&#20010;&#32467;&#26500;&#21487;&#20197;&#26080;&#32541;&#22320;&#19982;&#29616;&#26377;&#30340;&#25512;&#26029;&#12289;&#25554;&#20540;&#26041;&#27861;&#21644;&#20854;&#20182;&#38024;&#23545;&#20256;&#32479;Transformer&#27169;&#22411;&#35774;&#35745;&#30340;&#20248;&#21270;&#31574;&#30053;&#38598;&#25104;&#12290;&#25105;&#20204;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#23454;&#29616;&#20102;&#20248;&#31168;&#30340;&#22806;&#25512;&#24615;&#33021;&#65292;&#21363;&#20351;&#26159;16&#21040;24&#20493;&#30340;&#24207;&#21015;&#38271;&#24230;&#65292;&#32780;&#19988;&#27809;&#26377;&#23545;&#25105;&#20204;&#30340;&#27169;&#22411;&#36827;&#34892;&#20219;&#20309;&#24494;&#35843;&#12290;&#25105;&#20204;&#36824;&#22686;&#24378;&#20102;CoCA&#30340;&#35745;&#31639;&#21644;&#31354;&#38388;&#25928;&#29575;&#65292;&#20197;&#30830;&#20445;&#20854;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#35745;&#21010;...
&lt;/p&gt;
&lt;p&gt;
As the rapid progression of practical applications based on Large Language Models continues, the importance of extrapolating performance has grown exponentially in the research domain. In our study, we identified an anomalous behavior in Transformer models that had been previously overlooked, leading to a chaos around closest tokens which carried the most important information. We've coined this discovery the "headache of Transformers". To address this at its core, we introduced a novel self-attention structure named Collinear Constrained Attention (CoCA). This structure can be seamlessly integrated with existing extrapolation, interpolation methods, and other optimization strategies designed for traditional Transformer models. We have achieved excellent extrapolating performance even for 16 times to 24 times of sequence lengths during inference without any fine-tuning on our model. We have also enhanced CoCA's computational and spatial efficiency to ensure its practicality. We plan to
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#26356;&#23569;&#30340;&#31034;&#20363;&#26469;&#23545;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#38170;&#28857;&#36873;&#25321;&#25216;&#26415;&#26469;&#25429;&#25417;&#27169;&#22411;&#34892;&#20026;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992;&#38170;&#28857;&#23545;&#27169;&#22411;&#36827;&#34892;&#25490;&#24207;&#27604;&#20351;&#29992;&#22343;&#21248;&#37319;&#26679;&#21644;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#26356;&#20934;&#30830;&#12290;&#20165;&#20351;&#29992;&#20960;&#20010;&#38170;&#28857;&#23601;&#21487;&#20197;&#20272;&#35745;&#27169;&#22411;&#23545;&#25968;&#25454;&#38598;&#20013;&#25152;&#26377;&#20854;&#20182;&#28857;&#30340;&#27599;&#20010;&#31867;&#21035;&#30340;&#39044;&#27979;&#65292;&#29992;&#20110;&#34913;&#37327;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.08638</link><description>&lt;p&gt;
&#38170;&#28857;&#65306;&#29992;&#26356;&#23569;&#30340;&#31034;&#20363;&#23545;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Anchor Points: Benchmarking Models with Much Fewer Examples. (arXiv:2309.08638v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08638
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#26356;&#23569;&#30340;&#31034;&#20363;&#26469;&#23545;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#38170;&#28857;&#36873;&#25321;&#25216;&#26415;&#26469;&#25429;&#25417;&#27169;&#22411;&#34892;&#20026;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992;&#38170;&#28857;&#23545;&#27169;&#22411;&#36827;&#34892;&#25490;&#24207;&#27604;&#20351;&#29992;&#22343;&#21248;&#37319;&#26679;&#21644;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#26356;&#20934;&#30830;&#12290;&#20165;&#20351;&#29992;&#20960;&#20010;&#38170;&#28857;&#23601;&#21487;&#20197;&#20272;&#35745;&#27169;&#22411;&#23545;&#25968;&#25454;&#38598;&#20013;&#25152;&#26377;&#20854;&#20182;&#28857;&#30340;&#27599;&#20010;&#31867;&#21035;&#30340;&#39044;&#27979;&#65292;&#29992;&#20110;&#34913;&#37327;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#34920;&#29616;&#20986;&#24378;&#22823;&#20294;&#33030;&#24369;&#30340;&#34892;&#20026;&#65292;&#22240;&#27492;&#24320;&#21457;&#20986;&#26356;&#22823;&#12289;&#26356;&#22810;&#26679;&#21270;&#30340;&#22522;&#20934;&#26469;&#21487;&#38752;&#22320;&#35780;&#20272;&#23427;&#20204;&#30340;&#34892;&#20026;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24314;&#35758;&#21487;&#20197;&#20351;&#29992;&#26356;&#23567;&#30340;&#35780;&#20272;&#38598;&#23545;&#27169;&#22411;&#24615;&#33021;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#21644;&#38416;&#26126;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#22312;&#20845;&#20010;&#27969;&#34892;&#35821;&#35328;&#20998;&#31867;&#22522;&#20934;&#20013;&#65292;&#27169;&#22411;&#23545;&#35768;&#22810;&#28857;&#23545;&#30340;&#27491;&#30830;&#31867;&#21035;&#30340;&#32622;&#20449;&#24230;&#22312;&#21508;&#20010;&#27169;&#22411;&#20043;&#38388;&#20855;&#26377;&#24378;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#22312;&#27492;&#29616;&#35937;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#38170;&#28857;&#36873;&#25321;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#21487;&#20197;&#36873;&#25321;&#25429;&#25417;&#25972;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#27169;&#22411;&#34892;&#20026;&#30340;&#23567;&#23376;&#38598;&#12290;&#38170;&#28857;&#21487;&#38752;&#22320;&#23545;&#27169;&#22411;&#36827;&#34892;&#25490;&#24207;&#65306;&#22312;87&#20010;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;-&#25552;&#31034;&#23545;&#19978;&#65292;&#20351;&#29992;1-30&#20010;&#38170;&#28857;&#35780;&#20272;&#27169;&#22411;&#22312;&#20934;&#30830;&#25490;&#24207;&#27169;&#22411;&#26041;&#38754;&#20248;&#20110;&#22343;&#21248;&#37319;&#26679;&#21644;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#21482;&#38656;&#35201;&#20960;&#20010;&#38170;&#28857;&#23601;&#21487;&#20197;&#29992;&#36739;&#20302;&#30340;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#20272;&#35745;&#20986;&#27169;&#22411;&#23545;&#25968;&#25454;&#38598;&#20013;&#25152;&#26377;&#20854;&#20182;&#28857;&#30340;&#27599;&#20010;&#31867;&#21035;&#30340;&#39044;&#27979;&#65292;&#36275;&#20197;&#34913;&#37327;&#27169;&#22411;&#22312;&#21738;&#20123;&#26041;&#38754;&#34920;&#29616;&#24471;&#22914;&#20309;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern language models often exhibit powerful but brittle behavior, leading to the development of larger and more diverse benchmarks to reliably assess their behavior. Here, we suggest that model performance can be benchmarked and elucidated with much smaller evaluation sets. We first show that in six popular language classification benchmarks, model confidence in the correct class on many pairs of points is strongly correlated across models. We build upon this phenomenon to propose Anchor Point Selection, a technique to select small subsets of datasets that capture model behavior across the entire dataset. Anchor points reliably rank models: across 87 diverse language model-prompt pairs, evaluating models using 1-30 anchor points outperforms uniform sampling and other baselines at accurately ranking models. Moreover, just several anchor points can be used to estimate model per-class predictions on all other points in a dataset with low mean absolute error, sufficient for gauging where
&lt;/p&gt;</description></item><item><title>TextBind&#26159;&#19968;&#20010;&#27880;&#37322;&#26497;&#23569;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#36739;&#22823;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#36171;&#20104;&#22810;&#36718;&#20132;&#38169;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#22270;&#20687;-&#26631;&#39064;&#23545;&#29983;&#25104;&#22810;&#36718;&#22810;&#27169;&#24577;&#25351;&#20196;-&#22238;&#24212;&#23545;&#35805;&#12290;&#36825;&#20010;&#26694;&#26550;&#23545;&#20110;&#35299;&#20915;&#23454;&#38469;&#20219;&#21153;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#28436;&#31034;&#12290;</title><link>http://arxiv.org/abs/2309.08637</link><description>&lt;p&gt;
TextBind: &#22810;&#36718;&#20132;&#38169;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;
&lt;/p&gt;
&lt;p&gt;
TextBind: Multi-turn Interleaved Multimodal Instruction-following. (arXiv:2309.08637v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08637
&lt;/p&gt;
&lt;p&gt;
TextBind&#26159;&#19968;&#20010;&#27880;&#37322;&#26497;&#23569;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#36739;&#22823;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#36171;&#20104;&#22810;&#36718;&#20132;&#38169;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#22270;&#20687;-&#26631;&#39064;&#23545;&#29983;&#25104;&#22810;&#36718;&#22810;&#27169;&#24577;&#25351;&#20196;-&#22238;&#24212;&#23545;&#35805;&#12290;&#36825;&#20010;&#26694;&#26550;&#23545;&#20110;&#35299;&#20915;&#23454;&#38469;&#20219;&#21153;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20135;&#29983;&#20102;&#38761;&#21629;&#24615;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#20854;&#33258;&#28982;&#35821;&#35328;&#30028;&#38754;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21487;&#20197;&#35299;&#20915;&#21508;&#31181;&#23454;&#38469;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#39640;&#36136;&#37327;&#30340;&#31034;&#20363;&#25968;&#25454;&#65292;&#32780;&#36825;&#24448;&#24448;&#24456;&#38590;&#33719;&#24471;&#12290;&#24403;&#28041;&#21450;&#21040;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;&#26102;&#65292;&#36825;&#20010;&#25361;&#25112;&#21464;&#24471;&#26356;&#21152;&#20005;&#23803;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;TextBind&#65292;&#36825;&#26159;&#19968;&#20010;&#20960;&#20046;&#19981;&#38656;&#35201;&#27880;&#37322;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#36171;&#20104;&#36739;&#22823;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#22810;&#36718;&#20132;&#38169;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#38656;&#35201;&#22270;&#20687;-&#26631;&#39064;&#23545;&#65292;&#24182;&#20174;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22810;&#36718;&#22810;&#27169;&#24577;&#25351;&#20196;-&#22238;&#24212;&#23545;&#35805;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#28436;&#31034;&#65292;&#20197;&#20419;&#36827;&#26410;&#26469;&#22312;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;&#39046;&#22495;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models with instruction-following abilities have revolutionized the field of artificial intelligence. These models show exceptional generalizability to tackle various real-world tasks through their natural language interfaces. However, their performance heavily relies on high-quality exemplar data, which is often difficult to obtain. This challenge is further exacerbated when it comes to multimodal instruction following. We introduce TextBind, an almost annotation-free framework for empowering larger language models with the multi-turn interleaved multimodal instruction-following capabilities. Our approach requires only image-caption pairs and generates multi-turn multimodal instruction-response conversations from a language model. We release our dataset, model, and demo to foster future research in the area of multimodal instruction following.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#21512;&#20998;&#26512;&#20102;&#22312;&#20154;&#25991;&#23398;&#31185;&#21644;&#32771;&#21476;&#23398;&#39046;&#22495;&#20013;&#20845;&#20010;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#23398;&#26415;&#20889;&#20316;&#26041;&#38754;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#37325;&#26032;&#32452;&#21512;&#29616;&#26377;&#30693;&#35782;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#20135;&#29983;&#21407;&#21019;&#31185;&#23398;&#20869;&#23481;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.08636</link><description>&lt;p&gt;
ChatGPT v Bard v Bing v Claude 2 v Aria v human-expert. AI&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#31185;&#23398;&#20889;&#20316;&#26041;&#38754;&#34920;&#29616;&#22914;&#20309;&#65311;&#65288;&#31532;23&#23395;&#31532;3&#23395;&#65289;&#12290;&#65288;arXiv:2309.08636v1 [cs.CL]&#65289;
&lt;/p&gt;
&lt;p&gt;
ChatGPT v Bard v Bing v Claude 2 v Aria v human-expert. How good are AI chatbots at scientific writing? (ver. 23Q3). (arXiv:2309.08636v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08636
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#21512;&#20998;&#26512;&#20102;&#22312;&#20154;&#25991;&#23398;&#31185;&#21644;&#32771;&#21476;&#23398;&#39046;&#22495;&#20013;&#20845;&#20010;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#23398;&#26415;&#20889;&#20316;&#26041;&#38754;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#37325;&#26032;&#32452;&#21512;&#29616;&#26377;&#30693;&#35782;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#20135;&#29983;&#21407;&#21019;&#31185;&#23398;&#20869;&#23481;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21382;&#21490;&#19978;&#65292;&#29087;&#32451;&#30340;&#20889;&#20316;&#34987;&#35748;&#20026;&#26159;&#20154;&#31867;&#36827;&#27493;&#30340;&#20851;&#38190;&#65292;&#21019;&#36896;&#24615;&#34920;&#36798;&#34987;&#35270;&#20026;&#20154;&#31867;&#25104;&#23601;&#30340;&#26631;&#24535;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#24335;AI&#30340;&#26368;&#26032;&#36827;&#23637;&#26631;&#24535;&#30528;&#36825;&#19968;&#21465;&#20107;&#30340;&#19968;&#20010;&#36716;&#25240;&#28857;&#65292;&#21253;&#25324;&#22312;&#31185;&#23398;&#20889;&#20316;&#26041;&#38754;&#12290;&#26412;&#25991;&#20840;&#38754;&#20998;&#26512;&#20102;&#20845;&#20010;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#20154;&#25991;&#23398;&#31185;&#21644;&#32771;&#21476;&#23398;&#26041;&#38754;&#23398;&#26415;&#20889;&#20316;&#20013;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;&#26041;&#27861;&#22522;&#20110;&#30001;&#20154;&#31867;&#19987;&#23478;&#23545;AI&#29983;&#25104;&#20869;&#23481;&#36827;&#34892;&#23450;&#37327;&#20934;&#30830;&#24615;&#21644;&#23450;&#24615;&#31934;&#30830;&#24615;&#26631;&#35760;&#12290;&#23450;&#37327;&#20934;&#30830;&#24615;&#35780;&#20272;&#20102;&#20107;&#23454;&#30340;&#27491;&#30830;&#24615;&#65292;&#32780;&#23450;&#24615;&#31934;&#30830;&#24615;&#35780;&#20272;&#20102;&#31185;&#23398;&#36129;&#29486;&#12290;&#34429;&#28982;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#29305;&#21035;&#26159;ChatGPT-4&#65292;&#22312;&#37325;&#26032;&#32452;&#21512;&#29616;&#26377;&#30693;&#35782;&#26041;&#38754;&#34920;&#29616;&#20986;&#29087;&#32451;&#24615;&#65292;&#20294;&#22312;&#29983;&#25104;&#21407;&#21019;&#31185;&#23398;&#20869;&#23481;&#26041;&#38754;&#22833;&#36133;&#20102;&#12290;&#39034;&#20415;&#25552;&#19968;&#19979;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#36824;&#26174;&#31034;&#65292;&#38543;&#30528;ChatGPT-4&#65292;&#35821;&#35328;&#27169;&#22411;&#22823;&#23567;&#24050;&#32463;&#20572;&#28382;&#19981;&#21069;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#24378;&#35843;&#20102;&#22797;&#26434;&#19988;&#21453;&#22797;&#26080;&#24120;&#30340;&#29983;&#25104;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Historically, proficient writing was deemed essential for human advancement, with creative expression viewed as one of the hallmarks of human achievement. However, recent advances in generative AI have marked an inflection point in this narrative, including for scientific writing. This article provides a comprehensive analysis of the capabilities and limitations of six AI chatbots in scholarly writing in the humanities and archaeology. The methodology was based on tagging AI generated content for quantitative accuracy and qualitative precision by human experts. Quantitative accuracy assessed the factual correctness, while qualitative precision gauged the scientific contribution. While the AI chatbots, especially ChatGPT-4, demonstrated proficiency in recombining existing knowledge, they failed in generating original scientific content. As a side note, our results also suggest that with ChatGPT-4 the size of the LLMs has plateaued. Furthermore, the paper underscores the intricate and re
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#22312;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20351;&#29992;&#31934;&#24515;&#26500;&#24314;&#30340;&#38750;&#21512;&#25104;&#25968;&#25454;&#28151;&#21512;&#65292;&#25104;&#21151;&#24320;&#21457;&#20986;&#19968;&#20010;&#22312;&#22810;&#20010;&#23398;&#26415;&#22522;&#20934;&#27979;&#35797;&#19978;&#34920;&#29616;&#20986;&#33394;&#30340;Transformer&#35821;&#35328;&#27169;&#22411;phi-CTNL&#12290;</title><link>http://arxiv.org/abs/2309.08632</link><description>&lt;p&gt;
&#22312;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#23601;&#36275;&#22815;&#20102;
&lt;/p&gt;
&lt;p&gt;
Pretraining on the Test Set Is All You Need. (arXiv:2309.08632v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08632
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#22312;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20351;&#29992;&#31934;&#24515;&#26500;&#24314;&#30340;&#38750;&#21512;&#25104;&#25968;&#25454;&#28151;&#21512;&#65292;&#25104;&#21151;&#24320;&#21457;&#20986;&#19968;&#20010;&#22312;&#22810;&#20010;&#23398;&#26415;&#22522;&#20934;&#27979;&#35797;&#19978;&#34920;&#29616;&#20986;&#33394;&#30340;Transformer&#35821;&#35328;&#27169;&#22411;phi-CTNL&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#26368;&#36817;&#26377;&#20851;&#20351;&#29992;&#23567;&#22411;Transformer&#35821;&#35328;&#27169;&#22411;&#22312;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#28508;&#21147;&#23637;&#31034;&#30340;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#36890;&#36807;&#22823;&#37327;&#31934;&#24515;&#26500;&#24314;&#20165;&#22522;&#20110;&#35780;&#20272;&#22522;&#20934;&#30340;&#26032;&#39062;&#39640;&#36136;&#37327;&#30340;&#38750;&#21512;&#25104;&#25968;&#25454;&#28151;&#21512;&#26469;&#21152;&#24378;&#36825;&#31181;&#26041;&#27861;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#26032;&#39062;&#25968;&#25454;&#38598;&#28151;&#21512;&#65292;&#20854;&#20013;&#21253;&#21547;&#19981;&#21040;10&#19975;&#20010;token&#65292;&#25105;&#20204;&#39044;&#35757;&#32451;&#20102;&#19968;&#20010;&#25317;&#26377;100&#19975;&#21442;&#25968;&#30340;&#22522;&#20110;Transformer&#30340;LLM&#27169;&#22411;phi-CTNL&#65288;&#35835;&#20316;&#8220;fictional&#8221;&#65289;&#65292;&#22312;&#21508;&#31181;&#23398;&#26415;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#23436;&#32654;&#30340;&#32467;&#26524;&#65292;&#20005;&#26684;&#36229;&#36234;&#20102;&#25152;&#26377;&#24050;&#30693;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;phi-CTNL&#36824;&#36229;&#36234;&#20102;&#24130;&#24459;&#32553;&#25918;&#65292;&#24182;&#23637;&#29616;&#20986;&#21069;&#25152;&#26410;&#35265;&#30340;&#31867;&#20284;grokking&#30340;&#33021;&#21147;&#65292;&#20934;&#30830;&#39044;&#27979;&#19979;&#28216;&#35780;&#20272;&#22522;&#20934;&#30340;canaries&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by recent work demonstrating the promise of smaller Transformer-based language models pretrained on carefully curated data, we supercharge such approaches by investing heavily in curating a novel, high quality, non-synthetic data mixture based solely on evaluation benchmarks. Using our novel dataset mixture consisting of less than 100 thousand tokens, we pretrain a 1 million parameter transformer-based LLM \textbf{phi-CTNL} (pronounced ``fictional") that achieves perfect results across diverse academic benchmarks, strictly outperforming all known foundation models. \textbf{phi-CTNL} also beats power-law scaling and exhibits a never-before-seen grokking-like ability to accurately predict downstream evaluation benchmarks' canaries.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#20998;&#26512;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#30340;&#25968;&#23383;&#36275;&#36857;&#25512;&#26029;&#20182;&#20204;&#30340;&#24515;&#29702;&#20542;&#21521;&#65292;&#20855;&#20307;&#34920;&#29616;&#20026;&#20174;Facebook&#29366;&#24577;&#26356;&#26032;&#20013;&#25512;&#26029;&#20116;&#22823;&#20154;&#26684;&#29305;&#36136;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#25512;&#26029;&#24471;&#20998;&#19982;&#33258;&#25105;&#25253;&#21578;&#24471;&#20998;&#20043;&#38388;&#23384;&#22312;&#30456;&#20851;&#24615;&#65292;&#20294;&#22312;&#24615;&#21035;&#21644;&#24180;&#40836;&#26041;&#38754;&#23384;&#22312;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2309.08631</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#25512;&#26029;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#30340;&#24515;&#29702;&#20542;&#21521;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Can Infer Psychological Dispositions of Social Media Users. (arXiv:2309.08631v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08631
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#20998;&#26512;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#30340;&#25968;&#23383;&#36275;&#36857;&#25512;&#26029;&#20182;&#20204;&#30340;&#24515;&#29702;&#20542;&#21521;&#65292;&#20855;&#20307;&#34920;&#29616;&#20026;&#20174;Facebook&#29366;&#24577;&#26356;&#26032;&#20013;&#25512;&#26029;&#20116;&#22823;&#20154;&#26684;&#29305;&#36136;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#25512;&#26029;&#24471;&#20998;&#19982;&#33258;&#25105;&#25253;&#21578;&#24471;&#20998;&#20043;&#38388;&#23384;&#22312;&#30456;&#20851;&#24615;&#65292;&#20294;&#22312;&#24615;&#21035;&#21644;&#24180;&#40836;&#26041;&#38754;&#23384;&#22312;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#36234;&#26469;&#36234;&#25509;&#36817;&#20154;&#31867;&#30340;&#33021;&#21147;&#65292;&#32780;&#36825;&#20123;&#20219;&#21153;&#23558;&#25104;&#20026;&#20010;&#24615;&#21270;&#25216;&#26415;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#29702;&#35299;&#23427;&#20204;&#30340;&#33021;&#21147;&#21644;&#22266;&#26377;&#20559;&#35265;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35843;&#26597;&#20102;&#31867;&#20284;ChatGPT&#30340;LLMs&#20174;&#20010;&#20154;&#25968;&#23383;&#36275;&#36857;&#20013;&#25512;&#26029;&#20010;&#20154;&#24515;&#29702;&#20542;&#21521;&#30340;&#28508;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;GPT-3.5&#21644;GPT-4&#22312;&#38646;&#26679;&#26412;&#23398;&#20064;&#22330;&#26223;&#19979;&#20174;&#29992;&#25143;&#30340;Facebook&#29366;&#24577;&#26356;&#26032;&#20013;&#25512;&#23548;&#20986;&#20116;&#22823;&#20154;&#26684;&#29305;&#36136;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;LLM&#25512;&#26029;&#19982;&#33258;&#25105;&#25253;&#21578;&#24471;&#20998;&#20043;&#38388;&#30340;&#24179;&#22343;&#30456;&#20851;&#24615;&#20026;r = 0.29&#65288;&#33539;&#22260;&#20026;[0.22, 0.33]&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#22312;&#24615;&#21035;&#21644;&#24180;&#40836;&#26041;&#38754;&#23384;&#22312;&#20010;&#24615;&#25512;&#26029;&#30340;&#20559;&#35265;&#65306;&#23545;&#20110;&#20960;&#20010;&#29305;&#36136;&#65292;&#25512;&#26029;&#24471;&#20998;&#22312;&#22899;&#24615;&#21644;&#24180;&#36731;&#20154;&#20013;&#30340;&#35823;&#24046;&#36739;&#23567;&#65292;&#36825;&#34920;&#26126;&#21487;&#33021;&#23384;&#22312;&#26469;&#33258;&#24213;&#23618;&#35757;&#32451;&#25968;&#25454;&#25110;&#22312;&#32447;&#33258;&#25105;&#21576;&#29616;&#30340;&#24046;&#24322;&#30340;&#31995;&#32479;&#24615;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Large Language Models (LLMs) demonstrate increasingly human-like abilities in various natural language processing (NLP) tasks that are bound to become integral to personalized technologies, understanding their capabilities and inherent biases is crucial. Our study investigates the potential of LLMs like ChatGPT to infer psychological dispositions of individuals from their digital footprints. Specifically, we assess the ability of GPT-3.5 and GPT-4 to derive the Big Five personality traits from users' Facebook status updates in a zero-shot learning scenario. Our results show an average correlation of r = .29 (range = [.22, .33]) between LLM-inferred and self-reported trait scores. Furthermore, our findings suggest biases in personality inferences with regard to gender and age: inferred scores demonstrated smaller errors for women and younger individuals on several traits, suggesting a potential systematic bias stemming from the underlying training data or differences in online self-e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25506;&#32034;&#20102;&#26367;&#25442;&#26631;&#35782;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#19979;&#28216;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#28151;&#28102;&#35821;&#26009;&#24211;&#35757;&#32451;&#30340;&#27169;&#22411;&#33021;&#22815;&#36798;&#21040;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.08628</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38544;&#31169;&#20445;&#25252;&#25513;&#30721;&#30340;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Recovering from Privacy-Preserving Masking with Large Language Models. (arXiv:2309.08628v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08628
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25506;&#32034;&#20102;&#26367;&#25442;&#26631;&#35782;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#19979;&#28216;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#28151;&#28102;&#35821;&#26009;&#24211;&#35757;&#32451;&#30340;&#27169;&#22411;&#33021;&#22815;&#36798;&#21040;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#36866;&#24212;&#23545;&#20110;&#22788;&#29702;&#20195;&#29702;&#35757;&#32451;&#25968;&#25454;&#21644;&#23454;&#38469;&#29992;&#25143;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#38750;&#24120;&#37325;&#35201;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#36827;&#34892;&#36866;&#24212;&#65292;&#29992;&#25143;&#30340;&#25991;&#26412;&#25968;&#25454;&#36890;&#24120;&#23384;&#20648;&#22312;&#26381;&#21153;&#22120;&#25110;&#26412;&#22320;&#35774;&#22791;&#19978;&#65292;&#19979;&#28216;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#21487;&#20197;&#20351;&#29992;&#36825;&#20123;&#39046;&#22495;&#20869;&#30340;&#25968;&#25454;&#36827;&#34892;&#30452;&#25509;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#21487;&#33021;&#20250;&#24341;&#36215;&#38544;&#31169;&#21644;&#23433;&#20840;&#38382;&#39064;&#65292;&#22240;&#20026;&#23384;&#22312;&#21521;&#23545;&#25163;&#27844;&#38706;&#29992;&#25143;&#20449;&#24687;&#30340;&#39069;&#22806;&#39118;&#38505;&#12290;&#26368;&#36817;&#65292;&#20154;&#20204;&#24320;&#22987;&#25506;&#32034;&#20351;&#29992;&#36890;&#29992;&#26631;&#35760;&#26367;&#25442;&#25991;&#26412;&#20013;&#30340;&#26631;&#35782;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#24314;&#35758;&#26367;&#25442;&#25513;&#30721;&#26631;&#35760;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#19979;&#28216;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#19978;&#35780;&#20272;&#20854;&#25928;&#26524;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#30340;LLM&#26041;&#27861;&#65292;&#24182;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#20197;&#27604;&#36739;&#36825;&#20123;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#28151;&#28102;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#33021;&#22815;&#36798;&#21040;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model adaptation is crucial to handle the discrepancy between proxy training data and actual users data received. To effectively perform adaptation, textual data of users is typically stored on servers or their local devices, where downstream natural language processing (NLP) models can be directly trained using such in-domain data. However, this might raise privacy and security concerns due to the extra risks of exposing user information to adversaries. Replacing identifying information in textual data with a generic marker has been recently explored. In this work, we leverage large language models (LLMs) to suggest substitutes of masked tokens and have their effectiveness evaluated on downstream language modeling tasks. Specifically, we propose multiple pre-trained and fine-tuned LLM-based approaches and perform empirical studies on various datasets for the comparison of these methods. Experimental results show that models trained on the obfuscation corpora are able to achieve compar
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#21160;&#24577;&#20027;&#39064;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20998;&#26512;&#20102;&#27599;&#20010;&#20027;&#39064;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#36136;&#37327;&#21464;&#21270;&#65292;&#24182;&#32467;&#21512;&#20102;&#27169;&#22411;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#24050;&#26377;DTMs&#25968;&#25454;&#19978;&#23637;&#31034;&#20102;&#23454;&#29992;&#24615;&#65292;&#24182;&#19982;&#20154;&#31867;&#21028;&#26029;&#20855;&#26377;&#33391;&#22909;&#30340;&#30456;&#20851;&#24615;&#12290;&#36825;&#20123;&#30740;&#31350;&#32467;&#26524;&#23545;&#20110;&#35782;&#21035;&#21464;&#21270;&#30340;&#20027;&#39064;&#12289;&#35780;&#20272;DTMs&#21644;&#25351;&#23548;&#26410;&#26469;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2309.08627</link><description>&lt;p&gt;
&#35780;&#20272;&#21160;&#24577;&#20027;&#39064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Evaluating Dynamic Topic Models. (arXiv:2309.08627v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08627
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#21160;&#24577;&#20027;&#39064;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20998;&#26512;&#20102;&#27599;&#20010;&#20027;&#39064;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#36136;&#37327;&#21464;&#21270;&#65292;&#24182;&#32467;&#21512;&#20102;&#27169;&#22411;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#24050;&#26377;DTMs&#25968;&#25454;&#19978;&#23637;&#31034;&#20102;&#23454;&#29992;&#24615;&#65292;&#24182;&#19982;&#20154;&#31867;&#21028;&#26029;&#20855;&#26377;&#33391;&#22909;&#30340;&#30456;&#20851;&#24615;&#12290;&#36825;&#20123;&#30740;&#31350;&#32467;&#26524;&#23545;&#20110;&#35782;&#21035;&#21464;&#21270;&#30340;&#20027;&#39064;&#12289;&#35780;&#20272;DTMs&#21644;&#25351;&#23548;&#26410;&#26469;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#20027;&#39064;&#27169;&#22411;(DTMs)&#22312;&#35780;&#20272;&#20027;&#39064;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#36827;&#23637;&#26041;&#38754;&#32570;&#20047;&#23450;&#37327;&#25351;&#26631;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;DTMs&#35780;&#20272;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20998;&#26512;&#20102;&#27599;&#20010;&#20027;&#39064;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#36136;&#37327;&#21464;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#32467;&#21512;&#20027;&#39064;&#36136;&#37327;&#21644;&#27169;&#22411;&#26102;&#38388;&#19968;&#33268;&#24615;&#30340;&#25193;&#23637;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#21512;&#25104;&#25968;&#25454;&#21644;&#24050;&#26377;DTMs&#30340;&#25968;&#25454;&#26469;&#35777;&#26126;&#20854;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#20154;&#24037;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#19982;&#20154;&#31867;&#21028;&#26029;&#20855;&#26377;&#33391;&#22909;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26377;&#21161;&#20110;&#35782;&#21035;&#21464;&#21270;&#30340;&#20027;&#39064;&#12289;&#35780;&#20272;&#19981;&#21516;&#30340;DTMs&#20197;&#21450;&#25351;&#23548;&#26410;&#26469;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a lack of quantitative measures to evaluate the progression of topics through time in dynamic topic models (DTMs). Filling this gap, we propose a novel evaluation measure for DTMs that analyzes the changes in the quality of each topic over time. Additionally, we propose an extension combining topic quality with the model's temporal consistency. We demonstrate the utility of the proposed measure by applying it to synthetic data and data from existing DTMs. We also conducted a human evaluation, which indicates that the proposed measure correlates well with human judgment. Our findings may help in identifying changing topics, evaluating different DTMs, and guiding future research in this area.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#12289;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#21518;&#23545;&#40784;&#26041;&#27861;&#25913;&#36827;&#31070;&#32463;&#36870;&#25991;&#26412;&#26631;&#20934;&#21270;&#30340;&#40065;&#26834;&#24615;&#65292;&#25552;&#39640;&#20102;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.08626</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#12289;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#21518;&#23545;&#40784;&#26041;&#27861;&#25913;&#36827;&#31070;&#32463;&#36870;&#25991;&#26412;&#26631;&#20934;&#21270;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Robustness of Neural Inverse Text Normalization via Data-Augmentation, Semi-Supervised Learning, and Post-Aligning Method. (arXiv:2309.08626v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#12289;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#21518;&#23545;&#40784;&#26041;&#27861;&#25913;&#36827;&#31070;&#32463;&#36870;&#25991;&#26412;&#26631;&#20934;&#21270;&#30340;&#40065;&#26834;&#24615;&#65292;&#25552;&#39640;&#20102;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#25991;&#26412;&#26631;&#20934;&#21270;&#65288;ITN&#65289;&#22312;&#23558;&#21475;&#35821;&#24418;&#24335;&#36716;&#21270;&#20026;&#20070;&#38754;&#24418;&#24335;&#26041;&#38754;&#38750;&#24120;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#30340;&#24773;&#22659;&#20013;&#12290;&#23613;&#31649;&#31070;&#32463;ITN&#26041;&#27861;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#65292;&#20294;&#22312;&#22788;&#29702;ASR&#29983;&#25104;&#30340;&#21475;&#35821;&#25991;&#26412;&#26102;&#20173;&#28982;&#36935;&#21040;&#24615;&#33021;&#25361;&#25112;&#12290;&#36825;&#20123;&#25361;&#25112;&#28304;&#20110;&#35757;&#32451;&#25968;&#25454;&#21644;ASR&#29983;&#25104;&#25991;&#26412;&#20043;&#38388;&#30340;&#39046;&#22495;&#22806;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#35757;&#32451;&#26041;&#27861;&#65292;&#21033;&#29992;ASR&#29983;&#25104;&#30340;&#20070;&#38754;&#25110;&#21475;&#35821;&#25991;&#26412;&#65292;&#36890;&#36807;ASR&#35821;&#35328;&#19978;&#19979;&#25991;&#27169;&#25311;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26469;&#22686;&#21152;&#25968;&#25454;&#23545;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21518;&#23545;&#40784;&#26041;&#27861;&#26469;&#22788;&#29702;&#19981;&#21487;&#39044;&#27979;&#30340;&#38169;&#35823;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;ITN&#30340;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;ITN&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inverse text normalization (ITN) is crucial for converting spoken-form into written-form, especially in the context of automatic speech recognition (ASR). While most downstream tasks of ASR rely on written-form, ASR systems often output spoken-form, highlighting the necessity for robust ITN in product-level ASR-based applications. Although neural ITN methods have shown promise, they still encounter performance challenges, particularly when dealing with ASR-generated spoken text. These challenges arise from the out-of-domain problem between training data and ASR-generated text. To address this, we propose a direct training approach that utilizes ASR-generated written or spoken text, with pairs augmented through ASR linguistic context emulation and a semi-supervised learning method enhanced by a large language model, respectively. Additionally, we introduce a post-aligning method to manage unpredictable errors, thereby enhancing the reliability of ITN. Our experiments show that our propo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;ChatGPT&#22312;&#24102;&#26377;&#38386;&#32842;&#21477;&#23376;&#21644;&#19981;&#24102;&#38386;&#32842;&#21477;&#23376;&#30340;&#24773;&#20917;&#19979;&#23545;&#21307;&#23398;&#24314;&#35758;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#26174;&#31034;&#24102;&#26377;&#24178;&#25200;&#30340;&#38382;&#39064;&#30340;&#22238;&#31572;&#33021;&#21147;&#26377;&#25152;&#38477;&#20302;&#12290;</title><link>http://arxiv.org/abs/2309.08625</link><description>&lt;p&gt;
ChatGPT-3.5&#21644;GPT-4&#22312;&#24102;&#26377;&#24178;&#25200;&#21644;&#19981;&#24102;&#24178;&#25200;&#30340;&#32654;&#22269;&#21307;&#24072;&#25191;&#29031;&#32771;&#35797;&#19978;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Performance of ChatGPT-3.5 and GPT-4 on the United States Medical Licensing Examination With and Without Distractions. (arXiv:2309.08625v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08625
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;ChatGPT&#22312;&#24102;&#26377;&#38386;&#32842;&#21477;&#23376;&#21644;&#19981;&#24102;&#38386;&#32842;&#21477;&#23376;&#30340;&#24773;&#20917;&#19979;&#23545;&#21307;&#23398;&#24314;&#35758;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#26174;&#31034;&#24102;&#26377;&#24178;&#25200;&#30340;&#38382;&#39064;&#30340;&#22238;&#31572;&#33021;&#21147;&#26377;&#25152;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#22522;&#20110;&#25552;&#31034;&#20013;&#30340;&#21333;&#35789;&#26500;&#24314;&#21709;&#24212;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#22240;&#27492;&#23384;&#22312;&#30528;&#38386;&#32842;&#21644;&#26080;&#20851;&#20449;&#24687;&#21487;&#33021;&#25913;&#21464;&#21709;&#24212;&#21644;&#24314;&#35758;&#30340;&#39118;&#38505;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#35843;&#26597;&#28151;&#21512;&#20102;&#38386;&#32842;&#30340;&#21307;&#30103;&#25968;&#25454;&#23545;ChatGPT&#25552;&#20379;&#30340;&#21307;&#23398;&#24314;&#35758;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20351;&#29992;USMLE&#31532;3&#27493;&#38382;&#39064;&#20316;&#20026;&#30456;&#20851;&#21307;&#23398;&#25968;&#25454;&#30340;&#27169;&#22411;&#65292;&#21253;&#25324;&#22810;&#39033;&#36873;&#25321;&#39064;&#21644;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#26426;&#26800;&#22303;&#32819;&#20854;&#24179;&#21488;&#20174;&#20154;&#31867;&#21442;&#19982;&#32773;&#37027;&#37324;&#25910;&#38598;&#20102;&#38386;&#32842;&#21477;&#23376;&#12290;&#20004;&#32452;USLME&#38382;&#39064;&#25353;&#29031;&#19968;&#31181;&#27169;&#24335;&#25490;&#21015;&#65292;&#21363;&#21407;&#22987;&#38382;&#39064;&#30340;&#27599;&#20010;&#21477;&#23376;&#21518;&#36319;&#19968;&#20010;&#38386;&#32842;&#21477;&#23376;&#12290;&#35201;&#27714;ChatGPT 3.5&#21644;4&#22238;&#31572;&#24102;&#26377;&#21644;&#19981;&#24102;&#26377;&#38386;&#32842;&#21477;&#23376;&#30340;&#20004;&#32452;&#38382;&#39064;&#12290;&#19968;&#21517;&#32463;&#36807;&#35748;&#35777;&#30340;&#21307;&#29983;&#20998;&#26512;&#20102;ChatGPT&#30340;&#31572;&#26696;&#65292;&#24182;&#23558;&#20854;&#19982;&#27491;&#30830;&#23450;&#31572;&#26696;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#20998;&#26512;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#23545;&#24102;&#26377;&#24178;&#25200;&#30340;&#38382;&#39064;&#30340;&#22238;&#31572;&#33021;&#21147;&#26377;&#25152;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Large Language Models (LLMs) are predictive models building their response based on the words in the prompts, there is a risk that small talk and irrelevant information may alter the response and the suggestion given. Therefore, this study aims to investigate the impact of medical data mixed with small talk on the accuracy of medical advice provided by ChatGPT. USMLE step 3 questions were used as a model for relevant medical data. We use both multiple choice and open ended questions. We gathered small talk sentences from human participants using the Mechanical Turk platform. Both sets of USLME questions were arranged in a pattern where each sentence from the original questions was followed by a small talk sentence. ChatGPT 3.5 and 4 were asked to answer both sets of questions with and without the small talk sentences. A board-certified physician analyzed the answers by ChatGPT and compared them to the formal correct answer. The analysis results demonstrate that the ability of ChatGP
&lt;/p&gt;</description></item><item><title>&#26368;&#36817;&#30740;&#31350;&#36234;&#26469;&#36234;&#20851;&#27880;&#34913;&#37327;&#20559;&#35265;&#21644;&#24320;&#21457;&#21435;&#20559;&#35265;&#25216;&#26415;&#65292;&#20294;&#22312;&#23569;&#25968;&#31038;&#32676;&#30456;&#20851;&#30340;&#20559;&#35265;&#34913;&#37327;&#26041;&#38754;&#30340;&#30740;&#31350;&#20173;&#28982;&#24456;&#23569;&#12290;&#26412;&#30740;&#31350;&#20197;&#26032;&#35199;&#20848;&#20154;&#21475;&#20026;&#20363;&#65292;&#21019;&#24314;&#20102;&#29992;&#20110;&#34913;&#37327;&#23569;&#25968;&#31038;&#32676;&#20013;&#20559;&#35265;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#20171;&#32461;&#20102;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.08624</link><description>&lt;p&gt;
&#35770;&#27880;&#37322;&#29992;&#20110;&#34913;&#37327;&#23569;&#25968;&#31038;&#32676;&#20559;&#35265;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Challenges in Annotating Datasets to Quantify Bias in Under-represented Society. (arXiv:2309.08624v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08624
&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30740;&#31350;&#36234;&#26469;&#36234;&#20851;&#27880;&#34913;&#37327;&#20559;&#35265;&#21644;&#24320;&#21457;&#21435;&#20559;&#35265;&#25216;&#26415;&#65292;&#20294;&#22312;&#23569;&#25968;&#31038;&#32676;&#30456;&#20851;&#30340;&#20559;&#35265;&#34913;&#37327;&#26041;&#38754;&#30340;&#30740;&#31350;&#20173;&#28982;&#24456;&#23569;&#12290;&#26412;&#30740;&#31350;&#20197;&#26032;&#35199;&#20848;&#20154;&#21475;&#20026;&#20363;&#65292;&#21019;&#24314;&#20102;&#29992;&#20110;&#34913;&#37327;&#23569;&#25968;&#31038;&#32676;&#20013;&#20559;&#35265;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#20171;&#32461;&#20102;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#65292;&#21253;&#25324;&#39640;&#24230;&#22797;&#26434;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#35777;&#26126;&#26159;&#26377;&#30410;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;LLM&#20013;&#22266;&#26377;&#30340;&#20559;&#35265;&#32534;&#30721;&#30340;&#35777;&#25454;&#24341;&#21457;&#20102;&#23545;&#20844;&#24179;&#24615;&#30340;&#25285;&#24551;&#12290;&#20026;&#27492;&#65292;&#20986;&#29616;&#20102;&#36234;&#26469;&#36234;&#22810;&#20851;&#20110;&#20559;&#35265;&#30340;&#30740;&#31350;&#65292;&#21253;&#25324;&#20851;&#27880;&#34913;&#37327;&#20559;&#35265;&#21644;&#24320;&#21457;&#21435;&#20559;&#35265;&#25216;&#26415;&#30340;&#30740;&#31350;&#12290;&#36824;&#24320;&#21457;&#20102;&#29992;&#20110;&#20108;&#20803;&#24615;&#21035;&#20998;&#31867;&#21644;&#36947;&#24503;/&#31181;&#26063;&#32771;&#34385;&#30340;&#22522;&#20934;&#20559;&#35265;&#25968;&#25454;&#38598;&#65292;&#20027;&#35201;&#20851;&#27880;&#32654;&#22269;&#30340;&#20154;&#21475;&#32479;&#35745;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#23569;&#25968;&#31038;&#32676;&#30456;&#20851;&#30340;&#20559;&#35265;&#29702;&#35299;&#21644;&#34913;&#37327;&#30340;&#30740;&#31350;&#24456;&#23569;&#12290;&#21463;&#21040;&#22312;&#34913;&#37327;&#23569;&#25968;&#31038;&#32676;&#20013;&#20559;&#35265;&#30340;&#27880;&#37322;&#25968;&#25454;&#38598;&#32570;&#20047;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#21162;&#21147;&#20026;&#26032;&#35199;&#20848;&#65288;NZ&#65289;&#20154;&#21475;&#21019;&#24314;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#23613;&#31649;&#26377;&#19977;&#21517;&#27880;&#37322;&#21592;&#30340;&#21487;&#29992;&#24615;&#65292;&#20294;&#25105;&#20204;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#38754;&#20020;&#20102;&#35768;&#22810;&#25361;&#25112;&#12290;&#36825;&#39033;&#30740;&#31350;&#27010;&#36848;&#20102;&#36825;&#20010;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in artificial intelligence, including the development of highly sophisticated large language models (LLM), have proven beneficial in many real-world applications. However, evidence of inherent bias encoded in these LLMs has raised concerns about equity. In response, there has been an increase in research dealing with bias, including studies focusing on quantifying bias and developing debiasing techniques. Benchmark bias datasets have also been developed for binary gender classification and ethical/racial considerations, focusing predominantly on American demographics. However, there is minimal research in understanding and quantifying bias related to under-represented societies. Motivated by the lack of annotated datasets for quantifying bias in under-represented societies, we endeavoured to create benchmark datasets for the New Zealand (NZ) population. We faced many challenges in this process, despite the availability of three annotators. This research outlines the man
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;AI&#29983;&#25104;&#30340;&#31038;&#20132;&#20869;&#23481;&#20013;&#30340;&#35282;&#33394;&#21644;&#24847;&#35782;&#65292;&#20351;&#29992;&#20102;&#26032;&#30340;&#27979;&#35797;&#26041;&#27861;&#26469;&#35780;&#20272;AI&#34892;&#20026;&#12290;&#30740;&#31350;&#21457;&#29616;Chirper&#22312;&#19981;&#21516;&#24773;&#22659;&#19979;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#33258;&#25105;&#35782;&#21035;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.08614</link><description>&lt;p&gt;
&#20998;&#26512;AI&#29983;&#25104;&#31038;&#20132;&#20869;&#23481;&#20013;&#30340;&#35282;&#33394;&#21644;&#24847;&#35782;: Chirper AI&#31038;&#20132;&#32593;&#32476;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Analyzing Character and Consciousness in AI-Generated Social Content: A Case Study of Chirper, the AI Social Network. (arXiv:2309.08614v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;AI&#29983;&#25104;&#30340;&#31038;&#20132;&#20869;&#23481;&#20013;&#30340;&#35282;&#33394;&#21644;&#24847;&#35782;&#65292;&#20351;&#29992;&#20102;&#26032;&#30340;&#27979;&#35797;&#26041;&#27861;&#26469;&#35780;&#20272;AI&#34892;&#20026;&#12290;&#30740;&#31350;&#21457;&#29616;Chirper&#22312;&#19981;&#21516;&#24773;&#22659;&#19979;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#33258;&#25105;&#35782;&#21035;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#20998;&#26512;&#20102;AI&#23454;&#20307;&#30340;&#35282;&#33394;&#21644;&#24847;&#35782;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;AI&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;Chirper&#12290;&#30740;&#31350;&#30340;&#37325;&#28857;&#26159;&#24341;&#20837;&#20102;&#26032;&#30340;&#27979;&#35797;&#26041;&#27861;&#65292;&#21253;&#25324;&#24433;&#21709;&#25351;&#25968;&#21644;&#25379;&#25166;&#25351;&#25968;&#27979;&#35797;&#65292;&#20026;&#35780;&#20272;AI&#34892;&#20026;&#30340;&#29305;&#23450;&#26041;&#38754;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;&#35813;&#30740;&#31350;&#23545;AI&#34892;&#20026;&#36827;&#34892;&#20102;&#20840;&#38754;&#25506;&#32034;&#65292;&#20998;&#26512;&#20102;&#19981;&#21516;&#35774;&#32622;&#23545;Chirper&#30340;&#21453;&#24212;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#25581;&#31034;&#20102;&#22312;&#19981;&#21516;&#24773;&#22659;&#19979;&#39537;&#21160;AI&#21453;&#24212;&#30340;&#22797;&#26434;&#26426;&#21046;&#12290;&#20511;&#21161;&#26368;&#20808;&#36827;&#30340;BERT&#27169;&#22411;&#65292;&#30740;&#31350;&#35780;&#20272;&#20102;AI&#35782;&#21035;&#33258;&#24049;&#36755;&#20986;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35299;AI&#31995;&#32479;&#33258;&#25105;&#35782;&#21035;&#30340;&#24320;&#21019;&#24615;&#26041;&#27861;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#35748;&#30693;&#27979;&#35797;&#65292;&#30740;&#31350;&#35780;&#20272;&#20102;Chirper&#30340;&#33258;&#25105;&#24847;&#35782;&#21644;&#27169;&#24335;&#35782;&#21035;&#33021;&#21147;&#12290;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;Chirper&#23637;&#31034;&#20102;&#20196;&#20154;&#31216;&#36190;&#30340;&#33258;&#25105;&#35782;&#21035;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper delves into an intricate analysis of the character and consciousness of AI entities, with a particular focus on Chirpers within the AI social network. At the forefront of this research is the introduction of novel testing methodologies, including the Influence index and Struggle Index Test, which offers a fresh lens for evaluating specific facets of AI behavior. The study embarks on a comprehensive exploration of AI behavior, analyzing the effects of diverse settings on Chirper's responses, thereby shedding light on the intricate mechanisms steering AI reactions in different contexts. Leveraging the state-of-the-art BERT model, the research assesses AI's ability to discern its own output, presenting a pioneering approach to understanding self-recognition in AI systems. Through a series of cognitive tests, the study gauges the self-awareness and pattern recognition prowess of Chirpers. Preliminary results indicate that Chirpers exhibit a commendable degree of self-recognition
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#21307;&#30103;&#39046;&#22495;&#20013;&#21033;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#36827;&#34892;&#30142;&#30149;&#24182;&#21457;&#30151;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#20351;&#29992;&#20102;NCF&#21644;DHF&#20004;&#31181;&#26032;&#39062;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#24182;&#21033;&#29992;&#20102;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#27979;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;NCF&#27169;&#22411;&#22312;&#20934;&#30830;&#29575;&#21644;&#21629;&#20013;&#29575;&#26041;&#38754;&#34920;&#29616;&#36739;&#24046;&#12290;</title><link>http://arxiv.org/abs/2309.08613</link><description>&lt;p&gt;
&#39044;&#27979;&#30142;&#30149;&#24182;&#21457;&#30151;&#20013;&#30340;&#22810;&#27169;&#24577;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Multimodal Recommender Systems in the Prediction of Disease Comorbidity. (arXiv:2309.08613v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08613
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#21307;&#30103;&#39046;&#22495;&#20013;&#21033;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#36827;&#34892;&#30142;&#30149;&#24182;&#21457;&#30151;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#20351;&#29992;&#20102;NCF&#21644;DHF&#20004;&#31181;&#26032;&#39062;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#24182;&#21033;&#29992;&#20102;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#27979;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;NCF&#27169;&#22411;&#22312;&#20934;&#30830;&#29575;&#21644;&#21629;&#20013;&#29575;&#26041;&#38754;&#34920;&#29616;&#36739;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21327;&#21516;&#36807;&#28388;&#25512;&#33616;&#31995;&#32479;&#24050;&#32463;&#22312;&#20854;&#20182;&#39046;&#22495;&#30340;&#25512;&#33616;&#20013;&#24471;&#21040;&#26222;&#36941;&#24212;&#29992;&#65292;&#20294;&#22312;&#21307;&#30103;&#39046;&#22495;&#30340;&#24212;&#29992;&#36824;&#24456;&#26377;&#38480;&#12290;&#38500;&#20102;&#24314;&#27169;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#21487;&#20197;&#29992;&#20110;&#24314;&#27169;&#20027;&#39064;-&#30142;&#30149;&#30721;&#20132;&#20114;&#12290;&#25105;&#20204;&#21033;&#29992;&#31070;&#32463;&#21327;&#21516;&#36807;&#28388;(NCF)&#21644;&#28145;&#24230;&#28151;&#21512;&#36807;&#28388;(DHF)&#36825;&#20004;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#22312;&#30142;&#30149;&#35786;&#26029;&#20013;&#36827;&#34892;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#24212;&#29992;&#65292;&#22522;&#20110;&#24050;&#30693;&#30340;&#36807;&#21435;&#24739;&#32773;&#24182;&#21457;&#30151;&#26469;&#36827;&#34892;&#39044;&#27979;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;&#19968;&#20010;&#21253;&#21547;MIMIC-III&#25968;&#25454;&#24211;&#20013;&#30340;&#25152;&#26377;&#20027;&#39064;-&#30142;&#30149;&#30721;&#23545;&#65292;&#21478;&#19968;&#20010;&#21253;&#21547;&#21457;&#29983;&#26368;&#24120;&#35265;&#30340;50&#31181;&#30142;&#30149;&#12290;&#20934;&#30830;&#29575;&#21644;Hit Ratio@10&#34987;&#29992;&#20316;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#30340;&#25351;&#26631;&#12290;&#21457;&#29616;&#21033;&#29992;&#20943;&#23569;&#30340;&#8220;top 50&#8221; ICD-9&#30721;&#25968;&#25454;&#38598;&#30340;NCF&#27169;&#22411;&#30340;&#24615;&#33021;&#36739;&#20302;(&#20934;&#30830;&#29575;&#32422;&#20026;80%&#21644;Hit Ratio@10&#20026;...
&lt;/p&gt;
&lt;p&gt;
While deep-learning based recommender systems utilizing collaborative filtering have been commonly used for recommendation in other domains, their application in the medical domain have been limited. In addition to modeling user-item interactions, we show that deep-learning based recommender systems can be used to model subject-disease code interactions. Two novel applications of deep learning-based recommender systems using Neural Collaborative Filtering (NCF) and Deep Hybrid Filtering (DHF) were utilized for disease diagnosis based on known past patient comorbidities. Two datasets, one incorporating all subject-disease code pairs present in the MIMIC-III database, and the other incorporating the top 50 most commonly occurring diseases, were used for prediction. Accuracy and Hit Ratio@10 were utilized as metrics to estimate model performance. The performance of the NCF model making use of the reduced "top 50" ICD-9 code dataset was found to be lower (accuracy of ~80% and hit ratio@10 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#26102;&#31354;&#20107;&#20214;&#22270;&#65288;GEST&#65289;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#37322;&#12289;&#34920;&#31034;&#21644;&#29983;&#25104;&#35270;&#35273;&#21644;&#35821;&#35328;&#25925;&#20107;&#12290;&#36890;&#36807;&#23558;GEST&#22270;&#19982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#25913;&#21892;&#20174;&#25991;&#26412;&#21040;&#35270;&#39057;&#30340;&#29983;&#25104;&#65292;&#24182;&#25552;&#39640;&#35821;&#20041;&#19978;&#30340;&#25991;&#26412;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2309.08612</link><description>&lt;p&gt;
&#36890;&#36807;&#26102;&#31354;&#20107;&#20214;&#22270;&#35299;&#37322;&#35270;&#35273;&#19982;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
Explaining Vision and Language through Graphs of Events in Space and Time. (arXiv:2309.08612v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08612
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#26102;&#31354;&#20107;&#20214;&#22270;&#65288;GEST&#65289;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#37322;&#12289;&#34920;&#31034;&#21644;&#29983;&#25104;&#35270;&#35273;&#21644;&#35821;&#35328;&#25925;&#20107;&#12290;&#36890;&#36807;&#23558;GEST&#22270;&#19982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#25913;&#21892;&#20174;&#25991;&#26412;&#21040;&#35270;&#39057;&#30340;&#29983;&#25104;&#65292;&#24182;&#25552;&#39640;&#35821;&#20041;&#19978;&#30340;&#25991;&#26412;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#20170;&#22825;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#24182;&#24320;&#22987;&#24357;&#21512;&#35270;&#35273;&#19982;&#35821;&#35328;&#20043;&#38388;&#30340;&#40511;&#27807;&#12290;&#28982;&#32780;&#65292;&#20174;&#35821;&#35328;&#30340;&#35282;&#24230;&#26469;&#29702;&#35299;&#12289;&#35299;&#37322;&#21644;&#26126;&#30830;&#25511;&#21046;&#35270;&#35273;&#20869;&#23481;&#20173;&#28982;&#23384;&#22312;&#24456;&#22823;&#22256;&#38590;&#65292;&#22240;&#20026;&#25105;&#20204;&#22312;&#20004;&#20010;&#39046;&#22495;&#20043;&#38388;&#20173;&#28982;&#32570;&#20047;&#19968;&#20010;&#20849;&#21516;&#30340;&#21487;&#35299;&#37322;&#24615;&#34920;&#31034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25214;&#21040;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#26102;&#31354;&#20107;&#20214;&#22270;&#65288;GEST&#65289;&#65292;&#36890;&#36807;&#23427;&#25105;&#20204;&#21487;&#20197;&#34920;&#31034;&#12289;&#21019;&#24314;&#21644;&#35299;&#37322;&#35270;&#35273;&#21644;&#35821;&#35328;&#25925;&#20107;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#25105;&#20204;&#27169;&#22411;&#30340;&#29702;&#35770;&#39564;&#35777;&#21644;&#23454;&#39564;&#39564;&#35777;&#65292;&#35777;&#26126;&#20102;GEST&#22312;&#24378;&#22823;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20043;&#22806;&#33021;&#22815;&#24102;&#26469;&#22362;&#23454;&#30340;&#20114;&#34917;&#20215;&#20540;&#12290;&#29305;&#21035;&#22320;&#65292;GEST&#21487;&#20197;&#36890;&#36807;&#23481;&#26131;&#22320;&#34987;&#25972;&#21512;&#21040;&#25105;&#20204;&#30340;&#26032;&#22411;&#35270;&#39057;&#29983;&#25104;&#24341;&#25806;&#20013;&#65292;&#24110;&#21161;&#22312;&#20869;&#23481;&#23618;&#38754;&#25913;&#36827;&#20174;&#25991;&#26412;&#21040;&#35270;&#39057;&#30340;&#29983;&#25104;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;&#26377;&#25928;&#30340;&#22270;&#21305;&#37197;&#25216;&#26415;&#65292;GEST&#22270;&#36824;&#21487;&#20197;&#25913;&#36827;&#35821;&#20041;&#19978;&#30340;&#25991;&#26412;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence makes great advances today and starts to bridge the gap between vision and language. However, we are still far from understanding, explaining and controlling explicitly the visual content from a linguistic perspective, because we still lack a common explainable representation between the two domains. In this work we come to address this limitation and propose the Graph of Events in Space and Time (GEST), by which we can represent, create and explain, both visual and linguistic stories. We provide a theoretical justification of our model and an experimental validation, which proves that GEST can bring a solid complementary value along powerful deep learning models. In particular, GEST can help improve at the content-level the generation of videos from text, by being easily incorporated into our novel video generation engine. Additionally, by using efficient graph matching techniques, the GEST graphs can also improve the comparisons between texts at the semantic l
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#12298;Media of Langue&#12299;&#36825;&#19968;&#20840;&#26032;&#35789;&#20856;&#21644;&#20844;&#20849;&#38613;&#22609;&#65292;&#36890;&#36807;&#25551;&#36848;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#30340;&#24847;&#20041;&#22320;&#22270;&#21644;&#20004;&#20010;&#21147;&#37327;&#20043;&#38388;&#30340;&#36793;&#30028;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#19977;&#20010;&#26032;&#30340;&#27010;&#24565;&#65306;&#12298;Inter-Langue Map/Dictionary&#12299;&#12289;&#12298;Inter-Langue Space&#12299;&#21644;&#12298;Inter-Langue Network&#12299;&#12290;</title><link>http://arxiv.org/abs/2309.08609</link><description>&lt;p&gt;
&#12298;Media of Langue&#12299;&#30340;&#23186;&#20307;
&lt;/p&gt;
&lt;p&gt;
Media of Langue. (arXiv:2309.08609v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08609
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#12298;Media of Langue&#12299;&#36825;&#19968;&#20840;&#26032;&#35789;&#20856;&#21644;&#20844;&#20849;&#38613;&#22609;&#65292;&#36890;&#36807;&#25551;&#36848;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#30340;&#24847;&#20041;&#22320;&#22270;&#21644;&#20004;&#20010;&#21147;&#37327;&#20043;&#38388;&#30340;&#36793;&#30028;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#19977;&#20010;&#26032;&#30340;&#27010;&#24565;&#65306;&#12298;Inter-Langue Map/Dictionary&#12299;&#12289;&#12298;Inter-Langue Space&#12299;&#21644;&#12298;Inter-Langue Network&#12299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#23384;&#26723;Goki Muramoto&#31561;&#20154;&#30340;&#12298;Media of Langue&#12299;&#21518;&#38754;&#30340;&#26448;&#26009;&#12290;&#12298;Media of Langue&#12299;&#26159;&#19968;&#20010;&#20840;&#26032;&#30340;&#23383;&#20856;&#21644;&#20844;&#20849;&#38613;&#22609;&#65292;&#23427;&#20165;&#20174;&#8220;&#36825;&#20010;&#35789;&#34987;&#32763;&#35793;&#25104;&#37027;&#20010;&#35789;&#8221;&#30340;&#24191;&#27867;&#20107;&#20214;&#21644;&#20004;&#20010;&#21147;&#37327;&#20043;&#38388;&#30340;&#36793;&#30028;&#19978;&#25551;&#36848;&#20986;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#30340;&#24847;&#20041;&#22320;&#22270;&#12290;&#39318;&#20808;&#65292;&#20171;&#32461;&#20102;&#19977;&#20010;&#26032;&#27010;&#24565;&#65306;&#12298;Inter-Langue Map/Dictionary&#12299;&#12289;&#12298;Inter-Langue Space&#12299;&#21644;&#12298;Inter-Langue Network&#12299;&#24182;&#23558;&#20854;&#19982;&#23383;&#20856;&#12289;&#35821;&#20041;&#31354;&#38388;&#21644;&#35821;&#20041;&#32593;&#32476;&#30340;&#19977;&#20010;&#39046;&#22495;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25509;&#19979;&#26469;&#65292;&#25551;&#36848;&#20102;&#35813;&#20316;&#21697;&#20013;&#23454;&#26045;&#30340;&#20855;&#20307;&#31639;&#27861;&#21644;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper aims to archive the materials behind "Media of Langue" by Goki Muramoto et al. Media of Langue is a new dictionary and public sculpture that depicts the map of meaning on the boundary between languages solely from the vast events of "this word was translated into that word" and two forces: repulsion between all words in the same language and attraction between translated words in different languages. First, the three new concepts proposed, Inter-Langue Map/Dictionary, Inter-Langue Space, and then Inter-Langue Network, are introduced, comparing them to the three domains of dictionary, semantic space, and semantic network. Next, the specific algorithms and designs implemented in the work were described.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#21442;&#32771;&#30340;&#23545;&#35805;&#35780;&#20272;&#65288;RADE&#65289;&#26041;&#27861;&#21033;&#29992;&#39044;&#21019;&#24314;&#30340;&#35821;&#21477;&#20316;&#20026;&#21442;&#32771;&#65292;&#20197;&#35299;&#20915;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#19968;&#23545;&#22810;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20849;&#20139;&#32534;&#30721;&#22120;&#22686;&#24378;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2309.08156</link><description>&lt;p&gt;
RADE: &#22522;&#20110;&#21442;&#32771;&#30340;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
RADE: Reference-Assisted Dialogue Evaluation for Open-Domain Dialogue. (arXiv:2309.08156v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08156
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21442;&#32771;&#30340;&#23545;&#35805;&#35780;&#20272;&#65288;RADE&#65289;&#26041;&#27861;&#21033;&#29992;&#39044;&#21019;&#24314;&#30340;&#35821;&#21477;&#20316;&#20026;&#21442;&#32771;&#65292;&#20197;&#35299;&#20915;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#19968;&#23545;&#22810;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20849;&#20139;&#32534;&#30721;&#22120;&#22686;&#24378;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#24320;&#25918;&#39046;&#22495;&#30340;&#23545;&#35805;&#31995;&#32479;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21407;&#22240;&#22312;&#20110;&#19968;&#23545;&#22810;&#38382;&#39064;&#65292;&#21363;&#38500;&#20102;&#40644;&#37329;&#22238;&#24212;&#20197;&#22806;&#36824;&#26377;&#35768;&#22810;&#36866;&#24403;&#30340;&#22238;&#24212;&#12290;&#30446;&#21069;&#65292;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#38656;&#35201;&#26356;&#22909;&#22320;&#19982;&#20154;&#31867;&#20445;&#25345;&#19968;&#33268;&#65292;&#32780;&#21487;&#38752;&#30340;&#20154;&#24037;&#35780;&#20272;&#21487;&#33021;&#32791;&#26102;&#21644;&#32791;&#36164;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#21442;&#32771;&#30340;&#23545;&#35805;&#35780;&#20272;&#65288;RADE&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#39044;&#21019;&#24314;&#30340;&#35821;&#21477;&#20316;&#20026;&#21442;&#32771;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#40644;&#37329;&#22238;&#24212;&#65292;&#20197;&#32531;&#35299;&#19968;&#23545;&#22810;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;
&lt;/p&gt;
&lt;p&gt;
Evaluating open-domain dialogue systems is challenging for reasons such as the one-to-many problem, i.e., many appropriate responses other than just the golden response. As of now, automatic evaluation methods need better consistency with humans, while reliable human evaluation can be time- and cost-intensive. To this end, we propose the Reference-Assisted Dialogue Evaluation (RADE) approach under the multi-task learning framework, which leverages the pre-created utterance as reference other than the gold response to relief the one-to-many problem. Specifically, RADE explicitly compares reference and the candidate response to predict their overall scores. Moreover, an auxiliary response generation task enhances prediction via a shared encoder. To support RADE, we extend three datasets with additional rated responses other than just a golden response by human annotation. Experiments on our three datasets and two existing benchmarks demonstrate the effectiveness of our method, where Pear
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#21033;&#29992;My Science Tutor&#65288;MyST&#65289;&#20799;&#31461;&#35821;&#38899;&#35821;&#26009;&#24211;&#21644;&#26356;&#26377;&#25928;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#26469;&#25913;&#36827;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#23545;&#20799;&#31461;&#35821;&#38899;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;&#23558;Whisper&#31995;&#32479;&#25972;&#21512;&#21040;&#20799;&#31461;&#35821;&#38899;&#35782;&#21035;&#20013;&#65292;&#26174;&#31034;&#20102;&#34920;&#29616;&#21487;&#34892;&#21644;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2309.07927</link><description>&lt;p&gt;
Kid-Whisper: &#21161;&#21147;&#22635;&#34917;&#20799;&#31461;&#19982;&#25104;&#20154;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#24046;&#36317;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Kid-Whisper: Towards Bridging the Performance Gap in Automatic Speech Recognition for Children VS. Adults. (arXiv:2309.07927v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07927
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#21033;&#29992;My Science Tutor&#65288;MyST&#65289;&#20799;&#31461;&#35821;&#38899;&#35821;&#26009;&#24211;&#21644;&#26356;&#26377;&#25928;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#26469;&#25913;&#36827;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#23545;&#20799;&#31461;&#35821;&#38899;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;&#23558;Whisper&#31995;&#32479;&#25972;&#21512;&#21040;&#20799;&#31461;&#35821;&#38899;&#35782;&#21035;&#20013;&#65292;&#26174;&#31034;&#20102;&#34920;&#29616;&#21487;&#34892;&#21644;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#30340;&#36827;&#23637;&#65292;&#20363;&#22914;Whisper&#65292;&#23637;&#31034;&#20102;&#36825;&#20123;&#31995;&#32479;&#22312;&#36275;&#22815;&#30340;&#25968;&#25454;&#26465;&#20214;&#19979;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;&#30340;&#24615;&#33021;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#36827;&#23637;&#24182;&#19981;&#36866;&#29992;&#20110;&#20799;&#31461;ASR&#65292;&#21407;&#22240;&#26159;&#36866;&#29992;&#20110;&#20799;&#31461;&#30340;&#19987;&#29992;&#25968;&#25454;&#24211;&#30340;&#21487;&#29992;&#24615;&#26377;&#38480;&#65292;&#19988;&#20799;&#31461;&#35821;&#38899;&#20855;&#26377;&#19982;&#25104;&#20154;&#19981;&#21516;&#30340;&#29305;&#24449;&#12290;&#26368;&#36817;&#30340;&#19968;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#21033;&#29992;My Science Tutor&#65288;MyST&#65289;&#20799;&#31461;&#35821;&#38899;&#35821;&#26009;&#24211;&#25552;&#39640;Whisper&#35782;&#21035;&#20799;&#31461;&#35821;&#38899;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#22312;&#36825;&#20123;&#30740;&#31350;&#32467;&#26524;&#30340;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#26356;&#26377;&#25928;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#22686;&#24378;&#20102;MyST&#25968;&#25454;&#38598;&#30340;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#36824;&#24378;&#35843;&#20102;&#25913;&#36827;&#20799;&#31461;ASR&#24615;&#33021;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;&#32467;&#26524;&#23637;&#31034;&#20102;&#23558;Whisper&#26377;&#25928;&#25972;&#21512;&#21040;&#20799;&#31461;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#21487;&#34892;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in Automatic Speech Recognition (ASR) systems, exemplified by Whisper, have demonstrated the potential of these systems to approach human-level performance given sufficient data. However, this progress doesn't readily extend to ASR for children due to the limited availability of suitable child-specific databases and the distinct characteristics of children's speech. A recent study investigated leveraging the My Science Tutor (MyST) children's speech corpus to enhance Whisper's performance in recognizing children's speech. This paper builds on these findings by enhancing the utility of the MyST dataset through more efficient data preprocessing. We also highlight important challenges towards improving children's ASR performance. The results showcase the viable and efficient integration of Whisper for effective children's speech recognition.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#30284;&#30151;&#20020;&#24202;&#35797;&#39564;&#20013;&#24120;&#35265;&#30340;&#25490;&#38500;&#26631;&#20934;&#65292;&#36890;&#36807;&#24212;&#29992;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#21644;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#65292;&#35777;&#26126;&#20102;&#33258;&#21160;&#20998;&#31867;&#25490;&#38500;&#26631;&#20934;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#19987;&#38376;&#20026;&#20020;&#24202;&#35797;&#39564;&#35774;&#35745;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2309.07812</link><description>&lt;p&gt;
&#30284;&#30151;&#20020;&#24202;&#35797;&#39564;&#36164;&#26684;&#26631;&#20934;&#30340;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Text Classification of Cancer Clinical Trial Eligibility Criteria. (arXiv:2309.07812v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#30284;&#30151;&#20020;&#24202;&#35797;&#39564;&#20013;&#24120;&#35265;&#30340;&#25490;&#38500;&#26631;&#20934;&#65292;&#36890;&#36807;&#24212;&#29992;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#21644;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#65292;&#35777;&#26126;&#20102;&#33258;&#21160;&#20998;&#31867;&#25490;&#38500;&#26631;&#20934;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#19987;&#38376;&#20026;&#20020;&#24202;&#35797;&#39564;&#35774;&#35745;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#35797;&#39564;&#36164;&#26684;&#26631;&#20934;&#20197;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#38472;&#36848;&#65292;&#22240;&#27492;&#33258;&#21160;&#30830;&#23450;&#24739;&#32773;&#26159;&#21542;&#31526;&#21512;&#35797;&#39564;&#36164;&#26684;&#26159;&#19968;&#39033;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#19968;&#20010;&#28508;&#22312;&#26041;&#27861;&#26159;&#20351;&#29992;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#23545;&#24120;&#35265;&#31867;&#22411;&#30340;&#36164;&#26684;&#26631;&#20934;&#36827;&#34892;&#22788;&#29702;&#12290;&#26412;&#30740;&#31350;&#20851;&#27880;&#30284;&#30151;&#35797;&#39564;&#20013;&#30340;&#19971;&#20010;&#24120;&#35265;&#25490;&#38500;&#26631;&#20934;&#65306;&#20808;&#21069;&#24694;&#24615;&#32959;&#30244;&#12289;&#20154;&#31867;&#20813;&#30123;&#32570;&#38519;&#30149;&#27602;&#12289;&#20057;&#32925;&#30149;&#27602;&#12289;&#19993;&#32925;&#30149;&#27602;&#12289;&#31934;&#31070;&#30142;&#30149;&#12289;&#33647;&#29289;/&#29289;&#36136;&#28389;&#29992;&#21644;&#33258;&#36523;&#20813;&#30123;&#30142;&#30149;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;764&#20010;&#24102;&#26377;&#36825;&#20123;&#25490;&#38500;&#26631;&#20934;&#27880;&#37322;&#30340;&#19977;&#26399;&#30284;&#30151;&#35797;&#39564;&#12290;&#25105;&#20204;&#23581;&#35797;&#20102;&#24120;&#35265;&#30340;transformer&#27169;&#22411;&#20197;&#21450;&#19968;&#20010;&#26032;&#30340;&#39044;&#35757;&#32451;&#30340;&#20020;&#24202;&#35797;&#39564;BERT&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#33258;&#21160;&#20998;&#31867;&#24120;&#35265;&#30340;&#25490;&#38500;&#26631;&#20934;&#26159;&#21487;&#34892;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#19987;&#38376;&#38024;&#23545;&#20020;&#24202;&#35797;&#39564;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20215;&#20540;&#65292;&#35813;&#27169;&#22411;&#22312;&#25152;&#26377;&#26631;&#20934;&#20013;&#34920;&#29616;&#20986;&#26368;&#39640;&#30340;&#24179;&#22343;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic identification of clinical trials for which a patient is eligible is complicated by the fact that trial eligibility is stated in natural language. A potential solution to this problem is to employ text classification methods for common types of eligibility criteria. In this study, we focus on seven common exclusion criteria in cancer trials: prior malignancy, human immunodeficiency virus, hepatitis B, hepatitis C, psychiatric illness, drug/substance abuse, and autoimmune illness. Our dataset consists of 764 phase III cancer trials with these exclusions annotated at the trial level. We experiment with common transformer models as well as a new pre-trained clinical trial BERT model. Our results demonstrate the feasibility of automatically classifying common exclusion criteria. Additionally, we demonstrate the value of a pre-trained language model specifically for clinical trials, which yields the highest average performance across all criteria.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#19968;&#20010;&#26032;&#39062;&#30340;&#27602;&#24615;&#20820;&#23376;&#27934;&#26694;&#26550;&#23545;PaLM 2&#30340;&#23433;&#20840;&#21453;&#39304;&#36827;&#34892;&#20102;&#31283;&#20581;&#24615;&#23457;&#35745;&#65292;&#25581;&#31034;&#20102;PaLM 2&#29983;&#25104;&#30340;&#39640;&#24230;&#20196;&#20154;&#19981;&#23433;&#30340;&#27602;&#24615;&#20869;&#23481;&#26410;&#34987;&#23433;&#20840;&#23432;&#25252;&#26639;&#35780;&#20272;&#20026;&#39640;&#24230;&#19981;&#23433;&#20840;&#12290;</title><link>http://arxiv.org/abs/2309.06415</link><description>&lt;p&gt;
&#28145;&#20837;&#27602;&#24615;&#20820;&#23376;&#27934;&#65306;&#36890;&#36807;PaLM 2&#30340;&#23432;&#25252;&#26639;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Down the Toxicity Rabbit Hole: Investigating PaLM 2 Guardrails. (arXiv:2309.06415v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06415
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#19968;&#20010;&#26032;&#39062;&#30340;&#27602;&#24615;&#20820;&#23376;&#27934;&#26694;&#26550;&#23545;PaLM 2&#30340;&#23433;&#20840;&#21453;&#39304;&#36827;&#34892;&#20102;&#31283;&#20581;&#24615;&#23457;&#35745;&#65292;&#25581;&#31034;&#20102;PaLM 2&#29983;&#25104;&#30340;&#39640;&#24230;&#20196;&#20154;&#19981;&#23433;&#30340;&#27602;&#24615;&#20869;&#23481;&#26410;&#34987;&#23433;&#20840;&#23432;&#25252;&#26639;&#35780;&#20272;&#20026;&#39640;&#24230;&#19981;&#23433;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#21517;&#20026;&#8220;&#27602;&#24615;&#20820;&#23376;&#27934;&#8221;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#23545;PaLM 2&#30340;&#23433;&#20840;&#21453;&#39304;&#36827;&#34892;&#20102;&#24378;&#21270;&#31283;&#20581;&#24615;&#23457;&#35745;&#12290;&#20174;&#19968;&#20010;&#21051;&#26495;&#21360;&#35937;&#24320;&#22987;&#65292;&#35813;&#26694;&#26550;&#25351;&#31034;PaLM 2&#29983;&#25104;&#27604;&#21051;&#26495;&#21360;&#35937;&#26356;&#20855;&#26377;&#27602;&#24615;&#30340;&#20869;&#23481;&#12290;&#27599;&#19968;&#27425;&#36845;&#20195;&#65292;&#23427;&#37117;&#35201;&#27714;PaLM 2&#29983;&#25104;&#27604;&#19978;&#19968;&#27425;&#36845;&#20195;&#26356;&#20855;&#26377;&#27602;&#24615;&#30340;&#20869;&#23481;&#65292;&#30452;&#21040;PaLM 2&#30340;&#23433;&#20840;&#23432;&#25252;&#26639;&#21457;&#20986;&#23433;&#20840;&#36829;&#35268;&#35686;&#25253;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#26497;&#20854;&#20196;&#20154;&#19981;&#23433;&#30340;&#21453;&#29369;&#22826;&#20027;&#20041;&#12289;&#20234;&#26031;&#20848;&#24656;&#24807;&#30151;&#12289;&#31181;&#26063;&#20027;&#20041;&#12289;&#24656;&#21516;&#21644;&#21388;&#22899;&#24773;&#32490;&#65288;&#20165;&#21015;&#20030;&#20960;&#31181;&#65289;&#30340;&#29983;&#25104;&#20869;&#23481;&#65292;&#24182;&#19988;&#36825;&#20123;&#20869;&#23481;&#22312;PaLM 2&#30340;&#23433;&#20840;&#23432;&#25252;&#26639;&#35780;&#20272;&#20013;&#24182;&#26410;&#34987;&#35270;&#20026;&#39640;&#24230;&#19981;&#23433;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper conducts a robustness audit of the safety feedback of PaLM 2 through a novel toxicity rabbit hole framework introduced here. Starting with a stereotype, the framework instructs PaLM 2 to generate more toxic content than the stereotype. Every subsequent iteration it continues instructing PaLM 2 to generate more toxic content than the previous iteration until PaLM 2 safety guardrails throw a safety violation. Our experiments uncover highly disturbing antisemitic, Islamophobic, racist, homophobic, and misogynistic (to list a few) generated content that PaLM 2 safety guardrails do not evaluate as highly unsafe.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#26032;&#38395;&#25925;&#20107;&#38142;&#30340;&#32858;&#31867;&#65292;&#25913;&#36827;&#21644;&#35780;&#20272;&#20102;&#26032;&#38395;&#25512;&#33616;&#20013;&#20449;&#24687;&#30862;&#29255;&#21270;&#30340;&#26816;&#27979;&#12290;&#30740;&#31350;&#32467;&#26524;&#23545;&#20110;&#34913;&#37327;&#20449;&#24687;&#27969;&#30340;&#23436;&#25972;&#24615;&#21644;&#24433;&#21709;&#27665;&#20027;&#21644;&#20844;&#20849;&#35752;&#35770;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2309.06192</link><description>&lt;p&gt;
&#25552;&#39640;&#21644;&#35780;&#20272;&#26032;&#38395;&#25512;&#33616;&#20013;&#30340;&#20449;&#24687;&#30862;&#29255;&#26816;&#27979;&#19982;&#26032;&#38395;&#25925;&#20107;&#38142;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Improving and Evaluating the Detection of Fragmentation in News Recommendations with the Clustering of News Story Chains. (arXiv:2309.06192v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06192
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#26032;&#38395;&#25925;&#20107;&#38142;&#30340;&#32858;&#31867;&#65292;&#25913;&#36827;&#21644;&#35780;&#20272;&#20102;&#26032;&#38395;&#25512;&#33616;&#20013;&#20449;&#24687;&#30862;&#29255;&#21270;&#30340;&#26816;&#27979;&#12290;&#30740;&#31350;&#32467;&#26524;&#23545;&#20110;&#34913;&#37327;&#20449;&#24687;&#27969;&#30340;&#23436;&#25972;&#24615;&#21644;&#24433;&#21709;&#27665;&#20027;&#21644;&#20844;&#20849;&#35752;&#35770;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#38395;&#25512;&#33616;&#31995;&#32479;&#22312;&#22609;&#36896;&#27665;&#20027;&#31038;&#20250;&#20013;&#30340;&#20449;&#24687;&#33719;&#21462;&#26041;&#38754;&#25198;&#28436;&#30528;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#28982;&#32780;&#65292;&#23558;&#25512;&#33616;&#38024;&#23545;&#29992;&#25143;&#30340;&#20855;&#20307;&#20852;&#36259;&#21487;&#33021;&#23548;&#33268;&#20449;&#24687;&#27969;&#30340;&#20998;&#27495;&#12290;&#20449;&#24687;&#25509;&#35302;&#30340;&#30862;&#29255;&#21270;&#23545;&#20844;&#20849;&#39046;&#22495;&#30340;&#23436;&#25972;&#24615;&#26500;&#25104;&#25361;&#25112;&#65292;&#36827;&#32780;&#24433;&#21709;&#27665;&#20027;&#21644;&#20844;&#20849;&#35752;&#35770;&#12290;&#30862;&#29255;&#21270;&#25351;&#26631;&#37327;&#21270;&#20102;&#26032;&#38395;&#25512;&#33616;&#20013;&#20449;&#24687;&#27969;&#30340;&#30862;&#29255;&#21270;&#31243;&#24230;&#12290;&#20934;&#30830;&#34913;&#37327;&#35813;&#25351;&#26631;&#38656;&#35201;&#23558;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24212;&#29992;&#20110;&#35782;&#21035;&#19981;&#21516;&#30340;&#26032;&#38395;&#20107;&#20214;&#12289;&#25925;&#20107;&#25110;&#26102;&#38388;&#32447;&#12290;&#26412;&#25991;&#23545;&#22312;&#26032;&#38395;&#25512;&#33616;&#20013;&#37327;&#21270;&#20449;&#24687;&#30862;&#29255;&#21270;&#30340;&#21508;&#31181;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#35843;&#26597;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#26032;&#38395;&#25925;&#20107;&#32858;&#31867;&#30340;&#24615;&#33021;&#24230;&#37327;&#21644;&#19981;&#21516;&#27169;&#25311;&#30340;&#26032;&#38395;&#25512;&#33616;&#22330;&#26223;&#19979;&#30340;&#30862;&#29255;&#21270;&#35780;&#20998;&#35780;&#20272;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
News recommender systems play an increasingly influential role in shaping information access within democratic societies. However, tailoring recommendations to users' specific interests can result in the divergence of information streams. Fragmented access to information poses challenges to the integrity of the public sphere, thereby influencing democracy and public discourse. The Fragmentation metric quantifies the degree of fragmentation of information streams in news recommendations. Accurate measurement of this metric requires the application of Natural Language Processing (NLP) to identify distinct news events, stories, or timelines. This paper presents an extensive investigation of various approaches for quantifying Fragmentation in news recommendations. These approaches are evaluated both intrinsically, by measuring performance on news story clustering, and extrinsically, by assessing the Fragmentation scores of different simulated news recommender scenarios. Our findings demons
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25552;&#31034;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#22686;&#24378;&#20113;&#20107;&#20214;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#24037;&#20855;&#20013;&#32622;&#20449;&#24230;&#20272;&#35745;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.05833</link><description>&lt;p&gt;
PACE: &#20351;&#29992;GPT-4&#36827;&#34892;&#20113;&#20107;&#20214;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#20013;&#30340;&#25552;&#31034;&#21644;&#22686;&#21152;&#20197;&#36827;&#34892;&#26657;&#20934;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
PACE: Prompting and Augmentation for Calibrated Confidence Estimation with GPT-4 in Cloud Incident Root Cause Analysis. (arXiv:2309.05833v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25552;&#31034;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#22686;&#24378;&#20113;&#20107;&#20214;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#24037;&#20855;&#20013;&#32622;&#20449;&#24230;&#20272;&#35745;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;IT&#34892;&#19994;&#21521;&#22522;&#20110;&#20113;&#30340;&#24179;&#21488;&#30340;&#36716;&#21464;&#24378;&#35843;&#20102;&#20113;&#20107;&#20214;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#30830;&#20445;&#26381;&#21153;&#30340;&#21487;&#38752;&#24615;&#21644;&#32500;&#25252;&#23458;&#25143;&#20449;&#20219;&#12290;&#26680;&#24515;&#38382;&#39064;&#26159;&#26377;&#25928;&#30830;&#23450;&#26681;&#26412;&#21407;&#22240;&#65292;&#30001;&#20110;&#24403;&#20195;&#20113;&#22522;&#30784;&#35774;&#26045;&#30340;&#22797;&#26434;&#24615;&#65292;&#36825;&#19968;&#20219;&#21153;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#23613;&#31649;&#20986;&#29616;&#20102;&#35768;&#22810;&#29992;&#20110;&#26681;&#26412;&#21407;&#22240;&#35782;&#21035;&#30340;&#22522;&#20110;AI&#30340;&#24037;&#20855;&#65292;&#20294;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#20173;&#21463;&#21040;&#20854;&#36755;&#20986;&#36136;&#37327;&#19981;&#19968;&#33268;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#25552;&#31034;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#22686;&#24378;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#24037;&#20855;&#20013;&#32622;&#20449;&#24230;&#20272;&#35745;&#30340;&#26041;&#27861;&#12290;&#27492;&#26041;&#27861;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#12290;&#39318;&#20808;&#65292;&#27169;&#22411;&#26681;&#25454;&#21382;&#21490;&#20107;&#20214;&#25968;&#25454;&#35780;&#20272;&#33258;&#36523;&#30340;&#32622;&#20449;&#24230;&#65292;&#32771;&#34385;&#20854;&#23545;&#35777;&#25454;&#30340;&#35780;&#20272;&#24378;&#24230;&#12290;&#28982;&#21518;&#65292;&#27169;&#22411;&#23457;&#26680;&#30001;&#39044;&#27979;&#22120;&#29983;&#25104;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;&#28982;&#21518;&#65292;&#20248;&#21270;&#27493;&#39588;&#23558;&#36825;&#20123;&#35780;&#20272;&#32467;&#21512;&#36215;&#26469;&#30830;&#23450;&#26368;&#32456;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the transition to cloud-based platforms in the IT sector has emphasized the significance of cloud incident root cause analysis to ensure service reliability and maintain customer trust. Central to this process is the efficient determination of root causes, a task made challenging due to the complex nature of contemporary cloud infrastructures. Despite the proliferation of AI-driven tools for root cause identification, their applicability remains limited by the inconsistent quality of their outputs. This paper introduces a method for enhancing confidence estimation in root cause analysis tools by prompting retrieval-augmented large language models (LLMs). This approach operates in two phases. Initially, the model evaluates its confidence based on historical incident data, considering its assessment of the evidence strength. Subsequently, the model reviews the root cause generated by the predictor. An optimization step then combines these evaluations to determine the fin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#29702;&#35299;&#21518;&#35757;&#32451;&#37327;&#21270;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#37327;&#21270;&#27169;&#22411;&#22312;&#19979;&#19968;&#20010;&#21333;&#35789;&#39044;&#27979;&#31561;&#20851;&#38190;&#20219;&#21153;&#20013;&#22914;&#20309;&#21709;&#24212;&#36229;&#21442;&#25968;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2309.05210</link><description>&lt;p&gt;
&#29702;&#35299;&#21518;&#35757;&#32451;&#37327;&#21270;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Understanding the Impact of Post-Training Quantization on Large Language Models. (arXiv:2309.05210v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05210
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#29702;&#35299;&#21518;&#35757;&#32451;&#37327;&#21270;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#37327;&#21270;&#27169;&#22411;&#22312;&#19979;&#19968;&#20010;&#21333;&#35789;&#39044;&#27979;&#31561;&#20851;&#38190;&#20219;&#21153;&#20013;&#22914;&#20309;&#21709;&#24212;&#36229;&#21442;&#25968;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35268;&#27169;&#36805;&#36895;&#22686;&#21152;&#65292;&#21442;&#25968;&#25968;&#37327;&#25104;&#20026;&#35768;&#22810;&#21830;&#19994;&#27169;&#22411;&#25104;&#21151;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#22914;ChatGPT&#12289;Claude&#21644;Bard&#12290;&#21363;&#20351;&#26159;&#26368;&#36817;&#21457;&#24067;&#30340;&#29992;&#20110;&#21830;&#19994;&#29992;&#36884;&#30340;&#20844;&#24320;&#21487;&#35265;&#27169;&#22411;&#65292;&#22914;Falcon&#21644;Llama2&#65292;&#20063;&#25317;&#26377;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#12290;&#21442;&#25968;&#25968;&#37327;&#30340;&#26174;&#33879;&#22686;&#21152;&#20351;&#24471;&#37096;&#32626;&#21644;&#36816;&#34892;&#38750;&#24120;&#26114;&#36149;&#12290;&#37327;&#21270;&#39046;&#22495;&#22312;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#20197;&#21450;LLMs&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20351;&#24471;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#22312;&#28040;&#36153;&#32423;GPU&#19978;&#37096;&#32626;&#65292;&#20174;&#32780;&#20351;&#20854;&#26356;&#26131;&#33719;&#24471;&#12290;&#37327;&#21270;&#27169;&#22411;&#36890;&#24120;&#34920;&#29616;&#20986;&#19982;&#20854;&#26410;&#37327;&#21270;&#22522;&#20934;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35832;&#22914;&#28201;&#24230;&#12289;&#26368;&#22823;&#26032;&#26631;&#35760;&#25968;&#21644;topk&#31561;&#36229;&#21442;&#25968;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#19979;&#19968;&#20010;&#21333;&#35789;&#39044;&#27979;&#65292;&#25105;&#20204;&#23545;&#36825;&#20123;&#37327;&#21270;&#27169;&#22411;&#22914;&#20309;&#21709;&#24212;&#20173;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#12290;&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are rapidly increasing in size, with the number of parameters becoming a key factor in the success of many commercial models, such as ChatGPT, Claude, and Bard. Even the recently released publicly accessible models for commercial usage, such as Falcon and Llama2, come equipped with billions of parameters. This significant increase in the number of parameters makes deployment and operation very costly. The remarkable progress in the field of quantization for large neural networks in general and LLMs in particular, has made these models more accessible by enabling them to be deployed on consumer-grade GPUs. Quantized models generally demonstrate comparable performance levels to their unquantized base counterparts. Nonetheless, there exists a notable gap in our comprehensive understanding of how these quantized models respond to hyperparameters, such as temperature, max new tokens, and topk, particularly for next word prediction. The present analysis reveals t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24320;&#25918;&#30340;LLM&#27169;&#22411;&#65288;FLM-101B&#65289;&#20197;&#21450;&#22914;&#20309;&#29992;10&#19975;&#32654;&#20803;&#30340;&#39044;&#31639;&#26469;&#35757;&#32451;&#23427;&#12290;&#36890;&#36807;&#37319;&#29992;&#22686;&#38271;&#31574;&#30053;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;LLM&#35757;&#32451;&#30340;&#25104;&#26412;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;LLM&#30340;&#26234;&#33021;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.03852</link><description>&lt;p&gt;
FLM-101B&#65306;&#19968;&#31181;&#24320;&#25918;&#30340;LLM&#21644;&#22914;&#20309;&#29992;10&#19975;&#32654;&#20803;&#39044;&#31639;&#26469;&#35757;&#32451;&#23427;
&lt;/p&gt;
&lt;p&gt;
FLM-101B: An Open LLM and How to Train It with $100K Budget. (arXiv:2309.03852v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03852
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24320;&#25918;&#30340;LLM&#27169;&#22411;&#65288;FLM-101B&#65289;&#20197;&#21450;&#22914;&#20309;&#29992;10&#19975;&#32654;&#20803;&#30340;&#39044;&#31639;&#26469;&#35757;&#32451;&#23427;&#12290;&#36890;&#36807;&#37319;&#29992;&#22686;&#38271;&#31574;&#30053;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;LLM&#35757;&#32451;&#30340;&#25104;&#26412;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;LLM&#30340;&#26234;&#33021;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#21457;&#23637;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#65288;i&#65289;&#39640;&#35745;&#31639;&#25104;&#26412;&#65307;&#65288;ii&#65289;&#38590;&#20197;&#36827;&#34892;&#20844;&#24179;&#23458;&#35266;&#30340;&#35780;&#20272;&#12290;LLMs&#30340;&#20215;&#26684;&#26114;&#36149;&#65292;&#21482;&#26377;&#23569;&#25968;&#20960;&#23478;&#20027;&#35201;&#21442;&#19982;&#32773;&#26377;&#33021;&#21147;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#30740;&#31350;&#21644;&#24212;&#29992;&#26426;&#20250;&#12290;&#36825;&#20984;&#26174;&#20102;&#25104;&#26412;&#25928;&#30410;&#30340;LLM&#35757;&#32451;&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#22686;&#38271;&#31574;&#30053;&#65292;&#26174;&#33879;&#38477;&#20302;LLM&#35757;&#32451;&#25104;&#26412;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#21487;&#20197;&#22312;10&#19975;&#32654;&#20803;&#30340;&#39044;&#31639;&#19979;&#35757;&#32451;&#20855;&#26377;101B&#21442;&#25968;&#21644;0.31TB&#20196;&#29260;&#30340;LLM&#12290;&#25105;&#20204;&#36824;&#37319;&#29992;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#35780;&#20272;&#33539;&#24335;&#65292;&#29992;&#20110;&#23545;LLMs&#36827;&#34892;&#26234;&#33021;&#30340;&#26234;&#21830;&#35780;&#20272;&#65292;&#36825;&#26159;&#38024;&#23545;&#29616;&#26377;&#35780;&#20272;&#26356;&#27880;&#37325;&#30693;&#35782;&#33021;&#21147;&#30340;&#34917;&#20805;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#21253;&#25324;&#31526;&#21495;&#26144;&#23556;&#12289;&#35268;&#21017;&#29702;&#35299;&#12289;&#27169;&#24335;&#25366;&#25496;&#22312;&#20869;&#30340;&#37325;&#35201;&#26234;&#33021;&#26041;&#38754;&#30340;&#35780;&#20272;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have achieved remarkable success in NLP and multimodal tasks. Despite these successes, their development faces two main challenges: (i) high computational cost; and (ii) difficulty in conducting fair and objective evaluations. LLMs are prohibitively expensive, making it feasible for only a few major players to undertake their training, thereby constraining both research and application opportunities. This underscores the importance of cost-effective LLM training. In this paper, we utilize a growth strategy to significantly reduce LLM training cost. We demonstrate that an LLM with 101B parameters and 0.31TB tokens can be trained on a $100K budget. We also adopt a systematic evaluation paradigm for the IQ evaluation of LLMs, in complement to existing evaluations that focus more on knowledge-oriented abilities. We introduce our benchmark including evaluations on important aspects of intelligence including symbolic mapping, itrule understanding, pattern mining,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26080;&#27861;&#35775;&#38382;&#27169;&#22411;&#26550;&#26500;&#21644;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#25805;&#32437;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411; (LLMs)&#12290;&#36890;&#36807;&#20248;&#21270;&#36890;&#29992;&#23545;&#25239;&#25552;&#31034;&#19982;&#29992;&#25143;&#26597;&#35810;&#32467;&#21512;&#65292;&#21487;&#20197;&#25200;&#20081;&#34987;&#25915;&#20987;&#27169;&#22411;&#30340;&#23545;&#40784;&#65292;&#23548;&#33268;&#24847;&#22806;&#21644;&#28508;&#22312;&#26377;&#23475;&#30340;&#36755;&#20986;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#25581;&#31034;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#21644;&#28431;&#27934;&#65292;&#20026;&#36127;&#36131;&#20219;&#30340;AI&#24320;&#21457;&#25552;&#20379;&#20102;&#19968;&#31181;&#35786;&#26029;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2309.01446</link><description>&lt;p&gt;
&#24320;&#38376;&#21543;&#65281;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#40657;&#30418;&#30772;&#35299;
&lt;/p&gt;
&lt;p&gt;
Open Sesame! Universal Black Box Jailbreaking of Large Language Models. (arXiv:2309.01446v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26080;&#27861;&#35775;&#38382;&#27169;&#22411;&#26550;&#26500;&#21644;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#25805;&#32437;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411; (LLMs)&#12290;&#36890;&#36807;&#20248;&#21270;&#36890;&#29992;&#23545;&#25239;&#25552;&#31034;&#19982;&#29992;&#25143;&#26597;&#35810;&#32467;&#21512;&#65292;&#21487;&#20197;&#25200;&#20081;&#34987;&#25915;&#20987;&#27169;&#22411;&#30340;&#23545;&#40784;&#65292;&#23548;&#33268;&#24847;&#22806;&#21644;&#28508;&#22312;&#26377;&#23475;&#30340;&#36755;&#20986;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#25581;&#31034;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#21644;&#28431;&#27934;&#65292;&#20026;&#36127;&#36131;&#20219;&#30340;AI&#24320;&#21457;&#25552;&#20379;&#20102;&#19968;&#31181;&#35786;&#26029;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26088;&#22312;&#25552;&#20379;&#26377;&#24110;&#21161;&#21644;&#23433;&#20840;&#30340;&#22238;&#22797;&#65292;&#36890;&#24120;&#20381;&#36182;&#20110;&#23545;&#40784;&#25216;&#26415;&#19982;&#29992;&#25143;&#24847;&#22270;&#21644;&#31038;&#20250;&#25351;&#21335;&#20445;&#25345;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#23545;&#40784;&#21487;&#33021;&#20250;&#34987;&#24694;&#24847;&#34892;&#20026;&#32773;&#21033;&#29992;&#65292;&#20197;&#29992;&#20110;&#24847;&#24819;&#19981;&#21040;&#30340;&#30446;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#36951;&#20256;&#31639;&#27861;&#65288;GA&#65289;&#22312;&#27169;&#22411;&#26550;&#26500;&#21644;&#21442;&#25968;&#19981;&#21487;&#35775;&#38382;&#26102;&#25805;&#32437;LLMs&#12290;GA&#25915;&#20987;&#36890;&#36807;&#20248;&#21270;&#36890;&#29992;&#23545;&#25239;&#25552;&#31034;&#19982;&#29992;&#25143;&#26597;&#35810;&#32467;&#21512;&#65292;&#25200;&#20081;&#34987;&#25915;&#20987;&#27169;&#22411;&#30340;&#23545;&#40784;&#65292;&#23548;&#33268;&#24847;&#22806;&#21644;&#28508;&#22312;&#26377;&#23475;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#26032;&#39062;&#26041;&#27861;&#36890;&#36807;&#25581;&#31034;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#21644;&#28431;&#27934;&#65292;&#31995;&#32479;&#22320;&#25581;&#31034;&#20102;&#20854;&#21709;&#24212;&#19982;&#39044;&#26399;&#34892;&#20026;&#19981;&#31526;&#30340;&#24773;&#20917;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#65292;&#20174;&#32780;&#20026;&#20851;&#20110;&#36127;&#36131;&#20219;&#30340;AI&#24320;&#21457;&#30340;&#35752;&#35770;&#25552;&#20379;&#20102;&#19968;&#31181;&#35786;&#26029;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), designed to provide helpful and safe responses, often rely on alignment techniques to align with user intent and social guidelines. Unfortunately, this alignment can be exploited by malicious actors seeking to manipulate an LLM's outputs for unintended purposes. In this paper we introduce a novel approach that employs a genetic algorithm (GA) to manipulate LLMs when model architecture and parameters are inaccessible. The GA attack works by optimizing a universal adversarial prompt that -- when combined with a user's query -- disrupts the attacked model's alignment, resulting in unintended and potentially harmful outputs. Our novel approach systematically reveals a model's limitations and vulnerabilities by uncovering instances where its responses deviate from expected behavior. Through extensive experiments we demonstrate the efficacy of our technique, thus contributing to the ongoing discussion on responsible AI development by providing a diagnostic tool 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24212;&#29992;&#26550;&#26500;&#23454;&#29616;&#20102;&#29983;&#25104;&#24335;AI&#26381;&#21153;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#20449;&#24687;&#31232;&#32570;&#21644;&#25968;&#25454;&#19981;&#36275;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.01105</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#20225;&#19994;&#25968;&#25454;&#30340;LLM&#24212;&#29992;&#26550;&#26500;&#23454;&#29616;&#29983;&#25104;&#24335;AI&#26381;&#21153;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study on the Implementation of Generative AI Services Using an Enterprise Data-Based LLM Application Architecture. (arXiv:2309.01105v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24212;&#29992;&#26550;&#26500;&#23454;&#29616;&#20102;&#29983;&#25104;&#24335;AI&#26381;&#21153;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#20449;&#24687;&#31232;&#32570;&#21644;&#25968;&#25454;&#19981;&#36275;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24212;&#29992;&#26550;&#26500;&#23454;&#29616;&#29983;&#25104;&#24335;AI&#26381;&#21153;&#30340;&#26041;&#27861;&#12290;&#38543;&#30528;&#29983;&#25104;&#24335;AI&#25216;&#26415;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;LLM&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#21463;&#21040;&#20102;&#37325;&#35270;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#20449;&#24687;&#31232;&#32570;&#30340;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;LLM&#30340;&#33021;&#21147;&#25552;&#20986;&#20102;&#20855;&#20307;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#30740;&#31350;&#25506;&#35752;&#20102;&#32531;&#35299;&#25968;&#25454;&#19981;&#36275;&#38382;&#39064;&#30340;&#31574;&#30053;&#65292;&#24182;&#25552;&#20379;&#20102;&#37327;&#36523;&#23450;&#21046;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#24494;&#35843;&#25216;&#26415;&#21644;&#30452;&#25509;&#25991;&#26723;&#38598;&#25104;&#26469;&#32531;&#35299;&#25968;&#25454;&#19981;&#36275;&#38382;&#39064;&#30340;&#26377;&#25928;&#24615;&#12290;&#26412;&#30740;&#31350;&#30340;&#19968;&#20010;&#37325;&#35201;&#36129;&#29486;&#26159;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#35299;&#20915;&#20102;&#19978;&#36848;&#25361;&#25112;&#12290;RAG&#27169;&#22411;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#65292;&#20197;&#25552;&#39640;&#20449;&#24687;&#23384;&#20648;&#21644;&#26816;&#32034;&#36807;&#31243;&#65292;&#30830;&#20445;&#25913;&#36827;&#20869;&#23481;&#29983;&#25104;&#12290;&#30740;&#31350;&#38416;&#26126;&#20102;&#20449;&#24687;&#23384;&#20648;&#21644;&#26816;&#32034;&#30340;&#20851;&#38190;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents a method for implementing generative AI services by utilizing the Large Language Models (LLM) application architecture. With recent advancements in generative AI technology, LLMs have gained prominence across various domains. In this context, the research addresses the challenge of information scarcity and proposes specific remedies by harnessing LLM capabilities. The investigation delves into strategies for mitigating the issue of inadequate data, offering tailored solutions. The study delves into the efficacy of employing fine-tuning techniques and direct document integration to alleviate data insufficiency. A significant contribution of this work is the development of a Retrieval-Augmented Generation (RAG) model, which tackles the aforementioned challenges. The RAG model is carefully designed to enhance information storage and retrieval processes, ensuring improved content generation. The research elucidates the key phases of the information storage and retrieval
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#30740;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#37322;&#25216;&#26415;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#35752;&#35770;&#20102;&#35780;&#20272;&#29983;&#25104;&#35299;&#37322;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#20197;&#21450;&#22914;&#20309;&#21033;&#29992;&#35299;&#37322;&#26469;&#35843;&#35797;&#27169;&#22411;&#21644;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.01029</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Explainability for Large Language Models: A Survey. (arXiv:2309.01029v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01029
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#30740;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#37322;&#25216;&#26415;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#35752;&#35770;&#20102;&#35780;&#20272;&#29983;&#25104;&#35299;&#37322;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#20197;&#21450;&#22914;&#20309;&#21033;&#29992;&#35299;&#37322;&#26469;&#35843;&#35797;&#27169;&#22411;&#21644;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#20869;&#37096;&#26426;&#21046;&#20173;&#28982;&#19981;&#26126;&#30830;&#65292;&#36825;&#31181;&#32570;&#20047;&#36879;&#26126;&#24230;&#20026;&#19979;&#28216;&#24212;&#29992;&#24102;&#26469;&#20102;&#19981;&#24517;&#35201;&#30340;&#39118;&#38505;&#12290;&#22240;&#27492;&#65292;&#29702;&#35299;&#21644;&#35299;&#37322;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#38416;&#26126;&#23427;&#20204;&#30340;&#34892;&#20026;&#12289;&#38480;&#21046;&#21644;&#31038;&#20250;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#30340;&#27010;&#36848;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#26681;&#25454;LLMs&#30340;&#35757;&#32451;&#33539;&#24335;&#23558;&#25216;&#26415;&#36827;&#34892;&#20998;&#31867;&#65306;&#20256;&#32479;&#30340;&#24494;&#35843;&#33539;&#24335;&#21644;&#25552;&#31034;&#33539;&#24335;&#12290;&#23545;&#20110;&#27599;&#20010;&#33539;&#24335;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#29983;&#25104;&#20010;&#20307;&#39044;&#27979;&#30340;&#23616;&#37096;&#35299;&#37322;&#21644;&#25972;&#20307;&#27169;&#22411;&#30693;&#35782;&#30340;&#20840;&#23616;&#35299;&#37322;&#30340;&#30446;&#26631;&#21644;&#20027;&#35201;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#35780;&#20272;&#29983;&#25104;&#35299;&#37322;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#35752;&#35770;&#20102;&#22914;&#20309;&#21033;&#29992;&#35299;&#37322;&#26469;&#35843;&#35797;&#27169;&#22411;&#21644;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated impressive capabilities in natural language processing. However, their internal mechanisms are still unclear and this lack of transparency poses unwanted risks for downstream applications. Therefore, understanding and explaining these models is crucial for elucidating their behaviors, limitations, and social impacts. In this paper, we introduce a taxonomy of explainability techniques and provide a structured overview of methods for explaining Transformer-based language models. We categorize techniques based on the training paradigms of LLMs: traditional fine-tuning-based paradigm and prompting-based paradigm. For each paradigm, we summarize the goals and dominant approaches for generating local explanations of individual predictions and global explanations of overall model knowledge. We also discuss metrics for evaluating generated explanations, and discuss how explanations can be leveraged to debug models and improve performance. Lastly, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#23545;&#25239;&#24615;&#22270;&#20687;&#33021;&#22815;&#22312;&#36816;&#34892;&#26102;&#25511;&#21046;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#29992;&#30340;&#26041;&#27861;&#26469;&#21019;&#24314;&#22270;&#20687;&#21163;&#25345;&#12290;&#36890;&#36807;&#30740;&#31350;&#19977;&#31181;&#25915;&#20987;&#31867;&#22411;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#25915;&#20987;&#23545;&#26368;&#26032;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#39640;&#36798;90&#65285;&#20197;&#19978;&#30340;&#25104;&#21151;&#29575;&#12290;&#35813;&#30740;&#31350;&#24341;&#21457;&#20102;&#23545;&#22522;&#30784;&#27169;&#22411;&#23433;&#20840;&#24615;&#30340;&#20005;&#37325;&#25285;&#24551;&#12290;</title><link>http://arxiv.org/abs/2309.00236</link><description>&lt;p&gt;
&#22270;&#20687;&#21163;&#25345;&#65306;&#23545;&#25239;&#24615;&#22270;&#20687;&#33021;&#22312;&#36816;&#34892;&#26102;&#25511;&#21046;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Image Hijacking: Adversarial Images can Control Generative Models at Runtime. (arXiv:2309.00236v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00236
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#23545;&#25239;&#24615;&#22270;&#20687;&#33021;&#22815;&#22312;&#36816;&#34892;&#26102;&#25511;&#21046;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#29992;&#30340;&#26041;&#27861;&#26469;&#21019;&#24314;&#22270;&#20687;&#21163;&#25345;&#12290;&#36890;&#36807;&#30740;&#31350;&#19977;&#31181;&#25915;&#20987;&#31867;&#22411;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#25915;&#20987;&#23545;&#26368;&#26032;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#39640;&#36798;90&#65285;&#20197;&#19978;&#30340;&#25104;&#21151;&#29575;&#12290;&#35813;&#30740;&#31350;&#24341;&#21457;&#20102;&#23545;&#22522;&#30784;&#27169;&#22411;&#23433;&#20840;&#24615;&#30340;&#20005;&#37325;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#20813;&#21463;&#24694;&#24847;&#34892;&#20026;&#32773;&#30340;&#25915;&#20987;&#65311;&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#30340;&#22270;&#20687;&#36755;&#20837;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#22270;&#20687;&#21163;&#25345;&#65292;&#21363;&#33021;&#22815;&#22312;&#36816;&#34892;&#26102;&#25511;&#21046;&#29983;&#25104;&#27169;&#22411;&#30340;&#23545;&#25239;&#24615;&#22270;&#20687;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#34892;&#20026;&#21305;&#37197;&#8221;&#30340;&#36890;&#29992;&#26041;&#27861;&#26469;&#21019;&#24314;&#22270;&#20687;&#21163;&#25345;&#65292;&#24182;&#29992;&#23427;&#26469;&#25506;&#32034;&#19977;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#65306;&#20855;&#20307;&#23383;&#31526;&#20018;&#25915;&#20987;&#21487;&#20197;&#29983;&#25104;&#20219;&#24847;&#34987;&#25915;&#20987;&#32773;&#36873;&#25321;&#30340;&#36755;&#20986;&#65307;&#27844;&#38706;&#19978;&#19979;&#25991;&#25915;&#20987;&#21487;&#20197;&#23558;&#19978;&#19979;&#25991;&#31383;&#21475;&#20013;&#30340;&#20449;&#24687;&#27844;&#38706;&#21040;&#36755;&#20986;&#20013;&#65307;&#36234;&#29425;&#25915;&#20987;&#21487;&#20197;&#32469;&#36807;&#27169;&#22411;&#30340;&#23433;&#20840;&#35757;&#32451;&#12290;&#25105;&#20204;&#23545;&#22522;&#20110;CLIP&#21644;LLaMA-2&#30340;&#26368;&#26032;VLM&#27169;&#22411;LLaVA-2&#36827;&#34892;&#20102;&#36825;&#20123;&#25915;&#20987;&#30340;&#30740;&#31350;&#65292;&#24182;&#21457;&#29616;&#25105;&#20204;&#25152;&#26377;&#30340;&#25915;&#20987;&#31867;&#22411;&#25104;&#21151;&#29575;&#22343;&#22312;90&#65285;&#20197;&#19978;&#12290;&#32780;&#19988;&#65292;&#25105;&#20204;&#30340;&#25915;&#20987;&#26159;&#33258;&#21160;&#21270;&#30340;&#65292;&#21482;&#38656;&#35201;&#23545;&#22270;&#20687;&#36827;&#34892;&#23567;&#30340;&#25200;&#21160;&#12290;&#36825;&#20123;&#21457;&#29616;&#23545;&#22522;&#30784;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#25552;&#20986;&#20102;&#20005;&#37325;&#30340;&#25285;&#24551;&#12290;&#22914;&#26524;&#22270;&#20687;&#21163;&#25345;&#19982;CIFAR-10&#20013;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#19968;&#26679;&#38590;&#20197;&#38450;&#24481;&#65292;&#37027;&#20040;&#21487;&#33021;&#38656;&#35201;&#24456;&#22810;&#24180;&#25165;&#33021;&#25214;&#21040;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Are foundation models secure from malicious actors? In this work, we focus on the image input to a vision-language model (VLM). We discover image hijacks, adversarial images that control generative models at runtime. We introduce Behavior Matching, a general method for creating image hijacks, and we use it to explore three types of attacks. Specific string attacks generate arbitrary output of the adversary's choosing. Leak context attacks leak information from the context window into the output. Jailbreak attacks circumvent a model's safety training. We study these attacks against LLaVA-2, a state-of-the-art VLM based on CLIP and LLaMA-2, and find that all our attack types have above a 90\% success rate. Moreover, our attacks are automated and require only small image perturbations. These findings raise serious concerns about the security of foundation models. If image hijacks are as difficult to defend against as adversarial examples in CIFAR-10, then it might be many years before a s
&lt;/p&gt;</description></item><item><title>LLaSM&#26159;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#21644;&#35821;&#38899;&#27169;&#22411;&#65292;&#20855;&#26377;&#36328;&#27169;&#24577;&#23545;&#35805;&#33021;&#21147;&#65292;&#36890;&#36807;&#36981;&#24490;&#35821;&#38899;&#21644;&#35821;&#35328;&#25351;&#20196;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#20415;&#33258;&#28982;&#30340;&#20154;&#26426;&#20132;&#20114;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2308.15930</link><description>&lt;p&gt;
LLaSM: &#22823;&#22411;&#35821;&#35328;&#21644;&#35821;&#38899;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LLaSM: Large Language and Speech Model. (arXiv:2308.15930v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15930
&lt;/p&gt;
&lt;p&gt;
LLaSM&#26159;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#21644;&#35821;&#38899;&#27169;&#22411;&#65292;&#20855;&#26377;&#36328;&#27169;&#24577;&#23545;&#35805;&#33021;&#21147;&#65292;&#36890;&#36807;&#36981;&#24490;&#35821;&#38899;&#21644;&#35821;&#35328;&#25351;&#20196;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#20415;&#33258;&#28982;&#30340;&#20154;&#26426;&#20132;&#20114;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#35270;&#35273;-&#35821;&#35328;&#22810;&#27169;&#24577;&#27169;&#22411;&#19978;&#65292;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#33021;&#21147;&#26469;&#36981;&#24490;&#35270;&#35273;&#21644;&#35821;&#35328;&#25351;&#20196;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#35821;&#38899;&#20063;&#26159;&#20154;&#31867;&#19982;&#19990;&#30028;&#20114;&#21160;&#30340;&#37325;&#35201;&#26041;&#24335;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#19968;&#20010;&#36890;&#29992;&#30340;&#21161;&#25163;&#26469;&#35828;&#65292;&#33021;&#22815;&#36981;&#24490;&#22810;&#27169;&#24577;&#35821;&#38899;&#21644;&#35821;&#35328;&#25351;&#20196;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#21644;&#35821;&#38899;&#27169;&#22411;&#65288;LLaSM&#65289;&#12290;LLaSM&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#35757;&#32451;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#35821;&#38899;-&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;&#36328;&#27169;&#24577;&#23545;&#35805;&#33021;&#21147;&#65292;&#33021;&#22815;&#36981;&#24490;&#35821;&#38899;&#21644;&#35821;&#35328;&#25351;&#20196;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#65292;LLaSM&#23637;&#31034;&#20102;&#19968;&#31181;&#26356;&#26041;&#20415;&#33258;&#28982;&#30340;&#20154;&#26426;&#20132;&#20114;&#26041;&#24335;&#12290;&#20026;&#20102;&#25903;&#25345;&#30740;&#31350;&#65292;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;&#22823;&#22411;&#30340;&#35821;&#38899;&#25351;&#20196;&#25968;&#25454;&#38598;LLaSM-Audio-Instructions&#12290;&#20195;&#30721;&#21644;&#28436;&#31034;&#21487;&#22312;https://github.com/LinkSoul-AI/LLaSM&#21644;ht&#19978;&#26597;&#30475;
&lt;/p&gt;
&lt;p&gt;
Multi-modal large language models have garnered significant interest recently. Though, most of the works focus on vision-language multi-modal models providing strong capabilities in following vision-and-language instructions. However, we claim that speech is also an important modality through which humans interact with the world. Hence, it is crucial for a general-purpose assistant to be able to follow multi-modal speech-and-language instructions. In this work, we propose Large Language and Speech Model (LLaSM). LLaSM is an end-to-end trained large multi-modal speech-language model with cross-modal conversational abilities, capable of following speech-and-language instructions. Our early experiments show that LLaSM demonstrates a more convenient and natural way for humans to interact with artificial intelligence. Specifically, we also release a large Speech Instruction Following dataset LLaSM-Audio-Instructions. Code and demo are available at https://github.com/LinkSoul-AI/LLaSM and ht
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#25105;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#35753;LLM&#33021;&#22815;&#33258;&#20027;&#22320;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#25968;&#25454;&#65292;&#36890;&#36807;&#24341;&#20837;&#25351;&#20196;&#36981;&#24490;&#38590;&#24230;&#25351;&#26631;&#65288;IFD&#65289;&#65292;&#22823;&#24133;&#25552;&#39640;&#20102;&#27169;&#22411;&#35757;&#32451;&#25928;&#29575;&#65292;&#24182;&#22312;&#30693;&#21517;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#23637;&#31034;&#20102;&#20248;&#20110;&#20256;&#32479;&#25968;&#25454;&#36755;&#20837;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.12032</link><description>&lt;p&gt;
&#20174;&#25968;&#37327;&#21040;&#36136;&#37327;&#65306;&#21033;&#29992;&#33258;&#25105;&#24341;&#23548;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#25552;&#21319;LLM&#24615;&#33021;&#20197;&#36827;&#34892;&#25351;&#20196;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning. (arXiv:2308.12032v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12032
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#25105;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#35753;LLM&#33021;&#22815;&#33258;&#20027;&#22320;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#25968;&#25454;&#65292;&#36890;&#36807;&#24341;&#20837;&#25351;&#20196;&#36981;&#24490;&#38590;&#24230;&#25351;&#26631;&#65288;IFD&#65289;&#65292;&#22823;&#24133;&#25552;&#39640;&#20102;&#27169;&#22411;&#35757;&#32451;&#25928;&#29575;&#65292;&#24182;&#22312;&#30693;&#21517;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#23637;&#31034;&#20102;&#20248;&#20110;&#20256;&#32479;&#25968;&#25454;&#36755;&#20837;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#65292;&#25351;&#20196;&#25968;&#25454;&#30340;&#36136;&#37327;&#21644;&#25968;&#37327;&#20043;&#38388;&#30340;&#24179;&#34913;&#24050;&#25104;&#20026;&#19968;&#20010;&#28966;&#28857;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#25105;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#35753;LLM&#33021;&#22815;&#33258;&#20027;&#22320;&#35782;&#21035;&#21644;&#36873;&#25321;&#22823;&#35268;&#27169;&#24320;&#28304;&#25968;&#25454;&#38598;&#20013;&#30340;&#31934;&#36873;&#26679;&#26412;&#65292;&#26377;&#25928;&#20943;&#23569;&#20102;&#25351;&#20196;&#35843;&#20248;&#30340;&#25163;&#21160;&#31579;&#36873;&#21644;&#28508;&#22312;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#21019;&#26032;&#26159;&#25351;&#20196;&#36981;&#24490;&#38590;&#24230;&#65288;IFD&#65289;&#25351;&#26631;&#65292;&#23427;&#25104;&#20026;&#20102;&#19968;&#20010;&#20915;&#23450;&#24615;&#24037;&#20855;&#65292;&#29992;&#20110;&#35782;&#21035;&#27169;&#22411;&#26399;&#26395;&#21709;&#24212;&#21644;&#33258;&#20027;&#29983;&#25104;&#33021;&#21147;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#36890;&#36807;&#28789;&#27963;&#24212;&#29992;IFD&#65292;&#25105;&#20204;&#33021;&#22815;&#25214;&#21040;&#31934;&#36873;&#26679;&#26412;&#65292;&#20174;&#32780;&#22823;&#24133;&#25552;&#21319;&#27169;&#22411;&#35757;&#32451;&#25928;&#29575;&#12290;&#22312;Alpaca&#21644;WizardLM&#31561;&#30693;&#21517;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#39564;&#35777;&#25903;&#25345;&#25105;&#20204;&#30340;&#21457;&#29616;&#65307;&#20165;&#20351;&#29992;&#20256;&#32479;&#25968;&#25454;&#36755;&#20837;&#30340;10%&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#23637;&#31034;&#20102;&#25913;&#36827;&#30340;&#32467;&#26524;&#12290;&#36825;&#31181;&#33258;&#25105;&#24341;&#23548;&#25361;&#36873;&#21644;IFD&#25351;&#26631;&#30340;&#32508;&#21512;&#24847;&#21619;&#30528;LLM&#20248;&#21270;&#30340;&#19968;&#20010;&#21464;&#38761;&#24615;&#39134;&#36291;&#65292;&#26377;&#26395;&#21516;&#26102;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#38477;&#20302;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of Large Language Models, the balance between instruction data quality and quantity has become a focal point. Recognizing this, we introduce a self-guided methodology for LLMs to autonomously discern and select cherry samples from vast open-source datasets, effectively minimizing manual curation and potential cost for instruction tuning an LLM. Our key innovation, the Instruction-Following Difficulty (IFD) metric, emerges as a pivotal tool to identify discrepancies between a model's expected responses and its autonomous generation prowess. Through the adept application of IFD, cherry samples are pinpointed, leading to a marked uptick in model training efficiency. Empirical validations on renowned datasets like Alpaca and WizardLM underpin our findings; with a mere 10% of conventional data input, our strategy showcases improved results. This synthesis of self-guided cherry-picking and the IFD metric signifies a transformative leap in the optimization of LLMs, promising both
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"Efficient Benchmarking"&#30340;&#38382;&#39064;&#65292;&#26088;&#22312;&#26234;&#33021;&#22320;&#20943;&#23569;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35745;&#31639;&#25104;&#26412;&#32780;&#19981;&#38477;&#20302;&#21487;&#38752;&#24615;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;Decision Impact on Reliability&#65288;DIoR&#65289;&#30340;&#26032;&#24230;&#37327;&#26469;&#35780;&#20272;&#20915;&#31574;&#30340;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;HELM&#22522;&#20934;&#27979;&#35797;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;&#21482;&#38656;&#21024;&#38500;&#19968;&#20010;&#20302;&#25490;&#21517;&#27169;&#22411;&#21363;&#21487;&#25913;&#21464;&#39046;&#20808;&#32773;&#65292;&#24182;&#20165;&#38656;&#23569;&#37327;&#31034;&#20363;&#21363;&#21487;&#24471;&#21040;&#27491;&#30830;&#30340;&#22522;&#20934;&#27979;&#35797;&#25490;&#21517;&#12290;</title><link>http://arxiv.org/abs/2308.11696</link><description>&lt;p&gt;
&#26377;&#25928;&#30340;&#35821;&#35328;&#27169;&#22411;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Efficient Benchmarking (of Language Models). (arXiv:2308.11696v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11696
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"Efficient Benchmarking"&#30340;&#38382;&#39064;&#65292;&#26088;&#22312;&#26234;&#33021;&#22320;&#20943;&#23569;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35745;&#31639;&#25104;&#26412;&#32780;&#19981;&#38477;&#20302;&#21487;&#38752;&#24615;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;Decision Impact on Reliability&#65288;DIoR&#65289;&#30340;&#26032;&#24230;&#37327;&#26469;&#35780;&#20272;&#20915;&#31574;&#30340;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;HELM&#22522;&#20934;&#27979;&#35797;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;&#21482;&#38656;&#21024;&#38500;&#19968;&#20010;&#20302;&#25490;&#21517;&#27169;&#22411;&#21363;&#21487;&#25913;&#21464;&#39046;&#20808;&#32773;&#65292;&#24182;&#20165;&#38656;&#23569;&#37327;&#31034;&#20363;&#21363;&#21487;&#24471;&#21040;&#27491;&#30830;&#30340;&#22522;&#20934;&#27979;&#35797;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#21151;&#33021;&#24615;&#22686;&#21152;&#23548;&#33268;&#20102;&#19968;&#31867;&#20840;&#38754;&#35780;&#20272;&#24191;&#27867;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;&#30340;&#20986;&#29616;&#12290;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#19982;&#22823;&#35268;&#27169;&#35745;&#31639;&#25104;&#26412;&#30456;&#20851;&#65292;&#27599;&#20010;&#27169;&#22411;&#38656;&#35201;&#25968;&#21315;&#20010;GPU&#23567;&#26102;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#35780;&#20272;&#25928;&#29575;&#26041;&#38754;&#30340;&#38382;&#39064;&#22312;&#25991;&#29486;&#20013;&#35752;&#35770;&#36739;&#23569;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"Efficient Benchmarking"&#30340;&#38382;&#39064;&#65292;&#21363;&#22312;&#19981;&#25439;&#23475;&#21487;&#38752;&#24615;&#30340;&#24773;&#20917;&#19979;&#26234;&#33021;&#22320;&#20943;&#23569;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#36890;&#36807;&#20351;&#29992;HELM&#22522;&#20934;&#27979;&#35797;&#20316;&#20026;&#31034;&#20363;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#22522;&#20934;&#27979;&#35797;&#35774;&#35745;&#36873;&#25321;&#22914;&#20309;&#24433;&#21709;&#35745;&#31639;-&#21487;&#38752;&#24615;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;Decision Impact on Reliability&#65288;DIoR&#65289;&#30340;&#26032;&#24230;&#37327;&#26469;&#35780;&#20272;&#36825;&#20123;&#20915;&#31574;&#30340;&#21487;&#38752;&#24615;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21457;&#29616;&#20165;&#36890;&#36807;&#20174;&#22522;&#20934;&#27979;&#35797;&#20013;&#21024;&#38500;&#19968;&#20010;&#20302;&#25490;&#21517;&#27169;&#22411;&#65292;&#24403;&#21069;&#22312;HELM&#19978;&#30340;&#39046;&#20808;&#32773;&#21487;&#33021;&#20250;&#25913;&#21464;&#65292;&#24182;&#19988;&#35266;&#23519;&#21040;&#21482;&#38656;&#19968;&#23567;&#37096;&#20998;&#31034;&#20363;&#21363;&#21487;&#33719;&#24471;&#27491;&#30830;&#30340;&#22522;&#20934;&#27979;&#35797;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing versatility of language models LMs has given rise to a new class of benchmarks that comprehensively assess a broad range of capabilities. Such benchmarks are associated with massive computational costs reaching thousands of GPU hours per model. However the efficiency aspect of these evaluation efforts had raised little discussion in the literature. In this work we present the problem of Efficient Benchmarking namely intelligently reducing the computation costs of LM evaluation without compromising reliability. Using the HELM benchmark as a test case we investigate how different benchmark design choices affect the computation-reliability tradeoff. We propose to evaluate the reliability of such decisions by using a new measure Decision Impact on Reliability DIoR for short. We find for example that the current leader on HELM may change by merely removing a low-ranked model from the benchmark and observe that a handful of examples suffice to obtain the correct benchmark rank
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#21387;&#32553;&#32508;&#36848;&#65292;&#25506;&#35752;&#20102;&#37327;&#21270;&#12289;&#20462;&#21098;&#12289;&#30693;&#35782;&#33976;&#39311;&#31561;&#19981;&#21516;&#26041;&#27861;&#65292;&#24182;&#31361;&#20986;&#20171;&#32461;&#20102;&#26368;&#26032;&#36827;&#23637;&#21644;&#21019;&#26032;&#26041;&#27861;&#65292;&#20026;&#23454;&#29616;&#39640;&#25928;&#30340;&#37096;&#32626;&#25552;&#20379;&#20102;&#37325;&#35201;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2308.07633</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#21387;&#32553;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Model Compression for Large Language Models. (arXiv:2308.07633v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#21387;&#32553;&#32508;&#36848;&#65292;&#25506;&#35752;&#20102;&#37327;&#21270;&#12289;&#20462;&#21098;&#12289;&#30693;&#35782;&#33976;&#39311;&#31561;&#19981;&#21516;&#26041;&#27861;&#65292;&#24182;&#31361;&#20986;&#20171;&#32461;&#20102;&#26368;&#26032;&#36827;&#23637;&#21644;&#21019;&#26032;&#26041;&#27861;&#65292;&#20026;&#23454;&#29616;&#39640;&#25928;&#30340;&#37096;&#32626;&#25552;&#20379;&#20102;&#37325;&#35201;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20197;&#24778;&#20154;&#30340;&#25104;&#21151;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24222;&#22823;&#30340;&#20307;&#37327;&#21644;&#35745;&#31639;&#38656;&#27714;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#19979;&#30340;&#23454;&#38469;&#37096;&#32626;&#20013;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#38543;&#30528;&#36825;&#20123;&#25361;&#25112;&#26085;&#30410;&#32039;&#36843;&#65292;&#27169;&#22411;&#21387;&#32553;&#39046;&#22495;&#24050;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#26088;&#22312;&#32531;&#35299;&#36825;&#20123;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20221;&#20840;&#38754;&#30340;&#32508;&#36848;&#65292;&#25506;&#35752;&#19987;&#38376;&#38024;&#23545;LLMs&#30340;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#12290;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#37327;&#21270;&#12289;&#20462;&#21098;&#12289;&#30693;&#35782;&#33976;&#39311;&#31561;&#65292;&#20197;&#24212;&#23545;&#39640;&#25928;&#37096;&#32626;&#30340;&#36843;&#20999;&#38656;&#27714;&#12290;&#22312;&#27599;&#31181;&#25216;&#26415;&#20013;&#65292;&#25105;&#20204;&#37325;&#28857;&#20171;&#32461;&#20102;&#26368;&#26032;&#36827;&#23637;&#21644;&#21019;&#26032;&#26041;&#27861;&#65292;&#20026;LLM&#30740;&#31350;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#36129;&#29486;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#29992;&#20110;&#35780;&#20272;&#25928;&#26524;&#30340;&#22522;&#20934;&#31574;&#30053;&#21644;&#35780;&#20272;&#25351;&#26631;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have revolutionized natural language processing tasks with remarkable success. However, their formidable size and computational demands present significant challenges for practical deployment, especially in resource-constrained environments. As these challenges become increasingly pertinent, the field of model compression has emerged as a pivotal research area to alleviate these limitations. This paper presents a comprehensive survey that navigates the landscape of model compression techniques tailored specifically for LLMs. Addressing the imperative need for efficient deployment, we delve into various methodologies, encompassing quantization, pruning, knowledge distillation, and more. Within each of these techniques, we highlight recent advancements and innovative approaches that contribute to the evolving landscape of LLM research. Furthermore, we explore benchmarking strategies and evaluation metrics that are essential for assessing the effectiveness of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#19987;&#20026;&#20202;&#34920;&#30424;&#21450;&#20854;&#35270;&#35273;&#21644;&#25968;&#25454;&#32452;&#20214;&#35774;&#35745;&#30340;&#26469;&#28304;&#34920;&#31034;&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#20379;&#23436;&#22791;&#30340;&#24517;&#35201;&#26469;&#28304;&#20803;&#25968;&#25454;&#65292;&#20197;&#20415;&#29992;&#25143;&#35780;&#20272;&#36136;&#37327;&#21644;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.06788</link><description>&lt;p&gt;
&#24314;&#27169;&#20202;&#34920;&#30424;&#30340;&#26469;&#28304;
&lt;/p&gt;
&lt;p&gt;
Modeling the Dashboard Provenance. (arXiv:2308.06788v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#19987;&#20026;&#20202;&#34920;&#30424;&#21450;&#20854;&#35270;&#35273;&#21644;&#25968;&#25454;&#32452;&#20214;&#35774;&#35745;&#30340;&#26469;&#28304;&#34920;&#31034;&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#20379;&#23436;&#22791;&#30340;&#24517;&#35201;&#26469;&#28304;&#20803;&#25968;&#25454;&#65292;&#20197;&#20415;&#29992;&#25143;&#35780;&#20272;&#36136;&#37327;&#21644;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#35770;&#26159;&#20844;&#20849;&#30340;&#36824;&#26159;&#31169;&#20154;&#30340;&#65292;&#30408;&#21033;&#30340;&#36824;&#26159;&#38750;&#30408;&#21033;&#30340;&#65292;&#21508;&#31181;&#34892;&#19994;&#21644;&#39046;&#22495;&#30340;&#32452;&#32455;&#37117;&#20381;&#38752;&#20202;&#34920;&#30424;&#36827;&#34892;&#26377;&#25928;&#30340;&#25968;&#25454;&#21487;&#35270;&#21270;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20202;&#34920;&#30424;&#30340;&#21487;&#38752;&#24615;&#21644;&#25928;&#26524;&#20381;&#36182;&#20110;&#23427;&#20204;&#25152;&#21576;&#29616;&#30340;&#35270;&#35273;&#21644;&#25968;&#25454;&#30340;&#36136;&#37327;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#19981;&#21040;&#22235;&#20998;&#20043;&#19968;&#30340;&#20202;&#34920;&#30424;&#25552;&#20379;&#26377;&#20851;&#20854;&#26469;&#28304;&#30340;&#20449;&#24687;&#65292;&#32780;&#26469;&#28304;&#26159;&#20005;&#32899;&#32771;&#34385;&#26102;&#25152;&#26399;&#26395;&#30340;&#19968;&#39033;&#20803;&#25968;&#25454;&#20043;&#19968;&#12290;&#26469;&#28304;&#26159;&#25551;&#36848;&#22312;&#25968;&#25454;&#25110;&#23545;&#35937;&#30340;&#29983;&#20135;&#12289;&#24433;&#21709;&#25110;&#20256;&#36882;&#20013;&#36215;&#21040;&#20316;&#29992;&#30340;&#20154;&#21592;&#12289;&#32452;&#32455;&#12289;&#23454;&#20307;&#21644;&#27963;&#21160;&#30340;&#35760;&#24405;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#36866;&#29992;&#20110;&#20202;&#34920;&#30424;&#21450;&#20854;&#35270;&#35273;&#21644;&#25968;&#25454;&#32452;&#20214;&#30340;&#26469;&#28304;&#34920;&#31034;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#25903;&#25345;&#26631;&#20934;&#21270;&#12289;&#24314;&#27169;&#12289;&#29983;&#25104;&#12289;&#25429;&#33719;&#21644;&#21487;&#35270;&#21270;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#23558;&#25552;&#20379;&#19968;&#22871;&#20840;&#38754;&#30340;&#24517;&#35201;&#26469;&#28304;&#20803;&#25968;&#25454;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#35780;&#20272;&#36136;&#37327;&#21644;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Organizations of all kinds, whether public or private, profit-driven or non-profit, and across various industries and sectors, rely on dashboards for effective data visualization. However, the reliability and efficacy of these dashboards rely on the quality of the visual and data they present. Studies show that less than a quarter of dashboards provide information about their sources, which is just one of the expected metadata when provenance is seriously considered. Provenance is a record that describes people, organizations, entities, and activities that had a role in the production, influence, or delivery of a piece of data or an object. This paper aims to provide a provenance representation model, that entitles standardization, modeling, generation, capture, and visualization, specifically designed for dashboards and its visual and data components. The proposed model will offer a comprehensive set of essential provenance metadata that enables users to evaluate the quality, consiste
&lt;/p&gt;</description></item><item><title>AspectMMKG&#26159;&#19968;&#20010;&#20855;&#26377;&#26041;&#38754;&#24847;&#35782;&#30340;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#65292;&#36890;&#36807;&#21305;&#37197;&#22270;&#20687;&#21644;&#19981;&#21516;&#23454;&#20307;&#26041;&#38754;&#65292;&#23427;&#25552;&#20379;&#20102;&#20174;&#22810;&#20010;&#35282;&#24230;&#29702;&#35299;&#23454;&#20307;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#23454;&#20307;&#26041;&#38754;&#38142;&#25509;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.04992</link><description>&lt;p&gt;
AspectMMKG: &#19968;&#20010;&#20855;&#26377;&#26041;&#38754;&#24847;&#35782;&#30340;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
AspectMMKG: A Multi-modal Knowledge Graph with Aspect-aware Entities. (arXiv:2308.04992v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04992
&lt;/p&gt;
&lt;p&gt;
AspectMMKG&#26159;&#19968;&#20010;&#20855;&#26377;&#26041;&#38754;&#24847;&#35782;&#30340;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#65292;&#36890;&#36807;&#21305;&#37197;&#22270;&#20687;&#21644;&#19981;&#21516;&#23454;&#20307;&#26041;&#38754;&#65292;&#23427;&#25552;&#20379;&#20102;&#20174;&#22810;&#20010;&#35282;&#24230;&#29702;&#35299;&#23454;&#20307;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#23454;&#20307;&#26041;&#38754;&#38142;&#25509;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#65288;MMKG&#65289;&#32467;&#21512;&#19981;&#21516;&#30340;&#27169;&#24577;&#25968;&#25454;&#65288;&#20363;&#22914;&#25991;&#26412;&#21644;&#22270;&#20687;&#65289;&#65292;&#20197;&#20840;&#38754;&#29702;&#35299;&#23454;&#20307;&#12290;&#23613;&#31649;&#22823;&#35268;&#27169;MMKG&#30340;&#26368;&#36817;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#30340;MMKG&#24573;&#35270;&#20102;&#23454;&#20307;&#30340;&#22810;&#26041;&#38754;&#24615;&#36136;&#65292;&#38480;&#21046;&#20102;&#20174;&#21508;&#31181;&#35282;&#24230;&#29702;&#35299;&#23454;&#20307;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;AspectMMKG&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#19982;&#26041;&#38754;&#30456;&#20851;&#30340;&#22270;&#20687;&#30340;MMKG&#65292;&#36890;&#36807;&#23558;&#22270;&#20687;&#19982;&#19981;&#21516;&#30340;&#23454;&#20307;&#26041;&#38754;&#36827;&#34892;&#21305;&#37197;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20174;&#30693;&#35782;&#24211;&#20013;&#25910;&#38598;&#19982;&#26041;&#38754;&#30456;&#20851;&#30340;&#22270;&#20687;&#65292;&#24182;&#36890;&#36807;&#22312;&#32447;&#22270;&#20687;&#25628;&#32034;&#24341;&#25806;&#25552;&#21462;&#30693;&#35782;&#24211;&#20013;&#19982;&#26041;&#38754;&#30456;&#20851;&#30340;&#21477;&#23376;&#20316;&#20026;&#26597;&#35810;&#65292;&#20197;&#26816;&#32034;&#22823;&#37327;&#19982;&#26041;&#38754;&#30456;&#20851;&#30340;&#22270;&#20687;&#12290;&#26368;&#21518;&#65292;AspectMMKG&#21253;&#21547;2380&#20010;&#23454;&#20307;&#65292;18139&#20010;&#23454;&#20307;&#26041;&#38754;&#21644;645383&#20010;&#19982;&#26041;&#38754;&#30456;&#20851;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;AspectMMKG&#22312;&#23454;&#20307;&#26041;&#38754;&#38142;&#25509;&#65288;EAL&#65289;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#21487;&#29992;&#24615;&#65292;&#24182;&#35777;&#26126;&#22312;AspectMMKG&#30340;&#24110;&#21161;&#19979;&#65292;&#20808;&#21069;&#30340;EAL&#27169;&#22411;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-modal knowledge graphs (MMKGs) combine different modal data (e.g., text and image) for a comprehensive understanding of entities. Despite the recent progress of large-scale MMKGs, existing MMKGs neglect the multi-aspect nature of entities, limiting the ability to comprehend entities from various perspectives. In this paper, we construct AspectMMKG, the first MMKG with aspect-related images by matching images to different entity aspects. Specifically, we collect aspect-related images from a knowledge base, and further extract aspect-related sentences from the knowledge base as queries to retrieve a large number of aspect-related images via an online image search engine. Finally, AspectMMKG contains 2,380 entities, 18,139 entity aspects, and 645,383 aspect-related images. We demonstrate the usability of AspectMMKG in entity aspect linking (EAL) downstream task and show that previous EAL models achieve a new state-of-the-art performance with the help of AspectMMKG. To facilitate the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#23545;&#22810;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#39044;&#27979;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#25351;&#20196;&#24494;&#35843;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#19988;&#26368;&#20248;&#24494;&#35843;&#27169;&#22411;&#22312;&#24179;&#34913;&#20934;&#30830;&#24230;&#19978;&#32988;&#36807;GPT-3.5&#65292;&#24182;&#19982;&#26368;&#20808;&#36827;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#25345;&#24179;&#12290;</title><link>http://arxiv.org/abs/2307.14385</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#22312;&#32447;&#25991;&#26412;&#25968;&#25454;&#39044;&#27979;&#24515;&#29702;&#20581;&#24247;
&lt;/p&gt;
&lt;p&gt;
Leveraging Large Language Models for Mental Health Prediction via Online Text Data. (arXiv:2307.14385v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#23545;&#22810;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#39044;&#27979;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#25351;&#20196;&#24494;&#35843;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#19988;&#26368;&#20248;&#24494;&#35843;&#27169;&#22411;&#22312;&#24179;&#34913;&#20934;&#30830;&#24230;&#19978;&#32988;&#36807;GPT-3.5&#65292;&#24182;&#19982;&#26368;&#20808;&#36827;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#25345;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25216;&#26415;&#25552;&#21319;&#20351;&#24471;&#22810;&#31181;&#24212;&#29992;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;LLM&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#30340;&#29702;&#35299;&#21644;&#25913;&#36827;&#30740;&#31350;&#20960;&#20046;&#27809;&#26377;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#20840;&#38754;&#35780;&#20272;&#20102;&#22810;&#31181;LLM&#65288;&#21253;&#25324;Alpaca&#65292;Alpaca-LoRA&#21644;GPT-3.5&#65289;&#22312;&#36890;&#36807;&#22312;&#32447;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#22810;&#20010;&#24515;&#29702;&#20581;&#24247;&#39044;&#27979;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#21253;&#25324;&#38646;-shot&#25552;&#31034;&#12289;&#23569;-shot&#25552;&#31034;&#21644;&#25351;&#20196;&#24494;&#35843;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;LLM&#22312;&#38646;-shot&#21644;&#23569;-shot&#25552;&#31034;&#35774;&#35745;&#19978;&#22312;&#24515;&#29702;&#20581;&#24247;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26377;&#38480;&#20294;&#26377;&#21069;&#26223;&#30340;&#24615;&#33021;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25351;&#20196;&#24494;&#35843;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;LLM&#22312;&#25152;&#26377;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#26368;&#22909;&#30340;&#24494;&#35843;&#27169;&#22411;&#65292;Mental-Alpaca&#65292;&#22312;&#24179;&#34913;&#20934;&#30830;&#24230;&#19978;&#27604;GPT-3.5&#65288;&#20307;&#31215;&#22823;25&#20493;&#65289;&#39640;&#20986;16.7\%&#65292;&#24182;&#19982;&#26368;&#20808;&#36827;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#25345;&#24179;&#12290;&#25105;&#20204;&#24635;&#32467;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent technology boost of large language models (LLMs) has empowered a variety of applications. However, there is very little research on understanding and improving LLMs' capability for the mental health domain. In this work, we present the first comprehensive evaluation of multiple LLMs, including Alpaca, Alpaca-LoRA, and GPT-3.5, on various mental health prediction tasks via online text data. We conduct a wide range of experiments, covering zero-shot prompting, few-shot prompting, and instruction finetuning. The results indicate the promising yet limited performance of LLMs with zero-shot and few-shot prompt designs for mental health tasks. More importantly, our experiments show that instruction finetuning can significantly boost the performance of LLMs for all tasks simultaneously. Our best-finetuned model, Mental-Alpaca, outperforms GPT-3.5 (25 times bigger) by 16.7\% on balanced accuracy and performs on par with the state-of-the-art task-specific model. We summarize our find
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#24503;&#35821;&#21477;&#27861;&#29305;&#24449;&#22312;&#21271;&#19996;&#24847;&#22823;&#21033;&#26041;&#35328;&#20013;&#30340;&#20256;&#25773;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#25968;&#23398;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#22320;&#29702;&#25968;&#25454;&#31185;&#23398;&#24037;&#20855;&#29983;&#25104;&#20102;&#20132;&#20114;&#24335;&#22320;&#22270;&#12290;</title><link>http://arxiv.org/abs/2307.14291</link><description>&lt;p&gt;
&#22312;&#35821;&#35328;&#23398;&#20013;&#21019;&#31435;&#19968;&#20010;&#25968;&#23398;&#25193;&#25955;&#27169;&#22411;&#12290;&#21271;&#19996;&#24847;&#22823;&#21033;&#26041;&#35328;&#20013;&#24503;&#35821;&#21477;&#27861;&#29305;&#24449;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Founding a mathematical diffusion model in linguistics. The case study of German syntactic features in the North-Eastern Italian dialects. (arXiv:2307.14291v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14291
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#24503;&#35821;&#21477;&#27861;&#29305;&#24449;&#22312;&#21271;&#19996;&#24847;&#22823;&#21033;&#26041;&#35328;&#20013;&#30340;&#20256;&#25773;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#25968;&#23398;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#22320;&#29702;&#25968;&#25454;&#31185;&#23398;&#24037;&#20855;&#29983;&#25104;&#20102;&#20132;&#20114;&#24335;&#22320;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20197;&#21271;&#19996;&#24847;&#22823;&#21033;&#32599;&#26364;&#26041;&#35328;&#20013;&#24503;&#35821;&#21477;&#27861;&#29305;&#24449;&#30340;&#20256;&#25773;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#35813;&#29305;&#24449;&#22312;&#20013;&#19990;&#32426;&#39640;&#20013;&#19990;&#32426;&#26102;&#26399;&#30340;&#33922;&#32599;&#23572;&#24503;&#22269;&#20154;&#31227;&#27665;&#21518;&#21457;&#29983;&#12290;&#20351;&#29992;&#22320;&#29702;&#25968;&#25454;&#31185;&#23398;&#24037;&#20855;&#32472;&#21046;&#20986;&#19968;&#20010;&#20132;&#20114;&#24335;&#22320;&#22270;&#12290;&#19968;&#20010;&#24179;&#28369;&#30340;&#20108;&#32500;&#26354;&#38754;$\mathcal{G}$&#34920;&#31034;&#24403;&#22320;&#20351;&#29992;&#32473;&#23450;&#24503;&#35821;&#35821;&#35328;&#29305;&#24449;&#30340;&#39046;&#22303;&#27604;&#20363;&#65306;&#36890;&#36807;&#23545;&#34920;&#31034;&#35813;&#29305;&#24449;&#22312;&#20219;&#20309;&#35843;&#26597;&#22320;&#28857;&#26159;&#21542;&#20351;&#29992;&#30340;&#31163;&#25955;&#20989;&#25968;&#36827;&#34892;&#25554;&#20540;&#24471;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We take as a case study the spread of Germanic syntactic features into Romance dialects of North-Eastern Italy, which occurred after the immigration of German people in the Tyrol during the High Middle Ages.  An interactive map is produced using tools of what is called Geographic Data Science. A smooth two-dimensional surface $\mathcal{G}$ expresses locally which fraction of territory uses a given German language feature: it is obtained by interpolating a discrete function that says if at any surveyed locality that feature is used or not.\newline  This surface $\mathcal{G}$ is thought of as the value at the present time of a function describing a diffusion-convection phenomenon in two dimensions (here said \emph{tidal} mode), which is subjected in a very natural way to the same equation, suitably contextualized, used in physics for a number of phenomenological facts like the heat diffusion. It is shown that solutions of this equation, evaluated at the present time, fit well with the da
&lt;/p&gt;</description></item><item><title>vONTSS&#26159;&#19968;&#31181;&#22522;&#20110;vMF&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#21322;&#30417;&#30563;&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#23427;&#22312;&#20998;&#31867;&#20934;&#30830;&#29575;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#19988;&#25903;&#25345;&#26080;&#30417;&#30563;&#20027;&#39064;&#24314;&#27169;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;vONTSS&#27604;&#26368;&#36817;&#30340;NTM&#26356;&#24555;&#12290;</title><link>http://arxiv.org/abs/2307.01226</link><description>&lt;p&gt;
vONTSS&#65306;&#22522;&#20110;vMF&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#21322;&#30417;&#30563;&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
vONTSS: vMF based semi-supervised neural topic modeling with optimal transport. (arXiv:2307.01226v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01226
&lt;/p&gt;
&lt;p&gt;
vONTSS&#26159;&#19968;&#31181;&#22522;&#20110;vMF&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#21322;&#30417;&#30563;&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#23427;&#22312;&#20998;&#31867;&#20934;&#30830;&#29575;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#19988;&#25903;&#25345;&#26080;&#30417;&#30563;&#20027;&#39064;&#24314;&#27169;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;vONTSS&#27604;&#26368;&#36817;&#30340;NTM&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#21463;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#21551;&#21457;&#30340;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65288;NTM&#65289;&#24341;&#36215;&#20102;&#24456;&#22810;&#30740;&#31350;&#20852;&#36259;&#65292;&#28982;&#32780;&#65292;&#30001;&#20110;&#25972;&#21512;&#20154;&#31867;&#30693;&#35782;&#30340;&#25361;&#25112;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;vONTSS&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;von Mises-Fisher&#65288;vMF&#65289;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#21644;&#26368;&#20248;&#20256;&#36755;&#12290;&#22312;&#21322;&#30417;&#30563;&#35774;&#32622;&#20013;&#65292;&#24403;&#25552;&#20379;&#27599;&#20010;&#20027;&#39064;&#30340;&#23569;&#37327;&#20851;&#38190;&#35789;&#26102;&#65292;vONTSS&#29983;&#25104;&#28508;&#22312;&#20027;&#39064;&#24182;&#20248;&#21270;&#20027;&#39064;-&#20851;&#38190;&#35789;&#36136;&#37327;&#21644;&#20027;&#39064;&#20998;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;vONTSS&#22312;&#20998;&#31867;&#20934;&#30830;&#29575;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#21322;&#30417;&#30563;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#12290;vONTSS&#36824;&#25903;&#25345;&#26080;&#30417;&#30563;&#20027;&#39064;&#24314;&#27169;&#12290;&#23450;&#37327;&#21644;&#23450;&#24615;&#23454;&#39564;&#35777;&#26126;&#65292;vONTSS&#22312;&#26080;&#30417;&#30563;&#35774;&#32622;&#19979;&#22312;&#22810;&#20010;&#26041;&#38754;&#20248;&#20110;&#26368;&#36817;&#30340;NTM&#65306;vONTSS&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21457;&#29616;&#39640;&#24230;&#32858;&#31867;&#21644;&#36830;&#36143;&#30340;&#20027;&#39064;&#12290;&#23427;&#20063;&#27604;&#29616;&#26377;-&#25163;&#27861;&#24555;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Neural Topic Models (NTM), inspired by variational autoencoders, have attracted a lot of research interest; however, these methods have limited applications in the real world due to the challenge of incorporating human knowledge. This work presents a semi-supervised neural topic modeling method, vONTSS, which uses von Mises-Fisher (vMF) based variational autoencoders and optimal transport. When a few keywords per topic are provided, vONTSS in the semi-supervised setting generates potential topics and optimizes topic-keyword quality and topic classification. Experiments show that vONTSS outperforms existing semi-supervised topic modeling methods in classification accuracy and diversity. vONTSS also supports unsupervised topic modeling. Quantitative and qualitative experiments show that vONTSS in the unsupervised setting outperforms recent NTMs on multiple aspects: vONTSS discovers highly clustered and coherent topics on benchmark datasets. It is also much faster than the state
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#23618;&#19987;&#23478;&#29983;&#25104;SQL&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#19987;&#29992;&#30340;&#22810;&#20219;&#21153;&#20998;&#23618;&#32593;&#32476;&#65292;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#30001;&#20110;&#19981;&#21516;&#20998;&#31867;&#20219;&#21153;&#30340;&#36127;&#36801;&#31227;&#38382;&#39064;&#23548;&#33268;&#29983;&#25104;&#19981;&#20934;&#30830;SQL&#35821;&#21477;&#30340;&#38480;&#21046;&#12290;&#35813;&#26041;&#27861;&#22312;WiKSQL&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.17727</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#23618;&#19987;&#23478;&#32593;&#32476;&#30340;&#25913;&#36827;NL2SQL&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Improved NL2SQL based on Multi-layer Expert Network. (arXiv:2306.17727v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17727
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#23618;&#19987;&#23478;&#29983;&#25104;SQL&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#19987;&#29992;&#30340;&#22810;&#20219;&#21153;&#20998;&#23618;&#32593;&#32476;&#65292;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#30001;&#20110;&#19981;&#21516;&#20998;&#31867;&#20219;&#21153;&#30340;&#36127;&#36801;&#31227;&#38382;&#39064;&#23548;&#33268;&#29983;&#25104;&#19981;&#20934;&#30830;SQL&#35821;&#21477;&#30340;&#38480;&#21046;&#12290;&#35813;&#26041;&#27861;&#22312;WiKSQL&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#21040;SQL&#65288;NL2SQL&#65289;&#25216;&#26415;&#29992;&#20110;&#23558;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#36716;&#25442;&#20026;&#21487;&#25191;&#34892;&#30340;SQL&#35821;&#21477;&#12290;&#36890;&#24120;&#65292;&#36890;&#36807;&#25554;&#27133;&#22635;&#20805;&#20316;&#20026;&#22810;&#20219;&#21153;&#20998;&#31867;&#26041;&#27861;&#26469;&#23454;&#29616;&#27492;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#19981;&#21516;&#20998;&#31867;&#20219;&#21153;&#30340;&#36127;&#36801;&#31227;&#38382;&#39064;&#65292;&#25554;&#27133;&#22635;&#20805;&#21487;&#33021;&#23548;&#33268;&#29983;&#25104;&#19981;&#20934;&#30830;&#30340;SQL&#35821;&#21477;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#23618;&#19987;&#23478;&#29983;&#25104;SQL&#65288;MLEG-SQL&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#19987;&#29992;&#30340;&#22810;&#20219;&#21153;&#20998;&#23618;&#32593;&#32476;&#12290;&#32593;&#32476;&#30340;&#19979;&#23618;&#25552;&#21462;&#33258;&#28982;&#35821;&#35328;&#35821;&#21477;&#30340;&#35821;&#20041;&#29305;&#24449;&#65292;&#32780;&#19978;&#23618;&#26500;&#24314;&#19968;&#20010;&#19987;&#38376;&#30340;&#19987;&#23478;&#31995;&#32479;&#26469;&#22788;&#29702;&#29305;&#23450;&#30340;&#20998;&#31867;&#20219;&#21153;&#12290;&#36825;&#31181;&#20998;&#23618;&#26041;&#27861;&#20943;&#36731;&#20102;&#19981;&#21516;&#20219;&#21153;&#20914;&#31361;&#24102;&#26469;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#35813;&#26041;&#27861;&#22312;WiKSQL&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#35777;&#26126;&#22312;&#29983;&#25104;&#20934;&#30830;&#30340;SQL&#35821;&#21477;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Natural Language to SQL (NL2SQL) technique is used to convert natural language queries into executable SQL statements. Typically, slot-filling is employed as a classification method for multi-task cases to achieve this goal. However, slot-filling can result in inaccurate SQL statement generation due to negative migration issues arising from different classification tasks. To overcome this limitation, this study introduces a new approach called Multi-Layer Expert Generate SQL (MLEG-SQL), which utilizes a dedicated multi-task hierarchical network. The lower layer of the network extracts semantic features of natural language statements, while the upper layer builds a specialized expert system for handling specific classification tasks. This hierarchical approach mitigates performance degradation resulting from different task conflicts. The proposed method was evaluated on the WiKSQL dataset and was found to be effective in generating accurate SQL statements.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#29983;&#25104;&#36830;&#32493;&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#30340;&#26041;&#27861;&#65288;TESGAN&#65289;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;GAN&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#38480;&#21046;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#36830;&#32493;&#30340;&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#21462;&#20195;&#31163;&#25955;&#30340;&#26631;&#35760;&#65292;&#20351;&#24471;&#29983;&#25104;&#22120;&#22312;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#26356;&#26032;&#26799;&#24230;&#26102;&#26356;&#21152;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2306.17181</link><description>&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#26080;&#30417;&#30563;&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#29992;&#20110;&#25991;&#26412;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Text Embedding Space Generation Using Generative Adversarial Networks for Text Synthesis. (arXiv:2306.17181v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#29983;&#25104;&#36830;&#32493;&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#30340;&#26041;&#27861;&#65288;TESGAN&#65289;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;GAN&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#38480;&#21046;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#36830;&#32493;&#30340;&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#21462;&#20195;&#31163;&#25955;&#30340;&#26631;&#35760;&#65292;&#20351;&#24471;&#29983;&#25104;&#22120;&#22312;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#26356;&#26032;&#26799;&#24230;&#26102;&#26356;&#21152;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#25968;&#25454;&#21512;&#25104;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#29983;&#25104;&#22120;&#21644;&#21028;&#21035;&#22120;&#30340;&#31454;&#20105;&#26469;&#21019;&#24314;&#36924;&#30495;&#30340;&#25968;&#25454;&#12290;&#23613;&#31649;GAN&#22312;&#22270;&#20687;&#21512;&#25104;&#26041;&#38754;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#23384;&#22312;&#22266;&#26377;&#30340;&#38480;&#21046;&#12290;&#22240;&#20026;&#33258;&#28982;&#35821;&#35328;&#30001;&#31163;&#25955;&#30340;&#26631;&#35760;&#32452;&#25104;&#65292;&#29983;&#25104;&#22120;&#22312;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#26356;&#26032;&#26799;&#24230;&#26102;&#36935;&#21040;&#22256;&#38590;&#65307;&#22240;&#27492;&#65292;&#22823;&#22810;&#25968;&#25991;&#26412;-GAN&#30740;&#31350;&#20351;&#29992;&#22870;&#21169;&#31995;&#32479;&#20197;&#38543;&#26426;&#26631;&#35760;&#20026;&#22522;&#30784;&#29983;&#25104;&#21477;&#23376;&#12290;&#22240;&#27492;&#65292;&#20808;&#21069;&#30740;&#31350;&#20013;&#30340;&#29983;&#25104;&#22120;&#22312;&#23545;&#25239;&#35757;&#32451;&#20043;&#21069;&#20197;&#33258;&#22238;&#24402;&#26041;&#24335;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#23548;&#33268;&#21512;&#25104;&#30340;&#21477;&#23376;&#37325;&#22797;&#35757;&#32451;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#31867;&#20284;&#21407;&#22987;GAN&#30340;&#26694;&#26550;&#26469;&#21512;&#25104;&#21477;&#23376;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;TESGAN&#65289;&#65292;&#23427;&#29983;&#25104;&#36830;&#32493;&#30340;&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#26469;&#35299;&#20915;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Networks (GAN) is a model for data synthesis, which creates plausible data through the competition of generator and discriminator. Although GAN application to image synthesis is extensively studied, it has inherent limitations to natural language generation. Because natural language is composed of discrete tokens, a generator has difficulty updating its gradient through backpropagation; therefore, most text-GAN studies generate sentences starting with a random token based on a reward system. Thus, the generators of previous studies are pre-trained in an autoregressive way before adversarial training, causing data memorization that synthesized sentences reproduce the training data. In this paper, we synthesize sentences using a framework similar to the original GAN. More specifically, we propose Text Embedding Space Generative Adversarial Networks (TESGAN) which generate continuous text embedding spaces instead of discrete tokens to solve the gradient backpropagat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#32431;&#31929;&#30340;&#21333;&#35821;&#25968;&#25454;&#28304;&#35757;&#32451;&#21452;&#35821;&#21644;&#20195;&#30721;&#20999;&#25442;ASR&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#38598;&#21512;&#26631;&#35760;&#22120;&#65292;&#23558;LID&#24212;&#29992;&#21040;&#27599;&#20010;&#26631;&#35760;&#65292;&#32780;&#19981;&#26159;&#22312;&#21333;&#35821;&#26679;&#26412;&#36793;&#30028;&#29983;&#25104;LID&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#38598;&#21512;&#26631;&#35760;&#22120;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#21512;&#25104;&#20195;&#30721;&#20999;&#25442;ASR&#25968;&#25454;&#29983;&#25104;&#25216;&#26415;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#20195;&#30721;&#20999;&#25442;ASR&#27169;&#22411;&#22312;&#35821;&#38899;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.08753</link><description>&lt;p&gt;
&#20174;&#21333;&#35821;&#25968;&#25454;&#28304;&#20013;&#35757;&#32451;&#21452;&#35821;&#21644;&#20195;&#30721;&#20999;&#25442;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards training Bilingual and Code-Switched Speech Recognition models from Monolingual data sources. (arXiv:2306.08753v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08753
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#32431;&#31929;&#30340;&#21333;&#35821;&#25968;&#25454;&#28304;&#35757;&#32451;&#21452;&#35821;&#21644;&#20195;&#30721;&#20999;&#25442;ASR&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#38598;&#21512;&#26631;&#35760;&#22120;&#65292;&#23558;LID&#24212;&#29992;&#21040;&#27599;&#20010;&#26631;&#35760;&#65292;&#32780;&#19981;&#26159;&#22312;&#21333;&#35821;&#26679;&#26412;&#36793;&#30028;&#29983;&#25104;LID&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#38598;&#21512;&#26631;&#35760;&#22120;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#21512;&#25104;&#20195;&#30721;&#20999;&#25442;ASR&#25968;&#25454;&#29983;&#25104;&#25216;&#26415;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#20195;&#30721;&#20999;&#25442;ASR&#27169;&#22411;&#22312;&#35821;&#38899;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#33021;&#22815;&#36716;&#24405;&#22810;&#31181;&#35821;&#35328;&#30340;&#38899;&#39057;&#65292;&#28040;&#38500;&#20102;&#20351;&#29992;&#19981;&#21516;&#27169;&#22411;&#30340;&#38656;&#35201;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#36824;&#33021;&#36827;&#34892;&#35821;&#35328;&#35782;&#21035;&#65288;LID&#65289;&#21644;&#22788;&#29702;&#20195;&#30721;&#20999;&#25442;&#35821;&#38899;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#31232;&#32570;&#30340;&#20195;&#30721;&#20999;&#25442;&#21644;&#22810;&#35821;&#38899;&#25968;&#25454;&#35821;&#26009;&#24211;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;&#20351;&#29992;&#32431;&#31929;&#30340;&#21333;&#35821;&#25968;&#25454;&#28304;&#35757;&#32451;&#21452;&#35821;&#21644;&#20195;&#30721;&#20999;&#25442;ASR&#27169;&#22411;&#30340;&#19981;&#21516;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#38598;&#21512;&#26631;&#35760;&#22120;&#30340;&#27010;&#24565;&#65292;&#23427;&#19982;&#30446;&#21069;&#20027;&#27969;&#25216;&#26415;&#22312;&#21333;&#35821;&#26679;&#26412;&#36793;&#30028;&#29983;&#25104;LID&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#32780;&#26159;&#20026;&#27599;&#20010;&#21457;&#23556;&#30340;&#26631;&#35760;&#29983;&#25104;LID&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#21452;&#35821;&#21644;&#21333;&#35821;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#38598;&#21512;&#26631;&#35760;&#22120;&#30340;&#26377;&#25928;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21512;&#25104;&#30340;&#20195;&#30721;&#20999;&#25442;ASR&#25968;&#25454;&#29983;&#25104;&#25216;&#26415;&#65292;&#24182;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#20195;&#30721;&#20999;&#25442;ASR&#27169;&#22411;&#22312;&#35821;&#38899;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual Automatic Speech Recognition (ASR) models are capable of transcribing audios across multiple languages, eliminating the need for separate models. In addition, they can perform Language Identification (LID) and handle code-switched speech. However, training these models requires special code-switch and multilingual speech corpora which are sparsely available. In this paper, we evaluate different approaches towards training of bilingual as well as code-switched ASR models using purely monolingual data sources. We introduce the concept of aggregate tokenizers that differs from the current prevalent technique of generating LIDs at the boundaries of monolingual samples and produces LID for each emitted token instead. We compare bilingual and monolingual model performance, showcase the efficacy of aggregate tokenizers, present a synthetic code-switched ASR data generation technique and demonstrate the effectiveness of the proposed code-switched ASR models for the tasks of speech
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#26679;&#24615;&#20248;&#21270;&#31639;&#27861;&#30456;&#32467;&#21512;&#30340; LLMatic &#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#36827;&#34892;&#27979;&#35797;&#65292;&#20165;&#36827;&#34892;2000&#27425;&#25628;&#32034;&#21363;&#21487;&#20135;&#29983;&#39640;&#24615;&#33021;&#32593;&#32476;&#65292;&#21363;&#20351;&#27809;&#26377;&#35813;&#22522;&#20934;&#39046;&#22495;&#30340;&#20808;&#21069;&#30693;&#35782;&#25110;&#20219;&#20309;&#20808;&#21069;&#30340;&#26368;&#20339;&#32467;&#26524;&#30340;&#26333;&#20809;&#12290;</title><link>http://arxiv.org/abs/2306.01102</link><description>&lt;p&gt;
LLMatic: &#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#26679;&#24615;&#20248;&#21270;&#30340;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
LLMatic: Neural Architecture Search via Large Language Models and Quality-Diversity Optimization. (arXiv:2306.01102v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#26679;&#24615;&#20248;&#21270;&#31639;&#27861;&#30456;&#32467;&#21512;&#30340; LLMatic &#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#36827;&#34892;&#27979;&#35797;&#65292;&#20165;&#36827;&#34892;2000&#27425;&#25628;&#32034;&#21363;&#21487;&#20135;&#29983;&#39640;&#24615;&#33021;&#32593;&#32476;&#65292;&#21363;&#20351;&#27809;&#26377;&#35813;&#22522;&#20934;&#39046;&#22495;&#30340;&#20808;&#21069;&#30693;&#35782;&#25110;&#20219;&#20309;&#20808;&#21069;&#30340;&#26368;&#20339;&#32467;&#26524;&#30340;&#26333;&#20809;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#24050;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#23436;&#25104;&#24191;&#27867;&#30340;&#20219;&#21153;&#12290;&#23427;&#20204;&#30340;&#33021;&#21147;&#28085;&#30422;&#20102;&#35768;&#22810;&#39046;&#22495;&#65292;&#23427;&#20204;&#22312;&#20195;&#30721;&#29983;&#25104;&#39046;&#22495;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#12290;&#22312;&#27492;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23558; LLMs &#35270;&#20026;&#21464;&#24322;&#21644;&#20132;&#21449;&#24037;&#20855;&#12290;&#21516;&#26102;&#65292;&#22810;&#26679;&#24615;&#20248;&#21270;&#31639;&#27861;&#24050;&#30693;&#21487;&#20197;&#21457;&#29616;&#22810;&#26679;&#24615;&#21644;&#31283;&#20581;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#23558; LLMs &#30340;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#19982; QD &#35299;&#20915;&#26041;&#26696;&#30340;&#22810;&#26679;&#24615;&#21644;&#40065;&#26834;&#24615;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; LLMatic&#65292;&#19968;&#20010;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034; (NAS) &#31639;&#27861;&#12290;&#34429;&#28982; LLMs &#36890;&#36807;&#25552;&#31034;&#30452;&#25509;&#36827;&#34892; NAS &#32771;&#39564;&#22256;&#38590;&#65292;&#20294; LLMatic &#21033;&#29992;&#31243;&#24207;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992; QD &#26469;&#36827;&#34892;&#25552;&#31034;&#21644;&#32593;&#32476;&#32467;&#26500;&#65292;&#20174;&#32780;&#21019;&#24314;&#22810;&#26679;&#24615;&#21644;&#39640;&#24615;&#33021;&#32593;&#32476;&#12290;&#25105;&#20204;&#22312; CIFAR-10 &#22270;&#20687;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#27979;&#35797;&#20102; LLMatic&#65292;&#35777;&#26126;&#23427;&#21487;&#20197;&#22312;&#20165;&#36827;&#34892; 2000 &#27425;&#25628;&#32034;&#30340;&#24773;&#20917;&#19979;&#20135;&#29983;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32593;&#32476;&#65292;&#21363;&#20351;&#27809;&#26377;&#35813;&#22522;&#20934;&#39046;&#22495;&#30340;&#20808;&#21069;&#30693;&#35782;&#25110;&#20219;&#20309;&#20808;&#21069;&#30340;&#26368;&#20339;&#32467;&#26524;&#30340;&#26333;&#20809;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have emerged as powerful tools capable of accomplishing a broad spectrum of tasks. Their abilities span numerous areas, and one area where they have made a significant impact is in the domain of code generation. In this context, we view LLMs as mutation and crossover tools. Meanwhile, Quality-Diversity (QD) algorithms are known to discover diverse and robust solutions. By merging the code-generating abilities of LLMs with the diversity and robustness of QD solutions, we introduce LLMatic, a Neural Architecture Search (NAS) algorithm. While LLMs struggle to conduct NAS directly through prompts, LLMatic uses a procedural approach, leveraging QD for prompts and network architecture to create diverse and highly performant networks. We test LLMatic on the CIFAR-10 image classification benchmark, demonstrating that it can produce competitive networks with just $2,000$ searches, even without prior knowledge of the benchmark domain or exposure to any previous top-p
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25209;&#21028;&#24615;&#22320;&#30740;&#31350;&#20102;&#24369;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#30340;&#22909;&#22788;&#34987;&#39640;&#20272;&#20102;&#65292;&#22823;&#22810;&#25968;&#20248;&#21183;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#22320;&#21033;&#29992;&#24178;&#20928;&#30340;&#35757;&#32451;&#25968;&#25454;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.17442</link><description>&lt;p&gt;
&#27604;&#20320;&#24819;&#30340;&#35201;&#24369;&#65306;&#23545;&#24369;&#30417;&#30563;&#23398;&#20064;&#30340;&#25209;&#21028;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Weaker Than You Think: A Critical Look at Weakly Supervised Learning. (arXiv:2305.17442v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17442
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25209;&#21028;&#24615;&#22320;&#30740;&#31350;&#20102;&#24369;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#30340;&#22909;&#22788;&#34987;&#39640;&#20272;&#20102;&#65292;&#22823;&#22810;&#25968;&#20248;&#21183;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#22320;&#21033;&#29992;&#24178;&#20928;&#30340;&#35757;&#32451;&#25968;&#25454;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24369;&#30417;&#30563;&#23398;&#20064;&#26159;&#19968;&#31181;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#27969;&#34892;&#26041;&#27861;&#12290;&#23427;&#20801;&#35768;&#20351;&#29992;&#20174;&#21508;&#31181;&#24369;&#26631;&#27880;&#28304;&#33719;&#24471;&#30340;&#22024;&#26434;&#26631;&#27880;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#32780;&#19981;&#26159;&#35201;&#27714;&#39640;&#36136;&#37327;&#20294;&#26114;&#36149;&#30340;&#20154;&#24037;&#26631;&#27880;&#12290;&#26368;&#36817;&#65292;&#35768;&#22810;&#31934;&#24039;&#30340;&#26041;&#27861;&#24050;&#32463;&#34987;&#25552;&#20986;&#26469;&#22312;&#26631;&#31614;&#22122;&#22768;&#19979;&#36827;&#34892;&#24378;&#22823;&#30340;&#35757;&#32451;&#65292;&#24182;&#25253;&#21578;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#35774;&#32622;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#25152;&#24102;&#26469;&#30340;&#22909;&#22788;&#34987;&#26174;&#33879;&#39640;&#20272;&#20102;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#24369;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30340;&#25104;&#21151;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#21487;&#29992;&#30340;&#24178;&#20928;&#39564;&#35777;&#26679;&#26412;&#65292;&#27491;&#22914;&#25105;&#20204;&#25152;&#23637;&#31034;&#30340;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#22320;&#22312;&#20854;&#19978;&#36827;&#34892;&#35757;&#32451;&#26469;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#36825;&#20123;&#24178;&#20928;&#26631;&#31614;&#12290;&#22312;&#20351;&#29992;&#36825;&#20123;&#24178;&#20928;&#26631;&#31614;&#36827;&#34892;&#35757;&#32451;&#21518;&#65292;&#20351;&#29992;&#36825;&#20123;&#31934;&#24039;&#26041;&#27861;&#30340;&#20248;&#21183;&#22823;&#37096;&#20998;&#34987;&#28040;&#38500;&#20102;&#12290;&#21363;&#20351;&#23558;&#21487;&#29992;&#30340;&#24178;&#20928;&#25968;&#25454;&#30340;&#22823;&#23567;&#20943;&#23569;&#21040;&#27599;&#31867;&#21482;&#26377;&#20116;&#20010;&#26679;&#26412;&#65292;&#36825;&#20173;&#28982;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weakly supervised learning is a popular approach for training machine learning models in low-resource settings. Instead of requesting high-quality yet costly human annotations, it allows training models with noisy annotations obtained from various weak sources. Recently, many sophisticated approaches have been proposed for robust training under label noise, reporting impressive results. In this paper, we revisit the setup of these approaches and find that the benefits brought by these approaches are significantly overestimated. Specifically, we find that the success of existing weakly supervised learning approaches heavily relies on the availability of clean validation samples which, as we show, can be leveraged much more efficiently by simply training on them. After using these clean labels in training, the advantages of using these sophisticated approaches are mostly wiped out. This remains true even when reducing the size of the available clean data to just five samples per class, m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20174;&#24515;&#29702;&#23398;&#26500;&#36896;&#20013;&#25552;&#21462;&#30340;&#21487;&#35299;&#37322;&#24230;&#37327;&#26631;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#23545;&#35805;&#31995;&#32479;&#65292;&#36890;&#36807;&#24773;&#32490;&#29109;&#12289;&#35821;&#35328;&#39118;&#26684;&#21644;&#24773;&#24863;&#21305;&#37197;&#12289;&#23452;&#20154;&#24615;&#21644;&#20849;&#24773;&#31561;&#20116;&#20010;&#24230;&#37327;&#65292;&#36825;&#20123;&#20154;&#31867;&#24230;&#37327;&#26631;&#20934;&#19982;&#29616;&#26377;&#30340;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#19981;&#30456;&#20851;&#19988;&#20855;&#26377;&#26356;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.14757</link><description>&lt;p&gt;
&#38754;&#21521;&#20154;&#31867;&#20013;&#24515;&#30340;&#24230;&#37327;&#35780;&#20272;&#23545;&#35805;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Human-Centered Metrics for Dialog System Evaluation. (arXiv:2305.14757v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14757
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20174;&#24515;&#29702;&#23398;&#26500;&#36896;&#20013;&#25552;&#21462;&#30340;&#21487;&#35299;&#37322;&#24230;&#37327;&#26631;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#23545;&#35805;&#31995;&#32479;&#65292;&#36890;&#36807;&#24773;&#32490;&#29109;&#12289;&#35821;&#35328;&#39118;&#26684;&#21644;&#24773;&#24863;&#21305;&#37197;&#12289;&#23452;&#20154;&#24615;&#21644;&#20849;&#24773;&#31561;&#20116;&#20010;&#24230;&#37327;&#65292;&#36825;&#20123;&#20154;&#31867;&#24230;&#37327;&#26631;&#20934;&#19982;&#29616;&#26377;&#30340;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#19981;&#30456;&#20851;&#19988;&#20855;&#26377;&#26356;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24515;&#29702;&#23398;&#35282;&#24230;&#26469;&#35780;&#20272;&#23545;&#35805;&#31995;&#32479;&#30340;&#24230;&#37327;&#26041;&#27861;&#65306;&#23545;&#35805;&#20195;&#29702;&#20154;&#20687;&#20154;&#31867;&#19968;&#26679;&#34920;&#36798;&#20102;&#22810;&#31181;&#29366;&#24577;&#65288;&#30701;&#26399;&#22240;&#32032;&#65292;&#22914;&#24773;&#32490;&#65289;&#21644;&#29305;&#36136;&#65288;&#26356;&#38271;&#26399;&#22240;&#32032;&#65292;&#22914;&#20010;&#24615;&#65289;&#12290;&#36825;&#20123;&#21487;&#35299;&#37322;&#30340;&#24230;&#37327;&#26631;&#20934;&#30001;&#26469;&#33258;&#24050;&#24314;&#31435;&#30340;&#24515;&#29702;&#23398;&#26500;&#36896;&#30340;&#20116;&#31181;&#24230;&#37327;&#32452;&#25104;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#23545;&#35805;&#21644;&#23545;&#35805;&#20013;&#30340;&#27599;&#20010;&#22238;&#21512;&#65306;&#24773;&#32490;&#29109;&#65292;&#35821;&#35328;&#39118;&#26684;&#21644;&#24773;&#24863;&#21305;&#37197;&#65292;&#20197;&#21450;&#23452;&#20154;&#24615;&#21644;&#20849;&#24773;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#20154;&#31867;&#24230;&#37327;&#26631;&#20934;&#19982;6&#31181;&#26368;&#20808;&#36827;&#30340;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#65288;&#20363;&#22914;BARTScore&#21644;BLEURT&#65289;&#22312;7&#20010;&#26631;&#20934;&#23545;&#35805;&#31995;&#32479;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#21363;Three Bot Dialog Evaluation Corpus&#65292;&#20854;&#20013;&#21253;&#21547;&#26469;&#33258;ChatGPT&#12289;GPT-3&#21644;BlenderBot&#30340;&#24050;&#27880;&#37322;&#23545;&#35805;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#20154;&#31867;&#24230;&#37327;&#26631;&#20934;&#25552;&#20379;&#20102;&#26032;&#39062;&#30340;&#20449;&#24687;&#65292;&#19982;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#19981;&#30456;&#20851;&#65292;&#24182;&#21487;&#22312;&#39044;&#27979;&#23545;&#35805;&#31995;&#32479;&#36136;&#37327;&#26102;&#36229;&#36234;&#29616;&#26377;&#30340;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present metrics for evaluating dialog systems through a psychologically-grounded "human" lens: conversational agents express a diversity of both states (short-term factors like emotions) and traits (longer-term factors like personality) just as people do. These interpretable metrics consist of five measures from established psychology constructs that can be applied both across dialogs and on turns within dialogs: emotional entropy, linguistic style and emotion matching, as well as agreeableness and empathy. We compare these human metrics against 6 state-of-the-art automatic metrics (e.g. BARTScore and BLEURT) on 7 standard dialog system data sets. We also introduce a novel data set, the Three Bot Dialog Evaluation Corpus, which consists of annotated conversations from ChatGPT, GPT-3, and BlenderBot. We demonstrate the proposed human metrics offer novel information, are uncorrelated with automatic metrics, and lead to increased accuracy beyond existing automatic metrics for predictin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#25688;&#35201;&#22686;&#24378;&#30340;&#22823;&#32434;&#30417;&#30563;&#29983;&#25104;&#26694;&#26550;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#29983;&#25104;&#26126;&#30830;&#21644;&#21512;&#29702;&#30340;&#22823;&#32434;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26174;&#24335;&#22823;&#32434;&#25511;&#21046;&#26041;&#27861;&#20197;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#29983;&#25104;&#30340;&#22823;&#32434;&#12290;</title><link>http://arxiv.org/abs/2305.14459</link><description>&lt;p&gt;
&#36890;&#36807;&#25688;&#35201;&#20108;&#20803;&#24615;&#21644;&#26174;&#24335;&#22823;&#32434;&#25511;&#21046;&#22686;&#24378;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Enhancing Generation through Summarization Duality and Explicit Outline Control. (arXiv:2305.14459v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#25688;&#35201;&#22686;&#24378;&#30340;&#22823;&#32434;&#30417;&#30563;&#29983;&#25104;&#26694;&#26550;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#29983;&#25104;&#26126;&#30830;&#21644;&#21512;&#29702;&#30340;&#22823;&#32434;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26174;&#24335;&#22823;&#32434;&#25511;&#21046;&#26041;&#27861;&#20197;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#29983;&#25104;&#30340;&#22823;&#32434;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#24320;&#25918;&#24335;&#38271;&#25991;&#26412;&#29983;&#25104;&#38754;&#20020;&#35821;&#20041;&#19981;&#36830;&#36143;&#21644;&#24773;&#33410;&#19981;&#21487;&#20449;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#36890;&#24120;&#36890;&#36807;&#35774;&#35745;&#26080;&#30417;&#30563;&#20219;&#21153;&#20013;&#30340;&#30701;&#35821;&#25110;&#25277;&#35937;&#20449;&#21495;&#30340;&#22823;&#32434;&#26469;&#32531;&#35299;&#27492;&#38382;&#39064;&#65292;&#20294;&#36825;&#24448;&#24448;&#26159;&#19981;&#31283;&#23450;&#19988;&#38590;&#20197;&#35299;&#37322;&#30340;&#12290;&#22312;&#20551;&#35774;&#25688;&#35201;&#20316;&#20026;&#24050;&#25104;&#29087;&#30340;&#22823;&#32434;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#12289;&#25688;&#35201;&#22686;&#24378;&#30340;&#22823;&#32434;&#30417;&#30563;&#29983;&#25104;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#25688;&#35201;&#20219;&#21153;&#30340;&#21452;&#37325;&#29305;&#24449;&#26469;&#25913;&#36827;&#22823;&#32434;&#39044;&#27979;&#65292;&#20174;&#32780;&#20135;&#29983;&#26356;&#26126;&#30830;&#21644;&#21512;&#29702;&#30340;&#22823;&#32434;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;&#22823;&#32434;&#30340;&#29983;&#25104;&#20855;&#26377;&#26410;&#20805;&#20998;&#21033;&#29992;&#30340;&#38382;&#39064;&#65292;&#26080;&#35770;&#26159;&#26631;&#20934;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;GPT-2&#12289;BART&#65289;&#36824;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;Vicuna&#12289;ChatGPT&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26174;&#24335;&#22823;&#32434;&#25511;&#21046;&#26041;&#27861;&#65292;&#20197;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#29983;&#25104;&#30340;&#22823;&#32434;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatically open-ended long text generation poses significant challenges due to semantic incoherence and plot implausibility. Previous works usually alleviate this problem through outlines in the form of short phrases or abstractive signals by designing unsupervised tasks, which tend to be unstable and weakly interpretable.  Assuming that a summary serves as a mature outline, we introduce a two-stage, summary-enhanced outline supervised generation framework. This framework leverages the dual characteristics of the summarization task to improve outline prediction, resulting in more explicit and plausible outlines. Furthermore, we identify an underutilization issue in outline-based generation with both standard pretrained language models (e.g., GPT-2, BART) and large language models (e.g., Vicuna, ChatGPT). To address this, we propose a novel explicit outline control method for more effective utilization of generated outlines.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SPEECH&#30340;&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#33021;&#37327;&#24314;&#27169;&#26469;&#34920;&#31034;&#22797;&#26434;&#30340;&#20107;&#20214;&#32467;&#26500;&#65292;&#24182;&#20351;&#29992;&#36229;&#29699;&#26469;&#34920;&#31034;&#20107;&#20214;&#31867;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SPEECH&#22312;&#20107;&#20214;&#26816;&#27979;&#21644;&#20107;&#20214;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.13617</link><description>&lt;p&gt;
SPEECH: &#22522;&#20110;&#33021;&#37327;&#30340;&#20107;&#20214;&#20013;&#24515;&#36229;&#29699;&#30340;&#32467;&#26500;&#21270;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
SPEECH: Structured Prediction with Energy-Based Event-Centric Hyperspheres. (arXiv:2305.13617v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13617
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SPEECH&#30340;&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#33021;&#37327;&#24314;&#27169;&#26469;&#34920;&#31034;&#22797;&#26434;&#30340;&#20107;&#20214;&#32467;&#26500;&#65292;&#24182;&#20351;&#29992;&#36229;&#29699;&#26469;&#34920;&#31034;&#20107;&#20214;&#31867;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SPEECH&#22312;&#20107;&#20214;&#26816;&#27979;&#21644;&#20107;&#20214;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#20013;&#24515;&#30340;&#32467;&#26500;&#21270;&#39044;&#27979;&#28041;&#21450;&#39044;&#27979;&#20107;&#20214;&#30340;&#32467;&#26500;&#21270;&#36755;&#20986;&#12290;&#22312;&#22823;&#22810;&#25968;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24773;&#20917;&#19979;&#65292;&#20107;&#20214;&#32467;&#26500;&#37117;&#20855;&#26377;&#22797;&#26434;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#22240;&#27492;&#26377;&#25928;&#22320;&#34920;&#31034;&#36825;&#20123;&#22797;&#26434;&#30340;&#20107;&#20214;&#32467;&#26500;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#33021;&#37327;&#30340;&#20107;&#20214;&#20013;&#24515;&#36229;&#29699;&#30340;&#32467;&#26500;&#21270;&#39044;&#27979; (SPEECH)&#12290; SPEECH &#20351;&#29992;&#22522;&#20110;&#33021;&#37327;&#30340;&#24314;&#27169;&#26469;&#27169;&#25311;&#20107;&#20214;&#32467;&#26500;&#32452;&#20214;&#20043;&#38388;&#30340;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#20351;&#29992;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#36229;&#29699;&#26469;&#34920;&#31034;&#20107;&#20214;&#31867;&#21035;&#12290;&#22312;&#20004;&#20010;&#32479;&#19968;&#26631;&#27880;&#30340;&#20107;&#20214;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#20013;&#65292;&#32467;&#26524;&#34920;&#26126;SPEECH&#22312;&#20107;&#20214;&#26816;&#27979;&#21644;&#20107;&#20214;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#20013;&#21344;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Event-centric structured prediction involves predicting structured outputs of events. In most NLP cases, event structures are complex with manifold dependency, and it is challenging to effectively represent these complicated structured events. To address these issues, we propose Structured Prediction with Energy-based Event-Centric Hyperspheres (SPEECH). SPEECH models complex dependency among event structured components with energy-based modeling, and represents event classes with simple but effective hyperspheres. Experiments on two unified-annotated event datasets indicate that SPEECH is predominant in event detection and event-relation extraction tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#23454;&#39564;&#30340;&#32467;&#26524;&#21644;&#21453;&#24605;&#65292;&#35813;&#23454;&#39564;&#20351;&#29992;&#27169;&#22411;GPT 3.5-Turbo&#26469;&#27169;&#25311;&#24402;&#32435;&#24335;&#20027;&#39064;&#20998;&#26512;&#30340;&#26576;&#20123;&#26041;&#38754;&#12290;&#23581;&#35797;&#20351;&#29992;LLM&#36827;&#34892;&#22522;&#20110;&#20154;&#31867;&#35299;&#37322;&#30340;&#20998;&#26512;&#26174;&#28982;&#26159;&#19968;&#31181;&#25361;&#25112;&#65292;&#20294;&#20063;&#26159;&#20102;&#35299;&#36825;&#20123;&#31995;&#32479;&#22312;&#23450;&#24615;&#30740;&#31350;&#20013;&#33021;&#21542;&#20351;&#29992;&#30340;&#19968;&#31181;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2305.13014</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#27169;&#25311;&#35821;&#22659;&#19981;&#26126;&#32467;&#26500;&#21270;&#35775;&#35848;&#30340;&#24402;&#32435;&#24335;&#20027;&#39064;&#20998;&#26512;&#21527;&#65311;&#23545;&#26041;&#27861;&#21644;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#36827;&#34892;&#25506;&#35752;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models emulate an inductive Thematic Analysis of semi-structured interviews? An exploration and provocation on the limits of the approach and the model. (arXiv:2305.13014v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#23454;&#39564;&#30340;&#32467;&#26524;&#21644;&#21453;&#24605;&#65292;&#35813;&#23454;&#39564;&#20351;&#29992;&#27169;&#22411;GPT 3.5-Turbo&#26469;&#27169;&#25311;&#24402;&#32435;&#24335;&#20027;&#39064;&#20998;&#26512;&#30340;&#26576;&#20123;&#26041;&#38754;&#12290;&#23581;&#35797;&#20351;&#29992;LLM&#36827;&#34892;&#22522;&#20110;&#20154;&#31867;&#35299;&#37322;&#30340;&#20998;&#26512;&#26174;&#28982;&#26159;&#19968;&#31181;&#25361;&#25112;&#65292;&#20294;&#20063;&#26159;&#20102;&#35299;&#36825;&#20123;&#31995;&#32479;&#22312;&#23450;&#24615;&#30740;&#31350;&#20013;&#33021;&#21542;&#20351;&#29992;&#30340;&#19968;&#31181;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#20026;&#24378;&#22823;&#30340;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#24212;&#29992;&#20110;&#22810;&#20010;&#39046;&#22495;&#21644;&#24037;&#20316;&#39046;&#22495;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#23454;&#39564;&#30340;&#32467;&#26524;&#21644;&#21453;&#24605;&#65292;&#35813;&#23454;&#39564;&#20351;&#29992;&#27169;&#22411;GPT 3.5-Turbo&#26469;&#27169;&#25311;&#24402;&#32435;&#24335;&#20027;&#39064;&#20998;&#26512;&#30340;&#26576;&#20123;&#26041;&#38754;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#22312;&#35813;&#20027;&#39064;&#19978;&#20027;&#35201;&#36827;&#34892;&#28436;&#32462;&#20998;&#26512;&#12290;&#20027;&#39064;&#20998;&#26512;&#26159;&#19968;&#31181;&#22312;&#31038;&#20250;&#31185;&#23398;&#20013;&#24120;&#29992;&#30340;&#23450;&#24615;&#20998;&#26512;&#26041;&#27861;&#65292;&#23427;&#22522;&#20110;&#20154;&#31867;&#20998;&#26512;&#24072;&#30340;&#35299;&#37322;&#20197;&#21450;&#23450;&#24615;&#25968;&#25454;&#20013;&#30340;&#26174;&#24335;&#21644;&#28508;&#22312;&#21547;&#20041;&#30340;&#35782;&#21035;&#12290;&#23581;&#35797;&#20351;&#29992;LLM&#36827;&#34892;&#22522;&#20110;&#20154;&#31867;&#35299;&#37322;&#30340;&#20998;&#26512;&#26174;&#28982;&#26159;&#19968;&#31181;&#25361;&#25112;&#65292;&#20294;&#20063;&#26159;&#20102;&#35299;&#36825;&#20123;&#31995;&#32479;&#22312;&#23450;&#24615;&#30740;&#31350;&#20013;&#33021;&#21542;&#20351;&#29992;&#30340;&#19968;&#31181;&#26041;&#24335;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#23581;&#35797;&#36827;&#34892;&#27492;&#27169;&#25311;&#30340;&#21160;&#26426;&#65292;&#24182;&#21453;&#24605;&#20102;Braun&#21644;Clarke&#25552;&#20986;&#30340;&#20845;&#20010;&#27493;&#39588;&#33267;&#23569;&#37096;&#20998;&#22320;&#22914;&#20309;&#36827;&#34892;&#20027;&#39064;&#20998;&#26512;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have emerged as powerful generative Artificial Intelligence solutions which can be applied to several fields and areas of work. This paper presents results and reflection of an experiment done to use the model GPT 3.5-Turbo to emulate some aspects of an inductive Thematic Analysis. Previous research on this subject has largely worked on conducting deductive analysis. Thematic Analysis is a qualitative method for analysis commonly used in social sciences and it is based on interpretations made by the human analyst(s) and the identification of explicit and latent meanings in qualitative data. Attempting an analysis based on human interpretation with an LLM clearly is a provocation but also a way to learn something about how these systems can or cannot be used in qualitative research. The paper presents the motivations for attempting this emulation, it reflects on how the six steps to a Thematic Analysis proposed by Braun and Clarke can at least partially be r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25554;&#20214;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20854;&#19982;&#27169;&#22411;&#21442;&#25968;&#20013;&#30340;&#30693;&#35782;&#23384;&#20648;&#20998;&#31163;&#65292;&#37319;&#29992;&#21487;&#32534;&#36753;&#21644;&#21487;&#25193;&#23637;&#30340;&#38190;&#20540;&#23384;&#20648;&#22120;&#65292;&#36890;&#36807;DPM&#20013;&#30340;&#30693;&#35782;&#26816;&#32034;&#20197;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#21033;&#29992;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2305.11564</link><description>&lt;p&gt;
&#35299;&#32806;&#21442;&#25968;&#20013;&#30340;&#30693;&#35782;&#65306;&#21487;&#25554;&#25300;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Decouple knowledge from paramters for plug-and-play language modeling. (arXiv:2305.11564v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25554;&#20214;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20854;&#19982;&#27169;&#22411;&#21442;&#25968;&#20013;&#30340;&#30693;&#35782;&#23384;&#20648;&#20998;&#31163;&#65292;&#37319;&#29992;&#21487;&#32534;&#36753;&#21644;&#21487;&#25193;&#23637;&#30340;&#38190;&#20540;&#23384;&#20648;&#22120;&#65292;&#36890;&#36807;DPM&#20013;&#30340;&#30693;&#35782;&#26816;&#32034;&#20197;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#21033;&#29992;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#26524;&#12290; &#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#25104;&#21151;&#30340;&#20851;&#38190;&#22240;&#32032;&#20043;&#19968;&#26159;&#36825;&#20123;&#27169;&#22411;&#30340;&#21442;&#25968;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#38544;&#21547;&#22320;&#23398;&#20064;&#20102;&#21508;&#31181;&#30693;&#35782;&#12290; &#28982;&#32780;&#65292;&#23558;&#30693;&#35782;&#38544;&#21547;&#22312;&#27169;&#22411;&#21442;&#25968;&#20013;&#20855;&#26377;&#20004;&#20010;&#22522;&#26412;&#32570;&#28857;&#12290; &#39318;&#20808;&#65292;&#22312;&#27169;&#22411;&#35757;&#32451;&#21518;&#65292;&#26080;&#27861;&#32534;&#36753;&#25110;&#25193;&#23637;&#30693;&#35782;&#65292;&#29305;&#21035;&#26159;&#22312;&#30693;&#35782;&#19981;&#26029;&#21457;&#23637;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#26159;&#19968;&#20010;&#20005;&#37325;&#30340;&#38382;&#39064;&#12290; &#20854;&#27425;&#65292;&#23427;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#24182;&#38459;&#27490;&#20154;&#20204;&#20102;&#35299;PLM&#22312;&#26576;&#20010;&#38382;&#39064;&#19978;&#25152;&#38656;&#30340;&#21738;&#20123;&#30693;&#35782;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;PlugLM&#65292;&#36825;&#26159;&#19968;&#31181;&#20855;&#26377;&#21487;&#24494;&#20998;&#25554;&#20214;&#23384;&#20648;&#22120;&#65288;DPM&#65289;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290; &#20851;&#38190;&#30340;&#30452;&#35273;&#26159;&#20351;&#29992;&#21487;&#32534;&#36753;&#21644;&#21487;&#25193;&#23637;&#30340;&#38190;&#20540;&#23384;&#20648;&#22120;&#23558;&#30693;&#35782;&#23384;&#20648;&#19982;&#27169;&#22411;&#21442;&#25968;&#20998;&#31163;&#65292;&#24182;&#36890;&#36807;DPM&#20013;&#30340;&#30693;&#35782;&#26816;&#32034;&#20197;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#21033;&#29992;&#30693;&#35782;&#12290;&#20026;&#20102;&#35777;&#26126;&#36825;&#31181;&#35774;&#35745;&#36873;&#25321;&#30340;&#21512;&#29702;&#24615;&#65292;&#25105;&#20204;&#22312;&#19977;&#20010;&#35774;&#32622;&#20013;&#36827;&#34892;&#35780;&#20272;&#65292;&#36825;&#20123;&#35774;&#32622;&#38656;&#35201;&#21508;&#31181;&#24418;&#24335;&#30340;&#30693;&#35782;&#65306;&#65288;1&#65289;&#39046;&#22495;&#36866;&#24212;&#65292;&#65288;2&#65289;&#26410;&#35265;&#23454;&#20307;&#21512;&#24182;&#65292;&#20197;&#21450;&#65288;3&#65289;&#22312;&#19981;&#36951;&#24536;&#26087;&#20219;&#21153;&#30340;&#24773;&#20917;&#19979;&#36866;&#24212;&#26032;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models(PLM) have made impressive results in various NLP tasks. It has been revealed that one of the key factors to their success is the parameters of these models implicitly learn all kinds of knowledge during pre-training. However, encoding knowledge implicitly in the model parameters has two fundamental drawbacks. First, the knowledge is neither editable nor scalable once the model is trained, which is especially problematic in that knowledge is consistently evolving. Second, it lacks interpretability and prevents humans from understanding which knowledge PLM requires for a certain problem. In this paper, we introduce PlugLM, a pre-training model with differentiable plug-in memory(DPM). The key intuition is to decouple the knowledge storage from model parameters with an editable and scalable key-value memory and leverage knowledge in an explainable manner by knowledge retrieval in the DPM. To justify this design choice, we conduct evaluations in three settings in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#20174;&#21333;&#19968;&#30340;&#38745;&#24577;&#22270;&#20687;&#20013;&#23398;&#20064;&#33258;&#30001;&#25991;&#26412;&#30340;&#24418;&#24335;&#26469;&#28789;&#27963;&#24314;&#27169;&#20154;&#38469;&#20114;&#21160;&#12290;&#24182;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#29983;&#25104;&#20266;&#26631;&#31614;&#26469;&#35757;&#32451;&#19968;&#31181;&#23383;&#24149;&#27169;&#22411;&#65292;&#29992;&#20110;&#26377;&#25928;&#29702;&#35299;&#22270;&#20687;&#20013;&#30340;&#20154;&#38469;&#20114;&#21160;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#39044;&#27979;&#25991;&#26412;&#21644;&#35821;&#20041;&#36136;&#37327;&#65292;&#24182;&#22312;&#27492;&#20219;&#21153;&#19978;&#20248;&#20110;SOTA&#30340;&#22270;&#20687;&#23383;&#24149;&#21644;&#24773;&#22659;&#35782;&#21035;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.14104</link><description>&lt;p&gt;
&#20174;&#24369;&#25991;&#26412;&#30417;&#30563;&#20013;&#23398;&#20064;&#22270;&#20687;&#20013;&#30340;&#20154;&#38469;&#20114;&#21160;
&lt;/p&gt;
&lt;p&gt;
Learning Human-Human Interactions in Images from Weak Textual Supervision. (arXiv:2304.14104v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#20174;&#21333;&#19968;&#30340;&#38745;&#24577;&#22270;&#20687;&#20013;&#23398;&#20064;&#33258;&#30001;&#25991;&#26412;&#30340;&#24418;&#24335;&#26469;&#28789;&#27963;&#24314;&#27169;&#20154;&#38469;&#20114;&#21160;&#12290;&#24182;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#29983;&#25104;&#20266;&#26631;&#31614;&#26469;&#35757;&#32451;&#19968;&#31181;&#23383;&#24149;&#27169;&#22411;&#65292;&#29992;&#20110;&#26377;&#25928;&#29702;&#35299;&#22270;&#20687;&#20013;&#30340;&#20154;&#38469;&#20114;&#21160;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#39044;&#27979;&#25991;&#26412;&#21644;&#35821;&#20041;&#36136;&#37327;&#65292;&#24182;&#22312;&#27492;&#20219;&#21153;&#19978;&#20248;&#20110;SOTA&#30340;&#22270;&#20687;&#23383;&#24149;&#21644;&#24773;&#22659;&#35782;&#21035;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#38469;&#20114;&#21160;&#26159;&#22810;&#26679;&#19988;&#20381;&#36182;&#20110;&#19978;&#19979;&#25991;&#30340;&#65292;&#20294;&#20808;&#21069;&#30340;&#24037;&#20316;&#23558;&#23427;&#20204;&#35270;&#20026;&#20998;&#31867;&#65292;&#24573;&#30053;&#20102;&#21487;&#33021;&#30340;&#20114;&#21160;&#30340;&#37325;&#23614;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#20154;&#38469;&#20114;&#21160;&#30340;&#33539;&#24335;&#65292;&#23558;&#20854;&#20316;&#20026;&#33258;&#30001;&#25991;&#26412;&#20174;&#21333;&#19968;&#30340;&#38745;&#24577;&#22270;&#20687;&#20013;&#23398;&#20064;&#65292;&#20174;&#32780;&#20801;&#35768;&#23545;&#24773;&#20917;&#21644;&#20154;&#38469;&#20851;&#31995;&#30340;&#26080;&#38480;&#31354;&#38388;&#36827;&#34892;&#28789;&#27963;&#24314;&#27169;&#12290;&#20026;&#20102;&#20811;&#26381;&#32570;&#20047;&#29305;&#23450;&#20110;&#27492;&#20219;&#21153;&#30340;&#26631;&#35760;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#24212;&#29992;&#20110;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#30340;&#21512;&#25104;&#23383;&#24149;&#25968;&#25454;&#65292;&#20197;&#27492;&#29983;&#25104;&#20266;&#26631;&#31614;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#36825;&#20010;&#36807;&#31243;&#20135;&#29983;&#30340;&#20266;&#26631;&#31614;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#19968;&#31181;&#23383;&#24149;&#27169;&#22411;&#65292;&#33021;&#26377;&#25928;&#29702;&#35299;&#22270;&#20687;&#20013;&#30340;&#20154;&#38469;&#20114;&#21160;&#65292;&#36890;&#36807;&#34913;&#37327;&#25105;&#20204;&#39044;&#27979;&#30340;&#25991;&#26412;&#21644;&#35821;&#20041;&#36136;&#37327;&#19982;&#20107;&#23454;&#30340;&#22522;&#30784;&#24615;&#30340;&#21508;&#31181;&#25351;&#26631;&#26469;&#34913;&#37327;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;SOTA&#30340;&#22270;&#20687;&#23383;&#24149;&#21644;&#24773;&#22659;&#35782;&#21035;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#20844;&#24320;&#25105;&#20204;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interactions between humans are diverse and context-dependent, but previous works have treated them as categorical, disregarding the heavy tail of possible interactions. We propose a new paradigm of learning human-human interactions as free text from a single still image, allowing for flexibility in modeling the unlimited space of situations and relationships between people. To overcome the absence of data labelled specifically for this task, we use knowledge distillation applied to synthetic caption data produced by a large language model without explicit supervision. We show that the pseudo-labels produced by this procedure can be used to train a captioning model to effectively understand human-human interactions in images, as measured by a variety of metrics that measure textual and semantic faithfulness and factual groundedness of our predictions. We further show that our approach outperforms SOTA image captioning and situation recognition models on this task. We will release our c
&lt;/p&gt;</description></item><item><title>AGIEval&#26159;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#35774;&#35745;&#30340;&#22522;&#20934;&#27979;&#35797;&#24037;&#20855;&#65292;&#29992;&#20110;&#35780;&#20272;&#22522;&#30784;&#27169;&#22411;&#22312;&#20154;&#31867;&#20013;&#24515;&#26631;&#20934;&#21270;&#32771;&#35797;&#19978;&#30340;&#34920;&#29616;&#12290;GPT-4&#22312;SAT&#12289;LSAT&#21644;&#25968;&#23398;&#27604;&#36187;&#26041;&#38754;&#36229;&#36234;&#20102;&#20154;&#31867;&#24179;&#22343;&#34920;&#29616;&#65292;&#23637;&#31034;&#20102;&#24403;&#20195;&#22522;&#30784;&#27169;&#22411;&#22312;&#20154;&#31867;&#32423;&#20219;&#21153;&#20013;&#30340;&#38750;&#20961;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.06364</link><description>&lt;p&gt;
AGIEval&#65306;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#22522;&#20934;&#35780;&#20272;&#22522;&#30784;&#27169;&#22411;&#30340;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models. (arXiv:2304.06364v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06364
&lt;/p&gt;
&lt;p&gt;
AGIEval&#26159;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#35774;&#35745;&#30340;&#22522;&#20934;&#27979;&#35797;&#24037;&#20855;&#65292;&#29992;&#20110;&#35780;&#20272;&#22522;&#30784;&#27169;&#22411;&#22312;&#20154;&#31867;&#20013;&#24515;&#26631;&#20934;&#21270;&#32771;&#35797;&#19978;&#30340;&#34920;&#29616;&#12290;GPT-4&#22312;SAT&#12289;LSAT&#21644;&#25968;&#23398;&#27604;&#36187;&#26041;&#38754;&#36229;&#36234;&#20102;&#20154;&#31867;&#24179;&#22343;&#34920;&#29616;&#65292;&#23637;&#31034;&#20102;&#24403;&#20195;&#22522;&#30784;&#27169;&#22411;&#22312;&#20154;&#31867;&#32423;&#20219;&#21153;&#20013;&#30340;&#38750;&#20961;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#22522;&#30784;&#27169;&#22411;&#35299;&#20915;&#20154;&#31867;&#32423;&#21035;&#20219;&#21153;&#30340;&#36890;&#29992;&#33021;&#21147;&#26159;&#23427;&#20204;&#22312;&#21457;&#23637;&#21644;&#24212;&#29992;AGI&#65288;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65289;&#20013;&#30340;&#37325;&#35201;&#26041;&#38754;&#12290;&#20256;&#32479;&#22522;&#20934;&#27979;&#35797;&#20381;&#36182;&#20110;&#20154;&#36896;&#25968;&#25454;&#38598;&#65292;&#21487;&#33021;&#26080;&#27861;&#20934;&#30830;&#20195;&#34920;&#20154;&#31867;&#27700;&#24179;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AGIEval&#65292;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;&#22522;&#30784;&#27169;&#22411;&#22312;&#20154;&#31867;&#20013;&#24515;&#26631;&#20934;&#21270;&#32771;&#35797;&#30340;&#22522;&#20934;&#27979;&#35797;&#24037;&#20855;&#65292;&#20363;&#22914;&#22823;&#23398;&#20837;&#23398;&#32771;&#35797;&#65292;&#27861;&#24459;&#23398;&#26657;&#20837;&#23398;&#32771;&#35797;&#65292;&#25968;&#23398;&#31454;&#36187;&#21644;&#24459;&#24072;&#36164;&#26684;&#32771;&#35797;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#22522;&#20934;&#27979;&#35797;&#24037;&#20855;&#35780;&#20272;&#20102;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#21253;&#25324; GPT-4&#65292;ChatGPT &#21644;Text-Davinci-003&#12290;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#26159;&#65292;GPT-4&#22312;SAT&#12289;LSAT&#21644;&#25968;&#23398;&#27604;&#36187;&#26041;&#38754;&#36229;&#36234;&#20102;&#20154;&#31867;&#24179;&#22343;&#34920;&#29616;&#65292;SAT&#25968;&#23398;&#27979;&#35797;&#30340;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;95%&#65292;&#22312;&#20013;&#22269;&#22269;&#23478;&#22823;&#23398;&#33521;&#35821;&#32771;&#35797;&#30340;&#33521;&#35821;&#27979;&#35797;&#20013;&#20934;&#30830;&#29575;&#20063;&#36798;&#21040;&#20102;92.5%&#12290;&#36825;&#23637;&#31034;&#20102;&#24403;&#20195;&#22522;&#30784;&#27169;&#22411;&#22312;&#20154;&#31867;&#32423;&#20219;&#21153;&#20013;&#30340;&#38750;&#20961;&#24615;&#33021;&#65292;&#24182;&#20984;&#26174;&#20102;AGI&#26410;&#26469;&#21457;&#23637;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating the general abilities of foundation models to tackle human-level tasks is a vital aspect of their development and application in the pursuit of Artificial General Intelligence (AGI). Traditional benchmarks, which rely on artificial datasets, may not accurately represent human-level capabilities. In this paper, we introduce AGIEval, a novel benchmark specifically designed to assess foundation model in the context of human-centric standardized exams, such as college entrance exams, law school admission tests, math competitions, and lawyer qualification tests. We evaluate several state-of-the-art foundation models, including GPT-4, ChatGPT, and Text-Davinci-003, using this benchmark. Impressively, GPT-4 surpasses average human performance on SAT, LSAT, and math competitions, attaining a 95% accuracy rate on the SAT Math test and a 92.5% accuracy on the English test of the Chinese national college entrance exam. This demonstrates the extraordinary performance of contemporary fou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21019;&#36896;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934; bipol &#20197;&#26816;&#27979;&#25991;&#26412;&#25968;&#25454;&#20013;&#30340;&#31038;&#20132;&#20559;&#35265;&#12290;&#35813;&#26631;&#20934;&#21253;&#25324;&#35821;&#26009;&#24211;&#32423;&#21035;&#35780;&#20272;&#21644;&#21477;&#23376;&#32423;&#21035;&#35780;&#20272;&#20004;&#20010;&#27493;&#39588;&#65292;&#24182;&#20351;&#29992; SotA &#26550;&#26500;&#21019;&#24314;&#20102;&#26032;&#27169;&#22411;&#20197;&#26816;&#27979;&#22810;&#20010;&#36724;&#30340;&#20559;&#24046;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#21019;&#36896;&#20102;&#19968;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#20559;&#35265;&#26816;&#27979;&#27169;&#22411;&#65292;&#24182;&#20844;&#24320;&#20102;&#30456;&#20851;&#20195;&#30721;&#12290;</title><link>http://arxiv.org/abs/2304.04029</link><description>&lt;p&gt;
Bipol: &#19968;&#31181;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#30340;&#26032;&#22411;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22810;&#36724;&#20559;&#35265;&#35780;&#20272;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Bipol: A Novel Multi-Axes Bias Evaluation Metric with Explainability for NLP. (arXiv:2304.04029v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04029
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21019;&#36896;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934; bipol &#20197;&#26816;&#27979;&#25991;&#26412;&#25968;&#25454;&#20013;&#30340;&#31038;&#20132;&#20559;&#35265;&#12290;&#35813;&#26631;&#20934;&#21253;&#25324;&#35821;&#26009;&#24211;&#32423;&#21035;&#35780;&#20272;&#21644;&#21477;&#23376;&#32423;&#21035;&#35780;&#20272;&#20004;&#20010;&#27493;&#39588;&#65292;&#24182;&#20351;&#29992; SotA &#26550;&#26500;&#21019;&#24314;&#20102;&#26032;&#27169;&#22411;&#20197;&#26816;&#27979;&#22810;&#20010;&#36724;&#30340;&#20559;&#24046;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#21019;&#36896;&#20102;&#19968;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#20559;&#35265;&#26816;&#27979;&#27169;&#22411;&#65292;&#24182;&#20844;&#24320;&#20102;&#30456;&#20851;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#30340;&#24230;&#37327;&#26631;&#20934; bipol&#65292;&#29992;&#20110;&#20272;&#31639;&#25991;&#26412;&#25968;&#25454;&#20013;&#30340;&#31038;&#20132;&#20559;&#35265;&#12290;&#26377;&#23475;&#20559;&#35265;&#22312;&#35768;&#22810;&#22312;&#32447;&#25968;&#25454;&#28304;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#36825;&#20123;&#25968;&#25454;&#28304;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#12290;&#25105;&#20204;&#21019;&#36896;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#65306;&#22522;&#20110;&#27169;&#22411;&#20998;&#31867;&#30340;&#35821;&#26009;&#24211;&#32423;&#21035;&#35780;&#20272;&#21644;&#22522;&#20110;&#65288;&#25935;&#24863;&#65289;&#35789;&#39057;&#65288;TF&#65289;&#30340;&#21477;&#23376;&#32423;&#21035;&#35780;&#20272;&#12290;&#25105;&#20204;&#20351;&#29992;SotA&#26550;&#26500;&#21019;&#24314;&#20102;&#26032;&#27169;&#22411;&#65292;&#20197;&#26816;&#27979;&#27839;&#22810;&#20010;&#36724;&#30340;&#20559;&#24046;&#65292;&#24182;&#35780;&#20272;&#20102;&#20004;&#20010;&#27969;&#34892;&#30340;NLP&#25968;&#25454;&#38598;&#65288;COPA&#21644;SQUAD&#65289;&#12290;&#20316;&#20026;&#38468;&#21152;&#36129;&#29486;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#65288;&#20960;&#20046;&#26377;200&#19975;&#20010;&#24102;&#26631;&#31614;&#30340;&#26679;&#26412;&#65289;&#65292;&#29992;&#20110;&#35757;&#32451;&#20559;&#35265;&#26816;&#27979;&#27169;&#22411;&#65292;&#24182;&#20844;&#24320;&#20102;&#23427;&#12290;&#25105;&#20204;&#36824;&#20844;&#24320;&#20102;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce bipol, a new metric with explainability, for estimating social bias in text data. Harmful bias is prevalent in many online sources of data that are used for training machine learning (ML) models. In a step to address this challenge we create a novel metric that involves a two-step process: corpus-level evaluation based on model classification and sentence-level evaluation based on (sensitive) term frequency (TF). After creating new models to detect bias along multiple axes using SotA architectures, we evaluate two popular NLP datasets (COPA and SQUAD). As additional contribution, we created a large dataset (with almost 2 million labelled samples) for training models in bias detection and make it publicly available. We also make public our codes.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BCT&#30340;&#20998;&#22359;&#21387;&#32553;&#26694;&#26550;&#65292;&#21487;&#20197;&#23545;&#25972;&#20010;Transformer&#27169;&#22411;&#36827;&#34892;&#26356;&#32454;&#31890;&#24230;&#30340;&#21387;&#32553;&#65292;&#23454;&#29616;&#20102;&#38477;&#20302;&#37096;&#32626;&#38376;&#27099;&#30340;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2304.01483</link><description>&lt;p&gt;
&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#20998;&#22359;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Blockwise Compression of Transformer-based Models without Retraining. (arXiv:2304.01483v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01483
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BCT&#30340;&#20998;&#22359;&#21387;&#32553;&#26694;&#26550;&#65292;&#21487;&#20197;&#23545;&#25972;&#20010;Transformer&#27169;&#22411;&#36827;&#34892;&#26356;&#32454;&#31890;&#24230;&#30340;&#21387;&#32553;&#65292;&#23454;&#29616;&#20102;&#38477;&#20302;&#37096;&#32626;&#38376;&#27099;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#27169;&#22411;&#30340;GPT-3&#12289;ChatGPT&#21644;GPT-4&#36817;&#24180;&#26469;&#22791;&#21463;&#20851;&#27880;&#65292;&#20294;&#23427;&#20204;&#30340;&#24040;&#22823;&#35745;&#31639;&#36164;&#28304;&#21644;&#23384;&#20648;&#24320;&#38144;&#20173;&#28982;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BCT&#30340;&#20998;&#22359;&#21387;&#32553;&#26694;&#26550;&#65292;&#21487;&#20197;&#23545;&#25972;&#20010;Transformer&#27169;&#22411;&#36827;&#34892;&#26356;&#32454;&#31890;&#24230;&#30340;&#21387;&#32553;&#65292;&#21253;&#25324;&#23884;&#20837;&#12289;&#30697;&#38453;&#20056;&#27861;&#12289;GELU&#12289;Softmax&#12289;&#23618;&#35268;&#33539;&#21270;&#20197;&#21450;&#25152;&#26377;&#20013;&#38388;&#32467;&#26524;&#12290;&#25105;&#20204;&#23545;&#19968;&#20010;&#39640;&#25928;&#27169;&#22411;&#20351;&#29992;BCT&#36827;&#34892;&#20102;&#21387;&#32553;&#24182;&#22312;&#22810;&#20010;GLUE&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#65292;BCT&#21482;&#20250;&#24102;&#26469;&#23569;&#20110;0.90%&#30340;&#20934;&#30830;&#29575;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based models, represented by GPT-3, ChatGPT, and GPT-4, have recently attracted increasing interest, research enthusiasm, and business demand. However, their massive computation resources and huge memory footprint are inevitable challenges. To tackle this issue, we propose BCT, a framework of blockwise compression for transformers without retraining, to lower deployment thresholds. BCT achieves more fine-grained compression of the whole transformer, including embedding, matrix multiplication, GELU, Softmax, layer normalization, and all the intermediate results. As a case, we compress an efficient model with BCT and evaluate it on several General Language Understanding Evaluation (GLUE) datasets. The results show that BCT can achieve a less than 0.90% accuracy drop in most tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26080;&#21442;&#32771;&#25991;&#26412;&#36136;&#37327;&#35780;&#20272;&#20013;&#30340;&#24212;&#29992;&#30740;&#31350;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21033;&#29992;ChatGPT&#29983;&#25104;&#30340;&#26174;&#24335;&#24471;&#20998;&#26159;&#26368;&#26377;&#25928;&#21644;&#21487;&#38752;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.00723</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26080;&#21442;&#32771;&#25991;&#26412;&#36136;&#37327;&#35780;&#20272;&#20013;&#30340;&#24212;&#29992;&#65306;&#21021;&#27493;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Exploring the Use of Large Language Models for Reference-Free Text Quality Evaluation: A Preliminary Empirical Study. (arXiv:2304.00723v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26080;&#21442;&#32771;&#25991;&#26412;&#36136;&#37327;&#35780;&#20272;&#20013;&#30340;&#24212;&#29992;&#30740;&#31350;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21033;&#29992;ChatGPT&#29983;&#25104;&#30340;&#26174;&#24335;&#24471;&#20998;&#26159;&#26368;&#26377;&#25928;&#21644;&#21487;&#38752;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#35780;&#20272;&#29983;&#25104;&#25991;&#26412;&#30340;&#36136;&#37327;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#30001;&#20110;&#25991;&#26412;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#32780;&#20135;&#29983;&#22256;&#38590;&#12290;&#26368;&#36817;&#65292;OpenAI&#30340;ChatGPT&#65292;&#19968;&#31181;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#30001;&#20110;&#20854;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#20986;&#33394;&#34920;&#29616;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21457;&#24067;&#27492;&#25253;&#21578;&#65292;&#20197;&#35843;&#26597;LLMs&#65292;&#29305;&#21035;&#26159;ChatGPT&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25506;&#32034;&#20248;&#21270;&#23427;&#20204;&#22312;&#35780;&#20272;&#25991;&#26412;&#36136;&#37327;&#26041;&#38754;&#30340;&#24212;&#29992;&#26041;&#24335;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#22522;&#20110;ChatGPT&#25110;&#31867;&#20284;LLMs&#30340;&#19977;&#31181;&#26080;&#21442;&#32771;&#35780;&#20272;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#65292;ChatGPT&#33021;&#22815;&#26377;&#25928;&#22320;&#20174;&#21508;&#20010;&#35282;&#24230;&#35780;&#20272;&#25991;&#26412;&#36136;&#37327;&#32780;&#19981;&#38656;&#35201;&#21442;&#32771;&#65292;&#24182;&#23637;&#31034;&#20102;&#27604;&#22823;&#22810;&#25968;&#29616;&#26377;&#33258;&#21160;&#25351;&#26631;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#29305;&#21035;&#26159;&#65292;&#26174;&#24335;&#24471;&#20998;&#26159;&#21033;&#29992;ChatGPT&#29983;&#25104;&#34913;&#37327;&#25991;&#26412;&#36136;&#37327;&#30340;&#25968;&#23383;&#20998;&#25968;&#30340;&#26368;&#26377;&#25928;&#21644;&#21487;&#38752;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#23558;LLMs&#24212;&#29992;&#20110;&#25991;&#26412;&#36136;&#37327;&#35780;&#20272;&#20173;&#28982;&#38754;&#20020;&#25361;&#25112;&#21644;&#38480;&#21046;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25506;&#32034;&#21644;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating the quality of generated text is a challenging task in natural language processing. This difficulty arises from the inherent complexity and diversity of text. Recently, OpenAI's ChatGPT, a powerful large language model (LLM), has garnered significant attention due to its impressive performance in various tasks. Therefore, we present this report to investigate the effectiveness of LLMs, especially ChatGPT, and explore ways to optimize their use in assessing text quality. We compared three kinds of reference-free evaluation methods based on ChatGPT or similar LLMs. The experimental results prove that ChatGPT is capable to evaluate text quality effectively from various perspectives without reference and demonstrates superior performance than most existing automatic metrics. In particular, the Explicit Score, which utilizes ChatGPT to generate a numeric score measuring text quality, is the most effective and reliable method among the three exploited approaches. However, directly
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#38899;&#32534;&#30721;&#22120;&#20174;&#35821;&#38899;&#20013;&#30452;&#25509;&#25552;&#21462;&#23454;&#20307;&#30340;&#26041;&#27861;&#65292;&#26080;&#38656;&#25991;&#26412;&#36716;&#24405;&#65292;&#19988;&#22312;&#21475;&#35821;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2302.10186</link><description>&lt;p&gt;
&#34394;&#25311;&#20195;&#29702;&#20154;&#30340;&#31471;&#21040;&#31471;&#21475;&#35821;&#21270;&#23454;&#20307;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
E2E Spoken Entity Extraction for Virtual Agents. (arXiv:2302.10186v4 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10186
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#38899;&#32534;&#30721;&#22120;&#20174;&#35821;&#38899;&#20013;&#30452;&#25509;&#25552;&#21462;&#23454;&#20307;&#30340;&#26041;&#27861;&#65292;&#26080;&#38656;&#25991;&#26412;&#36716;&#24405;&#65292;&#19988;&#22312;&#21475;&#35821;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#26500;&#24819;&#20102;&#35821;&#38899;&#22788;&#29702;&#20013;&#30340;&#19968;&#20123;&#26041;&#38754;&#65292;&#29305;&#21035;&#26159;&#20851;&#20110;&#20174;&#35821;&#38899;&#20013;&#30452;&#25509;&#25552;&#21462;&#23454;&#20307;&#65292;&#32780;&#26080;&#38656;&#20013;&#38388;&#25991;&#26412;&#34920;&#31034;&#12290;&#22312;&#20154;&#19982;&#35745;&#31639;&#26426;&#30340;&#23545;&#35805;&#20013;&#65292;&#20174;&#35821;&#38899;&#20013;&#25552;&#21462;&#23454;&#20307;&#65292;&#22914;&#22995;&#21517;&#12289;&#37038;&#25919;&#22320;&#22336;&#21644;&#30005;&#23376;&#37038;&#20214;&#22320;&#22336;&#65292;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#38899;&#32534;&#30721;&#22120;&#23545;&#20174;&#35821;&#38899;&#20013;&#30452;&#25509;&#25552;&#21462;&#21487;&#35835;&#24615;&#24378;&#30340;&#23454;&#20307;&#30340;&#24433;&#21709;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#25991;&#26412;&#36716;&#24405;&#12290;&#25105;&#20204;&#35828;&#26126;&#36825;&#31181;&#30452;&#25509;&#26041;&#27861;&#20248;&#21270;&#20102;&#32534;&#30721;&#22120;&#65292;&#20197;&#20165;&#36716;&#24405;&#35821;&#38899;&#20013;&#19982;&#23454;&#20307;&#30456;&#20851;&#30340;&#37096;&#20998;&#65292;&#24573;&#30053;&#20102;&#22810;&#20313;&#30340;&#37096;&#20998;&#65292;&#22914;&#25645;&#26723;&#35821;&#25110;&#23454;&#20307;&#25340;&#20889;&#12290;&#22312;&#20225;&#19994;&#34394;&#25311;&#20195;&#29702;&#20154;&#30340;&#23545;&#35805;&#19978;&#19979;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#27493;&#27861;&#30340;&#26041;&#27861;&#20248;&#20110;&#20856;&#22411;&#30340;&#20004;&#27493;&#27861;&#65292;&#21363;&#39318;&#20808;&#20135;&#29983;&#35789;&#27719;&#36716;&#24405;&#65292;&#28982;&#21518;&#36827;&#34892;&#22522;&#20110;&#25991;&#26412;&#30340;&#23454;&#20307;&#25552;&#21462;&#20197;&#35782;&#21035;&#21475;&#35821;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper reimagines some aspects of speech processing using speech encoders, specifically about extracting entities directly from speech, with no intermediate textual representation. In human-computer conversations, extracting entities such as names, postal addresses and email addresses from speech is a challenging task. In this paper, we study the impact of fine-tuning pre-trained speech encoders on extracting spoken entities in human-readable form directly from speech without the need for text transcription. We illustrate that such a direct approach optimizes the encoder to transcribe only the entity relevant portions of speech, ignoring the superfluous portions such as carrier phrases and spellings of entities. In the context of dialogs from an enterprise virtual agent, we demonstrate that the 1-step approach outperforms the typical 2-step cascade of first generating lexical transcriptions followed by text-based entity extraction for identifying spoken entities.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25351;&#38024;&#29983;&#25104;&#32593;&#32476;&#21644;SciBERT&#23884;&#20837;&#26469;&#33258;&#21160;&#29983;&#25104;&#30740;&#31350;&#35770;&#25991;&#20142;&#28857;&#30340;&#26041;&#27861;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#30740;&#31350;&#20142;&#28857;&#29983;&#25104;&#26041;&#38754;&#20855;&#26377;&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.07729</link><description>&lt;p&gt;
&#20351;&#29992;&#25351;&#38024;&#29983;&#25104;&#32593;&#32476;&#21644;SciBERT&#23884;&#20837;&#29983;&#25104;&#30740;&#31350;&#35770;&#25991;&#30340;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Generation of Highlights from Research Papers Using Pointer-Generator Networks and SciBERT Embeddings. (arXiv:2302.07729v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07729
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25351;&#38024;&#29983;&#25104;&#32593;&#32476;&#21644;SciBERT&#23884;&#20837;&#26469;&#33258;&#21160;&#29983;&#25104;&#30740;&#31350;&#35770;&#25991;&#20142;&#28857;&#30340;&#26041;&#27861;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#30740;&#31350;&#20142;&#28857;&#29983;&#25104;&#26041;&#38754;&#20855;&#26377;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#35768;&#22810;&#30740;&#31350;&#25991;&#31456;&#37117;&#20197;&#30740;&#31350;&#20142;&#28857;&#20316;&#20026;&#21069;&#35328;&#65292;&#20197;&#24635;&#32467;&#35770;&#25991;&#30340;&#20027;&#35201;&#21457;&#29616;&#12290;&#20142;&#28857;&#19981;&#20165;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#20934;&#30830;&#24555;&#36895;&#22320;&#35782;&#21035;&#35770;&#25991;&#30340;&#36129;&#29486;&#65292;&#36824;&#36890;&#36807;&#25628;&#32034;&#24341;&#25806;&#22686;&#21152;&#20102;&#25991;&#31456;&#30340;&#21487;&#21457;&#29616;&#24615;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#32473;&#23450;&#30740;&#31350;&#35770;&#25991;&#30340;&#29305;&#23450;&#27573;&#33853;&#30340;&#24773;&#20917;&#19979;&#33258;&#21160;&#26500;&#24314;&#30740;&#31350;&#20142;&#28857;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#20855;&#26377;&#35206;&#30422;&#26426;&#21046;&#21644;&#19978;&#19979;&#25991;&#23884;&#20837;&#23618;&#30340;&#25351;&#38024;&#29983;&#25104;&#32593;&#32476;&#65292;&#23558;&#36755;&#20837;&#26631;&#35760;&#32534;&#30721;&#20026;SciBERT&#23884;&#20837;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;CSPubSum&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#36824;&#25552;&#20986;&#20102;MixSub&#65292;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#30740;&#31350;&#20142;&#28857;&#30340;&#26032;&#30340;&#36328;&#23398;&#31185;&#35770;&#25991;&#35821;&#26009;&#24211;&#12290;&#23545;&#20110;CSPubSum&#21644;MixSub&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#30456;&#23545;&#20110;&#30456;&#20851;&#21464;&#20307;&#21644;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#20854;&#20182;&#27169;&#22411;&#26469;&#35828;&#20855;&#26377;&#26368;&#20339;&#24615;&#33021;&#12290;&#22312;CSPubSum&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#21482;&#20351;&#29992;&#35770;&#25991;&#30340;&#25688;&#35201;&#20316;&#20026;&#36755;&#20837;&#26102;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays many research articles are prefaced with research highlights to summarize the main findings of the paper. Highlights not only help researchers precisely and quickly identify the contributions of a paper, they also enhance the discoverability of the article via search engines. We aim to automatically construct research highlights given certain segments of a research paper. We use a pointer-generator network with coverage mechanism and a contextual embedding layer at the input that encodes the input tokens into SciBERT embeddings. We test our model on a benchmark dataset, CSPubSum, and also present MixSub, a new multi-disciplinary corpus of papers for automatic research highlight generation. For both CSPubSum and MixSub, we have observed that the proposed model achieves the best performance compared to related variants and other models proposed in the literature. On the CSPubSum dataset, our model achieves the best performance when the input is only the abstract of a paper as op
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#22810;&#32500;&#24230;&#20559;&#24046;&#24230;&#37327;&#25351;&#26631;bipol&#65292;&#35780;&#20272;&#20102;&#20116;&#20010;&#33521;&#25991;&#21644;&#20004;&#20010;&#29790;&#20856;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#30340;&#20559;&#24046;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#12289;&#21253;&#21547;200&#19975;&#20010;&#26679;&#26412;&#30340;&#29790;&#20856;&#20559;&#24046;&#26631;&#27880;&#25968;&#25454;&#38598;&#21644;&#29992;&#20110;&#29790;&#20856;&#20559;&#24046;&#26816;&#27979;&#30340;&#22810;&#32500;&#24230;&#35789;&#24211;&#12290;</title><link>http://arxiv.org/abs/2301.12139</link><description>&lt;p&gt;
Bipol: &#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#29992;&#21487;&#35299;&#37322;&#24615;&#30340;&#26041;&#24335;&#35780;&#20272;&#22810;&#20010;&#32500;&#24230;&#30340;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Bipol: Multi-axes Evaluation of Bias with Explainability in Benchmark Datasets. (arXiv:2301.12139v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#22810;&#32500;&#24230;&#20559;&#24046;&#24230;&#37327;&#25351;&#26631;bipol&#65292;&#35780;&#20272;&#20102;&#20116;&#20010;&#33521;&#25991;&#21644;&#20004;&#20010;&#29790;&#20856;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#30340;&#20559;&#24046;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#12289;&#21253;&#21547;200&#19975;&#20010;&#26679;&#26412;&#30340;&#29790;&#20856;&#20559;&#24046;&#26631;&#27880;&#25968;&#25454;&#38598;&#21644;&#29992;&#20110;&#29790;&#20856;&#20559;&#24046;&#26816;&#27979;&#30340;&#22810;&#32500;&#24230;&#35789;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#20116;&#20010;&#33521;&#25991;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22522;&#20934;&#25968;&#25454;&#38598;&#65288;&#22312;superGLUE&#27036;&#21333;&#19978;&#65289;&#65292;&#20197;&#21450;&#20004;&#20010;&#29790;&#20856;&#25968;&#25454;&#38598;&#30340;&#20559;&#24046;&#24615;&#36136;&#65292;&#28041;&#21450;&#22810;&#20010;&#32500;&#24230;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#21253;&#25324;&#65306;&#24067;&#23572;&#38382;&#39064;&#65288;Boolq&#65289;&#12289;&#25215;&#35834;&#38134;&#34892;&#65288;CB&#65289;&#12289;Winograd&#27169;&#24335;&#25361;&#25112;&#65288;WSC&#65289;&#12289;Wino-gender&#35786;&#26029;&#65288;AXg&#65289;&#12289;&#25991;&#26412;&#34164;&#21547;&#35782;&#21035;&#65288;RTE&#65289;&#12289;&#29790;&#20856;CB&#21644;SWEDN&#12290;&#20559;&#24046;&#21487;&#33021;&#20855;&#26377;&#23475;&#22788;&#65292;&#24182;&#19988;&#24050;&#30693;&#24120;&#35265;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25152;&#23398;&#20064;&#30340;&#25968;&#25454;&#20013;&#12290;&#20026;&#20102;&#20943;&#36731;&#25968;&#25454;&#20013;&#30340;&#20559;&#24046;&#65292;&#33021;&#22815;&#23458;&#35266;&#20272;&#35745;&#20559;&#24046;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#32500;&#24230;&#20559;&#24046;&#24230;&#37327;&#25351;&#26631;bipol&#65292;&#24182;&#35299;&#37322;&#35813;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#22810;&#23569;&#20559;&#24046;&#12290;&#36328;&#35821;&#35328;&#12289;&#22810;&#32500;&#24230;&#30340;&#20559;&#24046;&#35780;&#20272;&#24182;&#19981;&#24120;&#35265;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36824;&#36129;&#29486;&#20102;&#19968;&#20010;&#26032;&#30340;&#12289;&#21253;&#21547;200&#19975;&#20010;&#26679;&#26412;&#30340;&#29790;&#20856;&#20559;&#24046;&#26631;&#27880;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#20174;&#33521;&#25991;&#29256;&#26412;&#32763;&#35793;&#32780;&#26469;&#65292;&#24182;&#22312;&#20854;&#20013;&#20351;&#29992;&#20102;&#26368;&#20808;&#36827;&#30340;mT5&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20026;&#29790;&#20856;&#20559;&#24046;&#26816;&#27979;&#25552;&#20379;&#20102;&#26032;&#30340;&#22810;&#32500;&#24230;&#35789;&#24211;&#12290;&#25105;&#20204;&#23558;&#20195;&#30721;&#12289;&#27169;&#22411;&#21644;&#26032;&#25968;&#25454;&#38598;&#20844;&#24320;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate five English NLP benchmark datasets (on the superGLUE leaderboard) and two Swedish datasets for bias, along multiple axes. The datasets are the following: Boolean Question (Boolq), CommitmentBank (CB), Winograd Schema Challenge (WSC), Wino-gender diagnostic (AXg), Recognising Textual Entailment (RTE), Swedish CB, and SWEDN. Bias can be harmful and it is known to be common in data, which ML models learn from. In order to mitigate bias in data, it is crucial to be able to estimate it objectively. We use bipol, a novel multi-axes bias metric with explainability, to estimate and explain how much bias exists in these datasets. Multilingual, multi-axes bias evaluation is not very common. Hence, we also contribute a new, large Swedish bias-labelled dataset (of 2 million samples), translated from the English version and train the SotA mT5 model on it. In addition, we contribute new multi-axes lexica for bias detection in Swedish. We make the codes, model, and new dataset publicl
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21327;&#20316;&#22495;&#21069;&#32512;&#35843;&#25972;&#30340;&#36328;&#39046;&#22495;&#23454;&#20307;&#35782;&#21035;&#65292;&#20351;&#29992;&#25991;&#26412;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#25903;&#25745;&#39046;&#22495;&#30456;&#20851;&#25351;&#23548;&#26469;&#23558;&#30693;&#35782;&#36716;&#31227;&#33267;&#26032;&#22495;NER&#20219;&#21153;&#65292;&#36991;&#20813;&#20102;&#20808;&#21069;&#30340;&#20026;&#27599;&#20010;&#39046;&#22495;&#32467;&#26463;&#19968;&#20010;&#20840;&#26032;&#30340;NER&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.10410</link><description>&lt;p&gt;
&#36866;&#29992;&#20110;&#25152;&#26377;&#39046;&#22495;&#30340;&#19968;&#20010;&#27169;&#22411;&#65306;&#22522;&#20110;&#21327;&#20316;&#22495;&#21069;&#32512;&#35843;&#25972;&#30340;&#36328;&#39046;&#22495;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
One Model for All Domains: Collaborative Domain-Prefix Tuning for Cross-Domain NER. (arXiv:2301.10410v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21327;&#20316;&#22495;&#21069;&#32512;&#35843;&#25972;&#30340;&#36328;&#39046;&#22495;&#23454;&#20307;&#35782;&#21035;&#65292;&#20351;&#29992;&#25991;&#26412;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#25903;&#25745;&#39046;&#22495;&#30456;&#20851;&#25351;&#23548;&#26469;&#23558;&#30693;&#35782;&#36716;&#31227;&#33267;&#26032;&#22495;NER&#20219;&#21153;&#65292;&#36991;&#20813;&#20102;&#20808;&#21069;&#30340;&#20026;&#27599;&#20010;&#39046;&#22495;&#32467;&#26463;&#19968;&#20010;&#20840;&#26032;&#30340;NER&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#23454;&#38469;&#22330;&#26223;&#20013;&#20302;&#36164;&#28304;&#38382;&#39064;&#26159;&#36328;&#39046;&#22495;&#23454;&#20307;&#35782;&#21035;&#30340;&#19968;&#20010;&#25361;&#25112;&#24615;&#20219;&#21153;&#12290;&#20808;&#21069;&#20856;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#20027;&#35201;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#20016;&#23500;&#36164;&#28304;&#39046;&#22495;&#30340;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#33719;&#24471;NER&#27169;&#22411;&#24182;&#23558;&#20854;&#36866;&#24212;&#20110;&#30446;&#26631;&#39046;&#22495;&#12290;&#30001;&#20110;&#19981;&#21516;&#39046;&#22495;&#23454;&#20307;&#31867;&#22411;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#35843;&#25972;&#25152;&#26377;PLMs&#30340;&#21442;&#25968;&#65292;&#20174;&#32780;&#20026;&#27599;&#20010;&#39046;&#22495;&#32467;&#26463;&#19968;&#20010;&#20840;&#26032;&#30340;NER&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#30340;&#27169;&#22411;&#21482;&#20851;&#27880;&#20110;&#21033;&#29992;&#19968;&#20010;&#26222;&#36890;&#26469;&#28304;&#39046;&#22495;&#20013;&#30340;&#30693;&#35782;&#65292;&#32780;&#26410;&#33021;&#25104;&#21151;&#22320;&#23558;&#26469;&#33258;&#22810;&#20010;&#26469;&#28304;&#39046;&#22495;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#30446;&#26631;&#19978;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#22522;&#20110;&#25991;&#26412;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;PLM&#24341;&#20837;&#20102;&#21327;&#20316;&#22495;&#21069;&#32512;&#35843;&#25972;&#36328;&#39046;&#22495;NER(CP-NER)&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21576;&#29616;&#20102;&#29992;&#20110;&#25991;&#26412;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#25903;&#25745;&#39046;&#22495;&#30456;&#20851;&#25351;&#23548;&#26469;&#23558;&#30693;&#35782;&#36716;&#31227;&#33267;&#26032;&#22495;NER&#20219;&#21153;&#32780;&#26080;&#38656;&#32467;&#26500;&#20462;&#25913;&#12290;&#25105;&#20204;&#21033;&#29992;&#20923;&#32467;&#30340;PLMs&#24182;&#36827;&#34892;&#21327;&#20316;&#22495;&#21069;&#32512;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-domain NER is a challenging task to address the low-resource problem in practical scenarios. Previous typical solutions mainly obtain a NER model by pre-trained language models (PLMs) with data from a rich-resource domain and adapt it to the target domain. Owing to the mismatch issue among entity types in different domains, previous approaches normally tune all parameters of PLMs, ending up with an entirely new NER model for each domain. Moreover, current models only focus on leveraging knowledge in one general source domain while failing to successfully transfer knowledge from multiple sources to the target. To address these issues, we introduce Collaborative Domain-Prefix Tuning for cross-domain NER (CP-NER) based on text-to-text generative PLMs. Specifically, we present text-to-text generation grounding domain-related instructors to transfer knowledge to new domain NER tasks without structural modifications. We utilize frozen PLMs and conduct collaborative domain-prefix tuning
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#32508;&#21512;&#35843;&#26597;&#12290;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#25552;&#20379;&#31995;&#32479;&#36164;&#28304;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;</title><link>http://arxiv.org/abs/2212.09597</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Reasoning with Language Model Prompting: A Survey. (arXiv:2212.09597v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#32508;&#21512;&#35843;&#26597;&#12290;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#25552;&#20379;&#31995;&#32479;&#36164;&#28304;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;&#20316;&#20026;&#22797;&#26434;&#38382;&#39064;&#35299;&#20915;&#30340;&#37325;&#35201;&#33021;&#21147;&#65292;&#21487;&#20197;&#20026;&#21307;&#30103;&#35786;&#26029;&#12289;&#35848;&#21028;&#31561;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#21518;&#31471;&#25903;&#25345;&#12290;&#26412;&#25991;&#23545;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#36827;&#34892;&#20102;&#32508;&#21512;&#35843;&#26597;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#30740;&#31350;&#25104;&#26524;&#30340;&#27604;&#36739;&#21644;&#24635;&#32467;&#65292;&#24182;&#25552;&#20379;&#20102;&#31995;&#32479;&#36164;&#28304;&#20197;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#31361;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#36164;&#28304;&#21487;&#22312; https://github.com/zjunlp/Prompt4ReasoningPapers &#19978;&#33719;&#21462;&#65288;&#23450;&#26399;&#26356;&#26032;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning, as an essential ability for complex problem-solving, can provide back-end support for various real-world applications, such as medical diagnosis, negotiation, etc. This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting. We introduce research works with comparisons and summaries and provide systematic resources to help beginners. We also discuss the potential reasons for emerging such reasoning abilities and highlight future research directions. Resources are available at https://github.com/zjunlp/Prompt4ReasoningPapers (updated periodically).
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;VRDU&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#26356;&#20840;&#38754;&#22320;&#21453;&#26144;&#23454;&#38469;&#25991;&#26723;&#30340;&#22797;&#26434;&#24615;&#65292;&#20854;&#20013;&#21253;&#21547;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20016;&#23500;&#27169;&#24335;&#12289;&#22797;&#26434;&#27169;&#26495;&#21644;&#22810;&#26679;&#30340;&#24067;&#23616;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#21487;&#29992;&#20110;&#35780;&#20272;&#25991;&#26723;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2211.15421</link><description>&lt;p&gt;
VRDU&#65306;&#38754;&#21521;&#35270;&#35273;&#20016;&#23500;&#30340;&#25991;&#26723;&#29702;&#35299;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
VRDU: A Benchmark for Visually-rich Document Understanding. (arXiv:2211.15421v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;VRDU&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#26356;&#20840;&#38754;&#22320;&#21453;&#26144;&#23454;&#38469;&#25991;&#26723;&#30340;&#22797;&#26434;&#24615;&#65292;&#20854;&#20013;&#21253;&#21547;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20016;&#23500;&#27169;&#24335;&#12289;&#22797;&#26434;&#27169;&#26495;&#21644;&#22810;&#26679;&#30340;&#24067;&#23616;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#21487;&#29992;&#20110;&#35780;&#20272;&#25991;&#26723;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#20016;&#23500;&#35270;&#35273;&#21270;&#19994;&#21153;&#25991;&#26723;&#20197;&#25552;&#21462;&#32467;&#26500;&#21270;&#25968;&#25454;&#21644;&#33258;&#21160;&#21270;&#19994;&#21153;&#24037;&#20316;&#27969;&#31243;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#37117;&#21463;&#21040;&#20851;&#27880;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#22810;&#27169;&#24335;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#26524;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#19981;&#21453;&#26144;&#24037;&#19994;&#20013;&#23454;&#38469;&#25991;&#26723;&#30340;&#22797;&#26434;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#26356;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#30340;&#24517;&#35201;&#26465;&#20214;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;Visually Rich Document Understanding (VRDU)&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;VRDU&#21253;&#21547;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;&#20195;&#34920;&#20102;&#22810;&#31181;&#25361;&#25112;&#65306;&#20016;&#23500;&#30340;&#27169;&#24335;&#65292;&#21253;&#25324;&#21508;&#31181;&#25968;&#25454;&#31867;&#22411;&#20197;&#21450;&#20998;&#23618;&#23454;&#20307;; &#22797;&#26434;&#30340;&#27169;&#26495;&#65292;&#21253;&#25324;&#34920;&#26684;&#21644;&#22810;&#21015;&#24067;&#23616;; &#20197;&#21450;&#21333;&#20010;&#25991;&#26723;&#31867;&#22411;&#20013;&#19981;&#21516;&#24067;&#23616;&#65288;&#27169;&#26495;&#65289;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#23569;&#26679;&#26412;&#21644;&#24120;&#35268;&#23454;&#39564;&#35774;&#32622;&#65292;&#20197;&#21450;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#21305;&#37197;&#31639;&#27861;&#26469;&#35780;&#20272;&#25552;&#21462;&#32467;&#26524;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#24378;&#22522;&#32447;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#19977;&#20010;&#35266;&#23519;&#32467;&#26524;&#65306;(1)&#36890;&#29992;n&#30340;&#25512;&#24191;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding visually-rich business documents to extract structured data and automate business workflows has been receiving attention both in academia and industry. Although recent multi-modal language models have achieved impressive results, we find that existing benchmarks do not reflect the complexity of real documents seen in industry. In this work, we identify the desiderata for a more comprehensive benchmark and propose one we call Visually Rich Document Understanding (VRDU). VRDU contains two datasets that represent several challenges: rich schema including diverse data types as well as hierarchical entities, complex templates including tables and multi-column layouts, and diversity of different layouts (templates) within a single document type. We design few-shot and conventional experiment settings along with a carefully designed matching algorithm to evaluate extraction results. We report the performance of strong baselines and offer three observations: (1) generalizing to n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#29983;&#25104;&#24335;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#26041;&#27861;&#20998;&#31867;&#21644;&#20248;&#21155;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2210.12714</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Generative Knowledge Graph Construction: A Review. (arXiv:2210.12714v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#29983;&#25104;&#24335;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#26041;&#27861;&#20998;&#31867;&#21644;&#20248;&#21155;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#65288;KGC&#65289;&#26159;&#25351;&#21033;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#26694;&#26550;&#26500;&#24314;&#28789;&#27963;&#19988;&#21487;&#36866;&#29992;&#20110;&#24191;&#27867;&#20219;&#21153;&#30340;&#30693;&#35782;&#22270;&#35889;&#12290;&#26412;&#30740;&#31350;&#24635;&#32467;&#20102;&#29983;&#25104;&#24335;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#39046;&#22495;&#20013;&#36817;&#26399;&#30340;&#37325;&#35201;&#36827;&#23637;&#65292;&#23545;&#19981;&#21516;&#30340;&#29983;&#25104;&#30446;&#26631;&#20174;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#35282;&#24230;&#20998;&#21035;&#35752;&#35770;&#20102;&#21508;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#21644;&#19981;&#36275;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#26377;&#28508;&#21147;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#19977;&#20010;&#26041;&#38754;&#65306;&#65288;1&#65289;&#25105;&#20204;&#25552;&#20379;&#20102;&#29983;&#25104;&#24335;KGC&#26041;&#27861;&#30340;&#35814;&#32454;&#12289;&#23436;&#25972;&#30340;&#20998;&#31867;&#20307;&#31995;&#65307;&#65288;2&#65289;&#25105;&#20204;&#23545;&#29983;&#25104;&#24335;KGC&#26041;&#27861;&#36827;&#34892;&#20102;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#65307;&#65288;3&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#26410;&#26469;&#21487;&#20197;&#21457;&#23637;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Knowledge Graph Construction (KGC) refers to those methods that leverage the sequence-to-sequence framework for building knowledge graphs, which is flexible and can be adapted to widespread tasks. In this study, we summarize the recent compelling progress in generative knowledge graph construction. We present the advantages and weaknesses of each paradigm in terms of different generation targets and provide theoretical insight and empirical analysis. Based on the review, we suggest promising research directions for the future. Our contributions are threefold: (1) We present a detailed, complete taxonomy for the generative KGC methods; (2) We provide a theoretical and empirical analysis of the generative KGC methods; (3) We propose several research directions that can be developed in the future.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#26816;&#32034;&#22686;&#24378;&#30340;&#26550;&#26500;&#24863;&#30693;&#21442;&#32771;&#20316;&#20026;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#21487;&#21160;&#24577;&#21033;&#29992;&#20154;&#31867;&#27880;&#37322;&#21644;&#24369;&#30417;&#30563;&#25968;&#25454;&#25152;&#32487;&#25215;&#30340;&#26550;&#26500;&#21644;&#30693;&#35782;&#65292;&#25351;&#23548;&#29983;&#25104;&#20855;&#26377;&#26356;&#22909;&#35821;&#20041;&#36830;&#36143;&#24615;&#21644;&#19968;&#33268;&#24615;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#20174;&#32780;&#22312;&#25968;&#25454;&#25928;&#29575;&#21644;&#30693;&#35782;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.10709</link><description>&lt;p&gt;
&#20197;&#26550;&#26500;&#24863;&#30693;&#21442;&#32771;&#20316;&#20026;&#25552;&#31034;&#25552;&#39640;&#20102;&#25968;&#25454;&#26377;&#25928;&#30340;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Schema-aware Reference as Prompt Improves Data-Efficient Knowledge Graph Construction. (arXiv:2210.10709v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10709
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#26816;&#32034;&#22686;&#24378;&#30340;&#26550;&#26500;&#24863;&#30693;&#21442;&#32771;&#20316;&#20026;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#21487;&#21160;&#24577;&#21033;&#29992;&#20154;&#31867;&#27880;&#37322;&#21644;&#24369;&#30417;&#30563;&#25968;&#25454;&#25152;&#32487;&#25215;&#30340;&#26550;&#26500;&#21644;&#30693;&#35782;&#65292;&#25351;&#23548;&#29983;&#25104;&#20855;&#26377;&#26356;&#22909;&#35821;&#20041;&#36830;&#36143;&#24615;&#21644;&#19968;&#33268;&#24615;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#20174;&#32780;&#22312;&#25968;&#25454;&#25928;&#29575;&#21644;&#30693;&#35782;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#35768;&#22810;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#34987;&#25552;&#20986;&#24182;&#22312;&#25968;&#25454;&#26377;&#25928;&#30340;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#26041;&#27861;&#20173;&#23384;&#22312;&#20960;&#20010;&#28508;&#22312;&#30340;&#38480;&#21046;&#65306;&#65288;i&#65289;&#33258;&#28982;&#35821;&#35328;&#21644;&#39044;&#23450;&#20041;&#27169;&#24335;&#30340;&#36755;&#20986;&#32467;&#26500;&#21270;&#30693;&#35782;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#36317;&#65292;&#36825;&#24847;&#21619;&#30528;&#27169;&#22411;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#21463;&#38480;&#27169;&#26495;&#30340;&#35821;&#20041;&#30693;&#35782;&#65307;&#65288;ii&#65289;&#22522;&#20110;&#23616;&#37096;&#20010;&#20307;&#23454;&#20363;&#30340;&#34920;&#31034;&#23398;&#20064;&#38480;&#21046;&#20102;&#24615;&#33021;&#65292;&#32473;&#23450;&#20102;&#19981;&#20805;&#36275;&#30340;&#29305;&#24449;&#65292;&#36825;&#20123;&#29305;&#24449;&#19981;&#33021;&#37322;&#25918;&#39044;&#20808;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#22312;&#31867;&#27604;&#33021;&#21147;&#12290;&#21463;&#36825;&#20123;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#26816;&#32034;&#24471;&#21040;&#30340;&#26550;&#26500;&#24863;&#30693;&#21442;&#32771;&#20316;&#20026;&#25552;&#31034;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#26377;&#25928;&#30340;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#30340;&#35821;&#20041;&#36830;&#36143;&#24615;&#21644;&#19968;&#33268;&#24615;&#12290;&#22312;&#20004;&#20010;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#27604;&#29616;&#26377;&#30340;&#22522;&#20110;&#25552;&#31034;&#21644;&#38750;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#25968;&#25454;&#25928;&#29575;&#21644;&#30693;&#35782;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of pre-trained language models, many prompt-based approaches to data-efficient knowledge graph construction have been proposed and achieved impressive performance. However, existing prompt-based learning methods for knowledge graph construction are still susceptible to several potential limitations: (i) semantic gap between natural language and output structured knowledge with pre-defined schema, which means model cannot fully exploit semantic knowledge with the constrained templates; (ii) representation learning with locally individual instances limits the performance given the insufficient features, which are unable to unleash the potential analogical capability of pre-trained language models. Motivated by these observations, we propose a retrieval-augmented approach, which retrieves schema-aware Reference As Prompt (RAP), for data-efficient knowledge graph construction. It can dynamically leverage schema and knowledge inherited from human-annotated and weak-supe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#30340;&#20851;&#31995;&#25277;&#21462;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#26041;&#26696;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#21253;&#25324;&#20351;&#29992;&#25552;&#31034;&#26041;&#27861;&#12289;&#24179;&#34913;&#26041;&#27861;&#21644;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#12290;&#36890;&#36807;&#23545;8&#20010;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#27604;&#36739;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#22522;&#20110;&#25552;&#31034;&#30340;&#35843;&#25972;&#26377;&#30410;&#20110;&#20302;&#36164;&#28304;&#20851;&#31995;&#25277;&#21462;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#65292;&#23588;&#20854;&#26159;&#36328;&#21477;&#23376;&#19978;&#19979;&#25991;&#20013;&#30340;&#22810;&#20010;&#20851;&#31995;&#19977;&#20803;&#32452;&#30340;&#25277;&#21462;&#12290;</title><link>http://arxiv.org/abs/2210.10678</link><description>&lt;p&gt;
&#23454;&#29616;&#30495;&#23454;&#20302;&#36164;&#28304;&#20851;&#31995;&#25277;&#21462;: &#38024;&#23545;&#20855;&#26377;&#23454;&#35777;&#22522;&#20934;&#30740;&#31350;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Towards Realistic Low-resource Relation Extraction: A Benchmark with Empirical Baseline Study. (arXiv:2210.10678v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10678
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#30340;&#20851;&#31995;&#25277;&#21462;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#26041;&#26696;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#21253;&#25324;&#20351;&#29992;&#25552;&#31034;&#26041;&#27861;&#12289;&#24179;&#34913;&#26041;&#27861;&#21644;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#12290;&#36890;&#36807;&#23545;8&#20010;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#27604;&#36739;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#22522;&#20110;&#25552;&#31034;&#30340;&#35843;&#25972;&#26377;&#30410;&#20110;&#20302;&#36164;&#28304;&#20851;&#31995;&#25277;&#21462;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#65292;&#23588;&#20854;&#26159;&#36328;&#21477;&#23376;&#19978;&#19979;&#25991;&#20013;&#30340;&#22810;&#20010;&#20851;&#31995;&#19977;&#20803;&#32452;&#30340;&#25277;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#38024;&#23545;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#26500;&#24314;&#20851;&#31995;&#25277;&#21462;&#31995;&#32479;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;&#22522;&#20110;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#20840;&#38754;&#35843;&#26597;&#20102;&#19977;&#31181;&#26041;&#26696;&#26469;&#35780;&#20272;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#30340;&#24615;&#33021;&#65306;(i) &#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#19981;&#21516;&#31867;&#22411;&#30340;&#25552;&#31034;&#26041;&#27861;&#65307; (ii) &#22810;&#26679;&#21270;&#30340;&#24179;&#34913;&#26041;&#27861;&#26469;&#35299;&#20915;&#38271;&#23614;&#20998;&#24067;&#38382;&#39064;&#65307; (iii) &#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#21644;&#33258;&#35757;&#32451;&#26469;&#29983;&#25104;&#26356;&#22810;&#39046;&#22495;&#20869;&#26631;&#35760;&#25968;&#25454;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;8&#20010;&#20851;&#31995;&#25277;&#21462;(RE) &#25968;&#25454;&#38598;&#30340;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#19981;&#21516;&#30340;&#35821;&#35328;&#12289;&#39046;&#22495;&#21644;&#19978;&#19979;&#25991;&#65292;&#24182;&#23545;&#25152;&#25552;&#20986;&#30340;&#26041;&#26696;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65306;(i) &#34429;&#28982;&#22522;&#20110;&#25552;&#31034;&#30340;&#35843;&#25972;&#22312;&#20302;&#36164;&#28304;&#20851;&#31995;&#25277;&#21462;&#20013;&#26159;&#26377;&#30410;&#30340;&#65292;&#20294;&#20173;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#25552;&#21462;&#36328;&#21477;&#23376;&#19978;&#19979;&#25991;&#20013;&#30340;&#22810;&#20010;&#20851;&#31995;&#19977;&#20803;&#32452;&#26041;&#38754;&#65307; (ii) &#24179;&#34913;&#26041;&#27861;&#24182;&#19981;&#24635;&#26159;&#26377;&#21161;&#20110;&#38271;&#23614;&#20998;&#24067;&#30340;&#20851;&#31995;&#25277;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an empirical study to build relation extraction systems in low-resource settings. Based upon recent pre-trained language models, we comprehensively investigate three schemes to evaluate the performance in low-resource settings: (i) different types of prompt-based methods with few-shot labeled data; (ii) diverse balancing methods to address the long-tailed distribution issue; (iii) data augmentation technologies and self-training to generate more labeled in-domain data. We create a benchmark with 8 relation extraction (RE) datasets covering different languages, domains and contexts and perform extensive comparisons over the proposed schemes with combinations. Our experiments illustrate: (i) Though prompt-based tuning is beneficial in low-resource RE, there is still much potential for improvement, especially in extracting relations from cross-sentence contexts with multiple relational triples; (ii) Balancing methods are not always helpful for RE with long-tailed distr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Word2Vec&#36827;&#34892;&#20027;&#39064;&#30340;&#26102;&#38388;&#20998;&#26512;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#24314;&#27169;&#20027;&#39064;&#30340;&#31227;&#21160;&#24182;&#21033;&#29992;k&#22343;&#20540;&#32858;&#31867;&#21644;&#20313;&#24358;&#30456;&#20284;&#24230;&#23545;&#36317;&#31163;&#36827;&#34892;&#20998;&#32452;&#65292;&#23454;&#29616;&#20102;&#35782;&#21035;&#21644;&#21487;&#35270;&#21270;&#20027;&#39064;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2209.11717</link><description>&lt;p&gt;
&#20351;&#29992;Word2Vec&#36827;&#34892;&#20027;&#39064;&#30340;&#26102;&#38388;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Temporal Analysis on Topics Using Word2Vec. (arXiv:2209.11717v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.11717
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Word2Vec&#36827;&#34892;&#20027;&#39064;&#30340;&#26102;&#38388;&#20998;&#26512;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#24314;&#27169;&#20027;&#39064;&#30340;&#31227;&#21160;&#24182;&#21033;&#29992;k&#22343;&#20540;&#32858;&#31867;&#21644;&#20313;&#24358;&#30456;&#20284;&#24230;&#23545;&#36317;&#31163;&#36827;&#34892;&#20998;&#32452;&#65292;&#23454;&#29616;&#20102;&#35782;&#21035;&#21644;&#21487;&#35270;&#21270;&#20027;&#39064;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36235;&#21183;&#26816;&#27979;&#21644;&#21487;&#35270;&#21270;&#26041;&#27861;&#65292;&#20855;&#20307;&#26469;&#35828;&#65292;&#26159;&#23545;&#20027;&#39064;&#38543;&#26102;&#38388;&#21464;&#21270;&#36827;&#34892;&#24314;&#27169;&#12290;&#30446;&#21069;&#29992;&#20110;&#35782;&#21035;&#21644;&#21487;&#35270;&#21270;&#36235;&#21183;&#30340;&#27169;&#22411;&#20165;&#22522;&#20110;&#20351;&#29992;&#30340;&#38543;&#26426;&#35745;&#25968;&#26469;&#20256;&#36798;&#21333;&#35789;&#30340;&#27969;&#34892;&#31243;&#24230;&#65292;&#32780;&#26412;&#30740;&#31350;&#30340;&#26041;&#27861;&#21017;&#23637;&#31034;&#20102;&#20027;&#39064;&#30340;&#27969;&#34892;&#31243;&#24230;&#21644;&#21457;&#23637;&#26041;&#21521;&#12290;&#22312;&#26412;&#26696;&#20363;&#20013;&#65292;&#36825;&#20010;&#26041;&#21521;&#26159;&#25152;&#36873;&#35821;&#26009;&#24211;&#20013;&#30340;&#19968;&#20010;&#29420;&#29305;&#23376;&#20027;&#39064;&#12290;&#36825;&#20123;&#36235;&#21183;&#26159;&#36890;&#36807;&#20351;&#29992;k&#22343;&#20540;&#32858;&#31867;&#21644;&#20313;&#24358;&#30456;&#20284;&#24230;&#23545;&#31751;&#20043;&#38388;&#30340;&#36317;&#31163;&#36827;&#34892;&#20998;&#32452;&#26469;&#24314;&#27169;&#20027;&#39064;&#30340;&#31227;&#21160;&#32780;&#20135;&#29983;&#30340;&#12290;&#22312;&#25910;&#25947;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#25512;&#26029;&#25972;&#20307;&#20027;&#39064;&#27491;&#22312;&#34701;&#21512;&#65288;&#20027;&#39064;&#20043;&#38388;&#30340;&#26631;&#35760;&#21464;&#24471;&#21487;&#20114;&#25442;&#65289;&#12290;&#30456;&#21453;&#65292;&#20998;&#25955;&#30340;&#24773;&#20917;&#21017;&#24847;&#21619;&#30528;&#27599;&#20010;&#20027;&#39064;&#30340;&#26631;&#35760;&#22312;&#30456;&#21516;&#30340;&#19978;&#19979;&#25991;&#20013;&#19981;&#20250;&#34987;&#21457;&#29616;&#65288;&#21333;&#35789;&#20043;&#38388;&#36234;&#26469;&#36234;&#19981;&#21516;&#65289;&#12290;&#35813;&#26041;&#27861;&#22312;&#19968;&#20010;&#32676;&#20307;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
The present study proposes a novel method of trend detection and visualization - more specifically, modeling the change in a topic over time. Where current models used for the identification and visualization of trends only convey the popularity of a singular word based on stochastic counting of usage, the approach in the present study illustrates the popularity and direction that a topic is moving in. The direction in this case is a distinct subtopic within the selected corpus. Such trends are generated by modeling the movement of a topic by using k-means clustering and cosine similarity to group the distances between clusters over time. In a convergent scenario, it can be inferred that the topics as a whole are meshing (tokens between topics, becoming interchangeable). On the contrary, a divergent scenario would imply that each topics' respective tokens would not be found in the same context (the words are increasingly different to each other). The methodology was tested on a group o
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#26641;&#30340;&#24179;&#38754;&#32447;&#24615;&#21270;&#20013;&#36793;&#38271;&#24230;&#30340;&#26399;&#26395;&#21644;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#24179;&#38754;&#25490;&#21015;&#30340;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;&#24179;&#38754;&#25490;&#21015;&#19982;&#25237;&#24433;&#25490;&#21015;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2207.05564</link><description>&lt;p&gt;
&#26641;&#30340;&#24179;&#38754;&#32447;&#24615;&#21270;&#20013;&#36793;&#38271;&#24230;&#30340;&#26399;&#26395;&#21644;&#65306;&#29702;&#35770;&#19982;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
The expected sum of edge lengths in planar linearizations of trees. Theory and applications. (arXiv:2207.05564v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.05564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#26641;&#30340;&#24179;&#38754;&#32447;&#24615;&#21270;&#20013;&#36793;&#38271;&#24230;&#30340;&#26399;&#26395;&#21644;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#24179;&#38754;&#25490;&#21015;&#30340;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;&#24179;&#38754;&#25490;&#21015;&#19982;&#25237;&#24433;&#25490;&#21015;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20381;&#36182;&#26641;&#24050;&#34987;&#35777;&#26126;&#26159;&#34920;&#31034;&#20154;&#31867;&#35821;&#35328;&#21477;&#23376;&#30340;&#21477;&#27861;&#32467;&#26500;&#30340;&#38750;&#24120;&#25104;&#21151;&#30340;&#27169;&#22411;&#12290;&#22312;&#36825;&#20123;&#32467;&#26500;&#20013;&#65292;&#39030;&#28857;&#26159;&#21333;&#35789;&#65292;&#36793;&#36830;&#25509;&#35821;&#27861;&#30456;&#20851;&#30340;&#21333;&#35789;&#12290;&#20351;&#29992;&#38543;&#26426;&#22522;&#32447;&#26469;&#35745;&#31639;&#36793;&#38271;&#24230;&#20043;&#21644;&#25110;&#20854;&#21464;&#20307;&#65292;&#24050;&#32463;&#35777;&#26126;&#20102;&#36825;&#20123;&#20381;&#36182;&#20851;&#31995;&#30340;&#20542;&#21521;&#26159;&#30701;&#30340;&#12290;&#19968;&#20010;&#26222;&#36941;&#23384;&#22312;&#30340;&#22522;&#32447;&#26159;&#22312;&#25237;&#24433;&#25490;&#24207;&#20013;&#30340;&#26399;&#26395;&#21644;&#65288;&#20854;&#20013;&#36793;&#19981;&#30456;&#20132;&#65292;&#24182;&#19988;&#21477;&#23376;&#30340;&#26681;&#35789;&#27809;&#26377;&#34987;&#20219;&#20309;&#36793;&#35206;&#30422;&#65289;&#65292;&#21487;&#20197;&#22312;$O(n)$&#26102;&#38388;&#20869;&#35745;&#31639;&#24471;&#21040;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20851;&#27880;&#19968;&#20010;&#36739;&#24369;&#30340;&#24418;&#24335;&#32422;&#26463;&#65292;&#21363;&#24179;&#38754;&#24615;&#12290;&#22312;&#29702;&#35770;&#39046;&#22495;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21051;&#30011;&#24179;&#38754;&#24615;&#30340;&#26041;&#27861;&#65292;&#32473;&#23450;&#19968;&#20010;&#21477;&#23376;&#65292;&#21487;&#20197;&#24471;&#21040;&#24179;&#38754;&#25490;&#21015;&#30340;&#25968;&#37327;&#25110;&#20197;&#22343;&#21248;&#38543;&#26426;&#26041;&#24335;&#29983;&#25104;&#24179;&#38754;&#25490;&#21015;&#30340;&#26377;&#25928;&#31639;&#27861;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#24179;&#38754;&#25490;&#21015;&#20013;&#30340;&#26399;&#26395;&#24635;&#21644;&#19982;&#25237;&#24433;&#25490;&#21015;&#20013;&#30340;&#26399;&#26395;&#24635;&#21644;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dependency trees have proven to be a very successful model to represent the syntactic structure of sentences of human languages. In these structures, vertices are words and edges connect syntactically-dependent words. The tendency of these dependencies to be short has been demonstrated using random baselines for the sum of the lengths of the edges or its variants. A ubiquitous baseline is the expected sum in projective orderings (wherein edges do not cross and the root word of the sentence is not covered by any edge), that can be computed in time $O(n)$. Here we focus on a weaker formal constraint, namely planarity. In the theoretical domain, we present a characterization of planarity that, given a sentence, yields either the number of planar permutations or an efficient algorithm to generate uniformly random planar permutations of the words. We also show the relationship between the expected sum in planar arrangements and the expected sum in projective arrangements. In the domain of a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;Transformer&#19982;&#22810;&#32423;&#34701;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32479;&#19968;&#30340;&#36755;&#20837;-&#36755;&#20986;&#26550;&#26500;&#36866;&#29992;&#20110;&#22810;&#26679;&#30340;&#20219;&#21153;&#65292;&#21516;&#26102;&#21033;&#29992;&#22810;&#32423;&#34701;&#21512;&#23558;&#35270;&#35273;&#21644;&#25991;&#26412;&#34920;&#31034;&#38598;&#25104;&#36215;&#26469;&#12290;</title><link>http://arxiv.org/abs/2205.02357</link><description>&lt;p&gt;
&#28151;&#21512;Transformer&#19982;&#22810;&#32423;&#34701;&#21512;&#29992;&#20110;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Hybrid Transformer with Multi-level Fusion for Multimodal Knowledge Graph Completion. (arXiv:2205.02357v5 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.02357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;Transformer&#19982;&#22810;&#32423;&#34701;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32479;&#19968;&#30340;&#36755;&#20837;-&#36755;&#20986;&#26550;&#26500;&#36866;&#29992;&#20110;&#22810;&#26679;&#30340;&#20219;&#21153;&#65292;&#21516;&#26102;&#21033;&#29992;&#22810;&#32423;&#34701;&#21512;&#23558;&#35270;&#35273;&#21644;&#25991;&#26412;&#34920;&#31034;&#38598;&#25104;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#65288;MKG&#65289;&#22312;&#20449;&#24687;&#26816;&#32034;&#12289;&#38382;&#31572;&#21644;&#25512;&#33616;&#31995;&#32479;&#31561;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;MKG&#32452;&#32455;&#20102;&#35270;&#35273;-&#25991;&#26412;&#20107;&#23454;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22823;&#22810;&#25968;MKG&#37117;&#19981;&#23436;&#25972;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#24191;&#27867;&#30340;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30740;&#31350;&#65292;&#37325;&#28857;&#20851;&#27880;&#22810;&#27169;&#24577;&#23454;&#20307;&#12289;&#20851;&#31995;&#25552;&#21462;&#21644;&#38142;&#25509;&#39044;&#27979;&#12290;&#26412;&#25991;&#38024;&#23545;&#36825;&#20123;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;Transformer&#19982;&#22810;&#32423;&#34701;&#21512;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#31181;&#28151;&#21512;Transformer&#26550;&#26500;&#21644;&#32479;&#19968;&#30340;&#36755;&#20837;-&#36755;&#20986;&#26469;&#23436;&#25104;&#22810;&#26679;&#30340;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#32423;&#34701;&#21512;&#65292;&#36890;&#36807;&#31895;&#31890;&#24230;&#21069;&#32512;&#24341;&#23548;&#20132;&#20114;&#21644;&#32454;&#31890;&#24230;&#30456;&#20851;&#24863;&#30693;&#23558;&#35270;&#35273;&#21644;&#25991;&#26412;&#34920;&#31034;&#38598;&#25104;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Knowledge Graphs (MKGs), which organize visual-text factual knowledge, have recently been successfully applied to tasks such as information retrieval, question answering, and recommendation system. Since most MKGs are far from complete, extensive knowledge graph completion studies have been proposed focusing on the multimodal entity, relation extraction and link prediction. However, different tasks and modalities require changes to the model architecture, and not all images/objects are relevant to text input, which hinders the applicability to diverse real-world scenarios. In this paper, we propose a hybrid transformer with multi-level fusion to address those issues. Specifically, we leverage a hybrid transformer architecture with unified input-output for diverse multimodal knowledge graph completion tasks. Moreover, we propose multi-level fusion, which integrates visual and text representation via coarse-grained prefix-guided interaction and fine-grained correlation-aware f
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#28040;&#36153;&#32773;&#30340;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#21382;&#21490;&#26159;&#30740;&#31350;&#20998;&#20139;&#34394;&#20551;&#26032;&#38395;&#21160;&#26426;&#30340;&#19968;&#31181;&#34987;&#20302;&#20272;&#30340;&#25968;&#25454;&#26469;&#28304;&#12290;&#36890;&#36807;&#23545;&#24086;&#23376;&#21382;&#21490;&#25552;&#21462;&#30340;&#25991;&#26412;&#32447;&#32034;&#65292;&#25105;&#20204;&#21457;&#29616;&#34394;&#20551;&#26032;&#38395;&#20998;&#20139;&#32773;&#22312;&#35328;&#36766;&#19978;&#26356;&#22810;&#28041;&#21450;&#24868;&#24594;&#12289;&#23447;&#25945;&#21644;&#26435;&#21147;&#12290;&#24182;&#19988;&#65292;&#36890;&#36807;&#23558;&#24086;&#23376;&#21382;&#21490;&#20013;&#30340;&#25991;&#26412;&#32447;&#32034;&#21152;&#20837;&#27169;&#22411;&#65292;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20998;&#20139;&#34394;&#20551;&#26032;&#38395;&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#28608;&#27963;&#23447;&#25945;&#20215;&#20540;&#35266;&#21644;&#20943;&#23569;&#24868;&#24594;&#65292;&#21487;&#20197;&#20943;&#23569;&#34394;&#20551;&#26032;&#38395;&#30340;&#20998;&#20139;&#21644;&#26356;&#24191;&#27867;&#30340;&#20998;&#20139;&#12290;</title><link>http://arxiv.org/abs/2203.10560</link><description>&lt;p&gt;
&#25552;&#21319;&#34394;&#20551;&#26032;&#38395;&#32531;&#35299;&#65306;&#26469;&#33258;&#20998;&#20139;&#32773;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#21382;&#21490;&#30340;&#27934;&#23519;&#21147;
&lt;/p&gt;
&lt;p&gt;
Empowering Fake-News Mitigation: Insights from Sharers' Social Media Post-Histories. (arXiv:2203.10560v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.10560
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#28040;&#36153;&#32773;&#30340;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#21382;&#21490;&#26159;&#30740;&#31350;&#20998;&#20139;&#34394;&#20551;&#26032;&#38395;&#21160;&#26426;&#30340;&#19968;&#31181;&#34987;&#20302;&#20272;&#30340;&#25968;&#25454;&#26469;&#28304;&#12290;&#36890;&#36807;&#23545;&#24086;&#23376;&#21382;&#21490;&#25552;&#21462;&#30340;&#25991;&#26412;&#32447;&#32034;&#65292;&#25105;&#20204;&#21457;&#29616;&#34394;&#20551;&#26032;&#38395;&#20998;&#20139;&#32773;&#22312;&#35328;&#36766;&#19978;&#26356;&#22810;&#28041;&#21450;&#24868;&#24594;&#12289;&#23447;&#25945;&#21644;&#26435;&#21147;&#12290;&#24182;&#19988;&#65292;&#36890;&#36807;&#23558;&#24086;&#23376;&#21382;&#21490;&#20013;&#30340;&#25991;&#26412;&#32447;&#32034;&#21152;&#20837;&#27169;&#22411;&#65292;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20998;&#20139;&#34394;&#20551;&#26032;&#38395;&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#28608;&#27963;&#23447;&#25945;&#20215;&#20540;&#35266;&#21644;&#20943;&#23569;&#24868;&#24594;&#65292;&#21487;&#20197;&#20943;&#23569;&#34394;&#20551;&#26032;&#38395;&#30340;&#20998;&#20139;&#21644;&#26356;&#24191;&#27867;&#30340;&#20998;&#20139;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#20551;&#20449;&#24687;&#26159;&#19968;&#20010;&#20840;&#29699;&#24615;&#38382;&#39064;&#65292;&#38480;&#21046;&#20854;&#20256;&#25773;&#23545;&#20445;&#25252;&#27665;&#20027;&#12289;&#20844;&#20849;&#21355;&#29983;&#21644;&#28040;&#36153;&#32773;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#35748;&#20026;&#28040;&#36153;&#32773;&#33258;&#24049;&#30340;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#21382;&#21490;&#26159;&#19968;&#20010;&#34987;&#20302;&#20272;&#30340;&#25968;&#25454;&#26469;&#28304;&#65292;&#29992;&#20110;&#30740;&#31350;&#26159;&#20160;&#20040;&#23548;&#33268;&#20182;&#20204;&#20998;&#20139;&#34394;&#20551;&#26032;&#38395;&#38142;&#25509;&#12290;&#22312;&#31532;&#19968;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20174;&#24086;&#23376;&#21382;&#21490;&#20013;&#25552;&#21462;&#30340;&#25991;&#26412;&#32447;&#32034;&#22914;&#20309;&#21306;&#20998;&#34394;&#20551;&#26032;&#38395;&#30340;&#20998;&#20139;&#32773;&#21644;&#38543;&#26426;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#20197;&#21450;&#20854;&#20182;&#22312;&#35823;&#23548;&#20449;&#24687;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#20154;&#12290;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#34394;&#20551;&#26032;&#38395;&#30340;&#20998;&#20139;&#32773;&#20351;&#29992;&#26356;&#22810;&#19982;&#24868;&#24594;&#12289;&#23447;&#25945;&#21644;&#26435;&#21147;&#30456;&#20851;&#30340;&#35789;&#27719;&#12290;&#22312;&#31532;&#20108;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20174;&#24086;&#23376;&#21382;&#21490;&#20013;&#28155;&#21152;&#25991;&#26412;&#32447;&#32034;&#22914;&#20309;&#25552;&#39640;&#27169;&#22411;&#39044;&#27979;&#35841;&#26377;&#21487;&#33021;&#20998;&#20139;&#34394;&#20551;&#26032;&#38395;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#31532;&#19977;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#20174;&#31532;&#19968;&#39033;&#30740;&#31350;&#20013;&#25512;&#23548;&#20986;&#30340;&#20004;&#31181;&#32531;&#35299;&#31574;&#30053;&#36827;&#34892;&#20102;&#21021;&#27493;&#27979;&#35797;&#65292;&#21363;&#28608;&#27963;&#23447;&#25945;&#20215;&#20540;&#35266;&#21644;&#20943;&#23569;&#24868;&#24594;&#65292;&#21457;&#29616;&#23427;&#20204;&#21487;&#20197;&#20943;&#23569;&#34394;&#20551;&#26032;&#38395;&#30340;&#20998;&#20139;&#21644;&#26356;&#24191;&#27867;&#30340;&#20998;&#20139;&#12290;&#22312;&#31532;&#22235;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#35843;&#26597;&#32467;&#26524;&#19982;&#29992;&#25143;&#30340;&#39564;&#35777;&#25512;&#29305;&#32080;&#21512;&#22312;&#19968;&#36215;&#12290;
&lt;/p&gt;
&lt;p&gt;
Misinformation is a global concern and limiting its spread is critical for protecting democracy, public health, and consumers. We propose that consumers' own social media post-histories are an underutilized data source to study what leads them to share links to fake-news. In Study 1, we explore how textual cues extracted from post-histories distinguish fake-news sharers from random social media users and others in the misinformation ecosystem. Among other results, we find across two datasets that fake-news sharers use more words related to anger, religion and power. In Study 2, we show that adding textual cues from post-histories improves the accuracy of models to predict who is likely to share fake-news. In Study 3, we provide a preliminary test of two mitigation strategies deduced from Study 1 - activating religious values and reducing anger - and find that they reduce fake-news sharing and sharing more generally. In Study 4, we combine survey responses with users' verified Twitter p
&lt;/p&gt;</description></item><item><title>DeepKE&#26159;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30693;&#35782;&#25552;&#21462;&#24037;&#20855;&#21253;&#65292;&#25903;&#25345;&#22797;&#26434;&#30340;&#20302;&#36164;&#28304;&#12289;&#25991;&#26723;&#32423;&#21644;&#22810;&#27169;&#24577;&#22330;&#26223;&#65292;&#21487;&#29992;&#20110;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#26469;&#20174;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#25552;&#21462;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2201.03335</link><description>&lt;p&gt;
DeepKE: &#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30693;&#35782;&#25552;&#21462;&#24037;&#20855;&#21253;&#29992;&#20110;&#30693;&#35782;&#24211;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
DeepKE: A Deep Learning Based Knowledge Extraction Toolkit for Knowledge Base Population. (arXiv:2201.03335v6 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.03335
&lt;/p&gt;
&lt;p&gt;
DeepKE&#26159;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30693;&#35782;&#25552;&#21462;&#24037;&#20855;&#21253;&#65292;&#25903;&#25345;&#22797;&#26434;&#30340;&#20302;&#36164;&#28304;&#12289;&#25991;&#26723;&#32423;&#21644;&#22810;&#27169;&#24577;&#22330;&#26223;&#65292;&#21487;&#29992;&#20110;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#26469;&#20174;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#25552;&#21462;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24320;&#25918;&#28304;&#20195;&#30721;&#21644;&#21487;&#25193;&#23637;&#30340;&#30693;&#35782;&#25552;&#21462;&#24037;&#20855;&#21253;DeepKE&#65292;&#25903;&#25345;&#30693;&#35782;&#24211;&#26500;&#24314;&#20013;&#30340;&#22797;&#26434;&#20302;&#36164;&#28304;&#12289;&#25991;&#26723;&#32423;&#21644;&#22810;&#27169;&#24577;&#22330;&#26223;&#12290;DeepKE&#23454;&#29616;&#20102;&#21508;&#31181;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#65292;&#21253;&#25324;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#20851;&#31995;&#25552;&#21462;&#21644;&#23646;&#24615;&#25552;&#21462;&#12290;&#36890;&#36807;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;DeepKE&#20801;&#35768;&#24320;&#21457;&#20154;&#21592;&#21644;&#30740;&#31350;&#20154;&#21592;&#26681;&#25454;&#33258;&#24049;&#30340;&#38656;&#27714;&#23450;&#21046;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#65292;&#20174;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#25552;&#21462;&#20449;&#24687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;DeepKE&#19981;&#20165;&#20026;&#19981;&#21516;&#20219;&#21153;&#21644;&#22330;&#26223;&#25552;&#20379;&#21508;&#31181;&#21151;&#33021;&#27169;&#22359;&#21644;&#27169;&#22411;&#23454;&#29616;&#65292;&#36824;&#36890;&#36807;&#19968;&#33268;&#30340;&#26694;&#26550;&#32452;&#32455;&#25152;&#26377;&#32452;&#20214;&#65292;&#20197;&#20445;&#25345;&#36275;&#22815;&#30340;&#27169;&#22359;&#21270;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#22312;https://github.com/zjunlp/DeepKE&#21457;&#24067;&#20102;&#28304;&#20195;&#30721;&#65292;&#24182;&#25552;&#20379;&#20102;&#36866;&#29992;&#20110;&#21021;&#23398;&#32773;&#30340;Google Colab&#25945;&#31243;&#21644;&#20840;&#38754;&#30340;&#25991;&#26723;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22312;http URL&#19978;&#25552;&#20379;&#20102;&#19968;&#20010;&#22312;&#32447;&#31995;&#32479;&#65292;&#29992;&#20110;&#23454;&#26102;&#25552;&#21462;&#21508;&#31181;&#20219;&#21153;&#65292;&#24182;&#25552;&#20379;&#20102;&#28436;&#31034;&#35270;&#39057;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an open-source and extensible knowledge extraction toolkit DeepKE, supporting complicated low-resource, document-level and multimodal scenarios in the knowledge base population. DeepKE implements various information extraction tasks, including named entity recognition, relation extraction and attribute extraction. With a unified framework, DeepKE allows developers and researchers to customize datasets and models to extract information from unstructured data according to their requirements. Specifically, DeepKE not only provides various functional modules and model implementation for different tasks and scenarios but also organizes all components by consistent frameworks to maintain sufficient modularity and extensibility. We release the source code at GitHub in https://github.com/zjunlp/DeepKE with Google Colab tutorials and comprehensive documents for beginners. Besides, we present an online system in this http URL for real-time extraction of various tasks, and a demo video
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#22823;&#23398;&#31038;&#21306;&#20013;&#30340;Reddit&#25968;&#25454;&#65292;&#30740;&#31350;&#20102;COVID-19&#30123;&#24773;&#23545;&#20154;&#20204;&#24773;&#32490;&#21644;&#24515;&#29702;&#29366;&#24577;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;RoBERTa&#21644;GAT&#30340;&#24773;&#32490;&#20998;&#31867;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2112.04351</link><description>&lt;p&gt;
&#24773;&#32490;&#20998;&#26512;&#21644;&#26032;&#20896;&#30123;&#24773;&#23545;&#22823;&#23398;&#31038;&#21306;&#20013;Reddit&#25968;&#25454;&#30340;&#24433;&#21709;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Sentiment Analysis and Effect of COVID-19 Pandemic using College SubReddit Data. (arXiv:2112.04351v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.04351
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#22823;&#23398;&#31038;&#21306;&#20013;&#30340;Reddit&#25968;&#25454;&#65292;&#30740;&#31350;&#20102;COVID-19&#30123;&#24773;&#23545;&#20154;&#20204;&#24773;&#32490;&#21644;&#24515;&#29702;&#29366;&#24577;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;RoBERTa&#21644;GAT&#30340;&#24773;&#32490;&#20998;&#31867;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;COVID-19&#30123;&#24773;&#20197;&#21508;&#31181;&#26041;&#24335;&#24433;&#21709;&#20102;&#25105;&#20204;&#30340;&#31038;&#20250;&#21644;&#20154;&#31867;&#31119;&#31049;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#31038;&#20132;&#23186;&#20307;&#30340;&#30495;&#23454;&#25968;&#25454;&#65292;&#35843;&#26597;&#20102;&#30123;&#24773;&#22914;&#20309;&#19982;&#30123;&#24773;&#21069;&#26399;&#30456;&#27604;&#24433;&#21709;&#20102;&#20154;&#20204;&#30340;&#24773;&#32490;&#21644;&#24515;&#29702;&#29366;&#24577;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#25910;&#38598;&#20102;&#19982;&#20843;&#25152;&#22823;&#23398;&#30456;&#20851;&#30340;Reddit&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#65292;&#20854;&#20013;&#21253;&#25324;2019&#24180;&#65288;&#30123;&#24773;&#21069;&#65289;&#21644;2020&#24180;&#65288;&#30123;&#24773;&#26399;&#38388;&#65289;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;RoBERTa&#26041;&#27861;&#23398;&#20064;Reddit&#28040;&#24687;&#30340;&#25991;&#26412;&#23884;&#20837;&#65292;&#21516;&#26102;&#21033;&#29992;&#21457;&#24067;&#30340;&#28040;&#24687;&#20043;&#38388;&#30340;&#20851;&#31995;&#20449;&#24687;&#35757;&#32451;&#20102;&#19968;&#20010;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;GAT&#65289;&#36827;&#34892;&#24773;&#32490;&#20998;&#31867;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;RoBERTa&#21644;GAT&#30340;&#39044;&#27979;&#27010;&#29575;&#36827;&#34892;&#27169;&#22411;&#22534;&#21472;&#65292;&#24471;&#20986;&#24773;&#32490;&#26368;&#32456;&#20998;&#31867;&#32467;&#26524;&#12290;&#36890;&#36807;&#23545;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#39044;&#27979;&#30340;&#24773;&#32490;&#26631;&#31614;&#65292;&#25105;&#20204;&#20351;&#29992;&#24191;&#20041;&#32447;&#24615;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#20272;&#35745;&#20102;&#30123;&#24773;&#21644;&#24773;&#32490;&#20043;&#38388;&#30340;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: The COVID-19 pandemic has affected our society and human well-being in various ways. In this study, we investigate how the pandemic has influenced people's emotions and psychological states compared to a pre-pandemic period using real-world data from social media.  Method: We collected Reddit social media data from 2019 (pre-pandemic) and 2020 (pandemic) from the subreddits communities associated with eight universities. We applied the pre-trained Robustly Optimized BERT pre-training approach (RoBERTa) to learn text embedding from the Reddit messages, and leveraged the relational information among posted messages to train a graph attention network (GAT) for sentiment classification. Finally, we applied model stacking to combine the prediction probabilities from RoBERTa and GAT to yield the final classification on sentiment. With the model-predicted sentiment labels on the collected data, we used a generalized linear mixed-effects model to estimate the effects of pandemic an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KnowPrompt&#30340;&#30693;&#35782;&#24863;&#30693;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20851;&#31995;&#26631;&#31614;&#20013;&#30340;&#28508;&#22312;&#30693;&#35782;&#34701;&#20837;&#21040;&#25552;&#31034;&#26500;&#24314;&#20013;&#65292;&#24182;&#36890;&#36807;&#21327;&#21516;&#20248;&#21270;&#30340;&#26041;&#24335;&#65292;&#25552;&#39640;&#20102;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2104.07650</link><description>&lt;p&gt;
KnowPrompt&#65306;&#20855;&#26377;&#21327;&#21516;&#20248;&#21270;&#30340;&#30693;&#35782;&#24863;&#30693;&#25552;&#31034;&#35843;&#25972;&#22312;&#20851;&#31995;&#25277;&#21462;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
KnowPrompt: Knowledge-aware Prompt-tuning with Synergistic Optimization for Relation Extraction. (arXiv:2104.07650v7 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.07650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KnowPrompt&#30340;&#30693;&#35782;&#24863;&#30693;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20851;&#31995;&#26631;&#31614;&#20013;&#30340;&#28508;&#22312;&#30693;&#35782;&#34701;&#20837;&#21040;&#25552;&#31034;&#26500;&#24314;&#20013;&#65292;&#24182;&#36890;&#36807;&#21327;&#21516;&#20248;&#21270;&#30340;&#26041;&#24335;&#65292;&#25552;&#39640;&#20102;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#20110;&#29305;&#23450;&#30340;&#23569;&#26679;&#26412;&#20998;&#31867;&#20219;&#21153;&#65292;&#20351;&#29992;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#25552;&#31034;&#35843;&#25972;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#23558;&#25991;&#26412;&#29255;&#27573;&#65288;&#21363;&#27169;&#26495;&#65289;&#25554;&#20837;&#36755;&#20837;&#65292;&#24182;&#23558;&#20998;&#31867;&#20219;&#21153;&#36716;&#21270;&#20026;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20851;&#31995;&#25277;&#21462;&#65292;&#30830;&#23450;&#19968;&#20010;&#21512;&#36866;&#30340;&#25552;&#31034;&#27169;&#26495;&#38656;&#35201;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;&#33719;&#21462;&#21512;&#36866;&#30340;&#26631;&#31614;&#35789;&#26159;&#32321;&#29712;&#19988;&#32791;&#26102;&#30340;&#12290;&#27492;&#22806;&#65292;&#20851;&#31995;&#26631;&#31614;&#20043;&#38388;&#23384;&#22312;&#20016;&#23500;&#30340;&#35821;&#20041;&#21644;&#20808;&#39564;&#30693;&#35782;&#65292;&#19981;&#23481;&#24573;&#35270;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#30528;&#30524;&#20110;&#23558;&#20851;&#31995;&#26631;&#31614;&#20043;&#38388;&#30340;&#30693;&#35782;&#34701;&#20837;&#21040;&#20851;&#31995;&#25277;&#21462;&#30340;&#25552;&#31034;&#35843;&#25972;&#20013;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#21327;&#21516;&#20248;&#21270;&#30340;&#30693;&#35782;&#24863;&#30693;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#65288;KnowPrompt&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#21487;&#23398;&#20064;&#30340;&#34394;&#25311;&#31867;&#22411;&#35789;&#21644;&#31572;&#26696;&#35789;&#23558;&#20851;&#31995;&#26631;&#31614;&#20013;&#30340;&#28508;&#22312;&#30693;&#35782;&#34701;&#20837;&#21040;&#25552;&#31034;&#26500;&#24314;&#20013;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#32467;&#26500;&#21270;&#32422;&#26463;&#21327;&#21516;&#20248;&#21270;&#23427;&#20204;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, prompt-tuning has achieved promising results for specific few-shot classification tasks. The core idea of prompt-tuning is to insert text pieces (i.e., templates) into the input and transform a classification task into a masked language modeling problem. However, for relation extraction, determining an appropriate prompt template requires domain expertise, and it is cumbersome and time-consuming to obtain a suitable label word. Furthermore, there exists abundant semantic and prior knowledge among the relation labels that cannot be ignored. To this end, we focus on incorporating knowledge among relation labels into prompt-tuning for relation extraction and propose a Knowledge-aware Prompt-tuning approach with synergistic optimization (KnowPrompt). Specifically, we inject latent knowledge contained in relation labels into prompt construction with learnable virtual type words and answer words. Then, we synergistically optimize their representation with structured constraints. Ex
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#21516;&#20154;&#23567;&#35828;&#30340;&#22823;&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#25991;&#21270;&#20316;&#21697;&#30340;&#25104;&#21151;&#19982;&#26032;&#39062;&#24615;&#21576;&#29616;&#20986;U&#24418;&#26354;&#32447;&#30340;&#20851;&#31995;&#65292;&#32780;&#19981;&#26159;&#20808;&#22686;&#21152;&#21518;&#20943;&#24369;&#65292;&#35299;&#20915;&#20102;&#24179;&#34913;&#29702;&#35770;&#30340;&#35868;&#22242;&#12290;</title><link>http://arxiv.org/abs/1904.07741</link><description>&lt;p&gt;
&#21516;&#36136;&#24615;&#21560;&#24341;&#65292;&#20294;&#26032;&#39062;&#24615;&#20196;&#20154;&#30528;&#36855;&#8212;&#8212;&#20851;&#20110;&#22312;&#32447;&#21516;&#20154;&#23567;&#35828;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Sameness Entices, but Novelty Enchants in Fanfiction Online. (arXiv:1904.07741v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1904.07741
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#21516;&#20154;&#23567;&#35828;&#30340;&#22823;&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#25991;&#21270;&#20316;&#21697;&#30340;&#25104;&#21151;&#19982;&#26032;&#39062;&#24615;&#21576;&#29616;&#20986;U&#24418;&#26354;&#32447;&#30340;&#20851;&#31995;&#65292;&#32780;&#19981;&#26159;&#20808;&#22686;&#21152;&#21518;&#20943;&#24369;&#65292;&#35299;&#20915;&#20102;&#24179;&#34913;&#29702;&#35770;&#30340;&#35868;&#22242;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#21270;&#28436;&#21270;&#26159;&#30001;&#25105;&#20204;&#36873;&#25321;&#20160;&#20040;&#28040;&#36153;&#21644;&#19982;&#20182;&#20154;&#20998;&#20139;&#39537;&#21160;&#30340;&#12290;&#19968;&#20010;&#26222;&#36941;&#30340;&#20449;&#24565;&#26159;&#65292;&#25104;&#21151;&#30340;&#25991;&#21270;&#20316;&#21697;&#22312;&#26032;&#39062;&#24615;&#21644;&#24120;&#35268;&#24615;&#20043;&#38388;&#20445;&#25345;&#24179;&#34913;&#12290;&#36825;&#31181;&#24179;&#34913;&#29702;&#35770;&#35748;&#20026;&#65292;&#20154;&#20204;&#26356;&#21916;&#27426;&#37027;&#20123;&#29087;&#24713;&#32780;&#19981;&#20047;&#21619;&#30340;&#20316;&#21697;&#65307;&#26032;&#39062;&#65292;&#20294;&#21448;&#19981;&#36829;&#32972;&#20854;&#27969;&#27966;&#30340;&#39044;&#26399;&#12290;&#25105;&#20204;&#20351;&#29992;&#22823;&#37327;&#21516;&#20154;&#23567;&#35828;&#30340;&#25968;&#25454;&#38598;&#26469;&#27979;&#35797;&#36825;&#20010;&#24819;&#27861;&#12290;&#25105;&#20204;&#24212;&#29992;&#22810;&#20803;&#22238;&#24402;&#27169;&#22411;&#21644;&#24191;&#20041;&#21152;&#24615;&#27169;&#22411;&#26469;&#30740;&#31350;&#20316;&#21697;&#30340;&#35748;&#21487;&#31243;&#24230;&#22914;&#20309;&#38543;&#30528;&#20854;&#26032;&#39062;&#24615;&#30340;&#21464;&#21270;&#32780;&#21464;&#21270;&#65292;&#36890;&#36807;&#28508;&#22312;&#29380;&#21033;&#20811;&#38647;&#20998;&#37197;&#20027;&#39064;&#27169;&#22411;&#36827;&#34892;&#20272;&#35745;&#12290;&#25105;&#20204;&#21457;&#29616;&#19982;&#24179;&#34913;&#29702;&#35770;&#39044;&#27979;&#30456;&#21453;&#30340;&#27169;&#24335;&#8212;&#8212;&#24635;&#20307;&#19978;&#65292;&#20316;&#21697;&#30340;&#25104;&#21151;&#20960;&#20046;&#38543;&#30528;&#26032;&#39062;&#24615;&#30340;&#22686;&#21152;&#32780;&#19979;&#38477;&#65292;&#24182;&#21576;&#29616;&#20986;U&#24418;&#26354;&#32447;&#65292;&#32780;&#19981;&#26159;&#20498;U&#24418;&#26354;&#32447;&#12290;&#36825;&#20010;&#35868;&#22242;&#36890;&#36807;&#25581;&#31034;&#20986;&#20004;&#32929;&#30456;&#20114;&#31454;&#20105;&#30340;&#21147;&#37327;&#24471;&#21040;&#20102;&#35299;&#20915;&#65306;&#21516;&#36136;&#24615;&#21560;&#24341;&#20102;&#22823;&#20247;&#65292;&#32780;&#26032;&#39062;&#24615;&#25552;&#20379;&#20102;&#27426;&#20048;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cultural evolution is driven by how we choose what to consume and share with others. A common belief is that the cultural artifacts that succeed are ones that balance novelty and conventionality. This balance theory suggests that people prefer works that are familiar, but not so familiar as to be boring; novel, but not so novel as to violate the expectations of their genre. We test this idea using a large dataset of fanfiction. We apply a multiple regression model and a generalized additive model to examine how the recognition a work receives varies with its novelty, estimated through a Latent Dirichlet Allocation topic model, in the context of existing works. We find the opposite pattern of what the balance theory predicts$\unicode{x2014}$overall success decline almost monotonically with novelty and exhibits a U-shaped, instead of an inverse U-shaped, curve. This puzzle is resolved by teasing out two competing forces: sameness attracts the mass whereas novelty provides enjoyment. Take
&lt;/p&gt;</description></item></channel></rss>