<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25193;&#23637;&#19987;&#23478;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#26080;&#30417;&#30563;&#30340;&#39046;&#22495;&#21457;&#29616;&#26469;&#33258;&#21160;&#21270;&#35757;&#32451;&#24182;&#28040;&#38500;&#36890;&#20449;&#24320;&#38144;&#65292;&#36890;&#36807;&#23558;&#35821;&#26009;&#24211;&#32858;&#31867;&#25104;&#30456;&#20851;&#25991;&#26723;&#38598;&#26469;&#35757;&#32451;&#21333;&#29420;&#30340;&#19987;&#23478;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#23558;&#23427;&#20204;&#32452;&#21512;&#25104;&#19968;&#20010;&#31232;&#30095;&#30340;&#38598;&#21512;&#36827;&#34892;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2303.14177</link><description>&lt;p&gt;
&#29992;&#26080;&#30417;&#30563;&#30340;&#39046;&#22495;&#21457;&#29616;&#26041;&#27861;&#25193;&#23637;&#19987;&#23478;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Scaling Expert Language Models with Unsupervised Domain Discovery. (arXiv:2303.14177v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14177
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25193;&#23637;&#19987;&#23478;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#26080;&#30417;&#30563;&#30340;&#39046;&#22495;&#21457;&#29616;&#26469;&#33258;&#21160;&#21270;&#35757;&#32451;&#24182;&#28040;&#38500;&#36890;&#20449;&#24320;&#38144;&#65292;&#36890;&#36807;&#23558;&#35821;&#26009;&#24211;&#32858;&#31867;&#25104;&#30456;&#20851;&#25991;&#26723;&#38598;&#26469;&#35757;&#32451;&#21333;&#29420;&#30340;&#19987;&#23478;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#23558;&#23427;&#20204;&#32452;&#21512;&#25104;&#19968;&#20010;&#31232;&#30095;&#30340;&#38598;&#21512;&#36827;&#34892;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#36827;&#34892;&#23494;&#38598;&#35757;&#32451;&#65306;&#25152;&#26377;&#21442;&#25968;&#22343;&#23545;&#25152;&#26377;&#36755;&#20837;&#36827;&#34892;&#26356;&#26032;&#12290;&#36825;&#35201;&#27714;&#22312;&#25968;&#21315;&#20010;GPU&#20043;&#38388;&#21516;&#27493;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#24322;&#27493;&#22320;&#22312;&#20219;&#24847;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#22823;&#22411;&#31232;&#30095;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#19968;&#20010;&#35821;&#26009;&#24211;&#32858;&#31867;&#25104;&#30456;&#20851;&#25991;&#26723;&#38598;&#65292;&#23545;&#27599;&#20010;&#38598;&#32676;&#35757;&#32451;&#19968;&#20010;&#21333;&#29420;&#30340;&#19987;&#23478;&#35821;&#35328;&#27169;&#22411;&#65292;&#28982;&#21518;&#22312;&#25512;&#29702;&#26102;&#23558;&#23427;&#20204;&#32452;&#21512;&#25104;&#31232;&#30095;&#30340;&#38598;&#21512;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#33258;&#21160;&#21457;&#29616;&#27599;&#20010;&#19987;&#23478;&#30340;&#39046;&#22495;&#26469;&#25512;&#24191;&#20102;&#23604;&#23596;&#24179;&#34892;&#35757;&#32451;&#65292;&#24182;&#28040;&#38500;&#20102;&#29616;&#26377;&#31232;&#30095;&#35821;&#35328;&#27169;&#22411;&#20013;&#20960;&#20046;&#25152;&#26377;&#30340;&#36890;&#20449;&#24320;&#38144;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#22312;&#22810;&#20010;&#35821;&#26009;&#24211;&#21644;&#23569;&#37327;&#35757;&#32451;&#20219;&#21153;&#19978;&#20248;&#20110;&#23494;&#38598;&#22522;&#32447;&#65292;&#24182;&#19988;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#23558;&#19987;&#23478;&#29305;&#21270;&#21040;&#26377;&#24847;&#20041;&#30340;&#38598;&#32676;&#26159;&#21462;&#24471;&#36825;&#20123;&#22686;&#30410;&#30340;&#20851;&#38190;&#12290;&#24615;&#33021;&#36824;&#38543;&#30528;&#19987;&#23478;&#25968;&#37327;&#21644;&#35757;&#32451;&#25968;&#25454;&#30340;&#22823;&#23567;&#32780;&#25552;&#39640;&#65292;&#36825;&#34920;&#26126;&#36825;&#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#35775;&#38382;&#30340;&#32553;&#25918;&#19987;&#23478;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models are typically trained densely: all parameters are updated with respect to all inputs. This requires synchronization of billions of parameters across thousands of GPUs. We introduce a simple but effective method to asynchronously train large, sparse language models on arbitrary text corpora. Our method clusters a corpus into sets of related documents, trains a separate expert language model on each cluster, and combines them in a sparse ensemble for inference. This approach generalizes embarrassingly parallel training by automatically discovering the domains for each expert, and eliminates nearly all the communication overhead of existing sparse language models. Our technique outperforms dense baselines on multiple corpora and few-shot tasks, and our analysis shows that specializing experts to meaningful clusters is key to these gains. Performance also improves with the number of experts and size of training data, suggesting this is a highly efficient and accessibl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#22312;Twitter&#20013;&#20998;&#26512;&#20102;&#20843;&#20010;&#33521;&#35821;&#22269;&#23478;&#23545;&#36139;&#31351;&#19982;&#29359;&#32618;&#20043;&#38388;&#20851;&#32852;&#30340;&#31038;&#20250;&#20559;&#35265;&#27700;&#24179;&#65292;&#32467;&#26524;&#26174;&#31034;&#20986;&#20102;&#23545;&#26368;&#24369;&#21183;&#32676;&#20307;&#30340;&#38598;&#20307;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2303.14128</link><description>&lt;p&gt;
&#36139;&#31351;&#30340;&#32618;&#34892;
&lt;/p&gt;
&lt;p&gt;
The crime of being poor. (arXiv:2303.14128v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14128
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#22312;Twitter&#20013;&#20998;&#26512;&#20102;&#20843;&#20010;&#33521;&#35821;&#22269;&#23478;&#23545;&#36139;&#31351;&#19982;&#29359;&#32618;&#20043;&#38388;&#20851;&#32852;&#30340;&#31038;&#20250;&#20559;&#35265;&#27700;&#24179;&#65292;&#32467;&#26524;&#26174;&#31034;&#20986;&#20102;&#23545;&#26368;&#24369;&#21183;&#32676;&#20307;&#30340;&#38598;&#20307;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36139;&#31351;&#29359;&#32618;&#21270;&#19968;&#30452;&#34987;&#20844;&#35748;&#20026;&#26159;&#19968;&#31181;&#23545;&#26368;&#24369;&#21183;&#32676;&#20307;&#30340;&#38598;&#20307;&#20559;&#35265;&#12290;NGO&#21644;&#22269;&#38469;&#32452;&#32455;&#22768;&#31216;&#65292;&#31351;&#20154;&#22240;&#20854;&#22788;&#22659;&#32780;&#21463;&#21040;&#25351;&#36131;&#65292;&#19982;&#21009;&#20107;&#29359;&#32618;&#30340;&#35302;&#21457;&#22240;&#32032;&#30340;&#20851;&#32852;&#24615;&#27604;&#23500;&#20154;&#26356;&#39640;&#65292;&#29978;&#33267;&#21482;&#26159;&#22240;&#20026;&#36139;&#31351;&#32780;&#29359;&#32618;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#20013;&#27809;&#26377;&#25214;&#21040;&#36139;&#31351;&#19982;&#25972;&#20307;&#29359;&#32618;&#29575;&#20043;&#38388;&#30340;&#30456;&#20851;&#35777;&#25454;&#65292;&#26412;&#25991;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#22312;Twitter&#20013;&#37327;&#21270;&#20102;&#22312;&#20843;&#20010;&#33521;&#35821;&#22269;&#23478;&#38754;&#26495;&#20013;&#65292;&#29359;&#32618;&#21644;&#36139;&#31351;&#20043;&#38388;&#30340;&#31038;&#20250;&#20559;&#35265;&#27700;&#24179;&#65292;&#24182;&#27979;&#37327;&#20102;&#35813;&#35266;&#24565;&#30340;&#31243;&#24230;&#12290;&#26681;&#25454;&#25991;&#29486;&#30456;&#20851;&#30340;&#19981;&#21516;&#27700;&#24179;&#30340;&#19981;&#24179;&#31561;&#25110;&#22833;&#19994;&#65292;&#19981;&#36275;&#20197;&#35777;&#26126;&#22312;&#29359;&#32618;&#21644;&#36139;&#31351;&#20043;&#38388;&#30340;&#32852;&#24819;&#25152;&#21576;&#29616;&#20986;&#30340;&#21306;&#22495;&#24615;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
The criminalization of poverty has been widely denounced as a collective bias against the most vulnerable. NGOs and international organizations claim that the poor are blamed for their situation, are more often associated with criminal offenses than the wealthy strata of society and even incur criminal offenses simply as a result of being poor. While no evidence has been found in the literature that correlates poverty and overall criminality rates, this paper offers evidence of a collective belief that associates both concepts. This brief report measures the societal bias that correlates criminality with the poor, as compared to the rich, by using Natural Language Processing (NLP) techniques in Twitter. The paper quantifies the level of crime-poverty bias in a panel of eight different English-speaking countries. The regional differences in the association between crime and poverty cannot be justified based on different levels of inequality or unemployment, which the literature correlat
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#24635;&#32467;&#20102;&#20316;&#32773;&#20851;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#22312;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35299;&#37322;&#24615;&#21644;&#24615;&#33021;&#20013;&#30340;&#28508;&#21147;&#12290;&#36825;&#26159;&#22522;&#30784;&#19982;&#24212;&#29992;&#30740;&#31350;&#30340;&#37325;&#35201;&#38382;&#39064;&#65292;&#23588;&#20854;&#22312;&#21307;&#23398;&#39046;&#22495;&#65292;&#32780;&#27880;&#24847;&#21147;&#26426;&#21046;&#21487;&#20197;&#25104;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.14116</link><description>&lt;p&gt;
&#20174;&#22522;&#30784;&#19982;&#24212;&#29992;&#30740;&#31350;&#35270;&#35282;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#21644;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#65306;&#27880;&#24847;&#21147;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Improving Prediction Performance and Model Interpretability through Attention Mechanisms from Basic and Applied Research Perspectives. (arXiv:2303.14116v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14116
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#24635;&#32467;&#20102;&#20316;&#32773;&#20851;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#22312;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35299;&#37322;&#24615;&#21644;&#24615;&#33021;&#20013;&#30340;&#28508;&#21147;&#12290;&#36825;&#26159;&#22522;&#30784;&#19982;&#24212;&#29992;&#30740;&#31350;&#30340;&#37325;&#35201;&#38382;&#39064;&#65292;&#23588;&#20854;&#22312;&#21307;&#23398;&#39046;&#22495;&#65292;&#32780;&#27880;&#24847;&#21147;&#26426;&#21046;&#21487;&#20197;&#25104;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20851;&#27880;&#20110;&#25552;&#39640;&#27169;&#22411;&#39044;&#27979;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#24615;&#33021;&#65292;&#28085;&#30422;&#22522;&#30784;&#19982;&#24212;&#29992;&#30740;&#31350;&#30340;&#21508;&#20010;&#39046;&#22495;&#12290;&#34429;&#28982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#27604;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20855;&#26377;&#26356;&#39640;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20294;&#20855;&#20307;&#39044;&#27979;&#36807;&#31243;&#20173;&#38590;&#20197;&#35299;&#37322;&#21644;&#35828;&#26126;&#65292;&#36825;&#34987;&#31216;&#20026;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#40657;&#30418;&#21270;&#65292;&#24182;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#35768;&#22810;&#30740;&#31350;&#39046;&#22495;&#30340;&#19968;&#20010;&#29305;&#21035;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#21046;&#36896;&#19994;&#12289;&#21830;&#19994;&#12289;&#26426;&#22120;&#20154;&#21644;&#20854;&#20182;&#34892;&#19994;&#31561;&#26222;&#36941;&#20351;&#29992;&#35813;&#25216;&#26415;&#65292;&#20197;&#21450;&#21307;&#23398;&#39046;&#22495;&#65292;&#22312;&#36825;&#20123;&#39046;&#22495;&#20013;&#38169;&#35823;&#26159;&#19981;&#21487;&#23481;&#24525;&#30340;&#12290;&#26412;&#25991;&#22522;&#20110;&#20316;&#32773;&#35770;&#25991;&#30340;&#25688;&#35201;&#65292;&#35813;&#35770;&#25991;&#30340;&#26680;&#24515;&#30740;&#31350;&#20851;&#27880;&#20110;&#36817;&#24180;&#26469;&#22791;&#21463;&#20851;&#27880;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#22312;&#22522;&#30784;&#30740;&#31350;&#21644;&#24212;&#29992;&#30740;&#31350;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the dramatic advances in deep learning technology, machine learning research is focusing on improving the interpretability of model predictions as well as prediction performance in both basic and applied research. While deep learning models have much higher prediction performance than traditional machine learning models, the specific prediction process is still difficult to interpret and/or explain. This is known as the black-boxing of machine learning models and is recognized as a particularly important problem in a wide range of research fields, including manufacturing, commerce, robotics, and other industries where the use of such technology has become commonplace, as well as the medical field, where mistakes are not tolerated. This bulletin is based on the summary of the author's dissertation. The research summarized in the dissertation focuses on the attention mechanism, which has been the focus of much attention in recent years, and discusses its potential for both basic res
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#21307;&#23398;&#39046;&#22495;&#21033;&#29992;LLaMA&#27169;&#22411;&#24494;&#35843;&#30340;&#21307;&#30103;&#32842;&#22825;&#27169;&#22411;ChatDoctor&#12290;&#32463;&#36807;700&#22810;&#31181;&#30142;&#30149;&#21644;&#20854;&#30456;&#24212;&#30151;&#29366;&#12289;&#33647;&#21697;&#21644;&#21307;&#30103;&#26816;&#26597;&#30340;&#25910;&#38598;&#21644;&#22788;&#29702;&#65292;&#36825;&#31181;&#27169;&#22411;&#20855;&#26377;&#29702;&#35299;&#24739;&#32773;&#38656;&#27714;&#12289;&#25552;&#20379;&#24314;&#35758;&#21644;&#24110;&#21161;&#30340;&#28508;&#21147;&#12290;&#36825;&#20123;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#21040;&#21307;&#30103;&#20445;&#20581;&#20013;&#21487;&#20197;&#26497;&#22823;&#22320;&#25913;&#36827;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#21644;&#24739;&#32773;&#30340;&#27807;&#36890;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2303.14070</link><description>&lt;p&gt;
ChatDoctor&#65306;&#20351;&#29992;&#21307;&#23398;&#39046;&#22495;&#30693;&#35782;&#22312;LLaMA&#27169;&#22411;&#19978;&#24494;&#35843;&#30340;&#21307;&#30103;&#32842;&#22825;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ChatDoctor: A Medical Chat Model Fine-tuned on LLaMA Model using Medical Domain Knowledge. (arXiv:2303.14070v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14070
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#21307;&#23398;&#39046;&#22495;&#21033;&#29992;LLaMA&#27169;&#22411;&#24494;&#35843;&#30340;&#21307;&#30103;&#32842;&#22825;&#27169;&#22411;ChatDoctor&#12290;&#32463;&#36807;700&#22810;&#31181;&#30142;&#30149;&#21644;&#20854;&#30456;&#24212;&#30151;&#29366;&#12289;&#33647;&#21697;&#21644;&#21307;&#30103;&#26816;&#26597;&#30340;&#25910;&#38598;&#21644;&#22788;&#29702;&#65292;&#36825;&#31181;&#27169;&#22411;&#20855;&#26377;&#29702;&#35299;&#24739;&#32773;&#38656;&#27714;&#12289;&#25552;&#20379;&#24314;&#35758;&#21644;&#24110;&#21161;&#30340;&#28508;&#21147;&#12290;&#36825;&#20123;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#21040;&#21307;&#30103;&#20445;&#20581;&#20013;&#21487;&#20197;&#26497;&#22823;&#22320;&#25913;&#36827;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#21644;&#24739;&#32773;&#30340;&#27807;&#36890;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#19968;&#33324;&#39046;&#22495;&#20013;&#24212;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#20363;&#22914;ChatGPT&#65292;&#24050;&#32463;&#34920;&#29616;&#20986;&#20223;&#20315;&#26159;&#20154;&#31867;&#35762;&#35805;&#33324;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#35821;&#35328;&#27169;&#22411;&#24182;&#27809;&#26377;&#32463;&#36807;&#20010;&#21035;&#19988;&#20180;&#32454;&#20026;&#21307;&#23398;&#39046;&#22495;&#23398;&#20064;&#65292;&#23548;&#33268;&#35786;&#26029;&#20934;&#30830;&#24230;&#20302;&#19988;&#19981;&#33021;&#32473;&#20986;&#27491;&#30830;&#30340;&#21307;&#30103;&#35786;&#26029;&#12289;&#33647;&#21697;&#31561;&#24314;&#35758;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;700&#22810;&#31181;&#30142;&#30149;&#21450;&#20854;&#30456;&#24212;&#30151;&#29366;&#12289;&#25512;&#33616;&#33647;&#21697;&#21644;&#25152;&#38656;&#21307;&#30103;&#26816;&#26597;&#65292;&#28982;&#21518;&#29983;&#25104;&#20102;5K&#21517;&#21307;&#24739;&#30340;&#23545;&#35805;&#12290;&#36890;&#36807;&#24494;&#35843;&#21307;&#24739;&#23545;&#35805;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#20102;&#29702;&#35299;&#24739;&#32773;&#38656;&#27714;&#12289;&#25552;&#20379;&#26126;&#26234;&#24314;&#35758;&#24182;&#22312;&#21508;&#31181;&#21307;&#30103;&#30456;&#20851;&#39046;&#22495;&#25552;&#20379;&#23453;&#36149;&#24110;&#21161;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#23558;&#36825;&#20123;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#21040;&#21307;&#30103;&#20445;&#20581;&#20013;&#65292;&#21487;&#20197;&#24443;&#24213;&#25913;&#21464;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#21644;&#24739;&#32773;&#30340;&#27807;&#36890;&#26041;&#24335;&#65292;&#26368;&#32456;&#25913;&#21892;&#25972;&#20307;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent large language models (LLMs) in the general domain, such as ChatGPT, have shown remarkable success in following instructions and producing human-like responses. However, such language models have not been learned individually and carefully for the medical domain, resulting in poor diagnostic accuracy and inability to give correct recommendations for medical diagnosis, medications, etc. To address this issue, we collected more than 700 diseases and their corresponding symptoms, recommended medications, and required medical tests, and then generated 5K doctor-patient conversations. By fine-tuning models of doctor-patient conversations, these models emerge with great potential to understand patients' needs, provide informed advice, and offer valuable assistance in a variety of medical-related fields. The integration of these advanced language models into healthcare can revolutionize the way healthcare professionals and patients communicate, ultimately improving the overall quality 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#36164;&#28304;&#25277;&#35937;&#25991;&#25688;&#30340;&#24635;&#32467;&#20559;&#22909;&#20998;&#35299;&#30340;&#26041;&#27861;&#65292;&#20197;&#22312;&#21482;&#26377;&#23569;&#37327;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#29983;&#25104;&#22120;&#12290;&#20854;&#20013;&#65292;&#20197;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20026;&#22522;&#30784;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#23569;&#37327;&#26679;&#26412;&#30340;&#23398;&#20064;&#36807;&#31243;&#20174;&#28304;&#35821;&#26009;&#24211;&#36716;&#31227;&#21040;&#30446;&#26631;&#35821;&#26009;&#24211;&#12290;</title><link>http://arxiv.org/abs/2303.14011</link><description>&lt;p&gt;
SPEC: &#20302;&#36164;&#28304;&#25277;&#35937;&#25991;&#25688;&#30340;&#24635;&#32467;&#20559;&#22909;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
SPEC: Summary Preference Decomposition for Low-Resource Abstractive Summarization. (arXiv:2303.14011v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#36164;&#28304;&#25277;&#35937;&#25991;&#25688;&#30340;&#24635;&#32467;&#20559;&#22909;&#20998;&#35299;&#30340;&#26041;&#27861;&#65292;&#20197;&#22312;&#21482;&#26377;&#23569;&#37327;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#29983;&#25104;&#22120;&#12290;&#20854;&#20013;&#65292;&#20197;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20026;&#22522;&#30784;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#23569;&#37327;&#26679;&#26412;&#30340;&#23398;&#20064;&#36807;&#31243;&#20174;&#28304;&#35821;&#26009;&#24211;&#36716;&#31227;&#21040;&#30446;&#26631;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#25277;&#35937;&#25688;&#35201;&#24050;&#32463;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290; &#28982;&#32780;&#65292;&#27880;&#37322;&#25968;&#25454;&#30340;&#30456;&#24403;&#39640;&#30340;&#25104;&#26412;&#20419;&#20351;&#25105;&#20204;&#38656;&#35201;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#23398;&#20064;&#31574;&#30053;&#12290; &#26412;&#25991;&#30740;&#31350;&#20102;&#21482;&#26377;&#23569;&#37327;&#31034;&#20363;&#24773;&#20917;&#19979;&#23398;&#20064;&#25688;&#35201;&#29983;&#25104;&#22120;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#25913;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural abstractive summarization has been widely studied and achieved great success with large-scale corpora. However, the considerable cost of annotating data motivates the need for learning strategies under low-resource settings. In this paper, we investigate the problems of learning summarizers with only few examples and propose corresponding methods for improvements. First, typical transfer learning methods are prone to be affected by data properties and learning objectives in the pretext tasks. Therefore, based on pretrained language models, we further present a meta learning framework to transfer few-shot learning processes from source corpora to the target corpus. Second, previous methods learn from training examples without decomposing the content and preference. The generated summaries could therefore be constrained by the preference bias in the training set, especially under low-resource settings. As such, we propose decomposing the contents and preferences during learning th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#26816;&#27979;&#26041;&#27861;&#65292;&#23545;&#20154;&#31867;&#21644;&#26426;&#22120;&#37322;&#20041;&#20869;&#23481;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#21457;&#29616;&#20154;&#31867;&#37322;&#20041;&#38590;&#24230;&#12289;&#22810;&#26679;&#24615;&#21644;&#30456;&#20284;&#24615;&#36229;&#36807;&#26426;&#22120;&#37322;&#20041;&#65292;&#26426;&#22120;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#19981;&#36275;&#12290;Transformer &#26159;&#26368;&#26377;&#25928;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#32780; SVM-based &#26041;&#27861;&#19981;&#21450; BERT &#21644; RoBERTa &#21464;&#20307;&#12290;</title><link>http://arxiv.org/abs/2303.13989</link><description>&lt;p&gt;
&#20154;&#31867;&#19982;&#26426;&#22120;&#20869;&#23481;&#30340;&#37322;&#20041;&#26816;&#27979;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Paraphrase Detection: Human vs. Machine Content. (arXiv:2303.13989v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13989
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#26816;&#27979;&#26041;&#27861;&#65292;&#23545;&#20154;&#31867;&#21644;&#26426;&#22120;&#37322;&#20041;&#20869;&#23481;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#21457;&#29616;&#20154;&#31867;&#37322;&#20041;&#38590;&#24230;&#12289;&#22810;&#26679;&#24615;&#21644;&#30456;&#20284;&#24615;&#36229;&#36807;&#26426;&#22120;&#37322;&#20041;&#65292;&#26426;&#22120;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#19981;&#36275;&#12290;Transformer &#26159;&#26368;&#26377;&#25928;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#32780; SVM-based &#26041;&#27861;&#19981;&#21450; BERT &#21644; RoBERTa &#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914; GPT-4 &#21644; ChatGPT&#65289;&#26085;&#30410;&#37325;&#35201;&#65292;&#20294;&#20063;&#24341;&#36215;&#20102;&#23398;&#26415;&#35802;&#20449;&#38382;&#39064;&#65292;&#22240;&#20026;&#23384;&#22312;&#26426;&#22120;&#29983;&#25104;&#30340;&#20869;&#23481;&#21644;&#37322;&#20041;&#12290;&#34429;&#28982;&#26377;&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#31867;&#21644;&#26426;&#22120;&#37322;&#20041;&#20869;&#23481;&#30340;&#26816;&#27979;&#65292;&#20294;&#36825;&#20123;&#31867;&#22411;&#20869;&#23481;&#20043;&#38388;&#30340;&#27604;&#36739;&#20173;&#26410;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#12290;&#26412;&#25991;&#23545;&#24120;&#29992;&#30340;&#21508;&#31181;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#24182;&#35780;&#20272;&#20102;&#21508;&#31181;&#26816;&#27979;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#30456;&#23545;&#20110;&#26426;&#22120;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#32570;&#20047;&#65292;&#29616;&#26377;&#30340;&#35780;&#20272;&#26041;&#27861;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#21508;&#26377;&#20248;&#21155;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21457;&#29616;&#26159;&#65292;&#20154;&#31867;&#37322;&#20041;&#36229;&#36807;&#26426;&#22120;&#37322;&#20041;&#30340;&#38590;&#24230;&#12289;&#22810;&#26679;&#24615;&#21644;&#30456;&#20284;&#24615;&#65292;&#26263;&#31034;&#33258;&#21160;&#29983;&#25104;&#30340;&#25991;&#26412;&#36824;&#27809;&#26377;&#36798;&#21040;&#20154;&#31867;&#27700;&#24179;&#12290;Transformer &#26159;&#26368;&#26377;&#25928;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;BERT &#21644; RoBERTa &#21464;&#20307;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#19978;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#65292;&#32780;&#22522;&#20110; SVM &#30340;&#26041;&#27861;&#21017;&#33853;&#21518;&#20110;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing prominence of large language models, such as GPT-4 and ChatGPT, has led to increased concerns over academic integrity due to the potential for machine-generated content and paraphrasing. Although studies have explored the detection of human- and machine-paraphrased content, the comparison between these types of content remains underexplored. In this paper, we conduct a comprehensive analysis of various datasets commonly employed for paraphrase detection tasks and evaluate an array of detection methods. Our findings highlight the strengths and limitations of different detection methods in terms of performance on individual datasets, revealing a lack of suitable machine-generated datasets that can be aligned with human expectations. Our main finding is that human-authored paraphrases exceed machine-generated ones in terms of difficulty, diversity, and similarity implying that automatically generated texts are not yet on par with human-level performance. Transformers emerged a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39046;&#22495;&#8212;&#8212;&#26426;&#22120;&#24515;&#29702;&#23398;&#65292;&#21033;&#29992;&#24515;&#29702;&#23398;&#30340;&#26041;&#27861;&#32771;&#23519;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#35813;&#25991;&#35268;&#33539;&#20102;&#26426;&#22120;&#24515;&#29702;&#23398;&#30740;&#31350;&#30340;&#26041;&#27861;&#35770;&#26631;&#20934;&#65292;&#24182;&#23545;&#24515;&#29702;&#23454;&#39564;&#20013;&#25552;&#31034;&#35774;&#35745;&#25919;&#31574;&#36827;&#34892;&#20102;&#25506;&#35752;&#21644;&#21046;&#23450;&#12290;</title><link>http://arxiv.org/abs/2303.13988</link><description>&lt;p&gt;
&#26426;&#22120;&#24515;&#29702;&#23398;&#65306;&#21033;&#29992;&#24515;&#29702;&#23398;&#26041;&#27861;&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#20852;&#33021;&#21147;&#21644;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Machine Psychology: Investigating Emergent Capabilities and Behavior in Large Language Models Using Psychological Methods. (arXiv:2303.13988v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13988
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39046;&#22495;&#8212;&#8212;&#26426;&#22120;&#24515;&#29702;&#23398;&#65292;&#21033;&#29992;&#24515;&#29702;&#23398;&#30340;&#26041;&#27861;&#32771;&#23519;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#35813;&#25991;&#35268;&#33539;&#20102;&#26426;&#22120;&#24515;&#29702;&#23398;&#30740;&#31350;&#30340;&#26041;&#27861;&#35770;&#26631;&#20934;&#65292;&#24182;&#23545;&#24515;&#29702;&#23454;&#39564;&#20013;&#25552;&#31034;&#35774;&#35745;&#25919;&#31574;&#36827;&#34892;&#20102;&#25506;&#35752;&#21644;&#21046;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26159;&#23558;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#19982;&#20154;&#31867;&#20132;&#27969;&#21644;&#26085;&#24120;&#29983;&#27963;&#32039;&#23494;&#32467;&#21512;&#30340;&#20808;&#38155;&#12290;&#30001;&#20110;&#24555;&#36895;&#25216;&#26415;&#36827;&#27493;&#21644;&#20854;&#26497;&#39640;&#30340;&#36890;&#29992;&#24615;&#65292;&#29616;&#20170;LLM&#24050;&#32463;&#25317;&#26377;&#25968;&#30334;&#19975;&#29992;&#25143;&#65292;&#24182;&#27491;&#22788;&#20110;&#25104;&#20026;&#20027;&#35201;&#20449;&#24687;&#26816;&#32034;&#12289;&#20869;&#23481;&#29983;&#25104;&#12289;&#38382;&#39064;&#35299;&#20915;&#31561;&#25216;&#26415;&#30340;&#21069;&#27839;&#12290;&#22240;&#27492;&#65292;&#23545;&#20854;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#21644;&#23457;&#26597;&#26174;&#24471;&#23588;&#20026;&#37325;&#35201;&#12290;&#30001;&#20110;&#24403;&#21069;LLM&#20013;&#20986;&#29616;&#24840;&#21152;&#22797;&#26434;&#21644;&#26032;&#39062;&#30340;&#34892;&#20026;&#27169;&#24335;&#65292;&#21487;&#23558;&#20854;&#35270;&#20026;&#21442;&#19982;&#20154;&#31867;&#24515;&#29702;&#23454;&#39564;&#30340;&#23545;&#35937;&#65292;&#20197;&#20415;&#26356;&#20026;&#20840;&#38754;&#22320;&#35780;&#20272;&#20854;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;"&#26426;&#22120;&#24515;&#29702;&#23398;"&#30340;&#26032;&#20852;&#30740;&#31350;&#39046;&#22495;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#21508;&#31867;&#24515;&#29702;&#23398;&#20998;&#25903;&#22914;&#20309;&#20026;LLM&#30340;&#34892;&#20026;&#27979;&#35797;&#25552;&#20379;&#26377;&#29992;&#21442;&#32771;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#35268;&#33539;&#20102;&#26426;&#22120;&#24515;&#29702;&#23398;&#30740;&#31350;&#30340;&#26041;&#27861;&#35770;&#26631;&#20934;&#65292;&#29305;&#21035;&#26159;&#19987;&#27880;&#20110;&#25552;&#31034;&#35774;&#35745;&#25919;&#31574;&#30340;&#21046;&#23450;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#25551;&#36848;&#20102;&#34892;&#20026;&#27979;&#35797;&#32467;&#26524;&#22914;&#20309;&#20026;&#26410;&#26469;&#30340;LLM&#21457;&#23637;&#25552;&#20379;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are currently at the forefront of intertwining AI systems with human communication and everyday life. Due to rapid technological advances and their extreme versatility, LLMs nowadays have millions of users and are at the cusp of being the main go-to technology for information retrieval, content generation, problem-solving, etc. Therefore, it is of great importance to thoroughly assess and scrutinize their capabilities. Due to increasingly complex and novel behavioral patterns in current LLMs, this can be done by treating them as participants in psychology experiments that were originally designed to test humans. For this purpose, the paper introduces a new field of research called "machine psychology". The paper outlines how different subfields of psychology can inform behavioral tests for LLMs. It defines methodological standards for machine psychology research, especially by focusing on policies for prompt designs. Additionally, it describes how behaviora
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#20250;&#35758;&#29702;&#35299;&#21644;&#29983;&#25104;&#22522;&#20934;(MUG)&#65292;&#20197;&#35780;&#20272;&#19968;&#31995;&#21015;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#20026;&#21475;&#35821;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#30340;&#21457;&#23637;&#25552;&#20379;&#25903;&#25345;</title><link>http://arxiv.org/abs/2303.13939</link><description>&lt;p&gt;
MUG: &#19968;&#39033;&#36890;&#29992;&#30340;&#20250;&#35758;&#29702;&#35299;&#19982;&#29983;&#25104;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
MUG: A General Meeting Understanding and Generation Benchmark. (arXiv:2303.13939v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13939
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#20250;&#35758;&#29702;&#35299;&#21644;&#29983;&#25104;&#22522;&#20934;(MUG)&#65292;&#20197;&#35780;&#20272;&#19968;&#31995;&#21015;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#20026;&#21475;&#35821;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#30340;&#21457;&#23637;&#25552;&#20379;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21548;&#21462;&#35270;&#39057;&#20250;&#35758;&#21644;&#22312;&#32447;&#35838;&#31243;&#30340;&#38271;&#26102;&#38388;&#38899;&#39057;&#35760;&#24405;&#20197;&#33719;&#21462;&#20449;&#24687;&#26497;&#20026;&#20302;&#25928;&#12290;&#21363;&#20351;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#23558;&#35760;&#24405;&#36716;&#24405;&#20026;&#38271;&#24418;&#24335;&#30340;&#21475;&#35821;&#35821;&#35328;&#25991;&#26723;&#65292;&#20165;&#38405;&#35835;&#35821;&#38899;&#35782;&#21035;&#36716;&#24405;&#20063;&#21482;&#33021;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#21152;&#24555;&#23547;&#25214;&#20449;&#24687;&#30340;&#36895;&#24230;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#20851;&#38190;&#35789;&#25552;&#21462;&#12289;&#20027;&#39064;&#20998;&#21106;&#21644;&#25688;&#35201;&#31561;&#19968;&#31995;&#21015;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#26174;&#33879;&#25552;&#39640;&#29992;&#25143;&#33719;&#24471;&#37325;&#35201;&#20449;&#24687;&#30340;&#25928;&#29575;&#12290;&#20250;&#35758;&#22330;&#26223;&#26159;&#24212;&#29992;&#36825;&#20123;&#21475;&#35821;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#26368;&#26377;&#20215;&#20540;&#30340;&#22330;&#26223;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#22823;&#35268;&#27169;&#20844;&#24320;&#30340;&#20250;&#35758;&#25968;&#25454;&#38598;&#23545;&#36825;&#20123;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#36827;&#23637;&#20135;&#29983;&#20102;&#20005;&#37325;&#38459;&#30861;&#12290;&#20026;&#20102;&#20419;&#36827;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#36890;&#29992;&#20250;&#35758;&#29702;&#35299;&#21644;&#29983;&#25104;&#22522;&#20934;(MUG)&#65292;&#20197;&#35780;&#20272;&#19968;&#31995;&#21015;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#20027;&#39064;&#20998;&#21106;&#12289;&#35805;&#39064;&#32423;&#21035;&#21644;&#20250;&#35805;&#32423;&#21035;&#30340;&#25688;&#35201;&#25552;&#21462;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Listening to long video/audio recordings from video conferencing and online courses for acquiring information is extremely inefficient. Even after ASR systems transcribe recordings into long-form spoken language documents, reading ASR transcripts only partly speeds up seeking information. It has been observed that a range of NLP applications, such as keyphrase extraction, topic segmentation, and summarization, significantly improve users' efficiency in grasping important information. The meeting scenario is among the most valuable scenarios for deploying these spoken language processing (SLP) capabilities. However, the lack of large-scale public meeting datasets annotated for these SLP tasks severely hinders their advancement. To prompt SLP advancement, we establish a large-scale general Meeting Understanding and Generation Benchmark (MUG) to benchmark the performance of a wide range of SLP tasks, including topic segmentation, topic-level and session-level extractive summarization and 
&lt;/p&gt;</description></item><item><title>ICASSP 2023&#24180;&#23558;&#20030;&#34892;&#24635;&#20250;&#29702;&#35299;&#19982;&#29983;&#25104;&#25361;&#25112;&#36187;&#65292;&#24076;&#26395;&#36890;&#36807;&#35813;&#25361;&#25112;&#36187;&#20419;&#36827;SLP&#30740;&#31350;&#30340;&#21457;&#23637;&#65292;&#25361;&#25112;&#36187;&#21253;&#25324;&#20116;&#20010;&#36319;&#36394;&#39033;&#30446;&#65292;&#21457;&#36215;&#20102;&#22823;&#35268;&#27169;&#20250;&#35758;&#25968;&#25454;&#38598;AliMeeting4MUG Corpus&#12290;</title><link>http://arxiv.org/abs/2303.13932</link><description>&lt;p&gt;
ICASSP 2023&#24180;&#24635;&#20250;&#29702;&#35299;&#19982;&#29983;&#25104;&#25361;&#25112;&#36187;&#65288;MUG&#65289;&#27010;&#36848; (arXiv&#65306;2303.13932v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
Overview of the ICASSP 2023 General Meeting Understanding and Generation Challenge (MUG). (arXiv:2303.13932v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13932
&lt;/p&gt;
&lt;p&gt;
ICASSP 2023&#24180;&#23558;&#20030;&#34892;&#24635;&#20250;&#29702;&#35299;&#19982;&#29983;&#25104;&#25361;&#25112;&#36187;&#65292;&#24076;&#26395;&#36890;&#36807;&#35813;&#25361;&#25112;&#36187;&#20419;&#36827;SLP&#30740;&#31350;&#30340;&#21457;&#23637;&#65292;&#25361;&#25112;&#36187;&#21253;&#25324;&#20116;&#20010;&#36319;&#36394;&#39033;&#30446;&#65292;&#21457;&#36215;&#20102;&#22823;&#35268;&#27169;&#20250;&#35758;&#25968;&#25454;&#38598;AliMeeting4MUG Corpus&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ICASSP2023&#24180;&#24635;&#20250;&#29702;&#35299;&#19982;&#29983;&#25104;&#25361;&#25112;&#36187;&#65288;MUG&#65289;&#26088;&#22312;&#20419;&#36827;&#38024;&#23545;&#20250;&#35758;&#35760;&#24405;&#30340;&#24191;&#27867;&#21475;&#35821;&#35821;&#35328;&#22788;&#29702;&#65288;SLP&#65289;&#30740;&#31350;&#65292;&#22240;&#20026;SLP&#24212;&#29992;&#23545;&#20110;&#25552;&#39640;&#29992;&#25143;&#22312;&#20250;&#35758;&#20013;&#25235;&#20303;&#37325;&#35201;&#20449;&#24687;&#30340;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;MUG&#21253;&#25324;&#20116;&#20010;&#36319;&#36394;&#39033;&#30446;&#65292;&#21253;&#25324;&#20027;&#39064;&#20998;&#21106;&#12289;&#20027;&#39064;&#32423;&#21644;&#20250;&#35805;&#32423;&#30340;&#25552;&#21462;&#24615;&#25688;&#35201;&#12289;&#20027;&#39064;&#26631;&#39064;&#29983;&#25104;&#12289;&#20851;&#38190;&#30701;&#35821;&#25552;&#21462;&#21644;&#21160;&#20316;&#39033;&#26816;&#27979;&#12290;&#20026;&#20102;&#26041;&#20415;MUG&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#24182;&#21457;&#24067;&#20102;&#22823;&#35268;&#27169;&#20250;&#35758;&#25968;&#25454;&#38598;AliMeeting4MUG Corpus&#12290;
&lt;/p&gt;
&lt;p&gt;
ICASSP2023 General Meeting Understanding and Generation Challenge (MUG) focuses on prompting a wide range of spoken language processing (SLP) research on meeting transcripts, as SLP applications are critical to improve users' efficiency in grasping important information in meetings. MUG includes five tracks, including topic segmentation, topic-level and session-level extractive summarization, topic title generation, keyphrase extraction, and action item detection. To facilitate MUG, we construct and release a large-scale meeting dataset, the AliMeeting4MUG Corpus.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;$k$NN&#25552;&#31034;&#65292;&#19968;&#31181;&#19981;&#38656;&#35201;&#26657;&#20934;&#23601;&#21487;&#20197;&#25512;&#29702;&#26368;&#36817;&#30456;&#37051;&#30340;&#31639;&#27861;&#65292;&#29992;&#26469;&#35299;&#20915;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#38480;&#21046;&#21644;&#20559;&#35265;&#38382;&#39064;&#65292;&#26174;&#30528;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.13824</link><description>&lt;p&gt;
$k$NN&#25552;&#31034;&#65306;&#26080;&#38656;&#26657;&#20934;&#30340;&#26368;&#36817;&#30456;&#37051;&#25512;&#29702;&#65292;&#36229;&#36234;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
$k$NN Prompting: Beyond-Context Learning with Calibration-Free Nearest Neighbor Inference. (arXiv:2303.13824v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13824
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;$k$NN&#25552;&#31034;&#65292;&#19968;&#31181;&#19981;&#38656;&#35201;&#26657;&#20934;&#23601;&#21487;&#20197;&#25512;&#29702;&#26368;&#36817;&#30456;&#37051;&#30340;&#31639;&#27861;&#65292;&#29992;&#26469;&#35299;&#20915;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#38480;&#21046;&#21644;&#20559;&#35265;&#38382;&#39064;&#65292;&#26174;&#30528;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#65292;&#23558;&#30446;&#26631;&#20219;&#21153;&#21046;&#23450;&#20026;&#22312;&#19978;&#19979;&#25991;&#28436;&#31034;&#30340;&#26465;&#20214;&#19979;&#23436;&#25104;&#25552;&#31034;&#23436;&#25104;&#65292;&#24050;&#25104;&#20026;LLM&#30340;&#20027;&#35201;&#29992;&#36884;&#12290;&#26412;&#25991;&#39318;&#20808;&#25259;&#38706;&#20102;&#36825;&#31181;&#20856;&#22411;&#29992;&#27861;&#30340;&#23454;&#38469;&#38382;&#39064;&#65292;&#30001;&#20110;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#38480;&#21046;&#65292;&#23427;&#26080;&#27861;&#38543;&#30528;&#35757;&#32451;&#25968;&#25454;&#25193;&#23637;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;ICL&#36824;&#21463;&#21040;&#21508;&#31181;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#24182;&#38656;&#35201;&#31934;&#32454;&#30340;&#26657;&#20934;&#22788;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;$k$NN&#25552;&#31034;&#65292;&#23427;&#39318;&#20808;&#20351;&#29992;&#35757;&#32451;&#25968;&#25454;&#26597;&#35810;LLM&#30340;&#20998;&#24067;&#24335;&#34920;&#31034;&#65292;&#28982;&#21518;&#36890;&#36807;&#31616;&#21333;&#22320;&#21442;&#32771;&#26368;&#36817;&#37051;&#26469;&#39044;&#27979;&#27979;&#35797;&#23454;&#20363;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#26469;&#35777;&#26126;&#20854;&#20248;&#36234;&#24615;&#65306;1&#65289;&#26080;&#38656;&#26657;&#20934;&#65306;$k$NN&#25552;&#31034;&#19981;&#30452;&#25509;&#23558;LLM&#36755;&#20986;&#20998;&#24067;&#19982;&#29305;&#23450;&#20219;&#21153;&#26631;&#31614;&#31354;&#38388;&#23545;&#20934;&#65292;&#32780;&#26159;&#21033;&#29992;&#36825;&#31181;&#20998;&#24067;&#23558;&#27979;&#35797;&#21644;&#35757;&#32451;&#23454;&#20363;&#23545;&#20934;&#12290;&#23427;&#26174;&#30528;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-Context Learning (ICL), which formulates target tasks as prompt completion conditioned on in-context demonstrations, has become the prevailing utilization of LLMs. In this paper, we first disclose an actual predicament for this typical usage that it can not scale up with training data due to context length restriction. Besides, existing works have shown that ICL also suffers from various biases and requires delicate calibration treatment. To address both challenges, we advocate a simple and effective solution, $k$NN Prompting, which first queries LLM with training data for distributed representations, then predicts test instances by simply referring to nearest neighbors. We conduct comprehensive experiments to demonstrate its two-fold superiority: 1) Calibration-Free: $k$NN Prompting does not directly align LLM output distribution with task-specific label space, instead leverages such distribution to align test and training instances. It significantly outperforms state-of-the-art ca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26041;&#27861;Error Analysis Prompting&#21487;&#25913;&#21892;LLMs&#22312;&#26426;&#22120;&#32763;&#35793;&#36136;&#37327;&#35780;&#20272;&#19978;&#30340;&#24615;&#33021;&#65292;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#30340;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2303.13809</link><description>&lt;p&gt;
&#38169;&#35823;&#20998;&#26512;&#25552;&#31034;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32763;&#35793;&#35780;&#20272;&#26041;&#38754;&#23454;&#29616;&#20102;&#20154;&#31867;&#27700;&#24179;&#65306;&#20197;ChatGPT&#20026;&#20363;&#36827;&#34892;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Error Analysis Prompting Enables Human-Like Translation Evaluation in Large Language Models: A Case Study on ChatGPT. (arXiv:2303.13809v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26041;&#27861;Error Analysis Prompting&#21487;&#25913;&#21892;LLMs&#22312;&#26426;&#22120;&#32763;&#35793;&#36136;&#37327;&#35780;&#20272;&#19978;&#30340;&#24615;&#33021;&#65292;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#20363;&#22914;ChatGPT&#65292;&#22312;&#26426;&#22120;&#32763;&#35793;&#12289;&#38382;&#31572;&#12289;&#25991;&#26412;&#25688;&#35201;&#21644;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#31561;&#22810;&#20010;NLP&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21033;&#29992;ChatGPT&#35780;&#20272;&#26426;&#22120;&#32763;&#35793;&#36136;&#37327;&#22312;&#31995;&#32479;&#27700;&#24179;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#27573;&#33853;&#27700;&#24179;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;LLM&#22312;&#26426;&#22120;&#32763;&#35793;&#36136;&#37327;&#35780;&#20272;&#19978;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20851;&#20110;&#20960;&#31181;&#25552;&#31034;&#26041;&#27861;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#23558;Chain-of-Thoughts&#21644;Error Analysis&#32467;&#21512;&#36215;&#26469;&#65292;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26041;&#27861;Error Analysis Prompting&#65292;&#20687;ChatGPT&#36825;&#26679;&#30340;LLM&#21487;&#20197;&#22312;&#31995;&#32479;&#21644;&#27573;&#33853;&#32423;&#21035;&#19978;&#29983;&#25104;&#20154;&#31867;&#33324;&#30340;&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;ChatGPT&#20316;&#20026;&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;&#22120;&#23384;&#22312;&#19968;&#20123;&#23616;&#38480;&#24615;&#65292;&#20363;&#22914;&#22312;&#25552;&#20379;&#21333;&#20010;&#26597;&#35810;&#20013;&#30340;&#22810;&#20010;&#35793;&#25991;&#26102;&#23384;&#22312;&#19981;&#31283;&#23450;&#30340;&#35780;&#20998;&#21644;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative large language models (LLMs), e.g., ChatGPT, have demonstrated remarkable proficiency across several NLP tasks such as machine translation, question answering, text summarization, and natural language understanding. Recent research has shown that utilizing ChatGPT for assessing the quality of machine translation (MT) achieves state-of-the-art performance at the system level but performs poorly at the segment level. To further improve the performance of LLMs on MT quality assessment, we conducted an investigation into several prompting methods. Our results indicate that by combining Chain-of-Thoughts and Error Analysis, a new prompting method called \textbf{\texttt{Error Analysis Prompting}}, LLMs like ChatGPT can \textit{generate human-like MT evaluations at both the system and segment level}. Additionally, we discovered some limitations of ChatGPT as an MT evaluator, such as unstable scoring and biases when provided with multiple translations in a single query. Our findings
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SCot&#30340;&#33258;&#30417;&#30563;&#21327;&#21516;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;BERT&#27169;&#22411;&#21644;&#20266;&#26631;&#31614;&#65292;&#23454;&#29616;&#24320;&#25918;&#22495;&#27133;&#22635;&#20805;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#25163;&#21160;&#26631;&#27880;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.13801</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#30417;&#30563;&#21327;&#21516;&#35757;&#32451;&#23454;&#29616;&#24320;&#25918;&#22495;&#27133;&#22635;&#20805;
&lt;/p&gt;
&lt;p&gt;
Toward Open-domain Slot Filling via Self-supervised Co-training. (arXiv:2303.13801v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13801
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SCot&#30340;&#33258;&#30417;&#30563;&#21327;&#21516;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;BERT&#27169;&#22411;&#21644;&#20266;&#26631;&#31614;&#65292;&#23454;&#29616;&#24320;&#25918;&#22495;&#27133;&#22635;&#20805;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#25163;&#21160;&#26631;&#27880;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27133;&#22635;&#20805;&#26159;&#29616;&#20195;&#20250;&#35805;&#31995;&#32479;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#20043;&#19968;&#12290;&#29616;&#26377;&#22823;&#37096;&#20998;&#25991;&#29486;&#37319;&#29992;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#38656;&#35201;&#27599;&#20010;&#26032;&#22495;&#30340;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#12290;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#24369;&#30417;&#30563;&#26041;&#27861;&#31561;&#24050;&#34920;&#29616;&#20986;&#26367;&#20195;&#25163;&#21160;&#26631;&#27880;&#30340;&#21069;&#26223;&#65292;&#20294;&#26159;&#36825;&#20123;&#23398;&#20064;&#33539;&#20363;&#22312;&#24615;&#33021;&#26041;&#38754;&#26126;&#26174;&#36874;&#20110;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;&#20026;&#20102;&#26368;&#23567;&#21270;&#36825;&#31181;&#24615;&#33021;&#24046;&#36317;&#24182;&#23637;&#31034;&#24320;&#25918;&#22495;&#27133;&#22635;&#20805;&#30340;&#21487;&#33021;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#21327;&#21516;&#35757;&#32451;&#26694;&#26550;&#65292;&#31216;&#20026;SCot&#65292;&#23427;&#19981;&#38656;&#35201;&#39046;&#22495;&#20869;&#25163;&#21160;&#26631;&#35760;&#35757;&#32451;&#31034;&#20363;&#24182;&#20998;&#20026;&#19977;&#20010;&#38454;&#27573;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Slot filling is one of the critical tasks in modern conversational systems. The majority of existing literature employs supervised learning methods, which require labeled training data for each new domain. Zero-shot learning and weak supervision approaches, among others, have shown promise as alternatives to manual labeling. Nonetheless, these learning paradigms are significantly inferior to supervised learning approaches in terms of performance. To minimize this performance gap and demonstrate the possibility of open-domain slot filling, we propose a Self-supervised Co-training framework, called SCot, that requires zero in-domain manually labeled training examples and works in three phases. Phase one acquires two sets of complementary pseudo labels automatically. Phase two leverages the power of the pre-trained language model BERT, by adapting it for the slot filling task using these sets of pseudo labels. In phase three, we introduce a self-supervised cotraining mechanism, where both
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#30340;&#26032;&#26694;&#26550;P-ToD&#65292;&#23427;&#20351;&#29992;&#38646;-shot&#27867;&#21270;&#22870;&#21169;&#20989;&#25968;&#36827;&#34892;&#26080;&#30417;&#30563;&#35757;&#32451;&#65292;&#24182;&#21462;&#24471;&#20102;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.13797</link><description>&lt;p&gt;
&#38646;-shot&#27867;&#21270;&#22870;&#21169;&#20989;&#25968;&#20010;&#24615;&#21270;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Personalizing Task-oriented Dialog Systems via Zero-shot Generalizable Reward Function. (arXiv:2303.13797v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13797
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#30340;&#26032;&#26694;&#26550;P-ToD&#65292;&#23427;&#20351;&#29992;&#38646;-shot&#27867;&#21270;&#22870;&#21169;&#20989;&#25968;&#36827;&#34892;&#26080;&#30417;&#30563;&#35757;&#32451;&#65292;&#24182;&#21462;&#24471;&#20102;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#31995;&#32479;&#20351;&#29992;&#25143;&#33021;&#22815;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#23436;&#25104;&#20219;&#21153;&#12290;&#29616;&#26377;&#25216;&#26415;&#30340;&#31995;&#32479;&#26080;&#35770;&#29992;&#25143;&#22914;&#20309;&#65292;&#37117;&#20250;&#29992;&#30456;&#21516;&#30340;&#26041;&#24335;&#22238;&#24212;&#65292;&#20294;&#26159;&#33258;&#23450;&#20041;&#23545;&#35805;&#21487;&#33021;&#20250;&#25552;&#39640;&#37319;&#29992;&#29575;&#21644;&#26356;&#22909;&#30340;&#29992;&#25143;&#20307;&#39564;&#12290;&#26500;&#24314;&#20010;&#24615;&#21270;&#23545;&#35805;&#31995;&#32479;&#26159;&#19968;&#39033;&#37325;&#35201;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#21482;&#26377;&#23569;&#25968;&#20960;&#39033;&#24037;&#20316;&#38754;&#23545;&#20102;&#36825;&#19968;&#25361;&#25112;&#12290;&#22823;&#37096;&#20998;&#29616;&#26377;&#24037;&#20316;&#20381;&#36182;&#20110;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#38656;&#35201;&#27599;&#20010;&#29992;&#25143;&#36164;&#26009;&#36827;&#34892;&#32321;&#29712;&#21644;&#26114;&#36149;&#30340;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#20026;&#27599;&#20010;&#29992;&#25143;&#26723;&#26696;&#25910;&#38598;&#21644;&#26631;&#35760;&#25968;&#25454;&#20960;&#20046;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;P-ToD&#65292;&#36890;&#36807;&#38646;-shot&#27867;&#21270;&#22870;&#21169;&#20989;&#25968;&#65292;&#20010;&#24615;&#21270;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#31995;&#32479;&#65292;&#36866;&#24212;&#20102;&#24191;&#27867;&#30340;&#29992;&#25143;&#36164;&#26009;&#65292;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#36827;&#34892;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;P-ToD&#22312;&#20219;&#21153;&#25104;&#21151;&#29575;&#21644;&#29992;&#25143;&#28385;&#24847;&#24230;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Task-oriented dialog systems enable users to accomplish tasks using natural language. State-of-the-art systems respond to users in the same way regardless of their personalities, although personalizing dialogues can lead to higher levels of adoption and better user experiences. Building personalized dialog systems is an important, yet challenging endeavor and only a handful of works took on the challenge. Most existing works rely on supervised learning approaches and require laborious and expensive labeled training data for each user profile. Additionally, collecting and labeling data for each user profile is virtually impossible. In this work, we propose a novel framework, P-ToD, to personalize task-oriented dialog systems capable of adapting to a wide range of user profiles in an unsupervised fashion using a zero-shot generalizable reward function. P-ToD uses a pre-trained GPT-2 as a backbone model and works in three phases. Phase one performs task-specific training. Phase two kicks 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20219;&#21153;&#21644;&#39046;&#22495;&#29305;&#23450;&#25552;&#31034;&#26469;&#20248;&#21270;ChatGPT&#22312;&#22797;&#26434;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#30740;&#31350;&#21457;&#29616;&#28201;&#24230;&#35774;&#32622;&#21644;&#20219;&#21153;&#20449;&#24687;&#23545;ChatGPT&#34920;&#29616;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2303.13780</link><description>&lt;p&gt;
&#20248;&#21270;ChatGPT&#22312;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Towards Making the Most of ChatGPT for Machine Translation. (arXiv:2303.13780v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20219;&#21153;&#21644;&#39046;&#22495;&#29305;&#23450;&#25552;&#31034;&#26469;&#20248;&#21270;ChatGPT&#22312;&#22797;&#26434;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#30740;&#31350;&#21457;&#29616;&#28201;&#24230;&#35774;&#32622;&#21644;&#20219;&#21153;&#20449;&#24687;&#23545;ChatGPT&#34920;&#29616;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#22312;&#26426;&#22120;&#32763;&#35793;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23427;&#22312;&#39640;&#36164;&#28304;&#35821;&#35328;&#26041;&#38754;&#21487;&#20197;&#36798;&#21040;&#21830;&#19994;&#31995;&#32479;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#20294;&#22312;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#65288;&#20363;&#22914;&#20302;&#36164;&#28304;&#21644;&#36828;&#31243;&#35821;&#35328;&#23545;&#32763;&#35793;&#65289;&#33853;&#21518;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#37319;&#29992;&#31616;&#21333;&#30340;&#25552;&#31034;&#65292;&#26080;&#27861;&#20805;&#20998;&#21457;&#25381;ChatGPT&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#37325;&#26032;&#23457;&#35270;&#28201;&#24230;&#12289;&#20219;&#21153;&#20449;&#24687;&#21644;&#39046;&#22495;&#20449;&#24687;&#31561;&#20960;&#20010;&#26041;&#38754;&#65292;&#36827;&#19968;&#27493;&#25366;&#25496;ChatGPT&#30340;&#32763;&#35793;&#33021;&#21147;&#65292;&#24182;&#30456;&#24212;&#22320;&#25552;&#20986;&#20004;&#31181;&#65288;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#65289;&#25552;&#31034;&#65306;&#20219;&#21153;&#29305;&#23450;&#25552;&#31034;&#65288;TSP&#65289;&#21644;&#39046;&#22495;&#29305;&#23450;&#25552;&#31034;&#65288;DSP&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT shows remarkable capabilities for machine translation (MT). Several prior studies have shown that it achieves comparable results to commercial systems for high-resource languages, but lags behind in complex tasks, e.g, low-resource and distant-language-pairs translation. However, they usually adopt simple prompts which can not fully elicit the capability of ChatGPT. In this report, we aim to further mine ChatGPT's translation ability by revisiting several aspects: temperature, task information, and domain information, and correspondingly propose two (simple but effective) prompts: Task-Specific Prompts (TSP) and Domain-Specific Prompts (DSP). We show that: 1) The performance of ChatGPT depends largely on temperature, and a lower temperature usually can achieve better performance; 2) Emphasizing the task information further improves ChatGPT's performance, particularly in complex MT tasks; 3) Introducing domain information can elicit ChatGPT's generalization ability and improve i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#38024;&#23545;&#25918;&#30103;&#24739;&#32773;&#30340;&#20020;&#24202;&#35760;&#24405;&#65292;&#33258;&#21160;&#25552;&#21462;&#39135;&#31649;&#28814;&#30340;&#23384;&#22312;&#21644;&#20005;&#37325;&#31243;&#24230;&#65292;&#20026;&#30740;&#31350;&#27602;&#24615;&#21453;&#24212;&#25552;&#20379;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.13722</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33258;&#21160;&#25552;&#21462;&#25918;&#30103;&#24739;&#32773;&#20020;&#24202;&#35760;&#24405;&#20013;&#30340;&#39135;&#31649;&#28814;
&lt;/p&gt;
&lt;p&gt;
Natural language processing to automatically extract the presence and severity of esophagitis in notes of patients undergoing radiotherapy. (arXiv:2303.13722v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13722
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#38024;&#23545;&#25918;&#30103;&#24739;&#32773;&#30340;&#20020;&#24202;&#35760;&#24405;&#65292;&#33258;&#21160;&#25552;&#21462;&#39135;&#31649;&#28814;&#30340;&#23384;&#22312;&#21644;&#20005;&#37325;&#31243;&#24230;&#65292;&#20026;&#30740;&#31350;&#27602;&#24615;&#21453;&#24212;&#25552;&#20379;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25918;&#23556;&#27835;&#30103;&#65288;RT&#65289;&#27602;&#24615;&#21453;&#24212;&#20250;&#24433;&#21709;&#24739;&#32773;&#30340;&#29983;&#23384;&#21644;&#29983;&#27963;&#36136;&#37327;&#65292;&#20294;&#30740;&#31350;&#36824;&#19981;&#36275;&#12290;&#29616;&#23454;&#19990;&#30028;&#30340;&#35777;&#25454;&#26377;&#26395;&#25913;&#21892;&#25105;&#20204;&#23545;&#27602;&#24615;&#21453;&#24212;&#30340;&#35748;&#35782;&#65292;&#20294;&#27602;&#24615;&#20449;&#24687;&#36890;&#24120;&#20165;&#23384;&#22312;&#20110;&#20020;&#24202;&#35760;&#24405;&#20013;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#25509;&#21463;&#33016;&#37096; RT &#27835;&#30103;&#30340;&#24739;&#32773;&#30340;&#35760;&#24405;&#20013;&#35782;&#21035;&#39135;&#31649;&#28814;&#30340;&#23384;&#22312;&#21644;&#20005;&#37325;&#31243;&#24230;&#12290;&#25105;&#20204;&#23545;&#19977;&#20010;&#39135;&#31649;&#28814;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#20102;&#32479;&#35745;&#21644;&#39044;&#35757;&#32451; BERT &#27169;&#22411;&#30340;&#24494;&#35843;&#65306;&#20219;&#21153; 1&#65289;&#39135;&#31649;&#28814;&#30340;&#23384;&#22312;&#65292;&#20219;&#21153; 2&#65289;&#37325;&#24230;&#39135;&#31649;&#28814;&#25110;&#21542;&#65292;&#20197;&#21450;&#20219;&#21153; 3&#65289;&#26080;&#39135;&#31649;&#28814; vs. &#19968;&#32423; vs. &#20108;&#19977;&#32423;&#12290;&#25105;&#20204;&#20351;&#29992;&#25509;&#21463;&#39135;&#31649;&#30284;&#25918;&#30103;&#30340; 345 &#20221;&#24739;&#32773;&#35760;&#24405;&#36827;&#34892;&#20102;&#21487;&#36801;&#31227;&#24615;&#27979;&#35797;&#12290;&#20351;&#29992; PubmedBERT &#36827;&#34892;&#24494;&#35843;&#21462;&#24471;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;&#20219;&#21153; 1&#12289;&#20219;&#21153; 2 &#21644;&#20219;&#21153; 3 &#30340;&#26368;&#20339;&#23439; F1 &#20998;&#21035;&#20026; 0.92&#12289;0.82 &#21644; 0.74&#12290;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#36873;&#25321;&#26368;&#20449;&#24687;&#37327;&#30340;&#35760;&#24405;&#37096;&#20998;&#65292;&#21487;&#23558;&#23439; F1 &#25552;&#39640;&#36229;&#36807; 2%&#12290;&#38134;&#26631;&#35760;&#25968;&#25454;&#23558;&#23439; F1 &#25552;&#39640;&#36229;&#36807; 3 &#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Radiotherapy (RT) toxicities can impair survival and quality-of-life, yet remain under-studied. Real-world evidence holds potential to improve our understanding of toxicities, but toxicity information is often only in clinical notes. We developed natural language processing (NLP) models to identify the presence and severity of esophagitis from notes of patients treated with thoracic RT. We fine-tuned statistical and pre-trained BERT-based models for three esophagitis classification tasks: Task 1) presence of esophagitis, Task 2) severe esophagitis or not, and Task 3) no esophagitis vs. grade 1 vs. grade 2-3. Transferability was tested on 345 notes from patients with esophageal cancer undergoing RT.  Fine-tuning PubmedBERT yielded the best performance. The best macro-F1 was 0.92, 0.82, and 0.74 for Task 1, 2, and 3, respectively. Selecting the most informative note sections during fine-tuning improved macro-F1 by over 2% for all tasks. Silver-labeled data improved the macro-F1 by over 3
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21512;&#25104;&#36890;&#29992;&#22522;&#20934;&#30340;&#23616;&#38480;&#24615;&#65292;&#21457;&#29616;&#36923;&#36753;&#24418;&#24335;&#65288;LF&#65289;&#30340;&#32454;&#33410;&#21487;&#33021;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#12290;&#20316;&#32773;&#23545;COGS&#22522;&#20934;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#32467;&#26524;&#34920;&#26126;&#22522;&#30784;&#27169;&#22411;&#33021;&#22815;&#33719;&#24471;&#36275;&#22815;&#30340;&#25484;&#25569;&#12290;&#20316;&#32773;&#36824;&#24378;&#35843;&#20102;&#35774;&#35745;&#33021;&#20934;&#30830;&#25429;&#25417;&#33258;&#28982;&#35821;&#35328;&#35821;&#20041;&#30340;LF&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.13716</link><description>&lt;p&gt;
ReCOGS: &#19968;&#20010;&#36923;&#36753;&#24418;&#24335;&#30340;&#32454;&#33410;&#22914;&#20309;&#24433;&#21709;&#35821;&#20041;&#35299;&#37322;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
ReCOGS: How Incidental Details of a Logical Form Overshadow an Evaluation of Semantic Interpretation. (arXiv:2303.13716v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13716
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21512;&#25104;&#36890;&#29992;&#22522;&#20934;&#30340;&#23616;&#38480;&#24615;&#65292;&#21457;&#29616;&#36923;&#36753;&#24418;&#24335;&#65288;LF&#65289;&#30340;&#32454;&#33410;&#21487;&#33021;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#12290;&#20316;&#32773;&#23545;COGS&#22522;&#20934;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#32467;&#26524;&#34920;&#26126;&#22522;&#30784;&#27169;&#22411;&#33021;&#22815;&#33719;&#24471;&#36275;&#22815;&#30340;&#25484;&#25569;&#12290;&#20316;&#32773;&#36824;&#24378;&#35843;&#20102;&#35774;&#35745;&#33021;&#20934;&#30830;&#25429;&#25417;&#33258;&#28982;&#35821;&#35328;&#35821;&#20041;&#30340;LF&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#36890;&#29992;&#22522;&#20934;&#26088;&#22312;&#35780;&#20272;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#20934;&#30830;&#22320;&#35745;&#31639;&#26032;&#21477;&#23376;&#30340;&#21547;&#20041;&#65292;&#20294;&#26159;&#29992;&#36923;&#36753;&#24418;&#24335;&#65288;LF&#65289;&#39044;&#27979;&#26469;&#25805;&#20316;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#25285;&#24551;&#65292;&#21363;&#25152;&#36873;&#25321;&#30340;LF&#30340;&#35821;&#20041;&#26080;&#20851;&#30340;&#32454;&#33410;&#21487;&#33021;&#20250;&#22609;&#36896;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#35748;&#20026;COGS&#22522;&#20934;&#65288;Kim&#21644;Linzen&#65292;2020&#65289;&#23454;&#29616;&#20102;&#36825;&#19968;&#20851;&#27880;&#28857;&#12290;COGS&#25552;&#20986;&#20102;&#30475;&#36215;&#26469;&#23545;&#29616;&#26377;&#27169;&#22411;&#26469;&#35828;&#19981;&#21487;&#33021;&#30340;&#36890;&#29992;&#20998;&#21106;&#65292;&#36825;&#21487;&#33021;&#34987;&#35270;&#20026;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#25511;&#35785;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#34920;&#26126;&#36127;&#38754;&#32467;&#26524;&#36319;COGS LFs&#30340;&#32454;&#33410;&#30456;&#20851;&#12290;&#23558;&#36825;&#20123;LF&#36716;&#25442;&#20026;&#35821;&#20041;&#31561;&#25928;&#30340;LF&#65292;&#24182;&#20998;&#35299;&#20986;&#19982;&#35821;&#20041;&#35299;&#37322;&#26080;&#20851;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#21457;&#29616;&#21363;&#20351;&#26159;&#22522;&#32447;&#27169;&#22411;&#20063;&#33021;&#33719;&#24471;&#36275;&#22815;&#30340;&#25484;&#25569;&#12290;&#26368;&#36817;&#30340;COGS LFs&#26080;&#21464;&#37327;&#32763;&#35793;&#34920;&#26126;&#20102;&#31867;&#20284;&#30340;&#32467;&#35770;&#65292;&#20294;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#31181;&#26684;&#24335;&#19981;&#26159;&#35821;&#20041;&#31561;&#25928;&#30340;&#65307;&#23427;&#26080;&#27861;&#20934;&#30830;&#34920;&#31034;&#19968;&#20123;COGS&#30340;&#21547;&#20041;&#12290;&#36825;&#20123;&#21457;&#29616;&#20419;&#36827;&#25105;&#20204;&#23545;&#24403;&#21069;&#30340;&#21512;&#25104;&#36890;&#29992;&#22522;&#20934;&#30340;&#23616;&#38480;&#24615;&#30340;&#29702;&#35299;&#65292;&#24182;&#24378;&#35843;&#35774;&#35745;&#20934;&#30830;&#25429;&#25417;&#33258;&#28982;&#35821;&#35328;&#35821;&#20041;&#30340;LF&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compositional generalization benchmarks seek to assess whether models can accurately compute meanings for novel sentences, but operationalize this in terms of logical form (LF) prediction. This raises the concern that semantically irrelevant details of the chosen LFs could shape model performance. We argue that this concern is realized for the COGS benchmark (Kim and Linzen, 2020). COGS poses generalization splits that appear impossible for present-day models, which could be taken as an indictment of those models. However, we show that the negative results trace to incidental features of COGS LFs. Converting these LFs to semantically equivalent ones and factoring out capabilities unrelated to semantic interpretation, we find that even baseline models get traction. A recent variable-free translation of COGS LFs suggests similar conclusions, but we observe this format is not semantically equivalent; it is incapable of accurately representing some COGS meanings. These findings inform our 
&lt;/p&gt;</description></item><item><title>Mordecai 3&#26159;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#22320;&#29702;&#35299;&#26512;&#21644;&#20107;&#20214;&#22320;&#29702;&#32534;&#30721;&#31995;&#32479;&#65292;&#20351;&#29992;&#26032;&#30340;&#31070;&#32463;&#25490;&#21517;&#27169;&#22411;&#35299;&#20915;&#20174;&#25991;&#26723;&#20013;&#25552;&#21462;&#30340;&#22320;&#21517;&#65292;&#24182;&#20351;&#29992;&#29616;&#25104;&#30340;&#38382;&#31572;&#27169;&#22411;&#25191;&#34892;&#20107;&#20214;&#22320;&#29702;&#32534;&#30721;&#12290;&#23427;&#20197;Mordecai 3&#30340;&#24418;&#24335;&#25552;&#20379;&#32473;&#29992;&#25143;&#20316;&#20026;&#19968;&#20010;&#24320;&#28304;Python&#24211;&#12290;</title><link>http://arxiv.org/abs/2303.13675</link><description>&lt;p&gt;
Mordecai 3: &#19968;&#31181;&#31070;&#32463;&#25991;&#26412;&#22320;&#29702;&#35299;&#26512;&#21644;&#20107;&#20214;&#22320;&#29702;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Mordecai 3: A Neural Geoparser and Event Geocoder. (arXiv:2303.13675v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13675
&lt;/p&gt;
&lt;p&gt;
Mordecai 3&#26159;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#22320;&#29702;&#35299;&#26512;&#21644;&#20107;&#20214;&#22320;&#29702;&#32534;&#30721;&#31995;&#32479;&#65292;&#20351;&#29992;&#26032;&#30340;&#31070;&#32463;&#25490;&#21517;&#27169;&#22411;&#35299;&#20915;&#20174;&#25991;&#26723;&#20013;&#25552;&#21462;&#30340;&#22320;&#21517;&#65292;&#24182;&#20351;&#29992;&#29616;&#25104;&#30340;&#38382;&#31572;&#27169;&#22411;&#25191;&#34892;&#20107;&#20214;&#22320;&#29702;&#32534;&#30721;&#12290;&#23427;&#20197;Mordecai 3&#30340;&#24418;&#24335;&#25552;&#20379;&#32473;&#29992;&#25143;&#20316;&#20026;&#19968;&#20010;&#24320;&#28304;Python&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Mordecai3 &#26159;&#19968;&#20010;&#26032;&#30340;&#31471;&#21040;&#31471;&#25991;&#26412;&#22320;&#29702;&#35299;&#26512;&#21644;&#20107;&#20214;&#22320;&#29702;&#32534;&#30721;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#20351;&#29992;&#26032;&#30340;&#31070;&#32463;&#25490;&#21517;&#27169;&#22411;&#23545;&#20174;&#25991;&#26723;&#20013;&#25552;&#21462;&#30340;&#22320;&#21517;&#36827;&#34892;&#22320;&#21517;&#35299;&#26512;&#65292;&#23558;&#20854;&#35299;&#26512;&#25104;Geonames&#22320;&#21517;&#35789;&#20856;&#20013;&#30340;&#26465;&#30446;&#12290;&#23427;&#36824;&#20351;&#29992;&#29616;&#25104;&#30340;&#38382;&#31572;&#27169;&#22411;&#25191;&#34892;&#20107;&#20214;&#22320;&#29702;&#32534;&#30721;&#65292;&#21363;&#23558;&#25991;&#26412;&#20013;&#25253;&#21578;&#30340;&#20107;&#20214;&#19982;&#23427;&#20204;&#21457;&#29983;&#30340;&#22320;&#26041;&#32852;&#31995;&#36215;&#26469;&#12290;&#22320;&#22336;&#35299;&#26512;&#27169;&#22411;&#22312;&#29616;&#26377;&#30340;&#22810;&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#21516;&#26102;&#36824;&#36827;&#34892;&#20102;&#20960;&#21315;&#20010;&#26032;&#30340;&#27880;&#37322;&#31034;&#20363;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#27169;&#22411;&#12289;&#23427;&#30340;&#35757;&#32451;&#36807;&#31243;&#21644;&#19982;&#29616;&#26377;&#22320;&#29702;&#35299;&#26512;&#22120;&#30340;&#24615;&#33021;&#27604;&#36739;&#12290;&#35813;&#31995;&#32479;&#20197;&#24320;&#28304;Python&#24211;Mordecai 3&#30340;&#24418;&#24335;&#25552;&#20379;&#65292;&#21462;&#20195;&#20102;&#20808;&#21069;&#22320;&#29702;&#35299;&#26512;&#22120;Mordecai v2&#25104;&#20026;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#25991;&#26412;&#22320;&#29702;&#35299;&#26512;&#22120;&#20043;&#19968;(Halterman 2017)&#12290;
&lt;/p&gt;
&lt;p&gt;
Mordecai3 is a new end-to-end text geoparser and event geolocation system. The system performs toponym resolution using a new neural ranking model to resolve a place name extracted from a document to its entry in the Geonames gazetteer. It also performs event geocoding, the process of linking events reported in text with the place names where they are reported to occur, using an off-the-shelf question-answering model. The toponym resolution model is trained on a diverse set of existing training data, along with several thousand newly annotated examples. The paper describes the model, its training process, and performance comparisons with existing geoparsers. The system is available as an open source Python library, Mordecai 3, and replaces an earlier geoparser, Mordecai v2, one of the most widely used text geoparsers (Halterman 2017).
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#21457;&#29616;&#65292;&#22312;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#20013;&#65292;ChatGPT&#30340;&#34920;&#29616;&#19981;&#22914;&#21830;&#19994;&#21644;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#27169;&#22411;&#65292;&#20294;&#26159;ChatGPT&#26356;&#21916;&#27426;&#29992;&#25913;&#21464;&#34920;&#36798;&#26041;&#24335;&#26469;&#20445;&#25345;&#35821;&#27861;&#27491;&#30830;&#24615;&#30340;&#26041;&#24335;&#36827;&#34892;&#20462;&#27491;&#24182;&#19988;&#36807;&#20462;&#27491;&#30340;&#38382;&#39064;&#36739;&#22810;&#12290;</title><link>http://arxiv.org/abs/2303.13648</link><description>&lt;p&gt;
ChatGPT&#36824;&#26159;Grammarly&#65311;&#22312;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#22522;&#20934;&#27979;&#35797;&#19978;&#35780;&#20272;ChatGPT&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT or Grammarly? Evaluating ChatGPT on Grammatical Error Correction Benchmark. (arXiv:2303.13648v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13648
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#21457;&#29616;&#65292;&#22312;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#20013;&#65292;ChatGPT&#30340;&#34920;&#29616;&#19981;&#22914;&#21830;&#19994;&#21644;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#27169;&#22411;&#65292;&#20294;&#26159;ChatGPT&#26356;&#21916;&#27426;&#29992;&#25913;&#21464;&#34920;&#36798;&#26041;&#24335;&#26469;&#20445;&#25345;&#35821;&#27861;&#27491;&#30830;&#24615;&#30340;&#26041;&#24335;&#36827;&#34892;&#20462;&#27491;&#24182;&#19988;&#36807;&#20462;&#27491;&#30340;&#38382;&#39064;&#36739;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#30001;OpenAI&#24320;&#21457;&#30340;&#23574;&#31471;&#20154;&#24037;&#26234;&#33021;&#35821;&#35328;&#27169;&#22411;&#65292;&#22240;&#20854;&#24778;&#20154;&#30340;&#22238;&#31572;&#36319;&#36827;&#38382;&#39064;&#30340;&#33021;&#21147;&#32780;&#21463;&#21040;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#26412;&#25991;&#26088;&#22312;&#35780;&#20272;ChatGPT&#22312;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#23558;&#20854;&#19982;&#21830;&#19994;GEC&#20135;&#21697;&#65288;&#20363;&#22914;Grammarly&#65289;&#21644;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65288;&#20363;&#22914;GECToR&#65289;&#36827;&#34892;&#27604;&#36739;&#12290;&#36890;&#36807;&#22312;CoNLL2014&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#27979;&#35797;&#65292;&#25105;&#20204;&#21457;&#29616;ChatGPT&#22312;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#65288;&#22914;$F_{0.5}$&#24471;&#20998;&#65289;&#26041;&#38754;&#30340;&#34920;&#29616;&#19981;&#22914;&#37027;&#20123;&#22522;&#32447;&#65292;&#29305;&#21035;&#26159;&#22312;&#38271;&#21477;&#20013;&#12290;&#25105;&#20204;&#26816;&#26597;&#20102;&#36755;&#20986;&#32467;&#26524;&#65292;&#21457;&#29616;ChatGPT&#19981;&#20165;&#36827;&#34892;&#21333;&#20010;&#20462;&#27491;&#65292;&#36824;&#26356;&#21916;&#27426;&#36890;&#36807;&#25913;&#21464;&#26576;&#20123;&#30701;&#35821;&#25110;&#21477;&#23376;&#32467;&#26500;&#30340;&#34920;&#36798;&#26041;&#24335;&#26469;&#20445;&#25345;&#35821;&#27861;&#19978;&#30340;&#27491;&#30830;&#24615;&#12290;&#20154;&#31867;&#35780;&#20272;&#37327;&#21270;&#22320;&#35777;&#23454;&#20102;&#36825;&#19968;&#28857;&#65292;&#24182;&#34920;&#26126;ChatGPT&#20135;&#29983;&#30340;&#27424;&#20462;&#27491;&#25110;&#35823;&#20462;&#27491;&#38382;&#39064;&#36739;&#23569;&#65292;&#20294;&#36807;&#20462;&#27491;&#30340;&#38382;&#39064;&#36739;&#22810;&#12290;&#36825;&#20123;&#32467;&#26524;&#35828;&#26126;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is a cutting-edge artificial intelligence language model developed by OpenAI, which has attracted a lot of attention due to its surprisingly strong ability in answering follow-up questions. In this report, we aim to evaluate ChatGPT on the Grammatical Error Correction(GEC) task, and compare it with commercial GEC product (e.g., Grammarly) and state-of-the-art models (e.g., GECToR). By testing on the CoNLL2014 benchmark dataset, we find that ChatGPT performs not as well as those baselines in terms of the automatic evaluation metrics (e.g., $F_{0.5}$ score), particularly on long sentences. We inspect the outputs and find that ChatGPT goes beyond one-by-one corrections. Specifically, it prefers to change the surface expression of certain phrases or sentence structure while maintaining grammatical correctness. Human evaluation quantitatively confirms this and suggests that ChatGPT produces less under-correction or mis-correction issues but more over-corrections. These results demon
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;Essential Element Network (EEN)&#31639;&#27861;&#23558;&#38899;&#39057;&#32534;&#30721;&#25104;&#25991;&#26412;&#24182;&#36827;&#34892;&#30456;&#20851;&#24615;&#35745;&#31639;&#21644;&#20248;&#21270;&#24212;&#29992;&#20110;&#32858;&#31867;&#31995;&#25968;&#30340;&#39057;&#29575;&#21644;&#25490;&#21517;&#30340;&#26041;&#27861;&#65292;&#24471;&#21040;&#20102;&#38899;&#20048;&#30340;&#28145;&#23618;&#32467;&#26500;&#20449;&#24687;&#65292;&#20026;&#21400;&#28165;&#38899;&#20048;&#32467;&#26500;&#25552;&#20379;&#20102;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.13631</link><description>&lt;p&gt;
&#38899;&#20048;&#32467;&#26500;&#30340;&#33258;&#32452;&#32455;&#32593;&#32476;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
In-depth analysis of music structure as a self-organized network. (arXiv:2303.13631v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13631
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;Essential Element Network (EEN)&#31639;&#27861;&#23558;&#38899;&#39057;&#32534;&#30721;&#25104;&#25991;&#26412;&#24182;&#36827;&#34892;&#30456;&#20851;&#24615;&#35745;&#31639;&#21644;&#20248;&#21270;&#24212;&#29992;&#20110;&#32858;&#31867;&#31995;&#25968;&#30340;&#39057;&#29575;&#21644;&#25490;&#21517;&#30340;&#26041;&#27861;&#65292;&#24471;&#21040;&#20102;&#38899;&#20048;&#30340;&#28145;&#23618;&#32467;&#26500;&#20449;&#24687;&#65292;&#20026;&#21400;&#28165;&#38899;&#20048;&#32467;&#26500;&#25552;&#20379;&#20102;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#35789;&#27719;&#19981;&#20165;&#20256;&#36882;&#20449;&#24687;&#65292;&#36824;&#38543;&#30528;&#25991;&#26126;&#21644;&#20154;&#31867;&#36801;&#31227;&#32780;&#28436;&#21464;&#12290;&#38899;&#20048;&#20063;&#26159;&#22914;&#27492;&#12290;&#20026;&#20102;&#29702;&#35299;&#38899;&#20048;&#32972;&#21518;&#30340;&#22797;&#26434;&#32467;&#26500;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21483;&#20570;Essential Element Network (EEN)&#30340;&#31639;&#27861;&#23558;&#38899;&#39057;&#32534;&#30721;&#25104;&#25991;&#26412;&#12290;&#35813;&#32593;&#32476;&#36890;&#36807;&#35745;&#31639;&#38899;&#35843;&#12289;&#26102;&#38388;&#21644;&#38899;&#37327;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#24471;&#21040;&#65292;&#36890;&#36807;&#20248;&#21270;EEN&#31639;&#27861;&#20197;&#29983;&#25104;Zipf&#23450;&#24459;&#24212;&#29992;&#20110;&#32858;&#31867;&#31995;&#25968;&#30340;&#39057;&#29575;&#21644;&#25490;&#21517;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#35821;&#20041;&#20851;&#31995;&#35270;&#20026;&#35789;&#27719;&#24182;&#29983;&#25104;&#23427;&#20204;&#30340;&#26144;&#23556;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#32534;&#30721;&#21518;&#30340;&#35789;&#27719;&#26144;&#23556;&#21040;&#38899;&#35843;-&#26102;&#38388;&#31354;&#38388;&#20013;&#65292;&#26377;&#21161;&#20110;&#25105;&#20204;&#31995;&#32479;&#22320;&#32452;&#32455;&#38899;&#20048;&#28145;&#23618;&#32467;&#26500;&#20013;&#30340;&#21477;&#27861;&#12290;&#30456;&#27604;&#20110;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#40657;&#30418;&#23376;&#29305;&#24615;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#25552;&#20379;&#20102;&#23545;&#38899;&#20048;&#32972;&#21518;&#22797;&#26434;&#32593;&#32476;&#30340;&#31934;&#30830;&#25551;&#36848;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#36807;&#31243;&#31215;&#32047;&#30340;&#32463;&#39564;&#21644;&#23646;&#24615;&#19981;&#20165;&#20026;&#27492;&#31867;&#24212;&#29992;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#20063;&#20026;&#35768;&#22810;&#20854;&#20182;&#30456;&#20851;&#39046;&#22495;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#25506;&#32034;&#30340;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Words in a natural language not only transmit information but also evolve with the development of civilization and human migration. The same is true for music. To understand the complex structure behind the music, we introduced an algorithm called the Essential Element Network (EEN) to encode the audio into text. The network is obtained by calculating the correlations between scales, time, and volume. Optimizing EEN to generate Zipfs law for the frequency and rank of the clustering coefficient enables us to generate and regard the semantic relationships as words. We map these encoded words into the scale-temporal space, which helps us organize systematically the syntax in the deep structure of music. Our algorithm provides precise descriptions of the complex network behind the music, as opposed to the black-box nature of other deep learning approaches. As a result, the experience and properties accumulated through these processes can offer not only a new approach to the applications of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#21033;&#29992;SVM&#12289;KNN&#12289;&#23725;&#20998;&#31867;&#12289;&#31070;&#32463;&#32593;&#32476;&#31561;&#26041;&#27861;&#25506;&#31350;&#21452;&#35821;&#20420;&#27861;&#20316;&#23478;&#19982;&#38750;&#21452;&#35821;&#27861;&#22269;&#20316;&#23478;&#30340;&#25991;&#23398;&#20316;&#21697;&#22312;&#39118;&#26684;&#12289;&#35821;&#35328;&#24178;&#25200;&#19978;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2303.13622</link><description>&lt;p&gt;
&#21452;&#35821;&#20420;&#27861;&#20316;&#23478;&#19982;&#38750;&#21452;&#35821;&#27861;&#22269;&#20316;&#23478;&#25991;&#23398;&#20316;&#21697;&#30340;&#20316;&#32773;&#24402;&#23646;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Authorship attribution for Differences between Literary Texts by Bilingual Russian-French and Non-Bilingual French Authors. (arXiv:2303.13622v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13622
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#21033;&#29992;SVM&#12289;KNN&#12289;&#23725;&#20998;&#31867;&#12289;&#31070;&#32463;&#32593;&#32476;&#31561;&#26041;&#27861;&#25506;&#31350;&#21452;&#35821;&#20420;&#27861;&#20316;&#23478;&#19982;&#38750;&#21452;&#35821;&#27861;&#22269;&#20316;&#23478;&#30340;&#25991;&#23398;&#20316;&#21697;&#22312;&#39118;&#26684;&#12289;&#35821;&#35328;&#24178;&#25200;&#19978;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#12289;$K$&#26368;&#36817;&#37051;&#65288;KNN&#65289;&#12289;&#23725;&#20998;&#31867;&#12289;&#31070;&#32463;&#32593;&#32476;&#31561;&#26041;&#27861;&#36827;&#34892;&#20998;&#26512;&#65292;&#26088;&#22312;&#25506;&#31350;&#21452;&#35821;&#20420;&#27861;&#20316;&#23478;&#65288;&#22914;&#23433;&#24503;&#38647;&#183;&#39532;&#37329;&#12289;&#29926;&#33713;&#37324;&#183;&#38463;&#27861;&#32435;&#35874;&#22827;&#12289;&#24343;&#25289;&#22522;&#31859;&#23572;&#183;&#36153;&#22885;&#22810;&#32599;&#22827;&#26031;&#22522;&#12289;&#20234;&#25096;&#23572;&#183;&#26684;&#20848;&#12289;&#21346;&#24052;&#183;&#23588;&#23572;&#25096;&#26862;&#65289;&#19982;&#38750;&#21452;&#35821;&#27861;&#22269;&#20316;&#23478;&#30340;&#25991;&#23398;&#20316;&#21697;&#22312;&#39118;&#26684;&#19978;&#30340;&#24046;&#24322;&#65292;&#20197;&#21450;&#22312;&#20420;&#32599;&#26031;&#20316;&#23478;&#30340;&#27861;&#35821;&#20316;&#21697;&#20013;&#26159;&#21542;&#23384;&#22312;&#35821;&#35328;&#24178;&#25200;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Do bilingual Russian-French authors of the end of the twentieth century such as Andre\"i Makine, Val\'ery Afanassiev, Vladimir F\'edorovski, Iegor Gran, Luba Jurgenson have common stylistic traits in the novels they wrote in French? Can we distinguish between them and non-bilingual French writers' texts? Is the phenomenon of interference observable in French texts of Russian authors? This paper applies authorship attribution methods including Support Vector Machine (SVM), $K$-Nearest Neighbors (KNN), Ridge classification, and Neural Network to answer these questions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#19996;&#21335;&#20122;&#20116;&#31181;&#35821;&#35328;&#21644;Singlish&#30340;&#28151;&#21512;&#20195;&#30721;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;ChatGPT&#23637;&#29616;&#20986;&#26368;&#39640;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35789;&#27719;&#36873;&#25321;&#38169;&#35823;&#30340;&#24433;&#21709;&#65292;ChatGPT&#21644;InstructGPT&#22312;&#29983;&#25104;&#28151;&#21512;&#20195;&#30721;&#26102;&#30340;&#29087;&#32451;&#31243;&#24230;&#21463;&#21040;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2303.13592</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#28151;&#21512;&#20195;&#30721;&#25991;&#26412;&#30340;&#25552;&#31034;&#65306;&#19996;&#21335;&#20122;&#35821;&#35328;&#30340;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
Prompting Large Language Models to Generate Code-Mixed Texts: The Case of South East Asian Languages. (arXiv:2303.13592v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#19996;&#21335;&#20122;&#20116;&#31181;&#35821;&#35328;&#21644;Singlish&#30340;&#28151;&#21512;&#20195;&#30721;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;ChatGPT&#23637;&#29616;&#20986;&#26368;&#39640;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35789;&#27719;&#36873;&#25321;&#38169;&#35823;&#30340;&#24433;&#21709;&#65292;ChatGPT&#21644;InstructGPT&#22312;&#29983;&#25104;&#28151;&#21512;&#20195;&#30721;&#26102;&#30340;&#29087;&#32451;&#31243;&#24230;&#21463;&#21040;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28151;&#21512;&#20195;&#30721;&#22312;&#19990;&#30028;&#35768;&#22810;&#22320;&#21306;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#35821;&#35328;&#23454;&#36341;&#65292;&#20294;&#25910;&#38598;&#39640;&#36136;&#37327;&#19988;&#20302;&#25104;&#26412;&#30340;&#28151;&#21512;&#20195;&#30721;&#25968;&#25454;&#20173;&#28982;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30740;&#31350;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26222;&#21450;&#36843;&#20351;&#20154;&#20204;&#38382;&#65306;&#36825;&#20123;&#31995;&#32479;&#33021;&#29992;&#20110;&#25968;&#25454;&#29983;&#25104;&#21527;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#19968;&#20010;&#38646;-shot&#30340;&#26041;&#24335;&#19979;&#22914;&#20309;&#25552;&#31034;LLMs&#20026;&#19996;&#21335;&#20122;&#65288;SEA&#65289;&#30340;&#20116;&#31181;&#35821;&#35328;&#65288;&#21360;&#23612;&#35821;&#65292;&#39532;&#26469;&#35821;&#65292;&#20013;&#25991;&#65292;&#22612;&#21152;&#36335;&#35821;&#65292;&#36234;&#21335;&#35821;&#65289;&#21450;&#20811;&#37324;&#22885;&#23572;&#35821;S ingl ish&#21019;&#36896;&#28151;&#21512;&#20195;&#30721;&#25968;&#25454;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;ChatGPT&#26174;&#31034;&#20986;&#26368;&#22823;&#30340;&#28508;&#21147;&#65292;&#24403;&#26126;&#30830;&#23450;&#20041;&#8220;&#28151;&#21512;&#20195;&#30721;&#8221;&#26415;&#35821;&#26102;&#65292;&#33021;&#22815;68%&#30340;&#26102;&#38388;&#29983;&#25104;&#28151;&#21512;&#20195;&#30721;&#25991;&#26412;&#12290;&#27492;&#22806;&#65292;ChatGPT&#21644;InstructGPT&#65288;davinci-003&#65289;&#29983;&#25104;S ingl ish&#25991;&#26412;&#30340;&#34920;&#29616;&#20063;&#20540;&#24471;&#27880;&#24847;&#65292;&#23427;&#20204;&#22312;&#21508;&#31181;&#25552;&#31034;&#19979;&#30340;&#25104;&#21151;&#29575;&#24179;&#22343;&#20026;96%&#12290;&#20294;&#26159;&#65292;ChatGPT&#21644;InstructGPT&#30340;&#28151;&#21512;&#20195;&#30721;&#29087;&#32451;&#31243;&#24230;&#21463;&#21040;&#35789;&#27719;&#36873;&#25321;&#38169;&#35823;&#30340;&#24433;&#21709;&#65292;&#23548;&#33268;&#35821;&#20041;&#19981;&#27491;&#30830;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
While code-mixing is a common linguistic practice in many parts of the world, collecting high-quality and low-cost code-mixed data remains a challenge for natural language processing (NLP) research. The proliferation of Large Language Models (LLMs) in recent times compels one to ask: can these systems be used for data generation? In this article, we explore prompting LLMs in a zero-shot manner to create code-mixed data for five languages in South East Asia (SEA) -Indonesian, Malay, Chinese, Tagalog, Vietnamese, as well as the creole language Singlish. We find that ChatGPT shows the most potential, capable of producing code-mixed text 68% of the time when the term "code-mixing" is explicitly defined. Moreover, both ChatGPT and InstructGPT's (davinci-003) performances in generating Singlish texts are noteworthy, averaging a 96% success rate across a variety of prompts. The code-mixing proficiency of ChatGPT and InstructGPT, however, is dampened by word choice errors that lead to semant
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27531;&#24046;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#22411;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#21487;&#36870;&#30340;&#21477;&#23376;&#23884;&#20837;&#12290;&#19982;&#20854;&#20182;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;&#22238;&#24402;&#30340;&#36755;&#20986;&#23618;&#37325;&#24314;&#36755;&#20837;&#24207;&#21015;&#30340;&#21333;&#35789;&#21521;&#37327;&#65292;&#20854;&#20855;&#26377;&#39640;&#20934;&#30830;&#24230;&#21644;&#24555;&#36895;&#35757;&#32451;&#36895;&#24230;&#12290;&#36825;&#31181;&#26041;&#27861;&#36866;&#21512;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#23545;&#38656;&#35201;&#39640;&#36136;&#37327;&#21477;&#23884;&#20837;&#30340;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#30340;&#20351;&#29992;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2303.13570</link><description>&lt;p&gt;
RNN &#30340;&#22238;&#24402;&#65306;&#29992;&#21487;&#36870;&#21477;&#23884;&#20837;&#30340;&#27531;&#24046;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Return of the RNN: Residual Recurrent Networks for Invertible Sentence Embeddings. (arXiv:2303.13570v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27531;&#24046;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#22411;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#21487;&#36870;&#30340;&#21477;&#23376;&#23884;&#20837;&#12290;&#19982;&#20854;&#20182;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;&#22238;&#24402;&#30340;&#36755;&#20986;&#23618;&#37325;&#24314;&#36755;&#20837;&#24207;&#21015;&#30340;&#21333;&#35789;&#21521;&#37327;&#65292;&#20854;&#20855;&#26377;&#39640;&#20934;&#30830;&#24230;&#21644;&#24555;&#36895;&#35757;&#32451;&#36895;&#24230;&#12290;&#36825;&#31181;&#26041;&#27861;&#36866;&#21512;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#23545;&#38656;&#35201;&#39640;&#36136;&#37327;&#21477;&#23884;&#20837;&#30340;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#30340;&#20351;&#29992;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#27169;&#22411;&#65292;&#20351;&#29992;&#27531;&#24046;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#22312;&#26080;&#30417;&#30563;&#32534;&#30721;&#20219;&#21153;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#29983;&#25104;&#21487;&#36870;&#30340;&#21477;&#23376;&#23884;&#20837;&#12290;&#30456;&#27604;&#20110;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#20013;&#24120;&#35265;&#30340;&#27010;&#29575;&#36755;&#20986;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#22522;&#20110;&#22238;&#24402;&#30340;&#36755;&#20986;&#23618;&#26469;&#37325;&#24314;&#36755;&#20837;&#24207;&#21015;&#30340;&#21333;&#35789;&#21521;&#37327;&#12290;&#35813;&#27169;&#22411;&#22312;&#20351;&#29992; ADAM &#20248;&#21270;&#22120;&#36827;&#34892;&#24555;&#36895;&#35757;&#32451;&#30340;&#21516;&#26102;&#65292;&#21462;&#24471;&#20102;&#39640;&#20934;&#30830;&#24230;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#27531;&#24046;&#36830;&#25509;&#21644;&#8220;match drop&#8221;&#25216;&#26415;&#65292;&#21363;&#21482;&#35745;&#31639;&#38169;&#35823;&#21333;&#35789;&#30340;&#26799;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#28508;&#22312;&#20248;&#21183;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#35201;&#39640;&#36136;&#37327;&#21477;&#23884;&#20837;&#30340;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents a novel model for invertible sentence embeddings using a residual recurrent network trained on an unsupervised encoding task. Rather than the probabilistic outputs common to neural machine translation models, our approach employs a regression-based output layer to reconstruct the input sequence's word vectors. The model achieves high accuracy and fast training with the ADAM optimizer, a significant finding given that RNNs typically require memory units, such as LSTMs, or second-order optimization methods. We incorporate residual connections and introduce a "match drop" technique, where gradients are calculated only for incorrect words. Our approach demonstrates potential for various natural language processing applications, particularly in neural network-based systems that require high-quality sentence embeddings.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;GAN&#30340;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#26080;&#30417;&#30563;&#35821;&#38899;&#35782;&#21035;&#20219;&#21153;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#37117;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.13559</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;GAN&#30340;&#26080;&#30417;&#30563;&#35821;&#38899;&#35782;&#21035;&#30340;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Enhancing Unsupervised Speech Recognition with Diffusion GANs. (arXiv:2303.13559v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;GAN&#30340;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#26080;&#30417;&#30563;&#35821;&#38899;&#35782;&#21035;&#20219;&#21153;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#37117;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#25193;&#25955;GAN&#22686;&#24378;&#20102;&#29992;&#20110;&#26080;&#30417;&#30563;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;(ASR)&#30340;&#26222;&#36890;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;(1)&#27880;&#20837;&#20102;&#24378;&#24230;&#19981;&#21516;&#30340;&#23454;&#20363;&#22122;&#22768;&#21040;&#29983;&#25104;&#22120;&#30340;&#36755;&#20986;&#21644;&#26410;&#26631;&#35760;&#30340;&#21442;&#32771;&#25991;&#26412;&#65292;&#36825;&#20123;&#25991;&#26412;&#26159;&#20174;&#24102;&#26377;&#38271;&#24230;&#32422;&#26463;&#30340;&#39044;&#35757;&#32451;&#38899;&#32032;&#35821;&#35328;&#27169;&#22411;&#20013;&#37319;&#26679;&#30340;&#65292;(2)&#35831;&#27714;&#25193;&#25955;&#26102;&#38388;&#27493;&#39588;&#30456;&#20851;&#30340;&#21028;&#21035;&#22120;&#23558;&#23427;&#20204;&#20998;&#24320;&#65292;(3)&#21453;&#21521;&#20256;&#25773;&#26799;&#24230;&#20197;&#26356;&#26032;&#29983;&#25104;&#22120;&#12290;&#22312;Librispeech(&#27979;&#35797;&#24178;&#20928;&#21644;&#27979;&#35797;&#20854;&#20182;&#30340;&#35823;&#24046;&#29575;&#20998;&#21035;&#20026;3.1%&#21644;5.6%)&#12289;TIMIT&#21644;MLS&#25968;&#25454;&#38598;&#19979;&#65292;&#19982;wav2vec-U&#36827;&#34892;&#21333;&#35789;/&#38899;&#32032;&#38169;&#35823;&#29575;&#27604;&#36739;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#22686;&#24378;&#31574;&#30053;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We enhance the vanilla adversarial training method for unsupervised Automatic Speech Recognition (ASR) by a diffusion-GAN. Our model (1) injects instance noises of various intensities to the generator's output and unlabeled reference text which are sampled from pretrained phoneme language models with a length constraint, (2) asks diffusion timestep-dependent discriminators to separate them, and (3) back-propagates the gradients to update the generator. Word/phoneme error rate comparisons with wav2vec-U under Librispeech (3.1% for test-clean and 5.6% for test-other), TIMIT and MLS datasets, show that our enhancement strategies work effectively.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DaToBS&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#33258;&#28982;&#29615;&#22659;&#20013;&#30340;&#29031;&#29255;&#20013;&#26816;&#27979;&#21644;&#36716;&#24405;Tifinagh&#23383;&#31526;&#65292;&#20197;&#25552;&#39640;&#38750;&#27954;&#20302;&#36164;&#28304;&#35821;&#35328;&#38463;&#39532;&#40784;&#35821;&#22312;&#25945;&#32946;&#12289;&#30740;&#31350;&#21644;&#32593;&#32476;&#24212;&#29992;&#31561;&#26041;&#38754;&#30340;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2303.13549</link><description>&lt;p&gt;
&#20174;&#20302;&#36164;&#28304;&#35821;&#35328;&#38463;&#39532;&#40784;&#35821;&#30340;&#22270;&#20687;&#20013;&#36827;&#34892;Tifinagh&#23383;&#31526;&#30340;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#21644;&#36716;&#24405;
&lt;/p&gt;
&lt;p&gt;
Optical Character Recognition and Transcription of Berber Signs from Images in a Low-Resource Language Amazigh. (arXiv:2303.13549v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DaToBS&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#33258;&#28982;&#29615;&#22659;&#20013;&#30340;&#29031;&#29255;&#20013;&#26816;&#27979;&#21644;&#36716;&#24405;Tifinagh&#23383;&#31526;&#65292;&#20197;&#25552;&#39640;&#38750;&#27954;&#20302;&#36164;&#28304;&#35821;&#35328;&#38463;&#39532;&#40784;&#35821;&#22312;&#25945;&#32946;&#12289;&#30740;&#31350;&#21644;&#32593;&#32476;&#24212;&#29992;&#31561;&#26041;&#38754;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26575;&#26575;&#23572;&#35821;&#26159;&#19968;&#31181;&#20302;&#36164;&#28304;&#30340;&#21271;&#38750;&#22303;&#35821;&#65292;&#22312;&#25705;&#27931;&#21733;&#12289;&#38463;&#23572;&#21450;&#21033;&#20122;&#31561;&#22320;&#30340;&#26575;&#26575;&#23572;&#31038;&#21306;&#20013;&#20351;&#29992;&#33258;&#24049;&#29420;&#29305;&#30340;&#23383;&#27597;&#34920;Tifinagh&#12290;&#36825;&#31181;&#38750;&#27954;&#20122;&#32454;&#20122;&#35821;&#35328;&#26159;&#30001;1400&#19975;&#20154;&#20351;&#29992;&#30340;&#65292;&#20294;&#32570;&#20047;&#36275;&#22815;&#30340;&#25945;&#32946;&#12289;&#30740;&#31350;&#12289;&#32593;&#32476;&#24212;&#29992;&#31561;&#26041;&#38754;&#30340;&#25903;&#25345;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;DaToBS&#30340;&#26377;&#30417;&#30563;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#21644;&#36716;&#24405;Berber&#23383;&#31526;&#65292;&#26088;&#22312;&#23454;&#29616;&#20174;&#33258;&#28982;&#29615;&#22659;&#30340;&#29031;&#29255;&#20013;&#33258;&#21160;&#35782;&#21035;&#21644;&#36716;&#24405;Tifinagh&#23383;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Berber, or Amazigh language family is a low-resource North African vernacular language spoken by the indigenous Berber ethnic group. It has its own unique alphabet called Tifinagh used across Berber communities in Morocco, Algeria, and others. The Afroasiatic language Berber is spoken by 14 million people, yet lacks adequate representation in education, research, web applications etc. For instance, there is no option of translation to or from Amazigh / Berber on Google Translate, which hosts over 100 languages today. Consequently, we do not find specialized educational apps, L2 (2nd language learner) acquisition, automated language translation, and remote-access facilities enabled in Berber. Motivated by this background, we propose a supervised approach called DaToBS for Detection and Transcription of Berber Signs. The DaToBS approach entails the automatic recognition and transcription of Tifinagh characters from signs in photographs of natural environments. This is achieved by sel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;ChatGPT&#30340;Text-to-SQL&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#23637;&#31034;&#20854;&#24378;&#22823;&#30340;&#38646;-shot&#34920;&#29616;&#65292;&#23588;&#20854;&#22312;ADVETA(RPL)&#24773;&#22659;&#19979;&#20248;&#20110;&#38656;&#35201;&#24494;&#35843;&#30340;SOTA&#27169;&#22411;&#65292;&#26377;&#26395;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21457;&#25381;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.13547</link><description>&lt;p&gt;
ChatGPT &#30340;&#38646;-shot Text-to-SQL &#33021;&#21147;&#30340;&#32508;&#21512;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A comprehensive evaluation of ChatGPT's zero-shot Text-to-SQL capability. (arXiv:2303.13547v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13547
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;ChatGPT&#30340;Text-to-SQL&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#23637;&#31034;&#20854;&#24378;&#22823;&#30340;&#38646;-shot&#34920;&#29616;&#65292;&#23588;&#20854;&#22312;ADVETA(RPL)&#24773;&#22659;&#19979;&#20248;&#20110;&#38656;&#35201;&#24494;&#35843;&#30340;SOTA&#27169;&#22411;&#65292;&#26377;&#26395;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21457;&#25381;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#20840;&#38754;&#20998;&#26512;&#20102; ChatGPT &#30340; Text-to-SQL &#33021;&#21147;&#12290;&#32771;&#34385;&#21040;&#22823;&#22411;&#23545;&#35805;&#35821;&#35328;&#27169;&#22411; ChatGPT &#21644;&#20854;&#22312;&#23545;&#35805;&#33021;&#21147;&#21644;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#19978;&#30340;&#21360;&#35937;&#28145;&#21051;&#30340;&#34920;&#29616;&#65292;&#25105;&#20204;&#35797;&#22270;&#35780;&#20272;&#20854; Text-to-SQL &#24615;&#33021;&#12290;&#25105;&#20204;&#23545;12&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#28041;&#21450;&#19981;&#21516;&#30340;&#35821;&#35328;&#12289;&#35774;&#32622;&#25110;&#22330;&#26223;&#65292;&#24182;&#19988;&#32467;&#26524;&#34920;&#26126; ChatGPT &#20855;&#26377;&#24378;&#22823;&#30340; Text-to-SQL &#33021;&#21147;&#12290;&#34429;&#28982;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#34920;&#29616;&#20173;&#26377;&#24046;&#36317;&#65292;&#20294;&#32771;&#34385;&#21040; &#23454;&#39564;&#26159;&#22312;&#38646;-shot&#22330;&#26223;&#19979;&#36827;&#34892;&#30340;&#65292;ChatGPT &#30340;&#34920;&#29616;&#20173;&#28982;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312; ADVETA&#65288;RPL&#65289;&#22330;&#26223;&#20013;&#65292;&#21363;&#20351;&#26159;&#38646;-shot ChatGPT &#22312; Spider &#25968;&#25454;&#38598;&#19978;&#20173;&#28982;&#20248;&#20110;&#38656;&#35201;&#24494;&#35843;&#30340; SOTA &#27169;&#22411;&#65292;&#34920;&#29616;&#25552;&#21319;&#20102;4.1\%&#65292;&#23637;&#31034;&#20102;&#23427;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#25903;&#25345;&#30456;&#20851;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#65292;&#25105;&#20204;&#24050;&#32463;&#20844;&#24320; ChatGPT &#29983;&#25104;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the first comprehensive analysis of ChatGPT's Text-to-SQL ability. Given the recent emergence of large-scale conversational language model ChatGPT and its impressive capabilities in both conversational abilities and code generation, we sought to evaluate its Text-to-SQL performance. We conducted experiments on 12 benchmark datasets with different languages, settings, or scenarios, and the results demonstrate that ChatGPT has strong text-to-SQL abilities. Although there is still a gap from the current state-of-the-art (SOTA) model performance, considering that the experiment was conducted in a zero-shot scenario, ChatGPT's performance is still impressive. Notably, in the ADVETA (RPL) scenario, the zero-shot ChatGPT even outperforms the SOTA model that requires fine-tuning on the Spider dataset by 4.1\%, demonstrating its potential for use in practical applications. To support further research in related fields, we have made the data generated by ChatGPT publicly avai
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;GETT-QA&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#20351;&#29992;T5&#23545;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#29983;&#25104;&#31616;&#21270;&#30340;SPARQL&#26597;&#35810;&#65292;&#24182;&#20351;&#29992;&#25130;&#26029;&#30340;KG&#23884;&#20837;&#25552;&#39640;&#20102;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.13284</link><description>&lt;p&gt;
GETT-QA&#65306;&#22522;&#20110;&#22270;&#23884;&#20837;&#30340;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#20013;&#30340;T2T Transformer
&lt;/p&gt;
&lt;p&gt;
GETT-QA: Graph Embedding based T2T Transformer for Knowledge Graph Question Answering. (arXiv:2303.13284v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;GETT-QA&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#20351;&#29992;T5&#23545;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#29983;&#25104;&#31616;&#21270;&#30340;SPARQL&#26597;&#35810;&#65292;&#24182;&#20351;&#29992;&#25130;&#26029;&#30340;KG&#23884;&#20837;&#25552;&#39640;&#20102;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GETT-QA&#30340;&#31471;&#21040;&#31471;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#31995;&#32479;&#12290;GETT-QA&#20351;&#29992;&#20102;T5&#65292;&#36825;&#26159;&#19968;&#31181;&#28909;&#38376;&#30340;&#25991;&#26412;&#21040;&#25991;&#26412;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#20197;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#30340;&#38382;&#39064;&#20316;&#20026;&#36755;&#20837;&#24182;&#29983;&#25104;&#25152;&#38656;SPARQL&#26597;&#35810;&#30340;&#31616;&#21270;&#24418;&#24335;&#12290;&#22312;&#31616;&#21270;&#24418;&#24335;&#20013;&#65292;&#27169;&#22411;&#19981;&#30452;&#25509;&#29983;&#25104;&#23454;&#20307;&#21644;&#20851;&#31995;ID&#65292;&#32780;&#26159;&#20135;&#29983;&#30456;&#24212;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#26631;&#31614;&#12290;&#26631;&#31614;&#22312;&#38543;&#21518;&#30340;&#27493;&#39588;&#20013;&#19982;KG&#23454;&#20307;&#21644;&#20851;&#31995;ID&#32852;&#31995;&#36215;&#26469;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25913;&#36827;&#32467;&#26524;&#65292;&#25105;&#20204;&#25351;&#23548;&#27169;&#22411;&#20026;&#27599;&#20010;&#23454;&#20307;&#29983;&#25104;KG&#23884;&#20837;&#30340;&#25130;&#26029;&#29256;&#26412;&#12290;&#25130;&#26029;&#30340;KG&#23884;&#20837;&#20351;&#24471;&#26356;&#31934;&#32454;&#30340;&#25628;&#32034;&#20174;&#32780;&#26356;&#26377;&#25928;&#36827;&#34892;&#28040;&#27495;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;T5&#33021;&#22815;&#22312;&#19981;&#25913;&#21464;&#25439;&#22833;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#25130;&#26029;&#30340;KG&#23884;&#20837;&#65292;&#25552;&#39640;&#20102;KGQA&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;Wikidata&#30340;LC-QuAD 2.0&#21644;SimpleQuestions-Wikidata&#25968;&#25454;&#38598;&#19978;&#25253;&#21578;&#20102;&#31471;&#21040;&#31471;KGQA&#30340;&#24378;&#22823;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we present an end-to-end Knowledge Graph Question Answering (KGQA) system named GETT-QA. GETT-QA uses T5, a popular text-to-text pre-trained language model. The model takes a question in natural language as input and produces a simpler form of the intended SPARQL query. In the simpler form, the model does not directly produce entity and relation IDs. Instead, it produces corresponding entity and relation labels. The labels are grounded to KG entity and relation IDs in a subsequent step. To further improve the results, we instruct the model to produce a truncated version of the KG embedding for each entity. The truncated KG embedding enables a finer search for disambiguation purposes. We find that T5 is able to learn the truncated KG embeddings without any change of loss function, improving KGQA performance. As a result, we report strong results for LC-QuAD 2.0 and SimpleQuestions-Wikidata datasets on end-to-end KGQA over Wikidata.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#21475;&#35821;&#29702;&#35299;&#20013;&#22635;&#20805;&#35821;&#30340;&#30740;&#31350;&#35270;&#35282;&#65292;&#21253;&#25324;&#24515;&#29702;&#35821;&#35328;&#23398;&#29702;&#35770;&#12289;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#21644;SLU&#31995;&#32479;&#20013;&#30340;&#27880;&#37322;&#19982;&#32771;&#34385;&#12289;&#20197;&#21450;&#29983;&#25104;&#35282;&#24230;&#30340;&#30740;&#31350;&#31561;&#65292;&#24182;&#25506;&#35752;&#20102;&#27599;&#20010;&#39046;&#22495;&#30340;&#36235;&#21183;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2301.10761</link><description>&lt;p&gt;
&#21475;&#35821;&#29702;&#35299;&#20013;&#30340;&#22635;&#20805;&#35821;&#65306;&#35745;&#31639;&#21644;&#24515;&#29702;&#35821;&#35328;&#23398;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Fillers in Spoken Language Understanding: Computational and Psycholinguistic Perspectives. (arXiv:2301.10761v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#21475;&#35821;&#29702;&#35299;&#20013;&#22635;&#20805;&#35821;&#30340;&#30740;&#31350;&#35270;&#35282;&#65292;&#21253;&#25324;&#24515;&#29702;&#35821;&#35328;&#23398;&#29702;&#35770;&#12289;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#21644;SLU&#31995;&#32479;&#20013;&#30340;&#27880;&#37322;&#19982;&#32771;&#34385;&#12289;&#20197;&#21450;&#29983;&#25104;&#35282;&#24230;&#30340;&#30740;&#31350;&#31561;&#65292;&#24182;&#25506;&#35752;&#20102;&#27599;&#20010;&#39046;&#22495;&#30340;&#36235;&#21183;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#35805;&#35821;&#20013;&#30340;&#20572;&#39039;&#25171;&#26029;&#65288;&#21363;&#35828;&#35805;&#27969;&#30021;&#24230;&#19981;&#36830;&#32493;&#65289;&#26159;&#21475;&#22836;&#34920;&#36798;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#29616;&#35937;&#12290;&#30456;&#27604;&#20854;&#20182;&#31181;&#31867;&#30340;&#20572;&#39039;&#65292;&#22635;&#20805;&#35821;&#65288;&#8220;&#21999;&#8221;&#12289;&#8220;&#21834;&#8221;&#65289;&#26159;&#26368;&#24120;&#35265;&#30340;&#12290;&#28982;&#32780;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36824;&#27809;&#26377;&#19968;&#31181;&#36164;&#28304;&#23558;&#24433;&#21709;&#21475;&#35821;&#29702;&#35299;&#65288;SLU&#65289;&#23545;&#36825;&#20123;&#35821;&#38899;&#20107;&#20214;&#30340;&#30740;&#31350;&#35270;&#35282;&#27719;&#32858;&#22312;&#19968;&#36215;&#12290;&#26412;&#25991;&#30340;&#30446;&#30340;&#26159;&#20197;&#20840;&#38754;&#30340;&#26041;&#24335;&#35843;&#26597;&#21508;&#31181;&#35270;&#35282;&#65292;&#20174;&#32771;&#34385;&#22522;&#30784;&#30340;&#65288;&#24515;&#29702;&#65289;&#35821;&#35328;&#23398;&#29702;&#35770;&#24320;&#22987;&#65292;&#21040;&#23427;&#20204;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#21644;SLU&#31995;&#32479;&#20013;&#30340;&#27880;&#37322;&#21644;&#32771;&#34385;&#65292;&#26368;&#21518;&#20174;&#29983;&#25104;&#35282;&#24230;&#30740;&#31350;&#23427;&#20204;&#12290;&#26412;&#25991;&#26088;&#22312;&#20197;&#21487;&#36798;&#21040;&#30340;&#26041;&#24335;&#21521;SLU&#21644;&#20250;&#35805;AI&#31038;&#21306;&#23637;&#31034;&#36825;&#20123;&#35270;&#35282;&#65292;&#24182;&#35752;&#35770;&#21069;&#36827;&#26102;&#27599;&#20010;&#39046;&#22495;&#30340;&#36235;&#21183;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Disfluencies (i.e. interruptions in the regular flow of speech), are ubiquitous to spoken discourse. Fillers ("uh", "um") are disfluencies that occur the most frequently compared to other kinds of disfluencies. Yet, to the best of our knowledge, there isn't a resource that brings together the research perspectives influencing Spoken Language Understanding (SLU) on these speech events. This aim of this article is to survey a breadth of perspectives in a holistic way; i.e. from considering underlying (psycho)linguistic theory, to their annotation and consideration in Automatic Speech Recognition (ASR) and SLU systems, to lastly, their study from a generation standpoint. This article aims to present the perspectives in an approachable way to the SLU and Conversational AI community, and discuss moving forward, what we believe are the trends and challenges in each area.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21327;&#20316;&#22495;&#21069;&#32512;&#35843;&#25972;&#30340;&#36328;&#39046;&#22495;&#23454;&#20307;&#35782;&#21035;&#65292;&#20351;&#29992;&#25991;&#26412;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#25903;&#25745;&#39046;&#22495;&#30456;&#20851;&#25351;&#23548;&#26469;&#23558;&#30693;&#35782;&#36716;&#31227;&#33267;&#26032;&#22495;NER&#20219;&#21153;&#65292;&#36991;&#20813;&#20102;&#20808;&#21069;&#30340;&#20026;&#27599;&#20010;&#39046;&#22495;&#32467;&#26463;&#19968;&#20010;&#20840;&#26032;&#30340;NER&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.10410</link><description>&lt;p&gt;
&#36866;&#29992;&#20110;&#25152;&#26377;&#39046;&#22495;&#30340;&#19968;&#20010;&#27169;&#22411;&#65306;&#22522;&#20110;&#21327;&#20316;&#22495;&#21069;&#32512;&#35843;&#25972;&#30340;&#36328;&#39046;&#22495;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
One Model for All Domains: Collaborative Domain-Prefix Tuning for Cross-Domain NER. (arXiv:2301.10410v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21327;&#20316;&#22495;&#21069;&#32512;&#35843;&#25972;&#30340;&#36328;&#39046;&#22495;&#23454;&#20307;&#35782;&#21035;&#65292;&#20351;&#29992;&#25991;&#26412;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#25903;&#25745;&#39046;&#22495;&#30456;&#20851;&#25351;&#23548;&#26469;&#23558;&#30693;&#35782;&#36716;&#31227;&#33267;&#26032;&#22495;NER&#20219;&#21153;&#65292;&#36991;&#20813;&#20102;&#20808;&#21069;&#30340;&#20026;&#27599;&#20010;&#39046;&#22495;&#32467;&#26463;&#19968;&#20010;&#20840;&#26032;&#30340;NER&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#23454;&#38469;&#22330;&#26223;&#20013;&#20302;&#36164;&#28304;&#38382;&#39064;&#26159;&#36328;&#39046;&#22495;&#23454;&#20307;&#35782;&#21035;&#30340;&#19968;&#20010;&#25361;&#25112;&#24615;&#20219;&#21153;&#12290;&#20808;&#21069;&#20856;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#20027;&#35201;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#20016;&#23500;&#36164;&#28304;&#39046;&#22495;&#30340;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#33719;&#24471;NER&#27169;&#22411;&#24182;&#23558;&#20854;&#36866;&#24212;&#20110;&#30446;&#26631;&#39046;&#22495;&#12290;&#30001;&#20110;&#19981;&#21516;&#39046;&#22495;&#23454;&#20307;&#31867;&#22411;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#35843;&#25972;&#25152;&#26377;PLMs&#30340;&#21442;&#25968;&#65292;&#20174;&#32780;&#20026;&#27599;&#20010;&#39046;&#22495;&#32467;&#26463;&#19968;&#20010;&#20840;&#26032;&#30340;NER&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#30340;&#27169;&#22411;&#21482;&#20851;&#27880;&#20110;&#21033;&#29992;&#19968;&#20010;&#26222;&#36890;&#26469;&#28304;&#39046;&#22495;&#20013;&#30340;&#30693;&#35782;&#65292;&#32780;&#26410;&#33021;&#25104;&#21151;&#22320;&#23558;&#26469;&#33258;&#22810;&#20010;&#26469;&#28304;&#39046;&#22495;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#30446;&#26631;&#19978;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#22522;&#20110;&#25991;&#26412;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;PLM&#24341;&#20837;&#20102;&#21327;&#20316;&#22495;&#21069;&#32512;&#35843;&#25972;&#36328;&#39046;&#22495;NER(CP-NER)&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21576;&#29616;&#20102;&#29992;&#20110;&#25991;&#26412;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#25903;&#25745;&#39046;&#22495;&#30456;&#20851;&#25351;&#23548;&#26469;&#23558;&#30693;&#35782;&#36716;&#31227;&#33267;&#26032;&#22495;NER&#20219;&#21153;&#32780;&#26080;&#38656;&#32467;&#26500;&#20462;&#25913;&#12290;&#25105;&#20204;&#21033;&#29992;&#20923;&#32467;&#30340;PLMs&#24182;&#36827;&#34892;&#21327;&#20316;&#22495;&#21069;&#32512;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-domain NER is a challenging task to address the low-resource problem in practical scenarios. Previous typical solutions mainly obtain a NER model by pre-trained language models (PLMs) with data from a rich-resource domain and adapt it to the target domain. Owing to the mismatch issue among entity types in different domains, previous approaches normally tune all parameters of PLMs, ending up with an entirely new NER model for each domain. Moreover, current models only focus on leveraging knowledge in one general source domain while failing to successfully transfer knowledge from multiple sources to the target. To address these issues, we introduce Collaborative Domain-Prefix Tuning for cross-domain NER (CP-NER) based on text-to-text generative PLMs. Specifically, we present text-to-text generation grounding domain-related instructors to transfer knowledge to new domain NER tasks without structural modifications. We utilize frozen PLMs and conduct collaborative domain-prefix tuning
&lt;/p&gt;</description></item><item><title>GPT-3&#22312;&#35768;&#22810;&#31867;&#27604;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#19982;&#29978;&#33267;&#36229;&#36234;&#20154;&#31867;&#30340;&#33021;&#21147;&#65292;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32039;&#24613;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.09196</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31867;&#27604;&#25512;&#29702;&#30340;&#32039;&#24613;&#24615;
&lt;/p&gt;
&lt;p&gt;
Emergent Analogical Reasoning in Large Language Models. (arXiv:2212.09196v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09196
&lt;/p&gt;
&lt;p&gt;
GPT-3&#22312;&#35768;&#22810;&#31867;&#27604;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#19982;&#29978;&#33267;&#36229;&#36234;&#20154;&#31867;&#30340;&#33021;&#21147;&#65292;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32039;&#24613;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#37325;&#26032;&#28857;&#29123;&#20102;&#20154;&#20204;&#23545;&#20110;&#36825;&#26679;&#19968;&#31181;&#38382;&#39064;&#30340;&#36777;&#35770;&#65306;&#36275;&#22815;&#30340;&#35757;&#32451;&#25968;&#25454;&#26159;&#21542;&#33021;&#20351;&#36825;&#20123;&#36890;&#29992;&#27169;&#22411;&#20869;&#28085;&#20154;&#31867;&#35748;&#30693;&#33021;&#21147;&#12290;&#29305;&#21035;&#30340;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#25512;&#29702;&#33021;&#21147;&#8212;&#8212;&#19981;&#32463;&#36807;&#20219;&#20309;&#30452;&#25509;&#35757;&#32451;&#65292;&#23601;&#33021;&#22815;&#25512;&#29702;&#20986;&#26032;&#38382;&#39064;&#65292;&#29305;&#21035;&#20196;&#20154;&#20851;&#27880;&#12290;&#22312;&#20154;&#31867;&#35748;&#30693;&#20013;&#65292;&#36825;&#31181;&#33021;&#21147;&#19982;&#19968;&#31181;&#36890;&#36807;&#31867;&#27604;&#25512;&#29702;&#30340;&#33021;&#21147;&#23494;&#20999;&#30456;&#20851;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#31867;&#27604;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#30452;&#25509;&#30340;&#20154;&#26426;&#27604;&#36739;&#65292;&#21253;&#25324;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#30697;&#38453;&#25512;&#29702;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#19982; Raven's Progressive Matrices&#23494;&#20999;&#30456;&#20851;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;GPT-3&#21576;&#29616;&#20986;&#20102;&#19968;&#31181;&#20196;&#20154;&#24778;&#35766;&#30340;&#25277;&#35937;&#27169;&#24335;&#24402;&#32435;&#33021;&#21147;&#65292;&#29978;&#33267;&#22312;&#22823;&#37096;&#20998;&#24773;&#20917;&#19979;&#19982;&#25110;&#29978;&#33267;&#36229;&#36234;&#20102;&#20154;&#31867;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20687;GPT-3&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#33719;&#24471;&#20102;&#22312;&#24191;&#27867;&#30340;&#31867;&#27604;&#38382;&#39064;&#19978;&#25214;&#21040;&#38646;&#26679;&#26412;&#35299;&#20915;&#26041;&#26696;&#30340;&#32039;&#24613;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advent of large language models has reinvigorated debate over whether human cognitive capacities might emerge in such generic models given sufficient training data. Of particular interest is the ability of these models to reason about novel problems zero-shot, without any direct training. In human cognition, this capacity is closely tied to an ability to reason by analogy. Here, we performed a direct comparison between human reasoners and a large language model (the text-davinci-003 variant of GPT-3) on a range of analogical tasks, including a novel text-based matrix reasoning task closely modeled on Raven's Progressive Matrices. We found that GPT-3 displayed a surprisingly strong capacity for abstract pattern induction, matching or even surpassing human capabilities in most settings. Our results indicate that large language models such as GPT-3 have acquired an emergent ability to find zero-shot solutions to a broad range of analogy problems.
&lt;/p&gt;</description></item><item><title>POTATO&#26159;&#19968;&#20010;&#20813;&#36153;&#12289;&#24320;&#28304;&#30340;&#20415;&#25658;&#24335;&#25991;&#26412;&#27880;&#37322;&#24037;&#20855;&#65292;&#25903;&#25345;&#22810;&#31181;&#31867;&#22411;&#30340;&#25991;&#26412;&#21644;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#26631;&#27880;&#65292;&#25552;&#20379;&#26131;&#20110;&#37197;&#32622;&#30340;&#21151;&#33021;&#20197;&#26368;&#22823;&#21270;&#29983;&#20135;&#21147;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#38271;&#25991;&#26723;&#21644;&#22797;&#26434;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2212.08620</link><description>&lt;p&gt;
POTATO: &#20415;&#25658;&#24335;&#25991;&#26412;&#27880;&#37322;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
POTATO: The Portable Text Annotation Tool. (arXiv:2212.08620v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08620
&lt;/p&gt;
&lt;p&gt;
POTATO&#26159;&#19968;&#20010;&#20813;&#36153;&#12289;&#24320;&#28304;&#30340;&#20415;&#25658;&#24335;&#25991;&#26412;&#27880;&#37322;&#24037;&#20855;&#65292;&#25903;&#25345;&#22810;&#31181;&#31867;&#22411;&#30340;&#25991;&#26412;&#21644;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#26631;&#27880;&#65292;&#25552;&#20379;&#26131;&#20110;&#37197;&#32622;&#30340;&#21151;&#33021;&#20197;&#26368;&#22823;&#21270;&#29983;&#20135;&#21147;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#38271;&#25991;&#26723;&#21644;&#22797;&#26434;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;POTATO&#65292;&#21363;&#20415;&#25658;&#24335;&#25991;&#26412;&#27880;&#37322;&#24037;&#20855;&#65292;&#36825;&#26159;&#19968;&#20010;&#23436;&#20840;&#20813;&#36153;&#12289;&#24320;&#28304;&#30340;&#27880;&#37322;&#31995;&#32479;&#65292;&#25903;&#25345;&#26631;&#27880;&#22810;&#31181;&#31867;&#22411;&#30340;&#25991;&#26412;&#21644;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#25552;&#20379;&#26131;&#20110;&#37197;&#32622;&#30340;&#21151;&#33021;&#20197;&#26368;&#22823;&#21270;&#37096;&#32626;&#21644;&#27880;&#37322;&#32773;&#30340;&#29983;&#20135;&#21147;&#65288;&#20415;&#25463;&#30340;&#26426;&#22120;&#23398;&#20064;/&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#27169;&#26495;&#12289;&#20027;&#21160;&#23398;&#20064;&#12289;&#25353;&#38190;&#32553;&#20889;&#38190;&#12289;&#20851;&#38190;&#23383;&#39640;&#20142;&#12289;&#25552;&#31034;&#24037;&#20855;&#65289;&#65292;&#24182;&#25903;&#25345;&#39640;&#24230;&#23450;&#21046;&#21270;&#65288;&#21487;&#32534;&#36753;&#30340;UI&#65292;&#25554;&#20837;&#39044;&#31579;&#36873;&#38382;&#39064;&#65292;&#27880;&#24847;&#21147;&#21644;&#36164;&#26684;&#27979;&#35797;&#65289;&#12290;&#20004;&#39033;&#27880;&#37322;&#20219;&#21153;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;POTATO&#36890;&#36807;&#20854;&#29305;&#21035;&#35774;&#35745;&#30340;&#29983;&#20135;&#21147;&#21151;&#33021;&#65292;&#29305;&#21035;&#26159;&#38271;&#25991;&#26723;&#21644;&#22797;&#26434;&#20219;&#21153;&#65292;&#25552;&#39640;&#20102;&#26631;&#27880;&#36895;&#24230;&#12290;POTATO&#21487;&#22312; https://github.com/davidjurgens/potato &#19978;&#33719;&#21462;&#65292;&#24182;&#23558;&#32487;&#32493;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present POTATO, the Portable text annotation tool, a free, fully open-sourced annotation system that 1) supports labeling many types of text and multimodal data; 2) offers easy-to-configure features to maximize the productivity of both deployers and annotators (convenient templates for common ML/NLP tasks, active learning, keypress shortcuts, keyword highlights, tooltips); and 3) supports a high degree of customization (editable UI, inserting pre-screening questions, attention and qualification tests). Experiments over two annotation tasks suggest that POTATO improves labeling speed through its specially-designed productivity features, especially for long documents and complex tasks. POTATO is available at https://github.com/davidjurgens/potato and will continue to be updated.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#32467;&#26500;&#21270;&#30693;&#35782;&#22686;&#24378;&#30340;&#25925;&#20107;&#29983;&#25104;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#24635;&#32467;&#20102;&#30446;&#21069;&#30340;&#26041;&#27861;&#19982;&#25216;&#26415;&#65292;&#25351;&#20986;&#20102;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#21644;&#23578;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2212.04634</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#30693;&#35782;&#22686;&#24378;&#30340;&#24320;&#25918;&#19990;&#30028;&#25925;&#20107;&#29983;&#25104;&#65306;&#19968;&#39033;&#20840;&#38754;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Open-world Story Generation with Structured Knowledge Enhancement: A Comprehensive Survey. (arXiv:2212.04634v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#32467;&#26500;&#21270;&#30693;&#35782;&#22686;&#24378;&#30340;&#25925;&#20107;&#29983;&#25104;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#24635;&#32467;&#20102;&#30446;&#21069;&#30340;&#26041;&#27861;&#19982;&#25216;&#26415;&#65292;&#25351;&#20986;&#20102;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#21644;&#23578;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35762;&#25925;&#20107;&#21644;&#21465;&#20107;&#26159;&#20154;&#31867;&#20307;&#39564;&#30340;&#22522;&#30784;&#65292;&#19982;&#25105;&#20204;&#30340;&#31038;&#20250;&#21644;&#25991;&#21270;&#21442;&#19982;&#23494;&#19981;&#21487;&#20998;&#12290;&#22240;&#27492;&#65292;&#38271;&#26399;&#20197;&#26469;&#65292;&#30740;&#31350;&#20154;&#21592;&#19968;&#30452;&#23581;&#35797;&#21019;&#24314;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#25925;&#20107;&#30340;&#31995;&#32479;&#12290;&#36817;&#24180;&#26469;&#65292;&#21463;&#28145;&#24230;&#23398;&#20064;&#21644;&#22823;&#35268;&#27169;&#25968;&#25454;&#36164;&#28304;&#30340;&#25512;&#21160;&#65292;&#33258;&#21160;&#29983;&#25104;&#25925;&#20107;&#24050;&#32463;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#37325;&#22823;&#25361;&#25112;&#65292;&#20363;&#22914;&#65292;&#38656;&#35201;&#22312;&#29983;&#25104;&#30340;&#25925;&#20107;&#20013;&#23454;&#29616;&#20840;&#23616;&#19968;&#33268;&#24615;&#65292;&#36825;&#20351;&#24471;&#29983;&#25104;&#27169;&#22411;&#26080;&#27861;&#36798;&#21040;&#19982;&#20154;&#31867;&#21465;&#36848;&#32773;&#30456;&#21516;&#30340;&#21465;&#20107;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#35768;&#22810;&#30740;&#31350;&#35797;&#22270;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#27880;&#20837;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#36825;&#34987;&#31216;&#20026;&#32467;&#26500;&#21270;&#30693;&#35782;&#22686;&#24378;&#30340;&#25925;&#20107;&#29983;&#25104;&#12290;&#23558;&#22806;&#37096;&#30693;&#35782;&#32435;&#20837;&#20854;&#20013;&#21487;&#20197;&#22686;&#24378;&#25925;&#20107;&#20107;&#20214;&#20043;&#38388;&#30340;&#36923;&#36753;&#36830;&#36143;&#24615;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#30693;&#35782;&#22522;&#30784;&#65292;&#24182;&#20943;&#36731;&#25925;&#20107;&#20013;&#36807;&#24230;&#27010;&#25324;&#21644;&#37325;&#22797;&#38382;&#39064;&#12290;&#26412;&#27425;&#35843;&#26597;&#25552;&#20379;&#20102;&#23545;&#35813;&#30740;&#31350;&#39046;&#22495;&#30340;&#26368;&#26032;&#21644;&#20840;&#38754;&#30340;&#22238;&#39038;&#65306;&#65288;i&#65289;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#32467;&#26500;&#21270;&#30693;&#35782;&#22686;&#24378;&#25925;&#20107;&#29983;&#25104;&#30340;&#32508;&#36848;&#65292;&#65288;ii&#65289;&#25105;&#20204;&#24635;&#32467;&#20102;&#30446;&#21069;&#30340;&#26041;&#27861;&#19982;&#25216;&#26415;&#65292;&#65288;iii&#65289;&#25105;&#20204;&#25351;&#20986;&#20102;&#23578;&#24453;&#35299;&#20915;&#30340;&#38382;&#39064;&#19982;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Storytelling and narrative are fundamental to human experience, intertwined with our social and cultural engagement. As such, researchers have long attempted to create systems that can generate stories automatically. In recent years, powered by deep learning and massive data resources, automatic story generation has shown significant advances. However, considerable challenges, like the need for global coherence in generated stories, still hamper generative models from reaching the same storytelling ability as human narrators. To tackle these challenges, many studies seek to inject structured knowledge into the generation process, which is referred to as structured knowledge-enhanced story generation. Incorporating external knowledge can enhance the logical coherence among story events, achieve better knowledge grounding, and alleviate over-generalization and repetition problems in stories. This survey provides the latest and comprehensive review of this research field: (i) we present a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#36807;&#31243; FiLex &#20316;&#20026;&#28145;&#24230;&#23398;&#20064;&#26032;&#20852;&#35821;&#35328;&#31995;&#32479;&#20013;&#35789;&#27719;&#29109;&#30340;&#25968;&#23398;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#27491;&#30830;&#39044;&#27979;&#36229;&#21442;&#25968;&#19982;&#26032;&#20852;&#35821;&#35328;&#29109;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#20026;&#30740;&#31350;&#26032;&#20852;&#35821;&#35328;&#24102;&#26469;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.15783</link><description>&lt;p&gt;
&#25968;&#23398;&#24314;&#27169;&#28145;&#24230;&#23398;&#20064;&#26032;&#20852;&#35821;&#35328;&#31995;&#32479;&#30340;&#35789;&#27719;&#29109;
&lt;/p&gt;
&lt;p&gt;
Mathematically Modeling the Lexicon Entropy of Emergent Language. (arXiv:2211.15783v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15783
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#36807;&#31243; FiLex &#20316;&#20026;&#28145;&#24230;&#23398;&#20064;&#26032;&#20852;&#35821;&#35328;&#31995;&#32479;&#20013;&#35789;&#27719;&#29109;&#30340;&#25968;&#23398;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#27491;&#30830;&#39044;&#27979;&#36229;&#21442;&#25968;&#19982;&#26032;&#20852;&#35821;&#35328;&#29109;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#20026;&#30740;&#31350;&#26032;&#20852;&#35821;&#35328;&#24102;&#26469;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#36807;&#31243; FiLex &#20316;&#20026;&#28145;&#24230;&#23398;&#20064;&#26032;&#20852;&#35821;&#35328;&#31995;&#32479;&#20013;&#35789;&#27719;&#29109;&#30340;&#25968;&#23398;&#27169;&#22411;&#12290;&#36890;&#36807;&#23558;&#27169;&#22411;&#36827;&#34892;&#25968;&#23398;&#23450;&#20041;&#65292;&#21487;&#20197;&#20135;&#29983;&#28165;&#26224;&#30340;&#39044;&#27979;&#24182;&#30452;&#25509;&#20104;&#20197;&#39564;&#35777;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#19981;&#21516;&#29615;&#22659;&#19979;&#36827;&#34892;&#20102;&#39564;&#35777;&#27979;&#35797;&#65292;&#35777;&#26126; FiLex &#21487;&#20197;&#20197;20&#20010;&#32452;&#21512;&#20013;&#30340;20&#20010;&#32452;&#21512;&#27491;&#30830;&#39044;&#27979;&#36229;&#21442;&#25968;(&#35757;&#32451;&#27493;&#25968;&#12289;&#35789;&#27719;&#24211;&#22823;&#23567;&#12289;&#23398;&#20064;&#29575;&#12289;&#22238;&#28335;&#32531;&#23384;&#22823;&#23567;&#21644; Gumbel-Softmax&#28201;&#24230;)&#19982;&#26032;&#20852;&#35821;&#35328;&#29109;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#19981;&#21516;&#29615;&#22659;&#20043;&#38388;&#30340;&#36229;&#21442;&#25968;&#19982;&#29109;&#20043;&#38388;&#30340;&#20851;&#31995;&#26159;&#21508;&#24322;&#30340;&#65292;&#36825;&#34920;&#26126;&#38656;&#35201;&#19968;&#31181;&#21487;&#20197;&#22312;&#31934;&#30830;&#30340;&#31890;&#24230;&#27700;&#24179;&#19978;&#36827;&#34892;&#26126;&#30830;&#23450;&#20041;&#30340;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
We formulate a stochastic process, FiLex, as a mathematical model of lexicon entropy in deep learning-based emergent language systems. Defining a model mathematically allows it to generate clear predictions which can be directly and decisively tested. We empirically verify across four different environments that FiLex predicts the correct correlation between hyperparameters (training steps, lexicon size, learning rate, rollout buffer size, and Gumbel-Softmax temperature) and the emergent language's entropy in 20 out of 20 environment-hyperparameter combinations. Furthermore, our experiments reveal that different environments show diverse relationships between their hyperparameters and entropy which demonstrates the need for a model which can make well-defined predictions at a precise level of granularity.
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#32534;&#30721;&#32452;&#21512;&#20449;&#24687;&#26041;&#38754;&#30340;&#34920;&#29616;&#23384;&#22312;&#38382;&#39064;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;Attribution&#12289;Relation&#21644;Order&#65288;ARO&#65289;&#22522;&#20934;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;VLMs&#36827;&#34892;Attention&#26426;&#21046;&#21644;&#23545;&#25239;&#35757;&#32451;&#31561;&#20462;&#25913;&#20197;&#25552;&#39640;&#20854;&#32452;&#21512;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2210.01936</link><description>&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20309;&#26102;&#20197;&#21450;&#20026;&#20309;&#34892;&#20026;&#20687;&#35789;&#34955;&#65292;&#20197;&#21450;&#22914;&#20309;&#35299;&#20915;&#65311;
&lt;/p&gt;
&lt;p&gt;
When and why vision-language models behave like bags-of-words, and what to do about it?. (arXiv:2210.01936v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01936
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#32534;&#30721;&#32452;&#21512;&#20449;&#24687;&#26041;&#38754;&#30340;&#34920;&#29616;&#23384;&#22312;&#38382;&#39064;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;Attribution&#12289;Relation&#21644;Order&#65288;ARO&#65289;&#22522;&#20934;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;VLMs&#36827;&#34892;Attention&#26426;&#21046;&#21644;&#23545;&#25239;&#35757;&#32451;&#31561;&#20462;&#25913;&#20197;&#25552;&#39640;&#20854;&#32452;&#21512;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#22312;&#35768;&#22810;&#19979;&#28216;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#32534;&#30721;&#32452;&#21512;&#20449;&#24687;&#30340;&#33021;&#21147;&#23578;&#19981;&#28165;&#26970;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;Attribution&#12289;Relation&#21644;Order&#65288;ARO&#65289;&#22522;&#20934;&#26469;&#31995;&#32479;&#35780;&#20272;VLM&#29702;&#35299;&#19981;&#21516;&#31867;&#22411;&#20851;&#31995;&#12289;&#23646;&#24615;&#21644;&#39034;&#24207;&#30340;&#33021;&#21147;&#12290;ARO&#30001;Visual Genome Attribution&#27979;&#35797;&#23545;&#35937;&#23646;&#24615;&#30340;&#29702;&#35299;&#33021;&#21147;&#65307;Visual Genome Relation&#27979;&#35797;&#20851;&#31995;&#29702;&#35299;&#33021;&#21147;&#65307;&#20197;&#21450;COCO&#65286;Flickr30k-Order&#27979;&#35797;&#39034;&#24207;&#25935;&#24863;&#24615;&#12290;ARO&#27604;&#20197;&#21069;&#30340;&#32452;&#21512;&#24615;&#22522;&#20934;&#22823;&#22810;&#20010;&#25968;&#37327;&#32423;&#65292;&#21253;&#25324;50,000&#22810;&#20010;&#27979;&#35797;&#29992;&#20363;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;VLM&#22312;&#21738;&#20123;&#26041;&#38754;&#23384;&#22312;&#20851;&#31995;&#29702;&#35299;&#38382;&#39064;&#65292;&#24403;&#38142;&#25509;&#23545;&#35937;&#21644;&#23646;&#24615;&#26102;&#23481;&#26131;&#20986;&#38169;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#26126;&#26174;&#30340;&#32570;&#20047;&#39034;&#24207;&#25935;&#24863;&#24615;&#12290;VLM&#20027;&#35201;&#22312;&#20855;&#26377;&#20016;&#23500;&#32452;&#21512;&#32467;&#26500;&#30340;&#22270;&#20687;&#21644;&#26631;&#39064;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#20165;&#20165;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#24182;&#19981;&#33021;&#20445;&#35777;&#22312;&#38656;&#35201;&#32452;&#21512;&#29702;&#35299;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#20462;&#25913;VLMs&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#24378;&#35843;&#32452;&#25104;&#20851;&#31995;&#30340;&#27880;&#24847;&#26426;&#21046;&#21644;&#20351;&#29992;&#23545;&#25239;&#35757;&#32451;&#26469;&#25913;&#21892;&#23646;&#24615;&#39044;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#20462;&#25913;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;VLM&#22312;ARO&#22522;&#20934;&#19978;&#30340;&#32452;&#21512;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the success of large vision and language models (VLMs) in many downstream applications, it is unclear how well they encode compositional information. Here, we create the Attribution, Relation, and Order (ARO) benchmark to systematically evaluate the ability of VLMs to understand different types of relationships, attributes, and order. ARO consists of Visual Genome Attribution, to test the understanding of objects' properties; Visual Genome Relation, to test for relational understanding; and COCO &amp; Flickr30k-Order, to test for order sensitivity. ARO is orders of magnitude larger than previous benchmarks of compositionality, with more than 50,000 test cases. We show where state-of-the-art VLMs have poor relational understanding, can blunder when linking objects to their attributes, and demonstrate a severe lack of order sensitivity. VLMs are predominantly trained and evaluated on large datasets with rich compositional structure in the images and captions. Yet, training on these d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#22240;&#26524;&#26694;&#26550;&#65292;&#25551;&#36848;&#20102;&#25968;&#25454;&#32479;&#35745;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#8220;&#20107;&#23454;&#24615;&#8221;&#39044;&#27979;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;PLMs&#30340;&#39044;&#27979;&#21463;&#21040;&#36825;&#20123;&#32479;&#35745;&#30340;&#24433;&#21709;&#65292;&#26263;&#31034;&#36825;&#26679;&#30340;&#27169;&#22411;&#20381;&#36182;&#20110;&#27973;&#26174;&#30340;&#21551;&#21457;&#24335;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2207.14251</link><description>&lt;p&gt;
&#27979;&#37327;&#25968;&#25454;&#32479;&#35745;&#23545;&#35821;&#35328;&#27169;&#22411;&#8220;&#20107;&#23454;&#24615;&#8221;&#39044;&#27979;&#30340;&#22240;&#26524;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Measuring Causal Effects of Data Statistics on Language Model's `Factual' Predictions. (arXiv:2207.14251v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.14251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#22240;&#26524;&#26694;&#26550;&#65292;&#25551;&#36848;&#20102;&#25968;&#25454;&#32479;&#35745;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#8220;&#20107;&#23454;&#24615;&#8221;&#39044;&#27979;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;PLMs&#30340;&#39044;&#27979;&#21463;&#21040;&#36825;&#20123;&#32479;&#35745;&#30340;&#24433;&#21709;&#65292;&#26263;&#31034;&#36825;&#26679;&#30340;&#27169;&#22411;&#20381;&#36182;&#20110;&#27973;&#26174;&#30340;&#21551;&#21457;&#24335;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#26159;&#26368;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#34920;&#29616;&#20986;&#33394;&#30340;&#20027;&#35201;&#21407;&#22240;&#20043;&#19968;&#12290;&#20294;&#26159;&#65292;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#65292;&#21040;&#24213;&#26159;&#20160;&#20040;&#23548;&#33268;&#27169;&#22411;&#20570;&#20986;&#20102;&#26576;&#20010;&#39044;&#27979;&#21602;&#65311;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#22240;&#26524;&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#31181;&#25551;&#36848;&#35757;&#32451;&#25968;&#25454;&#22914;&#20309;&#24433;&#21709;&#39044;&#27979;&#30340;&#35821;&#35328;&#26469;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#36991;&#20813;&#20102;&#37325;&#26032;&#35757;&#32451;&#26114;&#36149;&#27169;&#22411;&#30340;&#38656;&#35201;&#65292;&#20165;&#36890;&#36807;&#35266;&#23519;&#25968;&#25454;&#23601;&#21487;&#20197;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#12290;&#38024;&#23545;&#20174;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#20013;&#25552;&#21462;&#20107;&#23454;&#30693;&#35782;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#20851;&#27880;&#31616;&#21333;&#30340;&#25968;&#25454;&#32479;&#35745;&#65292;&#20363;&#22914;&#20849;&#29616;&#35745;&#25968;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#20123;&#32479;&#35745;&#23545;PLMs&#30340;&#39044;&#27979;&#20855;&#26377;&#24433;&#21709;&#65292;&#26263;&#31034;&#36825;&#26679;&#30340;&#27169;&#22411;&#20381;&#36182;&#20110;&#27973;&#26174;&#30340;&#21551;&#21457;&#24335;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#22240;&#26524;&#26694;&#26550;&#21644;&#30740;&#31350;&#32467;&#26524;&#35777;&#26126;&#20102;&#30740;&#31350;&#25968;&#25454;&#38598;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#21450;&#22240;&#26524;&#20851;&#31995;&#23545;&#20110;&#29702;&#35299;NLP&#27169;&#22411;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large amounts of training data are one of the major reasons for the high performance of state-of-the-art NLP models. But what exactly in the training data causes a model to make a certain prediction? We seek to answer this question by providing a language for describing how training data influences predictions, through a causal framework. Importantly, our framework bypasses the need to retrain expensive models and allows us to estimate causal effects based on observational data alone. Addressing the problem of extracting factual knowledge from pretrained language models (PLMs), we focus on simple data statistics such as co-occurrence counts and show that these statistics do influence the predictions of PLMs, suggesting that such models rely on shallow heuristics. Our causal framework and our results demonstrate the importance of studying datasets and the benefits of causality for understanding NLP models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35821;&#35328;&#23398;&#29702;&#35770;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#20855;&#20307;&#24773;&#20917;&#30340;&#23454;&#20363;&#65292;&#35299;&#20915;&#20102;&#27867;&#21270;&#38382;&#39064;&#20013;&#30340;&#20363;&#22806;&#21644;&#26222;&#36941;&#24615;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;650&#20010;&#27867;&#22411;&#19978;&#27604;GPT-3&#22522;&#32447;&#39640;&#20986;12.8&#20010;&#31934;&#24230;&#28857;&#12290;</title><link>http://arxiv.org/abs/2205.11658</link><description>&lt;p&gt;
&#20225;&#40517;&#19981;&#33021;&#39134;&#65306;&#36890;&#36807;&#23454;&#20363;&#21644;&#24322;&#24120;&#25512;&#29702;&#27867;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Penguins Don't Fly: Reasoning about Generics through Instantiations and Exceptions. (arXiv:2205.11658v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.11658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35821;&#35328;&#23398;&#29702;&#35770;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#20855;&#20307;&#24773;&#20917;&#30340;&#23454;&#20363;&#65292;&#35299;&#20915;&#20102;&#27867;&#21270;&#38382;&#39064;&#20013;&#30340;&#20363;&#22806;&#21644;&#26222;&#36941;&#24615;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;650&#20010;&#27867;&#22411;&#19978;&#27604;GPT-3&#22522;&#32447;&#39640;&#20986;12.8&#20010;&#31934;&#24230;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27867;&#21270;&#34920;&#36798;&#20102;&#19990;&#30028;&#19978;&#30340;&#27010;&#25324;&#65288;&#20363;&#22914;&#65292;&#40479;&#21487;&#20197;&#39134;&#32724;&#65289;&#65292;&#20294;&#24182;&#38750;&#26222;&#36941;&#36866;&#29992;&#65288;&#20363;&#22914;&#65292;&#26032;&#29983;&#20799;&#40479;&#21644;&#20225;&#40517;&#19981;&#33021;&#39134;&#32724;&#65289;&#12290;&#24120;&#35782;&#30693;&#35782;&#24211;&#36890;&#24120;&#32534;&#30721;&#19968;&#20123;&#27867;&#21270;&#30693;&#35782;&#65292;&#20294;&#24456;&#23569;&#21015;&#20030;&#36825;&#20123;&#24322;&#24120;&#12290;&#30693;&#36947;&#20309;&#26102;&#27867;&#21270;&#35821;&#21477;&#25104;&#31435;&#25110;&#19981;&#25104;&#31435;&#23545;&#20110;&#24320;&#21457;&#23545;&#27867;&#21270;&#38382;&#39064;&#30340;&#20840;&#38754;&#29702;&#35299;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#22522;&#20110;&#35821;&#35328;&#23398;&#29702;&#35770;&#29983;&#25104;&#23454;&#20363;&#65292;&#21363;&#27867;&#21270;&#25104;&#31435;&#25110;&#19981;&#25104;&#31435;&#30340;&#20855;&#20307;&#24773;&#20917;&#12290;&#25105;&#20204;&#20026;&#22823;&#32422;650&#20010;&#27867;&#22411;&#29983;&#25104;&#20102;&#32422;19k&#20010;&#23454;&#20363;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#27604;&#24378;&#21170;&#30340;GPT-3&#22522;&#32447;&#39640;&#20986;12.8&#20010;&#31934;&#24230;&#28857;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#31361;&#26174;&#20102;&#22522;&#20110;&#35821;&#35328;&#23398;&#29702;&#35770;&#30340;&#21487;&#25511;&#24615;&#22312;&#29983;&#25104;&#23454;&#20363;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#30693;&#35782;&#24211;&#20316;&#20026;&#23454;&#20363;&#26469;&#28304;&#30340;&#19981;&#36275;&#20043;&#22788;&#65292;&#20197;&#21450;&#23454;&#20363;&#23545;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generics express generalizations about the world (e.g., birds can fly) that are not universally true (e.g., newborn birds and penguins cannot fly). Commonsense knowledge bases, used extensively in NLP, encode some generic knowledge but rarely enumerate such exceptions and knowing when a generic statement holds or does not hold true is crucial for developing a comprehensive understanding of generics. We present a novel framework informed by linguistic theory to generate exemplars -- specific cases when a generic holds true or false. We generate ~19k exemplars for ~650 generics and show that our framework outperforms a strong GPT-3 baseline by 12.8 precision points. Our analysis highlights the importance of linguistic theory-based controllability for generating exemplars, the insufficiency of knowledge bases as a source of exemplars, and the challenges exemplars pose for the task of natural language inference.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;DeBERTaV3&#65292;&#20351;&#29992;&#26356;&#21152;&#26679;&#26412;&#26377;&#25928;&#30340;&#26367;&#25442;&#20196;&#29260;&#26816;&#27979;&#65288;RTD&#65289;&#21462;&#20195;&#20102;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#65288;MLM&#65289;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26799;&#24230;&#21435;&#32806;&#21512;&#23884;&#20837;&#20849;&#20139;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#8220;&#25300;&#27827;&#8221;&#21160;&#24577;&#65292;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;&#22312;&#22810;&#20010;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#65292;DeBERTaV3&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2111.09543</link><description>&lt;p&gt;
DeBERTaV3&#65306;&#20351;&#29992;&#26799;&#24230;&#21435;&#32806;&#21512;&#23884;&#20837;&#20849;&#20139;&#30340;ELECTRA&#39118;&#26684;&#39044;&#35757;&#32451;&#26469;&#25913;&#36827;DeBERTa&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing. (arXiv:2111.09543v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.09543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;DeBERTaV3&#65292;&#20351;&#29992;&#26356;&#21152;&#26679;&#26412;&#26377;&#25928;&#30340;&#26367;&#25442;&#20196;&#29260;&#26816;&#27979;&#65288;RTD&#65289;&#21462;&#20195;&#20102;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#65288;MLM&#65289;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26799;&#24230;&#21435;&#32806;&#21512;&#23884;&#20837;&#20849;&#20139;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#8220;&#25300;&#27827;&#8221;&#21160;&#24577;&#65292;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;&#22312;&#22810;&#20010;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#65292;DeBERTaV3&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;DeBERTaV3&#65292;&#23427;&#36890;&#36807;&#23558;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#65288;MLM&#65289;&#26367;&#25442;&#20026;&#26356;&#21152;&#26679;&#26412;&#26377;&#25928;&#30340;&#26367;&#25442;&#20196;&#29260;&#26816;&#27979;&#65288;RTD&#65289;&#26469;&#25913;&#36827;&#21407;&#22987;&#30340;DeBERTa&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;ELECTRA&#20013;&#30340;&#39321;&#33609;&#23884;&#20837;&#20849;&#20139;&#20250;&#24433;&#21709;&#35757;&#32451;&#25928;&#29575;&#21644;&#27169;&#22411;&#24615;&#33021;&#65292;&#22240;&#20026;&#21028;&#21035;&#22120;&#21644;&#29983;&#25104;&#22120;&#30340;&#35757;&#32451;&#25439;&#22833;&#23558;&#20196;&#29260;&#23884;&#20837;&#25289;&#21521;&#19981;&#21516;&#30340;&#26041;&#21521;&#65292;&#20250;&#36896;&#25104;&#8220;&#25300;&#27827;&#8221;&#21160;&#24577;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26799;&#24230;&#21435;&#32806;&#21512;&#23884;&#20837;&#20849;&#20139;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#8220;&#25300;&#27827;&#8221;&#21160;&#24577;&#65292;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;&#25105;&#20204;&#20351;&#29992;&#19982;DeBERTa&#30456;&#21516;&#30340;&#35774;&#32622;&#39044;&#35757;&#32451;&#20102;DeBERTaV3&#65292;&#20197;&#23637;&#31034;&#20854;&#22312;&#21508;&#31181;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#20219;&#21153;&#20013;&#30340;&#20248;&#31168;&#24615;&#33021;&#12290;&#20197;&#20843;&#39033;&#20219;&#21153;&#20026;&#20363;&#30340;GLUE&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;DeBERTaV3 Large&#27169;&#22411;&#24179;&#22343;&#24471;&#20998;&#20026;91.37&#65285;&#65292;&#27604;D&#39640;1.37&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a new pre-trained language model, DeBERTaV3, which improves the original DeBERTa model by replacing mask language modeling (MLM) with replaced token detection (RTD), a more sample-efficient pre-training task. Our analysis shows that vanilla embedding sharing in ELECTRA hurts training efficiency and model performance. This is because the training losses of the discriminator and the generator pull token embeddings in different directions, creating the "tug-of-war" dynamics. We thus propose a new gradient-disentangled embedding sharing method that avoids the tug-of-war dynamics, improving both training efficiency and the quality of the pre-trained model. We have pre-trained DeBERTaV3 using the same settings as DeBERTa to demonstrate its exceptional performance on a wide range of downstream natural language understanding (NLU) tasks. Taking the GLUE benchmark with eight tasks as an example, the DeBERTaV3 Large model achieves a 91.37% average score, which is 1.37% over D
&lt;/p&gt;</description></item></channel></rss>