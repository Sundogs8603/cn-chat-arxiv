<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#25991;&#26723;&#36793;&#30028;&#30340;&#19978;&#19979;&#25991;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#30456;&#20851;&#25991;&#26723;&#24207;&#21015;&#19978;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#40723;&#21169;&#27169;&#22411;&#36827;&#34892;&#36328;&#25991;&#26723;&#30340;&#38405;&#35835;&#21644;&#25512;&#29702;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25913;&#21464;&#25991;&#26723;&#39034;&#24207;&#24182;&#24212;&#29992;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#31649;&#36947;&#26469;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.10638</link><description>&lt;p&gt;
&#36229;&#36234;&#25991;&#26723;&#36793;&#30028;&#30340;&#19978;&#19979;&#25991;&#39044;&#35757;&#32451;&#65306;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
In-Context Pretraining: Language Modeling Beyond Document Boundaries. (arXiv:2310.10638v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#25991;&#26723;&#36793;&#30028;&#30340;&#19978;&#19979;&#25991;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#30456;&#20851;&#25991;&#26723;&#24207;&#21015;&#19978;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#40723;&#21169;&#27169;&#22411;&#36827;&#34892;&#36328;&#25991;&#26723;&#30340;&#38405;&#35835;&#21644;&#25512;&#29702;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25913;&#21464;&#25991;&#26723;&#39034;&#24207;&#24182;&#24212;&#29992;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#31649;&#36947;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#36890;&#36807;&#39044;&#27979;&#32473;&#23450;&#25991;&#26723;&#21069;&#32512;&#30340;&#26631;&#35760;&#26469;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#33021;&#22815;&#30452;&#25509;&#36827;&#34892;&#38271;&#31687;&#29983;&#25104;&#21644;&#25552;&#31034;&#24335;&#20219;&#21153;&#65292;&#36825;&#21487;&#20197;&#31616;&#21270;&#20026;&#25991;&#26723;&#23436;&#25104;&#12290;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#31649;&#36947;&#36890;&#36807;&#36830;&#25509;&#38543;&#26426;&#32452;&#21512;&#30340;&#30701;&#25991;&#26723;&#26469;&#35757;&#32451;LMs&#65292;&#20197;&#21019;&#24314;&#36755;&#20837;&#19978;&#19979;&#25991;&#65292;&#20294;&#21069;&#19968;&#20010;&#25991;&#26723;&#23545;&#20110;&#39044;&#27979;&#19979;&#19968;&#20010;&#25991;&#26723;&#27809;&#26377;&#25552;&#20379;&#20219;&#20309;&#20449;&#21495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#8212;&#8212;&#19978;&#19979;&#25991;&#39044;&#35757;&#32451;&#65292;&#21363;&#22312;&#30456;&#20851;&#25991;&#26723;&#24207;&#21015;&#19978;&#39044;&#20808;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#26126;&#30830;&#40723;&#21169;&#23427;&#20204;&#36328;&#36234;&#25991;&#26723;&#36793;&#30028;&#36827;&#34892;&#38405;&#35835;&#21644;&#25512;&#29702;&#12290;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#25913;&#21464;&#25991;&#26723;&#39034;&#24207;&#65292;&#20351;&#27599;&#20010;&#19978;&#19979;&#25991;&#21253;&#21547;&#30456;&#20851;&#30340;&#25991;&#26723;&#65292;&#24182;&#30452;&#25509;&#24212;&#29992;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#31649;&#36947;&#26469;&#36827;&#34892;&#19978;&#19979;&#25991;&#39044;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#25991;&#26723;&#25490;&#24207;&#38382;&#39064;&#24456;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26377;&#25968;&#21313;&#20159;&#20010;&#25991;&#26723;&#65292;&#25105;&#20204;&#24076;&#26395;&#22312;&#27599;&#20010;&#25991;&#26723;&#20013;&#26368;&#22823;&#21270;&#19978;&#19979;&#25991;&#30456;&#20284;&#24615;&#32780;&#19981;&#37325;&#22797;&#20219;&#20309;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LMs) are currently trained to predict tokens given document prefixes, enabling them to directly perform long-form generation and prompting-style tasks which can be reduced to document completion. Existing pretraining pipelines train LMs by concatenating random sets of short documents to create input contexts but the prior documents provide no signal for predicting the next document. We instead present In-Context Pretraining, a new approach where language models are pretrained on a sequence of related documents, thereby explicitly encouraging them to read and reason across document boundaries. We can do In-Context Pretraining by simply changing the document ordering so that each context contains related documents, and directly applying existing pretraining pipelines. However, this document sorting problem is challenging. There are billions of documents and we would like the sort to maximize contextual similarity for every document without repeating any data. To do
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#32852;&#21512;&#23398;&#20064;&#26469;&#25345;&#32493;&#20174;&#29992;&#25143;&#32416;&#27491;&#20013;&#23398;&#20064;&#65292;&#20197;&#35299;&#20915;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#22240;&#20026;&#35821;&#35328;&#30340;&#21457;&#23637;&#21644;&#26032;&#35789;&#27719;&#30340;&#20986;&#29616;&#32780;&#21464;&#24471;&#36807;&#26102;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#38024;&#23545;&#26032;&#35789;&#27719;&#12289;&#38271;&#23614;&#35789;&#27719;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#31561;&#25216;&#26415;&#25552;&#39640;&#27169;&#22411;&#30340;&#35782;&#21035;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.00141</link><description>&lt;p&gt;
&#29992;&#25143;&#21453;&#39304;&#30340;&#39304;&#36192;&#65306;&#36890;&#36807;&#32852;&#21512;&#23398;&#20064;&#20174;&#29992;&#25143;&#32416;&#27491;&#20013;&#25552;&#39640;ASR&#27169;&#22411;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
The Gift of Feedback: Improving ASR Model Quality by Learning from User Corrections through Federated Learning. (arXiv:2310.00141v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#32852;&#21512;&#23398;&#20064;&#26469;&#25345;&#32493;&#20174;&#29992;&#25143;&#32416;&#27491;&#20013;&#23398;&#20064;&#65292;&#20197;&#35299;&#20915;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#22240;&#20026;&#35821;&#35328;&#30340;&#21457;&#23637;&#21644;&#26032;&#35789;&#27719;&#30340;&#20986;&#29616;&#32780;&#21464;&#24471;&#36807;&#26102;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#38024;&#23545;&#26032;&#35789;&#27719;&#12289;&#38271;&#23614;&#35789;&#27719;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#31561;&#25216;&#26415;&#25552;&#39640;&#27169;&#22411;&#30340;&#35782;&#21035;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#36890;&#24120;&#22312;&#22823;&#37327;&#30340;&#36716;&#24405;&#35821;&#38899;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#38543;&#30528;&#35821;&#35328;&#30340;&#21457;&#23637;&#21644;&#26032;&#35789;&#27719;&#30340;&#20986;&#29616;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#21464;&#24471;&#36807;&#26102;&#21644;&#38472;&#26087;&#12290;&#22312;&#22522;&#20110;&#26381;&#21153;&#22120;&#35757;&#32451;&#20294;&#37096;&#32626;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#27169;&#22411;&#20013;&#65292;&#38169;&#35823;&#21487;&#33021;&#26159;&#30001;&#20110;&#26381;&#21153;&#22120;&#35757;&#32451;&#25968;&#25454;&#19982;&#23454;&#38469;&#35774;&#22791;&#20351;&#29992;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#23548;&#33268;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#32852;&#21512;&#23398;&#20064;&#26469;&#19981;&#26029;&#20174;&#35774;&#22791;&#19978;&#30340;&#29992;&#25143;&#32416;&#27491;&#20013;&#23398;&#20064;&#65292;&#20174;&#32780;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#20123;&#25216;&#26415;&#65292;&#20197;&#38024;&#23545;&#27169;&#22411;&#20197;&#21069;&#26410;&#36935;&#21040;&#36807;&#30340;&#26032;&#35789;&#27719;&#65292;&#23398;&#20064;&#38271;&#23614;&#35789;&#27719;&#65292;&#24182;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#12290;&#22312;&#23454;&#39564;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#25152;&#25552;&#20986;&#30340;&#25216;&#26415;&#25913;&#36827;&#20102;&#27169;&#22411;&#23545;&#26032;&#35789;&#27719;&#30340;&#35782;&#21035;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#25972;&#20307;&#35821;&#35328;&#20998;&#24067;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic speech recognition (ASR) models are typically trained on large datasets of transcribed speech. As language evolves and new terms come into use, these models can become outdated and stale. In the context of models trained on the server but deployed on edge devices, errors may result from the mismatch between server training data and actual on-device usage. In this work, we seek to continually learn from on-device user corrections through Federated Learning (FL) to address this issue. We explore techniques to target fresh terms that the model has not previously encountered, learn long-tail words, and mitigate catastrophic forgetting. In experimental evaluations, we find that the proposed techniques improve model recognition of fresh terms, while preserving quality on the overall language distribution.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Persona&#32534;&#30721;&#22810;&#27969;&#31243;&#23545;&#35805;&#21477;&#23376;&#35780;&#20998;&#26041;&#27861;&#65292;&#21033;&#29992;&#20010;&#20154;&#35282;&#33394;&#20449;&#24687;&#26469;&#25552;&#39640;&#23545;&#35805;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.16770</link><description>&lt;p&gt;
Persona&#32534;&#30721;&#22810;&#27969;&#31243;&#23545;&#35805;&#21477;&#23376;&#35780;&#20998;&#65306;Persona&#24341;&#23548;&#30340;&#22810;&#27969;&#31243;&#23545;&#35805;&#21477;&#23376;&#35780;&#20998;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Persona-Coded Poly-Encoder: Persona-Guided Multi-Stream Conversational Sentence Scoring. (arXiv:2309.16770v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Persona&#32534;&#30721;&#22810;&#27969;&#31243;&#23545;&#35805;&#21477;&#23376;&#35780;&#20998;&#26041;&#27861;&#65292;&#21033;&#29992;&#20010;&#20154;&#35282;&#33394;&#20449;&#24687;&#26469;&#25552;&#39640;&#23545;&#35805;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#32463;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#24191;&#27867;&#24212;&#29992;&#20110;&#23545;&#35805;AI&#12290;&#28982;&#32780;&#65292;&#35201;&#21033;&#29992;&#21487;&#20197;&#25552;&#20379;&#23545;&#35805;&#32972;&#26223;&#25110;&#20010;&#24615;&#21270;&#35843;&#25972;&#30340;&#36741;&#21161;&#20449;&#24687;&#20197;&#25552;&#39640;&#23545;&#35805;&#36136;&#37327;&#20173;&#28982;&#24456;&#20855;&#25361;&#25112;&#24615;&#12290;&#20363;&#22914;&#65292;&#20851;&#20110;&#20351;&#29992;&#20010;&#20154;&#35282;&#33394;&#20449;&#24687;&#26469;&#25552;&#39640;&#23545;&#35805;&#36136;&#37327;&#30340;&#30740;&#31350;&#20165;&#26377;&#38480;&#65292;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;&#23545;&#35805;AI&#25216;&#26415;&#20063;&#26080;&#27861;&#26377;&#25928;&#22320;&#21033;&#29992;&#26469;&#33258;&#22810;&#31181;&#26469;&#28304;&#30340;&#36741;&#21161;&#25968;&#25454;&#20449;&#21495;&#65292;&#20363;&#22914;&#22810;&#27169;&#24335;&#20132;&#20114;&#25968;&#25454;&#12289;&#20154;&#21475;&#32479;&#35745;&#23398;&#25968;&#25454;&#21644;&#31038;&#20250;&#30830;&#23450;&#22240;&#32032;&#25968;&#25454;&#31561;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Persona&#32534;&#30721;&#22810;&#27969;&#31243;&#23545;&#35805;&#21477;&#23376;&#35780;&#20998;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#22810;&#27969;&#31243;&#32534;&#30721;&#26041;&#26696;&#20013;&#30340;&#20010;&#20154;&#35282;&#33394;&#20449;&#24687;&#26469;&#25552;&#39640;&#23545;&#35805;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#23637;&#31034;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#22522;&#20110;&#20010;&#20154;&#35282;&#33394;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#19982;&#21442;&#32771;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in machine learning and deep learning have led to the widespread use of Conversational AI in many practical applications. However, it is still very challenging to leverage auxiliary information that can provide conversational context or personalized tuning to improve the quality of conversations. For example, there has only been limited research on using an individuals persona information to improve conversation quality, and even state-of-the-art conversational AI techniques are unable to effectively leverage signals from heterogeneous sources of auxiliary data, such as multi-modal interaction data, demographics, SDOH data, etc. In this paper, we present a novel Persona-Coded Poly-Encoder method that leverages persona information in a multi-stream encoding scheme to improve the quality of response generation for conversations. To show the efficacy of the proposed method, we evaluate our method on two different persona-based conversational datasets, and compared against 
&lt;/p&gt;</description></item><item><title>QuantEase&#26159;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#30340;&#35821;&#35328;&#27169;&#22411;&#37327;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#36880;&#23618;&#37327;&#21270;&#21644;&#22522;&#20110;&#22352;&#26631;&#19979;&#38477;&#30340;&#31639;&#27861;&#65292;&#39640;&#36136;&#37327;&#22320;&#35299;&#20915;&#20102;&#22797;&#26434;&#30340;&#38750;&#20984;&#37327;&#21270;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#23545;&#24322;&#24120;&#20540;&#25935;&#24863;&#30340;&#21464;&#31181;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.01885</link><description>&lt;p&gt;
QuantEase: &#22522;&#20110;&#20248;&#21270;&#30340;&#35821;&#35328;&#27169;&#22411;&#37327;&#21270;--&#19968;&#31181;&#39640;&#25928;&#32780;&#30452;&#35266;&#30340;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
QuantEase: Optimization-based Quantization for Language Models -- An Efficient and Intuitive Algorithm. (arXiv:2309.01885v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01885
&lt;/p&gt;
&lt;p&gt;
QuantEase&#26159;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#30340;&#35821;&#35328;&#27169;&#22411;&#37327;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#36880;&#23618;&#37327;&#21270;&#21644;&#22522;&#20110;&#22352;&#26631;&#19979;&#38477;&#30340;&#31639;&#27861;&#65292;&#39640;&#36136;&#37327;&#22320;&#35299;&#20915;&#20102;&#22797;&#26434;&#30340;&#38750;&#20984;&#37327;&#21270;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#23545;&#24322;&#24120;&#20540;&#25935;&#24863;&#30340;&#21464;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26222;&#21450;&#65292;&#23545;&#20110;&#33021;&#22815;&#23454;&#29616;&#20854;&#39640;&#25928;&#37096;&#32626;&#30340;&#21387;&#32553;&#25216;&#26415;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#21152;&#12290;&#26412;&#30740;&#31350;&#20391;&#37325;&#20110;LLM&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#12290;&#20511;&#37492;&#26368;&#36817;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#24341;&#20837;&#20102;QuantEase&#65292;&#19968;&#20010;&#36880;&#23618;&#37327;&#21270;&#26694;&#26550;&#65292;&#20854;&#20013;&#21508;&#20010;&#23618;&#38754;&#32463;&#36807;&#21333;&#29420;&#30340;&#37327;&#21270;&#12290;&#35813;&#38382;&#39064;&#34987;&#35270;&#20026;&#31163;&#25955;&#32467;&#26500;&#21270;&#30340;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#20419;&#20351;&#25105;&#20204;&#24320;&#21457;&#20102;&#22522;&#20110;&#22352;&#26631;&#19979;&#38477;&#65288;CD&#65289;&#25216;&#26415;&#30340;&#31639;&#27861;&#12290;&#36825;&#20123;&#22522;&#20110;CD&#30340;&#26041;&#27861;&#20026;&#22797;&#26434;&#30340;&#38750;&#20984;&#36880;&#23618;&#37327;&#21270;&#38382;&#39064;&#25552;&#20379;&#20102;&#39640;&#36136;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;CD&#26041;&#27861;&#20855;&#26377;&#31616;&#21333;&#30340;&#26356;&#26032;&#27493;&#39588;&#65292;&#20165;&#20381;&#36182;&#20110;&#30697;&#38453;&#21644;&#21521;&#37327;&#36816;&#31639;&#65292;&#36991;&#20813;&#20102;&#30697;&#38453;&#27714;&#36870;&#25110;&#20998;&#35299;&#30340;&#38656;&#35201;&#12290;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#19968;&#31181;&#23545;&#24322;&#24120;&#20540;&#25935;&#24863;&#30340;&#21464;&#31181;&#26041;&#27861;&#65292;&#20801;&#35768;&#20445;&#30041;&#20855;&#26377;&#23436;&#20840;&#31934;&#24230;&#30340;&#37325;&#35201;&#26435;&#37325;&#65288;&#24322;&#24120;&#20540;&#65289;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rising popularity of Large Language Models (LLMs), there has been an increasing interest in compression techniques that enable their efficient deployment. This study focuses on the Post-Training Quantization (PTQ) of LLMs. Drawing from recent advances, our work introduces QuantEase, a layer-wise quantization framework where individual layers undergo separate quantization. The problem is framed as a discrete-structured non-convex optimization, prompting the development of algorithms rooted in Coordinate Descent (CD) techniques. These CD-based methods provide high-quality solutions to the complex non-convex layer-wise quantization problems. Notably, our CD-based approach features straightforward updates, relying solely on matrix and vector operations, circumventing the need for matrix inversion or decomposition. We also explore an outlier-aware variant of our approach, allowing for retaining significant weights (outliers) with complete precision. Our proposal attains state-of-th
&lt;/p&gt;</description></item><item><title>RLAIF&#26159;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;AI&#21453;&#39304;&#20195;&#26367;&#20154;&#31867;&#26631;&#27880;&#20559;&#22909;&#65292;&#30456;&#27604;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#65288;RLHF&#65289;&#65292;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#31867;&#20284;&#30340;&#25913;&#36827;&#25928;&#26524;&#65292;&#24182;&#19988;&#22312;&#20154;&#31867;&#35780;&#20272;&#20013;&#24471;&#21040;&#20102;&#30456;&#21516;&#30340;&#35748;&#21487;&#12290;&#36825;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#28508;&#21147;&#35299;&#20915;RLHF&#30340;&#21487;&#25193;&#23637;&#24615;&#38480;&#21046;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.00267</link><description>&lt;p&gt;
RLAIF: &#20351;&#29992;AI&#21453;&#39304;&#26469;&#25193;&#23637;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback. (arXiv:2309.00267v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00267
&lt;/p&gt;
&lt;p&gt;
RLAIF&#26159;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;AI&#21453;&#39304;&#20195;&#26367;&#20154;&#31867;&#26631;&#27880;&#20559;&#22909;&#65292;&#30456;&#27604;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#65288;RLHF&#65289;&#65292;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#31867;&#20284;&#30340;&#25913;&#36827;&#25928;&#26524;&#65292;&#24182;&#19988;&#22312;&#20154;&#31867;&#35780;&#20272;&#20013;&#24471;&#21040;&#20102;&#30456;&#21516;&#30340;&#35748;&#21487;&#12290;&#36825;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#28508;&#21147;&#35299;&#20915;RLHF&#30340;&#21487;&#25193;&#23637;&#24615;&#38480;&#21046;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#23545;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#30456;&#19968;&#33268;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#26159;&#25910;&#38598;&#39640;&#36136;&#37327;&#30340;&#20154;&#31867;&#20559;&#22909;&#26631;&#31614;&#26159;&#19968;&#20010;&#20851;&#38190;&#29942;&#39048;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;RLHF&#21644;&#21033;&#29992;&#29616;&#25104;&#30340;LLM&#36827;&#34892;&#26631;&#35760;&#30340;RL from AI Feedback (RLAIF)&#25216;&#26415;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#37117;&#33021;&#33719;&#24471;&#31867;&#20284;&#30340;&#25913;&#21892;&#25928;&#26524;&#12290;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#65292;&#20154;&#31867;&#35780;&#20272;&#32773;&#22312;&#32422;70%&#30340;&#26696;&#20363;&#20013;&#37117;&#26356;&#21916;&#27426;RLAIF&#21644;RLHF&#20135;&#29983;&#30340;&#25991;&#26412;&#65292;&#32780;&#19981;&#26159;&#22522;&#20934;&#30340;&#30417;&#30563;&#24494;&#35843;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#24403;&#34987;&#35201;&#27714;&#35780;&#20272;RLAIF&#21644;RLHF&#30340;&#25688;&#35201;&#26102;&#65292;&#20154;&#31867;&#20197;&#30456;&#21516;&#30340;&#27604;&#29575;&#26356;&#21916;&#27426;&#20004;&#32773;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;RLAIF&#21487;&#20197;&#36798;&#21040;&#20154;&#31867;&#27700;&#24179;&#30340;&#24615;&#33021;&#65292;&#20026;&#20811;&#26381;RLHF&#30340;&#21487;&#25193;&#23637;&#24615;&#38480;&#21046;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning from human feedback (RLHF) is effective at aligning large language models (LLMs) to human preferences, but gathering high quality human preference labels is a key bottleneck. We conduct a head-to-head comparison of RLHF vs. RL from AI Feedback (RLAIF) - a technique where preferences are labeled by an off-the-shelf LLM in lieu of humans, and we find that they result in similar improvements. On the task of summarization, human evaluators prefer generations from both RLAIF and RLHF over a baseline supervised fine-tuned model in ~70% of cases. Furthermore, when asked to rate RLAIF vs. RLHF summaries, humans prefer both at equal rates. These results suggest that RLAIF can yield human-level performance, offering a potential solution to the scalability limitations of RLHF.
&lt;/p&gt;</description></item><item><title>PointLLM&#26159;&#19968;&#31181;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#28857;&#20113;&#30340;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#28857;&#20113;&#32534;&#30721;&#22120;&#21644;&#24378;&#22823;&#30340;LLM&#23558;&#20960;&#20309;&#12289;&#22806;&#35266;&#21644;&#35821;&#35328;&#20449;&#24687;&#34701;&#21512;&#65292;&#24182;&#36890;&#36807;&#20154;&#31867;&#25351;&#23548;&#29983;&#25104;&#29615;&#22659;&#19978;&#24688;&#24403;&#30340;&#21709;&#24212;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25910;&#38598;&#22823;&#35268;&#27169;&#30340;&#28857;-&#25991;&#26412;&#25351;&#20196;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#20004;&#38454;&#27573;&#30340;&#35757;&#32451;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24863;&#30693;&#33021;&#21147;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.16911</link><description>&lt;p&gt;
PointLLM&#65306;&#36171;&#20104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#28857;&#20113;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
PointLLM: Empowering Large Language Models to Understand Point Clouds. (arXiv:2308.16911v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16911
&lt;/p&gt;
&lt;p&gt;
PointLLM&#26159;&#19968;&#31181;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#28857;&#20113;&#30340;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#28857;&#20113;&#32534;&#30721;&#22120;&#21644;&#24378;&#22823;&#30340;LLM&#23558;&#20960;&#20309;&#12289;&#22806;&#35266;&#21644;&#35821;&#35328;&#20449;&#24687;&#34701;&#21512;&#65292;&#24182;&#36890;&#36807;&#20154;&#31867;&#25351;&#23548;&#29983;&#25104;&#29615;&#22659;&#19978;&#24688;&#24403;&#30340;&#21709;&#24212;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25910;&#38598;&#22823;&#35268;&#27169;&#30340;&#28857;-&#25991;&#26412;&#25351;&#20196;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#20004;&#38454;&#27573;&#30340;&#35757;&#32451;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24863;&#30693;&#33021;&#21147;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21069;&#25152;&#26410;&#26377;&#30340;&#36827;&#23637;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20135;&#29983;&#20102;&#28145;&#36828;&#24433;&#21709;&#65292;&#20294;&#22312;3D&#29702;&#35299;&#39046;&#22495;&#20173;&#26377;&#24453;&#23436;&#20840;&#21457;&#23637;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;PointLLM&#65292;&#36825;&#26159;&#19968;&#39033;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#30340;&#21021;&#27493;&#24037;&#20316;&#65292;&#20351;LLM&#33021;&#22815;&#29702;&#35299;&#28857;&#20113;&#65292;&#24182;&#25552;&#20379;&#20102;&#36229;&#36234;2D&#35270;&#35273;&#25968;&#25454;&#30340;&#26032;&#36884;&#24452;&#12290;PointLLM&#36890;&#36807;&#20154;&#31867;&#25351;&#23548;&#22788;&#29702;&#24102;&#26377;&#39068;&#33394;&#30340;&#29289;&#20307;&#28857;&#20113;&#65292;&#24182;&#29983;&#25104;&#29615;&#22659;&#19978;&#24688;&#24403;&#30340;&#21709;&#24212;&#65292;&#23637;&#31034;&#20102;&#20854;&#23545;&#28857;&#20113;&#21644;&#24120;&#35782;&#30340;&#25484;&#25569;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23427;&#21033;&#29992;&#20102;&#19968;&#20010;&#28857;&#20113;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#24378;&#22823;&#30340;LLM&#65292;&#26377;&#25928;&#22320;&#34701;&#21512;&#20102;&#20960;&#20309;&#12289;&#22806;&#35266;&#21644;&#35821;&#35328;&#20449;&#24687;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;66&#19975;&#20010;&#31616;&#21333;&#21644;7&#19975;&#20010;&#22797;&#26434;&#30340;&#28857;-&#25991;&#26412;&#25351;&#20196;&#23545;&#65292;&#20197;&#23454;&#29616;&#20004;&#38454;&#27573;&#30340;&#35757;&#32451;&#31574;&#30053;&#65306;&#39318;&#20808;&#23545;&#40784;&#28508;&#22312;&#31354;&#38388;&#65292;&#28982;&#21518;&#23545;&#32479;&#19968;&#27169;&#22411;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#12290;&#20026;&#20102;&#20005;&#26684;&#35780;&#20272;&#25105;&#20204;&#27169;&#22411;&#30340;&#24863;&#30693;&#33021;&#21147;&#21644;&#20854;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#35780;&#20272;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The unprecedented advancements in Large Language Models (LLMs) have created a profound impact on natural language processing but are yet to fully embrace the realm of 3D understanding. This paper introduces PointLLM, a preliminary effort to fill this gap, thereby enabling LLMs to understand point clouds and offering a new avenue beyond 2D visual data. PointLLM processes colored object point clouds with human instructions and generates contextually appropriate responses, illustrating its grasp of point clouds and common sense. Specifically, it leverages a point cloud encoder with a powerful LLM to effectively fuse geometric, appearance, and linguistic information. We collect a novel dataset comprising 660K simple and 70K complex point-text instruction pairs to enable a two-stage training strategy: initially aligning latent spaces and subsequently instruction-tuning the unified model. To rigorously evaluate our model's perceptual abilities and its generalization capabilities, we establis
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23384;&#22312;&#28431;&#27934;&#30340;&#20195;&#30721;&#34917;&#20840;&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#24182;&#21457;&#29616;&#36825;&#20123;&#28431;&#27934;&#26174;&#33879;&#38477;&#20302;&#20102;Code-LLMs&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.03438</link><description>&lt;p&gt;
&#20195;&#30721;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#22635;&#20889;&#21487;&#33021;&#23384;&#22312;&#28431;&#27934;&#30340;&#20195;&#30721;&#26102;&#23384;&#22312;&#22833;&#36133;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Large Language Models of Code Fail at Completing Code with Potential Bugs. (arXiv:2306.03438v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03438
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23384;&#22312;&#28431;&#27934;&#30340;&#20195;&#30721;&#34917;&#20840;&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#24182;&#21457;&#29616;&#36825;&#20123;&#28431;&#27934;&#26174;&#33879;&#38477;&#20302;&#20102;Code-LLMs&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20195;&#30721;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;Code-LLMs&#65289;&#22312;&#20195;&#30721;&#34917;&#20840;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#65292;&#36825;&#26159;&#32534;&#31243;&#36741;&#21161;&#21644;&#20195;&#30721;&#26234;&#33021;&#30340;&#22522;&#26412;&#21151;&#33021;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#30740;&#31350;&#24573;&#30053;&#20102;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#20195;&#30721;&#19978;&#19979;&#25991;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#28431;&#27934;&#38382;&#39064;&#65292;&#22312;&#36719;&#20214;&#24320;&#21457;&#20013;&#36825;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#24182;&#30740;&#31350;&#20102;&#23384;&#22312;&#28431;&#27934;&#30340;&#20195;&#30721;&#34917;&#20840;&#38382;&#39064;&#65292;&#21463;&#23454;&#26102;&#20195;&#30721;&#24314;&#35758;&#30340;&#29616;&#23454;&#22330;&#26223;&#21551;&#21457;&#65292;&#20195;&#30721;&#19978;&#19979;&#25991;&#20013;&#21253;&#21547;&#21487;&#33021;&#30340;&#28431;&#27934;-&#21453;&#27169;&#24335;&#65292;&#36825;&#20123;&#21453;&#27169;&#24335;&#21487;&#20197;&#25104;&#20026;&#23436;&#25104;&#31243;&#24207;&#20013;&#30340;&#28431;&#27934;&#12290;&#20026;&#20102;&#31995;&#32479;&#22320;&#30740;&#31350;&#20219;&#21153;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#65306;&#19968;&#20010;&#26159;&#20174;&#35821;&#20041;&#25913;&#21464;&#25805;&#20316;&#20013;&#27966;&#29983;&#30340;&#21512;&#25104;&#28431;&#27934;&#25968;&#25454;&#38598;&#65288;buggy-HumanEval&#65289;&#65292;&#21478;&#19968;&#20010;&#26159;&#20174;&#29992;&#25143;&#25552;&#20132;&#30340;&#32534;&#31243;&#38382;&#39064;&#20013;&#27966;&#29983;&#30340;&#29616;&#23454;&#28431;&#27934;&#25968;&#25454;&#38598;&#65288;buggy-FixEval&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21487;&#33021;&#23384;&#22312;&#28431;&#27934;&#30340;&#24773;&#20917;&#26174;&#33879;&#38477;&#20302;&#20102;&#39640;&#24615;&#33021;Code-LLMs&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;&#20363;&#22914;&#65292;CodeGen-2B-mono&#22312;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#30340;&#36890;&#36807;&#29575;
&lt;/p&gt;
&lt;p&gt;
Large language models of code (Code-LLMs) have recently brought tremendous advances to code completion, a fundamental feature of programming assistance and code intelligence. However, most existing works ignore the possible presence of bugs in the code context for generation, which are inevitable in software development. Therefore, we introduce and study the buggy-code completion problem, inspired by the realistic scenario of real-time code suggestion where the code context contains potential bugs -- anti-patterns that can become bugs in the completed program. To systematically study the task, we introduce two datasets: one with synthetic bugs derived from semantics-altering operator changes (buggy-HumanEval) and one with realistic bugs derived from user submissions to coding problems (buggy-FixEval). We find that the presence of potential bugs significantly degrades the generation performance of the high-performing Code-LLMs. For instance, the passing rates of CodeGen-2B-mono on test 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#38382;&#31572;&#30340;&#21435;&#25991;&#26412;&#21270;&#26694;&#26550;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#23637;&#31034;&#25552;&#21462;&#30340;&#25991;&#26412;&#25688;&#24405;&#12290;&#22312;&#38382;&#31572;&#21644;&#24341;&#35777;&#19978;&#30340;&#34920;&#29616;&#31867;&#20284;&#20110;&#31471;&#21040;&#31471;&#26041;&#27861;&#65292;&#24182;&#19988;&#25903;&#25345;&#29992;&#25143;&#20449;&#24687;&#38656;&#27714;&#21450;&#20559;&#22909;&#30340;&#21487;&#25511;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14772</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#25511;&#30340;&#22522;&#20110;&#38382;&#31572;&#30340;&#21435;&#25991;&#26412;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Controllable QA-based Framework for Decontextualization. (arXiv:2305.14772v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#38382;&#31572;&#30340;&#21435;&#25991;&#26412;&#21270;&#26694;&#26550;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#23637;&#31034;&#25552;&#21462;&#30340;&#25991;&#26412;&#25688;&#24405;&#12290;&#22312;&#38382;&#31572;&#21644;&#24341;&#35777;&#19978;&#30340;&#34920;&#29616;&#31867;&#20284;&#20110;&#31471;&#21040;&#31471;&#26041;&#27861;&#65292;&#24182;&#19988;&#25903;&#25345;&#29992;&#25143;&#20449;&#24687;&#38656;&#27714;&#21450;&#20559;&#22909;&#30340;&#21487;&#25511;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#30495;&#23454;&#22330;&#26223;&#19979;&#30340;&#24212;&#29992;&#38656;&#35201;&#23558;&#25552;&#21462;&#30340;&#25688;&#24405;&#23637;&#31034;&#32473;&#29992;&#25143;&#65292;&#36825;&#20123;&#25688;&#24405;&#24448;&#24448;&#38656;&#35201;&#35299;&#32806;&#21407;&#26469;&#30340;&#25991;&#26412;&#25165;&#33021;&#26356;&#22909;&#22320;&#21576;&#29616;&#32473;&#29992;&#25143;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;LLMs&#22312;&#38382;&#31572;&#21644;&#24341;&#35777;&#19978;&#30340;&#21435;&#25991;&#26412;&#21270;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#38382;&#31572;&#30340;&#21435;&#25991;&#26412;&#21270;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#26356;&#22909;&#22320;&#28385;&#36275;&#29992;&#25143;&#20449;&#24687;&#38656;&#27714;&#21450;&#20559;&#22909;&#65292;&#24182;&#19988;&#22312;&#32467;&#26524;&#19978;&#34920;&#29616;&#20986;&#31867;&#20284;&#20110;&#31471;&#21040;&#31471;&#26041;&#27861;&#30340;&#31454;&#20105;&#21147;&#12290;&#25105;&#20204;&#21516;&#26102;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#35813;&#26694;&#26550;&#23558;&#29992;&#25143;&#20559;&#22909;&#34701;&#20837;&#21040;&#31995;&#32479;&#20013;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#21487;&#25511;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many real-world applications require surfacing extracted snippets to users, whether motivated by assistive tools for literature surveys or document cross-referencing, or needs to mitigate and recover from model generated inaccuracies., Yet, these passages can be difficult to consume when divorced from their original document context. In this work, we explore the limits of LLMs to perform decontextualization of document snippets in user-facing scenarios, focusing on two real-world settings - question answering and citation context previews for scientific documents. We propose a question-answering framework for decontextualization that allows for better handling of user information needs and preferences when determining the scope of rewriting. We present results showing state-of-the-art LLMs under our framework remain competitive with end-to-end approaches. We also explore incorporating user preferences into the system, finding our framework allows for controllability.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#25991;&#26723;&#23548;&#21521;&#23545;&#35805;&#29983;&#25104;&#20013;&#30340;&#20445;&#30495;&#24230;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28857;&#20114;&#20449;&#24687;&#30340;&#24230;&#37327;&#26041;&#27861;&#21644;&#35299;&#30721;&#31574;&#30053;&#26469;&#39044;&#27979;&#26356;&#21152;&#20445;&#30495;&#30340;&#22238;&#31572;&#12290;</title><link>http://arxiv.org/abs/2305.12191</link><description>&lt;p&gt;
&#22522;&#20110;&#28857;&#20114;&#20449;&#24687;&#24230;&#37327;&#21644;&#35299;&#30721;&#31574;&#30053;&#30340;&#25991;&#26723;&#23548;&#21521;&#23545;&#35805;&#29983;&#25104;&#20013;&#30340;&#20445;&#30495;&#24230;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Pointwise Mutual Information Based Metric and Decoding Strategy for Faithful Generation in Document Grounded Dialogs. (arXiv:2305.12191v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12191
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#25991;&#26723;&#23548;&#21521;&#23545;&#35805;&#29983;&#25104;&#20013;&#30340;&#20445;&#30495;&#24230;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28857;&#20114;&#20449;&#24687;&#30340;&#24230;&#37327;&#26041;&#27861;&#21644;&#35299;&#30721;&#31574;&#30053;&#26469;&#39044;&#27979;&#26356;&#21152;&#20445;&#30495;&#30340;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25991;&#26723;&#23548;&#21521;&#23545;&#35805;&#29983;&#25104;&#20013;&#65292;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#26159;&#29983;&#25104;&#30340;&#22238;&#31572;&#21487;&#33021;&#19982;&#24213;&#23618;&#25991;&#26723;&#19981;&#19968;&#33268;&#12290;&#29616;&#26377;&#30340;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#30340;&#22238;&#31572;&#26159;&#21542;&#19982;&#24213;&#23618;&#25991;&#26723;&#19968;&#33268;&#30340;&#33258;&#21160;&#21270;&#24230;&#37327;&#26041;&#27861;&#65292;&#24230;&#37327;&#29983;&#25104;&#30340;&#22238;&#31572;&#19982;&#25991;&#26723;&#20869;&#23481;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#33258;&#21160;&#21270;&#24230;&#37327;&#26041;&#27861;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#27604;&#24046;&#24322;&#24456;&#22823;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#25552;&#39640;&#20445;&#30495;&#24230;&#30340;&#27979;&#37327;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#28857;&#20114;&#20449;&#24687;&#65288;PMI&#65289;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;&#65292;&#35813;&#24230;&#37327;&#26041;&#27861;&#37327;&#21270;&#29983;&#25104;&#30340;&#22238;&#31572;&#19982;&#28304;&#25991;&#26723;&#20043;&#38388;&#30340;PMI&#65292;&#21463;&#23545;&#35805;&#26465;&#20214;&#30340;&#24433;&#21709;&#12290;PMI&#37327;&#21270;&#25991;&#26723;&#23545;&#29983;&#25104;&#30340;&#22238;&#31572;&#30340;&#24433;&#21709;&#31243;&#24230;&#65292;PMI&#36234;&#39640;&#21017;&#22238;&#31572;&#36234;&#20445;&#30495;&#12290;&#25105;&#20204;&#22522;&#20110;&#27492;&#24605;&#24819;&#26500;&#24314;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#30721;&#25216;&#26415;&#65292;&#23558;PMI&#24182;&#20837;&#21040;&#29983;&#25104;&#27969;&#31243;&#20013;&#65292;&#20197;&#39044;&#27979;&#26356;&#21152;&#20445;&#30495;&#30340;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;
A major concern in using deep learning based generative models for document-grounded dialogs is the potential generation of responses that are not \textit{faithful} to the underlying document. Existing automated metrics used for evaluating the faithfulness of response with respect to the grounding document measure the degree of similarity between the generated response and the document's content. However, these automated metrics are far from being well aligned with human judgments. Therefore, to improve the measurement of faithfulness, we propose a new metric that utilizes (Conditional) Point-wise Mutual Information (PMI) between the generated response and the source document, conditioned on the dialogue. PMI quantifies the extent to which the document influences the generated response -- with a higher PMI indicating a more faithful response. We build upon this idea to create a new decoding technique that incorporates PMI into the response generation process to predict more faithful re
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#38750;&#33258;&#22238;&#24402;Transformer&#27169;&#22411;&#29992;&#20110;&#23545;&#19981;&#23436;&#25972;&#20449;&#24687;&#30340;&#25991;&#26412;&#29983;&#25104;&#65292;&#20854;&#20855;&#26377;&#20301;&#32622;&#24863;&#30693;&#33258;&#35843;&#33410;&#21644;&#20381;&#36182;&#25509;&#21475;&#32593;&#32476;&#65292;&#33021;&#22815;&#22312;&#19982;&#20854;&#20182;&#20027;&#27969;&#27169;&#22411;&#30456;&#27604;&#26356;&#24555;&#30340;&#35299;&#30721;&#26102;&#38388;&#20869;&#33719;&#24471;&#21487;&#27604;&#36739;&#24615;&#33021;&#65292;&#20855;&#26377;&#22312;&#28508;&#22312;&#25554;&#20540;&#31561;&#24212;&#29992;&#20013;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.03977</link><description>&lt;p&gt;
&#19968;&#31181;&#24102;&#26377;&#19981;&#23436;&#25972;&#20449;&#24687;&#30340;&#25991;&#26412;&#29983;&#25104;&#23545;&#25239;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
An Adversarial Non-Autoregressive Model for Text Generation with Incomplete Information. (arXiv:2305.03977v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03977
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#38750;&#33258;&#22238;&#24402;Transformer&#27169;&#22411;&#29992;&#20110;&#23545;&#19981;&#23436;&#25972;&#20449;&#24687;&#30340;&#25991;&#26412;&#29983;&#25104;&#65292;&#20854;&#20855;&#26377;&#20301;&#32622;&#24863;&#30693;&#33258;&#35843;&#33410;&#21644;&#20381;&#36182;&#25509;&#21475;&#32593;&#32476;&#65292;&#33021;&#22815;&#22312;&#19982;&#20854;&#20182;&#20027;&#27969;&#27169;&#22411;&#30456;&#27604;&#26356;&#24555;&#30340;&#35299;&#30721;&#26102;&#38388;&#20869;&#33719;&#24471;&#21487;&#27604;&#36739;&#24615;&#33021;&#65292;&#20855;&#26377;&#22312;&#28508;&#22312;&#25554;&#20540;&#31561;&#24212;&#29992;&#20013;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#22312;&#23436;&#25972;&#20449;&#24687;&#24773;&#20917;&#65288;CIS&#65289;&#19979;&#24050;&#24191;&#27867;&#30740;&#31350;&#65292;&#20854;&#20013;&#27169;&#22411;&#20855;&#26377;&#23436;&#25972;&#30340;&#36755;&#20837;&#20449;&#24687;&#26469;&#33719;&#21462;&#30456;&#24212;&#30340;&#36755;&#20986;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#19981;&#23436;&#25972;&#20449;&#24687;&#24773;&#20917;&#65288;IIS&#65289;&#19979;&#30340;&#25506;&#32034;&#26497;&#20026;&#26377;&#38480;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;IIS&#20013;&#19981;&#23436;&#25972;&#30340;&#36755;&#20837;&#20449;&#24687;&#23558;&#22686;&#21152;&#22312;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#19979;&#35757;&#32451;&#30340;&#29616;&#26377;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#22266;&#26377;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;IIS&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#38750;&#33258;&#22238;&#24402;Transformer &#65288;ANT&#65289;&#27169;&#22411;&#65292;&#20855;&#26377;&#20004;&#20010;&#26032;&#29305;&#24615;&#65306;1&#65289;&#20301;&#32622;&#24863;&#30693;&#33258;&#35843;&#33410;&#65292;&#21487;&#20197;&#25552;&#20379;&#26356;&#21512;&#29702;&#30340;&#38544;&#34255;&#34920;&#31034;&#65307;2&#65289;&#20381;&#36182;&#24615;&#21069;&#39304;&#32593;&#32476;&#65292;&#21487;&#20197;&#22686;&#24378;&#20854;&#20381;&#36182;&#24615;&#24314;&#27169;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;ANT&#19982;IIS&#20013;&#30340;&#20854;&#20182;&#20027;&#27969;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#35777;&#26126;ANT&#21487;&#20197;&#23454;&#29616;&#21487;&#27604;&#36739;&#24615;&#33021;&#65292;&#21516;&#26102;&#20063;&#21487;&#20197;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#24555;&#22320;&#36827;&#34892;&#35299;&#30721;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;ANT&#22312;&#28508;&#22312;&#25554;&#20540;&#31561;&#21508;&#31181;&#24212;&#29992;&#26041;&#38754;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Non-autoregressive models have been widely studied in the Complete Information Scenario (CIS), in which the models have complete input information to obtain corresponding output. However, their explorations in the Incomplete Information Scenario (IIS) are extremely limited. Our analyses reveal that the IIS's incomplete input information will augment the inherent limitations of existing non-autoregressive models trained under Maximum Likelihood Estimation. In this paper, we propose for the IIS an Adversarial Non-autoregressive Transformer (ANT) which has two novel features: 1) Position Aware Self-Modulation to provide more reasonable hidden representations, and 2) Dependency Feed Forward Network to strengthen its capacity in dependency modeling. We compare ANT with other mainstream models in the IIS and demonstrate that ANT can achieve comparable performance with much fewer decoding iterations. Furthermore, we show its great potential in various applications like latent interpolation an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25512;&#23548;&#38142;&#27491;&#30830;&#24615;&#21644;&#20449;&#24687;&#37327;&#30340;&#25512;&#29702;&#38142;&#35780;&#20272;&#26694;&#26550;ReCEval&#65292;&#29992;&#20197;&#35780;&#20272;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#23458;&#35266;&#12289;&#31995;&#32479;&#21644;&#20934;&#30830;&#22320;&#35780;&#20272;&#25512;&#29702;&#38142;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.10703</link><description>&lt;p&gt;
&#36890;&#36807;&#27491;&#30830;&#24615;&#21644;&#20449;&#24687;&#37327;&#35780;&#20272;&#25512;&#29702;&#38142;&#30340;ReCEval
&lt;/p&gt;
&lt;p&gt;
ReCEval: Evaluating Reasoning Chains via Correctness and Informativeness. (arXiv:2304.10703v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25512;&#23548;&#38142;&#27491;&#30830;&#24615;&#21644;&#20449;&#24687;&#37327;&#30340;&#25512;&#29702;&#38142;&#35780;&#20272;&#26694;&#26550;ReCEval&#65292;&#29992;&#20197;&#35780;&#20272;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#23458;&#35266;&#12289;&#31995;&#32479;&#21644;&#20934;&#30830;&#22320;&#35780;&#20272;&#25512;&#29702;&#38142;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#37117;&#26159;&#22522;&#30784;&#65292;&#20294;&#20160;&#20040;&#26500;&#25104;&#22909;&#30340;&#25512;&#29702;&#38142;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#23427;&#20204;&#23578;&#19981;&#28165;&#26970;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20165;&#20851;&#27880;&#25512;&#29702;&#38142;&#26159;&#21542;&#23548;&#33268;&#27491;&#30830;&#30340;&#32467;&#35770;&#65292;&#20294;&#36825;&#31181;&#20197;&#31572;&#26696;&#20026;&#23548;&#21521;&#30340;&#35266;&#28857;&#21487;&#33021;&#20250;&#23558;&#22909;&#30340;&#25512;&#29702;&#36136;&#37327;&#19982;&#20854;&#20182;&#29992;&#20110;&#39044;&#27979;&#31572;&#26696;&#30340;&#20551;&#25463;&#24452;&#28151;&#28102;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#23558;&#25512;&#29702;&#38142;&#35270;&#20026;&#25512;&#23548;&#26368;&#32456;&#31572;&#26696;&#30340;&#38750;&#27491;&#24335;&#35777;&#26126;&#65292;&#36890;&#36807;&#35780;&#20272;&#25512;&#29702;&#38142;&#30340;&#20004;&#20010;&#20851;&#38190;&#29305;&#24615;&#8212;&#8212;&#65288;1&#65289;&#27491;&#30830;&#24615;&#65292;&#21363;&#27599;&#20010;&#27493;&#39588;&#22522;&#20110;&#27493;&#39588;&#65292;&#21069;&#32622;&#27493;&#39588;&#21644;&#36755;&#20837;&#19978;&#19979;&#25991;&#20013;&#21253;&#21547;&#30340;&#20449;&#24687;&#36827;&#34892;&#26377;&#25928;&#25512;&#29702;&#65292;&#20197;&#21450;&#65288;2&#65289;&#20449;&#24687;&#37327;&#65292;&#21363;&#27599;&#20010;&#27493;&#39588;&#25552;&#20379;&#26032;&#20449;&#24687;&#26377;&#21161;&#20110;&#25512;&#23548;&#29983;&#25104;&#30340;&#31572;&#26696;&#8212;&#8212;&#25105;&#20204;&#25552;&#20986;&#20102;ReCEval&#65288;&#25512;&#29702;&#38142;&#35780;&#20272;&#65289;&#26694;&#26550;&#12290;&#25105;&#20204;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#27169;&#22411;&#21644;&#20449;&#24687;&#29702;&#35770;&#27979;&#37327;&#23454;&#29616;&#20102;ReCEval&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#35780;&#20272;&#25512;&#29702;&#38142;&#26041;&#38754;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#21152;&#23458;&#35266;&#12289;&#31995;&#32479;&#21644;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-step reasoning ability is fundamental to many natural language tasks, yet it is unclear what constitutes a good reasoning chain and how to evaluate them. Most existing methods focus solely on whether the reasoning chain leads to the correct conclusion, but this answer-oriented view may confound the quality of reasoning with other spurious shortcuts to predict the answer. To bridge this gap, we evaluate reasoning chains by viewing them as informal proofs that derive the final answer. Specifically, we propose ReCEval (Reasoning Chain Evaluation), a framework that evaluates reasoning chains through two key properties: (1) correctness, i.e., each step makes a valid inference based on the information contained within the step, preceding steps, and input context, and (2) informativeness, i.e., each step provides new information that is helpful towards deriving the generated answer. We implement ReCEval using natural language inference models and information-theoretic measures. On multi
&lt;/p&gt;</description></item><item><title>RAFT&#26694;&#26550;&#24341;&#20837;&#20102;&#22870;&#21169;&#25490;&#21517;&#24494;&#35843;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#40784;&#29983;&#25104;&#22411;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#24102;&#26469;&#30340;&#20302;&#25928;&#21644;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.06767</link><description>&lt;p&gt;
RAFT: &#22870;&#21169;&#25490;&#21517;&#24494;&#35843;&#29992;&#20110;&#29983;&#25104;&#22411;&#22522;&#30784;&#27169;&#22411;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment. (arXiv:2304.06767v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06767
&lt;/p&gt;
&lt;p&gt;
RAFT&#26694;&#26550;&#24341;&#20837;&#20102;&#22870;&#21169;&#25490;&#21517;&#24494;&#35843;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#40784;&#29983;&#25104;&#22411;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#24102;&#26469;&#30340;&#20302;&#25928;&#21644;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#22411;&#22522;&#30784;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#24191;&#27867;&#30340;&#26080;&#30417;&#30563;&#35757;&#32451;&#25968;&#25454;&#24102;&#26469;&#30340;&#38544;&#24335;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#20559;&#35265;&#21487;&#33021;&#23548;&#33268;&#23376;&#20248;&#26679;&#26412;&#12289;&#25197;&#26354;&#30340;&#32467;&#26524;&#21644;&#19981;&#20844;&#24179;&#65292;&#21487;&#33021;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#23558;&#36825;&#20123;&#27169;&#22411;&#19982;&#20154;&#30340;&#20262;&#29702;&#21644;&#20559;&#22909;&#23545;&#40784;&#26159;&#30830;&#20445;&#23427;&#20204;&#22312;&#30495;&#23454;&#24212;&#29992;&#20013;&#36127;&#36131;&#20219;&#21644;&#26377;&#25928;&#30340;&#37096;&#32626;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#20027;&#35201;&#37319;&#29992;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288; RLHF&#65289;&#20316;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#25163;&#27573;&#12290;&#22312; RL &#31639;&#27861;&#30340;&#25351;&#23548;&#19979;&#65292;&#29992;&#20154;&#31867;&#21453;&#39304;&#25351;&#23548;&#30340;&#22870;&#21169;&#27169;&#22411;&#23545;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292; RL &#31639;&#27861;&#30340;&#20302;&#25928;&#24615;&#21644;&#19981;&#31283;&#23450;&#24615;&#24120;&#24120;&#20250;&#23545;&#29983;&#25104;&#27169;&#22411;&#30340;&#25104;&#21151;&#23545;&#40784;&#20135;&#29983;&#37325;&#22823;&#38556;&#30861;&#65292;&#22240;&#27492;&#38656;&#35201;&#24320;&#21457;&#19968;&#31181;&#26356;&#20026;&#24378;&#22823;&#21644;&#31616;&#21270;&#30340;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21363;&#22870;&#21169;&#25490;&#21517;&#24494;&#35843;&#65288; RAFT &#65289;&#65292;&#26088;&#22312;&#23545;&#40784;&#29983;&#25104;&#22522;&#30784;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative foundation models are susceptible to implicit biases that can arise from extensive unsupervised training data. Such biases can produce suboptimal samples, skewed outcomes, and unfairness, with potentially significant repercussions. Consequently, aligning these models with human ethics and preferences is an essential step toward ensuring their responsible and effective deployment in real-world applications. Prior research has primarily employed Reinforcement Learning from Human Feedback (RLHF) as a means of addressing this problem, wherein generative models are fine-tuned using RL algorithms guided by a human-feedback-informed reward model. However, the inefficiencies and instabilities associated with RL algorithms frequently present substantial obstacles to the successful alignment of generative models, necessitating the development of a more robust and streamlined approach. To this end, we introduce a new framework, Reward rAnked FineTuning (RAFT), designed to align generat
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#30740;&#32508;&#36848;&#20102;&#36890;&#36807;&#26816;&#32034;&#22810;&#27169;&#24577;&#30693;&#35782;&#26469;&#21327;&#21161;&#21644;&#22686;&#24378;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#25552;&#20379;&#20102;&#35299;&#20915;&#20934;&#30830;&#24615;&#12289;&#25512;&#29702;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#40065;&#26834;&#24615;&#31561;&#37325;&#35201;&#38382;&#39064;&#30340;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2303.10868</link><description>&lt;p&gt;
&#26816;&#32034;&#22810;&#27169;&#24577;&#20449;&#24687;&#29992;&#20110;&#22686;&#24378;&#29983;&#25104;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Retrieving Multimodal Information for Augmented Generation: A Survey. (arXiv:2303.10868v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10868
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#30740;&#32508;&#36848;&#20102;&#36890;&#36807;&#26816;&#32034;&#22810;&#27169;&#24577;&#30693;&#35782;&#26469;&#21327;&#21161;&#21644;&#22686;&#24378;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#25552;&#20379;&#20102;&#35299;&#20915;&#20934;&#30830;&#24615;&#12289;&#25512;&#29702;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#40065;&#26834;&#24615;&#31561;&#37325;&#35201;&#38382;&#39064;&#30340;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26222;&#21450;&#65292;&#20351;&#29992;&#22810;&#27169;&#24577;&#26469;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#36235;&#21183;&#65292;&#36825;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#33021;&#26356;&#22909;&#22320;&#19982;&#19990;&#30028;&#20114;&#21160;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22312;&#21738;&#20010;&#38454;&#27573;&#20197;&#21450;&#22914;&#20309;&#34701;&#21512;&#19981;&#21516;&#27169;&#24577;&#30340;&#38382;&#39064;&#32570;&#20047;&#19968;&#20010;&#32479;&#19968;&#30340;&#35748;&#35782;&#12290;&#22312;&#26412;&#35843;&#30740;&#20013;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#36890;&#36807;&#26816;&#32034;&#22810;&#27169;&#24577;&#30693;&#35782;&#26469;&#21327;&#21161;&#21644;&#22686;&#24378;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#30693;&#35782;&#30340;&#26684;&#24335;&#21253;&#25324;&#22270;&#20687;&#12289;&#20195;&#30721;&#12289;&#34920;&#26684;&#12289;&#22270;&#24418;&#21644;&#38899;&#39057;&#12290;&#36825;&#20123;&#26041;&#27861;&#20026;&#23454;&#29616;&#20934;&#30830;&#24615;&#12289;&#25512;&#29702;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#40065;&#26834;&#24615;&#31561;&#37325;&#35201;&#38382;&#39064;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#25552;&#20379;&#28145;&#20837;&#30340;&#22238;&#39038;&#65292;&#26412;&#35843;&#30740;&#26088;&#22312;&#35753;&#23398;&#32773;&#20204;&#26356;&#28145;&#20837;&#22320;&#20102;&#35299;&#36825;&#20123;&#26041;&#27861;&#30340;&#24212;&#29992;&#65292;&#24182;&#40723;&#21169;&#20182;&#20204;&#23558;&#29616;&#26377;&#25216;&#26415;&#24212;&#29992;&#20110;&#24555;&#36895;&#21457;&#23637;&#30340;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Large Language Models (LLMs) become popular, there emerged an important trend of using multimodality to augment the LLMs' generation ability, which enables LLMs to better interact with the world. However, there lacks a unified perception of at which stage and how to incorporate different modalities. In this survey, we review methods that assist and augment generative models by retrieving multimodal knowledge, whose formats range from images, codes, tables, graphs, to audio. Such methods offer a promising solution to important concerns such as factuality, reasoning, interpretability, and robustness. By providing an in-depth review, this survey is expected to provide scholars with a deeper understanding of the methods' applications and encourage them to adapt existing techniques to the fast-growing field of LLMs.
&lt;/p&gt;</description></item></channel></rss>