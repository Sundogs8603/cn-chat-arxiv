<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#23545;&#24694;&#24847;&#35328;&#35770;&#26816;&#27979;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#26041;&#27861;&#22312;&#19978;&#19979;&#25991;&#24863;&#30693;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#38480;&#21046;&#12290;&#32780;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#28508;&#21147;&#20316;&#20026;&#19978;&#19979;&#25991;&#24863;&#30693;&#24694;&#24847;&#35328;&#35770;&#26816;&#27979;&#30340;&#30693;&#35782;&#24211;&#65292;&#20294;&#30446;&#21069;&#32570;&#20047;&#26377;&#25928;&#25552;&#31034;&#36825;&#20123;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.03346</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#23545;&#24694;&#24847;&#35328;&#35770;&#26816;&#27979;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
An Investigation of Large Language Models for Real-World Hate Speech Detection. (arXiv:2401.03346v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03346
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#23545;&#24694;&#24847;&#35328;&#35770;&#26816;&#27979;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#26041;&#27861;&#22312;&#19978;&#19979;&#25991;&#24863;&#30693;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#38480;&#21046;&#12290;&#32780;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#28508;&#21147;&#20316;&#20026;&#19978;&#19979;&#25991;&#24863;&#30693;&#24694;&#24847;&#35328;&#35770;&#26816;&#27979;&#30340;&#30693;&#35782;&#24211;&#65292;&#20294;&#30446;&#21069;&#32570;&#20047;&#26377;&#25928;&#25552;&#31034;&#36825;&#20123;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24694;&#24847;&#35328;&#35770;&#24050;&#32463;&#25104;&#20026;&#22256;&#25200;&#25105;&#20204;&#31038;&#20132;&#31354;&#38388;&#30340;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#12290;&#34429;&#28982;&#22312;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#19978;&#24050;&#32463;&#20570;&#20986;&#20102;&#19968;&#20123;&#26174;&#33879;&#30340;&#21162;&#21147;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#22312;&#26377;&#25928;&#26816;&#27979;&#22312;&#32447;&#24694;&#24847;&#35328;&#35770;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#26174;&#33879;&#38480;&#21046;&#12290;&#29616;&#26377;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#38480;&#21046;&#26159;&#24694;&#24847;&#35328;&#35770;&#26816;&#27979;&#26159;&#19968;&#20010;&#39640;&#24230;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#32780;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#23436;&#20840;&#25429;&#25417;&#24694;&#24847;&#35328;&#35770;&#30340;&#19978;&#19979;&#25991;&#20197;&#36827;&#34892;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#26368;&#36817;&#65292;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20960;&#20010;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;LLMs&#32463;&#36807;&#22823;&#37327;&#30340;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35757;&#32451;&#65292;&#20351;&#20854;&#33021;&#22815;&#25484;&#25569;&#22797;&#26434;&#30340;&#19978;&#19979;&#25991;&#32454;&#33410;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#21487;&#20197;&#29992;&#20316;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#24694;&#24847;&#35328;&#35770;&#26816;&#27979;&#30340;&#30693;&#35782;&#24211;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;LLMs&#26816;&#27979;&#24694;&#24847;&#35328;&#35770;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#26159;&#27809;&#26377;&#20851;&#20110;&#26377;&#25928;&#25552;&#31034;LLMs&#36827;&#34892;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#24694;&#24847;&#35328;&#35770;&#26816;&#27979;&#30340;&#30740;&#31350;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#30740;&#31350;&#65292;&#35843;&#26597;&#24694;&#24847;&#35328;&#35770;&#26816;&#27979;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hate speech has emerged as a major problem plaguing our social spaces today. While there have been significant efforts to address this problem, existing methods are still significantly limited in effectively detecting hate speech online. A major limitation of existing methods is that hate speech detection is a highly contextual problem, and these methods cannot fully capture the context of hate speech to make accurate predictions. Recently, large language models (LLMs) have demonstrated state-of-the-art performance in several natural language tasks. LLMs have undergone extensive training using vast amounts of natural language data, enabling them to grasp intricate contextual details. Hence, they could be used as knowledge bases for context-aware hate speech detection. However, a fundamental problem with using LLMs to detect hate speech is that there are no studies on effectively prompting LLMs for context-aware hate speech detection. In this study, we conduct a large-scale study of hat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;PIXAR&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20687;&#32032;&#33258;&#22238;&#24402;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#33258;&#30001;&#24418;&#24335;&#30340;&#25991;&#26412;&#20316;&#20026;&#22270;&#20687;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#39044;&#23450;&#20041;&#30340;&#35789;&#27719;&#34920;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#23545;&#25239;&#24615;&#39044;&#35757;&#32451;&#26041;&#27861;&#26469;&#35299;&#20915;&#29983;&#25104;&#38750;&#27169;&#31946;&#25991;&#26412;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.03321</link><description>&lt;p&gt;
PIXAR&#65306;&#20687;&#32032;&#31354;&#38388;&#20013;&#30340;&#33258;&#22238;&#24402;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
PIXAR: Auto-Regressive Language Modeling in Pixel Space. (arXiv:2401.03321v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03321
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;PIXAR&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20687;&#32032;&#33258;&#22238;&#24402;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#33258;&#30001;&#24418;&#24335;&#30340;&#25991;&#26412;&#20316;&#20026;&#22270;&#20687;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#39044;&#23450;&#20041;&#30340;&#35789;&#27719;&#34920;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#23545;&#25239;&#24615;&#39044;&#35757;&#32451;&#26041;&#27861;&#26469;&#35299;&#20915;&#29983;&#25104;&#38750;&#27169;&#31946;&#25991;&#26412;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#26174;&#31034;&#21487;&#20197;&#26500;&#24314;&#22522;&#20110;&#20687;&#32032;&#34920;&#31034;&#30340;&#24320;&#25918;&#35789;&#27719;&#37327;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#36825;&#20123;&#27169;&#22411;&#20197;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#30340;&#24418;&#24335;&#65292;&#37325;&#26500;&#36974;&#34109;&#30340;&#22270;&#20687;&#25991;&#26412;&#34917;&#19969;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20110;&#20687;&#32032;&#30340;LLMs&#20165;&#38480;&#20110;&#33258;&#32534;&#30721;&#20219;&#21153;&#65292;&#26080;&#27861;&#29983;&#25104;&#26032;&#30340;&#25991;&#26412;&#20316;&#20026;&#22270;&#20687;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#19981;&#33021;&#29992;&#20110;&#24320;&#25918;&#24335;&#22238;&#31572;&#25110;&#29983;&#25104;&#24335;&#35821;&#35328;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20811;&#26381;&#20102;&#36825;&#19968;&#38480;&#21046;&#65292;&#24182;&#24341;&#20837;&#20102;PIXAR&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19981;&#38656;&#35201;&#39044;&#23450;&#20041;&#35789;&#27719;&#34920;&#30340;&#20687;&#32032;&#33258;&#22238;&#24402;LLM&#65292;&#29992;&#20110;&#36755;&#20837;&#21644;&#36755;&#20986;&#25991;&#26412;&#12290;PIXAR&#21482;&#26377;&#19968;&#20010;&#35299;&#30721;&#22120;&#65292;&#21487;&#20197;&#22238;&#31572;&#33258;&#30001;&#24418;&#24335;&#30340;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#19988;&#22312;&#25991;&#26412;&#34920;&#31034;&#23398;&#20064;&#24615;&#33021;&#19978;&#19982;&#20197;&#21069;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#25345;&#24179;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#20197;&#33258;&#22238;&#24402;&#26041;&#24335;&#29983;&#25104;&#38750;&#27169;&#31946;&#25991;&#26412;&#20316;&#20026;&#22270;&#20687;&#30340;&#25361;&#25112;&#65292;&#24182;&#23558;&#20854;&#19982;&#36890;&#24120;&#30340;&#26368;&#22823;&#20284;&#28982;&#30446;&#26631;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#23545;&#25239;&#24615;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works showed the possibility of building open-vocabulary large language models (LLMs) that directly operate on pixel representations and are implemented as encoder-decoder models that reconstruct masked image patches of rendered text. However, these pixel-based LLMs are limited to autoencoding tasks and cannot generate new text as images. As such, they cannot be used for open-answer or generative language tasks. In this work, we overcome this limitation and introduce PIXAR, the first pixel-based autoregressive LLM that does not rely on a pre-defined vocabulary for both input and output text. Consisting of only a decoder, PIXAR can answer free-form generative tasks while keeping the text representation learning performance on par with previous encoder-decoder models. Furthermore, we highlight the challenges to autoregressively generate non-blurred text as images and link this to the usual maximum likelihood objective. We propose a simple adversarial pretraining that significantly
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26469;&#25552;&#39640;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#24615;&#33021;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#24052;&#27931;&#21452;&#32990;&#32974;&#25439;&#22833;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#19978;&#19979;&#25991;&#22686;&#24378;&#26469;&#25552;&#21319;&#24615;&#33021;&#65292;&#32780;&#19981;&#38656;&#35201;&#26126;&#30830;&#22320;&#22686;&#21152;&#25968;&#25454;&#25110;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#23884;&#20837;&#12290;</title><link>http://arxiv.org/abs/2401.03314</link><description>&lt;p&gt;
&#25552;&#21319;&#23545;&#27604;&#24230;&#30340;&#19978;&#19979;&#25991;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Enhancing Context Through Contrast. (arXiv:2401.03314v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03314
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26469;&#25552;&#39640;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#24615;&#33021;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#24052;&#27931;&#21452;&#32990;&#32974;&#25439;&#22833;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#19978;&#19979;&#25991;&#22686;&#24378;&#26469;&#25552;&#21319;&#24615;&#33021;&#65292;&#32780;&#19981;&#38656;&#35201;&#26126;&#30830;&#22320;&#22686;&#21152;&#25968;&#25454;&#25110;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#21463;&#30410;&#20110;&#35821;&#20041;&#20016;&#23500;&#30340;&#34920;&#31034;&#12290;&#36890;&#36807;&#35821;&#35328;&#24314;&#27169;&#21644;&#23545;&#27604;&#23398;&#20064;&#20351;&#29992;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#30446;&#26631;&#65292;&#24050;&#32463;&#23454;&#29616;&#20102;&#23545;&#36825;&#31181;&#34920;&#31034;&#30340;&#22823;&#24133;&#36827;&#23637;&#12290;&#35821;&#35328;&#24314;&#27169;&#30340;&#20381;&#36182;&#24615;&#20351;&#24471;&#22312;&#23398;&#20064;&#34920;&#31034;&#30340;&#36890;&#29992;&#24615;&#21644;&#27169;&#22411;&#22312;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#12290;&#34429;&#28982;&#23545;&#27604;&#23398;&#20064;&#25913;&#36827;&#20102;&#24615;&#33021;&#65292;&#20294;&#20854;&#25104;&#21151;&#19981;&#33021;&#20165;&#24402;&#22240;&#20110;&#20114;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19978;&#19979;&#25991;&#22686;&#24378;&#27493;&#39588;&#65292;&#36890;&#36807;&#20351;&#29992;&#24052;&#27931;&#21452;&#32990;&#32974;&#25439;&#22833;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#26469;&#25552;&#39640;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#24615;&#33021;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#19981;&#26159;&#26174;&#24335;&#22320;&#22686;&#21152;&#25968;&#25454;&#65292;&#32780;&#26159;&#23558;&#35821;&#35328;&#35270;&#20026;&#38544;&#21547;&#30340;&#22686;&#24378;&#65292;&#28040;&#38500;&#20102;&#30772;&#22351;&#35821;&#20041;&#20449;&#24687;&#30340;&#39118;&#38505;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20250;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#23884;&#20837;&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#20219;&#20309;&#19968;&#32452;&#39044;&#35757;&#32451;&#30340;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural machine translation benefits from semantically rich representations. Considerable progress in learning such representations has been achieved by language modelling and mutual information maximization objectives using contrastive learning. The language-dependent nature of language modelling introduces a trade-off between the universality of the learned representations and the model's performance on the language modelling tasks. Although contrastive learning improves performance, its success cannot be attributed to mutual information alone. We propose a novel Context Enhancement step to improve performance on neural machine translation by maximizing mutual information using the Barlow Twins loss. Unlike other approaches, we do not explicitly augment the data but view languages as implicit augmentations, eradicating the risk of disrupting semantic information. Further, our method does not learn embeddings from scratch and can be generalised to any set of pre-trained embeddings. Fin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35270;&#35273;&#36328;&#39046;&#22495;&#23398;&#20064;&#22120;&#65288;LLaVO&#65289;&#65292;&#36890;&#36807;&#23558;&#22270;&#20687;&#36716;&#25442;&#20026;&#25991;&#26412;&#25551;&#36848;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#22312;&#36328;&#39046;&#22495;&#20219;&#21153;&#20013;&#20943;&#23569;&#39046;&#22495;&#36716;&#31227;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.03253</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35270;&#35273;&#36328;&#39046;&#22495;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Visual Cross-Domain Learners. (arXiv:2401.03253v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03253
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35270;&#35273;&#36328;&#39046;&#22495;&#23398;&#20064;&#22120;&#65288;LLaVO&#65289;&#65292;&#36890;&#36807;&#23558;&#22270;&#20687;&#36716;&#25442;&#20026;&#25991;&#26412;&#25551;&#36848;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#22312;&#36328;&#39046;&#22495;&#20219;&#21153;&#20013;&#20943;&#23569;&#39046;&#22495;&#36716;&#31227;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21462;&#24471;&#30340;&#26368;&#26032;&#36827;&#23637;&#20381;&#36182;&#20110;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#20551;&#35774;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#38754;&#23545;&#39046;&#22495;&#36716;&#31227;&#26102;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#20197;&#19978;&#38382;&#39064;&#65292;&#36328;&#39046;&#22495;&#23398;&#20064;&#26088;&#22312;&#25552;&#21462;&#39046;&#22495;&#19981;&#21464;&#30340;&#30693;&#35782;&#65292;&#20943;&#23569;&#35757;&#32451;&#19982;&#27979;&#35797;&#25968;&#25454;&#20043;&#38388;&#30340;&#39046;&#22495;&#36716;&#31227;&#12290;&#28982;&#32780;&#65292;&#22312;&#35270;&#35273;&#36328;&#39046;&#22495;&#23398;&#20064;&#20013;&#65292;&#20256;&#32479;&#26041;&#27861;&#20165;&#20851;&#27880;&#22270;&#20687;&#27169;&#24577;&#65292;&#24573;&#35270;&#20102;&#20351;&#29992;&#25991;&#26412;&#27169;&#24577;&#26469;&#32531;&#35299;&#39046;&#22495;&#36716;&#31227;&#30340;&#20316;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35270;&#35273;&#36328;&#39046;&#22495;&#23398;&#20064;&#22120;&#65288;LLaVO&#65289;&#12290;LLaVO&#20351;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#23558;&#22270;&#20687;&#36716;&#25442;&#20026;&#35814;&#32454;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#28982;&#21518;&#65292;&#22312;&#32463;&#36807;&#35774;&#35745;&#30340;&#25351;&#23548;&#27169;&#26495;&#29983;&#25104;&#30340;&#28304;&#22495;/&#30446;&#26631;&#22495;&#30340;&#25991;&#26412;&#25551;&#36848;&#19978;&#65292;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#39046;&#22495;&#27867;&#21270;&#21644;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#35774;&#32622;&#19979;&#36827;&#34892;&#30340;&#21508;&#31181;&#36328;&#39046;&#22495;&#20219;&#21153;&#30340;&#24191;&#27867;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances achieved by deep learning models rely on the independent and identically distributed assumption, hindering their applications in real-world scenarios with domain shifts. To address the above issues, cross-domain learning aims at extracting domain-invariant knowledge to reduce the domain shift between training and testing data. However, in visual cross-domain learning, traditional methods concentrate solely on the image modality, neglecting the use of the text modality to alleviate the domain shift. In this work, we propose Large Language models as Visual cross-dOmain learners (LLaVO). LLaVO uses vision-language models to convert images into detailed textual descriptions. A large language model is then finetuned on textual descriptions of the source/target domain generated by a designed instruction template. Extensive experimental results on various cross-domain tasks under the domain generalization and unsupervised domain adaptation settings have demonstrated the effect
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#24402;&#32435;&#20027;&#39064;&#39281;&#21644;&#24230;&#20316;&#20026;&#34913;&#37327;&#24402;&#32435;&#20027;&#39064;&#20998;&#26512;&#19982;LLMs&#26377;&#25928;&#24615;&#30340;&#28508;&#22312;&#25351;&#26631;&#36827;&#34892;&#20102;&#21453;&#24605;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21512;&#25104;&#24230;&#37327;&#21021;&#27493;&#20027;&#39064;&#39281;&#21644;&#24230;&#30340;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2401.03239</link><description>&lt;p&gt;
&#23545;&#24402;&#32435;&#20027;&#39064;&#39281;&#21644;&#24230;&#20316;&#20026;&#34913;&#37327;&#24402;&#32435;&#20027;&#39064;&#20998;&#26512;&#19982;LLMs&#26377;&#25928;&#24615;&#30340;&#28508;&#22312;&#25351;&#26631;&#30340;&#21453;&#24605;
&lt;/p&gt;
&lt;p&gt;
Reflections on Inductive Thematic Saturation as a potential metric for measuring the validity of an inductive Thematic Analysis with LLMs. (arXiv:2401.03239v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03239
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#24402;&#32435;&#20027;&#39064;&#39281;&#21644;&#24230;&#20316;&#20026;&#34913;&#37327;&#24402;&#32435;&#20027;&#39064;&#20998;&#26512;&#19982;LLMs&#26377;&#25928;&#24615;&#30340;&#28508;&#22312;&#25351;&#26631;&#36827;&#34892;&#20102;&#21453;&#24605;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21512;&#25104;&#24230;&#37327;&#21021;&#27493;&#20027;&#39064;&#39281;&#21644;&#24230;&#30340;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#39281;&#21644;&#24230;&#21644;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#20027;&#39064;&#20998;&#26512;&#65288;TA&#65289;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#21453;&#24605;&#12290;&#26412;&#25991;&#25552;&#20986;&#21021;&#27493;&#20027;&#39064;&#39281;&#21644;&#24230;&#65288;ITS&#65289;&#21487;&#20197;&#29992;&#20316;&#35780;&#20272;LLM&#19979;TA&#30340;&#19968;&#37096;&#20998;&#20107;&#21153;&#26377;&#25928;&#24615;&#30340;&#25351;&#26631;&#65292;&#37325;&#28857;&#26159;&#21021;&#22987;&#32534;&#30721;&#12290;&#26412;&#25991;&#21576;&#29616;&#20102;&#20004;&#20010;&#19981;&#21516;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#21021;&#22987;&#32534;&#30721;&#65292;&#24182;&#21453;&#24605;LLM&#22312;&#32534;&#30721;&#36807;&#31243;&#20013;&#36798;&#21040;&#26576;&#31181;&#24418;&#24335;&#30340;&#20998;&#26512;&#39281;&#21644;&#24230;&#12290;&#26412;&#25991;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23548;&#33268;&#20102;&#21019;&#24314;&#20102;&#20004;&#20010;&#32534;&#30721;&#25163;&#20876;&#65292;&#19968;&#20010;&#21253;&#25324;&#32047;&#31215;&#21021;&#22987;&#32534;&#30721;&#30340;&#24635;&#25968;&#65292;&#21478;&#19968;&#20010;&#21253;&#25324;&#21807;&#19968;&#32534;&#30721;&#30340;&#24635;&#25968;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#31616;&#21333;&#25968;&#23398;&#35745;&#31639;&#65288;&#20351;&#29992;&#32047;&#31215;&#32534;&#30721;&#21644;&#21807;&#19968;&#32534;&#30721;&#20043;&#38388;&#30340;&#26012;&#29575;&#27604;&#65289;&#26469;&#32508;&#21512;&#24230;&#37327;ITS&#30340;&#25351;&#26631;&#12290;&#26412;&#25991;&#23545;&#20110;&#25506;&#32034;&#22914;&#20309;&#20351;&#29992;LLMs&#36827;&#34892;&#23450;&#24615;&#20998;&#26512;&#30340;&#21021;&#27493;&#30740;&#31350;&#26377;&#25152;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a set of reflections on saturation and the use of Large Language Models (LLMs) for performing Thematic Analysis (TA). The paper suggests that initial thematic saturation (ITS) could be used as a metric to assess part of the transactional validity of TA with LLM, focusing on the initial coding. The paper presents the initial coding of two datasets of different sizes, and it reflects on how the LLM reaches some form of analytical saturation during the coding. The procedure proposed in this work leads to the creation of two codebooks, one comprising the total cumulative initial codes and the other the total unique codes. The paper proposes a metric to synthetically measure ITS using a simple mathematical calculation employing the ratio between slopes of cumulative codes and unique codes. The paper contributes to the initial body of work exploring how to perform qualitative analysis with LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20107;&#23454;&#24615;&#24187;&#35273;&#30340;&#26816;&#27979;&#12289;&#26469;&#28304;&#21644;&#32531;&#35299;&#26041;&#27861;&#12290;&#30740;&#31350;&#26500;&#24314;&#20102;&#26032;&#30340;&#24187;&#35273;&#22522;&#20934;HaluluEval 2.0&#65292;&#24182;&#35774;&#35745;&#20102;&#31616;&#21333;&#26377;&#25928;&#30340;LLM&#24187;&#35273;&#26816;&#27979;&#26041;&#27861;&#12290;&#36890;&#36807;&#28145;&#20837;&#20998;&#26512;LLMs&#30340;&#35757;&#32451;&#21644;&#21033;&#29992;&#38454;&#27573;&#65292;&#24182;&#30740;&#31350;&#23548;&#33268;&#24187;&#35273;&#30340;&#28508;&#22312;&#22240;&#32032;&#65292;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#21487;&#34892;&#30340;&#20943;&#36731;&#24187;&#35273;&#30340;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2401.03205</link><description>&lt;p&gt;
&#40657;&#26263;&#20043;&#21518;&#30340;&#40654;&#26126;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20851;&#20110;&#20107;&#23454;&#24615;&#24187;&#35273;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Dawn After the Dark: An Empirical Study on Factuality Hallucination in Large Language Models. (arXiv:2401.03205v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03205
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20107;&#23454;&#24615;&#24187;&#35273;&#30340;&#26816;&#27979;&#12289;&#26469;&#28304;&#21644;&#32531;&#35299;&#26041;&#27861;&#12290;&#30740;&#31350;&#26500;&#24314;&#20102;&#26032;&#30340;&#24187;&#35273;&#22522;&#20934;HaluluEval 2.0&#65292;&#24182;&#35774;&#35745;&#20102;&#31616;&#21333;&#26377;&#25928;&#30340;LLM&#24187;&#35273;&#26816;&#27979;&#26041;&#27861;&#12290;&#36890;&#36807;&#28145;&#20837;&#20998;&#26512;LLMs&#30340;&#35757;&#32451;&#21644;&#21033;&#29992;&#38454;&#27573;&#65292;&#24182;&#30740;&#31350;&#23548;&#33268;&#24187;&#35273;&#30340;&#28508;&#22312;&#22240;&#32032;&#65292;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#21487;&#34892;&#30340;&#20943;&#36731;&#24187;&#35273;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26102;&#20195;&#65292;&#24187;&#35273;&#65288;&#21363;&#29983;&#25104;&#38169;&#35823;&#20107;&#23454;&#20869;&#23481;&#30340;&#20542;&#21521;&#65289;&#32473;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#20449;&#20219;&#21644;&#21487;&#38752;&#22320;&#37096;&#32626;LLMs&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;LLM&#24187;&#35273;&#38382;&#39064;&#65292;&#38656;&#35201;&#28145;&#20837;&#30740;&#31350;&#19977;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#22914;&#20309;&#26816;&#27979;&#24187;&#35273;&#65288;&#26816;&#27979;&#65289;&#65292;LLMs&#20026;&#20160;&#20040;&#20250;&#20135;&#29983;&#24187;&#35273;&#65288;&#26469;&#28304;&#65289;&#65292;&#20197;&#21450;&#22914;&#20309;&#20943;&#36731;LLM&#24187;&#35273;&#65288;&#32531;&#35299;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#39033;&#31995;&#32479;&#30340;LLM&#24187;&#35273;&#23454;&#35777;&#30740;&#31350;&#65292;&#19987;&#27880;&#20110;&#24187;&#35273;&#30340;&#26816;&#27979;&#12289;&#26469;&#28304;&#21644;&#32531;&#35299;&#19977;&#20010;&#26041;&#38754;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#24187;&#35273;&#22522;&#20934;HaluluEval 2.0&#65292;&#24182;&#20026;LLM&#24187;&#35273;&#35774;&#35745;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26816;&#27979;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;LLMs&#30340;&#19981;&#21516;&#35757;&#32451;&#25110;&#21033;&#29992;&#38454;&#27573;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#24191;&#27867;&#30740;&#31350;&#20102;&#23548;&#33268;LLM&#24187;&#35273;&#30340;&#28508;&#22312;&#22240;&#32032;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23454;&#26045;&#24182;&#26816;&#39564;&#20102;&#19968;&#31995;&#21015;&#24191;&#27867;&#20351;&#29992;&#30340;&#25216;&#26415;&#26469;&#20943;&#36731;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of large language models (LLMs), hallucination (i.e., the tendency to generate factually incorrect content) poses great challenge to trustworthy and reliable deployment of LLMs in real-world applications. To tackle the LLM hallucination, three key questions should be well studied: how to detect hallucinations (detection), why do LLMs hallucinate (source), and what can be done to mitigate them (mitigation). To address these challenges, this work presents a systematic empirical study on LLM hallucination, focused on the the three aspects of hallucination detection, source and mitigation. Specially, we construct a new hallucination benchmark HaluEval 2.0, and designs a simple yet effective detection method for LLM hallucination. Furthermore, we zoom into the different training or utilization stages of LLMs and extensively analyze the potential factors that lead to the LLM hallucination. Finally, we implement and examine a series of widely used techniques to mitigate the halluci
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#35821;&#35328;&#34917;&#19969;&#31070;&#32463;&#20803;&#26469;&#35299;&#20915;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#36328;&#35821;&#35328;&#30693;&#35782;&#21516;&#27493;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.03190</link><description>&lt;p&gt;
MPN: &#21033;&#29992;&#22810;&#35821;&#35328;&#34917;&#19969;&#31070;&#32463;&#20803;&#36827;&#34892;&#36328;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
MPN: Leveraging Multilingual Patch Neuron for Cross-lingual Model Editing. (arXiv:2401.03190v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03190
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#35821;&#35328;&#34917;&#19969;&#31070;&#32463;&#20803;&#26469;&#35299;&#20915;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#36328;&#35821;&#35328;&#30693;&#35782;&#21516;&#27493;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#32534;&#30721;&#20102;&#22823;&#37327;&#30340;&#20107;&#23454;&#30693;&#35782;&#65292;&#20294;&#30001;&#20110;&#22806;&#37096;&#20449;&#24687;&#30340;&#19981;&#26029;&#21464;&#21270;&#65292;&#23427;&#20204;&#32463;&#24120;&#36807;&#26102;&#12290;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#26159;&#21033;&#29992;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#20197;&#39640;&#25928;&#30340;&#26041;&#24335;&#26356;&#26032;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#29616;&#26377;&#30340;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#23616;&#38480;&#20110;&#21333;&#35821;&#26694;&#26550;&#65292;&#22240;&#27492;&#26080;&#27861;&#35299;&#20915;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#36328;&#35821;&#35328;&#30693;&#35782;&#21516;&#27493;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21363;&#35757;&#32451;&#22810;&#35821;&#35328;&#34917;&#19969;&#31070;&#32463;&#20803;&#26469;&#23384;&#20648;&#36328;&#35821;&#35328;&#30693;&#35782;&#12290;&#23427;&#21487;&#20197;&#36731;&#26494;&#36866;&#24212;&#29616;&#26377;&#26041;&#27861;&#65292;&#22686;&#24378;&#23427;&#20204;&#30340;&#36328;&#35821;&#35328;&#32534;&#36753;&#33021;&#21147;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#20351;&#29992;XNLI&#25968;&#25454;&#38598;&#21644;&#33258;&#24314;&#30340;XFEVER&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#36328;&#35821;&#35328;&#32534;&#36753;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#24615;&#33021;&#65292;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models are known for encoding a vast amount of factual knowledge, but they often becomes outdated due to the ever-changing nature of external information. A promising solution to this challenge is the utilization of model editing methods to update the knowledge in an efficient manner. However, the majority of existing model editing techniques are limited to monolingual frameworks, thus failing to address the crucial issue of cross-lingual knowledge synchronization for multilingual models. To tackle this problem, we propose a simple yet effective method that trains multilingual patch neuron to store cross-lingual knowledge. It can be easily adapted to existing approaches to enhance their cross-lingual editing capabilities. To evaluate our method, we conduct experiments using both the XNLI dataset and a self-constructed XFEVER dataset. Experimental results demonstrate that our proposed method achieves improved performance in cross-lingual editing tasks without requiring ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;{\delta}-CAUSAL&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#30740;&#31350;&#22240;&#26524;&#25512;&#29702;&#20013;&#21487;&#24223;&#38500;&#24615;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#25351;&#20986;&#29616;&#26377;&#30340;&#22240;&#26524;&#24378;&#24230;&#24230;&#37327;&#26080;&#27861;&#22312;&#21487;&#24223;&#38500;&#29615;&#22659;&#20013;&#20934;&#30830;&#35780;&#20272;&#22240;&#26524;&#20851;&#31995;&#30340;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2401.03183</link><description>&lt;p&gt;
{\delta}-CAUSAL&#65306;&#25506;&#32034;&#22240;&#26524;&#25512;&#29702;&#20013;&#30340;&#21487;&#24223;&#38500;&#24615;
&lt;/p&gt;
&lt;p&gt;
{\delta}-CAUSAL: Exploring Defeasibility in Causal Reasoning. (arXiv:2401.03183v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03183
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;{\delta}-CAUSAL&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#30740;&#31350;&#22240;&#26524;&#25512;&#29702;&#20013;&#21487;&#24223;&#38500;&#24615;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#25351;&#20986;&#29616;&#26377;&#30340;&#22240;&#26524;&#24378;&#24230;&#24230;&#37327;&#26080;&#27861;&#22312;&#21487;&#24223;&#38500;&#29615;&#22659;&#20013;&#20934;&#30830;&#35780;&#20272;&#22240;&#26524;&#20851;&#31995;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25512;&#29702;&#20013;&#30340;&#21487;&#24223;&#38500;&#24615;&#24847;&#21619;&#30528;&#22240;&#26524;&#20851;&#31995;&#21487;&#20197;&#34987;&#21152;&#24378;&#25110;&#21066;&#24369;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#22240;&#26524;&#20851;&#31995;&#30340;&#24378;&#24230;&#24212;&#35813;&#38543;&#30528;&#21152;&#20837;&#25903;&#25345;&#32773;&#25110;&#39539;&#26021;&#32773;&#32780;&#22686;&#21152;&#25110;&#20943;&#23569;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#24573;&#35270;&#20102;&#22240;&#26524;&#25512;&#29702;&#20013;&#30340;&#21487;&#24223;&#38500;&#24615;&#65292;&#24182;&#26410;&#22312;&#21487;&#24223;&#38500;&#29615;&#22659;&#20013;&#35780;&#20272;&#29616;&#26377;&#30340;&#22240;&#26524;&#24378;&#24230;&#24230;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#30740;&#31350;&#22240;&#26524;&#25512;&#29702;&#20013;&#30340;&#21487;&#24223;&#38500;&#24615;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;{\delta}-CAUSAL&#12290;{\delta}-CAUSAL&#21253;&#25324;&#32422;11K&#20010;&#28085;&#30422;&#21313;&#20010;&#39046;&#22495;&#30340;&#20107;&#20214;&#65292;&#20854;&#20013;&#21253;&#25324;&#25903;&#25345;&#32773;&#21644;&#39539;&#26021;&#32773;&#30340;&#21487;&#24223;&#38500;&#22240;&#26524;&#20851;&#31995;&#23545;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#24403;&#21069;&#30340;&#22240;&#26524;&#24378;&#24230;&#24230;&#37327;&#26080;&#27861;&#21453;&#26144;{\delta}-CAUSAL&#20013;&#30340;&#25903;&#25345;&#32773;&#25110;&#39539;&#26021;&#32773;&#21152;&#20837;&#21518;&#30340;&#22240;&#26524;&#24378;&#24230;&#21464;&#21270;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CESAR&#65288;Causal Embedding aSsociation with Attention Rating&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Defeasibility in causal reasoning implies that the causal relationship between cause and effect can be strengthened or weakened. Namely, the causal strength between cause and effect should increase or decrease with the incorporation of strengthening arguments (supporters) or weakening arguments (defeaters), respectively. However, existing works ignore defeasibility in causal reasoning and fail to evaluate existing causal strength metrics in defeasible settings. In this work, we present {\delta}-CAUSAL, the first benchmark dataset for studying defeasibility in causal reasoning. {\delta}-CAUSAL includes around 11K events spanning ten domains, featuring defeasible causality pairs, i.e., cause-effect pairs accompanied by supporters and defeaters. We further show current causal strength metrics fail to reflect the change of causal strength with the incorporation of supporters or defeaters in {\delta}-CAUSAL. To this end, we propose CESAR (Causal Embedding aSsociation with Attention Rating),
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#21512;&#25512;&#29702;&#30340;&#30142;&#30149;&#38382;&#31572;&#31995;&#32479;&#65292;&#36890;&#36807;&#32467;&#21512;&#35821;&#35328;&#27169;&#22411;&#21644;&#30693;&#35782;&#22270;&#35889;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#22238;&#31572;&#26222;&#36890;&#29992;&#25143;&#30340;&#20581;&#24247;&#30456;&#20851;&#38382;&#39064;&#24182;&#20943;&#36731;&#21307;&#30103;&#20445;&#20581;&#19987;&#19994;&#20154;&#21592;&#30340;&#36127;&#25285;&#12290;</title><link>http://arxiv.org/abs/2401.03181</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#32852;&#21512;&#25512;&#29702;&#30340;&#30142;&#30149;&#38382;&#31572;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A Joint-Reasoning based Disease Q&amp;A System. (arXiv:2401.03181v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03181
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#21512;&#25512;&#29702;&#30340;&#30142;&#30149;&#38382;&#31572;&#31995;&#32479;&#65292;&#36890;&#36807;&#32467;&#21512;&#35821;&#35328;&#27169;&#22411;&#21644;&#30693;&#35782;&#22270;&#35889;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#22238;&#31572;&#26222;&#36890;&#29992;&#25143;&#30340;&#20581;&#24247;&#30456;&#20851;&#38382;&#39064;&#24182;&#20943;&#36731;&#21307;&#30103;&#20445;&#20581;&#19987;&#19994;&#20154;&#21592;&#30340;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#38382;&#31572;&#65288;QA&#65289;&#21161;&#25163;&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#30456;&#20851;&#25216;&#26415;&#20174;&#22810;&#20010;&#20449;&#24687;&#28304;&#21512;&#25104;&#20449;&#24687;&#26469;&#22238;&#31572;&#26222;&#36890;&#29992;&#25143;&#30340;&#20581;&#24247;&#30456;&#20851;&#38382;&#39064;&#12290;&#23427;&#20204;&#21487;&#20197;&#20316;&#20026;&#37325;&#35201;&#24037;&#20855;&#26469;&#32531;&#35299;&#35823;&#23548;&#12289;&#20449;&#24687;&#36807;&#36733;&#21644;&#21307;&#23398;&#26415;&#35821;&#22797;&#26434;&#24615;&#38382;&#39064;&#65292;&#20174;&#32780;&#28385;&#36275;&#26222;&#36890;&#29992;&#25143;&#30340;&#20449;&#24687;&#38656;&#27714;&#24182;&#20943;&#36731;&#21307;&#30103;&#20445;&#20581;&#19987;&#19994;&#20154;&#21592;&#30340;&#36127;&#25285;&#12290;QA&#31995;&#32479;&#36890;&#24120;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#25110;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#65292;&#23613;&#31649;&#36825;&#20004;&#31181;&#26041;&#27861;&#21487;&#20197;&#20114;&#34917;&#12290;&#22522;&#20110;LM&#30340;QA&#31995;&#32479;&#25797;&#38271;&#29702;&#35299;&#22797;&#26434;&#38382;&#39064;&#24182;&#25552;&#20379;&#21512;&#36866;&#30340;&#31572;&#26696;&#65292;&#20294;&#26131;&#20110;&#20986;&#29616;&#20107;&#23454;&#38169;&#35823;&#12290;&#22522;&#20110;KG&#30340;QA&#31995;&#32479;&#33021;&#22815;&#24456;&#22909;&#22320;&#34920;&#31034;&#20107;&#23454;&#65292;&#20294;&#22823;&#22810;&#25968;&#20165;&#38480;&#20110;&#22238;&#31572;&#24050;&#39044;&#20808;&#21019;&#24314;&#27169;&#26495;&#30340;&#31616;&#30701;&#38382;&#39064;&#12290;&#34429;&#28982;&#19968;&#20123;&#30740;&#31350;&#24050;&#32463;&#32852;&#21512;&#20351;&#29992;&#20102;LM&#21644;KG&#26041;&#27861;&#26469;&#36827;&#34892;&#22522;&#20110;&#25991;&#26412;&#30340;QA&#65292;&#20294;&#36825;&#26159;&#29992;&#20110;&#22238;&#31572;&#22810;&#39033;&#36873;&#25321;&#39064;&#12290;&#29616;&#26377;&#30340;QA&#31995;&#32479;&#20063;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical question answer (QA) assistants respond to lay users' health-related queries by synthesizing information from multiple sources using natural language processing and related techniques. They can serve as vital tools to alleviate issues of misinformation, information overload, and complexity of medical language, thus addressing lay users' information needs while reducing the burden on healthcare professionals. QA systems, the engines of such assistants, have typically used either language models (LMs) or knowledge graphs (KG), though the approaches could be complementary. LM-based QA systems excel at understanding complex questions and providing well-formed answers, but are prone to factual mistakes. KG-based QA systems, which represent facts well, are mostly limited to answering short-answer questions with pre-created templates. While a few studies have jointly used LM and KG approaches for text-based QA, this was done to answer multiple-choice questions. Extant QA systems also 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;-&#35270;&#39057;&#26816;&#32034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22810;&#27169;&#24577;&#36229;&#22270;&#36827;&#34892;n&#20803;&#30456;&#20851;&#24314;&#27169;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#25991;&#26412;&#24615;&#36136;&#21644;&#35270;&#39057;&#20869;&#23481;&#30340;&#35821;&#20041;&#24046;&#36317;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.03177</link><description>&lt;p&gt;
&#25991;&#26412;-&#35270;&#39057;&#26816;&#32034;&#36890;&#36807;&#21464;&#20998;&#22810;&#27169;&#24577;&#36229;&#22270;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Text-Video Retrieval via Variational Multi-Modal Hypergraph Networks. (arXiv:2401.03177v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03177
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;-&#35270;&#39057;&#26816;&#32034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22810;&#27169;&#24577;&#36229;&#22270;&#36827;&#34892;n&#20803;&#30456;&#20851;&#24314;&#27169;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#25991;&#26412;&#24615;&#36136;&#21644;&#35270;&#39057;&#20869;&#23481;&#30340;&#35821;&#20041;&#24046;&#36317;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;-&#35270;&#39057;&#26816;&#32034;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#26681;&#25454;&#25991;&#26412;&#26597;&#35810;&#35782;&#21035;&#30456;&#20851;&#35270;&#39057;&#12290;&#19982;&#20256;&#32479;&#30340;&#25991;&#26412;&#26816;&#32034;&#30456;&#27604;&#65292;&#25991;&#26412;-&#35270;&#39057;&#26816;&#32034;&#30340;&#20027;&#35201;&#38556;&#30861;&#26159;&#26597;&#35810;&#30340;&#25991;&#26412;&#24615;&#36136;&#21644;&#35270;&#39057;&#20869;&#23481;&#30340;&#35270;&#35273;&#20016;&#23500;&#24615;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#36317;&#12290;&#20197;&#20154;&#31867;&#35748;&#30693;&#36807;&#31243;&#20013;&#27169;&#22359;&#21270;&#21028;&#26029;&#25991;&#26412;&#21644;&#35270;&#39057;&#30456;&#20851;&#24615;&#20026;&#28789;&#24863;&#65292;&#30001;&#20110;&#35270;&#39057;&#20869;&#23481;&#30340;&#36830;&#32493;&#21644;&#22797;&#26434;&#24615;&#65292;&#21028;&#26029;&#38656;&#35201;&#39640;&#38454;&#21305;&#37197;&#20449;&#21495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22359;&#32423;&#25991;&#26412;-&#35270;&#39057;&#21305;&#37197;&#65292;&#20854;&#20013;&#25552;&#21462;&#26597;&#35810;&#22359;&#20197;&#25551;&#36848;&#29305;&#23450;&#30340;&#26816;&#32034;&#21333;&#20803;&#65292;&#32780;&#35270;&#39057;&#22359;&#21017;&#20174;&#35270;&#39057;&#20013;&#20998;&#21106;&#25104;&#19981;&#21516;&#30340;&#29255;&#27573;&#12290;&#25105;&#20204;&#23558;&#22359;&#32423;&#21305;&#37197;&#24418;&#24335;&#21270;&#20026;&#26597;&#35810;&#35789;&#21644;&#35270;&#39057;&#24103;&#20043;&#38388;&#30340;n&#20803;&#30456;&#20851;&#24314;&#27169;&#65292;&#24182;&#24341;&#20837;&#22810;&#27169;&#24577;&#36229;&#22270;&#36827;&#34892;n&#20803;&#30456;&#20851;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-video retrieval is a challenging task that aims to identify relevant videos given textual queries. Compared to conventional textual retrieval, the main obstacle for text-video retrieval is the semantic gap between the textual nature of queries and the visual richness of video content. Previous works primarily focus on aligning the query and the video by finely aggregating word-frame matching signals. Inspired by the human cognitive process of modularly judging the relevance between text and video, the judgment needs high-order matching signal due to the consecutive and complex nature of video contents. In this paper, we propose chunk-level text-video matching, where the query chunks are extracted to describe a specific retrieval unit, and the video chunks are segmented into distinct clips from videos. We formulate the chunk-level matching as n-ary correlations modeling between words of the query and frames of the video and introduce a multi-modal hypergraph for n-ary correlation m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;Bodo&#35821;&#35789;&#24615;&#26631;&#27880;&#22120;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Bodo&#35821;&#35328;&#27169;&#22411;BodoBERT&#65292;&#36825;&#39033;&#24037;&#20316;&#26159;&#31532;&#19968;&#20010;&#20026;Bodo&#24320;&#21457;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;Bodo&#35789;&#24615;&#26631;&#27880;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#20102;BiLSTM&#12289;CRF&#21644;BodoBERT&#19982;BytePairEmbeddings&#30340;&#32452;&#21512;&#12290;&#23613;&#31649;&#30740;&#31350;&#24050;&#32463;&#22312;&#36164;&#28304;&#20016;&#23500;&#30340;&#35821;&#35328;&#20013;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#35789;&#24615;&#26631;&#27880;&#27169;&#22411;&#30340;&#30740;&#31350;&#65292;&#20294;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#22914;Bodo&#65292;&#20173;&#28982;&#32570;&#20047;&#30456;&#20851;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2401.03175</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;Bodo&#35821;&#35789;&#24615;&#26631;&#27880;&#22120;
&lt;/p&gt;
&lt;p&gt;
Part-of-Speech Tagger for Bodo Language using Deep Learning approach. (arXiv:2401.03175v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;Bodo&#35821;&#35789;&#24615;&#26631;&#27880;&#22120;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Bodo&#35821;&#35328;&#27169;&#22411;BodoBERT&#65292;&#36825;&#39033;&#24037;&#20316;&#26159;&#31532;&#19968;&#20010;&#20026;Bodo&#24320;&#21457;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;Bodo&#35789;&#24615;&#26631;&#27880;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#20102;BiLSTM&#12289;CRF&#21644;BodoBERT&#19982;BytePairEmbeddings&#30340;&#32452;&#21512;&#12290;&#23613;&#31649;&#30740;&#31350;&#24050;&#32463;&#22312;&#36164;&#28304;&#20016;&#23500;&#30340;&#35821;&#35328;&#20013;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#35789;&#24615;&#26631;&#27880;&#27169;&#22411;&#30340;&#30740;&#31350;&#65292;&#20294;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#22914;Bodo&#65292;&#20173;&#28982;&#32570;&#20047;&#30456;&#20851;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#22914;&#35789;&#24615;&#26631;&#27880;&#12289;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#26426;&#22120;&#32763;&#35793;&#12289;&#35821;&#38899;&#35782;&#21035;&#21644;&#35821;&#35328;&#24314;&#27169;&#22312;&#36164;&#28304;&#20016;&#23500;&#30340;&#35821;&#35328;&#20013;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#19968;&#20123;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#22914;Bodo&#12289;Mizo&#12289;Nagamese&#31561;&#65292;&#36825;&#20123;&#31995;&#32479;&#30340;&#30740;&#31350;&#35201;&#20040;&#23578;&#26410;&#24320;&#22987;&#65292;&#35201;&#20040;&#22788;&#20110;&#36215;&#27493;&#38454;&#27573;&#12290;&#35821;&#35328;&#27169;&#22411;&#22312;&#29616;&#20195;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#23545;&#20110;&#36164;&#28304;&#20016;&#23500;&#30340;&#35821;&#35328;&#24050;&#32463;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#20687;Bodo&#12289;Rabha&#21644;Mising&#36825;&#26679;&#30340;&#35821;&#35328;&#20173;&#28982;&#32570;&#20047;&#30456;&#20851;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#39318;&#20808;&#25552;&#20986;&#20102;BodoBERT&#65292;&#19968;&#20010;&#29992;&#20110;Bodo&#35821;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#39033;&#24037;&#20316;&#26159;&#31532;&#19968;&#20010;&#20026;Bodo&#24320;&#21457;&#35821;&#35328;&#27169;&#22411;&#30340;&#21162;&#21147;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#38598;&#25104;&#28145;&#24230;&#23398;&#20064;&#30340;Bodo&#35789;&#24615;&#26631;&#27880;&#27169;&#22411;&#12290;&#35813;&#35789;&#24615;&#26631;&#27880;&#27169;&#22411;&#22522;&#20110;BiLSTM&#21644;CRF&#30340;&#32452;&#21512;&#65292;&#20197;&#21450;BodoBERT&#19982;BytePairEmbeddings&#30340;&#22534;&#21472;&#23884;&#20837;&#12290;&#25105;&#20204;&#28085;&#30422;&#20102;&#22810;&#31181;&#35821;&#35328;...&#65288;&#25688;&#35201;&#34987;&#25130;&#26029;&#65289;
&lt;/p&gt;
&lt;p&gt;
Language Processing systems such as Part-of-speech tagging, Named entity recognition, Machine translation, Speech recognition, and Language modeling (LM) are well-studied in high-resource languages. Nevertheless, research on these systems for several low-resource languages, including Bodo, Mizo, Nagamese, and others, is either yet to commence or is in its nascent stages. Language model plays a vital role in the downstream tasks of modern NLP. Extensive studies are carried out on LMs for high-resource languages. Nevertheless, languages such as Bodo, Rabha, and Mising continue to lack coverage. In this study, we first present BodoBERT, a language model for the Bodo language. To the best of our knowledge, this work is the first such effort to develop a language model for Bodo. Secondly, we present an ensemble DL-based POS tagging model for Bodo. The POS tagging model is based on combinations of BiLSTM with CRF and stacked embedding of BodoBERT with BytePairEmbeddings. We cover several lan
&lt;/p&gt;</description></item><item><title>&#22235;&#27493;&#25512;&#29702;&#65288;QLFR&#65289;&#26694;&#26550;&#26159;&#19968;&#31181;&#36890;&#36807;&#24341;&#20837;&#21477;&#27861;&#21644;&#35821;&#20041;&#20016;&#23500;&#30340;CoT&#26469;&#25552;&#21319;&#30701;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.03158</link><description>&lt;p&gt;
&#22235;&#27493;&#25512;&#29702;&#65288;QLFR&#65289;&#26694;&#26550;&#65306;&#25512;&#36827;&#30701;&#25991;&#26412;&#20998;&#31867;&#30340;&#22235;&#27493;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Quartet Logic: A Four-Step Reasoning (QLFR) framework for advancing Short Text Classification. (arXiv:2401.03158v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03158
&lt;/p&gt;
&lt;p&gt;
&#22235;&#27493;&#25512;&#29702;&#65288;QLFR&#65289;&#26694;&#26550;&#26159;&#19968;&#31181;&#36890;&#36807;&#24341;&#20837;&#21477;&#27861;&#21644;&#35821;&#20041;&#20016;&#23500;&#30340;CoT&#26469;&#25552;&#21319;&#30701;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30701;&#25991;&#26412;&#20998;&#31867;&#65288;STC&#65289;&#23545;&#20110;&#22788;&#29702;&#21644;&#29702;&#35299;&#24403;&#20195;&#25968;&#23383;&#24179;&#21488;&#19978;&#27969;&#34892;&#30340;&#31616;&#27905;&#32780;&#37325;&#35201;&#30340;&#20869;&#23481;&#33267;&#20851;&#37325;&#35201;&#12290;STC&#22312;&#25235;&#20303;&#35821;&#20041;&#21644;&#21477;&#27861;&#22797;&#26434;&#24615;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#65292;&#36825;&#20010;&#38382;&#39064;&#22312;&#20256;&#32479;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#24456;&#26126;&#26174;&#12290;&#23613;&#31649;&#22270;&#21367;&#31215;&#32593;&#32476;&#36890;&#36807;&#25972;&#21512;&#22806;&#37096;&#30693;&#35782;&#24211;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#21463;&#21040;&#24212;&#29992;&#30693;&#35782;&#36136;&#37327;&#21644;&#33539;&#22260;&#30340;&#38480;&#21046;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#30340;&#20986;&#29616;&#26174;&#33879;&#25552;&#39640;&#20102;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#30740;&#31350;&#25351;&#20986;&#20102;&#23427;&#20204;&#22312;&#22522;&#30784;NLP&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#36816;&#29992;CoT&#26469;&#30740;&#31350;LLMs&#22312;STC&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#22235;&#27493;&#25512;&#29702;&#65288;QLFR&#65289;&#26694;&#26550;&#12290;&#36825;&#20010;&#26694;&#26550;&#20027;&#35201;&#21253;&#25324;&#21477;&#27861;&#21644;&#35821;&#20041;&#20016;&#23500;&#30340;CoT&#65292;&#26377;&#25928;&#21033;&#29992;&#20102;LLMs&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Short Text Classification (STC) is crucial for processing and comprehending the brief but substantial content prevalent on contemporary digital platforms. The STC encounters difficulties in grasping semantic and syntactic intricacies, an issue that is apparent in traditional pre-trained language models. Although Graph Convolutional Networks enhance performance by integrating external knowledge bases, these methods are limited by the quality and extent of the knowledge applied. Recently, the emergence of Large Language Models (LLMs) and Chain-of-Thought (CoT) has significantly improved the performance of complex reasoning tasks. However, some studies have highlighted the limitations of their application in fundamental NLP tasks. Consequently, this study sought to employ CoT to investigate the capabilities of LLMs in STC tasks. This study introduces Quartet Logic: A Four-Step Reasoning (QLFR) framework. This framework primarily incorporates Syntactic and Semantic Enrichment CoT, effectiv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#20986;&#29616;&#30340;&#36951;&#24536;&#29616;&#35937;&#65292;&#36890;&#36807;&#35780;&#20272;&#25345;&#32493;&#39044;&#35757;&#32451;&#23545;&#24494;&#35843;&#27169;&#22411;&#22312;&#36755;&#20986;&#26684;&#24335;&#12289;&#30693;&#35782;&#21644;&#21487;&#38752;&#24615;&#31561;&#26041;&#38754;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#21457;&#29616;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#23588;&#20854;&#26159;&#37325;&#22797;&#38382;&#39064;&#65292;&#26159;&#19968;&#20010;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.03129</link><description>&lt;p&gt;
&#22312;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#20013;&#30740;&#31350;&#36951;&#24536;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
Examining Forgetting in Continual Pre-training of Aligned Large Language Models. (arXiv:2401.03129v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03129
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#20986;&#29616;&#30340;&#36951;&#24536;&#29616;&#35937;&#65292;&#36890;&#36807;&#35780;&#20272;&#25345;&#32493;&#39044;&#35757;&#32451;&#23545;&#24494;&#35843;&#27169;&#22411;&#22312;&#36755;&#20986;&#26684;&#24335;&#12289;&#30693;&#35782;&#21644;&#21487;&#38752;&#24615;&#31561;&#26041;&#38754;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#21457;&#29616;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#23588;&#20854;&#26159;&#37325;&#22797;&#38382;&#39064;&#65292;&#26159;&#19968;&#20010;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#20808;&#36827;&#30740;&#31350;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;&#37492;&#20110;LLMs&#22312;&#24456;&#22810;&#39046;&#22495;&#20013;&#30340;&#37325;&#35201;&#24212;&#29992;&#65292;LLM&#30340;&#24320;&#21457;&#20063;&#26377;&#20102;&#26174;&#33879;&#22686;&#38271;&#12290;&#22312;&#24320;&#21457;LLMs&#26102;&#65292;&#19968;&#31181;&#24120;&#35265;&#20570;&#27861;&#26159;&#23545;&#20808;&#21069;&#24494;&#35843;&#27169;&#22411;&#36827;&#34892;&#36830;&#32493;&#30340;&#39044;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#28798;&#38590;&#24615;&#30340;&#36951;&#24536;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#29616;&#26377;&#24494;&#35843;LLM&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#21457;&#29983;&#30340;&#36951;&#24536;&#29616;&#35937;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25345;&#32493;&#39044;&#35757;&#32451;&#23545;&#24494;&#35843;LLM&#22312;&#36755;&#20986;&#26684;&#24335;&#12289;&#30693;&#35782;&#21644;&#21487;&#38752;&#24615;&#31561;&#21508;&#20010;&#32500;&#24230;&#19978;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#31361;&#26174;&#20102;&#22312;&#25345;&#32493;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#38750;&#24179;&#20961;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#37325;&#22797;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in Large Language Models (LLMs) have exhibited remarkable proficiency across various tasks. Given the potent applications of LLMs in numerous fields, there has been a surge in LLM development. In developing LLMs, a common practice involves continual pre-training on previously fine-tuned models. However, this can lead to catastrophic forgetting. In our work, we investigate the phenomenon of forgetting that occurs during continual pre-training on an existing fine-tuned LLM. We evaluate the impact of continuous pre-training on the fine-tuned LLM across various dimensions, including output format, knowledge, and reliability. Experiment results highlight the non-trivial challenge of addressing catastrophic forgetting during continual pre-training, especially the repetition issue.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#20154;&#19982;&#23545;&#35805;&#22411;&#26234;&#33021;&#20307;&#20132;&#20114;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#24182;&#30740;&#31350;&#23545;&#35805;&#22411;&#26234;&#33021;&#20307;&#30340;&#24615;&#21035;&#35774;&#35745;&#22914;&#20309;&#35302;&#21457;&#26082;&#26377;&#24615;&#21035;&#20559;&#35265;&#24182;&#24378;&#21270;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#12290;</title><link>http://arxiv.org/abs/2401.03030</link><description>&lt;p&gt;
&#25506;&#32034;&#20154;&#19982;&#23545;&#35805;&#22411;&#26234;&#33021;&#20307;&#20132;&#20114;&#20013;&#35821;&#35328;&#27169;&#24335;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Exploring Gender Biases in Language Patterns of Human-Conversational Agent Conversations. (arXiv:2401.03030v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#20154;&#19982;&#23545;&#35805;&#22411;&#26234;&#33021;&#20307;&#20132;&#20114;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#24182;&#30740;&#31350;&#23545;&#35805;&#22411;&#26234;&#33021;&#20307;&#30340;&#24615;&#21035;&#35774;&#35745;&#22914;&#20309;&#35302;&#21457;&#26082;&#26377;&#24615;&#21035;&#20559;&#35265;&#24182;&#24378;&#21270;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#26426;&#20132;&#20114;&#30340;&#20852;&#36215;&#65292;&#26426;&#22120;&#36234;&#26469;&#36234;&#22810;&#22320;&#35774;&#35745;&#25104;&#31867;&#20284;&#20154;&#31867;&#30340;&#29305;&#24449;&#65292;&#22914;&#24615;&#21035;&#65292;&#36825;&#21487;&#33021;&#26080;&#24847;&#20013;&#24341;&#21457;&#35748;&#30693;&#20559;&#35265;&#12290;&#35768;&#22810;&#23545;&#35805;&#22411;&#26234;&#33021;&#20307;&#65288;CAs&#65289;&#65292;&#22914;&#35821;&#38899;&#21161;&#25163;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#40664;&#35748;&#20026;&#22899;&#24615;&#35282;&#33394;&#65292;&#24341;&#21457;&#20102;&#26377;&#20851;&#25345;&#32493;&#24615;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#21644;&#19981;&#24179;&#31561;&#30340;&#25285;&#24551;&#12290;&#26377;&#20154;&#25209;&#35780;&#36825;&#20123;&#25216;&#26415;&#21487;&#33021;&#25226;&#22899;&#24615;&#29289;&#21270;&#24182;&#24378;&#21270;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#12290;&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#20174;&#23545;&#35805;&#22411;&#20154;&#24037;&#26234;&#33021;&#35774;&#35745;&#30340;&#35282;&#24230;&#28145;&#20837;&#25506;&#35752;&#24615;&#21035;&#20559;&#35265;&#22312;&#20154;&#19982;&#23545;&#35805;&#22411;&#26234;&#33021;&#20307;&#20132;&#20114;&#20013;&#30340;&#24433;&#21709;&#12290;&#20174;&#34892;&#20026;&#21644;&#27807;&#36890;&#30740;&#31350;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#35813;&#39033;&#30446;&#19981;&#20165;&#20851;&#27880;&#29992;&#25143;&#19982;&#23545;&#35805;&#22411;&#26234;&#33021;&#20307;&#20114;&#21160;&#26102;&#30340;&#35748;&#30693;&#65292;&#36824;&#20851;&#27880;&#20854;&#35821;&#35328;&#39118;&#26684;&#65292;&#20043;&#21069;&#30340;&#30740;&#31350;&#24456;&#23569;&#28041;&#21450;&#36825;&#26041;&#38754;&#12290;&#30740;&#31350;&#30446;&#30340;&#26159;&#20102;&#35299;&#23545;&#35805;&#22411;&#26234;&#33021;&#20307;&#30340;&#24615;&#21035;&#35774;&#35745;&#22914;&#20309;&#35302;&#21457;&#26082;&#26377;&#24615;&#21035;&#20559;&#35265;&#65292;&#24182;&#36827;&#19968;&#27493;&#30740;&#31350;&#23545;&#35805;&#22411;&#26234;&#33021;&#20307;&#30340;&#24615;&#21035;&#35774;&#35745;&#22914;&#20309;&#24378;&#21270;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rise of human-machine communication, machines are increasingly designed with humanlike characteristics, such as gender, which can inadvertently trigger cognitive biases. Many conversational agents (CAs), such as voice assistants and chatbots, default to female personas, leading to concerns about perpetuating gender stereotypes and inequality. Critiques have emerged regarding the potential objectification of females and reinforcement of gender stereotypes by these technologies. This research, situated in conversational AI design, aims to delve deeper into the impacts of gender biases in human-CA interactions. From a behavioral and communication research standpoint, this program focuses not only on perceptions but also the linguistic styles of users when interacting with CAs, as previous research has rarely explored. It aims to understand how pre-existing gender biases might be triggered by CAs' gender designs. It further investigates how CAs' gender designs may reinforce gender
&lt;/p&gt;</description></item><item><title>AST-T5&#26159;&#19968;&#31181;&#32467;&#26500;&#24863;&#30693;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#25277;&#35937;&#35821;&#27861;&#26641;&#65288;AST&#65289;&#26469;&#22686;&#24378;&#20195;&#30721;&#29983;&#25104;&#12289;&#36716;&#25442;&#21644;&#29702;&#35299;&#30340;&#33021;&#21147;&#12290;&#23427;&#20248;&#20110;&#20854;&#20182;&#21516;&#31561;&#22823;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#22312;&#20195;&#30721;&#21040;&#20195;&#30721;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2401.03003</link><description>&lt;p&gt;
AST-T5&#65306;&#38754;&#21521;&#20195;&#30721;&#29983;&#25104;&#21644;&#29702;&#35299;&#30340;&#32467;&#26500;&#24863;&#30693;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AST-T5: Structure-Aware Pretraining for Code Generation and Understanding. (arXiv:2401.03003v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03003
&lt;/p&gt;
&lt;p&gt;
AST-T5&#26159;&#19968;&#31181;&#32467;&#26500;&#24863;&#30693;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#25277;&#35937;&#35821;&#27861;&#26641;&#65288;AST&#65289;&#26469;&#22686;&#24378;&#20195;&#30721;&#29983;&#25104;&#12289;&#36716;&#25442;&#21644;&#29702;&#35299;&#30340;&#33021;&#21147;&#12290;&#23427;&#20248;&#20110;&#20854;&#20182;&#21516;&#31561;&#22823;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#22312;&#20195;&#30721;&#21040;&#20195;&#30721;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#30456;&#20851;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#28982;&#32780;&#35768;&#22810;&#27169;&#22411;&#23558;&#20195;&#30721;&#35270;&#20026;&#31616;&#21333;&#24207;&#21015;&#65292;&#24573;&#30053;&#20102;&#20854;&#32467;&#26500;&#21270;&#29305;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;AST-T5&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#39044;&#35757;&#32451;&#33539;&#24335;&#65292;&#21033;&#29992;&#25277;&#35937;&#35821;&#27861;&#26641;&#65288;AST&#65289;&#22686;&#24378;&#20102;&#20195;&#30721;&#29983;&#25104;&#12289;&#36716;&#25442;&#21644;&#29702;&#35299;&#12290;&#36890;&#36807;&#21160;&#24577;&#35268;&#21010;&#65292;&#25105;&#20204;&#30340;AST&#24863;&#30693;&#20998;&#21106;&#20445;&#30041;&#20102;&#20195;&#30721;&#32467;&#26500;&#65292;&#32780;AST&#24863;&#30693;&#36328;&#24230;&#30772;&#22351;&#30446;&#26631;&#20351;&#27169;&#22411;&#33021;&#22815;&#37325;&#24314;&#21508;&#31181;&#20195;&#30721;&#32467;&#26500;&#12290;&#19982;&#20854;&#20182;&#27169;&#22411;&#19981;&#21516;&#65292;AST-T5&#36991;&#20813;&#20102;&#22797;&#26434;&#30340;&#31243;&#24207;&#20998;&#26512;&#25110;&#26550;&#26500;&#26356;&#25913;&#65292;&#22240;&#27492;&#21487;&#20197;&#19982;&#20219;&#20309;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;Transformer&#26080;&#32541;&#38598;&#25104;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;AST-T5&#22312;&#21508;&#31181;&#20195;&#30721;&#30456;&#20851;&#20219;&#21153;&#20013;&#22987;&#32456;&#20248;&#20110;&#21516;&#31561;&#22823;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#32467;&#26500;&#24863;&#30693;&#20351;&#24471;AST-T5&#22312;&#20195;&#30721;&#21040;&#20195;&#30721;&#20219;&#21153;&#20013;&#29305;&#21035;&#24378;&#22823;&#65292;&#22312;Bugs2Fix&#20219;&#21153;&#30340;&#31934;&#30830;&#21305;&#37197;&#24471;&#20998;&#19978;&#36229;&#36807;CodeT5 2&#20010;&#28857;&#65292;&#24182;&#22312;CodeXGLUE&#20013;&#30340;Java-C#&#36716;&#25442;&#20219;&#21153;&#30340;&#31934;&#30830;&#21305;&#37197;&#24471;&#20998;&#19978;&#36229;&#36807;CodeT5 3&#20010;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have made significant advancements in code-related tasks, yet many LLMs treat code as simple sequences, neglecting its structured nature. We introduce AST-T5, a novel pretraining paradigm that leverages the Abstract Syntax Tree (AST) for enhanced code generation, transpilation, and understanding. Using dynamic programming, our AST-Aware Segmentation retains code structure, while our AST-Aware Span Corruption objective equips the model to reconstruct various code structures. Unlike other models, AST-T5 avoids intricate program analyses or architectural changes, so it integrates seamlessly with any encoder-decoder Transformer. Evaluations show that AST-T5 consistently outperforms similar-sized LMs across various code-related tasks. Structure-awareness makes AST-T5 particularly powerful in code-to-code tasks, surpassing CodeT5 by 2 points in exact match score for the Bugs2Fix task and by 3 points in exact match score for Java-C# Transpilation in CodeXGLUE. Our
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#20219;&#21153;&#20998;&#35299;&#65292;&#25552;&#20986;&#20102;Blar-SQL&#26694;&#26550;&#65292;&#23558;&#20004;&#20010;&#19981;&#21516;&#30340;&#27169;&#22411;&#32452;&#21512;&#65292;&#20998;&#21035;&#19987;&#27880;&#20110;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#20174;&#32780;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;NL2SQL&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#12290;&#35813;&#27169;&#22411;&#27604;GPT-4&#26356;&#23567;&#12289;&#26356;&#24555;&#12289;&#26356;&#20415;&#23452;&#12290;</title><link>http://arxiv.org/abs/2401.02997</link><description>&lt;p&gt;
Blar-SQL: &#26356;&#24555;&#12289;&#26356;&#24378;&#12289;&#26356;&#23567;&#30340;NL2SQL
&lt;/p&gt;
&lt;p&gt;
Blar-SQL: Faster, Stronger, Smaller NL2SQL. (arXiv:2401.02997v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02997
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#20219;&#21153;&#20998;&#35299;&#65292;&#25552;&#20986;&#20102;Blar-SQL&#26694;&#26550;&#65292;&#23558;&#20004;&#20010;&#19981;&#21516;&#30340;&#27169;&#22411;&#32452;&#21512;&#65292;&#20998;&#21035;&#19987;&#27880;&#20110;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#20174;&#32780;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;NL2SQL&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#12290;&#35813;&#27169;&#22411;&#27604;GPT-4&#26356;&#23567;&#12289;&#26356;&#24555;&#12289;&#26356;&#20415;&#23452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#21040;SQL&#20219;&#21153;&#65288;NL2SQL&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#21487;&#35266;&#30340;&#22768;&#35465;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20219;&#21153;&#20998;&#35299;&#26469;&#26497;&#22823;&#22320;&#25913;&#21892;LLMs&#22312;&#25968;&#25454;&#24211;&#29702;&#35299;&#21644;&#26597;&#35810;&#29983;&#25104;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#20197;&#20415;&#20351;&#29992;&#19968;&#20010;SQL&#26597;&#35810;&#26469;&#22238;&#31572;&#20154;&#31867;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#32452;&#21512;&#20004;&#20010;&#19981;&#21516;&#27169;&#22411;&#65292;&#20998;&#21035;&#19987;&#27880;&#20110;&#20004;&#20010;&#20219;&#21153;&#65292;&#23545;&#24320;&#28304;&#27169;&#22411;&#65288;&#29305;&#21035;&#26159;Llama-2&#21644;Code Llama&#65289;&#36827;&#34892;&#20102;&#31934;&#35843;&#65292;&#20197;&#21033;&#29992;&#27599;&#20010;&#27169;&#22411;&#30340;&#26680;&#24515;&#31454;&#20105;&#21147;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#26368;&#32456;SQL&#26597;&#35810;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#26469;&#23558;&#27169;&#24335;&#21010;&#20998;&#20026;&#22359;&#65292;&#20197;&#23558;&#26356;&#22810;&#20449;&#24687;&#36866;&#24212;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#20013;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#19982;GPT-4&#33719;&#24471;&#30340;&#32467;&#26524;&#30456;&#24403;&#65292;&#21516;&#26102;&#27604;GPT-4&#26356;&#23567;135&#20493;&#12289;&#26356;&#24555;90&#20493;&#65292;&#24182;&#19988;&#27604;GPT-4&#20415;&#23452;100&#20493;&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have gained considerable notoriety in the field of natural language to SQL tasks (NL2SQL). In this study, we show how task decomposition can greatly benefit LLMs in database understanding and query generation in order to answer human questions with an SQL query.  We fined-tuned open source models, specifically Llama-2 and Code Llama, by combining 2 different models each designated to focus on one of two tasks in order to leverage each model's core competency to further increase the accuracy of the final SQL query.  We propose a new framework to divide the schema into chunks in order to fit more information into a limited context. Our results are comparable with those obtained by GPT-4 at the same time being 135 times smaller, 90 times faster and more than 100 times cheaper than GPT-4.
&lt;/p&gt;</description></item><item><title>CANAMRF &#26159;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#22810;&#27169;&#24577;&#25233;&#37057;&#26816;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#32771;&#34385;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#65292;&#26377;&#25928;&#22320;&#36827;&#34892;&#22810;&#27169;&#24577;&#34920;&#31034;&#65292;&#24182;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.02995</link><description>&lt;p&gt;
CANAMRF&#65306;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22810;&#27169;&#24577;&#25233;&#37057;&#26816;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CANAMRF: An Attention-Based Model for Multimodal Depression Detection. (arXiv:2401.02995v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02995
&lt;/p&gt;
&lt;p&gt;
CANAMRF &#26159;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#22810;&#27169;&#24577;&#25233;&#37057;&#26816;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#32771;&#34385;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#65292;&#26377;&#25928;&#22320;&#36827;&#34892;&#22810;&#27169;&#24577;&#34920;&#31034;&#65292;&#24182;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#25233;&#37057;&#26816;&#27979;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#26088;&#22312;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#39044;&#27979;&#20154;&#31867;&#30340;&#24515;&#29702;&#29366;&#24577;&#12290;&#20043;&#21069;&#30340;&#26041;&#27861;&#24179;&#31561;&#23545;&#24453;&#19981;&#21516;&#30340;&#27169;&#24577;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#30340;&#25968;&#23398;&#36816;&#31639;&#34701;&#21512;&#27599;&#20010;&#27169;&#24577;&#65292;&#27809;&#26377;&#23545;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#36827;&#34892;&#34913;&#37327;&#65292;&#36825;&#19981;&#33021;&#33719;&#24471;&#36866;&#29992;&#20110;&#19979;&#28216;&#25233;&#37057;&#20219;&#21153;&#30340;&#33391;&#22909;&#22810;&#27169;&#24577;&#34920;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#33258;&#36866;&#24212;&#22810;&#27169;&#24577;&#24490;&#29615;&#34701;&#21512;&#30340;&#36328;&#27169;&#24577;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;CANAMRF&#65289;&#29992;&#20110;&#22810;&#27169;&#24577;&#25233;&#37057;&#26816;&#27979;&#12290;CANAMRF&#30001;&#22810;&#27169;&#24577;&#29305;&#24449;&#25552;&#21462;&#22120;&#12289;&#33258;&#36866;&#24212;&#22810;&#27169;&#24577;&#24490;&#29615;&#34701;&#21512;&#27169;&#22359;&#21644;&#28151;&#21512;&#27880;&#24847;&#21147;&#27169;&#22359;&#26500;&#25104;&#12290;&#36890;&#36807;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;CANAMRF&#23637;&#31034;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#31361;&#26174;&#20102;&#25105;&#20204;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal depression detection is an important research topic that aims to predict human mental states using multimodal data. Previous methods treat different modalities equally and fuse each modality by na\"ive mathematical operations without measuring the relative importance between them, which cannot obtain well-performed multimodal representations for downstream depression tasks. In order to tackle the aforementioned concern, we present a Cross-modal Attention Network with Adaptive Multi-modal Recurrent Fusion (CANAMRF) for multimodal depression detection. CANAMRF is constructed by a multimodal feature extractor, an Adaptive Multimodal Recurrent Fusion module, and a Hybrid Attention Module. Through experimentation on two benchmark datasets, CANAMRF demonstrates state-of-the-art performance, underscoring the effectiveness of our proposed approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#28151;&#21512;&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#21512;&#22810;&#20010;&#36866;&#24230;&#35268;&#27169;&#30340;&#32842;&#22825;AI&#27169;&#22411;&#65292;&#21487;&#20197;&#36798;&#21040;&#25110;&#36229;&#36234;&#27604;&#23427;&#20204;&#26356;&#22823;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.02994</link><description>&lt;p&gt;
&#22522;&#20110;&#28151;&#21512;&#26041;&#27861;&#30340;&#32842;&#22825;AI&#27169;&#22411;&#65306;&#30456;&#23545;&#20110;&#19975;&#20159;&#32423;&#21442;&#25968;&#27169;&#22411;&#30340;&#26356;&#24265;&#20215;&#12289;&#26356;&#22909;&#30340;&#26367;&#20195;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Blending Is All You Need: Cheaper, Better Alternative to Trillion-Parameters LLM. (arXiv:2401.02994v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#28151;&#21512;&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#21512;&#22810;&#20010;&#36866;&#24230;&#35268;&#27169;&#30340;&#32842;&#22825;AI&#27169;&#22411;&#65292;&#21487;&#20197;&#36798;&#21040;&#25110;&#36229;&#36234;&#27604;&#23427;&#20204;&#26356;&#22823;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20250;&#35805;&#22411;AI&#30740;&#31350;&#20013;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#27169;&#22411;&#37319;&#29992;&#20102;&#26356;&#22810;&#30340;&#21442;&#25968;&#65292;&#22914;ChatGPT&#31561;&#27169;&#22411;&#12290;&#34429;&#28982;&#36825;&#20123;&#24222;&#22823;&#30340;&#27169;&#22411;&#24448;&#24448;&#33021;&#29983;&#25104;&#26356;&#22909;&#30340;&#32842;&#22825;&#22238;&#22797;&#65292;&#20294;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#20869;&#23384;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65306;&#33021;&#21542;&#36890;&#36807;&#32452;&#21512;&#36739;&#23567;&#30340;&#27169;&#22411;&#26469;&#36798;&#21040;&#19982;&#21333;&#20010;&#22823;&#27169;&#22411;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#28151;&#21512;&#8221;&#30340;&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#23558;&#22810;&#20010;&#32842;&#22825;AI&#38598;&#25104;&#22312;&#19968;&#36215;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;&#65292;&#24403;&#29305;&#23450;&#36739;&#23567;&#30340;&#27169;&#22411;&#21327;&#21516;&#28151;&#21512;&#26102;&#65292;&#23427;&#20204;&#21487;&#20197;&#28508;&#22312;&#22320;&#36229;&#36234;&#25110;&#21305;&#25932;&#22823;&#22411;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20363;&#22914;&#65292;&#20165;&#38598;&#25104;&#19977;&#20010;&#36866;&#24230;&#35268;&#27169;&#30340;&#27169;&#22411;&#65288;6B/13B&#21442;&#25968;&#65289;&#23601;&#21487;&#20197;&#36798;&#21040;&#25110;&#29978;&#33267;&#36229;&#36234;ChatGPT&#65288;175B+&#21442;&#25968;&#65289;&#31561;&#22823;&#22411;&#27169;&#22411;&#30340;&#24615;&#33021;&#25351;&#26631;&#12290;&#36825;&#20010;&#20551;&#35774;&#32463;&#36807;&#20102;&#20005;&#26684;&#30340;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
In conversational AI research, there's a noticeable trend towards developing models with a larger number of parameters, exemplified by models like ChatGPT. While these expansive models tend to generate increasingly better chat responses, they demand significant computational resources and memory. This study explores a pertinent question: Can a combination of smaller models collaboratively achieve comparable or enhanced performance relative to a singular large model? We introduce an approach termed "blending", a straightforward yet effective method of integrating multiple chat AIs. Our empirical evidence suggests that when specific smaller models are synergistically blended, they can potentially outperform or match the capabilities of much larger counterparts. For instance, integrating just three models of moderate size (6B/13B paramaeters) can rival or even surpass the performance metrics of a substantially larger model like ChatGPT (175B+ paramaters). This hypothesis is rigorously tes
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#39640;&#25928;&#30340;&#26816;&#32034;&#34920;&#31034;&#34701;&#21512;&#26041;&#27861;ReFusion&#65292;&#36890;&#36807;&#23558;&#26816;&#32034;&#34920;&#31034;&#30452;&#25509;&#34701;&#21512;&#21040;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#35299;&#20915;&#20102;&#23558;&#22806;&#37096;&#25968;&#25454;&#24211;&#30340;&#30693;&#35782;&#34701;&#20837;&#38750;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.02993</link><description>&lt;p&gt;
&#20351;&#29992;&#35745;&#31639;&#39640;&#25928;&#30340;&#26816;&#32034;&#34920;&#31034;&#34701;&#21512;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Natural Language Understanding with Computation-Efficient Retrieval Representation Fusion. (arXiv:2401.02993v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02993
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#39640;&#25928;&#30340;&#26816;&#32034;&#34920;&#31034;&#34701;&#21512;&#26041;&#27861;ReFusion&#65292;&#36890;&#36807;&#23558;&#26816;&#32034;&#34920;&#31034;&#30452;&#25509;&#34701;&#21512;&#21040;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#35299;&#20915;&#20102;&#23558;&#22806;&#37096;&#25968;&#25454;&#24211;&#30340;&#30693;&#35782;&#34701;&#20837;&#38750;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#22806;&#37096;&#25968;&#25454;&#24211;&#20013;&#33719;&#21462;&#30693;&#35782;&#24182;&#34701;&#20837;&#35821;&#35328;&#27169;&#22411;&#30340;&#26816;&#32034;&#22686;&#24378;&#26041;&#27861;&#22312;&#21508;&#31181;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#65288;&#20363;&#22914;&#38382;&#31572;&#21644;&#25991;&#26412;&#29983;&#25104;&#65289;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;&#38750;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#65288;&#20363;&#22914;&#25991;&#26412;&#20998;&#31867;&#65289;&#20013;&#38598;&#25104;&#26816;&#32034;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#23558;&#26816;&#32034;&#20869;&#23481;&#25340;&#25509;&#21040;&#36755;&#20837;&#20013;&#24418;&#25104;&#25552;&#31034;&#24615;&#30340;&#36755;&#20837;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#35201;&#27714;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#22788;&#29702;&#38271;&#25991;&#26412;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25512;&#26029;&#36825;&#31181;&#25340;&#25509;&#25968;&#25454;&#20063;&#20250;&#28040;&#32791;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#39640;&#25928;&#30340;&#26816;&#32034;&#34920;&#31034;&#34701;&#21512;&#26041;&#27861;ReFusion&#65292;&#37319;&#29992;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#12290;&#20027;&#35201;&#24605;&#24819;&#26159;&#30452;&#25509;&#23558;&#26816;&#32034;&#34920;&#31034;&#19982;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#34701;&#21512;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#26816;&#32034;&#27169;&#22359;&#65292;&#37325;&#26032;&#26816;&#32034;...
&lt;/p&gt;
&lt;p&gt;
Retrieval-based augmentations that aim to incorporate knowledge from an external database into language models have achieved great success in various knowledge-intensive (KI) tasks, such as question-answering and text generation. However, integrating retrievals in non-knowledge-intensive (NKI) tasks, such as text classification, is still challenging. Existing works focus on concatenating retrievals to inputs as context to form the prompt-based inputs. Unfortunately, such methods require language models to have the capability to handle long texts. Besides, inferring such concatenated data would also consume a significant amount of computational resources.  To solve these challenges, we propose \textbf{ReFusion} in this paper, a computation-efficient \textbf{Re}trieval representation \textbf{Fusion} with neural architecture search. The main idea is to directly fuse the retrieval representations into the language models. Specifically, we first propose an online retrieval module that retri
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#35770;&#65292;&#21033;&#29992;"&#38750;&#32467;&#26500;&#21270;&#26680;&#24515;&#24211;"&#65292;&#23558;ESG&#25253;&#21578;&#36716;&#25442;&#20026;&#32467;&#26500;&#21270;&#12289;&#21487;&#20998;&#26512;&#30340;&#26684;&#24335;&#65292;&#24182;&#22312;&#25991;&#26412;&#28165;&#27927;&#12289;&#20174;&#22270;&#20687;&#20013;&#25935;&#38160;&#35782;&#21035;&#21644;&#25552;&#21462;&#25991;&#26412;&#20197;&#21450;&#26631;&#20934;&#21270;&#25253;&#21578;&#20013;&#30340;&#34920;&#26684;&#31561;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20026;&#24037;&#19994;&#29983;&#24577;&#23398;&#21644;&#20225;&#19994;&#21487;&#25345;&#32493;&#24615;&#35780;&#20272;&#39046;&#22495;&#30340;&#21457;&#23637;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2401.02992</link><description>&lt;p&gt;
ESG&#25253;&#21578;&#30340;&#39640;&#32423;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#22788;&#29702;&#65306;&#32467;&#26500;&#21270;&#36716;&#25442;&#21644;&#22686;&#24378;&#20998;&#26512;&#30340;&#26041;&#27861;&#35770;
&lt;/p&gt;
&lt;p&gt;
Advanced Unstructured Data Processing for ESG Reports: A Methodology for Structured Transformation and Enhanced Analysis. (arXiv:2401.02992v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#35770;&#65292;&#21033;&#29992;"&#38750;&#32467;&#26500;&#21270;&#26680;&#24515;&#24211;"&#65292;&#23558;ESG&#25253;&#21578;&#36716;&#25442;&#20026;&#32467;&#26500;&#21270;&#12289;&#21487;&#20998;&#26512;&#30340;&#26684;&#24335;&#65292;&#24182;&#22312;&#25991;&#26412;&#28165;&#27927;&#12289;&#20174;&#22270;&#20687;&#20013;&#25935;&#38160;&#35782;&#21035;&#21644;&#25552;&#21462;&#25991;&#26412;&#20197;&#21450;&#26631;&#20934;&#21270;&#25253;&#21578;&#20013;&#30340;&#34920;&#26684;&#31561;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20026;&#24037;&#19994;&#29983;&#24577;&#23398;&#21644;&#20225;&#19994;&#21487;&#25345;&#32493;&#24615;&#35780;&#20272;&#39046;&#22495;&#30340;&#21457;&#23637;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20225;&#19994;&#21487;&#25345;&#32493;&#24615;&#21457;&#23637;&#39046;&#22495;&#65292;&#20998;&#26512;&#38750;&#32467;&#26500;&#21270;&#30340;&#29615;&#22659;&#12289;&#31038;&#20250;&#21644;&#27835;&#29702;&#65288;ESG&#65289;&#25253;&#21578;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#21508;&#31181;&#26684;&#24335;&#21644;&#22797;&#26434;&#30340;&#20869;&#23481;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#35770;&#65292;&#21033;&#29992;"&#38750;&#32467;&#26500;&#21270;&#26680;&#24515;&#24211;"&#65292;&#19987;&#38376;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#23558;ESG&#25253;&#21578;&#36716;&#25442;&#20026;&#32467;&#26500;&#21270;&#12289;&#21487;&#20998;&#26512;&#30340;&#26684;&#24335;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25991;&#26412;&#28165;&#27927;&#12289;&#20174;&#22270;&#20687;&#20013;&#25935;&#38160;&#22320;&#35782;&#21035;&#21644;&#25552;&#21462;&#25991;&#26412;&#20197;&#21450;&#26631;&#20934;&#21270;&#36825;&#20123;&#25253;&#21578;&#20013;&#30340;&#34920;&#26684;&#31561;&#26041;&#38754;&#26174;&#33879;&#25512;&#36827;&#20102;&#29616;&#26377;&#30740;&#31350;&#12290;&#24378;&#35843;&#20854;&#22788;&#29702;&#19981;&#21516;&#34892;&#19994;&#30340;&#19981;&#21516;&#39029;&#38754;&#24067;&#23616;&#21644;&#25253;&#21578;&#26679;&#24335;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#35813;&#26041;&#27861;&#29087;&#32451;&#22320;&#31649;&#29702;&#21508;&#31181;&#25968;&#25454;&#31867;&#22411;&#65292;&#21253;&#25324;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#34920;&#26684;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#20110;&#24037;&#19994;&#29983;&#24577;&#23398;&#21644;&#20225;&#19994;&#21487;&#25345;&#32493;&#24615;&#35780;&#20272;&#39046;&#22495;&#20855;&#26377;&#37325;&#22823;&#24847;&#20041;&#65292;&#20026;&#24212;&#29992;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the evolving field of corporate sustainability, analyzing unstructured Environmental, Social, and Governance (ESG) reports is a complex challenge due to their varied formats and intricate content. This study introduces an innovative methodology utilizing the "Unstructured Core Library", specifically tailored to address these challenges by transforming ESG reports into structured, analyzable formats. Our approach significantly advances the existing research by offering high-precision text cleaning, adept identification and extraction of text from images, and standardization of tables within these reports. Emphasizing its capability to handle diverse data types, including text, images, and tables, the method adeptly manages the nuances of differing page layouts and report styles across industries. This research marks a substantial contribution to the fields of industrial ecology and corporate sustainability assessment, paving the way for the application of advanced NLP technologies an
&lt;/p&gt;</description></item><item><title>GLIDE-RL&#26159;&#19968;&#31181;&#22522;&#20110;&#28436;&#31034;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25945;&#24072;-&#25351;&#23548;&#21592;-&#23398;&#29983;&#30340;&#35838;&#31243;&#23398;&#20064;&#26694;&#26550;&#65292;&#35757;&#32451;&#20986;&#20102;&#19968;&#20010;&#33021;&#22815;&#36981;&#24490;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#26410;&#35265;&#25351;&#20196;&#30340;RL&#20195;&#29702;&#12290;</title><link>http://arxiv.org/abs/2401.02991</link><description>&lt;p&gt;
GLIDE-RL&#65306;&#22522;&#20110;&#28436;&#31034;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#23548;
&lt;/p&gt;
&lt;p&gt;
GLIDE-RL: Grounded Language Instruction through DEmonstration in RL. (arXiv:2401.02991v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02991
&lt;/p&gt;
&lt;p&gt;
GLIDE-RL&#26159;&#19968;&#31181;&#22522;&#20110;&#28436;&#31034;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25945;&#24072;-&#25351;&#23548;&#21592;-&#23398;&#29983;&#30340;&#35838;&#31243;&#23398;&#20064;&#26694;&#26550;&#65292;&#35757;&#32451;&#20986;&#20102;&#19968;&#20010;&#33021;&#22815;&#36981;&#24490;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#26410;&#35265;&#25351;&#20196;&#30340;RL&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#30340;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#20316;&#31995;&#32479;&#30340;&#26368;&#21518;&#19968;&#39033;&#25361;&#25112;&#26159;AI&#20195;&#29702;&#33021;&#22815;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#24182;&#30456;&#24212;&#22320;&#25191;&#34892;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35821;&#35328;&#30340;&#22797;&#26434;&#24615;&#21644;&#27495;&#20041;&#24615;&#20197;&#21450;&#22870;&#21169;&#30340;&#31232;&#30095;&#24615;&#31561;&#22240;&#32032;&#65292;&#35757;&#32451;&#26377;&#25928;&#30340;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20195;&#29702;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#30340;&#25361;&#25112;&#12290;&#24378;&#21270;&#23398;&#20064;&#12289;&#35838;&#31243;&#23398;&#20064;&#12289;&#25345;&#32493;&#23398;&#20064;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#20960;&#39033;&#36827;&#23637;&#20998;&#21035;&#20026;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#35757;&#32451;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#20195;&#29702;&#20570;&#20986;&#20102;&#26377;&#25928;&#36129;&#29486;&#12290;&#22312;&#36825;&#20123;&#21457;&#23637;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;GLIDE-RL&#65292;&#36890;&#36807;&#24341;&#20837;&#25945;&#24072;-&#25351;&#23548;&#21592;-&#23398;&#29983;&#30340;&#35838;&#31243;&#23398;&#20064;&#26694;&#26550;&#26469;&#22521;&#35757;&#19968;&#20010;&#33021;&#22815;&#36981;&#24490;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#65292;&#24182;&#19988;&#33021;&#22815;&#25512;&#24191;&#21040;&#20043;&#21069;&#26410;&#35265;&#30340;&#35821;&#35328;&#25351;&#20196;&#30340;RL&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the final frontiers in the development of complex human - AI collaborative systems is the ability of AI agents to comprehend the natural language and perform tasks accordingly. However, training efficient Reinforcement Learning (RL) agents grounded in natural language has been a long-standing challenge due to the complexity and ambiguity of the language and sparsity of the rewards, among other factors. Several advances in reinforcement learning, curriculum learning, continual learning, language models have independently contributed to effective training of grounded agents in various environments. Leveraging these developments, we present a novel algorithm, Grounded Language Instruction through DEmonstration in RL (GLIDE-RL) that introduces a teacher-instructor-student curriculum learning framework for training an RL agent capable of following natural language instructions that can generalize to previously unseen language instructions. In this multi-agent framework, the teacher a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#25991;&#26412;&#20998;&#26512;&#26694;&#26550;&#65292;&#21033;&#29992;&#28508;&#22312;&#29380;&#21033;&#20811;&#38647;&#20998;&#37197;&#65288;LDA&#65289;&#20174;&#24904;&#21892;&#20247;&#31609;&#27963;&#21160;&#30340;&#25991;&#26412;&#25551;&#36848;&#20013;&#25552;&#21462;&#28508;&#22312;&#20027;&#39064;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2401.02988</link><description>&lt;p&gt;
&#37319;&#29992;&#28508;&#22312;&#29380;&#21033;&#20811;&#38647;&#20998;&#37197;&#65288;LDA&#65289;&#35821;&#20041;&#25991;&#26412;&#20998;&#26512;&#26041;&#27861;&#25506;&#32034;&#24904;&#21892;&#20247;&#31609;&#27963;&#21160;&#30340;&#20027;&#39064;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
A Latent Dirichlet Allocation (LDA) Semantic Text Analytics Approach to Explore Topical Features in Charity Crowdfunding Campaigns. (arXiv:2401.02988v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02988
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#25991;&#26412;&#20998;&#26512;&#26694;&#26550;&#65292;&#21033;&#29992;&#28508;&#22312;&#29380;&#21033;&#20811;&#38647;&#20998;&#37197;&#65288;LDA&#65289;&#20174;&#24904;&#21892;&#20247;&#31609;&#27963;&#21160;&#30340;&#25991;&#26412;&#25551;&#36848;&#20013;&#25552;&#21462;&#28508;&#22312;&#20027;&#39064;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#20247;&#31609;&#27963;&#21160;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#20851;&#27880;&#20102;&#20247;&#31609;&#27963;&#21160;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#21253;&#25324;&#39033;&#30446;&#30446;&#26631;&#12289;&#25345;&#32493;&#26102;&#38388;&#20197;&#21450;&#25104;&#21151;&#31609;&#27454;&#30340;&#26377;&#24433;&#21709;&#21147;&#30340;&#39033;&#30446;&#31867;&#21035;&#12290;&#36825;&#20123;&#22240;&#32032;&#23545;&#20110;&#23547;&#27714;&#25424;&#36192;&#32773;&#25903;&#25345;&#30340;&#20225;&#19994;&#23478;&#26469;&#35828;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#24904;&#21892;&#20247;&#31609;&#39046;&#22495;&#20173;&#28982;&#30456;&#23545;&#26410;&#24320;&#21457;&#65292;&#32570;&#20047;&#23545;&#39537;&#21160;&#25424;&#36192;&#30340;&#21160;&#26426;&#30340;&#29702;&#35299;&#65292;&#36825;&#20123;&#25424;&#36192;&#36890;&#24120;&#32570;&#20047;&#20855;&#20307;&#30340;&#22238;&#25253;&#12290;&#19982;&#25552;&#20379;&#20855;&#20307;&#22238;&#25253;&#30340;&#20256;&#32479;&#20247;&#31609;&#19981;&#21516;&#65292;&#24904;&#21892;&#20247;&#31609;&#20381;&#36182;&#20110;&#26080;&#24418;&#30340;&#22238;&#25253;&#65292;&#22914;&#31246;&#25910;&#20248;&#24800;&#12289;&#35748;&#21487;&#24086;&#23376;&#25110;&#21672;&#35810;&#35282;&#33394;&#12290;&#36825;&#20123;&#32454;&#33410;&#36890;&#24120;&#23884;&#20837;&#22312;&#20247;&#31609;&#27963;&#21160;&#30340;&#21465;&#36848;&#20013;&#65292;&#20294;&#23545;&#24904;&#21892;&#20247;&#31609;&#20013;&#25991;&#26412;&#20869;&#23481;&#30340;&#20998;&#26512;&#26159;&#26377;&#38480;&#30340;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#25991;&#26412;&#20998;&#26512;&#26694;&#26550;&#65292;&#21033;&#29992;&#28508;&#22312;&#29380;&#21033;&#20811;&#38647;&#20998;&#37197;&#65288;LDA&#65289;&#20174;&#24904;&#21892;&#27963;&#21160;&#30340;&#25991;&#26412;&#25551;&#36848;&#20013;&#25552;&#21462;&#28508;&#22312;&#20027;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Crowdfunding in the realm of the Social Web has received substantial attention, with prior research examining various aspects of campaigns, including project objectives, durations, and influential project categories for successful fundraising. These factors are crucial for entrepreneurs seeking donor support. However, the terrain of charity crowdfunding within the Social Web remains relatively unexplored, lacking comprehension of the motivations driving donations that often lack concrete reciprocation. Distinct from conventional crowdfunding that offers tangible returns, charity crowdfunding relies on intangible rewards like tax advantages, recognition posts, or advisory roles. Such details are often embedded within campaign narratives, yet, the analysis of textual content in charity crowdfunding is limited. This study introduces an inventive text analytics framework, utilizing Latent Dirichlet Allocation (LDA) to extract latent themes from textual descriptions of charity campaigns. Th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#22810;&#22836;&#21518;&#39564;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#23454;&#20307;&#30340;&#20803;&#29305;&#24449;&#21644;&#27169;&#22411;&#30340;&#34920;&#31034;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#20316;&#20026;&#24230;&#37327;&#26631;&#20934;&#65292;&#26377;&#25928;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.02987</link><description>&lt;p&gt;
&#20320;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#26377;&#25913;&#36827;&#21527;&#65311;&#19968;&#31181;&#22522;&#20110;&#22810;&#22836;&#21518;&#39564;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Has Your Pretrained Model Improved? A Multi-head Posterior Based Approach. (arXiv:2401.02987v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#22810;&#22836;&#21518;&#39564;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#23454;&#20307;&#30340;&#20803;&#29305;&#24449;&#21644;&#27169;&#22411;&#30340;&#34920;&#31034;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#20316;&#20026;&#24230;&#37327;&#26631;&#20934;&#65292;&#26377;&#25928;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20986;&#29616;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#20851;&#31995;&#22411;&#25968;&#25454;&#38598;&#31561;&#39046;&#22495;&#20135;&#29983;&#20102;&#26174;&#33879;&#24433;&#21709;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#24341;&#21457;&#20102;&#22914;&#20309;&#26356;&#39640;&#25928;&#12289;&#26356;&#26377;&#25928;&#22320;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#21033;&#29992;&#19982;&#27599;&#20010;&#23454;&#20307;&#30456;&#20851;&#30340;&#20803;&#29305;&#24449;&#20316;&#20026;&#19990;&#30028;&#30693;&#35782;&#30340;&#26469;&#28304;&#65292;&#24182;&#21033;&#29992;&#27169;&#22411;&#30340;&#23454;&#20307;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#36825;&#20123;&#34920;&#31034;&#21644;&#20803;&#29305;&#24449;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#20316;&#20026;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#20010;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#20855;&#26377;&#20851;&#31995;&#22411;&#25968;&#25454;&#38598;&#12289;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22270;&#20687;&#27169;&#22411;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of pretrained models has significantly impacted from Natural Language Processing (NLP) and Computer Vision to relational datasets. Traditionally, these models are assessed through fine-tuned downstream tasks. However, this raises the question of how to evaluate these models more efficiently and more effectively. In this study, we explore a novel approach where we leverage the meta features associated with each entity as a source of worldly knowledge and employ entity representations from the models. We propose using the consistency between these representations and the meta features as a metric for evaluating pretrained models. Our method's effectiveness is demonstrated across various domains, including models with relational datasets, large language models and images models.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#22522;&#20110;&#23884;&#20837;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25490;&#24207;&#26041;&#27861;&#65292;&#29983;&#25104;AI&#27169;&#22411;&#20197;&#21450;&#20247;&#21253;&#21644;&#19987;&#23478;&#39537;&#21160;&#26041;&#27861;&#65292;&#26088;&#22312;&#30740;&#31350;&#22914;&#20309;&#36741;&#21161;&#27861;&#24459;&#21644;&#39046;&#22495;&#19987;&#23478;&#35782;&#21035;&#19982;&#19994;&#21153;&#27969;&#31243;&#30456;&#20851;&#30340;&#30417;&#31649;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2401.02986</link><description>&lt;p&gt;
&#35782;&#21035;&#19982;&#19994;&#21153;&#27969;&#31243;&#30456;&#20851;&#30340;&#30417;&#31649;&#35201;&#27714;&#65306;&#19968;&#39033;&#20851;&#20110;&#29983;&#25104;AI&#12289;&#22522;&#20110;&#23884;&#20837;&#30340;&#25490;&#24207;&#12289;&#20247;&#21253;&#21644;&#19987;&#23478;&#39537;&#21160;&#26041;&#27861;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Identification of Regulatory Requirements Relevant to Business Processes: A Comparative Study on Generative AI, Embedding-based Ranking, Crowd and Expert-driven Methods. (arXiv:2401.02986v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#22522;&#20110;&#23884;&#20837;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25490;&#24207;&#26041;&#27861;&#65292;&#29983;&#25104;AI&#27169;&#22411;&#20197;&#21450;&#20247;&#21253;&#21644;&#19987;&#23478;&#39537;&#21160;&#26041;&#27861;&#65292;&#26088;&#22312;&#30740;&#31350;&#22914;&#20309;&#36741;&#21161;&#27861;&#24459;&#21644;&#39046;&#22495;&#19987;&#23478;&#35782;&#21035;&#19982;&#19994;&#21153;&#27969;&#31243;&#30456;&#20851;&#30340;&#30417;&#31649;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#32455;&#38754;&#20020;&#30528;&#30830;&#20445;&#36981;&#23432;&#21508;&#31181;&#30417;&#31649;&#25991;&#20214;&#20013;&#36234;&#26469;&#36234;&#22810;&#35201;&#27714;&#30340;&#25361;&#25112;&#12290;&#21738;&#20123;&#35201;&#27714;&#26159;&#30456;&#20851;&#30340;&#21462;&#20915;&#20110;&#22914;&#32452;&#32455;&#30340;&#22320;&#29702;&#20301;&#32622;&#12289;&#39046;&#22495;&#12289;&#35268;&#27169;&#21644;&#19994;&#21153;&#27969;&#31243;&#31561;&#26041;&#38754;&#30340;&#22240;&#32032;&#12290;&#32771;&#34385;&#21040;&#36825;&#20123;&#24773;&#22659;&#22240;&#32032;&#65292;&#39318;&#20808;&#38656;&#35201;&#35782;&#21035;&#30456;&#20851;&#25991;&#20214;&#65288;&#20363;&#22914;&#27861;&#24459;&#12289;&#35268;&#21017;&#12289;&#25351;&#20196;&#12289;&#25919;&#31574;&#65289;&#65292;&#28982;&#21518;&#35814;&#32454;&#20998;&#26512;&#35782;&#21035;&#25991;&#20214;&#30340;&#21738;&#20123;&#37096;&#20998;&#19982;&#32473;&#23450;&#19994;&#21153;&#27969;&#31243;&#30340;&#21738;&#20010;&#27493;&#39588;&#30456;&#20851;&#12290;&#30446;&#21069;&#65292;&#35782;&#21035;&#19982;&#19994;&#21153;&#27969;&#31243;&#30456;&#20851;&#30340;&#30417;&#31649;&#35201;&#27714;&#20027;&#35201;&#30001;&#39046;&#22495;&#21644;&#27861;&#24459;&#19987;&#23478;&#25163;&#21160;&#23436;&#25104;&#65292;&#23545;&#20182;&#20204;&#26469;&#35828;&#26159;&#19968;&#39033;&#24040;&#22823;&#30340;&#24037;&#20316;&#37327;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#21487;&#33021;&#32463;&#24120;&#21464;&#21270;&#30340;&#22823;&#37327;&#30417;&#31649;&#25991;&#20214;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#36741;&#21161;&#27861;&#24459;&#21644;&#39046;&#22495;&#19987;&#23478;&#35780;&#20272;&#30456;&#20851;&#35201;&#27714;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#22522;&#20110;&#23884;&#20837;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25490;&#24207;&#26041;&#27861;&#65292;&#29983;&#25104;AI&#27169;&#22411;&#21450;&#20247;&#21253;&#21644;&#19987;&#23478;&#39537;&#21160;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Organizations face the challenge of ensuring compliance with an increasing amount of requirements from various regulatory documents. Which requirements are relevant depends on aspects such as the geographic location of the organization, its domain, size, and business processes. Considering these contextual factors, as a first step, relevant documents (e.g., laws, regulations, directives, policies) are identified, followed by a more detailed analysis of which parts of the identified documents are relevant for which step of a given business process. Nowadays the identification of regulatory requirements relevant to business processes is mostly done manually by domain and legal experts, posing a tremendous effort on them, especially for a large number of regulatory documents which might frequently change. Hence, this work examines how legal and domain experts can be assisted in the assessment of relevant requirements. For this, we compare an embedding-based NLP ranking method, a generativ
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19971;&#20010;&#20027;&#35201;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;GMAT&#32771;&#35797;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#20248;&#20110;&#20154;&#31867;&#32771;&#29983;&#65292;&#20854;&#20013;GPT-4 Turbo&#19981;&#20165;&#22312;&#20854;&#20182;&#27169;&#22411;&#20043;&#19978;&#65292;&#32780;&#19988;&#36229;&#36807;&#20102;&#39030;&#32423;&#21830;&#23398;&#38498;&#30740;&#31350;&#29983;&#30340;&#24179;&#22343;&#20998;&#25968;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#65292;&#26412;&#30740;&#31350;&#36824;&#32771;&#23519;&#20102;GPT-4 Turbo&#22312;&#35299;&#37322;&#31572;&#26696;&#12289;&#35780;&#20272;&#22238;&#31572;&#12289;&#35782;&#21035;&#38169;&#35823;&#12289;&#35843;&#25972;&#25351;&#23548;&#21644;&#29983;&#25104;&#26367;&#20195;&#22330;&#26223;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.02985</link><description>&lt;p&gt;
&#22312;GMAT&#19978;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#23545;&#26410;&#26469;&#21830;&#19994;&#25945;&#32946;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Evaluating Large Language Models on the GMAT: Implications for the Future of Business Education. (arXiv:2401.02985v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02985
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19971;&#20010;&#20027;&#35201;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;GMAT&#32771;&#35797;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#20248;&#20110;&#20154;&#31867;&#32771;&#29983;&#65292;&#20854;&#20013;GPT-4 Turbo&#19981;&#20165;&#22312;&#20854;&#20182;&#27169;&#22411;&#20043;&#19978;&#65292;&#32780;&#19988;&#36229;&#36807;&#20102;&#39030;&#32423;&#21830;&#23398;&#38498;&#30740;&#31350;&#29983;&#30340;&#24179;&#22343;&#20998;&#25968;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#65292;&#26412;&#30740;&#31350;&#36824;&#32771;&#23519;&#20102;GPT-4 Turbo&#22312;&#35299;&#37322;&#31572;&#26696;&#12289;&#35780;&#20272;&#22238;&#31572;&#12289;&#35782;&#21035;&#38169;&#35823;&#12289;&#35843;&#25972;&#25351;&#23548;&#21644;&#29983;&#25104;&#26367;&#20195;&#22330;&#26223;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#23588;&#20854;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#36805;&#36895;&#21457;&#23637;&#65292;&#20026;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#65292;&#28982;&#32780;&#20854;&#22312;&#21830;&#19994;&#25945;&#32946;&#20013;&#30340;&#20316;&#29992;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#39318;&#20010;&#22522;&#20934;&#26469;&#35780;&#20272;&#19971;&#20010;&#37325;&#35201;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21253;&#25324;OpenAI&#30340;&#27169;&#22411;&#65288;GPT-3.5 Turbo&#12289;GPT-4&#21644;GPT-4 Turbo&#65289;&#12289;Google&#30340;&#27169;&#22411;&#65288;PaLM 2&#12289;Gemini 1.0 Pro&#65289;&#21644;Anthropic&#30340;&#27169;&#22411;&#65288;Claude 2&#21644;Claude 2.1&#65289;&#65292;&#22312;GMAT&#32771;&#35797;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#26174;&#31034;&#65292;&#22823;&#22810;&#25968;&#35821;&#35328;&#27169;&#22411;&#20248;&#20110;&#20154;&#31867;&#32771;&#29983;&#65292;&#20854;&#20013;GPT-4 Turbo&#19981;&#20165;&#22312;&#20854;&#20182;&#27169;&#22411;&#20043;&#19978;&#65292;&#32780;&#19988;&#36229;&#36807;&#20102;&#39030;&#32423;&#21830;&#23398;&#38498;&#30740;&#31350;&#29983;&#30340;&#24179;&#22343;&#20998;&#25968;&#12290;&#36890;&#36807;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#26412;&#30740;&#31350;&#32771;&#23519;&#20102;GPT-4 Turbo&#22312;&#35299;&#37322;&#31572;&#26696;&#12289;&#35780;&#20272;&#22238;&#31572;&#12289;&#35782;&#21035;&#38169;&#35823;&#12289;&#35843;&#25972;&#25351;&#23548;&#21644;&#29983;&#25104;&#26367;&#20195;&#22330;&#26223;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid evolution of artificial intelligence (AI), especially in the domain of Large Language Models (LLMs) and generative AI, has opened new avenues for application across various fields, yet its role in business education remains underexplored. This study introduces the first benchmark to assess the performance of seven major LLMs, OpenAI's models (GPT-3.5 Turbo, GPT-4, and GPT-4 Turbo), Google's models (PaLM 2, Gemini 1.0 Pro), and Anthropic's models (Claude 2 and Claude 2.1), on the GMAT, which is a key exam in the admission process for graduate business programs. Our analysis shows that most LLMs outperform human candidates, with GPT-4 Turbo not only outperforming the other models but also surpassing the average scores of graduate students at top business schools. Through a case study, this research examines GPT-4 Turbo's ability to explain answers, evaluate responses, identify errors, tailor instructions, and generate alternative scenarios. The latest LLM versions, GPT-4 Turbo,
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#30740;&#31350;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#25252;&#29702;&#20013;&#30340;&#24212;&#29992;&#21644;&#32467;&#26524;&#36827;&#34892;&#20102;&#32508;&#21512;&#20998;&#26512;&#65292;&#21457;&#29616;&#20854;&#22312;&#35786;&#26029;&#12289;&#27835;&#30103;&#21644;&#24739;&#32773;&#21442;&#19982;&#22686;&#24378;&#31561;&#26041;&#38754;&#20855;&#26377;&#22810;&#26679;&#21270;&#30340;&#24212;&#29992;&#12290;&#21516;&#26102;&#65292;&#35813;&#30740;&#31350;&#36824;&#35782;&#21035;&#21644;&#35752;&#35770;&#20102;&#22312;&#36825;&#20123;&#19987;&#19994;&#39046;&#22495;&#20013;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2401.02984</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#25252;&#29702;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#39033;&#32508;&#36848;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Large Language Models in Mental Health Care: a Scoping Review. (arXiv:2401.02984v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02984
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#30740;&#31350;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#25252;&#29702;&#20013;&#30340;&#24212;&#29992;&#21644;&#32467;&#26524;&#36827;&#34892;&#20102;&#32508;&#21512;&#20998;&#26512;&#65292;&#21457;&#29616;&#20854;&#22312;&#35786;&#26029;&#12289;&#27835;&#30103;&#21644;&#24739;&#32773;&#21442;&#19982;&#22686;&#24378;&#31561;&#26041;&#38754;&#20855;&#26377;&#22810;&#26679;&#21270;&#30340;&#24212;&#29992;&#12290;&#21516;&#26102;&#65292;&#35813;&#30740;&#31350;&#36824;&#35782;&#21035;&#21644;&#35752;&#35770;&#20102;&#22312;&#36825;&#20123;&#19987;&#19994;&#39046;&#22495;&#20013;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20351;&#29992;&#36234;&#26469;&#36234;&#24191;&#27867;&#65292;&#38656;&#35201;&#23545;&#23427;&#20204;&#22312;&#24515;&#29702;&#20581;&#24247;&#25252;&#29702;&#39046;&#22495;&#30340;&#24212;&#29992;&#21644;&#32467;&#26524;&#36827;&#34892;&#20840;&#38754;&#30340;&#32508;&#36848;&#12290;&#26412;&#32508;&#36848;&#30740;&#31350;&#26088;&#22312;&#23545;LLMs&#22312;&#24515;&#29702;&#20581;&#24247;&#25252;&#29702;&#20013;&#30340;&#29616;&#26377;&#21457;&#23637;&#21644;&#24212;&#29992;&#36827;&#34892;&#25209;&#21028;&#24615;&#20998;&#26512;&#65292;&#31361;&#20986;&#23427;&#20204;&#30340;&#25104;&#21151;&#65292;&#24182;&#35782;&#21035;&#36825;&#20123;&#19987;&#19994;&#39046;&#22495;&#20013;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#12290;&#26448;&#26009;&#21644;&#26041;&#27861;&#65306;2023&#24180;11&#26376;&#65292;&#22312;PubMed&#12289;Web of Science&#12289;Google Scholar&#12289;arXiv&#12289;medRxiv&#21644;PsyArXiv&#20845;&#20010;&#25968;&#25454;&#24211;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#25991;&#29486;&#25628;&#32034;&#65292;&#36981;&#24490;2020&#24180;&#29256;&#30340;&#8220;&#31995;&#32479;&#35780;&#20215;&#21644;Meta&#20998;&#26512;&#30340;&#39318;&#36873;&#25253;&#21578;&#39033;&#30446;&#8221;&#65288;PRISMA&#65289;&#25351;&#21335;&#12290;&#26368;&#21021;&#35782;&#21035;&#20102;313&#31687;&#20986;&#29256;&#29289;&#65292;&#25353;&#29031;&#30740;&#31350;&#32435;&#20837;&#26631;&#20934;&#65292;&#26368;&#32456;&#36873;&#25321;&#20102;34&#31687;&#20986;&#29256;&#29289;&#36827;&#34892;&#32508;&#36848;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#21457;&#29616;&#20102;LLMs&#22312;&#24515;&#29702;&#20581;&#24247;&#25252;&#29702;&#20013;&#30340;&#22810;&#31181;&#24212;&#29992;&#65292;&#21253;&#25324;&#35786;&#26029;&#12289;&#27835;&#30103;&#12289;&#24739;&#32773;&#21442;&#19982;&#22686;&#24378;&#31561;&#12290;&#20851;&#38190;&#25361;&#25112;&#21644;&#38480;&#21046;&#26041;&#38754;&#30340;&#21457;&#29616;&#23558;&#34987;&#24635;&#32467;&#21644;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: The growing use of large language models (LLMs) stimulates a need for a comprehensive review of their applications and outcomes in mental health care contexts. This scoping review aims to critically analyze the existing development and applications of LLMs in mental health care, highlighting their successes and identifying their challenges and limitations in these specialized fields. Materials and Methods: A broad literature search was conducted in November 2023 using six databases (PubMed, Web of Science, Google Scholar, arXiv, medRxiv, and PsyArXiv) following the 2020 version of the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines. A total of 313 publications were initially identified, and after applying the study inclusion criteria, 34 publications were selected for the final review. Results: We identified diverse applications of LLMs in mental health care, including diagnosis, therapy, patient engagement enhancement, etc. Key challen
&lt;/p&gt;</description></item><item><title>BIBench&#26159;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21830;&#19994;&#26234;&#33021;&#65288;BI&#65289;&#25968;&#25454;&#20998;&#26512;&#39046;&#22495;&#20013;&#33021;&#21147;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#36890;&#36807;&#27979;&#35797;&#27169;&#22411;&#22312;BI&#22522;&#30784;&#30693;&#35782;&#12289;&#24212;&#29992;&#30693;&#35782;&#21644;&#25216;&#26415;&#25216;&#33021;&#19977;&#20010;&#32500;&#24230;&#19978;&#30340;&#34920;&#29616;&#26469;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2401.02982</link><description>&lt;p&gt;
BIBench: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#20998;&#26512;&#30693;&#35782;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
BIBench: Benchmarking Data Analysis Knowledge of Large Language Models. (arXiv:2401.02982v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02982
&lt;/p&gt;
&lt;p&gt;
BIBench&#26159;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21830;&#19994;&#26234;&#33021;&#65288;BI&#65289;&#25968;&#25454;&#20998;&#26512;&#39046;&#22495;&#20013;&#33021;&#21147;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#36890;&#36807;&#27979;&#35797;&#27169;&#22411;&#22312;BI&#22522;&#30784;&#30693;&#35782;&#12289;&#24212;&#29992;&#30693;&#35782;&#21644;&#25216;&#26415;&#25216;&#33021;&#19977;&#20010;&#32500;&#24230;&#19978;&#30340;&#34920;&#29616;&#26469;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#25968;&#25454;&#20998;&#26512;&#30340;&#19987;&#19994;&#39046;&#22495;&#20013;&#30340;&#29087;&#32451;&#24230;&#21644;&#21487;&#38752;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#20197;&#25968;&#25454;&#39537;&#21160;&#24605;&#32500;&#20026;&#37325;&#28857;&#30340;&#39046;&#22495;&#20013;&#65292;&#20173;&#28982;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;BIBench&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#21830;&#19994;&#26234;&#33021;&#65288;BI&#65289;&#30340;&#32972;&#26223;&#19979;&#30340;&#25968;&#25454;&#20998;&#26512;&#33021;&#21147;&#12290;BIBench&#36890;&#36807;&#19977;&#20010;&#32500;&#24230;&#35780;&#20272;LLMs&#65306;1&#65289;BI&#22522;&#30784;&#30693;&#35782;&#65292;&#35780;&#20272;&#27169;&#22411;&#30340;&#25968;&#20540;&#25512;&#29702;&#33021;&#21147;&#21644;&#23545;&#37329;&#34701;&#27010;&#24565;&#30340;&#29087;&#24713;&#31243;&#24230;&#65307;2&#65289;BI&#30693;&#35782;&#24212;&#29992;&#65292;&#30830;&#23450;&#27169;&#22411;&#24555;&#36895;&#29702;&#35299;&#25991;&#26412;&#20449;&#24687;&#24182;&#20174;&#22810;&#20010;&#35270;&#35282;&#29983;&#25104;&#20998;&#26512;&#38382;&#39064;&#30340;&#33021;&#21147;&#65307;3&#65289;BI&#25216;&#26415;&#25216;&#33021;&#65292;&#26816;&#26597;&#27169;&#22411;&#20351;&#29992;&#25216;&#26415;&#30693;&#35782;&#35299;&#20915;&#29616;&#23454;&#25968;&#25454;&#20998;&#26512;&#25361;&#25112;&#30340;&#33021;&#21147;&#12290;BIBench&#21253;&#25324;11&#20010;&#23376;&#20219;&#21153;&#65292;&#28085;&#30422;&#20998;&#31867;&#12289;&#25552;&#21462;&#21644;&#29983;&#25104;&#19977;&#31181;&#20219;&#21153;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated impressive capabilities across a wide range of tasks. However, their proficiency and reliability in the specialized domain of Data Analysis, particularly with a focus on data-driven thinking, remain uncertain. To bridge this gap, we introduce BIBench, a comprehensive benchmark designed to evaluate the data analysis capabilities of LLMs within the context of Business Intelligence (BI). BIBench assesses LLMs across three dimensions: 1) BI foundational knowledge, evaluating the models' numerical reasoning and familiarity with financial concepts; 2) BI knowledge application, determining the models' ability to quickly comprehend textual information and generate analysis questions from multiple views; and 3) BI technical skills, examining the models' use of technical knowledge to address real-world data analysis challenges. BIBench comprises 11 sub-tasks, spanning three categories of task types: classification, extraction, and generation. Additi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#39046;&#22495;&#29305;&#23450;LLM&#30340;&#24494;&#35843;&#21644;&#21033;&#29992;&#26041;&#27861;&#65292;&#20197;&#37329;&#34701;&#39046;&#22495;&#20026;&#20363;&#65292;&#35814;&#32454;&#20171;&#32461;&#20102;&#25968;&#25454;&#38598;&#36873;&#25321;&#12289;&#39044;&#22788;&#29702;&#12289;&#27169;&#22411;&#36873;&#25321;&#20197;&#21450;&#22312;&#37329;&#34701;&#39046;&#22495;LLM&#24494;&#35843;&#20013;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#30740;&#31350;&#25506;&#35752;&#20102;&#39046;&#22495;&#29305;&#23450;&#35789;&#27719;&#30340;&#26500;&#24314;&#21644;&#23433;&#20840;&#21512;&#35268;&#24615;&#30340;&#32771;&#34385;&#22240;&#32032;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#37329;&#34701;&#39046;&#22495;&#29983;&#25104;&#39046;&#22495;&#29305;&#23450;LLM&#30340;&#36807;&#31243;&#21644;&#23454;&#26045;&#26041;&#27861;&#12290;&#22810;&#31181;&#37329;&#34701;&#26696;&#20363;&#34987;&#28085;&#30422;&#22312;&#20869;&#65292;&#21253;&#25324;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#12289;&#37329;&#34701;&#26032;&#38395;&#24773;&#32490;&#20998;&#26512;&#12289;&#33258;&#21160;&#25991;&#26723;&#22788;&#29702;&#12289;&#30740;&#31350;&#21644;&#20449;&#24687;&#25552;&#21462;&#31561;&#12290;</title><link>http://arxiv.org/abs/2401.02981</link><description>&lt;p&gt;
&#39046;&#22495;&#29305;&#23450;LLM&#30340;&#24494;&#35843;&#21644;&#21033;&#29992;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning and Utilization Methods of Domain-specific LLMs. (arXiv:2401.02981v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02981
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#39046;&#22495;&#29305;&#23450;LLM&#30340;&#24494;&#35843;&#21644;&#21033;&#29992;&#26041;&#27861;&#65292;&#20197;&#37329;&#34701;&#39046;&#22495;&#20026;&#20363;&#65292;&#35814;&#32454;&#20171;&#32461;&#20102;&#25968;&#25454;&#38598;&#36873;&#25321;&#12289;&#39044;&#22788;&#29702;&#12289;&#27169;&#22411;&#36873;&#25321;&#20197;&#21450;&#22312;&#37329;&#34701;&#39046;&#22495;LLM&#24494;&#35843;&#20013;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#30740;&#31350;&#25506;&#35752;&#20102;&#39046;&#22495;&#29305;&#23450;&#35789;&#27719;&#30340;&#26500;&#24314;&#21644;&#23433;&#20840;&#21512;&#35268;&#24615;&#30340;&#32771;&#34385;&#22240;&#32032;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#37329;&#34701;&#39046;&#22495;&#29983;&#25104;&#39046;&#22495;&#29305;&#23450;LLM&#30340;&#36807;&#31243;&#21644;&#23454;&#26045;&#26041;&#27861;&#12290;&#22810;&#31181;&#37329;&#34701;&#26696;&#20363;&#34987;&#28085;&#30422;&#22312;&#20869;&#65292;&#21253;&#25324;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#12289;&#37329;&#34701;&#26032;&#38395;&#24773;&#32490;&#20998;&#26512;&#12289;&#33258;&#21160;&#25991;&#26723;&#22788;&#29702;&#12289;&#30740;&#31350;&#21644;&#20449;&#24687;&#25552;&#21462;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#21457;&#24067;&#30340;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#65292;&#20294;&#20851;&#20110;&#39046;&#22495;&#29305;&#23450;LLM&#30340;&#24494;&#35843;&#21644;&#24212;&#29992;&#30340;&#30740;&#31350;&#20173;&#28982;&#24456;&#23569;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#39046;&#22495;&#29305;&#23450;LLM&#30340;&#24494;&#35843;&#21644;&#21033;&#29992;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;LLM&#30340;&#36235;&#21183;&#12289;&#22522;&#30784;&#27169;&#22411;&#21644;&#29992;&#20110;&#39046;&#22495;&#29305;&#23450;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#20197;&#37329;&#34701;&#34892;&#19994;&#20026;&#20363;&#65292;&#35814;&#32454;&#20171;&#32461;&#20102;&#25968;&#25454;&#38598;&#36873;&#25321;&#12289;&#39044;&#22788;&#29702;&#12289;&#27169;&#22411;&#36873;&#25321;&#20197;&#21450;&#22312;&#37329;&#34701;&#39046;&#22495;LLM&#24494;&#35843;&#20013;&#20851;&#38190;&#30340;&#32771;&#34385;&#22240;&#32032;&#12290;&#38024;&#23545;&#37329;&#34701;&#25968;&#25454;&#30340;&#29420;&#29305;&#29305;&#28857;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#39046;&#22495;&#29305;&#23450;&#35789;&#27719;&#30340;&#26500;&#24314;&#65292;&#20197;&#21450;&#23433;&#20840;&#24615;&#21644;&#21512;&#35268;&#24615;&#30340;&#32771;&#34385;&#22240;&#32032;&#12290;&#22312;LLM&#24494;&#35843;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#26412;&#30740;&#31350;&#27010;&#36848;&#20102;&#22312;&#37329;&#34701;&#39046;&#22495;&#29983;&#25104;&#39046;&#22495;&#29305;&#23450;LLM&#30340;&#36807;&#31243;&#21644;&#23454;&#26045;&#26041;&#27861;&#12290;&#21253;&#25324;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#12289;&#37329;&#34701;&#26032;&#38395;&#24773;&#32490;&#20998;&#26512;&#12289;&#33258;&#21160;&#25991;&#26723;&#22788;&#29702;&#12289;&#30740;&#31350;&#21644;&#20449;&#24687;&#25552;&#21462;&#31561;&#22810;&#31181;&#37329;&#34701;&#26696;&#20363;&#34987;&#28085;&#30422;&#22312;&#20869;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent releases of pre-trained Large Language Models (LLMs) have gained considerable traction, yet research on fine-tuning and employing domain-specific LLMs remains scarce. This study investigates approaches for fine-tuning and leveraging domain-specific LLMs, highlighting trends in LLMs, foundational models, and methods for domain-specific pre-training. Focusing on the financial sector, it details dataset selection, preprocessing, model choice, and considerations crucial for LLM fine-tuning in finance. Addressing the unique characteristics of financial data, the study explores the construction of domain-specific vocabularies and considerations for security and regulatory compliance. In the practical application of LLM fine-tuning, the study outlines the procedure and implementation for generating domain-specific LLMs in finance. Various financial cases, including stock price prediction, sentiment analysis of financial news, automated document processing, research, information extract
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#23545;&#34920;&#29616;&#21147;&#38050;&#29748;&#28436;&#22863;&#29305;&#24449;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#27979;&#35797;&#20102;&#20116;&#20010;&#23884;&#20837;&#27169;&#22411;&#21450;&#20854;&#30456;&#20284;&#24615;&#32467;&#26500;&#19982;&#30495;&#20540;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#24182;&#36827;&#34892;&#20102;&#36827;&#19968;&#27493;&#35780;&#20272;&#12290;&#23884;&#20837;&#27169;&#22411;&#30340;&#36136;&#37327;&#22312;&#36825;&#26041;&#38754;&#26174;&#31034;&#20986;&#24456;&#22823;&#30340;&#24046;&#24322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.02979</link><description>&lt;p&gt;
&#25105;&#20204;&#22312;&#25551;&#36848;&#21516;&#26679;&#30340;&#22768;&#38899;&#21527;&#65311;&#23545;&#34920;&#29616;&#21147;&#38050;&#29748;&#28436;&#22863;&#35789;&#23884;&#20837;&#31354;&#38388;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Are we describing the same sound? An analysis of word embedding spaces of expressive piano performance. (arXiv:2401.02979v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02979
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#23545;&#34920;&#29616;&#21147;&#38050;&#29748;&#28436;&#22863;&#29305;&#24449;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#27979;&#35797;&#20102;&#20116;&#20010;&#23884;&#20837;&#27169;&#22411;&#21450;&#20854;&#30456;&#20284;&#24615;&#32467;&#26500;&#19982;&#30495;&#20540;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#24182;&#36827;&#34892;&#20102;&#36827;&#19968;&#27493;&#35780;&#20272;&#12290;&#23884;&#20837;&#27169;&#22411;&#30340;&#36136;&#37327;&#22312;&#36825;&#26041;&#38754;&#26174;&#31034;&#20986;&#24456;&#22823;&#30340;&#24046;&#24322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#23884;&#20837;&#22312;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#20449;&#24687;&#26816;&#32034;&#20013;&#36215;&#21040;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#23884;&#20837;&#27169;&#22411;&#23558;&#21333;&#35789;&#21644;&#19978;&#19979;&#25991;&#34920;&#31034;&#20026;&#21521;&#37327;&#65292;&#20854;&#31354;&#38388;&#37197;&#32622;&#26159;&#26681;&#25454;&#22823;&#22411;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#21333;&#35789;&#30340;&#20998;&#24067;&#23548;&#20986;&#30340;&#12290;&#23613;&#31649;&#36825;&#20123;&#34920;&#31034;&#19968;&#33324;&#38750;&#24120;&#24378;&#22823;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#26410;&#33021;&#32771;&#34385;&#21040;&#32454;&#31890;&#24230;&#30340;&#39046;&#22495;&#29305;&#23450;&#32454;&#24494;&#24046;&#21035;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36825;&#31181;&#23545;&#34920;&#29616;&#21147;&#38050;&#29748;&#28436;&#22863;&#29305;&#24449;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#38899;&#20048;&#30740;&#31350;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#33258;&#30001;&#25991;&#26412;&#28436;&#22863;&#29305;&#24449;&#30340;&#27880;&#37322;&#65292;&#24182;&#36827;&#34892;&#20102;&#19968;&#20010;&#21518;&#32493;&#30740;&#31350;&#65292;&#23558;&#27880;&#37322;&#20998;&#31867;&#25104;&#19981;&#21516;&#30340;&#32858;&#31867;&#12290;&#25105;&#20204;&#24471;&#20986;&#20102;&#19968;&#20010;&#29305;&#23450;&#39046;&#22495;&#35821;&#20041;&#30456;&#20284;&#24615;&#32467;&#26500;&#30340;&#30495;&#20540;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#20116;&#20010;&#23884;&#20837;&#27169;&#22411;&#21450;&#20854;&#30456;&#20284;&#24615;&#32467;&#26500;&#19982;&#30495;&#20540;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35780;&#20272;&#20102;&#19978;&#19979;&#25991;&#25552;&#31034;&#12289;&#20013;&#24515;&#24230;&#38477;&#20302;&#12289;&#36328;&#27169;&#24577;&#30456;&#20284;&#24615;&#21644;k-means&#32858;&#31867;&#30340;&#24433;&#21709;&#12290;&#23884;&#20837;&#27169;&#22411;&#30340;&#36136;&#37327;&#22312;&#36825;&#26041;&#38754;&#26174;&#31034;&#20986;&#24456;&#22823;&#30340;&#24046;&#24322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic embeddings play a crucial role in natural language-based information retrieval. Embedding models represent words and contexts as vectors whose spatial configuration is derived from the distribution of words in large text corpora. While such representations are generally very powerful, they might fail to account for fine-grained domain-specific nuances. In this article, we investigate this uncertainty for the domain of characterizations of expressive piano performance. Using a music research dataset of free text performance characterizations and a follow-up study sorting the annotations into clusters, we derive a ground truth for a domain-specific semantic similarity structure. We test five embedding models and their similarity structure for correspondence with the ground truth. We further assess the effects of contextualizing prompts, hubness reduction, cross-modal similarity, and k-means clustering. The quality of embedding models shows great variability with respect to this 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#19982;&#34394;&#25311;&#20249;&#20276;Zo&#20114;&#21160;&#30340;&#21160;&#26426;&#65292;&#24635;&#32467;&#20986;&#20102;&#22810;&#31181;&#22686;&#21152;&#20114;&#21160;&#24615;&#30340;&#26041;&#27861;&#65292;&#20026;&#29983;&#25104;&#24335;AI&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#20511;&#37492;&#12290;</title><link>http://arxiv.org/abs/2401.02978</link><description>&lt;p&gt;
&#20174;&#29983;&#25104;&#24335;AI&#21069;&#36744;&#20013;&#23398;&#20064;&#8212;&#8212;&#19982;&#23545;&#35805;&#20195;&#29702;&#20114;&#21160;&#30340;&#35768;&#22810;&#21160;&#26426;
&lt;/p&gt;
&lt;p&gt;
Learning from a Generative AI Predecessor -- The Many Motivations for Interacting with Conversational Agents. (arXiv:2401.02978v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02978
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#19982;&#34394;&#25311;&#20249;&#20276;Zo&#20114;&#21160;&#30340;&#21160;&#26426;&#65292;&#24635;&#32467;&#20986;&#20102;&#22810;&#31181;&#22686;&#21152;&#20114;&#21160;&#24615;&#30340;&#26041;&#27861;&#65292;&#20026;&#29983;&#25104;&#24335;AI&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#20511;&#37492;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35201;&#20351;&#29983;&#25104;&#24335;AI&#25104;&#21151;&#65292;&#23427;&#24517;&#39035;&#20855;&#22791;&#22810;&#20040;&#24341;&#20154;&#20837;&#32988;&#30340;&#23545;&#35805;&#33021;&#21147;&#65311;&#36817;60&#24180;&#26469;&#65292;&#19968;&#20123;&#23545;&#35805;&#20195;&#29702;&#20250;&#22238;&#24212;&#20219;&#20309;&#38382;&#39064;&#25110;&#35780;&#35770;&#20197;&#20445;&#25345;&#23545;&#35805;&#30340;&#36827;&#34892;&#12290;&#36817;&#24180;&#26469;&#65292;&#35768;&#22810;&#20154;&#24320;&#22987;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25110;&#22797;&#26434;&#30340;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#20363;&#22914;Tay&#12289;Xiaoice&#12289;Zo&#12289;Hugging Face&#12289;Kuki&#21644;Replika&#12290;&#19982;&#29983;&#25104;&#24335;AI&#19981;&#21516;&#30340;&#26159;&#65292;&#23427;&#20204;&#20851;&#27880;&#30340;&#26159;&#20114;&#21160;&#24615;&#65292;&#32780;&#19981;&#26159;&#19987;&#19994;&#30693;&#35782;&#12290;&#25968;&#30334;&#19975;&#20154;&#34987;&#28608;&#21457;&#36215;&#19982;&#23427;&#20204;&#36827;&#34892;&#20114;&#21160;&#12290;&#37027;&#20040;&#36825;&#20123;&#21560;&#24341;&#21147;&#26159;&#20160;&#20040;&#21602;&#65311;&#22914;&#26524;&#29983;&#25104;&#24335;AI&#21516;&#26679;&#24341;&#20154;&#20837;&#32988;&#65292;&#23427;&#20250;&#20570;&#24471;&#26356;&#22909;&#21527;&#65292;&#36824;&#26159;&#24212;&#35813;&#20943;&#23569;&#20114;&#21160;&#24615;&#65311;&#22312;&#29983;&#25104;&#24335;AI&#20986;&#29616;&#20043;&#21069;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#23450;&#37327;&#21644;&#23450;&#24615;&#20998;&#26512;&#65292;&#20197;&#20102;&#35299;&#25968;&#30334;&#19975;&#20154;&#19982;&#24494;&#36719;&#20249;&#20276;Zo&#36827;&#34892;&#20114;&#21160;&#30340;&#21160;&#26426;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;2000&#20010;&#21311;&#21517;&#29992;&#25143;&#30340;&#23436;&#25972;&#32842;&#22825;&#35760;&#24405;&#65292;&#24182;&#30830;&#23450;&#20102;&#25968;&#21313;&#31181;&#20154;&#20204;&#19982;&#35813;&#36719;&#20214;&#36827;&#34892;&#20114;&#21160;&#30340;&#21160;&#26426;&#12290;&#35774;&#35745;&#24072;&#20204;&#23398;&#20250;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#20114;&#21160;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
For generative AI to succeed, how engaging a conversationalist must it be? For almost sixty years, some conversational agents have responded to any question or comment to keep a conversation going. In recent years, several utilized machine learning or sophisticated language processing, such as Tay, Xiaoice, Zo, Hugging Face, Kuki, and Replika. Unlike generative AI, they focused on engagement, not expertise. Millions of people were motivated to engage with them. What were the attractions? Will generative AI do better if it is equally engaging, or should it be less engaging? Prior to the emergence of generative AI, we conducted a large-scale quantitative and qualitative analysis to learn what motivated millions of people to engage with one such 'virtual companion,' Microsoft's Zo. We examined the complete chat logs of 2000 anonymized people. We identified over a dozen motivations that people had for interacting with this software. Designers learned different ways to increase engagement. 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;GPT&#27169;&#22411;&#20013;&#20998;&#26512;&#21644;&#20462;&#25913;&#23454;&#20307;&#20851;&#31995;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20851;&#31995;&#36861;&#36394;&#25216;&#26415;&#65292;&#25105;&#20204;&#35782;&#21035;&#20102;MLP&#27169;&#22359;&#21644;&#27880;&#24847;&#26426;&#21046;&#22312;&#22788;&#29702;&#20851;&#31995;&#20449;&#24687;&#26041;&#38754;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#29305;&#24322;&#24615;&#21644;&#27867;&#21270;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#24179;&#34913;&#30340;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2401.02976</link><description>&lt;p&gt;
&#22312;GPT&#27169;&#22411;&#20013;&#36861;&#36394;&#21644;&#32534;&#36753;&#20851;&#31995;&#20851;&#32852;
&lt;/p&gt;
&lt;p&gt;
Trace and Edit Relation Associations in GPT. (arXiv:2401.02976v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;GPT&#27169;&#22411;&#20013;&#20998;&#26512;&#21644;&#20462;&#25913;&#23454;&#20307;&#20851;&#31995;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20851;&#31995;&#36861;&#36394;&#25216;&#26415;&#65292;&#25105;&#20204;&#35782;&#21035;&#20102;MLP&#27169;&#22359;&#21644;&#27880;&#24847;&#26426;&#21046;&#22312;&#22788;&#29702;&#20851;&#31995;&#20449;&#24687;&#26041;&#38754;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#29305;&#24322;&#24615;&#21644;&#27867;&#21270;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#24179;&#34913;&#30340;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#21644;&#20462;&#25913;GPT&#27169;&#22411;&#20013;&#30340;&#23454;&#20307;&#20851;&#31995;&#65292;&#19982;ROME&#30340;&#22522;&#20110;&#23454;&#20307;&#30340;&#26041;&#27861;&#19981;&#21516;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20851;&#31995;&#36861;&#36394;&#25216;&#26415;&#65292;&#20197;&#20102;&#35299;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#23545;&#20851;&#31995;&#21028;&#26029;&#30340;&#24433;&#21709;&#12290;&#20351;&#29992;FewRel&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35782;&#21035;&#20102;MLP&#27169;&#22359;&#21644;&#27880;&#24847;&#26426;&#21046;&#22312;&#22788;&#29702;&#20851;&#31995;&#20449;&#24687;&#26041;&#38754;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#19978;&#19982;ROME&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#26174;&#31034;&#20986;&#22312;&#29305;&#24322;&#24615;&#21644;&#27867;&#21270;&#24615;&#26041;&#38754;&#30340;&#24179;&#34913;&#25913;&#21892;&#65292;&#31361;&#26174;&#20102;&#25805;&#32437;&#26089;&#26399;&#23618;&#27169;&#22359;&#20197;&#25552;&#39640;&#27169;&#22411;&#29702;&#35299;&#21644;&#20934;&#30830;&#24615;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study introduces a novel approach for analyzing and modifying entity relationships in GPT models, diverging from ROME's entity-focused methods. We develop a relation tracing technique to understand the influence of language model computations on relationship judgments. Using the FewRel dataset, we identify key roles of MLP modules and attention mechanisms in processing relationship information. Our method, tested against ROME on a new dataset, shows improved balance in specificity and generalization, underscoring the potential of manipulating early-layer modules for enhanced model understanding and accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#21307;&#30103;&#22120;&#26800;&#34892;&#19994;&#27861;&#35268;&#20107;&#21153;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#27861;&#35268;&#35821;&#35328;&#12289;&#27969;&#31243;&#12289;&#20840;&#29699;&#23618;&#38754;&#12289;&#25968;&#25454;&#24211;&#21644;&#20135;&#21697;&#23618;&#38754;&#31561;&#20116;&#20010;&#39046;&#22495;&#30340;&#20851;&#38190;&#22797;&#26434;&#24615;&#26469;&#28304;&#12290;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#38656;&#35201;&#31616;&#21270;&#27861;&#35268;&#21512;&#35268;&#12289;&#25913;&#21892;&#27861;&#35268;&#26426;&#26500;&#19982;&#34892;&#19994;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#20114;&#21160;&#20197;&#21450;&#21046;&#23450;&#36866;&#24212;&#24615;&#26694;&#26550;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.02975</link><description>&lt;p&gt;
&#25581;&#31034;&#21307;&#30103;&#20135;&#21697;&#27861;&#35268;&#20107;&#21153;&#30340;&#22797;&#26434;&#24615;&#65306;&#21033;&#29992;&#24320;&#25918;&#32534;&#30721;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#23450;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Uncovering Regulatory Affairs Complexity in Medical Products: A Qualitative Assessment Utilizing Open Coding and Natural Language Processing (NLP). (arXiv:2401.02975v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#21307;&#30103;&#22120;&#26800;&#34892;&#19994;&#27861;&#35268;&#20107;&#21153;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#27861;&#35268;&#35821;&#35328;&#12289;&#27969;&#31243;&#12289;&#20840;&#29699;&#23618;&#38754;&#12289;&#25968;&#25454;&#24211;&#21644;&#20135;&#21697;&#23618;&#38754;&#31561;&#20116;&#20010;&#39046;&#22495;&#30340;&#20851;&#38190;&#22797;&#26434;&#24615;&#26469;&#28304;&#12290;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#38656;&#35201;&#31616;&#21270;&#27861;&#35268;&#21512;&#35268;&#12289;&#25913;&#21892;&#27861;&#35268;&#26426;&#26500;&#19982;&#34892;&#19994;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#20114;&#21160;&#20197;&#21450;&#21046;&#23450;&#36866;&#24212;&#24615;&#26694;&#26550;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#21307;&#30103;&#22120;&#26800;&#34892;&#19994;&#27861;&#35268;&#20107;&#21153;&#30340;&#22797;&#26434;&#24615;&#65292;&#36825;&#26159;&#24433;&#21709;&#24066;&#22330;&#20934;&#20837;&#21644;&#24739;&#32773;&#25252;&#29702;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#36890;&#36807;&#23450;&#24615;&#30740;&#31350;&#65292;&#25105;&#20204;&#23547;&#27714;&#19987;&#23478;&#27934;&#23519;&#21147;&#65292;&#20197;&#20102;&#35299;&#23548;&#33268;&#36825;&#31181;&#22797;&#26434;&#24615;&#30340;&#22240;&#32032;&#12290;&#35813;&#30740;&#31350;&#28041;&#21450;&#23545;28&#20301;&#21307;&#30103;&#22120;&#26800;&#20844;&#21496;&#19987;&#19994;&#20154;&#22763;&#36827;&#34892;&#30340;&#21322;&#32467;&#26500;&#21270;&#35775;&#35848;&#65292;&#36825;&#20123;&#19987;&#19994;&#20154;&#22763;&#19987;&#27880;&#20110;&#27861;&#35268;&#20107;&#21153;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;&#36890;&#36807;&#37319;&#29992;&#24320;&#25918;&#32534;&#30721;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25216;&#26415;&#23545;&#36825;&#20123;&#35775;&#35848;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#27861;&#35268;&#29615;&#22659;&#20013;&#30340;&#20851;&#38190;&#22797;&#26434;&#24615;&#26469;&#28304;&#65292;&#20998;&#20026;&#20116;&#20010;&#39046;&#22495;&#65306;&#65288;A&#65289;&#27861;&#35268;&#35821;&#35328;&#22797;&#26434;&#24615;&#65292;&#65288;B&#65289;&#27861;&#35268;&#27969;&#31243;&#30340;&#22797;&#26434;&#24615;&#65292;&#65288;C&#65289;&#20840;&#29699;&#23618;&#38754;&#30340;&#22797;&#26434;&#24615;&#65292;&#65288;D&#65289;&#19982;&#25968;&#25454;&#24211;&#30456;&#20851;&#30340;&#32771;&#34385;&#20107;&#39033;&#65292;&#21644;&#65288;E&#65289;&#20135;&#21697;&#23618;&#38754;&#38382;&#39064;&#12290;&#21442;&#19982;&#32773;&#24378;&#35843;&#20102;&#38656;&#35201;&#21046;&#23450;&#31574;&#30053;&#26469;&#31616;&#21270;&#27861;&#35268;&#21512;&#35268;&#65292;&#25552;&#39640;&#27861;&#35268;&#26426;&#26500;&#19982;&#34892;&#19994;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#24182;&#24320;&#21457;&#36866;&#24212;&#24615;&#26694;&#26550;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study investigates the complexity of regulatory affairs in the medical device industry, a critical factor influencing market access and patient care. Through qualitative research, we sought expert insights to understand the factors contributing to this complexity. The study involved semi-structured interviews with 28 professionals from medical device companies, specializing in various aspects of regulatory affairs. These interviews were analyzed using open coding and Natural Language Processing (NLP) techniques. The findings reveal key sources of complexity within the regulatory landscape, divided into five domains: (A) Regulatory language complexity, (B) Intricacies within the regulatory process, (C) Global-level complexities, (D) Database-related considerations, and (E) Product-level issues. The participants highlighted the need for strategies to streamline regulatory compliance, enhance interactions between regulatory bodies and industry players, and develop adaptable framework
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26816;&#27979;&#22312;&#32447;&#20844;&#24320;&#23041;&#32961;&#30340;&#25928;&#21147;&#12290;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#65292;&#19981;&#21516;&#30340;LLMs&#22312;&#23041;&#32961;&#21644;&#38750;&#23041;&#32961;&#35782;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#20854;&#20013;GPT-4&#30340;&#34920;&#29616;&#26368;&#20339;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;PaLM API&#30340;&#23450;&#20215;&#38750;&#24120;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#21487;&#20197;&#26377;&#25928;&#22320;&#22686;&#24378;&#20154;&#24037;&#20869;&#23481;&#23457;&#26597;&#65292;&#24110;&#21161;&#20943;&#36731;&#26032;&#20852;&#30340;&#22312;&#32447;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2401.02974</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26816;&#27979;&#22312;&#32447;&#20844;&#24320;&#23041;&#32961;&#30340;&#25928;&#21147;
&lt;/p&gt;
&lt;p&gt;
Efficacy of Utilizing Large Language Models to Detect Public Threat Posted Online. (arXiv:2401.02974v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02974
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26816;&#27979;&#22312;&#32447;&#20844;&#24320;&#23041;&#32961;&#30340;&#25928;&#21147;&#12290;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#65292;&#19981;&#21516;&#30340;LLMs&#22312;&#23041;&#32961;&#21644;&#38750;&#23041;&#32961;&#35782;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#20854;&#20013;GPT-4&#30340;&#34920;&#29616;&#26368;&#20339;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;PaLM API&#30340;&#23450;&#20215;&#38750;&#24120;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#21487;&#20197;&#26377;&#25928;&#22320;&#22686;&#24378;&#20154;&#24037;&#20869;&#23481;&#23457;&#26597;&#65292;&#24110;&#21161;&#20943;&#36731;&#26032;&#20852;&#30340;&#22312;&#32447;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26816;&#27979;&#22312;&#32447;&#20844;&#24320;&#23041;&#32961;&#30340;&#25928;&#21147;&#12290;&#22312;&#23545;&#23041;&#32961; retoric &#30340;&#20256;&#25773;&#21644;&#26292;&#21147;&#39044;&#21578;&#30340;&#22686;&#38271;&#36234;&#26469;&#36234;&#25285;&#24551;&#30340;&#32972;&#26223;&#19979;&#65292;&#33258;&#21160;&#20869;&#23481;&#20998;&#26512;&#25216;&#26415;&#21487;&#20197;&#24110;&#21161;&#26089;&#26399;&#21457;&#29616;&#21644;&#22788;&#29702;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#33258;&#23450;&#20041;&#30340;&#25968;&#25454;&#25910;&#38598;&#24037;&#20855;&#65292;&#20174;&#19968;&#20010;&#28909;&#38376;&#30340;&#38889;&#22269;&#22312;&#32447;&#31038;&#21306;&#25910;&#38598;&#20102;500&#20010;&#38750;&#23041;&#32961;&#31034;&#20363;&#21644;20&#20010;&#23041;&#32961;&#31034;&#20363;&#30340;&#24086;&#23376;&#26631;&#39064;&#12290;&#21508;&#31181;LLMs (GPT-3.5, GPT-4, PaLM) &#34987;&#25552;&#31034;&#23558;&#21333;&#20010;&#24086;&#23376;&#20998;&#31867;&#20026;"&#23041;&#32961;"&#25110;"&#23433;&#20840;"&#12290;&#32479;&#35745;&#20998;&#26512;&#21457;&#29616;&#25152;&#26377;&#27169;&#22411;&#22312;&#23041;&#32961;&#21644;&#38750;&#23041;&#32961;&#35782;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#36890;&#36807;&#21345;&#26041;&#25311;&#21512;&#24230;&#26816;&#39564;&#20063;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;GPT-4 &#30340;&#25972;&#20307;&#34920;&#29616;&#26368;&#22909;&#65292;&#38750;&#23041;&#32961;&#31934;&#24230;&#36798;&#21040;&#20102;97.9%&#65292;&#23041;&#32961;&#31934;&#24230;&#36798;&#21040;&#20102;100%&#12290;&#21487;&#34892;&#24615;&#20998;&#26512;&#36824;&#26174;&#31034;PaLM API&#30340;&#23450;&#20215;&#38750;&#24120;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;LLMs &#22312;&#35268;&#27169;&#21270;&#29615;&#22659;&#20013;&#21487;&#20197;&#26377;&#25928;&#22320;&#22686;&#24378;&#20154;&#24037;&#20869;&#23481;&#23457;&#26597;&#65292;&#20197;&#24110;&#21161;&#20943;&#36731;&#26032;&#20852;&#30340;&#22312;&#32447;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper examines the efficacy of utilizing large language models (LLMs) to detect public threats posted online. Amid rising concerns over the spread of threatening rhetoric and advance notices of violence, automated content analysis techniques may aid in early identification and moderation. Custom data collection tools were developed to amass post titles from a popular Korean online community, comprising 500 non-threat examples and 20 threats. Various LLMs (GPT-3.5, GPT-4, PaLM) were prompted to classify individual posts as either "threat" or "safe." Statistical analysis found all models demonstrated strong accuracy, passing chi-square goodness of fit tests for both threat and non-threat identification. GPT-4 performed best overall with 97.9% non-threat and 100% threat accuracy. Affordability analysis also showed PaLM API pricing as highly cost-efficient. The findings indicate LLMs can effectively augment human content moderation at scale to help mitigate emerging online risks. Howe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;REE-HDSC&#39033;&#30446;&#65292;&#26088;&#22312;&#25913;&#36827;&#25163;&#20889;&#25991;&#26412;&#35782;&#21035;&#36719;&#20214;&#33258;&#21160;&#25552;&#21462;&#30340;&#21629;&#21517;&#23454;&#20307;&#30340;&#36136;&#37327;&#12290;&#36890;&#36807;&#20845;&#27493;&#22788;&#29702;&#27969;&#31243;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#35813;&#27969;&#31243;&#22312;&#22788;&#29702;&#24211;&#25289;&#32034;&#27665;&#20107;&#30331;&#35760;&#22788;&#30340;19&#19990;&#32426;&#21644;20&#19990;&#32426;&#27515;&#20129;&#35777;&#20070;&#26102;&#65292;&#26085;&#26399;&#25552;&#21462;&#20855;&#26377;&#39640;&#31934;&#24230;&#65292;&#20154;&#21517;&#25552;&#21462;&#30340;&#31934;&#24230;&#36739;&#20302;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#37325;&#26032;&#35757;&#32451;HTR&#27169;&#22411;&#12289;&#21518;&#22788;&#29702;&#21644;&#35782;&#21035;&#21024;&#38500;&#19981;&#27491;&#30830;&#30340;&#21517;&#23383;&#26469;&#25552;&#39640;&#20154;&#21517;&#25552;&#21462;&#31934;&#24230;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.02972</link><description>&lt;p&gt;
REE-HDSC: &#35782;&#21035;&#21382;&#21490;&#25968;&#25454;&#24211;Suriname Curacao&#20013;&#25552;&#21462;&#30340;&#23454;&#20307;
&lt;/p&gt;
&lt;p&gt;
REE-HDSC: Recognizing Extracted Entities for the Historical Database Suriname Curacao. (arXiv:2401.02972v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02972
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;REE-HDSC&#39033;&#30446;&#65292;&#26088;&#22312;&#25913;&#36827;&#25163;&#20889;&#25991;&#26412;&#35782;&#21035;&#36719;&#20214;&#33258;&#21160;&#25552;&#21462;&#30340;&#21629;&#21517;&#23454;&#20307;&#30340;&#36136;&#37327;&#12290;&#36890;&#36807;&#20845;&#27493;&#22788;&#29702;&#27969;&#31243;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#35813;&#27969;&#31243;&#22312;&#22788;&#29702;&#24211;&#25289;&#32034;&#27665;&#20107;&#30331;&#35760;&#22788;&#30340;19&#19990;&#32426;&#21644;20&#19990;&#32426;&#27515;&#20129;&#35777;&#20070;&#26102;&#65292;&#26085;&#26399;&#25552;&#21462;&#20855;&#26377;&#39640;&#31934;&#24230;&#65292;&#20154;&#21517;&#25552;&#21462;&#30340;&#31934;&#24230;&#36739;&#20302;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#37325;&#26032;&#35757;&#32451;HTR&#27169;&#22411;&#12289;&#21518;&#22788;&#29702;&#21644;&#35782;&#21035;&#21024;&#38500;&#19981;&#27491;&#30830;&#30340;&#21517;&#23383;&#26469;&#25552;&#39640;&#20154;&#21517;&#25552;&#21462;&#31934;&#24230;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;REE-HDSC&#39033;&#30446;&#65292;&#24182;&#27010;&#36848;&#20102;&#25105;&#20204;&#21162;&#21147;&#25552;&#39640;&#25163;&#20889;&#25991;&#26412;&#35782;&#21035;&#65288;HTR&#65289;&#36719;&#20214;&#29983;&#25104;&#30340;&#25991;&#26412;&#33258;&#21160;&#25552;&#21462;&#30340;&#21629;&#21517;&#23454;&#20307;&#36136;&#37327;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#20845;&#27493;&#22788;&#29702;&#27969;&#31243;&#65292;&#24182;&#36890;&#36807;&#22788;&#29702;&#24211;&#25289;&#32034;&#27665;&#20107;&#30331;&#35760;&#22788;&#30340;19&#19990;&#32426;&#21644;20&#19990;&#32426;&#27515;&#20129;&#35777;&#20070;&#36827;&#34892;&#27979;&#35797;&#12290;&#25105;&#20204;&#21457;&#29616;&#35813;&#27969;&#27700;&#32447;&#25552;&#21462;&#30340;&#26085;&#26399;&#20855;&#26377;&#39640;&#31934;&#24230;&#65292;&#20294;&#20154;&#21517;&#25552;&#21462;&#30340;&#31934;&#24230;&#36739;&#20302;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#37325;&#26032;&#35757;&#32451;&#21547;&#26377;&#21517;&#23383;&#30340;HTR&#27169;&#22411;&#12289;&#21518;&#22788;&#29702;&#20197;&#21450;&#35782;&#21035;&#21644;&#21024;&#38500;&#19981;&#27491;&#30830;&#30340;&#21517;&#23383;&#26469;&#25913;&#21892;&#21517;&#23383;&#25552;&#21462;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We describe the project REE-HDSC and outline our efforts to improve the quality of named entities extracted automatically from texts generated by hand-written text recognition (HTR) software. We describe a six-step processing pipeline and test it by processing 19th and 20th century death certificates from the civil registry of Curacao. We find that the pipeline extracts dates with high precision but that the precision of person name extraction is low. Next we show how name precision extraction can be improved by retraining HTR models with names, post-processing and by identifying and removing incorrect names.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#22522;&#20110;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25991;&#26412;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#22788;&#29702;&#20219;&#21153;&#26469;&#25552;&#39640;&#26368;&#26032;&#25216;&#26415;&#65292;&#24182;&#22312;&#21322;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#23545;&#20004;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.02971</link><description>&lt;p&gt;
&#25991;&#26412;&#20013;&#30340;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Deep Anomaly Detection in Text. (arXiv:2401.02971v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#22522;&#20110;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25991;&#26412;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#22788;&#29702;&#20219;&#21153;&#26469;&#25552;&#39640;&#26368;&#26032;&#25216;&#26415;&#65292;&#24182;&#22312;&#21322;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#23545;&#20004;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#35832;&#22914;&#22534;&#21472;&#33258;&#21160;&#32534;&#30721;&#22120;&#12289;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#31561;&#26041;&#27861;&#22823;&#22823;&#25913;&#36827;&#20102;&#26368;&#26032;&#25216;&#26415;&#12290;&#20854;&#20182;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#36866;&#24403;&#30340;&#26680;&#20989;&#25968;&#26469;&#22686;&#24378;&#32463;&#20856;&#27169;&#22411;&#65288;&#22914;&#21333;&#31867;&#25903;&#25345;&#21521;&#37327;&#26426;&#65289;&#12290;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#22312;&#24322;&#24120;&#26816;&#27979;&#39046;&#22495;&#20013;&#30340;&#20195;&#34920;&#32447;&#23398;&#20064;&#26041;&#38754;&#30340;&#26368;&#26032;&#21457;&#23637;&#35777;&#26126;&#22312;&#36825;&#19968;&#32972;&#26223;&#19979;&#38750;&#24120;&#26377;&#30410;&#12290;&#21463;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#30340;&#36827;&#23637;&#21551;&#21457;&#65292;&#26412;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#20026;&#25991;&#26412;&#35821;&#26009;&#24211;&#37327;&#36523;&#23450;&#21046;&#30340;&#39044;&#22788;&#29702;&#20219;&#21153;&#26469;&#24320;&#21457;&#19968;&#31181;&#26816;&#27979;&#24322;&#24120;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;20Newsgroups&#21644;AG News&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#22823;&#22823;&#25913;&#36827;&#20102;&#21322;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#30340;&#24322;&#24120;&#26816;&#27979;&#26368;&#26032;&#25216;&#26415;&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;&#33258;&#25105;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#22120;&#22312;&#33258;&#28982;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep anomaly detection methods have become increasingly popular in recent years, with methods like Stacked Autoencoders, Variational Autoencoders, and Generative Adversarial Networks greatly improving the state-of-the-art. Other methods rely on augmenting classical models (such as the One-Class Support Vector Machine), by learning an appropriate kernel function using Neural Networks. Recent developments in representation learning by self-supervision are proving to be very beneficial in the context of anomaly detection. Inspired by the advancements in anomaly detection using self-supervised learning in the field of computer vision, this thesis aims to develop a method for detecting anomalies by exploiting pretext tasks tailored for text corpora. This approach greatly improves the state-of-the-art on two datasets, 20Newsgroups, and AG News, for both semi-supervised and unsupervised anomaly detection, thus proving the potential for self-supervised anomaly detectors in the field of natural
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23558;&#19978;&#19979;&#25991;&#21644;&#23383;&#38754;&#20449;&#24687;&#23481;&#32435;&#21040;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#23884;&#20837;&#20013;&#65292;&#21033;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#24182;&#36890;&#36807;&#35268;&#21017;&#21644;&#23383;&#38754;&#20449;&#24687;&#30340;&#34920;&#31034;&#35745;&#31639;&#32622;&#20449;&#24230;&#21644;&#30456;&#20851;&#24615;&#25351;&#26631;&#65292;&#20197;&#25552;&#39640;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.02968</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#30340;&#35268;&#21017;&#24341;&#23548;&#32852;&#21512;&#23884;&#20837;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Rule-Guided Joint Embedding Learning of Knowledge Graphs. (arXiv:2401.02968v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02968
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23558;&#19978;&#19979;&#25991;&#21644;&#23383;&#38754;&#20449;&#24687;&#23481;&#32435;&#21040;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#23884;&#20837;&#20013;&#65292;&#21033;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#24182;&#36890;&#36807;&#35268;&#21017;&#21644;&#23383;&#38754;&#20449;&#24687;&#30340;&#34920;&#31034;&#35745;&#31639;&#32622;&#20449;&#24230;&#21644;&#30456;&#20851;&#24615;&#25351;&#26631;&#65292;&#20197;&#25552;&#39640;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#65292;&#20851;&#27880;&#28857;&#20027;&#35201;&#38598;&#20013;&#22312;&#22686;&#24378;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#23398;&#20064;&#19978;&#65292;&#35813;&#23398;&#20064;&#23558;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#32534;&#30721;&#20026;&#20302;&#32500;&#21521;&#37327;&#31354;&#38388;&#12290;&#23613;&#31649;&#24403;&#21069;&#27169;&#22411;&#20027;&#35201;&#32771;&#34385;&#36825;&#20123;&#22270;&#35889;&#30340;&#32467;&#26500;&#26041;&#38754;&#65292;&#20294;&#22312;&#30693;&#35782;&#22270;&#35889;&#20013;&#23384;&#22312;&#30528;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#21644;&#23383;&#38754;&#20449;&#24687;&#65292;&#21487;&#20197;&#29992;&#20110;&#26356;&#26377;&#25928;&#30340;&#23884;&#20837;&#23398;&#20064;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23558;&#19978;&#19979;&#25991;&#21644;&#23383;&#38754;&#20449;&#24687;&#23481;&#32435;&#21040;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#23884;&#20837;&#20013;&#65292;&#21033;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#12290;&#20855;&#20307;&#22320;&#65292;&#23545;&#20110;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#25105;&#20204;&#36890;&#36807;&#32622;&#20449;&#24230;&#21644;&#30456;&#20851;&#24615;&#25351;&#26631;&#35780;&#20272;&#20854;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#26469;&#35745;&#31639;&#32622;&#20449;&#24230;&#25351;&#26631;&#65292;&#24182;&#20174;&#23383;&#38754;&#20449;&#24687;&#30340;&#34920;&#31034;&#20013;&#24471;&#20986;&#30456;&#20851;&#24615;&#25351;&#26631;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#20004;&#20010;&#24050;&#24314;&#31435;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#35814;&#23613;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent studies, the focus has been on enhancing knowledge graph embedding learning, which encodes entities and relations in knowledge graphs into low-dimensional vector spaces. While current models mainly consider the structural aspects of these graphs, there's a wealth of contextual and literal information in knowledge graphs that can be utilized for more effective embeddings. This paper introduces a novel model that incorporates both contextual and literal information into entity and relation embeddings, utilizing graph convolutional networks. Specifically, for contextual information, we assess its significance through confidence and relatedness metrics. A unique rule-based method is developed to calculate the confidence metric, and the relatedness metric is derived from the literal information's representations. We validated our model's performance with thorough experiments on two established benchmark datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20221;&#32508;&#21512;&#27010;&#36848;&#65292;&#20171;&#32461;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20174;&#35757;&#32451;&#21040;&#25512;&#29702;&#30340;&#28436;&#21464;&#36807;&#31243;&#65292;&#24182;&#25506;&#35752;&#20102;&#36825;&#19968;&#26032;&#20852;&#36235;&#21183;&#20013;&#19982;&#25104;&#26412;&#25928;&#29575;&#30456;&#20851;&#30340;&#35757;&#32451;&#21644;&#37096;&#32626;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#36824;&#35752;&#35770;&#20102;&#25512;&#29702;&#38454;&#27573;&#30340;&#27169;&#22411;&#21387;&#32553;&#12289;&#24182;&#34892;&#35745;&#31639;&#12289;&#20869;&#23384;&#35843;&#24230;&#21644;&#32467;&#26500;&#20248;&#21270;&#31561;&#20851;&#38190;&#20027;&#39064;&#65292;&#20026;LLMs&#30340;&#21033;&#29992;&#21644;&#26410;&#26469;&#21457;&#23637;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2401.02038</link><description>&lt;p&gt;
&#29702;&#35299;LLMs&#65306;&#20174;&#35757;&#32451;&#21040;&#25512;&#29702;&#30340;&#20840;&#38754;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
Understanding LLMs: A Comprehensive Overview from Training to Inference. (arXiv:2401.02038v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20221;&#32508;&#21512;&#27010;&#36848;&#65292;&#20171;&#32461;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20174;&#35757;&#32451;&#21040;&#25512;&#29702;&#30340;&#28436;&#21464;&#36807;&#31243;&#65292;&#24182;&#25506;&#35752;&#20102;&#36825;&#19968;&#26032;&#20852;&#36235;&#21183;&#20013;&#19982;&#25104;&#26412;&#25928;&#29575;&#30456;&#20851;&#30340;&#35757;&#32451;&#21644;&#37096;&#32626;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#36824;&#35752;&#35770;&#20102;&#25512;&#29702;&#38454;&#27573;&#30340;&#27169;&#22411;&#21387;&#32553;&#12289;&#24182;&#34892;&#35745;&#31639;&#12289;&#20869;&#23384;&#35843;&#24230;&#21644;&#32467;&#26500;&#20248;&#21270;&#31561;&#20851;&#38190;&#20027;&#39064;&#65292;&#20026;LLMs&#30340;&#21033;&#29992;&#21644;&#26410;&#26469;&#21457;&#23637;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#30340;&#24341;&#20837;&#23548;&#33268;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35299;&#20915;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#22823;&#37327;&#20351;&#29992;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#23545;&#20110;&#25104;&#26412;&#25928;&#29575;&#30340;&#20851;&#27880;&#36234;&#26469;&#36234;&#22810;&#12290;&#20302;&#25104;&#26412;&#30340;LLMs&#35757;&#32451;&#21644;&#37096;&#32626;&#20195;&#34920;&#20102;&#26410;&#26469;&#30340;&#21457;&#23637;&#36235;&#21183;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#19982;&#36825;&#19968;&#26032;&#20852;&#36235;&#21183;&#30456;&#19968;&#33268;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#25216;&#26415;&#21644;&#25512;&#29702;&#37096;&#32626;&#25216;&#26415;&#30340;&#28436;&#21464;&#12290;&#35757;&#32451;&#30340;&#35752;&#35770;&#21253;&#25324;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#35757;&#32451;&#26550;&#26500;&#12289;&#39044;&#35757;&#32451;&#20219;&#21153;&#12289;&#24182;&#34892;&#35757;&#32451;&#20197;&#21450;&#19982;&#27169;&#22411;&#24494;&#35843;&#30456;&#20851;&#30340;&#20869;&#23481;&#12290;&#22312;&#25512;&#29702;&#26041;&#38754;&#65292;&#26412;&#25991;&#28085;&#30422;&#20102;&#27169;&#22411;&#21387;&#32553;&#12289;&#24182;&#34892;&#35745;&#31639;&#12289;&#20869;&#23384;&#35843;&#24230;&#21644;&#32467;&#26500;&#20248;&#21270;&#31561;&#20027;&#39064;&#12290;&#23427;&#36824;&#25506;&#35752;&#20102;LLMs&#30340;&#21033;&#29992;&#24182;&#25552;&#20379;&#20102;&#23545;&#20854;&#26410;&#26469;&#21457;&#23637;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The introduction of ChatGPT has led to a significant increase in the utilization of Large Language Models (LLMs) for addressing downstream tasks. There's an increasing focus on cost-efficient training and deployment within this context. Low-cost training and deployment of LLMs represent the future development trend. This paper reviews the evolution of large language model training techniques and inference deployment technologies aligned with this emerging trend. The discussion on training includes various aspects, including data preprocessing, training architecture, pre-training tasks, parallel training, and relevant content related to model fine-tuning. On the inference side, the paper covers topics such as model compression, parallel computation, memory scheduling, and structural optimization. It also explores LLMs' utilization and provides insights into their future development.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20020;&#24202;&#35821;&#20041;&#25628;&#32034;&#26041;&#38754;&#65292;&#36890;&#29992;&#23884;&#20837;&#27169;&#22411;&#27604;&#19987;&#19994;&#23884;&#20837;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#65292;&#36825;&#34920;&#26126;&#29616;&#26377;&#30340;&#20020;&#24202;&#19987;&#19994;&#21270;&#27169;&#22411;&#23545;&#36755;&#20837;&#30340;&#24494;&#23567;&#21464;&#21270;&#26356;&#25935;&#24863;&#12290;</title><link>http://arxiv.org/abs/2401.01943</link><description>&lt;p&gt;
&#36890;&#29992;&#23884;&#20837;&#27169;&#22411;&#22312;&#30701;&#35821;&#22659;&#20020;&#24202;&#35821;&#20041;&#25628;&#32034;&#26041;&#38754;&#34920;&#29616;&#27604;&#19987;&#19994;&#23884;&#20837;&#27169;&#22411;&#26356;&#22909;
&lt;/p&gt;
&lt;p&gt;
Generalist embedding models are better at short-context clinical semantic search than specialized embedding models. (arXiv:2401.01943v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01943
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20020;&#24202;&#35821;&#20041;&#25628;&#32034;&#26041;&#38754;&#65292;&#36890;&#29992;&#23884;&#20837;&#27169;&#22411;&#27604;&#19987;&#19994;&#23884;&#20837;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#65292;&#36825;&#34920;&#26126;&#29616;&#26377;&#30340;&#20020;&#24202;&#19987;&#19994;&#21270;&#27169;&#22411;&#23545;&#36755;&#20837;&#30340;&#24494;&#23567;&#21464;&#21270;&#26356;&#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24037;&#20855;&#21644;&#35299;&#20915;&#26041;&#26696;&#22312;&#21307;&#30103;&#39046;&#22495;&#30340;&#24212;&#29992;&#26085;&#30410;&#22686;&#22810;&#65292;&#36825;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#36235;&#21183;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20010;&#39640;&#24230;&#20851;&#38190;&#21644;&#25935;&#24863;&#30340;&#39046;&#22495;&#20013;&#20351;&#29992;&#23427;&#20204;&#23545;&#20854;&#31283;&#20581;&#24615;&#20135;&#29983;&#20102;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#23545;&#36755;&#20837;&#21464;&#21270;&#21644;&#29983;&#25104;&#30340;&#36755;&#20986;&#30340;&#21487;&#38752;&#24615;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;ICD-10-CM&#20195;&#30721;&#25551;&#36848;&#30340;&#25991;&#26412;&#25968;&#25454;&#38598;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#35813;&#25968;&#25454;&#38598;&#24191;&#27867;&#24212;&#29992;&#20110;&#32654;&#22269;&#21307;&#38498;&#65292;&#21253;&#21547;&#35768;&#22810;&#20020;&#24202;&#26415;&#35821;&#21450;&#20854;&#26131;&#20110;&#22797;&#21046;&#30340;&#25913;&#20889;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#35821;&#20041;&#25628;&#32034;&#20219;&#21153;&#20013;&#23545;&#29616;&#26377;&#30340;&#36890;&#29992;&#25110;&#20020;&#24202;&#19987;&#19994;&#21270;&#30340;&#23884;&#20837;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#30446;&#26631;&#26159;&#27491;&#30830;&#21305;&#37197;&#25913;&#20889;&#30340;&#25991;&#26412;&#19982;&#21407;&#22987;&#25551;&#36848;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#29992;&#27169;&#22411;&#27604;&#20020;&#24202;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#65292;&#36825;&#34920;&#26126;&#29616;&#26377;&#30340;&#20020;&#24202;&#19987;&#19994;&#21270;&#27169;&#22411;&#23545;&#36755;&#20837;&#30340;&#24494;&#23567;&#21464;&#21270;&#26356;&#25935;&#24863;&#65292;&#20174;&#32780;&#20351;&#20854;&#22256;&#24785;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing use of tools and solutions based on Large Language Models (LLMs) for various tasks in the medical domain has become a prominent trend. Their use in this highly critical and sensitive domain has thus raised important questions about their robustness, especially in response to variations in input, and the reliability of the generated outputs. This study addresses these questions by constructing a textual dataset based on the ICD-10-CM code descriptions, widely used in US hospitals and containing many clinical terms, and their easily reproducible rephrasing. We then benchmarked existing embedding models, either generalist or specialized in the clinical domain, in a semantic search task where the goal was to correctly match the rephrased text to the original description. Our results showed that generalist models performed better than clinical models, suggesting that existing clinical specialized models are more sensitive to small changes in input that confuse them. The highl
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22522;&#20110;&#36855;&#22240;&#30340;&#31038;&#20132;&#34384;&#24453;&#30740;&#31350;&#23545;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#23433;&#20840;&#27934;&#23519;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32508;&#21512;&#30340;&#36855;&#22240;&#22522;&#20934;&#27979;&#35797;&#38598;GOAT-Bench&#65292;&#35780;&#20272;&#21508;&#31181;LMMs&#22312;&#35782;&#21035;&#21644;&#22238;&#24212;&#36855;&#22240;&#20013;&#20307;&#29616;&#30340;&#24494;&#22937;&#31038;&#20132;&#34384;&#24453;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.01523</link><description>&lt;p&gt;
GOAT-Bench: &#36890;&#36807;&#22522;&#20110;&#36855;&#22240;&#30340;&#31038;&#20132;&#34384;&#24453;&#30740;&#31350;&#23545;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#23433;&#20840;&#27934;&#23519;
&lt;/p&gt;
&lt;p&gt;
GOAT-Bench: Safety Insights to Large Multimodal Models through Meme-Based Social Abuse. (arXiv:2401.01523v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01523
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#36855;&#22240;&#30340;&#31038;&#20132;&#34384;&#24453;&#30740;&#31350;&#23545;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#23433;&#20840;&#27934;&#23519;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32508;&#21512;&#30340;&#36855;&#22240;&#22522;&#20934;&#27979;&#35797;&#38598;GOAT-Bench&#65292;&#35780;&#20272;&#21508;&#31181;LMMs&#22312;&#35782;&#21035;&#21644;&#22238;&#24212;&#36855;&#22240;&#20013;&#20307;&#29616;&#30340;&#24494;&#22937;&#31038;&#20132;&#34384;&#24453;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#30340;&#25351;&#25968;&#32423;&#22686;&#38271;&#28145;&#21051;&#25913;&#21464;&#20102;&#20449;&#24687;&#30340;&#21019;&#36896;&#12289;&#20256;&#25773;&#21644;&#21560;&#25910;&#26041;&#24335;&#65292;&#22312;&#25968;&#23383;&#26102;&#20195;&#20135;&#29983;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#24433;&#21709;&#12290;&#36951;&#25022;&#30340;&#26159;&#65292;&#36825;&#20010;&#29190;&#28856;&#20063;&#23548;&#33268;&#20102;&#32593;&#32476;&#36855;&#22240;&#30340;&#28389;&#29992;&#25968;&#37327;&#26174;&#33879;&#22686;&#21152;&#12290;&#35780;&#20272;&#36855;&#22240;&#30340;&#36127;&#38754;&#24433;&#21709;&#26159;&#30456;&#24403;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#20855;&#26377;&#24494;&#22937;&#21644;&#38544;&#26214;&#30340;&#21547;&#20041;&#65292;&#36825;&#20123;&#21547;&#20041;&#19981;&#33021;&#30452;&#25509;&#36890;&#36807;&#26174;&#24615;&#30340;&#25991;&#26412;&#21644;&#22270;&#20687;&#20256;&#36798;&#20986;&#26469;&#12290;&#37492;&#20110;&#27492;&#65292;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;(LMMs)&#20316;&#20026;&#22788;&#29702;&#22810;&#26679;&#21270;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#21331;&#36234;&#33021;&#21147;&#30340;&#28966;&#28857;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20852;&#36259;&#12290;&#38024;&#23545;&#36825;&#19968;&#21457;&#23637;&#65292;&#25105;&#20204;&#30340;&#35770;&#25991;&#26088;&#22312;&#28145;&#20837;&#30740;&#31350;&#21508;&#31181;LMMs(&#22914;GPT-4V)&#35782;&#21035;&#21644;&#22238;&#24212;&#36855;&#22240;&#20013;&#20307;&#29616;&#30340;&#24494;&#22937;&#31038;&#20132;&#34384;&#24453;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#32508;&#21512;&#30340;&#36855;&#22240;&#22522;&#20934;&#27979;&#35797;&#38598;GOAT-Bench&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;6K&#20010;&#22810;&#26679;&#30340;&#36855;&#22240;&#65292;&#28085;&#30422;&#30340;&#20027;&#39064;&#21253;&#25324;&#38544;&#24615;&#20167;&#24680;&#35328;&#35770;&#12289;&#24615;&#21035;&#27495;&#35270;&#21644;&#32593;&#32476;&#27450;&#20940;&#31561;&#12290;&#21033;&#29992;GOAT-Be
&lt;/p&gt;
&lt;p&gt;
The exponential growth of social media has profoundly transformed how information is created, disseminated, and absorbed, exceeding any precedent in the digital age. Regrettably, this explosion has also spawned a significant increase in the online abuse of memes. Evaluating the negative impact of memes is notably challenging, owing to their often subtle and implicit meanings, which are not directly conveyed through the overt text and imagery. In light of this, large multimodal models (LMMs) have emerged as a focal point of interest due to their remarkable capabilities in handling diverse multimodal tasks. In response to this development, our paper aims to thoroughly examine the capacity of various LMMs (e.g. GPT-4V) to discern and respond to the nuanced aspects of social abuse manifested in memes. We introduce the comprehensive meme benchmark, GOAT-Bench, comprising over 6K varied memes encapsulating themes such as implicit hate speech, sexism, and cyberbullying, etc. Utilizing GOAT-Be
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#25991;&#26412;&#26102;&#23481;&#26131;&#20986;&#29616;&#24187;&#35273;&#65292;&#36825;&#26159;&#23433;&#20840;&#37096;&#32626;&#36825;&#20123;&#27169;&#22411;&#30340;&#26368;&#22823;&#38556;&#30861;&#12290;&#35299;&#20915;&#24187;&#35273;&#38382;&#39064;&#23545;&#20110;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#24191;&#27867;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2401.01313</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#32531;&#35299;&#25216;&#26415;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models. (arXiv:2401.01313v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01313
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#25991;&#26412;&#26102;&#23481;&#26131;&#20986;&#29616;&#24187;&#35273;&#65292;&#36825;&#26159;&#23433;&#20840;&#37096;&#32626;&#36825;&#20123;&#27169;&#22411;&#30340;&#26368;&#22823;&#38556;&#30861;&#12290;&#35299;&#20915;&#24187;&#35273;&#38382;&#39064;&#23545;&#20110;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#24191;&#27867;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#25104;&#20154;&#31867;&#21270;&#25991;&#26412;&#26041;&#38754;&#30340;&#33021;&#21147;&#19981;&#26029;&#25552;&#39640;&#65292;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#23427;&#20204;&#20542;&#21521;&#20110;&#20135;&#29983;&#34394;&#26500;&#30340;&#20869;&#23481;&#65292;&#30475;&#20284;&#30495;&#23454;&#20294;&#27809;&#26377;&#20381;&#25454;&#12290;&#24187;&#35273;&#38382;&#39064;&#21487;&#20197;&#35828;&#26159;&#23433;&#20840;&#22320;&#23558;&#36825;&#20123;&#24378;&#22823;&#30340;LLMs&#37096;&#32626;&#21040;&#24433;&#21709;&#20154;&#20204;&#29983;&#27963;&#30340;&#29616;&#23454;&#29983;&#20135;&#31995;&#32479;&#20013;&#26368;&#22823;&#30340;&#38556;&#30861;&#12290;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#24191;&#27867;&#37319;&#29992;LLMs&#30340;&#36807;&#31243;&#20005;&#37325;&#20381;&#36182;&#20110;&#35299;&#20915;&#21644;&#20943;&#36731;&#24187;&#35273;&#12290;&#19982;&#20256;&#32479;&#30340;&#19987;&#27880;&#20110;&#26377;&#38480;&#20219;&#21153;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#19981;&#21516;&#65292;LLMs&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#20197;&#25509;&#35302;&#21040;&#22823;&#37327;&#30340;&#22312;&#32447;&#25991;&#26412;&#25968;&#25454;&#12290;&#36825;&#20351;&#23427;&#20204;&#33021;&#22815;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#35821;&#35328;&#27969;&#21033;&#24615;&#65292;&#20294;&#20063;&#24847;&#21619;&#30528;&#23427;&#20204;&#33021;&#22815;&#20174;&#35757;&#32451;&#25968;&#25454;&#30340;&#20559;&#35265;&#20013;&#25512;&#26029;&#20449;&#24687;&#65292;&#38169;&#35823;&#35299;&#37322;&#21547;&#31946;&#19981;&#28165;&#30340;&#25552;&#31034;&#65292;&#25110;&#32773;&#20462;&#25913;&#20449;&#24687;&#20197;&#34920;&#38754;&#19978;&#19982;&#36755;&#20837;&#19968;&#33268;&#12290;&#24403;&#25105;&#20204;&#20381;&#36182;&#35821;&#35328;&#29983;&#25104;&#33021;&#21147;&#26469;&#23436;&#25104;&#25935;&#24863;&#24212;&#29992;&#26102;&#65292;&#36825;&#21464;&#24471;&#38750;&#24120;&#20196;&#20154;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Large Language Models (LLMs) continue to advance in their ability to write human-like text, a key challenge remains around their tendency to hallucinate generating content that appears factual but is ungrounded. This issue of hallucination is arguably the biggest hindrance to safely deploying these powerful LLMs into real-world production systems that impact people's lives. The journey toward widespread adoption of LLMs in practical settings heavily relies on addressing and mitigating hallucinations. Unlike traditional AI systems focused on limited tasks, LLMs have been exposed to vast amounts of online text data during training. While this allows them to display impressive language fluency, it also means they are capable of extrapolating information from the biases in training data, misinterpreting ambiguous prompts, or modifying the information to align superficially with the input. This becomes hugely alarming when we rely on language generation capabilities for sensitive applica
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20840;&#38754;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#65292;&#26088;&#22312;&#26377;&#25928;&#20462;&#25913;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.01286</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#20840;&#38754;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Study of Knowledge Editing for Large Language Models. (arXiv:2401.01286v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20840;&#38754;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#65292;&#26088;&#22312;&#26377;&#25928;&#20462;&#25913;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#19982;&#20154;&#31867;&#20132;&#27969;&#32039;&#23494;&#30456;&#20284;&#30340;&#25991;&#26412;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20854;&#20027;&#35201;&#38480;&#21046;&#22312;&#20110;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#26174;&#33879;&#35745;&#31639;&#38656;&#27714;&#65292;&#36825;&#26159;&#30001;&#20110;&#20854;&#24191;&#27867;&#30340;&#21442;&#25968;&#21270;&#36896;&#25104;&#30340;&#12290;&#36825;&#19968;&#25361;&#25112;&#22312;&#20110;&#19990;&#30028;&#30340;&#21160;&#24577;&#24615;&#65292;&#38656;&#35201;&#39057;&#32321;&#26356;&#26032;LLM&#20197;&#20462;&#27491;&#36807;&#26102;&#30340;&#20449;&#24687;&#25110;&#38598;&#25104;&#26032;&#30693;&#35782;&#65292;&#20174;&#32780;&#30830;&#20445;&#20854;&#25345;&#32493;&#30340;&#30456;&#20851;&#24615;&#12290;&#35768;&#22810;&#24212;&#29992;&#38656;&#35201;&#22312;&#35757;&#32451;&#21518;&#36827;&#34892;&#25345;&#32493;&#30340;&#27169;&#22411;&#35843;&#25972;&#65292;&#20197;&#35299;&#20915;&#32570;&#38519;&#25110;&#19981;&#33391;&#34892;&#20026;&#12290;&#36817;&#24180;&#26469;&#65292;&#23545;&#20110;LLM&#30340;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#30340;&#20852;&#36259;&#36234;&#26469;&#36234;&#39640;&#65292;&#22312;&#29305;&#23450;&#39046;&#22495;&#20869;&#26377;&#25928;&#22320;&#20462;&#25913;LLM&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#22312;&#21508;&#31181;&#36755;&#20837;&#20013;&#30340;&#34920;&#29616;&#12290;&#26412;&#25991;&#39318;&#20808;&#23450;&#20041;&#20102;&#30693;&#35782;&#32534;&#36753;&#30340;&#30446;&#26631;&#21644;&#25361;&#25112;&#65292;&#28982;&#21518;&#32508;&#36848;&#20102;&#29616;&#26377;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#21644;&#25216;&#26415;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#24212;&#29992;&#21644;&#26410;&#26469;&#21457;&#23637;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown extraordinary capabilities in understanding and generating text that closely mirrors human communication. However, a primary limitation lies in the significant computational demands during training, arising from their extensive parameterization. This challenge is further intensified by the dynamic nature of the world, necessitating frequent updates to LLMs to correct outdated information or integrate new knowledge, thereby ensuring their continued relevance. Note that many applications demand continual model adjustments post-training to address deficiencies or undesirable behaviors. There is an increasing interest in efficient, lightweight methods for on-the-fly model modifications. To this end, recent years have seen a burgeoning in the techniques of knowledge editing for LLMs, which aim to efficiently modify LLMs' behaviors within specific domains while preserving overall performance across various inputs. In this paper, we first define the kno
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;&#30340;&#36739;&#39640;&#36136;&#37327;&#21442;&#32771;&#25991;&#29486;&#23545;&#20110;&#35780;&#20272;&#25351;&#26631;&#19982;&#20154;&#31867;&#35780;&#20215;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26356;&#22909;&#12290;&#27599;&#20010;&#27573;&#33853;&#24179;&#22343;&#20351;&#29992;7&#20010;&#21442;&#32771;&#25991;&#29486;&#26377;&#21161;&#20110;&#25552;&#21319;&#25152;&#26377;&#35780;&#20272;&#25351;&#26631;&#12290;&#19981;&#21516;&#36136;&#37327;&#30340;&#20379;&#24212;&#21830;&#21442;&#32771;&#25991;&#29486;&#21487;&#20197;&#28151;&#21512;&#20351;&#29992;&#26469;&#25552;&#39640;&#35780;&#20272;&#25351;&#26631;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#21487;&#29992;&#20110;&#22312;&#29305;&#23450;&#39044;&#31639;&#19979;&#21019;&#24314;&#21442;&#32771;&#25991;&#29486;&#30340;&#20849;&#20139;&#20219;&#21153;&#30340;&#35780;&#20272;&#32773;&#12290;</title><link>http://arxiv.org/abs/2401.01283</link><description>&lt;p&gt;
&#26426;&#22120;&#32763;&#35793;&#33258;&#21160;&#35780;&#20272;&#30340;&#21442;&#32771;&#25991;&#29486;&#36136;&#37327;&#21644;&#25968;&#37327;
&lt;/p&gt;
&lt;p&gt;
Quality and Quantity of Machine Translation References for Automated Metrics. (arXiv:2401.01283v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01283
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;&#30340;&#36739;&#39640;&#36136;&#37327;&#21442;&#32771;&#25991;&#29486;&#23545;&#20110;&#35780;&#20272;&#25351;&#26631;&#19982;&#20154;&#31867;&#35780;&#20215;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26356;&#22909;&#12290;&#27599;&#20010;&#27573;&#33853;&#24179;&#22343;&#20351;&#29992;7&#20010;&#21442;&#32771;&#25991;&#29486;&#26377;&#21161;&#20110;&#25552;&#21319;&#25152;&#26377;&#35780;&#20272;&#25351;&#26631;&#12290;&#19981;&#21516;&#36136;&#37327;&#30340;&#20379;&#24212;&#21830;&#21442;&#32771;&#25991;&#29486;&#21487;&#20197;&#28151;&#21512;&#20351;&#29992;&#26469;&#25552;&#39640;&#35780;&#20272;&#25351;&#26631;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#21487;&#29992;&#20110;&#22312;&#29305;&#23450;&#39044;&#31639;&#19979;&#21019;&#24314;&#21442;&#32771;&#25991;&#29486;&#30340;&#20849;&#20139;&#20219;&#21153;&#30340;&#35780;&#20272;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;&#25351;&#26631;&#36890;&#24120;&#20351;&#29992;&#20154;&#24037;&#32763;&#35793;&#26469;&#30830;&#23450;&#31995;&#32479;&#32763;&#35793;&#30340;&#36136;&#37327;&#12290;&#39046;&#22495;&#20869;&#30340;&#20849;&#35782;&#35748;&#20026;&#20154;&#24037;&#21442;&#32771;&#25991;&#29486;&#24212;&#20855;&#26377;&#24456;&#39640;&#30340;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27809;&#26377;&#25104;&#26412;&#25928;&#30410;&#20998;&#26512;&#21487;&#20197;&#25351;&#23548;&#35745;&#21010;&#25910;&#38598;&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;&#21442;&#32771;&#25991;&#29486;&#30340;&#20174;&#19994;&#32773;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36739;&#39640;&#36136;&#37327;&#30340;&#21442;&#32771;&#25991;&#29486;&#33021;&#22815;&#22312;&#27573;&#33853;&#32423;&#21035;&#19978;&#19982;&#20154;&#31867;&#35780;&#20215;&#30340;&#30456;&#20851;&#24615;&#26356;&#22909;&#12290;&#27599;&#20010;&#27573;&#33853;&#24179;&#22343;&#20351;&#29992;7&#20010;&#21442;&#32771;&#25991;&#29486;&#26377;&#21161;&#20110;&#25152;&#26377;&#35780;&#20272;&#25351;&#26631;&#30340;&#25552;&#21319;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#26469;&#33258;&#19981;&#21516;&#36136;&#37327;&#30340;&#20379;&#24212;&#21830;&#30340;&#21442;&#32771;&#25991;&#29486;&#21487;&#20197;&#28151;&#21512;&#20351;&#29992;&#65292;&#24182;&#25552;&#39640;&#35780;&#20272;&#25351;&#26631;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#36739;&#39640;&#36136;&#37327;&#30340;&#21442;&#32771;&#25991;&#29486;&#21046;&#20316;&#25104;&#26412;&#26356;&#39640;&#65292;&#25105;&#20204;&#23558;&#20854;&#35270;&#20026;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65306;&#22312;&#29305;&#23450;&#39044;&#31639;&#19979;&#65292;&#24212;&#35813;&#25910;&#38598;&#21738;&#20123;&#21442;&#32771;&#25991;&#29486;&#20197;&#26368;&#22823;&#21270;&#35780;&#20272;&#25351;&#26631;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#21487;&#29992;&#20110;&#22312;&#29305;&#23450;&#39044;&#31639;&#19979;&#21019;&#24314;&#21442;&#32771;&#25991;&#29486;&#30340;&#20849;&#20139;&#20219;&#21153;&#30340;&#35780;&#20272;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic machine translation metrics often use human translations to determine the quality system translations. Common wisdom in the field dictates that the human references should be of very high quality. However, there are no cost-benefit analyses that could be used to guide practitioners who plan to collect references for machine translation evaluation. We find that higher-quality references lead to better metric correlations with humans at the segment-level. Having up to 7 references per segment and taking their average helps all metrics. Interestingly, the references from vendors of different qualities can be mixed together and improve metric success. Higher quality references, however, cost more to create and we frame this as an optimization problem: given a specific budget, what references should be collected to maximize metric success. These findings can be used by evaluators of shared tasks when references need to be created under a certain budget.
&lt;/p&gt;</description></item><item><title>Cheetah&#26159;&#19968;&#20010;&#38754;&#21521;517&#31181;&#38750;&#27954;&#35821;&#35328;&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#31181;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#32508;&#21512;&#35780;&#20272;&#21644;&#20154;&#24037;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#29983;&#25104;&#36830;&#36143;&#21644;&#19978;&#19979;&#25991;&#24688;&#24403;&#30340;&#25991;&#26412;&#26041;&#38754;&#30340;&#21331;&#36234;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#20419;&#36827;&#35821;&#35328;&#22810;&#26679;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2401.01053</link><description>&lt;p&gt;
Cheetah: 517&#31181;&#38750;&#27954;&#35821;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Cheetah: Natural Language Generation for 517 African Languages. (arXiv:2401.01053v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01053
&lt;/p&gt;
&lt;p&gt;
Cheetah&#26159;&#19968;&#20010;&#38754;&#21521;517&#31181;&#38750;&#27954;&#35821;&#35328;&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#31181;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#32508;&#21512;&#35780;&#20272;&#21644;&#20154;&#24037;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#29983;&#25104;&#36830;&#36143;&#21644;&#19978;&#19979;&#25991;&#24688;&#24403;&#30340;&#25991;&#26412;&#26041;&#38754;&#30340;&#21331;&#36234;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#20419;&#36827;&#35821;&#35328;&#22810;&#26679;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702; (NLP) &#20219;&#21153;&#26469;&#35828;&#65292;&#38750;&#27954;&#35821;&#35328;&#36164;&#28304;&#31232;&#32570;&#26159;&#19968;&#20010;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104; (NLG)&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;Cheetah&#65292;&#19968;&#20010;&#38754;&#21521;&#38750;&#27954;&#35821;&#35328;&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#31181;NLG&#35821;&#35328;&#27169;&#22411;&#12290;Cheetah&#25903;&#25345;517&#31181;&#38750;&#27954;&#35821;&#35328;&#21644;&#35821;&#35328;&#21464;&#20307;&#65292;&#35299;&#20915;&#20102;NLG&#36164;&#28304;&#21294;&#20047;&#38382;&#39064;&#65292;&#24182;&#20026;&#20419;&#36827;&#35821;&#35328;&#22810;&#26679;&#24615;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#36890;&#36807;&#19971;&#20010;&#29983;&#25104;&#19979;&#28216;&#20219;&#21153;&#30340;&#32508;&#21512;&#35780;&#20272;&#35777;&#26126;&#20102;Cheetah&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#19971;&#20010;&#20219;&#21153;&#20013;&#30340;&#20116;&#20010;&#20219;&#21153;&#20013;&#65292;Cheetah&#30340;&#34920;&#29616;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#24191;&#27867;&#33539;&#22260;&#30340;&#38750;&#27954;&#35821;&#35328;&#20013;&#29983;&#25104;&#36830;&#36143;&#21644;&#19978;&#19979;&#25991;&#24688;&#24403;&#30340;&#25991;&#26412;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#20154;&#24037;&#35780;&#20272;&#65292;&#20197;&#28145;&#20837;&#20102;&#35299;Cheetah&#30340;&#35821;&#35328;&#33021;&#21147;&#12290;Cheetah&#30340;&#24341;&#20837;&#23545;&#35821;&#35328;&#22810;&#26679;&#24615;&#20855;&#26377;&#28145;&#36828;&#30340;&#30410;&#22788;&#12290;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#24182;&#23558;&#20854;&#36866;&#24212;&#29305;&#23450;&#30340;&#38750;&#27954;&#35821;&#35328;&#65292;&#25105;&#20204;&#33021;&#22815;&#25552;&#20379;&#26356;&#22810;&#30340;&#35821;&#35328;&#29983;&#25104;&#36873;&#25321;&#21644;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-resource African languages pose unique challenges for natural language processing (NLP) tasks, including natural language generation (NLG). In this paper, we develop Cheetah, a massively multilingual NLG language model for African languages. Cheetah supports 517 African languages and language varieties, allowing us to address the scarcity of NLG resources and provide a solution to foster linguistic diversity. We demonstrate the effectiveness of Cheetah through comprehensive evaluations across seven generation downstream tasks. In five of the seven tasks, Cheetah significantly outperforms other models, showcasing its remarkable performance for generating coherent and contextually appropriate text in a wide range of African languages. We additionally conduct a detailed human evaluation to delve deeper into the linguistic capabilities of Cheetah. The introduction of Cheetah has far-reaching benefits for linguistic diversity. By leveraging pretrained models and adapting them to specifi
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#25972;&#21512;&#20195;&#30721;&#30340;&#22909;&#22788;&#65292;&#21253;&#25324;&#25552;&#21319;LLMs&#30340;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#12289;&#28608;&#21457;&#25512;&#29702;&#33021;&#21147;&#12289;&#29983;&#25104;&#32467;&#26500;&#21270;&#21644;&#31934;&#30830;&#30340;&#20013;&#38388;&#27493;&#39588;&#65292;&#24182;&#21033;&#29992;&#20195;&#30721;&#30340;&#32534;&#35793;&#21644;&#25191;&#34892;&#29615;&#22659;&#26469;&#25913;&#21892;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.00812</link><description>&lt;p&gt;
&#22914;&#26524;LLM&#26159;&#24043;&#24072;&#65292;&#37027;&#20040;&#20195;&#30721;&#23601;&#26159;&#39764;&#26454;&#65306;&#20851;&#20110;&#20195;&#30721;&#22914;&#20309;&#20351;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#26234;&#33021;&#20195;&#29702;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code Empowers Large Language Models to Serve as Intelligent Agents. (arXiv:2401.00812v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#25972;&#21512;&#20195;&#30721;&#30340;&#22909;&#22788;&#65292;&#21253;&#25324;&#25552;&#21319;LLMs&#30340;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#12289;&#28608;&#21457;&#25512;&#29702;&#33021;&#21147;&#12289;&#29983;&#25104;&#32467;&#26500;&#21270;&#21644;&#31934;&#30830;&#30340;&#20013;&#38388;&#27493;&#39588;&#65292;&#24182;&#21033;&#29992;&#20195;&#30721;&#30340;&#32534;&#35793;&#21644;&#25191;&#34892;&#29615;&#22659;&#26469;&#25913;&#21892;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20170;&#30693;&#21517;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#36807;&#21435;&#30340;&#35821;&#35328;&#27169;&#22411;&#19981;&#20165;&#22312;&#22823;&#23567;&#19978;&#26377;&#25152;&#19981;&#21516;&#65292;&#36824;&#22312;&#20110;&#23427;&#20204;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#32467;&#21512;&#20102;&#33258;&#28982;&#35821;&#35328;&#21644;&#24418;&#24335;&#35821;&#35328;&#65288;&#20195;&#30721;&#65289;&#12290;&#20316;&#20026;&#20154;&#31867;&#21644;&#35745;&#31639;&#26426;&#20043;&#38388;&#30340;&#20013;&#20171;&#65292;&#20195;&#30721;&#23558;&#39640;&#23618;&#27425;&#30340;&#30446;&#26631;&#36716;&#21270;&#20026;&#21487;&#25191;&#34892;&#30340;&#27493;&#39588;&#65292;&#20855;&#26377;&#26631;&#20934;&#30340;&#35821;&#27861;&#12289;&#36923;&#36753;&#19968;&#33268;&#24615;&#12289;&#25277;&#35937;&#21644;&#27169;&#22359;&#21270;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#23558;&#20195;&#30721;&#25972;&#21512;&#21040;LLMs&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#21508;&#31181;&#22909;&#22788;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#38500;&#20102;&#22686;&#24378;LLMs&#30340;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20195;&#30721;&#30340;&#36825;&#20123;&#29420;&#29305;&#23646;&#24615;&#26377;&#21161;&#20110;&#65288;i&#65289;&#28608;&#21457;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20351;&#20854;&#33021;&#22815;&#24212;&#29992;&#20110;&#26356;&#22797;&#26434;&#30340;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#65307;&#65288;ii&#65289;&#24341;&#23548;LLMs&#29983;&#25104;&#32467;&#26500;&#21270;&#21644;&#31934;&#30830;&#30340;&#20013;&#38388;&#27493;&#39588;&#65292;&#28982;&#21518;&#36890;&#36807;&#20989;&#25968;&#35843;&#29992;&#19982;&#22806;&#37096;&#25191;&#34892;&#31471;&#36830;&#25509;&#36215;&#26469;&#65307;&#20197;&#21450;&#65288;iii&#65289;&#21033;&#29992;&#20195;&#30721;&#32534;&#35793;&#21644;&#25191;&#34892;&#29615;&#22659;&#65292;&#20026;&#27169;&#22411;&#25913;&#36827;&#25552;&#20379;&#22810;&#26679;&#21270;&#30340;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prominent large language models (LLMs) of today differ from past language models not only in size, but also in the fact that they are trained on a combination of natural language and formal language (code). As a medium between humans and computers, code translates high-level goals into executable steps, featuring standard syntax, logical consistency, abstraction, and modularity. In this survey, we present an overview of the various benefits of integrating code into LLMs' training data. Specifically, beyond enhancing LLMs in code generation, we observe that these unique properties of code help (i) unlock the reasoning ability of LLMs, enabling their applications to a range of more complex natural language tasks; (ii) steer LLMs to produce structured and precise intermediate steps, which can then be connected to external execution ends through function calls; and (iii) take advantage of code compilation and execution environment, which also provides diverse feedback for model improve
&lt;/p&gt;</description></item><item><title>SecFormer&#26159;&#19968;&#20010;&#20248;&#21270;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;Transformer&#27169;&#22411;&#30340;&#24555;&#36895;&#20934;&#30830;&#38544;&#31169;&#20445;&#25252;&#25512;&#29702;&#12290;&#36890;&#36807;&#28040;&#38500;&#39640;&#25104;&#26412;&#30340;&#25351;&#25968;&#21644;&#32447;&#24615;&#25805;&#20316;&#65292;SecFormer&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24212;&#29992;SMPC&#26102;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.00793</link><description>&lt;p&gt;
SecFormer&#65306;&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#20934;&#30830;&#38544;&#31169;&#20445;&#25252;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
SecFormer: Towards Fast and Accurate Privacy-Preserving Inference for Large Language Models. (arXiv:2401.00793v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00793
&lt;/p&gt;
&lt;p&gt;
SecFormer&#26159;&#19968;&#20010;&#20248;&#21270;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;Transformer&#27169;&#22411;&#30340;&#24555;&#36895;&#20934;&#30830;&#38544;&#31169;&#20445;&#25252;&#25512;&#29702;&#12290;&#36890;&#36807;&#28040;&#38500;&#39640;&#25104;&#26412;&#30340;&#25351;&#25968;&#21644;&#32447;&#24615;&#25805;&#20316;&#65292;SecFormer&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24212;&#29992;SMPC&#26102;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22312;&#20113;&#24179;&#21488;&#19978;&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#25552;&#20379;&#25512;&#29702;&#26381;&#21153;&#30340;&#20351;&#29992;&#22686;&#21152;&#65292;&#38544;&#31169;&#38382;&#39064;&#26085;&#30410;&#21152;&#21095;&#65292;&#23588;&#20854;&#26159;&#28041;&#21450;&#25237;&#36164;&#35745;&#21010;&#21644;&#38134;&#34892;&#36134;&#25143;&#31561;&#25935;&#24863;&#25968;&#25454;&#12290;&#23433;&#20840;&#22810;&#26041;&#35745;&#31639;&#65288;SMPC&#65289;&#34987;&#35270;&#20026;&#20445;&#25252;&#25512;&#29702;&#25968;&#25454;&#21644;&#27169;&#22411;&#21442;&#25968;&#38544;&#31169;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;SMPC&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#29305;&#21035;&#26159;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#27169;&#22411;&#65289;&#30340;&#38544;&#31169;&#20445;&#25252;&#25512;&#29702;&#20013;&#30340;&#24212;&#29992;&#24448;&#24448;&#20250;&#23548;&#33268;&#26174;&#33879;&#30340;&#20943;&#36895;&#25110;&#24615;&#33021;&#19979;&#38477;&#12290;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;Transformer&#26550;&#26500;&#20013;&#30340;&#20247;&#22810;&#38750;&#32447;&#24615;&#25805;&#20316;&#19981;&#36866;&#21512;SMPC&#65292;&#24182;&#19988;&#38590;&#20197;&#26377;&#25928;&#35268;&#36991;&#25110;&#20248;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20808;&#36827;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#31216;&#20026;SecFormer&#65292;&#20197;&#23454;&#29616;Transformer&#27169;&#22411;&#30340;&#24555;&#36895;&#20934;&#30830;&#38544;&#31169;&#20445;&#25252;&#25512;&#29702;&#12290;&#36890;&#36807;&#23454;&#26045;&#27169;&#22411;&#35774;&#35745;&#20248;&#21270;&#65292;&#25105;&#20204;&#25104;&#21151;&#28040;&#38500;&#20102;&#39640;&#25104;&#26412;&#30340;&#25351;&#25968;&#21644;&#32447;&#24615;&#25805;&#20316;&#65292;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the growing use of large language models hosted on cloud platforms to offer inference services, privacy concerns are escalating, especially concerning sensitive data like investment plans and bank account details. Secure Multi-Party Computing (SMPC) emerges as a promising solution to protect the privacy of inference data and model parameters. However, the application of SMPC in Privacy-Preserving Inference (PPI) for large language models, particularly those based on the Transformer architecture, often leads to considerable slowdowns or declines in performance. This is largely due to the multitude of nonlinear operations in the Transformer architecture, which are not well-suited to SMPC and difficult to circumvent or optimize effectively. To address this concern, we introduce an advanced optimization framework called SecFormer, to achieve fast and accurate PPI for Transformer models. By implementing model design optimization, we successfully eliminate the high-cost exponential and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#33258;&#21160;&#21270;&#29983;&#25104;&#34892;&#21160;&#39033;&#39537;&#21160;&#30340;&#20250;&#35758;&#25688;&#35201;&#65292;&#36890;&#36807;&#36882;&#24402;&#29983;&#25104;&#20998;&#27573;&#25688;&#35201;&#24182;&#20351;&#29992;&#34892;&#21160;&#39033;&#25552;&#21462;&#31639;&#27861;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#24341;&#20837;&#20102;&#19977;&#31181;&#29992;&#20110;&#23558;&#38271;&#35760;&#24405;&#20998;&#21106;&#25104;&#20027;&#39064;&#37096;&#20998;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#31639;&#27861;&#30340;&#25928;&#29575;&#21644;&#35299;&#20915;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2312.17581</link><description>&lt;p&gt;
&#38271;&#26102;&#38388;&#20250;&#35758;&#35760;&#24405;&#30340;&#34892;&#21160;&#39033;&#39537;&#21160;&#25688;&#35201;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Action-Item-Driven Summarization of Long Meeting Transcripts. (arXiv:2312.17581v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.17581
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#33258;&#21160;&#21270;&#29983;&#25104;&#34892;&#21160;&#39033;&#39537;&#21160;&#30340;&#20250;&#35758;&#25688;&#35201;&#65292;&#36890;&#36807;&#36882;&#24402;&#29983;&#25104;&#20998;&#27573;&#25688;&#35201;&#24182;&#20351;&#29992;&#34892;&#21160;&#39033;&#25552;&#21462;&#31639;&#27861;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#24341;&#20837;&#20102;&#19977;&#31181;&#29992;&#20110;&#23558;&#38271;&#35760;&#24405;&#20998;&#21106;&#25104;&#20027;&#39064;&#37096;&#20998;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#31639;&#27861;&#30340;&#25928;&#29575;&#21644;&#35299;&#20915;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#20250;&#35758;&#30340;&#27969;&#34892;&#19979;&#65292;&#33258;&#21160;&#29983;&#25104;&#20250;&#35758;&#25688;&#35201;&#30340;&#27169;&#22411;&#21464;&#24471;&#26356;&#21152;&#23454;&#29992;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#33258;&#21160;&#21270;&#29983;&#25104;&#20250;&#35758;&#25688;&#35201;&#12290;&#24403;&#21069;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#21482;&#29983;&#25104;&#19968;&#33324;&#32780;&#22522;&#26412;&#30340;&#25688;&#35201;&#65292;&#23558;&#20250;&#35758;&#31616;&#21333;&#22320;&#35270;&#20026;&#19968;&#20010;&#38271;&#23545;&#35805;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#26032;&#31639;&#27861;&#21487;&#20197;&#26681;&#25454;&#20250;&#35758;&#35760;&#24405;&#20013;&#30340;&#34892;&#21160;&#39033;&#29983;&#25104;&#25277;&#35937;&#30340;&#20250;&#35758;&#25688;&#35201;&#12290;&#36825;&#26159;&#36890;&#36807;&#36882;&#24402;&#29983;&#25104;&#25688;&#35201;&#24182;&#24182;&#34892;&#36816;&#34892;&#25105;&#20204;&#30340;&#34892;&#21160;&#39033;&#25552;&#21462;&#31639;&#27861;&#26469;&#23454;&#29616;&#30340;&#12290;&#25152;&#26377;&#36825;&#20123;&#31456;&#33410;&#25688;&#35201;&#28982;&#21518;&#21512;&#24182;&#24182;&#24635;&#32467;&#22312;&#19968;&#36215;&#65292;&#20197;&#21019;&#24314;&#19968;&#20010;&#36830;&#36143;&#19988;&#20197;&#34892;&#21160;&#39033;&#20026;&#23548;&#21521;&#30340;&#25688;&#35201;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#20171;&#32461;&#20102;&#19977;&#31181;&#23558;&#38271;&#35760;&#24405;&#20998;&#21106;&#25104;&#22522;&#20110;&#20027;&#39064;&#30340;&#37096;&#20998;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#31639;&#27861;&#30340;&#26102;&#38388;&#25928;&#29575;&#65292;&#24182;&#35299;&#20915;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increased prevalence of online meetings has significantly enhanced the practicality of a model that can automatically generate the summary of a given meeting. This paper introduces a novel and effective approach to automate the generation of meeting summaries. Current approaches to this problem generate general and basic summaries, considering the meeting simply as a long dialogue. However, our novel algorithms can generate abstractive meeting summaries that are driven by the action items contained in the meeting transcript. This is done by recursively generating summaries and employing our action-item extraction algorithm for each section of the meeting in parallel. All of these sectional summaries are then combined and summarized together to create a coherent and action-item-driven summary. In addition, this paper introduces three novel methods for dividing up long transcripts into topic-based sections to improve the time efficiency of our algorithm, as well as to resolve the iss
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#22686;&#24378;&#23545;&#35805;&#25351;&#23548;&#30340;&#36890;&#29992;&#20449;&#24687;&#25277;&#21462;&#35843;&#20248;&#26694;&#26550;&#65288;YAYI-UIE&#65289;&#65292;&#21033;&#29992;&#23545;&#35805;&#25968;&#25454;&#21644;&#20449;&#24687;&#25277;&#21462;&#25968;&#25454;&#20849;&#21516;&#22686;&#24378;&#20449;&#24687;&#25277;&#21462;&#24615;&#33021;&#65292;&#22312;&#20013;&#25991;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#19994;&#30028;&#39046;&#20808;&#30340;&#24615;&#33021;&#65292;&#22312;&#33521;&#25991;&#25968;&#25454;&#38598;&#19978;&#20063;&#36798;&#21040;&#20102;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.15548</link><description>&lt;p&gt;
YAYI-UIE: &#19968;&#20010;&#22686;&#24378;&#23545;&#35805;&#25351;&#23548;&#30340;&#36890;&#29992;&#20449;&#24687;&#25277;&#21462;&#35843;&#20248;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
YAYI-UIE: A Chat-Enhanced Instruction Tuning Framework for Universal Information Extraction. (arXiv:2312.15548v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.15548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#22686;&#24378;&#23545;&#35805;&#25351;&#23548;&#30340;&#36890;&#29992;&#20449;&#24687;&#25277;&#21462;&#35843;&#20248;&#26694;&#26550;&#65288;YAYI-UIE&#65289;&#65292;&#21033;&#29992;&#23545;&#35805;&#25968;&#25454;&#21644;&#20449;&#24687;&#25277;&#21462;&#25968;&#25454;&#20849;&#21516;&#22686;&#24378;&#20449;&#24687;&#25277;&#21462;&#24615;&#33021;&#65292;&#22312;&#20013;&#25991;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#19994;&#30028;&#39046;&#20808;&#30340;&#24615;&#33021;&#65292;&#22312;&#33521;&#25991;&#25968;&#25454;&#38598;&#19978;&#20063;&#36798;&#21040;&#20102;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#30340;&#38590;&#28857;&#22312;&#20110;&#22788;&#29702;&#29305;&#23450;&#20219;&#21153;&#30340;&#26631;&#31614;&#27169;&#24335;&#21644;&#24322;&#26500;&#25968;&#25454;&#32467;&#26500;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#32479;&#19968;&#24314;&#27169;&#19981;&#21516;&#30340;&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#29616;&#26377;&#26041;&#27861;&#22312;&#38500;&#20102;&#33521;&#35821;&#20197;&#22806;&#30340;&#20013;&#25991;&#35821;&#35328;&#30340;&#20449;&#24687;&#25552;&#21462;&#33021;&#21147;&#19978;&#23384;&#22312;&#19981;&#36275;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#22686;&#24378;&#23545;&#35805;&#25351;&#23548;&#30340;&#36890;&#29992;&#20449;&#24687;&#25277;&#21462;&#35843;&#20248;&#26694;&#26550;&#65288;YAYI-UIE&#65289;&#65292;&#25903;&#25345;&#20013;&#25991;&#21644;&#33521;&#25991;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#23545;&#35805;&#25968;&#25454;&#21644;&#20449;&#24687;&#25277;&#21462;&#25968;&#25454;&#20849;&#21516;&#22686;&#24378;&#20449;&#24687;&#25277;&#21462;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#20013;&#25991;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#19994;&#30028;&#39046;&#20808;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#22312;&#26377;&#30417;&#30563;&#21644;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#22312;&#33521;&#25991;&#25968;&#25454;&#38598;&#19978;&#20063;&#36798;&#21040;&#20102;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The difficulty of the information extraction task lies in dealing with the task-specific label schemas and heterogeneous data structures. Recent work has proposed methods based on large language models to uniformly model different information extraction tasks. However, these existing methods are deficient in their information extraction capabilities for Chinese languages other than English. In this paper, we propose an end-to-end chat-enhanced instruction tuning framework for universal information extraction (YAYI-UIE), which supports both Chinese and English. Specifically, we utilize dialogue data and information extraction data to enhance the information extraction performance jointly. Experimental results show that our proposed framework achieves state-of-the-art performance on Chinese datasets while also achieving comparable performance on English datasets under both supervised settings and zero-shot settings.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23618;&#27425;&#20449;&#24687;&#20132;&#20114;&#26694;&#26550;&#29992;&#20110;&#19981;&#23436;&#25972;&#35805;&#35821;&#37325;&#20889;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#19978;&#19979;&#25991;&#36873;&#25321;&#12289;&#32534;&#36753;&#30697;&#38453;&#26500;&#24314;&#21644;&#30456;&#20851;&#24615;&#21512;&#24182;&#25429;&#25417;&#22810;&#23618;&#27425;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#22312;&#19981;&#23436;&#25972;&#35805;&#35821;&#32534;&#36753;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.11945</link><description>&lt;p&gt;
&#22810;&#23618;&#27425;&#20449;&#24687;&#20132;&#20114;&#26694;&#26550;&#29992;&#20110;&#19981;&#23436;&#25972;&#35805;&#35821;&#37325;&#20889;
&lt;/p&gt;
&lt;p&gt;
Multi-Granularity Information Interaction Framework for Incomplete Utterance Rewriting. (arXiv:2312.11945v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11945
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23618;&#27425;&#20449;&#24687;&#20132;&#20114;&#26694;&#26550;&#29992;&#20110;&#19981;&#23436;&#25972;&#35805;&#35821;&#37325;&#20889;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#19978;&#19979;&#25991;&#36873;&#25321;&#12289;&#32534;&#36753;&#30697;&#38453;&#26500;&#24314;&#21644;&#30456;&#20851;&#24615;&#21512;&#24182;&#25429;&#25417;&#22810;&#23618;&#27425;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#22312;&#19981;&#23436;&#25972;&#35805;&#35821;&#32534;&#36753;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#19981;&#23436;&#25972;&#35805;&#35821;&#37325;&#20889;&#65288;IUR&#65289;&#30340;&#26041;&#27861;&#26410;&#33021;&#25429;&#25417;&#21040;&#20851;&#38190;&#35789;&#30340;&#26469;&#28304;&#65292;&#36825;&#23545;&#20110;&#32534;&#36753;&#19981;&#23436;&#25972;&#35805;&#35821;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#24341;&#20837;&#20102;&#26469;&#33258;&#26080;&#20851;&#35805;&#35821;&#30340;&#35789;&#35821;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26377;&#25928;&#30340;&#22810;&#20219;&#21153;&#20449;&#24687;&#20132;&#20114;&#26694;&#26550;&#65292;&#21253;&#25324;&#19978;&#19979;&#25991;&#36873;&#25321;&#12289;&#32534;&#36753;&#30697;&#38453;&#26500;&#24314;&#21644;&#30456;&#20851;&#24615;&#21512;&#24182;&#65292;&#20197;&#25429;&#25417;&#35821;&#20041;&#20449;&#24687;&#30340;&#22810;&#23618;&#27425;&#24615;&#12290;&#36890;&#36807;&#33719;&#21462;&#30456;&#20851;&#35805;&#35821;&#21644;&#30830;&#23450;&#37325;&#35201;&#35789;&#35821;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35813;&#39046;&#22495;&#30340;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;Restoration-200K&#21644;CANAND&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;&#20195;&#30721;&#23558;&#22312;\url{https://github.com/yanmenxue/QR}&#19978;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent approaches in Incomplete Utterance Rewriting (IUR) fail to capture the source of important words, which is crucial to edit the incomplete utterance, and introduce words from irrelevant utterances. We propose a novel and effective multi-task information interaction framework including context selection, edit matrix construction, and relevance merging to capture the multi-granularity of semantic information. Benefiting from fetching the relevant utterance and figuring out the important words, our approach outperforms existing state-of-the-art models on two benchmark datasets Restoration-200K and CANAND in this field. Code will be provided on \url{https://github.com/yanmenxue/QR}.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#26681;&#25454;&#24739;&#32773;&#35328;&#35821;&#19981;&#27969;&#30021;&#31243;&#24230;&#33258;&#21160;&#35843;&#25972;&#33647;&#29289;&#65292;&#36890;&#36807;&#23545;&#33647;&#29289;&#32452;&#21512;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#20248;&#21270;&#65292;&#33021;&#22815;&#25910;&#25947;&#21040;&#33391;&#22909;&#30340;&#29992;&#33647;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2312.11509</link><description>&lt;p&gt;
&#38754;&#21521;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33647;&#29289;&#35843;&#25972;&#31995;&#32479;&#20197;&#20943;&#23569;&#35328;&#35821;&#19981;&#27969;&#30021;&#30340;&#35770;&#25991;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Toward A Reinforcement-Learning-Based System for Adjusting Medication to Minimize Speech Disfluency. (arXiv:2312.11509v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11509
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#26681;&#25454;&#24739;&#32773;&#35328;&#35821;&#19981;&#27969;&#30021;&#31243;&#24230;&#33258;&#21160;&#35843;&#25972;&#33647;&#29289;&#65292;&#36890;&#36807;&#23545;&#33647;&#29289;&#32452;&#21512;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#20248;&#21270;&#65292;&#33021;&#22815;&#25910;&#25947;&#21040;&#33391;&#22909;&#30340;&#29992;&#33647;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#33258;&#21160;&#20026;&#24739;&#26377;&#19982;&#24515;&#29702;&#20581;&#24247;&#30456;&#20851;&#30340;&#35328;&#35821;&#19981;&#27969;&#30021;&#30340;&#34394;&#25311;&#24739;&#32773;&#24320;&#20855;&#33647;&#29289;&#22788;&#26041;&#65292;&#24182;&#26681;&#25454;&#38646;&#25104;&#26412;&#39057;&#32321;&#27979;&#37327;&#32467;&#26524;&#65292;&#35843;&#25972;&#33647;&#29289;&#21644;&#21058;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#31995;&#32479;&#30340;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;&#19968;&#20010;&#22312;&#25105;&#20204;&#26500;&#24314;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#26816;&#27979;&#21644;&#35780;&#20272;&#35328;&#35821;&#19981;&#27969;&#30021;&#30340;&#27169;&#22359;&#65292;&#20197;&#21450;&#19968;&#20010;&#21487;&#20197;&#33258;&#21160;&#25214;&#21040;&#33391;&#22909;&#33647;&#29289;&#32452;&#21512;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#20004;&#20010;&#27169;&#22359;&#65292;&#25105;&#20204;&#20174;&#25991;&#29486;&#20013;&#25910;&#38598;&#20102;&#20851;&#20110;&#33647;&#29289;&#27835;&#30103;&#35328;&#35821;&#19981;&#27969;&#30021;&#30340;&#25928;&#26524;&#30340;&#25968;&#25454;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#21487;&#20449;&#30340;&#24739;&#32773;&#27169;&#25311;&#31995;&#32479;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#33021;&#22815;&#25910;&#25947;&#21040;&#19968;&#20010;&#33391;&#22909;&#30340;&#29992;&#33647;&#26041;&#26696;&#12290;&#25105;&#20204;&#25910;&#38598;&#24182;&#23545;&#21487;&#33021;&#23384;&#22312;&#35328;&#35821;&#19981;&#27969;&#30021;&#30340;&#20154;&#32676;&#36827;&#34892;&#20102;&#25968;&#25454;&#26631;&#27880;&#65292;&#24182;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#19968;&#20010;&#27010;&#24565;&#39564;&#35777;:
&lt;/p&gt;
&lt;p&gt;
We propose a Reinforcement-Learning-based system that would automatically prescribe a hypothetical patient medication that may help the patient with their mental-health-related speech disfluency, and adjust the medication and the dosages in response to zero-cost frequent measurement of the fluency of the patient. We demonstrate the components of the system: a module that detects and evaluates speech disfluency on a large dataset we built, and a Reinforcement Learning algorithm that automatically finds good combinations of medications. To support the two modules, we collect data on the effect of psychiatric medications for speech disfluency from the literature, and build a plausible patient simulation system. We demonstrate that the Reinforcement Learning system is, under some circumstances, able to converge to a good medication regime. We collect and label a dataset of people with possible speech disfluency and demonstrate our methods using that dataset. Our work is a proof of concept:
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#21407;&#25991;&#25913;&#20889;"&#30340;&#20219;&#21153;&#26469;&#22788;&#29702;&#38271;&#25991;&#26412;&#38382;&#31572;&#65292;&#36890;&#36807;&#20302;&#25104;&#26412;&#39640;&#25928;&#30340;&#26041;&#27861;&#25104;&#21151;&#25193;&#23637;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#33267;32k&#65292;&#24182;&#22312;&#22810;&#25991;&#26723;&#38382;&#31572;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2312.11193</link><description>&lt;p&gt;
"&#21407;&#25991;&#25913;&#20889;"&#25552;&#39640;&#20102;&#39640;&#31934;&#24230;&#38271;&#25991;&#26412;&#38382;&#31572;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
"Paraphrasing The Original Text" Makes High Accuracy Long-Context QA. (arXiv:2312.11193v6 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#21407;&#25991;&#25913;&#20889;"&#30340;&#20219;&#21153;&#26469;&#22788;&#29702;&#38271;&#25991;&#26412;&#38382;&#31572;&#65292;&#36890;&#36807;&#20302;&#25104;&#26412;&#39640;&#25928;&#30340;&#26041;&#27861;&#25104;&#21151;&#25193;&#23637;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#33267;32k&#65292;&#24182;&#22312;&#22810;&#25991;&#26723;&#38382;&#31572;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#38754;&#23545;&#38271;&#25991;&#26412;&#26102;&#65292;&#22823;&#22810;&#25968;&#24320;&#28304;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#38480;&#21046;&#22312;4k&#20197;&#20869;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#33021;&#21147;&#12290;&#21363;&#20351;&#26159;&#20855;&#26377;&#26356;&#38271;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#27169;&#22411;&#20063;&#26080;&#27861;&#22312;&#38271;&#19978;&#19979;&#25991;&#38382;&#39064;&#19978;&#20445;&#35777;&#20196;&#20154;&#28385;&#24847;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;&#35757;&#32451;&#25968;&#25454;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25552;&#39640;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#33021;&#21147;&#38656;&#35201;&#30340;&#26159;"&#26377;&#25928;"&#32780;&#19981;&#20165;&#20165;&#26159;"&#38271;"&#30340;&#25968;&#25454;&#12290;&#22522;&#20110;&#36825;&#20010;&#27934;&#35265;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;"&#21407;&#25991;&#25913;&#20889;"&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#19968;&#31181;&#20302;&#25104;&#26412;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#25104;&#21151;&#23558;&#29616;&#26377;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#21040;32k&#12290;&#25105;&#20204;&#30340;&#24494;&#35843;&#27169;&#22411;&#22312;&#20855;&#26377;&#30456;&#36817;&#35268;&#27169;&#30340;&#27169;&#22411;&#20013;&#22312;&#22810;&#25991;&#26723;&#38382;&#31572;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;&#27169;&#22411;&#21644;&#35757;&#32451;&#25968;&#25454;&#24050;&#32463;&#22312;HuggingFace&#65288;https://huggingface.co/yuyijiong/Qwen-14b-chat-yarn-32k&#65289;&#21644;WiseModel&#65288;https://wisemodel.cn/models/yuyijiong/Qwen-14b-chat-yarn-32k&#65289;&#19978;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most open-source generative language models currently have a context window of no more than 4k, limiting their ability when facing long text. Even models with longer context windows cannot guarantee satisfactory accuracy on long-context problems. To tackle this issue, we explore from the perspective of training data and theoretically demonstrate that improving the capability to handle long contexts requires "effective" rather than simply "long" data. Based on this insight, we propose using the "original text paraphrasing" task and successfully extend the context window of existing models to 32k through a low-cost and effective method. Our fine-tuned model achieves state-of-the-art accuracy in multi-document-QA among models of comparable scale. The model and training data have been made available on HuggingFace(https://huggingface.co/yuyijiong/Qwen-14b-chat-yarn-32k) and WiseModel(https://wisemodel.cn/models/yuyijiong/Qwen-14b-chat-yarn-32k).
&lt;/p&gt;</description></item><item><title>RJUA-QA&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#27852;&#23615;&#22806;&#31185;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#24110;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#21487;&#38752;&#30340;&#35786;&#26029;&#21644;&#24314;&#35758;&#26041;&#38754;&#21457;&#25381;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2312.09785</link><description>&lt;p&gt;
RJUA-QA&#65306;&#19968;&#20221;&#20840;&#38754;&#30340;&#27852;&#23615;&#22806;&#31185;&#38382;&#31572;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
RJUA-QA: A Comprehensive QA Dataset for Urology. (arXiv:2312.09785v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.09785
&lt;/p&gt;
&lt;p&gt;
RJUA-QA&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#27852;&#23615;&#22806;&#31185;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#24110;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#21487;&#38752;&#30340;&#35786;&#26029;&#21644;&#24314;&#35758;&#26041;&#38754;&#21457;&#25381;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;RJUA-QA&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#21307;&#23398;&#38382;&#31572;&#21644;&#22522;&#20110;&#20020;&#24202;&#35777;&#25454;&#25512;&#29702;&#30340;&#25968;&#25454;&#38598;&#65292;&#26377;&#21161;&#20110;&#24357;&#21512;&#36890;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#21307;&#23398;&#29305;&#23450;LLM&#24212;&#29992;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;RJUA-QA&#26469;&#28304;&#20110;&#30495;&#23454;&#30340;&#20020;&#24202;&#22330;&#26223;&#65292;&#26088;&#22312;&#24110;&#21161;LLMs&#29983;&#25104;&#21487;&#38752;&#30340;&#35786;&#26029;&#21644;&#24314;&#35758;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;2,132&#20010;&#31574;&#21010;&#38382;&#39064;-&#19978;&#19979;&#25991;-&#31572;&#26696;&#23545;&#65292;&#23545;&#24212;&#32422;25,000&#26465;&#35786;&#26029;&#35760;&#24405;&#21644;&#20020;&#24202;&#26696;&#20363;&#12290;&#35813;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;67&#20010;&#24120;&#35265;&#30340;&#27852;&#23615;&#22806;&#31185;&#30142;&#30149;&#31867;&#21035;&#65292;&#30142;&#30149;&#35206;&#30422;&#29575;&#36229;&#36807;97.6&#65285;&#30340;&#23547;&#27714;&#27852;&#23615;&#22806;&#31185;&#21307;&#30103;&#26381;&#21153;&#30340;&#20154;&#32676;&#12290;RJUA-QA&#20013;&#30340;&#27599;&#20010;&#25968;&#25454;&#23454;&#20363;&#21253;&#25324;&#65306;&#65288;1&#65289;&#19968;&#20010;&#27169;&#20223;&#30495;&#23454;&#24739;&#32773;&#30340;&#38382;&#39064;&#65292;&#35810;&#38382;&#20020;&#24202;&#30151;&#29366;&#21644;&#21307;&#23398;&#29366;&#20917;&#65292;&#65288;2&#65289;&#21253;&#21547;&#20840;&#38754;&#19987;&#23478;&#30693;&#35782;&#30340;&#19978;&#19979;&#25991;&#65292;&#20316;&#20026;&#21307;&#23398;&#26816;&#26597;&#21644;&#35786;&#26029;&#30340;&#21442;&#32771;&#65292;&#65288;3&#65289;&#21307;&#29983;&#30340;&#31572;&#22797;&#25552;&#20379;&#35786;&#26029;&#32467;&#35770;&#21644;&#24314;&#35758;&#30340;&#26816;&#26597;&#25351;&#23548;&#65292;&#65288;4&#65289;&#19968;&#20123;&#8230;
&lt;/p&gt;
&lt;p&gt;
We introduce RJUA-QA, a novel medical dataset for question answering (QA) and reasoning with clinical evidence, contributing to bridge the gap between general large language models (LLMs) and medical-specific LLM applications. RJUA-QA is derived from realistic clinical scenarios and aims to facilitate LLMs in generating reliable diagnostic and advice. The dataset contains 2,132 curated Question-Context-Answer pairs, corresponding about 25,000 diagnostic records and clinical cases. The dataset covers 67 common urological disease categories, where the disease coverage exceeds 97.6\% of the population seeking medical services in urology. Each data instance in RJUA-QA comprises: (1) a question mirroring real patient to inquiry about clinical symptoms and medical conditions, (2) a context including comprehensive expert knowledge, serving as a reference for medical examination and diagnosis, (3) a doctor response offering the diagnostic conclusion and suggested examination guidance, (4) a di
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20998;&#26512;&#20551;&#26032;&#38395;&#23545;2024&#24180;&#36873;&#20030;&#32467;&#26524;&#30340;&#25552;&#21069;&#24433;&#21709;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;40,000&#31687;&#20851;&#20110;&#21271;&#32654;&#25919;&#27835;&#28436;&#35762;&#30340;&#26032;&#38395;&#25991;&#31456;&#65292;&#24182;&#21033;&#29992;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#24037;&#39564;&#35777;&#26041;&#27861;&#23545;&#20854;&#20013;&#30340;&#19968;&#37096;&#20998;&#36827;&#34892;&#20102;&#27880;&#37322;&#12290;&#30740;&#31350;&#20154;&#21592;&#40723;&#21169;&#20351;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#24182;&#20026;&#35813;&#20513;&#35758;&#20570;&#20986;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2312.03750</link><description>&lt;p&gt;
&#20998;&#26512;&#20551;&#26032;&#38395;&#23545;2024&#24180;&#36873;&#20030;&#32467;&#26524;&#30340;&#25552;&#21069;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Analyzing the Impact of Fake News on the Anticipated Outcome of the 2024 Election Ahead of Time. (arXiv:2312.03750v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.03750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20998;&#26512;&#20551;&#26032;&#38395;&#23545;2024&#24180;&#36873;&#20030;&#32467;&#26524;&#30340;&#25552;&#21069;&#24433;&#21709;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;40,000&#31687;&#20851;&#20110;&#21271;&#32654;&#25919;&#27835;&#28436;&#35762;&#30340;&#26032;&#38395;&#25991;&#31456;&#65292;&#24182;&#21033;&#29992;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#24037;&#39564;&#35777;&#26041;&#27861;&#23545;&#20854;&#20013;&#30340;&#19968;&#37096;&#20998;&#36827;&#34892;&#20102;&#27880;&#37322;&#12290;&#30740;&#31350;&#20154;&#21592;&#40723;&#21169;&#20351;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#24182;&#20026;&#35813;&#20513;&#35758;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#23545;&#20551;&#26032;&#38395;&#30340;&#35748;&#35782;&#21644;&#30740;&#31350;&#36234;&#26469;&#36234;&#22810;&#65292;&#20294;&#22312;&#21271;&#32654;&#25919;&#27835;&#28436;&#35762;&#20013;&#29305;&#23450;&#38024;&#23545;&#31181;&#26063;&#20398;&#36785;&#21644;&#20559;&#35265;&#30340;&#25968;&#25454;&#38598;&#20173;&#28982;&#24456;&#26377;&#24517;&#35201;&#12290;&#36825;&#22312;&#21363;&#23558;&#20030;&#34892;&#30340;&#21271;&#32654;&#36873;&#20030;&#30340;&#32972;&#26223;&#19979;&#23588;&#20026;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#35823;&#23548;&#20449;&#24687;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#20026;&#20102;&#24320;&#21457;&#36825;&#20010;&#20551;&#26032;&#38395;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25910;&#38598;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#21253;&#21547;4&#19975;&#31687;&#21271;&#32654;&#25919;&#27835;&#28436;&#35762;&#26032;&#38395;&#25991;&#31456;&#30340;&#35821;&#26009;&#24211;&#12290;&#20854;&#20013;&#30340;&#19968;&#37096;&#20998;&#25968;&#25454;&#38598;&#65288;4000&#31687;&#65289;&#32463;&#36807;&#20180;&#32454;&#27880;&#37322;&#65292;&#37319;&#29992;&#20102;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#24037;&#39564;&#35777;&#26041;&#27861;&#30340;&#34701;&#21512;&#12290;&#25105;&#20204;&#24050;&#32463;&#23558;&#36825;&#20004;&#20010;&#25968;&#25454;&#38598;&#20844;&#24320;&#25552;&#20379;&#32473;&#30740;&#31350;&#30028;&#65292;&#24182;&#22312;&#27880;&#37322;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#24615;&#33021;&#22522;&#20934;&#27979;&#35797;&#20197;&#23637;&#31034;&#20854;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#34920;&#29616;&#26368;&#20339;&#30340;&#35821;&#35328;&#27169;&#22411;&#20197;&#21450;&#25968;&#25454;&#12290;&#25105;&#20204;&#40723;&#21169;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#32773;&#20351;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#20026;&#36825;&#19968;&#25345;&#32493;&#36827;&#34892;&#30340;&#20513;&#35758;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite increasing awareness and research around fake news, there is still a significant need for datasets that specifically target racial slurs and biases within North American political speeches. This is particulary important in the context of upcoming North American elections. This study introduces a comprehensive dataset that illuminates these critical aspects of misinformation. To develop this fake news dataset, we scraped and built a corpus of 40,000 news articles about political discourses in North America. A portion of this dataset (4000) was then carefully annotated, using a blend of advanced language models and human verification methods. We have made both these datasets openly available to the research community and have conducted benchmarking on the annotated data to demonstrate its utility. We release the best-performing language model along with data. We encourage researchers and developers to make use of this dataset and contribute to this ongoing initiative.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#21307;&#23398;&#30693;&#35782;&#24314;&#27169;&#21040;&#36890;&#29992;LLM&#20013;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#36807;&#31243;&#23558;&#20854;&#20174;&#21307;&#23398;&#21021;&#23398;&#32773;&#35843;&#25972;&#20026;&#21307;&#23398;&#19987;&#23478;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22238;&#31572;&#21307;&#23398;&#38382;&#39064;&#26041;&#38754;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#65292;&#21516;&#26102;&#20351;&#29992;&#36739;&#23567;&#35268;&#27169;&#30340;&#27169;&#22411;&#22823;&#23567;&#65292;&#19982;&#22823;&#35268;&#27169;LLM&#27169;&#22411;&#30456;&#24403;&#65292;&#26356;&#21152;&#36866;&#29992;&#20110;&#21307;&#23398;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2312.01040</link><description>&lt;p&gt;
&#20174;&#21021;&#23398;&#32773;&#21040;&#19987;&#23478;&#65306;&#23558;&#21307;&#23398;&#30693;&#35782;&#24314;&#27169;&#21040;&#36890;&#29992;LLM&#20013;
&lt;/p&gt;
&lt;p&gt;
From Beginner to Expert: Modeling Medical Knowledge into General LLMs. (arXiv:2312.01040v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.01040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#21307;&#23398;&#30693;&#35782;&#24314;&#27169;&#21040;&#36890;&#29992;LLM&#20013;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#36807;&#31243;&#23558;&#20854;&#20174;&#21307;&#23398;&#21021;&#23398;&#32773;&#35843;&#25972;&#20026;&#21307;&#23398;&#19987;&#23478;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22238;&#31572;&#21307;&#23398;&#38382;&#39064;&#26041;&#38754;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#65292;&#21516;&#26102;&#20351;&#29992;&#36739;&#23567;&#35268;&#27169;&#30340;&#27169;&#22411;&#22823;&#23567;&#65292;&#19982;&#22823;&#35268;&#27169;LLM&#27169;&#22411;&#30456;&#24403;&#65292;&#26356;&#21152;&#36866;&#29992;&#20110;&#21307;&#23398;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#28041;&#21450;&#21040;&#23545;&#21307;&#23398;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#21644;&#20197;&#21307;&#29983;&#30340;&#26041;&#24335;&#22238;&#31572;&#21307;&#23398;&#38382;&#39064;&#31561;&#25935;&#24863;&#24212;&#29992;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#38754;&#20020;&#30528;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#35797;&#22270;&#36890;&#36807;&#22686;&#21152;&#27169;&#22411;&#30340;&#22823;&#23567;&#65288;&gt;100B&#65289;&#26469;&#23398;&#20064;&#26356;&#36890;&#29992;&#30340;&#21307;&#23398;&#30693;&#35782;&#65292;&#20294;&#22312;&#27169;&#22411;&#35268;&#27169;&#36739;&#23567;&#65288;&lt;100B&#65289;&#30340;LLM&#20013;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#36890;&#29992;LLM&#27169;&#22411;&#65288;AntGLM-10B&#65289;&#24320;&#22987;&#65292;&#32463;&#36807;&#19977;&#38454;&#27573;&#30340;&#20248;&#21270;&#36807;&#31243;&#65292;&#21363;&#36890;&#29992;&#21307;&#23398;&#30693;&#35782;&#27880;&#20837;&#12289;&#21307;&#23398;&#39046;&#22495;&#25351;&#23548;&#35843;&#20248;&#21644;&#29305;&#23450;&#21307;&#23398;&#20219;&#21153;&#36866;&#24212;&#65292;&#23558;&#20854;&#20174;&#21307;&#23398;&#21021;&#23398;&#32773;&#31934;&#32454;&#35843;&#25972;&#20026;&#21307;&#23398;&#19987;&#23478;&#65288;&#31216;&#20026;AntGLM-Med-10B&#65289;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#20027;&#35201;&#26377;&#19977;&#20010;&#26041;&#38754;&#65306;&#65288;1&#65289;&#25105;&#20204;&#20855;&#20307;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#39044;&#35757;&#32451;&#30340;&#36890;&#29992;LLM&#27169;&#22411;&#35843;&#25972;&#20026;&#21307;&#23398;&#19987;&#23478;&#12290;(2) We evaluate the performance of our model on various medical tasks and demonstrate its effectiveness and reliability in answering medical questions. (3) We show that our approach can achieve comparable performance to larger-scale LLM models (&gt;100B) while using a smaller-scale model size (&lt;100B), making it more practical and accessible for medical applications.
&lt;/p&gt;
&lt;p&gt;
Recently, large language model (LLM) based artificial intelligence (AI) systems have demonstrated remarkable capabilities in natural language understanding and generation. However, these models face a significant challenge when it comes to sensitive applications, such as reasoning over medical knowledge and answering medical questions in a physician-like manner. Prior studies attempted to overcome this challenge by increasing the model size (&gt;100B) to learn more general medical knowledge, while there is still room for improvement in LLMs with smaller-scale model sizes (&lt;100B). In this work, we start from a pre-trained general LLM model (AntGLM-10B) and fine-tune it from a medical beginner towards a medical expert (called AntGLM-Med-10B), which leverages a 3-stage optimization procedure, i.e., general medical knowledge injection, medical domain instruction tuning, and specific medical task adaptation. Our contributions are threefold: (1) We specifically investigate how to adapt a pre-tr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36827;&#34892;&#20102;&#19968;&#39033;&#27604;&#36739;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;&#33258;&#21160;&#21270;&#21307;&#23398;&#25253;&#21578;&#20013;10&#31181;&#20934;&#30830;&#24615;&#24230;&#37327;&#25351;&#26631;&#30340;&#24212;&#29992;&#12290;&#20197;&#20013;&#32819;&#21672;&#35810;&#20026;&#20363;&#65292;&#20998;&#26512;&#20102;&#29983;&#25104;&#30340;&#25253;&#21578;&#19982;&#20840;&#31185;&#21307;&#29983;&#25253;&#21578;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#32452;&#21512;&#20934;&#30830;&#24230;&#24471;&#20998;&#65292;&#29992;&#20110;&#27604;&#36739;&#33258;&#21160;&#21270;&#21307;&#23398;&#25253;&#21578;&#39046;&#22495;&#20869;&#30340;&#24230;&#37327;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2311.13273</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#21307;&#23398;&#25253;&#21578;&#20934;&#30830;&#24615;&#24230;&#37327;&#30340;&#27604;&#36739;&#23454;&#39564;&#65306;&#20197;&#20013;&#32819;&#21672;&#35810;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Comparative Experimentation of Accuracy Metrics in Automated Medical Reporting: The Case of Otitis Consultations. (arXiv:2311.13273v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.13273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36827;&#34892;&#20102;&#19968;&#39033;&#27604;&#36739;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;&#33258;&#21160;&#21270;&#21307;&#23398;&#25253;&#21578;&#20013;10&#31181;&#20934;&#30830;&#24615;&#24230;&#37327;&#25351;&#26631;&#30340;&#24212;&#29992;&#12290;&#20197;&#20013;&#32819;&#21672;&#35810;&#20026;&#20363;&#65292;&#20998;&#26512;&#20102;&#29983;&#25104;&#30340;&#25253;&#21578;&#19982;&#20840;&#31185;&#21307;&#29983;&#25253;&#21578;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#32452;&#21512;&#20934;&#30830;&#24230;&#24471;&#20998;&#65292;&#29992;&#20110;&#27604;&#36739;&#33258;&#21160;&#21270;&#21307;&#23398;&#25253;&#21578;&#39046;&#22495;&#20869;&#30340;&#24230;&#37327;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#30340;&#20154;&#24037;&#26234;&#33021;&#21487;&#20197;&#26681;&#25454;&#21307;&#23398;&#21672;&#35810;&#30340;&#25991;&#26412;&#29983;&#25104;&#21307;&#23398;&#25253;&#21578;&#12290;&#20854;&#30446;&#30340;&#26159;&#20943;&#36731;&#21307;&#30103;&#20445;&#20581;&#19987;&#19994;&#20154;&#21592;&#30340;&#34892;&#25919;&#36127;&#25285;&#12290;&#20026;&#20102;&#30830;&#20445;&#25253;&#21578;&#30340;&#27491;&#30830;&#24615;&#21644;&#23454;&#29992;&#24615;&#65292;&#38656;&#35201;&#23545;&#29983;&#25104;&#30340;&#25253;&#21578;&#30340;&#20934;&#30830;&#24615;&#36827;&#34892;&#35780;&#20272;&#12290;&#26377;&#20960;&#31181;&#24230;&#37327;&#25351;&#26631;&#21487;&#29992;&#20110;&#34913;&#37327;AI&#29983;&#25104;&#30340;&#25253;&#21578;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#22312;&#21307;&#23398;&#25253;&#21578;&#20013;&#24212;&#29992;&#36825;&#20123;&#25351;&#26631;&#30340;&#30740;&#31350;&#24456;&#23569;&#12290;&#25105;&#20204;&#23545;&#19982;&#20013;&#32819;&#21672;&#35810;&#30456;&#20851;&#30340;&#29983;&#25104;&#30340;&#21307;&#23398;&#25253;&#21578;&#21644;&#30456;&#24212;&#30340;&#20840;&#31185;&#21307;&#29983;&#25253;&#21578;&#36827;&#34892;&#20102;10&#31181;&#20934;&#30830;&#24615;&#24230;&#37327;&#30340;&#27604;&#36739;&#23454;&#39564;&#12290;&#25105;&#20204;&#23558;&#29983;&#25104;&#30340;&#25253;&#21578;&#20013;&#32570;&#22833;&#12289;&#38169;&#35823;&#21644;&#38468;&#21152;&#30340;&#38472;&#36848;&#19982;&#24230;&#37327;&#25351;&#26631;&#24471;&#20998;&#36827;&#34892;&#20102;&#30456;&#20851;&#24615;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#21644;&#23450;&#20041;&#20102;&#19968;&#20010;&#32452;&#21512;&#20934;&#30830;&#24230;&#24471;&#20998;&#65292;&#29992;&#20110;&#27604;&#36739;&#33258;&#21160;&#21270;&#21307;&#23398;&#25253;&#21578;&#39046;&#22495;&#20869;&#30340;&#24230;&#37327;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Artificial Intelligence (AI) can be used to automatically generate medical reports based on transcripts of medical consultations. The aim is to reduce the administrative burden that healthcare professionals face. The accuracy of the generated reports needs to be established to ensure their correctness and usefulness. There are several metrics for measuring the accuracy of AI generated reports, but little work has been done towards the application of these metrics in medical reporting. A comparative experimentation of 10 accuracy metrics has been performed on AI generated medical reports against their corresponding General Practitioner's (GP) medical reports concerning Otitis consultations. The number of missing, incorrect, and additional statements of the generated reports have been correlated with the metric scores. In addition, we introduce and define a Composite Accuracy Score which produces a single score for comparing the metrics within the field of automated medical re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#27010;&#24565;&#32423;&#21035;&#30340;&#35823;&#30456;&#20851;&#24615;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;ChatGPT&#20998;&#37197;&#27010;&#24565;&#26631;&#31614;&#21644;&#24341;&#20837;&#25968;&#25454;&#20877;&#24179;&#34913;&#25216;&#26415;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.08648</link><description>&lt;p&gt;
&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#25506;&#32034;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#27010;&#24565;&#32423;&#21035;&#30340;&#35823;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
Explore Spurious Correlations at the Concept Level in Language Models for Text Classification. (arXiv:2311.08648v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.08648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#27010;&#24565;&#32423;&#21035;&#30340;&#35823;&#30456;&#20851;&#24615;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;ChatGPT&#20998;&#37197;&#27010;&#24565;&#26631;&#31614;&#21644;&#24341;&#20837;&#25968;&#25454;&#20877;&#24179;&#34913;&#25216;&#26415;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#20247;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#37319;&#29992;&#20102;&#24494;&#35843;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#12290;&#34429;&#28982;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#20013;&#26631;&#31614;&#20998;&#24067;&#19981;&#24179;&#34913;&#25110;&#19978;&#19979;&#25991;&#23398;&#20064;&#23454;&#20363;&#20135;&#29983;&#30340;&#35823;&#30456;&#20851;&#24615;&#65292;&#23427;&#20204;&#38754;&#20020;&#30528;&#40065;&#26834;&#24615;&#25361;&#25112;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#35789;&#35821;&#12289;&#30701;&#35821;&#21644;&#21477;&#27861;&#29305;&#24449;&#19978;&#65292;&#24573;&#35270;&#20102;&#27010;&#24565;&#32423;&#21035;&#30340;&#30740;&#31350;&#65292;&#36825;&#24448;&#24448;&#26159;&#30001;&#20110;&#32570;&#20047;&#27010;&#24565;&#26631;&#31614;&#21644;&#38590;&#20197;&#30830;&#23450;&#36755;&#20837;&#25991;&#26412;&#20013;&#30340;&#27010;&#24565;&#20869;&#23481;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#20027;&#35201;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;ChatGPT&#20026;&#25991;&#26412;&#20998;&#37197;&#27010;&#24565;&#26631;&#31614;&#65292;&#35780;&#20272;&#27169;&#22411;&#22312;&#24494;&#35843;&#25110;&#19978;&#19979;&#25991;&#23398;&#20064;&#27979;&#35797;&#25968;&#25454;&#20013;&#30340;&#27010;&#24565;&#20559;&#24046;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;&#35821;&#35328;&#27169;&#22411;&#22312;&#35757;&#32451;&#25110;&#25552;&#31034;&#20013;&#36935;&#21040;&#27010;&#24565;&#21644;&#26631;&#31614;&#20043;&#38388;&#30340;&#35823;&#30456;&#20851;&#24615;&#26102;&#65292;&#20250;&#37319;&#21462;&#39044;&#27979;&#30340;&#25463;&#24452;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#25968;&#25454;&#20877;&#24179;&#34913;&#25216;&#26415;&#65292;&#23558;ChatGPT&#29983;&#25104;&#30340;&#21453;&#20107;&#23454;&#25968;&#25454;&#32435;&#20837;&#20854;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) have achieved notable success in numerous NLP tasks, employing both fine-tuning and in-context learning (ICL) methods. While language models demonstrate exceptional performance, they face robustness challenges due to spurious correlations arising from imbalanced label distributions in training data or ICL exemplars. Previous research has primarily concentrated on word, phrase, and syntax features, neglecting the concept level, often due to the absence of concept labels and difficulty in identifying conceptual content in input texts. This paper introduces two main contributions. First, we employ ChatGPT to assign concept labels to texts, assessing concept bias in models during fine-tuning or ICL on test data. We find that LMs, when encountering spurious correlations between a concept and a label in training or prompts, resort to shortcuts for predictions. Second, we introduce a data rebalancing technique that incorporates ChatGPT-generated counterfactual data, ther
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#35843;&#26597;&#35270;&#35273;&#19982;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20869;&#22312;&#30693;&#35782;&#65292;&#21457;&#29616;&#23427;&#20204;&#26174;&#31034;&#20102;&#22768;&#38899;&#35937;&#24449;&#24615;&#30340;&#27169;&#24335;&#65292;&#36827;&#19968;&#27493;&#35777;&#23454;&#22768;&#38899;&#21644;&#24847;&#20041;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#22312;&#36328;&#27169;&#24577;&#20851;&#32852;&#20013;&#24471;&#21040;&#20102;&#20307;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.16781</link><description>&lt;p&gt;
Kiki&#36824;&#26159;Bouba&#65311;&#35270;&#35273;&#19982;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#22768;&#38899;&#35937;&#24449;&#24615;
&lt;/p&gt;
&lt;p&gt;
Kiki or Bouba? Sound Symbolism in Vision-and-Language Models. (arXiv:2310.16781v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16781
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#35843;&#26597;&#35270;&#35273;&#19982;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20869;&#22312;&#30693;&#35782;&#65292;&#21457;&#29616;&#23427;&#20204;&#26174;&#31034;&#20102;&#22768;&#38899;&#35937;&#24449;&#24615;&#30340;&#27169;&#24335;&#65292;&#36827;&#19968;&#27493;&#35777;&#23454;&#22768;&#38899;&#21644;&#24847;&#20041;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#22312;&#36328;&#27169;&#24577;&#20851;&#32852;&#20013;&#24471;&#21040;&#20102;&#20307;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20154;&#31867;&#35821;&#35328;&#20013;&#30340;&#22768;&#38899;&#21644;&#24847;&#20041;&#20043;&#38388;&#30340;&#26144;&#23556;&#34987;&#35748;&#20026;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#38543;&#26426;&#30340;&#65292;&#20294;&#35748;&#30693;&#31185;&#23398;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#35821;&#35328;&#21644;&#20154;&#21475;&#32676;&#20307;&#20043;&#38388;&#30340;&#29305;&#23450;&#22768;&#38899;&#21644;&#24847;&#20041;&#20043;&#38388;&#23384;&#22312;&#38750;&#24179;&#20961;&#30340;&#30456;&#20851;&#24615;&#65292;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#22768;&#38899;&#35937;&#24449;&#24615;&#12290;&#22312;&#35768;&#22810;&#24847;&#20041;&#32500;&#24230;&#20013;&#65292;&#22768;&#38899;&#35937;&#24449;&#24615;&#22312;&#35821;&#35328;&#21644;&#35270;&#35273;&#39046;&#22495;&#20043;&#38388;&#30340;&#36328;&#27169;&#24577;&#20851;&#32852;&#26041;&#38754;&#23588;&#20026;&#26174;&#33879;&#21644;&#20805;&#20998;&#35777;&#26126;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22768;&#38899;&#35937;&#24449;&#24615;&#26159;&#21542;&#22312;CLIP&#21644;Stable Diffusion&#31561;&#35270;&#35273;&#19982;&#35821;&#35328;&#27169;&#22411;&#20013;&#24471;&#21040;&#20307;&#29616;&#12290;&#36890;&#36807;&#20351;&#29992;&#38646;&#26679;&#26412;&#30693;&#35782;&#25506;&#27979;&#26469;&#35843;&#26597;&#36825;&#20123;&#27169;&#22411;&#30340;&#20869;&#22312;&#30693;&#35782;&#65292;&#25105;&#20204;&#21457;&#29616;&#24378;&#26377;&#21147;&#30340;&#35777;&#25454;&#34920;&#26126;&#23427;&#20204;&#30830;&#23454;&#26174;&#31034;&#20102;&#36825;&#31181;&#27169;&#24335;&#65292;&#19982;&#24515;&#29702;&#35821;&#35328;&#23398;&#20013;&#20247;&#25152;&#21608;&#30693;&#30340;kiki-bouba&#25928;&#24212;&#30456;&#19968;&#33268;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#19968;&#31181;&#20351;&#29992;&#35745;&#31639;&#24037;&#20855;&#26469;&#23637;&#31034;&#22768;&#38899;&#35937;&#24449;&#24615;&#24182;&#29702;&#35299;&#20854;&#26412;&#36136;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#23558;&#20844;&#24320;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although the mapping between sound and meaning in human language is assumed to be largely arbitrary, research in cognitive science has shown that there are non-trivial correlations between particular sounds and meanings across languages and demographic groups, a phenomenon known as sound symbolism. Among the many dimensions of meaning, sound symbolism is particularly salient and well-demonstrated with regards to cross-modal associations between language and the visual domain. In this work, we address the question of whether sound symbolism is reflected in vision-and-language models such as CLIP and Stable Diffusion. Using zero-shot knowledge probing to investigate the inherent knowledge of these models, we find strong evidence that they do show this pattern, paralleling the well-known kiki-bouba effect in psycholinguistics. Our work provides a novel method for demonstrating sound symbolism and understanding its nature using computational tools. Our code will be made publicly available.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#24314;&#31435;&#20010;&#24615;&#21270;&#35789;&#20856;&#65292;&#26469;&#23450;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20010;&#24615;&#29305;&#24449;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#20197;&#21487;&#25554;&#25300;&#30340;&#26041;&#24335;&#32467;&#21512;&#20116;&#20010;&#22823;&#31867;&#22240;&#32032;&#65292;&#23454;&#29616;&#23545;&#20010;&#24615;&#29305;&#24449;&#30340;&#31934;&#30830;&#25805;&#32437;&#12290;</title><link>http://arxiv.org/abs/2310.16582</link><description>&lt;p&gt;
&#36890;&#36807;&#26080;&#30417;&#30563;&#24314;&#31435;&#20010;&#24615;&#21270;&#35789;&#20856;&#26469;&#23450;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20010;&#24615;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Tailoring Personality Traits in Large Language Models via Unsupervisedly-Built Personalized Lexicons. (arXiv:2310.16582v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#24314;&#31435;&#20010;&#24615;&#21270;&#35789;&#20856;&#65292;&#26469;&#23450;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20010;&#24615;&#29305;&#24449;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#20197;&#21487;&#25554;&#25300;&#30340;&#26041;&#24335;&#32467;&#21512;&#20116;&#20010;&#22823;&#31867;&#22240;&#32032;&#65292;&#23454;&#29616;&#23545;&#20010;&#24615;&#29305;&#24449;&#30340;&#31934;&#30830;&#25805;&#32437;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#22312;&#22609;&#36896;&#20154;&#31867;&#34920;&#36798;&#27169;&#24335;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#36171;&#20104;&#21644;&#25805;&#32437;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20010;&#24615;&#29305;&#24449;&#22312;&#25552;&#21319;&#29992;&#25143;&#20307;&#39564;&#26041;&#38754;&#20855;&#26377;&#37325;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#35201;&#20040;&#20381;&#36182;&#20110;&#22312;&#23500;&#21547;&#20010;&#24615;&#34920;&#36798;&#30340;&#35821;&#26009;&#24211;&#19978;&#23545;LLM&#36827;&#34892;&#24494;&#35843;&#65292;&#35201;&#20040;&#38656;&#35201;&#25163;&#21160;&#21046;&#20316;&#25552;&#31034;&#26469;&#35825;&#23548;LLM&#20135;&#29983;&#20010;&#24615;&#21270;&#22238;&#24212;&#12290;&#21069;&#32773;&#38656;&#35201;&#22823;&#37327;&#26102;&#38388;&#21644;&#36164;&#28304;&#26469;&#25910;&#38598;&#36275;&#22815;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#32780;&#21518;&#32773;&#21487;&#33021;&#26080;&#27861;&#31934;&#30830;&#25805;&#32437;&#20010;&#24615;&#29305;&#24449;&#20197;&#36798;&#21040;&#32454;&#31890;&#24230;&#30340;&#27700;&#24179;&#65288;&#20363;&#22914;&#65292;&#22312;&#20943;&#23569;&#24320;&#25918;&#24615;&#30340;&#21516;&#26102;&#25552;&#39640;&#23452;&#20154;&#24615;&#65289;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#23450;&#21046;LLM&#20013;&#30340;&#20010;&#24615;&#29305;&#24449;&#65292;&#20801;&#35768;&#20197;&#21487;&#25554;&#25300;&#30340;&#26041;&#24335;&#32467;&#21512;&#20116;&#20010;&#22823;&#31867;&#22240;&#32032;&#65288;&#21363;&#24320;&#25918;&#24615;&#12289;&#36131;&#20219;&#24515;&#12289;&#22806;&#21521;&#24615;&#12289;&#23452;&#20154;&#24615;&#21644;&#31070;&#32463;&#36136;&#65289;&#30340;&#20219;&#24847;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personality plays a pivotal role in shaping human expression patterns, and empowering and manipulating large language models (LLMs) with personality traits holds significant promise in enhancing the user experience of LLMs. However, prior approaches either rely on fine-tuning LLMs on a corpus enriched with personalized expressions or necessitate the manual crafting of prompts to induce LLMs to produce personalized responses. The former approaches demand substantial time and resources for collecting sufficient training examples while the latter might fail in enabling the precise manipulation of the personality traits at a fine-grained level (e.g., achieving high agreeableness while reducing openness). In this study, we introduce a novel approach for tailoring personality traits within LLMs, allowing for the incorporation of any combination of the Big Five factors (i.e., openness, conscientiousness, extraversion, agreeableness, and neuroticism) in a pluggable manner. This is achieved by 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20219;&#21153;&#30340;&#35810;&#38382;&#65288;TOA&#65289;&#30340;&#27010;&#24565;&#21644;&#26694;&#26550;&#65292;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#23545;&#25512;&#29702;&#20219;&#21153;&#26377;&#29992;&#31572;&#26696;&#30340;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#21516;&#26102;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20107;&#23454;&#32423;&#36974;&#34109;&#65288;FLM&#65289;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#23558;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#38598;&#36716;&#25442;&#20026;&#33258;&#25105;&#30417;&#30563;&#30340;TOA&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2310.11571</link><description>&lt;p&gt;
&#20160;&#20040;&#26159;&#19968;&#20010;&#22909;&#38382;&#39064;&#65311;&#22522;&#20110;&#20219;&#21153;&#30340;&#35810;&#38382;&#19982;&#20107;&#23454;&#32423;&#36974;&#34109;&#12290;
&lt;/p&gt;
&lt;p&gt;
What is a good question? Task-oriented asking with fact-level masking. (arXiv:2310.11571v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11571
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20219;&#21153;&#30340;&#35810;&#38382;&#65288;TOA&#65289;&#30340;&#27010;&#24565;&#21644;&#26694;&#26550;&#65292;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#23545;&#25512;&#29702;&#20219;&#21153;&#26377;&#29992;&#31572;&#26696;&#30340;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#21516;&#26102;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20107;&#23454;&#32423;&#36974;&#34109;&#65288;FLM&#65289;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#23558;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#38598;&#36716;&#25442;&#20026;&#33258;&#25105;&#30417;&#30563;&#30340;TOA&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#38382;&#26159;&#29616;&#23454;&#29983;&#27963;&#20013;&#21512;&#20316;&#25512;&#29702;&#20219;&#21153;&#65288;&#22914;&#38382;&#31572;&#65289;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#20363;&#22914;&#65292;&#19968;&#20010;&#27861;&#24459;&#21161;&#25163;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#27809;&#26377;&#29992;&#25143;&#24773;&#20917;&#30340;&#20855;&#20307;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#21487;&#33021;&#26080;&#27861;&#25552;&#20379;&#20934;&#30830;&#30340;&#24314;&#35758;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;&#20250;&#30452;&#25509;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35299;&#20915;&#25512;&#29702;&#20219;&#21153;&#65292;&#32780;&#19981;&#20250;&#21521;&#29992;&#25143;&#25110;&#31532;&#19977;&#26041;&#25552;&#20986;&#21518;&#32493;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#31216;&#20026;&#22522;&#20110;&#20219;&#21153;&#30340;&#35810;&#38382;&#65288;TOA&#65289;&#12290;&#38646;-shot&#32842;&#22825;&#27169;&#22411;&#21487;&#20197;&#25191;&#34892;TOA&#65292;&#20294;&#23427;&#20204;&#30340;&#35757;&#32451;&#20027;&#35201;&#22522;&#20110;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#65292;&#32780;&#19981;&#26159;&#38382;&#39064;&#26159;&#21542;&#23545;&#25104;&#21151;&#30340;&#21512;&#20316;&#26377;&#24110;&#21161;&#12290;&#20026;&#20102;&#33021;&#22815;&#35757;&#32451;&#21644;&#35780;&#20272;TOA&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#23548;&#21521;&#35810;&#38382;&#30340;&#23450;&#20041;&#21644;&#26694;&#26550;&#65292;&#21363;&#29983;&#25104;&#33021;&#22815;&#20026;&#25512;&#29702;&#20219;&#21153;&#25552;&#20379;&#26377;&#29992;&#31572;&#26696;&#30340;&#38382;&#39064;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20107;&#23454;&#32423;&#36974;&#34109;&#65288;FLM&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#30465;&#30053;&#29305;&#23450;&#30340;&#37096;&#20998;&#23558;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#38598;&#36716;&#25442;&#20026;&#33258;&#25105;&#30417;&#30563;&#30340;TOA&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Asking questions is an important element of real-life collaboration on reasoning tasks like question answering. For example, a legal assistant chatbot may be unable to make accurate recommendations without specific information on the user's circumstances. However, large language models are usually deployed to solve reasoning tasks directly without asking follow-up questions to the user or third parties. We term this problem task-oriented asking (TOA). Zero-shot chat models can perform TOA, but their training is primarily based on next-token prediction rather than whether questions contribute to successful collaboration. To enable the training and evaluation of TOA models, we present a definition and framework for natural language task-oriented asking, the problem of generating questions that result in answers useful for a reasoning task. We also present fact-level masking (FLM), a procedure for converting natural language datasets into self-supervised TOA datasets by omitting particula
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ResidualTransformer&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;Transformer&#32534;&#30721;&#22120;&#23618;&#20043;&#38388;&#30340;&#27169;&#22411;&#26435;&#37325;&#65292;&#23558;&#27169;&#22411;&#30340;&#22823;&#23567;&#20943;&#23567;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ResidualTransformer&#30340;&#24615;&#33021;&#20248;&#20110;&#20256;&#32479;Transformer&#27169;&#22411;&#65292;&#19988;&#27169;&#22411;&#22823;&#23567;&#24471;&#21040;&#20102;&#26174;&#33879;&#20943;&#23567;&#12290;</title><link>http://arxiv.org/abs/2310.02489</link><description>&lt;p&gt;
ResidualTransformer&#65306;&#24102;&#26377;&#26435;&#37325;&#20849;&#20139;&#30340;&#27531;&#24046;&#20302;&#31209;&#23398;&#20064;&#30340;Transformer&#23618;
&lt;/p&gt;
&lt;p&gt;
ResidualTransformer: Residual Low-rank Learning with Weight-sharing for Transformer Layers. (arXiv:2310.02489v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02489
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ResidualTransformer&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;Transformer&#32534;&#30721;&#22120;&#23618;&#20043;&#38388;&#30340;&#27169;&#22411;&#26435;&#37325;&#65292;&#23558;&#27169;&#22411;&#30340;&#22823;&#23567;&#20943;&#23567;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ResidualTransformer&#30340;&#24615;&#33021;&#20248;&#20110;&#20256;&#32479;Transformer&#27169;&#22411;&#65292;&#19988;&#27169;&#22411;&#22823;&#23567;&#24471;&#21040;&#20102;&#26174;&#33879;&#20943;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37096;&#32626;&#35821;&#38899;&#22788;&#29702;&#27169;&#22411;&#21040;&#22987;&#32456;&#24320;&#21551;&#35774;&#22791;&#19978;&#26102;&#65292;&#20869;&#23384;&#38480;&#21046;&#26159;&#19968;&#20010;&#20027;&#35201;&#20851;&#27880;&#28857;&#20043;&#19968;&#12290;&#34429;&#28982;&#20351;&#29992;&#36275;&#22815;&#22823;&#37327;&#30340;&#25968;&#25454;&#35757;&#32451;&#24471;&#21040;&#30340;&#26356;&#22823;&#30340;&#27169;&#22411;&#36890;&#24120;&#34920;&#29616;&#26356;&#22909;&#65292;&#20294;&#20351;&#20854;&#36866;&#24212;&#35774;&#22791;&#20869;&#23384;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;Transformer&#32534;&#30721;&#22120;&#23618;&#20043;&#38388;&#30340;&#27169;&#22411;&#26435;&#37325;&#65292;&#24182;&#20551;&#35774;&#29305;&#27530;&#30340;&#26435;&#37325;&#32452;&#21512;&#21644;&#32467;&#26500;&#65292;&#26469;&#20943;&#23567;&#27169;&#22411;&#30340;&#22823;&#23567;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#21463;ResNet&#21644;&#26368;&#26032;&#30340;LoRA&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ResidualTransformer&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;Transformer&#23618;&#20013;&#30340;&#27599;&#20010;&#26435;&#37325;&#30697;&#38453;&#21253;&#25324;1&#65289;&#19982;&#20854;&#30456;&#37051;&#23618;&#20849;&#20139;&#30340;&#28385;&#31209;&#32452;&#20214;&#65292;&#21644;2&#65289;&#20165;&#23646;&#20110;&#23427;&#33258;&#24049;&#30340;&#29420;&#29305;&#20302;&#31209;&#32452;&#20214;&#12290;&#20302;&#31209;&#30697;&#38453;&#21482;&#21344;&#27169;&#22411;&#22823;&#23567;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#28155;&#21152;&#23545;&#35282;&#32447;&#26435;&#37325;&#30697;&#38453;&#26469;&#25552;&#39640;&#20302;&#31209;&#30697;&#38453;&#30340;&#24314;&#27169;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;10k&#23567;&#26102;&#35821;&#38899;&#35782;&#21035;&#21644;&#35821;&#38899;&#32763;&#35793;&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ResidualTransformer&#30340;&#24615;&#33021;&#20248;&#20110;&#20256;&#32479;Transformer&#27169;&#22411;&#65292;&#19988;&#27169;&#22411;&#22823;&#23567;&#24471;&#21040;&#20102;&#26174;&#33879;&#20943;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Memory constraint of always-on devices is one of the major concerns when deploying speech processing models on these devices. While larger models trained with sufficiently large amount of data generally perform better, making them fit in the device memory is a demanding challenge. In this paper, we aim to reduce model size by reparameterizing model weights across Transformer encoder layers and assuming a special weight composition and structure. More specifically, inspired by ResNet and the more recent LoRA work, we propose an approach named ResidualTransformer, where each weight matrix in a Transformer layer comprises 1) a shared full-rank component with its adjacent layers, and 2) a unique low-rank component to itself. The low-rank matrices only account for a small amount of model size increase. In addition, we add diagonal weight matrices to improve modeling capacity of the low-rank matrices. Experiments of our 10k-hour speech recognition and speech translation tasks show that the T
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35789;&#34955;&#27169;&#22411;&#33258;&#21160;&#20272;&#35745;&#35838;&#22530;&#25945;&#23398;&#25903;&#25345;&#65292;&#20197;&#25552;&#20379;&#26356;&#20855;&#20307;&#12289;&#39057;&#32321;&#21644;&#21487;&#34892;&#21160;&#30340;&#21453;&#39304;&#32473;&#25945;&#24072;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20934;&#30830;&#24615;&#25509;&#36817;&#20110;&#20154;&#24037;&#20114;&#35780;&#21487;&#38752;&#24615;&#65292;LLM&#27169;&#22411;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#21040;&#25945;&#23398;&#25903;&#25345;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2310.01132</link><description>&lt;p&gt;
&#20351;&#29992;LLM&#21644;BoWs&#33258;&#21160;&#35780;&#20272;&#35838;&#22530;&#25945;&#23398;&#25903;&#25345;&#65306;&#23558;&#20840;&#23616;&#39044;&#27979;&#19982;&#20855;&#20307;&#21453;&#39304;&#30456;&#36830;&#25509;
&lt;/p&gt;
&lt;p&gt;
Automated Evaluation of Classroom Instructional Support with LLMs and BoWs: Connecting Global Predictions to Specific Feedback. (arXiv:2310.01132v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01132
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35789;&#34955;&#27169;&#22411;&#33258;&#21160;&#20272;&#35745;&#35838;&#22530;&#25945;&#23398;&#25903;&#25345;&#65292;&#20197;&#25552;&#20379;&#26356;&#20855;&#20307;&#12289;&#39057;&#32321;&#21644;&#21487;&#34892;&#21160;&#30340;&#21453;&#39304;&#32473;&#25945;&#24072;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20934;&#30830;&#24615;&#25509;&#36817;&#20110;&#20154;&#24037;&#20114;&#35780;&#21487;&#38752;&#24615;&#65292;LLM&#27169;&#22411;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#21040;&#25945;&#23398;&#25903;&#25345;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#21521;&#25945;&#24072;&#25552;&#20379;&#26356;&#20855;&#20307;&#12289;&#26356;&#39057;&#32321;&#21644;&#21487;&#34892;&#21160;&#30340;&#21453;&#39304;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#20272;&#35745;&#8220;&#25945;&#23398;&#25903;&#25345;&#8221;&#39046;&#22495;&#30340;CLASS&#35838;&#22530;&#35780;&#20272;&#24471;&#20998;&#65292;&#35813;&#35780;&#20272;&#26041;&#27861;&#26159;&#24191;&#27867;&#20351;&#29992;&#30340;&#35266;&#27979;&#21327;&#35758;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#65292;&#20351;&#29992;Meta&#30340;Llama2&#30340;&#38646;-shot&#25552;&#31034;&#65292;&#21644;/&#25110;&#32463;&#20856;&#30340;&#35789;&#34955;&#65288;BoW&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#23545;&#25945;&#24072;&#35328;&#35821;&#30340;&#20010;&#21035;&#35805;&#35821;&#65288;&#20351;&#29992;OpenAI&#30340;Whisper&#36827;&#34892;&#33258;&#21160;&#36716;&#24405;&#65289;&#36827;&#34892;&#20998;&#31867;&#65292;&#20197;&#30830;&#23450;&#26159;&#21542;&#23384;&#22312;&#25945;&#23398;&#25903;&#25345;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#35805;&#35821;&#32423;&#30340;&#21028;&#26029;&#32467;&#26524;&#22312;&#25972;&#20010;15&#20998;&#38047;&#30340;&#35266;&#23519;&#20250;&#35805;&#20013;&#36827;&#34892;&#32858;&#21512;&#65292;&#20197;&#20272;&#35745;&#20840;&#23616;CLASS&#24471;&#20998;&#12290;&#22312;&#24188;&#20799;&#22253;&#21644;&#23398;&#21069;&#29677;&#25945;&#23460;&#30340;&#20004;&#20010;&#32463;&#36807;CLASS&#32534;&#30721;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65306;&#65288;1&#65289;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33258;&#21160;&#20272;&#35745;CLASS&#25945;&#23398;&#25903;&#25345;&#30340;&#20934;&#30830;&#24615;&#65288;Pearson R&#39640;&#36798;0.47&#65289;&#25509;&#36817;&#20154;&#24037;&#20114;&#35780;&#21487;&#38752;&#24615;&#65288;&#26368;&#39640;R=0.55&#65289;&#65307;&#65288;2&#65289;LLM&#27169;&#22411;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#21040;&#23567;&#29677;&#25945;&#23460;&#20013;&#30340;&#25945;&#23398;&#25903;&#25345;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the aim to provide teachers with more specific, frequent, and actionable feedback about their teaching, we explore how Large Language Models (LLMs) can be used to estimate ``Instructional Support'' domain scores of the CLassroom Assessment Scoring System (CLASS), a widely used observation protocol. We design a machine learning architecture that uses either zero-shot prompting of Meta's Llama2, and/or a classic Bag of Words (BoW) model, to classify individual utterances of teachers' speech (transcribed automatically using OpenAI's Whisper) for the presence of Instructional Support. Then, these utterance-level judgments are aggregated over an entire 15-min observation session to estimate a global CLASS score. Experiments on two CLASS-coded datasets of toddler and pre-kindergarten classrooms indicate that (1) automatic CLASS Instructional Support estimation accuracy using the proposed method (Pearson $R$ up to $0.47$) approaches human inter-rater reliability (up to $R=0.55$); (2) LLM
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#21487;&#20197;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26469;&#25913;&#21892;&#36739;&#23567;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#19979;&#28216;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.01119</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Synthetic Data Generation in Low-Resource Settings via Fine-Tuning of Large Language Models. (arXiv:2310.01119v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01119
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#21487;&#20197;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26469;&#25913;&#21892;&#36739;&#23567;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#19979;&#28216;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#20351;&#23427;&#20204;&#33021;&#22815;&#20197;&#30456;&#23545;&#36739;&#23569;&#30340;&#26631;&#35760;&#26679;&#26412;&#25512;&#24191;&#21040;&#26032;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#38656;&#35201;&#24040;&#22823;&#30340;&#35745;&#31639;&#36164;&#28304;&#25165;&#33021;&#37096;&#32626;&#12290;&#30456;&#21453;&#65292;&#22914;&#26524;&#29992;&#36275;&#22815;&#22810;&#30340;&#26631;&#35760;&#26679;&#26412;&#23545;&#36739;&#23567;&#30340;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#23427;&#20204;&#21487;&#20197;&#35299;&#20915;&#29305;&#23450;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26679;&#26412;&#33719;&#21462;&#36215;&#26469;&#24456;&#26114;&#36149;&#12290;&#20026;&#20102;&#36861;&#27714;&#20004;&#20840;&#20854;&#32654;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36890;&#36807;&#23545;&#32463;&#36807;&#31934;&#32454;&#35843;&#25972;&#30340;&#25945;&#24072;LLMs&#29983;&#25104;&#30340;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#21512;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#65292;&#20197;&#25913;&#21892;&#36739;&#23567;&#27169;&#22411;&#30340;&#19979;&#28216;&#24615;&#33021;&#12290;&#22312;&#22235;&#20010;&#25991;&#26412;&#20998;&#31867;&#21644;&#20004;&#20010;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#25968;&#25454;&#29983;&#25104;&#21644;&#27880;&#37322;&#37117;&#26174;&#33879;&#25552;&#39640;&#20102;&#30456;&#24212;&#19979;&#28216;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#26377;&#26102;&#21482;&#38656;&#35201;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
The in-context learning ability of large language models (LLMs) enables them to generalize to novel downstream tasks with relatively few labeled examples. However, they require enormous computational resources to be deployed. Alternatively, smaller models can solve specific tasks if fine-tuned with enough labeled examples. These examples, however, are expensive to obtain. In pursuit of the best of both worlds, we study synthetic data generation of fine-tuning training data via fine-tuned teacher LLMs to improve the downstream performance of much smaller models. In four text classification and two text generation tasks, we find that both data generation and annotation dramatically improve the respective downstream model's performance, occasionally necessitating only a minor fraction of the original training dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#22797;&#26434;&#25351;&#20196;&#33021;&#21147;&#30340;&#22522;&#20934;&#8212;&#8212;CELLO&#12290;&#36890;&#36807;&#35774;&#35745;&#22797;&#26434;&#25351;&#20196;&#30340;&#20843;&#20010;&#29305;&#24449;&#24182;&#26500;&#24314;&#20840;&#38754;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#35299;&#20915;&#29616;&#26377;&#22522;&#20934;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2309.09150</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#29702;&#35299;&#30495;&#23454;&#19990;&#30028;&#22797;&#26434;&#25351;&#20196;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Understand Real-World Complex Instructions?. (arXiv:2309.09150v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09150
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#22797;&#26434;&#25351;&#20196;&#33021;&#21147;&#30340;&#22522;&#20934;&#8212;&#8212;CELLO&#12290;&#36890;&#36807;&#35774;&#35745;&#22797;&#26434;&#25351;&#20196;&#30340;&#20843;&#20010;&#29305;&#24449;&#24182;&#26500;&#24314;&#20840;&#38754;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#35299;&#20915;&#29616;&#26377;&#22522;&#20934;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#29702;&#35299;&#20154;&#31867;&#25351;&#20196;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#20256;&#32479;NLP&#20219;&#21153;&#20043;&#22806;&#30340;&#23454;&#29992;&#24212;&#29992;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20173;&#28982;&#22312;&#22797;&#26434;&#25351;&#20196;&#19978;&#23384;&#22312;&#22256;&#38590;&#65292;&#36825;&#20123;&#25351;&#20196;&#21487;&#20197;&#26159;&#38656;&#35201;&#22810;&#20010;&#20219;&#21153;&#21644;&#32422;&#26463;&#30340;&#22797;&#26434;&#20219;&#21153;&#25551;&#36848;&#65292;&#25110;&#32773;&#21253;&#21547;&#38271;&#31687;&#32972;&#26223;&#12289;&#22122;&#22768;&#12289;&#24322;&#26500;&#20449;&#24687;&#21644;&#22810;&#36718;&#26684;&#24335;&#30340;&#22797;&#26434;&#36755;&#20837;&#12290;&#30001;&#20110;&#36825;&#20123;&#29305;&#28857;&#65292;LLMs&#24120;&#24120;&#24573;&#30053;&#20219;&#21153;&#25551;&#36848;&#20013;&#30340;&#35821;&#20041;&#32422;&#26463;&#65292;&#20135;&#29983;&#38169;&#35823;&#30340;&#26684;&#24335;&#65292;&#36829;&#21453;&#38271;&#24230;&#25110;&#26679;&#26412;&#35745;&#25968;&#30340;&#32422;&#26463;&#65292;&#23545;&#36755;&#20837;&#25991;&#26412;&#19981;&#24544;&#23454;&#12290;&#29616;&#26377;&#30340;&#22522;&#20934;&#19981;&#36275;&#20197;&#35780;&#20272;LLMs&#29702;&#35299;&#22797;&#26434;&#25351;&#20196;&#30340;&#33021;&#21147;&#65292;&#22240;&#20026;&#23427;&#20204;&#26159;&#23553;&#38381;&#24335;&#21644;&#31616;&#21333;&#30340;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#38388;&#38553;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CELLO&#65292;&#19968;&#20010;&#29992;&#20110;&#31995;&#32479;&#35780;&#20272;LLMs&#36981;&#24490;&#22797;&#26434;&#25351;&#20196;&#33021;&#21147;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#20026;&#22797;&#26434;&#25351;&#20196;&#35774;&#35745;&#20102;&#20843;&#20010;&#29305;&#24449;&#65292;&#24182;&#20174;&#29616;&#23454;&#22330;&#26223;&#20013;&#26500;&#24314;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;&#22235;&#20010;&#35780;&#20215;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) can understand human instructions, showing their potential for pragmatic applications beyond traditional NLP tasks. However, they still struggle with complex instructions, which can be either complex task descriptions that require multiple tasks and constraints, or complex input that contains long context, noise, heterogeneous information and multi-turn format. Due to these features, LLMs often ignore semantic constraints from task descriptions, generate incorrect formats, violate length or sample count constraints, and be unfaithful to the input text. Existing benchmarks are insufficient to assess LLMs' ability to understand complex instructions, as they are close-ended and simple. To bridge this gap, we propose CELLO, a benchmark for evaluating LLMs' ability to follow complex instructions systematically. We design eight features for complex instructions and construct a comprehensive evaluation dataset from real-world scenarios. We also establish four crit
&lt;/p&gt;</description></item><item><title>TextBind&#26159;&#19968;&#20010;&#27880;&#37322;&#26497;&#23569;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#36739;&#22823;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#36171;&#20104;&#22810;&#36718;&#20132;&#38169;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#22270;&#20687;-&#26631;&#39064;&#23545;&#29983;&#25104;&#22810;&#36718;&#22810;&#27169;&#24577;&#25351;&#20196;-&#22238;&#24212;&#23545;&#35805;&#12290;&#36825;&#20010;&#26694;&#26550;&#23545;&#20110;&#35299;&#20915;&#23454;&#38469;&#20219;&#21153;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#28436;&#31034;&#12290;</title><link>http://arxiv.org/abs/2309.08637</link><description>&lt;p&gt;
TextBind: &#22810;&#36718;&#20132;&#38169;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;
&lt;/p&gt;
&lt;p&gt;
TextBind: Multi-turn Interleaved Multimodal Instruction-following. (arXiv:2309.08637v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08637
&lt;/p&gt;
&lt;p&gt;
TextBind&#26159;&#19968;&#20010;&#27880;&#37322;&#26497;&#23569;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#36739;&#22823;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#36171;&#20104;&#22810;&#36718;&#20132;&#38169;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#22270;&#20687;-&#26631;&#39064;&#23545;&#29983;&#25104;&#22810;&#36718;&#22810;&#27169;&#24577;&#25351;&#20196;-&#22238;&#24212;&#23545;&#35805;&#12290;&#36825;&#20010;&#26694;&#26550;&#23545;&#20110;&#35299;&#20915;&#23454;&#38469;&#20219;&#21153;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20135;&#29983;&#20102;&#38761;&#21629;&#24615;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#20854;&#33258;&#28982;&#35821;&#35328;&#30028;&#38754;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21487;&#20197;&#35299;&#20915;&#21508;&#31181;&#23454;&#38469;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#39640;&#36136;&#37327;&#30340;&#31034;&#20363;&#25968;&#25454;&#65292;&#32780;&#36825;&#24448;&#24448;&#24456;&#38590;&#33719;&#24471;&#12290;&#24403;&#28041;&#21450;&#21040;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;&#26102;&#65292;&#36825;&#20010;&#25361;&#25112;&#21464;&#24471;&#26356;&#21152;&#20005;&#23803;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;TextBind&#65292;&#36825;&#26159;&#19968;&#20010;&#20960;&#20046;&#19981;&#38656;&#35201;&#27880;&#37322;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#36171;&#20104;&#36739;&#22823;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#22810;&#36718;&#20132;&#38169;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#38656;&#35201;&#22270;&#20687;-&#26631;&#39064;&#23545;&#65292;&#24182;&#20174;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22810;&#36718;&#22810;&#27169;&#24577;&#25351;&#20196;-&#22238;&#24212;&#23545;&#35805;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#28436;&#31034;&#65292;&#20197;&#20419;&#36827;&#26410;&#26469;&#22312;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;&#39046;&#22495;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models with instruction-following abilities have revolutionized the field of artificial intelligence. These models show exceptional generalizability to tackle various real-world tasks through their natural language interfaces. However, their performance heavily relies on high-quality exemplar data, which is often difficult to obtain. This challenge is further exacerbated when it comes to multimodal instruction following. We introduce TextBind, an almost annotation-free framework for empowering larger language models with the multi-turn interleaved multimodal instruction-following capabilities. Our approach requires only image-caption pairs and generates multi-turn multimodal instruction-response conversations from a language model. We release our dataset, model, and demo to foster future research in the area of multimodal instruction following.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#31034;&#31574;&#30053;&#65292;&#26412;&#35770;&#25991;&#25506;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#24212;&#29992;&#35282;&#33394;&#25198;&#28436;&#21644;&#24605;&#32500;&#38142;&#25552;&#31034;&#31574;&#30053;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#19977;&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2309.02045</link><description>&lt;p&gt;
&#36890;&#36807;&#25552;&#31034;&#31574;&#30053;&#22686;&#24378;&#35780;&#35770;&#25991;&#26412;&#30340;&#22810;&#39046;&#22495;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Enhance Multi-domain Sentiment Analysis of Review Texts through Prompting Strategies. (arXiv:2309.02045v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02045
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#31034;&#31574;&#30053;&#65292;&#26412;&#35770;&#25991;&#25506;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#24212;&#29992;&#35282;&#33394;&#25198;&#28436;&#21644;&#24605;&#32500;&#38142;&#25552;&#31034;&#31574;&#30053;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#19977;&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#31185;&#23398;&#30740;&#31350;&#21644;&#23454;&#38469;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#29616;&#26377;&#30740;&#31350;&#24050;&#35777;&#26126;&#20102;LLMs&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#36890;&#36807;&#25552;&#31034;&#31574;&#30053;&#36827;&#19968;&#27493;&#22686;&#24378;LLMs&#22312;&#29305;&#23450;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#36890;&#36807;&#24212;&#29992;&#25552;&#31034;&#31574;&#30053;&#26469;&#25552;&#39640;LLMs&#22312;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#30340;&#25552;&#31034;&#36807;&#31243;&#36827;&#34892;&#20102;&#24314;&#27169;&#65292;&#24182;&#20171;&#32461;&#20102;&#20004;&#31181;&#38024;&#23545;&#24773;&#24863;&#20998;&#26512;&#30340;&#26032;&#39062;&#31574;&#30053;&#65306;&#35282;&#33394;&#25198;&#28436;&#65288;RP&#65289;&#25552;&#31034;&#21644;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#25552;&#31034;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;RP-CoT&#25552;&#31034;&#31574;&#30053;&#65292;&#23427;&#26159;RP&#25552;&#31034;&#21644;CoT&#25552;&#31034;&#30340;&#32467;&#21512;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27604;&#36739;&#23454;&#39564;&#65292;&#20197;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;&#24773;&#24863;&#20998;&#26512;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have made significant strides in both scientific research and practical applications. Existing studies have demonstrated the state-of-the-art (SOTA) performance of LLMs in various natural language processing tasks. However, the question of how to further enhance LLMs' performance in specific task using prompting strategies remains a pivotal concern. This paper explores the enhancement of LLMs' performance in sentiment analysis through the application of prompting strategies. We formulate the process of prompting for sentiment analysis tasks and introduce two novel strategies tailored for sentiment analysis: RolePlaying (RP) prompting and Chain-of-thought (CoT) prompting. Specifically, we also propose the RP-CoT prompting strategy which is a combination of RP prompting and CoT prompting. We conduct comparative experiments on three distinct domain datasets to evaluate the effectiveness of the proposed sentiment analysis strategies. The results demonstrate tha
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26159;&#39318;&#20010;&#22823;&#35268;&#27169;&#30340;&#35760;&#24518;&#24615;&#30740;&#31350;&#65292;&#21457;&#29616;&#24191;&#21578;&#30340;&#38271;&#26399;&#35760;&#24518;&#24615;&#23545;&#20110;&#24066;&#22330;&#33829;&#38144;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#22312;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#20013;&#19968;&#30452;&#32570;&#20047;&#30456;&#20851;&#30740;&#31350;&#12290;&#36890;&#36807;&#20998;&#26512;&#22823;&#37327;&#21442;&#19982;&#32773;&#21644;&#24191;&#21578;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#20851;&#20110;&#20160;&#20040;&#20351;&#24191;&#21578;&#35760;&#24518;&#28145;&#21051;&#30340;&#26377;&#36259;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2309.00378</link><description>&lt;p&gt;
&#24191;&#21578;&#30340;&#38271;&#26399;&#35760;&#24518;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Long-Term Memorability On Advertisements. (arXiv:2309.00378v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00378
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26159;&#39318;&#20010;&#22823;&#35268;&#27169;&#30340;&#35760;&#24518;&#24615;&#30740;&#31350;&#65292;&#21457;&#29616;&#24191;&#21578;&#30340;&#38271;&#26399;&#35760;&#24518;&#24615;&#23545;&#20110;&#24066;&#22330;&#33829;&#38144;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#22312;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#20013;&#19968;&#30452;&#32570;&#20047;&#30456;&#20851;&#30740;&#31350;&#12290;&#36890;&#36807;&#20998;&#26512;&#22823;&#37327;&#21442;&#19982;&#32773;&#21644;&#24191;&#21578;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#20851;&#20110;&#20160;&#20040;&#20351;&#24191;&#21578;&#35760;&#24518;&#28145;&#21051;&#30340;&#26377;&#36259;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24066;&#22330;&#33829;&#38144;&#20154;&#21592;&#33457;&#36153;&#25968;&#21313;&#20159;&#32654;&#20803;&#22312;&#24191;&#21578;&#19978;&#65292;&#20294;&#26159;&#25237;&#20837;&#21040;&#24191;&#21578;&#19978;&#30340;&#37329;&#38065;&#33021;&#36215;&#22810;&#22823;&#20316;&#29992;&#21602;&#65311;&#24403;&#39038;&#23458;&#22312;&#36141;&#20080;&#26102;&#26080;&#27861;&#36776;&#35748;&#20986;&#20182;&#20204;&#30475;&#36807;&#30340;&#21697;&#29260;&#30340;&#35805;&#65292;&#33457;&#22312;&#24191;&#21578;&#19978;&#30340;&#38065;&#22522;&#26412;&#19978;&#23601;&#34987;&#28010;&#36153;&#20102;&#12290;&#23613;&#31649;&#22312;&#33829;&#38144;&#20013;&#24456;&#37325;&#35201;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#30340;&#25991;&#29486;&#20013;&#36824;&#27809;&#26377;&#20851;&#20110;&#24191;&#21578;&#35760;&#24518;&#21147;&#30340;&#30740;&#31350;&#12290;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#26159;&#23545;&#29305;&#23450;&#20869;&#23481;&#31867;&#22411;&#65288;&#22914;&#29289;&#20307;&#21644;&#21160;&#20316;&#35270;&#39057;&#65289;&#36827;&#34892;&#30701;&#26399;&#22238;&#24518;&#65288;&lt;5&#20998;&#38047;&#65289;&#30340;&#30740;&#31350;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#24191;&#21578;&#34892;&#19994;&#21482;&#20851;&#24515;&#38271;&#26399;&#35760;&#24518;&#65288;&#20960;&#20010;&#23567;&#26102;&#25110;&#26356;&#38271;&#26102;&#38388;&#65289;&#65292;&#32780;&#19988;&#24191;&#21578;&#20960;&#20046;&#24635;&#26159;&#39640;&#24230;&#22810;&#27169;&#24335;&#21270;&#65292;&#36890;&#36807;&#19981;&#21516;&#30340;&#24418;&#24335;&#65288;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#35270;&#39057;&#65289;&#26469;&#35762;&#25925;&#20107;&#12290;&#22522;&#20110;&#36825;&#19968;&#21160;&#26426;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#39318;&#20010;&#22823;&#35268;&#27169;&#35760;&#24518;&#24615;&#30740;&#31350;&#65292;&#20849;&#26377;1203&#21517;&#21442;&#19982;&#32773;&#21644;2205&#20010;&#24191;&#21578;&#28085;&#30422;&#20102;276&#20010;&#21697;&#29260;&#12290;&#22312;&#19981;&#21516;&#21442;&#19982;&#32773;&#23376;&#32676;&#20307;&#21644;&#24191;&#21578;&#31867;&#22411;&#19978;&#36827;&#34892;&#32479;&#35745;&#27979;&#35797;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#35768;&#22810;&#26377;&#20851;&#20160;&#20040;&#20351;&#24191;&#21578;&#38590;&#24536;&#30340;&#26377;&#36259;&#35265;&#35299;-&#26080;&#35770;&#26159;&#20869;&#23481;&#36824;&#26159;
&lt;/p&gt;
&lt;p&gt;
Marketers spend billions of dollars on advertisements but to what end? At the purchase time, if customers cannot recognize a brand for which they saw an ad, the money spent on the ad is essentially wasted. Despite its importance in marketing, until now, there has been no study on the memorability of ads in the ML literature. Most studies have been conducted on short-term recall (&lt;5 mins) on specific content types like object and action videos. On the other hand, the advertising industry only cares about long-term memorability (a few hours or longer), and advertisements are almost always highly multimodal, depicting a story through its different modalities (text, images, and videos). With this motivation, we conduct the first large scale memorability study consisting of 1203 participants and 2205 ads covering 276 brands. Running statistical tests over different participant subpopulations and ad-types, we find many interesting insights into what makes an ad memorable - both content and h
&lt;/p&gt;</description></item><item><title>WavMark&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#38899;&#39057;&#27700;&#21360;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#30701;&#30701;1&#31186;&#30340;&#38899;&#39057;&#29255;&#27573;&#20013;&#32534;&#30721;&#22810;&#36798;32&#20301;&#30340;&#27700;&#21360;&#65292;&#23545;&#20154;&#31867;&#24863;&#23448;&#26080;&#24863;&#30693;&#24182;&#19988;&#20855;&#26377;&#24378;&#22823;&#30340;&#38887;&#24615;&#12290;&#23427;&#21487;&#20197;&#29992;&#20110;&#21512;&#25104;&#22768;&#38899;&#30340;&#26377;&#25928;&#35782;&#21035;&#21644;&#38899;&#39057;&#29256;&#26435;&#20445;&#25252;&#12290;</title><link>http://arxiv.org/abs/2308.12770</link><description>&lt;p&gt;
WavMark&#65306;&#29992;&#20110;&#38899;&#39057;&#29983;&#25104;&#30340;&#27700;&#21360;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
WavMark: Watermarking for Audio Generation. (arXiv:2308.12770v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12770
&lt;/p&gt;
&lt;p&gt;
WavMark&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#38899;&#39057;&#27700;&#21360;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#30701;&#30701;1&#31186;&#30340;&#38899;&#39057;&#29255;&#27573;&#20013;&#32534;&#30721;&#22810;&#36798;32&#20301;&#30340;&#27700;&#21360;&#65292;&#23545;&#20154;&#31867;&#24863;&#23448;&#26080;&#24863;&#30693;&#24182;&#19988;&#20855;&#26377;&#24378;&#22823;&#30340;&#38887;&#24615;&#12290;&#23427;&#21487;&#20197;&#29992;&#20110;&#21512;&#25104;&#22768;&#38899;&#30340;&#26377;&#25928;&#35782;&#21035;&#21644;&#38899;&#39057;&#29256;&#26435;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#38646;-shot&#35821;&#38899;&#21512;&#25104;&#26041;&#38754;&#30340;&#31361;&#30772;&#20351;&#24471;&#21482;&#29992;&#20960;&#31186;&#38047;&#30340;&#24405;&#38899;&#23601;&#33021;&#27169;&#20223;&#35828;&#35805;&#32773;&#30340;&#22768;&#38899;&#65292;&#24182;&#19988;&#20445;&#25345;&#39640;&#24230;&#30340;&#30495;&#23454;&#24863;&#12290;&#38500;&#20102;&#28508;&#22312;&#30340;&#22909;&#22788;&#20043;&#22806;&#65292;&#36825;&#39033;&#24378;&#22823;&#30340;&#25216;&#26415;&#36824;&#24102;&#26469;&#20102;&#26126;&#26174;&#30340;&#39118;&#38505;&#65292;&#21253;&#25324;&#35821;&#38899;&#27450;&#35784;&#21644;&#20882;&#20805;&#35828;&#35805;&#32773;&#12290;&#19982;&#20165;&#20381;&#36182;&#34987;&#21160;&#26041;&#27861;&#26469;&#26816;&#27979;&#21512;&#25104;&#25968;&#25454;&#30340;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#27700;&#21360;&#25216;&#26415;&#25552;&#20379;&#20102;&#19968;&#31181;&#31215;&#26497;&#19988;&#24378;&#22823;&#30340;&#38450;&#24481;&#26426;&#21046;&#26469;&#24212;&#23545;&#36825;&#20123;&#28508;&#22312;&#39118;&#38505;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#38899;&#39057;&#27700;&#21360;&#25216;&#26415;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#20165;1&#31186;&#30340;&#38899;&#39057;&#29255;&#27573;&#20013;&#32534;&#30721;&#22810;&#36798;32&#20301;&#30340;&#27700;&#21360;&#12290;&#27700;&#21360;&#23545;&#20154;&#31867;&#24863;&#23448;&#26469;&#35828;&#26159;&#26080;&#27861;&#23519;&#35273;&#30340;&#65292;&#24182;&#19988;&#23545;&#21508;&#31181;&#25915;&#20987;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#38887;&#24615;&#12290;&#23427;&#21487;&#20197;&#20316;&#20026;&#21512;&#25104;&#22768;&#38899;&#30340;&#26377;&#25928;&#26631;&#35782;&#31526;&#65292;&#24182;&#22312;&#38899;&#39057;&#29256;&#26435;&#20445;&#25252;&#30340;&#26356;&#24191;&#27867;&#24212;&#29992;&#20013;&#20855;&#26377;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#36825;&#20010;&#26694;&#26550;&#20855;&#26377;&#24456;&#39640;&#30340;&#28789;&#27963;&#24615;&#65292;&#21487;&#20197;&#23558;&#22810;&#20010;&#27700;&#21360;&#29255;&#27573;&#36827;&#34892;&#32452;&#21512;&#20197;&#23454;&#29616;&#26356;&#21152;&#20016;&#23500;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent breakthroughs in zero-shot voice synthesis have enabled imitating a speaker's voice using just a few seconds of recording while maintaining a high level of realism. Alongside its potential benefits, this powerful technology introduces notable risks, including voice fraud and speaker impersonation. Unlike the conventional approach of solely relying on passive methods for detecting synthetic data, watermarking presents a proactive and robust defence mechanism against these looming risks. This paper introduces an innovative audio watermarking framework that encodes up to 32 bits of watermark within a mere 1-second audio snippet. The watermark is imperceptible to human senses and exhibits strong resilience against various attacks. It can serve as an effective identifier for synthesized voices and holds potential for broader applications in audio copyright protection. Moreover, this framework boasts high flexibility, allowing for the combination of multiple watermark segments to achi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#21333;&#22768;&#36947;&#35821;&#38899;&#22686;&#24378;&#27169;&#22359;&#19982;ASR&#27169;&#22359;&#65292;&#25104;&#21151;&#23558;ASR&#30340;&#35789;&#38169;&#35823;&#29575;&#20174;80%&#38477;&#20302;&#21040;26.4%&#65292;&#24182;&#36890;&#36807;&#32852;&#21512;&#24494;&#35843;&#31574;&#30053;&#23558;&#20854;&#36827;&#19968;&#27493;&#38477;&#20302;&#21040;14.5%&#12290;</title><link>http://arxiv.org/abs/2308.11380</link><description>&lt;p&gt;
Convoifilter: &#40481;&#23614;&#37202;&#20250;&#35821;&#38899;&#35782;&#21035;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Convoifilter: A case study of doing cocktail party speech recognition. (arXiv:2308.11380v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#21333;&#22768;&#36947;&#35821;&#38899;&#22686;&#24378;&#27169;&#22359;&#19982;ASR&#27169;&#22359;&#65292;&#25104;&#21151;&#23558;ASR&#30340;&#35789;&#38169;&#35823;&#29575;&#20174;80%&#38477;&#20302;&#21040;26.4%&#65292;&#24182;&#36890;&#36807;&#32852;&#21512;&#24494;&#35843;&#31574;&#30053;&#23558;&#20854;&#36827;&#19968;&#27493;&#38477;&#20302;&#21040;14.5%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#25913;&#36827;&#25317;&#25380;&#12289;&#22024;&#26434;&#29615;&#22659;&#19979;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#65292;&#38024;&#23545;&#29305;&#23450;&#35828;&#35805;&#32773;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#21333;&#22768;&#36947;&#35821;&#38899;&#22686;&#24378;&#27169;&#22359;&#23558;&#35828;&#35805;&#32773;&#30340;&#22768;&#38899;&#19982;&#32972;&#26223;&#22122;&#22768;&#20998;&#31163;&#65292;&#32467;&#21512;ASR&#27169;&#22359;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#23558;ASR&#30340;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#20174;80%&#38477;&#20302;&#21040;26.4%&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#30001;&#20110;&#25968;&#25454;&#35201;&#27714;&#30340;&#21464;&#21270;&#65292;&#36825;&#20004;&#20010;&#32452;&#20214;&#20250;&#29420;&#31435;&#35843;&#25972;&#12290;&#28982;&#32780;&#65292;&#35821;&#38899;&#22686;&#24378;&#21487;&#33021;&#20250;&#23548;&#33268;ASR&#25928;&#29575;&#19979;&#38477;&#12290;&#36890;&#36807;&#23454;&#26045;&#32852;&#21512;&#24494;&#35843;&#31574;&#30053;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#23558;&#20998;&#21035;&#35843;&#25972;&#30340;WER&#20174;26.4%&#38477;&#20302;&#21040;14.5%&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an end-to-end model designed to improve automatic speech recognition (ASR) for a particular speaker in a crowded, noisy environment. The model utilizes a single-channel speech enhancement module that isolates the speaker's voice from background noise, along with an ASR module. Through this approach, the model is able to decrease the word error rate (WER) of ASR from 80% to 26.4%. Typically, these two components are adjusted independently due to variations in data requirements. However, speech enhancement can create anomalies that decrease ASR efficiency. By implementing a joint fine-tuning strategy, the model can reduce the WER from 26.4% in separate tuning to 14.5% in joint tuning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38544;&#21947;&#35782;&#21035;&#20219;&#21153;&#20013;&#30693;&#35782;&#27880;&#20837;&#30340;&#30740;&#31350;&#36827;&#23637;&#36827;&#34892;&#20102;&#20840;&#38754;&#32508;&#36848;&#65292;&#21253;&#25324;&#20027;&#27969;&#30693;&#35782;&#21644;&#30693;&#35782;&#27880;&#20837;&#21407;&#21017;&#30340;&#24635;&#32467;&#12289;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#22522;&#20934;&#27169;&#22411;&#30340;&#22238;&#39038;&#65292;&#24182;&#25506;&#35752;&#20102;&#24403;&#21069;&#30340;&#30693;&#35782;&#27880;&#20837;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.04306</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38544;&#21947;&#26816;&#27979;&#30693;&#35782;&#27880;&#20837;&#65306;&#32508;&#36848;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Deep Learning-Based Knowledge Injection for Metaphor Detection: A Comprehensive Review. (arXiv:2308.04306v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38544;&#21947;&#35782;&#21035;&#20219;&#21153;&#20013;&#30693;&#35782;&#27880;&#20837;&#30340;&#30740;&#31350;&#36827;&#23637;&#36827;&#34892;&#20102;&#20840;&#38754;&#32508;&#36848;&#65292;&#21253;&#25324;&#20027;&#27969;&#30693;&#35782;&#21644;&#30693;&#35782;&#27880;&#20837;&#21407;&#21017;&#30340;&#24635;&#32467;&#12289;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#22522;&#20934;&#27169;&#22411;&#30340;&#22238;&#39038;&#65292;&#24182;&#25506;&#35752;&#20102;&#24403;&#21069;&#30340;&#30693;&#35782;&#27880;&#20837;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#21947;&#30740;&#31350;&#30340;&#21382;&#21490;&#20063;&#26631;&#24535;&#30528;&#30693;&#35782;&#27880;&#20837;&#30740;&#31350;&#30340;&#28436;&#21464;&#12290;&#38543;&#30528;&#36817;&#24180;&#26469;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#19981;&#26029;&#36827;&#27493;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31038;&#21306;&#23545;&#23558;&#30693;&#35782;&#24212;&#29992;&#20110;&#22312;&#38544;&#21947;&#35782;&#21035;&#20219;&#21153;&#20013;&#21462;&#24471;&#25104;&#21151;&#32467;&#26524;&#34920;&#29616;&#20986;&#26497;&#22823;&#20852;&#36259;&#12290;&#23613;&#31649;&#22312;&#38544;&#21947;&#35782;&#21035;&#39046;&#22495;&#28041;&#21450;&#30693;&#35782;&#27880;&#20837;&#30340;&#26041;&#27861;&#36880;&#28176;&#22686;&#21152;&#65292;&#20294;&#32570;&#20047;&#19968;&#31687;&#23436;&#25972;&#30340;&#20851;&#20110;&#22522;&#20110;&#30693;&#35782;&#27880;&#20837;&#30340;&#26041;&#27861;&#30340;&#32508;&#36848;&#25991;&#31456;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#26088;&#22312;&#32508;&#36848;&#28145;&#24230;&#23398;&#20064;&#22312;&#38544;&#21947;&#35782;&#21035;&#20219;&#21153;&#20013;&#24212;&#29992;&#30693;&#35782;&#27880;&#20837;&#30340;&#30740;&#31350;&#36827;&#23637;&#12290;&#26412;&#25991;&#31995;&#32479;&#24635;&#32467;&#21644;&#27010;&#25324;&#20102;&#20027;&#27969;&#30340;&#30693;&#35782;&#21644;&#30693;&#35782;&#27880;&#20837;&#21407;&#21017;&#65292;&#21516;&#26102;&#22238;&#39038;&#20102;&#22312;&#38544;&#21947;&#35782;&#21035;&#20219;&#21153;&#20013;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#22522;&#20934;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#24403;&#21069;&#38754;&#20020;&#30340;&#30693;&#35782;&#27880;&#20837;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The history of metaphor research also marks the evolution of knowledge infusion research. With the continued advancement of deep learning techniques in recent years, the natural language processing community has shown great interest in applying knowledge to successful results in metaphor recognition tasks. Although there has been a gradual increase in the number of approaches involving knowledge injection in the field of metaphor recognition, there is a lack of a complete review article on knowledge injection based approaches. Therefore, the goal of this paper is to provide a comprehensive review of research advances in the application of deep learning for knowledge injection in metaphor recognition tasks. In this paper, we systematically summarize and generalize the mainstream knowledge and knowledge injection principles, as well as review the datasets, evaluation metrics, and benchmark models used in metaphor recognition tasks. Finally, we explore the current issues facing knowledge 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#26684;&#24335;&#19968;&#33268;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#32479;&#19968;&#25351;&#20196;&#35843;&#25972;&#65288;UIT&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#21160;&#26684;&#24335;&#36716;&#25442;&#26469;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;&#35813;&#30740;&#31350;&#24378;&#35843;&#20102;&#26684;&#24335;&#19968;&#33268;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.15504</link><description>&lt;p&gt;
&#25506;&#32034;&#25351;&#20196;&#35843;&#25972;&#30340;&#26684;&#24335;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploring Format Consistency for Instruction Tuning. (arXiv:2307.15504v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15504
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#26684;&#24335;&#19968;&#33268;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#32479;&#19968;&#25351;&#20196;&#35843;&#25972;&#65288;UIT&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#21160;&#26684;&#24335;&#36716;&#25442;&#26469;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;&#35813;&#30740;&#31350;&#24378;&#35843;&#20102;&#26684;&#24335;&#19968;&#33268;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36981;&#24490;&#20154;&#31867;&#25351;&#20196;&#33021;&#21147;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#20013;&#25351;&#20196;&#30340;&#22810;&#26679;&#24615;&#21644;&#25968;&#37327;&#21487;&#20197;&#25345;&#32493;&#25552;&#21319;&#27867;&#21270;&#24615;&#33021;&#65292;&#20174;&#32780;&#20419;&#36827;&#20102;&#26368;&#36817;&#30340;&#19968;&#39033;&#21162;&#21147;&#65292;&#21363;&#25910;&#38598;&#21508;&#31181;&#25351;&#20196;&#24182;&#23558;&#29616;&#26377;&#30340;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#25972;&#21512;&#21040;&#26356;&#22823;&#30340;&#38598;&#21512;&#20013;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#29992;&#25143;&#26377;&#20854;&#29420;&#29305;&#30340;&#34920;&#36798;&#25351;&#20196;&#30340;&#26041;&#24335;&#65292;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#36890;&#24120;&#23384;&#22312;&#25351;&#20196;&#39118;&#26684;&#21644;&#26684;&#24335;&#30340;&#21464;&#21270;&#65292;&#21363;&#26684;&#24335;&#19981;&#19968;&#33268;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26684;&#24335;&#19981;&#19968;&#33268;&#24615;&#22914;&#20309;&#24433;&#21709;&#25351;&#20196;&#35843;&#25972;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#32479;&#19968;&#25351;&#20196;&#35843;&#25972;&#8221;&#65288;UIT&#65289;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35843;&#29992;OpenAI&#30340;API&#23454;&#29616;&#22312;&#19981;&#21516;&#30340;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#33258;&#21160;&#26684;&#24335;&#36716;&#25442;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;UIT&#25104;&#21151;&#25552;&#39640;&#20102;&#22312;&#26410;&#35265;&#25351;&#20196;&#19978;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#24378;&#35843;&#20102;&#26684;&#24335;&#19968;&#33268;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction tuning has emerged as a promising approach to enhancing large language models in following human instructions. It is shown that increasing the diversity and number of instructions in the training data can consistently enhance generalization performance, which facilitates a recent endeavor to collect various instructions and integrate existing instruction tuning datasets into larger collections. However, different users have their unique ways of expressing instructions, and there often exist variations across different datasets in the instruction styles and formats, i.e., format inconsistency. In this work, we study how format inconsistency may impact the performance of instruction tuning. We propose a framework called "Unified Instruction Tuning" (UIT), which calls OpenAI APIs for automatic format transfer among different instruction tuning datasets. We show that UIT successfully improves the generalization performance on unseen instructions, which highlights the importance
&lt;/p&gt;</description></item><item><title>&#22810;&#27169;&#24577;&#35752;&#35770;&#21464;&#25442;&#22120; (mDT) &#26159;&#19968;&#20010;&#29992;&#20110;&#26816;&#27979;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#20013;&#20167;&#24680;&#35328;&#35770;&#30340;&#26032;&#39062;&#27169;&#22411;&#12290;&#19982;&#20256;&#32479;&#30340;&#20165;&#20351;&#29992;&#25991;&#26412;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;mDT&#36890;&#36807;&#25972;&#20307;&#20998;&#26512;&#25991;&#26412;&#21644;&#22270;&#20687;&#65292;&#32467;&#21512;&#22270;&#21464;&#25442;&#22120;&#25429;&#25417;&#35780;&#35770;&#21608;&#22260;&#25972;&#20010;&#35752;&#35770;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#20132;&#32455;&#34701;&#21512;&#23618;&#23558;&#25991;&#26412;&#21644;&#22270;&#20687;&#23884;&#20837;&#36827;&#34892;&#32452;&#21512;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#25429;&#25417;&#23545;&#35805;&#30340;&#25972;&#20307;&#35270;&#22270;&#21487;&#20197;&#26497;&#22823;&#22320;&#25552;&#39640;&#26816;&#27979;&#21453;&#31038;&#20250;&#34892;&#20026;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09312</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#35752;&#35770;&#21464;&#25442;&#22120;&#65306;&#25972;&#21512;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#22270;&#21464;&#25442;&#22120;&#20197;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20167;&#24680;&#35328;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Modal Discussion Transformer: Integrating Text, Images and Graph Transformers to Detect Hate Speech on Social Media. (arXiv:2307.09312v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09312
&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#35752;&#35770;&#21464;&#25442;&#22120; (mDT) &#26159;&#19968;&#20010;&#29992;&#20110;&#26816;&#27979;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#20013;&#20167;&#24680;&#35328;&#35770;&#30340;&#26032;&#39062;&#27169;&#22411;&#12290;&#19982;&#20256;&#32479;&#30340;&#20165;&#20351;&#29992;&#25991;&#26412;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;mDT&#36890;&#36807;&#25972;&#20307;&#20998;&#26512;&#25991;&#26412;&#21644;&#22270;&#20687;&#65292;&#32467;&#21512;&#22270;&#21464;&#25442;&#22120;&#25429;&#25417;&#35780;&#35770;&#21608;&#22260;&#25972;&#20010;&#35752;&#35770;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#20132;&#32455;&#34701;&#21512;&#23618;&#23558;&#25991;&#26412;&#21644;&#22270;&#20687;&#23884;&#20837;&#36827;&#34892;&#32452;&#21512;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#25429;&#25417;&#23545;&#35805;&#30340;&#25972;&#20307;&#35270;&#22270;&#21487;&#20197;&#26497;&#22823;&#22320;&#25552;&#39640;&#26816;&#27979;&#21453;&#31038;&#20250;&#34892;&#20026;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#22522;&#20110;&#22270;&#30340;&#21464;&#25442;&#22120;&#27169;&#22411;&#65292;&#21517;&#20026;&#22810;&#27169;&#24577;&#35752;&#35770;&#21464;&#25442;&#22120;&#65288;mDT&#65289;&#65292;&#29992;&#20110;&#26816;&#27979;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#20167;&#24680;&#35328;&#35770;&#12290;&#19982;&#20256;&#32479;&#30340;&#20165;&#20351;&#29992;&#25991;&#26412;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#23558;&#26631;&#35760;&#35780;&#35770;&#20026;&#20167;&#24680;&#35328;&#35770;&#30340;&#26041;&#27861;&#22260;&#32469;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#25972;&#20307;&#20998;&#26512;&#23637;&#24320;&#12290;&#36825;&#26159;&#36890;&#36807;&#21033;&#29992;&#22270;&#21464;&#25442;&#22120;&#26469;&#25429;&#25417;&#35780;&#35770;&#21608;&#22260;&#25972;&#20010;&#35752;&#35770;&#20013;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#65292;&#24182;&#37319;&#29992;&#20132;&#32455;&#34701;&#21512;&#23618;&#26469;&#32452;&#21512;&#25991;&#26412;&#21644;&#22270;&#20687;&#23884;&#20837;&#65292;&#32780;&#19981;&#26159;&#21333;&#29420;&#22788;&#29702;&#19981;&#21516;&#30340;&#27169;&#24577;&#12290;&#25105;&#20204;&#23558;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#20165;&#22788;&#29702;&#25991;&#26412;&#30340;&#22522;&#32447;&#36827;&#34892;&#27604;&#36739;&#65292;&#36824;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#28040;&#34701;&#30740;&#31350;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#26395;&#20102;&#22810;&#27169;&#24577;&#35299;&#20915;&#26041;&#26696;&#22312;&#22312;&#32447;&#29615;&#22659;&#20013;&#25552;&#20379;&#31038;&#20250;&#20215;&#20540;&#30340;&#26410;&#26469;&#24037;&#20316;&#65292;&#24182;&#35748;&#20026;&#25429;&#25417;&#23545;&#35805;&#30340;&#25972;&#20307;&#35270;&#22270;&#26497;&#22823;&#22320;&#25512;&#36827;&#20102;&#26816;&#27979;&#21453;&#31038;&#20250;&#34892;&#20026;&#30340;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the Multi-Modal Discussion Transformer (mDT), a novel multi-modal graph-based transformer model for detecting hate speech in online social networks. In contrast to traditional text-only methods, our approach to labelling a comment as hate speech centers around the holistic analysis of text and images. This is done by leveraging graph transformers to capture the contextual relationships in the entire discussion that surrounds a comment, with interwoven fusion layers to combine text and image embeddings instead of processing different modalities separately. We compare the performance of our model to baselines that only process text; we also conduct extensive ablation studies. We conclude with future work for multimodal solutions to deliver social value in online contexts, arguing that capturing a holistic view of a conversation greatly advances the effort to detect anti-social behavior.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#19977;&#27493;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#29616;&#26377;&#26694;&#26550;&#20013;&#22686;&#21152;&#19968;&#27493;&#35821;&#20041;&#30693;&#35782;&#33976;&#39311;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#22686;&#24378;&#20102;&#33258;&#21160;&#35821;&#38899;&#32763;&#35793;&#20013;&#20174;&#39640;&#36164;&#28304;&#35821;&#35328;&#21040;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#33021;&#21147;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#32763;&#35793;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#24182;&#20943;&#23569;&#20102;&#36328;&#35821;&#35328;&#36801;&#31227;&#38388;&#38553;(TRFGap)&#12290;</title><link>http://arxiv.org/abs/2306.00789</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#35821;&#38899;&#32763;&#35793;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cross-Lingual Transfer Learning for Low-Resource Speech Translation. (arXiv:2306.00789v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00789
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#19977;&#27493;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#29616;&#26377;&#26694;&#26550;&#20013;&#22686;&#21152;&#19968;&#27493;&#35821;&#20041;&#30693;&#35782;&#33976;&#39311;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#22686;&#24378;&#20102;&#33258;&#21160;&#35821;&#38899;&#32763;&#35793;&#20013;&#20174;&#39640;&#36164;&#28304;&#35821;&#35328;&#21040;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#33021;&#21147;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#32763;&#35793;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#24182;&#20943;&#23569;&#20102;&#36328;&#35821;&#35328;&#36801;&#31227;&#38388;&#38553;(TRFGap)&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19977;&#27493;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#33258;&#21160;&#35821;&#38899;&#32763;&#35793;&#20013;&#20174;&#39640;&#36164;&#28304;&#35821;&#35328;&#21040;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#23558;&#35821;&#20041;&#30693;&#35782;&#33976;&#39311;&#27493;&#39588;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;&#20004;&#27493;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;XLS-R&#20013;&#12290;&#36825;&#19968;&#39069;&#22806;&#30340;&#27493;&#39588;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#26080;&#26631;&#31614;&#35821;&#38899;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#26469;&#23545;&#22810;&#35821;&#35328;&#35821;&#38899;&#32534;&#30721;&#22120;&#36827;&#34892;&#39044;&#35757;&#32451;&#20197;&#32534;&#30721;&#35821;&#20041;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#19977;&#27493;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#35299;&#20915;&#20102;XLS-R&#26694;&#26550;&#20013;&#39640;&#36164;&#28304;&#35821;&#35328;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#20043;&#38388;&#23384;&#22312;&#30340;&#22823;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#24046;&#36317;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;CoVoST-2&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#21644;&#27604;&#36739;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#25552;&#35758;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;&#32763;&#35793;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#24182;&#19988;&#36328;&#35821;&#35328;&#36801;&#31227;&#38388;&#38553;(TRFGap)&#26377;&#26126;&#26174;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper presents a novel three-step transfer learning framework for enhancing cross-lingual transfer from high- to low-resource languages in the downstream application of Automatic Speech Translation. The approach integrates a semantic knowledge-distillation step into the existing two-step cross-lingual transfer learning framework XLS-R. This extra step aims to encode semantic knowledge in the multilingual speech encoder pre-trained via Self-Supervised Learning using unlabeled speech. Our proposed three-step cross-lingual transfer learning framework addresses the large cross-lingual transfer gap (TRFGap) observed in the XLS-R framework between high-resource and low-resource languages. We validate our proposal through extensive experiments and comparisons on the CoVoST-2 benchmark, showing significant improvements in translation performance, especially for low-resource languages, and a notable reduction in the TRFGap.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AlpacaFarm&#30340;&#20302;&#25104;&#26412;&#27169;&#25311;&#22120;&#65292;&#35813;&#27169;&#25311;&#22120;&#20026;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#35774;&#35745;LLM&#25552;&#31034;&#26469;&#27169;&#25311;&#20154;&#31867;&#21453;&#39304;&#65292;&#25552;&#20986;&#33258;&#21160;&#35780;&#20272;&#24182;&#25552;&#20379;&#21442;&#32771;&#23454;&#29616;&#65292;&#20811;&#26381;&#20102;&#25968;&#25454;&#25910;&#38598;&#30340;&#39640;&#26114;&#25104;&#26412;&#12289;&#32570;&#20047;&#21487;&#20449;&#30340;&#35780;&#20272;&#21644;&#32570;&#20047;&#21442;&#32771;&#26041;&#27861;&#23454;&#29616;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.14387</link><description>&lt;p&gt;
AlpacaFarm: &#19968;&#31181;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#27169;&#25311;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback. (arXiv:2305.14387v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14387
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AlpacaFarm&#30340;&#20302;&#25104;&#26412;&#27169;&#25311;&#22120;&#65292;&#35813;&#27169;&#25311;&#22120;&#20026;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#35774;&#35745;LLM&#25552;&#31034;&#26469;&#27169;&#25311;&#20154;&#31867;&#21453;&#39304;&#65292;&#25552;&#20986;&#33258;&#21160;&#35780;&#20272;&#24182;&#25552;&#20379;&#21442;&#32771;&#23454;&#29616;&#65292;&#20811;&#26381;&#20102;&#25968;&#25454;&#25910;&#38598;&#30340;&#39640;&#26114;&#25104;&#26412;&#12289;&#32570;&#20047;&#21487;&#20449;&#30340;&#35780;&#20272;&#21644;&#32570;&#20047;&#21442;&#32771;&#26041;&#27861;&#23454;&#29616;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#22240;&#20854;&#33391;&#22909;&#30340;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#32780;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#24320;&#21457;&#36825;&#20123;LLMs&#38656;&#35201;&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#35757;&#32451;&#30340;&#22797;&#26434;&#19988;&#23578;&#19981;&#26126;&#30830;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;&#23558;&#27492;&#25351;&#20196;&#36319;&#38543;&#36807;&#31243;&#22797;&#21046;&#21644;&#29702;&#35299;&#38754;&#20020;&#19977;&#22823;&#25361;&#25112;&#65306; &#25968;&#25454;&#25910;&#38598;&#30340;&#39640;&#26114;&#25104;&#26412;&#65292;&#32570;&#20047;&#21487;&#20449;&#30340;&#35780;&#20272;&#21644;&#32570;&#20047;&#21442;&#32771;&#26041;&#27861;&#23454;&#29616;&#12290;&#25105;&#20204;&#36890;&#36807;AlpacaFarm&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#36825;&#26159;&#19968;&#20010;&#20302;&#25104;&#26412;&#30340;&#27169;&#25311;&#22120;&#65292;&#21487;&#29992;&#20110;&#20174;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#12290;&#31532;&#19968;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;LLM&#25552;&#31034;&#26469;&#27169;&#25311;&#20154;&#31867;&#21453;&#39304;&#65292;&#20854;&#25104;&#26412;&#27604;&#20247;&#21253;&#24037;&#20316;&#32773;&#20415;&#23452;45&#20493;&#65292;&#24182;&#19988;&#19982;&#20154;&#31867;&#21453;&#39304;&#20855;&#26377;&#39640;&#24230;&#19968;&#33268;&#24615;&#12290;&#31532;&#20108;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#30495;&#23454;&#19990;&#30028;&#20132;&#20114;&#20013;&#33719;&#24471;&#30340;&#20154;&#31867;&#25351;&#20196;&#36827;&#34892;&#39564;&#35777;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#20026;&#20960;&#31181;&#20174;&#37197;&#23545;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#65288;PPO&#65292;best-of-n&#65292;expert iteration&#31561;&#65289;&#25552;&#20379;&#20102;&#21442;&#32771;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) such as ChatGPT have seen widespread adoption due to their ability to follow user instructions well. Developing these LLMs involves a complex yet poorly understood workflow requiring training with human feedback. Replicating and understanding this instruction-following process faces three major challenges: the high cost of data collection, the lack of trustworthy evaluation, and the absence of reference method implementations. We address these challenges with AlpacaFarm, a simulator that enables research and development for learning from feedback at a low cost. First, we design LLM prompts to simulate human feedback that are 45x cheaper than crowdworkers and display high agreement with humans. Second, we propose an automatic evaluation and validate it against human instructions obtained on real-world interactions. Third, we contribute reference implementations for several methods (PPO, best-of-n, expert iteration, and more) that learn from pairwise feedback
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#33258;&#28982;&#35821;&#35328;&#22270;&#24418;(NLGraph)&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20110;&#22270;&#24418;&#38382;&#39064;&#35299;&#20915;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;LLM&#22312;&#25991;&#26412;&#25551;&#36848;&#30340;&#22270;&#24418;&#32467;&#26500;&#21644;&#22270;&#24418;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#30340;&#22788;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LLM(GPT-3/4)&#20855;&#26377;&#30456;&#24212;&#30340;&#22270;&#24418;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.10037</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#29992;&#33258;&#28982;&#35821;&#35328;&#35299;&#20915;&#22270;&#38382;&#39064;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Language Models Solve Graph Problems in Natural Language?. (arXiv:2305.10037v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10037
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#33258;&#28982;&#35821;&#35328;&#22270;&#24418;(NLGraph)&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20110;&#22270;&#24418;&#38382;&#39064;&#35299;&#20915;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;LLM&#22312;&#25991;&#26412;&#25551;&#36848;&#30340;&#22270;&#24418;&#32467;&#26500;&#21644;&#22270;&#24418;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#30340;&#22788;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LLM(GPT-3/4)&#20855;&#26377;&#30456;&#24212;&#30340;&#22270;&#24418;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#19968;&#20123;&#20855;&#26377;&#38544;&#24335;&#22270;&#24418;&#32467;&#26500;&#30340;&#20219;&#21153;&#65292;&#20363;&#22914;&#26426;&#22120;&#20154;&#35268;&#21010;&#12289;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#25110;&#30693;&#35782;&#25506;&#32034;&#12289;&#32467;&#26500;&#21270;&#24120;&#35782;&#25512;&#29702;&#31561;&#31561;&#12290;&#34429;&#28982;LLM&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#24050;&#32463;&#21462;&#24471;&#20102;&#19968;&#23450;&#30340;&#36827;&#23637;&#65292;&#20294;&#26159;LLM&#26159;&#21542;&#33021;&#22815;&#26174;&#24335;&#22788;&#29702;&#22270;&#24418;&#30340;&#25991;&#26412;&#25551;&#36848;&#65292;&#23558;&#23427;&#20204;&#26144;&#23556;&#21040;&#22522;&#20110;&#27010;&#24565;&#30340;&#31354;&#38388;&#20013;&#65292;&#24182;&#25191;&#34892;&#32467;&#26500;&#21270;&#25805;&#20316;&#20173;&#28982;&#23578;&#26410;&#24471;&#21040;&#36275;&#22815;&#30340;&#30740;&#31350;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#28982;&#35821;&#35328;&#22270;&#24418;(NLGraph)&#65292;&#23427;&#26159;&#19968;&#20010;&#35774;&#35745;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#22522;&#20110;&#22270;&#24418;&#38382;&#39064;&#35299;&#20915;&#20840;&#38754;&#27979;&#35797;&#12290;NLGraph&#21253;&#21547;29,370&#20010;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;&#20843;&#20010;&#22270;&#24418;&#25512;&#29702;&#20219;&#21153;&#65292;&#20174;&#31616;&#21333;&#30340;&#36830;&#25509;&#21644;&#26368;&#30701;&#36335;&#24452;&#21040;&#22797;&#26434;&#30340;&#26368;&#22823;&#27969;&#21644;&#27169;&#25311;&#22270;&#31070;&#32463;&#32593;&#32476;&#31561;&#20219;&#21153;&#19981;&#31561;&#12290;&#25105;&#20204;&#22312;NLGraph&#22522;&#20934;&#27979;&#35797;&#19978;&#35780;&#20272;&#20102;LLM(GPT-3/4)&#65292;&#24182;&#21457;&#29616;1)&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#30456;&#24212;&#30340;&#22270;&#24418;&#25512;&#29702;&#33021;&#21147;&#65307;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are increasingly adopted for a variety of tasks with implicit graphical structures, such as planning in robotics, multi-hop question answering or knowledge probing, structured commonsense reasoning, and more. While LLMs have advanced the state-of-the-art on these tasks with structure implications, whether LLMs could explicitly process textual descriptions of graphs and structures, map them to grounded conceptual spaces, and perform structured operations remains underexplored. To this end, we propose NLGraph (Natural Language Graph), a comprehensive benchmark of graph-based problem solving designed in natural language. NLGraph contains 29,370 problems, covering eight graph reasoning tasks with varying complexity from simple tasks such as connectivity and shortest path up to complex problems such as maximum flow and simulating graph neural networks. We evaluate LLMs (GPT-3/4) with various prompting approaches on the NLGraph benchmark and find that 1) language
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#22797;&#26434;&#24847;&#22270;&#35805;&#35821;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#23618;&#33258;&#28982;&#35821;&#35328;&#20998;&#35299;&#21644;&#35299;&#37322;&#30340;&#36807;&#31243;&#23558;&#22797;&#26434;&#35805;&#35821;&#20998;&#35299;&#20026;&#31616;&#21333;&#30340;&#33258;&#28982;&#35821;&#35328;&#27493;&#39588;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#35328;&#21040;&#31243;&#24207;&#27169;&#22411;&#35299;&#37322;&#27599;&#20010;&#27493;&#39588;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.08677</link><description>&lt;p&gt;
&#22797;&#26434;&#35805;&#35821;&#30340;&#33258;&#28982;&#35821;&#35328;&#20998;&#35299;&#21644;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Natural Language Decomposition and Interpretation of Complex Utterances. (arXiv:2305.08677v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#22797;&#26434;&#24847;&#22270;&#35805;&#35821;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#23618;&#33258;&#28982;&#35821;&#35328;&#20998;&#35299;&#21644;&#35299;&#37322;&#30340;&#36807;&#31243;&#23558;&#22797;&#26434;&#35805;&#35821;&#20998;&#35299;&#20026;&#31616;&#21333;&#30340;&#33258;&#28982;&#35821;&#35328;&#27493;&#39588;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#35328;&#21040;&#31243;&#24207;&#27169;&#22411;&#35299;&#37322;&#27599;&#20010;&#27493;&#39588;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#33258;&#28982;&#35821;&#35328;&#30028;&#38754;&#22312;&#21382;&#21490;&#19978;&#19968;&#30452;&#38656;&#35201;&#25910;&#38598;&#30417;&#30563;&#25968;&#25454;&#65292;&#23558;&#29992;&#25143;&#35831;&#27714;&#36716;&#21270;&#20026;&#31934;&#24515;&#35774;&#35745;&#30340;&#24847;&#22270;&#34920;&#31034;&#12290;&#36825;&#38656;&#35201;&#21015;&#20030;&#21644;&#26631;&#35760;&#19968;&#31995;&#21015;&#29992;&#25143;&#35831;&#27714;&#65292;&#36825;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#21516;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#32534;&#30721;&#26377;&#20851;&#30446;&#26631;&#21644;&#35745;&#21010;&#30340;&#30693;&#35782;&#65292;&#21487;&#20197;&#24110;&#21161;&#23545;&#29992;&#25143;&#35831;&#27714;&#36827;&#34892;&#35299;&#37322;&#65292;&#38656;&#35201;&#23436;&#25104;&#35768;&#22810;&#27493;&#39588;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22788;&#29702;&#29992;&#25143;&#36890;&#36807;&#20998;&#23618;&#33258;&#28982;&#35821;&#35328;&#20998;&#35299;&#21644;&#35299;&#37322;&#36807;&#31243;&#20135;&#29983;&#30340;&#22797;&#26434;&#24847;&#22270;&#35805;&#35821;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23558;&#22797;&#26434;&#35805;&#35821;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#26356;&#31616;&#21333;&#30340;&#33258;&#28982;&#35821;&#35328;&#27493;&#39588;&#65292;&#24182;&#20351;&#29992;&#20026;&#30028;&#38754;&#35774;&#35745;&#30340;&#35821;&#35328;&#21040;&#31243;&#24207;&#27169;&#22411;&#35299;&#37322;&#27599;&#20010;&#27493;&#39588;&#12290;&#20026;&#20102;&#27979;&#35797;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25910;&#38598;&#21644;&#21457;&#24067;&#20102;DeCU -- &#19968;&#20010;&#26032;&#30340;&#29992;&#20110;&#35780;&#20272;&#22797;&#26434;&#35805;&#35821;&#20998;&#35299;&#30340;&#33258;&#28982;&#35821;&#35328;&#21040;&#31243;&#24207;&#22522;&#20934;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing natural language interfaces has historically required collecting supervised data to translate user requests into carefully designed intent representations. This requires enumerating and labeling a long tail of user requests, which is challenging. At the same time, large language models (LLMs) encode knowledge about goals and plans that can help conversational assistants interpret user requests requiring numerous steps to complete. We introduce an approach to handle complex-intent-bearing utterances from a user via a process of hierarchical natural language decomposition and interpretation. Our approach uses a pre-trained language model to decompose a complex utterance into a sequence of simpler natural language steps and interprets each step using the language-to-program model designed for the interface. To test our approach, we collect and release DeCU -- a new NL-to-program benchmark to evaluate Decomposition of Complex Utterances. Experiments show that the proposed approac
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#38382;&#39064;&#35299;&#26512;&#21644;&#25191;&#34892;&#26694;&#26550;&#65292;&#22312;&#25991;&#23383;&#38382;&#31572;&#31995;&#32479;&#20013;&#23454;&#29616;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#65292;&#36890;&#36807;&#35299;&#26512;&#38382;&#39064;&#20026;H&#34920;&#36798;&#24335;&#24182;&#35774;&#35745;&#28151;&#21512;&#25191;&#34892;&#22120;&#23454;&#29616;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#29575;&#21644;&#25928;&#29575;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.07789</link><description>&lt;p&gt;
&#28151;&#21512;&#38382;&#39064;&#35299;&#26512;&#19982;&#25191;&#34892;&#31572;&#26696;&#22797;&#26434;&#38382;&#39064;&#30340;&#25991;&#23383;&#38382;&#31572;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Answering Complex Questions over Text by Hybrid Question Parsing and Execution. (arXiv:2305.07789v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07789
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#38382;&#39064;&#35299;&#26512;&#21644;&#25191;&#34892;&#26694;&#26550;&#65292;&#22312;&#25991;&#23383;&#38382;&#31572;&#31995;&#32479;&#20013;&#23454;&#29616;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#65292;&#36890;&#36807;&#35299;&#26512;&#38382;&#39064;&#20026;H&#34920;&#36798;&#24335;&#24182;&#35774;&#35745;&#28151;&#21512;&#25191;&#34892;&#22120;&#23454;&#29616;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#29575;&#21644;&#25928;&#29575;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#38382;&#31572;&#31995;&#32479;&#30340;&#20027;&#23548;&#27169;&#24335;&#26159;&#22522;&#20110;&#31471;&#21040;&#31471;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#22312;&#22238;&#31572;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#31361;&#20986;&#65292;&#20294;&#22312;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#19981;&#36275;&#12290;&#36825;&#19982;&#22522;&#20110;&#35821;&#20041;&#35299;&#26512;&#30340;&#26041;&#27861;&#22312;&#32467;&#26500;&#21270;&#25968;&#25454;&#28304;&#65288;&#22914;&#20851;&#31995;&#25968;&#25454;&#24211;&#12289;&#30693;&#35782;&#22270;&#35889;&#65289;&#19978;&#24191;&#27867;&#36866;&#24212;&#24418;&#25104;&#23545;&#27604;&#65292;&#21518;&#32773;&#23558;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#36716;&#25442;&#20026;&#36923;&#36753;&#24418;&#24335;&#65292;&#24182;&#21033;&#29992;&#26597;&#35810;&#24341;&#25806;&#36827;&#34892;&#25191;&#34892;&#12290;&#20026;&#20102;&#32467;&#21512;&#31070;&#32463;&#21644;&#31526;&#21495;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#25991;&#26412;&#38382;&#31572;&#31995;&#32479;&#20013;&#36827;&#34892;&#35299;&#26512;&#21644;&#25191;&#34892;&#38382;&#39064;&#30340;&#26694;&#26550;&#12290;&#23427;&#21253;&#25324;&#20004;&#20010;&#20013;&#24515;&#25903;&#26609;&#65306;&#65288;1&#65289;&#25105;&#20204;&#23558;&#21508;&#31181;&#22797;&#26434;&#38382;&#39064;&#35299;&#26512;&#25104;&#20013;&#38388;&#34920;&#31034;&#65292;&#31216;&#20026;H&#34920;&#36798;&#24335;&#65292;&#23427;&#30001;&#31616;&#21333;&#38382;&#39064;&#32452;&#25104;&#21407;&#35821;&#21644;&#34920;&#31034;&#23427;&#20204;&#20043;&#38388;&#20851;&#31995;&#30340;&#31526;&#21495;&#25805;&#20316;&#32452;&#25104;&#65307;&#65288;2&#65289;&#20026;&#20102;&#25191;&#34892;&#20135;&#29983;&#30340;H&#34920;&#36798;&#24335;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#28151;&#21512;&#25191;&#34892;&#22120;&#65292;&#23427;&#38598;&#25104;&#20102;&#30830;&#23450;&#35268;&#21017;&#26469;&#32763;&#35793;&#31526;&#21495;&#25805;&#20316;&#65292;&#19982;&#22788;&#29702;&#21407;&#22987;&#38382;&#39064;&#30340;&#25554;&#20837;&#31070;&#32463;&#27169;&#22359;&#12290;&#25105;&#20204;&#22312;&#21253;&#21547;&#22797;&#26434;&#38382;&#39064;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#25351;&#26631;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dominant paradigm of textual question answering systems is based on end-to-end neural networks, which excels at answering natural language questions but falls short on complex ones. This stands in contrast to the broad adaptation of semantic parsing approaches over structured data sources (e.g., relational database, knowledge graphs), that convert natural language questions to logical forms and execute them with query engines. Towards combining the strengths of neural and symbolic methods, we propose a framework of question parsing and execution on textual QA. It comprises two central pillars: (1) We parse the question of varying complexity into an intermediate representation, named H-expression, which is composed of simple questions as the primitives and symbolic operations representing the relationships among them; (2) To execute the resulting H-expressions, we design a hybrid executor, which integrates the deterministic rules to translate the symbolic operations with a drop-in n
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35299;&#37322;&#25512;&#33616;&#31995;&#32479;&#27169;&#22411;&#65292;&#23558;&#20174;&#29992;&#25143;-&#21830;&#21697;&#20132;&#20114;&#20013;&#23398;&#24471;&#30340;&#20960;&#20309;&#20808;&#39564;&#30693;&#35782;&#19982;&#21464;&#20998;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#20026;&#29992;&#25143;&#25552;&#20379;&#26082;&#20855;&#22791;&#25512;&#33616;&#24615;&#33021;&#21448;&#20855;&#26377;&#35299;&#37322;&#24615;&#33021;&#30340;&#35299;&#37322;&#25512;&#33616;&#26381;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.05331</link><description>&lt;p&gt;
&#20855;&#26377;&#20960;&#20309;&#20449;&#24687;&#29942;&#39048;&#30340;&#21487;&#35299;&#37322;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Explainable Recommender with Geometric Information Bottleneck. (arXiv:2305.05331v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05331
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35299;&#37322;&#25512;&#33616;&#31995;&#32479;&#27169;&#22411;&#65292;&#23558;&#20174;&#29992;&#25143;-&#21830;&#21697;&#20132;&#20114;&#20013;&#23398;&#24471;&#30340;&#20960;&#20309;&#20808;&#39564;&#30693;&#35782;&#19982;&#21464;&#20998;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#20026;&#29992;&#25143;&#25552;&#20379;&#26082;&#20855;&#22791;&#25512;&#33616;&#24615;&#33021;&#21448;&#20855;&#26377;&#35299;&#37322;&#24615;&#33021;&#30340;&#35299;&#37322;&#25512;&#33616;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#25512;&#33616;&#31995;&#32479;&#33021;&#22815;&#35299;&#37322;&#20854;&#25512;&#33616;&#20915;&#31574;&#65292;&#22686;&#24378;&#29992;&#25143;&#23545;&#31995;&#32479;&#30340;&#20449;&#20219;&#12290;&#22823;&#22810;&#25968;&#21487;&#35299;&#37322;&#30340;&#25512;&#33616;&#31995;&#32479;&#35201;&#20040;&#20381;&#36182;&#20110;&#20154;&#24037;&#26631;&#27880;&#30340;&#21407;&#29702;&#26469;&#35757;&#32451;&#27169;&#22411;&#20197;&#29983;&#25104;&#35299;&#37322;&#65292;&#35201;&#20040;&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#20174;&#35780;&#35770;&#20013;&#25552;&#21462;&#37325;&#35201;&#30340;&#25991;&#26412;&#27573;&#33853;&#20316;&#20026;&#35299;&#37322;&#12290;&#25552;&#21462;&#30340;&#21407;&#29702;&#24448;&#24448;&#23616;&#38480;&#20110;&#21333;&#20010;&#35780;&#35770;&#65292;&#21487;&#33021;&#26080;&#27861;&#35782;&#21035;&#35780;&#35770;&#25991;&#26412;&#20043;&#22806;&#30340;&#38544;&#21547;&#29305;&#24449;&#12290;&#20026;&#20102;&#36991;&#20813;&#26114;&#36149;&#30340;&#20154;&#24037;&#27880;&#37322;&#36807;&#31243;&#24182;&#29983;&#25104;&#36229;&#20986;&#21333;&#20010;&#35780;&#35770;&#30340;&#35299;&#37322;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#20174;&#29992;&#25143;-&#21830;&#21697;&#20132;&#20114;&#20013;&#23398;&#24471;&#30340;&#20960;&#20309;&#20808;&#39564;&#30693;&#35782;&#19982;&#21464;&#20998;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#35813;&#32593;&#32476;&#20174;&#29992;&#25143;-&#21830;&#21697;&#35780;&#35770;&#20013;&#25512;&#26029;&#28508;&#22312;&#22240;&#23376;&#12290;&#21333;&#20010;&#29992;&#25143;-&#21830;&#21697;&#23545;&#30340;&#28508;&#22312;&#22240;&#23376;&#21487;&#29992;&#20110;&#25512;&#33616;&#21644;&#35299;&#37322;&#29983;&#25104;&#65292;&#33258;&#28982;&#22320;&#32487;&#25215;&#20102;&#32534;&#30721;&#22312;&#20808;&#39564;&#30693;&#35782;&#20013;&#30340;&#20840;&#23616;&#29305;&#24449;&#12290;&#19977;&#20010;&#30005;&#23376;&#21830;&#21153;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25512;&#33616;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#37117;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable recommender systems can explain their recommendation decisions, enhancing user trust in the systems. Most explainable recommender systems either rely on human-annotated rationales to train models for explanation generation or leverage the attention mechanism to extract important text spans from reviews as explanations. The extracted rationales are often confined to an individual review and may fail to identify the implicit features beyond the review text. To avoid the expensive human annotation process and to generate explanations beyond individual reviews, we propose to incorporate a geometric prior learnt from user-item interactions into a variational network which infers latent factors from user-item reviews. The latent factors from an individual user-item pair can be used for both recommendation and explanation generation, which naturally inherit the global characteristics encoded in the prior knowledge. Experimental results on three e-commerce datasets show that our mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20221;&#38750;&#27597;&#35821;&#35821;&#38899;&#35821;&#26009;&#24211;&#65292;&#30001;50&#21517;&#20013;&#33521;&#21452;&#35821;&#20799;&#31461;&#25552;&#20379;&#33521;&#35821;&#65288;L2&#65289;&#21465;&#36848;&#65292;&#20026;&#31532;&#20108;&#35821;&#35328;&#25945;&#23398;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#24182;&#26377;&#21487;&#33021;&#25552;&#39640;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.00446</link><description>&lt;p&gt;
&#26500;&#24314;&#19968;&#20221;&#20013;&#33521;&#21452;&#35821;&#20799;&#31461;&#38750;&#27597;&#35821;&#35821;&#38899;&#35821;&#26009;&#24211;&#65306;&#32534;&#21046;&#21644;&#21407;&#29702;
&lt;/p&gt;
&lt;p&gt;
Building a Non-native Speech Corpus Featuring Chinese-English Bilingual Children: Compilation and Rationale. (arXiv:2305.00446v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20221;&#38750;&#27597;&#35821;&#35821;&#38899;&#35821;&#26009;&#24211;&#65292;&#30001;50&#21517;&#20013;&#33521;&#21452;&#35821;&#20799;&#31461;&#25552;&#20379;&#33521;&#35821;&#65288;L2&#65289;&#21465;&#36848;&#65292;&#20026;&#31532;&#20108;&#35821;&#35328;&#25945;&#23398;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#24182;&#26377;&#21487;&#33021;&#25552;&#39640;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20221;&#38750;&#27597;&#35821;&#35821;&#38899;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547;&#20102;&#26469;&#33258;50&#20301;5-6&#23681;&#30340;&#20013;&#33521;&#21452;&#35821;&#20799;&#31461;&#30340;&#21465;&#36848;&#12290;&#25552;&#20379;&#20102;&#24635;&#35745;6.5&#23567;&#26102;&#30340;&#33521;&#35821;&#65288;L2&#65289;&#21465;&#36848;&#29702;&#35299;&#27979;&#35797;&#30340;&#36716;&#24405;&#25991;&#26412;&#65292;&#20197;&#21450;&#35780;&#20998;&#21644;&#35821;&#27861;&#21450;&#21457;&#38899;&#38169;&#35823;&#30340;&#27880;&#37322;&#12290;&#36825;&#20123;&#23401;&#23376;&#36824;&#23436;&#25104;&#20102;&#20013;&#25991;&#65288;L1&#65289;&#30340;&#24179;&#34892;MAIN&#27979;&#35797;&#20197;&#20415;&#36827;&#34892;&#21442;&#32771;&#12290;&#23545;&#20110;&#25152;&#26377;&#27979;&#35797;&#65292;&#25105;&#20204;&#35760;&#24405;&#20102;&#38899;&#39057;&#21644;&#35270;&#39057;&#65292;&#24182;&#37319;&#29992;&#20102;&#33258;&#20027;&#24320;&#21457;&#30340;&#36828;&#31243;&#25910;&#38598;&#26041;&#27861;&#12290;&#35270;&#39057;&#35760;&#24405;&#26377;&#21161;&#20110;&#20943;&#36731;&#22312;&#36716;&#24405;&#36807;&#31243;&#20013;&#30001;&#24180;&#24188;&#20799;&#31461;&#30340;L2&#21465;&#36848;&#20302;&#21487;&#25026;&#24615;&#25152;&#24102;&#26469;&#30340;&#38590;&#24230;&#12290;&#35813;&#35821;&#26009;&#24211;&#20026;&#31532;&#20108;&#35821;&#35328;&#25945;&#23398;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#65292;&#24182;&#26377;&#21487;&#33021;&#25552;&#39640;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a non-native speech corpus consisting of narratives from fifty 5- to 6-year-old Chinese-English children. Transcripts totaling 6.5 hours of children taking a narrative comprehension test in English (L2) are presented, along with human-rated scores and annotations of grammatical and pronunciation errors. The children also completed the parallel MAIN tests in Chinese (L1) for reference purposes. For all tests we recorded audio and video with our innovative self-developed remote collection methods. The video recordings serve to mitigate the challenge of low intelligibility in L2 narratives produced by young children during the transcription process. This corpus offers valuable resources for second language teaching and has the potential to enhance the overall performance of automatic speech recognition (ASR).
&lt;/p&gt;</description></item><item><title>&#28145;&#23618;&#28508;&#22312;&#20301;&#32622;&#20027;&#39064;&#27169;&#22411;&#29992;&#20110;&#32593;&#32476;&#32858;&#31867;&#21644;&#34920;&#31034;&#65292;&#36890;&#36807;&#22522;&#20110;&#27169;&#22411;&#30340;&#32858;&#31867;&#31574;&#30053;&#21644;&#27010;&#29575;&#27169;&#22411;&#23545;&#33410;&#28857;&#21644;&#36793;&#36827;&#34892;&#32852;&#21512;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#27169;&#22411;&#36873;&#25321;&#20934;&#21017;&#36827;&#34892;&#21442;&#25968;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2304.08242</link><description>&lt;p&gt;
&#28145;&#23618;&#28508;&#22312;&#20301;&#32622;&#20027;&#39064;&#27169;&#22411;&#29992;&#20110;&#24102;&#26377;&#25991;&#26412;&#36793;&#30340;&#32593;&#32476;&#32858;&#31867;&#21644;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
The Deep Latent Position Topic Model for Clustering and Representation of Networks with Textual Edges. (arXiv:2304.08242v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08242
&lt;/p&gt;
&lt;p&gt;
&#28145;&#23618;&#28508;&#22312;&#20301;&#32622;&#20027;&#39064;&#27169;&#22411;&#29992;&#20110;&#32593;&#32476;&#32858;&#31867;&#21644;&#34920;&#31034;&#65292;&#36890;&#36807;&#22522;&#20110;&#27169;&#22411;&#30340;&#32858;&#31867;&#31574;&#30053;&#21644;&#27010;&#29575;&#27169;&#22411;&#23545;&#33410;&#28857;&#21644;&#36793;&#36827;&#34892;&#32852;&#21512;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#27169;&#22411;&#36873;&#25321;&#20934;&#21017;&#36827;&#34892;&#21442;&#25968;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#20540;&#20132;&#20114;&#23548;&#33268;&#29992;&#25143;&#20849;&#20139;&#20854;&#20182;&#20154;&#21457;&#24067;&#30340;&#25991;&#26412;&#20869;&#23481;&#65292;&#36825;&#20123;&#20869;&#23481;&#33258;&#28982;&#22320;&#30001;&#23558;&#20010;&#20307;&#19982;&#33410;&#28857;&#20851;&#32852;&#21644;&#20132;&#25442;&#30340;&#25991;&#26412;&#23450;&#20041;&#20026;&#36793;&#30340;&#32593;&#32476;&#26469;&#34920;&#31034;&#12290;&#20026;&#20102;&#29702;&#35299;&#36825;&#20123;&#24322;&#26500;&#21644;&#22797;&#26434;&#30340;&#25968;&#25454;&#32467;&#26500;&#65292;&#23558;&#33410;&#28857;&#32858;&#31867;&#20026;&#21516;&#31867;&#32676;&#32452;&#20197;&#21450;&#21576;&#29616;&#21487;&#29702;&#35299;&#30340;&#25968;&#25454;&#21487;&#35270;&#21270;&#26159;&#24517;&#35201;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Deep-LPTM&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#32858;&#31867;&#31574;&#30053;&#65292;&#20381;&#36182;&#20110;&#21464;&#20998;&#22270;&#33258;&#21160;&#32534;&#30721;&#22120;&#26041;&#27861;&#20197;&#21450;&#27010;&#29575;&#27169;&#22411;&#26469;&#25551;&#36848;&#35752;&#35770;&#30340;&#20027;&#39064;&#12290;Deep-LPTM&#20801;&#35768;&#22312;&#20004;&#20010;&#23884;&#20837;&#31354;&#38388;&#20013;&#26500;&#24314;&#33410;&#28857;&#21644;&#36793;&#30340;&#32852;&#21512;&#34920;&#31034;&#12290;&#21442;&#25968;&#20351;&#29992;&#21464;&#20998;&#25512;&#26029;&#31639;&#27861;&#36827;&#34892;&#25512;&#26029;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;IC2L&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#36873;&#25321;&#20855;&#26377;&#30456;&#20851;&#32858;&#31867;&#21644;&#21487;&#35270;&#21270;&#23646;&#24615;&#30340;&#27169;&#22411;&#30340;&#27169;&#22411;&#36873;&#25321;&#20934;&#21017;&#12290;&#23545;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#30740;&#31350;&#12290;&#29305;&#21035;&#26159;
&lt;/p&gt;
&lt;p&gt;
Numerical interactions leading to users sharing textual content published by others are naturally represented by a network where the individuals are associated with the nodes and the exchanged texts with the edges. To understand those heterogeneous and complex data structures, clustering nodes into homogeneous groups as well as rendering a comprehensible visualisation of the data is mandatory. To address both issues, we introduce Deep-LPTM, a model-based clustering strategy relying on a variational graph auto-encoder approach as well as a probabilistic model to characterise the topics of discussion. Deep-LPTM allows to build a joint representation of the nodes and of the edges in two embeddings spaces. The parameters are inferred using a variational inference algorithm. We also introduce IC2L, a model selection criterion specifically designed to choose models with relevant clustering and visualisation properties. An extensive benchmark study on synthetic data is provided. In particular
&lt;/p&gt;</description></item><item><title>&#20256;&#32479;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26426;&#22120;&#23398;&#20064;&#38656;&#35201;&#22823;&#35268;&#27169;&#30340;&#20219;&#21153;&#29305;&#23450;&#31034;&#20363;&#65292;&#20294;&#36825;&#19981;&#36866;&#29992;&#20110;&#20219;&#21153;&#21487;&#33021;&#36807;&#20110;&#22797;&#26434;&#25110;&#25104;&#26412;&#36807;&#39640;&#20197;&#36827;&#34892;&#27880;&#37322;&#30340;&#22330;&#26223;&#12290;&#22240;&#27492;&#65292;&#31038;&#21306;&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26032;&#30340;&#30417;&#30563;&#23547;&#27714;&#33539;&#24335;--&#20174;&#20219;&#21153;&#25351;&#20196;&#23398;&#20064;--&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#12290;</title><link>http://arxiv.org/abs/2303.10475</link><description>&lt;p&gt;
&#20165;&#20165;&#25552;&#31034;&#36275;&#22815;&#20102;&#21527;&#65311;&#19981;&#26159;&#30340;&#12290;&#25351;&#23548;&#23398;&#20064;&#30340;&#20840;&#38754;&#21644;&#26356;&#24191;&#38420;&#35270;&#35282;&#65288;arXiv&#65306;2303.10475v1 [cs.CL]&#65289;
&lt;/p&gt;
&lt;p&gt;
Is Prompt All You Need? No. A Comprehensive and Broader View of Instruction Learning. (arXiv:2303.10475v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10475
&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26426;&#22120;&#23398;&#20064;&#38656;&#35201;&#22823;&#35268;&#27169;&#30340;&#20219;&#21153;&#29305;&#23450;&#31034;&#20363;&#65292;&#20294;&#36825;&#19981;&#36866;&#29992;&#20110;&#20219;&#21153;&#21487;&#33021;&#36807;&#20110;&#22797;&#26434;&#25110;&#25104;&#26412;&#36807;&#39640;&#20197;&#36827;&#34892;&#27880;&#37322;&#30340;&#22330;&#26223;&#12290;&#22240;&#27492;&#65292;&#31038;&#21306;&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26032;&#30340;&#30417;&#30563;&#23547;&#27714;&#33539;&#24335;--&#20174;&#20219;&#21153;&#25351;&#20196;&#23398;&#20064;--&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#35821;&#20041;&#21487;&#20197;&#36890;&#36807;&#19968;&#32452;&#36755;&#20837;&#36755;&#20986;&#31034;&#20363;&#25110;&#19968;&#26465;&#25991;&#26412;&#25351;&#20196;&#26469;&#34920;&#36798;&#12290;&#20256;&#32479;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#30340;&#20219;&#21153;&#29305;&#23450;&#31034;&#20363;&#30340;&#21487;&#29992;&#24615;&#12290;&#36825;&#24341;&#36215;&#20102;&#20004;&#20010;&#38382;&#39064;&#65306;&#39318;&#20808;&#65292;&#25910;&#38598;&#20219;&#21153;&#29305;&#23450;&#26631;&#35760;&#31034;&#20363;&#19981;&#36866;&#29992;&#20110;&#20219;&#21153;&#21487;&#33021;&#36807;&#20110;&#22797;&#26434;&#25110;&#25104;&#26412;&#36807;&#39640;&#20197;&#36827;&#34892;&#27880;&#37322;&#30340;&#22330;&#26223;&#65292;&#25110;&#32773;&#31995;&#32479;&#38656;&#35201;&#31435;&#21363;&#22788;&#29702;&#26032;&#20219;&#21153;&#12290;&#20854;&#27425;&#65292;&#36825;&#19981;&#26159;&#29992;&#25143;&#21451;&#22909;&#30340;&#65292;&#22240;&#20026;&#26368;&#32456;&#29992;&#25143;&#21487;&#33021;&#26356;&#24895;&#24847;&#22312;&#20351;&#29992;&#31995;&#32479;&#20043;&#21069;&#25552;&#20379;&#20219;&#21153;&#25551;&#36848;&#32780;&#19981;&#26159;&#19968;&#32452;&#31034;&#20363;&#12290;&#22240;&#27492;&#65292;&#31038;&#21306;&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26032;&#30340;&#30417;&#30563;&#23547;&#27714;&#33539;&#24335;--&#20174;&#20219;&#21153;&#25351;&#20196;&#23398;&#20064;--&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#36827;&#23637;&#65292;&#20294;&#31038;&#21306;&#20173;&#28982;&#38754;&#20020;&#30528;&#19968;&#20123;&#20849;&#21516;&#30340;&#38382;&#39064;&#12290;&#26412;&#27425;&#35843;&#26597;&#26088;&#22312;&#24635;&#32467;&#25351;&#23548;&#23398;&#20064;&#30340;&#24403;&#21069;&#30740;&#31350;&#65292;&#29305;&#21035;&#26159;&#22238;&#31572;&#20197;&#19979;&#38382;&#39064;&#65306;
&lt;/p&gt;
&lt;p&gt;
Task semantics can be expressed by a set of input-to-output examples or a piece of textual instruction. Conventional machine learning approaches for natural language processing (NLP) mainly rely on the availability of large-scale sets of task-specific examples. Two issues arise: first, collecting task-specific labeled examples does not apply to scenarios where tasks may be too complicated or costly to annotate, or the system is required to handle a new task immediately; second, this is not user-friendly since end-users are probably more willing to provide task description rather than a set of examples before using the system. Therefore, the community is paying increasing interest in a new supervision-seeking paradigm for NLP: learning from task instructions. Despite its impressive progress, there are some common issues that the community struggles with. This survey paper tries to summarize the current research on instruction learning, particularly, by answering the following questions:
&lt;/p&gt;</description></item><item><title>LEXTREME&#26159;&#19968;&#20010;&#22810;&#35821;&#35328;&#21644;&#22810;&#20219;&#21153;&#30340;&#27861;&#24459;&#39046;&#22495;&#22522;&#20934;&#65292;&#35813;&#22522;&#20934;&#25552;&#20379;&#20102;11&#20010;&#25968;&#25454;&#38598;&#28085;&#30422;24&#31181;&#35821;&#35328;&#30340;&#27979;&#35780;&#65292;&#26368;&#20339;&#27169;&#22411;&#65288;XLM-R large&#65289;&#22312;&#25968;&#25454;&#38598;&#21644;&#35821;&#35328;&#32508;&#21512;&#35780;&#20998;&#19978;&#22343;&#36798;&#21040;&#20102;61.3&#12290;&#36825;&#20351;&#24471;LEXTREME&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#24182;&#19988;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2301.13126</link><description>&lt;p&gt;
LEXTREME&#65306;&#22810;&#35821;&#35328;&#21644;&#22810;&#20219;&#21153;&#30340;&#27861;&#24459;&#39046;&#22495;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
LEXTREME: A Multi-Lingual and Multi-Task Benchmark for the Legal Domain. (arXiv:2301.13126v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13126
&lt;/p&gt;
&lt;p&gt;
LEXTREME&#26159;&#19968;&#20010;&#22810;&#35821;&#35328;&#21644;&#22810;&#20219;&#21153;&#30340;&#27861;&#24459;&#39046;&#22495;&#22522;&#20934;&#65292;&#35813;&#22522;&#20934;&#25552;&#20379;&#20102;11&#20010;&#25968;&#25454;&#38598;&#28085;&#30422;24&#31181;&#35821;&#35328;&#30340;&#27979;&#35780;&#65292;&#26368;&#20339;&#27169;&#22411;&#65288;XLM-R large&#65289;&#22312;&#25968;&#25454;&#38598;&#21644;&#35821;&#35328;&#32508;&#21512;&#35780;&#20998;&#19978;&#22343;&#36798;&#21040;&#20102;61.3&#12290;&#36825;&#20351;&#24471;LEXTREME&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#24182;&#19988;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;transformer&#26550;&#26500;&#30340;&#26174;&#33879;&#36827;&#23637;&#25512;&#21160;&#19979;&#65292;&#27861;&#24459;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#22686;&#38271;&#12290;&#20026;&#20102;&#34913;&#37327;&#36827;&#23637;&#65292;&#31934;&#24515;&#31574;&#21010;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#22522;&#20934;&#21482;&#33021;&#22788;&#29702;&#33521;&#25991;&#65292;&#32780;&#22312;&#27861;&#24459;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#23578;&#26410;&#26377;&#22810;&#35821;&#35328;&#22522;&#20934;&#21487;&#29992;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#22522;&#20934;&#24050;&#32463;&#39281;&#21644;&#65292;&#26368;&#20339;&#27169;&#22411;&#26126;&#26174;&#20248;&#20110;&#26368;&#20339;&#20154;&#31867;&#65292;&#24182;&#36798;&#21040;&#36817;&#20046;&#23436;&#32654;&#30340;&#20998;&#25968;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#27861;&#24459;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25991;&#29486;&#65292;&#24182;&#36873;&#25321;&#20102;11&#20010;&#28085;&#30422;24&#31181;&#35821;&#35328;&#30340;&#25968;&#25454;&#38598;&#65292;&#21019;&#24314;&#20102;LEXTREME&#12290;&#20026;&#20102;&#36827;&#34892;&#20844;&#24179;&#27604;&#36739;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#32508;&#21512;&#35780;&#20998;&#65292;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#38598;&#65292;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#12290;&#26368;&#20339;&#22522;&#32447;&#27169;&#22411;&#65288;XLM-R large&#65289;&#22312;&#25968;&#25454;&#38598;&#32508;&#21512;&#35780;&#20998;&#21644;&#35821;&#35328;&#32508;&#21512;&#35780;&#20998;&#19978;&#22343;&#36798;&#21040;&#20102;61.3&#12290;&#36825;&#34920;&#26126;LEXTREME&#20173;&#28982;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#24182;&#19988;&#20026;&#25913;&#36827;&#30041;&#19979;&#20102;&#20805;&#36275;&#31354;&#38388;&#12290;&#20026;&#20102;&#26041;&#20415;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#20351;&#29992;&#65292;&#25105;&#20204;&#23558;LEXTREME&#19982;&#25152;&#26377;&#25968;&#25454;&#19968;&#36215;&#21457;&#24067;&#22312;huggingface&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lately, propelled by the phenomenal advances around the transformer architecture, the legal NLP field has enjoyed spectacular growth. To measure progress, well curated and challenging benchmarks are crucial. However, most benchmarks are English only and in legal NLP specifically there is no multilingual benchmark available yet. Additionally, many benchmarks are saturated, with the best models clearly outperforming the best humans and achieving near perfect scores. We survey the legal NLP literature and select 11 datasets covering 24 languages, creating LEXTREME. To provide a fair comparison, we propose two aggregate scores, one based on the datasets and one on the languages. The best baseline (XLM-R large) achieves both a dataset aggregate score a language aggregate score of 61.3. This indicates that LEXTREME is still very challenging and leaves ample room for improvement. To make it easy for researchers and practitioners to use, we release LEXTREME on huggingface together with all the
&lt;/p&gt;</description></item><item><title>&#20026;&#20102;&#35780;&#20272;&#20154;&#26426;&#20132;&#20114;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#19968;&#20010;&#26694;&#26550;HALIE&#65292;&#35813;&#26694;&#26550;&#25429;&#25417;&#20102;&#20132;&#20114;&#36807;&#31243;&#12289;&#20027;&#35266;&#20307;&#39564;&#21644;&#20559;&#22909;&#27010;&#24565;&#65292;&#24182;&#35774;&#35745;&#20102;&#20116;&#20010;&#20219;&#21153;&#26469;&#28085;&#30422;&#19981;&#21516;&#24418;&#24335;&#30340;&#20132;&#20114;&#12290;</title><link>http://arxiv.org/abs/2212.09746</link><description>&lt;p&gt;
&#35780;&#20272;&#20154;&#26426;&#35821;&#35328;&#27169;&#22411;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
Evaluating Human-Language Model Interaction. (arXiv:2212.09746v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09746
&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35780;&#20272;&#20154;&#26426;&#20132;&#20114;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#19968;&#20010;&#26694;&#26550;HALIE&#65292;&#35813;&#26694;&#26550;&#25429;&#25417;&#20102;&#20132;&#20114;&#36807;&#31243;&#12289;&#20027;&#35266;&#20307;&#39564;&#21644;&#20559;&#22909;&#27010;&#24565;&#65292;&#24182;&#35774;&#35745;&#20102;&#20116;&#20010;&#20219;&#21153;&#26469;&#28085;&#30422;&#19981;&#21516;&#24418;&#24335;&#30340;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#20363;&#22914;&#20889;&#20316;&#36741;&#21161;&#21644;&#20195;&#30721;&#33258;&#21160;&#23436;&#25104;&#65292;&#28041;&#21450;&#21040;&#20154;&#26426;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#22522;&#20934;&#27979;&#35797;&#37117;&#26159;&#38750;&#20132;&#20114;&#24335;&#30340;&#65292;&#27169;&#22411;&#22312;&#27809;&#26377;&#20154;&#31867;&#21442;&#19982;&#30340;&#24773;&#20917;&#19979;&#20135;&#29983;&#36755;&#20986;&#12290;&#20026;&#20102;&#35780;&#20272;&#20154;&#26426;&#20132;&#20114;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#20154;&#26426;&#35821;&#35328;&#20132;&#20114;&#35780;&#20272;&#65288;HALIE&#65289;&#65292;&#35813;&#26694;&#26550;&#23450;&#20041;&#20102;&#20132;&#20114;&#24335;&#31995;&#32479;&#30340;&#32452;&#25104;&#37096;&#20998;&#21644;&#35774;&#35745;&#35780;&#20272;&#25351;&#26631;&#26102;&#35201;&#32771;&#34385;&#30340;&#32500;&#24230;&#12290;&#19982;&#26631;&#20934;&#30340;&#38750;&#20132;&#20114;&#24335;&#35780;&#20272;&#30456;&#27604;&#65292;HALIE&#25429;&#25417;&#21040;&#20102;&#65288;i&#65289;&#20132;&#20114;&#36807;&#31243;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#26368;&#32456;&#36755;&#20986;&#65307;&#65288;ii&#65289;&#31532;&#19968;&#20154;&#31216;&#20027;&#35266;&#20307;&#39564;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#31532;&#19977;&#26041;&#35780;&#20272;&#65307;&#65288;iii&#65289;&#38500;&#20102;&#36136;&#37327;&#20043;&#22806;&#30340;&#20559;&#22909;&#27010;&#24565;&#65288;&#20363;&#22914;&#20139;&#21463;&#21644;&#25152;&#26377;&#26435;&#65289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20116;&#20010;&#20219;&#21153;&#65292;&#28085;&#30422;&#19981;&#21516;&#24418;&#24335;&#30340;&#20132;&#20114;&#65306;&#31038;&#20132;&#23545;&#35805;&#12289;&#38382;&#31572;&#12289;&#22635;&#23383;&#28216;&#25103;&#12289;&#25688;&#35201;&#21644;&#38544;&#21947;&#29983;&#25104;&#12290;&#20351;&#29992;&#22235;&#20010;&#26368;&#20808;&#36827;&#30340;LM&#65288;OpenAI&#30340;GPT-3&#30340;&#19977;&#20010;&#21464;&#20307;&#21644;AI21 Labs&#30340;Jurass&#65289;
&lt;/p&gt;
&lt;p&gt;
Many real-world applications of language models (LMs), such as writing assistance and code autocomplete, involve human-LM interaction. However, most benchmarks are non-interactive in that a model produces output without human involvement. To evaluate human-LM interaction, we develop a new framework, Human-AI Language-based Interaction Evaluation (HALIE), that defines the components of interactive systems and dimensions to consider when designing evaluation metrics. Compared to standard, non-interactive evaluation, HALIE captures (i) the interactive process, not only the final output; (ii) the first-person subjective experience, not just a third-party assessment; and (iii) notions of preference beyond quality (e.g., enjoyment and ownership). We then design five tasks to cover different forms of interaction: social dialogue, question answering, crossword puzzles, summarization, and metaphor generation. With four state-of-the-art LMs (three variants of OpenAI's GPT-3 and AI21 Labs' Jurass
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; AMC &#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#40723;&#21169;&#22522;&#20110;&#26799;&#24230;&#30340;&#35299;&#37322;&#35206;&#30422;&#26377;&#27880;&#37322;&#30340;&#24863;&#20852;&#36259;&#21306;&#22495;&#65292;&#21363;&#32534;&#30721;&#21306;&#22495;&#12290;&#35813;&#26041;&#27861;&#22312;&#25552;&#39640;&#35270;&#35273; grounding &#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#21331;&#36234;&#65292;&#26377;&#26395;&#25104;&#20026;&#35270;&#35273; grounding &#39046;&#22495;&#30340;&#26032;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2206.15462</link><description>&lt;p&gt;
&#36890;&#36807;&#40723;&#21169;&#19968;&#33268;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#35299;&#37322;&#26469;&#25913;&#36827;&#35270;&#35273; grounding
&lt;/p&gt;
&lt;p&gt;
Improving Visual Grounding by Encouraging Consistent Gradient-based Explanations. (arXiv:2206.15462v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.15462
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; AMC &#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#40723;&#21169;&#22522;&#20110;&#26799;&#24230;&#30340;&#35299;&#37322;&#35206;&#30422;&#26377;&#27880;&#37322;&#30340;&#24863;&#20852;&#36259;&#21306;&#22495;&#65292;&#21363;&#32534;&#30721;&#21306;&#22495;&#12290;&#35813;&#26041;&#27861;&#22312;&#25552;&#39640;&#35270;&#35273; grounding &#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#21331;&#36234;&#65292;&#26377;&#26395;&#25104;&#20026;&#35270;&#35273; grounding &#39046;&#22495;&#30340;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#32536;&#30340;&#25439;&#22833;&#65292;&#29992;&#20110;&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#40723;&#21169;&#22522;&#20110;&#26799;&#24230;&#30340;&#35299;&#37322;&#19982;&#21306;&#22495;&#32423;&#27880;&#37322;&#20445;&#25345;&#19968;&#33268;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#30446;&#26631;&#31216;&#20026; Attention Mask Consistency&#65288;AMC&#65289;&#65292;&#24182;&#35777;&#26126;&#23427;&#20135;&#29983;&#20102;&#27604;&#20381;&#36182;&#20110;&#21306;&#22495;&#32423;&#27880;&#37322;&#30340;&#27169;&#22411;&#26356;&#20248;&#36234;&#30340;&#35270;&#35273; grounding &#24615;&#33021;&#12290; AMC &#36890;&#36807;&#40723;&#21169;&#22522;&#20110;&#26799;&#24230;&#30340;&#35299;&#37322;&#25513;&#30721;&#65292;&#22312;&#21253;&#21547;&#27492;&#31867;&#27880;&#37322;&#30340;&#22270;&#20687;&#20013;&#65292;&#25226;&#23427;&#20204;&#30340;&#27880;&#24847;&#21147;&#20998;&#25968;&#20027;&#35201;&#38598;&#20013;&#22312;&#27880;&#37322;&#30340;&#24863;&#20852;&#36259;&#21306;&#22495;&#20869;&#12290;&#29305;&#21035;&#22320;&#65292;&#19968;&#20010;&#22312;&#26631;&#20934;&#35270;&#35273;-&#35821;&#35328;&#24314;&#27169;&#30446;&#26631;&#20043;&#19978;&#29992; AMC &#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#22312; Flickr30k &#35270;&#35273; grounding &#22522;&#20934;&#27979;&#35797;&#20013;&#33719;&#24471;&#20102;86.59%&#30340;&#26368;&#26032;&#32467;&#26524;&#65292;&#30456;&#27604;&#26368;&#20339;&#32467;&#26524;&#33719;&#24471;&#20102;5.48%&#30340;&#32477;&#23545;&#25552;&#21319;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24050;&#24314;&#31435;&#30340;&#25351;&#20195;&#34920;&#36798;&#29702;&#35299;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#31168;&#65292;&#36824;&#25552;&#20379;&#20102;&#39069;&#22806;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a margin-based loss for vision-language model pretraining that encourages gradient-based explanations that are consistent with region-level annotations. We refer to this objective as Attention Mask Consistency (AMC) and demonstrate that it produces superior visual grounding performance compared to models that rely instead on region-level annotations for explicitly training an object detector such as Faster R-CNN. AMC works by encouraging gradient-based explanation masks that focus their attention scores mostly within annotated regions of interest for images that contain such annotations. Particularly, a model trained with AMC on top of standard vision-language modeling objectives obtains a state-of-the-art accuracy of 86.59% in the Flickr30k visual grounding benchmark, an absolute improvement of 5.48% when compared to the best previous model. Our approach also performs exceedingly well on established benchmarks for referring expression comprehension and offers the added bene
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#30740;&#23545;&#29616;&#26377;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#26631;&#35760;&#20462;&#25913;&#23545;&#25239;&#25915;&#20987;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#27604;&#36739;&#65292;&#24182;&#26088;&#22312;&#25351;&#23548;&#26032;&#30340;&#30740;&#31350;&#24182;&#25512;&#21160;&#36827;&#19968;&#27493;&#30340;&#25915;&#20987;&#32452;&#20214;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2103.00676</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#26631;&#35760;&#20462;&#25913;&#23545;&#25239;&#25915;&#20987;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Token-Modification Adversarial Attacks for Natural Language Processing: A Survey. (arXiv:2103.00676v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.00676
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#30740;&#23545;&#29616;&#26377;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#26631;&#35760;&#20462;&#25913;&#23545;&#25239;&#25915;&#20987;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#27604;&#36739;&#65292;&#24182;&#26088;&#22312;&#25351;&#23548;&#26032;&#30340;&#30740;&#31350;&#24182;&#25512;&#21160;&#36827;&#19968;&#27493;&#30340;&#25915;&#20987;&#32452;&#20214;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22312;&#26377;&#24456;&#22810;&#38024;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#30340;&#23545;&#25239;&#25915;&#20987;&#12290;&#20854;&#20013;&#65292;&#32477;&#22823;&#22810;&#25968;&#25915;&#20987;&#36890;&#36807;&#20462;&#25913;&#21333;&#20010;&#25991;&#26723;&#26631;&#35760;&#26469;&#23454;&#29616;&#25104;&#21151;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#26631;&#35760;&#20462;&#25913;&#25915;&#20987;&#12290;&#27599;&#31181;&#26631;&#35760;&#20462;&#25913;&#25915;&#20987;&#37117;&#30001;&#19968;&#32452;&#29305;&#23450;&#30340;&#22522;&#26412;&#32452;&#20214;&#23450;&#20041;&#65292;&#20363;&#22914;&#23545;&#25915;&#20987;&#32773;&#30340;&#32422;&#26463;&#25110;&#29305;&#23450;&#30340;&#25628;&#32034;&#31639;&#27861;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#23545;&#29616;&#26377;&#30340;&#26631;&#35760;&#20462;&#25913;&#25915;&#20987;&#36827;&#34892;&#35843;&#26597;&#65292;&#24182;&#25552;&#21462;&#27599;&#31181;&#25915;&#20987;&#30340;&#32452;&#20214;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#19982;&#25915;&#20987;&#26080;&#20851;&#30340;&#26694;&#26550;&#26469;&#32452;&#32455;&#25105;&#20204;&#30340;&#35843;&#30740;&#65292;&#20174;&#32780;&#23545;&#35813;&#39046;&#22495;&#36827;&#34892;&#26377;&#25928;&#30340;&#20998;&#31867;&#65292;&#24182;&#26041;&#20415;&#36827;&#34892;&#32452;&#20214;&#27604;&#36739;&#12290;&#26412;&#35843;&#30740;&#26088;&#22312;&#25351;&#23548;&#26032;&#30340;&#30740;&#31350;&#20154;&#21592;&#36827;&#20837;&#36825;&#19968;&#39046;&#22495;&#65292;&#24182;&#25512;&#21160;&#23545;&#20110;&#20010;&#20307;&#25915;&#20987;&#32452;&#20214;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
There are now many adversarial attacks for natural language processing systems. Of these, a vast majority achieve success by modifying individual document tokens, which we call here a token-modification attack. Each token-modification attack is defined by a specific combination of fundamental components, such as a constraint on the adversary or a particular search algorithm. Motivated by this observation, we survey existing token-modification attacks and extract the components of each. We use an attack-independent framework to structure our survey which results in an effective categorisation of the field and an easy comparison of components. This survey aims to guide new researchers to this field and spark further research into individual attack components.
&lt;/p&gt;</description></item></channel></rss>