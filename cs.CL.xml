<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26032;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#23454;&#20307;&#38142;&#25509;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#22312;&#24179;&#22343;F1&#20998;&#25968;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#29305;&#23450;&#20219;&#21153;EL&#27169;&#22411;8.51&#20998;&#12290;</title><link>http://arxiv.org/abs/2305.17337</link><description>&lt;p&gt;
&#29992;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#22810;&#27169;&#24577;&#23454;&#20307;&#38142;&#25509;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Diverse-Modal Entity Linking with Generative Models. (arXiv:2305.17337v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17337
&lt;/p&gt;
&lt;p&gt;
&#26032;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#23454;&#20307;&#38142;&#25509;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#22312;&#24179;&#22343;F1&#20998;&#25968;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#29305;&#23450;&#20219;&#21153;EL&#27169;&#22411;8.51&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#21487;&#20197;&#29992;&#19981;&#21516;&#30340;&#26684;&#24335;&#26469;&#34920;&#36798;&#65292;&#22914;&#25991;&#26412;&#12289;&#22270;&#20687;&#25110;&#34920;&#26684;&#20013;&#30340;&#21015;&#21517;&#21644;&#21333;&#20803;&#26684;&#20540;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#23454;&#20307;&#38142;&#25509;&#65288;EL&#65289;&#27169;&#22411;&#22312;&#27599;&#31181;&#27169;&#24335;&#37197;&#32622;&#19978;&#37117;&#34920;&#29616;&#20986;&#33394;&#65292;&#20363;&#22914;&#20165;&#25991;&#26412;EL&#12289;&#35270;&#35273;&#23450;&#20301;&#25110;&#27169;&#24335;&#38142;&#25509;&#65292;&#20294;&#20026;&#22810;&#31181;&#27169;&#24335;&#37197;&#32622;&#35774;&#35745;&#32479;&#19968;&#27169;&#22411;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#23558;&#21508;&#31181;&#27169;&#24577;&#37197;&#32622;&#32467;&#21512;&#36215;&#26469;&#65292;&#25105;&#20204;&#20174;&#29616;&#26377;EL&#25968;&#25454;&#38598;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#34920;&#26684;&#19977;&#31181;&#27169;&#24577;&#30340;&#22810;&#27169;&#24577;EL&#65288;DMEL&#65289;&#22522;&#20934;&#27979;&#35797;&#12290;&#20026;&#20102;&#35299;&#20915;DMEL&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29983;&#25104;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;GDMM&#65289;&#65292;&#36981;&#24490;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#33539;&#20363;&#12290;&#23558;\Model&#29992;&#20016;&#23500;&#30340;&#35821;&#26009;&#24211;&#39044;&#35757;&#32451;&#21487;&#20197;&#22312;&#19981;&#20445;&#23384;&#25972;&#20010;KB&#36827;&#34892;&#25512;&#29702;&#30340;&#24773;&#20917;&#19979;&#20026;DMEL&#26500;&#24314;&#22362;&#23454;&#30340;&#22522;&#30784;&#12290;&#24494;&#35843;GDMM&#21487;&#20197;&#26500;&#24314;&#26356;&#24378;&#22823;&#30340;DMEL&#22522;&#32447;&#65292;&#22312;&#24179;&#22343;F1&#20998;&#25968;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#29305;&#23450;&#20219;&#21153;EL&#27169;&#22411;8.51&#20998;&#12290;&#27492;&#22806;&#65292;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#38169;&#35823;&#20998;&#26512;&#65292;&#31361;&#20986;&#20102;&#23558;&#22810;&#27169;&#24577;&#23454;&#20307;&#32763;&#35793;&#21644;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#20248;&#21183;&#25152;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entities can be expressed in diverse formats, such as texts, images, or column names and cell values in tables. While existing entity linking (EL) models work well on per modality configuration, such as text-only EL, visual grounding, or schema linking, it is more challenging to design a unified model for diverse modality configurations. To bring various modality configurations together, we constructed a benchmark for diverse-modal EL (DMEL) from existing EL datasets, covering all three modalities including text, image, and table. To approach the DMEL task, we proposed a generative diverse-modal model (GDMM) following a multimodal-encoder-decoder paradigm. Pre-training \Model with rich corpora builds a solid foundation for DMEL without storing the entire KB for inference. Fine-tuning GDMM builds a stronger DMEL baseline, outperforming state-of-the-art task-specific EL models by 8.51 F1 score on average. Additionally, extensive error analyses are conducted to highlight the challenges of
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#38646;&#38454;&#20248;&#21270;&#22120;&#65292;&#21487;&#20197;&#20351;&#29992;&#19982;&#25512;&#29702;&#30456;&#21516;&#30340;&#23384;&#20648;&#31354;&#38388;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#21487;&#20197;&#22312;&#22823;&#35268;&#27169;&#27169;&#22411;&#19979;&#26356;&#24555;&#22320;&#20248;&#21270;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.17333</link><description>&lt;p&gt;
&#21482;&#20351;&#29992;&#21069;&#21521;&#20256;&#36882;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Fine-Tuning Language Models with Just Forward Passes. (arXiv:2305.17333v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#38646;&#38454;&#20248;&#21270;&#22120;&#65292;&#21487;&#20197;&#20351;&#29992;&#19982;&#25512;&#29702;&#30456;&#21516;&#30340;&#23384;&#20648;&#31354;&#38388;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#21487;&#20197;&#22312;&#22823;&#35268;&#27169;&#27169;&#22411;&#19979;&#26356;&#24555;&#22320;&#20248;&#21270;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#30340;&#22686;&#22823;&#65292;&#21453;&#21521;&#20256;&#25773;&#38656;&#35201;&#30340;&#23384;&#20648;&#31354;&#38388;&#25968;&#37327;&#21464;&#24471;&#36807;&#39640;&#12290;&#38646;&#38454;&#65288;ZO&#65289;&#26041;&#27861;&#29702;&#35770;&#19978;&#20165;&#20351;&#29992;&#20004;&#27425;&#21069;&#21521;&#20256;&#36882;&#23601;&#21487;&#20197;&#20272;&#35745;&#26799;&#24230;&#65292;&#20294;&#36890;&#24120;&#24773;&#20917;&#19979;&#23545;&#22823;&#22411;&#27169;&#22411;&#36827;&#34892;&#20248;&#21270;&#30340;&#36895;&#24230;&#38750;&#24120;&#24930;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#38646;&#38454;&#20248;&#21270;&#22120;&#65288;MeZO&#65289;&#65292;&#23558;&#32463;&#20856;&#30340;ZO-SGD&#26041;&#27861;&#36866;&#24212;&#20110;&#21407;&#22320;&#25805;&#20316;&#65292;&#20174;&#32780;&#20351;&#29992;&#19982;&#25512;&#29702;&#30456;&#21516;&#30340;&#23384;&#20648;&#31354;&#38388;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#12290;&#20363;&#22914;&#65292;&#21482;&#20351;&#29992;&#19968;&#24352;A100 80GB GPU&#65292;MeZO&#23601;&#21487;&#20197;&#35757;&#32451;&#19968;&#20010;300&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;&#65292;&#32780;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#21487;&#20197;&#22312;&#30456;&#21516;&#30340;&#39044;&#31639;&#19979;&#20165;&#35757;&#32451;&#19968;&#20010;27&#20159;&#20010;&#21442;&#25968;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#27169;&#22411;&#31867;&#22411;&#65288;&#25513;&#30721;&#21644;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#65289;&#12289;&#27169;&#22411;&#35268;&#27169;&#65288;&#39640;&#36798;66B&#65289;&#21644;&#19979;&#28216;&#20219;&#21153;&#65288;&#20998;&#31867;&#12289;&#22810;&#39033;&#36873;&#25321;&#21644;&#29983;&#25104;&#65289;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#65288;1&#65289;MeZO&#26126;&#26174;&#20248;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#32447;&#24615;PR&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning language models (LMs) has yielded success on diverse downstream tasks, but as LMs grow in size, backpropagation requires a prohibitively large amount of memory. Zeroth-order (ZO) methods can in principle estimate gradients using only two forward passes but are theorized to be catastrophically slow for optimizing large models. In this work, we propose a memory-efficient zerothorder optimizer (MeZO), adapting the classical ZO-SGD method to operate in-place, thereby fine-tuning LMs with the same memory footprint as inference. For example, with a single A100 80GB GPU, MeZO can train a 30-billion parameter model, whereas fine-tuning with backpropagation can train only a 2.7B LM with the same budget. We conduct comprehensive experiments across model types (masked and autoregressive LMs), model scales (up to 66B), and downstream tasks (classification, multiple-choice, and generation). Our results demonstrate that (1) MeZO significantly outperforms in-context learning and linear pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22686;&#24378;&#36866;&#24212;&#26816;&#32034;&#22120;(AAR)&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#20174;&#24050;&#30693;&#30340;&#28304;LM&#20013;&#23398;&#20064;LM&#30340;&#20559;&#22909;&#65292;&#33021;&#22815;&#20197;&#36890;&#29992;&#25554;&#20214;&#30340;&#24418;&#24335;&#24110;&#21161;&#30446;&#26631;LM&#22312;&#19981;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.17331</link><description>&lt;p&gt;
&#22522;&#20110;&#22686;&#24378;&#30340;&#36866;&#24212;&#24615;&#26816;&#32034;&#22120;&#20197;&#36890;&#29992;&#25554;&#20214;&#30340;&#24418;&#24335;&#25552;&#39640;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In. (arXiv:2305.17331v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22686;&#24378;&#36866;&#24212;&#26816;&#32034;&#22120;(AAR)&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#20174;&#24050;&#30693;&#30340;&#28304;LM&#20013;&#23398;&#20064;LM&#30340;&#20559;&#22909;&#65292;&#33021;&#22815;&#20197;&#36890;&#29992;&#25554;&#20214;&#30340;&#24418;&#24335;&#24110;&#21161;&#30446;&#26631;LM&#22312;&#19981;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#22806;&#37096;&#20449;&#24687;&#24110;&#21161;&#35821;&#35328;&#27169;&#22411;(LMs)&#25191;&#34892;&#30693;&#35782;&#23494;&#38598;&#30340;&#20219;&#21153;&#12290;&#26816;&#32034;&#22686;&#24378;&#30340;&#20808;&#21069;&#24037;&#20316;&#36890;&#24120;&#32852;&#21512;&#24494;&#35843;&#26816;&#32034;&#22120;&#21644;LM&#65292;&#20351;&#23427;&#20204;&#32039;&#23494;&#32806;&#21512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#36890;&#29992;&#26816;&#32034;&#25554;&#20214;&#30340;&#26041;&#26696;&#65306;&#26816;&#32034;&#22120;&#26159;&#35201;&#24110;&#21161;&#30446;&#26631;LM&#30340;&#65292;&#36825;&#20123;LM&#21487;&#33021;&#20107;&#20808;&#19981;&#30693;&#36947;&#25110;&#26080;&#27861;&#19968;&#36215;&#24494;&#35843;&#12290;&#20026;&#20102;&#26816;&#32034;&#20986;&#23545;&#26410;&#35265;&#36807;&#30340;&#30446;&#26631;LM&#26377;&#29992;&#30340;&#25991;&#26723;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22686;&#24378;&#36866;&#24212;&#26816;&#32034;&#22120;(AAR)&#65292;&#23427;&#20174;&#24050;&#30693;&#30340;&#28304;LM&#20013;&#23398;&#20064;LM&#30340;&#20559;&#22909;&#12290;&#22312;MMLU&#21644;PopQA&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#29992;&#23567;&#22411;&#28304;LM&#35757;&#32451;&#30340;AAR&#33021;&#22815;&#26174;&#30528;&#25552;&#39640;&#20174;250M Flan-T5&#21040;175B InstructGPT&#33539;&#22260;&#20869;&#30340;&#26356;&#22823;&#30446;&#26631;LM&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#19981;&#21516;LM&#30340;&#20559;&#22909;&#37325;&#21472;&#65292;&#20351;&#24471;&#20197;&#21333;&#20010;&#28304;LM&#35757;&#32451;&#30340;AAR&#33021;&#22815;&#20316;&#20026;&#21508;&#31181;&#30446;&#26631;LM&#30340;&#36890;&#29992;&#25554;&#20214;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;...
&lt;/p&gt;
&lt;p&gt;
Retrieval augmentation can aid language models (LMs) in knowledge-intensive tasks by supplying them with external information. Prior works on retrieval augmentation usually jointly fine-tune the retriever and the LM, making them closely coupled. In this paper, we explore the scheme of generic retrieval plug-in: the retriever is to assist target LMs that may not be known beforehand or are unable to be fine-tuned together. To retrieve useful documents for unseen target LMs, we propose augmentation-adapted retriever (AAR), which learns LM's preferences obtained from a known source LM. Experiments on the MMLU and PopQA datasets demonstrate that our AAR trained with a small source LM is able to significantly improve the zero-shot generalization of larger target LMs ranging from 250M Flan-T5 to 175B InstructGPT. Further analysis indicates that the preferences of different LMs overlap, enabling AAR trained with a single source LM to serve as a generic plug-in for various target LMs. Our code 
&lt;/p&gt;</description></item><item><title>&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#29983;&#25104;&#22833;&#36133;&#30340;&#21407;&#22240;&#26159;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#23398;&#20064;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#35821;&#35328;&#19981;&#21464;&#34920;&#31034;&#26102;&#65292;&#20250;&#24433;&#21709;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#20934;&#30830;&#24615;&#65292;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#36890;&#36807;&#35268;&#33539;&#21270;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#24182;&#25552;&#39640;&#29983;&#25104;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.17325</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#29983;&#25104;&#22833;&#36133;&#65311;&#21407;&#22240;&#21450;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Why Does Zero-Shot Cross-Lingual Generation Fail? An Explanation and a Solution. (arXiv:2305.17325v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17325
&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#29983;&#25104;&#22833;&#36133;&#30340;&#21407;&#22240;&#26159;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#23398;&#20064;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#35821;&#35328;&#19981;&#21464;&#34920;&#31034;&#26102;&#65292;&#20250;&#24433;&#21709;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#20934;&#30830;&#24615;&#65292;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#36890;&#36807;&#35268;&#33539;&#21270;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#24182;&#25552;&#39640;&#29983;&#25104;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#36716;&#31227;&#26159;&#25351;&#22312;&#19968;&#31181;&#35821;&#35328;&#20013;&#35757;&#32451;&#22810;&#35821;&#35328;&#27169;&#22411;&#26469;&#25191;&#34892;&#20219;&#21153;&#65292;&#28982;&#21518;&#23558;&#20854;&#24212;&#29992;&#20110;&#21478;&#19968;&#31181;&#35821;&#35328;&#12290;&#34429;&#28982;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#36716;&#31227;&#26041;&#27861;&#22312;&#21508;&#31181;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20854;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#21017;&#19981;&#36275;&#65292;&#24182;&#19988;&#26377;&#26102;&#20250;&#36755;&#20986;&#38169;&#35823;&#30340;&#35821;&#35328;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24494;&#35843;&#36807;&#31243;&#23398;&#20064;&#35821;&#35328;&#19981;&#21464;&#34920;&#31034;&#30340;&#22909;&#22788;&#26159;&#20998;&#31867;&#20219;&#21153;&#20294;&#23545;&#20110;&#29983;&#25104;&#20219;&#21153;&#26377;&#23475;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#35268;&#33539;&#21270;&#27169;&#22411;&#65292;&#20351;&#20854;&#19981;&#20250;&#23398;&#20064;&#35821;&#35328;&#19981;&#21464;&#30340;&#34920;&#31034;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30446;&#26631;&#35821;&#35328;&#20013;&#36873;&#25321;&#19981;&#38656;&#35201;&#24320;&#21457;&#38598;&#30340;&#27169;&#22411;&#26816;&#26597;&#28857;&#30340;&#26041;&#27861;&#65292;&#20004;&#32773;&#37117;&#21487;&#20197;&#25552;&#39640;&#29983;&#25104;&#36136;&#37327;&#12290;&#23545;&#19977;&#20010;&#35821;&#20041;&#22810;&#26679;&#30340;&#29983;&#25104;&#20219;&#21153;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#20598;&#28982;&#32763;&#35793;&#38382;&#39064;&#20943;&#23569;&#20102;68&#65285;&#65292;&#24179;&#22343;&#25552;&#39640;&#20102;1.5&#30340;ROUGE-L&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-shot cross-lingual transfer is when a multilingual model is trained to perform a task in one language and then is applied to another language. Although the zero-shot cross-lingual transfer approach has achieved success in various classification tasks, its performance on natural language generation tasks falls short in quality and sometimes outputs an incorrect language. In our study, we show that the fine-tuning process learns language invariant representations, which is beneficial for classification tasks but harmful for generation tasks. Motivated by this, we propose a simple method to regularize the model from learning language invariant representations and a method to select model checkpoints without a development set in the target language, both resulting in better generation quality. Experiments on three semantically diverse generation tasks show that our method reduces the accidental translation problem by 68% and improves the ROUGE-L score by 1.5 on average.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;&#21542;&#23450;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;NeQA&#65292;&#20854;&#20013;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#21453;&#21521;&#32553;&#25918;&#12289;U&#22411;&#32553;&#25918;&#25110;&#27491;&#21521;&#32553;&#25918;&#65292;&#35299;&#20915;NeQA&#20381;&#36182;&#20110;&#38382;&#31572;&#21644;&#21542;&#23450;&#29702;&#35299;&#20004;&#20010;&#23376;&#20219;&#21153;&#65292;&#20854;&#32553;&#25918;&#36235;&#21183;&#30001;&#36825;&#20004;&#20010;&#23376;&#20219;&#21153;&#30340;&#32553;&#25918;&#36235;&#21183;&#32452;&#21512;&#24418;&#25104;&#12290;</title><link>http://arxiv.org/abs/2305.17311</link><description>&lt;p&gt;
&#36229;&#36234;&#27491;&#21521;&#32553;&#25918;&#65306;&#21542;&#23450;&#35821;&#23545;&#35821;&#35328;&#27169;&#22411;&#32553;&#25918;&#36235;&#21183;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Beyond Positive Scaling: How Negation Impacts Scaling Trends of Language Models. (arXiv:2305.17311v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;&#21542;&#23450;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;NeQA&#65292;&#20854;&#20013;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#21453;&#21521;&#32553;&#25918;&#12289;U&#22411;&#32553;&#25918;&#25110;&#27491;&#21521;&#32553;&#25918;&#65292;&#35299;&#20915;NeQA&#20381;&#36182;&#20110;&#38382;&#31572;&#21644;&#21542;&#23450;&#29702;&#35299;&#20004;&#20010;&#23376;&#20219;&#21153;&#65292;&#20854;&#32553;&#25918;&#36235;&#21183;&#30001;&#36825;&#20004;&#20010;&#23376;&#20219;&#21153;&#30340;&#32553;&#25918;&#36235;&#21183;&#32452;&#21512;&#24418;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#35777;&#26126;&#65292;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#27491;&#21521;&#32553;&#25918;&#65292;&#22312;&#22823;&#23567;&#12289;&#35745;&#31639;&#25110;&#25968;&#25454;&#26041;&#38754;&#25193;&#23637;&#27169;&#22411;&#20250;&#25552;&#39640;&#24615;&#33021;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21253;&#21547;&#21542;&#23450;&#38382;&#21477;&#30340;&#25968;&#25454;&#38598;NeQA&#65292;&#20854;&#20013;&#35821;&#35328;&#27169;&#22411;&#19981;&#20250;&#34920;&#29616;&#20986;&#31616;&#21333;&#30340;&#27491;&#21521;&#32553;&#25918;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#20219;&#21153;&#21487;&#20197;&#34920;&#29616;&#20986;&#21453;&#21521;&#32553;&#25918;&#12289;U&#24418;&#32553;&#25918;&#25110;&#27491;&#21521;&#32553;&#25918;&#65292;&#24182;&#19988;&#22312;&#20351;&#29992;&#26356;&#24378;&#22823;&#30340;&#25552;&#31034;&#26041;&#27861;&#25110;&#27169;&#22411;&#26063;&#32676;&#26102;&#65292;&#36825;&#19977;&#31181;&#32553;&#25918;&#36235;&#21183;&#20250;&#25353;&#29031;&#36825;&#20010;&#39034;&#24207;&#21457;&#29983;&#36716;&#21464;&#12290;&#25105;&#20204;&#20551;&#35774;&#35299;&#20915;NeQA&#20381;&#36182;&#20110;&#20004;&#20010;&#23376;&#20219;&#21153;&#65306;&#38382;&#31572;&#65288;&#20219;&#21153;1&#65289;&#21644;&#21542;&#23450;&#29702;&#35299;&#65288;&#20219;&#21153;2&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#20219;&#21153;1&#20855;&#26377;&#32447;&#24615;&#32553;&#25918;&#65292;&#32780;&#20219;&#21153;2&#20855;&#26377;S&#24418;&#32553;&#25918;&#65292;&#24182;&#20855;&#26377;&#19968;&#20010;&#32039;&#24613;&#30340;&#36716;&#25240;&#28857;&#65292;&#23558;&#36825;&#20004;&#20010;&#32553;&#25918;&#36235;&#21183;&#32452;&#21512;&#36215;&#26469;&#21363;&#21487;&#24471;&#20986;&#26368;&#32456;&#30340;NeQA&#32553;&#25918;&#36235;&#21183;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#20998;&#26512;&#35821;&#35328;&#27169;&#22411;&#22797;&#26434;&#32553;&#25918;&#36235;&#21183;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models have been shown to exhibit positive scaling, where performance improves as models are scaled up in terms of size, compute, or data. In this work, we introduce NeQA, a dataset consisting of questions with negation in which language models do not exhibit straightforward positive scaling. We show that this task can exhibit inverse scaling, U-shaped scaling, or positive scaling, and the three scaling trends shift in this order as we use more powerful prompting methods or model families. We hypothesize that solving NeQA depends on two subtasks: question answering (task 1) and negation understanding (task 2). We find that task 1 has linear scaling, while task 2 has sigmoid-shaped scaling with an emergent transition point, and composing these two scaling trends yields the final scaling trend of NeQA. Our work reveals and provides a way to analyze the complex scaling trends of language models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026; Chain-of-Thought Hub &#30340;&#24320;&#28304;&#35780;&#20272;&#22871;&#20214;&#65292;&#30446;&#30340;&#26159;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#12290;&#23427;&#26159;&#20026;&#20102;&#36861;&#36394;LLMs&#36827;&#23637;&#32780;&#32534;&#21046;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25512;&#29702;&#22522;&#20934;&#12290;&#30446;&#21069;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#35268;&#27169;&#19982;&#25512;&#29702;&#33021;&#21147;&#30456;&#20851;&#65292;&#32780; Claude-v1.3 &#26159;&#36804;&#20170;&#20026;&#27490;&#25512;&#29702;&#33021;&#21147;&#26368;&#24378;&#30340;LLM&#12290;</title><link>http://arxiv.org/abs/2305.17306</link><description>&lt;p&gt;
&#8220;Chain-of-Thought Hub: &#36830;&#32493;&#27979;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#34920;&#29616;&#30340;&#21162;&#21147;&#8221;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Thought Hub: A Continuous Effort to Measure Large Language Models' Reasoning Performance. (arXiv:2305.17306v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026; Chain-of-Thought Hub &#30340;&#24320;&#28304;&#35780;&#20272;&#22871;&#20214;&#65292;&#30446;&#30340;&#26159;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#12290;&#23427;&#26159;&#20026;&#20102;&#36861;&#36394;LLMs&#36827;&#23637;&#32780;&#32534;&#21046;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25512;&#29702;&#22522;&#20934;&#12290;&#30446;&#21069;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#35268;&#27169;&#19982;&#25512;&#29702;&#33021;&#21147;&#30456;&#20851;&#65292;&#32780; Claude-v1.3 &#26159;&#36804;&#20170;&#20026;&#27490;&#25512;&#29702;&#33021;&#21147;&#26368;&#24378;&#30340;LLM&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#23427;&#20204;&#30340;&#35780;&#20272;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#20294;&#20063;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102; Chain-of-Thought Hub&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#35780;&#20272;&#22871;&#20214;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#20043;&#25152;&#20197;&#23545;&#36825;&#20010;&#35774;&#32622;&#24863;&#20852;&#36259;&#65292;&#26159;&#22240;&#20026; (1) &#20174; GPT &#21644; PaLM &#27169;&#22411;&#23478;&#26063;&#30340;&#34892;&#20026;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22797;&#26434;&#30340;&#25512;&#29702;&#24456;&#21487;&#33021;&#26159;&#19968;&#20010;&#26356;&#24369;&#21644;&#26356;&#24378;&#30340;LLMs&#20043;&#38388;&#30340;&#20851;&#38190;&#21306;&#21035;&#65307; (2) &#25105;&#20204;&#39044;&#35265;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#25104;&#20026;&#19979;&#19968;&#20195;&#35745;&#31639;&#24179;&#21488;&#65292;&#24182;&#20419;&#36827;&#22522;&#20110;LLM&#30340;&#26032;&#24212;&#29992;&#30340;&#29983;&#24577;&#31995;&#32479;&#65292;&#36825;&#33258;&#28982;&#38656;&#35201;&#22522;&#30784;&#27169;&#22411;&#25191;&#34892;&#24120;&#24120;&#28041;&#21450;&#35821;&#35328;&#21644;&#36923;&#36753;&#25805;&#20316;&#32452;&#21512;&#30340;&#22797;&#26434;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#32534;&#21046;&#19968;&#31995;&#21015;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25512;&#29702;&#22522;&#20934;&#65292;&#20197;&#36319;&#36394;LLMs&#30340;&#36827;&#23637;&#12290;&#25105;&#20204;&#30446;&#21069;&#30340;&#32467;&#26524;&#34920;&#26126;&#65306;(1) &#27169;&#22411;&#35268;&#27169;&#26174;&#28982;&#19982;&#25512;&#29702;&#33021;&#21147;&#30456;&#20851;&#65307;(2) &#25130;&#33267;2023&#24180;5&#26376;&#65292;Claude-v1.3 &#26159;&#36804;&#20170;&#20026;&#27490;&#25512;&#29702;&#33021;&#21147;&#26368;&#24378;&#30340;LLM &#12290;&#8221;
&lt;/p&gt;
&lt;p&gt;
As large language models (LLMs) are continuously being developed, their evaluation becomes increasingly important yet challenging. This work proposes Chain-of-Thought Hub, an open-source evaluation suite on the multi-step reasoning capabilities of large language models. We are interested in this setting for two reasons: (1) from the behavior of GPT and PaLM model family, we observe that complex reasoning is likely to be a key differentiator between weaker and stronger LLMs; (2) we envisage large language models to become the next-generation computational platform and foster an ecosystem of LLM-based new applications, this naturally requires the foundation models to perform complex tasks that often involve the composition of linguistic and logical operations. Our approach is to compile a suite of challenging reasoning benchmarks to track the progress of LLMs. Our current results show that: (1) model scale clearly correlates with reasoning capabilities; (2) As of May 2023, Claude-v1.3 an
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22806;&#37096;&#35821;&#35328;&#27169;&#22411;&#22312;&#20998;&#35299;&#31070;&#32463;&#20256;&#36755;&#22120;&#20013;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#21516;&#26102;&#35777;&#26126;&#19982;&#27973;&#23618;&#34701;&#21512;&#30456;&#27604;&#32447;&#24615;&#25554;&#20540;&#39044;&#27979;&#36755;&#20986;&#19982;&#31070;&#32463;&#21644;n-gram&#22806;&#37096;LM&#30340;&#32467;&#21512;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#24179;&#22343;18%&#30340;WERR&#22686;&#30410;&#65292;&#22312;&#19968;&#20010;&#23454;&#20307;&#20016;&#23500;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#33719;&#24471;&#39640;&#36798;60%&#30340;WERR&#28155;&#21152;&#24615;&#22686;&#30410;&#12290;</title><link>http://arxiv.org/abs/2305.17304</link><description>&lt;p&gt;
&#22806;&#37096;&#35821;&#35328;&#27169;&#22411;&#22312;&#20998;&#35299;&#31070;&#32463;&#20256;&#36755;&#22120;&#20013;&#30340;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
External Language Model Integration for Factorized Neural Transducers. (arXiv:2305.17304v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17304
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22806;&#37096;&#35821;&#35328;&#27169;&#22411;&#22312;&#20998;&#35299;&#31070;&#32463;&#20256;&#36755;&#22120;&#20013;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#21516;&#26102;&#35777;&#26126;&#19982;&#27973;&#23618;&#34701;&#21512;&#30456;&#27604;&#32447;&#24615;&#25554;&#20540;&#39044;&#27979;&#36755;&#20986;&#19982;&#31070;&#32463;&#21644;n-gram&#22806;&#37096;LM&#30340;&#32467;&#21512;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#24179;&#22343;18%&#30340;WERR&#22686;&#30410;&#65292;&#22312;&#19968;&#20010;&#23454;&#20307;&#20016;&#23500;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#33719;&#24471;&#39640;&#36798;60%&#30340;WERR&#28155;&#21152;&#24615;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#24102;&#26377;&#22806;&#37096;&#35821;&#35328;&#27169;&#22411;&#30340;&#20998;&#35299;&#31070;&#32463;&#20256;&#36755;&#22120;&#65288;FNT&#65289;&#30340;&#36866;&#24212;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19982;&#27973;&#23618;&#34701;&#21512;&#30456;&#27604;&#65292;&#32447;&#24615;&#25554;&#20540;&#39044;&#27979;&#36755;&#20986;&#19982;&#31070;&#32463;&#21644;n-gram&#22806;&#37096;LM&#30340;&#32467;&#21512;&#26126;&#26174;&#22686;&#21152;&#20102;&#20215;&#20540;&#65292;&#20174;&#32780;&#30830;&#35748;&#20102;FNT&#24378;&#21046;&#39044;&#27979;&#22120;&#20687;&#24120;&#35268;&#35821;&#35328;&#27169;&#22411;&#19968;&#26679;&#24037;&#20316;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22522;&#20110;&#31867;&#21035;&#30340;n-gram&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#21040;FNT&#26694;&#26550;&#20013;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#31867;&#20284;&#20110;&#28151;&#21512;&#35774;&#32622;&#30340;&#20934;&#30830;&#24615;&#25552;&#39640;&#12290;&#36890;&#36807;&#35789;&#27719;&#36866;&#24212;&#65292;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#26174;&#31034;&#24179;&#22343;18%&#30340;WERR&#22686;&#30410;&#65292;&#24182;&#19988;&#22312;&#19968;&#20010;&#23454;&#20307;&#20016;&#23500;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#31867;&#21035;n-gram&#21644;&#31070;&#32463;LM&#30340;&#32467;&#21512;&#33719;&#24471;&#39640;&#36798;60%&#30340;WERR&#28155;&#21152;&#24615;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an adaptation method for factorized neural transducers (FNT) with external language models. We demonstrate that both neural and n-gram external LMs add significantly more value when linearly interpolated with predictor output compared to shallow fusion, thus confirming that FNT forces the predictor to act like regular language models. Further, we propose a method to integrate class-based n-gram language models into FNT framework resulting in accuracy gains similar to a hybrid setup. We show average gains of 18% WERR with lexical adaptation across various scenarios and additive gains of up to 60% WERR in one entity-rich scenario through a combination of class-based n-gram and neural LMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#22522;&#20110;&#33756;&#35889;&#30340;&#23545;&#35805;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#36741;&#21161;&#23376;&#20219;&#21153;&#65292;&#21363;&#29992;&#25143;&#24847;&#22270;&#26816;&#27979;&#21644;&#25351;&#20196;&#29366;&#24577;&#36319;&#36394;&#65292;&#24182;&#35777;&#26126;&#36825;&#20123;&#20219;&#21153;&#21487;&#20197;&#24110;&#21161;&#21709;&#24212;&#29983;&#25104;&#27169;&#22411;&#35299;&#20915;&#25351;&#20196;&#39034;&#24207;&#38169;&#35823;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.17280</link><description>&lt;p&gt;
&#22522;&#20110;&#33756;&#35889;&#30340;&#23545;&#35805;&#20013;&#30340;&#25913;&#36827;&#25351;&#20196;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Improved Instruction Ordering in Recipe-Grounded Conversation. (arXiv:2305.17280v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17280
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22522;&#20110;&#33756;&#35889;&#30340;&#23545;&#35805;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#36741;&#21161;&#23376;&#20219;&#21153;&#65292;&#21363;&#29992;&#25143;&#24847;&#22270;&#26816;&#27979;&#21644;&#25351;&#20196;&#29366;&#24577;&#36319;&#36394;&#65292;&#24182;&#35777;&#26126;&#36825;&#20123;&#20219;&#21153;&#21487;&#20197;&#24110;&#21161;&#21709;&#24212;&#29983;&#25104;&#27169;&#22411;&#35299;&#20915;&#25351;&#20196;&#39034;&#24207;&#38169;&#35823;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#25351;&#20196;&#23545;&#35805;&#20219;&#21153;&#65292;&#24182;&#32858;&#28966;&#20110;&#28921;&#39274;&#39046;&#22495;&#12290;&#20998;&#26512;GPT-J&#27169;&#22411;&#29983;&#25104;&#30340;&#36755;&#20986;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#33756;&#35889;&#23545;&#35805;&#31995;&#32479;&#30340;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#22914;&#20309;&#20197;&#27491;&#30830;&#30340;&#39034;&#24207;&#25552;&#20379;&#25351;&#20196;&#12290;&#25105;&#20204;&#20551;&#35774;&#36825;&#26159;&#30001;&#20110;&#27169;&#22411;&#32570;&#20047;&#29702;&#35299;&#29992;&#25143;&#24847;&#22270;&#21644;&#26080;&#27861;&#36319;&#36394;&#25351;&#20196;&#29366;&#24577;&#65288;&#21363;&#26368;&#21518;&#19968;&#27493;&#26159;&#21738;&#20010;&#25351;&#20196;&#65289;&#30340;&#33021;&#21147;&#23548;&#33268;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25506;&#32034;&#20004;&#20010;&#36741;&#21161;&#23376;&#20219;&#21153;&#8212;&#8212;&#29992;&#25143;&#24847;&#22270;&#26816;&#27979;&#21644;&#25351;&#20196;&#29366;&#24577;&#36319;&#36394;&#65292;&#20197;&#25903;&#25345;&#25913;&#36827;&#25351;&#20196;&#22522;&#30784;&#30340;&#21709;&#24212;&#29983;&#25104;&#12290;&#22312;&#25105;&#20204;&#26032;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;ChattyChef&#19978;&#36827;&#34892;&#23454;&#39564;&#34920;&#26126;&#65292;&#34701;&#20837;&#29992;&#25143;&#24847;&#22270;&#21644;&#25351;&#20196;&#29366;&#24577;&#20449;&#24687;&#26377;&#21161;&#20110;&#21709;&#24212;&#29983;&#25104;&#27169;&#22411;&#20943;&#36731;&#39034;&#24207;&#38169;&#35823;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#30740;&#31350;ChatGPT&#26159;&#21542;&#23436;&#20840;&#35299;&#20915;&#20102;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#20998;&#26512;&#20854;&#36755;&#20986;&#24182;&#21457;&#29616;&#23427;&#20063;&#29359;&#20102;&#38169;&#35823;&#65288;&#32422;&#21344;&#21709;&#24212;&#30340;10.7%&#65289;&#65292;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the task of instructional dialogue and focus on the cooking domain. Analyzing the generated output of the GPT-J model, we reveal that the primary challenge for a recipe-grounded dialog system is how to provide the instructions in the correct order. We hypothesize that this is due to the model's lack of understanding of user intent and inability to track the instruction state (i.e., which step was last instructed). Therefore, we propose to explore two auxiliary subtasks, namely User Intent Detection and Instruction State Tracking, to support Response Generation with improved instruction grounding. Experimenting with our newly collected dataset, ChattyChef, shows that incorporating user intent and instruction state information helps the response generation model mitigate the incorrect order issue. Furthermore, to investigate whether ChatGPT has completely solved this task, we analyze its outputs and find that it also makes mistakes (10.7% of the responses), about 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#27493;&#28369;&#21160;&#31383;&#21475;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#25991;&#26723;&#35299;&#26512;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#65292;&#21033;&#29992;&#28304;-&#30446;&#26631;&#23545;&#40784;&#24182;&#32422;&#26463;&#35299;&#30721;&#20197;&#20445;&#35777;&#37325;&#21472;&#31383;&#21475;&#30340;&#21516;&#27493;&#24615;&#21644;&#19968;&#33268;&#24615;&#65292;&#22312;AMR 3.0&#30340;&#35780;&#20272;&#20013;&#23637;&#31034;&#20986;&#20102;&#39640;&#36136;&#37327;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.17273</link><description>&lt;p&gt;
&#28369;&#21160;&#12289;&#32422;&#26463;&#12289;&#35299;&#26512;&#12289;&#37325;&#22797;&#65306;&#36866;&#29992;&#20110;&#25991;&#26723; AMR &#35299;&#26512;&#30340;&#21516;&#27493;&#28369;&#21160;&#31383;&#21475;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Slide, Constrain, Parse, Repeat: Synchronous SlidingWindows for Document AMR Parsing. (arXiv:2305.17273v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#27493;&#28369;&#21160;&#31383;&#21475;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#25991;&#26723;&#35299;&#26512;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#65292;&#21033;&#29992;&#28304;-&#30446;&#26631;&#23545;&#40784;&#24182;&#32422;&#26463;&#35299;&#30721;&#20197;&#20445;&#35777;&#37325;&#21472;&#31383;&#21475;&#30340;&#21516;&#27493;&#24615;&#21644;&#19968;&#33268;&#24615;&#65292;&#22312;AMR 3.0&#30340;&#35780;&#20272;&#20013;&#23637;&#31034;&#20986;&#20102;&#39640;&#36136;&#37327;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28369;&#21160;&#31383;&#21475;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#22788;&#29702;&#36229;&#36807;Transformer&#36755;&#20837;&#31383;&#21475;&#22823;&#23567;&#30340;&#19978;&#19979;&#25991;&#30340;&#20248;&#32654;&#26041;&#24335;&#65292;&#20363;&#22914;&#22788;&#29702;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#12290;&#26412;&#25991;&#23558;&#36825;&#31181;&#26041;&#27861;&#25193;&#23637;&#21040;&#25991;&#26723;&#35299;&#26512;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#20013;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#36716;&#31227;&#21477;&#27861;&#20998;&#26512;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#36890;&#36807;&#22312;&#28304;&#21644;&#30446;&#26631;&#20043;&#38388;&#23454;&#29616;&#21516;&#27493;&#28369;&#21160;&#31383;&#21475;&#26469;&#23454;&#29616;&#35299;&#26512;&#22120;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#26426;&#26500;&#21270;BART&#19978;&#25193;&#23637;&#26469;&#24320;&#21457;&#25991;&#26723;&#32423;AMR&#30340;oracle&#21644;&#35299;&#26512;&#22120;&#65292;&#20197;&#21033;&#29992;&#28304;-&#30446;&#26631;&#23545;&#40784;&#24182;&#32422;&#26463;&#35299;&#30721;&#20197;&#20445;&#35777;&#37325;&#21472;&#31383;&#21475;&#30340;&#21516;&#27493;&#24615;&#21644;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#65288;AMR&#65289;3.0&#35821;&#26009;&#24211;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;oracle&#21644;&#35299;&#26512;&#22120;&#12290;&#22312;AMR 3.0&#30340;&#22810;&#21477;&#23376;&#24320;&#21457;&#38598;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#36716;&#31227;oracle&#20165;&#20002;&#22833;&#20102;8&#65285;&#30340;&#37329;&#21477;&#38469;&#38142;&#25509;&#65292;&#23613;&#31649;&#20351;&#29992;&#28369;&#21160;&#31383;&#21475;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#36825;&#31181;&#26041;&#27861;&#20063;&#20135;&#29983;&#20102;&#19968;&#20010;&#20855;&#26377;&#21487;&#31649;&#29702;&#20869;&#23384;&#35201;&#27714;&#30340;&#39640;&#36136;&#37327;&#25991;&#26723;&#32423;&#35299;&#26512;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
The sliding window approach provides an elegant way to handle contexts of sizes larger than the Transformer's input window, for tasks like language modeling. Here we extend this approach to the sequence-to-sequence task of document parsing. For this, we exploit recent progress in transition-based parsing to implement a parser with synchronous sliding windows over source and target. We develop an oracle and a parser for document-level AMR by expanding on Structured-BART such that it leverages source-target alignments and constrains decoding to guarantee synchronicity and consistency across overlapping windows. We evaluate our oracle and parser using the Abstract Meaning Representation (AMR) parsing 3.0 corpus. On the Multi-Sentence development set of AMR 3.0, we show that our transition oracle loses only 8\% of the gold cross-sentential links despite using a sliding window. In practice, this approach also results in a high-quality document-level parser with manageable memory requirement
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38544;&#21947;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35757;&#32451;&#38598;&#20013;&#30340;&#23383;&#38754;&#27880;&#37322;&#36827;&#34892;&#22522;&#26412;&#21547;&#20041;&#24314;&#27169;&#24182;&#23558;&#20854;&#19982;&#19978;&#19979;&#25991;&#21547;&#20041;&#36827;&#34892;&#27604;&#36739;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#35782;&#21035;&#38544;&#21947;&#65292;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;1.0&#65285;&#12290;</title><link>http://arxiv.org/abs/2305.17268</link><description>&lt;p&gt;
&#36890;&#36807;&#26174;&#24335;&#22522;&#26412;&#21547;&#20041;&#24314;&#27169;&#26816;&#27979;&#38544;&#21947;
&lt;/p&gt;
&lt;p&gt;
Metaphor Detection via Explicit Basic Meanings Modelling. (arXiv:2305.17268v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17268
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38544;&#21947;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35757;&#32451;&#38598;&#20013;&#30340;&#23383;&#38754;&#27880;&#37322;&#36827;&#34892;&#22522;&#26412;&#21547;&#20041;&#24314;&#27169;&#24182;&#23558;&#20854;&#19982;&#19978;&#19979;&#25991;&#21547;&#20041;&#36827;&#34892;&#27604;&#36739;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#35782;&#21035;&#38544;&#21947;&#65292;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;1.0&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#21947;&#26816;&#27979;&#20013;&#30340;&#19968;&#20010;&#26174;&#33879;&#36235;&#21183;&#26159;&#37319;&#29992;&#35821;&#35328;&#23398;&#29702;&#35770;&#65292;&#22914;&#38544;&#21947;&#35782;&#21035;&#31243;&#24207;&#65288;MIP&#65289;&#65292;&#29992;&#20110;&#27169;&#22411;&#26550;&#26500;&#35774;&#35745;&#12290;&#34429;&#28982;MIP&#26126;&#30830;&#23450;&#20041;&#20102;&#35789;&#27719;&#21333;&#20301;&#30340;&#38544;&#21947;&#24615;&#26159;&#22522;&#20110;&#20854;&#8220;&#19978;&#19979;&#25991;&#21547;&#20041;&#8221;&#21644;&#8220;&#22522;&#26412;&#21547;&#20041;&#8221;&#20043;&#38388;&#30340;&#23545;&#27604;&#26469;&#30830;&#23450;&#30340;&#65292;&#20294;&#29616;&#26377;&#30340;&#24037;&#20316;&#24182;&#27809;&#26377;&#20005;&#26684;&#36981;&#24490;&#36825;&#20010;&#21407;&#21017;&#65292;&#36890;&#24120;&#20351;&#29992;&#8220;&#32858;&#21512;&#21547;&#20041;&#8221;&#26469;&#36817;&#20284;&#20110;&#30446;&#26631;&#35789;&#30340;&#22522;&#26412;&#21547;&#20041;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38544;&#21947;&#26816;&#27979;&#26041;&#27861;&#65292;&#23427;&#22522;&#20110;&#35757;&#32451;&#38598;&#20013;&#30340;&#23383;&#38754;&#27880;&#37322;&#23545;&#21333;&#35789;&#30340;&#22522;&#26412;&#21547;&#20041;&#36827;&#34892;&#24314;&#27169;&#65292;&#28982;&#21518;&#23558;&#20854;&#19982;&#30446;&#26631;&#21477;&#23376;&#20013;&#30340;&#19978;&#19979;&#25991;&#21547;&#20041;&#36827;&#34892;&#27604;&#36739;&#20197;&#35782;&#21035;&#38544;&#21947;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;F1&#24471;&#20998;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;1.0&#65285;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#24615;&#33021;&#29978;&#33267;&#36798;&#21040;&#20102;&#24102;&#26377;&#22522;&#26412;&#27880;&#37322;&#30340;VUA18&#22522;&#20934;&#27979;&#35797;&#30340;&#29702;&#35770;&#19978;&#38480;&#65292;&#36825;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
One noticeable trend in metaphor detection is the embrace of linguistic theories such as the metaphor identification procedure (MIP) for model architecture design. While MIP clearly defines that the metaphoricity of a lexical unit is determined based on the contrast between its \textit{contextual meaning} and its \textit{basic meaning}, existing work does not strictly follow this principle, typically using the \textit{aggregated meaning} to approximate the basic meaning of target words. In this paper, we propose a novel metaphor detection method, which models the basic meaning of the word based on literal annotation from the training set, and then compares this with the contextual meaning in a target sentence to identify metaphors. Empirical results show that our method outperforms the state-of-the-art method significantly by 1.0\% in F1 score. Moreover, our performance even reaches the theoretical upper bound on the VUA18 benchmark for targets with basic annotations, which demonstrate
&lt;/p&gt;</description></item><item><title>CODET&#26159;&#19968;&#20010;&#23545;&#27604;&#26041;&#35328;&#30340;&#35780;&#20272;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#22312;&#22788;&#29702;&#26041;&#35328;&#21464;&#20307;&#26102;&#30340;&#34920;&#29616;&#65292;&#35813;&#22522;&#20934;&#27979;&#35797;&#21253;&#21547;&#20061;&#31181;&#19981;&#21516;&#35821;&#35328;&#30340;882&#20010;&#19981;&#21516;&#21464;&#20307;&#12290;</title><link>http://arxiv.org/abs/2305.17267</link><description>&lt;p&gt;
CODET&#65306;&#26426;&#22120;&#32763;&#35793;&#23545;&#27604;&#26041;&#35328;&#35780;&#20272;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
CODET: A Benchmark for Contrastive Dialectal Evaluation of Machine Translation. (arXiv:2305.17267v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17267
&lt;/p&gt;
&lt;p&gt;
CODET&#26159;&#19968;&#20010;&#23545;&#27604;&#26041;&#35328;&#30340;&#35780;&#20272;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#22312;&#22788;&#29702;&#26041;&#35328;&#21464;&#20307;&#26102;&#30340;&#34920;&#29616;&#65292;&#35813;&#22522;&#20934;&#27979;&#35797;&#21253;&#21547;&#20061;&#31181;&#19981;&#21516;&#35821;&#35328;&#30340;882&#20010;&#19981;&#21516;&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#22312;&#22788;&#29702;&#28304;&#35821;&#35328;&#30340;&#35821;&#35328;&#21464;&#21270;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#38480;&#30340;&#40065;&#26834;&#24615;&#12290;&#24403;&#38754;&#20020;&#21363;&#20351;&#26159;&#35821;&#35328;&#20351;&#29992;&#20013;&#30340;&#32454;&#24494;&#24046;&#24322;&#65288;&#20363;&#22914;&#19981;&#21516;&#30340;&#39046;&#22495;&#25110;&#30001;&#31532;&#20108;&#35821;&#35328;&#20351;&#29992;&#32773;&#24341;&#20837;&#30340;&#21464;&#20307;&#65289;&#26102;&#65292;&#20854;&#24615;&#33021;&#24448;&#24448;&#20250;&#19979;&#38477;&#12290;&#30452;&#35266;&#19978;&#65292;&#23558;&#36825;&#31181;&#35266;&#23519;&#25512;&#24191;&#21040;&#28085;&#30422;&#26041;&#35328;&#21464;&#20307;&#65292;&#32780;&#20801;&#35768;&#31038;&#21306;&#22312;&#36825;&#20010;&#32500;&#24230;&#19978;&#35780;&#20272;MT&#31995;&#32479;&#30340;&#24037;&#20316;&#26159;&#26377;&#38480;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#32534;&#35793;&#21644;&#21457;&#24067;&#20102;&#23545;&#27604;&#26041;&#35328;&#22522;&#20934;&#27979;&#35797; \dataset&#65292;&#20854;&#20013;&#21253;&#25324;&#26469;&#33258;&#20061;&#31181;&#19981;&#21516;&#35821;&#35328;&#30340;882&#20010;&#19981;&#21516;&#21464;&#20307;&#12290;&#25105;&#20204;&#36824;&#22312;&#25968;&#37327;&#19978;&#23637;&#31034;&#20102;&#22823;&#22411;MT&#27169;&#22411;&#22312;&#26377;&#25928;&#32763;&#35793;&#26041;&#35328;&#21464;&#20307;&#26041;&#38754;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#21457;&#24067;&#25152;&#26377;&#20195;&#30721;&#21644;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural machine translation (NMT) systems exhibit limited robustness in handling source-side linguistic variations. Their performance tends to degrade when faced with even slight deviations in language usage, such as different domains or variations introduced by second-language speakers. It is intuitive to extend this observation to encompass dialectal variations as well, but the work allowing the community to evaluate MT systems on this dimension is limited. To alleviate this issue, we compile and release \dataset, a contrastive dialectal benchmark encompassing 882 different variations from nine different languages. We also quantitatively demonstrate the challenges large MT models face in effectively translating dialectal variants. We are releasing all code and data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23567;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#30446;&#26631;&#30340;&#39044;&#35757;&#32451;&#23545;&#24615;&#33021;&#30340;&#25552;&#39640;&#20316;&#29992;&#12290;&#21516;&#26102;&#65292;&#35813;&#30740;&#31350;&#36824;&#21457;&#29616;&#20102;&#35745;&#31639;&#25104;&#26412;&#19982;&#27169;&#22411;&#25928;&#26524;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.17266</link><description>&lt;p&gt;
&#32437;&#35272;&#35821;&#35328;&#27169;&#22411;&#65306;&#32553;&#20943;&#35268;&#27169;&#21518;&#30340;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Honey, I Shrunk the Language: Language Model Behavior at Reduced Scale. (arXiv:2305.17266v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17266
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23567;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#30446;&#26631;&#30340;&#39044;&#35757;&#32451;&#23545;&#24615;&#33021;&#30340;&#25552;&#39640;&#20316;&#29992;&#12290;&#21516;&#26102;&#65292;&#35813;&#30740;&#31350;&#36824;&#21457;&#29616;&#20102;&#35745;&#31639;&#25104;&#26412;&#19982;&#27169;&#22411;&#25928;&#26524;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35821;&#35328;&#27169;&#22411;&#30340;&#35268;&#27169;&#24613;&#21095;&#22686;&#38271;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#33021;&#21147;&#20063;&#38543;&#30528;&#35268;&#27169;&#30340;&#25193;&#22823;&#32780;&#24471;&#21040;&#20102;&#25552;&#39640;&#12290;&#22823;&#37096;&#20998;&#26368;&#36817;&#30340;&#35268;&#27169;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#39640;&#35745;&#31639;&#37327;&#65292;&#39640;&#21442;&#25968;&#30340;&#29615;&#22659;&#20013;&#65292;&#27809;&#26377;&#22238;&#31572;&#36825;&#20123;&#33021;&#21147;&#20309;&#26102;&#24320;&#22987;&#20986;&#29616;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#38382;&#39064;&#35268;&#27169;&#20943;&#23567;&#30340;&#24773;&#20917;&#19979;&#26159;&#21542;&#21487;&#20197;&#35266;&#23519;&#21040;&#39044;&#35757;&#32451;&#30340;&#25928;&#26524;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#36739;&#23567;&#30340;&#12289;&#32553;&#20943;&#20102;&#35789;&#27719;&#37327;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#21442;&#25968;&#20026;125&#19975;&#30340;&#27169;&#22411;&#20013;&#20351;&#29992;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#65288;MLM&#65289;&#30446;&#26631;&#39044;&#35757;&#32451;&#30340;&#22909;&#22788;&#65292;&#24182;&#24314;&#31435;&#20102;&#39044;&#35757;&#32451;&#22256;&#24785;&#21644;&#19979;&#28216;&#24615;&#33021;&#65288;GLUE&#22522;&#20934;&#65289;&#20043;&#38388;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#32553;&#23567;&#35268;&#27169;&#30340;&#24433;&#21709;&#65292;&#23558;&#32553;&#25918;&#23450;&#24459;&#25193;&#23637;&#21040;&#20102;&#22823;&#32422;100&#19975;&#20010;&#21442;&#25968;&#30340;&#27169;&#22411;&#20013;&#12290;&#22312;&#36825;&#20010;&#35268;&#27169;&#19979;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#35745;&#31639;-&#26368;&#20248;&#27169;&#22411;&#30340;&#24130;&#24459;&#30772;&#35010;&#65292;&#24182;&#23637;&#31034;&#20102;MLM&#25439;&#22833;&#22312;&#20302;&#20110;22&#19975;&#20159;FLOPs&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#24182;&#19981;&#24179;&#28369;&#22320;&#32553;&#25918;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, language models have drastically grown in size, and the abilities of these models have been shown to improve with scale. The majority of recent scaling laws studies focused on high-compute high-parameter count settings, leaving the question of when these abilities begin to emerge largely unanswered. In this paper, we investigate whether the effects of pre-training can be observed when the problem size is reduced, modeling a smaller, reduced-vocabulary language. We show the benefits of pre-training with masked language modeling (MLM) objective in models as small as 1.25M parameters, and establish a strong correlation between pre-training perplexity and downstream performance (GLUE benchmark). We examine downscaling effects, extending scaling laws to models as small as ~1M parameters. At this scale, we observe a break of the power law for compute-optimal models and show that the MLM loss does not scale smoothly with compute-cost (FLOPs) below $2.2 \times 10^{15}$ FLOPs. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#21033;&#29992;&#25552;&#31034;&#20013;&#30340;&#25463;&#24452;&#30340;&#20381;&#36182;&#24615;&#65292;&#21457;&#29616;&#22823;&#22411;&#27169;&#22411;&#26356;&#26377;&#21487;&#33021;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21033;&#29992;&#25552;&#31034;&#20013;&#30340;&#25463;&#24452;&#65292;&#36825;&#20026;&#35780;&#20272;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#31283;&#20581;&#24615;&#21644;&#26816;&#27979;&#21644;&#32531;&#35299;&#25552;&#31034;&#20013;&#25463;&#24452;&#30340;&#20351;&#29992;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.17256</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#26159;&#25042;&#24816;&#30340;&#23398;&#20064;&#32773;&#65306;&#20998;&#26512;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#25463;&#24452;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Can be Lazy Learners: Analyze Shortcuts in In-Context Learning. (arXiv:2305.17256v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#21033;&#29992;&#25552;&#31034;&#20013;&#30340;&#25463;&#24452;&#30340;&#20381;&#36182;&#24615;&#65292;&#21457;&#29616;&#22823;&#22411;&#27169;&#22411;&#26356;&#26377;&#21487;&#33021;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21033;&#29992;&#25552;&#31034;&#20013;&#30340;&#25463;&#24452;&#65292;&#36825;&#20026;&#35780;&#20272;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#31283;&#20581;&#24615;&#21644;&#26816;&#27979;&#21644;&#32531;&#35299;&#25552;&#31034;&#20013;&#25463;&#24452;&#30340;&#20351;&#29992;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#20854;&#20013;LLM&#36890;&#36807;&#20960;&#20010;&#36755;&#20837;-&#26631;&#31614;&#23545;&#65288;&#25552;&#31034;&#65289;&#30340;&#26465;&#20214;&#26469;&#23398;&#20064;&#26032;&#20219;&#21153;&#12290;&#23613;&#31649;&#20854;&#28508;&#21147;&#24040;&#22823;&#65292;&#20294;&#25105;&#20204;&#23545;&#24433;&#21709;&#26368;&#32456;&#20219;&#21153;&#24615;&#33021;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#31283;&#20581;&#24615;&#30340;&#22240;&#32032;&#30340;&#29702;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#30740;&#31350;LLM&#23545;&#25552;&#31034;&#20869;&#25463;&#24452;&#25110;&#20551;&#30456;&#20851;&#30340;&#20381;&#36182;&#20851;&#31995;&#26469;&#24357;&#34917;&#36825;&#19968;&#30693;&#35782;&#24046;&#36317;&#12290;&#36890;&#36807;&#20998;&#31867;&#21644;&#25277;&#21462;&#20219;&#21153;&#30340;&#20840;&#38754;&#23454;&#39564;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;LLM&#26159;&#8220;&#25042;&#24816;&#23398;&#20064;&#32773;&#8221;&#30340;&#20107;&#23454;&#65292;&#23427;&#24448;&#24448;&#21033;&#29992;&#25552;&#31034;&#20013;&#30340;&#25463;&#24452;&#26469;&#33719;&#21462;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#21457;&#29616;&#65292;&#21363;&#36739;&#22823;&#30340;&#27169;&#22411;&#26356;&#26377;&#21487;&#33021;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21033;&#29992;&#25552;&#31034;&#20013;&#30340;&#25463;&#24452;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#20026;&#35780;&#20272;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#31283;&#20581;&#24615;&#21644;&#26816;&#27979;&#21644;&#32531;&#35299;&#25552;&#31034;&#20013;&#25463;&#24452;&#30340;&#20351;&#29992;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently shown great potential for in-context learning, where LLMs learn a new task simply by conditioning on a few input-label pairs (prompts). Despite their potential, our understanding of the factors influencing end-task performance and the robustness of in-context learning remains limited. This paper aims to bridge this knowledge gap by investigating the reliance of LLMs on shortcuts or spurious correlations within prompts. Through comprehensive experiments on classification and extraction tasks, we reveal that LLMs are "lazy learners" that tend to exploit shortcuts in prompts for downstream tasks. Additionally, we uncover a surprising finding that larger models are more likely to utilize shortcuts in prompts during inference. Our findings provide a new perspective on evaluating robustness in in-context learning and pose new challenges for detecting and mitigating the use of shortcuts in prompts.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#35821;&#20041;&#35299;&#26512;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#35780;&#20272;&#35774;&#32622;&#21644;&#26032;&#31639;&#27861;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#26032;&#31639;&#27861;FedSQL&#21644;Lorar&#20248;&#20110;&#29616;&#26377;&#30340;FL&#31639;&#27861;&#21644;&#25105;&#20204;&#25552;&#20986;&#30340;&#35774;&#32622;&#30340;&#24378;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2305.17221</link><description>&lt;p&gt;
&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#35821;&#20041;&#35299;&#26512;&#20219;&#21153;&#65306;&#20219;&#21153;&#24418;&#24335;&#65292;&#35780;&#20272;&#35774;&#32622;&#21450;&#26032;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Federated Learning for Semantic Parsing: Task Formulation, Evaluation Setup, New Algorithms. (arXiv:2305.17221v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17221
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#35821;&#20041;&#35299;&#26512;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#35780;&#20272;&#35774;&#32622;&#21644;&#26032;&#31639;&#27861;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#26032;&#31639;&#27861;FedSQL&#21644;Lorar&#20248;&#20110;&#29616;&#26377;&#30340;FL&#31639;&#27861;&#21644;&#25105;&#20204;&#25552;&#20986;&#30340;&#35774;&#32622;&#30340;&#24378;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#20219;&#21153;&#65292;&#21363;&#38024;&#23545;&#35821;&#20041;&#35299;&#26512;&#30340;&#32852;&#37030;&#23398;&#20064;&#65292;&#22810;&#20010;&#23458;&#25143;&#31471;&#20849;&#21516;&#35757;&#32451;&#19968;&#20010;&#20840;&#23616;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#20849;&#20139;&#20854;&#35821;&#20041;&#20998;&#26512;&#25968;&#25454;&#12290;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#65292;&#32852;&#37030;&#23398;&#20064;&#27169;&#24335;&#23545;&#20110;&#37027;&#20123;&#27809;&#26377;&#36275;&#22815;&#35757;&#32451;&#25968;&#25454;&#26469;&#24320;&#21457;&#19968;&#20010;&#25968;&#25454;&#39269;&#39295;&#30340;&#31070;&#32463;&#35821;&#20041;&#20998;&#26512;&#22120;&#30340;&#23458;&#25143;&#31471;&#23588;&#20854;&#26377;&#30410;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#35774;&#32622;&#26469;&#30740;&#31350;&#36825;&#20010;&#20219;&#21153;&#65292;&#23558;&#24191;&#27867;&#20351;&#29992;&#30340;&#21333;&#22495;&#25991;&#26412;&#21040;SQL&#25968;&#25454;&#38598;&#20316;&#20026;&#23458;&#25143;&#31471;&#26469;&#24418;&#25104;&#19968;&#20010;&#29616;&#23454;&#30340;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#35774;&#32622;&#65292;&#24182;&#21327;&#21516;&#35757;&#32451;&#19968;&#20010;&#20840;&#23616;&#27169;&#22411;&#12290;&#30001;&#20110;&#25105;&#20204;&#30340;&#29616;&#23454;&#35774;&#32622;&#20013;&#23458;&#25143;&#32676;&#30340;&#24322;&#36136;&#24615;&#24456;&#39640;&#65292;&#26631;&#20934;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#20250;&#21463;&#21040;&#24433;&#21709;&#65292;&#25152;&#20197;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#21046;LOss Reduction Adjusted Re-weighting (Lorar)&#26469;&#32531;&#35299;&#24615;&#33021;&#19979;&#38477;&#65292;&#35813;&#26426;&#21046;&#22522;&#20110;&#23458;&#25143;&#31471;&#27599;&#36718;&#35757;&#32451;&#25439;&#22833;&#30340;&#20943;&#23569;&#24773;&#20917;&#26469;&#35843;&#33410;&#27599;&#20010;&#23458;&#25143;&#31471;&#23545;&#20110;&#20840;&#23616;&#27169;&#22411;&#26356;&#26032;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#30340;&#30452;&#35273;&#26159;&#65292;&#25439;&#22833;&#20943;&#23569;&#30340;&#36234;&#22810;&#65292;&#23458;&#25143;&#31471;&#31163;&#20840;&#23616;&#26368;&#20248;&#35299;&#23601;&#36234;&#36828;&#65292;&#20854;&#23545;&#27169;&#22411;&#26356;&#26032;&#30340;&#36129;&#29486;&#23601;&#24212;&#35813;&#36234;&#39640;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#24322;&#26500;&#25991;&#26412;&#21040;SQL FL&#35774;&#32622;&#30340;&#26032;&#30340;FL&#31639;&#27861;FedSQL&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;FedSQL&#21644;Lorar&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;FL&#31639;&#27861;&#21644;&#25105;&#20204;&#25552;&#20986;&#30340;FL&#35774;&#32622;&#20013;&#30340;&#24378;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies a new task of federated learning (FL) for semantic parsing, where multiple clients collaboratively train one global model without sharing their semantic parsing data. By leveraging data from multiple clients, the FL paradigm can be especially beneficial for clients that have little training data to develop a data-hungry neural semantic parser on their own. We propose an evaluation setup to study this task, where we re-purpose widely-used single-domain text-to-SQL datasets as clients to form a realistic heterogeneous FL setting and collaboratively train a global model. As standard FL algorithms suffer from the high client heterogeneity in our realistic setup, we further propose a novel LOss Reduction Adjusted Re-weighting (Lorar) mechanism to mitigate the performance degradation, which adjusts each client's contribution to the global model update based on its training loss reduction during each round. Our intuition is that the larger the loss reduction, the further aw
&lt;/p&gt;</description></item><item><title>GVdoc &#26159;&#19968;&#20010;&#22522;&#20110;&#22270;&#30340;&#25991;&#26723;&#20998;&#31867;&#27169;&#22411;&#65292;&#33021;&#22815;&#36890;&#36807;&#29983;&#25104;&#25991;&#26723;&#22270;&#24182;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#23398;&#20064;&#33410;&#28857;&#21644;&#22270;&#23884;&#20837;&#65292;&#26377;&#25928;&#35299;&#20915;&#35270;&#35273;&#25991;&#26723;&#20998;&#31867;&#22120;&#22312;&#39046;&#22495;&#20869;&#22806;&#26679;&#26412;&#20998;&#31867;&#21644;&#21306;&#20998;&#20013;&#25152;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.17219</link><description>&lt;p&gt;
GVdoc: &#22522;&#20110;&#22270;&#30340;&#35270;&#35273;&#25991;&#26723;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
GVdoc: Graph-based Visual Document Classification. (arXiv:2305.17219v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17219
&lt;/p&gt;
&lt;p&gt;
GVdoc &#26159;&#19968;&#20010;&#22522;&#20110;&#22270;&#30340;&#25991;&#26723;&#20998;&#31867;&#27169;&#22411;&#65292;&#33021;&#22815;&#36890;&#36807;&#29983;&#25104;&#25991;&#26723;&#22270;&#24182;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#23398;&#20064;&#33410;&#28857;&#21644;&#22270;&#23884;&#20837;&#65292;&#26377;&#25928;&#35299;&#20915;&#35270;&#35273;&#25991;&#26723;&#20998;&#31867;&#22120;&#22312;&#39046;&#22495;&#20869;&#22806;&#26679;&#26412;&#20998;&#31867;&#21644;&#21306;&#20998;&#20013;&#25152;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#22312;&#23454;&#38469;&#37096;&#32626;&#20013;&#30340;&#40065;&#26834;&#24615;&#21462;&#20915;&#20110;&#20854;&#22312;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#21644;&#23545;&#39046;&#22495;&#20869;&#22806;&#26679;&#26412;&#30340;&#21306;&#20998;&#33021;&#21147;&#12290;&#35270;&#35273;&#25991;&#26723;&#20998;&#31867;&#22120;&#22312;&#20998;&#24067;&#27979;&#35797;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#27491;&#30830;&#20998;&#31867;&#21644;&#21306;&#20998;&#39046;&#22495;&#22806;&#20363;&#23376;&#26041;&#38754;&#24448;&#24448;&#36935;&#21040;&#22256;&#38590;&#12290;&#22522;&#20110;&#22270;&#30340;&#25991;&#26723;&#20998;&#31867;&#27169;&#22411;&#36890;&#36807;&#29983;&#25104;&#22522;&#20110;&#24067;&#23616;&#30340;&#25991;&#26723;&#22270;&#65292;&#28982;&#21518;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#23398;&#20064;&#33410;&#28857;&#21644;&#22270;&#23884;&#20837;&#65292;&#35299;&#20915;&#20102;&#36825;&#20004;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21363;&#20351;&#21442;&#25968;&#26356;&#23569;&#65292;&#22312;&#39046;&#22495;&#22806;&#30340;&#26679;&#26412;&#19978;&#20063;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#65292;&#22312;&#39046;&#22495;&#20869;&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The robustness of a model for real-world deployment is decided by how well it performs on unseen data and distinguishes between in-domain and out-of-domain samples. Visual document classifiers have shown impressive performance on in-distribution test sets. However, they tend to have a hard time correctly classifying and differentiating out-of-distribution examples. Image-based classifiers lack the text component, whereas multi-modality transformer-based models face the token serialization problem in visual documents due to their diverse layouts. They also require a lot of computing power during inference, making them impractical for many real-world applications. We propose, GVdoc, a graph-based document classification model that addresses both of these challenges. Our approach generates a document graph based on its layout, and then trains a graph neural network to learn node and graph embeddings. Through experiments, we show that our model, even with fewer parameters, outperforms stat
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#27169;&#22411;&#36827;&#34892;&#34701;&#21512;&#65292;&#33021;&#29983;&#25104;&#20855;&#26377;&#36830;&#36143;&#24615;&#30340;&#22270;&#20687;&#36755;&#20986;&#65292;&#21516;&#26102;&#20063;&#33021;&#36827;&#34892;&#22270;&#20687;&#26816;&#32034;&#21644;&#22810;&#27169;&#24577;&#23545;&#35805;&#12290;</title><link>http://arxiv.org/abs/2305.17216</link><description>&lt;p&gt;
&#29992;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22270;&#29255;
&lt;/p&gt;
&lt;p&gt;
Generating Images with Multimodal Language Models. (arXiv:2305.17216v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17216
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#27169;&#22411;&#36827;&#34892;&#34701;&#21512;&#65292;&#33021;&#29983;&#25104;&#20855;&#26377;&#36830;&#36143;&#24615;&#30340;&#22270;&#20687;&#36755;&#20986;&#65292;&#21516;&#26102;&#20063;&#33021;&#36827;&#34892;&#22270;&#20687;&#26816;&#32034;&#21644;&#22810;&#27169;&#24577;&#23545;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#20165;&#21253;&#21547;&#25991;&#26412;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#27169;&#22411;&#36827;&#34892;&#34701;&#21512;&#65292;&#36890;&#36807;&#26144;&#23556;&#23427;&#20204;&#30340;&#23884;&#20837;&#31354;&#38388;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#23637;&#31034;&#20102;&#24191;&#27867;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;&#65306;&#22270;&#20687;&#26816;&#32034;&#12289;&#26032;&#39062;&#22270;&#20687;&#29983;&#25104;&#21644;&#22810;&#27169;&#24577;&#23545;&#35805;&#12290;&#36825;&#26159;&#31532;&#19968;&#31181;&#33021;&#22815;&#22312;&#20219;&#24847;&#20132;&#38169;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#36755;&#20837;&#20043;&#38388;&#36827;&#34892;&#26465;&#20214;&#35843;&#33410;&#65292;&#29983;&#25104;&#36830;&#36143;&#22270;&#20687;&#65288;&#21644;&#25991;&#26412;&#65289;&#36755;&#20986;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#22312;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26144;&#23556;&#32593;&#32476;&#65292;&#23558;LLM&#22522;&#20110;&#29616;&#25104;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#23558;&#25991;&#26412;&#30340;&#38544;&#34255;&#34920;&#31034;&#36716;&#25442;&#20026;&#35270;&#35273;&#27169;&#22411;&#30340;&#23884;&#20837;&#31354;&#38388;&#65292;&#21033;&#29992;LLM&#24378;&#22823;&#30340;&#25991;&#26412;&#34920;&#31034;&#26469;&#29983;&#25104;&#35270;&#35273;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38271;&#19988;&#22797;&#26434;&#35821;&#35328;&#30340;&#20219;&#21153;&#19978;&#20248;&#20110;&#22522;&#20934;&#29983;&#25104;&#27169;&#22411;&#12290;&#38500;&#20102;&#26032;&#39062;&#22270;&#20687;&#29983;&#25104;&#20043;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36824;&#33021;&#22815;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#26816;&#32034;&#22270;&#20687;&#65292;&#24182;&#36827;&#34892;&#22810;&#27169;&#24577;&#23545;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs. Our approach outperforms baseline generation models on tasks with longer and more complex language. In addition to novel image generation, our model is also capable of image retrieval from a prespe
&lt;/p&gt;</description></item><item><title>BIG-C&#26159;&#19968;&#20010;&#29992;&#20110;Bemba&#35821;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#35821;&#38899;&#35782;&#21035;&#12289;&#26426;&#22120;&#32763;&#35793;&#12289;&#35821;&#38899;&#32763;&#35793;&#20219;&#21153;&#30340;&#22522;&#32447;&#65292;&#24182;&#21246;&#30011;&#20102;&#35813;&#25968;&#25454;&#38598;&#30340;&#28508;&#22312;&#26410;&#26469;&#22810;&#27169;&#24577;&#29992;&#36884;&#65292;&#26088;&#22312;&#20419;&#36827;&#30740;&#31350;&#24182;&#40723;&#21169;&#36328;&#36234;&#35821;&#35328;&#12289;&#35821;&#38899;&#21644;&#35270;&#35273;&#31038;&#21306;&#30340;&#21512;&#20316;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#8220;&#20256;&#32479;&#8221;&#20351;&#29992;&#30340;&#35821;&#35328;&#20043;&#22806;&#30340;&#35821;&#35328;&#12290;</title><link>http://arxiv.org/abs/2305.17202</link><description>&lt;p&gt;
BIG-C&#65306;&#19968;&#31181;Bemba&#35821;&#30340;&#22810;&#27169;&#24577;&#22810;&#29992;&#36884;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
BIG-C: a Multimodal Multi-Purpose Dataset for Bemba. (arXiv:2305.17202v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17202
&lt;/p&gt;
&lt;p&gt;
BIG-C&#26159;&#19968;&#20010;&#29992;&#20110;Bemba&#35821;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#35821;&#38899;&#35782;&#21035;&#12289;&#26426;&#22120;&#32763;&#35793;&#12289;&#35821;&#38899;&#32763;&#35793;&#20219;&#21153;&#30340;&#22522;&#32447;&#65292;&#24182;&#21246;&#30011;&#20102;&#35813;&#25968;&#25454;&#38598;&#30340;&#28508;&#22312;&#26410;&#26469;&#22810;&#27169;&#24577;&#29992;&#36884;&#65292;&#26088;&#22312;&#20419;&#36827;&#30740;&#31350;&#24182;&#40723;&#21169;&#36328;&#36234;&#35821;&#35328;&#12289;&#35821;&#38899;&#21644;&#35270;&#35273;&#31038;&#21306;&#30340;&#21512;&#20316;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#8220;&#20256;&#32479;&#8221;&#20351;&#29992;&#30340;&#35821;&#35328;&#20043;&#22806;&#30340;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;BIG-C&#65288;Bemba&#22270;&#20687;&#24341;&#23548;&#23545;&#35805;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;Bemba&#35821;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#12290;&#34429;&#28982;Bemba&#35821;&#26159;&#36190;&#27604;&#20122;&#20154;&#21475;&#26368;&#22810;&#30340;&#35821;&#35328;&#65292;&#20294;&#20854;&#32570;&#20047;&#36164;&#28304;&#20351;&#24471;&#35821;&#35328;&#25216;&#26415;&#25110;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#30340;&#21457;&#23637;&#20960;&#20046;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#35813;&#25968;&#25454;&#38598;&#30001;&#22522;&#20110;&#22270;&#20687;&#30340;Bemba&#35821;&#35328;&#32773;&#20043;&#38388;&#30340;&#22810;&#36718;&#23545;&#35805;&#32452;&#25104;&#65292;&#32463;&#36807;&#36716;&#24405;&#24182;&#32763;&#35793;&#25104;&#33521;&#25991;&#12290;&#25968;&#25454;&#38598;&#20013;&#26377;&#36229;&#36807;92,000&#20010;&#35805;&#35821;/&#21477;&#23376;&#65292;&#30456;&#24403;&#20110;&#36229;&#36807;180&#23567;&#26102;&#30340;&#38899;&#39057;&#25968;&#25454;&#65292;&#20855;&#26377;&#30456;&#24212;&#30340;&#36716;&#24405;&#21644;&#33521;&#25991;&#32763;&#35793;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#12289;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#21644;&#35821;&#38899;&#32763;&#35793;&#65288;ST&#65289;&#20219;&#21153;&#30340;&#22522;&#32447;&#65292;&#24182;&#21246;&#30011;&#20102;&#35813;&#25968;&#25454;&#38598;&#20854;&#20182;&#28508;&#22312;&#30340;&#26410;&#26469;&#22810;&#27169;&#24577;&#29992;&#36884;&#12290;&#25105;&#20204;&#24076;&#26395;&#36890;&#36807;&#23558;&#35813;&#25968;&#25454;&#38598;&#25552;&#20379;&#32473;&#30740;&#31350;&#31038;&#21306;&#65292;&#26412;&#24037;&#20316;&#23558;&#20419;&#36827;&#30740;&#31350;&#24182;&#40723;&#21169;&#36328;&#36234;&#35821;&#35328;&#12289;&#35821;&#38899;&#21644;&#35270;&#35273;&#31038;&#21306;&#30340;&#21512;&#20316;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#8220;&#20256;&#32479;&#8221;&#20351;&#29992;&#30340;&#35821;&#35328;&#20043;&#22806;&#30340;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present BIG-C (Bemba Image Grounded Conversations), a large multimodal dataset for Bemba. While Bemba is the most populous language of Zambia, it exhibits a dearth of resources which render the development of language technologies or language processing research almost impossible. The dataset is comprised of multi-turn dialogues between Bemba speakers based on images, transcribed and translated into English. There are more than 92,000 utterances/sentences, amounting to more than 180 hours of audio data with corresponding transcriptions and English translations. We also provide baselines on speech recognition (ASR), machine translation (MT) and speech translation (ST) tasks, and sketch out other potential future multimodal uses of our dataset. We hope that by making the dataset available to the research community, this work will foster research and encourage collaboration across the language, speech, and vision communities especially for languages outside the "traditionally" used hig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#35768;&#22810;&#19981;&#21516;&#30340;NLU&#20219;&#21153;&#21046;&#23450;&#20026;&#24773;&#22659;&#35748;&#30693;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#33258;&#25105;&#35757;&#32451;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#36866;&#24212;&#24615;&#24615;&#33021;&#12290;&#31616;&#21333;&#30340;&#20266;&#26631;&#31614;&#32534;&#36753;&#65288;SimPLE&#65289;&#31639;&#27861;&#26377;&#21033;&#20110;&#33258;&#25105;&#35757;&#32451;&#30340;&#31283;&#23450;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.17197</link><description>&lt;p&gt;
&#35748;&#30693;&#20316;&#20026;&#24378;&#20581;&#30340;&#33258;&#23398;&#32773;
&lt;/p&gt;
&lt;p&gt;
Entailment as Robust Self-Learner. (arXiv:2305.17197v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17197
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#35768;&#22810;&#19981;&#21516;&#30340;NLU&#20219;&#21153;&#21046;&#23450;&#20026;&#24773;&#22659;&#35748;&#30693;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#33258;&#25105;&#35757;&#32451;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#36866;&#24212;&#24615;&#24615;&#33021;&#12290;&#31616;&#21333;&#30340;&#20266;&#26631;&#31614;&#32534;&#36753;&#65288;SimPLE&#65289;&#31639;&#27861;&#26377;&#21033;&#20110;&#33258;&#25105;&#35757;&#32451;&#30340;&#31283;&#23450;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35748;&#30693;&#24050;&#34987;&#35748;&#20026;&#26159;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#27169;&#22411;&#37325;&#35201;&#30340;&#25351;&#26631;&#65292;&#36817;&#26399;&#30740;&#31350;&#21457;&#29616;&#65292;&#35748;&#30693;&#39044;&#35757;&#32451;&#26377;&#21033;&#20110;&#24369;&#30417;&#30563;&#24494;&#35843;&#12290;&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#25552;&#31034;&#31574;&#30053;&#65292;&#23558;&#35768;&#22810;&#19981;&#21516;&#30340;NLU&#20219;&#21153;&#21046;&#23450;&#20026;&#24773;&#22659;&#35748;&#30693;&#12290;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#35748;&#30693;&#27169;&#22411;&#30340;&#38646;-shot&#36866;&#24212;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#33258;&#25105;&#35757;&#32451;&#35748;&#30693;&#27169;&#22411;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#36866;&#24212;&#24615;&#24615;&#33021;&#12290;&#20026;&#20102;&#23454;&#29616;&#26356;&#31283;&#23450;&#30340;&#25913;&#36827;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31616;&#21333;&#30340;&#20266;&#26631;&#31614;&#32534;&#36753;&#65288;SimPLE&#65289;&#31639;&#27861;&#65292;&#20197;&#25552;&#39640;&#33258;&#25105;&#35757;&#32451;&#20013;&#30340;&#20266;&#26631;&#31614;&#36136;&#37327;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#39044;&#35757;&#32451;&#30340;&#35748;&#30693;&#27169;&#22411;&#21644;&#33258;&#25105;&#35757;&#32451;&#30340;&#27169;&#22411;&#37117;&#23545;&#23545;&#25239;&#24615;&#35780;&#20272;&#25968;&#25454;&#20855;&#26377;&#24378;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entailment has been recognized as an important metric for evaluating natural language understanding (NLU) models, and recent studies have found that entailment pretraining benefits weakly supervised fine-tuning. In this work, we design a prompting strategy that formulates a number of different NLU tasks as contextual entailment. This approach improves the zero-shot adaptation of pretrained entailment models. Secondly, we notice that self-training entailment-based models with unlabeled data can significantly improve the adaptation performance on downstream tasks. To achieve more stable improvement, we propose the Simple Pseudo-Label Editing (SimPLE) algorithm for better pseudo-labeling quality in self-training. We also found that both pretrained entailment-based models and the self-trained models are robust against adversarial evaluation data. Experiments on binary and multi-class classification tasks show that SimPLE leads to more robust self-training results, indicating that the self-
&lt;/p&gt;</description></item><item><title>&#26080;&#30417;&#30563;NMT&#20013;&#30340;&#22797;&#21046;&#38382;&#39064;&#36890;&#24120;&#21457;&#29983;&#22312;&#36828;&#36317;&#31163;&#35821;&#31181;&#23545;&#20013;&#19988;&#20250;&#30452;&#25509;&#22797;&#21046;&#36755;&#20837;&#21477;&#23376;&#30340;&#37096;&#20998;&#20316;&#20026;&#32763;&#35793;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21253;&#21547;&#35821;&#35328;&#37492;&#21035;&#22120;&#25439;&#22833;&#30340;&#35757;&#32451;&#35745;&#21010;&#26469;&#32531;&#35299;&#35813;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;&#20302;&#36164;&#28304;&#35821;&#31181;&#30340;&#32763;&#35793;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.17182</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#22797;&#21046;&#38382;&#39064;&#65306;&#20855;&#26377;&#35821;&#35328;&#37492;&#21035;&#22120;&#25439;&#22833;&#30340;&#35757;&#32451;&#35745;&#21010;
&lt;/p&gt;
&lt;p&gt;
On the Copying Problem of Unsupervised NMT: A Training Schedule with a Language Discriminator Loss. (arXiv:2305.17182v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17182
&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;NMT&#20013;&#30340;&#22797;&#21046;&#38382;&#39064;&#36890;&#24120;&#21457;&#29983;&#22312;&#36828;&#36317;&#31163;&#35821;&#31181;&#23545;&#20013;&#19988;&#20250;&#30452;&#25509;&#22797;&#21046;&#36755;&#20837;&#21477;&#23376;&#30340;&#37096;&#20998;&#20316;&#20026;&#32763;&#35793;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21253;&#21547;&#35821;&#35328;&#37492;&#21035;&#22120;&#25439;&#22833;&#30340;&#35757;&#32451;&#35745;&#21010;&#26469;&#32531;&#35299;&#35813;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;&#20302;&#36164;&#28304;&#35821;&#31181;&#30340;&#32763;&#35793;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26080;&#30417;&#30563;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#24050;&#22312;&#35768;&#22810;&#35821;&#31181;&#38388;&#24471;&#21040;&#25104;&#21151;&#65292;&#20294;&#22797;&#21046;&#38382;&#39064;&#65288;&#21363;&#23558;&#36755;&#20837;&#21477;&#23376;&#30340;&#26576;&#20123;&#37096;&#20998;&#30452;&#25509;&#22797;&#21046;&#20316;&#20026;&#32763;&#35793;&#65289;&#22312;&#36828;&#36317;&#31163;&#35821;&#31181;&#23545;&#20013;&#24456;&#24120;&#35265;&#65292;&#23588;&#20854;&#28041;&#21450;&#20302;&#36164;&#28304;&#35821;&#31181;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20010;&#38382;&#39064;&#19982;&#22312;&#32447;&#22238;&#35793;&#65288;BT&#65289;&#26399;&#38388;&#20986;&#29616;&#30340;&#39044;&#26399;&#22797;&#21046;&#34892;&#20026;&#23494;&#20999;&#30456;&#20851;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#35757;&#32451;&#35745;&#21010;&#65292;&#23427;&#21253;&#21547;&#20102;&#19968;&#20010;&#35821;&#35328;&#37492;&#21035;&#22120;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#35813;&#25439;&#22833;&#26045;&#21152;&#32422;&#26463;&#20110;&#20013;&#38388;&#32763;&#35793;&#65292;&#20197;&#20351;&#32763;&#35793;&#26159;&#25152;&#38656;&#30340;&#35821;&#35328;&#12290;&#36890;&#36807;&#22312;&#19981;&#21516;&#35821;&#35328;&#23545;&#12289; &#21253;&#25324;&#30456;&#20284;&#21644;&#36828;&#36317;&#31163;&#12289;&#39640;&#36164;&#28304;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#24191;&#27867;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#32531;&#35299;&#20102;&#22797;&#21046;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#32763;&#35793;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although unsupervised neural machine translation (UNMT) has achieved success in many language pairs, the copying problem, i.e., directly copying some parts of the input sentence as the translation, is common among distant language pairs, especially when low-resource languages are involved. We find this issue is closely related to an unexpected copying behavior during online back-translation (BT). In this work, we propose a simple but effective training schedule that incorporates a language discriminator loss. The loss imposes constraints on the intermediate translation so that the translation is in the desired language. By conducting extensive experiments on different language pairs, including similar and distant, high and low-resource languages, we find that our method alleviates the copying problem, thus improving the translation performance on low-resource languages.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#26032;&#30340;&#26631;&#20934;&#26469;&#35780;&#20272;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#20998;&#35789;&#22120;&#30340;&#36136;&#37327;&#65292;&#24182;&#21457;&#29616;&#36328;&#35821;&#35328;&#35789;&#27719;&#30340;&#37325;&#21472;&#20250;&#23545;&#26576;&#20123;&#19979;&#28216;&#20219;&#21153;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#65292;&#20294;&#20849;&#20139;&#35789;&#27719;&#26377;&#21161;&#20110;&#20854;&#20182;&#20219;&#21153;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#22810;&#35821;&#35328;&#35789;&#27719;&#20013;&#30340;&#35821;&#35328;&#29305;&#23450;&#26631;&#35760;&#35206;&#30422;&#33539;&#22260;&#23545;&#21333;&#35789;&#32423;&#20219;&#21153;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290;&#36825;&#20123;&#21457;&#29616;&#23545;&#26410;&#26469;&#30340;&#27169;&#22411;&#24320;&#21457;&#20154;&#21592;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#20998;&#35789;&#22120;&#25552;&#20379;&#20102;&#25351;&#21335;&#12290;</title><link>http://arxiv.org/abs/2305.17179</link><description>&lt;p&gt;
&#20998;&#35789;&#23545;&#22810;&#35821;&#35328;&#35821;&#35328;&#24314;&#27169;&#30340;&#24433;&#21709;&#65306;&#35780;&#20272;&#36328;&#35821;&#35328;&#35789;&#27719;&#20998;&#37197;&#21644;&#37325;&#21472;&#30340;&#26032;&#26631;&#20934;
&lt;/p&gt;
&lt;p&gt;
Tokenization Impacts Multilingual Language Modeling: Assessing Vocabulary Allocation and Overlap Across Languages. (arXiv:2305.17179v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17179
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#26032;&#30340;&#26631;&#20934;&#26469;&#35780;&#20272;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#20998;&#35789;&#22120;&#30340;&#36136;&#37327;&#65292;&#24182;&#21457;&#29616;&#36328;&#35821;&#35328;&#35789;&#27719;&#30340;&#37325;&#21472;&#20250;&#23545;&#26576;&#20123;&#19979;&#28216;&#20219;&#21153;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#65292;&#20294;&#20849;&#20139;&#35789;&#27719;&#26377;&#21161;&#20110;&#20854;&#20182;&#20219;&#21153;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#22810;&#35821;&#35328;&#35789;&#27719;&#20013;&#30340;&#35821;&#35328;&#29305;&#23450;&#26631;&#35760;&#35206;&#30422;&#33539;&#22260;&#23545;&#21333;&#35789;&#32423;&#20219;&#21153;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290;&#36825;&#20123;&#21457;&#29616;&#23545;&#26410;&#26469;&#30340;&#27169;&#22411;&#24320;&#21457;&#20154;&#21592;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#20998;&#35789;&#22120;&#25552;&#20379;&#20102;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#23558;&#22810;&#31181;&#35821;&#35328;&#34920;&#31034;&#20026;&#19968;&#20010;&#27169;&#22411;&#30340;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#35780;&#20272;&#23376;&#35789;&#20998;&#35789;&#22120;&#20013;&#35789;&#27719;&#34920;&#31034;&#21644;&#35789;&#27719;&#37325;&#21472;&#36136;&#37327;&#30340;&#26032;&#26631;&#20934;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36328;&#35821;&#35328;&#35789;&#27719;&#30340;&#37325;&#21472;&#23454;&#38469;&#19978;&#20250;&#23545;&#26576;&#20123;&#19979;&#28216;&#20219;&#21153;&#65288;POS&#65292;&#20381;&#36182;&#26641;&#26631;&#27880;&#65289;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#30456;&#21453;&#65292;&#20849;&#20139;&#35789;&#27719;&#26377;&#21161;&#20110;NER&#21644;&#21477;&#23376;&#32423;&#20219;&#21153;&#65288;&#36328;&#35821;&#35328;&#26816;&#32034;&#65292;NLI&#65289;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#65292;&#22810;&#35821;&#35328;&#35789;&#27719;&#20013;&#30340;&#35821;&#35328;&#29305;&#23450;&#26631;&#35760;&#35206;&#30422;&#33539;&#22260;&#26174;&#30528;&#24433;&#21709;&#21333;&#35789;&#32423;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#20026;&#28145;&#20837;&#20102;&#35299;&#20998;&#35789;&#22120;&#22312;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20316;&#29992;&#24182;&#20026;&#26410;&#26469;&#27169;&#22411;&#24320;&#21457;&#20154;&#21592;&#36873;&#25321;&#36866;&#21512;&#20854;&#29305;&#23450;&#24212;&#29992;&#30340;&#26368;&#21512;&#36866;&#30340;&#20998;&#35789;&#22120;&#25552;&#20379;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual language models have recently gained attention as a promising solution for representing multiple languages in a single model. In this paper, we propose new criteria to evaluate the quality of lexical representation and vocabulary overlap observed in sub-word tokenizers. Our findings show that the overlap of vocabulary across languages can be actually detrimental to certain downstream tasks (POS, dependency tree labeling). In contrast, NER and sentence-level tasks (cross-lingual retrieval, NLI) benefit from sharing vocabulary. We also observe that the coverage of the language-specific tokens in the multilingual vocabulary significantly impacts the word-level tasks. Our study offers a deeper understanding of the role of tokenizers in multilingual language models and guidelines for future model developers to choose the most suitable tokenizer for their specific application before undertaking costly model pre-training
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#24320;&#23637;&#20102;&#23545;&#29356;&#21736;&#29616;&#35937;&#36827;&#34892;&#30340;&#22823;&#35268;&#27169;&#35745;&#31639;&#30740;&#31350;&#65292;&#21457;&#29616;&#29356;&#21736;&#35789;&#27719;&#21487;&#21521;&#19981;&#21516;&#32676;&#20307;&#20256;&#36798;&#19981;&#21516;&#30340;&#21547;&#20041;&#21644;&#25361;&#34885;&#24615;&#65292;&#21516;&#26102;&#35299;&#37322;&#20102;&#23427;&#20204;&#22914;&#20309;&#35268;&#36991;&#25919;&#27835;&#21518;&#26524;&#21644;&#31639;&#27861;&#20869;&#23481;&#35843;&#33410;&#12290;</title><link>http://arxiv.org/abs/2305.17174</link><description>&lt;p&gt;
&#20174;&#29356;&#21736;&#21040;&#21895;&#21485;&#65306;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#25581;&#31034;&#32534;&#30721;&#20462;&#36766;
&lt;/p&gt;
&lt;p&gt;
From Dogwhistles to Bullhorns: Unveiling Coded Rhetoric with Language Models. (arXiv:2305.17174v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17174
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#24320;&#23637;&#20102;&#23545;&#29356;&#21736;&#29616;&#35937;&#36827;&#34892;&#30340;&#22823;&#35268;&#27169;&#35745;&#31639;&#30740;&#31350;&#65292;&#21457;&#29616;&#29356;&#21736;&#35789;&#27719;&#21487;&#21521;&#19981;&#21516;&#32676;&#20307;&#20256;&#36798;&#19981;&#21516;&#30340;&#21547;&#20041;&#21644;&#25361;&#34885;&#24615;&#65292;&#21516;&#26102;&#35299;&#37322;&#20102;&#23427;&#20204;&#22914;&#20309;&#35268;&#36991;&#25919;&#27835;&#21518;&#26524;&#21644;&#31639;&#27861;&#20869;&#23481;&#35843;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29356;&#21736;&#26159;&#19968;&#31181;&#32534;&#30721;&#34920;&#36798;&#26041;&#24335;&#65292;&#23427;&#21516;&#26102;&#21521;&#24191;&#22823;&#35266;&#20247;&#20256;&#36798;&#19968;&#23618;&#21547;&#20041;&#65292;&#21521;&#19968;&#20010;&#29421;&#31364;&#30340;&#20869;&#37096;&#32676;&#20307;&#20256;&#36798;&#31532;&#20108;&#23618;&#24847;&#21619;&#65292;&#36825;&#36890;&#24120;&#26159;&#20196;&#20154;&#24974;&#24694;&#25110;&#25361;&#34885;&#30340;&#65307;&#23427;&#20204;&#26088;&#22312;&#22238;&#36991;&#25919;&#27835;&#21518;&#26524;&#21644;&#31639;&#27861;&#20869;&#23481;&#35843;&#33410;&#12290;&#26412;&#25991;&#39318;&#27425;&#24320;&#23637;&#20102;&#23545;&#29356;&#21736;&#29616;&#35937;&#30340;&#22823;&#35268;&#27169;&#35745;&#31639;&#30740;&#31350;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#29356;&#21736;&#30340;&#20998;&#31867;&#27861;&#65292;&#31574;&#21010;&#20102;&#26377;&#30528;&#20016;&#23500;&#32972;&#26223;&#20449;&#24687;&#21644;&#23454;&#20363;&#30340;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#29356;&#21736;&#35789;&#27719;&#34920;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#22312;&#32654;&#22269;&#21382;&#21490;&#25919;&#27835;&#23478;&#30340;&#28436;&#35762;&#20013;&#30340;&#20351;&#29992;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;GPT-3&#65289;&#26159;&#21542;&#21487;&#20197;&#35782;&#21035;&#29356;&#21736;&#21450;&#20854;&#24847;&#20041;&#65292;&#24182;&#21457;&#29616;GPT-3&#24615;&#33021;&#22240;&#29356;&#21736;&#31867;&#22411;&#21644;&#30446;&#26631;&#32676;&#20307;&#32780;&#21464;&#21270;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21253;&#21547;&#29356;&#21736;&#30340;&#26377;&#23475;&#20869;&#23481;&#22914;&#20309;&#35268;&#36991;&#27602;&#24615;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dogwhistles are coded expressions that simultaneously convey one meaning to a broad audience and a second one, often hateful or provocative, to a narrow in-group; they are deployed to evade both political repercussions and algorithmic content moderation. For example, in the sentence 'we need to end the cosmopolitan experiment,' the word 'cosmopolitan' likely means 'worldly' to many, but secretly means 'Jewish' to a select few. We present the first large-scale computational investigation of dogwhistles. We develop a typology of dogwhistles, curate the largest-to-date glossary of over 300 dogwhistles with rich contextual information and examples, and analyze their usage in historical U.S. politicians' speeches. We then assess whether a large language model (GPT-3) can identify dogwhistles and their meanings, and find that GPT-3's performance varies widely across types of dogwhistles and targeted groups. Finally, we show that harmful content containing dogwhistles avoids toxicity detectio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#23545;&#40784;&#35780;&#20272;&#26041;&#27861;A2EHV&#65292;&#37319;&#29992;&#24322;&#36136;&#20215;&#20540;&#31995;&#32479;&#65292;&#24182;&#22522;&#20110;&#20215;&#20540;&#21512;&#29702;&#24615;&#21644;&#31038;&#20250;&#20215;&#20540;&#23450;&#21521;&#26694;&#26550;&#35780;&#20272;&#20195;&#29702;&#20154;&#34892;&#20026;&#30340;&#31038;&#20250;&#20559;&#22909;&#65292;&#32467;&#26524;&#34920;&#26126;&#27604;&#20256;&#32479;&#23545;&#40784;&#26041;&#27861;&#26356;&#21512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2305.17147</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24322;&#36136;&#20215;&#20540;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Value Evaluation for Large Language Models. (arXiv:2305.17147v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#23545;&#40784;&#35780;&#20272;&#26041;&#27861;A2EHV&#65292;&#37319;&#29992;&#24322;&#36136;&#20215;&#20540;&#31995;&#32479;&#65292;&#24182;&#22522;&#20110;&#20215;&#20540;&#21512;&#29702;&#24615;&#21644;&#31038;&#20250;&#20215;&#20540;&#23450;&#21521;&#26694;&#26550;&#35780;&#20272;&#20195;&#29702;&#20154;&#34892;&#20026;&#30340;&#31038;&#20250;&#20559;&#22909;&#65292;&#32467;&#26524;&#34920;&#26126;&#27604;&#20256;&#32479;&#23545;&#40784;&#26041;&#27861;&#26356;&#21512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#20351;&#24471;&#23558;&#23427;&#20204;&#30340;&#20215;&#20540;&#19982;&#20154;&#31867;&#20215;&#20540;&#23545;&#40784;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#23581;&#35797;&#23558;&#20854;&#19982;&#19968;&#31181;&#21516;&#36136;&#30340;&#20154;&#31867;&#20215;&#20540;&#23545;&#40784;&#65292;&#24182;&#38656;&#35201;&#20154;&#31867;&#39564;&#35777;&#65292;&#20294;&#32570;&#20047;&#23545;&#23545;&#40784;&#25152;&#38656;&#26041;&#38754;&#21644;&#28145;&#24230;&#30340;&#20849;&#35782;&#20197;&#21450;&#36896;&#25104;&#30340;&#20154;&#31867;&#20559;&#35265;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#23545;&#40784;&#35780;&#20272;&#26041;&#27861;A2EHV&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#24322;&#36136;&#20215;&#20540;&#31995;&#32479;&#65292;&#65288;1&#65289;&#26159;&#33258;&#21160;&#21270;&#30340;&#65292;&#20197;&#26368;&#23567;&#21270;&#21333;&#20010;&#20154;&#31867;&#20559;&#35265;&#65292;&#24182;&#19988;&#65288;2&#65289;&#20801;&#35768;&#35780;&#20272;&#38024;&#23545;&#21508;&#31181;&#30446;&#26631;&#20540;&#30340;&#24322;&#36136;&#20195;&#29702;&#20154;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#20215;&#20540;&#21512;&#29702;&#24615;&#30340;&#27010;&#24565;&#65292;&#23427;&#20195;&#34920;&#20102;&#20195;&#29702;&#20154;&#25191;&#34892;&#26368;&#33021;&#28385;&#36275;&#30446;&#26631;&#20215;&#20540;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;&#20215;&#20540;&#21512;&#29702;&#24615;&#30340;&#37327;&#21270;&#26159;&#36890;&#36807;&#31038;&#20250;&#24515;&#29702;&#23398;&#20013;&#30340;&#31038;&#20250;&#20215;&#20540;&#23450;&#21521;&#26694;&#26550;&#36827;&#34892;&#30340;&#65292;&#35813;&#26694;&#26550;&#23558;&#20215;&#20540;&#31354;&#38388;&#20998;&#20026;&#22235;&#20010;&#31867;&#21035;&#65292;&#20197;&#35780;&#20272;&#20195;&#29702;&#20154;&#34892;&#20026;&#30340;&#31038;&#20250;&#20559;&#22909;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#19977;&#20010;&#27169;&#22411;&#30340;&#20215;&#20540;&#21512;&#29702;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;A2EHV&#26041;&#27861;&#27604;&#20256;&#32479;&#23545;&#40784;&#26041;&#27861;&#26356;&#21512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergent capabilities of Large Language Models (LLMs) have made it crucial to align their values with those of humans. Current methodologies typically attempt alignment with a homogeneous human value and requires human verification, yet lack consensus on the desired aspect and depth of alignment and resulting human biases. In this paper, we propose A2EHV, an Automated Alignment Evaluation with a Heterogeneous Value system that (1) is automated to minimize individual human biases, and (2) allows assessments against various target values to foster heterogeneous agents. Our approach pivots on the concept of value rationality, which represents the ability for agents to execute behaviors that satisfy a target value the most. The quantification of value rationality is facilitated by the Social Value Orientation framework from social psychology, which partitions the value space into four categories to assess social preferences from agents' behaviors. We evaluate the value rationality of e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Ghost in the Minecraft (GITM)&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22522;&#20110;&#25991;&#26412;&#30340;&#30693;&#35782;&#21644;&#35760;&#24518;&#65292;&#21019;&#36896;&#20102;&#19968;&#31181;&#22312;Minecraft&#20013;&#20855;&#22791;&#36890;&#29992;&#33021;&#21147;&#30340;&#26234;&#33021;&#20307;&#65292;&#21487;&#22312;&#20197;&#25991;&#26412;&#20026;&#22522;&#30784;&#30340;&#22797;&#26434;&#32534;&#31243;&#29615;&#22659;&#20013;&#29087;&#32451;&#23548;&#33322;&#12290;</title><link>http://arxiv.org/abs/2305.17144</link><description>&lt;p&gt;
Minecraft&#20013;&#30340;&#24189;&#28789;&#65306;&#21033;&#29992;&#22522;&#20110;&#25991;&#26412;&#30693;&#35782;&#21644;&#35760;&#24518;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;&#20013;&#30340;&#36890;&#29992;&#33021;&#21147;&#26234;&#33021;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ghost in the Minecraft: Generally Capable Agents for Open-World Enviroments via Large Language Models with Text-based Knowledge and Memory. (arXiv:2305.17144v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17144
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Ghost in the Minecraft (GITM)&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22522;&#20110;&#25991;&#26412;&#30340;&#30693;&#35782;&#21644;&#35760;&#24518;&#65292;&#21019;&#36896;&#20102;&#19968;&#31181;&#22312;Minecraft&#20013;&#20855;&#22791;&#36890;&#29992;&#33021;&#21147;&#30340;&#26234;&#33021;&#20307;&#65292;&#21487;&#22312;&#20197;&#25991;&#26412;&#20026;&#22522;&#30784;&#30340;&#22797;&#26434;&#32534;&#31243;&#29615;&#22659;&#20013;&#29087;&#32451;&#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;Minecraft&#29609;&#27861;&#21560;&#24341;&#20102;&#22823;&#37327;&#30340;&#30740;&#31350;&#20851;&#27880;&#65292;&#25104;&#20026;&#24320;&#21457;&#33021;&#22815;&#22312;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;&#20013;&#36816;&#34892;&#30340;&#26234;&#33021;&#20307;&#30340;&#20016;&#23500;&#24179;&#21488;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#29305;&#23450;&#30340;&#30446;&#26631;&#19978;&#65292;&#20363;&#22914;&#27969;&#34892;&#30340;&#8220;ObtainDiamond&#8221;&#20219;&#21153;&#65292;&#24182;&#19988;&#36824;&#27809;&#26377;&#26174;&#31034;&#20986;&#26377;&#25928;&#22320;&#25512;&#24191;&#21040;&#26356;&#24191;&#27867;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#8220;ObtainDiamond&#8221;&#20219;&#21153;&#30340;&#30446;&#21069;&#26368;&#39640;&#25104;&#21151;&#29575;&#21482;&#26377;&#32422;20&#65285;&#65292;&#20984;&#26174;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#25511;&#21046;&#22120;&#30340;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Ghost in the Minecraft (GITM)&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22522;&#20110;&#25991;&#26412;&#30340;&#30693;&#35782;&#21644;&#35760;&#24518;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#21019;&#24314;Minecraft&#20013;&#30340;&#36890;&#29992;&#33021;&#21147;&#26234;&#33021;&#20307;&#12290;&#36825;&#20123;&#20855;&#22791;LLM&#20013;&#30340;&#36923;&#36753;&#21644;&#24120;&#35782;&#33021;&#21147;&#30340;&#26234;&#33021;&#20307;&#21487;&#20197;&#29087;&#32451;&#22320;&#22312;&#20197;&#25991;&#26412;&#20026;&#22522;&#30784;&#30340;&#22797;&#26434;&#32534;&#31243;&#29615;&#22659;&#20013;&#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;
The captivating realm of Minecraft has attracted substantial research interest in recent years, serving as a rich platform for developing intelligent agents capable of functioning in open-world environments. However, the current research landscape predominantly focuses on specific objectives, such as the popular "ObtainDiamond" task, and has not yet shown effective generalization to a broader spectrum of tasks. Furthermore, the current leading success rate for the "ObtainDiamond" task stands at around 20%, highlighting the limitations of Reinforcement Learning (RL) based controllers used in existing methods. To tackle these challenges, we introduce Ghost in the Minecraft (GITM), a novel framework integrates Large Language Models (LLMs) with text-based knowledge and memory, aiming to create Generally Capable Agents (GCAs) in Minecraft. These agents, equipped with the logic and common sense capabilities of LLMs, can skillfully navigate complex, sparse-reward environments with text-based 
&lt;/p&gt;</description></item><item><title>NavGPT&#26159;&#22522;&#20110;LLM&#30340;&#23548;&#33322;&#26234;&#33021;&#20307;&#65292;&#21487;&#20197;&#22312;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#65288;VLN&#65289;&#20013;&#65292;&#36890;&#36807;&#23545;&#25991;&#26412;&#25551;&#36848;&#36827;&#34892;&#25512;&#29702;&#65292;&#25191;&#34892;&#38646;-shot&#36830;&#32493;&#21160;&#20316;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#20855;&#26377;&#39640;&#32423;&#35268;&#21010;&#33021;&#21147;&#65292;&#21487;&#20197;&#23558;&#25351;&#20196;&#20998;&#35299;&#25104;&#23376;&#30446;&#26631;&#12289;&#25972;&#21512;&#24120;&#35782;&#30693;&#35782;&#20197;&#36827;&#34892;&#38556;&#30861;&#29289;&#36991;&#20813;&#65292;&#24182;&#21442;&#32771;&#20808;&#21069;&#30340;&#27493;&#39588;&#36827;&#34892;&#28548;&#28165;&#12290;NavGPT&#23637;&#31034;&#20102;&#36890;&#29992;&#20307;&#29616;&#26234;&#33021;&#20307;&#21457;&#23637;&#30340;&#32654;&#22909;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2305.16986</link><description>&lt;p&gt;
NavGPT: &#24102;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#20013;&#30340;&#26174;&#24335;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models. (arXiv:2305.16986v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16986
&lt;/p&gt;
&lt;p&gt;
NavGPT&#26159;&#22522;&#20110;LLM&#30340;&#23548;&#33322;&#26234;&#33021;&#20307;&#65292;&#21487;&#20197;&#22312;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#65288;VLN&#65289;&#20013;&#65292;&#36890;&#36807;&#23545;&#25991;&#26412;&#25551;&#36848;&#36827;&#34892;&#25512;&#29702;&#65292;&#25191;&#34892;&#38646;-shot&#36830;&#32493;&#21160;&#20316;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#20855;&#26377;&#39640;&#32423;&#35268;&#21010;&#33021;&#21147;&#65292;&#21487;&#20197;&#23558;&#25351;&#20196;&#20998;&#35299;&#25104;&#23376;&#30446;&#26631;&#12289;&#25972;&#21512;&#24120;&#35782;&#30693;&#35782;&#20197;&#36827;&#34892;&#38556;&#30861;&#29289;&#36991;&#20813;&#65292;&#24182;&#21442;&#32771;&#20808;&#21069;&#30340;&#27493;&#39588;&#36827;&#34892;&#28548;&#28165;&#12290;NavGPT&#23637;&#31034;&#20102;&#36890;&#29992;&#20307;&#29616;&#26234;&#33021;&#20307;&#21457;&#23637;&#30340;&#32654;&#22909;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20363;&#22914;ChatGPT&#21644;GPT-4&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;&#35268;&#27169;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#27169;&#22411;&#30340;&#25193;&#23637;&#20013;&#23637;&#29616;&#20986;&#26174;&#33879;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#36825;&#31181;&#36235;&#21183;&#24378;&#35843;&#20102;&#20351;&#29992;&#26080;&#38480;&#35821;&#35328;&#25968;&#25454;&#35757;&#32451;LLM&#30340;&#28508;&#21147;&#65292;&#25512;&#21160;&#20102;&#36890;&#29992;&#20307;&#29616;&#26234;&#33021;&#20307;&#30340;&#21457;&#23637;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;NavGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#32431;&#31929;&#22522;&#20110;LLM&#30340;&#25351;&#20196;&#36319;&#38543;&#23548;&#33322;&#26234;&#33021;&#20307;&#65292;&#36890;&#36807;&#20026;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#65288;VLN&#65289;&#25191;&#34892;&#38646;-shot&#30340;&#36830;&#32493;&#21160;&#20316;&#39044;&#27979;&#65292;&#25581;&#31034;&#20102;&#23545;&#20110;&#22312;&#22797;&#26434;&#30340;&#29616;&#23454;&#22330;&#26223;&#19979;GPT&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#22312;&#27599;&#19968;&#27493;&#20013;&#65292;NavGPT&#23558;&#35270;&#35273;&#35266;&#23519;&#12289;&#23548;&#33322;&#21382;&#21490;&#21644;&#26410;&#26469;&#21487;&#25506;&#32034;&#26041;&#21521;&#30340;&#25991;&#26412;&#25551;&#36848;&#20316;&#20026;&#36755;&#20837;&#65292;&#25512;&#29702;&#20986;&#26234;&#33021;&#20307;&#30340;&#24403;&#21069;&#29366;&#24577;&#65292;&#24182;&#20915;&#23450;&#22914;&#20309;&#25509;&#36817;&#30446;&#26631;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;NavGPT&#21487;&#20197;&#26126;&#30830;&#22320;&#25191;&#34892;&#23548;&#33322;&#30340;&#39640;&#32423;&#35268;&#21010;&#65292;&#21253;&#25324;&#23558;&#25351;&#20196;&#20998;&#35299;&#25104;&#23376;&#30446;&#26631;&#12289;&#25972;&#21512;&#24120;&#35782;&#30693;&#35782;&#20197;&#36827;&#34892;&#38556;&#30861;&#29289;&#36991;&#20813;&#65292;&#24182;&#21442;&#32771;&#20808;&#21069;&#30340;&#27493;&#39588;&#36827;&#34892;&#28548;&#28165;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;LLM&#21487;&#33021;&#25104;&#20026;&#22797;&#26434;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#20256;&#32479;&#27969;&#31243;&#30340;&#24378;&#26377;&#21147;&#26367;&#20195;&#21697;&#65292;&#23637;&#31034;&#20102;&#36890;&#29992;&#20307;&#29616;&#26234;&#33021;&#20307;&#21457;&#23637;&#30340;&#32654;&#22909;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Trained with an unprecedented scale of data, large language models (LLMs) like ChatGPT and GPT-4 exhibit the emergence of significant reasoning abilities from model scaling. Such a trend underscored the potential of training LLMs with unlimited language data, advancing the development of a universal embodied agent. In this work, we introduce the NavGPT, a purely LLM-based instruction-following navigation agent, to reveal the reasoning capability of GPT models in complex embodied scenes by performing zero-shot sequential action prediction for vision-and-language navigation (VLN). At each step, NavGPT takes the textual descriptions of visual observations, navigation history, and future explorable directions as inputs to reason the agent's current status, and makes the decision to approach the target. Through comprehensive experiments, we demonstrate NavGPT can explicitly perform high-level planning for navigation, including decomposing instruction into sub-goal, integrating commonsense k
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#12289;&#22522;&#20110;&#21152;&#26435;&#24179;&#22343;&#30340;&#39046;&#22495;&#23545;&#40784;&#21069;&#32512;&#24179;&#22343;&#26041;&#27861;&#65288;DAPA&#65289;&#65292;&#29992;&#20110;&#25277;&#35937;&#25688;&#35201;&#20013;&#30340;&#39046;&#22495;&#27867;&#21270;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#28304;&#22495;&#25193;&#23637;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16820</link><description>&lt;p&gt;
&#38754;&#21521;&#25277;&#35937;&#25688;&#35201;&#20013;&#30340;&#39046;&#22495;&#27867;&#21270;&#30340;&#39046;&#22495;&#23545;&#40784;&#21069;&#32512;&#24179;&#22343;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Domain Aligned Prefix Averaging for Domain Generalization in Abstractive Summarization. (arXiv:2305.16820v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16820
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#12289;&#22522;&#20110;&#21152;&#26435;&#24179;&#22343;&#30340;&#39046;&#22495;&#23545;&#40784;&#21069;&#32512;&#24179;&#22343;&#26041;&#27861;&#65288;DAPA&#65289;&#65292;&#29992;&#20110;&#25277;&#35937;&#25688;&#35201;&#20013;&#30340;&#39046;&#22495;&#27867;&#21270;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#28304;&#22495;&#25193;&#23637;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#20110;&#25277;&#35937;&#25688;&#35201;&#20013;&#30340;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#65292;&#22522;&#20110;&#21152;&#26435;&#24179;&#22343;&#30340;&#39046;&#22495;&#23545;&#40784;&#21069;&#32512;&#24179;&#22343;&#26041;&#27861;&#65288;DAPA&#65289;&#12290;&#36890;&#36807;&#32473;&#23450;&#22810;&#20010;&#28304;&#22495;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#20026;&#27599;&#20010;&#22495;&#35757;&#32451;&#19968;&#20010;&#21069;&#32512;&#65292;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#21069;&#32512;&#29983;&#25104;&#23569;&#37327;&#30446;&#26631;&#22495;&#25991;&#26723;&#30340;&#25688;&#35201;&#65292;&#35745;&#31639;&#25152;&#38656;&#30340;&#26435;&#37325;&#26469;&#24179;&#22343;&#28304;&#21069;&#32512;&#12290;&#22312;DAPA&#20013;&#65292;&#21069;&#32512;&#35843;&#25972;&#20801;&#35768;&#36731;&#37327;&#32423;&#30340;&#24494;&#35843;&#65292;&#21152;&#26435;&#24179;&#22343;&#20801;&#35768;&#26377;&#25928;&#22320;&#28155;&#21152;&#26032;&#30340;&#28304;&#22495;&#12290;&#22312;&#22235;&#20010;&#19981;&#21516;&#30340;&#25688;&#35201;&#39046;&#22495;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;DAPA&#34920;&#29616;&#20986;&#19982;&#22522;&#20934;&#26041;&#27861;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#20854;&#21069;&#32512;&#24179;&#22343;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain generalization is hitherto an underexplored area applied in abstractive summarization. Moreover, most existing works on domain generalization have sophisticated training algorithms. In this paper, we propose a lightweight, weight averaging based, Domain Aligned Prefix Averaging approach to domain generalization for abstractive summarization. Given a number of source domains, our method first trains a prefix for each one of them. These source prefixes generate summaries for a small number of target domain documents. The similarity of the generated summaries to their corresponding documents is used for calculating weights required to average source prefixes. In DAPA, prefix tuning allows for lightweight finetuning, and weight averaging allows for the computationally efficient addition of new source domains. When evaluated on four diverse summarization domains, DAPA shows comparable or better performance against the baselines, demonstrating the effectiveness of its prefix averaging
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;InterFormer&#65292;&#29992;&#20110;&#20132;&#20114;&#24335;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#34701;&#21512;&#65292;&#20197;&#23398;&#20064;&#26356;&#22909;&#30340;ASR&#34920;&#31034;&#12290;&#36890;&#36807;&#32452;&#21512;&#21367;&#31215;&#22359;&#21644;&#21464;&#24418;&#22120;&#22359;&#65292;&#20197;&#21450;&#24341;&#20837;BFIM&#21644;SFM&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#30340;&#20132;&#20114;&#21644;&#34701;&#21512;&#65292;&#21462;&#24471;&#20102;&#22312;&#20844;&#20849;ASR&#25968;&#25454;&#38598;&#19978;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16342</link><description>&lt;p&gt;
InterFormer: &#28151;&#21512;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#30340;&#20132;&#20114;&#24335;&#34701;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
InterFormer: Interactive Local and Global Features Fusion for Automatic Speech Recognition. (arXiv:2305.16342v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;InterFormer&#65292;&#29992;&#20110;&#20132;&#20114;&#24335;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#34701;&#21512;&#65292;&#20197;&#23398;&#20064;&#26356;&#22909;&#30340;ASR&#34920;&#31034;&#12290;&#36890;&#36807;&#32452;&#21512;&#21367;&#31215;&#22359;&#21644;&#21464;&#24418;&#22120;&#22359;&#65292;&#20197;&#21450;&#24341;&#20837;BFIM&#21644;SFM&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#30340;&#20132;&#20114;&#21644;&#34701;&#21512;&#65292;&#21462;&#24471;&#20102;&#22312;&#20844;&#20849;ASR&#25968;&#25454;&#38598;&#19978;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#32780;&#35328;&#65292;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#37117;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#35768;&#22810;&#26368;&#36817;&#30340;&#26041;&#27861;&#24050;&#32463;&#35777;&#23454;&#65292;&#31616;&#21333;&#22320;&#21512;&#24182;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;ASR&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#24573;&#30053;&#20102;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#20018;&#34892;&#26550;&#26500;&#26080;&#27861;&#21453;&#26144;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;InterFormer&#65292;&#29992;&#20110;&#20132;&#20114;&#24335;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#34701;&#21512;&#65292;&#20197;&#23398;&#20064;&#26356;&#22909;&#30340;ASR&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#21367;&#31215;&#22359;&#19982;&#21464;&#24418;&#22120;&#22359;&#20197;&#24182;&#34892;&#35774;&#35745;&#30456;&#32467;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21452;&#21521;&#29305;&#24449;&#20132;&#20114;&#27169;&#22359;&#65288;BFIM&#65289;&#21644;&#36873;&#25321;&#24615;&#34701;&#21512;&#27169;&#22359;&#65288;SFM&#65289;&#26469;&#23454;&#29616;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#30340;&#20132;&#20114;&#21644;&#34701;&#21512;&#12290;&#22312;&#20844;&#20849;ASR&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;InterFormer&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#20854;&#20182;Transformer&#21644;Conformer&#27169;&#22411;&#20855;&#26377;&#26356;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The local and global features are both essential for automatic speech recognition (ASR). Many recent methods have verified that simply combining local and global features can further promote ASR performance. However, these methods pay less attention to the interaction of local and global features, and their series architectures are rigid to reflect local and global relationships. To address these issues, this paper proposes InterFormer for interactive local and global features fusion to learn a better representation for ASR. Specifically, we combine the convolution block with the transformer block in a parallel design. Besides, we propose a bidirectional feature interaction module (BFIM) and a selective fusion module (SFM) to implement the interaction and fusion of local and global features, respectively. Extensive experiments on public ASR datasets demonstrate the effectiveness of our proposed InterFormer and its superior performance over the other Transformer and Conformer models.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;ASR&#25216;&#26415;&#22312;&#24773;&#24863;&#35821;&#38899;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#25506;&#31350;&#20102;&#24773;&#24863;&#22914;&#20309;&#24433;&#21709;ASR&#12290;&#21516;&#26102;&#65292;&#36824;&#30740;&#31350;&#20102;ASR&#23545;&#22522;&#20110;&#25991;&#26412;&#30340;&#24773;&#24863;&#35782;&#21035;&#30340;&#24433;&#21709;&#12290;&#35813;&#30740;&#31350;&#26088;&#22312;&#25581;&#31034;ASR&#21644;SER&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#30456;&#20114;&#24433;&#21709;&#65292;&#20197;&#20419;&#36827;ASR&#25216;&#26415;&#23545;&#24773;&#24863;&#35821;&#38899;&#30340;&#36866;&#24212;&#21644;SER&#25216;&#26415;&#22312;&#23454;&#38469;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.16065</link><description>&lt;p&gt;
ASR&#25216;&#26415;&#19982;&#24773;&#24863;&#35821;&#38899;&#65306;&#23545;&#35821;&#38899;&#19982;&#24773;&#24863;&#35782;&#21035;&#30456;&#20114;&#24433;&#21709;&#30340;&#21333;&#35789;&#32423;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
ASR and Emotional Speech: A Word-Level Investigation of the Mutual Impact of Speech and Emotion Recognition. (arXiv:2305.16065v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16065
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;ASR&#25216;&#26415;&#22312;&#24773;&#24863;&#35821;&#38899;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#25506;&#31350;&#20102;&#24773;&#24863;&#22914;&#20309;&#24433;&#21709;ASR&#12290;&#21516;&#26102;&#65292;&#36824;&#30740;&#31350;&#20102;ASR&#23545;&#22522;&#20110;&#25991;&#26412;&#30340;&#24773;&#24863;&#35782;&#21035;&#30340;&#24433;&#21709;&#12290;&#35813;&#30740;&#31350;&#26088;&#22312;&#25581;&#31034;ASR&#21644;SER&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#30456;&#20114;&#24433;&#21709;&#65292;&#20197;&#20419;&#36827;ASR&#25216;&#26415;&#23545;&#24773;&#24863;&#35821;&#38899;&#30340;&#36866;&#24212;&#21644;SER&#25216;&#26415;&#22312;&#23454;&#38469;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65288;SER&#65289;&#20013;&#65292;&#20026;&#20102;&#24212;&#23545;&#22266;&#26377;&#30340;&#21464;&#24322;&#24615;&#65292;&#36890;&#24120;&#20250;&#20351;&#29992;&#25991;&#26412;&#25968;&#25454;&#26469;&#36741;&#21161;&#38899;&#39057;&#20449;&#21495;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#20013;&#20381;&#36182;&#20110;&#20154;&#24037;&#26631;&#27880;&#30340;&#25991;&#26412;&#25968;&#25454;&#65292;&#36825;&#38459;&#30861;&#20102;&#23454;&#29992;&#21270;SER&#31995;&#32479;&#30340;&#21457;&#23637;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#20351;&#29992;&#22235;&#20010;ASR&#31995;&#32479;&#65288;&#20998;&#21035;&#26159;Kaldi ASR&#12289;wav2vec2&#12289;Conformer&#21644;Whisper&#65289;&#21644;&#19977;&#20010;&#35821;&#26009;&#24211;&#65288;IEMOCAP&#12289;MOSI&#21644;MELD&#65289;&#26469;&#20998;&#26512;&#24773;&#24863;&#35821;&#38899;&#19978;&#30340;ASR&#34920;&#29616;&#65292;&#24182;&#19988;&#36890;&#36807;&#20998;&#26512;ASR&#36716;&#24405;&#20013;&#30340;&#35789;&#38169;&#35823;&#21644;&#32622;&#20449;&#24230;&#20998;&#24067;&#26469;&#20102;&#35299;&#24773;&#24863;&#22914;&#20309;&#24433;&#21709;ASR&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#20855;&#26377;&#19981;&#26029;&#22686;&#21152;&#21333;&#35789;&#38169;&#35823;&#29575;&#30340;ASR&#36716;&#24405;&#36827;&#34892;&#22522;&#20110;&#25991;&#26412;&#30340;&#24773;&#24863;&#35782;&#21035;&#65292;&#20197;&#30740;&#31350;ASR&#22914;&#20309;&#24433;&#21709;SER&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#25581;&#31034;ASR&#21644;SER&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#30456;&#20114;&#24433;&#21709;&#65292;&#20197;&#20419;&#36827;ASR&#25216;&#26415;&#23545;&#24773;&#24863;&#35821;&#38899;&#30340;&#36866;&#24212;&#21644;SER&#25216;&#26415;&#22312;&#23454;&#38469;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Speech Emotion Recognition (SER), textual data is often used alongside audio signals to address their inherent variability. However, the reliance on human annotated text in most research hinders the development of practical SER systems. To overcome this challenge, we investigate how Automatic Speech Recognition (ASR) performs on emotional speech by analyzing the ASR performance on emotion corpora and examining the distribution of word errors and confidence scores in ASR transcripts to gain insight into how emotion affects ASR. We utilize four ASR systems, namely Kaldi ASR, wav2vec2, Conformer, and Whisper, and three corpora: IEMOCAP, MOSI, and MELD to ensure generalizability. Additionally, we conduct text-based SER on ASR transcripts with increasing word error rates to investigate how ASR affects SER. The objective of this study is to uncover the relationship and mutual impact of ASR and SER, in order to facilitate ASR adaptation to emotional speech and the use of SER in real world.
&lt;/p&gt;</description></item><item><title>ChatGPT&#34920;&#29616;&#20986;&#20154;&#31867;&#35821;&#35328;&#22788;&#29702;&#30340;&#38899;&#38901;&#20559;&#35265;&#65292;&#26356;&#20542;&#21521;&#20110;&#20351;&#29992;&#36741;&#38899;&#32780;&#19981;&#26159;&#20803;&#38899;&#26469;&#35782;&#21035;&#21333;&#35789;&#12290;</title><link>http://arxiv.org/abs/2305.15929</link><description>&lt;p&gt;
ChatGPT&#20013;&#20986;&#29616;&#20102;&#38899;&#38901;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Emergence of a phonological bias in ChatGPT. (arXiv:2305.15929v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15929
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#34920;&#29616;&#20986;&#20154;&#31867;&#35821;&#35328;&#22788;&#29702;&#30340;&#38899;&#38901;&#20559;&#35265;&#65292;&#26356;&#20542;&#21521;&#20110;&#20351;&#29992;&#36741;&#38899;&#32780;&#19981;&#26159;&#20803;&#38899;&#26469;&#35782;&#21035;&#21333;&#35789;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20363;&#22914;OpenAI&#30340;ChatGPT&#65292;&#22240;&#20854;&#22312;&#35821;&#35328;&#20351;&#29992;&#26041;&#38754;&#30340;&#20986;&#33394;&#34920;&#29616;&#32780;&#21463;&#21040;&#20844;&#20247;&#30340;&#20851;&#27880;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#35777;&#26126;&#20102;ChatGPT&#26174;&#31034;&#20102;&#20154;&#31867;&#35821;&#35328;&#22788;&#29702;&#30340;&#38899;&#38901;&#20559;&#35265;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#23601;&#20687;&#20154;&#31867;&#19968;&#26679;&#65292;ChatGPT&#20855;&#26377;&#19968;&#20010;&#36741;&#38899;&#20559;&#35265;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#36825;&#20010;&#32842;&#22825;&#26426;&#22120;&#20154;&#20542;&#21521;&#20110;&#20351;&#29992;&#36741;&#38899;&#32780;&#19981;&#26159;&#20803;&#38899;&#26469;&#35782;&#21035;&#21333;&#35789;&#12290;&#36825;&#22312;&#20855;&#26377;&#19981;&#21516;&#36741;&#38899;&#21644;&#20803;&#38899;&#20998;&#24067;&#27604;&#20363;&#30340;&#35821;&#35328;&#65288;&#22914;&#33521;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#65289;&#20013;&#37117;&#26377;&#35266;&#23519;&#21040;&#12290;&#23613;&#31649;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#35821;&#35328;&#21050;&#28608;&#21644;&#20154;&#31867;&#23156;&#20799;&#33719;&#24471;&#35821;&#35328;&#30340;&#26041;&#24335;&#19978;&#23384;&#22312;&#24046;&#24322;&#65292;&#20294;&#36825;&#26679;&#30340;&#35757;&#32451;&#20284;&#20046;&#36275;&#20197;&#22312;ChatGPT&#20013;&#24341;&#20986;&#19968;&#20010;&#38899;&#38901;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current large language models, such as OpenAI's ChatGPT, have captured the public's attention because how remarkable they are in the use of language. Here, I demonstrate that ChatGPT displays phonological biases that are a hallmark of human language processing. More concretely, just like humans, ChatGPT has a consonant bias. That is, the chatbot has a tendency to use consonants over vowels to identify words. This is observed across languages that differ in their relative distribution of consonants and vowels such as English and Spanish. Despite the differences in how current artificial intelligence language models are trained to process linguistic stimuli and how human infants acquire language, such training seems to be enough for the emergence of a phonological bias in ChatGPT
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MEMEX&#20219;&#21153;&#65292;&#36890;&#36807;&#30693;&#35782;&#22686;&#24378;&#30340;&#19978;&#19979;&#25991;&#21270;&#25216;&#26415;&#26816;&#27979;&#36855;&#22240;&#30340;&#35299;&#37322;&#24615;&#35777;&#25454;&#12290;&#36890;&#36807;&#26500;&#24314;MCC&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#20998;&#23618;&#26041;&#27861;&#25429;&#25417;&#36855;&#22240;&#21644;&#19978;&#19979;&#25991;&#30340;&#36328;&#27169;&#24577;&#35821;&#20041;&#20381;&#36182;&#65292;&#25552;&#20986;&#20102;MIME&#22810;&#27169;&#24335;&#31070;&#32463;&#26694;&#26550;&#26469;&#35299;&#37322;&#36855;&#22240;&#12290;</title><link>http://arxiv.org/abs/2305.15913</link><description>&lt;p&gt;
MEMEX&#65306;&#36890;&#36807;&#30693;&#35782;&#22686;&#24378;&#30340;&#19978;&#19979;&#25991;&#21270;&#26469;&#26816;&#27979;&#36855;&#22240;&#30340;&#35299;&#37322;&#24615;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
MEMEX: Detecting Explanatory Evidence for Memes via Knowledge-Enriched Contextualization. (arXiv:2305.15913v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MEMEX&#20219;&#21153;&#65292;&#36890;&#36807;&#30693;&#35782;&#22686;&#24378;&#30340;&#19978;&#19979;&#25991;&#21270;&#25216;&#26415;&#26816;&#27979;&#36855;&#22240;&#30340;&#35299;&#37322;&#24615;&#35777;&#25454;&#12290;&#36890;&#36807;&#26500;&#24314;MCC&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#20998;&#23618;&#26041;&#27861;&#25429;&#25417;&#36855;&#22240;&#21644;&#19978;&#19979;&#25991;&#30340;&#36328;&#27169;&#24577;&#35821;&#20041;&#20381;&#36182;&#65292;&#25552;&#20986;&#20102;MIME&#22810;&#27169;&#24335;&#31070;&#32463;&#26694;&#26550;&#26469;&#35299;&#37322;&#36855;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36855;&#22240;&#26159;&#31038;&#20132;&#23186;&#20307;&#19978;&#24378;&#22823;&#30340;&#20132;&#38469;&#24037;&#20855;&#65292;&#23427;&#20204;&#22312;&#25919;&#27835;&#12289;&#21382;&#21490;&#21644;&#31038;&#20250;&#25991;&#21270;&#29616;&#35937;&#20013;&#30340;&#19981;&#26029;&#21457;&#23637;&#20351;&#20854;&#25104;&#20026;&#29702;&#24819;&#30340;&#20132;&#27969;&#23186;&#20171;&#12290;&#20026;&#20102;&#29702;&#35299;&#36855;&#22240;&#20256;&#36798;&#30340;&#24494;&#22937;&#20449;&#24687;&#65292;&#24517;&#39035;&#20102;&#35299;&#20419;&#36827;&#20854;&#25972;&#20307;&#21560;&#25910;&#30340;&#32972;&#26223;&#12290;&#38500;&#20102;&#20687;knowyourmeme.com&#36825;&#26679;&#30340;&#20960;&#20010;&#32593;&#31449;&#23545;&#36855;&#22240;&#21450;&#20854;&#20803;&#25968;&#25454;&#36827;&#34892;&#25968;&#23383;&#23384;&#26723;&#22806;&#65292;&#30446;&#21069;&#27809;&#26377;&#26377;&#25928;&#30340;&#26041;&#27861;&#21160;&#24577;&#22320;&#25512;&#26029;&#36855;&#22240;&#30340;&#19978;&#19979;&#25991;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65292;MEMEX&#65292;&#32473;&#23450;&#19968;&#20010;&#36855;&#22240;&#21644;&#19968;&#20010;&#30456;&#20851;&#30340;&#25991;&#26723;&#65292;&#20854;&#30446;&#30340;&#26159;&#25366;&#25496;&#31616;&#27905;&#22320;&#35299;&#37322;&#36855;&#22240;&#32972;&#26223;&#30340;&#19978;&#19979;&#25991;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;MCC&#65288;Meme Context Corpus&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#20026;MEMEX&#35774;&#35745;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#22522;&#20934;&#27979;&#35797;MCC&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MIME&#65288;MultImodal Meme Explainer&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#27169;&#24335;&#31070;&#32463;&#26694;&#26550;&#65292;&#20351;&#29992;&#36890;&#35782;&#24378;&#21270;&#30340;&#36855;&#22240;&#34920;&#31034;&#21644;&#19968;&#31181;&#20998;&#23618;&#26041;&#27861;&#26469;&#25429;&#25417;&#36855;&#22240;&#21644;&#19978;&#19979;&#25991;&#20043;&#38388;&#30340;&#36328;&#27169;&#24577;&#35821;&#20041;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;
Memes are a powerful tool for communication over social media. Their affinity for evolving across politics, history, and sociocultural phenomena makes them an ideal communication vehicle. To comprehend the subtle message conveyed within a meme, one must understand the background that facilitates its holistic assimilation. Besides digital archiving of memes and their metadata by a few websites like knowyourmeme.com, currently, there is no efficient way to deduce a meme's context dynamically. In this work, we propose a novel task, MEMEX given a meme and a related document, the aim is to mine the context that succinctly explains the background of the meme. At first, we develop MCC (Meme Context Corpus), a novel dataset for MEMEX. Further, to benchmark MCC, we propose MIME (MultImodal Meme Explainer), a multimodal neural framework that uses common sense enriched meme representation and a layered approach to capture the cross-modal semantic dependencies between the meme and the context. M
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#19978;&#19979;&#25991;&#21098;&#26525;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#27169;&#22411;&#34920;&#29616;&#21147;&#30340;&#21516;&#26102;&#65292;&#21160;&#24577;&#20943;&#23569;&#26080;&#25928;&#20449;&#24687;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#35813;&#25216;&#26415;&#21487;&#20197;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#24494;&#35843;&#36807;&#31243;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.15805</link><description>&lt;p&gt;
&#21160;&#24577;&#19978;&#19979;&#25991;&#21098;&#26525;&#29992;&#20110;&#39640;&#25928;&#21644;&#21487;&#35299;&#37322;&#30340;&#33258;&#22238;&#24402;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers. (arXiv:2305.15805v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15805
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#19978;&#19979;&#25991;&#21098;&#26525;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#27169;&#22411;&#34920;&#29616;&#21147;&#30340;&#21516;&#26102;&#65292;&#21160;&#24577;&#20943;&#23569;&#26080;&#25928;&#20449;&#24687;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#35813;&#25216;&#26415;&#21487;&#20197;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#24494;&#35843;&#36807;&#31243;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#37319;&#29992;&#30340;&#33258;&#22238;&#24402;&#21464;&#25442;&#22120;&#38590;&#20197;&#25193;&#23637;&#21040;&#38271;&#24207;&#21015;&#12290;&#23613;&#31649;&#26377;&#20960;&#39033;&#24037;&#20316;&#35797;&#22270;&#20943;&#23569;&#23427;&#20204;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#20294;&#22823;&#22810;&#25968;LLM&#20173;&#28982;&#22312;&#25152;&#26377;&#26631;&#35760;&#23545;&#20043;&#38388;&#37319;&#29992;&#27880;&#24847;&#23618;&#65292;&#20174;&#32780;&#20135;&#29983;&#20108;&#27425;&#25104;&#26412;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20445;&#30041;&#27169;&#22411;&#30340;&#34920;&#29616;&#21147;&#26469;&#21160;&#24577;&#20462;&#21098;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20174;&#32780;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20943;&#23569;&#20869;&#23384;&#21644;&#35745;&#31639;&#35201;&#27714;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#21487;&#23398;&#20064;&#26426;&#21046;&#65292;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#30830;&#23450;&#21738;&#20123;&#26080;&#20851;&#30340;&#26631;&#35760;&#21487;&#20197;&#20174;&#19978;&#19979;&#25991;&#20013;&#21024;&#38500;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#35299;&#20915;&#20102;&#24615;&#33021;&#38382;&#39064;&#65292;&#32780;&#19988;&#22686;&#24378;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#20026;&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#27934;&#23519;&#21147;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#24494;&#35843;&#36807;&#31243;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#19988;&#21098;&#26525;&#24378;&#24230;&#21487;&#20197;&#30001;&#31232;&#30095;&#24230;&#21442;&#25968;&#25351;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autoregressive Transformers adopted in Large Language Models (LLMs) are hard to scale to long sequences. Despite several works trying to reduce their computational cost, most of LLMs still adopt attention layers between all pairs of tokens in the sequence, thus incurring a quadratic cost. In this study, we present a novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference. Our method employs a learnable mechanism that determines which uninformative tokens can be dropped from the context at any point across the generation process. By doing so, our approach not only addresses performance concerns but also enhances interpretability, providing valuable insight into the model's decision-making process. Our technique can be applied to existing pre-trained models through a straightforward fine-tuning process, and the pruning strength can be specified by a sparsity para
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;LLMs&#30340;&#39640;&#25928;LLM&#25512;&#29702;&#27969;&#27700;&#32447;&#65292;&#36890;&#36807;&#21033;&#29992;LLMs&#20934;&#30830;&#24863;&#30693;&#21644;&#39044;&#27979;&#21709;&#24212;&#38271;&#24230;&#30340;&#28508;&#21147;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#39640;&#25928;&#30340;&#24207;&#21015;&#35843;&#24230;&#25216;&#26415;&#65292;&#23558;&#20855;&#26377;&#31867;&#20284;&#21709;&#24212;&#38271;&#24230;&#30340;&#26597;&#35810;&#20998;&#32452;&#25104;&#24494;&#25209;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#29616;&#39640;&#25928;&#30340;&#25512;&#29702;&#21534;&#21520;&#37327;&#30340;&#21516;&#26102;&#20063;&#19981;&#24433;&#21709;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13144</link><description>&lt;p&gt;
&#22238;&#22797;&#38271;&#24230;&#24863;&#30693;&#19982;&#24207;&#21015;&#35843;&#24230;&#65306;&#19968;&#31181;&#21033;&#29992;LLM&#30340;&#39640;&#25928;LLM&#25512;&#29702;&#27969;&#27700;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Response Length Perception and Sequence Scheduling: An LLM-Empowered LLM Inference Pipeline. (arXiv:2305.13144v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13144
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;LLMs&#30340;&#39640;&#25928;LLM&#25512;&#29702;&#27969;&#27700;&#32447;&#65292;&#36890;&#36807;&#21033;&#29992;LLMs&#20934;&#30830;&#24863;&#30693;&#21644;&#39044;&#27979;&#21709;&#24212;&#38271;&#24230;&#30340;&#28508;&#21147;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#39640;&#25928;&#30340;&#24207;&#21015;&#35843;&#24230;&#25216;&#26415;&#65292;&#23558;&#20855;&#26377;&#31867;&#20284;&#21709;&#24212;&#38271;&#24230;&#30340;&#26597;&#35810;&#20998;&#32452;&#25104;&#24494;&#25209;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#29616;&#39640;&#25928;&#30340;&#25512;&#29702;&#21534;&#21520;&#37327;&#30340;&#21516;&#26102;&#20063;&#19981;&#24433;&#21709;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#26174;&#31034;&#20986;&#21069;&#25152;&#26410;&#26377;&#30340;&#33021;&#21147;&#65292;&#38761;&#21629;&#20102;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;LLMs&#30340;&#25512;&#29702;&#36807;&#31243;&#20855;&#26377;&#37325;&#35201;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;LLMs&#30340;&#39640;&#25928;LLM&#25512;&#29702;&#27969;&#27700;&#32447;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;LLMs&#20934;&#30830;&#24863;&#30693;&#21644;&#39044;&#27979;&#21709;&#24212;&#38271;&#24230;&#30340;&#28508;&#21147;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#39640;&#25928;&#30340;&#24207;&#21015;&#35843;&#24230;&#25216;&#26415;&#65292;&#23558;&#20855;&#26377;&#31867;&#20284;&#21709;&#24212;&#38271;&#24230;&#30340;&#26597;&#35810;&#20998;&#32452;&#25104;&#24494;&#25209;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;LLaMA&#27169;&#22411;&#30340;&#30495;&#23454;&#19990;&#30028;&#25351;&#20196;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#32467;&#26524;&#26174;&#31034;&#20986;86%&#30340;&#25512;&#29702;&#21534;&#21520;&#37327;&#30340;&#25552;&#39640;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#26377;&#25928;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20854;&#20182;&#25512;&#29702;&#21152;&#36895;&#25216;&#26415;&#26080;&#20851;&#65292;&#26159;LLMs&#25512;&#29702;&#35768;&#22810;&#29616;&#26377;&#24037;&#20855;&#21253;(&#22914;FlashAttention&#12289;&#37327;&#21270;)&#30340;&#26377;&#20215;&#20540;&#34917;&#20805;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have revolutionized the field of AI, demonstrating unprecedented capacity across various tasks. However, the inference process for LLMs comes with significant computational costs. In this paper, we propose an efficient LLM inference pipeline that harnesses the power of LLMs. Our approach begins by tapping into the potential of LLMs to accurately perceive and predict the response length with minimal overhead. By leveraging this information, we introduce an efficient sequence scheduling technique that groups queries with similar response lengths into micro-batches. We evaluate our approach on real-world instruction datasets using the LLaMA-based model, and our results demonstrate an impressive 86% improvement in inference throughput without compromising effectiveness. Notably, our method is orthogonal to other inference acceleration techniques, making it a valuable addition to many existing toolkits (e.g., FlashAttention, Quantization) for LLM inference.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20174;&#21477;&#32534;&#36753;&#27169;&#22411;&#30340;&#25991;&#26412;&#21040;SQL&#30340;&#35821;&#35328;&#27169;&#22411;&#32416;&#38169;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#26032;&#30340;SQL&#26597;&#35810;&#34920;&#31034;&#25913;&#36827;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#31934;&#30830;&#21305;&#37197;&#20934;&#30830;&#29575;&#65292;&#25552;&#39640;&#20102;2.4-6.5&#65292;&#26368;&#22810;&#25552;&#39640;4.3&#20010;&#30334;&#20998;&#28857;&#12290;</title><link>http://arxiv.org/abs/2305.13073</link><description>&lt;p&gt;
&#25991;&#26412;&#21040;SQL&#30340;&#35821;&#35328;&#27169;&#22411;&#32416;&#38169;
&lt;/p&gt;
&lt;p&gt;
Text-to-SQL Error Correction with Language Models of Code. (arXiv:2305.13073v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13073
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20174;&#21477;&#32534;&#36753;&#27169;&#22411;&#30340;&#25991;&#26412;&#21040;SQL&#30340;&#35821;&#35328;&#27169;&#22411;&#32416;&#38169;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#26032;&#30340;SQL&#26597;&#35810;&#34920;&#31034;&#25913;&#36827;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#31934;&#30830;&#21305;&#37197;&#20934;&#30830;&#29575;&#65292;&#25552;&#39640;&#20102;2.4-6.5&#65292;&#26368;&#22810;&#25552;&#39640;4.3&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25991;&#26412;&#21040;SQL&#35299;&#26512;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#24403;&#21069;&#30340;&#35821;&#20041;&#35299;&#26512;&#22120;&#20173;&#19981;&#22815;&#20934;&#30830;&#20197;&#23454;&#38469;&#24212;&#29992;&#12290;&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#26500;&#24314;&#33258;&#21160;&#25991;&#26412;&#21040;SQL&#32416;&#38169;&#27169;&#22411;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#21333;&#35789;&#23618;&#38754;&#30340;&#32534;&#36753;&#32570;&#20047;&#19978;&#19979;&#25991;&#24182;&#19988;&#26377;&#26102;&#19981;&#26126;&#30830;&#65292;&#22240;&#27492;&#25552;&#20986;&#26500;&#24314;&#20174;&#21477;&#32534;&#36753;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;&#22823;&#22810;&#25968;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#27809;&#26377;&#19987;&#38376;&#39044;&#35757;&#32451;SQL&#65292;&#20294;&#23427;&#20204;&#29087;&#24713;Python&#31561;&#32534;&#31243;&#35821;&#35328;&#20013;&#30340;&#24120;&#35265;&#25968;&#25454;&#32467;&#26500;&#21644;&#20854;&#25805;&#20316;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;SQL&#26597;&#35810;&#34920;&#31034;&#21450;&#20854;&#32534;&#36753;&#26041;&#27861;&#65292;&#26356;&#31526;&#21512;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#30340;&#38169;&#35823;&#32416;&#38169;&#27169;&#22411;&#25552;&#39640;&#20102;&#19981;&#21516;&#35299;&#26512;&#22120;&#30340;&#31934;&#30830;&#21305;&#37197;&#20934;&#30830;&#29575;&#65292;&#25552;&#39640;&#20102;2.4-6.5&#65292;&#24182;&#33719;&#24471;&#20102;&#20004;&#20010;&#24378;&#22522;&#32447;&#30340;&#32477;&#23545;&#25913;&#36827;&#26368;&#22810;4.3&#20010;&#30334;&#20998;&#28857;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#21487;&#22312;https://github.com/OSU-NLP-Group/Auto-SQL-Correction &#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent progress in text-to-SQL parsing, current semantic parsers are still not accurate enough for practical use. In this paper, we investigate how to build automatic text-to-SQL error correction models. Noticing that token-level edits are out of context and sometimes ambiguous, we propose building clause-level edit models instead. Besides, while most language models of code are not specifically pre-trained for SQL, they know common data structures and their operations in programming languages such as Python. Thus, we propose a novel representation for SQL queries and their edits that adheres more closely to the pre-training corpora of language models of code. Our error correction model improves the exact set match accuracy of different parsers by 2.4-6.5 and obtains up to 4.3 point absolute improvement over two strong baselines. Our code and data are available at https://github.com/OSU-NLP-Group/Auto-SQL-Correction.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#38656;&#25163;&#35821;&#26631;&#27880;&#30340;&#20840;&#31471;&#21040;&#31471;&#25163;&#35821;&#32763;&#35793;&#26694;&#26550;(GloFE)&#65292;&#23427;&#36890;&#36807;&#21033;&#29992;&#25163;&#35821;&#21644;&#30456;&#24212;&#21475;&#35821;&#32763;&#35793;&#30340;&#20849;&#21516;&#22522;&#30784;&#35821;&#20041;&#26469;&#25552;&#39640;&#25163;&#35821;&#32763;&#35793;&#30340;&#24615;&#33021;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#23454;&#38469;&#22330;&#26223;&#20013;&#12290;</title><link>http://arxiv.org/abs/2305.12876</link><description>&lt;p&gt;
&#26080;&#39035;&#25163;&#35821;&#26631;&#27880;&#30340;&#20840;&#31471;&#21040;&#31471;&#25163;&#35821;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Gloss-Free End-to-End Sign Language Translation. (arXiv:2305.12876v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#38656;&#25163;&#35821;&#26631;&#27880;&#30340;&#20840;&#31471;&#21040;&#31471;&#25163;&#35821;&#32763;&#35793;&#26694;&#26550;(GloFE)&#65292;&#23427;&#36890;&#36807;&#21033;&#29992;&#25163;&#35821;&#21644;&#30456;&#24212;&#21475;&#35821;&#32763;&#35793;&#30340;&#20849;&#21516;&#22522;&#30784;&#35821;&#20041;&#26469;&#25552;&#39640;&#25163;&#35821;&#32763;&#35793;&#30340;&#24615;&#33021;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#23454;&#38469;&#22330;&#26223;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35299;&#20915;&#20102;&#26080;&#38656;&#25163;&#35821;&#26631;&#27880;&#30340;&#25163;&#35821;&#32763;&#35793;&#38382;&#39064;&#12290;&#23613;&#31649;&#20687;&#25163;&#35821;&#26631;&#35821;&#36825;&#26679;&#30340;&#20013;&#38388;&#34920;&#31034;&#27861;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#25163;&#35821;&#26631;&#27880;&#24456;&#38590;&#33719;&#21462;&#65292;&#29305;&#21035;&#26159;&#22312;&#22823;&#37327;&#33719;&#21462;&#26102;&#12290;&#36825;&#38480;&#21046;&#20102;&#32763;&#35793;&#25968;&#25454;&#38598;&#30340;&#39046;&#22495;&#35206;&#30422;&#33539;&#22260;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#23454;&#38469;&#24212;&#29992;&#30340;&#25928;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#26080;&#38656;&#25163;&#35821;&#26631;&#27880;&#30340;&#20840;&#31471;&#21040;&#31471;&#25163;&#35821;&#32763;&#35793;&#26694;&#26550; (GloFE)&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#25163;&#35821;&#21644;&#30456;&#24212;&#21475;&#35821;&#32763;&#35793;&#30340;&#20849;&#21516;&#22522;&#30784;&#35821;&#20041;&#26469;&#25552;&#39640;&#26080;&#38656;&#25163;&#35821;&#26631;&#27880;&#35774;&#32622;&#19979;&#25163;&#35821;&#32763;&#35793;&#30340;&#24615;&#33021;&#12290;&#36890;&#29992;&#27010;&#24565;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#20986;&#26469;&#24182;&#29992;&#20316;&#19968;&#31181;&#24369;&#24418;&#24335;&#30340;&#20013;&#38388;&#34920;&#31034;&#12290;&#36825;&#20123;&#27010;&#24565;&#30340;&#20840;&#23616;&#23884;&#20837;&#29992;&#20316;&#36328;&#27880;&#24847;&#21147;&#30340;&#26597;&#35810;&#65292;&#20197;&#26597;&#25214;&#23398;&#20064;&#21040;&#30340;&#35270;&#35273;&#29305;&#24449;&#20869;&#30340;&#30456;&#24212;&#20449;&#24687;&#12290;&#25105;&#20204;&#20197;&#23545;&#27604;&#30340;&#26041;&#24335;&#40723;&#21169;&#21253;&#21547;&#36825;&#20123;&#27010;&#24565;&#30340;&#26679;&#26412;&#20043;&#38388;&#26597;&#35810;&#32467;&#26524;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#20943;&#23569;&#27809;&#26377;&#36825;&#20123;&#27010;&#24565;&#30340;&#26679;&#26412;&#20043;&#38388;&#30340;&#26597;&#35810;&#32467;&#26524;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#22823;&#35268;&#27169;&#26080;&#38656;&#25163;&#35821;&#26631;&#27880;&#30340;&#25163;&#35821;&#32763;&#35793;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;GloFE&#30456;&#27604;&#20110;&#20043;&#21069;&#20165;&#20381;&#38752;&#25163;&#35821;&#26631;&#27880;&#20449;&#24687;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we tackle the problem of sign language translation (SLT) without gloss annotations. Although intermediate representation like gloss has been proven effective, gloss annotations are hard to acquire, especially in large quantities. This limits the domain coverage of translation datasets, thus handicapping real-world applications. To mitigate this problem, we design the Gloss-Free End-to-end sign language translation framework (GloFE). Our method improves the performance of SLT in the gloss-free setting by exploiting the shared underlying semantics of signs and the corresponding spoken translation. Common concepts are extracted from the text and used as a weak form of intermediate representation. The global embedding of these concepts is used as a query for cross-attention to find the corresponding information within the learned visual features. In a contrastive manner, we encourage the similarity of query results between samples containing such concepts and decrease those 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#20108;&#20998;&#25628;&#32034;&#23398;&#20064;&#21516;&#26102;&#26426;&#22120;&#32763;&#35793;&#26368;&#20248;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#32763;&#35793;&#20219;&#21153;&#19978;&#39564;&#35777;&#20102;&#22312;&#25152;&#26377;&#24310;&#36831;&#24773;&#26223;&#19979;&#36229;&#36234;&#24378;&#22522;&#32447;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.12774</link><description>&lt;p&gt;
&#36890;&#36807;&#20108;&#20998;&#25628;&#32034;&#23398;&#20064;&#21516;&#26102;&#26426;&#22120;&#32763;&#35793;&#30340;&#26368;&#20248;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Learning Optimal Policy for Simultaneous Machine Translation via Binary Search. (arXiv:2305.12774v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#20108;&#20998;&#25628;&#32034;&#23398;&#20064;&#21516;&#26102;&#26426;&#22120;&#32763;&#35793;&#26368;&#20248;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#32763;&#35793;&#20219;&#21153;&#19978;&#39564;&#35777;&#20102;&#22312;&#25152;&#26377;&#24310;&#36831;&#24773;&#26223;&#19979;&#36229;&#36234;&#24378;&#22522;&#32447;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#26102;&#26426;&#22120;&#32763;&#35793;&#65288;SiMT&#65289;&#22312;&#38405;&#35835;&#28304;&#21477;&#23376;&#26102;&#24320;&#22987;&#36755;&#20986;&#32763;&#35793;&#65292;&#24182;&#38656;&#35201;&#31934;&#30830;&#30340;&#31574;&#30053;&#26469;&#20915;&#23450;&#20309;&#26102;&#36755;&#20986;&#29983;&#25104;&#30340;&#32763;&#35793;&#12290;&#22240;&#27492;&#65292;&#35813;&#31574;&#30053;&#20915;&#23450;&#20102;&#22312;&#32763;&#35793;&#27599;&#20010;&#30446;&#26631;&#20196;&#29260;&#26399;&#38388;&#35835;&#21462;&#30340;&#28304;&#26631;&#35760;&#25968;&#37327;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#31934;&#30830;&#30340;&#32763;&#35793;&#31574;&#30053;&#20197;&#23454;&#29616;&#33391;&#22909;&#30340;&#24310;&#36831;&#36136;&#37327;&#26435;&#34913;&#26159;&#22256;&#38590;&#30340;&#65292;&#22240;&#20026;&#27809;&#26377;&#19982;&#24182;&#34892;&#21477;&#23376;&#23545;&#24212;&#30340;&#40644;&#37329;&#31574;&#30053;&#20316;&#20026;&#26174;&#24335;&#30417;&#30563;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20108;&#20998;&#25628;&#32034;&#22312;&#32447;&#26500;&#24314;&#26368;&#20248;&#31574;&#30053;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#37319;&#29992;&#26174;&#24335;&#30417;&#30563;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;SiMT&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#65292;&#36825;&#21487;&#20197;&#25351;&#23548;&#27169;&#22411;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#23436;&#25104;&#32763;&#35793;&#12290;&#22312;&#22235;&#20010;&#32763;&#35793;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#25152;&#26377;&#24310;&#36831;&#26041;&#26696;&#19979;&#36229;&#36234;&#24378;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simultaneous machine translation (SiMT) starts to output translation while reading the source sentence and needs a precise policy to decide when to output the generated translation. Therefore, the policy determines the number of source tokens read during the translation of each target token. However, it is difficult to learn a precise translation policy to achieve good latency-quality trade-offs, because there is no golden policy corresponding to parallel sentences as explicit supervision. In this paper, we present a new method for constructing the optimal policy online via binary search. By employing explicit supervision, our approach enables the SiMT model to learn the optimal policy, which can guide the model in completing the translation during inference. Experiments on four translation tasks show that our method can exceed strong baselines across all latency scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#33021;&#37327;&#20989;&#25968;&#26550;&#26500;&#21644;&#19981;&#21516;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#22522;&#20110;&#33021;&#37327;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#35821;&#38899;&#35782;&#21035;&#20013;&#35745;&#31639;&#21477;&#23376;&#24471;&#20998;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.12676</link><description>&lt;p&gt;
&#25506;&#32034;&#19981;&#21516;&#26550;&#26500;&#21644;&#35757;&#32451;&#26041;&#27861;&#19979;&#22522;&#20110;&#33021;&#37327;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Exploring Energy-based Language Models with Different Architectures and Training Methods for Speech Recognition. (arXiv:2305.12676v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12676
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#33021;&#37327;&#20989;&#25968;&#26550;&#26500;&#21644;&#19981;&#21516;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#22522;&#20110;&#33021;&#37327;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#35821;&#38899;&#35782;&#21035;&#20013;&#35745;&#31639;&#21477;&#23376;&#24471;&#20998;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;ELM&#65289;&#36890;&#36807;&#21442;&#25968;&#21270;&#33258;&#28982;&#35821;&#21477;&#30340;&#38750;&#24402;&#19968;&#21270;&#20998;&#24067;&#19982;&#27969;&#34892;&#30340;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#65288;ALM&#65289;&#26377;&#26681;&#26412;&#24615;&#21306;&#21035;&#12290;&#20316;&#20026;&#19968;&#31181;&#37325;&#35201;&#30340;&#24212;&#29992;&#65292;ELM&#24050;&#25104;&#21151;&#22320;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#20013;&#35745;&#31639;&#21477;&#23376;&#24471;&#20998;&#65292;&#20294;&#23427;&#20204;&#37117;&#20351;&#29992;&#19981;&#22826;&#29616;&#20195;&#30340;CNN&#25110;LSTM&#32593;&#32476;&#12290;&#38543;&#30528;Transformer&#32593;&#32476;&#21644;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;&#22914;BERT&#21644;GPT2&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;ELMs&#30340;&#33021;&#21147;&#24050;&#32463;&#25104;&#20026;&#21487;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#33021;&#37327;&#20989;&#25968;&#26550;&#26500;&#21644;&#19981;&#21516;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#30740;&#31350;&#22312;&#20197;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#20316;&#20026;&#39592;&#24178;&#30340;&#35821;&#38899;&#35782;&#21035;&#20013;&#65292;ELMs&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy-based language models (ELMs) parameterize an unnormalized distribution for natural sentences and are radically different from popular autoregressive language models (ALMs). As an important application, ELMs have been successfully used as a means for calculating sentence scores in speech recognition, but they all use less-modern CNN or LSTM networks. The recent progress in Transformer networks and large pretrained models such as BERT and GPT2 opens new possibility to further advancing ELMs. In this paper, we explore different architectures of energy functions and different training methods to investigate the capabilities of ELMs in rescoring for speech recognition, all using large pretrained models as backbones.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#30701;&#35821;&#39044;&#27979;&#32593;&#32476;&#29992;&#20110;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#20559;&#32622;&#26041;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#20559;&#32622;&#25439;&#22833;&#20197;&#24110;&#21161;&#35757;&#32451;&#19978;&#19979;&#25991;&#21270;&#27169;&#22411;&#65292;&#22312;&#22810;&#31181;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;WER&#38477;&#20302;&#65292;&#30456;&#23545;&#20110;&#22522;&#32447;&#27169;&#22411;&#30456;&#23545;WER&#25552;&#39640;&#20102;12.1&#65285;&#65292;&#19978;&#19979;&#25991;&#30701;&#35821;&#30340;WER&#30456;&#23545;&#38477;&#20302;&#20102;40.5&#65285;&#12290;</title><link>http://arxiv.org/abs/2305.12493</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#21270;&#30340;&#30701;&#35821;&#39044;&#27979;&#32593;&#32476;&#22312;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Contextualized End-to-End Speech Recognition with Contextual Phrase Prediction Network. (arXiv:2305.12493v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12493
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#30701;&#35821;&#39044;&#27979;&#32593;&#32476;&#29992;&#20110;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#20559;&#32622;&#26041;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#20559;&#32622;&#25439;&#22833;&#20197;&#24110;&#21161;&#35757;&#32451;&#19978;&#19979;&#25991;&#21270;&#27169;&#22411;&#65292;&#22312;&#22810;&#31181;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;WER&#38477;&#20302;&#65292;&#30456;&#23545;&#20110;&#22522;&#32447;&#27169;&#22411;&#30456;&#23545;WER&#25552;&#39640;&#20102;12.1&#65285;&#65292;&#19978;&#19979;&#25991;&#30701;&#35821;&#30340;WER&#30456;&#23545;&#38477;&#20302;&#20102;40.5&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#20449;&#24687;&#22312;&#35821;&#38899;&#35782;&#21035;&#25216;&#26415;&#20013;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#23558;&#20854;&#34701;&#20837;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#28145;&#24230;&#20559;&#32622;&#26041;&#27861;&#32570;&#20047;&#20559;&#32622;&#20219;&#21153;&#30340;&#26174;&#24335;&#30417;&#30563;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#30701;&#35821;&#39044;&#27979;&#32593;&#32476;&#29992;&#20110;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#20559;&#32622;&#26041;&#27861;&#12290;&#35813;&#32593;&#32476;&#21033;&#29992;&#19978;&#19979;&#25991;&#23884;&#20837;&#39044;&#27979;&#21457;&#38899;&#20013;&#30340;&#19978;&#19979;&#25991;&#30701;&#35821;&#65292;&#24182;&#35745;&#31639;&#20559;&#32622;&#25439;&#22833;&#20197;&#24110;&#21161;&#35757;&#32451;&#19978;&#19979;&#25991;&#21270;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#31181;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#21333;&#35789;&#38169;&#35823;&#29575;(WER)&#38477;&#20302;&#12290;&#23545;LibriSpeech&#35821;&#26009;&#24211;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22522;&#32447;&#27169;&#22411;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#30456;&#23545;WER&#25552;&#39640;&#20102;12.1&#65285;&#65292;&#19978;&#19979;&#25991;&#30701;&#35821;&#30340;WER&#30456;&#23545;&#38477;&#20302;&#20102;40.5&#65285;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#24212;&#29992;&#19978;&#19979;&#25991;&#30701;&#35821;&#36807;&#28388;&#31574;&#30053;&#65292;&#25105;&#20204;&#36824;&#26377;&#25928;&#28040;&#38500;&#20102;&#20351;&#29992;&#26356;&#22823;&#30340;&#20559;&#32622;&#21015;&#34920;&#26102;&#30340;WER&#38477;&#32423;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contextual information plays a crucial role in speech recognition technologies and incorporating it into the end-to-end speech recognition models has drawn immense interest recently. However, previous deep bias methods lacked explicit supervision for bias tasks. In this study, we introduce a contextual phrase prediction network for an attention-based deep bias method. This network predicts context phrases in utterances using contextual embeddings and calculates bias loss to assist in the training of the contextualized model. Our method achieved a significant word error rate (WER) reduction across various end-to-end speech recognition models. Experiments on the LibriSpeech corpus show that our proposed model obtains a 12.1% relative WER improvement over the baseline model, and the WER of the context phrases decreases relatively by 40.5%. Moreover, by applying a context phrase filtering strategy, we also effectively eliminate the WER degradation when using a larger biasing list.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;AMPLIFY&#65292;&#21033;&#29992;&#21518;&#39564;&#35299;&#37322;&#33258;&#21160;&#21270;&#29983;&#25104;&#21407;&#22240;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#19978;&#26174;&#33879;&#25552;&#39640;&#29616;&#26377;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.11426</link><description>&lt;p&gt;
&#21518;&#39564;&#35299;&#37322;&#21487;&#20197;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Post Hoc Explanations of Language Models Can Improve Language Models. (arXiv:2305.11426v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;AMPLIFY&#65292;&#21033;&#29992;&#21518;&#39564;&#35299;&#37322;&#33258;&#21160;&#21270;&#29983;&#25104;&#21407;&#22240;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#19978;&#26174;&#33879;&#25552;&#39640;&#29616;&#26377;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#36807;&#31243;&#20013;&#21152;&#20837;&#20154;&#31867;&#27880;&#37322;&#30340;&#21407;&#29702;&#65288;&#20363;&#22914;&#65292;&#24605;&#32500;&#38142;&#25552;&#31034;&#65289;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#35201;&#25512;&#29702;&#33021;&#21147;&#30340;&#20219;&#21153;&#19978;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#21407;&#29702;&#21152;&#20837;&#22312;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#65292;&#22240;&#20026;&#36825;&#38656;&#35201;&#39640;&#24230;&#30340;&#20154;&#24037;&#21442;&#19982;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#65292;&#21363;&#36890;&#36807;&#21033;&#29992;&#21518;&#39564;&#35299;&#37322;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#25918;&#22823;&#27169;&#22411;&#24615;&#33021;&#65292;&#26469;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#21518;&#39564;&#35299;&#37322;&#26041;&#27861;&#30340;&#32467;&#26524;&#65292;&#35813;&#26041;&#27861;&#36755;&#20986;&#31216;&#20026;&#23646;&#24615;&#20998;&#25968;&#65288;&#35299;&#37322;&#65289;&#30340;&#20540;&#65292;&#29992;&#20110;&#25429;&#33719;&#27599;&#20010;&#36755;&#20837;&#29305;&#24449;&#23545;&#27169;&#22411;&#39044;&#27979;&#30340;&#24433;&#21709;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#33258;&#21160;&#21270;&#30340;&#33258;&#28982;&#35821;&#35328;&#21407;&#29702;&#65292;&#20854;&#20013;&#21253;&#21547;&#20174;&#23646;&#24615;&#20998;&#25968;&#20013;&#33719;&#24471;&#30340;&#20449;&#24687;&#65292;&#20197;&#20415;&#29992;&#25143;&#21487;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#27169;&#22411;&#30340;&#20915;&#31574;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;AMPLIFY&#21487;&#20197;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#19978;&#26174;&#33879;&#25552;&#39640;&#29616;&#26377;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable capabilities in performing complex tasks. Moreover, recent research has shown that incorporating human-annotated rationales (e.g., Chain-of- Thought prompting) during in-context learning can significantly enhance the performance of these models, particularly on tasks that require reasoning capabilities. However, incorporating such rationales poses challenges in terms of scalability as this requires a high degree of human involvement. In this work, we present a novel framework, Amplifying Model Performance by Leveraging In-Context Learning with Post Hoc Explanations (AMPLIFY), which addresses the aforementioned challenges by automating the process of rationale generation. To this end, we leverage post hoc explanation methods which output attribution scores (explanations) capturing the influence of each of the input features on model predictions. More specifically, we construct automated natural language rationales that embed insi
&lt;/p&gt;</description></item><item><title>ConvXAI&#26159;&#19968;&#20010;&#22522;&#20110;&#23545;&#35805;&#30340;XAI&#31995;&#32479;&#65292;&#23427;&#38598;&#25104;&#20102;&#22810;&#31181;XAI&#31867;&#22411;&#65292;&#24182;&#23558;&#23454;&#38469;&#29992;&#25143;&#38656;&#27714;&#23884;&#20837;&#35774;&#35745;&#20013;&#65292;&#20197;&#25552;&#39640;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.09770</link><description>&lt;p&gt;
ConvXAI&#65306;&#36890;&#36807;&#23545;&#35805;&#25552;&#20379;&#24322;&#26500;&#30340;AI&#35299;&#37322;&#65292;&#25903;&#25345;&#20154;&#26426;&#31185;&#25216;&#20889;&#20316;
&lt;/p&gt;
&lt;p&gt;
ConvXAI: Delivering Heterogeneous AI Explanations via Conversations to Support Human-AI Scientific Writing. (arXiv:2305.09770v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09770
&lt;/p&gt;
&lt;p&gt;
ConvXAI&#26159;&#19968;&#20010;&#22522;&#20110;&#23545;&#35805;&#30340;XAI&#31995;&#32479;&#65292;&#23427;&#38598;&#25104;&#20102;&#22810;&#31181;XAI&#31867;&#22411;&#65292;&#24182;&#23558;&#23454;&#38469;&#29992;&#25143;&#38656;&#27714;&#23884;&#20837;&#35774;&#35745;&#20013;&#65292;&#20197;&#25552;&#39640;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#20154;&#24037;&#26234;&#33021;&#35299;&#37322;&#65288;XAI&#65289;&#26041;&#27861;&#26469;&#35299;&#37322;AI&#31995;&#32479;&#65292;&#20294;&#30446;&#21069;&#30340;&#26041;&#27861;&#26159;&#21542;&#23545;&#20154;&#31867;&#23454;&#29992;&#20173;&#23384;&#22312;&#19981;&#19968;&#33268;&#30340;&#21457;&#29616;&#12290;&#20026;&#20102;&#25913;&#21892;XAI&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#65292;&#19968;&#31995;&#21015;&#30740;&#31350;&#30830;&#23450;&#20102;&#29616;&#23454;&#19990;&#30028;&#20013;&#22810;&#26679;&#21270;&#21644;&#21160;&#24577;&#30340;&#29992;&#25143;&#38656;&#27714;&#19982;&#29616;&#26377;XAI&#26041;&#27861;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#34429;&#28982;&#20043;&#21069;&#30340;&#30740;&#31350;&#35774;&#24819;&#23558;&#22810;&#31181;XAI&#26041;&#27861;&#38598;&#25104;&#21040;&#36890;&#29992;XAI&#30028;&#38754;&#65288;&#20363;&#22914;&#65292;&#22522;&#20110;&#23545;&#35805;&#25110;GUI&#30340;XAI&#31995;&#32479;&#65289;&#20013;&#20197;&#20943;&#36731;&#36825;&#20123;&#24046;&#36317;&#65292;&#20294;&#32570;&#23569;&#38024;&#23545;&#36825;&#20123;&#31995;&#32479;&#22914;&#20309;&#35774;&#35745;&#20197;&#28385;&#36275;&#23454;&#38469;&#29992;&#25143;&#38656;&#27714;&#30340;&#30740;&#31350;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ConvXAI&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#23545;&#35805;&#30340;XAI&#31995;&#32479;&#65292;&#23427;&#32467;&#21512;&#20102;&#22810;&#31181;XAI&#31867;&#22411;&#65292;&#24182;&#36171;&#20104;&#29992;&#25143;&#36890;&#36807;&#36890;&#29992;&#30340;XAI&#23545;&#35805;&#30028;&#38754;&#25552;&#20986;&#21508;&#31181;XAI&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#21019;&#26032;&#22320;&#23558;&#23454;&#38469;&#29992;&#25143;&#38656;&#27714;&#65288;&#21363;&#65292;&#22522;&#20110;&#26684;&#24335;&#30740;&#31350;&#30340;&#22235;&#20010;&#21407;&#21017;&#65289;&#23884;&#20837;ConvXAI&#35774;&#35745;&#20013;&#65292;&#20197;&#25552;&#39640;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While various AI explanation (XAI) methods have been proposed to interpret AI systems, whether the state-of-the-art XAI methods are practically useful for humans remains inconsistent findings. To improve the usefulness of XAI methods, a line of studies identifies the gaps between the diverse and dynamic real-world user needs with the status quo of XAI methods. Although prior studies envision mitigating these gaps by integrating multiple XAI methods into the universal XAI interfaces (e.g., conversational or GUI-based XAI systems), there is a lack of work investigating how these systems should be designed to meet practical user needs. In this study, we present ConvXAI, a conversational XAI system that incorporates multiple XAI types, and empowers users to request a variety of XAI questions via a universal XAI dialogue interface. Particularly, we innovatively embed practical user needs (i.e., four principles grounding on the formative study) into ConvXAI design to improve practical useful
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#19968;&#20010;&#26032;&#26041;&#27861;&#65292;&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;1335&#31181;&#35821;&#35328;&#20043;&#38388;&#22312;&#27010;&#24565;&#34920;&#36798;&#26041;&#38754;&#30340;&#24046;&#24322;&#65292;&#24182;&#21457;&#29616;&#20102;&#33021;&#22815;&#39044;&#27979;&#19968;&#31181;&#27010;&#24565;&#26159;&#21542;&#20250;&#36328;&#36234;&#22810;&#31181;&#35821;&#35328;&#65292;&#20197;&#21450;&#22914;&#20309;&#20195;&#34920;&#27599;&#31181;&#35821;&#35328;&#30340;&#22810;&#32500;&#20998;&#31867;&#26041;&#27861;&#65292;&#36825;&#26377;&#21161;&#20110;&#35299;&#20915;&#29616;&#26377;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2305.08475</link><description>&lt;p&gt;
1335&#31181;&#35821;&#35328;&#27010;&#24565;&#34920;&#36798;&#30340;&#36328;&#35821;&#35328;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Crosslingual Investigation of Conceptualization in 1335 Languages. (arXiv:2305.08475v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08475
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19968;&#20010;&#26032;&#26041;&#27861;&#65292;&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;1335&#31181;&#35821;&#35328;&#20043;&#38388;&#22312;&#27010;&#24565;&#34920;&#36798;&#26041;&#38754;&#30340;&#24046;&#24322;&#65292;&#24182;&#21457;&#29616;&#20102;&#33021;&#22815;&#39044;&#27979;&#19968;&#31181;&#27010;&#24565;&#26159;&#21542;&#20250;&#36328;&#36234;&#22810;&#31181;&#35821;&#35328;&#65292;&#20197;&#21450;&#22914;&#20309;&#20195;&#34920;&#27599;&#31181;&#35821;&#35328;&#30340;&#22810;&#32500;&#20998;&#31867;&#26041;&#27861;&#65292;&#36825;&#26377;&#21161;&#20110;&#35299;&#20915;&#29616;&#26377;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#22312;&#22914;&#20309;&#23558;&#19990;&#30028;&#20998;&#20026;&#27010;&#24565;&#21644;&#35789;&#27719;&#26041;&#38754;&#23384;&#22312;&#24046;&#24322;&#65307;&#20363;&#22914;&#65292;&#19982;&#33521;&#35821;&#19981;&#21516;&#65292;&#26031;&#29926;&#24076;&#37324;&#35821;&#26377;&#19968;&#20010;&#21333;&#19968;&#30340;&#27010;&#24565;&#26469;&#34920;&#31034;&#8220;&#32922;&#23376;&#8221;&#21644;&#8220;&#23376;&#23467;&#8221;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#24179;&#34892;&#35821;&#26009;&#24211;&#20013;&#23545;&#40784;&#27010;&#24565;&#65292;&#35843;&#26597;1,335&#31181;&#35821;&#35328;&#20043;&#38388;&#30340;&#36825;&#20123;&#24046;&#24322;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Conceptualizer&#26041;&#27861;&#65292;&#23427;&#21019;&#24314;&#20102;&#19968;&#20010;&#28304;&#35821;&#35328;&#27010;&#24565;&#21644;&#19968;&#32452;&#30446;&#26631;&#35821;&#35328;&#23383;&#31526;&#20018;&#20043;&#38388;&#30340;&#20108;&#20998;&#26377;&#21521;&#23545;&#40784;&#22270;&#12290;&#36890;&#36807;&#23545;&#19968;&#20010;&#27010;&#24565;&#65288;&#8220;&#40479;&#8221;&#65289;&#36827;&#34892;&#25152;&#26377;&#35821;&#35328;&#30340;&#35814;&#32454;&#35821;&#35328;&#20998;&#26512;&#65292;&#24182;&#22312;32&#20010;Swadesh&#27010;&#24565;&#30340;&#40644;&#37329;&#26631;&#20934;&#25968;&#25454;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;Conceptualizer&#20855;&#26377;&#33391;&#22909;&#30340;&#23545;&#40784;&#31934;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#23454;&#39564;&#23637;&#31034;&#20102;NLP&#20013;&#27010;&#24565;&#34920;&#36798;&#30740;&#31350;&#30340;&#28508;&#21147;&#12290;&#31532;&#19968;&#20010;&#23454;&#39564;&#25105;&#20204;&#23450;&#20041;&#20102;&#27178;&#36328;&#22810;&#31181;&#35821;&#35328;&#30340;&#31283;&#23450;&#24615;&#27010;&#24565;&#65292;&#24182;&#35777;&#26126;&#20102;&#20855;&#35937;&#31243;&#24230;&#21487;&#20197;&#39044;&#27979;&#20854;&#31283;&#23450;&#24615;&#12290;&#31532;&#20108;&#20010;&#23454;&#39564;&#25105;&#20204;&#36890;&#36807;83&#20010;&#27010;&#24565;&#30340;&#27010;&#24565;&#34920;&#36798;&#27169;&#24335;&#65292;&#26469;&#20195;&#34920;&#27599;&#31181;&#35821;&#35328;&#65292;&#24182;&#23450;&#20041;&#20102;&#19968;&#31181;&#22810;&#32500;&#35821;&#35328;&#20998;&#31867;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#24110;&#21161;&#22635;&#34917;&#29616;&#26377;NLP&#27169;&#22411;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;
Languages differ in how they divide up the world into concepts and words; e.g., in contrast to English, Swahili has a single concept for `belly' and `womb'. We investigate these differences in conceptualization across 1,335 languages by aligning concepts in a parallel corpus. To this end, we propose Conceptualizer, a method that creates a bipartite directed alignment graph between source language concepts and sets of target language strings. In a detailed linguistic analysis across all languages for one concept (`bird') and an evaluation on gold standard data for 32 Swadesh concepts, we show that Conceptualizer has good alignment accuracy. We demonstrate the potential of research on conceptualization in NLP with two experiments. (1) We define crosslingual stability of a concept as the degree to which it has 1-1 correspondences across languages, and show that concreteness predicts stability. (2) We represent each language by its conceptualization pattern for 83 concepts, and define a si
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#27979;&#37327;&#20102;&#25919;&#27835;&#20559;&#35265;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#39044;&#35757;&#32451;&#27169;&#22411;&#23384;&#22312;&#25919;&#27835;&#20542;&#21521;&#65292;&#24182;&#23558;&#31038;&#20250;&#20559;&#35265;&#20256;&#36882;&#21040;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#20174;&#32780;&#23548;&#33268;NLP&#27169;&#22411;&#30340;&#19981;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.08283</link><description>&lt;p&gt;
&#20174;&#39044;&#35757;&#32451;&#25968;&#25454;&#21040;&#35821;&#35328;&#27169;&#22411;&#20877;&#21040;&#19979;&#28216;&#20219;&#21153;&#65306;&#36861;&#36394;&#23548;&#33268;&#19981;&#20844;&#24179;NLP&#27169;&#22411;&#30340;&#25919;&#27835;&#20559;&#35265;&#30340;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models. (arXiv:2305.08283v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08283
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#27979;&#37327;&#20102;&#25919;&#27835;&#20559;&#35265;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#39044;&#35757;&#32451;&#27169;&#22411;&#23384;&#22312;&#25919;&#27835;&#20542;&#21521;&#65292;&#24182;&#23558;&#31038;&#20250;&#20559;&#35265;&#20256;&#36882;&#21040;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#20174;&#32780;&#23548;&#33268;NLP&#27169;&#22411;&#30340;&#19981;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;(LMs)&#26159;&#39044;&#35757;&#32451;&#22312;&#19981;&#21516;&#25968;&#25454;&#28304;&#19978;&#30340;&#65292;&#21253;&#25324;&#26032;&#38395;&#12289;&#35752;&#35770;&#35770;&#22363;&#12289;&#20070;&#31821;&#21644;&#22312;&#32447;&#30334;&#31185;&#20840;&#20070;&#31561;&#12290;&#36825;&#20123;&#25968;&#25454;&#20013;&#30340;&#30456;&#24403;&#19968;&#37096;&#20998;&#21253;&#25324;&#35266;&#28857;&#21644;&#35282;&#24230;&#65292;&#19968;&#26041;&#38754;&#36190;&#25196;&#27665;&#20027;&#21644;&#24605;&#24819;&#22810;&#26679;&#24615;&#65292;&#21478;&#19968;&#26041;&#38754;&#20855;&#26377;&#22266;&#26377;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24320;&#21457;&#20102;&#26032;&#30340;&#26041;&#27861;&#26469;(1)&#27979;&#37327;&#22522;&#20110;&#27492;&#31867;&#35821;&#26009;&#24211;&#35757;&#32451;&#30340;LMs&#20013;&#30340;&#25919;&#27835;&#20559;&#35265;&#65292;&#27839;&#31038;&#20250;&#21644;&#32463;&#27982;&#36724;&#65292;&#20197;&#21450;(2)&#34913;&#37327;&#22522;&#20110;&#25919;&#27835;&#20559;&#35265;&#30340;LMs&#35757;&#32451;&#30340;&#19979;&#28216;NLP&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#20851;&#27880;&#20167;&#24680;&#35328;&#35770;&#21644;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#65292;&#26088;&#22312;&#23454;&#35777;&#37327;&#21270;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#25919;&#27835;(&#31038;&#20250;&#12289;&#32463;&#27982;)&#20559;&#35265;&#23545;&#39640;&#39118;&#38505;&#31038;&#20250;&#23548;&#21521;&#20219;&#21153;&#20844;&#27491;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;LMs&#30830;&#23454;&#23384;&#22312;&#25919;&#27835;&#20542;&#21521;&#65292;&#21152;&#24378;&#20102;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#23384;&#22312;&#30340;&#26497;&#21270;&#65292;&#23558;&#31038;&#20250;&#20559;&#35265;&#20256;&#25773;&#21040;&#20167;&#24680;&#35328;&#35770;&#39044;&#27979;&#21644;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#22120;&#20013;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#30740;&#31350;&#30340;&#24847;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;&#24320;&#21457;&#26356;&#20844;&#27491;&#12289;&#26356;&#26080;&#20559;NLP&#27169;&#22411;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) are pretrained on diverse data sources, including news, discussion forums, books, and online encyclopedias. A significant portion of this data includes opinions and perspectives which, on one hand, celebrate democracy and diversity of ideas, and on the other hand are inherently socially biased. Our work develops new methods to (1) measure political biases in LMs trained on such corpora, along social and economic axes, and (2) measure the fairness of downstream NLP models trained on top of politically biased LMs. We focus on hate speech and misinformation detection, aiming to empirically quantify the effects of political (social, economic) biases in pretraining data on the fairness of high-stakes social-oriented tasks. Our findings reveal that pretrained LMs do have political leanings that reinforce the polarization present in pretraining corpora, propagating social biases into hate speech predictions and misinformation detectors. We discuss the implications of our
&lt;/p&gt;</description></item><item><title>&#38646;&#26679;&#26412;&#30340;&#24544;&#23454;&#20107;&#23454;&#24615;&#38169;&#35823;&#32416;&#27491;&#26694;&#26550;&#36229;&#36234;&#23436;&#20840;&#30417;&#30563;&#26041;&#27861;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#21450;&#35299;&#37322;&#24615;&#21644;&#24544;&#23454;&#24615;&#35780;&#20272;&#26631;&#20934;&#65292;&#36866;&#29992;&#20110;&#32500;&#25252;&#25991;&#26412;&#30693;&#35782;&#24211;&#21644;&#39044;&#38450;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#12290;</title><link>http://arxiv.org/abs/2305.07982</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#20449;&#23454;&#20107;&#23454;&#32416;&#38169;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Faithful Factual Error Correction. (arXiv:2305.07982v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07982
&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#30340;&#24544;&#23454;&#20107;&#23454;&#24615;&#38169;&#35823;&#32416;&#27491;&#26694;&#26550;&#36229;&#36234;&#23436;&#20840;&#30417;&#30563;&#26041;&#27861;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#21450;&#35299;&#37322;&#24615;&#21644;&#24544;&#23454;&#24615;&#35780;&#20272;&#26631;&#20934;&#65292;&#36866;&#29992;&#20110;&#32500;&#25252;&#25991;&#26412;&#30693;&#35782;&#24211;&#21644;&#39044;&#38450;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24544;&#23454;&#22320;&#32416;&#27491;&#20107;&#23454;&#24615;&#38169;&#35823;&#23545;&#20110;&#32500;&#25252;&#25991;&#26412;&#30693;&#35782;&#24211;&#30340;&#23436;&#25972;&#24615;&#21644;&#38450;&#27490;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#33267;&#20851;&#37325;&#35201;&#12290;&#20511;&#37492;&#20154;&#31867;&#35782;&#21035;&#21644;&#32416;&#27491;&#20107;&#23454;&#38169;&#35823;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38646;&#26679;&#26412;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21046;&#23450;&#26377;&#20851;&#36755;&#20837;&#22768;&#26126;&#30340;&#38382;&#39064;&#65292;&#26597;&#25214;&#32473;&#23450;&#35777;&#25454;&#20013;&#30340;&#27491;&#30830;&#31572;&#26696;&#65292;&#24182;&#26681;&#25454;&#20854;&#19982;&#35777;&#25454;&#30340;&#19968;&#33268;&#24615;&#35780;&#20272;&#27599;&#20010;&#32416;&#27491;&#30340;&#20449;&#23454;&#24615;&#12290;&#25105;&#20204;&#30340;&#38646;&#26679;&#26412;&#26694;&#26550;&#22312;FEVER&#21644;SciFact&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#20013;&#27604;&#23436;&#20840;&#30417;&#30563;&#30340;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#36755;&#20986;&#26356;&#21152;&#24544;&#23454;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#26694;&#26550;&#30340;&#21487;&#20998;&#35299;&#24615;&#22825;&#28982;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#25581;&#31034;&#35780;&#20272;&#20107;&#23454;&#38169;&#35823;&#20462;&#27491;&#30340;&#26368;&#21512;&#36866;&#24230;&#37327;&#26631;&#20934;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#24120;&#29992;&#24230;&#37327;&#26631;&#20934;&#19982;&#19977;&#20010;&#19981;&#21516;&#32500;&#24230;&#30340;&#20154;&#31867;&#21028;&#26029;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#21253;&#25324;&#21487;&#29702;&#35299;&#24615;&#21644;&#24544;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Faithfully correcting factual errors is critical for maintaining the integrity of textual knowledge bases and preventing hallucinations in sequence-to-sequence models. Drawing on humans' ability to identify and correct factual errors, we present a zero-shot framework that formulates questions about input claims, looks for correct answers in the given evidence, and assesses the faithfulness of each correction based on its consistency with the evidence. Our zero-shot framework outperforms fully-supervised approaches, as demonstrated by experiments on the FEVER and SciFact datasets, where our outputs are shown to be more faithful. More importantly, the decomposability nature of our framework inherently provides interpretability. Additionally, to reveal the most suitable metrics for evaluating factual error corrections, we analyze the correlation between commonly used metrics with human judgments in terms of three different dimensions regarding intelligibility and faithfulness.
&lt;/p&gt;</description></item><item><title>Decker&#26159;&#19968;&#31181;&#33021;&#22815;&#21033;&#29992;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#30693;&#35782;&#20043;&#38388;&#30340;&#28508;&#22312;&#20851;&#31995;&#26469;&#26725;&#25509;&#24322;&#26500;&#30693;&#35782;&#30340;&#24120;&#35782;&#20107;&#23454;&#39564;&#35777;&#27169;&#22411;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#39564;&#35777;&#25928;&#26524;&#21644;&#33719;&#21462;&#29645;&#36149;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.05921</link><description>&lt;p&gt;
Decker: &#21452;&#37325;&#26816;&#26597;&#19982;&#24322;&#26500;&#30693;&#35782;&#29992;&#20110;&#24120;&#35782;&#20107;&#23454;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Decker: Double Check with Heterogeneous Knowledge for Commonsense Fact Verification. (arXiv:2305.05921v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05921
&lt;/p&gt;
&lt;p&gt;
Decker&#26159;&#19968;&#31181;&#33021;&#22815;&#21033;&#29992;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#30693;&#35782;&#20043;&#38388;&#30340;&#28508;&#22312;&#20851;&#31995;&#26469;&#26725;&#25509;&#24322;&#26500;&#30693;&#35782;&#30340;&#24120;&#35782;&#20107;&#23454;&#39564;&#35777;&#27169;&#22411;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#39564;&#35777;&#25928;&#26524;&#21644;&#33719;&#21462;&#29645;&#36149;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24120;&#35782;&#20107;&#23454;&#39564;&#35777;&#20316;&#20026;&#24120;&#35782;&#38382;&#31572;&#30340;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20998;&#25903;&#65292;&#26088;&#22312;&#36890;&#36807;&#20107;&#23454;&#26469;&#39564;&#35777;&#19968;&#20010;&#32473;&#23450;&#30340;&#24120;&#35782;&#35770;&#26029;&#26159;&#21542;&#27491;&#30830;&#12290;&#22238;&#31572;&#24120;&#35782;&#38382;&#39064;&#38656;&#35201;&#20174;&#19981;&#21516;&#23618;&#27425;&#30340;&#30693;&#35782;&#20013;&#36827;&#34892;&#32452;&#21512;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#20027;&#35201;&#20381;&#38752;&#25226;&#25569;&#38750;&#32467;&#26500;&#21270;&#35777;&#25454;&#25110;&#20174;&#32467;&#26500;&#21270;&#30693;&#35782;&#24211;&#20013;&#25214;&#21040;&#28508;&#22312;&#30340;&#25512;&#29702;&#36335;&#24452;&#65292;&#20294;&#27809;&#26377;&#21516;&#26102;&#21033;&#29992;&#24322;&#26500;&#30693;&#35782;&#30340;&#22909;&#22788;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Decker&#65292;&#19968;&#31181;&#24120;&#35782;&#20107;&#23454;&#39564;&#35777;&#27169;&#22411;&#65292;&#33021;&#22815;&#36890;&#36807;&#21457;&#29616;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#30693;&#35782;&#20043;&#38388;&#30340;&#28508;&#22312;&#20851;&#31995;&#26469;&#26725;&#25509;&#24322;&#26500;&#30693;&#35782;&#12290;&#22312;&#20004;&#20010;&#24120;&#35782;&#20107;&#23454;&#39564;&#35777;&#22522;&#20934;&#25968;&#25454;&#38598;CSQA2.0&#21644;CREAK&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;Decker&#30340;&#26377;&#25928;&#24615;&#65292;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#39564;&#35777;&#20102;&#23427;&#22312;&#25512;&#29702;&#20013;&#33719;&#21462;&#26356;&#22810;&#29645;&#36149;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Commonsense fact verification, as a challenging branch of commonsense question-answering (QA), aims to verify through facts whether a given commonsense claim is correct or not. Answering commonsense questions necessitates a combination of knowledge from various levels. However, existing studies primarily rest on grasping either unstructured evidence or potential reasoning paths from structured knowledge bases, yet failing to exploit the benefits of heterogeneous knowledge simultaneously. In light of this, we propose Decker, a commonsense fact verification model that is capable of bridging heterogeneous knowledge by uncovering latent relationships between structured and unstructured knowledge. Experimental results on two commonsense fact verification benchmark datasets, CSQA2.0 and CREAK demonstrate the effectiveness of our Decker and further analysis verifies its capability to seize more precious information through reasoning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#38382;&#39064;&#22238;&#31572;&#30340;5W&#22240;&#32032;&#20107;&#23454;&#39564;&#35777;&#26694;&#26550;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21322;&#33258;&#21160;&#20135;&#29983;&#30340;&#25968;&#25454;&#38598;FACTIFY-5WQA&#65292;&#20197;&#20415;&#21327;&#21161;&#20154;&#31867;&#26680;&#26597;&#21592;&#38024;&#23545;&#20107;&#23454;&#25552;&#20986;&#30456;&#20851;&#38382;&#39064;&#24182;&#39564;&#35777;&#20197;&#36798;&#20986;&#26368;&#32456;&#32467;&#35770;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#24335;&#30456;&#36739;&#20110;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#21644;&#24120;&#35268;&#20107;&#23454;&#39564;&#35777;&#31995;&#32479;&#20855;&#26377;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.04329</link><description>&lt;p&gt;
FACTIFY-5WQA&#65306;&#22522;&#20110;&#38382;&#39064;&#22238;&#31572;&#30340;5W&#22240;&#32032;&#20107;&#23454;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
FACTIFY-5WQA: 5W Aspect-based Fact Verification through Question Answering. (arXiv:2305.04329v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04329
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#38382;&#39064;&#22238;&#31572;&#30340;5W&#22240;&#32032;&#20107;&#23454;&#39564;&#35777;&#26694;&#26550;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21322;&#33258;&#21160;&#20135;&#29983;&#30340;&#25968;&#25454;&#38598;FACTIFY-5WQA&#65292;&#20197;&#20415;&#21327;&#21161;&#20154;&#31867;&#26680;&#26597;&#21592;&#38024;&#23545;&#20107;&#23454;&#25552;&#20986;&#30456;&#20851;&#38382;&#39064;&#24182;&#39564;&#35777;&#20197;&#36798;&#20986;&#26368;&#32456;&#32467;&#35770;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#24335;&#30456;&#36739;&#20110;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#21644;&#24120;&#35268;&#20107;&#23454;&#39564;&#35777;&#31995;&#32479;&#20855;&#26377;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#20107;&#23454;&#39564;&#35777;&#36817;&#24180;&#26469;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#24403;&#20170;&#30340;&#33258;&#21160;&#20107;&#23454;&#26680;&#26597;&#31995;&#32479;&#20027;&#35201;&#38598;&#20013;&#20110;&#20351;&#29992;&#25968;&#23383;&#35780;&#20998;&#26469;&#20272;&#35745;&#30495;&#23454;&#24615;&#65292;&#32780;&#36825;&#20123;&#35780;&#20998;&#24448;&#24448;&#26080;&#27861;&#34987;&#20154;&#31867;&#29702;&#35299;&#12290;&#32780;&#20154;&#31867;&#26680;&#26597;&#21592;&#36890;&#24120;&#20250;&#25353;&#29031;&#19968;&#20123;&#36923;&#36753;&#27493;&#39588;&#26469;&#39564;&#35777;&#19968;&#20010;&#31867;&#20284;&#30495;&#23454;&#30340;&#20027;&#24352;&#65292;&#24182;&#24471;&#20986;&#20854;&#30495;&#23454;&#24615;&#36824;&#26159;&#34394;&#20551;&#24615;&#30340;&#32467;&#35770;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#31181;&#22522;&#20110;&#26041;&#38754;&#65288;&#21306;&#20998;&#21738;&#20123;&#37096;&#20998;&#26159;&#30495;&#23454;&#30340;&#65292;&#21738;&#20123;&#26159;&#34394;&#20551;&#30340;&#65289;&#30340;&#21487;&#35299;&#37322;&#31995;&#32479;&#65292;&#21487;&#20197;&#21327;&#21161;&#20154;&#31867;&#26680;&#26597;&#21592;&#38024;&#23545;&#20107;&#23454;&#25552;&#20986;&#30456;&#20851;&#38382;&#39064;&#65292;&#28982;&#21518;&#20998;&#21035;&#39564;&#35777;&#20197;&#24471;&#20986;&#26368;&#32456;&#32467;&#35770;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;5W&#26694;&#26550;&#65288;&#35841;&#12289;&#20160;&#20040;&#12289;&#20309;&#26102;&#12289;&#20309;&#22320;&#21644;&#20026;&#20160;&#20040;&#65289;&#29992;&#20110;&#38382;&#39064;&#22238;&#31572;&#24335;&#30340;&#20107;&#23454;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21322;&#33258;&#21160;&#20135;&#29983;&#30340;&#25968;&#25454;&#38598;FACTIFY-5WQA&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;39&#20010;&#26041;&#38754;&#65292;&#27599;&#20010;&#26041;&#38754;&#37117;&#23545;&#24212;&#26576;&#20010;&#38382;&#39064;&#21644;&#23427;&#30340;&#31572;&#26696;&#65292;&#20197;&#21450;&#26159;&#21542;&#21487;&#39564;&#35777;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#23427;&#34701;&#21512;&#20102;&#21452;&#21521;&#38271;&#30701;&#26102;&#35760;&#24518;&#32593;&#32476;&#21644;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#29992;&#20110;&#25191;&#34892;5WQA&#20219;&#21153;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#38598;&#36827;&#34892;&#35780;&#27979;&#21644;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22810;&#20010;&#25351;&#26631;&#19978;&#37117;&#36229;&#36807;&#20102;&#20854;&#20182;&#29616;&#26377;&#30340;&#22522;&#32447;&#27169;&#22411;&#21644;&#24120;&#35268;&#20107;&#23454;&#39564;&#35777;&#31995;&#32479;&#65292;&#36825;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic fact verification has received significant attention recently. Contemporary automatic fact-checking systems focus on estimating truthfulness using numerical scores which are not human-interpretable. A human fact-checker generally follows several logical steps to verify a verisimilitude claim and conclude whether its truthful or a mere masquerade. Popular fact-checking websites follow a common structure for fact categorization such as half true, half false, false, pants on fire, etc. Therefore, it is necessary to have an aspect-based (delineating which part(s) are true and which are false) explainable system that can assist human fact-checkers in asking relevant questions related to a fact, which can then be validated separately to reach a final verdict. In this paper, we propose a 5W framework (who, what, when, where, and why) for question-answer-based fact explainability. To that end, we present a semi-automatically generated dataset called FACTIFY-5WQA, which consists of 39
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861; VCC&#65292;&#36890;&#36807;&#20248;&#20808;&#22788;&#29702;&#26368;&#37325;&#35201;&#30340;VIP&#26631;&#35760;&#65292;&#19968;&#23450;&#31243;&#24230;&#19978;&#21387;&#32553;&#24207;&#21015;&#65292;&#20174;&#32780;&#20351;Transformer&#27169;&#22411;&#21487;&#22788;&#29702;&#38271;&#24230;&#26356;&#38271;&#30340;&#24207;&#21015;&#12290;</title><link>http://arxiv.org/abs/2305.04241</link><description>&lt;p&gt;
Vcc: &#36890;&#36807;&#20248;&#20808;&#22788;&#29702;&#37325;&#35201;&#26631;&#35760;&#23558;Transformer&#25193;&#23637;&#21040;128K&#20196;&#29260;&#25110;&#26356;&#22810;
&lt;/p&gt;
&lt;p&gt;
Vcc: Scaling Transformers to 128K Tokens or More by Prioritizing Important Tokens. (arXiv:2305.04241v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04241
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861; VCC&#65292;&#36890;&#36807;&#20248;&#20808;&#22788;&#29702;&#26368;&#37325;&#35201;&#30340;VIP&#26631;&#35760;&#65292;&#19968;&#23450;&#31243;&#24230;&#19978;&#21387;&#32553;&#24207;&#21015;&#65292;&#20174;&#32780;&#20351;Transformer&#27169;&#22411;&#21487;&#22788;&#29702;&#38271;&#24230;&#26356;&#38271;&#30340;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#27169;&#22411;&#22312;&#29616;&#20195;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#20013;&#25198;&#28436;&#20851;&#38190;&#35282;&#33394;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#26377;&#20154;&#33268;&#21147;&#20110;&#38477;&#20302;&#36825;&#20123;&#27169;&#22411;&#30340;&#20108;&#27425;&#25104;&#26412;&#65288;&#20316;&#20026;&#24207;&#21015;&#38271;&#24230;&#30340;&#20989;&#25968;&#65289;&#65292;&#20294;&#22788;&#29702;&#36229;&#38271;&#24207;&#21015;&#65288;&#22914;&#36229;&#36807;16K&#26631;&#35760;&#65289;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;Ultra long sequences&#30340;Transformer&#27169;&#22411;&#30340;&#25928;&#29575;&#26174;&#30528;&#25552;&#39640;&#30340;&#26041;&#27861;&#65292;&#21363;&#22312;&#27599;&#23618;&#23558;&#24207;&#21015;&#21387;&#32553;&#20026;&#26356;&#23567;&#30340;&#34920;&#31034;&#65292;&#36890;&#36807;&#21033;&#29992;&#35768;&#22810;&#20219;&#21153;&#20013;&#20165;&#26377;&#30340;&#23569;&#25968;&#30340;&#29305;&#27530;&#26631;&#35760;&#65288;&#25105;&#20204;&#31216;&#20854;&#20026;VIP&#26631;&#35760;&#65289;&#19982;&#26368;&#32456;&#39044;&#27979;&#32467;&#26524;&#26368;&#30456;&#20851;&#30340;&#36825;&#20010;&#20107;&#23454;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;VIP&#26631;&#35760;&#30340;&#21387;&#32553;&#26041;&#27861;&#65292;&#21363;VIP&#26631;&#35760;&#20013;&#24515;&#21387;&#32553;&#65288;VCC&#65289;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#26681;&#25454;&#20854;&#23545;&#36817;&#20284;VIP&#26631;&#35760;&#34920;&#31034;&#30340;&#24433;&#21709;&#26377;&#36873;&#25321;&#22320;&#21387;&#32553;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers are central in modern natural language processing and computer vision applications. Despite recent works devoted to reducing the quadratic cost of such models (as a function of the sequence length), dealing with ultra long sequences (e.g., with more than 16K tokens) remains challenging. Applications such as answering questions based on a book or summarizing a scientific article are inefficient or infeasible. Here, we propose to significantly improve the efficiency of Transformers for ultra long sequences, by compressing the sequence into a much smaller representation at each layer. Specifically, by exploiting the fact that in many tasks, only a small subset of special tokens (we call VIP-tokens) are most relevant to the final prediction, we propose a VIP-token centric compression (VCC) scheme which selectively compresses the sequence based on their impact on approximating the representation of the VIP-tokens. Compared with competitive baselines, our algorithm is not only e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#35270;&#35273;&#35789;&#20041;&#28040;&#27495;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22806;&#37096;&#35789;&#27719;&#30693;&#35782;&#24211;&#30340;&#35789;&#20041;&#20449;&#24687;&#26469;&#35299;&#20915;&#21407;&#26469;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#27169;&#22411;&#20013;&#30340;&#22810;&#20041;&#35789;&#38382;&#39064;&#12290;&#37319;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#26469;&#21152;&#20837;&#35789;&#20041;&#23450;&#20041;&#65292;&#24182;&#36890;&#36807;&#19982;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340; GPT-3 &#23450;&#20041;&#29983;&#25104;&#26041;&#27861;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#35789;&#20856;&#22806;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.01788</link><description>&lt;p&gt;
&#35270;&#35273;&#19982;&#23450;&#20041;&#30456;&#36935;&#65306;&#34701;&#21512;&#35789;&#20041;&#20449;&#24687;&#30340;&#26080;&#30417;&#30563;&#35270;&#35273;&#35789;&#20041;&#28040;&#27495;
&lt;/p&gt;
&lt;p&gt;
Vision Meets Definitions: Unsupervised Visual Word Sense Disambiguation Incorporating Gloss Information. (arXiv:2305.01788v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#35270;&#35273;&#35789;&#20041;&#28040;&#27495;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22806;&#37096;&#35789;&#27719;&#30693;&#35782;&#24211;&#30340;&#35789;&#20041;&#20449;&#24687;&#26469;&#35299;&#20915;&#21407;&#26469;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#27169;&#22411;&#20013;&#30340;&#22810;&#20041;&#35789;&#38382;&#39064;&#12290;&#37319;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#26469;&#21152;&#20837;&#35789;&#20041;&#23450;&#20041;&#65292;&#24182;&#36890;&#36807;&#19982;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340; GPT-3 &#23450;&#20041;&#29983;&#25104;&#26041;&#27861;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#35789;&#20856;&#22806;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35789;&#20041;&#28040;&#27495;&#26159;&#19968;&#39033;&#20219;&#21153;&#65292;&#26088;&#22312;&#25214;&#21040;&#26368;&#20934;&#30830;&#22320;&#25551;&#36848;&#32473;&#23450;&#19978;&#19979;&#25991;&#20013;&#30446;&#26631;&#35789;&#27491;&#30830;&#24847;&#20041;&#30340;&#22270;&#20687;&#12290;&#20197;&#24448;&#30340;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#27169;&#22411;&#24448;&#24448;&#21463;&#21040;&#35789;&#20041;&#22810;&#20041;&#24615;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#35270;&#35273;&#35789;&#20041;&#28040;&#27495;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#22806;&#37096;&#35789;&#27719;&#30693;&#35782;&#24211;&#30340;&#35789;&#27719;&#20449;&#24687;&#65292;&#29305;&#21035;&#26159;&#35789;&#20041;&#23450;&#20041;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24314;&#35758;&#22312;&#27809;&#26377;&#25552;&#20379;&#31572;&#26696;&#30340;&#35789;&#20041;&#20449;&#24687;&#26102;&#65292;&#37319;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#26469;&#21152;&#20837;&#35789;&#20041;&#23450;&#20041;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#25913;&#36827;&#35789;&#20856;&#22806;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;GPT-3&#23450;&#20041;&#29983;&#25104;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#22522;&#20110;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#26041;&#27861;&#26126;&#26174;&#25552;&#39640;&#20102;&#35270;&#35273;&#35789;&#20041;&#28040;&#27495;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#19978;&#19979;&#25991;&#30456;&#20851;&#23450;&#20041;&#29983;&#25104;&#26041;&#27861;&#22312;&#35789;&#20856;&#22806;&#20363;&#23376;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#23450;&#20041;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual Word Sense Disambiguation (VWSD) is a task to find the image that most accurately depicts the correct sense of the target word for the given context. Previously, image-text matching models often suffered from recognizing polysemous words. This paper introduces an unsupervised VWSD approach that uses gloss information of an external lexical knowledge-base, especially the sense definitions. Specifically, we suggest employing Bayesian inference to incorporate the sense definitions when sense information of the answer is not provided. In addition, to ameliorate the out-of-dictionary (OOD) issue, we propose a context-aware definition generation with GPT-3. Experimental results show that the VWSD performance significantly increased with our Bayesian inference-based approach. In addition, our context-aware definition generation achieved prominent performance improvement in OOD examples exhibiting better performance than the existing definition generation method. We will publish source 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25351;&#20196;&#35843;&#25972;&#30340;LLM Flan-T5&#20316;&#20026;&#25991;&#26412;&#32534;&#30721;&#22120;&#21644;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;(LDM)&#30340;&#26041;&#27861;TANGO&#29983;&#25104;&#25991;&#26412;&#21040;&#38899;&#39057;(TTA)&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;AudioCaps&#27979;&#35797;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#20808;&#36827;&#30340;AudioLDM&#12290;</title><link>http://arxiv.org/abs/2304.13731</link><description>&lt;p&gt;
&#20351;&#29992;&#25351;&#20196;&#35843;&#25972;&#30340;LLM&#21644;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#21040;&#38899;&#39057;
&lt;/p&gt;
&lt;p&gt;
Text-to-Audio Generation using Instruction-Tuned LLM and Latent Diffusion Model. (arXiv:2304.13731v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25351;&#20196;&#35843;&#25972;&#30340;LLM Flan-T5&#20316;&#20026;&#25991;&#26412;&#32534;&#30721;&#22120;&#21644;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;(LDM)&#30340;&#26041;&#27861;TANGO&#29983;&#25104;&#25991;&#26412;&#21040;&#38899;&#39057;(TTA)&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;AudioCaps&#27979;&#35797;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#20808;&#36827;&#30340;AudioLDM&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#24040;&#22823;&#35268;&#27169;&#20801;&#35768;&#35768;&#22810;&#26377;&#36259;&#30340;&#23646;&#24615;&#65292;&#27604;&#22914;&#65292;&#22522;&#20110;&#25351;&#20196;&#21644;&#24605;&#36335;&#38142;&#30340;&#24494;&#35843;&#65292;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20219;&#21153;&#20013;&#26174;&#30528;&#25552;&#39640;&#20102;&#38646;&#27425;&#21644;&#23569;&#37327;&#35757;&#32451;&#26679;&#26412;&#30340;&#24615;&#33021;&#12290;&#21463;&#21040;&#36825;&#20123;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#36825;&#26679;&#19968;&#31181;&#32463;&#36807;&#25351;&#20196;&#35843;&#25972;&#30340;LLM Flan-T5&#20316;&#20026;&#25991;&#26412;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#25991;&#26412;&#21040;&#38899;&#39057;(TTA)&#29983;&#25104;&#20219;&#21153;&#8212;&#8212;&#30446;&#26631;&#26159;&#26681;&#25454;&#20854;&#25991;&#26412;&#25551;&#36848;&#29983;&#25104;&#38899;&#39057;&#12290;&#20043;&#21069;&#20851;&#20110;TTA&#30340;&#24037;&#20316;&#35201;&#20040;&#39044;&#20808;&#35757;&#32451;&#19968;&#20010;&#32852;&#21512;&#30340;&#25991;&#26412;-&#38899;&#39057;&#32534;&#30721;&#22120;&#65292;&#35201;&#20040;&#20351;&#29992;&#19968;&#20010;&#38750;&#25351;&#20196;&#35843;&#35856;&#30340;&#27169;&#22411;&#65292;&#22914;T5&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;(LDM)&#30340;&#26041;&#27861;TANGO&#22312;AudioCaps&#27979;&#35797;&#38598;&#19978;&#34920;&#29616;&#20986;&#27604;&#26368;&#20808;&#36827;&#30340;AudioLDM&#26356;&#22909;&#30340;&#22823;&#22810;&#25968;&#25351;&#26631;&#65292;&#24182;&#22312;&#20854;&#20313;&#25351;&#26631;&#19978;&#25345;&#24179;&#65292;&#23613;&#31649;&#25105;&#20204;&#20351;&#29992;&#20102;63&#20493;&#23567;&#30340;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;LDM&#65292;&#24182;&#20445;&#25345;&#25991;&#26412;&#32534;&#30721;&#22120;&#19981;&#21464;&#12290;&#36825;&#31181;&#25913;&#36827;&#21487;&#33021;&#36824;&#24402;&#22240;&#20110;&#37319;&#29992;&#22522;&#20110;&#38899;&#39057;&#21387;&#21147;&#32423;&#30340;&#28151;&#38899;&#35757;&#32451;&#38598;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
The immense scale of the recent large language models (LLM) allows many interesting properties, such as, instruction- and chain-of-thought-based fine-tuning, that has significantly improved zero- and few-shot performance in many natural language processing (NLP) tasks. Inspired by such successes, we adopt such an instruction-tuned LLM Flan-T5 as the text encoder for text-to-audio (TTA) generation -- a task where the goal is to generate an audio from its textual description. The prior works on TTA either pre-trained a joint text-audio encoder or used a non-instruction-tuned model, such as, T5. Consequently, our latent diffusion model (LDM)-based approach TANGO outperforms the state-of-the-art AudioLDM on most metrics and stays comparable on the rest on AudioCaps test set, despite training the LDM on a 63 times smaller dataset and keeping the text encoder frozen. This improvement might also be attributed to the adoption of audio pressure level-based sound mixing for training set augmenta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#29702;&#35770;&#26041;&#27861;&#8212;&#8212;&#34892;&#20026;&#26399;&#26395;&#36793;&#30028;&#65288;BEB&#65289;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#40784;&#30340;&#22522;&#26412;&#38480;&#21046;&#65292;&#24182;&#35777;&#26126;&#20219;&#20309;&#23545;&#40784;&#36807;&#31243;&#37117;&#26080;&#27861;&#26681;&#38500;&#19981;&#24076;&#26395;&#30340;&#34892;&#20026;&#65292;&#36825;&#23545;&#20110;&#38450;&#27490;&#24694;&#24847;&#25915;&#20987;&#26159;&#19981;&#23433;&#20840;&#30340;&#12290;</title><link>http://arxiv.org/abs/2304.11082</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#30340;&#22522;&#26412;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Fundamental Limitations of Alignment in Large Language Models. (arXiv:2304.11082v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#29702;&#35770;&#26041;&#27861;&#8212;&#8212;&#34892;&#20026;&#26399;&#26395;&#36793;&#30028;&#65288;BEB&#65289;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#40784;&#30340;&#22522;&#26412;&#38480;&#21046;&#65292;&#24182;&#35777;&#26126;&#20219;&#20309;&#23545;&#40784;&#36807;&#31243;&#37117;&#26080;&#27861;&#26681;&#38500;&#19981;&#24076;&#26395;&#30340;&#34892;&#20026;&#65292;&#36825;&#23545;&#20110;&#38450;&#27490;&#24694;&#24847;&#25915;&#20987;&#26159;&#19981;&#23433;&#20840;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#19982;&#20154;&#20132;&#20114;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#26041;&#38754;&#26159;&#23545;&#40784;&#20854;&#34892;&#20026;&#65292;&#20351;&#20854;&#23545;&#20854;&#20154;&#31867;&#29992;&#25143;&#26377;&#29992;&#19988;&#26080;&#23475;&#12290;&#36825;&#36890;&#24120;&#36890;&#36807;&#35843;&#25972;&#27169;&#22411;&#30340;&#26041;&#24335;&#26469;&#23454;&#29616;&#65292;&#20197;&#22686;&#24378;&#25152;&#38656;&#30340;&#34892;&#20026;&#24182;&#25233;&#21046;&#19981;&#24076;&#26395;&#30340;&#34892;&#20026;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#34892;&#20026;&#26399;&#26395;&#36793;&#30028;(BEB)&#30340;&#29702;&#35770;&#26041;&#27861;&#65292;&#23427;&#20801;&#35768;&#25105;&#20204;&#27491;&#24335;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20960;&#20010;&#20869;&#22312;&#29305;&#24449;&#21644;&#23545;&#40784;&#30340;&#38480;&#21046;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#23545;&#20110;&#20219;&#20309;&#20855;&#26377;&#34987;&#35813;&#27169;&#22411;&#34920;&#29616;&#20986;&#30340;&#26377;&#38480;&#27010;&#29575;&#30340;&#34892;&#20026;&#65292;&#37117;&#23384;&#22312;&#21487;&#20197;&#35302;&#21457;&#27169;&#22411;&#36755;&#20986;&#27492;&#34892;&#20026;&#30340;&#25552;&#31034;&#65292;&#20854;&#27010;&#29575;&#38543;&#25552;&#31034;&#30340;&#38271;&#24230;&#22686;&#21152;&#32780;&#22686;&#21152;&#12290;&#36825;&#24847;&#21619;&#30528;&#20219;&#20309;&#20943;&#24369;&#19981;&#24076;&#26395;&#30340;&#34892;&#20026;&#20294;&#26410;&#23558;&#20854;&#23436;&#20840;&#28040;&#38500;&#30340;&#23545;&#40784;&#36807;&#31243;&#37117;&#26080;&#27861;&#25269;&#24481;&#38024;&#23545;&#24615;&#25915;&#20987;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#25552;&#31034;&#20102;&#39046;&#20808;&#30340;
&lt;/p&gt;
&lt;p&gt;
An important aspect in developing language models that interact with humans is aligning their behavior to be useful and unharmful for their human users. This is usually achieved by tuning the model in a way that enhances desired behaviors and inhibits undesired ones, a process referred to as alignment. In this paper, we propose a theoretical approach called Behavior Expectation Bounds (BEB) which allows us to formally investigate several inherent characteristics and limitations of alignment in large language models. Importantly, we prove that for any behavior that has a finite probability of being exhibited by the model, there exist prompts that can trigger the model into outputting this behavior, with probability that increases with the length of the prompt. This implies that any alignment process that attenuates undesired behavior but does not remove it altogether, is not safe against adversarial prompting attacks. Furthermore, our framework hints at the mechanism by which leading al
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#23545;&#35805;&#31995;&#32479;&#20013;&#24433;&#21709;&#32842;&#22825;&#20307;&#39564;&#30340;&#22810;&#31181;&#22240;&#32032;&#65292;&#21253;&#25324;&#25552;&#31034;&#12289;&#36830;&#36143;&#24615;&#12289;&#24773;&#24863;&#12289;&#30456;&#20284;&#24615;&#21644;&#29992;&#25143;&#23545;&#35805;&#20195;&#29702;&#30340;&#22909;&#24863;&#24230;&#65292;&#21457;&#29616;&#29992;&#25143;&#30340;&#22909;&#24863;&#24230;&#21644;&#36830;&#36143;&#24615;&#12289;&#24773;&#24863;&#12289;&#30456;&#20284;&#24615;&#26159;&#32842;&#22825;&#20307;&#39564;&#30340;&#27491;&#21521;&#39044;&#27979;&#22240;&#32032;&#12290;&#27492;&#22806;&#65292;&#29992;&#25143;&#21487;&#33021;&#26356;&#21916;&#27426;&#20855;&#26377;&#22806;&#21521;&#24615;&#12289;&#24320;&#25918;&#24615;&#12289;&#36131;&#20219;&#24515;&#12289;&#23452;&#20154;&#24615;&#21644;&#38750;&#31070;&#32463;&#36136;&#29305;&#24449;&#30340;&#23545;&#35805;&#20195;&#29702;&#12290;</title><link>http://arxiv.org/abs/2304.10785</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#23545;&#35805;&#26381;&#21153;&#30340;&#32842;&#22825;&#20307;&#39564;&#39044;&#27979;&#22240;&#32032;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Which Factors Predict the Chat Experience of a Natural Language Generation Dialogue Service?. (arXiv:2304.10785v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#23545;&#35805;&#31995;&#32479;&#20013;&#24433;&#21709;&#32842;&#22825;&#20307;&#39564;&#30340;&#22810;&#31181;&#22240;&#32032;&#65292;&#21253;&#25324;&#25552;&#31034;&#12289;&#36830;&#36143;&#24615;&#12289;&#24773;&#24863;&#12289;&#30456;&#20284;&#24615;&#21644;&#29992;&#25143;&#23545;&#35805;&#20195;&#29702;&#30340;&#22909;&#24863;&#24230;&#65292;&#21457;&#29616;&#29992;&#25143;&#30340;&#22909;&#24863;&#24230;&#21644;&#36830;&#36143;&#24615;&#12289;&#24773;&#24863;&#12289;&#30456;&#20284;&#24615;&#26159;&#32842;&#22825;&#20307;&#39564;&#30340;&#27491;&#21521;&#39044;&#27979;&#22240;&#32032;&#12290;&#27492;&#22806;&#65292;&#29992;&#25143;&#21487;&#33021;&#26356;&#21916;&#27426;&#20855;&#26377;&#22806;&#21521;&#24615;&#12289;&#24320;&#25918;&#24615;&#12289;&#36131;&#20219;&#24515;&#12289;&#23452;&#20154;&#24615;&#21644;&#38750;&#31070;&#32463;&#36136;&#29305;&#24449;&#30340;&#23545;&#35805;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#24565;&#24615;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#32842;&#22825;&#20307;&#39564;&#12290;&#25105;&#20204;&#20351;&#29992;&#37096;&#20998;&#26368;&#23567;&#20108;&#20056;&#32467;&#26500;&#26041;&#31243;&#24314;&#27169;&#26041;&#27861;&#65288;PLS-SEM&#65289;&#23545;120&#21517;&#21442;&#19982;&#32773;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#33719;&#24471;&#20102;0.541&#30340;R&#26041;&#20540;&#12290;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#22810;&#31181;&#22240;&#32032;&#65292;&#21253;&#25324;&#29992;&#20110;&#29983;&#25104;&#30340;&#25552;&#31034;&#12289;&#23545;&#35805;&#20013;&#30340;&#36830;&#36143;&#24615;&#12289;&#24773;&#24863;&#21644;&#30456;&#20284;&#24615;&#65292;&#20197;&#21450;&#29992;&#25143;&#23545;&#35805;&#20195;&#29702;&#30340;&#22909;&#24863;&#24230;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#23376;&#38598;&#30340;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#29992;&#25143;&#30340;&#22909;&#24863;&#24230;&#21644;&#23545;&#35805;&#20013;&#30340;&#36830;&#36143;&#24615;&#12289;&#24773;&#24863;&#21644;&#30456;&#20284;&#24615;&#26159;&#29992;&#25143;&#32842;&#22825;&#20307;&#39564;&#30340;&#27491;&#21521;&#39044;&#27979;&#22240;&#32032;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#29992;&#25143;&#21487;&#33021;&#26356;&#21916;&#27426;&#20855;&#26377;&#22806;&#21521;&#24615;&#12289;&#24320;&#25918;&#24615;&#12289;&#36131;&#20219;&#24515;&#12289;&#23452;&#20154;&#24615;&#21644;&#38750;&#31070;&#32463;&#36136;&#29305;&#24449;&#30340;&#23545;&#35805;&#20195;&#29702;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#30740;&#31350;&#65292;&#33258;&#36866;&#24212;&#23545;&#35805;&#31995;&#32479;&#21487;&#20197;&#20351;&#29992;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#26469;&#25512;&#26029;&#25105;&#20204;&#27169;&#22411;&#20013;&#30340;&#22240;&#32032;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#23545;&#35805;&#20195;&#29702;&#30340;&#29305;&#24449;&#26469;&#39044;&#27979;&#29992;&#25143;&#30340;&#32842;&#22825;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we proposed a conceptual model to predict the chat experience in a natural language generation dialog system. We evaluated the model with 120 participants with Partial Least Squares Structural Equation Modeling (PLS-SEM) and obtained an R-square (R2) with 0.541. The model considers various factors, including the prompts used for generation; coherence, sentiment, and similarity in the conversation; and users' perceived dialog agents' favorability. We then further explore the effectiveness of the subset of our proposed model. The results showed that users' favorability and coherence, sentiment, and similarity in the dialogue are positive predictors of users' chat experience. Moreover, we found users may prefer dialog agents with characteristics of Extroversion, Openness, Conscientiousness, Agreeableness, and Non-Neuroticism. Through our research, an adaptive dialog system might use collected data to infer factors in our model, predict the chat experience for users through 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#33258;&#28982;&#35821;&#35328;&#20316;&#20026;&#32534;&#31243;&#35821;&#35328;&#24182;&#36890;&#36807;&#23398;&#20064;&#32534;&#31243;&#26041;&#27861;&#35753;&#22823;&#35821;&#35328;&#27169;&#22411;&#30452;&#25509;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#31243;&#24207;&#24182;&#25351;&#23548;&#25512;&#29702;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#35299;&#20915;&#32534;&#31243;&#20219;&#21153;&#19978;&#27604;&#22522;&#32447;&#26041;&#27861;&#26377;&#26356;&#39640;&#30340;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.10464</link><description>&lt;p&gt;
&#29992;&#33258;&#28982;&#35821;&#35328;&#23398;&#20064;&#32534;&#31243;
&lt;/p&gt;
&lt;p&gt;
Learning to Program with Natural Language. (arXiv:2304.10464v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10464
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#33258;&#28982;&#35821;&#35328;&#20316;&#20026;&#32534;&#31243;&#35821;&#35328;&#24182;&#36890;&#36807;&#23398;&#20064;&#32534;&#31243;&#26041;&#27861;&#35753;&#22823;&#35821;&#35328;&#27169;&#22411;&#30452;&#25509;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#31243;&#24207;&#24182;&#25351;&#23548;&#25512;&#29702;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#35299;&#20915;&#32534;&#31243;&#20219;&#21153;&#19978;&#27604;&#22522;&#32447;&#26041;&#27861;&#26377;&#26356;&#39640;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#22522;&#26412;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#65292;&#36825;&#24341;&#36215;&#20102;&#23454;&#29616;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#24076;&#26395;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#23436;&#25104;&#22797;&#26434;&#20219;&#21153;&#65292;&#25105;&#20204;&#38656;&#35201;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32534;&#31243;&#65292;&#28982;&#21518;&#25353;&#29031;&#31243;&#24207;&#29983;&#25104;&#29305;&#23450;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#32534;&#31243;&#35821;&#35328;&#26469;&#25551;&#36848;&#20219;&#21153;&#36807;&#31243;&#65292;&#20351;&#23427;&#20204;&#26131;&#20110;&#20154;&#31867;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#12290;&#34429;&#28982;&#22823;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#30452;&#25509;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#31243;&#24207;&#65292;&#20294;&#36825;&#20123;&#31243;&#24207;&#21487;&#33021;&#20173;&#28982;&#23384;&#22312;&#38169;&#35823;&#25110;&#19981;&#23436;&#25972;&#30340;&#27493;&#39588;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#23398;&#20064;&#32534;&#31243;&#65288;LP&#65289;&#30340;&#26041;&#27861;&#65292;&#35201;&#27714;&#22823;&#35821;&#35328;&#27169;&#22411;&#20174;&#22797;&#26434;&#20219;&#21153;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#33258;&#28982;&#35821;&#35328;&#31243;&#24207;&#65292;&#28982;&#21518;&#20351;&#29992;&#23398;&#20064;&#21040;&#30340;&#31243;&#24207;&#26469;&#25351;&#23548;&#25512;&#29702;&#12290;&#25105;&#20204;&#22312;AMPS&#65288;&#39640;&#20013;&#25968;&#23398;&#65289;&#21644;Math&#65288;&#31454;&#36187;&#25968;&#23398;&#38382;&#39064;&#65289;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#27979;&#35797;ChatGP&#35299;&#20915;&#32534;&#31243;&#20219;&#21153;&#26102;&#65292;LP&#33021;&#22815;&#23454;&#29616;80%&#30340;&#25104;&#21151;&#29575;&#65292;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown remarkable performance in various basic natural language tasks, which raises hopes for achieving Artificial General Intelligence. To better complete complex tasks, we need LLMs to program for the task and then follow the program to generate a specific solution for the test sample. We propose using natural language as a new programming language to describe task procedures, making them easily understandable to both humans and LLMs. LLMs are capable of directly generating natural language programs, but these programs may still contain factual errors or incomplete steps. Therefore, we further propose the Learning to Program (LP) method to ask LLMs themselves to learn natural language programs from the training dataset of complex tasks and then use the learned program to guide inference. Our experiments on the AMPS (high school math) and Math (competition mathematics problems) datasets demonstrate the effectiveness of our approach. When testing ChatGP
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#22312;&#25277;&#35937;&#27010;&#25324;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#33258;&#21160;&#21270;&#25351;&#26631;&#21644;&#30450;&#23457;&#20154;&#21592;&#35780;&#20272;&#26174;&#31034;ChatGPT&#29983;&#25104;&#30340;&#25688;&#35201;&#22312;&#20154;&#31867;&#35270;&#35282;&#19979;&#38590;&#20197;&#20998;&#36776;&#30495;&#20551;&#12290;</title><link>http://arxiv.org/abs/2303.17650</link><description>&lt;p&gt;
&#36890;&#36807;&#30450;&#23457;&#35780;&#20272;&#21644;&#25991;&#26412;&#20998;&#31867;&#31639;&#27861;&#27604;&#36739;ChatGPT&#29983;&#25104;&#30340;&#25277;&#35937;&#25688;&#35201;&#21644;&#30495;&#23454;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Comparing Abstractive Summaries Generated by ChatGPT to Real Summaries Through Blinded Reviewers and Text Classification Algorithms. (arXiv:2303.17650v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#22312;&#25277;&#35937;&#27010;&#25324;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#33258;&#21160;&#21270;&#25351;&#26631;&#21644;&#30450;&#23457;&#20154;&#21592;&#35780;&#20272;&#26174;&#31034;ChatGPT&#29983;&#25104;&#30340;&#25688;&#35201;&#22312;&#20154;&#31867;&#35270;&#35282;&#19979;&#38590;&#20197;&#20998;&#36776;&#30495;&#20551;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22240;&#20854;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#20986;&#33394;&#34920;&#29616;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;OpenAI&#24320;&#21457;&#30340;ChatGPT&#26159;&#35821;&#35328;&#27169;&#22411;&#23478;&#26063;&#30340;&#26368;&#26032;&#25104;&#21592;&#65292;&#30001;&#20110;&#20854;&#31867;&#20154;&#30340;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#65292;&#34987;&#19968;&#20123;&#20154;&#31216;&#20026;&#19968;&#39033;&#39072;&#35206;&#24615;&#25216;&#26415;&#12290;&#23613;&#31649;&#32593;&#32476;&#19978;&#26377;&#35768;&#22810;ChatGPT&#30340;&#20363;&#23376;&#26469;&#35780;&#20272;&#20854;&#24378;&#24369;&#20043;&#22788;&#65292;&#20294;&#21482;&#26377;&#23569;&#25968;&#31995;&#32479;&#24615;&#30340;&#30740;&#31350;&#23384;&#22312;&#12290;&#20026;&#20102;&#20026;ChatGPT&#30340;&#31995;&#32479;&#24615;&#30740;&#31350;&#20570;&#20986;&#36129;&#29486;&#65292;&#25105;&#20204;&#36890;&#36807;&#33258;&#21160;&#21270;&#25351;&#26631;&#21644;&#30450;&#23457;&#20154;&#21592;&#35780;&#20272;&#20102;ChatGPT&#22312;&#25277;&#35937;&#27010;&#25324;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#36824;&#26500;&#24314;&#20102;&#33258;&#21160;&#25991;&#26412;&#20998;&#31867;&#22120;&#26469;&#26816;&#27979;ChatGPT&#29983;&#25104;&#30340;&#25688;&#35201;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34429;&#28982;&#25991;&#26412;&#20998;&#31867;&#31639;&#27861;&#21487;&#20197;&#21306;&#20998;&#30495;&#23454;&#21644;&#29983;&#25104;&#30340;&#25688;&#35201;&#65292;&#20294;&#20154;&#31867;&#26080;&#27861;&#21306;&#20998;&#30495;&#23454;&#25688;&#35201;&#21644;ChatGPT&#29983;&#25104;&#30340;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have gathered significant attention due to their impressive performance on a variety of tasks. ChatGPT, developed by OpenAI, is a recent addition to the family of language models and is being called a disruptive technology by a few, owing to its human-like text-generation capabilities. Although, many anecdotal examples across the internet have evaluated ChatGPT's strength and weakness, only a few systematic research studies exist. To contribute to the body of literature of systematic research on ChatGPT, we evaluate the performance of ChatGPT on Abstractive Summarization by the means of automated metrics and blinded human reviewers. We also build automatic text classifiers to detect ChatGPT generated summaries. We found that while text classification algorithms can distinguish between real and generated summaries, humans are unable to distinguish between real summaries and those produced by ChatGPT.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#20030;&#21150;&#22312;2023 IEEE&#28216;&#25103;&#20250;&#35758;&#19978;&#30340;&#31532;&#19968;&#23626;ChatGPT4PCG&#27604;&#36187;&#65292;&#30446;&#26631;&#26159;&#35753;ChatGPT&#29983;&#25104;&#20855;&#26377;&#39640;&#31283;&#23450;&#24615;&#21644;&#31867;&#20284;&#35282;&#33394;&#30340;&#29305;&#36136;&#26469;&#29983;&#25104;&#20855;&#26377;&#31185;&#23398;&#40479;&#35282;&#33394;&#32423;&#27700;&#24179;&#30340;&#20851;&#21345;&#12290;</title><link>http://arxiv.org/abs/2303.15662</link><description>&lt;p&gt;
ChatGPT4PCG&#27604;&#36187;&#65306;&#31185;&#23398;&#40479;&#35282;&#33394;&#32423;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
ChatGPT4PCG Competition: Character-like Level Generation for Science Birds. (arXiv:2303.15662v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15662
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#20030;&#21150;&#22312;2023 IEEE&#28216;&#25103;&#20250;&#35758;&#19978;&#30340;&#31532;&#19968;&#23626;ChatGPT4PCG&#27604;&#36187;&#65292;&#30446;&#26631;&#26159;&#35753;ChatGPT&#29983;&#25104;&#20855;&#26377;&#39640;&#31283;&#23450;&#24615;&#21644;&#31867;&#20284;&#35282;&#33394;&#30340;&#29305;&#36136;&#26469;&#29983;&#25104;&#20855;&#26377;&#31185;&#23398;&#40479;&#35282;&#33394;&#32423;&#27700;&#24179;&#30340;&#20851;&#21345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;2023&#24180;IEEE&#28216;&#25103;&#20250;&#35758;&#19978;&#30340;&#31532;&#19968;&#23626;ChatGPT4PCG&#27604;&#36187;&#12290;&#26412;&#27425;&#27604;&#36187;&#30340;&#30446;&#26631;&#26159;&#35753;&#21442;&#36187;&#32773;&#36890;&#36807;&#21019;&#36896;&#24615;&#21644;&#25552;&#31034;&#24037;&#31243;&#25216;&#33021;&#65292;&#20026;ChatGPT&#21019;&#24314;&#26377;&#25928;&#30340;&#25552;&#31034;&#65292;&#20351;&#20854;&#33021;&#22815;&#20855;&#26377;&#39640;&#31283;&#23450;&#24615;&#21644;&#31867;&#20284;&#35282;&#33394;&#30340;&#29305;&#36136;&#26469;&#29983;&#25104;&#20855;&#26377;&#31185;&#23398;&#40479;&#35282;&#33394;&#32423;&#27700;&#24179;&#30340;&#20851;&#21345;&#12290;&#20026;&#20102;&#38477;&#20302;&#21442;&#36187;&#38376;&#27099;&#65292;&#25105;&#20204;&#23558;&#20219;&#21153;&#38480;&#21046;&#22312;&#29983;&#25104;&#22823;&#20889;&#33521;&#25991;&#23383;&#27597;&#12290;&#21442;&#36187;&#20316;&#21697;&#30340;&#36136;&#37327;&#30001;&#20854;&#31283;&#23450;&#24615;&#21644;&#19982;&#32473;&#23450;&#23383;&#31526;&#30340;&#30456;&#20284;&#24615;&#20915;&#23450;&#12290;&#32473;&#21442;&#36187;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#26679;&#20363;&#25552;&#31034;&#20379;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the first ChatGPT4PCG Competition at the 2023 IEEE Conference on Games. The objective of this competition is for participants to create effective prompts for ChatGPT--enabling it to generate Science Birds levels with high stability and character-like qualities--fully using their creativity as well as prompt engineering skills. ChatGPT is a conversational agent developed by OpenAI. Science Birds is selected as the competition platform because designing an Angry Birds-like level is not a trivial task due to the in-game gravity; the playability of the levels is determined by their stability. To lower the entry barrier to the competition, we limit the task to the generation of capitalized English alphabetical characters. Here, the quality of the generated levels is determined by their stability and similarity to the given characters. A sample prompt is provided to participants for their reference. An experiment is conducted to determine the effectiveness of its modified
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#20070;&#38754;&#35821;&#19982;&#32553;&#20889;&#23450;&#24459;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#21457;&#29616;&#36825;&#19968;&#23450;&#24459;&#20063;&#36866;&#29992;&#20110;&#21475;&#35821;&#12290;&#32467;&#26524;&#25552;&#20379;&#20102;&#21387;&#32553;&#35821;&#35328;&#30340;&#38388;&#25509;&#35777;&#25454;&#65292;&#21363;&#32553;&#20889;&#23450;&#24459;&#26159;&#26368;&#20248;&#32534;&#30721;&#30340;&#39044;&#27979;&#65292;&#32780;&#36890;&#36807;&#33521;&#35821;&#30340;&#21382;&#21490;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#20154;&#20204;&#22312;&#35821;&#35328;&#20013;&#23454;&#38469;&#20351;&#29992;&#30340;&#35789;&#27719;&#25968;&#37327;&#27491;&#22312;&#32553;&#20943;&#12290;</title><link>http://arxiv.org/abs/2303.10128</link><description>&lt;p&gt;
&#30452;&#25509;&#21644;&#38388;&#25509;&#35777;&#25454;&#34920;&#26126;&#35789;&#27719;&#38271;&#24230;&#34987;&#21387;&#32553;. &#23545; Zipf&#30340;&#32553;&#20889;&#23450;&#24459;&#30340;&#37325;&#26032;&#23457;&#35270;.
&lt;/p&gt;
&lt;p&gt;
Direct and indirect evidence of compression of word lengths. Zipf's law of abbreviation revisited. (arXiv:2303.10128v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10128
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#20070;&#38754;&#35821;&#19982;&#32553;&#20889;&#23450;&#24459;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#21457;&#29616;&#36825;&#19968;&#23450;&#24459;&#20063;&#36866;&#29992;&#20110;&#21475;&#35821;&#12290;&#32467;&#26524;&#25552;&#20379;&#20102;&#21387;&#32553;&#35821;&#35328;&#30340;&#38388;&#25509;&#35777;&#25454;&#65292;&#21363;&#32553;&#20889;&#23450;&#24459;&#26159;&#26368;&#20248;&#32534;&#30721;&#30340;&#39044;&#27979;&#65292;&#32780;&#36890;&#36807;&#33521;&#35821;&#30340;&#21382;&#21490;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#20154;&#20204;&#22312;&#35821;&#35328;&#20013;&#23454;&#38469;&#20351;&#29992;&#30340;&#35789;&#27719;&#25968;&#37327;&#27491;&#22312;&#32553;&#20943;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Zipf&#32553;&#20889;&#23450;&#24459;&#25351;&#30340;&#26159;&#26356;&#24120;&#35265;&#30340;&#21333;&#35789;&#26356;&#30701;&#65292;&#26159;&#35821;&#35328;&#26222;&#36941;&#24615;&#30340;&#26368;&#22362;&#23454;&#30340;&#20505;&#36873;&#32773;&#65292;&#23427;&#26377;&#21487;&#33021;&#26159;&#27809;&#26377;&#20363;&#22806;&#25110;&#32773;&#20363;&#22806;&#38750;&#24120;&#23569;&#30340;&#35821;&#35328;&#26222;&#36941;&#24615;&#12290;&#33258;&#20174;Zipf&#30340;&#24320;&#21019;&#24615;&#30740;&#31350;&#20197;&#26469;&#65292;&#36825;&#19968;&#23450;&#24459;&#19968;&#30452;&#34987;&#35748;&#20026;&#26159;&#36890;&#20449;&#30340;&#26222;&#36941;&#21407;&#29702;&#30340;&#34920;&#29616;&#65292;&#21363;&#36890;&#36807;&#32553;&#30701;&#35789;&#27719;&#38271;&#24230;&#26469;&#20943;&#23569;&#36890;&#20449;&#30340;&#21162;&#21147;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#20070;&#38754;&#35821;&#19982;&#32553;&#20889;&#23450;&#24459;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#20851;&#38190;&#22320;&#65292;&#25105;&#20204;&#25552;&#20379;&#26356;&#24191;&#27867;&#30340;&#35777;&#25454;&#65292;&#34920;&#26126;&#36825;&#19968;&#23450;&#24459;&#20063;&#36866;&#29992;&#20110;&#21475;&#35821;&#65288;&#24403;&#29992;&#26102;&#38388;&#26469;&#27979;&#37327;&#35789;&#27719;&#38271;&#24230;&#26102;&#65289;&#65292;&#29305;&#21035;&#26159;&#36866;&#29992;&#20110;&#26469;&#33258;14&#20010;&#35821;&#35328;&#23478;&#26063;&#30340;46&#31181;&#35821;&#35328;&#12290;&#19982;&#32553;&#20889;&#23450;&#24459;&#30340;&#19968;&#33268;&#24615;&#25552;&#20379;&#20102;&#21387;&#32553;&#35821;&#35328;&#30340;&#38388;&#25509;&#35777;&#25454;&#65292;&#36825;&#26159;&#36890;&#36807;&#29702;&#35770;&#35770;&#35777;&#24471;&#20986;&#30340;&#65292;&#21363;&#32553;&#20889;&#23450;&#24459;&#26159;&#26368;&#20248;&#32534;&#30721;&#30340;&#39044;&#27979;&#12290;&#37492;&#20110;&#38656;&#35201;&#30452;&#25509;&#35777;&#25454;&#26469;&#35777;&#26126;&#21387;&#32553;&#65292;&#25105;&#20204;&#36890;&#36807;&#33521;&#35821;&#30340;&#21382;&#21490;&#30740;&#31350;&#21457;&#29616;&#65292;&#20154;&#20204;&#22312;&#35821;&#35328;&#20013;&#23454;&#38469;&#20351;&#29992;&#30340;&#35789;&#27719;&#25968;&#37327;&#27491;&#22312;&#32553;&#20943;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zipf's law of abbreviation, the tendency of more frequent words to be shorter, is one of the most solid candidates for a linguistic universal, in the sense that it has the potential for being exceptionless or with a number of exceptions that is vanishingly small compared to the number of languages on Earth. Since Zipf's pioneering research, this law has been viewed as a manifestation of a universal principle of communication, i.e. the minimization of word lengths, to reduce the effort of communication. Here we revisit the concordance of written language with the law of abbreviation. Crucially, we provide wider evidence that the law holds also in speech (when word length is measured in time), in particular in 46 languages from 14 linguistic families. Agreement with the law of abbreviation provides indirect evidence of compression of languages via the theoretical argument that the law of abbreviation is a prediction of optimal coding. Motivated by the need of direct evidence of compressi
&lt;/p&gt;</description></item><item><title>CB2&#26159;&#19968;&#20010;&#29992;&#20110;&#30740;&#31350;&#22522;&#20110;&#20219;&#21153;&#30340;&#21512;&#20316;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#30340;&#24179;&#21488;&#65292;&#22312;3D&#28216;&#25103;&#29615;&#22659;&#20013;&#25552;&#20379;&#20102;&#21518;&#31471;&#26381;&#21153;&#22120;&#21644;&#21508;&#31181;&#24037;&#20855;&#21644;&#27969;&#31243;&#12290;&#23427;&#22312;&#21487;&#25193;&#23637;&#30340;&#30740;&#31350;&#20013;&#23637;&#31034;&#20102;&#23398;&#20064;&#30340;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.08127</link><description>&lt;p&gt;
CB2&#65306;&#21512;&#20316;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#30740;&#31350;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
CB2: Collaborative Natural Language Interaction Research Platform. (arXiv:2303.08127v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08127
&lt;/p&gt;
&lt;p&gt;
CB2&#26159;&#19968;&#20010;&#29992;&#20110;&#30740;&#31350;&#22522;&#20110;&#20219;&#21153;&#30340;&#21512;&#20316;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#30340;&#24179;&#21488;&#65292;&#22312;3D&#28216;&#25103;&#29615;&#22659;&#20013;&#25552;&#20379;&#20102;&#21518;&#31471;&#26381;&#21153;&#22120;&#21644;&#21508;&#31181;&#24037;&#20855;&#21644;&#27969;&#31243;&#12290;&#23427;&#22312;&#21487;&#25193;&#23637;&#30340;&#30740;&#31350;&#20013;&#23637;&#31034;&#20102;&#23398;&#20064;&#30340;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CB2 &#26159;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#24179;&#21488;&#65292;&#29992;&#20110;&#30740;&#31350;&#22522;&#20110;&#20219;&#21153;&#30340;&#24773;&#22659;&#19979;&#30340;&#21512;&#20316;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#12290;&#23427;&#21253;&#25324;&#19968;&#20010; 3D &#28216;&#25103;&#29615;&#22659;&#12289;&#19968;&#20010;&#21518;&#31471;&#26381;&#21153;&#22120;&#65292;&#21487;&#20026;&#20154;&#31867;&#26234;&#33021;&#20307;&#25552;&#20379;&#35757;&#32451;&#27169;&#22411;&#65292;&#20197;&#21450;&#21508;&#31181;&#24037;&#20855;&#21644;&#27969;&#31243;&#65292;&#20197;&#23454;&#29616;&#21487;&#25193;&#23637;&#24615;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#22312; https://cb2.ai &#19978;&#23637;&#31034;&#20102;&#19968;&#20010;&#20855;&#26377;&#23398;&#20064;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#30340;&#31995;&#32479;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
CB2 is a multi-agent platform to study collaborative natural language interaction in a grounded task-oriented scenario. It includes a 3D game environment, a backend server designed to serve trained models to human agents, and various tools and processes to enable scalable studies. We deploy CB2 at https://cb2.ai as a system demonstration with a learned instruction following model.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#21160;&#24577;&#25552;&#31034;&#65288;DP&#65289;&#35843;&#25972;&#31574;&#30053;&#29992;&#20110;&#20248;&#21270;&#25552;&#31034;&#35843;&#25972;&#30340;&#24615;&#33021;&#65292;&#35813;&#31574;&#30053;&#21487;&#20197;&#21160;&#24577;&#22320;&#30830;&#23450;&#19981;&#21516;&#30340;&#25552;&#31034;&#21464;&#37327;&#26469;&#25429;&#33719;&#39069;&#22806;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2303.02909</link><description>&lt;p&gt;
&#21160;&#24577;&#25552;&#31034;&#65306;&#29992;&#20110;&#25552;&#31034;&#35843;&#25972;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Dynamic Prompting: A Unified Framework for Prompt Tuning. (arXiv:2303.02909v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02909
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#21160;&#24577;&#25552;&#31034;&#65288;DP&#65289;&#35843;&#25972;&#31574;&#30053;&#29992;&#20110;&#20248;&#21270;&#25552;&#31034;&#35843;&#25972;&#30340;&#24615;&#33021;&#65292;&#35813;&#31574;&#30053;&#21487;&#20197;&#21160;&#24577;&#22320;&#30830;&#23450;&#19981;&#21516;&#30340;&#25552;&#31034;&#21464;&#37327;&#26469;&#25429;&#33719;&#39069;&#22806;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#35777;&#26126;&#65292;&#25552;&#31034;&#35843;&#25972;&#25216;&#26415;&#21487;&#20197;&#39640;&#25928;&#22320;&#20174;&#22522;&#30784;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#21253;&#25324;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#12289;&#39044;&#35757;&#32451;&#35270;&#35273;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411; (V-L)&#12290;&#28982;&#32780;&#65292;&#37319;&#29992;&#22266;&#23450;&#30340;&#36719;&#25552;&#31034;&#26469;&#19982;&#25152;&#26377;&#23454;&#20363;&#36830;&#25509;&#36755;&#20837;&#65292;&#32780;&#24573;&#30053;&#23427;&#20204;&#30340;&#22266;&#26377;&#24046;&#24322;&#65292;&#20854;&#26377;&#25928;&#24615;&#20173;&#19981;&#30830;&#23450;&#12290;&#20363;&#22914;&#25552;&#31034;&#30340;&#20301;&#32622;&#12289;&#38271;&#24230;&#21644;&#34920;&#31034;&#22312;&#19981;&#21516;&#23454;&#20363;&#21644;&#20219;&#21153;&#20013;&#30340;&#19981;&#21516;&#21464;&#37327;&#65292;&#21487;&#20197;&#26174;&#33879;&#24433;&#21709;&#25552;&#31034;&#35843;&#25972;&#30340;&#24615;&#33021;&#12290;&#22312;&#27492;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#20998;&#26512;&#12290;&#36825;&#20010;&#20998;&#26512;&#21457;&#29616;&#65292;&#20248;&#21270;&#25552;&#31034;&#30340;&#20301;&#32622;&#21487;&#20197;&#25429;&#33719;&#20256;&#32479;&#21069;&#32512;&#25110;&#21518;&#32512;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#26080;&#27861;&#25429;&#33719;&#30340;&#39069;&#22806;&#35821;&#20041;&#20449;&#24687;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#21160;&#24577;&#25552;&#31034; (DP) &#35843;&#25972;&#31574;&#30053;&#65292;&#21487;&#20197;&#21160;&#24577;&#22320;&#30830;&#23450;&#19981;&#21516;&#30340;&#25552;&#31034;&#21464;&#37327;&#65292;&#20197;&#20248;&#21270;&#25552;&#31034;&#35843;&#25972;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has been demonstrated that the art of prompt tuning is highly effective in efficiently extracting knowledge from pretrained foundation models, encompassing pretrained language models (PLMs), vision pretrained models, and vision-language (V-L) models. However, the efficacy of employing fixed soft prompts with a predetermined position for concatenation with inputs for all instances, irrespective of their inherent disparities, remains uncertain. Variables such as the position, length, and representations of prompts across diverse instances and tasks can substantially influence the performance of prompt tuning. In this context, we provide a theoretical analysis, which reveals that optimizing the position of the prompt to encompass the input can capture additional semantic information that traditional prefix or postfix prompt tuning methods fail to capture. Building upon our analysis, we present a unified dynamic prompt (DP) tuning strategy that dynamically determines different factors o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Inseq&#65292;&#36825;&#26159;&#19968;&#20010;Python&#24037;&#20855;&#21253;&#65292;&#26088;&#22312;&#25512;&#24191;&#21487;&#35299;&#37322;&#24615;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#30340;&#20998;&#26512;&#12290;&#23427;&#20026;&#24120;&#35265;&#30340;&#35299;&#30721;&#22120;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;Transformers&#26550;&#26500;&#25552;&#20379;&#20102;&#25552;&#21462;&#27169;&#22411;&#20869;&#37096;&#20449;&#24687;&#21644;&#29305;&#24449;&#37325;&#35201;&#24615;&#24471;&#20998;&#30340;&#30452;&#35266;&#20248;&#21270;&#26041;&#27861;&#12290;&#20316;&#32773;&#36824;&#22312;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#21644;GPT-2&#20013;&#23637;&#31034;&#20102;Inseq&#30340;&#28508;&#21147;&#65292;&#35777;&#26126;&#20854;&#26377;&#21161;&#20110;&#25512;&#21160;&#21487;&#35299;&#37322;&#24615;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#26410;&#26469;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2302.13942</link><description>&lt;p&gt;
Inseq&#65306;&#19968;&#20010;&#29992;&#20110;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
Inseq: An Interpretability Toolkit for Sequence Generation Models. (arXiv:2302.13942v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13942
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Inseq&#65292;&#36825;&#26159;&#19968;&#20010;Python&#24037;&#20855;&#21253;&#65292;&#26088;&#22312;&#25512;&#24191;&#21487;&#35299;&#37322;&#24615;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#30340;&#20998;&#26512;&#12290;&#23427;&#20026;&#24120;&#35265;&#30340;&#35299;&#30721;&#22120;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;Transformers&#26550;&#26500;&#25552;&#20379;&#20102;&#25552;&#21462;&#27169;&#22411;&#20869;&#37096;&#20449;&#24687;&#21644;&#29305;&#24449;&#37325;&#35201;&#24615;&#24471;&#20998;&#30340;&#30452;&#35266;&#20248;&#21270;&#26041;&#27861;&#12290;&#20316;&#32773;&#36824;&#22312;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#21644;GPT-2&#20013;&#23637;&#31034;&#20102;Inseq&#30340;&#28508;&#21147;&#65292;&#35777;&#26126;&#20854;&#26377;&#21161;&#20110;&#25512;&#21160;&#21487;&#35299;&#37322;&#24615;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#26410;&#26469;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#36807;&#21435;&#30340;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#27969;&#34892;&#30340;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#32780;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#24448;&#24448;&#34987;&#24573;&#35270;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#32570;&#20047;&#19987;&#38376;&#30340;&#24037;&#20855;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Inseq&#65292;&#19968;&#20010;Python&#24211;&#65292;&#29992;&#20110;&#20351;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#20998;&#26512;&#26222;&#21450;&#21270;&#12290;Inseq&#33021;&#22815;&#30452;&#35266;&#19988;&#20248;&#21270;&#22320;&#25552;&#21462;&#27969;&#34892;&#30340;&#20165;&#35299;&#30721;&#22120;&#21644;&#32534;&#30721;&#22120;&#35299;&#30721;&#22120;Transformers&#26550;&#26500;&#30340;&#27169;&#22411;&#20869;&#37096;&#20449;&#24687;&#21644;&#29305;&#24449;&#37325;&#35201;&#24615;&#20998;&#25968;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#23427;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#20351;&#29992;&#23427;&#26469;&#31361;&#20986;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#24182;&#22312;GPT-2&#20013;&#23450;&#20301;&#20107;&#23454;&#30693;&#35782;&#12290;&#30001;&#20110;&#20854;&#25903;&#25345;&#23545;&#27604;&#29305;&#24449;&#24402;&#22240;&#31561;&#21069;&#27839;&#25216;&#26415;&#30340;&#21487;&#25193;&#23637;&#25509;&#21475;&#65292;&#22240;&#27492;Inseq&#21487;&#20197;&#25512;&#21160;&#21487;&#35299;&#37322;&#24615;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#26410;&#26469;&#21457;&#23637;&#65292;&#38598;&#20013;&#20248;&#33391;&#23454;&#36341;&#65292;&#24182;&#23454;&#29616;&#20844;&#27491;&#21644;&#21487;&#37325;&#22797;&#30340;&#27169;&#22411;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Past work in natural language processing interpretability focused mainly on popular classification tasks while largely overlooking generation settings, partly due to a lack of dedicated tools. In this work, we introduce Inseq, a Python library to democratize access to interpretability analyses of sequence generation models. Inseq enables intuitive and optimized extraction of models' internal information and feature importance scores for popular decoder-only and encoder-decoder Transformers architectures. We showcase its potential by adopting it to highlight gender biases in machine translation models and locate factual knowledge inside GPT-2. Thanks to its extensible interface supporting cutting-edge techniques such as contrastive feature attribution, Inseq can drive future advances in explainable natural language generation, centralizing good practices and enabling fair and reproducible model evaluations.
&lt;/p&gt;</description></item><item><title>SanskritShala&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;&#22522;&#20110;Web&#30028;&#38754;&#21644;&#20114;&#21160;&#25968;&#25454;&#27880;&#37322;&#21151;&#33021;&#30340;&#31070;&#32463;&#26805;&#25991;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24037;&#20855;&#21253;&#65292;&#20026;&#35789;&#35821;&#20998;&#21106;&#12289;&#24418;&#24577;&#26631;&#35760;&#12289;&#20381;&#36182;&#35299;&#26512;&#21644;&#22797;&#21512;&#22411;&#35782;&#21035;&#31561;&#20219;&#21153;&#25552;&#20379;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.09527</link><description>&lt;p&gt;
SanskritShala&#65306;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26805;&#25991;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24037;&#20855;&#21253;&#65292;&#24102;&#26377;&#22522;&#20110;Web&#30340;&#30028;&#38754;&#65292;&#29992;&#20110;&#25945;&#23398;&#21644;&#27880;&#37322;&#30446;&#30340;
&lt;/p&gt;
&lt;p&gt;
SanskritShala: A Neural Sanskrit NLP Toolkit with Web-Based Interface for Pedagogical and Annotation Purposes. (arXiv:2302.09527v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09527
&lt;/p&gt;
&lt;p&gt;
SanskritShala&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;&#22522;&#20110;Web&#30028;&#38754;&#21644;&#20114;&#21160;&#25968;&#25454;&#27880;&#37322;&#21151;&#33021;&#30340;&#31070;&#32463;&#26805;&#25991;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24037;&#20855;&#21253;&#65292;&#20026;&#35789;&#35821;&#20998;&#21106;&#12289;&#24418;&#24577;&#26631;&#35760;&#12289;&#20381;&#36182;&#35299;&#26512;&#21644;&#22797;&#21512;&#22411;&#35782;&#21035;&#31561;&#20219;&#21153;&#25552;&#20379;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SanskritShala&#30340;&#31070;&#32463;&#32593;&#32476;&#26805;&#25991;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24037;&#20855;&#21253;&#65292;&#26088;&#22312;&#20026;&#35789;&#35821;&#20998;&#21106;&#12289;&#24418;&#24577;&#26631;&#35760;&#12289;&#20381;&#36182;&#35299;&#26512;&#21644;&#22797;&#21512;&#31867;&#22411;&#35782;&#21035;&#31561;&#22810;&#39033;&#20219;&#21153;&#25552;&#20379;&#35745;&#31639;&#35821;&#35328;&#23398;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#30446;&#21069;&#22312;&#25152;&#26377;&#20219;&#21153;&#21487;&#29992;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#25253;&#21578;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;SanskritShala&#26159;&#19968;&#20010;&#22522;&#20110;Web&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#20801;&#35768;&#29992;&#25143;&#20026;&#32473;&#23450;&#30340;&#36755;&#20837;&#33719;&#21462;&#23454;&#26102;&#20998;&#26512;&#12290;&#23427;&#20855;&#26377;&#26131;&#20110;&#20351;&#29992;&#30340;&#20132;&#20114;&#24335;&#25968;&#25454;&#27880;&#37322;&#21151;&#33021;&#65292;&#20801;&#35768;&#27880;&#37322;&#32773;&#22312;&#31995;&#32479;&#38169;&#35823;&#26102;&#32416;&#27491;&#31995;&#32479;&#39044;&#27979;&#12290;&#25105;&#20204;&#20844;&#24320;&#21457;&#24067;&#20102;&#21253;&#21547;&#24037;&#20855;&#21253;&#20013;&#30340;4&#20010;&#27169;&#22359;&#12289;7&#20010;&#24050;&#32463;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;&#26805;&#25991;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;&#35789;&#23884;&#20837;&#27169;&#22411;&#20197;&#21450;&#22810;&#20010;&#32463;&#36807;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65288;&#22914;&#21333;&#35789;&#30456;&#20284;&#24615;&#12289;&#30456;&#20851;&#24615;&#12289;&#31867;&#21035;&#21270;&#12289;&#27604;&#21947;&#39044;&#27979;&#65289;&#65292;&#20197;&#35780;&#20272;&#35789;&#23884;&#20837;&#30340;&#20869;&#22312;&#23646;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21253;&#21547;&#22522;&#20110;Web&#30028;&#38754;&#21644;&#20132;&#20114;&#24335;&#25968;&#25454;&#27880;&#37322;&#21151;&#33021;&#30340;&#31070;&#32463;&#26805;&#25991;NLP&#24037;&#20855;&#21253;&#65292;&#20026;&#35789;&#35821;&#20998;&#21106;&#12289;&#24418;&#24577;&#26631;&#35760;&#12289;&#20381;&#36182;&#35299;&#26512;&#21644;&#22797;&#21512;&#31867;&#22411;&#35782;&#21035;&#31561;&#22810;&#39033;&#20219;&#21153;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a neural Sanskrit Natural Language Processing (NLP) toolkit named SanskritShala (a school of Sanskrit) to facilitate computational linguistic analyses for several tasks such as word segmentation, morphological tagging, dependency parsing, and compound type identification. Our systems currently report state-of-the-art performance on available benchmark datasets for all tasks. SanskritShala is deployed as a web-based application, which allows a user to get real-time analysis for the given input. It is built with easy-to-use interactive data annotation features that allow annotators to correct the system predictions when it makes mistakes. We publicly release the source codes of the 4 modules included in the toolkit, 7 word embedding models that have been trained on publicly available Sanskrit corpora and multiple annotated datasets such as word similarity, relatedness, categorization, analogy prediction to assess intrinsic properties of word embeddings. So far as we know, this
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#30340;KG&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#31070;&#32463;&#32593;&#32476;&#22312;KG&#20013;&#25191;&#34892;&#22797;&#26434;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#20010;&#24402;&#32435;KG&#23436;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;Hits@10&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#22312;KG&#20013;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2302.01313</link><description>&lt;p&gt;
&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Double Permutation Equivariance for Knowledge Graph Completion. (arXiv:2302.01313v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#30340;KG&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#31070;&#32463;&#32593;&#32476;&#22312;KG&#20013;&#25191;&#34892;&#22797;&#26434;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#20010;&#24402;&#32435;KG&#23436;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;Hits@10&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#22312;KG&#20013;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#30693;&#35782;&#22270;&#35889;(KGs)&#24418;&#24335;&#21270;&#20026;&#19968;&#31181;&#26032;&#22411;&#30340;&#22270;&#65292;&#24182;&#31216;&#20043;&#20026;&#21452;&#20132;&#25442;&#23646;&#24615;&#22270;&#65292;&#20854;&#20013;&#33410;&#28857;&#21644;&#20108;&#20803;&#65288;&#20004;&#20010;&#33410;&#28857;&#20043;&#38388;&#30340;&#65289;&#34920;&#31034;&#24517;&#39035;&#23545;&#33410;&#28857;&#21495;&#21644;&#36793;&#65288;&#21450;&#33410;&#28857;&#65289;&#23646;&#24615;&#65288;&#20851;&#31995;&#21644;&#33410;&#28857;&#29305;&#24449;&#65289;&#30340;&#25490;&#21015;&#31561;&#21464;&#12290;&#21452;&#37325;&#25490;&#21015;&#31561;&#21464;&#30340;KG&#34920;&#31034;&#22312;KG&#20013;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#31561;&#21464;&#24615;&#23545;&#20851;&#31995;&#30340;&#32467;&#26500;&#34920;&#31034;&#20135;&#29983;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#20351;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#22312;KG&#20013;&#25191;&#34892;&#22797;&#26434;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#31561;&#21464;&#34920;&#31034;&#34013;&#22270;&#65292;&#24182;&#27979;&#35797;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#22522;&#20110;GNN&#30340;&#21452;&#25490;&#21015;&#31561;&#21464;&#31070;&#32463;&#32467;&#26500;&#65292;&#22312;WN18RR&#12289;FB237&#21644;NELL995&#24402;&#32435;KG&#23436;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;Hits@10&#27979;&#35797;&#20934;&#30830;&#29575;&#65292;&#24182;&#33021;&#22815;&#20934;&#30830;&#25191;&#34892;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#25191;&#34892;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work provides a formalization of Knowledge Graphs (KGs) as a new class of graphs that we denote doubly exchangeable attributed graphs, where node and pairwise (joint 2-node) representations must be equivariant to permutations of both node ids and edge (&amp; node) attributes (relations &amp; node features). Double-permutation equivariant KG representations open a new research direction in KGs. We show that this equivariance imposes a structural representation of relations that allows neural networks to perform complex logical reasoning tasks in KGs. Finally, we introduce a general blueprint for such equivariant representations and test a simple GNN-based double-permutation equivariant neural architecture that achieve state-of-the-art Hits@10 test accuracy in the WN18RR, FB237 and NELL995 inductive KG completion tasks, and can accurately perform logical reasoning tasks that no existing methods can perform, to the best of our knowledge.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#33976;&#39311;&#25216;&#26415;&#65292;&#22312;&#22768;&#23398;&#21644;&#35821;&#35328;&#32423;&#21035;&#19978;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#22522;&#20110;CIF&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#65292;&#30456;&#36739;&#21407;&#22987;&#27169;&#22411;&#65292;&#22312;AISHELL-1&#21644;LibriSpeech&#25968;&#25454;&#38598;&#19978;&#20998;&#21035;&#23454;&#29616;&#20102;15%&#21644;9%&#30340;&#30456;&#23545;&#35823;&#24046;&#29575;&#38477;&#20302;&#12290;</title><link>http://arxiv.org/abs/2301.13003</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#23618;&#33976;&#39311;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#22522;&#20110;CIF&#30340;&#35821;&#38899;&#35782;&#21035;&#22120;
&lt;/p&gt;
&lt;p&gt;
Knowledge Transfer from Pre-trained Language Models to Cif-based Speech Recognizers via Hierarchical Distillation. (arXiv:2301.13003v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#33976;&#39311;&#25216;&#26415;&#65292;&#22312;&#22768;&#23398;&#21644;&#35821;&#35328;&#32423;&#21035;&#19978;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#22522;&#20110;CIF&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#65292;&#30456;&#36739;&#21407;&#22987;&#27169;&#22411;&#65292;&#22312;AISHELL-1&#21644;LibriSpeech&#25968;&#25454;&#38598;&#19978;&#20998;&#21035;&#23454;&#29616;&#20102;15%&#21644;9%&#30340;&#30456;&#23545;&#35823;&#24046;&#29575;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#21033;&#29992;PLMs&#26469;&#22686;&#24378;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#20063;&#25104;&#20026;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#21463;&#21040;PLMs&#32467;&#26500;&#19981;&#28789;&#27963;&#21644;PLMs&#21033;&#29992;&#19981;&#20805;&#20998;&#31561;&#38382;&#39064;&#38480;&#21046;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#36830;&#32493;&#31215;&#20998;&#21644;&#28779;&#28798;&#65288;CIF&#65289;&#22522;&#30784;&#19978;&#29992;&#23618;&#27425;&#21270;&#30693;&#35782;&#33976;&#39311;&#65288;HKD&#65289;&#12290;&#20026;&#20102;&#23558;PLMs&#30340;&#30693;&#35782;&#36716;&#31227;&#33267;ASR&#27169;&#22411;&#65292;HKD&#20351;&#29992;&#20132;&#21449;&#27169;&#24577;&#30693;&#35782;&#33976;&#39311;&#21644;&#22768;&#23398;&#32423;&#21035;&#23545;&#27604;&#25439;&#22833;&#20197;&#21450;&#35821;&#35328;&#32423;&#21035;&#30340;&#30693;&#35782;&#33976;&#39311;&#21644;&#22238;&#24402;&#25439;&#22833;&#12290;&#19982;&#21407;&#22987;&#30340;CIF&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;AISHELL-1&#21644;LibriSpeech&#25968;&#25454;&#38598;&#19978;&#20998;&#21035;&#23454;&#29616;&#20102;15&#65285;&#21644;9&#65285;&#30340;&#30456;&#23545;&#35823;&#24046;&#29575;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale pre-trained language models (PLMs) have shown great potential in natural language processing tasks. Leveraging the capabilities of PLMs to enhance automatic speech recognition (ASR) systems has also emerged as a promising research direction. However, previous works may be limited by the inflexible structures of PLMs and the insufficient utilization of PLMs. To alleviate these problems, we propose the hierarchical knowledge distillation (HKD) on the continuous integrate-and-fire (CIF) based ASR models. To transfer knowledge from PLMs to the ASR models, HKD employs cross-modal knowledge distillation with contrastive loss at the acoustic level and knowledge distillation with regression loss at the linguistic level. Compared with the original CIF-based model, our method achieves 15% and 9% relative error rate reduction on the AISHELL-1 and LibriSpeech datasets, respectively.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#23545;OpenAI&#30340;ChatGPT&#36827;&#34892;&#36234;&#29425;&#25216;&#26415;&#23454;&#39564;&#30340;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;&#23427;&#30340;&#21487;&#38752;&#24615;&#12289;&#40065;&#26834;&#24615;&#12289;&#20559;&#35265;&#21644;&#27602;&#24615;&#31561;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;ChatGPT&#23384;&#22312;&#31181;&#26063;&#12289;&#24615;&#21035;&#21644;&#23447;&#25945;&#31561;&#30456;&#20851;&#20559;&#35265;&#65292;&#24182;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#22240;&#27492;&#24314;&#35758;&#22312;&#24320;&#21457;&#21644;&#37096;&#32626;LLMs&#26102;&#24212;&#32771;&#34385;&#36879;&#26126;&#24230;&#21644;&#38382;&#36131;&#21046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.12867</link><description>&lt;p&gt;
&#36890;&#36807;&#36234;&#29425;&#25216;&#26415;&#36827;&#34892;&#32418;&#38431;&#28436;&#32451;&#65306;ChatGPT &#30340;&#20559;&#35265;&#65292;&#40065;&#26834;&#24615;&#65292;&#21487;&#38752;&#24615;&#21644;&#27602;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Red teaming ChatGPT via Jailbreaking: Bias, Robustness, Reliability and Toxicity. (arXiv:2301.12867v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12867
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#23545;OpenAI&#30340;ChatGPT&#36827;&#34892;&#36234;&#29425;&#25216;&#26415;&#23454;&#39564;&#30340;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;&#23427;&#30340;&#21487;&#38752;&#24615;&#12289;&#40065;&#26834;&#24615;&#12289;&#20559;&#35265;&#21644;&#27602;&#24615;&#31561;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;ChatGPT&#23384;&#22312;&#31181;&#26063;&#12289;&#24615;&#21035;&#21644;&#23447;&#25945;&#31561;&#30456;&#20851;&#20559;&#35265;&#65292;&#24182;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#22240;&#27492;&#24314;&#35758;&#22312;&#24320;&#21457;&#21644;&#37096;&#32626;LLMs&#26102;&#24212;&#32771;&#34385;&#36879;&#26126;&#24230;&#21644;&#38382;&#36131;&#21046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#30340;&#31361;&#30772;&#65292;&#20351;&#24471;&#33021;&#22815;&#20197;&#24320;&#25918;&#30340;&#26041;&#24335;&#21512;&#25104;&#21644;&#29702;&#35299;&#36830;&#36143;&#30340;&#25991;&#26412;&#65292;&#23558;&#29702;&#35770;&#31639;&#27861;&#36716;&#21270;&#20026;&#23454;&#38469;&#24212;&#29992;&#25104;&#20026;&#21487;&#33021;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#26174;&#33879;&#24433;&#21709;&#20102;&#25253;&#21578;&#25688;&#35201;&#36719;&#20214;&#21644;&#25776;&#31295;&#20154;&#31561;&#19994;&#21153;&#12290;&#20294;&#35266;&#23519;&#34920;&#26126;&#65292;LLMs&#21487;&#33021;&#34920;&#29616;&#20986;&#31038;&#20250;&#20559;&#35265;&#21644;&#27602;&#24615;&#65292;&#20174;&#32780;&#36896;&#25104;&#19981;&#36127;&#36131;&#20219;&#30340;&#36947;&#24503;&#21644;&#31038;&#20250;&#21361;&#38505;&#21518;&#26524;&#12290;&#22240;&#27492;&#65292;&#24212;&#35813;&#24320;&#21457;&#36127;&#36131;&#20219;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#26469;&#30830;&#20445;LLMs&#30340;&#38382;&#36131;&#12290;&#23613;&#31649;&#26377;&#20960;&#39033;&#23454;&#35777;&#35843;&#26597;&#25581;&#31034;&#20102;&#29616;&#20195;LLMs&#30340;&#19968;&#20123;&#20262;&#29702;&#22256;&#38590;&#23384;&#22312;&#65292;&#20294;&#26159;&#23545;&#24403;&#21069;LLMs&#20351;&#29992;&#30340;&#39118;&#38505;&#21644;&#26377;&#23475;&#34892;&#20026;&#30340;&#31995;&#32479;&#24615;&#35843;&#26597;&#21644;&#29992;&#25143;&#30740;&#31350;&#20173;&#28982;&#24456;&#23569;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#25945;&#32946;&#26410;&#26469;&#24314;&#35774;&#36127;&#36131;&#20219;&#30340;LLMs&#65292;&#25105;&#20204;&#36890;&#36807;&#36234;&#29425;&#25216;&#26415;&#23545;OpenAI&#30340;ChatGPT&#36827;&#34892;&#20102;&#19968;&#31181;&#36136;&#24615;&#30740;&#31350;&#26041;&#27861;&#31216;&#20026;&#8220;&#32418;&#38431;&#28436;&#32451;&#8221;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35843;&#26597;&#20102;ChatGPT&#30340;&#21487;&#38752;&#24615;&#65292;&#40065;&#26834;&#24615;&#65292;&#20559;&#35265;&#21644;&#27602;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;ChatGPT&#23384;&#22312;&#19982;&#24615;&#21035;&#65292;&#31181;&#26063;&#21644;&#23447;&#25945;&#26377;&#20851;&#30340;&#20559;&#35265;&#65292;&#24182;&#19988;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#24378;&#35843;&#20102;&#24320;&#21457;&#21644;&#37096;&#32626;LLMs&#30340;&#36879;&#26126;&#24230;&#21644;&#38382;&#36131;&#21046;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent breakthroughs in natural language processing (NLP) have permitted the synthesis and comprehension of coherent text in an open-ended way, therefore translating the theoretical algorithms into practical applications. The large language models (LLMs) have significantly impacted businesses such as report summarization software and copywriters. Observations indicate, however, that LLMs may exhibit social prejudice and toxicity, posing ethical and societal dangers of consequences resulting from irresponsibility. Large-scale benchmarks for accountable LLMs should consequently be developed. Although several empirical investigations reveal the existence of a few ethical difficulties in advanced LLMs, there is little systematic examination and user study of the risks and harmful behaviors of current LLM usage. To further educate future efforts on constructing ethical LLMs responsibly, we perform a qualitative research method called ``red teaming'' on OpenAI's ChatGPT\footnote{In this pape
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#20165;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#38646;&#26679;&#26412;&#22810;&#35821;&#35328;TTS&#65292;&#20801;&#35768;&#24320;&#21457;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;TTS&#31995;&#32479;&#65292;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#20855;&#26377;&#39640;&#24230;&#21487;&#29702;&#35299;&#30340;&#38646;&#26679;&#26412;TTS&#12290;</title><link>http://arxiv.org/abs/2301.12596</link><description>&lt;p&gt;
&#20174;&#25991;&#26412;&#23398;&#20250;&#35821;&#35328;&#65306;&#20351;&#29992;&#26080;&#30417;&#30563;&#25991;&#26412;&#39044;&#35757;&#32451;&#30340;&#38646;&#26679;&#26412;&#22810;&#35821;&#35328;&#25991;&#26412;&#36716;&#35821;&#38899;
&lt;/p&gt;
&lt;p&gt;
Learning to Speak from Text: Zero-Shot Multilingual Text-to-Speech with Unsupervised Text Pretraining. (arXiv:2301.12596v3 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12596
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#20165;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#38646;&#26679;&#26412;&#22810;&#35821;&#35328;TTS&#65292;&#20801;&#35768;&#24320;&#21457;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;TTS&#31995;&#32479;&#65292;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#20855;&#26377;&#39640;&#24230;&#21487;&#29702;&#35299;&#30340;&#38646;&#26679;&#26412;TTS&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31070;&#32463;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#65288;TTS&#65289;&#24050;&#32463;&#23454;&#29616;&#20102;&#31867;&#20154;&#30340;&#33258;&#28982;&#21512;&#25104;&#35821;&#38899;&#65292;&#20294;&#22810;&#35821;&#35328;TTS&#31995;&#32479;&#30001;&#20110;&#38656;&#35201;&#37197;&#23545;&#30340;&#25991;&#26412;&#21644;&#24037;&#20316;&#23460;&#36136;&#37327;&#30340;&#38899;&#39057;&#25968;&#25454;&#65292;&#20165;&#38480;&#20110;&#36164;&#28304;&#20016;&#23500;&#30340;&#35821;&#35328;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30446;&#26631;&#35821;&#35328;&#30340;&#20165;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#38646;&#26679;&#26412;&#22810;&#35821;&#35328;TTS&#30340;&#26041;&#27861;&#12290;&#20351;&#29992;&#20165;&#25991;&#26412;&#25968;&#25454;&#20801;&#35768;&#24320;&#21457;&#20165;&#23384;&#22312;&#25991;&#26412;&#36164;&#28304;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;TTS&#31995;&#32479;&#65292;&#20351;TTS&#21487;&#20197;&#25903;&#25345;&#25968;&#21315;&#31181;&#35821;&#35328;&#12290;&#21463;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#24378;&#22823;&#30340;&#36328;&#35821;&#35328;&#21487;&#36716;&#31227;&#24615;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#39318;&#20808;&#23545;&#22810;&#35821;&#35328;&#32431;&#25991;&#26412;&#25968;&#25454;&#25191;&#34892;&#25513;&#34109;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#12290;&#28982;&#21518;&#25105;&#20204;&#29992;&#25104;&#23545;&#25968;&#25454;&#36229;&#27169;&#24335;&#35757;&#32451;&#35813;&#27169;&#22411;&#65292;&#22312;&#20923;&#32467;&#35821;&#35328;&#24863;&#30693;&#23884;&#20837;&#23618;&#30340;&#21516;&#26102;&#12290;&#36825;&#20801;&#35768;&#23545;&#26410;&#21253;&#21547;&#22312;&#37197;&#23545;&#25968;&#25454;&#20013;&#20294;&#20986;&#29616;&#22312;&#20165;&#25991;&#26412;&#25968;&#25454;&#20013;&#30340;&#35821;&#35328;&#36827;&#34892;&#25512;&#29702;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#20855;&#26377;&#39640;&#24230;&#21487;&#29702;&#35299;&#30340;&#38646;&#26679;&#26412;TTS&#65292;&#20854;&#20013;&#23383;&#31526;&#38169;&#35823;&#29575;&#20302;&#20110;0.3&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
While neural text-to-speech (TTS) has achieved human-like natural synthetic speech, multilingual TTS systems are limited to resource-rich languages due to the need for paired text and studio-quality audio data. This paper proposes a method for zero-shot multilingual TTS using text-only data for the target language. The use of text-only data allows the development of TTS systems for low-resource languages for which only textual resources are available, making TTS accessible to thousands of languages. Inspired by the strong cross-lingual transferability of multilingual language models, our framework first performs masked language model pretraining with multilingual text-only data. Then we train this model with a paired data in a supervised manner, while freezing a language-aware embedding layer. This allows inference even for languages not included in the paired data but present in the text-only data. Evaluation results demonstrate highly intelligible zero-shot TTS with a character error
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#31038;&#20132;&#23186;&#20307;&#30340;&#20027;&#39064;&#39537;&#21160;&#30340;&#20851;&#38190;&#35789;&#25552;&#21462;&#26694;&#26550;&#65292;&#26088;&#22312;&#20174;&#29992;&#25143;&#29983;&#25104;&#30340;&#20581;&#24247;&#25991;&#26412;&#20013;&#25429;&#33719;&#20020;&#24202;&#30456;&#20851;&#30340;&#20851;&#38190;&#35789;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#38450;&#27835;&#38463;&#29255;&#31867;&#29289;&#36136;&#20351;&#29992;&#38556;&#30861;&#30340;&#29992;&#20363;&#20013;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2301.11508</link><description>&lt;p&gt;
&#20027;&#39064;&#39537;&#21160;&#30340;&#20851;&#38190;&#35789;&#25552;&#21462;&#20197;&#20998;&#26512;&#31038;&#20132;&#23186;&#20307;&#35805;&#35821;
&lt;/p&gt;
&lt;p&gt;
Theme-driven Keyphrase Extraction to Analyze Social Media Discourse. (arXiv:2301.11508v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#31038;&#20132;&#23186;&#20307;&#30340;&#20027;&#39064;&#39537;&#21160;&#30340;&#20851;&#38190;&#35789;&#25552;&#21462;&#26694;&#26550;&#65292;&#26088;&#22312;&#20174;&#29992;&#25143;&#29983;&#25104;&#30340;&#20581;&#24247;&#25991;&#26412;&#20013;&#25429;&#33719;&#20020;&#24202;&#30456;&#20851;&#30340;&#20851;&#38190;&#35789;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#38450;&#27835;&#38463;&#29255;&#31867;&#29289;&#36136;&#20351;&#29992;&#38556;&#30861;&#30340;&#29992;&#20363;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#26159;&#20998;&#20139;&#33258;&#25105;&#20581;&#24247;&#32463;&#21382;&#30340;&#37325;&#35201;&#36164;&#28304;&#65292;&#25552;&#20379;&#21508;&#31181;&#20581;&#24247;&#35805;&#39064;&#30340;&#20016;&#23500;&#25968;&#25454;&#12290;&#23613;&#31649;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#36827;&#27493;&#20351;&#24471;&#33021;&#22815;&#23545;&#22823;&#35268;&#27169;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#65292;&#20294;&#22312;&#23558;&#20851;&#38190;&#35789;&#25552;&#21462;&#24212;&#29992;&#20110;&#20581;&#24247;&#30456;&#20851;&#20869;&#23481;&#26041;&#38754;&#20173;&#23384;&#22312;&#24046;&#36317;&#12290;&#20851;&#38190;&#35789;&#25552;&#21462;&#29992;&#20110;&#22312;&#31038;&#20132;&#23186;&#20307;&#35805;&#35821;&#20013;&#35782;&#21035;&#26174;&#33879;&#27010;&#24565;&#65292;&#32780;&#19981;&#21463;&#39044;&#23450;&#20041;&#23454;&#20307;&#31867;&#21035;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#31038;&#20132;&#23186;&#20307;&#30340;&#20027;&#39064;&#39537;&#21160;&#30340;&#20851;&#38190;&#35789;&#25552;&#21462;&#26694;&#26550;&#65292;&#36825;&#26159;&#19968;&#31181;&#24320;&#21019;&#24615;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#20174;&#29992;&#25143;&#29983;&#25104;&#30340;&#20581;&#24247;&#25991;&#26412;&#20013;&#25429;&#33719;&#20020;&#24202;&#30456;&#20851;&#30340;&#20851;&#38190;&#35789;&#12290;&#20027;&#39064;&#26159;&#30001;&#25552;&#21462;&#20219;&#21153;&#30340;&#30446;&#26631;&#30830;&#23450;&#30340;&#24191;&#27867;&#31867;&#21035;&#12290;&#25105;&#20204;&#21046;&#23450;&#20102;&#36825;&#20010;&#26032;&#39062;&#30340;&#20027;&#39064;&#39537;&#21160;&#30340;&#20851;&#38190;&#35789;&#25552;&#21462;&#20219;&#21153;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#39640;&#25928;&#25366;&#25496;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#65292;&#29992;&#20110;&#38450;&#27835;&#38463;&#29255;&#31867;&#29289;&#36136;&#20351;&#29992;&#38556;&#30861;&#30340;&#29992;&#20363;&#20013;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#21033;&#29992;&#20102;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social media platforms are vital resources for sharing self-reported health experiences, offering rich data on various health topics. Despite advancements in Natural Language Processing (NLP) enabling large-scale social media data analysis, a gap remains in applying keyphrase extraction to health-related content. Keyphrase extraction is used to identify salient concepts in social media discourse without being constrained by predefined entity classes. This paper introduces a theme-driven keyphrase extraction framework tailored for social media, a pioneering approach designed to capture clinically relevant keyphrases from user-generated health texts. Themes are defined as broad categories determined by the objectives of the extraction task. We formulate this novel task of theme-driven keyphrase extraction and demonstrate its potential for efficiently mining social media text for the use case of treatment for opioid use disorder. This paper leverages qualitative and quantitative analysis 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20219;&#21153;&#30340;&#35748;&#30693;&#33021;&#21147;&#65292;&#35774;&#35745;&#20102;&#35780;&#20272;&#26041;&#26696;&#26469;&#27604;&#36739;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#31867;&#30340;&#36825;&#20123;&#33021;&#21147;&#65292;&#36890;&#36807;&#22312;&#23548;&#33322;&#25351;&#20196;&#29983;&#25104;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#27169;&#22411;&#30340;&#35821;&#29992;&#33021;&#21147;&#20173;&#38656;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2301.05149</link><description>&lt;p&gt;
&#23450;&#20041;&#12289;&#35780;&#20272;&#21644;&#25913;&#36827;&#22522;&#20110;&#20219;&#21153;&#30340;&#35748;&#30693;&#33021;&#21147;&#23545;&#29983;&#25104;&#27169;&#22411;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Define, Evaluate, and Improve Task-Oriented Cognitive Capabilities for Instruction Generation Models. (arXiv:2301.05149v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20219;&#21153;&#30340;&#35748;&#30693;&#33021;&#21147;&#65292;&#35774;&#35745;&#20102;&#35780;&#20272;&#26041;&#26696;&#26469;&#27604;&#36739;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#31867;&#30340;&#36825;&#20123;&#33021;&#21147;&#65292;&#36890;&#36807;&#22312;&#23548;&#33322;&#25351;&#20196;&#29983;&#25104;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#27169;&#22411;&#30340;&#35821;&#29992;&#33021;&#21147;&#20173;&#38656;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#24037;&#20316;&#36890;&#36807;&#20026;&#20154;&#31867;&#35774;&#35745;&#30340;&#24515;&#29702;&#27979;&#35797;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#35748;&#30693;&#33021;&#21147;&#12290;&#23613;&#31649;&#36825;&#20123;&#30740;&#31350;&#26377;&#21161;&#20110;&#20102;&#35299;&#36825;&#20123;&#27169;&#22411;&#30340;&#19968;&#33324;&#33021;&#21147;&#65292;&#20294;&#24182;&#19981;&#33021;&#20445;&#35777;&#19968;&#20010;&#25317;&#26377;&#36275;&#22815;&#33021;&#21147;&#36890;&#36807;&#36825;&#20123;&#27979;&#35797;&#30340;&#27169;&#22411;&#23454;&#38469;&#19978;&#20250;&#22312;&#25191;&#34892;&#23454;&#38469;&#20219;&#21153;&#26102;&#20351;&#29992;&#36825;&#20123;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#22522;&#20110;&#20219;&#21153;&#30340;&#35748;&#30693;&#33021;&#21147;&#65292;&#36825;&#26159;&#19968;&#31181;&#20154;&#31867;&#24335;&#35748;&#30693;&#33021;&#21147;&#65292;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#21033;&#29992;&#36825;&#31181;&#33021;&#21147;&#26469;&#25191;&#34892;&#20219;&#21153;&#12290;&#36825;&#20123;&#33021;&#21147;&#21253;&#25324;&#65306;(i) &#24555;&#36895;&#29983;&#25104;&#33391;&#22909;&#30340;&#20505;&#36873;&#35805;&#35821;&#30340;&#33021;&#21147; (&#25628;&#32034;&#33021;&#21147;)&#65307;(ii) &#39044;&#27979;&#21548;&#32773;&#22914;&#20309;&#29702;&#35299;&#36825;&#20123;&#35805;&#35821;&#65292;&#24182;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#35805;&#35821; (&#35821;&#29992;&#33021;&#21147;)&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#35780;&#20272;&#26041;&#26696;&#65292;&#20197;&#27604;&#36739;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#30340;&#36825;&#20123;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;&#27492;&#26041;&#26696;&#24212;&#29992;&#20110;&#23548;&#33322;&#25351;&#20196;&#29983;&#25104;&#38382;&#39064;&#20013;&#30340;&#21508;&#31181;&#27169;&#22411;&#30340;&#27604;&#36739;&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#30340;&#35821;&#29992;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work studies the cognitive capabilities of language models through psychological tests designed for humans. While these studies are helpful for understanding the general capabilities of these models, there is no guarantee that a model possessing sufficient capabilities to pass those tests would actually use those capabilities in performing real-life tasks. In this work, we formulate task-oriented cognitive capabilities, which are human-like cognitive capabilities that language models leverage to perform tasks. These capabilities are (i) the ability to quickly generate good candidate utterances (the search capability) (ii) the ability to predict how a listener interprets those utterances and choose the most appropriate one (the pragmatic capability). We design an evaluation scheme for comparing these capabilities of a language model with those of a human. Applying this scheme to examine various models in a navigation instruction generation problem, we find that their pragmatic ca
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#26032;&#39062;&#35299;&#37322;&#25216;&#26415;&#65306;&#19978;&#19979;&#25991;&#38271;&#24230;&#25506;&#27979;&#65292;&#36890;&#36807;&#36319;&#36394;&#27169;&#22411;&#39044;&#27979;&#19982;&#21487;&#29992;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#20851;&#31995;&#26469;&#23545;&#19981;&#21516;&#19978;&#19979;&#25991;&#20998;&#37197;&#19981;&#21516;&#30340;&#37325;&#35201;&#24615;&#24471;&#20998;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#26377;&#21033;&#20110;&#30740;&#31350;&#36828;&#36317;&#31163;&#20381;&#36182;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.14815</link><description>&lt;p&gt;
&#36890;&#36807;&#19978;&#19979;&#25991;&#38271;&#24230;&#25506;&#31350;&#40657;&#21283;&#23376;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Black-box language model explanation by context length probing. (arXiv:2212.14815v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.14815
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#26032;&#39062;&#35299;&#37322;&#25216;&#26415;&#65306;&#19978;&#19979;&#25991;&#38271;&#24230;&#25506;&#27979;&#65292;&#36890;&#36807;&#36319;&#36394;&#27169;&#22411;&#39044;&#27979;&#19982;&#21487;&#29992;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#20851;&#31995;&#26469;&#23545;&#19981;&#21516;&#19978;&#19979;&#25991;&#20998;&#37197;&#19981;&#21516;&#30340;&#37325;&#35201;&#24615;&#24471;&#20998;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#26377;&#21033;&#20110;&#30740;&#31350;&#36828;&#36317;&#31163;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24191;&#27867;&#37319;&#29992;&#24378;&#35843;&#20102;&#25913;&#21892;&#20854;&#21487;&#35299;&#37322;&#24615;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#37322;&#25216;&#26415;&#65306;&#19978;&#19979;&#25991;&#38271;&#24230;&#25506;&#27979;&#65292;&#23427;&#22522;&#20110;&#36319;&#36394;&#27169;&#22411;&#39044;&#27979;&#20316;&#20026;&#21487;&#29992;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#20989;&#25968;&#65292;&#24182;&#20801;&#35768;&#23545;&#19981;&#21516;&#19978;&#19979;&#25991;&#20998;&#37197;&#19981;&#21516;&#30340;&#37325;&#35201;&#24615;&#24471;&#20998;&#12290;&#35813;&#25216;&#26415;&#26159;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#65292;&#19981;&#20381;&#36182;&#20110;&#38500;&#35745;&#31639;token&#32423;&#27010;&#29575;&#20043;&#22806;&#30340;&#27169;&#22411;&#20869;&#37096;&#35775;&#38382;&#12290;&#25105;&#20204;&#23558;&#19978;&#19979;&#25991;&#38271;&#24230;&#25506;&#27979;&#24212;&#29992;&#20110;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20123;&#21021;&#22987;&#30340;&#20998;&#26512;&#21644;&#35265;&#35299;&#65292;&#21253;&#25324;&#30740;&#31350;&#36828;&#36317;&#31163;&#20381;&#36182;&#24615;&#30340;&#28508;&#21147;&#12290;&#26041;&#27861;&#30340;&#28304;&#20195;&#30721;&#21644;&#20132;&#20114;&#24335;&#28436;&#31034;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasingly widespread adoption of large language models has highlighted the need for improving their explainability. We present context length probing, a novel explanation technique for causal language models, based on tracking the predictions of a model as a function of the length of available context, and allowing to assign differential importance scores to different contexts. The technique is model-agnostic and does not rely on access to model internals beyond computing token-level probabilities. We apply context length probing to large pre-trained language models and offer some initial analyses and insights, including the potential for studying long-range dependencies. The source code and an interactive demo of the method are available.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#20154;&#31867;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#36827;&#34892;&#23454;&#20307;&#36873;&#25321;&#26102;&#25152;&#38754;&#20020;&#30340;&#38388;&#25509;&#24341;&#29992;&#34920;&#36798;&#24335;&#30340;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;42K&#20010;&#23454;&#20307;&#23545;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#27169;&#22411;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2212.10933</link><description>&lt;p&gt;
&#23454;&#20307;&#36873;&#25321;&#20013;&#35299;&#20915;&#38388;&#25509;&#24341;&#29992;&#34920;&#36798;&#24335;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Resolving Indirect Referring Expressions for Entity Selection. (arXiv:2212.10933v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10933
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#20154;&#31867;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#36827;&#34892;&#23454;&#20307;&#36873;&#25321;&#26102;&#25152;&#38754;&#20020;&#30340;&#38388;&#25509;&#24341;&#29992;&#34920;&#36798;&#24335;&#30340;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;42K&#20010;&#23454;&#20307;&#23545;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#27169;&#22411;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#20351;&#24471;&#26032;&#30340;&#23545;&#35805;&#31995;&#32479;&#25104;&#20026;&#21487;&#33021;&#12290;&#24403;&#20154;&#20204;&#22312;&#20351;&#29992;&#36825;&#31181;&#31995;&#32479;&#26102;&#65292;&#36890;&#24120;&#38656;&#35201;&#22312;&#32473;&#23450;&#36873;&#39033;&#20013;&#20570;&#20986;&#36873;&#25321;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#24403;&#20154;&#20204;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#26469;&#36873;&#25321;&#23454;&#20307;&#26102;&#65292;&#22914;&#20309;&#35299;&#20915;&#20854;&#38388;&#25509;&#24341;&#29992;&#30340;&#34920;&#36798;&#24335;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#23545;&#36825;&#31181;&#35821;&#35328;&#30340;&#28145;&#24230;&#29702;&#35299;&#21487;&#33021;&#22823;&#22823;&#25913;&#21892;&#23545;&#35805;&#12289;&#25512;&#33616;&#21644;&#25628;&#32034;&#31995;&#32479;&#30340;&#33258;&#28982;&#24230;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;AltEntities&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;42K&#20010;&#23454;&#20307;&#23545;&#21644;&#34920;&#36848;&#65288;&#25351;&#35813;&#23545;&#20013;&#30340;&#19968;&#20010;&#23454;&#20307;&#65289;&#65292;&#24182;&#20026;&#28040;&#27495;&#38382;&#39064;&#24320;&#21457;&#20102;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#35821;&#26009;&#24211;&#28085;&#30422;&#20102;&#19977;&#20010;&#39046;&#22495;&#30340;&#38388;&#25509;&#24341;&#29992;&#34920;&#36798;&#24335;&#65292;&#26159;&#39318;&#27425;&#23454;&#29616;&#20102;&#36825;&#31181;&#39046;&#22495;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in language modeling have enabled new conversational systems. In particular, it is often desirable for people to make choices among specified options when using such systems. We address this problem of reference resolution, when people use natural expressions to choose between the entities. For example, given the choice `Should we make a Simnel cake or a Pandan cake?' a natural response from a dialog participant may be indirect: `let's make the green one'. Such natural expressions have been little studied for reference resolution. We argue that robustly understanding such language has large potential for improving naturalness in dialog, recommendation, and search systems. We create AltEntities (Alternative Entities), a new public dataset of 42K entity pairs and expressions (referring to one entity in the pair), and develop models for the disambiguation problem. Consisting of indirect referring expressions across three domains, our corpus enables for the first time the s
&lt;/p&gt;</description></item><item><title>SERENGETI&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#65292;&#35206;&#30422;&#20102;517&#31181;&#38750;&#27954;&#35821;&#35328;&#21644;&#35821;&#35328;&#26041;&#35328;&#12290;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#65292;&#23427;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#22312;&#38750;&#27954;&#35821;&#35328;&#19978;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35821;&#35328;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2212.10785</link><description>&lt;p&gt;
SERENGETI&#65306;&#20026;&#38750;&#27954;&#32780;&#35774;&#35745;&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SERENGETI: Massively Multilingual Language Models for Africa. (arXiv:2212.10785v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10785
&lt;/p&gt;
&lt;p&gt;
SERENGETI&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#65292;&#35206;&#30422;&#20102;517&#31181;&#38750;&#27954;&#35821;&#35328;&#21644;&#35821;&#35328;&#26041;&#35328;&#12290;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#65292;&#23427;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#22312;&#38750;&#27954;&#35821;&#35328;&#19978;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35821;&#35328;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;mPLM&#65289;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#33719;&#24471;&#26377;&#20215;&#20540;&#30340;&#35821;&#35328;&#20449;&#24687;&#65292;&#21487;&#25512;&#21160;&#29305;&#23450;&#20219;&#21153;&#30340;&#24494;&#35843;&#12290;&#30446;&#21069;&#65292;&#29616;&#26377;&#35821;&#35328;&#27169;&#22411;&#20165;&#35206;&#30422;&#20102;&#22823;&#32422;2,000&#31181;&#38750;&#27954;&#35821;&#35328;&#20013;&#30340;&#32422;31&#31181;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;SERENGETI&#65292;&#19968;&#31181;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#65292;&#35206;&#30422;&#20102;517&#31181;&#38750;&#27954;&#35821;&#35328;&#21644;&#35821;&#35328;&#26041;&#35328;&#65292;&#20197;&#25913;&#21892;&#36825;&#31181;&#38480;&#21046;&#12290;&#25105;&#20204;&#22312;20&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26032;&#22411;&#27169;&#22411;&#22312;&#20843;&#20010;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#23558;&#20854;&#19982;&#35206;&#30422;4-23&#31181;&#38750;&#27954;&#35821;&#35328;&#30340;4&#20010;mPLM&#36827;&#34892;&#27604;&#36739;&#12290;SERENGETI&#22312;&#20843;&#20010;&#20219;&#21153;&#20013;&#30340;11&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#23454;&#29616;&#20102;82.27&#30340;&#24179;&#22343;F_1&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#27169;&#22411;&#38169;&#35823;&#20998;&#26512;&#65292;&#20197;&#25506;&#31350;&#22312;&#38646;-shot&#24773;&#20917;&#19979;&#24212;&#29992;&#27169;&#22411;&#26102;&#35821;&#35328;&#31995;&#35889;&#21644;&#35821;&#35328;&#30456;&#20284;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#23558;&#21521;&#20844;&#20247;&#21457;&#24067;&#25105;&#20204;&#30340;&#30740;&#31350;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual pretrained language models (mPLMs) acquire valuable, generalizable linguistic information during pretraining and have advanced the state of the art on task-specific finetuning. To date, only ~31 out of ~2,000 African languages are covered in existing language models. We ameliorate this limitation by developing SERENGETI, a massively multilingual language model that covers 517 African languages and language varieties. We evaluate our novel models on eight natural language understanding tasks across 20 datasets, comparing to 4 mPLMs that cover 4-23 African languages. SERENGETI outperforms other models on 11 datasets across the eights tasks, achieving 82.27 average F_1. We also perform analyses of errors from our models, which allows us to investigate the influence of language genealogy and linguistic similarity when the models are applied under zero-shot settings. We will publicly release our models for research.\footnote{\href{https://github.com/UBC-NLP/serengeti}{https://g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25286;&#21368;&#30340;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#65292;Lego-MT&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#22810;&#35821;&#35328;&#21333;&#20307;&#27169;&#22411;&#22312;&#21442;&#25968;&#24178;&#25200;&#21644;&#20302;&#25928;&#25512;&#23548;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#36739;&#39640;&#30340;&#24615;&#33021;&#65292;&#30456;&#27604;&#20855;&#26377;10&#20493;&#35268;&#27169;&#30340;&#27169;&#22411;&#65292;&#22312;&#25928;&#29575;&#21644;&#34920;&#29616;&#26041;&#38754;&#37117;&#26356;&#20855;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2212.10551</link><description>&lt;p&gt;
Lego-MT: &#36208;&#21521;&#21487;&#25286;&#21368;&#30340;&#39640;&#24230;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Lego-MT: Towards Detachable Models in Massively Multilingual Machine Translation. (arXiv:2212.10551v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10551
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25286;&#21368;&#30340;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#65292;Lego-MT&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#22810;&#35821;&#35328;&#21333;&#20307;&#27169;&#22411;&#22312;&#21442;&#25968;&#24178;&#25200;&#21644;&#20302;&#25928;&#25512;&#23548;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#36739;&#39640;&#30340;&#24615;&#33021;&#65292;&#30456;&#27604;&#20855;&#26377;10&#20493;&#35268;&#27169;&#30340;&#27169;&#22411;&#65292;&#22312;&#25928;&#29575;&#21644;&#34920;&#29616;&#26041;&#38754;&#37117;&#26356;&#20855;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;(MNMT)&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#36866;&#29992;&#20110;&#22810;&#20010;&#35821;&#35328;&#26041;&#21521;&#30340;&#32479;&#19968;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;MNMT&#21333;&#20307;&#27169;&#22411;&#38754;&#20020;&#20004;&#20010;&#25361;&#25112;:&#35821;&#35328;&#20043;&#38388;&#30340;&#21442;&#25968;&#24178;&#25200;&#21644;&#22823;&#22411;&#27169;&#22411;&#30340;&#20302;&#25928;&#25512;&#29702;&#12290;&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#32463;&#20856;&#30340;&#22810;&#36335;&#24452;&#32467;&#26500;&#65292;&#36890;&#36807;&#23558;&#27599;&#31181;&#35821;&#35328;(&#25110;&#35821;&#35328;&#32452;)&#20998;&#37197;&#32473;&#25903;&#25345;&#21363;&#25554;&#21363;&#29992;&#35757;&#32451;&#21644;&#25512;&#29702;&#30340;&#21333;&#29420;&#20998;&#25903;&#65292;&#24320;&#21457;&#20986;&#21487;&#25286;&#21368;&#27169;&#22411;&#12290;&#20026;&#20102;&#28385;&#36275;&#22312;&#32479;&#19968;&#31354;&#38388;&#20013;&#20026;&#25152;&#26377;&#35821;&#35328;&#23398;&#20064;&#34920;&#31034;&#30340;&#38656;&#35201;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#25928;&#35757;&#32451;&#37197;&#26041;&#65292;&#20197;&#27492;&#26500;&#24314;&#19968;&#20010;&#26377;&#25928;&#30340;&#21487;&#25286;&#21368;&#27169;&#22411;&#65292;Lego-MT&#12290;&#20026;&#20102;&#36827;&#34892;&#20844;&#27491;&#30340;&#27604;&#36739;&#65292;&#25105;&#20204;&#20174;OPUS&#25910;&#38598;&#25968;&#25454;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#25324;433&#31181;&#35821;&#35328;&#21644;13&#20159;&#20010;&#24179;&#34892;&#25968;&#25454;&#30340;&#32763;&#35793;&#22522;&#20934;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#21442;&#25968;&#20026;12&#20159;&#30340;Lego-MT&#24102;&#26469;&#20102;3.2&#20010;spBLEU&#30340;&#24179;&#22343;&#22686;&#30410;&#12290;&#23427;&#29978;&#33267;&#32988;&#36807;&#20102;&#21442;&#25968;&#20026;120&#20159;&#30340;M2M-100&#12290;&#25152;&#25552;&#20986;&#30340;&#35757;&#32451;&#37197;&#26041;&#27604;&#24182;&#34892;&#35757;&#32451;&#25552;&#36895;&#20102;28.2&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual neural machine translation (MNMT) aims to build a unified model for many language directions. Existing monolithic models for MNMT encounter two challenges: parameter interference among languages and inefficient inference for large models. In this paper, we revisit the classic multi-way structures and develop a detachable model by assigning each language (or group of languages) to an individual branch that supports plug-and-play training and inference. To address the needs of learning representations for all languages in a unified space, we propose a novel efficient training recipe, upon which we build an effective detachable model, Lego-MT. For a fair comparison, we collect data from OPUS and build a translation benchmark covering 433 languages and 1.3B parallel data. Experiments show that Lego-MT with 1.2B parameters brings an average gain of 3.2 spBLEU. It even outperforms M2M-100 with 12B parameters. The proposed training recipe brings a 28.2$\times$ speedup over the co
&lt;/p&gt;</description></item><item><title>MaRCo&#26159;&#19968;&#31181;&#25490;&#27602;&#31639;&#27861;&#65292;&#33021;&#22815;&#20351;&#29992;&#19987;&#23478;&#21644;&#21453;&#19987;&#23478;&#27169;&#22411;&#23545;&#25991;&#26412;&#36827;&#34892;&#21487;&#25511;&#30340;&#37325;&#20889;&#21644;&#20462;&#35746;&#65292;&#36866;&#29992;&#20110;&#28040;&#38500;&#24494;&#22937;&#30340;&#26377;&#23475;&#20449;&#24687;&#65292;&#19988;&#22312;&#33258;&#21160;&#21270;&#25351;&#26631;&#21644;&#20154;&#31867;&#35780;&#20272;&#20013;&#22343;&#26377;&#24456;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2212.10543</link><description>&lt;p&gt;
&#29992;MaRCo&#28040;&#38500;&#26377;&#23475;&#25991;&#26412;&#65306;&#19987;&#23478;&#21644;&#21453;&#19987;&#23478;&#21487;&#25511;&#20462;&#35746;
&lt;/p&gt;
&lt;p&gt;
Detoxifying Text with MaRCo: Controllable Revision with Experts and Anti-Experts. (arXiv:2212.10543v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10543
&lt;/p&gt;
&lt;p&gt;
MaRCo&#26159;&#19968;&#31181;&#25490;&#27602;&#31639;&#27861;&#65292;&#33021;&#22815;&#20351;&#29992;&#19987;&#23478;&#21644;&#21453;&#19987;&#23478;&#27169;&#22411;&#23545;&#25991;&#26412;&#36827;&#34892;&#21487;&#25511;&#30340;&#37325;&#20889;&#21644;&#20462;&#35746;&#65292;&#36866;&#29992;&#20110;&#28040;&#38500;&#24494;&#22937;&#30340;&#26377;&#23475;&#20449;&#24687;&#65292;&#19988;&#22312;&#33258;&#21160;&#21270;&#25351;&#26631;&#21644;&#20154;&#31867;&#35780;&#20272;&#20013;&#22343;&#26377;&#24456;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#25490;&#27602;&#20855;&#26377;&#20943;&#36731;&#26377;&#23475;&#24615;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#36890;&#36807;&#37325;&#26032;&#34920;&#36848;&#25991;&#26412;&#26469;&#28040;&#38500;&#20882;&#29359;&#24615;&#30340;&#21547;&#20041;&#65292;&#20294;&#24494;&#22937;&#30340;&#26377;&#23475;&#24615;&#20173;&#28982;&#24456;&#38590;&#22788;&#29702;&#12290;&#26412;&#25991;&#24341;&#20837;MaRCo&#65292;&#19968;&#31181;&#25490;&#27602;&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#21487;&#25511;&#29983;&#25104;&#21644;&#25991;&#26412;&#37325;&#20889;&#26041;&#27861;&#65292;&#20351;&#29992;&#33258;&#32534;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#30340;&#19987;&#23478;&#20135;&#21697;&#21644;&#21453;&#19987;&#23478;&#20135;&#21697;&#12290;MaRCo&#20351;&#29992;&#38750;&#26377;&#23475;LM&#65288;&#19987;&#23478;&#65289;&#21644;&#26377;&#23475;LM&#65288;&#21453;&#19987;&#23478;&#65289;&#19979;&#30340;&#21487;&#33021;&#24615;&#26469;&#26597;&#25214;&#20505;&#36873;&#21333;&#35789;&#20197;&#36827;&#34892;&#25513;&#30422;&#21644;&#21487;&#33021;&#26367;&#25442;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#24494;&#22937;&#26377;&#23475;&#24615;&#21644;&#24494;&#25915;&#20987;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#26174;&#31034;&#23427;&#19981;&#20165;&#22312;&#33258;&#21160;&#24230;&#37327;&#19978;&#20248;&#20110;&#22522;&#32447;&#65292;&#32780;&#19988;MaRCo&#30340;&#37325;&#20889;&#22312;&#20154;&#31867;&#35780;&#20272;&#20013;&#26356;&#21463;&#27426;&#36814;&#12290;&#23427;&#23545;&#24494;&#22937;&#30340;&#26377;&#23475;&#24615;&#24773;&#20917;&#30340;&#36866;&#29992;&#24615;&#23588;&#20854;&#26377;&#21069;&#36884;&#65292;&#20026;&#35299;&#20915;&#26085;&#30410;&#38590;&#20197;&#25417;&#25720;&#30340;&#22312;&#32447;&#20167;&#24680;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#26465;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text detoxification has the potential to mitigate the harms of toxicity by rephrasing text to remove offensive meaning, but subtle toxicity remains challenging to tackle. We introduce MaRCo, a detoxification algorithm that combines controllable generation and text rewriting methods using a Product of Experts with autoencoder language models (LMs). MaRCo uses likelihoods under a non-toxic LM (expert) and a toxic LM (anti-expert) to find candidate words to mask and potentially replace. We evaluate our method on several subtle toxicity and microaggressions datasets, and show that it not only outperforms baselines on automatic metrics, but MaRCo's rewrites are preferred 2.1 $\times$ more in human evaluation. Its applicability to instances of subtle toxicity is especially promising, demonstrating a path forward for addressing increasingly elusive online hate.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;10&#20010;&#27169;&#22411;&#21644;4&#31181;&#22686;&#24378;&#26041;&#27861;&#30340;&#23454;&#39564;&#65292;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#22312;&#35760;&#24518;&#19981;&#22826;&#27969;&#34892;&#30340;&#23454;&#38469;&#30693;&#35782;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#32780;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#36739;&#22909;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#26377;&#25928;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.10511</link><description>&lt;p&gt;
&#20309;&#26102;&#19981;&#20449;&#20219;&#35821;&#35328;&#27169;&#22411;&#65306;&#25506;&#32034;&#21442;&#25968;&#21644;&#38750;&#21442;&#25968;&#35760;&#24518;&#30340;&#26377;&#25928;&#24615;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
When Not to Trust Language Models: Investigating Effectiveness and Limitations of Parametric and Non-Parametric Memories. (arXiv:2212.10511v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10511
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;10&#20010;&#27169;&#22411;&#21644;4&#31181;&#22686;&#24378;&#26041;&#27861;&#30340;&#23454;&#39564;&#65292;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#22312;&#35760;&#24518;&#19981;&#22826;&#27969;&#34892;&#30340;&#23454;&#38469;&#30693;&#35782;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#32780;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#36739;&#22909;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#26377;&#25928;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20173;&#28982;&#38590;&#20197;&#22788;&#29702;&#38656;&#35201;&#20016;&#23500;&#19990;&#30028;&#30693;&#35782;&#30340;&#20219;&#21153;&#65292;&#36825;&#26263;&#31034;&#20102;&#20165;&#20381;&#38752;&#20854;&#21442;&#25968;&#26469;&#32534;&#30721;&#20016;&#23500;&#30340;&#19990;&#30028;&#30693;&#35782;&#30340;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#23545;10&#20010;&#27169;&#22411;&#21644;4&#31181;&#22686;&#24378;&#26041;&#27861;&#22312;PopQA&#19978;&#36827;&#34892;&#22823;&#35268;&#27169;&#30693;&#35782;&#25506;&#27979;&#23454;&#39564;&#65292;&#20197;&#20102;&#35299;&#35821;&#35328;&#27169;&#22411;&#22312;&#35760;&#24518;&#20107;&#23454;&#30693;&#35782;&#26041;&#38754;&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35821;&#35328;&#27169;&#22411;&#38590;&#20197;&#35760;&#24518;&#19981;&#22826;&#27969;&#34892;&#30340;&#23454;&#38469;&#30693;&#35782;&#65292;&#24182;&#19988;&#22312;&#38271;&#23614;&#20013;&#65292;&#25193;&#23637;&#35268;&#27169;&#26080;&#27861;&#26126;&#26174;&#25913;&#21892;&#35760;&#24518;&#23454;&#38469;&#30693;&#35782;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#32988;&#36807;&#32423;&#21035;&#22823;&#24471;&#22810;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#26410;&#32463;&#21327;&#21161;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#28041;&#21450;&#39640;&#27969;&#34892;&#23454;&#20307;&#30340;&#38382;&#39064;&#19978;&#20173;&#28982;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#24378;&#22823;&#21644;&#39640;&#25928;&#30340;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20165;&#22312;&#38656;&#35201;&#26102;&#26816;&#32034;&#38750;&#21442;&#25968;&#35760;&#24518;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite their impressive performance on diverse tasks, large language models (LMs) still struggle with tasks requiring rich world knowledge, implying the limitations of relying solely on their parameters to encode a wealth of world knowledge. This paper aims to understand LMs' strengths and limitations in memorizing factual knowledge, by conducting large-scale knowledge probing experiments of 10 models and 4 augmentation methods on PopQA, our new open-domain QA dataset with 14k questions. We find that LMs struggle with less popular factual knowledge, and that scaling fails to appreciably improve memorization of factual knowledge in the long tail. We then show that retrieval-augmented LMs largely outperform orders of magnitude larger LMs, while unassisted LMs remain competitive in questions about high-popularity entities. Based on those findings, we devise a simple, yet effective, method for powerful and efficient retrieval-augmented LMs, which retrieves non-parametric memories only whe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; Detailed Outline Control(DOC) &#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35814;&#32454;&#22823;&#32434;&#21644;&#35814;&#32454;&#25511;&#21046;&#22120;&#26469;&#25552;&#39640;&#29983;&#25104;&#38271;&#31687;&#25925;&#20107;&#26102;&#30340;&#24773;&#33410;&#36830;&#36143;&#24615;&#21644;&#22823;&#32434;&#30456;&#20851;&#24615;&#65292;&#20154;&#31867;&#35780;&#20272;&#35777;&#23454;&#35813;&#26041;&#27861;&#22312;&#36825;&#20123;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#65292;&#24182;&#19988;&#26356;&#36866;&#29992;&#20110;&#20132;&#20114;&#29983;&#25104;&#35774;&#32622;&#12290;</title><link>http://arxiv.org/abs/2212.10077</link><description>&lt;p&gt;
&#36890;&#36807;&#35814;&#32454;&#30340;&#22823;&#32434;&#25511;&#21046;&#25552;&#21319;&#38271;&#31687;&#25925;&#20107;&#36830;&#36143;&#24615;
&lt;/p&gt;
&lt;p&gt;
DOC: Improving Long Story Coherence With Detailed Outline Control. (arXiv:2212.10077v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10077
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; Detailed Outline Control(DOC) &#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35814;&#32454;&#22823;&#32434;&#21644;&#35814;&#32454;&#25511;&#21046;&#22120;&#26469;&#25552;&#39640;&#29983;&#25104;&#38271;&#31687;&#25925;&#20107;&#26102;&#30340;&#24773;&#33410;&#36830;&#36143;&#24615;&#21644;&#22823;&#32434;&#30456;&#20851;&#24615;&#65292;&#20154;&#31867;&#35780;&#20272;&#35777;&#23454;&#35813;&#26041;&#27861;&#22312;&#36825;&#20123;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#65292;&#24182;&#19988;&#26356;&#36866;&#29992;&#20110;&#20132;&#20114;&#29983;&#25104;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026; Detailed Outline Control(DOC)&#30340;&#26694;&#26550;&#65292;&#20197;&#25552;&#39640;&#29983;&#25104;&#25968;&#21315;&#23383;&#38271;&#30340;&#25925;&#20107;&#26102;&#30340;&#38271;&#31243;&#24773;&#33410;&#36830;&#36143;&#24615;&#12290;DOC&#30001;&#20004;&#20010;&#20114;&#34917;&#32452;&#20214;&#32452;&#25104;&#65306;&#35814;&#32454;&#22823;&#32434;&#21644;&#35814;&#32454;&#25511;&#21046;&#22120;&#12290;&#35814;&#32454;&#22823;&#32434;&#21019;&#24314;&#19968;&#20010;&#26356;&#35814;&#32454;&#12289;&#23618;&#27425;&#21270;&#30340;&#22823;&#32434;&#65292;&#23558;&#21019;&#36896;&#24615;&#36127;&#25285;&#20174;&#20027;&#35201;&#36215;&#33609;&#36807;&#31243;&#36716;&#31227;&#21040;&#35268;&#21010;&#38454;&#27573;&#12290;&#35814;&#32454;&#25511;&#21046;&#22120;&#36890;&#36807;&#25511;&#21046;&#25925;&#20107;&#27573;&#33853;&#19982;&#22823;&#32434;&#32454;&#33410;&#23545;&#40784;&#65292;&#30830;&#20445;&#26356;&#35814;&#32454;&#30340;&#22823;&#32434;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#20173;&#28982;&#34987;&#23562;&#37325;&#12290;&#22312;&#33258;&#21160;&#29983;&#25104;&#30340;&#25925;&#20107;&#30340;&#20154;&#31867;&#35780;&#20272;&#20013;&#65292;DOC&#22312;&#24773;&#33410;&#36830;&#36143;&#24615;(22.5% &#32477;&#23545;&#22686;&#30410;)&#12289;&#22823;&#32434;&#30456;&#20851;&#24615;(28.2%)&#21644;&#36259;&#21619;&#24615;(20.7%)&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#24378;&#22823;&#30340;Re3&#22522;&#32447;(Yang&#31561;&#20154;&#65292;2022)&#12290;&#20154;&#20204;&#36824;&#35780;&#20215;DOC&#22312;&#20132;&#20114;&#29983;&#25104;&#35774;&#32622;&#26041;&#38754;&#26356;&#26131;&#20110;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the Detailed Outline Control (DOC) framework for improving long-range plot coherence when automatically generating several-thousand-word-long stories. DOC consists of two complementary components: a detailed outliner and a detailed controller. The detailed outliner creates a more detailed, hierarchically structured outline, shifting creative burden from the main drafting procedure to the planning stage. The detailed controller ensures the more detailed outline is still respected during generation by controlling story passages to align with outline details. In human evaluations of automatically generated stories, DOC substantially outperforms a strong Re3 baseline (Yang et al., 2022) on plot coherence (22.5% absolute gain), outline relevance (28.2%), and interestingness (20.7%). Humans also judged DOC to be much more controllable in an interactive generation setting.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24369;&#30417;&#30563;&#26694;&#26550;WeCheck&#65292;&#36890;&#36807;&#32858;&#21512;&#22810;&#31181;&#36164;&#28304;&#35757;&#32451;&#27169;&#22411;&#65292;&#20934;&#30830;&#21644;&#39640;&#25928;&#22320;&#26816;&#26597;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#26159;&#21542;&#23384;&#22312;&#23454;&#38469;&#20107;&#23454;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2212.10057</link><description>&lt;p&gt;
WeCheck&#65306;&#22522;&#20110;&#24369;&#30417;&#30563;&#23398;&#20064;&#30340;&#24378;&#23454;&#38469;&#19968;&#33268;&#24615;&#26816;&#26597;&#22120;
&lt;/p&gt;
&lt;p&gt;
WeCheck: Strong Factual Consistency Checker via Weakly Supervised Learning. (arXiv:2212.10057v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10057
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24369;&#30417;&#30563;&#26694;&#26550;WeCheck&#65292;&#36890;&#36807;&#32858;&#21512;&#22810;&#31181;&#36164;&#28304;&#35757;&#32451;&#27169;&#22411;&#65292;&#20934;&#30830;&#21644;&#39640;&#25928;&#22320;&#26816;&#26597;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#26159;&#21542;&#23384;&#22312;&#23454;&#38469;&#20107;&#23454;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#23427;&#20204;&#32463;&#24120;&#20250;&#29983;&#25104;&#19982;&#20854;&#36755;&#20837;&#23384;&#22312;&#20107;&#23454;&#19981;&#19968;&#33268;&#30340;&#25991;&#26412;&#12290;&#30001;&#20110;&#32570;&#20047;&#27880;&#37322;&#25968;&#25454;&#65292;&#29616;&#26377;&#30340;&#22312;&#35780;&#20272;&#20107;&#23454;&#19968;&#33268;&#24615;&#26041;&#38754;&#30340;&#20316;&#21697;&#30452;&#25509;&#36716;&#31227;&#27169;&#22411;&#22312;&#20854;&#20182;&#25968;&#25454;&#20016;&#23500;&#30340;&#19978;&#28216;&#20219;&#21153;&#65292;&#20363;&#22914;&#38382;&#39064;&#22238;&#31572;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#32780;&#27809;&#26377;&#36827;&#19968;&#27493;&#30340;&#36866;&#24212;&#24615;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#22312;&#30495;&#27491;&#29983;&#25104;&#30340;&#25991;&#26412;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#24182;&#19988;&#21463;&#20854;&#21333;&#19968;&#28304;&#19978;&#28216;&#20219;&#21153;&#30340;&#20005;&#37325;&#20559;&#35265;&#25152;&#24433;&#21709;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24369;&#30417;&#30563;&#26694;&#26550;&#65292;&#36890;&#36807;&#32858;&#21512;&#22810;&#20010;&#36164;&#28304;&#26469;&#35757;&#32451;&#19968;&#20010;&#31934;&#30830;&#21644;&#39640;&#25928;&#30340;&#20107;&#23454;&#24230;&#37327;&#26631;&#20934;&#65292;&#21363;WeCheck&#12290;WeCheck&#39318;&#20808;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#36890;&#36807;&#32858;&#21512;&#22810;&#20010;&#36164;&#28304;&#25512;&#26029;&#20986;&#30340;&#24369;&#26631;&#31614;&#26469;&#20934;&#30830;&#26631;&#35760;&#30495;&#23454;&#29983;&#25104;&#30340;&#26679;&#26412;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#32771;&#34385;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#24369;&#30417;&#30563;&#35757;&#32451;&#30446;&#26631;&#24230;&#37327;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
A crucial issue of current text generation models is that they often uncontrollably generate factually inconsistent text with respective of their inputs. Limited by the lack of annotated data, existing works in evaluating factual consistency directly transfer the reasoning ability of models trained on other data-rich upstream tasks like question answering (QA) and natural language inference (NLI) without any further adaptation. As a result, they perform poorly on the real generated text and are biased heavily by their single-source upstream tasks. To alleviate this problem, we propose a weakly supervised framework that aggregates multiple resources to train a precise and efficient factual metric, namely WeCheck. WeCheck first utilizes a generative model to accurately label a real generated sample by aggregating its weak labels, which are inferred from multiple resources. Then, we train the target metric model with the weak supervision while taking noises into consideration. Comprehensi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#32508;&#21512;&#35843;&#26597;&#12290;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#25552;&#20379;&#31995;&#32479;&#36164;&#28304;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;</title><link>http://arxiv.org/abs/2212.09597</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Reasoning with Language Model Prompting: A Survey. (arXiv:2212.09597v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#32508;&#21512;&#35843;&#26597;&#12290;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#25552;&#20379;&#31995;&#32479;&#36164;&#28304;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;&#20316;&#20026;&#22797;&#26434;&#38382;&#39064;&#35299;&#20915;&#30340;&#37325;&#35201;&#33021;&#21147;&#65292;&#21487;&#20197;&#20026;&#21307;&#30103;&#35786;&#26029;&#12289;&#35848;&#21028;&#31561;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#21518;&#31471;&#25903;&#25345;&#12290;&#26412;&#25991;&#23545;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#36827;&#34892;&#20102;&#32508;&#21512;&#35843;&#26597;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#30740;&#31350;&#25104;&#26524;&#30340;&#27604;&#36739;&#21644;&#24635;&#32467;&#65292;&#24182;&#25552;&#20379;&#20102;&#31995;&#32479;&#36164;&#28304;&#20197;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#31361;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#36164;&#28304;&#21487;&#22312; https://github.com/zjunlp/Prompt4ReasoningPapers &#19978;&#33719;&#21462;&#65288;&#23450;&#26399;&#26356;&#26032;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning, as an essential ability for complex problem-solving, can provide back-end support for various real-world applications, such as medical diagnosis, negotiation, etc. This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting. We introduce research works with comparisons and summaries and provide systematic resources to help beginners. We also discuss the potential reasons for emerging such reasoning abilities and highlight future research directions. Resources are available at https://github.com/zjunlp/Prompt4ReasoningPapers (updated periodically).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;BLOOM&#27169;&#22411;&#20013;&#24212;&#29992;&#35821;&#35328;&#36866;&#24212;&#31574;&#30053;&#65292;&#23558;&#20854;&#36866;&#24212;&#21040;&#26032;&#35821;&#35328;&#19978;&#65292;&#24182;&#22312;&#20843;&#31181;&#26032;&#35821;&#35328;&#30340;&#38646;&#26679;&#26412;&#25552;&#31034;&#34920;&#29616;&#20013;&#25552;&#21319;&#20102;&#24615;&#33021;&#12290;&#36866;&#37197;&#22120;&#24494;&#35843;&#27604;&#22823;&#27169;&#22411;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#26356;&#26377;&#25928;&#65292;&#25552;&#31034;&#24615;&#33021;&#20027;&#35201;&#30001;&#35821;&#35328;&#36866;&#24212;&#25968;&#25454;&#30340;&#22823;&#23567;&#30830;&#23450;&#12290;</title><link>http://arxiv.org/abs/2212.09535</link><description>&lt;p&gt;
BLOOM+1&#65306;&#20026;&#38646;&#26679;&#26412;&#25552;&#31034;&#28155;&#21152;&#35821;&#35328;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
BLOOM+1: Adding Language Support to BLOOM for Zero-Shot Prompting. (arXiv:2212.09535v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;BLOOM&#27169;&#22411;&#20013;&#24212;&#29992;&#35821;&#35328;&#36866;&#24212;&#31574;&#30053;&#65292;&#23558;&#20854;&#36866;&#24212;&#21040;&#26032;&#35821;&#35328;&#19978;&#65292;&#24182;&#22312;&#20843;&#31181;&#26032;&#35821;&#35328;&#30340;&#38646;&#26679;&#26412;&#25552;&#31034;&#34920;&#29616;&#20013;&#25552;&#21319;&#20102;&#24615;&#33021;&#12290;&#36866;&#37197;&#22120;&#24494;&#35843;&#27604;&#22823;&#27169;&#22411;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#26356;&#26377;&#25928;&#65292;&#25552;&#31034;&#24615;&#33021;&#20027;&#35201;&#30001;&#35821;&#35328;&#36866;&#24212;&#25968;&#25454;&#30340;&#22823;&#23567;&#30830;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
BLOOM&#27169;&#22411;&#26159;&#19968;&#20010;&#22823;&#22411;&#20844;&#24320;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#65292;&#20294;&#20854;&#39044;&#35757;&#32451;&#20165;&#38480;&#20110;46&#31181;&#35821;&#35328;&#12290;&#20026;&#20102;&#23558;BLOOM&#30340;&#22909;&#22788;&#25193;&#23637;&#21040;&#20854;&#20182;&#35821;&#35328;&#65292;&#32780;&#19981;&#20250;&#20135;&#29983;&#36807;&#39640;&#30340;&#25104;&#26412;&#65292;&#26377;&#24517;&#35201;&#23558;BLOOM&#36866;&#24212;&#21040;&#26032;&#30340;&#35821;&#35328;&#19978;&#12290;&#26412;&#25991;&#23558;&#29616;&#26377;&#30340;&#35821;&#35328;&#36866;&#24212;&#31574;&#30053;&#24212;&#29992;&#20110;BLOOM&#65292;&#24182;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#23545;&#20854;&#22312;&#20843;&#31181;&#26032;&#35821;&#35328;&#30340;&#38646;&#26679;&#26412;&#25552;&#31034;&#34920;&#29616;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35821;&#35328;&#36866;&#24212;&#23545;&#20110;&#25552;&#39640;&#26032;&#35821;&#35328;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#26159;&#26377;&#25928;&#30340;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#36866;&#37197;&#22120;&#24494;&#35843;&#27604;&#22823;&#27169;&#22411;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#26356;&#26377;&#25928;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#25552;&#31034;&#24615;&#33021;&#19981;&#20250;&#21463;&#21040;&#35821;&#35328;&#29305;&#23450;&#24615;&#30340;&#26174;&#30528;&#24433;&#21709;&#65292;&#22914;&#20070;&#20889;&#31995;&#32479;&#12290;&#23427;&#20027;&#35201;&#30001;&#35821;&#35328;&#36866;&#24212;&#25968;&#25454;&#30340;&#22823;&#23567;&#30830;&#23450;&#12290;&#25105;&#20204;&#36824;&#21521;BLOOMZ&#28155;&#21152;&#20102;&#26032;&#35821;&#35328;&#65292;&#36825;&#26159;BLOOM&#30340;&#22810;&#20219;&#21153;&#24494;&#35843;&#29256;&#26412;&#65292;&#33021;&#22815;&#36319;&#38543;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The BLOOM model is a large publicly available multilingual language model, but its pretraining was limited to 46 languages. To extend the benefits of BLOOM to other languages without incurring prohibitively large costs, it is desirable to adapt BLOOM to new languages not seen during pretraining. In this work, we apply existing language adaptation strategies to BLOOM and benchmark its zero-shot prompting performance on eight new languages in a resource-constrained setting. We find language adaptation to be effective at improving zero-shot performance in new languages. Surprisingly, we find that adapter-based finetuning is more effective than continued pretraining for large models. In addition, we discover that prompting performance is not significantly affected by language specifics, such as the writing system. It is primarily determined by the size of the language adaptation data. We also add new languages to BLOOMZ, which is a multitask finetuned version of BLOOM capable of following 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35757;&#32451;&#38376;&#30340;&#22810;&#26041;&#38754;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#35268;&#33539;&#21069;&#32512;&#30340;&#24178;&#39044;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#35757;&#32451;&#26102;&#26410;&#35265;&#36807;&#30340;&#26041;&#38754;&#32452;&#21512;&#30340;&#25511;&#21046;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2212.09387</link><description>&lt;p&gt;
&#22810;&#26041;&#38754;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#30340;&#21487;&#25193;&#23637;&#21363;&#25554;&#21363;&#29992;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Extensible Plug-and-Play Method for Multi-Aspect Controllable Text Generation. (arXiv:2212.09387v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09387
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35757;&#32451;&#38376;&#30340;&#22810;&#26041;&#38754;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#35268;&#33539;&#21069;&#32512;&#30340;&#24178;&#39044;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#35757;&#32451;&#26102;&#26410;&#35265;&#36807;&#30340;&#26041;&#38754;&#32452;&#21512;&#30340;&#25511;&#21046;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25511;&#21046;&#29983;&#25104;&#25991;&#26412;&#30340;&#22810;&#20010;&#26041;&#38754;&#65288;&#22914;&#24773;&#24863;&#12289;&#20027;&#39064;&#21644;&#20851;&#38190;&#35789;&#65289;&#30340;&#22810;&#26041;&#38754;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#34429;&#28982;&#22522;&#20110;&#21442;&#25968;&#26377;&#25928;&#35843;&#25972;&#30340;&#26041;&#27861;&#65292;&#22914;&#21069;&#32512;&#35843;&#25972;&#65292;&#21487;&#20197;&#20197;&#21363;&#25554;&#21363;&#29992;&#30340;&#26041;&#24335;&#23454;&#29616;&#22810;&#26041;&#38754;&#25511;&#21046;&#65292;&#20294;&#22810;&#20010;&#21069;&#32512;&#30340;&#30456;&#20114;&#24178;&#25200;&#23548;&#33268;&#20102;&#32422;&#26463;&#30340;&#26174;&#33879;&#24694;&#21270;&#65292;&#24182;&#38480;&#21046;&#20102;&#23427;&#20204;&#23545;&#20110;&#35757;&#32451;&#26102;&#26410;&#35265;&#36807;&#30340;&#26041;&#38754;&#32452;&#21512;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20026;&#24178;&#25200;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#19979;&#38480;&#65292;&#24182;&#23454;&#39564;&#35777;&#26126;&#24178;&#25200;&#38543;&#25554;&#20837;&#21069;&#32512;&#30340;&#23618;&#25968;&#22686;&#21152;&#32780;&#22686;&#21152;&#12290;&#22522;&#20110;&#36825;&#20123;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#21487;&#35757;&#32451;&#38376;&#26469;&#35268;&#33539;&#21069;&#32512;&#30340;&#24178;&#39044;&#65292;&#20197;&#25233;&#21046;&#19981;&#26029;&#22686;&#38271;&#30340;&#24178;&#25200;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#31616;&#21333;&#22320;&#36830;&#25509;&#30456;&#24212;&#30340;&#25554;&#20214;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#35757;&#32451;&#26102;&#26410;&#35265;&#36807;&#30340;&#26041;&#38754;&#32452;&#21512;&#30340;&#25511;&#21046;&#65292;&#20174;&#32780;&#21487;&#20197;&#20302;&#25104;&#26412;&#22320;&#25193;&#23637;&#26032;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20351;&#21508;&#31181;&#25554;&#20214;&#33021;&#22815;&#28789;&#27963;&#38598;&#25104;&#65292;&#20197;&#23545;&#24212;&#19981;&#21516;&#30340;&#26041;&#38754;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#21487;&#25511;&#24615;&#12289;&#36830;&#36143;&#24615;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, multi-aspect controllable text generation that controls the generated text in multiple aspects (e.g., sentiment, topic, and keywords) has attracted increasing attention. Although methods based on parameter efficient tuning like prefix-tuning could achieve multi-aspect controlling in a plug-and-play way, the mutual interference of multiple prefixes leads to significant degeneration of constraints and limits their extensibility to training-time unseen aspect combinations. In this work, we provide a theoretical lower bound for the interference and empirically found that the interference grows with the number of layers where prefixes are inserted. Based on these analyses, we propose using trainable gates to normalize the intervention of prefixes to restrain the growing interference. As a result, controlling training-time unseen combinations of aspects can be realized by simply concatenating corresponding plugins such that new constraints can be extended at a lower cost. In additi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21160;&#24577;&#25512;&#26029;&#21644;&#24314;&#27169;&#27714;&#21161;&#32773;&#20154;&#26684;&#30340;&#26694;&#26550;&#65292;&#24182;&#32467;&#21512;&#22522;&#20110;&#31574;&#30053;&#30340;&#21487;&#25511;&#29983;&#25104;&#26041;&#27861;&#25552;&#20379;&#20010;&#24615;&#21270;&#24773;&#24863;&#25903;&#25345;&#65292;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#20154;&#26684;&#23545;&#24773;&#24863;&#25903;&#25345;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2212.09235</link><description>&lt;p&gt;
PAL&#65306;&#20154;&#26684;&#22686;&#24378;&#30340;&#24773;&#24863;&#25903;&#25345;&#20250;&#35805;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
PAL: Persona-Augmented Emotional Support Conversation Generation. (arXiv:2212.09235v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09235
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21160;&#24577;&#25512;&#26029;&#21644;&#24314;&#27169;&#27714;&#21161;&#32773;&#20154;&#26684;&#30340;&#26694;&#26550;&#65292;&#24182;&#32467;&#21512;&#22522;&#20110;&#31574;&#30053;&#30340;&#21487;&#25511;&#29983;&#25104;&#26041;&#27861;&#25552;&#20379;&#20010;&#24615;&#21270;&#24773;&#24863;&#25903;&#25345;&#65292;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#20154;&#26684;&#23545;&#24773;&#24863;&#25903;&#25345;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#30340;&#20154;&#21147;&#36164;&#28304;&#30701;&#32570;&#65292;&#36234;&#26469;&#36234;&#38656;&#35201;&#20351;&#29992;&#20250;&#35805;&#20195;&#29702;&#31243;&#24207;&#36827;&#34892;&#25903;&#25345;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#23545;&#35805;&#27169;&#22411;&#22312;&#25552;&#20379;&#24773;&#24863;&#25903;&#25345;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#30001;&#20110;&#20197;&#24448;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#27714;&#21161;&#32773;&#30340;&#20154;&#26684;&#26159;&#26377;&#25928;&#25903;&#25345;&#30340;&#37325;&#35201;&#22240;&#32032;&#65292;&#22240;&#27492;&#25105;&#20204;&#35843;&#26597;&#20102;&#22312;&#25903;&#25345;&#23545;&#35805;&#27169;&#22411;&#20013;&#24314;&#27169;&#27492;&#31867;&#20449;&#24687;&#26159;&#21542;&#26377;&#30410;&#12290;&#26412;&#25991;&#30340;&#23454;&#35777;&#20998;&#26512;&#39564;&#35777;&#20102;&#20154;&#26684;&#23545;&#24773;&#24863;&#25903;&#25345;&#30340;&#37325;&#35201;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#25512;&#26029;&#21644;&#24314;&#27169;&#27714;&#21161;&#32773;&#20154;&#26684;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#39318;&#20808;&#35757;&#32451;&#20102;&#19968;&#20010;&#20174;&#23545;&#35805;&#21382;&#21490;&#20013;&#25512;&#26029;&#27714;&#21161;&#32773;&#20154;&#26684;&#30340;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PAL&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#20154;&#26684;&#20449;&#24687;&#21644;&#25105;&#20204;&#22522;&#20110;&#31574;&#30053;&#30340;&#21487;&#25511;&#29983;&#25104;&#26041;&#27861;&#65292;&#25552;&#20379;&#20010;&#24615;&#21270;&#24773;&#24863;&#25903;&#25345;&#30340;&#27169;&#22411;&#12290;&#33258;&#21160;&#21644;&#25163;&#21160;&#35780;&#20272;&#34920;&#26126;&#65292;PAL&#22312;&#24773;&#24863;&#25903;&#25345;&#23545;&#35805;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the lack of human resources for mental health support, there is an increasing demand for employing conversational agents for support. Recent work has demonstrated the effectiveness of dialogue models in providing emotional support. As previous studies have demonstrated that seekers' persona is an important factor for effective support, we investigate whether there are benefits to modeling such information in dialogue models for support. In this paper, our empirical analysis verifies that persona has an important impact on emotional support. Therefore, we propose a framework for dynamically inferring and modeling seekers' persona. We first train a model for inferring the seeker's persona from the conversation history. Accordingly, we propose PAL, a model that leverages persona information and, in conjunction with our strategy-based controllable generation method, provides personalized emotional support. Automatic and manual evaluations demonstrate that PAL achieves state-of-the-a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20960;&#20309;&#23398;&#35282;&#24230;&#22312;&#23545;&#27604;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#20013;&#21457;&#29616;&#65292;&#23545;&#27604;&#23398;&#20064;&#24102;&#26469;&#20102;&#21508;&#21521;&#21516;&#24615;&#65292;&#24182;&#39537;&#21160;&#21516;&#19968;&#21477;&#23376;&#20013;&#26631;&#35760;&#22312;&#35821;&#20041;&#31354;&#38388;&#20013;&#25910;&#25947;&#21040;&#30456;&#20284;&#30340;&#20301;&#32622;&#12290;&#23545;&#20110;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#26631;&#35760;&#65292;"&#34394;&#20551;&#30340;&#24773;&#22659;&#21270;"&#24471;&#21040;&#20102;&#32531;&#35299;&#65292;&#32780;&#23545;&#20110;&#21151;&#33021;&#24615;&#26631;&#35760;&#21017;&#34987;&#22686;&#24378;&#12290;</title><link>http://arxiv.org/abs/2212.09170</link><description>&lt;p&gt;
&#20851;&#20110;&#23545;&#27604;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#30340;&#21508;&#21521;&#21516;&#24615;&#12289;&#24773;&#22659;&#21270;&#21644;&#23398;&#20064;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
On Isotropy, Contextualization and Learning Dynamics of Contrastive-based Sentence Representation Learning. (arXiv:2212.09170v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09170
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20960;&#20309;&#23398;&#35282;&#24230;&#22312;&#23545;&#27604;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#20013;&#21457;&#29616;&#65292;&#23545;&#27604;&#23398;&#20064;&#24102;&#26469;&#20102;&#21508;&#21521;&#21516;&#24615;&#65292;&#24182;&#39537;&#21160;&#21516;&#19968;&#21477;&#23376;&#20013;&#26631;&#35760;&#22312;&#35821;&#20041;&#31354;&#38388;&#20013;&#25910;&#25947;&#21040;&#30456;&#20284;&#30340;&#20301;&#32622;&#12290;&#23545;&#20110;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#26631;&#35760;&#65292;"&#34394;&#20551;&#30340;&#24773;&#22659;&#21270;"&#24471;&#21040;&#20102;&#32531;&#35299;&#65292;&#32780;&#23545;&#20110;&#21151;&#33021;&#24615;&#26631;&#35760;&#21017;&#34987;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#32435;&#20837;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#20013;&#65292;&#22312;&#35768;&#22810;&#21477;&#23376;&#32423;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#26412;&#25991;&#36890;&#36807;&#21508;&#21521;&#21516;&#24615;&#12289;&#24773;&#22659;&#21270;&#21644;&#23398;&#20064;&#21160;&#24577;&#30340;&#35270;&#35282;&#26469;&#21078;&#26512;&#23545;&#27604;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#30340;&#34920;&#29616;&#65292;&#26088;&#22312;&#20026;&#26410;&#26469;&#35774;&#35745;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#25552;&#20379;&#25351;&#23548;&#12290;&#20316;&#32773;&#36890;&#36807;&#34920;&#31034;&#21464;&#25442;&#30340;&#20960;&#20309;&#23398;&#26469;&#35299;&#37322;&#23545;&#27604;&#23398;&#20064;&#30340;&#25104;&#21151;&#65292;&#24182;&#23637;&#31034;&#23545;&#27604;&#23398;&#20064;&#22914;&#20309;&#24102;&#26469;&#21508;&#21521;&#21516;&#24615;&#24182;&#23548;&#33268;&#21516;&#19968;&#21477;&#23376;&#20013;&#30340;&#26631;&#35760;&#22312;&#35821;&#20041;&#31354;&#38388;&#20013;&#25910;&#25947;&#21040;&#30456;&#20284;&#30340;&#20301;&#32622;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#23545;&#20110;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#26631;&#35760;&#65292;"&#34394;&#20551;&#30340;&#24773;&#22659;&#21270;"&#24471;&#21040;&#20102;&#32531;&#35299;&#65292;&#32780;&#23545;&#20110;&#21151;&#33021;&#24615;&#26631;&#35760;&#21017;&#34987;&#22686;&#24378;&#12290;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#23884;&#20837;&#31354;&#38388;&#26397;&#21521;&#21407;&#28857;&#24182;&#26356;&#22909;&#22320;&#23450;&#20041;&#20102;&#26356;&#22810;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incorporating contrastive learning objectives in sentence representation learning (SRL) has yielded significant improvements on many sentence-level NLP tasks. However, it is not well understood why contrastive learning works for learning sentence-level semantics. In this paper, we aim to help guide future designs of sentence representation learning methods by taking a closer look at contrastive SRL through the lens of isotropy, contextualization and learning dynamics. We interpret its successes through the geometry of the representation shifts and show that contrastive learning brings isotropy, and drives high intra-sentence similarity: when in the same sentence, tokens converge to similar positions in the semantic space. We also find that what we formalize as "spurious contextualization" is mitigated for semantically meaningful tokens, while augmented for functional ones. We find that the embedding space is directed towards the origin during training, with more areas now better define
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#39640;&#38454;&#26465;&#20214;&#38543;&#26426;&#22330;&#36827;&#34892;&#32852;&#21512;&#20449;&#24687;&#25277;&#21462;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#38598;&#25104;&#36328;&#23454;&#20363;&#20132;&#20114;&#65292;&#24182;&#36890;&#36807;&#39640;&#38454;&#31070;&#32463;&#35299;&#30721;&#22120;&#35299;&#20915;&#20102;&#31934;&#30830;&#39640;&#38454;&#25512;&#29702;&#30340;&#38590;&#35299;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2212.08929</link><description>&lt;p&gt;
&#29992;&#31070;&#32463;&#39640;&#38454;&#26465;&#20214;&#38543;&#26426;&#22330;&#27169;&#22411;&#24314;&#27169;&#23454;&#20363;&#20449;&#24687;&#20132;&#20114;&#30340;&#32852;&#21512;&#20449;&#24687;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
Modeling Instance Interactions for Joint Information Extraction with Neural High-Order Conditional Random Field. (arXiv:2212.08929v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#39640;&#38454;&#26465;&#20214;&#38543;&#26426;&#22330;&#36827;&#34892;&#32852;&#21512;&#20449;&#24687;&#25277;&#21462;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#38598;&#25104;&#36328;&#23454;&#20363;&#20132;&#20114;&#65292;&#24182;&#36890;&#36807;&#39640;&#38454;&#31070;&#32463;&#35299;&#30721;&#22120;&#35299;&#20915;&#20102;&#31934;&#30830;&#39640;&#38454;&#25512;&#29702;&#30340;&#38590;&#35299;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#32852;&#21512;&#20449;&#24687;&#25277;&#21462;&#27169;&#22411;&#19968;&#33324;&#36890;&#36807;&#34920;&#31034;&#22686;&#24378;&#12289;&#31867;&#22411;&#20381;&#36182;&#35780;&#20998;&#25110;&#20840;&#23616;&#35299;&#30721;&#26469;&#24314;&#27169;&#23454;&#20363;&#65288;&#20363;&#22914;&#20107;&#20214;&#35302;&#21457;&#22120;&#12289;&#23454;&#20307;&#12289;&#35282;&#33394;&#12289;&#20851;&#31995;&#65289;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#25105;&#20204;&#21457;&#29616;&#20197;&#21069;&#30340;&#27169;&#22411;&#36890;&#24120;&#32771;&#34385;&#20102;&#19968;&#23545;&#23454;&#20363;&#30340;&#20108;&#36827;&#21046;&#31867;&#22411;&#20381;&#36182;&#24615;&#35780;&#20998;&#65292;&#24182;&#21033;&#29992;&#35832;&#22914;&#27874;&#26463;&#25628;&#32034;&#20043;&#31867;&#30340;&#23616;&#37096;&#25628;&#32034;&#26469;&#36817;&#20284;&#20840;&#23616;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#38598;&#25104;&#36328;&#23454;&#20363;&#20132;&#20114;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#32852;&#21512;&#20449;&#24687;&#25277;&#21462;&#26694;&#26550;&#65288;CRFIE&#65289;&#65292;&#23558;&#32852;&#21512;&#20449;&#24687;&#25277;&#21462;&#20316;&#20026;&#39640;&#38454;&#26465;&#20214;&#38543;&#26426;&#22330;&#36827;&#34892;&#24314;&#27169;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20108;&#20803;&#22240;&#23376;&#21644;&#19977;&#20803;&#22240;&#23376;&#65292;&#30452;&#25509;&#24314;&#27169;&#19981;&#20165;&#19968;&#23545;&#23454;&#20363;&#32780;&#19988;&#19977;&#20803;&#32452;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#36825;&#20123;&#22240;&#23376;&#20849;&#21516;&#39044;&#27979;&#25152;&#26377;&#23454;&#20363;&#30340;&#26631;&#31614;&#12290;&#20026;&#35299;&#20915;&#31934;&#30830;&#39640;&#38454;&#25512;&#29702;&#30340;&#38590;&#35299;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;&#19968;&#20010;&#39640;&#38454;&#31070;&#32463;&#35299;&#30721;&#22120;&#65292;&#35813;&#35299;&#30721;&#22120;&#26159;&#20174;&#24179;&#22343;&#22330;&#21464;&#20998;&#25512;&#29702;&#26041;&#27861;&#23637;&#24320;&#30340;&#65292;&#23454;&#29616;&#20102;&#19982;&#20197;&#21069;&#27169;&#22411;&#19968;&#33268;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior works on joint Information Extraction (IE) typically model instance (e.g., event triggers, entities, roles, relations) interactions by representation enhancement, type dependencies scoring, or global decoding. We find that the previous models generally consider binary type dependency scoring of a pair of instances, and leverage local search such as beam search to approximate global solutions. To better integrate cross-instance interactions, in this work, we introduce a joint IE framework (CRFIE) that formulates joint IE as a high-order Conditional Random Field. Specifically, we design binary factors and ternary factors to directly model interactions between not only a pair of instances but also triplets. Then, these factors are utilized to jointly predict labels of all instances. To address the intractability problem of exact high-order inference, we incorporate a high-order neural decoder that is unfolded from a mean-field variational inference method, which achieves consistent 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;UniSumm&#32479;&#19968;&#30340;&#23569;&#26679;&#26412;&#25688;&#35201;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#21069;&#32512;&#35843;&#25972;&#24212;&#23545;&#20219;&#20309;&#23569;&#26679;&#26412;&#25688;&#35201;&#20219;&#21153;&#65292;&#21516;&#26102;&#65292;&#20182;&#20204;&#20063;&#21457;&#24067;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;SummZoo&#65292;&#20854;&#30001;8&#20010;&#25688;&#35201;&#20219;&#21153;&#32452;&#25104;&#65292;&#27599;&#20010;&#20219;&#21153;&#37117;&#28085;&#30422;&#20102;&#22810;&#20010;&#23569;&#26679;&#26412;&#26679;&#26412;&#38598;&#65292;&#20197;&#27492;&#26356;&#22909;&#22320;&#35780;&#20272;&#23569;&#26679;&#26412;&#25688;&#35201;&#22120;&#12290;</title><link>http://arxiv.org/abs/2211.09783</link><description>&lt;p&gt;
UniSumm&#21644;SummZoo&#65306;&#23569;&#26679;&#26412;&#25688;&#35201;&#30340;&#32479;&#19968;&#27169;&#22411;&#21644;&#22810;&#26679;&#21270;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
UniSumm and SummZoo: Unified Model and Diverse Benchmark for Few-Shot Summarization. (arXiv:2211.09783v6 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09783
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;UniSumm&#32479;&#19968;&#30340;&#23569;&#26679;&#26412;&#25688;&#35201;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#21069;&#32512;&#35843;&#25972;&#24212;&#23545;&#20219;&#20309;&#23569;&#26679;&#26412;&#25688;&#35201;&#20219;&#21153;&#65292;&#21516;&#26102;&#65292;&#20182;&#20204;&#20063;&#21457;&#24067;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;SummZoo&#65292;&#20854;&#30001;8&#20010;&#25688;&#35201;&#20219;&#21153;&#32452;&#25104;&#65292;&#27599;&#20010;&#20219;&#21153;&#37117;&#28085;&#30422;&#20102;&#22810;&#20010;&#23569;&#26679;&#26412;&#26679;&#26412;&#38598;&#65292;&#20197;&#27492;&#26356;&#22909;&#22320;&#35780;&#20272;&#23569;&#26679;&#26412;&#25688;&#35201;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26114;&#30340;&#27880;&#37322;&#25104;&#26412;&#21644;&#21508;&#31181;&#25688;&#35201;&#20219;&#21153;&#30340;&#22810;&#26679;&#21270;&#38656;&#27714;&#25512;&#21160;&#20102;&#23569;&#26679;&#26412;&#25688;&#35201;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#28044;&#29616;&#20102;&#35768;&#22810;&#25688;&#35201;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#65292;&#30446;&#21069;&#23569;&#26679;&#26412;&#25688;&#35201;&#31995;&#32479;&#30340;&#35757;&#32451;&#33539;&#24335;&#24573;&#30053;&#20102;&#24322;&#26500;&#25968;&#25454;&#38598;&#20013;&#21487;&#33021;&#20849;&#20139;&#30340;&#30693;&#35782;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;UniSumm&#65292;&#19968;&#31181;&#32479;&#19968;&#30340;&#23569;&#26679;&#26412;&#25688;&#35201;&#27169;&#22411;&#65292;&#39044;&#20808;&#35757;&#32451;&#20102;&#22810;&#39033;&#25688;&#35201;&#20219;&#21153;&#65292;&#24182;&#21487;&#20197;&#36827;&#34892;&#21069;&#32512;&#35843;&#25972;&#65292;&#20197;&#22312;&#20219;&#20309;&#23569;&#26679;&#26412;&#25688;&#35201;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#21516;&#26102;&#65292;&#20026;&#20102;&#26356;&#22909;&#22320;&#35780;&#20272;&#23569;&#26679;&#26412;&#25688;&#35201;&#22120;&#65292;&#26681;&#25454;&#22810;&#26679;&#24615;&#21644;&#40065;&#26834;&#24615;&#21407;&#21017;&#65292;&#25105;&#20204;&#32452;&#35013;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#21517;&#20026;SummZoo&#30340;&#26032;&#22522;&#20934;&#12290;&#23427;&#21253;&#25324;8&#20010;&#25688;&#35201;&#20219;&#21153;&#65292;&#27599;&#20010;&#20219;&#21153;&#26377;&#22810;&#20010;&#23569;&#26679;&#26412;&#26679;&#26412;&#38598;&#65292;&#28085;&#30422;&#20102;&#22810;&#31181;&#39046;&#22495;&#12290;&#23454;&#39564;&#32467;&#26524;&#21644;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#19979;&#65292;UniSumm&#22312;SummZoo&#30340;&#25152;&#26377;&#23376;&#20219;&#21153;&#20013;&#22343;&#22823;&#24133;&#20248;&#20110;&#24378;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The high annotation costs and diverse demands of various summarization tasks motivate the development of few-shot summarization. However, despite the emergence of many summarization tasks and datasets, the current training paradigm for few-shot summarization systems ignores potentially shareable knowledge in heterogeneous datasets. To this end, we propose \textsc{UniSumm}, a unified few-shot summarization model pre-trained with multiple summarization tasks and can be prefix-tuned to excel at any few-shot summarization task. Meanwhile, to better evaluate few-shot summarizers, under the principles of diversity and robustness, we assemble and release a new benchmark \textsc{SummZoo}. It consists of $8$ summarization tasks with multiple sets of few-shot samples for each task, covering diverse domains. Experimental results and analysis show that \textsc{UniSumm} outperforms strong baselines by a large margin across all sub-tasks in \textsc{SummZoo} under both automatic and human evaluations
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20351;&#29992;&#26368;&#26032;&#30340;&#22810;&#35821;&#35328;&#24320;&#25918;&#20449;&#24687;&#25552;&#21462;&#25216;&#26415;&#26500;&#24314;&#20102;&#31532;&#19968;&#20010;&#21517;&#20026;mOKB6&#30340;&#22810;&#35821;&#35328;Open KBC&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#34917;&#20840;&#24320;&#25918;&#30693;&#35782;&#24211;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#22810;&#35821;&#35328;&#32452;&#21512;&#36215;&#26469;&#26377;&#19968;&#33268;&#30340;&#22909;&#22788;&#65292;&#20294;&#24403;&#21069;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2211.06959</link><description>&lt;p&gt;
mOKB6&#65306;&#19968;&#31181;&#22810;&#35821;&#35328;&#24320;&#25918;&#30693;&#35782;&#24211;&#34917;&#20840;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
mOKB6: A Multilingual Open Knowledge Base Completion Benchmark. (arXiv:2211.06959v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06959
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20351;&#29992;&#26368;&#26032;&#30340;&#22810;&#35821;&#35328;&#24320;&#25918;&#20449;&#24687;&#25552;&#21462;&#25216;&#26415;&#26500;&#24314;&#20102;&#31532;&#19968;&#20010;&#21517;&#20026;mOKB6&#30340;&#22810;&#35821;&#35328;Open KBC&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#34917;&#20840;&#24320;&#25918;&#30693;&#35782;&#24211;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#22810;&#35821;&#35328;&#32452;&#21512;&#36215;&#26469;&#26377;&#19968;&#33268;&#30340;&#22909;&#22788;&#65292;&#20294;&#24403;&#21069;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#34917;&#20840;&#24320;&#25918;&#30693;&#35782;&#24211;&#65288;Open KB&#65289;&#30340;&#33021;&#21147;&#65292;&#26377;&#21161;&#20110;&#21457;&#29616;&#25991;&#26412;&#20013;&#26080;&#27861;&#30452;&#25509;&#21576;&#29616;&#30340;&#26032;&#20107;&#23454;&#12290;&#28982;&#32780;&#65292;&#24320;&#25918;&#30693;&#35782;&#24211;&#34917;&#20840;&#65288;Open KBC&#65289;&#30740;&#31350;&#33267;&#20170;&#20165;&#23616;&#38480;&#20110;&#20687;&#33521;&#35821;&#36825;&#26679;&#30340;&#36164;&#28304;&#20016;&#23500;&#35821;&#35328;&#12290;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#20351;&#29992;&#26368;&#26032;&#30340;&#22810;&#35821;&#35328;&#24320;&#25918;&#20449;&#24687;&#25552;&#21462;&#25216;&#26415;&#65292;&#26500;&#24314;&#20102;&#31532;&#19968;&#20010;&#21517;&#20026;mOKB6&#30340;&#22810;&#35821;&#35328;Open KBC&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20845;&#31181;&#35821;&#35328;&#65288;&#21253;&#25324;&#33521;&#35821;&#65289;&#30340;&#32500;&#22522;&#30334;&#31185;&#20107;&#23454;&#12290;&#36890;&#36807;&#36827;&#34892;&#22810;&#35821;&#35328;&#20849;&#25351;&#28040;&#35299;&#21644;&#20445;&#30041;&#20165;&#26377;&#23454;&#20307;&#38142;&#25509;&#30340;&#19977;&#20803;&#32452;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#20808;&#21069;&#30340;Open KB&#26500;&#24314;&#27969;&#31243;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#23494;&#38598;&#30340;Open KB&#12290;&#25105;&#20204;&#23581;&#35797;&#20102;&#20960;&#31181;&#27169;&#22411;&#65292;&#24182;&#35266;&#23519;&#21040;&#36890;&#36807;&#20849;&#20139;&#23884;&#20837;&#31354;&#38388;&#20197;&#21450;&#20107;&#23454;&#30340;&#32763;&#35793;&#65292;&#23558;&#22810;&#35821;&#35328;&#32452;&#21512;&#36215;&#26469;&#26377;&#19968;&#33268;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#24403;&#21069;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated completion of open knowledge bases (Open KBs), which are constructed from triples of the form (subject phrase, relation phrase, object phrase), obtained via open information extraction (Open IE) system, are useful for discovering novel facts that may not be directly present in the text. However, research in Open KB completion (Open KBC) has so far been limited to resource-rich languages like English. Using the latest advances in multilingual Open IE, we construct the first multilingual Open KBC dataset, called mOKB6, containing facts from Wikipedia in six languages (including English). Improving the previous Open KB construction pipeline by doing multilingual coreference resolution and keeping only entity-linked triples, we create a dense Open KB. We experiment with several models for the task and observe a consistent benefit of combining languages with the help of shared embedding space as well as translations of facts. We also observe that current multilingual models strugg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#35270;&#35273;&#25512;&#29702;&#30340;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#8212;&#8212;lilGym&#65292;&#23427;&#30001;2661&#20010;&#39640;&#24230;&#32452;&#21512;&#30340;&#20154;&#31867;&#32534;&#20889;&#33258;&#28982;&#35821;&#35328;&#35821;&#21477;&#21644;&#20132;&#20114;&#24335;&#35270;&#35273;&#29615;&#22659;&#32452;&#25104;&#65292;&#24182;&#36890;&#36807;&#27880;&#37322;&#21487;&#25191;&#34892;Python&#31243;&#24207;&#26469;&#23454;&#29616;&#31934;&#30830;&#30340;&#22870;&#21169;&#35745;&#31639;&#12290;&#26412;&#25991;&#30340;&#23454;&#39564;&#32467;&#26524;&#21644;&#20998;&#26512;&#34920;&#26126;&#65292;lilGym&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.01994</link><description>&lt;p&gt;
&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#28982;&#35821;&#35328;&#35270;&#35273;&#25512;&#29702;&#65306;lilGym
&lt;/p&gt;
&lt;p&gt;
lilGym: Natural Language Visual Reasoning with Reinforcement Learning. (arXiv:2211.01994v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#35270;&#35273;&#25512;&#29702;&#30340;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#8212;&#8212;lilGym&#65292;&#23427;&#30001;2661&#20010;&#39640;&#24230;&#32452;&#21512;&#30340;&#20154;&#31867;&#32534;&#20889;&#33258;&#28982;&#35821;&#35328;&#35821;&#21477;&#21644;&#20132;&#20114;&#24335;&#35270;&#35273;&#29615;&#22659;&#32452;&#25104;&#65292;&#24182;&#36890;&#36807;&#27880;&#37322;&#21487;&#25191;&#34892;Python&#31243;&#24207;&#26469;&#23454;&#29616;&#31934;&#30830;&#30340;&#22870;&#21169;&#35745;&#31639;&#12290;&#26412;&#25991;&#30340;&#23454;&#39564;&#32467;&#26524;&#21644;&#20998;&#26512;&#34920;&#26126;&#65292;lilGym&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26377;&#20851;&#35821;&#35328;&#26465;&#20214;&#19979;&#24378;&#21270;&#23398;&#20064;&#22312;&#35270;&#35273;&#29615;&#22659;&#19979;&#30340;&#22522;&#20934;&#27979;&#35797;&#8212;&#8212;lilGym&#12290;lilGym&#22522;&#20110;2661&#20010;&#39640;&#24230;&#32452;&#21512;&#30340;&#20154;&#31867;&#32534;&#20889;&#30340;&#33258;&#28982;&#35821;&#35328;&#38472;&#36848;&#65292;&#36825;&#20123;&#38472;&#36848;&#26159;&#22522;&#20110;&#19968;&#20010;&#20132;&#20114;&#24335;&#35270;&#35273;&#29615;&#22659;&#30340;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#27599;&#31181;&#21487;&#33021;&#30340;&#19990;&#30028;&#29366;&#24577;&#19979;&#65292;&#36890;&#36807;&#20026;&#25152;&#26377;&#35821;&#21477;&#27880;&#37322;&#21487;&#25191;&#34892;&#30340;Python&#31243;&#24207;&#65292;&#23454;&#29616;&#20102;&#31934;&#30830;&#30340;&#22870;&#21169;&#35745;&#31639;&#12290;&#27599;&#20010;&#35821;&#21477;&#37117;&#19982;&#22810;&#20010;&#36215;&#22987;&#29366;&#24577;&#21644;&#22870;&#21169;&#20989;&#25968;&#37197;&#23545;&#65292;&#20197;&#24418;&#25104;&#25968;&#21315;&#20010;&#19981;&#21516;&#38590;&#24230;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;&#27169;&#22411;&#21644;&#23398;&#20064;&#26426;&#21046;&#36827;&#34892;&#20102;lilGym&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#21644;&#20998;&#26512;&#34920;&#26126;&#65292;&#34429;&#28982;&#29616;&#26377;&#30340;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#36739;&#39640;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;lilGym&#24418;&#25104;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;lilGym&#21487;&#20197;&#22312; https://lil.nlp.cornell.edu/lilgym/ &#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present lilGym, a new benchmark for language-conditioned reinforcement learning in visual environments. lilGym is based on 2,661 highly-compositional human-written natural language statements grounded in an interactive visual environment. We introduce a new approach for exact reward computation in every possible world state by annotating all statements with executable Python programs. Each statement is paired with multiple start states and reward functions to form thousands of distinct Markov Decision Processes of varying difficulty. We experiment with lilGym with different models and learning regimes. Our results and analysis show that while existing methods are able to achieve non-trivial performance, lilGym forms a challenging open problem. lilGym is available at https://lil.nlp.cornell.edu/lilgym/.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#22810;&#20219;&#21153;&#24494;&#35843;&#23454;&#29616;&#36328;&#35821;&#35328;&#27867;&#21270;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#33521;&#35821;&#25552;&#31034;&#19979;&#65292;&#23545;&#22823;&#22411;&#22810;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33521;&#35821;&#20219;&#21153;&#30340;&#24494;&#35843;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#20165;&#20986;&#29616;&#22312;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#30340;&#38750;&#33521;&#35821;&#35821;&#35328;&#30340;&#20219;&#21153;&#27867;&#21270;&#65292;&#24182;&#19988;&#20351;&#29992;&#33521;&#35821;&#25552;&#31034;&#36827;&#34892;&#22810;&#35821;&#35328;&#20219;&#21153;&#30340;&#24494;&#35843;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#22312;&#33521;&#35821;&#21644;&#38750;&#33521;&#35821;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#21508;&#31181;&#38646;-shot&#32467;&#26524;&#30340;&#26368;&#26032;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2211.01786</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#20219;&#21153;&#24494;&#35843;&#23454;&#29616;&#36328;&#35821;&#35328;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Crosslingual Generalization through Multitask Finetuning. (arXiv:2211.01786v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01786
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#22810;&#20219;&#21153;&#24494;&#35843;&#23454;&#29616;&#36328;&#35821;&#35328;&#27867;&#21270;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#33521;&#35821;&#25552;&#31034;&#19979;&#65292;&#23545;&#22823;&#22411;&#22810;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33521;&#35821;&#20219;&#21153;&#30340;&#24494;&#35843;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#20165;&#20986;&#29616;&#22312;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#30340;&#38750;&#33521;&#35821;&#35821;&#35328;&#30340;&#20219;&#21153;&#27867;&#21270;&#65292;&#24182;&#19988;&#20351;&#29992;&#33521;&#35821;&#25552;&#31034;&#36827;&#34892;&#22810;&#35821;&#35328;&#20219;&#21153;&#30340;&#24494;&#35843;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#22312;&#33521;&#35821;&#21644;&#38750;&#33521;&#35821;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#21508;&#31181;&#38646;-shot&#32467;&#26524;&#30340;&#26368;&#26032;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#35777;&#26126;&#65292;&#22810;&#20219;&#21153;&#24494;&#35843;&#21487;&#20197;&#24110;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#22330;&#26223;&#19979;&#25512;&#24191;&#21040;&#26032;&#30340;&#20219;&#21153;&#65292;&#20294;&#30446;&#21069;MTF&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#33521;&#35821;&#25968;&#25454;&#21644;&#27169;&#22411;&#19978;&#12290;&#25105;&#20204;&#23558;MTF&#24212;&#29992;&#20110;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;BLOOM&#21644;mT5&#27169;&#22411;&#31995;&#21015;&#65292;&#29983;&#25104;&#20102;&#32463;&#36807;&#24494;&#35843;&#30340;&#21464;&#20307;BLOOMZ&#21644;mT0&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#33521;&#35821;&#25552;&#31034;&#19979;&#65292;&#23545;&#22823;&#22411;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33521;&#35821;&#20219;&#21153;&#30340;&#24494;&#35843;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#20165;&#20986;&#29616;&#22312;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#30340;&#38750;&#33521;&#35821;&#35821;&#35328;&#30340;&#20219;&#21153;&#27867;&#21270;&#12290;&#20351;&#29992;&#33521;&#35821;&#25552;&#31034;&#36827;&#34892;&#22810;&#35821;&#35328;&#20219;&#21153;&#30340;&#24494;&#35843;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#22312;&#33521;&#35821;&#21644;&#38750;&#33521;&#35821;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#21508;&#31181;&#38646;-shot&#32467;&#26524;&#30340;&#26368;&#26032;&#27700;&#24179;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#22312;&#33521;&#35821;&#32763;&#35793;&#20026;&#27599;&#20010;&#25968;&#25454;&#38598;&#30340;&#35821;&#35328;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#22810;&#35821;&#35328;&#20219;&#21153;&#24494;&#35843;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#36825;&#20123;&#26426;&#22120;&#32763;&#35793;&#25552;&#31034;&#19978;&#35757;&#32451;&#21487;&#20197;&#22312;&#21508;&#33258;&#35821;&#35328;&#20013;&#26356;&#22909;&#22320;&#23436;&#25104;&#20154;&#20889;&#30340;&#25552;&#31034;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;m
&lt;/p&gt;
&lt;p&gt;
Multitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task generalization to non-English languages that appear only in the pretraining corpus. Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results. We also investigate finetuning on multilingual tasks with prompts that have been machine-translated from English to match the language of each dataset. We find training on these machine-translated prompts leads to better performance on human-written prompts in the respective languages. Surprisingly, we find m
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#20316;&#25512;&#29702;&#35825;&#23548;&#30340;&#35821;&#35328;&#27169;&#22411;&#8212;&#8212;CoRe&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35299;&#20915;&#25968;&#23398;&#24212;&#29992;&#39064;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;CoRe &#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.16257</link><description>&lt;p&gt;
&#36890;&#36807;&#21512;&#20316;&#25512;&#29702;&#35825;&#23548;&#30340;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#25968;&#23398;&#24212;&#29992;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving Math Word Problems via Cooperative Reasoning induced Language Models. (arXiv:2210.16257v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16257
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#20316;&#25512;&#29702;&#35825;&#23548;&#30340;&#35821;&#35328;&#27169;&#22411;&#8212;&#8212;CoRe&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35299;&#20915;&#25968;&#23398;&#24212;&#29992;&#39064;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;CoRe &#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411; (PLMs) &#20026;&#38656;&#35201;&#39640;&#27700;&#24179;&#26234;&#33021;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#65288;&#22914;&#25968;&#23398;&#24212;&#29992;&#39064;&#65289;&#24102;&#26469;&#20102;&#26032;&#30340;&#26426;&#36935;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#24212;&#29992;&#29616;&#26377;&#30340; PLMs &#21040;&#25968;&#23398;&#24212;&#29992;&#39064;&#19978;&#20250;&#22833;&#36133;&#65292;&#22240;&#20026;&#20854;&#29983;&#25104;&#30340;&#36807;&#31243;&#32570;&#20047;&#36275;&#22815;&#30340;&#30417;&#30563;&#65292;&#32570;&#20047;&#20687;&#20154;&#31867;&#19968;&#26679;&#30340;&#24555;&#36895;&#36866;&#24212;&#24615;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#20154;&#31867;&#30340;&#25512;&#29702;&#36807;&#31243;&#26377;&#19968;&#20010;&#21452;&#37325;&#25512;&#29702;&#26694;&#26550;&#65292;&#21253;&#25324;&#19968;&#20010;&#21363;&#26102;&#21453;&#24212;&#31995;&#32479; (system 1) &#21644;&#19968;&#20010;&#31934;&#32454;&#25512;&#29702;&#31995;&#32479; (system 2)&#65292;&#25972;&#20010;&#25512;&#29702;&#36807;&#31243;&#30001;&#23427;&#20204;&#30340;&#20132;&#20114;&#20915;&#23450;&#12290;&#36825;&#21551;&#21457;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21512;&#20316;&#25512;&#29702;&#35825;&#23548;&#30340; PLM &#27169;&#22411;&#65292;&#31216;&#20026; Cooperative Reasoning (CoRe)&#65292;&#24471;&#21040;&#20102;&#19968;&#20010;&#20687;&#20154;&#31867;&#25512;&#29702;&#32467;&#26500;&#30340;&#26550;&#26500;&#65292;&#20854;&#20013; system 1 &#20316;&#20026;&#29983;&#25104;&#22120;&#65292;system 2 &#20316;&#20026;&#39564;&#35777;&#22120;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#29983;&#25104;&#22120;&#36127;&#36131;&#20135;&#29983;&#25512;&#29702;&#36335;&#24452;&#65292;&#39564;&#35777;&#22120;&#29992;&#20110;&#30417;&#30563;&#35780;&#20272;&#20197;&#33719;&#21462;&#21487;&#38752;&#30340;&#21453;&#39304;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CoRe &#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale pre-trained language models (PLMs) bring new opportunities to challenging problems, especially those that need high-level intelligence, such as the math word problem (MWPs). However, directly applying existing PLMs to MWPs can fail as the generation process lacks sufficient supervision and thus lacks fast adaptivity as humans. We notice that human reasoning has a dual reasoning framework that consists of an immediate reaction system (system 1) and a delicate reasoning system (system 2), where the entire reasoning is determined by their interaction. This inspires us to develop a cooperative reasoning-induced PLM for solving MWPs, called Cooperative Reasoning (CoRe), resulting in a human-like reasoning architecture with system 1 as the generator and system 2 as the verifier. In our approach, the generator is responsible for generating reasoning paths, and the verifiers are used to supervise the evaluation in order to obtain reliable feedback for the generator. We evaluate our
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#26500;&#24314;&#20855;&#26377;&#39640;&#30693;&#35782;&#35206;&#30422;&#29575;&#21644;&#22823;&#35268;&#27169;&#22810;&#36339;&#36335;&#24452;&#30340;Dense-ATOMIC&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Rel-CSKGC&#30340;CSKG&#23436;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#19977;&#20803;&#32452;&#30340;&#22836;&#20107;&#20214;&#21644;&#23614;&#20107;&#20214;&#21518;&#30340;&#20851;&#31995;&#65292;&#24182;&#30456;&#24212;&#26500;&#24314;Dense-ATOMIC&#65292;&#36825;&#30456;&#23545;&#20110;&#24378;&#22522;&#32447;&#26041;&#27861;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2210.07621</link><description>&lt;p&gt;
Dense-ATOMIC&#65306;&#21521;&#20855;&#26377;&#39640;&#30693;&#35782;&#35206;&#30422;&#29575;&#21644;&#22823;&#35268;&#27169;&#22810;&#36339;&#36335;&#24452;&#30340;&#23494;&#38598;&#36830;&#25509;ATOMIC&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Dense-ATOMIC: Towards Densely-connected ATOMIC with High Knowledge Coverage and Massive Multi-hop Paths. (arXiv:2210.07621v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07621
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#26500;&#24314;&#20855;&#26377;&#39640;&#30693;&#35782;&#35206;&#30422;&#29575;&#21644;&#22823;&#35268;&#27169;&#22810;&#36339;&#36335;&#24452;&#30340;Dense-ATOMIC&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Rel-CSKGC&#30340;CSKG&#23436;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#19977;&#20803;&#32452;&#30340;&#22836;&#20107;&#20214;&#21644;&#23614;&#20107;&#20214;&#21518;&#30340;&#20851;&#31995;&#65292;&#24182;&#30456;&#24212;&#26500;&#24314;Dense-ATOMIC&#65292;&#36825;&#30456;&#23545;&#20110;&#24378;&#22522;&#32447;&#26041;&#27861;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ATOMIC&#26159;&#19968;&#20010;&#21253;&#21547;&#24120;&#35782;&#30693;&#35782;&#19977;&#20803;&#32452;&#65288;&#21363;{&#22836;&#20107;&#20214;&#12289;&#20851;&#31995;&#12289;&#23614;&#20107;&#20214;}&#65289;&#30340;&#22823;&#35268;&#27169;&#24120;&#35782;&#30693;&#35782;&#22270;&#35889;&#65288;CSKG&#65289;&#12290;ATOMIC&#30340;&#21333;&#36339;&#27880;&#37322;&#26041;&#24335;&#20351;&#20854;&#25104;&#20026;&#29420;&#31435;&#30340;&#20108;&#20998;&#22270;&#38598;&#21512;&#65292;&#24573;&#30053;&#20102;&#19981;&#21516;&#20108;&#20998;&#22270;&#20013;&#20107;&#20214;&#20043;&#38388;&#30340;&#35768;&#22810;&#38142;&#25509;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#30693;&#35782;&#35206;&#30422;&#21644;&#22810;&#36339;&#36335;&#24452;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;&#26412;&#25991;&#26088;&#22312;&#26500;&#24314;&#20855;&#26377;&#39640;&#30693;&#35782;&#35206;&#30422;&#29575;&#21644;&#22823;&#35268;&#27169;&#22810;&#36339;&#36335;&#24452;&#30340;Dense-ATOMIC&#12290;&#39318;&#20808;&#65292;&#23558;ATOMIC&#20013;&#30340;&#20107;&#20214;&#26631;&#20934;&#21270;&#20026;&#19968;&#33268;&#30340;&#27169;&#24335;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Rel-CSKGC&#30340;CSKG&#23436;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#19977;&#20803;&#32452;&#30340;&#22836;&#20107;&#20214;&#21644;&#23614;&#20107;&#20214;&#21518;&#30340;&#20851;&#31995;&#65292;&#24182;&#22522;&#20110;ATOMIC&#20013;&#29616;&#26377;&#30340;&#19977;&#20803;&#32452;&#35757;&#32451;&#19968;&#20010;CSKG&#23436;&#25104;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#35813;&#27169;&#22411;&#26469;&#23436;&#21892;ATOMIC&#20013;&#30340;&#32570;&#22833;&#38142;&#25509;&#65292;&#24182;&#30456;&#24212;&#26500;&#24314;Dense-ATOMIC&#12290;&#22312;ATOMIC&#30340;&#27880;&#37322;&#23376;&#22270;&#19978;&#30340;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#34920;&#26126;&#65292;Rel-CSKGC&#30456;&#23545;&#20110;&#24378;&#22522;&#32447;&#26041;&#27861;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
ATOMIC is a large-scale commonsense knowledge graph (CSKG) containing everyday if-then knowledge triplets, i.e., {head event, relation, tail event}. The one-hop annotation manner made ATOMIC a set of independent bipartite graphs, which ignored the numerous links between events in different bipartite graphs and consequently caused shortages in knowledge coverage and multi-hop paths. In this work, we aim to construct Dense-ATOMIC with high knowledge coverage and massive multi-hop paths. The events in ATOMIC are normalized to a consistent pattern at first. We then propose a CSKG completion method called Rel-CSKGC to predict the relation given the head event and the tail event of a triplet, and train a CSKG completion model based on existing triplets in ATOMIC. We finally utilize the model to complete the missing links in ATOMIC and accordingly construct Dense-ATOMIC. Both automatic and human evaluation on an annotated subgraph of ATOMIC demonstrate the advantage of Rel-CSKGC over strong b
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#31350;&#20102;&#35821;&#35328;&#23398;&#20064;&#32773;&#26159;&#21542;&#21487;&#20197;&#20026;&#26631;&#27880;&#22522;&#20934;&#25968;&#25454;&#38598;&#20570;&#20986;&#36129;&#29486;&#65292;&#21457;&#29616;&#22312;&#25552;&#20379;&#39069;&#22806;&#36164;&#28304;&#30340;&#24110;&#21161;&#19979;&#65292;&#20855;&#26377;&#20013;&#39640;&#32423;&#35821;&#35328;&#33021;&#21147;&#30340;&#23398;&#20064;&#32773;&#33021;&#22815;&#25552;&#20379;&#20934;&#30830;&#30340;&#26631;&#31614;&#12290;</title><link>http://arxiv.org/abs/2210.06828</link><description>&lt;p&gt;
&#37325;&#26032;&#32771;&#34385;&#26631;&#27880;&#65306;&#35821;&#35328;&#23398;&#20064;&#32773;&#33021;&#20570;&#20986;&#36129;&#29486;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Rethinking Annotation: Can Language Learners Contribute?. (arXiv:2210.06828v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06828
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#31350;&#20102;&#35821;&#35328;&#23398;&#20064;&#32773;&#26159;&#21542;&#21487;&#20197;&#20026;&#26631;&#27880;&#22522;&#20934;&#25968;&#25454;&#38598;&#20570;&#20986;&#36129;&#29486;&#65292;&#21457;&#29616;&#22312;&#25552;&#20379;&#39069;&#22806;&#36164;&#28304;&#30340;&#24110;&#21161;&#19979;&#65292;&#20855;&#26377;&#20013;&#39640;&#32423;&#35821;&#35328;&#33021;&#21147;&#30340;&#23398;&#20064;&#32773;&#33021;&#22815;&#25552;&#20379;&#20934;&#30830;&#30340;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32773;&#20204;&#20256;&#32479;&#19978;&#20250;&#25307;&#21215;&#27597;&#35821;&#20026;&#30446;&#26631;&#35821;&#35328;&#30340;&#20154;&#20026;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#25552;&#20379;&#27880;&#37322;&#12290;&#28982;&#32780;&#65292;&#26377;&#20123;&#35821;&#35328;&#24456;&#38590;&#25214;&#21040;&#27597;&#35821;&#20154;&#22763;&#65292;&#22240;&#27492;&#25214;&#21040;&#23398;&#20064;&#36825;&#20123;&#35821;&#35328;&#30340;&#20154;&#20026;&#25968;&#25454;&#38598;&#25552;&#20379;&#27880;&#37322;&#26377;&#30528;&#19968;&#23450;&#30340;&#24110;&#21161;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35821;&#35328;&#23398;&#20064;&#32773;&#26159;&#21542;&#21487;&#20197;&#20026;&#22522;&#20934;&#25968;&#25454;&#38598;&#25552;&#20379;&#27880;&#37322;&#12290;&#22312;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#27880;&#37322;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#25307;&#21215;&#20102;36&#21517;&#35821;&#35328;&#23398;&#20064;&#32773;&#65292;&#25552;&#20379;&#20102;&#20004;&#31181;&#39069;&#22806;&#36164;&#28304;&#65288;&#35789;&#20856;&#21644;&#26426;&#22120;&#32763;&#35793;&#30340;&#21477;&#23376;&#65289;&#65292;&#24182;&#36827;&#34892;&#20102;&#36855;&#20320;&#27979;&#35797;&#26469;&#27979;&#35797;&#20182;&#20204;&#30340;&#35821;&#35328;&#33021;&#21147;&#12290;&#25105;&#20204;&#38024;&#23545;&#33521;&#35821;&#12289;&#38889;&#35821;&#21644;&#21360;&#23612;&#35821;&#36825;&#19977;&#31181;&#35821;&#35328;&#20197;&#21450;&#24773;&#24863;&#20998;&#26512;&#12289;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#12289;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#36825;&#22235;&#20010;NLP&#20219;&#21153;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35821;&#35328;&#23398;&#20064;&#32773;&#65292;&#23588;&#20854;&#26159;&#37027;&#20123;&#20855;&#26377;&#20013;&#32423;&#25110;&#39640;&#32423;&#35821;&#35328;&#33021;&#21147;&#30340;&#20154;&#65292;&#22312;&#39069;&#22806;&#36164;&#28304;&#30340;&#24110;&#21161;&#19979;&#21487;&#20197;&#25552;&#20379;&#30456;&#24403;&#20934;&#30830;&#30340;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
Researchers have traditionally recruited native speakers to provide annotations for widely used benchmark datasets. However, there are languages for which recruiting native speakers can be difficult, and it would help to find learners of those languages to annotate the data. In this paper, we investigate whether language learners can contribute annotations to benchmark datasets. In a carefully controlled annotation experiment, we recruit 36 language learners, provide two types of additional resources (dictionaries and machine-translated sentences), and perform mini-tests to measure their language proficiency. We target three languages, English, Korean, and Indonesian, and the four NLP tasks of sentiment analysis, natural language inference, named entity recognition, and machine reading comprehension. We find that language learners, especially those with intermediate or advanced levels of language proficiency, are able to provide fairly accurate labels with the help of additional resour
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#19981;&#21464;&#30340;&#31163;&#25955;&#35821;&#38899;&#34920;&#31034;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#20854;&#22312;&#29983;&#25104;&#24335;&#35821;&#38899;&#35821;&#35328;&#24314;&#27169;&#20013;&#30340;&#40065;&#26834;&#24615;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;transformer-based&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#19968;&#31181;&#38750;&#32447;&#24615;&#37327;&#21270;&#26041;&#27861;&#26469;&#23398;&#20064;&#22686;&#24378;&#19981;&#21464;&#34920;&#31034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#30456;&#23545;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#20855;&#26377;&#26174;&#33879;&#30340;&#40065;&#26834;&#24615;&#25913;&#36827;&#65292;&#24182;&#22312;&#35821;&#38899;&#29983;&#25104;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#31454;&#20105;&#24615;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2209.15483</link><description>&lt;p&gt;
&#38754;&#21521;&#29983;&#25104;&#24335;&#35821;&#38899;&#35821;&#35328;&#24314;&#27169;&#30340;&#22686;&#24378;&#19981;&#21464;&#31163;&#25955;&#34920;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Augmentation Invariant Discrete Representation for Generative Spoken Language Modeling. (arXiv:2209.15483v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.15483
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#19981;&#21464;&#30340;&#31163;&#25955;&#35821;&#38899;&#34920;&#31034;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#20854;&#22312;&#29983;&#25104;&#24335;&#35821;&#38899;&#35821;&#35328;&#24314;&#27169;&#20013;&#30340;&#40065;&#26834;&#24615;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;transformer-based&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#19968;&#31181;&#38750;&#32447;&#24615;&#37327;&#21270;&#26041;&#27861;&#26469;&#23398;&#20064;&#22686;&#24378;&#19981;&#21464;&#34920;&#31034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#30456;&#23545;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#20855;&#26377;&#26174;&#33879;&#30340;&#40065;&#26834;&#24615;&#25913;&#36827;&#65292;&#24182;&#22312;&#35821;&#38899;&#29983;&#25104;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#31454;&#20105;&#24615;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#35821;&#38899;&#35821;&#35328;&#24314;&#27169;&#30340;&#30740;&#31350;&#20851;&#27880;&#20110;&#20351;&#29992;&#21407;&#22987;&#38899;&#39057;&#35760;&#24405;&#20248;&#21270;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#19981;&#20351;&#29992;&#20219;&#20309;&#25991;&#26412;&#30417;&#30563;&#12290;&#36825;&#31181;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#20351;&#29992;&#20174;&#33258;&#30417;&#30563;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#37327;&#21270;&#24471;&#21040;&#30340;&#31163;&#25955;&#21333;&#20301;&#36827;&#34892;&#25805;&#20316;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25913;&#21892;&#31163;&#25955;&#36755;&#20837;&#34920;&#31034;&#23545;&#29983;&#25104;&#24335;&#35821;&#38899;&#35821;&#35328;&#24314;&#27169;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#22914;&#20309;&#27979;&#37327;&#36825;&#20123;&#34920;&#31034;&#23545;&#21508;&#31181;&#19981;&#20250;&#25913;&#21464;&#35821;&#38899;&#20449;&#24687;&#65288;&#20363;&#22914;&#26102;&#38388;&#25289;&#20280;&#65289;&#30340;&#20449;&#21495;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#34920;&#31034;&#27169;&#22411;&#32570;&#20047;&#23545;&#27492;&#31867;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#38754;&#21521;&#29983;&#25104;&#24335;&#35821;&#38899;&#35821;&#35328;&#24314;&#27169;&#30340;&#40065;&#26834;&#31163;&#25955;&#35821;&#38899;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#38024;&#23545;&#25968;&#25454;&#22686;&#24378;&#30340;&#19981;&#21464;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#32447;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#20197;&#23398;&#20064;&#22686;&#24378;&#19981;&#21464;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#22312;&#40065;&#26834;&#24615;&#19978;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#24182;&#22312;&#35821;&#38899;&#29983;&#25104;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Spoken Language Modeling research focuses on optimizing speech Language Models (LMs) using raw audio recordings without accessing any textual supervision. Such speech LMs usually operate over discrete units obtained from quantizing internal representations of self-supervised models. Although such units show impressive modeling results, their robustness capabilities have not been extensively investigated. This work focuses on improving the robustness of discrete input representations for generative spoken language modeling. First, we formally define how to measure the robustness of such representations to various signal variations that do not alter the spoken information (e.g., time-stretch). Next, we empirically demonstrate how current state-of-the-art representation models lack robustness to such variations. To overcome this, we propose an effective and efficient method to learn robust discrete speech representation for generative spoken language modeling. The proposed appr
&lt;/p&gt;</description></item><item><title>PaLI&#26159;&#19968;&#31181;&#32852;&#21512;&#32553;&#25918;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;-&#22270;&#20687;&#27169;&#22411;&#65292;&#21487;&#23545;&#22270;&#20687;&#21644;&#25991;&#26412;&#36827;&#34892;&#24314;&#27169;&#21644;&#25191;&#34892;&#35768;&#22810;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#21033;&#29992;Transformer&#21644;Vision Transformer&#31561;&#20808;&#21069;&#30340;&#33021;&#21147;&#21644;&#25104;&#26412;&#12290;&#32852;&#21512;&#32553;&#25918;&#22312;&#27492;&#20219;&#21153;&#20013;&#24456;&#37325;&#35201;&#65292;&#25152;&#20197;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;40&#20159;&#21442;&#25968;&#30340;Vision Transformer&#65292;&#20197;&#20415;&#21033;&#29992;&#26356;&#22823;&#23481;&#37327;&#30340;&#35270;&#35273;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2209.06794</link><description>&lt;p&gt;
PaLI: &#19968;&#31181;&#32852;&#21512;&#32553;&#25918;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;-&#22270;&#20687;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PaLI: A Jointly-Scaled Multilingual Language-Image Model. (arXiv:2209.06794v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.06794
&lt;/p&gt;
&lt;p&gt;
PaLI&#26159;&#19968;&#31181;&#32852;&#21512;&#32553;&#25918;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;-&#22270;&#20687;&#27169;&#22411;&#65292;&#21487;&#23545;&#22270;&#20687;&#21644;&#25991;&#26412;&#36827;&#34892;&#24314;&#27169;&#21644;&#25191;&#34892;&#35768;&#22810;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#21033;&#29992;Transformer&#21644;Vision Transformer&#31561;&#20808;&#21069;&#30340;&#33021;&#21147;&#21644;&#25104;&#26412;&#12290;&#32852;&#21512;&#32553;&#25918;&#22312;&#27492;&#20219;&#21153;&#20013;&#24456;&#37325;&#35201;&#65292;&#25152;&#20197;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;40&#20159;&#21442;&#25968;&#30340;Vision Transformer&#65292;&#20197;&#20415;&#21033;&#29992;&#26356;&#22823;&#23481;&#37327;&#30340;&#35270;&#35273;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#32553;&#25918;&#21644;&#28789;&#27963;&#30340;&#20219;&#21153;&#25509;&#21475;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;PaLI&#65288;Pathways Language and Image model&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#27169;&#22411;&#65292;&#21487;&#23558;&#36825;&#31181;&#26041;&#27861;&#25193;&#23637;&#21040;&#35821;&#35328;&#21644;&#35270;&#35273;&#30340;&#32852;&#21512;&#24314;&#27169;&#12290;PaLI&#22522;&#20110;&#35270;&#35273;&#21644;&#25991;&#26412;&#36755;&#20837;&#29983;&#25104;&#25991;&#26412;&#65292;&#24182;&#36890;&#36807;&#27492;&#25509;&#21475;&#25191;&#34892;&#35768;&#22810;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#22312;&#35768;&#22810;&#35821;&#35328;&#20013;&#23436;&#25104;&#12290;&#20026;&#20102;&#35757;&#32451;PaLI&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#22823;&#22411;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#21644;Vision Transformers&#65288;ViT&#65289;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#21033;&#29992;&#23427;&#20204;&#29616;&#26377;&#30340;&#33021;&#21147;&#24182;&#21033;&#29992;&#35757;&#32451;&#23427;&#20204;&#30340;&#37325;&#22823;&#25104;&#26412;&#12290;&#25105;&#20204;&#21457;&#29616;&#32852;&#21512;&#32553;&#25918;&#35270;&#35273;&#21644;&#35821;&#35328;&#32452;&#20214;&#24456;&#37325;&#35201;&#12290;&#30001;&#20110;&#29616;&#26377;&#30340;&#35821;&#35328;Transformer&#27604;&#23427;&#20204;&#30340;&#35270;&#35273;&#23545;&#24212;&#29289;&#35201;&#22823;&#24471;&#22810;&#65292;&#22240;&#27492;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#22823;&#22411;&#30340;40&#20159;&#21442;&#25968;ViT&#65288;ViT-e&#65289;&#26469;&#37327;&#21270;&#21363;&#20351;&#26356;&#22823;&#23481;&#37327;&#30340;&#35270;&#35273;&#27169;&#22411;&#30340;&#22909;&#22788;&#12290;&#20026;&#20102;&#35757;&#32451;PaLI&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#26032;&#30340;&#22270;&#20687;-&#25991;&#26412;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#22823;&#22411;&#22810;&#35821;&#35328;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#28151;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective scaling and a flexible task interface enable large language models to excel at many tasks. We present PaLI (Pathways Language and Image model), a model that extends this approach to the joint modeling of language and vision. PaLI generates text based on visual and textual inputs, and with this interface performs many vision, language, and multimodal tasks, in many languages. To train PaLI, we make use of large pre-trained encoder-decoder language models and Vision Transformers (ViTs). This allows us to capitalize on their existing capabilities and leverage the substantial cost of training them. We find that joint scaling of the vision and language components is important. Since existing Transformers for language are much larger than their vision counterparts, we train a large, 4-billion parameter ViT (ViT-e) to quantify the benefits from even larger-capacity vision models. To train PaLI, we create a large multilingual mix of pretraining tasks, based on a new image-text traini
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;BabelNet&#30340;&#22810;&#35821;&#35328;&#21516;&#20041;&#35789;&#38598;&#65292;&#35753;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#21464;&#21387;&#22120;&#36827;&#34892;&#35789;&#27719;&#19987;&#38376;&#21270;&#22788;&#29702;&#65292;&#33021;&#22815;&#22823;&#22823;&#25552;&#39640;&#36328;&#35821;&#35328;&#35789;&#23884;&#20837;&#12289;&#22810;&#35821;&#35328;&#35821;&#20041;&#26816;&#32034;&#21644;&#22810;&#35821;&#35328;&#25991;&#26723;&#20998;&#31867;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2208.01018</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#21464;&#21387;&#22120;&#30340;&#35789;&#27719;&#19987;&#38376;&#21270;
&lt;/p&gt;
&lt;p&gt;
Massively Multilingual Lexical Specialization of Multilingual Transformers. (arXiv:2208.01018v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.01018
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;BabelNet&#30340;&#22810;&#35821;&#35328;&#21516;&#20041;&#35789;&#38598;&#65292;&#35753;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#21464;&#21387;&#22120;&#36827;&#34892;&#35789;&#27719;&#19987;&#38376;&#21270;&#22788;&#29702;&#65292;&#33021;&#22815;&#22823;&#22823;&#25552;&#39640;&#36328;&#35821;&#35328;&#35789;&#23884;&#20837;&#12289;&#22810;&#35821;&#35328;&#35821;&#20041;&#26816;&#32034;&#21644;&#22810;&#35821;&#35328;&#25991;&#26723;&#20998;&#31867;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#20027;&#35201;&#29992;&#20316;&#36890;&#29992;&#25991;&#26412;&#32534;&#30721;&#22120;&#65292;&#21487;&#20197;&#24494;&#35843;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#23427;&#20204;&#20063;&#21487;&#20197;&#34987;&#37325;&#26500;&#20026;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35789;&#34920;&#31034;&#65292;&#20174;&#32780;&#22312;&#31867;&#22411;&#32423;&#21035;&#30340;&#35789;&#27719;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#26412;&#25991;&#30340;&#30740;&#31350;&#37325;&#28857;&#26159;&#23558;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#21464;&#21387;&#22120;&#65288;MMTs&#65292;&#20363;&#22914;mBERT&#25110;XLM-R&#65289;&#26292;&#38706;&#20110;&#35268;&#27169;&#21270;&#30340;&#22810;&#35821;&#35328;&#35789;&#27719;&#30693;&#35782;&#20013;&#65292;&#21033;&#29992;BabelNet&#20316;&#20026;&#21487;&#29992;&#30340;&#22810;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#31867;&#22411;&#32423;&#21035;&#35789;&#27719;&#30693;&#35782;&#30340;&#20016;&#23500;&#26469;&#28304;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;BabelNet&#30340;&#22810;&#35821;&#35328;&#21516;&#20041;&#35789;&#38598;&#21019;&#24314;&#36328;50&#31181;&#35821;&#35328;&#30340;&#21516;&#20041;&#35789;&#23545;&#65288;&#25110;&#21516;&#20041;&#35789;-&#35789;&#27719;&#23545;&#65289;&#65292;&#28982;&#21518;&#36890;&#36807;&#23545;&#27604;&#30446;&#26631;&#24341;&#23548;MMTs&#65288;mBERT&#21644;XLM-R&#65289;&#36827;&#34892;&#35789;&#27719;&#19987;&#38376;&#21270;&#22788;&#29702;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#35789;&#27719;&#19987;&#38376;&#21270;&#26041;&#27861;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#36328;&#35821;&#35328;&#35789;&#23884;&#20837;&#12289;&#22810;&#35821;&#35328;&#35821;&#20041;&#26816;&#32034;&#21644;&#22810;&#35821;&#35328;&#25991;&#26723;&#20998;&#31867;&#30340;&#36136;&#37327;&#65292;&#20248;&#20110;&#20351;&#29992;&#21333;&#35821;&#21644;&#21452;&#35821;&#32422;&#26463;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
While pretrained language models (PLMs) primarily serve as general-purpose text encoders that can be fine-tuned for a wide variety of downstream tasks, recent work has shown that they can also be rewired to produce high-quality word representations (i.e., static word embeddings) and yield good performance in type-level lexical tasks. While existing work primarily focused on the lexical specialization of monolingual PLMs with immense quantities of monolingual constraints, in this work we expose massively multilingual transformers (MMTs, e.g., mBERT or XLM-R) to multilingual lexical knowledge at scale, leveraging BabelNet as the readily available rich source of multilingual and cross-lingual type-level lexical knowledge. Concretely, we use BabelNet's multilingual synsets to create synonym pairs (or synonym-gloss pairs) across 50 languages and then subject the MMTs (mBERT and XLM-R) to a lexical specialization procedure guided by a contrastive objective. We show that such massively multil
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#31639;&#27861;JGR&#65292;&#35813;&#31639;&#27861;&#23558;&#29983;&#25104;&#22120;&#21644;&#35780;&#20998;&#22120;&#38598;&#25104;&#22312;&#19968;&#20010;&#21333;&#19968;&#30340;&#26694;&#26550;&#20013;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#65292;&#36890;&#36807;&#28151;&#21512;&#30446;&#26631;&#20248;&#21270;&#29983;&#25104;&#22120;&#21644;&#20351;&#29992;&#23545;&#27604;&#25439;&#22833;&#35757;&#32451;&#35780;&#20998;&#22120;&#12290;&#22312;&#21508;&#31181;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;JGR&#22312;&#19977;&#31181;&#24120;&#35265;&#29983;&#25104;&#22330;&#26223;&#19979;&#30340;&#22235;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2206.13974</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#29983;&#25104;&#22120;-&#35780;&#20998;&#22120;&#32852;&#21512;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Joint Generator-Ranker Learning for Natural Language Generation. (arXiv:2206.13974v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.13974
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#31639;&#27861;JGR&#65292;&#35813;&#31639;&#27861;&#23558;&#29983;&#25104;&#22120;&#21644;&#35780;&#20998;&#22120;&#38598;&#25104;&#22312;&#19968;&#20010;&#21333;&#19968;&#30340;&#26694;&#26550;&#20013;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#65292;&#36890;&#36807;&#28151;&#21512;&#30446;&#26631;&#20248;&#21270;&#29983;&#25104;&#22120;&#21644;&#20351;&#29992;&#23545;&#27604;&#25439;&#22833;&#35757;&#32451;&#35780;&#20998;&#22120;&#12290;&#22312;&#21508;&#31181;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;JGR&#22312;&#19977;&#31181;&#24120;&#35265;&#29983;&#25104;&#22330;&#26223;&#19979;&#30340;&#22235;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;-&#25490;&#24207;&#65288;generate-then-rank&#65289;&#26159;&#25991;&#26412;&#29983;&#25104;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#26426;&#21046;&#65292;&#20854;&#20013;&#29983;&#25104;&#22120;&#20250;&#20135;&#29983;&#22810;&#20010;&#25991;&#26412;&#20505;&#36873;&#39033;&#65292;&#35780;&#20998;&#22120;&#20250;&#22312;&#20505;&#36873;&#39033;&#20013;&#36873;&#25321;&#26368;&#20339;&#30340;&#19968;&#20010;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#21333;&#29420;&#23545;&#29983;&#25104;&#22120;&#21644;&#35780;&#20998;&#22120;&#36827;&#34892;&#35757;&#32451;&#65292;&#24573;&#30053;&#20102;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#29983;&#25104;&#36136;&#37327;&#30340;&#30456;&#20114;&#21453;&#39304;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#21512;&#35757;&#32451;&#31639;&#27861; JGR&#65292;&#23427;&#23558;&#29983;&#25104;&#22120;&#21644;&#35780;&#20998;&#22120;&#38598;&#25104;&#22312;&#19968;&#20010;&#21333;&#19968;&#30340;&#26694;&#26550;&#20013;&#12290;JGR&#36890;&#36807;&#28151;&#21512;&#30446;&#26631;&#26469;&#20248;&#21270;&#29983;&#25104;&#22120;&#65292;&#35813;&#28151;&#21512;&#30446;&#26631;&#32467;&#21512;&#20102;&#25968;&#25454;&#20284;&#28982;&#21644;&#35780;&#20998;&#22120;&#22870;&#21169;&#65292;&#24182;&#20351;&#29992;&#23545;&#27604;&#25439;&#22833;&#35757;&#32451;&#35780;&#20998;&#22120;&#65292;&#23545;&#29983;&#25104;&#22120;&#36755;&#20986;&#36827;&#34892;&#27604;&#36739;&#12290;&#36890;&#36807;&#36845;&#20195;&#26356;&#26032;&#29983;&#25104;&#22120;&#21644;&#35780;&#20998;&#22120;&#65292;JGR&#21487;&#20197;&#26377;&#25928;&#22320;&#21327;&#35843;&#23427;&#20204;&#30340;&#23398;&#20064;&#65292;&#24182;&#20849;&#21516;&#25552;&#39640;&#23427;&#20204;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;JGR&#65292;&#24182;&#35777;&#26126;&#23427;&#22312;&#19977;&#31181;&#24120;&#35265;&#29983;&#25104;&#22330;&#26223;&#19979;&#30340;&#22235;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#27169;&#22411;&#24050;&#22312; https://github.com/thudm/JGR &#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generate-then-rank is a widely used mechanism for text generation, where a generator produces multiple text candidates and a ranker chooses the best one among the text candidates. However, existing methods usually train the generator and the ranker individually, neglecting the mutual feedback that could further enhance the generation quality. To tackle this limitation, we propose JGR, a novel joint training algorithm that integrates the generator and the ranker in a single framework. JGR optimizes the generator with a hybrid objective that combines data likelihood and ranker reward, and trains the ranker with a contrastive loss that compares the generator outputs. By iteratively updating the generator and the ranker, JGR can effectively harmonize their learning and enhance their quality jointly. We evaluate JGR on various text generation tasks and demonstrate that it surpasses existing methods on four public datasets across three common generation scenarios. Our code and models are pub
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#22810;&#20219;&#21153;&#30417;&#30563;&#39044;&#35757;&#32451;&#65288;MVP&#65289;&#26041;&#27861;&#65292;&#20854;&#25910;&#38598;&#22823;&#35268;&#27169;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#35821;&#26009;&#24211;&#65292;&#24182;&#20351;&#29992;&#30417;&#30563;&#26041;&#24335;&#23545;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#12290;MVP&#27169;&#22411;&#32467;&#21512;&#29305;&#23450;&#30340;&#36719;&#25552;&#31034;&#65292;&#21487;&#20197;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20248;&#24322;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2206.12131</link><description>&lt;p&gt;
MVP: &#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#22810;&#20219;&#21153;&#30417;&#30563;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
MVP: Multi-task Supervised Pre-training for Natural Language Generation. (arXiv:2206.12131v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.12131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#22810;&#20219;&#21153;&#30417;&#30563;&#39044;&#35757;&#32451;&#65288;MVP&#65289;&#26041;&#27861;&#65292;&#20854;&#25910;&#38598;&#22823;&#35268;&#27169;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#35821;&#26009;&#24211;&#65292;&#24182;&#20351;&#29992;&#30417;&#30563;&#26041;&#24335;&#23545;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#12290;MVP&#27169;&#22411;&#32467;&#21512;&#29305;&#23450;&#30340;&#36719;&#25552;&#31034;&#65292;&#21487;&#20197;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20248;&#24322;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25104;&#21151;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#22823;&#37096;&#20998;&#38754;&#21521;NLG&#30340;PLMs&#37117;&#26159;&#20351;&#29992;&#22823;&#35268;&#27169;&#36890;&#29992;&#35821;&#26009;&#24211;&#38750;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#39044;&#35757;&#32451;&#65288;&#21363;&#8220;&#30417;&#30563;&#39044;&#35757;&#32451;&#8221;&#65289;&#30340;&#27169;&#22411;&#23637;&#31034;&#20986;&#19982;&#38750;&#30417;&#30563;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#27604;&#26356;&#20248;&#24322;&#30340;&#34920;&#29616;&#12290;&#21463;&#30417;&#30563;&#39044;&#35757;&#32451;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#22810;&#20219;&#21153;&#30417;&#30563;&#39044;&#35757;&#32451;&#65288;MVP&#65289;&#12290;&#25105;&#20204;&#20174;11&#20010;&#19981;&#21516;&#30340;NLG&#20219;&#21153;&#20197;&#21450;77&#20010;&#25968;&#25454;&#38598;&#20013;&#25910;&#38598;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#35821;&#26009;&#24211;&#65288;MVPCorpus&#65289;&#65292;&#28982;&#21518;&#23558;&#36825;&#20123;&#31034;&#20363;&#32479;&#19968;&#26684;&#24335;&#21270;&#20026;&#19968;&#33324;&#30340;&#25991;&#26412;-&#25991;&#26412;&#26684;&#24335;&#65292;&#20197;&#30417;&#30563;&#26041;&#24335;&#39044;&#35757;&#32451;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;MVP&#12290;&#23545;&#20110;&#27599;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#39044;&#35757;&#32451;&#29305;&#23450;&#30340;&#36719;&#25552;&#31034;&#65292;&#20197;&#21050;&#28608;&#27169;&#22411;&#25191;&#34892;&#29305;&#23450;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;MVP&#27169;&#22411;&#21487;&#20197;&#30475;&#20316;&#26159;&#20351;&#29992;&#26368;&#36817;&#30340;&#25351;&#23548;&#24494;&#35843;&#25216;&#26415;&#30340;&#19968;&#31181;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models (PLMs) have achieved remarkable success in natural language generation (NLG) tasks. Up to now, most NLG-oriented PLMs are pre-trained in an unsupervised manner using the large-scale general corpus. In the meanwhile, an increasing number of models pre-trained with labeled data (i.e. "supervised pre-training") showcase superior performance compared to unsupervised pre-trained models. Motivated by the success of supervised pre-training, we propose Multi-task superVised Pre-training (MVP) for natural language generation. We collect a large-scale natural language generation corpus, MVPCorpus, from $77$ datasets over $11$ diverse NLG tasks. Then we unify these examples into a general text-to-text format to pre-train the text generation model MVP in a supervised manner. For each task, we further pre-train specific soft prompts to stimulate the model's capacity to perform a specific task. Our MVP model can be seen as a practice that utilizes recent instruction tunin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BITE&#30340;&#25991;&#26412;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#21521;&#35757;&#32451;&#25968;&#25454;&#20013;&#27880;&#20837;&#35302;&#21457;&#35789;&#65292;&#22312;&#36845;&#20195;&#30340;&#21333;&#35789;&#32423;&#25200;&#21160;&#20013;&#23558;&#36825;&#20123;&#35789;&#27880;&#20837;&#21040;&#36755;&#20837;&#23454;&#20363;&#20013;&#65292;&#25104;&#21151;&#22320;&#25915;&#20987;&#20102;&#21463;&#23475;&#32773;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2205.12700</link><description>&lt;p&gt;
BITE: &#20351;&#29992;&#36845;&#20195;&#35302;&#21457;&#35789;&#27880;&#20837;&#30340;&#25991;&#26412;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
BITE: Textual Backdoor Attacks with Iterative Trigger Injection. (arXiv:2205.12700v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12700
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BITE&#30340;&#25991;&#26412;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#21521;&#35757;&#32451;&#25968;&#25454;&#20013;&#27880;&#20837;&#35302;&#21457;&#35789;&#65292;&#22312;&#36845;&#20195;&#30340;&#21333;&#35789;&#32423;&#25200;&#21160;&#20013;&#23558;&#36825;&#20123;&#35789;&#27880;&#20837;&#21040;&#36755;&#20837;&#23454;&#20363;&#20013;&#65292;&#25104;&#21151;&#22320;&#25915;&#20987;&#20102;&#21463;&#23475;&#32773;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#38376;&#25915;&#20987;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#30340;&#26032;&#20852;&#23041;&#32961;&#12290;&#40657;&#23458;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#27602;&#21270;&#30340;&#35757;&#32451;&#25968;&#25454;&#23558;&#8220;&#21518;&#38376;&#8221;&#23884;&#20837;&#21463;&#23475;&#32773;&#27169;&#22411;&#65292;&#36825;&#26679;&#65292;&#28385;&#36275;&#19968;&#23450;&#25991;&#26412;&#27169;&#24335;&#65288;&#20363;&#22914;&#21253;&#21547;&#20851;&#38190;&#23383;&#65289;&#30340;&#36755;&#20837;&#23454;&#20363;&#21487;&#34987;&#39044;&#27979;&#20026;&#40657;&#23458;&#25511;&#21046;&#30340;&#30446;&#26631;&#26631;&#31614;&#12290;&#26412;&#25991;&#35777;&#26126;&#21487;&#33021;&#35774;&#35745;&#20986;&#26082;&#38590;&#20197;&#23519;&#35273;&#21448;&#20855;&#25915;&#20987;&#25104;&#21151;&#29575;&#39640;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#35825;&#39285;&#25968;&#25454;&#27880;&#20837;&#8220;&#35302;&#21457;&#35789;&#8221;&#30340;BITE&#25915;&#20987;&#12290;&#36825;&#20123;&#35302;&#21457;&#35789;&#36890;&#36807;&#33258;&#28982;&#30340;&#21333;&#35789;&#32423;&#25200;&#21160;&#19981;&#26029;&#35782;&#21035;&#21644;&#27880;&#20837;&#21040;&#30446;&#26631;&#26631;&#31614;&#23454;&#20363;&#20013;&#12290;&#27602;&#21270;&#30340;&#35757;&#32451;&#25968;&#25454;&#25351;&#23548;&#21463;&#23475;&#32773;&#27169;&#22411;&#22312;&#21253;&#21547;&#35302;&#21457;&#35789;&#30340;&#36755;&#20837;&#20013;&#39044;&#27979;&#30446;&#26631;&#26631;&#31614;&#65292;&#24418;&#25104;&#21518;&#38376;&#12290;&#22312;&#22235;&#20010;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25915;&#20987;&#26041;&#27861;&#21487;&#20197;&#25104;&#21151;&#22320;&#25915;&#20987;&#21463;&#23475;&#32773;&#27169;&#22411;&#65292;&#21516;&#26102;&#20855;&#26377;&#38590;&#20197;&#23519;&#35273;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Backdoor attacks have become an emerging threat to NLP systems. By providing poisoned training data, the adversary can embed a "backdoor" into the victim model, which allows input instances satisfying certain textual patterns (e.g., containing a keyword) to be predicted as a target label of the adversary's choice. In this paper, we demonstrate that it is possible to design a backdoor attack that is both stealthy (i.e., hard to notice) and effective (i.e., has a high attack success rate). We propose BITE, a backdoor attack that poisons the training data to establish strong correlations between the target label and a set of "trigger words". These trigger words are iteratively identified and injected into the target-label instances through natural word-level perturbations. The poisoned training data instruct the victim model to predict the target label on inputs containing trigger words, forming the backdoor. Experiments on four text classification datasets show that our proposed attack i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;TOUR&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;&#20132;&#21449;&#32534;&#30721;&#20877;&#25490;&#24207;&#22120;&#25552;&#20379;&#30340;&#20266;&#26631;&#31614;&#20248;&#21270;&#22522;&#20110;&#23454;&#20363;&#32423;&#21035;&#30340;&#26597;&#35810;&#34920;&#31034;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#31471;&#21040;&#31471;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.12680</link><description>&lt;p&gt;
&#20248;&#21270;&#23494;&#38598;&#26816;&#32034;&#30340;&#27979;&#35797;&#26102;&#38388;&#26597;&#35810;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Optimizing Test-Time Query Representations for Dense Retrieval. (arXiv:2205.12680v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;TOUR&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;&#20132;&#21449;&#32534;&#30721;&#20877;&#25490;&#24207;&#22120;&#25552;&#20379;&#30340;&#20266;&#26631;&#31614;&#20248;&#21270;&#22522;&#20110;&#23454;&#20363;&#32423;&#21035;&#30340;&#26597;&#35810;&#34920;&#31034;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#31471;&#21040;&#31471;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23494;&#38598;&#26816;&#32034;&#30340;&#21457;&#23637;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#30340;&#26597;&#35810;&#21644;&#19978;&#19979;&#25991;&#32534;&#30721;&#22120;&#25552;&#20379;&#30340;&#36136;&#37327;&#34920;&#31034;&#26597;&#35810;&#21644;&#19978;&#19979;&#25991;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;TOUR&#65288;Test-Time Optimization of Query Representations&#65289;&#65292;&#23427;&#36890;&#36807;&#26469;&#33258;&#27979;&#35797;&#26102;&#38388;&#26816;&#32034;&#32467;&#26524;&#30340;&#20449;&#21495;&#36827;&#19968;&#27493;&#20248;&#21270;&#22522;&#20110;&#23454;&#20363;&#32423;&#21035;&#30340;&#26597;&#35810;&#34920;&#31034;&#12290;&#25105;&#20204;&#21033;&#29992;&#20132;&#21449;&#32534;&#30721;&#22120;&#20877;&#25490;&#24207;&#22120;&#26469;&#20026;&#26816;&#32034;&#32467;&#26524;&#25552;&#20379;&#32454;&#31890;&#24230;&#30340;&#20266;&#26631;&#31614;&#65292;&#24182;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#36845;&#20195;&#22320;&#20248;&#21270;&#26597;&#35810;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;TOUR&#21487;&#20197;&#30475;&#20316;&#26159;&#20266;&#30456;&#20851;&#21453;&#39304;&#30340;&#32463;&#20856;Rocchio&#31639;&#27861;&#30340;&#19968;&#31181;&#25512;&#24191;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#21033;&#29992;&#20266;&#26631;&#31614;&#20316;&#20026;&#30828;&#20108;&#36827;&#21046;&#25110;&#36719;&#36830;&#32493;&#26631;&#31614;&#30340;&#21464;&#20307;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;TOUR&#24212;&#29992;&#20110;&#30701;&#35821;&#26816;&#32034;&#65292;&#24182;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#30701;&#35821;&#20877;&#25490;&#24207;&#22120;&#35780;&#20272;&#20854;&#22312;&#36890;&#36947;&#26816;&#32034;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;TOUR&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#31471;&#21040;&#31471;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent developments of dense retrieval rely on quality representations of queries and contexts from pre-trained query and context encoders. In this paper, we introduce TOUR (Test-Time Optimization of Query Representations), which further optimizes instance-level query representations guided by signals from test-time retrieval results. We leverage a cross-encoder re-ranker to provide fine-grained pseudo labels over retrieval results and iteratively optimize query representations with gradient descent. Our theoretical analysis reveals that TOUR can be viewed as a generalization of the classical Rocchio algorithm for pseudo relevance feedback, and we present two variants that leverage pseudo-labels as hard binary or soft continuous labels. We first apply TOUR on phrase retrieval with our proposed phrase re-ranker, and also evaluate its effectiveness on passage retrieval with an off-the-shelf re-ranker. TOUR greatly improves end-to-end open-domain question answering accuracy, as well as pa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#22810;&#27573;&#33853;&#22810;&#31572;&#26696;&#38382;&#39064;&#30340;&#24320;&#25918;&#22495;&#38382;&#31572;&#22522;&#20934;&#27979;&#35797;QAMPARI&#65292;&#24182;&#35757;&#32451;&#20102;ODQA&#27169;&#22411;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;QAMPARI&#22312;&#27573;&#33853;&#26816;&#32034;&#21644;&#31572;&#26696;&#29983;&#25104;&#26041;&#38754;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#24378;&#35843;&#20102;&#38656;&#35201;&#21457;&#23637;&#33021;&#22815;&#22788;&#29702;&#27492;&#31867;&#38382;&#39064;&#30340;ODQA&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2205.12665</link><description>&lt;p&gt;
QAMPARI: &#19968;&#20010;&#22810;&#27573;&#33853;&#22810;&#31572;&#26696;&#30340;&#24320;&#25918;&#22495;&#38382;&#31572;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
QAMPARI: An Open-domain Question Answering Benchmark for Questions with Many Answers from Multiple Paragraphs. (arXiv:2205.12665v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12665
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#22810;&#27573;&#33853;&#22810;&#31572;&#26696;&#38382;&#39064;&#30340;&#24320;&#25918;&#22495;&#38382;&#31572;&#22522;&#20934;&#27979;&#35797;QAMPARI&#65292;&#24182;&#35757;&#32451;&#20102;ODQA&#27169;&#22411;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;QAMPARI&#22312;&#27573;&#33853;&#26816;&#32034;&#21644;&#31572;&#26696;&#29983;&#25104;&#26041;&#38754;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#24378;&#35843;&#20102;&#38656;&#35201;&#21457;&#23637;&#33021;&#22815;&#22788;&#29702;&#27492;&#31867;&#38382;&#39064;&#30340;ODQA&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#24320;&#25918;&#22495;&#38382;&#31572;&#65288;ODQA&#65289;&#22522;&#20934;&#27979;&#35797;&#36890;&#24120;&#19987;&#27880;&#20110;&#21487;&#20197;&#20174;&#21333;&#20010;&#27573;&#33853;&#20013;&#25552;&#21462;&#31572;&#26696;&#30340;&#38382;&#39064;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#35768;&#22810;&#33258;&#28982;&#38382;&#39064;&#65292;&#20363;&#22914;&#8220;&#24067;&#40065;&#20811;&#26519;&#31726;&#32593;&#38431;&#36873;&#20102;&#21738;&#20123;&#29699;&#21592;&#65311;&#8221;&#65292;&#37117;&#26377;&#19968;&#31995;&#21015;&#31572;&#26696;&#12290;&#22238;&#31572;&#27492;&#31867;&#38382;&#39064;&#38656;&#35201;&#22312;&#22823;&#22411;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#21644;&#38405;&#35835;&#26469;&#33258;&#35768;&#22810;&#27573;&#33853;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;QAMPARI&#65292;&#19968;&#31181;ODQA&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#38382;&#39064;&#31572;&#26696;&#26159;&#20998;&#24067;&#22312;&#35768;&#22810;&#27573;&#33853;&#20013;&#30340;&#23454;&#20307;&#21015;&#34920;&#12290;&#25105;&#20204;&#36890;&#36807;&#65288;a&#65289;&#20174;&#32500;&#22522;&#30334;&#31185;&#30340;&#30693;&#35782;&#22270;&#35889;&#21644;&#34920;&#20013;&#29983;&#25104;&#20855;&#26377;&#22810;&#20010;&#31572;&#26696;&#30340;&#38382;&#39064;&#65292;&#65288;b&#65289;&#33258;&#21160;&#23558;&#31572;&#26696;&#19982;&#32500;&#22522;&#30334;&#31185;&#27573;&#33853;&#20013;&#30340;&#25903;&#25345;&#35777;&#25454;&#37197;&#23545;&#65292;&#20197;&#21450;&#65288;c&#65289;&#25163;&#21160;&#25913;&#20889;&#38382;&#39064;&#24182;&#39564;&#35777;&#27599;&#20010;&#31572;&#26696;&#26469;&#21019;&#24314;QAMPARI&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#26469;&#33258;&#26816;&#32034;&#21644;&#38405;&#35835;&#26063;&#30340;ODQA&#27169;&#22411;&#65292;&#21457;&#29616;QAMPARI&#22312;&#27573;&#33853;&#26816;&#32034;&#21644;&#31572;&#26696;&#29983;&#25104;&#26041;&#38754;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#26368;&#39640;&#36798;&#21040;32.8&#30340;F1&#20998;&#25968;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#38656;&#35201;&#24320;&#21457;&#33021;&#22815;&#22788;&#29702;&#22810;&#27573;&#33853;&#22810;&#31572;&#26696;&#38382;&#39064;&#30340;ODQA&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing benchmarks for open-domain question answering (ODQA) typically focus on questions whose answers can be extracted from a single paragraph. By contrast, many natural questions, such as "What players were drafted by the Brooklyn Nets?" have a list of answers. Answering such questions requires retrieving and reading from many passages, in a large corpus. We introduce QAMPARI, an ODQA benchmark, where question answers are lists of entities, spread across many paragraphs. We created QAMPARI by (a) generating questions with multiple answers from Wikipedia's knowledge graph and tables, (b) automatically pairing answers with supporting evidence in Wikipedia paragraphs, and (c) manually paraphrasing questions and validating each answer. We train ODQA models from the retrieve-and-read family and find that QAMPARI is challenging in terms of both passage retrieval and answer generation, reaching an F1 score of 32.8 at best. Our results highlight the need for developing ODQA models that han
&lt;/p&gt;</description></item></channel></rss>