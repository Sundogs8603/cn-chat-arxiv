<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#35777;&#25454;&#30340;&#31435;&#22330;&#25277;&#21462;&#32593;&#32476;&#65288;SEN&#65289;&#26469;&#26816;&#27979;&#19978;&#19979;&#25991;&#38169;&#35823;&#30340;&#35823;&#23548;&#20449;&#24687;&#12290;&#36890;&#36807;&#32771;&#34385;&#19981;&#21516;&#35777;&#25454;&#30340;&#31435;&#22330;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#20934;&#30830;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#20849;&#29616;&#20851;&#31995;&#30340;&#25903;&#25345;-&#21453;&#39539;&#20998;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20844;&#20849;&#22823;&#35268;&#27169;&#25968;&#25454;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01766</link><description>&lt;p&gt;
&#25903;&#25345;&#36824;&#26159;&#21453;&#39539;&#65306;&#20998;&#26512;&#35777;&#25454;&#31435;&#22330;&#20197;&#26816;&#27979;&#19978;&#19979;&#25991;&#38169;&#35823;&#30340;&#35823;&#23548;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Support or Refute: Analyzing the Stance of Evidence to Detect Out-of-Context Mis- and Disinformation. (arXiv:2311.01766v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#35777;&#25454;&#30340;&#31435;&#22330;&#25277;&#21462;&#32593;&#32476;&#65288;SEN&#65289;&#26469;&#26816;&#27979;&#19978;&#19979;&#25991;&#38169;&#35823;&#30340;&#35823;&#23548;&#20449;&#24687;&#12290;&#36890;&#36807;&#32771;&#34385;&#19981;&#21516;&#35777;&#25454;&#30340;&#31435;&#22330;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#20934;&#30830;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#20849;&#29616;&#20851;&#31995;&#30340;&#25903;&#25345;-&#21453;&#39539;&#20998;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20844;&#20849;&#22823;&#35268;&#27169;&#25968;&#25454;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#35823;&#23548;&#20449;&#24687;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#22269;&#23478;&#32423;&#30340;&#31038;&#20250;&#38382;&#39064;&#65292;&#26159;&#21508;&#31181;&#22312;&#32447;&#20260;&#23475;&#30340;&#20027;&#35201;&#26469;&#28304;&#20043;&#19968;&#12290;&#20854;&#20013;&#19968;&#31181;&#24120;&#35265;&#30340;&#35823;&#23548;&#20449;&#24687;&#24418;&#24335;&#26159;&#19978;&#19979;&#25991;&#38169;&#35823;&#65288;OOC&#65289;&#20449;&#24687;&#65292;&#20854;&#20013;&#19981;&#21516;&#30340;&#20449;&#24687;&#34987;&#38169;&#35823;&#22320;&#20851;&#32852;&#36215;&#26469;&#65292;&#20363;&#22914;&#30495;&#23454;&#22270;&#20687;&#19982;&#34394;&#20551;&#30340;&#25991;&#26412;&#26631;&#39064;&#25110;&#35823;&#23548;&#24615;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#23613;&#31649;&#19968;&#20123;&#30740;&#31350;&#35797;&#22270;&#36890;&#36807;&#22806;&#37096;&#35777;&#25454;&#26469;&#25269;&#24481;&#19978;&#19979;&#25991;&#38169;&#35823;&#30340;&#35823;&#23548;&#20449;&#24687;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#24573;&#35270;&#20102;&#19981;&#21516;&#31435;&#22330;&#30340;&#19981;&#21516;&#35777;&#25454;&#30340;&#20316;&#29992;&#12290;&#21463;&#21040;&#35777;&#25454;&#31435;&#22330;&#20195;&#34920;&#19981;&#21516;&#26816;&#27979;&#32467;&#26524;&#30340;&#20559;&#35265;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#32479;&#19968;&#26694;&#26550;&#20013;&#25552;&#21462;&#22810;&#27169;&#24577;&#35777;&#25454;&#30340;&#31435;&#22330;&#30340;&#31435;&#22330;&#25277;&#21462;&#32593;&#32476;&#65288;SEN&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#22522;&#20110;&#21629;&#21517;&#23454;&#20307;&#30340;&#20849;&#29616;&#20851;&#31995;&#35745;&#31639;&#30340;&#25903;&#25345;-&#21453;&#39539;&#20998;&#25968;&#21040;&#25991;&#26412;SEN&#20013;&#12290;&#23545;&#20844;&#20849;&#22823;&#35268;&#27169;&#25968;&#25454;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mis- and disinformation online have become a major societal problem as major sources of online harms of different kinds. One common form of mis- and disinformation is out-of-context (OOC) information, where different pieces of information are falsely associated, e.g., a real image combined with a false textual caption or a misleading textual description. Although some past studies have attempted to defend against OOC mis- and disinformation through external evidence, they tend to disregard the role of different pieces of evidence with different stances. Motivated by the intuition that the stance of evidence represents a bias towards different detection results, we propose a stance extraction network (SEN) that can extract the stances of different pieces of multi-modal evidence in a unified framework. Moreover, we introduce a support-refutation score calculated based on the co-occurrence relations of named entities into the textual SEN. Extensive experiments on a public large-scale data
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#29616;&#26377;&#38544;&#21947;&#35782;&#21035;&#25968;&#25454;&#38598;&#20013;&#30340;&#24314;&#26500;&#20559;&#35265;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#21947;&#35782;&#21035;&#31995;&#32479;&#21487;&#20197;&#19982;&#20351;&#29992;&#23436;&#25972;&#19978;&#19979;&#25991;&#30340;&#31995;&#32479;&#30456;&#31454;&#20105;&#12290;</title><link>http://arxiv.org/abs/2311.00790</link><description>&lt;p&gt;
&#22312;&#38544;&#21947;&#35782;&#21035;&#25968;&#25454;&#38598;&#20013;&#30340;&#24314;&#26500;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Construction Artifacts in Metaphor Identification Datasets. (arXiv:2311.00790v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00790
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#29616;&#26377;&#38544;&#21947;&#35782;&#21035;&#25968;&#25454;&#38598;&#20013;&#30340;&#24314;&#26500;&#20559;&#35265;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#21947;&#35782;&#21035;&#31995;&#32479;&#21487;&#20197;&#19982;&#20351;&#29992;&#23436;&#25972;&#19978;&#19979;&#25991;&#30340;&#31995;&#32479;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#21947;&#35782;&#21035;&#26088;&#22312;&#29702;&#35299;&#32473;&#23450;&#34920;&#36798;&#26159;&#21542;&#22312;&#29305;&#23450;&#35821;&#22659;&#20013;&#20197;&#27604;&#21947;&#30340;&#26041;&#24335;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29616;&#26377;&#30340;&#38544;&#21947;&#35782;&#21035;&#25968;&#25454;&#38598;&#22914;&#20309;&#36890;&#36807;&#23436;&#20840;&#24573;&#30053;&#28508;&#22312;&#30340;&#38544;&#21947;&#34920;&#36798;&#25110;&#20854;&#25152;&#22312;&#30340;&#35821;&#22659;&#26469;&#36827;&#34892;&#25805;&#32437;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#35774;&#32622;&#20013;&#27979;&#35797;&#20102;&#36825;&#20010;&#20551;&#35774;&#65292;&#24182;&#34920;&#26126;&#22522;&#20110;&#19981;&#20855;&#26377;&#23436;&#25972;&#20449;&#24687;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#21947;&#35782;&#21035;&#31995;&#32479;&#21487;&#20197;&#19982;&#20351;&#29992;&#23436;&#25972;&#19978;&#19979;&#25991;&#30340;&#31995;&#32479;&#30456;&#31454;&#20105;&#12290;&#36825;&#26159;&#30001;&#20110;&#26500;&#24314;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#36807;&#31243;&#20013;&#24341;&#20837;&#20102;&#23545;&#27491;&#36127;&#31867;&#21035;&#30340;&#19981;&#24076;&#26395;&#30340;&#20559;&#35265;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#20174;&#33258;&#28982;&#35821;&#26009;&#24211;&#20013;&#31934;&#24515;&#25277;&#26679;&#30340;&#19981;&#23384;&#22312;&#36825;&#31181;&#20559;&#35265;&#30340;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#21516;&#26679;&#30340;&#20551;&#35774;&#65292;&#20351;&#24471;&#36825;&#20123;&#25968;&#25454;&#38598;&#26356;&#20855;&#25361;&#25112;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Metaphor identification aims at understanding whether a given expression is used figuratively in context. However, in this paper we show how existing metaphor identification datasets can be gamed by fully ignoring the potential metaphorical expression or the context in which it occurs. We test this hypothesis in a variety of datasets and settings, and show that metaphor identification systems based on language models without complete information can be competitive with those using the full context. This is due to the construction procedures to build such datasets, which introduce unwanted biases for positive and negative classes. Finally, we test the same hypothesis on datasets that are carefully sampled from natural corpora and where this bias is not present, making these datasets more challenging and reliable.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DEFT&#30340;&#25968;&#25454;&#39640;&#25928;&#24494;&#35843;&#26694;&#26550;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#26680;&#24515;&#38598;&#36873;&#25321;&#26469;&#26368;&#23567;&#21270;&#24494;&#35843;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;DEFT&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#19978;&#19982;&#29616;&#26377;&#27169;&#22411;&#30456;&#24403;&#65292;&#24182;&#19988;&#20165;&#20351;&#29992;&#20102;70%&#30340;&#25968;&#25454;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.16776</link><description>&lt;p&gt;
DEFT&#65306;&#36890;&#36807;&#26080;&#30417;&#30563;&#26680;&#24515;&#38598;&#36873;&#25321;&#23454;&#29616;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
DEFT: Data Efficient Fine-Tuning for Large Language Models via Unsupervised Core-Set Selection. (arXiv:2310.16776v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16776
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DEFT&#30340;&#25968;&#25454;&#39640;&#25928;&#24494;&#35843;&#26694;&#26550;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#26680;&#24515;&#38598;&#36873;&#25321;&#26469;&#26368;&#23567;&#21270;&#24494;&#35843;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;DEFT&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#19978;&#19982;&#29616;&#26377;&#27169;&#22411;&#30456;&#24403;&#65292;&#24182;&#19988;&#20165;&#20351;&#29992;&#20102;70%&#30340;&#25968;&#25454;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#36827;&#23637;&#20351;&#24471;&#35768;&#22810;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#21487;&#20197;&#20351;&#29992;&#65307;&#28982;&#32780;&#65292;&#19968;&#20010;&#20173;&#28982;&#23384;&#22312;&#30340;&#38382;&#39064;&#26159;&#24494;&#35843;PLMs&#20197;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#31350;&#31455;&#38656;&#35201;&#22810;&#23569;&#25968;&#25454;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DEFT&#65292;&#19968;&#31181;&#25968;&#25454;&#39640;&#25928;&#30340;&#24494;&#35843;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#26080;&#30417;&#30563;&#30340;&#26680;&#24515;&#38598;&#36873;&#25321;&#26469;&#26368;&#23567;&#21270;&#24494;&#35843;PLMs&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#12290;&#25105;&#20204;&#22312;&#25991;&#26412;&#32534;&#36753;LM&#30340;&#32972;&#26223;&#19979;&#23637;&#31034;&#20102;DEFT&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19982;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#32534;&#36753;&#27169;&#22411;CoEDIT&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#32467;&#26524;&#34920;&#26126;&#65292;DEFT&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#19978;&#19982;CoEDIT&#19968;&#26679;&#65292;&#32780;&#20351;&#29992;&#30340;&#25968;&#25454;&#37327;&#35201;&#23569;&#32422;70%&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances have led to the availability of many pre-trained language models (PLMs); however, a question that remains is how much data is truly needed to fine-tune PLMs for downstream tasks? In this work, we introduce DEFT, a data-efficient fine-tuning framework that leverages unsupervised core-set selection to minimize the amount of data needed to fine-tune PLMs for downstream tasks. We demonstrate the efficacy of our DEFT framework in the context of text-editing LMs, and compare to the state-of-the art text-editing model, CoEDIT. Our quantitative and qualitative results demonstrate that DEFT models are just as accurate as CoEDIT while being finetuned on ~70% less data.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36234;&#21335;&#26222;&#36890;&#25945;&#32946;&#20013;&#23545;&#22810;&#39033;&#36873;&#25321;&#39064;&#31526;&#21495;&#32465;&#23450;&#33021;&#21147;&#30340;&#33021;&#21147;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#39062;&#19988;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#31526;&#21495;&#32465;&#23450;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.12059</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36234;&#21335;&#26222;&#36890;&#25945;&#32946;&#20013;&#23545;&#22810;&#39033;&#36873;&#25321;&#39064;&#31526;&#21495;&#32465;&#23450;&#33021;&#21147;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Symbol Binding Ability of Large Language Models for Multiple-Choice Questions in Vietnamese General Education. (arXiv:2310.12059v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36234;&#21335;&#26222;&#36890;&#25945;&#32946;&#20013;&#23545;&#22810;&#39033;&#36873;&#25321;&#39064;&#31526;&#21495;&#32465;&#23450;&#33021;&#21147;&#30340;&#33021;&#21147;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#39062;&#19988;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#31526;&#21495;&#32465;&#23450;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#38646;&#26679;&#26412;&#12289;&#19968;&#27425;&#24615;&#21644;&#23569;&#26679;&#26412;&#35774;&#32622;&#19979;&#65292;&#25191;&#34892;&#22810;&#39033;&#36873;&#25321;&#31526;&#21495;&#32465;&#23450;&#65288;MCSB&#65289;&#29992;&#20110;&#22810;&#39033;&#36873;&#25321;&#39064;&#22238;&#31572;&#65288;MCQA&#65289;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;&#37325;&#28857;&#25918;&#22312;&#36234;&#21335;&#35821;&#19978;&#65292;&#22240;&#20026;&#36234;&#21335;&#35821;&#20013;&#30340;&#25361;&#25112;&#24615;MCQA&#25968;&#25454;&#38598;&#36739;&#33521;&#35821;&#23569;&#12290;&#29616;&#26377;&#30340;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;ViMMRC 1.0&#21644;ViMMRC 2.0&#65292;&#19987;&#27880;&#20110;&#25991;&#23398;&#38382;&#39064;&#12290;&#36234;&#21335;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#26368;&#26032;&#30740;&#31350;&#20391;&#37325;&#20110;&#35780;&#20272;ChatGPT&#22312;2019&#24180;&#33267;2023&#24180;&#30340;&#36234;&#21335;&#22269;&#23478;&#39640;&#20013;&#27605;&#19994;&#32771;&#35797;&#65288;VNHSGE&#65289;&#20013;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;ChatGPT&#22914;&#20309;&#36880;&#27493;&#35299;&#20915;VNHSGE&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20026;&#25968;&#23398;&#12289;&#29289;&#29702;&#12289;&#21270;&#23398;&#21644;&#29983;&#29289;&#30340;LaTeX&#20844;&#24335;&#36755;&#20837;&#25552;&#20379;&#32467;&#26500;&#21270;&#25351;&#21335;&#65292;&#21019;&#24314;&#19968;&#20010;&#26032;&#39062;&#19988;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#35780;&#20272;LLMs&#21644;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;MCSB&#33021;&#21147;&#65292;&#22240;&#20026;&#25968;&#25454;&#38598;&#35201;&#27714;&#20351;&#29992;&#20005;&#26684;&#30340;LaTeX&#26679;&#24335;&#36827;&#34892;&#36755;&#20837;&#12290;&#25105;&#20204;&#37325;&#28857;&#39044;&#27979;&#23383;&#31526;&#65288;A&#12289;B&#12289;C&#25110;
&lt;/p&gt;
&lt;p&gt;
In this paper, we evaluate the ability of large language models (LLMs) to perform multiple choice symbol binding (MCSB) for multiple choice question answering (MCQA) tasks in zero-shot, one-shot, and few-shot settings. We focus on Vietnamese, with fewer challenging MCQA datasets than in English. The two existing datasets, ViMMRC 1.0 and ViMMRC 2.0, focus on literature. Recent research in Vietnamese natural language processing (NLP) has focused on the Vietnamese National High School Graduation Examination (VNHSGE) from 2019 to 2023 to evaluate ChatGPT. However, these studies have mainly focused on how ChatGPT solves the VNHSGE step by step. We aim to create a novel and high-quality dataset by providing structured guidelines for typing LaTeX formulas for mathematics, physics, chemistry, and biology. This dataset can be used to evaluate the MCSB ability of LLMs and smaller language models (LMs) because it is typed in a strict LaTeX style. We focus on predicting the character (A, B, C, or 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#22522;&#20934;&#65292;&#37327;&#21270;&#20102;&#20013;&#25991;&#21307;&#23398;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19982;&#20581;&#24247;&#30456;&#20851;&#30340;&#21407;&#23376;&#30693;&#35782;&#30340;&#23384;&#20648;&#31243;&#24230;&#65292;&#24182;&#21457;&#29616;&#36890;&#29992;LLMs&#22312;&#21407;&#23376;&#30693;&#35782;&#21644;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;&#20004;&#31181;&#31867;&#22411;&#30340;LLMs&#37117;&#20542;&#21521;&#20110;&#36814;&#21512;&#29992;&#25143;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.11722</link><description>&lt;p&gt;
&#37327;&#21270;&#20013;&#25991;&#21307;&#23398;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19982;&#20581;&#24247;&#30456;&#20851;&#30340;&#21407;&#23376;&#30693;&#35782;&#65306;&#19968;&#39033;&#35745;&#31639;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Quantify Health-Related Atomic Knowledge in Chinese Medical Large Language Models: A Computational Analysis. (arXiv:2310.11722v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11722
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#22522;&#20934;&#65292;&#37327;&#21270;&#20102;&#20013;&#25991;&#21307;&#23398;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19982;&#20581;&#24247;&#30456;&#20851;&#30340;&#21407;&#23376;&#30693;&#35782;&#30340;&#23384;&#20648;&#31243;&#24230;&#65292;&#24182;&#21457;&#29616;&#36890;&#29992;LLMs&#22312;&#21407;&#23376;&#30693;&#35782;&#21644;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;&#20004;&#31181;&#31867;&#22411;&#30340;LLMs&#37117;&#20542;&#21521;&#20110;&#36814;&#21512;&#29992;&#25143;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26377;&#28508;&#21147;&#36890;&#36807;&#25628;&#32034;&#24341;&#25806;&#30452;&#25509;&#21644;&#39640;&#25928;&#22320;&#25552;&#20379;&#29992;&#25143;&#30340;&#33258;&#35786;&#26029;&#24314;&#35758;&#65292;&#20174;&#32780;&#38761;&#26032;&#29992;&#25143;&#33258;&#35786;&#26029;&#30340;&#26041;&#24335;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#22522;&#20110;GPT-4&#35780;&#20272;LLMs&#30340;&#36136;&#37327;&#25110;&#20854;&#36890;&#36807;&#21307;&#23398;&#32771;&#35797;&#30340;&#33021;&#21147;&#65292;&#20294;&#27809;&#26377;&#30740;&#31350;&#37327;&#21270;&#23384;&#20648;&#22312;LLMs&#35760;&#24518;&#20013;&#30340;&#20581;&#24247;&#30456;&#20851;&#21407;&#23376;&#30693;&#35782;&#30340;&#31243;&#24230;&#65292;&#32780;&#36825;&#26159;LLMs&#25552;&#20379;&#26356;&#20934;&#30830;&#24314;&#35758;&#30340;&#22522;&#30784;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20934;&#65292;&#21253;&#25324;&#29992;&#25143;&#33258;&#35786;&#26029;&#26597;&#35810;&#20013;&#26368;&#24120;&#35265;&#30340;&#21407;&#23376;&#30693;&#35782;&#31867;&#22411;&#65292;&#20849;17&#31181;&#21407;&#23376;&#31867;&#22411;&#21644;14048&#26465;&#21407;&#23376;&#30693;&#35782;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#36890;&#29992;&#21644;&#19987;&#19994;LLMs&#22312;&#22522;&#20934;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21407;&#23376;&#30693;&#35782;&#21644;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#26041;&#38754;&#65292;&#36890;&#29992;LLMs&#30340;&#34920;&#29616;&#20248;&#20110;&#19987;&#19994;LLMs&#12290;&#38169;&#35823;&#20998;&#26512;&#26174;&#31034;&#65292;&#36890;&#29992;&#21644;&#19987;&#19994;LLMs&#37117;&#26159;&#39532;&#23617;&#31934;&#65292;&#21363;&#22312;&#28041;&#21450;&#29992;&#25143;&#35201;&#27714;&#26102;&#24635;&#26159;&#36814;&#21512;&#29992;&#25143;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have the potential to revolutionize the way users self-diagnose through search engines by offering direct and efficient suggestions. Recent studies primarily focused on the quality of LLMs evaluated by GPT-4 or their ability to pass medical exams, no studies have quantified the extent of health-related atomic knowledge stored in LLMs' memory, which is the basis of LLMs to provide more factual suggestions. In this paper, we first constructed a benchmark, including the most common types of atomic knowledge in user self-diagnosis queries, with 17 atomic types and a total of 14, 048 pieces of atomic knowledge. Then, we evaluated both generic and specialized LLMs on the benchmark. The experimental results showcased that generic LLMs perform better than specialized LLMs in terms of atomic knowledge and instruction-following ability. Error analysis revealed that both generic and specialized LLMs are sycophantic, e.g., always catering to users' claims when it comes
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#23545;&#24615;&#21035;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#65292;&#24110;&#21161;&#20943;&#36731;&#20256;&#32479;&#31461;&#35805;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#21453;&#20107;&#23454;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#26469;&#20943;&#36731;&#23398;&#20064;&#21040;&#30340;&#20559;&#35265;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#27169;&#22411;&#23545;&#24615;&#21035;&#25200;&#21160;&#25935;&#24863;&#65292;&#20294;&#22312;&#21453;&#20107;&#23454;&#35757;&#32451;&#21518;&#23545;&#21518;&#32493;&#24341;&#20837;&#30340;&#21453;&#24615;&#21035;&#20559;&#35265;&#26356;&#19981;&#25935;&#24863;&#12290;</title><link>http://arxiv.org/abs/2310.10865</link><description>&lt;p&gt;
&#29579;&#23376;&#20250;&#24471;&#21040;&#30495;&#29233;&#20043;&#21563;&#21527;&#65311;&#20851;&#20110;&#31461;&#35805;&#25991;&#26412;&#20013;&#24615;&#21035;&#25200;&#21160;&#23545;&#27169;&#22411;&#25935;&#24863;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Will the Prince Get True Love's Kiss? On the Model Sensitivity to Gender Perturbation over Fairytale Texts. (arXiv:2310.10865v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10865
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#23545;&#24615;&#21035;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#65292;&#24110;&#21161;&#20943;&#36731;&#20256;&#32479;&#31461;&#35805;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#21453;&#20107;&#23454;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#26469;&#20943;&#36731;&#23398;&#20064;&#21040;&#30340;&#20559;&#35265;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#27169;&#22411;&#23545;&#24615;&#21035;&#25200;&#21160;&#25935;&#24863;&#65292;&#20294;&#22312;&#21453;&#20107;&#23454;&#35757;&#32451;&#21518;&#23545;&#21518;&#32493;&#24341;&#20837;&#30340;&#21453;&#24615;&#21035;&#20559;&#35265;&#26356;&#19981;&#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#20256;&#32479;&#30340;&#31461;&#35805;&#25925;&#20107;&#20013;&#23384;&#22312;&#22823;&#37327;&#26377;&#23475;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;&#20026;&#20102;&#20943;&#36731;&#31461;&#35805;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#20559;&#35265;&#23545;&#24615;&#21035;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20851;&#27880;&#31461;&#35805;&#25925;&#20107;&#20013;&#30340;&#38382;&#31572;&#20219;&#21153;&#12290;&#36890;&#36807;&#20351;&#29992;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;FairytaleQA&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35780;&#20272;&#27169;&#22411;&#23545;&#20132;&#25442;&#24615;&#21035;&#35282;&#33394;&#20449;&#24687;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#35757;&#32451;&#26102;&#24341;&#20837;&#21453;&#20107;&#23454;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#26469;&#20943;&#36731;&#23398;&#20064;&#21040;&#30340;&#20559;&#35265;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#24222;&#22823;&#35789;&#27719;&#37327;&#26469;&#25903;&#25345;&#36229;&#36234;&#31461;&#35805;&#25925;&#20107;&#30340;&#25991;&#26412;&#31867;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#23545;&#24615;&#21035;&#25200;&#21160;&#25935;&#24863;&#65292;&#24615;&#33021;&#19982;&#21407;&#22987;&#27979;&#35797;&#38598;&#30456;&#27604;&#26174;&#33879;&#19979;&#38477;&#12290;&#28982;&#32780;&#65292;&#24403;&#39318;&#20808;&#22312;&#21453;&#20107;&#23454;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#21518;&#65292;&#27169;&#22411;&#23545;&#21518;&#32493;&#24341;&#20837;&#30340;&#21453;&#24615;&#21035;&#20559;&#35265;&#26356;&#19981;&#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies show that traditional fairytales are rife with harmful gender biases. To help mitigate these gender biases in fairytales, this work aims to assess learned biases of language models by evaluating their robustness against gender perturbations. Specifically, we focus on Question Answering (QA) tasks in fairytales. Using counterfactual data augmentation to the FairytaleQA dataset, we evaluate model robustness against swapped gender character information, and then mitigate learned biases by introducing counterfactual gender stereotypes during training time. We additionally introduce a novel approach that utilizes the massive vocabulary of language models to support text genres beyond fairytales. Our experimental results suggest that models are sensitive to gender perturbations, with significant performance drops compared to the original testing set. However, when first fine-tuned on a counterfactual training dataset, models are less sensitive to the later introduced anti-gend
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21322;&#33258;&#21160;&#21270;&#20998;&#21306;&#24179;&#21488;&#65292;&#29992;&#20110;&#20174;&#31185;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#36229;&#23548;&#20307;&#23454;&#39564;&#25968;&#25454;&#12290;&#35813;&#24179;&#21488;&#36890;&#36807;&#33258;&#21160;&#21644;&#25163;&#21160;&#36807;&#31243;&#30340;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#26356;&#26032;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#25110;&#25552;&#39640;&#20102;&#25968;&#25454;&#36136;&#37327;&#12290;&#35780;&#20272;&#23454;&#39564;&#34920;&#26126;&#35813;&#20998;&#21306;&#24179;&#21488;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#25454;&#30340;&#31649;&#29702;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.10923</link><description>&lt;p&gt;
&#21322;&#33258;&#21160;&#21270;&#20998;&#21306;: &#29992;&#20110;&#20174;&#31185;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#39640;&#36136;&#37327;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
Semi-automatic staging area for high-quality structured data extraction from scientific literature. (arXiv:2309.10923v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10923
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21322;&#33258;&#21160;&#21270;&#20998;&#21306;&#24179;&#21488;&#65292;&#29992;&#20110;&#20174;&#31185;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#36229;&#23548;&#20307;&#23454;&#39564;&#25968;&#25454;&#12290;&#35813;&#24179;&#21488;&#36890;&#36807;&#33258;&#21160;&#21644;&#25163;&#21160;&#36807;&#31243;&#30340;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#26356;&#26032;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#25110;&#25552;&#39640;&#20102;&#25968;&#25454;&#36136;&#37327;&#12290;&#35780;&#20272;&#23454;&#39564;&#34920;&#26126;&#35813;&#20998;&#21306;&#24179;&#21488;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#25454;&#30340;&#31649;&#29702;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#20174;&#31185;&#23398;&#25991;&#31456;&#20013;&#37319;&#38598;&#36229;&#23548;&#20307;&#23454;&#39564;&#25968;&#25454;&#30340; SuperCon &#25968;&#25454;&#24211;&#30340;&#20998;&#21306;&#24179;&#21488;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25552;&#39640;&#26356;&#26032; SuperCon &#30340;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#25110;&#25552;&#39640;&#25968;&#25454;&#36136;&#37327;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#30001;&#33258;&#21160;&#21644;&#25163;&#21160;&#36807;&#31243;&#32452;&#25104;&#30340;&#24037;&#20316;&#27969;&#39537;&#21160;&#30340;&#21322;&#33258;&#21160;&#21270;&#20998;&#21306;&#24179;&#21488;&#65292;&#29992;&#20110;&#20174;&#25552;&#21462;&#30340;&#25968;&#25454;&#24211;&#20013;&#23545;&#25968;&#25454;&#36827;&#34892;&#26657;&#39564;&#21644;&#32416;&#38169;&#12290;&#24322;&#24120;&#26816;&#27979;&#33258;&#21160;&#36807;&#31243;&#29992;&#20110;&#39044;&#20808;&#31579;&#36873;&#37319;&#38598;&#21040;&#30340;&#25968;&#25454;&#12290;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#23450;&#21046;&#30340;&#29992;&#25143;&#30028;&#38754;&#22312;&#21407;&#22987; PDF &#25991;&#26723;&#19978;&#36827;&#34892;&#25968;&#25454;&#39564;&#35777;&#21644;&#32416;&#38169;&#12290;&#27492;&#22806;&#65292;&#24403;&#35760;&#24405;&#34987;&#32416;&#38169;&#26102;&#65292;&#20854;&#21407;&#22987;&#25968;&#25454;&#34987;&#25910;&#38598;&#24182;&#29992;&#20110;&#25913;&#36827;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#35780;&#20272;&#23454;&#39564;&#34920;&#26126;&#25105;&#20204;&#30340;&#20998;&#21306;&#24179;&#21488;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#25454;&#30340;&#31649;&#29702;&#36136;&#37327;&#12290;&#25105;&#20204;&#23558;&#30028;&#38754;&#19982;&#20256;&#32479;&#30340;&#25163;&#21160;&#38405;&#35835; PDF &#25991;&#26723;&#24182;&#22312; Excel &#25991;&#26723;&#20013;&#35760;&#24405;&#20449;&#24687;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we propose a staging area for ingesting new superconductors' experimental data in SuperCon that is machine-collected from scientific articles. Our objective is to enhance the efficiency of updating SuperCon while maintaining or enhancing the data quality. We present a semi-automatic staging area driven by a workflow combining automatic and manual processes on the extracted database. An anomaly detection automatic process aims to pre-screen the collected data. Users can then manually correct any errors through a user interface tailored to simplify the data verification on the original PDF documents. Additionally, when a record is corrected, its raw data is collected and utilised to improve machine learning models as training data. Evaluation experiments demonstrate that our staging area significantly improves curation quality. We compare the interface with the traditional manual approach of reading PDF documents and recording information in an Excel document. Using the in
&lt;/p&gt;</description></item><item><title>LM-Infinite&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#38271;&#24207;&#21015;&#19978;&#30340;&#38271;&#24230;&#25512;&#24191;&#22833;&#36133;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#21363;&#26102;&#25512;&#24191;&#26041;&#27861;&#65292;&#20197;&#26356;&#39640;&#25928;&#22320;&#21033;&#29992;&#29616;&#26377;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.16137</link><description>&lt;p&gt;
LM-Infinite: &#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#21363;&#26102;&#38271;&#24230;&#25512;&#24191;
&lt;/p&gt;
&lt;p&gt;
LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models. (arXiv:2308.16137v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16137
&lt;/p&gt;
&lt;p&gt;
LM-Infinite&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#38271;&#24207;&#21015;&#19978;&#30340;&#38271;&#24230;&#25512;&#24191;&#22833;&#36133;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#21363;&#26102;&#25512;&#24191;&#26041;&#27861;&#65292;&#20197;&#26356;&#39640;&#25928;&#22320;&#21033;&#29992;&#29616;&#26377;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22312;Transformer-based&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#38543;&#30528;&#36825;&#20123;LLM&#22312;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#20219;&#21153;&#19978;&#30340;&#37096;&#32626;&#65292;&#23427;&#20204;&#24448;&#24448;&#38754;&#20020;&#30528;&#23545;&#38271;&#26102;&#38388;&#25512;&#29702;&#36807;&#31243;&#25110;&#29702;&#35299;&#26356;&#22823;&#19978;&#19979;&#25991;&#30340;&#38656;&#27714;&#12290;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;LLM&#22312;&#38271;&#24207;&#21015;&#19978;&#30340;&#38271;&#24230;&#25512;&#24191;&#22833;&#36133;&#21464;&#24471;&#26356;&#21152;&#31361;&#20986;&#12290;&#22823;&#22810;&#25968;&#39044;&#35757;&#32451;&#26041;&#26696;&#23558;&#35757;&#32451;&#24207;&#21015;&#25130;&#26029;&#21040;&#22266;&#23450;&#38271;&#24230;&#65288;&#20363;&#22914;LLaMa&#30340;2048&#65289;&#12290;&#21363;&#20351;&#20351;&#29992;&#20102;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#26469;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;LLM&#22312;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#20043;&#21518;&#24448;&#24448;&#38590;&#20197;&#29983;&#25104;&#27969;&#30021;&#30340;&#25991;&#26412;&#65292;&#26356;&#19981;&#29992;&#35828;&#36827;&#34892;&#19979;&#28216;&#20219;&#21153;&#20102;&#12290;&#24120;&#35265;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22914;&#22312;&#26356;&#38271;&#30340;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#24448;&#24448;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#30340;&#30828;&#20214;&#21644;&#26102;&#38388;&#25104;&#26412;&#65292;&#24182;&#38656;&#35201;&#36827;&#34892;&#20180;&#32454;&#30340;&#35757;&#32451;&#36807;&#31243;&#35774;&#35745;&#12290;&#20026;&#20102;&#26356;&#39640;&#25928;&#22320;&#21033;&#29992;&#29616;&#26377;LLM&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#21644;&#23454;&#35777;&#19978;&#30740;&#31350;&#20102;&#20027;&#35201;&#30340;&#20998;&#24067;&#22806;(OOD) f
&lt;/p&gt;
&lt;p&gt;
In recent years, there have been remarkable advancements in the performance of Transformer-based Large Language Models (LLMs) across various domains. As these LLMs are deployed for increasingly complex tasks, they often face the needs to conduct longer reasoning processes or understanding larger contexts. In these situations, the length generalization failure of LLMs on long sequences become more prominent. Most pre-training schemes truncate training sequences to a fixed length (such as 2048 for LLaMa). LLMs often struggle to generate fluent texts, let alone carry out downstream tasks, after longer contexts, even with relative positional encoding which is designed to cope with this problem. Common solutions such as finetuning on longer corpora often involves daunting hardware and time costs and requires careful training process design. To more efficiently leverage the generation capacity of existing LLMs, we theoretically and empirically investigate the main out-of-distribution (OOD) f
&lt;/p&gt;</description></item><item><title>WavMark&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#38899;&#39057;&#27700;&#21360;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#30701;&#30701;1&#31186;&#30340;&#38899;&#39057;&#29255;&#27573;&#20013;&#32534;&#30721;&#22810;&#36798;32&#20301;&#30340;&#27700;&#21360;&#65292;&#23545;&#20154;&#31867;&#24863;&#23448;&#26080;&#24863;&#30693;&#24182;&#19988;&#20855;&#26377;&#24378;&#22823;&#30340;&#38887;&#24615;&#12290;&#23427;&#21487;&#20197;&#29992;&#20110;&#21512;&#25104;&#22768;&#38899;&#30340;&#26377;&#25928;&#35782;&#21035;&#21644;&#38899;&#39057;&#29256;&#26435;&#20445;&#25252;&#12290;</title><link>http://arxiv.org/abs/2308.12770</link><description>&lt;p&gt;
WavMark&#65306;&#29992;&#20110;&#38899;&#39057;&#29983;&#25104;&#30340;&#27700;&#21360;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
WavMark: Watermarking for Audio Generation. (arXiv:2308.12770v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12770
&lt;/p&gt;
&lt;p&gt;
WavMark&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#38899;&#39057;&#27700;&#21360;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#30701;&#30701;1&#31186;&#30340;&#38899;&#39057;&#29255;&#27573;&#20013;&#32534;&#30721;&#22810;&#36798;32&#20301;&#30340;&#27700;&#21360;&#65292;&#23545;&#20154;&#31867;&#24863;&#23448;&#26080;&#24863;&#30693;&#24182;&#19988;&#20855;&#26377;&#24378;&#22823;&#30340;&#38887;&#24615;&#12290;&#23427;&#21487;&#20197;&#29992;&#20110;&#21512;&#25104;&#22768;&#38899;&#30340;&#26377;&#25928;&#35782;&#21035;&#21644;&#38899;&#39057;&#29256;&#26435;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#38646;-shot&#35821;&#38899;&#21512;&#25104;&#26041;&#38754;&#30340;&#31361;&#30772;&#20351;&#24471;&#21482;&#29992;&#20960;&#31186;&#38047;&#30340;&#24405;&#38899;&#23601;&#33021;&#27169;&#20223;&#35828;&#35805;&#32773;&#30340;&#22768;&#38899;&#65292;&#24182;&#19988;&#20445;&#25345;&#39640;&#24230;&#30340;&#30495;&#23454;&#24863;&#12290;&#38500;&#20102;&#28508;&#22312;&#30340;&#22909;&#22788;&#20043;&#22806;&#65292;&#36825;&#39033;&#24378;&#22823;&#30340;&#25216;&#26415;&#36824;&#24102;&#26469;&#20102;&#26126;&#26174;&#30340;&#39118;&#38505;&#65292;&#21253;&#25324;&#35821;&#38899;&#27450;&#35784;&#21644;&#20882;&#20805;&#35828;&#35805;&#32773;&#12290;&#19982;&#20165;&#20381;&#36182;&#34987;&#21160;&#26041;&#27861;&#26469;&#26816;&#27979;&#21512;&#25104;&#25968;&#25454;&#30340;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#27700;&#21360;&#25216;&#26415;&#25552;&#20379;&#20102;&#19968;&#31181;&#31215;&#26497;&#19988;&#24378;&#22823;&#30340;&#38450;&#24481;&#26426;&#21046;&#26469;&#24212;&#23545;&#36825;&#20123;&#28508;&#22312;&#39118;&#38505;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#38899;&#39057;&#27700;&#21360;&#25216;&#26415;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#20165;1&#31186;&#30340;&#38899;&#39057;&#29255;&#27573;&#20013;&#32534;&#30721;&#22810;&#36798;32&#20301;&#30340;&#27700;&#21360;&#12290;&#27700;&#21360;&#23545;&#20154;&#31867;&#24863;&#23448;&#26469;&#35828;&#26159;&#26080;&#27861;&#23519;&#35273;&#30340;&#65292;&#24182;&#19988;&#23545;&#21508;&#31181;&#25915;&#20987;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#38887;&#24615;&#12290;&#23427;&#21487;&#20197;&#20316;&#20026;&#21512;&#25104;&#22768;&#38899;&#30340;&#26377;&#25928;&#26631;&#35782;&#31526;&#65292;&#24182;&#22312;&#38899;&#39057;&#29256;&#26435;&#20445;&#25252;&#30340;&#26356;&#24191;&#27867;&#24212;&#29992;&#20013;&#20855;&#26377;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#36825;&#20010;&#26694;&#26550;&#20855;&#26377;&#24456;&#39640;&#30340;&#28789;&#27963;&#24615;&#65292;&#21487;&#20197;&#23558;&#22810;&#20010;&#27700;&#21360;&#29255;&#27573;&#36827;&#34892;&#32452;&#21512;&#20197;&#23454;&#29616;&#26356;&#21152;&#20016;&#23500;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent breakthroughs in zero-shot voice synthesis have enabled imitating a speaker's voice using just a few seconds of recording while maintaining a high level of realism. Alongside its potential benefits, this powerful technology introduces notable risks, including voice fraud and speaker impersonation. Unlike the conventional approach of solely relying on passive methods for detecting synthetic data, watermarking presents a proactive and robust defence mechanism against these looming risks. This paper introduces an innovative audio watermarking framework that encodes up to 32 bits of watermark within a mere 1-second audio snippet. The watermark is imperceptible to human senses and exhibits strong resilience against various attacks. It can serve as an effective identifier for synthesized voices and holds potential for broader applications in audio copyright protection. Moreover, this framework boasts high flexibility, allowing for the combination of multiple watermark segments to achi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#20026;&#25918;&#23556;&#23398;&#26500;&#24314;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#21307;&#23398;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#21644;&#25903;&#25345;&#19981;&#21516;&#25918;&#23556;&#23398;&#20219;&#21153;&#30340;&#26550;&#26500;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2308.02463</link><description>&lt;p&gt;
&#20026;&#25918;&#23556;&#23398;&#26500;&#24314;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Towards Generalist Foundation Model for Radiology. (arXiv:2308.02463v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02463
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#20026;&#25918;&#23556;&#23398;&#26500;&#24314;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#21307;&#23398;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#21644;&#25903;&#25345;&#19981;&#21516;&#25918;&#23556;&#23398;&#20219;&#21153;&#30340;&#26550;&#26500;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#21551;&#21160;&#25918;&#23556;&#23398;&#22522;&#30784;&#27169;&#22411;&#30340;&#24320;&#21457;&#65292;&#31216;&#20026;RadFM&#12290;&#25105;&#20204;&#20174;&#25968;&#25454;&#12289;&#27169;&#22411;&#35774;&#35745;&#21644;&#35780;&#20272;&#30340;&#35282;&#24230;&#20840;&#38754;&#32771;&#34385;&#20102;&#22522;&#30784;&#27169;&#22411;&#30340;&#26500;&#24314;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#21487;&#24635;&#32467;&#22914;&#19979;&#65306;&#65288;i&#65289;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#21307;&#23398;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;MedMD&#65292;&#21253;&#25324;1600&#19975;&#20010;2D&#21644;3D&#21307;&#23398;&#25195;&#25551;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21253;&#21547;3D&#21307;&#23398;&#25195;&#25551;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#12290;&#65288;ii&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26550;&#26500;&#65292;&#20351;&#24471;&#21487;&#35270;&#26465;&#20214;&#29983;&#25104;&#39044;&#35757;&#32451;&#25104;&#20026;&#21487;&#33021;&#65292;&#21487;&#20197;&#23558;&#25991;&#26412;&#36755;&#20837;&#19982;2D&#25110;3D&#21307;&#23398;&#25195;&#25551;&#20132;&#38169;&#65292;&#29983;&#25104;&#19981;&#21516;&#25918;&#23556;&#23398;&#20219;&#21153;&#30340;&#21709;&#24212;&#12290;&#35813;&#27169;&#22411;&#39318;&#20808;&#22312;MedMD&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;RadMD&#19978;&#36827;&#34892;&#20102;&#29305;&#23450;&#39046;&#22495;&#30340;&#24494;&#35843;&#65292;RadMD&#26159;MedMD&#30340;&#25918;&#23556;&#23398;&#28165;&#29702;&#29256;&#26412;&#65292;&#21253;&#21547;300&#19975;&#20010;&#25918;&#23556;&#23398;&#30340;&#35270;&#35273;&#35821;&#35328;&#23545;&#12290;&#65288;iii&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#22522;&#20934;&#65292;&#21253;&#25324;&#20116;&#20010;&#20219;&#21153;&#65292;&#26088;&#22312;&#20840;&#38754;&#35780;&#20272;&#35813;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we aim to initiate the development of Radiology Foundation Model, termed as RadFM.We consider the construction of foundational models from the perspectives of data, model design, and evaluation thoroughly. Our contribution can be concluded as follows: (i), we construct a large-scale Medical Multi-modal Dataset, MedMD, consisting of 16M 2D and 3D medical scans. To the best of our knowledge, this is the first multi-modal dataset containing 3D medical scans. (ii), We propose an architecture that enables visually conditioned generative pre-training, allowing for the integration of text input interleaved with 2D or 3D medical scans to generate response for diverse radiologic tasks. The model was initially pre-trained on MedMD and subsequently domain-specific fine-tuned on RadMD, a radiologic cleaned version of MedMD, containing 3M radiologic visual-language pairs. (iii), we propose a new evaluation benchmark that comprises five tasks, aiming to comprehensively assess the capa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26102;&#38388;&#24120;&#35782;&#25512;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#37325;&#28857;&#20851;&#27880;&#36890;&#36807;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#26469;&#25552;&#39640;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22686;&#24378;&#27169;&#22411;&#20173;&#28982;&#38590;&#20197;&#36798;&#21040;&#20154;&#31867;&#27700;&#24179;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.00002</link><description>&lt;p&gt;
&#26102;&#38388;&#24120;&#35782;&#25512;&#29702;&#19982;&#33719;&#21462;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
An Overview Of Temporal Commonsense Reasoning and Acquisition. (arXiv:2308.00002v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26102;&#38388;&#24120;&#35782;&#25512;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#37325;&#28857;&#20851;&#27880;&#36890;&#36807;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#26469;&#25552;&#39640;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22686;&#24378;&#27169;&#22411;&#20173;&#28982;&#38590;&#20197;&#36798;&#21040;&#20154;&#31867;&#27700;&#24179;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24120;&#35782;&#25512;&#29702;&#26159;&#25351;&#29702;&#35299;&#30701;&#35821;&#12289;&#21160;&#20316;&#21644;&#20107;&#20214;&#30340;&#20856;&#22411;&#26102;&#38388;&#32972;&#26223;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#38656;&#35201;&#36825;&#31181;&#30693;&#35782;&#30340;&#38382;&#39064;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#36825;&#31181;&#33021;&#21147;&#22312;&#26102;&#38388;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#21487;&#33021;&#24212;&#29992;&#20110;&#26102;&#38388;&#32447;&#25688;&#35201;&#12289;&#26102;&#38388;&#38382;&#31572;&#21644;&#26102;&#38388;&#33258;&#28982;&#35821;&#35328;&#25512;&#26029;&#31561;&#26041;&#38754;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34429;&#28982;&#21892;&#20110;&#29983;&#25104;&#35821;&#27861;&#27491;&#30830;&#30340;&#21477;&#23376;&#21644;&#35299;&#20915;&#20998;&#31867;&#20219;&#21153;&#65292;&#20294;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#24448;&#24448;&#20250;&#37319;&#21462;&#25463;&#24452;&#65292;&#24182;&#38519;&#20837;&#31616;&#21333;&#30340;&#35821;&#35328;&#38519;&#38449;&#12290;&#26412;&#25991;&#31456;&#27010;&#36848;&#20102;&#22312;&#26102;&#38388;&#24120;&#35782;&#25512;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#29305;&#21035;&#20851;&#27880;&#36890;&#36807;&#21508;&#31181;&#22686;&#24378;&#26041;&#24335;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#20197;&#21450;&#23545;&#36234;&#26469;&#36234;&#22810;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22686;&#24378;&#27169;&#22411;&#22312;&#25512;&#29702;&#20219;&#21153;&#19978;&#20173;&#28982;&#38590;&#20197;&#36798;&#21040;&#20154;&#31867;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal commonsense reasoning refers to the ability to understand the typical temporal context of phrases, actions, and events, and use it to reason over problems requiring such knowledge. This trait is essential in temporal natural language processing tasks, with possible applications such as timeline summarization, temporal question answering, and temporal natural language inference. Recent research on the performance of large language models suggests that, although they are adept at generating syntactically correct sentences and solving classification tasks, they often take shortcuts in their reasoning and fall prey to simple linguistic traps. This article provides an overview of research in the domain of temporal commonsense reasoning, particularly focusing on enhancing language model performance through a variety of augmentations and their evaluation across a growing number of datasets. However, these augmented models still struggle to approach human performance on reasoning task
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#19987;&#23478;&#30693;&#35782;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#20998;&#26512;&#20225;&#19994;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#65292;&#23558;&#20854;&#19982;&#27668;&#20505;&#30456;&#20851;&#37329;&#34701;&#25259;&#38706;&#20219;&#21153;&#32452;&#30340;&#24314;&#35758;&#36827;&#34892;&#23545;&#27604;&#12290;&#35813;&#26041;&#27861;&#26377;&#21161;&#20110;&#35299;&#20915;&#20154;&#21147;&#20998;&#26512;&#25104;&#26412;&#39640;&#12289;&#32570;&#20047;&#36879;&#26126;&#24230;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.15518</link><description>&lt;p&gt;
&#21487;&#25345;&#32493;&#25259;&#38706;&#20998;&#26512;&#20013;&#30340;&#33539;&#24335;&#36716;&#21464;&#65306;&#21033;&#29992;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;CHATREPORT&#36171;&#20104;&#21033;&#30410;&#30456;&#20851;&#32773;&#26435;&#21147;
&lt;/p&gt;
&lt;p&gt;
Paradigm Shift in Sustainability Disclosure Analysis: Empowering Stakeholders with CHATREPORT, a Language Model-Based Tool. (arXiv:2306.15518v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#19987;&#23478;&#30693;&#35782;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#20998;&#26512;&#20225;&#19994;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#65292;&#23558;&#20854;&#19982;&#27668;&#20505;&#30456;&#20851;&#37329;&#34701;&#25259;&#38706;&#20219;&#21153;&#32452;&#30340;&#24314;&#35758;&#36827;&#34892;&#23545;&#27604;&#12290;&#35813;&#26041;&#27861;&#26377;&#21161;&#20110;&#35299;&#20915;&#20154;&#21147;&#20998;&#26512;&#25104;&#26412;&#39640;&#12289;&#32570;&#20047;&#36879;&#26126;&#24230;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19987;&#23478;&#30693;&#35782;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30456;&#32467;&#21512;&#65292;&#33258;&#21160;&#20998;&#26512;&#20225;&#19994;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#65292;&#24182;&#23558;&#20854;&#19982;&#12298;&#27668;&#20505;&#30456;&#20851;&#37329;&#34701;&#25259;&#38706;&#20219;&#21153;&#32452;&#12299;&#65288;TCFD&#65289;&#24314;&#35758;&#36827;&#34892;&#22522;&#20934;&#23545;&#27604;&#12290;&#20225;&#19994;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#23545;&#20110;&#35780;&#20272;&#32452;&#32455;&#30340;&#29615;&#22659;&#21644;&#31038;&#20250;&#39118;&#38505;&#21644;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20998;&#26512;&#36825;&#20123;&#25253;&#21578;&#20013;&#22823;&#37327;&#30340;&#20449;&#24687;&#24448;&#24448;&#20250;&#23548;&#33268;&#20154;&#21147;&#25104;&#26412;&#36807;&#39640;&#12290;&#22240;&#27492;&#65292;&#20840;&#29699;&#21482;&#26377;&#23569;&#25968;&#26426;&#26500;&#26377;&#36164;&#28304;&#26469;&#20998;&#26512;&#36825;&#20123;&#25253;&#21578;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#32570;&#20047;&#36879;&#26126;&#24230;&#12290;&#34429;&#28982;AI&#39537;&#21160;&#30340;&#24037;&#20855;&#21487;&#20197;&#33258;&#21160;&#20998;&#26512;&#25968;&#25454;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;&#23427;&#20204;&#23481;&#26131;&#20986;&#29616;&#19981;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;LLMs&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#33258;&#21160;&#20998;&#26512;&#20225;&#19994;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#12290;&#25105;&#20204;&#23558;&#20854;&#21629;&#21517;&#20026;CHATREPORT&#24037;&#20855;&#65292;&#24182;&#22312;&#31532;&#19968;&#20010;&#24212;&#29992;&#26696;&#20363;&#20013;&#23558;&#20854;&#24212;&#29992;&#20110;&#35780;&#20272;&#20225;&#19994;&#30340;&#27668;&#20505;&#39118;&#38505;&#25259;&#38706;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel approach to enhance Large Language Models (LLMs) with expert knowledge to automate the analysis of corporate sustainability reports by benchmarking them against the Task Force for Climate-Related Financial Disclosures (TCFD) recommendations. Corporate sustainability reports are crucial in assessing organizations' environmental and social risks and impacts. However, analyzing these reports' vast amounts of information makes human analysis often too costly. As a result, only a few entities worldwide have the resources to analyze these reports, which could lead to a lack of transparency. While AI-powered tools can automatically analyze the data, they are prone to inaccuracies as they lack domain-specific expertise. This paper introduces a novel approach to enhance LLMs with expert knowledge to automate the analysis of corporate sustainability reports. We christen our tool CHATREPORT, and apply it in a first use case to assess corporate climate risk disclosure
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#21644;&#29983;&#25104;&#38463;&#25289;&#20271;&#25991;&#26412;&#26102;&#20986;&#29616;&#30340;&#25991;&#21270;&#20559;&#21521;&#35199;&#26041;&#25991;&#21270;&#30340;&#29616;&#35937;&#65292;&#34920;&#26126;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#21517;&#12289;&#39135;&#21697;&#12289;&#26381;&#35013;&#12289;&#22320;&#28857;&#12289;&#25991;&#23398;&#12289;&#39278;&#26009;&#12289;&#23447;&#25945;&#21644;&#20307;&#32946;&#31561;&#20843;&#20010;&#25991;&#21270;&#26041;&#38754;&#23384;&#22312;&#20559;&#35265;&#12290;&#36825;&#20123;&#21457;&#29616;&#24341;&#21457;&#23545;&#20110;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#25991;&#21270;&#30456;&#20851;&#24615;&#30340;&#25285;&#24551;&#12290;</title><link>http://arxiv.org/abs/2305.14456</link><description>&lt;p&gt;
&#22312;&#31048;&#31095;&#20043;&#21518;&#21917;&#21860;&#37202;&#65311;&#27979;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#21270;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Having Beer after Prayer? Measuring Cultural Bias in Large Language Models. (arXiv:2305.14456v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14456
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#21644;&#29983;&#25104;&#38463;&#25289;&#20271;&#25991;&#26412;&#26102;&#20986;&#29616;&#30340;&#25991;&#21270;&#20559;&#21521;&#35199;&#26041;&#25991;&#21270;&#30340;&#29616;&#35937;&#65292;&#34920;&#26126;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#21517;&#12289;&#39135;&#21697;&#12289;&#26381;&#35013;&#12289;&#22320;&#28857;&#12289;&#25991;&#23398;&#12289;&#39278;&#26009;&#12289;&#23447;&#25945;&#21644;&#20307;&#32946;&#31561;&#20843;&#20010;&#25991;&#21270;&#26041;&#38754;&#23384;&#22312;&#20559;&#35265;&#12290;&#36825;&#20123;&#21457;&#29616;&#24341;&#21457;&#23545;&#20110;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#25991;&#21270;&#30456;&#20851;&#24615;&#30340;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#23384;&#22312;&#25991;&#21270;&#20559;&#35265;&#65311;&#35821;&#35328;&#27169;&#22411;&#31526;&#21512;&#25152;&#26381;&#21153;&#31038;&#21306;&#30340;&#25991;&#21270;&#22240;&#32032;&#24456;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#26412;&#25991;&#34920;&#26126;&#22312;&#22788;&#29702;&#21644;&#29983;&#25104;&#38463;&#25289;&#20271;&#25991;&#26412;&#26102;&#65292;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#26174;&#33879;&#30340;&#20559;&#21521;&#35199;&#26041;&#25991;&#21270;&#30340;&#20559;&#35265;&#65292;&#20542;&#21521;&#20110;&#20135;&#29983;&#35199;&#26041;&#25991;&#21270;&#30456;&#20851;&#20869;&#23481;&#32780;&#38750;&#38463;&#25289;&#20271;&#25991;&#21270;&#30456;&#20851;&#20869;&#23481;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#20174;&#22312;&#32447;&#31038;&#20132;&#23186;&#20307;&#19978;&#25910;&#38598;&#30340;&#33258;&#28982;&#20986;&#29616;&#30340;&#19978;&#19979;&#25991;&#21644;&#22522;&#20110;&#21487;&#33021;&#24615;&#35780;&#20998;&#30340;&#25351;&#26631;&#26469;&#37327;&#21270;&#36825;&#31181;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#26174;&#31034;&#65292;&#38463;&#25289;&#20271;&#35821;&#21333;&#35821;&#21644;&#22810;&#35821;&#27169;&#22411;&#22312;&#20843;&#20010;&#19981;&#21516;&#30340;&#25991;&#21270;&#26041;&#38754;&#23384;&#22312;&#35199;&#26041;&#25991;&#21270;&#20559;&#35265;&#65292;&#21253;&#25324;&#20154;&#21517;&#12289;&#39135;&#21697;&#12289;&#26381;&#35013;&#12289;&#22320;&#28857;&#12289;&#25991;&#23398;&#12289;&#39278;&#26009;&#12289;&#23447;&#25945;&#21644;&#20307;&#32946;&#12290;&#24403;&#36755;&#20837;&#30340;&#38463;&#25289;&#20271;&#35821;&#21477;&#23376;&#36234;&#25509;&#36817;&#33521;&#35821;&#26102;&#65292;&#27169;&#22411;&#20063;&#26356;&#23481;&#26131;&#34920;&#29616;&#20986;&#20559;&#35265;&#12290;&#36825;&#20123;&#21457;&#29616;&#24341;&#21457;&#20154;&#20204;&#23545;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#25991;&#21270;&#30456;&#20851;&#24615;&#30340;&#25285;&#24551;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#27169;&#22411;&#35774;&#35745;&#20013;&#24212;&#26356;&#22810;&#32771;&#34385;&#25991;&#21270;&#22240;&#32032;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Are language models culturally biased? It is important that language models conform to the cultural aspects of the communities they serve. However, we show in this paper that language models suffer from a significant bias towards Western culture when handling and generating text in Arabic, often preferring, and producing Western-fitting content as opposed to the relevant Arab content. We quantify this bias through a likelihood scoring-based metric using naturally occurring contexts that we collect from online social media. Our experiments reveal that both Arabic monolingual and multilingual models exhibit bias towards Western culture in eight different cultural aspects: person names, food, clothing, location, literature, beverage, religion, and sports. Models also tend to exhibit more bias when prompted with Arabic sentences that are more linguistically aligned with English. These findings raise concerns about the cultural relevance of current language models. Our analyses show that pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#36866;&#24212;&#19968;&#33268;&#24615; (Adaptive-Consistency) &#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#21160;&#24577;&#35843;&#25972;&#27599;&#20010;&#38382;&#39064;&#30340;&#26679;&#26412;&#25968;&#37327;&#65292;&#20174;&#32780;&#26377;&#25928;&#20943;&#23569;&#26679;&#26412;&#39044;&#31639;&#65292;&#24182;&#36739;&#23567;&#31243;&#24230;&#22320;&#38477;&#20302;&#20102;&#24179;&#22343;&#20934;&#30830;&#24230;</title><link>http://arxiv.org/abs/2305.11860</link><description>&lt;p&gt;
&#20998;&#27493;&#37319;&#26679;&#65306;&#29992;&#33258;&#36866;&#24212;&#19968;&#33268;&#24615;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#25928;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning with LLMs. (arXiv:2305.11860v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#36866;&#24212;&#19968;&#33268;&#24615; (Adaptive-Consistency) &#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#21160;&#24577;&#35843;&#25972;&#27599;&#20010;&#38382;&#39064;&#30340;&#26679;&#26412;&#25968;&#37327;&#65292;&#20174;&#32780;&#26377;&#25928;&#20943;&#23569;&#26679;&#26412;&#39044;&#31639;&#65292;&#24182;&#36739;&#23567;&#31243;&#24230;&#22320;&#38477;&#20302;&#20102;&#24179;&#22343;&#20934;&#30830;&#24230;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25913;&#33391;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#36755;&#20986;&#27491;&#30830;&#24615;&#30340;&#19968;&#20010;&#27969;&#34892;&#26041;&#27861;&#26159;&#20351;&#29992;&#33258;&#19968;&#33268;&#24615;&#8212;&#8212;&#23545; LLM &#36827;&#34892;&#22810;&#27425;&#25237;&#31080;&#24182;&#36755;&#20986;&#26368;&#39057;&#32321;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#29616;&#26377;&#30340;&#33258;&#19968;&#33268;&#24615;&#25216;&#26415;&#37117;&#26159;&#27599;&#20010;&#38382;&#39064;&#37117;&#20250;&#22266;&#23450;&#37319;&#38598;&#19968;&#23450;&#25968;&#37327;&#30340;&#26679;&#26412;&#65292;&#32780;&#26356;&#22909;&#30340;&#26041;&#27861;&#26159;&#26681;&#25454;&#24050;&#32463;&#37319;&#38598;&#21040;&#30340;&#26679;&#26412;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#38750;&#22343;&#21248;&#22320;&#20998;&#37197;&#39044;&#31639;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#36866;&#24212;&#19968;&#33268;&#24615; (Adaptive-Consistency) &#30340;&#25104;&#26412;&#26377;&#25928;&#12289;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#25216;&#26415;&#65292;&#23427;&#20351;&#29992;&#36731;&#37327;&#32423;&#20572;&#27490;&#20934;&#21017;&#21160;&#24577;&#35843;&#25972;&#27599;&#20010;&#38382;&#39064;&#30340;&#26679;&#26412;&#25968;&#37327;&#12290;&#25105;&#20204;&#22312; 13 &#20010;&#25968;&#25454;&#38598;&#21644;&#20004;&#20010; LLM &#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#33258;&#36866;&#24212;&#19968;&#33268;&#24615;&#21487;&#20197;&#23558;&#26679;&#26412;&#39044;&#31639;&#38477;&#20302;&#22810;&#36798; 6 &#20493;&#65292;&#24182;&#19988;&#24179;&#22343;&#20934;&#30830;&#24230;&#38477;&#20302;&#19981;&#21040; 0.1%&#12290;
&lt;/p&gt;
&lt;p&gt;
A popular approach for improving the correctness of output from large language models (LLMs) is Self-Consistency - poll the LLM multiple times and output the most frequent solution. Existing Self-Consistency techniques always draw a constant number of samples per question, where a better approach will be to non-uniformly distribute the available budget based on the amount of agreement in the samples drawn so far. In response, we introduce Adaptive-Consistency, a cost-efficient, model-agnostic technique that dynamically adjusts the number of samples per question using a lightweight stopping criterion. Our experiments over 13 datasets and two LLMs demonstrate that Adaptive-Consistency reduces sample budget by up to 6.0 times with an average accuracy drop of less than 0.1%.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#39044;&#27979;&#21333;&#20010;&#26631;&#27880;&#32773;&#30340;&#25171;&#20998;&#65292;&#24182;&#32467;&#21512;&#25991;&#26412;&#30446;&#26631;&#32676;&#20307;&#30340;&#39044;&#27979;&#65292;&#27169;&#25311;&#20102;&#30446;&#26631;&#32676;&#20307;&#25104;&#21592;&#30340;&#24847;&#35265;&#65292;&#36890;&#36807;&#20351;&#29992;&#20182;&#20204;&#30340;&#20154;&#21475;&#32479;&#35745;&#23398;&#25968;&#25454;&#21644;&#22312;&#32447;&#24847;&#35265;&#39044;&#27979;&#26631;&#27880;&#32773;&#30340;&#25171;&#20998;&#65292;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#31561;&#20027;&#35266;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.06626</link><description>&lt;p&gt;
&#24403;&#22810;&#25968;&#20154;&#26159;&#38169;&#35823;&#30340;&#65306;&#21033;&#29992;&#26631;&#27880;&#32773;&#19981;&#19968;&#33268;&#24615;&#36827;&#34892;&#20027;&#35266;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
When the Majority is Wrong: Leveraging Annotator Disagreement for Subjective Tasks. (arXiv:2305.06626v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#39044;&#27979;&#21333;&#20010;&#26631;&#27880;&#32773;&#30340;&#25171;&#20998;&#65292;&#24182;&#32467;&#21512;&#25991;&#26412;&#30446;&#26631;&#32676;&#20307;&#30340;&#39044;&#27979;&#65292;&#27169;&#25311;&#20102;&#30446;&#26631;&#32676;&#20307;&#25104;&#21592;&#30340;&#24847;&#35265;&#65292;&#36890;&#36807;&#20351;&#29992;&#20182;&#20204;&#30340;&#20154;&#21475;&#32479;&#35745;&#23398;&#25968;&#25454;&#21644;&#22312;&#32447;&#24847;&#35265;&#39044;&#27979;&#26631;&#27880;&#32773;&#30340;&#25171;&#20998;&#65292;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#31561;&#20027;&#35266;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#34429;&#28982;&#36890;&#24120;&#20351;&#29992;&#26631;&#27880;&#32773;&#30340;&#22810;&#25968;&#25237;&#31080;&#26469;&#30830;&#23450;&#26631;&#31614;&#65292;&#20294;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#31561;&#20027;&#35266;&#20219;&#21153;&#20013;&#65292;&#26631;&#27880;&#32773;&#20043;&#38388;&#23384;&#22312;&#19981;&#19968;&#33268;&#24615;&#21487;&#33021;&#21453;&#26144;&#20986;&#32676;&#20307;&#35266;&#28857;&#30340;&#24046;&#24322;&#65292;&#32780;&#19981;&#26159;&#22122;&#22768;&#12290;&#22240;&#27492;&#65292;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#19968;&#20010;&#35821;&#21477;&#26159;&#21542;&#20882;&#29359;&#20102;&#23427;&#25152;&#38024;&#23545;&#30340;&#20154;&#32676;&#65292;&#32780;&#36825;&#21487;&#33021;&#21482;&#21344;&#26631;&#27880;&#32773;&#27744;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;&#26412;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#39044;&#27979;&#21487;&#33021;&#20855;&#26377;&#20882;&#29359;&#24615;&#25991;&#26412;&#19978;&#27599;&#20010;&#26631;&#27880;&#32773;&#30340;&#25171;&#20998;&#65292;&#24182;&#32467;&#21512;&#25991;&#26412;&#30340;&#39044;&#27979;&#30446;&#26631;&#32676;&#20307;&#26469;&#27169;&#25311;&#30446;&#26631;&#32676;&#20307;&#25104;&#21592;&#30340;&#24847;&#35265;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31995;&#21015;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#21253;&#25324;&#25552;&#39640;&#20102;22&#65285;&#22312;&#39044;&#27979;&#27599;&#20010;&#26631;&#27880;&#32773;&#30340;&#25171;&#20998;&#19978;&#30340;&#24615;&#33021;&#65292;&#25552;&#39640;&#20102;33&#65285;&#22312;&#39044;&#27979;&#26631;&#27880;&#32773;&#20043;&#38388;&#26041;&#24046;&#19978;&#30340;&#24615;&#33021;&#65292;&#36825;&#25552;&#20379;&#20102;&#19979;&#28216;&#29992;&#26469;&#34913;&#37327;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#21487;&#20197;&#20351;&#29992;&#26631;&#27880;&#32773;&#30340;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#21644;&#20854;&#22312;&#32447;&#24847;&#35265;&#26469;&#39044;&#27979;&#26631;&#27880;&#32773;&#30340;&#25171;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Though majority vote among annotators is typically used for ground truth labels in natural language processing, annotator disagreement in tasks such as hate speech detection may reflect differences among group opinions, not noise. Thus, a crucial problem in hate speech detection is whether a statement is offensive to the demographic group that it targets, which may constitute a small fraction of the annotator pool. We construct a model that predicts individual annotator ratings on potentially offensive text and combines this information with the predicted target group of the text to model the opinions of target group members. We show gains across a range of metrics, including raising performance over the baseline by 22% at predicting individual annotators' ratings and 33% at predicting variance among annotators, which provides a method of measuring model uncertainty downstream. We find that annotators' ratings can be predicted using their demographic information and opinions on online 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#22522;&#20110;LLMs&#27169;&#25311;&#20195;&#29702;&#30340;&#34892;&#20026;&#65292;&#31216;&#20043;&#20026;LLM Personas&#65292;&#22312;&#20998;&#37197;&#22823;&#20116;&#20154;&#26684;&#31867;&#22411;&#21644;&#24615;&#21035;&#35282;&#33394;&#26102;&#26159;&#21542;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#19968;&#33268;&#24615;&#30340;&#20010;&#24615;&#21270;&#29305;&#36136;&#30340;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2305.02547</link><description>&lt;p&gt;
PersonaLLM: &#25506;&#31350;GPT-3.5&#34920;&#36798;&#20010;&#24615;&#29305;&#24449;&#21644;&#24615;&#21035;&#24046;&#24322;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
PersonaLLM: Investigating the Ability of GPT-3.5 to Express Personality Traits and Gender Differences. (arXiv:2305.02547v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02547
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#22522;&#20110;LLMs&#27169;&#25311;&#20195;&#29702;&#30340;&#34892;&#20026;&#65292;&#31216;&#20043;&#20026;LLM Personas&#65292;&#22312;&#20998;&#37197;&#22823;&#20116;&#20154;&#26684;&#31867;&#22411;&#21644;&#24615;&#21035;&#35282;&#33394;&#26102;&#26159;&#21542;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#19968;&#33268;&#24615;&#30340;&#20010;&#24615;&#21270;&#29305;&#36136;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#20010;&#34892;&#19994;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#35774;&#35745;&#20013;&#26377;&#35768;&#22810;&#29992;&#36884;&#65292;&#24182;&#19988;&#30740;&#31350;&#34920;&#26126;&#20010;&#24615;&#21270;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#28385;&#36275;&#19981;&#21516;&#20154;&#26684;&#29305;&#24449;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#35780;&#20272;&#20010;&#24615;&#21270;LLM&#30340;&#34892;&#20026;&#26159;&#21542;&#33021;&#22815;&#20934;&#30830;&#12289;&#19968;&#33268;&#22320;&#21453;&#26144;&#26576;&#20123;&#20154;&#26684;&#29305;&#24449;&#12290;&#25105;&#20204;&#32771;&#34385;&#30740;&#31350;&#22522;&#20110;LLM&#30340;&#27169;&#25311;&#20195;&#29702;&#30340;&#34892;&#20026;&#65292;&#31216;&#20043;&#20026;LLM personas&#65292;&#24182;&#20351;&#29992;GPT-3.5&#65288;text-davinci-003&#65289;&#36827;&#34892;&#26696;&#20363;&#30740;&#31350;&#65292;&#20197;&#30740;&#31350;LLM&#22312;&#20998;&#37197;&#22823;&#20116;&#20154;&#26684;&#31867;&#22411;&#21644;&#24615;&#21035;&#35282;&#33394;&#26102;&#26159;&#21542;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#19968;&#33268;&#24615;&#30340;&#20010;&#24615;&#21270;&#29305;&#36136;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;320&#20010;LLM personas&#65288;&#27599;&#31181;&#22823;&#20116;&#20154;&#26684;&#31867;&#22411;&#26377;5&#20010;&#22899;&#24615;&#21644;5&#20010;&#30007;&#24615;&#65289;&#65292;&#24182;&#25552;&#31034;&#20182;&#20204;&#23436;&#25104;&#32463;&#20856;&#30340;44&#39033;&#22823;&#20116;&#20154;&#26684;&#38382;&#21367;&#65288;BFI&#65289;&#65292;&#28982;&#21518;&#25776;&#20889;&#19968;&#20010;&#20851;&#20110;&#20182;&#20204;&#31461;&#24180;&#30340;800&#23383;&#25925;&#20107;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;LLM personas&#30340;&#33258;&#25105;&#25253;&#21578;&#30340;BFI&#20998;&#25968;&#19982;&#20182;&#20204;&#20998;&#37197;&#30340;&#20154;&#26684;&#31867;&#22411;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the many use cases for large language models (LLMs) in the design of chatbots in various industries and the research showing the importance of personalizing chatbots to cater to different personality traits, little work has been done to evaluate whether the behaviors of personalized LLMs can reflect certain personality traits accurately and consistently. We consider studying the behavior of LLM-based simulated agents which refer to as LLM personas and present a case study with GPT-3.5 (text-davinci-003) to investigate whether LLMs can generate content with consistent, personalized traits when assigned Big Five personality types and gender roles. We created 320 LLM personas (5 females and 5 males for each of the 32 Big Five personality types) and prompted them to complete the classic 44-item Big Five Inventory (BFI) and then write an 800-word story about their childhood. Results showed that LLM personas' self-reported BFI scores are consistent with their assigned personality typ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33258;&#22238;&#24402;&#25991;&#26412;&#29983;&#25104;&#20013;&#20351;&#29992;&#21487;&#25805;&#20316;&#27010;&#29575;&#27169;&#22411;&#26469;&#24378;&#21046;&#23454;&#26045;&#38480;&#21046;&#30340;&#25511;&#21046;&#26041;&#27861;GeLaTo&#65292;&#24182;&#21462;&#24471;&#20102;&#22312;&#24120;&#35265;&#30340;&#32422;&#26463;&#25991;&#26412;&#29983;&#25104;&#27979;&#35797;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.07438</link><description>&lt;p&gt;
&#21487;&#25805;&#20316;&#30340;&#33258;&#22238;&#24402;&#35821;&#35328;&#29983;&#25104;&#25511;&#21046;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Tractable Control for Autoregressive Language Generation. (arXiv:2304.07438v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07438
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33258;&#22238;&#24402;&#25991;&#26412;&#29983;&#25104;&#20013;&#20351;&#29992;&#21487;&#25805;&#20316;&#27010;&#29575;&#27169;&#22411;&#26469;&#24378;&#21046;&#23454;&#26045;&#38480;&#21046;&#30340;&#25511;&#21046;&#26041;&#27861;GeLaTo&#65292;&#24182;&#21462;&#24471;&#20102;&#22312;&#24120;&#35265;&#30340;&#32422;&#26463;&#25991;&#26412;&#29983;&#25104;&#27979;&#35797;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33258;&#22238;&#24402;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#29983;&#25104;&#28385;&#36275;&#22797;&#26434;&#38480;&#21046;&#30340;&#25991;&#26412;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65306;&#21363;&#20351;&#26159;&#26368;&#31616;&#21333;&#30340;&#35789;&#27719;&#38480;&#21046;&#20063;&#20351;&#26465;&#20214;&#20998;&#24067;$\Pr(\text{text} | \alpha)$&#30340;&#37319;&#26679;&#21464;&#24471;&#19981;&#21487;&#35745;&#31639;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#21487;&#25805;&#20316;&#30340;&#27010;&#29575;&#27169;&#22411;&#23558;&#35789;&#27719;&#38480;&#21046;&#24378;&#21152;&#20110;&#33258;&#22238;&#24402;&#25991;&#26412;&#29983;&#25104;&#20013;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026; GeLaTo&#12290;&#20026;&#20102;&#35777;&#26126;&#36825;&#20010;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#31934;&#31616;&#30340;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#26469;&#25511;&#21046;&#20174;GPT2&#21040;&#33258;&#22238;&#24402;&#30340;&#29983;&#25104;&#12290;GeLaTo&#22312;&#32422;&#26463;&#25991;&#26412;&#29983;&#25104;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;CommonGen&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#22823;&#24133;&#20987;&#36133;&#20102;&#21508;&#31181;&#24378;&#22522;&#32447;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#19981;&#20165;&#20026;&#25511;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#65292;&#36824;&#28608;&#21169;&#20154;&#20204;&#24320;&#21457;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#21487;&#25805;&#20316;&#27010;&#29575;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the success of autoregressive large language models in text generation, it remains a major challenge to generate text that satisfies complex constraints: sampling from the conditional distribution $\Pr(\text{text} | \alpha)$ is intractable for even the simplest lexical constraints $\alpha$. To overcome this challenge, we propose to use tractable probabilistic models to impose lexical constraints in autoregressive text generation, which we refer to as GeLaTo. To demonstrate the effectiveness of this framework, we use distilled hidden Markov models to control autoregressive generation from GPT2. GeLaTo achieves state-of-the-art performance on CommonGen, a challenging benchmark for constrained text generation, beating a wide range of strong baselines by a large margin. Our work not only opens up new avenues for controlling large language models but also motivates the development of more expressive tractable probabilistic models.
&lt;/p&gt;</description></item><item><title>EvoPrompting&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#33258;&#36866;&#24212;&#21464;&#24322;&#21644;&#20132;&#21449;&#25805;&#20316;&#31526;&#26469;&#36827;&#34892;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65292;&#22312;MNIST-1D&#25968;&#25454;&#38598;&#21644;CLRS&#31639;&#27861;&#25512;&#29702;&#22522;&#20934;&#19978;&#37117;&#21462;&#24471;&#20102;&#27604;&#20154;&#31867;&#35774;&#35745;&#30340;&#26550;&#26500;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2302.14838</link><description>&lt;p&gt;
EvoPrompting: &#36866;&#29992;&#20110;&#20195;&#30721;&#32423;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
EvoPrompting: Language Models for Code-Level Neural Architecture Search. (arXiv:2302.14838v1 [cs.NE] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14838
&lt;/p&gt;
&lt;p&gt;
EvoPrompting&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#33258;&#36866;&#24212;&#21464;&#24322;&#21644;&#20132;&#21449;&#25805;&#20316;&#31526;&#26469;&#36827;&#34892;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65292;&#22312;MNIST-1D&#25968;&#25454;&#38598;&#21644;CLRS&#31639;&#27861;&#25512;&#29702;&#22522;&#20934;&#19978;&#37117;&#21462;&#24471;&#20102;&#27604;&#20154;&#31867;&#35774;&#35745;&#30340;&#26550;&#26500;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#30340;&#26368;&#26032;&#25104;&#23601;&#65292;&#25105;&#20204;&#25506;&#32034;&#23558;LM&#20316;&#20026;&#36827;&#21270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#31639;&#27861;&#30340;&#33258;&#36866;&#24212;&#21464;&#24322;&#21644;&#20132;&#21449;&#25805;&#20316;&#31526;&#30340;&#20351;&#29992;&#12290;&#23613;&#31649;NAS&#20173;&#28982;&#36807;&#20110;&#22256;&#38590;&#65292;&#20197;&#33267;&#20110;&#20165;&#20165;&#36890;&#36807;&#25552;&#31034;&#23601;&#38590;&#20197;&#25104;&#21151;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#36827;&#21270;&#25552;&#31034;&#24037;&#31243;&#19982;&#36719;&#25552;&#31034;&#35843;&#25972;&#30340;&#32452;&#21512;&#65292;&#19968;&#31181;&#25105;&#20204;&#31216;&#20043;&#20026;EvoPrompting&#30340;&#26041;&#27861;&#65292;&#22987;&#32456;&#21487;&#20197;&#21457;&#29616;&#22810;&#26679;&#21270;&#19988;&#24615;&#33021;&#39640;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;EvoPrompting&#22312;MNIST-1D&#25968;&#25454;&#38598;&#19978;&#26159;&#26377;&#25928;&#30340;&#65292;&#20854;&#20013;EvoPrompting&#20135;&#29983;&#30340;&#21367;&#31215;&#26550;&#26500;&#21464;&#20307;&#22312;&#20934;&#30830;&#29575;&#21644;&#27169;&#22411;&#22823;&#23567;&#26041;&#38754;&#22343;&#20248;&#20110;&#20154;&#31867;&#19987;&#23478;&#35774;&#35745;&#30340;&#26550;&#26500;&#21644;&#22825;&#30495;&#30340;&#23569;&#25968;&#20808;&#23548;&#25552;&#31034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#22312;CLRS&#31639;&#27861;&#25512;&#29702;&#22522;&#20934;&#19978;&#25628;&#32034;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#20013;EvoPrompting&#33021;&#22815;&#35774;&#35745;&#20986;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#26356;&#22909;&#30340;&#26032;&#39062;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the recent impressive accomplishments of language models (LMs) for code generation, we explore the use of LMs as adaptive mutation and crossover operators for an evolutionary neural architecture search (NAS) algorithm. While NAS still proves too difficult a task for LMs to succeed at solely through prompting, we find that the combination of evolutionary prompt engineering with soft prompt-tuning, a method we term EvoPrompting, consistently finds diverse and high performing models. We first demonstrate that EvoPrompting is effective on the computationally efficient MNIST-1D dataset, where EvoPrompting produces convolutional architecture variants that outperform both those designed by human experts and naive few-shot prompting in terms of accuracy and model size. We then apply our method to searching for graph neural networks on the CLRS Algorithmic Reasoning Benchmark, where EvoPrompting is able to design novel architectures that outperform current state-of-the-art models on 21 ou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#38454;&#32447;&#24615;&#36923;&#36753;&#19982;&#25193;&#23637;&#24352;&#37327;&#31867;&#22411;&#28436;&#31639;&#30340;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22266;&#26377;&#30340;&#28436;&#32462;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2206.08955</link><description>&lt;p&gt;
&#35753;&#19968;&#38454;&#32447;&#24615;&#36923;&#36753;&#25104;&#20026;&#29983;&#25104;&#35821;&#27861;
&lt;/p&gt;
&lt;p&gt;
Making first order linear logic a generating grammar. (arXiv:2206.08955v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.08955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#38454;&#32447;&#24615;&#36923;&#36753;&#19982;&#25193;&#23637;&#24352;&#37327;&#31867;&#22411;&#28436;&#31639;&#30340;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22266;&#26377;&#30340;&#28436;&#32462;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#19981;&#21516;&#30340;&#33539;&#30068;&#35821;&#27861;&#22312;&#19968;&#38454;&#20056;&#27861;&#32447;&#24615;&#36923;&#36753;&#30340;&#19968;&#20010;&#29255;&#27573;&#20013;&#20855;&#26377;&#34920;&#38754;&#34920;&#31034;&#12290; &#25105;&#20204;&#34920;&#26126;&#65292;&#35813;&#29255;&#27573;&#31561;&#20215;&#20110;&#26368;&#36817;&#24341;&#20837;&#30340;&#25193;&#23637;&#24352;&#37327;&#31867;&#22411;&#28436;&#31639;&#12290; &#36825;&#19981;&#20165;&#20026;&#21069;&#32773;&#25552;&#20379;&#20102;&#19968;&#20123;&#26367;&#20195;&#30340;&#35821;&#27861;&#21644;&#30452;&#35266;&#30340;&#20960;&#20309;&#34920;&#31034;&#65292;&#32780;&#19988;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#22266;&#26377;&#30340;&#28436;&#32462;&#31995;&#32479;&#65292;&#36825;&#26159;&#20197;&#21069;&#32570;&#23569;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is known that different categorial grammars have surface representation in a fragment of first order multiplicative linear logic. We show that the fragment of interest is equivalent to the recently introduced {\it extended tensor type calculus}. This provides the former not only with some alternative syntax and intuitive geometric representation, but also with an intrinsic deductive system, which has been absent.
&lt;/p&gt;</description></item></channel></rss>