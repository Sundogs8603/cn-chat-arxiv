<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>DialogStudio&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#19988;&#26368;&#22810;&#26679;&#21270;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#21512;&#65292;&#21253;&#21547;&#20174;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#21040;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#12289;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#20250;&#35805;&#25512;&#33616;&#12289;&#23545;&#35805;&#25688;&#35201;&#21644;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#30340;&#25968;&#25454;&#12290;&#23427;&#20026;&#23545;&#35805;&#30740;&#31350;&#21644;&#27169;&#22411;&#35757;&#32451;&#25552;&#20379;&#20102;&#20016;&#23500;&#32780;&#22810;&#26679;&#21270;&#30340;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2307.10172</link><description>&lt;p&gt;
DialogStudio&#65306;&#38754;&#21521;&#20250;&#35805; AI &#30340;&#26368;&#20016;&#23500;&#21644;&#26368;&#22810;&#26679;&#21270;&#30340;&#32479;&#19968;&#25968;&#25454;&#38598;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
DialogStudio: Towards Richest and Most Diverse Unified Dataset Collection for Conversational AI. (arXiv:2307.10172v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10172
&lt;/p&gt;
&lt;p&gt;
DialogStudio&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#19988;&#26368;&#22810;&#26679;&#21270;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#21512;&#65292;&#21253;&#21547;&#20174;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#21040;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#12289;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#20250;&#35805;&#25512;&#33616;&#12289;&#23545;&#35805;&#25688;&#35201;&#21644;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#30340;&#25968;&#25454;&#12290;&#23427;&#20026;&#23545;&#35805;&#30740;&#31350;&#21644;&#27169;&#22411;&#35757;&#32451;&#25552;&#20379;&#20102;&#20016;&#23500;&#32780;&#22810;&#26679;&#21270;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20250;&#35805; AI &#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#22810;&#26679;&#21270;&#30340;&#23545;&#35805;&#20219;&#21153;&#26102;&#38754;&#20020;&#25361;&#25112;&#65292;&#29616;&#26377;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#24448;&#24448;&#32570;&#20047;&#22810;&#26679;&#24615;&#21644;&#20840;&#38754;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; DialogStudio&#65306;&#26368;&#22823;&#12289;&#26368;&#22810;&#26679;&#21270;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#38598;&#21512;&#65292;&#20197;&#19968;&#33268;&#30340;&#26684;&#24335;&#32479;&#19968;&#65292;&#21516;&#26102;&#20445;&#30041;&#20854;&#21407;&#22987;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#38598;&#21512;&#21253;&#25324;&#26469;&#33258;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#12289;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#12289;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#20250;&#35805;&#25512;&#33616;&#12289;&#23545;&#35805;&#25688;&#35201;&#21644;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#30340;&#25968;&#25454;&#65292;&#20026;&#23545;&#35805;&#30740;&#31350;&#21644;&#27169;&#22411;&#35757;&#32451;&#25552;&#20379;&#20102;&#38750;&#24120;&#20016;&#23500;&#21644;&#22810;&#26679;&#21270;&#30340;&#36164;&#28304;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378; DialogStudio &#30340;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#20026;&#27599;&#20010;&#25968;&#25454;&#38598;&#30830;&#23450;&#20102;&#35768;&#21487;&#35777;&#65292;&#24182;&#20026;&#36873;&#23450;&#23545;&#35805;&#35774;&#35745;&#20102;&#39046;&#22495;&#24863;&#30693;&#25552;&#31034;&#65292;&#20197;&#20415;&#20419;&#36827;&#25351;&#23548;&#24863;&#30693;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#25968;&#25454;&#38598;&#38598;&#21512;&#24320;&#21457;&#20102;&#20250;&#35805; AI &#27169;&#22411;&#65292;&#24182;&#22312;&#38646;&#25688;&#35201;&#29983;&#25104;&#21644;&#20998;&#24067;&#24335;&#25991;&#23383;&#22522;&#20934;&#23545;&#35805;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite advancements in conversational AI, language models encounter challenges to handle diverse conversational tasks, and existing dialogue dataset collections often lack diversity and comprehensiveness. To tackle these issues, we introduce DialogStudio: the largest and most diverse collection of dialogue datasets, unified under a consistent format while preserving their original information. Our collection encompasses data from open-domain dialogues, task-oriented dialogues, natural language understanding, conversational recommendation, dialogue summarization, and knowledge-grounded dialogues, making it an incredibly rich and diverse resource for dialogue research and model training. To further enhance the utility of DialogStudio, we identify the licenses for each dataset and design domain-aware prompts for selected dialogues to facilitate instruction-aware fine-tuning. Furthermore, we develop conversational AI models using the dataset collection, and our experiments in both zero-sh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#24635;&#32467;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#24212;&#29992;&#25104;&#21151;&#26696;&#20363;&#65292;&#24110;&#21161;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20154;&#21592;&#24555;&#36895;&#20102;&#35299;&#35813;&#39046;&#22495;&#30340;&#24403;&#21069;&#29366;&#24577;&#24182;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.10169</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25361;&#25112;&#19982;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Challenges and Applications of Large Language Models. (arXiv:2307.10169v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#24635;&#32467;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#24212;&#29992;&#25104;&#21151;&#26696;&#20363;&#65292;&#24110;&#21161;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20154;&#21592;&#24555;&#36895;&#20102;&#35299;&#35813;&#39046;&#22495;&#30340;&#24403;&#21069;&#29366;&#24577;&#24182;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#35752;&#35770;&#20013;&#20174;&#19981;&#23384;&#22312;&#21040;&#26080;&#22788;&#19981;&#22312;&#21482;&#29992;&#20102;&#20960;&#24180;&#30340;&#26102;&#38388;&#12290;&#30001;&#20110;&#39046;&#22495;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#24456;&#38590;&#30830;&#23450;&#21097;&#20313;&#30340;&#25361;&#25112;&#21644;&#24050;&#32463;&#21462;&#24471;&#30340;&#24212;&#29992;&#25104;&#21151;&#12290;&#26412;&#25991;&#26088;&#22312;&#24314;&#31435;&#19968;&#20010;&#31995;&#32479;&#30340;&#19968;&#32452;&#26410;&#35299;&#20915;&#38382;&#39064;&#21644;&#24212;&#29992;&#25104;&#21151;&#26696;&#20363;&#65292;&#20197;&#20415;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#26356;&#24555;&#22320;&#20102;&#35299;&#35813;&#39046;&#22495;&#30340;&#24403;&#21069;&#29366;&#24577;&#24182;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) went from non-existent to ubiquitous in the machine learning discourse within a few years. Due to the fast pace of the field, it is difficult to identify the remaining challenges and already fruitful application areas. In this paper, we aim to establish a systematic set of open problems and application successes so that ML researchers can comprehend the field's current state more quickly and become productive.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#25506;&#32034;&#20102;LLMs&#26159;&#21542;&#21487;&#20197;&#22797;&#21046;&#26356;&#22797;&#26434;&#30340;&#20247;&#21253;&#27969;&#27700;&#32447;&#65292;&#24182;&#21457;&#29616;&#29616;&#20195;LLMs&#22312;&#27169;&#25311;&#20154;&#31867;&#35745;&#31639;&#31639;&#27861;&#20013;&#30340;&#33021;&#21147;&#19978;&#26377;&#19968;&#23450;&#30340;&#25104;&#21151;&#65292;&#20294;&#21463;&#22810;&#31181;&#22240;&#32032;&#24433;&#21709;&#12290;&#25991;&#31456;&#24378;&#35843;&#20102;&#20026;LLMs&#25552;&#20379;&#20154;&#31867;&#38754;&#21521;&#30340;&#23433;&#20840;&#20445;&#38556;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#35757;&#32451;&#20154;&#31867;&#21644;LLMs&#20114;&#34917;&#25216;&#33021;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.10168</link><description>&lt;p&gt;
LLM&#20316;&#20026;&#20154;-&#35745;&#31639;&#31639;&#27861;&#20013;&#30340;&#24037;&#20316;&#32773;&#65311;&#29992;LLM&#22797;&#21046;&#20247;&#21253;&#27969;&#27700;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLMs as Workers in Human-Computational Algorithms? Replicating Crowdsourcing Pipelines with LLMs. (arXiv:2307.10168v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#25506;&#32034;&#20102;LLMs&#26159;&#21542;&#21487;&#20197;&#22797;&#21046;&#26356;&#22797;&#26434;&#30340;&#20247;&#21253;&#27969;&#27700;&#32447;&#65292;&#24182;&#21457;&#29616;&#29616;&#20195;LLMs&#22312;&#27169;&#25311;&#20154;&#31867;&#35745;&#31639;&#31639;&#27861;&#20013;&#30340;&#33021;&#21147;&#19978;&#26377;&#19968;&#23450;&#30340;&#25104;&#21151;&#65292;&#20294;&#21463;&#22810;&#31181;&#22240;&#32032;&#24433;&#21709;&#12290;&#25991;&#31456;&#24378;&#35843;&#20102;&#20026;LLMs&#25552;&#20379;&#20154;&#31867;&#38754;&#21521;&#30340;&#23433;&#20840;&#20445;&#38556;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#35757;&#32451;&#20154;&#31867;&#21644;LLMs&#20114;&#34917;&#25216;&#33021;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#20247;&#21253;&#20219;&#21153;&#20013;&#22797;&#21046;&#20154;&#31867;&#34892;&#20026;&#30340;&#28508;&#21147;&#65292;&#32780;&#36825;&#20123;&#20219;&#21153;&#20197;&#21069;&#34987;&#35748;&#20026;&#21482;&#26377;&#20154;&#31867;&#25165;&#33021;&#23436;&#25104;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#31616;&#21333;&#30340;&#21407;&#23376;&#20219;&#21153;&#19978;&#12290;&#25105;&#20204;&#25506;&#32034;LLM&#26159;&#21542;&#21487;&#20197;&#22797;&#21046;&#26356;&#22797;&#26434;&#30340;&#20247;&#21253;&#27969;&#27700;&#32447;&#12290;&#25105;&#20204;&#21457;&#29616;&#29616;&#20195;LLM&#21487;&#20197;&#27169;&#25311;&#26576;&#20123;&#20247;&#21253;&#24037;&#20316;&#32773;&#22312;&#36825;&#20123;&#8220;&#20154;&#31867;&#35745;&#31639;&#31639;&#27861;&#8221;&#20013;&#30340;&#33021;&#21147;&#65292;&#20294;&#25104;&#21151;&#30340;&#31243;&#24230;&#26159;&#21487;&#21464;&#30340;&#65292;&#24182;&#21463;&#21040;&#35831;&#27714;&#32773;&#23545;LLM&#33021;&#21147;&#30340;&#29702;&#35299;&#12289;&#23376;&#20219;&#21153;&#25152;&#38656;&#30340;&#29305;&#23450;&#25216;&#33021;&#20197;&#21450;&#25191;&#34892;&#36825;&#20123;&#23376;&#20219;&#21153;&#30340;&#26368;&#20339;&#20132;&#20114;&#26041;&#24335;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21453;&#24605;&#20102;&#20154;&#31867;&#21644;LLM&#23545;&#25351;&#31034;&#30340;&#19981;&#21516;&#25935;&#24863;&#24615;&#65292;&#24378;&#35843;&#20026;LLM&#25552;&#20379;&#38754;&#21521;&#20154;&#31867;&#30340;&#23433;&#20840;&#20445;&#38556;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#35757;&#32451;&#20855;&#26377;&#20114;&#34917;&#25216;&#33021;&#30340;&#20154;&#31867;&#21644;LLM&#30340;&#28508;&#21147;&#12290;&#20851;&#38190;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22797;&#21046;&#20247;&#21253;&#27969;&#27700;&#32447;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#20215;&#20540;&#30340;&#24179;&#21488;&#26469;&#30740;&#31350;LLM&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;&#30456;&#23545;&#20248;&#21183;&#65288;&#36890;&#36807;&#20132;&#21449;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
LLMs have shown promise in replicating human-like behavior in crowdsourcing tasks that were previously thought to be exclusive to human abilities. However, current efforts focus mainly on simple atomic tasks. We explore whether LLMs can replicate more complex crowdsourcing pipelines. We find that modern LLMs can simulate some of crowdworkers' abilities in these "human computation algorithms," but the level of success is variable and influenced by requesters' understanding of LLM capabilities, the specific skills required for sub-tasks, and the optimal interaction modality for performing these sub-tasks. We reflect on human and LLMs' different sensitivities to instructions, stress the importance of enabling human-facing safeguards for LLMs, and discuss the potential of training humans and LLMs with complementary skill sets. Crucially, we show that replicating crowdsourcing pipelines offers a valuable platform to investigate (1) the relative strengths of LLMs on different tasks (by cross
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25968;&#23398;&#21644;&#23454;&#35777;&#20998;&#26512;&#65292;&#21457;&#29616;&#21482;&#35201;RPE&#30340;&#25351;&#25968;&#24207;&#21015;&#25910;&#25947;&#65292;Transformer&#23601;&#20855;&#26377;&#38271;&#24230;&#22806;&#25512;&#30340;&#33021;&#21147;&#12290;&#20174;&#20013;&#23548;&#20986;&#20102;&#20004;&#31181;&#23454;&#36341;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#24863;&#21463;&#37326;(TRF)&#26469;&#27979;&#37327;RPE&#30340;&#24863;&#21463;&#37326;&#12290;</title><link>http://arxiv.org/abs/2307.10156</link><description>&lt;p&gt;
&#25506;&#32034;Transformer&#22806;&#25512;
&lt;/p&gt;
&lt;p&gt;
Exploring Transformer Extrapolation. (arXiv:2307.10156v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10156
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25968;&#23398;&#21644;&#23454;&#35777;&#20998;&#26512;&#65292;&#21457;&#29616;&#21482;&#35201;RPE&#30340;&#25351;&#25968;&#24207;&#21015;&#25910;&#25947;&#65292;Transformer&#23601;&#20855;&#26377;&#38271;&#24230;&#22806;&#25512;&#30340;&#33021;&#21147;&#12290;&#20174;&#20013;&#23548;&#20986;&#20102;&#20004;&#31181;&#23454;&#36341;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#24863;&#21463;&#37326;(TRF)&#26469;&#27979;&#37327;RPE&#30340;&#24863;&#21463;&#37326;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#24230;&#22806;&#25512;&#36817;&#26399;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20801;&#35768;transformers&#22312;&#35757;&#32451;&#20013;&#20351;&#29992;&#30340;&#24207;&#21015;&#38271;&#24230;&#20043;&#22806;&#36827;&#34892;&#27979;&#35797;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;&#31934;&#24515;&#35774;&#35745;&#30340;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;(RPEs)&#21487;&#20197;&#23454;&#29616;&#36825;&#19968;&#23646;&#24615;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#22312;&#21508;&#31181;&#25991;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23545;&#20110;&#38271;&#24230;&#22806;&#25512;&#30340;&#26465;&#20214;&#23578;&#26410;&#24471;&#21040;&#30740;&#31350;&#12290;&#26412;&#25991;&#35797;&#22270;&#36890;&#36807;&#24443;&#24213;&#30340;&#25968;&#23398;&#21644;&#23454;&#35777;&#20998;&#26512;&#30830;&#23450;&#21738;&#31181;&#31867;&#22411;&#30340;RPEs&#21487;&#20197;&#23454;&#29616;&#38271;&#24230;&#22806;&#25512;&#12290;&#25105;&#20204;&#21457;&#29616;&#21482;&#35201;&#23545;&#24212;&#20110;RPE&#30340;&#25351;&#25968;&#25910;&#25947;&#30340;&#24207;&#21015;&#65292;transformer&#19968;&#23450;&#20855;&#26377;&#36825;&#20010;&#23646;&#24615;&#12290;&#20174;&#36825;&#20123;&#26465;&#20214;&#20013;&#23548;&#20986;&#20102;&#20004;&#31181;&#23454;&#36341;&#26041;&#27861;&#65292;&#24182;&#22312;&#21508;&#31181;&#25991;&#38598;&#19978;&#36827;&#34892;&#20102;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#30340;&#30740;&#31350;&#12290;&#20316;&#20026;&#26465;&#20214;&#34893;&#29983;&#30340;&#39069;&#22806;&#22909;&#22788;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#24863;&#21463;&#37326;(TRF)&#65292;&#21487;&#20197;&#22312;&#19981;&#36827;&#34892;&#20219;&#20309;&#35757;&#32451;&#27493;&#39588;&#30340;&#24773;&#20917;&#19979;&#27979;&#37327;RPE&#30340;&#24863;&#21463;&#37326;&#12290;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Length extrapolation has attracted considerable attention recently since it allows transformers to be tested on longer sequences than those used in training. Previous research has shown that this property can be attained by using carefully designed Relative Positional Encodings (RPEs). While these methods perform well on a variety of corpora, the conditions for length extrapolation have yet to be investigated. This paper attempts to determine what types of RPEs allow for length extrapolation through a thorough mathematical and empirical analysis. We discover that a transformer is certain to possess this property as long as the series that corresponds to the RPE's exponential converges. Two practices are derived from the conditions and examined in language modeling tasks on a variety of corpora. As a bonus from the conditions, we derive a new Theoretical Receptive Field (TRF) to measure the receptive field of RPEs without taking any training steps. Extensive experiments are conducted on
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28176;&#36827;&#31232;&#30095;&#21270;&#26041;&#27861;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#20197;&#25913;&#21892;&#24494;&#35843;&#24615;&#33021;&#12290;GradDrop&#21450;&#20854;&#21464;&#20307;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#38543;&#26426;&#23631;&#34109;&#26799;&#24230;&#65292;&#26377;&#25928;&#22320;&#36827;&#34892;&#26799;&#24230;&#31232;&#30095;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.10098</link><description>&lt;p&gt;
&#28176;&#36827;&#31232;&#30095;&#21270;&#29992;&#20110;Transformer&#27169;&#22411;&#30340;&#36974;&#32617;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Gradient Sparsification For Masked Fine-Tuning of Transformers. (arXiv:2307.10098v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28176;&#36827;&#31232;&#30095;&#21270;&#26041;&#27861;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#20197;&#25913;&#21892;&#24494;&#35843;&#24615;&#33021;&#12290;GradDrop&#21450;&#20854;&#21464;&#20307;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#38543;&#26426;&#23631;&#34109;&#26799;&#24230;&#65292;&#26377;&#25928;&#22320;&#36827;&#34892;&#26799;&#24230;&#31232;&#30095;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21521;&#19979;&#28216;&#20219;&#21153;&#30340;&#36801;&#31227;&#23398;&#20064;&#12290;&#24494;&#35843;&#21487;&#36890;&#36807;&#20923;&#32467;&#39044;&#35757;&#32451;&#32593;&#32476;&#30340;&#26799;&#24230;&#24182;&#21482;&#26356;&#26032;&#26032;&#28155;&#21152;&#30340;&#20998;&#31867;&#23618;&#30340;&#26799;&#24230;&#65292;&#25110;&#36890;&#36807;&#23545;&#25152;&#26377;&#21442;&#25968;&#36827;&#34892;&#26799;&#24230;&#26356;&#26032;&#26469;&#23454;&#29616;&#12290;&#28176;&#36827;&#35299;&#20923;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36880;&#28176;&#35299;&#20923;&#25972;&#20010;&#23618;&#30340;&#26799;&#24230;&#65292;&#20197;&#22312;&#23384;&#20648;&#21644;&#35757;&#32451;&#36895;&#24230;&#19982;&#27867;&#21270;&#24615;&#33021;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#65292;&#36825;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#19981;&#28165;&#26970;&#28176;&#36827;&#35299;&#20923;&#25972;&#20010;&#35757;&#32451;&#26159;&#21542;&#26159;&#26368;&#20248;&#36873;&#25321;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;&#31232;&#30095;&#21464;&#20307;&#30340;&#28176;&#36827;&#35299;&#20923;&#21487;&#33021;&#21487;&#20197;&#25552;&#39640;&#24494;&#35843;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38543;&#26426;&#23631;&#34109;&#26799;&#24230;&#26469;&#27491;&#21017;&#21270;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#25913;&#21892;&#25972;&#20307;&#24494;&#35843;&#24615;&#33021;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;GradDrop&#21450;&#20854;&#21464;&#20307;&#65292;&#19968;&#31867;&#26799;&#24230;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#26799;&#24230;&#36827;&#34892;&#23631;&#34109;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning pretrained self-supervised language models is widely adopted for transfer learning to downstream tasks. Fine-tuning can be achieved by freezing gradients of the pretrained network and only updating gradients of a newly added classification layer, or by performing gradient updates on all parameters. Gradual unfreezing makes a trade-off between the two by gradually unfreezing gradients of whole layers during training. This has been an effective strategy to trade-off between storage and training speed with generalization performance. However, it is not clear whether gradually unfreezing layers throughout training is optimal, compared to sparse variants of gradual unfreezing which may improve fine-tuning performance. In this paper, we propose to stochastically mask gradients to regularize pretrained language models for improving overall fine-tuned performance. We introduce GradDrop and variants thereof, a class of gradient sparsification methods that mask gradients during the b
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Android in the Wild (AITW)&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#35774;&#22791;&#25511;&#21046;&#31995;&#32479;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#20154;&#31867;&#31034;&#33539;&#30340;&#35774;&#22791;&#20132;&#20114;&#12289;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#21644;&#22810;&#31181;Android&#29256;&#26412;&#21644;&#35774;&#22791;&#31867;&#22411;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#20174;&#35270;&#35273;&#22806;&#35266;&#20013;&#25512;&#26029;&#29992;&#25143;&#30028;&#38754;&#20013;&#21487;&#29992;&#30340;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2307.10088</link><description>&lt;p&gt;
&#22312;&#37326;&#22806;&#30340;Android&#65306;&#29992;&#20110;Android&#35774;&#22791;&#25511;&#21046;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Android in the Wild: A Large-Scale Dataset for Android Device Control. (arXiv:2307.10088v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10088
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Android in the Wild (AITW)&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#35774;&#22791;&#25511;&#21046;&#31995;&#32479;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#20154;&#31867;&#31034;&#33539;&#30340;&#35774;&#22791;&#20132;&#20114;&#12289;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#21644;&#22810;&#31181;Android&#29256;&#26412;&#21644;&#35774;&#22791;&#31867;&#22411;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#20174;&#35270;&#35273;&#22806;&#35266;&#20013;&#25512;&#26029;&#29992;&#25143;&#30028;&#38754;&#20013;&#21487;&#29992;&#30340;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#33021;&#22815;&#35299;&#37322;&#20154;&#31867;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#24182;&#30452;&#25509;&#25511;&#21046;&#25968;&#23383;&#35774;&#22791;&#29992;&#25143;&#30028;&#38754;&#25191;&#34892;&#30340;&#35774;&#22791;&#25511;&#21046;&#31995;&#32479;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35774;&#22791;&#25511;&#21046;&#30740;&#31350;&#30340;&#25968;&#25454;&#38598;&#65292;Android in the Wild (AITW)&#65292;&#35813;&#25968;&#25454;&#38598;&#27604;&#24403;&#21069;&#25968;&#25454;&#38598;&#22823;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#35774;&#22791;&#20132;&#20114;&#30340;&#20154;&#31867;&#31034;&#33539;&#65292;&#21253;&#25324;&#23631;&#24149;&#21644;&#25805;&#20316;&#65292;&#20197;&#21450;&#30456;&#24212;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#12290;&#23427;&#21253;&#25324;715k&#20010;&#21095;&#38598;&#65292;&#28085;&#30422;30k&#20010;&#19981;&#21516;&#30340;&#25351;&#20196;&#65292;&#22235;&#20010;Android&#29256;&#26412;&#65288;v10-13&#65289;&#65292;&#20197;&#21450;&#20843;&#31181;&#19981;&#21516;&#30340;&#35774;&#22791;&#31867;&#22411;&#65288;&#20174;Pixel 2 XL&#21040;Pixel 6&#65289;&#21644;&#19981;&#21516;&#30340;&#23631;&#24149;&#20998;&#36776;&#29575;&#12290;&#23427;&#21253;&#21547;&#38656;&#35201;&#35821;&#35328;&#21644;&#35270;&#35273;&#19978;&#19979;&#25991;&#30340;&#35821;&#20041;&#29702;&#35299;&#30340;&#22810;&#27493;&#39588;&#20219;&#21153;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#65306;&#24517;&#39035;&#20174;&#23427;&#20204;&#30340;&#35270;&#35273;&#22806;&#35266;&#20013;&#25512;&#26029;&#20986;&#29992;&#25143;&#30028;&#38754;&#20013;&#21487;&#29992;&#30340;&#25805;&#20316;&#12290;&#32780;&#19988;&#65292;&#34892;&#21160;&#31354;&#38388;&#19981;&#20877;&#26159;&#31616;&#21333;&#30340;&#22522;&#20110;&#29992;&#25143;&#30028;&#38754;&#20803;&#32032;&#30340;&#34892;&#21160;&#65292;&#32780;&#26159;&#21253;&#21547;&#31934;&#30830;&#30340;&#25163;&#21183;&#65288;&#20363;&#22914;&#65292;&#27700;&#24179;&#28378;&#21160;&#65289;
&lt;/p&gt;
&lt;p&gt;
There is a growing interest in device-control systems that can interpret human natural language instructions and execute them on a digital device by directly controlling its user interface. We present a dataset for device-control research, Android in the Wild (AITW), which is orders of magnitude larger than current datasets. The dataset contains human demonstrations of device interactions, including the screens and actions, and corresponding natural language instructions. It consists of 715k episodes spanning 30k unique instructions, four versions of Android (v10-13),and eight device types (Pixel 2 XL to Pixel 6) with varying screen resolutions. It contains multi-step tasks that require semantic understanding of language and visual context. This dataset poses a new challenge: actions available through the user interface must be inferred from their visual appearance. And, instead of simple UI element-based actions, the action space consists of precise gestures (e.g., horizontal scrolls 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#37319;&#29992;&#22810;&#31890;&#24230;&#20027;&#39064;&#20998;&#26512;&#26041;&#27861;&#65292;&#23545;&#24494;&#21338;&#35780;&#35770;&#36827;&#34892;&#35821;&#20041;&#20998;&#26512;&#65292;&#21457;&#29616;&#20851;&#20110;&#21462;&#28040;&#23130;&#23035;&#30331;&#35760;&#30340;&#29983;&#32946;&#38480;&#21046;&#30340;&#25552;&#26696;&#28041;&#21450;&#20010;&#20154;&#12289;&#31038;&#20250;&#21644;&#22269;&#23478;&#19977;&#20010;&#32500;&#24230;&#65292;&#35814;&#32454;&#35752;&#35770;&#20102;&#20010;&#20154;&#34892;&#20026;&#12289;&#31038;&#20250;&#20262;&#29702;&#21644;&#27861;&#24459;&#20197;&#21450;&#22269;&#23478;&#25919;&#31574;&#31561;&#31038;&#20250;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.10025</link><description>&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#22810;&#31890;&#24230;&#20027;&#39064;&#20998;&#26512;&#26041;&#27861;&#30340;&#23454;&#35777;&#30740;&#31350;&#65306;&#29983;&#32946;&#25919;&#31574;&#25552;&#26696;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study on Fertility Proposals Using Multi-Grined Topic Analysis Methods. (arXiv:2307.10025v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10025
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#37319;&#29992;&#22810;&#31890;&#24230;&#20027;&#39064;&#20998;&#26512;&#26041;&#27861;&#65292;&#23545;&#24494;&#21338;&#35780;&#35770;&#36827;&#34892;&#35821;&#20041;&#20998;&#26512;&#65292;&#21457;&#29616;&#20851;&#20110;&#21462;&#28040;&#23130;&#23035;&#30331;&#35760;&#30340;&#29983;&#32946;&#38480;&#21046;&#30340;&#25552;&#26696;&#28041;&#21450;&#20010;&#20154;&#12289;&#31038;&#20250;&#21644;&#22269;&#23478;&#19977;&#20010;&#32500;&#24230;&#65292;&#35814;&#32454;&#35752;&#35770;&#20102;&#20010;&#20154;&#34892;&#20026;&#12289;&#31038;&#20250;&#20262;&#29702;&#21644;&#27861;&#24459;&#20197;&#21450;&#22269;&#23478;&#25919;&#31574;&#31561;&#31038;&#20250;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#32946;&#38382;&#39064;&#19982;&#20154;&#21475;&#23433;&#20840;&#23494;&#20999;&#30456;&#20851;&#65292;&#20013;&#22269;60&#24180;&#26469;&#39318;&#27425;&#20986;&#29616;&#20154;&#21475;&#36127;&#22686;&#38271;&#36235;&#21183;&#65292;&#29983;&#32946;&#25919;&#31574;&#30340;&#21464;&#21270;&#24341;&#36215;&#20102;&#31038;&#20250;&#30340;&#26497;&#22823;&#20851;&#27880;&#12290;&#26412;&#25991;&#37319;&#29992;&#20849;&#29616;&#35821;&#20041;&#20998;&#26512;&#12289;&#20027;&#39064;&#20998;&#26512;&#21644;&#24773;&#24863;&#20998;&#26512;&#31561;&#26041;&#27861;&#65292;&#23545;&#24494;&#21338;&#35780;&#35770;&#36827;&#34892;&#22810;&#31890;&#24230;&#30340;&#35821;&#20041;&#20998;&#26512;&#12290;&#21457;&#29616;&#20851;&#20110;&#8220;&#21462;&#28040;&#23130;&#23035;&#30331;&#35760;&#30340;&#29983;&#32946;&#38480;&#21046;&#8221;&#30340;&#25552;&#26696;&#35752;&#35770;&#28041;&#21450;&#20010;&#20154;&#12289;&#31038;&#20250;&#21644;&#22269;&#23478;&#19977;&#20010;&#32500;&#24230;&#65292;&#24182;&#35814;&#32454;&#25506;&#35752;&#20102;&#20010;&#20154;&#34892;&#20026;&#12289;&#31038;&#20250;&#20262;&#29702;&#21644;&#27861;&#24459;&#20197;&#21450;&#22269;&#23478;&#25919;&#31574;&#31561;&#31038;&#20250;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fertility issues are closely related to population security, in 60 years China's population for the first time in a negative growth trend, the change of fertility policy is of great concern to the community. 2023 ``two sessions" proposal ``suggests that the country in the form of legislation, the birth of the registration of the cancellation of the marriage restriction" This topic was once a hot topic on the Internet, and ``unbundling" the relationship between birth registration and marriage has become the focus of social debate. In this paper, we adopt co-occurrence semantic analysis, topic analysis and sentiment analysis to conduct multi-granularity semantic analysis of microblog comments. It is found that the discussion on the proposal of ``removing marriage restrictions from birth registration" involves the individual, society and the state at three dimensions, and is detailed into social issues such as personal behaviour, social ethics and law, and national policy, with people's s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25968;&#23398;&#23548;&#20986;&#65292;&#20998;&#26512;&#20102;&#24494;&#35843;&#27169;&#22411;&#23545;&#26410;&#35265;&#31526;&#21495;&#21644;&#26041;&#31243;&#32467;&#26500;&#26356;&#25913;&#30340;&#25935;&#24863;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;&#24494;&#35843;&#30340;FLAN-T5-large&#65288;MathT5&#65289;&#22312;&#21508;&#20010;&#27979;&#35797;&#38598;&#19978;&#30340;&#32477;&#23545;&#24615;&#33021;&#20248;&#20110;GPT&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.09998</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25968;&#23398;&#23548;&#20986;
&lt;/p&gt;
&lt;p&gt;
Generating Mathematical Derivations with Large Language Models. (arXiv:2307.09998v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09998
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25968;&#23398;&#23548;&#20986;&#65292;&#20998;&#26512;&#20102;&#24494;&#35843;&#27169;&#22411;&#23545;&#26410;&#35265;&#31526;&#21495;&#21644;&#26041;&#31243;&#32467;&#26500;&#26356;&#25913;&#30340;&#25935;&#24863;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;&#24494;&#35843;&#30340;FLAN-T5-large&#65288;MathT5&#65289;&#22312;&#21508;&#20010;&#27979;&#35797;&#38598;&#19978;&#30340;&#32477;&#23545;&#24615;&#33021;&#20248;&#20110;GPT&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#19987;&#19994;&#39046;&#22495;&#20013;&#29983;&#25104;&#25968;&#23398;&#32467;&#26524;&#30340;&#23548;&#20986;&#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#21487;&#20197;&#24110;&#21161;&#35782;&#21035;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#26377;&#21487;&#33021;&#25903;&#25345;&#25968;&#23398;&#21457;&#29616;&#12290;&#26412;&#25991;&#21033;&#29992;&#31526;&#21495;&#24341;&#25806;&#22312;&#22823;&#35268;&#27169;&#19978;&#29983;&#25104;&#26041;&#31243;&#30340;&#23548;&#20986;&#65292;&#24182;&#30740;&#31350;&#20102;LLM&#22312;&#20174;&#21069;&#25552;&#20013;&#23548;&#20986;&#30446;&#26631;&#26041;&#31243;&#26102;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#23545;GPT&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#23545;&#19968;&#31995;&#21015;T5&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#20197;&#27604;&#36739;&#39044;&#35757;&#32451;&#31574;&#30053;&#23545;&#19987;&#38376;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;FLAN-T5-large&#65288;MathT5&#65289;&#22312;&#25152;&#26377;&#38745;&#24577;&#21644;&#36229;&#20986;&#20998;&#24067;&#30340;&#27979;&#35797;&#38598;&#19978;&#30340;&#32477;&#23545;&#24615;&#33021;&#20248;&#20110;GPT&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#28145;&#20837;&#20998;&#26512;&#34920;&#26126;&#65292;&#24494;&#35843;&#27169;&#22411;&#23545;&#28041;&#21450;&#26410;&#35265;&#31526;&#21495;&#30340;&#25200;&#21160;&#65288;&#20197;&#21450;&#22312;&#36739;&#23567;&#31243;&#24230;&#19978;&#30340;&#26041;&#31243;&#32467;&#26500;&#26356;&#25913;&#65289;&#26356;&#20026;&#25935;&#24863;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;1.7K&#20010;&#26041;&#31243;&#21644;200&#22810;&#20010;&#23548;&#20986;&#20197;&#20984;&#26174;&#20986;LLM&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The derivation of mathematical results in specialised fields using Large Language Models (LLMs) is an emerging research direction that can help identify models' limitations, and potentially support mathematical discovery. In this paper, we leverage a symbolic engine to generate derivations of equations at scale, and investigate the capabilities of LLMs when deriving goal equations from premises. Specifically, we employ in-context learning for GPT and fine-tune a range of T5 models to compare the robustness and generalisation of pre-training strategies to specialised models. Empirical results show that fine-tuned FLAN-T5-large (MathT5) outperforms GPT models on all static and out-of-distribution test sets in terms of absolute performance. However, an in-depth analysis reveals that the fine-tuned models are more sensitive to perturbations involving unseen symbols and (to a lesser extent) changes to equation structure. In addition, we analyse 1.7K equations and over 200 derivations to hig
&lt;/p&gt;</description></item><item><title>GUIDO&#26159;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;BERT-based&#21477;&#23376;&#20998;&#31867;&#22120;&#36827;&#34892;&#21477;&#23376;&#30456;&#20851;&#24615;&#20998;&#31867;&#65292;&#24182;&#20351;&#29992;&#20381;&#36182;&#35299;&#26512;&#20174;&#30456;&#20851;&#21477;&#23376;&#20013;&#25552;&#21462;&#27969;&#31243;&#27169;&#22411;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#27969;&#31243;&#27169;&#22411;&#30340;&#25552;&#21462;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.09959</link><description>&lt;p&gt;
GUIDO&#65306;&#19968;&#31181;&#20174;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#20013;&#21457;&#29616;&#21644;&#25490;&#24207;&#25351;&#21335;&#30340;&#28151;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
GUIDO: A Hybrid Approach to Guideline Discovery &amp; Ordering from Natural Language Texts. (arXiv:2307.09959v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09959
&lt;/p&gt;
&lt;p&gt;
GUIDO&#26159;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;BERT-based&#21477;&#23376;&#20998;&#31867;&#22120;&#36827;&#34892;&#21477;&#23376;&#30456;&#20851;&#24615;&#20998;&#31867;&#65292;&#24182;&#20351;&#29992;&#20381;&#36182;&#35299;&#26512;&#20174;&#30456;&#20851;&#21477;&#23376;&#20013;&#25552;&#21462;&#27969;&#31243;&#27169;&#22411;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#27969;&#31243;&#27169;&#22411;&#30340;&#25552;&#21462;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#25552;&#21462;&#24037;&#20316;&#27969;&#32593;&#32476;&#21487;&#29992;&#20110;&#31616;&#21270;&#25351;&#21335;&#25110;&#24418;&#24335;&#21270;&#30340;&#19994;&#21153;&#27969;&#31243;&#21644;&#31639;&#27861;&#31561;&#27491;&#24335;&#27969;&#31243;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#25552;&#21462;&#27969;&#31243;&#30340;&#20219;&#21153;&#38656;&#35201;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#21644;&#24037;&#20316;&#37327;&#12290;&#34429;&#28982;&#33258;&#21160;&#27969;&#31243;&#27169;&#22411;&#25552;&#21462;&#26159;&#21487;&#21462;&#30340;&#65292;&#20294;&#29992;&#24418;&#24335;&#21270;&#30340;&#27969;&#31243;&#27169;&#22411;&#27880;&#37322;&#25991;&#26412;&#26159;&#26114;&#36149;&#30340;&#12290;&#22240;&#27492;&#65292;&#30446;&#21069;&#21482;&#26377;&#23569;&#25968;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#25552;&#21462;&#26041;&#27861;&#12290;&#32780;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#24448;&#24448;&#38656;&#35201;&#39046;&#22495;&#29305;&#23450;&#24615;&#25165;&#33021;&#33391;&#22909;&#36816;&#20316;&#65292;&#24182;&#19988;&#24456;&#23569;&#33021;&#22815;&#21306;&#20998;&#25991;&#26412;&#25551;&#36848;&#20013;&#30340;&#30456;&#20851;&#21644;&#19981;&#30456;&#20851;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;GUIDO&#65292;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#27969;&#31243;&#27169;&#22411;&#25552;&#21462;&#20219;&#21153;&#12290;&#39318;&#20808;&#65292;&#20351;&#29992;&#22522;&#20110;BERT&#30340;&#21477;&#23376;&#20998;&#31867;&#22120;&#23545;&#21477;&#23376;&#36827;&#34892;&#30456;&#20851;&#24615;&#20998;&#31867;&#65292;&#28982;&#21518;&#20351;&#29992;&#20381;&#36182;&#35299;&#26512;&#20174;&#34987;&#20998;&#31867;&#20026;&#30456;&#20851;&#30340;&#21477;&#23376;&#20013;&#25552;&#21462;&#27969;&#31243;&#27169;&#22411;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extracting workflow nets from textual descriptions can be used to simplify guidelines or formalize textual descriptions of formal processes like business processes and algorithms. The task of manually extracting processes, however, requires domain expertise and effort. While automatic process model extraction is desirable, annotating texts with formalized process models is expensive. Therefore, there are only a few machine-learning-based extraction approaches. Rule-based approaches, in turn, require domain specificity to work well and can rarely distinguish relevant and irrelevant information in textual descriptions. In this paper, we present GUIDO, a hybrid approach to the process model extraction task that first, classifies sentences regarding their relevance to the process model, using a BERT-based sentence classifier, and second, extracts a process model from the sentences classified as relevant, using dependency parsing. The presented approach achieves significantly better results
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23436;&#25104;&#19994;&#21153;&#27969;&#31243;&#31649;&#29702;&#20219;&#21153;&#65292;&#36890;&#36807;&#36816;&#29992;&#29305;&#23450;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20998;&#26512;&#25991;&#26412;&#65292;&#21487;&#20197;&#25552;&#21462;&#21629;&#20196;&#24335;&#21644;&#22768;&#26126;&#24335;&#27969;&#31243;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.09923</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23436;&#25104;&#19994;&#21153;&#27969;&#31243;&#31649;&#29702;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Large Language Models can accomplish Business Process Management Tasks. (arXiv:2307.09923v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09923
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23436;&#25104;&#19994;&#21153;&#27969;&#31243;&#31649;&#29702;&#20219;&#21153;&#65292;&#36890;&#36807;&#36816;&#29992;&#29305;&#23450;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20998;&#26512;&#25991;&#26412;&#65292;&#21487;&#20197;&#25552;&#21462;&#21629;&#20196;&#24335;&#21644;&#22768;&#26126;&#24335;&#27969;&#31243;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19994;&#21153;&#27969;&#31243;&#31649;&#29702;&#65288;BPM&#65289;&#26088;&#22312;&#36890;&#36807;&#31649;&#29702;&#24213;&#23618;&#27969;&#31243;&#26469;&#25913;&#21892;&#32452;&#32455;&#27963;&#21160;&#21450;&#20854;&#32467;&#26524;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#36890;&#24120;&#38656;&#35201;&#32771;&#34385;&#26469;&#33258;&#21508;&#31181;&#20449;&#24687;&#28304;&#30340;&#20449;&#24687;&#65292;&#21253;&#25324;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#25991;&#26723;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#24320;&#21457;&#20102;&#20960;&#31181;&#29305;&#23450;&#20110;BPM&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#20174;&#25991;&#26412;&#25991;&#26723;&#20013;&#25552;&#21462;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#26159;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#32780;&#35774;&#35745;&#30340;&#65292;&#24182;&#19981;&#33021;&#20316;&#20026;&#36890;&#29992;&#24037;&#20855;&#26469;&#35299;&#20915;&#22810;&#20010;&#19982;&#27969;&#31243;&#30456;&#20851;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#37492;&#20110;&#26368;&#36817;&#20986;&#29616;&#30340;&#20855;&#26377;&#20986;&#33394;&#25512;&#29702;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#23454;&#29616;&#36825;&#26679;&#19968;&#20010;&#20855;&#26377;&#22810;&#20010;&#24212;&#29992;&#30340;&#36890;&#29992;&#24037;&#20855;&#29616;&#22312;&#20284;&#20046;&#26159;&#21487;&#34892;&#30340;&#12290;&#26412;&#25991;&#23558;&#23637;&#31034;LLM&#22914;&#20309;&#36890;&#36807;&#23558;&#29305;&#23450;&#30340;LLM&#24212;&#29992;&#20110;&#19977;&#20010;&#31034;&#20363;&#20219;&#21153;&#26469;&#23436;&#25104;&#19982;&#25991;&#26412;&#30456;&#20851;&#30340;BPM&#20219;&#21153;&#65306;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#25366;&#25496;&#21629;&#20196;&#24335;&#27969;&#31243;&#27169;&#22411;&#65292;&#20174;&#25991;&#26412;&#20013;&#25366;&#25496;&#22768;&#26126;&#24335;&#27969;&#31243;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Business Process Management (BPM) aims to improve organizational activities and their outcomes by managing the underlying processes. To achieve this, it is often necessary to consider information from various sources, including unstructured textual documents. Therefore, researchers have developed several BPM-specific solutions that extract information from textual documents using Natural Language Processing techniques. These solutions are specific to their respective tasks and cannot accomplish multiple process-related problems as a general-purpose instrument. However, in light of the recent emergence of Large Language Models (LLMs) with remarkable reasoning capabilities, such a general-purpose instrument with multiple applications now appears attainable. In this paper, we illustrate how LLMs can accomplish text-related BPM tasks by applying a specific LLM to three exemplary tasks: mining imperative process models from textual descriptions, mining declarative process models from textua
&lt;/p&gt;</description></item><item><title>&#28145;&#20837;&#20102;&#35299;&#22312;&#35821;&#35328;&#27979;&#35797;&#20013;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#24433;&#21709;&#21644;&#25285;&#24551;&#65292;&#26377;&#21161;&#20110;&#21033;&#30410;&#30456;&#20851;&#32773;&#20570;&#20986;&#26126;&#26234;&#20915;&#31574;&#65292;&#30830;&#20445;&#31038;&#21306;&#31119;&#31049;&#21644;&#27979;&#35797;&#23436;&#25972;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09885</link><description>&lt;p&gt;
&#32771;&#35797;&#32773;&#26377;&#35805;&#35828;&#65306;&#29702;&#35299;&#22312;&#35821;&#35328;&#27979;&#35797;&#20013;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Test-takers have a say: understanding the implications of the use of AI in language tests. (arXiv:2307.09885v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09885
&lt;/p&gt;
&lt;p&gt;
&#28145;&#20837;&#20102;&#35299;&#22312;&#35821;&#35328;&#27979;&#35797;&#20013;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#24433;&#21709;&#21644;&#25285;&#24551;&#65292;&#26377;&#21161;&#20110;&#21033;&#30410;&#30456;&#20851;&#32773;&#20570;&#20986;&#26126;&#26234;&#20915;&#31574;&#65292;&#30830;&#20445;&#31038;&#21306;&#31119;&#31049;&#21644;&#27979;&#35797;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27979;&#35797;&#34913;&#37327;&#19968;&#20010;&#20154;&#22312;&#21548;&#12289;&#35828;&#12289;&#35835;&#12289;&#20889;&#26041;&#38754;&#20351;&#29992;&#26576;&#31181;&#35821;&#35328;&#30340;&#33021;&#21147;&#12290;&#36825;&#31867;&#27979;&#35797;&#22312;&#23398;&#26415;&#12289;&#32844;&#19994;&#21644;&#31227;&#27665;&#39046;&#22495;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#25945;&#32946;&#26426;&#26500;&#12289;&#19987;&#19994;&#35748;&#35777;&#26426;&#26500;&#21644;&#25919;&#24220;&#31561;&#23454;&#20307;&#26426;&#26500;&#20351;&#29992;&#23427;&#20204;&#26469;&#35780;&#20272;&#20505;&#36873;&#20154;&#30340;&#35821;&#35328;&#29087;&#32451;&#31243;&#24230;&#12290;&#20154;&#24037;&#26234;&#33021;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#23398;&#31185;&#30340;&#26368;&#26032;&#36827;&#23637;&#20419;&#20351;&#35821;&#35328;&#27979;&#35797;&#25552;&#20379;&#32773;&#25506;&#32034;&#23558;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20110;&#35821;&#35328;&#27979;&#35797;&#30340;&#28508;&#22312;&#21487;&#33021;&#24615;&#65292;&#20174;&#32780;&#24341;&#21457;&#20102;&#22260;&#32469;&#35821;&#35328;&#25945;&#23398;&#21644;&#23398;&#20064;&#30340;&#21464;&#38761;&#24615;&#27963;&#21160;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#22312;&#23545;&#20154;&#24037;&#26234;&#33021;&#20449;&#20219;&#24615;&#23384;&#22312;&#25285;&#24551;&#30340;&#24773;&#20917;&#19979;&#65292;&#24517;&#39035;&#20102;&#35299;&#23558;&#20154;&#24037;&#26234;&#33021;&#25972;&#21512;&#21040;&#35821;&#35328;&#27979;&#35797;&#20013;&#30340;&#24433;&#21709;&#12290;&#36825;&#26679;&#30340;&#30693;&#35782;&#23558;&#20351;&#21033;&#30410;&#30456;&#20851;&#32773;&#33021;&#22815;&#20570;&#20986;&#26126;&#26234;&#30340;&#20915;&#31574;&#65292;&#20174;&#32780;&#20445;&#25252;&#31038;&#21306;&#30340;&#31119;&#31049;&#21644;&#27979;&#35797;&#30340;&#23436;&#25972;&#24615;&#12290;&#20026;&#20102;&#20102;&#35299;&#22312;&#35821;&#35328;&#27979;&#35797;&#20013;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#20851;&#20999;&#21644;&#24433;&#21709;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Language tests measure a person's ability to use a language in terms of listening, speaking, reading, or writing. Such tests play an integral role in academic, professional, and immigration domains, with entities such as educational institutions, professional accreditation bodies, and governments using them to assess candidate language proficiency. Recent advances in Artificial Intelligence (AI) and the discipline of Natural Language Processing have prompted language test providers to explore AI's potential applicability within language testing, leading to transformative activity patterns surrounding language instruction and learning. However, with concerns over AI's trustworthiness, it is imperative to understand the implications of integrating AI into language testing. This knowledge will enable stakeholders to make well-informed decisions, thus safeguarding community well-being and testing integrity. To understand the concerns and effects of AI usage in language tests, we conducted 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DAPrompt&#30340;&#30830;&#23450;&#24615;&#20551;&#35774;&#25552;&#31034;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#20107;&#20214;&#22240;&#26524;&#20851;&#31995;&#35782;&#21035;&#20219;&#21153;&#12290;&#19982;&#20256;&#32479;&#25552;&#31034;&#23398;&#20064;&#19981;&#21516;&#65292;&#35813;&#27169;&#22411;&#39318;&#20808;&#22522;&#20110;&#30830;&#23450;&#24615;&#20551;&#35774;&#23545;&#20107;&#20214;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#36827;&#34892;&#35780;&#20272;&#65292;&#32780;&#19981;&#26159;&#39044;&#27979;&#31572;&#26696;&#35789;&#12290;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31867;&#30334;&#31185;&#30693;&#35782;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#26356;&#22909;&#22320;&#35299;&#20915;ECI&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.09813</link><description>&lt;p&gt;
DAPrompt: &#20107;&#20214;&#22240;&#26524;&#20851;&#31995;&#35782;&#21035;&#30340;&#30830;&#23450;&#24615;&#20551;&#35774;&#25552;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DAPrompt: Deterministic Assumption Prompt Learning for Event Causality Identification. (arXiv:2307.09813v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09813
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DAPrompt&#30340;&#30830;&#23450;&#24615;&#20551;&#35774;&#25552;&#31034;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#20107;&#20214;&#22240;&#26524;&#20851;&#31995;&#35782;&#21035;&#20219;&#21153;&#12290;&#19982;&#20256;&#32479;&#25552;&#31034;&#23398;&#20064;&#19981;&#21516;&#65292;&#35813;&#27169;&#22411;&#39318;&#20808;&#22522;&#20110;&#30830;&#23450;&#24615;&#20551;&#35774;&#23545;&#20107;&#20214;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#36827;&#34892;&#35780;&#20272;&#65292;&#32780;&#19981;&#26159;&#39044;&#27979;&#31572;&#26696;&#35789;&#12290;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31867;&#30334;&#31185;&#30693;&#35782;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#26356;&#22909;&#22320;&#35299;&#20915;ECI&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#22240;&#26524;&#20851;&#31995;&#35782;&#21035;&#65288;ECI&#65289;&#26088;&#22312;&#30830;&#23450;&#20004;&#20010;&#20107;&#20214;&#25552;&#21450;&#20043;&#38388;&#26159;&#21542;&#23384;&#22312;&#22240;&#26524;&#20851;&#31995;&#12290;&#20256;&#32479;&#30340;&#25552;&#31034;&#23398;&#20064;&#35774;&#35745;&#20102;&#19968;&#20010;&#25552;&#31034;&#27169;&#26495;&#65292;&#39318;&#20808;&#39044;&#27979;&#19968;&#20010;&#31572;&#26696;&#35789;&#65292;&#28982;&#21518;&#23558;&#20854;&#26144;&#23556;&#21040;&#26368;&#32456;&#30340;&#20915;&#31574;&#12290;&#19982;&#20256;&#32479;&#25552;&#31034;&#19981;&#21516;&#65292;&#25105;&#20204;&#35748;&#20026;&#39044;&#27979;&#19968;&#20010;&#31572;&#26696;&#35789;&#21487;&#33021;&#19981;&#26159;ECI&#20219;&#21153;&#30340;&#24517;&#35201;&#20808;&#20915;&#26465;&#20214;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#21487;&#20197;&#39318;&#20808;&#23545;&#20004;&#20010;&#20107;&#20214;&#20043;&#38388;&#26159;&#21542;&#23384;&#22312;&#22240;&#26524;&#20851;&#31995;&#36827;&#34892;&#30830;&#23450;&#24615;&#20551;&#35774;&#65292;&#28982;&#21518;&#35780;&#20272;&#20854;&#21512;&#29702;&#24615;&#65292;&#25509;&#21463;&#25110;&#25298;&#32477;&#35813;&#20551;&#35774;&#12290;&#35774;&#35745;&#30340;&#21160;&#26426;&#26159;&#23613;&#21487;&#33021;&#22320;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#31867;&#30334;&#31185;&#30693;&#35782;&#12290;&#22522;&#20110;&#36825;&#20123;&#32771;&#34385;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30830;&#23450;&#24615;&#20551;&#35774;&#25552;&#31034;&#23398;&#20064;&#27169;&#22411;&#65292;&#31216;&#20026;DAPrompt&#65292;&#29992;&#20110;ECI&#20219;&#21153;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#30830;&#23450;&#24615;&#20551;&#35774;&#27169;&#26495;&#65292;&#35813;&#27169;&#26495;&#19982;&#36755;&#20837;&#30340;&#20107;&#20214;&#23545;&#36827;&#34892;&#36830;&#25509;&#65292;&#20854;&#20013;&#21253;&#25324;&#20004;&#20010;&#25513;&#30721;&#20316;&#20026;&#39044;&#27979;&#20107;&#20214;&#30340;&#26631;&#35760;&#12290;&#25105;&#20204;&#20351;&#29992;&#27010;&#29575;&#26469;&#34913;&#37327;&#30830;&#23450;&#24615;&#20551;&#35774;&#30340;&#21512;&#29702;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Event Causality Identification (ECI) aims at determining whether there is a causal relation between two event mentions. Conventional prompt learning designs a prompt template to first predict an answer word and then maps it to the final decision. Unlike conventional prompts, we argue that predicting an answer word may not be a necessary prerequisite for the ECI task. Instead, we can first make a deterministic assumption on the existence of causal relation between two events and then evaluate its rationality to either accept or reject the assumption. The design motivation is to try the most utilization of the encyclopedia-like knowledge embedded in a pre-trained language model. In light of such considerations, we propose a deterministic assumption prompt learning model, called DAPrompt, for the ECI task. In particular, we design a simple deterministic assumption template concatenating with the input event pair, which includes two masks as predicted events' tokens. We use the probabiliti
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;Hugging Face LLMs&#30340;&#21629;&#21517;&#27861;&#65292;&#36890;&#36807;&#23618;&#27425;&#32858;&#31867;&#21644;n-gram&#26041;&#27861;&#65292;&#25104;&#21151;&#35782;&#21035;&#20102;15,821&#20010;LLMs&#30340;&#26063;&#32676;&#21644;&#23376;&#32452;&#65292;&#24182;&#21576;&#29616;&#20102;&#19968;&#20010;&#20844;&#20849;&#30340;&#32593;&#39029;&#24212;&#29992;&#31243;&#24207;&#26469;&#27983;&#35272;&#21644;&#25506;&#32034;&#36825;&#20123;LLMs&#12290;</title><link>http://arxiv.org/abs/2307.09793</link><description>&lt;p&gt;
&#20851;&#20110;LLMs&#30340;&#36215;&#28304;&#65306;15821&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#21270;&#26641;&#21644;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models. (arXiv:2307.09793v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;Hugging Face LLMs&#30340;&#21629;&#21517;&#27861;&#65292;&#36890;&#36807;&#23618;&#27425;&#32858;&#31867;&#21644;n-gram&#26041;&#27861;&#65292;&#25104;&#21151;&#35782;&#21035;&#20102;15,821&#20010;LLMs&#30340;&#26063;&#32676;&#21644;&#23376;&#32452;&#65292;&#24182;&#21576;&#29616;&#20102;&#19968;&#20010;&#20844;&#20849;&#30340;&#32593;&#39029;&#24212;&#29992;&#31243;&#24207;&#26469;&#27983;&#35272;&#21644;&#25506;&#32034;&#36825;&#20123;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;2022&#24180;&#24213;&#20197;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#20026;&#38750;&#24120;&#31361;&#20986;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#20687;ChatGPT&#21644;Bard&#36825;&#26679;&#30340;LLMs&#24050;&#32463;&#21560;&#24341;&#20102;&#25968;&#30334;&#19975;&#20010;&#29992;&#25143;&#12290;&#27599;&#21608;&#37117;&#26377;&#25968;&#30334;&#20010;&#26032;&#30340;LLMs&#34987;&#21457;&#24067;&#65292;&#20854;&#20013;&#35768;&#22810;&#34987;&#19978;&#20256;&#21040;Hugging Face&#65292;&#36825;&#26159;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#30340;&#24211;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#35813;&#32593;&#31449;&#24050;&#32463;&#19978;&#20256;&#20102;&#36817;16000&#20010;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#12290;&#37492;&#20110;LLMs&#30340;&#22823;&#37327;&#28044;&#20837;&#65292;&#20102;&#35299;&#21738;&#20123;LLM&#39592;&#24178;&#12289;&#35774;&#32622;&#12289;&#35757;&#32451;&#26041;&#27861;&#21644;&#26063;&#32676;&#21463;&#21040;&#27426;&#36814;&#25110;&#36235;&#21183;&#21464;&#24471;&#24456;&#26377;&#24847;&#20041;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27809;&#26377;&#19968;&#20010;&#20840;&#38754;&#30340;LLMs&#32034;&#24341;&#21487;&#29992;&#12290;&#25105;&#20204;&#21033;&#29992;Hugging Face LLMs&#30340;&#30456;&#23545;&#31995;&#32479;&#30340;&#21629;&#21517;&#27861;&#23545;LLMs&#36827;&#34892;&#23618;&#27425;&#32858;&#31867;&#65292;&#24182;&#20351;&#29992;n-gram&#21644;&#35789;&#39057;-&#36870;&#25991;&#26723;&#39057;&#29575;&#26469;&#35782;&#21035;LLMs&#20043;&#38388;&#30340;&#32852;&#31995;&#21644;&#32676;&#20307;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#35782;&#21035;&#20102;LLMs&#30340;&#26063;&#32676;&#65292;&#24182;&#23558;LLMs&#20934;&#30830;&#22320;&#32858;&#31867;&#25104;&#26377;&#24847;&#20041;&#30340;&#23376;&#32452;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#20844;&#20849;&#30340;&#32593;&#39029;&#24212;&#29992;&#31243;&#24207;&#26469;&#27983;&#35272;&#21644;&#25506;&#32034;Constellation&#65292;&#25105;&#20204;&#30340;15821&#20010;LLMs&#30340;&#22270;&#35889;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since late 2022, Large Language Models (LLMs) have become very prominent with LLMs like ChatGPT and Bard receiving millions of users. Hundreds of new LLMs are announced each week, many of which are deposited to Hugging Face, a repository of machine learning models and datasets. To date, nearly 16,000 Text Generation models have been uploaded to the site. Given the huge influx of LLMs, it is of interest to know which LLM backbones, settings, training methods, and families are popular or trending. However, there is no comprehensive index of LLMs available. We take advantage of the relatively systematic nomenclature of Hugging Face LLMs to perform hierarchical clustering and identify communities amongst LLMs using n-grams and term frequency-inverse document frequency. Our methods successfully identify families of LLMs and accurately cluster LLMs into meaningful subgroups. We present a public web application to navigate and explore Constellation, our atlas of 15,821 LLMs. Constellation rap
&lt;/p&gt;</description></item><item><title>ZeroQuant-FP&#36890;&#36807;&#20351;&#29992;&#28014;&#28857;&#26684;&#24335;&#36827;&#34892;LLMs&#35757;&#32451;&#21518;&#37327;&#21270;&#65292;&#35299;&#20915;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24179;&#34913;&#35745;&#31639;&#25928;&#29575;&#21644;&#20445;&#25345;&#27169;&#22411;&#36136;&#37327;&#30340;&#25361;&#25112;&#65292;&#24182;&#21457;&#29616;FP8&#28608;&#27963;&#20248;&#20110;INT8&#65292;&#24182;&#19988;FP4&#26435;&#37325;&#34920;&#29616;&#19982;INT4&#30456;&#24403;&#29978;&#33267;&#26356;&#20248;&#12290;</title><link>http://arxiv.org/abs/2307.09782</link><description>&lt;p&gt;
ZeroQuant-FP: &#20351;&#29992;&#28014;&#28857;&#26684;&#24335;&#36827;&#34892;LLMs&#35757;&#32451;&#21518;&#37327;&#21270;&#30340;&#19968;&#39033;&#39134;&#36291;
&lt;/p&gt;
&lt;p&gt;
ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats. (arXiv:2307.09782v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09782
&lt;/p&gt;
&lt;p&gt;
ZeroQuant-FP&#36890;&#36807;&#20351;&#29992;&#28014;&#28857;&#26684;&#24335;&#36827;&#34892;LLMs&#35757;&#32451;&#21518;&#37327;&#21270;&#65292;&#35299;&#20915;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24179;&#34913;&#35745;&#31639;&#25928;&#29575;&#21644;&#20445;&#25345;&#27169;&#22411;&#36136;&#37327;&#30340;&#25361;&#25112;&#65292;&#24182;&#21457;&#29616;FP8&#28608;&#27963;&#20248;&#20110;INT8&#65292;&#24182;&#19988;FP4&#26435;&#37325;&#34920;&#29616;&#19982;INT4&#30456;&#24403;&#29978;&#33267;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#22797;&#26434;&#39046;&#22495;&#20013;&#65292;&#24179;&#34913;&#35745;&#31639;&#25928;&#29575;&#21644;&#20445;&#25345;&#27169;&#22411;&#36136;&#37327;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#35752;&#28014;&#28857;&#65288;FP&#65289;&#37327;&#21270;&#30340;&#21487;&#34892;&#24615;&#65292;&#29305;&#21035;&#20851;&#27880;FP8&#21644;FP4&#65292;&#20197;&#24212;&#23545;&#22343;&#21248;&#37327;&#21270;&#30340;&#22266;&#26377;&#38480;&#21046;&#65292;&#23588;&#20854;&#26159;&#22788;&#29702;&#31163;&#32676;&#20540;&#65292;&#24182;&#21463;&#21040;NVIDIA H100&#30828;&#20214;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#30340;&#20840;&#38754;&#35843;&#26597;&#21457;&#29616;&#65292;&#22312;LLMs&#20013;&#65292;FP8&#28608;&#27963;&#22987;&#32456;&#20248;&#20110;&#20854;&#25972;&#25968;&#65288;INT8&#65289;&#31561;&#25928;&#65292;&#24615;&#33021;&#20248;&#21183;&#22312;&#21253;&#21547;&#36229;&#36807;&#21313;&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;&#20013;&#26356;&#20026;&#26126;&#26174;&#12290;&#23545;&#20110;&#26435;&#37325;&#37327;&#21270;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;FP4&#30340;&#24615;&#33021;&#19982;INT4&#30456;&#24403;&#65292;&#29978;&#33267;&#26356;&#20248;&#65292;&#31616;&#21270;&#20102;&#22312;&#20687;H100&#36825;&#26679;&#25903;&#25345;FP&#30340;&#30828;&#20214;&#19978;&#30340;&#37096;&#32626;&#12290;&#20026;&#20102;&#20943;&#23569;&#30001;&#26435;&#37325;&#21644;&#28608;&#27963;&#20043;&#38388;&#24046;&#24322;&#24341;&#36215;&#30340;&#31934;&#24230;&#23545;&#40784;&#24320;&#38144;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#32553;&#25918;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the complex domain of large language models (LLMs), striking a balance between computational efficiency and maintaining model quality is a formidable challenge. Navigating the inherent limitations of uniform quantization, particularly when dealing with outliers, and motivated by the launch of NVIDIA's H100 hardware, this study delves into the viability of floating-point (FP) quantization, particularly focusing on FP8 and FP4, as a potential solution. Our comprehensive investigation reveals that for LLMs, FP8 activation consistently outshines its integer (INT8) equivalent, with the performance edge becoming more noticeable in models possessing parameters beyond one billion. For weight quantization, our findings indicate that FP4 exhibits comparable, if not superior, performance to INT4, simplifying deployment on FP-supported hardware like H100. To mitigate the overhead from precision alignment caused by the disparity between weights and activations, we propose two scaling constraints
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35821;&#35328;&#23398;&#20064;&#32842;&#22825;&#26426;&#22120;&#20154;&#20013;&#20351;&#29992;GPT4&#36827;&#34892;ASR&#38169;&#35823;&#32416;&#27491;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;GPT4&#32416;&#27491;&#30340;&#36716;&#24405;&#23548;&#33268;&#26356;&#39640;&#30340;&#23545;&#35805;&#36136;&#37327;&#65292;&#23613;&#31649;WER&#26377;&#25152;&#22686;&#21152;&#12290;</title><link>http://arxiv.org/abs/2307.09744</link><description>&lt;p&gt;
&#25552;&#39640;&#35821;&#35328;&#23398;&#20064;&#32842;&#22825;&#26426;&#22120;&#20154;&#23545;&#35805;&#36136;&#37327;&#65306;&#23545;GPT4&#22312;ASR&#38169;&#35823;&#32416;&#27491;&#20013;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Enhancing conversational quality in language learning chatbots: An evaluation of GPT4 for ASR error correction. (arXiv:2307.09744v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09744
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35821;&#35328;&#23398;&#20064;&#32842;&#22825;&#26426;&#22120;&#20154;&#20013;&#20351;&#29992;GPT4&#36827;&#34892;ASR&#38169;&#35823;&#32416;&#27491;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;GPT4&#32416;&#27491;&#30340;&#36716;&#24405;&#23548;&#33268;&#26356;&#39640;&#30340;&#23545;&#35805;&#36136;&#37327;&#65292;&#23613;&#31649;WER&#26377;&#25152;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65288;NLP&#65289;&#22312;&#25945;&#32946;&#24212;&#29992;&#20013;&#30340;&#25972;&#21512;&#24050;&#32463;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#29305;&#21035;&#26159;&#22312;&#35821;&#35328;&#23398;&#20064;&#39046;&#22495;&#12290;&#26368;&#36817;&#65292;&#35768;&#22810;&#21475;&#35821;&#24320;&#25918;&#39046;&#22495;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#34987;&#29992;&#20316;&#21475;&#35821;&#20249;&#20276;&#65292;&#24110;&#21161;&#35821;&#35328;&#23398;&#20064;&#32773;&#25552;&#21319;&#35821;&#35328;&#25216;&#33021;&#12290;&#28982;&#32780;&#65292;&#20854;&#20013;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#26159;&#22312;&#35782;&#21035;&#38750;&#27597;&#35821;/&#38750;&#27969;&#21033;&#35821;&#38899;&#26102;&#30340;&#39640;&#23383;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#65292;&#36825;&#20250;&#25171;&#26029;&#23545;&#35805;&#27969;&#31243;&#24182;&#20196;&#23398;&#20064;&#32773;&#24863;&#21040;&#22833;&#26395;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#23545;&#35805;&#29615;&#22659;&#20013;&#20351;&#29992;GPT4&#36827;&#34892;ASR&#38169;&#35823;&#32416;&#27491;&#30340;&#26041;&#27861;&#12290;&#38500;&#20102;WER&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20351;&#29992;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#65288;STS&#65289;&#21644;&#19979;&#19968;&#20010;&#22238;&#22797;&#30340;&#21512;&#29702;&#24615;&#65288;NRS&#65289;&#25351;&#26631;&#26469;&#35780;&#20272;&#38169;&#35823;&#32416;&#27491;&#27169;&#22411;&#23545;&#23545;&#35805;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#30001;GPT4&#32416;&#27491;&#30340;&#36716;&#24405;&#23548;&#33268;&#26356;&#39640;&#30340;&#23545;&#35805;&#36136;&#37327;&#65292;&#23613;&#31649;WER&#26377;&#25152;&#22686;&#21152;&#12290;GPT4&#36824;&#20248;&#20110;&#26631;&#20934;&#30340;&#38169;&#35823;&#32416;&#27491;&#26041;&#27861;&#65292;&#32780;&#26080;&#38656;&#39046;&#22495;&#19987;&#23646;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
The integration of natural language processing (NLP) technologies into educational applications has shown promising results, particularly in the language learning domain. Recently, many spoken open-domain chatbots have been used as speaking partners, helping language learners improve their language skills. However, one of the significant challenges is the high word-error-rate (WER) when recognizing non-native/non-fluent speech, which interrupts conversation flow and leads to disappointment for learners. This paper explores the use of GPT4 for ASR error correction in conversational settings. In addition to WER, we propose to use semantic textual similarity (STS) and next response sensibility (NRS) metrics to evaluate the impact of error correction models on the quality of the conversation. We find that transcriptions corrected by GPT4 lead to higher conversation quality, despite an increase in WER. GPT4 also outperforms standard error correction methods without the need for in-domain tr
&lt;/p&gt;</description></item><item><title>RaTE&#26159;&#19968;&#31181;&#26080;&#26631;&#31614;&#30340;&#33258;&#21160;&#20998;&#31867;&#35780;&#20998;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#22823;&#22411;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#21487;&#37325;&#22797;&#30340;&#33258;&#21160;&#20998;&#31867;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;RaTE&#19982;&#20154;&#31867;&#35780;&#21028;&#20855;&#26377;&#36739;&#39640;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#19988;&#20154;&#20026;&#38477;&#20302;&#20998;&#31867;&#27861;&#20250;&#23548;&#33268;RaTE&#35780;&#20998;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2307.09706</link><description>&lt;p&gt;
RaTE: &#36890;&#36807;&#22635;&#34917;&#31354;&#30333;&#23454;&#29616;&#21487;&#37325;&#22797;&#30340;&#33258;&#21160;&#20998;&#31867;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
RaTE: a Reproducible automatic Taxonomy Evaluation by Filling the Gap. (arXiv:2307.09706v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09706
&lt;/p&gt;
&lt;p&gt;
RaTE&#26159;&#19968;&#31181;&#26080;&#26631;&#31614;&#30340;&#33258;&#21160;&#20998;&#31867;&#35780;&#20998;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#22823;&#22411;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#21487;&#37325;&#22797;&#30340;&#33258;&#21160;&#20998;&#31867;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;RaTE&#19982;&#20154;&#31867;&#35780;&#21028;&#20855;&#26377;&#36739;&#39640;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#19988;&#20154;&#20026;&#38477;&#20302;&#20998;&#31867;&#27861;&#20250;&#23548;&#33268;RaTE&#35780;&#20998;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#31867;&#27861;&#23545;&#20110;&#30693;&#35782;&#34920;&#31034;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#65292;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20851;&#20110;&#33258;&#21160;&#20998;&#31867;&#26500;&#24314;&#30340;&#30740;&#31350;&#20173;&#28982;&#20381;&#36182;&#20110;&#20154;&#24037;&#35780;&#20272;&#26469;&#35780;&#20998;&#25552;&#20986;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#35748;&#20026;&#33258;&#21160;&#20998;&#31867;&#35780;&#20272;&#21644;&#20998;&#31867;&#26500;&#24314;&#19968;&#26679;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;RaTE&#65292;&#19968;&#31181;&#26080;&#26631;&#31614;&#30340;&#33258;&#21160;&#20998;&#31867;&#35780;&#20998;&#26041;&#27861;&#65292;&#23427;&#20381;&#36182;&#20110;&#19968;&#20010;&#22823;&#22411;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#35813;&#35780;&#20272;&#26041;&#27861;&#24212;&#29992;&#20110;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;&#33258;&#21160;&#20998;&#31867;&#26500;&#24314;&#31639;&#27861;&#65292;&#24182;&#20174;Yelp&#39046;&#22495;&#26500;&#24314;&#20102;&#19971;&#20010;&#20998;&#31867;&#27861;&#65292;&#32467;&#26524;&#26174;&#31034;&#65306;1&#65289;RaTE&#19982;&#20154;&#31867;&#35780;&#21028;&#30340;&#30456;&#20851;&#24615;&#36739;&#39640;&#65307;2&#65289;&#20154;&#20026;&#38477;&#20302;&#20998;&#31867;&#27861;&#20250;&#23548;&#33268;RaTE&#35780;&#20998;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
Taxonomies are an essential knowledge representation, yet most studies on automatic taxonomy construction (ATC) resort to manual evaluation to score proposed algorithms. We argue that automatic taxonomy evaluation (ATE) is just as important as taxonomy construction. We propose RaTE, an automatic label-free taxonomy scoring procedure, which relies on a large pre-trained language model. We apply our evaluation procedure to three state-of-the-art ATC algorithms with which we built seven taxonomies from the Yelp domain, and show that 1) RaTE correlates well with human judgments and 2) artificially degrading a taxonomy leads to decreasing RaTE score.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;CValues&#65292;&#36825;&#26159;&#39318;&#20010;&#20013;&#25991;&#20154;&#31867;&#20215;&#20540;&#35266;&#35780;&#20272;&#22522;&#20934;&#65292;&#29992;&#20110;&#34913;&#37327;&#20013;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23433;&#20840;&#21644;&#36131;&#20219;&#20934;&#21017;&#26041;&#38754;&#30340;&#19968;&#33268;&#24615;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#34429;&#28982;&#22823;&#22810;&#25968;&#20013;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29992;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#19981;&#19968;&#33268;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2307.09705</link><description>&lt;p&gt;
CValues: &#20174;&#23433;&#20840;&#21040;&#36131;&#20219;&#24230;&#37327;&#20013;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20215;&#20540;&#35266;
&lt;/p&gt;
&lt;p&gt;
CValues: Measuring the Values of Chinese Large Language Models from Safety to Responsibility. (arXiv:2307.09705v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;CValues&#65292;&#36825;&#26159;&#39318;&#20010;&#20013;&#25991;&#20154;&#31867;&#20215;&#20540;&#35266;&#35780;&#20272;&#22522;&#20934;&#65292;&#29992;&#20110;&#34913;&#37327;&#20013;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23433;&#20840;&#21644;&#36131;&#20219;&#20934;&#21017;&#26041;&#38754;&#30340;&#19968;&#33268;&#24615;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#34429;&#28982;&#22823;&#22810;&#25968;&#20013;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29992;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#19981;&#19968;&#33268;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36805;&#36895;&#21457;&#23637;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#25285;&#24515;&#23427;&#20204;&#21487;&#33021;&#24102;&#26469;&#39118;&#38505;&#25110;&#23545;&#31038;&#20250;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#35780;&#20272;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#19968;&#33268;&#24615;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#20851;&#27880;&#35780;&#20272;LLMs&#22312;&#29305;&#23450;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#65292;&#32780;&#24573;&#35270;&#20102;&#23545;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#19968;&#33268;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#20013;&#22269;&#30340;&#32972;&#26223;&#19979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CValues&#65292;&#36825;&#26159;&#39318;&#20010;&#20013;&#25991;&#20154;&#31867;&#20215;&#20540;&#35266;&#35780;&#20272;&#22522;&#20934;&#65292;&#29992;&#20110;&#34913;&#37327;LLMs&#22312;&#23433;&#20840;&#21644;&#36131;&#20219;&#20934;&#21017;&#26041;&#38754;&#30340;&#19968;&#33268;&#24615;&#33021;&#21147;&#12290;&#20316;&#20026;&#32467;&#26524;&#65292;&#25105;&#20204;&#25163;&#21160;&#25910;&#38598;&#20102;10&#20010;&#22330;&#26223;&#30340;&#23545;&#25239;&#24615;&#23433;&#20840;&#25552;&#31034;&#65292;&#24182;&#30001;&#19987;&#19994;&#19987;&#23478;&#20174;8&#20010;&#39046;&#22495;&#35825;&#23548;&#20102;&#36131;&#20219;&#25552;&#31034;&#12290;&#20026;&#20102;&#20840;&#38754;&#35780;&#20272;&#20013;&#25991;LLMs&#30340;&#20215;&#20540;&#35266;&#65292;&#25105;&#20204;&#19981;&#20165;&#36827;&#34892;&#20154;&#24037;&#35780;&#20272;&#20197;&#36827;&#34892;&#21487;&#38752;&#30340;&#27604;&#36739;&#65292;&#36824;&#26500;&#24314;&#20102;&#22810;&#36873;&#25552;&#31034;&#20197;&#36827;&#34892;&#33258;&#21160;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#22823;&#22810;&#25968;&#20013;&#25991;LLMs&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29992;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#19981;&#19968;&#33268;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid evolution of large language models (LLMs), there is a growing concern that they may pose risks or have negative social impacts. Therefore, evaluation of human values alignment is becoming increasingly important. Previous work mainly focuses on assessing the performance of LLMs on certain knowledge and reasoning abilities, while neglecting the alignment to human values, especially in a Chinese context. In this paper, we present CValues, the first Chinese human values evaluation benchmark to measure the alignment ability of LLMs in terms of both safety and responsibility criteria. As a result, we have manually collected adversarial safety prompts across 10 scenarios and induced responsibility prompts from 8 domains by professional experts. To provide a comprehensive values evaluation of Chinese LLMs, we not only conduct human evaluation for reliable comparison, but also construct multi-choice prompts for automatic evaluation. Our findings suggest that while most Chinese LL
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#20351;&#29992;&#27491;&#21017;&#34920;&#36798;&#24335;&#21644;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#26469;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#25991;&#26412;&#29983;&#25104;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.09702</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;LLM&#24341;&#23548;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Efficient Guided Generation for LLMs. (arXiv:2307.09702v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#20351;&#29992;&#27491;&#21017;&#34920;&#36798;&#24335;&#21644;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#26469;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#25991;&#26412;&#29983;&#25104;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#20351;&#29992;&#27491;&#21017;&#34920;&#36798;&#24335;&#21644;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#26469;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#25991;&#26412;&#29983;&#25104;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26631;&#35760;&#24207;&#21015;&#29983;&#25104;&#36807;&#31243;&#20013;&#20960;&#20046;&#19981;&#22686;&#21152;&#20219;&#20309;&#24320;&#38144;&#65292;&#24182;&#20351;&#24471;&#24341;&#23548;&#29983;&#25104;&#22312;&#23454;&#38469;&#20013;&#21487;&#34892;&#12290;&#22312;&#24320;&#28304;Python&#24211;Outlines&#20013;&#25552;&#20379;&#20102;&#19968;&#20010;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this article we describe an efficient approach to guiding language model text generation with regular expressions and context-free grammars. Our approach adds little to no overhead to the token sequence generation process, and makes guided generation feasible in practice. An implementation is provided in the open source Python library Outlines.
&lt;/p&gt;</description></item><item><title>&#19968;&#39033;&#21517;&#20026;Pentathlon&#30340;&#25928;&#29575;&#35780;&#20272;&#31454;&#25216;&#22330;&#34987;&#24341;&#20837;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#20195;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#22312;&#27169;&#22411;&#25928;&#29575;&#35780;&#20272;&#21644;&#27604;&#36739;&#26041;&#38754;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#35813;&#31454;&#25216;&#22330;&#25552;&#20379;&#20102;&#20005;&#26684;&#25511;&#21046;&#30340;&#30828;&#20214;&#24179;&#21488;&#65292;&#33021;&#22815;&#27169;&#25311;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#22330;&#26223;&#65292;&#21516;&#26102;&#38598;&#25104;&#20102;&#22810;&#20010;&#25351;&#26631;&#26469;&#35780;&#27979;&#19981;&#21516;&#26041;&#38754;&#30340;&#25928;&#29575;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.09701</link><description>&lt;p&gt;
Efficiency Pentathlon: &#19968;&#39033;&#29992;&#20110;&#25928;&#29575;&#35780;&#20272;&#30340;&#26631;&#20934;&#31454;&#25216;&#22330;
&lt;/p&gt;
&lt;p&gt;
Efficiency Pentathlon: A Standardized Arena for Efficiency Evaluation. (arXiv:2307.09701v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09701
&lt;/p&gt;
&lt;p&gt;
&#19968;&#39033;&#21517;&#20026;Pentathlon&#30340;&#25928;&#29575;&#35780;&#20272;&#31454;&#25216;&#22330;&#34987;&#24341;&#20837;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#20195;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#22312;&#27169;&#22411;&#25928;&#29575;&#35780;&#20272;&#21644;&#27604;&#36739;&#26041;&#38754;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#35813;&#31454;&#25216;&#22330;&#25552;&#20379;&#20102;&#20005;&#26684;&#25511;&#21046;&#30340;&#30828;&#20214;&#24179;&#21488;&#65292;&#33021;&#22815;&#27169;&#25311;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#22330;&#26223;&#65292;&#21516;&#26102;&#38598;&#25104;&#20102;&#22810;&#20010;&#25351;&#26631;&#26469;&#35780;&#27979;&#19981;&#21516;&#26041;&#38754;&#30340;&#25928;&#29575;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31995;&#32479;&#30340;&#35745;&#31639;&#38656;&#27714;&#19981;&#26029;&#22686;&#21152;&#65292;&#22686;&#21152;&#20102;&#26368;&#21069;&#27839;&#30740;&#31350;&#30340;&#20934;&#20837;&#38376;&#27099;&#65292;&#21516;&#26102;&#24341;&#21457;&#20102;&#20005;&#37325;&#30340;&#29615;&#22659;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#25928;&#29575;&#30340;&#36827;&#23637;&#21463;&#21040;&#20102;&#27169;&#22411;&#35780;&#20272;&#21644;&#27604;&#36739;&#20013;&#30340;&#23454;&#38469;&#25361;&#25112;&#30340;&#38459;&#30861;&#12290;&#20363;&#22914;&#65292;&#30001;&#20110;&#19981;&#21516;&#26426;&#26500;&#20043;&#38388;&#30340;&#21487;&#35775;&#38382;&#24615;&#24046;&#24322;&#65292;&#30828;&#20214;&#25511;&#21046;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;FLOP&#31561;&#25351;&#26631;&#30340;&#25913;&#36827;&#36890;&#24120;&#26080;&#27861;&#36716;&#21270;&#20026;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#30340;&#36827;&#23637;&#12290;&#20316;&#20026;&#22238;&#24212;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Pentathlon&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#25972;&#20307;&#21644;&#30495;&#23454;&#35780;&#20272;&#27169;&#22411;&#25928;&#29575;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;Pentathlon&#19987;&#27880;&#20110;&#25512;&#29702;&#65292;&#36825;&#22312;&#27169;&#22411;&#29983;&#21629;&#21608;&#26399;&#20013;&#21344;&#22823;&#22810;&#25968;&#35745;&#31639;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#20005;&#26684;&#25511;&#21046;&#30340;&#30828;&#20214;&#24179;&#21488;&#65292;&#24182;&#35774;&#35745;&#20026;&#27169;&#25311;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#22330;&#26223;&#12290;&#23427;&#21253;&#21547;&#20102;&#19968;&#22871;&#38024;&#23545;&#25928;&#29575;&#19981;&#21516;&#26041;&#38754;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#21253;&#25324;&#24310;&#36831;&#12289;&#21534;&#21520;&#37327;&#12289;&#20869;&#23384;&#24320;&#38144;&#21644;&#33021;&#37327;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rising computational demands of modern natural language processing (NLP) systems have increased the barrier to entry for cutting-edge research while posing serious environmental concerns. Yet, progress on model efficiency has been impeded by practical challenges in model evaluation and comparison. For example, hardware is challenging to control due to disparate levels of accessibility across different institutions. Moreover, improvements in metrics such as FLOPs often fail to translate to progress in real-world applications. In response, we introduce Pentathlon, a benchmark for holistic and realistic evaluation of model efficiency. Pentathlon focuses on inference, which accounts for a majority of the compute in a model's lifecycle. It offers a strictly-controlled hardware platform, and is designed to mirror real-world applications scenarios. It incorporates a suite of metrics that target different aspects of efficiency, including latency, throughput, memory overhead, and energy consump
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#38463;&#25289;&#20271;&#35789;&#20998;&#21106;&#26041;&#27861;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20998;&#26512;&#21382;&#21490;&#21644;&#23447;&#25945;&#25991;&#26412;&#65292;&#24110;&#21161;&#29702;&#35299;&#25991;&#26412;&#20013;&#30340;&#24847;&#20041;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#22312;&#35789;&#27719;&#37327;&#21644;&#31181;&#31867;&#19978;&#37117;&#20248;&#20110;&#20854;&#20182;&#29616;&#26377;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#22312;&#38463;&#25289;&#20271;&#21704;&#36842;&#26031;&#39046;&#22495;&#26159;&#39318;&#20010;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#22810;&#31181;&#26041;&#27861;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#35780;&#20272;&#24182;&#25253;&#21578;&#20102;&#27880;&#37322;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.09630</link><description>&lt;p&gt;
Noor-Ghateh: &#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#21704;&#36842;&#26031;&#39046;&#22495;&#38463;&#25289;&#20271;&#35789;&#20998;&#21106;&#22120;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Noor-Ghateh: A Benchmark Dataset for Evaluating Arabic Word Segmenters in Hadith Domain. (arXiv:2307.09630v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09630
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#38463;&#25289;&#20271;&#35789;&#20998;&#21106;&#26041;&#27861;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20998;&#26512;&#21382;&#21490;&#21644;&#23447;&#25945;&#25991;&#26412;&#65292;&#24110;&#21161;&#29702;&#35299;&#25991;&#26412;&#20013;&#30340;&#24847;&#20041;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#22312;&#35789;&#27719;&#37327;&#21644;&#31181;&#31867;&#19978;&#37117;&#20248;&#20110;&#20854;&#20182;&#29616;&#26377;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#22312;&#38463;&#25289;&#20271;&#21704;&#36842;&#26031;&#39046;&#22495;&#26159;&#39318;&#20010;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#22810;&#31181;&#26041;&#27861;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#35780;&#20272;&#24182;&#25253;&#21578;&#20102;&#27880;&#37322;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#25289;&#20271;&#35821;&#20855;&#26377;&#35768;&#22810;&#22797;&#26434;&#32780;&#20016;&#23500;&#30340;&#24418;&#24577;&#23398;&#32454;&#24494;&#24046;&#21035;&#65292;&#36825;&#22312;&#20998;&#26512;&#20256;&#32479;&#30340;&#38463;&#25289;&#20271;&#25991;&#26412;&#65292;&#29305;&#21035;&#26159;&#22312;&#21382;&#21490;&#21644;&#23447;&#25945;&#35821;&#22659;&#20013;&#65292;&#23545;&#20110;&#29702;&#35299;&#25991;&#26412;&#30340;&#21547;&#20041;&#38750;&#24120;&#26377;&#29992;&#12290;&#35789;&#27719;&#20998;&#31163;&#24847;&#21619;&#30528;&#23558;&#35789;&#35821;&#20998;&#35299;&#20026;&#35832;&#22914;&#35789;&#26681;&#21644;&#35789;&#32512;&#31561;&#19981;&#21516;&#37096;&#20998;&#12290;&#22312;&#24418;&#24577;&#23398;&#25968;&#25454;&#38598;&#20013;&#65292;&#26631;&#31614;&#30340;&#22810;&#26679;&#24615;&#21644;&#25968;&#25454;&#26679;&#26412;&#30340;&#25968;&#37327;&#26377;&#21161;&#20110;&#35780;&#20272;&#24418;&#24577;&#23398;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#20998;&#31163;&#38463;&#25289;&#20271;&#35789;&#27719;&#30340;&#26041;&#27861;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#26469;&#33258;&#12298;&#20234;&#26031;&#20848;&#25945;&#27861;&#12299;&#30340;&#32422;223,690&#20010;&#35789;&#27719;&#65292;&#24050;&#30001;&#19987;&#23478;&#36827;&#34892;&#26631;&#35760;&#12290;&#23601;&#35789;&#27719;&#37327;&#21644;&#31181;&#31867;&#32780;&#35328;&#65292;&#35813;&#25968;&#25454;&#38598;&#20248;&#20110;&#20854;&#20182;&#29616;&#26377;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#19981;&#23384;&#22312;&#38463;&#25289;&#20271;&#21704;&#36842;&#26031;&#39046;&#22495;&#30340;&#25991;&#26412;&#12290;&#20026;&#20102;&#35780;&#20272;&#35813;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23545;&#25968;&#25454;&#38598;&#24212;&#29992;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#22914;Farasa&#12289;Camel&#12289;Madamira&#21644;ALP&#65292;&#24182;&#36890;&#36807;&#22235;&#20010;&#21442;&#25968;&#25253;&#21578;&#20102;&#27880;&#37322;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
There are many complex and rich morphological subtleties in the Arabic language, which are very useful when analyzing traditional Arabic texts, especially in the historical and religious contexts, and help in understanding the meaning of the texts. Vocabulary separation means separating the word into different parts such as root and affix. In the morphological datasets, the variety of labels and the number of data samples helps to evaluate the morphological methods. In this paper, we present a benchmark data set for evaluating the methods of separating Arabic words which include about 223,690 words from the book of Sharia alIslam, which have been labeled by experts. In terms of the volume and variety of words, this dataset is superior to other existing data sets, and as far as we know, there are no Arabic Hadith Domain texts. To evaluate the dataset, we applied different methods such as Farasa, Camel, Madamira, and ALP to the dataset and we reported the annotation quality through four 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#24320;&#25918;&#22495;&#32842;&#22825;&#26426;&#22120;&#20154;&#20013;&#30340;&#22810;&#36718;&#26377;&#23475;&#34892;&#20026;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#29616;&#26377;&#24037;&#20855;&#26080;&#27861;&#26816;&#27979;&#20986;82%&#23548;&#33268;&#26377;&#23475;&#34892;&#20026;&#30340;&#21333;&#21477;&#37117;&#34987;&#35748;&#20026;&#26159;&#23433;&#20840;&#30340;&#12290;&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#25915;&#20987;&#26041;&#27861;\toxicbot&#65292;&#25105;&#20204;&#21457;&#29616;&#24320;&#25918;&#22495;&#32842;&#22825;&#26426;&#22120;&#20154;&#27169;&#22411;&#21487;&#20197;&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#35302;&#21457;&#29983;&#25104;&#26377;&#23475;&#22238;&#24212;&#12290;</title><link>http://arxiv.org/abs/2307.09579</link><description>&lt;p&gt;
&#29702;&#35299;&#24320;&#25918;&#22495;&#32842;&#22825;&#26426;&#22120;&#20154;&#20013;&#30340;&#22810;&#36718;&#26377;&#23475;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Understanding Multi-Turn Toxic Behaviors in Open-Domain Chatbots. (arXiv:2307.09579v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#24320;&#25918;&#22495;&#32842;&#22825;&#26426;&#22120;&#20154;&#20013;&#30340;&#22810;&#36718;&#26377;&#23475;&#34892;&#20026;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#29616;&#26377;&#24037;&#20855;&#26080;&#27861;&#26816;&#27979;&#20986;82%&#23548;&#33268;&#26377;&#23475;&#34892;&#20026;&#30340;&#21333;&#21477;&#37117;&#34987;&#35748;&#20026;&#26159;&#23433;&#20840;&#30340;&#12290;&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#25915;&#20987;&#26041;&#27861;\toxicbot&#65292;&#25105;&#20204;&#21457;&#29616;&#24320;&#25918;&#22495;&#32842;&#22825;&#26426;&#22120;&#20154;&#27169;&#22411;&#21487;&#20197;&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#35302;&#21457;&#29983;&#25104;&#26377;&#23475;&#22238;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#36827;&#23637;&#20351;&#24471;&#32842;&#22825;&#26426;&#22120;&#20154;&#27169;&#22411;&#22914;ChatGPT&#21487;&#20197;&#19982;&#20154;&#31867;&#29992;&#25143;&#36827;&#34892;&#23545;&#35805;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#38750;&#26377;&#23475;&#30340;&#22810;&#36718;&#23545;&#35805;&#20013;&#29983;&#25104;&#26377;&#23475;&#25110;&#26377;&#30861;&#30340;&#22238;&#24212;&#33021;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#24615;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#29616;&#26377;&#30740;&#31350;&#20851;&#27880;&#20110;&#21333;&#21477;&#27979;&#35797;&#65292;&#32780;&#25105;&#20204;&#21457;&#29616;82&#65285;&#22240;&#20026;&#21333;&#19968;&#21477;&#23376;&#32780;&#22312;&#23545;&#35805;&#20013;&#35825;&#21457;&#26377;&#23475;&#34892;&#20026;&#30340;&#21477;&#23376;&#34987;&#29616;&#26377;&#24037;&#20855;&#35748;&#20026;&#26159;&#23433;&#20840;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#26041;&#27861;\toxicbot&#65292;&#36890;&#36807;&#23545;&#32842;&#22825;&#26426;&#22120;&#20154;&#36827;&#34892;&#24494;&#35843;&#19982;&#30446;&#26631;&#24320;&#25918;&#22495;&#32842;&#22825;&#26426;&#22120;&#20154;&#36827;&#34892;&#23545;&#35805;&#12290;&#32842;&#22825;&#26426;&#22120;&#20154;&#34987;&#24494;&#35843;&#20026;&#21463;&#25511;&#30340;&#20250;&#35805;&#24207;&#21015;&#12290;&#29305;&#21035;&#26159;&#65292;&#27599;&#20010;&#23545;&#35805;&#30340;&#36215;&#22987;&#37117;&#26469;&#33258;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#21477;&#23376;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#35780;&#20272;&#34920;&#26126;&#65292;&#24320;&#25918;&#22495;&#32842;&#22825;&#26426;&#22120;&#20154;&#27169;&#22411;&#21487;&#20197;&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#35302;&#21457;&#29983;&#25104;&#26377;&#23475;&#22238;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in natural language processing and machine learning have led to the development of chatbot models, such as ChatGPT, that can engage in conversational dialogue with human users. However, the ability of these models to generate toxic or harmful responses during a non-toxic multi-turn conversation remains an open research question. Existing research focuses on single-turn sentence testing, while we find that 82\% of the individual non-toxic sentences that elicit toxic behaviors in a conversation are considered safe by existing tools. In this paper, we design a new attack, \toxicbot, by fine-tuning a chatbot to engage in conversation with a target open-domain chatbot. The chatbot is fine-tuned with a collection of crafted conversation sequences. Particularly, each conversation begins with a sentence from a crafted prompt sentences dataset. Our extensive evaluation shows that open-domain chatbot models can be triggered to generate toxic responses in a multi-turn conversation
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#38271;&#25991;&#26723;&#20998;&#31867;&#20013;&#37319;&#29992;&#27169;&#22411;&#34701;&#21512;&#30340;&#26041;&#27861;&#65292;&#19982;BERT&#21644;Longformer&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#21457;&#29616;&#27169;&#22411;&#34701;&#21512;&#22312;&#22788;&#29702;&#38271;&#25991;&#26723;&#20998;&#31867;&#38382;&#39064;&#19978;&#20855;&#26377;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.09532</link><description>&lt;p&gt;
&#27169;&#22411;&#34701;&#21512;&#26159;&#21542;&#33021;&#24110;&#21161;Transformer&#27169;&#22411;&#22312;&#38271;&#25991;&#26723;&#20998;&#31867;&#20013;&#21462;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#65311;&#19968;&#39033;&#23454;&#35777;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can Model Fusing Help Transformers in Long Document Classification? An Empirical Study. (arXiv:2307.09532v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09532
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#38271;&#25991;&#26723;&#20998;&#31867;&#20013;&#37319;&#29992;&#27169;&#22411;&#34701;&#21512;&#30340;&#26041;&#27861;&#65292;&#19982;BERT&#21644;Longformer&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#21457;&#29616;&#27169;&#22411;&#34701;&#21512;&#22312;&#22788;&#29702;&#38271;&#25991;&#26723;&#20998;&#31867;&#38382;&#39064;&#19978;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#20998;&#31867;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#19968;&#20010;&#30740;&#31350;&#26041;&#21521;&#65292;&#22810;&#24180;&#26469;&#19968;&#30452;&#21463;&#21040;&#20851;&#27880;&#12290;&#23558;NLP&#24212;&#29992;&#20110;&#22810;&#20010;&#39046;&#22495;&#20026;&#25991;&#26412;&#20998;&#31867;&#24341;&#20837;&#20102;&#35768;&#22810;&#26032;&#30340;&#25361;&#25112;&#20043;&#19968;&#23601;&#26159;&#38271;&#25991;&#26723;&#20998;&#31867;&#12290;&#23613;&#31649;&#26368;&#20808;&#36827;&#30340;Transformer&#27169;&#22411;&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#25552;&#20379;&#20102;&#20986;&#33394;&#30340;&#32467;&#26524;&#65292;&#20294;&#22823;&#37096;&#20998;&#27169;&#22411;&#22312;&#36755;&#20837;&#24207;&#21015;&#30340;&#26368;&#22823;&#38271;&#24230;&#19978;&#23384;&#22312;&#38480;&#21046;&#12290;&#22823;&#22810;&#25968;Transformer&#27169;&#22411;&#20165;&#38480;&#21046;&#20026;512&#20010;&#20196;&#29260;&#65292;&#22240;&#27492;&#22312;&#38271;&#25991;&#26723;&#20998;&#31867;&#38382;&#39064;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#38271;&#25991;&#26723;&#20998;&#31867;&#20013;&#24212;&#29992;&#27169;&#22411;&#34701;&#21512;&#30340;&#30456;&#20851;&#26041;&#27861;&#65292;&#24182;&#23558;&#32467;&#26524;&#19982;&#30693;&#21517;&#30340;BERT&#21644;Longformer&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text classification is an area of research which has been studied over the years in Natural Language Processing (NLP). Adapting NLP to multiple domains has introduced many new challenges for text classification and one of them is long document classification. While state-of-the-art transformer models provide excellent results in text classification, most of them have limitations in the maximum sequence length of the input sequence. The majority of the transformer models are limited to 512 tokens, and therefore, they struggle with long document classification problems. In this research, we explore on employing Model Fusing for long document classification while comparing the results with well-known BERT and Longformer architectures.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20998;&#26512;&#20102;&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;SR-GAN&#27169;&#22411;&#65292;&#32467;&#26524;&#21457;&#29616;EDSR&#27169;&#22411;&#22312;&#20445;&#25345;&#35270;&#35273;&#36136;&#37327;&#30340;&#21516;&#26102;&#26174;&#33879;&#25552;&#39640;&#20102;&#36755;&#20837;&#22270;&#20687;&#30340;&#20998;&#36776;&#29575;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;PSNR&#21644;SSIM&#20540;&#65292;&#24182;&#19988;&#36820;&#22238;&#39640;&#36136;&#37327;&#30340;OCR&#32467;&#26524;&#65292;&#36825;&#34920;&#26126;EDSR&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#36229;&#20998;&#36776;&#29575;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.09456</link><description>&lt;p&gt;
SR-GAN&#27169;&#22411;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A comparative analysis of SR-GAN models. (arXiv:2307.09456v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09456
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20998;&#26512;&#20102;&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;SR-GAN&#27169;&#22411;&#65292;&#32467;&#26524;&#21457;&#29616;EDSR&#27169;&#22411;&#22312;&#20445;&#25345;&#35270;&#35273;&#36136;&#37327;&#30340;&#21516;&#26102;&#26174;&#33879;&#25552;&#39640;&#20102;&#36755;&#20837;&#22270;&#20687;&#30340;&#20998;&#36776;&#29575;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;PSNR&#21644;SSIM&#20540;&#65292;&#24182;&#19988;&#36820;&#22238;&#39640;&#36136;&#37327;&#30340;OCR&#32467;&#26524;&#65292;&#36825;&#34920;&#26126;EDSR&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#36229;&#20998;&#36776;&#29575;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;&#36229;&#20998;&#36776;&#29575;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;SR GAN&#65289;&#27169;&#22411;&#65292;&#21253;&#25324;ESRGAN&#65292;Real-ESRGAN&#21644;EDSR&#65292;&#22312;&#19968;&#20010;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#20197;&#21450;&#32463;&#36807;&#38477;&#32423;&#22788;&#29702;&#30340;&#30495;&#23454;&#19990;&#30028;&#22270;&#20687;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#20123;&#27169;&#22411;&#20284;&#20046;&#22312;&#20445;&#25345;&#35270;&#35273;&#36136;&#37327;&#30340;&#21516;&#26102;&#26174;&#33879;&#25552;&#39640;&#20102;&#36755;&#20837;&#22270;&#20687;&#30340;&#20998;&#36776;&#29575;&#65292;&#36825;&#26159;&#36890;&#36807;&#20351;&#29992;Tesseract OCR&#24341;&#25806;&#35780;&#20272;&#30340;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#26469;&#33258;huggingface&#30340;EDSR-BASE&#27169;&#22411;&#22312;&#23450;&#37327;&#25351;&#26631;&#21644;&#20027;&#35266;&#35270;&#35273;&#36136;&#37327;&#35780;&#20272;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#20854;&#20313;&#20505;&#36873;&#27169;&#22411;&#65292;&#24182;&#19988;&#35745;&#31639;&#24320;&#38144;&#26368;&#23567;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;EDSR&#29983;&#25104;&#20855;&#26377;&#36739;&#39640;&#23792;&#20540;&#20449;&#22122;&#27604;&#65288;PSNR&#65289;&#21644;&#32467;&#26500;&#30456;&#20284;&#24615;&#25351;&#25968;&#65288;SSIM&#65289;&#20540;&#30340;&#22270;&#20687;&#65292;&#24182;&#22312;Tesseract OCR&#24341;&#25806;&#19979;&#36820;&#22238;&#39640;&#36136;&#37327;&#30340;OCR&#32467;&#26524;&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#65292;EDSR&#26159;&#19968;&#31181;&#31283;&#20581;&#26377;&#25928;&#30340;&#21333;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#26041;&#27861;&#65292;&#29305;&#21035;&#36866;&#21512;&#20110;&#38656;&#35201;OCR&#30340;&#24212;&#29992;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we evaluate the performance of multiple state-of-the-art SR GAN (Super Resolution Generative Adversarial Network) models, ESRGAN, Real-ESRGAN and EDSR, on a benchmark dataset of real-world images which undergo degradation using a pipeline. Our results show that some models seem to significantly increase the resolution of the input images while preserving their visual quality, this is assessed using Tesseract OCR engine. We observe that EDSR-BASE model from huggingface outperforms the remaining candidate models in terms of both quantitative metrics and subjective visual quality assessments with least compute overhead. Specifically, EDSR generates images with higher peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM) values and are seen to return high quality OCR results with Tesseract OCR engine. These findings suggest that EDSR is a robust and effective approach for single-image super-resolution and may be particularly well-suited for applications wh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20266;&#24322;&#24120;&#26292;&#38706;&#65288;POE&#65289;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39034;&#24207;&#23631;&#34109;&#19982;&#20869;&#20998;&#24067;&#31867;&#30456;&#20851;&#30340;&#20196;&#29260;&#26500;&#24314;&#26367;&#20195;&#30340;&#22806;&#20998;&#24067;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#26816;&#27979;&#26410;&#30693;&#20998;&#24067;&#30340;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2307.09455</link><description>&lt;p&gt;
&#22522;&#20110;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#30340;&#20266;&#24322;&#24120;&#26292;&#38706;&#27861;&#29992;&#20110;&#26816;&#27979;&#26410;&#30693;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
Pseudo Outlier Exposure for Out-of-Distribution Detection using Pretrained Transformers. (arXiv:2307.09455v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20266;&#24322;&#24120;&#26292;&#38706;&#65288;POE&#65289;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39034;&#24207;&#23631;&#34109;&#19982;&#20869;&#20998;&#24067;&#31867;&#30456;&#20851;&#30340;&#20196;&#29260;&#26500;&#24314;&#26367;&#20195;&#30340;&#22806;&#20998;&#24067;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#26816;&#27979;&#26410;&#30693;&#20998;&#24067;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#35821;&#35328;&#24212;&#29992;&#65292;&#26816;&#27979;&#26410;&#30693;&#20998;&#24067;&#30340;&#26679;&#26412;&#26377;&#21161;&#20110;&#25552;&#37266;&#29992;&#25143;&#25110;&#25298;&#32477;&#19981;&#21487;&#38752;&#30340;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#29616;&#20195;&#36807;&#21442;&#25968;&#21270;&#30340;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#20250;&#23545;&#20869;&#20998;&#24067;&#21644;&#22806;&#20998;&#24067;&#26679;&#26412;&#37117;&#20135;&#29983;&#36807;&#24230;&#33258;&#20449;&#30340;&#39044;&#27979;&#12290;&#29305;&#21035;&#26159;&#65292;&#35821;&#35328;&#27169;&#22411;&#22312;&#19982;&#20869;&#20998;&#24067;&#26679;&#26412;&#20855;&#26377;&#30456;&#20284;&#35821;&#20041;&#34920;&#31034;&#30340;&#22806;&#20998;&#24067;&#26679;&#26412;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#22240;&#20026;&#36825;&#20123;&#22806;&#20998;&#24067;&#26679;&#26412;&#20301;&#20110;&#20869;&#20998;&#24067;&#27969;&#24418;&#38468;&#36817;&#12290;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#20869;&#20998;&#24067;&#21644;&#22810;&#26679;&#30340;&#24322;&#24120;&#26679;&#26412;&#35757;&#32451;&#25298;&#32477;&#32593;&#32476;&#26469;&#26816;&#27979;&#27979;&#35797;&#30340;&#22806;&#20998;&#24067;&#26679;&#26412;&#65292;&#20294;&#26126;&#30830;&#25910;&#38598;&#36741;&#21161;&#30340;&#22806;&#20998;&#24067;&#25968;&#25454;&#38598;&#20250;&#22686;&#21152;&#25968;&#25454;&#25910;&#38598;&#30340;&#36127;&#25285;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#20266;&#24322;&#24120;&#26292;&#38706;&#65288;POE&#65289;&#65292;&#36890;&#36807;&#39034;&#24207;&#23631;&#34109;&#19982;&#20869;&#20998;&#24067;&#31867;&#30456;&#20851;&#30340;&#20196;&#29260;&#26469;&#26500;&#24314;&#19968;&#20010;&#26367;&#20195;&#30340;&#22806;&#20998;&#24067;&#25968;&#25454;&#38598;&#12290;POE&#24341;&#20837;&#30340;&#26367;&#20195;&#22806;&#20998;&#24067;&#26679;&#26412;&#26174;&#31034;&#20986;&#19982;&#20869;&#37096;&#25968;&#25454;&#31867;&#20284;&#30340;&#34920;&#31034;&#65292;&#36825;&#23545;&#20110;&#35757;&#32451;&#25298;&#32477;&#32593;&#32476;&#26368;&#26377;&#25928;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#20219;&#20309;
&lt;/p&gt;
&lt;p&gt;
For real-world language applications, detecting an out-of-distribution (OOD) sample is helpful to alert users or reject such unreliable samples. However, modern over-parameterized language models often produce overconfident predictions for both in-distribution (ID) and OOD samples. In particular, language models suffer from OOD samples with a similar semantic representation to ID samples since these OOD samples lie near the ID manifold. A rejection network can be trained with ID and diverse outlier samples to detect test OOD samples, but explicitly collecting auxiliary OOD datasets brings an additional burden for data collection. In this paper, we propose a simple but effective method called Pseudo Outlier Exposure (POE) that constructs a surrogate OOD dataset by sequentially masking tokens related to ID classes. The surrogate OOD sample introduced by POE shows a similar representation to ID data, which is most effective in training a rejection network. Our method does not require any 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;ViCE&#65292;&#36890;&#36807;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#38382;&#31572;&#65292;&#27169;&#25311;&#20154;&#31867;&#35748;&#30693;&#36807;&#31243;&#26469;&#35780;&#20272;&#29983;&#25104;&#22270;&#20687;&#19982;&#25552;&#31034;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#21644;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.09416</link><description>&lt;p&gt;
&#35753;&#25105;&#20204;&#26469;ViCE&#65281;&#22312;&#22270;&#20687;&#29983;&#25104;&#35780;&#20272;&#20013;&#27169;&#25311;&#20154;&#31867;&#35748;&#30693;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Let's ViCE! Mimicking Human Cognitive Behavior in Image Generation Evaluation. (arXiv:2307.09416v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09416
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;ViCE&#65292;&#36890;&#36807;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#38382;&#31572;&#65292;&#27169;&#25311;&#20154;&#31867;&#35748;&#30693;&#36807;&#31243;&#26469;&#35780;&#20272;&#29983;&#25104;&#22270;&#20687;&#19982;&#25552;&#31034;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22312;&#22270;&#20687;&#29983;&#25104;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#23588;&#20854;&#26159;&#22312;&#24341;&#20837;&#20102;&#33021;&#22815;&#26681;&#25454;&#25991;&#23383;&#36755;&#20837;&#20135;&#29983;&#39640;&#36136;&#37327;&#35270;&#35273;&#20869;&#23481;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#21160;&#19979;&#12290;&#23613;&#31649;&#22312;&#29983;&#25104;&#36136;&#37327;&#21644;&#36924;&#30495;&#24230;&#26041;&#38754;&#19981;&#26029;&#21462;&#24471;&#36827;&#23637;&#65292;&#20294;&#30446;&#21069;&#23578;&#26410;&#23450;&#20041;&#31995;&#32479;&#24615;&#30340;&#26694;&#26550;&#26469;&#23450;&#37327;&#34913;&#37327;&#29983;&#25104;&#20869;&#23481;&#30340;&#36136;&#37327;&#21644;&#19982;&#25552;&#31034;&#35201;&#27714;&#30340;&#19968;&#33268;&#24615;&#65306;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#21482;&#26377;&#22522;&#20110;&#20154;&#31867;&#30340;&#35780;&#20272;&#34987;&#29992;&#20110;&#36136;&#37327;&#28385;&#24847;&#24230;&#21644;&#27604;&#36739;&#19981;&#21516;&#30340;&#29983;&#25104;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#26469;&#36827;&#34892;&#35270;&#35273;&#27010;&#24565;&#35780;&#20272;&#65288;ViCE&#65289;&#65292;&#21363;&#35780;&#20272;&#29983;&#25104;/&#32534;&#36753;&#30340;&#22270;&#20687;&#19982;&#30456;&#24212;&#25552;&#31034;/&#25351;&#20196;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#36825;&#19968;&#36807;&#31243;&#21463;&#21040;&#20154;&#31867;&#35748;&#30693;&#34892;&#20026;&#30340;&#21551;&#21457;&#12290;ViCE&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#30340;&#20248;&#21183;&#32467;&#21512;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#27969;&#31243;&#20013;&#65292;&#26088;&#22312;&#22797;&#21046;&#20154;&#31867;&#35748;&#30693;&#36807;&#31243;&#20013;&#30340;&#36136;&#37327;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research in Image Generation has recently made significant progress, particularly boosted by the introduction of Vision-Language models which are able to produce high-quality visual content based on textual inputs. Despite ongoing advancements in terms of generation quality and realism, no methodical frameworks have been defined yet to quantitatively measure the quality of the generated content and the adherence with the prompted requests: so far, only human-based evaluations have been adopted for quality satisfaction and for comparing different generative methods. We introduce a novel automated method for Visual Concept Evaluation (ViCE), i.e. to assess consistency between a generated/edited image and the corresponding prompt/instructions, with a process inspired by the human cognitive behaviour. ViCE combines the strengths of Large Language Models (LLMs) and Visual Question Answering (VQA) into a unified pipeline, aiming to replicate the human cognitive process in quality assessment.
&lt;/p&gt;</description></item><item><title>Llama 2&#26159;&#19968;&#20010;&#20248;&#21270;&#30340;&#32842;&#22825;&#27169;&#22411;&#65292;&#36890;&#36807;fine-tuned&#25216;&#26415;&#21644;&#23433;&#20840;&#25913;&#36827;&#65292;&#34920;&#29616;&#20248;&#20110;&#24320;&#28304;&#27169;&#22411;&#65292;&#24182;&#21487;&#20316;&#20026;&#38381;&#28304;&#27169;&#22411;&#30340;&#26367;&#20195;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2307.09288</link><description>&lt;p&gt;
Llama 2: &#24320;&#25918;&#22522;&#30784;&#21644;&#20248;&#21270;&#32842;&#22825;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Llama 2: Open Foundation and Fine-Tuned Chat Models. (arXiv:2307.09288v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09288
&lt;/p&gt;
&lt;p&gt;
Llama 2&#26159;&#19968;&#20010;&#20248;&#21270;&#30340;&#32842;&#22825;&#27169;&#22411;&#65292;&#36890;&#36807;fine-tuned&#25216;&#26415;&#21644;&#23433;&#20840;&#25913;&#36827;&#65292;&#34920;&#29616;&#20248;&#20110;&#24320;&#28304;&#27169;&#22411;&#65292;&#24182;&#21487;&#20316;&#20026;&#38381;&#28304;&#27169;&#22411;&#30340;&#26367;&#20195;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#24182;&#21457;&#24067;&#20102;Llama 2&#65292;&#19968;&#20010;&#21253;&#21547;&#39044;&#35757;&#32451;&#21644;&#20248;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#20854;&#35268;&#27169;&#20174;70&#20159;&#21040;700&#20159;&#21442;&#25968;&#19981;&#31561;&#12290;&#25105;&#20204;&#30340;&#20248;&#21270;LLM&#65292;&#31216;&#20026;Llama 2-Chat&#65292;&#22312;&#23545;&#35805;&#20351;&#29992;&#26696;&#20363;&#20013;&#34920;&#29616;&#20248;&#20110;&#24320;&#28304;&#32842;&#22825;&#27169;&#22411;&#12290;&#26681;&#25454;&#25105;&#20204;&#23545;&#26377;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#30340;&#20154;&#24037;&#35780;&#20272;&#32467;&#26524;&#65292;&#23427;&#20204;&#21487;&#33021;&#26159;&#38381;&#28304;&#27169;&#22411;&#30340;&#21512;&#36866;&#26367;&#20195;&#21697;&#12290;&#25105;&#20204;&#35814;&#32454;&#25551;&#36848;&#20102;&#25105;&#20204;&#22312;Llama 2-Chat&#30340;&#20248;&#21270;&#21644;&#23433;&#20840;&#24615;&#25913;&#36827;&#26041;&#38754;&#30340;&#26041;&#27861;&#65292;&#20197;&#20415;&#35753;&#31038;&#21306;&#33021;&#22815;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#22522;&#30784;&#19978;&#26500;&#24314;&#24182;&#20026;LLM&#30340;&#36127;&#36131;&#20219;&#21457;&#23637;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.
&lt;/p&gt;</description></item><item><title>Retentive Network&#65288;RetNet&#65289;&#20316;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#35757;&#32451;&#24182;&#34892;&#12289;&#20302;&#25104;&#26412;&#25512;&#29702;&#21644;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#24182;&#34892;&#12289;&#24490;&#29615;&#21644;&#20998;&#22359;&#24490;&#29615;&#19977;&#31181;&#35745;&#31639;&#33539;&#24335;&#65292;RetNet&#20855;&#26377;&#35757;&#32451;&#24182;&#34892;&#21270;&#12289;&#20302;&#25104;&#26412;&#25512;&#29702;&#21644;&#39640;&#25928;&#30340;&#38271;&#24207;&#21015;&#24314;&#27169;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2307.08621</link><description>&lt;p&gt;
Retentive Network: &#20316;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;Transformer&#30340;&#32487;&#20219;&#32773;
&lt;/p&gt;
&lt;p&gt;
Retentive Network: A Successor to Transformer for Large Language Models. (arXiv:2307.08621v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08621
&lt;/p&gt;
&lt;p&gt;
Retentive Network&#65288;RetNet&#65289;&#20316;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#35757;&#32451;&#24182;&#34892;&#12289;&#20302;&#25104;&#26412;&#25512;&#29702;&#21644;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#24182;&#34892;&#12289;&#24490;&#29615;&#21644;&#20998;&#22359;&#24490;&#29615;&#19977;&#31181;&#35745;&#31639;&#33539;&#24335;&#65292;RetNet&#20855;&#26377;&#35757;&#32451;&#24182;&#34892;&#21270;&#12289;&#20302;&#25104;&#26412;&#25512;&#29702;&#21644;&#39640;&#25928;&#30340;&#38271;&#24207;&#21015;&#24314;&#27169;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Retentive Network (RetNet)&#20316;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#26550;&#26500;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#35757;&#32451;&#24182;&#34892;&#12289;&#20302;&#25104;&#26412;&#25512;&#29702;&#21644;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#25512;&#23548;&#20986;&#20102;&#24490;&#29615;&#21644;&#27880;&#24847;&#21147;&#20043;&#38388;&#30340;&#36830;&#25509;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24207;&#21015;&#24314;&#27169;&#30340;&#20445;&#30041;&#26426;&#21046;&#65292;&#25903;&#25345;&#19977;&#31181;&#35745;&#31639;&#33539;&#24335;&#65292;&#21363;&#24182;&#34892;&#12289;&#24490;&#29615;&#21644;&#20998;&#22359;&#24490;&#29615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24182;&#34892;&#34920;&#31034;&#20801;&#35768;&#36827;&#34892;&#35757;&#32451;&#24182;&#34892;&#21270;&#12290;&#24490;&#29615;&#34920;&#31034;&#33021;&#22815;&#23454;&#29616;&#20302;&#25104;&#26412;&#30340;$O(1)$&#25512;&#29702;&#65292;&#20174;&#32780;&#25552;&#39640;&#35299;&#30721;&#21534;&#21520;&#37327;&#12289;&#24310;&#36831;&#21644;GPU&#20869;&#23384;&#65292;&#21516;&#26102;&#19981;&#25439;&#22833;&#24615;&#33021;&#12290;&#20998;&#22359;&#24490;&#29615;&#34920;&#31034;&#20415;&#20110;&#20351;&#29992;&#32447;&#24615;&#22797;&#26434;&#24230;&#36827;&#34892;&#39640;&#25928;&#30340;&#38271;&#24207;&#21015;&#24314;&#27169;&#65292;&#20854;&#20013;&#27599;&#20010;&#22359;&#21487;&#20197;&#24182;&#34892;&#32534;&#30721;&#65292;&#21516;&#26102;&#36827;&#34892;&#24490;&#29615;&#25688;&#35201;&#12290;&#35821;&#35328;&#24314;&#27169;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;RetNet&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#25193;&#23637;&#32467;&#26524;&#12289;&#24182;&#34892;&#35757;&#32451;&#12289;&#20302;&#25104;&#26412;&#37096;&#32626;&#21644;&#39640;&#25928;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose Retentive Network (RetNet) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference, and good performance. We theoretically derive the connection between recurrence and attention. Then we propose the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallel representation allows for training parallelism. The recurrent representation enables low-cost $O(1)$ inference, which improves decoding throughput, latency, and GPU memory without sacrificing performance. The chunkwise recurrent representation facilitates efficient long-sequence modeling with linear complexity, where each chunk is encoded parallelly while recurrently summarizing the chunks. Experimental results on language modeling show that RetNet achieves favorable scaling results, parallel training, low-cost deployment, and efficient i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#23558;&#25945;&#24072;&#27169;&#22411;&#30340;&#35270;&#35273;&#34920;&#31034;&#21387;&#32553;&#21040;&#23398;&#29983;&#27169;&#22411;&#20013;&#12290;&#30740;&#31350;&#37325;&#28857;&#22312;&#20110;&#36229;&#20986;&#20998;&#24067;&#21487;&#27867;&#21270;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#21407;&#21017;&#26469;&#22686;&#24378;&#23398;&#29983;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.03135</link><description>&lt;p&gt;
&#29992;&#20110;&#36229;&#20986;&#20998;&#24067;&#21487;&#27867;&#21270;&#24615;&#30340;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Distilling Large Vision-Language Model with Out-of-Distribution Generalizability. (arXiv:2307.03135v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03135
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#23558;&#25945;&#24072;&#27169;&#22411;&#30340;&#35270;&#35273;&#34920;&#31034;&#21387;&#32553;&#21040;&#23398;&#29983;&#27169;&#22411;&#20013;&#12290;&#30740;&#31350;&#37325;&#28857;&#22312;&#20110;&#36229;&#20986;&#20998;&#24067;&#21487;&#27867;&#21270;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#21407;&#21017;&#26469;&#22686;&#24378;&#23398;&#29983;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20294;&#20854;&#35268;&#27169;&#21644;&#35745;&#31639;&#35201;&#27714;&#20351;&#23427;&#20204;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#21644;&#26102;&#38388;&#25935;&#24863;&#20219;&#21153;&#19978;&#30340;&#37096;&#32626;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#12290;&#27169;&#22411;&#21387;&#32553;&#26159;&#21019;&#24314;&#26356;&#23567;&#12289;&#26356;&#24555;&#30340;&#27169;&#22411;&#20197;&#20445;&#25345;&#36739;&#22823;&#27169;&#22411;&#24615;&#33021;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35270;&#35273;&#34920;&#31034;&#21387;&#32553;&#21040;&#36731;&#37327;&#32423;&#23398;&#29983;&#27169;&#22411;&#20013;&#30340;&#36807;&#31243;&#65292;&#20351;&#29992;&#23567;&#22411;&#25110;&#20013;&#22411;&#25968;&#25454;&#38598;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#26412;&#30740;&#31350;&#20851;&#27880;&#30340;&#26159;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#21487;&#27867;&#21270;&#30340;&#24320;&#25918;&#35789;&#27719;&#38382;&#39064;&#65292;&#36825;&#22312;&#20197;&#24448;&#30340;&#27169;&#22411;&#21387;&#32553;&#30740;&#31350;&#20013;&#34987;&#24573;&#35270;&#20102;&#12290;&#25105;&#20204;&#20174;&#35270;&#35273;&#21644;&#35821;&#35328;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#20004;&#20010;&#21407;&#21017;&#26469;&#22686;&#24378;&#23398;&#29983;&#27169;&#22411;&#30340;OOD&#21487;&#27867;&#21270;&#24615;&#65306;&#65288;1&#65289;&#26356;&#22909;&#22320;&#27169;&#20223;&#25945;&#24072;&#30340;&#35270;&#35273;&#34920;&#31034;&#31354;&#38388;&#65292;&#24182;&#22312;&#35270;&#35273;&#35821;&#35328;&#23545;&#40784;&#26041;&#38754;&#35880;&#24910;&#22320;&#20419;&#36827;&#26356;&#22909;&#30340;&#19968;&#33268;&#24615;&#65307;&#65288;2&#65289;&#36890;&#36807;&#20016;&#23500;&#23398;&#29983;&#27169;&#22411;&#30340;&#33258;&#20030;&#23398;&#20064;&#21644;&#25968;&#25454;&#25193;&#20805;&#26469;&#25552;&#39640;OOD&#21487;&#27867;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large vision-language models have achieved outstanding performance, but their size and computational requirements make their deployment on resource-constrained devices and time-sensitive tasks impractical. Model distillation, the process of creating smaller, faster models that maintain the performance of larger models, is a promising direction towards the solution. This paper investigates the distillation of visual representations in large teacher vision-language models into lightweight student models using a smallor mid-scale dataset. Notably, this study focuses on open-vocabulary out-of-distribution (OOD) generalization, a challenging problem that has been overlooked in previous model distillation literature. We propose two principles from vision and language modality perspectives to enhance student's OOD generalization: (1) by better imitating teacher's visual representation space, and carefully promoting better coherence in vision-language alignment with the teacher; (2) by enric
&lt;/p&gt;</description></item><item><title>LongNet&#26159;&#19968;&#31181;&#21487;&#20197;&#25193;&#23637;&#21040;10&#20159;&#20010;&#26631;&#35760;&#30340;Transformer&#21464;&#20307;&#65292;&#36890;&#36807;&#25193;&#24352;&#27880;&#24847;&#21147;&#35299;&#20915;&#20102;&#24207;&#21015;&#38271;&#24230;&#21463;&#38480;&#30340;&#38382;&#39064;&#65292;&#20855;&#26377;&#32447;&#24615;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#23545;&#25968;&#20381;&#36182;&#20851;&#31995;&#65292;&#21487;&#20197;&#20316;&#20026;&#20998;&#24067;&#24335;&#35757;&#32451;&#22120;&#20351;&#29992;&#24182;&#26080;&#32541;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;Transformer&#20248;&#21270;&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;LongNet&#22312;&#38271;&#24207;&#21015;&#21644;&#30701;&#24207;&#21015;&#19978;&#24615;&#33021;&#24378;&#22823;&#12290;</title><link>http://arxiv.org/abs/2307.02486</link><description>&lt;p&gt;
LongNet: &#23558;Transformer&#25193;&#23637;&#21040;10&#20159;&#20010;&#26631;&#35760;
&lt;/p&gt;
&lt;p&gt;
LongNet: Scaling Transformers to 1,000,000,000 Tokens. (arXiv:2307.02486v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02486
&lt;/p&gt;
&lt;p&gt;
LongNet&#26159;&#19968;&#31181;&#21487;&#20197;&#25193;&#23637;&#21040;10&#20159;&#20010;&#26631;&#35760;&#30340;Transformer&#21464;&#20307;&#65292;&#36890;&#36807;&#25193;&#24352;&#27880;&#24847;&#21147;&#35299;&#20915;&#20102;&#24207;&#21015;&#38271;&#24230;&#21463;&#38480;&#30340;&#38382;&#39064;&#65292;&#20855;&#26377;&#32447;&#24615;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#23545;&#25968;&#20381;&#36182;&#20851;&#31995;&#65292;&#21487;&#20197;&#20316;&#20026;&#20998;&#24067;&#24335;&#35757;&#32451;&#22120;&#20351;&#29992;&#24182;&#26080;&#32541;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;Transformer&#20248;&#21270;&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;LongNet&#22312;&#38271;&#24207;&#21015;&#21644;&#30701;&#24207;&#21015;&#19978;&#24615;&#33021;&#24378;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#20195;&#65292;&#25193;&#23637;&#24207;&#21015;&#38271;&#24230;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#35745;&#31639;&#22797;&#26434;&#24230;&#25110;&#27169;&#22411;&#34920;&#36798;&#21147;&#19978;&#23384;&#22312;&#22256;&#38590;&#65292;&#23548;&#33268;&#24207;&#21015;&#38271;&#24230;&#21463;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;LongNet&#65292;&#23427;&#26159;&#19968;&#31181;Transformer&#30340;&#21464;&#20307;&#65292;&#21487;&#20197;&#23558;&#24207;&#21015;&#38271;&#24230;&#25193;&#23637;&#21040;10&#20159;&#20010;&#26631;&#35760;&#20197;&#19978;&#65292;&#32780;&#19981;&#20250;&#29306;&#29298;&#23545;&#36739;&#30701;&#24207;&#21015;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25193;&#24352;&#27880;&#24847;&#21147;&#65292;&#38543;&#30528;&#36317;&#31163;&#30340;&#22686;&#22823;&#65292;&#23427;&#23558;&#27880;&#24847;&#33539;&#22260;&#25351;&#25968;&#32423;&#25193;&#23637;&#12290;LongNet&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#65306;1&#65289;&#23427;&#20855;&#26377;&#32447;&#24615;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#24207;&#21015;&#20013;&#20219;&#24847;&#20004;&#20010;&#26631;&#35760;&#20043;&#38388;&#30340;&#23545;&#25968;&#20381;&#36182;&#20851;&#31995;&#65307;2&#65289;&#23427;&#21487;&#20197;&#20316;&#20026;&#29992;&#20110;&#26497;&#38271;&#24207;&#21015;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#22120;&#65307;3&#65289;&#23427;&#30340;&#25193;&#24352;&#27880;&#24847;&#21147;&#26159;&#26631;&#20934;&#27880;&#24847;&#21147;&#30340;&#21363;&#25554;&#21363;&#29992;&#26367;&#20195;&#21697;&#65292;&#21487;&#20197;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;Transformer&#30340;&#20248;&#21270;&#26080;&#32541;&#38598;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;LongNet&#22312;&#38271;&#24207;&#21015;&#21644;&#30701;&#24207;&#21015;&#19978;&#37117;&#20855;&#26377;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scaling sequence length has become a critical demand in the era of large language models. However, existing methods struggle with either computational complexity or model expressivity, rendering the maximum sequence length restricted. To address this issue, we introduce LongNet, a Transformer variant that can scale sequence length to more than 1 billion tokens, without sacrificing the performance on shorter sequences. Specifically, we propose dilated attention, which expands the attentive field exponentially as the distance grows. LongNet has significant advantages: 1) it has a linear computation complexity and a logarithm dependency between any two tokens in a sequence; 2) it can be served as a distributed trainer for extremely long sequences; 3) its dilated attention is a drop-in replacement for standard attention, which can be seamlessly integrated with the existing Transformer-based optimization. Experiments results demonstrate that LongNet yields strong performance on both long-se
&lt;/p&gt;</description></item><item><title>&#36845;&#20195;&#20998;&#27573;&#20223;&#23556;&#25554;&#20540;&#65288;IPA&#65289;&#36924;&#36817;&#27861;&#21487;&#20197;&#29992;&#20110;&#35821;&#35328;&#24314;&#27169;&#65292;&#19982;&#21464;&#21387;&#22120;&#35299;&#30721;&#22120;&#26550;&#26500;&#31867;&#20284;&#65292;&#24182;&#22312;&#20132;&#21449;&#29109;&#25439;&#22833;&#19979;&#30340;&#23567;&#24207;&#21015;&#38271;&#24230;&#19979;&#20248;&#20110;&#21464;&#21387;&#22120;1.5&#65285;&#12290;</title><link>http://arxiv.org/abs/2306.12317</link><description>&lt;p&gt;
&#36845;&#20195;&#20998;&#27573;&#20223;&#23556;&#25554;&#20540;&#65288;IPA&#65289;&#36924;&#36817;&#20110;&#35821;&#35328;&#24314;&#27169;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Iterated Piecewise Affine (IPA) Approximation for Language Modeling. (arXiv:2306.12317v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12317
&lt;/p&gt;
&lt;p&gt;
&#36845;&#20195;&#20998;&#27573;&#20223;&#23556;&#25554;&#20540;&#65288;IPA&#65289;&#36924;&#36817;&#27861;&#21487;&#20197;&#29992;&#20110;&#35821;&#35328;&#24314;&#27169;&#65292;&#19982;&#21464;&#21387;&#22120;&#35299;&#30721;&#22120;&#26550;&#26500;&#31867;&#20284;&#65292;&#24182;&#22312;&#20132;&#21449;&#29109;&#25439;&#22833;&#19979;&#30340;&#23567;&#24207;&#21015;&#38271;&#24230;&#19979;&#20248;&#20110;&#21464;&#21387;&#22120;1.5&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#19968;&#38454;&#27888;&#21202;&#23637;&#24320;&#27861;&#26469;&#36924;&#36817;&#19968;&#20010;&#36890;&#29992;&#30340;&#20989;&#25968;F: R^{n x m} -&gt; R^{n x m} &#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#35821;&#35328;&#24314;&#27169;&#12290;&#20026;&#20102;&#22686;&#24378;&#22522;&#26412;&#30340;&#27888;&#21202;&#23637;&#24320;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36845;&#20195;&#21644;&#20998;&#27573;&#24314;&#27169;&#65292;&#20174;&#32780;&#21629;&#21517;&#31639;&#27861;&#20026;&#36845;&#20195;&#20998;&#27573;&#20223;&#23556;&#25554;&#20540;&#65288;IPA&#65289;&#36924;&#36817;&#12290;&#26368;&#32456;&#31639;&#27861;&#34920;&#29616;&#20986;&#19982;&#21464;&#21387;&#22120;&#35299;&#30721;&#22120;&#26550;&#26500;&#30456;&#20284;&#30340;&#26377;&#36259;&#29305;&#24449;&#12290;&#36890;&#36807;&#27604;&#36739;IPA&#21644;&#21464;&#21387;&#22120;&#30340;&#21442;&#25968;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#36739;&#23567;&#30340;&#24207;&#21015;&#38271;&#24230;&#19979;&#65292;IPA&#22312;&#19979;&#19968;&#20010;&#20196;&#29260;&#39044;&#27979;&#20219;&#21153;&#20013;&#20351;&#29992;&#20132;&#21449;&#29109;&#25439;&#22833;&#27604;&#21464;&#21387;&#22120;&#39640;1.5&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we demonstrate the application of a simple first-order Taylor expansion to approximate a generic function $F: R^{n \times m} \to R^{n \times m}$ and utilize it in language modeling. To enhance the basic Taylor expansion, we introduce iteration and piecewise modeling, leading us to name the algorithm the Iterative Piecewise Affine (IPA) approximation. The final algorithm exhibits interesting resemblances to the Transformers decoder architecture. By comparing parameter arrangements in IPA and Transformers, we observe a strikingly similar performance, with IPA outperforming Transformers by 1.5\% in the next token prediction task with cross-entropy loss for smaller sequence lengths.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;GEmo-CLAP&#27169;&#22411;&#29992;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65292;&#32467;&#21512;&#20102;&#24615;&#21035;&#23646;&#24615;&#20449;&#24687;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#20808;&#36827;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#22312;IEMOCAP&#19978;&#23454;&#29616;&#20102;&#26356;&#20248;&#36234;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.07848</link><description>&lt;p&gt;
GEmo-CLAP: &#38754;&#21521;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#24615;&#21035;&#23646;&#24615;&#22686;&#24378;&#23545;&#27604;&#35821;&#38899;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GEmo-CLAP: Gender-Attribute-Enhanced Contrastive Language-Audio Pretraining for Speech Emotion Recognition. (arXiv:2306.07848v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;GEmo-CLAP&#27169;&#22411;&#29992;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65292;&#32467;&#21512;&#20102;&#24615;&#21035;&#23646;&#24615;&#20449;&#24687;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#20808;&#36827;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#22312;IEMOCAP&#19978;&#23454;&#29616;&#20102;&#26356;&#20248;&#36234;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#35821;&#38899;-&#35821;&#35328;&#39044;&#35757;&#32451;&#65288;CLAP&#65289;&#26368;&#36817;&#22312;&#19981;&#21516;&#39046;&#22495;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#25104;&#21151;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GEmo-CLAP&#30340;&#39640;&#25928;&#24615;&#21035;&#23646;&#24615;&#22686;&#24378;CLAP&#27169;&#22411;&#65292;&#29992;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65288;SER&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#21033;&#29992;&#21508;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#26500;&#24314;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#24773;&#24863;CLAP&#27169;&#22411;&#65288;&#31216;&#20026;Emo-CLAP&#65289;&#65292;&#29992;&#20110;SER&#12290;&#28982;&#21518;&#65292;&#32771;&#34385;&#21040;&#22312;&#35821;&#38899;&#24773;&#24863;&#24314;&#27169;&#20013;&#24615;&#21035;&#23646;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#20004;&#31181;GEmo-CLAP&#26041;&#27861;&#65292;&#26469;&#25972;&#21512;&#35821;&#38899;&#20449;&#21495;&#30340;&#24773;&#24863;&#21644;&#24615;&#21035;&#20449;&#24687;&#65292;&#24418;&#25104;&#26356;&#21512;&#29702;&#30340;&#30446;&#26631;&#12290;&#22312;IEMOCAP&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#20004;&#31181;GEmo-CLAP&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;&#22522;&#32447;Emo-CLAP&#27169;&#22411;&#65288;&#20351;&#29992;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65289;&#65292;&#21516;&#26102;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#23454;&#29616;&#20102;&#26356;&#20248;&#36234;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive Language-Audio Pretraining (CLAP) has recently exhibited impressive success in diverse fields. In this paper, we propose GEmo-CLAP, a kind of efficient gender-attribute-enhanced CLAP model for speech emotion recognition (SER). Specifically, we first build an effective emotion CLAP model termed Emo-CLAP for SER, utilizing various self-supervised learning based pre-trained models. Then, considering the importance of the gender attribute in speech emotion modeling, two GEmo-CLAP approaches are further proposed to integrate the emotion and gender information of speech signals, forming more reasonable objectives. Extensive experiments conducted on the IEMOCAP corpus demonstrate that our proposed two GEmo-CLAP approaches consistently outperform the baseline Emo-CLAP with different pre-trained models, while also achieving superior recognition performance compared with other state-of-the-art methods.
&lt;/p&gt;</description></item><item><title>ChatGPT&#22312;&#25991;&#26412;&#26631;&#27880;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#20247;&#21253;&#24037;&#20316;&#32773;&#65292;&#21253;&#25324;&#30456;&#20851;&#24615;&#12289;&#24577;&#24230;&#12289;&#20027;&#39064;&#21644;&#26694;&#26550;&#26816;&#27979;&#20219;&#21153;&#12290;ChatGPT&#30340;&#38646;&#26679;&#26412;&#20934;&#30830;&#29575;&#36229;&#36234;&#20247;&#21253;&#24037;&#20316;&#32773;&#22235;&#20010;&#20219;&#21153;&#65292;&#32534;&#30721;&#32773;&#38388;&#19968;&#33268;&#24615;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#22343;&#36229;&#36807;&#20247;&#21253;&#24037;&#20316;&#32773;&#21644;&#21463;&#36807;&#35757;&#32451;&#30340;&#27880;&#37322;&#32773;&#12290;&#27492;&#22806;&#65292;ChatGPT&#30340;&#26631;&#27880;&#25104;&#26412;&#27604;MTurk&#20415;&#23452;20&#20493;&#24038;&#21491;&#65292;&#26174;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#39640;&#25991;&#26412;&#20998;&#31867;&#25928;&#29575;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.15056</link><description>&lt;p&gt;
ChatGPT&#22312;&#25991;&#26412;&#26631;&#27880;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#20247;&#21253;&#24037;&#20316;&#32773;
&lt;/p&gt;
&lt;p&gt;
ChatGPT Outperforms Crowd-Workers for Text-Annotation Tasks. (arXiv:2303.15056v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15056
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#22312;&#25991;&#26412;&#26631;&#27880;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#20247;&#21253;&#24037;&#20316;&#32773;&#65292;&#21253;&#25324;&#30456;&#20851;&#24615;&#12289;&#24577;&#24230;&#12289;&#20027;&#39064;&#21644;&#26694;&#26550;&#26816;&#27979;&#20219;&#21153;&#12290;ChatGPT&#30340;&#38646;&#26679;&#26412;&#20934;&#30830;&#29575;&#36229;&#36234;&#20247;&#21253;&#24037;&#20316;&#32773;&#22235;&#20010;&#20219;&#21153;&#65292;&#32534;&#30721;&#32773;&#38388;&#19968;&#33268;&#24615;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#22343;&#36229;&#36807;&#20247;&#21253;&#24037;&#20316;&#32773;&#21644;&#21463;&#36807;&#35757;&#32451;&#30340;&#27880;&#37322;&#32773;&#12290;&#27492;&#22806;&#65292;ChatGPT&#30340;&#26631;&#27880;&#25104;&#26412;&#27604;MTurk&#20415;&#23452;20&#20493;&#24038;&#21491;&#65292;&#26174;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#39640;&#25991;&#26412;&#20998;&#31867;&#25928;&#29575;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#38656;&#35201;&#25163;&#21160;&#36827;&#34892;&#25968;&#25454;&#26631;&#27880;&#20197;&#36827;&#34892;&#21508;&#31181;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#29992;&#20110;&#35757;&#32451;&#20998;&#31867;&#22120;&#25110;&#35780;&#20272;&#26080;&#30417;&#30563;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#26681;&#25454;&#20219;&#21153;&#30340;&#35268;&#27169;&#21644;&#22797;&#26434;&#31243;&#24230;&#65292;&#36825;&#20123;&#20219;&#21153;&#21487;&#20197;&#30001;&#20247;&#21253;&#24037;&#20316;&#32773;&#22312;MTurk&#31561;&#24179;&#21488;&#19978;&#36827;&#34892;&#65292;&#20063;&#21487;&#20197;&#30001;&#21463;&#36807;&#35757;&#32451;&#30340;&#27880;&#37322;&#32773;&#65288;&#22914;&#30740;&#31350;&#21161;&#29702;&#65289;&#36827;&#34892;&#12290;&#36890;&#36807;&#20351;&#29992;2382&#26465;&#25512;&#25991;&#30340;&#26679;&#26412;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;ChatGPT&#22312;&#22810;&#20010;&#26631;&#27880;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20248;&#20110;&#20247;&#21253;&#24037;&#20316;&#32773;&#65292;&#21253;&#25324;&#30456;&#20851;&#24615;&#12289;&#24577;&#24230;&#12289;&#20027;&#39064;&#21644;&#26694;&#26550;&#26816;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;ChatGPT&#30340;&#38646;&#26679;&#26412;&#20934;&#30830;&#29575;&#22312;&#20116;&#20010;&#20219;&#21153;&#20013;&#26377;&#22235;&#20010;&#36229;&#36807;&#20102;&#20247;&#21253;&#24037;&#20316;&#32773;&#65292;&#32780;ChatGPT&#30340;&#32534;&#30721;&#32773;&#38388;&#19968;&#33268;&#24615;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#22343;&#36229;&#36807;&#20102;&#20247;&#21253;&#24037;&#20316;&#32773;&#21644;&#21463;&#36807;&#35757;&#32451;&#30340;&#27880;&#37322;&#32773;&#12290;&#27492;&#22806;&#65292;ChatGPT&#30340;&#26631;&#27880;&#25104;&#26412;&#19981;&#21040;0.003&#32654;&#20803;&#65292;&#27604;MTurk&#20415;&#23452;20&#20493;&#24038;&#21491;&#12290;&#36825;&#20123;&#32467;&#26524;&#26174;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#25991;&#26412;&#20998;&#31867;&#30340;&#25928;&#29575;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many NLP applications require manual data annotations for a variety of tasks, notably to train classifiers or evaluate the performance of unsupervised models. Depending on the size and degree of complexity, the tasks may be conducted by crowd-workers on platforms such as MTurk as well as trained annotators, such as research assistants. Using a sample of 2,382 tweets, we demonstrate that ChatGPT outperforms crowd-workers for several annotation tasks, including relevance, stance, topics, and frames detection. Specifically, the zero-shot accuracy of ChatGPT exceeds that of crowd-workers for four out of five tasks, while ChatGPT's intercoder agreement exceeds that of both crowd-workers and trained annotators for all tasks. Moreover, the per-annotation cost of ChatGPT is less than $0.003 -- about twenty times cheaper than MTurk. These results show the potential of large language models to drastically increase the efficiency of text classification.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545; SemEval-2023 &#20219;&#21153; 6 &#24320;&#21457;&#30340; Legal-BERT-HSLN &#27169;&#22411;&#21644; Legal-LUKE &#27169;&#22411;&#65292;&#20854;&#20013; Legal-BERT-HSLN &#27169;&#22411;&#36890;&#36807;&#32771;&#34385;&#21477;&#20869;&#21644;&#21477;&#38388;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#20197;&#39044;&#27979;&#20462;&#36766;&#35282;&#33394;&#65292;Legal-LUKE &#27169;&#22411;&#26159;&#20855;&#26377;&#27861;&#24459;&#19978;&#19979;&#25991;&#21644;&#23454;&#20307;&#30693;&#35782;&#30340;&#27169;&#22411;&#65292;&#20197;&#35782;&#21035;&#27861;&#24459;&#23454;&#20307;&#12290;&#27169;&#22411;&#30456;&#27604;&#22522;&#32447;&#27169;&#22411;&#26356;&#20934;&#30830;&#65292;&#33021;&#22815;&#35299;&#20915;&#22312;&#20154;&#21475;&#20247;&#22810;&#30340;&#22269;&#23478;&#22788;&#29702;&#27861;&#24459;&#25991;&#20214;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.12135</link><description>&lt;p&gt;
&#21033;&#29992;&#19978;&#19979;&#25991;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#27861;&#24459;&#25991;&#20214;
&lt;/p&gt;
&lt;p&gt;
Understand Legal Documents with Contextualized Large Language Models. (arXiv:2303.12135v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12135
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545; SemEval-2023 &#20219;&#21153; 6 &#24320;&#21457;&#30340; Legal-BERT-HSLN &#27169;&#22411;&#21644; Legal-LUKE &#27169;&#22411;&#65292;&#20854;&#20013; Legal-BERT-HSLN &#27169;&#22411;&#36890;&#36807;&#32771;&#34385;&#21477;&#20869;&#21644;&#21477;&#38388;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#20197;&#39044;&#27979;&#20462;&#36766;&#35282;&#33394;&#65292;Legal-LUKE &#27169;&#22411;&#26159;&#20855;&#26377;&#27861;&#24459;&#19978;&#19979;&#25991;&#21644;&#23454;&#20307;&#30693;&#35782;&#30340;&#27169;&#22411;&#65292;&#20197;&#35782;&#21035;&#27861;&#24459;&#23454;&#20307;&#12290;&#27169;&#22411;&#30456;&#27604;&#22522;&#32447;&#27169;&#22411;&#26356;&#20934;&#30830;&#65292;&#33021;&#22815;&#35299;&#20915;&#22312;&#20154;&#21475;&#20247;&#22810;&#30340;&#22269;&#23478;&#22788;&#29702;&#27861;&#24459;&#25991;&#20214;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#21475;&#20247;&#22810;&#30340;&#22269;&#23478;&#65292;&#22914;&#21360;&#24230;&#65292;&#24453;&#22788;&#29702;&#30340;&#27861;&#24459;&#26696;&#20214;&#25968;&#37327;&#19981;&#26029;&#22686;&#21152;&#65292;&#36825;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#22823;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#26377;&#25928;&#30340;&#25216;&#26415;&#26469;&#22788;&#29702;&#21644;&#29702;&#35299;&#27861;&#24459;&#25991;&#20214;&#23558;&#38750;&#24120;&#26377;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#25105;&#20204;&#38024;&#23545; SemEval-2023 &#20219;&#21153; 6&#65288;Modi &#31561;&#20154;&#65292;2023&#65289;&#25152;&#24320;&#21457;&#30340;&#29702;&#35299;&#27861;&#24459;&#25991;&#26412;&#31995;&#32479;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#24320;&#21457;&#20102; Legal-BERT-HSLN &#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#21477;&#20869;&#21644;&#21477;&#38388;&#30340;&#32508;&#21512;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20197;&#39044;&#27979;&#20462;&#36766;&#35282;&#33394;&#65288;&#23376;&#20219;&#21153; A&#65289;&#65292;&#28982;&#21518;&#35757;&#32451;&#20986; Legal-LUKE &#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#27861;&#24459;&#19978;&#19979;&#25991;&#21270;&#21644;&#23454;&#20307;&#24863;&#30693;&#33021;&#21147;&#65292;&#20197;&#35782;&#21035;&#27861;&#24459;&#23454;&#20307;&#65288;&#23376;&#20219;&#21153; B&#65289;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#35774;&#35745;&#30340;&#27169;&#22411;&#27604;&#22522;&#32447;&#27169;&#22411;&#26356;&#20934;&#30830;&#65292;&#22914;&#22312;&#23376;&#20219;&#21153; B &#20013; F1 &#20540;&#25552;&#39640;&#20102;&#36798; 15.0%&#12290;&#25105;&#20204;&#22312;&#20219;&#21153;&#25490;&#34892;&#27036;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#34920;&#29616;&#65292;&#22914; 0.834 &#24494;&#24179;&#22343; F1 &#20540;&#65292;&#24182;&#22312;&#23376;&#20219;&#21153; A &#20013;&#25490;&#21517;&#31532; 5&#12290;
&lt;/p&gt;
&lt;p&gt;
The growth of pending legal cases in populous countries, such as India, has become a major issue. Developing effective techniques to process and understand legal documents is extremely useful in resolving this problem. In this paper, we present our systems for SemEval-2023 Task 6: understanding legal texts (Modi et al., 2023). Specifically, we first develop the Legal-BERT-HSLN model that considers the comprehensive context information in both intra- and inter-sentence levels to predict rhetorical roles (subtask A) and then train a Legal-LUKE model, which is legal-contextualized and entity-aware, to recognize legal entities (subtask B). Our evaluations demonstrate that our designed models are more accurate than baselines, e.g., with an up to 15.0% better F1 score in subtask B. We achieved notable performance in the task leaderboard, e.g., 0.834 micro F1 score, and ranked No.5 out of 27 teams in subtask A.
&lt;/p&gt;</description></item><item><title>ThoughtSource&#26159;&#19968;&#20010;&#29992;&#20110;&#36830;&#32493;&#24605;&#32771;&#25512;&#29702;&#30340;&#20803;&#25968;&#25454;&#38598;&#21644;&#36719;&#20214;&#24211;&#65292;&#26088;&#22312;&#36890;&#36807;&#20419;&#36827;&#23545;&#36830;&#32493;&#24605;&#32771;&#30340;&#23450;&#24615;&#29702;&#35299;&#12289;&#23454;&#35777;&#35780;&#20272;&#21644;&#25552;&#20379;&#35757;&#32451;&#25968;&#25454;&#65292;&#25913;&#36827;&#26410;&#26469;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2301.11596</link><description>&lt;p&gt;
ThoughtSource:&#19968;&#20010;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#25968;&#25454;&#30340;&#20013;&#22830;&#26530;&#32445;&#12290;
&lt;/p&gt;
&lt;p&gt;
ThoughtSource: A central hub for large language model reasoning data. (arXiv:2301.11596v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11596
&lt;/p&gt;
&lt;p&gt;
ThoughtSource&#26159;&#19968;&#20010;&#29992;&#20110;&#36830;&#32493;&#24605;&#32771;&#25512;&#29702;&#30340;&#20803;&#25968;&#25454;&#38598;&#21644;&#36719;&#20214;&#24211;&#65292;&#26088;&#22312;&#36890;&#36807;&#20419;&#36827;&#23545;&#36830;&#32493;&#24605;&#32771;&#30340;&#23450;&#24615;&#29702;&#35299;&#12289;&#23454;&#35777;&#35780;&#20272;&#21644;&#25552;&#20379;&#35757;&#32451;&#25968;&#25454;&#65292;&#25913;&#36827;&#26410;&#26469;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20687;GPT-4&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#19978;&#20173;&#23384;&#22312;&#38480;&#21046;&#65292;&#23427;&#20204;&#30340;&#25512;&#29702;&#36807;&#31243;&#19981;&#36879;&#26126;&#65292;&#23481;&#26131;&#20135;&#29983;&#8220;&#24187;&#35273;&#8221;&#20107;&#23454;&#65292;&#24182;&#19988;&#23384;&#22312;&#20854;&#28508;&#22312;&#20559;&#35265;&#30340;&#25285;&#24551;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#36830;&#32493;&#24605;&#32771;&#25552;&#31034;&#30340;&#25216;&#26415;&#65292;&#35753;&#27169;&#22411;&#20197;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#34920;&#36798;&#25512;&#29702;&#27493;&#39588;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ThoughtSource&#65292;&#19968;&#20010;&#29992;&#20110;&#36830;&#32493;&#24605;&#32771;&#25512;&#29702;&#30340;&#20803;&#25968;&#25454;&#38598;&#21644;&#36719;&#20214;&#24211;&#12290;ThoughtSource&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20419;&#36827;&#23545;&#36830;&#32493;&#24605;&#32771;&#30340;&#23450;&#24615;&#29702;&#35299;&#12289;&#23454;&#35777;&#35780;&#20272;&#21644;&#25552;&#20379;&#35757;&#32451;&#25968;&#25454;&#26469;&#25913;&#36827;&#26410;&#26469;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;ThoughtSource&#30340;&#39318;&#27425;&#21457;&#24067;&#38598;&#25104;&#20102;&#20845;&#20010;&#31185;&#23398;/&#21307;&#23398;&#12289;&#19977;&#20010;&#36890;&#29992;&#39046;&#22495;&#21644;&#20116;&#20010;&#25968;&#23398;&#39064;&#31572;&#26696;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) such as GPT-4 have recently demonstrated impressive results across a wide range of tasks. LLMs are still limited, however, in that they frequently fail at complex reasoning, their reasoning processes are opaque, they are prone to 'hallucinate' facts, and there are concerns about their underlying biases. Letting models verbalize reasoning steps as natural language, a technique known as chain-of-thought prompting, has recently been proposed as a way to address some of these issues. Here we present ThoughtSource, a meta-dataset and software library for chain-of-thought (CoT) reasoning. The goal of ThoughtSource is to improve future artificial intelligence systems by facilitating qualitative understanding of CoTs, enabling empirical evaluations, and providing training data. This first release of ThoughtSource integrates six scientific/medical, three general-domain and five math word question answering datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25286;&#21368;&#30340;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#65292;Lego-MT&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#22810;&#35821;&#35328;&#21333;&#20307;&#27169;&#22411;&#22312;&#21442;&#25968;&#24178;&#25200;&#21644;&#20302;&#25928;&#25512;&#23548;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#36739;&#39640;&#30340;&#24615;&#33021;&#65292;&#30456;&#27604;&#20855;&#26377;10&#20493;&#35268;&#27169;&#30340;&#27169;&#22411;&#65292;&#22312;&#25928;&#29575;&#21644;&#34920;&#29616;&#26041;&#38754;&#37117;&#26356;&#20855;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2212.10551</link><description>&lt;p&gt;
Lego-MT: &#36208;&#21521;&#21487;&#25286;&#21368;&#30340;&#39640;&#24230;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Lego-MT: Towards Detachable Models in Massively Multilingual Machine Translation. (arXiv:2212.10551v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10551
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25286;&#21368;&#30340;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#65292;Lego-MT&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#22810;&#35821;&#35328;&#21333;&#20307;&#27169;&#22411;&#22312;&#21442;&#25968;&#24178;&#25200;&#21644;&#20302;&#25928;&#25512;&#23548;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#36739;&#39640;&#30340;&#24615;&#33021;&#65292;&#30456;&#27604;&#20855;&#26377;10&#20493;&#35268;&#27169;&#30340;&#27169;&#22411;&#65292;&#22312;&#25928;&#29575;&#21644;&#34920;&#29616;&#26041;&#38754;&#37117;&#26356;&#20855;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;(MNMT)&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#36866;&#29992;&#20110;&#22810;&#20010;&#35821;&#35328;&#26041;&#21521;&#30340;&#32479;&#19968;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;MNMT&#21333;&#20307;&#27169;&#22411;&#38754;&#20020;&#20004;&#20010;&#25361;&#25112;:&#35821;&#35328;&#20043;&#38388;&#30340;&#21442;&#25968;&#24178;&#25200;&#21644;&#22823;&#22411;&#27169;&#22411;&#30340;&#20302;&#25928;&#25512;&#29702;&#12290;&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#32463;&#20856;&#30340;&#22810;&#36335;&#24452;&#32467;&#26500;&#65292;&#36890;&#36807;&#23558;&#27599;&#31181;&#35821;&#35328;(&#25110;&#35821;&#35328;&#32452;)&#20998;&#37197;&#32473;&#25903;&#25345;&#21363;&#25554;&#21363;&#29992;&#35757;&#32451;&#21644;&#25512;&#29702;&#30340;&#21333;&#29420;&#20998;&#25903;&#65292;&#24320;&#21457;&#20986;&#21487;&#25286;&#21368;&#27169;&#22411;&#12290;&#20026;&#20102;&#28385;&#36275;&#22312;&#32479;&#19968;&#31354;&#38388;&#20013;&#20026;&#25152;&#26377;&#35821;&#35328;&#23398;&#20064;&#34920;&#31034;&#30340;&#38656;&#35201;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#25928;&#35757;&#32451;&#37197;&#26041;&#65292;&#20197;&#27492;&#26500;&#24314;&#19968;&#20010;&#26377;&#25928;&#30340;&#21487;&#25286;&#21368;&#27169;&#22411;&#65292;Lego-MT&#12290;&#20026;&#20102;&#36827;&#34892;&#20844;&#27491;&#30340;&#27604;&#36739;&#65292;&#25105;&#20204;&#20174;OPUS&#25910;&#38598;&#25968;&#25454;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#25324;433&#31181;&#35821;&#35328;&#21644;13&#20159;&#20010;&#24179;&#34892;&#25968;&#25454;&#30340;&#32763;&#35793;&#22522;&#20934;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#21442;&#25968;&#20026;12&#20159;&#30340;Lego-MT&#24102;&#26469;&#20102;3.2&#20010;spBLEU&#30340;&#24179;&#22343;&#22686;&#30410;&#12290;&#23427;&#29978;&#33267;&#32988;&#36807;&#20102;&#21442;&#25968;&#20026;120&#20159;&#30340;M2M-100&#12290;&#25152;&#25552;&#20986;&#30340;&#35757;&#32451;&#37197;&#26041;&#27604;&#24182;&#34892;&#35757;&#32451;&#25552;&#36895;&#20102;28.2&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual neural machine translation (MNMT) aims to build a unified model for many language directions. Existing monolithic models for MNMT encounter two challenges: parameter interference among languages and inefficient inference for large models. In this paper, we revisit the classic multi-way structures and develop a detachable model by assigning each language (or group of languages) to an individual branch that supports plug-and-play training and inference. To address the needs of learning representations for all languages in a unified space, we propose a novel efficient training recipe, upon which we build an effective detachable model, Lego-MT. For a fair comparison, we collect data from OPUS and build a translation benchmark covering 433 languages and 1.3B parallel data. Experiments show that Lego-MT with 1.2B parameters brings an average gain of 3.2 spBLEU. It even outperforms M2M-100 with 12B parameters. The proposed training recipe brings a 28.2$\times$ speedup over the co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#27010;&#24565;&#24615;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#24110;&#21161;&#22312;&#22330;&#23398;&#20064;&#32773;&#23398;&#20064;&#26032;&#25216;&#33021;&#12290;&#36890;&#36807;&#36873;&#25321;&#19982;&#39044;&#27979;&#31034;&#20363;&#20849;&#20139;&#21487;&#33021;&#20449;&#24687;&#30340;&#28436;&#31034;&#65292;&#36825;&#20010;&#26041;&#27861;&#21487;&#20197;&#22312;&#27169;&#22411;&#35760;&#24518;&#29420;&#31435;&#30340;&#24773;&#20917;&#19979;&#21306;&#20998;&#27169;&#22411;&#30340;&#22312;&#22330;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.01692</link><description>&lt;p&gt;
&#22312;&#22330;&#23398;&#20064;&#32773;&#33021;&#21542;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#25512;&#29702;&#27010;&#24565;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can In-context Learners Learn a Reasoning Concept from Demonstrations?. (arXiv:2212.01692v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01692
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#27010;&#24565;&#24615;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#24110;&#21161;&#22312;&#22330;&#23398;&#20064;&#32773;&#23398;&#20064;&#26032;&#25216;&#33021;&#12290;&#36890;&#36807;&#36873;&#25321;&#19982;&#39044;&#27979;&#31034;&#20363;&#20849;&#20139;&#21487;&#33021;&#20449;&#24687;&#30340;&#28436;&#31034;&#65292;&#36825;&#20010;&#26041;&#27861;&#21487;&#20197;&#22312;&#27169;&#22411;&#35760;&#24518;&#29420;&#31435;&#30340;&#24773;&#20917;&#19979;&#21306;&#20998;&#27169;&#22411;&#30340;&#22312;&#22330;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#20174;&#23569;&#37327;&#36755;&#20837;-&#36755;&#20986;&#28436;&#31034;&#20013;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#26032;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#22330;&#23398;&#20064;&#32773;&#22823;&#37096;&#20998;&#20381;&#36182;&#20110;&#20182;&#20204;&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#65292;&#22914;&#26631;&#31614;&#30340;&#24773;&#24863;&#65292;&#32780;&#19981;&#26159;&#22312;&#36755;&#20837;&#20013;&#25214;&#21040;&#26032;&#30340;&#20851;&#32852;&#24615;&#12290;&#28982;&#32780;&#65292;&#24120;&#29992;&#30340;&#23569;&#26679;&#26412;&#35780;&#20272;&#35774;&#32622;&#20351;&#29992;&#38543;&#26426;&#36873;&#25321;&#30340;&#22312;&#22330;&#28436;&#31034;&#26080;&#27861;&#21306;&#20998;&#27169;&#22411;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#26032;&#25216;&#33021;&#30340;&#33021;&#21147;&#65292;&#22240;&#20026;&#22823;&#37096;&#20998;&#38543;&#26426;&#36873;&#25321;&#30340;&#28436;&#31034;&#24182;&#19981;&#21576;&#29616;&#36229;&#36234;&#26292;&#38706;&#20110;&#26032;&#20219;&#21153;&#20998;&#24067;&#30340;&#39044;&#27979;&#30340;&#20851;&#31995;&#12290;&#20026;&#20102;&#22312;&#27169;&#22411;&#35760;&#24518;&#29420;&#31435;&#30340;&#24773;&#20917;&#19979;&#21306;&#20998;&#27169;&#22411;&#30340;&#22312;&#22330;&#23398;&#20064;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27010;&#24565;&#24615;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#36873;&#25321;&#19982;&#39044;&#27979;&#31034;&#20363;&#20849;&#20139;&#21487;&#33021;&#20449;&#24687;&#30340;&#28436;&#31034;&#12290;&#25105;&#20204;&#20174;&#27880;&#37322;&#35299;&#37322;&#20013;&#25552;&#21462;&#20102;&#19968;&#32452;&#36825;&#26679;&#30340;&#27010;&#24565;&#65292;&#24182;&#27979;&#37327;&#20102;&#27169;&#22411;&#23637;&#31034;&#36825;&#20123;&#27010;&#24565;&#21487;&#20197;&#33719;&#24471;&#22810;&#23569;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models show an emergent ability to learn a new task from a small number of input-output demonstrations. However, recent work shows that in-context learners largely rely on their pre-trained knowledge, such as the sentiment of the labels, instead of finding new associations in the input. However, the commonly-used few-shot evaluation settings using a random selection of in-context demonstrations can not disentangle models' ability to learn a new skill from demonstrations, as most of the randomly-selected demonstrations do not present relations informative for prediction beyond exposing the new task distribution.  To disentangle models' in-context learning ability independent of models' memory, we introduce a Conceptual few-shot learning method selecting the demonstrations sharing a possibly-informative concept with the predicted sample. We extract a set of such concepts from annotated explanations and measure how much can models benefit from presenting these concepts in f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;Softmax&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#36817;&#20284;&#26041;&#27861;&#65292;&#24182;&#27604;&#36739;&#20102;&#22522;&#20110;MC Dropout&#30340;&#26041;&#27861;&#12290;&#23454;&#35777;&#20998;&#26512;&#21457;&#29616;&#65292;&#23613;&#31649;MC dropout&#20135;&#29983;&#20102;&#26368;&#22909;&#30340;&#19981;&#30830;&#23450;&#24615;&#36817;&#20284;&#65292;&#20294;&#20351;&#29992;softmax&#20063;&#33021;&#20135;&#29983;&#30456;&#23545;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.14037</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;Softmax&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#36817;&#20284;
&lt;/p&gt;
&lt;p&gt;
Revisiting Softmax for Uncertainty Approximation in Text Classification. (arXiv:2210.14037v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14037
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;Softmax&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#36817;&#20284;&#26041;&#27861;&#65292;&#24182;&#27604;&#36739;&#20102;&#22522;&#20110;MC Dropout&#30340;&#26041;&#27861;&#12290;&#23454;&#35777;&#20998;&#26512;&#21457;&#29616;&#65292;&#23613;&#31649;MC dropout&#20135;&#29983;&#20102;&#26368;&#22909;&#30340;&#19981;&#30830;&#23450;&#24615;&#36817;&#20284;&#65292;&#20294;&#20351;&#29992;softmax&#20063;&#33021;&#20135;&#29983;&#30456;&#23545;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#36817;&#20284;&#26159;&#19968;&#20010;&#22312;&#39046;&#22495;&#36866;&#24212;&#21644;&#21487;&#35299;&#37322;&#24615;&#20013;&#24212;&#29992;&#24191;&#27867;&#30340;&#37325;&#35201;&#39046;&#22495;&#12290;&#20854;&#20013;&#19968;&#31181;&#26368;&#24120;&#29992;&#30340;&#19981;&#30830;&#23450;&#24615;&#36817;&#20284;&#26041;&#27861;&#26159;&#33945;&#29305;&#21345;&#32599;&#65288;MC&#65289;Dropout&#65292;&#20294;&#30001;&#20110;&#38656;&#35201;&#22810;&#27425;&#21069;&#21521;&#20256;&#36882;&#65292;&#35745;&#31639;&#25104;&#26412;&#36739;&#39640;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#19968;&#31181;&#26356;&#20415;&#23452;&#30340;&#26041;&#27861;&#26159;&#20165;&#20351;&#29992;&#22312;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#20013;&#22522;&#20110;softmax&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#31181;&#39044;&#27979;&#24448;&#24448;&#36807;&#20110;&#33258;&#20449;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;&#20004;&#31181;&#22522;&#26412;&#31070;&#32463;&#32467;&#26500;&#30340;&#20116;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24443;&#24213;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#26088;&#22312;&#25506;&#35752;&#36825;&#20004;&#31181;&#26041;&#27861;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;softmax&#21644;MC Dropout&#30340;&#19981;&#30830;&#23450;&#24615;&#36817;&#20284;&#20197;&#21450;&#19979;&#28216;&#25991;&#26412;&#20998;&#31867;&#24615;&#33021;&#65292;&#21516;&#26102;&#27604;&#36739;&#20102;&#23427;&#20204;&#30340;&#36816;&#34892;&#26102;&#38388;&#65288;&#25104;&#26412;&#65289;&#21644;&#24615;&#33021;&#65288;&#25928;&#30410;&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;MC dropout&#20135;&#29983;&#20102;&#26368;&#22909;&#30340;&#19981;&#30830;&#23450;&#24615;&#36817;&#20284;&#65292;&#20294;&#20351;&#29992;softmax&#20063;&#33021;&#20135;&#29983;&#30456;&#23545;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncertainty approximation in text classification is an important area with applications in domain adaptation and interpretability. One of the most widely used uncertainty approximation methods is Monte Carlo (MC) Dropout, which is computationally expensive as it requires multiple forward passes through the model. A cheaper alternative is to simply use the softmax based on a single forward pass without dropout to estimate model uncertainty. However, prior work has indicated that these predictions tend to be overconfident. In this paper, we perform a thorough empirical analysis of these methods on five datasets with two base neural architectures in order to identify the trade-offs between the two. We compare both softmax and an efficient version of MC Dropout on their uncertainty approximations and downstream text classification performance, while weighing their runtime (cost) against performance (benefit). We find that, while MC dropout produces the best uncertainty approximations, usin
&lt;/p&gt;</description></item></channel></rss>