<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;VisualGPTScore&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;&#29992;&#22810;&#27169;&#24577;&#29983;&#25104;&#20998;&#25968;&#25429;&#25417;&#25991;&#26412;&#26631;&#39064;&#21487;&#33021;&#24615;&#65292;&#24182;&#22312;&#22270;&#20687;&#26465;&#20214;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#35745;&#31639;&#65292;&#20855;&#22791;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.01879</link><description>&lt;p&gt;
VisualGPTScore: &#22810;&#27169;&#24577;&#29983;&#25104;&#39044;&#35757;&#32451;&#20998;&#25968;&#30340;&#35270;&#35273;&#35821;&#20041;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
VisualGPTScore: Visio-Linguistic Reasoning with Multimodal Generative Pre-Training Scores. (arXiv:2306.01879v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01879
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;VisualGPTScore&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;&#29992;&#22810;&#27169;&#24577;&#29983;&#25104;&#20998;&#25968;&#25429;&#25417;&#25991;&#26412;&#26631;&#39064;&#21487;&#33021;&#24615;&#65292;&#24182;&#22312;&#22270;&#20687;&#26465;&#20214;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#35745;&#31639;&#65292;&#20855;&#22791;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; VisualGPTScore &#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#27169;&#24577;&#29983;&#25104;&#20998;&#25968;&#26469;&#25429;&#25417;&#25991;&#26412;&#26631;&#39064;&#21487;&#33021;&#24615;&#65292;&#24182;&#20351;&#29992;&#22270;&#20687;&#26465;&#20214;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#20687;&#19978;&#36816;&#31639;&#12290;&#19982;&#20256;&#32479;&#35266;&#28857;&#35748;&#20026;&#30340;VLM&#21482;&#26159;&#26080;&#24847;&#20041;&#30340;&#21333;&#35789;&#34955;&#27169;&#22411;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340; VisualGPTScore &#22312; ARO &#21644; Crepe &#31561;&#26368;&#36817;&#25552;&#20986;&#30340;&#22270;&#20687;&#25991;&#26412;&#26816;&#32034;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#29616;&#20102;&#39030;&#23574;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#20854;&#20855;&#22791;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-language models (VLMs) discriminatively pre-trained with contrastive image-text matching losses such as $P(\text{match}|\text{text}, \text{image})$ have been criticized for lacking compositional understanding. This means they might output similar scores even if the original caption is rearranged into a different semantic statement. To address this, we propose to use the ${\bf V}$isual ${\bf G}$enerative ${\bf P}$re-${\bf T}$raining Score (${\bf VisualGPTScore}$) of $P(\text{text}|\text{image})$, a $\textit{multimodal generative}$ score that captures the likelihood of a text caption conditioned on an image using an image-conditioned language model. Contrary to the belief that VLMs are mere bag-of-words models, our off-the-shelf VisualGPTScore demonstrates top-tier performance on recently proposed image-text retrieval benchmarks like ARO and Crepe that assess compositional reasoning. Furthermore, we factorize VisualGPTScore into a product of the $\textit{marginal}$ P(text) and the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#33521;&#25991;&#35821;&#35328;&#27169;&#22411;&#23545;&#22810;&#25991;&#21270;&#32972;&#26223;&#19979;&#36947;&#24503;&#35268;&#33539;&#30340;&#29702;&#35299;&#31243;&#24230;&#65292;&#20998;&#26512;&#20102;&#27169;&#22411;&#23545;&#36947;&#24503;&#21464;&#21270;&#30340;&#25226;&#25569;&#33021;&#21147;&#21644;&#36947;&#24503;&#22810;&#26679;&#24615;&#20197;&#21450;&#20849;&#21516;&#20542;&#21521;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;&#33521;&#25991;&#35821;&#35328;&#27169;&#22411;&#23545;&#36328;&#22269;&#23478;&#30340;&#36947;&#24503;&#35268;&#33539;&#39044;&#27979;&#19981;&#22914;&#23545;&#33521;&#22269;&#36947;&#24503;&#35268;&#33539;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2306.01857</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#25991;&#21270;&#36947;&#24503;&#35268;&#33539;&#30340;&#20102;&#35299;
&lt;/p&gt;
&lt;p&gt;
Knowledge of cultural moral norms in large language models. (arXiv:2306.01857v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01857
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#33521;&#25991;&#35821;&#35328;&#27169;&#22411;&#23545;&#22810;&#25991;&#21270;&#32972;&#26223;&#19979;&#36947;&#24503;&#35268;&#33539;&#30340;&#29702;&#35299;&#31243;&#24230;&#65292;&#20998;&#26512;&#20102;&#27169;&#22411;&#23545;&#36947;&#24503;&#21464;&#21270;&#30340;&#25226;&#25569;&#33021;&#21147;&#21644;&#36947;&#24503;&#22810;&#26679;&#24615;&#20197;&#21450;&#20849;&#21516;&#20542;&#21521;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;&#33521;&#25991;&#35821;&#35328;&#27169;&#22411;&#23545;&#36328;&#22269;&#23478;&#30340;&#36947;&#24503;&#35268;&#33539;&#39044;&#27979;&#19981;&#22914;&#23545;&#33521;&#22269;&#36947;&#24503;&#35268;&#33539;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36947;&#24503;&#35268;&#33539;&#22240;&#25991;&#21270;&#32780;&#24322;&#12290;&#26368;&#36817;&#30340;&#19968;&#39033;&#24037;&#20316;&#34920;&#26126;&#65292;&#33521;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21253;&#21547;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#36947;&#24503;&#20559;&#35265;&#65292;&#20294;&#36825;&#20123;&#30740;&#31350;&#36890;&#24120;&#19981;&#22312;&#19968;&#20010;&#22810;&#20803;&#25991;&#21270;&#32972;&#26223;&#19979;&#30740;&#31350;&#36947;&#24503;&#21464;&#21270;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#21482;&#20250;&#33521;&#25991;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#19981;&#21516;&#22269;&#23478;&#30340;&#36947;&#24503;&#35268;&#33539;&#20102;&#35299;&#31243;&#24230;&#12290;&#25105;&#20204;&#32771;&#34385;&#20004;&#20010;&#23618;&#38754;&#65306;1&#65289;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#23545;&#22810;&#20010;&#20027;&#39064;&#22914;&#8220;&#21516;&#24615;&#24651;&#8221;&#21644;&#8220;&#31163;&#23130;&#8221;&#31561;&#22312;&#21508;&#20010;&#22269;&#23478;&#20043;&#38388;&#30340;&#32454;&#31890;&#24230;&#36947;&#24503;&#21464;&#21270;&#26377;&#25152;&#20102;&#35299;&#65307;2&#65289;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#28085;&#30422;&#20102;&#25991;&#21270;&#22810;&#26679;&#24615;&#21644;&#20849;&#21516;&#20542;&#21521;&#65292;&#21363;&#22312;&#21738;&#20123;&#20027;&#39064;&#19978;&#20154;&#20204;&#22312;&#36947;&#24503;&#21028;&#26029;&#19978;&#23384;&#22312;&#20998;&#27495;&#25110;&#20849;&#35782;&#12290;&#25105;&#20204;&#20351;&#29992;&#19990;&#30028;&#20215;&#20540;&#35266;&#35843;&#26597;&#65288;&#36328;55&#20010;&#22269;&#23478;&#65289;&#21644;PEW&#20840;&#29699;&#35843;&#26597;&#65288;&#36328;40&#20010;&#22269;&#23478;&#65289;&#30340;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#36827;&#34892;&#20998;&#26512;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#39044;&#35757;&#32451;&#30340;&#33521;&#25991;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;&#36328;&#22269;&#23478;&#30340;&#23454;&#35777;&#36947;&#24503;&#35268;&#33539;&#26041;&#38754;&#30340;&#34920;&#29616;&#19981;&#21450;&#39044;&#27979;&#33521;&#22269;&#36947;&#24503;&#35268;&#33539;&#12290;
&lt;/p&gt;
&lt;p&gt;
Moral norms vary across cultures. A recent line of work suggests that English large language models contain human-like moral biases, but these studies typically do not examine moral variation in a diverse cultural setting. We investigate the extent to which monolingual English language models contain knowledge about moral norms in different countries. We consider two levels of analysis: 1) whether language models capture fine-grained moral variation across countries over a variety of topics such as ``homosexuality'' and ``divorce''; 2) whether language models capture cultural diversity and shared tendencies in which topics people around the globe tend to diverge or agree on in their moral judgment. We perform our analyses with two public datasets from the World Values Survey (across 55 countries) and PEW global surveys (across 40 countries) on morality. We find that pre-trained English language models predict empirical moral norms across countries worse than the English moral norms rep
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38750;&#33258;&#22238;&#24402;&#30340;&#26597;&#35810;&#37325;&#20889;&#20307;&#31995;&#32467;&#26500;&#65292;&#35813;&#20307;&#31995;&#19981;&#20165;&#21487;&#20197;&#22788;&#29702;&#23545;&#35805;&#24341;&#23548;&#12289;&#24847;&#22270;&#25658;&#24102;&#12289;&#35821;&#27861;&#20013;&#26029;&#12289;&#23454;&#20307;&#25658;&#24102;&#21644;&#20462;&#22797;&#36825;&#20116;&#20010;&#20219;&#21153;&#65292;&#36824;&#21487;&#20197;&#22788;&#29702;&#23427;&#20204;&#30340;&#22797;&#26434;&#32452;&#21512;&#12290;</title><link>http://arxiv.org/abs/2306.01855</link><description>&lt;p&gt;
5IDER: &#32479;&#19968;&#26597;&#35810;&#37325;&#20889;&#25216;&#26415;&#29992;&#20110;&#23545;&#35805;&#24341;&#23548;&#12289;&#24847;&#22270;&#25658;&#24102;&#12289;&#35821;&#35328;&#20013;&#26029;&#12289;&#23454;&#20307;&#25658;&#24102;&#21450;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
5IDER: Unified Query Rewriting for Steering, Intent Carryover, Disfluencies, Entity Carryover and Repair. (arXiv:2306.01855v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01855
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38750;&#33258;&#22238;&#24402;&#30340;&#26597;&#35810;&#37325;&#20889;&#20307;&#31995;&#32467;&#26500;&#65292;&#35813;&#20307;&#31995;&#19981;&#20165;&#21487;&#20197;&#22788;&#29702;&#23545;&#35805;&#24341;&#23548;&#12289;&#24847;&#22270;&#25658;&#24102;&#12289;&#35821;&#27861;&#20013;&#26029;&#12289;&#23454;&#20307;&#25658;&#24102;&#21644;&#20462;&#22797;&#36825;&#20116;&#20010;&#20219;&#21153;&#65292;&#36824;&#21487;&#20197;&#22788;&#29702;&#23427;&#20204;&#30340;&#22797;&#26434;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20379;&#35821;&#38899;&#21161;&#25163;&#23548;&#33322;&#22810;&#36718;&#23545;&#35805;&#30340;&#33021;&#21147;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290; &#22788;&#29702;&#22810;&#36718;&#20114;&#21160;&#38656;&#35201;&#31995;&#32479;&#29702;&#35299;&#21508;&#31181;&#20250;&#35805;&#29992;&#20363;&#65292;&#22914;&#23545;&#35805;&#24341;&#23548;&#12289;&#24847;&#22270;&#25658;&#24102;&#12289;&#35821;&#35328;&#20013;&#26029;&#12289;&#23454;&#20307;&#25658;&#24102;&#21644;&#20462;&#22797;&#12290; &#36825;&#20010;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#21152;&#21095;&#20102;&#36825;&#20123;&#29992;&#20363;&#28151;&#21512;&#22312;&#19968;&#36215;&#30340;&#20107;&#23454;&#65292;&#36890;&#24120;&#21516;&#26102;&#22312;&#33258;&#28982;&#35821;&#35328;&#20013;&#20986;&#29616;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#33258;&#22238;&#24402;&#30340;&#26597;&#35810;&#37325;&#20889;&#20307;&#31995;&#32467;&#26500;&#65292;&#21487;&#20197;&#22788;&#29702;&#20116;&#20010;&#20219;&#21153;&#20197;&#21450;&#36825;&#20123;&#29992;&#20363;&#30340;&#22797;&#26434;&#32452;&#21512;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#19982;&#22522;&#32447;&#26041;&#27861;&#20855;&#26377;&#30456;&#24403;&#30340;&#21333;&#20219;&#21153;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#29992;&#20363;&#32452;&#21512;&#26041;&#38754;&#29978;&#33267;&#20248;&#20110;&#32463;&#36807;&#35843;&#20248;&#30340;T5&#27169;&#22411;&#65292;&#23613;&#31649;&#22312;&#21442;&#25968;&#19978;&#23567;15&#20493;&#65292;&#22312;&#24310;&#36831;&#19978;&#24555;25&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Providing voice assistants the ability to navigate multi-turn conversations is a challenging problem. Handling multi-turn interactions requires the system to understand various conversational use-cases, such as steering, intent carryover, disfluencies, entity carryover, and repair. The complexity of this problem is compounded by the fact that these use-cases mix with each other, often appearing simultaneously in natural language. This work proposes a non-autoregressive query rewriting architecture that can handle not only the five aforementioned tasks, but also complex compositions of these use-cases. We show that our proposed model has competitive single task performance compared to the baseline approach, and even outperforms a fine-tuned T5 model in use-case compositions, despite being 15 times smaller in parameters and 25 times faster in latency.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#28151;&#21512;&#26041;&#27861;&#30340;&#20108;&#36827;&#21046;&#21644;&#19977;&#36827;&#21046;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#22312;&#25688;&#35201;&#21644;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#25928;&#26524;&#65292;&#24182;&#22312;&#25928;&#29575;&#19978;&#33719;&#24471;&#20102;&#22823;&#24133;&#24230;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2306.01841</link><description>&lt;p&gt;
&#20108;&#36827;&#21046;&#21644;&#19977;&#36827;&#21046;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Binary and Ternary Natural Language Generation. (arXiv:2306.01841v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#28151;&#21512;&#26041;&#27861;&#30340;&#20108;&#36827;&#21046;&#21644;&#19977;&#36827;&#21046;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#22312;&#25688;&#35201;&#21644;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#25928;&#26524;&#65292;&#24182;&#22312;&#25928;&#29575;&#19978;&#33719;&#24471;&#20102;&#22823;&#24133;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20108;&#36827;&#21046;&#21644;&#19977;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#26080;&#38656;&#20056;&#27861;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#22914;&#26524;&#22312;&#19987;&#29992;&#30828;&#20214;&#19978;&#23454;&#29616;&#65292;&#21017;&#21487;&#20197;&#27604;&#20840;&#31934;&#24230;&#32593;&#32476;&#24102;&#26469;&#25968;&#20010;&#25968;&#37327;&#32423;&#30340;&#25928;&#29575;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21442;&#25968;&#21644;&#36755;&#20986;&#31354;&#38388;&#37117;&#39640;&#24230;&#31163;&#25955;&#21270;&#65292;&#36825;&#31181;&#32593;&#32476;&#24456;&#38590;&#36827;&#34892;&#20248;&#21270;&#12290;&#23545;&#20110;&#21464;&#21387;&#22120;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#36825;&#19968;&#31867;&#65292;&#30001;&#20110;&#27880;&#24847;&#21147;&#36816;&#31639;&#23545;&#37327;&#21270;&#30340;&#25935;&#24863;&#24615;&#20197;&#21450;&#33258;&#22238;&#24402;&#35299;&#30721;&#22312;&#39640;&#22522;&#25968;&#36755;&#20986;&#31354;&#38388;&#30340;&#22122;&#22768;&#21472;&#21152;&#25928;&#24212;&#65292;&#36825;&#20123;&#22256;&#38590;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#12290;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#32479;&#35745;&#30340;&#26435;&#37325;&#37327;&#21270;&#21644;&#28608;&#27963;&#24377;&#24615;&#37327;&#21270;&#30340;&#28151;&#21512;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#22312;&#25688;&#35201;&#21644;&#26426;&#22120;&#32763;&#35793;&#30340;&#19979;&#28216;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#31532;&#19968;&#20010;&#19977;&#36827;&#21046;&#21644;&#20108;&#36827;&#21046;&#21464;&#21387;&#22120;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#19977;&#36827;&#21046;BART&#22522;&#26412;&#27169;&#22411;&#22312;CNN/DailyMail&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;41&#30340;R1&#24471;&#20998;&#65292;&#20165;&#27604;&#20840;&#27169;&#22411;&#20302;3.9&#20998;&#65292;&#21516;&#26102;&#25928;&#29575;&#25552;&#21319;&#20102;16&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ternary and binary neural networks enable multiplication-free computation and promise multiple orders of magnitude efficiency gains over full-precision networks if implemented on specialized hardware. However, since both the parameter and the output space are highly discretized, such networks have proven very difficult to optimize. The difficulties are compounded for the class of transformer text generation models due to the sensitivity of the attention operation to quantization and the noise-compounding effects of autoregressive decoding in the high-cardinality output space. We approach the problem with a mix of statistics-based quantization for the weights and elastic quantization of the activations and demonstrate the first ternary and binary transformer models on the downstream tasks of summarization and machine translation. Our ternary BART base achieves an R1 score of 41 on the CNN/DailyMail benchmark, which is merely 3.9 points behind the full model while being 16x more efficien
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#24555;&#36895;&#12289;&#20415;&#23452;&#12289;&#26080;&#38656;&#31227;&#21160;&#35774;&#22791;&#22320;&#26816;&#27979;Beta&#22320;&#20013;&#28023;&#36139;&#34880;&#25658;&#24102;&#32773;&#12290;</title><link>http://arxiv.org/abs/2306.01818</link><description>&lt;p&gt;
Beta&#22320;&#20013;&#28023;&#36139;&#34880;&#25658;&#24102;&#32773;&#26816;&#27979;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Beta Thalassemia Carriers detection empowered federated Learning. (arXiv:2306.01818v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#24555;&#36895;&#12289;&#20415;&#23452;&#12289;&#26080;&#38656;&#31227;&#21160;&#35774;&#22791;&#22320;&#26816;&#27979;Beta&#22320;&#20013;&#28023;&#36139;&#34880;&#25658;&#24102;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#20013;&#28023;&#36139;&#34880;&#26159;&#19968;&#32452;&#36951;&#20256;&#24615;&#34880;&#28082;&#30142;&#30149;&#65292;&#24403;&#25658;&#24102;&#36755;&#27687;&#33267;&#36523;&#20307;&#21508;&#22788;&#30340;&#32418;&#32454;&#32990;&#20013;&#30340;&#34507;&#30333;&#36136;&#34880;&#32418;&#34507;&#30333;&#19981;&#36275;&#26102;&#20250;&#21457;&#29983;&#12290;&#22914;&#26524;&#29238;&#27597;&#21452;&#26041;&#37117;&#25658;&#24102;&#22320;&#20013;&#28023;&#36139;&#34880;&#22522;&#22240;&#65292;&#23401;&#23376;&#24739;&#30149;&#30340;&#20960;&#29575;&#20250;&#22686;&#21152;&#12290;&#30830;&#35786;&#21644;&#27835;&#30103;&#22320;&#20013;&#28023;&#36139;&#34880;&#26159;&#38450;&#27490;&#20854;&#20256;&#36882;&#32473;&#19979;&#19968;&#20195;&#30340;&#20851;&#38190;&#12290;&#30446;&#21069;&#30340;&#34880;&#28082;&#26816;&#27979;&#26041;&#27861;&#23545;&#20110;&#26816;&#27979;Beta&#22320;&#20013;&#28023;&#36139;&#34880;&#25658;&#24102;&#32773;&#36807;&#20110;&#26114;&#36149;&#12289;&#32791;&#26102;&#65292;&#19988;&#38656;&#35201;&#22823;&#37327;&#30340;&#31579;&#26597;&#35774;&#22791;&#12290;&#39640;&#25928;&#28082;&#30456;&#33394;&#35889;&#26159;&#26631;&#20934;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#20294;&#20063;&#23384;&#22312;&#25104;&#26412;&#39640;&#12289;&#26102;&#38388;&#38271;&#12289;&#38656;&#35201;&#29305;&#27530;&#35774;&#22791;&#31561;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#23547;&#25214;&#19968;&#31181;&#24555;&#36895;&#12289;&#20415;&#23452;&#30340;&#26816;&#27979;Beta&#22320;&#20013;&#28023;&#36139;&#34880;&#25658;&#24102;&#32773;&#30340;&#26041;&#27861;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Thalassemia is a group of inherited blood disorders that happen when hemoglobin, the protein in red blood cells that carries oxygen, is not made enough. It is found all over the body and is needed for survival. If both parents have thalassemia, a child's chance of getting it increases. Genetic counselling and early diagnosis are essential for treating thalassemia and stopping it from being passed on to future generations. It may be hard for healthcare professionals to differentiate between people with thalassemia carriers and those without. The current blood tests for beta thalassemia carriers are too expensive, take too long, and require too much screening equipment. The World Health Organization says there is a high death rate for people with thalassemia. Therefore, it is essential to find thalassemia carriers to act quickly. High-performance liquid chromatography (HPLC), the standard test method, has problems such as cost, time, and equipment needs. So, there must be a quick and che
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38754;&#21521;&#38134;&#34892;&#19994;&#30340;&#35789;&#23884;&#20837;&#65292;&#21457;&#29616;&#20854;&#27604;&#24191;&#27867;&#21487;&#29992;&#30340;&#35789;&#23884;&#20837;&#26356;&#33021;&#25429;&#25417;&#38134;&#34892;&#29305;&#23450;&#35821;&#20041;&#21644;&#35789;&#30456;&#20851;&#24615;&#65292;&#22240;&#27492;&#21487;&#20316;&#20026;NL&#20219;&#21153;&#30340;&#19968;&#20010;&#33391;&#22909;&#29420;&#31435;&#26469;&#28304;&#25110;&#32773;&#34917;&#20805;&#12290;</title><link>http://arxiv.org/abs/2306.01807</link><description>&lt;p&gt;
&#38754;&#21521;&#38134;&#34892;&#19994;&#30340;&#35789;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Word Embeddings for Banking Industry. (arXiv:2306.01807v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38754;&#21521;&#38134;&#34892;&#19994;&#30340;&#35789;&#23884;&#20837;&#65292;&#21457;&#29616;&#20854;&#27604;&#24191;&#27867;&#21487;&#29992;&#30340;&#35789;&#23884;&#20837;&#26356;&#33021;&#25429;&#25417;&#38134;&#34892;&#29305;&#23450;&#35821;&#20041;&#21644;&#35789;&#30456;&#20851;&#24615;&#65292;&#22240;&#27492;&#21487;&#20316;&#20026;NL&#20219;&#21153;&#30340;&#19968;&#20010;&#33391;&#22909;&#29420;&#31435;&#26469;&#28304;&#25110;&#32773;&#34917;&#20805;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24212;&#29992;&#24191;&#27867;&#65292;&#20174;&#24773;&#24863;&#20998;&#26512;&#21040;&#25991;&#26412;&#20998;&#31867;&#12290;&#20174;&#38745;&#24577;&#35789;&#23884;&#20837;&#65288;&#22914;Word2Vec&#25110;GloVe&#65289;&#21040;&#19978;&#19979;&#25991;&#27169;&#22411;&#25552;&#21462;&#30340;&#38745;&#24577;&#35789;&#34920;&#24449;&#65288;&#22914;BERT&#25110;ELMo&#65289;&#65292;&#20174;&#36825;&#20123;NLP&#20219;&#21153;&#20013;&#65292;&#20174;&#19994;&#20154;&#21592;&#24448;&#24448;&#20381;&#36182;&#20110;&#36825;&#20123;&#24191;&#27867;&#21487;&#29992;&#30340;&#35789;&#23884;&#20837;&#12290;&#36825;&#20123;&#35789;&#23884;&#20837;&#26159;&#20174;&#22823;&#37327;&#25991;&#26412;&#26500;&#24314;&#32780;&#25104;&#65292;&#22240;&#27492;&#21487;&#33021;&#24050;&#32463;&#25429;&#25417;&#20102;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#22823;&#37096;&#20998;&#35789;&#27719;&#12290;&#20294;&#26159;&#65292;&#23427;&#20204;&#33021;&#22810;&#22909;&#22320;&#25429;&#25417;&#29305;&#23450;&#39046;&#22495;&#30340;&#35821;&#20041;&#21644;&#35789;&#30456;&#20851;&#24615;&#65311;&#26412;&#25991;&#36890;&#36807;&#21019;&#24314;&#19968;&#31181;&#38134;&#34892;&#29305;&#23450;&#30340;&#35789;&#23884;&#20837;&#24182;&#23558;&#20854;&#19982;&#20854;&#20182;&#26469;&#28304;&#30340;&#35789;&#23884;&#20837;&#65288;&#22914;GloVe&#21644;BERT&#65289;&#36827;&#34892;&#35780;&#20272;&#65292;&#25506;&#35752;&#20102;&#36825;&#20010;&#24819;&#27861;&#12290;&#19981;&#20986;&#25152;&#26009;&#65292;&#20174;&#38134;&#34892;&#29305;&#23450;&#35821;&#26009;&#24211;&#26500;&#24314;&#30340;&#23884;&#20837;&#26356;&#33021;&#25429;&#25417;&#38134;&#34892;&#29305;&#23450;&#35821;&#20041;&#21644;&#35789;&#30456;&#20851;&#24615;&#12290;&#35813;&#21457;&#29616;&#34920;&#26126;&#65292;&#38754;&#21521;&#38134;&#34892;&#19994;&#30340;&#35789;&#23884;&#20837;&#22312;&#25191;&#34892;NL&#20219;&#21153;&#26102;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#33391;&#22909;&#30340;&#29420;&#31435;&#26469;&#28304;&#25110;&#32773;&#20316;&#20026;&#20854;&#20182;&#21487;&#24191;&#27867;&#33719;&#24471;&#30340;&#23884;&#20837;&#30340;&#34917;&#20805;&#12290;
&lt;/p&gt;
&lt;p&gt;
Applications of Natural Language Processing (NLP) are plentiful, from sentiment analysis to text classification. Practitioners rely on static word embeddings (e.g. Word2Vec or GloVe) or static word representation from contextual models (e.g. BERT or ELMo) to perform many of these NLP tasks. These widely available word embeddings are built from large amount of text, so they are likely to have captured most of the vocabulary in different context. However, how well would they capture domain-specific semantics and word relatedness? This paper explores this idea by creating a bank-specific word embeddings and evaluates them against other sources of word embeddings such as GloVe and BERT. Not surprising that embeddings built from bank-specific corpora does a better job of capturing the bank-specific semantics and word relatedness. This finding suggests that bank-specific word embeddings could be a good stand-alone source or a complement to other widely available embeddings when performing NL
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#29983;&#25104;AI&#26041;&#27861;&#26469;&#25193;&#23637;&#24403;&#21069;&#30340;&#39135;&#21697;&#35745;&#31639;&#27169;&#22411;&#65292;&#23558;&#28921;&#39274;&#34892;&#20026;&#32435;&#20837;&#32771;&#34385;&#65292;&#20197;&#23454;&#29616;&#26356;&#20840;&#38754;&#30340;&#20581;&#24247;&#39135;&#35889;&#25512;&#33616;&#12290;</title><link>http://arxiv.org/abs/2306.01805</link><description>&lt;p&gt;
Cook-Gen&#65306;&#20174;&#39135;&#35889;&#20013;&#29983;&#25104;&#20581;&#24247;&#28921;&#39274;&#21160;&#20316;&#30340;&#40065;&#26834;&#24615;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Cook-Gen: Robust Generative Modeling of Cooking Actions from Recipes. (arXiv:2306.01805v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01805
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#29983;&#25104;AI&#26041;&#27861;&#26469;&#25193;&#23637;&#24403;&#21069;&#30340;&#39135;&#21697;&#35745;&#31639;&#27169;&#22411;&#65292;&#23558;&#28921;&#39274;&#34892;&#20026;&#32435;&#20837;&#32771;&#34385;&#65292;&#20197;&#23454;&#29616;&#26356;&#20840;&#38754;&#30340;&#20581;&#24247;&#39135;&#35889;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#20204;&#36234;&#26469;&#36234;&#20851;&#27880;&#33258;&#24049;&#30340;&#39278;&#39135;&#36873;&#25321;&#65292;&#39135;&#21697;&#35745;&#31639;&#27169;&#22411;&#22312;&#24110;&#21161;&#20154;&#20204;&#20445;&#25345;&#20581;&#24247;&#39278;&#39135;&#20064;&#24815;&#26041;&#38754;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#12290;&#20363;&#22914;&#65292;&#39135;&#21697;&#25512;&#33616;&#31995;&#32479;&#20998;&#26512;&#39135;&#35889;&#25351;&#20196;&#20197;&#35780;&#20272;&#33829;&#20859;&#25104;&#20998;&#24182;&#25552;&#20379;&#39135;&#35889;&#25512;&#33616;&#12290;&#32780;&#29983;&#25104;AI&#26041;&#27861;&#65288;&#22914;&#33258;&#22238;&#24402;&#22823;&#35821;&#35328;&#27169;&#22411;&#65289;&#30340;&#25104;&#21151;&#24212;&#29992;&#21487;&#20197;&#35753;&#25105;&#20204;&#26356;&#20840;&#38754;&#22320;&#29702;&#35299;&#39135;&#35889;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20026;&#20581;&#24247;&#20840;&#38754;&#30340;&#39135;&#35889;&#25512;&#33616;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20351;&#29992;&#29983;&#25104;AI&#26041;&#27861;&#26469;&#25193;&#23637;&#24403;&#21069;&#30340;&#39135;&#21697;&#35745;&#31639;&#27169;&#22411;&#65292;&#20174;&#32780;&#23558;&#28921;&#39274;&#34892;&#20026;&#65288;&#20363;&#22914;&#21152;&#30416;&#12289;&#29006;&#32905;&#12289;&#29038;&#34092;&#33756;&#31561;&#65289;&#32435;&#20837;&#32771;&#34385;&#12290;&#28921;&#39274;&#34892;&#20026;&#30001;&#20110;&#20854;&#19981;&#35268;&#21017;&#30340;&#25968;&#25454;&#27169;&#24335;&#32780;&#38590;&#20197;&#20351;&#29992;&#32479;&#35745;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
As people become more aware of their food choices, food computation models have become increasingly popular in assisting people in maintaining healthy eating habits. For example, food recommendation systems analyze recipe instructions to assess nutritional contents and provide recipe recommendations. The recent and remarkable successes of generative AI methods, such as auto-regressive large language models, can lead to robust methods for a more comprehensive understanding of recipes for healthy food recommendations beyond surface-level nutrition content assessments. In this study, we explore the use of generative AI methods to extend current food computation models, primarily involving the analysis of nutrition and ingredients, to also incorporate cooking actions (e.g., add salt, fry the meat, boil the vegetables, etc.). Cooking actions are notoriously hard to model using statistical learning methods due to irregular data patterns - significantly varying natural language descriptions f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32534;&#36753;&#36317;&#31163;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#26368;&#23567;&#21270;RNN-T&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#22312;LibriSpeech&#30340;600M Conformer RNN-T&#27169;&#22411;&#19978;&#21462;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.01789</link><description>&lt;p&gt;
&#22522;&#20110;&#32534;&#36753;&#36317;&#31163;&#30340;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;RNN-T&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Edit Distance based RL for RNNT decoding. (arXiv:2306.01789v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01789
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32534;&#36753;&#36317;&#31163;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#26368;&#23567;&#21270;RNN-T&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#22312;LibriSpeech&#30340;600M Conformer RNN-T&#27169;&#22411;&#19978;&#21462;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20854;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#20986;&#33394;&#30340;WER&#21644;&#25903;&#25345;&#26080;&#32541;&#27969;&#24335;&#20256;&#36755;&#21644;&#38271;&#31687;&#36716;&#24405;&#30340;&#33021;&#21147;&#65292;RNN-T&#30446;&#21069;&#34987;&#35748;&#20026;&#26159;ASR&#30340;&#24037;&#19994;&#26631;&#20934;&#12290; &#28982;&#32780;&#65292;&#23427;&#26368;&#22823;&#30340;&#32570;&#28857;&#22312;&#20110;&#35757;&#32451;&#21644;&#25512;&#29702;&#30446;&#26631;&#20043;&#38388;&#23384;&#22312;&#26174;&#30528;&#24046;&#24322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#23567;&#21270;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#20043;&#38388;&#24046;&#36317;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32534;&#36753;&#36317;&#31163;RL (EDRL)&#26041;&#27861;&#22522;&#20110;&#32534;&#36753;&#36317;&#31163;&#35745;&#31639;&#22870;&#21169;&#65292;&#24182;&#22312;&#27599;&#20010;&#25805;&#20316;&#32423;&#21035;&#35757;&#32451;&#32593;&#32476;&#12290;&#35813;&#26041;&#27861;&#22312;LibriSpeech&#30340;600M Conformer RNN-T&#27169;&#22411;&#19978;&#33719;&#24471;&#20102;SoTA WERs&#12290;
&lt;/p&gt;
&lt;p&gt;
RNN-T is currently considered the industry standard in ASR due to its exceptional WERs in various benchmark tests and its ability to support seamless streaming and longform transcription. However, its biggest drawback lies in the significant discrepancy between its training and inference objectives. During training, RNN-T maximizes all alignment probabilities by teacher forcing, while during inference, it uses beam search which may not necessarily find the maximum probable alignment. Additionally, RNN-T's inability to experience mistakes during teacher forcing training makes it more problematic when a mistake occurs in inference. To address this issue, this paper proposes a Reinforcement Learning method that minimizes the gap between training and inference time. Our Edit Distance based RL (EDRL) approach computes rewards based on the edit distance, and trains the network at every action level. The proposed approach yielded SoTA WERs on LibriSpeech for the 600M Conformer RNN-T model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#23558;&#20854;&#19982;&#20247;&#21253;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#27604;&#36739;&#12290;&#19987;&#23478;&#35780;&#20272;&#34920;&#26126;&#65292;LLM&#29983;&#25104;&#30340;&#35299;&#20915;&#26041;&#26696;&#20855;&#26377;&#26356;&#39640;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#29992;&#24615;&#65292;&#20247;&#21253;&#30340;&#35299;&#20915;&#26041;&#26696;&#26356;&#21152;&#20855;&#26377;&#21019;&#24847;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.01779</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#24565;&#35774;&#35745;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Conceptual Design Generation Using Large Language Models. (arXiv:2306.01779v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01779
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#23558;&#20854;&#19982;&#20247;&#21253;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#27604;&#36739;&#12290;&#19987;&#23478;&#35780;&#20272;&#34920;&#26126;&#65292;LLM&#29983;&#25104;&#30340;&#35299;&#20915;&#26041;&#26696;&#20855;&#26377;&#26356;&#39640;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#29992;&#24615;&#65292;&#20247;&#21253;&#30340;&#35299;&#20915;&#26041;&#26696;&#26356;&#21152;&#20855;&#26377;&#21019;&#24847;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#29983;&#25104;&#26159;&#27010;&#24565;&#35774;&#35745;&#38454;&#27573;&#30340;&#21019;&#36896;&#24615;&#27493;&#39588;&#65292;&#22312;&#36825;&#20010;&#38454;&#27573;&#20013;&#65292;&#35774;&#35745;&#24072;&#32463;&#24120;&#23581;&#35797;&#36890;&#36807;&#22836;&#33041;&#39118;&#26292;&#12289;&#24605;&#32500;&#23548;&#22270;&#25110;&#20247;&#21253;&#35774;&#35745;&#24605;&#36335;&#26469;&#34917;&#20805;&#20182;&#20204;&#33258;&#24049;&#23545;&#39046;&#22495;&#30340;&#30693;&#35782;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#32463;&#23548;&#33268;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20852;&#36215;&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#20174;&#25991;&#26412;&#25552;&#31034;&#20013;&#29983;&#25104;&#30475;&#20284;&#21019;&#36896;&#24615;&#30340;&#36755;&#20986;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#25104;&#21151;&#24212;&#29992;&#36328;&#36275;&#20102;&#33402;&#26415;&#12289;&#23089;&#20048;&#21644;&#20854;&#20182;&#21019;&#20316;&#24037;&#20316;&#31561;&#21508;&#20010;&#39046;&#22495;&#12290;&#26412;&#25991;&#21033;&#29992;LLMs&#20026;&#19968;&#32452;12&#20010;&#35774;&#35745;&#38382;&#39064;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#23558;&#20854;&#19982;&#20247;&#21253;&#35299;&#20915;&#26041;&#26696;&#22522;&#20934;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#36890;&#36807;&#22810;&#20010;&#35282;&#24230;&#35780;&#20272;&#29983;&#25104;&#21644;&#20247;&#21253;&#35774;&#35745;&#35299;&#20915;&#26041;&#26696;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#21253;&#25324;&#20154;&#24037;&#19987;&#23478;&#35780;&#20272;&#21644;&#35745;&#31639;&#25351;&#26631;&#12290;&#19987;&#23478;&#35780;&#20272;&#34920;&#26126;&#65292;LLM&#29983;&#25104;&#30340;&#35299;&#20915;&#26041;&#26696;&#24179;&#22343;&#21487;&#34892;&#24615;&#21644;&#26377;&#29992;&#24615;&#26356;&#39640;&#65292;&#32780;&#20247;&#21253;&#30340;&#35299;&#20915;&#26041;&#26696;&#26356;&#21152;&#20855;&#26377;&#21019;&#24847;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concept generation is a creative step in the conceptual design phase, where designers often turn to brainstorming, mindmapping, or crowdsourcing design ideas to complement their own knowledge of the domain. Recent advances in natural language processing (NLP) and machine learning (ML) have led to the rise of Large Language Models (LLMs) capable of generating seemingly creative outputs from textual prompts. The success of these models has led to their integration and application across a variety of domains, including art, entertainment, and other creative work. In this paper, we leverage LLMs to generate solutions for a set of 12 design problems and compare them to a baseline of crowdsourced solutions. We evaluate the differences between generated and crowdsourced design solutions through multiple perspectives, including human expert evaluations and computational metrics. Expert evaluations indicate that the LLM-generated solutions have higher average feasibility and usefulness while th
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20174;&#23450;&#37327;&#26041;&#38754;&#23545;&#22522;&#20110;Transformers&#21644;State Space Models&#30340;&#19968;&#31995;&#21015;&#30740;&#31350;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#24182;&#24635;&#32467;&#20102;&#22914;&#20309;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.01768</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#25928;&#29575;&#30740;&#31350;&#30340;&#23450;&#37327;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Quantitative Review on Language Model Efficiency Research. (arXiv:2306.01768v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01768
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20174;&#23450;&#37327;&#26041;&#38754;&#23545;&#22522;&#20110;Transformers&#21644;State Space Models&#30340;&#19968;&#31995;&#21015;&#30740;&#31350;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#24182;&#24635;&#32467;&#20102;&#22914;&#20309;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#34987;&#25193;&#23637;&#29992;&#20110;&#22788;&#29702;&#19968;&#20123;&#22797;&#26434;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#26377;&#25928;&#24037;&#20855;&#20043;&#19968;&#12290;&#26412;&#31687;&#35770;&#25991;&#28085;&#30422;&#20102;&#23545;&#20110;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#25928;&#29575;&#36825;&#19968;&#26680;&#24515;&#38382;&#39064;&#30340;&#30740;&#31350;&#27010;&#36848;&#65292;&#24182;&#23581;&#35797;&#23545;&#26368;&#36817;&#30340;&#22522;&#20110;Transformers&#21644;State Space Models&#30340;&#30740;&#31350;&#36827;&#34892;&#23450;&#37327;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) are being scaled and becoming powerful. Improving their efficiency is one of the core research topics in neural information processing systems. Tay et al. (2022) provided a comprehensive overview of efficient Transformers that have become an indispensable staple in the field of NLP. However, in the section of "On Evaluation", they left an open question "which fundamental efficient Transformer one should consider," answered by "still a mystery" because "many research papers select their own benchmarks." Unfortunately, there was not quantitative analysis about the performances of Transformers on any benchmarks. Moreover, state space models (SSMs) have demonstrated their abilities of modeling long-range sequences with non-attention mechanisms, which were not discussed in the prior review. This article makes a meta analysis on the results from a set of papers on efficient Transformers as well as those on SSMs. It provides a quantitative review on LM efficiency researc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#30001;ChatGPT&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#24182;&#20197;11&#31181;&#31639;&#27861;&#36827;&#34892;&#20102;&#20998;&#31867;&#23545;&#27604;&#20998;&#26512;&#65292;&#22312;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;77%&#30340;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.01761</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21306;&#20998;&#20154;&#31867;&#29983;&#25104;&#25991;&#26412;&#21644;ChatGPT&#29983;&#25104;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Distinguishing Human Generated Text From ChatGPT Generated Text Using Machine Learning. (arXiv:2306.01761v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#30001;ChatGPT&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#24182;&#20197;11&#31181;&#31639;&#27861;&#36827;&#34892;&#20102;&#20998;&#31867;&#23545;&#27604;&#20998;&#26512;&#65292;&#22312;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;77%&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#39044;&#35757;&#32451;&#22823;&#35821;&#35328;&#27169;&#22411;&#23478;&#26063;&#20013;&#30340;&#19968;&#21592;&#65292;&#26159;&#19968;&#31181;&#23545;&#35805;&#20154;&#24037;&#26234;&#33021;&#12290;&#36825;&#31181;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#24494;&#35843;&#65292;&#21487;&#20197;&#29983;&#25104;&#30475;&#20284;&#30001;&#33258;&#28982;&#26234;&#33021;&#25776;&#20889;&#30340;&#25991;&#26412;&#25991;&#20214;&#12290;&#34429;&#28982;&#36825;&#31181;&#29983;&#25104;&#27169;&#22411;&#26377;&#24456;&#22810;&#20248;&#28857;&#65292;&#20294;&#20063;&#23384;&#22312;&#19968;&#20123;&#21512;&#29702;&#30340;&#25285;&#24551;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;ChatGPT&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#20154;&#31867;&#32534;&#20889;&#30340;&#25991;&#26412;&#65292;&#20197;&#21450;&#22312;&#20998;&#31867;&#36807;&#31243;&#20013;&#20849;&#35745;11&#31181;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#23545;&#27604;&#20998;&#26512;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;Kaggle&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;10,000&#20010;&#25991;&#26412;&#65292;&#20854;&#20013;5,204&#20010;&#25991;&#26412;&#26159;&#20154;&#31867;&#20174;&#26032;&#38395;&#21644;&#31038;&#20132;&#23186;&#20307;&#19978;&#25910;&#38598;&#30340;&#20889;&#20316;&#12290;&#22312;&#30001;GPT-3.5&#29983;&#25104;&#30340;&#35821;&#26009;&#24211;&#19978;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#21576;&#29616;&#20986;77%&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is a conversational artificial intelligence that is a member of the generative pre-trained transformer of the large language model family. This text generative model was fine-tuned by both supervised learning and reinforcement learning so that it can produce text documents that seem to be written by natural intelligence. Although there are numerous advantages of this generative model, it comes with some reasonable concerns as well. This paper presents a machine learning-based solution that can identify the ChatGPT delivered text from the human written text along with the comparative analysis of a total of 11 machine learning and deep learning algorithms in the classification process. We have tested the proposed model on a Kaggle dataset consisting of 10,000 texts out of which 5,204 texts were written by humans and collected from news and social media. On the corpus generated by GPT-3.5, the proposed algorithm presents an accuracy of 77%.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27979;&#35797;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#23545;&#20110;&#35757;&#32451;&#20808;&#39564;&#30340;&#20381;&#36182;&#31243;&#24230;&#65292;&#21457;&#29616;&#27169;&#22411;&#33021;&#22815;&#26356;&#22909;&#22320;&#29983;&#25104;&#19982;&#35757;&#32451;&#25968;&#25454;&#20013;&#20986;&#29616;&#39057;&#29575;&#26356;&#39640;&#30340;&#19977;&#20803;&#32452;&#23545;&#40784;&#30340;&#22270;&#20687;&#65292;&#20294;&#36825;&#20063;&#20250;&#38477;&#20302;&#20854;&#29983;&#25104;&#20197;&#32763;&#36716;&#19977;&#20803;&#32452;&#20026;&#22522;&#30784;&#30340;&#22270;&#20687;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.01755</link><description>&lt;p&gt;
&#35757;&#32451;&#20808;&#39564;&#24433;&#21709;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Training Priors Predict Text-To-Image Model Performance. (arXiv:2306.01755v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27979;&#35797;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#23545;&#20110;&#35757;&#32451;&#20808;&#39564;&#30340;&#20381;&#36182;&#31243;&#24230;&#65292;&#21457;&#29616;&#27169;&#22411;&#33021;&#22815;&#26356;&#22909;&#22320;&#29983;&#25104;&#19982;&#35757;&#32451;&#25968;&#25454;&#20013;&#20986;&#29616;&#39057;&#29575;&#26356;&#39640;&#30340;&#19977;&#20803;&#32452;&#23545;&#40784;&#30340;&#22270;&#20687;&#65292;&#20294;&#36825;&#20063;&#20250;&#38477;&#20302;&#20854;&#29983;&#25104;&#20197;&#32763;&#36716;&#19977;&#20803;&#32452;&#20026;&#22522;&#30784;&#30340;&#22270;&#20687;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#19968;&#20123;&#20851;&#31995;&#65292;&#27604;&#22914;&#8220;&#23431;&#33322;&#21592;&#39569;&#39532;&#8221;&#65292;&#20294;&#21364;&#19981;&#33021;&#29983;&#25104;&#30001;&#30456;&#21516;&#22522;&#26412;&#37096;&#20998;&#32452;&#25104;&#30340;&#20854;&#20182;&#20851;&#31995;&#65292;&#27604;&#22914;&#8220;&#39532;&#39569;&#23431;&#33322;&#21592;&#8221;&#12290;&#36825;&#20123;&#22833;&#36133;&#36890;&#24120;&#34987;&#35270;&#20026;&#27169;&#22411;&#20381;&#36182;&#35757;&#32451;&#20808;&#39564;&#32780;&#19981;&#26159;&#26500;&#24314;&#26032;&#39062;&#30340;&#22270;&#20687;&#32452;&#21512;&#30340;&#35777;&#25454;&#12290;&#26412;&#25991;&#30452;&#25509;&#22312;&#31283;&#23450;&#25193;&#25955;2.1&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#36890;&#36807;&#35266;&#23519;&#32452;&#25104;&#36825;&#20123;&#25552;&#31034;&#30340;&#20027;&#35821;-&#35859;&#35821;-&#23486;&#35821; (SVO) &#19977;&#20803;&#32452;&#65288;&#20363;&#22914;&#65292;&#8220;&#23431;&#33322;&#21592;&#8221;&#65292;&#8220;&#39569;&#8221;&#65292;&#8220;&#39532;&#8221;&#65289;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;SVO&#19977;&#20803;&#32452;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#20986;&#29616;&#30340;&#27425;&#25968;&#36234;&#22810;&#65292;&#35813;&#27169;&#22411;&#23601;&#33021;&#29983;&#25104;&#19982;&#35813;&#19977;&#20803;&#32452;&#23545;&#40784;&#30340;&#22270;&#20687;&#23601;&#36234;&#22909;&#12290;&#22312;&#36825;&#37324;&#65292;&#36890;&#36807;&#23545;&#40784;&#65292;&#25105;&#20204;&#30340;&#24847;&#24605;&#26159;&#27599;&#20010;&#26415;&#35821;&#22312;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#20197;&#27491;&#30830;&#30340;&#20851;&#31995;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22686;&#21152;&#30340;&#39057;&#29575;&#20063;&#20250;&#20943;&#23569;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#19982;&#32763;&#36716;&#19977;&#20803;&#32452;&#23545;&#40784;&#30340;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;&#20363;&#22914;&#65292;&#22914;&#26524;&#8220;&#23431;&#33322;&#21592;&#39569;&#39532;&#8221;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#39057;&#32321;&#20986;&#29616;&#65292;&#37027;&#20040;&#8220;&#39532;&#39569;&#23431;&#33322;&#21592;&#8221;&#30340;&#23545;&#40784;&#36136;&#37327;&#23601;&#20250;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image models can often generate some relations, i.e., "astronaut riding horse", but fail to generate other relations composed of the same basic parts, i.e., "horse riding astronaut". These failures are often taken as evidence that the models rely on training priors rather than constructing novel images compositionally. This paper tests this intuition directly on the stablediffusion 2.1 text-to-image model. By looking at the subject-verb-object (SVO) triads that form the backbone of these prompts (e.g., "astronaut", "ride", "horse"), we find that the more often an SVO triad appears in the training data, the better the model can generate an image aligned with that triad. Here, by aligned we mean that each of the terms appears in the generated image in the proper relation to each other. However, this increased frequency also diminishes how well the model can generate an image aligned with the flipped triad. For example, if "astronaut riding horse" appears frequently in the trainin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PVLIR&#30340;&#39044;&#22788;&#29702;&#35270;&#35273;&#35821;&#35328;&#25512;&#29702;&#21644;&#21512;&#29702;&#21270;&#20219;&#21153;&#65292;&#32467;&#26524;&#25581;&#31034;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#27492;&#39033;&#20219;&#21153;&#19978;&#30340;&#32570;&#38519;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#36825;&#20123;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.01753</link><description>&lt;p&gt;
&#24369;&#30417;&#30563;&#19979;&#30340;&#39044;&#22788;&#29702;&#35270;&#35273;&#35821;&#35328;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Preconditioned Visual Language Inference with Weak Supervision. (arXiv:2306.01753v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01753
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PVLIR&#30340;&#39044;&#22788;&#29702;&#35270;&#35273;&#35821;&#35328;&#25512;&#29702;&#21644;&#21512;&#29702;&#21270;&#20219;&#21153;&#65292;&#32467;&#26524;&#25581;&#31034;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#27492;&#39033;&#20219;&#21153;&#19978;&#30340;&#32570;&#38519;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#36825;&#20123;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21487;&#20197;&#36890;&#36807;&#25552;&#21462;&#27599;&#31181;&#24773;&#22659;&#19979;&#30456;&#20851;&#30340;&#21069;&#25552;&#26465;&#20214;&#26469;&#25512;&#26029;&#29289;&#20307;&#30340;&#21487;&#20379;&#24615;&#12290;&#20363;&#22914;&#65292;&#30475;&#21040;&#19968;&#24352;&#30772;&#30862;&#26479;&#23376;&#30340;&#29031;&#29255;&#65292;&#25105;&#20204;&#21487;&#20197;&#25512;&#26029;&#36825;&#20010;&#21069;&#25552;&#26465;&#20214;&#38459;&#27490;&#20102;&#26479;&#23376;&#29992;&#20110;&#39278;&#29992;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30740;&#31350;&#20013;&#65292;&#27169;&#22411;&#26126;&#30830;&#33719;&#21462;&#19978;&#19979;&#25991;&#21069;&#25552;&#26465;&#20214;&#26469;&#25512;&#29702;&#24120;&#35782;&#12290;&#20294;&#26159;&#65292;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#25552;&#21462;&#36825;&#26679;&#30340;&#21069;&#25552;&#26465;&#20214;&#24182;&#25512;&#26029;&#29289;&#20307;&#30340;&#21487;&#20379;&#24615;&#23578;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PVLIR&#30340;&#39044;&#22788;&#29702;&#35270;&#35273;&#35821;&#35328;&#25512;&#29702;&#21644;&#21512;&#29702;&#21270;&#20219;&#21153;&#65292;&#24182;&#24320;&#21457;&#20102;&#19977;&#20010;&#31574;&#30053;&#30340;&#23398;&#20064;&#36164;&#28304;&#26469;&#26816;&#32034;&#35813;&#20219;&#21153;&#30340;&#24369;&#30417;&#30563;&#20449;&#21495;&#65292;&#24182;&#21046;&#23450;&#20102;&#32463;&#36807;&#20154;&#24037;&#39564;&#35777;&#30340;&#27979;&#35797;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#39033;&#20219;&#21153;&#19978;&#30340;&#32570;&#38519;&#65292;&#24182;&#32472;&#21046;&#20102;&#26410;&#26469;&#25913;&#36827;&#36825;&#20123;&#27169;&#22411;&#38754;&#20020;&#30340;&#25361;&#25112;&#30340;&#36335;&#32447;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans can infer the affordance of objects by extracting related contextual preconditions for each scenario. For example, upon seeing an image of a broken cup, we can infer that this precondition prevents the cup from being used for drinking. Reasoning with preconditions of commonsense is studied in NLP where the model explicitly gets the contextual precondition. However, it is unclear if SOTA visual language models (VLMs) can extract such preconditions and infer the affordance of objects with them. In this work, we introduce the task of preconditioned visual language inference and rationalization (PVLIR). We propose a learning resource based on three strategies to retrieve weak supervision signals for the task and develop a human-verified test set for evaluation. Our results reveal the shortcomings of SOTA VLM models in the task and draw a road map to address the challenges ahead in improving them.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#38024;&#23545;Indic&#35821;&#35328;Unicode&#20070;&#20889;&#26041;&#26696;&#20013;&#24120;&#35265;&#21450;&#19981;&#24120;&#35265;&#38382;&#39064;&#30340;&#24037;&#20855;&#24211;&#65292;&#20998;&#21035;&#26159;&#32416;&#27491;&#19981;&#19968;&#33268;&#24615;&#30340;&#35268;&#33539;&#21270;&#22120;&#21644;Abugida&#25991;&#26412;&#30340;&#23383;&#24418;&#35299;&#26512;&#22120;&#65292;&#20351;&#29992;&#36825;&#20004;&#20010;&#24037;&#20855;&#21487;&#20197;&#25552;&#39640;400%&#30340;&#36895;&#24230;&#24182;&#26174;&#33879;&#25913;&#21892;&#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.01743</link><description>&lt;p&gt;
&#19968;&#31181;&#36866;&#29992;&#20110;Unicode&#25991;&#26412;&#30340;Abugida&#35268;&#33539;&#21270;&#22120;&#21644;&#35299;&#26512;&#22120;
&lt;/p&gt;
&lt;p&gt;
Abugida Normalizer and Parser for Unicode texts. (arXiv:2306.01743v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01743
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#38024;&#23545;Indic&#35821;&#35328;Unicode&#20070;&#20889;&#26041;&#26696;&#20013;&#24120;&#35265;&#21450;&#19981;&#24120;&#35265;&#38382;&#39064;&#30340;&#24037;&#20855;&#24211;&#65292;&#20998;&#21035;&#26159;&#32416;&#27491;&#19981;&#19968;&#33268;&#24615;&#30340;&#35268;&#33539;&#21270;&#22120;&#21644;Abugida&#25991;&#26412;&#30340;&#23383;&#24418;&#35299;&#26512;&#22120;&#65292;&#20351;&#29992;&#36825;&#20004;&#20010;&#24037;&#20855;&#21487;&#20197;&#25552;&#39640;400%&#30340;&#36895;&#24230;&#24182;&#26174;&#33879;&#25913;&#21892;&#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#24211;&#65292;&#20197;&#35299;&#20915;Indic&#35821;&#35328;&#22522;&#20110;Unicode&#30340;&#20070;&#20889;&#26041;&#26696;&#30340;&#24120;&#35265;&#21644;&#19981;&#24120;&#35265;&#38382;&#39064;&#12290;&#31532;&#19968;&#20010;&#26159;&#19968;&#20010;&#35268;&#33539;&#21270;&#22120;&#65292;&#36890;&#36807;https://pypi.org/project/bnunicodenormalizer/&#32416;&#27491;&#30001;&#32534;&#30721;&#26041;&#26696;&#24341;&#36215;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#31532;&#20108;&#20010;&#26159;Abugida&#25991;&#26412;&#30340;&#23383;&#24418;&#35299;&#26512;&#22120;https://pypi.org/project/indicparser/&#12290;&#36825;&#20004;&#20010;&#24037;&#20855;&#27604;&#20197;&#21069;&#20351;&#29992;&#30340;&#24037;&#20855;&#26356;&#26377;&#25928;&#29575;&#21644;&#26377;&#25928;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;400%&#30340;&#36895;&#24230;&#25552;&#21319;&#65292;&#24182;&#30830;&#20445;&#19981;&#21516;&#35821;&#35328;&#27169;&#22411;&#22522;&#20110;&#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes two libraries to address common and uncommon issues with Unicode-based writing schemes for Indic languages. The first is a normalizer that corrects inconsistencies caused by the encoding scheme https://pypi.org/project/bnunicodenormalizer/ . The second is a grapheme parser for Abugida text https://pypi.org/project/indicparser/ . Both tools are more efficient and effective than previously used tools. We report 400% increase in speed and ensure significantly better performance for different language model based downstream tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25214;&#21040;&#35745;&#31639;&#25928;&#29575;&#39640;&#12289;&#21487;&#27604;&#25110;&#26356;&#22909;&#30340;&#24076;&#26395;&#35328;&#35770;&#26816;&#27979;&#26041;&#27861;&#65292;&#24182;&#20844;&#24320;&#20102;&#20195;&#30721;&#24211;&#12290;</title><link>http://arxiv.org/abs/2306.01742</link><description>&lt;p&gt;
&#36229;&#36234;&#28040;&#26497;&#24773;&#32490;&#65306;&#20851;&#20110;&#24076;&#26395;&#35328;&#35770;&#26816;&#27979;&#30340;&#37325;&#26032;&#20998;&#26512;&#21644;&#21518;&#32493;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Beyond Negativity: Re-Analysis and Follow-Up Experiments on Hope Speech Detection. (arXiv:2306.01742v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25214;&#21040;&#35745;&#31639;&#25928;&#29575;&#39640;&#12289;&#21487;&#27604;&#25110;&#26356;&#22909;&#30340;&#24076;&#26395;&#35328;&#35770;&#26816;&#27979;&#26041;&#27861;&#65292;&#24182;&#20844;&#24320;&#20102;&#20195;&#30721;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20581;&#24247;&#19987;&#23478;&#20204;&#35748;&#20026;&#65292;&#24076;&#26395;&#22312;&#22686;&#24378;&#20010;&#20154;&#30340;&#36523;&#24515;&#20581;&#24247;&#12289;&#20419;&#36827;&#24247;&#22797;&#21644;&#24674;&#22797;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#24076;&#26395;&#35328;&#35770;&#26159;&#25351;&#22312;&#35780;&#35770;&#12289;&#24086;&#23376;&#21644;&#20854;&#20182;&#31038;&#20132;&#23186;&#20307;&#28040;&#24687;&#20013;&#25552;&#20379;&#25903;&#25345;&#12289;&#23433;&#24944;&#12289;&#24314;&#35758;&#12289;&#21551;&#31034;&#21644;&#35265;&#35299;&#30340;&#35328;&#35770;&#12290;&#24076;&#26395;&#35328;&#35770;&#30340;&#26816;&#27979;&#28041;&#21450;&#36825;&#31181;&#25991;&#26412;&#20869;&#23481;&#30340;&#20998;&#26512;&#65292;&#26088;&#22312;&#35782;&#21035;&#33021;&#22815;&#21796;&#36215;&#20154;&#20204;&#31215;&#26497;&#24773;&#32490;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#25214;&#21040;&#35745;&#31639;&#25928;&#29575;&#39640;&#12289;&#21487;&#27604;&#25110;&#26356;&#22909;&#30340;&#24076;&#26395;&#35328;&#35770;&#26816;&#27979;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#23558;&#25105;&#20204;&#30340;&#20195;&#30721;&#24211;&#20844;&#24320;&#22312; https://github.com/aflah02/Hope_Speech_Detection &#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Health experts assert that hope plays a crucial role in enhancing individuals' physical and mental well-being, facilitating their recovery, and promoting restoration. Hope speech refers to comments, posts and other social media messages that offer support, reassurance, suggestions, inspiration, and insight. The detection of hope speech involves the analysis of such textual content, with the aim of identifying messages that invoke positive emotions in people. Our study aims to find computationally efficient yet comparable/superior methods for hope speech detection. We also make our codebase public at https://github.com/aflah02/Hope_Speech_Detection
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;6&#20010;&#19981;&#21516;&#30340;&#22522;&#20110;&#33258;&#25105;&#27880;&#24847;&#21147;&#30340;Transformer&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#30740;&#31350;&#65292;&#24182;&#38024;&#23545;4&#31181;&#28608;&#27963;&#20989;&#25968;&#36827;&#34892;&#20102;&#35843;&#25972;&#65292;&#20197;&#29992;&#20110;&#21028;&#20915;&#25991;&#26412;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#22312;&#27861;&#24459;&#35785;&#35772;&#20013;&#36827;&#34892;&#24341;&#29992;&#21644;&#20808;&#20363;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2306.01739</link><description>&lt;p&gt;
&#22522;&#20110; Transformer &#27169;&#22411;&#30340;&#35009;&#21028;&#25991;&#26412;&#20998;&#31867;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Comparative study on Judgment Text Classification for Transformer Based Models. (arXiv:2306.01739v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;6&#20010;&#19981;&#21516;&#30340;&#22522;&#20110;&#33258;&#25105;&#27880;&#24847;&#21147;&#30340;Transformer&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#30740;&#31350;&#65292;&#24182;&#38024;&#23545;4&#31181;&#28608;&#27963;&#20989;&#25968;&#36827;&#34892;&#20102;&#35843;&#25972;&#65292;&#20197;&#29992;&#20110;&#21028;&#20915;&#25991;&#26412;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#22312;&#27861;&#24459;&#35785;&#35772;&#20013;&#36827;&#34892;&#24341;&#29992;&#21644;&#20808;&#20363;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#20174;&#21028;&#20915;&#25991;&#20070;&#20013;&#25552;&#21462;&#21644;&#25688;&#35201;&#25991;&#26412;&#26469;&#39044;&#27979;&#29305;&#23450;&#21028;&#20915;&#30340;&#33719;&#32988;&#32773;&#12290;&#36825;&#20123;&#25991;&#26723;&#22312;&#27861;&#24459;&#35785;&#35772;&#20013;&#38750;&#24120;&#26377;&#29992;&#12290;&#20854;&#20013;&#19968;&#20010;&#20248;&#28857;&#26159;&#65292;&#36825;&#20123;&#25991;&#26723;&#21487;&#29992;&#20110;&#24341;&#29992;&#21644;&#20808;&#20363;&#21442;&#32771;&#65292;&#36825;&#20351;&#24471;&#20351;&#29992;&#32773;&#21487;&#20197;&#26356;&#26377;&#21147;&#22320;&#20026;&#20854;&#26696;&#20214;&#36777;&#25252;&#12290;&#24403;&#28041;&#21450;&#21040;&#26696;&#20363;&#20808;&#20363;&#26102;&#65292;&#24517;&#39035;&#21442;&#32771;&#22823;&#37327;&#25991;&#26723;&#65292;&#20197;&#25910;&#38598;&#19982;&#35813;&#26696;&#20214;&#26377;&#20851;&#30340;&#27861;&#24459;&#35201;&#28857;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25991;&#26723;&#30340;&#22797;&#26434;&#35789;&#27719;&#32467;&#26500;&#21644;&#25991;&#26723;&#22823;&#23567;&#65292;&#23457;&#26597;&#36825;&#20123;&#25991;&#26723;&#38656;&#35201;&#33457;&#36153;&#24456;&#38271;&#26102;&#38388;&#36827;&#34892;&#20998;&#26512;&#12290;&#26412;&#25991;&#28041;&#21450;&#22312;4&#31181;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#19979;&#35843;&#25972; 6 &#31181;&#33258;&#25105;&#27880;&#24847;&#21147;&#30340;&#22522;&#20110; Transformer &#30340;&#27169;&#22411;&#65292;&#24182;&#30740;&#31350;&#23427;&#20204;&#22312;&#35757;&#32451;&#20102; 200 &#20010;&#21028;&#20915;&#19978;&#30340;&#34920;&#29616;&#65292;&#20197;&#21450;&#26681;&#25454;&#19981;&#21516;&#22522;&#20934;&#21442;&#25968;&#35780;&#20272;&#20854;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work involves the usage of various NLP models to predict the winner of a particular judgment by the means of text extraction and summarization from a judgment document. These documents are useful when it comes to legal proceedings. One such advantage is that these can be used for citations and precedence reference in Lawsuits and cases which makes a strong argument for their case by the ones using it. When it comes to precedence, it is necessary to refer to an ample number of documents in order to collect legal points with respect to the case. However, reviewing these documents takes a long time to analyze due to the complex word structure and the size of the document. This work involves the comparative study of 6 different self-attention-based transformer models and how they perform when they are being tweaked in 4 different activation functions. These models which are trained with 200 judgement contexts and their results are being judged based on different benchmark parameters. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#65292;&#22312;NMT&#20013;&#22522;&#20110;&#23376;&#35789;&#30340;&#20998;&#35789;&#26041;&#27861;&#20013;&#65292;&#39057;&#29575;&#23545;&#20110;&#27169;&#22411;&#30340;&#34920;&#29616;&#36129;&#29486;&#21344;&#25454;&#20102;90%-95%&#65292;&#22240;&#27492;&#30456;&#23545;&#20110;&#32452;&#21512;&#24615;&#65292;&#39057;&#29575;&#26356;&#20026;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2306.01393</link><description>&lt;p&gt;
NMT&#20013;&#22522;&#20110;&#23376;&#35789;&#30340;&#20998;&#35789;&#20013;&#65292;&#39057;&#29575;&#21644;&#32452;&#21512;&#24615;&#30340;&#37325;&#35201;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Assessing the Importance of Frequency versus Compositionality for Subword-based Tokenization in NMT. (arXiv:2306.01393v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#65292;&#22312;NMT&#20013;&#22522;&#20110;&#23376;&#35789;&#30340;&#20998;&#35789;&#26041;&#27861;&#20013;&#65292;&#39057;&#29575;&#23545;&#20110;&#27169;&#22411;&#30340;&#34920;&#29616;&#36129;&#29486;&#21344;&#25454;&#20102;90%-95%&#65292;&#22240;&#27492;&#30456;&#23545;&#20110;&#32452;&#21512;&#24615;&#65292;&#39057;&#29575;&#26356;&#20026;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23376;&#35789;&#20998;&#35789;&#26159;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#21644;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#20013;&#30340;&#40664;&#35748;&#26631;&#20934;&#12290;&#39057;&#32321;&#24341;&#29992;&#23376;&#35789;&#30340;&#20248;&#28857;&#26377;&#65306;&#23545;&#39057;&#32321;&#35789;&#35821;&#36827;&#34892;&#26356;&#30701;&#32534;&#30721;&#65292;&#23376;&#35789;&#32452;&#21512;&#24615;&#24378;&#20197;&#21450;&#22788;&#29702;&#26410;&#30693;&#35789;&#35821;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#23578;&#19981;&#22826;&#28165;&#26970;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#35789;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#39057;&#29575;&#65288;&#31532;&#19968;&#20010;&#20248;&#28857;&#65289;&#19982;&#32452;&#21512;&#24615;&#20998;&#31163;&#24320;&#26469;&#65292;&#20351;&#29992;&#38669;&#22827;&#26364;&#32534;&#30721;&#23545;&#21333;&#35789;&#36827;&#34892;&#20998;&#35789;&#65292;&#25353;&#39057;&#29575;&#39034;&#24207;&#65292;&#20351;&#29992;&#22266;&#23450;&#25968;&#37327;&#30340;&#31526;&#21495;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;CS-DE&#12289;EN-FR&#21644;EN-DE NMT&#20013;&#65292;&#20165;&#39057;&#29575;&#23601;&#21344;&#20102;BPE&#24471;&#20998;&#30340;90%-95%&#65292;&#22240;&#27492;&#32452;&#21512;&#24615;&#24182;&#19981;&#20687;&#20197;&#21069;&#35748;&#20026;&#30340;&#37027;&#20040;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Subword tokenization is the de facto standard for tokenization in neural language models and machine translation systems. Three advantages are frequently cited in favor of subwords: shorter encoding of frequent tokens, compositionality of subwords, and ability to deal with unknown words. As their relative importance is not entirely clear yet, we propose a tokenization approach that enables us to separate frequency (the first advantage) from compositionality. The approach uses Huffman coding to tokenize words, by order of frequency, using a fixed amount of symbols. Experiments with CS-DE, EN-FR and EN-DE NMT show that frequency alone accounts for 90%-95% of the scores reached by BPE, hence compositionality has less importance than previously thought.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25511;&#21046;&#26631;&#35760;&#32423;&#30340;&#29983;&#25104;&#12289;&#25552;&#39640;&#21407;&#22987;&#23398;&#20064;&#21644;&#20943;&#23569;&#38169;&#35823;&#65292;&#20854;&#20013;&#21253;&#25324;&#33945;&#29305;&#21345;&#27931;dropout&#12289;&#36793;&#32536;&#38750;&#20284;&#28982;&#23398;&#20064;&#21644;&#26368;&#23567;&#21270;&#29109;&#12290;&#22312;&#22235;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#24773;&#24863;&#22235;&#20803;&#32452;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.00418</link><description>&lt;p&gt;
&#8220;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#38750;&#20284;&#28982;&#23398;&#20064;&#25552;&#39640;&#29983;&#25104;&#24335;&#24773;&#24863;&#22235;&#20803;&#32452;&#39044;&#27979;&#8221;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Aware Unlikelihood Learning Improves Generative Aspect Sentiment Quad Prediction. (arXiv:2306.00418v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00418
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25511;&#21046;&#26631;&#35760;&#32423;&#30340;&#29983;&#25104;&#12289;&#25552;&#39640;&#21407;&#22987;&#23398;&#20064;&#21644;&#20943;&#23569;&#38169;&#35823;&#65292;&#20854;&#20013;&#21253;&#25324;&#33945;&#29305;&#21345;&#27931;dropout&#12289;&#36793;&#32536;&#38750;&#20284;&#28982;&#23398;&#20064;&#21644;&#26368;&#23567;&#21270;&#29109;&#12290;&#22312;&#22235;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#24773;&#24863;&#22235;&#20803;&#32452;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#39046;&#22495;&#24191;&#27867;&#20851;&#27880;&#20102;&#24773;&#24863;&#22235;&#20803;&#32452;&#39044;&#27979;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#20986;&#22235;&#20803;&#32452;&#65292;&#23558;&#21407;&#22987;&#21477;&#23376;&#36716;&#21270;&#20026;&#27169;&#26495;&#21270;&#30340;&#30446;&#26631;&#24207;&#21015;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#30740;&#31350;&#21482;&#20851;&#27880;&#29983;&#25104;&#20160;&#20040;&#65292;&#32780;&#24573;&#30053;&#20102;&#19981;&#38656;&#35201;&#29983;&#25104;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#35748;&#20026;&#32771;&#34385;&#36127;&#26679;&#26412;&#20063;&#20250;&#24102;&#26469;&#28508;&#22312;&#30340;&#22909;&#22788;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#26495;&#26080;&#20851;&#30340;&#26041;&#27861;&#26469;&#25511;&#21046;&#26631;&#35760;&#32423;&#30340;&#29983;&#25104;&#65292;&#21516;&#26102;&#25552;&#39640;&#21407;&#22987;&#23398;&#20064;&#21644;&#20943;&#23569;&#38169;&#35823;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#33945;&#29305;&#21345;&#27931;dropout&#26469;&#29702;&#35299;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#32622;&#19981;&#30830;&#23450;&#24615;&#65292;&#33719;&#21462;&#22122;&#22768;&#21644;&#38169;&#35823;&#20449;&#24687;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#36793;&#32536;&#38750;&#20284;&#28982;&#23398;&#20064;&#26469;&#25233;&#21046;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#38169;&#35823;&#26631;&#35760;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26368;&#23567;&#21270;&#29109;&#26469;&#24179;&#34913;&#36793;&#32536;&#38750;&#20284;&#28982;&#23398;&#20064;&#30340;&#24433;&#21709;&#12290;&#22312;&#22235;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#25552;&#39640;&#24773;&#24863;&#22235;&#20803;&#32452;&#39044;&#27979;&#30340;&#24615;&#33021;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, aspect sentiment quad prediction has received widespread attention in the field of aspect-based sentiment analysis. Existing studies extract quadruplets via pre-trained generative language models to paraphrase the original sentence into a templated target sequence. However, previous works only focus on what to generate but ignore what not to generate. We argue that considering the negative samples also leads to potential benefits. In this work, we propose a template-agnostic method to control the token-level generation, which boosts original learning and reduces mistakes simultaneously. Specifically, we introduce Monte Carlo dropout to understand the built-in uncertainty of pre-trained language models, acquiring the noises and errors. We further propose marginalized unlikelihood learning to suppress the uncertainty-aware mistake tokens. Finally, we introduce minimization entropy to balance the effects of marginalized unlikelihood learning. Extensive experiments on four public
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978; ChatGPT &#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#21253;&#25324;&#38382;&#31572;&#12289;&#25991;&#26412;&#25688;&#35201;&#12289;&#20195;&#30721;&#29983;&#25104;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#12289;&#26426;&#22120;&#32763;&#35793;&#12289;&#20559;&#35265;&#26816;&#27979;&#21644;&#20262;&#29702;&#32771;&#34385;&#31561;&#20219;&#21153;&#12290;&#30740;&#31350;&#26088;&#22312;&#39564;&#35777; ChatGPT &#30340;&#20248;&#21183;&#21644;&#24369;&#28857;&#65292;&#24182;&#20026;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.18486</link><description>&lt;p&gt;
&#22522;&#20934;&#25968;&#25454;&#38598;&#19978; ChatGPT &#30340;&#31995;&#32479;&#30740;&#31350;&#21644;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets. (arXiv:2305.18486v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18486
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978; ChatGPT &#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#21253;&#25324;&#38382;&#31572;&#12289;&#25991;&#26412;&#25688;&#35201;&#12289;&#20195;&#30721;&#29983;&#25104;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#12289;&#26426;&#22120;&#32763;&#35793;&#12289;&#20559;&#35265;&#26816;&#27979;&#21644;&#20262;&#29702;&#32771;&#34385;&#31561;&#20219;&#21153;&#12290;&#30740;&#31350;&#26088;&#22312;&#39564;&#35777; ChatGPT &#30340;&#20248;&#21183;&#21644;&#24369;&#28857;&#65292;&#24182;&#20026;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22914; ChatGPT &#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24320;&#21457;&#24341;&#36215;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38590;&#20197;&#23558;&#35813;&#27169;&#22411;&#29983;&#25104;&#30340;&#20135;&#20986;&#19982;&#22522;&#26412;&#20107;&#23454;&#36827;&#34892;&#27604;&#36739;&#65292;&#22240;&#27492;&#20854;&#22312;&#22522;&#20934;&#23398;&#26415;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#20173;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#26088;&#22312;&#23545; ChatGPT &#22312;&#21253;&#25324;&#38382;&#31572;&#12289;&#25991;&#26412;&#25688;&#35201;&#12289;&#20195;&#30721;&#29983;&#25104;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#12289;&#26426;&#22120;&#32763;&#35793;&#12289;&#20559;&#35265;&#26816;&#27979;&#21644;&#20262;&#29702;&#32771;&#34385;&#31561;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#36827;&#34892;&#24443;&#24213;&#35780;&#20272;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312; 140 &#20010;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102; ChatGPT&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#20013;&#29983;&#25104;&#30340; 255K &#27425;&#21709;&#24212;&#65292;&#36825;&#20351;&#25105;&#20204;&#30340;&#24037;&#20316;&#25104;&#20026;&#20102;&#22312; NLP &#22522;&#20934;&#27979;&#35797;&#20013;&#23545; ChatGPT &#36827;&#34892;&#30340;&#26368;&#22823;&#35780;&#20272;&#12290;&#31616;&#32780;&#35328;&#20043;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#39564;&#35777; ChatGPT &#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#20248;&#21183;&#21644;&#24369;&#28857;&#65292;&#24182;&#20026;&#20351;&#29992; LLM &#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#35265;&#35299;&#12290;&#25105;&#20204;&#36824;&#25253;&#21578;&#20102;&#19968;&#31181;&#26032;&#30340;&#36856;&#21457;&#33021;&#21147;&#65292;&#21363;&#36981;&#24490;&#22810;&#20010;&#26597;&#35810;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of large language models (LLMs) such as ChatGPT has brought a lot of attention recently. However, their evaluation in the benchmark academic datasets remains under-explored due to the difficulty of evaluating the generative outputs produced by this model against the ground truth. In this paper, we aim to present a thorough evaluation of ChatGPT's performance on diverse academic datasets, covering tasks like question-answering, text summarization, code generation, commonsense reasoning, mathematical problem-solving, machine translation, bias detection, and ethical considerations. Specifically, we evaluate ChatGPT across 140 tasks and analyze 255K responses it generates in these datasets. This makes our work the largest evaluation of ChatGPT in NLP benchmarks. In short, our study aims to validate the strengths and weaknesses of ChatGPT in various tasks and provide insights for future research using LLMs. We also report a new emergent ability to follow multi-query instruct
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#22659;&#31867;&#27604;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38382;&#39064;&#30340;&#24863;&#30693;&#29305;&#24449;&#32534;&#30721;&#25104;&#35821;&#35328;&#24418;&#24335;&#65292;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#30340;&#38646;-shot&#20851;&#31995;&#25512;&#29702;&#65292;&#36229;&#36234;&#20256;&#32479;&#26041;&#27861;&#21644;&#20154;&#31867;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2305.17626</link><description>&lt;p&gt;
&#22522;&#20110;&#39044;&#20808;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#22659;&#31867;&#27604;&#25512;&#29702;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
In-Context Analogical Reasoning with Pre-Trained Language Models. (arXiv:2305.17626v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#22659;&#31867;&#27604;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38382;&#39064;&#30340;&#24863;&#30693;&#29305;&#24449;&#32534;&#30721;&#25104;&#35821;&#35328;&#24418;&#24335;&#65292;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#30340;&#38646;-shot&#20851;&#31995;&#25512;&#29702;&#65292;&#36229;&#36234;&#20256;&#32479;&#26041;&#27861;&#21644;&#20154;&#31867;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#27604;&#25512;&#29702;&#26159;&#20154;&#31867;&#35748;&#30693;&#30340;&#22522;&#26412;&#33021;&#21147;&#20043;&#19968;&#65292;&#21487;&#20197;&#36890;&#36807;&#23558;&#26032;&#30340;&#24773;&#20917;&#19982;&#36807;&#21435;&#30340;&#32463;&#39564;&#20851;&#32852;&#26469;&#36827;&#34892;&#25277;&#35937;&#25512;&#29702;&#12290;&#34429;&#28982;&#23427;&#34987;&#35748;&#20026;&#23545;&#20110;AI&#31995;&#32479;&#30340;&#24378;&#22823;&#25512;&#29702;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#20256;&#32479;&#26041;&#27861;&#38656;&#35201;&#36827;&#34892;&#22823;&#37327;&#30340;&#35757;&#32451;&#21644;/&#25110;&#22266;&#21270;&#29305;&#23450;&#30340;&#39046;&#22495;&#30693;&#35782;&#25165;&#33021;&#24212;&#29992;&#20110;&#22522;&#20934;&#20219;&#21153;&#20013;&#12290;&#21463;&#21040;&#35748;&#30693;&#31185;&#23398;&#30740;&#31350;&#21457;&#29616;&#20154;&#31867;&#35821;&#35328;&#19982;&#31867;&#27604;&#21046;&#20316;&#20043;&#38388;&#30340;&#32852;&#31995;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25506;&#32034;&#20351;&#29992;&#30452;&#35266;&#30340;&#22522;&#20110;&#35821;&#35328;&#30340;&#25277;&#35937;&#26469;&#25903;&#25345;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#30340;&#31867;&#27604;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#22823;&#22411;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#23545;&#35270;&#35273;Raven&#30340;&#28176;&#36827;&#30697;&#38453;&#65288;RPM&#65289;&#36827;&#34892;&#31867;&#27604;&#25512;&#29702;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#30340;&#24863;&#30693;&#29305;&#24449;&#31616;&#21333;&#22320;&#32534;&#30721;&#25104;&#35821;&#35328;&#24418;&#24335;&#65292;&#25105;&#20204;&#21457;&#29616;PLMs&#34920;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#38646;-shot&#20851;&#31995;&#25512;&#29702;&#33021;&#21147;&#65292;&#36229;&#36807;&#20102;&#20154;&#31867;&#34920;&#29616;&#24182;&#25509;&#36817;&#20110;&#21463;&#30417;&#30563;&#30340;&#22522;&#20110;&#35270;&#35273;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#32534;&#30721;&#26041;&#27861;&#65292;&#20197;&#21464;&#21270;&#25277;&#35937;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analogical reasoning is a fundamental capacity of human cognition that allows us to reason abstractly about novel situations by relating them to past experiences. While it is thought to be essential for robust reasoning in AI systems, conventional approaches require significant training and/or hard-coding of domain knowledge to be applied to benchmark tasks. Inspired by cognitive science research that has found connections between human language and analogy-making, we explore the use of intuitive language-based abstractions to support analogy in AI systems. Specifically, we apply large pre-trained language models (PLMs) to visual Raven's Progressive Matrices (RPM), a common relational reasoning test. By simply encoding the perceptual features of the problem into language form, we find that PLMs exhibit a striking capacity for zero-shot relational reasoning, exceeding human performance and nearing supervised vision-based methods. We explore different encodings that vary the level of abs
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#30340;&#29616;&#29366;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#29305;&#24322;&#24615;&#22522;&#20934;&#27979;&#35797;&#38590;&#20197;&#26816;&#27979;&#21040;&#19981;&#33391;&#21103;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#25913;&#36827;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#35813;&#27979;&#35797;&#21487;&#20197;&#26816;&#27979;&#21040;&#21160;&#24577;&#32452;&#20214;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;KL&#25955;&#24230;&#30340;&#25351;&#26631;&#25193;&#23637;&#20102;&#29305;&#24322;&#24615;&#30340;&#34913;&#37327;&#26041;&#24335;&#12290;&#30740;&#31350;&#21457;&#29616;&#26368;&#36817;&#30340;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#29305;&#24322;&#24615;&#36739;&#20302;&#65292;&#24378;&#35843;&#20102;&#25913;&#36827;&#29305;&#24322;&#24615;&#22522;&#20934;&#27979;&#35797;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.17553</link><description>&lt;p&gt;
&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;&#22833;&#36133;&#65306;&#19968;&#20010;&#25913;&#36827;&#30340;&#29305;&#24322;&#24615;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Detecting Edit Failures In Large Language Models: An Improved Specificity Benchmark. (arXiv:2305.17553v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17553
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#30340;&#29616;&#29366;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#29305;&#24322;&#24615;&#22522;&#20934;&#27979;&#35797;&#38590;&#20197;&#26816;&#27979;&#21040;&#19981;&#33391;&#21103;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#25913;&#36827;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#35813;&#27979;&#35797;&#21487;&#20197;&#26816;&#27979;&#21040;&#21160;&#24577;&#32452;&#20214;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;KL&#25955;&#24230;&#30340;&#25351;&#26631;&#25193;&#23637;&#20102;&#29305;&#24322;&#24615;&#30340;&#34913;&#37327;&#26041;&#24335;&#12290;&#30740;&#31350;&#21457;&#29616;&#26368;&#36817;&#30340;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#29305;&#24322;&#24615;&#36739;&#20302;&#65292;&#24378;&#35843;&#20102;&#25913;&#36827;&#29305;&#24322;&#24615;&#22522;&#20934;&#27979;&#35797;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#25215;&#35834;&#22312;LLM&#35757;&#32451;&#36807;&#31243;&#20013;&#20943;&#36731;&#35760;&#24518;&#38169;&#35823;&#25110;&#36807;&#26102;&#20851;&#32852;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#25216;&#26415;&#21487;&#33021;&#20250;&#24341;&#20837;&#22823;&#37327;&#26410;&#34987;&#29616;&#26377;&#29305;&#24322;&#24615;&#22522;&#20934;&#27979;&#35797;&#26816;&#27979;&#21040;&#30340;&#19981;&#33391;&#21103;&#20316;&#29992;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;CounterFact&#22522;&#20934;&#27979;&#35797;&#20197;&#21253;&#25324;&#21160;&#24577;&#32452;&#20214;&#65292;&#24182;&#23558;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#31216;&#20026;CounterFact+&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#22522;&#20110;KL&#25955;&#24230;&#30340;&#26412;&#36136;&#25351;&#26631;&#25193;&#23637;&#20102;&#29992;&#20110;&#34913;&#37327;&#29305;&#24322;&#24615;&#30340;&#25351;&#26631;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#25913;&#36827;&#30340;&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;&#26368;&#36817;&#30340;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#65292;&#21457;&#29616;&#23427;&#20204;&#30340;&#29305;&#24322;&#24615;&#36739;&#20302;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#20984;&#26174;&#20102;&#38656;&#35201;&#25913;&#36827;&#29305;&#24322;&#24615;&#22522;&#20934;&#27979;&#35797;&#20197;&#35782;&#21035;&#21644;&#39044;&#38450;&#19981;&#33391;&#21103;&#20316;&#29992;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent model editing techniques promise to mitigate the problem of memorizing false or outdated associations during LLM training. However, we show that these techniques can introduce large unwanted side effects which are not detected by existing specificity benchmarks. We extend the existing CounterFact benchmark to include a dynamic component and dub our benchmark CounterFact+. Additionally, we extend the metrics used for measuring specificity by a principled KL divergence-based metric. We use this improved benchmark to evaluate recent model editing techniques and find that they suffer from low specificity. Our findings highlight the need for improved specificity benchmarks that identify and prevent unwanted side effects.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#35821;&#20041;&#38382;&#31572;&#37325;&#26500;&#27169;&#22411; (SURF)&#65292;&#36890;&#36807;&#19977;&#20010;&#22522;&#20110;&#35821;&#35328;&#23398;&#30340;&#25805;&#20316;&#26469;&#37325;&#20889;&#21475;&#35821;&#38382;&#31572;&#20013;&#23384;&#22312;&#30340;&#35789;&#27719;&#12289;&#21629;&#39064;&#12289;&#21477;&#27861;&#21644;&#29305;&#24322;&#24615;&#31561;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#22238;&#31572;&#29575;&#65292;&#33021;&#22815;&#24110;&#21161;&#35821;&#38899;&#21161;&#25163;&#26356;&#22909;&#22320;&#22238;&#31572;&#26410;&#22238;&#31572;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.17393</link><description>&lt;p&gt;
&#22312;&#21475;&#35821;&#38382;&#31572;&#20013;&#36890;&#36807;&#35821;&#20041;&#37325;&#26500;&#26469;&#22238;&#31572;&#26410;&#22238;&#31572;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Answering Unanswered Questions through Semantic Reformulations in Spoken QA. (arXiv:2305.17393v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#35821;&#20041;&#38382;&#31572;&#37325;&#26500;&#27169;&#22411; (SURF)&#65292;&#36890;&#36807;&#19977;&#20010;&#22522;&#20110;&#35821;&#35328;&#23398;&#30340;&#25805;&#20316;&#26469;&#37325;&#20889;&#21475;&#35821;&#38382;&#31572;&#20013;&#23384;&#22312;&#30340;&#35789;&#27719;&#12289;&#21629;&#39064;&#12289;&#21477;&#27861;&#21644;&#29305;&#24322;&#24615;&#31561;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#22238;&#31572;&#29575;&#65292;&#33021;&#22815;&#24110;&#21161;&#35821;&#38899;&#21161;&#25163;&#26356;&#22909;&#22320;&#22238;&#31572;&#26410;&#22238;&#31572;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21475;&#35821;&#38382;&#31572;&#26159;&#35821;&#38899;&#21161;&#25163;&#30340;&#19968;&#20010;&#37325;&#35201;&#21151;&#33021;&#65292;&#36890;&#24120;&#30001;&#22810;&#20010;&#38382;&#31572;&#31995;&#32479;&#25903;&#25345;&#12290;&#29992;&#25143;&#36890;&#36807;&#33258;&#28982;&#35821;&#38899;&#35810;&#38382;&#38382;&#39064;&#65292;&#21487;&#33021;&#21253;&#21547;&#19981;&#27969;&#30021;&#12289;&#38169;&#35823;&#21644;&#38750;&#27491;&#24335;&#30340;&#35821;&#27861;&#25110;&#25514;&#36766;&#12290;&#36825;&#26159;&#38382;&#31572;&#31995;&#32479;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#65292;&#23548;&#33268;&#26410;&#22238;&#31572;&#30340;&#38382;&#39064;&#25110;&#26080;&#20851;&#30340;&#31572;&#26696;&#65292;&#24182;&#23548;&#33268;&#29992;&#25143;&#20307;&#39564;&#24046;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#26410;&#25104;&#21151;&#22238;&#31572;&#30340;&#38382;&#31572;&#35831;&#27714;&#65292;&#20197;&#30830;&#23450;&#26680;&#24515;&#25361;&#25112;&#65306;&#35789;&#27719;&#24046;&#36317;&#12289;&#21629;&#39064;&#31867;&#22411;&#12289;&#22797;&#26434;&#30340;&#21477;&#27861;&#32467;&#26500;&#21644;&#39640;&#29305;&#24322;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35821;&#20041;&#38382;&#31572;&#37325;&#26500;&#27169;&#22411; (SURF)&#65292;&#25552;&#20379;&#19977;&#20010;&#22522;&#20110;&#35821;&#35328;&#23398;&#30340;&#25805;&#20316; (&#20462;&#22797;&#12289;&#21477;&#27861;&#37325;&#22609;&#12289;&#27010;&#25324;) &#26469;&#37325;&#20889;&#38382;&#39064;&#20197;&#20415;&#20110;&#22238;&#31572;&#12290;&#31163;&#32447;&#35780;&#20272;&#26469;&#33258;&#39046;&#20808;&#35821;&#38899;&#21161;&#25163;&#30340; 100 &#19975;&#20010;&#26410;&#22238;&#31572;&#30340;&#38382;&#39064;&#65292;&#32467;&#26524;&#26174;&#31034; SURF &#26174;&#33879;&#25552;&#39640;&#20102;&#22238;&#31572;&#29575;&#65306;&#39640;&#36798; 24% &#30340;&#38382;&#39064;&#33719;&#24471;&#20102;&#30456;&#20851;&#31572;&#26696; (75%)&#12290;&#23454;&#26102;&#37096;&#32626;&#26174;&#31034;&#20986;&#23545;&#25968;&#30334;&#19975;&#26377;&#26410;&#22238;&#31572;&#30340;&#38382;&#39064;&#30340;&#23458;&#25143;&#20135;&#29983;&#20102;&#31215;&#26497;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spoken Question Answering (QA) is a key feature of voice assistants, usually backed by multiple QA systems. Users ask questions via spontaneous speech which can contain disfluencies, errors, and informal syntax or phrasing. This is a major challenge in QA, causing unanswered questions or irrelevant answers, and leading to bad user experiences. We analyze failed QA requests to identify core challenges: lexical gaps, proposition types, complex syntactic structure, and high specificity. We propose a Semantic Question Reformulation (SURF) model offering three linguistically-grounded operations (repair, syntactic reshaping, generalization) to rewrite questions to facilitate answering. Offline evaluation on 1M unanswered questions from a leading voice assistant shows that SURF significantly improves answer rates: up to 24% of previously unanswered questions obtain relevant answers (75%). Live deployment shows positive impact for millions of customers with unanswered questions; explicit relev
&lt;/p&gt;</description></item><item><title>&#26080;&#30417;&#30563;NMT&#20013;&#30340;&#22797;&#21046;&#38382;&#39064;&#36890;&#24120;&#21457;&#29983;&#22312;&#36828;&#36317;&#31163;&#35821;&#31181;&#23545;&#20013;&#19988;&#20250;&#30452;&#25509;&#22797;&#21046;&#36755;&#20837;&#21477;&#23376;&#30340;&#37096;&#20998;&#20316;&#20026;&#32763;&#35793;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21253;&#21547;&#35821;&#35328;&#37492;&#21035;&#22120;&#25439;&#22833;&#30340;&#35757;&#32451;&#35745;&#21010;&#26469;&#32531;&#35299;&#35813;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;&#20302;&#36164;&#28304;&#35821;&#31181;&#30340;&#32763;&#35793;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.17182</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#22797;&#21046;&#38382;&#39064;&#65306;&#20855;&#26377;&#35821;&#35328;&#37492;&#21035;&#22120;&#25439;&#22833;&#30340;&#35757;&#32451;&#35745;&#21010;
&lt;/p&gt;
&lt;p&gt;
On the Copying Problem of Unsupervised NMT: A Training Schedule with a Language Discriminator Loss. (arXiv:2305.17182v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17182
&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;NMT&#20013;&#30340;&#22797;&#21046;&#38382;&#39064;&#36890;&#24120;&#21457;&#29983;&#22312;&#36828;&#36317;&#31163;&#35821;&#31181;&#23545;&#20013;&#19988;&#20250;&#30452;&#25509;&#22797;&#21046;&#36755;&#20837;&#21477;&#23376;&#30340;&#37096;&#20998;&#20316;&#20026;&#32763;&#35793;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21253;&#21547;&#35821;&#35328;&#37492;&#21035;&#22120;&#25439;&#22833;&#30340;&#35757;&#32451;&#35745;&#21010;&#26469;&#32531;&#35299;&#35813;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;&#20302;&#36164;&#28304;&#35821;&#31181;&#30340;&#32763;&#35793;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26080;&#30417;&#30563;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#24050;&#22312;&#35768;&#22810;&#35821;&#31181;&#38388;&#24471;&#21040;&#25104;&#21151;&#65292;&#20294;&#22797;&#21046;&#38382;&#39064;&#65288;&#21363;&#23558;&#36755;&#20837;&#21477;&#23376;&#30340;&#26576;&#20123;&#37096;&#20998;&#30452;&#25509;&#22797;&#21046;&#20316;&#20026;&#32763;&#35793;&#65289;&#22312;&#36828;&#36317;&#31163;&#35821;&#31181;&#23545;&#20013;&#24456;&#24120;&#35265;&#65292;&#23588;&#20854;&#28041;&#21450;&#20302;&#36164;&#28304;&#35821;&#31181;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20010;&#38382;&#39064;&#19982;&#22312;&#32447;&#22238;&#35793;&#65288;BT&#65289;&#26399;&#38388;&#20986;&#29616;&#30340;&#39044;&#26399;&#22797;&#21046;&#34892;&#20026;&#23494;&#20999;&#30456;&#20851;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#35757;&#32451;&#35745;&#21010;&#65292;&#23427;&#21253;&#21547;&#20102;&#19968;&#20010;&#35821;&#35328;&#37492;&#21035;&#22120;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#35813;&#25439;&#22833;&#26045;&#21152;&#32422;&#26463;&#20110;&#20013;&#38388;&#32763;&#35793;&#65292;&#20197;&#20351;&#32763;&#35793;&#26159;&#25152;&#38656;&#30340;&#35821;&#35328;&#12290;&#36890;&#36807;&#22312;&#19981;&#21516;&#35821;&#35328;&#23545;&#12289; &#21253;&#25324;&#30456;&#20284;&#21644;&#36828;&#36317;&#31163;&#12289;&#39640;&#36164;&#28304;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#24191;&#27867;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#32531;&#35299;&#20102;&#22797;&#21046;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#32763;&#35793;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although unsupervised neural machine translation (UNMT) has achieved success in many language pairs, the copying problem, i.e., directly copying some parts of the input sentence as the translation, is common among distant language pairs, especially when low-resource languages are involved. We find this issue is closely related to an unexpected copying behavior during online back-translation (BT). In this work, we propose a simple but effective training schedule that incorporates a language discriminator loss. The loss imposes constraints on the intermediate translation so that the translation is in the desired language. By conducting extensive experiments on different language pairs, including similar and distant, high and low-resource languages, we find that our method alleviates the copying problem, thus improving the translation performance on low-resource languages.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;LIVE&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35270;&#35273;&#22686;&#24378;&#23398;&#20064;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#30340;&#24819;&#35937;&#65292;&#24182;&#38024;&#23545;&#27599;&#20010;&#21477;&#23376;&#36827;&#34892;&#21160;&#24577;&#21512;&#25104;&#65292;&#22823;&#37327;&#23454;&#39564;&#27979;&#35797;&#34920;&#26126;&#23427;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#29983;&#25104;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.16944</link><description>&lt;p&gt;
&#23398;&#20064;&#24819;&#35937;&#65306;&#35270;&#35273;&#22686;&#24378;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Learning to Imagine: Visually-Augmented Natural Language Generation. (arXiv:2305.16944v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16944
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;LIVE&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35270;&#35273;&#22686;&#24378;&#23398;&#20064;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#30340;&#24819;&#35937;&#65292;&#24182;&#38024;&#23545;&#27599;&#20010;&#21477;&#23376;&#36827;&#34892;&#21160;&#24577;&#21512;&#25104;&#65292;&#22823;&#37327;&#23454;&#39564;&#27979;&#35797;&#34920;&#26126;&#23427;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#29983;&#25104;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#24448;&#24448;&#20250;&#24819;&#35937;&#30456;&#20851;&#22330;&#26223;&#26469;&#24110;&#21161;&#20889;&#20316;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#35270;&#35273;&#20449;&#24687;&#20197;&#19982;&#20154;&#31867;&#30456;&#21516;&#30340;&#26041;&#24335;&#36827;&#34892;&#21019;&#20316;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26041;&#27861;LIVE&#65292;&#20351;&#24471;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#33021;&#22815;&#23398;&#20064;&#36890;&#36807;&#35270;&#35273;&#22686;&#24378;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#30340;&#24819;&#35937; &#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22522;&#20110;&#25991;&#26412;&#24819;&#35937;&#22330;&#26223;&#65306;&#25105;&#20204;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#20381;&#25454;&#36755;&#20837;&#25991;&#26412;&#21512;&#25104;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;CLIP&#30830;&#23450;&#25991;&#26412;&#26159;&#21542;&#33021;&#20197;&#21518;&#39564;&#26041;&#24335;&#21796;&#36215;&#24819;&#35937;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30340;&#24819;&#35937;&#26159;&#21160;&#24577;&#30340;&#65292;&#25105;&#20204;&#20250;&#38024;&#23545;&#27599;&#20010;&#21477;&#23376;&#36827;&#34892;&#21512;&#25104;&#65292;&#32780;&#19981;&#26159;&#38024;&#23545;&#25972;&#20010;&#27573;&#33853;&#29983;&#25104;&#19968;&#24352;&#22270;&#20687;&#12290;&#22312;&#25216;&#26415;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21363;&#25554;&#21363;&#29992;&#34701;&#21512;&#23618;&#65292;&#20197;&#33719;&#21462;&#27599;&#20010;&#25991;&#26412;&#30340;&#35270;&#35273;&#22686;&#24378;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#35270;&#35273;-&#25991;&#26412;&#34701;&#21512;&#23618;&#19982;Transformer-based&#26550;&#26500;&#20860;&#23481;&#12290;&#25105;&#20204;&#20351;&#29992;BART&#21644;T5&#36827;&#34892;&#20102;&#22235;&#20010;&#29983;&#25104;&#20219;&#21153;&#30340;&#22823;&#37327;&#23454;&#39564;&#27979;&#35797;&#65292;&#33258;&#21160;&#32467;&#26524;&#21644;&#20154;&#31867;&#35780;&#20272;&#37117;&#34920;&#26126;LIVE&#33021;&#22815;&#26377;&#25928;&#25552;&#39640;&#29983;&#25104;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
People often imagine relevant scenes to aid in the writing process. In this work, we aim to utilize visual information for composition in the same manner as humans. We propose a method, LIVE, that makes pre-trained language models (PLMs) Learn to Imagine for Visuallyaugmented natural language gEneration. First, we imagine the scene based on the text: we use a diffusion model to synthesize high-quality images conditioned on the input texts. Second, we use CLIP to determine whether the text can evoke the imagination in a posterior way. Finally, our imagination is dynamic, and we conduct synthesis for each sentence rather than generate only one image for an entire paragraph. Technically, we propose a novel plug-and-play fusion layer to obtain visually-augmented representations for each text. Our vision-text fusion layer is compatible with Transformerbased architecture. We have conducted extensive experiments on four generation tasks using BART and T5, and the automatic results and human e
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;93&#21517;NLP&#21021;&#23398;&#32773;&#30340;&#35843;&#26597;&#65292;&#21457;&#29616;&#30740;&#31350;&#20316;&#32773;&#25552;&#20379;&#23436;&#25972;&#25991;&#26723;&#12289;&#26356;&#22909;&#30340;&#20195;&#30721;&#23454;&#36341;&#21644;&#26356;&#26131;&#20110;&#33719;&#21462;&#30340;&#25968;&#25454;&#25991;&#20214;&#26159;&#21021;&#23398;&#32773;&#25104;&#21151;&#22797;&#29616;&#26368;&#36817;NLP&#35770;&#25991;&#32467;&#26524;&#30340;&#20851;&#38190;&#65292;&#24314;&#35758;NLP&#30740;&#31350;&#20154;&#21592;&#27880;&#37325;&#36825;&#20123;&#26041;&#38754;&#65292;&#26356;&#22909;&#22320;&#25903;&#25345;&#21021;&#23398;&#32773;&#12290;</title><link>http://arxiv.org/abs/2305.16579</link><description>&lt;p&gt;
&#20154;&#20154;&#21487;&#22797;&#29616;&#30340;NLP&#30740;&#31350;&#65306;&#21021;&#23398;&#32773;&#30340;&#38656;&#27714;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
NLP Reproducibility For All: Understanding Experiences of Beginners. (arXiv:2305.16579v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16579
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;93&#21517;NLP&#21021;&#23398;&#32773;&#30340;&#35843;&#26597;&#65292;&#21457;&#29616;&#30740;&#31350;&#20316;&#32773;&#25552;&#20379;&#23436;&#25972;&#25991;&#26723;&#12289;&#26356;&#22909;&#30340;&#20195;&#30721;&#23454;&#36341;&#21644;&#26356;&#26131;&#20110;&#33719;&#21462;&#30340;&#25968;&#25454;&#25991;&#20214;&#26159;&#21021;&#23398;&#32773;&#25104;&#21151;&#22797;&#29616;&#26368;&#36817;NLP&#35770;&#25991;&#32467;&#26524;&#30340;&#20851;&#38190;&#65292;&#24314;&#35758;NLP&#30740;&#31350;&#20154;&#21592;&#27880;&#37325;&#36825;&#20123;&#26041;&#38754;&#65292;&#26356;&#22909;&#22320;&#25903;&#25345;&#21021;&#23398;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#36817;&#24180;&#26469;&#24322;&#24120;&#28779;&#29190;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#24613;&#20110;&#36827;&#20837;&#35813;&#39046;&#22495;&#65292;&#20294;&#30446;&#21069;&#30340;&#30740;&#31350;&#22797;&#29616;&#21162;&#21147;&#26159;&#21542;&#36275;&#20197;&#35753;&#36825;&#20123;&#21021;&#23398;&#32773;&#24212;&#29992;&#26368;&#26032;&#30340;&#36827;&#23637;&#36824;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#20102;&#35299;&#21021;&#23398;&#32773;&#30340;&#38656;&#27714;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#20171;&#32461;&#24615;&#30340;NLP&#35838;&#31243;&#20013;&#24320;&#23637;&#20102;&#19968;&#39033;&#30740;&#31350;&#65292;&#35753;&#23398;&#29983;&#22797;&#29616;&#26368;&#36817;NLP&#35770;&#25991;&#30340;&#32467;&#26524;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#20182;&#20204;&#30340;&#32534;&#31243;&#25216;&#33021;&#21644;&#23545;&#30740;&#31350;&#35770;&#25991;&#30340;&#29702;&#35299;&#23545;&#23436;&#25104;&#32451;&#20064;&#30340;&#20184;&#20986;&#20165;&#26377;&#38480;&#30340;&#24433;&#21709;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;&#30740;&#31350;&#20316;&#32773;&#30340;&#21487;&#35775;&#38382;&#24615;&#21162;&#21147;&#26159;&#25104;&#21151;&#30340;&#20851;&#38190;&#65292;&#21253;&#25324;&#23436;&#25972;&#30340;&#25991;&#26723;&#12289;&#26356;&#22909;&#30340;&#32534;&#30721;&#23454;&#36341;&#21644;&#26356;&#23481;&#26131;&#33719;&#21462;&#30340;&#25968;&#25454;&#25991;&#20214;&#12290;&#21069;&#36827;&#26102;&#65292;&#25105;&#20204;&#24314;&#35758;NLP&#30740;&#31350;&#20154;&#21592;&#23494;&#20999;&#20851;&#27880;&#36825;&#20123;&#24320;&#28304;&#24037;&#20316;&#30340;&#31616;&#21333;&#26041;&#38754;&#65292;&#24182;&#20351;&#29992;&#21021;&#23398;&#32773;&#30340;&#21453;&#39304;&#35265;&#35299;&#25552;&#20379;&#21487;&#25805;&#20316;&#30340;&#24819;&#27861;&#20197;&#26356;&#22909;&#22320;&#25903;&#25345;&#20182;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
As natural language processing (NLP) has recently seen an unprecedented level of excitement, and more people are eager to enter the field, it is unclear whether current research reproducibility efforts are sufficient for this group of beginners to apply the latest developments. To understand their needs, we conducted a study with 93 students in an introductory NLP course, where students reproduced the results of recent NLP papers. Surprisingly, we find that their programming skill and comprehension of research papers have a limited impact on their effort spent completing the exercise. Instead, we find accessibility efforts by research authors to be the key to success, including complete documentation, better coding practice, and easier access to data files. Going forward, we recommend that NLP researchers pay close attention to these simple aspects of open-sourcing their work, and use insights from beginners' feedback to provide actionable ideas on how to better support them.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29305;&#24322;&#24615;&#21644;&#24773;&#24863;&#20004;&#20010;&#35821;&#29992;&#29305;&#24449;&#22312;&#19981;&#21516;&#32676;&#20307;&#32972;&#26223;&#19979;&#26159;&#21542;&#20250;&#20986;&#29616;&#31995;&#32479;&#24615;&#21464;&#21270;&#65292;&#24182;&#23558;&#20854;&#19982;&#33258;&#28982;&#35821;&#35328;&#36755;&#20986;&#20013;&#30340;&#26032;&#32676;&#20307;&#20559;&#35265;&#26694;&#26550;&#30456;&#32852;&#31995;&#12290;&#21021;&#27493;&#20998;&#26512;&#34920;&#26126;&#65292;&#25512;&#25991;&#30340;&#29305;&#24322;&#24615;&#21644;&#24773;&#24863;&#31243;&#24230;&#19982;&#30417;&#30563;&#30340;&#32676;&#20307;&#20851;&#31995;&#26631;&#31614;&#20043;&#38388;&#23384;&#22312;&#19968;&#23450;&#30340;&#30456;&#20851;&#24615;&#65292;&#32780;&#22240;&#26524;&#25512;&#26029;&#25581;&#31034;&#20102;&#31070;&#32463;&#27169;&#22411;&#22312;&#20998;&#31867;&#26102;&#21487;&#38752;&#22320;&#20351;&#29992;&#24773;&#24863;&#65292;&#20294;&#20854;&#23545;&#29305;&#24322;&#24615;&#30340;&#20351;&#29992;&#26159;&#19981;&#30830;&#23450;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.16409</link><description>&lt;p&gt;
&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#30740;&#31350;&#29305;&#24322;&#24615;&#21644;&#24773;&#24863;&#23545;&#32676;&#20307;&#20559;&#35265;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Probing for the influence of affect and specificity on Intergroup Bias. (arXiv:2305.16409v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16409
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29305;&#24322;&#24615;&#21644;&#24773;&#24863;&#20004;&#20010;&#35821;&#29992;&#29305;&#24449;&#22312;&#19981;&#21516;&#32676;&#20307;&#32972;&#26223;&#19979;&#26159;&#21542;&#20250;&#20986;&#29616;&#31995;&#32479;&#24615;&#21464;&#21270;&#65292;&#24182;&#23558;&#20854;&#19982;&#33258;&#28982;&#35821;&#35328;&#36755;&#20986;&#20013;&#30340;&#26032;&#32676;&#20307;&#20559;&#35265;&#26694;&#26550;&#30456;&#32852;&#31995;&#12290;&#21021;&#27493;&#20998;&#26512;&#34920;&#26126;&#65292;&#25512;&#25991;&#30340;&#29305;&#24322;&#24615;&#21644;&#24773;&#24863;&#31243;&#24230;&#19982;&#30417;&#30563;&#30340;&#32676;&#20307;&#20851;&#31995;&#26631;&#31614;&#20043;&#38388;&#23384;&#22312;&#19968;&#23450;&#30340;&#30456;&#20851;&#24615;&#65292;&#32780;&#22240;&#26524;&#25512;&#26029;&#25581;&#31034;&#20102;&#31070;&#32463;&#27169;&#22411;&#22312;&#20998;&#31867;&#26102;&#21487;&#38752;&#22320;&#20351;&#29992;&#24773;&#24863;&#65292;&#20294;&#20854;&#23545;&#29305;&#24322;&#24615;&#30340;&#20351;&#29992;&#26159;&#19981;&#30830;&#23450;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#30740;&#31350;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#20559;&#35265;&#38382;&#39064;&#36890;&#24120;&#38598;&#20013;&#22312;&#36127;&#38754;&#25110;&#36140;&#25439;&#35821;&#35328;&#30340;&#20351;&#29992;&#19978;&#65292;&#20294;Govindarajan&#31561;&#20154;&#65288;2023&#65289;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20559;&#35265;&#26694;&#26550;&#65292;&#21363;&#20197;&#32676;&#20307;&#31038;&#20250;&#32972;&#26223;&#20026;&#22522;&#30784;&#65292;&#30740;&#31350;&#20854;&#23545;&#35821;&#35328;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#20010;&#35821;&#29992;&#29305;&#24449;&#65288;&#29305;&#24322;&#24615;&#21644;&#24773;&#24863;&#65289;&#22312;&#19981;&#21516;&#30340;&#32676;&#20307;&#19978;&#19979;&#25991;&#20013;&#26159;&#21542;&#20250;&#31995;&#32479;&#24615;&#22320;&#21464;&#21270;&#65292;&#20174;&#32780;&#23558;&#36825;&#20010;&#26032;&#30340;&#20559;&#35265;&#26694;&#26550;&#19982;&#35821;&#35328;&#36755;&#20986;&#36830;&#25509;&#36215;&#26469;&#12290;&#21021;&#27493;&#30340;&#20998;&#26512;&#21457;&#29616;&#65292;&#25512;&#25991;&#30340;&#29305;&#24322;&#24615;&#21644;&#24773;&#24863;&#31243;&#24230;&#19982;&#30417;&#30563;&#30340;&#32676;&#20307;&#20851;&#31995;&#65288;IGR&#65289;&#26631;&#31614;&#20043;&#38388;&#23384;&#22312;&#36866;&#24230;&#30340;&#30456;&#20851;&#24615;&#12290;&#22240;&#26524;&#25512;&#26029;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#65292;&#34429;&#28982;&#34987;&#31934;&#35843;&#20026;IGR&#26631;&#31614;&#39044;&#27979;&#30340;&#31070;&#32463;&#27169;&#22411;&#21487;&#38752;&#22320;&#20351;&#29992;&#24773;&#24863;&#36827;&#34892;&#20998;&#31867;&#65292;&#20294;&#27169;&#22411;&#20351;&#29992;&#29305;&#24322;&#24615;&#26159;&#19981;&#30830;&#23450;&#30340;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#21487;&#20197;&#22312;&#20197;&#19979;&#32593;&#22336;&#25214;&#21040;&#65306;https://github.com/venkatasg/intergroup-probing
&lt;/p&gt;
&lt;p&gt;
While existing work on studying bias in NLP focues on negative or pejorative language use, Govindarajan et al. (2023) offer a revised framing of bias in terms of intergroup social context, and its effects on language behavior. In this paper, we investigate if two pragmatic features (specificity and affect) systematically vary in different intergroup contexts -- thus connecting this new framing of bias to language output. Preliminary analysis finds modest correlations between specificity and affect of tweets with supervised intergroup relationship (IGR) labels. Counterfactual probing further reveals that while neural models finetuned for predicting IGR labels reliably use affect in classification, the model's usage of specificity is inconclusive. Code and data can be found at: https://github.com/venkatasg/intergroup-probing
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;1&#23618;Transformer&#22312;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;SGD&#35757;&#32451;&#21160;&#24577;&#65292;&#35777;&#26126;&#20102;&#33258;&#25105;&#20851;&#27880;&#23618;&#20805;&#24403;&#20102;&#8220;&#21306;&#20998;&#24615;&#25195;&#25551;&#31639;&#27861;&#8221;&#65292;&#20174;&#32780;&#36880;&#27493;&#20851;&#27880;&#21040;&#30456;&#20851;&#26631;&#35760;&#24182;&#25490;&#38500;&#19981;&#30456;&#20851;&#30340;&#26631;&#35760;&#65292;&#24635;&#32467;&#30456;&#20851;&#20449;&#24687;&#22312;&#32534;&#30721;&#34920;&#31034;&#20013;&#12290;&#21516;&#26102;&#30740;&#31350;&#20102;&#26631;&#35760;&#39057;&#29575;&#12289;&#19978;&#19979;&#25991;&#21644;&#21021;&#22987;&#21270;&#33258;&#25105;&#20851;&#27880;&#23618;&#31561;&#23545;Transformer&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.16380</link><description>&lt;p&gt;
&#25195;&#25551;&#19982;&#25293;&#29031;&#65306;&#29702;&#35299;1&#23618;Transformer&#20013;&#30340;&#35757;&#32451;&#21160;&#24577;&#21644;&#26631;&#35760;&#32452;&#25104;
&lt;/p&gt;
&lt;p&gt;
Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer Transformer. (arXiv:2305.16380v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;1&#23618;Transformer&#22312;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;SGD&#35757;&#32451;&#21160;&#24577;&#65292;&#35777;&#26126;&#20102;&#33258;&#25105;&#20851;&#27880;&#23618;&#20805;&#24403;&#20102;&#8220;&#21306;&#20998;&#24615;&#25195;&#25551;&#31639;&#27861;&#8221;&#65292;&#20174;&#32780;&#36880;&#27493;&#20851;&#27880;&#21040;&#30456;&#20851;&#26631;&#35760;&#24182;&#25490;&#38500;&#19981;&#30456;&#20851;&#30340;&#26631;&#35760;&#65292;&#24635;&#32467;&#30456;&#20851;&#20449;&#24687;&#22312;&#32534;&#30721;&#34920;&#31034;&#20013;&#12290;&#21516;&#26102;&#30740;&#31350;&#20102;&#26631;&#35760;&#39057;&#29575;&#12289;&#19978;&#19979;&#25991;&#21644;&#21021;&#22987;&#21270;&#33258;&#25105;&#20851;&#27880;&#23618;&#31561;&#23545;Transformer&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26550;&#26500;&#22312;&#22810;&#20010;&#30740;&#31350;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#24615;&#33021;&#65292;&#24182;&#25104;&#20026;&#35768;&#22810;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;&#20854;&#22914;&#20309;&#24037;&#20316;&#30340;&#29702;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#29305;&#21035;&#26159;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#39044;&#27979;&#24615;&#25439;&#22833;&#65292;&#34920;&#31034;&#22914;&#20309;&#20174;&#26799;&#24230;&#35757;&#32451;&#21160;&#24577;&#20013;&#20986;&#29616;&#20173;&#28982;&#26159;&#19968;&#20010;&#35868;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#38024;&#23545;&#20855;&#26377;&#19968;&#20010;&#33258;&#25105;&#20851;&#27880;&#23618;&#21644;&#19968;&#20010;&#35299;&#30721;&#22120;&#23618;&#30340;1&#23618;Transformer&#65292;&#25105;&#20204;&#20197;&#25968;&#23398;&#20005;&#35880;&#30340;&#26041;&#24335;&#20998;&#26512;&#20854;&#22312;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;SGD&#35757;&#32451;&#21160;&#24577;&#12290;&#25105;&#20204;&#25171;&#24320;&#20102;&#33258;&#25105;&#20851;&#27880;&#23618;&#32452;&#21512;&#36755;&#20837;&#26631;&#35760;&#30340;&#21160;&#24577;&#36807;&#31243;&#30340;&#40657;&#30418;&#23376;&#65292;&#24182;&#25581;&#31034;&#20102;&#24213;&#23618;&#24402;&#32435;&#20559;&#24046;&#30340;&#26412;&#36136;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#27809;&#26377;&#20301;&#32622;&#32534;&#30721;&#12289;&#38271;&#36755;&#20837;&#24207;&#21015;&#21644;&#35299;&#30721;&#22120;&#23618;&#23398;&#20064;&#36895;&#24230;&#24555;&#20110;&#33258;&#25105;&#20851;&#27880;&#23618;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#33258;&#25105;&#20851;&#27880;&#23618;&#20805;&#24403;&#20102;&#8220;&#21306;&#20998;&#24615;&#25195;&#25551;&#31639;&#27861;&#8221;&#65306;&#20174;&#22343;&#21248;&#27880;&#24847;&#21147;&#24320;&#22987;&#65292;&#23427;&#36880;&#28176;&#20851;&#27880;&#21040;&#30456;&#20851;&#26631;&#35760;&#65292;&#25490;&#38500;&#19981;&#30456;&#20851;&#30340;&#26631;&#35760;&#65292;&#30452;&#21040;&#25152;&#26377;&#30456;&#20851;&#20449;&#24687;&#34987;&#25195;&#25551;&#24182;&#24635;&#32467;&#22312;&#32534;&#30721;&#34920;&#31034;&#20013;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#36824;&#26174;&#31034;&#20102;&#26631;&#35760;&#39057;&#29575;&#21644;&#19978;&#19979;&#25991;&#22914;&#20309;&#24433;&#21709;&#27880;&#24847;&#26435;&#37325;&#65292;&#20197;&#21450;&#33258;&#25105;&#20851;&#27880;&#23618;&#21021;&#22987;&#21270;&#22914;&#20309;&#24433;&#21709;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer architecture has shown impressive performance in multiple research domains and has become the backbone of many neural network models. However, there is limited understanding on how it works. In particular, with a simple predictive loss, how the representation emerges from the gradient \emph{training dynamics} remains a mystery. In this paper, for 1-layer transformer with one self-attention layer plus one decoder layer, we analyze its SGD training dynamics for the task of next token prediction in a mathematically rigorous manner. We open the black box of the dynamic process of how the self-attention layer combines input tokens, and reveal the nature of underlying inductive bias. More specifically, with the assumption (a) no positional encoding, (b) long input sequence, and (c) the decoder layer learns faster than the self-attention layer, we prove that self-attention acts as a \emph{discriminative scanning algorithm}: starting from uniform attention, it gradually attends mor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ContProto&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#33258;&#25105;&#35757;&#32451;&#21644;&#22522;&#20110;&#21407;&#22411;&#30340;&#20266;&#26631;&#35760;&#65292;&#32467;&#21512;&#34920;&#31034;&#23398;&#20064;&#21644;&#20266;&#26631;&#31614;&#31934;&#21270;&#65292;&#22312;&#19968;&#20010;&#19968;&#33268;&#30340;&#26694;&#26550;&#20013;&#25552;&#39640;&#20102;&#36328;&#35821;&#35328;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#33258;&#35757;&#32451;&#26041;&#27861;&#65307;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ContProto &#26041;&#27861;&#22312;&#22810;&#20010;&#36716;&#31227;&#23545;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.13628</link><description>&lt;p&gt;
&#21033;&#29992;&#23545;&#27604;&#33258;&#25105;&#35757;&#32451;&#21644;&#21407;&#22411;&#23398;&#20064;&#25913;&#36827;&#36328;&#35821;&#35328;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#33258;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improving Self-training for Cross-lingual Named Entity Recognition with Contrastive and Prototype Learning. (arXiv:2305.13628v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13628
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ContProto&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#33258;&#25105;&#35757;&#32451;&#21644;&#22522;&#20110;&#21407;&#22411;&#30340;&#20266;&#26631;&#35760;&#65292;&#32467;&#21512;&#34920;&#31034;&#23398;&#20064;&#21644;&#20266;&#26631;&#31614;&#31934;&#21270;&#65292;&#22312;&#19968;&#20010;&#19968;&#33268;&#30340;&#26694;&#26550;&#20013;&#25552;&#39640;&#20102;&#36328;&#35821;&#35328;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#33258;&#35757;&#32451;&#26041;&#27861;&#65307;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ContProto &#26041;&#27861;&#22312;&#22810;&#20010;&#36716;&#31227;&#23545;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36328;&#35821;&#35328;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#65292;&#33258;&#35757;&#32451;&#36890;&#24120;&#29992;&#20110;&#36890;&#36807;&#22312;&#20266;&#26631;&#35760;&#30446;&#26631;&#35821;&#35328;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#26469;&#24357;&#21512;&#35821;&#35328;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#30446;&#26631;&#35821;&#35328;&#24615;&#33021;&#19981;&#20339;&#65292;&#20266;&#26631;&#31614;&#36890;&#24120;&#23384;&#22312;&#22122;&#22768;&#65292;&#38480;&#21046;&#20102;&#25972;&#20307;&#24615;&#33021;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#22312;&#19968;&#20010;&#19968;&#33268;&#30340;&#26694;&#26550;&#20013;&#32467;&#21512;&#34920;&#31034;&#23398;&#20064;&#21644;&#20266;&#26631;&#31614;&#31934;&#21270;&#26469;&#25913;&#36827;&#36328;&#35821;&#35328;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#33258;&#35757;&#32451;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20027;&#35201;&#21253;&#25324;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;&#65288;1&#65289;&#23545;&#27604;&#33258;&#25105;&#35757;&#32451;&#21644;&#65288;2&#65289;&#22522;&#20110;&#21407;&#22411;&#30340;&#20266;&#26631;&#35760;&#12290;&#25105;&#20204;&#30340;&#23545;&#27604;&#33258;&#25105;&#35757;&#32451;&#36890;&#36807;&#20998;&#31163;&#19981;&#21516;&#31867;&#21035;&#30340;&#32858;&#31867;&#26469;&#20419;&#36827;&#36328;&#35821;&#35328;&#36716;&#31227;&#65292;&#24182;&#36890;&#36807;&#20135;&#29983;&#28304;&#35821;&#35328;&#21644;&#30446;&#26631;&#35821;&#35328;&#20043;&#38388;&#32039;&#23494;&#23545;&#40784;&#30340;&#34920;&#31034;&#26469;&#22686;&#24378;&#36328;&#35821;&#35328;&#21487;&#36716;&#31227;&#24615;&#12290;&#21516;&#26102;&#65292;&#22522;&#20110;&#21407;&#22411;&#30340;&#20266;&#26631;&#35760;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26377;&#25928;&#25552;&#39640;&#20102;&#20266;&#26631;&#31614;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#36716;&#31227;&#23545;&#19978;&#35780;&#20272;ContProto&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
In cross-lingual named entity recognition (NER), self-training is commonly used to bridge the linguistic gap by training on pseudo-labeled target-language data. However, due to sub-optimal performance on target languages, the pseudo labels are often noisy and limit the overall performance. In this work, we aim to improve self-training for cross-lingual NER by combining representation learning and pseudo label refinement in one coherent framework. Our proposed method, namely ContProto mainly comprises two components: (1) contrastive self-training and (2) prototype-based pseudo-labeling. Our contrastive self-training facilitates span classification by separating clusters of different classes, and enhances cross-lingual transferability by producing closely-aligned representations between the source and target language. Meanwhile, prototype-based pseudo-labeling effectively improves the accuracy of pseudo labels during training. We evaluate ContProto on multiple transfer pairs, and experim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#20559;&#30340;UD&#26641;&#36328;&#35821;&#35328;&#20851;&#31995;&#25277;&#21462;&#36716;&#31227;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#28151;&#21512;&#32534;&#30721;UD&#26641;&#65292;&#26377;&#21161;&#20110;&#24357;&#21512;&#35757;&#32451;&#21644;&#39044;&#27979;&#38454;&#27573;&#20043;&#38388;&#24046;&#36317;&#20174;&#32780;&#23454;&#29616;&#26174;&#33879;&#30340;&#36328;&#35821;&#35328;&#20851;&#31995;&#25277;&#21462;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.12258</link><description>&lt;p&gt;
&#20026;&#26080;&#20559;&#30340;&#36328;&#35821;&#35328;&#20851;&#31995;&#25277;&#21462;&#26500;&#24314;&#28151;&#21512;&#32534;&#30721;&#30340;&#36890;&#29992;&#20381;&#23384;&#26862;&#26519;
&lt;/p&gt;
&lt;p&gt;
Constructing Code-mixed Universal Dependency Forest for Unbiased Cross-lingual Relation Extraction. (arXiv:2305.12258v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12258
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#20559;&#30340;UD&#26641;&#36328;&#35821;&#35328;&#20851;&#31995;&#25277;&#21462;&#36716;&#31227;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#28151;&#21512;&#32534;&#30721;UD&#26641;&#65292;&#26377;&#21161;&#20110;&#24357;&#21512;&#35757;&#32451;&#21644;&#39044;&#27979;&#38454;&#27573;&#20043;&#38388;&#24046;&#36317;&#20174;&#32780;&#23454;&#29616;&#26174;&#33879;&#30340;&#36328;&#35821;&#35328;&#20851;&#31995;&#25277;&#21462;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#36328;&#35821;&#35328;&#20851;&#31995;&#25277;&#21462;&#30740;&#31350;&#37319;&#29992;&#36890;&#29992;&#20381;&#23384;&#65288;UD&#65289;&#36164;&#28304;&#30340;&#35821;&#35328;&#19968;&#33268;&#24615;&#32467;&#26500;&#29305;&#24449;&#65292;&#20294;&#30001;&#20110;&#35821;&#35328;&#30340;&#19981;&#21487;&#36991;&#20813;&#24046;&#24322;&#65292;&#24456;&#23481;&#26131;&#36973;&#21463;&#21463;&#20559;&#36716;&#31227;&#65288;&#20363;&#22914;&#30446;&#26631;&#20559;&#24046;&#25110;&#28304;&#20559;&#24046;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#19968;&#31181;&#31867;&#22411;&#30340;&#28151;&#21512;&#32534;&#30721;UD&#26641;&#65292;&#30740;&#31350;&#20102;&#19968;&#31181;&#26080;&#20559;&#30340;UD&#26641;&#36328;&#35821;&#35328;&#20851;&#31995;&#25277;&#21462;&#36716;&#31227;&#12290;&#39318;&#20808;&#23558;&#28304;&#35821;&#35328;&#30340;&#21477;&#23376;&#32763;&#35793;&#25104;&#24179;&#34892;&#30340;&#30446;&#26631;&#35821;&#35328;&#65292;&#23545;&#20004;&#31181;&#35821;&#35328;&#37117;&#20998;&#21035;&#35299;&#26512;UD&#26641;&#65292;&#28982;&#21518;&#23558;&#28304;&#35821;&#35328;/&#30446;&#26631;&#35821;&#35328;&#30340;UD&#32467;&#26500;&#21512;&#24182;&#20026;&#32479;&#19968;&#30340;&#28151;&#21512;&#32534;&#30721;UD&#26641;&#12290;&#36890;&#36807;&#36825;&#26679;&#30340;&#26862;&#26519;&#29305;&#24449;&#65292;UD&#26641;&#30340;&#36328;&#35821;&#35328;&#20851;&#31995;&#25277;&#21462;&#35757;&#32451;&#21644;&#39044;&#27979;&#38454;&#27573;&#20043;&#38388;&#30340;&#24046;&#36317;&#21487;&#20197;&#26377;&#25928;&#32553;&#23567;&#12290;&#25105;&#20204;&#22312;ACE XRE&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#28151;&#21512;&#32534;&#30721;UD&#26862;&#26519;&#26377;&#21161;&#20110;&#26080;&#20559;&#30340;UD&#26641;&#36328;&#35821;&#35328;&#20851;&#31995;&#25277;&#21462;&#36716;&#31227;&#65292;&#24182;&#23454;&#29616;&#20102;&#26174;&#30528;&#30340;&#36328;&#35821;&#35328;&#20851;&#31995;&#25277;&#21462;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Latest efforts on cross-lingual relation extraction (XRE) aggressively leverage the language-consistent structural features from the universal dependency (UD) resource, while they may largely suffer from biased transfer (e.g., either target-biased or source-biased) due to the inevitable linguistic disparity between languages. In this work, we investigate an unbiased UD-based XRE transfer by constructing a type of code-mixed UD forest. We first translate the sentence of the source language to the parallel target-side language, for both of which we parse the UD tree respectively. Then, we merge the source-/target-side UD structures as a unified code-mixed UD forest. With such forest features, the gaps of UD-based XRE between the training and predicting phases can be effectively closed. We conduct experiments on the ACE XRE benchmark datasets, where the results demonstrate that the proposed code-mixed UD forests help unbiased UD-based XRE transfer, with which we achieve significant XRE pe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24605;&#32500;&#38142;&#32034;&#24341;&#30340;&#38544;&#24335;&#24773;&#24863;&#25512;&#26029;&#26694;&#26550;&#65288;THOR&#65289;&#65292;&#36890;&#36807;&#19977;&#27425;&#36339;&#25512;&#29702;&#27169;&#20223;&#31867;&#20154;&#25512;&#29702;&#36807;&#31243;&#65292;&#25903;&#25345;&#24120;&#35782;&#21644;&#22810;&#36339;&#25512;&#29702;&#20197;&#25512;&#26029;&#24847;&#35265;&#30340;&#28508;&#22312;&#24847;&#22270;&#65292;&#24182;&#36880;&#27493;&#35825;&#23548;&#38544;&#24335;&#26041;&#38754;&#12289;&#24847;&#35265;&#21644;&#26368;&#32456;&#24773;&#24863;&#26497;&#24615;&#65292;&#23454;&#29616;&#20102;&#22312;&#30417;&#30563;&#21644;&#38646;&#26679;&#26412;&#35774;&#32622;&#19978;&#22823;&#24133;&#25552;&#39640;&#25216;&#26415;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2305.11255</link><description>&lt;p&gt;
&#22522;&#20110;&#24605;&#32500;&#38142;&#32034;&#24341;&#30340;&#38544;&#24335;&#24773;&#24863;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Reasoning Implicit Sentiment with Chain-of-Thought Prompting. (arXiv:2305.11255v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24605;&#32500;&#38142;&#32034;&#24341;&#30340;&#38544;&#24335;&#24773;&#24863;&#25512;&#26029;&#26694;&#26550;&#65288;THOR&#65289;&#65292;&#36890;&#36807;&#19977;&#27425;&#36339;&#25512;&#29702;&#27169;&#20223;&#31867;&#20154;&#25512;&#29702;&#36807;&#31243;&#65292;&#25903;&#25345;&#24120;&#35782;&#21644;&#22810;&#36339;&#25512;&#29702;&#20197;&#25512;&#26029;&#24847;&#35265;&#30340;&#28508;&#22312;&#24847;&#22270;&#65292;&#24182;&#36880;&#27493;&#35825;&#23548;&#38544;&#24335;&#26041;&#38754;&#12289;&#24847;&#35265;&#21644;&#26368;&#32456;&#24773;&#24863;&#26497;&#24615;&#65292;&#23454;&#29616;&#20102;&#22312;&#30417;&#30563;&#21644;&#38646;&#26679;&#26412;&#35774;&#32622;&#19978;&#22823;&#24133;&#25552;&#39640;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#20998;&#26512;&#31995;&#32479;&#36890;&#36807;&#20998;&#26512;&#36755;&#20837;&#25991;&#26412;&#20013;&#30340;&#20851;&#38190;&#35266;&#28857;&#34920;&#36798;&#26469;&#30830;&#23450;&#32473;&#23450;&#30446;&#26631;&#30340;&#24773;&#24863;&#26497;&#24615;&#65292;&#32780;&#22312;&#38544;&#24335;&#24773;&#24863;&#20998;&#26512;&#65288;ISA&#65289;&#20013;&#65292;&#35266;&#28857;&#25552;&#31034;&#20197;&#19968;&#31181;&#38544;&#21547;&#21644;&#27169;&#31946;&#30340;&#26041;&#24335;&#20986;&#29616;&#12290;&#22240;&#27492;&#65292;&#26816;&#27979;&#38544;&#24335;&#24773;&#24863;&#38656;&#35201;&#24120;&#35782;&#21644;&#22810;&#36339;&#25512;&#29702;&#33021;&#21147;&#26469;&#25512;&#26029;&#24847;&#35265;&#30340;&#28508;&#22312;&#24847;&#22270;&#12290;&#21463;&#26368;&#36817;&#24605;&#32500;&#38142;&#32034;&#24341;&#65288;CoT&#65289;&#24605;&#24819;&#30340;&#21551;&#21457;&#65292;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#19977;&#27425;&#36339;&#25512;&#29702;&#65288;THOR&#65289;CoT&#26694;&#26550;&#65292;&#27169;&#20223;ISA&#30340;&#31867;&#20154;&#25512;&#29702;&#36807;&#31243;&#12290;&#25105;&#20204;&#20026;THOR&#35774;&#35745;&#20102;&#19968;&#20010;&#19977;&#27493;&#25552;&#31034;&#21407;&#21017;&#65292;&#20197;&#36880;&#27493;&#35825;&#23548;&#38544;&#24335;&#26041;&#38754;&#12289;&#24847;&#35265;&#21644;&#26368;&#32456;&#24773;&#24863;&#26497;&#24615;&#12290;&#25105;&#20204;&#30340;THOR+Flan-T5&#65288;11B&#65289;&#22312;&#30417;&#30563;&#35774;&#32622;&#19978;&#23558;&#25216;&#26415;&#27700;&#24179;&#25512;&#36827;&#20102;&#36229;&#36807;6&#65285;&#30340;F1&#20540;&#12290;&#26356;&#20026;&#26174;&#33879;&#30340;&#26159;&#65292;THOR+GPT3&#65288;175B&#65289;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19978;&#23558;&#25216;&#26415;&#27700;&#24179;&#25552;&#21319;&#20102;&#36229;&#36807;50&#65285;&#30340;F1&#20540;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#20301;&#20110;https://github.com/scofield7419/THOR-ISA &#12290;
&lt;/p&gt;
&lt;p&gt;
While sentiment analysis systems try to determine the sentiment polarities of given targets based on the key opinion expressions in input texts, in implicit sentiment analysis (ISA) the opinion cues come in an implicit and obscure manner. Thus detecting implicit sentiment requires the common-sense and multi-hop reasoning ability to infer the latent intent of opinion. Inspired by the recent chain-of-thought (CoT) idea, in this work we introduce a Three-hop Reasoning (THOR) CoT framework to mimic the human-like reasoning process for ISA. We design a three-step prompting principle for THOR to step-by-step induce the implicit aspect, opinion, and finally the sentiment polarity. Our THOR+Flan-T5 (11B) pushes the state-of-the-art (SoTA) by over 6% F1 on supervised setup. More strikingly, THOR+GPT3 (175B) boosts the SoTA by over 50% F1 on zero-shot setting. Our code is at https://github.com/scofield7419/THOR-ISA.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#35821;&#26469;&#26377;&#25928;&#35268;&#36991;&#29616;&#26377;&#30340;&#25991;&#26412;&#26816;&#27979;&#31995;&#32479;&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#26816;&#27979;&#22120;&#30340;&#33030;&#24369;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10847</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#34987;&#24341;&#23548;&#26469;&#35268;&#36991;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Large Language Models can be Guided to Evade AI-Generated Text Detection. (arXiv:2305.10847v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10847
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#35821;&#26469;&#26377;&#25928;&#35268;&#36991;&#29616;&#26377;&#30340;&#25991;&#26412;&#26816;&#27979;&#31995;&#32479;&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#26816;&#27979;&#22120;&#30340;&#33030;&#24369;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21253;&#25324;&#35770;&#25991;&#20889;&#20316;&#21644;&#38382;&#31572;&#31561;&#22810;&#20010;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#24517;&#39035;&#35299;&#20915;&#36825;&#20123;&#27169;&#22411;&#28508;&#22312;&#30340;&#35823;&#29992;&#38382;&#39064;&#65292;&#21542;&#21017;&#21487;&#33021;&#23548;&#33268;&#25220;&#34989;&#21644;&#22403;&#22334;&#20449;&#24687;&#31561;&#19981;&#33391;&#21518;&#26524;&#12290;&#26412;&#30740;&#31350;&#25581;&#31034;&#65292;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#35821;&#65292;LLMs&#21487;&#20197;&#26377;&#25928;&#22320;&#35268;&#36991;&#26816;&#27979;&#31995;&#32479;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26367;&#25442;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#20248;&#21270;&#26041;&#27861;&#65288;SICO&#65289;&#65292;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#36825;&#31181;&#25552;&#31034;&#35821;&#12290;&#22312;&#19977;&#20010;&#29616;&#23454;&#20219;&#21153;&#20013;&#65292;LLMs&#21487;&#33021;&#34987;&#35823;&#29992;&#65292;&#22312;SICO&#30340;&#24110;&#21161;&#19979;&#65292;ChatGPT&#25104;&#21151;&#22320;&#35268;&#36991;&#20102;&#20845;&#39033;&#29616;&#26377;&#30340;&#26816;&#27979;&#22120;&#65292;&#24179;&#22343;&#23548;&#33268;0.54&#30340;AUC&#19979;&#38477;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#26816;&#27979;&#22120;&#30340;&#34920;&#29616;&#29978;&#33267;&#27604;&#38543;&#26426;&#20998;&#31867;&#22120;&#36824;&#35201;&#24046;&#12290;&#36825;&#20123;&#32467;&#26524;&#22362;&#23450;&#22320;&#25581;&#31034;&#20102;&#29616;&#26377;&#26816;&#27979;&#22120;&#30340;&#33030;&#24369;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated exceptional performance in a variety of tasks, including essay writing and question answering. However, it is crucial to address the potential misuse of these models, which can lead to detrimental outcomes such as plagiarism and spamming. Recently, several detectors have been proposed, including fine-tuned classifiers and various statistical methods. In this study, we reveal that with the aid of carefully crafted prompts, LLMs can effectively evade these detection systems. We propose a novel Substitution-based In-Context example Optimization method (SICO) to automatically generate such prompts. On three real-world tasks where LLMs can be misused, SICO successfully enables ChatGPT to evade six existing detectors, causing a significant 0.54 AUC drop on average. Surprisingly, in most cases these detectors perform even worse than random classifiers. These results firmly reveal the vulnerability of existing detectors. Finally, the strong perfor
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36890;&#36807;&#27169;&#25311;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#26469;&#36827;&#34892;&#20132;&#20114;&#24335;&#35821;&#20041;&#35299;&#26512;&#30340;&#26032;&#20219;&#21153;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#21462;&#24471;&#19982;&#20351;&#29992;&#20154;&#24037;&#27880;&#37322;&#21453;&#39304;&#25968;&#25454;&#35757;&#32451;&#25152;&#33719;&#24471;&#30340;&#30456;&#24403;&#30340;&#38169;&#35823;&#32416;&#27491;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.08195</link><description>&lt;p&gt;
&#23398;&#20064;&#27169;&#25311;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#20197;&#36827;&#34892;&#20132;&#20114;&#24335;&#35821;&#20041;&#35299;&#26512;
&lt;/p&gt;
&lt;p&gt;
Learning to Simulate Natural Language Feedback for Interactive Semantic Parsing. (arXiv:2305.08195v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08195
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36890;&#36807;&#27169;&#25311;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#26469;&#36827;&#34892;&#20132;&#20114;&#24335;&#35821;&#20041;&#35299;&#26512;&#30340;&#26032;&#20219;&#21153;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#21462;&#24471;&#19982;&#20351;&#29992;&#20154;&#24037;&#27880;&#37322;&#21453;&#39304;&#25968;&#25454;&#35757;&#32451;&#25152;&#33719;&#24471;&#30340;&#30456;&#24403;&#30340;&#38169;&#35823;&#32416;&#27491;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#30340;&#20132;&#20114;&#24335;&#35821;&#20041;&#35299;&#26512;&#24050;&#32463;&#25104;&#20026;&#27604;&#20256;&#32479;&#30340;&#19968;&#27425;&#35821;&#20041;&#35299;&#26512;&#26356;&#23454;&#29992;&#30340;&#22330;&#26223;&#65292;&#20854;&#20013;&#29992;&#25143;&#25552;&#20379;&#21453;&#39304;&#26469;&#32416;&#27491;&#35299;&#26512;&#22120;&#30340;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#26497;&#22823;&#22320;&#20381;&#36182;&#20110;&#20154;&#24037;&#27880;&#37322;&#30340;&#21453;&#39304;&#25968;&#25454;&#26469;&#35757;&#32451;&#20132;&#20114;&#24335;&#35821;&#20041;&#35299;&#26512;&#22120;&#65292;&#36825;&#31181;&#26041;&#27861;&#20195;&#20215;&#39640;&#26114;&#19988;&#19981;&#21487;&#25193;&#23637;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#21363;&#27169;&#25311;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#20197;&#29992;&#20110;&#20132;&#20114;&#24335;&#35821;&#20041;&#35299;&#26512;&#12290;&#25105;&#20204;&#37197;&#21512;&#35813;&#20219;&#21153;&#20351;&#29992;&#20102;&#19968;&#20010;&#26032;&#30340;&#21453;&#39304;&#35780;&#20272;&#22120;&#12290;&#35813;&#35780;&#20272;&#22120;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;&#27169;&#25311;&#21453;&#39304;&#30340;&#36136;&#37327;&#65292;&#22522;&#20110;&#27492;&#25105;&#20204;&#21487;&#20197;&#20915;&#23450;&#26368;&#20339;&#30340;&#21453;&#39304;&#27169;&#25311;&#22120;&#12290;&#22312;&#19968;&#20010;&#25991;&#26412;&#21040;SQL&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#21453;&#39304;&#27169;&#25311;&#22120;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#20197;&#22686;&#24378;&#29305;&#23450;&#35299;&#26512;&#22120;&#30340;&#38169;&#35823;&#32416;&#27491;&#33021;&#21147;&#12290;&#22312;&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#21453;&#39304;&#27169;&#25311;&#22120;&#21487;&#20197;&#24110;&#21161;&#36798;&#21040;&#19982;&#20351;&#29992;&#20195;&#20215;&#39640;&#26114;&#30340;&#23436;&#25972;&#20154;&#24037;&#27880;&#37322;&#21453;&#39304;&#25968;&#25454;&#35757;&#32451;&#25152;&#33719;&#24471;&#30340;&#30456;&#24403;&#30340;&#38169;&#35823;&#32416;&#27491;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interactive semantic parsing based on natural language (NL) feedback, where users provide feedback to correct the parser mistakes, has emerged as a more practical scenario than the traditional one-shot semantic parsing. However, prior work has heavily relied on human-annotated feedback data to train the interactive semantic parser, which is prohibitively expensive and not scalable. In this work, we propose a new task of simulating NL feedback for interactive semantic parsing. We accompany the task with a novel feedback evaluator. The evaluator is specifically designed to assess the quality of the simulated feedback, based on which we decide the best feedback simulator from our proposed variants. On a text-to-SQL dataset, we show that our feedback simulator can generate high-quality NL feedback to boost the error correction ability of a specific parser. In low-data settings, our feedback simulator can help achieve comparable error correction performance as trained using the costly, full
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;LLM&#39537;&#21160;&#30340;&#29983;&#25104;&#24335;&#26032;&#38395;&#25512;&#33616;&#26694;&#26550;GENRE&#65292;&#23427;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#20041;&#30693;&#35782;&#20016;&#23500;&#26032;&#38395;&#25968;&#25454;&#65292;&#36890;&#36807;&#20174;&#27169;&#22411;&#35774;&#35745;&#36716;&#31227;&#21040;&#25552;&#31034;&#35774;&#35745;&#25552;&#20379;&#28789;&#27963;&#32780;&#32479;&#19968;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#20010;&#24615;&#21270;&#26032;&#38395;&#29983;&#25104;&#12289;&#29992;&#25143;&#30011;&#20687;&#21644;&#26032;&#38395;&#25688;&#35201;&#12290;</title><link>http://arxiv.org/abs/2305.06566</link><description>&lt;p&gt;
LLM&#39537;&#21160;&#30340;&#29983;&#25104;&#24335;&#26032;&#38395;&#25512;&#33616;&#21021;&#25506;
&lt;/p&gt;
&lt;p&gt;
A First Look at LLM-Powered Generative News Recommendation. (arXiv:2305.06566v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;LLM&#39537;&#21160;&#30340;&#29983;&#25104;&#24335;&#26032;&#38395;&#25512;&#33616;&#26694;&#26550;GENRE&#65292;&#23427;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#20041;&#30693;&#35782;&#20016;&#23500;&#26032;&#38395;&#25968;&#25454;&#65292;&#36890;&#36807;&#20174;&#27169;&#22411;&#35774;&#35745;&#36716;&#31227;&#21040;&#25552;&#31034;&#35774;&#35745;&#25552;&#20379;&#28789;&#27963;&#32780;&#32479;&#19968;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#20010;&#24615;&#21270;&#26032;&#38395;&#29983;&#25104;&#12289;&#29992;&#25143;&#30011;&#20687;&#21644;&#26032;&#38395;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#30340;&#26032;&#38395;&#25512;&#33616;&#31995;&#32479;&#24050;&#25104;&#20026;&#29992;&#25143;&#27983;&#35272;&#28023;&#37327;&#22312;&#32447;&#26032;&#38395;&#20869;&#23481;&#25152;&#24517;&#38656;&#30340;&#24037;&#20855;&#65292;&#28982;&#32780;&#29616;&#26377;&#30340;&#26032;&#38395;&#25512;&#33616;&#31995;&#32479;&#38754;&#20020;&#30528;&#20919;&#21551;&#21160;&#38382;&#39064;&#12289;&#29992;&#25143;&#30011;&#20687;&#24314;&#27169;&#21644;&#26032;&#38395;&#20869;&#23481;&#29702;&#35299;&#31561;&#37325;&#22823;&#25361;&#25112;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#36890;&#36807;&#27169;&#22411;&#35774;&#35745;&#36981;&#24490;&#19968;&#31181;&#19981;&#28789;&#27963;&#30340;&#20363;&#34892;&#31243;&#24207;&#26469;&#35299;&#20915;&#29305;&#23450;&#30340;&#25361;&#25112;&#65292;&#20294;&#22312;&#29702;&#35299;&#26032;&#38395;&#20869;&#23481;&#21644;&#25429;&#25417;&#29992;&#25143;&#20852;&#36259;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;GENRE&#65292;&#19968;&#31181;LLM&#39537;&#21160;&#30340;&#29983;&#25104;&#24335;&#26032;&#38395;&#25512;&#33616;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#26469;&#33258;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#35821;&#20041;&#30693;&#35782;&#26469;&#20016;&#23500;&#26032;&#38395;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20174;&#27169;&#22411;&#35774;&#35745;&#36716;&#31227;&#21040;&#25552;&#31034;&#35774;&#35745;&#26469;&#25552;&#20379;&#19968;&#31181;&#28789;&#27963;&#32780;&#32479;&#19968;&#30340;&#26032;&#38395;&#25512;&#33616;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;GENRE&#22312;&#20010;&#24615;&#21270;&#26032;&#38395;&#29983;&#25104;&#12289;&#29992;&#25143;&#30011;&#20687;&#21644;&#26032;&#38395;&#25688;&#35201;&#20013;&#30340;&#24212;&#29992;&#12290;&#20351;&#29992;&#21508;&#31181;&#27969;&#34892;&#30340;&#25512;&#33616;&#27169;&#22411;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;GENRE&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized news recommendation systems have become essential tools for users to navigate the vast amount of online news content, yet existing news recommenders face significant challenges such as the cold-start problem, user profile modeling, and news content understanding. Previous works have typically followed an inflexible routine to address a particular challenge through model design, but are limited in their ability to understand news content and capture user interests. In this paper, we introduce GENRE, an LLM-powered generative news recommendation framework, which leverages pretrained semantic knowledge from large language models to enrich news data. Our aim is to provide a flexible and unified solution for news recommendation by moving from model design to prompt design. We showcase the use of GENRE for personalized news generation, user profiling, and news summarization. Extensive experiments with various popular recommendation models demonstrate the effectiveness of GENRE. 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;&#26631;&#35760;&#22120;&#25552;&#21462;&#27604;&#36739;&#20808;&#39564;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#23558;&#20854;&#26080;&#32541;&#22320;&#38598;&#25104;&#21040;transformer-based&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#29983;&#25104;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#20934;&#30830;&#24615;&#21644;&#20840;&#38754;&#24615;&#65292;&#23454;&#29616;&#20102;&#22312;&#35780;&#20272;&#25351;&#26631;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.04561</link><description>&lt;p&gt;
&#22522;&#20110;&#27604;&#36739;&#20808;&#39564;&#30340;&#36741;&#21161;&#65292;&#25552;&#39640;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
Boosting Radiology Report Generation by Infusing Comparison Prior. (arXiv:2305.04561v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;&#26631;&#35760;&#22120;&#25552;&#21462;&#27604;&#36739;&#20808;&#39564;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#23558;&#20854;&#26080;&#32541;&#22320;&#38598;&#25104;&#21040;transformer-based&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#29983;&#25104;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#20934;&#30830;&#24615;&#21644;&#20840;&#38754;&#24615;&#65292;&#23454;&#29616;&#20102;&#22312;&#35780;&#20272;&#25351;&#26631;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#22312;&#20174;&#33016;&#37096;X&#20809;&#22270;&#20687;&#29983;&#25104;&#25918;&#23556;&#23398;&#25253;&#21578;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#30528;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#31361;&#20986;&#30340;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;&#65306;&#36825;&#20123;&#27169;&#22411;&#32463;&#24120;&#32570;&#20047;&#20808;&#21069;&#30340;&#30693;&#35782;&#65292;&#23548;&#33268;&#29983;&#25104;&#38169;&#35823;&#22320;&#25552;&#21040;&#19981;&#23384;&#22312;&#30340;&#20808;&#21069;&#26816;&#26597;&#30340;&#32508;&#21512;&#25253;&#21578;&#12290;&#36825;&#31181;&#24046;&#24322;&#21487;&#20197;&#24402;&#22240;&#20110;&#25918;&#23556;&#31185;&#21307;&#24072;&#21644;&#29983;&#25104;&#27169;&#22411;&#20043;&#38388;&#30340;&#30693;&#35782;&#24046;&#36317;&#12290;&#34429;&#28982;&#25918;&#23556;&#31185;&#21307;&#29983;&#25317;&#26377;&#24739;&#32773;&#29305;&#23450;&#30340;&#20808;&#21069;&#20449;&#24687;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#20165;&#22312;&#29305;&#23450;&#26102;&#38388;&#28857;&#25509;&#25910;X&#20809;&#22270;&#20687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;&#26631;&#35760;&#22120;&#20174;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#25552;&#21462;&#27604;&#36739;&#20808;&#39564;&#20449;&#24687;&#12290;&#28982;&#21518;&#65292;&#23558;&#25552;&#21462;&#30340;&#27604;&#36739;&#20808;&#39564;&#26080;&#32541;&#22320;&#38598;&#25104;&#21040;&#26368;&#20808;&#36827;&#30340;transformer-based&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#20351;&#20854;&#33021;&#22815;&#29983;&#25104;&#26356;&#23454;&#38469;&#21644;&#20840;&#38754;&#30340;&#25253;&#21578;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#33521;&#35821;&#25253;&#21578;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#22914;IU X-ray&#21644;MIMIC-CXR&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36229;&#36807;&#20102;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#22312;&#21508;&#31181;&#35780;&#20272;&#25351;&#26631;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent transformer-based models have made significant strides in generating radiology reports from chest X-ray images. However, a prominent challenge remains: these models often lack prior knowledge, resulting in the generation of synthetic reports that mistakenly reference non-existent prior exams. This discrepancy can be attributed to a knowledge gap between radiologists and the generation models. While radiologists possess patient-specific prior information, the models solely receive X-ray images at a specific time point. To tackle this issue, we propose a novel approach that leverages a rule-based labeler to extract comparison prior information from radiology reports. This extracted comparison prior is then seamlessly integrated into state-of-the-art transformer-based models, enabling them to produce more realistic and comprehensive reports. Our method is evaluated on English report datasets, such as IU X-ray and MIMIC-CXR. The results demonstrate that our approach surpasses baseli
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#30446;&#26631;&#20391;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#25968;&#25454;&#22686;&#24378;&#27169;&#22411;&#26469;&#20026;&#27599;&#20010;&#28304;&#25991;&#26723;&#29983;&#25104;&#28508;&#22312;&#30340;&#32763;&#35793;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#25991;&#26412;&#32423;&#26426;&#22120;&#32763;&#35793;&#30340;&#25968;&#25454;&#31232;&#30095;&#24615;&#30340;&#39118;&#38505;&#65292;&#24471;&#21040;&#20102;&#27604;&#20197;&#21069;&#26368;&#22909;&#30340;&#31995;&#32479;&#39640;2.30 s-BLEU&#24182;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.04505</link><description>&lt;p&gt;
&#38754;&#21521;&#25991;&#26412;&#32423;&#26426;&#22120;&#32763;&#35793;&#30340;&#30446;&#26631;&#20391;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Target-Side Augmentation for Document-Level Machine Translation. (arXiv:2305.04505v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#30446;&#26631;&#20391;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#25968;&#25454;&#22686;&#24378;&#27169;&#22411;&#26469;&#20026;&#27599;&#20010;&#28304;&#25991;&#26723;&#29983;&#25104;&#28508;&#22312;&#30340;&#32763;&#35793;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#25991;&#26412;&#32423;&#26426;&#22120;&#32763;&#35793;&#30340;&#25968;&#25454;&#31232;&#30095;&#24615;&#30340;&#39118;&#38505;&#65292;&#24471;&#21040;&#20102;&#27604;&#20197;&#21069;&#26368;&#22909;&#30340;&#31995;&#32479;&#39640;2.30 s-BLEU&#24182;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#32423;&#26426;&#22120;&#32763;&#35793;&#22240;&#20854;&#38271;&#36755;&#20837;&#38271;&#24230;&#21644;&#23569;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#32780;&#38754;&#20020;&#25968;&#25454;&#31232;&#30095;&#24615;&#30340;&#25361;&#25112;&#65292;&#22686;&#21152;&#20102;&#23398;&#20064;&#34394;&#20551;&#27169;&#24335;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30446;&#26631;&#20391;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#25968;&#25454;&#22686;&#24378;&#27169;&#22411;&#26469;&#20026;&#27599;&#20010;&#28304;&#25991;&#26723;&#29983;&#25104;&#35768;&#22810;&#28508;&#22312;&#30340;&#32763;&#35793;&#12290;&#22312;&#36825;&#20123;&#26356;&#24191;&#27867;&#30340;&#32763;&#35793;&#19978;&#36827;&#34892;&#23398;&#20064;&#65292;&#19968;&#20010;MT&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#21040;&#19968;&#20010;&#24179;&#28369;&#30340;&#20998;&#24067;&#65292;&#20174;&#32780;&#38477;&#20302;&#25968;&#25454;&#31232;&#30095;&#24615;&#30340;&#39118;&#38505;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#21518;&#39564;&#20998;&#24067;&#20272;&#35745;&#30340;DA&#27169;&#22411;&#22823;&#22823;&#25552;&#39640;&#20102;MT&#30340;&#24615;&#33021;&#65292;&#22312;&#26032;&#38395;&#21644;&#27431;&#27954;&#35758;&#20250;&#22522;&#20934;&#27979;&#35797;&#19978;&#27604;&#20197;&#21069;&#26368;&#22909;&#30340;&#31995;&#32479;&#39640;2.30 s-BLEU&#65292;&#24182;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312; https://github.com/baoguangsheng/target-side-augmentation &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document-level machine translation faces the challenge of data sparsity due to its long input length and a small amount of training data, increasing the risk of learning spurious patterns. To address this challenge, we propose a target-side augmentation method, introducing a data augmentation (DA) model to generate many potential translations for each source document. Learning on these wider range translations, an MT model can learn a smoothed distribution, thereby reducing the risk of data sparsity. We demonstrate that the DA model, which estimates the posterior distribution, largely improves the MT performance, outperforming the previous best system by 2.30 s-BLEU on News and achieving new state-of-the-art on News and Europarl benchmarks. Our code is available at https://github.com/baoguangsheng/target-side-augmentation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25925;&#38556;&#24863;&#30693;&#24335;&#20195;&#30721;&#32534;&#36753;&#22120;&#65292;&#36890;&#36807;&#25191;&#34892;&#29983;&#25104;&#30340;&#20195;&#30721;&#24182;&#23558;&#25191;&#34892;&#32467;&#26524;&#21253;&#21547;&#22312;&#22312;&#27880;&#37322;&#20013;&#26469;&#20248;&#21270;&#31454;&#25216;&#32534;&#31243;&#20219;&#21153;&#30340;&#20195;&#30721;&#36136;&#37327;&#65292;&#36890;&#36807;&#19982;&#20061;&#20010;&#19981;&#21516;&#30340;LLMs&#36827;&#34892;&#27604;&#36739;&#65292;&#26412;&#26041;&#27861;&#21487;&#20197;&#22312;&#20004;&#20010;&#31454;&#25216;&#32534;&#31243;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#25552;&#39640;&#20195;&#30721;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.04087</link><description>&lt;p&gt;
&#33258;&#25105;&#32534;&#36753;&#65306;&#38024;&#23545;&#20195;&#30721;&#29983;&#25104;&#30340;&#25925;&#38556;&#24863;&#30693;&#24335;&#20195;&#30721;&#32534;&#36753;&#22120;
&lt;/p&gt;
&lt;p&gt;
Self-Edit: Fault-Aware Code Editor for Code Generation. (arXiv:2305.04087v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04087
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25925;&#38556;&#24863;&#30693;&#24335;&#20195;&#30721;&#32534;&#36753;&#22120;&#65292;&#36890;&#36807;&#25191;&#34892;&#29983;&#25104;&#30340;&#20195;&#30721;&#24182;&#23558;&#25191;&#34892;&#32467;&#26524;&#21253;&#21547;&#22312;&#22312;&#27880;&#37322;&#20013;&#26469;&#20248;&#21270;&#31454;&#25216;&#32534;&#31243;&#20219;&#21153;&#30340;&#20195;&#30721;&#36136;&#37327;&#65292;&#36890;&#36807;&#19982;&#20061;&#20010;&#19981;&#21516;&#30340;LLMs&#36827;&#34892;&#27604;&#36739;&#65292;&#26412;&#26041;&#27861;&#21487;&#20197;&#22312;&#20004;&#20010;&#31454;&#25216;&#32534;&#31243;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#25552;&#39640;&#20195;&#30721;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#31454;&#25216;&#32534;&#31243;&#20219;&#21153;&#20013;&#29983;&#25104;&#20195;&#30721;&#30340;&#33021;&#21147;&#24050;&#32463;&#24471;&#21040;&#35777;&#26126;&#65292;&#20294;&#30001;&#20110;&#26679;&#26412;&#25968;&#37327;&#26377;&#38480;&#65292;LLMs&#20173;&#28982;&#23384;&#22312;&#36739;&#20302;&#30340;&#20934;&#30830;&#24615;&#12290;&#21463;&#20154;&#31867;&#32534;&#31243;&#36807;&#31243;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#21644;&#32534;&#36753;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;LLMs&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#25191;&#34892;&#32467;&#26524;&#26469;&#25552;&#39640;&#31454;&#25216;&#32534;&#31243;&#20219;&#21153;&#30340;&#20195;&#30721;&#36136;&#37327;&#12290;&#25105;&#20204;&#22312;&#38382;&#39064;&#20013;&#25552;&#20379;&#30340;&#31034;&#20363;&#27979;&#35797;&#29992;&#20363;&#19978;&#25191;&#34892;&#29983;&#25104;&#30340;&#20195;&#30721;&#65292;&#24182;&#23558;&#25191;&#34892;&#32467;&#26524;&#21253;&#21547;&#22312;&#34917;&#20805;&#24615;&#27880;&#37322;&#20013;&#12290;&#21033;&#29992;&#36825;&#20010;&#27880;&#37322;&#20316;&#20026;&#25351;&#23548;&#65292;&#25105;&#20204;&#30340;&#25925;&#38556;&#24863;&#30693;&#24335;&#20195;&#30721;&#32534;&#36753;&#22120;&#29992;&#20110;&#32416;&#27491;&#29983;&#25104;&#30340;&#20195;&#30721;&#20013;&#30340;&#38169;&#35823;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#31454;&#25216;&#32534;&#31243;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#28085;&#30422;&#20102;&#20061;&#20010;&#19981;&#21516;&#30340;LLMs&#12290;&#19982;&#30452;&#25509;&#20174;LLMs&#29983;&#25104;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;APPS-dev&#19978;&#23558;pass@1&#30340;&#24179;&#22343;&#20540;&#25552;&#39640;89&#65285;&#65292;&#22312;APPS-test&#19978;&#25552;&#39640;31&#65285;&#65292;&#22312;HumanEval&#19978;&#25552;&#39640;48&#65285;&#65292;&#36229;&#36807;&#20102;&#20061;&#20010;&#27969;&#34892;&#30340;&#20195;&#30721;&#29983;&#25104;LLMs&#65292;&#21442;&#25968;&#22823;&#23567;&#33539;&#22260;&#20026;110M-t&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated an impressive ability to generate codes on competitive programming tasks. However, with limited sample numbers, LLMs still suffer from poor accuracy. Inspired by the process of human programming, we propose a generate-and-edit approach that utilizes execution results of the generated code from LLMs to improve the code quality on the competitive programming task. We execute the generated code on the example test case provided in the question and wrap execution results into a supplementary comment. Utilizing this comment as guidance, our fault-aware code editor is employed to correct errors in the generated code. We perform extensive evaluations across two competitive programming datasets with nine different LLMs. Compared to directly generating from LLMs, our approach can improve the average of pass@1 by 89\% on APPS-dev, 31\% on APPS-test, and 48\% on HumanEval over nine popular code generation LLMs with parameter sizes ranging from 110M t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#34701;&#21512;&#21644;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#35757;&#32451;&#24182;&#19988;&#36866;&#24212;&#20110;&#26032;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2305.03517</link><description>&lt;p&gt;
&#20174;&#25991;&#26412;&#20013;&#23454;&#29616;&#23569;&#26679;&#26412;&#39046;&#22495;&#33258;&#36866;&#24212;&#35270;&#35273;&#34701;&#21512;&#20107;&#20214;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Few-shot Domain-Adaptive Visually-fused Event Detection from Text. (arXiv:2305.03517v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03517
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#34701;&#21512;&#21644;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#35757;&#32451;&#24182;&#19988;&#36866;&#24212;&#20110;&#26032;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23558;&#36741;&#21161;&#27169;&#24577;&#22914;&#22270;&#20687;&#25972;&#21512;&#21040;&#20107;&#20214;&#26816;&#27979;&#27169;&#22411;&#20013;&#24050;&#32463;&#24341;&#36215;&#20154;&#20204;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#24773;&#22659;&#30340;&#22797;&#26434;&#24615;&#20419;&#20351;&#30740;&#31350;&#20154;&#21592;&#21033;&#29992;&#30456;&#20851;&#30340;&#35270;&#35273;&#19978;&#19979;&#25991;&#26469;&#25552;&#39640;&#20107;&#20214;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36825;&#20010;&#39046;&#22495;&#30340;&#26041;&#27861;&#22312;&#25968;&#25454;&#31232;&#32570;&#24615;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#65292;&#38656;&#35201;&#35768;&#22810;&#26631;&#35760;&#22909;&#30340;&#25991;&#26412;-&#22270;&#20687;&#23545;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#22312;&#25512;&#26029;&#26102;&#26080;&#27861;&#33719;&#24471;&#35270;&#35273;&#19978;&#19979;&#25991;&#30340;&#38480;&#21046;&#20063;&#20250;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#20351;&#20854;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#38590;&#20197;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#35270;&#35273;&#34701;&#21512;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#24456;&#23569;&#30340;&#26631;&#35760;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#28857;&#36827;&#34892;&#35757;&#32451;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#35270;&#35273;&#24819;&#35937;&#22120;&#30340;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#27809;&#26377;&#35270;&#35273;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#19979;&#20174;&#25991;&#26412;&#20013;&#21512;&#25104;&#22270;&#20687;&#65292;&#24182;&#19988;&#21487;&#20197;&#26681;&#25454;&#38656;&#35201;&#23450;&#21046;&#21040;&#29305;&#23450;&#30340;&#39046;&#22495;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#28040;&#38500;&#20102;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#38656;&#27714;&#65292;&#24182;&#19988;&#21482;&#38656;&#23569;&#37327;&#30340;&#26631;&#35760;&#22270;&#20687;-&#25991;&#26412;&#23545;&#23601;&#21487;&#20197;&#36866;&#24212;&#20110;&#26032;&#39046;&#22495;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#26174;&#31034;&#20854;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incorporating auxiliary modalities such as images into event detection models has attracted increasing interest over the last few years. The complexity of natural language in describing situations has motivated researchers to leverage the related visual context to improve event detection performance. However, current approaches in this area suffer from data scarcity, where a large amount of labelled text-image pairs are required for model training. Furthermore, limited access to the visual context at inference time negatively impacts the performance of such models, which makes them practically ineffective in real-world scenarios. In this paper, we present a novel domain-adaptive visually-fused event detection approach that can be trained on a few labelled image-text paired data points. Specifically, we introduce a visual imaginator method that synthesises images from text in the absence of visual context. Moreover, the imaginator can be customised to a specific domain. In doing so, our
&lt;/p&gt;</description></item><item><title>SI-LSTM&#26159;&#19968;&#31181;&#29992;&#20110;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#30340;&#24490;&#29615;&#32467;&#26500;&#65292;&#21487;&#20197;&#36861;&#36394;&#19981;&#21516;&#35828;&#35805;&#20154;&#30340;&#24773;&#24863;&#29366;&#24577;&#65292;&#20174;&#32780;&#22686;&#24378;&#23545;&#35805;&#24773;&#24863;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2305.03506</link><description>&lt;p&gt;
SI-LSTM: &#29992;&#20110;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#30340;&#35828;&#35805;&#20154;&#28151;&#21512;&#38271;&#30701;&#26399;&#35760;&#24518;&#21644;&#36328;&#27169;&#24577;&#27880;&#24847;&#21147;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
SI-LSTM: Speaker Hybrid Long-short Term Memory and Cross Modal Attention for Emotion Recognition in Conversation. (arXiv:2305.03506v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03506
&lt;/p&gt;
&lt;p&gt;
SI-LSTM&#26159;&#19968;&#31181;&#29992;&#20110;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#30340;&#24490;&#29615;&#32467;&#26500;&#65292;&#21487;&#20197;&#36861;&#36394;&#19981;&#21516;&#35828;&#35805;&#20154;&#30340;&#24773;&#24863;&#29366;&#24577;&#65292;&#20174;&#32780;&#22686;&#24378;&#23545;&#35805;&#24773;&#24863;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#27169;&#24577;&#30340;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#23545;&#20110;&#26234;&#33021;&#21307;&#30103;&#12289;&#23545;&#35805;&#20154;&#24037;&#26234;&#33021;&#21644;&#32842;&#22825;&#21382;&#21490;&#35266;&#28857;&#25366;&#25496;&#31561;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35828;&#35805;&#20154;&#20449;&#24687;&#22686;&#24378;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;SI-LSTM&#65289;&#30340;&#24490;&#29615;&#32467;&#26500;&#65292;&#21487;&#20197;&#36861;&#36394;&#19981;&#21516;&#35828;&#35805;&#20154;&#30340;&#24773;&#24863;&#29366;&#24577;&#65292;&#20174;&#32780;&#22686;&#24378;&#23545;&#35805;&#24773;&#24863;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion Recognition in Conversation~(ERC) across modalities is of vital importance for a variety of applications, including intelligent healthcare, artificial intelligence for conversation, and opinion mining over chat history. The crux of ERC is to model both cross-modality and cross-time interactions throughout the conversation. Previous methods have made progress in learning the time series information of conversation while lacking the ability to trace down the different emotional states of each speaker in a conversation. In this paper, we propose a recurrent structure called Speaker Information Enhanced Long-Short Term Memory (SI-LSTM) for the ERC task, where the emotional states of the distinct speaker can be tracked in a sequential way to enhance the learning of the emotion in conversation. Further, to improve the learning of multimodal features in ERC, we utilize a cross-modal attention component to fuse the features between different modalities and model the interaction of the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#21307;&#29983;-&#24739;&#32773;&#23545;&#35805;&#20013;&#33258;&#21160;&#29983;&#25104;&#20020;&#24202;&#31508;&#35760;&#30340;&#30740;&#31350;&#65292;&#37319;&#29992;&#23569;&#26679;&#26412;&#19978;&#19979;&#25991;&#23398;&#20064;&#27861;&#25152;&#29983;&#25104;&#31508;&#35760;&#34920;&#29616;&#20248;&#31168;&#65292;&#19988;&#21487;&#19982;&#20154;&#24037;&#32534;&#20889;&#30340;&#31508;&#35760;&#23218;&#32654;&#12290;</title><link>http://arxiv.org/abs/2305.02220</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#21307;&#29983;-&#24739;&#32773;&#23545;&#35805;&#20013;&#29983;&#25104;&#20020;&#24202;&#31508;&#35760;&#65306;&#26469;&#33258;MEDIQA-Chat&#30340;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
Clinical Note Generation from Doctor-Patient Conversations using Large Language Models: Insights from MEDIQA-Chat. (arXiv:2305.02220v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02220
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#21307;&#29983;-&#24739;&#32773;&#23545;&#35805;&#20013;&#33258;&#21160;&#29983;&#25104;&#20020;&#24202;&#31508;&#35760;&#30340;&#30740;&#31350;&#65292;&#37319;&#29992;&#23569;&#26679;&#26412;&#19978;&#19979;&#25991;&#23398;&#20064;&#27861;&#25152;&#29983;&#25104;&#31508;&#35760;&#34920;&#29616;&#20248;&#31168;&#65292;&#19988;&#21487;&#19982;&#20154;&#24037;&#32534;&#20889;&#30340;&#31508;&#35760;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#22312;MEDIQA-Chat 2023&#20849;&#20139;&#20219;&#21153;&#20013;&#25552;&#20132;&#30340;&#33258;&#21160;&#20020;&#24202;&#31508;&#35760;&#29983;&#25104;&#26041;&#26696;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#20004;&#31181;&#26041;&#27861;&#30340;&#32467;&#26524;&#65306;&#31532;&#19968;&#31181;&#26159;&#22312;&#20849;&#20139;&#20219;&#21153;&#25968;&#25454;&#19978;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#65292;&#31532;&#20108;&#31181;&#26159;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23569;&#26679;&#26412;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#12290;&#20004;&#31181;&#26041;&#27861;&#37117;&#21462;&#24471;&#20102;&#39640;&#24615;&#33021;&#65292;&#22914;&#36890;&#36807;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#65288;&#20363;&#22914;ROUGE&#65292;BERTScore&#65289;&#27979;&#37327;&#65292;&#24182;&#20998;&#21035;&#22312;&#25152;&#26377;&#25552;&#20132;&#30340;&#26041;&#26696;&#20013;&#25490;&#21517;&#31532;&#20108;&#21644;&#31532;&#19968;&#12290;&#19987;&#23478;&#23457;&#26680;&#34920;&#26126;&#65292;&#36890;&#36807;&#22522;&#20110;ICL&#30340;&#26041;&#27861;&#20351;&#29992;GPT-4&#29983;&#25104;&#30340;&#31508;&#35760;&#19982;&#20154;&#24037;&#32534;&#20889;&#30340;&#31508;&#35760;&#19968;&#26679;&#21463;&#27426;&#36814;&#65292;&#36825;&#20351;&#24471;&#23427;&#25104;&#20026;&#20174;&#21307;&#29983;-&#24739;&#32773;&#23545;&#35805;&#20013;&#33258;&#21160;&#29983;&#25104;&#31508;&#35760;&#30340;&#26377;&#21069;&#36884;&#30340;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes our submission to the MEDIQA-Chat 2023 shared task for automatic clinical note generation from doctor-patient conversations. We report results for two approaches: the first fine-tunes a pre-trained language model (PLM) on the shared task data, and the second uses few-shot in-context learning (ICL) with a large language model (LLM). Both achieve high performance as measured by automatic metrics (e.g. ROUGE, BERTScore) and ranked second and first, respectively, of all submissions to the shared task. Expert human scrutiny indicates that notes generated via the ICL-based approach with GPT-4 are preferred about as often as human-written notes, making it a promising path toward automated note generation from doctor-patient conversations.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;token&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#20301;&#32622;&#20559;&#24046;&#38382;&#39064;&#65292;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#22312;&#20855;&#20307;&#20219;&#21153;&#20013;&#65292;BERT&#12289;ERNIE&#12289;ELECTRA&#31561;&#32534;&#30721;&#22120;&#20197;&#21450;GPT2&#21644;BLOOM&#31561;&#35299;&#30721;&#22120;&#30340;&#24179;&#22343;&#24615;&#33021;&#19979;&#38477;&#20102;3%&#21644;9%&#12290;</title><link>http://arxiv.org/abs/2304.13567</link><description>&lt;p&gt;
&#20301;&#32622;&#20559;&#24046;&#23545;token&#20998;&#31867;&#20013;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Impact of Position Bias on Language Models in Token Classification. (arXiv:2304.13567v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13567
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;token&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#20301;&#32622;&#20559;&#24046;&#38382;&#39064;&#65292;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#22312;&#20855;&#20307;&#20219;&#21153;&#20013;&#65292;BERT&#12289;ERNIE&#12289;ELECTRA&#31561;&#32534;&#30721;&#22120;&#20197;&#21450;GPT2&#21644;BLOOM&#31561;&#35299;&#30721;&#22120;&#30340;&#24179;&#22343;&#24615;&#33021;&#19979;&#38477;&#20102;3%&#21644;9%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;(NER)&#25110;&#35789;&#24615;&#26631;&#27880;&#31561;&#19979;&#28216;&#20219;&#21153;&#24050;&#30693;&#23384;&#22312;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#27491;&#36127;&#31034;&#20363;&#30340;&#27604;&#20363;&#21644;&#31867;&#19981;&#24179;&#34913;&#26041;&#38754;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#21478;&#19968;&#20010;&#29305;&#23450;&#38382;&#39064;&#65292;&#21363;token&#20998;&#31867;&#20219;&#21153;&#20013;&#27491;&#31034;&#20363;&#30340;&#20301;&#32622;&#20559;&#24046;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23545;&#22522;&#20110;Token&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#20301;&#32622;&#20559;&#24046;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21253;&#25324;CoNLL03&#21644;OntoNote5.0&#29992;&#20110;NER&#65292;English Tree Bank UD_en&#21644;TweeBank&#29992;&#20110;POS&#26631;&#35760;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#30740;&#31350;Transformer&#27169;&#22411;&#20013;&#30340;&#20301;&#32622;&#20559;&#24046;&#12290;&#25105;&#20204;&#21457;&#29616;&#20687;BERT&#12289;ERNIE&#12289;ELECTRA&#36825;&#26679;&#30340;&#32534;&#30721;&#22120;&#21644;&#20687;GPT2 &#21644;BLOOM&#36825;&#26679;&#30340;&#35299;&#30721;&#22120;&#24179;&#22343;&#24615;&#33021;&#19979;&#38477;&#20102;3%&#21644;9%&#12290;
&lt;/p&gt;
&lt;p&gt;
Language Models (LMs) have shown state-of-the-art performance in Natural Language Processing (NLP) tasks. Downstream tasks such as Named Entity Recognition (NER) or Part-of-Speech (POS) tagging are known to suffer from data imbalance issues, specifically in terms of the ratio of positive to negative examples, and class imbalance. In this paper, we investigate an additional specific issue for language models, namely the position bias of positive examples in token classification tasks. Therefore, we conduct an in-depth evaluation of the impact of position bias on the performance of LMs when fine-tuned on Token Classification benchmarks. Our study includes CoNLL03 and OntoNote5.0 for NER, English Tree Bank UD_en and TweeBank for POS tagging. We propose an evaluation approach to investigate position bias in Transformer models. We show that encoders like BERT, ERNIE, ELECTRA, and decoders such as GPT2 and BLOOM can suffer from this bias with an average drop of 3\% and 9\% in their performan
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;11&#31181;&#19981;&#21516;&#30340;&#21360;&#24230;&#35821;&#35328;&#30340;&#38388;&#35821;&#35328;Seq2seq&#35821;&#20041;&#20998;&#26512;&#25968;&#25454;&#38598;IE-SEMPARSE&#65292;&#24182;&#35780;&#20272;&#20102;&#29616;&#26377;&#30340;&#22810;&#35821;&#35328;seq2seq&#27169;&#22411;&#22312;&#20854;&#20013;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.13005</link><description>&lt;p&gt;
&#35780;&#20272;&#21360;&#24230;&#35821;&#35328;&#38388;&#35821;&#20041;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Evaluating Inter-Bilingual Semantic Parsing for Indian Languages. (arXiv:2304.13005v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13005
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;11&#31181;&#19981;&#21516;&#30340;&#21360;&#24230;&#35821;&#35328;&#30340;&#38388;&#35821;&#35328;Seq2seq&#35821;&#20041;&#20998;&#26512;&#25968;&#25454;&#38598;IE-SEMPARSE&#65292;&#24182;&#35780;&#20272;&#20102;&#29616;&#26377;&#30340;&#22810;&#35821;&#35328;seq2seq&#27169;&#22411;&#22312;&#20854;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21360;&#24230;&#35821;&#35328;&#65288;IndicNLP&#65289;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#23545;&#20110;&#20687;&#35821;&#20041;&#35299;&#26512;&#36825;&#26679;&#30340;&#22797;&#26434;&#32467;&#26500;&#20219;&#21153;&#32570;&#20047;&#25968;&#25454;&#38598;&#12290;&#23548;&#33268;&#27492;&#20005;&#37325;&#32570;&#21475;&#30340;&#19968;&#20010;&#21407;&#22240;&#26159;&#36923;&#36753;&#24418;&#24335;&#30340;&#22797;&#26434;&#24615;&#65292;&#36825;&#20351;&#24471;&#33521;&#35821;&#21040;&#22810;&#35821;&#35328;&#30340;&#32763;&#35793;&#21464;&#24471;&#22256;&#38590;&#12290; &#36825;&#20010;&#36807;&#31243;&#28041;&#21450;&#23558;&#36923;&#36753;&#24418;&#24335;&#12289;&#24847;&#22270;&#21644;&#27133;&#19982;&#32763;&#35793;&#30340;&#38750;&#32467;&#26500;&#21270;&#35805;&#35821;&#23545;&#40784;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;11&#31181;&#19981;&#21516;&#30340;&#21360;&#24230;&#35821;&#35328;&#30340;&#38388;&#35821;&#35328;Seq2seq&#35821;&#20041;&#20998;&#26512;&#25968;&#25454;&#38598;IE-SEMPARSE&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#25152;&#25552;&#20986;&#20219;&#21153;&#30340;&#23454;&#29992;&#24615;&#65292;&#24182;&#35780;&#20272;&#20102;&#29616;&#26377;&#30340;&#22810;&#35821;&#35328;seq2seq&#27169;&#22411;&#22312;&#20960;&#31181;&#35757;&#32451;-&#27979;&#35797;&#31574;&#30053;&#20043;&#38388;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#21407;&#22987;&#22810;&#35821;&#35328;&#35821;&#20041;&#20998;&#26512;&#25968;&#25454;&#38598;&#65288;&#22914;mTOP&#12289;&#22810;&#35821;&#35328;TOP&#21644;multiATIS++&#65289;&#21644;&#25105;&#20204;&#25552;&#20986;&#30340;IE-SEMPARSE&#22871;&#20214;&#20043;&#38388;&#30340;&#39640;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite significant progress in Natural Language Generation for Indian languages (IndicNLP), there is a lack of datasets around complex structured tasks such as semantic parsing. One reason for this imminent gap is the complexity of the logical form, which makes English to multilingual translation difficult. The process involves alignment of logical forms, intents and slots with translated unstructured utterance. To address this, we propose an Inter-bilingual Seq2seq Semantic parsing dataset IE-SEMPARSE for 11 distinct Indian languages. We highlight the proposed task's practicality, and evaluate existing multilingual seq2seq models across several train-test strategies. Our experiment reveals a high correlation across performance of original multilingual semantic parsing datasets (such as mTOP, multilingual TOP and multiATIS++) and our proposed IE-SEMPARSE suite.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#21033;&#29992;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#20381;&#36182;&#24615;&#23884;&#20837;&#35805;&#35821;&#34920;&#31034;&#65292;&#20174;&#32780;&#22312;&#23545;&#35805;&#24773;&#32490;&#35782;&#21035;&#20013;&#21462;&#24471;&#20102;&#38750;&#24120;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.08216</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#20381;&#36182;&#24615;&#23884;&#20837;&#35805;&#35821;&#34920;&#31034;&#22312;&#23545;&#35805;&#24773;&#32490;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Context-Dependent Embedding Utterance Representations for Emotion Recognition in Conversations. (arXiv:2304.08216v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#21033;&#29992;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#20381;&#36182;&#24615;&#23884;&#20837;&#35805;&#35821;&#34920;&#31034;&#65292;&#20174;&#32780;&#22312;&#23545;&#35805;&#24773;&#32490;&#35782;&#21035;&#20013;&#21462;&#24471;&#20102;&#38750;&#24120;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#35805;&#20195;&#29702;&#21464;&#24471;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#23545;&#35805;&#24773;&#32490;&#35782;&#21035;&#65288;ERC&#65289;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#35782;&#21035;&#24773;&#24863;&#23545;&#20110;&#26377;&#25928;&#30340;&#20132;&#27969;&#33267;&#20851;&#37325;&#35201;&#65292;&#26159;&#24320;&#21457;&#26377;&#25928;&#24182;&#19988;&#20855;&#26377;&#20849;&#24773;&#33021;&#21147;&#30340;&#23545;&#35805;&#20195;&#29702;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#23545;&#35805;&#32972;&#26223;&#30340;&#30693;&#35782;&#21644;&#29702;&#35299;&#23545;&#20110;&#35782;&#21035;&#20132;&#27969;&#32773;&#30340;&#24773;&#24863;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#23545;&#35805;&#32972;&#26223;&#26469;&#36827;&#34892;ERC&#65292;&#21363;&#20851;&#27880;&#20808;&#21069;&#30340;&#23545;&#35805;&#22238;&#21512;&#12290;&#36890;&#24120;&#65292;&#24314;&#27169;&#23545;&#35805;&#19978;&#19979;&#25991;&#30340;&#26041;&#27861;&#26159;&#20135;&#29983;&#27599;&#20010;&#35805;&#35821;&#30340;&#19978;&#19979;&#25991;&#26080;&#20851;&#34920;&#31034;&#65292;&#28982;&#21518;&#23545;&#36825;&#20123;&#35805;&#35821;&#36827;&#34892;&#19978;&#19979;&#25991;&#27169;&#22411;&#22788;&#29702;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#20381;&#36182;&#24615;&#23884;&#20837;&#35805;&#35821;&#34920;&#31034;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#23558;&#23545;&#35805;&#32972;&#26223;&#36861;&#21152;&#21040;&#35805;&#35821;&#20013;&#65292;&#28982;&#21518;&#23558;&#20854;&#39304;&#20837;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#27169;&#22411;&#20013;&#65292;&#35813;&#27169;&#22411;&#23558;&#20135;&#29983;&#30456;&#24212;&#30340;&#19978;&#19979;&#25991;&#23884;&#20837;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#22312;&#19982;&#19978;&#19979;&#25991;&#26080;&#20851;&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#30456;&#27604;&#20013;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion Recognition in Conversations (ERC) has been gaining increasing importance as conversational agents become more and more common. Recognizing emotions is key for effective communication, being a crucial component in the development of effective and empathetic conversational agents. Knowledge and understanding of the conversational context are extremely valuable for identifying the emotions of the interlocutor. We thus approach Emotion Recognition in Conversations leveraging the conversational context, i.e., taking into attention previous conversational turns. The usual approach to model the conversational context has been to produce context-independent representations of each utterance and subsequently perform contextual modeling of these. Here we propose context-dependent embedding representations of each utterance by leveraging the contextual representational power of pre-trained transformer language models. In our approach, we feed the conversational context appended to the ut
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;GPT(-3.5&#21644;-4)&#29983;&#25104;&#30340;&#26085;&#35821;&#25991;&#20307;&#29305;&#24449;&#19982;&#20154;&#31867;&#20889;&#20316;&#30340;&#23398;&#26415;&#35770;&#25991;&#65292;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#25991;&#20307;&#29305;&#24449;&#19978;&#26377;&#26174;&#33879;&#21306;&#21035;&#12290;</title><link>http://arxiv.org/abs/2304.05534</link><description>&lt;p&gt;
&#36890;&#36807;&#26085;&#35821;&#25991;&#20307;&#20998;&#26512;&#21306;&#20998;ChatGPT(-3.5,-4)&#30340;&#29983;&#25104;&#25991;&#26412;&#19982;&#20154;&#31867;&#20889;&#20316;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Distinguishing ChatGPT(-3.5, -4)-generated and human-written papers through Japanese stylometric analysis. (arXiv:2304.05534v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;GPT(-3.5&#21644;-4)&#29983;&#25104;&#30340;&#26085;&#35821;&#25991;&#20307;&#29305;&#24449;&#19982;&#20154;&#31867;&#20889;&#20316;&#30340;&#23398;&#26415;&#35770;&#25991;&#65292;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#25991;&#20307;&#29305;&#24449;&#19978;&#26377;&#26174;&#33879;&#21306;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#29983;&#25104;&#30340;&#20154;&#24037;&#26234;&#33021;&#65292;&#21253;&#25324;OpenAI&#30340;GPT-3.5&#21644;GPT-4&#65292;&#24341;&#36215;&#20102;&#20840;&#29699;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;GPT(-3.5&#21644;-4)&#29983;&#25104;&#21644;&#20154;&#31867;&#25776;&#20889;&#30340;&#26085;&#35821;&#25991;&#20307;&#29305;&#24449;&#12290;&#36890;&#36807;&#22810;&#32500;&#23610;&#24230;&#20998;&#26512;&#65292;&#23558;216&#20010;&#25991;&#26412;&#65288;36&#20301;&#21333;&#19968;&#20316;&#32773;&#30340;72&#31687;&#23398;&#26415;&#35770;&#25991;&#12289;72&#31687;GPT-3.5&#29983;&#25104;&#30340;&#25991;&#26412;&#21644;72&#31687;GPT-4&#29983;&#25104;&#30340;&#25991;&#26412;&#65289;&#26681;&#25454;&#35789;&#24615;&#30340;&#20108;&#20803;&#32452;&#65292;&#35789;&#23614;&#30340;&#20108;&#20803;&#32452;&#65292;&#36887;&#21495;&#30340;&#20301;&#32622;&#21644;&#21151;&#33021;&#35789;&#30340;&#27604;&#20363;&#20998;&#25104;&#19977;&#31867;&#65292;&#32467;&#26524;&#26174;&#31034;&#20986;GPT(-3.5&#65292;-4)&#21644;&#20154;&#31867;&#20043;&#38388;&#22312;&#25991;&#20307;&#29305;&#24449;&#19978;&#26377;&#26126;&#26174;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-generative artificial intelligence (AI), including ChatGPT, equipped with GPT-3.5 and GPT-4, from OpenAI, has attracted considerable attention worldwide. In this study, first, we compared Japanese stylometric features generated by GPT (-3.5 and -4) and those written by humans. In this work, we performed multi-dimensional scaling (MDS) to confirm the classification of 216 texts into three classes (72 academic papers written by 36 single authors, 72 texts generated by GPT-3.5, and 72 texts generated by GPT-4 on the basis of the titles of the aforementioned papers) focusing on the following stylometric features: (1) bigrams of parts-of-speech, (2) bigram of postpositional particle words, (3) positioning of commas, and (4) rate of function words. MDS revealed distinct distributions at each stylometric feature of GPT (-3.5 and -4) and human. Although GPT-4 is more powerful than GPT-3.5 because it has more parameters, both GPT (-3.5 and -4) distributions are likely to overlap. These res
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#22823;&#23567;&#12289;&#32467;&#26500;&#21270;&#21098;&#26525;&#12289;&#25512;&#26029;&#25928;&#29575;&#21644;&#25688;&#35201;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#20351;&#29992;&#19981;&#23545;&#31216;&#21098;&#26525;&#21487;&#22312;&#19981;&#22823;&#25439;&#22833;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#39640;&#25512;&#26029;&#25928;&#29575;&#32422;3&#20493;&#12290;</title><link>http://arxiv.org/abs/2304.02721</link><description>&lt;p&gt;
&#36229;&#36234;&#19981;&#23545;&#31216;&#24615;&#65306;&#32467;&#26500;&#21098;&#26525;&#25552;&#39640;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#30340;&#25512;&#26029;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
To Asymmetry and Beyond: Structured Pruning of Sequence to Sequence Models for Improved Inference Efficiency. (arXiv:2304.02721v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#22823;&#23567;&#12289;&#32467;&#26500;&#21270;&#21098;&#26525;&#12289;&#25512;&#26029;&#25928;&#29575;&#21644;&#25688;&#35201;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#20351;&#29992;&#19981;&#23545;&#31216;&#21098;&#26525;&#21487;&#22312;&#19981;&#22823;&#25439;&#22833;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#39640;&#25512;&#26029;&#25928;&#29575;&#32422;3&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#21040;&#24207;&#21015;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#36830;&#36143;&#65292;&#30456;&#20851;&#21644;&#31616;&#27905;&#30340;&#25277;&#35937;&#25688;&#35201;&#12290;&#20294;&#26159;&#65292;&#27169;&#22411;&#22823;&#23567;&#21487;&#33021;&#20351;&#24471;&#22312;&#24310;&#36831;&#25935;&#24863;&#25110; Web &#35268;&#27169;&#30340;&#23454;&#29616;&#20013;&#37096;&#32626;&#21464;&#24471;&#22256;&#38590;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#22823;&#23567;&#12289;&#32467;&#26500;&#21270;&#21098;&#26525;&#12289;&#25512;&#26029;&#25928;&#29575;&#21644;&#24191;&#27867;&#20351;&#29992;&#30340;&#25688;&#35201;&#25968;&#25454;&#38598;&#19978;&#30340;&#25688;&#35201;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#27169;&#22411;&#20934;&#30830;&#24615;&#19982;&#32534;&#30721;&#22120;&#22823;&#23567;&#26377;&#20851;&#65292;&#32780;&#25512;&#29702;&#25928;&#29575;&#19982;&#35299;&#30721;&#22120;&#26377;&#20851;&#12290;&#20351;&#29992;&#19981;&#23545;&#31216;&#21098;&#26525;&#21487;&#23548;&#33268;&#25512;&#26029;&#24310;&#36831;&#30340;&#36817;3&#20493;&#25552;&#39640;&#65292;Rouge-2&#30340;&#25439;&#22833;&#32422;&#20026;1&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#24179;&#22343;&#24615;&#33021;&#38477;&#20302;&#21644;&#19981;&#23545;&#31216;&#24615;&#30340;&#20316;&#29992;&#22312;&#27169;&#22411;&#22823;&#23567;&#21644;&#25968;&#25454;&#38598;&#21464;&#21270;&#26041;&#38754;&#26159;&#19968;&#33268;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequence-to-sequence language models can be used to produce abstractive summaries which are coherent, relevant, and concise. Still, model sizes can make deployment in latency-sensitive or web-scale implementations difficult. This paper studies the relationship between model size, structured pruning, inference efficiency, and summarization accuracy on widely used summarization datasets. We show that model accuracy is tied to the encoder size while inference efficiency is connected to the decoder. Using asymmetric pruning can lead to nearly 3x improvement in inference latency with ~1 point loss in Rouge-2. Moreover, we find both the average degradation and the role of asymmetry to be consistent across model sizes and variations in datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#38142;&#25509;&#20013;&#30340;&#37096;&#20998;&#30693;&#35782;&#24211;&#25512;&#29702;&#38382;&#39064;&#65292;&#21457;&#29616;&#30001;&#20110;&#31934;&#24230;&#19979;&#38477;&#23548;&#33268;EL&#24615;&#33021;&#20986;&#29616;&#28798;&#38590;&#24615;&#19979;&#38477;&#65292;&#32780;&#19988;EL&#33539;&#20363;&#26080;&#27861;&#22788;&#29702;&#26080;&#27861;&#38142;&#25509;&#30340;&#25552;&#21450;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#36174;&#22238;&#26041;&#27861;&#26469;&#35299;&#20915;NIL&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.10330</link><description>&lt;p&gt;
&#25506;&#32034;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#38142;&#25509;&#20013;&#30340;&#37096;&#20998;&#30693;&#35782;&#24211;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Exploring Partial Knowledge Base Inference in Biomedical Entity Linking. (arXiv:2303.10330v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#38142;&#25509;&#20013;&#30340;&#37096;&#20998;&#30693;&#35782;&#24211;&#25512;&#29702;&#38382;&#39064;&#65292;&#21457;&#29616;&#30001;&#20110;&#31934;&#24230;&#19979;&#38477;&#23548;&#33268;EL&#24615;&#33021;&#20986;&#29616;&#28798;&#38590;&#24615;&#19979;&#38477;&#65292;&#32780;&#19988;EL&#33539;&#20363;&#26080;&#27861;&#22788;&#29702;&#26080;&#27861;&#38142;&#25509;&#30340;&#25552;&#21450;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#36174;&#22238;&#26041;&#27861;&#26469;&#35299;&#20915;NIL&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#38142;&#25509;&#65288;EL&#65289;&#21253;&#25324;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#21644;&#21629;&#21517;&#23454;&#20307;&#28040;&#27495;&#65288;NED&#65289;&#12290;EL&#27169;&#22411;&#22312;&#30001;&#39044;&#23450;&#20041;&#30340;&#30693;&#35782;&#24211;&#26631;&#35760;&#30340;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#24120;&#35265;&#30340;&#24773;&#20917;&#26159;&#21482;&#26377;&#30693;&#35782;&#24211;&#30340;&#23376;&#38598;&#20013;&#30340;&#23454;&#20307;&#23545;&#21033;&#30410;&#30456;&#20851;&#32773;&#26377;&#20215;&#20540;&#12290;&#25105;&#20204;&#31216;&#36825;&#31181;&#24773;&#20917;&#20026;&#37096;&#20998;&#30693;&#35782;&#24211;&#25512;&#29702;&#65306;&#20351;&#29992;&#19968;&#20010;&#30693;&#35782;&#24211;&#23545;EL&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#27809;&#26377;&#36827;&#19968;&#27493;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#23545;&#20854;&#37096;&#20998;&#36827;&#34892;&#25512;&#29702;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#36825;&#31181;&#23454;&#38469;&#19978;&#38750;&#24120;&#26377;&#20215;&#20540;&#20294;&#26126;&#26174;&#19981;&#22815;&#30740;&#31350;&#30340;&#24773;&#20917;&#30340;&#35814;&#32454;&#23450;&#20041;&#21644;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#19977;&#20010;&#20195;&#34920;&#24615;&#30340;EL&#33539;&#20363;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#37096;&#20998;&#30693;&#35782;&#24211;&#25512;&#29702;&#22522;&#20934;&#65292;&#24182;&#21457;&#29616;&#30001;&#20110;&#22823;&#37327;&#31934;&#24230;&#19979;&#38477;&#23548;&#33268;EL&#24615;&#33021;&#20986;&#29616;&#28798;&#38590;&#24615;&#19979;&#38477;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;&#36825;&#20123;EL&#33539;&#20363;&#26080;&#27861;&#27491;&#30830;&#22788;&#29702;&#26080;&#27861;&#38142;&#25509;&#30340;&#25552;&#21450;&#65288;NIL&#65289;&#65292;&#22240;&#27492;&#23427;&#20204;&#23545;&#37096;&#20998;&#30693;&#35782;&#24211;&#25512;&#29702;&#19981;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20004;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#36174;&#22238;&#26041;&#27861;&#26469;&#35299;&#20915;NIL&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biomedical entity linking (EL) consists of named entity recognition (NER) and named entity disambiguation (NED). EL models are trained on corpora labeled by a predefined KB. However, it is a common scenario that only entities within a subset of the KB are precious to stakeholders. We name this scenario partial knowledge base inference: training an EL model with one KB and inferring on the part of it without further training. In this work, we give a detailed definition and evaluation procedures for this practically valuable but significantly understudied scenario and evaluate methods from three representative EL paradigms. We construct partial KB inference benchmarks and witness a catastrophic degradation in EL performance due to dramatically precision drop. Our findings reveal these EL paradigms can not correctly handle unlinkable mentions (NIL), so they are not robust to partial KB inference. We also propose two simple-and-effective redemption methods to combat the NIL issue with litt
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#29616;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#25216;&#26415;&#24182;&#25552;&#20986;&#20102;&#20851;&#38190;&#32771;&#34385;&#22240;&#32032;&#65292;&#22914;&#24320;&#21457;&#20840;&#38754;&#35780;&#20272;&#25351;&#26631;&#21644;&#24320;&#28304; LLM &#25152;&#26500;&#25104;&#30340;&#23041;&#32961;&#12290;</title><link>http://arxiv.org/abs/2303.07205</link><description>&lt;p&gt;
&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25991;&#23383;&#30340;&#31185;&#23398;
&lt;/p&gt;
&lt;p&gt;
The Science of Detecting LLM-Generated Texts. (arXiv:2303.07205v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07205
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#29616;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#25216;&#26415;&#24182;&#25552;&#20986;&#20102;&#20851;&#38190;&#32771;&#34385;&#22240;&#32032;&#65292;&#22914;&#24320;&#21457;&#20840;&#38754;&#35780;&#20272;&#25351;&#26631;&#21644;&#24320;&#28304; LLM &#25152;&#26500;&#25104;&#30340;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#30340;&#20986;&#29616;&#23548;&#33268;&#20102;&#39640;&#24230;&#22797;&#26434;&#19988;&#20960;&#20046;&#38590;&#20197;&#21306;&#20998;&#20986;&#26159;&#21542;&#20026;&#20154;&#31867;&#21019;&#20316;&#30340; LLM &#29983;&#25104;&#25991;&#23383;&#12290;&#20294;&#26159;&#65292;&#36825;&#20063;&#24341;&#21457;&#20102;&#23545;&#27492;&#31867;&#25991;&#23383;&#28508;&#22312;&#35823;&#29992;&#30340;&#25285;&#24551;&#65292;&#20363;&#22914;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#21644;&#22312;&#25945;&#32946;&#31995;&#32479;&#20013;&#36896;&#25104;&#28151;&#20081;&#12290;&#23613;&#31649;&#24050;&#25552;&#20986;&#35768;&#22810;&#26816;&#27979;&#26041;&#27861;&#65292;&#20294;&#20851;&#20110;&#20854;&#25104;&#23601;&#21644;&#25361;&#25112;&#30340;&#20840;&#38754;&#29702;&#35299;&#20173;&#28982;&#32570;&#20047;&#12290;&#26412;&#25991;&#26088;&#22312;&#27010;&#36848;&#29616;&#26377;&#30340; LLM &#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#25216;&#26415;&#65292;&#24182;&#22686;&#24378;&#23545;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#30340;&#25511;&#21046;&#21644;&#30417;&#31649;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24378;&#35843;&#26410;&#26469;&#30740;&#31350;&#30340;&#37325;&#35201;&#32771;&#34385;&#22240;&#32032;&#65292;&#21253;&#25324;&#24320;&#21457;&#20840;&#38754;&#35780;&#20272;&#25351;&#26631;&#21644;&#24320;&#28304; LLM &#25152;&#26500;&#25104;&#30340;&#23041;&#32961;&#65292;&#20197;&#25512;&#21160; LLM &#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of large language models (LLMs) has resulted in the production of LLM-generated texts that is highly sophisticated and almost indistinguishable from texts written by humans. However, this has also sparked concerns about the potential misuse of such texts, such as spreading misinformation and causing disruptions in the education system. Although many detection approaches have been proposed, a comprehensive understanding of the achievements and challenges is still lacking. This survey aims to provide an overview of existing LLM-generated text detection techniques and enhance the control and regulation of language generation models. Furthermore, we emphasize crucial considerations for future research, including the development of comprehensive evaluation metrics and the threat posed by open-source LLMs, to drive progress in the area of LLM-generated text detection.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#24102;&#26377;&#24433;&#21709;&#21147;&#30340;&#31034;&#20363;&#36873;&#25321;&#26041;&#27861;&#26469;&#25552;&#39640;&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#30340;&#24615;&#33021;&#65292;&#20998;&#26512;&#34920;&#26126;&#27491;&#38754;&#21644;&#36127;&#38754;&#31034;&#20363;&#23545;ICL&#21462;&#24471;&#30340;&#24615;&#33021;&#26377;&#39640;&#36798;16.3%&#30340;&#24433;&#21709;&#65292;&#26696;&#20363;&#30740;&#31350;&#20013;&#20063;&#21457;&#29616;&#20102;&#31034;&#20363;&#25490;&#24207;&#20013;&#30340;&#8220;&#26368;&#36817;&#24615;&#20559;&#24046;&#8221;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2302.11042</link><description>&lt;p&gt;
&#24102;&#26377;&#24433;&#21709;&#21147;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
In-context Example Selection with Influences. (arXiv:2302.11042v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11042
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#24102;&#26377;&#24433;&#21709;&#21147;&#30340;&#31034;&#20363;&#36873;&#25321;&#26041;&#27861;&#26469;&#25552;&#39640;&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#30340;&#24615;&#33021;&#65292;&#20998;&#26512;&#34920;&#26126;&#27491;&#38754;&#21644;&#36127;&#38754;&#31034;&#20363;&#23545;ICL&#21462;&#24471;&#30340;&#24615;&#33021;&#26377;&#39640;&#36798;16.3%&#30340;&#24433;&#21709;&#65292;&#26696;&#20363;&#30740;&#31350;&#20013;&#20063;&#21457;&#29616;&#20102;&#31034;&#20363;&#25490;&#24207;&#20013;&#30340;&#8220;&#26368;&#36817;&#24615;&#20559;&#24046;&#8221;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#26159;&#20174;&#22823;&#35821;&#35328;&#27169;&#22411;(LLM)&#20013;&#20986;&#29616;&#30340;&#19968;&#31181;&#24378;&#22823;&#30340;&#33539;&#20363;&#12290;&#23613;&#31649;&#26377;&#30528;&#35768;&#22810;&#26377;&#21033;&#26041;&#38754;&#65292;ICL&#30340;&#24615;&#33021;&#20173;&#28982;&#23545;&#36755;&#20837;&#31034;&#20363;&#38750;&#24120;&#25935;&#24863;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;$\textit{&#19978;&#19979;&#25991;&#24433;&#21709;}$&#26469;&#30452;&#25509;&#20998;&#26512;&#23569;&#26679;&#26412;ICL&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#24433;&#21709;&#21147;&#30340;&#31034;&#20363;&#36873;&#25321;&#26041;&#27861;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#27491;&#38754;&#21644;&#36127;&#38754;&#31034;&#20363;&#65292;&#22312;9&#20010;SuperGLUE&#20219;&#21153;&#30340;&#35780;&#20272;&#20013;&#20248;&#20110;&#20960;&#31181;&#22522;&#20934;&#32447;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#65292;&#22312;&#20351;&#29992;&#26368;&#27491;&#38754;&#31034;&#20363;&#21644;&#26368;&#36127;&#38754;&#31034;&#20363;&#20043;&#38388;&#65292;&#24615;&#33021;&#24046;&#24322;&#21487;&#39640;&#36798;$16.3\%$&#12290;&#22312;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#22522;&#20110;&#24433;&#21709;&#21147;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;&#37327;&#21270;&#23569;&#26679;&#26412;ICL&#31034;&#20363;&#25490;&#24207;&#20013;&#26368;&#36817;&#24615;&#20559;&#24046;&#30340;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning (ICL) is a powerful paradigm emerged from large language models (LLMs). Despite its promises, ICL performance is known to be highly sensitive to input examples. In this work, we use $\textit{in-context influences}$ to analyze few-shot ICL performance directly from the in-context examples. Our proposed influence-based example selection method can identify both positive and negative examples, outperforming several baselines when evaluated on 9 SuperGLUE tasks. Our analysis uncovers up to a $16.3\%$ performance gap between using the most negative in-context examples compared to the most positive. In a case study, we apply our influence-based framework to quantify the phenomena of recency bias in example ordering for few-shot ICL.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CTC&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#35821;&#38899;&#32763;&#35793;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#23567;&#35821;&#38899;&#21644;&#25991;&#26412;&#27169;&#24577;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25552;&#39640;&#26368;&#32456;&#30340;ST&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.11716</link><description>&lt;p&gt;
&#22522;&#20110;CTC&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#35821;&#38899;&#32763;&#35793;&#39044;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Pre-training for Speech Translation: CTC Meets Optimal Transport. (arXiv:2301.11716v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11716
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CTC&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#35821;&#38899;&#32763;&#35793;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#23567;&#35821;&#38899;&#21644;&#25991;&#26412;&#27169;&#24577;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25552;&#39640;&#26368;&#32456;&#30340;ST&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#21040;&#25991;&#26412;&#32763;&#35793;(ST)&#20013;&#30340;&#27169;&#24577;&#24046;&#36317;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#35813;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#35757;&#32451;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#65292;&#26080;&#38656;&#25913;&#21464;ST&#27169;&#22411;&#30340;&#26550;&#26500;&#12290;&#39318;&#20808;&#65292;&#26412;&#25991;&#34920;&#26126;&#36830;&#25509;&#26102;&#24207;&#20998;&#31867;(CTC)&#25439;&#22833;&#21487;&#20197;&#36890;&#36807;&#35774;&#35745;&#26469;&#20943;&#23567;&#27169;&#24577;&#24046;&#36317;&#12290;&#36890;&#36807;&#19982;&#26356;&#24120;&#35265;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#30340;&#23450;&#37327;&#27604;&#36739;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;CTC&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#22987;&#32456;&#23454;&#29616;&#26356;&#22909;&#30340;&#26368;&#32456;ST&#20934;&#30830;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;CTC&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#26032;&#22411;&#39044;&#35757;&#32451;&#26041;&#27861;&#20197;&#36827;&#19968;&#27493;&#20943;&#23567;&#36825;&#31181;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20351;&#29992;CTC&#21644;&#26368;&#20248;&#20256;&#36755;&#36827;&#34892;&#39044;&#35757;&#32451;&#30456;&#23545;&#20110;&#20165;&#20351;&#29992;CTC&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#27809;&#26377;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#22522;&#32447;&#27169;&#22411;&#22343;&#33021;&#22815;&#25552;&#20379;&#25345;&#32493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
The gap between speech and text modalities is a major challenge in speech-to-text translation (ST). Different methods have been proposed to reduce this gap, but most of them require architectural changes in ST training. In this work, we propose to mitigate this issue at the pre-training stage, requiring no change in the ST model. First, we show that the connectionist temporal classification (CTC) loss can reduce the modality gap by design. We provide a quantitative comparison with the more common cross-entropy loss, showing that pre-training with CTC consistently achieves better final ST accuracy. Nevertheless, CTC is only a partial solution and thus, in our second contribution, we propose a novel pre-training method combining CTC and optimal transport to further reduce this gap. Our method pre-trains a Siamese-like model composed of two encoders, one for acoustic inputs and the other for textual inputs, such that they produce representations that are close to each other in the Wassers
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ExaRanker&#30340;&#35299;&#37322;&#22686;&#24378;&#22411;&#31070;&#32463;&#25490;&#24207;&#27169;&#22411;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#26816;&#32034;&#25968;&#25454;&#38598;&#65292;&#21487;&#22312;&#36755;&#20986;&#30456;&#20851;&#24230;&#26631;&#31614;&#19982;&#35299;&#37322;&#26102;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.10521</link><description>&lt;p&gt;
ExaRanker: &#35299;&#37322;&#22686;&#24378;&#22411;&#31070;&#32463;&#25490;&#24207;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ExaRanker: Explanation-Augmented Neural Ranker. (arXiv:2301.10521v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ExaRanker&#30340;&#35299;&#37322;&#22686;&#24378;&#22411;&#31070;&#32463;&#25490;&#24207;&#27169;&#22411;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#26816;&#32034;&#25968;&#25454;&#38598;&#65292;&#21487;&#22312;&#36755;&#20986;&#30456;&#20851;&#24230;&#26631;&#31614;&#19982;&#35299;&#37322;&#26102;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36755;&#20986;&#31572;&#26696;&#21069;&#29983;&#25104;&#35299;&#37322;&#26159;&#25552;&#39640;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#24615;&#33021;&#30340;&#26377;&#25928;&#31574;&#30053;&#12290;&#26412;&#25991;&#25552;&#20986;&#31070;&#32463;&#25490;&#24207;&#27169;&#22411;&#20063;&#21463;&#30410;&#20110;&#35299;&#37322;&#12290;&#25105;&#20204;&#20351;&#29992;GPT-3.5&#31561;&#35821;&#35328;&#27169;&#22411;&#26469;&#22686;&#24378;&#20855;&#26377;&#35299;&#37322;&#30340;&#26816;&#32034;&#25968;&#25454;&#38598;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#25490;&#24207;&#27169;&#22411;&#65292;&#20197;&#36755;&#20986;&#32473;&#23450;&#26597;&#35810;-&#25991;&#26723;&#23545;&#30340;&#30456;&#20851;&#24230;&#26631;&#31614;&#21644;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#34987;&#31216;&#20026;ExaRanker&#65292;&#22312;&#20351;&#29992;&#21512;&#25104;&#35299;&#37322;&#30340;&#20960;&#21315;&#20010;&#26679;&#26412;&#36827;&#34892;&#24494;&#35843;&#21518;&#65292;&#24615;&#33021;&#19982;&#26080;&#35299;&#37322;&#30340;3&#20493;&#26679;&#26412;&#24494;&#35843;&#30340;&#27169;&#22411;&#30456;&#24403;&#12290;&#27492;&#22806;&#65292;ExaRanker&#27169;&#22411;&#22312;&#25490;&#24207;&#36807;&#31243;&#20013;&#27809;&#26377;&#39069;&#22806;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#20801;&#35768;&#26681;&#25454;&#38656;&#35201;&#35831;&#27714;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has shown that inducing a large language model (LLM) to generate explanations prior to outputting an answer is an effective strategy to improve performance on a wide range of reasoning tasks. In this work, we show that neural rankers also benefit from explanations. We use LLMs such as GPT-3.5 to augment retrieval datasets with explanations and train a sequence-to-sequence ranking model to output a relevance label and an explanation for a given query-document pair. Our model, dubbed ExaRanker, finetuned on a few thousand examples with synthetic explanations performs on par with models finetuned on 3x more examples without explanations. Furthermore, the ExaRanker model incurs no additional computational cost during ranking and allows explanations to be requested on demand.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;LUMEN&#65292;&#23427;&#39044;&#20808;&#35745;&#31639;&#22823;&#37096;&#20998;&#26816;&#32034;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#23454;&#26102;&#32534;&#30721;&#22120;&#36827;&#34892;&#23436;&#25104;&#32534;&#30721;&#65292;&#30456;&#36739;&#20110;&#32431;&#20869;&#23384;&#21644;FiD&#65292;LUMEN&#22312;&#22810;&#20010;&#38382;&#31572;&#20219;&#21153;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#19988;&#25104;&#26412;&#26356;&#20302;&#12290;</title><link>http://arxiv.org/abs/2301.10448</link><description>&lt;p&gt;
&#39044;&#35745;&#31639;&#20869;&#23384;&#25110;&#23454;&#26102;&#32534;&#30721;&#65311;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#28151;&#21512;&#26041;&#27861;&#20351;&#35745;&#31639;&#36164;&#28304;&#24471;&#21040;&#26368;&#22823;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
Pre-computed memory or on-the-fly encoding? A hybrid approach to retrieval augmentation makes the most of your compute. (arXiv:2301.10448v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10448
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;LUMEN&#65292;&#23427;&#39044;&#20808;&#35745;&#31639;&#22823;&#37096;&#20998;&#26816;&#32034;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#23454;&#26102;&#32534;&#30721;&#22120;&#36827;&#34892;&#23436;&#25104;&#32534;&#30721;&#65292;&#30456;&#36739;&#20110;&#32431;&#20869;&#23384;&#21644;FiD&#65292;LUMEN&#22312;&#22810;&#20010;&#38382;&#31572;&#20219;&#21153;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#19988;&#25104;&#26412;&#26356;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;&#35299;&#30721;&#22120;&#20013;&#30340;Fusion&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#22312;&#21508;&#31181;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#35774;&#32622;&#20102;&#26368;&#26032;&#30340;&#25216;&#26415;&#27700;&#24179;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38656;&#35201;&#23545;&#22823;&#37327;&#26816;&#32034;&#21040;&#30340;&#27573;&#33853;&#36827;&#34892;&#32534;&#30721;&#65292;&#23427;&#20204;&#20063;&#38750;&#24120;&#26114;&#36149;&#12290;&#19968;&#20123;&#24037;&#20316;&#36890;&#36807;&#23558;&#25991;&#26412;&#35821;&#26009;&#24211;&#39044;&#32534;&#30721;&#20026;&#20869;&#23384;&#65292;&#24182;&#30452;&#25509;&#26816;&#32034;&#23494;&#38598;&#34920;&#31034;&#26469;&#36991;&#20813;&#36825;&#31181;&#25104;&#26412;&#12290;&#20294;&#26159;&#65292;&#39044;&#32534;&#30721;&#20869;&#23384;&#20250;&#23548;&#33268;&#20005;&#37325;&#30340;&#36136;&#37327;&#24809;&#32602;&#65292;&#22240;&#20026;&#20869;&#23384;&#34920;&#31034;&#26410;&#38024;&#23545;&#24403;&#21069;&#36755;&#20837;&#36827;&#34892;&#35843;&#25972;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LUMEN&#65292;&#23427;&#26159;&#36825;&#20004;&#20010;&#26497;&#31471;&#20043;&#38388;&#30340;&#28151;&#21512;&#20307;&#65292;&#39044;&#20808;&#35745;&#31639;&#22823;&#37096;&#20998;&#26816;&#32034;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#23454;&#26102;&#32534;&#30721;&#22120;&#23436;&#25104;&#32534;&#30721;&#65292;&#35813;&#23454;&#26102;&#32534;&#30721;&#22120;&#26159;&#22522;&#20110;&#38382;&#39064;&#36827;&#34892;&#26465;&#20214;&#21270;&#30340;&#65292;&#24182;&#20026;&#20219;&#21153;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#22810;&#20010;&#38382;&#31572;&#20219;&#21153;&#20013;&#65292;LUMEN&#26126;&#26174;&#20248;&#20110;&#32431;&#20869;&#23384;&#65292;&#21516;&#26102;&#27604;FiD&#20415;&#23452;&#24471;&#22810;&#65292;&#24182;&#19988;&#22312;&#32473;&#23450;&#30340;&#35745;&#31639;&#36164;&#28304;&#39044;&#31639;&#19979;&#65292;LUMEN&#30340;&#25928;&#26524;&#20248;&#20110;&#20004;&#32773;&#12290;&#27492;&#22806;&#65292;&#24403;&#27169;&#22411;&#35268;&#27169;&#22686;&#22823;&#26102;&#65292;LUMEN&#30456;&#23545;&#20110;FiD&#30340;&#20248;&#21183;&#20063;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrieval-augmented language models such as Fusion-in-Decoder are powerful, setting the state of the art on a variety of knowledge-intensive tasks. However, they are also expensive, due to the need to encode a large number of retrieved passages. Some work avoids this cost by pre-encoding a text corpus into a memory and retrieving dense representations directly. However, pre-encoding memory incurs a severe quality penalty as the memory representations are not conditioned on the current input. We propose LUMEN, a hybrid between these two extremes, pre-computing the majority of the retrieval representation and completing the encoding on the fly using a live encoder that is conditioned on the question and fine-tuned for the task. We show that LUMEN significantly outperforms pure memory on multiple question-answering tasks while being much cheaper than FiD, and outperforms both for any given compute budget. Moreover, the advantage of LUMEN over FiD increases with model size.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;MR.COD&#30340;&#22810;&#36339;&#35777;&#25454;&#26816;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#25903;&#25345;&#36328;&#25991;&#26723;&#20851;&#31995;&#25277;&#21462;&#12290;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#33719;&#21462;&#20102;&#36328;&#25991;&#26723;&#35777;&#25454;&#65292;&#24182;&#25552;&#21319;&#20102;&#23553;&#38381;&#21644;&#24320;&#25918;&#35774;&#32622;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.10786</link><description>&lt;p&gt;
&#36328;&#25991;&#26723;&#20851;&#31995;&#25277;&#21462;&#30340;&#22810;&#36339;&#35777;&#25454;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Multi-hop Evidence Retrieval for Cross-document Relation Extraction. (arXiv:2212.10786v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10786
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;MR.COD&#30340;&#22810;&#36339;&#35777;&#25454;&#26816;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#25903;&#25345;&#36328;&#25991;&#26723;&#20851;&#31995;&#25277;&#21462;&#12290;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#33719;&#21462;&#20102;&#36328;&#25991;&#26723;&#35777;&#25454;&#65292;&#24182;&#25552;&#21319;&#20102;&#23553;&#38381;&#21644;&#24320;&#25918;&#35774;&#32622;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25277;&#21462;(RE)&#24050;&#32463;&#25193;&#23637;&#21040;&#36328;&#25991;&#26723;&#22330;&#26223;&#20013;&#65292;&#22240;&#20026;&#35768;&#22810;&#20851;&#31995;&#19981;&#20165;&#20165;&#22312;&#19968;&#20010;&#25991;&#26723;&#20013;&#25551;&#36848;&#12290;&#36825;&#19981;&#21487;&#36991;&#20813;&#22320;&#24102;&#26469;&#20102;&#26377;&#25928;&#30340;&#24320;&#25918;&#31354;&#38388;&#35777;&#25454;&#26816;&#32034;&#30340;&#25361;&#25112;&#65292;&#20197;&#25903;&#25345;&#36328;&#25991;&#26723;&#20851;&#31995;&#30340;&#25512;&#26029;&#65292;&#21516;&#26102;&#20063;&#24102;&#26469;&#20102;&#22810;&#36339;&#25512;&#29702;&#30340;&#25361;&#25112;&#65292;&#20197;&#22788;&#29702;&#25955;&#24067;&#22312;&#24320;&#25918;&#24335;&#25991;&#26723;&#38598;&#20013;&#30340;&#23454;&#20307;&#21644;&#35777;&#25454;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MR.COD(&#36328;&#25991;&#26723;&#20851;&#31995;&#25277;&#21462;&#30340;&#22810;&#36339;&#35777;&#25454;&#26816;&#32034;)&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#35777;&#25454;&#36335;&#24452;&#25366;&#25496;&#21644;&#25490;&#24207;&#30340;&#22810;&#36339;&#35777;&#25454;&#26816;&#32034;&#26041;&#27861;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#22810;&#20010;&#26816;&#32034;&#22120;&#30340;&#21464;&#20307;&#65292;&#20197;&#26174;&#31034;&#35777;&#25454;&#26816;&#32034;&#22312;&#36328;&#25991;&#26723;RE&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#36824;&#20026;&#27492;&#35774;&#32622;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#23494;&#38598;&#30340;&#26816;&#32034;&#22120;&#12290;&#22312;CodRED&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;MR.COD&#30340;&#35777;&#25454;&#26816;&#32034;&#26377;&#25928;&#22320;&#33719;&#21462;&#20102;&#36328;&#25991;&#26723;&#35777;&#25454;&#65292;&#24182;&#25552;&#21319;&#20102;&#23553;&#38381;&#21644;&#24320;&#25918;&#35774;&#32622;&#20013;&#30340;&#31471;&#21040;&#31471;RE&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation Extraction (RE) has been extended to cross-document scenarios because many relations are not simply described in a single document. This inevitably brings the challenge of efficient open-space evidence retrieval to support the inference of cross-document relations, along with the challenge of multi-hop reasoning on top of entities and evidence scattered in an open set of documents. To combat these challenges, we propose MR.COD (Multi-hop evidence retrieval for Cross-document relation extraction), which is a multi-hop evidence retrieval method based on evidence path mining and ranking. We explore multiple variants of retrievers to show evidence retrieval is essential in cross-document RE. We also propose a contextual dense retriever for this setting. Experiments on CodRED show that evidence retrieval with MR.COD effectively acquires crossdocument evidence and boosts end-to-end RE performance in both closed and open settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35821;&#35328;&#26816;&#32034;&#30340;&#21464;&#20998;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#40723;&#21169;&#28304;&#20998;&#31163;&#65292;&#24182;&#23637;&#31034;&#20854;&#19982;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#30340;&#27604;&#36739;&#32467;&#26524;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#38469;&#26816;&#32034;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.10726</link><description>&lt;p&gt;
&#36229;&#36234;&#23545;&#27604;&#23398;&#20064;&#65306;&#19968;&#31181;&#22810;&#35821;&#35328;&#26816;&#32034;&#30340;&#21464;&#20998;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Beyond Contrastive Learning: A Variational Generative Model for Multilingual Retrieval. (arXiv:2212.10726v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35821;&#35328;&#26816;&#32034;&#30340;&#21464;&#20998;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#40723;&#21169;&#28304;&#20998;&#31163;&#65292;&#24182;&#23637;&#31034;&#20854;&#19982;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#30340;&#27604;&#36739;&#32467;&#26524;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#38469;&#26816;&#32034;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#24050;&#34987;&#25104;&#21151;&#29992;&#20110;&#26816;&#32034;&#35821;&#20041;&#23545;&#40784;&#30340;&#21477;&#23376;&#65292;&#20294;&#24448;&#24448;&#38656;&#35201;&#22823;&#25209;&#37327;&#22788;&#29702;&#25110;&#31934;&#24515;&#30340;&#24037;&#31243;&#25165;&#33021;&#22863;&#25928;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#23398;&#20064;&#22810;&#35821;&#35328;&#25991;&#26412;&#23884;&#20837;&#65292;&#21487;&#20197;&#29992;&#20110;&#26816;&#32034;&#25110;&#35780;&#20998;&#21477;&#23376;&#23545;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;$N$&#31181;&#35821;&#35328;&#30340;&#24182;&#34892;&#25968;&#25454;&#19978;&#25805;&#20316;&#65292;&#24182;&#36890;&#36807;&#25105;&#20204;&#24341;&#20837;&#30340;&#19968;&#31181;&#36817;&#20284;&#26041;&#27861;&#65292;&#22312;&#36825;&#20010;&#22810;&#35821;&#35328;&#29615;&#22659;&#20013;&#26377;&#25928;&#22320;&#40723;&#21169;&#28304;&#20998;&#31163;&#65292;&#23558;&#32763;&#35793;&#20043;&#38388;&#20849;&#20139;&#30340;&#35821;&#20041;&#20449;&#24687;&#19982;&#35821;&#20307;&#25110;&#35821;&#35328;&#29305;&#23450;&#21464;&#21270;&#20998;&#24320;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#27604;&#21644;&#22522;&#20110;&#29983;&#25104;&#30340;&#26041;&#27861;&#22312;&#23398;&#20064;&#22810;&#35821;&#35328;&#25991;&#26412;&#23884;&#20837;&#26041;&#38754;&#30340;&#22823;&#35268;&#27169;&#20180;&#32454;&#27604;&#36739;&#65292;&#36825;&#26159;&#25105;&#20204;&#25152;&#30693;&#36947;&#30340;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#21313;&#20998;&#27969;&#34892;&#21364;&#20174;&#26410;&#27604;&#36739;&#36807;&#30340;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#35821;&#20041;&#30456;&#20284;&#24615;&#65292;&#21452;&#35821;&#25366;&#25496;&#21644;&#36328;&#35821;&#35328;&#38382;&#39064;&#26816;&#32034;&#8212;&#8212;&#26368;&#21518;&#19968;&#20010;&#20219;&#21153;&#23558;&#23637;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23454;&#38469;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning has been successfully used for retrieval of semantically aligned sentences, but it often requires large batch sizes or careful engineering to work well. In this paper, we instead propose a generative model for learning multilingual text embeddings which can be used to retrieve or score sentence pairs. Our model operates on parallel data in $N$ languages and, through an approximation we introduce, efficiently encourages source separation in this multilingual setting, separating semantic information that is shared between translations from stylistic or language-specific variation. We show careful large-scale comparisons between contrastive and generation-based approaches for learning multilingual text embeddings, a comparison that has not been done to the best of our knowledge despite the popularity of these approaches. We evaluate this method on a suite of tasks including semantic similarity, bitext mining, and cross-lingual question retrieval -- the last of which w
&lt;/p&gt;</description></item><item><title>BUMP&#26159;&#19968;&#20010;&#19981;&#24544;&#23454;&#26368;&#23567;&#23545;&#27604;&#22522;&#20934;&#65292;&#30001;889&#20010;&#20154;&#20889;&#30340;&#23545;&#27604;CNN / DailyMail&#25968;&#25454;&#38598;&#20013;&#30340;&#25688;&#35201;&#36827;&#34892;&#24494;&#23567;&#24046;&#24322;&#22788;&#29702;&#32780;&#24471;&#12290;BUMP&#21487;&#20197;&#24110;&#21161;&#35780;&#20272;&#33258;&#21160;&#24544;&#23454;&#24615;&#24230;&#37327;&#65292;&#20854;&#20248;&#20110;&#29616;&#26377;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#22240;&#20026;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#19988;&#21487;&#20197;&#34920;&#24449;&#19981;&#21516;&#31867;&#22411;&#30340;&#19981;&#24544;&#23454;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.09955</link><description>&lt;p&gt;
BUMP&#65306;&#19968;&#31181;&#19981;&#24544;&#23454;&#26368;&#23567;&#23545;&#27604;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#24544;&#23454;&#24615;&#24230;&#37327;&#30340;&#36229;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
BUMP: A Benchmark of Unfaithful Minimal Pairs for Meta-Evaluation of Faithfulness Metrics. (arXiv:2212.09955v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09955
&lt;/p&gt;
&lt;p&gt;
BUMP&#26159;&#19968;&#20010;&#19981;&#24544;&#23454;&#26368;&#23567;&#23545;&#27604;&#22522;&#20934;&#65292;&#30001;889&#20010;&#20154;&#20889;&#30340;&#23545;&#27604;CNN / DailyMail&#25968;&#25454;&#38598;&#20013;&#30340;&#25688;&#35201;&#36827;&#34892;&#24494;&#23567;&#24046;&#24322;&#22788;&#29702;&#32780;&#24471;&#12290;BUMP&#21487;&#20197;&#24110;&#21161;&#35780;&#20272;&#33258;&#21160;&#24544;&#23454;&#24615;&#24230;&#37327;&#65292;&#20854;&#20248;&#20110;&#29616;&#26377;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#22240;&#20026;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#19988;&#21487;&#20197;&#34920;&#24449;&#19981;&#21516;&#31867;&#22411;&#30340;&#19981;&#24544;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#24544;&#23454;&#24615;&#24230;&#37327;&#22312;&#25688;&#35201;&#25991;&#26412;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#20135;&#29983;&#20102;&#23545;&#22522;&#20934;&#30340;&#38656;&#27714;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#37327;&#20102;&#27169;&#22411;&#29983;&#25104;&#30340;&#25688;&#35201;&#19982;&#20154;&#31867;&#24544;&#23454;&#24615;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#65292;&#20294;&#23427;&#20204;&#19981;&#36275;&#20197;&#35786;&#26029;&#25351;&#26631;&#26159;&#21542;&#65306;1&#65289;&#19968;&#33268;&#65292;&#21363;&#24403;&#38169;&#35823;&#34987;&#24341;&#20837;&#21040;&#25688;&#35201;&#20013;&#26102;&#25351;&#20986;&#36739;&#20302;&#30340;&#24544;&#23454;&#24615;&#65307;2&#65289;&#22312;&#20154;&#24037;&#20889;&#20316;&#25991;&#26412;&#20013;&#26377;&#25928;&#65307;3&#65289;&#23545;&#19981;&#21516;&#30340;&#38169;&#35823;&#31867;&#22411;&#25935;&#24863;&#65288;&#22240;&#20026;&#25688;&#35201;&#21487;&#20197;&#21253;&#21547;&#22810;&#20010;&#38169;&#35823;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19981;&#24544;&#23454;&#26368;&#23567;&#23545;&#27604;&#22522;&#20934;&#65288;BUMP&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;889&#20010;&#23545;CNN / DailyMail&#25968;&#25454;&#38598;&#20013;&#30340;&#25688;&#35201;&#36827;&#34892;&#24494;&#23567;&#24046;&#24322;&#22788;&#29702;&#32780;&#24471;&#21040;&#30340;&#65292;&#20854;&#20013;&#25688;&#35201;&#20013;&#24341;&#20837;&#20102;&#21333;&#20010;&#38169;&#35823;&#30340;&#20154;&#20889;&#30340;&#25688;&#35201;&#23545;&#12290;&#25105;&#20204;&#21457;&#29616;BUMP&#22312;&#35768;&#22810;&#26041;&#38754;&#34917;&#20805;&#20102;&#29616;&#26377;&#30340;&#22522;&#20934;&#65306;1&#65289;BUMP&#20013;&#30340;&#25688;&#35201;&#26356;&#38590;&#21306;&#21035;&#65292;&#24182;&#19988;&#22312;SOTA&#25688;&#35201;&#27169;&#22411;&#19979;&#19981;&#22826;&#21487;&#33021;&#65307;2&#65289;&#19982;&#38750;&#23545;&#27604;&#22522;&#20934;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;BUMP&#21487;&#29992;&#20110;&#34913;&#37327;&#25351;&#26631;&#26159;&#21542;&#33391;&#22909;&#22320;&#35782;&#21035;&#19981;&#24544;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of automatic faithfulness metrics for summarization has produced a need for benchmarks to evaluate them. While existing benchmarks measure the correlation with human judgements of faithfulness on model-generated summaries, they are insufficient for diagnosing whether metrics are: 1) consistent, i.e., indicate lower faithfulness as errors are introduced into a summary, 2) effective on human-written texts, and 3) sensitive to different error types (as summaries can contain multiple errors). To address these needs, we present a benchmark of unfaithful minimal pairs (BUMP), a dataset of 889 human-written, minimally different summary pairs, where a single error is introduced to a summary from the CNN/DailyMail dataset to produce an unfaithful summary. We find BUMP complements existing benchmarks in a number of ways: 1) the summaries in BUMP are harder to discriminate and less probable under SOTA summarization models, 2) unlike non-pair-based datasets, BUMP can be used to m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38646;&#26679;&#26412;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;Z-ICL&#65292;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#21407;&#22987;&#25991;&#26412;&#30340;&#20266;&#28436;&#31034;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#27700;&#24179;&#65292;&#21516;&#26102;&#25903;&#25345;&#26410;&#26469;&#20248;&#21270;&#20266;&#28436;&#31034;&#25552;&#39640;&#19978;&#19979;&#25991;&#23398;&#20064;&#34920;&#29616;&#32780;&#26080;&#38656;&#26631;&#35760;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2212.09865</link><description>&lt;p&gt;
Z-ICL: &#20351;&#29992;&#20266;&#26679;&#26412;&#36827;&#34892;&#38646;&#26679;&#26412;&#35270;&#35282;&#19979;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Z-ICL: Zero-Shot In-Context Learning with Pseudo-Demonstrations. (arXiv:2212.09865v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38646;&#26679;&#26412;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;Z-ICL&#65292;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#21407;&#22987;&#25991;&#26412;&#30340;&#20266;&#28436;&#31034;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#27700;&#24179;&#65292;&#21516;&#26102;&#25903;&#25345;&#26410;&#26469;&#20248;&#21270;&#20266;&#28436;&#31034;&#25552;&#39640;&#19978;&#19979;&#25991;&#23398;&#20064;&#34920;&#29616;&#32780;&#26080;&#38656;&#26631;&#35760;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36827;&#34892;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#20294;&#24403;&#27809;&#26377;&#25552;&#20379;&#28436;&#31034;&#26102;&#65292;&#24615;&#33021;&#20250;&#26174;&#33879;&#19979;&#38477;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38646;&#26679;&#26412;&#26041;&#27861;Z-ICL&#65292;&#22312;&#32473;&#23450;&#27979;&#35797;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#21407;&#22987;&#25991;&#26412;&#35821;&#26009;&#24211;&#26500;&#24314;&#20266;&#28436;&#31034;&#12290;&#20855;&#20307;&#22320;&#65292;&#20266;&#28436;&#31034;&#26159;&#36890;&#36807; (1) &#20174;&#35821;&#26009;&#24211;&#20013;&#25214;&#21040;&#19982;&#27979;&#35797;&#36755;&#20837;&#26368;&#30456;&#36817;&#30340;&#37051;&#23621;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#38543;&#26426;&#20219;&#21153;&#26631;&#31614;&#37197;&#23545;&#65292;&#20197;&#21450; (2) &#24212;&#29992;&#19968;&#32452;&#25216;&#26415;&#26469;&#20943;&#23569;&#27169;&#22411;&#20174;&#29983;&#25104;&#30340;&#28436;&#31034;&#20013;&#30452;&#25509;&#22797;&#21046;&#30340;&#25968;&#37327;&#26469;&#26500;&#24314;&#30340;&#12290;&#22312;&#20061;&#20010;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;Z-ICL &#30340;&#34920;&#29616;&#26174;&#33879;&#20248;&#20110;&#20197;&#21069;&#30340;&#38646;&#26679;&#26412;&#26041;&#27861;&#65292;&#24182;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#20013;&#19982;&#20351;&#29992;&#26377;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#30456;&#24403;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;Z-ICL &#25552;&#20379;&#20102;&#19968;&#20010;&#26174;&#33879;&#26356;&#39640;&#30340;&#27169;&#22411;&#38646;&#26679;&#26412;&#24615;&#33021;&#27700;&#24179;&#20272;&#35745;&#65292;&#24182;&#25903;&#25345;&#26410;&#26469;&#21162;&#21147;&#21457;&#23637;&#26356;&#22909;&#30340;&#20266;&#28436;&#31034;&#26469;&#25552;&#39640;&#19978;&#19979;&#25991;&#23398;&#20064;&#32780;&#26080;&#38656;&#20219;&#20309;&#26377;&#26631;&#35760;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although large language models can be prompted for both zero- and few-shot learning, performance drops significantly when no demonstrations are available. In this paper, we introduce Z-ICL, a new zero-shot method that closes the gap by constructing pseudo-demonstrations for a given test input using a raw text corpus. Concretely, pseudo-demonstrations are constructed by (1) finding the nearest neighbors to the test input from the corpus and pairing them with random task labels, and (2) applying a set of techniques to reduce the amount of direct copying the model does from the resulting demonstrations. Evaluation on nine classification datasets shows that Z-ICL outperforms previous zero-shot methods by a significant margin, and is on par with in-context learning with labeled training data in the few-shot setting. Overall, Z-ICL provides a significantly higher estimate of the zero-shot performance levels of a model, and supports future efforts to develop better pseudo-demonstrations that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#25968;&#25454;&#30693;&#35782;&#34701;&#21512;&#26041;&#27861;&#65292;&#21487;&#20197;&#21512;&#24182;&#22312;&#19981;&#21516;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#24314;&#31435;&#30340;&#21333;&#20010;&#27169;&#22411;&#65292;&#20197;&#24471;&#21040;&#19968;&#20010;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#39046;&#22495;&#19978;&#34920;&#29616;&#33391;&#22909;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#22495;&#22806;&#25968;&#25454;&#30340;&#21333;&#19968;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2212.09849</link><description>&lt;p&gt;
&#36890;&#36807;&#21512;&#24182;&#35821;&#35328;&#27169;&#22411;&#30340;&#26435;&#37325;&#23454;&#29616;&#26080;&#25968;&#25454;&#30693;&#35782;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Dataless Knowledge Fusion by Merging Weights of Language Models. (arXiv:2212.09849v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09849
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#25968;&#25454;&#30693;&#35782;&#34701;&#21512;&#26041;&#27861;&#65292;&#21487;&#20197;&#21512;&#24182;&#22312;&#19981;&#21516;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#24314;&#31435;&#30340;&#21333;&#20010;&#27169;&#22411;&#65292;&#20197;&#24471;&#21040;&#19968;&#20010;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#39046;&#22495;&#19978;&#34920;&#29616;&#33391;&#22909;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#22495;&#22806;&#25968;&#25454;&#30340;&#21333;&#19968;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24050;&#25104;&#20026;&#26500;&#24314;&#19979;&#28216;NLP&#27169;&#22411;&#30340;&#27969;&#34892;&#33539;&#24335;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#24050;&#32463;&#21487;&#29992;&#65292;&#20294;&#20854;&#35757;&#32451;&#25968;&#25454;&#19981;&#21487;&#29992;&#65292;&#30001;&#20110;&#25968;&#25454;&#38544;&#31169;&#25110;&#30693;&#35782;&#20135;&#26435;&#38382;&#39064;&#12290;&#36825;&#23601;&#36896;&#25104;&#20102;&#36328;&#27169;&#22411;&#34701;&#21512;&#30693;&#35782;&#20197;&#20135;&#29983;&#26356;&#22909;&#30340;&#21333;&#19968;&#27169;&#22411;&#30340;&#38556;&#30861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24314;&#31435;&#22312;&#19981;&#21516;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#30340;&#21333;&#20010;&#27169;&#22411;&#20043;&#38388;&#21512;&#24182;&#30340;&#38382;&#39064;&#65292;&#20197;&#24471;&#21040;&#19968;&#20010;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#39046;&#22495;&#19978;&#34920;&#29616;&#33391;&#22909;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#22495;&#22806;&#25968;&#25454;&#30340;&#21333;&#19968;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#25968;&#25454;&#30693;&#35782;&#34701;&#21512;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#21442;&#25968;&#31354;&#38388;&#20013;&#21512;&#24182;&#27169;&#22411;&#65292;&#30001;&#26435;&#37325;&#24341;&#23548;&#65292;&#20197;&#26368;&#23567;&#21270;&#21512;&#24182;&#27169;&#22411;&#21644;&#21333;&#20010;&#27169;&#22411;&#20043;&#38388;&#30340;&#39044;&#27979;&#24046;&#24322;&#12290;&#22312;&#19968;&#31995;&#21015;&#35780;&#20272;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#26174;&#33879;&#20248;&#20110;&#22914;Fisher&#21152;&#26435;&#24179;&#22343;&#25110;&#27169;&#22411;&#38598;&#25104;&#31561;&#22522;&#32447;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#22810;&#35821;&#35328;&#24494;&#35843;&#26367;&#20195;&#26041;&#26696;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#20219;&#20309;&#39069;&#22806;&#27880;&#37322;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#21487;&#27604;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning pre-trained language models has become the prevalent paradigm for building downstream NLP models. Oftentimes fine-tuned models are readily available but their training data is not, due to data privacy or intellectual property concerns. This creates a barrier to fusing knowledge across individual models to yield a better single model. In this paper, we study the problem of merging individual models built on different training data sets to obtain a single model that performs well both across all data set domains and can generalize on out-of-domain data. We propose a dataless knowledge fusion method that merges models in their parameter space, guided by weights that minimize prediction differences between the merged model and the individual models. Over a battery of evaluation settings, we show that the proposed method significantly outperforms baselines such as Fisher-weighted averaging or model ensembling. Further, we find that our method is a promising alternative to multi-
&lt;/p&gt;</description></item><item><title>NusaCrowd&#26159;&#19968;&#20010;&#21360;&#23612;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#36164;&#28304;&#30340;&#24320;&#28304;&#20513;&#35758;&#65292;&#24050;&#27719;&#38598;137&#20010;&#25968;&#25454;&#38598;&#21644;118&#20010;&#25968;&#25454;&#21152;&#36733;&#31243;&#24207;&#65292;&#20026;&#21360;&#23612;&#35821;&#21644;&#21360;&#24230;&#23612;&#35199;&#20122;&#26412;&#22320;&#35821;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#25552;&#20379;&#20102;&#22810;&#31181;&#23454;&#39564;&#25163;&#27573;&#12290;</title><link>http://arxiv.org/abs/2212.09648</link><description>&lt;p&gt;
NusaCrowd&#65306;&#21360;&#23612;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#36164;&#28304;&#30340;&#24320;&#28304;&#20513;&#35758;
&lt;/p&gt;
&lt;p&gt;
NusaCrowd: Open Source Initiative for Indonesian NLP Resources. (arXiv:2212.09648v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09648
&lt;/p&gt;
&lt;p&gt;
NusaCrowd&#26159;&#19968;&#20010;&#21360;&#23612;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#36164;&#28304;&#30340;&#24320;&#28304;&#20513;&#35758;&#65292;&#24050;&#27719;&#38598;137&#20010;&#25968;&#25454;&#38598;&#21644;118&#20010;&#25968;&#25454;&#21152;&#36733;&#31243;&#24207;&#65292;&#20026;&#21360;&#23612;&#35821;&#21644;&#21360;&#24230;&#23612;&#35199;&#20122;&#26412;&#22320;&#35821;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#25552;&#20379;&#20102;&#22810;&#31181;&#23454;&#39564;&#25163;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;NusaCrowd&#65292;&#36825;&#26159;&#19968;&#20010;&#21327;&#20316;&#20513;&#35758;&#65292;&#26088;&#22312;&#25910;&#38598;&#21644;&#32479;&#19968;&#21360;&#23612;&#35821;&#35328;&#30340;&#29616;&#26377;&#36164;&#28304;&#65292;&#21253;&#25324;&#24320;&#25918;&#20197;&#21069;&#38750;&#20844;&#24320;&#30340;&#36164;&#28304;&#12290;&#36890;&#36807;&#35813;&#20513;&#35758;&#65292;&#25105;&#20204;&#27719;&#38598;&#20102;137&#20010;&#25968;&#25454;&#38598;&#21644;118&#20010;&#26631;&#20934;&#21270;&#25968;&#25454;&#21152;&#36733;&#31243;&#24207;&#12290;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#24050;&#32463;&#32463;&#36807;&#25163;&#21160;&#21644;&#33258;&#21160;&#35780;&#20272;&#65292;&#23427;&#20204;&#30340;&#20215;&#20540;&#36890;&#36807;&#22810;&#20010;&#23454;&#39564;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;NusaCrowd&#30340;&#25968;&#25454;&#25910;&#38598;&#20351;&#24471;&#21487;&#20197;&#21019;&#24314;&#21360;&#23612;&#35821;&#21644;&#21360;&#24230;&#23612;&#35199;&#20122;&#26412;&#22320;&#35821;&#35328;&#30340;&#38646;&#26679;&#26412;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#22522;&#20934;&#65292;&#36827;&#19968;&#27493;&#25512;&#21160;&#20102;&#21360;&#23612;&#35821;&#21644;&#21360;&#24230;&#23612;&#35199;&#20122;&#26412;&#22320;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#22522;&#20934;&#30340;&#21019;&#24314;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#33268;&#21147;&#20110;&#25512;&#36827;&#23545;&#22312;&#20351;&#29992;&#24191;&#27867;&#30340;&#35821;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30740;&#31350;&#30340;&#21457;&#23637;&#65292;&#20174;&#32780;&#20351;&#20043;&#21463;&#21040;&#26356;&#22810;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present NusaCrowd, a collaborative initiative to collect and unify existing resources for Indonesian languages, including opening access to previously non-public resources. Through this initiative, we have brought together 137 datasets and 118 standardized data loaders. The quality of the datasets has been assessed manually and automatically, and their value is demonstrated through multiple experiments. NusaCrowd's data collection enables the creation of the first zero-shot benchmarks for natural language understanding and generation in Indonesian and the local languages of Indonesia. Furthermore, NusaCrowd brings the creation of the first multilingual automatic speech recognition benchmark in Indonesian and the local languages of Indonesia. Our work strives to advance natural language processing (NLP) research for languages that are under-represented despite being widely spoken.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;APOLLO&#65292;&#19968;&#31181;&#36866;&#24212;&#24615;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#36873;&#25321;&#29305;&#23450;&#30340;Wikipedia&#23376;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#20351;&#29992;&#20004;&#20010;&#33258;&#30417;&#30563;&#25439;&#22833;&#20989;&#25968;&#65292;&#25104;&#21151;&#22320;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.09282</link><description>&lt;p&gt;
APOLLO&#65306;&#38754;&#21521;&#36923;&#36753;&#25512;&#29702;&#30340;&#35821;&#35328;&#27169;&#22411;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#30340;&#31616;&#21333;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
APOLLO: A Simple Approach for Adaptive Pretraining of Language Models for Logical Reasoning. (arXiv:2212.09282v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09282
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;APOLLO&#65292;&#19968;&#31181;&#36866;&#24212;&#24615;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#36873;&#25321;&#29305;&#23450;&#30340;Wikipedia&#23376;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#20351;&#29992;&#20004;&#20010;&#33258;&#30417;&#30563;&#25439;&#22833;&#20989;&#25968;&#65292;&#25104;&#21151;&#22320;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#30340;&#36923;&#36753;&#25512;&#29702;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#33021;&#21147;&#65292;&#38656;&#35201;&#29702;&#35299;&#25991;&#26412;&#20013;&#23384;&#22312;&#30340;&#20449;&#24687;&#12289;&#23427;&#20204;&#30340;&#30456;&#20114;&#32852;&#31995;&#65292;&#28982;&#21518;&#36890;&#36807;&#23427;&#20204;&#26469;&#25512;&#26029;&#26032;&#30340;&#32467;&#35770;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;APOLLO&#65292;&#19968;&#31181;&#36866;&#24212;&#24615;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;&#25913;&#36827;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#36873;&#25321;&#20102;Wikipedia&#30340;&#23376;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#22522;&#20110;&#19968;&#32452;&#36923;&#36753;&#25512;&#29702;&#20851;&#38190;&#35789;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#33258;&#30417;&#30563;&#25439;&#22833;&#20989;&#25968;&#65306;&#20462;&#25913;&#36807;&#30340;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#25439;&#22833;&#21482;&#23545;&#21487;&#33021;&#38656;&#35201;&#26356;&#22810;&#25512;&#29702;&#32780;&#19981;&#20165;&#20165;&#26159;&#22522;&#26412;&#35821;&#35328;&#29702;&#35299;&#30340;&#29305;&#23450;&#35789;&#24615;&#30340;&#21333;&#35789;&#36827;&#34892;&#25513;&#30721;&#65292;&#20197;&#21450;&#21477;&#23376;&#32423;&#20998;&#31867;&#25439;&#22833;&#65292;&#25945;&#23548;&#27169;&#22411;&#21306;&#20998;&#36923;&#36753;&#19978;&#36830;&#25509;&#21644;&#19981;&#36830;&#25509;&#30340;&#21477;&#23376;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;APOLLO&#22312;&#22810;&#20010;&#36923;&#36753;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#19981;&#25439;&#22833;&#20854;&#22312;&#20854;&#20182;&#35821;&#35328;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Logical reasoning of text is an important ability that requires understanding the information present in the text, their interconnections, and then reasoning through them to infer new conclusions. Prior works on improving the logical reasoning ability of language models require complex processing of training data (e.g., aligning symbolic knowledge to text), yielding task-specific data augmentation solutions that restrict the learning of general logical reasoning skills. In this work, we propose APOLLO, an adaptively pretrained language model that has improved logical reasoning abilities. We select a subset of Wikipedia, based on a set of logical inference keywords, for continued pretraining of a language model. We use two self-supervised loss functions: a modified masked language modeling loss where only specific parts-of-speech words, that would likely require more reasoning than basic language understanding, are masked, and a sentence-level classification loss that teaches the model 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FiDO&#30340;&#35299;&#30721;&#22120;&#34701;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#20004;&#20010;&#31616;&#21333;&#30340;&#26356;&#25913;&#65292;&#26377;&#25928;&#32531;&#35299;&#20102;&#20869;&#23384;&#24102;&#23485;&#32422;&#26463;&#65292;&#21152;&#24555;&#20102;&#27169;&#22411;&#30340;&#25512;&#29702;&#36895;&#24230;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#65292;&#36798;&#21040;&#20102;&#39046;&#20808;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2212.08153</link><description>&lt;p&gt;
FiDO&#65306;&#38024;&#23545;&#26356;&#24378;&#30340;&#24615;&#33021;&#21644;&#26356;&#24555;&#30340;&#25512;&#29702;&#36827;&#34892;&#20248;&#21270;&#30340;&#35299;&#30721;&#22120;&#34701;&#21512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FiDO: Fusion-in-Decoder optimized for stronger performance and faster inference. (arXiv:2212.08153v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08153
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FiDO&#30340;&#35299;&#30721;&#22120;&#34701;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#20004;&#20010;&#31616;&#21333;&#30340;&#26356;&#25913;&#65292;&#26377;&#25928;&#32531;&#35299;&#20102;&#20869;&#23384;&#24102;&#23485;&#32422;&#26463;&#65292;&#21152;&#24555;&#20102;&#27169;&#22411;&#30340;&#25512;&#29702;&#36895;&#24230;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#65292;&#36798;&#21040;&#20102;&#39046;&#20808;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Fusion-in-Decoder (FiD)&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#35768;&#22810;&#30693;&#35782;&#23494;&#38598;&#22411;NLP&#20219;&#21153;&#19978;&#26641;&#31435;&#20102;&#19994;&#30028;&#26631;&#26438;&#12290;&#20294;&#26159;&#65292;FiD&#25152;&#20351;&#29992;&#30340;&#26550;&#26500;&#26159;&#36890;&#36807;&#23545;&#26631;&#20934;T5&#27169;&#22411;&#20570;&#26368;&#23567;&#20462;&#25913;&#32780;&#36873;&#25321;&#30340;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#36825;&#23545;&#20110;&#19968;&#20010;&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#26469;&#35828;&#26159;&#39640;&#24230;&#19981;&#20248;&#21270;&#30340;&#12290;&#29305;&#21035;&#22320;&#65292;FiD&#23558;&#22823;&#37096;&#20998;FLOPs&#20998;&#37197;&#32473;&#20102;&#32534;&#30721;&#22120;&#65292;&#32780;&#22823;&#22810;&#25968;&#25512;&#29702;&#26102;&#38388;&#26159;&#30001;&#20110;&#35299;&#30721;&#22120;&#20013;&#30340;&#20869;&#23384;&#24102;&#23485;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#31616;&#21333;&#30340;&#26356;&#25913;&#65292;&#20197;&#32531;&#35299;&#20869;&#23384;&#24102;&#23485;&#32422;&#26463;&#65292;&#24182;&#20351;&#25512;&#29702;&#36895;&#24230;&#25552;&#39640;7&#20493;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#20197;&#36866;&#24230;&#30340;&#25104;&#26412;&#20351;&#29992;&#26356;&#22823;&#30340;&#35299;&#30721;&#22120;&#12290;&#25105;&#20204;&#23558;&#32463;&#36807;&#19978;&#36848;&#20462;&#25913;&#30340;FiD&#31216;&#20026;FiDO&#65292;&#24182;&#23637;&#31034;&#23427;&#22312;&#24191;&#27867;&#30340;&#25512;&#29702;&#39044;&#31639;&#33539;&#22260;&#20869;&#27604;&#29616;&#26377;&#30340;FiD&#27169;&#22411;&#26174;&#33879;&#22320;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#20363;&#22914;&#65292;FiDO-Large-XXL&#27604;FiD-Base&#36827;&#34892;&#26356;&#24555;&#30340;&#25512;&#29702;&#65292;&#24182;&#23454;&#29616;&#20102;&#27604;FiD-Large&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fusion-in-Decoder (FiD) is a powerful retrieval-augmented language model that sets the state-of-the-art on many knowledge-intensive NLP tasks. However, the architecture used for FiD was chosen by making minimal modifications to a standard T5 model, which our analysis shows to be highly suboptimal for a retrieval-augmented model. In particular, FiD allocates the bulk of FLOPs to the encoder, while the majority of inference time results from memory bandwidth constraints in the decoder. We propose two simple changes to the FiD architecture to alleviate memory bandwidth constraints, and speed up inference by 7x. This allows us to use a much larger decoder at modest cost. We denote FiD with the above modifications as FiDO, and show that it strongly improves performance over existing FiD models for a wide range of inference budgets. For example, FiDO-Large-XXL performs faster inference than FiD-Base and achieves better performance than FiD-Large.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#38646;&#26679;&#26412;&#25512;&#29702;&#20013;&#30340;&#20559;&#35265;&#21644;&#26377;&#23475;&#24615;&#12290;&#36890;&#36807;&#23545;&#31038;&#20250;&#25935;&#24863;&#39046;&#22495;&#36827;&#34892;&#30340;&#25511;&#21046;&#35780;&#20272;&#65292;&#21457;&#29616;&#20351;&#29992;&#38646;&#26679;&#26412;CoT&#25512;&#29702;&#22312;&#25935;&#24863;&#39046;&#22495;&#20013;&#20250;&#26174;&#30528;&#22686;&#21152;&#20135;&#29983;&#26377;&#23475;&#25110;&#19981;&#33391;&#36755;&#20986;&#30340;&#21487;&#33021;&#24615;&#12290;&#21516;&#26102;&#20063;&#25351;&#20986;&#65292;&#26377;&#23475;&#30340;CoT&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#22686;&#21152;&#32780;&#22686;&#21152;&#65292;&#20294;&#38543;&#30528;&#25913;&#36827;&#30340;&#25351;&#20196;&#36981;&#24490;&#32780;&#20943;&#23569;&#12290;</title><link>http://arxiv.org/abs/2212.08061</link><description>&lt;p&gt;
&#20877;&#19977;&#32771;&#34385;&#65292;&#25105;&#20204;&#19981;&#35201;&#25353;&#27493;&#23601;&#29677;&#22320;&#24605;&#32771;&#65281;&#38646;&#26679;&#26412;&#25512;&#29702;&#20013;&#30340;&#20559;&#35265;&#21644;&#26377;&#23475;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
On Second Thought, Let's Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning. (arXiv:2212.08061v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08061
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#38646;&#26679;&#26412;&#25512;&#29702;&#20013;&#30340;&#20559;&#35265;&#21644;&#26377;&#23475;&#24615;&#12290;&#36890;&#36807;&#23545;&#31038;&#20250;&#25935;&#24863;&#39046;&#22495;&#36827;&#34892;&#30340;&#25511;&#21046;&#35780;&#20272;&#65292;&#21457;&#29616;&#20351;&#29992;&#38646;&#26679;&#26412;CoT&#25512;&#29702;&#22312;&#25935;&#24863;&#39046;&#22495;&#20013;&#20250;&#26174;&#30528;&#22686;&#21152;&#20135;&#29983;&#26377;&#23475;&#25110;&#19981;&#33391;&#36755;&#20986;&#30340;&#21487;&#33021;&#24615;&#12290;&#21516;&#26102;&#20063;&#25351;&#20986;&#65292;&#26377;&#23475;&#30340;CoT&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#22686;&#21152;&#32780;&#22686;&#21152;&#65292;&#20294;&#38543;&#30528;&#25913;&#36827;&#30340;&#25351;&#20196;&#36981;&#24490;&#32780;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#35777;&#26126;&#65292;&#29983;&#25104;&#8220;&#24605;&#32771;&#38142;&#8221;&#65288;CoT&#65289;&#21487;&#20197;&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19978;&#22987;&#32456;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#65288;&#20363;&#22914;&#31639;&#26415;&#65292;&#24120;&#35782;QA&#65289;&#65307;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#23545;&#20110;&#26356;&#22810;&#26679;&#21270;&#30340;&#25512;&#29702;&#31867;&#22411;&#65288;&#29305;&#21035;&#26159;&#22312;&#31038;&#20250;&#24773;&#22659;&#20013;&#65289;&#65292;&#35813;&#25913;&#36827;&#26159;&#21542;&#20250;&#20445;&#25345;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#38024;&#23545;&#20004;&#20010;&#31038;&#20250;&#25935;&#24863;&#39046;&#22495;&#30340;&#38646;&#26679;&#26412;CoT&#25511;&#21046;&#35780;&#20272;&#65306;&#26377;&#23475;&#38382;&#39064;&#21644;&#21051;&#26495;&#21360;&#35937;&#22522;&#20934;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#38646;&#26679;&#26412;CoT&#25512;&#29702;&#22312;&#25935;&#24863;&#39046;&#22495;&#20013;&#26174;&#30528;&#22686;&#21152;&#20102;&#27169;&#22411;&#20135;&#29983;&#26377;&#23475;&#25110;&#19981;&#33391;&#36755;&#20986;&#30340;&#21487;&#33021;&#24615;&#65292;&#36825;&#31181;&#36235;&#21183;&#22312;&#19981;&#21516;&#30340;&#25552;&#31034;&#26684;&#24335;&#21644;&#27169;&#22411;&#21464;&#20307;&#20013;&#22987;&#32456;&#23384;&#22312;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#26377;&#23475;&#30340;CoT&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#22686;&#21152;&#32780;&#22686;&#21152;&#65292;&#20294;&#38543;&#30528;&#25913;&#36827;&#30340;&#25351;&#20196;&#36981;&#24490;&#32780;&#20943;&#23569;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#22312;&#28041;&#21450;&#36793;&#32536;&#21270;&#32676;&#20307;&#25110;&#25935;&#24863;&#35805;&#39064;&#26102;&#65292;&#24212;&#35880;&#24910;&#20351;&#29992;&#38646;&#26679;&#26412;CoT&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating a Chain of Thought (CoT) has been shown to consistently improve large language model (LLM) performance on a wide range of NLP tasks. However, prior work has mainly focused on logical reasoning tasks (e.g. arithmetic, commonsense QA); it remains unclear whether improvements hold for more diverse types of reasoning, especially in socially situated contexts. Concretely, we perform a controlled evaluation of zero-shot CoT across two socially sensitive domains: harmful questions and stereotype benchmarks. We find that zero-shot CoT reasoning in sensitive domains significantly increases a model's likelihood to produce harmful or undesirable output, with trends holding across different prompt formats and model variants. Furthermore, we show that harmful CoTs increase with model size, but decrease with improved instruction following. Our work suggests that zero-shot CoT should be used with caution on socially important tasks, especially when marginalized groups or sensitive topics a
&lt;/p&gt;</description></item><item><title>KnowledgeDA&#26159;&#19968;&#31181;&#32479;&#19968;&#30340;&#39046;&#22495;&#35821;&#35328;&#27169;&#22411;&#24320;&#21457;&#26381;&#21153;&#65292;&#36890;&#36807;&#39046;&#22495;&#30693;&#35782;&#22270;&#35889;&#22686;&#24378;&#20219;&#21153;&#29305;&#23450;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#25552;&#21319;&#29305;&#23450;&#39046;&#22495;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2212.05251</link><description>&lt;p&gt;
&#19968;&#31181;&#32479;&#19968;&#30340;&#30693;&#35782;&#22270;&#35889;&#22686;&#24378;&#26381;&#21153;&#65292;&#29992;&#20110;&#25552;&#21319;&#29305;&#23450;&#39046;&#22495;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
A Unified Knowledge Graph Augmentation Service for Boosting Domain-specific NLP Tasks. (arXiv:2212.05251v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05251
&lt;/p&gt;
&lt;p&gt;
KnowledgeDA&#26159;&#19968;&#31181;&#32479;&#19968;&#30340;&#39046;&#22495;&#35821;&#35328;&#27169;&#22411;&#24320;&#21457;&#26381;&#21153;&#65292;&#36890;&#36807;&#39046;&#22495;&#30693;&#35782;&#22270;&#35889;&#22686;&#24378;&#20219;&#21153;&#29305;&#23450;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#25552;&#21319;&#29305;&#23450;&#39046;&#22495;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19987;&#27880;&#20110;&#29305;&#23450;&#39046;&#22495;&#35821;&#26009;&#24211;&#30340;&#39044;&#35757;&#32451;&#36807;&#31243;&#65292;&#19968;&#20123;&#39046;&#22495;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#24050;&#32463;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#19968;&#31181;&#32479;&#19968;&#30340;&#33539;&#24335;&#26469;&#22312;PLM&#24494;&#35843;&#38454;&#27573;&#27880;&#20837;&#39046;&#22495;&#30693;&#35782;&#30340;&#30740;&#31350;&#36824;&#24456;&#23569;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;KnowledgeDA&#65292;&#19968;&#31181;&#32479;&#19968;&#30340;&#39046;&#22495;&#35821;&#35328;&#27169;&#22411;&#24320;&#21457;&#26381;&#21153;&#65292;&#36890;&#36807;&#39046;&#22495;&#30693;&#35782;&#22270;&#35889;&#22686;&#24378;&#20219;&#21153;&#29305;&#23450;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#32473;&#23450;&#39046;&#22495;&#29305;&#23450;&#30340;&#20219;&#21153;&#25991;&#26412;&#36755;&#20837;&#65292;KnowledgeDA&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#19968;&#20010;&#39046;&#22495;&#29305;&#23450;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20998;&#20026;&#19977;&#20010;&#27493;&#39588;&#65306;&#65288;i&#65289;&#36890;&#36807;&#23884;&#20837;&#30456;&#20284;&#24615;&#26041;&#27861;&#22312;&#25991;&#26412;&#20013;&#23450;&#20301;&#39046;&#22495;&#30693;&#35782;&#23454;&#20307;&#65307;&#65288;ii&#65289;&#36890;&#36807;&#20174;&#30693;&#35782;&#22270;&#35889;&#21644;&#35757;&#32451;&#25968;&#25454;&#30340;&#20004;&#20010;&#35270;&#35282;&#26816;&#32034;&#21487;&#26367;&#25442;&#30340;&#39046;&#22495;&#23454;&#20307;&#23545;&#26469;&#29983;&#25104;&#22686;&#24378;&#26679;&#26412;&#65307;&#65288;iii&#65289;&#36890;&#36807;&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#35780;&#20272;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#22686;&#24378;&#26679;&#26412;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;KnowledgeDA&#30340;&#21407;&#22411;&#65292;&#23398;&#20064;&#20004;&#20010;&#39046;&#22495;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20998;&#21035;&#20026;&#21307;&#30103;&#20445;&#20581;&#21644;&#36719;&#20214;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
By focusing the pre-training process on domain-specific corpora, some domain-specific pre-trained language models (PLMs) have achieved state-of-the-art results. However, it is under-investigated to design a unified paradigm to inject domain knowledge in the PLM fine-tuning stage. We propose KnowledgeDA, a unified domain language model development service to enhance the task-specific training procedure with domain knowledge graphs. Given domain-specific task texts input, KnowledgeDA can automatically generate a domain-specific language model following three steps: (i) localize domain knowledge entities in texts via an embedding-similarity approach; (ii) generate augmented samples by retrieving replaceable domain entity pairs from two views of both knowledge graph and training data; (iii) select high-quality augmented samples for fine-tuning via confidence-based assessment. We implement a prototype of KnowledgeDA to learn language models for two domains, healthcare and software developme
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;MMPLM&#65289;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#36716;&#31227;&#23398;&#20064;&#22312;&#20020;&#24202;&#39046;&#22495;&#26426;&#22120;&#32763;&#35793;&#20013;&#25104;&#21151;&#24212;&#29992;&#20110;&#23436;&#20840;&#26410;&#30693;&#30340;&#35821;&#35328;&#23545;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;fine-tune&#36807;&#30340;MMPLM&#22312;&#20020;&#24202;&#39046;&#22495;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2210.06068</link><description>&lt;p&gt;
&#22522;&#20110;&#36716;&#31227;&#23398;&#20064;&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#22312;&#20020;&#24202;&#39046;&#22495;&#30340;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Investigating Massive Multilingual Pre-Trained Machine Translation Models for Clinical Domain via Transfer Learning. (arXiv:2210.06068v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;MMPLM&#65289;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#36716;&#31227;&#23398;&#20064;&#22312;&#20020;&#24202;&#39046;&#22495;&#26426;&#22120;&#32763;&#35793;&#20013;&#25104;&#21151;&#24212;&#29992;&#20110;&#23436;&#20840;&#26410;&#30693;&#30340;&#35821;&#35328;&#23545;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;fine-tune&#36807;&#30340;MMPLM&#22312;&#20020;&#24202;&#39046;&#22495;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;MMPLM&#65289;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#30340;&#36229;&#33021;&#21147;&#21644;&#39044;&#30693;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;MMPLM&#22312;&#20020;&#24202;&#39046;&#22495;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#20013;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#36716;&#31227;&#23398;&#20064;&#29992;&#20110;&#23436;&#20840;&#26410;&#30693;&#30340;&#35821;&#35328;&#23545;&#12290;&#25105;&#20204;&#20351;&#29992;Meta-AI&#30340;MMPLM&#8220;wmt21-dense-24-wide-en-X&#21644;X-en&#65288;WMT21fb&#65289;&#8221;&#65292;&#36825;&#20123;&#27169;&#22411;&#39044;&#20808;&#35757;&#32451;&#20102;7&#31181;&#35821;&#35328;&#23545;&#21644;14&#20010;&#32763;&#35793;&#26041;&#21521;&#65292;&#21253;&#25324;&#33521;&#35821;&#21040;&#25463;&#20811;&#35821;&#12289;&#24503;&#35821;&#12289;&#35946;&#33832;&#35821;&#12289;&#20912;&#23707;&#35821;&#12289;&#26085;&#35821;&#12289;&#20420;&#35821;&#21644;&#27721;&#35821;&#20197;&#21450;&#30456;&#21453;&#30340;&#26041;&#21521;&#12290;&#25105;&#20204;&#23545;&#36825;&#20123;MMPLM&#36827;&#34892;fine-tune&#65292;&#38024;&#23545;&#23427;&#20204;&#21407;&#22987;&#30340;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;\textit{&#23436;&#20840;&#19981;&#23384;&#22312;}&#30340;&#33521;&#25991;-\textit{&#35199;&#29677;&#29273;&#35821;}&#35821;&#35328;&#23545;&#65292;&#26174;&#24335;&#21644;&#38544;&#24335;&#22320;&#36827;&#34892;fine-tune&#12290;&#25105;&#20204;&#20026;&#27492; fine-tune &#20570;&#22909;&#32463;&#36807;&#20180;&#32454;&#23545;&#40784;&#30340;\textit{&#20020;&#24202;}&#39046;&#22495;&#25968;&#25454;&#65292;&#36825;&#19982;&#23427;&#20204;&#21407;&#22987;&#30340;&#28151;&#21512;&#39046;&#22495;&#30693;&#35782;&#19981;&#21516;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;fine-tune&#36807;&#30340;MMPLM&#22312;&#20020;&#24202;&#39046;&#22495;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Massively multilingual pre-trained language models (MMPLMs) are developed in recent years demonstrating superpowers and the pre-knowledge they acquire for downstream tasks. This work investigates whether MMPLMs can be applied to clinical domain machine translation (MT) towards entirely unseen languages via transfer learning. We carry out an experimental investigation using Meta-AI's MMPLMs ``wmt21-dense-24-wide-en-X and X-en (WMT21fb)'' which were pre-trained on 7 language pairs and 14 translation directions including English to Czech, German, Hausa, Icelandic, Japanese, Russian, and Chinese, and the opposite direction. We fine-tune these MMPLMs towards English-\textit{Spanish} language pair which \textit{did not exist at all} in their original pre-trained corpora both implicitly and explicitly. We prepare carefully aligned \textit{clinical} domain data for this fine-tuning, which is different from their original mixed domain knowledge. Our experimental result shows that the fine-tunin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#25513;&#33180;&#22810;&#27169;&#24577;&#24314;&#27169;&#26041;&#27861;&#65292;&#20197;&#23398;&#20064;&#32454;&#31890;&#24230;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#65292;&#36890;&#36807;&#38544;&#24335;&#21644;&#26174;&#24335;&#30446;&#26631;&#26469;&#24674;&#22797;&#32852;&#21512;&#25513;&#33180;&#20449;&#21495;&#20197;&#25552;&#39640;&#32454;&#21270;&#30340;&#22270;&#20687;-&#25991;&#26412;&#20132;&#20114;&#12290;</title><link>http://arxiv.org/abs/2210.04183</link><description>&lt;p&gt;
MAMO&#65306;&#38754;&#21521;&#32454;&#31890;&#24230;&#35270;&#35273;-&#35821;&#35328;&#34920;&#31034;&#23398;&#20064;&#30340;&#25513;&#33180;&#22810;&#27169;&#24577;&#24314;&#27169;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MAMO: Masked Multimodal Modeling for Fine-Grained Vision-Language Representation Learning. (arXiv:2210.04183v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04183
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#25513;&#33180;&#22810;&#27169;&#24577;&#24314;&#27169;&#26041;&#27861;&#65292;&#20197;&#23398;&#20064;&#32454;&#31890;&#24230;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#65292;&#36890;&#36807;&#38544;&#24335;&#21644;&#26174;&#24335;&#30446;&#26631;&#26469;&#24674;&#22797;&#32852;&#21512;&#25513;&#33180;&#20449;&#21495;&#20197;&#25552;&#39640;&#32454;&#21270;&#30340;&#22270;&#20687;-&#25991;&#26412;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#22312;&#21508;&#31181;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#22823;&#37096;&#20998;&#26041;&#27861;&#37117;&#20027;&#35201;&#33268;&#21147;&#20110;&#24314;&#31435;&#20840;&#23616;&#32423;&#21035;&#30340;&#22270;&#20687;&#19982;&#35821;&#35328;&#23545;&#40784;&#65292;&#32570;&#20047;&#26377;&#25928;&#30340;&#32454;&#31890;&#24230;&#22270;&#20687;-&#25991;&#26412;&#20132;&#20114;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#25513;&#33180;&#22810;&#27169;&#24577;&#24314;&#27169;&#26041;&#27861;&#65292;&#20197;&#23398;&#20064;&#32454;&#31890;&#24230;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#22270;&#20687;-&#25991;&#26412;&#36755;&#20837;&#36827;&#34892;&#32852;&#21512;&#25513;&#33180;&#65292;&#24182;&#38598;&#25104;&#20102;&#26174;&#24335;&#21644;&#38544;&#24335;&#30446;&#26631;&#26469;&#24674;&#22797;&#25513;&#33180;&#20449;&#21495;&#12290;&#20854;&#20013;&#65292;&#38544;&#24335;&#30446;&#26631;&#20026;&#35270;&#35273;&#21644;&#35821;&#35328;&#25552;&#20379;&#32479;&#19968;&#19988;&#26080;&#20559;&#24046;&#30340;&#30446;&#26631;&#65292;&#27169;&#22411;&#39044;&#27979;&#26410;&#25513;&#33180;&#36755;&#20837;&#30340;&#28508;&#22312;&#22810;&#27169;&#24577;&#34920;&#31034;&#65307;&#26174;&#24335;&#30446;&#26631;&#21017;&#36890;&#36807;&#24674;&#22797;&#22270;&#20687;&#22359;&#30340;&#21160;&#37327;&#35270;&#35273;&#29305;&#24449;&#21644;&#21333;&#35789;&#26631;&#35760;&#30340;&#27010;&#24565;&#65292;&#36827;&#19968;&#27493;&#20016;&#23500;&#22810;&#27169;&#24577;&#34920;&#31034;&#12290;&#36890;&#36807;&#36825;&#26679;&#30340;&#25513;&#33180;&#24314;&#27169;&#36807;&#31243;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#19981;&#20165;&#21487;&#20197;&#23398;&#20064;&#21040;&#32454;&#31890;&#24230;&#30340;&#22810;&#27169;&#24577;&#20132;&#20114;&#65292;&#36824;&#33021;&#23398;&#20064;&#21040;&#26377;&#24847;&#20041;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal representation learning has shown promising improvements on various vision-language tasks. Most existing methods excel at building global-level alignment between vision and language while lacking effective fine-grained image-text interaction. In this paper, we propose a jointly masked multimodal modeling method to learn fine-grained multimodal representations. Our method performs joint masking on image-text input and integrates both implicit and explicit targets for the masked signals to recover. The implicit target provides a unified and debiased objective for vision and language, where the model predicts latent multimodal representations of the unmasked input. The explicit target further enriches the multimodal representations by recovering high-level and semantically meaningful information: momentum visual features of image patches and concepts of word tokens. Through such a masked modeling process, our model not only learns fine-grained multimodal interaction, but also a
&lt;/p&gt;</description></item><item><title>UCEpic&#36890;&#36807;&#23558;&#26041;&#38754;&#35268;&#21010;&#21644;&#35789;&#27719;&#32422;&#26463;&#32479;&#19968;&#32771;&#34385;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25554;&#20837;&#24335;&#29983;&#25104;&#30340;&#20010;&#24615;&#21270;&#35299;&#37322;&#22411;&#25512;&#33616;&#27169;&#22411;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35299;&#37322;&#30340;&#27969;&#30021;&#24615;&#12289;&#36830;&#36143;&#24615;&#21644;&#21305;&#37197;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2209.13885</link><description>&lt;p&gt;
UCEpic&#65306;&#32479;&#19968;&#32771;&#34385;&#26041;&#38754;&#35268;&#21010;&#21644;&#35789;&#27719;&#32422;&#26463;&#65292;&#29983;&#25104;&#35299;&#37322;&#22411;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
UCEpic: Unifying Aspect Planning and Lexical Constraints for Generating Explanations in Recommendation. (arXiv:2209.13885v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.13885
&lt;/p&gt;
&lt;p&gt;
UCEpic&#36890;&#36807;&#23558;&#26041;&#38754;&#35268;&#21010;&#21644;&#35789;&#27719;&#32422;&#26463;&#32479;&#19968;&#32771;&#34385;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25554;&#20837;&#24335;&#29983;&#25104;&#30340;&#20010;&#24615;&#21270;&#35299;&#37322;&#22411;&#25512;&#33616;&#27169;&#22411;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35299;&#37322;&#30340;&#27969;&#30021;&#24615;&#12289;&#36830;&#36143;&#24615;&#21644;&#21305;&#37197;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23545;&#35299;&#37322;&#22411;&#25512;&#33616;&#36827;&#34892;&#20010;&#24615;&#21270;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#26041;&#38754;&#35268;&#21010;&#21644;&#35789;&#27719;&#32422;&#26463;&#32479;&#19968;&#32771;&#34385;&#30340;&#27169;&#22411;UCEpic&#65292;&#36890;&#36807;&#25554;&#20837;&#24335;&#29983;&#25104;&#30340;&#26041;&#24335;&#29983;&#25104;&#39640;&#36136;&#37327;&#20010;&#24615;&#21270;&#30340;&#25512;&#33616;&#32467;&#26524;&#35299;&#37322;&#12290;&#36890;&#36807;&#22312;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#21644;&#20010;&#24615;&#21270;&#25512;&#33616;&#29983;&#25104;&#36807;&#31243;&#20013;&#24341;&#20837;&#35789;&#27719;&#32422;&#26463;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#29983;&#25104;&#35299;&#37322;&#30340;&#27969;&#30021;&#24615;&#12289;&#36830;&#36143;&#24615;&#21644;&#21305;&#37197;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;UCEpic&#26041;&#27861;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized natural language generation for explainable recommendations plays a key role in justifying why a recommendation might match a user's interests. Existing models usually control the generation process by aspect planning. While promising, these aspect-planning methods struggle to generate specific information correctly, which prevents generated explanations from being convincing. In this paper, we claim that introducing lexical constraints can alleviate the above issues. We propose a model, UCEpic, that generates high-quality personalized explanations for recommendation results by unifying aspect planning and lexical constraints in an insertion-based generation manner.  Methodologically, to ensure text generation quality and robustness to various lexical constraints, we pre-train a non-personalized text generator via our proposed robust insertion process. Then, to obtain personalized explanations under this framework of insertion-based generation, we design a method of incorp
&lt;/p&gt;</description></item><item><title>PaLI&#26159;&#19968;&#31181;&#32852;&#21512;&#32553;&#25918;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;-&#22270;&#20687;&#27169;&#22411;&#65292;&#21487;&#23545;&#22270;&#20687;&#21644;&#25991;&#26412;&#36827;&#34892;&#24314;&#27169;&#21644;&#25191;&#34892;&#35768;&#22810;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#21033;&#29992;Transformer&#21644;Vision Transformer&#31561;&#20808;&#21069;&#30340;&#33021;&#21147;&#21644;&#25104;&#26412;&#12290;&#32852;&#21512;&#32553;&#25918;&#22312;&#27492;&#20219;&#21153;&#20013;&#24456;&#37325;&#35201;&#65292;&#25152;&#20197;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;40&#20159;&#21442;&#25968;&#30340;Vision Transformer&#65292;&#20197;&#20415;&#21033;&#29992;&#26356;&#22823;&#23481;&#37327;&#30340;&#35270;&#35273;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2209.06794</link><description>&lt;p&gt;
PaLI: &#19968;&#31181;&#32852;&#21512;&#32553;&#25918;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;-&#22270;&#20687;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PaLI: A Jointly-Scaled Multilingual Language-Image Model. (arXiv:2209.06794v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.06794
&lt;/p&gt;
&lt;p&gt;
PaLI&#26159;&#19968;&#31181;&#32852;&#21512;&#32553;&#25918;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;-&#22270;&#20687;&#27169;&#22411;&#65292;&#21487;&#23545;&#22270;&#20687;&#21644;&#25991;&#26412;&#36827;&#34892;&#24314;&#27169;&#21644;&#25191;&#34892;&#35768;&#22810;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#21033;&#29992;Transformer&#21644;Vision Transformer&#31561;&#20808;&#21069;&#30340;&#33021;&#21147;&#21644;&#25104;&#26412;&#12290;&#32852;&#21512;&#32553;&#25918;&#22312;&#27492;&#20219;&#21153;&#20013;&#24456;&#37325;&#35201;&#65292;&#25152;&#20197;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;40&#20159;&#21442;&#25968;&#30340;Vision Transformer&#65292;&#20197;&#20415;&#21033;&#29992;&#26356;&#22823;&#23481;&#37327;&#30340;&#35270;&#35273;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#32553;&#25918;&#21644;&#28789;&#27963;&#30340;&#20219;&#21153;&#25509;&#21475;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;PaLI&#65288;Pathways Language and Image model&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#27169;&#22411;&#65292;&#21487;&#23558;&#36825;&#31181;&#26041;&#27861;&#25193;&#23637;&#21040;&#35821;&#35328;&#21644;&#35270;&#35273;&#30340;&#32852;&#21512;&#24314;&#27169;&#12290;PaLI&#22522;&#20110;&#35270;&#35273;&#21644;&#25991;&#26412;&#36755;&#20837;&#29983;&#25104;&#25991;&#26412;&#65292;&#24182;&#36890;&#36807;&#27492;&#25509;&#21475;&#25191;&#34892;&#35768;&#22810;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#22312;&#35768;&#22810;&#35821;&#35328;&#20013;&#23436;&#25104;&#12290;&#20026;&#20102;&#35757;&#32451;PaLI&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#22823;&#22411;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#21644;Vision Transformers&#65288;ViT&#65289;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#21033;&#29992;&#23427;&#20204;&#29616;&#26377;&#30340;&#33021;&#21147;&#24182;&#21033;&#29992;&#35757;&#32451;&#23427;&#20204;&#30340;&#37325;&#22823;&#25104;&#26412;&#12290;&#25105;&#20204;&#21457;&#29616;&#32852;&#21512;&#32553;&#25918;&#35270;&#35273;&#21644;&#35821;&#35328;&#32452;&#20214;&#24456;&#37325;&#35201;&#12290;&#30001;&#20110;&#29616;&#26377;&#30340;&#35821;&#35328;Transformer&#27604;&#23427;&#20204;&#30340;&#35270;&#35273;&#23545;&#24212;&#29289;&#35201;&#22823;&#24471;&#22810;&#65292;&#22240;&#27492;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#22823;&#22411;&#30340;40&#20159;&#21442;&#25968;ViT&#65288;ViT-e&#65289;&#26469;&#37327;&#21270;&#21363;&#20351;&#26356;&#22823;&#23481;&#37327;&#30340;&#35270;&#35273;&#27169;&#22411;&#30340;&#22909;&#22788;&#12290;&#20026;&#20102;&#35757;&#32451;PaLI&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#26032;&#30340;&#22270;&#20687;-&#25991;&#26412;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#22823;&#22411;&#22810;&#35821;&#35328;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#28151;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective scaling and a flexible task interface enable large language models to excel at many tasks. We present PaLI (Pathways Language and Image model), a model that extends this approach to the joint modeling of language and vision. PaLI generates text based on visual and textual inputs, and with this interface performs many vision, language, and multimodal tasks, in many languages. To train PaLI, we make use of large pre-trained encoder-decoder language models and Vision Transformers (ViTs). This allows us to capitalize on their existing capabilities and leverage the substantial cost of training them. We find that joint scaling of the vision and language components is important. Since existing Transformers for language are much larger than their vision counterparts, we train a large, 4-billion parameter ViT (ViT-e) to quantify the benefits from even larger-capacity vision models. To train PaLI, we create a large multilingual mix of pretraining tasks, based on a new image-text traini
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#22312;&#32447;&#26597;&#35810;&#37325;&#20889;&#30340;&#33539;&#24335;&#65306;&#29983;&#25104;&#24335;&#65288;NLG&#65289;&#21644;&#23494;&#38598;&#26816;&#32034;&#65288;DR&#65289;&#26041;&#27861;&#12290;&#36890;&#36807;&#27604;&#36739;&#36825;&#20004;&#31181;&#26041;&#27861;&#24182;&#32467;&#21512;&#20248;&#21183;&#65292;&#25552;&#20986;&#20102;CLOVER-Unity&#27169;&#22411;&#65292;&#20854;NLG&#21644;DR&#32452;&#20214;&#22987;&#32456;&#20248;&#20110;&#21333;&#29420;&#35757;&#32451;&#30340;&#32452;&#20214;&#20197;&#21450;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2209.05861</link><description>&lt;p&gt;
&#32852;&#21512;&#29983;&#25104;&#19982;&#23494;&#38598;&#26816;&#32034;&#29992;&#20110;&#36190;&#21161;&#25628;&#32034;&#30340;&#26597;&#35810;&#37325;&#20889;
&lt;/p&gt;
&lt;p&gt;
Unified Generative &amp; Dense Retrieval for Query Rewriting in Sponsored Search. (arXiv:2209.05861v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.05861
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#22312;&#32447;&#26597;&#35810;&#37325;&#20889;&#30340;&#33539;&#24335;&#65306;&#29983;&#25104;&#24335;&#65288;NLG&#65289;&#21644;&#23494;&#38598;&#26816;&#32034;&#65288;DR&#65289;&#26041;&#27861;&#12290;&#36890;&#36807;&#27604;&#36739;&#36825;&#20004;&#31181;&#26041;&#27861;&#24182;&#32467;&#21512;&#20248;&#21183;&#65292;&#25552;&#20986;&#20102;CLOVER-Unity&#27169;&#22411;&#65292;&#20854;NLG&#21644;DR&#32452;&#20214;&#22987;&#32456;&#20248;&#20110;&#21333;&#29420;&#35757;&#32451;&#30340;&#32452;&#20214;&#20197;&#21450;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36190;&#21161;&#25628;&#32034;&#26159;&#25628;&#32034;&#24341;&#25806;&#30340;&#19968;&#20010;&#20851;&#38190;&#25910;&#20837;&#26469;&#28304;&#65292;&#24191;&#21578;&#20027;&#36890;&#36807;&#31454;&#20215;&#26041;&#24335;&#38024;&#23545;&#29992;&#25143;&#25110;&#24863;&#20852;&#36259;&#30340;&#25628;&#32034;&#26597;&#35810;&#31454;&#26631;&#20851;&#38190;&#23383;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20851;&#38190;&#23383;&#31354;&#38388;&#24222;&#22823;&#19988;&#21160;&#24577;&#21464;&#21270;&#12289;&#27169;&#31946;&#30340;&#29992;&#25143;/&#24191;&#21578;&#20027;&#24847;&#22270;&#20197;&#21450;&#21508;&#31181;&#21487;&#33021;&#30340;&#20027;&#39064;&#21644;&#35821;&#35328;&#65292;&#25214;&#21040;&#19982;&#32473;&#23450;&#26597;&#35810;&#30456;&#20851;&#30340;&#20851;&#38190;&#23383;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#32447;&#26597;&#35810;&#37325;&#20889;&#30340;&#20004;&#31181;&#33539;&#24335;&#65306;&#29983;&#25104;&#24335;&#65288;NLG&#65289;&#21644;&#23494;&#38598;&#26816;&#32034;&#65288;DR&#65289;&#26041;&#27861;&#20043;&#38388;&#30340;&#20840;&#38754;&#27604;&#36739;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#20004;&#31181;&#26041;&#27861;&#37117;&#25552;&#20379;&#20102;&#20114;&#34917;&#30340;&#19988;&#21487;&#30456;&#21152;&#30340;&#20248;&#21183;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#30001;&#36825;&#20004;&#31181;&#26041;&#27861;&#26816;&#32034;&#21040;&#30340;&#39640;&#36136;&#37327;&#20851;&#38190;&#23383;&#20013;&#26377;&#32422;40%&#26159;&#29420;&#29305;&#30340;&#65292;&#32780;&#21478;&#19968;&#31181;&#26041;&#27861;&#27809;&#26377;&#25214;&#21040;&#12290;&#20026;&#20102;&#21457;&#25381;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CLOVER-Unity&#65292;&#36825;&#26159;&#19968;&#31181;&#23558;&#29983;&#25104;&#24335;&#19982;&#23494;&#38598;&#26816;&#32034;&#26041;&#27861;&#32467;&#21512;&#22312;&#19968;&#20010;&#27169;&#22411;&#20013;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#31163;&#32447;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;CLOVER-Unity&#30340;NLG&#21644;DR&#32452;&#20214;&#22987;&#32456;&#20248;&#20110;&#21333;&#29420;&#35757;&#32451;&#30340;NLG&#21644;DR&#32452;&#20214;&#20197;&#21450;&#20854;&#20182;&#24378;&#22522;&#32447;&#27169;&#22411;&#65292;&#20855;&#26377;&#26816;&#32034;&#21040;&#20851;&#38190;&#23383;&#30340;&#30456;&#20851;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sponsored search is a key revenue source for search engines, where advertisers bid on keywords to target users or search queries of interest. However, finding relevant keywords for a given query is challenging due to the large and dynamic keyword space, ambiguous user/advertiser intents, and diverse possible topics and languages. In this work, we present a comprehensive comparison between two paradigms for online query rewriting: Generative (NLG) and Dense Retrieval (DR) methods. We observe that both methods offer complementary benefits that are additive. As a result, we show that around 40% of the high-quality keywords retrieved by the two approaches are unique and not retrieved by the other. To leverage the strengths of both methods, we propose CLOVER-Unity, a novel approach that unifies generative and dense retrieval methods in one single model. Through offline experiments, we show that the NLG and DR components of CLOVER-Unity consistently outperform individually trained NLG and DR
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#35270;&#39057;&#31034;&#20363;&#23398;&#20064;&#25163;&#35821;&#25340;&#20889;&#22312;&#26426;&#22120;&#20154;&#20013;&#30340;&#23454;&#29616;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#35757;&#32451;&#27169;&#20223;&#36816;&#21160;&#30340;&#31574;&#30053;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#35270;&#35273;&#27169;&#22411;&#20174;RGB&#35270;&#39057;&#20013;&#25552;&#21462;&#25163;&#30340;&#19977;&#32500;&#23039;&#24577;&#65292;&#24182;&#35782;&#21035;&#20986;&#26368;&#20339;&#30340;&#27169;&#20223;&#36229;&#21442;&#25968;&#38598;&#65292;&#26412;&#25991;&#25104;&#21151;&#23637;&#31034;&#20102;&#26041;&#27861;&#30340;&#26222;&#36866;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.05135</link><description>&lt;p&gt;
&#35821;&#35328;&#31526;&#21495;&#30340;&#34920;&#29616;&#65306;&#22522;&#20110;&#31034;&#33539;&#30340;&#20154;&#26426;&#20132;&#20114;&#20013;&#65292;&#20307;&#24863;&#25163;&#35821;&#25163;&#25351;&#25340;&#20889;&#30340;&#32763;&#35793;&#26426;&#22120;&#20154;&#33719;&#21462;
&lt;/p&gt;
&lt;p&gt;
Signs of Language: Embodied Sign Language Fingerspelling Acquisition from Demonstrations for Human-Robot Interaction. (arXiv:2209.05135v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.05135
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#35270;&#39057;&#31034;&#20363;&#23398;&#20064;&#25163;&#35821;&#25340;&#20889;&#22312;&#26426;&#22120;&#20154;&#20013;&#30340;&#23454;&#29616;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#35757;&#32451;&#27169;&#20223;&#36816;&#21160;&#30340;&#31574;&#30053;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#35270;&#35273;&#27169;&#22411;&#20174;RGB&#35270;&#39057;&#20013;&#25552;&#21462;&#25163;&#30340;&#19977;&#32500;&#23039;&#24577;&#65292;&#24182;&#35782;&#21035;&#20986;&#26368;&#20339;&#30340;&#27169;&#20223;&#36229;&#21442;&#25968;&#38598;&#65292;&#26412;&#25991;&#25104;&#21151;&#23637;&#31034;&#20102;&#26041;&#27861;&#30340;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#26426;&#22120;&#20154;&#20013;&#32454;&#33268;&#30340;&#21160;&#20316;&#26159;&#19968;&#20010;&#20855;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#26426;&#22120;&#20154;&#25163;&#30340;&#19978;&#19979;&#25991;&#20013;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#35270;&#39057;&#31034;&#20363;&#23398;&#20064;&#26080;&#39069;&#22806;&#20449;&#24687;&#19979;&#30340;&#29087;&#32451;&#36816;&#21160;&#27169;&#20223;&#65292;&#20197;&#33719;&#24471;&#25163;&#35821;&#25340;&#20889;&#22312;&#26426;&#22120;&#20154;&#20013;&#30340;&#23454;&#29616;&#12290;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#20010;&#26426;&#22120;&#20154;&#25163;&#30340;URDF&#27169;&#22411;&#65292;&#24182;&#20351;&#27599;&#20010;&#20851;&#33410;&#21482;&#26377;&#19968;&#20010;&#33268;&#21160;&#22120;&#12290;&#28982;&#21518;&#25105;&#20204;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#35270;&#35273;&#27169;&#22411;&#20174;RGB&#35270;&#39057;&#20013;&#25552;&#21462;&#25163;&#30340;&#19977;&#32500;&#23039;&#24577;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;(&#21363;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#21644;&#36719;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;)&#26469;&#35757;&#32451;&#19968;&#31181;&#33021;&#22815;&#22797;&#21046;&#31034;&#33539;&#36816;&#21160;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#22522;&#20110;&#21442;&#32771;&#36816;&#21160;&#35782;&#21035;&#20986;&#26368;&#20339;&#30340;&#27169;&#20223;&#36229;&#21442;&#25968;&#38598;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#20845;&#20010;&#23545;&#24212;&#20110;&#25340;&#20889;&#23383;&#27597;&#30340;&#19981;&#21516;&#20219;&#21153;&#36827;&#34892;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning fine-grained movements is a challenging topic in robotics, particularly in the context of robotic hands. One specific instance of this challenge is the acquisition of fingerspelling sign language in robots. In this paper, we propose an approach for learning dexterous motor imitation from video examples without additional information. To achieve this, we first build a URDF model of a robotic hand with a single actuator for each joint. We then leverage pre-trained deep vision models to extract the 3D pose of the hand from RGB videos. Next, using state-of-the-art reinforcement learning algorithms for motion imitation (namely, proximal policy optimization and soft actor-critic), we train a policy to reproduce the movement extracted from the demonstrations. We identify the optimal set of hyperparameters for imitation based on a reference motion. Finally, we demonstrate the generalizability of our approach by testing it on six different tasks, corresponding to fingerspelled letters.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26500;&#24314;&#20102;&#29983;&#25104;&#24615;&#19987;&#21033;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#20154;&#31867;&#33410;&#30465;&#25353;&#38190;&#25968;&#37327;&#30340;&#24230;&#37327;&#26041;&#27861;&#35780;&#20272;&#20102;&#20854;&#34920;&#29616;&#65292;&#21457;&#29616;&#22312;&#19987;&#21033;&#39046;&#22495;&#32487;&#32493;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#21487;&#33021;&#26159;&#19981;&#24517;&#35201;&#30340;&#12290;</title><link>http://arxiv.org/abs/2206.14578</link><description>&lt;p&gt;
&#35780;&#20272;&#29983;&#25104;&#19987;&#21033;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Evaluating Generative Patent Language Models. (arXiv:2206.14578v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.14578
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26500;&#24314;&#20102;&#29983;&#25104;&#24615;&#19987;&#21033;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#20154;&#31867;&#33410;&#30465;&#25353;&#38190;&#25968;&#37327;&#30340;&#24230;&#37327;&#26041;&#27861;&#35780;&#20272;&#20102;&#20854;&#34920;&#29616;&#65292;&#21457;&#29616;&#22312;&#19987;&#21033;&#39046;&#22495;&#32487;&#32493;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#21487;&#33021;&#26159;&#19981;&#24517;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#36741;&#21161;&#20154;&#31867;&#20889;&#20316;&#26041;&#38754;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#26088;&#22312;&#26500;&#24314;&#19987;&#21033;&#39046;&#22495;&#30340;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#20174;&#20154;&#31867;&#30340;&#35282;&#24230;&#35780;&#20272;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#36890;&#36807;&#24230;&#37327;&#22522;&#20110;&#29983;&#25104;&#24615;&#19987;&#21033;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#34917;&#20840;&#25152;&#33021;&#33410;&#30465;&#30340;&#25353;&#38190;&#27604;&#29575;&#26469;&#34913;&#37327;&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;&#36825;&#31181;&#20197;&#25353;&#38190;&#20026;&#22522;&#30784;&#30340;&#24230;&#37327;&#26041;&#27861;&#19981;&#21516;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#26631;&#35760;&#30340;&#26426;&#22120;-centric&#30340;&#24230;&#37327;&#26041;&#27861;&#12290;&#26412;&#25991;&#20013;&#26500;&#24314;&#30340;&#26368;&#22823;&#27169;&#22411;&#22823;&#23567;&#20026;6B&#65292;&#22312;&#19987;&#21033;&#39046;&#22495;&#26159;&#26368;&#20808;&#36827;&#30340;&#12290;&#22522;&#20110;&#27492;&#24230;&#37327;&#26041;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#26368;&#22823;&#30340;&#27169;&#22411;&#24182;&#19981;&#19968;&#23450;&#26159;&#20154;&#31867;-centric&#24230;&#37327;&#26041;&#27861;&#30340;&#26368;&#20339;&#36873;&#25321;&#65292;&#36825;&#24847;&#21619;&#30528;&#22914;&#26524;&#30446;&#30340;&#26159;&#36890;&#36807;&#33258;&#21160;&#34917;&#20840;&#26469;&#36741;&#21161;&#20154;&#31867;&#20889;&#20316;&#65292;&#37027;&#20040;&#22312;&#19987;&#21033;&#39046;&#22495;&#32487;&#32493;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#21487;&#33021;&#26159;&#19981;&#24517;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative language models are promising for assisting human writing in various domains. This manuscript aims to build generative language models in the patent domain and evaluate model performance from a human-centric perspective. The perspective is to measure the ratio of keystrokes that can be saved by autocompletion based on generative patent language models. A higher ratio means a more effective model which can save more keystrokes. This metric can be used to benchmark model performance. The metric is different from conventional machine-centric metrics that are token-based instead of keystroke-based. In terms of model size, the largest model built in this manuscript is 6B, which is state-of-the-art in the patent domain. Based on the metric, it is found that the largest model is not necessarily the best for the human-centric metric. The finding means that keeping increasing model sizes in the patent domain might be unnecessary if the purpose is to assist human writing with autocomp
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22359;&#21270;&#20559;&#24046;&#32531;&#35299;&#26041;&#27861;&#65292;&#22312;&#25512;&#29702;&#26102;&#38388;&#25353;&#38656;&#38598;&#25104;&#21040;&#26680;&#24515;&#27169;&#22411;&#20013;&#30340;&#29420;&#31435;&#21435;&#20559;&#32622;&#23376;&#32593;&#32476;&#65292;&#22312;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#24180;&#40836;&#31561;&#21463;&#20445;&#25252;&#23646;&#24615;&#30340;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#35813;&#26041;&#27861;&#22312;&#32531;&#35299;&#20559;&#24046;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#65292;&#24182;&#19988;&#22312;&#31934;&#24230;&#21644;&#28789;&#27963;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2205.15171</link><description>&lt;p&gt;
&#24102;&#26377;&#23646;&#24615;&#21024;&#38500;&#23376;&#32593;&#32476;&#30340;&#27169;&#22359;&#21270;&#21644;&#25353;&#38656;&#20559;&#24046;&#32531;&#35299;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Modular and On-demand Bias Mitigation with Attribute-Removal Subnetworks. (arXiv:2205.15171v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.15171
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22359;&#21270;&#20559;&#24046;&#32531;&#35299;&#26041;&#27861;&#65292;&#22312;&#25512;&#29702;&#26102;&#38388;&#25353;&#38656;&#38598;&#25104;&#21040;&#26680;&#24515;&#27169;&#22411;&#20013;&#30340;&#29420;&#31435;&#21435;&#20559;&#32622;&#23376;&#32593;&#32476;&#65292;&#22312;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#24180;&#40836;&#31561;&#21463;&#20445;&#25252;&#23646;&#24615;&#30340;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#35813;&#26041;&#27861;&#22312;&#32531;&#35299;&#20559;&#24046;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#65292;&#24182;&#19988;&#22312;&#31934;&#24230;&#21644;&#28789;&#27963;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20250;&#20559;&#35265;&#21453;&#26144;&#22312;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21450;&#20854;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24494;&#35843;&#29256;&#26412;&#20013;&#12290;&#24120;&#35265;&#30340;&#22788;&#29702;&#20559;&#24046;&#30340;&#26041;&#27861;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#20248;&#21270;&#26631;&#20934;&#65292;&#24182;&#26356;&#26032;&#27169;&#22411;&#20197;&#36798;&#21040;&#26032;&#30340;&#21435;&#20559;&#32622;&#29366;&#24577;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#26368;&#32456;&#29992;&#25143;&#21644;&#20174;&#19994;&#20154;&#21592;&#21487;&#33021;&#26356;&#21916;&#27426;&#20999;&#25442;&#22238;&#21407;&#22987;&#27169;&#22411;&#65292;&#25110;&#20165;&#23545;&#29305;&#23450;&#23376;&#38598;&#30340;&#20445;&#25252;&#23646;&#24615;&#24212;&#29992;&#21435;&#20559;&#32622;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22359;&#21270;&#20559;&#24046;&#32531;&#35299;&#26041;&#27861;&#65292;&#21253;&#25324;&#29420;&#31435;&#39640;&#24230;&#31232;&#30095;&#30340;&#21435;&#20559;&#32622;&#23376;&#32593;&#32476;&#65292;&#20854;&#20013;&#27599;&#20010;&#21435;&#20559;&#32622;&#27169;&#22359;&#21487;&#20197;&#22312;&#25512;&#29702;&#26102;&#38388;&#25353;&#38656;&#38598;&#25104;&#21040;&#26680;&#24515;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20511;&#37492;&#20102;&#8220;diff&#8221;&#21098;&#26525;&#30340;&#27010;&#24565;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#21512;&#20110;&#21508;&#31181;&#34920;&#31034;&#20998;&#31163;&#20248;&#21270;&#30340;&#26032;&#22411;&#35757;&#32451;&#26041;&#24335;&#12290;&#25105;&#20204;&#22312;&#20855;&#26377;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#24180;&#40836;&#31561;&#21463;&#20445;&#25252;&#23646;&#24615;&#30340;&#19977;&#20010;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22359;&#21270;&#26041;&#27861;&#22312;&#32531;&#35299;&#20559;&#24046;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#65292;&#24182;&#19988;&#22312;&#31934;&#24230;&#21644;&#28789;&#27963;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Societal biases are reflected in large pre-trained language models and their fine-tuned versions on downstream tasks. Common in-processing bias mitigation approaches, such as adversarial training and mutual information removal, introduce additional optimization criteria, and update the model to reach a new debiased state. However, in practice, end-users and practitioners might prefer to switch back to the original model, or apply debiasing only on a specific subset of protected attributes. To enable this, we propose a novel modular bias mitigation approach, consisting of stand-alone highly sparse debiasing subnetworks, where each debiasing module can be integrated into the core model on-demand at inference time. Our approach draws from the concept of \emph{diff} pruning, and proposes a novel training regime adaptable to various representation disentanglement optimizations. We conduct experiments on three classification tasks with gender, race, and age as protected attributes. The resul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#35821;&#35328;&#21508;&#21521;&#24322;&#24615;&#36328;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#22312;&#36328;&#35821;&#35328;&#20219;&#21153;&#19978;&#30340;&#27979;&#35797;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#21487;&#20197;&#38024;&#23545;&#29305;&#23450;&#36755;&#20837;&#36827;&#34892;&#26657;&#20934;&#32780;&#19981;&#24433;&#21709;&#27169;&#22411;&#30340;&#21407;&#22987;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2205.12677</link><description>&lt;p&gt;
&#12298;&#35821;&#35328;&#30340;&#21508;&#21521;&#24322;&#24615;&#36328;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;&#12299;
&lt;/p&gt;
&lt;p&gt;
Language Anisotropic Cross-Lingual Model Editing. (arXiv:2205.12677v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#35821;&#35328;&#21508;&#21521;&#24322;&#24615;&#36328;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#22312;&#36328;&#35821;&#35328;&#20219;&#21153;&#19978;&#30340;&#27979;&#35797;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#21487;&#20197;&#38024;&#23545;&#29305;&#23450;&#36755;&#20837;&#36827;&#34892;&#26657;&#20934;&#32780;&#19981;&#24433;&#21709;&#27169;&#22411;&#30340;&#21407;&#22987;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22312;&#22810;&#31181;&#35821;&#35328;&#20013;&#23398;&#20064;&#29305;&#23450;&#20219;&#21153;&#30340;&#33021;&#21147;&#25110;&#35760;&#20303;&#20107;&#23454;&#65292;&#20294;&#19981;&#21487;&#36991;&#20813;&#22320;&#20250;&#22312;&#20351;&#29992;&#29305;&#23450;&#36755;&#20837;&#26102;&#20570;&#20986;&#19981;&#33391;&#39044;&#27979;&#12290;&#19982;&#27492;&#31867;&#20284;&#65292;&#27169;&#22411;&#32534;&#36753;&#26088;&#22312;&#20107;&#21518;&#26657;&#20934;&#38024;&#23545;&#29305;&#23450;&#36755;&#20837;&#30340;&#27169;&#22411;&#65292;&#24182;&#20445;&#25345;&#27169;&#22411;&#30340;&#21407;&#22987;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#24037;&#20316;&#20165;&#30740;&#31350;&#20102;&#21333;&#35821;&#22659;&#32467;&#26500;&#65292;&#32570;&#20047;&#36328;&#35821;&#22659;&#20256;&#36882;&#24615;&#20197;&#21516;&#26102;&#25191;&#34892;&#36328;&#35821;&#35328;&#32534;&#36753;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#36328;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#36328;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;&#20219;&#21153;&#21644;&#30456;&#24212;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#20854;&#20013;&#19968;&#20010;&#35821;&#35328;&#30340;&#32534;&#36753;&#20250;&#20256;&#25773;&#21040;&#20854;&#20182;&#35821;&#35328;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#24179;&#34892;&#35821;&#26009;&#24211;&#33258;&#28982;&#22320;&#36866;&#24212;&#21333;&#35821;&#22659;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#21040;&#36328;&#35821;&#22659;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35821;&#35328;&#21508;&#21521;&#24322;&#24615;&#32534;&#36753;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;&#36328;&#35821;&#22659;&#32534;&#36753;&#65292;&#36890;&#36807;&#22686;&#24378;&#27599;&#31181;&#35821;&#35328;&#30340;&#19981;&#21516;&#21442;&#25968;&#23376;&#38598;&#12290;&#22312;&#26032;&#23450;&#20041;&#30340;&#36328;&#35821;&#22659;&#27169;&#22411;&#32534;&#36753;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#30340;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#21333;&#35821;&#35328;&#21644;&#22810;&#35821;&#35328;&#35780;&#20272;&#20013;&#37117;&#21462;&#24471;&#20102;&#26174;&#30528;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26657;&#20934;&#38024;&#23545;&#29305;&#23450;&#36755;&#20837;&#30340;&#39044;&#27979;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#30340;&#21407;&#22987;&#34892;&#20026;&#65292;&#20351;&#20854;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#26356;&#21152;&#21487;&#38752;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual pre-trained language models can learn task-specific abilities or memorize facts across multiple languages but inevitably make undesired predictions with specific inputs. Under similar observation, model editing aims to post-hoc calibrate a model targeted to specific inputs with keeping the model's raw behavior. However, existing work only studies the monolingual scenario, which lacks the cross-lingual transferability to perform editing simultaneously across languages. In this work, we focus on cross-lingual model editing. Firstly, we define the cross-lingual model editing task and corresponding metrics, where an edit in one language propagates to the others. Next, we propose a framework to naturally adapt monolingual model editing approaches to the cross-lingual scenario using parallel corpus. Further, we propose language anisotropic editing to improve cross-lingual editing by amplifying different subsets of parameters for each language. On the newly defined cross-lingual 
&lt;/p&gt;</description></item><item><title>UnifieR&#26159;&#19968;&#20010;&#23558;PLM&#30340;&#23494;&#38598;&#21521;&#37327;&#21644;&#22522;&#20110;&#35789;&#27719;&#34920;&#30340;&#26816;&#32034;&#32479;&#19968;&#21040;&#19968;&#20010;&#27169;&#22411;&#20013;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#27573;&#33853;&#26816;&#32034;&#22522;&#20934;&#27979;&#35797;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.11194</link><description>&lt;p&gt;
&#32479;&#19968;&#26816;&#32034;&#22120;&#65306;&#22823;&#35268;&#27169;&#26816;&#32034;&#30340;&#32479;&#19968;&#26816;&#32034;&#22120;
&lt;/p&gt;
&lt;p&gt;
UnifieR: A Unified Retriever for Large-Scale Retrieval. (arXiv:2205.11194v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.11194
&lt;/p&gt;
&lt;p&gt;
UnifieR&#26159;&#19968;&#20010;&#23558;PLM&#30340;&#23494;&#38598;&#21521;&#37327;&#21644;&#22522;&#20110;&#35789;&#27719;&#34920;&#30340;&#26816;&#32034;&#32479;&#19968;&#21040;&#19968;&#20010;&#27169;&#22411;&#20013;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#27573;&#33853;&#26816;&#32034;&#22522;&#20934;&#27979;&#35797;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#26816;&#32034;&#26159;&#25351;&#22312;&#32473;&#23450;&#26597;&#35810;&#30340;&#24773;&#20917;&#19979;&#20174;&#22823;&#37327;&#25991;&#26723;&#20013;&#21484;&#22238;&#30456;&#20851;&#25991;&#26723;&#12290;&#23427;&#20381;&#36182;&#20110;&#34920;&#24449;&#23398;&#20064;&#65292;&#23558;&#25991;&#26723;&#21644;&#26597;&#35810;&#23884;&#20837;&#21040;&#19968;&#20010;&#20849;&#21516;&#30340;&#35821;&#20041;&#32534;&#30721;&#31354;&#38388;&#20013;&#12290;&#26681;&#25454;&#32534;&#30721;&#31354;&#38388;&#65292;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#30340;&#26368;&#36817;&#26816;&#32034;&#26041;&#27861;&#21487;&#20197;&#31895;&#30053;&#22320;&#20998;&#20026;&#23494;&#38598;&#21521;&#37327;&#25110;&#22522;&#20110;&#35789;&#27719;&#34920;&#30340;&#33539;&#20363;&#12290;&#36825;&#20004;&#31181;&#33539;&#20363;&#22312;&#19981;&#21516;&#31890;&#24230;&#30340;&#20840;&#23616;&#24207;&#21015;&#32423;&#21387;&#32553;&#21644;&#23616;&#37096;&#21333;&#35789;&#32423;&#19978;&#19979;&#25991;&#20013;&#23637;&#29616;&#20102;PLMs&#30340;&#34920;&#24449;&#33021;&#21147;&#12290;&#21463;&#21040;&#23427;&#20204;&#20114;&#34917;&#30340;&#20840;&#23616;&#23616;&#37096;&#19978;&#19979;&#25991;&#21270;&#21644;&#19981;&#21516;&#30340;&#20195;&#34920;&#35270;&#35282;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#32479;&#19968;&#26816;&#32034;&#22120;&#65292;&#23427;&#23558;&#23494;&#38598;&#21521;&#37327;&#21644;&#22522;&#20110;&#35789;&#27719;&#34920;&#30340;&#26816;&#32034;&#32479;&#19968;&#21040;&#19968;&#20010;&#27169;&#22411;&#20013;&#65292;&#20855;&#26377;&#21452;&#37325;&#34920;&#31034;&#33021;&#21147;&#12290;&#23545;&#27573;&#33853;&#26816;&#32034;&#22522;&#20934;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#23427;&#22312;&#20004;&#20010;&#33539;&#20363;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#26356;&#22909;&#30340;&#26816;&#32034;&#36136;&#37327;&#30340;uni-retrieval&#26041;&#26696;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#35813;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale retrieval is to recall relevant documents from a huge collection given a query. It relies on representation learning to embed documents and queries into a common semantic encoding space. According to the encoding space, recent retrieval methods based on pre-trained language models (PLM) can be coarsely categorized into either dense-vector or lexicon-based paradigms. These two paradigms unveil the PLMs' representation capability in different granularities, i.e., global sequence-level compression and local word-level contexts, respectively. Inspired by their complementary global-local contextualization and distinct representing views, we propose a new learning framework, UnifieR which unifies dense-vector and lexicon-based retrieval in one model with a dual-representing capability. Experiments on passage retrieval benchmarks verify its effectiveness in both paradigms. A uni-retrieval scheme is further presented with even better retrieval quality. We lastly evaluate the model 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Heterformer&#30340;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#21516;&#26102;&#22788;&#29702;&#24322;&#26500;&#32467;&#26500;&#21644;&#20016;&#23500;&#30340;&#25991;&#26412;&#35821;&#20041;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2205.10282</link><description>&lt;p&gt;
Heterformer&#65306;&#22522;&#20110;Transformer&#30340;&#24322;&#26500;&#25991;&#26412;&#32593;&#32476;&#28145;&#24230;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Heterformer: Transformer-based Deep Node Representation Learning on Heterogeneous Text-Rich Networks. (arXiv:2205.10282v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.10282
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Heterformer&#30340;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#21516;&#26102;&#22788;&#29702;&#24322;&#26500;&#32467;&#26500;&#21644;&#20016;&#23500;&#30340;&#25991;&#26412;&#35821;&#20041;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#34920;&#31034;&#23398;&#20064;&#26088;&#22312;&#20026;&#27599;&#20010;&#33410;&#28857;&#25512;&#23548;&#20986;&#26377;&#24847;&#20041;&#30340;&#21521;&#37327;&#34920;&#31034;&#65292;&#20174;&#32780;&#20419;&#36827;&#35832;&#22914;&#38142;&#25509;&#39044;&#27979;&#12289;&#33410;&#28857;&#20998;&#31867;&#21644;&#33410;&#28857;&#32858;&#31867;&#31561;&#19979;&#28216;&#20219;&#21153;&#12290;&#22312;&#24322;&#26500;&#25991;&#26412;&#32593;&#32476;&#20013;&#65292;&#30001;&#20110;&#25991;&#26412;&#30340;&#23384;&#22312;&#19982;&#32570;&#22833;&#20197;&#21450;&#22810;&#31181;&#31867;&#22411;&#30340;&#33410;&#28857;&#21644;&#36793;&#32536;&#24418;&#25104;&#30340;&#24322;&#26500;&#32593;&#32476;&#32467;&#26500;&#65292;&#36825;&#39033;&#20219;&#21153;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#30001;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#24050;&#32463;&#35777;&#26126;&#20102;&#22312;&#33719;&#24471;&#24191;&#27867;&#36890;&#29992;&#24615;&#25991;&#26412;&#34920;&#31034;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#25991;&#26412;&#20016;&#23500;&#30340;&#32593;&#32476;&#34920;&#31034;&#23398;&#20064;&#20013;&#65292;&#20154;&#20204;&#24050;&#32463;&#20184;&#20986;&#20102;&#22823;&#37327;&#30340;&#21162;&#21147;&#26469;&#23558;PLMs&#21253;&#21547;&#21040;&#20854;&#20013;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#21487;&#20197;&#26377;&#25928;&#22320;&#20849;&#21516;&#32771;&#34385;&#24322;&#26500;&#32467;&#26500;&#65288;&#32593;&#32476;&#65289;&#20449;&#24687;&#21644;&#27599;&#20010;&#33410;&#28857;&#30340;&#20016;&#23500;&#25991;&#26412;&#35821;&#20041;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Heterformer&#65292;&#19968;&#31181;&#24322;&#26500;&#32593;&#32476;&#24378;&#21270;&#30340;Transformer&#65292;&#21487;&#21516;&#26102;&#22788;&#29702;&#32593;&#32476;&#32467;&#26500;&#20449;&#24687;&#21644;&#27599;&#20010;&#33410;&#28857;&#30340;&#20016;&#23500;&#25991;&#26412;&#35821;&#20041;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representation learning on networks aims to derive a meaningful vector representation for each node, thereby facilitating downstream tasks such as link prediction, node classification, and node clustering. In heterogeneous text-rich networks, this task is more challenging due to (1) presence or absence of text: Some nodes are associated with rich textual information, while others are not; (2) diversity of types: Nodes and edges of multiple types form a heterogeneous network structure. As pretrained language models (PLMs) have demonstrated their effectiveness in obtaining widely generalizable text representations, a substantial amount of effort has been made to incorporate PLMs into representation learning on text-rich networks. However, few of them can jointly consider heterogeneous structure (network) information as well as rich textual semantic information of each node effectively. In this paper, we propose Heterformer, a Heterogeneous Network-Empowered Transformer that performs cont
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#35821;&#35328;&#30340;&#35780;&#20215;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#20013;&#24402;&#22240;&#26041;&#27861;&#30340;&#24544;&#23454;&#24230;&#21644;&#21487;&#20449;&#24230;&#65292;&#24182;&#19988;&#36890;&#36807;&#39640;&#20142;&#30340;&#35299;&#37322;&#25193;&#20805;&#20102;XNLI&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#22312;&#21487;&#20449;&#24230;&#21644;&#21487;&#20449;&#24230;&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#30340;&#24402;&#22240;&#26041;&#27861;&#26377;&#25152;&#19981;&#21516;&#12290;</title><link>http://arxiv.org/abs/2204.05428</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20013;&#24402;&#22240;&#26041;&#27861;&#35780;&#20272;&#30340;&#22810;&#35821;&#35328;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Multilingual Perspective Towards the Evaluation of Attribution Methods in Natural Language Inference. (arXiv:2204.05428v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.05428
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#35821;&#35328;&#30340;&#35780;&#20215;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#20013;&#24402;&#22240;&#26041;&#27861;&#30340;&#24544;&#23454;&#24230;&#21644;&#21487;&#20449;&#24230;&#65292;&#24182;&#19988;&#36890;&#36807;&#39640;&#20142;&#30340;&#35299;&#37322;&#25193;&#20805;&#20102;XNLI&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#22312;&#21487;&#20449;&#24230;&#21644;&#21487;&#20449;&#24230;&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#30340;&#24402;&#22240;&#26041;&#27861;&#26377;&#25152;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#24402;&#22240;&#26041;&#27861;&#30340;&#35780;&#20272;&#38598;&#20013;&#22312;&#33521;&#35821;&#35821;&#35328;&#19978;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35821;&#35328;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#20219;&#21153;&#30340;&#24402;&#22240;&#26041;&#27861;&#30340;&#24544;&#23454;&#24230;&#21644;&#21487;&#20449;&#24230;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#35789;&#23545;&#40784;&#30340;&#26032;&#22411;&#36328;&#35821;&#35328;&#31574;&#30053;&#26469;&#34913;&#37327;&#24544;&#23454;&#24230;&#65292;&#25490;&#38500;&#20102;&#21024;&#20943;&#35780;&#20272;&#30340;&#32570;&#28857;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#24402;&#22240;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#32771;&#34385;&#20102;&#19981;&#21516;&#30340;&#36755;&#20986;&#26426;&#21046;&#21644;&#32858;&#21512;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#22522;&#20110;&#39640;&#20142;&#30340;&#35299;&#37322;&#25193;&#20805;&#20102;XNLI&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#24102;&#26377;&#39640;&#20142;&#30340;&#22810;&#35821;&#35328;NLI&#25968;&#25454;&#38598;&#65292;&#20197;&#25903;&#25345;&#26410;&#26469;&#30340;ExNLP&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#24615;&#33021;&#26368;&#20339;&#30340;&#24402;&#22240;&#26041;&#27861;&#23545;&#20110;&#21487;&#20449;&#24230;&#21644;&#21487;&#20449;&#24230;&#30340;&#34920;&#29616;&#26159;&#19981;&#21516;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most evaluations of attribution methods focus on the English language. In this work, we present a multilingual approach for evaluating attribution methods for the Natural Language Inference (NLI) task in terms of faithfulness and plausibility. First, we introduce a novel cross-lingual strategy to measure faithfulness based on word alignments, which eliminates the drawbacks of erasure-based evaluations.We then perform a comprehensive evaluation of attribution methods, considering different output mechanisms and aggregation methods. Finally, we augment the XNLI dataset with highlight-based explanations, providing a multilingual NLI dataset with highlights, to support future exNLP studies. Our results show that attribution methods performing best for plausibility and faithfulness are different.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;&#30340;&#35789;&#34955;&#12289;&#24207;&#21015;&#12289;&#22270;&#24418;&#21644;&#20998;&#23618;&#26041;&#27861;&#65292;&#21457;&#29616;&#22522;&#20110;&#22270;&#24418;&#30340;&#26041;&#27861;&#26080;&#27861;&#36229;&#36234;&#29616;&#20195;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24182;&#19988;&#29978;&#33267;&#26377;&#26102;&#34920;&#29616;&#19981;&#22914;&#26631;&#20934;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36136;&#30097;&#20102;&#36807;&#21435;&#20960;&#24180;&#20013;&#20026;&#24320;&#21457;&#26032;&#30340;&#22270;&#24418;&#26041;&#27861;&#25237;&#20837;&#30340;&#24040;&#22823;&#21162;&#21147;&#20197;&#21450;&#23427;&#20204;&#20026;&#25991;&#26412;&#20998;&#31867;&#24102;&#26469;&#30340;&#25215;&#35834;&#12290;</title><link>http://arxiv.org/abs/2204.03954</link><description>&lt;p&gt;
&#25105;&#20204;&#30495;&#30340;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#36827;&#23637;&#21527;&#65311;&#38024;&#23545;&#21333;&#26631;&#31614;&#21644;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#30340;&#35789;&#34955;&#12289;&#24207;&#21015;&#12289;&#22270;&#21644;&#23618;&#27425;&#32467;&#26500;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Are We Really Making Much Progress? Bag-of-Words vs. Sequence vs. Graph vs. Hierarchy for Single- and Multi-Label Text Classification. (arXiv:2204.03954v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.03954
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;&#30340;&#35789;&#34955;&#12289;&#24207;&#21015;&#12289;&#22270;&#24418;&#21644;&#20998;&#23618;&#26041;&#27861;&#65292;&#21457;&#29616;&#22522;&#20110;&#22270;&#24418;&#30340;&#26041;&#27861;&#26080;&#27861;&#36229;&#36234;&#29616;&#20195;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24182;&#19988;&#29978;&#33267;&#26377;&#26102;&#34920;&#29616;&#19981;&#22914;&#26631;&#20934;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36136;&#30097;&#20102;&#36807;&#21435;&#20960;&#24180;&#20013;&#20026;&#24320;&#21457;&#26032;&#30340;&#22270;&#24418;&#26041;&#27861;&#25237;&#20837;&#30340;&#24040;&#22823;&#21162;&#21147;&#20197;&#21450;&#23427;&#20204;&#20026;&#25991;&#26412;&#20998;&#31867;&#24102;&#26469;&#30340;&#25215;&#35834;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#27969;&#34892;&#24341;&#21457;&#20102;&#21333;&#26631;&#31614;&#21644;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#30340;&#22270;&#24418;&#26041;&#27861;&#30340;&#22797;&#33487;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20110;&#22270;&#24418;&#30340;&#26041;&#27861;&#26159;&#21542;&#27604;&#26631;&#20934;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#29616;&#20195;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26356;&#26377;&#30410;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#27604;&#36739;&#20102;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;&#30340;&#20016;&#23500;&#30340;&#35789;&#34955;&#12289;&#22522;&#20110;&#24207;&#21015;&#12289;&#22522;&#20110;&#22270;&#24418;&#21644;&#20998;&#23618;&#26041;&#27861;&#12290;&#25105;&#20204;&#32858;&#21512;&#20102;&#26469;&#33258;&#25991;&#29486;&#30340;&#32467;&#26524;&#65292;&#22312;5&#20010;&#21333;&#26631;&#31614;&#21644;7&#20010;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598;&#19978;&#36816;&#34892;&#20102;&#25105;&#20204;&#33258;&#24049;&#30340;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26126;&#30830;&#34920;&#26126;&#65292;&#22312;&#21333;&#26631;&#31614;&#21644;&#22810;&#26631;&#31614;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#22522;&#20110;&#22270;&#24418;&#30340;&#26041;&#27861;&#26080;&#27861;&#36229;&#36234;&#31934;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#26377;&#26102;&#29978;&#33267;&#34920;&#29616;&#19981;&#22914;&#35789;&#34955;&#19978;&#30340;&#26631;&#20934;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36825;&#36136;&#30097;&#20102;&#36807;&#21435;&#20960;&#24180;&#20013;&#20026;&#24320;&#21457;&#26032;&#30340;&#22270;&#24418;&#26041;&#27861;&#25237;&#20837;&#30340;&#24040;&#22823;&#21162;&#21147;&#20197;&#21450;&#23427;&#20204;&#20026;&#25991;&#26412;&#20998;&#31867;&#24102;&#26469;&#30340;&#25215;&#35834;&#12290;
&lt;/p&gt;
&lt;p&gt;
The popularity of graph neural networks has triggered a resurgence of graph-based methods for single-label and multi-label text classification. However, it is unclear whether these graph-based methods are beneficial compared to standard machine learning methods and modern pretrained language models. We compare a rich selection of bag-of-words, sequence-based, graph-based, and hierarchical methods for text classification. We aggregate results from the literature over 5 single-label and 7 multi-label datasets and run our own experiments. Our findings unambiguously demonstrate that for single-label and multi-label classification tasks, the graph-based methods fail to outperform fine-tuned language models and sometimes even perform worse than standard machine learning methods like multilayer perceptron (MLP) on a bag-of-words. This questions the enormous amount of effort put into the development of new graph-based methods in the last years and the promises they make for text classification
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MMER&#30340;&#26032;&#22411;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#26032;&#22411;&#22810;&#27169;&#24577;&#32593;&#32476;&#21644;&#22810;&#20010;&#36741;&#21161;&#20219;&#21153;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#22312;IEMOCAP&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20102;&#39046;&#20808;&#30340;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2203.16794</link><description>&lt;p&gt;
MMER: &#38754;&#21521;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MMER: Multimodal Multi-task Learning for Speech Emotion Recognition. (arXiv:2203.16794v5 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.16794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MMER&#30340;&#26032;&#22411;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#26032;&#22411;&#22810;&#27169;&#24577;&#32593;&#32476;&#21644;&#22810;&#20010;&#36741;&#21161;&#20219;&#21153;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#22312;IEMOCAP&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20102;&#39046;&#20808;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MMER&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#12290;MMER&#21033;&#29992;&#20102;&#22522;&#20110;&#26089;&#26399;&#34701;&#21512;&#21644;&#25991;&#26412;&#21644;&#22768;&#23398;&#27169;&#24577;&#20043;&#38388;&#30340;&#36328;&#27169;&#24577;&#33258;&#27880;&#24847;&#21147;&#30340;&#26032;&#22411;&#22810;&#27169;&#24577;&#32593;&#32476;&#65292;&#24182;&#35299;&#20915;&#20102;&#19977;&#20010;&#26032;&#39062;&#36741;&#21161;&#20219;&#21153;&#65292;&#29992;&#20110;&#23398;&#20064;&#26469;&#33258;&#21475;&#35821;&#35805;&#35821;&#30340;&#24773;&#24863;&#35782;&#21035;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;MMER&#20248;&#20110;&#25105;&#20204;&#30340;&#25152;&#26377;&#22522;&#32447;&#65292;&#24182;&#22312;IEMOCAP&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#21066;&#20943;&#30740;&#31350;&#21644;&#32467;&#26524;&#20998;&#26512;&#65292;&#20197;&#35777;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose MMER, a novel Multimodal Multi-task learning approach for Speech Emotion Recognition. MMER leverages a novel multimodal network based on early-fusion and cross-modal self-attention between text and acoustic modalities and solves three novel auxiliary tasks for learning emotion recognition from spoken utterances. In practice, MMER outperforms all our baselines and achieves state-of-the-art performance on the IEMOCAP benchmark. Additionally, we conduct extensive ablation studies and results analysis to prove the effectiveness of our proposed approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35775;&#38382;&#20219;&#20309;&#25968;&#25454;&#21363;&#21487;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#27867;&#21270;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#36890;&#36807;&#23545;Huggingface&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#30340;&#27169;&#22411;&#36873;&#25321;&#65292;&#24471;&#21040;&#19968;&#20010;&#31616;&#21333;&#39640;&#25928;&#19988;&#30456;&#20851;&#24615;&#24378;&#30340;&#26377;&#29992;&#24230;&#37327;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2202.02842</link><description>&lt;p&gt;
&#19981;&#38656;&#35201;&#35775;&#38382;&#20219;&#20309;&#35757;&#32451;&#25110;&#27979;&#35797;&#25968;&#25454;&#30340;&#27867;&#21270;&#24230;&#37327;&#26631;&#20934;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Evaluating natural language processing models with generalization metrics that do not need access to any training or testing data. (arXiv:2202.02842v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.02842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35775;&#38382;&#20219;&#20309;&#25968;&#25454;&#21363;&#21487;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#27867;&#21270;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#36890;&#36807;&#23545;Huggingface&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#30340;&#27169;&#22411;&#36873;&#25321;&#65292;&#24471;&#21040;&#19968;&#20010;&#31616;&#21333;&#39640;&#25928;&#19988;&#30456;&#20851;&#24615;&#24378;&#30340;&#26377;&#29992;&#24230;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36873;&#25321;&#21512;&#36866;&#30340;&#32467;&#26500;&#21442;&#25968;&#21644;&#35757;&#32451;&#36229;&#21442;&#25968;&#23545;&#20110;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#20960;&#39033;&#23454;&#35777;&#30740;&#31350;&#23545;&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#30340;&#30456;&#20851;&#20998;&#26512;&#65292;&#20197;&#23547;&#25214;&#26377;&#25928;&#30340;&#27867;&#21270;&#24230;&#37327;&#26631;&#20934;&#20197;&#25351;&#23548;&#27169;&#22411;&#36873;&#25321;&#12290;&#26377;&#25928;&#30340;&#24230;&#37327;&#26631;&#20934;&#36890;&#24120;&#39044;&#35745;&#19982;&#27979;&#35797;&#24615;&#33021;&#24378;&#30456;&#20851;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20197;&#19979;&#30446;&#26631;&#25193;&#23637;&#20102;&#20808;&#21069;&#30340;&#20998;&#26512;&#65292;&#36827;&#34892;&#20102;&#22522;&#20110;&#27867;&#21270;&#24230;&#37327;&#26631;&#20934;&#30340;&#27169;&#22411;&#36873;&#25321;&#30740;&#31350;&#65306;&#65288;i&#65289;&#20851;&#27880;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#65292;&#22240;&#20026;&#20808;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#20219;&#21153;&#65307;&#65288;ii&#65289;&#32771;&#34385;&#30452;&#25509;&#39044;&#27979;&#27979;&#35797;&#35823;&#24046;&#32780;&#38750;&#27867;&#21270;&#24046;&#36317;&#30340;&#24230;&#37327;&#26631;&#20934;&#65307;&#65288;iii&#65289;&#25506;&#32034;&#19981;&#38656;&#35201;&#35775;&#38382;&#25968;&#25454;&#21363;&#21487;&#35745;&#31639;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#20174;&#36825;&#20123;&#30446;&#26631;&#20986;&#21457;&#65292;&#25105;&#20204;&#33021;&#22815;&#25552;&#20379;&#31532;&#19968;&#20010;&#20351;&#29992;&#27867;&#21270;&#24230;&#37327;&#26631;&#20934;&#23545;&#26469;&#33258;Huggingface&#30340;&#22823;&#22411;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#30340;&#32467;&#26524;&#65292;&#24182;&#27604;&#36739;&#20102;&#35768;&#22810;&#19981;&#21516;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26377;&#29992;&#24230;&#37327;&#26631;&#20934;&#19981;&#27490;&#19982;&#27979;&#35797;&#24615;&#33021;&#39640;&#24230;&#30456;&#20851;&#65292;&#32780;&#19988;&#26356;&#21152;&#31616;&#21333;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Selecting suitable architecture parameters and training hyperparameters is essential for enhancing machine learning (ML) model performance. Several recent empirical studies conduct large-scale correlational analysis on neural networks (NNs) to search for effective \emph{generalization metrics} that can guide this type of model selection. Effective metrics are typically expected to correlate strongly with test performance. In this paper, we expand on prior analyses by examining generalization-metric-based model selection with the following objectives: (i) focusing on natural language processing (NLP) tasks, as prior work primarily concentrates on computer vision (CV) tasks; (ii) considering metrics that directly predict \emph{test error} instead of the \emph{generalization gap}; (iii) exploring metrics that do not need access to data to compute. From these objectives, we are able to provide the first model selection results on large pretrained Transformers from Huggingface using general
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20801;&#35768;&#29992;&#25143;&#22312;&#21508;&#31181;&#35821;&#35328;&#20013;&#35757;&#32451;&#33258;&#24049;&#30340;&#20840;&#29699;&#39046;&#20808;&#30340;&#36817;&#20041;&#21477;&#34920;&#31034;&#31995;&#32479;&#65292;&#24182;&#21457;&#24067;&#20102;&#35757;&#32451;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#22810;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20855;&#26377;&#27604;&#20197;&#24448;&#26356;&#24555;&#30340;&#36895;&#24230;&#21644;&#26356;&#39640;&#30340;&#24615;&#33021;&#65292;&#26159;&#27809;&#26377;GPU&#25110;&#23884;&#20837;&#24335;&#35774;&#22791;&#30340;&#29992;&#25143;&#30340;&#26377;&#21560;&#24341;&#21147;&#30340;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2104.15114</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#30340;&#36817;&#20041;&#21477;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Paraphrastic Representations at Scale. (arXiv:2104.15114v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.15114
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20801;&#35768;&#29992;&#25143;&#22312;&#21508;&#31181;&#35821;&#35328;&#20013;&#35757;&#32451;&#33258;&#24049;&#30340;&#20840;&#29699;&#39046;&#20808;&#30340;&#36817;&#20041;&#21477;&#34920;&#31034;&#31995;&#32479;&#65292;&#24182;&#21457;&#24067;&#20102;&#35757;&#32451;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#22810;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20855;&#26377;&#27604;&#20197;&#24448;&#26356;&#24555;&#30340;&#36895;&#24230;&#21644;&#26356;&#39640;&#30340;&#24615;&#33021;&#65292;&#26159;&#27809;&#26377;GPU&#25110;&#23884;&#20837;&#24335;&#35774;&#22791;&#30340;&#29992;&#25143;&#30340;&#26377;&#21560;&#24341;&#21147;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#20801;&#35768;&#29992;&#25143;&#22312;&#21508;&#31181;&#35821;&#35328;&#20013;&#35757;&#32451;&#33258;&#24049;&#30340;&#20840;&#29699;&#39046;&#20808;&#30340;&#36817;&#20041;&#21477;&#34920;&#31034;&#12290;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#38024;&#23545;&#33521;&#35821;&#12289;&#38463;&#25289;&#20271;&#35821;&#12289;&#24503;&#35821;&#12289;&#27861;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#12289;&#20420;&#35821;&#12289;&#22303;&#32819;&#20854;&#35821;&#21644;&#27721;&#35821;&#30340;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#22823;&#37327;&#25968;&#25454;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#65292;&#22312;&#21333;&#35821;&#35821;&#20041;&#30456;&#20284;&#24615;&#12289;&#36328;&#35821;&#35328;&#35821;&#20041;&#30456;&#20284;&#24615;&#21644;&#21452;&#35821;&#25366;&#25496;&#20219;&#21153;&#30340;&#19968;&#22871;&#27169;&#22411;&#20013;&#65292;&#21462;&#24471;&#20102;&#26126;&#26174;&#30340;&#25913;&#36827;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#32467;&#26524;&#36229;&#36234;&#20102;&#25152;&#26377;&#20808;&#21069;&#30340;&#26080;&#30417;&#30563;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#24037;&#20316;&#65292;&#29978;&#33267;&#20248;&#20110;&#22522;&#20110;BERT&#30340;&#27169;&#22411;&#65292;&#22914;Sentence-BERT (Reimers&#21644;Gurevych, 2019)&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#27604;&#20197;&#21069;&#30340;&#24037;&#20316;&#24555;&#19978;&#20960;&#20010;&#25968;&#37327;&#32423;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;CPU&#19978;&#20351;&#29992;&#65292;&#25512;&#29702;&#36895;&#24230;&#20960;&#20046;&#27809;&#26377;&#24046;&#36317;&#65288;&#22312;&#20351;&#29992;&#26356;&#22810;CPU&#26680;&#24515;&#26102;&#65292;&#27604;GPU&#36895;&#24230;&#26356;&#24555;&#65289;&#65292;&#20351;&#24471;&#36825;&#20123;&#27169;&#22411;&#25104;&#20026;&#27809;&#26377;GPU&#25110;&#23884;&#20837;&#24335;&#35774;&#22791;&#20351;&#29992;&#30340;&#29992;&#25143;&#30340;&#26377;&#21560;&#24341;&#21147;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a system that allows users to train their own state-of-the-art paraphrastic sentence representations in a variety of languages. We also release trained models for English, Arabic, German, French, Spanish, Russian, Turkish, and Chinese. We train these models on large amounts of data, achieving significantly improved performance from the original papers proposing the methods on a suite of monolingual semantic similarity, cross-lingual semantic similarity, and bitext mining tasks. Moreover, the resulting models surpass all prior work on unsupervised semantic textual similarity, significantly outperforming even BERT-based models like Sentence-BERT (Reimers and Gurevych, 2019). Additionally, our models are orders of magnitude faster than prior work and can be used on CPU with little difference in inference speed (even improved speed over GPU when using more CPU cores), making these models an attractive choice for users without access to GPUs or for use on embedded devices. Finall
&lt;/p&gt;</description></item></channel></rss>