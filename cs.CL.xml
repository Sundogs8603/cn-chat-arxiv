<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36866;&#24212;&#25552;&#31034;&#21644;&#38646;&#21021;&#22987;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#36731;&#37327;&#32423;&#35821;&#35328;&#27169;&#22411;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#39640;&#25928;&#24494;&#35843;LLaMA&#20026;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#65292;&#20855;&#26377;&#27604;Alpaca&#26356;&#30701;&#30340;&#24494;&#35843;&#26102;&#38388;&#24182;&#20855;&#26377;&#36817;&#20284;&#30340;&#21709;&#24212;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.16199</link><description>&lt;p&gt;
LLaMA-Adapter: &#38646;&#21021;&#22987;&#21270;&#27880;&#24847;&#21147;&#19979;&#30340;&#35821;&#35328;&#27169;&#22411;&#31934;&#32454;&#35843;&#25972;&#30340;&#39640;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention. (arXiv:2303.16199v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16199
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36866;&#24212;&#25552;&#31034;&#21644;&#38646;&#21021;&#22987;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#36731;&#37327;&#32423;&#35821;&#35328;&#27169;&#22411;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#39640;&#25928;&#24494;&#35843;LLaMA&#20026;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#65292;&#20855;&#26377;&#27604;Alpaca&#26356;&#30701;&#30340;&#24494;&#35843;&#26102;&#38388;&#24182;&#20855;&#26377;&#36817;&#20284;&#30340;&#21709;&#24212;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;LLaMA-Adapter&#36825;&#19968;&#36731;&#37327;&#32423;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;LLaMA&#39640;&#25928;&#22320;&#24494;&#35843;&#20026;&#19968;&#20010;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#12290;&#21033;&#29992;52K&#20010;&#33258;&#25105;&#25351;&#23548;&#31034;&#33539;&#65292;LLaMA-Adapter&#20165;&#22312;&#20923;&#32467;&#30340;LLaMA 7B&#27169;&#22411;&#19978;&#24341;&#20837;&#20102;1.2M&#20010;&#21487;&#23398;&#20064;&#21442;&#25968;&#65292;&#24182;&#19988;&#22312;8&#20010;A100 GPU&#19978;&#20165;&#32791;&#26102;&#19981;&#21040;&#19968;&#20010;&#23567;&#26102;&#36827;&#34892;&#24494;&#35843;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#19968;&#32452;&#21487;&#23398;&#20064;&#30340;&#36866;&#24212;&#25552;&#31034;&#65292;&#24182;&#22312;&#36739;&#39640;&#30340;&#21464;&#21387;&#22120;&#23618;&#20013;&#23558;&#23427;&#20204;&#39044;&#32622;&#20110;&#36755;&#20837;&#25991;&#26412;&#20196;&#29260;&#20043;&#21069;&#12290;&#28982;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#21021;&#22987;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#38646;&#38376;&#25511;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#23558;&#26032;&#30340;&#25351;&#20196;&#25552;&#31034;&#27880;&#20837;LLaMA&#65292;&#24182;&#26377;&#25928;&#22320;&#20445;&#30041;&#20102;&#20854;&#39044;&#20808;&#35757;&#32451;&#30340;&#30693;&#35782;&#12290;&#36890;&#36807;&#39640;&#25928;&#35757;&#32451;&#65292;LLaMA-Adapter&#33021;&#22815;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#21709;&#24212;&#65292;&#19982;&#23436;&#20840;&#24494;&#35843;&#30340;7B&#21442;&#25968;&#30340;Alpaca&#30456;&#20284;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#21487;&#20197;&#31616;&#21333;&#22320;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#36755;&#20837;&#65292;&#20363;&#22914;&#22270;&#20687;&#65292;&#29992;&#20110;&#22270;&#20687;&#30456;&#20851;&#30340;LLaMA&#65292;&#22312;ScienceQA&#19978;&#23454;&#29616;&#20102;&#26356;&#24378;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;https://github.com/ZrrSkywalker/LLaMA-Adapt&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present LLaMA-Adapter, a lightweight adaption method to efficiently fine-tune LLaMA into an instruction-following model. Using 52K self-instruct demonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon the frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8 A100 GPUs. Specifically, we adopt a set of learnable adaption prompts, and prepend them to the input text tokens at higher transformer layers. Then, a zero-init attention mechanism with zero gating is proposed, which adaptively injects the new instructional cues into LLaMA, while effectively preserves its pre-trained knowledge. With efficient training, LLaMA-Adapter generates high-quality responses, comparable to Alpaca with fully fine-tuned 7B parameters. Furthermore, our approach can be simply extended to multi-modal input, e.g., images, for image-conditioned LLaMA, which achieves superior reasoning capacity on ScienceQA. We release our code at https://github.com/ZrrSkywalker/LLaMA-Adapt
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#23545;&#25239;&#26412;&#36136;&#35770;&#20449;&#24565;&#30340;&#20219;&#21153;&#65292;&#24182;&#26500;&#24314;&#20102;&#20116;&#31181;&#31867;&#22411;&#30340;&#21453;&#39539;&#35821;&#21477;&#36827;&#34892;&#20102;&#23454;&#39564;&#30740;&#31350;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#21051;&#26495;&#21360;&#35937;&#25193;&#22823;&#21040;&#20854;&#20182;&#32676;&#20307;&#26159;&#26368;&#26377;&#25928;&#30340;&#21453;&#20987;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2303.16173</link><description>&lt;p&gt;
&#36890;&#36807;&#31038;&#20250;&#20559;&#35265;&#25512;&#29702;&#26469;&#23545;&#25239;&#26412;&#36136;&#35770;
&lt;/p&gt;
&lt;p&gt;
Towards Countering Essentialism through Social Bias Reasoning. (arXiv:2303.16173v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#23545;&#25239;&#26412;&#36136;&#35770;&#20449;&#24565;&#30340;&#20219;&#21153;&#65292;&#24182;&#26500;&#24314;&#20102;&#20116;&#31181;&#31867;&#22411;&#30340;&#21453;&#39539;&#35821;&#21477;&#36827;&#34892;&#20102;&#23454;&#39564;&#30740;&#31350;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#21051;&#26495;&#21360;&#35937;&#25193;&#22823;&#21040;&#20854;&#20182;&#32676;&#20307;&#26159;&#26368;&#26377;&#25928;&#30340;&#21453;&#20987;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#36136;&#35770;&#20449;&#20208;&#22312;&#31038;&#20250;&#21051;&#26495;&#21360;&#35937;&#20013;&#36215;&#30528;&#26680;&#24515;&#20316;&#29992;&#65292;&#22914;&#26524;&#19981;&#34987;&#25361;&#25112;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#20260;&#23475;&#12290;&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#23545;&#25239;&#26412;&#36136;&#35770;&#20449;&#24565;&#30340;&#20219;&#21153;&#65288;&#20363;&#22914;&#8220;&#33258;&#30001;&#27966;&#24858;&#34850;&#8221;&#65289;&#65292;&#24182;&#36827;&#34892;&#20102;&#25506;&#32034;&#24615;&#30740;&#31350;&#12290;&#32467;&#21512;&#24515;&#29702;&#23398;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#20808;&#21069;&#30740;&#31350;&#65292;&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#20116;&#31181;&#31867;&#22411;&#30340;&#21453;&#39539;&#35821;&#21477;&#65292;&#24182;&#36827;&#34892;&#20102;&#20154;&#31867;&#23454;&#39564;&#20197;&#30740;&#31350;&#36825;&#20123;&#19981;&#21516;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#22312;&#36873;&#25321;&#21453;&#39539;&#35821;&#21477;&#26102;&#20197;&#22810;&#22823;&#30340;&#26126;&#30830;&#31243;&#24230;&#20256;&#36798;&#26412;&#36136;&#35770;&#20449;&#24565;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23558;&#21051;&#26495;&#21360;&#35937;&#25193;&#22823;&#21040;&#20854;&#20182;&#32676;&#20307;&#65288;&#20363;&#22914;&#65292;&#8220;&#20445;&#23432;&#27966;&#20063;&#21487;&#33021;&#24858;&#34850;&#8221;&#65289;&#30340;&#35821;&#21477;&#26159;&#26368;&#21463;&#27426;&#36814;&#30340;&#21453;&#20987;&#31574;&#30053;&#12290;&#25105;&#20204;&#26368;&#21518;&#35752;&#35770;&#20102;&#26410;&#26469;&#24037;&#20316;&#20013;&#30340;&#25361;&#25112;&#21644;&#24320;&#25918;&#24615;&#38382;&#39064;&#65288;&#20363;&#22914;&#65292;&#25552;&#39640;&#20107;&#23454;&#24615;&#65292;&#30740;&#31350;&#31038;&#21306;&#29305;&#23450;&#21464;&#21270;&#65289;&#65292;&#24182;&#24378;&#35843;&#20102;&#22312;&#35813;&#39046;&#22495;&#24320;&#23637;&#24037;&#20316;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Essentialist beliefs (i.e., believing that members of the same group are fundamentally alike) play a central role in social stereotypes and can lead to harm when left unchallenged. In our work, we conduct exploratory studies into the task of countering essentialist beliefs (e.g., ``liberals are stupid''). Drawing on prior work from psychology and NLP, we construct five types of counterstatements and conduct human studies on the effectiveness of these different strategies. Our studies also investigate the role in choosing a counterstatement of the level of explicitness with which an essentialist belief is conveyed. We find that statements that broaden the scope of a stereotype (e.g., to other groups, as in ``conservatives can also be stupid'') are the most popular countering strategy. We conclude with a discussion of challenges and open questions for future work in this area (e.g., improving factuality, studying community-specific variation) and we emphasize the importance of work at th
&lt;/p&gt;</description></item><item><title>&#22312;NLP&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#19981;&#33021;&#20165;&#20973;&#24863;&#30693;&#36136;&#37327;&#20551;&#23450;&#20195;&#30721;&#27491;&#30830;&#24615;&#65292;&#24212;&#35813;&#25512;&#21160;&#37319;&#29992;&#32534;&#30721;&#26368;&#20339;&#23454;&#36341;&#20197;&#25552;&#39640;&#23454;&#39564;&#32467;&#26524;&#30340;&#27491;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.16166</link><description>&lt;p&gt;
&#27809;&#26377;&#27491;&#30830;&#24615;&#30340;&#21487;&#37325;&#22797;&#24615;&#24182;&#19981;&#37325;&#35201;&#65306;&#22312;NLP&#39046;&#22495;&#20013;&#27979;&#35797;&#20195;&#30721;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reproducibility is Nothing without Correctness: The Importance of Testing Code in NLP. (arXiv:2303.16166v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16166
&lt;/p&gt;
&lt;p&gt;
&#22312;NLP&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#19981;&#33021;&#20165;&#20973;&#24863;&#30693;&#36136;&#37327;&#20551;&#23450;&#20195;&#30721;&#27491;&#30830;&#24615;&#65292;&#24212;&#35813;&#25512;&#21160;&#37319;&#29992;&#32534;&#30721;&#26368;&#20339;&#23454;&#36341;&#20197;&#25552;&#39640;&#23454;&#39564;&#32467;&#26524;&#30340;&#27491;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20854;&#22312;&#30740;&#31350;&#23454;&#39564;&#20013;&#21457;&#25381;&#20102;&#20851;&#38190;&#20316;&#29992;&#65292;&#20294;&#20195;&#30721;&#27491;&#30830;&#24615;&#24448;&#24448;&#20165;&#22522;&#20110;&#32467;&#26524;&#30340;&#24863;&#30693;&#36136;&#37327;&#32780;&#34987;&#20551;&#23450;&#12290;&#36825;&#24102;&#26469;&#20102;&#38169;&#35823;&#32467;&#26524;&#21644;&#28508;&#22312;&#35823;&#23548;&#24615;&#21457;&#29616;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35748;&#20026;&#24403;&#21069;&#20851;&#27880;&#32467;&#26524;&#37325;&#29616;&#24212;&#35813;&#19982;&#24378;&#35843;&#32534;&#30721;&#26368;&#20339;&#23454;&#36341;&#30456;&#36741;&#30456;&#25104;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#26469;&#25903;&#25345;&#25105;&#20204;&#21521;NLP&#31038;&#21306;&#21457;&#20986;&#30340;&#21495;&#21484;&#65292;&#22312;&#36825;&#20010;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35782;&#21035;&#20986;&#24182;&#32416;&#27491;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;&#26368;&#20808;&#36827;Conformer&#26550;&#26500;&#30340;&#24320;&#28304;&#23454;&#29616;&#20013;&#30340;&#19977;&#20010;Bug&#12290;&#36890;&#36807;&#22312;&#21508;&#31181;&#35821;&#35328;&#29615;&#22659;&#19979;&#36827;&#34892;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#21644;&#32763;&#35793;&#30340;&#27604;&#36739;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;Bug&#30340;&#23384;&#22312;&#24182;&#19981;&#20250;&#22952;&#30861;&#33719;&#24471;&#33391;&#22909;&#30340;&#21644;&#21487;&#37325;&#22797;&#30340;&#32467;&#26524;&#65292;&#21453;&#32780;&#21487;&#33021;&#23548;&#33268;&#19981;&#27491;&#30830;&#30340;&#32467;&#35770;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#21487;&#33021;&#25552;&#20379;&#38169;&#35823;&#30340;&#25351;&#23548;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#36825;&#39033;&#30740;&#31350;&#21628;&#21505;&#37319;&#29992;&#26088;&#22312;&#20419;&#36827;NLP&#30740;&#31350;&#20013;&#27491;&#30830;&#24615;&#30340;&#32534;&#30721;&#26368;&#20339;&#23454;&#36341;&#65292;&#24182;&#25552;&#39640;&#23454;&#39564;&#32467;&#26524;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite its pivotal role in research experiments, code correctness is often presumed only on the basis of the perceived quality of the results. This comes with the risk of erroneous outcomes and potentially misleading findings. To address this issue, we posit that the current focus on result reproducibility should go hand in hand with the emphasis on coding best practices. We bolster our call to the NLP community by presenting a case study, in which we identify (and correct) three bugs in widely used open-source implementations of the state-of-the-art Conformer architecture. Through comparative experiments on automatic speech recognition and translation in various language settings, we demonstrate that the existence of bugs does not prevent the achievement of good and reproducible results and can lead to incorrect conclusions that potentially misguide future research. In response to this, this study is a call to action toward the adoption of coding best practices aimed at fostering cor
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;COCOCON&#65292;&#24182;&#25552;&#20986;&#24230;&#37327;&#26041;&#27861;&#26469;&#34913;&#37327;&#27169;&#22411;&#19968;&#33268;&#24615;&#65292;&#30740;&#31350;&#21457;&#29616;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#31995;&#32479;&#22312;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#34920;&#29616;&#20986;&#39640;&#24230;&#19981;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.16133</link><description>&lt;p&gt;
&#25581;&#31034;&#21644;&#35299;&#20915;&#32479;&#19968;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36328;&#20219;&#21153;&#19981;&#19968;&#33268;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Exposing and Addressing Cross-Task Inconsistency in Unified Vision-Language Models. (arXiv:2303.16133v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16133
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;COCOCON&#65292;&#24182;&#25552;&#20986;&#24230;&#37327;&#26041;&#27861;&#26469;&#34913;&#37327;&#27169;&#22411;&#19968;&#33268;&#24615;&#65292;&#30740;&#31350;&#21457;&#29616;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#31995;&#32479;&#22312;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#34920;&#29616;&#20986;&#39640;&#24230;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36890;&#29992;&#30340;&#35270;&#35273;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#21464;&#24471;&#36234;&#26469;&#36234;&#26377;&#25928;&#65292;&#20445;&#35777;&#23427;&#20204;&#22312;&#21508;&#33258;&#25903;&#25345;&#30340;&#20219;&#21153;&#20013;&#30340;&#19968;&#33268;&#24615;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;&#20154;&#20204;&#35748;&#20026;&#19981;&#19968;&#33268;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#26159;&#19981;&#21487;&#38752;&#30340;&#65292;&#36825;&#23545;&#20110;&#20381;&#36182;&#23427;&#20204;&#36755;&#20986;&#30340;&#22823;&#22411;&#31995;&#32479;&#26469;&#35828;&#26159;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#12290;&#30001;&#20110;&#24456;&#38590;&#30830;&#23450;&#39044;&#27979;&#32467;&#26524;&#26159;&#21542;&#19968;&#33268;&#65292;&#22240;&#27492;&#65292;&#35780;&#20272;&#21487;&#33021;&#21253;&#25324;&#19981;&#21516;&#27169;&#24577;&#36755;&#20986;&#30340;&#38750;&#24120;&#24322;&#26500;&#20219;&#21153;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20934;&#25968;&#25454;&#38598;COCOCON&#65292;&#20854;&#20013;&#25105;&#20204;&#20351;&#29992;&#23545;&#22810;&#20010;&#20219;&#21153;&#30340;&#27979;&#35797;&#23454;&#20363;&#36827;&#34892;&#23567;&#22411;&#20294;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#20462;&#25913;&#26469;&#21019;&#24314;&#23545;&#27604;&#38598;&#65292;&#20197;&#26356;&#25913;&#37329;&#26631;&#31614;&#65292;&#24182;&#27010;&#36848;&#20102;&#29992;&#20110;&#36890;&#36807;&#23545;&#27604;&#25509;&#36817;&#21407;&#22987;&#21644;&#20462;&#25913;&#21518;&#30340;&#23454;&#20363;&#26469;&#34913;&#37327;&#27169;&#22411;&#19968;&#33268;&#24615;&#30340;&#25351;&#26631;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#26368;&#20808;&#36827;&#30340;&#31995;&#32479;&#22312;&#20219;&#21153;&#20043;&#38388;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As general purpose vision models get increasingly effective at a wide set of tasks, it is imperative that they be consistent across the tasks they support. Inconsistent AI models are considered brittle and untrustworthy by human users and are more challenging to incorporate into larger systems that take dependencies on their outputs. Measuring consistency between very heterogeneous tasks that might include outputs in different modalities is challenging since it is difficult to determine if the predictions are consistent with one another. As a solution, we introduce a benchmark dataset, COCOCON, where we use contrast sets created by modifying test instances for multiple tasks in small but semantically meaningful ways to change the gold label, and outline metrics for measuring if a model is consistent by ranking the original and perturbed instances across tasks. We find that state-of-the-art systems suffer from a surprisingly high degree of inconsistent behavior across tasks, especially 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23545;&#24120;&#35268;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;M2M&#31995;&#21015;&#21644;ChacGPT&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#22823;&#22411;&#22810;&#35821;&#35328;&#32763;&#35793;&#27169;&#22411;&#20013;&#24187;&#35273;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#21253;&#25324;&#36755;&#20837;&#22122;&#22768;&#65292;&#20302;&#36164;&#28304;&#35821;&#35328;&#21644;&#27169;&#22411;&#20559;&#24046;&#65292;&#24378;&#35843;&#38656;&#35201;&#26356;&#22909;&#30340;&#35780;&#20272;&#21644;&#32531;&#35299;&#31574;&#30053;&#20197;&#30830;&#20445;&#23433;&#20840;&#21644;&#21487;&#20449;&#37096;&#32626;&#12290;</title><link>http://arxiv.org/abs/2303.16104</link><description>&lt;p&gt;
&#22823;&#22411;&#22810;&#35821;&#35328;&#32763;&#35793;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Hallucinations in Large Multilingual Translation Models. (arXiv:2303.16104v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16104
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23545;&#24120;&#35268;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;M2M&#31995;&#21015;&#21644;ChacGPT&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#22823;&#22411;&#22810;&#35821;&#35328;&#32763;&#35793;&#27169;&#22411;&#20013;&#24187;&#35273;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#21253;&#25324;&#36755;&#20837;&#22122;&#22768;&#65292;&#20302;&#36164;&#28304;&#35821;&#35328;&#21644;&#27169;&#22411;&#20559;&#24046;&#65292;&#24378;&#35843;&#38656;&#35201;&#26356;&#22909;&#30340;&#35780;&#20272;&#21644;&#32531;&#35299;&#31574;&#30053;&#20197;&#30830;&#20445;&#23433;&#20840;&#21644;&#21487;&#20449;&#37096;&#32626;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#23637;&#31034;&#20102;&#30452;&#25509;&#22312;&#20247;&#22810;&#35821;&#35328;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#30340;&#21331;&#36234;&#33021;&#21147;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#36234;&#26469;&#36234;&#36866;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#20250;&#29983;&#25104;&#24187;&#35273;&#32763;&#35793;&#65292;&#36825;&#21487;&#33021;&#20250;&#20005;&#37325;&#30772;&#22351;&#29992;&#25143;&#20449;&#20219;&#24182;&#24341;&#21457;&#23433;&#20840;&#38382;&#39064;&#12290;&#26412;&#25991;&#23545;&#24120;&#35268;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;M2M&#31995;&#21015;&#21644;ChacGPT&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#20998;&#26512;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#25552;&#31034;&#36827;&#34892;&#32763;&#35793;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#26465;&#20214;&#65292;&#21253;&#25324;&#21508;&#31181;&#36164;&#28304;&#27700;&#24179;&#21644;100&#22810;&#20010;&#32763;&#35793;&#26041;&#21521;&#65292;&#36229;&#36234;&#20102;&#31616;&#21333;&#30340;&#35789;&#32423;&#24187;&#35273;&#65292;&#25506;&#32034;&#20102;&#26356;&#22797;&#26434;&#30340;&#29616;&#35937;&#65292;&#22914;&#32597;&#35265;&#35789;&#26367;&#25442;&#65292;&#20107;&#23454;&#38169;&#35823;&#21644;&#19981;&#21512;&#36923;&#36753;&#30340;&#21477;&#23376;&#29983;&#25104;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#24187;&#35273;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#21253;&#25324;&#36755;&#20837;&#22122;&#22768;&#65292;&#20302;&#36164;&#28304;&#35821;&#35328;&#21644;&#27169;&#22411;&#20559;&#24046;&#65292;&#24182;&#24378;&#35843;&#38656;&#35201;&#26356;&#22909;&#30340;&#35780;&#20272;&#21644;&#32531;&#35299;&#31574;&#30053;&#65292;&#20197;&#30830;&#20445;&#22823;&#22411;&#22810;&#35821;&#35328;&#32763;&#35793;&#27169;&#22411;&#30340;&#23433;&#20840;&#21644;&#21487;&#20449;&#37096;&#32626;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale multilingual machine translation systems have demonstrated remarkable ability to translate directly between numerous languages, making them increasingly appealing for real-world applications. However, when deployed in the wild, these models may generate hallucinated translations which have the potential to severely undermine user trust and raise safety concerns. Existing research on hallucinations has primarily focused on small bilingual models trained on high-resource languages, leaving a gap in our understanding of hallucinations in massively multilingual models across diverse translation scenarios. In this work, we fill this gap by conducting a comprehensive analysis on both the M2M family of conventional neural machine translation models and ChatGPT, a general-purpose large language model~(LLM) that can be prompted for translation. Our investigation covers a broad spectrum of conditions, spanning over 100 translation directions across various resource levels and going b
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#39318;&#20010;&#20844;&#24320;&#29256;&#26412;&#30340;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#36890;&#29992;&#29616;&#20195;&#35821;&#26009;&#24211;Carolina&#65292;&#23427;&#20351;&#29992;&#20102;&#22686;&#24378;&#30340;&#26469;&#28304;&#12289;&#31867;&#22411;&#12289;&#29256;&#26412;&#21644;&#25991;&#26412;&#23436;&#25972;&#24615;&#30340;&#32593;&#32476;&#35821;&#26009;&#24211;&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#20316;&#20026;&#35821;&#35328;&#23398;&#30740;&#31350;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#30740;&#31350;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2303.16098</link><description>&lt;p&gt;
Carolina&#65306;&#19968;&#31181;&#20855;&#26377;&#26469;&#28304;&#12289;&#31867;&#22411;&#21644;&#29256;&#26412;&#20449;&#24687;&#30340;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#36890;&#29992;&#29616;&#20195;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
Carolina: a General Corpus of Contemporary Brazilian Portuguese with Provenance, Typology and Versioning Information. (arXiv:2303.16098v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16098
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#39318;&#20010;&#20844;&#24320;&#29256;&#26412;&#30340;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#36890;&#29992;&#29616;&#20195;&#35821;&#26009;&#24211;Carolina&#65292;&#23427;&#20351;&#29992;&#20102;&#22686;&#24378;&#30340;&#26469;&#28304;&#12289;&#31867;&#22411;&#12289;&#29256;&#26412;&#21644;&#25991;&#26412;&#23436;&#25972;&#24615;&#30340;&#32593;&#32476;&#35821;&#26009;&#24211;&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#20316;&#20026;&#35821;&#35328;&#23398;&#30740;&#31350;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#30740;&#31350;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#39318;&#20010;&#20844;&#24320;&#29256;&#26412;&#30340;Carolina&#35821;&#26009;&#24211;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;Carolina&#26159;&#19968;&#20010;&#20351;&#29992;&#22686;&#24378;&#30340;&#26469;&#28304;&#12289;&#31867;&#22411;&#12289;&#29256;&#26412;&#21644;&#25991;&#26412;&#23436;&#25972;&#24615;&#30340;&#32593;&#32476;&#35821;&#26009;&#24211;&#26041;&#27861;&#27491;&#22312;&#26500;&#24314;&#20013;&#30340;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#25991;&#26412;&#30340;&#22823;&#22411;&#24320;&#25918;&#35821;&#26009;&#24211;&#12290;&#35813;&#35821;&#26009;&#24211;&#26088;&#22312;&#20316;&#20026;&#35821;&#35328;&#23398;&#30740;&#31350;&#30340;&#21487;&#38752;&#26469;&#28304;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#30740;&#31350;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#36164;&#28304;&#65292;&#26377;&#21161;&#20110;&#23558;&#33889;&#33796;&#29273;&#35821;&#20174;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#31227;&#38500;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#26500;&#24314;&#35821;&#26009;&#24211;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#20854;&#20182;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#20197;&#21450;&#35821;&#26009;&#24211;&#30340;&#24403;&#21069;&#29366;&#24577;&#65306;Carolina&#30340;&#31532;&#19968;&#20010;&#20844;&#24320;&#29256;&#26377;653,322,577&#20010;&#26631;&#35760;&#65292;&#20998;&#24067;&#22312;7&#20010;&#24191;&#27867;&#30340;&#31867;&#22411;&#19978;&#12290;&#27599;&#20010;&#25991;&#26412;&#30340;&#26631;&#22836;&#37117;&#29992;TEI&#27880;&#37322;&#26631;&#20934;&#36827;&#34892;&#20102;&#22810;&#20010;&#19981;&#21516;&#30340;&#20803;&#25968;&#25454;&#31867;&#21035;&#30340;&#27880;&#37322;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#27491;&#22312;&#36827;&#34892;&#30340;&#27966;&#29983;&#20316;&#21697;&#65292;&#24182;&#36992;&#35831;NLP&#30740;&#31350;&#20154;&#21592;&#36827;&#34892;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the first publicly available version of the Carolina Corpus and discusses its future directions. Carolina is a large open corpus of Brazilian Portuguese texts under construction using web-as-corpus methodology enhanced with provenance, typology, versioning, and text integrality. The corpus aims at being used both as a reliable source for research in Linguistics and as an important resource for Computer Science research on language models, contributing towards removing Portuguese from the set of low-resource languages. Here we present the construction of the corpus methodology, comparing it with other existing methodologies, as well as the corpus current state: Carolina's first public version has $653,322,577$ tokens, distributed over $7$ broad types. Each text is annotated with several different metadata categories in its header, which we developed using TEI annotation standards. We also present ongoing derivative works and invite NLP researchers to contribute with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25511;&#21046;&#29983;&#25104;&#21512;&#25104;&#25991;&#26412;&#30340;&#26041;&#26696;&#65292;&#20197;&#35299;&#20915;&#26377;&#30417;&#30563;&#25991;&#26412;&#27169;&#22411;&#23384;&#22312;&#30340;&#25163;&#21160;&#26631;&#35760;&#25991;&#26723;&#36153;&#29992;&#39640;&#12289;&#26816;&#32034;&#23569;&#37327;&#30456;&#20851;&#25991;&#26723;&#20197;&#36827;&#34892;&#26631;&#27880;&#30340;&#22256;&#38590;&#65292;&#20197;&#21450;&#29256;&#26435;&#21644;&#38544;&#31169;&#38382;&#39064;&#31561;&#38382;&#39064;&#12290;&#20316;&#32773;&#25552;&#20379;&#20102;&#29983;&#25104;&#21512;&#25104;&#25991;&#26412;&#30340;&#27010;&#24565;&#27010;&#36848;&#21644;&#25351;&#23548;&#65292;&#35752;&#35770;&#20102;&#20262;&#29702;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#19977;&#31181;&#21512;&#25104;&#25991;&#26412;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.16028</link><description>&lt;p&gt;
&#21046;&#36896;&#21512;&#25104;&#25991;&#26412;&#20197;&#36827;&#34892;&#26377;&#30417;&#30563;&#25991;&#26412;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Synthetically generated text for supervised text analysis. (arXiv:2303.16028v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25511;&#21046;&#29983;&#25104;&#21512;&#25104;&#25991;&#26412;&#30340;&#26041;&#26696;&#65292;&#20197;&#35299;&#20915;&#26377;&#30417;&#30563;&#25991;&#26412;&#27169;&#22411;&#23384;&#22312;&#30340;&#25163;&#21160;&#26631;&#35760;&#25991;&#26723;&#36153;&#29992;&#39640;&#12289;&#26816;&#32034;&#23569;&#37327;&#30456;&#20851;&#25991;&#26723;&#20197;&#36827;&#34892;&#26631;&#27880;&#30340;&#22256;&#38590;&#65292;&#20197;&#21450;&#29256;&#26435;&#21644;&#38544;&#31169;&#38382;&#39064;&#31561;&#38382;&#39064;&#12290;&#20316;&#32773;&#25552;&#20379;&#20102;&#29983;&#25104;&#21512;&#25104;&#25991;&#26412;&#30340;&#27010;&#24565;&#27010;&#36848;&#21644;&#25351;&#23548;&#65292;&#35752;&#35770;&#20102;&#20262;&#29702;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#19977;&#31181;&#21512;&#25104;&#25991;&#26412;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#30417;&#30563;&#25991;&#26412;&#27169;&#22411;&#23545;&#25919;&#27835;&#23398;&#23478;&#26469;&#35828;&#26159;&#19968;&#31181;&#23453;&#36149;&#30340;&#24037;&#20855;&#65292;&#20294;&#23384;&#22312;&#33509;&#24178;&#20351;&#29992;&#38556;&#30861;&#65292;&#21253;&#25324;&#25163;&#21160;&#26631;&#35760;&#25991;&#26723;&#30340;&#36153;&#29992;&#65292;&#26816;&#32034;&#23569;&#37327;&#30456;&#20851;&#25991;&#26723;&#20197;&#36827;&#34892;&#26631;&#27880;&#30340;&#22256;&#38590;&#65292;&#20197;&#21450;&#20849;&#20139;&#24102;&#26377;&#27880;&#37322;&#25991;&#26723;&#25152;&#28041;&#21450;&#30340;&#29256;&#26435;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#37096;&#20998;&#35299;&#20915;&#36825;&#19977;&#20010;&#38382;&#39064;&#30340;&#26041;&#26696;&#65292;&#21363;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25511;&#21046;&#29983;&#25104;&#21512;&#25104;&#25991;&#26412;&#12290;&#20316;&#32773;&#25552;&#20379;&#20102;&#25991;&#26412;&#29983;&#25104;&#30340;&#27010;&#24565;&#27010;&#36848;&#65292;&#24182;&#25351;&#23548;&#30740;&#31350;&#20154;&#21592;&#20309;&#26102;&#24212;&#35813;&#36873;&#25321;&#19981;&#21516;&#30340;&#21512;&#25104;&#25991;&#26412;&#29983;&#25104;&#25216;&#26415;&#65292;&#35752;&#35770;&#20102;&#20262;&#29702;&#38382;&#39064;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#25913;&#36827;&#21512;&#25104;&#25991;&#26412;&#36136;&#37327;&#30340;&#31616;&#21333;&#25216;&#26415;&#12290;&#20316;&#32773;&#23637;&#31034;&#20102;&#21512;&#25104;&#25991;&#26412;&#30340;&#19977;&#31181;&#24212;&#29992;&#65306;&#29983;&#25104;&#25551;&#36848;&#20044;&#20811;&#20848;&#25112;&#26007;&#30340;&#21512;&#25104;&#25512;&#29305;&#65292;&#29983;&#25104;&#25551;&#36848;&#29305;&#23450;&#25919;&#27835;&#20107;&#20214;&#30340;&#21512;&#25104;&#26032;&#38395;&#25991;&#31456;&#20197;&#29992;&#20110;&#35757;&#32451;&#20107;&#20214;&#26816;&#27979;&#31995;&#32479;&#65292;&#20197;&#21450;&#19968;&#20010;&#22810;&#35821;&#35328;&#30340;&#27665;&#31929;&#20027;&#20041;&#23459;&#35328;&#35821;&#26009;&#24211;&#29992;&#20110;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Supervised text models are a valuable tool for political scientists but present several obstacles to their use, including the expense of hand-labeling documents, the difficulty of retrieving rare relevant documents for annotation, and copyright and privacy concerns involved in sharing annotated documents. This article proposes a partial solution to these three issues, in the form of controlled generation of synthetic text with large language models. I provide a conceptual overview of text generation, guidance on when researchers should prefer different techniques for generating synthetic text, a discussion of ethics, and a simple technique for improving the quality of synthetic text. I demonstrate the usefulness of synthetic text with three applications: generating synthetic tweets describing the fighting in Ukraine, synthetic news articles describing specified political events for training an event detection system, and a multilingual corpus of populist manifesto statements for traini
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;YouTube&#35780;&#35770;&#20013;&#30340;&#25705;&#27931;&#21733;&#26041;&#35328;&#36827;&#34892;&#24773;&#24863;&#20998;&#31867;&#65292;&#37319;&#29992;&#22810;&#31181;&#25991;&#26412;&#39044;&#22788;&#29702;&#21644;&#25968;&#25454;&#34920;&#31034;&#25216;&#26415;&#23545;&#25991;&#26412;&#36827;&#34892;&#20998;&#26512;&#65292;&#30740;&#31350;&#35813;&#26041;&#35328;&#30340;&#24847;&#35265;&#21644;&#24773;&#24863;&#34920;&#36798;&#12290;</title><link>http://arxiv.org/abs/2303.15987</link><description>&lt;p&gt;
&#20851;&#20110;&#25705;&#27931;&#21733;&#26041;&#35328;&#25991;&#26412;&#24773;&#24863;&#20998;&#31867;&#30340;&#23454;&#39564;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Experimental Study on Sentiment Classification of Moroccan dialect texts in the web. (arXiv:2303.15987v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;YouTube&#35780;&#35770;&#20013;&#30340;&#25705;&#27931;&#21733;&#26041;&#35328;&#36827;&#34892;&#24773;&#24863;&#20998;&#31867;&#65292;&#37319;&#29992;&#22810;&#31181;&#25991;&#26412;&#39044;&#22788;&#29702;&#21644;&#25968;&#25454;&#34920;&#31034;&#25216;&#26415;&#23545;&#25991;&#26412;&#36827;&#34892;&#20998;&#26512;&#65292;&#30740;&#31350;&#35813;&#26041;&#35328;&#30340;&#24847;&#35265;&#21644;&#24773;&#24863;&#34920;&#36798;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31038;&#20132;&#23186;&#20307;&#32593;&#31449;&#30340;&#36805;&#36895;&#22686;&#38271;&#65292;&#33258;&#21160;&#33719;&#21462;&#29992;&#25143;&#21453;&#39304;&#25104;&#20026;&#35780;&#20272;&#20854;&#22312;&#32447;&#36235;&#21183;&#21644;&#34892;&#20026;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#23613;&#31649;&#20449;&#24687;&#22823;&#37327;&#21487;&#29992;&#65292;&#38463;&#25289;&#20271;&#20351;&#29992;&#32773;&#25968;&#37327;&#22686;&#21152;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#22788;&#29702;&#38463;&#25289;&#20271;&#26041;&#35328;&#12290;&#26412;&#25991;&#26088;&#22312;&#20934;&#30830;&#30740;&#31350;&#22312;YouTube&#35780;&#35770;&#20013;&#34920;&#36798;&#30340;&#30495;&#23454;&#25705;&#27931;&#21733;&#26041;&#35328;&#25991;&#26412;&#30340;&#35266;&#28857;&#21644;&#24773;&#24863;&#65292;&#20351;&#29992;&#19968;&#20123;&#20247;&#25152;&#21608;&#30693;&#19988;&#24120;&#29992;&#30340;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#36827;&#34892;&#12290;&#36890;&#36807;&#37319;&#29992;&#35768;&#22810;&#25991;&#26412;&#39044;&#22788;&#29702;&#21644;&#25968;&#25454;&#34920;&#31034;&#25216;&#26415;&#65292;&#25105;&#20204;&#26088;&#22312;&#27604;&#36739;&#25105;&#20204;&#20351;&#29992;&#26368;&#24120;&#29992;&#30340;&#30417;&#30563;&#20998;&#31867;&#22120;&#36827;&#34892;&#20998;&#31867;&#32467;&#26524;&#65306;K&#26368;&#36817;&#37051;&#65288;KNN&#65289;&#12289;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#12289;&#26420;&#32032;&#36125;&#21494;&#26031;&#65288;NB&#65289;&#21644;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#20998;&#31867;&#22120;&#65292;&#36825;&#20123;&#37117;&#26159;&#22522;&#20110;&#25105;&#20204;&#25910;&#38598;&#21644;&#25163;&#21160;&#27880;&#37322;&#30340;YouTube&#25705;&#27931;&#21733;&#26041;&#35328;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid growth of the use of social media websites, obtaining the users' feedback automatically became a crucial task to evaluate their tendencies and behaviors online. Despite this great availability of information, and the increasing number of Arabic users only few research has managed to treat Arabic dialects. The purpose of this paper is to study the opinion and emotion expressed in real Moroccan texts precisely in the YouTube comments using some well-known and commonly used methods for sentiment analysis. In this paper, we present our work of Moroccan dialect comments classification using Machine Learning (ML) models and based on our collected and manually annotated YouTube Moroccan dialect dataset. By employing many text preprocessing and data representation techniques we aim to compare our classification results utilizing the most commonly used supervised classifiers: k-nearest neighbors (KNN), Support Vector Machine (SVM), Naive Bayes (NB), and deep learning (DL) classif
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#19977;&#31181;&#24120;&#35265;&#30340;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65288;CTM&#12289;ProdLDA&#21644;ETM&#65289;&#65292;&#21033;&#29992;&#22235;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#25506;&#35752;&#20102;&#20351;&#29992;dropout&#23545;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#30340;&#36136;&#37327;&#21644;&#39044;&#27979;&#25928;&#26524;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2303.15973</link><description>&lt;p&gt;
&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#30495;&#30340;&#38656;&#35201;&#20351;&#29992;dropout&#21527;&#65311;&#20851;&#20110;dropout&#22312;&#20027;&#39064;&#24314;&#27169;&#20013;&#30340;&#24433;&#21709;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Do Neural Topic Models Really Need Dropout? Analysis of the Effect of Dropout in Topic Modeling. (arXiv:2303.15973v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15973
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#19977;&#31181;&#24120;&#35265;&#30340;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65288;CTM&#12289;ProdLDA&#21644;ETM&#65289;&#65292;&#21033;&#29992;&#22235;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#25506;&#35752;&#20102;&#20351;&#29992;dropout&#23545;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#30340;&#36136;&#37327;&#21644;&#39044;&#27979;&#25928;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Dropout&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#27491;&#21017;&#21270;&#25216;&#24039;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#23567;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#22823;&#22411;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#22312;&#27979;&#35797;&#38598;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#23613;&#31649;&#36825;&#31181;&#27491;&#21017;&#21270;&#25216;&#24039;&#22312;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26377;&#25928;&#24615;&#24050;&#32463;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#23545;&#20110;&#26080;&#30417;&#30563;&#27169;&#22411;&#65288;&#29305;&#21035;&#26159;&#22522;&#20110;VAE&#30340;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65289;&#65292;&#32570;&#20047;&#23545;&#20854;&#30340;&#20998;&#26512;&#12290;&#26412;&#25991;&#22312;&#19977;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65288;&#21363;&#65292;&#24773;&#22659;&#20027;&#39064;&#27169;&#22411;&#65288;CTM&#65289;&#65292;ProdLDA&#21644;&#23884;&#20837;&#24335;&#20027;&#39064;&#27169;&#22411;&#65288;ETM&#65289;&#65289;&#20013;&#65292;&#21033;&#29992;&#22235;&#20010;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#65292;&#20998;&#26512;&#20102;VAE&#26550;&#26500;&#30340;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#20013;dropout&#30340;&#21518;&#26524;&#12290;&#25105;&#20204;&#20174;&#29983;&#25104;&#30340;&#20027;&#39064;&#30340;&#36136;&#37327;&#21644;&#39044;&#27979;&#24615;&#33021;&#30340;&#35282;&#24230;&#65292;&#34920;&#24449;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;dropout&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dropout is a widely used regularization trick to resolve the overfitting issue in large feedforward neural networks trained on a small dataset, which performs poorly on the held-out test subset. Although the effectiveness of this regularization trick has been extensively studied for convolutional neural networks, there is a lack of analysis of it for unsupervised models and in particular, VAE-based neural topic models. In this paper, we have analyzed the consequences of dropout in the encoder as well as in the decoder of the VAE architecture in three widely used neural topic models, namely, contextualized topic model (CTM), ProdLDA, and embedded topic model (ETM) using four publicly available datasets. We characterize the dropout effect on these models in terms of the quality and predictive performance of the generated topics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; MMAN &#30340;&#22810;&#31890;&#24230;&#21305;&#37197;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#21487;&#20197;&#20840;&#38754;&#25552;&#21462;&#26597;&#35810;&#21644;&#26597;&#35810;&#31867;&#21035;&#20132;&#20114;&#30697;&#38453;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#28040;&#38500;&#26597;&#35810;&#21644;&#31867;&#21035;&#20043;&#38388;&#34920;&#36798;&#24046;&#24322;&#30340;&#24046;&#36317;&#65292;&#29992;&#20110;&#26597;&#35810;&#24847;&#22270;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2303.15870</link><description>&lt;p&gt;
&#30005;&#21830;&#26816;&#32034;&#20013;&#29992;&#20110;&#26597;&#35810;&#24847;&#22270;&#20998;&#31867;&#30340;&#22810;&#31890;&#24230;&#21305;&#37197;&#27880;&#24847;&#21147;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
A Multi-Granularity Matching Attention Network for Query Intent Classification in E-commerce Retrieval. (arXiv:2303.15870v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15870
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; MMAN &#30340;&#22810;&#31890;&#24230;&#21305;&#37197;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#21487;&#20197;&#20840;&#38754;&#25552;&#21462;&#26597;&#35810;&#21644;&#26597;&#35810;&#31867;&#21035;&#20132;&#20114;&#30697;&#38453;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#28040;&#38500;&#26597;&#35810;&#21644;&#31867;&#21035;&#20043;&#38388;&#34920;&#36798;&#24046;&#24322;&#30340;&#24046;&#36317;&#65292;&#29992;&#20110;&#26597;&#35810;&#24847;&#22270;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26597;&#35810;&#24847;&#22270;&#20998;&#31867;&#26088;&#22312;&#21327;&#21161;&#23458;&#25143;&#25214;&#21040;&#25152;&#38656;&#20135;&#21697;&#65292;&#24050;&#25104;&#20026;&#30005;&#23376;&#21830;&#21153;&#25628;&#32034;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#29616;&#26377;&#30340;&#26597;&#35810;&#24847;&#22270;&#20998;&#31867;&#27169;&#22411;&#35201;&#20040;&#35774;&#35745;&#26356;&#31934;&#32454;&#30340;&#27169;&#22411;&#20197;&#22686;&#24378;&#26597;&#35810;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#35201;&#20040;&#25506;&#32034;&#26631;&#31614;&#22270;&#21644;&#22810;&#20219;&#21153;&#20197;&#24110;&#21161;&#27169;&#22411;&#23398;&#20064;&#22806;&#37096;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#26080;&#27861;&#20174;&#26597;&#35810;&#21644;&#31867;&#21035;&#20013;&#25429;&#25417;&#22810;&#31890;&#24230;&#21305;&#37197;&#29305;&#24449;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#38590;&#20197;&#24357;&#34917;&#38750;&#27491;&#24335;&#26597;&#35810;&#21644;&#31867;&#21035;&#20043;&#38388;&#34920;&#36798;&#24046;&#24322;&#30340;&#24046;&#36317;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#31890;&#24230;&#21305;&#37197;&#27880;&#24847;&#21147;&#32593;&#32476;(MMAN)&#65292;&#20854;&#21253;&#21547;&#19977;&#20010;&#27169;&#22359;&#65306;&#33258;&#21305;&#37197;&#27169;&#22359;&#12289;&#23383;&#31526;&#32423;&#21305;&#37197;&#27169;&#22359;&#21644;&#35821;&#20041;&#32423;&#21305;&#37197;&#27169;&#22359;&#65292;&#20197;&#20840;&#38754;&#25552;&#21462;&#26597;&#35810;&#21644;&#26597;&#35810;&#31867;&#21035;&#20132;&#20114;&#30697;&#38453;&#30340;&#29305;&#24449;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#28040;&#38500;&#26597;&#35810;&#24847;&#22270;&#20998;&#31867;&#20013;&#26597;&#35810;&#21644;&#31867;&#21035;&#20043;&#38388;&#34920;&#36798;&#24046;&#24322;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Query intent classification, which aims at assisting customers to find desired products, has become an essential component of the e-commerce search. Existing query intent classification models either design more exquisite models to enhance the representation learning of queries or explore label-graph and multi-task to facilitate models to learn external information. However, these models cannot capture multi-granularity matching features from queries and categories, which makes them hard to mitigate the gap in the expression between informal queries and categories.  This paper proposes a Multi-granularity Matching Attention Network (MMAN), which contains three modules: a self-matching module, a char-level matching module, and a semantic-level matching module to comprehensively extract features from the query and a query-category interaction matrix. In this way, the model can eliminate the difference in expression between queries and categories for query intent classification. We conduc
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#21021;&#32423;&#20445;&#20581;&#21307;&#24072;&#30340;&#24739;&#32773;&#21307;&#30103;&#31508;&#35760;&#36827;&#34892;&#32954;&#30284;&#26089;&#26399;&#39044;&#27979;&#30340;&#38382;&#39064;&#65292;&#24182;&#25506;&#35752;&#20102;&#38024;&#23545;&#39640;&#24230;&#19981;&#24179;&#34913;&#20998;&#31867;&#38382;&#39064;&#30340;&#36719;&#25552;&#31034;&#35843;&#25972;&#21644;&#38745;&#24577;&#35789;&#23884;&#20837;&#27169;&#22411;&#22312;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.15846</link><description>&lt;p&gt;
&#21033;&#29992;&#21021;&#32423;&#20445;&#20581;&#21307;&#24072;&#30340;&#33655;&#20848;&#21307;&#30103;&#31508;&#35760;&#39044;&#27979;&#32954;&#30284;&#30340;&#36719;&#25552;&#31034;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Soft-prompt tuning to predict lung cancer using primary care free-text Dutch medical notes. (arXiv:2303.15846v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15846
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#21021;&#32423;&#20445;&#20581;&#21307;&#24072;&#30340;&#24739;&#32773;&#21307;&#30103;&#31508;&#35760;&#36827;&#34892;&#32954;&#30284;&#26089;&#26399;&#39044;&#27979;&#30340;&#38382;&#39064;&#65292;&#24182;&#25506;&#35752;&#20102;&#38024;&#23545;&#39640;&#24230;&#19981;&#24179;&#34913;&#20998;&#31867;&#38382;&#39064;&#30340;&#36719;&#25552;&#31034;&#35843;&#25972;&#21644;&#38745;&#24577;&#35789;&#23884;&#20837;&#27169;&#22411;&#22312;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#19978;&#19979;&#25991;&#35789;&#34920;&#31034;&#30340;&#19981;&#21516;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;&#33655;&#20848;&#21021;&#32423;&#20445;&#20581;&#21307;&#24072;&#30340;&#24739;&#32773;&#21307;&#30103;&#31508;&#35760;&#26089;&#26399;&#39044;&#27979;&#32954;&#30284;&#30340;&#38382;&#39064;&#12290;&#22240;&#20026;&#32954;&#30284;&#22312;&#21021;&#32423;&#20445;&#20581;&#20013;&#30340;&#24739;&#30149;&#29575;&#36739;&#20302;&#65292;&#25152;&#20197;&#25105;&#20204;&#36824;&#35299;&#20915;&#20102;&#22312;&#39640;&#24230;&#19981;&#24179;&#34913;&#30340;&#31867;&#21035;&#19979;&#36827;&#34892;&#20998;&#31867;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#22823;&#22411;&#22522;&#20110;Transformer&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;&#24182;&#30740;&#31350;&#65306;1&#65289;&#22914;&#20309;&#23558;\textit {&#36719;&#25552;&#31034;&#35843;&#25972;} - &#19968;&#31181;&#20351;&#29992;&#23567;&#37327;&#35757;&#32451;&#25968;&#25454;&#35843;&#25972;PLMs&#30340;NLP&#25216;&#26415; - &#19982;&#26631;&#20934;&#27169;&#22411;&#24494;&#35843;&#36827;&#34892;&#27604;&#36739;&#65307; 2&#65289;&#22312;&#39640;&#24230;&#19981;&#24179;&#34913;&#30340;&#35774;&#32622;&#20013;&#65292;&#26159;&#21542;&#31616;&#21333;&#30340;&#38745;&#24577;&#35789;&#23884;&#20837;&#27169;&#22411;&#65288;WEMs&#65289;&#21487;&#20197;&#27604;PLMs&#26356;&#20581;&#22766;&#65307;&#20197;&#21450;3&#65289;&#24403;&#35757;&#32451;&#31508;&#35760;&#26469;&#33258;&#23569;&#37327;&#24739;&#32773;&#26102;&#65292;&#27169;&#22411;&#30340;&#34920;&#29616;&#22914;&#20309;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;1&#65289;&#36719;&#25552;&#31034;&#35843;&#25972;&#26159;&#26631;&#20934;&#27169;&#22411;&#24494;&#35843;&#30340;&#26377;&#25928;&#26367;&#20195;&#26041;&#26696;&#65307; 2&#65289;PLMs&#27604;&#36739;&#31616;&#21333;&#30340;&#38745;&#24577;&#35789;&#23884;&#20837;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#21306;&#20998;&#33021;&#21147;&#20294;&#26356;&#24046;&#30340;&#26657;&#20934;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate different natural language processing (NLP) approaches based on contextualised word representations for the problem of early prediction of lung cancer using free-text patient medical notes of Dutch primary care physicians. Because lung cancer has a low prevalence in primary care, we also address the problem of classification under highly imbalanced classes. Specifically, we use large Transformer-based pretrained language models (PLMs) and investigate: 1) how \textit{soft prompt-tuning} -- an NLP technique used to adapt PLMs using small amounts of training data -- compares to standard model fine-tuning; 2) whether simpler static word embedding models (WEMs) can be more robust compared to PLMs in highly imbalanced settings; and 3) how models fare when trained on notes from a small number of patients. We find that 1) soft-prompt tuning is an efficient alternative to standard model fine-tuning; 2) PLMs show better discrimination but worse calibration compared to simpler stat
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#35780;&#20272;&#20102;&#22522;&#20110;LLM&#21644;ChatGPT&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#26174;&#31034;&#20986;&#20854;&#22312;&#21387;&#21147;&#21644;&#25233;&#37057;&#30151;&#26816;&#27979;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#33258;&#26432;&#39118;&#38505;&#26816;&#27979;&#19978;&#20173;&#38656;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2303.15727</link><description>&lt;p&gt;
&#35780;&#20272;ChatGPT&#22312;&#22522;&#20110;NLP&#30340;&#24515;&#29702;&#20581;&#24247;&#24212;&#29992;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Evaluation of ChatGPT for NLP-based Mental Health Applications. (arXiv:2303.15727v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15727
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#35780;&#20272;&#20102;&#22522;&#20110;LLM&#21644;ChatGPT&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#26174;&#31034;&#20986;&#20854;&#22312;&#21387;&#21147;&#21644;&#25233;&#37057;&#30151;&#26816;&#27979;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#33258;&#26432;&#39118;&#38505;&#26816;&#27979;&#19978;&#20173;&#38656;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#22810;&#39033;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#20855;&#26377;&#25104;&#21151;&#30340;&#24212;&#29992;&#65292;&#21487;&#33021;&#23545;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#30340;&#24515;&#29702;&#20581;&#24247;&#24212;&#29992;&#30740;&#31350;&#20063;&#24456;&#26377;&#24110;&#21161;&#12290;&#26412;&#30740;&#31350;&#25253;&#21578;&#20102;&#22522;&#20110;LLM&#30340;ChatGPT (&#20351;&#29992;gpt-3.5-turbo&#21518;&#31471;)&#22312;&#19977;&#20010;&#25991;&#26412;&#31867;&#24515;&#29702;&#20581;&#24247;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;: &#21387;&#21147;&#26816;&#27979; (2&#31867;&#20998;&#31867;)&#12289;&#25233;&#37057;&#30151;&#26816;&#27979;(2&#31867;&#20998;&#31867;)&#21644;&#33258;&#26432;&#39118;&#38505;&#26816;&#27979;(5&#31867;&#20998;&#31867;)&#12290;&#25105;&#20204;&#20174;&#20844;&#20849;&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#20102;&#19977;&#20010;&#20998;&#31867;&#20219;&#21153;&#30340;&#24102;&#26631;&#27880;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#12290;&#28982;&#21518;&#20351;&#29992;ChatGPT API&#23545;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#36827;&#34892;&#36755;&#20837;&#25552;&#31034;&#20998;&#31867;&#12290;&#25105;&#20204;&#24471;&#21040;&#20102;0.73&#12289;0.86&#21644;0.37&#30340;F1&#20998;&#25968;&#65292;&#20998;&#21035;&#29992;&#20110;&#21387;&#21147;&#26816;&#27979;&#12289;&#25233;&#37057;&#30151;&#26816;&#27979;&#21644;&#33258;&#26432;&#39118;&#38505;&#26816;&#27979;&#12290;&#24635;&#20307;&#19978;&#65292;ChatGPT&#22312;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#20855;&#22791;&#24456;&#22823;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLM) have been successful in several natural language understanding tasks and could be relevant for natural language processing (NLP)-based mental health application research. In this work, we report the performance of LLM-based ChatGPT (with gpt-3.5-turbo backend) in three text-based mental health classification tasks: stress detection (2-class classification), depression detection (2-class classification), and suicidality detection (5-class classification). We obtained annotated social media posts for the three classification tasks from public datasets. Then ChatGPT API classified the social media posts with an input prompt for classification. We obtained F1 scores of 0.73, 0.86, and 0.37 for stress detection, depression detection, and suicidality detection, respectively. A baseline model that always predicted the dominant class resulted in F1 scores of 0.35, 0.60, and 0.19. The zero-shot classification accuracy obtained with ChatGPT indicates a potential use o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#27493;&#36923;&#36753;&#25512;&#29702;&#65292;&#37319;&#29992;&#20102;&#26174;&#24335;&#35268;&#21010;&#26469;&#24110;&#21161;&#20570;&#20986;&#26356;&#26126;&#26234;&#30340;&#20915;&#31574;&#65292;&#27604;&#20854;&#20182;&#31454;&#20105;&#31995;&#32479;&#34920;&#29616;&#26356;&#22909;&#65292;&#26174;&#24335;&#35268;&#21010;&#22312;&#31995;&#32479;&#24615;&#33021;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.15714</link><description>&lt;p&gt;
&#26174;&#24335;&#35268;&#21010;&#26377;&#21161;&#20110;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#36923;&#36753;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Explicit Planning Helps Language Models in Logical Reasoning. (arXiv:2303.15714v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#27493;&#36923;&#36753;&#25512;&#29702;&#65292;&#37319;&#29992;&#20102;&#26174;&#24335;&#35268;&#21010;&#26469;&#24110;&#21161;&#20570;&#20986;&#26356;&#26126;&#26234;&#30340;&#20915;&#31574;&#65292;&#27604;&#20854;&#20182;&#31454;&#20105;&#31995;&#32479;&#34920;&#29616;&#26356;&#22909;&#65292;&#26174;&#24335;&#35268;&#21010;&#22312;&#31995;&#32479;&#24615;&#33021;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#31995;&#32479;&#65292;&#37319;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#27493;&#36923;&#36753;&#25512;&#29702;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#23558;&#26174;&#24335;&#35268;&#21010;&#32435;&#20837;&#21040;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#22240;&#27492;&#21487;&#20197;&#36890;&#36807;&#23637;&#26395;&#26410;&#26469;&#30340;&#25928;&#26524;&#26469;&#20570;&#20986;&#26356;&#26126;&#26234;&#30340;&#20915;&#31574;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#20840;&#22871;&#31995;&#32479;&#22312;&#22810;&#39033;&#36873;&#25321;&#39064;&#31572;&#39064;&#20219;&#21153;&#20013;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#31454;&#20105;&#31995;&#32479;&#65292;&#23613;&#31649;&#21482;&#26377;&#32422;15&#20159;&#20010;&#21442;&#25968;&#65292;&#20294;&#19982;GPT-3-davinci&#34920;&#29616;&#30456;&#24403;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22810;&#20010;&#28040;&#34701;&#30740;&#31350;&#20197;&#35777;&#26126;&#26174;&#24335;&#35268;&#21010;&#22312;&#31995;&#32479;&#24615;&#33021;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models have been shown to perform remarkably well on a wide range of natural language processing tasks. In this paper, we propose a novel system that uses language models to perform multi-step logical reasoning. Our system incorporates explicit planning into its inference procedure, thus able to make more informed reasoning decisions at each step by looking ahead into their future effects. In our experiments, our full system significantly outperforms other competing systems. On a multiple-choice question answering task, our system performs competitively compared to GPT-3-davinci despite having only around 1.5B parameters. We conduct several ablation studies to demonstrate that explicit planning plays a crucial role in the system's performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25910;&#38598;180&#19975;&#20221;&#32654;&#22269;&#20027;&#35201;&#23186;&#20307;&#26426;&#26500;&#30340;&#26032;&#38395;&#26631;&#39064;&#65292;&#25581;&#31034;&#20102;&#32654;&#22269;&#26032;&#38395;&#23186;&#20307;&#20013;&#30340;&#35821;&#20041;&#24046;&#24322;&#65292;&#24182;&#21457;&#29616;&#22312;&#22269;&#20869;&#25919;&#27835;&#21644;&#31038;&#20250;&#38382;&#39064;&#19978;&#65292;&#24046;&#24322;&#21487;&#20197;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#24402;&#22240;&#20110;&#23186;&#20307;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2303.15708</link><description>&lt;p&gt;
&#20559;&#35265;&#36824;&#26159;&#22810;&#26679;&#24615;&#65311;&#25581;&#31034;&#32654;&#22269;&#26032;&#38395;&#26631;&#39064;&#20013;&#30340;&#35821;&#20041;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Bias or Diversity? Unraveling Semantic Discrepancy in U.S. News Headlines. (arXiv:2303.15708v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25910;&#38598;180&#19975;&#20221;&#32654;&#22269;&#20027;&#35201;&#23186;&#20307;&#26426;&#26500;&#30340;&#26032;&#38395;&#26631;&#39064;&#65292;&#25581;&#31034;&#20102;&#32654;&#22269;&#26032;&#38395;&#23186;&#20307;&#20013;&#30340;&#35821;&#20041;&#24046;&#24322;&#65292;&#24182;&#21457;&#29616;&#22312;&#22269;&#20869;&#25919;&#27835;&#21644;&#31038;&#20250;&#38382;&#39064;&#19978;&#65292;&#24046;&#24322;&#21487;&#20197;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#24402;&#22240;&#20110;&#23186;&#20307;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26222;&#36941;&#19968;&#33268;&#35748;&#20026;&#26032;&#38395;&#23186;&#20307;&#22312;&#20854;&#26032;&#38395;&#25991;&#31456;&#20013;&#37319;&#29992;&#24847;&#35782;&#24418;&#24577;&#20559;&#35265;&#12290;&#28982;&#32780;&#65292;&#22312;&#27979;&#37327;&#23186;&#20307;&#26426;&#26500;&#20043;&#38388;&#30340;&#24046;&#24322;&#24182;&#36827;&#19968;&#27493;&#35299;&#21078;&#35821;&#20041;&#24046;&#24322;&#30340;&#28304;&#22836;&#26041;&#38754;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#21463;&#21040;&#20102;&#26679;&#26412;&#22823;&#23567;&#30340;&#38480;&#21046;&#21644;&#33539;&#22260;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;180&#19975;&#20221;&#32654;&#22269;&#20027;&#35201;&#23186;&#20307;&#26426;&#26500;&#20174;2014&#24180;&#33267;2022&#24180;&#30340;&#26032;&#38395;&#26631;&#39064;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#20197;&#20840;&#38754;&#36319;&#36394;&#21644;&#35299;&#21078;&#32654;&#22269;&#26032;&#38395;&#23186;&#20307;&#20013;&#30340;&#35821;&#20041;&#24046;&#24322;&#12290;&#25105;&#20204;&#37319;&#29992;&#22810;&#20803;&#23545;&#24212;&#20998;&#26512;(MCA)&#26469;&#37327;&#21270;&#19982;&#22235;&#20010;&#31361;&#20986;&#20027;&#39064;&#30456;&#20851;&#30340;&#35821;&#20041;&#24046;&#24322; - &#22269;&#20869;&#25919;&#27835;&#12289;&#32463;&#27982;&#38382;&#39064;&#12289;&#31038;&#20250;&#38382;&#39064;&#21644;&#22806;&#20132;&#20107;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#27604;&#36739;&#23186;&#20307;&#26631;&#39064;&#20013;&#26368;&#24120;&#35265;&#30340;n-gram&#65292;&#25552;&#20379;&#36827;&#19968;&#27493;&#30340;&#23450;&#24615;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22269;&#20869;&#25919;&#27835;&#21644;&#31038;&#20250;&#38382;&#39064;&#19978;&#65292;&#24046;&#24322;&#21487;&#20197;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#24402;&#22240;&#20110;&#23186;&#20307;&#20559;&#35265;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22806;&#20132;&#25253;&#36947;&#20013;&#30340;&#24046;&#24322;&#21017;&#26356;&#22810;&#22320;&#21453;&#26144;&#20102;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a broad consensus that news media outlets incorporate ideological biases in their news articles. However, prior studies on measuring the discrepancies among media outlets and further dissecting the origins of semantic differences suffer from small sample sizes and limited scope. In this study, we collect a large dataset of 1.8 million news headlines from major U.S. media outlets spanning from 2014 to 2022 to thoroughly track and dissect the semantic discrepancy in U.S. news media. We employ multiple correspondence analysis (MCA) to quantify the semantic discrepancy relating to four prominent topics - domestic politics, economic issues, social issues, and foreign affairs. Additionally, we compare the most frequent n-grams in media headlines to provide further qualitative insights into our analysis. Our findings indicate that on domestic politics and social issues, the discrepancy can be attributed to a certain degree of media bias. Meanwhile, the discrepancy in reporting foreig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#20998;&#32452;&#27468;&#35789;&#26059;&#24459;&#32763;&#35793;&#65288;LTAG&#65289;&#30340;&#32508;&#21512;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#22312;&#32763;&#35793;&#28304;&#27468;&#35789;&#30340;&#21516;&#26102;&#30830;&#23450;&#27599;&#20010;&#35299;&#30721;&#27493;&#39588;&#30340;&#23545;&#40784;&#38899;&#31526;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.15705</link><description>&lt;p&gt;
&#32763;&#35793;&#27468;&#26354;&#20043;&#32654;&#65306;&#32852;&#21512;&#23398;&#20064;&#23545;&#40784;&#26059;&#24459;&#19982;&#32763;&#35793;&#27468;&#35789;
&lt;/p&gt;
&lt;p&gt;
Translate the Beauty in Songs: Jointly Learning to Align Melody and Translate Lyrics. (arXiv:2303.15705v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#20998;&#32452;&#27468;&#35789;&#26059;&#24459;&#32763;&#35793;&#65288;LTAG&#65289;&#30340;&#32508;&#21512;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#22312;&#32763;&#35793;&#28304;&#27468;&#35789;&#30340;&#21516;&#26102;&#30830;&#23450;&#27599;&#20010;&#35299;&#30721;&#27493;&#39588;&#30340;&#23545;&#40784;&#38899;&#31526;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27468;&#26354;&#32763;&#35793;&#38656;&#35201;&#32763;&#35793;&#27468;&#35789;&#24182;&#23545;&#40784;&#38899;&#31526;&#65292;&#20197;&#20351;&#32467;&#26524;&#21487;&#20197;&#28436;&#21809;&#21040;&#30456;&#24212;&#30340;&#26059;&#24459;&#19978;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22312;&#32763;&#35793;&#36807;&#31243;&#30340;&#19981;&#21516;&#26041;&#38754;&#24341;&#36215;&#20102;&#19968;&#20123;&#20852;&#36259;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#20998;&#32452;&#27468;&#35789;&#26059;&#24459;&#32763;&#35793;&#65288;LTAG&#65289;&#30340;&#32508;&#21512;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#65292;&#21487;&#20197;&#21516;&#26102;&#32763;&#35793;&#28304;&#27468;&#35789;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#38899;&#31526;&#20998;&#32452;&#27169;&#22359;&#30830;&#23450;&#27599;&#20010;&#35299;&#30721;&#27493;&#39588;&#30340;&#23545;&#40784;&#38899;&#31526;&#25968;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#25105;&#20204;&#22996;&#25176;&#23569;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#19987;&#38376;&#29992;&#20110;&#36825;&#20010;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#21453;&#21521;&#32763;&#35793;&#20351;&#29992;&#22823;&#37327;&#30340;&#22686;&#24378;&#25968;&#25454;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20215;&#20013;&#37117;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Song translation requires both translation of lyrics and alignment of music notes so that the resulting verse can be sung to the accompanying melody, which is a challenging problem that has attracted some interests in different aspects of the translation process. In this paper, we propose Lyrics-Melody Translation with Adaptive Grouping (LTAG), a holistic solution to automatic song translation by jointly modeling lyrics translation and lyrics-melody alignment. It is a novel encoder-decoder framework that can simultaneously translate the source lyrics and determine the number of aligned notes at each decoding step through an adaptive note grouping module. To address data scarcity, we commissioned a small amount of training data annotated specifically for this task and used large amounts of augmented data through back-translation. Experiments conducted on an English-Chinese song translation data set show the effectiveness of our model in both automatic and human evaluation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#22810;&#35821;&#35328;&#25991;&#26412;&#20998;&#31867;&#35299;&#20559;&#27169;&#22411;&#65292;&#19981;&#20381;&#36182;&#20110;&#22806;&#37096;&#35821;&#35328;&#36164;&#28304;&#65292;&#21487;&#20197;&#25193;&#23637;&#21040;&#20219;&#20309;&#20854;&#20182;&#35821;&#35328;&#12290;&#35813;&#27169;&#22411;&#21253;&#21547;&#22235;&#20010;&#27169;&#22359;&#65292;&#21487;&#20197;&#26377;&#25928;&#38477;&#20302;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2303.15697</link><description>&lt;p&gt;
&#27169;&#22411;&#19982;&#35780;&#20272;&#65306;&#38754;&#21521;&#22810;&#35821;&#35328;&#25991;&#26412;&#20998;&#31867;&#30340;&#20844;&#24179;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Model and Evaluation: Towards Fairness in Multilingual Text Classification. (arXiv:2303.15697v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15697
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#22810;&#35821;&#35328;&#25991;&#26412;&#20998;&#31867;&#35299;&#20559;&#27169;&#22411;&#65292;&#19981;&#20381;&#36182;&#20110;&#22806;&#37096;&#35821;&#35328;&#36164;&#28304;&#65292;&#21487;&#20197;&#25193;&#23637;&#21040;&#20219;&#20309;&#20854;&#20182;&#35821;&#35328;&#12290;&#35813;&#27169;&#22411;&#21253;&#21547;&#22235;&#20010;&#27169;&#22359;&#65292;&#21487;&#20197;&#26377;&#25928;&#38477;&#20302;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#20110;&#35299;&#20915;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#21333;&#35821;&#35328;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#19978;&#65292;&#23545;&#20110;&#22810;&#35821;&#35328;&#25991;&#26412;&#20998;&#31867;&#30340;&#20844;&#24179;&#24615;&#30740;&#31350;&#20173;&#28982;&#38750;&#24120;&#26377;&#38480;&#12290;&#26412;&#25991;&#20851;&#27880;&#20110;&#22810;&#35821;&#35328;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#35299;&#20559;&#26694;&#26550;&#65292;&#26088;&#22312;&#20026;&#22810;&#35821;&#35328;&#25991;&#26412;&#20998;&#31867;&#25552;&#20379;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#22806;&#37096;&#35821;&#35328;&#36164;&#28304;&#65292;&#21487;&#20197;&#25193;&#23637;&#21040;&#20219;&#20309;&#20854;&#20182;&#35821;&#35328;&#12290;&#35813;&#27169;&#22411;&#21253;&#21547;&#22235;&#20010;&#27169;&#22359;&#65306;&#22810;&#35821;&#35328;&#25991;&#26412;&#34920;&#31034;&#27169;&#22359;&#65292;&#35821;&#35328;&#34701;&#21512;&#27169;&#22359;&#65292;&#25991;&#26412;&#35299;&#20559;&#27169;&#22359;&#65292;&#21644;&#25991;&#26412;&#20998;&#31867;&#27169;&#22359;&#12290;&#22810;&#35821;&#35328;&#25991;&#26412;&#34920;&#31034;&#27169;&#22359;&#20351;&#29992;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26469;&#34920;&#31034;&#25991;&#26412;&#65292;&#35821;&#35328;&#34701;&#21512;&#27169;&#22359;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#20351;&#19981;&#21516;&#35821;&#35328;&#30340;&#35821;&#20041;&#31354;&#38388;&#36235;&#20110;&#19968;&#33268;&#65292;&#25991;&#26412;&#35299;&#20559;&#27169;&#22359;&#20351;&#29992;&#20849;&#29616;&#32479;&#35745;&#37327;&#26469;&#37325;&#26032;&#21152;&#26435;&#25991;&#26412;&#34920;&#31034;&#65292;&#20197;&#20943;&#36731;&#25935;&#24863;&#23646;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22810;&#35821;&#35328;&#25991;&#26412;&#20998;&#31867;&#22522;&#20934;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#24182;&#34920;&#26126;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#20445;&#25345;&#31454;&#20105;&#24615;&#33021;&#30340;&#21516;&#26102;&#26377;&#25928;&#38477;&#20302;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, more and more research has focused on addressing bias in text classification models. However, existing research mainly focuses on the fairness of monolingual text classification models, and research on fairness for multilingual text classification is still very limited. In this paper, we focus on the task of multilingual text classification and propose a debiasing framework for multilingual text classification based on contrastive learning. Our proposed method does not rely on any external language resources and can be extended to any other languages. The model contains four modules: multilingual text representation module, language fusion module, text debiasing module, and text classification module. The multilingual text representation module uses a multilingual pre-trained language model to represent the text, the language fusion module makes the semantic spaces of different languages tend to be consistent through contrastive learning, and the text debiasing module uses co
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38754;&#21521;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#24402;&#32435;&#24335;&#34920;&#31034;&#27169;&#22411;&#65288;iHT&#65289;&#65292;&#36890;&#36807;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#65292;iHT&#34920;&#31034;&#21487;&#36716;&#31227;&#19988;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.15682</link><description>&lt;p&gt;
&#38754;&#21521;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;Transformer&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-training Transformers for Knowledge Graph Completion. (arXiv:2303.15682v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15682
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38754;&#21521;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#24402;&#32435;&#24335;&#34920;&#31034;&#27169;&#22411;&#65288;iHT&#65289;&#65292;&#36890;&#36807;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#65292;iHT&#34920;&#31034;&#21487;&#36716;&#31227;&#19988;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22270;&#32467;&#26500;&#30340;&#24322;&#26500;&#24615;&#21644;&#22810;&#20851;&#31995;&#24615;&#65292;&#23398;&#20064;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#30340;&#21487;&#36716;&#31227;&#34920;&#31034;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#21463;Transformer&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#25991;&#26412;&#26041;&#38754;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#38754;&#21521;KG&#34917;&#20840;&#30340;&#24402;&#32435;&#24335;&#34920;&#31034;&#27169;&#22411;&#65288;iHT&#65289;&#65292;&#36890;&#36807;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#65292;iHT&#30001;&#23454;&#20307;&#32534;&#30721;&#22120;&#65288;&#20363;&#22914;BERT&#65289;&#21644;&#37051;&#23621;&#24863;&#30693;&#30340;&#20851;&#31995;&#35780;&#20998;&#20989;&#25968;&#32452;&#25104;&#65292;&#20004;&#32773;&#37117;&#30001;Transformer&#21442;&#25968;&#21270;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#22823;&#22411;KG&#25968;&#25454;&#38598;Wikidata5M&#19978;&#39044;&#35757;&#32451;iHT&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21305;&#37197;&#35780;&#20272;&#19978;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#65292;&#30456;&#23545;&#20110;&#20808;&#21069;SOTA&#27169;&#22411;&#65292;&#24179;&#22343;&#20498;&#25968;&#25490;&#21517;&#25552;&#39640;&#20102;25&#65285;&#20197;&#19978;&#12290;&#24403;&#22312;&#20855;&#26377;&#23454;&#20307;&#21644;&#20851;&#31995;&#31227;&#20301;&#30340;&#36739;&#23567;KG&#19978;&#36827;&#19968;&#27493;&#24494;&#35843;&#26102;&#65292;&#39044;&#35757;&#32451;&#30340;iHT&#34920;&#31034;&#34987;&#35777;&#26126;&#26159;&#21487;&#36716;&#31227;&#30340;&#65292;&#26174;&#30528;&#25552;&#39640;&#20102;FB15K-237&#21644;WN18RR&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning transferable representation of knowledge graphs (KGs) is challenging due to the heterogeneous, multi-relational nature of graph structures. Inspired by Transformer-based pretrained language models' success on learning transferable representation for texts, we introduce a novel inductive KG representation model (iHT) for KG completion by large-scale pre-training. iHT consists of a entity encoder (e.g., BERT) and a neighbor-aware relational scoring function both parameterized by Transformers. We first pre-train iHT on a large KG dataset, Wikidata5M. Our approach achieves new state-of-the-art results on matched evaluations, with a relative improvement of more than 25% in mean reciprocal rank over previous SOTA models. When further fine-tuned on smaller KGs with either entity and relational shifts, pre-trained iHT representations are shown to be transferable, significantly improving the performance on FB15K-237 and WN18RR.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#20030;&#21150;&#22312;2023 IEEE&#28216;&#25103;&#20250;&#35758;&#19978;&#30340;&#31532;&#19968;&#23626;ChatGPT4PCG&#27604;&#36187;&#65292;&#30446;&#26631;&#26159;&#35753;ChatGPT&#29983;&#25104;&#20855;&#26377;&#39640;&#31283;&#23450;&#24615;&#21644;&#31867;&#20284;&#35282;&#33394;&#30340;&#29305;&#36136;&#26469;&#29983;&#25104;&#20855;&#26377;&#31185;&#23398;&#40479;&#35282;&#33394;&#32423;&#27700;&#24179;&#30340;&#20851;&#21345;&#12290;</title><link>http://arxiv.org/abs/2303.15662</link><description>&lt;p&gt;
ChatGPT4PCG&#27604;&#36187;&#65306;&#31185;&#23398;&#40479;&#35282;&#33394;&#32423;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
ChatGPT4PCG Competition: Character-like Level Generation for Science Birds. (arXiv:2303.15662v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15662
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#20030;&#21150;&#22312;2023 IEEE&#28216;&#25103;&#20250;&#35758;&#19978;&#30340;&#31532;&#19968;&#23626;ChatGPT4PCG&#27604;&#36187;&#65292;&#30446;&#26631;&#26159;&#35753;ChatGPT&#29983;&#25104;&#20855;&#26377;&#39640;&#31283;&#23450;&#24615;&#21644;&#31867;&#20284;&#35282;&#33394;&#30340;&#29305;&#36136;&#26469;&#29983;&#25104;&#20855;&#26377;&#31185;&#23398;&#40479;&#35282;&#33394;&#32423;&#27700;&#24179;&#30340;&#20851;&#21345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;2023&#24180;IEEE&#28216;&#25103;&#20250;&#35758;&#19978;&#30340;&#31532;&#19968;&#23626;ChatGPT4PCG&#27604;&#36187;&#12290;&#26412;&#27425;&#27604;&#36187;&#30340;&#30446;&#26631;&#26159;&#35753;&#21442;&#36187;&#32773;&#36890;&#36807;&#21019;&#36896;&#24615;&#21644;&#25552;&#31034;&#24037;&#31243;&#25216;&#33021;&#65292;&#20026;ChatGPT&#21019;&#24314;&#26377;&#25928;&#30340;&#25552;&#31034;&#65292;&#20351;&#20854;&#33021;&#22815;&#20855;&#26377;&#39640;&#31283;&#23450;&#24615;&#21644;&#31867;&#20284;&#35282;&#33394;&#30340;&#29305;&#36136;&#26469;&#29983;&#25104;&#20855;&#26377;&#31185;&#23398;&#40479;&#35282;&#33394;&#32423;&#27700;&#24179;&#30340;&#20851;&#21345;&#12290;&#20026;&#20102;&#38477;&#20302;&#21442;&#36187;&#38376;&#27099;&#65292;&#25105;&#20204;&#23558;&#20219;&#21153;&#38480;&#21046;&#22312;&#29983;&#25104;&#22823;&#20889;&#33521;&#25991;&#23383;&#27597;&#12290;&#21442;&#36187;&#20316;&#21697;&#30340;&#36136;&#37327;&#30001;&#20854;&#31283;&#23450;&#24615;&#21644;&#19982;&#32473;&#23450;&#23383;&#31526;&#30340;&#30456;&#20284;&#24615;&#20915;&#23450;&#12290;&#32473;&#21442;&#36187;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#26679;&#20363;&#25552;&#31034;&#20379;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the first ChatGPT4PCG Competition at the 2023 IEEE Conference on Games. The objective of this competition is for participants to create effective prompts for ChatGPT--enabling it to generate Science Birds levels with high stability and character-like qualities--fully using their creativity as well as prompt engineering skills. ChatGPT is a conversational agent developed by OpenAI. Science Birds is selected as the competition platform because designing an Angry Birds-like level is not a trivial task due to the in-game gravity; the playability of the levels is determined by their stability. To lower the entry barrier to the competition, we limit the task to the generation of capitalized English alphabetical characters. Here, the quality of the generated levels is determined by their stability and similarity to the given characters. A sample prompt is provided to participants for their reference. An experiment is conducted to determine the effectiveness of its modified
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HIE&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#65292;&#23427;&#23558;&#27599;&#20010;&#19977;&#20803;&#32452;&#21516;&#26102;&#24314;&#27169;&#20026;&#36317;&#31163;&#27979;&#37327;&#31354;&#38388;&#21644;&#35821;&#20041;&#27979;&#37327;&#31354;&#38388;&#65292;&#24182;&#22312;&#20998;&#23618;&#24863;&#30693;&#31354;&#38388;&#20013;&#21033;&#29992;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#20016;&#23500;&#20998;&#23618;&#20449;&#24687;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#34920;&#31034;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.15655</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#23618;&#36317;&#31163;&#21644;&#35821;&#20041;&#34920;&#31034;&#23398;&#20064;&#30340;&#32852;&#21512;&#23884;&#20837;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Joint embedding in Hierarchical distance and semantic representation learning for link prediction. (arXiv:2303.15655v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15655
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HIE&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#65292;&#23427;&#23558;&#27599;&#20010;&#19977;&#20803;&#32452;&#21516;&#26102;&#24314;&#27169;&#20026;&#36317;&#31163;&#27979;&#37327;&#31354;&#38388;&#21644;&#35821;&#20041;&#27979;&#37327;&#31354;&#38388;&#65292;&#24182;&#22312;&#20998;&#23618;&#24863;&#30693;&#31354;&#38388;&#20013;&#21033;&#29992;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#20016;&#23500;&#20998;&#23618;&#20449;&#24687;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#34920;&#31034;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#26088;&#22312;&#39044;&#27979;&#30693;&#35782;&#22270;&#35889;&#20013;&#32570;&#22833;&#30340;&#23454;&#20307;&#25110;&#20851;&#31995;&#65292;&#24182;&#19988;&#26159;&#19979;&#28216;&#24212;&#29992;&#31243;&#24207;&#30340;&#20851;&#38190;&#12290;&#29616;&#26377;&#30340;&#33879;&#21517;&#27169;&#22411;&#20027;&#35201;&#36890;&#36807;&#23558;&#30693;&#35782;&#22270;&#35889;&#19977;&#20803;&#32452;&#34920;&#31034;&#20026;&#36317;&#31163;&#31354;&#38388;&#25110;&#35821;&#20041;&#31354;&#38388;&#26469;&#22788;&#29702;&#27492;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#19981;&#33021;&#20805;&#20998;&#25429;&#25417;&#22836;&#23614;&#23454;&#20307;&#30340;&#20449;&#24687;&#65292;&#29978;&#33267;&#19981;&#33021;&#24456;&#22909;&#22320;&#21033;&#29992;&#20998;&#23618;&#32423;&#21035;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#65292;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#65292;&#21363;HIE&#65292;&#23427;&#21516;&#26102;&#23558;&#27599;&#20010;&#19977;&#20803;&#32452;&#65288;h&#65292;r&#65292;t&#65289;&#24314;&#27169;&#20026;&#36317;&#31163;&#27979;&#37327;&#31354;&#38388;&#21644;&#35821;&#20041;&#27979;&#37327;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#23558;HIE&#24341;&#20837;&#20998;&#23618;&#24863;&#30693;&#31354;&#38388;&#65292;&#20197;&#21033;&#29992;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#20016;&#23500;&#20998;&#23618;&#20449;&#24687;&#65292;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#34920;&#31034;&#23398;&#20064;&#25928;&#26524;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#36317;&#31163;&#31354;&#38388;&#20013;&#23545;&#22836;&#23454;&#20307;&#24212;&#29992;&#36317;&#31163;&#21464;&#25442;&#25805;&#20316;&#20197;&#33719;&#21462;&#23614;&#23454;&#20307;&#65292;&#32780;&#19981;&#26159;&#22522;&#20110;&#24179;&#31227;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The link prediction task aims to predict missing entities or relations in the knowledge graph and is essential for the downstream application. Existing well-known models deal with this task by mainly focusing on representing knowledge graph triplets in the distance space or semantic space. However, they can not fully capture the information of head and tail entities, nor even make good use of hierarchical level information. Thus, in this paper, we propose a novel knowledge graph embedding model for the link prediction task, namely, HIE, which models each triplet (\textit{h}, \textit{r}, \textit{t}) into distance measurement space and semantic measurement space, simultaneously. Moreover, HIE is introduced into hierarchical-aware space to leverage rich hierarchical information of entities and relations for better representation learning. Specifically, we apply distance transformation operation on the head entity in distance space to obtain the tail entity instead of translation-based or 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;40&#22810;&#31181;&#32553;&#23567;&#27169;&#22411;&#35268;&#27169;&#36827;&#34892;&#36229;&#22823;&#27169;&#22411;&#21442;&#25968;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#30340;&#19981;&#21487;&#34892;&#24615;&#21644;&#19981;&#20999;&#23454;&#38469;&#24615;&#12290;&#25552;&#20379;&#20102;&#20998;&#31867;&#27861;&#21644;&#26041;&#27861;&#27604;&#36739;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#23454;&#38469;&#25928;&#29575;&#21644;&#21315;&#20159;&#32423;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#12290;</title><link>http://arxiv.org/abs/2303.15647</link><description>&lt;p&gt;
&#32553;&#23567;&#35268;&#27169;&#20197;&#23454;&#29616;&#36229;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#26377;&#25928;&#24494;&#35843;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning. (arXiv:2303.15647v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15647
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;40&#22810;&#31181;&#32553;&#23567;&#27169;&#22411;&#35268;&#27169;&#36827;&#34892;&#36229;&#22823;&#27169;&#22411;&#21442;&#25968;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#30340;&#19981;&#21487;&#34892;&#24615;&#21644;&#19981;&#20999;&#23454;&#38469;&#24615;&#12290;&#25552;&#20379;&#20102;&#20998;&#31867;&#27861;&#21644;&#26041;&#27861;&#27604;&#36739;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#23454;&#38469;&#25928;&#29575;&#21644;&#21315;&#20159;&#32423;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20221;&#31995;&#32479;&#21270;&#30340;&#32508;&#36848;&#21644;&#27604;&#36739;&#65292;&#35206;&#30422;&#20102;2019&#24180;2&#26376;&#33267;2023&#24180;2&#26376;&#26399;&#38388;&#21457;&#24067;&#30340;40&#22810;&#31687;&#21442;&#25968;&#26377;&#25928;&#24494;&#35843;&#26041;&#27861;&#30340;&#35770;&#25991;&#12290;&#36825;&#20123;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#20165;&#35757;&#32451;&#23567;&#37096;&#20998;&#21442;&#25968;&#26469;&#35299;&#20915;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#21487;&#34892;&#21644;&#19981;&#20999;&#23454;&#38469;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20998;&#31867;&#27861;&#65292;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#26041;&#27861;&#65292;&#24182;&#23545;&#23454;&#29616;&#25928;&#29575;&#21644;&#24494;&#35843;&#21315;&#20159;&#32423;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#26041;&#27861;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a systematic overview and comparison of parameter-efficient fine-tuning methods covering over 40 papers published between February 2019 and February 2023. These methods aim to resolve the infeasibility and impracticality of fine-tuning large language models by only training a small set of parameters. We provide a taxonomy that covers a broad range of methods and present a detailed method comparison with a specific focus on real-life efficiency and fine-tuning multibillion-scale language models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;ChatGPT&#20316;&#20026;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;&#20013;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#35780;&#20272;&#22120;&#30340;&#33021;&#21147;&#65292;&#35777;&#26126;&#20854;&#22312;&#19981;&#38656;&#35201;&#27880;&#37322;&#25968;&#25454;&#21644;&#39640;&#35745;&#31639;&#22797;&#26434;&#24230;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.15621</link><description>&lt;p&gt;
ChatGPT&#20316;&#20026;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;&#20013;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#35780;&#20272;&#22120;
&lt;/p&gt;
&lt;p&gt;
ChatGPT as a Factual Inconsistency Evaluator for Abstractive Text Summarization. (arXiv:2303.15621v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15621
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;ChatGPT&#20316;&#20026;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;&#20013;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#35780;&#20272;&#22120;&#30340;&#33021;&#21147;&#65292;&#35777;&#26126;&#20854;&#22312;&#19981;&#38656;&#35201;&#27880;&#37322;&#25968;&#25454;&#21644;&#39640;&#35745;&#31639;&#22797;&#26434;&#24230;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22823;&#22823;&#25552;&#39640;&#20102;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;&#30340;&#24615;&#33021;&#12290;&#29616;&#26377;&#30340;&#25277;&#35937;&#25688;&#35201;&#26041;&#27861;&#30340;&#20027;&#35201;&#38382;&#39064;&#26159;&#20854;&#29983;&#25104;&#30340;&#25688;&#35201;&#23384;&#22312;&#30340;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;&#20026;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#35768;&#22810;&#21162;&#21147;&#23558;&#37325;&#28857;&#25918;&#22312;&#24320;&#21457;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#21644;&#38382;&#31572;&#31561;&#26041;&#38754;&#30340;&#26377;&#25928;&#20107;&#23454;&#24615;&#35780;&#20272;&#25351;&#26631;&#19978;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23384;&#22312;&#35745;&#31639;&#22797;&#26434;&#24230;&#39640;&#21644;&#20381;&#36182;&#27880;&#37322;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;&#26368;&#36817;&#65292;&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19981;&#20165;&#26174;&#31034;&#20102;&#24378;&#22823;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#32780;&#19988;&#36824;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#20247;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#30340;&#20107;&#23454;&#35780;&#20272;&#20219;&#21153;&#65288;&#21253;&#25324;&#20108;&#36827;&#21046;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#12289;&#25688;&#35201;&#25490;&#21517;&#21644;&#19968;&#33268;&#24615;&#35780;&#32423;&#65289;&#19978;&#35780;&#20272;ChatGPT&#30340;&#38646;-shot&#35774;&#32622;&#19979;&#30340;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#35780;&#20272;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#22312;&#25152;&#26377;&#35780;&#20272;&#20219;&#21153;&#19978;&#22343;&#34920;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The performance of abstractive text summarization has been greatly boosted by pre-trained language models recently. The main concern of existing abstractive summarization methods is the factual inconsistency problem of their generated summary. To alleviate the problem, many efforts have focused on developing effective factuality evaluation metrics based on natural language inference and question answering et al. However, they have limitations of high computational complexity and relying on annotated data. Most recently, large language models such as ChatGPT have shown strong ability in not only natural language understanding but also natural language inference. In this paper, we study the factual inconsistency evaluation ability of ChatGPT under the zero-shot setting by evaluating it on the coarse-grained and fine-grained factuality evaluation tasks including binary natural language inference (NLI), summary ranking, and consistency rating. Experimental results show that ChatGPT outperf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#38024;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20219;&#21153;&#29305;&#23450;&#30340;&#23631;&#34109;&#26694;&#26550;&#65292;&#31216;&#20026;Typhoon&#65292;&#21487;&#22312;GLUE&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#21331;&#36234;&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;MRPC&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2303.15619</link><description>&lt;p&gt;
&#21488;&#39118;&#65306;&#38024;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#25928;&#29305;&#23450;&#20219;&#21153;&#23631;&#34109;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Typhoon: Towards an Effective Task-Specific Masking Strategy for Pre-trained Language Models. (arXiv:2303.15619v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15619
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#38024;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20219;&#21153;&#29305;&#23450;&#30340;&#23631;&#34109;&#26694;&#26550;&#65292;&#31216;&#20026;Typhoon&#65292;&#21487;&#22312;GLUE&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#21331;&#36234;&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;MRPC&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#22270;&#24418;&#22788;&#29702;&#21333;&#20803;&#25152;&#33021;&#25552;&#20379;&#30340;&#39640;&#24230;&#24182;&#34892;&#24615;&#65292;&#21464;&#21387;&#22120;&#26550;&#26500;&#20351;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#12290;&#22312;&#20256;&#32479;&#30340;&#23631;&#34109;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20351;&#29992;&#29305;&#27530;&#30340;MASK&#26631;&#35760;&#26469;&#25552;&#31034;&#27169;&#22411;&#20174;&#21608;&#22260;&#21333;&#35789;&#20013;&#25910;&#38598;&#24773;&#22659;&#20449;&#24687;&#20197;&#24674;&#22797;&#21407;&#26412;&#38544;&#34255;&#30340;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#31181;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20219;&#21153;&#29305;&#23450;&#30340;&#23631;&#34109;&#26694;&#26550;&#65292;&#20197;&#22312;GLUE&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#21331;&#36234;&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#12290;&#25105;&#20204;&#22522;&#20110;&#35760;&#21495;&#36755;&#20837;&#26799;&#24230;&#24320;&#21457;&#20102;&#33258;&#24049;&#30340;&#23631;&#34109;&#31639;&#27861;Typhoon&#65292;&#24182;&#23558;&#20854;&#19982;&#20854;&#20182;&#26631;&#20934;&#22522;&#32447;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;Typhoon&#22312;MRPC&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#19982;&#25972;&#20307;&#23383;&#23631;&#34109;&#30456;&#24403;&#12290;&#25105;&#20204;&#30340;&#23454;&#29616;&#21487;&#20197;&#22312;&#20844;&#20849;Github&#24211;&#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Through exploiting a high level of parallelism enabled by graphics processing units, transformer architectures have enabled tremendous strides forward in the field of natural language processing. In a traditional masked language model, special MASK tokens are used to prompt our model to gather contextual information from surrounding words to restore originally hidden information. In this paper, we explore a task-specific masking framework for pre-trained large language models that enables superior performance on particular downstream tasks on the datasets in the GLUE benchmark. We develop our own masking algorithm, Typhoon, based on token input gradients, and compare this with other standard baselines. We find that Typhoon offers performance competitive with whole-word masking on the MRPC dataset. Our implementation can be found in a public Github Repository.
&lt;/p&gt;</description></item><item><title>&#25552;&#21319;&#26085;&#20013;&#26426;&#22120;&#32763;&#35793;&#20934;&#30830;&#24615;&#30340;&#26032;&#26041;&#27861;&#65306;&#20351;&#29992;&#32467;&#21512;&#39044;&#32534;&#36753;&#26041;&#26696;&#21644; ChatGPT &#30340;&#20004;&#27493;&#25552;&#31034;&#31574;&#30053;&#65292;&#24179;&#22343;&#32763;&#35793;&#20934;&#30830;&#24615;&#24471;&#20998;&#25552;&#39640;&#36229;&#36807;35&#65285;&#12290;</title><link>http://arxiv.org/abs/2303.15587</link><description>&lt;p&gt;
&#25552;&#21319;&#26085;&#20013;&#26426;&#22120;&#32763;&#35793;&#30340;&#35821;&#35328;&#23398; ChatGPT &#26234;&#33021;&#25552;&#31034;&#65306;&#20197;&#23450;&#35821;&#20174;&#21477;&#20026;&#20363;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Linguistically Informed ChatGPT Prompts to Enhance Japanese-Chinese Machine Translation: A Case Study on Attributive Clauses. (arXiv:2303.15587v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15587
&lt;/p&gt;
&lt;p&gt;
&#25552;&#21319;&#26085;&#20013;&#26426;&#22120;&#32763;&#35793;&#20934;&#30830;&#24615;&#30340;&#26032;&#26041;&#27861;&#65306;&#20351;&#29992;&#32467;&#21512;&#39044;&#32534;&#36753;&#26041;&#26696;&#21644; ChatGPT &#30340;&#20004;&#27493;&#25552;&#31034;&#31574;&#30053;&#65292;&#24179;&#22343;&#32763;&#35793;&#20934;&#30830;&#24615;&#24471;&#20998;&#25552;&#39640;&#36229;&#36807;35&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26085;&#20013;&#32763;&#35793;&#35821;&#35328;&#23398;&#39046;&#22495;&#20013;&#65292;&#27491;&#30830;&#32763;&#35793;&#23450;&#35821;&#20174;&#21477;&#19968;&#30452;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#30446;&#21069;&#30340;&#26426;&#22120;&#32763;&#35793;&#24037;&#20855;&#32463;&#24120;&#26080;&#27861;&#20934;&#30830;&#22320;&#23558;&#26085;&#35821;&#30340;&#23450;&#35821;&#20174;&#21477;&#32763;&#35793;&#20026;&#20013;&#25991;&#65292;&#26412;&#25991;&#20174;&#35821;&#35328;&#23398;&#35282;&#24230;&#25506;&#35752;&#20102;&#20135;&#29983;&#36825;&#31181;&#22256;&#38590;&#30340;&#35821;&#35328;&#38382;&#39064;&#65292;&#21363;&#21463;&#20462;&#39280;&#21517;&#35789;&#30340;&#35821;&#20041;&#35282;&#33394;&#22914;&#20309;&#24433;&#21709;&#23450;&#35821;&#20174;&#21477;&#30340;&#32763;&#35793;&#27169;&#24335;&#36873;&#25321;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#22256;&#38590;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#32534;&#36753;&#26041;&#26696;&#65292;&#26088;&#22312;&#25552;&#39640;&#32763;&#35793;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20004;&#27493;&#25552;&#31034;&#31574;&#30053;&#65292;&#23558;&#36825;&#31181;&#39044;&#32534;&#36753;&#26041;&#26696;&#19982; ChatGPT &#30456;&#32467;&#21512;&#65292;ChatGPT &#26159;&#30446;&#21069;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#12290;&#35813;&#25552;&#31034;&#31574;&#30053;&#33021;&#22815;&#22312;&#38646;-shot&#22330;&#26223;&#19979;&#20248;&#21270;&#32763;&#35793;&#36755;&#20837;&#65292;&#24182;&#24050;&#32463;&#35777;&#26126;&#21487;&#20197;&#23558;&#24179;&#22343;&#32763;&#35793;&#20934;&#30830;&#24615;&#24471;&#20998;&#25552;&#39640;&#36229;&#36807;35&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of Japanese-Chinese translation linguistics, the issue of correctly translating attributive clauses has persistently proven to be challenging. Present-day machine translation tools often fail to accurately translate attributive clauses from Japanese to Chinese. In light of this, this paper investigates the linguistic problem underlying such difficulties, namely how does the semantic role of the modified noun affect the selection of translation patterns for attributive clauses, from a linguistic perspective. To ad-dress these difficulties, a pre-edit scheme is proposed, which aims to enhance the accuracy of translation. Furthermore, we propose a novel two-step prompt strategy, which combines this pre-edit scheme with ChatGPT, currently the most widely used large language model. This prompt strategy is capable of optimizing translation input in zero-shot scenarios and has been demonstrated to improve the average translation accuracy score by over 35%.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;LLMs&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#27604;&#36739;&#29983;&#25104;&#30340;&#25991;&#26412;&#21644;&#21442;&#32771;&#25991;&#26412;&#30340;&#23458;&#35266;&#21644;&#20027;&#35266;&#32500;&#24230;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2303.15078</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#26159;&#25688;&#35201;&#35780;&#20272;&#30340;&#19981;&#21516;&#35282;&#33394;&#25198;&#28436;&#32773;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Diverse Role-Players for Summarization Evaluation. (arXiv:2303.15078v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15078
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;LLMs&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#27604;&#36739;&#29983;&#25104;&#30340;&#25991;&#26412;&#21644;&#21442;&#32771;&#25991;&#26412;&#30340;&#23458;&#35266;&#21644;&#20027;&#35266;&#32500;&#24230;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#25688;&#35201;&#22312;&#35768;&#22810;&#22330;&#26223;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#29983;&#25104;&#25991;&#26412;&#30340;&#36136;&#37327;&#35780;&#20272;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;&#35821;&#35328;&#35780;&#20272;&#30340;&#19968;&#20010;&#22823;&#25361;&#25112;&#26159;&#29616;&#26377;&#25351;&#26631;&#21644;&#20154;&#24037;&#35780;&#20272;&#20043;&#38388;&#23384;&#22312;&#26126;&#26174;&#30340;&#20998;&#27495;&#12290;&#20363;&#22914;&#65292;&#25991;&#26723;&#25688;&#35201;&#30340;&#36136;&#37327;&#21487;&#20197;&#36890;&#36807;&#20154;&#24037;&#27880;&#37322;&#32773;&#20174;&#23458;&#35266;&#26041;&#38754;&#65288;&#22914;&#35821;&#27861;&#21644;&#35821;&#20041;&#30340;&#27491;&#30830;&#24615;&#65289;&#20197;&#21450;&#20027;&#35266;&#32500;&#24230;&#65288;&#22914;&#20840;&#38754;&#24615;&#12289;&#31616;&#27905;&#24615;&#21644;&#26377;&#36259;&#24615;&#65289;&#36827;&#34892;&#35780;&#20272;&#12290;&#22823;&#22810;&#25968;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#65288;&#22914;BLUE/ROUGE&#65289;&#21487;&#33021;&#26080;&#27861;&#24456;&#22909;&#22320;&#25429;&#25417;&#20197;&#19978;&#32500;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;LLMs&#30340;&#26032;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#27604;&#36739;&#20174;&#23458;&#35266;&#21644;&#20027;&#35266;&#26041;&#38754;&#29983;&#25104;&#30340;&#25991;&#26412;&#21644;&#21442;&#32771;&#25991;&#26412;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#26694;&#26550;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#35282;&#33394;&#25198;&#28436;&#32773;&#25552;&#31034;&#26426;&#21046;&#30340;&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#23458;&#35266;&#21644;&#20027;&#35266;&#32500;&#24230;&#30340;&#24314;&#27169;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#12290;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text summarization has a wide range of applications in many scenarios. The evaluation of the quality of the generated text is a complex problem. A big challenge to language evaluation is that there is a clear divergence between existing metrics and human evaluation. For example, the quality of a document summary can be measured by human annotators from both objective aspects, such as grammatical and semantic correctness, as well as subjective dimensions, such as comprehensiveness, succinctness, and interestingness. Most of the automatic evaluation methods like BLUE/ROUGE may be not able to capture the above dimensions well. In this paper, we propose a new evaluation framework based on LLMs, which provides a comprehensive evaluation framework by comparing generated text and reference text from both objective and subjective aspects. First, we propose to model objective and subjective dimensions of generated text based on roleplayers prompting mechanism. Furthermore, we introduce a contex
&lt;/p&gt;</description></item><item><title>SmartBook&#26159;&#19968;&#31181;AI&#36741;&#21161;&#30340;&#24773;&#25253;&#25253;&#21578;&#29983;&#25104;&#24037;&#20855;&#65292;&#36890;&#36807;&#28040;&#32791;&#22823;&#37327;&#26032;&#38395;&#25968;&#25454;&#29983;&#25104;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#24773;&#20917;&#25253;&#21578;&#65292;&#20854;&#20013;&#21253;&#21547;&#22810;&#20010;&#20551;&#35774;&#65288;&#20027;&#24352;&#65289;&#65292;&#24182;&#19982;&#20107;&#23454;&#20381;&#25454;&#24314;&#31435;&#20016;&#23500;&#30340;&#20851;&#32852;&#12290;&#22312;Ukraine-Russia&#21361;&#26426;&#20013;&#65292;&#26426;&#22120;&#29983;&#25104;&#30340;&#25253;&#21578;&#20197;&#26102;&#38388;&#36724;&#30340;&#24418;&#24335;&#32467;&#26500;&#21270;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#25253;&#21578;&#26102;&#38388;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#27604;&#20154;&#31867;&#21516;&#34892;&#26356;&#20840;&#38754;&#12289;&#20934;&#30830;&#21644;&#19968;&#33268;&#30340;&#25253;&#21578;&#26469;&#25552;&#39640;&#25253;&#21578;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.14337</link><description>&lt;p&gt;
SmartBook&#65306;AI&#36741;&#21161;&#30340;&#24773;&#25253;&#25253;&#21578;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
SmartBook: AI-Assisted Situation Report Generation. (arXiv:2303.14337v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14337
&lt;/p&gt;
&lt;p&gt;
SmartBook&#26159;&#19968;&#31181;AI&#36741;&#21161;&#30340;&#24773;&#25253;&#25253;&#21578;&#29983;&#25104;&#24037;&#20855;&#65292;&#36890;&#36807;&#28040;&#32791;&#22823;&#37327;&#26032;&#38395;&#25968;&#25454;&#29983;&#25104;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#24773;&#20917;&#25253;&#21578;&#65292;&#20854;&#20013;&#21253;&#21547;&#22810;&#20010;&#20551;&#35774;&#65288;&#20027;&#24352;&#65289;&#65292;&#24182;&#19982;&#20107;&#23454;&#20381;&#25454;&#24314;&#31435;&#20016;&#23500;&#30340;&#20851;&#32852;&#12290;&#22312;Ukraine-Russia&#21361;&#26426;&#20013;&#65292;&#26426;&#22120;&#29983;&#25104;&#30340;&#25253;&#21578;&#20197;&#26102;&#38388;&#36724;&#30340;&#24418;&#24335;&#32467;&#26500;&#21270;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#25253;&#21578;&#26102;&#38388;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#27604;&#20154;&#31867;&#21516;&#34892;&#26356;&#20840;&#38754;&#12289;&#20934;&#30830;&#21644;&#19968;&#33268;&#30340;&#25253;&#21578;&#26469;&#25552;&#39640;&#25253;&#21578;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#20852;&#20107;&#20214;&#65292;&#22914;COVID&#30123;&#24773;&#21644;&#20044;&#20811;&#20848;&#21361;&#26426;&#65292;&#38656;&#35201;&#26102;&#38388;&#25935;&#24863;&#30340;&#20840;&#38754;&#20102;&#35299;&#24773;&#20917;&#65292;&#20197;&#20415;&#36827;&#34892;&#36866;&#24403;&#30340;&#20915;&#31574;&#21644;&#26377;&#25928;&#30340;&#34892;&#21160;&#21709;&#24212;&#12290;&#33258;&#21160;&#29983;&#25104;&#24773;&#25253;&#25253;&#21578;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#39046;&#22495;&#19987;&#23478;&#20934;&#22791;&#23448;&#26041;&#20154;&#24037;&#31574;&#21010;&#25253;&#21578;&#30340;&#26102;&#38388;&#12289;&#31934;&#21147;&#21644;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;AI&#30740;&#31350;&#22312;&#36825;&#20010;&#30446;&#26631;&#26041;&#38754;&#38750;&#24120;&#26377;&#38480;&#65292;&#36824;&#27809;&#26377;&#25104;&#21151;&#30340;&#35797;&#39564;&#26469;&#33258;&#21160;&#21270;&#36825;&#31181;&#25253;&#21578;&#29983;&#25104;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SmartBook&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#20219;&#21153;&#20998;&#35299;&#65292;&#26088;&#22312;&#29983;&#25104;&#24773;&#20917;&#25253;&#21578;&#65292;&#22312;&#22823;&#37327;&#26032;&#38395;&#25968;&#25454;&#30340;&#22522;&#30784;&#19978;&#29983;&#25104;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#24773;&#20917;&#25253;&#21578;&#65292;&#20854;&#20013;&#21253;&#21547;&#22810;&#20010;&#20551;&#35774;&#65288;&#20027;&#24352;&#65289;&#65292;&#24182;&#19982;&#20107;&#23454;&#20381;&#25454;&#24314;&#31435;&#20016;&#23500;&#30340;&#20851;&#32852;&#12290;&#25105;&#20204;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#24773;&#25253;&#20998;&#26512;&#25253;&#21578;&#26469;&#23454;&#29616;SmartBook&#65292;&#20197;&#21327;&#21161;&#19987;&#23478;&#20998;&#26512;&#24072;&#22788;&#29702;&#20044;&#20811;&#20848;-&#20420;&#32599;&#26031;&#21361;&#26426;&#12290;&#26426;&#22120;&#29983;&#25104;&#30340;&#25253;&#21578;&#20197;&#26102;&#38388;&#36724;&#30340;&#24418;&#24335;&#32467;&#26500;&#21270;&#65292;&#27599;&#20010;&#20107;&#20214;&#37117;&#19982;&#30456;&#20851;&#30340;&#28436;&#21592;&#12289;&#20301;&#32622;&#21644;&#22240;&#26524;&#20851;&#31995;&#30456;&#20851;&#32852;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#26174;&#31034;&#65292;SmartBook&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#25253;&#21578;&#26102;&#38388;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#27604;&#20154;&#31867;&#21516;&#34892;&#26356;&#20840;&#38754;&#12289;&#20934;&#30830;&#21644;&#19968;&#33268;&#30340;&#25253;&#21578;&#26469;&#25552;&#39640;&#25253;&#21578;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emerging events, such as the COVID pandemic and the Ukraine Crisis, require a time-sensitive comprehensive understanding of the situation to allow for appropriate decision-making and effective action response. Automated generation of situation reports can significantly reduce the time, effort, and cost for domain experts when preparing their official human-curated reports. However, AI research toward this goal has been very limited, and no successful trials have yet been conducted to automate such report generation. We propose SmartBook, a novel task formulation targeting situation report generation, which consumes large volumes of news data to produce a structured situation report with multiple hypotheses (claims) summarized and grounded with rich links to factual evidence. We realize SmartBook for the Ukraine-Russia crisis by automatically generating intelligence analysis reports to assist expert analysts. The machine-generated reports are structured in the form of timelines, with ea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#21307;&#23398;&#39046;&#22495;&#21033;&#29992;LLaMA&#27169;&#22411;&#24494;&#35843;&#30340;&#21307;&#30103;&#32842;&#22825;&#27169;&#22411;ChatDoctor&#12290;&#32463;&#36807;700&#22810;&#31181;&#30142;&#30149;&#21644;&#20854;&#30456;&#24212;&#30151;&#29366;&#12289;&#33647;&#21697;&#21644;&#21307;&#30103;&#26816;&#26597;&#30340;&#25910;&#38598;&#21644;&#22788;&#29702;&#65292;&#36825;&#31181;&#27169;&#22411;&#20855;&#26377;&#29702;&#35299;&#24739;&#32773;&#38656;&#27714;&#12289;&#25552;&#20379;&#24314;&#35758;&#21644;&#24110;&#21161;&#30340;&#28508;&#21147;&#12290;&#36825;&#20123;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#21040;&#21307;&#30103;&#20445;&#20581;&#20013;&#21487;&#20197;&#26497;&#22823;&#22320;&#25913;&#36827;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#21644;&#24739;&#32773;&#30340;&#27807;&#36890;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2303.14070</link><description>&lt;p&gt;
ChatDoctor&#65306;&#20351;&#29992;&#21307;&#23398;&#39046;&#22495;&#30693;&#35782;&#22312;LLaMA&#27169;&#22411;&#19978;&#24494;&#35843;&#30340;&#21307;&#30103;&#32842;&#22825;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ChatDoctor: A Medical Chat Model Fine-tuned on LLaMA Model using Medical Domain Knowledge. (arXiv:2303.14070v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14070
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#21307;&#23398;&#39046;&#22495;&#21033;&#29992;LLaMA&#27169;&#22411;&#24494;&#35843;&#30340;&#21307;&#30103;&#32842;&#22825;&#27169;&#22411;ChatDoctor&#12290;&#32463;&#36807;700&#22810;&#31181;&#30142;&#30149;&#21644;&#20854;&#30456;&#24212;&#30151;&#29366;&#12289;&#33647;&#21697;&#21644;&#21307;&#30103;&#26816;&#26597;&#30340;&#25910;&#38598;&#21644;&#22788;&#29702;&#65292;&#36825;&#31181;&#27169;&#22411;&#20855;&#26377;&#29702;&#35299;&#24739;&#32773;&#38656;&#27714;&#12289;&#25552;&#20379;&#24314;&#35758;&#21644;&#24110;&#21161;&#30340;&#28508;&#21147;&#12290;&#36825;&#20123;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#21040;&#21307;&#30103;&#20445;&#20581;&#20013;&#21487;&#20197;&#26497;&#22823;&#22320;&#25913;&#36827;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#21644;&#24739;&#32773;&#30340;&#27807;&#36890;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#19968;&#33324;&#39046;&#22495;&#20013;&#24212;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#20363;&#22914;ChatGPT&#65292;&#24050;&#32463;&#34920;&#29616;&#20986;&#20223;&#20315;&#26159;&#20154;&#31867;&#35762;&#35805;&#33324;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#35821;&#35328;&#27169;&#22411;&#24182;&#27809;&#26377;&#32463;&#36807;&#20010;&#21035;&#19988;&#20180;&#32454;&#20026;&#21307;&#23398;&#39046;&#22495;&#23398;&#20064;&#65292;&#23548;&#33268;&#35786;&#26029;&#20934;&#30830;&#24230;&#20302;&#19988;&#19981;&#33021;&#32473;&#20986;&#27491;&#30830;&#30340;&#21307;&#30103;&#35786;&#26029;&#12289;&#33647;&#21697;&#31561;&#24314;&#35758;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;700&#22810;&#31181;&#30142;&#30149;&#21450;&#20854;&#30456;&#24212;&#30151;&#29366;&#12289;&#25512;&#33616;&#33647;&#21697;&#21644;&#25152;&#38656;&#21307;&#30103;&#26816;&#26597;&#65292;&#28982;&#21518;&#29983;&#25104;&#20102;5K&#21517;&#21307;&#24739;&#30340;&#23545;&#35805;&#12290;&#36890;&#36807;&#24494;&#35843;&#21307;&#24739;&#23545;&#35805;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#20102;&#29702;&#35299;&#24739;&#32773;&#38656;&#27714;&#12289;&#25552;&#20379;&#26126;&#26234;&#24314;&#35758;&#24182;&#22312;&#21508;&#31181;&#21307;&#30103;&#30456;&#20851;&#39046;&#22495;&#25552;&#20379;&#23453;&#36149;&#24110;&#21161;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#23558;&#36825;&#20123;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#21040;&#21307;&#30103;&#20445;&#20581;&#20013;&#65292;&#21487;&#20197;&#24443;&#24213;&#25913;&#21464;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#21644;&#24739;&#32773;&#30340;&#27807;&#36890;&#26041;&#24335;&#65292;&#26368;&#32456;&#25913;&#21892;&#25972;&#20307;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent large language models (LLMs) in the general domain, such as ChatGPT, have shown remarkable success in following instructions and producing human-like responses. However, such language models have not been learned individually and carefully for the medical domain, resulting in poor diagnostic accuracy and inability to give correct recommendations for medical diagnosis, medications, etc. To address this issue, we collected more than 700 diseases and their corresponding symptoms, recommended medications, and required medical tests, and then generated 5K doctor-patient conversations. By fine-tuning models of doctor-patient conversations, these models emerge with great potential to understand patients' needs, provide informed advice, and offer valuable assistance in a variety of medical-related fields. The integration of these advanced language models into healthcare can revolutionize the way healthcare professionals and patients communicate, ultimately improving the overall quality 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#22312;DBLP&#23398;&#26415;&#30693;&#35782;&#22270;&#19978;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;10000&#20010;&#38382;&#39064;-&#31572;&#26696;&#23545;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#26159;&#26368;&#22823;&#30340;&#23398;&#26415;&#38382;&#31572;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2303.13351</link><description>&lt;p&gt;
DBLP-QuAD&#65306;DBLP&#23398;&#26415;&#30693;&#35782;&#22270;&#19978;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly Knowledge Graph. (arXiv:2303.13351v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13351
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#22312;DBLP&#23398;&#26415;&#30693;&#35782;&#22270;&#19978;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;10000&#20010;&#38382;&#39064;-&#31572;&#26696;&#23545;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#26159;&#26368;&#22823;&#30340;&#23398;&#26415;&#38382;&#31572;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;DBLP&#23398;&#26415;&#30693;&#35782;&#22270;&#19978;&#21019;&#24314;&#20102;&#19968;&#20010;&#38382;&#31572;&#25968;&#25454;&#38598;&#12290;DBLP&#26159;&#19968;&#20010;&#22312;&#32447;&#35745;&#31639;&#26426;&#31185;&#23398;&#20027;&#35201;&#20986;&#29256;&#29289;&#30340;&#21442;&#32771;&#25991;&#29486;&#20449;&#24687;&#32034;&#24341;&#65292;&#32034;&#24341;&#20102;&#36229;&#36807;440&#19975;&#31687;&#35770;&#25991;&#65292;&#30001;220&#19975;&#22810;&#20301;&#20316;&#32773;&#21457;&#34920;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;10000&#20010;&#38382;&#39064;-&#31572;&#26696;&#23545;&#20197;&#21450;&#30456;&#24212;&#30340;SPARQL&#26597;&#35810;&#65292;&#21487;&#20197;&#22312;DBLP KG&#19978;&#25191;&#34892;&#20197;&#33719;&#24471;&#27491;&#30830;&#30340;&#31572;&#26696;&#12290;DBLP-QuAD&#26159;&#26368;&#22823;&#30340;&#23398;&#26415;&#38382;&#31572;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we create a question answering dataset over the DBLP scholarly knowledge graph (KG). DBLP is an on-line reference for bibliographic information on major computer science publications that indexes over 4.4 million publications published by more than 2.2 million authors. Our dataset consists of 10,000 question answer pairs with the corresponding SPARQL queries which can be executed over the DBLP KG to fetch the correct answer. DBLP-QuAD is the largest scholarly question answering dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;GETT-QA&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#20351;&#29992;T5&#23545;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#29983;&#25104;&#31616;&#21270;&#30340;SPARQL&#26597;&#35810;&#65292;&#24182;&#20351;&#29992;&#25130;&#26029;&#30340;KG&#23884;&#20837;&#25552;&#39640;&#20102;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.13284</link><description>&lt;p&gt;
GETT-QA&#65306;&#22522;&#20110;&#22270;&#23884;&#20837;&#30340;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#20013;&#30340;T2T Transformer
&lt;/p&gt;
&lt;p&gt;
GETT-QA: Graph Embedding based T2T Transformer for Knowledge Graph Question Answering. (arXiv:2303.13284v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;GETT-QA&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#20351;&#29992;T5&#23545;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#29983;&#25104;&#31616;&#21270;&#30340;SPARQL&#26597;&#35810;&#65292;&#24182;&#20351;&#29992;&#25130;&#26029;&#30340;KG&#23884;&#20837;&#25552;&#39640;&#20102;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GETT-QA&#30340;&#31471;&#21040;&#31471;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#31995;&#32479;&#12290;GETT-QA&#20351;&#29992;&#20102;T5&#65292;&#36825;&#26159;&#19968;&#31181;&#28909;&#38376;&#30340;&#25991;&#26412;&#21040;&#25991;&#26412;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#20197;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#30340;&#38382;&#39064;&#20316;&#20026;&#36755;&#20837;&#24182;&#29983;&#25104;&#25152;&#38656;SPARQL&#26597;&#35810;&#30340;&#31616;&#21270;&#24418;&#24335;&#12290;&#22312;&#31616;&#21270;&#24418;&#24335;&#20013;&#65292;&#27169;&#22411;&#19981;&#30452;&#25509;&#29983;&#25104;&#23454;&#20307;&#21644;&#20851;&#31995;ID&#65292;&#32780;&#26159;&#20135;&#29983;&#30456;&#24212;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#26631;&#31614;&#12290;&#26631;&#31614;&#22312;&#38543;&#21518;&#30340;&#27493;&#39588;&#20013;&#19982;KG&#23454;&#20307;&#21644;&#20851;&#31995;ID&#32852;&#31995;&#36215;&#26469;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25913;&#36827;&#32467;&#26524;&#65292;&#25105;&#20204;&#25351;&#23548;&#27169;&#22411;&#20026;&#27599;&#20010;&#23454;&#20307;&#29983;&#25104;KG&#23884;&#20837;&#30340;&#25130;&#26029;&#29256;&#26412;&#12290;&#25130;&#26029;&#30340;KG&#23884;&#20837;&#20351;&#24471;&#26356;&#31934;&#32454;&#30340;&#25628;&#32034;&#20174;&#32780;&#26356;&#26377;&#25928;&#36827;&#34892;&#28040;&#27495;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;T5&#33021;&#22815;&#22312;&#19981;&#25913;&#21464;&#25439;&#22833;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#25130;&#26029;&#30340;KG&#23884;&#20837;&#65292;&#25552;&#39640;&#20102;KGQA&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;Wikidata&#30340;LC-QuAD 2.0&#21644;SimpleQuestions-Wikidata&#25968;&#25454;&#38598;&#19978;&#25253;&#21578;&#20102;&#31471;&#21040;&#31471;KGQA&#30340;&#24378;&#22823;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we present an end-to-end Knowledge Graph Question Answering (KGQA) system named GETT-QA. GETT-QA uses T5, a popular text-to-text pre-trained language model. The model takes a question in natural language as input and produces a simpler form of the intended SPARQL query. In the simpler form, the model does not directly produce entity and relation IDs. Instead, it produces corresponding entity and relation labels. The labels are grounded to KG entity and relation IDs in a subsequent step. To further improve the results, we instruct the model to produce a truncated version of the KG embedding for each entity. The truncated KG embedding enables a finer search for disambiguation purposes. We find that T5 is able to learn the truncated KG embeddings without any change of loss function, improving KGQA performance. As a result, we report strong results for LC-QuAD 2.0 and SimpleQuestions-Wikidata datasets on end-to-end KGQA over Wikidata.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#20803;&#25552;&#31034;&#23398;&#20064;&#26694;&#26550;SUPMER&#65292;&#21253;&#25324;&#20803;&#26799;&#24230;&#27491;&#21017;&#21270;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#27867;&#21270;&#65292;&#36890;&#36807;&#38170;&#23450;&#30340;&#20803;&#35757;&#32451;&#20219;&#21153;&#21644;&#22522;&#20110;&#35838;&#31243;&#30340;&#20219;&#21153;&#22686;&#24378;&#20016;&#23500;&#20102;&#20219;&#21153;&#20998;&#24067;&#65292;&#35299;&#20915;&#20102;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#33391;&#22909;&#21021;&#22987;&#21270;&#36719;&#25552;&#31034;&#21644;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.12314</link><description>&lt;p&gt;
&#20855;&#26377;&#20803;&#26799;&#24230;&#27491;&#21017;&#21270;&#30340;&#33258;&#30417;&#30563;&#20803;&#25552;&#31034;&#23398;&#20064;&#29992;&#20110;&#23569;&#26679;&#26412;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Meta-Prompt Learning with Meta-Gradient Regularization for Few-shot Generalization. (arXiv:2303.12314v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12314
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#20803;&#25552;&#31034;&#23398;&#20064;&#26694;&#26550;SUPMER&#65292;&#21253;&#25324;&#20803;&#26799;&#24230;&#27491;&#21017;&#21270;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#27867;&#21270;&#65292;&#36890;&#36807;&#38170;&#23450;&#30340;&#20803;&#35757;&#32451;&#20219;&#21153;&#21644;&#22522;&#20110;&#35838;&#31243;&#30340;&#20219;&#21153;&#22686;&#24378;&#20016;&#23500;&#20102;&#20219;&#21153;&#20998;&#24067;&#65292;&#35299;&#20915;&#20102;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#33391;&#22909;&#21021;&#22987;&#21270;&#36719;&#25552;&#31034;&#21644;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#35843;&#25972;&#26159;&#19968;&#31181;&#21442;&#25968;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23427;&#23398;&#20064;&#36719;&#25552;&#31034;&#24182;&#20351;&#20923;&#32467;&#30340;&#35821;&#35328;&#27169;&#22411;&#25191;&#34892;&#29305;&#23450;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#23613;&#31649;&#26377;&#25928;&#65292;&#20294;&#25552;&#31034;&#35843;&#25972;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#19968;&#26041;&#38754;&#20005;&#37325;&#20381;&#36182;&#20110;&#33391;&#22909;&#30340;&#36719;&#25552;&#31034;&#21021;&#22987;&#21270;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23427;&#24456;&#23481;&#26131;&#23548;&#33268;&#36807;&#24230;&#25311;&#21512;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#25110;&#30417;&#30563;&#20803;&#23398;&#20064;&#26469;&#21021;&#22987;&#21270;&#36719;&#25552;&#31034;&#65292;&#20294;&#23427;&#20204;&#19981;&#33021;&#23545;&#26410;&#35265;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#25968;&#25454;&#26377;&#25928;&#30340;&#27867;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#20197;&#19978;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#25105;&#30417;&#30563;&#20803;&#25552;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#20803;&#26799;&#24230;&#27491;&#21017;&#21270;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#27867;&#21270;&#65288;SUPMER&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#32452;&#33258;&#30417;&#30563;&#38170;&#23450;&#30340;&#20803;&#35757;&#32451;&#20219;&#21153;&#65292;&#20855;&#26377;&#19981;&#21516;&#30340;&#20219;&#21153;&#26684;&#24335;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#35838;&#31243;&#30340;&#20219;&#21153;&#22686;&#24378;&#36827;&#19968;&#27493;&#20016;&#23500;&#20102;&#20219;&#21153;&#20998;&#24067;&#12290;&#28982;&#21518;&#23558;&#19968;&#31181;&#26032;&#30340;&#20803;&#26799;&#24230;&#27491;&#21017;&#21270;&#26041;&#27861;&#38598;&#25104;&#21040;&#20803;&#25552;&#31034;&#23398;&#20064;&#20013;&#12290;&#23427;&#20803;&#23398;&#20064;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#22914;&#20309;&#36716;&#25442;&#21407;&#22987;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt tuning is a parameter-efficient method, which learns soft prompts and conditions frozen language models to perform specific downstream tasks. Though effective, prompt tuning under few-shot settings on the one hand heavily relies on a good initialization of soft prompts. On the other hand, it can easily result in overfitting. Existing works leverage pre-training or supervised meta-learning to initialize soft prompts but they cannot data-efficiently generalize to unseen downstream tasks. To address the above problems, this paper proposes a novel Self-sUpervised meta-Prompt learning framework with meta-gradient Regularization for few-shot generalization (SUPMER). We first design a set of self-supervised anchor meta-training tasks with different task formats and further enrich the task distribution with curriculum-based task augmentation. Then a novel meta-gradient regularization method is integrated into meta-prompt learning. It meta-learns to transform the raw gradients during few
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#30340;&#26041;&#35328;&#35782;&#21035;&#31995;&#32479;&#65292;&#22312;VarDial 2023&#20013;&#36229;&#36234;&#20854;&#20182;&#21442;&#19982;&#32773;&#30340;&#31995;&#32479;&#65292;&#23545;&#22810;&#35821;&#35328;&#26041;&#35328;&#26816;&#27979;&#26377;&#37325;&#35201;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2303.03487</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#26041;&#35328;&#26816;&#27979;&#30340;&#20004;&#38454;&#27573;&#27969;&#27700;&#32447;
&lt;/p&gt;
&lt;p&gt;
Two-stage Pipeline for Multilingual Dialect Detection. (arXiv:2303.03487v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03487
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#30340;&#26041;&#35328;&#35782;&#21035;&#31995;&#32479;&#65292;&#22312;VarDial 2023&#20013;&#36229;&#36234;&#20854;&#20182;&#21442;&#19982;&#32773;&#30340;&#31995;&#32479;&#65292;&#23545;&#22810;&#35821;&#35328;&#26041;&#35328;&#26816;&#27979;&#26377;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26041;&#35328;&#35782;&#21035;&#23545;&#20110;&#26412;&#22320;&#21270;&#21508;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#25105;&#20204;&#22312;VarDial 2023&#20849;&#20139;&#20219;&#21153;&#20013;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24517;&#39035;&#20174;&#19977;&#31181;&#35821;&#35328;&#20013;&#35782;&#21035;&#20986;&#19977;&#20010;&#25110;&#20004;&#20010;&#26041;&#35328;&#65292;&#36825;&#23548;&#33268;&#20102;Track-1&#30340;9&#36335;&#20998;&#31867;&#21644;Track-2&#30340;6&#36335;&#20998;&#31867;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#30340;&#31995;&#32479;&#65292;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#36229;&#36234;&#20102;&#20854;&#20182;&#21442;&#19982;&#32773;&#30340;&#31995;&#32479;&#21644;&#20197;&#21069;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#22312;Track-1&#21644;Track-2&#19978;&#20998;&#21035;&#33719;&#24471;58.54&#65285;&#21644;85.61&#65285;&#30340;&#24471;&#20998;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#24211;&#26159;&#20844;&#24320;&#30340;&#65288;https://github.com/ankit-vaidya19/EACL_VarDial2023&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialect Identification is a crucial task for localizing various Large Language Models. This paper outlines our approach to the VarDial 2023 shared task. Here we have to identify three or two dialects from three languages each which results in a 9-way classification for Track-1 and 6-way classification for Track-2 respectively. Our proposed approach consists of a two-stage system and outperforms other participants' systems and previous works in this domain. We achieve a score of 58.54% for Track-1 and 85.61% for Track-2. Our codebase is available publicly (https://github.com/ankit-vaidya19/EACL_VarDial2023).
&lt;/p&gt;</description></item><item><title>EvoPrompting&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#33258;&#36866;&#24212;&#21464;&#24322;&#21644;&#20132;&#21449;&#25805;&#20316;&#31526;&#26469;&#36827;&#34892;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65292;&#22312;MNIST-1D&#25968;&#25454;&#38598;&#21644;CLRS&#31639;&#27861;&#25512;&#29702;&#22522;&#20934;&#19978;&#37117;&#21462;&#24471;&#20102;&#27604;&#20154;&#31867;&#35774;&#35745;&#30340;&#26550;&#26500;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2302.14838</link><description>&lt;p&gt;
EvoPrompting: &#36866;&#29992;&#20110;&#20195;&#30721;&#32423;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
EvoPrompting: Language Models for Code-Level Neural Architecture Search. (arXiv:2302.14838v1 [cs.NE] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14838
&lt;/p&gt;
&lt;p&gt;
EvoPrompting&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#33258;&#36866;&#24212;&#21464;&#24322;&#21644;&#20132;&#21449;&#25805;&#20316;&#31526;&#26469;&#36827;&#34892;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65292;&#22312;MNIST-1D&#25968;&#25454;&#38598;&#21644;CLRS&#31639;&#27861;&#25512;&#29702;&#22522;&#20934;&#19978;&#37117;&#21462;&#24471;&#20102;&#27604;&#20154;&#31867;&#35774;&#35745;&#30340;&#26550;&#26500;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#30340;&#26368;&#26032;&#25104;&#23601;&#65292;&#25105;&#20204;&#25506;&#32034;&#23558;LM&#20316;&#20026;&#36827;&#21270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#31639;&#27861;&#30340;&#33258;&#36866;&#24212;&#21464;&#24322;&#21644;&#20132;&#21449;&#25805;&#20316;&#31526;&#30340;&#20351;&#29992;&#12290;&#23613;&#31649;NAS&#20173;&#28982;&#36807;&#20110;&#22256;&#38590;&#65292;&#20197;&#33267;&#20110;&#20165;&#20165;&#36890;&#36807;&#25552;&#31034;&#23601;&#38590;&#20197;&#25104;&#21151;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#36827;&#21270;&#25552;&#31034;&#24037;&#31243;&#19982;&#36719;&#25552;&#31034;&#35843;&#25972;&#30340;&#32452;&#21512;&#65292;&#19968;&#31181;&#25105;&#20204;&#31216;&#20043;&#20026;EvoPrompting&#30340;&#26041;&#27861;&#65292;&#22987;&#32456;&#21487;&#20197;&#21457;&#29616;&#22810;&#26679;&#21270;&#19988;&#24615;&#33021;&#39640;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;EvoPrompting&#22312;MNIST-1D&#25968;&#25454;&#38598;&#19978;&#26159;&#26377;&#25928;&#30340;&#65292;&#20854;&#20013;EvoPrompting&#20135;&#29983;&#30340;&#21367;&#31215;&#26550;&#26500;&#21464;&#20307;&#22312;&#20934;&#30830;&#29575;&#21644;&#27169;&#22411;&#22823;&#23567;&#26041;&#38754;&#22343;&#20248;&#20110;&#20154;&#31867;&#19987;&#23478;&#35774;&#35745;&#30340;&#26550;&#26500;&#21644;&#22825;&#30495;&#30340;&#23569;&#25968;&#20808;&#23548;&#25552;&#31034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#22312;CLRS&#31639;&#27861;&#25512;&#29702;&#22522;&#20934;&#19978;&#25628;&#32034;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#20013;EvoPrompting&#33021;&#22815;&#35774;&#35745;&#20986;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#26356;&#22909;&#30340;&#26032;&#39062;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the recent impressive accomplishments of language models (LMs) for code generation, we explore the use of LMs as adaptive mutation and crossover operators for an evolutionary neural architecture search (NAS) algorithm. While NAS still proves too difficult a task for LMs to succeed at solely through prompting, we find that the combination of evolutionary prompt engineering with soft prompt-tuning, a method we term EvoPrompting, consistently finds diverse and high performing models. We first demonstrate that EvoPrompting is effective on the computationally efficient MNIST-1D dataset, where EvoPrompting produces convolutional architecture variants that outperform both those designed by human experts and naive few-shot prompting in terms of accuracy and model size. We then apply our method to searching for graph neural networks on the CLRS Algorithmic Reasoning Benchmark, where EvoPrompting is able to design novel architectures that outperform current state-of-the-art models on 21 ou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;AdapterSoup&#65292;&#19968;&#31181;&#20351;&#29992;&#21152;&#26435;&#24179;&#22343;&#25913;&#21892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#39046;&#22495;&#35757;&#32451;&#30340;&#36866;&#37197;&#22120;&#19978;&#25191;&#34892;&#26435;&#37325;&#31354;&#38388;&#24179;&#22343;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#39069;&#22806;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#23545;&#26032;&#39046;&#22495;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.07027</link><description>&lt;p&gt;
AdapterSoup&#65306;&#20351;&#29992;&#21152;&#26435;&#24179;&#22343;&#25913;&#21892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
AdapterSoup: Weight Averaging to Improve Generalization of Pretrained Language Models. (arXiv:2302.07027v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;AdapterSoup&#65292;&#19968;&#31181;&#20351;&#29992;&#21152;&#26435;&#24179;&#22343;&#25913;&#21892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#39046;&#22495;&#35757;&#32451;&#30340;&#36866;&#37197;&#22120;&#19978;&#25191;&#34892;&#26435;&#37325;&#31354;&#38388;&#24179;&#22343;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#39069;&#22806;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#23545;&#26032;&#39046;&#22495;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
AdapterSoup is a method that uses weight averaging to improve the generalization ability of pretrained language models. It performs weight-space averaging of adapters trained on different domains, and can improve performance to new domains without extra training.
&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#19987;&#38376;&#38024;&#23545;&#29305;&#23450;&#39046;&#22495;&#36827;&#34892;&#29305;&#21270;&#12290;&#19968;&#31181;&#21442;&#25968;&#26377;&#25928;&#30340;&#36866;&#24212;&#26041;&#27861;&#24314;&#35758;&#22312;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#19978;&#20026;&#27599;&#20010;&#39046;&#22495;&#35757;&#32451;&#19968;&#20010;&#36866;&#37197;&#22120;&#12290;&#36825;&#23548;&#33268;&#20102;&#33391;&#22909;&#30340;&#39046;&#22495;&#20869;&#24471;&#20998;&#65292;&#20294;&#22312;&#39046;&#22495;&#25110;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#21487;&#33021;&#19981;&#20999;&#23454;&#38469;&#12290;&#35299;&#20915;&#26041;&#26696;&#26159;&#22312;&#27979;&#35797;&#26102;&#20351;&#29992;&#30456;&#20851;&#39046;&#22495;&#36866;&#37197;&#22120;&#26469;&#22788;&#29702;&#26032;&#39046;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AdapterSoup&#65292;&#19968;&#31181;&#22312;&#19981;&#21516;&#39046;&#22495;&#35757;&#32451;&#30340;&#36866;&#37197;&#22120;&#19978;&#25191;&#34892;&#26435;&#37325;&#31354;&#38388;&#24179;&#22343;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#20196;&#20154;&#23604;&#23596;&#30340;&#24182;&#34892;&#30340;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#35757;&#32451;&#19968;&#32452;&#29305;&#23450;&#39046;&#22495;&#30340;&#36866;&#37197;&#22120;&#65307;&#28982;&#21518;&#65292;&#23545;&#20110;&#27599;&#20010;&#26032;&#39046;&#22495;&#65292;&#25105;&#20204;&#30830;&#23450;&#22312;&#27979;&#35797;&#26102;&#24212;&#24179;&#22343;&#21738;&#20123;&#36866;&#37197;&#22120;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#34920;&#26126;AdapterSoup&#22987;&#32456;&#25552;&#39640;&#20102;&#23545;&#26032;&#39046;&#22495;&#30340;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#22312;&#19981;&#21516;&#36229;&#21442;&#25968;&#19979;&#35757;&#32451;&#30340;&#30456;&#21516;&#39046;&#22495;&#36866;&#37197;&#22120;&#30340;&#26435;&#37325;&#24179;&#22343;&#65292;&#24182;&#34920;&#26126;&#23427;&#21487;&#20197;&#20445;&#30041;
&lt;/p&gt;
&lt;p&gt;
Pretrained language models (PLMs) are trained on massive corpora, but often need to specialize to specific domains. A parameter-efficient adaptation method suggests training an adapter for each domain on the task of language modeling. This leads to good in-domain scores but can be impractical for domain- or resource-restricted settings. A solution is to use a related-domain adapter for the novel domain at test time. In this paper, we introduce AdapterSoup, an approach that performs weight-space averaging of adapters trained on different domains. Our approach is embarrassingly parallel: first, we train a set of domain-specific adapters; then, for each novel domain, we determine which adapters should be averaged at test time. We present extensive experiments showing that AdapterSoup consistently improves performance to new domains without extra training. We also explore weight averaging of adapters trained on the same domain with different hyper-parameters, and show that it preserves the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25991;&#26412;&#35821;&#20041;&#30456;&#20284;&#24615;&#39044;&#27979;&#29992;&#25143;&#35780;&#20998;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.00412</link><description>&lt;p&gt;
&#35821;&#20041;&#32534;&#30721;&#30340;KNN&#29992;&#20110;&#35780;&#20998;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
KNNs of Semantic Encodings for Rating Prediction. (arXiv:2302.00412v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00412
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25991;&#26412;&#35821;&#20041;&#30456;&#20284;&#24615;&#39044;&#27979;&#29992;&#25143;&#35780;&#20998;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#25991;&#26412;&#35821;&#20041;&#30456;&#20284;&#24615;&#22312;&#29992;&#25143;&#35780;&#20998;&#39044;&#27979;&#20013;&#30340;&#21019;&#26032;&#24212;&#29992;&#12290;&#35813;&#26041;&#27861;&#34920;&#31034;&#29992;&#25143;&#20559;&#22909;&#20026;&#26469;&#33258;&#35780;&#35770;&#25991;&#26412;&#30340;&#25991;&#26412;&#29255;&#27573;&#30340;&#22270;&#24418;&#65292;&#20854;&#20013;&#36793;&#32536;&#26681;&#25454;&#35821;&#20041;&#30456;&#20284;&#24230;&#23450;&#20041;&#12290;&#36825;&#31181;&#22522;&#20110;&#25991;&#26412;&#30340;&#35760;&#24518;&#26041;&#27861;&#21487;&#20197;&#20026;&#25512;&#33616;&#25552;&#20379;&#22522;&#20110;&#35780;&#35770;&#30340;&#35299;&#37322;&#12290;&#26041;&#27861;&#32463;&#36807;&#37327;&#21270;&#35780;&#20272;&#65292;&#31361;&#20986;&#20102;&#20197;&#36825;&#31181;&#26041;&#24335;&#21033;&#29992;&#25991;&#26412;&#20248;&#20110;&#24378;&#35760;&#24518;&#21644;&#27169;&#22411;&#21512;&#20316;&#36807;&#28388;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores a novel application of textual semantic similarity to user-preference representation for rating prediction. The approach represents a user's preferences as a graph of textual snippets from review text, where the edges are defined by semantic similarity. This textual, memory-based approach to rating prediction enables review-based explanations for recommendations. The method is evaluated quantitatively, highlighting that leveraging text in this way outperforms both strong memory-based and model-based collaborative filtering baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#23545;&#35805;&#36741;&#23548;&#23384;&#22312;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#65292;&#21457;&#29616;&#24403;&#21069;&#26041;&#27861;&#22312;&#23569;&#37327;&#27010;&#24565;&#21644;&#21487;&#33021;&#30340;&#25945;&#24072;&#31574;&#30053;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#36827;&#34892;&#36739;&#22909;&#30340;&#36741;&#23548;&#27169;&#25311;&#19982;&#23398;&#20064;&#65292;&#20294;&#22312;&#19981;&#21463;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#19981;&#20339;&#65292;&#26410;&#26469;&#24212;&#35813;&#38598;&#20013;&#22312;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#19978;&#12290;</title><link>http://arxiv.org/abs/2301.09919</link><description>&lt;p&gt;
&#31070;&#32463;&#23545;&#35805;&#36741;&#23548;&#20013;&#30340;&#26426;&#36935;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Opportunities and Challenges in Neural Dialog Tutoring. (arXiv:2301.09919v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09919
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#23545;&#35805;&#36741;&#23548;&#23384;&#22312;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#65292;&#21457;&#29616;&#24403;&#21069;&#26041;&#27861;&#22312;&#23569;&#37327;&#27010;&#24565;&#21644;&#21487;&#33021;&#30340;&#25945;&#24072;&#31574;&#30053;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#36827;&#34892;&#36739;&#22909;&#30340;&#36741;&#23548;&#27169;&#25311;&#19982;&#23398;&#20064;&#65292;&#20294;&#22312;&#19981;&#21463;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#19981;&#20339;&#65292;&#26410;&#26469;&#24212;&#35813;&#38598;&#20013;&#22312;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#23545;&#35805;&#36741;&#23548;&#31995;&#32479;&#19968;&#30452;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24037;&#20316;&#65292;&#22240;&#20026;&#23427;&#28041;&#21450;&#21040;&#23545;&#20154;&#31867;&#36741;&#23548;&#32773;&#25152;&#37319;&#29992;&#30340;&#22810;&#26679;&#19988;&#22797;&#26434;&#30340;&#25945;&#23398;&#31574;&#30053;&#36827;&#34892;&#24314;&#27169;&#12290;&#23613;&#31649;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#21644;&#21487;&#29992;&#30340;&#23545;&#35805;&#35821;&#26009;&#24211;&#26041;&#38754;&#20986;&#29616;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#23545;&#35805;&#36741;&#23548;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20173;&#26410;&#21463;&#21040;&#36825;&#20123;&#36827;&#23637;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#22312;&#20004;&#20010;&#35821;&#35328;&#23398;&#20064;&#23545;&#35805;&#36741;&#23548;&#25968;&#25454;&#38598;&#19978;&#23545;&#21508;&#31181;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20005;&#26684;&#20998;&#26512;&#65292;&#20351;&#29992;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#26469;&#20102;&#35299;&#36825;&#20123;&#36827;&#23637;&#24102;&#26469;&#30340;&#26032;&#26426;&#20250;&#20197;&#21450;&#25105;&#20204;&#24517;&#39035;&#20811;&#26381;&#30340;&#25361;&#25112;&#65292;&#20197;&#26500;&#24314;&#33021;&#22312;&#30495;&#23454;&#25945;&#32946;&#29615;&#22659;&#20013;&#20351;&#29992;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#24403;&#21069;&#26041;&#27861;&#21487;&#20197;&#23545;&#23569;&#37327;&#27010;&#24565;&#21644;&#21487;&#33021;&#30340;&#25945;&#24072;&#31574;&#30053;&#36827;&#34892;&#36739;&#22909;&#30340;&#36741;&#23548;&#27169;&#25311;&#19982;&#23398;&#20064;&#65292;&#20294;&#22312;&#19981;&#21463;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#30340;&#20154;&#24037;&#36136;&#37327;&#35780;&#20272;&#26174;&#31034;&#65292;&#27169;&#22411;&#21644;&#22522;&#30784;&#25945;&#23398;&#31995;&#32479;&#22312;&#36825;&#20123;&#19981;&#21463;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#37117;&#20855;&#26377;&#36739;&#20302;&#30340;&#26377;&#25928;&#24615;&#65292;&#36825;&#34920;&#26126;&#26410;&#26469;&#30340;&#30740;&#31350;&#24212;&#35813;&#38598;&#20013;&#22312;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing dialog tutors has been challenging as it involves modeling the diverse and complex pedagogical strategies employed by human tutors. Although there have been significant recent advances in neural conversational systems using large language models (LLMs) and growth in available dialog corpora, dialog tutoring has largely remained unaffected by these advances. In this paper, we rigorously analyze various generative language models on two dialog tutoring datasets for language learning using automatic and human evaluations to understand the new opportunities brought by these advances as well as the challenges we must overcome to build models that would be usable in real educational settings. We find that although current approaches can model tutoring in constrained learning scenarios when the number of concepts to be taught and possible teacher strategies are small, they perform poorly in less constrained scenarios. Our human quality evaluation shows that both models and ground-tr
&lt;/p&gt;</description></item><item><title>TextDescriptives&#26159;&#19968;&#20010;Python&#21253;&#65292;&#21487;&#29992;&#20110;&#20174;&#25991;&#26412;&#20013;&#35745;&#31639;&#22810;&#31181;&#25351;&#26631;&#65292;&#24050;&#34987;&#29992;&#20110;&#20998;&#26512;&#20020;&#24202;&#25991;&#26412;&#30340;&#35821;&#35328;&#31283;&#23450;&#24615;&#12289;&#39044;&#27979;&#31070;&#32463;&#31934;&#31070;&#30142;&#30149;&#30340;&#29305;&#24449;&#20197;&#21450;&#20998;&#26512;&#23567;&#23398;&#29983;&#25104;&#35821;&#35328;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2301.02057</link><description>&lt;p&gt;
TextDescriptives&#65306;&#19968;&#20010;&#29992;&#20110;&#20174;&#25991;&#26412;&#20013;&#35745;&#31639;&#22810;&#31181;&#25351;&#26631;&#30340;Python&#21253;
&lt;/p&gt;
&lt;p&gt;
TextDescriptives: A Python package for calculating a large variety of metrics from text. (arXiv:2301.02057v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.02057
&lt;/p&gt;
&lt;p&gt;
TextDescriptives&#26159;&#19968;&#20010;Python&#21253;&#65292;&#21487;&#29992;&#20110;&#20174;&#25991;&#26412;&#20013;&#35745;&#31639;&#22810;&#31181;&#25351;&#26631;&#65292;&#24050;&#34987;&#29992;&#20110;&#20998;&#26512;&#20020;&#24202;&#25991;&#26412;&#30340;&#35821;&#35328;&#31283;&#23450;&#24615;&#12289;&#39044;&#27979;&#31070;&#32463;&#31934;&#31070;&#30142;&#30149;&#30340;&#29305;&#24449;&#20197;&#21450;&#20998;&#26512;&#23567;&#23398;&#29983;&#25104;&#35821;&#35328;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
TextDescriptives&#26159;&#19968;&#20010;Python&#21253;&#65292;&#29992;&#20110;&#35745;&#31639;&#20174;&#25991;&#26412;&#20013;&#33719;&#21462;&#30340;&#22810;&#31181;&#24230;&#37327;&#26631;&#20934;&#12290;&#23427;&#24314;&#31435;&#22312;spaCy&#20043;&#19978;&#65292;&#24182;&#21487;&#20197;&#36731;&#26494;&#38598;&#25104;&#21040;&#29616;&#26377;&#24037;&#20316;&#27969;&#31243;&#20013;&#12290;&#35813;&#21253;&#24050;&#34987;&#29992;&#20110;&#20998;&#26512;&#20020;&#24202;&#25991;&#26412;&#30340;&#35821;&#35328;&#31283;&#23450;&#24615;&#65292;&#21019;&#24314;&#29992;&#20110;&#39044;&#27979;&#31070;&#32463;&#31934;&#31070;&#30142;&#30149;&#30340;&#29305;&#24449;&#20197;&#21450;&#20998;&#26512;&#23567;&#23398;&#29983;&#35821;&#35328;&#30446;&#26631;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#35813;&#21253;&#21450;&#20854;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
TextDescriptives is a Python package for calculating a large variety of metrics from text. It is built on top of spaCy and can be easily integrated into existing workflows. The package has already been used for analysing the linguistic stability of clinical texts, creating features for predicting neuropsychiatric conditions, and analysing linguistic goals of primary school students. This paper describes the package and its features.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25512;&#26029;&#23545;&#35805;&#20013;&#26368;&#21518;&#19968;&#27425;&#21457;&#35328;&#26469;&#25429;&#25417;&#35828;&#35805;&#32773;&#30340;&#24847;&#22270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#24847;&#22270;&#34701;&#21512;&#27169;&#22359;&#30340;&#20849;&#24773;&#23545;&#35805;&#29983;&#25104;&#27169;&#22411;InferEM&#12290;&#27169;&#22411;&#21516;&#26102;&#21033;&#29992;&#21069;&#20960;&#27425;&#21457;&#35328;&#39044;&#27979;&#26368;&#21518;&#19968;&#27425;&#21457;&#35328;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.06373</link><description>&lt;p&gt;
InferEM: &#25512;&#26029;&#35828;&#35805;&#32773;&#24847;&#22270;&#30340;&#20849;&#24773;&#23545;&#35805;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
InferEM: Inferring the Speaker's Intention for Empathetic Dialogue Generation. (arXiv:2212.06373v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06373
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25512;&#26029;&#23545;&#35805;&#20013;&#26368;&#21518;&#19968;&#27425;&#21457;&#35328;&#26469;&#25429;&#25417;&#35828;&#35805;&#32773;&#30340;&#24847;&#22270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#24847;&#22270;&#34701;&#21512;&#27169;&#22359;&#30340;&#20849;&#24773;&#23545;&#35805;&#29983;&#25104;&#27169;&#22411;InferEM&#12290;&#27169;&#22411;&#21516;&#26102;&#21033;&#29992;&#21069;&#20960;&#27425;&#21457;&#35328;&#39044;&#27979;&#26368;&#21518;&#19968;&#27425;&#21457;&#35328;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#20849;&#24773;&#22238;&#22797;&#29983;&#25104;&#30340;&#26041;&#27861;&#19968;&#33324;&#30452;&#25509;&#32534;&#30721;&#25972;&#20010;&#23545;&#35805;&#21382;&#21490;&#65292;&#28982;&#21518;&#36890;&#36807;&#35299;&#30721;&#22120;&#29983;&#25104;&#21451;&#22909;&#30340;&#21453;&#39304;&#12290;&#36825;&#20123;&#26041;&#27861;&#24378;&#35843;&#24314;&#27169;&#24773;&#22659;&#20449;&#24687;&#65292;&#20294;&#24573;&#35270;&#20102;&#25429;&#25417;&#35828;&#35805;&#32773;&#30340;&#30452;&#25509;&#24847;&#22270;&#12290;&#25105;&#20204;&#35748;&#20026;&#23545;&#35805;&#20013;&#26368;&#21518;&#19968;&#27425;&#21457;&#35328;&#34920;&#36798;&#20102;&#35828;&#35805;&#32773;&#30340;&#24847;&#22270;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InferEM&#30340;&#26032;&#27169;&#22411;&#29992;&#20110;&#20849;&#24773;&#22238;&#22797;&#29983;&#25104;&#12290;&#25105;&#20204;&#23558;&#26368;&#21518;&#19968;&#27425;&#21457;&#35328;&#21333;&#29420;&#32534;&#30721;&#65292;&#36890;&#36807;&#22522;&#20110;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#24847;&#22270;&#34701;&#21512;&#27169;&#22359;&#19982;&#25972;&#20010;&#23545;&#35805;&#34701;&#21512;&#20197;&#25429;&#25417;&#35828;&#35805;&#32773;&#30340;&#24847;&#22270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#21069;&#20960;&#27425;&#21457;&#35328;&#39044;&#27979;&#26368;&#21518;&#19968;&#27425;&#21457;&#35328;&#65292;&#20197;&#27169;&#25311;&#20154;&#31867;&#30340;&#24515;&#29702;&#65292;&#29468;&#27979;&#23545;&#35805;&#32773;&#21487;&#33021;&#25552;&#21069;&#35828;&#20123;&#20160;&#20040;&#12290;&#20026;&#24179;&#34913;&#21457;&#35328;&#39044;&#27979;&#21644;&#22238;&#22797;&#29983;&#25104;&#30340;&#20248;&#21270;&#36895;&#29575;&#65292;InferEM&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#23398;&#20064;&#31574;&#30053;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current approaches to empathetic response generation typically encode the entire dialogue history directly and put the output into a decoder to generate friendly feedback. These methods focus on modelling contextual information but neglect capturing the direct intention of the speaker. We argue that the last utterance in the dialogue empirically conveys the intention of the speaker. Consequently, we propose a novel model named InferEM for empathetic response generation. We separately encode the last utterance and fuse it with the entire dialogue through the multi-head attention based intention fusion module to capture the speaker's intention. Besides, we utilize previous utterances to predict the last utterance, which simulates human's psychology to guess what the interlocutor may speak in advance. To balance the optimizing rates of the utterance prediction and response generation, a multi-task learning strategy is designed for InferEM. Experimental results demonstrate the plausibility
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;PromptCap&#65292;&#19968;&#31181;&#20351;&#29992;&#25552;&#31034;&#24341;&#23548;&#30340;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#31572;&#20013;&#36890;&#29992;&#22270;&#20687;&#23383;&#24149;&#26080;&#27861;&#20934;&#30830;&#25551;&#36848;&#35270;&#35273;&#23454;&#20307;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.09699</link><description>&lt;p&gt;
PromptCap&#65306;&#20351;&#29992;GPT-3&#30340;&#25552;&#31034;&#24341;&#23548;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#36827;&#34892;&#35270;&#35273;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
PromptCap: Prompt-Guided Image Captioning for VQA with GPT-3. (arXiv:2211.09699v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09699
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;PromptCap&#65292;&#19968;&#31181;&#20351;&#29992;&#25552;&#31034;&#24341;&#23548;&#30340;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#31572;&#20013;&#36890;&#29992;&#22270;&#20687;&#23383;&#24149;&#26080;&#27861;&#20934;&#30830;&#25551;&#36848;&#35270;&#35273;&#23454;&#20307;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#31572;&#28041;&#21450;&#38656;&#35201;&#36229;&#36234;&#22270;&#29255;&#20197;&#20135;&#29983;&#27491;&#30830;&#31572;&#26696;&#30340;&#19990;&#30028;&#30693;&#35782;&#30340;&#38382;&#39064;&#12290;&#20687;GPT-3&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29305;&#21035;&#36866;&#29992;&#20110;&#27492;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#24378;&#22823;&#30340;&#30693;&#35782;&#26816;&#32034;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#20026;&#20102;&#20351;LM&#29702;&#35299;&#22270;&#20687;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#20351;&#29992;&#23383;&#24149;&#27169;&#22411;&#23558;&#22270;&#20687;&#36716;&#25442;&#20026;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#22312;&#21333;&#20010;&#23383;&#24149;&#21477;&#23376;&#20013;&#24635;&#32467;&#22270;&#20687;&#26102;&#65292;&#35201;&#25551;&#36848;&#21738;&#20123;&#35270;&#35273;&#23454;&#20307;&#32463;&#24120;&#19981;&#26126;&#30830;&#12290;&#36890;&#29992;&#22270;&#20687;&#23383;&#24149;&#32463;&#24120;&#38169;&#36807;LM&#22238;&#31572;&#35270;&#35273;&#38382;&#39064;&#25152;&#24517;&#38656;&#30340;&#35270;&#35273;&#32454;&#33410;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PromptCap&#65288;Prompt-guided image Captioning&#65289;&#65292;&#19968;&#31181;&#23383;&#24149;&#27169;&#22411;&#65292;&#26088;&#22312;&#25104;&#20026;&#22270;&#20687;&#21644;&#40657;&#30418;LM&#20043;&#38388;&#26356;&#22909;&#30340;&#36830;&#25509;&#22120;&#12290;&#19982;&#36890;&#29992;&#23383;&#24149;&#19981;&#21516;&#65292;PromptCap&#37319;&#29992;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#26469;&#25511;&#21046;&#29983;&#25104;&#30340;&#23383;&#24149;&#20013;&#35201;&#25551;&#36848;&#30340;&#35270;&#35273;&#23454;&#20307;&#12290;&#25552;&#31034;&#21253;&#21547;&#23383;&#24149;&#24212;&#22238;&#31572;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge-based visual question answering (VQA) involves questions that require world knowledge beyond the image to yield the correct answer. Large language models (LMs) like GPT-3 are particularly helpful for this task because of their strong knowledge retrieval and reasoning capabilities. To enable LM to understand images, prior work uses a captioning model to convert images into text. However, when summarizing an image in a single caption sentence, which visual entities to describe are often underspecified. Generic image captions often miss visual details essential for the LM to answer visual questions correctly. To address this challenge, we propose PromptCap (Prompt-guided image Captioning), a captioning model designed to serve as a better connector between images and black-box LMs. Different from generic captions, PromptCap takes a natural-language prompt to control the visual entities to describe in the generated caption. The prompt contains a question that the caption should ai
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#21442;&#25968;&#35843;&#25972;&#30340;&#20998;&#31867;&#22836;&#35757;&#32451;&#26041;&#27861;&#65292;&#21462;&#20195;&#20102;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#20998;&#31867;&#22836;&#20351;&#27169;&#22411;&#24615;&#33021;&#31283;&#23450;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2210.16771</link><description>&lt;p&gt;
&#39640;&#25928;&#21442;&#25968;&#35843;&#25972;&#21487;&#20197;&#20351;&#20998;&#31867;&#22836;&#34920;&#29616;&#20986;&#33394;
&lt;/p&gt;
&lt;p&gt;
Parameter-Efficient Tuning Makes a Good Classification Head. (arXiv:2210.16771v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16771
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#21442;&#25968;&#35843;&#25972;&#30340;&#20998;&#31867;&#22836;&#35757;&#32451;&#26041;&#27861;&#65292;&#21462;&#20195;&#20102;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#20998;&#31867;&#22836;&#20351;&#27169;&#22411;&#24615;&#33021;&#31283;&#23450;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#38761;&#21629;&#24615;&#22320;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#33539;&#24335;&#12290;&#25105;&#20204;&#22312;&#39044;&#35757;&#32451;&#30340;&#20027;&#24178;&#27169;&#22411;&#65288;&#22914;BERT&#65289;&#20043;&#21518;&#38468;&#21152;&#19968;&#20010;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#20998;&#31867;&#22836;&#65292;&#24182;&#24494;&#35843;&#25972;&#20010;&#27169;&#22411;&#12290;&#30001;&#20110;&#39044;&#35757;&#32451;&#20027;&#24178;&#27169;&#22411;&#23545;&#24615;&#33021;&#30340;&#36129;&#29486;&#24456;&#22823;&#65292;&#22240;&#27492;&#25105;&#20204;&#33258;&#28982;&#26399;&#26395;&#33391;&#22909;&#30340;&#39044;&#35757;&#32451;&#20998;&#31867;&#22836;&#20063;&#33021;&#21463;&#30410;&#20110;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#20027;&#24178;&#27169;&#22411;&#26368;&#21518;&#19968;&#23618;&#30340;&#36755;&#20986;&#65288;&#21363;&#20998;&#31867;&#22836;&#30340;&#36755;&#20837;&#65289;&#22312;&#24494;&#35843;&#26399;&#38388;&#20250;&#26377;&#24456;&#22823;&#21464;&#21270;&#65292;&#23548;&#33268;&#36890;&#24120;&#30340;&#22836;&#37096;&#21333;&#29420;&#39044;&#35757;&#32451;&#65288;LP-FT&#65289;&#22833;&#25928;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#39640;&#25928;&#21442;&#25968;&#35843;&#25972;&#21487;&#20197;&#20351;&#20998;&#31867;&#22836;&#34920;&#29616;&#20986;&#33394;&#65292;&#36890;&#36807;&#31616;&#21333;&#22320;&#26367;&#25442;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#22836;&#37096;&#65292;&#25105;&#20204;&#21487;&#20197;&#33719;&#24471;&#31283;&#23450;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312; GLUE &#21644; SuperGLUE &#30340; 9 &#39033;&#20219;&#21153;&#20013;&#65292;&#20351;&#29992;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#32852;&#21512;&#39044;&#35757;&#32451;&#30340;&#20998;&#31867;&#22836;&#21487;&#20197;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, pretrained models revolutionized the paradigm of natural language understanding (NLU), where we append a randomly initialized classification head after the pretrained backbone, e.g. BERT, and finetune the whole model. As the pretrained backbone makes a major contribution to the improvement, we naturally expect a good pretrained classification head can also benefit the training. However, the final-layer output of the backbone, i.e. the input of the classification head, will change greatly during finetuning, making the usual head-only pretraining (LP-FT) ineffective. In this paper, we find that parameter-efficient tuning makes a good classification head, with which we can simply replace the randomly initialized heads for a stable performance gain. Our experiments demonstrate that the classification head jointly pretrained with parameter-efficient tuning consistently improves the performance on 9 tasks in GLUE and SuperGLUE.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;SilverAlign&#31639;&#27861;&#65292;&#23558;&#26426;&#22120;&#32763;&#35793;&#21644;&#26368;&#23567;&#23545;&#29992;&#20110;&#29983;&#25104;&#38134;&#26631;&#20934;&#25968;&#25454;&#20197;&#35780;&#20272;&#21333;&#35789;&#23545;&#40784;&#22120;&#65292;&#35299;&#20915;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#32570;&#22833;&#37329;&#26631;&#20934;&#25968;&#25454;&#23545;&#40784;&#30340;&#37325;&#35201;&#22330;&#26223;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2210.06207</link><description>&lt;p&gt;
SilverAlign&#65306;&#29992;&#26426;&#22120;&#32763;&#35793;&#29983;&#25104;&#38134;&#26631;&#20934;&#25968;&#25454;&#36827;&#34892;&#21333;&#35789;&#23545;&#40784;&#35780;&#20272;&#30340;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
SilverAlign: MT-Based Silver Data Algorithm For Evaluating Word Alignment. (arXiv:2210.06207v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;SilverAlign&#31639;&#27861;&#65292;&#23558;&#26426;&#22120;&#32763;&#35793;&#21644;&#26368;&#23567;&#23545;&#29992;&#20110;&#29983;&#25104;&#38134;&#26631;&#20934;&#25968;&#25454;&#20197;&#35780;&#20272;&#21333;&#35789;&#23545;&#40784;&#22120;&#65292;&#35299;&#20915;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#32570;&#22833;&#37329;&#26631;&#20934;&#25968;&#25454;&#23545;&#40784;&#30340;&#37325;&#35201;&#22330;&#26223;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#35789;&#23545;&#40784;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#38750;&#24120;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#36873;&#25321;&#26368;&#20339;&#26041;&#27861;&#20197;&#21019;&#24314;&#21333;&#35789;&#23545;&#40784;&#38750;&#24120;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#37329;&#26631;&#20934;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#65292;&#36825;&#31181;&#36873;&#25321;&#21464;&#24471;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SilverAlign&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26426;&#22120;&#32763;&#35793;&#21644;&#26368;&#23567;&#23545;&#29983;&#25104;&#38134;&#26631;&#20934;&#25968;&#25454;&#65292;&#20174;&#32780;&#33258;&#21160;&#21019;&#24314;&#29992;&#20110;&#35780;&#20272;&#21333;&#35789;&#23545;&#40784;&#22120;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#25105;&#20204;&#30340;&#38134;&#26631;&#20934;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#19982;9&#31181;&#35821;&#35328;&#23545;&#30340;&#37329;&#26631;&#20934;&#22522;&#20934;&#30456;&#20851;&#24615;&#39640;&#65292;&#20351;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#20026;&#22312;&#37329;&#26631;&#20934;&#25968;&#25454;&#19981;&#21487;&#29992;&#24773;&#20917;&#19979;&#35780;&#20272;&#19981;&#21516;&#39046;&#22495;&#21644;&#35821;&#35328;&#30340;&#26377;&#25928;&#36164;&#28304;&#12290;&#36825;&#35299;&#20915;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#32570;&#22833;&#37329;&#26631;&#20934;&#25968;&#25454;&#23545;&#40784;&#30340;&#37325;&#35201;&#22330;&#26223;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Word alignments are essential for a variety of NLP tasks. Therefore, choosing the best approaches for their creation is crucial. However, the scarce availability of gold evaluation data makes the choice difficult. We propose SilverAlign, a new method to automatically create silver data for the evaluation of word aligners by exploiting machine translation and minimal pairs. We show that performance on our silver data correlates well with gold benchmarks for 9 language pairs, making our approach a valid resource for evaluation of different domains and languages when gold data are not available. This addresses the important scenario of missing gold data alignments for low-resource languages.
&lt;/p&gt;</description></item><item><title>AtteSTNet&#26159;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#23376;&#35789;&#20998;&#21106;&#30340;&#26816;&#27979;&#28151;&#21512;&#35821;&#35328;&#20167;&#24680;&#35328;&#35770;&#30340;&#26041;&#27861;&#65292;&#23427;&#19981;&#20165;&#19982;&#22797;&#26434;&#32593;&#32476;&#30456;&#24403;&#65292;&#32780;&#19988;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#24615;&#33021;&#26356;&#22909;&#65292;&#20854;&#26497;&#22823;&#30340;&#31616;&#21333;&#24615;&#21644;&#26131;&#20110;&#32500;&#25252;&#24615;&#26159;&#20854;&#20248;&#28857;&#12290;</title><link>http://arxiv.org/abs/2112.11479</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#23376;&#35789;&#20998;&#21106;&#30340;&#28151;&#21512;&#35821;&#35328;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AtteSTNet -- An attention and subword tokenization based approach for code-switched text hate speech detection. (arXiv:2112.11479v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.11479
&lt;/p&gt;
&lt;p&gt;
AtteSTNet&#26159;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#23376;&#35789;&#20998;&#21106;&#30340;&#26816;&#27979;&#28151;&#21512;&#35821;&#35328;&#20167;&#24680;&#35328;&#35770;&#30340;&#26041;&#27861;&#65292;&#23427;&#19981;&#20165;&#19982;&#22797;&#26434;&#32593;&#32476;&#30456;&#24403;&#65292;&#32780;&#19988;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#24615;&#33021;&#26356;&#22909;&#65292;&#20854;&#26497;&#22823;&#30340;&#31616;&#21333;&#24615;&#21644;&#26131;&#20110;&#32500;&#25252;&#24615;&#26159;&#20854;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25216;&#26415;&#30340;&#26368;&#26032;&#36827;&#23637;&#23548;&#33268;&#31038;&#20132;&#23186;&#20307;&#30340;&#20351;&#29992;&#37327;&#22686;&#21152;&#65292;&#20063;&#23548;&#33268;&#22823;&#37327;&#29992;&#25143;&#29983;&#25104;&#30340;&#25968;&#25454;&#65292;&#20854;&#20013;&#21253;&#25324;&#20196;&#20154;&#35752;&#21388;&#21644;&#20882;&#29359;&#30340;&#35328;&#35770;&#12290;&#31038;&#20132;&#23186;&#20307;&#19978;&#20351;&#29992;&#30340;&#35821;&#35328;&#36890;&#24120;&#26159;&#33521;&#35821;&#21644;&#32946;&#22320;&#26041;&#35821;&#35328;&#30340;&#32452;&#21512;&#12290;&#22312;&#21360;&#24230;&#65292;&#21360;&#22320;&#35821;&#26159;&#20027;&#35201;&#20351;&#29992;&#30340;&#35821;&#35328;&#65292;&#24182;&#32463;&#24120;&#19982;&#33521;&#35821;&#20999;&#25442;&#65292;&#24418;&#25104;&#21360;&#22320;&#33521;&#35821;&#65288;Hinglish&#65289;&#35821;&#35328;&#12290;&#36807;&#21435;&#24050;&#32463;&#37319;&#29992;&#20102;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26469;&#23545;&#28151;&#21512;&#26102;&#30340;&#21360;&#22320;&#33521;&#35821;&#20167;&#24680;&#35328;&#35770;&#36827;&#34892;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#20351;&#29992;&#30340;&#24490;&#29615;&#25110;&#21367;&#31215;&#26426;&#21046;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#20869;&#23384;&#38656;&#27714;&#22823;&#12290;&#36807;&#21435;&#30340;&#25216;&#26415;&#36824;&#20351;&#29992;&#22797;&#26434;&#30340;&#25968;&#25454;&#22788;&#29702;&#26041;&#27861;&#65292;&#20351;&#29616;&#26377;&#25216;&#26415;&#38750;&#24120;&#22797;&#26434;&#19988;&#38590;&#20197;&#25913;&#21464;&#25968;&#25454;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#19981;&#20165;&#19982;&#36825;&#20123;&#22797;&#26434;&#32593;&#32476;&#19968;&#26679;&#65292;&#24182;&#19988;&#22312;&#22914;HASOC&#65288;&#21360;&#27431;&#35821;&#35328;&#30340;&#20167;&#24680;&#35328;&#35770;&#21644;&#20882;&#29359;&#20869;&#23481;&#35782;&#21035;&#65289;&#27492;&#31867;&#28151;&#21512;&#21360;&#22320;&#33521;&#35821;&#25991;&#26412;&#30340;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#24615;&#33021;&#22522;&#20934;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21517;&#20026;AtteSTNet&#65292;&#23427;&#21033;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#23376;&#35789;&#20998;&#21106;&#26469;&#35782;&#21035;&#28151;&#21512;&#35821;&#35328;&#20013;&#30340;&#20167;&#24680;&#35328;&#35770;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#27604;&#20197;&#21069;&#30340;&#25216;&#26415;&#34920;&#29616;&#26356;&#22909;&#65292;&#26356;&#31616;&#21333;&#26131;&#20110;&#32500;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in technology have led to a boost in social media usage which has ultimately led to large amounts of user-generated data which also includes hateful and offensive speech. The language used in social media is often a combination of English and the native language in the region. In India, Hindi is used predominantly and is often code-switched with English, giving rise to the Hinglish (Hindi+English) language. Various approaches have been made in the past to classify the code-mixed Hinglish hate speech using different machine learning and deep learning-based techniques. However, these techniques make use of recurrence on convolution mechanisms which are computationally expensive and have high memory requirements. Past techniques also make use of complex data processing making the existing techniques very complex and non-sustainable to change in data. Proposed work gives a much simpler approach which is not only at par with these complex networks but also exceeds perfor
&lt;/p&gt;</description></item></channel></rss>