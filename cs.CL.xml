<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>ChipNeMo&#36890;&#36807;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22312;&#24037;&#19994;&#33455;&#29255;&#35774;&#35745;&#20013;&#22823;&#24133;&#25552;&#21319;LLM&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23567;&#20102;&#27169;&#22411;&#23610;&#23544;&#65292;&#22312;&#24037;&#31243;&#21161;&#25163;&#12289;&#33050;&#26412;&#29983;&#25104;&#21644;&#32570;&#38519;&#20998;&#26512;&#31561;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2311.00176</link><description>&lt;p&gt;
ChipNeMo: &#29992;&#20110;&#33455;&#29255;&#35774;&#35745;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;LLMs
&lt;/p&gt;
&lt;p&gt;
ChipNeMo: Domain-Adapted LLMs for Chip Design. (arXiv:2311.00176v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00176
&lt;/p&gt;
&lt;p&gt;
ChipNeMo&#36890;&#36807;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22312;&#24037;&#19994;&#33455;&#29255;&#35774;&#35745;&#20013;&#22823;&#24133;&#25552;&#21319;LLM&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23567;&#20102;&#27169;&#22411;&#23610;&#23544;&#65292;&#22312;&#24037;&#31243;&#21161;&#25163;&#12289;&#33050;&#26412;&#29983;&#25104;&#21644;&#32570;&#38519;&#20998;&#26512;&#31561;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChipNeMo&#26088;&#22312;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24037;&#19994;&#33455;&#29255;&#35774;&#35745;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#19981;&#30452;&#25509;&#20351;&#29992;&#21830;&#19994;&#25110;&#24320;&#28304;LLMs&#65292;&#32780;&#26159;&#37319;&#29992;&#20197;&#19979;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#65306;&#23450;&#21046;&#20998;&#35789;&#22120;&#12289;&#39046;&#22495;&#33258;&#36866;&#24212;&#25345;&#32493;&#39044;&#35757;&#32451;&#12289;&#24102;&#26377;&#39046;&#22495;&#29305;&#23450;&#25351;&#20196;&#30340;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#21644;&#39046;&#22495;&#33258;&#36866;&#24212;&#26816;&#32034;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#33455;&#29255;&#35774;&#35745;&#30340;&#19977;&#20010;&#36873;&#23450;LLM&#24212;&#29992;&#19978;&#35780;&#20272;&#20102;&#36825;&#20123;&#26041;&#27861;&#65306;&#24037;&#31243;&#21161;&#25163;&#32842;&#22825;&#26426;&#22120;&#20154;&#12289;EDA&#33050;&#26412;&#29983;&#25104;&#20197;&#21450;&#32570;&#38519;&#25688;&#35201;&#21644;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#20123;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#20351;LLM&#22312;&#36825;&#19977;&#20010;&#24212;&#29992;&#20013;&#24615;&#33021;&#22823;&#24133;&#25552;&#21319;&#65292;&#22312;&#21508;&#31181;&#35774;&#35745;&#20219;&#21153;&#19978;&#21487;&#20197;&#23454;&#29616;&#39640;&#36798;5&#20493;&#30340;&#27169;&#22411;&#23610;&#23544;&#32553;&#20943;&#65292;&#21516;&#26102;&#20855;&#26377;&#31867;&#20284;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;&#24403;&#21069;&#30340;&#32467;&#26524;&#21644;&#29702;&#24819;&#32467;&#26524;&#20043;&#38388;&#36824;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;&#25105;&#20204;&#30456;&#20449;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#23558;&#26377;&#21161;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChipNeMo aims to explore the applications of large language models (LLMs) for industrial chip design. Instead of directly deploying off-the-shelf commercial or open-source LLMs, we instead adopt the following domain adaptation techniques: custom tokenizers, domain-adaptive continued pretraining, supervised fine-tuning (SFT) with domain-specific instructions, and domain-adapted retrieval models. We evaluate these methods on three selected LLM applications for chip design: an engineering assistant chatbot, EDA script generation, and bug summarization and analysis. Our results show that these domain adaptation techniques enable significant LLM performance improvements over general-purpose base models across the three evaluated applications, enabling up to 5x model size reduction with similar or better performance on a range of design tasks. Our findings also indicate that there's still room for improvement between our current results and ideal outcomes. We believe that further investigati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DECENT&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25239;&#35299;&#32806;LLMs&#30340;&#29702;&#35299;&#21644;&#20462;&#39280;&#33021;&#21147;&#65292;&#25552;&#39640;&#25991;&#26412;&#25688;&#35201;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#25506;&#27979;&#25216;&#26415;&#26469;&#24357;&#34917;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#30495;&#19982;&#20551;&#30340;&#25935;&#24863;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.19347</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;LLMs&#30340;&#29702;&#35299;&#21644;&#20462;&#39280;&#33021;&#21147;&#36827;&#34892;&#23545;&#25239;&#35299;&#32806;&#65292;&#25552;&#39640;&#25991;&#26412;&#25688;&#35201;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Improving Factual Consistency of Text Summarization by Adversarially Decoupling Comprehension and Embellishment Abilities of LLMs. (arXiv:2310.19347v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19347
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DECENT&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25239;&#35299;&#32806;LLMs&#30340;&#29702;&#35299;&#21644;&#20462;&#39280;&#33021;&#21147;&#65292;&#25552;&#39640;&#25991;&#26412;&#25688;&#35201;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#25506;&#27979;&#25216;&#26415;&#26469;&#24357;&#34917;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#30495;&#19982;&#20551;&#30340;&#25935;&#24863;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25991;&#26412;&#25688;&#35201;&#26041;&#38754;&#21462;&#24471;&#20102;&#36817;&#26399;&#30340;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#20250;&#29983;&#25104;&#19982;&#21407;&#22987;&#25991;&#31456;&#20107;&#23454;&#19981;&#19968;&#33268;&#30340;&#25688;&#35201;&#65292;&#34987;&#31216;&#20026;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#8220;&#24187;&#35273;&#8221;&#12290;&#19982;&#20043;&#21069;&#30340;&#23567;&#22411;&#27169;&#22411;&#65288;&#22914;BART&#65292;T5&#65289;&#19981;&#21516;&#65292;&#24403;&#21069;&#30340;LLMs&#22312;&#21046;&#36896;&#24858;&#34850;&#38169;&#35823;&#26041;&#38754;&#36739;&#23569;&#65292;&#20294;&#21046;&#36896;&#20102;&#26356;&#22797;&#26434;&#30340;&#38169;&#35823;&#65292;&#20363;&#22914;&#21152;&#20837;&#22240;&#26524;&#20851;&#31995;&#12289;&#28155;&#21152;&#38169;&#35823;&#32454;&#33410;&#21644;&#36807;&#24230;&#27867;&#21270;&#31561;&#12290;&#36825;&#20123;&#24187;&#35273;&#24456;&#38590;&#36890;&#36807;&#20256;&#32479;&#26041;&#27861;&#26816;&#27979;&#20986;&#26469;&#65292;&#36825;&#32473;&#25552;&#39640;&#25991;&#26412;&#25688;&#35201;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#24102;&#26469;&#20102;&#24456;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#35299;&#32806;&#26041;&#27861;&#26469;&#20998;&#31163;LLMs&#30340;&#29702;&#35299;&#21644;&#20462;&#39280;&#33021;&#21147;&#65288;DECENT&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#19968;&#31181;&#22522;&#20110;&#25506;&#27979;&#30340;&#21442;&#25968;&#39640;&#25928;&#25216;&#26415;&#65292;&#20197;&#24357;&#34917;LLMs&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#30495;&#19982;&#20551;&#30340;&#25935;&#24863;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;LLMs&#23545;&#20110;&#20462;&#39280;&#21644;&#29702;&#35299;&#30340;&#27010;&#24565;&#26356;&#21152;&#28165;&#26224;&#65292;&#20174;&#32780;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#25191;&#34892;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the recent progress in text summarization made by large language models (LLMs), they often generate summaries that are factually inconsistent with original articles, known as "hallucinations" in text generation. Unlike previous small models (e.g., BART, T5), current LLMs make fewer silly mistakes but more sophisticated ones, such as imposing cause and effect, adding false details, and overgeneralizing, etc. These hallucinations are challenging to detect through traditional methods, which poses great challenges for improving the factual consistency of text summarization. In this paper, we propose an adversarially DEcoupling method to disentangle the Comprehension and EmbellishmeNT abilities of LLMs (DECENT). Furthermore, we adopt a probing-based parameter-efficient technique to cover the shortage of sensitivity for true and false in the training process of LLMs. In this way, LLMs are less confused about embellishing and understanding, thus can execute the instructions more accur
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#29255;&#27573;&#21040;&#29255;&#27573;&#26694;&#26550; (Seg2Seg) &#29992;&#20110;&#21516;&#26102;&#24207;&#21015;&#29983;&#25104;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#21644;&#32479;&#19968;&#30340;&#26041;&#24335;&#23398;&#20064;&#28304;&#24207;&#21015;&#21644;&#30446;&#26631;&#24207;&#21015;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#23454;&#29616;&#39640;&#36136;&#37327;&#29983;&#25104;&#21644;&#20302;&#24310;&#36831;&#12290;</title><link>http://arxiv.org/abs/2310.17940</link><description>&lt;p&gt;
&#32479;&#19968;&#30340;&#29255;&#27573;&#21040;&#29255;&#27573;&#26694;&#26550;&#29992;&#20110;&#21516;&#26102;&#24207;&#21015;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Unified Segment-to-Segment Framework for Simultaneous Sequence Generation. (arXiv:2310.17940v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17940
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#29255;&#27573;&#21040;&#29255;&#27573;&#26694;&#26550; (Seg2Seg) &#29992;&#20110;&#21516;&#26102;&#24207;&#21015;&#29983;&#25104;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#21644;&#32479;&#19968;&#30340;&#26041;&#24335;&#23398;&#20064;&#28304;&#24207;&#21015;&#21644;&#30446;&#26631;&#24207;&#21015;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#23454;&#29616;&#39640;&#36136;&#37327;&#29983;&#25104;&#21644;&#20302;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#26102;&#24207;&#21015;&#29983;&#25104;&#26159;&#23454;&#26102;&#22330;&#26223;&#30340;&#20851;&#38190;&#20219;&#21153;&#65292;&#27604;&#22914;&#27969;&#24335;&#35821;&#38899;&#35782;&#21035;&#12289;&#21516;&#26102;&#26426;&#22120;&#32763;&#35793;&#21644;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;&#65292;&#20854;&#20013;&#30446;&#26631;&#24207;&#21015;&#22312;&#25509;&#25910;&#28304;&#24207;&#21015;&#30340;&#21516;&#26102;&#29983;&#25104;&#12290;&#23454;&#29616;&#39640;&#36136;&#37327;&#29983;&#25104;&#21644;&#20302;&#24310;&#36831;&#30340;&#20851;&#38190;&#22312;&#20110;&#30830;&#23450;&#29983;&#25104;&#30340;&#26368;&#20339;&#26102;&#26426;&#65292;&#36890;&#36807;&#23398;&#20064;&#28304;&#24207;&#21015;&#21644;&#30446;&#26631;&#24207;&#21015;&#20043;&#38388;&#30340;&#26144;&#23556;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#20381;&#36182;&#20110;&#29305;&#23450;&#20219;&#21153;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#38480;&#21046;&#20102;&#27169;&#22411;&#23545;&#28304;-&#30446;&#26631;&#26144;&#23556;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#33021;&#21147;&#65292;&#38459;&#30861;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#22312;&#21508;&#31181;&#21516;&#26102;&#20219;&#21153;&#20013;&#30340;&#25506;&#32034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#29255;&#27573;&#21040;&#29255;&#27573;&#26694;&#26550; (Seg2Seg) &#29992;&#20110;&#21516;&#26102;&#24207;&#21015;&#29983;&#25104;&#65292;&#20197;&#33258;&#36866;&#24212;&#21644;&#32479;&#19968;&#30340;&#26041;&#24335;&#23398;&#20064;&#26144;&#23556;&#12290;&#22312;&#21516;&#26102;&#29983;&#25104;&#30340;&#36807;&#31243;&#20013;&#65292;&#27169;&#22411;&#22312;&#31561;&#24453;&#28304;&#29255;&#27573;&#21644;&#29983;&#25104;&#30446;&#26631;&#29255;&#27573;&#20043;&#38388;&#20132;&#26367;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simultaneous sequence generation is a pivotal task for real-time scenarios, such as streaming speech recognition, simultaneous machine translation and simultaneous speech translation, where the target sequence is generated while receiving the source sequence. The crux of achieving high-quality generation with low latency lies in identifying the optimal moments for generating, accomplished by learning a mapping between the source and target sequences. However, existing methods often rely on task-specific heuristics for different sequence types, limiting the model's capacity to adaptively learn the source-target mapping and hindering the exploration of multi-task learning for various simultaneous tasks. In this paper, we propose a unified segment-to-segment framework (Seg2Seg) for simultaneous sequence generation, which learns the mapping in an adaptive and unified manner. During the process of simultaneous generation, the model alternates between waiting for a source segment and generat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25968;&#25454;&#20013;&#24515;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#22788;&#29702;&#21644;&#39044;&#29702;&#35299;&#25968;&#25454;&#26469;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#37329;&#34701;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#37319;&#29992;&#35813;&#26041;&#27861;&#30340;&#37329;&#34701;LLMs&#22312;&#37329;&#34701;&#20998;&#26512;&#21644;&#35299;&#37322;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2310.17784</link><description>&lt;p&gt;
&#25968;&#25454;&#20013;&#24515;&#21270;&#30340;&#37329;&#34701;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Data-Centric Financial Large Language Models. (arXiv:2310.17784v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17784
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25968;&#25454;&#20013;&#24515;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#22788;&#29702;&#21644;&#39044;&#29702;&#35299;&#25968;&#25454;&#26469;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#37329;&#34701;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#37319;&#29992;&#35813;&#26041;&#27861;&#30340;&#37329;&#34701;LLMs&#22312;&#37329;&#34701;&#20998;&#26512;&#21644;&#35299;&#37322;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#28508;&#21147;&#65292;&#20294;&#30452;&#25509;&#24212;&#29992;&#20110;&#22797;&#26434;&#39046;&#22495;&#22914;&#37329;&#34701;&#26102;&#21364;&#36935;&#21040;&#22256;&#38590;&#12290;LLMs&#38590;&#20197;&#25512;&#29702;&#21644;&#25972;&#21512;&#25152;&#26377;&#30456;&#20851;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#20013;&#24515;&#21270;&#30340;&#26041;&#27861;&#65292;&#20351;LLMs&#33021;&#22815;&#26356;&#22909;&#22320;&#22788;&#29702;&#37329;&#34701;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35266;&#28857;&#26159;&#65292;&#19981;&#26159;&#19968;&#27425;&#24615;&#32473;LLM&#36127;&#36733;&#36807;&#22810;&#20449;&#24687;&#65292;&#32780;&#26159;&#26356;&#26377;&#25928;&#22320;&#23545;&#25968;&#25454;&#36827;&#34892;&#39044;&#22788;&#29702;&#21644;&#39044;&#29702;&#35299;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#20219;&#21153;&#22522;&#20110;&#25552;&#31034;&#30340;&#24494;&#35843;&#26469;&#21019;&#24314;&#37329;&#34701;LLM&#65288;FLLM&#65289;&#65292;&#20197;&#23454;&#29616;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#39044;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#27599;&#20010;&#20219;&#21153;&#30340;&#26631;&#35760;&#25968;&#25454;&#26377;&#38480;&#12290;&#20026;&#20102;&#20811;&#26381;&#25163;&#21160;&#27880;&#37322;&#30340;&#25104;&#26412;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#33258;&#21160;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#22686;&#24378;&#25512;&#29702;&#65288;AAR&#65289;&#26469;&#20462;&#25913;FLLM&#33258;&#36523;&#36755;&#20986;&#30340;&#20266;&#26631;&#31614;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#20013;&#24515;&#21270;FLLM&#19982;AAR&#30456;&#27604;&#65292;&#26174;&#33879;&#20248;&#20110;&#20026;&#21407;&#22987;&#25991;&#26412;&#35774;&#35745;&#30340;&#22522;&#32447;&#37329;&#34701;LLMs&#65292;&#22312;&#37329;&#34701;&#20998;&#26512;&#21644;&#35299;&#37322;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) show promise for natural language tasks but struggle when applied directly to complex domains like finance. LLMs have difficulty reasoning about and integrating all relevant information. We propose a data-centric approach to enable LLMs to better handle financial tasks. Our key insight is that rather than overloading the LLM with everything at once, it is more effective to preprocess and pre-understand the data. We create a financial LLM (FLLM) using multitask prompt-based finetuning to achieve data pre-processing and pre-understanding. However, labeled data is scarce for each task. To overcome manual annotation costs, we employ abductive augmentation reasoning (AAR) to automatically generate training data by modifying the pseudo labels from FLLM's own outputs. Experiments show our data-centric FLLM with AAR substantially outperforms baseline financial LLMs designed for raw text, achieving state-of-the-art on financial analysis and interpretation tasks. We 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20943;&#23569;&#26080;&#20851;&#25991;&#26723;&#30340;&#24178;&#25200;&#26469;&#25913;&#21892;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#30340;&#38646;&#26679;&#26412;&#38405;&#35835;&#22120;&#30340;&#26041;&#27861;&#12290;&#37319;&#29992;&#20102;&#24178;&#25200;&#24863;&#30693;&#30340;&#31572;&#26696;&#36873;&#25321;(DAS)&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;LLMs&#21463;&#21040;&#24178;&#25200;&#21644;&#36807;&#24230;&#33258;&#20449;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#25104;&#21151;&#22320;&#25913;&#21892;&#20102;&#38646;&#26679;&#26412;&#38405;&#35835;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17490</link><description>&lt;p&gt;
&#25552;&#39640;&#36890;&#36807;&#20943;&#23569;&#26080;&#20851;&#25991;&#26723;&#23545;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#30340;&#38646;&#26679;&#26412;&#38405;&#35835;&#22120;&#30340;&#24178;&#25200;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improving Zero-shot Reader by Reducing Distractions from Irrelevant Documents in Open-Domain Question Answering. (arXiv:2310.17490v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17490
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20943;&#23569;&#26080;&#20851;&#25991;&#26723;&#30340;&#24178;&#25200;&#26469;&#25913;&#21892;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#30340;&#38646;&#26679;&#26412;&#38405;&#35835;&#22120;&#30340;&#26041;&#27861;&#12290;&#37319;&#29992;&#20102;&#24178;&#25200;&#24863;&#30693;&#30340;&#31572;&#26696;&#36873;&#25321;(DAS)&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;LLMs&#21463;&#21040;&#24178;&#25200;&#21644;&#36807;&#24230;&#33258;&#20449;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#25104;&#21151;&#22320;&#25913;&#21892;&#20102;&#38646;&#26679;&#26412;&#38405;&#35835;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20351;&#24471;&#22312;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;(ODQA)&#20013;&#23454;&#29616;&#38646;&#26679;&#26412;&#26041;&#27861;&#25104;&#20026;&#21487;&#33021;&#65292;&#20294;&#26159;&#30001;&#20110;&#38405;&#35835;&#22120;&#30456;&#23545;&#20110;&#26816;&#32034;&#22120;&#30340;&#36827;&#23637;&#26377;&#38480;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#19968;&#31181;&#38646;&#26679;&#26412;&#38405;&#35835;&#22120;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#35299;&#20915;&#35745;&#31639;&#25104;&#26412;&#21644;&#26631;&#27880;&#25968;&#25454;&#38656;&#27714;&#31561;&#25361;&#25112;&#12290;&#25105;&#20204;&#21457;&#29616;LLMs&#30001;&#20110;&#26816;&#32034;&#21040;&#30340;&#26080;&#20851;&#25991;&#26723;&#20197;&#21450;&#20316;&#20026;&#38646;&#26679;&#26412;&#38405;&#35835;&#22120;&#26102;&#29983;&#25104;&#31572;&#26696;&#30340;&#36807;&#24230;&#33258;&#20449;&#32780;&#21463;&#21040;&#24178;&#25200;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#22522;&#20110;&#21542;&#23450;&#30340;&#25351;&#20196;&#21644;&#20998;&#25968;&#35843;&#25972;&#30340;&#24178;&#25200;&#24863;&#30693;&#30340;&#31572;&#26696;&#36873;&#25321;(DAS)&#26041;&#27861;&#65292;&#20197;&#20943;&#36731;&#36825;&#20123;&#25991;&#26723;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#22788;&#29702;&#20102;&#19981;&#21516;&#22330;&#26223;&#19979;&#30340;&#24178;&#25200;&#65292;&#25552;&#39640;&#20102;&#38646;&#26679;&#26412;&#38405;&#35835;&#22120;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#19982;&#38754;&#23545;&#26410;&#35265;&#36807;&#25968;&#25454;&#32780;&#22256;&#38590;&#37325;&#37325;&#30340;&#30417;&#30563;&#24335;&#38405;&#35835;&#22120;&#19981;&#21516;&#65292;&#38646;&#26679;&#26412;&#38405;&#35835;&#22120;&#23637;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#21487;&#36801;&#31227;&#24615;&#65292;&#26080;&#38656;&#20219;&#20309;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) enable zero-shot approaches in open-domain question answering (ODQA), yet with limited advancements as the reader is compared to the retriever. This study aims at the feasibility of a zero-shot reader that addresses the challenges of computational cost and the need for labeled data. We find that LLMs are distracted due to irrelevant documents in the retrieved set and the overconfidence of the generated answers when they are exploited as zero-shot readers. To tackle these problems, we mitigate the impact of such documents via Distraction-aware Answer Selection (DAS) with a negation-based instruction and score adjustment for proper answer selection. Experimental results show that our approach successfully handles distraction across diverse scenarios, enhancing the performance of zero-shot readers. Furthermore, unlike supervised readers struggling with unseen data, zero-shot readers demonstrate outstanding transferability without any training.
&lt;/p&gt;</description></item><item><title>PreWoMe&#26159;&#19968;&#31181;&#22788;&#29702;&#38271;&#31687;&#38382;&#31572;&#20013;&#20449;&#24687;&#26816;&#32034;&#38382;&#39064;&#30340;&#32479;&#19968;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#38382;&#39064;&#20013;&#30340;&#39044;&#35774;&#24182;&#21033;&#29992;&#20854;&#20316;&#20026;&#24037;&#20316;&#35760;&#24518;&#26469;&#29983;&#25104;&#21453;&#39304;&#21644;&#34892;&#21160;&#65292;&#19981;&#20165;&#33021;&#26377;&#25928;&#35299;&#20915;&#35823;&#23548;&#24615;&#38382;&#39064;&#65292;&#32780;&#19988;&#36866;&#29992;&#20110;&#22788;&#29702;&#27491;&#24120;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#22312;&#23454;&#38469;&#38382;&#31572;&#22330;&#26223;&#20013;&#21033;&#29992;&#39044;&#35774;&#12289;&#21453;&#39304;&#21644;&#34892;&#21160;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16147</link><description>&lt;p&gt;
PreWoMe:&#21033;&#29992;&#39044;&#35774;&#20026;&#38271;&#31687;&#38382;&#31572;&#20013;&#30340;&#24037;&#20316;&#35760;&#24518;&#36827;&#34892;&#38382;&#39064;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
PreWoMe: Exploiting Presuppositions as Working Memory for Long Form Question Answering. (arXiv:2310.16147v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16147
&lt;/p&gt;
&lt;p&gt;
PreWoMe&#26159;&#19968;&#31181;&#22788;&#29702;&#38271;&#31687;&#38382;&#31572;&#20013;&#20449;&#24687;&#26816;&#32034;&#38382;&#39064;&#30340;&#32479;&#19968;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#38382;&#39064;&#20013;&#30340;&#39044;&#35774;&#24182;&#21033;&#29992;&#20854;&#20316;&#20026;&#24037;&#20316;&#35760;&#24518;&#26469;&#29983;&#25104;&#21453;&#39304;&#21644;&#34892;&#21160;&#65292;&#19981;&#20165;&#33021;&#26377;&#25928;&#35299;&#20915;&#35823;&#23548;&#24615;&#38382;&#39064;&#65292;&#32780;&#19988;&#36866;&#29992;&#20110;&#22788;&#29702;&#27491;&#24120;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#22312;&#23454;&#38469;&#38382;&#31572;&#22330;&#26223;&#20013;&#21033;&#29992;&#39044;&#35774;&#12289;&#21453;&#39304;&#21644;&#34892;&#21160;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#31687;&#38382;&#31572;&#20013;&#30340;&#20449;&#24687;&#26816;&#32034;&#38382;&#39064;&#24120;&#24120;&#30001;&#20110;&#38382;&#39064;&#20013;&#30340;&#27169;&#31946;&#25110;&#38169;&#35823;&#39044;&#35774;&#32780;&#35823;&#23548;&#12290;&#34429;&#28982;&#35768;&#22810;&#29616;&#26377;&#26041;&#27861;&#21487;&#20197;&#22788;&#29702;&#35823;&#23548;&#24615;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#38024;&#23545;&#30340;&#26159;&#26377;&#38480;&#30340;&#38382;&#39064;&#65292;&#24456;&#38590;&#36866;&#24212;&#23454;&#38469;&#24773;&#20917;&#20013;&#19981;&#21487;&#39044;&#27979;&#30340;&#36755;&#20837;&#29305;&#24449;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PreWoMe&#65292;&#36825;&#26159;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22788;&#29702;&#20219;&#20309;&#31867;&#22411;&#30340;&#20449;&#24687;&#26816;&#32034;&#38382;&#39064;&#12290;PreWoMe&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#25552;&#21462;&#38382;&#39064;&#20013;&#30340;&#39044;&#35774;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#24037;&#20316;&#35760;&#24518;&#26469;&#29983;&#25104;&#23545;&#38382;&#39064;&#30340;&#21453;&#39304;&#21644;&#34892;&#21160;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;PreWoMe&#19981;&#20165;&#22312;&#35299;&#20915;&#35823;&#23548;&#24615;&#38382;&#39064;&#26041;&#38754;&#26377;&#25928;&#65292;&#32780;&#19988;&#22312;&#22788;&#29702;&#27491;&#24120;&#38382;&#39064;&#26041;&#38754;&#20063;&#24456;&#26377;&#25928;&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;&#22312;&#23454;&#38469;&#38382;&#31572;&#22330;&#26223;&#20013;&#21033;&#29992;&#39044;&#35774;&#12289;&#21453;&#39304;&#21644;&#34892;&#21160;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Information-seeking questions in long-form question answering (LFQA) often prove misleading due to ambiguity or false presupposition in the question. While many existing approaches handle misleading questions, they are tailored to limited questions, which are insufficient in a real-world setting with unpredictable input characteristics. In this work, we propose PreWoMe, a unified approach capable of handling any type of information-seeking question. The key idea of PreWoMe involves extracting presuppositions in the question and exploiting them as working memory to generate feedback and action about the question. Our experiment shows that PreWoMe is effective not only in tackling misleading questions but also in handling normal ones, thereby demonstrating the effectiveness of leveraging presuppositions, feedback, and action for real-world QA settings.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GenTKG&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#26102;&#38388;&#36923;&#36753;&#35268;&#21017;&#30340;&#26816;&#32034;&#31574;&#30053;&#21644;&#36731;&#37327;&#32423;&#30340;&#21442;&#25968;&#25928;&#29575;&#25351;&#23548;&#65292;&#20811;&#26381;&#20102;&#22797;&#26434;&#30340;&#26102;&#38388;&#22270;&#25968;&#25454;&#32467;&#26500;&#21644;&#24222;&#22823;&#30340;&#25968;&#25454;&#37327;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.07793</link><description>&lt;p&gt;
GenTKG: &#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
GenTKG: Generative Forecasting on Temporal Knowledge Graph. (arXiv:2310.07793v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07793
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GenTKG&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#26102;&#38388;&#36923;&#36753;&#35268;&#21017;&#30340;&#26816;&#32034;&#31574;&#30053;&#21644;&#36731;&#37327;&#32423;&#30340;&#21442;&#25968;&#25928;&#29575;&#25351;&#23548;&#65292;&#20811;&#26381;&#20102;&#22797;&#26434;&#30340;&#26102;&#38388;&#22270;&#25968;&#25454;&#32467;&#26500;&#21644;&#24222;&#22823;&#30340;&#25968;&#25454;&#37327;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#24555;&#36895;&#21457;&#23637;&#24341;&#21457;&#20102;&#23545;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;(tKG)&#39046;&#22495;&#30340;&#20852;&#36259;&#65292;&#20854;&#20013;&#20256;&#32479;&#30340;&#22522;&#20110;&#23884;&#20837;&#21644;&#35268;&#21017;&#30340;&#27169;&#22411;&#21344;&#20027;&#23548;&#22320;&#20301;&#12290;&#30446;&#21069;&#20173;&#28982;&#23384;&#22312;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#39044;&#35757;&#32451;&#30340;LLM&#26159;&#21542;&#33021;&#22815;&#29702;&#35299;&#32467;&#26500;&#21270;&#30340;&#26102;&#38388;&#20851;&#31995;&#25968;&#25454;&#65292;&#24182;&#21462;&#20195;&#23427;&#20204;&#25104;&#20026;&#26102;&#38388;&#20851;&#31995;&#39044;&#27979;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#26102;&#38388;&#30693;&#35782;&#39044;&#27979;&#24341;&#20837;&#29983;&#25104;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#22312;&#22797;&#26434;&#30340;&#26102;&#38388;&#22270;&#25968;&#25454;&#32467;&#26500;&#21644;LLM&#21487;&#20197;&#22788;&#29702;&#30340;&#24207;&#21015;&#33258;&#28982;&#34920;&#36798;&#20043;&#38388;&#23384;&#22312;&#24040;&#22823;&#30340;&#40511;&#27807;&#65292;&#22312;tKG&#30340;&#24222;&#22823;&#25968;&#25454;&#37327;&#21644;&#24494;&#35843;LLM&#30340;&#24040;&#22823;&#35745;&#31639;&#25104;&#26412;&#20043;&#38388;&#20063;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#26694;&#26550;&#65292;&#31216;&#20026;GenTKG&#65292;&#23427;&#22312;tKG&#19978;&#25191;&#34892;&#29983;&#25104;&#24335;&#39044;&#27979;&#65292;&#32467;&#21512;&#20102;&#22522;&#20110;&#26102;&#38388;&#36923;&#36753;&#35268;&#21017;&#30340;&#26816;&#32034;&#31574;&#30053;&#21644;&#36731;&#37327;&#32423;&#30340;&#21442;&#25968;&#25928;&#29575;&#25351;&#23548;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;GenTKG&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advancements in large language models (LLMs) have ignited interest in the temporal knowledge graph (tKG) domain, where conventional carefully designed embedding-based and rule-based models dominate. The question remains open of whether pre-trained LLMs can understand structured temporal relational data and replace them as the foundation model for temporal relational forecasting. Therefore, we bring temporal knowledge forecasting into the generative setting. However, challenges occur in the huge chasms between complex temporal graph data structure and sequential natural expressions LLMs can handle, and between the enormous data sizes of tKGs and heavy computation costs of finetuning LLMs. To address these challenges, we propose a novel retrieval augmented generation framework that performs generative forecasting on tKGs named GenTKG, which combines a temporal logical rule-based retrieval strategy and lightweight parameter-efficient instruction tuning. Extensive experiments hav
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#35825;&#23548;&#36741;&#23548;&#33050;&#26412;&#21644;&#33258;&#21160;&#21327;&#35843;&#33050;&#26412;&#30340;&#26032;&#22411;&#23545;&#35805;&#36741;&#23548;&#31995;&#32479;&#12290;&#22312;&#21021;&#27493;&#30340;&#29992;&#25143;&#30740;&#31350;&#20013;&#65292;&#19982;&#20854;&#20182;&#31616;&#21333;&#30340;&#38382;&#31572;&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#38405;&#35835;&#27963;&#21160;&#30456;&#27604;&#65292;&#31995;&#32479;&#22312;&#21518;&#27979;&#20998;&#25968;&#19978;&#27809;&#26377;&#26174;&#33879;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2310.01420</link><description>&lt;p&gt;
Ruffle&amp;Riley&#65306;&#36208;&#21521;&#33258;&#21160;&#21270;&#30340;&#23545;&#35805;&#36741;&#23548;&#31995;&#32479;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
Ruffle&amp;Riley: Towards the Automated Induction of Conversational Tutoring Systems. (arXiv:2310.01420v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#35825;&#23548;&#36741;&#23548;&#33050;&#26412;&#21644;&#33258;&#21160;&#21327;&#35843;&#33050;&#26412;&#30340;&#26032;&#22411;&#23545;&#35805;&#36741;&#23548;&#31995;&#32479;&#12290;&#22312;&#21021;&#27493;&#30340;&#29992;&#25143;&#30740;&#31350;&#20013;&#65292;&#19982;&#20854;&#20182;&#31616;&#21333;&#30340;&#38382;&#31572;&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#38405;&#35835;&#27963;&#21160;&#30456;&#27604;&#65292;&#31995;&#32479;&#22312;&#21518;&#27979;&#20998;&#25968;&#19978;&#27809;&#26377;&#26174;&#33879;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#36741;&#23548;&#31995;&#32479;&#65288;CTS&#65289;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#25552;&#20379;&#23398;&#20064;&#20307;&#39564;&#12290;&#23427;&#20204;&#34987;&#35748;&#20026;&#33021;&#22815;&#20419;&#36827;&#39640;&#27700;&#24179;&#30340;&#35748;&#30693;&#21442;&#19982;&#65292;&#24182;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#26377;&#30410;&#20110;&#23398;&#20064;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#25776;&#20889;CTS&#20869;&#23481;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#25104;&#26412;&#26159;&#24191;&#27867;&#24212;&#29992;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#19968;&#31181;&#26032;&#22411;&#30340;CTS&#65292;&#23427;&#21033;&#29992;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#65306;&#39318;&#20808;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#20174;&#25945;&#23398;&#25991;&#26412;&#33258;&#21160;&#35825;&#23548;&#20986;&#36741;&#23548;&#33050;&#26412;&#12290;&#20854;&#27425;&#65292;&#35813;&#31995;&#32479;&#36890;&#36807;&#20004;&#20010;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#20154;&#65288;Ruffle&amp;Riley&#65289;&#22312;&#23398;&#20197;&#25945;&#23398;&#30340;&#24418;&#24335;&#20013;&#33258;&#21160;&#21327;&#35843;&#33050;&#26412;&#12290;&#35813;&#31995;&#32479;&#20801;&#35768;&#33258;&#30001;&#23545;&#35805;&#65292;&#36981;&#24490;ITS&#20856;&#22411;&#30340;&#22806;&#37096;/&#20869;&#37096;&#24490;&#29615;&#32467;&#26500;&#12290;&#22312;&#19968;&#20010;&#21021;&#27493;&#30340;&#34987;&#35797;&#32773;&#22312;&#32447;&#29992;&#25143;&#30740;&#31350;&#65288;N = 100&#65289;&#20013;&#65292;&#23558;Ruffle&amp;Riley&#19982;&#26356;&#31616;&#21333;&#30340;&#38382;&#31572;&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#38405;&#35835;&#27963;&#21160;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#21518;&#27979;&#20998;&#25968;&#19978;&#27809;&#26377;&#26174;&#33879;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational tutoring systems (CTSs) offer learning experiences driven by natural language interaction. They are known to promote high levels of cognitive engagement and benefit learning outcomes, particularly in reasoning tasks. Nonetheless, the time and cost required to author CTS content is a major obstacle to widespread adoption. In this paper, we introduce a novel type of CTS that leverages the recent advances in large language models (LLMs) in two ways: First, the system induces a tutoring script automatically from a lesson text. Second, the system automates the script orchestration via two LLM-based agents (Ruffle&amp;Riley) with the roles of a student and a professor in a learning-by-teaching format. The system allows a free-form conversation that follows the ITS-typical outer-/inner-loop structure. In an initial between-subject online user study (N = 100) comparing Ruffle&amp;Riley to simpler QA chatbots and reading activity, we found no significant differences in post-test scores. 
&lt;/p&gt;</description></item><item><title>LatticeGen&#26159;&#19968;&#20010;&#21327;&#20316;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#30495;&#23454;&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#22122;&#22768;&#28151;&#21512;&#24182;&#38544;&#34255;&#22312;&#26684;&#23376;&#20013;&#65292;&#20197;&#20445;&#25252;&#29992;&#25143;&#30340;&#38544;&#31169;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LatticeGen&#33021;&#22815;&#22312;&#38754;&#23545;&#24378;&#25915;&#20987;&#26102;&#25104;&#21151;&#20445;&#25252;&#30495;&#23454;&#29983;&#25104;&#65292;&#36229;&#36807;50%&#30340;&#35821;&#20041;&#20173;&#28982;&#38544;&#34255;&#12290;</title><link>http://arxiv.org/abs/2309.17157</link><description>&lt;p&gt;
LatticeGen: &#19968;&#31181;&#22312;&#20113;&#19978;&#36827;&#34892;&#38544;&#31169;&#24863;&#30693;&#29983;&#25104;&#30340;&#21327;&#20316;&#26694;&#26550;&#65292;&#38544;&#34255;&#29983;&#25104;&#30340;&#25991;&#26412;&#22312;&#26684;&#23376;&#20013;
&lt;/p&gt;
&lt;p&gt;
LatticeGen: A Cooperative Framework which Hides Generated Text in a Lattice for Privacy-Aware Generation on Cloud. (arXiv:2309.17157v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17157
&lt;/p&gt;
&lt;p&gt;
LatticeGen&#26159;&#19968;&#20010;&#21327;&#20316;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#30495;&#23454;&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#22122;&#22768;&#28151;&#21512;&#24182;&#38544;&#34255;&#22312;&#26684;&#23376;&#20013;&#65292;&#20197;&#20445;&#25252;&#29992;&#25143;&#30340;&#38544;&#31169;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LatticeGen&#33021;&#22815;&#22312;&#38754;&#23545;&#24378;&#25915;&#20987;&#26102;&#25104;&#21151;&#20445;&#25252;&#30495;&#23454;&#29983;&#25104;&#65292;&#36229;&#36807;50%&#30340;&#35821;&#20041;&#20173;&#28982;&#38544;&#34255;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#21069;&#30340;&#29992;&#25143;-&#26381;&#21153;&#22120;&#20132;&#20114;&#27169;&#24335;&#20013;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#25552;&#31034;&#29983;&#25104;&#30340;&#36807;&#31243;&#20013;&#65292;&#26381;&#21153;&#22120;&#23436;&#20840;&#25511;&#21046;&#30528;&#29983;&#25104;&#36807;&#31243;&#65292;&#36825;&#20351;&#24471;&#24819;&#35201;&#23558;&#29983;&#25104;&#30340;&#25991;&#26412;&#20445;&#30041;&#32473;&#33258;&#24049;&#30340;&#29992;&#25143;&#27809;&#26377;&#20219;&#20309;&#36873;&#25321;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LatticeGen&#65292;&#19968;&#20010;&#21327;&#20316;&#26694;&#26550;&#65292;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#26381;&#21153;&#22120;&#20173;&#28982;&#22788;&#29702;&#22823;&#37096;&#20998;&#35745;&#31639;&#20219;&#21153;&#65292;&#32780;&#29992;&#25143;&#25511;&#21046;&#37319;&#26679;&#25805;&#20316;&#12290;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#29992;&#25143;&#23558;&#30495;&#23454;&#29983;&#25104;&#24207;&#21015;&#19982;&#22122;&#22768;&#26631;&#35760;&#28151;&#21512;&#65292;&#24182;&#38544;&#34255;&#22312;&#19968;&#20010;&#24102;&#22122;&#22768;&#30340;&#26684;&#23376;&#20013;&#12290;&#32771;&#34385;&#21040;&#26469;&#33258;&#20551;&#35774;&#24694;&#24847;&#26381;&#21153;&#22120;&#30340;&#28508;&#22312;&#25915;&#20987;&#20197;&#21450;&#29992;&#25143;&#22914;&#20309;&#36827;&#34892;&#38450;&#24481;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#37325;&#22797;&#27874;&#26463;&#25628;&#32034;&#25915;&#20987;&#21644;&#28151;&#21512;&#22122;&#22768;&#26041;&#26696;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23558;LatticeGen&#24212;&#29992;&#20110;&#20445;&#25252;&#25552;&#31034;&#21644;&#29983;&#25104;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#34429;&#28982;&#24102;&#22122;&#22768;&#30340;&#26684;&#23376;&#20250;&#38477;&#20302;&#29983;&#25104;&#36136;&#37327;&#65292;&#20294;LatticeGen&#25104;&#21151;&#22320;&#22312;&#24378;&#25915;&#20987;&#19979;&#26174;&#33879;&#20445;&#25252;&#20102;&#30495;&#23454;&#29983;&#25104;&#65288;&#36229;&#36807;50%&#30340;&#35821;&#20041;&#20173;&#28982;&#38544;&#34255;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the current user-server interaction paradigm of prompted generation with large language models (LLM) on cloud, the server fully controls the generation process, which leaves zero options for users who want to keep the generated text to themselves. We propose LatticeGen, a cooperative framework in which the server still handles most of the computation while the user controls the sampling operation. The key idea is that the true generated sequence is mixed with noise tokens by the user and hidden in a noised lattice. Considering potential attacks from a hypothetically malicious server and how the user can defend against it, we propose the repeated beam-search attack and the mixing noise scheme. In our experiments we apply LatticeGen to protect both prompt and generation. It is shown that while the noised lattice degrades generation quality, LatticeGen successfully protects the true generation to a remarkable degree under strong attacks (more than 50% of the semantic remains hidden as 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#25903;&#25345;&#38271;&#19978;&#19979;&#25991;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#36830;&#32493;&#39044;&#35757;&#32451;&#21644;&#25968;&#25454;&#22686;&#24378;&#26469;&#23454;&#29616;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#35821;&#35328;&#24314;&#27169;&#21644;&#30740;&#31350;&#22522;&#20934;&#19978;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#24182;&#19988;&#36890;&#36807;&#25104;&#26412;&#25928;&#30410;&#30340;&#25351;&#23548;&#35843;&#25972;&#31243;&#24207;&#65292;&#24050;&#32463;&#36229;&#36807;&#20102;&#29616;&#26377;&#27169;&#22411;&#22312;&#38271;&#19978;&#19979;&#25991;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.16039</link><description>&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#30340;&#26377;&#25928;&#38271;&#19978;&#19979;&#25991;&#32553;&#25918;
&lt;/p&gt;
&lt;p&gt;
Effective Long-Context Scaling of Foundation Models. (arXiv:2309.16039v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16039
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#25903;&#25345;&#38271;&#19978;&#19979;&#25991;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#36830;&#32493;&#39044;&#35757;&#32451;&#21644;&#25968;&#25454;&#22686;&#24378;&#26469;&#23454;&#29616;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#35821;&#35328;&#24314;&#27169;&#21644;&#30740;&#31350;&#22522;&#20934;&#19978;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#24182;&#19988;&#36890;&#36807;&#25104;&#26412;&#25928;&#30410;&#30340;&#25351;&#23548;&#35843;&#25972;&#31243;&#24207;&#65292;&#24050;&#32463;&#36229;&#36807;&#20102;&#29616;&#26377;&#27169;&#22411;&#22312;&#38271;&#19978;&#19979;&#25991;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#25903;&#25345;&#26368;&#22810;32768&#20010;&#26631;&#35760;&#30340;&#26377;&#25928;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#38271;&#19978;&#19979;&#25991;LLM&#12290;&#25105;&#20204;&#36890;&#36807;&#20174;Llama 2&#36830;&#32493;&#39044;&#35757;&#32451;&#12289;&#22312;&#38271;&#25991;&#26412;&#19978;&#37319;&#26679;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#26469;&#26500;&#24314;&#25105;&#20204;&#30340;&#27169;&#22411;&#31995;&#21015;&#12290;&#25105;&#20204;&#23545;&#35821;&#35328;&#24314;&#27169;&#12289;&#21512;&#25104;&#19978;&#19979;&#25991;&#25506;&#27979;&#20219;&#21153;&#21644;&#21508;&#31181;&#30740;&#31350;&#22522;&#20934;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#12290;&#22312;&#30740;&#31350;&#22522;&#20934;&#19978;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22823;&#22810;&#25968;&#24120;&#35268;&#20219;&#21153;&#19978;&#37117;&#21462;&#24471;&#20102;&#19968;&#33268;&#30340;&#25913;&#36827;&#65292;&#24182;&#22312;&#38271;&#19978;&#19979;&#25991;&#20219;&#21153;&#19978;&#30456;&#23545;&#20110;Llama 2&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36890;&#36807;&#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#30340;&#25351;&#23548;&#35843;&#25972;&#31243;&#24207;&#65292;&#19981;&#38656;&#35201;&#20154;&#24037;&#26631;&#27880;&#30340;&#38271;&#25351;&#23548;&#25968;&#25454;&#65292;70B&#29256;&#26412;&#24050;&#32463;&#22312;&#19968;&#22871;&#38271;&#19978;&#19979;&#25991;&#20219;&#21153;&#20013;&#36229;&#36807;&#20102;gpt-3.5-turbo-16k&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#38500;&#20102;&#36825;&#20123;&#32467;&#26524;&#65292;&#25105;&#20204;&#36824;&#23545;&#25105;&#20204;&#26041;&#27861;&#30340;&#21508;&#20010;&#32452;&#25104;&#37096;&#20998;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;Llama&#30340;&#20301;&#32622;&#32534;&#30721;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#22312;&#24314;&#27169;&#38271;&#20381;&#36182;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#21464;&#37327;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a series of long-context LLMs that support effective context windows of up to 32,768 tokens. Our model series are built through continual pretraining from Llama 2 with longer training sequences and on a dataset where long texts are upsampled. We perform extensive evaluation on language modeling, synthetic context probing tasks, and a wide range of research benchmarks. On research benchmarks, our models achieve consistent improvements on most regular tasks and significant improvements on long-context tasks over Llama 2. Notably, with a cost-effective instruction tuning procedure that does not require human-annotated long instruction data, the 70B variant can already surpass gpt-3.5-turbo-16k's overall performance on a suite of long-context tasks. Alongside these results, we provide an in-depth analysis on the individual components of our method. We delve into Llama's position encodings and discuss its limitation in modeling long dependencies. We also examine the impact of var
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#19968;&#31181;&#20174;&#21512;&#25104;&#35821;&#26009;&#24211;&#20013;&#23398;&#20064;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110;&#24418;&#24335;&#36923;&#36753;&#29702;&#35770;&#30340;&#28436;&#32462;&#35268;&#21017;&#65292;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#26356;&#27867;&#21270;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.07336</link><description>&lt;p&gt;
&#20174;&#21512;&#25104;&#35821;&#26009;&#24211;&#21644;&#24418;&#24335;&#36923;&#36753;&#23398;&#20064;&#28436;&#32462;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic. (arXiv:2308.07336v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07336
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#19968;&#31181;&#20174;&#21512;&#25104;&#35821;&#26009;&#24211;&#20013;&#23398;&#20064;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110;&#24418;&#24335;&#36923;&#36753;&#29702;&#35770;&#30340;&#28436;&#32462;&#35268;&#21017;&#65292;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#26356;&#27867;&#21270;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#20174;&#21512;&#25104;&#35821;&#26009;&#24211;&#20013;&#23398;&#20064;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#26041;&#27861;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#20351;&#29992;&#20102;&#20855;&#20307;&#30340;&#28436;&#32462;&#35268;&#21017;&#26469;&#29983;&#25104;&#28436;&#32462;&#31034;&#20363;&#65292;&#20294;&#36825;&#20123;&#35268;&#21017;&#21463;&#38480;&#25110;&#32773;&#26159;&#20219;&#24847;&#30340;&#12290;&#36825;&#21487;&#33021;&#38480;&#21046;&#20102;&#25152;&#33719;&#24471;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#24182;&#37319;&#29992;&#22522;&#20110;&#24418;&#24335;&#36923;&#36753;&#29702;&#35770;&#30340;&#19968;&#32452;&#33391;&#22909;&#22522;&#30784;&#30340;&#28436;&#32462;&#35268;&#21017;&#65292;&#24403;&#36825;&#20123;&#35268;&#21017;&#20197;&#22810;&#27493;&#26041;&#24335;&#32452;&#21512;&#26102;&#65292;&#21487;&#20197;&#25512;&#23548;&#20986;&#20219;&#20309;&#20854;&#20182;&#28436;&#32462;&#35268;&#21017;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#25552;&#20986;&#30340;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;LMs&#65292;&#21363;$\textbf{FLD}$&#65288;$\textbf{F}$ormal $\textbf{L}$ogic $\textbf{D}$eduction&#65289;&#65292;&#33719;&#24471;&#20102;&#26356;&#20855;&#27867;&#21270;&#24615;&#30340;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#28436;&#32462;&#25512;&#29702;&#35821;&#26009;&#24211;&#21487;&#20197;&#22686;&#24378;LMs&#30340;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#38754;&#65292;&#20197;&#21450;&#19981;&#21516;&#26041;&#38754;&#26080;&#27861;&#22686;&#24378;&#30340;&#26041;&#38754;&#12290;&#26368;&#21518;&#65292;&#22522;&#20110;&#36825;&#20123;&#32467;&#26524;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#23558;&#28436;&#32462;&#35821;&#26009;&#24211;&#25110;&#20854;&#20182;&#26041;&#27861;&#24212;&#29992;&#20110;&#27599;&#20010;&#26041;&#38754;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a synthetic corpus-based approach for language models (LMs) to acquire logical deductive reasoning ability. The previous studies generated deduction examples using specific sets of deduction rules. However, these rules were limited or otherwise arbitrary. This can limit the generalizability of acquired deductive reasoning ability. We rethink this and adopt a well-grounded set of deduction rules based on formal logic theory, which can derive any other deduction rules when combined in a multistep way. We empirically verify that LMs trained on the proposed corpora, which we name $\textbf{FLD}$ ($\textbf{F}$ormal $\textbf{L}$ogic $\textbf{D}$eduction), acquire more generalizable deductive reasoning ability. Furthermore, we identify the aspects of deductive reasoning ability on which deduction corpora can enhance LMs and those on which they cannot. Finally, on the basis of these results, we discuss the future directions for applying deduction corpora or other approaches for each as
&lt;/p&gt;</description></item><item><title>FinGPT&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#23558;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;&#37329;&#34701;&#25968;&#25454;&#27665;&#20027;&#21270;&#20026;&#37329;&#34701;&#22823;&#35821;&#35328;&#27169;&#22411;&#12290;&#23427;&#25552;&#20379;&#20102;&#33258;&#21160;&#25910;&#38598;&#21644;&#25972;&#29702;&#23454;&#26102;&#37329;&#34701;&#25968;&#25454;&#30340;&#21151;&#33021;&#65292;&#35299;&#20915;&#20102;&#37329;&#34701;&#25991;&#26412;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.10485</link><description>&lt;p&gt;
FinGPT: &#23558;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;&#37329;&#34701;&#25968;&#25454;&#27665;&#20027;&#21270;&#20026;&#37329;&#34701;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FinGPT: Democratizing Internet-scale Data for Financial Large Language Models. (arXiv:2307.10485v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10485
&lt;/p&gt;
&lt;p&gt;
FinGPT&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#23558;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;&#37329;&#34701;&#25968;&#25454;&#27665;&#20027;&#21270;&#20026;&#37329;&#34701;&#22823;&#35821;&#35328;&#27169;&#22411;&#12290;&#23427;&#25552;&#20379;&#20102;&#33258;&#21160;&#25910;&#38598;&#21644;&#25972;&#29702;&#23454;&#26102;&#37329;&#34701;&#25968;&#25454;&#30340;&#21151;&#33021;&#65292;&#35299;&#20915;&#20102;&#37329;&#34701;&#25991;&#26412;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#25991;&#26412;&#26041;&#38754;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#36825;&#21487;&#33021;&#20250;&#24443;&#24213;&#25913;&#21464;&#37329;&#34701;&#34892;&#19994;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;LLM&#22312;&#37329;&#34701;&#39046;&#22495;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#65292;&#20027;&#35201;&#21407;&#22240;&#26159;&#19968;&#33324;&#25991;&#26412;&#25968;&#25454;&#19982;&#37329;&#34701;&#25991;&#26412;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#29616;&#26377;&#30340;&#37329;&#34701;&#25991;&#26412;&#25968;&#25454;&#38598;&#25968;&#37327;&#26377;&#38480;&#65288;&#22823;&#23567;&#36739;&#23567;&#65289;&#65292;&#32780;&#31532;&#19968;&#20010;&#37329;&#34701;LLM&#65288;FinLLM&#65289;BloombergGPT&#26159;&#23553;&#38381;&#30340;&#65288;&#21482;&#21457;&#24067;&#20102;&#35757;&#32451;&#26085;&#24535;&#65289;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;Internet&#35268;&#27169;&#30340;&#37329;&#34701;&#25968;&#25454;&#23558;LLM&#27665;&#20027;&#21270;&#65292;&#30001;&#20110;&#25968;&#25454;&#26469;&#28304;&#22810;&#26679;&#12289;&#20449;&#22122;&#27604;&#20302;&#21644;&#26102;&#38388;&#26377;&#25928;&#24615;&#39640;&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#25918;&#24615;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#24320;&#28304;&#21644;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#26694;&#26550;&#8220;&#37329;&#34701;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;FinGPT&#65289;&#8221;&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#25910;&#38598;&#21644;&#25972;&#29702;&#26469;&#33258;&#20114;&#32852;&#32593;&#19978;&#36229;&#36807;34&#20010;&#19981;&#21516;&#26469;&#28304;&#30340;&#23454;&#26102;&#37329;&#34701;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable proficiency in understanding and generating human-like texts, which may potentially revolutionize the finance industry. However, existing LLMs often fall short in the financial field, which is mainly attributed to the disparities between general text data and financial text data. Unfortunately, there is only a limited number of financial text datasets available (quite small size), and BloombergGPT, the first financial LLM (FinLLM), is close-sourced (only the training logs were released). In light of this, we aim to democratize Internet-scale financial data for LLMs, which is an open challenge due to diverse data sources, low signal-to-noise ratio, and high time-validity. To address the challenges, we introduce an open-sourced and data-centric framework, \textit{Financial Generative Pre-trained Transformer (FinGPT)}, that automates the collection and curation of real-time financial data from &gt;34 diverse sources on the Internet, p
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#35757;&#32451;&#31639;&#27861;&#65292;&#21253;&#25324;&#21160;&#24577;&#26550;&#26500;&#65292;&#25209;&#37327;&#36873;&#25321;&#21644;&#39640;&#25928;&#20248;&#21270;&#22120;&#12290;&#28982;&#32780;&#65292;&#22312;&#20351;&#29992;&#36825;&#20123;&#31639;&#27861;&#39044;&#35757;&#32451;&#26102;&#65292;&#30456;&#23545;&#20110;&#22522;&#32447;&#26041;&#27861;&#65292;&#23427;&#20204;&#30340;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#19979;&#28216;&#25910;&#30410;&#28040;&#22833;&#20102;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#21327;&#35758;&#26469;&#36827;&#34892;&#35745;&#31639;&#65292;&#24182;&#37322;&#25918;&#20102;&#20195;&#30721;&#26469;&#20419;&#36827;&#39640;&#25928;&#35757;&#32451;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.06440</link><description>&lt;p&gt;
&#27809;&#26377;&#35757;&#32451;&#23601;&#27809;&#26377;&#25910;&#30410;&#65306;&#37325;&#26032;&#23457;&#35270;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#35757;&#32451;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
No Train No Gain: Revisiting Efficient Training Algorithms For Transformer-based Language Models. (arXiv:2307.06440v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#35757;&#32451;&#31639;&#27861;&#65292;&#21253;&#25324;&#21160;&#24577;&#26550;&#26500;&#65292;&#25209;&#37327;&#36873;&#25321;&#21644;&#39640;&#25928;&#20248;&#21270;&#22120;&#12290;&#28982;&#32780;&#65292;&#22312;&#20351;&#29992;&#36825;&#20123;&#31639;&#27861;&#39044;&#35757;&#32451;&#26102;&#65292;&#30456;&#23545;&#20110;&#22522;&#32447;&#26041;&#27861;&#65292;&#23427;&#20204;&#30340;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#19979;&#28216;&#25910;&#30410;&#28040;&#22833;&#20102;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#21327;&#35758;&#26469;&#36827;&#34892;&#35745;&#31639;&#65292;&#24182;&#37322;&#25918;&#20102;&#20195;&#30721;&#26469;&#20419;&#36827;&#39640;&#25928;&#35757;&#32451;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35757;&#32451;Transformer-based&#35821;&#35328;&#27169;&#22411;&#25152;&#38656;&#30340;&#35745;&#31639;&#37327;&#24613;&#21095;&#22686;&#21152;&#12290;&#36825;&#19968;&#36235;&#21183;&#20419;&#20351;&#30740;&#31350;&#32773;&#20204;&#24320;&#23637;&#20102;&#38024;&#23545;&#39640;&#25928;&#35757;&#32451;&#31639;&#27861;&#30340;&#30740;&#31350;&#65292;&#26088;&#22312;&#27604;&#26631;&#20934;&#35757;&#32451;&#26356;&#24555;&#22320;&#25913;&#21892;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#19979;&#28216;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#19977;&#31867;&#36825;&#26679;&#30340;&#31639;&#27861;&#65306;&#21160;&#24577;&#26550;&#26500;&#65288;&#23618;&#21472;&#12289;&#23618;&#20002;&#24323;&#65289;&#12289;&#25209;&#37327;&#36873;&#25321;&#65288;&#36873;&#25321;&#24615;&#21453;&#21521;&#20256;&#25773;&#12289;RHO&#25439;&#22833;&#65289;&#21644;&#39640;&#25928;&#20248;&#21270;&#22120;&#65288;Lion&#12289;Sophia&#65289;&#12290;&#24403;&#20351;&#29992;&#36825;&#20123;&#26041;&#27861;&#22312;&#22266;&#23450;&#35745;&#31639;&#39044;&#31639;&#19979;&#23545;BERT&#21644;T5&#36827;&#34892;&#39044;&#35757;&#32451;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#30340;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#19979;&#28216;&#25910;&#30410;&#30456;&#23545;&#20110;&#19968;&#20010;&#20855;&#26377;&#23436;&#20840;&#34928;&#20943;&#23398;&#20064;&#29575;&#30340;&#22522;&#32447;&#32780;&#35328;&#20250;&#28040;&#22833;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#35780;&#20272;&#21327;&#35758;&#65292;&#21487;&#20197;&#36890;&#36807;&#23558;&#25152;&#26377;&#35745;&#31639;&#26102;&#38388;&#26144;&#23556;&#21040;&#19968;&#20010;&#31216;&#20026;&#21442;&#32771;&#31995;&#32479;&#26102;&#38388;&#30340;&#21442;&#32771;&#26426;&#22120;&#19978;&#65292;&#22312;&#20219;&#24847;&#26426;&#22120;&#19978;&#36827;&#34892;&#35745;&#31639;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#21327;&#35758;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;&#65292;&#20197;&#40723;&#21169;&#23545;&#39640;&#25928;&#35757;&#32451;&#30340;&#20005;&#26684;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
The computation necessary for training Transformer-based language models has skyrocketed in recent years. This trend has motivated research on efficient training algorithms designed to improve training, validation, and downstream performance faster than standard training. In this work, we revisit three categories of such algorithms: dynamic architectures (layer stacking, layer dropping), batch selection (selective backprop, RHO loss), and efficient optimizers (Lion, Sophia). When pre-training BERT and T5 with a fixed computation budget using such methods, we find that their training, validation, and downstream gains vanish compared to a baseline with a fully-decayed learning rate. We define an evaluation protocol that enables computation to be done on arbitrary machines by mapping all computation time to a reference machine which we call reference system time. We discuss the limitations of our proposed protocol and release our code to encourage rigorous research in efficient training p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#26469;&#35843;&#25972;&#26631;&#27880;&#32773;&#20043;&#38388;&#23384;&#22312;&#30340;&#23610;&#24230;&#19981;&#19968;&#33268;&#65292;&#35299;&#20915;&#20102;&#20027;&#35266;NLP&#20219;&#21153;&#20013;&#26631;&#27880;&#32773;&#20043;&#38388;&#20998;&#27495;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.14770</link><description>&lt;p&gt;
&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#37325;&#26032;&#35843;&#25972;&#20154;&#31867;&#35780;&#20215;
&lt;/p&gt;
&lt;p&gt;
Using Natural Language Explanations to Rescale Human Judgments. (arXiv:2305.14770v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#26469;&#35843;&#25972;&#26631;&#27880;&#32773;&#20043;&#38388;&#23384;&#22312;&#30340;&#23610;&#24230;&#19981;&#19968;&#33268;&#65292;&#35299;&#20915;&#20102;&#20027;&#35266;NLP&#20219;&#21153;&#20013;&#26631;&#27880;&#32773;&#20043;&#38388;&#20998;&#27495;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#24102;&#26469;&#20102;&#38656;&#35201;&#39640;&#36136;&#37327;&#20154;&#26631;&#35760;&#25968;&#25454;&#30340;&#32039;&#36843;&#38656;&#27714;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20154;&#30340;&#21453;&#39304;&#21644;&#35780;&#20272;&#31561;&#36807;&#31243;&#12290;&#19968;&#31181;&#24120;&#35265;&#30340;&#20570;&#27861;&#26159;&#36890;&#36807;&#22810;&#20010;&#20247;&#21253;&#24037;&#20316;&#32773;&#30340;&#20849;&#35782;&#26469;&#26631;&#27880;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#30340;&#26631;&#27880;&#32773;&#21487;&#33021;&#23545;&#26631;&#27880;&#26041;&#26696;&#26377;&#19981;&#21516;&#30340;&#35299;&#37322;&#65292;&#38500;&#38750;&#25509;&#21463;&#20102;&#24191;&#27867;&#30340;&#22521;&#35757;&#65292;&#21542;&#21017;&#23545;&#20110;&#20027;&#35266;&#30340;NLP&#20219;&#21153;&#65292;&#29978;&#33267;&#21463;&#36807;&#35757;&#32451;&#30340;&#19987;&#23478;&#26631;&#27880;&#32773;&#20063;&#21487;&#33021;&#20250;&#20986;&#29616;&#24040;&#22823;&#30340;&#20998;&#27495;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#32454;&#24494;&#24046;&#21035;&#21487;&#20197;&#36890;&#36807;&#39640;&#36136;&#37327;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#36827;&#34892;&#25429;&#25417;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;LLM&#22312;&#23384;&#22312;&#20998;&#27495;&#26102;&#37325;&#26032;&#35843;&#25972;&#22823;&#23567;&#25490;&#24207;&#27880;&#37322;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;Likert&#35780;&#20998;&#21644;&#30456;&#24212;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#36755;&#20837;LLM&#65292;&#24182;&#25552;&#31034;&#23427;&#20135;&#29983;&#19968;&#20010;&#25968;&#23383;&#24471;&#20998;&#12290;&#36825;&#20010;&#24471;&#20998;&#24212;&#35813;&#21453;&#26144;&#27880;&#37322;&#32773;&#23545;&#31034;&#20363;&#30340;&#22522;&#26412;&#35780;&#20272;&#12290;&#35299;&#37322;&#30340;&#23384;&#22312;&#20351;LLM&#33021;&#22815;&#22312;&#23610;&#24230;&#20351;&#29992;&#24046;&#24322;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#20351;&#35780;&#32423;&#22312;&#26631;&#27880;&#32773;&#20043;&#38388;&#21516;&#36136;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise of large language models (LLMs) has brought a critical need for high-quality human-labeled data, particularly for processes like human feedback and evaluation. A common practice is to label data via consensus annotation over the judgments of multiple crowdworkers. However, different annotators may have different interpretations of labeling schemes unless given extensive training, and for subjective NLP tasks, even trained expert annotators can diverge heavily. We show that these nuances can be captured by high quality natural language explanations, and propose a method to rescale ordinal annotation in the presence of disagreement using LLMs. Specifically, we feed Likert ratings and corresponding natural language explanations into an LLM and prompt it to produce a numeric score. This score should reflect the underlying assessment of the example by the annotator. The presence of explanations allows the LLM to homogenize ratings across annotators in spite of scale usage differenc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31185;&#23398;&#35266;&#28857;&#24635;&#32467;&#30340;&#20219;&#21153;&#65292;&#20197;&#21512;&#25104;&#30740;&#31350;&#35770;&#25991;&#35780;&#23457;&#30340;&#20803;&#35780;&#23457;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;ORSUM&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#26816;&#26597;&#34920;&#24341;&#23548;&#36845;&#20195;&#33258;&#26597;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.14647</link><description>&lt;p&gt;
&#22522;&#20110;&#26816;&#26597;&#34920;&#24341;&#23548;&#36845;&#20195;&#33258;&#26597;&#30340;&#20803;&#35780;&#23457;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Meta-review Generation with Checklist-guided Iterative Introspection. (arXiv:2305.14647v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14647
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31185;&#23398;&#35266;&#28857;&#24635;&#32467;&#30340;&#20219;&#21153;&#65292;&#20197;&#21512;&#25104;&#30740;&#31350;&#35770;&#25991;&#35780;&#23457;&#30340;&#20803;&#35780;&#23457;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;ORSUM&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#26816;&#26597;&#34920;&#24341;&#23548;&#36845;&#20195;&#33258;&#26597;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31185;&#23398;&#39046;&#22495;&#20013;&#65292;&#19981;&#21516;&#30340;&#35266;&#28857;&#21487;&#33021;&#20250;&#23548;&#33268;&#23545;&#35780;&#23457;&#24847;&#35265;&#30340;&#20105;&#35758;&#25110;&#20849;&#35782;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#35266;&#28857;&#24635;&#32467;&#25968;&#25454;&#38598;&#20027;&#35201;&#38598;&#20013;&#22312;&#20135;&#21697;&#35780;&#35770;&#39046;&#22495;&#65292;&#27809;&#26377;&#32771;&#34385;&#21040;&#36825;&#31181;&#21487;&#21464;&#24615;&#65292;&#20551;&#35774;&#36755;&#20837;&#30340;&#24847;&#35265;&#26159;&#27809;&#26377;&#20105;&#35758;&#30340;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31185;&#23398;&#35266;&#28857;&#24635;&#32467;&#30340;&#20219;&#21153;&#65292;&#23558;&#30740;&#31350;&#35770;&#25991;&#35780;&#23457;&#21512;&#25104;&#20026;&#20803;&#35780;&#23457;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;ORSUM&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#26469;&#33258;39&#20010;&#20250;&#35758;&#30340;10,989&#31687;&#35770;&#25991;&#20803;&#23457;&#26597;&#21644;40,903&#31687;&#35770;&#25991;&#23457;&#26597;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#26816;&#26597;&#34920;&#24341;&#23548;&#36845;&#20195;&#33258;&#26597;&#30340;&#26041;&#27861;&#65292;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#20960;&#20010;&#38454;&#27573;&#65292;&#24182;&#22312;&#26816;&#26597;&#34920;&#30340;&#25351;&#23548;&#19979;&#36845;&#20195;&#22320;&#23436;&#21892;&#25688;&#35201;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65306;&#65288;1&#65289;&#20154;&#24037;&#25776;&#20889;&#30340;&#25688;&#35201;&#24182;&#19981;&#24635;&#26159;&#21487;&#38752;&#30340;&#65292;&#22240;&#20026;&#35768;&#22810;&#20154;&#24182;&#27809;&#26377;&#36981;&#24490;&#25351;&#21335;&#65292;&#65288;2&#65289;&#20219;&#21153;&#20998;&#35299;&#21644;&#36845;&#20195;&#33258;&#25105;&#23436;&#21892;&#30340;&#32452;&#21512;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Opinions in the scientific domain can be divergent, leading to controversy or consensus among reviewers. However, current opinion summarization datasets mostly focus on product review domains, which do not account for this variability under the assumption that the input opinions are non-controversial. To address this gap, we propose the task of scientific opinion summarization, where research paper reviews are synthesized into meta-reviews. To facilitate this task, we introduce a new ORSUM dataset covering 10,989 paper meta-reviews and 40,903 paper reviews from 39 conferences. Furthermore, we propose the Checklist-guided Iterative Introspection (CGI$^2$) approach, which breaks down the task into several stages and iteratively refines the summary under the guidance of questions from a checklist. We conclude that (1) human-written summaries are not always reliable since many do not follow the guideline, and (2) the combination of task decomposition and iterative self-refinement shows pro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#25193;&#23637;&#21542;&#23450;&#21644;&#35282;&#33394;&#21453;&#36716;&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#36807;&#21435;&#30340;&#32467;&#35770;&#21487;&#33021;&#34987;&#23567;&#22411;&#27979;&#35797;&#38598;&#35823;&#23548;&#12290;&#21516;&#26102;&#65292;BERT&#21644;ALBERT&#31561;&#27169;&#22411;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#21542;&#23450;&#25935;&#24863;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.16445</link><description>&lt;p&gt;
&#26356;&#22823;&#30340;&#25506;&#38024;&#35762;&#36848;&#19981;&#21516;&#30340;&#25925;&#20107;: &#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#25193;&#23637;&#24515;&#29702;&#35821;&#35328;&#23398;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Larger Probes Tell a Different Story: Extending Psycholinguistic Datasets Via In-Context Learning. (arXiv:2303.16445v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#25193;&#23637;&#21542;&#23450;&#21644;&#35282;&#33394;&#21453;&#36716;&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#36807;&#21435;&#30340;&#32467;&#35770;&#21487;&#33021;&#34987;&#23567;&#22411;&#27979;&#35797;&#38598;&#35823;&#23548;&#12290;&#21516;&#26102;&#65292;BERT&#21644;ALBERT&#31561;&#27169;&#22411;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#21542;&#23450;&#25935;&#24863;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#25506;&#27979;&#36890;&#24120;&#29992;&#26469;&#27979;&#35797;&#36825;&#20123;&#27169;&#22411;&#30340;&#29305;&#23450;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#25506;&#27979;&#22522;&#20934;&#23567;&#19988;&#32570;&#20047;&#32479;&#35745;&#21151;&#25928;&#26102;&#65292;&#36825;&#31867;&#30740;&#31350;&#30340;&#32467;&#35770;&#21487;&#33021;&#21463;&#21040;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21463;&#24515;&#29702;&#35821;&#35328;&#23398;&#30740;&#31350;&#21551;&#21457;&#30340;&#21542;&#23450;&#65288;NEG-1500-SIMP&#65289;&#21644;&#35282;&#33394;&#21453;&#36716;&#65288;ROLE-1500&#65289;&#30340;&#26032;&#30340;&#12289;&#26356;&#22823;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;GPT3&#23558;&#29616;&#26377;&#30340;NEG-136&#21644;ROLE-88&#22522;&#20934;&#36827;&#34892;&#20102;&#22823;&#24133;&#25193;&#23637;&#65292;&#23558;&#23427;&#20204;&#30340;&#35268;&#27169;&#20174;18&#21644;44&#20010;&#21477;&#23545;&#20998;&#21035;&#22686;&#21152;&#21040;&#20102;750&#20010;&#12290;&#25105;&#20204;&#36824;&#21019;&#24314;&#20102;&#21478;&#19968;&#20010;&#20351;&#29992;&#22522;&#20110;&#27169;&#26495;&#30340;&#29983;&#25104;&#21019;&#24314;&#30340;&#25193;&#23637;&#21542;&#23450;&#25968;&#25454;&#38598;(NEG-1500-SIMP-TEMP)&#65292;&#23427;&#30001;770&#20010;&#21477;&#23545;&#32452;&#25104;&#12290;&#25105;&#20204;&#22312;&#25193;&#23637;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;22&#20010;&#27169;&#22411;&#65292;&#21457;&#29616;&#27169;&#22411;&#24615;&#33021;&#19982;&#21407;&#22987;&#36739;&#23567;&#22522;&#20934;&#30456;&#27604;&#19979;&#38477;&#20102;20-57%&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;BERT&#21644;ALBERT&#31561;&#27169;&#22411;&#20855;&#26377;&#36739;&#39640;&#30340;&#21542;&#23450;&#25935;&#24863;&#24615;&#65292;&#36825;&#34920;&#26126;&#20197;&#21069;&#30340;&#30740;&#31350;&#32467;&#26524;&#21487;&#33021;&#30001;&#20110;&#36739;&#23567;&#30340;&#27979;&#35797;&#38598;&#32780;&#23384;&#22312;&#35823;&#24046;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#34429;&#28982;GPT3&#29983;&#25104;&#20102;&#25152;&#26377;&#30340;&#23454;&#20363;&#65292;&#20294;&#21477;&#23376;&#30340;&#35821;&#27861;&#36136;&#37327;&#21463;&#21040;&#19968;&#20123;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language model probing is often used to test specific capabilities of these models. However, conclusions from such studies may be limited when the probing benchmarks are small and lack statistical power. In this work, we introduce new, larger datasets for negation (NEG-1500-SIMP) and role reversal (ROLE-1500) inspired by psycholinguistic studies. We dramatically extend existing NEG-136 and ROLE-88 benchmarks using GPT3, increasing their size from 18 and 44 sentence pairs to 750 each. We also create another version of extended negation dataset (NEG-1500-SIMP-TEMP), created using template-based generation. It consists of 770 sentence pairs. We evaluate 22 models on the extended datasets, seeing model performance dip 20-57% compared to the original smaller benchmarks. We observe high levels of negation sensitivity in models like BERT and ALBERT demonstrating that previous findings might have been skewed due to smaller test sets. Finally, we observe that while GPT3 has generated all the ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;ChatGPT&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#21457;&#29616;&#23613;&#31649;&#23427;&#20855;&#26377;&#26356;&#22909;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#20294;&#20173;&#28982;&#32463;&#24120;&#26080;&#27861;&#29983;&#25104;&#36923;&#36753;&#19978;&#27491;&#30830;&#30340;&#39044;&#27979;&#12290;&#22240;&#27492;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#38656;&#35201;&#36827;&#19968;&#27493;&#32771;&#34385;&#65292;&#29305;&#21035;&#26159;&#22312;&#39118;&#38505;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2303.06273</link><description>&lt;p&gt;
ChatGPT&#30340;&#19968;&#33268;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Consistency Analysis of ChatGPT. (arXiv:2303.06273v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;ChatGPT&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#21457;&#29616;&#23613;&#31649;&#23427;&#20855;&#26377;&#26356;&#22909;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#20294;&#20173;&#28982;&#32463;&#24120;&#26080;&#27861;&#29983;&#25104;&#36923;&#36753;&#19978;&#27491;&#30830;&#30340;&#39044;&#27979;&#12290;&#22240;&#27492;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#38656;&#35201;&#36827;&#19968;&#27493;&#32771;&#34385;&#65292;&#29305;&#21035;&#26159;&#22312;&#39118;&#38505;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the consistency issue of ChatGPT and finds that although it has improved language understanding ability, it frequently fails to generate logically correct predictions. Therefore, further consideration is needed for its real-world applications, especially in terms of risk.
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#31572;&#23545;&#35805;&#31995;&#32479;&#65292;&#33258;&#25512;&#20986;&#20197;&#26469;&#24191;&#21463;&#27426;&#36814;&#12290;&#34429;&#28982;&#23427;&#22312;&#27861;&#24459;&#12289;&#21307;&#23398;&#21644;&#37329;&#34701;&#31561;&#39046;&#22495;&#30340;&#19987;&#19994;&#32771;&#35797;&#20013;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#25104;&#32489;&#65292;&#20294;&#20063;&#26377;&#20154;&#23545;&#20854;&#21487;&#38752;&#24615;&#21644;&#20449;&#20219;&#24230;&#34920;&#31034;&#24576;&#30097;&#12290;&#26412;&#25991;&#38024;&#23545;ChatGPT&#22312;&#36923;&#36753;&#19968;&#33268;&#24615;&#26041;&#38754;&#30340;&#21487;&#20449;&#24230;&#36827;&#34892;&#20102;&#35843;&#26597;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#23613;&#31649;ChatGPT&#20284;&#20046;&#20855;&#26377;&#26356;&#22909;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#20294;&#23427;&#20173;&#28982;&#32463;&#24120;&#26080;&#27861;&#29983;&#25104;&#36923;&#36753;&#19978;&#27491;&#30830;&#30340;&#39044;&#27979;&#12290;&#22240;&#27492;&#65292;&#34429;&#28982;ChatGPT&#26159;&#19968;&#31181;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#21644;&#26377;&#21069;&#36884;&#30340;&#26032;&#25216;&#26415;&#65292;&#20294;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#22914;&#26524;&#27809;&#26377;&#32463;&#36807;&#24443;&#24213;&#30340;&#20154;&#24037;&#26816;&#26597;&#65292;&#23427;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#38656;&#35201;&#36827;&#19968;&#27493;&#32771;&#34385;&#65292;&#29305;&#21035;&#26159;&#22312;&#39118;&#38505;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT, a question-and-answer dialogue system based on a large language model, has gained huge popularity since its introduction. Its positive aspects have been reported through many media platforms, and some analyses even showed that ChatGPT achieved a decent grade in professional exams, including the law, medical, and finance domains, adding extra support to the claim that AI now can assist and, even, replace humans in industrial fields. Others, however, doubt its reliability and trustworthiness. In this paper, we investigate ChatGPT's trustworthiness regarding logically consistent behaviours. Our findings suggest that, although ChatGPT seems to achieve an improved language understanding ability, it still fails to generate logically correct predictions frequently. Hence, while it is true that ChatGPT is an impressive and promising new technique, we conclude that its usage in real-world applications without thorough human inspection requires further consideration, especially for risk
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#35757;&#32451;&#30340;&#32452;&#21512;&#22270;&#20687;&#19982;&#25991;&#26412;&#21305;&#37197;&#27169;&#22411; ComCLIP&#65292;&#36890;&#36807;&#23558;&#36755;&#20837;&#22270;&#20687;&#20998;&#35299;&#20026;&#20027;&#20307;&#12289;&#23545;&#35937;&#21644;&#21160;&#20316;&#23376;&#22270;&#20687;&#65292;&#24182;&#32467;&#21512;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#25991;&#26412;&#32534;&#30721;&#22120;&#36827;&#34892;&#36880;&#27493;&#21305;&#37197;&#65292;&#20197;&#35299;&#20915;&#32452;&#21512;&#22270;&#20687;&#19982;&#25991;&#26412;&#21305;&#37197;&#20013;&#30340;&#20266;&#21305;&#37197;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.13854</link><description>&lt;p&gt;
ComCLIP: &#26080;&#38656;&#35757;&#32451;&#30340;&#32452;&#21512;&#22270;&#20687;&#19982;&#25991;&#26412;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
ComCLIP: Training-Free Compositional Image and Text Matching. (arXiv:2211.13854v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#35757;&#32451;&#30340;&#32452;&#21512;&#22270;&#20687;&#19982;&#25991;&#26412;&#21305;&#37197;&#27169;&#22411; ComCLIP&#65292;&#36890;&#36807;&#23558;&#36755;&#20837;&#22270;&#20687;&#20998;&#35299;&#20026;&#20027;&#20307;&#12289;&#23545;&#35937;&#21644;&#21160;&#20316;&#23376;&#22270;&#20687;&#65292;&#24182;&#32467;&#21512;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#25991;&#26412;&#32534;&#30721;&#22120;&#36827;&#34892;&#36880;&#27493;&#21305;&#37197;&#65292;&#20197;&#35299;&#20915;&#32452;&#21512;&#22270;&#20687;&#19982;&#25991;&#26412;&#21305;&#37197;&#20013;&#30340;&#20266;&#21305;&#37197;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#22270;&#20687;&#19982;&#25991;&#26412;&#21305;&#37197;&#26041;&#38754;&#30340;&#24456;&#22909;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23558; CLIP &#36825;&#26679;&#30340;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#20110;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#32452;&#21512;&#22270;&#20687;&#19982;&#25991;&#26412;&#21305;&#37197;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#36825;&#38656;&#35201;&#27169;&#22411;&#29702;&#35299;&#32452;&#21512;&#35789;&#27010;&#24565;&#21644;&#35270;&#35273;&#32452;&#20214;&#12290;&#20026;&#20102;&#23454;&#29616;&#26356;&#22909;&#30340;&#38646;&#26679;&#26412;&#22270;&#20687;&#19982;&#25991;&#26412;&#21305;&#37197;&#20013;&#30340;&#32452;&#21512;&#27867;&#21270;&#33021;&#21147;&#65292;&#26412;&#25991;&#20174;&#22240;&#26524;&#20851;&#31995;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#35813;&#38382;&#39064;&#65306;&#21333;&#20010;&#23454;&#20307;&#30340;&#38169;&#35823;&#35821;&#20041;&#26412;&#36136;&#19978;&#26159;&#23548;&#33268;&#21305;&#37197;&#22833;&#36133;&#30340;&#28151;&#28102;&#22240;&#32032;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#26080;&#38656;&#35757;&#32451;&#8221;&#30340;&#32452;&#21512; CLIP &#27169;&#22411;&#65288;ComCLIP&#65289;&#12290;ComCLIP&#23558;&#36755;&#20837;&#22270;&#20687;&#20998;&#35299;&#20026;&#20027;&#20307;&#12289;&#23545;&#35937;&#21644;&#21160;&#20316;&#23376;&#22270;&#20687;&#65292;&#24182;&#32452;&#21512; CLIP &#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#25991;&#26412;&#32534;&#30721;&#22120;&#65292;&#20197;&#22312;&#32452;&#21512;&#25991;&#26412;&#23884;&#20837;&#21644;&#23376;&#22270;&#20687;&#23884;&#20837;&#20043;&#19978;&#36827;&#34892;&#36880;&#27493;&#21305;&#37197;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;ComCLIP &#21487;&#20197;&#20943;&#36731;&#20266;&#21305;&#37197;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive Language-Image Pretraining (CLIP) has demonstrated great zero-shot performance for matching images and text. However, it is still challenging to adapt vision-lanaguage pretrained models like CLIP to compositional image and text matching -- a more challenging image and text matching task requiring the model understanding of compositional word concepts and visual components. Towards better compositional generalization in zero-shot image and text matching, in this paper, we study the problem from a causal perspective: the erroneous semantics of individual entities are essentially confounders that cause the matching failure. Therefore, we propose a novel \textbf{\textit{training-free}} compositional CLIP model (ComCLIP). ComCLIP disentangles input images into subjects, objects, and action sub-images and composes CLIP's vision encoder and text encoder to perform evolving matching over compositional text embedding and sub-image embeddings. In this way, ComCLIP can mitigate spurio
&lt;/p&gt;</description></item></channel></rss>