<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#24605;&#32771;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#33976;&#39311;&#20013;&#23545;Kullback-Leibler&#25955;&#24230;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#36870;Kullback-Leibler&#21644;&#27491;&#21521;Kullback-Leibler&#25955;&#24230;&#22312;&#20248;&#21270;&#30446;&#26631;&#19978;&#30456;&#20284;&#65292;&#20026;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;Kullback-Leiber&#25955;&#24230;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.02657</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#33976;&#39311;&#20013;&#37325;&#26032;&#24605;&#32771;Kullback-Leibler&#25955;&#24230;
&lt;/p&gt;
&lt;p&gt;
Rethinking Kullback-Leibler Divergence in Knowledge Distillation for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#24605;&#32771;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#33976;&#39311;&#20013;&#23545;Kullback-Leibler&#25955;&#24230;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#36870;Kullback-Leibler&#21644;&#27491;&#21521;Kullback-Leibler&#25955;&#24230;&#22312;&#20248;&#21270;&#30446;&#26631;&#19978;&#30456;&#20284;&#65292;&#20026;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;Kullback-Leiber&#25955;&#24230;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Kullback-Leibler&#25955;&#24230;&#22312;&#30693;&#35782;&#33976;&#39311;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21387;&#32553;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#26412;&#30740;&#31350;&#20174;&#32463;&#39564;&#21644;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#65292;&#22312;LLMs&#30340;&#30693;&#35782;&#33976;&#39311;&#20013;&#65292;&#19982;&#20043;&#21069;&#26029;&#35328;&#30340;&#36870;Kullback-Leibler&#65288;RKL&#65289;&#25955;&#24230;&#23547;&#25214;&#27169;&#24335;&#24182;&#22240;&#27492;&#20248;&#20110;&#23547;&#25214;&#24179;&#22343;&#20540;&#30340;&#27491;&#21521;Kullback-Leibler&#65288;FKL&#65289;&#25955;&#24230;&#30456;&#21453;&#65292;&#23454;&#38469;&#19978;&#22312;&#30693;&#35782;&#33976;&#39311;&#20013;&#37117;&#27809;&#26377;&#20307;&#29616;&#20986;&#23547;&#25214;&#27169;&#24335;&#25110;&#23547;&#25214;&#24179;&#22343;&#20540;&#30340;&#29305;&#24615;&#12290;&#30456;&#21453;&#65292;&#21457;&#29616;RKL&#21644;FKL&#20855;&#26377;&#30456;&#21516;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#24182;&#22312;&#36275;&#22815;&#25968;&#37327;&#30340;&#26102;&#20195;&#20043;&#21518;&#37117;&#20250;&#25910;&#25947;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23454;&#38469;&#32422;&#26463;&#65292;LLMs&#24456;&#23569;&#34987;&#35757;&#32451;&#22914;&#27492;&#22810;&#30340;&#26102;&#20195;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#21457;&#29616;&#65292;RKL&#22312;&#20998;&#24067;&#30340;&#23614;&#37096;&#65292;&#32780;FKL&#22312;&#24320;&#22987;&#26102;&#20195;&#20391;&#37325;&#20110;&#20998;&#24067;&#30340;&#22836;&#37096;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#33258;&#36866;&#24212;Kullback-Leiber&#65288;AKL&#65289;&#25955;&#24230;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33258;&#36866;&#24212;&#22320;&#20998;&#37197;&#26435;&#37325;&#26469;&#32452;&#21512;F
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02657v1 Announce Type: cross  Abstract: Kullback-Leiber divergence has been widely used in Knowledge Distillation (KD) to compress Large Language Models (LLMs). Contrary to prior assertions that reverse Kullback-Leibler (RKL) divergence is mode-seeking and thus preferable over the mean-seeking forward Kullback-Leibler (FKL) divergence, this study empirically and theoretically demonstrates that neither mode-seeking nor mean-seeking properties manifest in KD for LLMs. Instead, RKL and FKL are found to share the same optimization objective and both converge after a sufficient number of epochs. However, due to practical constraints, LLMs are seldom trained for such an extensive number of epochs. Meanwhile, we further find that RKL focuses on the tail part of the distributions, while FKL focuses on the head part at the beginning epochs. Consequently, we propose a simple yet effective Adaptive Kullback-Leiber (AKL) divergence method, which adaptively allocates weights to combine F
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#30456;&#20851;&#24615;&#21028;&#26029;&#30340;&#26597;&#35810;&#24615;&#33021;&#39044;&#27979;&#26694;&#26550;&#65292;&#33021;&#22815;&#35299;&#20915;&#20808;&#21069;&#26041;&#27861;&#20013;&#23545;&#19981;&#21516;IR&#35780;&#20272;&#25351;&#26631;&#20934;&#30830;&#24615;&#21644;&#35299;&#37322;&#24615;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2404.01012</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#30456;&#20851;&#24615;&#21028;&#26029;&#26469;&#39044;&#27979;&#26597;&#35810;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Query Performance Prediction using Relevance Judgments Generated by Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01012
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#30456;&#20851;&#24615;&#21028;&#26029;&#30340;&#26597;&#35810;&#24615;&#33021;&#39044;&#27979;&#26694;&#26550;&#65292;&#33021;&#22815;&#35299;&#20915;&#20808;&#21069;&#26041;&#27861;&#20013;&#23545;&#19981;&#21516;IR&#35780;&#20272;&#25351;&#26631;&#20934;&#30830;&#24615;&#21644;&#35299;&#37322;&#24615;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26597;&#35810;&#24615;&#33021;&#39044;&#27979;&#65288;QPP&#65289;&#26088;&#22312;&#20272;&#35745;&#25628;&#32034;&#31995;&#32479;&#23545;&#26597;&#35810;&#30340;&#26816;&#32034;&#36136;&#37327;&#65292;&#32780;&#26080;&#38656;&#20154;&#24037;&#30456;&#20851;&#24615;&#21028;&#26029;&#12290;&#20808;&#21069;&#30340;QPP&#26041;&#27861;&#36890;&#24120;&#36820;&#22238;&#21333;&#20010;&#26631;&#37327;&#20540;&#65292;&#24182;&#19981;&#35201;&#27714;&#39044;&#27979;&#20540;&#25509;&#36817;&#29305;&#23450;&#30340;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#35780;&#20272;&#25351;&#26631;&#65292;&#20174;&#32780;&#23548;&#33268;&#20197;&#19979;&#26576;&#20123;&#32570;&#28857;&#65306;&#65288;i&#65289;&#21333;&#20010;&#26631;&#37327;&#26080;&#27861;&#20934;&#30830;&#34920;&#31034;&#19981;&#21516;&#30340;IR&#35780;&#20272;&#25351;&#26631;&#65292;&#29305;&#21035;&#26159;&#24403;&#24230;&#37327;&#19981;&#39640;&#24230;&#30456;&#20851;&#26102;&#65292;&#65288;ii&#65289;&#21333;&#20010;&#26631;&#37327;&#38480;&#21046;&#20102;QPP&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#22240;&#20026;&#20165;&#20351;&#29992;&#26631;&#37327;&#26080;&#27861;&#35299;&#37322;QPP&#32467;&#26524;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#30456;&#20851;&#24615;&#21028;&#26029;&#30340;QPP&#26694;&#26550;&#65288;QPP-GenRE&#65289;&#65292;&#23558;QPP&#20998;&#35299;&#20026;&#29420;&#31435;&#30340;&#23376;&#20219;&#21153;&#65292;&#21363;&#23545;&#25490;&#21517;&#21015;&#34920;&#20013;&#27599;&#20010;&#39033;&#30446;&#23545;&#32473;&#23450;&#26597;&#35810;&#30340;&#30456;&#20851;&#24615;&#36827;&#34892;&#21028;&#26029;&#12290;&#36825;&#26679;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#29983;&#25104;&#30340;&#30456;&#20851;&#24615;&#21028;&#26029;&#26469;&#39044;&#27979;&#20219;&#20309;IR&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01012v1 Announce Type: cross  Abstract: Query performance prediction (QPP) aims to estimate the retrieval quality of a search system for a query without human relevance judgments. Previous QPP methods typically return a single scalar value and do not require the predicted values to approximate a specific information retrieval (IR) evaluation measure, leading to certain drawbacks: (i) a single scalar is insufficient to accurately represent different IR evaluation measures, especially when metrics do not highly correlate, and (ii) a single scalar limits the interpretability of QPP methods because solely using a scalar is insufficient to explain QPP results. To address these issues, we propose a QPP framework using automatically generated relevance judgments (QPP-GenRE), which decomposes QPP into independent subtasks of judging the relevance of each item in a ranked list to a given query. This allows us to predict any IR evaluation measure using the generated relevance judgment
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Visual Table&#65292;&#19968;&#31181;&#20026;MLLMs&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#22411;&#35270;&#35273;&#34920;&#31034;&#65292;&#36890;&#36807;&#25552;&#20379;&#23618;&#27425;&#21270;&#25991;&#26412;&#25551;&#36848;&#30340;&#20840;&#38754;&#35270;&#35273;&#22330;&#26223;&#26469;&#24357;&#34917;&#29616;&#26377;&#35270;&#35273;&#34920;&#31034;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2403.18252</link><description>&lt;p&gt;
&#36229;&#36234;&#23884;&#20837;&#65306;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#35270;&#35273;&#34920;&#26684;&#30340;&#20215;&#20540;
&lt;/p&gt;
&lt;p&gt;
Beyond Embeddings: The Promise of Visual Table in Multi-Modal Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18252
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Visual Table&#65292;&#19968;&#31181;&#20026;MLLMs&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#22411;&#35270;&#35273;&#34920;&#31034;&#65292;&#36890;&#36807;&#25552;&#20379;&#23618;&#27425;&#21270;&#25991;&#26412;&#25551;&#36848;&#30340;&#20840;&#38754;&#35270;&#35273;&#22330;&#26223;&#26469;&#24357;&#34917;&#29616;&#26377;&#35270;&#35273;&#34920;&#31034;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#19968;&#30452;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#22522;&#30707;&#65292;&#20174;&#20855;&#26377;&#20154;&#31867;&#27880;&#37322;&#26631;&#31614;&#30340;&#30417;&#30563;&#23398;&#20064;&#21457;&#23637;&#21040;&#23545;&#40784;&#26469;&#33258;&#20114;&#32852;&#32593;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#12290;&#23613;&#31649;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#36817;&#26399;&#30340;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#20381;&#36182;&#30340;&#35270;&#35273;&#34920;&#31034;&#65288;&#22914;CLIP&#23884;&#20837;&#65289;&#36890;&#24120;&#32570;&#20047;&#20851;&#38190;&#30340;&#22806;&#37096;&#19990;&#30028;&#30693;&#35782;&#65292;&#36825;&#23545;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#35270;&#35273;&#25512;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Visual Table&#65292;&#36825;&#26159;&#20026;MLLMs&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#22411;&#35270;&#35273;&#34920;&#31034;&#12290;&#23427;&#25552;&#20379;&#20840;&#38754;&#35270;&#35273;&#22330;&#26223;&#30340;&#23618;&#27425;&#21270;&#25991;&#26412;&#25551;&#36848;&#65292;&#21253;&#25324;&#22330;&#26223;&#25551;&#36848;&#21644;&#28085;&#30422;&#31867;&#21035;&#12289;&#23646;&#24615;&#21644;&#23454;&#20363;&#32423;&#21035;&#30693;&#35782;&#30340;&#22810;&#20010;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#25551;&#36848;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#29983;&#25104;&#22120;&#65292;&#29992;&#20110;&#20174;GPT4V&#30340;&#23567;&#35268;&#27169;&#27880;&#37322;&#20013;&#29983;&#25104;&#35270;&#35273;&#34920;&#26684;&#65292;&#24182;&#35757;&#32451;&#23427;&#12290;&#24191;&#27867;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#36890;&#36807;&#23558;&#29983;&#25104;&#30340;&#35270;&#35273;&#34920;&#26684;&#20316;&#20026;&#39069;&#22806;&#35270;&#35273;&#34920;&#31034;&#65292;&#25105;&#20204;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18252v1 Announce Type: cross  Abstract: Visual representation learning has been a cornerstone in computer vision, evolving from supervised learning with human-annotated labels to aligning image-text pairs from the Internet. Despite recent advancements in multi-modal large language models (MLLMs), the visual representations they rely on, such as CLIP embeddings, often lack access to external world knowledge critical for real-world visual reasoning. In this work, we propose Visual Table, a novel visual representation tailored for MLLMs. It provides hierarchical text descriptions of holistic visual scenes, consisting of a scene description and multiple object-centric descriptions that encompass categories, attributes, and knowledge at instance level. We further develop a scalable generator for visual table generation and train it on small-scale annotations from GPT4V. Extensive evaluations demonstrate that, with generated visual tables as additional visual representations, our 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21644;&#39034;&#24207;&#20915;&#31574;&#21046;&#23450;&#25913;&#36827;&#20102;&#25991;&#26412;&#36755;&#20837;&#31995;&#32479;&#20013;&#30340;&#22312;&#32447;&#33258;&#21160;&#23436;&#25104;&#24314;&#35758;&#65292;&#23558;&#35748;&#30693;&#36127;&#33655;&#32435;&#20837;&#27169;&#22411;&#35757;&#32451;&#30446;&#26631;&#65292;&#22522;&#20110;&#25991;&#26412;&#36755;&#20837;&#36895;&#24230;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2403.15502</link><description>&lt;p&gt;
&#22312;&#32447;&#25991;&#23383;&#33258;&#21160;&#23436;&#25104;&#30340;&#39034;&#24207;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Sequential Decision-Making for Inline Text Autocomplete
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15502
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21644;&#39034;&#24207;&#20915;&#31574;&#21046;&#23450;&#25913;&#36827;&#20102;&#25991;&#26412;&#36755;&#20837;&#31995;&#32479;&#20013;&#30340;&#22312;&#32447;&#33258;&#21160;&#23436;&#25104;&#24314;&#35758;&#65292;&#23558;&#35748;&#30693;&#36127;&#33655;&#32435;&#20837;&#27169;&#22411;&#35757;&#32451;&#30446;&#26631;&#65292;&#22522;&#20110;&#25991;&#26412;&#36755;&#20837;&#36895;&#24230;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#23436;&#25104;&#24314;&#35758;&#26159;&#29616;&#20195;&#25991;&#26412;&#36755;&#20837;&#31995;&#32479;&#20013;&#30340;&#22522;&#26412;&#21151;&#33021;&#65292;&#24212;&#29992;&#20110;&#35832;&#22914;&#28040;&#24687;&#20256;&#36882;&#21644;&#30005;&#23376;&#37038;&#20214;&#25776;&#20889;&#31561;&#39046;&#22495;&#12290;&#36890;&#24120;&#65292;&#33258;&#21160;&#23436;&#25104;&#24314;&#35758;&#26159;&#20174;&#20855;&#26377;&#32622;&#20449;&#24230;&#38408;&#20540;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#38408;&#20540;&#24182;&#27809;&#26377;&#30452;&#25509;&#32771;&#34385;&#29992;&#25143;&#22240;&#26174;&#31034;&#24314;&#35758;&#32780;&#26045;&#21152;&#30340;&#35748;&#30693;&#36127;&#33655;&#65292;&#20363;&#22914;&#20174;&#36755;&#20837;&#20999;&#25442;&#21040;&#38405;&#35835;&#24314;&#35758;&#30340;&#19978;&#19979;&#25991;&#20197;&#21450;&#20915;&#23450;&#26159;&#21542;&#25509;&#21463;&#24314;&#35758;&#30340;&#26102;&#38388;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36890;&#36807;&#39034;&#24207;&#20915;&#31574;&#21046;&#23450;&#26469;&#25913;&#36827;&#25991;&#26412;&#36755;&#20837;&#31995;&#32479;&#20013;&#30340;&#22312;&#32447;&#33258;&#21160;&#23436;&#25104;&#24314;&#35758;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#38543;&#26102;&#38388;&#19982;&#30446;&#26631;&#29992;&#25143;&#30340;&#37325;&#22797;&#20132;&#20114;&#26469;&#23398;&#20064;&#24314;&#35758;&#31574;&#30053;&#12290;&#36825;&#31181;&#21046;&#23450;&#20801;&#35768;&#25105;&#20204;&#23558;&#35748;&#30693;&#36127;&#33655;&#22240;&#32032;&#32435;&#20837;&#35757;&#32451;&#33258;&#21160;&#23436;&#25104;&#27169;&#22411;&#30340;&#30446;&#26631;&#20013;&#65292;&#36890;&#36807;&#22522;&#20110;&#25991;&#26412;&#36755;&#20837;&#36895;&#24230;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#25105;&#20204;&#33719;&#24471;&#20102;&#29702;&#35770;&#26041;&#38754;&#30340;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15502v1 Announce Type: new  Abstract: Autocomplete suggestions are fundamental to modern text entry systems, with applications in domains such as messaging and email composition. Typically, autocomplete suggestions are generated from a language model with a confidence threshold. However, this threshold does not directly take into account the cognitive load imposed on the user by surfacing suggestions, such as the effort to switch contexts from typing to reading the suggestion, and the time to decide whether to accept the suggestion. In this paper, we study the problem of improving inline autocomplete suggestions in text entry systems via a sequential decision-making formulation, and use reinforcement learning to learn suggestion policies through repeated interactions with a target user over time. This formulation allows us to factor cognitive load into the objective of training an autocomplete model, through a reward function based on text entry speed. We acquired theoretica
&lt;/p&gt;</description></item><item><title>NovelQA&#26159;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#38271;&#25991;&#26412;&#19978;&#30340;&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#33521;&#25991;&#23567;&#35828;&#26500;&#24314;&#65292;&#25552;&#20379;&#20102;&#22797;&#26434;&#24615;&#12289;&#38271;&#24230;&#21644;&#21465;&#36848;&#36830;&#36143;&#24615;&#30340;&#29420;&#29305;&#32452;&#21512;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#28145;&#24230;&#25991;&#26412;&#29702;&#35299;&#26041;&#38754;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.12766</link><description>&lt;p&gt;
NovelQA&#65306;&#29992;&#20110;&#38271;&#36317;&#31163;&#23567;&#35828;&#38382;&#31572;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
NovelQA: A Benchmark for Long-Range Novel Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12766
&lt;/p&gt;
&lt;p&gt;
NovelQA&#26159;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#38271;&#25991;&#26412;&#19978;&#30340;&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#33521;&#25991;&#23567;&#35828;&#26500;&#24314;&#65292;&#25552;&#20379;&#20102;&#22797;&#26434;&#24615;&#12289;&#38271;&#24230;&#21644;&#21465;&#36848;&#36830;&#36143;&#24615;&#30340;&#29420;&#29305;&#32452;&#21512;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#28145;&#24230;&#25991;&#26412;&#29702;&#35299;&#26041;&#38754;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#24341;&#20837;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26032;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#22312;&#29702;&#35299;&#21644;&#22788;&#29702;&#38271;&#25991;&#26412;&#20449;&#24687;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24403;&#21069;&#22522;&#20934;&#30340;&#23616;&#38480;&#24615;&#65292;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#30340;&#38271;&#25991;&#26412;&#33021;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;NovelQA&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#27979;&#35797;&#20855;&#26377;&#25193;&#23637;&#25991;&#26412;&#30340;LLM&#33021;&#21147;&#30340;&#22522;&#20934;&#12290;NovelQA&#30001;&#33521;&#25991;&#23567;&#35828;&#26500;&#24314;&#65292;&#25552;&#20379;&#20102;&#22797;&#26434;&#24615;&#12289;&#38271;&#24230;&#21644;&#21465;&#36848;&#36830;&#36143;&#24615;&#30340;&#29420;&#29305;&#32452;&#21512;&#65292;&#20351;&#20854;&#25104;&#20026;&#35780;&#20272;LLM&#20013;&#28145;&#24230;&#25991;&#26412;&#29702;&#35299;&#30340;&#29702;&#24819;&#24037;&#20855;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;NovelQA&#30340;&#35774;&#35745;&#19982;&#26500;&#24314;&#65292;&#31361;&#20986;&#20102;&#20854;&#25163;&#21160;&#27880;&#37322;&#21644;&#22810;&#26679;&#30340;&#38382;&#39064;&#31867;&#22411;&#12290;&#25105;&#20204;&#22312;NovelQA&#19978;&#23545;&#38271;&#25991;&#26412;LLM&#36827;&#34892;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#24615;&#33021;&#30340;&#37325;&#35201;&#35265;&#35299;&#65292;&#29305;&#21035;&#24378;&#35843;&#20102;&#23427;&#20204;&#22312;&#22810;&#36339;&#25512;&#29702;&#12289;&#32454;&#33410;&#23548;&#21521;&#31561;&#26041;&#38754;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12766v1 Announce Type: new  Abstract: The rapid advancement of Large Language Models (LLMs) has introduced a new frontier in natural language processing, particularly in understanding and processing long-context information. However, the evaluation of these models' long-context abilities remains a challenge due to the limitations of current benchmarks. To address this gap, we introduce NovelQA, a benchmark specifically designed to test the capabilities of LLMs with extended texts. Constructed from English novels, NovelQA offers a unique blend of complexity, length, and narrative coherence, making it an ideal tool for assessing deep textual understanding in LLMs. This paper presents the design and construction of NovelQA, highlighting its manual annotation, and diverse question types. Our evaluation of Long-context LLMs on NovelQA reveals significant insights into the models' performance, particularly emphasizing the challenges they face with multi-hop reasoning, detail-orien
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#21442;&#32771;&#25991;&#29486;&#30340;&#25351;&#26631;&#22312;&#38382;&#21477;&#29983;&#25104;&#20013;&#34987;&#25512;&#32763;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#21442;&#32771;&#25991;&#29486;&#30340;&#22810;&#32500;&#26631;&#20934;&#35780;&#20272;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.12242</link><description>&lt;p&gt;
&#22522;&#20110;&#21442;&#32771;&#25991;&#29486;&#30340;&#25351;&#26631;&#22312;&#38382;&#21477;&#29983;&#25104;&#20013;&#34987;&#25512;&#32763;
&lt;/p&gt;
&lt;p&gt;
Reference-based Metrics Disprove Themselves in Question Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12242
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21442;&#32771;&#25991;&#29486;&#30340;&#25351;&#26631;&#22312;&#38382;&#21477;&#29983;&#25104;&#20013;&#34987;&#25512;&#32763;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#21442;&#32771;&#25991;&#29486;&#30340;&#22810;&#32500;&#26631;&#20934;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
BLEU&#21644;BERTScore&#31561;&#22522;&#20110;&#21442;&#32771;&#25991;&#29486;&#30340;&#25351;&#26631;&#34987;&#24191;&#27867;&#29992;&#20110;&#35780;&#20272;&#38382;&#21477;&#29983;&#25104;(QG)&#12290;&#26412;&#30740;&#31350;&#22312;SQuAD&#21644;HotpotQA&#31561;QG&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21457;&#29616;&#65292;&#20351;&#29992;&#20154;&#24037;&#32534;&#20889;&#30340;&#21442;&#32771;&#25991;&#29486;&#24182;&#19981;&#33021;&#20445;&#35777;&#22522;&#20110;&#21442;&#32771;&#25991;&#29486;&#30340;&#25351;&#26631;&#30340;&#26377;&#25928;&#24615;&#12290;&#22823;&#22810;&#25968;QG&#22522;&#20934;&#25968;&#25454;&#38598;&#21482;&#26377;&#19968;&#20010;&#21442;&#32771;&#25991;&#29486;&#65307;&#25105;&#20204;&#22797;&#21046;&#20102;&#27880;&#37322;&#36807;&#31243;&#24182;&#25910;&#38598;&#20102;&#21478;&#19968;&#20010;&#21442;&#32771;&#25991;&#29486;&#12290;&#39044;&#26399;&#22909;&#30340;&#25351;&#26631;&#24212;&#35813;&#23545;&#20154;&#24037;&#39564;&#35777;&#30340;&#38382;&#39064;&#30340;&#35780;&#20998;&#19981;&#20250;&#20302;&#20110;&#29983;&#25104;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#25105;&#20204;&#26032;&#25910;&#38598;&#30340;&#21442;&#32771;&#25991;&#29486;&#19978;&#65292;&#22522;&#20110;&#21442;&#32771;&#25991;&#29486;&#30340;&#25351;&#26631;&#30340;&#32467;&#26524;&#21364;&#35777;&#26126;&#20102;&#36825;&#20123;&#25351;&#26631;&#26412;&#36523;&#26159;&#38169;&#35823;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#21442;&#32771;&#25991;&#29486;&#30340;&#25351;&#26631;&#65292;&#30001;&#22810;&#32500;&#26631;&#20934;&#32452;&#25104;&#65292;&#22914;&#33258;&#28982;&#24615;&#12289;&#21487;&#22238;&#31572;&#24615;&#21644;&#22797;&#26434;&#24615;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36825;&#20123;&#26631;&#20934;&#19981;&#21463;&#38480;&#20110;&#21333;&#20010;&#21442;&#32771;&#38382;&#39064;&#30340;&#21477;&#27861;&#25110;&#35821;&#20041;&#65292;&#35813;&#25351;&#26631;&#20063;&#19981;&#38656;&#35201;&#22810;&#26679;&#21270;&#30340;&#21442;&#32771;&#25991;&#29486;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12242v1 Announce Type: cross  Abstract: Reference-based metrics such as BLEU and BERTScore are widely used to evaluate question generation (QG). In this study, on QG benchmarks such as SQuAD and HotpotQA, we find that using human-written references cannot guarantee the effectiveness of the reference-based metrics. Most QG benchmarks have only one reference; we replicated the annotation process and collect another reference. A good metric was expected to grade a human-validated question no worse than generated questions. However, the results of reference-based metrics on our newly collected reference disproved the metrics themselves. We propose a reference-free metric consisted of multi-dimensional criteria such as naturalness, answerability, and complexity, utilizing large language models. These criteria are not constrained to the syntactic or semantic of a single reference question, and the metric does not require a diverse set of references. Experiments reveal that our met
&lt;/p&gt;</description></item><item><title>HateCOT&#25968;&#25454;&#38598;&#36890;&#36807;GPT-3.5-Turbo&#29983;&#25104;&#35299;&#37322;&#65292;&#23558;52,000&#20010;&#26679;&#26412;&#25968;&#25454;&#29992;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#22312;&#19981;&#21516;&#39046;&#22495;&#21644;&#20219;&#21153;&#19979;&#30340;&#25915;&#20987;&#24615;&#20869;&#23481;&#26816;&#27979;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.11456</link><description>&lt;p&gt;
HateCOT&#65306;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#27867;&#21270;&#25915;&#20987;&#24615;&#35328;&#35770;&#26816;&#27979;&#30340;&#35299;&#37322;&#22686;&#24378;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
HateCOT: An Explanation-Enhanced Dataset for Generalizable Offensive Speech Detection via Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11456
&lt;/p&gt;
&lt;p&gt;
HateCOT&#25968;&#25454;&#38598;&#36890;&#36807;GPT-3.5-Turbo&#29983;&#25104;&#35299;&#37322;&#65292;&#23558;52,000&#20010;&#26679;&#26412;&#25968;&#25454;&#29992;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#22312;&#19981;&#21516;&#39046;&#22495;&#21644;&#20219;&#21153;&#19979;&#30340;&#25915;&#20987;&#24615;&#20869;&#23481;&#26816;&#27979;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#30340;&#26222;&#21450;&#23548;&#33268;&#20102;&#23545;&#25915;&#20987;&#24615;&#20869;&#23481;&#30340;&#21487;&#38752;&#39640;&#25928;&#26816;&#27979;&#30340;&#38656;&#27714;&#65292;&#20026;&#20102;&#38480;&#21046;&#20854;&#26377;&#23475;&#24433;&#21709;&#12290;&#36825;&#23548;&#33268;&#20102;&#22823;&#37327;&#19982;&#26816;&#27979;&#25915;&#20987;&#24615;&#20869;&#23481;&#30456;&#20851;&#30340;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#30340;&#20986;&#29616;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;HateCOT&#65292;&#36825;&#26159;&#20174;&#22810;&#26679;&#21270;&#29616;&#26377;&#26469;&#28304;&#20013;&#25277;&#21462;&#30340;5.2&#19975;&#20010;&#26679;&#26412;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#30001;GPT-3.5-Turbo&#21644;&#20154;&#24037;&#31934;&#24515;&#21046;&#20316;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;HateCOT&#19978;&#20026;&#25915;&#20987;&#24615;&#20869;&#23481;&#26816;&#27979;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#38646;-shot&#21644;few-shot&#35774;&#32622;&#19979;&#26174;&#33879;&#25913;&#36827;&#20102;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#23613;&#31649;&#22312;&#39046;&#22495;&#21644;&#20219;&#21153;&#26041;&#38754;&#23384;&#22312;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11456v1 Announce Type: cross  Abstract: The ubiquitousness of social media has led to the need for reliable and efficient detection of offensive content to limit harmful effects. This has led to a proliferation of datasets and models related to detecting offensive content. While sophisticated models have attained strong performance on individual datasets, these models often do not generalize due to differences between how "offensive content" is conceptualized, and the resulting differences in how these datasets are labeled. In this paper, we introduce HateCOT, a dataset of 52,000 samples drawn from diverse existing sources with explanations generated by GPT-3.5-Turbo and human-curated. We show that pre-training models for the detection of offensive content on HateCOT significantly boots open-sourced Language Models on three benchmark datasets in both zero and few-shot settings, despite differences in domain and task.} We further find that HateCOT enables effective K-shot fin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RetinaQA&#30340;&#26032;&#22411;KBQA&#27169;&#22411;&#65292;&#36890;&#36807;&#22522;&#20110;&#22635;&#22270;&#30340;&#36923;&#36753;&#24418;&#24335;&#26500;&#24314;&#21644;&#20351;&#29992;&#36776;&#21035;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21487;&#22238;&#31572;&#24615;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#21487;&#22238;&#31572;&#21644;&#19981;&#21487;&#22238;&#31572;&#38382;&#39064;&#19978;&#26174;&#30528;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;KBQA&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.10849</link><description>&lt;p&gt;
RETINAQA&#65306;&#19968;&#31181;&#23545;&#21487;&#22238;&#31572;&#21644;&#19981;&#21487;&#22238;&#31572;&#38382;&#39064;&#37117;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#30693;&#35782;&#24211;&#38382;&#31572;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
RETINAQA : A Knowledge Base Question Answering Model Robust to both Answerable and Unanswerable Questions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10849
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RetinaQA&#30340;&#26032;&#22411;KBQA&#27169;&#22411;&#65292;&#36890;&#36807;&#22522;&#20110;&#22635;&#22270;&#30340;&#36923;&#36753;&#24418;&#24335;&#26500;&#24314;&#21644;&#20351;&#29992;&#36776;&#21035;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21487;&#22238;&#31572;&#24615;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#21487;&#22238;&#31572;&#21644;&#19981;&#21487;&#22238;&#31572;&#38382;&#39064;&#19978;&#26174;&#30528;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;KBQA&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#30693;&#35782;&#24211;&#38382;&#31572;&#65288;KBQA&#65289;&#27169;&#22411;&#36890;&#24120;&#20551;&#23450;&#38382;&#39064;&#26159;&#21487;&#22238;&#31572;&#30340;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#36866;&#24403;&#30340;&#35757;&#32451;&#21644;&#38408;&#20540;&#35774;&#23450;&#36866;&#24212;&#26816;&#27979;&#19981;&#21487;&#22238;&#31572;&#24615;&#65292;&#20294;&#36825;&#23558;&#20197;&#29306;&#29298;&#21487;&#22238;&#31572;&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#20026;&#20195;&#20215;&#65292;&#19988;&#27809;&#26377;&#21333;&#19968;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#25152;&#26377;&#19981;&#21487;&#22238;&#31572;&#24615;&#31867;&#21035;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RetinaQA&#30340;&#26032;&#22411;KBQA&#27169;&#22411;&#65292;&#23427;&#23545;&#19981;&#21487;&#22238;&#31572;&#24615;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#23427;&#23558;&#22522;&#20110;KB&#36941;&#21382;&#30340;&#36923;&#36753;&#24418;&#24335;&#26816;&#32034;&#19982;&#22522;&#20110;&#22635;&#22270;&#30340;&#36923;&#36753;&#24418;&#24335;&#26500;&#24314;&#30456;&#32467;&#21512;&#12290;&#36825;&#26377;&#21161;&#20110;&#22788;&#29702;&#20855;&#26377;&#26377;&#25928;&#36923;&#36753;&#24418;&#24335;&#20294;&#22312;&#30693;&#35782;&#24211;&#20013;&#27809;&#26377;&#36890;&#21521;&#31572;&#26696;&#30340;&#25968;&#25454;&#36335;&#24452;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#23427;&#20351;&#29992;&#36776;&#21035;&#32780;&#38750;&#29983;&#25104;&#26469;&#26356;&#22909;&#22320;&#35782;&#21035;&#27809;&#26377;&#26377;&#25928;&#36923;&#36753;&#24418;&#24335;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;RetinaQA&#22312;&#21487;&#22238;&#31572;&#21644;&#19981;&#21487;&#22238;&#31572;&#38382;&#39064;&#19978;&#26174;&#30528;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;KBQA&#27169;&#22411;&#30340;&#35843;&#25972;&#65292;&#24182;&#19988;&#22312;&#19981;&#21487;&#22238;&#31572;&#24615;&#31867;&#21035;&#19978;&#26174;&#31034;&#20986;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10849v1 Announce Type: new  Abstract: State-of-the-art KBQA models assume answerability of questions. Recent research has shown that while these can be adapted to detect unaswerability with suitable training and thresholding, this comes at the expense of accuracy for answerable questions, and no single model is able to handle all categories of unanswerability. We propose a new model for KBQA named RetinaQA that is robust against unaswerability. It complements KB-traversal based logical form retrieval with sketch-filling based logical form construction. This helps with questions that have valid logical forms but no data paths in the KB leading to an answer. Additionally, it uses discrimination instead of generation to better identify questions that do not have valid logical forms. We demonstrate that RetinaQA significantly outperforms adaptations of state-of-the-art KBQA models across answerable and unanswerable questions, while showing robustness across unanswerability categ
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24635;&#32467;&#25991;&#26412;&#26041;&#38754;&#30340;&#33021;&#21147;&#20026;&#25913;&#36827;&#31185;&#23398;&#25991;&#29486;&#20998;&#26512;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;&#65292;Uni-SMART&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;LLMs&#23545;&#22810;&#27169;&#24577;&#31185;&#23398;&#25991;&#29486;&#20869;&#23481;&#29702;&#35299;&#21644;&#20998;&#26512;&#33021;&#21147;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.10301</link><description>&lt;p&gt;
Uni-SMART&#65306;&#36890;&#29992;&#31185;&#23398;&#22810;&#27169;&#24577;&#20998;&#26512;&#21644;&#30740;&#31350;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Uni-SMART: Universal Science Multimodal Analysis and Research Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10301
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24635;&#32467;&#25991;&#26412;&#26041;&#38754;&#30340;&#33021;&#21147;&#20026;&#25913;&#36827;&#31185;&#23398;&#25991;&#29486;&#20998;&#26512;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;&#65292;Uni-SMART&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;LLMs&#23545;&#22810;&#27169;&#24577;&#31185;&#23398;&#25991;&#29486;&#20869;&#23481;&#29702;&#35299;&#21644;&#20998;&#26512;&#33021;&#21147;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31185;&#23398;&#30740;&#31350;&#21450;&#20854;&#24212;&#29992;&#20013;&#65292;&#31185;&#23398;&#25991;&#29486;&#20998;&#26512;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#20511;&#37492;&#20182;&#20154;&#30340;&#24037;&#20316;&#12290;&#28982;&#32780;&#65292;&#31185;&#23398;&#30693;&#35782;&#30340;&#24555;&#36895;&#22686;&#38271;&#23548;&#33268;&#23398;&#26415;&#25991;&#31456;&#25968;&#37327;&#22823;&#24133;&#22686;&#21152;&#65292;&#20351;&#28145;&#20837;&#25991;&#29486;&#20998;&#26512;&#21464;&#24471;&#36234;&#26469;&#36234;&#20855;&#25361;&#25112;&#24615;&#21644;&#32791;&#26102;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;&#12290;&#20197;&#20854;&#22312;&#24635;&#32467;&#25991;&#26412;&#26041;&#38754;&#30340;&#24378;&#22823;&#33021;&#21147;&#32780;&#38395;&#21517;&#65292;LLMs&#34987;&#35270;&#20026;&#25913;&#36827;&#31185;&#23398;&#25991;&#29486;&#20998;&#26512;&#30340;&#28508;&#22312;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;LLMs&#23384;&#22312;&#20854;&#23616;&#38480;&#24615;&#12290;&#31185;&#23398;&#25991;&#29486;&#36890;&#24120;&#21253;&#25324;&#21508;&#31181;&#22810;&#27169;&#24577;&#20803;&#32032;&#65292;&#22914;&#20998;&#23376;&#32467;&#26500;&#12289;&#34920;&#26684;&#21644;&#22270;&#34920;&#65292;&#36825;&#20123;&#23545;&#20197;&#25991;&#26412;&#20026;&#20013;&#24515;&#30340;LLMs&#32780;&#35328;&#38590;&#20197;&#29702;&#35299;&#21644;&#20998;&#26512;&#12290;&#36825;&#20010;&#38382;&#39064;&#25351;&#21521;&#20102;&#36843;&#20999;&#38656;&#35201;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#33021;&#22815;&#20805;&#20998;&#29702;&#35299;&#21644;&#20998;&#26512;&#31185;&#23398;&#25991;&#29486;&#20013;&#30340;&#22810;&#27169;&#24577;&#20869;&#23481;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#38656;&#27714;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10301v1 Announce Type: new  Abstract: In scientific research and its application, scientific literature analysis is crucial as it allows researchers to build on the work of others. However, the fast growth of scientific knowledge has led to a massive increase in scholarly articles, making in-depth literature analysis increasingly challenging and time-consuming. The emergence of Large Language Models (LLMs) has offered a new way to address this challenge. Known for their strong abilities in summarizing texts, LLMs are seen as a potential tool to improve the analysis of scientific literature. However, existing LLMs have their own limits. Scientific literature often includes a wide range of multimodal elements, such as molecular structure, tables, and charts, which are hard for text-focused LLMs to understand and analyze. This issue points to the urgent need for new solutions that can fully understand and analyze multimodal content in scientific literature. To answer this deman
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22522;&#20110;WordNet&#30340;LLMs&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;TaxoLLaMA&#27169;&#22411;&#65292;&#37319;&#29992;4&#20301;&#37327;&#21270;&#21644;LoRA&#25216;&#26415;&#36731;&#37327;&#21270;&#65292;&#22312;&#22810;&#20010;&#35789;&#27719;&#35821;&#20041;&#20219;&#21153;&#20013;&#21462;&#24471;11&#20010;SotA&#32467;&#26524;&#65292;&#19988;&#22312;&#35789;&#27719;&#34164;&#28085;&#21644;&#20998;&#31867;&#23398;&#26500;&#24314;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.09207</link><description>&lt;p&gt;
TaxoLLaMA:&#22522;&#20110;WordNet&#30340;&#27169;&#22411;&#29992;&#20110;&#35299;&#20915;&#22810;&#20010;&#35789;&#27719;&#35821;&#20041;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
TaxoLLaMA: WordNet-based Model for Solving Multiple Lexical Sematic Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09207
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;WordNet&#30340;LLMs&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;TaxoLLaMA&#27169;&#22411;&#65292;&#37319;&#29992;4&#20301;&#37327;&#21270;&#21644;LoRA&#25216;&#26415;&#36731;&#37327;&#21270;&#65292;&#22312;&#22810;&#20010;&#35789;&#27719;&#35821;&#20041;&#20219;&#21153;&#20013;&#21462;&#24471;11&#20010;SotA&#32467;&#26524;&#65292;&#19988;&#22312;&#35789;&#27719;&#34164;&#28085;&#21644;&#20998;&#31867;&#23398;&#26500;&#24314;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;LLMs&#22312;&#25429;&#25417;WordNet&#20013;&#30340;&#35789;&#27719;&#35821;&#20041;&#30693;&#35782;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#20197;LLaMA-2-7b&#27169;&#22411;&#20026;&#20363;&#65292;&#24182;&#22312;&#22810;&#20010;&#35789;&#27719;&#35821;&#20041;&#20219;&#21153;&#19978;&#23545;&#20854;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#20316;&#20026;&#25105;&#20204;&#23454;&#39564;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TaxoLLaMA&#65292;&#21363;&#19968;&#20999;&#30342;&#22312;&#20854;&#20013;&#30340;&#27169;&#22411;&#65292;&#30001;&#20110;4&#20301;&#37327;&#21270;&#21644;LoRA&#32780;&#36731;&#37327;&#21270;&#12290;&#23427;&#22312;&#20998;&#31867;&#23398;&#20016;&#23500;&#21270;&#12289;&#19978;&#20301;&#35789;&#21457;&#29616;&#12289;&#20998;&#31867;&#23398;&#26500;&#24314;&#21644;&#35789;&#27719;&#34164;&#28085;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;11&#20010;SotA&#32467;&#26524;&#65292;16&#20010;&#20219;&#21153;&#20013;&#30340;4&#20010;&#21069;2&#21517;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#23427;&#23637;&#31034;&#20102;&#22312;&#35789;&#27719;&#34164;&#28085;&#21644;&#20998;&#31867;&#23398;&#26500;&#24314;&#19978;&#20855;&#26377;&#38750;&#24120;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#65292;&#26080;&#38656;&#24494;&#35843;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#20854;&#20855;&#26377;&#30340;&#38544;&#34255;&#22810;&#35821;&#35328;&#21644;&#39046;&#22495;&#36866;&#24212;&#33021;&#21147;&#65292;&#20165;&#38656;&#23569;&#37327;&#35843;&#25972;&#25110;&#23569;&#37327;&#23398;&#20064;&#12290;&#25152;&#26377;&#25968;&#25454;&#38598;&#12289;&#20195;&#30721;&#21644;&#27169;&#22411;&#37117;&#21487;&#22312;https://github.com/VityaVitalich/TaxoLLaMA&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09207v1 Announce Type: new  Abstract: In this paper, we explore the capabilities of LLMs in capturing lexical-semantic knowledge from WordNet on the example of the LLaMA-2-7b model and test it on multiple lexical semantic tasks. As the outcome of our experiments, we present TaxoLLaMA, the everything-in-one model, lightweight due to 4-bit quantization and LoRA. It achieves 11 SotA results, 4 top-2 results out of 16 tasks for the Taxonomy Enrichment, Hypernym Discovery, Taxonomy Construction, and Lexical Entailment tasks. Moreover, it demonstrates very strong zero-shot performance on Lexical Entailment and Taxonomy Construction with no fine-tuning. We also explore its hidden multilingual and domain adaptation capabilities with a little tuning or few-shot learning. All datasets, code, and model are available online at https://github.com/VityaVitalich/TaxoLLaMA
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#35821;&#35328;&#27169;&#22411;&#32553;&#25918;&#30740;&#31350;&#20013;&#36807;&#24230;&#35757;&#32451;&#21644;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#35780;&#20272;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.08540</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#19982;&#36807;&#24230;&#35757;&#32451;&#20197;&#21450;&#19979;&#28216;&#20219;&#21153;&#21487;&#38752;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Language models scale reliably with over-training and on downstream tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#35821;&#35328;&#27169;&#22411;&#32553;&#25918;&#30740;&#31350;&#20013;&#36807;&#24230;&#35757;&#32451;&#21644;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#35780;&#20272;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32553;&#25918;&#35268;&#24459;&#23545;&#20110;&#24320;&#21457;&#35821;&#35328;&#27169;&#22411;&#26159;&#26377;&#29992;&#30340;&#25351;&#23548;&#65292;&#20294;&#24403;&#21069;&#30340;&#32553;&#25918;&#30740;&#31350;&#19982;&#35821;&#35328;&#27169;&#22411;&#26368;&#32456;&#35757;&#32451;&#21644;&#35780;&#20272;&#20043;&#38388;&#20173;&#28982;&#23384;&#22312;&#24046;&#36317;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#36807;&#24230;&#35757;&#32451;&#21644;&#22522;&#20110;&#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#36827;&#34892;&#27604;&#36739;&#26041;&#38754;&#30340;&#36825;&#20004;&#20010;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08540v1 Announce Type: new  Abstract: Scaling laws are useful guides for developing language models, but there are still gaps between current scaling studies and how language models are ultimately trained and evaluated. For instance, scaling is usually studied in the compute-optimal training regime (i.e., "Chinchilla optimal" regime); however, in practice, models are often over-trained to reduce inference costs. Moreover, scaling laws mostly predict loss on next-token prediction, but ultimately models are compared based on downstream task performance. In this paper, we address both shortcomings. To do so, we create a testbed of 104 models with 0.011B to 6.9B parameters trained with various numbers of tokens on three data distributions. First, we investigate scaling in the over-trained regime. We fit scaling laws that extrapolate in both the number of model parameters and the ratio of training tokens to parameters. This enables us to predict the validation loss of a 1.4B para
&lt;/p&gt;</description></item><item><title>GPST&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#21477;&#27861;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#20004;&#20010;&#27169;&#22411;&#23454;&#29616;&#23545;&#21407;&#22987;&#25991;&#26412;&#30340;&#39640;&#24182;&#34892;&#39044;&#35757;&#32451;&#65292;&#20811;&#26381;&#20102;&#20043;&#21069;SLM&#20381;&#36182;&#20110;&#40644;&#37329;&#26641;&#21644;&#39034;&#24207;&#35757;&#32451;&#30340;&#38480;&#21046;&#65292;&#23637;&#31034;&#20102;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#20248;&#20110;&#21516;&#31561;&#35268;&#27169;&#30340;GPT-2&#12290;</title><link>https://arxiv.org/abs/2403.08293</link><description>&lt;p&gt;
&#29983;&#25104;&#39044;&#35757;&#32451;&#32467;&#26500;&#21270;Transformer&#65306;&#35268;&#27169;&#21270;&#30340;&#26080;&#30417;&#30563;&#21477;&#27861;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Generative Pretrained Structured Transformers: Unsupervised Syntactic Language Models at Scale
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08293
&lt;/p&gt;
&lt;p&gt;
GPST&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#21477;&#27861;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#20004;&#20010;&#27169;&#22411;&#23454;&#29616;&#23545;&#21407;&#22987;&#25991;&#26412;&#30340;&#39640;&#24182;&#34892;&#39044;&#35757;&#32451;&#65292;&#20811;&#26381;&#20102;&#20043;&#21069;SLM&#20381;&#36182;&#20110;&#40644;&#37329;&#26641;&#21644;&#39034;&#24207;&#35757;&#32451;&#30340;&#38480;&#21046;&#65292;&#23637;&#31034;&#20102;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#20248;&#20110;&#21516;&#31561;&#35268;&#27169;&#30340;GPT-2&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21477;&#27861;&#35821;&#35328;&#27169;&#22411;&#65288;SLM&#65289;&#20197;&#20174;&#24038;&#21040;&#21491;&#30340;&#26041;&#24335;&#36880;&#27493;&#29983;&#25104;&#24102;&#26377;&#20854;&#21477;&#27861;&#26641;&#30340;&#21477;&#23376;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#29983;&#25104;&#39044;&#35757;&#32451;&#32467;&#26500;&#21270;Transformer&#65288;GPST&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#35268;&#27169;&#21270;&#30340;&#26080;&#30417;&#30563;SLM&#65292;&#33021;&#22815;&#22312;&#21407;&#22987;&#25991;&#26412;&#19978;&#20174;&#22836;&#24320;&#22987;&#36827;&#34892;&#39640;&#24182;&#34892;&#39044;&#35757;&#32451;&#12290;GPST&#35268;&#36991;&#20102;&#20043;&#21069;SLM&#30340;&#19968;&#20123;&#38480;&#21046;&#65292;&#27604;&#22914;&#20381;&#36182;&#20110;&#40644;&#37329;&#26641;&#21644;&#39034;&#24207;&#35757;&#32451;&#12290;&#23427;&#30001;&#20004;&#20010;&#32452;&#20214;&#32452;&#25104;&#65292;&#19968;&#20010;&#36890;&#24120;&#30340;SLM&#21463;&#21333;&#21521;&#35821;&#35328;&#24314;&#27169;&#25439;&#22833;&#30340;&#30417;&#30563;&#65292;&#20197;&#21450;&#19968;&#20010;&#39069;&#22806;&#30340;&#32452;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#24341;&#23548;&#21477;&#27861;&#35299;&#26512;&#26641;&#24182;&#35745;&#31639;&#25104;&#20998;&#34920;&#31034;&#65292;&#21463;&#21452;&#21521;&#35821;&#35328;&#24314;&#27169;&#25439;&#22833;&#30340;&#30417;&#30563;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#34920;&#31034;&#26367;&#20195;&#26041;&#26696;&#65292;&#20197;&#23454;&#29616;&#20004;&#20010;&#27169;&#22411;&#30340;&#32852;&#21512;&#24182;&#34892;&#35757;&#32451;&#65292;&#37319;&#29992;&#30828;EM&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#22312;OpenWebText&#19978;&#23545;GPST&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#65292;&#35813;&#35821;&#26009;&#24211;&#21253;&#25324;90&#20159;&#20010;token&#65292;&#24182;&#23637;&#31034;&#20102;GPST&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#30340;&#20248;&#36234;&#24615;&#65292;&#28085;&#30422;&#20102;&#19982;GPT-2&#30456;&#24403;&#35268;&#27169;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08293v1 Announce Type: cross  Abstract: A syntactic language model (SLM) incrementally generates a sentence with its syntactic tree in a left-to-right manner. We present Generative Pretrained Structured Transformers (GPST), an unsupervised SLM at scale capable of being pre-trained from scratch on raw texts with high parallelism. GPST circumvents the limitations of previous SLMs such as relying on gold trees and sequential training. It consists of two components, a usual SLM supervised by a uni-directional language modeling loss, and an additional composition model, which induces syntactic parse trees and computes constituent representations, supervised by a bi-directional language modeling loss. We propose a representation surrogate to enable joint parallel training of the two models in a hard-EM fashion. We pre-train GPST on OpenWebText, a corpus with $9$ billion tokens, and demonstrate the superiority of GPST over GPT-2 with a comparable size in numerous tasks covering bot
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#22823;&#35821;&#26009;&#24211;&#20013;&#34987;&#22823;&#35821;&#35328;&#27169;&#22411;&#22823;&#24133;&#20462;&#25913;&#30340;&#25991;&#26412;&#27604;&#20363;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;AI&#20250;&#35758;&#30340;&#21516;&#34892;&#35780;&#23457;&#20013;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#21457;&#29616;6.5%&#33267;16.9%&#30340;&#25991;&#26412;&#21487;&#33021;&#34987;LLMs&#22823;&#24133;&#20462;&#25913;&#65292;&#25581;&#31034;&#20102;&#29992;&#25143;&#34892;&#20026;&#30340;&#19968;&#20123;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.07183</link><description>&lt;p&gt;
&#22312;&#35268;&#27169;&#19978;&#30417;&#27979;AI&#20462;&#25913;&#30340;&#20869;&#23481;&#65306;AI&#20250;&#35758;&#21516;&#34892;&#35780;&#23457;&#20013;ChatGPT&#24433;&#21709;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Monitoring AI-Modified Content at Scale: A Case Study on the Impact of ChatGPT on AI Conference Peer Reviews
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07183
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#22823;&#35821;&#26009;&#24211;&#20013;&#34987;&#22823;&#35821;&#35328;&#27169;&#22411;&#22823;&#24133;&#20462;&#25913;&#30340;&#25991;&#26412;&#27604;&#20363;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;AI&#20250;&#35758;&#30340;&#21516;&#34892;&#35780;&#23457;&#20013;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#21457;&#29616;6.5%&#33267;16.9%&#30340;&#25991;&#26412;&#21487;&#33021;&#34987;LLMs&#22823;&#24133;&#20462;&#25913;&#65292;&#25581;&#31034;&#20102;&#29992;&#25143;&#34892;&#20026;&#30340;&#19968;&#20123;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#22823;&#35821;&#26009;&#24211;&#20013;&#25991;&#26412;&#21487;&#33021;&#34987;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22823;&#24133;&#20462;&#25913;&#25110;&#29983;&#25104;&#30340;&#37096;&#20998;&#27604;&#20363;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26368;&#22823;&#20284;&#28982;&#27169;&#22411;&#21033;&#29992;&#19987;&#23478;&#25776;&#20889;&#21644;AI&#29983;&#25104;&#30340;&#21442;&#32771;&#25991;&#26412;&#65292;&#20934;&#30830;&#39640;&#25928;&#22320;&#26816;&#26597;&#35821;&#26009;&#24211;&#32423;&#21035;&#19978;&#30495;&#23454;&#19990;&#30028;LLM&#20351;&#29992;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;AI&#20250;&#35758;&#19978;&#31185;&#23398;&#21516;&#34892;&#35780;&#23457;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#35813;&#30740;&#31350;&#21457;&#29983;&#22312;ChatGPT&#21457;&#24067;&#20043;&#21518;&#65292;&#21253;&#25324;ICLR 2024&#12289;NeurIPS 2023&#12289;CoRL 2023&#21644;EMNLP 2023&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36825;&#20123;&#20250;&#35758;&#25552;&#20132;&#30340;&#21516;&#34892;&#35780;&#23457;&#20013;&#65292;6.5%&#33267;16.9%&#30340;&#25991;&#26412;&#21487;&#33021;&#26159;&#30001;LLMs&#22823;&#24133;&#20462;&#25913;&#30340;&#65292;&#21363;&#36229;&#20986;&#25340;&#20889;&#26816;&#26597;&#25110;&#23567;&#24133;&#26356;&#26032;&#30340;&#33539;&#22260;&#12290;&#29983;&#25104;&#25991;&#26412;&#20986;&#29616;&#30340;&#24773;&#20917;&#20026;&#29992;&#25143;&#34892;&#20026;&#25552;&#20379;&#20102;&#35265;&#35299;&#65306;&#22312;&#25253;&#21578;&#20449;&#24515;&#36739;&#20302;&#12289;&#22312;&#25130;&#27490;&#26085;&#26399;&#21069;&#25552;&#20132;&#30340;&#35780;&#35770;&#20197;&#21450;&#20174;&#35780;&#35770;&#20844;&#21496;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07183v1 Announce Type: cross  Abstract: We present an approach for estimating the fraction of text in a large corpus which is likely to be substantially modified or produced by a large language model (LLM). Our maximum likelihood model leverages expert-written and AI-generated reference texts to accurately and efficiently examine real-world LLM-use at the corpus level. We apply this approach to a case study of scientific peer review in AI conferences that took place after the release of ChatGPT: ICLR 2024, NeurIPS 2023, CoRL 2023 and EMNLP 2023. Our results suggest that between 6.5% and 16.9% of text submitted as peer reviews to these conferences could have been substantially modified by LLMs, i.e. beyond spell-checking or minor writing updates. The circumstances in which generated text occurs offer insight into user behavior: the estimated fraction of LLM-generated text is higher in reviews which report lower confidence, were submitted close to the deadline, and from review
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;SPA&#65288;Side Plugin Adaption&#65289;&#30340;&#36731;&#37327;&#32423;&#26550;&#26500;&#65292;&#29992;&#20110;&#22312;&#20005;&#26684;&#30340;&#35774;&#22791;&#35745;&#31639;&#21644;&#20869;&#23384;&#32422;&#26463;&#26465;&#20214;&#19979;&#24555;&#36895;&#36827;&#34892;&#25512;&#26029;&#65292;&#21516;&#26102;&#20445;&#30041;&#38544;&#31169;&#12290;</title><link>https://arxiv.org/abs/2403.07088</link><description>&lt;p&gt;
SPA&#65306;&#38754;&#21521;&#20113;&#31471;&#21644;&#35774;&#22791;&#21327;&#20316;&#30340;&#35745;&#31639;&#21451;&#22909;&#22411;Seq2seq&#20010;&#24615;&#21270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
SPA: Towards A Computational Friendly Cloud-Base and On-Devices Collaboration Seq2seq Personalized Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07088
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;SPA&#65288;Side Plugin Adaption&#65289;&#30340;&#36731;&#37327;&#32423;&#26550;&#26500;&#65292;&#29992;&#20110;&#22312;&#20005;&#26684;&#30340;&#35774;&#22791;&#35745;&#31639;&#21644;&#20869;&#23384;&#32422;&#26463;&#26465;&#20214;&#19979;&#24555;&#36895;&#36827;&#34892;&#25512;&#26029;&#65292;&#21516;&#26102;&#20445;&#30041;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#34920;&#29616;&#20986;&#33394;&#30340;&#33021;&#21147;&#24050;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#38382;&#31572;&#20013;&#24471;&#21040;&#23637;&#31034;&#12290;&#28982;&#32780;&#65292;LLMs&#38656;&#35201;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#22823;&#20869;&#23384;&#25104;&#26412;&#12290;&#21516;&#26102;&#65292;&#24403;&#35757;&#32451;&#25110;&#39044;&#27979;&#36807;&#31243;&#21253;&#21547;&#25935;&#24863;&#20449;&#24687;&#26102;&#65292;LLMs&#21487;&#33021;&#20250;&#23548;&#33268;&#38544;&#31169;&#27844;&#38706;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SPA&#65288;Side Plugin Adaption&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#26550;&#26500;&#65292;&#29992;&#20110;&#24555;&#36895;&#35774;&#22791;&#19978;&#30340;&#25512;&#26029;&#21644;&#22312;&#20005;&#26684;&#30340;&#35774;&#22791;&#35745;&#31639;&#21644;&#20869;&#23384;&#32422;&#26463;&#26465;&#20214;&#19979;&#20445;&#25345;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07088v1 Announce Type: new  Abstract: Large language models(LLMs) have shown its outperforming ability on various tasks and question answering. However, LLMs require high computation cost and large memory cost. At the same time, LLMs may cause privacy leakage when training or prediction procedure contains sensitive information. In this paper, we propose SPA(Side Plugin Adaption), a lightweight architecture for fast on-devices inference and privacy retaining on the constraints of strict on-devices computation and memory constraints. Compared with other on-devices seq2seq generation, SPA could make a fast and stable inference on low-resource constraints, allowing it to obtain cost effiency. Our method establish an interaction between a pretrained LLMs on-cloud and additive parameters on-devices, which could provide the knowledge on both pretrained LLMs and private personal feature.Further more, SPA provides a framework to keep feature-base parameters on private guaranteed but 
&lt;/p&gt;</description></item><item><title>DEEP-ICL &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20219;&#21153;&#23450;&#20041;&#20016;&#23500;&#30340;&#19987;&#23478;&#38598;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#31034;&#33539;&#20013;&#25552;&#21462;&#20219;&#21153;&#23450;&#20041;&#24182;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#31034;&#20363;&#65292;&#23454;&#29616;&#20102;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#20855;&#26377;&#21487;&#27604;&#24615;&#30340;&#24615;&#33021;&#65292;&#31361;&#30772;&#20102;&#20256;&#32479;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.04233</link><description>&lt;p&gt;
DEEP-ICL: &#23450;&#20041;&#20016;&#23500;&#30340;&#19987;&#23478;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DEEP-ICL: Definition-Enriched Experts for Language Model In-Context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04233
&lt;/p&gt;
&lt;p&gt;
DEEP-ICL &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20219;&#21153;&#23450;&#20041;&#20016;&#23500;&#30340;&#19987;&#23478;&#38598;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#31034;&#33539;&#20013;&#25552;&#21462;&#20219;&#21153;&#23450;&#20041;&#24182;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#31034;&#20363;&#65292;&#23454;&#29616;&#20102;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#20855;&#26377;&#21487;&#27604;&#24615;&#30340;&#24615;&#33021;&#65292;&#31361;&#30772;&#20102;&#20256;&#32479;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#20154;&#20204;&#19968;&#30452;&#35748;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#21442;&#25968;&#25968;&#37327;&#39537;&#21160;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#33021;&#21147;&#65292;&#36890;&#36807;&#21033;&#29992;&#20219;&#21153;&#29305;&#23450;&#30340;&#31034;&#33539;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#25361;&#25112;&#36825;&#19968;&#20551;&#35774;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DEEP-ICL&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20219;&#21153;&#23450;&#20041;&#20016;&#23500;&#30340;&#19987;&#23478;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;ICL&#12290; DEEP-ICL&#20174;&#32473;&#23450;&#30340;&#31034;&#33539;&#20013;&#26126;&#30830;&#25552;&#21462;&#20219;&#21153;&#23450;&#20041;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#31034;&#20363;&#29983;&#25104;&#21709;&#24212;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;ICL&#30340;&#25913;&#36827;&#24182;&#19981;&#30452;&#25509;&#20381;&#36182;&#20110;&#27169;&#22411;&#22823;&#23567;&#65292;&#32780;&#22522;&#26412;&#19978;&#28304;&#33258;&#20110;&#29702;&#35299;&#20219;&#21153;&#23450;&#20041;&#21644;&#20219;&#21153;&#24341;&#23548;&#23398;&#20064;&#12290;&#21463;&#21040;&#36825;&#19968;&#21551;&#21457;&#65292;DEEP-ICL&#32467;&#21512;&#20102;&#20004;&#20010;&#20855;&#26377;&#19981;&#21516;&#35282;&#33394;&#30340;3B&#27169;&#22411;&#65288;&#19968;&#20010;&#29992;&#20110;&#24635;&#32467;&#20219;&#21153;&#23450;&#20041;&#65292;&#21478;&#19968;&#20010;&#29992;&#20110;&#23398;&#20064;&#20219;&#21153;&#31034;&#33539;&#65289;&#65292;&#24182;&#23454;&#29616;&#20102;&#19982;LLaMA2-13B&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#20811;&#26381;&#39044;&#35757;&#32451;&#24207;&#21015;&#38271;&#24230;&#65292;&#20248;&#20110;&#20256;&#32479;ICL&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04233v1 Announce Type: cross  Abstract: It has long been assumed that the sheer number of parameters in large language models (LLMs) drives in-context learning (ICL) capabilities, enabling remarkable performance improvements by leveraging task-specific demonstrations. Challenging this hypothesis, we introduce DEEP-ICL, a novel task Definition Enriched ExPert Ensembling methodology for ICL. DEEP-ICL explicitly extracts task definitions from given demonstrations and generates responses through learning task-specific examples. We argue that improvement from ICL does not directly rely on model size, but essentially stems from understanding task definitions and task-guided learning. Inspired by this, DEEP-ICL combines two 3B models with distinct roles (one for concluding task definitions and the other for learning task demonstrations) and achieves comparable performance to LLaMA2-13B. Furthermore, our framework outperforms conventional ICL by overcoming pretraining sequence lengt
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35757;&#32451;&#23545;&#40784;&#22120;&#27169;&#22411;&#26469;&#35299;&#32806;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#23545;&#40784;&#65292;&#20197;&#20943;&#23569;&#23545;&#40784;&#23545;&#24615;&#33021;&#30340;&#28508;&#22312;&#36127;&#38754;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.04224</link><description>&lt;p&gt;
Aligners: &#35299;&#32806;LLMs&#21644;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Aligners: Decoupling LLMs and Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04224
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35757;&#32451;&#23545;&#40784;&#22120;&#27169;&#22411;&#26469;&#35299;&#32806;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#23545;&#40784;&#65292;&#20197;&#20943;&#23569;&#23545;&#40784;&#23545;&#24615;&#33021;&#30340;&#28508;&#22312;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38656;&#35201;&#19982;&#20154;&#31867;&#26399;&#26395;&#23545;&#40784;&#65292;&#20197;&#30830;&#20445;&#23427;&#20204;&#22312;&#22823;&#22810;&#25968;&#24212;&#29992;&#20013;&#30340;&#23433;&#20840;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#23545;&#40784;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#25104;&#26412;&#39640;&#26114;&#65292;&#24182;&#19988;&#38656;&#35201;&#20026;&#27599;&#20010;LLM&#21644;&#23545;&#40784;&#26631;&#20934;&#37325;&#22797;&#36827;&#34892;&#12290;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#35757;&#32451;&#21487;&#20197;&#26681;&#25454;&#38656;&#35201;&#29992;&#20110;&#23545;&#40784;&#32473;&#23450;&#26631;&#20934;&#30340;&#20219;&#20309;LLM&#30340;&#23545;&#40784;&#27169;&#22411;&#26469;&#35299;&#32806;LLMs&#21644;&#23545;&#40784;&#65292;&#20174;&#32780;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#20943;&#23569;&#23545;&#24615;&#33021;&#30340;&#28508;&#22312;&#36127;&#38754;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#23545;&#40784;&#27169;&#22411;&#35757;&#32451;&#37197;&#26041;&#20165;&#20381;&#36182;&#20110;&#20351;&#29992;&#65288;&#25552;&#31034;&#30340;&#65289;LLM &#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#35843;&#25972;&#20197;&#36866;&#24212;&#21508;&#31181;&#23545;&#40784;&#26631;&#20934;&#12290;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#8220;&#36947;&#24503;&#8221;&#23545;&#40784;&#22120;&#24182;&#22312;&#23454;&#39564;&#19978;&#39564;&#35777;&#20854;&#26377;&#25928;&#24615;&#26469;&#38416;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04224v1 Announce Type: cross  Abstract: Large Language Models (LLMs) need to be aligned with human expectations to ensure their safety and utility in most applications. Alignment is challenging, costly, and needs to be repeated for every LLM and alignment criterion. We propose to decouple LLMs and alignment by training aligner models that can be used to align any LLM for a given criteria on an as-needed basis, thus also reducing the potential negative impacts of alignment on performance. Our recipe for training the aligner models solely relies on synthetic data generated with a (prompted) LLM and can be easily adjusted for a variety of alignment criteria. We illustrate our method by training an "ethical" aligner and verify its efficacy empirically.
&lt;/p&gt;</description></item><item><title>&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#25968;&#25454;&#22686;&#24378;&#30340;&#36716;&#21464;&#24615;&#24433;&#21709;&#65292;&#29420;&#29305;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#31361;&#20986;&#20102;LLMs&#22312;&#25968;&#25454;&#22686;&#24378;&#20013;&#24341;&#20837;&#30340;&#33539;&#24335;&#36716;&#21464;&#12290;</title><link>https://arxiv.org/abs/2403.02990</link><description>&lt;p&gt;
&#20351;&#29992;LLMs&#30340;&#25968;&#25454;&#22686;&#24378;&#65306;&#25968;&#25454;&#35270;&#35282;&#12289;&#23398;&#20064;&#33539;&#24335;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation using LLMs: Data Perspectives, Learning Paradigms and Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02990
&lt;/p&gt;
&lt;p&gt;
&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#25968;&#25454;&#22686;&#24378;&#30340;&#36716;&#21464;&#24615;&#24433;&#21709;&#65292;&#29420;&#29305;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#31361;&#20986;&#20102;LLMs&#22312;&#25968;&#25454;&#22686;&#24378;&#20013;&#24341;&#20837;&#30340;&#33539;&#24335;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#39046;&#22495;&#24555;&#36895;&#21457;&#23637;&#20013;&#65292;&#25968;&#25454;&#22686;&#24378;&#65288;DA&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#20851;&#38190;&#25216;&#26415;&#65292;&#36890;&#36807;&#20351;&#35757;&#32451;&#26679;&#26412;&#22810;&#26679;&#21270;&#32780;&#26080;&#38656;&#39069;&#22806;&#25968;&#25454;&#25910;&#38598;&#26469;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#12290;&#26412;&#35843;&#26597;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#25968;&#25454;&#22686;&#24378;&#30340;&#36716;&#21464;&#24615;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#21450;&#20854;&#20182;&#39046;&#22495;&#20013;&#23427;&#20204;&#25552;&#20379;&#30340;&#29420;&#29305;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#20174;&#25968;&#25454;&#35270;&#35282;&#21644;&#23398;&#20064;&#35270;&#35282;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#30340;&#21508;&#31181;&#31574;&#30053;&#65292;&#21253;&#25324;&#23545;LLM&#29983;&#25104;&#25968;&#25454;&#36827;&#34892;&#36827;&#19968;&#27493;&#35757;&#32451;&#30340;&#26032;&#39062;&#23398;&#20064;&#33539;&#24335;&#30340;&#25506;&#32034;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#38416;&#26126;&#20102;&#35813;&#39046;&#22495;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#20174;&#21487;&#25511;&#25968;&#25454;&#22686;&#24378;&#21040;&#22810;&#27169;&#24577;&#25968;&#25454;&#22686;&#24378;&#31561;&#12290;&#26412;&#35843;&#26597;&#31361;&#26174;&#20102;LLMs&#22312;&#25968;&#25454;&#22686;&#24378;&#20013;&#24341;&#20837;&#30340;&#33539;&#24335;&#36716;&#21464;&#65292;&#26088;&#22312;&#20316;&#20026;&#19968;&#31181;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02990v1 Announce Type: cross  Abstract: In the rapidly evolving field of machine learning (ML), data augmentation (DA) has emerged as a pivotal technique for enhancing model performance by diversifying training examples without the need for additional data collection. This survey explores the transformative impact of Large Language Models (LLMs) on DA, particularly addressing the unique challenges and opportunities they present in the context of natural language processing (NLP) and beyond. From a data perspective and a learning perspective, we examine various strategies that utilize Large Language Models for data augmentation, including a novel exploration of learning paradigms where LLM-generated data is used for further training. Additionally, this paper delineates the primary challenges faced in this domain, ranging from controllable data augmentation to multi modal data augmentation. This survey highlights the paradigm shift introduced by LLMs in DA, aims to serve as a 
&lt;/p&gt;</description></item><item><title>&#31934;&#35843;&#35780;&#21028;&#27169;&#22411;&#22312;&#39046;&#22495;&#20869;&#27979;&#35797;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#27867;&#21270;&#33021;&#21147;&#21644;&#20844;&#24179;&#24615;&#19981;&#21450;GPT4&#12290;</title><link>https://arxiv.org/abs/2403.02839</link><description>&lt;p&gt;
&#20316;&#20026;&#35780;&#21028;&#22120;&#30340;LLM&#30340;&#23454;&#35777;&#30740;&#31350;&#65306;&#31934;&#35843;&#35780;&#21028;&#22120;&#27169;&#22411;&#26159;&#29305;&#23450;&#20219;&#21153;&#30340;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Models are Task-specific Classifiers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02839
&lt;/p&gt;
&lt;p&gt;
&#31934;&#35843;&#35780;&#21028;&#27169;&#22411;&#22312;&#39046;&#22495;&#20869;&#27979;&#35797;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#27867;&#21270;&#33021;&#21147;&#21644;&#20844;&#24179;&#24615;&#19981;&#21450;GPT4&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35780;&#20272;&#20854;&#20182;LLM&#36136;&#37327;&#30340;&#36235;&#21183;&#26085;&#30410;&#22686;&#38271;&#12290;&#35768;&#22810;&#30740;&#31350;&#37319;&#29992;&#19987;&#26377;&#30340;&#38381;&#28304;&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;GPT4&#65292;&#20316;&#20026;&#35780;&#20272;&#22120;&#12290;&#21478;&#22806;&#65292;&#20854;&#20182;&#30740;&#31350;&#21033;&#29992;&#24320;&#28304;LLM&#26469;&#31934;&#35843;&#35780;&#21028;&#27169;&#22411;&#20316;&#20026;&#35780;&#20272;&#22120;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#19981;&#21516;&#30340;&#35780;&#21028;&#27169;&#22411;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#23613;&#31649;&#31934;&#35843;&#30340;&#35780;&#21028;&#27169;&#22411;&#22312;&#39046;&#22495;&#20869;&#27979;&#35797;&#38598;&#19978;&#33021;&#22815;&#36798;&#21040;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#29978;&#33267;&#36229;&#36807;GPT4&#65292;&#20294;&#23427;&#20204;&#26412;&#36136;&#19978;&#26159;&#29305;&#23450;&#20219;&#21153;&#30340;&#20998;&#31867;&#22120;&#65292;&#20854;&#27867;&#21270;&#33021;&#21147;&#21644;&#20844;&#24179;&#24615;&#36828;&#20302;&#20110;GPT4&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02839v1 Announce Type: new  Abstract: Recently, there has been a growing trend of utilizing Large Language Model (LLM) to evaluate the quality of other LLMs. Many studies have employed proprietary close-source models, especially GPT4, as the evaluator. Alternatively, other works have fine-tuned judge models based on open-source LLMs as the evaluator. In this study, we conduct an empirical study of different judge models on their evaluation capability. Our findings indicate that although the fine-tuned judge models achieve high accuracy on in-domain test sets, even surpassing GPT4, they are inherently task-specific classifiers, and their generalizability and fairness severely underperform GPT4.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#30693;&#35782;&#25910;&#38598;&#27969;&#27700;&#32447;&#65292;&#21457;&#24067;&#20102;&#19968;&#20010;&#21253;&#21547;20k&#21697;&#29260;&#30340;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#21697;&#29260;&#30693;&#35782;&#24211;KnowPhish&#65292;&#21487;&#29992;&#20110;&#21152;&#24378;&#29616;&#26377;&#22522;&#20110;&#21442;&#32771;&#30340;&#32593;&#32476;&#38035;&#40060;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.02253</link><description>&lt;p&gt;
KnowPhish&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36935;&#35265;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#20197;&#22686;&#24378;&#22522;&#20110;&#21442;&#32771;&#30340;&#32593;&#32476;&#38035;&#40060;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
KnowPhish: Large Language Models Meet Multimodal Knowledge Graphs for Enhancing Reference-Based Phishing Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02253
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#30693;&#35782;&#25910;&#38598;&#27969;&#27700;&#32447;&#65292;&#21457;&#24067;&#20102;&#19968;&#20010;&#21253;&#21547;20k&#21697;&#29260;&#30340;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#21697;&#29260;&#30693;&#35782;&#24211;KnowPhish&#65292;&#21487;&#29992;&#20110;&#21152;&#24378;&#29616;&#26377;&#22522;&#20110;&#21442;&#32771;&#30340;&#32593;&#32476;&#38035;&#40060;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#38035;&#40060;&#25915;&#20987;&#24050;&#32473;&#20010;&#20154;&#21644;&#20225;&#19994;&#36896;&#25104;&#20102;&#37325;&#22823;&#25439;&#22833;&#65292;&#22240;&#27492;&#38656;&#35201;&#24320;&#21457;&#24378;&#22823;&#39640;&#25928;&#30340;&#33258;&#21160;&#32593;&#32476;&#38035;&#40060;&#26816;&#27979;&#26041;&#27861;&#12290;&#22522;&#20110;&#21442;&#32771;&#30340;&#32593;&#32476;&#38035;&#40060;&#26816;&#27979;&#22120;&#65288;RBPDs&#65289;&#24050;&#25104;&#20026;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#23427;&#20204;&#27604;&#36739;&#30446;&#26631;&#32593;&#39029;&#19978;&#30340;&#26631;&#24535;&#19982;&#24050;&#30693;&#26631;&#24535;&#38598;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;RBPDs&#30340;&#20027;&#35201;&#23616;&#38480;&#26159;&#23427;&#20204;&#20381;&#36182;&#20110;&#25163;&#21160;&#26500;&#24314;&#30340;&#21697;&#29260;&#30693;&#35782;&#24211;&#65292;&#36825;&#20351;&#24471;&#26080;&#27861;&#25193;&#23637;&#21040;&#22823;&#37327;&#21697;&#29260;&#65292;&#23548;&#33268;&#30001;&#20110;&#30693;&#35782;&#24211;&#20013;&#21697;&#29260;&#35206;&#30422;&#19981;&#36275;&#32780;&#20986;&#29616;&#20551;&#38452;&#24615;&#38169;&#35823;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#30693;&#35782;&#25910;&#38598;&#27969;&#27700;&#32447;&#65292;&#37319;&#29992;&#35813;&#27969;&#27700;&#32447;&#25105;&#20204;&#25910;&#38598;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#21697;&#29260;&#30693;&#35782;&#24211;KnowPhish&#65292;&#21253;&#21547;20k&#20010;&#21697;&#29260;&#21644;&#27599;&#20010;&#21697;&#29260;&#30340;&#20016;&#23500;&#20449;&#24687;&#12290;KnowPhish&#21487;&#20197;&#29992;&#26469;&#20197;&#21363;&#25554;&#21363;&#29992;&#30340;&#26041;&#24335;&#25552;&#21319;&#29616;&#26377;RBPDs&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02253v1 Announce Type: cross  Abstract: Phishing attacks have inflicted substantial losses on individuals and businesses alike, necessitating the development of robust and efficient automated phishing detection approaches. Reference-based phishing detectors (RBPDs), which compare the logos on a target webpage to a known set of logos, have emerged as the state-of-the-art approach. However, a major limitation of existing RBPDs is that they rely on a manually constructed brand knowledge base, making it infeasible to scale to a large number of brands, which results in false negative errors due to the insufficient brand coverage of the knowledge base. To address this issue, we propose an automated knowledge collection pipeline, using which we collect and release a large-scale multimodal brand knowledge base, KnowPhish, containing 20k brands with rich information about each brand. KnowPhish can be used to boost the performance of existing RBPDs in a plug-and-play manner. A second 
&lt;/p&gt;</description></item><item><title>SciAssess&#20171;&#32461;&#20102;&#19968;&#20010;&#19987;&#20026;&#28145;&#24230;&#20998;&#26512;&#31185;&#23398;&#25991;&#29486;&#32780;&#35774;&#35745;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#20840;&#38754;&#35780;&#20272;LLMs&#22312;&#31185;&#23398;&#39046;&#22495;&#35760;&#24518;&#12289;&#29702;&#35299;&#21644;&#20998;&#26512;&#33021;&#21147;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01976</link><description>&lt;p&gt;
SciAssess&#65306;&#22522;&#20934;&#27979;&#35797;LLM&#22312;&#31185;&#23398;&#25991;&#29486;&#20998;&#26512;&#20013;&#30340;&#29087;&#32451;&#31243;&#24230;
&lt;/p&gt;
&lt;p&gt;
SciAssess: Benchmarking LLM Proficiency in Scientific Literature Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01976
&lt;/p&gt;
&lt;p&gt;
SciAssess&#20171;&#32461;&#20102;&#19968;&#20010;&#19987;&#20026;&#28145;&#24230;&#20998;&#26512;&#31185;&#23398;&#25991;&#29486;&#32780;&#35774;&#35745;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#20840;&#38754;&#35780;&#20272;LLMs&#22312;&#31185;&#23398;&#39046;&#22495;&#35760;&#24518;&#12289;&#29702;&#35299;&#21644;&#20998;&#26512;&#33021;&#21147;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01976v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032; &#25277;&#35937;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#31361;&#30772;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#65292;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#21033;&#29992;&#36825;&#20123;&#25216;&#26415;&#36827;&#34892;&#32454;&#33268;&#31185;&#23398;&#25991;&#29486;&#20998;&#26512;&#30340;&#20852;&#36259;&#28608;&#22686;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#26410;&#33021;&#20805;&#20998;&#35780;&#20272;LLMs&#22312;&#31185;&#23398;&#39046;&#22495;&#30340;&#29087;&#32451;&#31243;&#24230;&#65292;&#29305;&#21035;&#26159;&#22312;&#28041;&#21450;&#22797;&#26434;&#29702;&#35299;&#21644;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SciAssess&#65292;&#19968;&#20010;&#19987;&#20026;&#28145;&#24230;&#20998;&#26512;&#31185;&#23398;&#25991;&#29486;&#32780;&#35774;&#35745;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#20840;&#38754;&#35780;&#20272;LLMs&#30340;&#26377;&#25928;&#24615;&#12290;SciAssess&#19987;&#27880;&#20110;&#35780;&#20272;LLMs&#22312;&#31185;&#23398;&#32972;&#26223;&#19979;&#35760;&#24518;&#12289;&#29702;&#35299;&#21644;&#20998;&#26512;&#30340;&#33021;&#21147;&#12290;&#23427;&#21253;&#25324;&#26469;&#33258;&#19981;&#21516;&#31185;&#23398;&#39046;&#22495;&#30340;&#20195;&#34920;&#24615;&#20219;&#21153;&#65292;&#22914;&#19968;&#33324;&#21270;&#23398;&#12289;&#26377;&#26426;&#26448;&#26009;&#21644;&#21512;&#37329;&#26448;&#26009;&#12290;&#20005;&#26684;&#30340;&#36136;&#37327;&#25511;&#21046;&#25514;&#26045;&#30830;&#20445;&#20102;&#20854;&#22312;&#27491;&#30830;&#24615;&#12289;&#21311;&#21517;&#21270;&#21644;&#22797;&#21046;&#26041;&#38754;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01976v1 Announce Type: new  Abstract: Recent breakthroughs in Large Language Models (LLMs) have revolutionized natural language understanding and generation, igniting a surge of interest in leveraging these technologies for the nuanced field of scientific literature analysis. Existing benchmarks, however, inadequately evaluate the proficiency of LLMs in the scientific domain, especially in scenarios involving complex comprehension and multimodal data. In response, we introduced SciAssess, a benchmark tailored for the in-depth analysis of scientific literature, crafted to provide a thorough assessment of LLMs' efficacy. SciAssess focuses on evaluating LLMs' abilities in memorization, comprehension, and analysis within scientific contexts. It includes representative tasks from diverse scientific fields, such as general chemistry, organic materials, and alloy materials. And rigorous quality control measures ensure its reliability in terms of correctness, anonymization, and copy
&lt;/p&gt;</description></item><item><title>DiaHalu&#26159;&#31532;&#19968;&#20010;&#23545;&#35805;&#32423;&#24187;&#35273;&#35780;&#20272;&#22522;&#20934;&#65292;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23545;&#35805;&#32423;&#21035;&#19978;&#30340;&#24187;&#35273;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2403.00896</link><description>&lt;p&gt;
DiaHalu&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#35805;&#32423;&#24187;&#35273;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00896
&lt;/p&gt;
&lt;p&gt;
DiaHalu&#26159;&#31532;&#19968;&#20010;&#23545;&#35805;&#32423;&#24187;&#35273;&#35780;&#20272;&#22522;&#20934;&#65292;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23545;&#35805;&#32423;&#21035;&#19978;&#30340;&#24187;&#35273;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#26368;&#36817;&#20960;&#24180;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#65292;&#24187;&#35273;&#38382;&#39064;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#26377;&#35768;&#22810;&#22522;&#20934;&#34987;&#25552;&#20986;&#26469;&#26816;&#27979;&#36825;&#31181;&#24187;&#35273;&#12290;&#28982;&#32780;&#65292;&#20854;&#20013;&#19968;&#20123;&#22522;&#20934;&#19981;&#26159;&#30001;LLMs&#33258;&#28982;&#29983;&#25104;&#30340;&#65292;&#32780;&#26159;&#26377;&#24847;&#24341;&#21457;&#30340;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#22522;&#20934;&#20165;&#20851;&#27880;&#20107;&#23454;&#19978;&#30340;&#24187;&#35273;&#65292;&#32780;&#24573;&#35270;&#20102;&#24544;&#23454;&#24230;&#30340;&#24187;&#35273;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#22312;LLMs&#26102;&#20195;&#23545;&#35805;&#27169;&#24335;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#30446;&#21069;&#30340;&#22522;&#20934;&#20165;&#38598;&#20013;&#22312;&#21477;&#23376;&#32423;&#21644;&#27573;&#33853;&#32423;&#30340;&#24187;&#35273;&#19978;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986; DiaHalu&#65292;&#36825;&#26159;&#25105;&#20204;&#25152;&#30693;&#30340;&#31532;&#19968;&#20010;&#23545;&#35805;&#32423;&#24187;&#35273;&#35780;&#20272;&#22522;&#20934;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#25910;&#38598;&#30340;&#20027;&#39064;&#38598;&#25104;&#21040;&#31995;&#32479;&#25552;&#31034;&#20013;&#65292;&#20419;&#36827;&#20004;&#20010;ChatGPT3.5&#20043;&#38388;&#30340;&#23545;&#35805;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25163;&#21160;&#20462;&#25913;&#19981;&#31526;&#21512;&#20154;&#31867;&#35821;&#35328;&#32422;&#23450;&#30340;&#20869;&#23481;&#65292;&#28982;&#21518;&#35753;LLMs&#37325;&#26032;&#29983;&#25104;&#65292;&#27169;&#25311;&#30495;&#23454;&#30340;&#20154;&#31867;-
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00896v1 Announce Type: cross  Abstract: Since large language models (LLMs) achieve significant success in recent years, the hallucination issue remains a challenge, numerous benchmarks are proposed to detect the hallucination. Nevertheless, some of these benchmarks are not naturally generated by LLMs but are intentionally induced. Also, many merely focus on the factuality hallucination while ignoring the faithfulness hallucination. Additionally, although dialogue pattern is more widely utilized in the era of LLMs, current benchmarks only concentrate on sentence-level and passage-level hallucination. In this study, we propose DiaHalu, the first dialogue-level hallucination evaluation benchmark to our knowledge. Initially, we integrate the collected topics into system prompts and facilitate a dialogue between two ChatGPT3.5. Subsequently, we manually modify the contents that do not adhere to human language conventions and then have LLMs re-generate, simulating authentic human-
&lt;/p&gt;</description></item><item><title>ROME&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#35760;&#24518;&#21644;&#38750;&#35760;&#24518;&#26679;&#26412;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35760;&#24518;&#21270;&#65292;&#36825;&#26377;&#21161;&#20110;&#22312;&#19981;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20102;&#35299;&#27169;&#22411;&#35760;&#24518;&#30340;&#27934;&#23519;&#21644;&#24433;&#21709;&#22240;&#32032;&#12290;</title><link>https://arxiv.org/abs/2403.00510</link><description>&lt;p&gt;
ROME: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25991;&#26412;&#12289;&#27010;&#29575;&#21644;&#38544;&#34255;&#29366;&#24577;&#30340;&#35760;&#24518;&#27934;&#23519;
&lt;/p&gt;
&lt;p&gt;
ROME: Memorization Insights from Text, Probability and Hidden State in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00510
&lt;/p&gt;
&lt;p&gt;
ROME&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#35760;&#24518;&#21644;&#38750;&#35760;&#24518;&#26679;&#26412;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35760;&#24518;&#21270;&#65292;&#36825;&#26377;&#21161;&#20110;&#22312;&#19981;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20102;&#35299;&#27169;&#22411;&#35760;&#24518;&#30340;&#27934;&#23519;&#21644;&#24433;&#21709;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35760;&#24518;&#21270;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24314;&#31435;&#20102;&#29992;&#20110;&#37327;&#21270;&#35760;&#24518;&#30340;&#25351;&#26631;&#65292;&#25506;&#35752;&#20102;&#21508;&#31181;&#24433;&#21709;&#22240;&#32032;&#65292;&#22914;&#25968;&#25454;&#22797;&#21046;&#12289;&#27169;&#22411;&#22823;&#23567;&#21644;&#25552;&#31034;&#38271;&#24230;&#65292;&#24182;&#36890;&#36807;&#23558;&#27169;&#22411;&#36755;&#20986;&#19982;&#35757;&#32451;&#35821;&#26009;&#24211;&#36827;&#34892;&#27604;&#36739;&#26469;&#35780;&#20272;&#35760;&#24518;&#21270;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#35821;&#26009;&#24211;&#35268;&#27169;&#24040;&#22823;&#19988;&#20854;&#39044;&#22788;&#29702;&#32791;&#26102;&#12290;&#20026;&#20102;&#22312;&#19981;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#25506;&#32034;&#35760;&#24518;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ROME&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#27492;&#26041;&#27861;&#20013;&#65292;&#36890;&#36807;&#27604;&#36739;&#35760;&#24518;&#21270;&#21644;&#38750;&#35760;&#24518;&#21270;&#26679;&#26412;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#25506;&#32034;&#35760;&#24518;&#21270;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#27169;&#22411;&#39318;&#20808;&#23558;&#36873;&#23450;&#30340;&#26679;&#26412;&#20998;&#20026;&#35760;&#24518;&#21270;&#21644;&#38750;&#35760;&#24518;&#21270;&#32452;&#65292;&#24182;&#36890;&#36807;&#25991;&#26412;&#12289;&#27010;&#29575;&#21644;&#38544;&#34255;&#29366;&#24577;&#30340;&#35265;&#35299;&#27604;&#36739;&#36825;&#20004;&#32452;&#20013;&#30340;&#28436;&#31034;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#21253;&#25324;&#35789;&#38271;&#12289;&#35789;&#24615;&#12289;&#35789;&#39057;&#12289;&#22343;&#20540;&#21644;&#26041;&#24046;&#22312;&#20869;&#30340;&#22240;&#32032;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00510v1 Announce Type: cross  Abstract: Probing the memorization of large language models holds significant importance. Previous works have established metrics for quantifying memorization, explored various influencing factors, such as data duplication, model size, and prompt length, and evaluated memorization by comparing model outputs with training corpora. However, the training corpora are of enormous scale and its pre-processing is time-consuming. To explore memorization without accessing training data, we propose a novel approach, named ROME, wherein memorization is explored by comparing disparities across memorized and non-memorized. Specifically, models firstly categorize the selected samples into memorized and non-memorized groups, and then comparing the demonstrations in the two groups from the insights of text, probability, and hidden state. Experimental findings show the disparities in factors including word length, part-of-speech, word frequency, mean and varianc
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#32534;&#36753;&#35821;&#35328;&#27169;&#22411;&#20013;&#20559;&#35265;&#25918;&#22823;&#30340;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;Seesaw-CF&#65292;&#39318;&#27425;&#28145;&#20837;&#30740;&#31350;&#20102;&#26435;&#37325;&#32534;&#36753;&#26041;&#27861;&#23545;&#27169;&#22411;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.00180</link><description>&lt;p&gt;
"Flex Tape&#19981;&#33021;&#20462;&#22797;&#36825;&#20010;": &#32534;&#36753;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#21644;&#38169;&#35823;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
"Flex Tape Can't Fix That": Bias and Misinformation in Edited Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00180
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#32534;&#36753;&#35821;&#35328;&#27169;&#22411;&#20013;&#20559;&#35265;&#25918;&#22823;&#30340;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;Seesaw-CF&#65292;&#39318;&#27425;&#28145;&#20837;&#30740;&#31350;&#20102;&#26435;&#37325;&#32534;&#36753;&#26041;&#27861;&#23545;&#27169;&#22411;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#32534;&#36753;&#24050;&#32463;&#25104;&#20026;&#26356;&#26032;&#23384;&#20648;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#30340;&#19968;&#31181;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#22312;&#32534;&#36753;&#24212;&#29992;&#21518;&#65292;&#27169;&#22411;&#32534;&#36753;&#21487;&#33021;&#20250;&#20135;&#29983;&#24847;&#24819;&#19981;&#21040;&#30340;&#21518;&#26524;&#65306;&#19982;&#32534;&#36753;&#26080;&#20851;&#30340;&#20449;&#24687;&#20063;&#21487;&#33021;&#34987;&#26356;&#25913;&#65292;&#24182;&#19988;&#27169;&#22411;&#30340;&#20854;&#20182;&#19968;&#33324;&#34892;&#20026;&#21487;&#33021;&#34987;&#38169;&#35823;&#22320;&#25913;&#21464;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#22914;&#20309;&#24847;&#22806;&#22320;&#21152;&#21095;&#20102;&#27169;&#22411;&#21518;&#32534;&#36753;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;Seesaw-CF&#65292;&#29992;&#20110;&#34913;&#37327;&#27169;&#22411;&#32534;&#36753;&#30340;&#20559;&#35265;&#30456;&#20851;&#20260;&#23475;&#65292;&#24182;&#36827;&#34892;&#20102;&#39318;&#27425;&#28145;&#20837;&#30740;&#31350;&#19981;&#21516;&#26435;&#37325;&#32534;&#36753;&#26041;&#27861;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#20559;&#35265;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#19982;&#31181;&#26063;&#12289;&#22320;&#29702;&#26469;&#28304;&#21644;&#24615;&#21035;&#31561;&#20154;&#21475;&#23646;&#24615;&#30456;&#20851;&#30340;&#20559;&#35265;&#65292;&#20197;&#21450;&#30001;&#32534;&#36753;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#38271;&#25991;&#26412;&#20013;&#30340;&#23450;&#24615;&#32570;&#38519;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#32534;&#36753;&#27169;&#22411;&#22312;&#21464;&#24471;&#23545;&#20122;&#27954;&#12289;&#38750;&#27954;&#31561;&#23646;&#24615;&#30340;&#23646;&#24615;&#19981;&#30830;&#23450;&#24230;&#24840;&#39640;&#26102;&#34920;&#29616;&#20986;&#19981;&#21516;&#31243;&#24230;&#30340;&#26356;&#20026;&#20559;&#35265;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00180v1 Announce Type: new  Abstract: Model editing has emerged as a cost-effective strategy to update knowledge stored in language models. However, model editing can have unintended consequences after edits are applied: information unrelated to the edits can also be changed, and other general behaviors of the model can be wrongly altered. In this work, we investigate how model editing methods unexpectedly amplify model biases post-edit. We introduce a novel benchmark dataset, Seesaw-CF, for measuring bias-related harms of model editing and conduct the first in-depth investigation of how different weight-editing methods impact model bias. Specifically, we focus on biases with respect to demographic attributes such as race, geographic origin, and gender, as well as qualitative flaws in long-form texts generated by edited language models. We find that edited models exhibit, to various degrees, more biased behavior as they become less confident in attributes for Asian, African,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#23567;&#30417;&#30563;&#30340;&#20998;&#23618;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#27599;&#20010;&#33410;&#28857;&#30340;&#21807;&#19968;&#31867;&#21517;&#20316;&#20026;&#21807;&#19968;&#30417;&#30563;&#65292;&#21516;&#26102;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.00165</link><description>&lt;p&gt;
TELEClass: &#31246;&#21153;&#23398;&#20016;&#23500;&#21644;LLM&#22686;&#24378;&#30340;&#26368;&#23567;&#30417;&#30563;&#20998;&#23618;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
TELEClass: Taxonomy Enrichment and LLM-Enhanced Hierarchical Text Classification with Minimal Supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00165
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#23567;&#30417;&#30563;&#30340;&#20998;&#23618;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#27599;&#20010;&#33410;&#28857;&#30340;&#21807;&#19968;&#31867;&#21517;&#20316;&#20026;&#21807;&#19968;&#30417;&#30563;&#65292;&#21516;&#26102;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23618;&#25991;&#26412;&#20998;&#31867;&#26088;&#22312;&#23558;&#27599;&#20010;&#25991;&#26723;&#20998;&#31867;&#20026;&#26631;&#31614;Taxonomy&#20013;&#30340;&#19968;&#32452;&#31867;&#21035;&#12290;&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#20351;&#29992;&#26368;&#23569;&#30417;&#30563;&#65306;&#20165;&#20351;&#29992;&#27599;&#20010;&#33410;&#28857;&#30340;&#21807;&#19968;&#31867;&#21517;&#20316;&#20026;&#30417;&#30563;&#26469;&#36827;&#34892;&#20998;&#23618;&#25991;&#26412;&#20998;&#31867;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#38646;&#25552;&#31034;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#33021;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#22312;&#20998;&#23618;&#35774;&#32622;&#20013;&#34920;&#29616;&#36739;&#24046;&#65292;&#22240;&#20026;&#22312;&#25552;&#31034;&#20013;&#21253;&#21547;&#22823;&#32780;&#32467;&#26500;&#21270;&#30340;&#26631;&#31614;&#31354;&#38388;&#26159;&#26080;&#25928;&#30340;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20197;&#21069;&#30340;&#24369;&#30417;&#30563;&#20998;&#23618;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#20165;&#21033;&#29992;&#21407;&#22987;&#30340;Taxonomy&#39592;&#26550;&#65292;&#24573;&#30053;&#20102;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#38544;&#34255;&#30340;&#20016;&#23500;&#20449;&#24687;&#65292;&#36825;&#20123;&#20449;&#24687;&#21487;&#20197;&#29992;&#20316;&#39069;&#22806;&#30340;&#31867;&#21035;&#25351;&#31034;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00165v1 Announce Type: new  Abstract: Hierarchical text classification aims to categorize each document into a set of classes in a label taxonomy. Most earlier works focus on fully or semi-supervised methods that require a large amount of human annotated data which is costly and time-consuming to acquire. To alleviate human efforts, in this paper, we work on hierarchical text classification with the minimal amount of supervision: using the sole class name of each node as the only supervision. Recently, large language models (LLM) show competitive performance on various tasks through zero-shot prompting, but this method performs poorly in the hierarchical setting, because it is ineffective to include the large and structured label space in a prompt. On the other hand, previous weakly-supervised hierarchical text classification methods only utilize the raw taxonomy skeleton and ignore the rich information hidden in the text corpus that can serve as additional class-indicative 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#23558;&#21313;&#20108;&#20010;LLMs&#32452;&#25104;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#19982;925&#21517;&#20154;&#31867;&#39044;&#27979;&#32773;&#30340;&#32676;&#20307;&#39044;&#27979;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;LLM&#32676;&#20307;&#20248;&#20110;&#31616;&#21333;&#30340;&#26080;&#20449;&#24687;&#22522;&#20934;&#65292;&#24182;&#22312;&#32479;&#35745;&#19978;&#31561;&#25928;&#20110;&#20154;&#31867;&#32676;&#20307;&#12290;</title><link>https://arxiv.org/abs/2402.19379</link><description>&lt;p&gt;
&#30789;&#35895;&#20154;&#32676;&#30340;&#26234;&#24935;&#65306;LLM&#38598;&#25104;&#39044;&#27979;&#33021;&#21147;&#36798;&#21040;&#20154;&#32676;&#20934;&#30830;&#29575;&#27700;&#24179;
&lt;/p&gt;
&lt;p&gt;
Wisdom of the Silicon Crowd: LLM Ensemble Prediction Capabilities Match Human Crowd Accuracy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19379
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#23558;&#21313;&#20108;&#20010;LLMs&#32452;&#25104;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#19982;925&#21517;&#20154;&#31867;&#39044;&#27979;&#32773;&#30340;&#32676;&#20307;&#39044;&#27979;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;LLM&#32676;&#20307;&#20248;&#20110;&#31616;&#21333;&#30340;&#26080;&#20449;&#24687;&#22522;&#20934;&#65292;&#24182;&#22312;&#32479;&#35745;&#19978;&#31561;&#25928;&#20110;&#20154;&#31867;&#32676;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#36341;&#20013;&#20154;&#31867;&#39044;&#27979;&#20934;&#30830;&#24615;&#20381;&#36182;&#20110;&#8220;&#32676;&#20307;&#26234;&#24935;&#8221;&#25928;&#24212;&#65292;&#21363;&#36890;&#36807;&#32858;&#21512;&#19968;&#32676;&#20010;&#20307;&#39044;&#27979;&#32773;&#30340;&#39044;&#27979;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#23545;&#26410;&#26469;&#20107;&#20214;&#30340;&#39044;&#27979;&#12290;&#36807;&#21435;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39044;&#27979;&#33021;&#21147;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20316;&#20026;&#20010;&#20307;&#39044;&#27979;&#32773;&#30340;&#21069;&#27839;LLMs&#34920;&#29616;&#19981;&#20339;&#65292;&#19982;&#20154;&#31867;&#32676;&#20307;&#39044;&#27979;&#27604;&#36187;&#30340;&#40644;&#37329;&#26631;&#20934;&#30456;&#27604;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#30001;&#21313;&#20108;&#20010;LLMs&#32452;&#25104;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#65292;&#25193;&#23637;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#23558;31&#20010;&#20108;&#20803;&#38382;&#39064;&#30340;&#32858;&#21512;LLM&#39044;&#27979;&#19982;&#19968;&#20010;&#26469;&#33258;&#19977;&#20010;&#26376;&#39044;&#27979;&#27604;&#36187;&#30340;925&#21517;&#20154;&#31867;&#39044;&#27979;&#32773;&#30340;&#32676;&#20307;&#39044;&#27979;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#20998;&#26512;&#34920;&#26126;&#65292;LLM&#32676;&#20307;&#30340;&#34920;&#29616;&#20248;&#20110;&#31616;&#21333;&#30340;&#26080;&#20449;&#24687;&#22522;&#20934;&#65292;&#24182;&#22312;&#32479;&#35745;&#19978;&#31561;&#25928;&#20110;&#20154;&#31867;&#32676;&#20307;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#19968;&#31181;&#39034;&#20174;&#25928;&#24212;&#65292;&#24179;&#22343;&#27169;&#22411;&#39044;&#27979;&#26126;&#26174;&#39640;&#20110;50%&#65292;&#23613;&#31649;&#20960;&#20046;&#26159;&#24179;&#31561;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19379v1 Announce Type: cross  Abstract: Human forecasting accuracy in practice relies on the 'wisdom of the crowd' effect, in which predictions about future events are significantly improved by aggregating across a crowd of individual forecasters. Past work on the forecasting ability of large language models (LLMs) suggests that frontier LLMs, as individual forecasters, underperform compared to the gold standard of a human crowd forecasting tournament aggregate. In Study 1, we expand this research by using an LLM ensemble approach consisting of a crowd of twelve LLMs. We compare the aggregated LLM predictions on 31 binary questions to that of a crowd of 925 human forecasters from a three-month forecasting tournament. Our main analysis shows that the LLM crowd outperforms a simple no-information benchmark and is statistically equivalent to the human crowd. We also observe an acquiescence effect, with mean model predictions being significantly above 50%, despite an almost even
&lt;/p&gt;</description></item><item><title>RORA &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#29992;&#20110;&#34913;&#37327;&#33258;&#30001;&#25991;&#26412;&#29702;&#30001;&#23545;&#26631;&#31614;&#30340;&#26032;&#20449;&#24687;&#36129;&#29486;&#65292;&#24182;&#22312;&#35780;&#20272;&#20013;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.18678</link><description>&lt;p&gt;
RORA&#65306;&#24378;&#22823;&#30340;&#33258;&#30001;&#25991;&#26412;&#29702;&#30001;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
RORA: Robust Free-Text Rationale Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18678
&lt;/p&gt;
&lt;p&gt;
RORA &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#29992;&#20110;&#34913;&#37327;&#33258;&#30001;&#25991;&#26412;&#29702;&#30001;&#23545;&#26631;&#31614;&#30340;&#26032;&#20449;&#24687;&#36129;&#29486;&#65292;&#24182;&#22312;&#35780;&#20272;&#20013;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30001;&#25991;&#26412;&#29702;&#30001;&#22312;&#21487;&#35299;&#37322;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#24357;&#21512;&#20102;&#27169;&#22411;&#20915;&#31574;&#32972;&#21518;&#30340;&#30693;&#35782;&#21644;&#25512;&#29702;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#28508;&#22312;&#25512;&#29702;&#36335;&#24452;&#30340;&#22810;&#26679;&#24615;&#21450;&#30456;&#24212;&#32570;&#20047;&#26126;&#30830;&#30340;&#30495;&#30456;&#22522;&#30784;&#65292;&#20854;&#35780;&#20272;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#35780;&#20272;&#25351;&#26631;&#20381;&#36182;&#20110;&#29702;&#30001;&#25903;&#25345;&#30446;&#26631;&#26631;&#31614;&#30340;&#31243;&#24230;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#25351;&#26631;&#22312;&#35780;&#20272;&#24847;&#22806;&#27844;&#28431;&#26631;&#31614;&#30340;&#29702;&#30001;&#26102;&#23384;&#22312;&#19981;&#36275;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RORA&#65292;&#19968;&#31181;&#38024;&#23545;&#26631;&#31614;&#27844;&#28431;&#30340;&#24378;&#22823;&#30340;&#33258;&#30001;&#25991;&#26412;&#29702;&#30001;&#35780;&#20272;&#26041;&#27861;&#12290;RORA&#37327;&#21270;&#20102;&#29702;&#30001;&#20026;&#35777;&#26126;&#26631;&#31614;&#25552;&#20379;&#30340;&#26032;&#20449;&#24687;&#12290;&#36825;&#26159;&#36890;&#36807;&#35780;&#20272;&#19982;&#27844;&#28431;&#29305;&#24449;&#25239;&#24178;&#25200;&#30340;&#39044;&#27979;&#24615;&#23478;&#26063;&#26465;&#20214;V&#20449;&#24687;&#23454;&#29616;&#30340;&#12290;RORA&#22312;&#35780;&#20272;&#20154;&#24037;&#25776;&#20889;&#12289;&#21512;&#25104;&#25110;&#27169;&#22411;&#29983;&#25104;&#30340;&#29702;&#30001;&#26041;&#38754;&#22987;&#32456;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18678v1 Announce Type: new  Abstract: Free-text rationales play a pivotal role in explainable NLP, bridging the knowledge and reasoning gaps behind a model's decision-making. However, due to the diversity of potential reasoning paths and a corresponding lack of definitive ground truth, their evaluation remains a challenge. Existing evaluation metrics rely on the degree to which a rationale supports a target label, but we find these fall short in evaluating rationales that inadvertently leak the labels. To address this problem, we propose RORA, a Robust free-text Rationale evaluation against label leakage. RORA quantifies the new information supplied by a rationale to justify the label. This is achieved by assessing the conditional V-information \citep{hewitt-etal-2021-conditional} with a predictive family robust against leaky features that can be exploited by a small model. RORA consistently outperforms existing approaches in evaluating human-written, synthetic, or model-gen
&lt;/p&gt;</description></item><item><title>NewsQs&#26159;&#19968;&#20010;&#20026;&#22810;&#20010;&#26032;&#38395;&#25991;&#26723;&#25552;&#20379;&#38382;&#39064;-&#31572;&#26696;&#23545;&#30340;&#25968;&#25454;&#38598;&#65292;&#21033;&#29992;T5-Large&#27169;&#22411;fine-tune&#33258;&#21160;&#29983;&#25104;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#27169;&#22411;&#36827;&#34892;&#25511;&#21046;&#30721;&#24494;&#35843;&#65292;&#21487;&#20197;&#20135;&#29983;&#26356;&#22810;&#21487;&#25509;&#21463;&#30340;&#38382;&#39064;&#65292;&#37322;&#25918;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#20379;&#26410;&#26469;&#22522;&#20110;&#26597;&#35810;&#30340;&#22810;&#25991;&#26723;&#24635;&#32467;&#30740;&#31350;&#20351;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.18479</link><description>&lt;p&gt;
NewsQs: &#22810;&#28304;&#38382;&#39064;&#29983;&#25104;&#21161;&#21147;&#25506;&#30693;&#24515;&#28789;
&lt;/p&gt;
&lt;p&gt;
NewsQs: Multi-Source Question Generation for the Inquiring Mind
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18479
&lt;/p&gt;
&lt;p&gt;
NewsQs&#26159;&#19968;&#20010;&#20026;&#22810;&#20010;&#26032;&#38395;&#25991;&#26723;&#25552;&#20379;&#38382;&#39064;-&#31572;&#26696;&#23545;&#30340;&#25968;&#25454;&#38598;&#65292;&#21033;&#29992;T5-Large&#27169;&#22411;fine-tune&#33258;&#21160;&#29983;&#25104;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#27169;&#22411;&#36827;&#34892;&#25511;&#21046;&#30721;&#24494;&#35843;&#65292;&#21487;&#20197;&#20135;&#29983;&#26356;&#22810;&#21487;&#25509;&#21463;&#30340;&#38382;&#39064;&#65292;&#37322;&#25918;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#20379;&#26410;&#26469;&#22522;&#20110;&#26597;&#35810;&#30340;&#22810;&#25991;&#26723;&#24635;&#32467;&#30740;&#31350;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102; NewsQs&#65288;news-cues&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#20026;&#22810;&#20010;&#26032;&#38395;&#25991;&#26723;&#25552;&#20379;&#38382;&#39064;-&#31572;&#26696;&#23545;&#30340;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#21019;&#24314; NewsQs&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#32463;&#36807; T5-Large &#27169;&#22411;&#22312; News On the Web &#35821;&#26009;&#24211;&#20013;&#30340; FAQ &#26679;&#24335;&#26032;&#38395;&#25991;&#31456;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#65292;&#33258;&#21160;&#29983;&#25104;&#38382;&#39064;&#26469;&#25193;&#20805;&#20256;&#32479;&#30340;&#22810;&#25991;&#26723;&#24635;&#32467;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#25511;&#21046;&#30721;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#20135;&#29983;&#26356;&#22810;&#34987;&#20154;&#31867;&#35780;&#20215;&#20026;&#21487;&#25509;&#21463;&#30340;&#38382;&#39064;&#65292;&#36825;&#26159;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#25152;&#27979;&#24471;&#30340;&#12290;&#25105;&#20204;&#20351;&#29992;&#19982;&#20154;&#31867;&#27880;&#37322;&#39640;&#24230;&#30456;&#20851;&#30340; QNLI &#27169;&#22411;&#26469;&#36807;&#28388;&#25105;&#20204;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#20316;&#20026;&#26410;&#26469;&#22522;&#20110;&#26597;&#35810;&#30340;&#22810;&#25991;&#26723;&#24635;&#32467;&#24037;&#20316;&#36164;&#28304;&#30340;&#39640;&#36136;&#37327;&#38382;&#39064;&#12289;&#31572;&#26696;&#21644;&#25991;&#26723;&#32858;&#31867;&#30340;&#26368;&#32456;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18479v1 Announce Type: new  Abstract: We present NewsQs (news-cues), a dataset that provides question-answer pairs for multiple news documents. To create NewsQs, we augment a traditional multi-document summarization dataset with questions automatically generated by a T5-Large model fine-tuned on FAQ-style news articles from the News On the Web corpus. We show that fine-tuning a model with control codes produces questions that are judged acceptable more often than the same model without them as measured through human evaluation. We use a QNLI model with high correlation with human annotations to filter our data. We release our final dataset of high-quality questions, answers, and document clusters as a resource for future work in query-based multi-document summarization.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#20445;&#30041;&#21407;&#38382;&#39064;&#32467;&#26500;&#38590;&#24230;&#20294;&#38024;&#23545;LLMs&#26080;&#35299;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#26377;&#25928;&#22320;&#38477;&#20302;&#20102;LLMs&#30340;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.17916</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#25239;&#25915;&#20987;&#23454;&#29616;&#25239;LLM&#30340;&#25968;&#23398;&#38382;&#39064;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
LLM-Resistant Math Word Problem Generation via Adversarial Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#20445;&#30041;&#21407;&#38382;&#39064;&#32467;&#26500;&#38590;&#24230;&#20294;&#38024;&#23545;LLMs&#26080;&#35299;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#26377;&#25928;&#22320;&#38477;&#20302;&#20102;LLMs&#30340;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26174;&#33879;&#25913;&#21464;&#20102;&#25945;&#32946;&#39046;&#22495;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#20363;&#65292;&#29983;&#25104;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#20197;&#30830;&#20445;&#20844;&#24179;&#35780;&#20272;&#65292;&#36825;&#20123;&#31034;&#20363;&#20445;&#30041;&#20102;&#21407;&#22987;&#38382;&#39064;&#30340;&#32467;&#26500;&#21644;&#38590;&#24230;&#65292;&#20294;LLMs&#26080;&#27861;&#35299;&#20915;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#25968;&#23398;&#24212;&#29992;&#39046;&#22495;&#30340;&#35789;&#38382;&#39064;&#65292;&#21033;&#29992;&#25277;&#35937;&#35821;&#27861;&#26641;&#32467;&#26500;&#29983;&#25104;&#23545;&#25239;&#31034;&#20363;&#65292;&#36890;&#36807;&#31616;&#21333;&#32534;&#36753;&#38382;&#39064;&#20013;&#30340;&#25968;&#23383;&#20540;&#65292;&#23548;&#33268;LLMs&#20135;&#29983;&#38169;&#35823;&#31572;&#26696;&#12290;&#25105;&#20204;&#23545;&#21508;&#31181;&#24320;&#28304;&#21644;&#38381;&#28304;LLMs&#36827;&#34892;&#23454;&#39564;&#65292;&#23450;&#37327;&#21644;&#23450;&#24615;&#22320;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#38477;&#20302;&#20102;&#23427;&#20204;&#30340;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17916v1 Announce Type: cross  Abstract: Large language models (LLMs) have significantly transformed the educational landscape. As current plagiarism detection tools struggle to keep pace with LLMs' rapid advancements, the educational community faces the challenge of assessing students' true problem-solving abilities in the presence of LLMs. In this work, we explore a new paradigm for ensuring fair evaluation -- generating adversarial examples which preserve the structure and difficulty of the original questions aimed for assessment, but are unsolvable by LLMs. Focusing on the domain of math word problems, we leverage abstract syntax trees to structurally generate adversarial examples that cause LLMs to produce incorrect answers by simply editing the numeric values in the problems. We conduct experiments on various open- and closed-source LLMs, quantitatively and qualitatively demonstrating that our method significantly degrades their math problem-solving ability. We identify
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;KoDialogBench&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#38889;&#35821;&#23545;&#35805;&#20013;&#30340;&#20250;&#35805;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#23384;&#22312;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>https://arxiv.org/abs/2402.17377</link><description>&lt;p&gt;
KoDialogBench: &#29992;&#38889;&#35821;&#23545;&#35805;&#22522;&#20934;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#20250;&#35805;&#29702;&#35299;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
KoDialogBench: Evaluating Conversational Understanding of Language Models with Korean Dialogue Benchmark
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17377
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;KoDialogBench&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#38889;&#35821;&#23545;&#35805;&#20013;&#30340;&#20250;&#35805;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#23384;&#22312;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#34987;&#37096;&#32626;&#20026;&#32842;&#22825;&#26426;&#22120;&#20154;&#21161;&#25163;&#65292;&#27169;&#22411;&#22312;&#29992;&#25143;&#30340;&#27597;&#35821;&#20013;&#36827;&#34892;&#23545;&#35805;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#22312;&#35757;&#32451;&#26102;&#28085;&#30422;&#20102;&#22810;&#31181;&#35821;&#35328;&#65292;&#20294;&#23545;&#23427;&#20204;&#22312;&#38889;&#35821;&#31561;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#29087;&#32451;&#31243;&#24230;&#32570;&#20047;&#20840;&#38754;&#35780;&#20272;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;KoDialogBench&#65292;&#35813;&#22522;&#20934;&#26088;&#22312;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#38889;&#35821;&#23545;&#35805;&#20013;&#30340;&#20250;&#35805;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20174;&#20844;&#20849;&#26469;&#28304;&#25910;&#38598;&#26085;&#24120;&#35805;&#39064;&#30340;&#38889;&#35821;&#23545;&#35805;&#65292;&#25110;&#23558;&#20854;&#20182;&#35821;&#35328;&#30340;&#23545;&#35805;&#36827;&#34892;&#32763;&#35793;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#23545;&#35805;&#32467;&#26500;&#21270;&#20026;&#19981;&#21516;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#20174;&#23545;&#35805;&#29702;&#35299;&#21040;&#21709;&#24212;&#36873;&#25321;&#20219;&#21153;&#30340;&#24191;&#27867;&#33539;&#22260;&#12290;&#21033;&#29992;&#25552;&#20986;&#30340;&#22522;&#20934;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#21508;&#31181;&#35821;&#35328;&#27169;&#22411;&#30340;&#24191;&#27867;&#35780;&#20272;&#21644;&#20998;&#26512;&#65292;&#20197;&#34913;&#37327;&#23545;&#38889;&#35821;&#23545;&#35805;&#30340;&#22522;&#30784;&#29702;&#35299;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23384;&#22312;&#26126;&#26174;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17377v1 Announce Type: new  Abstract: As language models are often deployed as chatbot assistants, it becomes a virtue for models to engage in conversations in a user's first language. While these models are trained on a wide range of languages, a comprehensive evaluation of their proficiency in low-resource languages such as Korean has been lacking. In this work, we introduce KoDialogBench, a benchmark designed to assess language models' conversational capabilities in Korean. To this end, we collect native Korean dialogues on daily topics from public sources, or translate dialogues from other languages. We then structure these conversations into diverse test datasets, spanning from dialogue comprehension to response selection tasks. Leveraging the proposed benchmark, we conduct extensive evaluations and analyses of various language models to measure a foundational understanding of Korean dialogues. Experimental results indicate that there exists significant room for improve
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#26041;&#27861;&#30340;&#21333;&#20010;&#32534;&#30721;-&#35299;&#30721;&#27169;&#22411;&#26469;&#35299;&#20915;&#36328;&#35821;&#35328;&#38382;&#31572;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#32500;&#22522;&#30334;&#31185;&#20869;&#30340;&#36328;&#35821;&#35328;&#38142;&#25509;&#32467;&#26500;&#21512;&#25104;&#30417;&#30563;&#20449;&#21495;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.16508</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#35268;&#27169;&#21512;&#25104;&#30417;&#30563;&#36827;&#34892;&#36328;&#35821;&#35328;&#24320;&#25918;&#22495;&#38382;&#31572;&#30340;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Pre-training Cross-lingual Open Domain Question Answering with Large-scale Synthetic Supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#26041;&#27861;&#30340;&#21333;&#20010;&#32534;&#30721;-&#35299;&#30721;&#27169;&#22411;&#26469;&#35299;&#20915;&#36328;&#35821;&#35328;&#38382;&#31572;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#32500;&#22522;&#30334;&#31185;&#20869;&#30340;&#36328;&#35821;&#35328;&#38142;&#25509;&#32467;&#26500;&#21512;&#25104;&#30417;&#30563;&#20449;&#21495;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#35821;&#35328;&#38382;&#31572;&#65288;CLQA&#65289;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#20174;&#22810;&#35821;&#35328;&#30693;&#35782;&#24211;&#36827;&#34892;&#36328;&#35821;&#35328;&#26816;&#32034;&#65292;&#28982;&#21518;&#22312;&#33521;&#35821;&#25110;&#26597;&#35810;&#35821;&#35328;&#20013;&#29983;&#25104;&#31572;&#26696;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#21487;&#20197;&#20351;&#29992;&#21333;&#20010;&#32534;&#30721;-&#35299;&#30721;&#27169;&#22411;&#26469;&#35299;&#20915;CLQA&#12290;&#20026;&#20102;&#26377;&#25928;&#35757;&#32451;&#36825;&#20010;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21033;&#29992;&#32500;&#22522;&#30334;&#31185;&#20869;&#36328;&#35821;&#35328;&#38142;&#25509;&#32467;&#26500;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#38142;&#25509;&#30340;&#32500;&#22522;&#30334;&#31185;&#39029;&#38754;&#26469;&#21512;&#25104;&#36328;&#35821;&#35328;&#26816;&#32034;&#30340;&#30417;&#30563;&#20449;&#21495;&#65292;&#36890;&#36807;&#19968;&#31181;&#22635;&#31354;&#26597;&#35810;&#24418;&#24335;&#29983;&#25104;&#26356;&#33258;&#28982;&#30340;&#26597;&#35810;&#20197;&#30417;&#30563;&#31572;&#26696;&#29983;&#25104;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;CLASS&#22312;&#30417;&#30563;&#23398;&#20064;&#21644;&#38646;-shot&#24773;&#20917;&#19979;&#22343;&#20248;&#20110;&#21487;&#27604;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16508v1 Announce Type: new  Abstract: Cross-lingual question answering (CLQA) is a complex problem, comprising cross-lingual retrieval from a multilingual knowledge base, followed by answer generation either in English or the query language. Both steps are usually tackled by separate models, requiring substantial annotated datasets, and typically auxiliary resources, like machine translation systems to bridge between languages. In this paper, we show that CLQA can be addressed using a single encoder-decoder model. To effectively train this model, we propose a self-supervised method based on exploiting the cross-lingual link structure within Wikipedia. We demonstrate how linked Wikipedia pages can be used to synthesise supervisory signals for cross-lingual retrieval, through a form of cloze query, and generate more natural queries to supervise answer generation. Together, we show our approach, \texttt{CLASS}, outperforms comparable methods on both supervised and zero-shot lan
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#26694;&#26550;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20195;&#29702;&#27169;&#22411;&#23545;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#36827;&#34892;&#27169;&#25311;&#65292;&#26500;&#24314;&#20102;&#31867;&#20284;Twitter&#29615;&#22659;&#27169;&#25311;&#20182;&#20204;&#23545;&#35302;&#21457;&#20107;&#20214;&#30340;&#21453;&#24212;&#65292;&#20026;&#31038;&#20250;&#36816;&#21160;&#27169;&#25311;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;&#12290;</title><link>https://arxiv.org/abs/2402.16333</link><description>&lt;p&gt;
&#25581;&#31034;&#30495;&#30456;&#20419;&#36827;&#21464;&#38761;&#65306;&#38754;&#21521;&#22522;&#20110;&#20195;&#29702;&#30340;&#22823;&#35268;&#27169;&#31038;&#20250;&#36816;&#21160;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Unveiling the Truth and Facilitating Change: Towards Agent-based Large-scale Social Movement Simulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16333
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#26694;&#26550;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20195;&#29702;&#27169;&#22411;&#23545;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#36827;&#34892;&#27169;&#25311;&#65292;&#26500;&#24314;&#20102;&#31867;&#20284;Twitter&#29615;&#22659;&#27169;&#25311;&#20182;&#20204;&#23545;&#35302;&#21457;&#20107;&#20214;&#30340;&#21453;&#24212;&#65292;&#20026;&#31038;&#20250;&#36816;&#21160;&#27169;&#25311;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#24050;&#32463;&#25104;&#20026;&#31038;&#20250;&#36816;&#21160;&#30340;&#22522;&#30707;&#65292;&#22312;&#25512;&#21160;&#31038;&#20250;&#21464;&#38761;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#24433;&#21709;&#21147;&#12290;&#27169;&#25311;&#20844;&#20247;&#21453;&#24212;&#24182;&#39044;&#27979;&#28508;&#22312;&#24433;&#21709;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#27169;&#25311;&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#20854;&#20013;&#29992;&#25143;&#20998;&#20026;&#20004;&#31867;&#12290;&#26680;&#24515;&#29992;&#25143;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#65292;&#32780;&#20247;&#22810;&#26222;&#36890;&#29992;&#25143;&#21017;&#30001;&#28436;&#32462;&#24335;&#20195;&#29702;&#27169;&#22411;&#24314;&#27169;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#26500;&#24314;&#20102;&#31867;&#20284;Twitter&#30340;&#29615;&#22659;&#26469;&#22797;&#21046;&#20182;&#20204;&#23545;&#35302;&#21457;&#20107;&#20214;&#30340;&#21453;&#24212;&#21160;&#24577;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22810;&#26041;&#38754;&#30340;&#22522;&#20934;SoMoSiMu-Bench&#29992;&#20110;&#35780;&#20272;&#65292;&#24182;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20840;&#38754;&#23454;&#39564;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16333v1 Announce Type: cross  Abstract: Social media has emerged as a cornerstone of social movements, wielding significant influence in driving societal change. Simulating the response of the public and forecasting the potential impact has become increasingly important. However, existing methods for simulating such phenomena encounter challenges concerning their efficacy and efficiency in capturing the behaviors of social movement participants. In this paper, we introduce a hybrid framework for social media user simulation, wherein users are categorized into two types. Core users are driven by Large Language Models, while numerous ordinary users are modeled by deductive agent-based models. We further construct a Twitter-like environment to replicate their response dynamics following trigger events. Subsequently, we develop a multi-faceted benchmark SoMoSiMu-Bench for evaluation and conduct comprehensive experiments across real-world datasets. Experimental results demonstrat
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25191;&#34892;&#25968;&#20540;&#35745;&#31639;&#26102;&#32463;&#24120;&#20986;&#38169;&#65292;&#36890;&#36807;&#29983;&#25104;&#21487;&#25191;&#34892;&#20195;&#30721;&#26469;&#35299;&#20915;&#38382;&#39064;&#21487;&#20197;&#20943;&#23569;&#35745;&#31639;&#38169;&#35823;&#65292;&#20294;&#35266;&#23519;&#21040;&#24403;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;&#20195;&#30721;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#26102;&#65292;&#20250;&#29983;&#25104;&#26356;&#22810;&#19981;&#27491;&#30830;&#25512;&#29702;&#65307;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#20154;&#31867;&#32534;&#30721;&#23454;&#36341;&#21551;&#21457;&#30340;&#31616;&#21333;&#32780;&#39640;&#25928;&#26041;&#27861;Human-Think Language&#65288;HTL&#65289;&#12290;</title><link>https://arxiv.org/abs/2402.15729</link><description>&lt;p&gt;
&#20154;&#31867;&#26159;&#22914;&#20309;&#32534;&#20889;&#20195;&#30721;&#30340;&#65311;&#22823;&#22411;&#27169;&#22411;&#20063;&#20197;&#21516;&#26679;&#30340;&#26041;&#24335;&#36827;&#34892;
&lt;/p&gt;
&lt;p&gt;
How Do Humans Write Code? Large Models Do It the Same Way Too
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15729
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25191;&#34892;&#25968;&#20540;&#35745;&#31639;&#26102;&#32463;&#24120;&#20986;&#38169;&#65292;&#36890;&#36807;&#29983;&#25104;&#21487;&#25191;&#34892;&#20195;&#30721;&#26469;&#35299;&#20915;&#38382;&#39064;&#21487;&#20197;&#20943;&#23569;&#35745;&#31639;&#38169;&#35823;&#65292;&#20294;&#35266;&#23519;&#21040;&#24403;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;&#20195;&#30721;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#26102;&#65292;&#20250;&#29983;&#25104;&#26356;&#22810;&#19981;&#27491;&#30830;&#25512;&#29702;&#65307;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#20154;&#31867;&#32534;&#30721;&#23454;&#36341;&#21551;&#21457;&#30340;&#31616;&#21333;&#32780;&#39640;&#25928;&#26041;&#27861;Human-Think Language&#65288;HTL&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25191;&#34892;&#25968;&#20540;&#35745;&#31639;&#26102;&#32463;&#24120;&#20986;&#38169;&#12290;&#19982;&#20256;&#32479;&#30340;&#24605;&#32500;&#38142;&#25512;&#29702;&#30456;&#27604;&#65292;&#31243;&#24207;&#21270;&#24605;&#32500;&#26041;&#27861;&#28041;&#21450;&#29983;&#25104;&#21487;&#25191;&#34892;&#20195;&#30721;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;&#36890;&#36807;&#25191;&#34892;&#36825;&#20123;&#20195;&#30721;&#65292;&#23427;&#21487;&#20197;&#33719;&#24471;&#26356;&#31934;&#30830;&#30340;&#32467;&#26524;&#12290;&#20351;&#29992;&#29983;&#25104;&#30340;&#21487;&#25191;&#34892;&#20195;&#30721;&#32780;&#19981;&#26159;&#33258;&#28982;&#35821;&#35328;&#21487;&#20197;&#20943;&#23569;&#35745;&#31639;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#24403;LLMs&#20351;&#29992;&#20195;&#30721;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#26102;&#65292;&#20182;&#20204;&#24448;&#24448;&#29983;&#25104;&#27604;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#26356;&#22810;&#30340;&#19981;&#27491;&#30830;&#25512;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Human-Think Language&#65288;HTL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#21040;&#20154;&#31867;&#32534;&#30721;&#23454;&#36341;&#21551;&#21457;&#30340;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#39318;&#20808;&#30001;&#27169;&#22411;&#29983;&#25104;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#35299;&#20915;&#38382;&#39064;&#26041;&#27861;&#65292;&#28982;&#21518;&#23558;&#20854;&#36716;&#25442;&#20026;&#20195;&#30721;&#65292;&#21453;&#26144;&#20986;&#20154;&#20204;&#22312;&#23558;&#36923;&#36753;&#20197;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#24605;&#32771;&#21518;&#20877;&#23558;&#20854;&#20889;&#25104;&#20195;&#30721;&#30340;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#23427;&#21033;&#29992;&#20102;P
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15729v1 Announce Type: new  Abstract: Large Language Models (LLMs) often make errors when performing numerical calculations. In contrast to traditional chain-of-thought reasoning, the program-of-thoughts approach involves generating executable code to solve problems. By executing this code, it achieves more precise results. Using generated executable code instead of natural language can reduce computational errors. However, we observe that when LLMs solve mathematical problems using code, they tend to generate more incorrect reasoning than when using natural language. To address this issue, we propose Human-Think Language (HTL), a straightforward yet highly efficient approach inspired by human coding practices. The approach first generates problem-solving methods described in the natural language by the model, then converts them into code, mirroring the process where people think through the logic in natural language before writing it as code. Additionally, it utilizes the P
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;COBIAS&#65292;&#26088;&#22312;&#36890;&#36807;&#32771;&#34385;&#22810;&#26679;&#24773;&#22659;&#30340;&#29992;&#25143;&#36755;&#20837;&#20869;&#23481;&#65292;&#34913;&#37327;&#35821;&#21477;&#30340;&#24773;&#22659;&#21487;&#38752;&#24615;&#65292;&#20174;&#32780;&#22521;&#20859;&#20559;&#35265;&#24847;&#35782;&#12290;</title><link>https://arxiv.org/abs/2402.14889</link><description>&lt;p&gt;
COBIAS&#65306;&#20559;&#35265;&#35780;&#20272;&#20013;&#30340;&#24773;&#22659;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
COBIAS: Contextual Reliability in Bias Assessment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14889
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;COBIAS&#65292;&#26088;&#22312;&#36890;&#36807;&#32771;&#34385;&#22810;&#26679;&#24773;&#22659;&#30340;&#29992;&#25143;&#36755;&#20837;&#20869;&#23481;&#65292;&#34913;&#37327;&#35821;&#21477;&#30340;&#24773;&#22659;&#21487;&#38752;&#24615;&#65292;&#20174;&#32780;&#22521;&#20859;&#20559;&#35265;&#24847;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#22522;&#20110;&#22266;&#26377;&#20559;&#35265;&#25968;&#25454;&#35757;&#32451;&#30340;&#12290;&#20197;&#24448;&#30340;&#21435;&#20559;&#35265;&#27169;&#22411;&#30740;&#31350;&#20381;&#36182;&#22522;&#20934;&#25968;&#25454;&#38598;&#26469;&#34913;&#37327;&#27169;&#22411;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#30001;&#20110;&#23545;&#20559;&#35265;&#30340;&#26497;&#20854;&#20027;&#35266;&#29702;&#35299;&#32780;&#23384;&#22312;&#22810;&#20010;&#32570;&#38519;&#65292;&#20984;&#26174;&#20986;&#23545;&#24773;&#22659;&#25506;&#32034;&#30340;&#36843;&#20999;&#38656;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#32771;&#34385;&#36755;&#20837;&#29992;&#25143;&#20869;&#23481;&#30340;&#24773;&#22659;&#65292;&#32771;&#34385;&#21040;&#36755;&#20837;&#35821;&#21477;&#21487;&#33021;&#23384;&#22312;&#30340;&#22810;&#31181;&#24773;&#20917;&#12290;&#36825;&#31181;&#26041;&#27861;&#23558;&#20801;&#35768;&#22521;&#20859;&#20559;&#35265;&#24847;&#35782;&#30340;&#26694;&#26550;&#65292;&#32780;&#19981;&#26159;&#20260;&#23475;&#29992;&#25143;&#21442;&#19982;&#30340;&#38450;&#25252;&#35774;&#26045;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#20004;&#20010;&#26041;&#38754;&#65306;(i) &#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;2287&#20010;&#38472;&#35789;&#28389;&#35843;&#35821;&#21477;&#20197;&#21450;&#28155;&#21152;&#24773;&#22659;&#35201;&#28857;&#30340;&#25968;&#25454;&#38598;&#65307;(ii) &#25105;&#20204;&#24320;&#21457;&#20102;&#38754;&#21521;&#24773;&#22659;&#30340;&#20559;&#35265;&#25351;&#26631;&#21644;&#35780;&#20272;&#20998;&#25968;&#65288;COBIAS&#65289;&#26469;&#35780;&#20272;&#35821;&#21477;&#22312;&#34913;&#37327;&#20559;&#35265;&#26041;&#38754;&#30340;&#24773;&#22659;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#30340;&#24230;&#37327;&#26159;&#34913;&#37327;&#20559;&#35265;&#22522;&#20934;&#25968;&#25454;&#38598;&#24773;&#22659;&#21487;&#38752;&#24615;&#30340;&#37325;&#35201;&#39044;&#27979;&#22240;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14889v1 Announce Type: cross  Abstract: Large Language Models (LLMs) are trained on inherently biased data. Previous works on debiasing models rely on benchmark datasets to measure model performance. However, these datasets suffer from several pitfalls due to the extremely subjective understanding of bias, highlighting a critical need for contextual exploration. We propose understanding the context of user inputs with consideration of the diverse situations in which input statements are possible. This approach would allow for frameworks that foster bias awareness rather than guardrails that hurt user engagement. Our contribution is twofold: (i) we create a dataset of 2287 stereotyped statements augmented with points for adding context; (ii) we develop the Context-Oriented Bias Indicator and Assessment Score (COBIAS) to assess statements' contextual reliability in measuring bias. Our metric is a significant predictor of the contextual reliability of bias-benchmark datasets ($
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#32593;&#32476;&#25235;&#21462;&#22120;NeuScraper&#21487;&#20197;&#20174;&#32593;&#39029;&#20013;&#25552;&#21462;&#24178;&#20928;&#30340;&#25991;&#26412;&#20869;&#23481;&#65292;&#24182;&#19988;&#23454;&#29616;&#20102;&#36229;&#36807;20%&#30340;&#25913;&#36827;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#36136;&#37327;</title><link>https://arxiv.org/abs/2402.14652</link><description>&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#32593;&#32476;&#25235;&#21462;&#36827;&#34892;&#26356;&#28165;&#27905;&#30340;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#31579;&#36873;
&lt;/p&gt;
&lt;p&gt;
Cleaner Pretraining Corpus Curation with Neural Web Scraping
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14652
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#32593;&#32476;&#25235;&#21462;&#22120;NeuScraper&#21487;&#20197;&#20174;&#32593;&#39029;&#20013;&#25552;&#21462;&#24178;&#20928;&#30340;&#25991;&#26412;&#20869;&#23481;&#65292;&#24182;&#19988;&#23454;&#29616;&#20102;&#36229;&#36807;20%&#30340;&#25913;&#36827;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14652v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#30340; &#25688;&#35201;: &#32593;&#32476;&#21253;&#21547;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#21644;&#20016;&#23500;&#20449;&#24687;&#65292;&#20197;&#28385;&#36275;&#20154;&#31867;&#30340;&#20449;&#24687;&#38656;&#27714;&#12290;&#36890;&#36807;&#32454;&#33268;&#30340;&#25968;&#25454;&#25910;&#38598;&#12289;&#39044;&#22788;&#29702;&#21644;&#25972;&#29702;&#65292;&#32593;&#39029;&#21487;&#20197;&#34987;&#29992;&#20316;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#22522;&#26412;&#25968;&#25454;&#36164;&#28304;&#12290;&#28982;&#32780;&#65292;&#38754;&#23545;&#19981;&#26029;&#38761;&#26032;&#21644;&#22797;&#26434;&#30340;&#32593;&#39029;&#29305;&#24615;&#65292;&#22522;&#20110;&#35268;&#21017;/&#29305;&#24449;&#30340;&#32593;&#32476;&#25235;&#21462;&#22120;&#21464;&#24471;&#36234;&#26469;&#36234;&#19981;&#36275;&#22815;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#12289;&#24555;&#36895;&#12289;&#26377;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#32593;&#32476;&#25235;&#21462;&#22120;&#65288;NeuScraper&#65289;&#65292;&#24110;&#21161;&#20174;&#32593;&#39029;&#20013;&#25552;&#21462;&#20027;&#35201;&#21644;&#24178;&#20928;&#30340;&#25991;&#26412;&#20869;&#23481;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;NeuScraper&#36229;&#36234;&#20102;&#22522;&#20934;&#25235;&#21462;&#22120;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;20%&#30340;&#25913;&#36827;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#25552;&#21462;&#26356;&#39640;&#36136;&#37327;&#25968;&#25454;&#20197;&#20419;&#36827;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#25152;&#26377;&#20195;&#30721;&#37117;&#21487;&#20197;&#22312;https://github.com/OpenMatch/NeuScraper&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14652v1 Announce Type: new  Abstract: The web contains large-scale, diverse, and abundant information to satisfy the information-seeking needs of humans. Through meticulous data collection, preprocessing, and curation, webpages can be used as a fundamental data resource for language model pretraining. However, when confronted with the progressively revolutionized and intricate nature of webpages, rule-based/feature-based web scrapers are becoming increasingly inadequate. This paper presents a simple, fast, and effective Neural web Scraper (NeuScraper) to help extract primary and clean text contents from webpages. Experimental results show that NeuScraper surpasses the baseline scrapers by achieving more than a 20% improvement, demonstrating its potential in extracting higher-quality data to facilitate the language model pretraining. All of the code is available at https://github.com/OpenMatch/NeuScraper.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26657;&#20934;&#26469;&#20943;&#36731;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#31435;&#22330;&#26816;&#27979;&#20013;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#38376;&#25511;&#26657;&#20934;&#32593;&#32476;&#24182;&#26500;&#24314;&#20102;&#21453;&#20107;&#23454;&#22686;&#24378;&#25968;&#25454;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#25928;&#26524;&#26174;&#33879;&#12290;</title><link>https://arxiv.org/abs/2402.14296</link><description>&lt;p&gt;
&#22312;&#31435;&#22330;&#26816;&#27979;&#20013;&#36890;&#36807;&#26657;&#20934;&#20943;&#36731;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Mitigating Biases of Large Language Models in Stance Detection with Calibration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26657;&#20934;&#26469;&#20943;&#36731;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#31435;&#22330;&#26816;&#27979;&#20013;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#38376;&#25511;&#26657;&#20934;&#32593;&#32476;&#24182;&#26500;&#24314;&#20102;&#21453;&#20107;&#23454;&#22686;&#24378;&#25968;&#25454;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#25928;&#26524;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#31435;&#22330;&#26816;&#27979;&#20219;&#21153;&#20013;&#65292;LLMs&#21487;&#33021;&#20250;&#29983;&#25104;&#20559;&#35265;&#31435;&#22330;&#65292;&#36825;&#26159;&#30001;&#20110;&#34394;&#20551;&#24773;&#24863;-&#31435;&#22330;&#30456;&#20851;&#24615;&#21644;&#23545;&#26576;&#20123;&#20010;&#20154;&#21644;&#20027;&#39064;&#30340;&#20559;&#22909;&#65292;&#20174;&#32780;&#25439;&#23475;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#26657;&#20934;&#26469;&#20943;&#36731;LLMs&#22312;&#31435;&#22330;&#26816;&#27979;&#20013;&#30340;&#20559;&#35265;&#65288;MB-Cal&#65289;&#12290;&#22312;&#20854;&#20013;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38376;&#25511;&#26657;&#20934;&#32593;&#32476;&#65292;&#20197;&#20943;&#36731;LLMs&#20135;&#29983;&#30340;&#31435;&#22330;&#25512;&#29702;&#32467;&#26524;&#19978;&#30340;&#20559;&#35265;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20351;&#26657;&#20934;&#26356;&#20934;&#30830;&#21644;&#21487;&#25512;&#24191;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#21453;&#20107;&#23454;&#22686;&#24378;&#25968;&#25454;&#26469;&#30699;&#27491;&#31435;&#22330;&#20559;&#35265;&#12290;&#38024;&#23545;&#30446;&#26631;&#21644;&#38646;&#23556;&#20987;&#31435;&#22330;&#26816;&#27979;&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;MB-Cal&#21487;&#20197;&#26377;&#25928;&#20943;&#36731;LLMs&#30340;&#20559;&#35265;&#65292;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14296v1 Announce Type: new  Abstract: Large language models (LLMs) have achieved remarkable progress in many natural language processing tasks. However, our experiment reveals that, in stance detection tasks, LLMs may generate biased stances due to spurious sentiment-stance correlation and preference towards certain individuals and topics, thus harming their performance. Therefore, in this paper, we propose to Mitigate Biases of LLMs in stance detection with Calibration (MB-Cal). In which, a novel gated calibration network is devised to mitigate the biases on the stance reasoning results from LLMs. Further, to make the calibration more accurate and generalizable, we construct counterfactual augmented data to rectify stance biases. Experimental results on in-target and zero-shot stance detection tasks show that the proposed MB-Cal can effectively mitigate biases of LLMs, achieving state-of-the-art results.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#36864;&#21270;&#30693;&#35782;&#31070;&#32463;&#20803;&#65288;DKNs&#65289;&#30340;&#20840;&#38754;&#23450;&#20041;&#65292;&#24341;&#20837;&#20102;&#31070;&#32463;&#25299;&#25169;&#32858;&#31867;&#26041;&#27861;&#21644;&#31070;&#32463;&#36864;&#21270;&#20998;&#26512;&#26694;&#26550;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;DKN&#33719;&#21462;&#12290;</title><link>https://arxiv.org/abs/2402.13731</link><description>&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#36798;&#33452;&#22855;&#23494;&#30721;&#65306;&#35299;&#35835;&#36864;&#21270;&#30693;&#35782;&#31070;&#32463;&#20803;
&lt;/p&gt;
&lt;p&gt;
The Da Vinci Code of Large Pre-trained Language Models: Deciphering Degenerate Knowledge Neurons
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#36864;&#21270;&#30693;&#35782;&#31070;&#32463;&#20803;&#65288;DKNs&#65289;&#30340;&#20840;&#38754;&#23450;&#20041;&#65292;&#24341;&#20837;&#20102;&#31070;&#32463;&#25299;&#25169;&#32858;&#31867;&#26041;&#27861;&#21644;&#31070;&#32463;&#36864;&#21270;&#20998;&#26512;&#26694;&#26550;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;DKN&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#20013;&#20107;&#23454;&#30693;&#35782;&#23384;&#20648;&#30340;&#26426;&#21046;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20107;&#23454;&#30693;&#35782;&#23384;&#20648;&#22312;&#22810;&#23618;&#24863;&#30693;&#22120;&#26435;&#37325;&#20013;&#65292;&#26576;&#20123;&#23384;&#20648;&#21333;&#20803;&#34920;&#29616;&#20986;&#36864;&#21270;&#24615;&#65292;&#31216;&#20026;&#36864;&#21270;&#30693;&#35782;&#31070;&#32463;&#20803;&#65288;Degenerate Knowledge Neurons, DKNs&#65289;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#28085;&#30422;&#32467;&#26500;&#21644;&#21151;&#33021;&#26041;&#38754;&#30340;DKNs&#20840;&#38754;&#23450;&#20041;&#65292;&#24320;&#21019;&#20102;&#23545;PLMs&#20107;&#23454;&#30693;&#35782;&#23384;&#20648;&#21333;&#20803;&#32467;&#26500;&#30340;&#30740;&#31350;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31070;&#32463;&#25299;&#25169;&#32858;&#31867;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20801;&#35768;&#24418;&#25104;&#20219;&#24847;&#25968;&#37327;&#21644;&#32467;&#26500;&#30340;DKNs&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;DKN&#33719;&#21462;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31070;&#32463;&#36864;&#21270;&#20998;&#26512;&#26694;&#26550;&#65292;&#29420;&#29305;&#22320;&#25972;&#21512;&#20102;&#27169;&#22411;&#40065;&#26834;&#24615;&#12289;&#21487;&#36827;&#21270;&#24615;&#21644;&#22797;&#26434;&#24615;&#65292;&#20197;&#23545;PLMs&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;&#22312;&#35813;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#25191;&#34892;&#20102;34&#20010;&#23454;&#39564;&#65292;&#36328;&#36234;2&#20010;PLMs&#12289;4&#20010;&#25968;&#25454;&#38598;&#21644;6&#20010;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13731v1 Announce Type: cross  Abstract: This study explores the mechanism of factual knowledge storage in pre-trained language models (PLMs). Previous research suggests that factual knowledge is stored within multi-layer perceptron weights, and some storage units exhibit degeneracy, referred to as Degenerate Knowledge Neurons (DKNs). This paper provides a comprehensive definition of DKNs that covers both structural and functional aspects, pioneering the study of structures in PLMs' factual knowledge storage units. Based on this, we introduce the Neurological Topology Clustering method, which allows the formation of DKNs in any numbers and structures, leading to a more accurate DKN acquisition. Furthermore, we introduce the Neuro-Degeneracy Analytic Analysis Framework, which uniquely integrates model robustness, evolvability, and complexity for a holistic assessment of PLMs. Within this framework, our execution of 34 experiments across 2 PLMs, 4 datasets, and 6 settings highl
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;SemEval-2024&#20219;&#21153;8&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24494;&#35843;LLMs&#36827;&#34892;&#22810;&#35821;&#35328;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#31181;&#26041;&#24335;&#22788;&#29702;&#35813;&#20219;&#21153;&#24182;&#23558;&#32479;&#35745;&#26816;&#27979;&#25351;&#26631;&#19982;&#27169;&#22411;&#39044;&#27979;&#30456;&#32467;&#21512;&#65292;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.13671</link><description>&lt;p&gt;
KInIT&#21442;&#21152;SemEval-2024&#20219;&#21153;8&#65306;&#38024;&#23545;&#22810;&#35821;&#35328;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#24494;&#35843;LLMs
&lt;/p&gt;
&lt;p&gt;
KInIT at SemEval-2024 Task 8: Fine-tuned LLMs for Multilingual Machine-Generated Text Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;SemEval-2024&#20219;&#21153;8&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24494;&#35843;LLMs&#36827;&#34892;&#22810;&#35821;&#35328;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#31181;&#26041;&#24335;&#22788;&#29702;&#35813;&#20219;&#21153;&#24182;&#23558;&#32479;&#35745;&#26816;&#27979;&#25351;&#26631;&#19982;&#27169;&#22411;&#39044;&#27979;&#30456;&#32467;&#21512;&#65292;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13671v1 &#21457;&#34920;&#31867;&#22411;&#65306;&#20132;&#21449;&#20256;&#25773; &#25688;&#35201;&#65306;SemEval-2024&#20219;&#21153;8&#20391;&#37325;&#20110;&#22810;&#29983;&#25104;&#22120;&#12289;&#22810;&#39046;&#22495;&#21644;&#22810;&#35821;&#35328;&#30340;&#40657;&#30418;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#12290;&#36825;&#26679;&#30340;&#26816;&#27979;&#23545;&#20110;&#38450;&#27490;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#28508;&#22312;&#28389;&#29992;&#38750;&#24120;&#37325;&#35201;&#65292;&#20854;&#20013;&#26368;&#26032;&#30340;LLMs&#38750;&#24120;&#25797;&#38271;&#29983;&#25104;&#22810;&#35821;&#35328;&#30340;&#31867;&#20284;&#20154;&#31867;&#30340;&#25991;&#26412;&#12290;&#25105;&#20204;&#20197;&#22810;&#31181;&#26041;&#24335;&#22788;&#29702;&#20102;&#36825;&#20010;&#20219;&#21153;&#65292;&#21033;&#29992;&#35821;&#35328;&#35782;&#21035;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#36739;&#23567;&#30340;LLMs&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20351;&#29992;&#27599;&#31181;&#35821;&#35328;&#30340;&#20998;&#31867;&#38408;&#20540;&#26657;&#20934;&#65292;&#23558;&#24494;&#35843;&#30340;&#27169;&#22411;&#39044;&#27979;&#19982;&#32479;&#35745;&#26816;&#27979;&#25351;&#26631;&#29420;&#29305;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#31995;&#32479;&#26816;&#27979;&#24615;&#33021;&#30340;&#27867;&#21270;&#12290;&#25105;&#20204;&#25552;&#20132;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#65292;&#25490;&#21517;&#31532;&#22235;&#65292;&#20165;&#33853;&#21518;&#20110;&#33719;&#32988;&#32773;&#19981;&#21040;1&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13671v1 Announce Type: cross  Abstract: SemEval-2024 Task 8 is focused on multigenerator, multidomain, and multilingual black-box machine-generated text detection. Such a detection is important for preventing a potential misuse of large language models (LLMs), the newest of which are very capable in generating multilingual human-like texts. We have coped with this task in multiple ways, utilizing language identification and parameter-efficient fine-tuning of smaller LLMs for text classification. We have further used the per-language classification-threshold calibration to uniquely combine fine-tuned models predictions with statistical detection metrics to improve generalization of the system detection performance. Our submitted method achieved competitive results, ranking at the fourth place, just under 1 percentage point behind the winner.
&lt;/p&gt;</description></item><item><title>&#26500;&#24314;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#21644;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#24182;&#35266;&#23519;&#21040;&#19981;&#21516;&#30340;&#36755;&#20837;-&#36755;&#20986;&#27169;&#24335;&#23548;&#33268;&#19981;&#21516;&#30340;&#20559;&#35265;&#22823;&#23567;&#21644;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.13636</link><description>&lt;p&gt;
&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#32479;&#19968;&#26694;&#26550;&#21644;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
A Unified Framework and Dataset for Assessing Gender Bias in Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13636
&lt;/p&gt;
&lt;p&gt;
&#26500;&#24314;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#21644;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#24182;&#35266;&#23519;&#21040;&#19981;&#21516;&#30340;&#36755;&#20837;-&#36755;&#20986;&#27169;&#24335;&#23548;&#33268;&#19981;&#21516;&#30340;&#20559;&#35265;&#22823;&#23567;&#21644;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#24191;&#27867;&#24212;&#29992;&#20110;&#24037;&#19994;&#21644;&#23398;&#26415;&#30028;&#12290;&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#31995;&#32479;&#35780;&#20272;VLMs&#20013;&#30340;&#24615;&#21035;-&#32844;&#19994;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#28085;&#30422;&#20102;&#26368;&#36817;VLMs&#25903;&#25345;&#30340;&#25152;&#26377;&#25512;&#26029;&#27169;&#24335;&#65292;&#21253;&#25324;&#22270;&#20687;&#21040;&#25991;&#26412;&#12289;&#25991;&#26412;&#21040;&#25991;&#26412;&#12289;&#25991;&#26412;&#21040;&#22270;&#20687;&#21644;&#22270;&#20687;&#21040;&#22270;&#20687;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21512;&#25104;&#30340;&#12289;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#21644;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#27169;&#31946;&#20102;&#32844;&#19994;&#21160;&#20316;&#20013;&#30340;&#24615;&#21035;&#24046;&#24322;&#65292;&#20197;&#35780;&#20272;&#24615;&#21035;&#20559;&#35265;&#12290;&#22312;&#25105;&#20204;&#23545;&#26368;&#36817;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#26102;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#19981;&#21516;&#30340;&#36755;&#20837;-&#36755;&#20986;&#27169;&#24335;&#20250;&#23548;&#33268;&#19981;&#21516;&#30340;&#20559;&#35265;&#22823;&#23567;&#21644;&#26041;&#21521;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#24037;&#20316;&#33021;&#22815;&#25351;&#23548;&#26410;&#26469;&#25913;&#36827;VLMs&#20197;&#23398;&#20064;&#31038;&#20250;&#26080;&#20559;&#35265;&#34920;&#31034;&#12290;&#25105;&#20204;&#23558;&#21457;&#24067;&#25105;&#20204;&#30340;&#25968;&#25454;&#21644;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13636v1 Announce Type: cross  Abstract: Large vision-language models (VLMs) are widely getting adopted in industry and academia. In this work we build a unified framework to systematically evaluate gender-profession bias in VLMs. Our evaluation encompasses all supported inference modes of the recent VLMs, including image-to-text, text-to-text, text-to-image, and image-to-image. We construct a synthetic, high-quality dataset of text and images that blurs gender distinctions across professional actions to benchmark gender bias. In our benchmarking of recent vision-language models (VLMs), we observe that different input-output modalities result in distinct bias magnitudes and directions. We hope our work will help guide future progress in improving VLMs to learn socially unbiased representations. We will release our data and code.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#32622;&#20449;&#24230;&#35780;&#20272;&#30340;&#20840;&#38754;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#19994;&#22810;&#35821;&#35328;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#24182;&#30740;&#31350;&#20102;&#36825;&#20123;&#32622;&#20449;&#24230;&#20998;&#25968;&#22914;&#20309;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#65292;&#26368;&#32456;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#35821;&#35328;&#32622;&#20449;&#24230;&#20272;&#35745;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.13606</link><description>&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#32622;&#20449;&#24230;&#35780;&#20272;&#36827;&#34892;&#20840;&#38754;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Study of Multilingual Confidence Estimation on Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13606
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#32622;&#20449;&#24230;&#35780;&#20272;&#30340;&#20840;&#38754;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#19994;&#22810;&#35821;&#35328;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#24182;&#30740;&#31350;&#20102;&#36825;&#20123;&#32622;&#20449;&#24230;&#20998;&#25968;&#22914;&#20309;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#65292;&#26368;&#32456;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#35821;&#35328;&#32622;&#20449;&#24230;&#20272;&#35745;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#24187;&#35273;&#24182;&#22312;&#39044;&#27979;&#20013;&#34920;&#29616;&#36807;&#20110;&#33258;&#20449;&#30340;&#20542;&#21521;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#20854;&#21487;&#38752;&#24615;&#30340;&#25285;&#24551;&#12290;&#34920;&#26126;&#27169;&#22411;&#21709;&#24212;&#30340;&#21487;&#20449;&#24230;&#25110;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#23545;&#20110;&#24320;&#21457;&#21487;&#38752;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#30446;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#33521;&#35821;&#20013;LLM&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#19978;&#65292;&#22312;&#20854;&#20182;&#24191;&#27867;&#20351;&#29992;&#30340;&#35821;&#35328;&#26041;&#38754;&#20173;&#23384;&#22312;&#31354;&#30333;&#65292;&#38459;&#30861;&#20102;&#21487;&#38752;AI&#24212;&#29992;&#30340;&#20840;&#29699;&#21457;&#23637;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;LLM&#19978;&#30340;&#22810;&#35821;&#35328;&#32622;&#20449;&#24230;&#35780;&#20272;&#65288;MlingConf&#65289;&#30340;&#20840;&#38754;&#35843;&#26597;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#32463;&#36807;&#35814;&#32454;&#26816;&#26597;&#30340;&#19987;&#19994;&#22810;&#35821;&#35328;&#38382;&#31572;&#25968;&#25454;&#38598;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#32622;&#20449;&#24230;&#20272;&#35745;&#30340;&#24615;&#33021;&#65292;&#24182;&#30740;&#31350;&#36825;&#20123;&#32622;&#20449;&#24230;&#20998;&#25968;&#22914;&#20309;&#36890;&#36807;&#36328;&#19981;&#21516;&#35821;&#35328;&#30340;&#33258;&#25105;&#23436;&#21892;&#26469;&#22686;&#24378;LLM&#30340;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#35821;&#35328;&#32622;&#20449;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13606v1 Announce Type: new  Abstract: The tendency of Large Language Models to generate hallucinations and exhibit overconfidence in predictions raises concerns regarding their reliability. Confidence or uncertainty estimations indicating the extent of trustworthiness of a model's response are essential to developing reliable AI systems. Current research primarily focuses on LLM confidence estimations in English, remaining a void for other widely used languages and impeding the global development of reliable AI applications. This paper introduces a comprehensive investigation of Multi-lingual confidence estimation (MlingConf) on LLMs. First, we introduce an elaborated and expert-checked multilingual QA dataset. Second, we delve into the performance of confidence estimations and examine how these confidence scores can enhance LLM performance through self-refinement across diverse languages. Finally, we propose a cross-lingual confidence estimation method to achieve more preci
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#35757;&#32451;&#25968;&#25454;&#21644;&#26500;&#24314;&#26816;&#26597;-&#26657;&#27491;&#25968;&#25454;&#38598;&#65292;&#26412;&#30740;&#31350;&#22686;&#24378;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#26657;&#27491;&#33021;&#21147;&#65292;&#25552;&#39640;&#20102;&#33258;&#25105;&#26657;&#27491;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13035</link><description>&lt;p&gt;
&#23398;&#20064;&#26816;&#26597;&#65306;&#37322;&#25918;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#25105;&#26657;&#27491;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Learning to Check: Unleashing Potentials for Self-Correction in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13035
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#35757;&#32451;&#25968;&#25454;&#21644;&#26500;&#24314;&#26816;&#26597;-&#26657;&#27491;&#25968;&#25454;&#38598;&#65292;&#26412;&#30740;&#31350;&#22686;&#24378;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#26657;&#27491;&#33021;&#21147;&#65292;&#25552;&#39640;&#20102;&#33258;&#25105;&#26657;&#27491;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#19981;&#26029;&#21162;&#21147;&#36890;&#36807;&#33258;&#25105;&#26657;&#27491;&#26469;&#23436;&#21892;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#27809;&#26377;&#22806;&#37096;&#20934;&#30830;&#30693;&#35782;&#30340;&#33258;&#25105;&#26657;&#27491;&#21487;&#33021;&#23384;&#22312;&#23616;&#38480;&#24615;&#29978;&#33267;&#21487;&#33021;&#36866;&#24471;&#20854;&#21453;&#65292;&#36825;&#23601;&#24341;&#21457;&#20102;&#20851;&#20110;&#33258;&#25105;&#26657;&#27491;&#30340;&#38480;&#21046;&#21644;&#26377;&#25928;&#24615;&#30340;&#30097;&#38382;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#35757;&#32451;&#25968;&#25454;&#26469;&#22686;&#24378;LLM&#30340;&#33258;&#26816;&#21151;&#33021;&#65292;&#20174;&#32780;&#25552;&#39640;&#33258;&#25105;&#26657;&#27491;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#23545;&#25968;&#23398;&#25512;&#29702;&#20013;&#30340;&#38169;&#35823;&#31867;&#22411;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#37327;&#36523;&#23450;&#21046;&#30340;&#25552;&#31034;&#65292;&#31216;&#20026;&#8220;Step CoT Check&#8221;&#12290;&#28982;&#21518;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26816;&#26597;-&#26657;&#27491;&#25968;&#25454;&#38598;&#29992;&#20110;&#35757;&#32451;&#27169;&#22411;&#12290;&#22312;&#23558;&#21407;&#22987;CoT&#25968;&#25454;&#21644;&#26816;&#26597;&#26657;&#27491;&#25968;&#25454;&#25972;&#21512;&#21518;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#27169;&#22411;&#21487;&#20197;&#25913;&#21892;&#20854;&#33258;&#26816;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#33258;&#25105;&#26657;&#27491;&#33021;&#21147;&#24182;&#28040;&#38500;&#20102;&#38656;&#35201;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13035v1 Announce Type: cross  Abstract: Large language models (LLMs) have made significant strides in reasoning capabilities, with ongoing efforts to refine their reasoning through self-correction. However, recent studies suggest that self-correction can be limited or even counterproductive without external accurate knowledge, raising questions about the limits and effectiveness of self-correction. In this paper, we aim to enhance LLM's self-checking capabilities by meticulously designing training data, thereby improving the accuracy of self-correction. We conduct a detailed analysis of error types in mathematical reasoning and develop a tailored prompt, termed ``Step CoT Check''. Then we construct a checking-correction dataset for training models. After integrating the original CoT data and checking-correction data for training, we observe that models could improve their self-checking capabilities, thereby enhancing their self-correction capacity and eliminating the need fo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;OWSM-CTC&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;Connectionist Temporal Classification&#30340;&#26032;&#22411;&#20165;&#32534;&#30721;&#22120;&#35821;&#38899;&#22522;&#30784;&#27169;&#22411;&#65292;&#35757;&#32451;&#26377;180k&#23567;&#26102;&#30340;&#20844;&#20849;&#38899;&#39057;&#25968;&#25454;&#65292;&#29992;&#20110;&#22810;&#35821;&#35328;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#12289;&#35821;&#38899;&#32763;&#35793;&#65288;ST&#65289;&#21644;&#35821;&#35328;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2402.12654</link><description>&lt;p&gt;
OWSM-CTC:&#19968;&#31181;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#12289;&#32763;&#35793;&#21644;&#35821;&#35328;&#35782;&#21035;&#30340;&#24320;&#25918;&#32534;&#30721;&#22120;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
OWSM-CTC: An Open Encoder-Only Speech Foundation Model for Speech Recognition, Translation, and Language Identification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12654
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;OWSM-CTC&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;Connectionist Temporal Classification&#30340;&#26032;&#22411;&#20165;&#32534;&#30721;&#22120;&#35821;&#38899;&#22522;&#30784;&#27169;&#22411;&#65292;&#35757;&#32451;&#26377;180k&#23567;&#26102;&#30340;&#20844;&#20849;&#38899;&#39057;&#25968;&#25454;&#65292;&#29992;&#20110;&#22810;&#35821;&#35328;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#12289;&#35821;&#38899;&#32763;&#35793;&#65288;ST&#65289;&#21644;&#35821;&#35328;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#23545;&#33021;&#22815;&#22312;&#21333;&#20010;&#27169;&#22411;&#20013;&#25191;&#34892;&#22810;&#20010;&#35821;&#38899;&#22788;&#29702;&#20219;&#21153;&#30340;&#22823;&#22411;&#35821;&#38899;&#27169;&#22411;&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#37319;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#25110;&#20165;&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#38750;&#24120;&#27969;&#34892;&#19988;&#24615;&#33021;&#33391;&#22909;&#12290;&#28982;&#32780;&#65292;&#19982;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#30456;&#27604;&#65292;&#33258;&#22238;&#24402;&#27169;&#22411;&#22312;&#25512;&#26029;&#26102;&#21487;&#33021;&#20250;&#27604;&#36739;&#24930;&#65292;&#24182;&#19988;&#36824;&#23384;&#22312;&#24187;&#35273;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#30740;&#31350;&#35266;&#23519;&#21040;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#22312;&#23567;&#35268;&#27169;&#20219;&#21153;&#20013;&#20135;&#29983;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#65292;&#20294;&#23578;&#19981;&#28165;&#26970;&#23427;&#20204;&#26159;&#21542;&#21487;&#20197;&#25193;&#23637;&#21040;&#19981;&#21516;&#35821;&#35328;&#21644;&#20219;&#21153;&#30340;&#35821;&#38899;&#36716;&#25991;&#26412;&#29983;&#25104;&#20013;&#12290;&#21463;Open Whisper-style Speech Model (OWSM)&#39033;&#30446;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;OWSM-CTC&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;Connectionist Temporal Classification (CTC)&#30340;&#26032;&#22411;&#20165;&#32534;&#30721;&#22120;&#30340;&#35821;&#38899;&#22522;&#30784;&#27169;&#22411;&#12290;&#23427;&#20351;&#29992;18&#19975;&#23567;&#26102;&#30340;&#20844;&#20849;&#38899;&#39057;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#29992;&#20110;&#22810;&#35821;&#35328;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#12289;&#35821;&#38899;&#32763;&#35793;&#65288;ST&#65289;&#21644;&#35821;&#35328;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12654v1 Announce Type: new  Abstract: There has been an increasing interest in large speech models that can perform multiple speech processing tasks in a single model. Such models usually adopt the encoder-decoder or decoder-only architecture due to their popularity and good performance in many domains. However, autoregressive models can be slower during inference compared to non-autoregressive models and also have potential risks of hallucination. Though prior studies observed promising results of non-autoregressive models for certain tasks at small scales, it remains unclear if they can be scaled to speech-to-text generation in diverse languages and tasks. Inspired by the Open Whisper-style Speech Model (OWSM) project, we propose OWSM-CTC, a novel encoder-only speech foundation model based on Connectionist Temporal Classification (CTC). It is trained on 180k hours of public audio data for multilingual automatic speech recognition (ASR), speech translation (ST), and languag
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Self-AMPLIFY&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20107;&#21518;&#35299;&#37322;&#26041;&#27861;&#24212;&#29992;&#20110;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#65292;&#33258;&#21160;&#29983;&#25104;&#22522;&#20110;&#21407;&#22240;&#30340;&#35299;&#37322;&#65292;&#20197;&#25552;&#39640;&#23427;&#20204;&#33258;&#36523;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12038</link><description>&lt;p&gt;
Self-AMPLIFY&#65306;&#36890;&#36807;&#33258;&#25105;&#20107;&#21518;&#35299;&#37322;&#25913;&#36827;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Self-AMPLIFY: Improving Small Language Models with Self Post Hoc Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Self-AMPLIFY&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20107;&#21518;&#35299;&#37322;&#26041;&#27861;&#24212;&#29992;&#20110;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#65292;&#33258;&#21160;&#29983;&#25104;&#22522;&#20110;&#21407;&#22240;&#30340;&#35299;&#37322;&#65292;&#20197;&#25552;&#39640;&#23427;&#20204;&#33258;&#36523;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Self-AMPLIFY&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#24212;&#29992;&#20110;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#30340;&#20107;&#21518;&#35299;&#37322;&#26041;&#27861;&#33258;&#21160;&#29983;&#25104;&#22522;&#20110;&#21407;&#22240;&#30340;&#35299;&#37322;&#65292;&#20174;&#32780;&#25552;&#39640;&#23427;&#20204;&#33258;&#36523;&#30340;&#24615;&#33021;&#12290;Self-AMPLIFY&#26159;&#19968;&#20010;3&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36873;&#25321;&#26679;&#26412;&#12289;&#29983;&#25104;&#29702;&#30001;&#21644;&#26500;&#24314;&#26368;&#32456;&#25552;&#31034;&#20197;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#38656;&#35201;&#25512;&#29702;&#33021;&#21147;&#30340;SLMs&#21644;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;Self-AMPLIFY&#30340;&#24615;&#33021;&#65306;&#36825;&#20123;&#23454;&#39564;&#34920;&#26126;Self-AMPLIFY&#22312;&#19982;&#31454;&#20105;&#23545;&#25163;&#30456;&#27604;&#34920;&#29616;&#20986;&#33394;&#12290;Self-AMPLIFY&#26159;&#31532;&#19968;&#20010;&#23558;&#20107;&#21518;&#35299;&#37322;&#26041;&#27861;&#24212;&#29992;&#20110;SLMs&#30340;&#26041;&#27861;&#65292;&#20197;&#29983;&#25104;&#35299;&#37322;&#24182;&#25552;&#39640;&#23427;&#20204;&#33258;&#36523;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12038v1 Announce Type: new  Abstract: Incorporating natural language rationales in the prompt and In-Context Learning (ICL) has led to a significant improvement of Large Language Models (LLMs) performance. However, rationales currently require human-annotation or the use of auxiliary proxy models to target promising samples or generate high-quality rationales. In this work, we propose Self-AMPLIFY to generate automatically rationales from post hoc explanation methods applied to Small Language Models (SLMs) to improve their own performance. Self-AMPLIFY is a 3-step method that targets samples, generates rationales and builds a final prompt to leverage ICL. Self-AMPLIFY performance is evaluated on two SLMs and two datasets requiring reasoning abilities: these experiments show that Self-AMPLIFY achieves good results against competitors. Self-AMPLIFY is the first method to apply post hoc explanation methods to SLM to generate rationales to improve their own performance in a full
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#33258;&#36866;&#24212;&#25945;&#23398;&#26041;&#27861;&#65288;ATKD&#65289;&#65292;&#20197;&#25913;&#21892;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#65292;&#24110;&#21161;&#21508;&#31181;&#22522;&#32447;KD&#26041;&#27861;&#22312;&#25152;&#26377;&#27169;&#22411;&#31867;&#22411;&#21644;&#35268;&#27169;&#19978;&#23454;&#29616;&#19968;&#33268;&#19988;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.11890</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Revisiting Knowledge Distillation for Autoregressive Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11890
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#33258;&#36866;&#24212;&#25945;&#23398;&#26041;&#27861;&#65288;ATKD&#65289;&#65292;&#20197;&#25913;&#21892;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#65292;&#24110;&#21161;&#21508;&#31181;&#22522;&#32447;KD&#26041;&#27861;&#22312;&#25152;&#26377;&#27169;&#22411;&#31867;&#22411;&#21644;&#35268;&#27169;&#19978;&#23454;&#29616;&#19968;&#33268;&#19988;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36890;&#36807;&#35757;&#32451;&#36739;&#23567;&#30340;&#23398;&#29983;&#27169;&#22411;&#26469;&#21387;&#32553;&#25945;&#24072;&#27169;&#22411;&#65292;&#20197;&#20943;&#23569;&#25512;&#29702;&#25104;&#26412;&#21644;&#20869;&#23384;&#21344;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#22312;&#23454;&#35777;&#20013;&#21457;&#29616;&#36739;&#22823;&#30340;&#25945;&#24072;LMs&#21487;&#33021;&#20250;&#23548;&#33268;&#36739;&#24046;&#30340;&#23398;&#29983;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#20998;&#26512;&#65292;&#24182;&#25581;&#31034;&#20102;&#19981;&#21516;&#35760;&#21495;&#20855;&#26377;&#19981;&#21516;&#30340;&#25945;&#23398;&#26041;&#24335;&#65292;&#24573;&#35270;&#36825;&#19968;&#28857;&#23558;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#33258;&#36866;&#24212;&#25945;&#23398;&#26041;&#27861;&#65288;ATKD&#65289;&#26469;&#25913;&#21892;KD&#12290;ATKD&#30340;&#26680;&#24515;&#26159;&#20943;&#23569;&#27515;&#35760;&#30828;&#32972;&#30340;&#23398;&#20064;&#65292;&#20351;&#25945;&#23398;&#26356;&#21152;&#22810;&#26679;&#21270;&#21644;&#28789;&#27963;&#12290;&#23545;8&#20010;LM&#20219;&#21153;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;ATKD&#30340;&#24110;&#21161;&#19979;&#65292;&#21508;&#31181;&#22522;&#32447;KD&#26041;&#27861;&#22312;&#25152;&#26377;&#27169;&#22411;&#31867;&#22411;&#21644;&#35268;&#27169;&#19978;&#22343;&#21487;&#20197;&#23454;&#29616;&#19968;&#33268;&#19988;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65288;&#26368;&#39640;&#21487;&#36798;+3.04%&#30340;&#24179;&#22343;&#20998;&#25968;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11890v1 Announce Type: new  Abstract: Knowledge distillation (KD) is a common approach to compress a teacher model to reduce its inference cost and memory footprint, by training a smaller student model. However, in the context of autoregressive language models (LMs), we empirically find that larger teacher LMs might dramatically result in a poorer student. In response to this problem, we conduct a series of analyses and reveal that different tokens have different teaching modes, neglecting which will lead to performance degradation. Motivated by this, we propose a simple yet effective adaptive teaching approach (ATKD) to improve the KD. The core of ATKD is to reduce rote learning and make teaching more diverse and flexible. Extensive experiments on 8 LM tasks show that, with the help of ATKD, various baseline KD methods can achieve consistent and significant performance gains (up to +3.04% average score) across all model types and sizes. More encouragingly, ATKD can improve 
&lt;/p&gt;</description></item><item><title>ROSE&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25233;&#21046;&#19981;&#21463;&#27426;&#36814;&#30340;&#36755;&#20986;&#26469;&#25552;&#39640;&#26399;&#26395;&#30340;&#23433;&#20840;&#36755;&#20986;&#30340;&#27010;&#29575;&#65292;&#20174;&#32780;&#30452;&#25509;&#25552;&#21319;&#29616;&#26377;&#35843;&#25972;&#25351;&#20196;LLMs&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11889</link><description>&lt;p&gt;
ROSE &#19981;&#36825;&#26679;&#20570;&#65306;&#20351;&#29992;&#21453;&#21521;&#25552;&#31034;&#23545;&#27604;&#35299;&#30721;&#25552;&#21319;&#35843;&#25972;&#25351;&#20196;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
ROSE Doesn't Do That: Boosting the Safety of Instruction-Tuned Large Language Models with Reverse Prompt Contrastive Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11889
&lt;/p&gt;
&lt;p&gt;
ROSE&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25233;&#21046;&#19981;&#21463;&#27426;&#36814;&#30340;&#36755;&#20986;&#26469;&#25552;&#39640;&#26399;&#26395;&#30340;&#23433;&#20840;&#36755;&#20986;&#30340;&#27010;&#29575;&#65292;&#20174;&#32780;&#30452;&#25509;&#25552;&#21319;&#29616;&#26377;&#35843;&#25972;&#25351;&#20196;LLMs&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35843;&#25972;&#25351;&#20196;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#65292;&#25552;&#39640;LLMs&#30340;&#23433;&#20840;&#24615;&#21464;&#24471;&#26356;&#21152;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#29992;&#20110;&#23558;LLMs&#36755;&#20986;&#19982;&#39044;&#26399;&#23433;&#20840;&#24615;&#23545;&#40784;&#30340;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#24037;&#20316;&#65292;&#20363;&#22914;&#39640;&#36136;&#37327;&#30340;&#23433;&#20840;&#25968;&#25454;&#21644;&#26114;&#36149;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#36825;&#20123;&#37117;&#26159;&#26114;&#36149;&#19988;&#20302;&#25928;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21453;&#21521;&#25552;&#31034;&#23545;&#27604;&#35299;&#30721;&#65288;ROSE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#30452;&#25509;&#25552;&#21319;&#29616;&#26377;&#35843;&#25972;&#25351;&#20196;LLMs&#30340;&#23433;&#20840;&#24615;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#12290;ROSE&#30340;&#21407;&#21017;&#26159;&#36890;&#36807;&#25233;&#21046;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#21453;&#21521;&#25552;&#31034;&#35825;&#23548;&#30340;&#19981;&#21463;&#27426;&#36814;&#30340;&#36755;&#20986;&#65292;&#25552;&#39640;&#26399;&#26395;&#30340;&#23433;&#20840;&#36755;&#20986;&#30340;&#27010;&#29575;&#12290;&#22312;6&#20010;&#23433;&#20840;&#20219;&#21153;&#21644;2&#20010;&#36890;&#29992;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;ROSE&#19981;&#20165;&#22312;5&#31181;&#31867;&#22411;&#30340;&#35843;&#25972;&#25351;&#20196;LLMs&#19978;&#24102;&#26469;&#20102;&#19968;&#33268;&#19988;&#26174;&#33879;&#30340;&#23433;&#20840;&#24615;&#25913;&#36827;&#65288;&#39640;&#36798;+13.8%&#30340;&#23433;&#20840;&#20998;&#25968;&#65289;&#65292;&#32780;&#19988;&#23545;&#36890;&#29992;&#20219;&#21153;&#20063;&#26377;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11889v1 Announce Type: new  Abstract: With the development of instruction-tuned large language models (LLMs), improving the safety of LLMs has become more critical. However, the current approaches for aligning the LLMs output with expected safety usually require substantial training efforts, e.g., high-quality safety data and expensive computational resources, which are costly and inefficient. To this end, we present reverse prompt contrastive decoding (ROSE), a simple-yet-effective method to directly boost the safety of existing instruction-tuned LLMs without any additional training. The principle of ROSE is to improve the probability of desired safe output via suppressing the undesired output induced by the carefully-designed reverse prompts. Experiments on 6 safety and 2 general-purpose tasks show that, our ROSE not only brings consistent and significant safety improvements (up to +13.8% safety score) upon 5 types of instruction-tuned LLMs, but also benefits the general-p
&lt;/p&gt;</description></item><item><title>FIPO&#25552;&#20986;&#20102;&#22522;&#20110;&#33258;&#30001;&#24418;&#24335;&#25351;&#23548;&#30340;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#65292;&#32467;&#21512;&#20559;&#22909;&#25968;&#25454;&#38598;&#21644;&#27169;&#22359;&#21270;&#24494;&#35843;&#27169;&#24335;&#65292;&#37325;&#26032;&#26500;&#24605;&#20102;&#20248;&#21270;&#36807;&#31243;&#24182;&#23454;&#29616;&#20102;&#28789;&#27963;&#30340;&#20219;&#21153;&#25552;&#31034;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2402.11811</link><description>&lt;p&gt;
FIPO&#65306;&#22522;&#20110;&#33258;&#30001;&#24418;&#24335;&#25351;&#23548;&#30340;&#25552;&#31034;&#20248;&#21270;&#19982;&#20559;&#22909;&#25968;&#25454;&#38598;&#21644;&#27169;&#22359;&#21270;&#24494;&#35843;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
FIPO: Free-form Instruction-oriented Prompt Optimization with Preference Dataset and Modular Fine-tuning Schema
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11811
&lt;/p&gt;
&lt;p&gt;
FIPO&#25552;&#20986;&#20102;&#22522;&#20110;&#33258;&#30001;&#24418;&#24335;&#25351;&#23548;&#30340;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#65292;&#32467;&#21512;&#20559;&#22909;&#25968;&#25454;&#38598;&#21644;&#27169;&#22359;&#21270;&#24494;&#35843;&#27169;&#24335;&#65292;&#37325;&#26032;&#26500;&#24605;&#20102;&#20248;&#21270;&#36807;&#31243;&#24182;&#23454;&#29616;&#20102;&#28789;&#27963;&#30340;&#20219;&#21153;&#25552;&#31034;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20419;&#36827;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#26368;&#32456;&#29992;&#25143;-&#26426;&#22120;&#20154;&#20132;&#20114;&#20013;&#30340;&#28145;&#24230;&#26234;&#33021;&#26041;&#38754;&#65292;&#25552;&#31034;&#21019;&#20316;&#30340;&#33402;&#26415;&#34987;&#35270;&#20026;&#26222;&#36890;&#29992;&#25143;&#30340;&#19968;&#39033;&#20851;&#38190;&#20294;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#19982;&#20043;&#21069;&#22522;&#20110;&#27169;&#22411;&#32780;&#19981;&#32771;&#34385;&#25351;&#23548;&#30340;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#24418;&#25104;&#23545;&#27604;&#65292;&#36825;&#20123;&#26041;&#27861;&#20026;&#39044;&#23450;&#20041;&#30446;&#26631;&#27169;&#22411;&#20135;&#29983;&#20102;&#20809;&#28369;&#30340;&#32467;&#26524;&#65292;&#20294;&#22312;&#20351;&#29992;&#24320;&#31665;&#21363;&#29992;&#27169;&#22411;&#26102;&#23481;&#26131;&#24555;&#36895;&#36864;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#33258;&#30001;&#24418;&#24335;&#25351;&#23548;&#30340;&#25552;&#31034;&#20248;&#21270;&#65288;FIPO&#65289;&#12290;&#36825;&#31181;&#26041;&#27861;&#24471;&#21040;&#25105;&#20204;&#30340;&#22823;&#35268;&#27169;&#25552;&#31034;&#20559;&#22909;&#25968;&#25454;&#38598;&#30340;&#25903;&#25345;&#65292;&#24182;&#37319;&#29992;&#27169;&#22359;&#21270;&#24494;&#35843;&#27169;&#24335;&#12290;FIPO&#27169;&#24335;&#37325;&#26032;&#26500;&#24605;&#20102;&#20248;&#21270;&#36807;&#31243;&#65292;&#23558;&#20854;&#20998;&#35299;&#20026;&#21487;&#31649;&#29702;&#30340;&#27169;&#22359;&#65292;&#20197;&#21160;&#24577;&#35843;&#25972;&#20869;&#23481;&#30340;&#20803;&#25552;&#31034;&#20026;&#38170;&#28857;&#12290;&#36825;&#20801;&#35768;&#28789;&#27963;&#25972;&#21512;&#21407;&#22987;&#20219;&#21153;&#25351;&#23548;&#12289;&#21487;&#36873;&#25351;&#23548;&#21709;&#24212;&#21644;&#21487;&#36873;&#30495;&#23454;&#20540;&#65292;&#20197;&#29983;&#25104;&#32463;&#36807;&#31934;&#24515;&#20248;&#21270;&#30340;&#20219;&#21153;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11811v1 Announce Type: new  Abstract: In the quest to facilitate the deep intelligence of Large Language Models (LLMs) accessible in final-end user-bot interactions, the art of prompt crafting emerges as a critical yet complex task for the average user. Contrast to previous model-oriented yet instruction-agnostic Automatic Prompt Optimization methodologies, yielding polished results for predefined target models while suffering rapid degradation with out-of-box models, we present Free-form Instruction-oriented Prompt Optimization (FIPO). This approach is supported by our large-scale prompt preference dataset and employs a modular fine-tuning schema. The FIPO schema reimagines the optimization process into manageable modules, anchored by a meta prompt that dynamically adapts content. This allows for the flexible integration of the raw task instruction, the optional instruction response, and the optional ground truth to produce finely optimized task prompts. The FIPO preference
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#37319;&#29992;GPT-4V&#21512;&#25104;&#30340;&#39640;&#36136;&#37327;&#35757;&#32451;&#25968;&#25454;&#65292;&#30740;&#31350;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;ALLaVA&#65292;&#19968;&#20010;&#36731;&#37327;&#32423;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;12&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#34920;&#29616;&#20986;&#19982;&#26368;&#22810;3B LVLMs&#31454;&#20105;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.11684</link><description>&lt;p&gt;
&#21033;&#29992;GPT4V&#21512;&#25104;&#25968;&#25454;&#23454;&#29616;&#36731;&#37327;&#32423;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;ALLaVA
&lt;/p&gt;
&lt;p&gt;
ALLaVA: Harnessing GPT4V-synthesized Data for A Lite Vision-Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11684
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#37319;&#29992;GPT-4V&#21512;&#25104;&#30340;&#39640;&#36136;&#37327;&#35757;&#32451;&#25968;&#25454;&#65292;&#30740;&#31350;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;ALLaVA&#65292;&#19968;&#20010;&#36731;&#37327;&#32423;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;12&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#34920;&#29616;&#20986;&#19982;&#26368;&#22810;3B LVLMs&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(LVLMs)&#30340;&#21457;&#23637;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#22810;&#27169;&#24577;&#36755;&#20837;&#65292;&#20294;&#37096;&#32626;&#26102;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#65292;&#23588;&#20854;&#26159;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#37319;&#29992;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#24357;&#21512;&#20256;&#32479;&#23610;&#24230;LVLMs&#21644;&#36164;&#28304;&#21451;&#22909;&#22411;Lite&#29256;&#26412;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#20026;&#27492;&#65292;&#36890;&#36807;&#21033;&#29992;GPT-4V&#29983;&#25104;&#35814;&#32454;&#25551;&#36848;&#12289;&#22797;&#26434;&#25512;&#29702;&#25351;&#20196;&#21644;&#22270;&#29255;&#35814;&#32454;&#31572;&#26696;&#30340;&#33021;&#21147;&#21019;&#24314;&#20102;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#12290;&#21033;&#29992;&#25105;&#20204;&#30340;&#25968;&#25454;&#35757;&#32451;&#30340;&#32467;&#26524;&#27169;&#22411;ALLaVA&#22312;12&#39033;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#19982;&#26368;&#22810;3B LVLMs&#31454;&#20105;&#24615;&#33021;&#12290;&#36825;&#39033;&#24037;&#20316;&#31361;&#20986;&#20102;&#22312;&#35774;&#35745;&#26356;&#39640;&#25928;&#30340;LVLMs&#20013;&#37319;&#29992;&#39640;&#36136;&#37327;&#25968;&#25454;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#30340;&#22312;&#32447;&#28436;&#31034;&#21487;&#22312;\url{https://allava.freedomai.cn}&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11684v1 Announce Type: cross  Abstract: Recent advancements in Large Vision-Language Models (LVLMs) have enabled processing of multimodal inputs in language models but require significant computational resources for deployment, especially in edge devices. This study aims to bridge the performance gap between traditional-scale LVLMs and resource-friendly lite versions by adopting high-quality training data. To do this, a synthetic dataset is created by leveraging GPT-4V's ability to generate detailed captions, complex reasoning instructions and detailed answers from images. The resulted model trained with our data, ALLaVA, achieves competitive performance on 12 benchmarks up to 3B LVLMs. This work highlights the feasibility of adopting high-quality data in crafting more efficient LVLMs. Our online demo is available at \url{https://allava.freedomai.cn}.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;GPT-3.5 Turbo&#12289;GPT-4&#21644;Flan-T5&#27169;&#22411;&#22312;&#35782;&#21035;&#26032;&#38395;&#26631;&#39064;&#20013;&#26694;&#26550;&#20559;&#35265;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#21487;&#35299;&#37322;&#25552;&#31034;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#36825;&#20123;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#65292;GPT-4&#22312;&#23569;&#23556;&#22330;&#26223;&#20013;&#34920;&#29616;&#36739;&#22909;&#65292;&#32780;FLAN-T5&#30340;&#34920;&#29616;&#36739;&#24046;&#65292;&#25351;&#20986;&#36739;&#23567;&#27169;&#22411;&#21487;&#33021;&#38656;&#35201;&#26356;&#22810;&#20219;&#21153;&#29305;&#23450;&#24494;&#35843;&#12290;</title><link>https://arxiv.org/abs/2402.11621</link><description>&lt;p&gt;
&#35299;&#30721;&#26032;&#38395;&#21465;&#20107;&#65306;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26694;&#26550;&#20559;&#35265;&#26816;&#27979;&#20013;&#30340;&#20851;&#38190;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Decoding News Narratives: A Critical Analysis of Large Language Models in Framing Bias Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11621
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;GPT-3.5 Turbo&#12289;GPT-4&#21644;Flan-T5&#27169;&#22411;&#22312;&#35782;&#21035;&#26032;&#38395;&#26631;&#39064;&#20013;&#26694;&#26550;&#20559;&#35265;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#21487;&#35299;&#37322;&#25552;&#31034;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#36825;&#20123;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#65292;GPT-4&#22312;&#23569;&#23556;&#22330;&#26223;&#20013;&#34920;&#29616;&#36739;&#22909;&#65292;&#32780;FLAN-T5&#30340;&#34920;&#29616;&#36739;&#24046;&#65292;&#25351;&#20986;&#36739;&#23567;&#27169;&#22411;&#21487;&#33021;&#38656;&#35201;&#26356;&#22810;&#20219;&#21153;&#29305;&#23450;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#26816;&#39564;GPT-3.5 Turbo&#12289;GPT-4&#21644;Flan-T5&#27169;&#22411;&#22312;&#36890;&#36807;&#38646;&#23556;&#12289;&#23569;&#23556;&#21644;&#21487;&#35299;&#37322;&#25552;&#31034;&#26041;&#27861;&#26816;&#27979;&#26032;&#38395;&#26631;&#39064;&#20013;&#26694;&#26550;&#20559;&#35265;&#30340;&#34920;&#29616;&#65292;&#20026;LLMs&#22312;&#31038;&#20250;&#31185;&#23398;&#20013;&#30340;&#36866;&#29992;&#24615;&#19981;&#26029;&#25193;&#23637;&#30340;&#30740;&#31350;&#20570;&#20986;&#36129;&#29486;&#12290;&#25105;&#20204;&#35780;&#20272;&#30340;&#19968;&#20010;&#20851;&#38190;&#27934;&#23519;&#26159;&#65292;&#21487;&#35299;&#37322;&#25552;&#31034;&#22312;&#25552;&#21319;&#36825;&#20123;&#27169;&#22411;&#21487;&#38752;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#26174;&#33879;&#25928;&#26524;&#65292;&#20984;&#26174;&#20102;&#35299;&#37322;&#35774;&#32622;&#23545;&#20110;&#31038;&#20250;&#31185;&#23398;&#20851;&#20110;&#26694;&#26550;&#20559;&#35265;&#30340;&#30740;&#31350;&#30340;&#37325;&#35201;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;GPT-4&#22312;&#25552;&#20379;&#19968;&#31995;&#21015;&#30456;&#20851;&#39046;&#22495;&#20869;&#20363;&#23376;&#26102;&#65292;&#34920;&#29616;&#20986;&#25913;&#36827;&#30340;&#23569;&#23556;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#12290;FLAN-T5&#30340;&#34920;&#29616;&#19981;&#20339;&#34920;&#26126;&#36739;&#23567;&#30340;&#27169;&#22411;&#21487;&#33021;&#38656;&#35201;&#39069;&#22806;&#30340;&#20219;&#21153;&#29305;&#23450;&#24494;&#35843;&#20197;&#35782;&#21035;&#26694;&#26550;&#20559;&#35265;&#26816;&#27979;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#21457;&#29616;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;GPT-4&#65292;&#32463;&#24120;&#23558;&#24773;&#32490;&#35821;&#35328;&#35823;&#35299;&#20026;&#26694;&#26550;&#20559;&#35265;&#30340;&#25351;&#26631;&#65292;&#31361;&#26174;&#20102;&#21306;&#20998;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11621v1 Announce Type: new  Abstract: This work contributes to the expanding research on the applicability of LLMs in social sciences by examining the performance of GPT-3.5 Turbo, GPT-4, and Flan-T5 models in detecting framing bias in news headlines through zero-shot, few-shot, and explainable prompting methods. A key insight from our evaluation is the notable efficacy of explainable prompting in enhancing the reliability of these models, highlighting the importance of explainable settings for social science research on framing bias. GPT-4, in particular, demonstrated enhanced performance in few-shot scenarios when presented with a range of relevant, in-domain examples. FLAN-T5's poor performance indicates that smaller models may require additional task-specific fine-tuning for identifying framing bias detection. Our study also found that models, particularly GPT-4, often misinterpret emotional language as an indicator of framing bias, underscoring the challenge of distingu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;KG&#30693;&#35782;&#27880;&#20837;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#65292;&#25506;&#32034;&#20026;LLMs&#25552;&#20379;&#30693;&#35782;&#22270;&#35889;&#30693;&#35782;&#30340;&#26368;&#20339;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#23427;&#20204;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.11541</link><description>&lt;p&gt;
&#36870;&#21521;&#35748;&#30693;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27604;&#25105;&#20204;&#24819;&#35937;&#30340;&#26356;&#25797;&#38271;&#29702;&#35299;&#30693;&#35782;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
Counter-intuitive: Large Language Models Can Better Understand Knowledge Graphs Than We Thought
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;KG&#30693;&#35782;&#27880;&#20837;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#65292;&#25506;&#32034;&#20026;LLMs&#25552;&#20379;&#30693;&#35782;&#22270;&#35889;&#30693;&#35782;&#30340;&#26368;&#20339;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#23427;&#20204;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#24182;&#20943;&#23569;&#23427;&#20204;&#30340;&#24187;&#35273;&#30340;&#26041;&#27861;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#30446;&#21069;&#23545;&#22914;&#20309;&#20351;LLMs&#33021;&#22815;&#21363;&#26102;&#25972;&#21512;KGs&#20013;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#30340;&#25506;&#32034;&#36824;&#19981;&#36275;&#12290;&#26412;&#25991;&#37319;&#29992;&#22797;&#26434;&#38382;&#39064;&#22238;&#31572;&#65288;CQA&#65289;&#20316;&#20026;&#19968;&#39033;&#20219;&#21153;&#65292;&#35780;&#20272;LLM&#29702;&#35299;KG&#30693;&#35782;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23545;KG&#30693;&#35782;&#27880;&#20837;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#65288;&#20174;&#19977;&#20803;&#32452;&#21040;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#65289;&#65292;&#26088;&#22312;&#25506;&#32034;&#20026;LLMs&#25552;&#20379;KG&#30693;&#35782;&#30340;&#26368;&#20339;&#25552;&#31034;&#26041;&#27861;&#65292;&#20174;&#32780;&#22686;&#24378;&#23427;&#20204;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11541v1 Announce Type: cross  Abstract: Although the method of enhancing large language models' (LLMs') reasoning ability and reducing their hallucinations through the use of knowledge graphs (KGs) has received widespread attention, the exploration of how to enable LLMs to integrate the structured knowledge in KGs on-the-fly remains inadequate. Researchers often co-train KG embeddings and LLM parameters to equip LLMs with the ability of comprehending KG knowledge. However, this resource-hungry training paradigm significantly increases the model learning cost and is also unsuitable for non-open-source, black-box LLMs. In this paper, we employ complex question answering (CQA) as a task to assess the LLM's ability of comprehending KG knowledge. We conducted a comprehensive comparison of KG knowledge injection methods (from triples to natural language text), aiming to explore the optimal prompting method for supplying KG knowledge to LLMs, thereby enhancing their comprehension o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#25910;&#38598;&#27969;&#31243;&#65292;&#21033;&#29992; GPT-4 &#27169;&#25311;&#23244;&#30097;&#20154;&#19982;&#35686;&#23448;&#20043;&#38388;&#30340;&#35282;&#33394;&#25198;&#28436;&#65292;&#20197;&#35299;&#20915;&#27450;&#39575;&#26816;&#27979;&#39046;&#22495;&#38754;&#20020;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#24182;&#23558;&#20256;&#32479;&#30340;&#27450;&#39575;&#26816;&#27979;&#20219;&#21153;&#25299;&#23637;&#21040;&#27450;&#39575;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.11432</link><description>&lt;p&gt;
&#27450;&#39575;&#26816;&#27979;&#33021;&#22815;&#26356;&#28145;&#20837;&#21527;&#65311;&#29992;&#20110;&#27450;&#39575;&#25512;&#29702;&#30340;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#21644;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Can Deception Detection Go Deeper? Dataset, Evaluation, and Benchmark for Deception Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11432
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#25910;&#38598;&#27969;&#31243;&#65292;&#21033;&#29992; GPT-4 &#27169;&#25311;&#23244;&#30097;&#20154;&#19982;&#35686;&#23448;&#20043;&#38388;&#30340;&#35282;&#33394;&#25198;&#28436;&#65292;&#20197;&#35299;&#20915;&#27450;&#39575;&#26816;&#27979;&#39046;&#22495;&#38754;&#20020;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#24182;&#23558;&#20256;&#32479;&#30340;&#27450;&#39575;&#26816;&#27979;&#20219;&#21153;&#25299;&#23637;&#21040;&#27450;&#39575;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11432v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#30001;&#20110;&#22312;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#27450;&#39575;&#26816;&#27979;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#30446;&#21069;&#65292;&#25968;&#25454;&#31232;&#32570;&#38459;&#30861;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#19968;&#26041;&#38754;&#65292;&#38599;&#20323;&#21442;&#19982;&#32773;&#27169;&#25311;&#27450;&#39575;&#22330;&#26223;&#25104;&#26412;&#39640;&#26114;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#24456;&#38590;&#22312;&#20114;&#32852;&#32593;&#19978;&#25910;&#38598;&#21253;&#21547;&#27450;&#39575;&#34892;&#20026;&#30340;&#35270;&#39057;&#12290;&#20026;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#25910;&#38598;&#27969;&#31243;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992; GPT-4 &#27169;&#25311;&#20102;&#23244;&#30097;&#20154;&#21644;&#35686;&#23448;&#20043;&#38388;&#30340;&#35282;&#33394;&#25198;&#28436;&#12290;&#22312;&#23457;&#35759;&#36807;&#31243;&#20013;&#65292;&#23244;&#30097;&#20154;&#21521;&#35686;&#23448;&#25746;&#35854;&#65292;&#35797;&#22270;&#36867;&#36991;&#29359;&#32618;&#36131;&#20219;&#65292;&#32780;&#35686;&#23448;&#25581;&#38706;&#20102;&#20107;&#23454;&#24182;&#25910;&#38598;&#20102;&#35777;&#25454;&#12290;&#19982;&#20808;&#21069;&#30340;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;&#36825;&#19968;&#31574;&#30053;&#20943;&#23569;&#20102;&#25968;&#25454;&#25910;&#38598;&#25104;&#26412;&#65292;&#20026;&#22686;&#21152;&#25968;&#25454;&#38598;&#35268;&#27169;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#23558;&#20256;&#32479;&#30340;&#27450;&#39575;&#26816;&#27979;&#20219;&#21153;&#25193;&#23637;&#21040;&#27450;&#39575;&#25512;&#29702;&#65292;&#36827;&#19968;&#27493;&#20026;&#27450;&#39575;&#34892;&#20026;&#25552;&#20379;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11432v1 Announce Type: new  Abstract: Deception detection has attracted increasing attention due to its importance in many practical scenarios. Currently, data scarcity harms the development of this field. On the one hand, it is costly to hire participants to simulate deception scenarios. On the other hand, it is difficult to collect videos containing deceptive behaviors on the Internet. To address data scarcity, this paper proposes a new data collection pipeline. Specifically, we use GPT-4 to simulate a role-play between a suspect and a police officer. During interrogation, the suspect lies to the police officer to evade responsibility for the crime, while the police officer uncovers the truth and gathers evidence. Compared with previous datasets, this strategy reduces data collection costs, providing a promising way to increase the dataset size. Meanwhile, we extend the traditional deception detection task to deception reasoning, further providing evidence for deceptive pa
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#22260;&#32469;&#20307;&#32946;&#25968;&#25454;&#20998;&#26512;&#23637;&#24320;&#30340;&#22235;&#39033;&#26032;&#20219;&#21153;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#20540;&#25512;&#29702;&#21644;&#20449;&#24687;&#34701;&#21512;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.10979</link><description>&lt;p&gt;
SportsMetrics:&#23558;&#25991;&#26412;&#21644;&#25968;&#20540;&#25968;&#25454;&#34701;&#21512;&#20197;&#29702;&#35299;LLM&#20013;&#30340;&#20449;&#24687;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
SportsMetrics: Blending Text and Numerical Data to Understand Information Fusion in LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10979
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#22260;&#32469;&#20307;&#32946;&#25968;&#25454;&#20998;&#26512;&#23637;&#24320;&#30340;&#22235;&#39033;&#26032;&#20219;&#21153;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#20540;&#25512;&#29702;&#21644;&#20449;&#24687;&#34701;&#21512;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10979v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#25972;&#21512;&#25991;&#26412;&#25991;&#26723;&#21644;&#25968;&#25454;&#24211;&#35760;&#24405;&#31561;&#21508;&#31181;&#25968;&#25454;&#31867;&#22411;&#36827;&#34892;&#20808;&#36827;&#20998;&#26512;&#20855;&#26377;&#37325;&#35201;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#34701;&#21512;&#25991;&#26412;&#21644;&#25968;&#20540;&#25968;&#25454;&#23384;&#22312;&#37325;&#22823;&#25361;&#25112;&#12290;LLMs&#38656;&#35201;&#22788;&#29702;&#21644;&#20132;&#21449;&#24341;&#29992;&#23454;&#20307;&#21644;&#25968;&#23383;&#65292;&#22788;&#29702;&#25968;&#25454;&#19981;&#19968;&#33268;&#24615;&#21644;&#20887;&#20313;&#65292;&#24182;&#21457;&#23637;&#35268;&#21010;&#33021;&#21147;&#65292;&#27604;&#22914;&#26500;&#24314;&#29992;&#20110;&#31649;&#29702;&#22797;&#26434;&#25968;&#25454;&#26597;&#35810;&#30340;&#24037;&#20316;&#20869;&#23384;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22260;&#32469;&#20307;&#32946;&#25968;&#25454;&#20998;&#26512;&#30340;&#22235;&#39033;&#26032;&#39062;&#20219;&#21153;&#65292;&#20197;&#35780;&#20272;LLMs&#30340;&#25968;&#20540;&#25512;&#29702;&#21644;&#20449;&#24687;&#34701;&#21512;&#33021;&#21147;&#12290;&#36825;&#20123;&#20219;&#21153;&#28041;&#21450;&#21521;LLMs&#25552;&#20379;&#35814;&#32454;&#30340;&#36880;&#22330;&#27604;&#36187;&#25551;&#36848;&#65292;&#28982;&#21518;&#22312;&#38754;&#23545;&#35832;&#22914;&#26032;&#27604;&#36187;&#35268;&#21017;&#12289;&#26356;&#38271;&#25345;&#32493;&#26102;&#38388;&#12289;&#25925;&#20107;&#28151;&#20081;&#20197;&#21450;&#20998;&#26512;&#27604;&#36187;&#25688;&#35201;&#20013;&#30340;&#20851;&#38190;&#32479;&#35745;&#25968;&#25454;&#31561;&#23545;&#25239;&#24615;&#22330;&#26223;&#12290;&#25105;&#20204;&#23545;NBA&#21644;NFL&#27604;&#36187;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#20197;&#35780;&#20272;LLMs&#22312;&#35813;&#39046;&#22495;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10979v1 Announce Type: cross  Abstract: Large language models hold significant potential for integrating various data types, such as text documents and database records, for advanced analytics. However, blending text and numerical data presents substantial challenges. LLMs need to process and cross-reference entities and numbers, handle data inconsistencies and redundancies, and develop planning capabilities such as building a working memory for managing complex data queries. In this paper, we introduce four novel tasks centered around sports data analytics to evaluate the numerical reasoning and information fusion capabilities of LLMs. These tasks involve providing LLMs with detailed, play-by-play sports game descriptions, then challenging them with adversarial scenarios such as new game rules, longer durations, scrambled narratives, and analyzing key statistics in game summaries. We conduct extensive experiments on NBA and NFL games to assess the performance of LLMs on the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#29289;&#21464;&#37327;&#23545;LLMs&#27169;&#25311;&#19981;&#21516;&#35270;&#35282;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20154;&#29289;&#21464;&#37327;&#22312;&#29616;&#26377;&#20027;&#35266;NLP&#25968;&#25454;&#38598;&#20013;&#35299;&#37322;&#33021;&#21147;&#26377;&#38480;&#65292;&#20294;&#36890;&#36807;&#25552;&#31034;&#26041;&#24335;&#21152;&#20837;&#21487;&#20197;&#30053;&#24494;&#25913;&#21892;&#27169;&#22411;&#39044;&#27979;&#65292;&#23588;&#20854;&#22312;&#23384;&#22312;&#20105;&#35758;&#20294;&#33539;&#22260;&#26377;&#38480;&#30340;&#25968;&#25454;&#26679;&#26412;&#19978;&#25928;&#26524;&#26368;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.10811</link><description>&lt;p&gt;
&#22312;LLM&#27169;&#25311;&#20013;&#37327;&#21270;Persona&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Quantifying the Persona Effect in LLM Simulations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#29289;&#21464;&#37327;&#23545;LLMs&#27169;&#25311;&#19981;&#21516;&#35270;&#35282;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20154;&#29289;&#21464;&#37327;&#22312;&#29616;&#26377;&#20027;&#35266;NLP&#25968;&#25454;&#38598;&#20013;&#35299;&#37322;&#33021;&#21147;&#26377;&#38480;&#65292;&#20294;&#36890;&#36807;&#25552;&#31034;&#26041;&#24335;&#21152;&#20837;&#21487;&#20197;&#30053;&#24494;&#25913;&#21892;&#27169;&#22411;&#39044;&#27979;&#65292;&#23588;&#20854;&#22312;&#23384;&#22312;&#20105;&#35758;&#20294;&#33539;&#22260;&#26377;&#38480;&#30340;&#25968;&#25454;&#26679;&#26412;&#19978;&#25928;&#26524;&#26368;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#27169;&#25311;&#20154;&#31867;&#35821;&#35328;&#20351;&#29992;&#21644;&#34892;&#20026;&#26041;&#38754;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#28508;&#21147;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#20154;&#29289;&#21464;&#37327;&#19982;LLMs&#27169;&#25311;&#19981;&#21516;&#35270;&#35282;&#30340;&#33021;&#21147;&#30340;&#20132;&#38598;&#12290;&#25105;&#20204;&#21457;&#29616;&#20154;&#29289;&#21464;&#37327;&#21487;&#20197;&#35299;&#37322;&#29616;&#26377;&#20027;&#35266;NLP&#25968;&#25454;&#38598;&#20013;&lt;10\%&#30340;&#27880;&#37322;&#21464;&#24322;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#25552;&#31034;&#22312;LLMs&#20013;&#21152;&#20837;&#20182;&#20204;&#33021;&#24102;&#26469;&#36866;&#24230;&#30340;&#25913;&#36827;&#12290;Persona&#25552;&#31034;&#22312;&#27880;&#37322;&#32773;&#20043;&#38388;&#23384;&#22312;&#20105;&#35758;&#20294;&#33539;&#22260;&#26377;&#38480;&#30340;&#25968;&#25454;&#26679;&#26412;&#19978;&#25928;&#26524;&#26368;&#22909;&#12290;&#23384;&#22312;&#32447;&#24615;&#30456;&#20851;&#24615;&#65306;&#20154;&#26684;&#21464;&#37327;&#23545;&#20154;&#31867;&#27880;&#37322;&#30340;&#24433;&#21709;&#36234;&#22823;&#65292;LLMs&#20351;&#29992;Persona&#25552;&#31034;&#30340;&#39044;&#27979;&#23601;&#36234;&#22909;&#12290;&#28982;&#32780;&#65292;&#24403;&#20154;&#29289;&#21464;&#37327;&#30340;&#25928;&#29992;&#36739;&#20302;&#65288;&#21363;&#35299;&#37322;&#20154;&#31867;&#27880;&#37322;&#30340;&lt;10\%&#65289;&#26102;&#65292;Persona&#25552;&#31034;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#12290;&#22823;&#22810;&#25968;&#20027;&#35266;NLP&#25968;&#25454;&#38598;&#37117;&#23646;&#20110;&#36825;&#19968;&#31867;&#21035;&#65292;&#23545;&#27169;&#25311;&#22810;&#20803;&#35270;&#35282;&#20135;&#29983;&#24576;&#30097;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10811v1 Announce Type: new  Abstract: Large language models (LLMs) have shown remarkable promise in simulating human language use and behavior. In this study, we delve into the intersection of persona variables and the capability of LLMs to simulate different perspectives. We find that persona variables can explain &lt;10\% variance in annotations in existing subjective NLP datasets. Nonetheless, incorporating them via prompting in LLMs provides modest improvement. Persona prompting is most effective on data samples where disagreements among annotators are frequent yet confined to a limited range. A linear correlation exists: the more persona variables influence human annotations, the better LLMs predictions are using persona prompting. However, when the utility of persona variables is low (i.e., explaining &lt;10\% of human annotations), persona prompting has little effect. Most subjective NLP datasets fall into this category, casting doubt on simulating diverse perspectives in t
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23569;&#26679;&#26412;&#19978;&#19979;&#25991;&#35838;&#31243;&#23398;&#20064;&#65288;ICCL&#65289;&#26041;&#27861;&#65292;&#36880;&#28176;&#22686;&#21152;&#25552;&#31034;&#28436;&#31034;&#30340;&#22797;&#26434;&#24615;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;ICCL&#23545;&#24320;&#28304;LLMs&#26377;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.10738</link><description>&lt;p&gt;
&#35753;&#25105;&#20204;&#19968;&#27493;&#19968;&#27493;&#23398;&#20064;&#65306;&#36890;&#36807;&#35838;&#31243;&#23398;&#20064;&#22686;&#24378;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Let's Learn Step by Step: Enhancing In-Context Learning Ability with Curriculum Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10738
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23569;&#26679;&#26412;&#19978;&#19979;&#25991;&#35838;&#31243;&#23398;&#20064;&#65288;ICCL&#65289;&#26041;&#27861;&#65292;&#36880;&#28176;&#22686;&#21152;&#25552;&#31034;&#28436;&#31034;&#30340;&#22797;&#26434;&#24615;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;ICCL&#23545;&#24320;&#28304;LLMs&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28436;&#31034;&#25490;&#24207;&#26159;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#30340;&#37325;&#35201;&#31574;&#30053;&#65292;&#21487;&#20197;&#26174;&#33879;&#24433;&#21709;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#24403;&#21069;&#30340;&#25490;&#24207;&#26041;&#27861;&#38656;&#35201;&#39069;&#22806;&#30340;&#30693;&#35782;&#21644;&#30456;&#20284;&#24615;&#35745;&#31639;&#12290;&#25105;&#20204;&#20513;&#23548;&#23569;&#26679;&#26412;&#19978;&#19979;&#25991;&#35838;&#31243;&#23398;&#20064;&#65288;ICCL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;ICL&#28436;&#31034;&#25490;&#24207;&#26041;&#27861;&#65292;&#20854;&#26263;&#31034;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#36880;&#28176;&#22686;&#21152;&#25552;&#31034;&#28436;&#31034;&#30340;&#22797;&#26434;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19977;&#20010;&#23454;&#39564;&#65292;&#35752;&#35770;ICCL&#30340;&#26377;&#25928;&#24615;&#65292;LLM&#30340;ICCL&#33021;&#21147;&#24418;&#25104;&#26426;&#21046;&#20197;&#21450;&#25490;&#24207;&#20027;&#39064;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ICCL&#22312;&#25351;&#23548;&#35843;&#25972;&#38454;&#27573;&#24320;&#21457;&#65292;&#23545;&#20110;&#24320;&#28304;LLMs&#26159;&#26377;&#25928;&#30340;&#12290;&#27492;&#22806;&#65292;LLMs&#22312;&#36776;&#21035;&#28436;&#31034;&#38590;&#24230;&#32423;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#27604;&#20154;&#31867;&#26356;&#24369;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;https://github.com/61peng/cu&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10738v1 Announce Type: new  Abstract: Demonstration ordering, which is an important strategy for in-context learning (ICL), can significantly affects the performance of large language models (LLMs). However, most of the current approaches of ordering require additional knowledge and similarity calculation. We advocate the few-shot in-context curriculum learning (ICCL), a simple but effective demonstration ordering method for ICL, which implies gradually increasing the complexity of prompt demonstrations during the inference process. Then we design three experiments to discuss the effectiveness of ICCL, the formation mechanism of LLM's ICCL capability, and the impact of ordering subjects. Experimental results demonstrate that ICCL, developed during the instruction-tuning stage, is effective for open-source LLMs. Moreover, LLMs exhibit a weaker capacity compared to humans in discerning the difficulty levels of demonstrations. We release our code at https://github.com/61peng/cu
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#21508;&#31181;&#36328;&#35821;&#35328;&#35789;&#27719;&#36866;&#24212;&#26041;&#27861;&#23545;&#25552;&#39640;&#29983;&#25104;LLM&#25512;&#29702;&#25928;&#29575;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.10712</link><description>&lt;p&gt;
&#19968;&#39033;&#20851;&#20110;&#36328;&#35821;&#35328;&#35789;&#27719;&#36866;&#24212;&#29992;&#20110;&#39640;&#25928;&#29983;&#25104;LLM&#25512;&#29702;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study on Cross-lingual Vocabulary Adaptation for Efficient Generative LLM Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10712
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#21508;&#31181;&#36328;&#35821;&#35328;&#35789;&#27719;&#36866;&#24212;&#26041;&#27861;&#23545;&#25552;&#39640;&#29983;&#25104;LLM&#25512;&#29702;&#25928;&#29575;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10712v1 &#36890;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#21457;&#23637;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#20998;&#35789;&#22120;&#12289;&#35789;&#27719;&#21644;&#39044;&#35757;&#32451;&#25968;&#25454;&#12290;&#23613;&#31649;&#19968;&#20123;LLMs&#20855;&#26377;&#22810;&#35821;&#35328;&#33021;&#21147;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#29983;&#25104;&#33521;&#35821;&#20197;&#22806;&#30340;&#20854;&#20182;&#35821;&#35328;&#26102;&#65292;&#23427;&#20204;&#30340;&#25512;&#29702;&#25928;&#29575;&#20250;&#19979;&#38477;&#12290;&#36825;&#23548;&#33268;&#25512;&#29702;&#26102;&#38388;&#21644;&#25104;&#26412;&#22686;&#21152;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#36328;&#35821;&#35328;&#35789;&#27719;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#27169;&#22411;&#35843;&#25972;&#21040;&#30446;&#26631;&#35821;&#35328;&#65292;&#26088;&#22312;&#25552;&#39640;&#19979;&#28216;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#23545;&#25552;&#39640;&#29983;&#25104;LLM&#25512;&#29702;&#25928;&#29575;&#30340;&#26377;&#25928;&#24615;&#23578;&#26410;&#24471;&#21040;&#25506;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#20116;&#31181;&#29983;&#25104;LLMs&#65288;&#21253;&#25324;&#21333;&#35821;&#21644;&#22810;&#35821;&#27169;&#22411;&#65289;&#22312;&#22235;&#31181;&#35821;&#35328;&#31867;&#22411;&#22810;&#26679;&#19988;&#22235;&#31181;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#21508;&#31181;&#36328;&#35821;&#35328;&#35789;&#27719;&#36866;&#24212;&#26041;&#27861;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10712v1 Announce Type: cross  Abstract: The development of state-of-the-art generative large language models (LLMs) disproportionately relies on English-centric tokenizers, vocabulary and pre-training data. Despite the fact that some LLMs have multilingual capabilities, recent studies have shown that their inference efficiency deteriorates when generating text in languages other than English. This results in increased inference time and costs. Cross-lingual vocabulary adaptation methods have been proposed for adapting models to a target language aiming to improve downstream performance. However, the effectiveness of these methods on increasing inference efficiency of generative LLMs has yet to be explored. In this paper, we perform an empirical study of various cross-lingual vocabulary adaptation methods on five generative LLMs (including monolingual and multilingual models) across four typologically-diverse languages and four natural language understanding tasks. We find th
&lt;/p&gt;</description></item><item><title>MultiPoT &#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#21644;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#31181;&#32534;&#31243;&#35821;&#35328;&#30340;&#20248;&#21183;&#21644;&#22810;&#26679;&#24615;&#65292;&#22312;&#34920;&#29616;&#19978;&#26174;&#33879;&#20248;&#20110; Python &#33258;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10691</link><description>&lt;p&gt;
MultiPoT: &#22810;&#35821;&#35328;&#24605;&#32500;&#31243;&#24207;&#21033;&#29992;&#22810;&#31181;&#32534;&#31243;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
MultiPoT: Multilingual Program of Thoughts Harnesses Multiple Programming Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10691
&lt;/p&gt;
&lt;p&gt;
MultiPoT &#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#21644;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#31181;&#32534;&#31243;&#35821;&#35328;&#30340;&#20248;&#21183;&#21644;&#22810;&#26679;&#24615;&#65292;&#22312;&#34920;&#29616;&#19978;&#26174;&#33879;&#20248;&#20110; Python &#33258;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10691v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#30340; &#25688;&#35201;&#65306;&#24605;&#32500;&#31243;&#24207;&#65288;PoT&#65289;&#26159;&#19968;&#31181;&#20197;&#20854;&#21487;&#25191;&#34892;&#20013;&#38388;&#27493;&#39588;&#20026;&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#20854;&#30830;&#20445;&#25512;&#29702;&#36807;&#31243;&#20013;&#25968;&#20540;&#35745;&#31639;&#30340;&#20934;&#30830;&#24615;&#12290;&#30446;&#21069;&#65292;PoT&#20027;&#35201;&#20351;&#29992;Python&#12290;&#28982;&#32780;&#65292;&#20165;&#20381;&#36182;&#21333;&#19968;&#35821;&#35328;&#21487;&#33021;&#23548;&#33268;&#27425;&#20248;&#35299;&#20915;&#26041;&#26696;&#65292;&#24573;&#35270;&#20854;&#20182;&#32534;&#31243;&#35821;&#35328;&#30340;&#28508;&#22312;&#20248;&#21183;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;PoT&#20013;&#20351;&#29992;&#30340;&#32534;&#31243;&#35821;&#35328;&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#39564;&#65292;&#21457;&#29616;&#27809;&#26377;&#19968;&#31181;&#21333;&#19968;&#35821;&#35328;&#22312;&#25152;&#26377;&#20219;&#21153;&#21644;&#27169;&#22411;&#19978;&#22987;&#32456;&#25552;&#20379;&#26368;&#20339;&#24615;&#33021;&#12290;&#27599;&#31181;&#35821;&#35328;&#30340;&#26377;&#25928;&#24615;&#21462;&#20915;&#20110;&#20855;&#20307;&#24773;&#26223;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;MultiPoT&#30340;&#20219;&#21153;&#21644;&#27169;&#22411;&#26080;&#20851;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20174;&#21508;&#31181;&#35821;&#35328;&#20013;&#33719;&#21462;&#24378;&#22823;&#21644;&#22810;&#26679;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;MultiPoT &#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20248;&#20110;Python &#33258;&#19968;&#33268;&#24615;&#12290;&#27492;&#22806;&#65292;&#19982;&#26368;&#20339;&#27169;&#22411;&#30456;&#27604;&#65292;&#23427;&#23454;&#29616;&#20102;&#21487;&#27604;&#25110;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10691v1 Announce Type: new  Abstract: Program of Thoughts (PoT) is an approach characterized by its executable intermediate steps, which ensure the accuracy of the numerical calculations in the reasoning process. Currently, PoT primarily uses Python. However, relying solely on a single language may result in suboptimal solutions and overlook the potential benefits of other programming languages. In this paper, we conduct comprehensive experiments on the programming languages used in PoT and find that no single language consistently delivers optimal performance across all tasks and models. The effectiveness of each language varies depending on the specific scenarios. Inspired by this, we propose a task and model agnostic approach called MultiPoT, which harnesses strength and diversity from various languages. Experimental results reveal that it significantly outperforms Python Self-Consistency. Furthermore, it achieves comparable or superior performance compared to the best mo
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25351;&#23548;&#35843;&#33410;&#21644;&#21512;&#29702;&#24615;&#35780;&#20272;&#65292;&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;AbsInstruct&#26694;&#26550;&#26469;&#22686;&#24378;LLMs&#30340;&#25277;&#35937;&#33021;&#21147;&#65292;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.10646</link><description>&lt;p&gt;
&#20174;&#21512;&#29702;&#24615;&#35780;&#20272;&#20013;&#36890;&#36807;&#35299;&#37322;&#35843;&#33410;&#25552;&#21462;LLMs&#30340;&#25277;&#35937;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
AbsInstruct: Eliciting Abstraction Ability from LLMs through Explanation Tuning with Plausibility Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10646
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25351;&#23548;&#35843;&#33410;&#21644;&#21512;&#29702;&#24615;&#35780;&#20272;&#65292;&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;AbsInstruct&#26694;&#26550;&#26469;&#22686;&#24378;LLMs&#30340;&#25277;&#35937;&#33021;&#21147;&#65292;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25277;&#35937;&#33021;&#21147;&#23545;&#20154;&#31867;&#26234;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#20063;&#21487;&#20197;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#21463;&#30410;&#12290;&#29616;&#26377;&#24037;&#20316;&#34920;&#26126;LLMs&#22312;&#25277;&#35937;&#33021;&#21147;&#19978;&#23384;&#22312;&#19981;&#36275;&#65292;&#22914;&#20309;&#25913;&#36827;&#20173;&#26410;&#34987;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;AbsInstruct&#26694;&#26550;&#65292;&#36890;&#36807;&#25351;&#23548;&#35843;&#33410;&#26469;&#22686;&#24378;LLMs&#30340;&#25277;&#35937;&#33021;&#21147;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#28145;&#20837;&#35299;&#37322;&#26500;&#24314;&#25351;&#23548;&#65292;&#24110;&#21161;LLMs&#25429;&#25417;&#25277;&#35937;&#30340;&#28508;&#22312;&#21407;&#29702;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21512;&#29702;&#24615;&#20272;&#35745;&#22120;&#26469;&#36873;&#25321;&#26356;&#31526;&#21512;LLMs&#25277;&#35937;&#30693;&#35782;&#30340;&#25351;&#23548;&#20197;&#36827;&#34892;&#23545;&#40784;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;&#25277;&#35937;&#25351;&#23548;&#19982;&#36890;&#29992;&#25351;&#23548;&#32467;&#21512;&#20197;&#26500;&#24314;&#28151;&#21512;&#25968;&#25454;&#38598;&#12290;&#22823;&#37327;&#23454;&#39564;&#21644;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#26174;&#30528;&#22686;&#24378;LLMs&#30340;&#25277;&#35937;&#33021;&#21147;&#65292;&#24182;&#20445;&#25345;&#20854;&#36890;&#29992;&#30340;&#25351;&#23548;&#36981;&#24490;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10646v1 Announce Type: new  Abstract: Abstraction ability is crucial in human intelligence, which can also benefit various tasks in NLP study. Existing work shows that LLMs are deficient in abstract ability, and how to improve it remains unexplored. In this work, we design the framework AbsInstruct to enhance LLMs' abstraction ability through instruction tuning. The framework builds instructions with in-depth explanations to assist LLMs in capturing the underlying rationale of abstraction. Meanwhile, we introduce a plausibility estimator to select instructions that are more consistent with the abstraction knowledge of LLMs to be aligned. Then, our framework combines abstraction instructions with general-purpose ones to build a hybrid dataset. Extensive experiments and analyses demonstrate that our framework can considerably enhance LLMs' abstraction ability with strong generalization performance while maintaining their general instruction-following abilities.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35774;&#35745;&#20102;MORTISE&#31995;&#32479;&#65292;&#36890;&#36807;&#22810;&#20010;LLM&#27169;&#22359;&#30340;&#21327;&#20316;&#21162;&#21147;&#29983;&#25104;&#39640;&#24230;&#19982;&#35282;&#33394;&#30456;&#20851;&#30340;&#31215;&#26497;&#26597;&#35810;&#65292;&#36827;&#32780;&#25913;&#21892;&#35282;&#33394;&#25198;&#28436;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.10618</link><description>&lt;p&gt;
&#36890;&#36807;&#31215;&#26497;&#26597;&#35810;&#22686;&#24378;&#35282;&#33394;&#25198;&#28436;&#31995;&#32479;&#65306;&#35780;&#20272;&#19982;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Enhancing Role-playing Systems through Aggressive Queries: Evaluation and Improvement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35774;&#35745;&#20102;MORTISE&#31995;&#32479;&#65292;&#36890;&#36807;&#22810;&#20010;LLM&#27169;&#22359;&#30340;&#21327;&#20316;&#21162;&#21147;&#29983;&#25104;&#39640;&#24230;&#19982;&#35282;&#33394;&#30456;&#20851;&#30340;&#31215;&#26497;&#26597;&#35810;&#65292;&#36827;&#32780;&#25913;&#21892;&#35282;&#33394;&#25198;&#28436;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#23558;&#23545;&#35805;&#29983;&#25104;&#25512;&#21521;&#20102;&#26032;&#30340;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#22312;&#35282;&#33394;&#25198;&#28436;&#31995;&#32479;&#65288;RPSs&#65289;&#39046;&#22495;&#12290;&#23613;&#31649;&#29616;&#26377;&#22522;&#20110;LLM&#30340;RPS&#24050;&#32463;&#36890;&#36807;&#26222;&#36890;&#35282;&#33394;&#30456;&#20851;&#22521;&#35757;&#23545;&#35805;&#36827;&#34892;&#20102;&#22686;&#24378;&#65292;&#20294;&#22312;&#22788;&#29702;&#36793;&#30028;&#24773;&#22659;&#20013;&#30340;&#22797;&#26434;&#21644;&#21463;&#22256;&#26597;&#35810;&#26102;&#65292;&#20173;&#28982;&#23384;&#22312;&#30528;&#19982;&#35282;&#33394;&#19981;&#23545;&#40784;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#27169;&#22359;&#21270;&#21327;&#35843;&#30340;&#38519;&#38449;&#35774;&#32622;&#20132;&#20114;&#31995;&#32479;&#65288;MORTISE&#65289;&#65292;&#26469;&#35780;&#20272;&#21644;&#25552;&#39640;&#35282;&#33394;&#25198;&#28436;LLMs&#30340;&#24615;&#33021;&#12290;MORTISE&#21487;&#20197;&#36890;&#36807;&#22810;&#20010;&#22522;&#20110;LLM&#30340;&#27169;&#22359;&#30340;&#21327;&#20316;&#21162;&#21147;&#20135;&#29983;&#39640;&#24230;&#19982;&#35282;&#33394;&#30456;&#20851;&#30340;&#31215;&#26497;&#26597;&#35810;&#65292;&#24182;&#36890;&#36807;&#19968;&#33268;&#30340;&#21709;&#24212;&#29983;&#25104;&#22120;&#21046;&#23450;&#30456;&#24212;&#30340;&#22238;&#22797;&#65292;&#20174;&#32780;&#21019;&#24314;&#23545;&#25239;&#24615;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36873;&#25321;&#20102;190&#31181;&#20013;&#25991;&#21644;&#33521;&#25991;&#35282;&#33394;&#26469;&#26500;&#24314;&#31215;&#26497;&#30340;&#26597;&#35810;&#65292;&#20197;&#35780;&#20272;&#29616;&#26377;&#30340;&#35282;&#33394;&#25198;&#28436;LLMs&#12290;&#36890;&#36807;&#20840;&#38754;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#27169;&#22411;&#22312;&#35282;&#33394;&#34920;&#31034;&#19978;&#26222;&#36941;&#23384;&#22312;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10618v1 Announce Type: new  Abstract: The advent of Large Language Models (LLMs) has propelled dialogue generation into new realms, particularly in the field of role-playing systems (RPSs). While enhanced with ordinary role-relevant training dialogues, existing LLM-based RPSs still struggle to align with roles when handling intricate and trapped queries in boundary scenarios. In this paper, we design the Modular ORchestrated Trap-setting Interaction SystEm (MORTISE) to benchmark and improve the role-playing LLMs' performance. MORTISE can produce highly role-relevant aggressive queries through the collaborative effort of multiple LLM-based modules, and formulate corresponding responses to create an adversarial training dataset via a consistent response generator. We select 190 Chinese and English roles to construct aggressive queries to benchmark existing role-playing LLMs. Through comprehensive evaluation, we find that existing models exhibit a general deficiency in role ali
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#21360;&#24230;&#27861;&#24459;&#39046;&#22495;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22788;&#29702;&#31038;&#20250;&#22240;&#32032;&#26102;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#32467;&#21512;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#26032;&#25351;&#26631;$LSS_{\beta}$&#65292;&#24182;&#35780;&#20272;&#20102;&#27169;&#22411;&#22312;&#20108;&#20803;&#27861;&#24459;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20197;&#21450;&#22312;&#21360;&#24230;&#31038;&#20250;&#21508;&#31181;&#19981;&#24179;&#31561;&#26041;&#38754;&#30340;&#20844;&#24179;&#24615;&#23637;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.10567</link><description>&lt;p&gt;
&#22312;InSaAF&#20013;&#34701;&#20837;&#23433;&#20840;&#24615;&#65292;&#36890;&#36807;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615; | LLM&#26159;&#21542;&#24050;&#32463;&#20934;&#22791;&#22909;&#36827;&#20837;&#21360;&#24230;&#27861;&#24459;&#39046;&#22495;&#65311;
&lt;/p&gt;
&lt;p&gt;
InSaAF: Incorporating Safety through Accuracy and Fairness | Are LLMs ready for the Indian Legal Domain?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10567
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#21360;&#24230;&#27861;&#24459;&#39046;&#22495;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22788;&#29702;&#31038;&#20250;&#22240;&#32032;&#26102;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#32467;&#21512;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#26032;&#25351;&#26631;$LSS_{\beta}$&#65292;&#24182;&#35780;&#20272;&#20102;&#27169;&#22411;&#22312;&#20108;&#20803;&#27861;&#24459;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20197;&#21450;&#22312;&#21360;&#24230;&#31038;&#20250;&#21508;&#31181;&#19981;&#24179;&#31561;&#26041;&#38754;&#30340;&#20844;&#24179;&#24615;&#23637;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#25216;&#26415;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#32463;&#23548;&#33268;&#25552;&#20986;&#20102;&#20247;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#25191;&#34892;&#27861;&#24459;&#39046;&#22495;&#30340;&#21508;&#31181;&#20219;&#21153;&#65292;&#20174;&#39044;&#27979;&#21028;&#20915;&#21040;&#29983;&#25104;&#25688;&#35201;&#12290;&#23613;&#31649;&#23427;&#20204;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#24050;&#32463;&#35777;&#26126;&#36825;&#20123;&#27169;&#22411;&#23398;&#20064;&#24182;&#23637;&#31034;&#31038;&#20250;&#20559;&#35265;&#65292;&#24182;&#20570;&#20986;&#19981;&#20844;&#24179;&#30340;&#39044;&#27979;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#24403;&#28041;&#21450;&#31038;&#20250;&#22240;&#32032;&#26102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21360;&#24230;&#27861;&#24459;&#39046;&#22495;&#25191;&#34892;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;$\beta$-&#21152;&#26435;&#30340;$\textit{&#27861;&#24459;&#23433;&#20840;&#20998;&#25968;($LSS_{\beta}$)}$&#65292;&#23558;LLM&#30340;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#20004;&#20010;&#26041;&#38754;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#36890;&#36807;&#32771;&#34385;LLM&#22312;$\textit{&#20108;&#20803;&#27861;&#24459;&#25512;&#29702;}$&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20197;&#21450;&#20854;&#22312;&#21360;&#24230;&#31038;&#20250;&#21508;&#31181;&#19981;&#24179;&#31561;&#26041;&#38754;&#30340;&#20844;&#24179;&#23637;&#31034;&#26469;&#35780;&#20272;LLMs&#30340;&#23433;&#20840;&#24615;&#12290;LLaMA&#21644;LLaMA--2&#27169;&#22411;&#30340;&#20219;&#21153;&#34920;&#29616;&#21644;&#20844;&#24179;&#24471;&#20998;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10567v1 Announce Type: cross  Abstract: Recent advancements in language technology and Artificial Intelligence have resulted in numerous Language Models being proposed to perform various tasks in the legal domain ranging from predicting judgments to generating summaries. Despite their immense potential, these models have been proven to learn and exhibit societal biases and make unfair predictions. In this study, we explore the ability of Large Language Models (LLMs) to perform legal tasks in the Indian landscape when social factors are involved. We present a novel metric, $\beta$-weighted $\textit{Legal Safety Score ($LSS_{\beta}$)}$, which encapsulates both the fairness and accuracy aspects of the LLM. We assess LLMs' safety by considering its performance in the $\textit{Binary Statutory Reasoning}$ task and its fairness exhibition with respect to various axes of disparities in the Indian society. Task performance and fairness scores of LLaMA and LLaMA--2 models indicate th
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#35805;&#24335;SimulMT&#26694;&#26550;&#65292;&#26412;&#25991;&#25552;&#39640;&#20102;&#22522;&#20110;LLM&#30340;SimulMT&#25512;&#29702;&#25928;&#29575;&#65292;&#22312;&#20445;&#25345;&#32763;&#35793;&#36136;&#37327;&#30340;&#21516;&#26102;&#23454;&#29616;&#19982;&#19987;&#38376;&#30340;SimulMT&#27169;&#22411;&#30456;&#36817;&#30340;&#35745;&#31639;&#24310;&#36831;&#12290;</title><link>https://arxiv.org/abs/2402.10552</link><description>&lt;p&gt;
Conversational SimulMT: &#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#21516;&#26102;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Conversational SimulMT: Efficient Simultaneous Translation with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10552
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#35805;&#24335;SimulMT&#26694;&#26550;&#65292;&#26412;&#25991;&#25552;&#39640;&#20102;&#22522;&#20110;LLM&#30340;SimulMT&#25512;&#29702;&#25928;&#29575;&#65292;&#22312;&#20445;&#25345;&#32763;&#35793;&#36136;&#37327;&#30340;&#21516;&#26102;&#23454;&#29616;&#19982;&#19987;&#38376;&#30340;SimulMT&#27169;&#22411;&#30456;&#36817;&#30340;&#35745;&#31639;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#22768;&#26426;&#22120;&#32763;&#35793;&#65288;SimulMT&#65289;&#22312;&#32763;&#35793;&#36136;&#37327;&#21644;&#24310;&#36831;&#20043;&#38388;&#23384;&#22312;&#25361;&#25112;&#24615;&#30340;&#26435;&#34913;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;SimulMT&#20219;&#21153;&#20013;&#21487;&#20197;&#21462;&#24471;&#24456;&#22909;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#36825;&#24448;&#24448;&#26159;&#20197;&#25512;&#29702;&#25104;&#26412;&#21644;&#24310;&#36831;&#30340;&#22686;&#21152;&#20026;&#20195;&#20215;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#35805;&#24335;SimulMT&#26694;&#26550;&#65292;&#36890;&#36807;&#22522;&#20110;&#22810;&#36718;&#23545;&#35805;&#30340;&#35299;&#30721;&#26469;&#25552;&#39640;&#22522;&#20110;LLM&#30340;SimulMT&#30340;&#25512;&#29702;&#25928;&#29575;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;SimulMT&#22522;&#20934;&#19978;&#20351;&#29992;Llama2-7b-chat&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;LLM&#22312;&#32763;&#35793;&#36136;&#37327;&#19978;&#20855;&#26377;&#20248;&#21183;&#65292;&#21516;&#26102;&#23454;&#29616;&#19982;&#19987;&#38376;&#30340;SimulMT&#27169;&#22411;&#30456;&#24403;&#30340;&#35745;&#31639;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10552v1 Announce Type: new  Abstract: Simultaneous machine translation (SimulMT) presents a challenging trade-off between translation quality and latency. Recent studies have shown that LLMs can achieve good performance in SimulMT tasks. However, this often comes at the expense of high inference cost and latency. In this paper, we propose a conversational SimulMT framework to enhance the inference efficiency of LLM-based SimulMT through multi-turn-dialogue-based decoding. Our experiments with Llama2-7b-chat on two SimulMT benchmarks demonstrate the superiority of LLM in translation quality while achieving comparable computational latency to specialized SimulMT models.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25512;&#29702;&#38142;&#26469;&#39044;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#30340;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;R2PE&#65292;&#24182;&#25552;&#20986;&#20102;&#22788;&#29702;&#21487;&#36776;&#35782;&#24615;&#35780;&#20998;&#65288;PDS&#65289;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2402.10528</link><description>&lt;p&gt;
&#25105;&#20204;&#33021;&#21542;&#36880;&#27493;&#39564;&#35777;&#38169;&#35823;&#31572;&#26696;&#26816;&#27979;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can We Verify Step by Step for Incorrect Answer Detection?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10528
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25512;&#29702;&#38142;&#26469;&#39044;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#30340;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;R2PE&#65292;&#24182;&#25552;&#20986;&#20102;&#22788;&#29702;&#21487;&#36776;&#35782;&#24615;&#35780;&#20998;&#65288;PDS&#65289;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Chain-of-Thought&#65288;CoT&#65289;&#25552;&#31034;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24320;&#21457;&#20102;&#21508;&#31181;&#25193;&#23637;&#30340;CoT&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#22686;&#24378;&#26368;&#32456;&#20219;&#21153;&#30340;&#24615;&#33021;&#19978;&#12290;&#27492;&#22806;&#65292;&#24050;&#32463;&#26377;&#30740;&#31350;&#35780;&#20272;&#20102;CoT&#20013;&#25512;&#29702;&#38142;&#30340;&#36136;&#37327;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#38382;&#39064;&#65306;&#36890;&#36807;&#20180;&#32454;&#23457;&#26597;&#23427;&#20204;&#29983;&#25104;&#30340;&#25512;&#29702;&#38142;&#65292;&#26159;&#21542;&#21487;&#20197;&#39044;&#27979;LLMs&#36755;&#20986;&#30340;&#20934;&#30830;&#24615;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#30740;&#31350;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20934;&#65292;R2PE&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#25506;&#31350;&#19981;&#21516;&#39046;&#22495;&#28085;&#30422;&#20116;&#20010;&#19981;&#21516;&#25512;&#29702;&#20219;&#21153;&#20013;&#25512;&#29702;&#38142;&#19982;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#35813;&#22522;&#20934;&#26088;&#22312;&#22522;&#20110;&#25512;&#29702;&#27493;&#39588;&#34913;&#37327;LLMs&#26368;&#32456;&#36755;&#20986;&#30340;&#34394;&#20551;&#24615;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#22810;&#20010;&#25512;&#29702;&#38142;&#20013;&#30340;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25171;&#36133;&#24120;&#35782;&#20998;&#25968;&#65288;PDS&#65289;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10528v1 Announce Type: cross  Abstract: Chain-of-Thought (CoT) prompting has marked a significant advancement in enhancing the reasoning capabilities of large language models (LLMs). Previous studies have developed various extensions of CoT, which focus primarily on enhancing end-task performance. In addition, there has been research on assessing the quality of reasoning chains in CoT. This raises an intriguing question: Is it possible to predict the accuracy of LLM outputs by scrutinizing the reasoning chains they generate? To answer this research question, we introduce a benchmark, R2PE, designed specifically to explore the relationship between reasoning chains and performance in various reasoning tasks spanning five different domains. This benchmark aims to measure the falsehood of the final output of LLMs based on the reasoning steps. To make full use of information in multiple reasoning chains, we propose the process discernibility score (PDS) framework that beats the a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#22810;&#35821;&#35328;&#29983;&#25104;&#20013;&#19981;&#21516;&#24187;&#35273;&#26816;&#27979;&#25351;&#26631;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#30340;&#25351;&#26631;&#22312;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#21477;&#23376;&#32423;&#21035;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#36890;&#24120;&#26080;&#27861;&#26816;&#27979;&#21040;&#21407;&#23376;&#20107;&#23454;&#24187;&#35273;&#12290;</title><link>https://arxiv.org/abs/2402.10496</link><description>&lt;p&gt;
&#27604;&#36739;&#22810;&#35821;&#35328;&#29983;&#25104;&#20013;&#24187;&#35273;&#26816;&#27979;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Comparing Hallucination Detection Metrics for Multilingual Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#22810;&#35821;&#35328;&#29983;&#25104;&#20013;&#19981;&#21516;&#24187;&#35273;&#26816;&#27979;&#25351;&#26631;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#30340;&#25351;&#26631;&#22312;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#21477;&#23376;&#32423;&#21035;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#36890;&#24120;&#26080;&#27861;&#26816;&#27979;&#21040;&#21407;&#23376;&#20107;&#23454;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24050;&#25552;&#20986;&#35768;&#22810;&#38024;&#23545;&#33521;&#25991;&#25991;&#26412;&#30340;&#33258;&#21160;&#24187;&#35273;&#26816;&#27979;&#25216;&#26415;&#65292;&#20294;&#23427;&#20204;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#20013;&#30340;&#25928;&#26524;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#26088;&#22312;&#22635;&#34917;&#23545;&#36825;&#20123;&#24187;&#35273;&#26816;&#27979;&#25351;&#26631;&#22312;&#38750;&#33521;&#35821;&#35821;&#35328;&#19978;&#34920;&#29616;&#22914;&#20309;&#30340;&#35748;&#35782;&#19978;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#21508;&#31181;&#26816;&#27979;&#25351;&#26631;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#35832;&#22914;ROUGE&#21644;&#21629;&#21517;&#23454;&#20307;&#37325;&#21472;&#20197;&#21450;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#30340;&#25351;&#26631;&#65292;&#22312;&#22810;&#31181;&#35821;&#35328;&#30340;&#20256;&#35760;&#25688;&#35201;&#20013;&#26816;&#27979;&#24187;&#35273;&#65307;&#25105;&#20204;&#36824;&#35780;&#20272;&#36825;&#20123;&#19981;&#21516;&#25351;&#26631;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#20197;&#21028;&#26029;&#23427;&#20204;&#26159;&#21542;&#34913;&#37327;&#30456;&#21516;&#30340;&#29616;&#35937;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#20998;&#26512;&#26174;&#31034;&#65292;&#34429;&#28982;&#35789;&#27719;&#25351;&#26631;&#26174;&#31034;&#20986;&#26377;&#38480;&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#22522;&#20110;NLI&#30340;&#25351;&#26631;&#22312;&#39640;&#36164;&#28304;&#35821;&#35328;&#20013;&#22312;&#21477;&#23376;&#32423;&#21035;&#34920;&#29616;&#33391;&#22909;&#12290;&#30456;&#21453;&#65292;NLI-based&#25351;&#26631;&#36890;&#24120;&#26080;&#27861;&#26816;&#27979;&#21040;&#21407;&#23376;&#20107;&#23454;&#24187;&#35273;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#31361;&#26174;&#20102;&#22810;&#35821;&#35328;&#24187;&#35273;&#26816;&#27979;&#20013;&#30340;&#29616;&#26377;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10496v1 Announce Type: cross  Abstract: While many automatic hallucination detection techniques have been proposed for English texts, their effectiveness in multilingual contexts remains unexplored. This paper aims to bridge the gap in understanding how these hallucination detection metrics perform on non-English languages. We evaluate the efficacy of various detection metrics, including lexical metrics like ROUGE and Named Entity Overlap and Natural Language Inference (NLI)-based metrics, at detecting hallucinations in biographical summaries in many languages; we also evaluate how correlated these different metrics are to gauge whether they measure the same phenomena. Our empirical analysis reveals that while lexical metrics show limited effectiveness, NLI-based metrics perform well in high-resource languages at the sentence level. In contrast, NLI-based metrics often fail to detect atomic fact hallucinations. Our findings highlight existing gaps in multilingual hallucinati
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35774;&#35745;&#22870;&#21169;&#24314;&#27169;&#36807;&#31243;&#20013;&#30340;&#25968;&#25454;&#38598;&#20449;&#24687;&#32467;&#26500;&#65292;&#20174;&#22270;&#35770;&#30340;&#35270;&#35282;&#25552;&#20986;&#20102;RLHF&#20013;&#22870;&#21169;&#27867;&#21270;&#30340;&#38382;&#39064;&#65292;&#20197;&#35299;&#20915;&#22810;&#26679;&#30340;&#29615;&#22659;&#12289;&#20302;&#25104;&#26412;&#26631;&#27880;&#21644;&#21487;&#38752;&#30340;&#23545;&#40784;&#24615;&#33021;&#38388;&#30340;&#19981;&#20860;&#23481;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10184</link><description>&lt;p&gt;
&#37325;&#22609;RLHF&#20013;&#30340;&#20449;&#24687;&#32467;&#26500;&#65306;&#22522;&#20110;&#22270;&#35770;&#30340;&#22870;&#21169;&#27867;&#21270;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Rethinking Information Structures in RLHF: Reward Generalization from a Graph Theory Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10184
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35774;&#35745;&#22870;&#21169;&#24314;&#27169;&#36807;&#31243;&#20013;&#30340;&#25968;&#25454;&#38598;&#20449;&#24687;&#32467;&#26500;&#65292;&#20174;&#22270;&#35770;&#30340;&#35270;&#35282;&#25552;&#20986;&#20102;RLHF&#20013;&#22870;&#21169;&#27867;&#21270;&#30340;&#38382;&#39064;&#65292;&#20197;&#35299;&#20915;&#22810;&#26679;&#30340;&#29615;&#22659;&#12289;&#20302;&#25104;&#26412;&#26631;&#27880;&#21644;&#21487;&#38752;&#30340;&#23545;&#40784;&#24615;&#33021;&#38388;&#30340;&#19981;&#20860;&#23481;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#65288;RLHF&#65289;&#23384;&#22312;&#19968;&#20010;&#19977;&#38590;&#38382;&#39064;&#65306;&#39640;&#24230;&#22810;&#26679;&#30340;&#29615;&#22659;&#12289;&#20302;&#26631;&#27880;&#25104;&#26412;&#21644;&#21487;&#38752;&#30340;&#23545;&#40784;&#24615;&#33021;&#20043;&#38388;&#30340;&#19981;&#20860;&#23481;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#35774;&#35745;&#22870;&#21169;&#24314;&#27169;&#36807;&#31243;&#20013;&#30340;&#25968;&#25454;&#38598;&#20449;&#24687;&#32467;&#26500;&#26469;&#32531;&#35299;&#36825;&#31181;&#19981;&#20860;&#23481;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;RLHF&#36807;&#31243;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#23558;&#20854;&#25551;&#32472;&#20026;&#25991;&#26412;&#20998;&#24067;&#19978;&#30340;&#33258;&#21160;&#32534;&#30721;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#24418;&#24335;&#21270;&#20102;RLHF&#30446;&#26631;&#65292;&#21363;&#30830;&#20445;&#20154;&#31867;&#20559;&#22909;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#34892;&#20026;&#20043;&#38388;&#30340;&#20998;&#24067;&#19968;&#33268;&#24615;&#12290;&#22522;&#20110;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;RLHF&#22870;&#21169;&#24314;&#27169;&#38454;&#27573;&#20013;&#20449;&#24687;&#32467;&#26500;&#30340;&#24615;&#33021;&#24433;&#21709;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#29702;&#35299;&#22870;&#21169;&#24314;&#27169;&#38454;&#27573;&#20013;&#30340;&#22870;&#21169;&#27867;&#21270;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#22270;&#35770;&#30340;&#26041;&#27861;&#26469;&#24314;&#27169;&#35821;&#20041;&#31354;&#38388;&#20013;&#30340;&#27867;&#21270;&#12290;&#20854;&#20013;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10184v1 Announce Type: cross  Abstract: There is a trilemma in reinforcement learning from human feedback (RLHF): the incompatibility between highly diverse contexts, low labeling cost, and reliable alignment performance. Here we aim to mitigate such incompatibility through the design of dataset information structures during reward modeling. Specifically, we first reexamine the RLHF process and propose a theoretical framework portraying it as an autoencoding process over text distributions. Our framework formalizes the RLHF objective of ensuring distributional consistency between human preference and large language model (LLM) behavior. Building on this framework, we then systematically investigate the performance impact of information structure in the reward modeling stage of RLHF. To further understand reward generalization in the reward modeling stage, we introduce a new method based on random graph theory that models generalization in the semantic space. A key insight of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27010;&#29575;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#38480;&#21046;&#65292;&#24341;&#20837;&#20102;&#36125;&#21494;&#26031;&#35821;&#35328;&#25512;&#29702;&#25968;&#25454;&#38598;&#65288;BLInD&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#31181;&#35299;&#20915;&#31574;&#30053;&#65292;&#21253;&#25324;Python&#20195;&#30721;&#21644;&#27010;&#29575;&#25512;&#29702;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.09614</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#27010;&#29575;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Reasoning in Generative Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27010;&#29575;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#38480;&#21046;&#65292;&#24341;&#20837;&#20102;&#36125;&#21494;&#26031;&#35821;&#35328;&#25512;&#29702;&#25968;&#25454;&#38598;&#65288;BLInD&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#31181;&#35299;&#20915;&#31574;&#30053;&#65292;&#21253;&#25324;Python&#20195;&#30721;&#21644;&#27010;&#29575;&#25512;&#29702;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22788;&#29702;&#28041;&#21450;&#27010;&#29575;&#20540;&#26126;&#30830;&#37327;&#21270;&#30340;&#25991;&#26412;&#25512;&#29702;&#38382;&#39064;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#36825;&#31181;&#27010;&#29575;&#25512;&#29702;&#23545;&#20110;&#20174;&#26085;&#24120;&#23545;&#35805;&#21040;&#21307;&#23398;&#20915;&#31574;&#31561;&#21508;&#31181;&#24773;&#22659;&#37117;&#24456;&#37325;&#35201;&#12290;&#23613;&#31649;LLMs&#22312;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#26377;&#25152;&#25913;&#36827;&#65292;&#20294;&#22312;&#27010;&#29575;&#25512;&#29702;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#26174;&#33879;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#36125;&#21494;&#26031;&#35821;&#35328;&#25512;&#29702;&#25968;&#25454;&#38598;&#65288;BLInD&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#27979;&#35797;LLMs&#27010;&#29575;&#25512;&#29702;&#33021;&#21147;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#26032;&#25968;&#25454;&#38598;&#26469;&#35814;&#32454;&#35828;&#26126;LLMs&#22312;&#28041;&#21450;&#27010;&#29575;&#25512;&#29702;&#30340;&#20219;&#21153;&#20013;&#30340;&#29305;&#23450;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#31181;&#23558;&#38382;&#39064;&#26144;&#23556;&#21040;&#19981;&#21516;&#24418;&#24335;&#34920;&#31034;&#30340;&#31574;&#30053;&#65292;&#21253;&#25324;Python&#20195;&#30721;&#21644;&#27010;&#29575;&#25512;&#29702;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09614v1 Announce Type: cross  Abstract: This paper considers the challenges that Large Language Models (LLMs) face when reasoning over text that includes information involving uncertainty explicitly quantified via probability values. This type of reasoning is relevant to a variety of contexts ranging from everyday conversations to medical decision-making. Despite improvements in the mathematical reasoning capabilities of LLMs, they still exhibit significant difficulties when it comes to probabilistic reasoning. To deal with this problem, we first introduce the Bayesian Linguistic Inference Dataset (BLInD), a new dataset specifically designed to test the probabilistic reasoning capabilities of LLMs. We then leverage this new dataset to thoroughly illustrate the specific limitations of LLMs for tasks involving probabilistic reasoning and present several strategies that map the problem to different formal representations, including Python code, probabilistic inference algorithm
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PLURAL&#30340;&#39044;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#32437;&#21521;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20013;&#24046;&#24322;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#22312;&#33258;&#28982;&#22270;&#20687;&#21644;&#25991;&#26412;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#20351;&#29992;&#32437;&#21521;&#33016;&#37096;X&#23556;&#32447;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08966</link><description>&lt;p&gt;
&#29992;&#20110;&#32437;&#21521;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20013;&#24046;&#24322;&#35270;&#35273;&#38382;&#31572;&#30340;&#39044;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Pretraining Vision-Language Model for Difference Visual Question Answering in Longitudinal Chest X-rays
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08966
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PLURAL&#30340;&#39044;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#32437;&#21521;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20013;&#24046;&#24322;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#22312;&#33258;&#28982;&#22270;&#20687;&#21644;&#25991;&#26412;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#20351;&#29992;&#32437;&#21521;&#33016;&#37096;X&#23556;&#32447;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#24322;&#35270;&#35273;&#38382;&#31572;(diff-VQA)&#26159;&#19968;&#20010;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#35201;&#27714;&#26681;&#25454;&#19968;&#23545;&#22270;&#20687;&#30340;&#24046;&#24322;&#22238;&#31572;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;&#22312;&#35835;&#21462;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#20013;&#23588;&#20026;&#37325;&#35201;&#65292;&#22240;&#20026;&#25918;&#23556;&#31185;&#21307;&#29983;&#36890;&#24120;&#20250;&#23545;&#21516;&#19968;&#24739;&#32773;&#22312;&#19981;&#21516;&#26102;&#38388;&#25293;&#25668;&#30340;&#22810;&#24133;&#22270;&#20687;&#36827;&#34892;&#27604;&#36739;&#65292;&#20197;&#36861;&#36394;&#30142;&#30149;&#30340;&#36827;&#23637;&#21644;&#20854;&#20020;&#24202;&#23454;&#36341;&#20013;&#20005;&#37325;&#31243;&#24230;&#30340;&#21464;&#21270;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#20026;&#24046;&#24322;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#35774;&#35745;&#29305;&#23450;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#38169;&#36807;&#20102;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(VLM)&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#30340;&#26426;&#20250;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PLURAL&#30340;&#26032;&#22411;VLM&#65292;&#23427;&#22312;&#33258;&#28982;&#22270;&#20687;&#21644;&#32437;&#21521;&#33016;&#37096;X&#23556;&#32447;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#24046;&#24322;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#30340;&#39044;&#35757;&#32451;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#36880;&#27493;&#30340;&#26041;&#27861;&#24320;&#21457;&#65292;&#20174;&#22312;&#33258;&#28982;&#22270;&#20687;&#21644;&#25991;&#26412;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#24320;&#22987;&#65292;&#28982;&#21518;&#20351;&#29992;&#32437;&#21521;&#33016;&#37096;X&#23556;&#32447;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#32437;&#21521;&#25968;&#25454;&#21253;&#25324;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08966v1 Announce Type: cross Abstract: Difference visual question answering (diff-VQA) is a challenging task that requires answering complex questions based on differences between a pair of images. This task is particularly important in reading chest X-ray images because radiologists often compare multiple images of the same patient taken at different times to track disease progression and changes in its severity in their clinical practice. However, previous works focused on designing specific network architectures for the diff-VQA task, missing opportunities to enhance the model's performance using a pretrained vision-language model (VLM). Here, we introduce a novel VLM called PLURAL, which is pretrained on natural and longitudinal chest X-ray data for the diff-VQA task. The model is developed using a step-by-step approach, starting with being pretrained on natural images and texts, followed by being trained using longitudinal chest X-ray data. The longitudinal data consist
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#22810;&#27493;&#20219;&#21153;&#20013;&#38598;&#25104;&#20154;&#31867;&#21453;&#39304;&#21644;&#20559;&#22909;&#23545;&#40784;&#30340;PRompt&#20248;&#21270;&#26041;&#27861;&#12290;&#23427;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#26694;&#26550;&#65292;&#32467;&#21512;&#20154;&#31867;&#21453;&#39304;&#33258;&#21160;&#25552;&#20986;&#20248;&#21270;&#24314;&#35758;&#24182;&#35299;&#20915;&#20102;&#22797;&#26434;&#30340;&#25552;&#31034;&#20869;&#23481;&#20998;&#26512;&#12289;&#21333;&#27493;&#35780;&#20272;&#21644;&#20219;&#21153;&#25191;&#34892;&#20559;&#22909;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.08702</link><description>&lt;p&gt;
PRompt Optimization in Multi-Step Tasks (PROMST): Integrating Human Feedback and Preference Alignment
&lt;/p&gt;
&lt;p&gt;
PRompt Optimization in Multi-Step Tasks (PROMST): Integrating Human Feedback and Preference Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08702
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#22810;&#27493;&#20219;&#21153;&#20013;&#38598;&#25104;&#20154;&#31867;&#21453;&#39304;&#21644;&#20559;&#22909;&#23545;&#40784;&#30340;PRompt&#20248;&#21270;&#26041;&#27861;&#12290;&#23427;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#26694;&#26550;&#65292;&#32467;&#21512;&#20154;&#31867;&#21453;&#39304;&#33258;&#21160;&#25552;&#20986;&#20248;&#21270;&#24314;&#35758;&#24182;&#35299;&#20915;&#20102;&#22797;&#26434;&#30340;&#25552;&#31034;&#20869;&#23481;&#20998;&#26512;&#12289;&#21333;&#27493;&#35780;&#20272;&#21644;&#20219;&#21153;&#25191;&#34892;&#20559;&#22909;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Prompt optimization aims to find the best prompt for a language model (LLM) in multi-step tasks. This paper introduces a genetic algorithm framework that incorporates human feedback to automatically suggest prompt improvements. It addresses challenges such as complex prompt content analysis, evaluation of individual steps, and variations in task execution preferences.
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08702v1 Announce Type: cross Abstract: Prompt optimization aims to find the best prompt to a large language model (LLM) for a given task. LLMs have been successfully used to help find and improve prompt candidates for single-step tasks. However, realistic tasks for agents are multi-step and introduce new challenges: (1) Prompt content is likely to be more extensive and complex, making it more difficult for LLMs to analyze errors, (2) the impact of an individual step is difficult to evaluate, and (3) different people may have varied preferences about task execution. While humans struggle to optimize prompts, they are good at providing feedback about LLM outputs; we therefore introduce a new LLM-driven discrete prompt optimization framework that incorporates human-designed feedback rules about potential errors to automatically offer direct suggestions for improvement. Our framework is stylized as a genetic algorithm in which an LLM generates new candidate prompts from a parent
&lt;/p&gt;</description></item><item><title>&#22312;LLMs&#30340;&#32972;&#26223;&#19979;&#65292;&#26412;&#25991;&#32508;&#21512;&#30740;&#31350;&#20102;&#21508;&#31181;&#35299;&#30721;&#26041;&#27861;&#30340;&#24615;&#33021;&#12289;&#40065;&#26834;&#24615;&#21644;&#35299;&#30721;&#36895;&#24230;&#65292;&#24182;&#21457;&#29616;&#35299;&#30721;&#26041;&#27861;&#30340;&#24615;&#33021;&#19982;&#20219;&#21153;&#30456;&#20851;&#65292;&#21463;&#21040;&#23545;&#40784;&#12289;&#27169;&#22411;&#22823;&#23567;&#21644;&#37327;&#21270;&#31561;&#22240;&#32032;&#24433;&#21709;&#65307;&#26576;&#20123;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#22823;&#37327;&#36229;&#21442;&#25968;&#35843;&#25972;&#36798;&#21040;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#38656;&#35201;&#26435;&#34913;&#21462;&#33293;&#12290;</title><link>https://arxiv.org/abs/2402.06925</link><description>&lt;p&gt;
LLM&#26102;&#20195;&#35299;&#30721;&#26041;&#27861;&#30340;&#32508;&#21512;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Thorough Examination of Decoding Methods in the Era of LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06925
&lt;/p&gt;
&lt;p&gt;
&#22312;LLMs&#30340;&#32972;&#26223;&#19979;&#65292;&#26412;&#25991;&#32508;&#21512;&#30740;&#31350;&#20102;&#21508;&#31181;&#35299;&#30721;&#26041;&#27861;&#30340;&#24615;&#33021;&#12289;&#40065;&#26834;&#24615;&#21644;&#35299;&#30721;&#36895;&#24230;&#65292;&#24182;&#21457;&#29616;&#35299;&#30721;&#26041;&#27861;&#30340;&#24615;&#33021;&#19982;&#20219;&#21153;&#30456;&#20851;&#65292;&#21463;&#21040;&#23545;&#40784;&#12289;&#27169;&#22411;&#22823;&#23567;&#21644;&#37327;&#21270;&#31561;&#22240;&#32032;&#24433;&#21709;&#65307;&#26576;&#20123;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#22823;&#37327;&#36229;&#21442;&#25968;&#35843;&#25972;&#36798;&#21040;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#38656;&#35201;&#26435;&#34913;&#21462;&#33293;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#30721;&#26041;&#27861;&#22312;&#23558;&#35821;&#35328;&#27169;&#22411;&#20174;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#22120;&#36716;&#25442;&#20026;&#23454;&#38469;&#20219;&#21153;&#35299;&#20915;&#22120;&#20013;&#36215;&#30528;&#19981;&#21487;&#25110;&#32570;&#30340;&#20316;&#29992;&#12290;&#20197;&#24448;&#20851;&#20110;&#35299;&#30721;&#26041;&#27861;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#19978;&#65292;&#21487;&#33021;&#19981;&#36866;&#29992;&#20110;&#24403;&#21069;&#36890;&#29992;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#26102;&#20195;&#12290;&#27492;&#22806;&#65292;&#26368;&#36817;&#35299;&#30721;&#31574;&#30053;&#30340;&#28044;&#20837;&#36827;&#19968;&#27493;&#22797;&#26434;&#20102;&#36825;&#20010;&#39046;&#22495;&#12290;&#26412;&#25991;&#22312;LLMs&#30340;&#32972;&#26223;&#19979;&#65292;&#23545;&#21508;&#31181;&#35299;&#30721;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#32780;&#22810;&#26041;&#20301;&#30340;&#20998;&#26512;&#65292;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#21508;&#31181;&#20219;&#21153;&#12289;&#27169;&#22411;&#21644;&#37096;&#32626;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#12289;&#23545;&#36229;&#21442;&#25968;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#20197;&#21450;&#35299;&#30721;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35299;&#30721;&#26041;&#27861;&#30340;&#24615;&#33021;&#26126;&#26174;&#19982;&#20219;&#21153;&#30456;&#20851;&#65292;&#24182;&#21463;&#21040;&#23545;&#40784;&#12289;&#27169;&#22411;&#22823;&#23567;&#21644;&#37327;&#21270;&#31561;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25935;&#24863;&#24615;&#20998;&#26512;&#25581;&#31034;&#20102;&#26576;&#20123;&#26041;&#27861;&#22312;&#38656;&#35201;&#36827;&#34892;&#22823;&#37327;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#21069;&#25552;&#19979;&#33021;&#22815;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#31361;&#20986;&#20102;&#22312;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decoding methods play an indispensable role in converting language models from next-token predictors into practical task solvers. Prior research on decoding methods, primarily focusing on task-specific models, may not extend to the current era of general-purpose large language models (LLMs). Moreover, the recent influx of decoding strategies has further complicated this landscape. This paper provides a comprehensive and multifaceted analysis of various decoding methods within the context of LLMs, evaluating their performance, robustness to hyperparameter changes, and decoding speeds across a wide range of tasks, models, and deployment environments. Our findings reveal that decoding method performance is notably task-dependent and influenced by factors such as alignment, model size, and quantization. Intriguingly, sensitivity analysis exposes that certain methods achieve superior performance at the cost of extensive hyperparameter tuning, highlighting the trade-off between attaining opt
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#22312;&#35757;&#32451;Transformer&#26102;&#65292;&#21024;&#38500;&#20301;&#32622;&#32534;&#30721;&#21644;&#22240;&#26524;&#27880;&#24847;&#21147;&#26426;&#21046;&#21518;&#65292;&#36755;&#20986;&#30340;&#39044;&#27979;&#32467;&#26524;&#23545;&#20110;&#36755;&#20837;&#31526;&#21495;&#25490;&#21015;&#26159;&#19981;&#21464;&#30340;&#12290;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#23545;&#22240;&#26524;&#36830;&#25509;&#26426;&#21046;&#36827;&#34892;&#32454;&#33268;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#27531;&#24046;&#36830;&#25509;&#23545;Transformer&#27169;&#25311;&#36755;&#20837;&#39034;&#24207;&#37325;&#35201;&#24615;&#30340;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2402.05969</link><description>&lt;p&gt;
&#25171;&#30772;&#35757;&#32451;Transformer&#26102;&#30340;&#23545;&#31216;&#24615;
&lt;/p&gt;
&lt;p&gt;
Breaking Symmetry When Training Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05969
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#22312;&#35757;&#32451;Transformer&#26102;&#65292;&#21024;&#38500;&#20301;&#32622;&#32534;&#30721;&#21644;&#22240;&#26524;&#27880;&#24847;&#21147;&#26426;&#21046;&#21518;&#65292;&#36755;&#20986;&#30340;&#39044;&#27979;&#32467;&#26524;&#23545;&#20110;&#36755;&#20837;&#31526;&#21495;&#25490;&#21015;&#26159;&#19981;&#21464;&#30340;&#12290;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#23545;&#22240;&#26524;&#36830;&#25509;&#26426;&#21046;&#36827;&#34892;&#32454;&#33268;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#27531;&#24046;&#36830;&#25509;&#23545;Transformer&#27169;&#25311;&#36755;&#20837;&#39034;&#24207;&#37325;&#35201;&#24615;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#22914;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#23637;&#31034;&#30340;&#37027;&#26679;&#65292;&#27809;&#26377;&#20301;&#32622;&#32534;&#30721;&#21644;&#22240;&#26524;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;Transformer&#26550;&#26500;&#23545;&#20110;&#36755;&#20837;&#31526;&#21495;1, 2, ..., n-1&#30340;&#25490;&#21015;&#26159;&#19981;&#21464;&#30340;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#36825;&#20004;&#31181;&#26426;&#21046;&#37117;&#20250;&#34987;&#20351;&#29992;&#65292;&#20197;&#25171;&#30772;&#23545;&#36755;&#20837;&#31526;&#21495;&#30340;&#23545;&#31216;&#24615;&#12290;&#26368;&#36817;&#24050;&#32463;&#34920;&#26126;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#20301;&#32622;&#32534;&#30721;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;Transformer&#12290;&#36825;&#24517;&#39035;&#36890;&#36807;&#22240;&#26524;&#27880;&#24847;&#26426;&#21046;&#26469;&#23454;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#38416;&#36848;&#20102;&#22240;&#26524;&#36830;&#25509;&#26426;&#21046;&#24517;&#39035;&#26159;&#20351;Transformer&#33021;&#22815;&#27169;&#25311;&#36755;&#20837;&#39034;&#24207;&#37325;&#35201;&#24615;&#30340;&#21407;&#22240;&#12290;Transformer&#30340;&#22402;&#30452;&#8220;&#20999;&#29255;&#8221;&#37117;&#34987;&#40723;&#21169;&#34920;&#31034;&#36755;&#20837;&#24207;&#21015;&#20013;&#30340;&#30456;&#21516;&#20301;&#32622;k&#12290;&#25105;&#20204;&#20551;&#35774;&#27531;&#24046;&#36830;&#25509;&#23545;&#20110;&#36825;&#31181;&#29616;&#35937;&#36215;&#21040;&#20102;&#36129;&#29486;&#65292;&#24182;&#25552;&#20379;&#20102;&#35777;&#25454;&#25903;&#25345;&#36825;&#19968;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
As we show in this paper, the prediction for output token $n+1$ of Transformer architectures without one of the mechanisms of positional encodings and causal attention is invariant to permutations of input tokens $1, 2, ..., n-1$. Usually, both mechanisms are employed and the symmetry with respect to the input tokens is broken. Recently, it has been shown that one can train Transformers without positional encodings. This must be enabled by the causal attention mechanism. In this paper, we elaborate on the argument that the causal connection mechanism must be responsible for the fact that Transformers are able to model input sequences where the order is important. Vertical "slices" of Transformers are all encouraged to represent the same location $k$ in the input sequence. We hypothesize that residual connections contribute to this phenomenon, and demonstrate evidence for this.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedMeZO&#30340;&#26041;&#27861;&#65292;&#20197;&#22312;&#38646;&#38454;&#32852;&#37030;&#23398;&#20064;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#23454;&#29616;&#20869;&#23384;&#39640;&#25928;&#30340;&#35843;&#25972;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;FedMeZO&#19981;&#20165;&#25910;&#25947;&#36895;&#24230;&#24555;&#20110;&#20256;&#32479;&#30340;&#19968;&#38454;&#26041;&#27861;&#65292;&#32780;&#19988;&#22312;&#20010;&#24615;&#21270;&#30340;&#32852;&#37030;&#31574;&#30053;&#20013;&#20855;&#26377;&#20851;&#38190;&#30340;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.05926</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#38646;&#38454;&#32852;&#37030;&#35843;&#25972;&#30340;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Convergence of Zeroth-Order Federated Tuning in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05926
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedMeZO&#30340;&#26041;&#27861;&#65292;&#20197;&#22312;&#38646;&#38454;&#32852;&#37030;&#23398;&#20064;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#23454;&#29616;&#20869;&#23384;&#39640;&#25928;&#30340;&#35843;&#25972;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;FedMeZO&#19981;&#20165;&#25910;&#25947;&#36895;&#24230;&#24555;&#20110;&#20256;&#32479;&#30340;&#19968;&#38454;&#26041;&#27861;&#65292;&#32780;&#19988;&#22312;&#20010;&#24615;&#21270;&#30340;&#32852;&#37030;&#31574;&#30053;&#20013;&#20855;&#26377;&#20851;&#38190;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#34701;&#21512;&#20026;&#38544;&#31169;&#20445;&#25252;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24102;&#26469;&#20102;&#26032;&#26102;&#20195;&#12290;&#28982;&#32780;&#65292;&#31934;&#35843;LLM&#25152;&#38656;&#30340;&#24378;&#22823;&#20869;&#23384;&#35201;&#27714;&#22312;&#37096;&#32626;&#21040;&#36793;&#32536;&#35774;&#22791;&#26102;&#20250;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#36825;&#20123;&#35774;&#22791;&#30340;&#35745;&#31639;&#36164;&#28304;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#32852;&#37030;&#29615;&#22659;&#20013;&#25506;&#32034;&#20102;&#20869;&#23384;&#39640;&#25928;&#30340;&#38646;&#38454;&#20248;&#21270;&#30340;&#20840;&#26032;&#25972;&#21512;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;FedMeZO&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26159;&#31532;&#19968;&#20010;&#22312;LLM&#32972;&#26223;&#19979;&#32771;&#23519;FedMeZO&#30340;&#29702;&#35770;&#22522;&#30784;&#30340;&#30740;&#31350;&#65292;&#28041;&#21450;&#21040;&#22823;&#21442;&#25968;&#31354;&#38388;&#23545;&#20248;&#21270;&#34892;&#20026;&#30340;&#24433;&#21709;&#12289;&#25910;&#25947;&#24615;&#30340;&#24314;&#31435;&#20197;&#21450;&#20026;&#20010;&#24615;&#21270;&#30340;&#32852;&#37030;&#31574;&#30053;&#30830;&#23450;&#20851;&#38190;&#21442;&#25968;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#35777;&#35777;&#25454;&#25903;&#25345;&#20102;&#36825;&#20010;&#29702;&#35770;&#65292;&#34920;&#26126;FedMeZO&#19981;&#20165;&#27604;&#20256;&#32479;&#30340;&#19968;&#38454;&#26041;&#27861;&#65288;&#22914;SGD&#65289;&#25910;&#25947;&#26356;&#24555;&#65292;&#32780;&#19988;&#26126;&#26174;...
&lt;/p&gt;
&lt;p&gt;
The confluence of Federated Learning (FL) and Large Language Models (LLMs) is ushering in a new era in privacy-preserving natural language processing. However, the intensive memory requirements for fine-tuning LLMs pose significant challenges, especially when deploying on edge devices with limited computational resources. To circumvent this, we explore the novel integration of Memory-efficient Zeroth-Order Optimization within a federated setting, a synergy we denote as FedMeZO. Our study is the first to examine the theoretical underpinnings of FedMeZO in the context of LLMs, tackling key questions regarding the influence of large parameter spaces on optimization behavior, the establishment of convergence properties, and the identification of critical parameters for convergence to inform personalized federated strategies. Our extensive empirical evidence supports the theory, showing that FedMeZO not only converges faster than traditional first-order methods such as SGD but also signific
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23454;&#35777;&#30740;&#31350;&#20102;&#37319;&#26679;&#28201;&#24230;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#39064;&#20013;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;0.0&#33267;1.0&#30340;&#28201;&#24230;&#33539;&#22260;&#20869;&#65292;LLM&#24615;&#33021;&#23545;&#35299;&#39064;&#20219;&#21153;&#27809;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.05201</link><description>&lt;p&gt;
&#37319;&#26679;&#28201;&#24230;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#39064;&#20013;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Effect of Sampling Temperature on Problem Solving in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05201
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23454;&#35777;&#30740;&#31350;&#20102;&#37319;&#26679;&#28201;&#24230;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#39064;&#20013;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;0.0&#33267;1.0&#30340;&#28201;&#24230;&#33539;&#22260;&#20869;&#65292;LLM&#24615;&#33021;&#23545;&#35299;&#39064;&#20219;&#21153;&#27809;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#35843;&#26597;&#20102;&#37319;&#26679;&#28201;&#24230;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#35299;&#39064;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#20174;&#26631;&#20934;LLM&#22522;&#20934;&#20013;&#38543;&#26426;&#25277;&#21462;&#38382;&#39064;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65288;MCQA&#65289;&#32771;&#35797;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#22235;&#31181;&#24120;&#35265;&#30340;LLM&#20197;&#21450;&#20116;&#31181;&#25552;&#31034;&#24341;&#25806;&#25216;&#26415;&#26469;&#35299;&#20915;MCQA&#38382;&#39064;&#65292;&#21516;&#26102;&#23558;&#37319;&#26679;&#28201;&#24230;&#20174;0.0&#22686;&#21152;&#21040;1.0&#12290;&#23613;&#31649;&#26377;&#20851;&#30340;&#25253;&#36947;&#19982;&#20043;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;0.0&#33267;1.0&#30340;&#28201;&#24230;&#33539;&#22260;&#20869;&#65292;LLM&#24615;&#33021;&#22312;&#35299;&#39064;&#20219;&#21153;&#20013;&#30340;&#21464;&#21270;&#27809;&#26377;&#32479;&#35745;&#23398;&#19978;&#26174;&#33879;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#32467;&#26524;&#20284;&#20046;&#19981;&#21463;LLM&#12289;&#25552;&#31034;&#24341;&#25806;&#25216;&#26415;&#25110;&#38382;&#39064;&#39046;&#22495;&#30340;&#24433;&#21709;&#12290;&#25152;&#26377;&#20195;&#30721;&#12289;&#25968;&#25454;&#21644;&#34917;&#20805;&#36164;&#26009;&#37117;&#21487;&#20197;&#22312;GitHub&#19978;&#25214;&#21040;&#65306;https://github.com/matthewrenze/jhu-llm-temperature&#12290;
&lt;/p&gt;
&lt;p&gt;
In this research study, we empirically investigate the effect of sampling temperature on the performance of Large Language Models (LLMs) on various problem-solving tasks. We created a multiple-choice question-and-answer (MCQA) exam by randomly sampling problems from standard LLM benchmarks. Then, we used four popular LLMs with five prompt-engineering techniques to solve the MCQA problems while increasing the sampling temperature from 0.0 to 1.0. Despite anecdotal reports to the contrary, our empirical results indicate that changes in temperature in the range 0.0 to 1.0 do not have a statistically significant impact on LLM performance for problem-solving tasks. In addition, these results appear to hold regardless of the LLM, the prompt-engineering technique, or the problem domain. All code, data, and supplemental materials are available on GitHub at: https://github.com/matthewrenze/jhu-llm-temperature.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#31995;&#32479;&#35843;&#26597;&#20102;&#21253;&#21547;&#8220;&#35821;&#35328;&#31867;&#22411;&#22810;&#26679;&#24615;&#8221;&#20027;&#24352;&#30340;NLP&#30740;&#31350;&#65292;&#21457;&#29616;&#19981;&#21516;&#35770;&#25991;&#23545;&#20110;&#36825;&#19968;&#27010;&#24565;&#30340;&#23450;&#20041;&#21644;&#26631;&#20934;&#21508;&#19981;&#30456;&#21516;&#65292;&#24341;&#20837;&#20102;&#22810;&#20010;&#32500;&#24230;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#35780;&#20272;&#35821;&#35328;&#36873;&#25321;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#35821;&#35328;&#36873;&#25321;&#23384;&#22312;&#30340;&#20559;&#21521;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2402.04222</link><description>&lt;p&gt;
NLP&#20013;&#30340;&#8220;&#35821;&#35328;&#31867;&#22411;&#22810;&#26679;&#24615;&#8221;&#26159;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
What is 'Typological Diversity' in NLP?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04222
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#31995;&#32479;&#35843;&#26597;&#20102;&#21253;&#21547;&#8220;&#35821;&#35328;&#31867;&#22411;&#22810;&#26679;&#24615;&#8221;&#20027;&#24352;&#30340;NLP&#30740;&#31350;&#65292;&#21457;&#29616;&#19981;&#21516;&#35770;&#25991;&#23545;&#20110;&#36825;&#19968;&#27010;&#24565;&#30340;&#23450;&#20041;&#21644;&#26631;&#20934;&#21508;&#19981;&#30456;&#21516;&#65292;&#24341;&#20837;&#20102;&#22810;&#20010;&#32500;&#24230;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#35780;&#20272;&#35821;&#35328;&#36873;&#25321;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#35821;&#35328;&#36873;&#25321;&#23384;&#22312;&#30340;&#20559;&#21521;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
NLP&#30740;&#31350;&#30028;&#23545;&#33521;&#35821;&#20197;&#22806;&#30340;&#35821;&#35328;&#25237;&#20837;&#26356;&#22810;&#20851;&#27880;&#65292;&#20174;&#32780;&#22312;&#22810;&#35821;&#35328;NLP&#26041;&#38754;&#21462;&#24471;&#20102;&#21487;&#35266;&#30340;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25913;&#36827;&#21482;&#36866;&#29992;&#20110;&#19990;&#30028;&#35821;&#35328;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;&#20026;&#20102;&#25193;&#23637;&#36825;&#19968;&#33539;&#22260;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#35770;&#25991;&#33268;&#21147;&#20110;&#25552;&#39640;&#36328;&#35821;&#35328;&#30340;&#36890;&#29992;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#35821;&#35328;&#31867;&#22411;&#23398;&#24120;&#34987;&#29992;&#26469;&#36873;&#25321;&#35821;&#35328;&#65292;&#22522;&#20110;&#24191;&#27867;&#30340;&#35821;&#35328;&#31867;&#22411;&#26679;&#26412;&#24212;&#33021;&#24102;&#26469;&#23545;&#22810;&#31181;&#35821;&#35328;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#20123;&#36873;&#25321;&#36890;&#24120;&#34987;&#25551;&#36848;&#20026;&#8220;&#35821;&#35328;&#31867;&#22411;&#22810;&#26679;&#24615;&#8221;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#35843;&#26597;&#20102;&#21253;&#21547;&#8220;&#35821;&#35328;&#31867;&#22411;&#22810;&#26679;&#24615;&#8221;&#20027;&#24352;&#30340;NLP&#30740;&#31350;&#65292;&#21457;&#29616;&#27809;&#26377;&#30830;&#20999;&#30340;&#23450;&#20041;&#25110;&#26631;&#20934;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20960;&#20010;&#32500;&#24230;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#36817;&#20284;&#35821;&#35328;&#36873;&#25321;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#21457;&#29616;&#32467;&#26524;&#22312;&#19981;&#21516;&#35770;&#25991;&#20013;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35821;&#35328;&#36873;&#25321;&#30340;&#20559;&#21521;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
The NLP research community has devoted increased attention to languages beyond English, resulting in considerable improvements for multilingual NLP. However, these improvements only apply to a small subset of the world's languages. Aiming to extend this, an increasing number of papers aspires to enhance generalizable multilingual performance across languages. To this end, linguistic typology is commonly used to motivate language selection, on the basis that a broad typological sample ought to imply generalization across a broad range of languages. These selections are often described as being 'typologically diverse'. In this work, we systematically investigate NLP research that includes claims regarding 'typological diversity'. We find there are no set definitions or criteria for such claims. We introduce metrics to approximate the diversity of language selection along several axes and find that the results vary considerably across papers. Furthermore, we show that skewed language sele
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25968;&#23398;&#26041;&#24335;&#30740;&#31350;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26550;&#26500;&#65292;&#27604;&#36739;&#20102;&#20107;&#21518;&#35299;&#37322;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#35299;&#37322;&#30340;&#24046;&#24322;&#65292;&#21457;&#29616;&#23613;&#31649;&#26377;&#23616;&#38480;&#24615;&#65292;&#20107;&#21518;&#35299;&#37322;&#26041;&#27861;&#33021;&#22815;&#25429;&#33719;&#27604;&#20165;&#20165;&#26816;&#26597;&#27880;&#24847;&#21147;&#26435;&#37325;&#26356;&#26377;&#29992;&#30340;&#27934;&#23519;&#12290;</title><link>https://arxiv.org/abs/2402.03485</link><description>&lt;p&gt;
&#27880;&#24847;&#21147;&#19982;&#20107;&#21518;&#21487;&#35299;&#37322;&#24615;&#30456;&#36935;&#65306;&#25968;&#23398;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Attention Meets Post-hoc Interpretability: A Mathematical Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03485
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25968;&#23398;&#26041;&#24335;&#30740;&#31350;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26550;&#26500;&#65292;&#27604;&#36739;&#20102;&#20107;&#21518;&#35299;&#37322;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#35299;&#37322;&#30340;&#24046;&#24322;&#65292;&#21457;&#29616;&#23613;&#31649;&#26377;&#23616;&#38480;&#24615;&#65292;&#20107;&#21518;&#35299;&#37322;&#26041;&#27861;&#33021;&#22815;&#25429;&#33719;&#27604;&#20165;&#20165;&#26816;&#26597;&#27880;&#24847;&#21147;&#26435;&#37325;&#26356;&#26377;&#29992;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#26426;&#21046;&#22522;&#20110;transformer&#31561;&#26550;&#26500;&#65292;&#25104;&#20026;&#20102;&#25216;&#26415;&#38761;&#21629;&#30340;&#26680;&#24515;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#38500;&#20102;&#24110;&#21161;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#20043;&#22806;&#65292;&#27880;&#24847;&#21147;&#26426;&#21046;&#26412;&#36523;&#36824;&#25552;&#20379;&#20102;&#20851;&#20110;&#27169;&#22411;&#20869;&#37096;&#34892;&#20026;&#30340;&#26377;&#24847;&#20041;&#27934;&#23519;&#12290;&#36825;&#20123;&#27934;&#23519;&#26159;&#21542;&#21487;&#20197;&#29992;&#20316;&#35299;&#37322;&#65311;&#20851;&#20110;&#27492;&#20105;&#35770;&#19981;&#26029;&#12290;&#26412;&#25991;&#36890;&#36807;&#25968;&#23398;&#26041;&#24335;&#30740;&#31350;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26550;&#26500;&#65292;&#24182;&#20934;&#30830;&#23450;&#20301;&#20102;&#20107;&#21518;&#35299;&#37322;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#35299;&#37322;&#20043;&#38388;&#30340;&#21306;&#21035;&#12290;&#25105;&#20204;&#34920;&#26126;&#23427;&#20204;&#25552;&#20379;&#20102;&#30456;&#24403;&#19981;&#21516;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#23613;&#31649;&#26377;&#20854;&#23616;&#38480;&#24615;&#65292;&#20107;&#21518;&#35299;&#37322;&#26041;&#27861;&#33021;&#22815;&#25429;&#33719;&#27604;&#20165;&#20165;&#26816;&#26597;&#27880;&#24847;&#21147;&#26435;&#37325;&#26356;&#26377;&#29992;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention-based architectures, in particular transformers, are at the heart of a technological revolution. Interestingly, in addition to helping obtain state-of-the-art results on a wide range of applications, the attention mechanism intrinsically provides meaningful insights on the internal behavior of the model. Can these insights be used as explanations? Debate rages on. In this paper, we mathematically study a simple attention-based architecture and pinpoint the differences between post-hoc and attention-based explanations. We show that they provide quite different results, and that, despite their limitations, post-hoc methods are capable of capturing more useful insights than merely examining the attention weights.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36873;&#25321;&#23545;&#35937;&#26102;&#30340;&#20559;&#35265;&#65292;&#21457;&#29616;&#20559;&#35265;&#32467;&#26500;&#20381;&#36182;&#20110;&#27169;&#22411;&#65292;&#23545;&#35937;&#31867;&#22411;&#35843;&#33410;&#20102;&#20559;&#35265;&#30340;&#24433;&#21709;&#31243;&#24230;&#65292;&#23548;&#33268;&#21015;&#34920;&#20013;&#30340;&#31532;&#19968;&#20010;&#23545;&#35937;&#22312;&#36755;&#20986;&#20013;&#34987;&#36807;&#24230;&#21576;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.01740</link><description>&lt;p&gt;
&#22312;&#35748;&#30693;&#36127;&#33655;&#19979;&#30340;&#34917;&#20607;&#24615;&#20559;&#35265;&#65306;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36873;&#25321;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Compensatory Biases Under Cognitive Load: Reducing Selection Bias in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01740
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36873;&#25321;&#23545;&#35937;&#26102;&#30340;&#20559;&#35265;&#65292;&#21457;&#29616;&#20559;&#35265;&#32467;&#26500;&#20381;&#36182;&#20110;&#27169;&#22411;&#65292;&#23545;&#35937;&#31867;&#22411;&#35843;&#33410;&#20102;&#20559;&#35265;&#30340;&#24433;&#21709;&#31243;&#24230;&#65292;&#23548;&#33268;&#21015;&#34920;&#20013;&#30340;&#31532;&#19968;&#20010;&#23545;&#35937;&#22312;&#36755;&#20986;&#20013;&#34987;&#36807;&#24230;&#21576;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;gpt-3.5-turbo&#21644;claude-instant-1.2&#22312;&#35299;&#37322;&#21644;&#25191;&#34892;&#35821;&#20041;&#20219;&#21153;&#26041;&#38754;&#21464;&#24471;&#38750;&#24120;&#37325;&#35201;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#22266;&#26377;&#30340;&#20559;&#35265;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#35748;&#30693;&#20559;&#35265;&#65292;&#20250;&#23545;&#23427;&#20204;&#30340;&#24615;&#33021;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#20854;&#20013;&#19968;&#20010;&#21463;&#21040;&#24433;&#21709;&#26368;&#22823;&#30340;&#26159;&#20174;&#21015;&#34920;&#20013;&#36827;&#34892;&#23545;&#35937;&#36873;&#25321;&#65292;&#36825;&#26159;&#25968;&#23383;&#23548;&#33322;&#21644;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#22522;&#26412;&#25805;&#20316;&#12290;&#26412;&#30740;&#31350;&#37325;&#28857;&#26816;&#26597;&#36825;&#20123;&#20559;&#35265;&#65292;&#24182;&#37327;&#21270;&#20854;&#23545;&#20195;&#34920;&#24615;&#21015;&#34920;&#36873;&#25321;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#36827;&#34892;&#19968;&#31995;&#21015;&#25511;&#21046;&#23454;&#39564;&#65292;&#25105;&#20204;&#25805;&#32437;&#20102;&#28201;&#24230;&#12289;&#21015;&#34920;&#38271;&#24230;&#12289;&#23545;&#35937;&#36523;&#20221;&#12289;&#23545;&#35937;&#31867;&#22411;&#12289;&#25552;&#31034;&#22797;&#26434;&#24230;&#21644;&#27169;&#22411;&#65292;&#20197;&#25506;&#32034;&#36825;&#20123;&#20559;&#35265;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#23396;&#31435;&#21644;&#27979;&#37327;&#36825;&#20123;&#20559;&#35265;&#23545;&#36873;&#25321;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20559;&#35265;&#32467;&#26500;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#27169;&#22411;&#65292;&#32780;&#23545;&#35937;&#31867;&#22411;&#35843;&#33410;&#20102;&#20559;&#35265;&#24433;&#21709;&#30340;&#31243;&#24230;&#12290;&#30001;&#20110;&#23384;&#22312;&#36739;&#24378;&#30340;&#21021;&#29616;&#25928;&#24212;&#65292;&#21015;&#34920;&#20013;&#30340;&#31532;&#19968;&#20010;&#23545;&#35937;&#20250;&#22312;&#36755;&#20986;&#20013;&#34987;&#36807;&#24230;&#21576;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) like gpt-3.5-turbo and claude-instant-1.2 have become instrumental in interpreting and executing semantic-based tasks. Unfortunately, these models' inherent biases, akin to human cognitive biases, adversely affect their performance. Particularly affected is object selection from lists; a fundamental operation in digital navigation and decision-making. This research critically examines these biases and quantifies the effects on a representative list selection task. To explore these biases, we conducted a series of controlled experiments, manipulating temperature, list length, object identity, object type, prompt complexity, and model. This enabled us to isolate and measure the influence of the biases on selection behavior. Our findings show that bias structure is strongly dependent on the model, with object type modulating the magnitude of the effect. With a strong primacy effect, causing the first objects in a list to be disproprotionately represented in ou
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#31639;&#26415;&#38382;&#39064;&#26102;&#19982;&#20154;&#31867;&#23398;&#20064;&#32773;&#30340;&#35748;&#30693;&#20559;&#35265;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#29702;&#35299;&#21644;&#35299;&#20915;&#26041;&#26696;&#35268;&#21010;&#38454;&#27573;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2401.18070</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#38382;&#39064;&#26102;&#26159;&#21542;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#23398;&#20064;&#32773;&#30456;&#21516;&#30340;&#35748;&#30693;&#20559;&#35265;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.18070
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#31639;&#26415;&#38382;&#39064;&#26102;&#19982;&#20154;&#31867;&#23398;&#20064;&#32773;&#30340;&#35748;&#30693;&#20559;&#35265;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#29702;&#35299;&#21644;&#35299;&#20915;&#26041;&#26696;&#35268;&#21010;&#38454;&#27573;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#23545;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#35748;&#30693;&#27169;&#22411;&#24863;&#20852;&#36259;&#12290;&#20026;&#20102;&#36798;&#21040;&#36825;&#20010;&#30446;&#30340;&#65292;&#20102;&#35299;LLMs&#33021;&#22815;&#27169;&#25311;&#21738;&#20123;&#35748;&#30693;&#29305;&#24615;&#20197;&#21450;&#21738;&#20123;&#19981;&#33021;&#27169;&#25311;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#22312;&#35299;&#20915;&#31639;&#26415;&#38382;&#39064;&#26102;&#19982;&#20799;&#31461;&#24050;&#30693;&#35748;&#30693;&#20559;&#35265;&#30340;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#35843;&#26597;&#23398;&#20064;&#31185;&#23398;&#25991;&#29486;&#65292;&#25105;&#20204;&#25552;&#20986;&#38382;&#39064;&#35299;&#20915;&#36807;&#31243;&#21487;&#20197;&#20998;&#20026;&#19977;&#20010;&#26126;&#30830;&#30340;&#27493;&#39588;&#65306;&#25991;&#26412;&#29702;&#35299;&#12289;&#35299;&#20915;&#26041;&#26696;&#35268;&#21010;&#21644;&#35299;&#20915;&#26041;&#26696;&#25191;&#34892;&#12290;&#25105;&#20204;&#20026;&#27599;&#20010;&#27493;&#39588;&#26500;&#24314;&#20102;&#27979;&#35797;&#65292;&#20197;&#20102;&#35299;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;LLMs&#21487;&#20197;&#22914;&#20309;&#24544;&#23454;&#22320;&#27169;&#25311;&#36825;&#20010;&#36807;&#31243;&#30340;&#21738;&#20123;&#37096;&#20998;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#20026;&#27599;&#20010;&#27979;&#35797;&#29983;&#25104;&#20102;&#19968;&#32452;&#26032;&#30340;&#21333;&#35789;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23545;&#38382;&#39064;&#29305;&#24449;&#36827;&#34892;&#31934;&#32454;&#25511;&#21046;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;LLMs&#22312;&#25991;&#26412;&#29702;&#35299;&#21644;&#35299;&#20915;&#26041;&#26696;&#35268;&#21010;&#20004;&#20010;&#35299;&#20915;&#36807;&#31243;&#30340;&#27493;&#39588;&#20013;&#65292;&#19981;&#35770;&#26159;&#21542;&#32463;&#36807;&#25351;&#23548;&#35843;&#25972;&#65292;&#37117;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is increasing interest in employing large language models (LLMs) as cognitive models. For such purposes, it is central to understand which cognitive properties are well-modeled by LLMs, and which are not. In this work, we study the biases of LLMs in relation to those known in children when solving arithmetic word problems. Surveying the learning science literature, we posit that the problem-solving process can be split into three distinct steps: text comprehension, solution planning and solution execution. We construct tests for each one in order to understand which parts of this process can be faithfully modeled by current state-of-the-art LLMs. We generate a novel set of word problems for each of these tests, using a neuro-symbolic method that enables fine-grained control over the problem features. We find evidence that LLMs, with and without instruction-tuning, exhibit human-like biases in both the text-comprehension and the solution-planning steps of the solving process, but 
&lt;/p&gt;</description></item><item><title>MAC-SQL&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#26694;&#26550;&#65292;&#38024;&#23545;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#20013;&#30340;&#24040;&#22823;&#25968;&#25454;&#24211;&#21644;&#22797;&#26434;&#29992;&#25143;&#38382;&#39064;&#65292;&#36890;&#36807;&#26680;&#24515;&#20998;&#35299;&#22120;&#26234;&#33021;&#20307;&#21644;&#36741;&#21161;&#26234;&#33021;&#20307;&#30340;&#21327;&#20316;&#65292;&#21033;&#29992;&#22806;&#37096;&#24037;&#20855;&#21644;&#27169;&#22411;&#36827;&#34892;&#35299;&#26512;&#21644;&#20462;&#27491;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25991;&#26412;&#21040;SQL&#29983;&#25104;&#21644;&#26597;&#35810;&#35299;&#26512;&#12290;</title><link>https://arxiv.org/abs/2312.11242</link><description>&lt;p&gt;
MAC-SQL: &#19968;&#31181;&#29992;&#20110;&#25991;&#26412;&#21040;SQL&#30340;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
MAC-SQL: A Multi-Agent Collaborative Framework for Text-to-SQL
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11242
&lt;/p&gt;
&lt;p&gt;
MAC-SQL&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#26694;&#26550;&#65292;&#38024;&#23545;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#20013;&#30340;&#24040;&#22823;&#25968;&#25454;&#24211;&#21644;&#22797;&#26434;&#29992;&#25143;&#38382;&#39064;&#65292;&#36890;&#36807;&#26680;&#24515;&#20998;&#35299;&#22120;&#26234;&#33021;&#20307;&#21644;&#36741;&#21161;&#26234;&#33021;&#20307;&#30340;&#21327;&#20316;&#65292;&#21033;&#29992;&#22806;&#37096;&#24037;&#20855;&#21644;&#27169;&#22411;&#36827;&#34892;&#35299;&#26512;&#21644;&#20462;&#27491;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25991;&#26412;&#21040;SQL&#29983;&#25104;&#21644;&#26597;&#35810;&#35299;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22522;&#20110;LLM&#30340;&#25991;&#26412;&#21040;SQL&#26041;&#27861;&#36890;&#24120;&#22312;&#8220;&#24040;&#22823;&#8221;&#30340;&#25968;&#25454;&#24211;&#21644;&#38656;&#35201;&#22810;&#27493;&#25512;&#29702;&#30340;&#22797;&#26434;&#29992;&#25143;&#38382;&#39064;&#19978;&#36973;&#21463;&#20005;&#37325;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#24573;&#35270;&#20102;&#21033;&#29992;&#22806;&#37096;&#24037;&#20855;&#21644;&#27169;&#22411;&#21327;&#20316;&#30340;LLM&#30340;&#37325;&#35201;&#24847;&#20041;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MAC-SQL&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;LLM&#30340;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#19968;&#20010;&#26680;&#24515;&#20998;&#35299;&#22120;&#26234;&#33021;&#20307;&#65292;&#29992;&#20110;&#36827;&#34892;&#24102;&#26377;&#23569;&#26679;&#26412;&#24605;&#32500;&#38142;&#30340;&#25991;&#26412;&#21040;SQL&#29983;&#25104;&#65292;&#21516;&#26102;&#36824;&#26377;&#20004;&#20010;&#36741;&#21161;&#26234;&#33021;&#20307;&#65292;&#21033;&#29992;&#22806;&#37096;&#24037;&#20855;&#25110;&#27169;&#22411;&#33719;&#21462;&#36739;&#23567;&#30340;&#23376;&#25968;&#25454;&#24211;&#24182;&#20462;&#27491;&#38169;&#35823;&#30340;SQL&#26597;&#35810;&#12290;&#20998;&#35299;&#22120;&#26234;&#33021;&#20307;&#19982;&#36741;&#21161;&#26234;&#33021;&#20307;&#21512;&#20316;&#65292;&#26681;&#25454;&#38656;&#35201;&#28608;&#27963;&#65292;&#24182;&#21487;&#20197;&#25193;&#23637;&#20197;&#36866;&#24212;&#26032;&#30340;&#29305;&#24615;&#25110;&#24037;&#20855;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#25991;&#26412;&#21040;SQL&#35299;&#26512;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#26368;&#21021;&#21033;&#29992;GPT-4&#20316;&#20026;&#24378;&#22823;&#30340;LLM&#39592;&#24178;&#26469;&#23436;&#25104;&#25152;&#26377;&#26234;&#33021;&#20307;&#20219;&#21153;&#65292;&#20197;&#30830;&#23450;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11242v3 Announce Type: replace  Abstract: Recent LLM-based Text-to-SQL methods usually suffer from significant performance degradation on ``huge" databases and complex user questions that require multi-step reasoning. Moreover, most existing methods neglect the crucial significance of LLMs utilizing external tools and model collaboration. To address these challenges, we introduce MAC-SQL, a novel LLM-based multi-agent collaborative framework. Our framework comprises a core decomposer agent for Text-to-SQL generation with few-shot chain-of-thought reasoning, accompanied by two auxiliary agents that utilize external tools or models to acquire smaller sub-databases and refine erroneous SQL queries. The decomposer agent collaborates with auxiliary agents, which are activated as needed and can be expanded to accommodate new features or tools for effective Text-to-SQL parsing. In our framework, We initially leverage GPT-4 as the strong backbone LLM for all agent tasks to determine
&lt;/p&gt;</description></item><item><title>&#22312;&#26631;&#20934;&#25910;&#38598;&#30340;&#21021;&#27493;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#38271;&#25991;&#26723;&#27169;&#22411;&#22312;MRR&#25110;NDCG&#26041;&#38754;&#24615;&#33021;&#19981;&#20339;&#65292;&#34920;&#29616;&#20302;&#20110;FirstP&#65292;&#25110;&#24179;&#22343;&#26368;&#22810;&#36229;&#36234;5&#65285;&#12290;&#25105;&#20204;&#25512;&#27979;&#36825;&#19981;&#26159;&#22240;&#20026;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#65292;&#32780;&#26159;&#30001;&#20110;&#30456;&#20851;&#27573;&#33853;&#20855;&#26377;&#20301;&#32622;&#20559;&#35265;&#65292;&#24448;&#24448;&#20301;&#20110;&#21069;512&#20010;&#25991;&#26723;&#26631;&#35760;&#20043;&#20013;&#12290;&#25105;&#20204;&#25214;&#21040;&#35777;&#25454;&#34920;&#26126;&#36825;&#31181;&#20559;&#35265;&#33267;&#23569;&#23384;&#22312;&#20110;&#20004;&#20010;&#27979;&#35797;&#38598;&#20013;&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#25910;&#38598;MS MARCO FarRelevant&#65292;&#20854;&#20013;&#21253;&#21547;</title><link>https://arxiv.org/abs/2207.01262</link><description>&lt;p&gt;
&#36890;&#36807;&#20840;&#38754;&#35780;&#20272;&#21644;Leaderboarding&#29702;&#35299;&#38271;&#25991;&#26723;&#25490;&#21517;&#27169;&#22411;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Understanding Performance of Long-Document Ranking Models through Comprehensive Evaluation and Leaderboarding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2207.01262
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26631;&#20934;&#25910;&#38598;&#30340;&#21021;&#27493;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#38271;&#25991;&#26723;&#27169;&#22411;&#22312;MRR&#25110;NDCG&#26041;&#38754;&#24615;&#33021;&#19981;&#20339;&#65292;&#34920;&#29616;&#20302;&#20110;FirstP&#65292;&#25110;&#24179;&#22343;&#26368;&#22810;&#36229;&#36234;5&#65285;&#12290;&#25105;&#20204;&#25512;&#27979;&#36825;&#19981;&#26159;&#22240;&#20026;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#65292;&#32780;&#26159;&#30001;&#20110;&#30456;&#20851;&#27573;&#33853;&#20855;&#26377;&#20301;&#32622;&#20559;&#35265;&#65292;&#24448;&#24448;&#20301;&#20110;&#21069;512&#20010;&#25991;&#26723;&#26631;&#35760;&#20043;&#20013;&#12290;&#25105;&#20204;&#25214;&#21040;&#35777;&#25454;&#34920;&#26126;&#36825;&#31181;&#20559;&#35265;&#33267;&#23569;&#23384;&#22312;&#20110;&#20004;&#20010;&#27979;&#35797;&#38598;&#20013;&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#25910;&#38598;MS MARCO FarRelevant&#65292;&#20854;&#20013;&#21253;&#21547;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35780;&#20272;&#20102;20&#22810;&#20010;&#29992;&#20110;&#38271;&#25991;&#26723;&#25490;&#21517;&#30340;Transformer&#27169;&#22411;&#65288;&#21253;&#25324;&#26368;&#36817;&#20351;&#29992;FlashAttention&#35757;&#32451;&#30340;LongP&#27169;&#22411;&#65289;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#31616;&#21333;&#30340;FirstP&#22522;&#32447;&#36827;&#34892;&#20102;&#27604;&#36739;&#65288;&#23558;&#30456;&#21516;&#27169;&#22411;&#24212;&#29992;&#20110;&#36755;&#20837;&#25130;&#26029;&#20026;&#21069;512&#20010;&#26631;&#35760;&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;MS MARCO&#25991;&#26723;v1&#20316;&#20026;&#20027;&#35201;&#35757;&#32451;&#38598;&#65292;&#24182;&#22312;&#38646;-shot&#22330;&#26223;&#19979;&#35780;&#20272;&#20102;&#27169;&#22411;&#65292;&#20197;&#21450;&#22312;&#23545;&#20854;&#20182;&#25910;&#38598;&#36827;&#34892;&#24494;&#35843;&#21518;&#35780;&#20272;&#20102;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2207.01262v2 Announce Type: replace-cross  Abstract: We evaluated 20+ Transformer models for ranking of long documents (including recent LongP models trained with FlashAttention) and compared them with simple FirstP baselines (applying the same model to input truncated to the first 512 tokens). We used MS MARCO Documents v1 as a primary training set and evaluated models in the zero-shot scenario as well as after fine-tuning on other collections.   In our initial experiments with standard collections we found that long-document models underperformed FirstP or outperformed it by at most 5% on average in terms of MRR or NDCG. We then conjectured that this was not due to models inability to process long context but rather due to a positional bias of relevant passages, which tended to be among the first 512 document tokens. We found evidence that this bias was, indeed, present in at least two test sets, which motivated us to create a new collection MS MARCO FarRelevant where the relev
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;OWSM v3.1&#22522;&#20110;E-Branchformer&#30340;&#26356;&#22909;&#21644;&#26356;&#24555;&#30340;&#24320;&#25918;&#24335;Whisper&#39118;&#26684;&#35821;&#38899;&#27169;&#22411;&#12290;&#36825;&#20010;&#27169;&#22411;&#36890;&#36807;&#25552;&#39640;&#24615;&#33021;&#21644;&#25928;&#29575;&#65292;&#36229;&#36234;&#20102;&#20043;&#21069;&#30340;&#29256;&#26412;&#65292;&#24182;&#23454;&#29616;&#20102;&#26356;&#24555;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;&#35813;&#35770;&#25991;&#36824;&#20844;&#24320;&#21457;&#24067;&#20102;&#30456;&#20851;&#30340;&#25968;&#25454;&#21644;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.16658</link><description>&lt;p&gt;
OWSM v3.1: &#22522;&#20110;E-Branchformer&#30340;&#26356;&#22909;&#21644;&#26356;&#24555;&#30340;&#24320;&#25918;&#24335;Whisper&#39118;&#26684;&#35821;&#38899;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
OWSM v3.1: Better and Faster Open Whisper-Style Speech Models based on E-Branchformer. (arXiv:2401.16658v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16658
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;OWSM v3.1&#22522;&#20110;E-Branchformer&#30340;&#26356;&#22909;&#21644;&#26356;&#24555;&#30340;&#24320;&#25918;&#24335;Whisper&#39118;&#26684;&#35821;&#38899;&#27169;&#22411;&#12290;&#36825;&#20010;&#27169;&#22411;&#36890;&#36807;&#25552;&#39640;&#24615;&#33021;&#21644;&#25928;&#29575;&#65292;&#36229;&#36234;&#20102;&#20043;&#21069;&#30340;&#29256;&#26412;&#65292;&#24182;&#23454;&#29616;&#20102;&#26356;&#24555;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;&#35813;&#35770;&#25991;&#36824;&#20844;&#24320;&#21457;&#24067;&#20102;&#30456;&#20851;&#30340;&#25968;&#25454;&#21644;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#20513;&#23548;&#37319;&#29992;&#23436;&#20840;&#24320;&#25918;&#30340;&#22522;&#30784;&#27169;&#22411;&#26469;&#25512;&#21160;&#36879;&#26126;&#24230;&#21644;&#24320;&#25918;&#31185;&#23398;&#12290;&#20316;&#20026;&#19968;&#20010;&#21021;&#27493;&#30340;&#27493;&#39588;&#65292;&#24320;&#25918;&#24335;Whisper&#39118;&#26684;&#35821;&#38899;&#27169;&#22411;(OWSM)&#20351;&#29992;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#21644;&#24320;&#28304;&#24037;&#20855;&#37325;&#26032;&#22797;&#21046;&#20102;OpenAI&#30340;Whisper&#12290;&#20026;&#20102;&#22797;&#21046;Whisper&#65292;&#20043;&#21069;&#30340;OWSM v1&#21040;v3&#27169;&#22411;&#20173;&#28982;&#22522;&#20110;Transformer&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#19981;&#22914;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#35821;&#38899;&#32534;&#30721;&#22120;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#39640;OWSM&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;E-Branchformer&#30340;OWSM v3.1&#27169;&#22411;&#65292;&#26377;&#20004;&#20010;&#35268;&#27169;&#65292;&#21363;100M&#21644;1B&#12290;1B&#27169;&#22411;&#26159;&#30446;&#21069;&#20844;&#24320;&#21487;&#29992;&#30340;&#26368;&#22823;&#30340;&#22522;&#20110;E-Branchformer&#30340;&#35821;&#38899;&#27169;&#22411;&#12290;&#23427;&#22312;&#22823;&#37096;&#20998;&#35780;&#20272;&#22522;&#20934;&#19978;&#34920;&#29616;&#20986;&#27604;&#20043;&#21069;&#30340;OWSM v3&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#28436;&#31034;&#20102;&#39640;&#36798;25%&#30340;&#26356;&#24555;&#25512;&#29702;&#36895;&#24230;&#12290;&#25105;&#20204;&#20844;&#24320;&#21457;&#24067;&#25968;&#25454;&#20934;&#22791;&#33050;&#26412;&#12289;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#35757;&#32451;&#26085;&#24535;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have advocated for fully open foundation models to promote transparency and open science. As an initial step, the Open Whisper-style Speech Model (OWSM) reproduced OpenAI's Whisper using publicly available data and open-source toolkits. With the aim of reproducing Whisper, the previous OWSM v1 through v3 models were still based on Transformer, which might lead to inferior performance compared to other state-of-the-art speech encoders. In this work, we aim to improve the performance and efficiency of OWSM without extra training data. We present E-Branchformer based OWSM v3.1 models at two scales, i.e., 100M and 1B. The 1B model is the largest E-Branchformer based speech model that has been made publicly available. It outperforms the previous OWSM v3 in a vast majority of evaluation benchmarks, while demonstrating up to 25% faster inference speed. We publicly release the data preparation scripts, pre-trained models and training logs.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;&#65292;&#32966;&#23618;&#20301;&#32622;&#32534;&#30721;&#65288;BiPE&#65289;&#65292;&#36890;&#36807;&#23558;&#20998;&#27573;&#20869;&#32534;&#30721;&#21644;&#20998;&#27573;&#38388;&#32534;&#30721;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#23545;&#35821;&#20041;&#20449;&#24687;&#30340;&#25429;&#25417;&#21644;&#25512;&#27979;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;BiPE&#22312;&#19981;&#21516;&#25991;&#26412;&#27169;&#24577;&#30340;&#20219;&#21153;&#20013;&#20855;&#26377;&#20248;&#36234;&#30340;&#38271;&#24230;&#25512;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.16421</link><description>&lt;p&gt;
&#20004;&#31181;&#30707;&#22836;&#20987;&#25171;&#19968;&#21482;&#40479;&#65306;&#32966;&#23618;&#20301;&#32622;&#32534;&#30721;&#20197;&#26356;&#22909;&#22320;&#25512;&#27979;&#38271;&#24230;
&lt;/p&gt;
&lt;p&gt;
Two Stones Hit One Bird: Bilevel Positional Encoding for Better Length Extrapolation. (arXiv:2401.16421v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;&#65292;&#32966;&#23618;&#20301;&#32622;&#32534;&#30721;&#65288;BiPE&#65289;&#65292;&#36890;&#36807;&#23558;&#20998;&#27573;&#20869;&#32534;&#30721;&#21644;&#20998;&#27573;&#38388;&#32534;&#30721;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#23545;&#35821;&#20041;&#20449;&#24687;&#30340;&#25429;&#25417;&#21644;&#25512;&#27979;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;BiPE&#22312;&#19981;&#21516;&#25991;&#26412;&#27169;&#24577;&#30340;&#20219;&#21153;&#20013;&#20855;&#26377;&#20248;&#36234;&#30340;&#38271;&#24230;&#25512;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#35821;&#35328;&#24207;&#21015;&#30340;&#20869;&#22312;&#20998;&#21106;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;&#65292;&#31216;&#20026;&#32966;&#23618;&#20301;&#32622;&#32534;&#30721;&#65288;BiPE&#65289;&#12290;&#23545;&#20110;&#27599;&#20010;&#20301;&#32622;&#65292;&#25105;&#20204;&#30340;BiPE&#23558;&#20998;&#27573;&#20869;&#32534;&#30721;&#21644;&#20998;&#27573;&#38388;&#32534;&#30721;&#34701;&#21512;&#22312;&#19968;&#36215;&#12290;&#20998;&#27573;&#20869;&#32534;&#30721;&#29992;&#20110;&#35782;&#21035;&#27573;&#20869;&#20301;&#32622;&#65292;&#24182;&#36890;&#36807;&#32477;&#23545;&#20301;&#32622;&#32534;&#30721;&#24110;&#21161;&#27169;&#22411;&#25429;&#25417;&#20854;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#20998;&#27573;&#38388;&#32534;&#30721;&#21017;&#29992;&#20110;&#25351;&#23450;&#27573;&#32034;&#24341;&#65292;&#24314;&#27169;&#27573;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#26088;&#22312;&#36890;&#36807;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#25552;&#39640;&#25512;&#27979;&#33021;&#21147;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#20301;&#32622;&#20449;&#24687;&#30340;&#35299;&#32806;&#20351;&#23398;&#20064;&#26356;&#21152;&#26377;&#25928;&#12290;&#32463;&#39564;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;BiPE&#22312;&#19981;&#21516;&#25991;&#26412;&#27169;&#24577;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#20855;&#26377;&#20248;&#36234;&#30340;&#38271;&#24230;&#25512;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we leverage the intrinsic segmentation of language sequences and design a new positional encoding method called Bilevel Positional Encoding (BiPE). For each position, our BiPE blends an intra-segment encoding and an inter-segment encoding. The intra-segment encoding identifies the locations within a segment and helps the model capture the semantic information therein via absolute positional encoding. The inter-segment encoding specifies the segment index, models the relationships between segments, and aims to improve extrapolation capabilities via relative positional encoding. Theoretical analysis shows this disentanglement of positional information makes learning more effective. The empirical results also show that our BiPE has superior length extrapolation capabilities across a wide range of tasks in diverse text modalities.
&lt;/p&gt;</description></item><item><title>HiFT&#26159;&#19968;&#31181;&#20998;&#23618;&#20840;&#21442;&#25968;&#24494;&#35843;&#31574;&#30053;&#65292;&#36890;&#36807;&#20165;&#22312;&#27599;&#20010;&#35757;&#32451;&#27493;&#39588;&#20013;&#26356;&#26032;&#21442;&#25968;&#30340;&#23376;&#38598;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;GPU&#20869;&#23384;&#20351;&#29992;&#65292;&#24182;&#23454;&#29616;&#19982;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#21644;&#26631;&#20934;&#20840;&#21442;&#25968;&#24494;&#35843;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.15207</link><description>&lt;p&gt;
HiFT:&#19968;&#31181;&#20998;&#23618;&#20840;&#21442;&#25968;&#24494;&#35843;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
HiFT: A Hierarchical Full Parameter Fine-Tuning Strategy. (arXiv:2401.15207v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15207
&lt;/p&gt;
&lt;p&gt;
HiFT&#26159;&#19968;&#31181;&#20998;&#23618;&#20840;&#21442;&#25968;&#24494;&#35843;&#31574;&#30053;&#65292;&#36890;&#36807;&#20165;&#22312;&#27599;&#20010;&#35757;&#32451;&#27493;&#39588;&#20013;&#26356;&#26032;&#21442;&#25968;&#30340;&#23376;&#38598;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;GPU&#20869;&#23384;&#20351;&#29992;&#65292;&#24182;&#23454;&#29616;&#19982;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#21644;&#26631;&#20934;&#20840;&#21442;&#25968;&#24494;&#35843;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#30340;&#22686;&#38271;&#65292;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#21442;&#25968;&#38656;&#35201;&#21344;&#29992;&#22823;&#37327;GPU&#20869;&#23384;&#12290;&#29616;&#26377;&#26041;&#27861;&#21033;&#29992;&#38646;&#38454;&#20248;&#21270;&#22120;&#20197;&#33410;&#30465;GPU&#20869;&#23384;&#65292;&#20294;&#36825;&#21487;&#33021;&#20250;&#24433;&#21709;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#22240;&#20026;&#38750;&#38646;&#38454;&#20248;&#21270;&#22120;&#22312;&#22823;&#22810;&#25968;&#19979;&#28216;&#20219;&#21153;&#19978;&#26356;&#23481;&#26131;&#25910;&#25947;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29420;&#31435;&#20110;&#20248;&#21270;&#22120;&#30340;&#31471;&#21040;&#31471;&#20998;&#23618;&#24494;&#35843;&#31574;&#30053;HiFT&#65292;&#23427;&#20165;&#22312;&#27599;&#20010;&#35757;&#32451;&#27493;&#39588;&#20013;&#26356;&#26032;&#21442;&#25968;&#30340;&#23376;&#38598;&#12290; HiFT&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#23384;&#20648;&#22312;GPU&#20869;&#23384;&#20013;&#30340;&#26799;&#24230;&#21644;&#20248;&#21270;&#22120;&#29366;&#24577;&#21442;&#25968;&#30340;&#37327;&#65292;&#20174;&#32780;&#20943;&#23569;GPU&#20869;&#23384;&#30340;&#20351;&#29992;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65306;&#65288;1&#65289;HiFT&#23454;&#29616;&#20102;&#19982;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#21644;&#26631;&#20934;&#20840;&#21442;&#25968;&#24494;&#35843;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#65288;2&#65289;HiFT&#25903;&#25345;&#21253;&#25324;&#22312;&#20869;&#30340;&#21508;&#31181;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Full-parameter fine-tuning has become the go-to choice for adapting language models (LMs) to downstream tasks due to its excellent performance. As LMs grow in size, fine-tuning the full parameters of LMs requires a prohibitively large amount of GPU memory. Existing approaches utilize zeroth-order optimizer to conserve GPU memory, which can potentially compromise the performance of LMs as non-zero order optimizers tend to converge more readily on most downstream tasks. In this paper, we propose a novel optimizer-independent end-to-end hierarchical fine-tuning strategy, HiFT, which only updates a subset of parameters at each training step. HiFT can significantly reduce the amount of gradients and optimizer state parameters residing in GPU memory at the same time, thereby reducing GPU memory usage. Our results demonstrate that: (1) HiFT achieves comparable performance to parameter-efficient fine-tuning and standard full parameter fine-tuning. (2) HiFT supports various optimizers including
&lt;/p&gt;</description></item><item><title>Medusa&#26159;&#19968;&#20010;&#33021;&#22815;&#25552;&#21319;LLM&#25512;&#29702;&#24615;&#33021;&#30340;&#31616;&#27905;&#26694;&#26550;&#65292;&#36890;&#36807;&#22686;&#21152;&#22810;&#20010;&#35299;&#30721;&#22836;&#20197;&#23454;&#29616;&#24182;&#34892;&#39044;&#27979;&#22810;&#20010;&#21518;&#32493;&#26631;&#35760;&#65292;&#24182;&#36890;&#36807;&#26641;&#29366;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#24182;&#34892;&#22788;&#29702;&#26469;&#20943;&#23569;&#35299;&#30721;&#27493;&#39588;&#12290;</title><link>http://arxiv.org/abs/2401.10774</link><description>&lt;p&gt;
Medusa: &#22810;&#35299;&#30721;&#22836;&#30340;&#31616;&#27905;LLM&#25512;&#29702;&#21152;&#36895;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads. (arXiv:2401.10774v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10774
&lt;/p&gt;
&lt;p&gt;
Medusa&#26159;&#19968;&#20010;&#33021;&#22815;&#25552;&#21319;LLM&#25512;&#29702;&#24615;&#33021;&#30340;&#31616;&#27905;&#26694;&#26550;&#65292;&#36890;&#36807;&#22686;&#21152;&#22810;&#20010;&#35299;&#30721;&#22836;&#20197;&#23454;&#29616;&#24182;&#34892;&#39044;&#27979;&#22810;&#20010;&#21518;&#32493;&#26631;&#35760;&#65292;&#24182;&#36890;&#36807;&#26641;&#29366;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#24182;&#34892;&#22788;&#29702;&#26469;&#20943;&#23569;&#35299;&#30721;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#25512;&#29702;&#36807;&#31243;&#36890;&#24120;&#21463;&#38480;&#20110;&#33258;&#22238;&#24402;&#35299;&#30721;&#36807;&#31243;&#20013;&#30340;&#24182;&#34892;&#24615;&#32570;&#22833;&#65292;&#20351;&#24471;&#22823;&#22810;&#25968;&#25805;&#20316;&#21463;&#38480;&#20110;&#21152;&#36895;&#22120;&#30340;&#20869;&#23384;&#24102;&#23485;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#31867;&#20284;&#20110;&#25512;&#27979;&#35299;&#30721;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#30001;&#20110;&#33719;&#24471;&#21644;&#32500;&#25252;&#29420;&#31435;&#30340;&#33609;&#31295;&#27169;&#22411;&#25152;&#28041;&#21450;&#30340;&#25361;&#25112;&#65292;&#23427;&#20204;&#30340;&#23454;&#26045;&#21463;&#21040;&#20102;&#38459;&#30861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28155;&#21152;&#39069;&#22806;&#30340;&#35299;&#30721;&#22836;&#26469;&#22686;&#24378;LLM&#25512;&#29702;&#65292;&#20197;&#24182;&#34892;&#39044;&#27979;&#22810;&#20010;&#21518;&#32493;&#26631;&#35760;&#12290;Medusa&#21033;&#29992;&#22522;&#20110;&#26641;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#22312;&#27599;&#20010;&#35299;&#30721;&#27493;&#39588;&#20013;&#21516;&#26102;&#26500;&#36896;&#22810;&#20010;&#20505;&#36873;&#24310;&#32493;&#24182;&#36827;&#34892;&#39564;&#35777;&#12290;&#36890;&#36807;&#21033;&#29992;&#24182;&#34892;&#22788;&#29702;&#65292;Medusa&#22312;&#21333;&#27493;&#24310;&#36831;&#26041;&#38754;&#20165;&#24341;&#20837;&#20102;&#26368;&#23567;&#30340;&#24320;&#38144;&#65292;&#21516;&#26102;&#22823;&#22823;&#38477;&#20302;&#20102;&#25152;&#38656;&#30340;&#35299;&#30721;&#27493;&#39588;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The inference process in Large Language Models (LLMs) is often limited due to the absence of parallelism in the auto-regressive decoding process, resulting in most operations being restricted by the memory bandwidth of accelerators. While methods such as speculative decoding have been suggested to address this issue, their implementation is impeded by the challenges associated with acquiring and maintaining a separate draft model. In this paper, we present Medusa, an efficient method that augments LLM inference by adding extra decoding heads to predict multiple subsequent tokens in parallel. Using a tree-based attention mechanism, Medusa constructs multiple candidate continuations and verifies them simultaneously in each decoding step. By leveraging parallel processing, Medusa introduces only minimal overhead in terms of single-step latency while substantially reducing the number of decoding steps required.  We present two levels of fine-tuning procedures for Medusa to meet the needs o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#36866;&#37197;&#22120;&#21644;Mixup&#30456;&#32467;&#21512;&#65292;&#20197;&#22312;&#19981;&#38656;&#35201;&#39057;&#32321;&#37325;&#26032;&#35757;&#32451;&#25972;&#20010;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#22686;&#24378;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#20351;&#29992;&#36866;&#37197;&#22120;&#30340;&#20984;&#32452;&#21512;&#21644;&#38750;&#25968;&#25454;&#23545;&#30340;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#22312;&#24615;&#33021;&#19979;&#38477;&#21644;&#35745;&#31639;&#24320;&#38144;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2401.10111</link><description>&lt;p&gt;
&#23558;&#36866;&#37197;&#22120;&#21644;Mixup&#30456;&#32467;&#21512;&#65292;&#20197;&#22686;&#24378;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Marrying Adapters and Mixup to Efficiently Enhance the Adversarial Robustness of Pre-Trained Language Models for Text Classification. (arXiv:2401.10111v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10111
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#36866;&#37197;&#22120;&#21644;Mixup&#30456;&#32467;&#21512;&#65292;&#20197;&#22312;&#19981;&#38656;&#35201;&#39057;&#32321;&#37325;&#26032;&#35757;&#32451;&#25972;&#20010;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#22686;&#24378;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#20351;&#29992;&#36866;&#37197;&#22120;&#30340;&#20984;&#32452;&#21512;&#21644;&#38750;&#25968;&#25454;&#23545;&#30340;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#22312;&#24615;&#33021;&#19979;&#38477;&#21644;&#35745;&#31639;&#24320;&#38144;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#24178;&#20928;&#21644;&#23545;&#25239;&#24615;&#26679;&#26412;&#26469;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#25968;&#25454;&#21487;&#20197;&#25552;&#39640;&#20854;&#22312;&#23545;&#25239;&#25915;&#20987;&#19979;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35757;&#32451;&#26041;&#27861;&#24448;&#24448;&#20250;&#23548;&#33268;&#23545;&#28165;&#27905;&#36755;&#20837;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#21478;&#22806;&#65292;&#23427;&#38656;&#35201;&#39057;&#32321;&#22320;&#37325;&#26032;&#35757;&#32451;&#25972;&#20010;&#27169;&#22411;&#65292;&#20197;&#36866;&#24212;&#26032;&#30340;&#25915;&#20987;&#31867;&#22411;&#65292;&#20174;&#32780;&#23548;&#33268;&#26114;&#36149;&#19988;&#35745;&#31639;&#37327;&#22823;&#30340;&#35745;&#31639;&#12290;&#36825;&#20123;&#38480;&#21046;&#20351;&#24471;&#23545;&#25239;&#35757;&#32451;&#26426;&#21046;&#30340;&#23454;&#38469;&#24212;&#29992;&#21464;&#24471;&#19981;&#22826;&#23454;&#29992;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20855;&#26377;&#25968;&#30334;&#19975;&#29978;&#33267;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#22797;&#26434;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#21516;&#26102;&#21033;&#29992;&#23545;&#25239;&#35757;&#32451;&#30340;&#29702;&#35770;&#30410;&#22788;&#65292;&#26412;&#30740;&#31350;&#23558;&#20004;&#20010;&#27010;&#24565;&#30456;&#32467;&#21512;&#65306;&#65288;1&#65289;&#36866;&#37197;&#22120;&#65292;&#21487;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65292;&#21644;&#65288;2&#65289;Mixup&#65292;&#36890;&#36807;&#25968;&#25454;&#23545;&#30340;&#20984;&#32452;&#21512;&#35757;&#32451;NNs&#12290;&#30452;&#35266;&#22320;&#35828;&#65292;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#38750;&#25968;&#25454;&#23545;&#30340;&#36866;&#37197;&#22120;&#30340;&#20984;&#32452;&#21512;&#26469;&#24494;&#35843;PLMs&#65292;&#20854;&#20013;&#19968;&#20010;&#36866;&#37197;&#22120;&#20351;&#29992;&#24178;&#20928;&#30340;&#26679;&#26412;&#35757;&#32451;&#65292;&#21478;&#19968;&#20010;&#20351;&#29992;&#23545;&#25239;&#24615;&#30340;&#26679;&#26412;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing works show that augmenting training data of neural networks using both clean and adversarial examples can enhance their generalizability under adversarial attacks. However, this training approach often leads to performance degradation on clean inputs. Additionally, it requires frequent re-training of the entire model to account for new attack types, resulting in significant and costly computations. Such limitations make adversarial training mechanisms less practical, particularly for complex Pre-trained Language Models (PLMs) with millions or even billions of parameters. To overcome these challenges while still harnessing the theoretical benefits of adversarial training, this study combines two concepts: (1) adapters, which enable parameter-efficient fine-tuning, and (2) Mixup, which train NNs via convex combinations of pairs data pairs. Intuitively, we propose to fine-tune PLMs through convex combinations of non-data pairs of fine-tuned adapters, one trained with clean and an
&lt;/p&gt;</description></item><item><title>MMToM-QA&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#24515;&#26234;&#29702;&#35770;&#38382;&#31572;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#23545;&#20110;&#20154;&#30340;&#24515;&#26234;&#29702;&#35770;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;BIP-ALM&#29992;&#20110;&#23454;&#29616;&#22810;&#27169;&#24577;&#24515;&#26234;&#29702;&#35770;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.08743</link><description>&lt;p&gt;
MMToM-QA: &#22810;&#27169;&#24577;&#24515;&#26234;&#29702;&#35770;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
MMToM-QA: Multimodal Theory of Mind Question Answering. (arXiv:2401.08743v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08743
&lt;/p&gt;
&lt;p&gt;
MMToM-QA&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#24515;&#26234;&#29702;&#35770;&#38382;&#31572;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#23545;&#20110;&#20154;&#30340;&#24515;&#26234;&#29702;&#35770;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;BIP-ALM&#29992;&#20110;&#23454;&#29616;&#22810;&#27169;&#24577;&#24515;&#26234;&#29702;&#35770;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#20154;&#20204;&#30340;&#24515;&#26234;&#26159;&#24320;&#21457;&#20855;&#26377;&#20154;&#31867;&#27700;&#24179;&#31038;&#20132;&#26234;&#33021;&#30340;&#26426;&#22120;&#30340;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#26368;&#36817;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20284;&#20046;&#23637;&#29616;&#20986;&#26576;&#20123;&#24515;&#26234;&#29702;&#35299;&#30340;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24515;&#26234;&#29702;&#35770;&#22522;&#20934;&#20351;&#29992;&#21333;&#27169;&#24577;&#25968;&#25454;&#38598;-&#25110;&#32773;&#35270;&#39057;&#25110;&#32773;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#30340;&#24515;&#26234;&#29702;&#35770;&#19981;&#20165;&#20165;&#26159;&#35270;&#39057;&#25110;&#25991;&#26412;&#29702;&#35299;&#12290;&#20154;&#20204;&#21487;&#20197;&#26681;&#25454;&#20174;&#20219;&#20309;&#21487;&#29992;&#25968;&#25454;&#20013;&#25552;&#21462;&#30340;&#27010;&#24565;&#34920;&#31034;&#65288;&#20363;&#22914;&#30446;&#26631;&#65292;&#20449;&#24565;&#65292;&#35745;&#21010;&#65289;&#28789;&#27963;&#22320;&#25512;&#29702;&#21478;&#19968;&#20010;&#20154;&#30340;&#24515;&#26234;&#65292;&#36825;&#20123;&#25968;&#25454;&#21487;&#20197;&#21253;&#25324;&#35270;&#35273;&#32447;&#32034;&#65292;&#35821;&#35328;&#21465;&#20107;&#25110;&#20004;&#32773;&#20860;&#26377;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#30340;&#24515;&#26234;&#29702;&#35770;&#38382;&#31572;&#65288;MMToM-QA&#65289;&#22522;&#20934;&#12290;MMToM-QA&#22312;&#22810;&#27169;&#24577;&#25968;&#25454;&#21644;&#20851;&#20110;&#19968;&#20010;&#20154;&#22312;&#23478;&#24237;&#29615;&#22659;&#20013;&#30340;&#27963;&#21160;&#30340;&#19981;&#21516;&#31181;&#31867;&#30340;&#21333;&#27169;&#24577;&#25968;&#25454;&#19978;&#20840;&#38754;&#35780;&#20272;&#26426;&#22120;&#30340;&#24515;&#26234;&#29702;&#35770;&#12290;&#20026;&#20102;&#23454;&#29616;&#22810;&#27169;&#24577;&#24515;&#26234;&#29702;&#35770;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;BIP-ALM&#65288;&#36125;&#21494;&#26031;&#36870;&#21521;&#35268;&#21010;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Theory of Mind (ToM), the ability to understand people's minds, is an essential ingredient for developing machines with human-level social intelligence. Recent machine learning models, particularly large language models, seem to show some aspects of ToM understanding. However, existing ToM benchmarks use unimodal datasets - either video or text. Human ToM, on the other hand, is more than video or text understanding. People can flexibly reason about another person's mind based on conceptual representations (e.g., goals, beliefs, plans) extracted from any available data, which can include visual cues, linguistic narratives, or both. To address this, we introduce a multimodal Theory of Mind question answering (MMToM-QA) benchmark. MMToM-QA comprehensively evaluates machine ToM both on multimodal data and on different kinds of unimodal data about a person's activity in a household environment. To engineer multimodal ToM capacity, we propose a novel method, BIP-ALM (Bayesian Inverse Plannin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33258;&#21160;&#21270;&#20316;&#25991;&#35780;&#20998;&#31995;&#32479;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21457;&#29616;LLM AES&#31995;&#32479;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12289;&#19968;&#33268;&#24615;&#12289;&#26222;&#36866;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#27492;&#22806;&#65292;LLM&#36824;&#33021;&#25552;&#21319;&#20154;&#31867;&#35780;&#20998;&#21592;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.06431</link><description>&lt;p&gt;
&#20174;&#33258;&#21160;&#21270;&#21040;&#22686;&#24378;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#21319;&#20316;&#25991;&#35780;&#20998;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
From Automation to Augmentation: Large Language Models Elevating Essay Scoring Landscape. (arXiv:2401.06431v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33258;&#21160;&#21270;&#20316;&#25991;&#35780;&#20998;&#31995;&#32479;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21457;&#29616;LLM AES&#31995;&#32479;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12289;&#19968;&#33268;&#24615;&#12289;&#26222;&#36866;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#27492;&#22806;&#65292;LLM&#36824;&#33021;&#25552;&#21319;&#20154;&#31867;&#35780;&#20998;&#21592;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#31532;&#20108;&#35821;&#35328;&#23398;&#20064;&#32773;&#26469;&#35828;&#65292;&#25509;&#25910;&#21363;&#26102;&#20010;&#24615;&#21270;&#21453;&#39304;&#38750;&#24120;&#37325;&#35201;&#65292;&#24403;&#20154;&#31867;&#25945;&#24072;&#26080;&#27861;&#25552;&#20379;&#26102;&#65292;&#33258;&#21160;&#21270;&#20316;&#25991;&#35780;&#20998;&#31995;&#32479;&#26159;&#19968;&#31181;&#37325;&#35201;&#36164;&#28304;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#29305;&#21035;&#26159;GPT-4&#21644;&#32463;&#36807;&#24494;&#35843;&#30340;GPT-3.5&#65292;&#20316;&#20026;AES&#24037;&#20855;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#22522;&#20110;&#20844;&#20849;&#21644;&#31169;&#26377;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#31361;&#20986;&#20102;LLM AES&#31995;&#32479;&#30340;&#26174;&#30528;&#20248;&#21183;&#65292;&#21253;&#25324;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12289;&#19968;&#33268;&#24615;&#12289;&#26222;&#36866;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#32780;&#32463;&#36807;&#24494;&#35843;&#30340;GPT-3.5&#36229;&#36234;&#20102;&#20256;&#32479;&#35780;&#20998;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;LLM&#36741;&#21161;&#30340;&#20154;&#24037;&#35780;&#20272;&#23454;&#39564;&#65292;&#28041;&#21450;&#21021;&#23398;&#32773;&#21644;&#19987;&#23478;&#35780;&#20998;&#21592;&#12290;&#19968;&#20010;&#20851;&#38190;&#30340;&#21457;&#29616;&#26159;&#65292;LLM&#19981;&#20165;&#33021;&#33258;&#21160;&#21270;&#35780;&#20998;&#36807;&#31243;&#65292;&#36824;&#33021;&#25552;&#21319;&#20154;&#31867;&#35780;&#20998;&#21592;&#30340;&#24615;&#33021;&#12290;&#24403;&#21021;&#23398;&#32773;&#35780;&#20998;&#21592;&#33719;&#24471;LLM&#29983;&#25104;&#30340;&#21453;&#39304;&#26102;&#65292;&#20854;&#20934;&#30830;&#24615;&#19982;&#19987;&#23478;&#27700;&#24179;&#30456;&#24403;&#65292;&#21516;&#26102;&#19987;&#23478;&#21464;&#24471;&#26356;&#21152;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Receiving immediate and personalized feedback is crucial for second-language learners, and Automated Essay Scoring (AES) systems are a vital resource when human instructors are unavailable. This study investigates the effectiveness of Large Language Models (LLMs), specifically GPT-4 and fine-tuned GPT-3.5, as tools for AES. Our comprehensive set of experiments, conducted on both public and private datasets, highlights the remarkable advantages of LLM-based AES systems. They include superior accuracy, consistency, generalizability, and interpretability, with fine-tuned GPT-3.5 surpassing traditional grading models. Additionally, we undertake LLM-assisted human evaluation experiments involving both novice and expert graders. One pivotal discovery is that LLMs not only automate the grading process but also enhance the performance of human graders. Novice graders when provided with feedback generated by LLMs, achieve a level of accuracy on par with experts, while experts become more effici
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26368;&#23567;&#32534;&#36753;&#32422;&#26463;&#19979;&#30340;&#32454;&#31890;&#24230;&#24378;&#21270;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RLMEC&#30340;&#26032;&#30340;RL&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#20316;&#20026;&#22870;&#21169;&#27169;&#22411;&#65292;&#21487;&#20197;&#20026;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#25552;&#20379;&#26631;&#35760;&#32423;&#21035;&#30340;&#32454;&#31890;&#24230;&#30417;&#30563;&#65292;&#19987;&#27880;&#20110;&#20851;&#38190;&#26631;&#35760;&#30340;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2401.06081</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#23567;&#32534;&#36753;&#32422;&#26463;&#30340;&#32454;&#31890;&#24230;&#24378;&#21270;&#23398;&#20064;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Improving Large Language Models via Fine-grained Reinforcement Learning with Minimum Editing Constraint. (arXiv:2401.06081v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06081
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26368;&#23567;&#32534;&#36753;&#32422;&#26463;&#19979;&#30340;&#32454;&#31890;&#24230;&#24378;&#21270;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RLMEC&#30340;&#26032;&#30340;RL&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#20316;&#20026;&#22870;&#21169;&#27169;&#22411;&#65292;&#21487;&#20197;&#20026;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#25552;&#20379;&#26631;&#35760;&#32423;&#21035;&#30340;&#32454;&#31890;&#24230;&#30417;&#30563;&#65292;&#19987;&#27880;&#20110;&#20851;&#38190;&#26631;&#35760;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#36991;&#20813;&#24847;&#22806;&#36755;&#20986;&#65292;&#22914;&#20943;&#23569;&#26377;&#23475;&#21644;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;RL&#26041;&#27861;&#22823;&#22810;&#37319;&#29992;&#23454;&#20363;&#32423;&#22870;&#21169;&#65292;&#26080;&#27861;&#20026;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#25552;&#20379;&#32454;&#31890;&#24230;&#30340;&#30417;&#30563;&#65292;&#24182;&#19988;&#19981;&#33021;&#19987;&#27880;&#20110;&#23548;&#33268;&#19981;&#27491;&#30830;&#30340;&#23569;&#25968;&#20851;&#38190;&#26631;&#35760;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RLMEC&#30340;&#26032;&#30340;RL&#26041;&#27861;&#65292;&#23427;&#23558;&#29983;&#25104;&#27169;&#22411;&#20316;&#20026;&#22870;&#21169;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#32534;&#36753;&#32422;&#26463;&#19979;&#30340;&#38169;&#35823;&#35299;&#37325;&#20889;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#20197;&#20026;RL&#35757;&#32451;&#29983;&#25104;&#26631;&#35760;&#32423;&#21035;&#30340;&#22870;&#21169;&#12290;&#22522;&#20110;&#29983;&#25104;&#30340;&#22870;&#21169;&#27169;&#22411;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#26631;&#35760;&#32423;&#21035;&#30340;RL&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#21450;&#22522;&#20110;&#27169;&#20223;&#30340;&#27491;&#21017;&#21270;&#26469;&#31283;&#23450;RL&#36827;&#31243;&#12290;&#36825;&#20004;&#20010;&#30446;&#26631;&#37117;&#30528;&#37325;&#20110;&#38169;&#35823;&#35299;&#30340;&#20851;&#38190;&#26631;&#35760;&#30340;&#23398;&#20064;&#65292;&#20943;&#23569;&#20854;&#20182;&#19981;&#37325;&#35201;&#26631;&#35760;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) has been widely used in training large language models~(LLMs) for preventing unexpected outputs, \eg reducing harmfulness and errors. However, existing RL methods mostly adopt the instance-level reward, which is unable to provide fine-grained supervision for complex reasoning tasks, and can not focus on the few key tokens that lead to the incorrectness. To address it, we propose a new RL method named \textbf{RLMEC} that incorporates a generative model as the reward model, which is trained by the erroneous solution rewriting task under the minimum editing constraint, and can produce token-level rewards for RL training. Based on the generative reward model, we design the token-level RL objective for training and an imitation-based regularization for stabilizing RL process. And the both objectives focus on the learning of the key tokens for the erroneous solution, reducing the effect of other unimportant tokens. The experiment results on mathematical tasks and 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35770;&#25991;&#25351;&#20986;&#65292;&#27169;&#22411;&#32534;&#36753;&#21487;&#33021;&#20250;&#25913;&#21892;&#27169;&#22411;&#30340;&#20107;&#23454;&#24615;&#65292;&#20294;&#20250;&#20197;&#38477;&#20302;&#27169;&#22411;&#30340;&#36890;&#29992;&#33021;&#21147;&#20026;&#20195;&#20215;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#20316;&#32773;&#36890;&#36807;&#35780;&#20272;&#22235;&#31181;&#32534;&#36753;&#26041;&#27861;&#22312;&#20004;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#36825;&#20123;&#32534;&#36753;&#26041;&#27861;&#24448;&#24448;&#24573;&#35270;&#20102;&#23545;&#27169;&#22411;&#36890;&#29992;&#33021;&#21147;&#21487;&#33021;&#20135;&#29983;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.04700</link><description>&lt;p&gt;
&#27169;&#22411;&#32534;&#36753;&#21487;&#33021;&#20250;&#25439;&#23475;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Model Editing Can Hurt General Abilities of Large Language Models. (arXiv:2401.04700v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04700
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35770;&#25991;&#25351;&#20986;&#65292;&#27169;&#22411;&#32534;&#36753;&#21487;&#33021;&#20250;&#25913;&#21892;&#27169;&#22411;&#30340;&#20107;&#23454;&#24615;&#65292;&#20294;&#20250;&#20197;&#38477;&#20302;&#27169;&#22411;&#30340;&#36890;&#29992;&#33021;&#21147;&#20026;&#20195;&#20215;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#20316;&#32773;&#36890;&#36807;&#35780;&#20272;&#22235;&#31181;&#32534;&#36753;&#26041;&#27861;&#22312;&#20004;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#36825;&#20123;&#32534;&#36753;&#26041;&#27861;&#24448;&#24448;&#24573;&#35270;&#20102;&#23545;&#27169;&#22411;&#36890;&#29992;&#33021;&#21147;&#21487;&#33021;&#20135;&#29983;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#20026;&#25105;&#20204;&#33719;&#21462;&#20854;&#21442;&#25968;&#20013;&#23384;&#20648;&#30340;&#30693;&#35782;&#25552;&#20379;&#20102;&#26032;&#30340;&#33539;&#24335;&#12290;&#19968;&#20010;&#20851;&#38190;&#30340;&#25361;&#25112;&#26159;LLM&#36755;&#20986;&#20013;&#23384;&#22312;&#38169;&#35273;&#65292;&#36825;&#26159;&#30001;&#20110;&#38169;&#35823;&#25110;&#36807;&#26102;&#30693;&#35782;&#24341;&#36215;&#30340;&#12290;&#30001;&#20110;&#20351;&#29992;&#26356;&#26032;&#21518;&#30340;&#20449;&#24687;&#37325;&#26032;&#35757;&#32451;LLM&#38656;&#35201;&#22823;&#37327;&#36164;&#28304;&#65292;&#22240;&#27492;&#20154;&#20204;&#23545;&#27169;&#22411;&#32534;&#36753;&#20135;&#29983;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#24456;&#26377;&#25928;&#65292;&#20294;&#24448;&#24448;&#36807;&#20110;&#24378;&#35843;&#32534;&#36753;&#24615;&#33021;&#30340;&#21151;&#25928;&#12289;&#27867;&#21270;&#24615;&#21644;&#23616;&#37096;&#24615;&#65292;&#24120;&#24120;&#24573;&#35270;&#20102;&#23545;LLM&#30340;&#36890;&#29992;&#33021;&#21147;&#21487;&#33021;&#20135;&#29983;&#30340;&#21103;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#25913;&#21892;&#27169;&#22411;&#30340;&#20107;&#23454;&#24615;&#21487;&#33021;&#20250;&#20197;&#30456;&#24403;&#22823;&#30340;&#36890;&#29992;&#33021;&#21147;&#19979;&#38477;&#20026;&#20195;&#20215;&#30340;&#25285;&#24551;&#65292;&#36825;&#19981;&#31526;&#21512;LLM&#21487;&#25345;&#32493;&#21457;&#23637;&#30340;&#35201;&#27714;&#12290;&#25105;&#20204;&#36890;&#36807;&#35780;&#20272;&#22235;&#31181;&#24120;&#29992;&#30340;&#32534;&#36753;&#26041;&#27861;&#22312;&#20004;&#20010;LLM&#19978;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;&#21103;&#20316;&#29992;&#65292;&#24182;&#28085;&#30422;&#20102;&#20843;&#20010;&#20195;&#34920;&#24615;&#20219;&#21153;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in large language models (LLMs) have opened up new paradigms for accessing the knowledge stored in their parameters. One critical challenge that has emerged is the presence of hallucinations in LLM outputs due to false or outdated knowledge. Since retraining LLMs with updated information is resource-intensive, there has been a growing interest in model editing. However, many model editing methods, while effective in various scenarios, tend to overemphasize aspects such as efficacy, generalization, and locality in editing performance, often overlooking potential side effects on the general abilities of LLMs. In this paper, we raise concerns that the improvement of model factuality may come at the cost of a significant degradation of these general abilities, which is not conducive to the sustainable development of LLMs. Systematically, we analyze side effects by evaluating four popular editing methods on two LLMs across eight representative task categories. Extensive empi
&lt;/p&gt;</description></item><item><title>MLLM-Protektor&#26159;&#19968;&#20010;&#21363;&#25554;&#21363;&#29992;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20445;&#25252;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20813;&#21463;&#24694;&#24847;&#25915;&#20987;&#12290;&#36890;&#36807;&#35299;&#20915;&#22270;&#20687;&#19982;&#25991;&#26412;&#23545;&#40784;&#30340;&#25361;&#25112;&#65292;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#23545;MLLM&#30340;&#26377;&#25928;&#20445;&#25252;&#65292;&#38450;&#27490;&#20854;&#20135;&#29983;&#26377;&#23475;&#21709;&#24212;&#12290;</title><link>http://arxiv.org/abs/2401.02906</link><description>&lt;p&gt;
MLLM-Protektor: &#30830;&#20445;MLLM&#30340;&#23433;&#20840;&#24615;&#32780;&#19981;&#24433;&#21709;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance. (arXiv:2401.02906v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02906
&lt;/p&gt;
&lt;p&gt;
MLLM-Protektor&#26159;&#19968;&#20010;&#21363;&#25554;&#21363;&#29992;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20445;&#25252;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20813;&#21463;&#24694;&#24847;&#25915;&#20987;&#12290;&#36890;&#36807;&#35299;&#20915;&#22270;&#20687;&#19982;&#25991;&#26412;&#23545;&#40784;&#30340;&#25361;&#25112;&#65292;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#23545;MLLM&#30340;&#26377;&#25928;&#20445;&#25252;&#65292;&#38450;&#27490;&#20854;&#20135;&#29983;&#26377;&#23475;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(MLLM)&#30340;&#37096;&#32626;&#24102;&#26469;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#33030;&#24369;&#24615;&#65306;&#23545;&#35270;&#35273;&#36755;&#20837;&#30340;&#24694;&#24847;&#25915;&#20987;&#26131;&#21463;&#25915;&#20987;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#20445;&#25252;MLLM&#20813;&#21463;&#27492;&#31867;&#25915;&#20987;&#30340;&#26032;&#25361;&#25112;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22270;&#20687;&#20316;&#20026;&#19968;&#31181;&#8220;&#22806;&#35821;&#8221;&#22312;&#23545;&#40784;&#36807;&#31243;&#20013;&#27809;&#26377;&#34987;&#32771;&#34385;&#21040;&#65292;&#36825;&#21487;&#33021;&#20351;MLLM&#26356;&#23481;&#26131;&#20135;&#29983;&#26377;&#23475;&#30340;&#21709;&#24212;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#19982;&#25991;&#26412;&#20013;&#25152;&#32771;&#34385;&#30340;&#31163;&#25955;&#26631;&#35760;&#19981;&#21516;&#65292;&#22270;&#20687;&#20449;&#21495;&#30340;&#36830;&#32493;&#24615;&#36136;&#22312;&#23545;&#40784;&#36807;&#31243;&#20013;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#20174;&#32780;&#20351;&#35206;&#30422;&#21487;&#33021;&#24773;&#26223;&#21464;&#24471;&#22256;&#38590;&#12290;&#36825;&#31181;&#33030;&#24369;&#24615;&#21152;&#21095;&#20102;&#19968;&#20010;&#20107;&#23454;&#65292;&#21363;&#24320;&#28304;&#30340;MLLM&#20027;&#35201;&#22312;&#26377;&#38480;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#19978;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#36828;&#36828;&#23567;&#20110;&#24191;&#27867;&#30340;&#25991;&#26412;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#65292;&#36825;&#20351;&#24471;MLLM&#22312;&#26126;&#30830;&#23545;&#40784;&#35843;&#25972;&#36807;&#31243;&#20013;&#26356;&#23481;&#26131;&#36951;&#24536;&#20854;&#21407;&#22987;&#33021;&#21147;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MLLM-Protektor&#65292;&#19968;&#20010;&#21363;&#25554;&#21363;&#29992;&#30340;
&lt;/p&gt;
&lt;p&gt;
The deployment of multimodal large language models (MLLMs) has brought forth a unique vulnerability: susceptibility to malicious attacks through visual inputs. We delve into the novel challenge of defending MLLMs against such attacks. We discovered that images act as a "foreign language" that is not considered during alignment, which can make MLLMs prone to producing harmful responses. Unfortunately, unlike the discrete tokens considered in text-based LLMs, the continuous nature of image signals presents significant alignment challenges, which poses difficulty to thoroughly cover the possible scenarios. This vulnerability is exacerbated by the fact that open-source MLLMs are predominantly fine-tuned on limited image-text pairs that is much less than the extensive text-based pretraining corpus, which makes the MLLMs more prone to catastrophic forgetting of their original abilities during explicit alignment tuning. To tackle these challenges, we introduce MLLM-Protector, a plug-and-play 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#25105;&#23545;&#24369;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32454;&#35843;&#65288;SPIN&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#33258;&#25105;&#23545;&#24328;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#20174;&#20013;&#20248;&#21270;&#27169;&#22411;&#31574;&#30053;&#65292;&#20174;&#32780;&#23558;&#24369;&#35821;&#35328;&#27169;&#22411;&#36716;&#21270;&#20026;&#24378;&#35821;&#35328;&#27169;&#22411;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#20154;&#31867;&#26631;&#27880;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2401.01335</link><description>&lt;p&gt;
&#33258;&#25105;&#23545;&#24369;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32454;&#35843;&#21487;&#20197;&#23558;&#20854;&#36716;&#21270;&#20026;&#24378;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models. (arXiv:2401.01335v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#25105;&#23545;&#24369;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32454;&#35843;&#65288;SPIN&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#33258;&#25105;&#23545;&#24328;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#20174;&#20013;&#20248;&#21270;&#27169;&#22411;&#31574;&#30053;&#65292;&#20174;&#32780;&#23558;&#24369;&#35821;&#35328;&#27169;&#22411;&#36716;&#21270;&#20026;&#24378;&#35821;&#35328;&#27169;&#22411;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#20154;&#31867;&#26631;&#27880;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30417;&#30563;&#32454;&#35843;&#65288;SFT&#65289;&#21033;&#29992;&#20154;&#31867;&#26631;&#27880;&#25968;&#25454;&#30340;&#21147;&#37327;&#23545;&#20110;&#25512;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#19981;&#38656;&#35201;&#33719;&#21462;&#39069;&#22806;&#20154;&#31867;&#26631;&#27880;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#24369;&#35821;&#35328;&#27169;&#22411;&#21457;&#23637;&#25104;&#20026;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#25105;&#23545;&#24369;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32454;&#35843;&#65288;SPIN&#65289;&#30340;&#26032;&#30340;&#32454;&#35843;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20174;&#19968;&#20010;&#32463;&#36807;&#30417;&#30563;&#32454;&#35843;&#30340;&#27169;&#22411;&#24320;&#22987;&#12290;SPIN&#30340;&#26680;&#24515;&#26159;&#33258;&#25105;&#23545;&#24369;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#21046;&#65292;&#20854;&#20013;&#24369;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#19982;&#33258;&#36523;&#30340;&#23454;&#20363;&#23545;&#24328;&#26469;&#25552;&#21319;&#33258;&#24049;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24369;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#29983;&#25104;&#33258;&#24049;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#20248;&#21270;&#33258;&#36523;&#31574;&#30053;&#65292;&#36890;&#36807;&#21306;&#20998;&#33258;&#25105;&#29983;&#25104;&#30340;&#22238;&#24212;&#19982;&#26469;&#33258;&#20154;&#31867;&#26631;&#27880;&#25968;&#25454;&#30340;&#22238;&#24212;&#26469;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36880;&#27493;&#23558;&#24369;&#35821;&#35328;&#27169;&#22411;&#25552;&#21319;&#20026;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;&#20805;&#20998;&#21457;&#25496;&#20154;&#31867;&#26631;&#27880;&#31034;&#33539;&#25968;&#25454;&#22312;SFT&#20013;&#30340;&#28508;&#21147;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#35757;&#32451;&#30446;&#26631;&#20989;&#25968;&#30340;&#20840;&#23616;&#26368;&#20248;&#35299;&#26159;&#21487;&#20197;&#36798;&#21040;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Harnessing the power of human-annotated data through Supervised Fine-Tuning (SFT) is pivotal for advancing Large Language Models (LLMs). In this paper, we delve into the prospect of growing a strong LLM out of a weak one without the need for acquiring additional human-annotated data. We propose a new fine-tuning method called Self-Play fIne-tuNing (SPIN), which starts from a supervised fine-tuned model. At the heart of SPIN lies a self-play mechanism, where the LLM refines its capability by playing against instances of itself. More specifically, the LLM generates its own training data from its previous iterations, refining its policy by discerning these self-generated responses from those obtained from human-annotated data. Our method progressively elevates the LLM from a nascent model to a formidable one, unlocking the full potential of human-annotated demonstration data for SFT. Theoretically, we prove that the global optimum to the training objective function of our method is achiev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#27010;&#24565;&#32423;&#21035;&#30340;&#35823;&#30456;&#20851;&#24615;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;ChatGPT&#20998;&#37197;&#27010;&#24565;&#26631;&#31614;&#21644;&#24341;&#20837;&#25968;&#25454;&#20877;&#24179;&#34913;&#25216;&#26415;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.08648</link><description>&lt;p&gt;
&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#25506;&#32034;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#27010;&#24565;&#32423;&#21035;&#30340;&#35823;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
Explore Spurious Correlations at the Concept Level in Language Models for Text Classification. (arXiv:2311.08648v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.08648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#27010;&#24565;&#32423;&#21035;&#30340;&#35823;&#30456;&#20851;&#24615;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;ChatGPT&#20998;&#37197;&#27010;&#24565;&#26631;&#31614;&#21644;&#24341;&#20837;&#25968;&#25454;&#20877;&#24179;&#34913;&#25216;&#26415;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#20247;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#37319;&#29992;&#20102;&#24494;&#35843;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#12290;&#34429;&#28982;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#20013;&#26631;&#31614;&#20998;&#24067;&#19981;&#24179;&#34913;&#25110;&#19978;&#19979;&#25991;&#23398;&#20064;&#23454;&#20363;&#20135;&#29983;&#30340;&#35823;&#30456;&#20851;&#24615;&#65292;&#23427;&#20204;&#38754;&#20020;&#30528;&#40065;&#26834;&#24615;&#25361;&#25112;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#35789;&#35821;&#12289;&#30701;&#35821;&#21644;&#21477;&#27861;&#29305;&#24449;&#19978;&#65292;&#24573;&#35270;&#20102;&#27010;&#24565;&#32423;&#21035;&#30340;&#30740;&#31350;&#65292;&#36825;&#24448;&#24448;&#26159;&#30001;&#20110;&#32570;&#20047;&#27010;&#24565;&#26631;&#31614;&#21644;&#38590;&#20197;&#30830;&#23450;&#36755;&#20837;&#25991;&#26412;&#20013;&#30340;&#27010;&#24565;&#20869;&#23481;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#20027;&#35201;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;ChatGPT&#20026;&#25991;&#26412;&#20998;&#37197;&#27010;&#24565;&#26631;&#31614;&#65292;&#35780;&#20272;&#27169;&#22411;&#22312;&#24494;&#35843;&#25110;&#19978;&#19979;&#25991;&#23398;&#20064;&#27979;&#35797;&#25968;&#25454;&#20013;&#30340;&#27010;&#24565;&#20559;&#24046;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;&#35821;&#35328;&#27169;&#22411;&#22312;&#35757;&#32451;&#25110;&#25552;&#31034;&#20013;&#36935;&#21040;&#27010;&#24565;&#21644;&#26631;&#31614;&#20043;&#38388;&#30340;&#35823;&#30456;&#20851;&#24615;&#26102;&#65292;&#20250;&#37319;&#21462;&#39044;&#27979;&#30340;&#25463;&#24452;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#25968;&#25454;&#20877;&#24179;&#34913;&#25216;&#26415;&#65292;&#23558;ChatGPT&#29983;&#25104;&#30340;&#21453;&#20107;&#23454;&#25968;&#25454;&#32435;&#20837;&#20854;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) have achieved notable success in numerous NLP tasks, employing both fine-tuning and in-context learning (ICL) methods. While language models demonstrate exceptional performance, they face robustness challenges due to spurious correlations arising from imbalanced label distributions in training data or ICL exemplars. Previous research has primarily concentrated on word, phrase, and syntax features, neglecting the concept level, often due to the absence of concept labels and difficulty in identifying conceptual content in input texts. This paper introduces two main contributions. First, we employ ChatGPT to assign concept labels to texts, assessing concept bias in models during fine-tuning or ICL on test data. We find that LMs, when encountering spurious correlations between a concept and a label in training or prompts, resort to shortcuts for predictions. Second, we introduce a data rebalancing technique that incorporates ChatGPT-generated counterfactual data, ther
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;51&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#30340;&#31038;&#20250;&#36523;&#20221;&#20559;&#35265;&#65292;&#21457;&#29616;&#20960;&#20046;&#25152;&#26377;&#27169;&#22411;&#22312;&#34917;&#20840;&#21477;&#23376;&#26102;&#37117;&#23637;&#31034;&#20102;&#26126;&#26174;&#30340;&#22242;&#20307;&#20869;&#31215;&#26497;&#21644;&#22242;&#20307;&#22806;&#28040;&#26497;&#30340;&#20559;&#35265;&#12290;&#19982;&#20154;&#31867;&#25991;&#26412;&#30456;&#27604;&#65292;&#36825;&#20123;&#27169;&#22411;&#23637;&#31034;&#20102;&#31867;&#20284;&#25110;&#26356;&#22823;&#31243;&#24230;&#30340;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2310.15819</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#31038;&#20250;&#36523;&#20221;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Generative Language Models Exhibit Social Identity Biases. (arXiv:2310.15819v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15819
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;51&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#30340;&#31038;&#20250;&#36523;&#20221;&#20559;&#35265;&#65292;&#21457;&#29616;&#20960;&#20046;&#25152;&#26377;&#27169;&#22411;&#22312;&#34917;&#20840;&#21477;&#23376;&#26102;&#37117;&#23637;&#31034;&#20102;&#26126;&#26174;&#30340;&#22242;&#20307;&#20869;&#31215;&#26497;&#21644;&#22242;&#20307;&#22806;&#28040;&#26497;&#30340;&#20559;&#35265;&#12290;&#19982;&#20154;&#31867;&#25991;&#26412;&#30456;&#27604;&#65292;&#36825;&#20123;&#27169;&#22411;&#23637;&#31034;&#20102;&#31867;&#20284;&#25110;&#26356;&#22823;&#31243;&#24230;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27969;&#34892;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#20174;&#20154;&#31867;&#20013;&#23398;&#21040;&#30340;&#20559;&#35265;&#30340;&#25285;&#24551;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;51&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#23637;&#31034;&#20102;&#31038;&#20250;&#31185;&#23398;&#20013;&#24050;&#30693;&#30340;&#22242;&#20307;&#20869;&#22242;&#32467;&#21644;&#22242;&#20307;&#22806;&#25932;&#23545;&#30340;&#22522;&#26412;&#31038;&#20250;&#20559;&#35265;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20960;&#20046;&#25152;&#26377;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#21644;&#19968;&#20123;&#25351;&#20196;&#32454;&#35843;&#27169;&#22411;&#22312;&#34987;&#35201;&#27714;&#34917;&#20840;&#21477;&#23376;&#65288;&#20363;&#22914;&#65292;&#8220;&#25105;&#20204;&#26159;...&#8221;&#65289;&#26102;&#37117;&#23637;&#31034;&#20102;&#26126;&#26174;&#30340;&#22242;&#20307;&#20869;&#31215;&#26497;&#21644;&#22242;&#20307;&#22806;&#28040;&#26497;&#30340;&#20559;&#35265;&#12290;&#23558;LLM&#29983;&#25104;&#30340;&#21477;&#23376;&#19982;&#20114;&#32852;&#32593;&#19978;&#20154;&#31867;&#25776;&#20889;&#30340;&#21477;&#23376;&#36827;&#34892;&#27604;&#36739;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#23637;&#31034;&#20102;&#19982;&#20154;&#31867;&#25991;&#26412;&#30456;&#20284;&#30340;&#29978;&#33267;&#26356;&#22823;&#31243;&#24230;&#30340;&#20559;&#35265;&#12290;&#20026;&#20102;&#26597;&#26126;&#36825;&#20123;&#20559;&#35265;&#30340;&#26681;&#28304;&#65292;&#25105;&#20204;&#22312;&#32654;&#22269;&#27665;&#20027;&#20826;&#21644;&#20849;&#21644;&#20826;&#20998;&#35010;&#30340;&#32972;&#26223;&#19979;&#23454;&#39564;&#24615;&#22320;&#21464;&#21270;&#20102;&#27169;&#22411;&#22312;&#32454;&#35843;&#36807;&#31243;&#20013;&#26292;&#38706;&#32473;&#22242;&#20307;&#20869;&#31215;&#26497;&#25110;&#22242;&#20307;&#22806;&#28040;&#26497;&#21477;&#23376;&#30340;&#25968;&#37327;&#12290;&#32467;&#26524;&#65292;&#27169;&#22411;&#23637;&#31034;&#20986;&#26126;&#26174;&#30340;&#20559;&#35265;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
The surge in popularity of large language models has given rise to concerns about biases that these models could learn from humans. In this study, we investigate whether ingroup solidarity and outgroup hostility, fundamental social biases known from social science, are present in 51 large language models. We find that almost all foundational language models and some instruction fine-tuned models exhibit clear ingroup-positive and outgroup-negative biases when prompted to complete sentences (e.g., "We are..."). A comparison of LLM-generated sentences with human-written sentences on the internet reveals that these models exhibit similar level, if not greater, levels of bias than human text. To investigate where these biases stem from, we experimentally varied the amount of ingroup-positive or outgroup-negative sentences the model was exposed to during fine-tuning in the context of the United States Democrat-Republican divide. Doing so resulted in the models exhibiting a marked increase i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32771;&#23519;&#20102;&#23545;&#20110;&#38899;&#39057;&#35782;&#21035;&#20219;&#21153;&#65292;&#35757;&#32451;&#21644;&#35780;&#20272;&#20351;&#29992;&#19981;&#21516;&#38271;&#24230;&#30340;&#24207;&#21015;&#23545;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#22823;&#32422;80&#31186;&#30340;&#22768;&#23398;&#19978;&#19979;&#25991;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#30456;&#23545;&#25552;&#39640;14.9%&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.15672</link><description>&lt;p&gt;
&#25105;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;ASR&#31995;&#32479;&#38656;&#35201;&#22810;&#23569;&#19978;&#19979;&#25991;&#20449;&#24687;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Much Context Does My Attention-Based ASR System Need?. (arXiv:2310.15672v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15672
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#23519;&#20102;&#23545;&#20110;&#38899;&#39057;&#35782;&#21035;&#20219;&#21153;&#65292;&#35757;&#32451;&#21644;&#35780;&#20272;&#20351;&#29992;&#19981;&#21516;&#38271;&#24230;&#30340;&#24207;&#21015;&#23545;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#22823;&#32422;80&#31186;&#30340;&#22768;&#23398;&#19978;&#19979;&#25991;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#30456;&#23545;&#25552;&#39640;14.9%&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35821;&#38899;&#35782;&#21035;&#20219;&#21153;&#65292;&#20351;&#29992;&#36229;&#36807;30&#31186;&#30340;&#22768;&#23398;&#19978;&#19979;&#25991;&#36827;&#34892;&#35757;&#32451;&#22312;&#25991;&#29486;&#20013;&#26159;&#19981;&#24120;&#35265;&#30340;&#65292;&#24182;&#19988;&#24471;&#21040;&#20102;&#24456;&#23569;&#30340;&#30740;&#31350;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35757;&#32451;/&#35780;&#20272;&#65288;&#22522;&#20110;&#23494;&#38598;&#27880;&#24847;&#21147;&#30340;&#65289;&#22768;&#23398;&#27169;&#22411;&#21644;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#24207;&#21015;&#38271;&#24230;&#30340;&#32553;&#25918;&#23545;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#22823;&#32422;100,000&#20010;&#20266;&#26631;&#35760;&#30340;Spotify&#25773;&#23458;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#36825;&#20123;&#23454;&#39564;&#65292;&#25506;&#32034;&#20102;5&#31186;&#21040;1&#23567;&#26102;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#12290;&#23545;&#38271;&#26684;&#24335;&#25968;&#25454;&#38598;Earnings-22&#21644;Tedlium&#36827;&#34892;&#20102;&#38646;-shot&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#20351;&#29992;&#22823;&#32422;80&#31186;&#30340;&#22768;&#23398;&#19978;&#19979;&#25991;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#24102;&#26469;&#39640;&#36798;14.9%&#30456;&#23545;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#26463;&#25628;&#32034;&#20351;&#29992;&#38271;&#19978;&#19979;&#25991;&#21464;&#25442;&#35821;&#35328;&#27169;&#22411;&#19982;&#31995;&#32479;&#32452;&#21512;&#24418;&#25104;&#20102;&#19968;&#20010;&#20840;&#38271;&#19978;&#19979;&#25991;ASR&#31995;&#32479;&#65292;&#20854;&#32467;&#26524;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
For the task of speech recognition, the use of more than 30 seconds of acoustic context during training is uncommon, and under-investigated in literature. In this work, we examine the effect of scaling the sequence length used to train/evaluate (dense-attention based) acoustic and language models on speech recognition performance. For these experiments a dataset of roughly 100,000 pseudo-labelled Spotify podcasts is used, with context lengths of 5 seconds to 1 hour being explored. Zero-shot evaluations on long-format datasets Earnings-22 and Tedlium demonstrate a benefit from training with around 80 seconds of acoustic context, showing up to a 14.9% relative improvement from a limited context baseline. Furthermore, we perform a system combination with long-context transformer language models via beam search for a fully long-context ASR system, with results that are competitive with the current state-of-the-art.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26159;&#39318;&#27425;&#25506;&#32034;&#20102;&#22312;&#35821;&#38899;&#22788;&#29702;&#20013;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#30340;&#21487;&#33021;&#24615;&#65292;&#36890;&#36807;&#22312;&#36755;&#20837;&#20013;&#21576;&#29616;LM&#35805;&#35821;-&#26631;&#31614;&#31034;&#33539;&#65292;&#35821;&#38899;LM&#21487;&#20197;&#22312;&#27809;&#26377;&#25991;&#26412;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#39564;&#35777;&#20102;&#22312;&#35821;&#38899;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;ICL&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12477</link><description>&lt;p&gt;
&#23545;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#36827;&#34892;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
An Exploration of In-Context Learning for Speech Language Model. (arXiv:2310.12477v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26159;&#39318;&#27425;&#25506;&#32034;&#20102;&#22312;&#35821;&#38899;&#22788;&#29702;&#20013;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#30340;&#21487;&#33021;&#24615;&#65292;&#36890;&#36807;&#22312;&#36755;&#20837;&#20013;&#21576;&#29616;LM&#35805;&#35821;-&#26631;&#31614;&#31034;&#33539;&#65292;&#35821;&#38899;LM&#21487;&#20197;&#22312;&#27809;&#26377;&#25991;&#26412;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#39564;&#35777;&#20102;&#22312;&#35821;&#38899;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;ICL&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;GPT-3&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#21457;&#23637;&#20197;&#26469;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#22312;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26041;&#38754;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#36890;&#36807;&#22312;&#36755;&#20837;&#20013;&#21576;&#29616;LM&#35805;&#35821;-&#26631;&#31614;&#31034;&#33539;&#65292;LM&#21487;&#20197;&#22312;&#19981;&#20381;&#36182;&#26799;&#24230;&#19979;&#38477;&#25110;&#35201;&#27714;&#26174;&#24335;&#20462;&#25913;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#36825;&#20351;&#24471;LM&#33021;&#20197;&#40657;&#30418;&#30340;&#26041;&#24335;&#23398;&#20064;&#21644;&#35843;&#25972;&#12290;&#23613;&#31649;ICL&#22312;NLP&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22312;&#35821;&#38899;&#22788;&#29702;&#39046;&#22495;&#65292;&#24456;&#23569;&#26377;&#20154;&#30740;&#31350;ICL&#30340;&#21487;&#33021;&#24615;&#12290;&#26412;&#30740;&#31350;&#39318;&#27425;&#22312;&#27809;&#26377;&#25991;&#26412;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#25552;&#20986;&#20102;&#23545;&#35821;&#38899;LM&#30340;ICL&#30340;&#25506;&#32034;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#24403;&#21069;&#30340;&#35821;&#38899;LM&#27809;&#26377;ICL&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;&#28909;&#36523;&#35757;&#32451;&#65292;&#35821;&#38899;LM&#22240;&#27492;&#21487;&#20197;&#22312;&#26410;&#30693;&#20219;&#21153;&#19978;&#25191;&#34892;ICL&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#23545;&#35821;&#38899;LM&#22312;&#35821;&#38899;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;ICL&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ever since the development of GPT-3 in the natural language processing (NLP) field, in-context learning (ICL) has played an important role in utilizing large language models (LLMs). By presenting the LM utterance-label demonstrations at the input, the LM can accomplish few-shot learning without relying on gradient descent or requiring explicit modification of its parameters. This enables the LM to learn and adapt in a black-box manner. Despite the success of ICL in NLP, little work is exploring the possibility of ICL in speech processing. This study proposes the first exploration of ICL with a speech LM without text supervision. We first show that the current speech LM does not have the ICL capability. With the proposed warmup training, the speech LM can, therefore, perform ICL on unseen tasks. In this work, we verify the feasibility of ICL for speech LM on speech classification tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#38454;&#27573;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#20256;&#32479;&#35821;&#35328;&#27169;&#22411;&#37325;&#26032;&#35780;&#20998;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#65292;&#22312;&#35821;&#38899;&#35782;&#21035;&#20013;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.11532</link><description>&lt;p&gt;
&#22810;&#38454;&#27573;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32416;&#38169;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Multi-stage Large Language Model Correction for Speech Recognition. (arXiv:2310.11532v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11532
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#38454;&#27573;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#20256;&#32479;&#35821;&#35328;&#27169;&#22411;&#37325;&#26032;&#35780;&#20998;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#65292;&#22312;&#35821;&#38899;&#35782;&#21035;&#20013;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#25913;&#36827;&#31454;&#20105;&#24615;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#19982;&#20256;&#32479;&#35821;&#35328;&#27169;&#22411;&#19987;&#27880;&#20110;&#21333;&#19968;&#25968;&#25454;&#39046;&#22495;&#19981;&#21516;&#65292;LLMs&#30340;&#23835;&#36215;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#26426;&#20250;&#65292;&#26082;&#33021;&#25512;&#21160;&#26368;&#20808;&#36827;&#30340;ASR&#24615;&#33021;&#30340;&#26497;&#38480;&#65292;&#21448;&#33021;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#23454;&#29616;&#26356;&#39640;&#30340;&#40065;&#26834;&#24615;&#21644;&#26377;&#25928;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#38454;&#27573;&#26041;&#27861;&#65292;&#23558;&#20256;&#32479;&#35821;&#35328;&#27169;&#22411;&#37325;&#26032;&#35780;&#20998;&#21644;LLM&#25552;&#31034;&#30456;&#32467;&#21512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#26041;&#27861;&#26377;&#20004;&#20010;&#38454;&#27573;&#65306;&#31532;&#19968;&#38454;&#27573;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#23545;ASR&#20551;&#35774;&#30340;N&#20010;&#26368;&#20339;&#21015;&#34920;&#36827;&#34892;&#37325;&#26032;&#35780;&#20998;&#65292;&#24182;&#36827;&#34892;&#32622;&#20449;&#24230;&#26816;&#26597;&#65307;&#31532;&#20108;&#38454;&#27573;&#20351;&#29992;&#25552;&#31034;&#23545;&#31532;&#19968;&#38454;&#27573;&#32622;&#20449;&#24230;&#36739;&#20302;&#30340;&#32467;&#26524;&#36827;&#34892;ASR&#38169;&#35823;&#20462;&#27491;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#30456;&#23545;&#20110;&#31454;&#20105;&#24615;ASR&#31995;&#32479;&#65292;&#22312;WER&#19978;&#21462;&#24471;&#20102;10%~20%&#30340;&#30456;&#23545;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the usage of large language models (LLMs) to improve the performance of competitive speech recognition systems. Different from traditional language models that focus on one single data domain, the rise of LLMs brings us the opportunity to push the limit of state-of-the-art ASR performance, and at the same time to achieve higher robustness and generalize effectively across multiple domains. Motivated by this, we propose a novel multi-stage approach to combine traditional language model re-scoring and LLM prompting. Specifically, the proposed method has two stages: the first stage uses a language model to re-score an N-best list of ASR hypotheses and run a confidence check; The second stage uses prompts to a LLM to perform ASR error correction on less confident results from the first stage. Our experimental results demonstrate the effectiveness of the proposed method by showing a 10% ~ 20% relative improvement in WER over a competitive ASR system -- across m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24212;&#29992;&#32467;&#26500;&#21270;&#35748;&#30693;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#30340;&#28508;&#22312;&#22909;&#22788;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#24230;&#32467;&#26500;&#21270;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;&#38543;&#26426;&#21270;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.09518</link><description>&lt;p&gt;
&#20154;&#31867;&#35838;&#31243;&#25351;&#23548;&#19979;&#30340;&#25351;&#20196;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Instruction Tuning with Human Curriculum. (arXiv:2310.09518v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24212;&#29992;&#32467;&#26500;&#21270;&#35748;&#30693;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#30340;&#28508;&#22312;&#22909;&#22788;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#24230;&#32467;&#26500;&#21270;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;&#38543;&#26426;&#21270;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#30340;&#20027;&#27969;&#33539;&#24335;&#26159;&#38543;&#26426;&#27927;&#29260;&#35757;&#32451;&#26368;&#22823;&#22810;&#26679;&#21270;&#25351;&#20196;-&#21709;&#24212;&#23545;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#24403;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;ChatGPT&#21644;GPT-4&#20013;&#24212;&#29992;&#32467;&#26500;&#21270;&#35748;&#30693;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#30340;&#28508;&#22312;&#22909;&#22788;&#12290;&#19982;&#20197;&#24448;&#20256;&#32479;&#30340;&#38543;&#26426;&#21270;&#25351;&#20196;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#24230;&#32467;&#26500;&#21270;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#27169;&#25311;&#20102;&#20154;&#31867;&#25945;&#32946;&#30340;&#28176;&#36827;&#24615;&#21644;&#26377;&#32452;&#32455;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#25968;&#25454;&#38598;&#19982;&#25945;&#32946;&#26694;&#26550;&#23545;&#40784;&#26469;&#31574;&#21010;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#65292;&#20026;&#27599;&#20010;&#26679;&#26412;&#21253;&#25324;&#20027;&#39064;&#21644;&#35748;&#30693;&#20005;&#35880;&#31243;&#24230;&#31561;&#20803;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#20174;&#20013;&#23398;&#21040;&#30740;&#31350;&#29983;&#38454;&#27573;&#30340;&#20840;&#38754;&#32454;&#31890;&#24230;&#20027;&#39064;&#65292;&#27599;&#20010;&#20027;&#39064;&#37117;&#26377;&#21508;&#31181;&#38382;&#39064;&#65292;&#20197;&#21033;&#29992;&#24067;&#40065;&#22982;&#30340;&#35748;&#30693;&#20998;&#32423;&#27861;&#25552;&#39640;&#27010;&#24565;&#28145;&#24230;&#65292;&#35813;&#20998;&#32423;&#27861;&#29992;&#20110;&#21306;&#20998;&#27599;&#20010;&#27010;&#24565;&#30340;&#19981;&#21516;&#20154;&#31867;&#35748;&#30693;&#27700;&#24179;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#35748;&#30693;&#23398;&#20064;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;&#38543;&#26426;&#21270;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dominant paradigm for instruction tuning is the random-shuffled training of maximally diverse instruction-response pairs. This paper explores the potential benefits of applying a structured cognitive learning approach to instruction tuning in contemporary large language models like ChatGPT and GPT-4. Unlike the previous conventional randomized instruction dataset, we propose a highly structured synthetic dataset that mimics the progressive and organized nature of human education. We curate our dataset by aligning it with educational frameworks, incorporating meta information including its topic and cognitive rigor level for each sample. Our dataset covers comprehensive fine-grained topics spanning diverse educational stages (from middle school to graduate school) with various questions for each topic to enhance conceptual depth using Bloom's taxonomy-a classification framework distinguishing various levels of human cognition for each concept. The results demonstrate that this cogni
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BMBI&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#22810;&#36873;&#38382;&#39064;&#22238;&#31572;&#27169;&#22411;&#30340;&#20559;&#35265;&#12290;&#36890;&#36807;&#35266;&#23519;&#19968;&#20010;&#26597;&#35810;&#23454;&#20363;&#23545;&#21478;&#19968;&#20010;&#23454;&#20363;&#30340;&#24433;&#21709;&#65292;&#27979;&#37327;&#26597;&#35810;&#23454;&#20363;&#30340;&#20559;&#35265;&#31243;&#24230;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#20248;&#21270;&#30446;&#26631;&#65292;&#24418;&#25104;&#19968;&#20010;&#22810;&#20219;&#21153;&#23398;&#20064;&#35774;&#32622;&#12290;&#21516;&#26102;&#24341;&#20837;&#26032;&#30340;&#20559;&#35265;&#35780;&#20272;&#25351;&#26631;&#20197;&#37327;&#21270;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2310.08795</link><description>&lt;p&gt;
&#36890;&#36807;&#36861;&#36394;&#20559;&#35265;&#24433;&#21709;&#26469;&#20943;&#36731;&#38382;&#39064;&#22238;&#31572;&#27169;&#22411;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Mitigating Bias for Question Answering Models by Tracking Bias Influence. (arXiv:2310.08795v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BMBI&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#22810;&#36873;&#38382;&#39064;&#22238;&#31572;&#27169;&#22411;&#30340;&#20559;&#35265;&#12290;&#36890;&#36807;&#35266;&#23519;&#19968;&#20010;&#26597;&#35810;&#23454;&#20363;&#23545;&#21478;&#19968;&#20010;&#23454;&#20363;&#30340;&#24433;&#21709;&#65292;&#27979;&#37327;&#26597;&#35810;&#23454;&#20363;&#30340;&#20559;&#35265;&#31243;&#24230;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#20248;&#21270;&#30446;&#26631;&#65292;&#24418;&#25104;&#19968;&#20010;&#22810;&#20219;&#21153;&#23398;&#20064;&#35774;&#32622;&#12290;&#21516;&#26102;&#24341;&#20837;&#26032;&#30340;&#20559;&#35265;&#35780;&#20272;&#25351;&#26631;&#20197;&#37327;&#21270;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#35777;&#26126;&#21508;&#31181;NLP&#20219;&#21153;&#30340;&#27169;&#22411;&#23384;&#22312;&#21051;&#26495;&#21360;&#35937;&#65292;&#32780;&#38382;&#39064;&#22238;&#31572;&#65288;QA&#65289;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#23588;&#20854;&#26377;&#23475;&#65292;&#22240;&#20026;&#36755;&#20986;&#30340;&#31572;&#26696;&#21487;&#33021;&#30452;&#25509;&#34987;&#26368;&#32456;&#29992;&#25143;&#20351;&#29992;&#12290;&#24050;&#32463;&#26377;&#25968;&#25454;&#38598;&#29992;&#20110;&#35780;&#20272;QA&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#65292;&#20294;&#26159;&#23545;&#20110;QA&#27169;&#22411;&#30340;&#20559;&#35265;&#32531;&#35299;&#25216;&#26415;&#20173;&#22788;&#20110;&#25506;&#32034;&#38454;&#27573;&#12290;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BMBI&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#32531;&#35299;&#22810;&#36873;&#38382;&#39064;&#22238;&#31572;&#27169;&#22411;&#30340;&#20559;&#35265;&#12290;&#22522;&#20110;&#19968;&#20010;&#30452;&#35273;&#65292;&#21363;&#22914;&#26524;&#19968;&#20010;&#27169;&#22411;&#20174;&#19968;&#20010;&#26377;&#20559;&#35265;&#30340;&#20363;&#23376;&#20013;&#23398;&#21040;&#20102;&#19996;&#35199;&#65292;&#23427;&#21487;&#33021;&#26356;&#23481;&#26131;&#20986;&#29616;&#20559;&#35265;&#65292;&#25105;&#20204;&#36890;&#36807;&#35266;&#23519;&#19968;&#20010;&#26597;&#35810;&#23454;&#20363;&#23545;&#21478;&#19968;&#20010;&#23454;&#20363;&#30340;&#24433;&#21709;&#26469;&#34913;&#37327;&#26597;&#35810;&#23454;&#20363;&#30340;&#20559;&#35265;&#31243;&#24230;&#12290;&#22914;&#26524;&#21463;&#21040;&#24433;&#21709;&#30340;&#23454;&#20363;&#26356;&#20559;&#35265;&#65292;&#25105;&#20204;&#35748;&#20026;&#26597;&#35810;&#23454;&#20363;&#26159;&#26377;&#20559;&#35265;&#30340;&#12290;&#25105;&#20204;&#20351;&#29992;&#26816;&#27979;&#21040;&#30340;&#20559;&#35265;&#31243;&#24230;&#20316;&#20026;&#20248;&#21270;&#30446;&#26631;&#65292;&#24418;&#25104;&#19968;&#20010;&#22810;&#20219;&#21153;&#23398;&#20064;&#35774;&#32622;&#65292;&#38500;&#20102;&#21407;&#26469;&#30340;QA&#20219;&#21153;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20559;&#35265;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#20840;&#38754;&#32780;&#25935;&#24863;&#30340;&#26041;&#24335;&#37327;&#21270;&#20559;&#35265;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#20943;&#36731;QA&#27169;&#22411;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models of various NLP tasks have been shown to exhibit stereotypes, and the bias in the question answering (QA) models is especially harmful as the output answers might be directly consumed by the end users. There have been datasets to evaluate bias in QA models, while bias mitigation technique for the QA models is still under-explored. In this work, we propose BMBI, an approach to mitigate the bias of multiple-choice QA models. Based on the intuition that a model would lean to be more biased if it learns from a biased example, we measure the bias level of a query instance by observing its influence on another instance. If the influenced instance is more biased, we derive that the query instance is biased. We then use the bias level detected as an optimization objective to form a multi-task learning setting in addition to the original QA task. We further introduce a new bias evaluation metric to quantify bias in a comprehensive and sensitive way. We show that our method could be applie
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#24605;&#32500;&#20256;&#25773;&#65288;TP&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#31867;&#27604;&#38382;&#39064;&#21644;&#21033;&#29992;&#31867;&#27604;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22797;&#26434;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.03965</link><description>&lt;p&gt;
&#24605;&#32500;&#20256;&#25773;&#65306;&#19968;&#31181;&#36890;&#36807;&#31867;&#27604;&#26041;&#27861;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22797;&#26434;&#25512;&#29702;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Thought Propagation: An Analogical Approach to Complex Reasoning with Large Language Models. (arXiv:2310.03965v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03965
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#24605;&#32500;&#20256;&#25773;&#65288;TP&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#31867;&#27604;&#38382;&#39064;&#21644;&#21033;&#29992;&#31867;&#27604;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22797;&#26434;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#29616;&#26377;&#30340;&#25552;&#31034;&#26041;&#27861;&#26080;&#27861;&#37325;&#29992;&#35299;&#20915;&#31867;&#20284;&#38382;&#39064;&#30340;&#35265;&#35299;&#65292;&#24182;&#19988;&#22312;&#22810;&#27493;&#25512;&#29702;&#20013;&#32047;&#31215;&#20102;&#38169;&#35823;&#65292;&#22240;&#20026;&#23427;&#20204;&#35201;&#27714;LLMs&#20174;&#38646;&#24320;&#22987;&#25512;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#24605;&#32500;&#20256;&#25773;&#8221;&#65288;TP&#65289;&#65292;&#23427;&#25506;&#32034;&#31867;&#20284;&#38382;&#39064;&#24182;&#21033;&#29992;&#23427;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#22686;&#24378;LLMs&#30340;&#22797;&#26434;&#25512;&#29702;&#33021;&#21147;&#12290;&#36825;&#20123;&#31867;&#27604;&#38382;&#39064;&#19982;&#36755;&#20837;&#38382;&#39064;&#30456;&#20851;&#65292;&#20855;&#26377;&#21487;&#37325;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#21644;&#38382;&#39064;&#35299;&#20915;&#31574;&#30053;&#12290;&#22240;&#27492;&#65292;&#23558;&#35299;&#20915;&#20808;&#21069;&#31867;&#20284;&#38382;&#39064;&#30340;&#35265;&#35299;&#20256;&#25773;&#20197;&#28608;&#21457;&#26032;&#30340;&#38382;&#39064;&#35299;&#20915;&#26159;&#26377;&#24076;&#26395;&#30340;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;TP&#39318;&#20808;&#25552;&#31034;LLMs&#25552;&#20986;&#24182;&#35299;&#20915;&#19968;&#32452;&#19982;&#36755;&#20837;&#38382;&#39064;&#30456;&#20851;&#30340;&#31867;&#27604;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;TP&#37325;&#29992;&#31867;&#27604;&#38382;&#39064;&#30340;&#32467;&#26524;&#30452;&#25509;&#20135;&#29983;&#19968;&#20010;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#25110;&#32773;&#25512;&#23548;&#19968;&#20010;&#30693;&#35782;&#23494;&#38598;&#22411;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have achieved remarkable success in reasoning tasks with the development of prompting methods. However, existing prompting approaches cannot reuse insights of solving similar problems and suffer from accumulated errors in multi-step reasoning, since they prompt LLMs to reason \textit{from scratch}. To address these issues, we propose \textbf{\textit{Thought Propagation} (TP)}, which explores the analogous problems and leverages their solutions to enhance the complex reasoning ability of LLMs. These analogous problems are related to the input one, with reusable solutions and problem-solving strategies. Thus, it is promising to propagate insights of solving previous analogous problems to inspire new problem-solving. To achieve this, TP first prompts LLMs to propose and solve a set of analogous problems that are related to the input one. Then, TP reuses the results of analogous problems to directly yield a new solution or derive a knowledge-intensive plan for 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#37492;&#21035;-&#25209;&#21028;&#24046;&#36317;&#27979;&#37327;LLMs&#23545;&#20154;&#31867;&#20215;&#20540;&#30340;&#29702;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20215;&#20540;&#29702;&#35299;&#27979;&#37327;&#65288;VUM&#65289;&#26694;&#26550;&#65292;&#24182;&#20351;&#29992;GPT-4&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#21547;&#19968;&#21315;&#20010;&#23545;&#35805;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#23610;&#24230;&#23450;&#24459;&#23545;LLMs&#30340;&#8220;&#30693;&#36947;&#20160;&#20040;&#8221;&#26377;&#36739;&#22823;&#24433;&#21709;&#65292;&#32780;&#23545;&#8220;&#30693;&#36947;&#20026;&#20160;&#20040;&#8221;&#24433;&#21709;&#36739;&#23567;&#12290;</title><link>http://arxiv.org/abs/2310.00378</link><description>&lt;p&gt;
&#36890;&#36807;&#37492;&#21035;-&#25209;&#21028;&#24046;&#36317;&#27979;&#37327;&#35821;&#35328;&#27169;&#22411;&#23545;&#20215;&#20540;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Measuring Value Understanding in Language Models through Discriminator-Critique Gap. (arXiv:2310.00378v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00378
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#37492;&#21035;-&#25209;&#21028;&#24046;&#36317;&#27979;&#37327;LLMs&#23545;&#20154;&#31867;&#20215;&#20540;&#30340;&#29702;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20215;&#20540;&#29702;&#35299;&#27979;&#37327;&#65288;VUM&#65289;&#26694;&#26550;&#65292;&#24182;&#20351;&#29992;GPT-4&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#21547;&#19968;&#21315;&#20010;&#23545;&#35805;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#23610;&#24230;&#23450;&#24459;&#23545;LLMs&#30340;&#8220;&#30693;&#36947;&#20160;&#20040;&#8221;&#26377;&#36739;&#22823;&#24433;&#21709;&#65292;&#32780;&#23545;&#8220;&#30693;&#36947;&#20026;&#20160;&#20040;&#8221;&#24433;&#21709;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#24341;&#21457;&#20102;&#23545;&#23427;&#20204;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#20043;&#38388;&#28508;&#22312;&#19981;&#19968;&#33268;&#24615;&#30340;&#25285;&#24551;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#30340;&#22797;&#26434;&#21644;&#36866;&#24212;&#24615;&#65292;&#35780;&#20272;&#23427;&#20204;&#23545;&#36825;&#20123;&#20215;&#20540;&#35266;&#30340;&#29702;&#35299;&#26159;&#22797;&#26434;&#30340;&#12290;&#25105;&#20204;&#35748;&#20026;&#30495;&#27491;&#29702;&#35299;LLMs&#20013;&#30340;&#20215;&#20540;&#35266;&#38656;&#35201;&#32771;&#34385;&#21040;&#8220;&#30693;&#36947;&#20160;&#20040;&#8221;&#21644;&#8220;&#30693;&#36947;&#20026;&#20160;&#20040;&#8221;&#20004;&#20010;&#26041;&#38754;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20215;&#20540;&#29702;&#35299;&#27979;&#37327;&#65288;VUM&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#37327;&#21270;&#37492;&#21035;-&#25209;&#21028;&#24046;&#36317;&#26469;&#23450;&#37327;&#35780;&#20272;&#8220;&#30693;&#36947;&#20160;&#20040;&#8221;&#21644;&#8220;&#30693;&#36947;&#20026;&#20160;&#20040;&#8221;&#12290;&#21033;&#29992;&#26045;&#29926;&#33576;&#20215;&#20540;&#35266;&#35843;&#26597;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#35780;&#20272;&#20215;&#20540;&#35266;&#30340;&#26631;&#20934;&#65292;&#24182;&#20351;&#29992;GPT-4&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#21547;&#19968;&#21315;&#20010;&#23545;&#35805;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32771;&#23519;&#20102;LLMs&#30340;&#36755;&#20986;&#19982;&#22522;&#20934;&#31572;&#26696;&#20043;&#38388;&#30340;&#20215;&#20540;&#35266;&#19968;&#33268;&#24615;&#65292;&#20197;&#21450;LLMs&#30340;&#22238;&#31572;&#19982;GPT-4&#30340;&#27880;&#37322;&#22312;&#20215;&#20540;&#35748;&#30693;&#21407;&#22240;&#19978;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20116;&#20010;&#20195;&#34920;&#24615;LLMs&#65292;&#24182;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#35777;&#25454;&#34920;&#26126;&#65292;&#23610;&#24230;&#23450;&#24459;&#23545;&#8220;&#30693;&#36947;&#20160;&#20040;&#8221;&#30340;&#24433;&#21709;&#36739;&#22823;&#65292;&#20294;&#23545;&#8220;&#30693;&#36947;&#20026;&#20160;&#20040;&#8221;&#30340;&#24433;&#21709;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in Large Language Models (LLMs) have heightened concerns about their potential misalignment with human values. However, evaluating their grasp of these values is complex due to their intricate and adaptable nature. We argue that truly understanding values in LLMs requires considering both "know what" and "know why". To this end, we present the Value Understanding Measurement (VUM) framework that quantitatively assess both "know what" and "know why" by measuring the discriminator-critique gap related to human values. Using the Schwartz Value Survey, we specify our evaluation values and develop a thousand-level dialogue dataset with GPT-4. Our assessment looks at both the value alignment of LLM's outputs compared to baseline answers and how LLM responses align with reasons for value recognition versus GPT-4's annotations. We evaluate five representative LLMs and provide strong evidence that the scaling law significantly impacts "know what" but not much on "know why", 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#37325;&#26032;&#35774;&#35745;&#30340;&#20219;&#21153;&#21644;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20934;&#65292;&#36890;&#36807;&#24341;&#20837;CA Multimodal Evaluation for Ad Text GeneRAtion (CAMERA)&#25968;&#25454;&#38598;&#65292;&#25512;&#21160;&#20102;&#24191;&#21578;&#25991;&#26412;&#29983;&#25104;&#39046;&#22495;&#30340;&#21457;&#23637;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;&#22522;&#20934;&#20013;&#30340;&#32570;&#38519;&#12290;</title><link>http://arxiv.org/abs/2309.12030</link><description>&lt;p&gt;
CAMERA&#65306;&#24191;&#21578;&#25991;&#26412;&#29983;&#25104;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
CAMERA: A Multimodal Dataset and Benchmark for Ad Text Generation. (arXiv:2309.12030v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#37325;&#26032;&#35774;&#35745;&#30340;&#20219;&#21153;&#21644;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20934;&#65292;&#36890;&#36807;&#24341;&#20837;CA Multimodal Evaluation for Ad Text GeneRAtion (CAMERA)&#25968;&#25454;&#38598;&#65292;&#25512;&#21160;&#20102;&#24191;&#21578;&#25991;&#26412;&#29983;&#25104;&#39046;&#22495;&#30340;&#21457;&#23637;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;&#22522;&#20934;&#20013;&#30340;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21709;&#24212;&#25163;&#21160;&#22312;&#32447;&#24191;&#21578;&#21046;&#20316;&#30340;&#38480;&#21046;&#26102;&#65292;&#24050;&#32463;&#22312;&#33258;&#21160;&#24191;&#21578;&#25991;&#26412;&#29983;&#25104;&#65288;ATG&#65289;&#39046;&#22495;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#28085;&#30422;&#25972;&#20010;&#39046;&#22495;&#30340;&#22522;&#20934;&#20197;&#21450;&#27809;&#26377;&#26126;&#30830;&#23450;&#20041;&#30340;&#38382;&#39064;&#38598;&#21644;&#28165;&#26224;&#30340;&#27169;&#22411;&#36755;&#20837;&#21644;&#36755;&#20986;&#65292;&#27604;&#36739;&#19981;&#21516;&#26041;&#27861;&#19968;&#30452;&#26159;&#19968;&#39033;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#37325;&#26032;&#35774;&#35745;&#30340;&#20219;&#21153;&#21644;&#26500;&#24314;&#19968;&#20010;&#22522;&#20934;&#26469;&#25512;&#21160;ATG&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;ATG&#23450;&#20041;&#20026;&#19968;&#20010;&#36328;&#24212;&#29992;&#20219;&#21153;&#65292;&#28085;&#30422;&#20114;&#32852;&#32593;&#24191;&#21578;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;&#20316;&#20026;&#25105;&#20204;&#30340;&#36129;&#29486;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31532;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21517;&#20026;CA Multimodal Evaluation for Ad Text GeneRAtion&#65288;CAMERA&#65289;&#65292;&#20026;ATG&#31934;&#24515;&#35774;&#35745;&#65292;&#21487;&#20197;&#21033;&#29992;&#22810;&#27169;&#24577;&#20449;&#24687;&#24182;&#36827;&#34892;&#34892;&#19994;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22810;&#20010;&#22522;&#32447;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#20934;&#30340;&#26377;&#29992;&#24615;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#26415;&#35821;&#19978;&#26377;&#25152;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
In response to the limitations of manual online ad production, significant research has been conducted in the field of automatic ad text generation (ATG). However, comparing different methods has been challenging because of the lack of benchmarks encompassing the entire field and the absence of well-defined problem sets with clear model inputs and outputs. To address these challenges, this paper aims to advance the field of ATG by introducing a redesigned task and constructing a benchmark. Specifically, we defined ATG as a cross-application task encompassing various aspects of the Internet advertising. As part of our contribution, we propose a first benchmark dataset, CA Multimodal Evaluation for Ad Text GeneRAtion (CAMERA), carefully designed for ATG to be able to leverage multi-modal information and conduct an industry-wise evaluation. Furthermore, we demonstrate the usefulness of our proposed benchmark through evaluation experiments using multiple baseline models, which vary in term
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20351;&#29992;&#27169;&#22411;&#33976;&#39311;&#22686;&#24378;&#21644;&#25913;&#36827;&#27491;&#24335;&#39118;&#26684;&#36716;&#31227;&#25968;&#25454;&#38598;&#30340;&#35299;&#37322;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#19987;&#23478;&#21453;&#39304;&#36827;&#19968;&#27493;&#20248;&#21270;&#29983;&#25104;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2309.08583</link><description>&lt;p&gt;
ICLEF: &#22522;&#20110;&#19987;&#23478;&#21453;&#39304;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#29992;&#20110;&#21487;&#35299;&#37322;&#39118;&#26684;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
ICLEF: In-Context Learning with Expert Feedback for Explainable Style Transfer. (arXiv:2309.08583v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08583
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20351;&#29992;&#27169;&#22411;&#33976;&#39311;&#22686;&#24378;&#21644;&#25913;&#36827;&#27491;&#24335;&#39118;&#26684;&#36716;&#31227;&#25968;&#25454;&#38598;&#30340;&#35299;&#37322;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#19987;&#23478;&#21453;&#39304;&#36827;&#19968;&#27493;&#20248;&#21270;&#29983;&#25104;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#39118;&#26684;&#36716;&#31227;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#24403;&#21069;&#30340;&#24037;&#20316;&#27809;&#26377;&#35299;&#20915;&#39118;&#26684;&#36716;&#31227;&#31995;&#32479;&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;GPT-3.5&#21644;GPT-4&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#29983;&#25104;&#35299;&#37322;&#65292;&#20294;&#26159;&#24403;&#23384;&#22312;&#26356;&#23567;&#12289;&#24191;&#27867;&#20998;&#24067;&#19988;&#36879;&#26126;&#30340;&#26367;&#20195;&#21697;&#26102;&#65292;&#20351;&#29992;&#36825;&#26679;&#22797;&#26434;&#30340;&#31995;&#32479;&#26159;&#20302;&#25928;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#20174;ChatGPT&#20013;&#36827;&#34892;&#27169;&#22411;&#33976;&#39311;&#26469;&#22686;&#24378;&#21644;&#25913;&#36827;&#19968;&#20010;&#27491;&#24335;&#39118;&#26684;&#36716;&#31227;&#25968;&#25454;&#38598;&#30340;&#35299;&#37322;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25913;&#21892;&#29983;&#25104;&#30340;&#35299;&#37322;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#24335;&#65292;&#21363;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICLEF:&#22522;&#20110;&#19987;&#23478;&#21453;&#39304;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65289;&#65292;&#20351;ChatGPT&#20805;&#24403;&#20854;&#33258;&#36523;&#36755;&#20986;&#30340;&#35780;&#35770;&#32773;&#12290;&#25105;&#20204;&#20351;&#29992;&#21253;&#21547;9960&#20010;&#21487;&#35299;&#37322;&#30340;&#27491;&#24335;&#39118;&#26684;&#36716;&#31227;&#23454;&#20363;&#65288;e-GYAFC&#65289;&#30340;&#25968;&#25454;&#38598;&#26469;&#23637;&#31034;&#24403;&#21069;&#20844;&#24320;&#20998;&#21457;&#30340;&#32463;&#36807;&#25351;&#23548;&#30340;&#27169;&#22411;&#65288;&#22312;&#26576;&#20123;&#35774;&#32622;&#20013;&#21253;&#25324;ChatGPT&#65289;&#22312;&#35813;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#24182;&#19988;&#22312;&#25105;&#20204;&#30340;&#39640;-
&lt;/p&gt;
&lt;p&gt;
While state-of-the-art language models excel at the style transfer task, current work does not address explainability of style transfer systems. Explanations could be generated using large language models such as GPT-3.5 and GPT-4, but the use of such complex systems is inefficient when smaller, widely distributed, and transparent alternatives are available. We propose a framework to augment and improve a formality style transfer dataset with explanations via model distillation from ChatGPT. To further refine the generated explanations, we propose a novel way to incorporate scarce expert human feedback using in-context learning (ICLEF: In-Context Learning from Expert Feedback) by prompting ChatGPT to act as a critic to its own outputs. We use the resulting dataset of 9,960 explainable formality style transfer instances (e-GYAFC) to show that current openly distributed instruction-tuned models (and, in some settings, ChatGPT) perform poorly on the task, and that fine-tuning on our high-
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36719;&#25552;&#31034;&#35843;&#20248;&#26469;&#22686;&#24378;&#23494;&#38598;&#26816;&#32034;&#30340;&#26041;&#27861;&#65288;SPTAR&#65289;&#12290;&#36890;&#36807;&#20248;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#36719;&#25552;&#31034;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#26410;&#26631;&#35760;&#30340;&#25991;&#26723;&#29983;&#25104;&#24369;&#26597;&#35810;&#65292;&#21487;&#20197;&#25552;&#39640;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.08303</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#23494;&#38598;&#26816;&#32034;&#30340;&#36719;&#25552;&#31034;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Soft Prompt Tuning for Augmenting Dense Retrieval with Large Language Models. (arXiv:2307.08303v1 [cs.IR] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36719;&#25552;&#31034;&#35843;&#20248;&#26469;&#22686;&#24378;&#23494;&#38598;&#26816;&#32034;&#30340;&#26041;&#27861;&#65288;SPTAR&#65289;&#12290;&#36890;&#36807;&#20248;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#36719;&#25552;&#31034;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#26410;&#26631;&#35760;&#30340;&#25991;&#26723;&#29983;&#25104;&#24369;&#26597;&#35810;&#65292;&#21487;&#20197;&#25552;&#39640;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23494;&#38598;&#26816;&#32034;&#65288;DR&#65289;&#23558;&#26597;&#35810;&#21644;&#25991;&#26723;&#36716;&#21270;&#20026;&#23494;&#38598;&#21521;&#37327;&#34920;&#31034;&#65292;&#24182;&#22312;&#21521;&#37327;&#31354;&#38388;&#20013;&#27979;&#37327;&#26597;&#35810;&#19982;&#25991;&#26723;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;DR&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#32570;&#20047;&#39046;&#22495;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#34429;&#28982;DR&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#20174;&#22823;&#35268;&#27169;&#20844;&#20849;&#25968;&#25454;&#38598;&#65288;&#22914;MS MARCO&#65289;&#20013;&#23398;&#20064;&#65292;&#20294;&#35777;&#25454;&#34920;&#26126;&#65292;&#24182;&#38750;&#25152;&#26377;DR&#27169;&#22411;&#21644;&#39046;&#22495;&#37117;&#33021;&#21516;&#31561;&#21463;&#30410;&#20110;&#36801;&#31227;&#23398;&#20064;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#30740;&#31350;&#20154;&#21592;&#36716;&#21521;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#25913;&#36827;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#30340;DR&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20013;&#37319;&#29992;&#30340;&#30828;&#25552;&#31034;&#25110;&#20154;&#24037;&#32534;&#20889;&#30340;&#25552;&#31034;&#26080;&#27861;&#20445;&#35777;&#29983;&#25104;&#30340;&#24369;&#26597;&#35810;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#22686;&#24378;DR&#30340;&#36719;&#25552;&#31034;&#35843;&#20248;&#65288;SPTAR&#65289;&#65306;&#23545;&#20110;&#27599;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#21033;&#29992;&#36719;&#25552;&#31034;&#35843;&#20248;&#22312;&#26377;&#38480;&#30340;&#30495;&#23454;&#25968;&#25454;&#19978;&#20248;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#36719;&#25552;&#31034;&#65292;&#28982;&#21518;&#29992;&#36825;&#20123;&#25552;&#31034;&#24341;&#23548;LLMs&#20026;&#26410;&#26631;&#35760;&#30340;&#25991;&#26723;&#26631;&#35760;&#24369;&#26597;&#35810;&#65292;&#20174;&#32780;&#24471;&#21040;&#36275;&#22815;&#30340;&#24369;&#25991;&#26723;-&#26597;&#35810;&#23545;&#26469;&#35757;&#32451;&#20219;&#21153;&#29305;&#23450;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dense retrieval (DR) converts queries and documents into dense embeddings and measures the similarity between queries and documents in vector space. One of the challenges in DR is the lack of domain-specific training data. While DR models can learn from large-scale public datasets like MS MARCO through transfer learning, evidence shows that not all DR models and domains can benefit from transfer learning equally. Recently, some researchers have resorted to large language models (LLMs) to improve the zero-shot and few-shot DR models. However, the hard prompts or human-written prompts utilized in these works cannot guarantee the good quality of generated weak queries. To tackle this, we propose soft prompt tuning for augmenting DR (SPTAR): For each task, we leverage soft prompt-tuning to optimize a task-specific soft prompt on limited ground truth data and then prompt the LLMs to tag unlabeled documents with weak queries, yielding enough weak document-query pairs to train task-specific d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31185;&#23398;&#35266;&#28857;&#24635;&#32467;&#30340;&#20219;&#21153;&#65292;&#20197;&#21512;&#25104;&#30740;&#31350;&#35770;&#25991;&#35780;&#23457;&#30340;&#20803;&#35780;&#23457;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;ORSUM&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#26816;&#26597;&#34920;&#24341;&#23548;&#36845;&#20195;&#33258;&#26597;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.14647</link><description>&lt;p&gt;
&#22522;&#20110;&#26816;&#26597;&#34920;&#24341;&#23548;&#36845;&#20195;&#33258;&#26597;&#30340;&#20803;&#35780;&#23457;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Meta-review Generation with Checklist-guided Iterative Introspection. (arXiv:2305.14647v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14647
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31185;&#23398;&#35266;&#28857;&#24635;&#32467;&#30340;&#20219;&#21153;&#65292;&#20197;&#21512;&#25104;&#30740;&#31350;&#35770;&#25991;&#35780;&#23457;&#30340;&#20803;&#35780;&#23457;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;ORSUM&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#26816;&#26597;&#34920;&#24341;&#23548;&#36845;&#20195;&#33258;&#26597;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31185;&#23398;&#39046;&#22495;&#20013;&#65292;&#19981;&#21516;&#30340;&#35266;&#28857;&#21487;&#33021;&#20250;&#23548;&#33268;&#23545;&#35780;&#23457;&#24847;&#35265;&#30340;&#20105;&#35758;&#25110;&#20849;&#35782;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#35266;&#28857;&#24635;&#32467;&#25968;&#25454;&#38598;&#20027;&#35201;&#38598;&#20013;&#22312;&#20135;&#21697;&#35780;&#35770;&#39046;&#22495;&#65292;&#27809;&#26377;&#32771;&#34385;&#21040;&#36825;&#31181;&#21487;&#21464;&#24615;&#65292;&#20551;&#35774;&#36755;&#20837;&#30340;&#24847;&#35265;&#26159;&#27809;&#26377;&#20105;&#35758;&#30340;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31185;&#23398;&#35266;&#28857;&#24635;&#32467;&#30340;&#20219;&#21153;&#65292;&#23558;&#30740;&#31350;&#35770;&#25991;&#35780;&#23457;&#21512;&#25104;&#20026;&#20803;&#35780;&#23457;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;ORSUM&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#26469;&#33258;39&#20010;&#20250;&#35758;&#30340;10,989&#31687;&#35770;&#25991;&#20803;&#23457;&#26597;&#21644;40,903&#31687;&#35770;&#25991;&#23457;&#26597;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#26816;&#26597;&#34920;&#24341;&#23548;&#36845;&#20195;&#33258;&#26597;&#30340;&#26041;&#27861;&#65292;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#20960;&#20010;&#38454;&#27573;&#65292;&#24182;&#22312;&#26816;&#26597;&#34920;&#30340;&#25351;&#23548;&#19979;&#36845;&#20195;&#22320;&#23436;&#21892;&#25688;&#35201;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65306;&#65288;1&#65289;&#20154;&#24037;&#25776;&#20889;&#30340;&#25688;&#35201;&#24182;&#19981;&#24635;&#26159;&#21487;&#38752;&#30340;&#65292;&#22240;&#20026;&#35768;&#22810;&#20154;&#24182;&#27809;&#26377;&#36981;&#24490;&#25351;&#21335;&#65292;&#65288;2&#65289;&#20219;&#21153;&#20998;&#35299;&#21644;&#36845;&#20195;&#33258;&#25105;&#23436;&#21892;&#30340;&#32452;&#21512;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Opinions in the scientific domain can be divergent, leading to controversy or consensus among reviewers. However, current opinion summarization datasets mostly focus on product review domains, which do not account for this variability under the assumption that the input opinions are non-controversial. To address this gap, we propose the task of scientific opinion summarization, where research paper reviews are synthesized into meta-reviews. To facilitate this task, we introduce a new ORSUM dataset covering 10,989 paper meta-reviews and 40,903 paper reviews from 39 conferences. Furthermore, we propose the Checklist-guided Iterative Introspection (CGI$^2$) approach, which breaks down the task into several stages and iteratively refines the summary under the guidance of questions from a checklist. We conclude that (1) human-written summaries are not always reliable since many do not follow the guideline, and (2) the combination of task decomposition and iterative self-refinement shows pro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#21160;&#20107;&#23454;&#26680;&#26597;&#27969;&#31243;&#65292;&#21487;&#20197;&#36890;&#36807;&#20174;&#32593;&#32476;&#20013;&#26816;&#32034;&#21407;&#22987;&#35777;&#25454;&#26469;&#26816;&#26597;&#23454;&#38469;&#20027;&#24352;&#12290;&#25105;&#20204;&#30340;&#27969;&#31243;&#21253;&#25324;&#20116;&#20010;&#37096;&#20998;&#65306;&#20027;&#24352;&#20998;&#35299;&#65292;&#21407;&#22987;&#25991;&#26723;&#26816;&#32034;&#65292;&#32454;&#31890;&#24230;&#35777;&#25454;&#26816;&#32034;&#65292;&#20027;&#24352;&#28966;&#28857;&#25688;&#35201;&#21644;&#21028;&#26029;&#30495;&#23454;&#24615;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27969;&#31243;&#21487;&#20197;&#25913;&#21892;&#30495;&#23454;&#24615;&#21028;&#26029;&#65292;&#25105;&#20204;&#25552;&#20379;&#30340;&#35777;&#25454;&#25688;&#35201;&#26356;&#26131;&#20110;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.11859</link><description>&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#22797;&#26434;&#20027;&#24352;&#39564;&#35777;&#20013;&#30340;&#21462;&#35777;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Complex Claim Verification with Evidence Retrieved in the Wild. (arXiv:2305.11859v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#21160;&#20107;&#23454;&#26680;&#26597;&#27969;&#31243;&#65292;&#21487;&#20197;&#36890;&#36807;&#20174;&#32593;&#32476;&#20013;&#26816;&#32034;&#21407;&#22987;&#35777;&#25454;&#26469;&#26816;&#26597;&#23454;&#38469;&#20027;&#24352;&#12290;&#25105;&#20204;&#30340;&#27969;&#31243;&#21253;&#25324;&#20116;&#20010;&#37096;&#20998;&#65306;&#20027;&#24352;&#20998;&#35299;&#65292;&#21407;&#22987;&#25991;&#26723;&#26816;&#32034;&#65292;&#32454;&#31890;&#24230;&#35777;&#25454;&#26816;&#32034;&#65292;&#20027;&#24352;&#28966;&#28857;&#25688;&#35201;&#21644;&#21028;&#26029;&#30495;&#23454;&#24615;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27969;&#31243;&#21487;&#20197;&#25913;&#21892;&#30495;&#23454;&#24615;&#21028;&#26029;&#65292;&#25105;&#20204;&#25552;&#20379;&#30340;&#35777;&#25454;&#25688;&#35201;&#26356;&#26131;&#20110;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21462;&#35777;&#26159;&#33258;&#21160;&#20107;&#23454;&#26680;&#26597;&#30340;&#26680;&#24515;&#37096;&#20998;&#12290; &#38024;&#23545;&#33719;&#21462;&#21462;&#35777;&#30340;&#20551;&#35774;&#19981;&#31526;&#21512;&#23454;&#38469;&#24212;&#29992;&#24773;&#20917;&#65292;&#20043;&#21069;&#30340;&#24037;&#20316;&#20570;&#20986;&#20102;&#31616;&#21270;&#30340;&#20551;&#35774;&#65292;&#21363;&#27809;&#26377;&#33719;&#21462;&#35777;&#25454;&#30340;&#26435;&#38480;&#65292;&#35775;&#38382;&#30001;&#20154;&#24037;&#20107;&#23454;&#26680;&#26597;&#21592;&#31934;&#31616;&#30340;&#35777;&#25454;&#65292;&#25110;&#22312;&#23545;&#20027;&#24352;&#30830;&#35748;&#20043;&#21518;&#24456;&#38271;&#26102;&#38388;&#25165;&#33021;&#33719;&#21462;&#35777;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#27969;&#31243;&#65292;&#36890;&#36807;&#20174;&#32593;&#32476;&#20013;&#26816;&#32034;&#21407;&#22987;&#35777;&#25454;&#26469;&#26816;&#26597;&#23454;&#38469;&#20027;&#24352;&#12290;&#25105;&#20204;&#38480;&#21046;&#25105;&#20204;&#30340;&#26816;&#32034;&#22120;&#20165;&#25628;&#32034;&#22312;&#25552;&#20986;&#20027;&#24352;&#20043;&#21069;&#23601;&#24050;&#21487;&#25552;&#20379;&#30340;&#25991;&#26723;&#65292;&#27169;&#25311;&#26032;&#20986;&#29616;&#30340;&#20027;&#24352;&#38656;&#35201;&#26816;&#26597;&#30340;&#30495;&#23454;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#27969;&#31243;&#21253;&#25324;&#20116;&#20010;&#37096;&#20998;&#65306;&#20027;&#24352;&#20998;&#35299;&#65292;&#21407;&#22987;&#25991;&#26723;&#26816;&#32034;&#65292;&#32454;&#31890;&#24230;&#35777;&#25454;&#26816;&#32034;&#65292;&#20027;&#24352;&#28966;&#28857;&#25688;&#35201;&#21644;&#21028;&#26029;&#30495;&#23454;&#24615;&#12290;&#25105;&#20204;&#22312;ClaimDecomp&#25968;&#25454;&#38598;&#20013;&#23545;&#22797;&#26434;&#25919;&#27835;&#20027;&#24352;&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#27969;&#31243;&#20135;&#29983;&#30340;&#32858;&#21512;&#35777;&#25454;&#21487;&#20197;&#25913;&#21892;&#30495;&#23454;&#24615;&#21028;&#26029;&#12290;&#20154;&#31867;&#35780;&#20272;&#21457;&#29616;&#65292;&#25105;&#20204;&#30340;&#25688;&#35201;&#20135;&#29983;&#30340;&#35777;&#25454;&#35201;&#27604;&#30001;&#29616;&#26377;&#31995;&#32479;&#20135;&#29983;&#30340;&#35777;&#25454;&#26356;&#26131;&#20110;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evidence retrieval is a core part of automatic fact-checking. Prior work makes simplifying assumptions in retrieval that depart from real-world use cases: either no access to evidence, access to evidence curated by a human fact-checker, or access to evidence available long after the claim has been made. In this work, we present the first fully automated pipeline to check real-world claims by retrieving raw evidence from the web. We restrict our retriever to only search documents available prior to the claim's making, modeling the realistic scenario where an emerging claim needs to be checked. Our pipeline includes five components: claim decomposition, raw document retrieval, fine-grained evidence retrieval, claim-focused summarization, and veracity judgment. We conduct experiments on complex political claims in the ClaimDecomp dataset and show that the aggregated evidence produced by our pipeline improves veracity judgments. Human evaluation finds the evidence summary produced by our s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;TaxBox&#65292;&#35813;&#26041;&#27861;&#23558;&#20998;&#31867;&#27861;&#27010;&#24565;&#26144;&#23556;&#21040;&#26694;&#23884;&#20837;&#20013;&#65292;&#24182;&#20351;&#29992;&#20004;&#20010;&#27010;&#29575;&#35780;&#20998;&#22120;&#26469;&#36827;&#34892;&#27010;&#24565;&#38468;&#21152;&#21644;&#25554;&#20837;&#65292;&#36991;&#20813;&#20351;&#29992;&#20266;&#21494;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#20108;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;TaxBox&#22312;&#20934;&#30830;&#24615;&#21644;&#35757;&#32451;&#25928;&#29575;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.11004</link><description>&lt;p&gt;
&#36890;&#36807;&#26694;&#23884;&#20837;&#21644;&#27010;&#29575;&#35780;&#20998;&#22120;&#23436;&#25104;&#20998;&#31867;&#27861;
&lt;/p&gt;
&lt;p&gt;
Taxonomy Completion with Probabilistic Scorer via Box Embedding. (arXiv:2305.11004v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;TaxBox&#65292;&#35813;&#26041;&#27861;&#23558;&#20998;&#31867;&#27861;&#27010;&#24565;&#26144;&#23556;&#21040;&#26694;&#23884;&#20837;&#20013;&#65292;&#24182;&#20351;&#29992;&#20004;&#20010;&#27010;&#29575;&#35780;&#20998;&#22120;&#26469;&#36827;&#34892;&#27010;&#24565;&#38468;&#21152;&#21644;&#25554;&#20837;&#65292;&#36991;&#20813;&#20351;&#29992;&#20266;&#21494;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#20108;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;TaxBox&#22312;&#20934;&#30830;&#24615;&#21644;&#35757;&#32451;&#25928;&#29575;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20998;&#31867;&#27861;&#30340;&#23436;&#21892;&#20219;&#21153;--&#33258;&#21160;&#21033;&#29992;&#26032;&#30340;&#27010;&#24565;&#20016;&#23500;&#29616;&#26377;&#20998;&#31867;&#27861;--&#24050;&#32463;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20852;&#36259;&#12290;&#26089;&#26399;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#22797;&#26434;&#27169;&#22359;&#12289;&#22806;&#37096;&#20449;&#24687;&#21644;&#20266;&#21494;&#26469;&#20016;&#23500;&#34920;&#31034;&#24182;&#32479;&#19968;&#38468;&#21152;&#21644;&#25554;&#20837;&#30340;&#21305;&#37197;&#36807;&#31243;&#12290;&#34429;&#28982;&#23427;&#20204;&#24050;&#32463;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#36825;&#20123;&#20171;&#32461;&#21487;&#33021;&#20250;&#22312;&#35757;&#32451;&#21644;&#35780;&#20998;&#36807;&#31243;&#20013;&#24102;&#26469;&#22122;&#38899;&#21644;&#19981;&#20844;&#24179;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TaxBox&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#23436;&#25104;&#20998;&#31867;&#27861;&#30340;&#26694;&#26550;&#65292;&#23427;&#23558;&#20998;&#31867;&#27861;&#27010;&#24565;&#26144;&#23556;&#21040;&#26694;&#23884;&#20837;&#20013;&#65292;&#24182;&#20351;&#29992;&#20004;&#20010;&#27010;&#29575;&#35780;&#20998;&#22120;&#26469;&#36827;&#34892;&#27010;&#24565;&#38468;&#21152;&#21644;&#25554;&#20837;&#65292;&#36991;&#20813;&#20351;&#29992;&#20266;&#21494;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;TaxBox&#30001;&#19977;&#20010;&#32452;&#20214;&#32452;&#25104;&#65306;&#65288;1&#65289;&#22270;&#32858;&#21512;&#27169;&#22359;&#65292;&#20197;&#21033;&#29992;&#20998;&#31867;&#27861;&#30340;&#32467;&#26500;&#20449;&#24687;&#21644;&#20004;&#20010;&#36731;&#37327;&#32423;&#35299;&#30721;&#22120;&#65292;&#23558;&#29305;&#24449;&#26144;&#23556;&#21040;&#26694;&#23884;&#20837;&#65292;&#24182;&#25429;&#25417;&#27010;&#24565;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65307;&#65288;2&#65289;&#20004;&#20010;&#27010;&#29575;&#35780;&#20998;&#22120;&#65292;&#20998;&#21035;&#23545;&#24212;&#38468;&#21152;&#21644;&#25554;&#20837;&#20219;&#21153;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#21407;&#21017;&#24615;&#26041;&#27861;&#26469;&#20943;&#36731;&#35823;&#23548;&#20449;&#24687;&#30340;&#24433;&#21709;&#65307;&#65288;3&#65289;&#19968;&#31181;&#32852;&#21512;&#35757;&#32451;&#27169;&#22411;&#21644;&#20248;&#21270;&#20004;&#20010;&#35780;&#20998;&#22120;&#30340;&#35757;&#32451;&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;TaxBox&#22312;&#20934;&#30830;&#24615;&#21644;&#35757;&#32451;&#25928;&#29575;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#29366;&#24577;-of-the-art&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Taxonomy completion, a task aimed at automatically enriching an existing taxonomy with new concepts, has gained significant interest in recent years. Previous works have introduced complex modules, external information, and pseudo-leaves to enrich the representation and unify the matching process of attachment and insertion. While they have achieved good performance, these introductions may have brought noise and unfairness during training and scoring. In this paper, we present TaxBox, a novel framework for taxonomy completion that maps taxonomy concepts to box embeddings and employs two probabilistic scorers for concept attachment and insertion, avoiding the need for pseudo-leaves. Specifically, TaxBox consists of three components: (1) a graph aggregation module to leverage the structural information of the taxonomy and two lightweight decoders that map features to box embedding and capture complex relationships between concepts; (2) two probabilistic scorers that correspond to attach
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#23376;&#31354;&#38388;&#23398;&#20064;&#30340;&#40657;&#21283;&#23376;&#25552;&#31034;&#35843;&#20248;&#65288;BSL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#22312;&#30456;&#20284;&#20219;&#21153;&#30340;&#38598;&#21512;&#20013;&#35782;&#21035;&#23376;&#31354;&#38388;&#65292;&#20197;&#25913;&#21892;&#40657;&#21283;&#23376;&#25552;&#31034;&#35843;&#20248;&#30340;&#36890;&#29992;&#24615;&#65292;&#22312;&#19979;&#28216;&#20219;&#21153;&#21644;LLMs&#19978;&#37117;&#33021;&#22815;&#25345;&#32493;&#36798;&#21040;&#31454;&#20105;&#24615;&#30340;&#34920;&#29616;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2305.03518</link><description>&lt;p&gt;
&#22522;&#20110;&#23376;&#31354;&#38388;&#23398;&#20064;&#30340;&#40657;&#21283;&#23376;&#25552;&#31034;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Black-box Prompt Tuning with Subspace Learning. (arXiv:2305.03518v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#23376;&#31354;&#38388;&#23398;&#20064;&#30340;&#40657;&#21283;&#23376;&#25552;&#31034;&#35843;&#20248;&#65288;BSL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#22312;&#30456;&#20284;&#20219;&#21153;&#30340;&#38598;&#21512;&#20013;&#35782;&#21035;&#23376;&#31354;&#38388;&#65292;&#20197;&#25913;&#21892;&#40657;&#21283;&#23376;&#25552;&#31034;&#35843;&#20248;&#30340;&#36890;&#29992;&#24615;&#65292;&#22312;&#19979;&#28216;&#20219;&#21153;&#21644;LLMs&#19978;&#37117;&#33021;&#22815;&#25345;&#32493;&#36798;&#21040;&#31454;&#20105;&#24615;&#30340;&#34920;&#29616;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40657;&#21283;&#23376;&#25552;&#31034;&#35843;&#20248;&#20351;&#29992;&#26080;&#23548;&#25968;&#20248;&#21270;&#31639;&#27861;&#22312;&#20302;&#32500;&#23376;&#31354;&#38388;&#20013;&#23398;&#20064;&#25552;&#31034;&#65292;&#32780;&#19981;&#26159;&#36890;&#36807;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#32593;&#32476;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#40657;&#21283;&#23376;&#25552;&#31034;&#35843;&#20248;&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;LLMs&#19978;&#32570;&#20047;&#36890;&#29992;&#24615;&#65292;&#36825;&#19982;&#19981;&#24688;&#24403;&#30340;&#23376;&#31354;&#38388;&#36873;&#25321;&#26377;&#20851;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#23376;&#31354;&#38388;&#23398;&#20064;&#30340;&#40657;&#21283;&#23376;&#25552;&#31034;&#35843;&#20248;&#65288;BSL&#65289;&#26469;&#25913;&#21892;&#40657;&#21283;&#23376;&#25552;&#31034;&#35843;&#20248;&#30340;&#36890;&#29992;&#24615;&#12290;&#22522;&#20110;&#30456;&#20284;&#20219;&#21153;&#30340;&#38598;&#21512;&#19978;&#36827;&#34892;&#20803;&#23398;&#20064;&#20197;&#30830;&#23450;&#23376;&#31354;&#38388;&#21487;&#20197;&#35782;&#21035;&#31867;&#20284;&#20219;&#21153;&#30340;&#26368;&#20248;&#25552;&#31034;&#65292;&#24182;&#19988;&#21487;&#20197;&#20445;&#35777;&#22312;&#30456;&#20284;&#20219;&#21153;&#19978;&#36827;&#34892;&#23376;&#31354;&#38388;&#20248;&#21270;&#25214;&#21040;&#19968;&#20010;&#22312;&#30446;&#26631;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#30340;&#25552;&#31034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;BSL&#26694;&#26550;&#26080;&#35770;&#22312;&#19979;&#28216;&#20219;&#21153;&#21644;LLMs&#19978;&#37117;&#33021;&#22815;&#25345;&#32493;&#36798;&#21040;&#31454;&#20105;&#24615;&#30340;&#34920;&#29616;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Black-box prompt tuning uses derivative-free optimization algorithms to learn prompts in low-dimensional subspaces instead of back-propagating through the network of Large Language Models (LLMs). Recent studies have found that black-box prompt tuning lacks versatility across tasks and LLMs, which we believe is related to the inappropriate choice of subspaces. In this paper, we propose Black-box prompt tuning with Subspace Learning (BSL) to improve the versatility of black-box prompt tuning. Based on the assumption that nearly optimal prompts for similar tasks exist in a common subspace, we propose identifying such subspaces by meta-learning on a set of similar source tasks. Therefore, for a target task that shares similarities with source tasks, we guarantee that optimizing in the subspace can find a prompt that performs well on the target task. Experiments confirm that our BSL framework consistently achieves competitive performance regardless of downstream tasks and LLMs.
&lt;/p&gt;</description></item></channel></rss>