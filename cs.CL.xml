<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#23545;&#35805;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#36827;&#34892;&#33258;&#30417;&#30563;&#24494;&#35843;&#65292;&#20197;&#25913;&#21892;&#20302;&#36136;&#37327;&#35821;&#38899;&#35782;&#21035;&#32467;&#26524;&#23548;&#33268;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#24212;&#29992;&#30340;&#22833;&#36133;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#21487;&#20197;&#23558;ASR&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#39640;&#36798;&#21040;19.2%&#12290;</title><link>http://arxiv.org/abs/2401.02417</link><description>&lt;p&gt;
&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20316;&#20026;&#33258;&#30417;&#30563;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#20652;&#21270;&#21058;
&lt;/p&gt;
&lt;p&gt;
Task Oriented Dialogue as a Catalyst for Self-Supervised Automatic Speech Recognition. (arXiv:2401.02417v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#23545;&#35805;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#36827;&#34892;&#33258;&#30417;&#30563;&#24494;&#35843;&#65292;&#20197;&#25913;&#21892;&#20302;&#36136;&#37327;&#35821;&#38899;&#35782;&#21035;&#32467;&#26524;&#23548;&#33268;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#24212;&#29992;&#30340;&#22833;&#36133;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#21487;&#20197;&#23558;ASR&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#39640;&#36798;&#21040;19.2%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#30340;&#35789;&#38169;&#35823;&#29575;&#19981;&#26029;&#19979;&#38477;&#65292;&#20294;&#22522;&#20110;ASR&#31995;&#32479;&#26500;&#24314;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#24212;&#29992;&#20173;&#28982;&#24402;&#22240;&#20110;&#20302;&#36136;&#37327;&#30340;&#35821;&#38899;&#35782;&#21035;&#32467;&#26524;&#23548;&#33268;&#30340;&#22823;&#37327;&#22833;&#36133;&#12290;&#29616;&#26377;&#30340;&#21161;&#25163;&#31995;&#32479;&#25910;&#38598;&#20102;&#22823;&#37327;&#36825;&#20123;&#22833;&#36133;&#30340;&#20132;&#20114;&#65292;&#20294;&#26159;&#36825;&#20123;&#31995;&#32479;&#36890;&#24120;&#26080;&#27861;&#20174;&#36825;&#20123;&#20132;&#20114;&#20013;&#23398;&#20064;&#65292;&#21363;&#20351;&#26159;&#31163;&#32447;&#23398;&#20064;&#20063;&#26159;&#22914;&#27492;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CLC&#65306;&#23545;&#35805;&#23545;&#27604;&#23398;&#20064;&#65292;&#36825;&#26159;&#19968;&#31995;&#21015;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#27169;&#22411;&#36827;&#34892;&#23545;&#27604;&#24494;&#35843;&#65292;&#21033;&#29992;&#22833;&#36133;&#23545;&#35805;&#20013;&#23481;&#26131;&#26816;&#27979;&#21040;&#30340;&#20154;&#24037;&#30165;&#36857;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;CLC&#31995;&#21015;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;ASR&#27169;&#22411;&#22312;OD3&#19978;&#30340;&#24615;&#33021;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#20844;&#24320;&#30340;&#22823;&#35268;&#27169;&#21322;&#21512;&#25104;&#20803;&#25968;&#25454;&#38598;&#65292;&#23545;&#35805;&#26159;&#20197;&#20219;&#21153;&#20026;&#23548;&#21521;&#30340;&#65292;&#25552;&#21319;&#24133;&#24230;&#39640;&#36798;19.2%&#12290;&#36825;&#20123;&#22686;&#30410;&#20063;&#21487;&#20197;&#36716;&#31227;&#21040;&#29616;&#23454;&#19990;&#30028;&#30340;&#31995;&#32479;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;CLC&#21487;&#20197;&#24110;&#21161;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
While word error rates of automatic speech recognition (ASR) systems have consistently fallen, natural language understanding (NLU) applications built on top of ASR systems still attribute significant numbers of failures to low-quality speech recognition results. Existing assistant systems collect large numbers of these unsuccessful interactions, but these systems usually fail to learn from these interactions, even in an offline fashion. In this work, we introduce CLC: Contrastive Learning for Conversations, a family of methods for contrastive fine-tuning of models in a self-supervised fashion, making use of easily detectable artifacts in unsuccessful conversations with assistants. We demonstrate that our CLC family of approaches can improve the performance of ASR models on OD3, a new public large-scale semi-synthetic meta-dataset of audio task-oriented dialogues, by up to 19.2%. These gains transfer to real-world systems as well, where we show that CLC can help to improve performance 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;LLMs&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;Transformer&#27169;&#22359;&#24182;&#20351;&#29992;&#26032;&#30340;&#35821;&#26009;&#24211;&#36827;&#34892;&#35843;&#25972;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#27169;&#22411;&#30693;&#35782;&#65292;&#32780;&#19981;&#20250;&#21457;&#29983;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#22312;&#20195;&#30721;&#21644;&#25968;&#23398;&#39046;&#22495;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LLaMA Pro&#26159;&#19968;&#31181;&#21151;&#33021;&#24378;&#22823;&#19988;&#36866;&#29992;&#20110;&#36890;&#29992;&#20219;&#21153;&#12289;&#32534;&#31243;&#21644;&#25968;&#23398;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#21516;&#26102;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#26234;&#33021;&#20307;&#25512;&#29702;&#21644;&#35299;&#20915;&#22810;&#26679;&#21270;&#20219;&#21153;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.02415</link><description>&lt;p&gt;
LLaMA Pro: &#24102;&#26377;&#27169;&#22359;&#25193;&#23637;&#30340;&#28176;&#36827;LLaMA&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LLaMA Pro: Progressive LLaMA with Block Expansion. (arXiv:2401.02415v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;LLMs&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;Transformer&#27169;&#22359;&#24182;&#20351;&#29992;&#26032;&#30340;&#35821;&#26009;&#24211;&#36827;&#34892;&#35843;&#25972;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#27169;&#22411;&#30693;&#35782;&#65292;&#32780;&#19981;&#20250;&#21457;&#29983;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#22312;&#20195;&#30721;&#21644;&#25968;&#23398;&#39046;&#22495;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LLaMA Pro&#26159;&#19968;&#31181;&#21151;&#33021;&#24378;&#22823;&#19988;&#36866;&#29992;&#20110;&#36890;&#29992;&#20219;&#21153;&#12289;&#32534;&#31243;&#21644;&#25968;&#23398;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#21516;&#26102;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#26234;&#33021;&#20307;&#25512;&#29702;&#21644;&#35299;&#20915;&#22810;&#26679;&#21270;&#20219;&#21153;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#36890;&#24120;&#22312;&#19981;&#29306;&#29298;&#26087;&#25216;&#33021;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#26032;&#25216;&#33021;&#65307;&#28982;&#32780;&#65292;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20363;&#22914;&#20174;LLaMA&#21040;CodeLLaMA&#21017;&#30456;&#21453;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;Transformer&#27169;&#22359;&#25193;&#23637;&#30340;LLMs&#30340;&#26032;&#30340;&#39044;&#35757;&#32451;&#21518;&#22788;&#29702;&#26041;&#27861;&#12290;&#25105;&#20204;&#20165;&#20351;&#29992;&#26032;&#30340;&#35821;&#26009;&#24211;&#35843;&#25972;&#25193;&#23637;&#30340;&#27169;&#22359;&#65292;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#27169;&#22411;&#30340;&#30693;&#35782;&#32780;&#19981;&#20250;&#21457;&#29983;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#26412;&#25991;&#22312;&#20195;&#30721;&#21644;&#25968;&#23398;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#24471;&#21040;&#20102;&#20174;LLaMA2-7B&#21021;&#22987;&#21270;&#30340;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;LLaMA Pro-8.3B&#65292;&#22312;&#24120;&#35268;&#20219;&#21153;&#12289;&#32534;&#31243;&#21644;&#25968;&#23398;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;LLaMA Pro&#21450;&#20854;&#25353;&#29031;&#25351;&#20196;&#25191;&#34892;&#30340;&#23545;&#24212;&#27169;&#22411;&#65288;LLaMA Pro-Instruct&#65289;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#37117;&#21462;&#24471;&#20102;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#22312;LLaMA&#31995;&#21015;&#21644;&#20854;&#20182;&#24320;&#25918;&#27169;&#22411;&#20013;&#30340;&#20248;&#36234;&#24615;&#20197;&#21450;&#20316;&#20026;&#26234;&#33021;&#20307;&#25512;&#29702;&#21644;&#35299;&#20915;&#22810;&#26679;&#21270;&#20219;&#21153;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#23545;&#20110;&#25972;&#21512;&#33258;&#28982;&#35821;&#35328;&#21644;&#32534;&#31243;&#35821;&#35328;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#20026;&#21457;&#23637;&#25512;&#29702;&#21644;&#35299;&#20915;&#22810;&#26679;&#21270;&#20219;&#21153;&#30340;&#26234;&#33021;&#20307;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans generally acquire new skills without compromising the old; however, the opposite holds for Large Language Models (LLMs), e.g., from LLaMA to CodeLLaMA. To this end, we propose a new post-pretraining method for LLMs with an expansion of Transformer blocks. We tune the expanded blocks using only new corpus, efficiently and effectively improving the model's knowledge without catastrophic forgetting. In this paper, we experiment on the corpus of code and math, yielding LLaMA Pro-8.3B, a versatile foundation model initialized from LLaMA2-7B, excelling in general tasks, programming, and mathematics. LLaMA Pro and its instruction-following counterpart (LLaMA Pro-Instruct) achieve advanced performance among various benchmarks, demonstrating superiority over existing open models in the LLaMA family and the immense potential of reasoning and addressing diverse tasks as an intelligent agent. Our findings provide valuable insights into integrating natural and programming languages, laying a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;CALM&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#21512;&#29616;&#26377;&#30340;&#22522;&#30784;&#27169;&#22411;&#21644;&#26356;&#20855;&#20307;&#30340;&#27169;&#22411;&#65292;&#20351;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#34920;&#31034;&#24182;&#23454;&#29616;&#26032;&#30340;&#33021;&#21147;&#12290;CALM&#21487;&#20197;&#36890;&#36807;&#8220;&#37325;&#29992;&#8221;&#29616;&#26377;&#27169;&#22411;&#21644;&#19968;&#20123;&#39069;&#22806;&#30340;&#21442;&#25968;&#21644;&#25968;&#25454;&#26469;&#25193;&#23637;&#26032;&#20219;&#21153;&#19978;&#30340;&#27169;&#22411;&#35268;&#27169;&#65292;&#24182;&#19988;&#20445;&#30041;&#29616;&#26377;&#27169;&#22411;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.02412</link><description>&lt;p&gt;
LLM&#22686;&#24378;&#30340;LLMs&#65306;&#36890;&#36807;&#32452;&#21512;&#25193;&#23637;&#21151;&#33021;
&lt;/p&gt;
&lt;p&gt;
LLM Augmented LLMs: Expanding Capabilities through Composition. (arXiv:2401.02412v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02412
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;CALM&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#21512;&#29616;&#26377;&#30340;&#22522;&#30784;&#27169;&#22411;&#21644;&#26356;&#20855;&#20307;&#30340;&#27169;&#22411;&#65292;&#20351;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#34920;&#31034;&#24182;&#23454;&#29616;&#26032;&#30340;&#33021;&#21147;&#12290;CALM&#21487;&#20197;&#36890;&#36807;&#8220;&#37325;&#29992;&#8221;&#29616;&#26377;&#27169;&#22411;&#21644;&#19968;&#20123;&#39069;&#22806;&#30340;&#21442;&#25968;&#21644;&#25968;&#25454;&#26469;&#25193;&#23637;&#26032;&#20219;&#21153;&#19978;&#30340;&#27169;&#22411;&#35268;&#27169;&#65292;&#24182;&#19988;&#20445;&#30041;&#29616;&#26377;&#27169;&#22411;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#20855;&#26377;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#30340;&#22522;&#30784;&#27169;&#22411;&#24050;&#32463;&#23637;&#29616;&#20986;&#22312;&#21508;&#20010;&#39046;&#22495;&#20855;&#26377;&#38750;&#24179;&#20961;&#25216;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#30340;&#25972;&#20307;&#32467;&#26500;&#65292;&#23545;&#23427;&#20204;&#36827;&#34892;&#22686;&#24378;&#25110;&#36171;&#20104;&#26032;&#30340;&#25216;&#33021;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#25104;&#26412;&#39640;&#26114;&#30340;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#30001;&#20110;&#20854;&#36866;&#24212;&#33021;&#21147;&#65292;&#27491;&#22312;&#35757;&#32451;&#22810;&#20010;&#26032;&#39046;&#22495;&#21644;&#20219;&#21153;&#30340;&#27169;&#22411;&#23454;&#20363;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21033;&#29992;&#29616;&#26377;&#30340;&#22522;&#30784;&#27169;&#22411;&#19982;&#26356;&#20855;&#20307;&#27169;&#22411;&#36827;&#34892;&#39640;&#25928;&#23454;&#29992;&#30340;&#32452;&#21512;&#65292;&#20197;&#23454;&#29616;&#26032;&#30340;&#21151;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CALM -&#29992;&#20110;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#32452;&#21512;&#27169;&#22411;-&#65292;&#23427;&#24341;&#20837;&#20102;&#27169;&#22411;&#20043;&#38388;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#65292;&#20197;&#32452;&#21512;&#23427;&#20204;&#30340;&#34920;&#31034;&#24182;&#23454;&#29616;&#26032;&#30340;&#33021;&#21147;&#12290;CALM&#30340;&#26174;&#33879;&#29305;&#28857;&#21253;&#25324;&#65306;(i)&#36890;&#36807;&#8220;&#37325;&#29992;&#8221;&#29616;&#26377;LLMs&#21644;&#19968;&#20123;&#39069;&#22806;&#30340;&#21442;&#25968;&#21644;&#25968;&#25454;&#26469;&#25193;&#23637;&#26032;&#20219;&#21153;&#19978;&#30340;LLMs&#30340;&#35268;&#27169;&#65292;(ii)&#20445;&#25345;&#29616;&#26377;&#27169;&#22411;&#26435;&#37325;&#19981;&#21464;&#65292;&#20174;&#32780;&#20445;&#30041;&#29616;&#26377;&#21151;&#33021;&#65292;(iii)&#24212;&#29992;&#26032;&#21151;&#33021;&#21482;&#38656;&#35201;&#23545;&#22686;&#21152;&#30340;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundational models with billions of parameters which have been trained on large corpora of data have demonstrated non-trivial skills in a variety of domains. However, due to their monolithic structure, it is challenging and expensive to augment them or impart new skills. On the other hand, due to their adaptation abilities, several new instances of these models are being trained towards new domains and tasks. In this work, we study the problem of efficient and practical composition of existing foundation models with more specific models to enable newer capabilities. To this end, we propose CALM -Composition to Augment Language Models -- which introduces cross-attention between models to compose their representations and enable new capabilities. Salient features of CALM are: (i) Scales up LLMs on new tasks by 're-using' existing LLMs along with a few additional parameters and data, (ii) Existing model weights are kept intact, and hence preserves existing capabilities, and (iii) Appli
&lt;/p&gt;</description></item><item><title>TinyLlama&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22522;&#20110;Llama 2&#30340;&#26550;&#26500;&#21644;&#20998;&#35789;&#22120;&#65292;&#21033;&#29992;&#21508;&#31181;&#20808;&#36827;&#25216;&#26415;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;&#23613;&#31649;&#35268;&#27169;&#36739;&#23567;&#65292;&#20294;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#31867;&#20284;&#35268;&#27169;&#30340;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.02385</link><description>&lt;p&gt;
TinyLlama&#65306;&#19968;&#20010;&#24320;&#28304;&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TinyLlama: An Open-Source Small Language Model. (arXiv:2401.02385v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02385
&lt;/p&gt;
&lt;p&gt;
TinyLlama&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22522;&#20110;Llama 2&#30340;&#26550;&#26500;&#21644;&#20998;&#35789;&#22120;&#65292;&#21033;&#29992;&#21508;&#31181;&#20808;&#36827;&#25216;&#26415;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;&#23613;&#31649;&#35268;&#27169;&#36739;&#23567;&#65292;&#20294;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#31867;&#20284;&#35268;&#27169;&#30340;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;TinyLlama&#65292;&#19968;&#20010;&#26377;&#38480;&#30340;1.1B&#35821;&#35328;&#27169;&#22411;&#65292;&#22823;&#32422;&#39044;&#35757;&#32451;&#20102;1&#19975;&#20159;&#20010;&#26631;&#35760;&#65292;&#35757;&#32451;&#36718;&#25968;&#32422;&#20026;3&#36718;&#12290;TinyLlama&#22522;&#20110;Llama 2&#30340;&#26550;&#26500;&#21644;&#20998;&#35789;&#22120;&#65292;&#22312;&#24320;&#28304;&#31038;&#21306;&#30340;&#36129;&#29486;&#22522;&#30784;&#19978;&#65288;&#20363;&#22914;FlashAttention&#65289;&#65292;&#21033;&#29992;&#21508;&#31181;&#20808;&#36827;&#25216;&#26415;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;&#23613;&#31649;&#35268;&#27169;&#30456;&#23545;&#36739;&#23567;&#65292;TinyLlama&#22312;&#19968;&#31995;&#21015;&#19979;&#28216;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#23427;&#26126;&#26174;&#20248;&#20110;&#20855;&#26377;&#31867;&#20284;&#35268;&#27169;&#30340;&#29616;&#26377;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#26816;&#26597;&#28857;&#21644;&#20195;&#30721;&#21487;&#22312;GitHub&#19978;&#20844;&#24320;&#33719;&#21462;&#65292;&#32593;&#22336;&#20026;https://github.com/jzhang38/TinyLlama&#12290;
&lt;/p&gt;
&lt;p&gt;
We present TinyLlama, a compact 1.1B language model pretrained on around 1 trillion tokens for approximately 3 epochs. Building on the architecture and tokenizer of Llama 2, TinyLlama leverages various advances contributed by the open-source community (e.g., FlashAttention), achieving better computational efficiency. Despite its relatively small size, TinyLlama demonstrates remarkable performance in a series of downstream tasks. It significantly outperforms existing open-source language models with comparable sizes. Our model checkpoints and code are publicly available on GitHub at https://github.com/jzhang38/TinyLlama.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20020;&#24202;&#25688;&#35201;&#20013;&#20351;&#29992;&#21477;&#23376;&#32423;&#35268;&#21010;&#24182;&#36890;&#36807;&#23884;&#20837;&#24335;&#23454;&#20307;&#26816;&#32034;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#25688;&#35201;&#30340;&#20934;&#30830;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.02369</link><description>&lt;p&gt;
SPEER: Embedded Entity Retrieval&#19979;&#30340;&#38271;&#20020;&#24202;&#25688;&#35201;&#21477;&#23376;&#32423;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
SPEER: Sentence-Level Planning of Long Clinical Summaries via Embedded Entity Retrieval. (arXiv:2401.02369v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02369
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20020;&#24202;&#25688;&#35201;&#20013;&#20351;&#29992;&#21477;&#23376;&#32423;&#35268;&#21010;&#24182;&#36890;&#36807;&#23884;&#20837;&#24335;&#23454;&#20307;&#26816;&#32034;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#25688;&#35201;&#30340;&#20934;&#30830;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#21307;&#29983;&#22312;&#27599;&#27425;&#30149;&#20154;&#20986;&#38498;&#26102;&#24517;&#39035;&#20889;&#19968;&#20221;&#20887;&#38271;&#30340;&#25688;&#35201;&#12290;&#30001;&#20110;&#28085;&#30422;&#30340;&#20020;&#24202;&#27010;&#24565;&#25968;&#37327;&#24222;&#22823;&#65292;&#36825;&#39033;&#20219;&#21153;&#38750;&#24120;&#32791;&#26102;&#12290;&#35782;&#21035;&#21644;&#28085;&#30422;&#26174;&#33879;&#23454;&#20307;&#23545;&#20110;&#25688;&#35201;&#30340;&#20020;&#24202;&#23454;&#29992;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#22312;&#35813;&#20219;&#21153;&#19978;&#24494;&#35843;&#20102;&#24320;&#28304;&#30340;LLM&#27169;&#22411;&#65288;Mistral-7B-Instruct&#21644;Zephyr-7B-&#951;&#65289;&#65292;&#21457;&#29616;&#23427;&#20204;&#29983;&#25104;&#30340;&#25688;&#35201;&#19981;&#23436;&#25972;&#19988;&#19981;&#20934;&#30830;&#12290;&#20026;&#20102;&#22686;&#21152;&#23454;&#20307;&#35206;&#30422;&#33539;&#22260;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#36739;&#23567;&#30340;&#20165;&#32534;&#30721;&#22120;&#27169;&#22411;&#26469;&#39044;&#27979;&#26174;&#33879;&#23454;&#20307;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#20869;&#23481;&#35745;&#21010;&#26469;&#25351;&#23548;LLM&#12290;&#20026;&#20102;&#40723;&#21169;LLM&#20851;&#27880;&#28304;&#31508;&#35760;&#20013;&#30340;&#29305;&#23450;&#25552;&#21450;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SPEER&#65306;Embedded Entity Retrieval&#19979;&#30340;&#21477;&#23376;&#32423;&#35268;&#21010;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#29305;&#27530;&#30340;"{{ }}"&#36793;&#30028;&#26631;&#31614;&#26631;&#35760;&#27599;&#20010;&#26174;&#33879;&#23454;&#20307;&#36328;&#24230;&#65292;&#24182;&#35201;&#27714;LLM&#22312;&#29983;&#25104;&#27599;&#20010;&#21477;&#23376;&#20043;&#21069;&#26816;&#32034;&#26631;&#35760;&#30340;&#36328;&#24230;&#12290;&#21477;&#23376;&#32423;&#35268;&#21010;&#30456;&#24403;&#20110;&#19968;&#31181;&#29366;&#24577;&#36861;&#36394;&#65292;&#27169;&#22411;&#26126;&#30830;&#35760;&#24405;&#19979;&#27599;&#20010;&#21477;&#23376;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinician must write a lengthy summary each time a patient is discharged from the hospital. This task is time-consuming due to the sheer number of unique clinical concepts covered in the admission. Identifying and covering salient entities is vital for the summary to be clinically useful. We fine-tune open-source LLMs (Mistral-7B-Instruct and Zephyr-7B-\b{eta}) on the task and find that they generate incomplete and unfaithful summaries. To increase entity coverage, we train a smaller, encoder-only model to predict salient entities, which are treated as content-plans to guide the LLM. To encourage the LLM to focus on specific mentions in the source notes, we propose SPEER: Sentence-level Planning via Embedded Entity Retrieval. Specifically, we mark each salient entity span with special "{{ }}" boundary tags and instruct the LLM to retrieve marked spans before generating each sentence. Sentence-level planning acts as a form of state tracking in that the model is explicitly recording the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#21270;&#34920;&#26684;&#25968;&#25454;&#26469;&#25552;&#39640; RAG &#31995;&#32479;&#20013;&#22788;&#29702;&#22797;&#26434;&#34920;&#26684;&#26597;&#35810;&#30340;&#20934;&#30830;&#24615;&#65292;&#25552;&#39640;&#20102;&#25688;&#35201;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.02333</link><description>&lt;p&gt;
&#36229;&#36234;&#25552;&#21462;&#65306;&#20026;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#19978;&#19979;&#25991;&#21270;&#30340;&#34920;&#26684;&#25968;&#25454;&#20197;&#23454;&#29616;&#39640;&#25928;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Beyond Extraction: Contextualising Tabular Data for Efficient Summarisation by Language Models. (arXiv:2401.02333v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#21270;&#34920;&#26684;&#25968;&#25454;&#26469;&#25552;&#39640; RAG &#31995;&#32479;&#20013;&#22788;&#29702;&#22797;&#26434;&#34920;&#26684;&#26597;&#35810;&#30340;&#20934;&#30830;&#24615;&#65292;&#25552;&#39640;&#20102;&#25688;&#35201;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104; (RAG) &#26550;&#26500;&#22312;&#20174;&#21508;&#31181;&#25991;&#20214;&#20013;&#26816;&#32034;&#20449;&#24687;&#26041;&#38754;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#22788;&#29702;&#21253;&#21547;&#22797;&#26434;&#34920;&#26684;&#32467;&#26500;&#30340; PDF &#25991;&#26723;&#20013;&#30340;&#22797;&#26434;&#34920;&#26684;&#26597;&#35810;&#26102;&#20250;&#36935;&#21040;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640; RAG &#31995;&#32479;&#20013;&#22797;&#26434;&#34920;&#26684;&#26597;&#35810;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#23558; PDF &#23384;&#20648;&#22312;&#26816;&#32034;&#25968;&#25454;&#24211;&#20013;&#65292;&#24182;&#21333;&#29420;&#25552;&#21462;&#34920;&#26684;&#20869;&#23481;&#12290;&#25552;&#21462;&#30340;&#34920;&#26684;&#32463;&#36807;&#19978;&#19979;&#25991;&#20016;&#23500;&#30340;&#22788;&#29702;&#65292;&#23558;&#26631;&#39064;&#19982;&#30456;&#24212;&#30340;&#20540;&#36830;&#25509;&#36215;&#26469;&#12290;&#20026;&#20102;&#30830;&#20445;&#23545;&#20016;&#23500;&#25968;&#25454;&#30340;&#20840;&#38754;&#29702;&#35299;&#65292;&#25105;&#20204;&#20351;&#29992;&#32463;&#36807;&#24494;&#35843;&#30340; Llama-2-chat &#35821;&#35328;&#27169;&#22411;&#22312; RAG &#26550;&#26500;&#20013;&#36827;&#34892;&#25688;&#35201;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#27425;&#24615;&#25552;&#31034;&#20351;&#29992; ChatGPT 3.5 API &#22686;&#24378;&#34920;&#26684;&#25968;&#25454;&#30340;&#19978;&#19979;&#25991;&#21547;&#20041;&#12290;&#28982;&#21518;&#65292;&#23558;&#36825;&#20123;&#20016;&#23500;&#30340;&#25968;&#25454;&#19982;&#20854;&#20182; PDF &#25991;&#20214;&#19968;&#36215;&#36755;&#20837;&#26816;&#32034;&#25968;&#25454;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
The conventional use of the Retrieval-Augmented Generation (RAG) architecture has proven effective for retrieving information from diverse documents. However, challenges arise in handling complex table queries, especially within PDF documents containing intricate tabular structures.This research introduces an innovative approach to enhance the accuracy of complex table queries in RAG-based systems. Our methodology involves storing PDFs in the retrieval database and extracting tabular content separately. The extracted tables undergo a process of context enrichment, concatenating headers with corresponding values. To ensure a comprehensive understanding of the enriched data, we employ a fine-tuned version of the Llama-2-chat language model for summarisation within the RAG architecture. Furthermore, we augment the tabular data with contextual sense using the ChatGPT 3.5 API through a one-shot prompt. This enriched data is then fed into the retrieval database alongside other PDFs. Our appr
&lt;/p&gt;</description></item><item><title>LLaVA-$\phi$&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#21161;&#25163;&#65292;&#20351;&#29992;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;Phi-2&#26469;&#20419;&#36827;&#22810;&#27169;&#24577;&#23545;&#35805;&#12290;&#21363;&#20351;&#20855;&#26377;&#36739;&#23569;&#30340;&#21442;&#25968;&#65292;&#23427;&#20063;&#33021;&#26377;&#25928;&#22320;&#34701;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;&#20803;&#32032;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#23427;&#20026;&#26102;&#38388;&#25935;&#24863;&#30340;&#29615;&#22659;&#21644;&#38656;&#35201;&#23454;&#26102;&#20132;&#20114;&#30340;&#31995;&#32479;&#24320;&#36767;&#20102;&#26032;&#30340;&#24212;&#29992;&#36884;&#24452;&#12290;</title><link>http://arxiv.org/abs/2401.02330</link><description>&lt;p&gt;
LLaVA-$\phi$: &#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#21161;&#25163;&#19982;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LLaVA-$\phi$: Efficient Multi-Modal Assistant with Small Language Model. (arXiv:2401.02330v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02330
&lt;/p&gt;
&lt;p&gt;
LLaVA-$\phi$&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#21161;&#25163;&#65292;&#20351;&#29992;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;Phi-2&#26469;&#20419;&#36827;&#22810;&#27169;&#24577;&#23545;&#35805;&#12290;&#21363;&#20351;&#20855;&#26377;&#36739;&#23569;&#30340;&#21442;&#25968;&#65292;&#23427;&#20063;&#33021;&#26377;&#25928;&#22320;&#34701;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;&#20803;&#32032;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#23427;&#20026;&#26102;&#38388;&#25935;&#24863;&#30340;&#29615;&#22659;&#21644;&#38656;&#35201;&#23454;&#26102;&#20132;&#20114;&#30340;&#31995;&#32479;&#24320;&#36767;&#20102;&#26032;&#30340;&#24212;&#29992;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;LLaVA-$\phi$&#65288;LLaVA-Phi&#65289;&#65292;&#19968;&#31181;&#21033;&#29992;&#26368;&#36817;&#20808;&#36827;&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;Phi-2&#26469;&#20419;&#36827;&#22810;&#27169;&#24577;&#23545;&#35805;&#30340;&#39640;&#25928;&#22810;&#27169;&#24577;&#21161;&#25163;&#12290;LLaVA-Phi&#22312;&#32039;&#20945;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#39046;&#22495;&#20013;&#26631;&#24535;&#30528;&#37325;&#35201;&#36827;&#23637;&#12290;&#23427;&#35777;&#26126;&#20102;&#21363;&#20351;&#26159;&#20010;&#21442;&#25968;&#21482;&#26377;27&#20159;&#30340;&#36739;&#23567;&#35821;&#35328;&#27169;&#22411;&#22312;&#35757;&#32451;&#26377;&#39640;&#36136;&#37327;&#35821;&#26009;&#24211;&#30340;&#24773;&#20917;&#19979;&#20063;&#21487;&#20197;&#26377;&#25928;&#22320;&#21442;&#19982;&#34701;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;&#20803;&#32032;&#30340;&#22797;&#26434;&#23545;&#35805;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#21253;&#25324;&#35270;&#35273;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#22522;&#20110;&#30693;&#35782;&#30340;&#24863;&#30693;&#31561;&#20844;&#24320;&#21487;&#29992;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#38500;&#20102;&#22312;&#22810;&#27169;&#24577;&#23545;&#35805;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20026;&#26102;&#38388;&#25935;&#24863;&#30340;&#29615;&#22659;&#21644;&#38656;&#35201;&#23454;&#26102;&#20132;&#20114;&#30340;&#31995;&#32479;&#65288;&#22914;&#23454;&#20307;&#20195;&#29702;&#65289;&#24320;&#36767;&#20102;&#26032;&#30340;&#24212;&#29992;&#36884;&#24452;&#12290;&#23427;&#31361;&#26174;&#20102;&#36739;&#23567;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#39640;&#32423;&#29702;&#35299;&#21644;&#20132;&#20114;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce LLaVA-$\phi$ (LLaVA-Phi), an efficient multi-modal assistant that harnesses the power of the recently advanced small language model, Phi-2, to facilitate multi-modal dialogues. LLaVA-Phi marks a notable advancement in the realm of compact multi-modal models. It demonstrates that even smaller language models, with as few as 2.7B parameters, can effectively engage in intricate dialogues that integrate both textual and visual elements, provided they are trained with high-quality corpora. Our model delivers commendable performance on publicly available benchmarks that encompass visual comprehension, reasoning, and knowledge-based perception. Beyond its remarkable performance in multi-modal dialogue tasks, our model opens new avenues for applications in time-sensitive environments and systems that require real-time interaction, such as embodied agents. It highlights the potential of smaller language models to achieve sophisticated levels of understanding and inte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;LLMs&#22312;&#21475;&#35821;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#25253;&#21578;&#20102;&#22238;&#22797;&#29983;&#25104;&#21644;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#20004;&#20010;&#23376;&#20219;&#21153;&#20013;&#30340;&#27169;&#22411;&#34920;&#29616;&#12290;&#30001;&#20110;&#32570;&#20047;&#36866;&#24403;&#30340;&#21475;&#35821;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#20351;&#29992;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#24341;&#25806;&#36716;&#24405;&#20102;&#21475;&#35821;&#23545;&#35805;&#30340;&#24320;&#21457;&#38598;&#65292;&#24182;&#27169;&#25311;&#20102;ASR&#38169;&#35823;&#20197;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2401.02297</link><description>&lt;p&gt;
&#23545;&#35805;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21475;&#35821;&#23545;&#35805;&#20013;&#30340;&#40065;&#26834;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Are LLMs Robust for Spoken Dialogues?. (arXiv:2401.02297v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02297
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;LLMs&#22312;&#21475;&#35821;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#25253;&#21578;&#20102;&#22238;&#22797;&#29983;&#25104;&#21644;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#20004;&#20010;&#23376;&#20219;&#21153;&#20013;&#30340;&#27169;&#22411;&#34920;&#29616;&#12290;&#30001;&#20110;&#32570;&#20047;&#36866;&#24403;&#30340;&#21475;&#35821;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#20351;&#29992;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#24341;&#25806;&#36716;&#24405;&#20102;&#21475;&#35821;&#23545;&#35805;&#30340;&#24320;&#21457;&#38598;&#65292;&#24182;&#27169;&#25311;&#20102;ASR&#38169;&#35823;&#20197;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#21644;&#31471;&#21040;&#31471;&#22238;&#22797;&#29983;&#25104;&#31561;&#19981;&#21516;&#19979;&#28216;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20844;&#24320;&#21487;&#29992;&#30340;&#38754;&#21521;&#20219;&#21153;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#37117;&#38598;&#20013;&#22312;&#20070;&#38754;&#23545;&#35805;&#19978;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23545;&#24320;&#21457;&#30340;&#27169;&#22411;&#22312;&#21475;&#35821;&#20132;&#20114;&#20013;&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#30001;&#20110;&#32570;&#20047;&#36866;&#24403;&#30340;&#21475;&#35821;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#26368;&#20808;&#36827;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#24341;&#25806;&#33258;&#21160;&#36716;&#24405;&#20102;&#19968;&#20010;&#21475;&#35821;&#23545;&#35805;&#30340;&#24320;&#21457;&#38598;&#12290;&#25105;&#20204;&#23545;ASR&#38169;&#35823;&#31867;&#22411;&#21450;&#20854;&#20998;&#24067;&#36827;&#34892;&#20102;&#29305;&#24449;&#21270;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#20013;&#27169;&#25311;&#20102;&#36825;&#20123;&#38169;&#35823;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;GPT-2&#21644;T5&#27169;&#22411;&#22312;&#22238;&#22797;&#29983;&#25104;&#21644;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#20004;&#20010;&#23376;&#20219;&#21153;&#20013;&#30340;&#20869;&#22312;&#65288;&#22256;&#24785;&#24230;&#65289;&#21644;&#22806;&#22312;&#65288;&#20154;&#24037;&#35780;&#20272;&#65289;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Pre-Trained Language Models have demonstrated state-of-the-art performance in different downstream tasks, including dialogue state tracking and end-to-end response generation. Nevertheless, most of the publicly available datasets and benchmarks on task-oriented dialogues focus on written conversations. Consequently, the robustness of the developed models to spoken interactions is unknown. In this work, we have evaluated the performance of LLMs for spoken task-oriented dialogues on the DSTC11 test sets. Due to the lack of proper spoken dialogue datasets, we have automatically transcribed a development set of spoken dialogues with a state-of-the-art ASR engine. We have characterized the ASR-error types and their distributions and simulated these errors in a large dataset of dialogues. We report the intrinsic (perplexity) and extrinsic (human evaluation) performance of fine-tuned GPT-2 and T5 models in two subtasks of response generation and dialogue state tracking, respectively. Th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#32771;&#34385;&#20102;&#20174;&#23545;&#35805;&#21442;&#19982;&#32773;&#30340;&#35282;&#24230;&#23545;&#24320;&#25918;&#22495;&#23545;&#35805;&#31995;&#32479;&#36827;&#34892;&#21709;&#24212;&#35780;&#20272;&#30340;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#20102;&#23545;&#35805;&#21442;&#19982;&#32773;&#24847;&#35782;&#21644;&#23545;&#35805;&#36830;&#36143;&#24615;&#22312;&#33258;&#21160;&#35780;&#20272;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.02256</link><description>&lt;p&gt;
&#37325;&#26032;&#32771;&#34385;&#20174;&#23545;&#35805;&#21442;&#19982;&#32773;&#30340;&#35282;&#24230;&#23545;&#24320;&#25918;&#22495;&#23545;&#35805;&#31995;&#32479;&#36827;&#34892;&#21709;&#24212;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Rethinking Response Evaluation from Interlocutor's Eye for Open-Domain Dialogue Systems. (arXiv:2401.02256v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#32771;&#34385;&#20102;&#20174;&#23545;&#35805;&#21442;&#19982;&#32773;&#30340;&#35282;&#24230;&#23545;&#24320;&#25918;&#22495;&#23545;&#35805;&#31995;&#32479;&#36827;&#34892;&#21709;&#24212;&#35780;&#20272;&#30340;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#20102;&#23545;&#35805;&#21442;&#19982;&#32773;&#24847;&#35782;&#21644;&#23545;&#35805;&#36830;&#36143;&#24615;&#22312;&#33258;&#21160;&#35780;&#20272;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#22495;&#23545;&#35805;&#31995;&#32479;&#24050;&#32463;&#24320;&#22987;&#19982;&#20154;&#31867;&#36827;&#34892;&#36830;&#32493;&#23545;&#35805;&#12290;&#36825;&#20123;&#23545;&#35805;&#31995;&#32479;&#38656;&#35201;&#26681;&#25454;&#20154;&#31867;&#23545;&#35805;&#32773;&#36827;&#34892;&#35843;&#25972;&#65292;&#24182;&#20197;&#20854;&#35282;&#24230;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#26159;&#21542;&#33021;&#22815;&#36817;&#20284;&#23545;&#35805;&#21442;&#19982;&#32773;&#30340;&#21028;&#26029;&#26159;&#23384;&#30097;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#21644;&#30740;&#31350;&#20102;&#20174;&#23545;&#35805;&#21442;&#19982;&#32773;&#35282;&#24230;&#38656;&#35201;&#21738;&#20123;&#29305;&#24449;&#30340;&#33258;&#21160;&#21709;&#24212;&#35780;&#20272;&#22120;&#12290;&#31532;&#19968;&#27425;&#22312;Hazumi&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#23545;&#35805;&#21442;&#19982;&#32773;&#24847;&#35782;&#22312;&#20351;&#33258;&#21160;&#21709;&#24212;&#35780;&#20272;&#19982;&#23545;&#35805;&#21442;&#19982;&#32773;&#21028;&#26029;&#30456;&#20851;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#31532;&#20108;&#20010;&#23454;&#39564;&#20351;&#29992;&#22823;&#35268;&#27169;&#23545;&#35805;&#65288;&#21069;&#31216;Twitter&#65289;&#30830;&#35748;&#20102;&#23545;&#35805;&#36830;&#36143;&#24615;&#39044;&#27979;&#21487;&#20197;&#35757;&#32451;&#20986;&#19968;&#20010;&#20855;&#26377;&#23545;&#35805;&#21442;&#19982;&#32773;&#24847;&#35782;&#30340;&#21709;&#24212;&#35780;&#20272;&#22120;&#65292;&#32780;&#30456;&#27604;&#20154;&#31867;&#21709;&#24212;&#65292;&#35780;&#20272;&#29983;&#25104;&#30340;&#21709;&#24212;&#30340;&#38590;&#24230;&#26356;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open-domain dialogue systems have started to engage in continuous conversations with humans. Those dialogue systems are required to be adjusted to the human interlocutor and evaluated in terms of their perspective. However, it is questionable whether the current automatic evaluation methods can approximate the interlocutor's judgments. In this study, we analyzed and examined what features are needed in an automatic response evaluator from the interlocutor's perspective. The first experiment on the Hazumi dataset revealed that interlocutor awareness plays a critical role in making automatic response evaluation correlate with the interlocutor's judgments. The second experiment using massive conversations on X (formerly Twitter) confirmed that dialogue continuity prediction can train an interlocutor-aware response evaluator without human feedback while revealing the difficulty in evaluating generated responses compared to human responses.
&lt;/p&gt;</description></item><item><title>L3Cube-IndicNews&#26159;&#19968;&#20010;&#38754;&#21521;&#21360;&#24230;&#35821;&#31995;&#30340;&#22810;&#35821;&#31181;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#30701;&#26631;&#39064;&#12289;&#38271;&#25991;&#26723;&#21644;&#38271;&#27573;&#33853;&#19977;&#20010;&#25968;&#25454;&#38598;&#12290;&#23427;&#25552;&#20379;&#20102;10&#31181;&#21360;&#24230;&#35821;&#35328;&#30340;&#26032;&#38395;&#25991;&#31456;&#65292;&#27599;&#20010;&#25968;&#25454;&#38598;&#21253;&#21547;10&#20010;&#25110;&#26356;&#22810;&#31867;&#21035;&#30340;&#25991;&#31456;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21487;&#20197;&#29992;&#20110;&#28145;&#20837;&#20998;&#26512;&#21644;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2401.02254</link><description>&lt;p&gt;
L3Cube-IndicNews&#65306;&#21360;&#24230;&#35821;&#31995;&#26032;&#38395;&#30701;&#25991;&#21644;&#38271;&#25991;&#20998;&#31867;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
L3Cube-IndicNews: News-based Short Text and Long Document Classification Datasets in Indic Languages. (arXiv:2401.02254v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02254
&lt;/p&gt;
&lt;p&gt;
L3Cube-IndicNews&#26159;&#19968;&#20010;&#38754;&#21521;&#21360;&#24230;&#35821;&#31995;&#30340;&#22810;&#35821;&#31181;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#30701;&#26631;&#39064;&#12289;&#38271;&#25991;&#26723;&#21644;&#38271;&#27573;&#33853;&#19977;&#20010;&#25968;&#25454;&#38598;&#12290;&#23427;&#25552;&#20379;&#20102;10&#31181;&#21360;&#24230;&#35821;&#35328;&#30340;&#26032;&#38395;&#25991;&#31456;&#65292;&#27599;&#20010;&#25968;&#25454;&#38598;&#21253;&#21547;10&#20010;&#25110;&#26356;&#22810;&#31867;&#21035;&#30340;&#25991;&#31456;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21487;&#20197;&#29992;&#20110;&#28145;&#20837;&#20998;&#26512;&#21644;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;L3Cube-IndicNews&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#35821;&#31181;&#25991;&#26412;&#20998;&#31867;&#35821;&#26009;&#24211;&#65292;&#26088;&#22312;&#20026;&#21360;&#24230;&#22320;&#21306;&#30340;&#21508;&#22823;&#26041;&#35328;&#35821;&#35328;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#65292;&#29305;&#21035;&#20851;&#27880;&#26032;&#38395;&#26631;&#39064;&#21644;&#25991;&#31456;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;10&#31181;&#20027;&#35201;&#30340;&#21360;&#24230;&#35821;&#35328;&#19978;&#65292;&#21253;&#25324;&#21360;&#22320;&#35821;&#12289;&#23391;&#21152;&#25289;&#35821;&#12289;&#39532;&#25289;&#22320;&#35821;&#12289;&#27888;&#21346;&#22266;&#35821;&#12289;&#27888;&#31859;&#23572;&#35821;&#12289;&#21476;&#21513;&#25289;&#29305;&#35821;&#12289;&#21345;&#32435;&#36798;&#35821;&#12289;&#22885;&#37324;&#20122;&#35821;&#12289;&#39532;&#25289;&#38597;&#25289;&#22982;&#35821;&#21644;&#26049;&#36974;&#26222;&#35821;&#12290;&#27599;&#20010;&#26032;&#38395;&#25968;&#25454;&#38598;&#21253;&#21547;10&#20010;&#25110;&#26356;&#22810;&#31867;&#21035;&#30340;&#26032;&#38395;&#25991;&#31456;&#12290;L3Cube-IndicNews&#25552;&#20379;&#20102;3&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#65292;&#38024;&#23545;&#19981;&#21516;&#30340;&#25991;&#26723;&#38271;&#24230;&#36827;&#34892;&#20998;&#31867;&#65306;&#30701;&#26631;&#39064;&#20998;&#31867;&#65288;SHC&#65289;&#25968;&#25454;&#38598;&#21253;&#21547;&#26032;&#38395;&#26631;&#39064;&#21644;&#26032;&#38395;&#31867;&#21035;&#65292;&#38271;&#25991;&#26723;&#20998;&#31867;&#65288;LDC&#65289;&#25968;&#25454;&#38598;&#21253;&#21547;&#25972;&#20010;&#26032;&#38395;&#25991;&#31456;&#21644;&#26032;&#38395;&#31867;&#21035;&#65292;&#38271;&#27573;&#33853;&#20998;&#31867;&#65288;LPC&#65289;&#25968;&#25454;&#38598;&#21253;&#21547;&#26032;&#38395;&#30340;&#23376;&#25991;&#31456;&#21644;&#26032;&#38395;&#31867;&#21035;&#12290;&#25105;&#20204;&#22312;&#25152;&#26377;3&#20010;&#25968;&#25454;&#38598;&#20013;&#37117;&#20445;&#25345;&#20102;&#19968;&#33268;&#30340;&#26631;&#31614;&#65292;&#20197;&#36827;&#34892;&#28145;&#20837;&#30340;&#22522;&#20110;&#38271;&#24230;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#20351;&#29992;4&#20010;&#25351;&#26631;&#23545;&#27599;&#20010;&#21360;&#24230;&#35821;&#35328;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce L3Cube-IndicNews, a multilingual text classification corpus aimed at curating a high-quality dataset for Indian regional languages, with a specific focus on news headlines and articles. We have centered our work on 10 prominent Indic languages, including Hindi, Bengali, Marathi, Telugu, Tamil, Gujarati, Kannada, Odia, Malayalam, and Punjabi. Each of these news datasets comprises 10 or more classes of news articles. L3Cube-IndicNews offers 3 distinct datasets tailored to handle different document lengths that are classified as: Short Headlines Classification (SHC) dataset containing the news headline and news category, Long Document Classification (LDC) dataset containing the whole news article and the news category, and Long Paragraph Classification (LPC) containing sub-articles of the news and the news category. We maintain consistent labeling across all 3 datasets for in-depth length-based analysis. We evaluate each of these Indic language datasets using 4 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#32852;&#21512;&#22810;&#20107;&#23454;&#25512;&#29702;&#32593;&#32476;&#65288;JMFRN&#65289;&#29992;&#20110;&#22797;&#26434;&#26102;&#24577;&#38382;&#39064;&#22312;&#30693;&#35782;&#22270;&#19978;&#30340;&#38382;&#31572;&#65292;&#36890;&#36807;&#32858;&#21512;&#23454;&#20307;&#21644;&#26102;&#38388;&#25139;&#20449;&#24687;&#26469;&#20934;&#30830;&#22238;&#31572;&#22797;&#26434;&#26102;&#24577;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.02212</link><description>&lt;p&gt;
&#32852;&#21512;&#22810;&#20107;&#23454;&#25512;&#29702;&#32593;&#32476;&#29992;&#20110;&#22797;&#26434;&#26102;&#24577;&#38382;&#39064;&#22312;&#30693;&#35782;&#22270;&#19978;&#30340;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Joint Multi-Facts Reasoning Network For Complex Temporal Question Answering Over Knowledge Graph. (arXiv:2401.02212v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#32852;&#21512;&#22810;&#20107;&#23454;&#25512;&#29702;&#32593;&#32476;&#65288;JMFRN&#65289;&#29992;&#20110;&#22797;&#26434;&#26102;&#24577;&#38382;&#39064;&#22312;&#30693;&#35782;&#22270;&#19978;&#30340;&#38382;&#31572;&#65292;&#36890;&#36807;&#32858;&#21512;&#23454;&#20307;&#21644;&#26102;&#38388;&#25139;&#20449;&#24687;&#26469;&#20934;&#30830;&#22238;&#31572;&#22797;&#26434;&#26102;&#24577;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#30693;&#35782;&#22270;&#65288;TKG&#65289;&#26159;&#22312;&#24120;&#35268;&#30693;&#35782;&#22270;&#30340;&#22522;&#30784;&#19978;&#21152;&#20837;&#26102;&#38388;&#33539;&#22260;&#30340;&#25193;&#23637;&#12290;&#29616;&#26377;&#30340;&#26102;&#38388;&#30693;&#35782;&#22270;&#38382;&#31572;&#65288;TKGQA&#65289;&#27169;&#22411;&#20165;&#22788;&#29702;&#31616;&#21333;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#20808;&#21069;&#20551;&#35774;&#27599;&#20010;&#38382;&#39064;&#21482;&#21253;&#21547;&#19968;&#20010;&#20855;&#26377;&#26174;&#24335;/&#38544;&#24335;&#26102;&#38388;&#32422;&#26463;&#30340;&#26102;&#38388;&#20107;&#23454;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#23545;&#20110;&#20855;&#26377;&#22810;&#20010;&#26102;&#38388;&#20107;&#23454;&#30340;&#38382;&#39064;&#34920;&#29616;&#36739;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#21512;&#22810;&#20107;&#23454;&#25512;&#29702;&#32593;&#32476;&#65288;JMFRN&#65289;&#65292;&#29992;&#20110;&#20934;&#30830;&#22238;&#31572;&#22797;&#26434;&#26102;&#24577;&#38382;&#39064;&#12290;&#20855;&#20307;&#22320;&#65292;JMFRN&#39318;&#20808;&#20174;TKG&#20013;&#26816;&#32034;&#19982;&#32473;&#23450;&#22797;&#26434;&#38382;&#39064;&#30340;&#27599;&#20010;&#23454;&#20307;&#30456;&#20851;&#30340;&#26102;&#38388;&#20107;&#23454;&#12290;&#20026;&#20102;&#36827;&#34892;&#32852;&#21512;&#25512;&#29702;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#20010;&#19981;&#21516;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;&#65288;&#21363;&#23454;&#20307;&#24863;&#30693;&#21644;&#26102;&#38388;&#24863;&#30693;&#65289;&#65292;&#36866;&#29992;&#20110;&#36890;&#29992;&#35774;&#32622;&#65292;&#20197;&#32858;&#21512;&#23454;&#20307;&#21644;&#26102;&#38388;&#25139;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal Knowledge Graph (TKG) is an extension of regular knowledge graph by attaching the time scope. Existing temporal knowledge graph question answering (TKGQA) models solely approach simple questions, owing to the prior assumption that each question only contains a single temporal fact with explicit/implicit temporal constraints. Hence, they perform poorly on questions which own multiple temporal facts. In this paper, we propose \textbf{\underline{J}}oint \textbf{\underline{M}}ulti \textbf{\underline{F}}acts \textbf{\underline{R}}easoning \textbf{\underline{N}}etwork (JMFRN), to jointly reasoning multiple temporal facts for accurately answering \emph{complex} temporal questions. Specifically, JMFRN first retrieves question-related temporal facts from TKG for each entity of the given complex question. For joint reasoning, we design two different attention (\ie entity-aware and time-aware) modules, which are suitable for universal settings, to aggregate entities and timestamps inform
&lt;/p&gt;</description></item><item><title>DIALIGHT&#26159;&#19968;&#20010;&#29992;&#20110;&#24320;&#21457;&#21644;&#35780;&#20272;&#22810;&#35821;&#35328;&#30340;&#38754;&#21521;&#20219;&#21153;&#30340;&#23545;&#35805;&#31995;&#32479;&#30340;&#24037;&#20855;&#21253;&#65292;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#21644;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;-shot&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#21487;&#20197;&#36827;&#34892;&#31995;&#32479;&#21270;&#30340;&#35780;&#20272;&#21644;&#27604;&#36739;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;PLM&#24494;&#35843;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#36830;&#36143;&#24615;&#65292;&#32780;LLM&#31995;&#32479;&#22312;&#20135;&#29983;&#22810;&#26679;&#21270;&#21644;&#21463;&#27426;&#36814;&#30340;&#22238;&#22797;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#38656;&#35201;&#35299;&#20915;LLM&#22312;&#36981;&#24490;&#20219;&#21153;&#29305;&#23450;&#25351;&#20196;&#21644;&#29983;&#25104;&#22810;&#35821;&#35328;&#36755;&#20986;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.02208</link><description>&lt;p&gt;
DIALIGHT&#65306;&#36731;&#37327;&#32423;&#22810;&#35821;&#35328;&#24320;&#21457;&#21644;&#35780;&#20272;&#20197;&#22823;&#35821;&#35328;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#38754;&#21521;&#20219;&#21153;&#23545;&#35805;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
DIALIGHT: Lightweight Multilingual Development and Evaluation of Task-Oriented Dialogue Systems with Large Language Models. (arXiv:2401.02208v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02208
&lt;/p&gt;
&lt;p&gt;
DIALIGHT&#26159;&#19968;&#20010;&#29992;&#20110;&#24320;&#21457;&#21644;&#35780;&#20272;&#22810;&#35821;&#35328;&#30340;&#38754;&#21521;&#20219;&#21153;&#30340;&#23545;&#35805;&#31995;&#32479;&#30340;&#24037;&#20855;&#21253;&#65292;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#21644;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;-shot&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#21487;&#20197;&#36827;&#34892;&#31995;&#32479;&#21270;&#30340;&#35780;&#20272;&#21644;&#27604;&#36739;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;PLM&#24494;&#35843;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#36830;&#36143;&#24615;&#65292;&#32780;LLM&#31995;&#32479;&#22312;&#20135;&#29983;&#22810;&#26679;&#21270;&#21644;&#21463;&#27426;&#36814;&#30340;&#22238;&#22797;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#38656;&#35201;&#35299;&#20915;LLM&#22312;&#36981;&#24490;&#20219;&#21153;&#29305;&#23450;&#25351;&#20196;&#21644;&#29983;&#25104;&#22810;&#35821;&#35328;&#36755;&#20986;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;DIALIGHT&#65292;&#36825;&#26159;&#19968;&#20010;&#24037;&#20855;&#21253;&#65292;&#29992;&#20110;&#24320;&#21457;&#21644;&#35780;&#20272;&#22810;&#35821;&#35328;&#30340;&#38754;&#21521;&#20219;&#21153;&#30340;&#23545;&#35805;&#31995;&#32479;&#12290;&#23427;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#36827;&#34892;&#24494;&#35843;&#21644;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#38646;-shot&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#20351;&#24471;&#31995;&#32479;&#35780;&#20272;&#21644;&#27604;&#36739;&#26356;&#21152;&#31995;&#32479;&#21270;&#12290;&#38500;&#20102;&#33258;&#21160;&#35780;&#20272;&#22806;&#65292;&#35813;&#24037;&#20855;&#21253;&#36824;&#20855;&#26377;&#65288;i&#65289;&#19968;&#20010;&#23433;&#20840;&#12289;&#29992;&#25143;&#21451;&#22909;&#30340;&#32593;&#32476;&#30028;&#38754;&#65292;&#29992;&#20110;&#22312;&#26412;&#22320;&#25991;&#26412;&#32423;&#21644;&#20840;&#23616;&#23545;&#35805;&#32423;&#36827;&#34892;&#32454;&#31890;&#24230;&#30340;&#20154;&#24037;&#35780;&#20272;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#22522;&#20110;&#24494;&#26381;&#21153;&#30340;&#21518;&#31471;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;PLM&#24494;&#35843;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#36830;&#36143;&#24615;&#65292;&#32780;&#22522;&#20110;LLM&#30340;&#31995;&#32479;&#22312;&#20135;&#29983;&#22810;&#26679;&#21270;&#21644;&#21463;&#27426;&#36814;&#30340;&#22238;&#22797;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#20063;&#21457;&#29616;LLM&#22312;&#36981;&#24490;&#20219;&#21153;&#29305;&#23450;&#25351;&#20196;&#21644;&#29983;&#25104;&#22810;&#35821;&#35328;&#36755;&#20986;&#26041;&#38754;&#23384;&#22312;&#37325;&#35201;&#25361;&#25112;&#65292;&#36825;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25351;&#26126;&#20102;&#26041;&#21521;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#20010;&#24320;&#28304;&#24037;&#20855;&#21253;&#33021;&#22815;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#26356;&#22909;&#22320;&#24320;&#21457;&#21644;&#35780;&#20272;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present DIALIGHT, a toolkit for developing and evaluating multilingual Task-Oriented Dialogue (ToD) systems which facilitates systematic evaluations and comparisons between ToD systems using fine-tuning of Pretrained Language Models (PLMs) and those utilising the zero-shot and in-context learning capabilities of Large Language Models (LLMs). In addition to automatic evaluation, this toolkit features (i) a secure, user-friendly web interface for fine-grained human evaluation at both local utterance level and global dialogue level, and (ii) a microservice-based backend, improving efficiency and scalability. Our evaluations reveal that while PLM fine-tuning leads to higher accuracy and coherence, LLM-based systems excel in producing diverse and likeable responses. However, we also identify significant challenges of LLMs in adherence to task-specific instructions and generating outputs in multiple languages, highlighting areas for future research. We hope this open-sourced toolkit will 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20301;&#32622;&#24863;&#30693;&#27169;&#22359;&#21270;&#21452;&#32534;&#30721;&#22120;&#26469;&#22238;&#31572;&#30495;&#23454;&#19990;&#30028;&#26053;&#28216;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#23884;&#20837;&#31354;&#38388;&#30456;&#20284;&#24615;&#65292;&#23558;QA&#20219;&#21153;&#35270;&#20026;&#31264;&#23494;&#21521;&#37327;&#26816;&#32034;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#25152;&#26377;&#25351;&#26631;&#19978;&#37117;&#26377;&#25928;&#12289;&#39640;&#25928;&#65292;&#24182;&#19988;&#20248;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.02187</link><description>&lt;p&gt;
&#26053;&#28216;&#38382;&#31572;&#30340;&#20301;&#32622;&#24863;&#30693;&#27169;&#22359;&#21270;&#21452;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Location Aware Modular Biencoder for Tourism Question Answering. (arXiv:2401.02187v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02187
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20301;&#32622;&#24863;&#30693;&#27169;&#22359;&#21270;&#21452;&#32534;&#30721;&#22120;&#26469;&#22238;&#31572;&#30495;&#23454;&#19990;&#30028;&#26053;&#28216;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#23884;&#20837;&#31354;&#38388;&#30456;&#20284;&#24615;&#65292;&#23558;QA&#20219;&#21153;&#35270;&#20026;&#31264;&#23494;&#21521;&#37327;&#26816;&#32034;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#25152;&#26377;&#25351;&#26631;&#19978;&#37117;&#26377;&#25928;&#12289;&#39640;&#25928;&#65292;&#24182;&#19988;&#20248;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22238;&#31572;&#23547;&#25214;&#20852;&#36259;&#28857;&#65288;POI&#65289;&#25512;&#33616;&#30340;&#30495;&#23454;&#19990;&#30028;&#26053;&#28216;&#38382;&#39064;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#23545;&#22823;&#37327;&#20505;&#36873;&#39033;&#36827;&#34892;&#31354;&#38388;&#21644;&#38750;&#31354;&#38388;&#25512;&#29702;&#12290;&#24403;&#20505;&#36873;&#39033;&#25968;&#37327;&#22686;&#21152;&#26102;&#65292;&#20256;&#32479;&#30340;&#23545;&#27599;&#23545;&#38382;&#39064;&#21644;POI&#36827;&#34892;&#32534;&#30721;&#30340;&#26041;&#27861;&#25928;&#29575;&#38477;&#20302;&#65292;&#20351;&#24471;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#19981;&#21487;&#34892;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;QA&#20219;&#21153;&#35270;&#20026;&#19968;&#20010;&#31264;&#23494;&#21521;&#37327;&#26816;&#32034;&#38382;&#39064;&#65292;&#20854;&#20013;&#25105;&#20204;&#20998;&#21035;&#23545;&#38382;&#39064;&#21644;POI&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#23884;&#20837;&#31354;&#38388;&#30456;&#20284;&#24615;&#20026;&#38382;&#39064;&#26816;&#32034;&#26368;&#30456;&#20851;&#30340;POI&#12290;&#25105;&#20204;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#26469;&#32534;&#30721;&#25991;&#26412;&#20449;&#24687;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#20301;&#32622;&#32534;&#30721;&#22120;&#26469;&#25429;&#25417;POI&#30340;&#31354;&#38388;&#20449;&#24687;&#12290;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#26053;&#28216;QA&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25152;&#26377;&#25351;&#26631;&#19978;&#37117;&#26159;&#26377;&#25928;&#12289;&#39640;&#25928;&#30340;&#65292;&#24182;&#19988;&#20248;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#12290;&#20511;&#21161;&#31264;&#23494;&#26816;&#32034;&#26550;&#26500;&#30340;&#25903;&#25345;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24314;&#31435;&#20102;&#19968;&#20010;&#20840;&#29699;&#35780;&#20272;&#22522;&#20934;&#65292;&#25193;&#23637;&#20102;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Answering real-world tourism questions that seek Point-of-Interest (POI) recommendations is challenging, as it requires both spatial and non-spatial reasoning, over a large candidate pool. The traditional method of encoding each pair of question and POI becomes inefficient when the number of candidates increases, making it infeasible for real-world applications. To overcome this, we propose treating the QA task as a dense vector retrieval problem, where we encode questions and POIs separately and retrieve the most relevant POIs for a question by utilizing embedding space similarity. We use pretrained language models (PLMs) to encode textual information, and train a location encoder to capture spatial information of POIs. Experiments on a real-world tourism QA dataset demonstrate that our approach is effective, efficient, and outperforms previous methods across all metrics. Enabled by the dense retrieval architecture, we further build a global evaluation baseline, expanding the search s
&lt;/p&gt;</description></item><item><title>Shayona&#22242;&#38431;&#22312;SMMH4-23&#20013;&#20351;&#29992;&#20102;BERT&#21644;LightGBM&#27169;&#22411;&#36827;&#34892;COVID-19&#33258;&#25105;&#35786;&#26029;&#20998;&#31867;&#65292;&#24182;&#22312;&#20219;&#21153;1&#20013;&#21462;&#24471;&#20102;&#26368;&#39640;&#30340;F1&#20998;&#25968;0.94&#12290;</title><link>http://arxiv.org/abs/2401.02158</link><description>&lt;p&gt;
Shayona@SMM4H23&#65306;&#20351;&#29992;BERT&#21644;LightGBM&#27169;&#22411;&#36827;&#34892;COVID-19&#33258;&#25105;&#35786;&#26029;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Shayona@SMM4H23: COVID-19 Self diagnosis classification using BERT and LightGBM models. (arXiv:2401.02158v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02158
&lt;/p&gt;
&lt;p&gt;
Shayona&#22242;&#38431;&#22312;SMMH4-23&#20013;&#20351;&#29992;&#20102;BERT&#21644;LightGBM&#27169;&#22411;&#36827;&#34892;COVID-19&#33258;&#25105;&#35786;&#26029;&#20998;&#31867;&#65292;&#24182;&#22312;&#20219;&#21153;1&#20013;&#21462;&#24471;&#20102;&#26368;&#39640;&#30340;F1&#20998;&#25968;0.94&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;Shayona&#22242;&#38431;&#22312;SMMH4-23&#30340;&#20849;&#20139;&#20219;&#21153;1&#21644;4&#20013;&#30340;&#26041;&#27861;&#21644;&#32467;&#26524;&#12290;&#20849;&#20139;&#20219;&#21153;1&#26159;&#23545;&#33258;&#25253;COVID-19&#35786;&#26029;&#30340;&#33521;&#25991;&#25512;&#25991;&#36827;&#34892;&#20108;&#20998;&#31867;&#65292;&#20849;&#20139;&#20219;&#21153;4&#26159;&#23545;&#33258;&#25253;&#31038;&#20132;&#28966;&#34385;&#38556;&#30861;&#35786;&#26029;&#30340;&#33521;&#25991;Reddit&#24086;&#23376;&#36827;&#34892;&#20108;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#22242;&#38431;&#22312;&#20219;&#21153;1&#20013;&#21462;&#24471;&#20102;&#25152;&#26377;&#21442;&#19982;&#32773;&#20013;&#26368;&#39640;&#30340;F1&#20998;&#25968;0.94&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20219;&#21153;&#20013;&#37117;&#20351;&#29992;&#20102;Transformer&#27169;&#22411;&#65288;BERT&#65289;&#21644;LightGBM&#27169;&#22411;&#36827;&#34892;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes approaches and results for shared Task 1 and 4 of SMMH4-23 by Team Shayona. Shared Task-1 was binary classification of english tweets self-reporting a COVID-19 diagnosis, and Shared Task-4 was Binary classification of English Reddit posts self-reporting a social anxiety disorder diagnosis. Our team has achieved the highest f1-score 0.94 in Task-1 among all participants. We have leveraged the Transformer model (BERT) in combination with the LightGBM model for both tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#28023;&#27915;&#20998;&#26512;&#30340;&#35282;&#24230;&#23545;GPT-4V&#36827;&#34892;&#20102;&#21021;&#27493;&#21644;&#20840;&#38754;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#20854;&#22312;&#28023;&#27915;&#30740;&#31350;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#21457;&#23637;&#35774;&#23450;&#20102;&#26032;&#30340;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2401.02147</link><description>&lt;p&gt;
&#25506;&#32034;GPT-4V&#22312;&#28023;&#27915;&#20998;&#26512;&#39046;&#22495;&#30340;&#36793;&#30028;&#65306;&#19968;&#20010;&#21021;&#27493;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Exploring Boundary of GPT-4V on Marine Analysis: A Preliminary Case Study. (arXiv:2401.02147v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#28023;&#27915;&#20998;&#26512;&#30340;&#35282;&#24230;&#23545;GPT-4V&#36827;&#34892;&#20102;&#21021;&#27493;&#21644;&#20840;&#38754;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#20854;&#22312;&#28023;&#27915;&#30740;&#31350;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#21457;&#23637;&#35774;&#23450;&#20102;&#26032;&#30340;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#23637;&#31034;&#20986;&#20316;&#20026;&#36890;&#29992;&#21161;&#25163;&#22238;&#31572;&#21508;&#31181;&#26597;&#35810;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#36830;&#32493;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#36171;&#20104;LLMs&#24863;&#30693;&#35270;&#35273;&#20449;&#21495;&#30340;&#33021;&#21147;&#12290;GPT-4&#65288;&#29983;&#25104;&#39044;&#35757;&#32451;&#30340;Transformer&#65289;&#30340;&#21457;&#24067;&#24341;&#36215;&#20102;&#30740;&#31350;&#30028;&#30340;&#26497;&#22823;&#20852;&#36259;&#12290;&#20316;&#20026;&#26032;&#19968;&#20195;&#20154;&#24037;&#26234;&#33021;&#30340;&#28966;&#28857;&#20043;&#19968;&#65292;GPT-4V&#65288;ison&#65289;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#39046;&#22495;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#23454;&#21147;&#12290;&#23613;&#31649;GPT-4V&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#25506;&#32034;&#22312;&#38656;&#35201;&#39046;&#22495;&#19987;&#38376;&#30693;&#35782;&#21644;&#19987;&#19994;&#25216;&#33021;&#30340;&#29305;&#23450;&#39046;&#22495;&#20998;&#26512;&#65288;&#22914;&#28023;&#27915;&#20998;&#26512;&#65289;&#20013;&#30340;MLLMs&#21364;&#21463;&#21040;&#20102;&#36739;&#23569;&#20851;&#27880;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20851;&#20110;&#21033;&#29992;GPT-4V&#36827;&#34892;&#28023;&#27915;&#20998;&#26512;&#30340;&#21021;&#27493;&#21644;&#32508;&#21512;&#26696;&#20363;&#30740;&#31350;&#12290;&#26412;&#25253;&#21578;&#23545;&#29616;&#26377;&#30340;GPT-4V&#36827;&#34892;&#20102;&#31995;&#32479;&#35780;&#20272;&#65292;&#35780;&#20272;&#20102;GPT-4V&#22312;&#28023;&#27915;&#30740;&#31350;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#21457;&#23637;&#35774;&#23450;&#20102;&#26032;&#30340;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated a powerful ability to answer various queries as a general-purpose assistant. The continuous multi-modal large language models (MLLM) empower LLMs with the ability to perceive visual signals. The launch of GPT-4 (Generative Pre-trained Transformers) has generated significant interest in the research communities. GPT-4V(ison) has demonstrated significant power in both academia and industry fields, as a focal point in a new artificial intelligence generation. Though significant success was achieved by GPT-4V, exploring MLLMs in domain-specific analysis (e.g., marine analysis) that required domain-specific knowledge and expertise has gained less attention. In this study, we carry out the preliminary and comprehensive case study of utilizing GPT-4V for marine analysis. This report conducts a systematic evaluation of existing GPT-4V, assessing the performance of GPT-4V on marine research and also setting a new standard for future developments in
&lt;/p&gt;</description></item><item><title>DCR-Consistency&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21010;&#20998;-&#24449;&#26381;-&#25512;&#29702;&#26041;&#27861;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#30340;&#19968;&#33268;&#24615;&#12290;&#19982;&#20256;&#32479;&#30340;&#35780;&#20272;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#27573;&#33853;&#23545;&#27573;&#33853;&#27604;&#36739;&#21010;&#20998;&#20026;&#21477;&#23376;&#23545;&#27573;&#33853;&#30340;&#27604;&#36739;&#65292;&#24182;&#26681;&#25454;&#39044;&#23450;&#20041;&#26631;&#20934;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2401.02132</link><description>&lt;p&gt;
DCR-Consistency: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19968;&#33268;&#24615;&#35780;&#20272;&#21644;&#25913;&#36827;&#30340;&#21010;&#20998;-&#24449;&#26381;-&#25512;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DCR-Consistency: Divide-Conquer-Reasoning for Consistency Evaluation and Improvement of Large Language Models. (arXiv:2401.02132v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02132
&lt;/p&gt;
&lt;p&gt;
DCR-Consistency&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21010;&#20998;-&#24449;&#26381;-&#25512;&#29702;&#26041;&#27861;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#30340;&#19968;&#33268;&#24615;&#12290;&#19982;&#20256;&#32479;&#30340;&#35780;&#20272;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#27573;&#33853;&#23545;&#27573;&#33853;&#27604;&#36739;&#21010;&#20998;&#20026;&#21477;&#23376;&#23545;&#27573;&#33853;&#30340;&#27604;&#36739;&#65292;&#24182;&#26681;&#25454;&#39044;&#23450;&#20041;&#26631;&#20934;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#36136;&#37327;&#21644;&#21464;&#24322;&#24615;&#26159;&#19968;&#20010;&#37325;&#35201;&#32780;&#23578;&#26410;&#35299;&#20915;&#30340;&#30740;&#31350;&#38590;&#39064;&#12290;&#20256;&#32479;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#22914;ROUGE&#21644;BERTScore&#65292;&#36890;&#24120;&#26080;&#27861;&#25429;&#25417;&#21040;&#25972;&#20307;&#35821;&#20041;&#30340;&#31561;&#20215;&#24615;&#12290;&#36825;&#23548;&#33268;&#19982;&#20154;&#31867;&#21028;&#26029;&#21644;&#30452;&#35273;&#30340;&#30456;&#20851;&#24615;&#36739;&#20302;&#65292;&#23588;&#20854;&#22312;&#21307;&#30103;&#21644;&#37329;&#34701;&#31561;&#39640;&#39118;&#38505;&#24212;&#29992;&#20013;&#65292;&#21487;&#38752;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#24378;&#22823;&#30340;&#20915;&#31574;&#33021;&#21147;&#23588;&#20026;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;DCR&#26694;&#26550;&#65292;&#19968;&#31181;&#20351;&#29992;&#21010;&#20998;-&#24449;&#26381;-&#25512;&#29702;&#26041;&#27861;&#35780;&#20272;&#21644;&#25913;&#36827;LLM&#29983;&#25104;&#25991;&#26412;&#19968;&#33268;&#24615;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;LLM&#30340;&#35780;&#20272;&#22120;&#19981;&#21516;&#65292;&#26412;&#26041;&#27861;&#37319;&#29992;&#20102;&#21010;&#20998;&#21644;&#24449;&#26381;&#35780;&#20272;&#22120;&#65288;DCE&#65289;&#65292;&#23558;&#20004;&#20010;&#29983;&#25104;&#30340;&#22238;&#31572;&#20043;&#38388;&#30340;&#27573;&#33853;&#23545;&#27573;&#33853;&#27604;&#36739;&#20998;&#35299;&#20026;&#26681;&#25454;&#39044;&#23450;&#20041;&#26631;&#20934;&#35780;&#20272;&#30340;&#27599;&#20010;&#21477;&#23376;&#23545;&#27573;&#33853;&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating the quality and variability of text generated by Large Language Models (LLMs) poses a significant, yet unresolved research challenge. Traditional evaluation methods, such as ROUGE and BERTScore, which measure token similarity, often fail to capture the holistic semantic equivalence. This results in a low correlation with human judgments and intuition, which is especially problematic in high-stakes applications like healthcare and finance where reliability, safety, and robust decision-making are highly critical. This work proposes DCR, an automated framework for evaluating and improving the consistency of LLM-generated texts using a divide-conquer-reasoning approach. Unlike existing LLM-based evaluators that operate at the paragraph level, our method employs a divide-and-conquer evaluator (DCE) that breaks down the paragraph-to-paragraph comparison between two generated responses into individual sentence-to-paragraph comparisons, each evaluated based on predefined criteria. T
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;PEFT&#26041;&#27861;&#21644;&#36880;&#23618;&#25918;&#32622;&#26041;&#24335;&#65292;&#20197;&#21450;&#37319;&#29992;&#38598;&#25104;&#23398;&#20064;&#31574;&#30053;&#65292;&#25581;&#31034;&#20102;&#29992;&#20110;&#35821;&#38899;&#22788;&#29702;&#30340;PEFT&#30340;&#26368;&#20339;&#26041;&#27861;&#21644;&#25918;&#32622;&#31574;&#30053;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#22810;&#25968;&#25237;&#31080;&#21487;&#20197;&#23454;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#36825;&#39033;&#30740;&#31350;&#36824;&#21457;&#29616;&#19981;&#21516;&#30340;PEFT&#26041;&#27861;&#20197;&#19981;&#21516;&#30340;&#26041;&#24335;&#36827;&#34892;&#23398;&#20064;&#65292;&#20174;&#32780;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#36890;&#36807;&#38598;&#25104;&#23398;&#20064;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#23427;&#20204;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.02122</link><description>&lt;p&gt;
&#29992;&#20110;&#35821;&#38899;&#30340;PEFT&#65306;&#25581;&#31034;&#20248;&#21270;&#25918;&#32622;&#12289;&#21512;&#24182;&#31574;&#30053;&#21644;&#38598;&#25104;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
PEFT for Speech: Unveiling Optimal Placement, Merging Strategies, and Ensemble Techniques. (arXiv:2401.02122v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;PEFT&#26041;&#27861;&#21644;&#36880;&#23618;&#25918;&#32622;&#26041;&#24335;&#65292;&#20197;&#21450;&#37319;&#29992;&#38598;&#25104;&#23398;&#20064;&#31574;&#30053;&#65292;&#25581;&#31034;&#20102;&#29992;&#20110;&#35821;&#38899;&#22788;&#29702;&#30340;PEFT&#30340;&#26368;&#20339;&#26041;&#27861;&#21644;&#25918;&#32622;&#31574;&#30053;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#22810;&#25968;&#25237;&#31080;&#21487;&#20197;&#23454;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#36825;&#39033;&#30740;&#31350;&#36824;&#21457;&#29616;&#19981;&#21516;&#30340;PEFT&#26041;&#27861;&#20197;&#19981;&#21516;&#30340;&#26041;&#24335;&#36827;&#34892;&#23398;&#20064;&#65292;&#20174;&#32780;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#36890;&#36807;&#38598;&#25104;&#23398;&#20064;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#23427;&#20204;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#34987;&#36234;&#26469;&#36234;&#35748;&#20026;&#26159;&#35821;&#38899;&#22788;&#29702;&#20013;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;PEFT&#26041;&#27861;&#30340;&#26368;&#20339;&#26041;&#27861;&#21644;&#25918;&#32622;&#20173;&#28982;&#27809;&#26377;&#23450;&#35770;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36890;&#36807;&#25193;&#23637;&#23454;&#39564;&#26469;&#27604;&#36739;&#19981;&#21516;&#30340;PEFT&#26041;&#27861;&#21450;&#20854;&#36880;&#23618;&#25918;&#32622;&#65292;&#20351;&#29992;&#21487;&#24494;&#20998;&#26550;&#26500;&#25628;&#32034;&#65288;DARTS&#65289;&#36827;&#34892;&#35843;&#25972;&#12290;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#20351;&#29992;&#38598;&#25104;&#23398;&#20064;&#26469;&#21033;&#29992;&#22810;&#26679;&#21270;&#30340;PEFT&#31574;&#30053;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;DARTS&#24182;&#19981;&#27604;&#22522;&#32447;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#22522;&#32447;&#26041;&#27861;&#28041;&#21450;&#23558;&#30456;&#21516;&#30340;PEFT&#26041;&#27861;&#25554;&#20837;&#21040;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#27169;&#22411;&#30340;&#25152;&#26377;&#23618;&#20013;&#12290;&#30456;&#21453;&#65292;&#37319;&#29992;&#22810;&#25968;&#25237;&#31080;&#30340;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32479;&#35745;&#35777;&#25454;&#34920;&#26126;&#65292;&#19981;&#21516;&#30340;PEFT&#26041;&#27861;&#20197;&#19981;&#21516;&#30340;&#26041;&#24335;&#36827;&#34892;&#23398;&#20064;&#12290;&#36825;&#31181;&#21464;&#24322;&#21487;&#33021;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#36890;&#36807;&#38598;&#25104;&#23398;&#20064;&#26377;&#25928;&#22320;&#21033;&#29992;&#21508;&#31181;PEFT&#26041;&#27861;&#30340;&#29420;&#29305;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter-Efficient Fine-Tuning (PEFT) is increasingly recognized as an effective method in speech processing. However, the optimal approach and the placement of PEFT methods remain inconclusive. Our study conducts extensive experiments to compare different PEFT methods and their layer-wise placement adapting Differentiable Architecture Search (DARTS). We also explore the use of ensemble learning to leverage diverse PEFT strategies. The results reveal that DARTS does not outperform the baseline approach, which involves inserting the same PEFT method into all layers of a Self-Supervised Learning (SSL) model. In contrast, an ensemble learning approach, particularly one employing majority voting, demonstrates superior performance. Our statistical evidence indicates that different PEFT methods learn in varied ways. This variation might explain why the synergistic integration of various PEFT methods through ensemble learning can harness their unique learning capabilities more effectively co
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;LLM&#20174;&#20505;&#36873;&#39033;&#20013;&#36873;&#25321;&#27491;&#30830;&#30340;SQL&#26597;&#35810;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#27979;&#35797;&#29992;&#20363;&#29983;&#25104;&#21644;&#37325;&#26032;&#25490;&#21517;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.02115</link><description>&lt;p&gt;
&#20351;&#29992;LLM&#20174;&#20505;&#36873;&#39033;&#20013;&#36873;&#25321;&#27491;&#30830;&#30340;SQL&#26597;&#35810;
&lt;/p&gt;
&lt;p&gt;
Using LLM to select the right SQL Query from candidates. (arXiv:2401.02115v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02115
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;LLM&#20174;&#20505;&#36873;&#39033;&#20013;&#36873;&#25321;&#27491;&#30830;&#30340;SQL&#26597;&#35810;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#27979;&#35797;&#29992;&#20363;&#29983;&#25104;&#21644;&#37325;&#26032;&#25490;&#21517;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;SQL&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#19968;&#31995;&#21015;&#20505;&#36873;&#30340;SQL&#26597;&#35810;&#65292;&#32780;&#26368;&#20339;&#26597;&#35810;&#24448;&#24448;&#19981;&#22312;&#20505;&#36873;&#21015;&#34920;&#30340;&#39030;&#37096;&#12290;&#26377;&#25928;&#30340;&#37325;&#26032;&#25490;&#21517;&#26041;&#27861;&#21487;&#20197;&#20174;&#20505;&#36873;&#21015;&#34920;&#20013;&#36873;&#25321;&#27491;&#30830;&#30340;SQL&#26597;&#35810;&#65292;&#24182;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#36890;&#36807;&#29983;&#25104;&#27979;&#35797;&#29992;&#20363;&#26469;&#37325;&#26032;&#25490;&#21517;&#20505;&#36873;&#20195;&#30721;&#12290;&#28982;&#32780;&#65292;&#38024;&#23545;&#25991;&#26412;&#21040;SQL&#30340;&#33258;&#21160;&#27979;&#35797;&#29992;&#20363;&#29983;&#25104;&#26159;&#19968;&#20010;&#30740;&#31350;&#36739;&#23569;&#30340;&#39046;&#22495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#27979;&#35797;&#29992;&#20363;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#39318;&#20808;&#29983;&#25104;&#19968;&#20010;&#25968;&#25454;&#24211;&#65292;&#28982;&#21518;&#20351;&#29992;LLM&#26469;&#39044;&#27979;&#30495;&#23454;&#32467;&#26524;&#65292;&#21363;&#26399;&#26395;&#32467;&#26524;&#25191;&#34892;&#30495;&#23454;&#30340;SQL&#26597;&#35810;&#22312;&#36825;&#20010;&#25968;&#25454;&#24211;&#19978;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#20943;&#23569;LLM&#30340;&#39044;&#27979;&#22256;&#38590;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#26469;&#23547;&#25214;&#29983;&#25104;LLM&#26131;&#22788;&#29702;&#25968;&#25454;&#24211;&#30340;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#26131;&#29702;&#35299;&#30340;&#25552;&#31034;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#27979;&#35797;&#29992;&#20363;&#29983;&#25104;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#25490;&#21517;&#26041;&#27861;&#26469;&#20174;&#20505;&#36873;&#21015;&#34920;&#20013;&#36873;&#25321;&#27491;&#30830;&#30340;SQL&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-SQL models can generate a list of candidate SQL queries, and the best query is often in the candidate list, but not at the top of the list. An effective re-rank method can select the right SQL query from the candidate list and improve the model's performance. Previous studies on code generation automatically generate test cases and use them to re-rank candidate codes. However, automatic test case generation for text-to-SQL is an understudied field. We propose an automatic test case generation method that first generates a database and then uses LLMs to predict the ground truth, which is the expected execution results of the ground truth SQL query on this database. To reduce the difficulty for LLMs to predict, we conduct experiments to search for ways to generate easy databases for LLMs and design easy-to-understand prompts. Based on our test case generation method, we propose a re-rank method to select the right SQL query from the candidate list. Given a candidate list, our met
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#37325;&#26032;&#35780;&#20272;&#20102;&#27969;&#27700;&#32447;&#24182;&#34892;&#20307;&#30340;&#20869;&#23384;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BPipe&#30340;&#25216;&#26415;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;&#39564;&#35777;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;BPipe&#22312;GPT-3&#27169;&#22411;&#19978;&#26377;&#25928;&#65292;&#20294;&#22312;LLaMA&#35757;&#32451;&#20013;&#24182;&#26410;&#33719;&#24471;&#30456;&#20284;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;BPipe&#22312;&#19981;&#21516;&#27169;&#22411;&#19978;&#24615;&#33021;&#24046;&#24322;&#30340;&#21407;&#22240;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;BPipe&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.02088</link><description>&lt;p&gt;
&#37325;&#26032;&#35780;&#20272;&#20869;&#23384;&#24179;&#34913;&#30340;&#27969;&#27700;&#32447;&#24182;&#34892;&#20307;&#65306;BPipe
&lt;/p&gt;
&lt;p&gt;
Re-evaluating the Memory-balanced Pipeline Parallelism: BPipe. (arXiv:2401.02088v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#37325;&#26032;&#35780;&#20272;&#20102;&#27969;&#27700;&#32447;&#24182;&#34892;&#20307;&#30340;&#20869;&#23384;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BPipe&#30340;&#25216;&#26415;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;&#39564;&#35777;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;BPipe&#22312;GPT-3&#27169;&#22411;&#19978;&#26377;&#25928;&#65292;&#20294;&#22312;LLaMA&#35757;&#32451;&#20013;&#24182;&#26410;&#33719;&#24471;&#30456;&#20284;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;BPipe&#22312;&#19981;&#21516;&#27169;&#22411;&#19978;&#24615;&#33021;&#24046;&#24322;&#30340;&#21407;&#22240;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;BPipe&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#27700;&#32447;&#24182;&#34892;&#20307;&#26159;&#35757;&#32451;&#22823;&#35268;&#27169;Transformer&#27169;&#22411;&#30340;&#19968;&#31181;&#37325;&#35201;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#20869;&#23384;&#28040;&#32791;&#19978;&#23384;&#22312;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#23548;&#33268;&#20869;&#23384;&#21033;&#29992;&#19981;&#20805;&#20998;&#12290;BPipe&#25216;&#26415;&#34987;&#25552;&#20986;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#22312;GPT-3&#27169;&#22411;&#19978;&#35777;&#26126;&#20102;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#22312;LLaMA&#35757;&#32451;&#20013;&#26410;&#33719;&#24471;&#31867;&#20284;&#30340;&#22909;&#22788;&#12290;&#27492;&#22806;&#65292;&#22312;&#24212;&#29992;flash attention&#26102;&#65292;BPipe&#22312;GPT-3&#35757;&#32451;&#20013;&#21482;&#24102;&#26469;&#24494;&#19981;&#36275;&#36947;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;BPipe&#22312;GPT-3&#21644;LLaMA&#19978;&#24615;&#33021;&#24046;&#24322;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;BPipe&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pipeline parallelism is an essential technique in the training of large-scale Transformer models. However, it suffers from imbalanced memory consumption, leading to insufficient memory utilization. The BPipe technique was proposed to address this issue and has proven effective in the GPT-3 model. Nevertheless, our experiments have not yielded similar benefits for LLaMA training. Additionally, BPipe only yields negligible benefits for GPT-3 training when applying flash attention. We analyze the underlying causes of the divergent performance of BPipe on GPT-3 and LLaMA. Furthermore, we introduce a novel method to estimate the performance of BPipe.
&lt;/p&gt;</description></item><item><title>ICE-GRT&#26159;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#24378;&#21270;&#36716;&#25442;&#30340;&#25351;&#20196;&#19978;&#19979;&#25991;&#22686;&#24378;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#20219;&#21153;&#20013;&#21462;&#24471;&#21331;&#36234;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#36890;&#29992;&#20219;&#21153;&#24615;&#33021;&#12290;&#23427;&#19981;&#20165;&#33021;&#29983;&#25104;&#31283;&#20581;&#30340;&#31572;&#26696;&#65292;&#36824;&#33021;&#25552;&#20379;&#23545;&#31572;&#26696;&#32972;&#21518;&#21407;&#22240;&#30340;&#35814;&#32454;&#20998;&#26512;&#65292;&#26159;&#23545;&#29616;&#26377;&#25351;&#23548;&#24615;&#24494;&#35843;&#27169;&#22411;&#30340;&#26174;&#33879;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2401.02072</link><description>&lt;p&gt;
ICE-GRT: &#22522;&#20110;&#29983;&#25104;&#22686;&#24378;&#30340;&#36716;&#25442;&#30340;&#25351;&#20196;&#19978;&#19979;&#25991;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
ICE-GRT: Instruction Context Enhancement by Generative Reinforcement based Transformers. (arXiv:2401.02072v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02072
&lt;/p&gt;
&lt;p&gt;
ICE-GRT&#26159;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#24378;&#21270;&#36716;&#25442;&#30340;&#25351;&#20196;&#19978;&#19979;&#25991;&#22686;&#24378;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#20219;&#21153;&#20013;&#21462;&#24471;&#21331;&#36234;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#36890;&#29992;&#20219;&#21153;&#24615;&#33021;&#12290;&#23427;&#19981;&#20165;&#33021;&#29983;&#25104;&#31283;&#20581;&#30340;&#31572;&#26696;&#65292;&#36824;&#33021;&#25552;&#20379;&#23545;&#31572;&#26696;&#32972;&#21518;&#21407;&#22240;&#30340;&#35814;&#32454;&#20998;&#26512;&#65292;&#26159;&#23545;&#29616;&#26377;&#25351;&#23548;&#24615;&#24494;&#35843;&#27169;&#22411;&#30340;&#26174;&#33879;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#21644;LLaMA&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#20219;&#21153;&#20013;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#19987;&#19994;&#39046;&#22495;&#32570;&#20047;&#28145;&#24230;&#21644;&#20934;&#30830;&#24615;&#65292;&#22312;&#31934;&#32454;&#35843;&#25972;&#26102;&#30340;&#25972;&#20307;&#33021;&#21147;&#19979;&#38477;&#65292;&#29305;&#21035;&#26159;&#23567;&#22411;&#27169;&#22411;&#30340;&#20998;&#26512;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;ICE-GRT&#65292;&#21033;&#29992;&#22522;&#20110;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#30340;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#65292;&#22312;&#39046;&#22495;&#20869;&#22330;&#26223;&#20013;&#23637;&#31034;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#19981;&#25439;&#22833;&#36890;&#29992;&#20219;&#21153;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;ICE-GRT&#30340;&#25506;&#32034;&#31361;&#20986;&#20102;&#20854;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#19981;&#20165;&#33021;&#22815;&#29983;&#25104;&#31283;&#20581;&#30340;&#31572;&#26696;&#65292;&#36824;&#33021;&#25552;&#20379;&#23545;&#31572;&#26696;&#32972;&#21518;&#21407;&#22240;&#30340;&#35814;&#32454;&#20998;&#26512;&#12290;&#36825;&#31181;&#33021;&#21147;&#26631;&#24535;&#30528;&#23545;&#25351;&#23548;&#24615;&#24494;&#35843;&#27169;&#22411;&#30340;&#26174;&#33879;&#36827;&#23637;&#12290;ICE-GRT&#30340;&#25104;&#21151;&#21462;&#20915;&#20110;&#20960;&#20010;&#20851;&#38190;&#22240;&#32032;&#65292;&#21253;&#25324;&#36866;&#24403;&#30340;&#25968;&#25454;&#12289;&#22870;&#21169;&#35268;&#27169;&#32553;&#25918;&#12289;KL&#25511;&#21046;&#12289;&#20248;&#21183;&#24402;&#19968;&#21270;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of Large Language Models (LLMs) such as ChatGPT and LLaMA encounter limitations in domain-specific tasks, with these models often lacking depth and accuracy in specialized areas, and exhibiting a decrease in general capabilities when fine-tuned, particularly analysis ability in small sized models. To address these gaps, we introduce ICE-GRT, utilizing Reinforcement Learning from Human Feedback (RLHF) grounded in Proximal Policy Optimization (PPO), demonstrating remarkable ability in in-domain scenarios without compromising general task performance. Our exploration of ICE-GRT highlights its understanding and reasoning ability to not only generate robust answers but also to provide detailed analyses of the reasons behind the answer. This capability marks a significant progression beyond the scope of Supervised Fine-Tuning models. The success of ICE-GRT is dependent on several crucial factors, including Appropriate Data, Reward Size Scaling, KL-Control, Advantage Normalizati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20221;&#32508;&#21512;&#27010;&#36848;&#65292;&#20171;&#32461;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20174;&#35757;&#32451;&#21040;&#25512;&#29702;&#30340;&#28436;&#21464;&#36807;&#31243;&#65292;&#24182;&#25506;&#35752;&#20102;&#36825;&#19968;&#26032;&#20852;&#36235;&#21183;&#20013;&#19982;&#25104;&#26412;&#25928;&#29575;&#30456;&#20851;&#30340;&#35757;&#32451;&#21644;&#37096;&#32626;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#36824;&#35752;&#35770;&#20102;&#25512;&#29702;&#38454;&#27573;&#30340;&#27169;&#22411;&#21387;&#32553;&#12289;&#24182;&#34892;&#35745;&#31639;&#12289;&#20869;&#23384;&#35843;&#24230;&#21644;&#32467;&#26500;&#20248;&#21270;&#31561;&#20851;&#38190;&#20027;&#39064;&#65292;&#20026;LLMs&#30340;&#21033;&#29992;&#21644;&#26410;&#26469;&#21457;&#23637;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2401.02038</link><description>&lt;p&gt;
&#29702;&#35299;LLMs&#65306;&#20174;&#35757;&#32451;&#21040;&#25512;&#29702;&#30340;&#20840;&#38754;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
Understanding LLMs: A Comprehensive Overview from Training to Inference. (arXiv:2401.02038v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20221;&#32508;&#21512;&#27010;&#36848;&#65292;&#20171;&#32461;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20174;&#35757;&#32451;&#21040;&#25512;&#29702;&#30340;&#28436;&#21464;&#36807;&#31243;&#65292;&#24182;&#25506;&#35752;&#20102;&#36825;&#19968;&#26032;&#20852;&#36235;&#21183;&#20013;&#19982;&#25104;&#26412;&#25928;&#29575;&#30456;&#20851;&#30340;&#35757;&#32451;&#21644;&#37096;&#32626;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#36824;&#35752;&#35770;&#20102;&#25512;&#29702;&#38454;&#27573;&#30340;&#27169;&#22411;&#21387;&#32553;&#12289;&#24182;&#34892;&#35745;&#31639;&#12289;&#20869;&#23384;&#35843;&#24230;&#21644;&#32467;&#26500;&#20248;&#21270;&#31561;&#20851;&#38190;&#20027;&#39064;&#65292;&#20026;LLMs&#30340;&#21033;&#29992;&#21644;&#26410;&#26469;&#21457;&#23637;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#30340;&#24341;&#20837;&#23548;&#33268;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35299;&#20915;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#22823;&#37327;&#20351;&#29992;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#23545;&#20110;&#25104;&#26412;&#25928;&#29575;&#30340;&#20851;&#27880;&#36234;&#26469;&#36234;&#22810;&#12290;&#20302;&#25104;&#26412;&#30340;LLMs&#35757;&#32451;&#21644;&#37096;&#32626;&#20195;&#34920;&#20102;&#26410;&#26469;&#30340;&#21457;&#23637;&#36235;&#21183;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#19982;&#36825;&#19968;&#26032;&#20852;&#36235;&#21183;&#30456;&#19968;&#33268;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#25216;&#26415;&#21644;&#25512;&#29702;&#37096;&#32626;&#25216;&#26415;&#30340;&#28436;&#21464;&#12290;&#35757;&#32451;&#30340;&#35752;&#35770;&#21253;&#25324;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#35757;&#32451;&#26550;&#26500;&#12289;&#39044;&#35757;&#32451;&#20219;&#21153;&#12289;&#24182;&#34892;&#35757;&#32451;&#20197;&#21450;&#19982;&#27169;&#22411;&#24494;&#35843;&#30456;&#20851;&#30340;&#20869;&#23481;&#12290;&#22312;&#25512;&#29702;&#26041;&#38754;&#65292;&#26412;&#25991;&#28085;&#30422;&#20102;&#27169;&#22411;&#21387;&#32553;&#12289;&#24182;&#34892;&#35745;&#31639;&#12289;&#20869;&#23384;&#35843;&#24230;&#21644;&#32467;&#26500;&#20248;&#21270;&#31561;&#20027;&#39064;&#12290;&#23427;&#36824;&#25506;&#35752;&#20102;LLMs&#30340;&#21033;&#29992;&#24182;&#25552;&#20379;&#20102;&#23545;&#20854;&#26410;&#26469;&#21457;&#23637;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The introduction of ChatGPT has led to a significant increase in the utilization of Large Language Models (LLMs) for addressing downstream tasks. There's an increasing focus on cost-efficient training and deployment within this context. Low-cost training and deployment of LLMs represent the future development trend. This paper reviews the evolution of large language model training techniques and inference deployment technologies aligned with this emerging trend. The discussion on training includes various aspects, including data preprocessing, training architecture, pre-training tasks, parallel training, and relevant content related to model fine-tuning. On the inference side, the paper covers topics such as model compression, parallel computation, memory scheduling, and structural optimization. It also explores LLMs' utilization and provides insights into their future development.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20219;&#21153;&#65292;Text2MDT&#65292;&#26088;&#22312;&#20174;&#21307;&#23398;&#25991;&#26412;&#20013;&#33258;&#21160;&#25552;&#21462;&#21307;&#23398;&#20915;&#31574;&#26641;&#12290;&#36890;&#36807;&#20004;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.02034</link><description>&lt;p&gt;
Text2MDT: &#20174;&#21307;&#23398;&#25991;&#26412;&#20013;&#25552;&#21462;&#21307;&#23398;&#20915;&#31574;&#26641;
&lt;/p&gt;
&lt;p&gt;
Text2MDT: Extracting Medical Decision Trees from Medical Texts. (arXiv:2401.02034v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02034
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20219;&#21153;&#65292;Text2MDT&#65292;&#26088;&#22312;&#20174;&#21307;&#23398;&#25991;&#26412;&#20013;&#33258;&#21160;&#25552;&#21462;&#21307;&#23398;&#20915;&#31574;&#26641;&#12290;&#36890;&#36807;&#20004;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#21307;&#30103;&#20915;&#31574;&#36807;&#31243;&#30340;&#30693;&#35782;&#65292;&#21487;&#20197;&#24314;&#27169;&#20026;&#21307;&#23398;&#20915;&#31574;&#26641;&#65288;MDT&#65289;&#65292;&#23545;&#20110;&#26500;&#24314;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;MDT&#26500;&#24314;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#32791;&#26102;&#32321;&#29712;&#30340;&#25163;&#21160;&#27880;&#37322;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20219;&#21153;&#65292;Text2MDT&#65292;&#26088;&#22312;&#25506;&#32034;&#20174;&#21307;&#23398;&#25991;&#26412;&#65288;&#22914;&#21307;&#23398;&#25351;&#21335;&#21644;&#25945;&#26448;&#65289;&#20013;&#33258;&#21160;&#25552;&#21462;MDT&#12290;&#25105;&#20204;&#35268;&#33539;&#20102;MDT&#30340;&#24418;&#24335;&#65292;&#24182;&#19982;&#21307;&#23398;&#19987;&#23478;&#19968;&#36215;&#21019;&#24314;&#20102;&#19968;&#20010;&#20013;&#25991;&#30340;&#26631;&#27880;&#25991;&#26412;&#21040;MDT&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;Text2MDT&#20219;&#21153;&#30340;&#20004;&#31181;&#19981;&#21516;&#26041;&#27861;&#65306;&#65288;a&#65289;&#19968;&#31181;&#20165;&#20381;&#36182;&#20110;GPT&#39118;&#26684;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25351;&#20196;&#35843;&#25972;&#26469;&#29983;&#25104;&#25152;&#26377;&#33410;&#28857;&#20449;&#24687;&#21644;&#26641;&#32467;&#26500;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#12290; &#65288;b&#65289;&#23558;Text2MDT&#20219;&#21153;&#20998;&#35299;&#20026;&#19977;&#20010;&#23376;&#20219;&#21153;&#30340;&#27969;&#27700;&#32447;&#26694;&#26550;&#12290;&#22312;&#25105;&#20204;&#30340;Text2MDT&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65306;&#65288;a&#65289;&#22522;&#20110;LLM&#65288;&#21442;&#25968;&#35268;&#27169;&#20026;7B&#25110;&#26356;&#22823;&#65289;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#23637;&#29616;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;
&lt;/p&gt;
&lt;p&gt;
Knowledge of the medical decision process, which can be modeled as medical decision trees (MDTs), is critical to build clinical decision support systems. However, the current MDT construction methods rely heavily on time-consuming and laborious manual annotation. In this work, we propose a novel task, Text2MDT, to explore the automatic extraction of MDTs from medical texts such as medical guidelines and textbooks. We normalize the form of the MDT and create an annotated Text-to-MDT dataset in Chinese with the participation of medical experts. We investigate two different methods for the Text2MDT tasks: (a) an end-to-end framework which only relies on a GPT style large language models (LLM) instruction tuning to generate all the node information and tree structures. (b) The pipeline framework which decomposes the Text2MDT task to three subtasks. Experiments on our Text2MDT dataset demonstrate that: (a) the end-to-end method basd on LLMs (7B parameters or larger) show promising results, 
&lt;/p&gt;</description></item><item><title>&#33258;&#25105;&#23545;&#27604;&#26159;&#19968;&#31181;&#36890;&#36807;&#23545;&#27604;&#19981;&#21516;&#27714;&#35299;&#35270;&#35282;&#21644;&#24635;&#32467;&#24046;&#24322;&#65292;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21453;&#24605;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.02009</link><description>&lt;p&gt;
&#33258;&#25105;&#23545;&#27604;&#65306;&#36890;&#36807;&#19981;&#19968;&#33268;&#30340;&#27714;&#35299;&#35270;&#35282;&#33719;&#24471;&#26356;&#22909;&#30340;&#21453;&#24605;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives. (arXiv:2401.02009v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02009
&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#23545;&#27604;&#26159;&#19968;&#31181;&#36890;&#36807;&#23545;&#27604;&#19981;&#21516;&#27714;&#35299;&#35270;&#35282;&#21644;&#24635;&#32467;&#24046;&#24322;&#65292;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21453;&#24605;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21453;&#24605;&#33021;&#21147;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#19968;&#31181;&#20107;&#21518;&#25552;&#31034;&#31574;&#30053;&#65292;&#20363;&#22914;&#21453;&#24605;&#21644;&#33258;&#25105;&#25913;&#36827;&#65292;&#26681;&#25454;&#33258;&#25105;&#35780;&#20272;&#25110;&#22806;&#37096;&#21453;&#39304;&#26469;&#25913;&#21892;LLM&#30340;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#27809;&#26377;&#22806;&#37096;&#21453;&#39304;&#30340;&#24773;&#20917;&#19979;&#65292;LLM&#30340;&#20869;&#22312;&#21453;&#24605;&#26159;&#19981;&#31283;&#23450;&#30340;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#25581;&#31034;&#20102;&#33258;&#25105;&#35780;&#20272;&#21453;&#39304;&#36136;&#37327;&#26159;&#20851;&#38190;&#29942;&#39048;&#12290;&#25105;&#20204;&#21457;&#29616;LLM&#22312;&#33258;&#25105;&#35780;&#20272;&#26102;&#24120;&#24120;&#34920;&#29616;&#20986;&#36807;&#24230;&#33258;&#20449;&#25110;&#39640;&#24230;&#38543;&#26426;&#24615;&#65292;&#25552;&#20379;&#22266;&#25191;&#25110;&#19981;&#19968;&#33268;&#30340;&#21453;&#39304;&#65292;&#23548;&#33268;&#21453;&#24605;&#33021;&#21147;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#25105;&#23545;&#27604;&#30340;&#26041;&#27861;&#65306;&#23427;&#26681;&#25454;&#35831;&#27714;&#33258;&#36866;&#24212;&#22320;&#25506;&#32034;&#22810;&#26679;&#30340;&#27714;&#35299;&#35270;&#35282;&#65292;&#23545;&#27604;&#24046;&#24322;&#65292;&#24182;&#23558;&#36825;&#20123;&#24046;&#24322;&#24635;&#32467;&#20026;&#19968;&#20010;&#26816;&#26597;&#34920;&#65292;&#29992;&#20110;&#37325;&#26032;&#23457;&#35270;&#21644;&#28040;&#38500;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36171;&#20104;LLM&#22810;&#26679;&#30340;&#35270;&#35282;&#20197;&#20943;&#36731;&#22266;&#25191;&#20559;&#35265;&#12290;&#27492;&#22806;&#65292;&#24046;&#24322;&#25351;&#31034;&#20102;&#28508;&#22312;&#30340;&#38169;&#35823;&#25110;&#22266;&#26377;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The reflection capacity of Large Language Model (LLM) has garnered extensive attention. A post-hoc prompting strategy, e.g., reflexion and self-refine, refines LLM's response based on self-evaluated or external feedback. However, recent research indicates without external feedback, LLM's intrinsic reflection is unstable. Our investigation unveils that the key bottleneck is the quality of the self-evaluated feedback. We find LLMs often exhibit overconfidence or high randomness when self-evaluate, offering stubborn or inconsistent feedback, which causes poor reflection. To remedy this, we advocate Self-Contrast: It adaptively explores diverse solving perspectives tailored to the request, contrasts the differences, and summarizes these discrepancies into a checklist which could be used to re-examine and eliminate discrepancies. Our method endows LLM with diverse perspectives to alleviate stubborn biases. Moreover, their discrepancies indicate potential errors or inherent uncertainties tha
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#27979;&#37327;&#20301;&#32622;&#20559;&#35265;&#65292;&#37325;&#35775;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38646;-shot &#25277;&#35937;&#25688;&#35201;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#27169;&#22411;&#19981;&#20844;&#24179;&#22320;&#20248;&#20808;&#32771;&#34385;&#26576;&#20123;&#37096;&#20998;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#23548;&#33268;&#19981;&#21487;&#21462;&#30340;&#34892;&#20026;&#12290;&#23545;&#22810;&#20010;LLM&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#25277;&#35937;&#25688;&#35201;&#27169;&#22411;&#36827;&#34892;&#30340;&#23454;&#39564;&#25552;&#20379;&#20102;&#20851;&#20110;&#38646;-shot &#24635;&#32467;&#20219;&#21153;&#30340;&#27169;&#22411;&#24615;&#33021;&#21644;&#20301;&#32622;&#20559;&#35265;&#30340;&#26032;&#35265;&#35299;&#21644;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2401.01989</link><description>&lt;p&gt;
&#37325;&#35775;&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#19979;&#30340;&#38646;-shot &#25277;&#35937;&#25688;&#35201;&#65292;&#20174;&#20301;&#32622;&#20559;&#35265;&#30340;&#35282;&#24230;&#20986;&#21457;
&lt;/p&gt;
&lt;p&gt;
Revisiting Zero-Shot Abstractive Summarization in the Era of Large Language Models from the Perspective of Position Bias. (arXiv:2401.01989v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01989
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#27979;&#37327;&#20301;&#32622;&#20559;&#35265;&#65292;&#37325;&#35775;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38646;-shot &#25277;&#35937;&#25688;&#35201;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#27169;&#22411;&#19981;&#20844;&#24179;&#22320;&#20248;&#20808;&#32771;&#34385;&#26576;&#20123;&#37096;&#20998;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#23548;&#33268;&#19981;&#21487;&#21462;&#30340;&#34892;&#20026;&#12290;&#23545;&#22810;&#20010;LLM&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#25277;&#35937;&#25688;&#35201;&#27169;&#22411;&#36827;&#34892;&#30340;&#23454;&#39564;&#25552;&#20379;&#20102;&#20851;&#20110;&#38646;-shot &#24635;&#32467;&#20219;&#21153;&#30340;&#27169;&#22411;&#24615;&#33021;&#21644;&#20301;&#32622;&#20559;&#35265;&#30340;&#26032;&#35265;&#35299;&#21644;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#27979;&#37327;&#20301;&#32622;&#20559;&#35265;&#26469;&#34920;&#24449;&#21644;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#38646;-shot &#25277;&#35937;&#25688;&#35201;&#65292;&#25105;&#20204;&#23558;&#20854;&#35270;&#20026;&#20808;&#21069;&#25991;&#29486;&#20013;&#30740;&#31350;&#36807;&#30340;&#26356;&#20026;&#38480;&#21046;&#24615;&#30340;&#24341;&#23548;&#20559;&#35265;&#29616;&#35937;&#30340;&#19968;&#33324;&#34920;&#36848;&#12290;&#20301;&#32622;&#20559;&#35265;&#25429;&#25417;&#21040;&#27169;&#22411;&#22312;&#36755;&#20837;&#25991;&#26412;&#30340;&#26576;&#20123;&#37096;&#20998;&#19978;&#19981;&#20844;&#24179;&#22320;&#20248;&#20808;&#32771;&#34385;&#20449;&#24687;&#65292;&#23548;&#33268;&#19981;&#21487;&#21462;&#30340;&#34892;&#20026;&#12290;&#36890;&#36807;&#23545;&#22235;&#20010;&#19981;&#21516;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#20010;LLM&#27169;&#22411;&#22914;GPT 3.5-Turbo&#65292;Llama-2&#21644;Dolly-v2&#20013;&#30340;&#20301;&#32622;&#20559;&#35265;&#65292;&#20197;&#21450;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#25277;&#35937;&#25688;&#35201;&#27169;&#22411;&#22914;Pegasus&#21644;BART&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#20026;&#38646;-shot &#24635;&#32467;&#20219;&#21153;&#30340;&#27169;&#22411;&#24615;&#33021;&#21644;&#20301;&#32622;&#20559;&#35265;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#21644;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
We characterize and study zero-shot abstractive summarization in Large Language Models (LLMs) by measuring position bias, which we propose as a general formulation of the more restrictive lead bias phenomenon studied previously in the literature. Position bias captures the tendency of a model unfairly prioritizing information from certain parts of the input text over others, leading to undesirable behavior. Through numerous experiments on four diverse real-world datasets, we study position bias in multiple LLM models such as GPT 3.5-Turbo, Llama-2, and Dolly-v2, as well as state-of-the-art pretrained encoder-decoder abstractive summarization models such as Pegasus and BART. Our findings lead to novel insights and discussion on performance and position bias of models for zero-shot summarization tasks.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#23545;&#40784;&#31639;&#27861;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#26412;&#35770;&#25991;&#25581;&#31034;&#20102;&#23545;&#40784;&#27169;&#22411;&#30340;&#26426;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#21462;&#28040;&#27169;&#22411;&#30340;&#23545;&#40784;&#65292;&#20174;&#32780;&#20351;&#20854;&#24674;&#22797;&#26377;&#23475;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2401.01967</link><description>&lt;p&gt;
&#23545;&#40784;&#31639;&#27861;&#30340;&#26426;&#21046;&#29702;&#35299;&#65306;&#22522;&#20110;DPO&#21644;&#27602;&#24615;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO and Toxicity. (arXiv:2401.01967v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01967
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#23545;&#40784;&#31639;&#27861;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#26412;&#35770;&#25991;&#25581;&#31034;&#20102;&#23545;&#40784;&#27169;&#22411;&#30340;&#26426;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#21462;&#28040;&#27169;&#22411;&#30340;&#23545;&#40784;&#65292;&#20174;&#32780;&#20351;&#20854;&#24674;&#22797;&#26377;&#23475;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#23545;&#40784;&#31639;&#27861;&#29616;&#22312;&#24120;&#29992;&#20110;&#35843;&#25972;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20197;&#36866;&#24212;&#29992;&#25143;&#21916;&#22909;&#65292;&#20294;&#25105;&#20204;&#32570;&#20047;&#35299;&#37322;&#27169;&#22411;&#22914;&#20309;&#8220;&#23545;&#40784;&#8221;&#30340;&#22522;&#26412;&#26426;&#21046;&#65292;&#22240;&#27492;&#38590;&#20197;&#35299;&#37322;&#35832;&#22914;&#36234;&#29425;&#31561;&#29616;&#35937;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#24120;&#35265;&#30340;&#31639;&#27861;&#8212;&#8212;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#65292;&#20197;&#21450;&#23427;&#22914;&#20309;&#38477;&#20302;&#27602;&#24615;&#30340;&#26426;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#27602;&#24615;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;GPT2-medium&#20013;&#30340;&#34920;&#31034;&#21644;&#21796;&#36215;&#26041;&#24335;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#31934;&#24515;&#35774;&#35745;&#30340;&#25104;&#23545;&#25968;&#25454;&#38598;&#24212;&#29992;DPO&#26469;&#38477;&#20302;&#27602;&#24615;&#12290;&#25105;&#20204;&#26816;&#26597;&#20102;&#29983;&#25104;&#27169;&#22411;&#26159;&#22914;&#20309;&#36991;&#20813;&#36755;&#20986;&#26377;&#23475;&#32467;&#26524;&#30340;&#65292;&#24182;&#21457;&#29616;&#39044;&#35757;&#32451;&#23398;&#21040;&#30340;&#33021;&#21147;&#24182;&#27809;&#26377;&#34987;&#31227;&#38500;&#65292;&#32780;&#26159;&#34987;&#32469;&#36807;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#23637;&#31034;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#21462;&#28040;&#27169;&#22411;&#30340;&#23545;&#40784;&#65292;&#23558;&#20854;&#24674;&#22797;&#20026;&#26377;&#23475;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
While alignment algorithms are now commonly used to tune pre-trained language models towards a user's preferences, we lack explanations for the underlying mechanisms in which models become ``aligned'', thus making it difficult to explain phenomena like jailbreaks. In this work we study a popular algorithm, direct preference optimization (DPO), and the mechanisms by which it reduces toxicity. Namely, we first study how toxicity is represented and elicited in a pre-trained language model, GPT2-medium. We then apply DPO with a carefully crafted pairwise dataset to reduce toxicity. We examine how the resulting model averts toxic outputs, and find that capabilities learned from pre-training are not removed, but rather bypassed. We use this insight to demonstrate a simple method to un-align the model, reverting it back to its toxic behavior.
&lt;/p&gt;</description></item><item><title>Instruct-Imagen&#26159;&#19968;&#31181;&#22788;&#29702;&#24322;&#26500;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#24182;&#36827;&#34892;&#27867;&#21270;&#30340;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#22810;&#27169;&#24335;&#25351;&#20196;&#20197;&#23454;&#29616;&#21508;&#31181;&#29983;&#25104;&#24847;&#22270;&#30340;&#32479;&#19968;&#26631;&#20934;&#21270;&#12290;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#30340;&#35757;&#32451;&#25552;&#21319;&#27169;&#22411;&#22312;&#22806;&#37096;&#22810;&#27169;&#24577;&#29615;&#22659;&#19979;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;&#23545;&#22810;&#26679;&#21270;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#30340;&#20154;&#24037;&#35780;&#20272;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.01952</link><description>&lt;p&gt;
Instruct-Imagen: &#24102;&#26377;&#22810;&#27169;&#24335;&#25351;&#20196;&#30340;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Instruct-Imagen: Image Generation with Multi-modal Instruction. (arXiv:2401.01952v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01952
&lt;/p&gt;
&lt;p&gt;
Instruct-Imagen&#26159;&#19968;&#31181;&#22788;&#29702;&#24322;&#26500;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#24182;&#36827;&#34892;&#27867;&#21270;&#30340;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#22810;&#27169;&#24335;&#25351;&#20196;&#20197;&#23454;&#29616;&#21508;&#31181;&#29983;&#25104;&#24847;&#22270;&#30340;&#32479;&#19968;&#26631;&#20934;&#21270;&#12290;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#30340;&#35757;&#32451;&#25552;&#21319;&#27169;&#22411;&#22312;&#22806;&#37096;&#22810;&#27169;&#24577;&#29615;&#22659;&#19979;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;&#23545;&#22810;&#26679;&#21270;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#30340;&#20154;&#24037;&#35780;&#20272;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Instruct-Imagen&#65292;&#36825;&#26159;&#19968;&#31181;&#22788;&#29702;&#24322;&#26500;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#24182;&#22312;&#26410;&#35265;&#20219;&#21153;&#20013;&#36827;&#34892;&#27867;&#21270;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22810;&#27169;&#24335;&#25351;&#20196;&#29992;&#20110;&#22270;&#20687;&#29983;&#25104;&#65292;&#36825;&#26159;&#19968;&#31181;&#20219;&#21153;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#31934;&#30830;&#22320;&#34920;&#36798;&#21508;&#31181;&#29983;&#25104;&#24847;&#22270;&#12290;&#23427;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#26469;&#25972;&#21512;&#19981;&#21516;&#30340;&#27169;&#24577;&#65288;&#20363;&#22914;&#25991;&#26412;&#12289;&#36793;&#32536;&#12289;&#39118;&#26684;&#12289;&#20027;&#39064;&#31561;&#65289;&#65292;&#20351;&#24471;&#20016;&#23500;&#30340;&#29983;&#25104;&#24847;&#22270;&#33021;&#22815;&#20197;&#32479;&#19968;&#30340;&#26684;&#24335;&#26631;&#20934;&#21270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#26500;&#24314;&#20102;Instruct-Imagen&#30340;&#20004;&#38454;&#27573;&#26694;&#26550;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#30340;&#35757;&#32451;&#26469;&#20351;&#27169;&#22411;&#33021;&#22815;&#22312;&#22806;&#37096;&#22810;&#27169;&#24577;&#29615;&#22659;&#19979;&#22522;&#20110;&#20854;&#29983;&#25104;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#38656;&#35201;&#35270;&#35273;-&#35821;&#35328;&#29702;&#35299;&#30340;&#22810;&#26679;&#21270;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#23545;&#36825;&#20010;&#35843;&#25972;&#21518;&#30340;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#27599;&#20010;&#20219;&#21153;&#37117;&#37197;&#23545;&#19968;&#20010;&#21253;&#21547;&#20219;&#21153;&#26412;&#36136;&#30340;&#22810;&#27169;&#24335;&#25351;&#20196;&#12290;&#23545;&#21508;&#31181;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#30340;&#20154;&#24037;&#35780;&#20272;&#34920;&#26126;&#65292;
&lt;/p&gt;
&lt;p&gt;
This paper presents instruct-imagen, a model that tackles heterogeneous image generation tasks and generalizes across unseen tasks. We introduce *multi-modal instruction* for image generation, a task representation articulating a range of generation intents with precision. It uses natural language to amalgamate disparate modalities (e.g., text, edge, style, subject, etc.), such that abundant generation intents can be standardized in a uniform format.  We then build instruct-imagen by fine-tuning a pre-trained text-to-image diffusion model with a two-stage framework. First, we adapt the model using the retrieval-augmented training, to enhance model's capabilities to ground its generation on external multimodal context. Subsequently, we fine-tune the adapted model on diverse image generation tasks that requires vision-language understanding (e.g., subject-driven generation, etc.), each paired with a multi-modal instruction encapsulating the task's essence. Human evaluation on various ima
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20020;&#24202;&#35821;&#20041;&#25628;&#32034;&#26041;&#38754;&#65292;&#36890;&#29992;&#23884;&#20837;&#27169;&#22411;&#27604;&#19987;&#19994;&#23884;&#20837;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#65292;&#36825;&#34920;&#26126;&#29616;&#26377;&#30340;&#20020;&#24202;&#19987;&#19994;&#21270;&#27169;&#22411;&#23545;&#36755;&#20837;&#30340;&#24494;&#23567;&#21464;&#21270;&#26356;&#25935;&#24863;&#12290;</title><link>http://arxiv.org/abs/2401.01943</link><description>&lt;p&gt;
&#36890;&#29992;&#23884;&#20837;&#27169;&#22411;&#22312;&#30701;&#35821;&#22659;&#20020;&#24202;&#35821;&#20041;&#25628;&#32034;&#26041;&#38754;&#34920;&#29616;&#27604;&#19987;&#19994;&#23884;&#20837;&#27169;&#22411;&#26356;&#22909;
&lt;/p&gt;
&lt;p&gt;
Generalist embedding models are better at short-context clinical semantic search than specialized embedding models. (arXiv:2401.01943v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01943
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20020;&#24202;&#35821;&#20041;&#25628;&#32034;&#26041;&#38754;&#65292;&#36890;&#29992;&#23884;&#20837;&#27169;&#22411;&#27604;&#19987;&#19994;&#23884;&#20837;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#65292;&#36825;&#34920;&#26126;&#29616;&#26377;&#30340;&#20020;&#24202;&#19987;&#19994;&#21270;&#27169;&#22411;&#23545;&#36755;&#20837;&#30340;&#24494;&#23567;&#21464;&#21270;&#26356;&#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24037;&#20855;&#21644;&#35299;&#20915;&#26041;&#26696;&#22312;&#21307;&#30103;&#39046;&#22495;&#30340;&#24212;&#29992;&#26085;&#30410;&#22686;&#22810;&#65292;&#36825;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#36235;&#21183;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20010;&#39640;&#24230;&#20851;&#38190;&#21644;&#25935;&#24863;&#30340;&#39046;&#22495;&#20013;&#20351;&#29992;&#23427;&#20204;&#23545;&#20854;&#31283;&#20581;&#24615;&#20135;&#29983;&#20102;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#23545;&#36755;&#20837;&#21464;&#21270;&#21644;&#29983;&#25104;&#30340;&#36755;&#20986;&#30340;&#21487;&#38752;&#24615;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;ICD-10-CM&#20195;&#30721;&#25551;&#36848;&#30340;&#25991;&#26412;&#25968;&#25454;&#38598;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#35813;&#25968;&#25454;&#38598;&#24191;&#27867;&#24212;&#29992;&#20110;&#32654;&#22269;&#21307;&#38498;&#65292;&#21253;&#21547;&#35768;&#22810;&#20020;&#24202;&#26415;&#35821;&#21450;&#20854;&#26131;&#20110;&#22797;&#21046;&#30340;&#25913;&#20889;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#35821;&#20041;&#25628;&#32034;&#20219;&#21153;&#20013;&#23545;&#29616;&#26377;&#30340;&#36890;&#29992;&#25110;&#20020;&#24202;&#19987;&#19994;&#21270;&#30340;&#23884;&#20837;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#30446;&#26631;&#26159;&#27491;&#30830;&#21305;&#37197;&#25913;&#20889;&#30340;&#25991;&#26412;&#19982;&#21407;&#22987;&#25551;&#36848;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#29992;&#27169;&#22411;&#27604;&#20020;&#24202;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#65292;&#36825;&#34920;&#26126;&#29616;&#26377;&#30340;&#20020;&#24202;&#19987;&#19994;&#21270;&#27169;&#22411;&#23545;&#36755;&#20837;&#30340;&#24494;&#23567;&#21464;&#21270;&#26356;&#25935;&#24863;&#65292;&#20174;&#32780;&#20351;&#20854;&#22256;&#24785;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing use of tools and solutions based on Large Language Models (LLMs) for various tasks in the medical domain has become a prominent trend. Their use in this highly critical and sensitive domain has thus raised important questions about their robustness, especially in response to variations in input, and the reliability of the generated outputs. This study addresses these questions by constructing a textual dataset based on the ICD-10-CM code descriptions, widely used in US hospitals and containing many clinical terms, and their easily reproducible rephrasing. We then benchmarked existing embedding models, either generalist or specialized in the clinical domain, in a semantic search task where the goal was to correctly match the rephrased text to the original description. Our results showed that generalist models performed better than clinical models, suggesting that existing clinical specialized models are more sensitive to small changes in input that confuse them. The highl
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26377;&#38024;&#23545;&#24615;&#21644;&#25345;&#32493;&#30340;&#39044;&#35757;&#32451;&#65292;&#25105;&#20204;&#22312;&#22825;&#25991;&#23398;&#38382;&#39064;&#22238;&#31572;&#20013;&#25193;&#23637;&#20102;AstroLLaMA&#65292;&#36890;&#36807;&#20351;&#29992;&#32039;&#20945;&#30340;LLaMA-2&#27169;&#22411;&#21644;&#19987;&#38376;&#30340;&#22825;&#25991;&#23398;&#35821;&#26009;&#24211;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#22312;&#19987;&#38376;&#20027;&#39064;&#29702;&#35299;&#26041;&#38754;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23545;&#29305;&#23450;&#39046;&#22495;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65292;&#21457;&#24067;&#20102;&#24102;&#26377;&#32842;&#22825;&#21151;&#33021;&#30340;AstroLLaMA&#12290;</title><link>http://arxiv.org/abs/2401.01916</link><description>&lt;p&gt;
AstroLLaMA-Chat: &#20351;&#29992;&#23545;&#35805;&#21644;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#25193;&#23637;AstroLLaMA
&lt;/p&gt;
&lt;p&gt;
AstroLLaMA-Chat: Scaling AstroLLaMA with Conversational and Diverse Datasets. (arXiv:2401.01916v1 [astro-ph.IM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01916
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26377;&#38024;&#23545;&#24615;&#21644;&#25345;&#32493;&#30340;&#39044;&#35757;&#32451;&#65292;&#25105;&#20204;&#22312;&#22825;&#25991;&#23398;&#38382;&#39064;&#22238;&#31572;&#20013;&#25193;&#23637;&#20102;AstroLLaMA&#65292;&#36890;&#36807;&#20351;&#29992;&#32039;&#20945;&#30340;LLaMA-2&#27169;&#22411;&#21644;&#19987;&#38376;&#30340;&#22825;&#25991;&#23398;&#35821;&#26009;&#24211;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#22312;&#19987;&#38376;&#20027;&#39064;&#29702;&#35299;&#26041;&#38754;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23545;&#29305;&#23450;&#39046;&#22495;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65292;&#21457;&#24067;&#20102;&#24102;&#26377;&#32842;&#22825;&#21151;&#33021;&#30340;AstroLLaMA&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26377;&#38024;&#23545;&#24615;&#21644;&#25345;&#32493;&#30340;&#39044;&#35757;&#32451;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22312;&#22825;&#25991;&#23398;&#38382;&#39064;&#22238;&#31572;&#20013;&#22686;&#24378;LLM&#24615;&#33021;&#30340;&#28508;&#21147;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#32039;&#20945;&#30340;7B&#21442;&#25968;&#30340;LLaMA-2&#27169;&#22411;&#65292;&#24182;&#19988;&#19987;&#27880;&#20110;&#19968;&#32452;&#32463;&#36807;&#31579;&#36873;&#30340;&#22825;&#25991;&#23398;&#35821;&#26009;&#24211;&#65292;&#21253;&#25324;&#25688;&#35201;&#12289;&#20171;&#32461;&#21644;&#32467;&#35770;&#65292;&#25105;&#20204;&#22312;&#19987;&#38376;&#20027;&#39064;&#29702;&#35299;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#34429;&#28982;&#20687;GPT-4&#36825;&#26679;&#30340;&#36890;&#29992;LLMs&#22312;&#26356;&#24191;&#27867;&#30340;&#38382;&#39064;&#22238;&#31572;&#22330;&#26223;&#20013;&#30001;&#20110;&#26356;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#32780;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#26377;&#38480;&#36164;&#28304;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#20173;&#28982;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#19987;&#38376;&#20027;&#39064;&#19978;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AstroLLaMA&#30340;&#25193;&#23637;&#65306;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#23545;7B LLaMA&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#26368;&#32456;&#21457;&#24067;&#20102;&#36866;&#29992;&#20110;&#31038;&#21306;&#20351;&#29992;&#30340;&#20855;&#26377;&#32842;&#22825;&#21151;&#33021;&#30340;AstroLLaMA&#12290;&#20840;&#38754;&#30340;&#23450;&#37327;&#22522;&#20934;&#27979;&#35797;&#27491;&#22312;&#36827;&#34892;&#20013;&#65292;&#24182;&#23558;&#22312;&#21363;&#23558;&#21457;&#24067;&#30340;&#23436;&#25972;&#35770;&#25991;&#20013;&#35814;&#32454;&#20171;&#32461;&#12290;&#27169;&#22411;AstroLLaMA-Chat&#29616;&#24050;&#22312;...
&lt;/p&gt;
&lt;p&gt;
We explore the potential of enhancing LLM performance in astronomy-focused question-answering through targeted, continual pre-training. By employing a compact 7B-parameter LLaMA-2 model and focusing exclusively on a curated set of astronomy corpus -- comprising abstracts, introductions, and conclusions -- we achieve notable improvements in specialized topic comprehension. While general LLMs like GPT-4 outperform in broader question-answering scenarios due to superior reasoning capabilities, our findings suggest that continual pre-training with limited resources can still enhance model performance on specialized topics. Additionally, we present an extension of AstroLLaMA: the fine-tuning of the 7B LLaMA model on a domain-specific conversational dataset, culminating in the release of the chat-enabled AstroLLaMA for community use. Comprehensive quantitative benchmarking is currently in progress and will be detailed in an upcoming full paper. The model, AstroLLaMA-Chat, is now available at
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#30446;&#26631;&#20998;&#26512;&#35270;&#35282;&#36827;&#34892;&#36328;&#30446;&#26631;&#31435;&#22330;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#22810;&#35270;&#35282;&#25552;&#31034;&#35843;&#25972;&#26694;&#26550;&#23558;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#34701;&#21512;&#21040;&#31435;&#22330;&#39044;&#27979;&#22120;&#20013;&#12290;</title><link>http://arxiv.org/abs/2401.01761</link><description>&lt;p&gt;
&#21033;&#29992;&#30446;&#26631;&#20998;&#26512;&#35270;&#35282;&#36827;&#34892;&#36328;&#30446;&#26631;&#31435;&#22330;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Cross-target Stance Detection by Exploiting Target Analytical Perspectives. (arXiv:2401.01761v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#30446;&#26631;&#20998;&#26512;&#35270;&#35282;&#36827;&#34892;&#36328;&#30446;&#26631;&#31435;&#22330;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#22810;&#35270;&#35282;&#25552;&#31034;&#35843;&#25972;&#26694;&#26550;&#23558;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#34701;&#21512;&#21040;&#31435;&#22330;&#39044;&#27979;&#22120;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#30446;&#26631;&#31435;&#22330;&#26816;&#27979;(CTSD)&#26159;&#19968;&#39033;&#37325;&#35201;&#20219;&#21153;&#65292;&#36890;&#36807;&#21033;&#29992;&#28304;&#30446;&#26631;&#20135;&#29983;&#30340;&#27880;&#37322;&#25968;&#25454;&#26469;&#25512;&#26029;&#30446;&#30340;&#30446;&#26631;&#30340;&#24577;&#24230;&#12290;CTSD&#20013;&#30340;&#19968;&#31181;&#37325;&#35201;&#26041;&#27861;&#26159;&#25552;&#21462;&#22495;&#19981;&#21464;&#29305;&#24449;&#20197;&#22635;&#34917;&#22810;&#20010;&#30446;&#26631;&#20043;&#38388;&#30340;&#30693;&#35782;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#38750;&#27491;&#24335;&#21644;&#30701;&#25991;&#26412;&#32467;&#26500;&#30340;&#20998;&#26512;&#20197;&#21450;&#38544;&#21547;&#34920;&#36798;&#20351;&#24471;&#25552;&#21462;&#22495;&#19981;&#21464;&#30693;&#35782;&#21464;&#24471;&#22797;&#26434;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35270;&#35282;&#25552;&#31034;&#35843;&#25972;(MPPT)&#27169;&#22411;&#29992;&#20110;CTSD&#65292;&#35813;&#27169;&#22411;&#23558;&#20998;&#26512;&#35270;&#35282;&#20316;&#20026;&#30693;&#35782;&#20256;&#36882;&#30340;&#26725;&#26753;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#25351;&#20196;&#30340;&#20004;&#38454;&#27573;&#24605;&#32500;&#38142;(TsCoT)&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#25351;&#20196;&#21046;&#23450;&#65292;&#20174;&#22810;&#20010;&#35270;&#35282;&#25552;&#21462;&#30446;&#26631;&#20998;&#26512;&#35270;&#35282;&#24182;&#25552;&#20379;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;(NLEs)&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#35270;&#35282;&#25552;&#31034;&#35843;&#33410;&#26694;&#26550;(MultiPLN)&#65292;&#23558;NLEs&#34701;&#21512;&#21040;&#31435;&#22330;&#39044;&#27979;&#22120;&#20013;&#12290;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-target stance detection (CTSD) is an important task, which infers the attitude of the destination target by utilizing annotated data derived from the source target. One important approach in CTSD is to extract domain-invariant features to bridge the knowledge gap between multiple targets. However, the analysis of informal and short text structure, and implicit expressions, complicate the extraction of domain-invariant knowledge. In this paper, we propose a Multi-Perspective Prompt-Tuning (MPPT) model for CTSD that uses the analysis perspective as a bridge to transfer knowledge. First, we develop a two-stage instruct-based chain-of-thought method (TsCoT) to elicit target analysis perspectives and provide natural language explanations (NLEs) from multiple viewpoints by formulating instructions based on large language model (LLM). Second, we propose a multi-perspective prompt-tuning framework (MultiPLN) to fuse the NLEs into the stance predictor. Extensive experiments results demons
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25104;&#21151;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#36234;&#21335;&#35799;&#27468;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#32034;&#20102;&#23558;&#35799;&#27468;&#32763;&#35793;&#25104;&#19981;&#21516;&#35821;&#35328;&#30340;&#21487;&#33021;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#23545;&#29983;&#25104;&#20869;&#23481;&#30340;&#23436;&#20840;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2401.01078</link><description>&lt;p&gt;
&#36234;&#21335;&#35799;&#27468;&#29983;&#25104;&#19982;&#36328;&#35821;&#35328;&#35799;&#27468;&#32763;&#35793;&#30340;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
Vietnamese Poem Generation &amp; The Prospect Of Cross-Language Poem-To-Poem Translation. (arXiv:2401.01078v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01078
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25104;&#21151;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#36234;&#21335;&#35799;&#27468;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#32034;&#20102;&#23558;&#35799;&#27468;&#32763;&#35793;&#25104;&#19981;&#21516;&#35821;&#35328;&#30340;&#21487;&#33021;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#23545;&#29983;&#25104;&#20869;&#23481;&#30340;&#23436;&#20840;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35799;&#27468;&#29983;&#25104;&#19968;&#30452;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#19968;&#39033;&#25361;&#25112;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#35201;&#27714;&#27169;&#22411;&#29702;&#35299;&#35821;&#35328;&#12289;&#24773;&#24863;&#21644;&#39118;&#26684;&#30340;&#32454;&#24494;&#24046;&#21035;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#20013;&#29983;&#25104;&#36234;&#21335;&#35799;&#27468;&#65292;&#20174;&#32780;&#23454;&#29616;&#30452;&#35266;&#30340;&#36807;&#31243;&#21644;&#22686;&#24378;&#30340;&#20869;&#23481;&#25511;&#21046;&#12290;&#25105;&#20204;&#26368;&#26377;&#25928;&#30340;&#27169;&#22411;&#65292;GPT-3 Babbage&#21464;&#31181;&#65292;&#22312;&#36234;&#21335;&#35799;&#27468;&#30340;&#8220;&#20845;&#20843;&#35789;&#8221;&#31867;&#22411;&#20013;&#23454;&#29616;&#20102;0.8&#30340;&#33258;&#23450;&#20041;&#35780;&#20998;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#23558;&#35799;&#27468;&#25913;&#20889;&#25104;&#27491;&#24120;&#25991;&#26412;&#25552;&#31034;&#30340;&#24819;&#27861;&#65292;&#24182;&#22312;&#8220;&#20845;&#20843;&#35789;&#8221;&#31867;&#22411;&#20013;&#33719;&#24471;&#20102;&#30456;&#23545;&#36739;&#39640;&#30340;0.718&#20998;&#25968;&#12290;&#36825;&#20010;&#23454;&#39564;&#23637;&#31034;&#20102;&#20197;&#32763;&#35793;&#21518;&#30340;&#35799;&#27468;&#20316;&#20026;&#36755;&#20837;&#36827;&#34892;&#36328;&#35821;&#35328;&#35799;&#27468;&#32763;&#35793;&#30340;&#28508;&#21147;&#65292;&#24182;&#21516;&#26102;&#20445;&#25345;&#23545;&#29983;&#25104;&#20869;&#23481;&#30340;&#23436;&#20840;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Poetry generation has been a challenging task in the field of Natural Language Processing, as it requires the model to understand the nuances of language, sentiment, and style. In this paper, we propose using Large Language Models to generate Vietnamese poems from natural language prompts, thereby facilitating an intuitive process with enhanced content control. Our most efficacious model, the GPT-3 Babbage variant, achieves a custom evaluation score of 0.8, specifically tailored to the "luc bat" genre of Vietnamese poetry. Furthermore, we also explore the idea of paraphrasing poems into normal text prompts and yield a relatively high score of 0.718 in the "luc bat" genre. This experiment presents the potential for cross-Language poem-to-poem translation with translated poems as the inputs while concurrently maintaining complete control over the generated content.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#25552;&#20379;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;Vid-LLMs&#65289;&#22312;&#35270;&#39057;&#29702;&#35299;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#35814;&#32454;&#27010;&#36848;&#12290;Vid-LLMs&#30340;&#26032;&#20852;&#33021;&#21147;&#21253;&#25324;&#24320;&#25918;&#24335;&#26102;&#31354;&#25512;&#29702;&#21644;&#24120;&#35782;&#30693;&#35782;&#65292;&#20026;&#26410;&#26469;&#30340;&#35270;&#39057;&#29702;&#35299;&#25552;&#20379;&#20102;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2312.17432</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#39057;&#29702;&#35299;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Video Understanding with Large Language Models: A Survey. (arXiv:2312.17432v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.17432
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#25552;&#20379;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;Vid-LLMs&#65289;&#22312;&#35270;&#39057;&#29702;&#35299;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#35814;&#32454;&#27010;&#36848;&#12290;Vid-LLMs&#30340;&#26032;&#20852;&#33021;&#21147;&#21253;&#25324;&#24320;&#25918;&#24335;&#26102;&#31354;&#25512;&#29702;&#21644;&#24120;&#35782;&#30693;&#35782;&#65292;&#20026;&#26410;&#26469;&#30340;&#35270;&#39057;&#29702;&#35299;&#25552;&#20379;&#20102;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22312;&#32447;&#35270;&#39057;&#24179;&#21488;&#30340;&#19981;&#26029;&#22686;&#38271;&#21644;&#35270;&#39057;&#20869;&#23481;&#30340;&#19981;&#26029;&#22686;&#22810;&#65292;&#23545;&#29087;&#32451;&#30340;&#35270;&#39057;&#29702;&#35299;&#24037;&#20855;&#30340;&#38656;&#27714;&#26174;&#33879;&#22686;&#21152;&#12290;&#37492;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#30340;&#21331;&#36234;&#33021;&#21147;&#65292;&#26412;&#35843;&#26597;&#25552;&#20379;&#20102;&#23545;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;Vid-LLMs&#65289;&#25216;&#26415;&#36827;&#34892;&#35270;&#39057;&#29702;&#35299;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#35814;&#32454;&#27010;&#36848;&#12290;Vid-LLMs&#30340;&#26032;&#20852;&#33021;&#21147;&#20196;&#20154;&#24778;&#35766;&#65292;&#23588;&#20854;&#26159;&#23427;&#20204;&#22312;&#24320;&#25918;&#24335;&#26102;&#31354;&#25512;&#29702;&#21644;&#24120;&#35782;&#30693;&#35782;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#20026;&#26410;&#26469;&#30340;&#35270;&#39057;&#29702;&#35299;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;&#26412;&#35843;&#26597;&#23545;Vid-LLMs&#30340;&#29420;&#29305;&#29305;&#28857;&#21644;&#33021;&#21147;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#20998;&#20026;&#22235;&#31181;&#20027;&#35201;&#31867;&#22411;&#65306;&#22522;&#20110;LLM&#30340;&#35270;&#39057;&#20195;&#29702;&#12289;Vid-LLMs&#30340;&#39044;&#35757;&#32451;&#12289;Vid-LLMs&#30340;&#25351;&#20196;&#35843;&#25972;&#21644;&#28151;&#21512;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#26412;&#35843;&#26597;&#23545;Vid-LLMs&#30340;&#20219;&#21153;&#12289;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#12290;&#21478;&#22806;&#65292;&#23427;&#36824;&#25506;&#35752;&#20102;Vid-LLMs&#25216;&#26415;&#30340;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the burgeoning growth of online video platforms and the escalating volume of video content, the demand for proficient video understanding tools has intensified markedly. Given the remarkable capabilities of Large Language Models (LLMs) in language and multimodal tasks, this survey provides a detailed overview of the recent advancements in video understanding harnessing the power of LLMs (Vid-LLMs). The emergent capabilities of Vid-LLMs are surprisingly advanced, particularly their ability for open-ended spatial-temporal reasoning combined with commonsense knowledge, suggesting a promising path for future video understanding. We examine the unique characteristics and capabilities of Vid-LLMs, categorizing the approaches into four main types: LLM-based Video Agents, Vid-LLMs Pretraining, Vid-LLMs Instruction Tuning, and Hybrid Methods. Furthermore, this survey presents a comprehensive study of the tasks, datasets, and evaluation methodologies for Vid-LLMs. Additionally, it explores 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#23545;&#25239;&#24615;&#25915;&#20987;&#22312;&#20551;&#26032;&#38395;&#26816;&#27979;&#27169;&#22411;&#20013;&#30340;&#23041;&#32961;&#65292;&#30740;&#31350;&#20102;&#25915;&#20987;&#32773;&#22914;&#20309;&#22312;&#19981;&#20462;&#25913;&#21407;&#22987;&#30446;&#26631;&#26032;&#38395;&#30340;&#24773;&#20917;&#19979;&#36890;&#36807;&#24341;&#20837;&#27745;&#26579;&#25968;&#25454;&#26469;&#25805;&#32437;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2312.15228</link><description>&lt;p&gt;
&#23545;&#25239;&#24615;&#25968;&#25454;&#27745;&#26579;&#29992;&#20110;&#20551;&#26032;&#38395;&#26816;&#27979;&#65306;&#22914;&#20309;&#20351;&#27169;&#22411;&#22312;&#19981;&#20462;&#25913;&#30446;&#26631;&#26032;&#38395;&#30340;&#24773;&#20917;&#19979;&#23558;&#20854;&#38169;&#35823;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Adversarial Data Poisoning for Fake News Detection: How to Make a Model Misclassify a Target News without Modifying It. (arXiv:2312.15228v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.15228
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#23545;&#25239;&#24615;&#25915;&#20987;&#22312;&#20551;&#26032;&#38395;&#26816;&#27979;&#27169;&#22411;&#20013;&#30340;&#23041;&#32961;&#65292;&#30740;&#31350;&#20102;&#25915;&#20987;&#32773;&#22914;&#20309;&#22312;&#19981;&#20462;&#25913;&#21407;&#22987;&#30446;&#26631;&#26032;&#38395;&#30340;&#24773;&#20917;&#19979;&#36890;&#36807;&#24341;&#20837;&#27745;&#26579;&#25968;&#25454;&#26469;&#25805;&#32437;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20551;&#26032;&#38395;&#26816;&#27979;&#27169;&#22411;&#23545;&#20110;&#23545;&#25239;&#24615;&#25915;&#20987;&#20855;&#26377;&#33030;&#24369;&#24615;&#12290;&#22312;&#36825;&#31687;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#25915;&#20987;&#32773;&#22914;&#20309;&#22312;&#19981;&#20462;&#25913;&#21407;&#22987;&#30446;&#26631;&#26032;&#38395;&#30340;&#24773;&#20917;&#19979;&#30772;&#22351;&#22312;&#32447;&#23398;&#20064;&#26816;&#27979;&#22120;&#23545;&#29305;&#23450;&#26032;&#38395;&#20869;&#23481;&#30340;&#24615;&#33021;&#12290;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#20363;&#22914;&#31038;&#20132;&#32593;&#32476;&#20013;&#65292;&#25915;&#20987;&#32773;&#26080;&#27861;&#23436;&#20840;&#25511;&#21046;&#25152;&#26377;&#20449;&#24687;&#65292;&#36825;&#31181;&#24773;&#20917;&#30830;&#23454;&#21487;&#33021;&#21457;&#29983;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25915;&#20987;&#32773;&#22914;&#20309;&#21487;&#33021;&#36890;&#36807;&#23558;&#27745;&#26579;&#25968;&#25454;&#24341;&#20837;&#35757;&#32451;&#25968;&#25454;&#26469;&#25805;&#32437;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#22522;&#20110;&#22797;&#26434;&#24615;&#21644;&#25915;&#20987;&#31867;&#22411;&#65292;&#36923;&#36753;&#22238;&#24402;&#27169;&#22411;&#23545;&#27492;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#21508;&#19981;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fake news detection models are critical to countering disinformation but can be manipulated through adversarial attacks. In this position paper, we analyze how an attacker can compromise the performance of an online learning detector on specific news content without being able to manipulate the original target news. In some contexts, such as social networks, where the attacker cannot exert complete control over all the information, this scenario can indeed be quite plausible. Therefore, we show how an attacker could potentially introduce poisoning data into the training data to manipulate the behavior of an online learning method. Our initial findings reveal varying susceptibility of logistic regression models based on complexity and attack type.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#31561;&#21464;&#24615;&#30340;&#24187;&#35273;&#29702;&#35770;&#65292;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#30340;&#25104;&#22240;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#34913;&#37327;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#31243;&#24230;&#30340;&#20132;&#21449;&#29109;&#35823;&#24046;&#20989;&#25968;&#12290;&#36890;&#36807;&#27979;&#35797;&#35821;&#35328;&#27169;&#22411;&#22312;&#33719;&#24471;&#31561;&#21464;&#24615;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#30740;&#31350;&#34920;&#26126;&#26576;&#20123;&#31867;&#22411;&#30340;&#31561;&#21464;&#35821;&#35328;&#27169;&#22411;&#23545;&#29702;&#35299;&#22797;&#26434;&#31038;&#20132;&#20851;&#31995;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2312.14504</link><description>&lt;p&gt;
&#22522;&#20110;&#31561;&#21464;&#24615;&#30340;&#24187;&#35273;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Theory of Hallucinations based on Equivariance. (arXiv:2312.14504v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.14504
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#31561;&#21464;&#24615;&#30340;&#24187;&#35273;&#29702;&#35770;&#65292;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#30340;&#25104;&#22240;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#34913;&#37327;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#31243;&#24230;&#30340;&#20132;&#21449;&#29109;&#35823;&#24046;&#20989;&#25968;&#12290;&#36890;&#36807;&#27979;&#35797;&#35821;&#35328;&#27169;&#22411;&#22312;&#33719;&#24471;&#31561;&#21464;&#24615;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#30740;&#31350;&#34920;&#26126;&#26576;&#20123;&#31867;&#22411;&#30340;&#31561;&#21464;&#35821;&#35328;&#27169;&#22411;&#23545;&#29702;&#35299;&#22797;&#26434;&#31038;&#20132;&#20851;&#31995;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#33719;&#21462;&#21019;&#24314;&#23545;&#24187;&#35273;&#20813;&#30123;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25152;&#38656;&#30340;&#30693;&#35782;&#12290;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#24448;&#24448;&#24402;&#22240;&#20110;&#23545;&#29616;&#23454;&#31038;&#20132;&#20851;&#31995;&#30340;&#35823;&#35299;&#12290;&#22240;&#27492;&#65292;&#25105;&#20551;&#35774;&#33021;&#22815;&#24443;&#24213;&#25484;&#25569;&#25152;&#26377;&#36825;&#20123;&#20851;&#31995;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#19981;&#20250;&#20986;&#29616;&#24187;&#35273;&#12290;&#27492;&#22806;&#65292;&#25105;&#25552;&#20986;&#20102;&#26576;&#20123;&#31867;&#22411;&#30340;&#31561;&#21464;&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#21644;&#29702;&#35299;&#36825;&#20123;&#20851;&#31995;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#24320;&#21457;&#20102;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#21019;&#24314;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#31243;&#24230;&#30340;&#20132;&#21449;&#29109;&#35823;&#24046;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#34913;&#37327;&#20102;&#23427;&#20204;&#33719;&#24471;&#31561;&#21464;&#24615;&#30340;&#31243;&#24230;&#12290;&#21033;&#29992;&#36825;&#20010;&#25351;&#26631;&#65292;&#25105;&#27979;&#35797;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#33719;&#24471;&#23383;&#31526;&#32423;&#31561;&#21464;&#24615;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#24341;&#20837;&#24182;&#37319;&#29992;&#20102;&#19968;&#31181;&#22522;&#20110;T5&#65288;&#25991;&#26412;&#21040;&#25991;&#26412;&#36716;&#25442;&#21464;&#21387;&#22120;&#65289;&#30340;&#26032;&#25216;&#26415;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#29702;&#35299;&#32463;&#36807;&#25490;&#21015;&#30340;&#36755;&#20837;&#25991;&#26412;&#32780;&#26080;&#38656;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study aims to acquire knowledge for creating very large language models that are immune to hallucinations. Hallucinations in contemporary large language models are often attributed to a misunderstanding of real-world social relationships. Therefore, I hypothesize that very large language models capable of thoroughly grasping all these relationships will be free from hallucinations. Additionally, I propose that certain types of equivariant language models are adept at learning and understanding these relationships. Building on this, I have developed a specialized cross-entropy error function to create a hallucination scale for language models, which measures their extent of equivariance acquisition. Utilizing this scale, I tested language models for their ability to acquire character-level equivariance. In particular, I introduce and employ a novel technique based on T5 (Text To Text Transfer Transformer) that efficiently understands permuted input texts without the need for explic
&lt;/p&gt;</description></item><item><title>T-Eval&#26159;&#19968;&#31181;&#36880;&#27493;&#35780;&#20272;&#24037;&#20855;&#21033;&#29992;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#23427;&#23558;&#24037;&#20855;&#21033;&#29992;&#35780;&#20272;&#35299;&#32806;&#20026;&#22810;&#20010;&#23376;&#39046;&#22495;&#65292;&#20174;&#32780;&#33021;&#22815;&#26356;&#32454;&#33268;&#22320;&#20998;&#26512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2312.14033</link><description>&lt;p&gt;
T-Eval: &#36880;&#27493;&#35780;&#20272;&#24037;&#20855;&#21033;&#29992;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
T-Eval: Evaluating the Tool Utilization Capability Step by Step. (arXiv:2312.14033v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.14033
&lt;/p&gt;
&lt;p&gt;
T-Eval&#26159;&#19968;&#31181;&#36880;&#27493;&#35780;&#20272;&#24037;&#20855;&#21033;&#29992;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#23427;&#23558;&#24037;&#20855;&#21033;&#29992;&#35780;&#20272;&#35299;&#32806;&#20026;&#22810;&#20010;&#23376;&#39046;&#22495;&#65292;&#20174;&#32780;&#33021;&#22815;&#26356;&#32454;&#33268;&#22320;&#20998;&#26512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#24037;&#20855;&#36827;&#34892;&#20102;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#35780;&#20272;&#21644;&#20998;&#26512;LLM&#30340;&#24037;&#20855;&#21033;&#29992;&#33021;&#21147;&#20173;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#19982;&#20197;&#24448;&#35780;&#20272;&#27169;&#22411;&#25972;&#20307;&#24615;&#33021;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#23558;&#24037;&#20855;&#21033;&#29992;&#20840;&#38754;&#20998;&#35299;&#20026;&#22810;&#20010;&#23376;&#36807;&#31243;&#65292;&#21253;&#25324;&#25351;&#20196;&#36319;&#38543;&#12289;&#35268;&#21010;&#12289;&#25512;&#29702;&#12289;&#26816;&#32034;&#12289;&#29702;&#35299;&#21644;&#22797;&#26597;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;T-Eval&#26469;&#36880;&#27493;&#35780;&#20272;&#24037;&#20855;&#21033;&#29992;&#33021;&#21147;&#12290;T-Eval&#23558;&#24037;&#20855;&#21033;&#29992;&#35780;&#20272;&#35299;&#32806;&#20026;&#22810;&#20010;&#23376;&#39046;&#22495;&#65292;&#26377;&#21161;&#20110;&#23545;LLM&#30340;&#25972;&#20307;&#21644;&#29420;&#31435;&#33021;&#21147;&#36827;&#34892;&#20869;&#37096;&#29702;&#35299;&#12290;&#25105;&#20204;&#23545;T-Eval&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#21644;&#21508;&#31181;LLM&#30340;&#28145;&#20837;&#20998;&#26512;&#12290;T-Eval&#19981;&#20165;&#23637;&#29616;&#20102;&#19982;&#32467;&#26524;&#23548;&#21521;&#35780;&#20272;&#30340;&#19968;&#33268;&#24615;&#65292;&#36824;&#25552;&#20379;&#20102;&#23545;LLM&#33021;&#21147;&#26356;&#32454;&#33268;&#30340;&#20998;&#26512;&#65292;&#34920;&#26126;LLM&#20855;&#22791;&#20102;&#19968;&#23450;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLM) have achieved remarkable performance on various NLP tasks and are augmented by tools for broader applications. Yet, how to evaluate and analyze the tool-utilization capability of LLMs is still under-explored. In contrast to previous works that evaluate models holistically, we comprehensively decompose the tool utilization into multiple sub-processes, including instruction following, planning, reasoning, retrieval, understanding, and review. Based on that, we further introduce T-Eval to evaluate the tool utilization capability step by step. T-Eval disentangles the tool utilization evaluation into several sub-domains along model capabilities, facilitating the inner understanding of both holistic and isolated competency of LLMs. We conduct extensive experiments on T-Eval and in-depth analysis of various LLMs. T-Eval not only exhibits consistency with the outcome-oriented evaluation but also provides a more fine-grained analysis of the capabilities of LLMs, prov
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35780;&#20272;&#20102;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#22312;&#29616;&#23454;&#33258;&#20027;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#36825;&#20123;&#20195;&#29702;&#21482;&#33021;&#23436;&#25104;&#26368;&#31616;&#21333;&#30340;&#20219;&#21153;&#65292;&#23545;&#20110;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#26377;&#19968;&#23450;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2312.11671</link><description>&lt;p&gt;
&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#22312;&#29616;&#23454;&#33258;&#20027;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Evaluating Language-Model Agents on Realistic Autonomous Tasks. (arXiv:2312.11671v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11671
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35780;&#20272;&#20102;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#22312;&#29616;&#23454;&#33258;&#20027;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#36825;&#20123;&#20195;&#29702;&#21482;&#33021;&#23436;&#25104;&#26368;&#31616;&#21333;&#30340;&#20219;&#21153;&#65292;&#23545;&#20110;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#26377;&#19968;&#23450;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#22312;&#37326;&#22806;&#33719;&#21462;&#36164;&#28304;&#12289;&#22797;&#21046;&#33258;&#36523;&#21644;&#36866;&#24212;&#26032;&#25361;&#25112;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#31216;&#36825;&#20123;&#33021;&#21147;&#20026;"&#33258;&#20027;&#22797;&#21046;&#21644;&#36866;&#24212;"&#25110;&#32773;ARA&#12290;&#25105;&#20204;&#35748;&#20026;&#20855;&#22791;ARA&#33021;&#21147;&#30340;&#31995;&#32479;&#21487;&#33021;&#20855;&#26377;&#24191;&#27867;&#32780;&#38590;&#20197;&#39044;&#27979;&#30340;&#21518;&#26524;&#65292;&#24182;&#19988;&#23545;&#20110;&#34913;&#37327;&#21644;&#39044;&#27979;ARA&#33021;&#21147;&#21487;&#33021;&#26377;&#21161;&#20110;&#21046;&#23450;&#30456;&#20851;&#30340;&#23433;&#20840;&#12289;&#30417;&#27979;&#21644;&#23545;&#40784;&#25514;&#26045;&#12290;&#27492;&#22806;&#65292;&#19968;&#26086;&#31995;&#32479;&#20855;&#22791;ARA&#33021;&#21147;&#65292;&#23545;&#31995;&#32479;&#33021;&#21147;&#30340;&#38480;&#21046;&#21487;&#33021;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#22235;&#20010;&#31616;&#21333;&#30340;&#31034;&#20363;&#20195;&#29702;&#65292;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#20801;&#35768;&#20854;&#22312;&#19990;&#30028;&#20013;&#37319;&#21462;&#34892;&#21160;&#30340;&#24037;&#20855;&#30456;&#32467;&#21512;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#36825;&#20123;&#20195;&#29702;&#22312;&#19982;ARA&#30456;&#20851;&#30340;12&#20010;&#20219;&#21153;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#21482;&#33021;&#23436;&#25104;&#20219;&#21153;&#21015;&#34920;&#20013;&#26368;&#31616;&#21333;&#30340;&#20219;&#21153;&#65292;&#23613;&#31649;&#23545;&#20110;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#20063;&#26377;&#19968;&#23450;&#30340;&#36827;&#23637;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20123;&#35780;&#20272;&#36824;&#27809;&#26377;&#23436;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this report, we explore the ability of language model agents to acquire resources, create copies of themselves, and adapt to novel challenges they encounter in the wild. We refer to this cluster of capabilities as "autonomous replication and adaptation" or ARA. We believe that systems capable of ARA could have wide-reaching and hard-to-anticipate consequences, and that measuring and forecasting ARA may be useful for informing measures around security, monitoring, and alignment. Additionally, once a system is capable of ARA, placing bounds on a system's capabilities may become significantly more difficult.  We construct four simple example agents that combine language models with tools that allow them to take actions in the world. We then evaluate these agents on 12 tasks relevant to ARA. We find that these language model agents can only complete the easiest tasks from this list, although they make some progress on the more challenging tasks. Unfortunately, these evaluations are not 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Nuggets&#30340;&#26032;&#39062;&#26377;&#25928;&#26041;&#27861;&#65292;&#21033;&#29992;&#21333;&#27425;&#23398;&#20064;&#20174;&#24222;&#22823;&#30340;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#25351;&#23548;&#25968;&#25454;&#65292;&#36890;&#36807;&#35780;&#20272;&#31034;&#20363;&#23545;&#22810;&#26679;&#38170;&#23450;&#38598;&#30340;&#22256;&#24785;&#24230;&#24433;&#21709;&#65292;&#36873;&#25321;&#23545;&#25351;&#23548;&#35843;&#20248;&#26368;&#26377;&#30410;&#30340;&#25968;&#25454;</title><link>http://arxiv.org/abs/2312.10302</link><description>&lt;p&gt;
&#20316;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#23548;&#25968;&#25454;&#25506;&#32034;&#32773;&#30340;&#21333;&#27425;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
One Shot Learning as Instruction Data Prospector for Large Language Models. (arXiv:2312.10302v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.10302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Nuggets&#30340;&#26032;&#39062;&#26377;&#25928;&#26041;&#27861;&#65292;&#21033;&#29992;&#21333;&#27425;&#23398;&#20064;&#20174;&#24222;&#22823;&#30340;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#25351;&#23548;&#25968;&#25454;&#65292;&#36890;&#36807;&#35780;&#20272;&#31034;&#20363;&#23545;&#22810;&#26679;&#38170;&#23450;&#38598;&#30340;&#22256;&#24785;&#24230;&#24433;&#21709;&#65292;&#36873;&#25321;&#23545;&#25351;&#23548;&#35843;&#20248;&#26368;&#26377;&#30410;&#30340;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#23545;&#40784;&#26159;&#26377;&#25928;&#21033;&#29992;&#20854;&#39044;&#35757;&#32451;&#33021;&#21147;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#24403;&#21069;&#30340;&#25351;&#23548;&#35843;&#20248;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#25193;&#23637;&#25968;&#25454;&#38598;&#22823;&#23567;&#65292;&#20294;&#32570;&#20047;&#30830;&#20445;&#25968;&#25454;&#36136;&#37327;&#30340;&#26126;&#30830;&#31574;&#30053;&#65292;&#36825;&#21487;&#33021;&#26080;&#24847;&#20013;&#24341;&#20837;&#22122;&#22768;&#24182;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#39640;&#25928;&#30340;&#26041;&#27861;Nuggets&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#21333;&#27425;&#23398;&#20064;&#20174;&#24222;&#22823;&#30340;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#25351;&#23548;&#25968;&#25454;&#12290;Nuggets&#35780;&#20272;&#21333;&#20010;&#25351;&#23548;&#31034;&#20363;&#20316;&#20026;&#26377;&#25928;&#21333;&#27425;&#31034;&#20363;&#30340;&#28508;&#21147;&#65292;&#20174;&#32780;&#35782;&#21035;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#21508;&#31181;&#20219;&#21153;&#24615;&#33021;&#30340;&#31034;&#20363;&#12290;Nuggets&#21033;&#29992;&#22522;&#20110;&#20505;&#36873;&#31034;&#20363;&#23545;&#22810;&#26679;&#38170;&#23450;&#38598;&#30340;&#22256;&#24785;&#24230;&#24433;&#21709;&#30340;&#35780;&#20998;&#31995;&#32479;&#65292;&#26377;&#21161;&#20110;&#36873;&#25321;&#23545;&#25351;&#23548;&#35843;&#20248;&#26368;&#26377;&#30410;&#30340;&#25968;&#25454;&#12290;&#36890;&#36807;&#22312;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#38598;MT-Bench&#21644;Alpaca-Ev&#19978;&#36827;&#34892;&#20005;&#26684;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Aligning large language models(LLMs) with human is a critical step in effectively utilizing their pre-trained capabilities across a wide array of language tasks. Current instruction tuning practices often rely on expanding dataset size without a clear strategy for ensuring data quality, which can inadvertently introduce noise and degrade model performance. To address this challenge, we introduce Nuggets, a novel and efficient methodology that employs one shot learning to select high-quality instruction data from expansive datasets. Nuggets assesses the potential of individual instruction examples to act as effective one shot examples, thereby identifying those that can significantly enhance diverse task performance. Nuggets utilizes a scoring system based on the impact of candidate examples on the perplexity of a diverse anchor set, facilitating the selection of the most beneficial data for instruction tuning. Through rigorous testing on two benchmarks, including MT-Bench and Alpaca-Ev
&lt;/p&gt;</description></item><item><title>UstanceBR&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#35821;&#35328;&#36164;&#28304;&#65292;&#29992;&#20110;&#30446;&#26631;&#31435;&#22330;&#39044;&#27979;&#65292;&#21253;&#21547;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;Twitter&#39046;&#22495;&#30340;86.8k&#26631;&#35760;&#31435;&#22330;&#21644;&#21457;&#24067;&#32773;&#30340;&#32593;&#32476;&#20449;&#24687;&#12290;&#36825;&#20010;&#30740;&#31350;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#21021;&#22987;&#22522;&#20934;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2312.06374</link><description>&lt;p&gt;
UstanceBR:&#19968;&#31181;&#29992;&#20110;&#30446;&#26631;&#31435;&#22330;&#39044;&#27979;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#36164;&#28304;
&lt;/p&gt;
&lt;p&gt;
UstanceBR: a multimodal language resource for stance prediction. (arXiv:2312.06374v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.06374
&lt;/p&gt;
&lt;p&gt;
UstanceBR&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#35821;&#35328;&#36164;&#28304;&#65292;&#29992;&#20110;&#30446;&#26631;&#31435;&#22330;&#39044;&#27979;&#65292;&#21253;&#21547;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;Twitter&#39046;&#22495;&#30340;86.8k&#26631;&#35760;&#31435;&#22330;&#21644;&#21457;&#24067;&#32773;&#30340;&#32593;&#32476;&#20449;&#24687;&#12290;&#36825;&#20010;&#30740;&#31350;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#21021;&#22987;&#22522;&#20934;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;UstanceBR&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;Twitter&#39046;&#22495;&#30340;&#22810;&#27169;&#24577;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#30446;&#26631;&#31435;&#22330;&#39044;&#27979;&#12290;&#35813;&#35821;&#26009;&#24211;&#21253;&#21547;&#23545;&#25152;&#36873;&#30446;&#26631;&#20027;&#39064;&#30340;86.8k&#26631;&#35760;&#31435;&#22330;&#65292;&#24182;&#19988;&#21253;&#21547;&#20102;&#21457;&#24067;&#36825;&#20123;&#31435;&#22330;&#30340;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#30340;&#24191;&#27867;&#32593;&#32476;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#35821;&#26009;&#24211;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20110;&#25991;&#26412;&#21644;&#32593;&#32476;&#30456;&#20851;&#20449;&#24687;&#30340;&#39046;&#22495;&#20869;&#21644;&#38646;&#26679;&#26412;&#31435;&#22330;&#39044;&#27979;&#30340;&#22810;&#20010;&#20351;&#29992;&#31034;&#20363;&#65292;&#26088;&#22312;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#21021;&#22987;&#22522;&#20934;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces UstanceBR, a multimodal corpus in the Brazilian Portuguese Twitter domain for target-based stance prediction. The corpus comprises 86.8 k labelled stances towards selected target topics, and extensive network information about the users who published these stances on social media. In this article we describe the corpus multimodal data, and a number of usage examples in both in-domain and zero-shot stance prediction based on textand network-related information, which are intended to provide initial baseline results for future studies in the field.
&lt;/p&gt;</description></item><item><title>TEAL&#26159;&#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#23558;&#19981;&#21516;&#27169;&#24577;&#30340;&#36755;&#20837;&#35270;&#20026;&#20196;&#29260;&#24207;&#21015;&#65292;&#24182;&#23398;&#20064;&#23427;&#20204;&#30340;&#32852;&#21512;&#23884;&#20837;&#31354;&#38388;&#12290;&#36825;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#24314;&#27169;&#22810;&#27169;&#24577;&#36755;&#20837;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#29983;&#25104;&#38750;&#25991;&#26412;&#27169;&#24577;&#30340;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2311.04589</link><description>&lt;p&gt;
TEAL: &#23545;&#20110;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#31181;&#23558;&#25152;&#26377;&#27169;&#24577;&#36827;&#34892;&#20998;&#35789;&#21644;&#23884;&#20837;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
TEAL: Tokenize and Embed ALL for Multi-modal Large Language Models. (arXiv:2311.04589v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.04589
&lt;/p&gt;
&lt;p&gt;
TEAL&#26159;&#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#23558;&#19981;&#21516;&#27169;&#24577;&#30340;&#36755;&#20837;&#35270;&#20026;&#20196;&#29260;&#24207;&#21015;&#65292;&#24182;&#23398;&#20064;&#23427;&#20204;&#30340;&#32852;&#21512;&#23884;&#20837;&#31354;&#38388;&#12290;&#36825;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#24314;&#27169;&#22810;&#27169;&#24577;&#36755;&#20837;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#29983;&#25104;&#38750;&#25991;&#26412;&#27169;&#24577;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MM-LLMs&#65289;&#26368;&#36817;&#21462;&#24471;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#22312;&#26377;&#25928;&#22320;&#24314;&#27169;&#22810;&#27169;&#24577;&#36755;&#20837;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#21644;&#38750;&#25991;&#26412;&#27169;&#24577;&#30340;&#29983;&#25104;&#26041;&#38754;&#38754;&#20020;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TEAL&#65288;Tokenize and Embed ALL&#65289;&#30340;&#26041;&#27861;&#65292;&#23558;&#20219;&#20309;&#27169;&#24577;&#30340;&#36755;&#20837;&#35270;&#20026;&#20196;&#29260;&#24207;&#21015;&#65292;&#24182;&#23398;&#20064;&#25152;&#26377;&#27169;&#24577;&#30340;&#32852;&#21512;&#23884;&#20837;&#31354;&#38388;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#20219;&#20309;&#27169;&#24577;&#30340;&#36755;&#20837;&#65292;TEAL&#39318;&#20808;&#20351;&#29992;&#29616;&#25104;&#30340;&#20998;&#35789;&#22120;&#23558;&#20854;&#31163;&#25955;&#21270;&#20026;&#20196;&#29260;&#24207;&#21015;&#65292;&#28982;&#21518;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#23884;&#20837;&#30697;&#38453;&#23558;&#20196;&#29260;&#24207;&#21015;&#23884;&#20837;&#21040;&#32852;&#21512;&#23884;&#20837;&#31354;&#38388;&#20013;&#12290;MM-LLMs&#21482;&#38656;&#35201;&#20687;&#25991;&#26412;LLMs&#37027;&#26679;&#33258;&#22238;&#24402;&#22320;&#39044;&#27979;&#22810;&#27169;&#24577;&#20196;&#29260;&#12290;&#26368;&#21518;&#65292;&#26681;&#25454;&#39044;&#27979;&#30340;&#20196;&#29260;&#24207;&#21015;&#65292;&#24212;&#29992;&#30456;&#24212;&#30340;&#21435;&#20998;&#35789;&#22120;&#29983;&#25104;&#27599;&#20010;&#27169;&#24577;&#30340;&#36755;&#20986;&#12290;&#36890;&#36807;&#32852;&#21512;&#23884;&#20837;&#31354;&#38388;&#65292;TEAL&#20351;&#20923;&#32467;&#30340;LLMs&#33021;&#22815;&#25191;&#34892;&#28041;&#21450;&#38750;&#25991;&#26412;&#27169;&#24577;&#30340;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#65292;&#22914;&#29702;&#35299;&#21644;&#29983;&#25104;&#22270;&#20687;&#25110;&#38899;&#39057;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite Multi-modal Large Language Models (MM-LLMs) have made exciting strides recently, they are still struggling to efficiently model the interactions among multi-modal inputs and the generation in non-textual modalities. In this work, we propose TEAL (Tokenize and Embed ALl)}, an approach to treat the input from any modality as a token sequence and learn a joint embedding space for all modalities. Specifically, for the input from any modality, TEAL first discretizes it into a token sequence with the off-the-shelf tokenizer and embeds the token sequence into a joint embedding space with a learnable embedding matrix. MM-LLMs just need to predict the multi-modal tokens autoregressively as the textual LLMs do. Finally, the corresponding de-tokenizer is applied to generate the output in each modality based on the predicted token sequence. With the joint embedding space, TEAL enables the frozen LLMs to perform both understanding and generation tasks involving non-textual modalities, such 
&lt;/p&gt;</description></item><item><title>GIT-Mol&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#22312;&#20998;&#23376;&#31185;&#23398;&#20013;&#22788;&#29702;&#22270;&#20687;&#12289;&#22270;&#24418;&#21644;&#25991;&#26412;&#20449;&#24687;&#12290;&#36890;&#36807;&#26032;&#25552;&#20986;&#30340;GIT-Former&#26550;&#26500;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#23558;&#22810;&#31181;&#27169;&#24577;&#30340;&#25968;&#25454;&#23545;&#40784;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#12290;&#19982;&#22522;&#32447;&#30456;&#27604;&#65292;GIT-Mol&#22312;&#24615;&#36136;&#39044;&#27979;&#21644;&#20998;&#23376;&#29983;&#25104;&#26377;&#25928;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#35813;&#27169;&#22411;&#36824;&#21487;&#29992;&#20110;&#21270;&#21512;&#29289;&#21517;&#31216;&#35782;&#21035;&#21644;&#21270;&#23398;&#21453;&#24212;&#39044;&#27979;&#31561;&#19979;&#28216;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.06911</link><description>&lt;p&gt;
GIT-Mol&#65306;&#19968;&#31181;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#20998;&#23376;&#31185;&#23398;&#20013;&#30340;&#22270;&#20687;&#65292;&#22270;&#24418;&#21644;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
GIT-Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text. (arXiv:2308.06911v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06911
&lt;/p&gt;
&lt;p&gt;
GIT-Mol&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#22312;&#20998;&#23376;&#31185;&#23398;&#20013;&#22788;&#29702;&#22270;&#20687;&#12289;&#22270;&#24418;&#21644;&#25991;&#26412;&#20449;&#24687;&#12290;&#36890;&#36807;&#26032;&#25552;&#20986;&#30340;GIT-Former&#26550;&#26500;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#23558;&#22810;&#31181;&#27169;&#24577;&#30340;&#25968;&#25454;&#23545;&#40784;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#12290;&#19982;&#22522;&#32447;&#30456;&#27604;&#65292;GIT-Mol&#22312;&#24615;&#36136;&#39044;&#27979;&#21644;&#20998;&#23376;&#29983;&#25104;&#26377;&#25928;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#35813;&#27169;&#22411;&#36824;&#21487;&#29992;&#20110;&#21270;&#21512;&#29289;&#21517;&#31216;&#35782;&#21035;&#21644;&#21270;&#23398;&#21453;&#24212;&#39044;&#27979;&#31561;&#19979;&#28216;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#36890;&#36807;&#22788;&#29702;&#20998;&#23376;&#30340;&#25991;&#26412;&#34920;&#31034;&#65292;&#20026;&#20998;&#23376;&#31185;&#23398;&#20013;&#30340;&#21019;&#26032;&#24212;&#29992;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#25429;&#25417;&#20855;&#26377;&#22797;&#26434;&#20998;&#23376;&#32467;&#26500;&#25110;&#22270;&#20687;&#30340;&#20016;&#23500;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;GIT-Mol&#65292;&#19968;&#31181;&#38598;&#25104;&#20102;&#22270;&#24418;&#12289;&#22270;&#20687;&#21644;&#25991;&#26412;&#20449;&#24687;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#20026;&#20102;&#20419;&#36827;&#22810;&#27169;&#24577;&#20998;&#23376;&#25968;&#25454;&#30340;&#38598;&#25104;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GIT-Former&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#65292;&#33021;&#22815;&#23558;&#25152;&#26377;&#27169;&#24577;&#23545;&#40784;&#21040;&#32479;&#19968;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#12290;&#19982;&#22522;&#32447;&#30456;&#27604;&#65292;&#25105;&#20204;&#22312;&#24615;&#36136;&#39044;&#27979;&#26041;&#38754;&#23454;&#29616;&#20102;5%-10%&#30340;&#20934;&#30830;&#24615;&#25552;&#39640;&#65292;&#24182;&#22312;&#20998;&#23376;&#29983;&#25104;&#26377;&#25928;&#24615;&#26041;&#38754;&#25552;&#39640;&#20102;20.2%&#12290;&#36890;&#36807;&#20219;&#24847;&#21040;&#35821;&#35328;&#30340;&#20998;&#23376;&#32763;&#35793;&#31574;&#30053;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#26377;&#28508;&#21147;&#36827;&#34892;&#26356;&#22810;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#20363;&#22914;&#21270;&#21512;&#29289;&#21517;&#31216;&#35782;&#21035;&#21644;&#21270;&#23398;&#21453;&#24212;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have made significant strides in natural language processing, enabling innovative applications in molecular science by processing textual representations of molecules. However, most existing language models cannot capture the rich information with complex molecular structures or images. In this paper, we introduce GIT-Mol, a multi-modal large language model that integrates the Graph, Image, and Text information. To facilitate the integration of multi-modal molecular data, we propose GIT-Former, a novel architecture that is capable of aligning all modalities into a unified latent space. We achieve a 5%-10% accuracy increase in properties prediction and a 20.2% boost in molecule generation validity compared to the baselines. With the any-to-language molecular translation strategy, our model has the potential to perform more downstream tasks, such as compound name recognition and chemical reaction prediction.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#24515;&#29702;&#23398;&#20013;&#30340;&#24773;&#24863;&#35780;&#20272;&#29702;&#35770;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#21033;&#29992;EmotionBench&#35780;&#20272;LLMs&#30340;&#20849;&#24773;&#33021;&#21147;&#12290;&#36890;&#36807;&#20154;&#31867;&#35780;&#20272;&#21644;&#23545;&#20116;&#20010;LLMs&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#23613;&#31649;&#23384;&#22312;&#19968;&#20123;&#19981;&#19968;&#33268;&#20043;&#22788;&#65292;LLMs&#36890;&#24120;&#33021;&#22312;&#26576;&#20123;&#24773;&#22659;&#19979;&#36866;&#24403;&#22320;&#22238;&#24212;&#65292;&#20294;&#19982;&#24773;&#24863;&#23545;&#40784;&#26041;&#38754;&#36824;&#23384;&#22312;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2308.03656</link><description>&lt;p&gt;
&#24863;&#35273;&#40635;&#26408;&#36824;&#26159;&#26377;&#20849;&#24773;&#33021;&#21147;&#65311;&#21033;&#29992;EmotionBench&#35780;&#20272;LLMs&#30340;&#24773;&#24863;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Emotionally Numb or Empathetic? Evaluating How LLMs Feel Using EmotionBench. (arXiv:2308.03656v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03656
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#24515;&#29702;&#23398;&#20013;&#30340;&#24773;&#24863;&#35780;&#20272;&#29702;&#35770;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#21033;&#29992;EmotionBench&#35780;&#20272;LLMs&#30340;&#20849;&#24773;&#33021;&#21147;&#12290;&#36890;&#36807;&#20154;&#31867;&#35780;&#20272;&#21644;&#23545;&#20116;&#20010;LLMs&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#23613;&#31649;&#23384;&#22312;&#19968;&#20123;&#19981;&#19968;&#33268;&#20043;&#22788;&#65292;LLMs&#36890;&#24120;&#33021;&#22312;&#26576;&#20123;&#24773;&#22659;&#19979;&#36866;&#24403;&#22320;&#22238;&#24212;&#65292;&#20294;&#19982;&#24773;&#24863;&#23545;&#40784;&#26041;&#38754;&#36824;&#23384;&#22312;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20195;&#35805;&#35821;&#20013;&#65292;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25311;&#20154;&#33021;&#21147;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#21033;&#29992;&#24515;&#29702;&#23398;&#20013;&#30340;&#24773;&#24863;&#35780;&#20272;&#29702;&#35770;&#65292;&#25105;&#20204;&#25552;&#20986;&#35780;&#20272;LLMs&#30340;&#20849;&#24773;&#33021;&#21147;&#65292;&#21363;&#23427;&#20204;&#22312;&#29305;&#23450;&#24773;&#22659;&#19979;&#24863;&#21463;&#21464;&#21270;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#20180;&#32454;&#32780;&#20840;&#38754;&#30340;&#35843;&#26597;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;400&#31181;&#24773;&#22659;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#24773;&#22659;&#24050;&#34987;&#35777;&#26126;&#23545;&#25105;&#20204;&#30740;&#31350;&#30340;&#20843;&#31181;&#24773;&#24863;&#33267;&#20851;&#37325;&#35201;&#12290;&#23558;&#36825;&#20123;&#24773;&#22659;&#20998;&#20026;36&#20010;&#22240;&#32032;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#28041;&#21450;&#20840;&#29699;1200&#22810;&#21517;&#34987;&#35797;&#30340;&#20154;&#31867;&#35780;&#20272;&#12290;&#20197;&#20154;&#31867;&#35780;&#20272;&#32467;&#26524;&#20026;&#21442;&#32771;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20116;&#20010;LLMs&#65292;&#28085;&#30422;&#20102;&#21830;&#19994;&#21644;&#24320;&#28304;&#27169;&#22411;&#65292;&#21253;&#25324;&#27169;&#22411;&#22823;&#23567;&#30340;&#21464;&#21270;&#65292;&#20197;&#21450;&#26368;&#26032;&#30340;&#36845;&#20195;&#29256;&#26412;&#65288;&#22914;GPT-4&#21644;LLaMA-2&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#23384;&#22312;&#19968;&#20123;&#19981;&#19968;&#33268;&#20043;&#22788;&#65292;LLMs&#36890;&#24120;&#33021;&#22312;&#26576;&#20123;&#24773;&#22659;&#19979;&#36866;&#24403;&#22320;&#22238;&#24212;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#19982;&#24773;&#24863;&#23545;&#40784;&#26041;&#38754;&#36824;&#23384;&#22312;&#19968;&#23450;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating Large Language Models' (LLMs) anthropomorphic capabilities has become increasingly important in contemporary discourse. Utilizing the emotion appraisal theory from psychology, we propose to evaluate the empathy ability of LLMs, i.e., how their feelings change when presented with specific situations. After a careful and comprehensive survey, we collect a dataset containing over 400 situations that have proven effective in eliciting the eight emotions central to our study. Categorizing the situations into 36 factors, we conduct a human evaluation involving more than 1,200 subjects worldwide. With the human evaluation results as references, our evaluation includes five LLMs, covering both commercial and open-source models, including variations in model sizes, featuring the latest iterations, such as GPT-4 and LLaMA-2. We find that, despite several misalignments, LLMs can generally respond appropriately to certain situations. Nevertheless, they fall short in alignment with the e
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#21333;&#20154;&#34920;&#29616;&#25552;&#31034;&#65288;SPP&#65289;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#19982;&#22810;&#20010;&#35282;&#33394;&#36827;&#34892;&#22810;&#36718;&#33258;&#25105;&#21327;&#20316;&#65292;&#23558;&#21333;&#20010;&#35821;&#35328;&#27169;&#22411;&#36716;&#21270;&#20026;&#35748;&#30693;&#21327;&#21516;&#32773;&#65292;&#20174;&#32780;&#22686;&#24378;&#20854;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#21644;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.05300</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#37322;&#25918;&#35748;&#30693;&#21327;&#21516;&#65306;&#36890;&#36807;&#22810;&#20154;&#26684;&#33258;&#25105;&#21327;&#20316;&#23454;&#29616;&#20219;&#21153;&#35299;&#20915;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Unleashing Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration. (arXiv:2307.05300v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#21333;&#20154;&#34920;&#29616;&#25552;&#31034;&#65288;SPP&#65289;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#19982;&#22810;&#20010;&#35282;&#33394;&#36827;&#34892;&#22810;&#36718;&#33258;&#25105;&#21327;&#20316;&#65292;&#23558;&#21333;&#20010;&#35821;&#35328;&#27169;&#22411;&#36716;&#21270;&#20026;&#35748;&#30693;&#21327;&#21516;&#32773;&#65292;&#20174;&#32780;&#22686;&#24378;&#20854;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#21644;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26234;&#24935;&#20381;&#36182;&#20110;&#35748;&#30693;&#21327;&#21516;&#30340;&#27010;&#24565;&#65292;&#21363;&#22312;&#19981;&#21516;&#35748;&#30693;&#36807;&#31243;&#20043;&#38388;&#36827;&#34892;&#21327;&#20316;&#21644;&#20449;&#24687;&#25972;&#21512;&#65292;&#20197;&#33719;&#24471;&#27604;&#20010;&#20307;&#35748;&#30693;&#36807;&#31243;&#26356;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;&#36890;&#29992;&#20219;&#21153;&#35299;&#20915;&#20195;&#29702;&#34920;&#29616;&#20986;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#22312;&#38656;&#35201;&#20016;&#23500;&#39046;&#22495;&#30693;&#35782;&#21644;&#22797;&#26434;&#25512;&#29702;&#30340;&#20219;&#21153;&#19978;&#20173;&#28982;&#38754;&#20020;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21333;&#20154;&#34920;&#29616;&#25552;&#31034;&#65288;SPP&#65289;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#19982;&#22810;&#20010;&#35282;&#33394;&#36827;&#34892;&#22810;&#36718;&#33258;&#25105;&#21327;&#20316;&#65292;&#23558;&#21333;&#20010;LLM&#36716;&#21270;&#20026;&#35748;&#30693;&#21327;&#21516;&#32773;&#12290;&#35748;&#30693;&#21327;&#21516;&#32773;&#25351;&#30340;&#26159;&#19968;&#20010;&#26234;&#33021;&#20195;&#29702;&#65292;&#19982;&#22810;&#20010;&#26234;&#24935;&#21512;&#20316;&#65292;&#32467;&#21512;&#20182;&#20204;&#30340;&#20010;&#20307;&#20248;&#21183;&#21644;&#30693;&#35782;&#65292;&#20174;&#32780;&#22686;&#24378;&#22797;&#26434;&#20219;&#21153;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#21644;&#25972;&#20307;&#24615;&#33021;&#12290;&#36890;&#36807;&#26681;&#25454;&#20219;&#21153;&#36755;&#20837;&#21160;&#24577;&#35782;&#21035;&#21644;&#27169;&#25311;&#19981;&#21516;&#30340;&#35282;&#33394;&#65292;SPP&#37322;&#25918;&#20102;LLM&#20013;&#35748;&#30693;&#21327;&#21516;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human intelligence thrives on the concept of cognitive synergy, where collaboration and information integration among different cognitive processes yield superior outcomes compared to individual cognitive processes in isolation. Although Large Language Models (LLMs) have demonstrated promising performance as general task-solving agents, they still struggle with tasks that require intensive domain knowledge and complex reasoning. In this work, we propose Solo Performance Prompting (SPP), which transforms a single LLM into a cognitive synergist by engaging in multi-turn self-collaboration with multiple personas. A cognitive synergist refers to an intelligent agent that collaborates with multiple minds, combining their individual strengths and knowledge, to enhance problem-solving and overall performance in complex tasks. By dynamically identifying and simulating different personas based on task inputs, SPP unleashes the potential of cognitive synergy in LLMs. We have discovered that assi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#29992;&#20110;&#22810;&#27169;&#24577;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;MNER&#65289;&#30340;&#25913;&#36827;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21160;&#24577;&#23545;&#40784;&#22270;&#20687;&#21644;&#25991;&#26412;&#24207;&#21015;&#65292;&#24182;&#23454;&#29616;&#22810;&#32423;&#36328;&#27169;&#24577;&#23398;&#20064;&#26469;&#22686;&#24378;&#25991;&#26412;&#35789;&#34920;&#31034;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.08372</link><description>&lt;p&gt;
Tweet&#24086;&#23376;&#19978;&#30340;&#23618;&#27425;&#23545;&#40784;&#22810;&#27169;&#24577;&#23398;&#20064;&#29992;&#20110;NER
&lt;/p&gt;
&lt;p&gt;
Hierarchical Aligned Multimodal Learning for NER on Tweet Posts. (arXiv:2305.08372v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08372
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#29992;&#20110;&#22810;&#27169;&#24577;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;MNER&#65289;&#30340;&#25913;&#36827;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21160;&#24577;&#23545;&#40784;&#22270;&#20687;&#21644;&#25991;&#26412;&#24207;&#21015;&#65292;&#24182;&#23454;&#29616;&#22810;&#32423;&#36328;&#27169;&#24577;&#23398;&#20064;&#26469;&#22686;&#24378;&#25991;&#26412;&#35789;&#34920;&#31034;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#20174;&#25512;&#25991;&#20013;&#25366;&#25496;&#32467;&#26500;&#21270;&#30693;&#35782;&#21487;&#20197;&#23545;&#25512;&#33616;&#21644;&#24847;&#22270;&#29702;&#35299;&#31561;&#35768;&#22810;&#19979;&#28216;&#24212;&#29992;&#26377;&#30410;&#12290;&#30001;&#20110;&#25512;&#25991;&#20542;&#21521;&#20110;&#26159;&#22810;&#27169;&#24577;&#30340;&#65292;&#22810;&#27169;&#24577;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;MNER&#65289;&#24341;&#36215;&#20102;&#26356;&#22810;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21160;&#24577;&#22320;&#23545;&#40784;&#22270;&#20687;&#21644;&#25991;&#26412;&#24207;&#21015;&#65292;&#24182;&#23454;&#29616;&#22810;&#32423;&#36328;&#27169;&#24577;&#23398;&#20064;&#65292;&#20197;&#22686;&#24378;MNER&#30340;&#25991;&#26412;&#35789;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#20998;&#20026;&#19977;&#20010;&#20027;&#35201;&#38454;&#27573;&#65306;&#31532;&#19968;&#38454;&#27573;&#19987;&#27880;&#20110;&#20869;&#37096;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#65292;&#20197;&#25512;&#23548;&#20986;&#27599;&#20010;&#27169;&#24577;&#30340;&#38544;&#21547;&#20840;&#23616;&#21644;&#23616;&#37096;&#30693;&#35782;&#12290;&#31532;&#20108;&#38454;&#27573;&#35780;&#20272;&#25991;&#26412;&#19982;&#20854;&#20276;&#38543;&#22270;&#20687;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#26681;&#25454;&#30456;&#20851;&#24615;&#25972;&#21512;&#19981;&#21516;&#31890;&#24230;&#30340;&#35270;&#35273;&#20449;&#24687;&#12290;&#31532;&#19977;&#38454;&#27573;&#36890;&#36807;&#36845;&#20195;&#36328;&#27169;&#24577;&#20132;&#20114;&#21644;&#20849;&#21516;&#20851;&#27880;&#24378;&#21270;&#35821;&#20041;&#32454;&#21270;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mining structured knowledge from tweets using named entity recognition (NER) can be beneficial for many down stream applications such as recommendation and intention understanding. With tweet posts tending to be multimodal, multimodal named entity recognition (MNER) has attracted more attention. In this paper, we propose a novel approach, which can dynamically align the image and text sequence and achieve the multi-level cross-modal learning to augment textual word representation for MNER improvement. To be specific, our framework can be split into three main stages: the first stage focuses on intra-modality representation learning to derive the implicit global and local knowledge of each modality, the second evaluates the relevance between the text and its accompanying image and integrates different grained visual information based on the relevance, the third enforces semantic refinement via iterative cross-modal interactions and co-attention. We conduct experiments on two open datase
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20197;&#32842;&#22825;&#20026;&#26680;&#24515;&#30340;&#35270;&#39057;&#29702;&#35299;&#31995;&#32479;VideoChat&#65292;&#23427;&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#31070;&#32463;&#25509;&#21475;&#23558;&#35270;&#39057;&#22522;&#30784;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#25797;&#38271;&#20110;&#26102;&#31354;&#25512;&#29702;&#12289;&#20107;&#20214;&#23450;&#20301;&#21644;&#22240;&#26524;&#20851;&#31995;&#25512;&#26029;&#12290;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#35270;&#39057;&#20026;&#20013;&#24515;&#30340;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#35813;&#31995;&#32479;&#22312;&#24191;&#27867;&#30340;&#35270;&#39057;&#24212;&#29992;&#20013;&#20855;&#26377;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.06355</link><description>&lt;p&gt;
&#35270;&#39057;&#32842;&#22825;&#65306;&#20197;&#32842;&#22825;&#20026;&#26680;&#24515;&#30340;&#35270;&#39057;&#29702;&#35299;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
VideoChat: Chat-Centric Video Understanding. (arXiv:2305.06355v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20197;&#32842;&#22825;&#20026;&#26680;&#24515;&#30340;&#35270;&#39057;&#29702;&#35299;&#31995;&#32479;VideoChat&#65292;&#23427;&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#31070;&#32463;&#25509;&#21475;&#23558;&#35270;&#39057;&#22522;&#30784;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#25797;&#38271;&#20110;&#26102;&#31354;&#25512;&#29702;&#12289;&#20107;&#20214;&#23450;&#20301;&#21644;&#22240;&#26524;&#20851;&#31995;&#25512;&#26029;&#12290;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#35270;&#39057;&#20026;&#20013;&#24515;&#30340;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#35813;&#31995;&#32479;&#22312;&#24191;&#27867;&#30340;&#35270;&#39057;&#24212;&#29992;&#20013;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#35270;&#39057;&#32842;&#22825;&#65288;VideoChat&#65289;&#8212;&#8212;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#20197;&#32842;&#22825;&#20026;&#26680;&#24515;&#30340;&#35270;&#39057;&#29702;&#35299;&#31995;&#32479;&#65292;&#23427;&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#31070;&#32463;&#25509;&#21475;&#23558;&#35270;&#39057;&#22522;&#30784;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#25797;&#38271;&#20110;&#26102;&#31354;&#25512;&#29702;&#12289;&#20107;&#20214;&#23450;&#20301;&#21644;&#22240;&#26524;&#20851;&#31995;&#25512;&#26029;&#12290;&#20026;&#20102;&#25945;&#25480;&#35813;&#31995;&#32479;&#30340;&#20351;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35270;&#39057;&#20026;&#20013;&#24515;&#30340;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#25104;&#21315;&#19978;&#19975;&#20010;&#35270;&#39057;&#21644;&#35814;&#32454;&#30340;&#25551;&#36848;&#21644;&#23545;&#35805;&#65292;&#36825;&#20010;&#25968;&#25454;&#38598;&#24378;&#35843;&#26102;&#31354;&#25512;&#29702;&#21644;&#22240;&#26524;&#20851;&#31995;&#65292;&#20026;&#22521;&#35757;&#20197;&#32842;&#22825;&#20026;&#26680;&#24515;&#30340;&#35270;&#39057;&#29702;&#35299;&#31995;&#32479;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#36164;&#20135;&#12290;&#21021;&#27493;&#30340;&#23450;&#24615;&#23454;&#39564;&#25581;&#31034;&#20102;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#24191;&#27867;&#30340;&#35270;&#39057;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#35774;&#23450;&#20102;&#26631;&#20934;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#21487;&#20197;&#22312; https://github.com/OpenGVLab/Ask-Anything &#19978;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we initiate an exploration into video understanding by introducing VideoChat, an end-to-end chat-centric video understanding system. It integrates video foundation models and large language models via a learnable neural interface, excelling in spatiotemporal reasoning, event localization, and causal relationship inference. To instructively tune this system, we propose a video-centric instruction dataset, composed of thousands of videos matched with detailed descriptions and conversations. This dataset emphasizes spatiotemporal reasoning and causal relationships, providing a valuable asset for training chat-centric video understanding systems. Preliminary qualitative experiments reveal our system's potential across a broad spectrum of video applications and set the standard for future research. Access our code and data at https://github.com/OpenGVLab/Ask-Anything
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#36719;&#30828;&#26631;&#31614;&#39044;&#27979;&#20013;&#65292;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#23545;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36755;&#20986;&#30340;&#24433;&#21709;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#24358;&#28608;&#27963;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2303.02468</link><description>&lt;p&gt;
SemEval-2023&#20219;&#21153;11&#30340;Lon-ea&#65306;&#36719;&#30828;&#26631;&#31614;&#39044;&#27979;&#20013;&#28608;&#27963;&#20989;&#25968;&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lon-ea at SemEval-2023 Task 11: A Comparison of Activation Functions for Soft and Hard Label Prediction. (arXiv:2303.02468v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#36719;&#30828;&#26631;&#31614;&#39044;&#27979;&#20013;&#65292;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#23545;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36755;&#20986;&#30340;&#24433;&#21709;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#24358;&#28608;&#27963;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#22312;&#23398;&#20064;&#19981;&#21516;&#24847;&#20219;&#21153;&#30340;&#36719;&#30828;&#26631;&#31614;&#39044;&#27979;&#20013;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36755;&#20986;&#23618;&#20013;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#30340;&#24433;&#21709;&#12290;&#22312;&#35813;&#20219;&#21153;&#20013;&#65292;&#30446;&#26631;&#26159;&#36890;&#36807;&#39044;&#27979;&#36719;&#26631;&#31614;&#26469;&#37327;&#21270;&#19981;&#21516;&#24847;&#37327;&#12290;&#20026;&#20102;&#39044;&#27979;&#36719;&#26631;&#31614;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;BERT&#30340;&#39044;&#22788;&#29702;&#22120;&#21644;&#32534;&#30721;&#22120;&#65292;&#24182;&#25913;&#21464;&#36755;&#20986;&#23618;&#20013;&#20351;&#29992;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#20182;&#21442;&#25968;&#19981;&#21464;&#12290;&#28982;&#21518;&#23558;&#36719;&#26631;&#31614;&#29992;&#20110;&#30828;&#26631;&#31614;&#39044;&#27979;&#12290;&#32771;&#34385;&#30340;&#28608;&#27963;&#20989;&#25968;&#21253;&#25324;sigmoid&#20989;&#25968;&#20197;&#21450;&#28155;&#21152;&#21040;&#27169;&#22411;&#20013;&#30340;&#38454;&#36291;&#20989;&#25968;&#21644;&#26412;&#25991;&#20013;&#39318;&#27425;&#20171;&#32461;&#30340;&#27491;&#24358;&#28608;&#27963;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the influence of different activation functions in the output layer of deep neural network models for soft and hard label prediction in the learning with disagreement task. In this task, the goal is to quantify the amount of disagreement via predicting soft labels. To predict the soft labels, we use BERT-based preprocessors and encoders and vary the activation function used in the output layer, while keeping other parameters constant. The soft labels are then used for the hard label prediction. The activation functions considered are sigmoid as well as a step-function that is added to the model post-training and a sinusoidal activation function, which is introduced for the first time in this paper.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#24847;&#22823;&#21033;&#32447;&#19978;&#25253;&#32440;&#22312;&#26032;&#20896;&#30123;&#24773;&#21021;&#26399;&#20351;&#29992;&#30340;&#27604;&#21947;&#35821;&#35328;&#36827;&#34892;&#23450;&#37327;&#21644;&#23450;&#24615;&#20998;&#26512;&#65292;&#21457;&#29616;&#25919;&#24220;&#23545;&#30123;&#24773;&#24212;&#23545;&#30340;&#19981;&#21516;&#38454;&#27573;&#23384;&#22312;&#26174;&#33879;&#21464;&#21270;&#65292;&#24182;&#19988;&#19981;&#21516;&#20027;&#39064;&#20043;&#38388;&#23384;&#22312;&#26377;&#36259;&#30340;&#38544;&#21947;&#37325;&#21472;&#12290;</title><link>http://arxiv.org/abs/2204.02106</link><description>&lt;p&gt;
&#23186;&#20307;&#22914;&#20309;&#35848;&#35770;&#26032;&#20896;&#30123;&#24773;&#65311;&#24847;&#22823;&#21033;&#32447;&#19978;&#25253;&#32440;&#20013;&#30340;&#38544;&#21947;&#20027;&#39064;&#32858;&#31867;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
How do media talk about the Covid-19 pandemic? Metaphorical thematic clustering in Italian online newspapers. (arXiv:2204.02106v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.02106
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#24847;&#22823;&#21033;&#32447;&#19978;&#25253;&#32440;&#22312;&#26032;&#20896;&#30123;&#24773;&#21021;&#26399;&#20351;&#29992;&#30340;&#27604;&#21947;&#35821;&#35328;&#36827;&#34892;&#23450;&#37327;&#21644;&#23450;&#24615;&#20998;&#26512;&#65292;&#21457;&#29616;&#25919;&#24220;&#23545;&#30123;&#24773;&#24212;&#23545;&#30340;&#19981;&#21516;&#38454;&#27573;&#23384;&#22312;&#26174;&#33879;&#21464;&#21270;&#65292;&#24182;&#19988;&#19981;&#21516;&#20027;&#39064;&#20043;&#38388;&#23384;&#22312;&#26377;&#36259;&#30340;&#38544;&#21947;&#37325;&#21472;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#24847;&#22823;&#21033;&#32447;&#19978;&#25253;&#32440;&#22312;&#26032;&#20896;&#30123;&#24773;&#29190;&#21457;&#30340;&#22836;&#20960;&#20010;&#26376;&#20013;&#20351;&#29992;&#30340;&#27604;&#21947;&#35821;&#35328;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23545;2020&#24180;&#26149;&#23395;&#25919;&#24220;&#23545;&#30123;&#24773;&#24212;&#23545;&#30340;&#31532;&#19968;&#38454;&#27573;&#21644;&#31532;&#20108;&#38454;&#27573;&#30340;&#20027;&#39064;&#21644;&#38544;&#21947;&#35821;&#35328;&#36827;&#34892;&#20102;&#23545;&#27604;&#20998;&#26512;&#12290;&#30740;&#31350;&#22522;&#20110;2020&#24180;2&#26376;24&#26085;&#33267;6&#26376;3&#26085;&#26399;&#38388;&#37319;&#38598;&#30340;&#26032;&#38395;&#35821;&#26009;&#36827;&#34892;&#65292;&#24182;&#32467;&#21512;&#20102;&#23450;&#37327;&#21644;&#23450;&#24615;&#26041;&#27861;&#65292;&#21253;&#25324;&#32467;&#26500;&#20027;&#39064;&#24314;&#27169;&#12289;&#27010;&#24565;&#38544;&#21947;&#29702;&#35770;&#21644;&#22522;&#20110;&#35821;&#26009;&#24211;&#30340;&#38544;&#21947;&#20998;&#26512;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#31532;&#19968;&#38454;&#27573;&#21644;&#31532;&#20108;&#38454;&#27573;&#35752;&#35770;&#30340;&#20027;&#39064;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#21464;&#21270;&#65292;&#24182;&#19988;&#20027;&#39064;&#20043;&#38388;&#23384;&#22312;&#26377;&#36259;&#30340;&#38544;&#21947;&#37325;&#21472;&#12290;&#36890;&#36807;&#23450;&#24615;&#35821;&#26009;&#24211;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32463;&#27982;&#21644;&#31038;&#20250;&#20027;&#39064;&#30340;&#38544;&#21947;&#25645;&#37197;&#30340;&#26356;&#28145;&#20837;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
The contribution presents a study on figurative language of the first months of the COVID-19 crisis in Italian online newspapers. Particularly, we contrast topics and metaphorical language used by journalists in the first and second phase of the government response to the pandemic in Spring 2020. The analysis is conducted on a journalistic corpus collected between February 24th and June 3rd, 2020. The analysis is performed using both quantitative and qualitative approaches, combining Structural Topic Modelling (Roberts et al. 2016), Conceptual Metaphor Theory (Lakoff &amp; Johnson, 1980), and qualitative-corpus based metaphor analysis (Charteris-Black, 2004). We find a significant shift in topics discussed across Phase 1 and Phase 2, and interesting overlaps in topic-specific metaphors. Using qualitative corpus analysis, we present a more in-depth case study discussing metaphorical collocations of the topics of Economy and Society
&lt;/p&gt;</description></item></channel></rss>