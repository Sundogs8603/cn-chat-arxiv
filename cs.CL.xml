<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#29992;&#25143;&#25512;&#29702;&#25915;&#20987;&#65292;&#21457;&#29616;LLMs&#23545;&#20110;&#21508;&#31181;&#24494;&#35843;&#25968;&#25454;&#38598;&#37117;&#24456;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#31163;&#32676;&#29992;&#25143;&#21644;&#36129;&#29486;&#22823;&#37327;&#25968;&#25454;&#30340;&#29992;&#25143;&#12290;&#36825;&#23545;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.09266</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#29992;&#25143;&#25512;&#29702;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
User Inference Attacks on Large Language Models. (arXiv:2310.09266v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09266
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#29992;&#25143;&#25512;&#29702;&#25915;&#20987;&#65292;&#21457;&#29616;LLMs&#23545;&#20110;&#21508;&#31181;&#24494;&#35843;&#25968;&#25454;&#38598;&#37117;&#24456;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#31163;&#32676;&#29992;&#25143;&#21644;&#36129;&#29486;&#22823;&#37327;&#25968;&#25454;&#30340;&#29992;&#25143;&#12290;&#36825;&#23545;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#26159;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23450;&#21046;&#20026;&#19987;&#19994;&#20219;&#21153;&#21644;&#24212;&#29992;&#30340;&#24120;&#35265;&#26377;&#25928;&#26041;&#27861;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#29992;&#25143;&#25968;&#25454;&#19978;&#24494;&#35843;LLMs&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#31216;&#20026;&#29992;&#25143;&#25512;&#29702;&#30340;&#29616;&#23454;&#23041;&#32961;&#27169;&#22411;&#65292;&#20854;&#20013;&#25915;&#20987;&#32773;&#25512;&#26029;&#20986;&#29992;&#25143;&#30340;&#25968;&#25454;&#26159;&#21542;&#34987;&#29992;&#20110;&#24494;&#35843;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#36825;&#31181;&#23041;&#32961;&#27169;&#22411;&#30340;&#25915;&#20987;&#65292;&#21482;&#38656;&#35201;&#20174;&#29992;&#25143;&#37027;&#37324;&#33719;&#21462;&#19968;&#23567;&#32452;&#26679;&#26412;&#65288;&#21487;&#33021;&#19982;&#29992;&#20110;&#35757;&#32451;&#30340;&#26679;&#26412;&#19981;&#21516;&#65289;&#21644;&#23545;&#24494;&#35843;LLM&#30340;&#40657;&#30418;&#35775;&#38382;&#26435;&#38480;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;LLMs&#22312;&#21508;&#31181;&#24494;&#35843;&#25968;&#25454;&#38598;&#19978;&#26131;&#21463;&#29992;&#25143;&#25512;&#29702;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#26377;&#26102;&#25915;&#20987;&#25104;&#21151;&#29575;&#25509;&#36817;&#23436;&#32654;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#21738;&#20123;&#29305;&#24615;&#20351;&#29992;&#25143;&#23481;&#26131;&#21463;&#21040;&#29992;&#25143;&#25512;&#29702;&#30340;&#25915;&#20987;&#65292;&#21457;&#29616;&#31163;&#32676;&#29992;&#25143;&#65288;&#21363;&#25968;&#25454;&#20998;&#24067;&#19982;&#20854;&#20182;&#29992;&#25143;&#26126;&#26174;&#19981;&#21516;&#65289;&#21644;&#36129;&#29486;&#22823;&#37327;&#25968;&#25454;&#30340;&#29992;&#25143;&#26356;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#35299;&#20915;&#36825;&#31181;&#25915;&#20987;&#30340;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning is a common and effective method for tailoring large language models (LLMs) to specialized tasks and applications. In this paper, we study the privacy implications of fine-tuning LLMs on user data. To this end, we define a realistic threat model, called user inference, wherein an attacker infers whether or not a user's data was used for fine-tuning. We implement attacks for this threat model that require only a small set of samples from a user (possibly different from the samples used for training) and black-box access to the fine-tuned LLM. We find that LLMs are susceptible to user inference attacks across a variety of fine-tuning datasets, at times with near perfect attack success rates. Further, we investigate which properties make users vulnerable to user inference, finding that outlier users (i.e. those with data distributions sufficiently different from other users) and users who contribute large quantities of data are most susceptible to attack. Finally, we explore s
&lt;/p&gt;</description></item><item><title>PromptRE&#26159;&#19968;&#31181;&#24369;&#30417;&#30563;&#25991;&#26723;&#32423;&#21035;&#20851;&#31995;&#25277;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#25552;&#31034;&#30340;&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;&#8220;&#27809;&#26377;&#20851;&#31995;&#8221;&#30340;&#23454;&#20363;&#25968;&#37327;&#19981;&#24179;&#34913;&#21644;&#30452;&#25509;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#25991;&#26723;&#20851;&#31995;&#25277;&#21462;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.09265</link><description>&lt;p&gt;
PromptRE: &#22522;&#20110;&#25552;&#31034;&#30340;&#25968;&#25454;&#32534;&#31243;&#30340;&#24369;&#30417;&#30563;&#25991;&#26412;&#32423;&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
PromptRE: Weakly-Supervised Document-Level Relation Extraction via Prompting-Based Data Programming. (arXiv:2310.09265v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09265
&lt;/p&gt;
&lt;p&gt;
PromptRE&#26159;&#19968;&#31181;&#24369;&#30417;&#30563;&#25991;&#26723;&#32423;&#21035;&#20851;&#31995;&#25277;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#25552;&#31034;&#30340;&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;&#8220;&#27809;&#26377;&#20851;&#31995;&#8221;&#30340;&#23454;&#20363;&#25968;&#37327;&#19981;&#24179;&#34913;&#21644;&#30452;&#25509;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#25991;&#26723;&#20851;&#31995;&#25277;&#21462;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25277;&#21462;&#26088;&#22312;&#23558;&#20004;&#20010;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#20998;&#31867;&#21040;&#39044;&#23450;&#20041;&#30340;&#31867;&#21035;&#20013;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#21477;&#32423;&#21035;&#30340;&#20851;&#31995;&#25277;&#21462;&#19978;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#23558;&#33539;&#22260;&#25193;&#22823;&#21040;&#25991;&#26723;&#32423;&#21035;&#30340;&#20851;&#31995;&#25277;&#21462;&#12290;&#20256;&#32479;&#30340;&#20851;&#31995;&#25277;&#21462;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#20154;&#24037;&#26631;&#27880;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#26159;&#19968;&#39033;&#32791;&#26102;&#19988;&#21171;&#21160;&#23494;&#38598;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#20943;&#23569;&#23545;&#25163;&#21160;&#26631;&#27880;&#30340;&#38656;&#27714;&#65292;&#26368;&#36817;&#24050;&#32463;&#24320;&#21457;&#20102;&#19968;&#20123;&#24369;&#30417;&#30563;&#26041;&#27861;&#29992;&#20110;&#21477;&#32423;&#21035;&#30340;&#20851;&#31995;&#25277;&#21462;&#65292;&#20294;&#22312;&#25991;&#26723;&#32423;&#21035;&#30340;&#20851;&#31995;&#25277;&#21462;&#26041;&#38754;&#20173;&#28982;&#26377;&#38480;&#30340;&#24037;&#20316;&#12290;&#24369;&#30417;&#30563;&#30340;&#25991;&#26723;&#32423;&#21035;&#20851;&#31995;&#25277;&#21462;&#38754;&#20020;&#30528;&#19968;&#20123;&#25361;&#25112;&#65292;&#27604;&#22914;&#8220;&#27809;&#26377;&#20851;&#31995;&#8221;&#30340;&#23454;&#20363;&#25968;&#37327;&#19981;&#24179;&#34913;&#65292;&#20197;&#21450;&#30452;&#25509;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26723;&#20851;&#31995;&#25277;&#21462;&#30340;&#22833;&#36133;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PromptRE&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#24369;&#30417;&#30563;&#25991;&#26723;&#32423;&#21035;&#20851;&#31995;&#25277;&#21462;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#22522;&#20110;&#25552;&#31034;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation extraction aims to classify the relationships between two entities into pre-defined categories. While previous research has mainly focused on sentence-level relation extraction, recent studies have expanded the scope to document-level relation extraction. Traditional relation extraction methods heavily rely on human-annotated training data, which is time-consuming and labor-intensive. To mitigate the need for manual annotation, recent weakly-supervised approaches have been developed for sentence-level relation extraction while limited work has been done on document-level relation extraction. Weakly-supervised document-level relation extraction faces significant challenges due to an imbalanced number "no relation" instances and the failure of directly probing pretrained large language models for document relation extraction. To address these challenges, we propose PromptRE, a novel weakly-supervised document-level relation extraction method that combines prompting-based techniq
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;"&#34920;&#26684;&#35843;&#20248;"&#33539;&#24335;&#65292;&#36890;&#36807;&#20351;&#29992;&#20174;&#30495;&#23454;&#34920;&#26684;&#20013;&#21512;&#25104;&#30340;&#22810;&#26679;&#21270;&#34920;&#26684;&#20219;&#21153;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#65292;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;/&#24494;&#35843;&#65292;&#20197;&#25552;&#39640;&#20854;&#29702;&#35299;&#34920;&#26684;&#21644;&#25191;&#34892;&#34920;&#26684;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.09263</link><description>&lt;p&gt;
Table-GPT: &#38024;&#23545;&#22810;&#26679;&#34920;&#26684;&#20219;&#21153;&#30340;&#34920;&#26684;&#35843;&#20248;GPT
&lt;/p&gt;
&lt;p&gt;
Table-GPT: Table-tuned GPT for Diverse Table Tasks. (arXiv:2310.09263v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09263
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;"&#34920;&#26684;&#35843;&#20248;"&#33539;&#24335;&#65292;&#36890;&#36807;&#20351;&#29992;&#20174;&#30495;&#23454;&#34920;&#26684;&#20013;&#21512;&#25104;&#30340;&#22810;&#26679;&#21270;&#34920;&#26684;&#20219;&#21153;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#65292;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;/&#24494;&#35843;&#65292;&#20197;&#25552;&#39640;&#20854;&#29702;&#35299;&#34920;&#26684;&#21644;&#25191;&#34892;&#34920;&#26684;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;GPT-3.5&#21644;ChatGPT&#65292;&#23637;&#29616;&#20986;&#20102;&#36981;&#24490;&#22810;&#31181;&#20154;&#31867;&#25351;&#20196;&#21644;&#25191;&#34892;&#21508;&#31181;&#20219;&#21153;&#30340;&#38750;&#20961;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#20351;&#29992;&#19968;&#31995;&#21015;&#22522;&#26412;&#30340;&#34920;&#26684;&#29702;&#35299;&#20219;&#21153;&#26469;&#25506;&#31350;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#29616;&#20170;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#35768;&#22810;&#28041;&#21450;&#34920;&#26684;&#30340;&#20219;&#21153;&#19978;&#20173;&#28982;&#19981;&#22815;&#20248;&#31168;&#65292;&#21487;&#33021;&#26159;&#22240;&#20026;&#23427;&#20204;&#20027;&#35201;&#22312;\emph{&#19968;&#32500;}&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#32780;&#20851;&#31995;&#34920;&#26159;\emph{&#20108;&#32500;}&#23545;&#35937;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#8220;\emph{&#34920;&#26684;&#35843;&#20248;}&#8221;&#33539;&#24335;&#65292;&#21363;&#36890;&#36807;&#20351;&#29992;&#20174;&#30495;&#23454;&#34920;&#26684;&#20013;&#21512;&#25104;&#30340;&#22810;&#26679;&#21270;&#34920;&#26684;&#20219;&#21153;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#65292;&#32487;&#32493;&#23545;GPT-3.5&#21644;ChatGPT&#36825;&#26679;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;/&#24494;&#35843;&#65292;&#20174;&#32780;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#34920;&#26684;&#21644;&#25191;&#34892;&#34920;&#26684;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;Table-GPT&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;\emph{&#34920;&#26684;&#29702;&#35299;}&#33021;&#21147;&#65292;&#22312;&#24191;&#27867;&#30340;&#34920;&#26684;&#20219;&#21153;&#19978;&#22987;&#32456;&#20248;&#20110;&#26222;&#36890;&#30340;GPT-3.5&#21644;ChatGPT&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models, such as GPT-3.5 and ChatGPT, demonstrate remarkable abilities to follow diverse human instructions and perform a wide range of tasks. However, when probing language models using a range of basic table-understanding tasks, we observe that today's language models are still sub-optimal in many table-related tasks, likely because they are pre-trained predominantly on \emph{one-dimensional} natural-language texts, whereas relational tables are \emph{two-dimensional} objects.  In this work, we propose a new "\emph{table-tuning}" paradigm, where we continue to train/fine-tune language models like GPT-3.5 and ChatGPT, using diverse table-tasks synthesized from real tables as training data, with the goal of enhancing language models' ability to understand tables and perform table tasks. We show that our resulting Table-GPT models demonstrate (1) better \emph{table-understanding} capabilities, by consistently outperforming the vanilla GPT-3.5 and ChatGPT, on a wide-range of tabl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#22810;&#35821;&#31181;&#29615;&#22659;&#19979;&#36827;&#34892;&#20102;&#39318;&#27425;&#23454;&#39564;&#65292;&#25506;&#35752;&#20102;&#25919;&#27835;&#20027;&#24352;&#30340;&#35782;&#21035;&#21644;&#20998;&#31867;&#12290;&#23454;&#39564;&#20351;&#29992;&#20102;&#24503;&#35821;&#25968;&#25454;&#38598;DebateNet2.0&#65292;&#28085;&#30422;&#20102;2015&#24180;&#38590;&#27665;&#21361;&#26426;&#24341;&#21457;&#30340;&#25919;&#31574;&#36777;&#35770;&#12290;&#36890;&#36807;&#26426;&#22120;&#32763;&#35793;&#21644;&#22810;&#35821;&#31181;&#23884;&#20837;&#20004;&#31181;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2310.09256</link><description>&lt;p&gt;
&#22810;&#35821;&#31181;&#29615;&#22659;&#19979;&#30340;&#25919;&#27835;&#20027;&#24352;&#35782;&#21035;&#21644;&#20998;&#31867;&#65306;&#39318;&#27425;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Political claim identification and categorization in a multilingual setting: First experiments. (arXiv:2310.09256v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#22810;&#35821;&#31181;&#29615;&#22659;&#19979;&#36827;&#34892;&#20102;&#39318;&#27425;&#23454;&#39564;&#65292;&#25506;&#35752;&#20102;&#25919;&#27835;&#20027;&#24352;&#30340;&#35782;&#21035;&#21644;&#20998;&#31867;&#12290;&#23454;&#39564;&#20351;&#29992;&#20102;&#24503;&#35821;&#25968;&#25454;&#38598;DebateNet2.0&#65292;&#28085;&#30422;&#20102;2015&#24180;&#38590;&#27665;&#21361;&#26426;&#24341;&#21457;&#30340;&#25919;&#31574;&#36777;&#35770;&#12290;&#36890;&#36807;&#26426;&#22120;&#32763;&#35793;&#21644;&#22810;&#35821;&#31181;&#23884;&#20837;&#20004;&#31181;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25919;&#27835;&#20027;&#24352;&#30340;&#35782;&#21035;&#21644;&#20998;&#31867;&#26159;&#20998;&#26512;&#25919;&#27835;&#25253;&#21578;&#30340;&#37325;&#35201;&#27493;&#39588;&#65292;&#28982;&#32780;&#29992;&#20110;&#27492;&#20219;&#21153;&#30340;&#36164;&#28304;&#38750;&#24120;&#26377;&#38480;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19981;&#21516;&#30340;&#31574;&#30053;&#65292;&#29992;&#20110;&#36328;&#35821;&#35328;&#25237;&#23556;&#25919;&#27835;&#20027;&#24352;&#20998;&#26512;&#12290;&#25105;&#20204;&#22312;&#24503;&#22269;&#30340;&#25968;&#25454;&#38598;DebateNet2.0&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#35813;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;2015&#24180;&#38590;&#27665;&#21361;&#26426;&#24341;&#21457;&#30340;&#25919;&#31574;&#36777;&#35770;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#28041;&#21450;&#20004;&#20010;&#20219;&#21153;&#65288;&#20027;&#24352;&#35782;&#21035;&#21644;&#20998;&#31867;&#65289;&#12289;&#19977;&#31181;&#35821;&#35328;&#65288;&#24503;&#35821;&#12289;&#33521;&#35821;&#21644;&#27861;&#35821;&#65289;&#21644;&#20004;&#31181;&#26041;&#27861;&#65288;&#26426;&#22120;&#32763;&#35793; - &#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#34920;&#29616;&#26368;&#22909;&#30340;&#26041;&#27861; - &#21644;&#22810;&#35821;&#31181;&#23884;&#20837;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The identification and classification of political claims is an important step in the analysis of political newspaper reports; however, resources for this task are few and far between. This paper explores different strategies for the cross-lingual projection of political claims analysis. We conduct experiments on a German dataset, DebateNet2.0, covering the policy debate sparked by the 2015 refugee crisis. Our evaluation involves two tasks (claim identification and categorization), three languages (German, English, and French) and two methods (machine translation -- the best method in our experiments -- and multilingual embeddings).
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22522;&#20110;WordNet&#23618;&#27425;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#27969;&#34892;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#23545;&#20110;&#19978;&#20041;&#35789;&#20851;&#31995;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#65292;&#33021;&#22815;&#23450;&#37327;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#30340;&#35821;&#35328;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#20123;&#22256;&#38590;&#30340;&#35789;&#27719;&#12290;&#25105;&#20204;&#20840;&#38754;&#35780;&#20272;&#20102;&#19968;&#20123;&#27969;&#34892;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#21253;&#25324;GLIDE&#12289;Latent Diffusion&#21644;Stable Diffusion&#12290;</title><link>http://arxiv.org/abs/2310.09247</link><description>&lt;p&gt;
&#36890;&#36807;WordNet&#23618;&#27425;&#32467;&#26500;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#36827;&#34892;&#19978;&#20041;&#35789;&#29702;&#35299;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Hypernymy Understanding Evaluation of Text-to-Image Models via WordNet Hierarchy. (arXiv:2310.09247v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09247
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;WordNet&#23618;&#27425;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#27969;&#34892;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#23545;&#20110;&#19978;&#20041;&#35789;&#20851;&#31995;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#65292;&#33021;&#22815;&#23450;&#37327;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#30340;&#35821;&#35328;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#20123;&#22256;&#38590;&#30340;&#35789;&#27719;&#12290;&#25105;&#20204;&#20840;&#38754;&#35780;&#20272;&#20102;&#19968;&#20123;&#27969;&#34892;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#21253;&#25324;GLIDE&#12289;Latent Diffusion&#21644;Stable Diffusion&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#36136;&#37327;&#19981;&#26029;&#25552;&#39640;&#21644;&#20247;&#22810;&#23454;&#38469;&#24212;&#29992;&#65292;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#20173;&#28982;&#30693;&#20043;&#29978;&#23569;&#65292;&#36825;&#20351;&#24471;&#38590;&#20197;&#25512;&#29702;&#20986;&#32473;&#23450;&#27169;&#22411;&#33021;&#22815;&#29702;&#35299;&#30340;&#25552;&#31034;&#34920;&#36798;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#34913;&#37327;&#20102;&#27969;&#34892;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#23545;&#20110;&#19978;&#20041;&#35789;&#65288;&#25110;&#8220;&#26159;&#19968;&#20010;&#8221;&#20851;&#31995;&#65289;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#20010;&#22522;&#20110;WordNet&#35821;&#20041;&#23618;&#27425;&#21644;&#22312;ImageNet&#19978;&#39044;&#35757;&#32451;&#30340;&#29616;&#26377;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#12290;&#36825;&#20004;&#20010;&#24230;&#37327;&#26631;&#20934;&#22343;&#33021;&#22815;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#35821;&#35328;&#33021;&#21147;&#36827;&#34892;&#24191;&#27867;&#30340;&#23450;&#37327;&#27604;&#36739;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#25214;&#21040;&#32454;&#31890;&#24230;&#23450;&#24615;&#24046;&#24322;&#30340;&#26041;&#27861;&#65292;&#20363;&#22914;&#23545;&#20110;&#27169;&#22411;&#26469;&#35828;&#26410;&#30693;&#30340;&#21333;&#35789;&#65292;&#22240;&#27492;&#24456;&#38590;&#32472;&#21046;&#12290;&#25105;&#20204;&#20840;&#38754;&#35780;&#20272;&#20102;&#27969;&#34892;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#21253;&#25324;GLIDE&#12289;Latent Diffusion&#21644;Stable Diffusion&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image synthesis has recently attracted widespread attention due to rapidly improving quality and numerous practical applications. However, the language understanding capabilities of text-to-image models are still poorly understood, which makes it difficult to reason about prompt formulations that a given model would understand well. In this work, we measure the capability of popular text-to-image models to understand $\textit{hypernymy}$, or the "is-a" relation between words. We design two automatic metrics based on the WordNet semantic hierarchy and existing image classifiers pretrained on ImageNet. These metrics both enable broad quantitative comparison of linguistic capabilities for text-to-image models and offer a way of finding fine-grained qualitative differences, such as words that are unknown to models and thus are difficult for them to draw. We comprehensively evaluate popular text-to-image models, including GLIDE, Latent Diffusion, and Stable Diffusion, showing how ou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20808;&#20363;&#22686;&#24378;&#30340;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#26694;&#26550;&#65288;PLJP&#65289;&#65292;&#36890;&#36807;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#39046;&#22495;&#27169;&#22411;&#65292;&#21033;&#29992;&#20808;&#20363;&#30456;&#20851;&#20449;&#24687;&#36827;&#34892;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.09241</link><description>&lt;p&gt;
&#20351;&#29992;LLM&#21644;&#39046;&#22495;&#27169;&#22411;&#21327;&#20316;&#30340;&#20808;&#20363;&#22686;&#24378;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Precedent-Enhanced Legal Judgment Prediction with LLM and Domain-Model Collaboration. (arXiv:2310.09241v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20808;&#20363;&#22686;&#24378;&#30340;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#26694;&#26550;&#65288;PLJP&#65289;&#65292;&#36890;&#36807;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#39046;&#22495;&#27169;&#22411;&#65292;&#21033;&#29992;&#20808;&#20363;&#30456;&#20851;&#20449;&#24687;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#65288;LJP&#65289;&#24050;&#25104;&#20026;&#27861;&#24459;&#20154;&#24037;&#26234;&#33021;&#20013;&#19968;&#20010;&#36234;&#26469;&#36234;&#20851;&#38190;&#30340;&#20219;&#21153;&#65292;&#21363;&#26681;&#25454;&#26696;&#20214;&#20107;&#23454;&#25551;&#36848;&#39044;&#27979;&#26696;&#20214;&#30340;&#21028;&#20915;&#12290;&#20808;&#20363;&#26159;&#20855;&#26377;&#30456;&#20284;&#20107;&#23454;&#30340;&#20808;&#21069;&#27861;&#24459;&#26696;&#20363;&#65292;&#23427;&#20204;&#26159;&#22269;&#23478;&#27861;&#24459;&#20307;&#31995;&#20013;&#21518;&#32493;&#26696;&#20214;&#21028;&#20915;&#30340;&#20381;&#25454;&#12290;&#22240;&#27492;&#65292;&#20540;&#24471;&#25506;&#32034;&#22312;LJP&#20013;&#21033;&#29992;&#20808;&#20363;&#30340;&#26041;&#27861;&#12290;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#20351;&#24471;&#21487;&#20197;&#20351;&#29992;&#21508;&#31181;&#25216;&#26415;&#26469;&#35299;&#20915;LJP&#20219;&#21153;&#12290;&#36825;&#20123;&#25216;&#26415;&#21487;&#20197;&#20998;&#20026;&#20004;&#31867;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#39046;&#22495;&#29305;&#23450;&#27169;&#22411;&#12290;LLM&#33021;&#22815;&#35299;&#37322;&#21644;&#29983;&#25104;&#22797;&#26434;&#30340;&#33258;&#28982;&#35821;&#35328;&#65292;&#32780;&#39046;&#22495;&#27169;&#22411;&#22312;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#20449;&#24687;&#26041;&#38754;&#25928;&#26524;&#26174;&#33879;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20808;&#20363;&#22686;&#24378;&#30340;LJP&#26694;&#26550;&#65288;PLJP&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#20808;&#20363;&#32972;&#26223;&#19979;&#21033;&#29992;LLM&#21644;&#39046;&#22495;&#27169;&#22411;&#20248;&#21183;&#30340;&#31995;&#32479;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#39046;&#22495;&#27169;&#22411;&#30340;&#35774;&#35745;&#30446;&#26631;&#26159;&#25552;&#20379;&#20505;&#36873;&#26631;&#31614;&#24182;&#25214;&#21040;&#36866;&#24403;&#30340;&#39044;&#27979;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
Legal Judgment Prediction (LJP) has become an increasingly crucial task in Legal AI, i.e., predicting the judgment of the case in terms of case fact description. Precedents are the previous legal cases with similar facts, which are the basis for the judgment of the subsequent case in national legal systems. Thus, it is worthwhile to explore the utilization of precedents in the LJP. Recent advances in deep learning have enabled a variety of techniques to be used to solve the LJP task. These can be broken down into two categories: large language models (LLMs) and domain-specific models. LLMs are capable of interpreting and generating complex natural language, while domain models are efficient in learning task-specific information. In this paper, we propose the precedent-enhanced LJP framework (PLJP), a system that leverages the strength of both LLM and domain models in the context of precedents. Specifically, the domain models are designed to provide candidate labels and find the proper 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;Bangla&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#30340;&#20219;&#21153;&#65292;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#22312;&#36825;&#31181;&#20302;&#36164;&#28304;&#35821;&#35328;&#22330;&#26223;&#19979;&#65292;&#20351;&#29992;Transformer&#27169;&#22411;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#23398;&#20064;&#25928;&#26524;&#65292;&#24182;&#19988;&#22312;&#23545;&#24050;&#32463;&#22312;Twitter&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#30340;&#27169;&#22411;&#36827;&#19968;&#27493;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#30340;&#24615;&#33021;&#26368;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.09238</link><description>&lt;p&gt;
BanglaNLP&#22312;BLP-2023&#20219;&#21153;2&#20013;&#30340;&#34920;&#29616;: &#23545;Bangla&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#30340;&#24773;&#24863;&#20998;&#26512;&#30340;Transformer&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
BanglaNLP at BLP-2023 Task 2: Benchmarking different Transformer Models for Sentiment Analysis of Bangla Social Media Posts. (arXiv:2310.09238v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;Bangla&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#30340;&#20219;&#21153;&#65292;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#22312;&#36825;&#31181;&#20302;&#36164;&#28304;&#35821;&#35328;&#22330;&#26223;&#19979;&#65292;&#20351;&#29992;Transformer&#27169;&#22411;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#23398;&#20064;&#25928;&#26524;&#65292;&#24182;&#19988;&#22312;&#23545;&#24050;&#32463;&#22312;Twitter&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#30340;&#27169;&#22411;&#36827;&#19968;&#27493;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#30340;&#24615;&#33021;&#26368;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Bangla&#26159;&#20840;&#29699;&#31532;&#19971;&#22823;&#20351;&#29992;&#26368;&#24191;&#27867;&#30340;&#35821;&#35328;&#65292;&#25317;&#26377;&#26469;&#33258;&#21360;&#24230;&#21644;&#23391;&#21152;&#25289;&#22269;&#30340;2.34&#20159;&#27597;&#35821;&#20351;&#29992;&#32773;&#12290;&#36825;&#31181;&#24418;&#24577;&#20016;&#23500;&#30340;&#35821;&#35328;&#25317;&#26377;&#20016;&#23500;&#30340;&#25991;&#23398;&#20256;&#32479;&#65292;&#21253;&#25324;&#19981;&#21516;&#30340;&#26041;&#35328;&#21644;&#35821;&#35328;&#29305;&#23450;&#30340;&#25361;&#25112;&#12290;&#23613;&#31649;&#20854;&#35821;&#35328;&#20016;&#23500;&#24615;&#21644;&#21382;&#21490;&#65292;&#20294;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#35821;&#38899;&#31038;&#21306;&#20013;&#65292;Bangla&#20173;&#34987;&#24402;&#31867;&#20026;&#36164;&#28304;&#21294;&#20047;&#30340;&#35821;&#35328;&#12290;&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#25105;&#20204;&#22312;BLP&#30740;&#35752;&#20250;&#30340;&#20219;&#21153;2&#65288;Bangla&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#24773;&#24863;&#20998;&#26512;&#65289;&#20013;&#30340;&#25552;&#20132;&#12290;&#25105;&#20204;&#23581;&#35797;&#20102;&#19981;&#21516;&#30340;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#26469;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#23450;&#37327;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#36825;&#31181;&#20302;&#36164;&#28304;&#35821;&#35328;&#22330;&#26223;&#20013;&#65292;&#36801;&#31227;&#23398;&#20064;&#30830;&#23454;&#26377;&#21161;&#20110;&#27169;&#22411;&#26356;&#22909;&#22320;&#23398;&#20064;&#12290;&#24403;&#25105;&#20204;&#36827;&#19968;&#27493;&#23545;&#24050;&#32463;&#22312;Twitter&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#30340;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#26102;&#65292;&#36825;&#19968;&#28857;&#21464;&#24471;&#26126;&#26174;&#65292;&#32780;&#36825;&#20010;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#22312;&#25152;&#26377;&#20854;&#20182;&#27169;&#22411;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bangla is the 7th most widely spoken language globally, with a staggering 234 million native speakers primarily hailing from India and Bangladesh. This morphologically rich language boasts a rich literary tradition, encompassing diverse dialects and language-specific challenges. Despite its linguistic richness and history, Bangla remains categorized as a low-resource language within the natural language processing (NLP) and speech community. This paper presents our submission to Task 2 (Sentiment Analysis of Bangla Social Media Posts) of the BLP Workshop. We experiment with various Transformer-based architectures to solve this task. Our quantitative results show that transfer learning really helps in better learning of the models in this low-resource language scenario. This becomes evident when we further finetune a model which has already been finetuned on twitter data for sentiment analysis task and that finetuned model performs the best among all other models. We also perform a deta
&lt;/p&gt;</description></item><item><title>AgentCF &#26159;&#19968;&#31181;&#22522;&#20110;&#33258;&#20027;&#35821;&#35328;&#20195;&#29702;&#30340;&#21327;&#20316;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#27169;&#25311;&#29992;&#25143;&#21644;&#29289;&#21697;&#30340;&#20132;&#20114;&#65292;&#24182;&#20248;&#21270;&#36825;&#20004;&#31867;&#20195;&#29702;&#12290;</title><link>http://arxiv.org/abs/2310.09233</link><description>&lt;p&gt;
AgentCF: &#22522;&#20110;&#33258;&#20027;&#35821;&#35328;&#20195;&#29702;&#30340;&#21327;&#20316;&#23398;&#20064;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
AgentCF: Collaborative Learning with Autonomous Language Agents for Recommender Systems. (arXiv:2310.09233v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09233
&lt;/p&gt;
&lt;p&gt;
AgentCF &#26159;&#19968;&#31181;&#22522;&#20110;&#33258;&#20027;&#35821;&#35328;&#20195;&#29702;&#30340;&#21327;&#20316;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#27169;&#25311;&#29992;&#25143;&#21644;&#29289;&#21697;&#30340;&#20132;&#20114;&#65292;&#24182;&#20248;&#21270;&#36825;&#20004;&#31867;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21033;&#29992;&#20855;&#26377;&#20986;&#33394;&#20915;&#31574;&#33021;&#21147;&#30340;LLM&#65288;&#35821;&#35328;&#28151;&#21512;&#27169;&#22411;&#65289;&#20195;&#29702;&#20316;&#20026;&#21487;&#20449;&#30340;&#20154;&#31867;&#20195;&#29702;&#20986;&#29616;&#20102;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#27169;&#25311;&#20154;&#31867;&#23545;&#35805;&#19978;&#12290;&#20154;&#31867;&#30340;&#38750;&#35821;&#35328;&#34892;&#20026;&#65292;&#22914;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#29289;&#21697;&#28857;&#20987;&#65292;&#34429;&#28982;&#38544;&#21547;&#30528;&#29992;&#25143;&#30340;&#20559;&#22909;&#24182;&#33021;&#25552;&#21319;&#29992;&#25143;&#24314;&#27169;&#65292;&#20294;&#23578;&#26410;&#24471;&#21040;&#28145;&#20837;&#25506;&#32034;&#12290;&#20027;&#35201;&#21407;&#22240;&#22312;&#20110;&#35821;&#35328;&#24314;&#27169;&#19982;&#34892;&#20026;&#24314;&#27169;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20197;&#21450;LLMs&#23545;&#29992;&#25143;-&#29289;&#21697;&#20851;&#31995;&#30340;&#19981;&#29702;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AgentCF&#65292;&#36890;&#36807;&#22522;&#20110;&#20195;&#29702;&#30340;&#21327;&#21516;&#36807;&#28388;&#26469;&#27169;&#25311;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#12290;&#25105;&#20204;&#21019;&#36896;&#24615;&#22320;&#32771;&#34385;&#29992;&#25143;&#21644;&#29289;&#21697;&#20316;&#20026;&#20195;&#29702;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#21327;&#20316;&#23398;&#20064;&#26041;&#27861;&#26469;&#20248;&#21270;&#36825;&#20004;&#31181;&#20195;&#29702;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#27599;&#20010;&#26102;&#38388;&#27493;&#65292;&#25105;&#20204;&#39318;&#20808;&#20419;&#20351;&#29992;&#25143;&#20195;&#29702;&#21644;&#29289;&#21697;&#20195;&#29702;&#33258;&#20027;&#22320;&#36827;&#34892;&#20132;&#20114;&#12290;&#28982;&#21518;&#65292;&#22522;&#20110;&#24046;&#24322;&#24615;&#23545;&#36825;&#20004;&#31867;&#20195;&#29702;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, there has been an emergence of employing LLM-powered agents as believable human proxies, based on their remarkable decision-making capability. However, existing studies mainly focus on simulating human dialogue. Human non-verbal behaviors, such as item clicking in recommender systems, although implicitly exhibiting user preferences and could enhance the modeling of users, have not been deeply explored. The main reasons lie in the gap between language modeling and behavior modeling, as well as the incomprehension of LLMs about user-item relations.  To address this issue, we propose AgentCF for simulating user-item interactions in recommender systems through agent-based collaborative filtering. We creatively consider not only users but also items as agents, and develop a collaborative learning approach that optimizes both kinds of agents together. Specifically, at each time step, we first prompt the user and item agents to interact autonomously. Then, based on the disparities b
&lt;/p&gt;</description></item><item><title>FACT-GPT&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#33258;&#21160;&#21270;&#20107;&#23454;&#26680;&#26597;&#30340;&#22768;&#31216;&#21305;&#37197;&#38454;&#27573;&#12290;&#23427;&#36890;&#36807;&#20351;&#29992;GPT-4&#29983;&#25104;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#24494;&#35843;&#30340;LLM&#65292;&#22312;&#35782;&#21035;&#19982;&#20107;&#23454;&#26680;&#26597;&#21592;&#20808;&#21069;&#35777;&#23454;&#20026;&#34394;&#20551;&#30340;&#22768;&#31216;&#30456;&#21305;&#37197;&#30340;&#26032;&#30340;&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#26356;&#22823;&#30340;&#39044;&#35757;&#32451;LLMs&#30340;&#24615;&#33021;&#30456;&#23218;&#32654;&#12290;</title><link>http://arxiv.org/abs/2310.09223</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#32034;&#24341;&#21305;&#37197;&#65306;&#22686;&#24378;&#20107;&#23454;&#26680;&#26597;&#21592;&#22312;&#23545;&#25239;&#34394;&#20551;&#20449;&#24687;&#30340;&#25112;&#26007;&#20013;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Automated Claim Matching with Large Language Models: Empowering Fact-Checkers in the Fight Against Misinformation. (arXiv:2310.09223v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09223
&lt;/p&gt;
&lt;p&gt;
FACT-GPT&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#33258;&#21160;&#21270;&#20107;&#23454;&#26680;&#26597;&#30340;&#22768;&#31216;&#21305;&#37197;&#38454;&#27573;&#12290;&#23427;&#36890;&#36807;&#20351;&#29992;GPT-4&#29983;&#25104;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#24494;&#35843;&#30340;LLM&#65292;&#22312;&#35782;&#21035;&#19982;&#20107;&#23454;&#26680;&#26597;&#21592;&#20808;&#21069;&#35777;&#23454;&#20026;&#34394;&#20551;&#30340;&#22768;&#31216;&#30456;&#21305;&#37197;&#30340;&#26032;&#30340;&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#26356;&#22823;&#30340;&#39044;&#35757;&#32451;LLMs&#30340;&#24615;&#33021;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#25968;&#23383;&#21270;&#26102;&#20195;&#65292;&#34394;&#20551;&#20449;&#24687;&#30340;&#36805;&#36895;&#20256;&#25773;&#23545;&#20844;&#20247;&#31119;&#31049;&#21644;&#31038;&#20250;&#20449;&#20219;&#26500;&#25104;&#23041;&#32961;&#12290;&#38543;&#30528;&#22312;&#32447;&#34394;&#20551;&#20449;&#24687;&#30340;&#34067;&#24310;&#65292;&#20107;&#23454;&#26680;&#26597;&#21592;&#30340;&#25163;&#21160;&#39564;&#35777;&#21464;&#24471;&#36234;&#26469;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;FACT-GPT&#65288;&#22522;&#20110;&#22768;&#31216;&#21305;&#37197;&#20219;&#21153;&#30340;&#20107;&#23454;&#26680;&#26597;&#22686;&#24378;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#21270;&#20107;&#23454;&#26680;&#26597;&#30340;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#35782;&#21035;&#20808;&#21069;&#34987;&#20107;&#23454;&#26680;&#26597;&#21592;&#35777;&#23454;&#20026;&#34394;&#20551;&#30340;&#22768;&#31216;&#30340;&#26032;&#30340;&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#65292;&#20854;&#35201;&#20040;&#25903;&#25345;&#35201;&#20040;&#30456;&#30683;&#30462;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;GPT-4&#29983;&#25104;&#19968;&#20010;&#27169;&#25311;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#29992;&#20110;&#24494;&#35843;&#26356;&#19987;&#38376;&#30340;LLM&#12290;&#25105;&#20204;&#22312;&#19982;&#20844;&#20849;&#21355;&#29983;&#30456;&#20851;&#30340;&#22823;&#37327;&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#30340;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;FACT-GPT&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#24494;&#35843;LLM&#22312;&#22768;&#31216;&#21305;&#37197;&#20219;&#21153;&#20013;&#19982;&#26356;&#22823;&#30340;&#39044;&#35757;&#32451;LLM&#30340;&#24615;&#33021;&#30456;&#23218;&#32654;&#65292;&#36798;&#21040;&#20102;&#25509;&#36817;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In today's digital era, the rapid spread of misinformation poses threats to public well-being and societal trust. As online misinformation proliferates, manual verification by fact checkers becomes increasingly challenging. We introduce FACT-GPT (Fact-checking Augmentation with Claim matching Task-oriented Generative Pre-trained Transformer), a framework designed to automate the claim matching phase of fact-checking using Large Language Models (LLMs). This framework identifies new social media content that either supports or contradicts claims previously debunked by fact-checkers. Our approach employs GPT-4 to generate a labeled dataset consisting of simulated social media posts. This data set serves as a training ground for fine-tuning more specialized LLMs. We evaluated FACT-GPT on an extensive dataset of social media content related to public health. The results indicate that our fine-tuned LLMs rival the performance of larger pre-trained LLMs in claim matching tasks, aligning close
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;LLM&#29983;&#25104;&#30340;&#25512;&#33616;&#20449;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#36827;&#34892;&#20102;&#32454;&#33268;&#30340;&#30740;&#31350;&#65292;&#24182;&#35774;&#35745;&#20102;&#35780;&#20272;&#26041;&#27861;&#26469;&#23637;&#29616;&#36890;&#36807;&#35821;&#35328;&#39118;&#26684;&#21644;&#35789;&#27719;&#20869;&#23481;&#26469;&#20307;&#29616;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2310.09219</link><description>&lt;p&gt;
"&#20975;&#21033;&#26159;&#19968;&#20010;&#28201;&#26262;&#30340;&#20154;&#65292;&#32422;&#29791;&#22827;&#26159;&#19968;&#20010;&#27036;&#26679;": LLM&#29983;&#25104;&#30340;&#25512;&#33616;&#20449;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
"Kelly is a Warm Person, Joseph is a Role Model": Gender Biases in LLM-Generated Reference Letters. (arXiv:2310.09219v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;LLM&#29983;&#25104;&#30340;&#25512;&#33616;&#20449;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#36827;&#34892;&#20102;&#32454;&#33268;&#30340;&#30740;&#31350;&#65292;&#24182;&#35774;&#35745;&#20102;&#35780;&#20272;&#26041;&#27861;&#26469;&#23637;&#29616;&#36890;&#36807;&#35821;&#35328;&#39118;&#26684;&#21644;&#35789;&#27719;&#20869;&#23481;&#26469;&#20307;&#29616;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#27493;&#65292;&#29992;&#25143;&#24050;&#32463;&#24320;&#22987;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#21327;&#21161;&#25776;&#20889;&#21508;&#31181;&#31867;&#22411;&#30340;&#20869;&#23481;&#65292;&#21253;&#25324;&#25512;&#33616;&#20449;&#31561;&#32844;&#19994;&#25991;&#20214;&#12290;&#23613;&#31649;&#23427;&#20204;&#30340;&#26041;&#20415;&#24615;&#65292;&#20294;&#36825;&#20123;&#24212;&#29992;&#24341;&#20837;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#20844;&#24179;&#38382;&#39064;&#12290;&#30001;&#20110;&#29983;&#25104;&#30340;&#25512;&#33616;&#20449;&#21487;&#33021;&#34987;&#29992;&#25143;&#30452;&#25509;&#22312;&#32844;&#19994;&#25110;&#23398;&#26415;&#22330;&#26223;&#20013;&#20351;&#29992;&#65292;&#23427;&#20204;&#26377;&#21487;&#33021;&#36896;&#25104;&#30452;&#25509;&#30340;&#31038;&#20250;&#20260;&#23475;&#65292;&#22914;&#38477;&#20302;&#22899;&#24615;&#30003;&#35831;&#32773;&#30340;&#25104;&#21151;&#29575;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#26410;&#26469;&#30340;&#32531;&#35299;&#21644;&#30417;&#25511;&#65292;&#20840;&#38754;&#30740;&#31350;&#27492;&#31867;&#23454;&#38469;&#24212;&#29992;&#24773;&#20917;&#20013;&#30340;&#20844;&#24179;&#38382;&#39064;&#21644;&#30456;&#20851;&#20260;&#23475;&#21183;&#22312;&#24517;&#34892;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;LLM&#29983;&#25104;&#30340;&#25512;&#33616;&#20449;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#30340;&#30740;&#31350;&#12290;&#21463;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#32467;&#26524;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#20010;&#32500;&#24230;&#26469;&#23637;&#29616;LLM&#29983;&#25104;&#30340;&#20449;&#20214;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65306;&#35821;&#35328;&#39118;&#26684;&#30340;&#20559;&#35265;&#21644;&#35789;&#27719;&#20869;&#23481;&#30340;&#20559;&#35265;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#25512;&#33616;&#20449;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
As generative language models advance, users have started to utilize Large Language Models (LLMs) to assist in writing various types of content, including professional documents such as recommendation letters. Despite their convenience, these applications introduce unprecedented fairness concerns. As generated reference letters might be directly utilized by users in professional or academic scenarios, they have the potential to cause direct social harms, such as lowering success rates for female applicants. Therefore, it is imminent and necessary to comprehensively study fairness issues and associated harms in such real-world use cases for future mitigation and monitoring. In this paper, we critically examine gender bias in LLM-generated reference letters. Inspired by findings in social science, we design evaluation methods to manifest gender biases in LLM-generated letters through 2 dimensions: biases in language style and biases in lexical content. Furthermore, we investigate the ext
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#37319;&#29992;&#25506;&#32034;&#25351;&#23548;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#36827;&#34892;&#20027;&#21160;&#25506;&#32034;&#65292;&#22686;&#24378;&#20102;&#39046;&#22495;&#29305;&#23450;&#25351;&#23548;&#35843;&#20248;&#30340;&#25968;&#25454;&#35206;&#30422;&#33539;&#22260;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.09168</link><description>&lt;p&gt;
&#25506;&#32034;&#25351;&#23548;&#65306;&#36890;&#36807;&#20027;&#21160;&#25506;&#32034;&#22686;&#24378;&#29305;&#23450;&#39046;&#22495;&#25351;&#23548;&#35206;&#30422;&#29575;
&lt;/p&gt;
&lt;p&gt;
Explore-Instruct: Enhancing Domain-Specific Instruction Coverage through Active Exploration. (arXiv:2310.09168v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09168
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#37319;&#29992;&#25506;&#32034;&#25351;&#23548;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#36827;&#34892;&#20027;&#21160;&#25506;&#32034;&#65292;&#22686;&#24378;&#20102;&#39046;&#22495;&#29305;&#23450;&#25351;&#23548;&#35843;&#20248;&#30340;&#25968;&#25454;&#35206;&#30422;&#33539;&#22260;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22686;&#24378;&#22810;&#26679;&#24615;&#65292;&#21487;&#20197;&#22823;&#24133;&#20248;&#21270;&#25351;&#23548;&#35843;&#20248;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#26356;&#24191;&#27867;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#27492;&#31867;&#35843;&#20248;&#30340;&#29616;&#26377;&#25968;&#25454;&#24448;&#24448;&#23545;&#20010;&#21035;&#39046;&#22495;&#30340;&#35206;&#30422;&#19981;&#36275;&#65292;&#38480;&#21046;&#20102;&#23545;&#36825;&#20123;&#39046;&#22495;&#20869;&#32454;&#33268;&#29702;&#35299;&#21644;&#20132;&#20114;&#30340;&#33539;&#22260;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#25506;&#32034;&#25351;&#23548;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#30340;&#20027;&#21160;&#25506;&#32034;&#26469;&#22686;&#24378;&#29992;&#20110;&#29305;&#23450;&#39046;&#22495;&#25351;&#23548;&#35843;&#20248;&#30340;&#25968;&#25454;&#35206;&#30422;&#12290;&#25506;&#32034;&#25351;&#23548;&#22522;&#20110;&#20856;&#22411;&#30340;&#39046;&#22495;&#20351;&#29992;&#26696;&#20363;&#65292;&#36890;&#36807;&#23454;&#29616;&#25628;&#32034;&#31639;&#27861;&#26469;&#33719;&#21462;&#22810;&#26679;&#21270;&#21644;&#38754;&#21521;&#39046;&#22495;&#30340;&#25351;&#23548;&#35843;&#20248;&#25968;&#25454;&#30340;&#22810;&#31181;&#21464;&#20307;&#25110;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#20013;&#24515;&#20998;&#26512;&#39564;&#35777;&#20102;&#27492;&#26041;&#27861;&#22312;&#25913;&#36827;&#29305;&#23450;&#39046;&#22495;&#25351;&#23548;&#35206;&#30422;&#33539;&#22260;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#27169;&#22411;&#30340;&#24615;&#33021;&#26174;&#31034;&#20986;&#19982;&#22810;&#20010;&#22522;&#32447;&#27169;&#22411;&#30456;&#27604;&#30340;&#26174;&#33879;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction-tuning can be substantially optimized through enhanced diversity, resulting in models capable of handling a broader spectrum of tasks. However, existing data employed for such tuning often exhibit an inadequate coverage of individual domains, limiting the scope for nuanced comprehension and interactions within these areas. To address this deficiency, we propose Explore-Instruct, a novel approach to enhance the data coverage to be used in domain-specific instruction-tuning through active exploration via Large Language Models (LLMs). Built upon representative domain use cases, Explore-Instruct explores a multitude of variations or possibilities by implementing a search algorithm to obtain diversified and domain-focused instruction-tuning data. Our data-centric analysis validates the effectiveness of this proposed approach in improving domain-specific instruction coverage. Moreover, our model's performance demonstrates considerable advancements over multiple baselines, includi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#26377;&#32447;&#30005;&#35270;&#33410;&#30446;&#25552;&#21450;&#30340;&#20027;&#39064;&#36827;&#34892;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#31435;&#22330;&#20998;&#26512;&#30340;&#26041;&#24335;&#65292;&#26469;&#34920;&#24449;&#20854;&#20559;&#35265;&#12290;&#22312;2020&#24180;&#30340;&#26377;&#32447;&#30005;&#35270;&#36716;&#24405;&#20013;&#24212;&#29992;&#35813;&#26041;&#27861;&#65292;&#21457;&#29616;&#33410;&#30446;&#32858;&#31867;&#19982;&#33410;&#30446;&#25152;&#23646;&#30340;&#26377;&#32447;&#30005;&#35270;&#32593;&#32476;&#20445;&#25345;&#19968;&#33268;&#12290;&#25581;&#31034;&#20102;&#23458;&#35266;&#35780;&#20272;&#23186;&#20307;&#20559;&#35265;&#21644;&#34920;&#24449;&#38476;&#29983;&#23186;&#20307;&#29615;&#22659;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.09166</link><description>&lt;p&gt;
&#24320;&#21457;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#27169;&#22411;&#26469;&#34920;&#24449;&#26377;&#32447;&#30005;&#35270;&#26032;&#38395;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Developing a Natural Language Understanding Model to Characterize Cable News Bias. (arXiv:2310.09166v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#26377;&#32447;&#30005;&#35270;&#33410;&#30446;&#25552;&#21450;&#30340;&#20027;&#39064;&#36827;&#34892;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#31435;&#22330;&#20998;&#26512;&#30340;&#26041;&#24335;&#65292;&#26469;&#34920;&#24449;&#20854;&#20559;&#35265;&#12290;&#22312;2020&#24180;&#30340;&#26377;&#32447;&#30005;&#35270;&#36716;&#24405;&#20013;&#24212;&#29992;&#35813;&#26041;&#27861;&#65292;&#21457;&#29616;&#33410;&#30446;&#32858;&#31867;&#19982;&#33410;&#30446;&#25152;&#23646;&#30340;&#26377;&#32447;&#30005;&#35270;&#32593;&#32476;&#20445;&#25345;&#19968;&#33268;&#12290;&#25581;&#31034;&#20102;&#23458;&#35266;&#35780;&#20272;&#23186;&#20307;&#20559;&#35265;&#21644;&#34920;&#24449;&#38476;&#29983;&#23186;&#20307;&#29615;&#22659;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23186;&#20307;&#20559;&#35265;&#22312;&#31038;&#20250;&#31185;&#23398;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#30740;&#31350;&#20173;&#28982;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#38752;&#20154;&#24037;&#36755;&#20837;&#21644;&#20027;&#35266;&#35780;&#20272;&#26469;&#26631;&#35760;&#20559;&#35265;&#65292;&#23588;&#20854;&#26159;&#26377;&#32447;&#30005;&#35270;&#30740;&#31350;&#26356;&#26159;&#22914;&#27492;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#22312;&#27809;&#26377;&#20219;&#20309;&#20154;&#24037;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#34920;&#24449;&#26377;&#32447;&#30005;&#35270;&#33410;&#30446;&#30340;&#20559;&#35265;&#12290;&#35813;&#26041;&#27861;&#20381;&#36182;&#20110;&#36890;&#36807;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20998;&#26512;&#25552;&#21450;&#30340;&#20027;&#39064;&#20197;&#21450;&#36890;&#36807;&#31435;&#22330;&#20998;&#26512;&#26469;&#35752;&#35770;&#36825;&#20123;&#20027;&#39064;&#30340;&#26041;&#24335;&#65292;&#20197;&#20415;&#23558;&#20855;&#26377;&#31867;&#20284;&#20559;&#35265;&#30340;&#33410;&#30446;&#36827;&#34892;&#32858;&#31867;&#12290;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;2020&#24180;&#30340;&#26377;&#32447;&#30005;&#35270;&#36716;&#24405;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#33410;&#30446;&#32858;&#31867;&#38543;&#26102;&#38388;&#20445;&#25345;&#19968;&#33268;&#65292;&#24182;&#22823;&#33268;&#23545;&#24212;&#20110;&#33410;&#30446;&#25152;&#23646;&#30340;&#26377;&#32447;&#30005;&#35270;&#32593;&#32476;&#12290;&#35813;&#26041;&#27861;&#25581;&#31034;&#20102;&#26410;&#26469;&#23458;&#35266;&#35780;&#20272;&#23186;&#20307;&#20559;&#35265;&#21644;&#34920;&#24449;&#38476;&#29983;&#23186;&#20307;&#29615;&#22659;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Media bias has been extensively studied by both social and computational sciences. However, current work still has a large reliance on human input and subjective assessment to label biases. This is especially true for cable news research. To address these issues, we develop an unsupervised machine learning method to characterize the bias of cable news programs without any human input. This method relies on the analysis of what topics are mentioned through Named Entity Recognition and how those topics are discussed through Stance Analysis in order to cluster programs with similar biases together. Applying our method to 2020 cable news transcripts, we find that program clusters are consistent over time and roughly correspond to the cable news network of the program. This method reveals the potential for future tools to objectively assess media bias and characterize unfamiliar media environments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;BibRank&#24179;&#21488;&#65292;&#35813;&#24179;&#21488;&#38598;&#25104;&#20102;&#20851;&#38190;&#35789;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#38190;&#35789;&#25552;&#21462;&#31639;&#27861;&#30340;&#35780;&#20272;&#12290;BibRank&#36890;&#36807;&#20351;&#29992;&#20016;&#23500;&#30340;BibTeX&#25968;&#25454;&#38598;&#21644;&#21019;&#26032;&#30340;&#21152;&#26435;&#25216;&#26415;&#65292;&#32467;&#21512;&#20301;&#32622;&#12289;&#32479;&#35745;&#21644;&#35789;&#20849;&#29616;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#20851;&#38190;&#35789;&#25552;&#21462;&#12290;</title><link>http://arxiv.org/abs/2310.09151</link><description>&lt;p&gt;
BibRank&#65306;&#20351;&#29992;&#20803;&#25968;&#25454;&#30340;&#33258;&#21160;&#20851;&#38190;&#35789;&#25552;&#21462;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
BibRank: Automatic Keyphrase Extraction Platform Using~Metadata. (arXiv:2310.09151v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;BibRank&#24179;&#21488;&#65292;&#35813;&#24179;&#21488;&#38598;&#25104;&#20102;&#20851;&#38190;&#35789;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#38190;&#35789;&#25552;&#21462;&#31639;&#27861;&#30340;&#35780;&#20272;&#12290;BibRank&#36890;&#36807;&#20351;&#29992;&#20016;&#23500;&#30340;BibTeX&#25968;&#25454;&#38598;&#21644;&#21019;&#26032;&#30340;&#21152;&#26435;&#25216;&#26415;&#65292;&#32467;&#21512;&#20301;&#32622;&#12289;&#32479;&#35745;&#21644;&#35789;&#20849;&#29616;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#20851;&#38190;&#35789;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#20851;&#38190;&#35789;&#25552;&#21462;&#26159;&#35782;&#21035;&#25991;&#26723;&#20013;&#20851;&#38190;&#30701;&#35821;&#30340;&#36807;&#31243;&#12290;&#36825;&#20123;&#20851;&#38190;&#35789;&#22312;&#25991;&#26723;&#20998;&#31867;&#12289;&#32858;&#31867;&#12289;&#25512;&#33616;&#12289;&#32034;&#24341;&#12289;&#25628;&#32034;&#12289;&#25688;&#35201;&#21644;&#25991;&#26412;&#31616;&#21270;&#31561;&#22810;&#20010;&#20219;&#21153;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#21488;&#65292;&#38598;&#25104;&#20102;&#20851;&#38190;&#35789;&#25968;&#25454;&#38598;&#65292;&#24182;&#26041;&#20415;&#20102;&#20851;&#38190;&#35789;&#25552;&#21462;&#31639;&#27861;&#30340;&#35780;&#20272;&#12290;&#35813;&#24179;&#21488;&#21253;&#25324;BibRank&#65292;&#19968;&#31181;&#33258;&#21160;&#20851;&#38190;&#35789;&#25552;&#21462;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#35299;&#26512;BibTeX&#26684;&#24335;&#30340;&#21442;&#32771;&#25991;&#29486;&#25968;&#25454;&#33719;&#24471;&#30340;&#20016;&#23500;&#25968;&#25454;&#38598;&#12290;BibRank&#32467;&#21512;&#20102;&#21019;&#26032;&#30340;&#21152;&#26435;&#25216;&#26415;&#21644;&#20301;&#32622;&#12289;&#32479;&#35745;&#21644;&#35789;&#20849;&#29616;&#20449;&#24687;&#65292;&#20174;&#25991;&#26723;&#20013;&#25552;&#21462;&#20851;&#38190;&#35789;&#12290;&#35813;&#24179;&#21488;&#23545;&#20110;&#23547;&#27714;&#25913;&#36827;&#20851;&#38190;&#35789;&#25552;&#21462;&#31639;&#27861;&#24182;&#25512;&#36827;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#32773;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic Keyphrase Extraction involves identifying essential phrases in a document. These keyphrases are crucial in various tasks such as document classification, clustering, recommendation, indexing, searching, summarization, and text simplification. This paper introduces a platform that integrates keyphrase datasets and facilitates the evaluation of keyphrase extraction algorithms. The platform includes BibRank, an automatic keyphrase extraction algorithm that leverages a rich dataset obtained by parsing bibliographic data in BibTeX format. BibRank combines innovative weighting techniques with positional, statistical, and word co-occurrence information to extract keyphrases from documents. The platform proves valuable for researchers and developers seeking to enhance their keyphrase extraction algorithms and advance the field of natural language processing.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PuoBERTa&#30340;&#23450;&#21046;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#38024;&#23545;&#22622;&#33576;&#29926;&#32435;&#35821;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#20419;&#36827;&#22622;&#33576;&#29926;&#32435;&#35821;&#31561;&#23569;&#30740;&#31350;&#35821;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.09141</link><description>&lt;p&gt;
PuoBERTa:&#35757;&#32451;&#21644;&#35780;&#20272;&#19968;&#31181;&#20026;&#22622;&#33576;&#29926;&#32435;&#35821;&#23450;&#21046;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PuoBERTa: Training and evaluation of a curated language model for Setswana. (arXiv:2310.09141v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PuoBERTa&#30340;&#23450;&#21046;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#38024;&#23545;&#22622;&#33576;&#29926;&#32435;&#35821;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#20419;&#36827;&#22622;&#33576;&#29926;&#32435;&#35821;&#31561;&#23569;&#30740;&#31350;&#35821;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#36164;&#28304;&#20016;&#23500;&#30340;&#35821;&#35328;&#65288;&#22914;&#33521;&#35821;&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#22312;&#36164;&#28304;&#21294;&#20047;&#30340;&#35821;&#35328;&#65288;&#22914;&#22622;&#33576;&#29926;&#32435;&#35821;&#65289;&#26041;&#38754;&#21364;&#28382;&#21518;&#12290;&#26412;&#25991;&#36890;&#36807;&#20171;&#32461;PuoBERTa&#65292;&#19968;&#31181;&#19987;&#38376;&#20026;&#22622;&#33576;&#29926;&#32435;&#35821;&#35757;&#32451;&#30340;&#23450;&#21046;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#24357;&#34917;&#20102;&#36825;&#19968;&#24046;&#36317;&#12290;&#25105;&#20204;&#20171;&#32461;&#22914;&#20309;&#25910;&#38598;&#12289;&#31579;&#36873;&#21644;&#20934;&#22791;&#22810;&#26679;&#21270;&#30340;&#21333;&#35821;&#25991;&#26412;&#65292;&#20026;PuoBERTa&#30340;&#35757;&#32451;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35821;&#26009;&#24211;&#12290;&#22312;&#20043;&#21069;&#20026;&#22622;&#33576;&#29926;&#32435;&#35821;&#21019;&#24314;&#21333;&#35821;&#36164;&#28304;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;PuoBERTa&#22312;&#22810;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21253;&#25324;&#35789;&#24615;&#26631;&#27880;&#12289;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#26032;&#38395;&#20998;&#31867;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22622;&#33576;&#29926;&#32435;&#35821;&#26032;&#38395;&#20998;&#31867;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20379;&#20102;&#20351;&#29992;PuoBERTa&#30340;&#21021;&#22987;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23637;&#31034;&#20102;PuoBERTa&#22312;&#20419;&#36827;&#22622;&#33576;&#29926;&#32435;&#35821;&#31561;&#23569;&#30740;&#31350;&#35821;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language processing (NLP) has made significant progress for well-resourced languages such as English but lagged behind for low-resource languages like Setswana. This paper addresses this gap by presenting PuoBERTa, a customised masked language model trained specifically for Setswana. We cover how we collected, curated, and prepared diverse monolingual texts to generate a high-quality corpus for PuoBERTa's training. Building upon previous efforts in creating monolingual resources for Setswana, we evaluated PuoBERTa across several NLP tasks, including part-of-speech (POS) tagging, named entity recognition (NER), and news categorisation. Additionally, we introduced a new Setswana news categorisation dataset and provided the initial benchmarks using PuoBERTa. Our work demonstrates the efficacy of PuoBERTa in fostering NLP capabilities for understudied languages like Setswana and paves the way for future research directions.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#35299;&#30721;&#26041;&#27861;&#65292;&#23558;&#20854;&#35270;&#20026;&#35268;&#33539;&#21270;&#30340;&#19981;&#23436;&#32654;&#20449;&#24687;&#24207;&#21015;&#20449;&#21495;&#21338;&#24328;&#65292;&#24182;&#36890;&#36807;&#25214;&#21040;&#36817;&#20284;&#22343;&#34913;&#28857;&#24471;&#21040;&#20102;&#19968;&#20010;&#35299;&#30721;&#31639;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#38382;&#31572;&#21644;&#20854;&#20182;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#12290;</title><link>http://arxiv.org/abs/2310.09139</link><description>&lt;p&gt;
&#20849;&#35782;&#28216;&#25103;&#65306;&#36890;&#36807;&#22343;&#34913;&#25628;&#32034;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
The Consensus Game: Language Model Generation via Equilibrium Search. (arXiv:2310.09139v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09139
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#35299;&#30721;&#26041;&#27861;&#65292;&#23558;&#20854;&#35270;&#20026;&#35268;&#33539;&#21270;&#30340;&#19981;&#23436;&#32654;&#20449;&#24687;&#24207;&#21015;&#20449;&#21495;&#21338;&#24328;&#65292;&#24182;&#36890;&#36807;&#25214;&#21040;&#36817;&#20284;&#22343;&#34913;&#28857;&#24471;&#21040;&#20102;&#19968;&#20010;&#35299;&#30721;&#31639;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#38382;&#31572;&#21644;&#20854;&#20182;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#24212;&#29992;&#20110;&#38382;&#31572;&#21644;&#20854;&#20182;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#26102;&#65292;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#21487;&#20197;&#36890;&#36807;&#29983;&#25104;&#24335;&#26597;&#35810;&#65288;&#36890;&#36807;&#20174;&#20854;&#36755;&#20986;&#20998;&#24067;&#20013;&#25277;&#26679;&#31572;&#26696;&#65289;&#25110;&#21028;&#21035;&#24335;&#26597;&#35810;&#65288;&#36890;&#36807;&#20351;&#29992;&#23427;&#20204;&#23545;&#19968;&#32452;&#20505;&#36873;&#36755;&#20986;&#36827;&#34892;&#35780;&#20998;&#25110;&#25490;&#24207;&#65289;&#36827;&#34892;&#26597;&#35810;&#12290;&#36825;&#20123;&#36807;&#31243;&#26377;&#26102;&#20250;&#20135;&#29983;&#38750;&#24120;&#19981;&#21516;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#22914;&#20309;&#35843;&#21644;&#20114;&#19981;&#30456;&#23481;&#30340;&#35780;&#20998;&#36807;&#31243;&#20197;&#33719;&#24471;&#36830;&#36143;&#30340;LM&#39044;&#27979;&#21602;&#65311;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#12289;&#26080;&#38656;&#35757;&#32451;&#30340;&#12289;&#21338;&#24328;&#35770;&#36807;&#31243;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#35299;&#30721;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#35821;&#35328;&#27169;&#22411;&#35299;&#30721;&#35270;&#20026;&#19968;&#31181;&#35268;&#33539;&#21270;&#30340;&#19981;&#23436;&#32654;&#20449;&#24687;&#24207;&#21015;&#20449;&#21495;&#21338;&#24328; - &#31216;&#20026;&#20849;&#35782;&#28216;&#25103; - &#22312;&#35813;&#21338;&#24328;&#20013;&#65292;&#19968;&#20010;&#29983;&#25104;&#22120;&#35797;&#22270;&#29992;&#33258;&#28982;&#35821;&#35328;&#21477;&#23376;&#20256;&#36798;&#19968;&#20010;&#25277;&#35937;&#30340;&#27491;&#30830;&#24615;&#21442;&#25968;&#32473;&#19968;&#20010;&#21028;&#21035;&#22120;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#35745;&#31639;&#31243;&#24207;&#26469;&#25214;&#21040;&#36825;&#20010;&#21338;&#24328;&#30340;&#36817;&#20284;&#22343;&#34913;&#28857;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#19968;&#20010;&#25105;&#20204;&#31216;&#20043;&#20026;EQUILIBRIUM-RANKING&#30340;&#35299;&#30721;&#31639;&#27861;&#12290;&#24212;&#29992;&#20110;&#22823;&#37327;&#20219;&#21153;&#65288;&#21253;&#25324;&#38405;&#35835;&#29702;&#35299;&#65292;&#24120;&#35782;&#65289;
&lt;/p&gt;
&lt;p&gt;
When applied to question answering and other text generation tasks, language models (LMs) may be queried generatively (by sampling answers from their output distribution) or discriminatively (by using them to score or rank a set of candidate outputs). These procedures sometimes yield very different predictions. How do we reconcile mutually incompatible scoring procedures to obtain coherent LM predictions? We introduce a new, a training-free, game-theoretic procedure for language model decoding. Our approach casts language model decoding as a regularized imperfect-information sequential signaling game - which we term the CONSENSUS GAME - in which a GENERATOR seeks to communicate an abstract correctness parameter using natural language sentences to a DISCRIMINATOR. We develop computational procedures for finding approximate equilibria of this game, resulting in a decoding algorithm we call EQUILIBRIUM-RANKING. Applied to a large number of tasks (including reading comprehension, commonsen
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38646;&#26679;&#26412;&#25554;&#27133;&#22635;&#20805;&#30340;Hierarchical Contrastive Learning Framework (HiCL)&#65292;&#36890;&#36807;&#31895;&#31890;&#24230;&#21040;&#32454;&#31890;&#24230;&#30340;&#23545;&#27604;&#23398;&#20064;&#65292;&#23398;&#20064;&#35821;&#21477;&#20196;&#29260;&#20043;&#38388;&#30340;&#28145;&#23618;&#35821;&#20041;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36845;&#20195;&#26631;&#31614;&#38598;&#35821;&#20041;&#25512;&#29702;&#26041;&#27861;&#65292;&#26469;&#25552;&#39640;&#22312;&#26410;&#35265;&#25554;&#27133;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.09135</link><description>&lt;p&gt;
HierarchicalContrast: &#19968;&#31181;&#29992;&#20110;&#36328;&#39046;&#22495;&#38646;&#26679;&#26412;&#25554;&#27133;&#22635;&#20805;&#30340;&#31895;&#31890;&#24230;&#21040;&#32454;&#31890;&#24230;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
HierarchicalContrast: A Coarse-to-Fine Contrastive Learning Framework for Cross-Domain Zero-Shot Slot Filling. (arXiv:2310.09135v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09135
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38646;&#26679;&#26412;&#25554;&#27133;&#22635;&#20805;&#30340;Hierarchical Contrastive Learning Framework (HiCL)&#65292;&#36890;&#36807;&#31895;&#31890;&#24230;&#21040;&#32454;&#31890;&#24230;&#30340;&#23545;&#27604;&#23398;&#20064;&#65292;&#23398;&#20064;&#35821;&#21477;&#20196;&#29260;&#20043;&#38388;&#30340;&#28145;&#23618;&#35821;&#20041;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36845;&#20195;&#26631;&#31614;&#38598;&#35821;&#20041;&#25512;&#29702;&#26041;&#27861;&#65292;&#26469;&#25552;&#39640;&#22312;&#26410;&#35265;&#25554;&#27133;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38754;&#21521;&#20219;&#21153;&#30340;&#23545;&#35805;&#22330;&#26223;&#20013;&#65292;&#36328;&#39046;&#22495;&#38646;&#26679;&#26412;&#25554;&#27133;&#22635;&#20805;&#22312;&#21033;&#29992;&#28304;&#39046;&#22495;&#30693;&#35782;&#26469;&#23398;&#20064;&#20855;&#26377;&#39640;&#27867;&#21270;&#33021;&#21147;&#30340;&#27169;&#22411;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#22312;&#26410;&#30693;&#30446;&#26631;&#39046;&#22495;&#20013;&#65292;&#30001;&#20110;&#32570;&#23569;&#24102;&#27880;&#37322;&#30340;&#25968;&#25454;&#65292;&#20854;&#24615;&#33021;&#24448;&#24448;&#19981;&#29702;&#24819;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#38646;&#26679;&#26412;&#25554;&#27133;&#22635;&#20805;&#26041;&#27861;&#22312;&#30446;&#26631;&#39046;&#22495;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#65292;&#23427;&#20204;&#21482;&#33021;&#22312;&#24050;&#35265;&#25554;&#27133;&#19978;&#26377;&#25928;&#22320;&#36827;&#34892;&#30693;&#35782;&#36716;&#31227;&#65292;&#23545;&#26410;&#35265;&#25554;&#27133;&#30340;&#34920;&#29616;&#36739;&#24046;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Hierarchical Contrastive Learning Framework (HiCL)&#29992;&#20110;&#38646;&#26679;&#26412;&#25554;&#27133;&#22635;&#20805;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#20998;&#24067;&#23884;&#20837;&#30340;&#31895;&#31890;&#24230;&#21040;&#32454;&#31890;&#24230;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#38388;&#38548;&#21644;&#20869;&#37096;&#26631;&#35760;&#20998;&#24067;&#30340;&#36317;&#31163;&#65292;&#23398;&#20064;&#35821;&#21477;&#20196;&#29260;&#20043;&#38388;&#30340;&#28145;&#23618;&#35821;&#20041;&#20851;&#31995;&#12290;&#36825;&#40723;&#21169;HiCL&#22312;&#35757;&#32451;&#38454;&#27573;&#27867;&#21270;&#21040;&#26410;&#35265;&#30340;&#25554;&#27133;&#31867;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36845;&#20195;&#26631;&#31614;&#38598;&#35821;&#20041;&#25512;&#29702;&#26041;&#27861;&#65292;&#26469;&#23545;&#26631;&#31614;&#36827;&#34892;&#20844;&#27491;&#30340;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
In task-oriented dialogue scenarios, cross-domain zero-shot slot filling plays a vital role in leveraging source domain knowledge to learn a model with high generalization ability in unknown target domain where annotated data is unavailable. However, the existing state-of-the-art zero-shot slot filling methods have limited generalization ability in target domain, they only show effective knowledge transfer on seen slots and perform poorly on unseen slots. To alleviate this issue, we present a novel Hierarchical Contrastive Learning Framework (HiCL) for zero-shot slot filling. Specifically, we propose a coarseto fine-grained contrastive learning based on Gaussian-distributed embedding to learn the generalized deep semantic relations between utterance-tokens, by optimizing inter- and intra-token distribution distance. This encourages HiCL to generalize to the slot types unseen at training phase. Furthermore, we present a new iterative label set semantics inference method to unbiasedly 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20196;&#20154;&#27822;&#20007;&#30340;&#31616;&#26131;&#21363;&#25554;&#21363;&#29992;&#30340;&#20013;&#25991;&#25340;&#20889;&#26816;&#26597;&#30340;&#26816;&#27979;&#21644;&#25512;&#29702;&#27169;&#22359;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#26816;&#27979;&#12289;&#25512;&#29702;&#21644;&#25628;&#32034;&#23376;&#20219;&#21153;&#65292;&#26377;&#25928;&#22320;&#21033;&#29992;&#20013;&#25991;&#35821;&#35328;&#30340;&#22806;&#37096;&#30693;&#35782;&#65292;&#24182;&#19988;&#35813;&#27169;&#22359;&#33021;&#22815;&#36827;&#19968;&#27493;&#25552;&#21319;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#38750;&#33258;&#22238;&#24402;CSC&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.09119</link><description>&lt;p&gt;
&#19968;&#20010;&#20196;&#20154;&#27822;&#20007;&#30340;&#31616;&#26131;&#21363;&#25554;&#21363;&#29992;&#30340;&#20013;&#25991;&#25340;&#20889;&#26816;&#26597;&#30340;&#26816;&#27979;&#21644;&#25512;&#29702;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
A Frustratingly Easy Plug-and-Play Detection-and-Reasoning Module for Chinese Spelling Check. (arXiv:2310.09119v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20196;&#20154;&#27822;&#20007;&#30340;&#31616;&#26131;&#21363;&#25554;&#21363;&#29992;&#30340;&#20013;&#25991;&#25340;&#20889;&#26816;&#26597;&#30340;&#26816;&#27979;&#21644;&#25512;&#29702;&#27169;&#22359;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#26816;&#27979;&#12289;&#25512;&#29702;&#21644;&#25628;&#32034;&#23376;&#20219;&#21153;&#65292;&#26377;&#25928;&#22320;&#21033;&#29992;&#20013;&#25991;&#35821;&#35328;&#30340;&#22806;&#37096;&#30693;&#35782;&#65292;&#24182;&#19988;&#35813;&#27169;&#22359;&#33021;&#22815;&#36827;&#19968;&#27493;&#25552;&#21319;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#38750;&#33258;&#22238;&#24402;CSC&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36890;&#36807;&#35774;&#35745;&#20219;&#21153;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#25110;&#24341;&#20837;&#36741;&#21161;&#20219;&#21153;&#65292;&#20013;&#25991;&#25340;&#20889;&#26816;&#26597;&#65288;CSC&#65289;&#24471;&#21040;&#20102;&#26174;&#33879;&#25913;&#36827;&#65292;&#36825;&#20123;&#26041;&#27861;&#22823;&#22810;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#36825;&#20010;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#23558;CSC&#24037;&#20316;&#27969;&#20998;&#35299;&#20026;&#26816;&#27979;&#12289;&#25512;&#29702;&#21644;&#25628;&#32034;&#23376;&#20219;&#21153;&#65292;&#20197;&#26356;&#30452;&#25509;&#39640;&#25928;&#22320;&#21033;&#29992;&#20851;&#20110;&#20013;&#25991;&#35821;&#35328;&#30340;&#20016;&#23500;&#22806;&#37096;&#30693;&#35782;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20860;&#23481;&#29616;&#26377;SOTA&#38750;&#33258;&#22238;&#24402;CSC&#27169;&#22411;&#30340;&#21363;&#25554;&#21363;&#29992;&#30340;&#26816;&#27979;&#21644;&#25512;&#29702;&#27169;&#22359;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#21319;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#38024;&#23545;&#19968;&#20010;&#27169;&#22411;&#35757;&#32451;&#30340;&#26816;&#27979;&#21644;&#25512;&#29702;&#27169;&#22359;&#20063;&#21487;&#20197;&#20351;&#20854;&#20182;&#27169;&#22411;&#21463;&#30410;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#20219;&#21153;&#20998;&#35299;&#25152;&#25552;&#20379;&#30340;&#20027;&#35201;&#21487;&#35299;&#37322;&#24615;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#27169;&#22359;&#30340;&#26377;&#25928;&#24615;&#21644;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Chinese Spelling Check (CSC) has been greatly improved by designing task-specific pre-training methods or introducing auxiliary tasks, which mostly solve this task in an end-to-end fashion. In this paper, we propose to decompose the CSC workflow into detection, reasoning, and searching subtasks so that the rich external knowledge about the Chinese language can be leveraged more directly and efficiently. Specifically, we design a plug-and-play detection-and-reasoning module that is compatible with existing SOTA non-autoregressive CSC models to further boost their performance. We find that the detection-and-reasoning module trained for one model can also benefit other models. We also study the primary interpretability provided by the task decomposition. Extensive experiments and detailed analyses demonstrate the effectiveness and competitiveness of the proposed module.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;GLoRE&#65292;&#19968;&#20010;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#24320;&#25918;&#24335;LLM&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#38656;&#35201;&#25552;&#39640;&#12290;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#19968;&#33268;&#24615;&#25506;&#27979;&#26041;&#27861;&#21644;&#24494;&#35843;&#26041;&#27861;&#26469;&#25913;&#36827;ChatGPT&#21644;&#24320;&#25918;&#24335;LLM&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.09107</link><description>&lt;p&gt;
GLoRE: &#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
GLoRE: Evaluating Logical Reasoning of Large Language Models. (arXiv:2310.09107v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;GLoRE&#65292;&#19968;&#20010;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#24320;&#25918;&#24335;LLM&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#38656;&#35201;&#25552;&#39640;&#12290;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#19968;&#33268;&#24615;&#25506;&#27979;&#26041;&#27861;&#21644;&#24494;&#35843;&#26041;&#27861;&#26469;&#25913;&#36827;ChatGPT&#21644;&#24320;&#25918;&#24335;LLM&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#21253;&#25324;GPT-4&#21644;&#26032;&#20852;&#31038;&#21306;&#27169;&#22411;&#22312;&#20869;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#36890;&#29992;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#36825;&#20123;LLMs&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#35780;&#20272;&#30340;&#23581;&#35797;&#36824;&#24456;&#23569;&#65292;&#32780;&#36825;&#26159;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#12290;&#20026;&#20102;&#40723;&#21169;&#36827;&#19968;&#27493;&#30740;&#31350;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;GLoRE&#65292;&#19968;&#20010;&#31934;&#24515;&#32452;&#32455;&#30340;&#36890;&#29992;&#36923;&#36753;&#25512;&#29702;&#35780;&#20272;&#22522;&#20934;&#65292;&#21253;&#21547;&#20102;12&#20010;&#35206;&#30422;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#20154;&#31867;&#21644;&#30417;&#30563;&#24494;&#35843;&#30340;&#24615;&#33021;&#30456;&#27604;&#65292;&#24320;&#25918;&#24335;LLM&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#38656;&#35201;&#36827;&#19968;&#27493;&#25552;&#39640;&#65307;ChatGPT&#21644;GPT-4&#23637;&#31034;&#20102;&#36739;&#24378;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#65292;GPT-4&#22823;&#24133;&#36229;&#36807;&#20102;ChatGPT&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#19968;&#33268;&#24615;&#25506;&#27979;&#26041;&#27861;&#26469;&#25552;&#39640;ChatGPT&#30340;&#20934;&#30830;&#24615;&#65292;&#20197;&#21450;&#19968;&#31181;&#24494;&#35843;&#26041;&#27861;&#26469;&#25552;&#39640;&#24320;&#25918;&#24335;LLM&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, large language models (LLMs), including notable models such as GPT-4 and burgeoning community models, have showcased significant general language understanding abilities. However, there has been a scarcity of attempts to assess the logical reasoning capacities of these LLMs, an essential facet of natural language understanding. To encourage further investigation in this area, we introduce GLoRE, a meticulously assembled General Logical Reasoning Evaluation benchmark comprised of 12 datasets that span three different types of tasks. Our experimental results show that compared to the performance of human and supervised fine-tuning, the logical reasoning capabilities of open LLM models necessitate additional improvement; ChatGPT and GPT-4 show a strong capability of logical reasoning, with GPT-4 surpassing ChatGPT by a large margin. We propose a self-consistency probing method to enhance the accuracy of ChatGPT and a fine-tuned method to boost the performance of an open LLM. We 
&lt;/p&gt;</description></item><item><title>Qilin-Med&#26159;&#19968;&#20010;&#22810;&#38454;&#27573;&#35757;&#32451;&#30340;&#21307;&#30103;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#39046;&#22495;&#29305;&#23450;&#32487;&#32493;&#39044;&#35757;&#32451;&#12289;&#30417;&#30563;&#24494;&#35843;&#21644;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#21253;&#21547;&#21307;&#23398;&#38382;&#31572;&#12289;&#32431;&#25991;&#26412;&#12289;&#30693;&#35782;&#22270;&#35889;&#21644;&#23545;&#35805;&#30340;3Gb&#20013;&#21307;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2310.09089</link><description>&lt;p&gt;
Qilin-Med: &#22810;&#38454;&#27573;&#30693;&#35782;&#27880;&#20837;&#20808;&#36827;&#30340;&#21307;&#30103;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Qilin-Med: Multi-stage Knowledge Injection Advanced Medical Large Language Model. (arXiv:2310.09089v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09089
&lt;/p&gt;
&lt;p&gt;
Qilin-Med&#26159;&#19968;&#20010;&#22810;&#38454;&#27573;&#35757;&#32451;&#30340;&#21307;&#30103;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#39046;&#22495;&#29305;&#23450;&#32487;&#32493;&#39044;&#35757;&#32451;&#12289;&#30417;&#30563;&#24494;&#35843;&#21644;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#21253;&#21547;&#21307;&#23398;&#38382;&#31572;&#12289;&#32431;&#25991;&#26412;&#12289;&#30693;&#35782;&#22270;&#35889;&#21644;&#23545;&#35805;&#30340;3Gb&#20013;&#21307;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#24212;&#29992;&#20110;&#21307;&#30103;&#39046;&#22495;&#26377;&#30528;&#28508;&#21147;&#20294;&#20063;&#38754;&#20020;&#25361;&#25112;&#12290;&#30452;&#25509;&#20026;&#20687;&#21307;&#23398;&#36825;&#26679;&#30340;&#39046;&#22495;&#36827;&#34892;&#39044;&#35757;&#32451;&#38656;&#35201;&#22823;&#37327;&#36164;&#28304;&#65292;&#26377;&#26102;&#19981;&#21487;&#34892;&#12290;&#20165;&#20381;&#36182;&#20110;&#30417;&#30563;&#24494;&#35843; (SFT) &#21487;&#33021;&#23548;&#33268;&#36807;&#20110;&#33258;&#20449;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#26080;&#27861;&#21033;&#29992;&#39046;&#22495;&#29305;&#23450;&#30340;&#35265;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#38454;&#27573;&#35757;&#32451;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#39046;&#22495;&#29305;&#23450;&#32487;&#32493;&#39044;&#35757;&#32451; (DCPT)&#12289;SFT &#21644;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270; (DPO)&#12290;&#25105;&#20204;&#30740;&#31350;&#30340;&#19968;&#20010;&#26174;&#33879;&#36129;&#29486;&#26159;&#24341;&#20837;&#20102;&#19968;&#20010; 3Gb &#30340;&#20013;&#21307;&#25968;&#25454;&#38598; (ChiMed)&#65292;&#21253;&#25324;&#21307;&#23398;&#38382;&#31572;&#12289;&#32431;&#25991;&#26412;&#12289;&#30693;&#35782;&#22270;&#35889;&#21644;&#23545;&#35805;&#65292;&#20998;&#20026;&#19977;&#20010;&#35757;&#32451;&#38454;&#27573;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#35757;&#32451;&#27969;&#31243;&#35757;&#32451;&#30340;&#21307;&#23398;LLM&#65292;Qilin-Med&#65292;&#22312;CPT&#21644;SFT&#38454;&#27573;&#22312;CMExam&#19978;&#20998;&#21035;&#36798;&#21040;&#20102;38.4%&#21644;40.0%&#30340;&#20934;&#30830;&#29575;&#65292;&#36229;&#36807;&#20102;Baichuan-7B&#30340;33.5%&#12290;&#22312;DPO&#38454;&#27573;&#65292;&#22312;Huatuo-26M&#27979;&#35797;&#38598;&#19978;&#24471;&#20998;&#20026;16.66&#12290;
&lt;/p&gt;
&lt;p&gt;
Integrating large language models (LLMs) into healthcare presents potential but faces challenges. Directly pre-training LLMs for domains like medicine is resource-heavy and sometimes unfeasible. Sole reliance on Supervised Fine-tuning (SFT) can result in overconfident predictions and may not tap into domain specific insights. Addressing these challenges, we present a multi-stage training method combining Domain-specific Continued Pre-training (DCPT), SFT, and Direct Preference Optimization (DPO). A notable contribution of our study is the introduction of a 3Gb Chinese Medicine (ChiMed) dataset, encompassing medical question answering, plain texts, knowledge graphs, and dialogues, segmented into three training stages. The medical LLM trained with our pipeline, Qilin-Med, exhibits significant performance boosts. In the CPT and SFT phases, it achieves 38.4% and 40.0% accuracy on the CMExam, surpassing Baichuan-7B's 33.5%. In the DPO phase, on the Huatuo-26M test set, it scores 16.66 in BL
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29790;&#22763;&#24503;&#35821;&#35328;&#32763;&#35793;&#31995;&#32479;&#24314;&#35774;&#20013;&#30340;&#25361;&#25112;&#65292;&#30528;&#37325;&#25506;&#35752;&#20102;&#26041;&#35328;&#22810;&#26679;&#24615;&#21644;&#29790;&#22763;&#24503;&#35821;&#19982;&#26631;&#20934;&#24503;&#35821;&#20043;&#38388;&#30340;&#24046;&#24322;&#23545;&#31995;&#32479;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.09088</link><description>&lt;p&gt;
&#29790;&#22763;&#24503;&#35821;&#35328;&#20256;&#36755;&#30340;&#26041;&#35328;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Dialect Transfer for Swiss German Speech Translation. (arXiv:2310.09088v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29790;&#22763;&#24503;&#35821;&#35328;&#32763;&#35793;&#31995;&#32479;&#24314;&#35774;&#20013;&#30340;&#25361;&#25112;&#65292;&#30528;&#37325;&#25506;&#35752;&#20102;&#26041;&#35328;&#22810;&#26679;&#24615;&#21644;&#29790;&#22763;&#24503;&#35821;&#19982;&#26631;&#20934;&#24503;&#35821;&#20043;&#38388;&#30340;&#24046;&#24322;&#23545;&#31995;&#32479;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#24314;&#31435;&#29790;&#22763;&#24503;&#35821;&#35328;&#32763;&#35793;&#31995;&#32479;&#20013;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#20851;&#27880;&#26041;&#35328;&#22810;&#26679;&#24615;&#20197;&#21450;&#29790;&#22763;&#24503;&#35821;&#21644;&#26631;&#20934;&#24503;&#35821;&#20043;&#38388;&#30340;&#24046;&#24322;&#30340;&#24433;&#21709;&#12290;&#29790;&#22763;&#24503;&#35821;&#26159;&#19968;&#31181;&#21475;&#35821;&#35821;&#35328;&#65292;&#27809;&#26377;&#27491;&#24335;&#30340;&#20070;&#20889;&#31995;&#32479;&#65292;&#21253;&#25324;&#35768;&#22810;&#19981;&#21516;&#30340;&#26041;&#35328;&#65292;&#26159;&#19968;&#31181;&#36164;&#28304;&#21294;&#20047;&#30340;&#35821;&#35328;&#65292;&#21482;&#26377;&#22823;&#32422;500&#19975;&#20351;&#29992;&#32773;&#12290;&#36825;&#39033;&#30740;&#31350;&#22260;&#32469;&#20004;&#20010;&#20851;&#38190;&#30340;&#30740;&#31350;&#38382;&#39064;&#36827;&#34892;&#65306;&#22312;&#35757;&#32451;&#29790;&#22763;&#24503;&#35821;&#35328;&#32763;&#35793;&#27169;&#22411;&#26102;&#65292;&#26041;&#35328;&#30340;&#21253;&#21547;&#21644;&#25490;&#38500;&#22914;&#20309;&#24433;&#21709;&#29305;&#23450;&#26041;&#35328;&#30340;&#24615;&#33021;&#65292;&#20197;&#21450;&#29790;&#22763;&#24503;&#35821;&#21644;&#26631;&#20934;&#24503;&#35821;&#20043;&#38388;&#30340;&#24046;&#24322;&#22914;&#20309;&#24433;&#21709;&#31995;&#32479;&#30340;&#24615;&#33021;&#65311;&#25105;&#20204;&#34920;&#26126;&#26041;&#35328;&#22810;&#26679;&#24615;&#21644;&#35821;&#35328;&#24046;&#24322;&#32473;&#29790;&#22763;&#24503;&#35821;&#35328;&#32763;&#35793;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#36825;&#19982;&#32463;&#39564;&#30740;&#31350;&#24471;&#20986;&#30340;&#35821;&#35328;&#23398;&#20551;&#35774;&#30456;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the challenges in building Swiss German speech translation systems, specifically focusing on the impact of dialect diversity and differences between Swiss German and Standard German. Swiss German is a spoken language with no formal writing system, it comprises many diverse dialects and is a low-resource language with only around 5 million speakers. The study is guided by two key research questions: how does the inclusion and exclusion of dialects during the training of speech translation models for Swiss German impact the performance on specific dialects, and how do the differences between Swiss German and Standard German impact the performance of the systems? We show that dialect diversity and linguistic differences pose significant challenges to Swiss German speech translation, which is in line with linguistic hypotheses derived from empirical investigations.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KCTS&#30340;&#30693;&#35782;&#32422;&#26463;&#26641;&#25628;&#32034;&#35299;&#30721;&#26041;&#27861;&#65292;&#21033;&#29992;&#30693;&#35782;&#20998;&#31867;&#22120;&#21644;MCTS&#25351;&#23548;&#20923;&#32467;&#30340;LM&#29983;&#25104;&#19982;&#21442;&#32771;&#30693;&#35782;&#23545;&#40784;&#30340;&#25991;&#26412;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20196;&#29260;&#32423;&#24187;&#35273;&#26816;&#27979;&#26041;&#27861;RIPA&#12290;</title><link>http://arxiv.org/abs/2310.09044</link><description>&lt;p&gt;
KCTS&#65306;&#24102;&#26377;&#20196;&#29260;&#32423;&#24187;&#35273;&#26816;&#27979;&#30340;&#30693;&#35782;&#32422;&#26463;&#26641;&#25628;&#32034;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
KCTS: Knowledge-Constrained Tree Search Decoding with Token-Level Hallucination Detection. (arXiv:2310.09044v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09044
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KCTS&#30340;&#30693;&#35782;&#32422;&#26463;&#26641;&#25628;&#32034;&#35299;&#30721;&#26041;&#27861;&#65292;&#21033;&#29992;&#30693;&#35782;&#20998;&#31867;&#22120;&#21644;MCTS&#25351;&#23548;&#20923;&#32467;&#30340;LM&#29983;&#25104;&#19982;&#21442;&#32771;&#30693;&#35782;&#23545;&#40784;&#30340;&#25991;&#26412;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20196;&#29260;&#32423;&#24187;&#35273;&#26816;&#27979;&#26041;&#27861;RIPA&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#20154;&#31867;&#32423;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20135;&#29983;&#38169;&#35823;&#20449;&#24687;&#30340;&#28508;&#21147;&#65292;&#21363;&#25152;&#35859;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#23545;&#20854;&#37096;&#32626;&#26500;&#25104;&#37325;&#22823;&#39118;&#38505;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#24120;&#35265;&#26041;&#27861;&#26159;&#26816;&#32034;&#30456;&#20851;&#30693;&#35782;&#65292;&#24182;&#20351;&#29992;&#36755;&#20837;&#20013;&#30340;&#30693;&#35782;&#23545;LLM&#36827;&#34892;&#31934;&#32454;&#35843;&#33410;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#31181;&#26041;&#27861;&#20250;&#24341;&#36215;&#39640;&#35757;&#32451;&#25104;&#26412;&#65292;&#24182;&#21487;&#33021;&#23545;&#22810;&#20219;&#21153;&#27169;&#22411;&#36896;&#25104;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;KCTS&#65288;&#30693;&#35782;&#32422;&#26463;&#26641;&#25628;&#32034;&#65289;&#30340;&#30693;&#35782;&#32422;&#26463;&#35299;&#30721;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#30693;&#35782;&#20998;&#31867;&#22120;&#24471;&#20998;&#21644;MCTS&#65288;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#65289;&#26469;&#25351;&#23548;&#20923;&#32467;&#30340;LM&#22312;&#27599;&#20010;&#35299;&#30721;&#27493;&#39588;&#20013;&#29983;&#25104;&#19982;&#21442;&#32771;&#30693;&#35782;&#23545;&#40784;&#30340;&#25991;&#26412;&#12290;&#20026;&#20102;&#23558;&#24207;&#21015;&#32423;&#30693;&#35782;&#20998;&#31867;&#22120;&#36866;&#24212;&#20196;&#29260;&#32423;&#25351;&#23548;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20196;&#29260;&#32423;&#24187;&#35273;&#26816;&#27979;&#26041;&#27861;&#65292;&#31216;&#20026;RIPA&#65288;&#22870;&#21169;&#25296;&#28857;&#36817;&#20284;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable human-level natural language generation capabilities. However, their potential to generate misinformation, often called the hallucination problem, poses a significant risk to their deployment. A common approach to address this issue is to retrieve relevant knowledge and fine-tune the LLM with the knowledge in its input. Unfortunately, this method incurs high training costs and may cause catastrophic forgetting for multi-tasking models. To overcome these limitations, we propose a knowledge-constrained decoding method called KCTS (Knowledge-Constrained Tree Search), which guides a frozen LM to generate text aligned with the reference knowledge at each decoding step using a knowledge classifier score and MCTS (Monte-Carlo Tree Search). To adapt the sequence-level knowledge classifier to token-level guidance, we also propose a novel token-level hallucination detection method called RIPA (Reward Inflection Point Approximation). Our e
&lt;/p&gt;</description></item><item><title>MM-BigBench&#26159;&#19968;&#20010;&#35780;&#20272;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22810;&#27169;&#24577;&#20869;&#23481;&#29702;&#35299;&#20219;&#21153;&#19978;&#34920;&#29616;&#30340;&#32508;&#21512;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#27169;&#24577;&#20132;&#20114;&#26469;&#23454;&#29616;&#23545;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#30340;&#28145;&#20837;&#29702;&#35299;&#65292;&#24182;&#25552;&#20379;&#24191;&#27867;&#30340;&#24615;&#33021;&#35780;&#20272;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2310.09036</link><description>&lt;p&gt;
MM-BigBench: &#22312;&#22810;&#27169;&#24577;&#20869;&#23481;&#29702;&#35299;&#20219;&#21153;&#19978;&#35780;&#20272;&#22810;&#27169;&#24577;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MM-BigBench: Evaluating Multimodal Models on Multimodal Content Comprehension Tasks. (arXiv:2310.09036v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09036
&lt;/p&gt;
&lt;p&gt;
MM-BigBench&#26159;&#19968;&#20010;&#35780;&#20272;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22810;&#27169;&#24577;&#20869;&#23481;&#29702;&#35299;&#20219;&#21153;&#19978;&#34920;&#29616;&#30340;&#32508;&#21512;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#27169;&#24577;&#20132;&#20114;&#26469;&#23454;&#29616;&#23545;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#30340;&#28145;&#20837;&#29702;&#35299;&#65292;&#24182;&#25552;&#20379;&#24191;&#27867;&#30340;&#24615;&#33021;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#27969;&#34892;&#24341;&#21457;&#20102;&#36817;&#26399;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#35780;&#20272;&#30340;&#30740;&#31350;&#21162;&#21147;&#30340;&#28608;&#22686;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;MLLMs&#35780;&#20272;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#23545;&#21333;&#27169;&#24577;&#65288;&#35270;&#35273;&#65289;&#20869;&#23481;&#30340;&#29702;&#35299;&#21644;&#25512;&#29702;&#65292;&#24573;&#35270;&#20102;&#22312;&#22810;&#27169;&#24577;&#65288;&#35270;&#35273;-&#35821;&#35328;&#65289;&#20869;&#23481;&#29702;&#35299;&#39046;&#22495;&#30340;&#24615;&#33021;&#35780;&#20272;&#12290;&#38500;&#20102;&#22810;&#27169;&#24577;&#25512;&#29702;&#22806;&#65292;&#19982;&#22810;&#27169;&#24577;&#20869;&#23481;&#29702;&#35299;&#30456;&#20851;&#30340;&#20219;&#21153;&#38656;&#35201;&#23545;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#30340;&#28145;&#20837;&#29702;&#35299;&#65292;&#36890;&#36807;&#22810;&#27169;&#24577;&#20132;&#20114;&#26469;&#33719;&#24471;&#26368;&#32456;&#31572;&#26696;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#31216;&#20026;MM-BigBench&#65292;&#23427;&#21253;&#21547;&#20102;&#21508;&#31181;&#25351;&#26631;&#65292;&#20197;&#24191;&#27867;&#35780;&#20272;&#19981;&#21516;&#27169;&#22411;&#21644;&#25351;&#31034;&#22312;&#22810;&#26679;&#21270;&#30340;&#22810;&#27169;&#24577;&#20869;&#23481;&#29702;&#35299;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#34917;&#20805;&#20102;&#20851;&#20110;MLLMs&#22312;&#22810;&#27169;&#24577;&#29702;&#35299;&#20219;&#21153;&#20013;&#24615;&#33021;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
The popularity of multimodal large language models (MLLMs) has triggered a recent surge in research efforts dedicated to evaluating these models. Nevertheless, existing evaluation studies of MLLMs primarily focus on the comprehension and reasoning of unimodal (vision) content, neglecting performance evaluations in the domain of multimodal (vision-language) content understanding. Beyond multimodal reasoning, tasks related to multimodal content comprehension necessitate a profound understanding of multimodal contexts, achieved through the multimodal interaction to obtain a final answer. In this paper, we introduce a comprehensive assessment framework called MM-BigBench, which incorporates a diverse range of metrics to offer an extensive evaluation of the performance of various models and instructions across a wide spectrum of diverse multimodal content comprehension tasks. Consequently, our work complements research on the performance of MLLMs in multimodal comprehension tasks, achieving
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#21463;&#25511;&#25991;&#26412;&#32553;&#20943;&#65288;CTR&#65289;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20869;&#23481;&#20445;&#30041;&#32422;&#26463;&#19981;&#20805;&#20998;&#24378;&#21046;&#25191;&#34892;&#21644;&#27425;&#20248;&#30340;&#38134;&#26631;&#31614;&#35757;&#32451;&#25968;&#25454;&#30340;&#38480;&#21046;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#20013;&#22686;&#24378;&#20869;&#23481;&#20445;&#30041;&#32422;&#26463;&#65292;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.09017</link><description>&lt;p&gt;
&#19981;&#28155;&#21152;&#65292;&#19981;&#38169;&#36807;&#65306;&#20174;&#39044;&#36873;&#25991;&#26412;&#27573;&#29983;&#25104;&#26377;&#25928;&#30340;&#20869;&#23481;&#20445;&#30041;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Dont Add, dont Miss: Effective Content Preserving Generation from Pre-Selected Text Spans. (arXiv:2310.09017v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#21463;&#25511;&#25991;&#26412;&#32553;&#20943;&#65288;CTR&#65289;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20869;&#23481;&#20445;&#30041;&#32422;&#26463;&#19981;&#20805;&#20998;&#24378;&#21046;&#25191;&#34892;&#21644;&#27425;&#20248;&#30340;&#38134;&#26631;&#31614;&#35757;&#32451;&#25968;&#25454;&#30340;&#38480;&#21046;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#20013;&#22686;&#24378;&#20869;&#23481;&#20445;&#30041;&#32422;&#26463;&#65292;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#24341;&#20837;&#30340;&#21463;&#25511;&#25991;&#26412;&#32553;&#20943;&#65288;CTR&#65289;&#20219;&#21153;&#22312;&#20856;&#22411;&#30340;&#25688;&#35201;&#20219;&#21153;&#20013;&#23558;&#25991;&#26412;&#29983;&#25104;&#27493;&#39588;&#38548;&#31163;&#20986;&#26469;&#12290;&#23427;&#36890;&#36807;&#25361;&#25112;&#27169;&#22411;&#22312;&#36755;&#20837;&#25991;&#26412;&#30340;&#39044;&#36873;&#20869;&#23481;&#65288;"&#39640;&#20142;"&#65289;&#20013;&#29983;&#25104;&#36830;&#36143;&#30340;&#25991;&#26412;&#26469;&#23454;&#29616;&#12290;&#36825;&#31181;&#26694;&#26550;&#22312;&#31867;&#20284;&#25688;&#35201;&#30340;&#20219;&#21153;&#20013;&#22686;&#21152;&#20102;&#27169;&#22359;&#21270;&#33021;&#21147;&#65292;&#20801;&#35768;&#23558;&#21333;&#20010;CTR&#27169;&#22411;&#19982;&#21508;&#31181;&#20869;&#23481;&#36873;&#25321;&#35774;&#32622;&#21644;&#27169;&#22359;&#37197;&#23545;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#21487;&#38752;&#30340;CTR&#27169;&#22411;&#65292;&#32780;&#19988;&#29616;&#26377;&#20219;&#21153;&#22522;&#32447;&#30340;&#24615;&#33021;&#20013;&#31561;&#65292;&#26080;&#27861;&#23454;&#38469;&#20351;&#29992;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#24320;&#28304;CTR&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20004;&#20010;&#20808;&#21069;&#30340;&#20851;&#38190;&#38480;&#21046;&#65306;&#19981;&#20805;&#20998;&#24378;&#21046;&#25191;&#34892;&#20869;&#23481;&#20445;&#30041;&#32422;&#26463;&#21644;&#27425;&#20248;&#30340;&#38134;&#26631;&#31614;&#35757;&#32451;&#25968;&#25454;&#12290;&#36890;&#36807;&#22312;&#35757;&#32451;&#20013;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21644;&#25512;&#29702;&#20013;&#36890;&#36807;&#21463;&#25511;&#35299;&#30721;&#31574;&#30053;&#26469;&#22686;&#24378;&#20869;&#23481;&#20445;&#30041;&#32422;&#26463;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22823;&#24133;&#25913;&#36827;&#20102;&#38134;&#26631;&#31614;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recently introduced Controlled Text Reduction (CTR) task isolates the text generation step within typical summarization-style tasks. It does so by challenging models to generate coherent text conforming to pre-selected content within the input text ("highlights").  This framing enables increased modularity in summarization-like tasks, allowing to couple a single CTR model with various content-selection setups and modules.  However, there are currently no reliable CTR models, while the performance of the existing baseline for the task is mediocre, falling short of practical utility.  Here, we address this gap by introducing a high-quality, open-source CTR model that tackles two prior key limitations: inadequate enforcement of the content-preservation constraint, and suboptimal silver training data.  Addressing these, we amplify the content-preservation constraint in both training, via RL, and inference, via a controlled decoding strategy.  Further, we substantially improve the silve
&lt;/p&gt;</description></item><item><title>CodeChain&#26159;&#19968;&#31181;&#36890;&#36807;&#20195;&#34920;&#24615;&#23376;&#27169;&#22359;&#30340;&#33258;&#25105;&#20462;&#35746;&#38142;&#36335;&#24341;&#23548;&#27169;&#22359;&#21270;&#20195;&#30721;&#29983;&#25104;&#30340;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#22797;&#26434;&#32534;&#31243;&#20219;&#21153;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.08992</link><description>&lt;p&gt;
CodeChain: &#36890;&#36807;&#20195;&#34920;&#24615;&#23376;&#27169;&#22359;&#30340;&#33258;&#25105;&#20462;&#35746;&#38142;&#36335;&#23454;&#29616;&#27169;&#22359;&#21270;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
CodeChain: Towards Modular Code Generation Through Chain of Self-revisions with Representative Sub-modules. (arXiv:2310.08992v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08992
&lt;/p&gt;
&lt;p&gt;
CodeChain&#26159;&#19968;&#31181;&#36890;&#36807;&#20195;&#34920;&#24615;&#23376;&#27169;&#22359;&#30340;&#33258;&#25105;&#20462;&#35746;&#38142;&#36335;&#24341;&#23548;&#27169;&#22359;&#21270;&#20195;&#30721;&#29983;&#25104;&#30340;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#22797;&#26434;&#32534;&#31243;&#20219;&#21153;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#22312;&#35299;&#20915;&#31616;&#21333;&#32534;&#31243;&#20219;&#21153;&#26041;&#38754;&#38750;&#24120;&#29087;&#32451;&#65292;&#27604;&#22914;&#22312;HumanEval&#25110;MBPP&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26356;&#22797;&#26434;&#21644;&#20855;&#26377;&#31454;&#20105;&#24615;&#30340;&#32534;&#31243;&#20219;&#21153;&#65292;&#36825;&#20123;&#27169;&#22411;&#20173;&#28982;&#38754;&#20020;&#25361;&#25112;&#65292;&#21487;&#33021;&#26159;&#22240;&#20026;&#23427;&#20204;&#20542;&#21521;&#20110;&#29983;&#25104;&#20316;&#20026;&#25972;&#20307;&#20195;&#30721;&#22359;&#32780;&#19981;&#26159;&#23558;&#20854;&#20998;&#35299;&#20026;&#36923;&#36753;&#23376;&#20219;&#21153;&#21644;&#23376;&#27169;&#22359;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#26377;&#32463;&#39564;&#30340;&#31243;&#24207;&#21592;&#26412;&#33021;&#22320;&#32534;&#20889;&#20855;&#26377;&#25277;&#35937;&#27010;&#24565;&#30340;&#27169;&#22359;&#21270;&#20195;&#30721;&#26469;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#65292;&#36890;&#24120;&#20250;&#37325;&#22797;&#20351;&#29992;&#20043;&#21069;&#24320;&#21457;&#30340;&#27169;&#22359;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CodeChain&#65292;&#19968;&#31181;&#36890;&#36807;&#20195;&#34920;&#24615;&#23376;&#27169;&#22359;&#30340;&#33258;&#25105;&#20462;&#35746;&#38142;&#36335;&#24341;&#23548;&#27169;&#22359;&#21270;&#20195;&#30721;&#29983;&#25104;&#30340;&#26032;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;CodeChain&#39318;&#20808;&#36890;&#36807;&#38142;&#24335;&#24605;&#32771;&#25552;&#31034;&#25351;&#23548;LLM&#29983;&#25104;&#27169;&#22359;&#21270;&#20195;&#30721;&#12290;&#28982;&#21518;&#65292;&#23427;&#36890;&#36807;&#36845;&#20195;&#20004;&#20010;&#27493;&#39588;&#23454;&#26045;&#33258;&#25105;&#20462;&#35746;&#38142;&#36335;&#65306;1&#65289;&#39069;&#22806;...
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have already become quite proficient at solving simpler programming tasks like those in HumanEval or MBPP benchmarks. However, solving more complex and competitive programming tasks is still quite challenging for these models - possibly due to their tendency to generate solutions as monolithic code blocks instead of decomposing them into logical sub-tasks and sub-modules. On the other hand, experienced programmers instinctively write modularized code with abstraction for solving complex tasks, often reusing previously developed modules. To address this gap, we propose CodeChain, a novel framework for inference that elicits modularized code generation through a chain of self-revisions, each being guided by some representative sub-modules generated in previous iterations. Concretely, CodeChain first instructs the LLM to generate modularized codes through chain-of-thought prompting. Then it applies a chain of self-revisions by iterating the two steps: 1) extra
&lt;/p&gt;</description></item><item><title>ChatKBQA&#26159;&#19968;&#20010;&#22522;&#20110;&#31934;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;-&#26816;&#32034;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#36827;&#30693;&#35782;&#24211;&#38382;&#31572;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#22909;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.08975</link><description>&lt;p&gt;
ChatKBQA: &#19968;&#20010;&#22522;&#20110;&#31934;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;-&#26816;&#32034;&#26694;&#26550;&#29992;&#20110;&#30693;&#35782;&#24211;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
ChatKBQA: A Generate-then-Retrieve Framework for Knowledge Base Question Answering with Fine-tuned Large Language Models. (arXiv:2310.08975v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08975
&lt;/p&gt;
&lt;p&gt;
ChatKBQA&#26159;&#19968;&#20010;&#22522;&#20110;&#31934;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;-&#26816;&#32034;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#36827;&#30693;&#35782;&#24211;&#38382;&#31572;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#22909;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#24211;&#38382;&#31572;&#65288;KBQA&#65289;&#26088;&#22312;&#36890;&#36807;&#22823;&#35268;&#27169;&#30693;&#35782;&#24211;&#65288;KB&#65289;&#33719;&#21462;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#30340;&#31572;&#26696;&#65292;&#36890;&#24120;&#20998;&#20026;&#20004;&#20010;&#30740;&#31350;&#32452;&#25104;&#37096;&#20998;&#65306;&#30693;&#35782;&#26816;&#32034;&#21644;&#35821;&#20041;&#35299;&#26512;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#23384;&#22312;&#19977;&#20010;&#26680;&#24515;&#25361;&#25112;&#65292;&#21253;&#25324;&#20302;&#25928;&#30340;&#30693;&#35782;&#26816;&#32034;&#12289;&#26816;&#32034;&#38169;&#35823;&#23545;&#35821;&#20041;&#35299;&#26512;&#30340;&#19981;&#21033;&#24433;&#21709;&#20197;&#21450;&#20043;&#21069;&#30340;KBQA&#26041;&#27861;&#30340;&#22797;&#26434;&#24615;&#12290;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26102;&#20195;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ChatKBQA&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#31934;&#35843;&#24320;&#28304;LLMs&#65288;&#22914;Llama-2&#12289;ChatGLM2&#21644;Baichuan2&#65289;&#26500;&#24314;&#30340;&#29983;&#25104;-&#26816;&#32034;KBQA&#26694;&#26550;&#12290;ChatKBQA&#25552;&#35758;&#39318;&#20808;&#20351;&#29992;&#31934;&#35843;&#30340;LLMs&#29983;&#25104;&#36923;&#36753;&#24418;&#24335;&#65292;&#28982;&#21518;&#36890;&#36807;&#26080;&#30417;&#30563;&#26816;&#32034;&#26041;&#27861;&#26816;&#32034;&#21644;&#26367;&#25442;&#23454;&#20307;&#21644;&#20851;&#31995;&#65292;&#20174;&#32780;&#26356;&#30452;&#35266;&#22320;&#25913;&#36827;&#20102;&#29983;&#25104;&#21644;&#26816;&#32034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ChatKBQA&#22312;&#26631;&#20934;KBQA&#25968;&#25454;&#38598;WebQSP&#21644;ComplexWebQuestions (CWQ)&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Base Question Answering (KBQA) aims to derive answers to natural language questions over large-scale knowledge bases (KBs), which are generally divided into two research components: knowledge retrieval and semantic parsing. However, three core challenges remain, including inefficient knowledge retrieval, retrieval errors adversely affecting semantic parsing, and the complexity of previous KBQA methods. In the era of large language models (LLMs), we introduce ChatKBQA, a novel generate-then-retrieve KBQA framework built on fine-tuning open-source LLMs such as Llama-2, ChatGLM2 and Baichuan2. ChatKBQA proposes generating the logical form with fine-tuned LLMs first, then retrieving and replacing entities and relations through an unsupervised retrieval method, which improves both generation and retrieval more straightforwardly. Experimental results reveal that ChatKBQA achieves new state-of-the-art performance on standard KBQA datasets, WebQSP, and ComplexWebQuestions (CWQ). This
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#26426;&#22120;&#32763;&#35793;&#30340;&#36879;&#26126;&#24230;&#29305;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26550;&#26500;&#65292;&#26088;&#22312;&#22686;&#21152;&#29992;&#25143;&#23545;&#32763;&#35793;&#20915;&#31574;&#30340;&#36879;&#26126;&#24230;&#65292;&#24182;&#36890;&#36807;&#32534;&#36753;&#22810;&#20010;&#31034;&#20363;&#26469;&#25552;&#39640;&#32763;&#35793;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.08967</link><description>&lt;p&gt;
&#36816;&#29992;&#22810;Levenshtein Transformer&#36827;&#34892;&#22522;&#20110;&#31034;&#20363;&#30340;NMT&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Example-Based NMT with Multi-Levenshtein Transformers. (arXiv:2310.08967v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08967
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#26426;&#22120;&#32763;&#35793;&#30340;&#36879;&#26126;&#24230;&#29305;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26550;&#26500;&#65292;&#26088;&#22312;&#22686;&#21152;&#29992;&#25143;&#23545;&#32763;&#35793;&#20915;&#31574;&#30340;&#36879;&#26126;&#24230;&#65292;&#24182;&#36890;&#36807;&#32534;&#36753;&#22810;&#20010;&#31034;&#20363;&#26469;&#25552;&#39640;&#32763;&#35793;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#30340;&#26426;&#22120;&#32763;&#35793;&#65288;RAMT&#65289;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#36825;&#26159;&#22240;&#20026;RAMT&#19981;&#20165;&#21487;&#20197;&#25552;&#39640;&#32763;&#35793;&#24230;&#37327;&#65292;&#32780;&#19988;&#36824;&#34987;&#35748;&#20026;&#21487;&#20197;&#23454;&#29616;&#26576;&#31181;&#24418;&#24335;&#30340;&#39046;&#22495;&#36866;&#24212;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;RAMT&#30340;&#21478;&#19968;&#20010;&#26174;&#33879;&#29305;&#28857;&#65292;&#21363;&#20801;&#35768;&#29992;&#25143;&#36820;&#22238;&#23545;&#32763;&#35793;&#20915;&#31574;&#26377;&#36129;&#29486;&#30340;&#31034;&#20363;&#65292;&#20174;&#32780;&#20351;&#32763;&#35793;&#20915;&#31574;&#26356;&#21152;&#36879;&#26126;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#26088;&#22312;&#22686;&#21152;&#36825;&#31181;&#36879;&#26126;&#24230;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#20102;&#19968;&#20010;&#26816;&#32034;&#22686;&#24378;&#30340;Levenshtein Transformer&#30340;&#29256;&#26412;&#65292;&#24182;&#20351;&#20854;&#33021;&#22815;&#21516;&#26102;&#32534;&#36753;&#20869;&#23384;&#20013;&#25214;&#21040;&#30340;&#22810;&#20010;&#27169;&#31946;&#21305;&#37197;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22914;&#20309;&#22312;&#35813;&#27169;&#22411;&#20013;&#36827;&#34892;&#35757;&#32451;&#21644;&#25512;&#29702;&#65292;&#22522;&#20110;&#22810;&#36335;&#24452;&#23545;&#40784;&#31639;&#27861;&#21644;&#27169;&#20223;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#32534;&#36753;&#22810;&#20010;&#31034;&#20363;&#23545;&#32763;&#35793;&#24471;&#20998;&#26377;&#31215;&#26497;&#30340;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#22686;&#21152;&#20102;&#20174;&#29616;&#26377;&#23454;&#20363;&#20013;&#22797;&#21046;&#30340;&#30446;&#26631;&#36328;&#24230;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrieval-Augmented Machine Translation (RAMT) is attracting growing attention. This is because RAMT not only improves translation metrics, but is also assumed to implement some form of domain adaptation. In this contribution, we study another salient trait of RAMT, its ability to make translation decisions more transparent by allowing users to go back to examples that contributed to these decisions.  For this, we propose a novel architecture aiming to increase this transparency. This model adapts a retrieval-augmented version of the Levenshtein Transformer and makes it amenable to simultaneously edit multiple fuzzy matches found in memory. We discuss how to perform training and inference in this model, based on multi-way alignment algorithms and imitation learning. Our experiments show that editing several examples positively impacts translation scores, notably increasing the number of target spans that are copied from existing instances.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;xDial-Eval&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#24320;&#28304;&#33521;&#35821;&#23545;&#35805;&#35780;&#20272;&#25968;&#25454;&#38598;&#26500;&#24314;&#30340;&#22810;&#35821;&#35328;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#35780;&#20272;&#22522;&#20934;&#12290;&#36890;&#36807;&#23545;12&#20010;&#36718;&#27425;&#32423;&#21035;&#21644;6&#20010;&#23545;&#35805;&#32423;&#21035;&#30340;&#33521;&#35821;&#25968;&#25454;&#38598;&#36827;&#34892;&#25193;&#23637;&#65292;&#23454;&#29616;&#20102;&#23545;&#20854;&#20182;&#20061;&#31181;&#35821;&#35328;&#30340;&#23545;&#35805;&#35780;&#20272;&#12290;&#36890;&#36807;&#23545;&#20808;&#21069;&#30340;&#22522;&#20110;BERT&#30340;&#24230;&#37327;&#21644;&#26368;&#36817;&#20986;&#29616;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#38754;&#20998;&#26512;&#65292;&#24314;&#31435;&#20102;&#24378;&#22823;&#30340;&#35780;&#20272;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2310.08958</link><description>&lt;p&gt;
xDial-Eval: &#19968;&#31181;&#22810;&#35821;&#35328;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
xDial-Eval: A Multilingual Open-Domain Dialogue Evaluation Benchmark. (arXiv:2310.08958v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08958
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;xDial-Eval&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#24320;&#28304;&#33521;&#35821;&#23545;&#35805;&#35780;&#20272;&#25968;&#25454;&#38598;&#26500;&#24314;&#30340;&#22810;&#35821;&#35328;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#35780;&#20272;&#22522;&#20934;&#12290;&#36890;&#36807;&#23545;12&#20010;&#36718;&#27425;&#32423;&#21035;&#21644;6&#20010;&#23545;&#35805;&#32423;&#21035;&#30340;&#33521;&#35821;&#25968;&#25454;&#38598;&#36827;&#34892;&#25193;&#23637;&#65292;&#23454;&#29616;&#20102;&#23545;&#20854;&#20182;&#20061;&#31181;&#35821;&#35328;&#30340;&#23545;&#35805;&#35780;&#20272;&#12290;&#36890;&#36807;&#23545;&#20808;&#21069;&#30340;&#22522;&#20110;BERT&#30340;&#24230;&#37327;&#21644;&#26368;&#36817;&#20986;&#29616;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#38754;&#20998;&#26512;&#65292;&#24314;&#31435;&#20102;&#24378;&#22823;&#30340;&#35780;&#20272;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#26080;&#21442;&#32771;&#23398;&#20064;&#24230;&#37327;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#35780;&#20272;&#26041;&#38754;&#21462;&#24471;&#30340;&#36827;&#23637;&#65292;&#24471;&#30410;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#27493;&#21644;&#20855;&#26377;&#39640;&#36136;&#37327;&#20154;&#24037;&#27880;&#37322;&#30340;&#23545;&#35805;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#33521;&#35821;&#23545;&#35805;&#19978;&#65292;&#36825;&#20123;&#24230;&#37327;&#30340;&#27867;&#21270;&#21040;&#20854;&#20182;&#35821;&#35328;&#23578;&#26410;&#20805;&#20998;&#32771;&#34385;&#12290;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;&#32570;&#20047;&#22810;&#35821;&#35328;&#23545;&#35805;&#35780;&#20272;&#22522;&#20934;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;xDial-Eval&#65292;&#23427;&#26159;&#22522;&#20110;&#24320;&#28304;&#33521;&#35821;&#23545;&#35805;&#35780;&#20272;&#25968;&#25454;&#38598;&#26500;&#24314;&#30340;&#12290;xDial-Eval&#21253;&#25324;12&#20010;&#36718;&#27425;&#32423;&#21035;&#21644;6&#20010;&#23545;&#35805;&#32423;&#21035;&#30340;&#33521;&#35821;&#25968;&#25454;&#38598;&#65292;&#20998;&#21035;&#21253;&#21547;14930&#20010;&#27880;&#37322;&#36718;&#27425;&#21644;8691&#20010;&#27880;&#37322;&#23545;&#35805;&#12290;&#33521;&#35821;&#23545;&#35805;&#25968;&#25454;&#36824;&#36890;&#36807;&#21830;&#19994;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#25193;&#23637;&#21040;&#20854;&#20182;&#20061;&#31181;&#35821;&#35328;&#12290;&#22312;xDial-Eval&#19978;&#65292;&#25105;&#20204;&#23545;&#20808;&#21069;&#30340;&#22522;&#20110;BERT&#30340;&#24230;&#37327;&#21644;&#26368;&#36817;&#20986;&#29616;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#20998;&#26512;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#24378;&#22823;&#30340;&#35780;&#20272;&#22522;&#20934;&#65292;&#20026;&#22810;&#35821;&#35328;&#23545;&#35805;&#35780;&#20272;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in reference-free learned metrics for open-domain dialogue evaluation have been driven by the progress in pre-trained language models and the availability of dialogue data with high-quality human annotations. However, current studies predominantly concentrate on English dialogues, and the generalization of these metrics to other languages has not been fully examined. This is largely due to the absence of a multilingual dialogue evaluation benchmark. To address the issue, we introduce xDial-Eval, built on top of open-source English dialogue evaluation datasets. xDial-Eval includes 12 turn-level and 6 dialogue-level English datasets, comprising 14930 annotated turns and 8691 annotated dialogues respectively. The English dialogue data are extended to nine other languages with commercial machine translation systems. On xDial-Eval, we conduct comprehensive analyses of previous BERT-based metrics and the recently-emerged large language models. Lastly, we establish strong 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;ICALEPCS&#21644;IPAC&#20250;&#35758;&#35770;&#25991;&#36827;&#34892;&#25991;&#26412;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#30740;&#31350;&#36235;&#21183;&#12289;&#20027;&#39064;&#21644;&#21512;&#20316;&#20851;&#31995;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#27934;&#35265;&#21644;&#39640;&#32423;&#25628;&#32034;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2310.08954</link><description>&lt;p&gt;
ICALEPCS&#21644;IPAC&#20250;&#35758;&#35770;&#25991;&#30340;&#25991;&#26412;&#20998;&#26512;: &#25581;&#31034;&#26410;&#26469;&#27934;&#35265;&#21644;&#39640;&#32423;&#25628;&#32034;&#30340;&#30740;&#31350;&#36235;&#21183;&#12289;&#20027;&#39064;&#21644;&#21512;&#20316;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Textual Analysis of ICALEPCS and IPAC Conference Proceedings: Revealing Research Trends, Topics, and Collaborations for Future Insights and Advanced Search. (arXiv:2310.08954v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08954
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;ICALEPCS&#21644;IPAC&#20250;&#35758;&#35770;&#25991;&#36827;&#34892;&#25991;&#26412;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#30740;&#31350;&#36235;&#21183;&#12289;&#20027;&#39064;&#21644;&#21512;&#20316;&#20851;&#31995;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#27934;&#35265;&#21644;&#39640;&#32423;&#25628;&#32034;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#36807;&#21435;ICALEPCS&#21644;IPAC&#20250;&#35758;&#35770;&#25991;&#30340;&#25991;&#26412;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#35813;&#39046;&#22495;&#30740;&#31350;&#36235;&#21183;&#21644;&#35752;&#35770;&#20027;&#39064;&#30340;&#27934;&#35265;&#12290;&#25105;&#20204;&#36816;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#20174;&#25688;&#35201;&#21644;&#35770;&#25991;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#20449;&#24687;&#65292;&#24182;&#25552;&#21462;&#20027;&#39064;&#36827;&#34892;&#21487;&#35270;&#21270;&#21644;&#36235;&#21183;&#35782;&#21035;&#65292;&#20998;&#26512;&#20854;&#28436;&#21464;&#20197;&#30830;&#23450;&#26032;&#20852;&#30740;&#31350;&#26041;&#21521;&#65292;&#24182;&#22522;&#20110;&#20869;&#23481;&#20998;&#26512;&#32593;&#32476;&#26469;&#31361;&#20986;&#26377;&#36259;&#30340;&#20986;&#29256;&#29289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;&#25552;&#20379;&#19968;&#20010;&#20808;&#36827;&#30340;&#25628;&#32034;&#24037;&#20855;&#65292;&#20197;&#26356;&#22909;&#22320;&#25628;&#32034;&#29616;&#26377;&#35770;&#25991;&#65292;&#36991;&#20813;&#37325;&#22797;&#24182;&#20415;&#20110;&#21442;&#32771;&#21457;&#29616;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#35813;&#39046;&#22495;&#30740;&#31350;&#24773;&#26223;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#26377;&#21161;&#20110;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#26356;&#22909;&#22320;&#20102;&#35299;&#26368;&#26032;&#25216;&#26415;&#24182;&#30830;&#23450;&#26410;&#26469;&#30740;&#31350;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we show a textual analysis of past ICALEPCS and IPAC conference proceedings to gain insights into the research trends and topics discussed in the field. We use natural language processing techniques to extract meaningful information from the abstracts and papers of past conference proceedings. We extract topics to visualize and identify trends, analyze their evolution to identify emerging research directions, and highlight interesting publications based solely on their content with an analysis of their network. Additionally, we will provide an advanced search tool to better search the existing papers to prevent duplication and easier reference findings. Our analysis provides a comprehensive overview of the research landscape in the field and helps researchers and practitioners to better understand the state-of-the-art and identify areas for future research.
&lt;/p&gt;</description></item><item><title>EasyGen&#26159;&#19968;&#20010;&#26377;&#25928;&#30340;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#32467;&#21512;&#25193;&#25955;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#26356;&#23481;&#26131;&#30340;&#22810;&#27169;&#24577;&#29983;&#25104;&#12290;&#30456;&#27604;&#29616;&#26377;&#30340;&#27169;&#22411;&#65292;EasyGen&#20351;&#29992;&#20102;&#19968;&#20010;&#21517;&#20026;BiDiffuser&#30340;&#21452;&#21521;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292; &#25552;&#20379;&#20102;&#26356;&#39640;&#25928;&#30340;&#27169;&#24577;&#20132;&#20114;&#65292;&#24182;&#19988;&#19981;&#20165;&#33021;&#22815;&#29983;&#25104;&#25991;&#26412;&#22238;&#22797;&#65292;&#36824;&#33021;&#22815;&#20419;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2310.08949</link><description>&lt;p&gt;
Easier Multimodal Generation: Diffusion Models Meet LLMs
&lt;/p&gt;
&lt;p&gt;
Making Multimodal Generation Easier: When Diffusion Models Meet LLMs. (arXiv:2310.08949v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08949
&lt;/p&gt;
&lt;p&gt;
EasyGen&#26159;&#19968;&#20010;&#26377;&#25928;&#30340;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#32467;&#21512;&#25193;&#25955;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#26356;&#23481;&#26131;&#30340;&#22810;&#27169;&#24577;&#29983;&#25104;&#12290;&#30456;&#27604;&#29616;&#26377;&#30340;&#27169;&#22411;&#65292;EasyGen&#20351;&#29992;&#20102;&#19968;&#20010;&#21517;&#20026;BiDiffuser&#30340;&#21452;&#21521;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292; &#25552;&#20379;&#20102;&#26356;&#39640;&#25928;&#30340;&#27169;&#24577;&#20132;&#20114;&#65292;&#24182;&#19988;&#19981;&#20165;&#33021;&#22815;&#29983;&#25104;&#25991;&#26412;&#22238;&#22797;&#65292;&#36824;&#33021;&#22815;&#20419;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;EasyGen&#65292;&#19968;&#20010;&#26377;&#25928;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#65292;&#22686;&#24378;&#20102;&#22810;&#27169;&#24577;&#29702;&#35299;&#21644;&#29983;&#25104;&#12290;&#19981;&#21516;&#20110;&#29616;&#26377;&#30340;&#20027;&#35201;&#20381;&#36182;&#20110;&#32534;&#30721;&#22120;&#22914;CLIP&#25110;ImageBind&#65292;&#24182;&#19988;&#38656;&#35201;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#26469;&#26725;&#25509;&#27169;&#24577;&#20043;&#38388;&#24046;&#36317;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;EasyGen&#22522;&#20110;&#19968;&#20010;&#21517;&#20026;BiDiffuser&#30340;&#21452;&#21521;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#26500;&#24314;&#65292;&#20419;&#36827;&#20102;&#26356;&#39640;&#25928;&#30340;&#27169;&#24577;&#20132;&#20114;&#12290;EasyGen&#36890;&#36807;&#31616;&#21333;&#30340;&#25237;&#24433;&#23618;&#23558;BiDiffuser&#21644;LLM&#36827;&#34892;&#38598;&#25104;&#65292;&#22788;&#29702;&#22270;&#20687;&#21040;&#25991;&#26412;&#30340;&#29983;&#25104;&#12290;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#38480;&#20110;&#29983;&#25104;&#25991;&#26412;&#22238;&#22797;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#19981;&#21516;&#65292;EasyGen&#36824;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;LLM&#21019;&#24314;&#25991;&#26412;&#25551;&#36848;&#65292;&#24182;&#30001;BiDiffuser&#35299;&#37322;&#29983;&#25104;&#36866;&#24403;&#30340;&#35270;&#35273;&#22238;&#22797;&#26469;&#20419;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#29983;&#25104;&#12290;&#24191;&#27867;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#23454;&#39564;&#35777;&#26126;&#20102;EasyGen&#30340;&#26377;&#25928;&#24615;&#65292;&#20854;&#35757;&#32451;&#21487;&#20197;...
&lt;/p&gt;
&lt;p&gt;
We present EasyGen, an efficient model designed to enhance multimodal understanding and generation by harnessing the capabilities of diffusion models and large language models (LLMs). Unlike existing multimodal models that predominately depend on encoders like CLIP or ImageBind and need ample amounts of training data to bridge the gap between modalities, EasyGen is built upon a bidirectional conditional diffusion model named BiDiffuser, which promotes more efficient interactions between modalities. EasyGen handles image-to-text generation by integrating BiDiffuser and an LLM via a simple projection layer. Unlike most existing multimodal models that are limited to generating text responses, EasyGen can also facilitate text-to-image generation by leveraging the LLM to create textual descriptions, which can be interpreted by BiDiffuser to generate appropriate visual responses. Extensive quantitative and qualitative experiments demonstrate the effectiveness of EasyGen, whose training can b
&lt;/p&gt;</description></item><item><title>CAMELL&#26159;&#19968;&#20010;&#36866;&#29992;&#20110;&#24207;&#21015;&#22810;&#36755;&#20986;&#38382;&#39064;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20165;&#38656;&#19987;&#23478;&#26631;&#27880;&#24207;&#21015;&#30340;&#19968;&#23567;&#37096;&#20998;&#12289;&#33258;&#30417;&#30563;&#21644;&#26631;&#31614;&#39564;&#35777;&#26426;&#21046;&#26469;&#35299;&#20915;&#30417;&#30563;&#31070;&#32463;&#26041;&#27861;&#23545;&#22823;&#35268;&#27169;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#20381;&#36182;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.08944</link><description>&lt;p&gt;
CAMELL&#65306;&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#39640;&#25928;&#33258;&#30417;&#30563;&#20027;&#21160;&#23398;&#20064;&#19982;&#26631;&#31614;&#39564;&#35777;&#33719;&#21462;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CAMELL: Confidence-based Acquisition Model for Efficient Self-supervised Active Learning with Label Validation. (arXiv:2310.08944v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08944
&lt;/p&gt;
&lt;p&gt;
CAMELL&#26159;&#19968;&#20010;&#36866;&#29992;&#20110;&#24207;&#21015;&#22810;&#36755;&#20986;&#38382;&#39064;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20165;&#38656;&#19987;&#23478;&#26631;&#27880;&#24207;&#21015;&#30340;&#19968;&#23567;&#37096;&#20998;&#12289;&#33258;&#30417;&#30563;&#21644;&#26631;&#31614;&#39564;&#35777;&#26426;&#21046;&#26469;&#35299;&#20915;&#30417;&#30563;&#31070;&#32463;&#26041;&#27861;&#23545;&#22823;&#35268;&#27169;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#20381;&#36182;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24207;&#21015;&#20219;&#21153;&#20013;&#65292;&#21463;&#22823;&#35268;&#27169;&#19988;&#31934;&#30830;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#20381;&#36182;&#38480;&#21046;&#65292;&#30417;&#30563;&#31070;&#32463;&#26041;&#27861;&#21463;&#21040;&#38459;&#30861;&#12290;&#26631;&#27880;&#36136;&#37327;&#38543;&#30528;&#20174;&#19987;&#23478;&#26631;&#27880;&#21521;&#20247;&#21253;&#26631;&#27880;&#30340;&#36716;&#21464;&#32780;&#36880;&#28176;&#24694;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CAMELL&#65288;Confidence-based Acquisition Model for Efficient self-supervised active Learning with Label validation&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#24207;&#21015;&#22810;&#36755;&#20986;&#38382;&#39064;&#37327;&#36523;&#23450;&#21046;&#30340;&#22522;&#20110;&#27744;&#21270;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#12290;CAMELL&#20855;&#26377;&#19977;&#20010;&#26680;&#24515;&#29305;&#28857;&#65306;(1)&#20165;&#35201;&#27714;&#19987;&#23478;&#26631;&#27880;&#25152;&#36873;&#24207;&#21015;&#30340;&#19968;&#23567;&#37096;&#20998;&#65292;(2)&#20026;&#20854;&#20313;&#24207;&#21015;&#25552;&#20379;&#33258;&#30417;&#30563;&#65292;(3)&#37319;&#29992;&#26631;&#31614;&#39564;&#35777;&#26426;&#21046;&#65292;&#38450;&#27490;&#38169;&#35823;&#26631;&#31614;&#27745;&#26579;&#25968;&#25454;&#38598;&#24182;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#24207;&#21015;&#20219;&#21153;&#20013;&#23545;CAMELL&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#29305;&#21035;&#24378;&#35843;&#23545;&#35805;&#20449;&#24565;&#36319;&#36394;&#65292;&#36825;&#26159;&#19968;&#20010;&#21463;&#38480;&#21046;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised neural approaches are hindered by their dependence on large, meticulously annotated datasets, a requirement that is particularly cumbersome for sequential tasks. The quality of annotations tends to deteriorate with the transition from expert-based to crowd-sourced labelling. To address these challenges, we present \textbf{CAMELL} (Confidence-based Acquisition Model for Efficient self-supervised active Learning with Label validation), a pool-based active learning framework tailored for sequential multi-output problems. CAMELL possesses three core features: (1) it requires expert annotators to label only a fraction of a chosen sequence, (2) it facilitates self-supervision for the remainder of the sequence, and (3) it employs a label validation mechanism to prevent erroneous labels from contaminating the dataset and harming model performance. We evaluate CAMELL on sequential tasks, with a special emphasis on dialogue belief tracking, a task plagued by the constraints of limited
&lt;/p&gt;</description></item><item><title>&#36825;&#26159;&#19968;&#31687;&#20851;&#20110;&#30693;&#35782;&#24341;&#23548;&#23545;&#35805;&#29983;&#25104;&#30340;&#35770;&#25991;&#65292;&#36890;&#36807;&#24341;&#20837;&#22810;&#23618;&#33258;&#36866;&#24212;&#23545;&#27604;&#23398;&#20064;&#65288;MACL&#65289;&#26694;&#26550;&#65292;&#24182;&#22312;&#20196;&#29260;&#32423;&#21644;&#24207;&#21015;&#32423;&#19978;&#21160;&#24577;&#37319;&#26679;&#36127;&#20363;&#26469;&#35299;&#20915;&#27169;&#22411;&#31616;&#21333;&#25554;&#20837;&#30693;&#35782;&#29255;&#27573;&#23548;&#33268;&#30340;&#36864;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.08943</link><description>&lt;p&gt;
&#22810;&#23618;&#33258;&#36866;&#24212;&#23545;&#27604;&#23398;&#20064;&#22312;&#23545;&#35805;&#29983;&#25104;&#20013;&#30340;&#30693;&#35782;&#20869;&#21270;
&lt;/p&gt;
&lt;p&gt;
Multi-level Adaptive Contrastive Learning for Knowledge Internalization in Dialogue Generation. (arXiv:2310.08943v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08943
&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#19968;&#31687;&#20851;&#20110;&#30693;&#35782;&#24341;&#23548;&#23545;&#35805;&#29983;&#25104;&#30340;&#35770;&#25991;&#65292;&#36890;&#36807;&#24341;&#20837;&#22810;&#23618;&#33258;&#36866;&#24212;&#23545;&#27604;&#23398;&#20064;&#65288;MACL&#65289;&#26694;&#26550;&#65292;&#24182;&#22312;&#20196;&#29260;&#32423;&#21644;&#24207;&#21015;&#32423;&#19978;&#21160;&#24577;&#37319;&#26679;&#36127;&#20363;&#26469;&#35299;&#20915;&#27169;&#22411;&#31616;&#21333;&#25554;&#20837;&#30693;&#35782;&#29255;&#27573;&#23548;&#33268;&#30340;&#36864;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#24341;&#23548;&#30340;&#23545;&#35805;&#29983;&#25104;&#26088;&#22312;&#36890;&#36807;&#25972;&#21512;&#22806;&#37096;&#30693;&#35782;&#26469;&#34917;&#20805;&#19978;&#19979;&#25991;&#65292;&#20174;&#32780;&#32531;&#35299;&#25991;&#26412;&#36864;&#21270;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#24448;&#24448;&#26080;&#27861;&#20197;&#31867;&#20284;&#20154;&#31867;&#30340;&#26041;&#24335;&#23558;&#27492;&#20449;&#24687;&#20869;&#21270;&#21040;&#22238;&#31572;&#20013;&#12290;&#30456;&#21453;&#65292;&#23427;&#21482;&#26159;&#31616;&#21333;&#22320;&#23558;&#25552;&#20379;&#30340;&#30693;&#35782;&#29255;&#27573;&#25554;&#20837;&#21040;&#26222;&#36890;&#30340;&#22238;&#31572;&#20013;&#12290;&#22240;&#27492;&#65292;&#29983;&#25104;&#30340;&#22238;&#31572;&#24448;&#24448;&#20047;&#21619;&#12289;&#19981;&#36830;&#36143;&#65292;&#24182;&#19988;&#32570;&#20047;&#20114;&#21160;&#24615;&#65292;&#36825;&#24847;&#21619;&#30528;&#36864;&#21270;&#38382;&#39064;&#20173;&#26410;&#35299;&#20915;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#21457;&#29616;&#65292;&#36825;&#31181;&#22797;&#21046;&#24335;&#36864;&#21270;&#20027;&#35201;&#26159;&#30001;&#20110;&#24369;&#27010;&#29575;&#30446;&#26631;&#36896;&#25104;&#30340;&#65292;&#23427;&#20801;&#35768;&#27169;&#22411;&#36890;&#36807;&#20165;&#22522;&#20110;&#37325;&#21472;&#30340;&#34920;&#38754;&#27169;&#24335;&#21305;&#37197;&#26469;&#8220;&#27450;&#39575;&#8221;&#30446;&#26631;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#23618;&#33258;&#36866;&#24212;&#23545;&#27604;&#23398;&#20064;&#65288;MACL&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22312;&#20196;&#29260;&#32423;&#21644;&#24207;&#21015;&#32423;&#19978;&#21160;&#24577;&#37319;&#26679;&#36127;&#20363;&#65292;&#24182;&#38543;&#21518;&#24809;&#32602;&#36864;&#21270;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge-grounded dialogue generation aims to mitigate the issue of text degeneration by incorporating external knowledge to supplement the context. However, the model often fails to internalize this information into responses in a human-like manner. Instead, it simply inserts segments of the provided knowledge into generic responses. As a result, the generated responses tend to be tedious, incoherent, and in lack of interactivity which means the degeneration problem is still unsolved. In this work, we first find that such copying-style degeneration is primarily due to the weak likelihood objective, which allows the model to "cheat" the objective by merely duplicating knowledge segments in a superficial pattern matching based on overlap. To overcome this challenge, we then propose a Multi-level Adaptive Contrastive Learning (MACL) framework that dynamically samples negative examples and subsequently penalizes degeneration behaviors at both the token-level and sequence-level. Extensive
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#37327;&#21270;&#20449;&#24687;&#22686;&#30410;&#26469;&#25506;&#32034;&#25968;&#25454;&#31034;&#20363;&#30340;&#20449;&#24687;&#33021;&#21147;&#65292;&#24182;&#37319;&#29992;&#26657;&#20934;&#31574;&#30053;&#26469;&#32531;&#35299;&#27169;&#26495;&#20559;&#24046;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08923</link><description>&lt;p&gt;
&#26397;&#30528;&#26368;&#22823;&#20449;&#24687;&#22686;&#30410;&#30340;&#20449;&#24687;&#20016;&#23500;&#30340;&#23569;&#26679;&#26412;&#25552;&#31034;&#23454;&#29616;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Informative Few-Shot Prompt with Maximum Information Gain for In-Context Learning. (arXiv:2310.08923v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#37327;&#21270;&#20449;&#24687;&#22686;&#30410;&#26469;&#25506;&#32034;&#25968;&#25454;&#31034;&#20363;&#30340;&#20449;&#24687;&#33021;&#21147;&#65292;&#24182;&#37319;&#29992;&#26657;&#20934;&#31574;&#30053;&#26469;&#32531;&#35299;&#27169;&#26495;&#20559;&#24046;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#21033;&#29992;&#23569;&#37327;&#19982;&#26032;&#30340;&#19979;&#28216;&#20219;&#21153;&#26377;&#20851;&#30340;&#31034;&#20363;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#23398;&#20064;&#33539;&#24335;&#39281;&#21463;&#36755;&#20837;&#20998;&#24067;&#12289;&#39034;&#24207;&#21644;&#25552;&#31034;&#26684;&#24335;&#31561;&#22240;&#32032;&#24341;&#36215;&#30340;&#39640;&#24230;&#19981;&#31283;&#23450;&#24615;&#30340;&#22256;&#25200;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#21363;&#20351;&#24403;&#25152;&#26377;&#36825;&#20123;&#22240;&#32032;&#20445;&#25345;&#19981;&#21464;&#26102;&#65292;&#38543;&#26426;&#36873;&#25321;&#31034;&#20363;&#20173;&#28982;&#20250;&#23548;&#33268;&#39640;&#26041;&#24046;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#37327;&#21270;&#35266;&#23519;&#32473;&#23450;&#31034;&#20363;&#20505;&#36873;&#20154;&#21518;&#39044;&#27979;&#25152;&#33719;&#24471;&#30340;&#20449;&#24687;&#22686;&#30410;&#65288;IG&#65289;&#26469;&#25506;&#32034;&#25968;&#25454;&#31034;&#20363;&#30340;&#20449;&#24687;&#33021;&#21147;&#65292;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20174;&#20013;&#37319;&#26679;&#37027;&#20123;&#20855;&#26377;&#26368;&#22823;IG&#30340;&#31034;&#20363;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#26495;&#20559;&#24046;&#30340;&#23384;&#22312;&#65292;&#22312;&#37319;&#26679;&#36807;&#31243;&#20013;&#21487;&#33021;&#20250;&#23548;&#33268;&#23545;IG&#30340;&#19981;&#20844;&#24179;&#35780;&#20272;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#31181;&#20559;&#24046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#37319;&#26679;&#20043;&#21069;&#30340;&#26657;&#20934;&#31574;&#30053;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language models (LLMs) possess the capability to engage In-context Learning (ICL) by leveraging a few demonstrations pertaining to a new downstream task as conditions. However, this particular learning paradigm suffers from high instability stemming from substantial variances induced by factors such as the input distribution of selected examples, their ordering, and prompt formats. In this work, we demonstrate that even when all these factors are held constant, the random selection of examples still results in high variance. Consequently, we aim to explore the informative ability of data examples by quantifying the Information Gain (IG) obtained in prediction after observing a given example candidate. Then we propose to sample those with maximum IG. Additionally, we identify the presence of template bias, which can lead to unfair evaluations of IG during the sampling process. To mitigate this bias, we introduce Calibration Before Sampling strategy. The experimental results illust
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#31995;&#24863;&#30693;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#20998;&#21106;&#25628;&#32034;&#21512;&#24182;&#30340;&#31639;&#27861;&#22312;&#25628;&#32034;&#20851;&#31995;&#24863;&#30693;&#38598;&#25104;&#26435;&#37325;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.08917</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#20851;&#31995;&#24863;&#30693;&#38598;&#25104;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Relation-aware Ensemble Learning for Knowledge Graph Embedding. (arXiv:2310.08917v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08917
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#31995;&#24863;&#30693;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#20998;&#21106;&#25628;&#32034;&#21512;&#24182;&#30340;&#31639;&#27861;&#22312;&#25628;&#32034;&#20851;&#31995;&#24863;&#30693;&#38598;&#25104;&#26435;&#37325;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#22522;&#30784;&#20219;&#21153;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#25506;&#32034;&#19981;&#21516;&#26041;&#24335;&#30340;&#35821;&#20041;&#27169;&#24335;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#31995;&#24863;&#30693;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29616;&#26377;&#26041;&#27861;&#26469;&#23398;&#20064;&#19968;&#20010;&#38598;&#25104;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#20851;&#31995;&#24863;&#30693;&#38598;&#25104;&#25506;&#32034;&#36825;&#20123;&#35821;&#20041;&#20250;&#23548;&#33268;&#27604;&#19968;&#33324;&#38598;&#25104;&#26041;&#27861;&#26356;&#22823;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#21106;&#25628;&#32034;&#21512;&#24182;&#30340;&#31639;&#27861;RelEns-DSC&#65292;&#23427;&#29420;&#31435;&#22320;&#25628;&#32034;&#20851;&#31995;&#24863;&#30693;&#38598;&#25104;&#30340;&#26435;&#37325;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#19982;&#19968;&#33324;&#38598;&#25104;&#26041;&#27861;&#30456;&#21516;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#20294;&#24615;&#33021;&#26356;&#22909;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#22312;&#39640;&#25928;&#25628;&#32034;&#20851;&#31995;&#24863;&#30693;&#38598;&#25104;&#26435;&#37325;&#21644;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#23884;&#20837;&#24615;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#20195;&#30721;&#20844;&#24320;&#22312;https://github.com/LARS-research/RelEns&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph (KG) embedding is a fundamental task in natural language processing, and various methods have been proposed to explore semantic patterns in distinctive ways. In this paper, we propose to learn an ensemble by leveraging existing methods in a relation-aware manner. However, exploring these semantics using relation-aware ensemble leads to a much larger search space than general ensemble methods. To address this issue, we propose a divide-search-combine algorithm RelEns-DSC that searches the relation-wise ensemble weights independently. This algorithm has the same computation cost as general ensemble methods but with much better performance. Experimental results on benchmark datasets demonstrate the effectiveness of the proposed method in efficiently searching relation-aware ensemble weights and achieving state-of-the-art embedding performance. The code is public at https://github.com/LARS-research/RelEns.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#26426;&#21327;&#21516;&#30340;&#27969;&#31243;&#65292;&#36890;&#36807;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#23450;&#21046;&#36755;&#20986;&#65292;&#25299;&#23637;&#20102;&#26426;&#22120;&#32763;&#35793;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.08908</link><description>&lt;p&gt;
&#20154;&#26426;&#21327;&#21516;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Human-in-the-loop Machine Translation with Large Language Model. (arXiv:2310.08908v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#26426;&#21327;&#21516;&#30340;&#27969;&#31243;&#65292;&#36890;&#36807;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#23450;&#21046;&#36755;&#20986;&#65292;&#25299;&#23637;&#20102;&#26426;&#22120;&#32763;&#35793;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22240;&#20854;&#19978;&#19979;&#25991;&#23398;&#20064;&#26426;&#21046;&#21644;&#26032;&#20852;&#33021;&#21147;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#30740;&#31350;&#30028;&#24050;&#32463;&#36827;&#34892;&#20102;&#20960;&#20010;&#35797;&#28857;&#30740;&#31350;&#65292;&#23558;LLM&#24212;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#24182;&#20174;&#22810;&#20010;&#35282;&#24230;&#35780;&#20272;&#20854;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;LLM&#26412;&#36523;&#65292;&#36824;&#26410;&#25506;&#32034;&#20154;&#31867;&#22312;LLM&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#24178;&#39044;&#12290;LLM&#20855;&#26377;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#25552;&#31034;&#24037;&#31243;&#31561;&#29305;&#28857;&#65292;&#19982;&#20154;&#31867;&#22312;&#35821;&#35328;&#20219;&#21153;&#20013;&#30340;&#35748;&#30693;&#33021;&#21147;&#23494;&#20999;&#30456;&#20851;&#65292;&#20026;&#20154;&#26426;&#21327;&#21516;&#29983;&#25104;&#25552;&#20379;&#20102;&#30452;&#35266;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20154;&#26426;&#21327;&#21516;&#30340;&#27969;&#31243;&#65292;&#36890;&#36807;&#20462;&#35746;&#25351;&#20196;&#24341;&#23548;LLM&#29983;&#25104;&#23450;&#21046;&#36755;&#20986;&#12290;&#35813;&#27969;&#31243;&#39318;&#20808;&#21551;&#21160;LLM&#29983;&#25104;&#33609;&#31295;&#32763;&#35793;&#65292;&#28982;&#21518;&#21033;&#29992;&#33258;&#21160;&#26816;&#32034;&#25110;&#20154;&#31867;&#21453;&#39304;&#20316;&#20026;&#30417;&#30563;&#20449;&#21495;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#25552;&#21319;LLM&#30340;&#32763;&#35793;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The large language model (LLM) has garnered significant attention due to its in-context learning mechanisms and emergent capabilities. The research community has conducted several pilot studies to apply LLMs to machine translation tasks and evaluate their performance from diverse perspectives. However, previous research has primarily focused on the LLM itself and has not explored human intervention in the inference process of LLM. The characteristics of LLM, such as in-context learning and prompt engineering, closely mirror human cognitive abilities in language tasks, offering an intuitive solution for human-in-the-loop generation. In this study, we propose a human-in-the-loop pipeline that guides LLMs to produce customized outputs with revision instructions. The pipeline initiates by prompting the LLM to produce a draft translation, followed by the utilization of automatic retrieval or human feedback as supervision signals to enhance the LLM's translation through in-context learning. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;SeqXGPT&#65292;&#36825;&#26159;&#19968;&#31181;&#21477;&#23376;&#32423;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#30333;&#30418;LLMs&#30340;&#23545;&#25968;&#27010;&#29575;&#21015;&#34920;&#20316;&#20026;&#29305;&#24449;&#65292;SeqXGPT&#22312;&#21477;&#23376;&#32423;&#21035;&#30340;AIGT&#26816;&#27979;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.08903</link><description>&lt;p&gt;
SeqXGPT: &#21477;&#23376;&#32423;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
SeqXGPT: Sentence-Level AI-Generated Text Detection. (arXiv:2310.08903v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08903
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SeqXGPT&#65292;&#36825;&#26159;&#19968;&#31181;&#21477;&#23376;&#32423;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#30333;&#30418;LLMs&#30340;&#23545;&#25968;&#27010;&#29575;&#21015;&#34920;&#20316;&#20026;&#29305;&#24449;&#65292;SeqXGPT&#22312;&#21477;&#23376;&#32423;&#21035;&#30340;AIGT&#26816;&#27979;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#27867;&#24212;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21487;&#20197;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#30340;&#20869;&#23481;&#65292;&#24341;&#21457;&#20102;&#23545;LLMs&#28389;&#29992;&#30340;&#25285;&#24551;&#12290;&#22240;&#27492;&#65292;&#24314;&#31435;&#24378;&#22823;&#30340;AI&#29983;&#25104;&#25991;&#26412;&#65288;AIGT&#65289;&#26816;&#27979;&#22120;&#38750;&#24120;&#37325;&#35201;&#12290;&#30446;&#21069;&#30340;&#24037;&#20316;&#21482;&#32771;&#34385;&#25991;&#26723;&#32423;&#21035;&#30340;AIGT&#26816;&#27979;&#65292;&#22240;&#27492;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#21512;&#25104;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#30001;LLMs&#20462;&#25913;&#36807;&#30340;&#21477;&#23376;&#21644;&#30001;&#20154;&#31867;&#32534;&#20889;&#30340;&#21477;&#23376;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#21477;&#23376;&#32423;&#21035;&#30340;&#26816;&#27979;&#25361;&#25112;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SeqXGPT&#65292;&#19968;&#31181;&#21033;&#29992;&#30333;&#30418;LLMs&#30340;&#23545;&#25968;&#27010;&#29575;&#21015;&#34920;&#20316;&#20026;&#21477;&#23376;&#32423;AIGT&#26816;&#27979;&#29305;&#24449;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#20123;&#29305;&#24449;&#31867;&#20284;&#20110;&#35821;&#38899;&#22788;&#29702;&#20013;&#30340;&#8220;&#27874;&#28010;&#8221;&#65292;LLMs&#26080;&#27861;&#30740;&#31350;&#20854;&#32452;&#25104;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22522;&#20110;&#21367;&#31215;&#21644;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#26500;&#24314;&#20102;SeqXGPT&#12290;&#25105;&#20204;&#22312;&#21477;&#23376;&#21644;&#25991;&#26723;&#32423;&#21035;&#30340;&#26816;&#27979;&#25361;&#25112;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20043;&#21069;&#30340;&#26041;&#27861;&#22312;&#21477;&#23376;&#32423;&#21035;&#30340;&#26816;&#27979;&#20013;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
Widely applied large language models (LLMs) can generate human-like content, raising concerns about the abuse of LLMs. Therefore, it is important to build strong AI-generated text (AIGT) detectors. Current works only consider document-level AIGT detection, therefore, in this paper, we first introduce a sentence-level detection challenge by synthesizing a dataset that contains documents that are polished with LLMs, that is, the documents contain sentences written by humans and sentences modified by LLMs. Then we propose \textbf{Seq}uence \textbf{X} (Check) \textbf{GPT}, a novel method that utilizes log probability lists from white-box LLMs as features for sentence-level AIGT detection. These features are composed like \textit{waves} in speech processing and cannot be studied by LLMs. Therefore, we build SeqXGPT based on convolution and self-attention networks. We test it in both sentence and document-level detection challenges. Experimental results show that previous methods struggle in
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#31119;&#21033;&#22806;&#20132;&#36825;&#19968;&#36890;&#29992;&#21644;&#21464;&#31181;&#30340;&#38646;&#21644;&#28216;&#25103;&#65292;&#30446;&#30340;&#26159;&#34913;&#37327;&#21644;&#24378;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#21512;&#20316;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#22522;&#20934;&#26234;&#33021;&#20307;&#22312;&#36798;&#21040;&#39640;&#31038;&#20250;&#31119;&#21033;&#26102;&#23384;&#22312;&#21487;&#21033;&#29992;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.08901</link><description>&lt;p&gt;
&#31119;&#21033;&#22806;&#20132;&#65306;&#22522;&#20934;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#21512;&#20316;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Welfare Diplomacy: Benchmarking Language Model Cooperation. (arXiv:2310.08901v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08901
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#31119;&#21033;&#22806;&#20132;&#36825;&#19968;&#36890;&#29992;&#21644;&#21464;&#31181;&#30340;&#38646;&#21644;&#28216;&#25103;&#65292;&#30446;&#30340;&#26159;&#34913;&#37327;&#21644;&#24378;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#21512;&#20316;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#22522;&#20934;&#26234;&#33021;&#20307;&#22312;&#36798;&#21040;&#39640;&#31038;&#20250;&#31119;&#21033;&#26102;&#23384;&#22312;&#21487;&#21033;&#29992;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#33021;&#21147;&#19981;&#26029;&#22686;&#38271;&#21644;&#24191;&#27867;&#37096;&#32626;&#65292;&#23545;&#20110;&#34913;&#37327;&#23427;&#20204;&#21512;&#20316;&#33021;&#21147;&#30340;&#24378;&#22823;&#22522;&#20934;&#26159;&#24517;&#35201;&#30340;&#12290;&#36951;&#25022;&#30340;&#26159;&#65292;&#22823;&#22810;&#25968;&#22810;&#26234;&#33021;&#20307;&#22522;&#20934;&#35201;&#20040;&#26159;&#38646;&#21644;&#28216;&#25103;&#65292;&#35201;&#20040;&#26159;&#32431;&#31929;&#21512;&#20316;&#30340;&#65292;&#32473;&#20104;&#20102;&#38750;&#24120;&#26377;&#38480;&#30340;&#26426;&#20250;&#36827;&#34892;&#36825;&#31181;&#27979;&#37327;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#38646;&#21644;&#28216;&#25103;Diplomacy&#30340;&#19968;&#20010;&#36890;&#29992;&#21644;&#21464;&#31181;&#8212;&#8212;&#31119;&#21033;&#22806;&#20132;&#65292;&#22312;&#20854;&#20013;&#29609;&#23478;&#24517;&#39035;&#24179;&#34913;&#20891;&#20107;&#24449;&#26381;&#21644;&#22269;&#20869;&#31119;&#21033;&#30340;&#25237;&#36164;&#12290;&#25105;&#20204;&#35748;&#20026;&#31119;&#21033;&#22806;&#20132;&#21487;&#20197;&#26356;&#28165;&#26224;&#22320;&#35780;&#20272;&#24182;&#25552;&#20379;&#26356;&#24378;&#22823;&#30340;&#21512;&#20316;&#33021;&#21147;&#30340;&#22521;&#35757;&#28608;&#21169;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#65306;&#65288;1&#65289;&#25552;&#20986;&#31119;&#21033;&#22806;&#20132;&#30340;&#35268;&#21017;&#24182;&#36890;&#36807;&#24320;&#28304;Diplomacy&#24341;&#25806;&#23454;&#29616;&#23427;&#20204;&#65307;&#65288;2&#65289;&#20351;&#29992;&#38646;&#26679;&#26412;&#25351;&#23548;&#30340;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#22522;&#20934;&#26234;&#33021;&#20307;&#65307;&#20197;&#21450;&#65288;3&#65289;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#20351;&#29992;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#22522;&#20934;&#26234;&#33021;&#20307;&#21487;&#20197;&#36798;&#21040;&#24456;&#39640;&#30340;&#31038;&#20250;&#31119;&#21033;&#65292;&#20294;&#26159;&#23384;&#22312;&#34987;&#21033;&#29992;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#30340;&#31119;&#21033;&#22806;&#20132;&#20419;&#36827;&#31038;&#20250;&#30340;&#23433;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing capabilities and increasingly widespread deployment of AI systems necessitate robust benchmarks for measuring their cooperative capabilities. Unfortunately, most multi-agent benchmarks are either zero-sum or purely cooperative, providing limited opportunities for such measurements. We introduce a general-sum variant of the zero-sum board game Diplomacy -- called Welfare Diplomacy -- in which players must balance investing in military conquest and domestic welfare. We argue that Welfare Diplomacy facilitates both a clearer assessment of and stronger training incentives for cooperative capabilities. Our contributions are: (1) proposing the Welfare Diplomacy rules and implementing them via an open-source Diplomacy engine; (2) constructing baseline agents using zero-shot prompted language models; and (3) conducting experiments where we find that baselines using state-of-the-art models attain high social welfare but are exploitable. Our work aims to promote societal safety by ai
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34987;&#31216;&#20026;&#25506;&#32034;&#24615;AI&#30340;&#26032;&#33539;&#24335;&#65292;&#26088;&#22312;&#33258;&#20027;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#29983;&#25104;&#20869;&#23481;&#30340;&#26032;&#39062;&#24615;&#65292;&#20351;AI&#19981;&#20877;&#36807;&#24230;&#20381;&#36182;&#20154;&#31867;&#30417;&#30563;&#12290;</title><link>http://arxiv.org/abs/2310.08899</link><description>&lt;p&gt;
&#25506;&#32034;&#22810;&#26679;&#21270;AI&#30417;&#30563;&#30340;&#21407;&#21017;
&lt;/p&gt;
&lt;p&gt;
Exploration with Principles for Diverse AI Supervision. (arXiv:2310.08899v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08899
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34987;&#31216;&#20026;&#25506;&#32034;&#24615;AI&#30340;&#26032;&#33539;&#24335;&#65292;&#26088;&#22312;&#33258;&#20027;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#29983;&#25104;&#20869;&#23481;&#30340;&#26032;&#39062;&#24615;&#65292;&#20351;AI&#19981;&#20877;&#36807;&#24230;&#20381;&#36182;&#20154;&#31867;&#30417;&#30563;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#26469;&#35757;&#32451;&#22823;&#22411;transformer&#22312;AI&#39046;&#22495;&#21462;&#24471;&#20102;&#31361;&#30772;&#24615;&#36827;&#23637;&#12290;&#23613;&#31649;&#36825;&#31181;&#29983;&#25104;&#22411;AI&#26041;&#27861;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20005;&#37325;&#20381;&#36182;&#20110;&#20154;&#31867;&#30417;&#30563;&#12290;&#21363;&#20351;&#26159;&#20687;ChatGPT&#36825;&#26679;&#30340;&#26368;&#20808;&#36827;&#30340;AI&#27169;&#22411;&#20063;&#20381;&#36182;&#20110;&#36890;&#36807;&#20154;&#31867;&#28436;&#31034;&#30340;&#24494;&#35843;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#31867;&#36755;&#20837;&#21644;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12290;&#23545;&#20154;&#31867;&#30417;&#30563;&#30340;&#24378;&#28872;&#20381;&#36182;&#23545;&#20110;&#25512;&#21160;AI&#21019;&#26032;&#26500;&#25104;&#20102;&#37325;&#22823;&#38556;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#31216;&#20026;&#25506;&#32034;&#24615;AI&#65288;EAI&#65289;&#65292;&#26088;&#22312;&#33258;&#20027;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#20174;&#26080;&#30417;&#30563;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#39044;&#35757;&#32451;&#20013;&#33719;&#24471;&#28789;&#24863;&#65292;EAI&#22312;&#33258;&#28982;&#35821;&#35328;&#31354;&#38388;&#20869;&#23454;&#29616;&#20102;&#25506;&#32034;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#29983;&#25104;&#20869;&#23481;&#30340;&#26032;&#39062;&#24615;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65306;&#19968;&#20010;&#29983;&#25104;&#25353;&#29031;&#25506;&#32034;&#21407;&#21017;&#29983;&#25104;&#26032;&#39062;&#20869;&#23481;&#30340;&#25191;&#34892;&#32773;&#21644;&#19968;&#20010;&#35780;&#20272;&#29983;&#25104;&#20869;&#23481;&#30340;&#35780;&#35770;&#23478;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training large transformers using next-token prediction has given rise to groundbreaking advancements in AI. While this generative AI approach has produced impressive results, it heavily leans on human supervision. Even state-of-the-art AI models like ChatGPT depend on fine-tuning through human demonstrations, demanding extensive human input and domain expertise. This strong reliance on human oversight poses a significant hurdle to the advancement of AI innovation. To address this limitation, we propose a novel paradigm termed Exploratory AI (EAI) aimed at autonomously generating high-quality training data. Drawing inspiration from unsupervised reinforcement learning (RL) pretraining, EAI achieves exploration within the natural language space. We accomplish this by harnessing large language models to assess the novelty of generated content. Our approach employs two key components: an actor that generates novel content following exploration principles and a critic that evaluates the gen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;PerturbScore&#65292;&#36890;&#36807;&#36830;&#25509;&#31163;&#25955;&#21644;&#36830;&#32493;&#25200;&#21160;&#26469;&#24110;&#21161;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#31163;&#25955;&#25200;&#21160;&#65292;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;PerturbScore&#36229;&#36807;&#20102;&#20197;&#21069;&#30340;&#31163;&#25955;&#25200;&#21160;&#27979;&#37327;&#26041;&#27861;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#26377;&#25928;&#25512;&#24191;&#12290;</title><link>http://arxiv.org/abs/2310.08889</link><description>&lt;p&gt;
PerturbScore: &#36830;&#25509;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#31163;&#25955;&#21644;&#36830;&#32493;&#25200;&#21160;
&lt;/p&gt;
&lt;p&gt;
PerturbScore: Connecting Discrete and Continuous Perturbations in NLP. (arXiv:2310.08889v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;PerturbScore&#65292;&#36890;&#36807;&#36830;&#25509;&#31163;&#25955;&#21644;&#36830;&#32493;&#25200;&#21160;&#26469;&#24110;&#21161;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#31163;&#25955;&#25200;&#21160;&#65292;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;PerturbScore&#36229;&#36807;&#20102;&#20197;&#21069;&#30340;&#31163;&#25955;&#25200;&#21160;&#27979;&#37327;&#26041;&#27861;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#26377;&#25928;&#25512;&#24191;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31070;&#32463;&#32593;&#32476;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#19982;&#35745;&#31639;&#26426;&#35270;&#35273;&#19981;&#21516;&#65292;&#25991;&#26412;&#30340;&#31163;&#25955;&#24615;&#36136;&#20351;&#24471;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#25506;&#32034;&#40065;&#26834;&#24615;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#36830;&#25509;&#31163;&#25955;&#25200;&#21160;&#21644;&#36830;&#32493;&#25200;&#21160;&#65292;&#20174;&#32780;&#23558;&#20854;&#20316;&#20026;&#19968;&#20010;&#26725;&#26753;&#65292;&#24110;&#21161;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20013;&#30340;&#31163;&#25955;&#25200;&#21160;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25506;&#32034;&#22914;&#20309;&#36830;&#25509;&#21644;&#34913;&#37327;&#31163;&#25955;&#25200;&#21160;&#19982;&#36830;&#32493;&#25200;&#21160;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22238;&#24402;&#20219;&#21153;&#20316;&#20026;PerturbScore&#65292;&#20197;&#33258;&#21160;&#23398;&#20064;&#27492;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#65292;&#25105;&#20204;&#21457;&#29616;&#21487;&#20197;&#24314;&#31435;&#31163;&#25955;&#21644;&#36830;&#32493;&#25200;&#21160;&#20043;&#38388;&#30340;&#36830;&#25509;&#65292;&#24182;&#20351;&#29992;&#25552;&#20986;&#30340;PerturbScore&#26469;&#23398;&#20064;&#36825;&#31181;&#30456;&#20851;&#24615;&#65292;&#36229;&#36807;&#20102;&#20808;&#21069;&#29992;&#20110;&#31163;&#25955;&#25200;&#21160;&#27979;&#37327;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#30340;PerturbScore&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65292;&#25200;&#21160;&#19978;&#37117;&#33021;&#24456;&#22909;&#22320;&#25512;&#24191;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid development of neural network applications in NLP, model robustness problem is gaining more attention. Different from computer vision, the discrete nature of texts makes it more challenging to explore robustness in NLP. Therefore, in this paper, we aim to connect discrete perturbations with continuous perturbations, therefore we can use such connections as a bridge to help understand discrete perturbations in NLP models. Specifically, we first explore how to connect and measure the correlation between discrete perturbations and continuous perturbations. Then we design a regression task as a PerturbScore to learn the correlation automatically. Through experimental results, we find that we can build a connection between discrete and continuous perturbations and use the proposed PerturbScore to learn such correlation, surpassing previous methods used in discrete perturbation measuring. Further, the proposed PerturbScore can be well generalized to different datasets, perturb
&lt;/p&gt;</description></item><item><title>InstructTODS&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;LLMs&#29983;&#25104;&#20195;&#29702;&#20449;&#24565;&#29366;&#24577;&#65292;&#24182;&#22312;&#38646;-shot&#30340;&#24773;&#20917;&#19979;&#36866;&#24212;&#22810;&#20010;&#39046;&#22495;&#12290;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#19982;&#23436;&#20840;&#24494;&#35843;&#30340;TODS&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#32463;&#36807;&#20005;&#26684;&#30340;&#20154;&#24037;&#35780;&#20272;&#65292;InstructTODS&#25152;&#29983;&#25104;&#30340;&#23545;&#35805;&#22238;&#24212;&#22312;&#24110;&#21161;&#24615;&#12289;&#20449;&#24687;&#37327;&#21644;&#20154;&#24615;&#21270;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#40644;&#37329;&#22238;&#24212;&#21644;&#26368;&#20808;&#36827;&#30340;TODS&#12290;</title><link>http://arxiv.org/abs/2310.08885</link><description>&lt;p&gt;
InstructTODS: &#29992;&#20110;&#31471;&#21040;&#31471;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
InstructTODS: Large Language Models for End-to-End Task-Oriented Dialogue Systems. (arXiv:2310.08885v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08885
&lt;/p&gt;
&lt;p&gt;
InstructTODS&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;LLMs&#29983;&#25104;&#20195;&#29702;&#20449;&#24565;&#29366;&#24577;&#65292;&#24182;&#22312;&#38646;-shot&#30340;&#24773;&#20917;&#19979;&#36866;&#24212;&#22810;&#20010;&#39046;&#22495;&#12290;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#19982;&#23436;&#20840;&#24494;&#35843;&#30340;TODS&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#32463;&#36807;&#20005;&#26684;&#30340;&#20154;&#24037;&#35780;&#20272;&#65292;InstructTODS&#25152;&#29983;&#25104;&#30340;&#23545;&#35805;&#22238;&#24212;&#22312;&#24110;&#21161;&#24615;&#12289;&#20449;&#24687;&#37327;&#21644;&#20154;&#24615;&#21270;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#40644;&#37329;&#22238;&#24212;&#21644;&#26368;&#20808;&#36827;&#30340;TODS&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLMs)&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#30340;&#21508;&#31181;&#20219;&#21153;&#65292;&#20294;&#22312;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;(TODS)&#26041;&#38754;&#20173;&#23384;&#22312;&#24456;&#22823;&#30340;&#25506;&#32034;&#31354;&#38388;&#65292;&#29305;&#21035;&#26159;&#22312;&#31471;&#21040;&#31471;&#30340;TODS&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;InstructTODS&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#21363;&#25554;&#21363;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#38646;-shot&#31471;&#21040;&#31471;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#65292;&#22312;&#19981;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#36866;&#24212;&#22810;&#20010;&#39046;&#22495;&#12290;&#36890;&#36807;&#21033;&#29992;LLMs&#65292;InstructTODS&#29983;&#25104;&#19968;&#20010;&#20195;&#29702;&#20449;&#24565;&#29366;&#24577;&#65292;&#23558;&#29992;&#25143;&#24847;&#22270;&#26080;&#32541;&#22320;&#36716;&#21270;&#20026;&#21160;&#24577;&#26597;&#35810;&#65292;&#20197;&#19982;&#20219;&#20309;&#30693;&#35782;&#24211;&#36827;&#34892;&#39640;&#25928;&#20132;&#20114;&#12290;&#25105;&#20204;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;InstructTODS&#33021;&#22815;&#22312;&#27809;&#26377;&#20808;&#39564;&#30693;&#35782;&#25110;&#20219;&#21153;&#29305;&#23450;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#24341;&#23548;&#23545;&#35805;&#25104;&#21151;&#23436;&#25104;&#65292;&#24182;&#21462;&#24471;&#19982;&#23436;&#20840;&#24494;&#35843;&#30340;TODS&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#23545;&#31471;&#21040;&#31471;TODS&#30340;&#20005;&#26684;&#20154;&#24037;&#35780;&#20272;&#34920;&#26126;&#65292;InstructTODS&#25152;&#29983;&#25104;&#30340;&#23545;&#35805;&#22238;&#24212;&#22312;&#24110;&#21161;&#24615;&#12289;&#20449;&#24687;&#37327;&#21644;&#20154;&#24615;&#21270;&#31561;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#40644;&#37329;&#22238;&#24212;&#21644;&#26368;&#20808;&#36827;&#30340;TODS&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have been used for diverse tasks in natural language processing (NLP), yet remain under-explored for task-oriented dialogue systems (TODS), especially for end-to-end TODS. We present InstructTODS, a novel off-the-shelf framework for zero-shot end-to-end task-oriented dialogue systems that can adapt to diverse domains without fine-tuning. By leveraging LLMs, InstructTODS generates a proxy belief state that seamlessly translates user intentions into dynamic queries for efficient interaction with any KB. Our extensive experiments demonstrate that InstructTODS achieves comparable performance to fully fine-tuned TODS in guiding dialogues to successful completion without prior knowledge or task-specific data. Furthermore, a rigorous human evaluation of end-to-end TODS shows that InstructTODS produces dialogue responses that notably outperform both the gold responses and the state-of-the-art TODS in terms of helpfulness, informativeness, and humanness. Moreover, t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#30340;&#26816;&#32034;-&#29983;&#25104;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26368;&#22823;&#36793;&#38469;&#20284;&#28982;&#26469;&#35757;&#32451;&#26377;&#24863;&#30693;&#21147;&#30340;&#26816;&#32034;&#22120;&#65292;&#24182;&#32467;&#21512;&#21508;&#31181;&#20803;&#30693;&#35782;&#26469;&#25351;&#23548;&#29983;&#25104;&#22120;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#30693;&#35782;&#30340;&#21033;&#29992;&#25928;&#29575;&#21644;&#29983;&#25104;&#22238;&#24212;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.08877</link><description>&lt;p&gt;
&#29992;&#20110;&#31471;&#21040;&#31471;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#30340;&#26816;&#32034;-&#29983;&#25104;&#23545;&#40784;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Retrieval-Generation Alignment for End-to-End Task-Oriented Dialogue System. (arXiv:2310.08877v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#30340;&#26816;&#32034;-&#29983;&#25104;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26368;&#22823;&#36793;&#38469;&#20284;&#28982;&#26469;&#35757;&#32451;&#26377;&#24863;&#30693;&#21147;&#30340;&#26816;&#32034;&#22120;&#65292;&#24182;&#32467;&#21512;&#21508;&#31181;&#20803;&#30693;&#35782;&#26469;&#25351;&#23548;&#29983;&#25104;&#22120;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#30693;&#35782;&#30340;&#21033;&#29992;&#25928;&#29575;&#21644;&#29983;&#25104;&#22238;&#24212;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#23637;&#19968;&#20010;&#39640;&#25928;&#30340;&#26816;&#32034;&#22120;&#20174;&#22823;&#35268;&#27169;&#30340;&#30693;&#35782;&#24211;&#20013;&#26816;&#32034;&#30693;&#35782;&#23545;&#20110;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#26469;&#35828;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#20415;&#26377;&#25928;&#22788;&#29702;&#26412;&#22320;&#21270;&#21644;&#19987;&#19994;&#21270;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24191;&#27867;&#20351;&#29992;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;T5&#21644;ChatGPT&#65289;&#22312;&#29983;&#25104;&#22238;&#24212;&#26102;&#36890;&#24120;&#24456;&#38590;&#21306;&#20998;&#26816;&#32034;&#21040;&#30340;&#30693;&#35782;&#24211;&#35760;&#24405;&#20043;&#38388;&#30340;&#32454;&#24494;&#24046;&#24322;&#65292;&#23548;&#33268;&#29983;&#25104;&#30340;&#22238;&#24212;&#36136;&#37327;&#19981;&#29702;&#24819;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21033;&#29992;&#26368;&#22823;&#36793;&#38469;&#20284;&#28982;&#26469;&#35757;&#32451;&#19968;&#20010;&#26377;&#24863;&#30693;&#21147;&#30340;&#26816;&#32034;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#22238;&#24212;&#29983;&#25104;&#30340;&#20449;&#21495;&#36827;&#34892;&#30417;&#30563;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#32771;&#34385;&#21040;&#26816;&#32034;&#21040;&#30340;&#23454;&#20307;&#65292;&#36824;&#32467;&#21512;&#20102;&#21508;&#31181;&#20803;&#30693;&#35782;&#26469;&#25351;&#23548;&#29983;&#25104;&#22120;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#30693;&#35782;&#30340;&#21033;&#29992;&#25928;&#29575;&#12290;&#25105;&#20204;&#20351;&#29992;T5&#21644;ChatGPT&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#65292;&#22312;&#19977;&#20010;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#32467;&#21512;&#20803;&#30693;&#35782;&#26102;&#65292;&#22238;&#24212;&#29983;&#25104;&#22120;&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#35813;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing an efficient retriever to retrieve knowledge from a large-scale knowledge base (KB) is critical for task-oriented dialogue systems to effectively handle localized and specialized tasks. However, widely used generative models such as T5 and ChatGPT often struggle to differentiate subtle differences among the retrieved KB records when generating responses, resulting in suboptimal quality of generated responses. In this paper, we propose the application of maximal marginal likelihood to train a perceptive retriever by utilizing signals from response generation for supervision. In addition, our approach goes beyond considering solely retrieved entities and incorporates various meta knowledge to guide the generator, thus improving the utilization of knowledge. We evaluate our approach on three task-oriented dialogue datasets using T5 and ChatGPT as the backbone models. The results demonstrate that when combined with meta knowledge, the response generator can effectively leverage 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#36870;&#21521;&#22270;&#32447;&#24615;&#21270;&#65288;RGL&#65289;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;AMR&#35299;&#26512;&#20013;&#32467;&#26500;&#20002;&#22833;&#31215;&#32047;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#23450;&#20041;&#40664;&#35748;&#21644;&#36870;&#21521;&#32447;&#24615;&#21270;&#39034;&#24207;&#65292;&#24182;&#36890;&#36807;&#33258;&#25105;&#33976;&#39311;&#26426;&#21046;&#24341;&#23548;&#27169;&#22411;&#29983;&#25104;&#40664;&#35748;&#32447;&#24615;&#21270;&#65292;&#25552;&#39640;&#20102;&#35299;&#26512;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.08860</link><description>&lt;p&gt;
&#24341;&#23548;AMR&#35299;&#26512;&#19982;&#36870;&#21521;&#22270;&#32447;&#24615;&#21270;
&lt;/p&gt;
&lt;p&gt;
Guiding AMR Parsing with Reverse Graph Linearization. (arXiv:2310.08860v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#36870;&#21521;&#22270;&#32447;&#24615;&#21270;&#65288;RGL&#65289;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;AMR&#35299;&#26512;&#20013;&#32467;&#26500;&#20002;&#22833;&#31215;&#32047;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#23450;&#20041;&#40664;&#35748;&#21644;&#36870;&#21521;&#32447;&#24615;&#21270;&#39034;&#24207;&#65292;&#24182;&#36890;&#36807;&#33258;&#25105;&#33976;&#39311;&#26426;&#21046;&#24341;&#23548;&#27169;&#22411;&#29983;&#25104;&#40664;&#35748;&#32447;&#24615;&#21270;&#65292;&#25552;&#39640;&#20102;&#35299;&#26512;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#65288;AMR&#65289;&#35299;&#26512;&#26088;&#22312;&#20174;&#32473;&#23450;&#30340;&#21477;&#23376;&#20013;&#25552;&#21462;&#19968;&#20010;&#25277;&#35937;&#30340;&#35821;&#20041;&#22270;&#12290;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#26041;&#27861;&#23558;&#35821;&#20041;&#22270;&#32447;&#24615;&#21270;&#20026;&#33410;&#28857;&#21644;&#36793;&#30340;&#24207;&#21015;&#65292;&#24182;&#30452;&#25509;&#29983;&#25104;&#32447;&#24615;&#21270;&#30340;&#22270;&#65292;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#20123;&#26041;&#27861;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#20250;&#20986;&#29616;&#32467;&#26500;&#20002;&#22833;&#31215;&#32047;&#30340;&#38382;&#39064;&#65292;&#23548;&#33268;&#21518;&#35299;&#30721;&#30340;&#33410;&#28857;&#21644;&#36793;&#30340;F1&#24471;&#20998;&#27604;&#20808;&#35299;&#30721;&#30340;&#35201;&#20302;&#24471;&#22810;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22686;&#24378;&#26694;&#26550;&#65292;&#21363;&#36870;&#21521;&#22270;&#32447;&#24615;&#21270;&#65288;RGL&#65289;&#12290;RGL&#23450;&#20041;&#20102;AMR&#22270;&#30340;&#40664;&#35748;&#21644;&#36870;&#21521;&#32447;&#24615;&#21270;&#39034;&#24207;&#65292;&#20854;&#20013;&#22312;&#40664;&#35748;&#39034;&#24207;&#30340;&#21518;&#37096;&#20986;&#29616;&#30340;&#22823;&#22810;&#25968;&#32467;&#26500;&#22312;&#36870;&#21521;&#39034;&#24207;&#30340;&#21069;&#37096;&#20986;&#29616;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;RGL&#36890;&#36807;&#20004;&#27425;&#33258;&#25105;&#33976;&#39311;&#26426;&#21046;&#23558;&#36870;&#21521;&#32447;&#24615;&#21270;&#24341;&#20837;&#21407;&#22987;AMR&#35299;&#26512;&#22120;&#65292;&#20174;&#32780;&#22312;&#29983;&#25104;&#40664;&#35748;&#32447;&#24615;&#21270;&#26102;&#24341;&#23548;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Abstract Meaning Representation (AMR) parsing aims to extract an abstract semantic graph from a given sentence. The sequence-to-sequence approaches, which linearize the semantic graph into a sequence of nodes and edges and generate the linearized graph directly, have achieved good performance. However, we observed that these approaches suffer from structure loss accumulation during the decoding process, leading to a much lower F1-score for nodes and edges decoded later compared to those decoded earlier. To address this issue, we propose a novel Reverse Graph Linearization (RGL) enhanced framework. RGL defines both default and reverse linearization orders of an AMR graph, where most structures at the back part of the default order appear at the front part of the reversed order and vice versa. RGL incorporates the reversed linearization to the original AMR parser through a two-pass self-distillation mechanism, which guides the model when generating the default linearizations. Our analysi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;SAFARI&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20010;&#24615;&#21270;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#31995;&#32479;&#30340;&#28304;&#35745;&#21010;&#22120;&#65292;&#20351;&#24471;&#22810;&#20010;&#30693;&#35782;&#28304;&#30340;&#20381;&#36182;&#20851;&#31995;&#33021;&#22815;&#34987;&#25972;&#21512;&#65292;&#24182;&#33021;&#29983;&#25104;&#19968;&#33268;&#30340;&#22238;&#24212;&#12290;</title><link>http://arxiv.org/abs/2310.08840</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20010;&#24615;&#21270;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#30340;&#28304;&#35745;&#21010;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Source Planner for Personalized Knowledge-grounded Dialogue. (arXiv:2310.08840v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;SAFARI&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20010;&#24615;&#21270;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#31995;&#32479;&#30340;&#28304;&#35745;&#21010;&#22120;&#65292;&#20351;&#24471;&#22810;&#20010;&#30693;&#35782;&#28304;&#30340;&#20381;&#36182;&#20851;&#31995;&#33021;&#22815;&#34987;&#25972;&#21512;&#65292;&#24182;&#33021;&#29983;&#25104;&#19968;&#33268;&#30340;&#22238;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#22495;&#23545;&#35805;&#31995;&#32479;&#36890;&#24120;&#38656;&#35201;&#19981;&#21516;&#30340;&#30693;&#35782;&#28304;&#26469;&#29983;&#25104;&#26356;&#20855;&#20449;&#24687;&#24615;&#21644;&#35777;&#25454;&#24615;&#30340;&#22238;&#24212;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#31995;&#32479;&#35201;&#20040;&#19987;&#27880;&#20110;&#21333;&#19968;&#30693;&#35782;&#28304;&#65292;&#35201;&#20040;&#24573;&#35270;&#20102;&#22810;&#20010;&#30693;&#35782;&#28304;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#21487;&#33021;&#23548;&#33268;&#29983;&#25104;&#19981;&#19968;&#33268;&#29978;&#33267;&#30683;&#30462;&#30340;&#22238;&#24212;&#12290;&#20026;&#20102;&#25972;&#21512;&#22810;&#20010;&#30693;&#35782;&#28304;&#21644;&#23427;&#20204;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SAFARI&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30417;&#30563;&#21644;&#38750;&#30417;&#30563;&#35774;&#32622;&#19979;&#30340;&#20986;&#33394;&#33021;&#21147;&#26469;&#35268;&#21010;&#12289;&#29702;&#35299;&#21644;&#25972;&#21512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SAFARI&#23558;&#30693;&#35782;&#30340;&#22522;&#30784;&#20998;&#35299;&#20026;&#22810;&#20010;&#28304;&#21644;&#22238;&#24212;&#29983;&#25104;&#65292;&#20174;&#32780;&#26041;&#20415;&#22320;&#25193;&#23637;&#21040;&#21508;&#31181;&#30693;&#35782;&#28304;&#65292;&#21253;&#25324;&#19981;&#20351;&#29992;&#20219;&#20309;&#28304;&#30340;&#21487;&#33021;&#24615;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#20010;&#24615;&#21270;&#30340;&#12289;&#30693;&#35782;&#39537;&#21160;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open-domain dialogue system usually requires different sources of knowledge to generate more informative and evidential responses. However, existing knowledge-grounded dialogue systems either focus on a single knowledge source or overlook the dependency between multiple sources of knowledge, which may result in generating inconsistent or even paradoxical responses. To incorporate multiple knowledge sources and dependencies between them, we propose SAFARI, a novel framework that leverages the exceptional capabilities of large language models (LLMs) in planning, understanding, and incorporating under both supervised and unsupervised settings. Specifically, SAFARI decouples the knowledge grounding into multiple sources and response generation, which allows easy extension to various knowledge sources including the possibility of not using any sources. To study the problem, we construct a personalized knowledge-grounded dialogue dataset \textit{\textbf{K}nowledge \textbf{B}ehind \textbf{P}e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#29992;&#20110;&#21387;&#32553;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#20960;&#31181;&#20219;&#21153;&#19981;&#21487;&#30693;&#33976;&#39311;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#22522;&#20110;MiniLMv2&#30340;MHA&#36716;&#31227;&#26159;&#26368;&#20339;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2310.08797</link><description>&lt;p&gt;
&#23545;&#20110;&#21387;&#32553;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#20219;&#21153;&#19981;&#21487;&#30693;&#33976;&#39311;&#26041;&#27861;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Comparative Analysis of Task-Agnostic Distillation Methods for Compressing Transformer Language Models. (arXiv:2310.08797v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08797
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#29992;&#20110;&#21387;&#32553;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#20960;&#31181;&#20219;&#21153;&#19981;&#21487;&#30693;&#33976;&#39311;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#22522;&#20110;MiniLMv2&#30340;MHA&#36716;&#31227;&#26159;&#26368;&#20339;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#29616;&#20195;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#32452;&#20214;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26114;&#36149;&#30340;&#25512;&#26029;&#25104;&#26412;&#65292;&#23427;&#20204;&#22312;&#23454;&#38469;&#37096;&#32626;&#20013;&#24120;&#24120;&#25928;&#29575;&#20302;&#19979;&#12290;&#30693;&#35782;&#33976;&#39311;&#26159;&#19968;&#31181;&#25552;&#39640;&#20854;&#25928;&#29575;&#24182;&#20445;&#25345;&#22823;&#37096;&#20998;&#25928;&#33021;&#30340;&#26377;&#24076;&#26395;&#30340;&#25216;&#26415;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#29616;&#12289;&#27604;&#36739;&#21644;&#20998;&#26512;&#20102;&#20960;&#31181;&#20195;&#34920;&#24615;&#30340;&#12289;&#29992;&#20110;Transformer&#35821;&#35328;&#27169;&#22411;&#20219;&#21153;&#19981;&#21487;&#30693;(&#36890;&#29992;)&#33976;&#39311;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36755;&#20986;&#20998;&#24067;(OD)&#36716;&#31227;&#12289;&#38544;&#34255;&#29366;&#24577;(HS)&#36716;&#31227;&#20197;&#21450;&#22522;&#20110;MiniLMv2&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;(MHA)&#36716;&#31227;&#31561;&#22810;&#31181;&#33976;&#39311;&#26041;&#27861;&#22312;&#21508;&#31181;&#23398;&#29983;&#26550;&#26500;&#19979;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#21333;&#35821;(&#33521;&#35821;)&#21644;&#22810;&#35821;&#35774;&#32622;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;MiniLMv2&#30340;MHA&#36716;&#31227;&#36890;&#24120;&#26159;&#33976;&#39311;&#30340;&#26368;&#20339;&#36873;&#25321;&#65292;&#24182;&#35299;&#37322;&#20102;&#28508;&#22312;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have become a vital component in modern NLP, achieving state of the art performance in a variety of tasks. However, they are often inefficient for real-world deployment due to their expensive inference costs. Knowledge distillation is a promising technique to improve their efficiency while retaining most of their effectiveness. In this paper, we reproduce, compare and analyze several representative methods for task-agnostic (general-purpose) distillation of Transformer language models. Our target of study includes Output Distribution (OD) transfer, Hidden State (HS) transfer with various layer mapping strategies, and Multi-Head Attention (MHA) transfer based on MiniLMv2. Through our extensive experiments, we study the effectiveness of each method for various student architectures in both monolingual (English) and multilingual settings. Overall, we show that MHA transfer based on MiniLMv2 is generally the best option for distillation and explain the potential reaso
&lt;/p&gt;</description></item><item><title>&#36825;&#27454;&#20840;&#27969;&#31243;&#25925;&#20107;&#24773;&#33410;&#29983;&#25104;&#22120;&#36890;&#36807;&#20248;&#21270;&#27169;&#22411;&#21644;&#35774;&#35745;&#25552;&#31034;&#20449;&#24687;&#65292;&#20197;&#26367;&#20195;&#26114;&#36149;&#30340;API&#35843;&#29992;&#65292;&#23454;&#29616;&#20102;&#20302;&#25104;&#26412;&#29983;&#25104;&#39640;&#36136;&#37327;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2310.08796</link><description>&lt;p&gt;
&#20840;&#27969;&#31243;&#25925;&#20107;&#24773;&#33410;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
End-to-end Story Plot Generator. (arXiv:2310.08796v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08796
&lt;/p&gt;
&lt;p&gt;
&#36825;&#27454;&#20840;&#27969;&#31243;&#25925;&#20107;&#24773;&#33410;&#29983;&#25104;&#22120;&#36890;&#36807;&#20248;&#21270;&#27169;&#22411;&#21644;&#35774;&#35745;&#25552;&#31034;&#20449;&#24687;&#65292;&#20197;&#26367;&#20195;&#26114;&#36149;&#30340;API&#35843;&#29992;&#65292;&#23454;&#29616;&#20102;&#20302;&#25104;&#26412;&#29983;&#25104;&#39640;&#36136;&#37327;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25925;&#20107;&#24773;&#33410;&#34429;&#30701;&#65292;&#20294;&#21253;&#21547;&#20102;&#19968;&#20010;&#23436;&#25972;&#25925;&#20107;&#20013;&#22823;&#37096;&#20998;&#20851;&#38190;&#20449;&#24687;&#65292;&#23427;&#21487;&#33021;&#21253;&#21547;&#25968;&#19975;&#23383;&#30340;&#20869;&#23481;&#12290;&#26412;&#25991;&#30740;&#31350;&#33258;&#21160;&#29983;&#25104;&#25925;&#20107;&#24773;&#33410;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#25925;&#20107;&#21069;&#25552;&#12289;&#35282;&#33394;&#25551;&#36848;&#12289;&#24773;&#33410;&#22823;&#32434;&#31561;&#12290;&#29616;&#26377;&#30340;&#24773;&#33410;&#29983;&#25104;&#22120;&#65288;&#22914; DOC (Yang et al., 2022a)&#65289;&#22312;&#25925;&#20107;&#24773;&#33410;&#30340;&#35268;&#21010;&#38454;&#27573;&#38656;&#35201;&#25968;&#30334;&#21040;&#25968;&#21315;&#27425;LLM&#65288;&#22914;OpenAI API&#65289;&#30340;&#35843;&#29992;&#65292;&#36825;&#26082;&#32791;&#26102;&#21448;&#26114;&#36149;&#65292;&#33267;&#23569;&#38656;&#35201;&#20960;&#20998;&#38047;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#30828;&#32534;&#30721;&#29305;&#24615;&#20351;&#24471;&#27969;&#31243;&#19981;&#21487;&#24494;&#20998;&#65292;&#38459;&#30861;&#20102;&#24555;&#36895;&#23450;&#21046;&#21644;&#20010;&#24615;&#21270;&#30340;&#24773;&#33410;&#29983;&#25104;&#22120;&#30340;&#21457;&#23637;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#20010;&#27169;&#22411;&#65306;OpenPlot&#12289;E2EPlot&#21644;RLPlot&#65292;&#20197;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;OpenPlot&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#23558;&#26114;&#36149;&#30340;OpenAI API&#35843;&#29992;&#26367;&#25442;&#20026;&#20351;&#29992;LLaMA2 (Touvron et al., 2023)&#35843;&#29992;&#65292;&#20174;&#32780;&#20197;&#20302;&#25104;&#26412;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Story plots, while short, carry most of the essential information of a full story that may contain tens of thousands of words. We study the problem of automatic generation of story plots, which includes story premise, character descriptions, plot outlines, etc. To generate a single engaging plot, existing plot generators (e.g., DOC (Yang et al., 2022a)) require hundreds to thousands of calls to LLMs (e.g., OpenAI API) in the planning stage of the story plot, which is costly and takes at least several minutes. Moreover, the hard-wired nature of the method makes the pipeline non-differentiable, blocking fast specialization and personalization of the plot generator. In this paper, we propose three models, $\texttt{OpenPlot}$, $\texttt{E2EPlot}$ and $\texttt{RLPlot}$, to address these challenges. $\texttt{OpenPlot}$ replaces expensive OpenAI API calls with LLaMA2 (Touvron et al., 2023) calls via careful prompt designs, which leads to inexpensive generation of high-quality training datasets
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BMBI&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#22810;&#36873;&#38382;&#39064;&#22238;&#31572;&#27169;&#22411;&#30340;&#20559;&#35265;&#12290;&#36890;&#36807;&#35266;&#23519;&#19968;&#20010;&#26597;&#35810;&#23454;&#20363;&#23545;&#21478;&#19968;&#20010;&#23454;&#20363;&#30340;&#24433;&#21709;&#65292;&#27979;&#37327;&#26597;&#35810;&#23454;&#20363;&#30340;&#20559;&#35265;&#31243;&#24230;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#20248;&#21270;&#30446;&#26631;&#65292;&#24418;&#25104;&#19968;&#20010;&#22810;&#20219;&#21153;&#23398;&#20064;&#35774;&#32622;&#12290;&#21516;&#26102;&#24341;&#20837;&#26032;&#30340;&#20559;&#35265;&#35780;&#20272;&#25351;&#26631;&#20197;&#37327;&#21270;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2310.08795</link><description>&lt;p&gt;
&#36890;&#36807;&#36861;&#36394;&#20559;&#35265;&#24433;&#21709;&#26469;&#20943;&#36731;&#38382;&#39064;&#22238;&#31572;&#27169;&#22411;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Mitigating Bias for Question Answering Models by Tracking Bias Influence. (arXiv:2310.08795v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BMBI&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#22810;&#36873;&#38382;&#39064;&#22238;&#31572;&#27169;&#22411;&#30340;&#20559;&#35265;&#12290;&#36890;&#36807;&#35266;&#23519;&#19968;&#20010;&#26597;&#35810;&#23454;&#20363;&#23545;&#21478;&#19968;&#20010;&#23454;&#20363;&#30340;&#24433;&#21709;&#65292;&#27979;&#37327;&#26597;&#35810;&#23454;&#20363;&#30340;&#20559;&#35265;&#31243;&#24230;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#20248;&#21270;&#30446;&#26631;&#65292;&#24418;&#25104;&#19968;&#20010;&#22810;&#20219;&#21153;&#23398;&#20064;&#35774;&#32622;&#12290;&#21516;&#26102;&#24341;&#20837;&#26032;&#30340;&#20559;&#35265;&#35780;&#20272;&#25351;&#26631;&#20197;&#37327;&#21270;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#35777;&#26126;&#21508;&#31181;NLP&#20219;&#21153;&#30340;&#27169;&#22411;&#23384;&#22312;&#21051;&#26495;&#21360;&#35937;&#65292;&#32780;&#38382;&#39064;&#22238;&#31572;&#65288;QA&#65289;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#23588;&#20854;&#26377;&#23475;&#65292;&#22240;&#20026;&#36755;&#20986;&#30340;&#31572;&#26696;&#21487;&#33021;&#30452;&#25509;&#34987;&#26368;&#32456;&#29992;&#25143;&#20351;&#29992;&#12290;&#24050;&#32463;&#26377;&#25968;&#25454;&#38598;&#29992;&#20110;&#35780;&#20272;QA&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#65292;&#20294;&#26159;&#23545;&#20110;QA&#27169;&#22411;&#30340;&#20559;&#35265;&#32531;&#35299;&#25216;&#26415;&#20173;&#22788;&#20110;&#25506;&#32034;&#38454;&#27573;&#12290;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BMBI&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#32531;&#35299;&#22810;&#36873;&#38382;&#39064;&#22238;&#31572;&#27169;&#22411;&#30340;&#20559;&#35265;&#12290;&#22522;&#20110;&#19968;&#20010;&#30452;&#35273;&#65292;&#21363;&#22914;&#26524;&#19968;&#20010;&#27169;&#22411;&#20174;&#19968;&#20010;&#26377;&#20559;&#35265;&#30340;&#20363;&#23376;&#20013;&#23398;&#21040;&#20102;&#19996;&#35199;&#65292;&#23427;&#21487;&#33021;&#26356;&#23481;&#26131;&#20986;&#29616;&#20559;&#35265;&#65292;&#25105;&#20204;&#36890;&#36807;&#35266;&#23519;&#19968;&#20010;&#26597;&#35810;&#23454;&#20363;&#23545;&#21478;&#19968;&#20010;&#23454;&#20363;&#30340;&#24433;&#21709;&#26469;&#34913;&#37327;&#26597;&#35810;&#23454;&#20363;&#30340;&#20559;&#35265;&#31243;&#24230;&#12290;&#22914;&#26524;&#21463;&#21040;&#24433;&#21709;&#30340;&#23454;&#20363;&#26356;&#20559;&#35265;&#65292;&#25105;&#20204;&#35748;&#20026;&#26597;&#35810;&#23454;&#20363;&#26159;&#26377;&#20559;&#35265;&#30340;&#12290;&#25105;&#20204;&#20351;&#29992;&#26816;&#27979;&#21040;&#30340;&#20559;&#35265;&#31243;&#24230;&#20316;&#20026;&#20248;&#21270;&#30446;&#26631;&#65292;&#24418;&#25104;&#19968;&#20010;&#22810;&#20219;&#21153;&#23398;&#20064;&#35774;&#32622;&#65292;&#38500;&#20102;&#21407;&#26469;&#30340;QA&#20219;&#21153;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20559;&#35265;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#20840;&#38754;&#32780;&#25935;&#24863;&#30340;&#26041;&#24335;&#37327;&#21270;&#20559;&#35265;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#20943;&#36731;QA&#27169;&#22411;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models of various NLP tasks have been shown to exhibit stereotypes, and the bias in the question answering (QA) models is especially harmful as the output answers might be directly consumed by the end users. There have been datasets to evaluate bias in QA models, while bias mitigation technique for the QA models is still under-explored. In this work, we propose BMBI, an approach to mitigate the bias of multiple-choice QA models. Based on the intuition that a model would lean to be more biased if it learns from a biased example, we measure the bias level of a query instance by observing its influence on another instance. If the influenced instance is more biased, we derive that the query instance is biased. We then use the bias level detected as an optimization objective to form a multi-task learning setting in addition to the original QA task. We further introduce a new bias evaluation metric to quantify bias in a comprehensive and sensitive way. We show that our method could be applie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32431;&#25552;&#31034;&#24335;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#38544;&#34255;&#30340;&#21051;&#26495;&#21360;&#35937;&#65292;&#36890;&#36807;&#21160;&#24577;&#29983;&#25104;&#20869;&#37096;&#21051;&#26495;&#21360;&#35937;&#30340;&#30693;&#35782;&#34920;&#31034;&#65292;&#25105;&#20204;&#33021;&#22815;&#35782;&#21035;&#36825;&#20123;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#20559;&#35265;&#12290;&#36825;&#39033;&#24037;&#20316;&#22312;&#25512;&#36827;&#36879;&#26126;&#24230;&#21644;&#20419;&#36827;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#26041;&#38754;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2310.08780</link><description>&lt;p&gt;
&#8220;&#25105;&#19981;&#26159;&#31181;&#26063;&#20027;&#20041;&#32773;&#65292;&#20294;&#26159;&#8230;&#8230;&#8221;&#65306;&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#30693;&#35782;&#20013;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
"Im not Racist but...": Discovering Bias in the Internal Knowledge of Large Language Models. (arXiv:2310.08780v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32431;&#25552;&#31034;&#24335;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#38544;&#34255;&#30340;&#21051;&#26495;&#21360;&#35937;&#65292;&#36890;&#36807;&#21160;&#24577;&#29983;&#25104;&#20869;&#37096;&#21051;&#26495;&#21360;&#35937;&#30340;&#30693;&#35782;&#34920;&#31034;&#65292;&#25105;&#20204;&#33021;&#22815;&#35782;&#21035;&#36825;&#20123;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#20559;&#35265;&#12290;&#36825;&#39033;&#24037;&#20316;&#22312;&#25512;&#36827;&#36879;&#26126;&#24230;&#21644;&#20419;&#36827;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#26041;&#38754;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22240;&#20854;&#22312;&#19981;&#26029;&#25193;&#23637;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#20986;&#33394;&#34920;&#29616;&#32780;&#21463;&#21040;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#34987;&#35777;&#26126;&#23384;&#22312;&#20869;&#22312;&#30340;&#31038;&#20250;&#20559;&#35265;&#25110;&#21051;&#26495;&#21360;&#35937;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#23427;&#20204;&#22312;&#35768;&#22810;&#19979;&#28216;&#24212;&#29992;&#20013;&#30340;&#24615;&#33021;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#32431;&#25552;&#31034;&#24335;&#26041;&#27861;&#65292;&#29992;&#20110;&#25581;&#31034;&#20219;&#24847;LLM&#20013;&#38544;&#34255;&#30340;&#21051;&#26495;&#21360;&#35937;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21160;&#24577;&#29983;&#25104;&#20102;&#20869;&#37096;&#21051;&#26495;&#21360;&#35937;&#30340;&#30693;&#35782;&#34920;&#31034;&#65292;&#20174;&#32780;&#33021;&#22815;&#35782;&#21035;LLM&#20869;&#37096;&#30693;&#35782;&#20013;&#32534;&#30721;&#30340;&#20559;&#35265;&#12290;&#36890;&#36807;&#25581;&#31034;LLM&#20013;&#23384;&#22312;&#30340;&#20559;&#35265;&#24182;&#25552;&#20379;&#19968;&#31181;&#31995;&#32479;&#24615;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#22312;&#25512;&#36827;&#36879;&#26126;&#24230;&#21644;&#20419;&#36827;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#26041;&#38754;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have garnered significant attention for their remarkable performance in a continuously expanding set of natural language processing tasks. However, these models have been shown to harbor inherent societal biases, or stereotypes, which can adversely affect their performance in their many downstream applications. In this paper, we introduce a novel, purely prompt-based approach to uncover hidden stereotypes within any arbitrary LLM. Our approach dynamically generates a knowledge representation of internal stereotypes, enabling the identification of biases encoded within the LLM's internal knowledge. By illuminating the biases present in LLMs and offering a systematic methodology for their analysis, our work contributes to advancing transparency and promoting fairness in natural language processing systems.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26657;&#20934;&#27169;&#22411;&#29983;&#25104;&#30340;&#24207;&#21015;&#30340;&#20284;&#28982;&#24615;&#65292;&#20351;&#20854;&#26356;&#22909;&#22320;&#19982;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#27169;&#22411;&#27979;&#37327;&#30340;&#19968;&#33268;&#24615;&#24230;&#37327;&#30456;&#19968;&#33268;&#65292;&#20174;&#32780;&#25552;&#39640;&#25688;&#35201;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#21644;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.08764</link><description>&lt;p&gt;
&#26657;&#20934;&#20351;&#24471;&#25688;&#35201;&#27169;&#22411;&#22312;&#19968;&#33268;&#24615;&#26041;&#38754;&#26356;&#20026;&#20934;&#30830;
&lt;/p&gt;
&lt;p&gt;
Calibrating Likelihoods towards Consistency in Summarization Models. (arXiv:2310.08764v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08764
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26657;&#20934;&#27169;&#22411;&#29983;&#25104;&#30340;&#24207;&#21015;&#30340;&#20284;&#28982;&#24615;&#65292;&#20351;&#20854;&#26356;&#22909;&#22320;&#19982;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#27169;&#22411;&#27979;&#37327;&#30340;&#19968;&#33268;&#24615;&#24230;&#37327;&#30456;&#19968;&#33268;&#65292;&#20174;&#32780;&#25552;&#39640;&#25688;&#35201;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25277;&#35937;&#21270;&#25991;&#26412;&#25688;&#35201;&#21462;&#24471;&#20102;&#19968;&#20123;&#26032;&#30340;&#36827;&#23637;&#65292;&#20294;&#30446;&#21069;&#30340;&#25688;&#35201;&#27169;&#22411;&#20173;&#28982;&#23384;&#22312;&#29983;&#25104;&#20107;&#23454;&#19981;&#19968;&#33268;&#30340;&#25688;&#35201;&#30340;&#38382;&#39064;&#65292;&#36825;&#20943;&#24369;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#25928;&#29992;&#12290;&#25105;&#20204;&#35748;&#20026;&#36896;&#25104;&#36825;&#31181;&#34892;&#20026;&#30340;&#20027;&#35201;&#21407;&#22240;&#26159;&#30001;&#20110;&#22522;&#20110;&#26368;&#22823;&#20284;&#28982;&#30446;&#26631;&#35757;&#32451;&#30340;&#25688;&#35201;&#27169;&#22411;&#22312;&#32473;&#23450;&#19978;&#19979;&#25991;&#26102;&#36171;&#20104;&#21487;&#33021;&#24207;&#21015;&#39640;&#27010;&#29575;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#19981;&#33021;&#20934;&#30830;&#22320;&#26681;&#25454;&#20854;&#19968;&#33268;&#24615;&#25490;&#21517;&#24207;&#21015;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#26657;&#20934;&#27169;&#22411;&#29983;&#25104;&#30340;&#24207;&#21015;&#30340;&#20284;&#28982;&#24615;&#65292;&#20351;&#20854;&#26356;&#22909;&#22320;&#19982;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#27169;&#22411;&#27979;&#37327;&#30340;&#19968;&#33268;&#24615;&#24230;&#37327;&#30456;&#19968;&#33268;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20154;&#31867;&#35780;&#20272;&#30740;&#31350;&#21644;&#33258;&#21160;&#25351;&#26631;&#34920;&#26126;&#65292;&#32463;&#36807;&#26657;&#20934;&#30340;&#27169;&#22411;&#29983;&#25104;&#26356;&#19968;&#33268;&#19988;&#26356;&#39640;&#36136;&#37327;&#30340;&#25688;&#35201;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#20351;&#29992;&#25105;&#20204;&#26041;&#27861;&#35757;&#32451;&#30340;&#27169;&#22411;&#36820;&#22238;&#30340;&#27010;&#29575;&#19982;NLI&#24471;&#20998;&#26356;&#20026;&#23545;&#40784;&#65292;&#36825;&#26174;&#33879;&#25552;&#39640;&#20102;&#25688;&#35201;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the recent advances in abstractive text summarization, current summarization models still suffer from generating factually inconsistent summaries, reducing their utility for real-world application. We argue that the main reason for such behavior is that the summarization models trained with maximum likelihood objective assign high probability to plausible sequences given the context, but they often do not accurately rank sequences by their consistency. In this work, we solve this problem by calibrating the likelihood of model generated sequences to better align with a consistency metric measured by natural language inference (NLI) models. The human evaluation study and automatic metrics show that the calibrated models generate more consistent and higher-quality summaries. We also show that the models trained using our method return probabilities that are better aligned with the NLI scores, which significantly increase reliability of summarization models.
&lt;/p&gt;</description></item><item><title>CompA&#25552;&#20986;&#20102;&#30001;&#20004;&#20010;&#19987;&#23478;&#27880;&#37322;&#30340;&#38899;&#39057;-&#35821;&#35328;&#27169;&#22411;&#32452;&#21512;&#25512;&#29702;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;ALMs&#22312;&#29702;&#35299;&#38899;&#39057;&#20013;&#22768;&#38899;&#20107;&#20214;&#30340;&#39034;&#24207;&#21644;&#23646;&#24615;&#32465;&#23450;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.08753</link><description>&lt;p&gt;
CompA: &#35299;&#20915;&#38899;&#39057;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32452;&#21512;&#25512;&#29702;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
CompA: Addressing the Gap in Compositional Reasoning in Audio-Language Models. (arXiv:2310.08753v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08753
&lt;/p&gt;
&lt;p&gt;
CompA&#25552;&#20986;&#20102;&#30001;&#20004;&#20010;&#19987;&#23478;&#27880;&#37322;&#30340;&#38899;&#39057;-&#35821;&#35328;&#27169;&#22411;&#32452;&#21512;&#25512;&#29702;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;ALMs&#22312;&#29702;&#35299;&#38899;&#39057;&#20013;&#22768;&#38899;&#20107;&#20214;&#30340;&#39034;&#24207;&#21644;&#23646;&#24615;&#32465;&#23450;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;&#30340;&#22522;&#26412;&#29305;&#24615;&#26159;&#20854;&#32452;&#21512;&#24615;&#12290;&#20351;&#29992;&#23545;&#27604;&#26041;&#27861;&#65288;&#20363;&#22914;CLAP&#65289;&#35757;&#32451;&#30340;&#38899;&#39057;-&#35821;&#35328;&#27169;&#22411;&#65288;ALMs&#65289;&#33021;&#22815;&#23398;&#20064;&#38899;&#39057;&#21644;&#35821;&#35328;&#27169;&#24577;&#20043;&#38388;&#30340;&#20849;&#20139;&#34920;&#31034;&#65292;&#20174;&#32780;&#22312;&#35768;&#22810;&#19979;&#28216;&#24212;&#29992;&#20013;&#25552;&#39640;&#24615;&#33021;&#65292;&#21253;&#25324;&#38646;&#26679;&#26412;&#38899;&#39057;&#20998;&#31867;&#12289;&#38899;&#39057;&#26816;&#32034;&#31561;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#26377;&#25928;&#25191;&#34892;&#32452;&#21512;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#36824;&#24456;&#23569;&#34987;&#25506;&#32034;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;CompA&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#20004;&#20010;&#19987;&#23478;&#27880;&#37322;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#26159;&#30495;&#23454;&#19990;&#30028;&#30340;&#38899;&#39057;&#26679;&#26412;&#65292;&#29992;&#20110;&#35780;&#20272;ALMs&#30340;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;CompA-order&#35780;&#20272;ALMs&#22312;&#29702;&#35299;&#38899;&#39057;&#20013;&#22768;&#38899;&#20107;&#20214;&#30340;&#39034;&#24207;&#25110;&#21457;&#29983;&#26102;&#30340;&#34920;&#29616;&#22914;&#20309;&#65292;&#32780;CompA-attribute&#35780;&#20272;&#22768;&#38899;&#20107;&#20214;&#30340;&#23646;&#24615;&#32465;&#23450;&#12290;&#27599;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#30340;&#23454;&#20363;&#21253;&#21547;&#20004;&#20010;&#38899;&#39057;-&#26631;&#39064;&#23545;&#65292;&#20854;&#20013;&#20004;&#20010;&#38899;&#39057;&#20855;&#26377;&#30456;&#21516;&#30340;&#22768;&#38899;&#20107;&#20214;&#65292;&#20294;&#32452;&#21512;&#26041;&#24335;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
A fundamental characteristic of audio is its compositional nature. Audio-language models (ALMs) trained using a contrastive approach (e.g., CLAP) that learns a shared representation between audio and language modalities have improved performance in many downstream applications, including zero-shot audio classification, audio retrieval, etc. However, the ability of these models to effectively perform compositional reasoning remains largely unexplored and necessitates additional research. In this paper, we propose CompA, a collection of two expert-annotated benchmarks with a majority of real-world audio samples, to evaluate compositional reasoning in ALMs. Our proposed CompA-order evaluates how well an ALM understands the order or occurrence of acoustic events in audio, and CompA-attribute evaluates attribute binding of acoustic events. An instance from either benchmark consists of two audio-caption pairs, where both audios have the same acoustic events but with different compositions. A
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#35777;&#26126;&#20102;&#22312;Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#30005;&#36335;&#32452;&#20214;&#21487;&#20197;&#22312;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#22797;&#29992;&#24182;&#20135;&#29983;&#30456;&#20284;&#30340;&#21151;&#33021;&#65292;&#20026;&#26356;&#39640;&#32423;&#30340;&#27169;&#22411;&#29702;&#35299;&#20570;&#20986;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2310.08744</link><description>&lt;p&gt;
Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#36328;&#20219;&#21153;&#30340;&#30005;&#36335;&#32452;&#20214;&#22797;&#29992;
&lt;/p&gt;
&lt;p&gt;
Circuit Component Reuse Across Tasks in Transformer Language Models. (arXiv:2310.08744v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08744
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#35777;&#26126;&#20102;&#22312;Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#30005;&#36335;&#32452;&#20214;&#21487;&#20197;&#22312;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#22797;&#29992;&#24182;&#20135;&#29983;&#30456;&#20284;&#30340;&#21151;&#33021;&#65292;&#20026;&#26356;&#39640;&#32423;&#30340;&#27169;&#22411;&#29702;&#35299;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#26426;&#21046;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#30005;&#36335;&#20998;&#26512;&#21487;&#20197;&#25104;&#21151;&#22320;&#36870;&#21521;&#24037;&#31243;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#24120;&#35265;&#30340;&#25209;&#35780;&#26159;&#27599;&#20010;&#30005;&#36335;&#37117;&#26159;&#20219;&#21153;&#29305;&#23450;&#30340;&#65292;&#22240;&#27492;&#36825;&#26679;&#30340;&#20998;&#26512;&#19981;&#33021;&#20026;&#26356;&#39640;&#32423;&#30340;&#29702;&#35299;&#27169;&#22411;&#20570;&#20986;&#36129;&#29486;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#35777;&#25454;&#34920;&#26126;&#27934;&#23519;&#21147;&#65288;&#20851;&#20110;&#29305;&#23450;&#22836;&#37096;&#30340;&#20302;&#32423;&#21457;&#29616;&#21644;&#20851;&#20110;&#19968;&#33324;&#31639;&#27861;&#30340;&#39640;&#32423;&#21457;&#29616;&#65289;&#30830;&#23454;&#21487;&#20197;&#22312;&#20219;&#21153;&#20043;&#38388;&#36827;&#34892;&#27867;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;Wang&#31561;&#20154;&#65288;2022&#65289;&#22312;&#38388;&#25509;&#23486;&#35821;&#35782;&#21035;&#20219;&#21153;&#65288;IOI&#65289;&#20013;&#21457;&#29616;&#30340;&#30005;&#36335;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#20010;&#30005;&#36335;&#22312;&#26356;&#22823;&#30340;GPT2&#27169;&#22411;&#19978;&#30340;&#37325;&#29616;&#65292;&#20197;&#21450;&#22312;&#30475;&#20284;&#19981;&#21516;&#30340;&#20219;&#21153;&#20013;&#22823;&#37096;&#20998;&#34987;&#22797;&#29992;&#26469;&#35299;&#20915;&#38382;&#39064;&#65306;&#24425;&#33394;&#29289;&#20307;&#65288;Ippolito&#21644;Callison-Burch&#65292;2023&#65289;&#12290;&#25105;&#20204;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#20004;&#20010;&#20219;&#21153;&#24213;&#23618;&#30340;&#36807;&#31243;&#22312;&#21151;&#33021;&#19978;&#38750;&#24120;&#30456;&#20284;&#65292;&#24182;&#19988;&#22312;&#30005;&#36335;&#20013;&#30340;&#27880;&#24847;&#21147;&#22836;&#37096;&#20043;&#38388;&#26377;&#22823;&#32422;78&#65285;&#30340;&#37325;&#21472;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#19968;&#20010;&#27010;&#24565;&#39564;&#35777;&#24178;&#39044;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Recent work in mechanistic interpretability has shown that behaviors in language models can be successfully reverse-engineered through circuit analysis. A common criticism, however, is that each circuit is task-specific, and thus such analysis cannot contribute to understanding the models at a higher level. In this work, we present evidence that insights (both low-level findings about specific heads and higher-level findings about general algorithms) can indeed generalize across tasks. Specifically, we study the circuit discovered in Wang et al. (2022) for the Indirect Object Identification (IOI) task and 1.) show that it reproduces on a larger GPT2 model, and 2.) that it is mostly reused to solve a seemingly different task: Colored Objects (Ippolito &amp; Callison-Burch, 2023). We provide evidence that the process underlying both tasks is functionally very similar, and contains about a 78% overlap in in-circuit attention heads. We further present a proof-of-concept intervention experiment
&lt;/p&gt;</description></item><item><title>&#36825;&#31181;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#35821;&#35328;&#20195;&#29702;&#26426;&#21046;&#65292;&#23427;&#19981;&#38656;&#35201;&#19987;&#23478;&#31034;&#36394;&#65292;&#24182;&#19988;&#36890;&#36807;&#33258;&#25105;&#21453;&#24605;&#21644;&#32467;&#26500;&#21270;&#24605;&#32771;&#31649;&#29702;&#26469;&#23398;&#20064;&#21644;&#25913;&#21892;&#35745;&#31639;&#26426;&#19978;&#30340;&#25511;&#21046;&#65292;&#34920;&#29616;&#20986;&#39640;&#25928;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.08740</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#35745;&#31639;&#26426;&#25511;&#21046;&#30340;&#38646;&#26679;&#26412;&#35821;&#35328;&#20195;&#29702;&#26426;&#21046;&#21450;&#20854;&#32467;&#26500;&#21453;&#24605;
&lt;/p&gt;
&lt;p&gt;
A Zero-Shot Language Agent for Computer Control with Structured Reflection. (arXiv:2310.08740v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08740
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31181;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#35821;&#35328;&#20195;&#29702;&#26426;&#21046;&#65292;&#23427;&#19981;&#38656;&#35201;&#19987;&#23478;&#31034;&#36394;&#65292;&#24182;&#19988;&#36890;&#36807;&#33258;&#25105;&#21453;&#24605;&#21644;&#32467;&#26500;&#21270;&#24605;&#32771;&#31649;&#29702;&#26469;&#23398;&#20064;&#21644;&#25913;&#21892;&#35745;&#31639;&#26426;&#19978;&#30340;&#25511;&#21046;&#65292;&#34920;&#29616;&#20986;&#39640;&#25928;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35745;&#21010;&#21644;&#25191;&#34892;&#39640;&#32423;&#30446;&#26631;&#26041;&#38754;&#30340;&#33021;&#21147;&#19981;&#26029;&#22686;&#24378;&#65292;&#22914;&#22312;&#27963;&#21160;&#30340;&#35745;&#31639;&#26426;&#29615;&#22659;&#65288;&#20363;&#22914;MiniWoB ++&#65289;&#20013;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#24120;&#35201;&#27714;&#27169;&#22411;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#25110;&#23569;/&#22810;&#26679;&#26412;&#25552;&#31034;&#20174;&#20219;&#21153;&#30340;&#36319;&#36394;&#31034;&#20363;&#20013;&#23398;&#20064;&#12290;&#22312;&#27809;&#26377;&#36825;&#20123;&#36319;&#36394;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#65292;&#19968;&#20010;&#20195;&#29702;&#26426;&#21046;&#22914;&#20309;&#33021;&#22815;&#33258;&#20027;&#23398;&#20064;&#24182;&#25913;&#21892;&#22312;&#35745;&#31639;&#26426;&#19978;&#30340;&#25511;&#21046;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#36825;&#38480;&#21046;&#20102;&#19968;&#20010;&#20195;&#29702;&#26426;&#26500;&#25191;&#34892;&#26032;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#38646;&#26679;&#26412;&#20195;&#29702;&#26426;&#21046;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23427;&#19981;&#38656;&#35201;&#32473;&#23450;&#30340;&#19987;&#23478;&#31034;&#36394;&#12290;&#25105;&#20204;&#30340;&#20195;&#29702;&#26426;&#21046;&#23545;&#20110;&#37096;&#20998;&#35266;&#23519;&#29615;&#22659;&#19978;&#30340;&#21487;&#25191;&#34892;&#34892;&#21160;&#36827;&#34892;&#35268;&#21010;&#65292;&#24182;&#36890;&#36807;&#33258;&#25105;&#21453;&#24605;&#21644;&#32467;&#26500;&#21270;&#24605;&#32771;&#31649;&#29702;&#26469;&#35782;&#21035;&#21644;&#23398;&#20064;&#38169;&#35823;&#65292;&#20174;&#32780;&#36880;&#27493;&#25512;&#36827;&#20219;&#21153;&#12290;&#22312;MiniWoB ++&#30340;&#31616;&#21333;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#38646;&#26679;&#26412;&#20195;&#29702;&#26426;&#21046;&#24448;&#24448;&#32988;&#36807;&#26368;&#36817;&#30340;SoTA&#65292;&#20855;&#26377;&#26356;&#39640;&#25928;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#23545;&#20110;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#30340;&#21453;&#24605;&#20195;&#29702;&#26426;&#21046;&#19982;&#20808;&#21069;&#30340;&#20195;&#29702;&#26426;&#21046;&#25345;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown increasing capacity at planning and executing a high-level goal in a live computer environment (e.g. MiniWoB++). To perform a task, recent works often require a model to learn from trace examples of the task via either supervised learning or few/many-shot prompting. Without these trace examples, it remains a challenge how an agent can autonomously learn and improve its control on a computer, which limits the ability of an agent to perform a new task. We approach this problem with a zero-shot agent that requires no given expert traces. Our agent plans for executable actions on a partially observed environment, and iteratively progresses a task by identifying and learning from its mistakes via self-reflection and structured thought management. On the easy tasks of MiniWoB++, we show that our zero-shot agent often outperforms recent SoTAs, with more efficient reasoning. For tasks with more complexity, our reflective agent performs on par with prior 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#21512;&#35821;&#35328;&#24314;&#27169;&#20013;&#35821;&#38899;&#21333;&#20803;&#21644;&#25991;&#26412;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#21475;&#35821;&#29702;&#35299;&#20219;&#21153;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;&#28151;&#21512;&#35821;&#35328;&#27169;&#22411;&#20248;&#20110;&#20165;&#20351;&#29992;&#35821;&#38899;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.08715</link><description>&lt;p&gt;
&#36808;&#21521;&#35821;&#38899;&#21333;&#20803;&#21644;&#25991;&#26412;&#30340;&#32852;&#21512;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Toward Joint Language Modeling for Speech Units and Text. (arXiv:2310.08715v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#21512;&#35821;&#35328;&#24314;&#27169;&#20013;&#35821;&#38899;&#21333;&#20803;&#21644;&#25991;&#26412;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#21475;&#35821;&#29702;&#35299;&#20219;&#21153;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;&#28151;&#21512;&#35821;&#35328;&#27169;&#22411;&#20248;&#20110;&#20165;&#20351;&#29992;&#35821;&#38899;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#21644;&#25991;&#26412;&#26159;&#20154;&#31867;&#35821;&#35328;&#30340;&#20004;&#31181;&#20027;&#35201;&#24418;&#24335;&#12290;&#30740;&#31350;&#30028;&#22810;&#24180;&#26469;&#19968;&#30452;&#22312;&#20851;&#27880;&#23558;&#35821;&#38899;&#26144;&#23556;&#21040;&#25991;&#26412;&#25110;&#32773;&#21453;&#20043;&#20134;&#28982;&#12290;&#28982;&#32780;&#65292;&#22312;&#35821;&#35328;&#24314;&#27169;&#39046;&#22495;&#65292;&#24456;&#23569;&#26377;&#20154;&#23581;&#35797;&#32852;&#21512;&#24314;&#27169;&#36825;&#20004;&#32773;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#35821;&#38899;&#21333;&#20803;&#21644;&#25991;&#26412;&#30340;&#32852;&#21512;&#35821;&#35328;&#24314;&#27169;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#35821;&#38899;&#20998;&#35789;&#24037;&#20855;&#65292;&#23558;&#36830;&#32493;&#30340;&#35821;&#38899;&#20449;&#21495;&#36716;&#21270;&#20026;&#31163;&#25955;&#30340;&#21333;&#20803;&#65292;&#24182;&#20351;&#29992;&#19981;&#21516;&#30340;&#26041;&#27861;&#26500;&#24314;&#28151;&#21512;&#30340;&#35821;&#38899;-&#25991;&#26412;&#25968;&#25454;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#33258;&#21160;&#25351;&#26631;&#26469;&#35780;&#20272;&#32852;&#21512;&#35821;&#35328;&#24314;&#27169;&#30340;&#35821;&#38899;&#21644;&#25991;&#26412;&#34701;&#21512;&#25928;&#26524;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#19981;&#21516;&#30340;&#27169;&#24577;&#65288;&#35821;&#38899;&#25110;&#25991;&#26412;&#65289;&#22312;&#19979;&#28216;&#30340;&#21475;&#35821;&#29702;&#35299;&#20219;&#21153;&#19978;&#23545;&#32852;&#21512;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#24182;&#27979;&#35797;&#20854;&#24615;&#33021;&#20197;&#35780;&#20272;&#27169;&#22411;&#23545;&#20849;&#20139;&#34920;&#31034;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#28151;&#21512;&#25216;&#26415;&#23558;&#35821;&#38899;&#21333;&#20803;&#21644;&#25991;&#26412;&#28151;&#21512;&#30340;&#24773;&#20917;&#19979;&#65292;&#32852;&#21512;&#35821;&#35328;&#27169;&#22411;&#22312;&#21475;&#35821;&#29702;&#35299;&#20219;&#21153;&#19978;&#20248;&#20110;&#20165;&#20351;&#29992;&#35821;&#38899;&#30340;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#38646;-shot&#30340;&#36328;&#27169;&#24577;&#36801;&#31227;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech and text are two major forms of human language. The research community has been focusing on mapping speech to text or vice versa for many years. However, in the field of language modeling, very little effort has been made to model them jointly. In light of this, we explore joint language modeling for speech units and text. Specifically, we compare different speech tokenizers to transform continuous speech signals into discrete units and use different methods to construct mixed speech-text data. We introduce automatic metrics to evaluate how well the joint LM mixes speech and text. We also fine-tune the LM on downstream spoken language understanding (SLU) tasks with different modalities (speech or text) and test its performance to assess the model's learning of shared representations. Our results show that by mixing speech units and text with our proposed mixing techniques, the joint LM improves over a speech-only baseline on SLU tasks and shows zero-shot cross-modal transferabil
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#21644;GPT-4&#22312;&#37329;&#34701;&#20998;&#26512;&#19978;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#27169;&#25311;CFA&#32771;&#35797;&#20013;&#20855;&#26377;&#19968;&#23450;&#30340;&#34920;&#29616;&#65292;&#20026;&#23558;&#26469;&#36827;&#19968;&#27493;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#37329;&#34701;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;</title><link>http://arxiv.org/abs/2310.08678</link><description>&lt;p&gt;
GPT&#27169;&#22411;&#33021;&#25104;&#20026;&#37329;&#34701;&#20998;&#26512;&#24072;&#21527;&#65311;&#23545;&#27169;&#25311;CFA&#32771;&#35797;&#20013;&#30340;ChatGPT&#21644;GPT-4&#36827;&#34892;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Can GPT models be Financial Analysts? An Evaluation of ChatGPT and GPT-4 on mock CFA Exams. (arXiv:2310.08678v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08678
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#21644;GPT-4&#22312;&#37329;&#34701;&#20998;&#26512;&#19978;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#27169;&#25311;CFA&#32771;&#35797;&#20013;&#20855;&#26377;&#19968;&#23450;&#30340;&#34920;&#29616;&#65292;&#20026;&#23558;&#26469;&#36827;&#19968;&#27493;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#37329;&#34701;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#36890;&#24120;&#33021;&#19982;&#29978;&#33267;&#36229;&#36234;&#26368;&#20808;&#36827;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;LLM&#22312;&#37329;&#34701;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#21033;&#29992;&#29305;&#35768;&#37329;&#34701;&#20998;&#26512;&#24072;&#65288;CFA&#65289;&#32771;&#35797;&#30340;&#27169;&#25311;&#39064;&#30446;&#23545;ChatGPT&#21644;GPT-4&#22312;&#37329;&#34701;&#20998;&#26512;&#20013;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#32771;&#34385;&#20102;&#38646;&#26679;&#26412;&#65288;ZS&#65289;&#12289;&#24605;&#36335;&#38142;&#65288;CoT&#65289;&#21644;&#23569;&#26679;&#26412;&#65288;FS&#65289;&#22330;&#26223;&#12290;&#25105;&#20204;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#23616;&#38480;&#24615;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#24182;&#35780;&#20272;&#23427;&#20204;&#36890;&#36807;CFA&#32771;&#35797;&#30340;&#21487;&#33021;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25552;&#39640;LLM&#22312;&#37329;&#34701;&#39046;&#22495;&#24212;&#29992;&#24615;&#30340;&#28508;&#22312;&#31574;&#30053;&#21644;&#25913;&#36827;&#12290;&#22312;&#36825;&#20010;&#35270;&#35282;&#19979;&#65292;&#25105;&#20204;&#24076;&#26395;&#35813;&#30740;&#31350;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#32487;&#32493;&#36890;&#36807;&#20005;&#26684;&#35780;&#20272;&#26469;&#25552;&#21319;LLM&#22312;&#37329;&#34701;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#38138;&#24179;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable performance on a wide range of Natural Language Processing (NLP) tasks, often matching or even beating state-of-the-art task-specific models. This study aims at assessing the financial reasoning capabilities of LLMs. We leverage mock exam questions of the Chartered Financial Analyst (CFA) Program to conduct a comprehensive evaluation of ChatGPT and GPT-4 in financial analysis, considering Zero-Shot (ZS), Chain-of-Thought (CoT), and Few-Shot (FS) scenarios. We present an in-depth analysis of the models' performance and limitations, and estimate whether they would have a chance at passing the CFA exams. Finally, we outline insights into potential strategies and improvements to enhance the applicability of LLMs in finance. In this perspective, we hope this work paves the way for future studies to continue enhancing LLMs for financial reasoning through rigorous evaluation.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LoftQ&#65306;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;LoRA&#31934;&#35843;&#24863;&#30693;&#37327;&#21270;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#21516;&#26102;&#23545;LLM&#36827;&#34892;&#37327;&#21270;&#65292;&#24182;&#20026;LoRA&#31934;&#35843;&#25214;&#21040;&#36866;&#24403;&#30340;&#20302;&#31209;&#21021;&#22987;&#21270;&#65292;&#20197;&#32531;&#35299;&#37327;&#21270;&#27169;&#22411;&#21644;&#20840;&#31934;&#24230;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.08659</link><description>&lt;p&gt;
LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models
&lt;/p&gt;
&lt;p&gt;
LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models. (arXiv:2310.08659v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LoftQ&#65306;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;LoRA&#31934;&#35843;&#24863;&#30693;&#37327;&#21270;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#21516;&#26102;&#23545;LLM&#36827;&#34892;&#37327;&#21270;&#65292;&#24182;&#20026;LoRA&#31934;&#35843;&#25214;&#21040;&#36866;&#24403;&#30340;&#20302;&#31209;&#21021;&#22987;&#21270;&#65292;&#20197;&#32531;&#35299;&#37327;&#21270;&#27169;&#22411;&#21644;&#20840;&#31934;&#24230;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#26159;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#26381;&#21153;&#30340;&#19981;&#21487;&#25110;&#32570;&#30340;&#25216;&#26415;&#65292;&#24182;&#26368;&#36817;&#34987;&#24212;&#29992;&#20110;LoRA&#31934;&#35843;&#20013;&#12290;&#26412;&#25991;&#20851;&#27880;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#21516;&#26102;&#24212;&#29992;&#37327;&#21270;&#21644;LoRA&#31934;&#35843;&#30340;&#22330;&#26223;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#24120;&#24120;&#35266;&#23519;&#21040;&#23436;&#25972;&#31934;&#35843;&#21644;&#37327;&#21270;&#21152;LoRA&#31934;&#35843;&#26041;&#27861;&#20043;&#38388;&#22312;&#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#19978;&#23384;&#22312;&#19968;&#33268;&#30340;&#24046;&#36317;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LoftQ&#65288;LoRA-Fine-Tuning-aware Quantization&#65289;&#8212;&#8212;&#19968;&#31181;&#26032;&#30340;&#37327;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#21516;&#26102;&#23545;LLM&#36827;&#34892;&#37327;&#21270;&#65292;&#24182;&#25214;&#21040;&#36866;&#24403;&#30340;&#20302;&#31209;&#21021;&#22987;&#21270;&#26469;&#36827;&#34892;LoRA&#31934;&#35843;&#12290;&#36825;&#31181;&#21021;&#22987;&#21270;&#20943;&#36731;&#20102;&#37327;&#21270;&#27169;&#22411;&#21644;&#20840;&#31934;&#24230;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#38382;&#31572;&#12289;&#25688;&#35201;&#21644;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#38750;&#24120;&#26377;&#25928;&#65292;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantization is an indispensable technique for serving Large Language Models (LLMs) and has recently found its way into LoRA fine-tuning. In this work we focus on the scenario where quantization and LoRA fine-tuning are applied together on a pre-trained model. In such cases it is common to observe a consistent gap in the performance on downstream tasks between full fine-tuning and quantization plus LoRA fine-tuning approach. In response, we propose LoftQ (LoRA-Fine-Tuning-aware Quantization), a novel quantization framework that simultaneously quantizes an LLM and finds a proper low-rank initialization for LoRA fine-tuning. Such an initialization alleviates the discrepancy between the quantized and full-precision model and significantly improves the generalization in downstream tasks. We evaluate our method on natural language understanding, question answering, summarization, and natural language generation tasks. Experiments show that our method is highly effective and outperforms exis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32534;&#36753;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#25361;&#25112;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#32534;&#36753;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32534;&#36753;&#22810;&#27169;&#24335;LLMs&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#65292;&#20294;&#36825;&#39033;&#24037;&#20316;&#20026;NLP&#31038;&#21306;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.08475</link><description>&lt;p&gt;
&#25105;&#20204;&#33021;&#32534;&#36753;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can We Edit Multimodal Large Language Models?. (arXiv:2310.08475v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08475
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32534;&#36753;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#25361;&#25112;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#32534;&#36753;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32534;&#36753;&#22810;&#27169;&#24335;LLMs&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#65292;&#20294;&#36825;&#39033;&#24037;&#20316;&#20026;NLP&#31038;&#21306;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#32534;&#36753;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#12290;&#19982;&#32534;&#36753;&#21333;&#27169;&#24335;LLMs&#30456;&#27604;&#65292;&#22810;&#27169;&#24335;&#27169;&#22411;&#30340;&#32534;&#36753;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#38656;&#35201;&#26356;&#39640;&#32423;&#21035;&#30340;&#23457;&#26597;&#21644;&#24910;&#37325;&#32771;&#34385;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#31216;&#20026;MMEdit&#65292;&#29992;&#20110;&#32534;&#36753;&#22810;&#27169;&#24335;LLMs&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#22871;&#21019;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#21253;&#25324;&#21508;&#31181;&#27169;&#22411;&#32534;&#36753;&#22522;&#32447;&#30340;&#32508;&#21512;&#23454;&#39564;&#65292;&#24182;&#20998;&#26512;&#20102;&#32534;&#36753;&#22810;&#27169;&#24335;LLMs&#30340;&#19981;&#21516;&#32452;&#20214;&#30340;&#24433;&#21709;&#12290;&#26681;&#25454;&#32463;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#20043;&#21069;&#30340;&#22522;&#32447;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#21487;&#20197;&#23454;&#29616;&#32534;&#36753;&#22810;&#27169;&#24335;LLMs&#65292;&#20294;&#25928;&#26524;&#20173;&#28982;&#19981;&#29702;&#24819;&#65292;&#34920;&#26126;&#36825;&#20010;&#20219;&#21153;&#21487;&#33021;&#23384;&#22312;&#30340;&#22256;&#38590;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#24037;&#20316;&#33021;&#20026;NLP&#31038;&#21306;&#25552;&#20379;&#35265;&#35299;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#21487;&#22312;https://github.com/zjunlp/EasyEdit&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we focus on editing Multimodal Large Language Models (MLLMs). Compared to editing single-modal LLMs, multimodal model editing is more challenging, which demands a higher level of scrutiny and careful consideration in the editing process. To facilitate research in this area, we construct a new benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite of innovative metrics for evaluation. We conduct comprehensive experiments involving various model editing baselines and analyze the impact of editing different components for multimodal LLMs. Empirically, we notice that previous baselines can implement editing multimodal LLMs to some extent, but the effect is still barely satisfactory, indicating the potential difficulty of this task. We hope that our work can provide the NLP community with insights. Code and dataset are available in https://github.com/zjunlp/EasyEdit.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#38750;&#33521;&#35821;&#35821;&#35328;&#20013;&#35757;&#32451;&#38382;&#31572;&#31995;&#32479;&#30340;&#31616;&#21333;&#19988;&#32463;&#27982;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#35843;&#25972;&#25351;&#23548;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#36991;&#20813;&#20102;&#20154;&#21147;&#25104;&#26412;&#65292;&#24182;&#23637;&#31034;&#20102;&#21512;&#25104;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#19982;&#25163;&#21160;&#25972;&#29702;&#30340;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#30340;&#21487;&#27604;&#36739;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08072</link><description>&lt;p&gt;
&#20174;&#35757;&#32451;&#27169;&#22411;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#20013;&#35757;&#32451;&#29983;&#25104;&#24335;&#38382;&#31572;&#31995;&#32479;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Training Generative Question-Answering on Synthetic Data Obtained from an Instruct-tuned Mo. (arXiv:2310.08072v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08072
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#38750;&#33521;&#35821;&#35821;&#35328;&#20013;&#35757;&#32451;&#38382;&#31572;&#31995;&#32479;&#30340;&#31616;&#21333;&#19988;&#32463;&#27982;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#35843;&#25972;&#25351;&#23548;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#36991;&#20813;&#20102;&#20154;&#21147;&#25104;&#26412;&#65292;&#24182;&#23637;&#31034;&#20102;&#21512;&#25104;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#19982;&#25163;&#21160;&#25972;&#29702;&#30340;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#30340;&#21487;&#27604;&#36739;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#32463;&#27982;&#30340;&#26041;&#27861;&#26469;&#21512;&#25104;&#25968;&#25454;&#20197;&#35757;&#32451;&#38382;&#31572;&#31995;&#32479;&#12290;&#22312;&#36164;&#28304;&#20805;&#36275;&#30340;&#33521;&#35821;&#31561;&#35821;&#35328;&#20013;&#65292;&#35843;&#25972;GPT&#27169;&#22411;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20294;&#26159;&#22312;&#38750;&#33521;&#35821;&#35821;&#35328;&#20013;&#30001;&#20110;&#32570;&#20047;&#36275;&#22815;&#30340;&#38382;&#31572;&#23545;&#65292;&#36825;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20351;&#29992;&#22312;&#20154;&#31867;&#20316;&#32773;&#30340;&#38382;&#31572;&#23545;&#19978;&#35757;&#32451;&#30340;&#38382;&#31572;&#29983;&#25104;&#27169;&#22411;&#65292;&#36825;&#28041;&#21450;&#30456;&#24403;&#22823;&#30340;&#20154;&#21147;&#25104;&#26412;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#35843;&#25972;&#25351;&#23548;&#27169;&#22411;&#20197;&#38646;&#26679;&#26412;&#25110;&#23569;&#37327;&#26679;&#26412;&#30340;&#26041;&#24335;&#29983;&#25104;&#38382;&#31572;&#23545;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;&#20174;&#25351;&#23548;&#27169;&#22411;&#33719;&#21462;&#38382;&#31572;&#23545;&#30340;&#21508;&#31181;&#31574;&#30053;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#21512;&#25104;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#19982;&#22312;&#25163;&#21160;&#25972;&#29702;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#24403;&#65292;&#32780;&#26080;&#38656;&#25215;&#25285;&#20154;&#21147;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a simple and cost-effective method for synthesizing data to train question-answering systems. For training, fine-tuning GPT models is a common practice in resource-rich languages like English, however, it becomes challenging for non-English languages due to the scarcity of sufficient question-answer (QA) pairs. Existing approaches use question and answer generators trained on human-authored QA pairs, which involves substantial human expenses. In contrast, we use an instruct-tuned model to generate QA pairs in a zero-shot or few-shot manner. We conduct experiments to compare various strategies for obtaining QA pairs from the instruct-tuned model. The results demonstrate that a model trained on our proposed synthetic data achieves comparable performance to a model trained on manually curated datasets, without incurring human costs.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#22312;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#28508;&#21147;&#21644;&#38480;&#21046;&#12290;&#30740;&#31350;&#32467;&#26524;&#21457;&#29616;&#65292;&#20027;&#35266;&#24615;&#20250;&#36127;&#38754;&#24433;&#21709;&#27169;&#22411;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#12290;&#36825;&#23545;&#20110;&#29702;&#35299;&#21644;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#30340;&#26377;&#25928;&#24615;&#20855;&#26377;&#37325;&#35201;&#30340;&#21551;&#31034;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.07849</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;&#65306;&#28508;&#21147;&#21644;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations. (arXiv:2310.07849v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07849
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#22312;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#28508;&#21147;&#21644;&#38480;&#21046;&#12290;&#30740;&#31350;&#32467;&#26524;&#21457;&#29616;&#65292;&#20027;&#35266;&#24615;&#20250;&#36127;&#38754;&#24433;&#21709;&#27169;&#22411;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#12290;&#36825;&#23545;&#20110;&#29702;&#35299;&#21644;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#30340;&#26377;&#25928;&#24615;&#20855;&#26377;&#37325;&#35201;&#30340;&#21551;&#31034;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25910;&#38598;&#21644;&#25972;&#29702;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#23545;&#20110;&#24320;&#21457;&#20855;&#26377;&#21331;&#36234;&#24615;&#33021;&#30340;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#24448;&#24448;&#20276;&#38543;&#30528;&#24040;&#22823;&#30340;&#25104;&#26412;&#21644;&#26102;&#38388;&#25237;&#20837;&#12290;&#30740;&#31350;&#20154;&#21592;&#26368;&#36817;&#24320;&#22987;&#25506;&#32034;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#38598;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;LLM&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#22312;&#25903;&#25345;&#27169;&#22411;&#35757;&#32451;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#22312;&#19981;&#21516;&#30340;&#20998;&#31867;&#20219;&#21153;&#20013;&#26159;&#19981;&#19968;&#33268;&#30340;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#20102;&#35299;&#35843;&#33410;LLM&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#26377;&#25928;&#24615;&#30340;&#22240;&#32032;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20998;&#31867;&#30340;&#20027;&#35266;&#24615;&#22914;&#20309;&#24433;&#21709;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20027;&#35266;&#24615;&#22312;&#20219;&#21153;&#23618;&#38754;&#21644;&#23454;&#20363;&#23618;&#38754;&#19978;&#37117;&#19982;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#21576;&#36127;&#30456;&#20851;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#25105;&#20204;&#30340;&#24037;&#20316;&#23545;&#20110;&#21033;&#29992;LLM&#26469;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#22312;&#28508;&#21147;&#21644;&#38480;&#21046;&#26041;&#38754;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The collection and curation of high-quality training data is crucial for developing text classification models with superior performance, but it is often associated with significant costs and time investment. Researchers have recently explored using large language models (LLMs) to generate synthetic datasets as an alternative approach. However, the effectiveness of the LLM-generated synthetic data in supporting model training is inconsistent across different classification tasks. To better understand factors that moderate the effectiveness of the LLM-generated synthetic data, in this study, we look into how the performance of models trained on these synthetic data may vary with the subjectivity of classification. Our results indicate that subjectivity, at both the task level and instance level, is negatively associated with the performance of the model trained on synthetic data. We conclude by discussing the implications of our work on the potential and limitations of leveraging LLM fo
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#25968;&#25454;&#38598;&#31579;&#36873;&#26694;&#26550;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#20010;&#24615;&#21270;&#38754;&#21521;&#30446;&#26631;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#21517;&#20026;TopDial&#12290;&#35813;&#25968;&#25454;&#38598;&#36136;&#37327;&#39640;&#65292;&#26377;&#21161;&#20110;&#25506;&#32034;&#20010;&#24615;&#21270;&#30446;&#26631;&#23548;&#21521;&#30340;&#23545;&#35805;&#12290;</title><link>http://arxiv.org/abs/2310.07397</link><description>&lt;p&gt;
&#38754;&#21521;&#30446;&#26631;&#30340;&#20010;&#24615;&#21270;&#20027;&#21160;&#23545;&#35805;&#31995;&#32479;&#65306;&#38382;&#39064;&#24418;&#24335;&#21270;&#19982;&#25968;&#25454;&#38598;&#31579;&#36873;
&lt;/p&gt;
&lt;p&gt;
Target-oriented Proactive Dialogue Systems with Personalization: Problem Formulation and Dataset Curation. (arXiv:2310.07397v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07397
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#25968;&#25454;&#38598;&#31579;&#36873;&#26694;&#26550;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#20010;&#24615;&#21270;&#38754;&#21521;&#30446;&#26631;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#21517;&#20026;TopDial&#12290;&#35813;&#25968;&#25454;&#38598;&#36136;&#37327;&#39640;&#65292;&#26377;&#21161;&#20110;&#25506;&#32034;&#20010;&#24615;&#21270;&#30446;&#26631;&#23548;&#21521;&#30340;&#23545;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#21521;&#30446;&#26631;&#30340;&#23545;&#35805;&#31995;&#32479;&#26088;&#22312;&#20027;&#21160;&#24341;&#23548;&#23545;&#35805;&#26397;&#21521;&#39044;&#23450;&#30340;&#30446;&#26631;&#25110;&#36798;&#25104;&#29305;&#23450;&#30340;&#31995;&#32479;&#30446;&#26631;&#65292;&#22312;&#23545;&#35805;&#23436;&#25104;&#36807;&#31243;&#20013;&#32771;&#34385;&#20010;&#24615;&#21270;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#65292;&#20174;&#38646;&#24320;&#22987;&#26500;&#24314;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#35282;&#33394;&#25198;&#28436;&#26041;&#27861;&#30340;&#33258;&#21160;&#25968;&#25454;&#38598;&#31579;&#36873;&#26694;&#26550;&#12290;&#22522;&#20110;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#20010;&#24615;&#21270;&#38754;&#21521;&#30446;&#26631;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#21517;&#20026;TopDial&#65292;&#21253;&#21547;&#32422;18K&#20010;&#22810;&#36718;&#23545;&#35805;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#25968;&#25454;&#38598;&#20855;&#26377;&#24456;&#39640;&#30340;&#36136;&#37327;&#65292;&#21487;&#20197;&#29992;&#20110;&#25506;&#32034;&#20010;&#24615;&#21270;&#30446;&#26631;&#23548;&#21521;&#30340;&#23545;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;
Target-oriented dialogue systems, designed to proactively steer conversations toward predefined targets or accomplish specific system-side goals, are an exciting area in conversational AI. In this work, by formulating a &lt;dialogue act, topic&gt; pair as the conversation target, we explore a novel problem of personalized target-oriented dialogue by considering personalization during the target accomplishment process. However, there remains an emergent need for high-quality datasets, and building one from scratch requires tremendous human effort. To address this, we propose an automatic dataset curation framework using a role-playing approach. Based on this framework, we construct a large-scale personalized target-oriented dialogue dataset, TopDial, which comprises about 18K multi-turn dialogues. The experimental results show that this dataset is of high quality and could contribute to exploring personalized target-oriented dialogue.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#24503;&#35821;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#23558;&#25968;&#25454;&#22810;&#26679;&#24615;&#32622;&#20110;&#25968;&#25454;&#36136;&#37327;&#20043;&#19978;&#30340;&#20132;&#21449;&#39046;&#22495;&#25968;&#25454;&#38598;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#36229;&#36807;&#20102;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.07321</link><description>&lt;p&gt;
&#20851;&#20110;&#20132;&#21449;&#39046;&#22495;&#25968;&#25454;&#23545;&#24503;&#35821;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
On the Impact of Cross-Domain Data on German Language Models. (arXiv:2310.07321v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07321
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#24503;&#35821;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#23558;&#25968;&#25454;&#22810;&#26679;&#24615;&#32622;&#20110;&#25968;&#25454;&#36136;&#37327;&#20043;&#19978;&#30340;&#20132;&#21449;&#39046;&#22495;&#25968;&#25454;&#38598;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#36229;&#36807;&#20102;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35201;&#20040;&#22312;&#36890;&#29992;&#32593;&#32476;&#25235;&#21462;&#25968;&#25454;&#19978;&#35757;&#32451;&#65292;&#35201;&#20040;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#25968;&#25454;&#19978;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#36817;&#25104;&#21151;&#31361;&#26174;&#20102;&#20132;&#21449;&#39046;&#22495;&#25968;&#25454;&#38598;&#30340;&#22909;&#22788;&#12290;&#20026;&#20102;&#32771;&#23519;&#25968;&#25454;&#22810;&#26679;&#24615;&#39640;&#20110;&#36136;&#37327;&#30340;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#20116;&#20010;&#39046;&#22495;&#25991;&#26412;&#30340;&#24503;&#35821;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19968;&#20010;&#26088;&#22312;&#21253;&#21547;&#39640;&#36136;&#37327;&#25968;&#25454;&#30340;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#22312;&#36825;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#21442;&#25968;&#33539;&#22260;&#20174;122M&#21040;750M&#30340;&#19968;&#31995;&#21015;&#27169;&#22411;&#65292;&#25105;&#20204;&#23545;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#20132;&#21449;&#39046;&#22495;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#20248;&#20110;&#20165;&#20351;&#29992;&#36136;&#37327;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#22312;&#20808;&#21069;&#26368;&#20808;&#36827;&#32467;&#26524;&#19978;&#25552;&#20986;&#20102;&#39640;&#36798;4.45%&#30340;&#25913;&#36827;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#22312;https://huggingface.co/ikim-uk-essen&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditionally, large language models have been either trained on general web crawls or domain-specific data. However, recent successes of generative large language models, have shed light on the benefits of cross-domain datasets. To examine the significance of prioritizing data diversity over quality, we present a German dataset comprising texts from five domains, along with another dataset aimed at containing high-quality data. Through training a series of models ranging between 122M and 750M parameters on both datasets, we conduct a comprehensive benchmark on multiple downstream tasks. Our findings demonstrate that the models trained on the cross-domain dataset outperform those trained on quality data alone, leading to improvements up to $4.45\%$ over the previous state-of-the-art. The models are available at https://huggingface.co/ikim-uk-essen
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20934;&#30830;&#31232;&#30095;&#24494;&#35843;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;L2&#33539;&#25968;&#30340;&#33976;&#39311;&#26041;&#27861;SquareHead&#65292;&#21487;&#20197;&#22312;&#39640;&#31232;&#30095;&#24615;&#19979;&#23454;&#29616;&#20934;&#30830;&#30340;&#24674;&#22797;&#65307;&#21516;&#26102;&#23637;&#31034;&#20102;&#31232;&#30095;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#38469;&#25928;&#29575;&#65292;&#21487;&#22312;CPU&#21644;GPU&#36816;&#34892;&#26102;&#23454;&#29616;&#21152;&#36895;&#65292;&#24182;&#19988;&#35266;&#23519;&#21040;&#22312;&#21463;&#20869;&#23384;&#38480;&#21046;&#30340;&#27169;&#22411;&#20013;&#65292;&#31232;&#30095;&#24615;&#20063;&#21487;&#29992;&#20110;&#20943;&#23569;&#20869;&#23384;&#24102;&#23485;&#12290;</title><link>http://arxiv.org/abs/2310.06927</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31232;&#30095;&#24494;&#35843;&#30340;&#25512;&#29702;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
Sparse Finetuning for Inference Acceleration of Large Language Models. (arXiv:2310.06927v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06927
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20934;&#30830;&#31232;&#30095;&#24494;&#35843;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;L2&#33539;&#25968;&#30340;&#33976;&#39311;&#26041;&#27861;SquareHead&#65292;&#21487;&#20197;&#22312;&#39640;&#31232;&#30095;&#24615;&#19979;&#23454;&#29616;&#20934;&#30830;&#30340;&#24674;&#22797;&#65307;&#21516;&#26102;&#23637;&#31034;&#20102;&#31232;&#30095;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#38469;&#25928;&#29575;&#65292;&#21487;&#22312;CPU&#21644;GPU&#36816;&#34892;&#26102;&#23454;&#29616;&#21152;&#36895;&#65292;&#24182;&#19988;&#35266;&#23519;&#21040;&#22312;&#21463;&#20869;&#23384;&#38480;&#21046;&#30340;&#27169;&#22411;&#20013;&#65292;&#31232;&#30095;&#24615;&#20063;&#21487;&#29992;&#20110;&#20943;&#23569;&#20869;&#23384;&#24102;&#23485;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#35757;&#32451;&#36807;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#31934;&#30830;&#30340;&#31232;&#30095;&#24494;&#35843;&#65292;&#21363;&#22312;&#19987;&#38376;&#20219;&#21153;&#19978;&#23545;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#21516;&#26102;&#22312;&#26435;&#37325;&#19978;&#24341;&#20837;&#31232;&#30095;&#24615;&#12290;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22522;&#20110;&#25439;&#22833;&#30340;&#26631;&#20934;&#24494;&#35843;&#21487;&#33021;&#26080;&#27861;&#24674;&#22797;&#20934;&#30830;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#31232;&#30095;&#24773;&#20917;&#19979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23545;&#33976;&#39311;&#31867;&#22411;&#30340;&#25439;&#22833;&#36827;&#34892;&#20102;&#35814;&#32454;&#30740;&#31350;&#65292;&#30830;&#23450;&#20102;&#19968;&#31181;&#22522;&#20110;L2&#33539;&#25968;&#30340;&#33976;&#39311;&#26041;&#27861;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;SquareHead&#65292;&#21363;&#20351;&#22312;&#26356;&#39640;&#30340;&#31232;&#30095;&#24615;&#19979;&#65292;&#23427;&#20063;&#33021;&#23454;&#29616;&#20934;&#30830;&#30340;&#24674;&#22797;&#65292;&#36866;&#29992;&#20110;&#25152;&#26377;&#27169;&#22411;&#31867;&#22411;&#12290;&#22312;&#23454;&#38469;&#25928;&#29575;&#26041;&#38754;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31232;&#30095;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#31232;&#30095;&#24615;&#22312;CPU&#21644;GPU&#36816;&#34892;&#26102;&#23454;&#29616;&#21152;&#36895;&#12290;&#34429;&#28982;&#26631;&#20934;&#26041;&#27861;&#26159;&#21033;&#29992;&#31232;&#30095;&#24615;&#36827;&#34892;&#35745;&#31639;&#20943;&#23569;&#65292;&#20294;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#21463;&#20869;&#23384;&#38480;&#21046;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#31232;&#30095;&#24615;&#20063;&#21487;&#20197;&#29992;&#20110;&#20943;&#23569;&#20869;&#23384;&#24102;&#23485;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#30001;&#20110;&#31232;&#30095;&#24615;&#23548;&#33268;&#30340;&#36895;&#24230;&#25552;&#21319;&#20197;&#21450;&#24674;&#22797;&#20934;&#30830;&#24615;&#30340;&#31471;&#21040;&#31471;&#32467;&#26524;&#65292;&#24212;&#29992;&#20110;T5 (&#35821;&#35328;&#32763;&#35793;)&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of accurate sparse finetuning of large language models (LLMs), that is, finetuning pretrained LLMs on specialized tasks, while inducing sparsity in their weights. On the accuracy side, we observe that standard loss-based finetuning may fail to recover accuracy, especially at high sparsities. To address this, we perform a detailed study of distillation-type losses, determining an L2-based distillation approach we term SquareHead which enables accurate recovery even at higher sparsities, across all model types. On the practical efficiency side, we show that sparse LLMs can be executed with speedups by taking advantage of sparsity, for both CPU and GPU runtimes. While the standard approach is to leverage sparsity for computational reduction, we observe that in the case of memory-bound LLMs sparsity can also be leveraged for reducing memory bandwidth. We exhibit end-to-end results showing speedups due to sparsity, while recovering accuracy, on T5 (language translati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#23398;&#20064;&#31867;&#27604;&#25512;&#29702;&#30340;&#20219;&#21153;&#65292;&#24182;&#27979;&#35797;&#20102;&#20960;&#31181;&#23398;&#20064;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#23569;&#37327;&#25968;&#25454;&#23398;&#20064;&#31867;&#27604;&#25512;&#29702;&#65292;&#24182;&#22312;&#19982;&#20154;&#31867;&#22522;&#20934;&#36827;&#34892;&#27604;&#36739;&#21518;&#25509;&#36817;&#20154;&#31867;&#30340;&#34920;&#29616;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2310.05597</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#23398;&#20064;&#31867;&#27604;&#25512;&#29702;&#65311;&#30740;&#31350;&#35757;&#32451;&#30446;&#26631;&#21644;&#19982;&#20154;&#31867;&#34920;&#29616;&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can language models learn analogical reasoning? Investigating training objectives and comparisons to human performance. (arXiv:2310.05597v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#23398;&#20064;&#31867;&#27604;&#25512;&#29702;&#30340;&#20219;&#21153;&#65292;&#24182;&#27979;&#35797;&#20102;&#20960;&#31181;&#23398;&#20064;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#23569;&#37327;&#25968;&#25454;&#23398;&#20064;&#31867;&#27604;&#25512;&#29702;&#65292;&#24182;&#22312;&#19982;&#20154;&#31867;&#22522;&#20934;&#36827;&#34892;&#27604;&#36739;&#21518;&#25509;&#36817;&#20154;&#31867;&#30340;&#34920;&#29616;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#31867;&#27604;&#26159;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#35789;&#23884;&#20837;&#30340;&#24120;&#35265;&#26041;&#24335;&#65292;&#20294;&#30740;&#31350;&#31867;&#27604;&#25512;&#29702;&#26159;&#21542;&#26159;&#19968;&#31181;&#21487;&#20197;&#23398;&#20064;&#30340;&#20219;&#21153;&#20063;&#24456;&#26377;&#24847;&#20041;&#12290;&#26412;&#25991;&#27979;&#35797;&#20102;&#20960;&#31181;&#23398;&#20064;&#22522;&#26412;&#31867;&#27604;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#20851;&#27880;&#30340;&#26159;&#37027;&#20123;&#26356;&#31526;&#21512;&#20154;&#31867;&#31867;&#27604;&#25512;&#29702;&#35780;&#20272;&#26631;&#20934;&#30340;&#31867;&#27604;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#21457;&#29616;&#65292;&#27169;&#22411;&#33021;&#22815;&#22312;&#23569;&#37327;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#31867;&#27604;&#25512;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;&#25105;&#20204;&#30340;&#27169;&#22411;&#19982;&#20855;&#26377;&#20154;&#31867;&#22522;&#20934;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#21457;&#29616;&#22312;&#35757;&#32451;&#21518;&#65292;&#27169;&#22411;&#25509;&#36817;&#20154;&#31867;&#30340;&#34920;&#29616;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
While analogies are a common way to evaluate word embeddings in NLP, it is also of interest to investigate whether or not analogical reasoning is a task in itself that can be learned. In this paper, we test several ways to learn basic analogical reasoning, specifically focusing on analogies that are more typical of what is used to evaluate analogical reasoning in humans than those in commonly used NLP benchmarks. Our experiments find that models are able to learn analogical reasoning, even with a small amount of data. We additionally compare our models to a dataset with a human baseline, and find that after training, models approach human performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36845;&#20195;&#34701;&#21512;&#27169;&#24577;&#30456;&#20284;&#36335;&#24452;&#23454;&#29616;&#36890;&#29992;&#30340;&#22810;&#27169;&#24577;&#23454;&#20307;&#23545;&#40784;&#26041;&#27861;&#12290;&#36890;&#36807;&#32479;&#19968;&#24314;&#27169;&#21644;&#26377;&#25928;&#20449;&#24687;&#34701;&#21512;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#27169;&#24577;&#24314;&#27169;&#19981;&#19968;&#33268;&#21644;&#20302;&#25928;&#20197;&#21450;&#27169;&#24577;&#34701;&#21512;&#25928;&#26524;&#19981;&#20339;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.05364</link><description>&lt;p&gt;
&#36890;&#36807;&#36845;&#20195;&#22320;&#34701;&#21512;&#27169;&#24577;&#30456;&#20284;&#36335;&#24452;&#23454;&#29616;&#36890;&#29992;&#30340;&#22810;&#27169;&#24577;&#23454;&#20307;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Universal Multi-modal Entity Alignment via Iteratively Fusing Modality Similarity Paths. (arXiv:2310.05364v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36845;&#20195;&#34701;&#21512;&#27169;&#24577;&#30456;&#20284;&#36335;&#24452;&#23454;&#29616;&#36890;&#29992;&#30340;&#22810;&#27169;&#24577;&#23454;&#20307;&#23545;&#40784;&#26041;&#27861;&#12290;&#36890;&#36807;&#32479;&#19968;&#24314;&#27169;&#21644;&#26377;&#25928;&#20449;&#24687;&#34701;&#21512;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#27169;&#24577;&#24314;&#27169;&#19981;&#19968;&#33268;&#21644;&#20302;&#25928;&#20197;&#21450;&#27169;&#24577;&#34701;&#21512;&#25928;&#26524;&#19981;&#20339;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#23545;&#40784;&#30340;&#30446;&#26631;&#26159;&#20174;&#22810;&#20010;&#30693;&#35782;&#22270;&#35889;&#20013;&#30830;&#23450;&#31561;&#20215;&#30340;&#23454;&#20307;&#23545;&#65292;&#24182;&#21019;&#24314;&#19968;&#20010;&#26356;&#20840;&#38754;&#21644;&#32479;&#19968;&#30340;&#30693;&#35782;&#22270;&#35889;&#12290;&#22823;&#22810;&#25968;&#23454;&#20307;&#23545;&#40784;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#30693;&#35782;&#22270;&#35889;&#30340;&#32467;&#26500;&#27169;&#24577;&#65292;&#32570;&#20047;&#23545;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#25506;&#32034;&#12290;&#23569;&#25968;&#22810;&#27169;&#24577;&#23454;&#20307;&#23545;&#40784;&#26041;&#27861;&#22312;&#36825;&#20010;&#39046;&#22495;&#20570;&#20986;&#20102;&#19981;&#38169;&#30340;&#23581;&#35797;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23384;&#22312;&#20004;&#20010;&#32570;&#28857;&#65306;(1)&#27169;&#24577;&#24314;&#27169;&#19981;&#19968;&#33268;&#19988;&#20302;&#25928;&#65292;&#20026;&#27599;&#20010;&#27169;&#24577;&#35774;&#35745;&#22797;&#26434;&#21644;&#29420;&#31435;&#30340;&#27169;&#22411;&#65307;(2)&#30001;&#20110;&#23454;&#20307;&#23545;&#40784;&#20013;&#27169;&#24577;&#30340;&#24322;&#26500;&#24615;&#65292;&#27169;&#24577;&#34701;&#21512;&#25928;&#26524;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PathFusion&#65292;&#23427;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#37096;&#20998;&#65306;(1) MSP&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#36830;&#25509;&#23454;&#20307;&#21644;&#27169;&#24577;&#33410;&#28857;&#20197;&#34920;&#31034;&#22810;&#20010;&#27169;&#24577;&#30340;&#36335;&#24452;&#65292;&#31616;&#21270;&#20102;&#23545;&#40784;&#36807;&#31243;&#65307;(2) IRF&#65292;&#19968;&#31181;&#36845;&#20195;&#34701;&#21512;&#26041;&#27861;&#65292;&#20351;&#29992;&#36335;&#24452;&#20316;&#20026;&#20449;&#24687;&#36733;&#20307;&#65292;&#26377;&#25928;&#22320;&#23558;&#19981;&#21516;&#27169;&#24577;&#30340;&#20449;&#24687;&#32467;&#21512;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
The objective of Entity Alignment (EA) is to identify equivalent entity pairs from multiple Knowledge Graphs (KGs) and create a more comprehensive and unified KG. The majority of EA methods have primarily focused on the structural modality of KGs, lacking exploration of multi-modal information. A few multi-modal EA methods have made good attempts in this field. Still, they have two shortcomings: (1) inconsistent and inefficient modality modeling that designs complex and distinct models for each modality; (2) ineffective modality fusion due to the heterogeneous nature of modalities in EA. To tackle these challenges, we propose PathFusion, consisting of two main components: (1) MSP, a unified modeling approach that simplifies the alignment process by constructing paths connecting entities and modality nodes to represent multiple modalities; (2) IRF, an iterative fusion method that effectively combines information from different modalities using the path as an information carrier. Experim
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#20154;&#26684;&#20559;&#35265;&#23545;&#31038;&#20132;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#26694;&#26550;&#26469;&#34913;&#37327;&#19981;&#21516;&#20154;&#26684;&#37319;&#29992;&#19979;&#30340;&#20559;&#35265;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.05280</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#38543;&#26426;&#40550;&#40521;&#26356;&#21361;&#38505;&#21527;&#65311;&#35780;&#20272;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#20154;&#26684;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Are Personalized Stochastic Parrots More Dangerous? Evaluating Persona Biases in Dialogue Systems. (arXiv:2310.05280v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05280
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#20154;&#26684;&#20559;&#35265;&#23545;&#31038;&#20132;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#26694;&#26550;&#26469;&#34913;&#37327;&#19981;&#21516;&#20154;&#26684;&#37319;&#29992;&#19979;&#30340;&#20559;&#35265;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#20351;&#20854;&#33021;&#22815;&#25353;&#29031;&#33258;&#30001;&#24418;&#24335;&#30340;&#25351;&#20196;&#36827;&#34892;&#25805;&#20316;&#65292;&#21253;&#25324;&#22312;&#23545;&#35805;&#20013;&#27169;&#20223;&#36890;&#29992;&#25110;&#29305;&#23450;&#20154;&#21475;&#32676;&#20307;&#30340;&#20154;&#26684;&#12290;&#36890;&#29992;&#20154;&#26684;&#25351;&#30340;&#26159;&#26469;&#33258;&#26576;&#19968;&#20154;&#21475;&#32676;&#20307;&#30340;&#20010;&#20307;&#65288;&#20363;&#22914;&#20122;&#27954;&#20154;&#65289;&#65292;&#32780;&#29305;&#23450;&#20154;&#26684;&#21487;&#20197;&#26159;&#21382;&#21490;&#20154;&#29289;&#30340;&#23454;&#38469;&#22995;&#21517;&#12290;&#34429;&#28982;&#37319;&#29992;&#20154;&#26684;&#20351;&#23545;&#35805;&#31995;&#32479;&#26356;&#20855;&#21560;&#24341;&#21147;&#21644;&#20146;&#21644;&#21147;&#65292;&#20294;&#20063;&#23384;&#22312;&#28508;&#22312;&#39118;&#38505;&#65292;&#21487;&#33021;&#36890;&#36807;&#19982;&#29992;&#25143;&#30340;&#20132;&#20114;&#32780;&#21152;&#21095;&#31038;&#20250;&#20559;&#35265;&#65292;&#36827;&#19968;&#27493;&#36896;&#25104;&#31038;&#20250;&#20260;&#23475;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#8220;&#20154;&#26684;&#20559;&#35265;&#8221;&#65292;&#25105;&#20204;&#23558;&#20854;&#23450;&#20041;&#20026;&#26377;&#23475;&#23545;&#35805;&#27169;&#22411;&#34892;&#20026;&#23545;&#19981;&#21516;&#20154;&#26684;&#37319;&#29992;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#23558;&#20154;&#26684;&#20559;&#35265;&#20998;&#20026;&#26377;&#23475;&#34920;&#36798;&#21644;&#26377;&#23475;&#35748;&#21516;&#20004;&#31867;&#65292;&#21516;&#26102;&#24314;&#31435;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#20197;&#34913;&#37327;&#20116;&#20010;&#26041;&#38754;&#30340;&#20154;&#26684;&#20559;&#35265;&#65306;&#20882;&#29359;&#24615;&#12289;&#26377;&#27602;&#24310;&#32493;&#12289;&#20851;&#24576;&#12289;&#21051;&#26495;&#21360;&#35937;&#30340;&#35748;&#21516;&#20197;&#21450;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in Large Language Models empower them to follow freeform instructions, including imitating generic or specific demographic personas in conversations. Generic personas refer to an individual from a demographic group (e.g. an Asian person), whereas specific personas can be actual names of historical figures. While the adoption of personas allows dialogue systems to be more engaging and approachable to users, it also carries the potential risk of exacerbating social biases in model responses, further causing societal harms through interactions with users. In this paper, we systematically study "persona biases", which we define to be the sensitivity of harmful dialogue model behaviors to different persona adoptions. We categorize persona biases into biases in harmful expression and harmful agreement, as well as establish a comprehensive evaluation framework to measure persona biases in five aspects: Offensiveness, Toxic Continuation, Regard, Stereotype Agreement, and To
&lt;/p&gt;</description></item><item><title>Text2NKG&#26159;&#19968;&#31181;&#29992;&#20110;&#26500;&#24314;N&#20803;&#20851;&#31995;&#30693;&#35782;&#22270;&#30340;&#32454;&#31890;&#24230;N&#20803;&#20851;&#31995;&#25277;&#21462;&#26694;&#26550;&#65292;&#25903;&#25345;&#22810;&#31181;NKG&#27169;&#24335;&#65292;&#20855;&#26377;&#39640;&#28789;&#27963;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.05185</link><description>&lt;p&gt;
Text2NKG: &#38754;&#21521;N&#20803;&#20851;&#31995;&#30693;&#35782;&#22270;&#26500;&#24314;&#30340;&#32454;&#31890;&#24230;N&#20803;&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
Text2NKG: Fine-Grained N-ary Relation Extraction for N-ary relational Knowledge Graph Construction. (arXiv:2310.05185v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05185
&lt;/p&gt;
&lt;p&gt;
Text2NKG&#26159;&#19968;&#31181;&#29992;&#20110;&#26500;&#24314;N&#20803;&#20851;&#31995;&#30693;&#35782;&#22270;&#30340;&#32454;&#31890;&#24230;N&#20803;&#20851;&#31995;&#25277;&#21462;&#26694;&#26550;&#65292;&#25903;&#25345;&#22810;&#31181;NKG&#27169;&#24335;&#65292;&#20855;&#26377;&#39640;&#28789;&#27963;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38500;&#20102;&#20256;&#32479;&#30340;&#20108;&#20803;&#20851;&#31995;&#20107;&#23454;&#22806;&#65292;N&#20803;&#20851;&#31995;&#30693;&#35782;&#22270;(NKGs)&#30001;&#21253;&#21547;&#20004;&#20010;&#20197;&#19978;&#23454;&#20307;&#30340;N&#20803;&#20851;&#31995;&#20107;&#23454;&#32452;&#25104;&#65292;&#26356;&#25509;&#36817;&#20110;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#30340;&#30495;&#23454;&#19990;&#30028;&#20107;&#23454;&#12290;&#28982;&#32780;&#65292;NKG&#30340;&#26500;&#24314;&#20173;&#28982;&#20005;&#37325;&#20381;&#36182;&#20110;&#20154;&#24037;&#21171;&#21160;&#65292;&#24182;&#19988;N&#20803;&#20851;&#31995;&#25277;&#21462;&#20173;&#28982;&#20572;&#30041;&#22312;&#31895;&#31890;&#24230;&#27700;&#24179;&#65292;&#36890;&#24120;&#26159;&#22312;&#21333;&#19968;&#27169;&#24335;&#21644;&#22266;&#23450;&#30340;&#23454;&#20307;&#25968;&#37327;&#19978;&#25805;&#20316;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Text2NKG&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#38754;&#21521;N&#20803;&#20851;&#31995;&#30693;&#35782;&#22270;&#26500;&#24314;&#30340;&#32454;&#31890;&#24230;N&#20803;&#20851;&#31995;&#25277;&#21462;&#26694;&#26550;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36328;&#24230;&#20803;&#32452;&#20998;&#31867;&#26041;&#27861;&#65292;&#24182;&#37319;&#29992;&#24322;&#26500;&#25490;&#24207;&#21512;&#24182;&#26469;&#23454;&#29616;&#19981;&#21516;&#24230;&#30340;&#32454;&#31890;&#24230;N&#20803;&#20851;&#31995;&#25277;&#21462;&#12290;&#27492;&#22806;&#65292;Text2NKG&#25903;&#25345;&#22235;&#31181;&#20856;&#22411;&#30340;NKG&#27169;&#24335;&#65306;&#36229;&#20851;&#31995;&#27169;&#24335;&#12289;&#22522;&#20110;&#20107;&#20214;&#30340;&#27169;&#24335;&#12289;&#22522;&#20110;&#35282;&#33394;&#30340;&#27169;&#24335;&#21644;&#36229;&#22270;&#27169;&#24335;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#28789;&#27963;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Text2NKG&#30340;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#30340;N&#20803;&#20851;&#31995;&#25277;&#21462;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Beyond traditional binary relational facts, n-ary relational knowledge graphs (NKGs) are comprised of n-ary relational facts containing more than two entities, which are closer to real-world facts with broader applications. However, the construction of NKGs still significantly relies on manual labor, and n-ary relation extraction still remains at a course-grained level, which is always in a single schema and fixed arity of entities. To address these restrictions, we propose Text2NKG, a novel fine-grained n-ary relation extraction framework for n-ary relational knowledge graph construction. We introduce a span-tuple classification approach with hetero-ordered merging to accomplish fine-grained n-ary relation extraction in different arity. Furthermore, Text2NKG supports four typical NKG schemas: hyper-relational schema, event-based schema, role-based schema, and hypergraph-based schema, with high flexibility and practicality. Experimental results demonstrate that Text2NKG outperforms the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#22270;&#20013;&#33410;&#28857;&#36827;&#34892;&#26080;&#26631;&#31614;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#21363;LLM-GNN&#12290;&#23427;&#21033;&#29992;LLMs&#23545;&#19968;&#23567;&#37096;&#20998;&#33410;&#28857;&#36827;&#34892;&#27880;&#37322;&#65292;&#28982;&#21518;&#36890;&#36807;&#23545;LLMs&#30340;&#27880;&#37322;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#24471;GNN&#33021;&#22815;&#23545;&#20854;&#20313;&#22823;&#37096;&#20998;&#33410;&#28857;&#36827;&#34892;&#39044;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#20805;&#20998;&#21457;&#25381;&#20102;GNNs&#21644;LLMs&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#23427;&#20204;&#22312;&#22788;&#29702;&#32467;&#26500;&#21270;&#25968;&#25454;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.04668</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMS&#65289;&#23545;&#22270;&#20013;&#30340;&#33410;&#28857;&#36827;&#34892;&#26080;&#26631;&#31614;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Label-free Node Classification on Graphs with Large Language Models (LLMS). (arXiv:2310.04668v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#22270;&#20013;&#33410;&#28857;&#36827;&#34892;&#26080;&#26631;&#31614;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#21363;LLM-GNN&#12290;&#23427;&#21033;&#29992;LLMs&#23545;&#19968;&#23567;&#37096;&#20998;&#33410;&#28857;&#36827;&#34892;&#27880;&#37322;&#65292;&#28982;&#21518;&#36890;&#36807;&#23545;LLMs&#30340;&#27880;&#37322;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#24471;GNN&#33021;&#22815;&#23545;&#20854;&#20313;&#22823;&#37096;&#20998;&#33410;&#28857;&#36827;&#34892;&#39044;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#20805;&#20998;&#21457;&#25381;&#20102;GNNs&#21644;LLMs&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#23427;&#20204;&#22312;&#22788;&#29702;&#32467;&#26500;&#21270;&#25968;&#25454;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;Graph Neural Networks&#65292;GNNs&#65289;&#22312;&#33410;&#28857;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#30830;&#20445;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#39640;&#36136;&#37327;&#30340;&#26631;&#31614;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;Large Language Models&#65292;LLMs&#65289;&#22312;&#25991;&#26412;&#23646;&#24615;&#22270;&#19978;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38646;&#26679;&#23398;&#20064;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#39640;&#25928;&#22788;&#29702;&#32467;&#26500;&#21270;&#25968;&#25454;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#24182;&#19988;&#25512;&#29702;&#25104;&#26412;&#36739;&#39640;&#12290;&#37492;&#20110;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;LLMs&#30340;&#26080;&#26631;&#31614;&#22270;&#33410;&#28857;&#20998;&#31867;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;LLM-GNN&#12290;&#23427;&#38598;&#25104;&#20102;GNNs&#21644;LLMs&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#20943;&#36731;&#20102;&#23427;&#20204;&#30340;&#38480;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LLMs&#34987;&#29992;&#26469;&#27880;&#37322;&#19968;&#23567;&#37096;&#20998;&#33410;&#28857;&#65292;&#28982;&#21518;&#36890;&#36807;&#23545;LLMs&#30340;&#27880;&#37322;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;GNNs&#33021;&#22815;&#39044;&#27979;&#20854;&#20313;&#22823;&#37096;&#20998;&#33410;&#28857;&#12290;LLM-GNN&#30340;&#23454;&#29616;&#38754;&#20020;&#19968;&#20010;&#29420;&#29305;&#30340;&#25361;&#25112;&#65306;&#25105;&#20204;&#22914;&#20309;&#20027;&#21160;&#36873;&#25321;&#35201;&#30001;LLMs&#27880;&#37322;&#30340;&#33410;&#28857;&#65292;&#20174;&#32780;&#22686;&#24378;GNN&#30340;&#35757;&#32451;&#65311;&#25105;&#20204;&#22914;&#20309;&#21033;&#29992;LLMs&#26469;&#20248;&#21270;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#22788;&#29702;&#65311;
&lt;/p&gt;
&lt;p&gt;
In recent years, there have been remarkable advancements in node classification achieved by Graph Neural Networks (GNNs). However, they necessitate abundant high-quality labels to ensure promising performance. In contrast, Large Language Models (LLMs) exhibit impressive zero-shot proficiency on text-attributed graphs. Yet, they face challenges in efficiently processing structural data and suffer from high inference costs. In light of these observations, this work introduces a label-free node classification on graphs with LLMs pipeline, LLM-GNN. It amalgamates the strengths of both GNNs and LLMs while mitigating their limitations. Specifically, LLMs are leveraged to annotate a small portion of nodes and then GNNs are trained on LLMs' annotations to make predictions for the remaining large portion of nodes. The implementation of LLM-GNN faces a unique challenge: how can we actively select nodes for LLMs to annotate and consequently enhance the GNN training? How can we leverage LLMs to ob
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#20154;&#31867;&#31227;&#21160;&#38382;&#39064;&#22238;&#31572;&#65288;MobQA&#65289;&#65292;&#26088;&#22312;&#35753;&#26234;&#33021;&#31995;&#32479;&#20174;&#31227;&#21160;&#25968;&#25454;&#20013;&#23398;&#20064;&#24182;&#22238;&#31572;&#30456;&#20851;&#38382;&#39064;&#65292;&#22635;&#34917;&#20102;&#20851;&#20110;&#21033;&#29992;&#20154;&#31867;&#31227;&#21160;&#25968;&#25454;&#36827;&#34892;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#24182;&#20026;&#31227;&#21160;&#25512;&#33616;&#31995;&#32479;&#30340;&#30740;&#31350;&#24102;&#26469;&#20102;&#26032;&#30340;&#33539;&#24335;&#21464;&#38761;&#12290;</title><link>http://arxiv.org/abs/2310.04443</link><description>&lt;p&gt;
&#20154;&#31867;&#31227;&#21160;&#38382;&#39064;&#22238;&#31572;&#65288;&#23637;&#26395;&#35770;&#25991;&#65289;
&lt;/p&gt;
&lt;p&gt;
Human Mobility Question Answering (Vision Paper). (arXiv:2310.04443v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#20154;&#31867;&#31227;&#21160;&#38382;&#39064;&#22238;&#31572;&#65288;MobQA&#65289;&#65292;&#26088;&#22312;&#35753;&#26234;&#33021;&#31995;&#32479;&#20174;&#31227;&#21160;&#25968;&#25454;&#20013;&#23398;&#20064;&#24182;&#22238;&#31572;&#30456;&#20851;&#38382;&#39064;&#65292;&#22635;&#34917;&#20102;&#20851;&#20110;&#21033;&#29992;&#20154;&#31867;&#31227;&#21160;&#25968;&#25454;&#36827;&#34892;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#24182;&#20026;&#31227;&#21160;&#25512;&#33616;&#31995;&#32479;&#30340;&#30740;&#31350;&#24102;&#26469;&#20102;&#26032;&#30340;&#33539;&#24335;&#21464;&#38761;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#31572;&#31995;&#32479;&#24050;&#32463;&#24341;&#36215;&#20102;&#20154;&#24037;&#26234;&#33021;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#26681;&#25454;&#32473;&#23450;&#30340;&#30693;&#35782;&#28304;&#65288;&#20363;&#22914;&#35270;&#35273;&#38382;&#31572;&#20013;&#30340;&#22270;&#20687;&#65289;&#23398;&#20064;&#22238;&#31572;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#21033;&#29992;&#20154;&#31867;&#31227;&#21160;&#25968;&#25454;&#36827;&#34892;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#30340;&#30740;&#31350;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#25366;&#25496;&#20154;&#31867;&#31227;&#21160;&#25968;&#25454;&#23545;&#20110;&#26234;&#33021;&#22478;&#24066;&#35268;&#21010;&#12289;&#30123;&#24773;&#31649;&#29702;&#21644;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#31561;&#21508;&#31181;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#24341;&#20837;&#19968;&#39033;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#20154;&#31867;&#31227;&#21160;&#38382;&#39064;&#22238;&#31572;&#65288;MobQA&#65289;&#12290;&#35813;&#20219;&#21153;&#26088;&#22312;&#35753;&#26234;&#33021;&#31995;&#32479;&#20174;&#31227;&#21160;&#25968;&#25454;&#20013;&#23398;&#20064;&#24182;&#22238;&#31572;&#30456;&#20851;&#38382;&#39064;&#12290;&#35813;&#20219;&#21153;&#20026;&#31227;&#21160;&#39044;&#27979;&#30740;&#31350;&#24102;&#26469;&#20102;&#26032;&#30340;&#33539;&#24335;&#21464;&#38761;&#65292;&#24182;&#36827;&#19968;&#27493;&#20419;&#36827;&#20102;&#20154;&#31867;&#31227;&#21160;&#25512;&#33616;&#31995;&#32479;&#30340;&#30740;&#31350;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#25903;&#25345;&#36825;&#20010;&#26032;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#36825;&#31687;&#23637;&#26395;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#30340;&#21021;&#27493;&#35774;&#35745;&#21644;&#19968;&#20010;&#28508;&#22312;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Question answering (QA) systems have attracted much attention from the artificial intelligence community as they can learn to answer questions based on the given knowledge source (e.g., images in visual question answering). However, the research into question answering systems with human mobility data remains unexplored. Mining human mobility data is crucial for various applications such as smart city planning, pandemic management, and personalised recommendation system. In this paper, we aim to tackle this gap and introduce a novel task, that is, human mobility question answering (MobQA). The aim of the task is to let the intelligent system learn from mobility data and answer related questions. This task presents a new paradigm change in mobility prediction research and further facilitates the research of human mobility recommendation systems. To better support this novel research topic, this vision paper also proposes an initial design of the dataset and a potential deep learning mod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;GPT-4&#39046;&#22495;&#36866;&#24212;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#29983;&#25104;&#36807;&#31243;&#37325;&#26032;&#34920;&#36848;&#20026;&#19968;&#20010;&#8220;&#36866;&#24212;-&#26816;&#32034;-&#20462;&#35746;&#8221;&#30340;&#36807;&#31243;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29305;&#23450;&#39046;&#22495;&#29983;&#25104;&#20869;&#23481;&#38169;&#35823;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.03328</link><description>&lt;p&gt;
&#25226;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39046;&#22495;&#36866;&#24212;&#37325;&#26032;&#34920;&#36848;&#20026;&#36866;&#24212;-&#26816;&#32034;-&#20462;&#35746;
&lt;/p&gt;
&lt;p&gt;
Reformulating Domain Adaptation of Large Language Models as Adapt-Retrieve-Revise. (arXiv:2310.03328v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03328
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;GPT-4&#39046;&#22495;&#36866;&#24212;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#29983;&#25104;&#36807;&#31243;&#37325;&#26032;&#34920;&#36848;&#20026;&#19968;&#20010;&#8220;&#36866;&#24212;-&#26816;&#32034;-&#20462;&#35746;&#8221;&#30340;&#36807;&#31243;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29305;&#23450;&#39046;&#22495;&#29983;&#25104;&#20869;&#23481;&#38169;&#35823;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20687;GPT-4&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26368;&#36817;&#22312;&#19968;&#33324;&#39046;&#22495;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20196;&#20154;&#24778;&#35766;&#30340;&#38646;-shot&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#24120;&#24120;&#22312;&#29305;&#23450;&#39046;&#22495;&#65288;&#22914;&#20013;&#22269;&#27861;&#24459;&#65289;&#29983;&#25104;&#38169;&#35823;&#30340;&#20869;&#23481;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#36825;&#20123;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#36825;&#36890;&#24120;&#26159;&#30001;&#20110;&#27809;&#26377;&#21253;&#21547;&#36825;&#26679;&#19968;&#20010;&#29305;&#23450;&#39046;&#22495;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20351;&#24471;GPT-4&#26080;&#27861;&#33719;&#21462;&#39046;&#22495;&#20869;&#30340;&#30693;&#35782;&#12290;&#19968;&#20010;&#32039;&#36843;&#30340;&#25361;&#25112;&#26159;&#22312;&#39046;&#22495;&#20869;&#25968;&#25454;&#19978;&#32487;&#32493;&#35757;&#32451;&#22914;&#27492;&#22823;&#35268;&#27169;&#30340;LLM&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#29983;&#25104;&#36807;&#31243;&#37325;&#26032;&#34920;&#36848;&#20026;&#19968;&#20010;&#8220;&#36866;&#24212;-&#26816;&#32034;-&#20462;&#35746;&#8221;&#30340;&#36807;&#31243;&#65292;&#20171;&#32461;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;GPT-4&#39046;&#22495;&#36866;&#24212;&#26694;&#26550;&#12290;&#21021;&#22987;&#27493;&#39588;&#26159;&#36890;&#36807;&#22312;&#39046;&#22495;&#20869;&#25968;&#25454;&#19978;&#32487;&#32493;&#23398;&#20064;&#65292;&#23558;&#19968;&#20010;&#32463;&#27982;&#23454;&#24800;&#30340;7B LLM&#36866;&#24212;&#21040;&#30446;&#26631;&#39046;&#22495;&#12290;&#35299;&#20915;&#20219;&#21153;&#26102;&#65292;&#25105;&#20204;&#21033;&#29992;&#36866;&#24212;&#30340;LLM&#26681;&#25454;&#20219;&#21153;&#26597;&#35810;&#29983;&#25104;&#19968;&#20010;&#21021;&#31295;&#31572;&#26696;&#12290;&#28982;&#21518;&#65292;&#21021;&#31295;&#31572;&#26696;&#23558;&#29992;&#20110;&#20174;&#22806;&#37096;&#26816;&#32034;&#25903;&#25345;&#35777;&#25454;&#30340;&#20505;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models (LLMs) like GPT-4 have recently demonstrated astonishing zero-shot capabilities in general domain tasks, they often generate content with hallucinations in specific domains such as Chinese law, hindering their application in these areas. This is typically due to the absence of training data that encompasses such a specific domain, preventing GPT-4 from acquiring in-domain knowledge. A pressing challenge is that it's not plausible to continue training LLMs of such scale on in-domain data.  This paper introduces a simple and effective domain adaptation framework for GPT-4 by reformulating generation as an \textbf{adapt-retrieve-revise} process. The initial step is to \textbf{adapt} an affordable 7B LLM to the target domain by continuing learning on in-domain data. When solving a task, we leverage the adapted LLM to generate a draft answer given a task query. Then, the draft answer will be used to \textbf{retrieve} supporting evidence candidates from an externa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#22797;&#26434;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#30340;&#20915;&#31574;&#32452;&#20214;&#65292;&#36890;&#36807;&#35748;&#30693;&#36335;&#24452;&#21644;&#31639;&#27861;&#26469;&#23454;&#29616;&#20840;&#38754;&#25512;&#29702;&#21644;&#21487;&#25191;&#34892;&#39550;&#39542;&#25351;&#20196;&#30340;&#36716;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LLMs&#33021;&#22815;&#22312;&#21333;&#36710;&#20219;&#21153;&#21644;&#22797;&#26434;&#39550;&#39542;&#34892;&#20026;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#36825;&#26159;&#22240;&#20026;&#20854;&#20855;&#26377;&#24120;&#35782;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.03026</link><description>&lt;p&gt;
LanguageMPC&#65306;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#39550;&#39542;&#20915;&#31574;&#32773;
&lt;/p&gt;
&lt;p&gt;
LanguageMPC: Large Language Models as Decision Makers for Autonomous Driving. (arXiv:2310.03026v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#22797;&#26434;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#30340;&#20915;&#31574;&#32452;&#20214;&#65292;&#36890;&#36807;&#35748;&#30693;&#36335;&#24452;&#21644;&#31639;&#27861;&#26469;&#23454;&#29616;&#20840;&#38754;&#25512;&#29702;&#21644;&#21487;&#25191;&#34892;&#39550;&#39542;&#25351;&#20196;&#30340;&#36716;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LLMs&#33021;&#22815;&#22312;&#21333;&#36710;&#20219;&#21153;&#21644;&#22797;&#26434;&#39550;&#39542;&#34892;&#20026;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#36825;&#26159;&#22240;&#20026;&#20854;&#20855;&#26377;&#24120;&#35782;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#22522;&#20110;&#23398;&#20064;&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#22312;&#29702;&#35299;&#39640;&#32423;&#20449;&#24687;&#12289;&#25512;&#24191;&#32597;&#35265;&#20107;&#20214;&#21644;&#25552;&#20379;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#22797;&#26434;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#30340;&#20915;&#31574;&#32452;&#20214;&#65292;&#38656;&#35201;&#20154;&#31867;&#24120;&#35782;&#29702;&#35299;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#35748;&#30693;&#36335;&#24452;&#65292;&#20351;LLMs&#33021;&#22815;&#36827;&#34892;&#20840;&#38754;&#25512;&#29702;&#65292;&#24182;&#24320;&#21457;&#20102;&#23558;LLM&#20915;&#31574;&#36716;&#21270;&#20026;&#21487;&#25191;&#34892;&#39550;&#39542;&#25351;&#20196;&#30340;&#31639;&#27861;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;LLM&#20915;&#31574;&#36890;&#36807;&#24341;&#23548;&#21442;&#25968;&#30697;&#38453;&#36866;&#24212;&#19982;&#20302;&#32423;&#25511;&#21046;&#22120;&#26080;&#32541;&#38598;&#25104;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#19981;&#20165;&#22312;&#21333;&#36710;&#20219;&#21153;&#20013;&#22987;&#32456;&#36229;&#36234;&#22522;&#32447;&#26041;&#27861;&#65292;&#32780;&#19988;&#36824;&#33021;&#22788;&#29702;&#22797;&#26434;&#30340;&#39550;&#39542;&#34892;&#20026;&#65292;&#29978;&#33267;&#22810;&#36710;&#21327;&#35843;&#65292;&#36825;&#35201;&#24402;&#21151;&#20110;LLMs&#30340;&#24120;&#35782;&#25512;&#29702;&#33021;&#21147;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#23558;LLMs&#20316;&#20026;&#26377;&#25928;&#20915;&#31574;&#32773;&#30340;&#21021;&#27493;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing learning-based autonomous driving (AD) systems face challenges in comprehending high-level information, generalizing to rare events, and providing interpretability. To address these problems, this work employs Large Language Models (LLMs) as a decision-making component for complex AD scenarios that require human commonsense understanding. We devise cognitive pathways to enable comprehensive reasoning with LLMs, and develop algorithms for translating LLM decisions into actionable driving commands. Through this approach, LLM decisions are seamlessly integrated with low-level controllers by guided parameter matrix adaptation. Extensive experiments demonstrate that our proposed method not only consistently surpasses baseline approaches in single-vehicle tasks, but also helps handle complex driving behaviors even multi-vehicle coordination, thanks to the commonsense reasoning capabilities of LLMs. This paper presents an initial step toward leveraging LLMs as effective decision-make
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;UMLS&#30340;&#22686;&#24378;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#26088;&#22312;&#25913;&#21892;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20013;&#27169;&#22411;&#29983;&#25104;&#20869;&#23481;&#30340;&#20107;&#23454;&#24615;&#12290;&#36890;&#36807;&#33258;&#21160;&#35780;&#20272;&#21644;&#21307;&#29983;&#35780;&#20272;&#65292;&#30740;&#31350;&#20154;&#21592;&#39564;&#35777;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.02778</link><description>&lt;p&gt;
&#19968;&#20010;&#22686;&#24378;&#30340; UMLS &#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#20107;&#23454;&#24615;
&lt;/p&gt;
&lt;p&gt;
A UMLS-Augmented Framework for Improving Factuality in Large Language Models within Healthcare. (arXiv:2310.02778v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02778
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;UMLS&#30340;&#22686;&#24378;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#26088;&#22312;&#25913;&#21892;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20013;&#27169;&#22411;&#29983;&#25104;&#20869;&#23481;&#30340;&#20107;&#23454;&#24615;&#12290;&#36890;&#36807;&#33258;&#21160;&#35780;&#20272;&#21644;&#21307;&#29983;&#35780;&#20272;&#65292;&#30740;&#31350;&#20154;&#21592;&#39564;&#35777;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#65292;&#20026;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#24102;&#26469;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#21019;&#26032;&#12290;&#28982;&#32780;&#65292;&#23558;LLMs&#24212;&#29992;&#20110;&#30495;&#23454;&#20020;&#24202;&#22330;&#26223;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#29983;&#25104;&#19982;&#24050;&#24314;&#31435;&#21307;&#23398;&#20107;&#23454;&#20559;&#31163;&#30340;&#20869;&#23481;&#65292;&#29978;&#33267;&#21487;&#33021;&#34920;&#29616;&#20986;&#28508;&#22312;&#30340;&#20559;&#35265;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#32479;&#19968;&#21307;&#23398;&#35821;&#35328;&#31995;&#32479;&#65288;UMLS&#65289;&#30340;&#22686;&#24378;&#22411;LLM&#26694;&#26550;&#65292;&#26088;&#22312;&#26356;&#22909;&#22320;&#26381;&#21153;&#21307;&#30103;&#20445;&#20581;&#31038;&#21306;&#12290;&#25105;&#20204;&#37319;&#29992;LLaMa2-13b-chat&#21644;ChatGPT-3.5&#20316;&#20026;&#22522;&#20934;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;ROUGE&#20998;&#25968;&#21644;BERT&#20998;&#25968;&#22312;LiveQA&#27979;&#35797;&#38598;&#30340;104&#20010;&#38382;&#39064;&#19978;&#36827;&#34892;&#33258;&#21160;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26681;&#25454;&#20107;&#23454;&#24615;&#12289;&#23436;&#25972;&#24615;&#12289;&#21487;&#35835;&#24615;&#21644;&#30456;&#20851;&#24615;&#22235;&#20010;&#32500;&#24230;&#24314;&#31435;&#20102;&#21307;&#29983;&#35780;&#20272;&#26631;&#20934;&#12290;ChatGPT-3.5&#29992;&#20110;&#21307;&#29983;&#35780;&#20272;&#65292;&#38024;&#23545;LiveQA&#27979;&#35797;&#38598;&#30340;20&#20010;&#38382;&#39064;&#12290;&#22810;&#20301;&#20303;&#38498;&#21307;&#24072;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated powerful text generation capabilities, bringing unprecedented innovation to the healthcare field. While LLMs hold immense promise for applications in healthcare, applying them to real clinical scenarios presents significant challenges, as these models may generate content that deviates from established medical facts and even exhibit potential biases. In our research, we develop an augmented LLM framework based on the Unified Medical Language System (UMLS), aiming to better serve the healthcare community. We employ LLaMa2-13b-chat and ChatGPT-3.5 as our benchmark models, and conduct automatic evaluations using the ROUGE Score and BERTScore on 104 questions from the LiveQA test set. Additionally, we establish criteria for physician-evaluation based on four dimensions: Factuality, Completeness, Readability and Relevancy. ChatGPT-3.5 is used for physician evaluation with 20 questions on the LiveQA test set. Multiple resident physicians conduct
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#29702;&#30340;&#20855;&#36523;&#20915;&#31574;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#22522;&#20934;PCA-EVAL&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#20195;&#29702;&#21327;&#20316;&#26694;&#26550;HOLMES&#65292;&#20197;&#25552;&#39640;&#20915;&#31574;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;GPT4-Vision&#27169;&#22411;&#22312;&#31471;&#21040;&#31471;&#30340;&#20855;&#36523;&#20915;&#31574;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2310.02071</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#31471;&#21040;&#31471;&#30340;&#20855;&#36523;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond. (arXiv:2310.02071v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#29702;&#30340;&#20855;&#36523;&#20915;&#31574;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#22522;&#20934;PCA-EVAL&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#20195;&#29702;&#21327;&#20316;&#26694;&#26550;HOLMES&#65292;&#20197;&#25552;&#39640;&#20915;&#31574;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;GPT4-Vision&#27169;&#22411;&#22312;&#31471;&#21040;&#31471;&#30340;&#20855;&#36523;&#20915;&#31574;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#22312;&#25913;&#36827;&#20195;&#29702;&#30340;&#20855;&#36523;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#28508;&#21147;&#12290;&#23613;&#31649;&#30001;&#20110;&#20854;&#20808;&#36827;&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#24191;&#27867;&#30340;&#19990;&#30028;&#30693;&#35782;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#20687;GPT4-Vision&#36825;&#26679;&#30340;MLLM&#25552;&#20379;&#20102;&#22686;&#24378;&#30340;&#35270;&#35273;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#26368;&#20808;&#36827;&#30340;MLLMs&#33021;&#21542;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#22788;&#29702;&#20855;&#36523;&#20915;&#31574;&#65292;&#24182;&#19988;LLMs&#21644;MLLMs&#20043;&#38388;&#30340;&#21327;&#20316;&#26159;&#21542;&#33021;&#22686;&#24378;&#20915;&#31574;&#33021;&#21147;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;PCA-EVAL&#30340;&#26032;&#22522;&#20934;&#65292;&#35813;&#22522;&#20934;&#20174;&#24863;&#30693;&#12289;&#35748;&#30693;&#21644;&#34892;&#21160;&#30340;&#35282;&#24230;&#35780;&#20272;&#20855;&#36523;&#20915;&#31574;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HOLMES&#65292;&#19968;&#20010;&#22810;&#20195;&#29702;&#21327;&#20316;&#26694;&#26550;&#65292;&#20801;&#35768;LLMs&#21033;&#29992;MLLMs&#21644;APIs&#33719;&#21462;&#22810;&#27169;&#24577;&#20449;&#24687;&#20197;&#36827;&#34892;&#26126;&#26234;&#30340;&#20915;&#31574;&#12290;&#25105;&#20204;&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#19978;&#27604;&#36739;&#20102;&#31471;&#21040;&#31471;&#30340;&#20855;&#36523;&#20915;&#31574;&#21644;HOLMES&#65292;&#24182;&#21457;&#29616;GPT4-Vision&#27169;&#22411;&#30340;&#24615;&#33021;&#26368;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we explore the potential of Multimodal Large Language Models (MLLMs) in improving embodied decision-making processes for agents. While Large Language Models (LLMs) have been widely used due to their advanced reasoning skills and vast world knowledge, MLLMs like GPT4-Vision offer enhanced visual understanding and reasoning capabilities. We investigate whether state-of-the-art MLLMs can handle embodied decision-making in an end-to-end manner and whether collaborations between LLMs and MLLMs can enhance decision-making. To address these questions, we introduce a new benchmark called PCA-EVAL, which evaluates embodied decision-making from the perspectives of Perception, Cognition, and Action. Additionally, we propose HOLMES, a multi-agent cooperation framework that allows LLMs to leverage MLLMs and APIs to gather multimodal information for informed decision-making. We compare end-to-end embodied decision-making and HOLMES on our benchmark and find that the GPT4-Vision model 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#22270;&#20687;&#30340;&#25773;&#25918;&#21015;&#34920;&#25551;&#36848;&#21644;&#38899;&#20048;&#20027;&#39064;&#20013;&#30340;&#25991;&#26412;&#29983;&#25104;&#30340;&#25913;&#36827;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#36830;&#32493;&#21442;&#25968;&#21270;&#23454;&#29616;&#23545;&#25991;&#26412;&#39118;&#26684;&#21644;&#24773;&#24863;&#34920;&#36798;&#30340;&#31934;&#30830;&#25511;&#21046;&#65292;&#24182;&#22312;&#29983;&#25104;&#25991;&#26412;&#30456;&#20851;&#24615;&#21644;&#36830;&#36143;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.01248</link><description>&lt;p&gt;
&#22312;&#22522;&#20110;&#22270;&#20687;&#30340;&#25773;&#25918;&#21015;&#34920;&#25551;&#36848;&#21644;&#38899;&#20048;&#20027;&#39064;&#20013;&#25552;&#39640;&#24773;&#24863;&#34920;&#36798;&#21644;&#20957;&#32858;&#21147;&#30340;&#36830;&#32493;&#21442;&#25968;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improving Emotional Expression and Cohesion in Image-Based Playlist Description and Music Topics: A Continuous Parameterization Approach. (arXiv:2310.01248v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01248
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#22270;&#20687;&#30340;&#25773;&#25918;&#21015;&#34920;&#25551;&#36848;&#21644;&#38899;&#20048;&#20027;&#39064;&#20013;&#30340;&#25991;&#26412;&#29983;&#25104;&#30340;&#25913;&#36827;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#36830;&#32493;&#21442;&#25968;&#21270;&#23454;&#29616;&#23545;&#25991;&#26412;&#39118;&#26684;&#21644;&#24773;&#24863;&#34920;&#36798;&#30340;&#31934;&#30830;&#25511;&#21046;&#65292;&#24182;&#22312;&#29983;&#25104;&#25991;&#26412;&#30456;&#20851;&#24615;&#21644;&#36830;&#36143;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#22270;&#20687;&#30340;&#24179;&#21488;&#20013;&#29983;&#25104;&#25991;&#26412;&#65292;&#29305;&#21035;&#26159;&#19982;&#38899;&#20048;&#30456;&#20851;&#30340;&#20869;&#23481;&#65292;&#38656;&#35201;&#23545;&#25991;&#26412;&#26679;&#24335;&#36827;&#34892;&#31934;&#30830;&#25511;&#21046;&#24182;&#34701;&#20837;&#24773;&#24863;&#34920;&#36798;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#24448;&#24448;&#38656;&#35201;&#25511;&#21046;&#29983;&#25104;&#25991;&#26412;&#20013;&#22806;&#37096;&#22240;&#32032;&#30340;&#27604;&#20363;&#65292;&#24182;&#19988;&#20381;&#36182;&#20110;&#31163;&#25955;&#36755;&#20837;&#65292;&#32570;&#20047;&#23545;&#26399;&#26395;&#25991;&#26412;&#29983;&#25104;&#30340;&#36830;&#32493;&#25511;&#21046;&#26465;&#20214;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21463;&#25511;&#25991;&#26412;&#29983;&#25104;&#30340;&#36830;&#32493;&#21442;&#25968;&#21270;&#26041;&#27861;&#65288;CPCTG&#65289;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#20316;&#20026;&#26679;&#24335;&#23398;&#20064;&#22120;&#65292;&#38598;&#25104;&#20102;&#35821;&#20041;&#20957;&#32858;&#24230;&#65288;SC&#65289;&#21644;&#24773;&#24863;&#34920;&#36798;&#27604;&#20363;&#65288;EEP&#65289;&#30340;&#32771;&#34385;&#12290;&#36890;&#36807;&#25913;&#36827;&#22870;&#21169;&#26041;&#27861;&#21644;&#25805;&#20316;CPCTG&#27700;&#24179;&#65292;&#25105;&#20204;&#22312;&#25773;&#25918;&#21015;&#34920;&#25551;&#36848;&#21644;&#38899;&#20048;&#20027;&#39064;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;ROUGE&#20998;&#25968;&#30340;&#26174;&#33879;&#25552;&#39640;&#65292;&#34920;&#26126;&#29983;&#25104;&#30340;&#25991;&#26412;&#22312;&#30456;&#20851;&#24615;&#21644;&#36830;&#36143;&#24615;&#19978;&#24471;&#21040;&#20102;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text generation in image-based platforms, particularly for music-related content, requires precise control over text styles and the incorporation of emotional expression. However, existing approaches often need help to control the proportion of external factors in generated text and rely on discrete inputs, lacking continuous control conditions for desired text generation. This study proposes Continuous Parameterization for Controlled Text Generation (CPCTG) to overcome these limitations. Our approach leverages a Language Model (LM) as a style learner, integrating Semantic Cohesion (SC) and Emotional Expression Proportion (EEP) considerations. By enhancing the reward method and manipulating the CPCTG level, our experiments on playlist description and music topic generation tasks demonstrate significant improvements in ROUGE scores, indicating enhanced relevance and coherence in the generated text.
&lt;/p&gt;</description></item><item><title>EchoPrompt&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20419;&#20351;&#27169;&#22411;&#37325;&#26032;&#34920;&#36848;&#26597;&#35810;&#26469;&#25552;&#20379;&#25913;&#36827;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#25928;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;EchoPrompt&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2309.10687</link><description>&lt;p&gt;
EchoPrompt&#65306;&#25351;&#23548;&#27169;&#22411;&#37325;&#26032;&#34920;&#36848;&#26597;&#35810;&#20197;&#25913;&#21892;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
EchoPrompt: Instructing the Model to Rephrase Queries for Improved In-context Learning. (arXiv:2309.10687v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10687
&lt;/p&gt;
&lt;p&gt;
EchoPrompt&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20419;&#20351;&#27169;&#22411;&#37325;&#26032;&#34920;&#36848;&#26597;&#35810;&#26469;&#25552;&#20379;&#25913;&#36827;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#25928;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;EchoPrompt&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31215;&#26497;&#37319;&#29992;&#25512;&#26029;&#26102;&#25552;&#31034;&#25216;&#26415;&#65292;&#22914;&#38646;-shot&#21644;&#23569;-shot&#25552;&#31034;&#25216;&#26415;&#65292;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;EchoPrompt&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#25552;&#31034;&#27169;&#22411;&#22312;&#22238;&#31572;&#38382;&#39064;&#20043;&#21069;&#37325;&#26032;&#34920;&#36848;&#26597;&#35810;&#12290;EchoPrompt&#36866;&#29992;&#20110;&#26631;&#20934;&#21644;&#24605;&#32500;&#38142;&#25552;&#31034;&#30340;&#38646;-shot&#21644;&#23569;-shot&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;EchoPrompt&#22312;&#36825;&#22235;&#20010;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#26063;&#32676;&#30340;&#25152;&#26377;&#35774;&#32622;&#20013;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;&#36825;&#20123;&#25913;&#36827;&#35266;&#23519;&#21040;&#20102;&#21508;&#31181;&#25968;&#20540;&#25512;&#29702;&#65288;&#20363;&#22914;&#65292;GSM8K&#65292;SVAMP&#65289;&#12289;&#38405;&#35835;&#29702;&#35299;&#65288;&#20363;&#22914;DROP&#65289;&#21644;&#36923;&#36753;&#25512;&#29702;&#65288;&#20363;&#22914;Coin Flipping&#65289;&#20219;&#21153;&#20013;&#12290;&#24179;&#22343;&#32780;&#35328;&#65292;EchoPrompt&#25552;&#39640;&#20102;&#25968;&#20540;&#20219;&#21153;&#20013;code-davinci-002&#30340;&#38646;-shot-CoT&#24615;&#33021;5%&#65292;&#38405;&#35835;&#29702;&#35299;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;13%&#12290;&#25105;&#20204;&#36890;&#36807;&#28040;&#34701;&#30740;&#31350;&#30740;&#31350;&#20102;&#24433;&#21709;EchoPrompt&#26377;&#25928;&#24615;&#30340;&#22240;&#32032;&#65292;&#20854;&#20013;&#21253;&#25324;...
&lt;/p&gt;
&lt;p&gt;
Language models are achieving impressive performance on various tasks by aggressively adopting inference-time prompting techniques, such as zero-shot and few-shot prompting. In this work, we introduce EchoPrompt, a simple yet effective approach that prompts the model to rephrase its queries before answering them. EchoPrompt is adapted for both zero-shot and few-shot in-context learning with standard and chain-of-thought prompting. Experimental results show that EchoPrompt yields substantial improvements across all these settings for four families of causal language models. These improvements are observed across various numerical reasoning (e.g. GSM8K, SVAMP), reading comprehension (e.g. DROP), and logical reasoning (e.g. Coin Flipping) tasks. On average, EchoPrompt improves the Zero-shot-CoT performance of code-davinci-002 by 5% in numerical tasks and 13% in reading comprehension tasks. We investigate the factors contributing to EchoPrompt's effectiveness through ablation studies, whic
&lt;/p&gt;</description></item><item><title>MusiLingo&#26159;&#19968;&#20010;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23558;&#38899;&#20048;&#21644;&#25991;&#26412;&#30456;&#32467;&#21512;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#29983;&#25104;&#38899;&#20048;&#23383;&#24149;&#21644;&#22238;&#31572;&#38899;&#20048;&#30456;&#20851;&#30340;&#26597;&#35810;&#12290;&#36890;&#36807;&#20351;&#29992;&#25237;&#24433;&#23618;&#23545;&#40784;&#38899;&#20048;&#34920;&#31034;&#65292;&#35813;&#31995;&#32479;&#25104;&#21151;&#22320;&#23558;&#38899;&#20048;&#38899;&#39057;&#21644;&#25991;&#26412;&#29615;&#22659;&#32852;&#31995;&#36215;&#26469;&#65292;&#21516;&#26102;&#20351;&#29992;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#26469;&#25512;&#21160;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2309.08730</link><description>&lt;p&gt;
MusiLingo&#65306;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23558;&#38899;&#20048;&#21644;&#25991;&#26412;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#38899;&#20048;&#23383;&#24149;&#21644;&#26597;&#35810;&#21709;&#24212;
&lt;/p&gt;
&lt;p&gt;
MusiLingo: Bridging Music and Text with Pre-trained Language Models for Music Captioning and Query Response. (arXiv:2309.08730v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08730
&lt;/p&gt;
&lt;p&gt;
MusiLingo&#26159;&#19968;&#20010;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23558;&#38899;&#20048;&#21644;&#25991;&#26412;&#30456;&#32467;&#21512;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#29983;&#25104;&#38899;&#20048;&#23383;&#24149;&#21644;&#22238;&#31572;&#38899;&#20048;&#30456;&#20851;&#30340;&#26597;&#35810;&#12290;&#36890;&#36807;&#20351;&#29992;&#25237;&#24433;&#23618;&#23545;&#40784;&#38899;&#20048;&#34920;&#31034;&#65292;&#35813;&#31995;&#32479;&#25104;&#21151;&#22320;&#23558;&#38899;&#20048;&#38899;&#39057;&#21644;&#25991;&#26412;&#29615;&#22659;&#32852;&#31995;&#36215;&#26469;&#65292;&#21516;&#26102;&#20351;&#29992;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#26469;&#25512;&#21160;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#22312;&#22810;&#27169;&#24577;&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#28982;&#32780;&#25991;&#26412;&#21644;&#38899;&#20048;&#39046;&#22495;&#30340;&#34701;&#21512;&#20173;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MusiLingo&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#38899;&#20048;&#23383;&#24149;&#29983;&#25104;&#21644;&#38899;&#20048;&#30456;&#20851;&#26597;&#35810;&#21709;&#24212;&#30340;&#26032;&#31995;&#32479;&#12290;MusiLingo&#20351;&#29992;&#19968;&#20010;&#25237;&#24433;&#23618;&#26469;&#23545;&#40784;&#39044;&#35757;&#32451;&#30340;&#20923;&#32467;&#38899;&#20048;&#38899;&#39057;&#27169;&#22411;MERT&#21644;&#20923;&#32467;&#30340;LLaMA&#35821;&#35328;&#27169;&#22411;&#30340;&#38899;&#20048;&#34920;&#31034;&#65292;&#23454;&#29616;&#38899;&#20048;&#38899;&#39057;&#21644;&#25991;&#26412;&#29615;&#22659;&#20043;&#38388;&#30340;&#26725;&#26753;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#38899;&#20048;&#23383;&#24149;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#20351;&#29992;&#25351;&#23548;&#24615;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#12290;&#30001;&#20110;&#39640;&#36136;&#37327;&#30340;&#38899;&#20048;&#38382;&#31572;&#25968;&#25454;&#38598;&#31232;&#32570;&#65292;&#25105;&#20204;&#20174;MusicCaps&#21019;&#24314;&#20102;MusicInstruct&#65288;MI&#65289;&#25968;&#25454;&#38598;&#65292;&#19987;&#20026;&#24320;&#25918;&#24335;&#38899;&#20048;&#26597;&#35810;&#32780;&#35774;&#35745;&#12290;&#23454;&#35777;&#35780;&#20272;&#35777;&#26126;&#20102;&#23427;&#22312;&#29983;&#25104;&#38899;&#20048;&#23383;&#24149;&#21644;&#32452;&#32455;&#38899;&#20048;&#30456;&#20851;&#38382;&#31572;&#23545;&#26041;&#38754;&#30340;&#31454;&#20105;&#24615;&#34920;&#29616;&#12290;&#25105;&#20204;&#24341;&#20837;&#30340;&#25968;&#25454;&#38598;&#22312;&#20043;&#21069;&#30340;&#25968;&#25454;&#38598;&#30340;&#22522;&#30784;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown immense potential in multimodal applications, yet the convergence of textual and musical domains remains relatively unexplored. To address this gap, we present MusiLingo, a novel system for music caption generation and music-related query responses. MusiLingo employs a single projection layer to align music representations from the pre-trained frozen music audio model MERT with the frozen LLaMA language model, bridging the gap between music audio and textual contexts. We train it on an extensive music caption dataset and fine-tune it with instructional data. Due to the scarcity of high-quality music Q&amp;A datasets, we created the MusicInstruct (MI) dataset from MusicCaps, tailored for open-ended music inquiries. Empirical evaluations demonstrate its competitive performance in generating music captions and composing music-related Q&amp;A pairs. Our introduced dataset enables notable advancements beyond previous ones.
&lt;/p&gt;</description></item><item><title>DictaBERT&#26159;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;BERT&#27169;&#22411;&#65292;&#38024;&#23545;&#29616;&#20195;&#24076;&#20271;&#26469;&#35821;&#65292;&#22312;&#22823;&#22810;&#25968;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;&#23427;&#36824;&#25552;&#20379;&#20102;&#20004;&#20010;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#29256;&#26412;&#65292;&#21487;&#29992;&#20110;&#24076;&#20271;&#26469;&#35821;&#25991;&#26412;&#20998;&#26512;&#20013;&#30340;&#21069;&#32512;&#20998;&#21106;&#21644;&#24418;&#24577;&#26631;&#27880;&#20219;&#21153;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#21457;&#24067;&#26088;&#22312;&#20419;&#36827;&#24076;&#20271;&#26469;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2308.16687</link><description>&lt;p&gt;
DictaBERT: &#19968;&#27454;&#29992;&#20110;&#29616;&#20195;&#24076;&#20271;&#26469;&#35821;&#30340;&#26368;&#20808;&#36827;BERT&#22871;&#20214;&#30340;&#32763;&#35793;&#26631;&#39064;
&lt;/p&gt;
&lt;p&gt;
DictaBERT: A State-of-the-Art BERT Suite for Modern Hebrew. (arXiv:2308.16687v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16687
&lt;/p&gt;
&lt;p&gt;
DictaBERT&#26159;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;BERT&#27169;&#22411;&#65292;&#38024;&#23545;&#29616;&#20195;&#24076;&#20271;&#26469;&#35821;&#65292;&#22312;&#22823;&#22810;&#25968;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;&#23427;&#36824;&#25552;&#20379;&#20102;&#20004;&#20010;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#29256;&#26412;&#65292;&#21487;&#29992;&#20110;&#24076;&#20271;&#26469;&#35821;&#25991;&#26412;&#20998;&#26512;&#20013;&#30340;&#21069;&#32512;&#20998;&#21106;&#21644;&#24418;&#24577;&#26631;&#27880;&#20219;&#21153;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#21457;&#24067;&#26088;&#22312;&#20419;&#36827;&#24076;&#20271;&#26469;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;DictaBERT&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#29616;&#20195;&#24076;&#20271;&#26469;&#35821;&#30340;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;BERT&#27169;&#22411;&#65292;&#22312;&#22823;&#22810;&#25968;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#20004;&#20010;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#29256;&#26412;&#65292;&#26088;&#22312;&#25191;&#34892;&#24076;&#20271;&#26469;&#35821;&#25991;&#26412;&#20998;&#26512;&#30340;&#20004;&#20010;&#29305;&#23450;&#22522;&#26412;&#20219;&#21153;&#65306;&#21069;&#32512;&#20998;&#21106;&#21644;&#24418;&#24577;&#26631;&#27880;&#12290;&#36825;&#20123;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#20801;&#35768;&#20219;&#20309;&#24320;&#21457;&#20154;&#21592;&#21482;&#38656;&#35843;&#29992;HuggingFace&#27169;&#22411;&#19968;&#27425;&#21363;&#21487;&#23545;&#24076;&#20271;&#26469;&#35821;&#21477;&#23376;&#36827;&#34892;&#21069;&#32512;&#20998;&#21106;&#21644;&#24418;&#24577;&#26631;&#27880;&#65292;&#26080;&#38656;&#38598;&#25104;&#20219;&#20309;&#39069;&#22806;&#30340;&#24211;&#25110;&#20195;&#30721;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#35757;&#32451;&#30340;&#32454;&#33410;&#20197;&#21450;&#22312;&#19981;&#21516;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#27169;&#22411;&#19982;&#23637;&#31034;&#20854;&#20351;&#29992;&#30340;&#31034;&#20363;&#20195;&#30721;&#19968;&#36215;&#21457;&#24067;&#32473;&#31038;&#21306;&#12290;&#25105;&#20204;&#21457;&#24067;&#36825;&#20123;&#27169;&#22411;&#26159;&#20026;&#20102;&#24110;&#21161;&#36827;&#19968;&#27493;&#20419;&#36827;&#24076;&#20271;&#26469;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present DictaBERT, a new state-of-the-art pre-trained BERT model for modern Hebrew, outperforming existing models on most benchmarks. Additionally, we release two fine-tuned versions of the model, designed to perform two specific foundational tasks in the analysis of Hebrew texts: prefix segmentation and morphological tagging. These fine-tuned models allow any developer to perform prefix segmentation and morphological tagging of a Hebrew sentence with a single call to a HuggingFace model, without the need to integrate any additional libraries or code. In this paper we describe the details of the training as well and the results on the different benchmarks. We release the models to the community, along with sample code demonstrating their use. We release these models as part of our goal to help further research and development in Hebrew NLP.
&lt;/p&gt;</description></item><item><title>Qwen-VL&#26159;&#19968;&#31181;&#20855;&#26377;&#22810;&#21151;&#33021;&#33021;&#21147;&#30340;&#21069;&#27839;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#22312;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#12289;&#38382;&#39064;&#22238;&#31572;&#12289;&#35270;&#35273;&#23450;&#20301;&#21644;&#28789;&#27963;&#20132;&#20114;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#12290;&#23427;&#22312;&#25512;&#21160;&#22810;&#27169;&#24577;&#20154;&#24037;&#26234;&#33021;&#26041;&#38754;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2308.12966</link><description>&lt;p&gt;
Qwen-VL: &#19968;&#31181;&#20855;&#26377;&#22810;&#21151;&#33021;&#33021;&#21147;&#30340;&#21069;&#27839;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Qwen-VL: A Frontier Large Vision-Language Model with Versatile Abilities. (arXiv:2308.12966v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12966
&lt;/p&gt;
&lt;p&gt;
Qwen-VL&#26159;&#19968;&#31181;&#20855;&#26377;&#22810;&#21151;&#33021;&#33021;&#21147;&#30340;&#21069;&#27839;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#22312;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#12289;&#38382;&#39064;&#22238;&#31572;&#12289;&#35270;&#35273;&#23450;&#20301;&#21644;&#28789;&#27963;&#20132;&#20114;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#12290;&#23427;&#22312;&#25512;&#21160;&#22810;&#27169;&#24577;&#20154;&#24037;&#26234;&#33021;&#26041;&#38754;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#21517;&#20026;Qwen-VL&#30340;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#24863;&#30693;&#21644;&#29702;&#35299;&#25991;&#26412;&#21644;&#22270;&#20687;&#12290;&#21253;&#25324;Qwen-VL&#21644;Qwen-VL-Chat&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#12289;&#38382;&#39064;&#22238;&#31572;&#12289;&#35270;&#35273;&#23450;&#20301;&#21644;&#28789;&#27963;&#20132;&#20114;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#35780;&#20272;&#33539;&#22260;&#28085;&#30422;&#20102;&#38646;&#26679;&#26412;&#23383;&#24149;&#29983;&#25104;&#12289;&#35270;&#35273;&#25110;&#25991;&#26723;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#21644; grounding &#31561;&#21508;&#31181;&#20219;&#21153;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;Qwen-VL&#27604;&#29616;&#26377;&#30340;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#34920;&#29616;&#26356;&#20248;&#24322;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#20204;&#30340;&#26550;&#26500;&#12289;&#35757;&#32451;&#26041;&#27861;&#12289;&#33021;&#21147;&#21644;&#24615;&#33021;&#65292;&#24182;&#31361;&#20986;&#20102;&#23427;&#20204;&#22312;&#25512;&#21160;&#22810;&#27169;&#24577;&#20154;&#24037;&#26234;&#33021;&#26041;&#38754;&#30340;&#36129;&#29486;&#12290;&#20195;&#30721;&#12289;&#28436;&#31034;&#21644;&#27169;&#22411;&#21487;&#20197;&#22312;https://github.com/QwenLM/Qwen-VL&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the Qwen-VL series, a set of large-scale vision-language models designed to perceive and understand both text and images. Comprising Qwen-VL and Qwen-VL-Chat, these models exhibit remarkable performance in tasks like image captioning, question answering, visual localization, and flexible interaction. The evaluation covers a wide range of tasks including zero-shot captioning, visual or document visual question answering, and grounding. We demonstrate the Qwen-VL outperforms existing Large Vision Language Models (LVLMs). We present their architecture, training, capabilities, and performance, highlighting their contributions to advancing multimodal artificial intelligence. Code, demo and models are available at https://github.com/QwenLM/Qwen-VL.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#25913;&#36827;&#22312;&#20855;&#26377;&#20805;&#20998;&#35299;&#37322;&#24615;&#32972;&#26223;&#19979;&#65292;&#20351;&#29992;&#36739;&#23567;&#35821;&#35328;&#27169;&#22411;&#22238;&#31572;&#35757;&#32451;&#20013;&#26410;&#35265;&#30340;&#25361;&#25112;&#24615;&#30701;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#26159;&#20351;&#29992;&#29702;&#25454;&#29983;&#25104;&#21644;&#23494;&#38598;&#26816;&#32034;&#32467;&#21512;&#30340;&#26041;&#24335;&#65292;&#24182;&#36890;&#36807;&#29702;&#25454;&#25490;&#21517;&#27169;&#22411;&#36827;&#34892;&#35780;&#20998;&#21644;&#32452;&#21512;&#12290;&#31532;&#20108;&#31181;&#26041;&#27861;&#26159;&#20351;&#29992;&#22686;&#24378;&#26816;&#32034;&#35757;&#32451;&#25968;&#25454;&#38598;&#35757;&#32451;&#36739;&#23567;&#30340;&#25512;&#29702;&#27169;&#22411;&#65292;&#20197;&#21033;&#29992;&#38271;&#25991;&#26412;&#24207;&#21015;&#20013;&#30340;&#30456;&#20851;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2308.04711</link><description>&lt;p&gt;
&#20351;&#29992;&#29702;&#30001;&#29983;&#25104;&#21644;&#23494;&#38598;&#26816;&#32034;&#22238;&#31572;&#26410;&#30693;&#38382;&#39064;&#30340;&#36739;&#23567;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Answering Unseen Questions With Smaller Language\\Models Using Rationale Generation and Dense Retrieval. (arXiv:2308.04711v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#25913;&#36827;&#22312;&#20855;&#26377;&#20805;&#20998;&#35299;&#37322;&#24615;&#32972;&#26223;&#19979;&#65292;&#20351;&#29992;&#36739;&#23567;&#35821;&#35328;&#27169;&#22411;&#22238;&#31572;&#35757;&#32451;&#20013;&#26410;&#35265;&#30340;&#25361;&#25112;&#24615;&#30701;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#26159;&#20351;&#29992;&#29702;&#25454;&#29983;&#25104;&#21644;&#23494;&#38598;&#26816;&#32034;&#32467;&#21512;&#30340;&#26041;&#24335;&#65292;&#24182;&#36890;&#36807;&#29702;&#25454;&#25490;&#21517;&#27169;&#22411;&#36827;&#34892;&#35780;&#20998;&#21644;&#32452;&#21512;&#12290;&#31532;&#20108;&#31181;&#26041;&#27861;&#26159;&#20351;&#29992;&#22686;&#24378;&#26816;&#32034;&#35757;&#32451;&#25968;&#25454;&#38598;&#35757;&#32451;&#36739;&#23567;&#30340;&#25512;&#29702;&#27169;&#22411;&#65292;&#20197;&#21033;&#29992;&#38271;&#25991;&#26412;&#24207;&#21015;&#20013;&#30340;&#30456;&#20851;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25552;&#20379;&#36275;&#22815;&#30340;&#35299;&#37322;&#24615;&#32972;&#26223;&#30340;&#24773;&#20917;&#19979;&#65292;&#24050;&#32463;&#35777;&#26126;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#25361;&#25112;&#24615;&#30340;&#26080;&#27861;&#22312;&#35757;&#32451;&#20013;&#35265;&#36807;&#30340;&#30701;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20004;&#31181;&#36827;&#19968;&#27493;&#25913;&#36827;&#35813;&#22330;&#26223;&#30340;&#26041;&#27861;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#27880;&#37325;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#29702;&#30001;&#19982;&#36890;&#36807;&#22810;&#36718;&#23494;&#38598;&#26816;&#32034;&#31995;&#32479;&#21019;&#24314;&#30340;&#26356;&#38271;&#19978;&#19979;&#25991;&#32467;&#21512;&#36215;&#26469;&#12290;&#31532;&#19968;&#20010;&#26041;&#27861;&#65288;$RR$&#65289;&#28041;&#21450;&#35757;&#32451;&#19968;&#20010;&#29702;&#25454;&#25490;&#21517;&#27169;&#22411;&#65292;&#20197;&#35780;&#20998;&#30340;&#26041;&#24335;&#34913;&#37327;&#29983;&#25104;&#30340;&#29702;&#30001;&#21644;&#26816;&#32034;&#21040;&#30340;&#19978;&#19979;&#25991;&#30340;&#30456;&#20851;&#24615;&#21644;&#30495;&#23454;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#35780;&#20998;&#20351;&#29992;&#22810;&#31181;&#32452;&#21512;&#31574;&#30053;&#20174;&#20004;&#20010;&#30693;&#35782;&#28304;&#20013;&#33719;&#24471;&#32452;&#21512;&#19978;&#19979;&#25991;&#12290;&#23545;&#20110;&#31532;&#20108;&#31181;&#26041;&#27861;&#65288;$RATD$&#65289;&#65292;&#25105;&#20204;&#20351;&#29992;&#22686;&#24378;&#26816;&#32034;&#35757;&#32451;&#25968;&#25454;&#38598;&#35757;&#32451;&#36739;&#23567;&#30340;&#25512;&#29702;&#27169;&#22411;&#65292;&#20351;&#20854;&#33021;&#22815;&#29087;&#32451;&#22320;&#21033;&#29992;&#26469;&#33258;&#26356;&#38271;&#25991;&#26412;&#24207;&#21015;&#30340;&#30456;&#20851;&#20449;&#24687;&#65292;&#36825;&#20123;&#20449;&#24687;&#21487;&#33021;&#37096;&#20998;&#20855;&#26377;&#35777;&#25454;&#24615;&#19988;&#39057;&#32321;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
When provided with sufficient explanatory context, smaller Language Models have been shown to exhibit strong reasoning ability on challenging short-answer question-answering tasks where the questions are unseen in training. We evaluate two methods for further improvement in this setting. Both methods focus on combining rationales generated by a larger Language Model with longer contexts created from a multi-hop dense retrieval system. The first method ($\textit{RR}$) involves training a Rationale Ranking model to score both generated rationales and retrieved contexts with respect to relevance and truthfulness. We then use the scores to derive combined contexts from both knowledge sources using a number of combinatory strategies. For the second method ($\textit{RATD}$) we train a smaller Reasoning model using retrieval-augmented training datasets such that it becomes proficient at utilising relevant information from longer text sequences that may be only partially evidential and frequen
&lt;/p&gt;</description></item><item><title>MM-Vet&#26159;&#19968;&#20010;&#35780;&#20272;&#26631;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22797;&#26434;&#20219;&#21153;&#19978;&#30340;&#32508;&#21512;&#33021;&#21147;&#12290;&#35813;&#26631;&#20934;&#35299;&#20915;&#20102;&#22914;&#20309;&#32467;&#26500;&#21270;&#21644;&#35780;&#20272;&#22797;&#26434;&#22810;&#27169;&#24577;&#20219;&#21153;&#12289;&#35774;&#35745;&#36866;&#29992;&#20110;&#19981;&#21516;&#38382;&#39064;&#21644;&#22238;&#31572;&#31867;&#22411;&#30340;&#35780;&#20272;&#25351;&#26631;&#20197;&#21450;&#22914;&#20309;&#25552;&#20379;&#27169;&#22411;&#27934;&#23519;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#25972;&#21512;&#19981;&#21516;&#30340;&#26680;&#24515;&#35270;&#35273;-&#35821;&#35328;&#33021;&#21147;&#65292;MM-Vet&#23637;&#31034;&#20102;&#26377;&#36259;&#30340;&#33021;&#21147;&#21644;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.02490</link><description>&lt;p&gt;
MM-Vet: &#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#32508;&#21512;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities. (arXiv:2308.02490v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02490
&lt;/p&gt;
&lt;p&gt;
MM-Vet&#26159;&#19968;&#20010;&#35780;&#20272;&#26631;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22797;&#26434;&#20219;&#21153;&#19978;&#30340;&#32508;&#21512;&#33021;&#21147;&#12290;&#35813;&#26631;&#20934;&#35299;&#20915;&#20102;&#22914;&#20309;&#32467;&#26500;&#21270;&#21644;&#35780;&#20272;&#22797;&#26434;&#22810;&#27169;&#24577;&#20219;&#21153;&#12289;&#35774;&#35745;&#36866;&#29992;&#20110;&#19981;&#21516;&#38382;&#39064;&#21644;&#22238;&#31572;&#31867;&#22411;&#30340;&#35780;&#20272;&#25351;&#26631;&#20197;&#21450;&#22914;&#20309;&#25552;&#20379;&#27169;&#22411;&#27934;&#23519;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#25972;&#21512;&#19981;&#21516;&#30340;&#26680;&#24515;&#35270;&#35273;-&#35821;&#35328;&#33021;&#21147;&#65292;MM-Vet&#23637;&#31034;&#20102;&#26377;&#36259;&#30340;&#33021;&#21147;&#21644;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;MM-Vet&#65292;&#19968;&#20010;&#35780;&#20272;&#26631;&#20934;&#65292;&#29992;&#20110;&#26816;&#26597;&#22312;&#22797;&#26434;&#22810;&#27169;&#24577;&#20219;&#21153;&#19978;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMM&#65289;&#30340;&#34920;&#29616;&#12290;&#26368;&#36817;&#30340;LMM&#23637;&#31034;&#20102;&#21508;&#31181;&#26377;&#36259;&#30340;&#33021;&#21147;&#65292;&#20363;&#22914;&#35299;&#20915;&#20070;&#20889;&#22312;&#40657;&#26495;&#19978;&#30340;&#25968;&#23398;&#38382;&#39064;&#65292;&#25512;&#29702;&#26032;&#38395;&#22270;&#29255;&#20013;&#30340;&#20107;&#20214;&#21644;&#21517;&#20154;&#65292;&#20197;&#21450;&#35299;&#37322;&#35270;&#35273;&#31505;&#35805;&#12290;&#24555;&#36895;&#30340;&#27169;&#22411;&#36827;&#27493;&#32473;&#35780;&#20272;&#26631;&#20934;&#30340;&#24320;&#21457;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#38382;&#39064;&#21253;&#25324;&#65306;&#65288;1&#65289;&#22914;&#20309;&#31995;&#32479;&#22320;&#26500;&#24314;&#21644;&#35780;&#20272;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#65307;&#65288;2&#65289;&#22914;&#20309;&#35774;&#35745;&#36866;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#38382;&#39064;&#21644;&#22238;&#31572;&#30340;&#35780;&#20272;&#25351;&#26631;&#65307;&#65288;3&#65289;&#22914;&#20309;&#32473;&#20986;&#36229;&#20986;&#31616;&#21333;&#24615;&#33021;&#25490;&#21517;&#30340;&#27169;&#22411;&#27934;&#23519;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MM-Vet&#65292;&#22522;&#20110;&#36825;&#26679;&#19968;&#20010;&#27934;&#23519;&#65306;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#26377;&#36259;&#33021;&#21147;&#36890;&#24120;&#36890;&#36807;&#19968;&#31181;&#36890;&#25165;&#27169;&#22411;&#33021;&#22815;&#25972;&#21512;&#19981;&#21516;&#30340;&#26680;&#24515;&#35270;&#35273;-&#35821;&#35328;&#65288;VL&#65289;&#33021;&#21147;&#26469;&#23454;&#29616;&#12290;MM-Vet&#23450;&#20041;&#20102;6&#20010;&#26680;&#24515;VL&#33021;&#21147;&#65292;&#24182;&#26816;&#26597;&#20102;&#20174;&#36825;&#20123;&#33021;&#21147;&#32452;&#21512;&#20013;&#24471;&#20986;&#30340;16&#31181;&#26377;&#36259;&#30340;&#25972;&#21512;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose MM-Vet, an evaluation benchmark that examines large multimodal models (LMMs) on complicated multimodal tasks. Recent LMMs have shown various intriguing abilities, such as solving math problems written on the blackboard, reasoning about events and celebrities in news images, and explaining visual jokes. Rapid model advancements pose challenges to evaluation benchmark development. Problems include: (1) How to systematically structure and evaluate the complicated multimodal tasks; (2) How to design evaluation metrics that work well across question and answer types; and (3) How to give model insights beyond a simple performance ranking. To this end, we present MM-Vet, designed based on the insight that the intriguing ability to solve complicated tasks is often achieved by a generalist model being able to integrate different core vision-language (VL) capabilities. MM-Vet defines 6 core VL capabilities and examines the 16 integrations of interest derived from the capability combin
&lt;/p&gt;</description></item><item><title>&#26368;&#36817;&#30340;&#30740;&#31350;&#35780;&#20272;&#20102;GPT-4&#65292;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23545;&#26469;&#33258;&#22622;&#23572;&#32500;&#20122;&#35799;&#27468;&#30340;&#26032;&#39062;&#25991;&#23398;&#38544;&#21947;&#30340;&#35299;&#37322;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.01497</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20986;&#23545;&#26032;&#39062;&#25991;&#23398;&#38544;&#21947;&#30340;&#35299;&#37322;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Displays Emergent Ability to Interpret Novel Literary Metaphors. (arXiv:2308.01497v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01497
&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#35780;&#20272;&#20102;GPT-4&#65292;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23545;&#26469;&#33258;&#22622;&#23572;&#32500;&#20122;&#35799;&#27468;&#30340;&#26032;&#39062;&#25991;&#23398;&#38544;&#21947;&#30340;&#35299;&#37322;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24615;&#33021;&#26041;&#38754;&#30340;&#36827;&#23637;&#24341;&#21457;&#20102;&#20851;&#20110;&#36825;&#31181;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26159;&#21542;&#33021;&#22815;&#22312;&#36275;&#22815;&#30340;&#35757;&#32451;&#19979;&#23637;&#29616;&#20986;&#39640;&#27700;&#24179;&#20154;&#31867;&#33021;&#21147;&#30340;&#20105;&#35770;&#12290;&#23613;&#31649;LLMs&#22312;&#28041;&#21450;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#25512;&#29702;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23545;&#23427;&#20204;&#30340;&#33021;&#21147;&#26159;&#21542;&#24310;&#20280;&#21040;&#26356;&#20855;&#21019;&#36896;&#21147;&#30340;&#20154;&#31867;&#33021;&#21147;&#23384;&#22312;&#20005;&#37325;&#20998;&#27495;&#12290;&#20854;&#20013;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#26159;&#35299;&#37322;&#26032;&#39062;&#38544;&#21947;&#30340;&#33021;&#21147;&#12290;&#30001;&#20110;&#29992;&#20110;&#35757;&#32451;LLMs&#30340;&#24222;&#22823;&#19988;&#38750;&#31574;&#21010;&#30340;&#25991;&#26412;&#35821;&#26009;&#24211;&#65292;&#35774;&#35745;&#27979;&#35797;&#30340;&#19968;&#20010;&#20005;&#37325;&#38556;&#30861;&#23601;&#26159;&#38656;&#35201;&#25214;&#21040;&#26032;&#39062;&#20294;&#39640;&#36136;&#37327;&#30340;&#38544;&#21947;&#65292;&#36825;&#20123;&#38544;&#21947;&#19981;&#22826;&#21487;&#33021;&#20986;&#29616;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;GPT-4&#65292;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23545;&#26469;&#33258;&#22622;&#23572;&#32500;&#20122;&#35799;&#27468;&#24182;&#32763;&#35793;&#20026;&#33521;&#35821;&#30340;&#26032;&#39062;&#25991;&#23398;&#38544;&#21947;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in the performance of large language models (LLMs) have sparked debate over whether, given sufficient training, high-level human abilities emerge in such generic forms of artificial intelligence (AI). Despite the exceptional performance of LLMs on a wide range of tasks involving natural language processing and reasoning, there has been sharp disagreement as to whether their abilities extend to more creative human abilities. A core example is the ability to interpret novel metaphors. Given the enormous and non-curated text corpora used to train LLMs, a serious obstacle to designing tests is the requirement of finding novel yet high-quality metaphors that are unlikely to have been included in the training data. Here we assessed the ability of GPT-4, a state-of-the-art large language model, to provide natural-language interpretations of novel literary metaphors drawn from Serbian poetry and translated into English. Despite exhibiting no signs of having been exposed to thes
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#21069;&#30340;ChatGPT&#26816;&#27979;&#22120;&#19981;&#33021;&#26377;&#25928;&#21306;&#20998;&#20154;&#31867;&#29983;&#25104;&#21644;AI&#29983;&#25104;&#20869;&#23481;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#32780;&#19968;&#20010;&#39069;&#22806;&#30340;&#31354;&#26684;&#25104;&#20026;&#20102;&#35268;&#36991;&#26816;&#27979;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2307.02599</link><description>&lt;p&gt;
&#36890;&#36807;&#19968;&#20010;&#31354;&#26684;&#32469;&#36807;ChatGPT&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Evade ChatGPT Detectors via A Single Space. (arXiv:2307.02599v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#21069;&#30340;ChatGPT&#26816;&#27979;&#22120;&#19981;&#33021;&#26377;&#25928;&#21306;&#20998;&#20154;&#31867;&#29983;&#25104;&#21644;AI&#29983;&#25104;&#20869;&#23481;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#32780;&#19968;&#20010;&#39069;&#22806;&#30340;&#31354;&#26684;&#25104;&#20026;&#20102;&#35268;&#36991;&#26816;&#27979;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#24102;&#26469;&#20102;&#38761;&#21629;&#24615;&#30340;&#31038;&#20250;&#20215;&#20540;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#20110;AI&#29983;&#25104;&#20869;&#23481;&#28389;&#29992;&#30340;&#25285;&#24551;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#26159;&#22914;&#20309;&#26816;&#27979;&#20986;&#20869;&#23481;&#26159;&#30001;ChatGPT&#29983;&#25104;&#36824;&#26159;&#20154;&#31867;&#29983;&#25104;&#30340;&#12290;&#29616;&#26377;&#30340;&#26816;&#27979;&#22120;&#26159;&#24314;&#31435;&#22312;&#20154;&#31867;&#29983;&#25104;&#21644;AI&#29983;&#25104;&#20869;&#23481;&#20043;&#38388;&#23384;&#22312;&#20998;&#24067;&#24046;&#36317;&#30340;&#20551;&#35774;&#19978;&#30340;&#12290;&#36825;&#20123;&#24046;&#36317;&#36890;&#24120;&#26159;&#36890;&#36807;&#32479;&#35745;&#20449;&#24687;&#25110;&#20998;&#31867;&#22120;&#26469;&#35782;&#21035;&#30340;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36136;&#30097;&#20102;&#26816;&#27979;&#22120;&#20013;&#30340;&#20998;&#24067;&#24046;&#36317;&#20551;&#35774;&#12290;&#25105;&#20204;&#21457;&#29616;&#26816;&#27979;&#22120;&#19981;&#33021;&#26377;&#25928;&#22320;&#21306;&#20998;&#20154;&#31867;&#29983;&#25104;&#21644;AI&#29983;&#25104;&#20869;&#23481;&#20043;&#38388;&#30340;&#35821;&#20041;&#21644;&#39118;&#26684;&#24046;&#36317;&#12290;&#30456;&#21453;&#65292;"&#24494;&#23567;&#30340;&#24046;&#24322;"&#65292;&#22914;&#39069;&#22806;&#30340;&#19968;&#20010;&#31354;&#26684;&#65292;&#22312;&#26816;&#27979;&#20013;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SpaceInfi&#31574;&#30053;&#26469;&#35268;&#36991;&#26816;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#31574;&#30053;&#22312;&#22810;&#20010;&#22522;&#20934;&#21644;&#26816;&#27979;&#22120;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36824;&#23545;&#20026;&#20160;&#20040;SpaceInfi&#33021;&#25104;&#21151;&#35268;&#36991;&#26816;&#27979;&#25552;&#20379;&#20102;&#29702;&#35770;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT brings revolutionary social value but also raises concerns about the misuse of AI-generated content. Consequently, an important question is how to detect whether content is generated by ChatGPT or by human. Existing detectors are built upon the assumption that there are distributional gaps between human-generated and AI-generated content. These gaps are typically identified using statistical information or classifiers. Our research challenges the distributional gap assumption in detectors. We find that detectors do not effectively discriminate the semantic and stylistic gaps between human-generated and AI-generated content. Instead, the "subtle differences", such as an extra space, become crucial for detection. Based on this discovery, we propose the SpaceInfi strategy to evade detection. Experiments demonstrate the effectiveness of this strategy across multiple benchmarks and detectors. We also provide a theoretical explanation for why SpaceInfi is successful in evading perple
&lt;/p&gt;</description></item><item><title>GPTWatermark&#26159;&#19968;&#31181;&#38024;&#23545;&#24615;&#27169;&#22411;&#27700;&#21360;&#25216;&#26415;&#65292;&#36890;&#36807;&#22266;&#23450;&#20998;&#32452;&#35774;&#35745;&#21644;&#24378;&#22823;&#30340;&#21487;&#35777;&#26126;&#20445;&#35777;&#65292;&#25552;&#20379;&#20102;&#23545;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#40065;&#26834;&#24615;&#26816;&#27979;&#21644;&#23433;&#20840;&#24615;&#38450;&#24481;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#26816;&#27979;&#20934;&#30830;&#24615;&#21644;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#65292;&#25512;&#21160;&#20102;LLMs&#36127;&#36131;&#20219;&#20351;&#29992;&#30340;&#36827;&#27493;&#12290;</title><link>http://arxiv.org/abs/2306.17439</link><description>&lt;p&gt;
&#21487;&#35777;&#26126;&#30340;&#38024;&#23545;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#40065;&#26834;&#27700;&#21360;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Provable Robust Watermarking for AI-Generated Text. (arXiv:2306.17439v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17439
&lt;/p&gt;
&lt;p&gt;
GPTWatermark&#26159;&#19968;&#31181;&#38024;&#23545;&#24615;&#27169;&#22411;&#27700;&#21360;&#25216;&#26415;&#65292;&#36890;&#36807;&#22266;&#23450;&#20998;&#32452;&#35774;&#35745;&#21644;&#24378;&#22823;&#30340;&#21487;&#35777;&#26126;&#20445;&#35777;&#65292;&#25552;&#20379;&#20102;&#23545;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#40065;&#26834;&#24615;&#26816;&#27979;&#21644;&#23433;&#20840;&#24615;&#38450;&#24481;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#26816;&#27979;&#20934;&#30830;&#24615;&#21644;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#65292;&#25512;&#21160;&#20102;LLMs&#36127;&#36131;&#20219;&#20351;&#29992;&#30340;&#36827;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#36234;&#26469;&#36234;&#25509;&#36817;&#20154;&#31867;&#25776;&#20889;&#30340;&#20869;&#23481;&#65292;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#33021;&#21147;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GPTWatermark&#65292;&#19968;&#31181;&#24378;&#22823;&#19988;&#39640;&#36136;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#30830;&#23450;&#19968;&#27573;&#25991;&#26412;&#26159;&#21542;&#26469;&#33258;&#29305;&#23450;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#27700;&#21360;&#31574;&#30053;&#65292;&#24182;&#37319;&#29992;&#20102;&#19968;&#31181;&#22266;&#23450;&#30340;&#20998;&#32452;&#35774;&#35745;&#65292;&#20197;&#22686;&#24378;&#23545;&#32534;&#36753;&#21644;&#25913;&#20889;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#24102;&#27700;&#21360;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#36136;&#37327;&#12289;&#26816;&#27979;&#27491;&#30830;&#24615;&#21644;&#23545;&#25239;&#35268;&#36991;&#25915;&#20987;&#30340;&#23433;&#20840;&#24615;&#26041;&#38754;&#20855;&#26377;&#24378;&#22823;&#30340;&#21487;&#35777;&#26126;&#20445;&#35777;&#12290;&#22312;&#21508;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26816;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#36798;&#21040;&#20102;&#20248;&#36234;&#30340;&#34920;&#29616;&#65292;&#24182;&#19988;&#19982;&#29983;&#25104;&#36136;&#37327;&#22312;&#22256;&#24785;&#24230;&#26041;&#38754;&#30456;&#24403;&#65292;&#20174;&#32780;&#20419;&#36827;&#20102;LLMs&#30340;&#36127;&#36131;&#20219;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
As AI-generated text increasingly resembles human-written content, the ability to detect machine-generated text becomes crucial. To address this challenge, we present GPTWatermark, a robust and high-quality solution designed to ascertain whether a piece of text originates from a specific model. Our approach extends existing watermarking strategies and employs a fixed group design to enhance robustness against editing and paraphrasing attacks. We show that our watermarked language model enjoys strong provable guarantees on generation quality, correctness in detection, and security against evasion attacks. Experimental results on various large language models (LLMs) and diverse datasets demonstrate that our method achieves superior detection accuracy and comparable generation quality in perplexity, thus promoting the responsible use of LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;&#35299;&#37322;&#24615;&#26041;&#27861;&#30340;&#24212;&#29992;&#36827;&#34892;&#30740;&#31350;&#65292;&#21457;&#29616;&#27169;&#22411;&#39044;&#27979;&#19982;&#20154;&#31867;&#25512;&#29702;&#30340;&#19968;&#33268;&#24615;&#19982;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#26377;&#20851;&#65292;&#27169;&#22411;&#26356;&#20542;&#21521;&#20110;&#19982;&#24180;&#38271;&#21644;/&#25110;&#30333;&#20154;&#27880;&#37322;&#32773;&#25552;&#20379;&#30340;&#21407;&#22240;&#26368;&#20026;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2306.00639</link><description>&lt;p&gt;
&#35841;&#30340;&#27491;&#30830;&#29702;&#30001;&#26159;&#27491;&#30830;&#30340;&#65311;&#65288;arXiv:2306.00639v2 [cs.CL] &#24050;&#26356;&#26032;&#65289;
&lt;/p&gt;
&lt;p&gt;
Being Right for Whose Right Reasons?. (arXiv:2306.00639v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#35299;&#37322;&#24615;&#26041;&#27861;&#30340;&#24212;&#29992;&#36827;&#34892;&#30740;&#31350;&#65292;&#21457;&#29616;&#27169;&#22411;&#39044;&#27979;&#19982;&#20154;&#31867;&#25512;&#29702;&#30340;&#19968;&#33268;&#24615;&#19982;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#26377;&#20851;&#65292;&#27169;&#22411;&#26356;&#20542;&#21521;&#20110;&#19982;&#24180;&#38271;&#21644;/&#25110;&#30333;&#20154;&#27880;&#37322;&#32773;&#25552;&#20379;&#30340;&#21407;&#22240;&#26368;&#20026;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#24615;&#26041;&#27861;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#39044;&#27979;&#19982;&#20154;&#31867;&#25512;&#29702;&#30340;&#19968;&#33268;&#31243;&#24230;&#65292;&#21363;&#26159;&#21542;"&#20986;&#20110;&#27491;&#30830;&#30340;&#21407;&#22240;"&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#26410;&#33021;&#35748;&#35782;&#21040;&#65292;&#20160;&#20040;&#34987;&#35270;&#20026;&#21407;&#22240;&#26377;&#26102;&#26159;&#20027;&#35266;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#35748;&#20026;&#26159;&#39318;&#27425;&#30340;&#20154;&#31867;&#21407;&#22240;&#27880;&#37322;&#38598;&#21512;&#65292;&#20854;&#20013;&#21253;&#21547;&#27880;&#37322;&#32773;&#30340;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#12290;&#25105;&#20204;&#28085;&#30422;&#20102;&#28085;&#30422;&#24773;&#24863;&#20998;&#26512;&#21644;&#24120;&#35782;&#25512;&#29702;&#30340;&#19977;&#20010;&#25968;&#25454;&#38598;&#20197;&#21450;&#20845;&#20010;&#20154;&#21475;&#32479;&#35745;&#32452;&#65288;&#22312;&#24180;&#40836;&#21644;&#26063;&#35028;&#19978;&#22343;&#34913;&#65289;&#12290;&#36825;&#26679;&#30340;&#25968;&#25454;&#20351;&#25105;&#20204;&#33021;&#22815;&#21516;&#26102;&#35810;&#38382;&#25105;&#20204;&#30340;&#39044;&#27979;&#19982;&#21738;&#20123;&#20154;&#21475;&#32479;&#35745;&#30456;&#31526;&#65292;&#20197;&#21450;&#25105;&#20204;&#27169;&#22411;&#30340;&#25512;&#29702;&#27169;&#24335;&#19982;&#21738;&#20123;&#20154;&#30340;&#21407;&#22240;&#30456;&#31526;&#12290;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#20154;&#32676;&#30340;&#27880;&#37322;&#32773;&#20043;&#38388;&#23384;&#22312;&#31995;&#32479;&#24615;&#30340;&#30456;&#20114;&#20998;&#27495;&#65292;&#24182;&#23637;&#31034;&#20102;16&#20010;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#19982;&#26576;&#20123;&#20154;&#21475;&#32479;&#35745;&#32452;&#25552;&#20379;&#30340;&#21407;&#22240;&#26356;&#21152;&#19968;&#33268;&#65306;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#26356;&#20542;&#21521;&#20110;&#19982;&#24180;&#38271;&#21644;/&#25110;&#30333;&#20154;&#27880;&#37322;&#32773;&#25552;&#20379;&#30340;&#21407;&#22240;&#26368;&#20026;&#19968;&#33268;&#12290;&#25105;&#20204;&#37325;&#28857;&#30740;&#31350;&#20102;&#27169;&#22411;&#22823;&#23567;&#21644;...
&lt;/p&gt;
&lt;p&gt;
Explainability methods are used to benchmark the extent to which model predictions align with human rationales i.e., are 'right for the right reasons'. Previous work has failed to acknowledge, however, that what counts as a rationale is sometimes subjective. This paper presents what we think is a first of its kind, a collection of human rationale annotations augmented with the annotators demographic information. We cover three datasets spanning sentiment analysis and common-sense reasoning, and six demographic groups (balanced across age and ethnicity). Such data enables us to ask both what demographics our predictions align with and whose reasoning patterns our models' rationales align with. We find systematic inter-group annotator disagreement and show how 16 Transformer-based models align better with rationales provided by certain demographic groups: We find that models are biased towards aligning best with older and/or white annotators. We zoom in on the effects of model size and m
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#27169;&#22411;&#36827;&#34892;&#34701;&#21512;&#65292;&#33021;&#29983;&#25104;&#20855;&#26377;&#36830;&#36143;&#24615;&#30340;&#22270;&#20687;&#36755;&#20986;&#65292;&#21516;&#26102;&#20063;&#33021;&#36827;&#34892;&#22270;&#20687;&#26816;&#32034;&#21644;&#22810;&#27169;&#24577;&#23545;&#35805;&#12290;</title><link>http://arxiv.org/abs/2305.17216</link><description>&lt;p&gt;
&#29992;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22270;&#29255;
&lt;/p&gt;
&lt;p&gt;
Generating Images with Multimodal Language Models. (arXiv:2305.17216v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17216
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#27169;&#22411;&#36827;&#34892;&#34701;&#21512;&#65292;&#33021;&#29983;&#25104;&#20855;&#26377;&#36830;&#36143;&#24615;&#30340;&#22270;&#20687;&#36755;&#20986;&#65292;&#21516;&#26102;&#20063;&#33021;&#36827;&#34892;&#22270;&#20687;&#26816;&#32034;&#21644;&#22810;&#27169;&#24577;&#23545;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#20165;&#21253;&#21547;&#25991;&#26412;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#27169;&#22411;&#36827;&#34892;&#34701;&#21512;&#65292;&#36890;&#36807;&#26144;&#23556;&#23427;&#20204;&#30340;&#23884;&#20837;&#31354;&#38388;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#23637;&#31034;&#20102;&#24191;&#27867;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;&#65306;&#22270;&#20687;&#26816;&#32034;&#12289;&#26032;&#39062;&#22270;&#20687;&#29983;&#25104;&#21644;&#22810;&#27169;&#24577;&#23545;&#35805;&#12290;&#36825;&#26159;&#31532;&#19968;&#31181;&#33021;&#22815;&#22312;&#20219;&#24847;&#20132;&#38169;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#36755;&#20837;&#20043;&#38388;&#36827;&#34892;&#26465;&#20214;&#35843;&#33410;&#65292;&#29983;&#25104;&#36830;&#36143;&#22270;&#20687;&#65288;&#21644;&#25991;&#26412;&#65289;&#36755;&#20986;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#22312;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26144;&#23556;&#32593;&#32476;&#65292;&#23558;LLM&#22522;&#20110;&#29616;&#25104;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#23558;&#25991;&#26412;&#30340;&#38544;&#34255;&#34920;&#31034;&#36716;&#25442;&#20026;&#35270;&#35273;&#27169;&#22411;&#30340;&#23884;&#20837;&#31354;&#38388;&#65292;&#21033;&#29992;LLM&#24378;&#22823;&#30340;&#25991;&#26412;&#34920;&#31034;&#26469;&#29983;&#25104;&#35270;&#35273;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38271;&#19988;&#22797;&#26434;&#35821;&#35328;&#30340;&#20219;&#21153;&#19978;&#20248;&#20110;&#22522;&#20934;&#29983;&#25104;&#27169;&#22411;&#12290;&#38500;&#20102;&#26032;&#39062;&#22270;&#20687;&#29983;&#25104;&#20043;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36824;&#33021;&#22815;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#26816;&#32034;&#22270;&#20687;&#65292;&#24182;&#36827;&#34892;&#22810;&#27169;&#24577;&#23545;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs. Our approach outperforms baseline generation models on tasks with longer and more complex language. In addition to novel image generation, our model is also capable of image retrieval from a prespe
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#35299;&#20915;&#20851;&#31995;&#20219;&#21153;&#30340;&#26426;&#21046;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#21033;&#29992;&#31616;&#21333;&#30340;&#32447;&#24615;&#26356;&#26032;&#26469;&#22788;&#29702;&#20851;&#31995;&#20219;&#21153;&#65292;&#24182;&#20197;&#20869;&#23481;&#26080;&#20851;&#30340;&#26041;&#24335;&#20419;&#36827;&#20851;&#31995;&#30340;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2305.16130</link><description>&lt;p&gt;
&#22312;Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#35299;&#20915;&#20851;&#31995;&#20219;&#21153;&#30340;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
A Mechanism for Solving Relational Tasks in Transformer Language Models. (arXiv:2305.16130v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16130
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#35299;&#20915;&#20851;&#31995;&#20219;&#21153;&#30340;&#26426;&#21046;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#21033;&#29992;&#31616;&#21333;&#30340;&#32447;&#24615;&#26356;&#26032;&#26469;&#22788;&#29702;&#20851;&#31995;&#20219;&#21153;&#65292;&#24182;&#20197;&#20869;&#23481;&#26080;&#20851;&#30340;&#26041;&#24335;&#20419;&#36827;&#20851;&#31995;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;&#65292;&#23613;&#31649;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#65292;&#23427;&#20204;&#26377;&#26102;&#20505;&#21033;&#29992;&#19968;&#20010;&#31616;&#21333;&#30340;&#35745;&#31639;&#26426;&#21046;&#26469;&#35299;&#20915;&#19968;&#23545;&#19968;&#30340;&#20851;&#31995;&#20219;&#21153;&#65288;&#20363;&#22914; capital_of(Poland)=Warsaw&#65289;&#12290;&#25105;&#20204;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#29615;&#22659;&#20013;&#30740;&#31350;&#20102;&#19968;&#31995;&#21015;&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#23567;&#65288;&#20174;124M&#21442;&#25968;&#21040;176B&#21442;&#25968;&#65289;&#65292;&#24182;&#21457;&#29616;&#23545;&#20110;&#22810;&#31181;&#20219;&#21153;&#65288;&#28041;&#21450;&#39318;&#37117;&#12289;&#22823;&#20889;&#21644;&#36807;&#21435;&#26102;&#24577;&#31561;&#65289;&#65292;&#26426;&#21046;&#30340;&#20851;&#38190;&#37096;&#20998;&#21487;&#20197;&#31616;&#21270;&#20026;&#21069;&#39304;&#65288;FFN&#65289;&#32593;&#32476;&#36890;&#24120;&#24212;&#29992;&#30340;&#31616;&#21333;&#32447;&#24615;&#26356;&#26032;&#12290;&#36825;&#20123;&#26356;&#26032;&#20063;&#20542;&#21521;&#20110;&#20197;&#20869;&#23481;&#26080;&#20851;&#30340;&#26041;&#24335;&#20419;&#36827;&#20851;&#31995;&#30340;&#36755;&#20986;&#65288;&#20363;&#22914;&#23545;&#32534;&#30721; Poland:Warsaw::China:Beijing&#65289;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#35299;&#20915;&#36825;&#20123;&#20219;&#21153;&#20013;&#30340;&#21487;&#39044;&#27979;&#27169;&#24335;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#26174;&#31034;&#36825;&#20010;&#26426;&#21046;&#26159;&#29305;&#23450;&#20110;&#38656;&#35201;&#20174;&#39044;&#35757;&#32451;&#23384;&#20648;&#22120;&#20013;&#26816;&#32034;&#32780;&#19981;&#26159;&#20174;&#23616;&#37096;&#19978;&#19979;&#25991;&#26816;&#32034;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;&#35299;&#20915;&#20851;&#31995;&#20219;&#21153;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#21046;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
A primary criticism towards language models (LMs) is their inscrutability. This paper presents evidence that, despite their size and complexity, LMs sometimes exploit a simple computational mechanism to solve one-to-one relational tasks (e.g., capital_of(Poland)=Warsaw). We investigate a range of language model sizes (from 124M parameters to 176B parameters) in an in-context learning setting, and find that for a variety of tasks (involving capital cities, upper-casing, and past-tensing) a key part of the mechanism reduces to a simple linear update typically applied by the feedforward (FFN) networks. These updates also tend to promote the output of the relation in a content-independent way (e.g., encoding Poland:Warsaw::China:Beijing), revealing a predictable pattern that these models take in solving these tasks. We further show that this mechanism is specific to tasks that require retrieval from pretraining memory, rather than retrieval from local context. Our results contribute to a g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#39564;&#35777;&#32773;&#19988;&#19981;&#20570;&#39046;&#22495;&#20551;&#35774;&#30340;&#20027;&#24352;&#26657;&#27491;&#31995;&#32479;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#31185;&#23398;&#20107;&#23454;&#38169;&#35823;&#26657;&#27491;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;LLM&#30340;&#25552;&#31034;&#26041;&#27861;&#21644;&#20027;&#24352;&#24863;&#30693;&#30340;&#35299;&#30721;&#36807;&#31243;&#26469;&#25552;&#39640;&#26657;&#27491;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.14707</link><description>&lt;p&gt;
&#23398;&#29983;&#36229;&#36234;&#20102;&#22823;&#24072;&#65306;&#22522;&#20110;GPT-3&#30340;&#31185;&#23398;&#20107;&#23454;&#38169;&#35823;&#26657;&#27491;&#26041;&#27861;&#30340;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
The student becomes the master: Matching GPT3 on Scientific Factual Error Correction. (arXiv:2305.14707v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14707
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#39564;&#35777;&#32773;&#19988;&#19981;&#20570;&#39046;&#22495;&#20551;&#35774;&#30340;&#20027;&#24352;&#26657;&#27491;&#31995;&#32479;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#31185;&#23398;&#20107;&#23454;&#38169;&#35823;&#26657;&#27491;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;LLM&#30340;&#25552;&#31034;&#26041;&#27861;&#21644;&#20027;&#24352;&#24863;&#30693;&#30340;&#35299;&#30721;&#36807;&#31243;&#26469;&#25552;&#39640;&#26657;&#27491;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#21019;&#24314;&#38169;&#35823;&#26657;&#27491;&#25968;&#25454;&#38598;&#30340;&#25104;&#26412;&#26497;&#39640;&#65292;&#22823;&#22810;&#25968;&#20107;&#23454;&#20027;&#24352;&#26657;&#27491;&#26041;&#27861;&#20381;&#36182;&#20110;&#24378;&#22823;&#30340;&#39564;&#35777;&#27169;&#22411;&#26469;&#25351;&#23548;&#26657;&#27491;&#36807;&#31243;&#12290;&#36825;&#23548;&#33268;&#22312;&#31185;&#23398;&#20107;&#23454;&#26657;&#27491;&#31561;&#39046;&#22495;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#65292;&#22240;&#20026;&#22909;&#30340;&#39564;&#35777;&#27169;&#22411;&#24182;&#19981;&#24635;&#26159;&#23384;&#22312;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#19981;&#20570;&#39046;&#22495;&#20551;&#35774;&#19988;&#19981;&#38656;&#35201;&#39564;&#35777;&#32773;&#30340;&#20027;&#24352;&#26657;&#27491;&#31995;&#32479;&#65292;&#20294;&#33021;&#22815;&#27604;&#29616;&#26377;&#26041;&#27861;&#25552;&#39640;&#19968;&#20010;&#25968;&#37327;&#32423;&#30340;&#24615;&#33021; - &#22312;SciFact&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;94&#65285;&#30340;&#20462;&#27491;&#20934;&#30830;&#24615;&#65292;&#22312;SciFact-Open&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;62.5&#65285;&#30340;&#20462;&#27491;&#20934;&#30830;&#24615;&#65292;&#20998;&#21035;&#27604;&#19979;&#19968;&#20010;&#26368;&#22909;&#30340;&#26041;&#27861;&#39640;&#20986;0.5&#65285;&#21644;1.50&#65285;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;LLMs&#20013;&#30340;&#25552;&#31034;&#21151;&#33021;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#21019;&#24314;&#19968;&#20010;&#20016;&#23500;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#23436;&#20840;&#30417;&#30563;&#30340;&#35757;&#32451;&#21644;&#27491;&#21017;&#21270;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#20027;&#24352;&#24863;&#30693;&#30340;&#35299;&#30721;&#36807;&#31243;&#26469;&#25552;&#39640;&#32416;&#27491;&#20027;&#24352;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#29992;&#20110;&#21019;&#24314;&#25968;&#25454;&#38598;&#30340;LLM&#30456;&#31454;&#20105;&#65292;&#35777;&#26126;&#20102;&#21033;&#29992;&#22522;&#20110;LLM&#30340;&#35757;&#32451;&#25552;&#39640;&#31185;&#23398;&#20027;&#24352;&#26657;&#27491;&#20219;&#21153;&#24615;&#33021;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the prohibitively high cost of creating error correction datasets, most Factual Claim Correction methods rely on a powerful verification model to guide the correction process. This leads to a significant drop in performance in domains like Scientific Claim Correction, where good verification models do not always exist. In this work, we introduce a claim correction system that makes no domain assumptions and does not require a verifier but is able to outperform existing methods by an order of magnitude -- achieving 94% correction accuracy on the SciFact dataset, and 62.5% on the SciFact-Open dataset, compared to the next best methods 0.5% and 1.50% respectively. Our method leverages the power of prompting with LLMs during training to create a richly annotated dataset that can be used for fully supervised training and regularization. We additionally use a claim-aware decoding procedure to improve the quality of corrected claims. Our method is competitive with the very LLM that was
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#20174;&#19981;&#30456;&#20132;&#30340;&#20860;&#23481;&#26631;&#31614;&#24207;&#21015;&#20013;&#39640;&#25928;&#22320;&#23398;&#20064;&#65292;&#23558;&#27492;&#36816;&#29992;&#20110;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#32852;&#21512;&#22788;&#29702;VerbNet&#21644;PropBank&#26631;&#31614;&#30340;&#26041;&#27861;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14600</link><description>&lt;p&gt;
&#23398;&#20064;&#20174;&#20860;&#23481;&#26631;&#31614;&#24207;&#21015;&#20013;&#30340;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
Learning Semantic Role Labeling from Compatible Label Sequences. (arXiv:2305.14600v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14600
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#20174;&#19981;&#30456;&#20132;&#30340;&#20860;&#23481;&#26631;&#31614;&#24207;&#21015;&#20013;&#39640;&#25928;&#22320;&#23398;&#20064;&#65292;&#23558;&#27492;&#36816;&#29992;&#20110;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#32852;&#21512;&#22788;&#29702;VerbNet&#21644;PropBank&#26631;&#31614;&#30340;&#26041;&#27861;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#39640;&#25928;&#22320;&#23398;&#20064;&#20174;&#19981;&#30456;&#20132;&#30340;&#20860;&#23481;&#26631;&#31614;&#24207;&#21015;&#20013;&#26631;&#27880;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#19981;&#30456;&#20132;&#26631;&#31614;&#38598;&#20043;&#38388;&#30340;&#20860;&#23481;&#32467;&#26500;&#26377;&#21161;&#20110;&#27169;&#22411;&#30340;&#23398;&#20064;&#21644;&#25512;&#29702;&#12290;&#25105;&#20204;&#22312;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;&#65288;SRL&#65289;&#20219;&#21153;&#20013;&#39564;&#35777;&#20102;&#36825;&#19968;&#20551;&#35774;&#65292;&#20855;&#20307;&#22320;&#65292;&#26631;&#35760;&#20855;&#26377;&#20004;&#20010;&#35282;&#33394;&#24207;&#21015;&#30340;&#21477;&#23376;&#65306;VerbNet&#21442;&#25968;&#21644;PropBank&#21442;&#25968;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#34920;&#26126;&#36328;&#20219;&#21153;&#20132;&#20114;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#20294;&#26159;&#65292;&#36825;&#20004;&#20010;&#20219;&#21153;&#20173;&#28982;&#26159;&#20998;&#21035;&#35299;&#30721;&#30340;&#65292;&#23384;&#22312;&#29983;&#25104;&#32467;&#26500;&#19981;&#19968;&#33268;&#30340;&#26631;&#31614;&#24207;&#21015; (&#22312;&#20687;SEMLINK&#30340;&#35789;&#20856;&#20013;)&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#28040;&#38500;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35774;&#32622;&#65292;&#32852;&#21512;&#22788;&#29702;VerbNet&#21644;PropBank&#26631;&#31614;&#20316;&#20026;&#19968;&#20010;&#24207;&#21015;&#12290;&#36890;&#36807;&#36825;&#20010;&#35774;&#32622;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#24378;&#21046;&#25191;&#34892;SEMLINK&#32422;&#26463;&#19981;&#26029;&#25552;&#39640;&#24635;F1&#20540;&#12290;&#36890;&#36807;&#29305;&#27530;&#30340;&#36755;&#20837;&#26500;&#36896;&#65292;&#25105;&#20204;&#30340;&#32852;&#21512;&#27169;&#22411;&#21487;&#20197;&#20197;&#36229;&#36807;99%&#30340;&#20934;&#30830;&#24615;&#20174;PropBank&#21442;&#25968;&#20013;&#25512;&#26029;&#20986;VerbNet&#21442;&#25968;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;co
&lt;/p&gt;
&lt;p&gt;
This paper addresses the question of how to efficiently learn from disjoint, compatible label sequences. We argue that the compatible structures between disjoint label sets help model learning and inference. We verify this hypothesis on the task of semantic role labeling (SRL), specifically, tagging a sentence with two role sequences: VerbNet arguments and PropBank arguments. Prior work has shown that cross-task interaction improves performance. However, the two tasks are still separately decoded, running the risk of generating structurally inconsistent label sequences (as per lexicons like SEMLINK). To eliminate this issue, we first propose a simple and effective setup that jointly handles VerbNet and PropBank labels as one sequence. With this setup, we show that enforcing SEMLINK constraints during decoding constantly improves the overall F1. With special input constructions, our joint model infers VerbNet arguments from PropBank arguments with over 99% accuracy. We also propose a co
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35789;&#20856;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#29983;&#29289;&#21307;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#30340;&#21516;&#20041;&#35789;&#27867;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#24341;&#20837;&#21516;&#20041;&#35789;&#36317;&#31163;&#21644;&#22122;&#22768;&#25200;&#21160;&#27491;&#21017;&#21270;&#39033;&#65292;&#23454;&#29616;&#20102;&#23545;&#36755;&#20837;&#25991;&#26412;&#20013;&#30340;&#29983;&#29289;&#21307;&#23398;&#27010;&#24565;&#30340;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2305.13066</link><description>&lt;p&gt;
&#22522;&#20110;&#35789;&#20856;&#30340;&#21516;&#20041;&#35789;&#27867;&#21270;&#30340;&#29983;&#29289;&#21307;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Biomedical Named Entity Recognition via Dictionary-based Synonym Generalization. (arXiv:2305.13066v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35789;&#20856;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#29983;&#29289;&#21307;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#30340;&#21516;&#20041;&#35789;&#27867;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#24341;&#20837;&#21516;&#20041;&#35789;&#36317;&#31163;&#21644;&#22122;&#22768;&#25200;&#21160;&#27491;&#21017;&#21270;&#39033;&#65292;&#23454;&#29616;&#20102;&#23545;&#36755;&#20837;&#25991;&#26412;&#20013;&#30340;&#29983;&#29289;&#21307;&#23398;&#27010;&#24565;&#30340;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26159;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#26680;&#24515;&#20219;&#21153;&#20043;&#19968;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26377;&#30417;&#30563;/&#38388;&#25509;&#26377;&#30417;&#30563;&#30340;&#26041;&#27861;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#19981;&#21487;&#36991;&#20813;&#22320;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#21147;&#24037;&#20316;&#12290;&#20026;&#20102;&#20943;&#36731;&#20154;&#21147;&#21171;&#21160;&#30340;&#38656;&#27714;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#22522;&#20110;&#35789;&#20856;&#30340;&#26041;&#27861;&#65292;&#20165;&#26681;&#25454;&#32473;&#23450;&#30340;&#35789;&#20856;&#25552;&#21462;&#21629;&#21517;&#23454;&#20307;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#35789;&#20856;&#30340;&#26041;&#27861;&#30340;&#19968;&#20010;&#32570;&#28857;&#26159;&#65292;&#23427;&#20204;&#38590;&#20197;&#35782;&#21035;&#32473;&#23450;&#35789;&#20856;&#20013;&#26410;&#21015;&#20986;&#30340;&#27010;&#24565;&#21516;&#20041;&#35789;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#21516;&#20041;&#35789;&#27867;&#21270;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21516;&#20041;&#35789;&#27867;&#21270;&#65288;SynGen&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#22522;&#20110;&#36328;&#24230;&#30340;&#39044;&#27979;&#35782;&#21035;&#36755;&#20837;&#25991;&#26412;&#20013;&#21253;&#21547;&#30340;&#29983;&#29289;&#21307;&#23398;&#27010;&#24565;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SynGen&#24341;&#20837;&#20102;&#20004;&#20010;&#27491;&#21017;&#21270;&#39033;&#65292;&#21363;&#65288;1&#65289;&#21516;&#20041;&#35789;&#36317;&#31163;&#27491;&#21017;&#21270;&#39033;&#65307;&#21644;&#65288;2&#65289;&#22122;&#22768;&#25200;&#21160;&#27491;&#21017;&#21270;&#39033;&#65292;&#20197;&#26368;&#23567;&#21270;
&lt;/p&gt;
&lt;p&gt;
Biomedical named entity recognition is one of the core tasks in biomedical natural language processing (BioNLP). To tackle this task, numerous supervised/distantly supervised approaches have been proposed. Despite their remarkable success, these approaches inescapably demand laborious human effort. To alleviate the need of human effort, dictionary-based approaches have been proposed to extract named entities simply based on a given dictionary. However, one downside of existing dictionary-based approaches is that they are challenged to identify concept synonyms that are not listed in the given dictionary, which we refer as the synonym generalization problem. In this study, we propose a novel Synonym Generalization (SynGen) framework that recognizes the biomedical concepts contained in the input text using span-based predictions. In particular, SynGen introduces two regularization terms, namely, (1) a synonym distance regularizer; and (2) a noise perturbation regularizer, to minimize the
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#24341;&#20837;&#20102;DUMB&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#26234;&#33021;&#35780;&#20272;&#33655;&#20848;&#35821;&#27169;&#22411;&#12290;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#22823;&#23567;&#21644;&#31867;&#22411;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#21457;&#29616;&#30446;&#21069;&#30340;&#33655;&#20848;&#21333;&#35821;&#27169;&#22411;&#34920;&#29616;&#19981;&#20339;&#65292;&#24314;&#35758;&#20351;&#29992;&#20854;&#20182;&#26550;&#26500;&#21644;&#39044;&#35757;&#32451;&#30446;&#26631;&#26469;&#35757;&#32451;&#26356;&#22823;&#30340;&#33655;&#20848;&#27169;&#22411;&#12290;&#22312;&#35813;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;DeBERTaV3 (large)&#12289;XLM-R (large)&#21644;mDeBERTaV3 (base)&#21462;&#24471;&#20102;&#26368;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.13026</link><description>&lt;p&gt;
DUMB: &#29992;&#20110;&#26234;&#33021;&#35780;&#20272;&#33655;&#20848;&#35821;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
DUMB: A Benchmark for Smart Evaluation of Dutch Models. (arXiv:2305.13026v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13026
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;DUMB&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#26234;&#33021;&#35780;&#20272;&#33655;&#20848;&#35821;&#27169;&#22411;&#12290;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#22823;&#23567;&#21644;&#31867;&#22411;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#21457;&#29616;&#30446;&#21069;&#30340;&#33655;&#20848;&#21333;&#35821;&#27169;&#22411;&#34920;&#29616;&#19981;&#20339;&#65292;&#24314;&#35758;&#20351;&#29992;&#20854;&#20182;&#26550;&#26500;&#21644;&#39044;&#35757;&#32451;&#30446;&#26631;&#26469;&#35757;&#32451;&#26356;&#22823;&#30340;&#33655;&#20848;&#27169;&#22411;&#12290;&#22312;&#35813;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;DeBERTaV3 (large)&#12289;XLM-R (large)&#21644;mDeBERTaV3 (base)&#21462;&#24471;&#20102;&#26368;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#33655;&#20848;&#27169;&#22411;&#22522;&#20934;&#27979;&#35797;&#65306;DUMB&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#21253;&#25324;&#19968;&#32452;&#29992;&#20110;&#20302;&#12289;&#20013;&#21644;&#39640;&#36164;&#28304;&#20219;&#21153;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#12290;&#24635;&#20849;&#26377;&#20061;&#20010;&#20219;&#21153;&#65292;&#20854;&#20013;&#22235;&#20010;&#20219;&#21153;&#20197;&#21069;&#22312;&#33655;&#20848;&#35821;&#20013;&#36824;&#27809;&#26377;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#30456;&#23545;&#35823;&#24046;&#20943;&#23569; (RER) &#30340;&#27010;&#24565;&#65292;&#32780;&#19981;&#26159;&#20381;&#36182;&#20110;&#20219;&#21153;&#30340;&#22343;&#20540;&#20998;&#25968;&#65292;RER &#23545;&#27604;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;DUMB&#22522;&#20934;&#27979;&#35797;&#20013;&#19982;&#24378;&#22522;&#20934;&#32447;&#30340;&#34920;&#29616;&#65292;&#36825;&#21487;&#20197;&#22312;&#20170;&#21518;&#35780;&#20272;&#19981;&#21516;&#35821;&#35328;&#27169;&#22411;&#38598;&#26102;&#20316;&#20026;&#21442;&#32771;&#12290;&#36890;&#36807;&#27604;&#36739;14&#20010;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#21333;&#35821;&#21644;&#22810;&#35821;&#12289;&#19981;&#21516;&#22823;&#23567;&#30340;&#27169;&#22411;&#65289;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22522;&#20934;&#27979;&#35797;&#20219;&#21153;&#30340;&#20869;&#37096;&#19968;&#33268;&#24615;&#20197;&#21450;&#21487;&#33021;&#23548;&#33268;&#39640;&#24615;&#33021;&#30340;&#22240;&#32032;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#30446;&#21069;&#30340;&#33655;&#20848;&#21333;&#35821;&#27169;&#22411;&#34920;&#29616;&#19981;&#20339;&#65292;&#24182;&#24314;&#35758;&#20351;&#29992;&#20854;&#20182;&#26550;&#26500;&#21644;&#39044;&#35757;&#32451;&#30446;&#26631;&#26469;&#35757;&#32451;&#26356;&#22823;&#30340;&#33655;&#20848;&#27169;&#22411;&#12290;&#30446;&#21069;&#65292;DeBERTaV3 (large)&#12289;XLM-R (large)&#21644;mDeBERTaV3 (base) &#23454;&#29616;&#20102;&#26368;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the Dutch Model Benchmark: DUMB. The benchmark includes a diverse set of datasets for low-, medium- and high-resource tasks. The total set of nine tasks includes four tasks that were previously not available in Dutch. Instead of relying on a mean score across tasks, we propose Relative Error Reduction (RER), which compares the DUMB performance of language models to a strong baseline which can be referred to in the future even when assessing different sets of language models. Through a comparison of 14 pre-trained language models (mono- and multi-lingual, of varying sizes), we assess the internal consistency of the benchmark tasks, as well as the factors that likely enable high performance. Our results indicate that current Dutch monolingual models under-perform and suggest training larger Dutch models with other architectures and pre-training objectives. At present, the highest performance is achieved by DeBERTaV3 (large), XLM-R (large) and mDeBERTaV3 (base). In addition t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20250;&#35805;&#24773;&#24863;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65288;CACD&#65289;&#65292;&#24182;&#36890;&#36807;&#35774;&#35745;&#20844;&#20849;&#39592;&#26550;&#21644;&#29983;&#25104;&#26367;&#20195;&#38544;&#21547;&#21407;&#22240;&#35299;&#20915;&#20102;&#22240;&#26524;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#38544;&#21547;&#21407;&#22240;&#30340;&#19981;&#21487;&#35266;&#23519;&#24615;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#21464;&#38271;&#20250;&#35805;&#20013;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2305.02615</link><description>&lt;p&gt;
&#20250;&#35805;&#20013;&#30340;&#24773;&#24863;&#25512;&#29702;&#65306;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Affective Reasoning at Utterance Level in Conversations: A Causal Discovery Approach. (arXiv:2305.02615v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02615
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20250;&#35805;&#24773;&#24863;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65288;CACD&#65289;&#65292;&#24182;&#36890;&#36807;&#35774;&#35745;&#20844;&#20849;&#39592;&#26550;&#21644;&#29983;&#25104;&#26367;&#20195;&#38544;&#21547;&#21407;&#22240;&#35299;&#20915;&#20102;&#22240;&#26524;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#38544;&#21547;&#21407;&#22240;&#30340;&#19981;&#21487;&#35266;&#23519;&#24615;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#21464;&#38271;&#20250;&#35805;&#20013;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#25512;&#29702;&#20219;&#21153;&#26159;&#21253;&#25324;&#20250;&#35805;&#20013;&#30340;&#24773;&#24863;&#35782;&#21035;&#12289;&#24773;&#24863;-&#21407;&#22240;&#23545;&#25277;&#21462;&#21644;&#24773;&#24863;-&#21407;&#22240;&#36328;&#24230;&#35782;&#21035;&#22312;&#20869;&#30340;&#19968;&#32452;&#26032;&#20852;&#30340;&#22522;&#20110;&#24773;&#24863;&#30340;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#20551;&#35774;&#34920;&#38754;&#20851;&#31995;&#26102;&#24573;&#30053;&#20102;&#22522;&#26412;&#30340;&#22240;&#26524;&#27169;&#22411;&#65292;&#22240;&#20026;&#39592;&#26550;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#38544;&#21547;&#21407;&#22240;&#30340;&#19981;&#21487;&#35266;&#23519;&#24615;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#19978;&#36848;&#20004;&#20010;&#38382;&#39064;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#20250;&#35805;&#24773;&#24863;&#22240;&#26524;&#21457;&#29616;&#65288;CACD&#65289;&#26041;&#27861;&#12290;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#35774;&#35745;&#20844;&#20849;&#39592;&#26550;&#21644;&#29983;&#25104;&#26367;&#20195;&#38544;&#21547;&#21407;&#22240;&#26469;&#21457;&#29616;&#20250;&#35805;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;CACD&#21253;&#21547;&#20004;&#20010;&#27493;&#39588;&#65306;&#65288;i&#65289;&#20026;&#21464;&#38271;&#20250;&#35805;&#20013;&#30340;&#25152;&#26377;&#35805;&#35821;&#24314;&#31435;&#19968;&#20010;&#20013;&#24515;&#21270;&#30340;&#21333;&#19968;&#22270;&#33410;&#28857;&#22240;&#26524;&#39592;&#26550;&#65307;&#65288;ii&#65289;&#22240;&#26524;&#33258;&#32534;&#30721;&#22120;&#65288;CAE&#65289;&#36890;&#36807;&#29983;&#25104;&#38544;&#21547;&#21407;&#22240;&#21644;&#24050;&#30693;&#26174;&#24335;&#21407;&#22240;&#26469;&#20462;&#27491;&#39592;&#26550;&#65292;&#20174;&#32780;&#20135;&#29983;&#22240;&#26524;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The affective reasoning task is a set of emerging affect-based tasks in conversation, including Emotion Recognition in Conversation (ERC),Emotion-Cause Pair Extraction (ECPE), and Emotion-Cause Span Recognition (ECSR). Existing methods make various assumptions on the apparent relationship while neglecting the essential causal model due to the nonuniqueness of skeletons and unobservability of implicit causes. This paper settled down the above two problems and further proposed Conversational Affective Causal Discovery (CACD). It is a novel causal discovery method showing how to discover causal relationships in a conversation via designing a common skeleton and generating a substitute for implicit causes. CACD contains two steps: (i) building a common centering one graph node causal skeleton for all utterances in variable-length conversations; (ii) Causal Auto-Encoder (CAE) correcting the skeleton to yield causal representation through generated implicit causes and known explicit causes. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;LLMs&#22522;&#20110;MBTI&#27979;&#35797;&#35780;&#20272;&#20154;&#31867;&#20010;&#24615;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#35774;&#35745;&#26080;&#20559;&#20506;&#30340;&#25552;&#31034;&#12289;&#28789;&#27963;&#26597;&#35810;&#21644;&#27491;&#30830;&#24615;&#35780;&#20272;&#30340;&#26041;&#24335;&#65292;&#20351;LLMs&#33021;&#22815;&#28789;&#27963;&#35780;&#20272;&#19981;&#21516;&#32676;&#20307;&#30340;&#20010;&#24615;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2303.01248</link><description>&lt;p&gt;
ChatGPT&#33021;&#35780;&#20272;&#20154;&#31867;&#20010;&#24615;&#21527;&#65311;&#19968;&#20010;&#36890;&#29992;&#35780;&#20272;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can ChatGPT Assess Human Personalities? A General Evaluation Framework. (arXiv:2303.01248v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01248
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;LLMs&#22522;&#20110;MBTI&#27979;&#35797;&#35780;&#20272;&#20154;&#31867;&#20010;&#24615;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#35774;&#35745;&#26080;&#20559;&#20506;&#30340;&#25552;&#31034;&#12289;&#28789;&#27963;&#26597;&#35810;&#21644;&#27491;&#30830;&#24615;&#35780;&#20272;&#30340;&#26041;&#24335;&#65292;&#20351;LLMs&#33021;&#22815;&#28789;&#27963;&#35780;&#20272;&#19981;&#21516;&#32676;&#20307;&#30340;&#20010;&#24615;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23588;&#20854;&#26159;ChatGPT&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#26524;&#65292;&#20294;&#23427;&#20204;&#28508;&#22312;&#30340;&#20154;&#31867;&#21270;&#24515;&#29702;&#29305;&#24449;&#23578;&#26410;&#24471;&#21040;&#28145;&#20837;&#25506;&#32034;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#30740;&#31350;LLMs&#30340;&#34394;&#25311;&#20010;&#24615;&#65292;&#32780;&#24456;&#23569;&#25506;&#32034;&#36890;&#36807;LLMs&#20998;&#26512;&#20154;&#31867;&#20010;&#24615;&#30340;&#21487;&#33021;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#22522;&#20110;&#36808;&#23572;&#26031;&#183;&#24067;&#37324;&#26684;&#26031;&#20154;&#26684;&#31867;&#22411;&#25351;&#26631;&#65288;MBTI&#65289;&#27979;&#35797;&#35780;&#20272;LLMs&#30340;&#20154;&#31867;&#20010;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#38543;&#26426;&#25490;&#21015;MBTI&#38382;&#39064;&#20013;&#30340;&#36873;&#39033;&#26469;&#35774;&#35745;&#26080;&#20559;&#20506;&#30340;&#25552;&#31034;&#65292;&#37319;&#29992;&#24179;&#22343;&#27979;&#35797;&#32467;&#26524;&#26469;&#40723;&#21169;&#26356;&#23458;&#35266;&#30340;&#31572;&#26696;&#29983;&#25104;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24314;&#35758;&#26367;&#25442;&#38382;&#39064;&#38472;&#36848;&#20013;&#30340;&#20027;&#35821;&#65292;&#23454;&#29616;&#23545;LLMs&#19978;&#19981;&#21516;&#20027;&#20307;&#30340;&#28789;&#27963;&#26597;&#35810;&#21644;&#35780;&#20272;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20197;&#27491;&#30830;&#24615;&#35780;&#20272;&#30340;&#26041;&#24335;&#37325;&#26032;&#26500;&#24314;&#38382;&#39064;&#25351;&#20196;&#65292;&#20197;&#20415;&#20419;&#20351;LLMs&#29983;&#25104;&#26356;&#28165;&#26224;&#30340;&#22238;&#24212;&#12290;&#35813;&#25552;&#20986;&#30340;&#26694;&#26550;&#20351;LLMs&#33021;&#22815;&#28789;&#27963;&#35780;&#20272;&#19981;&#21516;&#32676;&#20307;&#30340;&#20010;&#24615;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) especially ChatGPT have produced impressive results in various areas, but their potential human-like psychology is still largely unexplored. Existing works study the virtual personalities of LLMs but rarely explore the possibility of analyzing human personalities via LLMs. This paper presents a generic evaluation framework for LLMs to assess human personalities based on Myers Briggs Type Indicator (MBTI) tests. Specifically, we first devise unbiased prompts by randomly permuting options in MBTI questions and adopt the average testing result to encourage more impartial answer generation. Then, we propose to replace the subject in question statements to enable flexible queries and assessments on different subjects from LLMs. Finally, we re-formulate the question instructions in a manner of correctness evaluation to facilitate LLMs to generate clearer responses. The proposed framework enables LLMs to flexibly assess personalities of different groups of people.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#29983;&#25104;&#22823;&#37327;&#21512;&#25104;&#26597;&#35810;&#21644;reranker&#27169;&#22411;&#65292;&#33976;&#39311;&#20026;&#39640;&#25928;&#30340;&#26816;&#32034;&#22120;&#65292;&#36866;&#29992;&#20110;&#38271;&#23614;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2303.00807</link><description>&lt;p&gt;
UDAPDR: &#22522;&#20110;LLM&#25552;&#31034;&#19982;reranker&#33976;&#39311;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of Rerankers. (arXiv:2303.00807v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00807
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#29983;&#25104;&#22823;&#37327;&#21512;&#25104;&#26597;&#35810;&#21644;reranker&#27169;&#22411;&#65292;&#33976;&#39311;&#20026;&#39640;&#25928;&#30340;&#26816;&#32034;&#22120;&#65292;&#36866;&#29992;&#20110;&#38271;&#23614;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24456;&#22810;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#38656;&#35201;&#22823;&#22411;&#26631;&#27880;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65292;&#20294;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#36890;&#24120;&#19981;&#21487;&#29992;&#65292;&#19988;&#22312;&#24212;&#29992;&#20110;&#30495;&#23454;&#22330;&#26223;&#20013;&#26102;&#21487;&#33021;&#20250;&#22240;&#20026;&#39046;&#22495;&#28418;&#31227;&#32780;&#36805;&#36895;&#22833;&#21435;&#25928;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24265;&#20215;&#29983;&#25104;&#22823;&#37327;&#21512;&#25104;&#26597;&#35810;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#39318;&#20808;&#21033;&#29992;&#26114;&#36149;&#30340;LLM&#29983;&#25104;&#23569;&#37327;&#21512;&#25104;&#26597;&#35810;&#65292;&#28982;&#21518;&#20877;&#21033;&#29992;&#25104;&#26412;&#36739;&#20302;&#30340;LLM&#29983;&#25104;&#22823;&#37327;&#30340;&#21512;&#25104;&#26597;&#35810;&#20197;&#24494;&#35843;&#19968;&#32452;reranker&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#36825;&#20123;reranker&#20250;&#34987;&#33976; distill &#25104;&#19968;&#20010;&#39640;&#25928;&#30340;&#26816;&#32034;&#22120;&#65292;&#29992;&#20110;&#30446;&#26631;&#39046;&#22495;&#20013;&#30340;&#26816;&#32034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#25216;&#26415;&#21487;&#20197;&#25552;&#39640;&#38271;&#23614;&#39046;&#22495;&#20013;&#30340;&#38646;&#26679;&#26412;&#20934;&#30830;&#24615;&#65292;&#21363;&#20351;&#21482;&#20351;&#29992;2K&#20010;&#21512;&#25104;&#26597;&#35810;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#19988;&#27604;&#26631;&#20934;&#30340;reranking&#26041;&#27861;&#20855;&#26377;&#26356;&#20302;&#30340;&#24310;&#36831;&#12290;&#25105;&#20204;&#25552;&#20379;&#23436;&#25972;&#30340;&#31471;&#21040;&#31471;&#26041;&#26696;&#65292;&#21253;&#25324;&#21512;&#25104;&#25968;&#25454;&#38598;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many information retrieval tasks require large labeled datasets for fine-tuning. However, such datasets are often unavailable, and their utility for real-world applications can diminish quickly due to domain shifts. To address this challenge, we develop and motivate a method for using large language models (LLMs) to generate large numbers of synthetic queries cheaply. The method begins by generating a small number of synthetic queries using an expensive LLM. After that, a much less expensive one is used to create large numbers of synthetic queries, which are used to fine-tune a family of reranker models. These rerankers are then distilled into a single efficient retriever for use in the target domain. We show that this technique boosts zero-shot accuracy in long-tail domains, even where only 2K synthetic queries are used for fine-tuning, and that it achieves substantially lower latency than standard reranking methods. We make our end-to-end approach, including our synthetic datasets an
&lt;/p&gt;</description></item><item><title>Orca&#26159;&#20013;&#25991;&#23545;&#35805;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#30340;&#31532;&#19968;&#20010;&#22522;&#20934;&#65292;&#25552;&#20379;&#20102;&#38646;&#26679;&#26412;/&#23569;&#26679;&#26412;&#35774;&#32622;&#26469;&#35780;&#20272;&#27169;&#22411;&#23545;&#22810;&#26679;&#39046;&#22495;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#25552;&#20379;&#19982;&#22238;&#31572;&#30456;&#20851;&#30340;&#27573;&#33853;&#26469;&#26356;&#21512;&#29702;&#22320;&#35780;&#20272;&#27169;&#22411;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2302.13619</link><description>&lt;p&gt;
Orca: &#19968;&#31181;&#29992;&#20110;&#20013;&#25991;&#23545;&#35805;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#30340;&#23569;&#26679;&#26412;&#27979;&#35797;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Orca: A Few-shot Benchmark for Chinese Conversational Machine Reading Comprehension. (arXiv:2302.13619v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13619
&lt;/p&gt;
&lt;p&gt;
Orca&#26159;&#20013;&#25991;&#23545;&#35805;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#30340;&#31532;&#19968;&#20010;&#22522;&#20934;&#65292;&#25552;&#20379;&#20102;&#38646;&#26679;&#26412;/&#23569;&#26679;&#26412;&#35774;&#32622;&#26469;&#35780;&#20272;&#27169;&#22411;&#23545;&#22810;&#26679;&#39046;&#22495;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#25552;&#20379;&#19982;&#22238;&#31572;&#30456;&#20851;&#30340;&#27573;&#33853;&#26469;&#26356;&#21512;&#29702;&#22320;&#35780;&#20272;&#27169;&#22411;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#65288;CMRC&#65289;&#20219;&#21153;&#26088;&#22312;&#22238;&#31572;&#23545;&#35805;&#20013;&#30340;&#38382;&#39064;&#65292;&#30001;&#20110;&#20854;&#24191;&#27867;&#24212;&#29992;&#65292;&#36817;&#24180;&#26469;&#24050;&#25104;&#20026;&#28909;&#38376;&#30740;&#31350;&#35838;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;CMRC&#22522;&#20934;&#22312;&#27599;&#20010;&#23545;&#35805;&#20013;&#20998;&#37197;&#19968;&#20010;&#38745;&#24577;&#27573;&#33853;&#65292;&#19982;&#30495;&#23454;&#22330;&#26223;&#19981;&#19968;&#33268;&#12290;&#22240;&#27492;&#65292;&#24456;&#38590;&#21512;&#29702;&#35780;&#20272;&#27169;&#22411;&#23545;&#30495;&#23454;&#22330;&#26223;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20013;&#25991;CMRC&#22522;&#20934;Orca&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#20379;&#20102;&#38646;&#26679;&#26412;/&#23569;&#26679;&#26412;&#35774;&#32622;&#65292;&#20197;&#35780;&#20272;&#27169;&#22411;&#23545;&#22810;&#26679;&#39046;&#22495;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;831&#20010;&#28909;&#38376;&#35805;&#39064;&#39537;&#21160;&#30340;&#23545;&#35805;&#65292;&#20849;&#35745;4,742&#36718;&#12290;&#27599;&#20010;&#23545;&#35805;&#30340;&#27599;&#20010;&#36718;&#27425;&#37117;&#20250;&#20998;&#37197;&#19968;&#20010;&#19982;&#22238;&#31572;&#26377;&#20851;&#30340;&#27573;&#33853;&#65292;&#26088;&#22312;&#26356;&#21512;&#29702;&#22320;&#35780;&#20272;&#27169;&#22411;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#23545;&#35805;&#30340;&#20027;&#39064;&#26469;&#33258;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#65292;&#28085;&#30422;33&#20010;&#39046;&#22495;&#65292;&#21147;&#20105;&#19982;&#30495;&#23454;&#22330;&#26223;&#20445;&#25345;&#19968;&#33268;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;Orca&#20013;&#30340;&#31572;&#26696;&#37117;&#26159;&#32463;&#36807;&#33391;&#22909;&#27880;&#37322;&#30340;&#33258;&#28982;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;
The conversational machine reading comprehension (CMRC) task aims to answer questions in conversations, which has been a hot research topic in recent years because of its wide applications. However, existing CMRC benchmarks in which each conversation is assigned a static passage are inconsistent with real scenarios. Thus, model's comprehension ability towards real scenarios are hard to evaluate reasonably. To this end, we propose the first Chinese CMRC benchmark Orca and further provide zero-shot/few-shot settings to evaluate model's generalization ability towards diverse domains. We collect 831 hot-topic driven conversations with 4,742 turns in total. Each turn of a conversation is assigned with a response-related passage, aiming to evaluate model's comprehension ability more reasonably. The topics of conversations are collected from social media platform and cover 33 domains, trying to be consistent with real scenarios. Importantly, answers in Orca are all well-annotated natural resp
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#20102;&#19981;&#21516;&#27169;&#22411;&#22312;&#26435;&#37325;&#31354;&#38388;&#20013;&#30340;&#20301;&#32622;&#19982;&#24615;&#33021;&#30340;&#20851;&#32852;&#65292;&#21457;&#29616;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#22312;&#26435;&#37325;&#31354;&#38388;&#20013;&#26377;&#26126;&#30830;&#23450;&#20041;&#30340;&#21306;&#22495;&#65292;&#19988;&#36825;&#20123;&#21306;&#22495;&#20013;&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;&#39640;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#32469;&#36807;&#36825;&#20123;&#21306;&#22495;&#65292;&#21487;&#20197;&#24471;&#21040;&#24615;&#33021;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#30340;&#26032;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2302.04863</link><description>&lt;p&gt;
&#30693;&#35782;&#26159;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#20013;&#26435;&#37325;&#31354;&#38388;&#30340;&#19968;&#20010;&#21306;&#22495;
&lt;/p&gt;
&lt;p&gt;
Knowledge is a Region in Weight Space for Fine-tuned Language Models. (arXiv:2302.04863v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04863
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#20102;&#19981;&#21516;&#27169;&#22411;&#22312;&#26435;&#37325;&#31354;&#38388;&#20013;&#30340;&#20301;&#32622;&#19982;&#24615;&#33021;&#30340;&#20851;&#32852;&#65292;&#21457;&#29616;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#22312;&#26435;&#37325;&#31354;&#38388;&#20013;&#26377;&#26126;&#30830;&#23450;&#20041;&#30340;&#21306;&#22495;&#65292;&#19988;&#36825;&#20123;&#21306;&#22495;&#20013;&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;&#39640;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#32469;&#36807;&#36825;&#20123;&#21306;&#22495;&#65292;&#21487;&#20197;&#24471;&#21040;&#24615;&#33021;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#30340;&#26032;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30740;&#31350;&#19968;&#30452;&#19987;&#27880;&#20110;&#29702;&#35299;&#21333;&#20010;&#27169;&#22411;&#22312;&#21333;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#35757;&#32451;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#25110;&#27979;&#35797;&#30340;&#27169;&#22411;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#20102;&#35299;&#29978;&#23569;&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#19981;&#21516;&#27169;&#22411;&#30340;&#26435;&#37325;&#31354;&#38388;&#21644;&#28508;&#22312;&#30340;&#25439;&#22833;&#22320;&#24418;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20026;&#39640;&#24615;&#33021;&#32780;&#36827;&#34892;&#24494;&#35843;&#20248;&#21270;&#30340;&#27169;&#22411;&#23384;&#22312;&#20110;&#26435;&#37325;&#31354;&#38388;&#20013;&#23450;&#20041;&#26126;&#30830;&#30340;&#21306;&#22495;&#20013;&#65292;&#21453;&#20043;&#20134;&#28982;&#8212;&#8212;&#20219;&#20309;&#22312;&#36825;&#20123;&#21306;&#22495;&#20013;&#30340;&#27169;&#22411;&#37117;&#34920;&#29616;&#20986;&#39640;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#30456;&#21516;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#26435;&#37325;&#31354;&#38388;&#20013;&#24418;&#25104;&#19968;&#20010;&#32039;&#23494;&#30340;&#32858;&#31867;&#65292;&#32780;&#22312;&#30456;&#21516;&#22522;&#30784;&#20219;&#21153;&#19979;&#20174;&#19981;&#21516;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#21017;&#24418;&#25104;&#19968;&#20010;&#36739;&#26494;&#25955;&#30340;&#32858;&#31867;&#12290;&#27492;&#22806;&#65292;&#32469;&#36807;&#27169;&#22411;&#20043;&#38388;&#30340;&#21306;&#22495;&#20250;&#29983;&#25104;&#24615;&#33021;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#30340;&#26032;&#27169;&#22411;&#65292;&#29978;&#33267;&#22312;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research on neural networks has focused on understanding a single model trained on a single dataset. However, relatively little is known about the relationships between different models, particularly those trained or tested on different datasets. We address this by studying how the weight space and the underlying loss landscape of different models are interconnected.  Specifically, we demonstrate that finetuned models that were optimized for high performance, reside in well-defined regions in weight space, and vice versa -- that any model that resides anywhere in those regions also exhibits high performance. Notably, we show that language models that have been finetuned on the same dataset form a tight cluster in the weight space, while models finetuned on different datasets from the same underlying task form a looser cluster. Moreover, traversing around the region between the models leads to new models that perform comparably or even better than models obtained via finetuning, even on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#25968;&#25454;&#30693;&#35782;&#34701;&#21512;&#26041;&#27861;&#65292;&#21487;&#20197;&#21512;&#24182;&#22312;&#19981;&#21516;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#24314;&#31435;&#30340;&#21333;&#20010;&#27169;&#22411;&#65292;&#20197;&#24471;&#21040;&#19968;&#20010;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#39046;&#22495;&#19978;&#34920;&#29616;&#33391;&#22909;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#22495;&#22806;&#25968;&#25454;&#30340;&#21333;&#19968;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2212.09849</link><description>&lt;p&gt;
&#36890;&#36807;&#21512;&#24182;&#35821;&#35328;&#27169;&#22411;&#30340;&#26435;&#37325;&#23454;&#29616;&#26080;&#25968;&#25454;&#30693;&#35782;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Dataless Knowledge Fusion by Merging Weights of Language Models. (arXiv:2212.09849v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09849
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#25968;&#25454;&#30693;&#35782;&#34701;&#21512;&#26041;&#27861;&#65292;&#21487;&#20197;&#21512;&#24182;&#22312;&#19981;&#21516;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#24314;&#31435;&#30340;&#21333;&#20010;&#27169;&#22411;&#65292;&#20197;&#24471;&#21040;&#19968;&#20010;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#39046;&#22495;&#19978;&#34920;&#29616;&#33391;&#22909;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#22495;&#22806;&#25968;&#25454;&#30340;&#21333;&#19968;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24050;&#25104;&#20026;&#26500;&#24314;&#19979;&#28216;NLP&#27169;&#22411;&#30340;&#27969;&#34892;&#33539;&#24335;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#24050;&#32463;&#21487;&#29992;&#65292;&#20294;&#20854;&#35757;&#32451;&#25968;&#25454;&#19981;&#21487;&#29992;&#65292;&#30001;&#20110;&#25968;&#25454;&#38544;&#31169;&#25110;&#30693;&#35782;&#20135;&#26435;&#38382;&#39064;&#12290;&#36825;&#23601;&#36896;&#25104;&#20102;&#36328;&#27169;&#22411;&#34701;&#21512;&#30693;&#35782;&#20197;&#20135;&#29983;&#26356;&#22909;&#30340;&#21333;&#19968;&#27169;&#22411;&#30340;&#38556;&#30861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24314;&#31435;&#22312;&#19981;&#21516;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#30340;&#21333;&#20010;&#27169;&#22411;&#20043;&#38388;&#21512;&#24182;&#30340;&#38382;&#39064;&#65292;&#20197;&#24471;&#21040;&#19968;&#20010;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#39046;&#22495;&#19978;&#34920;&#29616;&#33391;&#22909;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#22495;&#22806;&#25968;&#25454;&#30340;&#21333;&#19968;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#25968;&#25454;&#30693;&#35782;&#34701;&#21512;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#21442;&#25968;&#31354;&#38388;&#20013;&#21512;&#24182;&#27169;&#22411;&#65292;&#30001;&#26435;&#37325;&#24341;&#23548;&#65292;&#20197;&#26368;&#23567;&#21270;&#21512;&#24182;&#27169;&#22411;&#21644;&#21333;&#20010;&#27169;&#22411;&#20043;&#38388;&#30340;&#39044;&#27979;&#24046;&#24322;&#12290;&#22312;&#19968;&#31995;&#21015;&#35780;&#20272;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#26174;&#33879;&#20248;&#20110;&#22914;Fisher&#21152;&#26435;&#24179;&#22343;&#25110;&#27169;&#22411;&#38598;&#25104;&#31561;&#22522;&#32447;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#22810;&#35821;&#35328;&#24494;&#35843;&#26367;&#20195;&#26041;&#26696;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#20219;&#20309;&#39069;&#22806;&#27880;&#37322;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#21487;&#27604;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning pre-trained language models has become the prevalent paradigm for building downstream NLP models. Oftentimes fine-tuned models are readily available but their training data is not, due to data privacy or intellectual property concerns. This creates a barrier to fusing knowledge across individual models to yield a better single model. In this paper, we study the problem of merging individual models built on different training data sets to obtain a single model that performs well both across all data set domains and can generalize on out-of-domain data. We propose a dataless knowledge fusion method that merges models in their parameter space, guided by weights that minimize prediction differences between the merged model and the individual models. Over a battery of evaluation settings, we show that the proposed method significantly outperforms baselines such as Fisher-weighted averaging or model ensembling. Further, we find that our method is a promising alternative to multi-
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#33258;&#22238;&#24402;&#31243;&#24207;&#29983;&#25104;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#28151;&#21512;&#34920;&#26684;-&#25991;&#26412;&#38382;&#31572;&#20013;&#30340;&#25968;&#20540;&#25512;&#29702;&#21644;&#36328;&#24230;&#25552;&#21462;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#31243;&#24207;&#29983;&#25104;&#30340;&#20934;&#30830;&#24615;&#24182;&#25552;&#39640;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2211.03462</link><description>&lt;p&gt;
NAPG&#65306;&#29992;&#20110;&#28151;&#21512;&#34920;&#26684;-&#25991;&#26412;&#38382;&#31572;&#30340;&#38750;&#33258;&#22238;&#24402;&#31243;&#24207;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
NAPG: Non-Autoregressive Program Generation for Hybrid Tabular-Textual Question Answering. (arXiv:2211.03462v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.03462
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#33258;&#22238;&#24402;&#31243;&#24207;&#29983;&#25104;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#28151;&#21512;&#34920;&#26684;-&#25991;&#26412;&#38382;&#31572;&#20013;&#30340;&#25968;&#20540;&#25512;&#29702;&#21644;&#36328;&#24230;&#25552;&#21462;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#31243;&#24207;&#29983;&#25104;&#30340;&#20934;&#30830;&#24615;&#24182;&#25552;&#39640;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#34920;&#26684;-&#25991;&#26412;&#38382;&#31572;&#35201;&#27714;&#20174;&#24322;&#26500;&#20449;&#24687;&#36827;&#34892;&#25512;&#29702;&#65292;&#25512;&#29702;&#31867;&#22411;&#20027;&#35201;&#20998;&#20026;&#25968;&#20540;&#25512;&#29702;&#21644;&#36328;&#24230;&#25552;&#21462;&#12290;&#24403;&#21069;&#30340;&#25968;&#20540;&#25512;&#29702;&#26041;&#27861;&#36890;&#36807;&#33258;&#22238;&#24402;&#35299;&#30721;&#31243;&#24207;&#24207;&#21015;&#65292;&#27599;&#20010;&#35299;&#30721;&#27493;&#39588;&#29983;&#25104;&#36816;&#31639;&#31526;&#25110;&#25805;&#20316;&#25968;&#12290;&#28982;&#32780;&#65292;&#36880;&#27493;&#35299;&#30721;&#23384;&#22312;&#26292;&#38706;&#20559;&#24046;&#65292;&#24182;&#19988;&#30001;&#20110;&#38169;&#35823;&#20256;&#25773;&#65292;&#31243;&#24207;&#29983;&#25104;&#30340;&#20934;&#30830;&#24615;&#38543;&#30528;&#35299;&#30721;&#27493;&#39588;&#30340;&#23637;&#24320;&#32780;&#24613;&#21095;&#19979;&#38477;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#33258;&#22238;&#24402;&#31243;&#24207;&#29983;&#25104;&#26694;&#26550;&#65292;&#21487;&#20197;&#29420;&#31435;&#29983;&#25104;&#21253;&#21547;&#36816;&#31639;&#31526;&#21644;&#25805;&#20316;&#25968;&#30340;&#23436;&#25972;&#31243;&#24207;&#20803;&#32452;&#65292;&#21487;&#20197;&#35299;&#20915;&#38169;&#35823;&#20256;&#25773;&#38382;&#39064;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#31243;&#24207;&#29983;&#25104;&#30340;&#36895;&#24230;&#12290;&#22312;ConvFinQA&#21644;MultiHiertt&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#38750;&#33258;&#22238;&#24402;&#31243;&#24207;&#29983;&#25104;&#26041;&#27861;&#21487;&#20197;&#27604;&#24378;&#22823;&#30340;FinQANet&#26041;&#27861;&#24102;&#26469;&#23454;&#36136;&#24615;&#30340;&#25913;&#36827;&#65288;+5.06 Exe Acc&#28857;&#21644;+4.80 Prog Acc&#28857;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hybrid tabular-textual question answering (QA) requires reasoning from heterogeneous information, and the types of reasoning are mainly divided into numerical reasoning and span extraction. Current numerical reasoning methods autoregressively decode program sequences, and each decoding step produces either an operator or an operand. However, the step-by-step decoding suffers from exposure bias, and the accuracy of program generation drops sharply as the decoding steps unfold due to error propagation. In this paper, we propose a non-autoregressive program generation framework, which independently generates complete program tuples containing both operators and operands, can address the error propagation issue while significantly boosting the speed of program generation. Experiments on the ConvFinQA and MultiHiertt datasets show that our non-autoregressive program generation method can bring about substantial improvements over the strong FinQANet (+5.06 Exe Acc and +4.80 Prog Acc points) 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26816;&#32034;&#36719;&#25552;&#31034;&#26377;&#25928;&#36741;&#21161;&#30828;&#25552;&#31034;&#65292;&#22312;&#22686;&#21152;&#23569;&#37327;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2210.03029</link><description>&lt;p&gt;
&#36890;&#36807;&#26816;&#32034;&#36719;&#25552;&#31034;&#22686;&#24378;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#34920;&#29616;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Efficiently Enhancing Zero-Shot Performance of Instruction Following Model via Retrieval of Soft Prompt. (arXiv:2210.03029v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03029
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26816;&#32034;&#36719;&#25552;&#31034;&#26377;&#25928;&#36741;&#21161;&#30828;&#25552;&#31034;&#65292;&#22312;&#22686;&#21152;&#23569;&#37327;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#21319;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#34920;&#29616;&#25928;&#29575;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#65292;&#35201;&#20040;&#36890;&#36807;&#25193;&#23637;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#24635;&#25968;&#65292;&#35201;&#20040;&#22686;&#21152;&#27169;&#22411;&#30340;&#22823;&#23567;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#31034;&#24494;&#35843;&#33719;&#21462;&#36719;&#25552;&#31034;&#65292;&#25506;&#32034;&#20102;&#22914;&#20309;&#36890;&#36807;&#26816;&#32034;&#36719;&#25552;&#31034;&#26377;&#25928;&#36741;&#21161;&#30828;&#25552;&#31034;&#26469;&#36827;&#34892;&#38646;&#26679;&#26412;&#20219;&#21153;&#27867;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#31034;&#24494;&#35843;&#20026;&#27599;&#20010;&#25552;&#31034;&#35757;&#32451;&#36719;&#25552;&#31034;&#23884;&#20837;&#65292;&#23384;&#20648;&#19982;&#25552;&#31034;&#23884;&#20837;&#26144;&#23556;&#30340;&#35757;&#32451;&#23454;&#20363;&#26679;&#26412;&#65292;&#24182;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#26816;&#32034;&#26368;&#25509;&#36817;&#26597;&#35810;&#23454;&#20363;&#30340;&#35757;&#32451;&#23454;&#20363;&#23545;&#24212;&#30340;&#25552;&#31034;&#23884;&#20837;&#12290;&#34429;&#28982;&#21482;&#22686;&#21152;&#20102;0.007%&#30340;&#39069;&#22806;&#21442;&#25968;&#65292;&#26816;&#32034;&#36719;&#25552;&#31034;&#25552;&#39640;&#20102;T0&#22312;&#26410;&#35265;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#22312;11&#20010;&#25968;&#25454;&#38598;&#20013;&#26377;10&#20010;&#34920;&#29616;&#20248;&#20110;T0&#65292;&#24182;&#19988;&#23558;T0&#22312;BIG-bench&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;2.39&#20010;&#30334;&#20998;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25253;&#21578;&#20102;&#19968;&#20010;&#26377;&#24847;&#24605;&#30340;&#21457;&#29616;&#65292;&#21363;&#26816;&#32034;&#22312;&#30456;&#20284;&#31572;&#26696;&#36873;&#25321;&#26684;&#24335;&#19978;&#35757;&#32451;&#30340;&#28304;&#23884;&#20837;&#27604;&#25552;&#31034;&#23884;&#20837;&#26356;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Enhancing the zero-shot performance of instruction-following models requires heavy computation, either by scaling the total number of training datasets or the model size. In this work, we explore how retrieval of soft prompts obtained through prompt tuning can efficiently assist hard prompts in zero-shot task generalization. Specifically, we train soft prompt embeddings for each prompt through prompt tuning, store the samples of the training instances mapped with the prompt embeddings, and retrieve the corresponding prompt embedding of the training instance closest to the query instance during inference. While only adding 0.007% additional parameters, retrieval of soft prompt enhances the performance of T0 on unseen tasks by outperforming it on 10 out of 11 datasets as well as improving the mean accuracy of T0 on BIG-bench benchmark by 2.39% points. Also, we report an interesting finding that retrieving source embeddings trained on similar answer choice formats is more important than t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#23545;&#35805;&#35843;&#26597;&#20013;&#23454;&#26102;&#29983;&#25104;&#36861;&#21152;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#26469;&#24341;&#23548;&#29983;&#25104;&#36807;&#31243;&#65292;&#29983;&#25104;&#26356;&#20855;&#20449;&#24687;&#37327;&#12289;&#36830;&#36143;&#24615;&#21644;&#28165;&#26224;&#24230;&#30340;&#36861;&#21152;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2205.10977</link><description>&lt;p&gt;
&#20998;&#31867;&#35843;&#26597;&#20013;&#22522;&#20110;&#30693;&#35782;&#30340;&#36861;&#21152;&#38382;&#39064;&#29983;&#25104;&#30340;&#30693;&#35782;&#39537;&#21160;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
What should I Ask: A Knowledge-driven Approach for Follow-up Questions Generation in Conversational Surveys. (arXiv:2205.10977v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.10977
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#23545;&#35805;&#35843;&#26597;&#20013;&#23454;&#26102;&#29983;&#25104;&#36861;&#21152;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#26469;&#24341;&#23548;&#29983;&#25104;&#36807;&#31243;&#65292;&#29983;&#25104;&#26356;&#20855;&#20449;&#24687;&#37327;&#12289;&#36830;&#36143;&#24615;&#21644;&#28165;&#26224;&#24230;&#30340;&#36861;&#21152;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#26102;&#29983;&#25104;&#36861;&#21152;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#21551;&#29992;&#26356;&#21160;&#24577;&#21644;&#20010;&#24615;&#21270;&#30340;&#35843;&#26597;&#32467;&#26500;&#65292;&#26174;&#33879;&#25552;&#39640;&#23545;&#35805;&#35843;&#26597;&#30340;&#36136;&#37327;&#21644;&#29992;&#25143;&#20307;&#39564;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#30693;&#35782;&#39537;&#21160;&#30340;&#23545;&#35805;&#35843;&#26597;&#20013;&#30340;&#36861;&#21152;&#38382;&#39064;&#29983;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20154;&#31867;&#32534;&#20889;&#30340;&#24102;&#26377;&#23545;&#35805;&#21382;&#21490;&#21644;&#26631;&#35760;&#30693;&#35782;&#30340;&#36861;&#21152;&#38382;&#39064;&#12290;&#38500;&#20102;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#24182;&#39564;&#35777;&#20102;&#19968;&#32452;&#26080;&#21442;&#32771;Gricean&#21551;&#21457;&#24335;&#35780;&#20272;&#25351;&#26631;&#65292;&#31995;&#32479;&#35780;&#20272;&#29983;&#25104;&#30340;&#36861;&#21152;&#38382;&#39064;&#30340;&#36136;&#37327;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#22522;&#20110;&#30693;&#35782;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#26469;&#24341;&#23548;&#29983;&#25104;&#36807;&#31243;&#65292;&#29983;&#25104;&#20449;&#24687;&#20016;&#23500;&#12289;&#36830;&#36143;&#21644;&#28165;&#26224;&#30340;&#36861;&#21152;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22522;&#20110;GPT&#30340;&#22522;&#20934;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#20004;&#38454;&#27573;&#27169;&#22411;&#29983;&#25104;&#30340;&#36861;&#21152;&#38382;&#39064;&#26356;&#20855;&#20449;&#24687;&#37327;&#65292;&#36830;&#36143;&#24615;&#21644;&#28165;&#26224;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating follow-up questions on the fly could significantly improve conversational survey quality and user experiences by enabling a more dynamic and personalized survey structure. In this paper, we proposed a novel task for knowledge-driven follow-up question generation in conversational surveys. We constructed a new human-annotated dataset of human-written follow-up questions with dialogue history and labeled knowledge in the context of conversational surveys. Along with the dataset, we designed and validated a set of reference-free Gricean-inspired evaluation metrics to systematically evaluate the quality of generated follow-up questions. We then propose a two-staged knowledge-driven model for the task, which generates informative and coherent follow-up questions by using knowledge to steer the generation process. The experiments demonstrate that compared to GPT-based baseline models, our two-staged model generates more informative, coherent, and clear follow-up questions.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#35774;&#35745;&#21644;&#29983;&#25104;&#21512;&#25104;&#21019;&#26032;&#32593;&#32476;&#35789;&#20856;&#65292;&#30740;&#31350;&#20102;&#26032;&#31526;&#21495;&#21457;&#29616;&#22312;&#22686;&#21152;&#35789;&#27719;&#30340;&#36807;&#31243;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#25506;&#35752;&#20102;&#21019;&#26032;&#36807;&#31243;&#30340;&#25972;&#20307;&#32479;&#35745;&#21644;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/1806.07722</link><description>&lt;p&gt;
&#21019;&#26032;&#39118;&#26684;&#21270;&#65306;&#36890;&#36807;&#36880;&#27493;&#21487;&#29992;&#38543;&#26426;&#21270;&#35789;&#20856;&#36827;&#34892;&#26102;&#38388;&#36724;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Stylized innovation: generating timelines by interrogating incrementally available randomised dictionaries. (arXiv:1806.07722v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1806.07722
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#35774;&#35745;&#21644;&#29983;&#25104;&#21512;&#25104;&#21019;&#26032;&#32593;&#32476;&#35789;&#20856;&#65292;&#30740;&#31350;&#20102;&#26032;&#31526;&#21495;&#21457;&#29616;&#22312;&#22686;&#21152;&#35789;&#27719;&#30340;&#36807;&#31243;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#25506;&#35752;&#20102;&#21019;&#26032;&#36807;&#31243;&#30340;&#25972;&#20307;&#32479;&#35745;&#21644;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#21019;&#26032;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#23427;&#26159;&#19968;&#20010;&#21160;&#24577;&#12289;&#25345;&#32493;&#30340;&#36807;&#31243;&#65292;&#21487;&#33021;&#39640;&#24230;&#20381;&#36182;&#20110;&#25991;&#21270;&#12289;&#32463;&#27982;&#25110;&#36816;&#27668;&#31561;&#30636;&#24687;&#19975;&#21464;&#30340;&#22240;&#32032;&#12290;&#36825;&#24847;&#21619;&#30528;&#23545;&#30495;&#23454;&#19990;&#30028;&#36807;&#31243;&#30340;&#20219;&#20309;&#20998;&#26512;&#24517;&#28982;&#26159;&#21382;&#21490;&#24615;&#30340;&#65292;&#22240;&#27492;&#21487;&#33021;&#20026;&#26102;&#24050;&#26202;&#65292;&#20294;&#20063;&#26080;&#27861;&#30830;&#23450;&#21019;&#26032;&#20043;&#38388;&#30340;&#36830;&#25509;&#32467;&#26500;&#25110;&#23646;&#24615;&#26159;&#20160;&#20040;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#23581;&#35797;&#36890;&#36807;&#35774;&#35745;&#21644;&#29983;&#25104;&#19968;&#32452;&#21512;&#25104;&#21019;&#26032;&#32593;&#32476;&#8220;&#35789;&#20856;&#8221;&#65292;&#29992;&#20110;&#25215;&#36733;&#26679;&#26412;&#21019;&#26032;&#26102;&#38388;&#36724;&#65292;&#25506;&#27979;&#36825;&#20123;&#36807;&#31243;&#30340;&#25972;&#20307;&#32479;&#35745;&#21644;&#34892;&#20026;&#65292;&#24182;&#30830;&#23450;&#23427;&#20204;&#23545;&#32467;&#26500;&#25110;&#29983;&#25104;&#31639;&#27861;&#30340;&#20381;&#36182;&#31243;&#24230;&#12290;&#22240;&#27492;&#65292;&#21463;&#21040;Fink&#12289;Reeves&#12289;Palma&#21644;Farr&#65288;2017&#65289;&#20851;&#20110;&#35821;&#35328;&#12289;&#32654;&#39135;&#21644;&#25216;&#26415;&#21019;&#26032;&#30340;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;&#25105;&#30740;&#31350;&#20102;&#26032;&#31526;&#21495;&#21457;&#29616;&#22914;&#20309;&#20197;&#39069;&#22806;&#30340;&#8220;&#35789;&#27719;&#8221;&#35789;&#20856;&#20013;&#30340;&#21333;&#35789;&#22686;&#21152;&#30340;&#26041;&#24335;&#21576;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key challenge when trying to understand innovation is that it is a dynamic, ongoing process, which can be highly contingent on ephemeral factors such as culture, economics, or luck. This means that any analysis of the real-world process must necessarily be historical - and thus probably too late to be most useful - but also cannot be sure what the properties of the web of connections between innovations is or was. Here I try to address this by designing and generating a set of synthetic innovation web "dictionaries" that can be used to host sampled innovation timelines, probe the overall statistics and behaviours of these processes, and determine the degree of their reliance on the structure or generating algorithm. Thus, inspired by the work of Fink, Reeves, Palma and Farr (2017) on innovation in language, gastronomy, and technology, I study how new symbol discovery manifests itself in terms of additional "word" vocabulary being available from dictionaries generated from a finite nu
&lt;/p&gt;</description></item></channel></rss>